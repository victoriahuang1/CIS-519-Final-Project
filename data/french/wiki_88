<doc id="16680" url="https://fr.wikipedia.org/wiki?curid=16680" title="Interface de programmation">
Interface de programmation

En informatique, une interface de programmation applicative (souvent désignée par le terme pour "") est un ensemble normalisé de classes, de méthodes ou de fonctions qui sert de façade par laquelle un logiciel offre des services à d'autres logiciels. Elle est offerte par une bibliothèque logicielle ou un service web, le plus souvent accompagnée d'une description qui spécifie comment des programmes "consommateurs" peuvent se servir des fonctionnalités du programme "fournisseur".

Dans l'industrie contemporaine du logiciel, les applications informatiques se servent de nombreuses interfaces de programmation, la programmation se fait en réutilisant des "briques" de fonctionnalités fournies par des logiciels tiers. Cette construction par assemblage nécessite pour le programmeur de connaître la manière d’interagir avec les autres logiciels, qui dépend de leur interface de programmation. Le programmeur n'a pas besoin de connaître les détails de la logique interne du logiciel tiers, et celle-ci n'est généralement pas documentée par le fournisseur.

Des logiciels tels que les systèmes d'exploitation, les systèmes de gestion de base de données, les langages de programmation, ou les serveurs d'applications comportent une interface de programmation.

Une interface de programmation est une façade clairement délimitée par laquelle un logiciel offre des services à d'autres logiciels. L'objectif est de fournir une porte d'accès à une fonctionnalité en cachant les détails de la mise en œuvre. Une interface de programmation peut comporter des classes, des méthodes ou des fonctions, des types de données et des constantes. Le plus souvent, une interface de programmation est mise en œuvre par une bibliothèque logicielle qui fournit une solution à un problème informatique en faisant abstraction de son fonctionnement.

La description de l'interface de programmation spécifie comment des clients peuvent interagir avec un logiciel en mettant l'accent sur les fonctionnalités offertes par le logiciel et en cachant les détails de son fonctionnement. Une interface de programmation peut être utilisée dans de nombreux programmes et sert alors de jeu de construction, offrant des pièces de fonctionnalités qui peuvent être incorporées dans des applications. Les programmeurs créent des interfaces de programmation pour les autres programmeurs, pour l'industrie informatique, mais aussi parfois pour leurs propres besoins.

La création et l'utilisation des interfaces de programmation est un sujet incontournable de la programmation contemporaine. Une application se sert généralement de nombreuses interfaces de programmation ; mises en œuvre par des bibliothèques logicielles qui peuvent elles-mêmes se servir d'autres interfaces de programmation.

En architecture orientée services les applications peuvent dépendre de fonctionnalités tierces offertes par des logiciels via des interfaces de programmation mises en œuvre par des services web.

Les interfaces de programmation permettent de gagner du temps par la collaboration et la spécialisation des équipes de développement de logiciel. Par exemple plus personne n'écrit un SGBD maison pour une application informatique. Les programmeurs réutilisent les SGBD existants dans le commerce, fournis par des entreprises spécialisées dans ce type de produit, et se concentrent sur la logique propre à leur application. De nombreux produits d'infrastructure sont ainsi disponibles sous forme de framework ou de bibliothèque.

Une interface de programmation permet par exemple à un programme d'accéder aux services offerts par le système d'exploitation qui héberge le programme. L'interface "sockets" est un exemple classique d'interface de programmation qui permet à un programme d'exploiter les possibilités de la couche réseau du système d'exploitation.

Une des interfaces les plus connues est la Windows API. C'est une collection de fonctions, de types de données et de constantes, en langage de programmation C, qui permet à des programmeurs de créer des applications pour les systèmes d'exploitation Windows. Elle offre la possibilité de manipuler des fichiers, des processus, communiquer par les réseaux et manipuler des interfaces graphiques.

Dans l'industrie contemporaine du logiciel, les applications informatiques se servent de nombreux logiciels tiers tels que par exemple un système d'exploitation, des bibliothèques standard, des serveurs d'application. Chacun de ces produits a sa propre interface de programmation. Ainsi chaque programmeur impliqué dans le développement de tels produits est impliqué, consciemment ou non, dans le "business" des interfaces de programmation. Du point de vue "business" une interface de programmation est un contrat par lequel un fournisseur de fonctionnalité décrit son offre, les fonctionnalités qu'il offre et les éventuelles limitations.

La construction par assemblage de logiciels nécessite pour le constructeur de savoir comment les pièces interagissent entre elles. Celui qui réalise l'assemblage n'a pas besoin de connaitre tous les détails des mécanismes internes d'une pièce, du moment qu'il connait son interface de programmation. L'interface de programmation est le minimum qu'un programmeur a besoin de connaitre pour pouvoir se servir d'une pièce de logiciel tierce. Pour une fonction c'est le nom de celle-ci, les paramètres et le type du résultat, ainsi que les effets observables de la fonction.

Une interface de programmation est distribuée sur le marché comme un produit logiciel ordinaire, régulièrement amélioré. La mise en œuvre des fonctionnalités utilisables à travers l'interface de programmation peut changer régulièrement, tout en maintenant la façade conforme au contrat.



Les interfaces de programmation peuvent avoir plusieurs fonctions. Par exemple, dans Java EE, on trouve les fonctions suivantes :

Des API peuvent servir à simplifier l'accès à d'autres API 

Les interfaces de programmation peuvent véhiculer des métadonnées.


L'API Management est une discipline, un outil de gestion des API, une solution informatique qui a pour vocation d'aider les organisations, les entreprises à exploiter et à publier au mieux les API sans mettre en péril le système d’information. C'est un moyen d’interconnecter de façon sécurisée des services internes et externes. En outre, l'API Management permet aussi d’avoir une certaine visibilité sur les demandes d’accès à ses APIs. Celui-ci sécurise les API exposées, facilite la gestion de leur cycle de vie, aide à piloter la consommation des API, outille le reporting pour le suivi des contrats. L'API Management implique donc la création de ressources d'assistance à l'utilisateur, qui définissent et documentent les API.




</doc>
<doc id="16686" url="https://fr.wikipedia.org/wiki?curid=16686" title="Ill (France)">
Ill (France)

LIll est une rivière française qui baigne la plaine d'Alsace. Important affluent gauche du Rhin et principale rivière d'Alsace, elle coule dans les départements du Haut-Rhin et du Bas-Rhin, dans la région .

La longueur de son cours est de .
L’Ill prend sa source sur le Glaserberg dans le Jura alsacien à Winkel avec une résurgence à Ligsdorf. A Winkel elle prend sa source à l'Est du village, en un endroit désigné, en 1591, sous le nom de "Illentsprung" c'est-à-dire "origine de l'Ill", dans un verger sur les pentes du Glaserberg. Son courant permettait de faire tourner les roues à aubes des scieries et des moulins.
Elle contourne alors Ferrette par l’est. La Largue (autre rivière sundgauvienne) se jette dans l’Ill à Illfurth. L’Ill bifurque vers le nord et se jette dans le Rhin à Offendorf, en aval de Strasbourg, après le barrage hydro-électrique de Gambsheim. Avant les travaux de rectification du Rhin, l'Ill se jetait dans le Rhin à la hauteur de la Wantzenau.

Elle arrose successivement Altkirch, Mulhouse, Colmar, Sélestat et Strasbourg.

À Mulhouse, l’Ill se divisait originellement en deux bras pour former une petite île sur laquelle, selon la légende, se serait implanté un moulin à eau autour duquel se serait bâtie la ville de Mulhouse. Ces bras ont ensuite, au cours du Moyen Âge, été aménagés et divisés pour servir de douves aux remparts de la ville. De nos jours, la plupart des douves ont été comblées. Seuls deux bras subsistent, tous deux recouverts :
À son entrée dans Mulhouse, l’Ill se déverse en grande partie dans le canal de décharge qui dévie le débit de l’Ill dans la Doller et protège le centre historique de Mulhouse des inondations.

Au niveau d’Erstein, l’Ill est reliée à un canal de décharge et un canal d’alimentation qui permettent d’en contrôler le débit à l’entrée de Strasbourg. Dans la traversée de Strasbourg, elle se divise en plusieurs bras et concourt à la renommée touristique du quartier de la "Petite France". Dans les péniches, on trouve bistrots, restaurants et discothèques.

L'Ill est polluée aux PCB sur la quasi-totalité de son cours.
Cinq communes alsaciennes tirent leur nom de l'hydronyme de cette rivière : 
D'autres toponymes ont la même racine : l"Illwald" (la "forêt de l'Ill", proche de Sélestat) en est un exemple.

Il est très probable aussi que le nom de la région d'Alsace provienne de celui de l'Ill. Le nom alsacien de la région est "Elsass :"
Dans cette logique, "Elsass" signifierait « le pays au bord de l'Ill » ou le « pays de l'Ill », le substantif "pays" étant dérivé de "saß" ("l'assise"/"le siège").

(Haut Rhin) : , Ligsdorf, Raedersdorf, Oltingue, Fislis, Werentzhouse, Durmenach, Roppentzwiller, Waldighofen, Illtal, Oberdorf (Haut-Rhin), Grentzingen, Henflingen, Bettendorf (Haut-Rhin), Hirsingue, Hirtzbach, Carspach, Altkirch, Walheim (Haut-Rhin), Tagolsheim, Illfurth, , Frœningen, Hochstatt, , Zillisheim, Brunstatt-Didenheim, Mulhouse, Bourtzwiller, Illzach, Sausheim, Ruelisheim, Ensisheim, réguisheim, Meyenheim, Oberentzen, Niederentzen, Biltzheim, Oberhergheim, Niederhergheim, Sainte-Croix-en-Plaine, Logelheim, Sundhoffen, Andolsheim, Horbourg-Wihr, Colmar, Illhaeusern,

(Bas-Rhin) : , Sélestat, Ehnwihr (Muttersholtz), Ebersmunster, Kogenheim, Sermersheim, Huttenheim, Benfeld, Sand (Bas-Rhin), Matzenheim, Osthouse, Erstein, Nordhouse, Hipsheim, Wibolsheim (Eschau), Eschau, Ohnheim (Fegersheim), Illkirch-Graffenstaden, Ostwald, Meinau (Strasbourg), Montagne Verte (Strasbourg), Strasbourg, Quartier européen de Strasbourg, Schiltigheim, Bischheim, Robertsau (Strasbourg), Hœnheim, La Wantzenau, Gambsheim, Offendorf, .

Ses principaux affluents, hormis la Largue, issue comme l'Ill du Jura sundgauvien, lui parviennent par sa rive gauche, issus du massif vosgien : la Doller, la Thur, la Lauch, la Fecht, le Giessen (et son affluent la Liepvrette), l’Andlau, l'Ehn, la Bruche et la Souffel. Elle est aussi alimentée du côté droit par la nappe phréatique rhénane qui lui apporte de petits affluents dont la Blind à Ehnwihr, entre les confluents de la Fecht et du Giessen.

Le régime de l’Ill est pluvio-océanique, caractérisé par des hautes eaux en hiver et au début du printemps, avec des basses eaux en été ainsi qu'au début de l’automne.
Le débit de l'Ill a été observé pendant une période de 41 ans (1974-2014), à la station hydrométrique de Chasseur-Froid, La Robertsau, commune de Strasbourg (chef-lieu du département du Bas-Rhin), située peu avant la confluence de l'Ill avec le Rhin, à . La surface étudiée à cet endroit est de soit près de 97 % de la totalité du bassin versant de la rivière qui est de .

Le module de la rivière à Strasbourg est de .

L'Ill présente des fluctuations saisonnières de débit fort peu importantes. Les hautes eaux se déroulent en hiver et sont caractérisées par des débits mensuels moyens allant de à , de décembre à mars inclus (avec un très léger maximum en février). Dès le mois d'avril, le débit baisse très doucement jusqu'aux basses eaux qui ont lieu en été de juillet à octobre, avec une baisse du débit moyen mensuel allant jusqu'à aux mois d'août et de septembre, ce qui reste très abondant. Mais les variations de débit peuvent être plus importantes selon les années.

Ainsi le VCN3 ou débit d'étiage peut chuter jusqu'à en cas de période quinquennale sèche, soit , ce qui doit être considéré comme restant plus que confortable : environ du débit nominal.

Toutefois les crues peuvent être assez importantes, sans être vraiment dévastatrices, d'autant plus que des canaux de décharge existent (avant Strasbourg notamment). Les QIX 2 et QIX 5 valent respectivement 140 et . Le QIX 10 vaut tandis que le QIX 20 se monte à et le QIX 50 à .
Le débit instantané maximal enregistré à Strasbourg, depuis 1974, a été de le 17 février 1990, tandis que la valeur journalière maximale avait été de la veille. En comparant la première de ces valeurs aux différents QIX de la rivière, il apparaît que cette crue de février 1990 était supérieure à celle prévue par le QIX 50 et était donc tout à fait exceptionnelle.

Au total, l'Ill est une rivière abondante, puissamment alimentée par les fortes précipitations vosgiennes, partiellement compensées cependant par la faiblesse des précipitations sur la plaine d'Alsace. La lame d'eau écoulée dans le bassin versant de l'Ill est de annuellement, ce qui est assez élevé, nettement supérieur à la moyenne d'ensemble de la France (plus ou moins par an, et même supérieur à la totalité du bassin du Rhin ( par an environ) bénéficiant pourtant des très importantes précipitations alpines. Le débit spécifique (ou Qsp) atteint par seconde et par kilomètre carré de bassin.

L'Ill est navigable d'Ostwald à l'embouchure du Canal de la Marne au Rhin, ce qui représente seulement 10 km de voie navigable. Il est cependant à noter que le barrage Vauban se trouvant au porte de Strasbourg dans le quartier de la Petite France coupe cette voie navigable en deux. Les portes de celui-ci ne sont en effet conçues que pour des embarcations de petit gabarit. Cet obstacle peut néanmoins être contourné par le Canal du Rhône au Rhin et le Canal de la Marne au Rhin par lesquels l'Ill est aussi relié de façon navigable au Port autonome de Strasbourg et au Rhin.

La partie navigable de l'Ill est quasi exclusivement utilisée par la flotte de bateaux-mouche du Port autonome de Strasbourg utilisant aussi le Canal du Faux-Rempart afin de faire le tour de la Grande Île et circulant entre le Barrage Vauban et le Quartier européen avec un départ non loin du Palais Rohan.



</doc>
<doc id="16689" url="https://fr.wikipedia.org/wiki?curid=16689" title="Fork (programmation)">
Fork (programmation)

La fonction fork fait partie des appels système standard d'UNIX (norme POSIX). Cette fonction permet à un processus (un programme en cours d'exécution) de donner naissance à un nouveau processus qui est sa copie conforme, par exemple en vue de réaliser un second traitement parallèlement au premier. Un bon moyen de visualiser l'effet d'un fork sur un processus est d'imaginer une division cellulaire.

Il existe une filiation dans les processus : le créateur d'un nouveau processus est appelé le père et le nouveau processus, le fils.
La plupart des attributs système du père (par exemple les droits sur le système de fichier) sont transmis au fils, de la même manière que l'héritage. Au démarrage d'un système Unix, un seul processus existe (de numéro 1). Tous les autres processus qui peuvent exister au cours de la vie du système descendent de ce premier processus, appelé codice_1, via des appels système comme codice_2, codice_3, codice_4 ou d'autres moyens. Sur les premiers UNIX, seul l'appel système codice_2 permet de créer de nouveaux processus.

L'appel système fork fournit une valeur résultat qui est entière. Pour identifier le père du fils nouvellement créé, il suffit de regarder la valeur de retour du fork(). Celle-ci peut être le PID du fils, auquel cas nous sommes dans le processus père, ou bien 0 auquel cas nous sommes dans le processus fils. Le fork() peut également renvoyer la valeur -1, qui traduit une erreur lors de l’exécution de la commande.

Afin d'obtenir le numéro du processus, il suffit de faire l'appel système getpid(), ou getppid() pour obtenir le numéro du père.

Il est possible d’interagir entre processus de plusieurs manières différentes. Premièrement, on peut envoyer des signaux. En langage de commande "kill <pid>" permet de tuer le processus ayant pour pid ce que l'on entre dans la commande.

Il est possible de faire attendre un processus grâce à "sleep(n)" pour bloquer le processus pendant n secondes, ou en utilisant "pause()" qui bloque jusqu'à la réception d'un signal.

Pour mettre fin à un processus, on peut utiliser "exit(etat)" sachant que etat est un code de fin, par convention 0 si ok, code d'erreur sinon (cf. errno).

Il peut être très pratique que le père attende la fin de l'un de ses fils, pour ce faire on utilise "pid_t wait(int *ptr_etat)" qui donne comme valeur de retour le pid du fils qui a terminé, et le code de fin est stocké dans ptr_etat.

On peut également attendre la fin du fils grâce à son pid : "pid_t waitpid(pid_t pid, int *ptr_etat, int options)."

Un terme commun dans la partie « Système » de l'informatique est ce que l'on appelle les processus zombies. Cela arrive quand le processus est terminé mais que le père n'a pas attendu son fils, c'est-à-dire qu'il n'a pas fait d'appels à "wait()". C'est une situation qu'il convient d'éviter absolument car le processus ne peut plus s'exécuter mais consomme encore des ressources. 

Chaque processus d'un fork possède son propre espace d'adressage, qu'il est coûteux de dupliquer, même avec des astuces comme le "copie-sur-écriture". Il est parfois avantageux de remplacer les forks par des fils (processus légers) qui partagent le même espace mémoire… aux risques et périls du programmeur, cependant.

La fonction "fork" est largement utilisée dans les applications client-serveur avec plusieurs clients simultanés.



</doc>
<doc id="16690" url="https://fr.wikipedia.org/wiki?curid=16690" title="Héphaïstos">
Héphaïstos

Dans la mythologie grecque, Héphaïstos ou Héphaestos (en grec ancien ) est le dieu du feu, de la forge, de la métallurgie et des volcans. 

Selon les sources, il est le fils d'Héra et de Zeus, ou d'Héra seule. Il est habituellement représenté sous les traits d'un forgeron boiteux, mais il est d'abord un inventeur divin et un créateur d'objets magiques. Dès Homère, son nom est utilisé par métonymie pour désigner le feu.

Selon l'habitude de l"'interpretatio romana", il est assimilé par les Romains au dieu Vulcain.

Les Anciens expliquaient le nom d'Héphaïstos comme « », c'est-à-dire « ce(lui) qui brûle, qui est allumé ». Diverses autres hypothèses ont également été avancées, comme un rapprochement avec / , « briller ». En réalité, l'étymologie de son nom est particulièrement obscure, comme c'est souvent le cas des noms de divinités grecques.

Néanmoins, un dérivé de "haph-" (présent "háptō") « attacher » et « enflammer » est vraisemblable.

Héphaïstos est unanimement présenté comme le fils d'Héra, mais il n'est pas sûr qu'il ait eu un père. Homère en fait le fils de Zeus et Cinéthon celui de Talos, un Géant de bronze, mais dans la version majoritaire, Héra, jalouse du fait que Zeus ait engendré seul Athéna, et pour lui montrer qu'elle pouvait se passer de lui, engendre seule Héphaïstos, une caractéristique semblable à celles d'autres « Feux divins ». C'est originellement un Feu « fils de lui-même ».

Lorsqu'elle lui donne le jour, elle le trouve si laid qu'elle le jette en bas de l'Olympe, et c'est depuis cette chute qu'il boite. Il tombe alors dans la mer et est recueilli par Thétis et Eurynomé, qui l'élèvent pendant neuf ans, à l'insu de tous, dans une grotte de l'île de Lemnos, où il fait son apprentissage d'artisan en façonnant des bijoux.

Pour se venger de sa mère, Héphaïstos fabrique un trône d'or aux bras articulés, qui emprisonne quiconque s'y assoit, et l'envoie dans l'Olympe en guise de présent. Héra s'y installe imprudemment et se trouve immobilisée, sans que nul ne sache comment la délivrer. Les dieux confient d'abord à Arès le soin d'aller chercher Héphaïstos, en vain. Enivré par Dionysos, Héphaïstos se laisse fléchir et revient dans l'Olympe délivrer sa mère. Zeus, soulagé, propose au dieu forgeron d'exaucer l'un de ses vœux : sur le conseil de Poséidon, Héphaïstos demande la main d'Aphrodite – requête à laquelle, visiblement, il n'est pas donné suite ; de son côté, Dionysos, auréolé de sa réussite, obtient le droit d'entrer dans l'assemblée des dieux.

Une autre légende se rattache à l'infirmité du dieu : Héphaïstos prend le parti de sa mère lors d'une querelle entre Zeus et celle-ci ; il reproche à son père de l'avoir laissée suspendue dans les airs, une chaîne d'or au poignet et une enclume à chaque cheville. Furieux, Zeus saisit Héphaïstos par un pied et le précipite du haut de l'Olympe. La chute du dieu dure une journée entière ; il atterrit sur l'île de Lemnos, dont les habitants, les Sintiens, le recueillent et le soignent. Le récit semble contradictoire avec celui où Héra se débarrasse d'Héphaïstos de la même façon, mais il pourrait s'agir de deux incidents séparés. On peut retrouver une certaine similitude avec Kagutsuchi, dieu du feu dans la mythologie japonaise, qui fut lui aussi puni par son père Izanagi qui le découpa en morceaux.

Les épithètes traditionnellement attachées à Héphaïstos sont « aux pieds courbes », « boiteux » et « les pieds tournés vers l'arrière ». Homère décrit ainsi son allure et sa démarche :

Cette infirmité suscite la curiosité dès l'Antiquité. Certains mythographes pensent qu'elle se rattache, à l'origine, aux démons chthoniens à l'apparence traditionnellement monstrueuse. Pour d'autres, son infirmité trouve sa source en Égypte, où le dieu Ptah-Patèque est représenté comme un nain difforme. Mais, celle-ci se rattache plus probablement au motif hérité du Feu artisan. Lié au feu du foyer souvent décrit comme faible et démuni, sa survie dépend de celui qui l'entretient. Les feux de la forge sont sédentaires et parfois infirmes comme Wieland / Völund.

À l'époque moderne, les commentateurs ont évoqué la possibilité d'une personnalisation de déformations typiques des forgerons et toreutes grecs, dues à leur exposition chronique aux métaux lourds (plomb, arsenic, mercure) contenus dans les matériaux qu'ils travaillent. Ainsi, l'intoxication à l'arsenic (élément présent sous forme d'impureté dans le cuivre) donne classiquement lieu à une atteinte nerveuse avec faiblesse musculaire, voire paralysie des muscles inférieurs. Utilisé pour durcir des métaux, l'arsenic est oxydé, lors de la fonte des minerais métallifères, en trioxyde d'arsenic. Les métallurgistes antiques, exposés à ce composé toxique, devaient probablement développer des neuropathies périphériques à l'origine de difformités et paralysies et ont ainsi associé leur dieu à ce handicap.

D'après l’"Iliade", Héphaïstos est marié à l'une des Charites (ou Grâces), qui porte simplement le nom de Charis (littéralement « Grâce »). Il en va de même dans la "Théogonie" (v. 907), mais Hésiode cite explicitement le nom d'Aglaé, la plus jeune des Charites. Cependant la tradition la plus populaire en fait le mari d'Aphrodite, cette version étant d'ailleurs déjà attestée dans un épisode fameux de l’"Odyssée" (chant ), où il tend un piège à sa femme qui le trompe avec Arès, et devient la risée des dieux. Dans les deux cas, le dieu épouse une incarnation de la beauté : il peut s'agir d'un simple contraste comique entre la belle et le boiteux, ou d'une réflexion plus profonde sur le rapport étroit entre l'artisan/artiste et la beauté. Cependant l'union d'un dieu Feu avec Aphrodite qui est originellement une déesse Aurore a d'autres justifications : le feu qu'on allume ou qu'on ranime le matin et le rite de la présentation de la jeune épouse au feu du foyer.

Contrairement à d'autres dieux, Héphaïstos n'est guère renommé pour ses aventures extraconjugales. On sait cependant qu'après avoir été abandonné par Aphrodite, il poursuit de ses avances Athéna : son sperme se répand sur la cuisse de la déesse qui l'essuie avec de la laine ( / ) qu'elle jette à terre ( / ) ; la terre ainsi fécondée donne naissance à Érichthonios, qu'Athéna recueille et élève. De ce fait, on trouve parfois l'expression « enfants d'Héphaïstos » pour désigner les Athéniens.

Sa descendance est peu nombreuse. On lui attribue notamment la paternité de :

Hygin mentionne également Philammon, Cécrops, Cercyon (lui aussi tué par Thésée sur la route d'Athènes), Philottos et Spinther.

Très habile dans son art, Héphaïstos façonne des armes remarquables, comme celles d'Achille, dont le bouclier offre une représentation parfaite du monde, celles de Memnon, la cuirasse de Diomède ou encore les cnémides d'Héraclès. Il fabrique aussi :

Héphaïstos prend part à la Gigantomachie et combat armé du feu dévorant : il tue ainsi Mimas en le recouvrant de fer en fusion. Il se range aux côtés des Achéens dans la guerre de Troie. Lorsqu'au début de l'Iliade Zeus et Héra se querellent parce que le roi des dieux a promis à Thétis d'avantager les Troyens, Héphaïstos détend l'atmosphère en remplaçant Ganymède dans son office d'échanson : voyant le dieu boiteux prendre la place du gracieux jeune homme, les dieux éclatent tous d'un rire inextinguible. Il intervient rarement dans la bataille, si ce n'est pour secourir Idéos, l'un des fils de son prêtre Darès, menacé par Diomède. Par la suite, à la demande de Thétis, il sauve Achille des eaux du Scamandre en asséchant par ses flammes le dieu-fleuve.

Il est associé dans certaines versions du mythe à la naissance d'Athéna : d'un coup de sa hache de bronze, il fait jaillir la déesse tout armée du crâne de Zeus.

Certains auteurs font également d'Héphaïstos le gardien du feu, que Prométhée dérobe pour le donner aux humains. À la demande de Zeus, Héphaïstos enchaîne le voleur au rocher de son supplice, où un aigle viendra tous les jours lui dévorer le foie. Dans cette scène représentée par Eschyle, le dieu, plein de pitié, regrette l'ordre donné par son père, et plaint le Titan pour les souffrances qu'il va endurer.

Un mythe minoritaire veut qu'il dispute à Déméter la souveraineté de la Sicile ; de la nymphe Etna, qui habite le volcan du même nom, il a les Palikes, des démons des sources chaudes. Quoi qu'il en soit, il installe sa forge dans le volcan où il travaille, aidé par les Cyclopes. Il veille ainsi au châtiment de Typhon, que Zeus a foudroyé et qui gît, inerte, sous les racines de la montagne.

L'importance du travail de la forge dans les civilisations de l'âge du bronze et à l'âge du fer explique que le personnage du forgeron ait été étroitement associé au pouvoir politique et à la religion. Ainsi, à Citium (actuelle Larnaca à Chypre), un culte est rendu au à des divinités du lingot de cuivre, particulièrement abondant sur l'île ; de même, il existe un lien direct entre les forges et le sanctuaire. Le nom d'Héphaïstos à proprement parler semble avoir déjà existé sous la forme "a-pa-i-ti-jo" à l'époque mycénienne.

Le travail de la forge perd de son importance à l'époque archaïque, puis classique. Le culte d'Héphaïstos est donc peu répandu. Il est vénéré principalement à Lemnos, Athènes et dans le Sud de l'Italie. La première, dont la mythologie fait la résidence du dieu, a pour capitale Héphaïsties, habitée jusqu'au par une population non-grecque que les Grecs appellent Tyrséniens. Elle accueille une fête de purification où le feu nouveau est allumé, puis distribué aux artisans.

À Athènes, l'importance du rôle joué par Héphaïstos s'explique par sa tentative de viol d'Athéna et la naissance d'Érichthonios qui en résulte. Platon lui attribue une souveraineté commune avec Athéna sur la cité de l'Attique :

Il possède trois lieux de culte à Athènes :

À l'instar de Zeus Phratrios et d'Athéna Phratria, le dieu reçoit un sacrifice lors de la fête des phratries, les Apatouries. Il est également à l'honneur de la fête des artisans, les Chalkeia, en même temps qu'Athéna "Erganè" (« industrieuse »). Enfin, les Héphaisties lui sont spécialement consacrées ; comme les Panathénées et les fêtes de Prométhée (Προμηθεια), elles comportent une lampadédromie, c'est-à-dire une course aux flambeaux qui fête le feu nouveau.

Enfin, Héphaïstos est vénéré dans le Sud de l'Italie : dans les îles Lipari et la région de l'Etna, où sa forge est située à partir de l'époque classique. Selon Pythéas, les îles sont même le théâtre d'événements miraculeux : il suffit d'y déposer du fer avec un peu d'or en guise de salaire, et l'on retrouve le lendemain le fer ouvragé de manière remarquable.

Dans l'hymne orphique qui lui est consacré, Héphaïstos est identifié à un Feu divin : « Ô feu infatigable » ! [...] « Toi le plus haut qui dévores, domptes et consumes tout. ».

Héphaïstos est rarement représenté dans l'art grec à l'exception de quelques scènes très appréciées des artistes. L'immobilisation d'Héra fait partie du décor sculpté du trône d'Amyclées, mais le retour dans l'Olympe est de loin le thème le plus populaire. Le plus souvent, le dieu est représenté monté à dos de mulet, escorté par Dionysos et des silènes et conduit devant Zeus et Héra, fréquemment accompagnés par d'autres dieux : Athéna, Arès, Artémis, Poséidon ou encore Hermès. L'exemple le plus célèbre est celui du vase François, un cratère à volutes de 570 av. J.-C. environ. La scène apparaissait également dans la décoration du sanctuaire d'Athéna "Khalkiokos" (« à la Maison de Bronze ») à Sparte, aujourd'hui disparu.

La naissance d'Athéna, où Héphaïstos intervient avec sa hache, est également un thème très prisé. On voit aussi Héphaïstos dans des représentations de la Gigantomachie, armé de tenailles et d'un soufflet de forge, comme sur les frises du trésor de Siphnos, ou encore dans un char ailé, probablement l'une de ses inventions. Quelques vases le montrent enfin dans sa forge, entouré de satyres : c'est une allusion à deux drames satyriques perdus et tous deux intitulés "Héphaïstos", l'un d'Achaïos, l'autre d'Épicharme — de même que le vase représentant un combat entre Arès et le dieu forgeron.

Héphaïstos est identifié dans l'art grec comme un artisan : il porte la hache, les pinces, le bonnet d'artisan ("pilos") ou encore la tunique à manches ("exômis"). L'infirmité du dieu est représentée par des pieds tournés en dehors, comme sur le vase François, par une béquille, comme sur la frise Est du Parthénon, ou encore par le fait de monter en amazone sur son mulet. Selon Cicéron, la statue du dieu par Alcamène le représente « à peine affublé d'une légère claudication non dénuée de grâce ».






</doc>
<doc id="16691" url="https://fr.wikipedia.org/wiki?curid=16691" title="Porto Rico">
Porto Rico

Porto Rico (en espagnol et en anglais : "Puerto Rico") est un territoire non incorporé des États-Unis avec un statut de "commonwealth". Située dans les Grandes Antilles, l'île est baignée au nord par l'océan Atlantique et au sud par la mer des Caraïbes. Le territoire est constitué de l'île de Porto Rico proprement dite, ainsi que de plusieurs îles plus petites, dont Vieques, Culebra et Isla Mona.

Christophe Colomb nomma l'île , en l'honneur de saint Jean Baptiste alors que le port fut nommé (). Finalement, les marchands et marins en sont venus à se référer à l'ensemble de l'île sous le nom de Puerto Rico tandis que San Juan est devenu le nom utilisé pour le port de commerce qui deviendra la capitale de l'île.

Les Portoricains appellent souvent leur île , dérivé de , son nom indigène taïno qui signifie . Les noms et dérivés respectivement de et sont communément utilisés pour identifier quelqu'un originaire de Porto Rico. L'île est aussi connue en espagnol sous le nom de ().

L'histoire de l'archipel de Porto Rico avant l'arrivée de Christophe Colomb n'est pas bien connue. Les connaissances actuelles viennent des recherches archéologiques et des premiers témoignages espagnols. Le premier livre approfondi sur l'histoire du Porto Rico a été écrit par en 1786, 293 ans après que les premiers Espagnols sont arrivés sur l'île.

Les premiers habitants dont on ait une trace étaient des Ortoiroides, pêcheurs et chasseurs, ils avaient développé une poterie primitive mais pas l'agriculture, on les classe dans la période archaïque. Les Archaïques venaient de Floride. En 1990, une fouille archéologique dans l'île de Vieques fit la découverte de ce qu'on pense être un homme archaïque (appelé homme Puerto Ferro), daté environ à 2000

Entre 120 et 400, les Igneris (saladoïdes), une tribu de la région sud-américaine d'Orinoco, arrivèrent. Tribu d'Arawaks, les Igneris étaient une civilisation plus avancée que celle des Archaïques. Entre le et , les Archaïques et les Igneris coexistèrent (et peut-être s'opposèrent).

Entre le et le , une autre tribu d'Arawak arriva. La culture des Taïnos se développa sur l'île, et vers l'an 1000, ils étaient devenus dominants. Les Taïnos avaient développé l'agriculture cependant ils ne connaissaient pas la poterie.

Porto Rico fut découverte par Christophe Colomb, lors de son second voyage, et la baptisa « San Juan Bautista », en l'honneur de Jean, Prince des Asturies, (1478-1497), fils de Ferdinand II d'Aragon et d'Isabelle de Castille. Il en prit possession au nom de la Couronne de Castille, le en débarquant sur la plage de l'actuelle ville d'Aguadilla.

La colonisation de l'île par les Espagnols ne commença néanmoins qu'en 1508. Elle inaugura une ère qui devait se prolonger jusqu'à la fin du et pendant laquelle l'île fut soumise aux règles des politiques mercantilistes des autorités espagnoles qui ne laissèrent aux habitants de l'île que peu d'occasions d'accumuler le capital qui aurait permis de la développer.

L'île était habitée par des Amérindiens Taïnos qui furent bientôt réduits en esclavage et décimés par les dures conditions de travail imposées par l'occupant, ainsi que par les maladies européennes contractées au contact des Espagnols. Des esclaves africains remplacèrent les Taïnos. Porto Rico devint un bastion et un port important pour l'empire espagnol.

Au et au début du l'emphase coloniale était sur les territoires plus prospères du continent américain.

Après la rapide indépendance des États d'Amérique du Sud et d'Amérique centrale dans la première partie du , Porto Rico et Cuba devinrent les seuls restes du grand empire espagnol d'Amérique.

À la suite de réformes, la population augmenta et l'économie s'améliora. Mais en 1868, la pauvreté et l'aliénation politique avec l'Espagne menèrent à un petit mais significatif soulèvement connu sous le nom de "Grito de Lares".

Le , pendant la guerre hispano-américaine, Porto Rico fut envahie par les États-Unis après un débarquement à Guánica.
Le , le traité de Paris, signé entre les États-Unis d'Amérique et l'Espagne, est ratifié par le Sénat américain après un débat houleux. En échange de 20 millions de dollars, l'Espagne cède ses dernières possessions d'Amérique latine ainsi que les Philippines.

En 1945, Luis Muñoz Marin gagne les premières élections démocratiques de l'histoire de Porto Rico, et en 1952, il aide Porto Rico à obtenir une autonomie partielle vis-à-vis des États-Unis.

En 1963, le radiotélescope d'Arecibo est inauguré. Avec son miroir sphérique de plus de de diamètre encastré dans le paysage karstique de la région d'Arecibo, il fut le plus grand radiotélescope du monde jusque dans les années 2010 et la construction du Radiotélescope sphérique de cinq cents mètres d'ouverture (FAST) en Chine. À son inauguration, un message fut envoyé vers l'espace à destination d'éventuelles civilisations extraterrestres.

En et , le Comité spécial de la décolonisation de l'ONU a demandé aux États-Unis de permettre ainsi que la restitution des terres occupées par les bases militaires de Vieques et de Ceiba.

En 2005, à la suite de l'assassinat du leader indépendantiste Filiberto Ojeda Rios, la situation se crispe de nouveau, malgré les annonces de George W. Bush.

Le , les États-Unis interrompent le système de prêts à Porto Rico rendant impossible le paiement des salaires des fonctionnaires. À la suite de ces événements, l'ONU, via le Comité spécial de la décolonisation, décide de délibérer sur la situation portoricaine le 12 juin suivant.

Le , la Chambre des représentants des États-Unis permet, par un vote de 223 voix contre 169, un processus formel d'autodétermination pour l'île. Le , le gouverneur de Porto Rico organise un référendum en deux questions demandant aux Portoricains de proroger jusqu'en 2020 le statut actuel d'« État libre associé » ou "commonwealth" et de choisir la forme future de l'administration de l'île au-delà de cette date. La volonté de changer de statut avant 2020 est approuvée à 53 % des suffrages et la volonté de devenir un État des États-Unis reçoit le soutien de 65 % des votes.

En août 2015, Porto Rico en difficulté économique récurrente depuis la crise économique de 2008, fait un défaut de paiement sur sa dette, après un non-paiement d'une tranche de 58 millions de dollars de sa dette qui s'élève au total à 73 milliards de dollars. Le mois suivant, l'administration de l'île met en avant un plan de réduction des dépenses publiques de 72 milliards de dollars sur 5 ans, avec une diminution des subventions aux subdivisions locales, une hausse de la TVA et une réduction de la masse salariale publique.

En 2017, un référendum non contraignant sur le statut de Porto Rico a lieu, alors que le territoire, toujours lourdement endetté, subit une politique d'austérité. Le rattachement aux États-Unis est choisi par 97 % des votants portoricains, mais le référendum est largement boycotté : le taux de participation est de 22,7 %.

La situation juridique et diplomatique de Porto Rico est complexe :


L'île est divisée administrativement en 78 communes, il n'y a pas d'autres divisions administratives, mais il existe une division politique-législative de 8 districts sénatoriaux (San Juan, Bayamón, Arecibo, Mayagüez-Aguadilla, Ponce, Guayama, Humacao et Carolina) et 40 districts représentatifs.
Les plus grandes villes sont : 

Porto Rico est un archipel situé dans les Caraïbes, entre la mer des Caraïbes et l’océan Atlantique. L'île principale, qui a donné son nom à l'archipel, se localise à à l’ouest de l'île Saint Thomas, aux îles Vierges, et à à l’est de la République dominicaine. L’île de Porto Rico est la plus petite et la plus orientale des îles des grandes Antilles (après Cuba, Saint-Domingue et la Jamaïque). Outre Porto Rico, les autres îles de l'archipel sont Vieques, Culebra, à l'est de l'île principale, Isla Mona, à l'ouest, et quelques îles secondaires. Seules Porto Rico, Vieques et Culebra sont habitées de manière permanente. Le nord de l'archipel forme aussi l'angle méridional du triangle des Bermudes.


Plusieurs sites de l'île font l'objet d'une protection de la faune et de la flore :

Considérée comme la « Grèce des Caraïbes », Porto Rico a une dette d'environ 73 milliards de dollars équivalant à 100 % du PIB. L'allemand Wolfgang Schäuble a répondu au secrétaire américain au Trésor : lorsque Jacob Lew a fait des remarques sur la gestion de la crise grecque. Le journal "La Tribune" conclut qu' « une banqueroute de Porto Rico coûtera fort cher aux épargnants américains. »

En juin 2016, alors que la crise économique (dix années de récession) a été aggravée par un exode massif (-9 % de résidents, -20 % de la population active) de jeunes éduqués, en particulier vers la Floride, et que le Parti républicain critique les élites politiques de l'île qui ont pratiqué un endettement massif pour financer des services publics pléthoriques sans pour autant rendre Porto Rico attractif en termes d'infrastructures et de système éducatif, le Congrès américain adopte une loi lui permettant de rénégocier sa dette et d'échapper aux saisies. Le 3 mai 2017 le gouverneur de l'île Ricardo Rossello annonce avoir demandé à entrer dans une procédure judiciaire de restructuration de dette similaire à une faillite de façon à pouvoir préserver les intérêts du peuple portoricain. La dette de Puerto Rico s'élève à 70 milliards de dollars.

Les langues officielles y sont l'espagnol et l'anglais. Bien que l'espagnol soit la langue du quotidien ainsi que la langue de l'enseignement, l'anglais reste officiel du fait de l'appartenance aux États-Unis.

Selon l"'American Community Survey", pour la période 2011-2015, 94,50 % de la population âgée de plus de 5 ans déclare parler l'espagnol à la maison, 5,34 % déclare parler l'anglais et 0,16 % une autre langue.

Selon le Pew Research Center, en 2014, 89 % des habitants de Porto Rico sont chrétiens, principalement catholiques (56 %) et dans une moindre mesure protestants (33 %). De plus, 8 % de la population ne pratiquent aucune religion et 2 % en pratiquent une autre.

La culture de Porto Rico est le résultat de plusieurs influences indigènes et internationales, principalement taïno, espagnole, africaine et nord-américaine.

L'île de Porto Rico est une importante destination touristique des Caraïbes, entre autres du fait de son rattachement administratif aux États-Unis ; les formalités d'accès sont les mêmes que pour ceux-ci. De plus, le climat est constant toute l'année avec des températures de l'ordre de 15 à .

La capitale San Juan, fondée en 1521, possède une riche histoire. Elle est le lieu d'attraction principal de l'île, notamment la vieille ville construite par les Espagnols. Elle possède de nombreux bâtiments historiques (forts, églises, etc.) et quelques musées. L'autre grande ville de l'île, Ponce, possède une atmosphère totalement différente, beaucoup moins touristique, mais plus coloniale et plus bourgeoise, industrieuse, notamment en raison de la production historique de canne à sucre dans la région pour la fabrication de rhum (c'est historiquement le siège de Seralles, la grande distillerie portoricaine).

Porto Rico est aussi le lieu d'accueil du radiotélescope d'Arecibo.

Porto Rico possède à l'est une forêt tropicale humide montagneuse préservée par un parc national nommée El Yunque. De nombreux chemins de randonnée sont accessibles à tous, pourvu que l'équipement soit adapté (il tombe en moyenne d'eau par an sur la forêt). Ce parc est le lieu de prédilection d'un des symboles de l'île, une petite grenouille très bruyante appelée "coquí".
Il existe également des lagunes naturellement bioluminescentes en trois endroits de l'île (Fajardo, Vieques et vers Ponce).
Les plages de l'île (surtout dans la partie nord-est et dans les îles de Culebra et Vieques) sont très attractives pour différentes activités : plongée, surf...

L'île est un des berceaux de la civilisation précolombienne taïno. Elle en possède de nombreux vestiges dont les plus remarquables se situent à Tibes, près de Ponce, où a été ouvert le centre des cérémonies indigènes.

Dans le film "West Side Story" (1961), des Portoricaines de New York chantaient (« Porto Rico, île hideuse … Je voudrais vivre en Amérique »). Les responsables du tourisme de l'île mettent tout en œuvre pour changer cette représentation.

Porto Rico, et principalement la ville de San Juan, est desservie par le principal aéroport des Grandes Antilles : l'aéroport international Luis-Muñoz-Marín, qui se trouve sur le territoire de la ville de Carolina.

San Juan est, avec Saint-Domingue, l'une des seules villes des Caraïbes a posséder un réseau de métro : le Tren Urbano.

Porto Rico a pour codes :




</doc>
<doc id="16692" url="https://fr.wikipedia.org/wiki?curid=16692" title="Étoile à neutrons">
Étoile à neutrons

Une étoile à neutrons est un astre principalement composé de neutrons maintenus ensemble par les forces de gravitation. De tels objets sont le résidu compact issu de l'effondrement gravitationnel du cœur d'une étoile massive quand celle-ci a épuisé son combustible nucléaire. Cet effondrement s'accompagne d'une explosion des couches externes de l'étoile, qui sont complètement disloquées et rendues au milieu interstellaire, phénomène appelé supernova. Le résidu compact n'a d'étoile que le nom : il n'est plus le siège de réactions nucléaires et sa structure est radicalement différente de celle d'une étoile ordinaire. Sa masse volumique est en effet extraordinairement élevée, de l'ordre de mille milliards de tonnes par litre, et sa masse comprise dans une fourchette très étroite, entre la masse du Soleil (voir masse de Chandrasekhar). Ainsi, une étoile à neutrons est une boule de seulement 20 à 40 kilomètres de diamètre.

À leur naissance, les étoiles à neutrons sont dotées d'une vitesse de rotation très élevée, de plusieurs dizaines de tours par seconde. Elles possèdent également un champ magnétique très intense, allant jusqu'à . Leur intérieur est également très atypique, étant principalement composé de neutrons dans un état visqueux superfluide. On y trouve également des proportions plus modestes de protons et d'électrons supraconducteurs. La région la plus centrale d'une étoile à neutrons est actuellement mal connue du fait de sa densité trop élevée. Elle peut être composée de neutrons ou de formes de matière plus exotiques ; c'est en fait un état inconnu non actuellement déterminé ni déterminable en physique.

Selon les circonstances, une étoile à neutrons peut se manifester sous divers aspects. Si elle tourne rapidement sur elle-même et qu'elle possède un puissant champ magnétique, elle projette alors le long de son axe magnétique un mince pinceau traversant ou pas de radiations, et un observateur placé approximativement dans la direction de cet axe observera une émission pulsée par un effet de phare, appelée pour cette raison pulsar. Une étoile à neutrons située dans un système binaire peut arracher de la matière à son étoile compagnon et donner lieu à une émission pulsée ou continue dans le domaine des rayons X et gamma. Isolée et sans son émission pulsée, une étoile à neutrons est nettement plus difficile à détecter car seule l'émission thermique de sa surface est éventuellement décelable.

Le concept d'étoiles à neutrons est né immédiatement après la découverte du neutron en 1932 par James Chadwick. Le physicien Lev Landau proposa alors qu'il puisse exister des astres presque entièrement composés de neutrons et dont la structure serait déterminée par un effet de mécanique quantique appelé pression de dégénérescence, à l'instar d'une autre classe d'astres, les naines blanches dont la structure est déterminée par la pression de dégénérescence des électrons. Deux ans plus tard, en 1934, les astronomes Walter Baade et Fritz Zwicky eurent l'intuition que le passage d'une étoile ordinaire hypermassive à une étoile à neutrons libérerait une quantité considérable d'énergie et donc de rayonnement électromagnétique, donnant l'illusion de l'allumage d'un astre nouveau. Ils proposèrent alors le terme de « super-nova » pour décrire ce phénomène, par opposition au phénomène de nova bien documenté et largement moins énergétique, terme finalement transformé en « supernova ». Certaines étoiles plus massives, les hypernovas, génèrent une explosion 100 fois plus puissante et projettent deux jets traversant de matière à la quasi-vitesse de la lumière au niveau des pôles.

L'étude des étoiles à neutrons n'a pris son essor qu'à partir de leur phénomène d'émission pulsée les révélant sous la forme de pulsar. Le premier pulsar découvert fut en 1967, par Jocelyn Bell, alors étudiante d'Antony Hewish. Le lien entre pulsar et étoiles à neutrons fut fait presque immédiatement par l'identification d'un pulsar au sein de la Nébuleuse du Crabe, le rémanent de la supernova historique , prouvant ainsi que les étoiles à neutrons étaient effectivement produites lors de l'explosion de supernovæ. Par la suite, de nombreux autres pulsars furent découverts au sein de rémanents de supernova. Cependant, la durée de vie d'un rémanent de supernova avant sa dispersion dans le milieu interstellaire est nettement plus brève que la durée pendant laquelle l'émission pulsée de l'étoile à neutrons est observable. Aussi la plupart des pulsars ne sont pas associés à un rémanent.

Aujourd'hui (2008) près de sont connues, la majeure partie — plus de — étant détectée sous la forme de pulsars, l'autre sous la forme de sources de rayons X (principalement binaires X ou plus rarement par leur émission de surface). Leur étude permet de reconstituer certains aspects de la physique des étoiles à neutrons.

Comme dans tout astre, la densité d'une étoile à neutrons augmente à mesure que l'on s'approche du centre. On distingue ainsi dans une étoile à neutrons plusieurs couches, selon la densité et les propriétés de la matière qui les compose.

Il n'est bien sûr pas possible d'avoir un accès direct aux régions internes des étoiles à neutrons. Cependant, certaines propriétés peuvent être mises en évidence par l'observation, comme la mesure de la masse, du rayon d'une étoile à neutrons, ou d'une combinaison de ces deux quantités.

D'autres phénomènes, comme le ralentissement des pulsars, et de brusques variations de leur vitesse angulaire (appelés "") permettent également de déterminer l'ordre de grandeur de leur champ magnétique, ainsi que de prouver que leur intérieur est superfluide.

Il est difficile de déterminer la masse d'une étoile à neutrons isolée. En revanche, si celle-ci fait partie d'un système binaire, il est possible de connaître sa masse par l'étude de son orbite. En pratique cela n'est faisable de façon robuste que si l'on a un système très serré de deux étoiles à neutrons et que l'on observe l'émission pulsée de l'une d'entre elles (voire les deux). De tels systèmes sont appelés pulsars binaires, ou pulsars doubles quand on observe l'émission pulsée des deux astres. Dans de telles configurations, il est possible de déterminer la masse des deux astres, en raison d'effets dus à la relativité générale qui dépendent de diverses combinaisons des deux masses. La prise en compte de ces effets relativistes appelés pour des raisons évidentes paramètres post-képlériens est ici indispensable, car en ne tenant compte que des effets de gravitation universelle, un seul paramètre appelé fonction de masse est déterminable, celui-ci ne donnant que peu d'information sur les deux masses. En tenant compte des corrections de relativité générale, les paramètres post-képlériens permettent de contraindre les masses de ces objets.

Le phénomène de précession du périastre est dû à la relativité générale. Celui-ci a été la première confirmation observationnelle de la relativité générale quand Albert Einstein le calcula pour la planète Mercure pour laquelle il montra qu'il expliquait les irrégularités alors inexpliquées de son orbite. Pour un système binaire dont les composantes possèdent les masses "M" et "M" et dont l'orbite a une excentricité "e" et une période "P", la précession relativiste du périastre formula_1 s'écrit :
où on a introduit la quantité formula_3 correspondant au temps caractéristique associé au rayon de Schwarzschild d'un objet d'une masse solaire, soit :
("G" est la constante de gravitation, "c" la vitesse de la lumière et "M" la masse du Soleil, soit environ .) La précession peut se réécrire :
Historiquement, la première mesure de la précession relativiste d'un pulsar binaire fut réalisée au milieu des avec le premier pulsar binaire découvert, , dont la période orbitale est de , l'excentricité de 0,6171308. La précession observée de par an permet alors de déduire une masse totale du système de solaires, soit à une bonne précision près le double de la masse de Chandrasekhar, comme attendue pour deux étoiles à neutrons. L'effet est également observé dans d'autres pulsars binaires comme ( par an), ( par an), ( par an) et ( par an). Dans tous les cas, la masse totale du système est de l'ordre de deux fois la masse de Chandrasekhar, soit dans les solaires.

Il est en principe possible que la précession observée ait d'autres causes, du moins pour partie, que l'effet de relativité générale. Cependant, l'analyse des autres sources possibles de précession (effets de marée, aplatissement des astres) indique que ces effets sont négligeables.

Une étoile à neutrons vue comme un pulsar se comporte à une excellente approximation comme une horloge dont on observe les pulsations émis à intervalles réguliers. De plus, une horloge située dans le champ gravitationnel d'un astre suffisamment massif est vue comme retardant lentement par rapport à une horloge identique restée sur Terre. Ceci provient du fait que la présence d'un champ gravitationnel affecte l'écoulement du temps. Dans l'hypothèse où une étoile à neutrons est elle-même plongée dans le champ gravitationnel d'un autre astre, l'écoulement du temps y est donc modifié par la présence à proximité de cet autre astre. Si maintenant, l'étoile à neutrons se déplace dans le champ gravitationnel de cet astre, alors cet effet d'écoulement du temps va être modulé du fait de la variation du champ gravitationnel ressenti par l'étoile à neutrons. 
Cette dernière contribution s'écrit, en notant "T" le temps « vécu » par l'étoile à neutrons (appelé temps propre) et "t" celui d'un observateur loin du champ gravitationnel de l'étoile compagnon :
"M" étant la masse du pulsar observé, "M" celle de son compagnon (observé ou non), "a" le demi-grand axe de l'orbite et "E" l'anomalie excentrique. Le premier terme n'est pas directement observable, n’étant pas distinguable de l'effet de ralentissement du temps existant à la surface de l'étoile à neutrons elle-même. Le second terme est, lui, observable dès que l'orbite est non circulaire. Il vaut :
L'effet est traditionnellement exprimé en remplaçant le demi-grand axe "a" par sa valeur donnée par la troisième loi de Kepler, soit :
Cet effet périodique est d'amplitude faible : même pour une orbite serrée (période de ), l'amplitude est de l'ordre de quelques millièmes de seconde ( pour , bien aidé par la forte excentricité du système).

La différence d'écoulement du temps en fonction du champ gravitationnel affecte aussi le temps de propagation des signaux, ce à quoi s'ajoute un effet supplémentaire dû au fait que les signaux lumineux émis par le pulsar ne se propagent pas en ligne droite quand ils passent au voisinage d'un éventuel compagnon. Ceci affecte l'intervalle de temps entre les différentes pulsations reçues du pulsar et est connu sous le nom d'effet Shapiro, du nom d'Irwin Shapiro, qui en fit la prédiction en 1964 avant sa détection grâce aux sondes Viking posées sur Mars. 
Au cours d'une orbite, les temps d'arrivée des signaux sont modulés de la quantité :
où ω est la longitude du périastre, qui est mesuré indépendamment par l'étude de l'orbite. Les quantités "r" et "s" sont appelées respectivement paramètre d'amplitude et paramètre de forme. 
Ils dépendent des masses par les formules :
Le paramètre "s" est en général inutile pour contraindre les masses, car il dépend du sinus de l'angle d'inclinaison "i" qu'il n'est pas possible de déterminer, sauf cas très particulier (par exemple en cas de binaire à éclipses). Par contre le paramètre "r" donne immédiatement la masse du compagnon de l'étoile à neutrons. L'effet Shapiro reste extrêmement faible. Son amplitude est de l'ordre du temps mis par la lumière pour parcourir une distance de l'ordre du rayon de Schwarzschild de l'étoile, soit quelques microsecondes. Il n'est ainsi pas mis en évidence dans , mais l'est dans et , qui incidemment sont tous deux vus quasiment par la tranche ("i" très proche de , son sinus étant très proche de 1).

Un système de deux corps massifs en orbite l'un avec l'autre va être le siège de l'émission d'ondes gravitationnelles, à l'instar de deux objets possédant une charge électrique qui sont le siège de l'émission de rayonnement électromagnétique, quand ils se trouvent accélérés l'un par rapport à l'autre. Les ondes gravitationnelles, prédites par Albert Einstein dans le cadre de la relativité générale en 1916, n'ont été observées directement qu'en 2015, mais leur mise en évidence explicite a été réalisée avec des étoiles à neutrons, en l'occurrence au sein du pulsar binaire . L'émission d'ondes gravitationnelles provoque une lente usure de l'orbite des deux corps, qui lentement spiralent l'un avec l'autre. En pratique, cette émission se traduit par l'observation d'une baisse de la période orbitale du système. 
Un calcul classique permet d'évaluer cette variation selon la formule :
L'effet étant cumulatif au cours du temps, il n'est pas difficile à mettre en évidence pour un pulsar binaire en orbite serrée. Par contre, il est très difficile de distinguer cette usure réelle de l'orbite par une variation apparente de la période orbitale qui, elle, est due à des considérations purement cinématiques. Si le système observé accélère ou décélère par rapport à la Terre, une variation supplémentaire de la période du signal émis (quel qu'il soit) se superpose à sa variation intrinsèque par le simple fait que la distance parcourue par le signal entre l'émission et la réception varie de façon non linéaire. En pratique, cela se produit dans deux cas : soit l'objet est effectivement accéléré, par exemple s'il tombe vers le centre d'un amas globulaire, auquel cas on parle d'accélération séculaire, soit il se déplace en ligne droite suivant un mouvement rectiligne et uniforme, mais suffisamment vite pour que sa distance varie de façon non linéaire. On parle alors d'effet Shklovski. Dans les cas où il est possible de contraindre ces effets, on peut utiliser la formule du rayonnement gravitationnel pour contraindre les masses, comme ce fut le cas pour , ce qui valut le Prix Nobel de physique aux découvreurs de cet objet, Russell Alan Hulse et Joseph Taylor, qui mirent en évidence son rayonnement gravitationnel. Le pulsar binaire est un exemple de pulsar binaire, dont on observe une usure de la période orbitale, mais dont l'amplitude ne correspond pas à la valeur attendue, les masses étant connues par ailleurs grâce aux autres paramètres post-képlériens. Il est considéré que ce désaccord provient d'une contribution notable de l'effet Shlovski que l'on contraint ici dans le cas de ce pulsar.

Plus d'une douzaine de couples d'étoiles à neutrons sont connus à ce jour, dont six ou sept permettent de déterminer assez précisément les masses des deux astres. Parmi ceux-ci, un seul est un pulsar double, , les autres ne laissant voir qu'un pulsar et un compagnon sombre. La masse déduite du compagnon étant dans la même plage de masse ( solaire), il est interprété comme étant une autre étoile à neutrons : il n'est ni assez massif pour être un trou noir, ni assez lumineux pour être une naine blanche.

Il est en principe possible de déterminer le rayon d'une étoile à neutron si l'on observe l'émission thermique en provenance de sa surface. La puissance rayonnée par un objet de rayon "R" et dont la surface est portée à la température "T" s'écrit en effet :
σ étant la constante de Stefan. Un certain nombre de pulsars sont suffisamment proches pour que leur émission de surface soit, semble-t-il, visible. C'est, entre autres, le cas de (Geminga), (pulsar de Vela), ou . Malheureusement, il est extrêmement difficile d'exploiter ce genre de relations, pour plusieurs raisons :
Finalement, une modélisation complexe est nécessaire pour pouvoir tenter d'extraire le rayon de l'étoile à neutrons, avec un résultat guère efficace. Par exemple, l'hypothèse d'une atmosphère d'éléments lourds ne marche pas du tout pour extraire le rayon des étoiles à neutrons jeunes, ceux-ci s'avérant avec cette modélisation largement trop petits pour être acceptables. C'est le cas de (l'étoile à neutrons du rémanent Puppis A), pour lequel on trouve un rayon compris entre , le pulsar de Vela (entre ), ou (entre ). Pour les étoiles à neutrons plus âgées (plus de ), le modèle s'avère plus satisfaisant, avec des résultats autorisant ou en tout cas s'approchant des valeurs de l'ordre de pour le rayon. C'est le cas de Geminga (entre ), (entre ), (plus de ), ou (entre ). L'incertitude sur la distance est ici extrêmement handicapante : l'incertitude d'un pour la distance de ou explique à elle seule l'incertitude finale sur le rayon. Il est donc à l'heure actuelle impossible de faire une analyse suffisamment précise du rayon de l'étoile pour contraindre sa structure interne et notamment la composition du cœur.

À l'inverse, la modélisation des étoiles à neutrons par une atmosphère d'hydrogène permet d'obtenir des valeurs (fort imprécises) plus compatibles avec les valeurs attendues.

Dans l'hypothèse où des atomes (éventuellement fortement ionisés) se trouvent à la surface d'une étoile à neutrons, il est en principe possible d'observer des raies d'émission ou d'absorption venant d'eux, et par suite mesurer le décalage vers le rouge d'origine gravitationnelle issu de la surface de l'étoile à neutrons. Celui-ci, noté comme de coutume "z", est donné en fonction de la masse "M" et du rayon "R" de l'étoile à neutron par la formule :
"G" et "c" étant respectivement la constante de Newton et la vitesse de la lumière. L'observation du décalage vers le rouge permet donc d'accéder directement au rapport masse-rayon de l'étoile. Or les observations des pulsars binaires contraignant assez directement la masse à une fourchette relativement étroite aux alentours de solaire, on est ainsi en mesure d'obtenir le rayon. De plus, le rapport masse-rayon ne dépend guère de la masse de l'étoile, mais surtout de sa densité centrale, qui est elle-même essentiellement déterminée par la nature de la matière qui s'y trouve. Il est ainsi possible de tester directement certains aspects relatifs à l'équation d'état de la matière des étoiles à neutrons, et, dans l'idéal de contraindre les hypothèses relatives aux étoiles étranges ou étoiles à quarks, dont le cœur est susceptible d'abriter une forme relativement exotique de matière. La raison à cela est que ces deux formes possibles d'étoiles sont notablement plus compactes qu'une étoile à neutrons ordinaire, aussi une valeur élevée du décalage vers le rouge gravitationnel est-elle un bon moyen d'attester leur existence.

Par exemple, la binaire X à faible masse , découverte par le satellite artificiel EXOSAT. Lors d'une phase d'activité en 2000, il avait été possible d'identifier de probables raies d'émission de l' et du et , c'est-à-dire d'atomes ne possédant plus qu'un seul ou deux électrons, permettant d'associer à l'astre un décalage vers le rouge gravitationnel de 0,35. D'autres détections de décalage vers le rouge avaient été effectuées auparavant, mais uniquement sur des astres à très fort champ magnétique, qui lui-même influence la position des raies spectrales (voir Effet Zeeman). L'utilisation de , dont le champ magnétique déduit des propriétés du rayonnement X de l'astre est relativement faible, échappe donc à ce type de biais.

Le décalage vers le rouge observé est relativement intéressant. Sa valeur est incompatible avec une étoile à neutrons de masse et d'équation d'état usuelles ( solaire et pas d'étoile étrange ou d'étoile à quarks). Si le cœur de l'étoile n'est pas composé de matière exotique, alors sa masse est comprise, selon les équations d'état envisageables, entre solaire, alors que si sa masse est bien de solaire, le décalage vers le rouge pointe assez fortement en faveur d'une étoile à quarks dont le cœur est dit en phase CFL (""). La mesure de la masse de l'étoile à neutrons est ainsi ici indispensable pour discriminer entre ces deux hypothèses.

À noter au passage que ce type de mesure est extrêmement sensible aux détails de la phase d'activité de l'astre. Lors d'une autre phase d'activité en 2003, et malgré un temps d'observation considérable (, soit près de ) avec le télescope spatial XMM-Newton, aucune raie précédemment mise en évidence sur cet astre n'a été revue.

La croûte d'une étoile à neutrons correspond à la région principalement composée de noyaux atomiques.

Un aperçu de la structure de la croûte peut être donné en calculant l'état de plus basse énergie de la matière à haute pression. Tant que celle-ci n'est pas trop élevée (voir ci-dessous), l'état le plus stable est a priori un cristal de noyaux atomiques tous identiques. Sous sa forme la plus compacte, le cristal est de type cubique centré. On sait qu'à pression nulle, ce cristal est composé de noyaux de fer 56 (c'est-à-dire composé de et ). Leur masse volumique est de par centimètre cube. À mesure que l'on augmente la pression la composition du noyau le plus stable est susceptible de changer, principalement pour la raison suivante : si l'on considère une maille élémentaire du cristal, celui-ci contient un noyau, possédant "Z" protons et "A" nucléons, ainsi que "Z" électrons (pas nécessairement liés au noyau, celui-ci étant susceptible d'être ionisé), alors l'équation donnant le potentiel chimique "μ" d'un des "A" nucléons s'écrit :
μ étant le potentiel chimique des électrons et "W" l'énergie de noyau, incluant son énergie de masse et son énergie de liaison. 
Cette équation se réécrit :
Les électrons vont assez vite pour ne plus être liés aux noyaux : les électrons étant des fermions, le nombre de ceux-ci ayant une basse énergie est limité, et la pression aidant, la quasi-totalité d'entre eux acquièrent une énergie suffisante pour ne plus être liés au noyau. Les électrons se comportent ainsi comme un gaz de Fermi. Dans un tel gaz, la dépendance du potentiel chimique avec la pression "P" est connue, en l'occurrence on a formula_17. Le phénomène d'enrichissement en neutrons de la matière dense peut alors s'expliquer ainsi : passer d'un noyau ("A", "Z") à un noyau ("A", "Z") peut se faire même si l'énergie de liaison par nucléon du second noyau est moindre, dans l'hypothèse où la baisse du rapport "Z"/"A" en facteur de la contribution au potentiel chimique des électrons la compense.

La difficulté de la méthode ci-dessus réside dans celle du calcul de l'énergie "W" du noyau. Celle-ci peut être obtenue expérimentalement pour des noyaux pas trop instables, mais nécessite au bout d'un moment de faire appel à l'extrapolation de formules établies, ou alors à des calculs complexes de physique nucléaire. Doit en particulier être pris en compte le fait qu'un noyau atomique peut être décrit par un formalisme appelé modèle en couches, qui révèle que certaines valeurs pour le nombre de protons et de neutrons, appelées nombres magiques confèrent une meilleure stabilité aux noyaux, à l'instar des atomes qui sont chimiquement plus stables quand ils comportent un certain nombre d'électrons (c'est la fameuse série de gaz rares, avec dans l'ordre 2, 10, 18, 26, 54, pour respectivement l'hélium, le néon, l'argon, le krypton et le radon). En physique nucléaire, les nombres magiques sont 2, 8, 20, 28, 50, 82, 126. Il existe également des « sous-couches » relativement stables, notamment celle à ou protons. Les calculs de la configuration la plus stable des noyaux remonte à 1971, avec un travail désormais classique de Gordon Baym, et Peter Sutherland, et a été amélioré par la suite par Pawel Haensel et ses collaborateurs. Les calculs révèlent ainsi qu'à partir d'une masse volumique de par centimètre cube, l'état le plus stable est d'abord composé de noyaux de nickel, à (un des nombres magiques). Ces noyaux de nickel ont au départ (soit ), et s'enrichissent de 2, puis quatre neutrons supplémentaires (nickel 66). Passé , l'état le plus stable est formé de noyaux à , s'appauvrissant progressivement en protons, allant du krypton 86 () au nickel 78 (, ce noyau est stabilisé par le fait que le nombre de neutrons et protons est à chaque fois un nombre magique). Ce noyau est le plus neutronisé à avoir été obtenu en laboratoire. La suite des prédictions, quand la masse volumique dépasse les est plus incertaine, en l'absence de données expérimentales. Il semble que le nouvel état le plus stable fasse appel à des noyaux à , s'appauvrissant progressivement en protons. Le premier de la liste est le ruthénium 126 ( et ), et le dernier le krypton 118 ( et ).
Les incertitudes sur cette dernière partie sont non négligeables, Pawel Haensel ayant fait remarquer qu'il était possible que ce soit une configuration avec une sous-couche de (zirconium) qui soit préférée.

Le tableau ci-dessous résume la succession des noyaux supposés les plus stables à mesure que la densité de la matière augmente.

Ce type de structure cristalline existe jusqu'à environ , moment où l'état le plus stable n'est plus un cristal de noyaux baignant dans un liquide d'électrons, mais un mélange noyau-neutrons libres-électrons. Cette transition est traditionnellement appelée point de fuite neutronique, car c'est le moment où il devient thermodynamiquement avantageux pour les neutrons de diffuser en dehors des noyaux.



</doc>
<doc id="16693" url="https://fr.wikipedia.org/wiki?curid=16693" title="Contradiction">
Contradiction

Une contradiction existe lorsque deux phénomènes scientifiques, affirmations, idées, ou actions s'excluent mutuellement. 

Le « principe de non-contradiction » est la loi qui veut qu’on ne peut affirmer et nier simultanément le même terme ou la même proposition : « Il est impossible qu’un même attribut appartienne et n’appartienne pas en même temps et sous le même rapport à une même chose ». Assurément, une chose peut être blanche aujourd’hui ou d’une autre couleur demain. De même, cette chose est plus grande ou plus petite qu’une autre à un moment donné. Mais, il est impossible que ces déterminations apparaissent simultanément et s’appliquent du même point de vue à cette chose. Impossible donc qu’à la fois une chose soit et ne soit pas.

La contradiction est une relation existant entre deux ou plusieurs termes ou deux ou plusieurs propositions dont l’un(e) affirme ce que l’autre nie : « A » et « non-A » sont contradictoires, les phrases « Tous les hommes sont barbus » et « Quelques hommes ne sont pas barbus » sont contradictoires. 

En logique formelle (c.-à-d. lorsque la proposition est exprimée dans le langage formel des mathématiques), la phrase « A et non-A » est l'exemple le plus caractéristique de contradiction. En définitive, toute contradiction peut être reformulée sous cette forme. La logique formelle rejette la contradiction comme une absurdité. Ainsi à partir du théorème du calcul des propositions, n'importe quoi dérive d'une contradiction. La démonstration est la suivante :

C'est l'explosion logique. Un système d'axiomes qui permet de démontrer un théorème qui est une contradiction permet de démontrer n'importe quoi (par exemple que 1=0, ou 1=1, ou 1=2, etc.). Un tel système d'axiomes n'a donc aucun intérêt. 

« A est non-A » est une phrase fausse. Autrement dit, il est possible de démontrer à l'aide du calcul des propositions que le contraire d'une contradiction est toujours vrai. Ceci est utilisé dans le cadre du raisonnement par l'absurde.

Il faut savoir que la dialectique n'exclut pas la logique formelle. La logique formelle est contenue dans la dialectique. Mais contrairement à la dialectique, elle reste dans l'instantanéité et ne prend pas en compte les phénomènes en interactions et évolutifs dans le temps. La logique formelle est donc limitée dans son application sur les grands systèmes. En effet, dans les domaines scientifiques, il est courant de rencontrer des situations qui paraissent incongrues ou inintelligibles du point de vue de la logique comme l'observation de zones extensives en montagne formée par un mouvement compressif. La dialectique du point de vue matérialiste et scientifique permet de comprendre et ainsi de dépasser les contradictions. . Ce sont essentiellement des phénomènes dynamiques de natures cycliques ou quasi-périodiques dont l'amplitude dépend de la configuration de départ.

Être non contradictoire apparaît comme essentiel à toute personne soucieuse de découvrir ce qu'est « la raison », et de ce que signifie pour elle être « raisonnable ». 

Ainsi, les phrases « Tous les hommes sont barbus » et « Quelques hommes ne sont pas barbus » sont contradictoires. Une unique phrase peut être en soi une contradiction; pour la simple raison que deux phrases, qui se contredisent, peuvent toujours être réunies en une phrase par la conjonction « et ». On peut donc (également) définir une « contradiction » par : « Une contradiction est une phrase fausse par elle-même ».

Dans le langage courant, la nature contradictoire d'une phrase devient subjective. Pour résumer : « par elle-même » signifie que l'on peut assurer (démontrer) la fausseté de la phrase sans faire appel à une affirmation ou une information complémentaire. Ainsi, par exemple, la phrase « le ciel est vert » n'est pas une contradiction, car établir sa fausseté nécessite l'observation du ciel (l'on dira alors que « le ciel est vert » contredit l'observation). En revanche « le ciel est bleu et vert » est une contradiction quelle que soit la couleur du ciel.

Ces propos sont critiquables : Il y a un nécessaire présupposé (donc des informations complémentaires) qui nous permet d'appréhender les notions de « ciel », « bleu », « vert ». Dans cet exemple, des présupposés (suffisant pour valider le propos) seraient :

Le premier présupposé est particulièrement récusable, car on peut juger que « le ciel n'est pas vert » découle de la définition du ciel ou est une vérité universellement admise. On peut également objecter, au second présupposé, que des objets peuvent être multicolores.

Ainsi, contrairement au langage formel, les présupposés du langage courant ne peuvent être considérés comme objectivement connus, ce qui laisse une part de flou dans tout propos.

L’esprit de contradiction est la tendance à prendre le contre-pied de ses interlocuteurs lors d’une discussion indépendamment de l’avis qu’ils expriment.

De manière simpliste, un paradoxe est une double contradiction : une contradiction est un paradoxe si sa contraposée (sa négation) est également une contradiction. En d'autres termes : « Un paradoxe est une phrase qui n'est par elle-même ni vraie ni fausse. ».

Dans cet exemple le philosophe Han Fei Zi montre un paradoxe du langage. Ainsi, il raconte dans un de ses ouvrages l'histoire d'un vendeur d'armes très renommé qui présente deux de ses produits au roi : une lance capable de transpercer n'importe quelle défense et un bouclier résistant à toute arme. Le roi fit remarquer la contradiction évidente dans le discours du vendeur. De cette histoire, vient le mot chinois "máodùn" (littéralement « lance-bouclier ») signifiant « contradiction » ou « paradoxe ». 




</doc>
<doc id="16701" url="https://fr.wikipedia.org/wiki?curid=16701" title="Sagittaire à feuilles en flèche">
Sagittaire à feuilles en flèche

La sagittaire à feuilles en flèche ou flèche d'eau, "(Sagittaria sagittifolia)" est une plante d'eau herbacée de la famille des Alismatacées (Monocotylédones). Elle fleurit en été.

Son nom vient du latin "sagitta", la flèche, en référence à la forme des feuilles.

Noms communs : flèche d'eau, flèchière, sagette.
"en : arrowhead, de : Pfeilkraut, es : saeta de agua, it : saetta."

C'est une plante aquatique, vivace, répandue dans les régions tempérées du vieux continent (Europe, Russie, Chine), et très commune dans les plans d'eau et les cours d'eau.

Exemple de polymorphisme foliaire, elle présente trois types de feuilles : des feuilles dressées hors de l'eau en forme de flèche aiguë, des feuilles nageantes arrondies en forme de cœur, et des feuilles immergées allongées en forme de ruban. Les fleurs, de couleur blanc rosé, sont disposées le long de la tige par verticilles de trois, les fleurs mâles en haut et les fleurs femelle dessous. La tige souterraine forme de nombreux rejets terminés en tubercules, qui servent à la multiplication.

Cette plante est cultivée pour la décoration des pièces d'eau. Il en existe une variété horticole à fleurs doubles. Elle est considérée comme plante envahissante aux États-Unis.

D'autres espèces du genre "Sagittaria" sont cultivées pour l'ornementation des aquariums. Les tubercules des Sagittaires sont consommés au Japon à la nouvelle année dans le osechi.


</doc>
<doc id="16704" url="https://fr.wikipedia.org/wiki?curid=16704" title="Nébuleuse de l'Haltère">
Nébuleuse de l'Haltère

La nébuleuse de l'Haltère est une nébuleuse planétaire située dans la constellation du Petit Renard à . Cette nébuleuse, découverte par Charles Messier le , est la première nébuleuse planétaire observée de l'histoire de l'astronomie. Elle porte le numéro 27 de son catalogue.

Cet objet est particulièrement brillant et possède un diamètre apparent très large, puisque la partie la plus lumineuse atteint 1/5 de celui de la Lune. Sachant que la vitesse d'expansion atteint 6,8 secondes d'arc par siècle, son âge est estimé à ou .

L'étoile centrale (à l'origine de la nébuleuse) a une magnitude apparente de 13,5, ce qui la rend difficilement observable pour un astronome amateur. C'est une naine blanche de couleur bleue très chaude (). Elle est peut-être accompagnée d'une autre étoile, encore plus faible (magnitude 17), à 6,5 secondes d'arc de distance apparente.

La forme particulière de la partie lumineuse a valu à cette nébuleuse le nom de "Nébuleuse de l'Haltère" ("Dumbbell" en anglais). On lui connaît également les surnoms de "Trognon de pomme", de "Sablier" (attention à la confusion avec d'autres objets !) voire de "Diabolo".

Sa magnitude empêche son observation à l'œil nu, mais avec des jumelles et de bonnes conditions météo, on peut espérer voir M27. À partir d'un télescope de 150 ou , on obtient plus facilement le brillant « trognon ». Il est encore trop tôt pour espérer voir l'étoile centrale (qui ne se dévoilera qu'en photographie à ce niveau). Avec un , muni d'un filtre interférentiel de type OIII (recommandé pour l'observation de nébuleuses planétaire et diffuses), le trognon apparaît nettement et l'étoile centrale peut être envisagée mais seulement sous un très bon ciel, sans pollution parasite, sans turbulence (l'appareil doit être à température) : l'utilisation de la vision décalée permet de mieux voir cette naine blanche. Toutefois, sous un ciel de très haute qualité (dans le Quercy, en haute montagne, en Drôme provençale) plusieurs astronomes amateurs ont observé cette étoile centrale dans des télescopes de .

Elle est observable entre mai et septembre environ. Elle se situe dans la constellation du Petit Renard, soit en plein Triangle d'été. La recherche de l'objet est relativement facile, voici les étapes à suivre avec un télescope :

Pendant cette remontée, une tache floue devrait se détacher du reste des étoiles environnantes : c'est Dumbbell. En observant plus longuement, on distingue cette forme qui la caractérise.



</doc>
<doc id="16705" url="https://fr.wikipedia.org/wiki?curid=16705" title="Louis-Philippe Ier">
Louis-Philippe Ier

, né le à Paris en France et mort le à Claremont au Royaume-Uni, est le second et dernier souverain français à avoir régné sur la France avec le titre de « roi des Français », le premier ayant été . Bien moins traditionaliste que ses prédécesseurs, il incarna un tournant majeur dans la conception et l'image de la royauté en France.

Premier prince du sang sous la Restauration, le prince Louis-Philippe a, au cours de sa vie, porté successivement les titres de duc de Valois (1773-1785), duc de Chartres (1785-1793) et enfin celui de duc d’Orléans (1793-1830) avant d’accéder à la couronne en 1830, son cousin ayant été renversé par les « Trois Glorieuses ».

Dix-huit ans à la tête d’un royaume en profondes mutations sociales, économiques et politiques, Louis-Philippe – par la monarchie de Juillet – a tenté de pacifier une Nation profondément divisée avec les armes de son époque : mise en place d’un régime parlementaire, accession de la bourgeoisie aux affaires manufacturières et financières, permettant un essor économique de première importance en France (révolution industrielle). Cependant, la chute du régime qu’il a fait naître a pour principales causes d'une part la paupérisation des « classes laborieuses » (paysans et ouvriers) et d'autre part le manque de compréhension de la part des élites de la monarchie de Juillet pour les aspirations de l’ensemble de la société française.

Louis-Philippe d'Orléans est né au Palais-Royal à Paris le 6 octobre 1773 et il est ondoyé le même jour par André Gautier, docteur en Sorbonne et aumônier du duc d'Orléans, en présence de Jean-Jacques Poupart, curé de l'église Saint-Eustache à Paris et confesseur du roi. Petit-fils de Louis-Philippe d'Orléans, duc d'Orléans, il est le fils de Louis "Philippe" Joseph d'Orléans, duc de Chartres (1747-1793), (connu plus tard sous le nom de « Philippe Égalité ») et de Louise "Marie Adélaïde" de Bourbon, "Mademoiselle de Penthièvre" (1753-1821). Il est titré duc de Valois de sa naissance à la mort de son grand-père en 1785, puis, son père ayant relevé le titre de duc d'Orléans, duc de Chartres. 

Le , Louis-Philippe d'Orléans, est baptisé le même jour que son frère Antoine d'Orléans, dans la chapelle royale du château de Versailles par l'évêque de Metz et grand aumônier de France Louis-Joseph de Montmorency-Laval en présence d'Aphrodise Jacob, curé de l'église Notre-Dame de Versailles : son parrain est le roi et sa marraine est la reine Marie-Antoinette.

Son éducation est dans un premier temps confiée à la marquise de Rochambeau, nommée gouvernante et à Madame Desroys, sous-gouvernante. À l'âge de cinq ans, le jeune duc de Valois passe entre les mains du Chevalier de Bonnard nommé sous-gouverneur en décembre 1777. Suite aux intrigues de la comtesse de Genlis, proche du duc et de la duchesse de Chartres, Bonnard est congédié au début de l'année 1782, alors que la comtesse de Genlis est nommée Gouvernante des enfants royaux. Cette dernière, adepte d’une pédagogie rousseauiste et moralisatrice, subjugue Louis-Philippe qui confie dans ses "Mémoires" qu'en dépit de sa sévérité, il a été adolescent quasiment amoureux d’elle.

Comme son père le duc d'Orléans, Louis-Philippe, devenu duc de Chartres en 1785, est un partisan de la Révolution française. Sous l'influence de sa gouvernante, Madame de Genlis, il entre au club des Jacobins et soutient notamment la formation de la Constitution civile du clergé.

Entamant une carrière militaire, le duc de Chartres prend le commandement le juin 1791, du régiment de dragons avec le grade de colonel. Il est promu maréchal de camp le 7 mai 1792, puis il participe à la tête de la en tant que lieutenant-général aux batailles de Valmy, Jemappes où il joue un rôle non négligeable en évitant la retraite du centre lors du premier assaut, et Neerwinden (son titre de lieutenant-général au service des armées républicaines et plus tard au service de , lui vaut d'ailleurs son inscription sur l’arc de triomphe de l'Étoile).

Il tente de persuader son père de ne pas participer au procès de . "Philippe Égalité" vote cependant la mort du roi. Portant le poids de la responsabilité du régicide de son père, il sera regardé avec hostilité par les émigrés royalistes.

Il rejoint la Belgique en avril 1793 à la suite de son chef, le général Dumouriez, après sa tentative de putsch contre la Convention.

Il est proscrit par le gouvernement révolutionnaire, accusé de collusion avec le « traître » Dumouriez. Pendant la Terreur, son père est jugé et exécuté le 6 novembre 1793. Devenu duc d'Orléans, il passe en Suisse où il exerce le métier de professeur au collège de dans les Grisons sous le nom de Chabaud-Latour mais sa fausse identité est démasquée, l'obligeant encore à émigrer. Les années suivantes, toujours sous un nom d'emprunt, il visite les pays scandinaves, part pour une expédition en Laponie qui le conduit jusqu'au cap Nord. « Premier Français à gagner le cap Nord, il en garde fierté et envoie en 1838, une frégate porter sur les lieux son buste en bronze ». 

En 1796, le Directoire consent à la libération des deux jeunes frères de Louis-Philippe à la condition que celui-ci s'embarque aux États-Unis avec eux. Ils s'installent à Philadelphie, puis effectuent un périple « "authentiquement aventureux" » de quatre mois au nord-est du pays. Entre le printemps 1798 et l'automne 1799, ils séjournent à La Havane avant d'en être chassés par le gouvernement espagnol, désireux de se rapprocher du Directoire. L’arrivée au pouvoir de Bonaparte ne met pas fin à son exil durant l’Empire, et Louis-Philippe et ses frères s'installent en Angleterre en janvier 1800.

En 1809, Louis-Philippe met fin à de vagues projets de mariage avec la fille du roi , Élisabeth de Hanovre, qui rencontrent de nombreuses difficultés. Il se réfugie en Sicile et épouse Amélie de Bourbon (1782-1866), princesse des Deux-Siciles et fille du roi des Deux-Siciles (elle est la nièce de Marie-Antoinette, sœur de sa mère et donc cousine de et de "Madame Royale"). Le couple s'installe alors à Palerme, au palais d'Orléans, et ils ont dix enfants. 

Par deux fois en 1808 et 1810, Louis-Philippe tente de prendre les armes en Espagne contre les armées napoléoniennes mais voit ses projets contrariés par le refus du gouvernement britannique.

Après l’abdication de Napoléon Bonaparte en 1814, Louis-Philippe rentre vivre en France, confirmé dans le titre de duc d’Orléans dont il a hérité à la mort de son père, et sa demeure, le Palais-Royal.

Sous la Restauration, les règnes de et de , la popularité de Louis-Philippe grandit. Il incarne une opposition mesurée à la politique des "ultras" du royalisme et ne rejette pas l'intégralité de la Révolution française. Opposition qui s'illustre notamment par sa réprobation de la Terreur blanche et son exil volontaire en Angleterre entre 1815 et 1817.

Louis-Philippe prend garde à se conduire modestement et bourgeoisement, envoyant ses fils au lycée Henri-IV. Néanmoins, cette « comédie des manières simples » ne correspond qu’imparfaitement au caractère de Louis-Philippe, qui possède l'« orgueil de sa race » et est entiché de sa naissance. Au lendemain de la mort de , il obtient ainsi le rang d'altesse royale accordé par . De même, grâce au nouveau roi, il est le plus grand des indemnisés de la loi du milliard aux émigrés de 1825.

En 1830, la révolution des « Trois Glorieuses » renverse , qui abdique (avec le contre-seing de son fils le dauphin Louis de France) en faveur de son petit-fils le duc de Bordeaux. Cependant les députés instituent le duc d’Orléans comme lieutenant-général du Royaume le 30 juillet 1830 et Charles X prend finalement le chemin de l'exil.

Louis-Philippe se fait proclamer '"roi des Français"' par la Chambre des députés le 9 août 1830, par la grâce d'une charte valant constitution. Ce nouveau titre, déjà porté par de 1789 à 1792, est une innovation constitutionnelle liant la nouvelle monarchie populaire au peuple, et non plus au pays, au territoire. Il s'agit donc d'une monarchie contractuelle fondée sur un "Pacte" entre le roi et les représentants de la nation.

Un autre symbole fort de la nouvelle monarchie, appelée « monarchie de Juillet », est l’usage du drapeau tricolore pour remplacer le drapeau blanc de la Restauration. Cette arrivée au pouvoir à la faveur d’un soulèvement populaire vaut à Louis-Philippe l’hostilité des cours européennes et le surnom de « roi des barricades » ou encore « roi bourgeois ».

Les partisans de « », qui contestent la légitimité de l'accession au trône de Louis-Philippe, font partie des légitimistes qu'on désigne comme les "henriquinquistes". En effet, les « vrais » légitimistes considèrent que est toujours roi et que son abdication est nulle, Louis-Philippe étant considéré comme un usurpateur. Sa légitimité est non seulement remise en cause par le comte de Chambord, mais aussi par les Républicains. Louis-Philippe gouverne donc au centre regroupant la tendance royaliste (orléaniste) et libérale.

En 1832, la fille de Louis-Philippe, Louise, devient la première reine des Belges en épousant .

Le 28 juillet 1835, le roi réchappe de justesse à la « machine infernale » du Corse Giuseppe Fieschi.

Le 25 juin 1836, le roi réchappe de justesse au « fusil-canne » de Louis Alibaud, qui est condamné à la peine du parricide.
En 1838, il envoie une expédition au Mexique pour ce qui est appelé la guerre des Pâtisseries.
À partir de 1842 commence l'installation en Côte d'Ivoire, par le traité de Grand-Bassam. Les troupes françaises s'emparent d'abord de la zone lagunaire.

En 1843, par l'intermédiaire de Rochet d'Héricourt, un traité d'amitié et de commerce est signé avec le souverain du Choa Sahlé Sellassié.

En signe d"'Entente cordiale" entre la France et le Royaume-Uni, le roi Louis-Philippe reçoit la reine Victoria dans son château d'Eu, à deux reprises en 1843 et 1845, tandis qu'il visite la souveraine britannique au château de Windsor en 1844.

Victor Hugo mentionne dans "Choses vues" que ce roi graciait volontiers les condamnés à mort sous son règne, .

Pendant quelques années, Louis-Philippe règne plutôt modestement, évitant l'arrogance, la pompe et les dépenses excessives de ses prédécesseurs. En dépit de cette apparence de simplicité, les soutiens du roi viennent de la moyenne bourgeoisie. Au début, il est aimé et appelé le « Roi Citoyen », mais sa popularité souffre quand son gouvernement est perçu comme de plus en plus conservateur et monarchique. Il est régulièrement raillé, caricaturé, brocardé et les doutes sur ses talents de monarque bourgeois se cristallisent dans ce mot de Victor Hugo : « Le roi actuel a une grande quantité de petites qualités . »

Le soutien donné d'abord au parti du « Mouvement » dirigé par Adolphe Thiers fait place au conservatisme incarné par François Guizot. Sous sa conduite, les conditions de vie des classes populaires se détériorent, les écarts de revenus augmentant considérablement. Une crise économique en 1846-1848 et les scandales liant des personnalités du gouvernement (affaire Teste-Cubières, affaire Choiseul-Praslin), joints aux actions du parti républicain qui organise la campagne des Banquets, amènent le peuple à une nouvelle révolution contre le roi lorsque celui-ci interdit le banquet du 22 février 1848, entraînant la démission de Guizot le 23 février.

Dans la semaine précédant la révolution, le roi ne prend pas conscience de la gravité des événements qui se préparent. Le prince Jérôme Napoléon essaie, lors d’une visite aux Tuileries, de l’en avertir. Il raconte la scène à Victor Hugo, qui la rapporte dans ses carnets à la date du 19 février. Le roi se contente de sourire et de dire : 

« Mon prince, je ne crains rien. » Et il ajoute : « Je suis nécessaire. » 

Devant le déroulement de l’insurrection, Louis-Philippe abdique le 24 février 1848 en faveur de son jeune petit-fils « » (son fils et héritier, le prince royal Ferdinand-Philippe, étant mort dans un accident à Neuilly-sur-Seine en 1842) :

Craignant de subir le même sort que et Marie-Antoinette, il se déguise et quitte Paris pour Dreux, où il passe la nuit.

Cependant la Chambre des députés, quoique prête, de prime abord, à accepter son petit-fils comme roi, doit faire face à des insurgés qui envahissent le palais Bourbon. Suivant l’opinion publique, elle décide de confier le pouvoir à un gouvernement provisoire qui, dans la soirée, à l’hôtel de ville de Paris, proclame la Deuxième République dans des circonstances controversées.

Voyageant dans une voiture banale sous le nom de « Mr. Smith », le roi déchu embarque le 2 mars au Havre sur un paquebot en direction de l'Angleterre où il s'installe avec sa famille au château de Claremont (Surrey) mis à disposition par la reine Victoria.
Louis-Philippe meurt le 26 août 1850 dans son lieu d'exil et est inhumé dans la chapelle Saint-Charles Borromée à Weybridge. En 1876, son corps ainsi que celui de sa femme la reine Marie-Amélie, morte le 24 mars 1866, sont ramenés à la chapelle royale Saint-Louis, nécropole familiale que sa mère a fait construire en 1816 à Dreux, et qu'il a lui-même fait agrandir pendant son règne.


1804 : Élisabeth du Royaume-Uni (1770-1840), fille du roi ; le mariage n'aboutit pas.

1809 : Louise-Marie-Amélie de Bourbon-Siciles, princesse des Deux-Siciles (1782-1866), fille du roi des Deux-Siciles et de l'archiduchesse Marie-Caroline d'Autriche.



 (François Guizot)









</doc>
<doc id="16709" url="https://fr.wikipedia.org/wiki?curid=16709" title="Berthe Morisot">
Berthe Morisot

Berthe Marie Pauline Morisot, née le à Bourges et morte le à Paris, est une peintre française, membre fondateur et doyenne du mouvement d'avant-garde que fut l'Impressionnisme.

Elle était dans le groupe impressionniste, respectée par ses camarades et admirée. À sa table, se réunissent son beau-frère Édouard Manet qui est le plus mondain, Edgar Degas, le plus ombrageux, Pierre-Auguste Renoir, le plus sociable, et Claude Monet le plus indépendant du groupe. Stéphane Mallarmé l'introduit auprès de ses amis écrivains.

Les étapes de la carrière de Berthe Morisot ne sont pas très marquées, car elle a détruit toutes ses œuvres de jeunesse. C'est à peine si l'on discerne une influence d'Édouard Manet ou de Pierre-Auguste Renoir vers la fin de sa vie. Après sa mort, la galerie Durand-Ruel a organisé une rétrospective de ses peintures, aquarelles, pastels, dessins et sculptures : il y avait plus de quatre cents pièces.

En 1983, Elizabeth Kennan, rectrice du Mount Holyoke College et C. Douglas Lewis, conservateur du département de sculptures de la National Gallery of Art admirent la peinture de Berthe Morisot et ils décident, pour célébrer le cinquantième anniversaire de la création du Mount Holyoke College, d'organiser une grande rétrospective des œuvres de l'artiste à la National Gallery of Art, car les quatre principaux mécènes du college ont été parmi les premiers à collectionner les œuvres de Berthe Morisot. Ils ont été les pionniers d'une reconnaissance qu'on ne lui accordait pas, sans doute par sexisme, selon Sophie Monneret, car les femmes-peintres ont une place restreinte dans les musées, mais depuis quelques années, on constate une forme de "réhabilitation" de Berthe Morisot. La Fondation Gianadda de Martigny a accueilli en 2002 une grande exposition de ses œuvres. Le musée Marmottan lui a consacré une grande rétrospective de mars à août 2012. C'était la première rétrospective qu'on lui accordait à Paris depuis près de cinquante ans.

Berthe Morisot était une « rebelle ». Tournant le dos très jeune à l'enseignement académique du peintre lyonnais Chocarne, elle a fondé avec Claude Monet, Auguste Renoir, Alfred Sisley, Camille Pissarro, Edgar Degas le groupe d'avant-garde les « Artistes Anonymes Associés », qui allait devenir la Société anonyme des artistes peintres, sculpteurs et graveurs regroupant des impressionnistes. Sa volonté de rupture avec les traditions, la transcendance de ses modèles, et son talent ont fait d'elle « la grande dame de la peinture » selon Anne Higonnet.

Berthe Morisot naît à Bourges où son père, Edme Tiburce Morisot, est préfet du département du Cher. Sa mère Marie-Joséphine-Cornélie Thomas est une petite-nièce du peintre Jean Honoré Fragonard.
Berthe avait deux sœurs. L'une, Yves, 1838-1893, devint plus tard Madame Théodore Gobillard, peinte par Edgar Degas sous le titre "Madame Théodore Gobillard", Metropolitan Museum of Art. Yves est bien le prénom de la jeune fille. Sa deuxième sœur, Edma, 1839-1921, pratiquait la peinture avec Berthe dont elle a fait le portrait en 1865 (collection privée). Les deux sœurs exposèrent ensemble pour la première fois au Salon en 1864, mais Edma abandonna ses pinceaux aussitôt après son mariage avec un officier de marine de Cherbourg. Les sœurs Morisot avaient aussi un frère, Tiburce, dont on ne connaît rien d'autre que la date de naissance (11 décembre 1845) et qu'on confond avec son père également prénommé Tiburce. C'est le père qui rapporte les propos enflammés que Joseph Guichard tenait à son épouse sur le talent de ses filles .

C'est en effet la mère des sœurs Morisot qui leur avait offert des leçons de peinture pour faire une surprise à son mari qui, lui-même, avait étudié l'architecture et était amateur d'art. Le père venait d'être nommé à la Cour des Comptes, mais selon les souvenirs rapportés par Tiburce, le jeune frère de neuf ans, l'enseignement de Geoffroy-Alphonse Chocarne, dans le style néo-classique, ne plaisait pas du tout aux jeunes filles. Et comme l'École des beaux-arts n'était pas ouverte aux femmes, Madame Morisot trouva un autre professeur, Joseph Guichard, dont Edma et Berthe apprécièrent beaucoup l'enseignement.

Cependant, après avoir rencontré les "copistes" au Louvre, notamment Fantin-Latour qui s'enthousiasmait pour Horace Lecoq de Boisbaudran et ses méthodes originales, Edma et Berthe demandèrent à Guichard des leçons de peinture en plein air. Guichard les confia au paysagiste Achille Oudinot, qui les confia à son tour à son ami Jean-Baptiste Camille Corot.

La famille Morisot loua une maison à Ville-d'Avray, pendant l'été, pour que les jeunes filles puissent peindre auprès de Corot, qui devint bientôt un familier de leur domicile parisien rue Franklin. Comme il était opposé à toute forme d'enseignement traditionnel, on ne sait pas si Corot donna souvent des leçons aux jeunes filles, et dans quel lieu. On remarque néanmoins que Berthe tient de lui sa palette claire et son goût pour les traces apparentes de pinceaux, ou pour les petites études de paysages.

En 1863, il y eut un phénomène qui devait marquer l'histoire de l'art : le Salon de peinture et de sculpture accepta les toiles de Corot. Mais il refusa un si grand nombre d'artistes parmi les cinq mille qui présentaient des œuvres, et cela créa un tel scandale, que l'empereur ouvrit un autre Salon : le Salon des refusés.

Cette agitation n'empêchait pas les sœurs Morisot de préparer leur premier envoi au Salon de 1864. Les Morisot louèrent une ferme dans un quartier de Pontoise nommé « Le Chou », sur les bords de l'Oise, près d'Auvers-sur-Oise. Edma et Berthe furent alors présentées à Charles-François Daubigny, Honoré Daumier et Émile Zola. Pour son premier envoi, Berthe fut admise au Salon avec "Souvenir des bords de l'Oise" et "Un vieux chemin à Auvers", Edma avec une scène de rivière à la manière de Corot. Deux critiques d'art remarquèrent les tableaux des sœurs et notèrent l'influence de Corot, mais on leur accorda peu d'attention.

L'année suivante, l'envoi de Berthe au Salon de 1865 fut remarqué par Paul Mantz, critique d'art à la " Gazette des beaux-arts", qui y voyait : , appréciation qui contraste avec celle qu'il va porter en 1881 sur la peinture lorsqu'elle montrera plus d'audace dans son style. Il est vrai que jusqu'en 1867, Berthe présentait encore des œuvres qui ne dérangeaient pas comme "La Brémondière", scène de rivière aujourd'hui disparue. Il reste un de ses premiers chefs-d'œuvre "Chaumière en Normandie" (collection particulière) où son talent éclate dans la manière de strier la toile de troncs d'arbres pour faire apparaître en arrière-plan des vues d'une chaumière.

Au Louvre, les deux sœurs ont rencontré Édouard Manet avec les copistes. Les parents Morisot donnaient des soirées où ils rencontraient les Manet. Madame Manet-mère donnait également des soirées où elle recevait les Morisot, et tout ce monde se retrouvait encore aux soirées de monsieur de Gas (père d'Edgar Degas) où étaient présents Charles Baudelaire, Emmanuel Chabrier, Charles Cros, James Tissot, Pierre Puvis de Chavannes. Cette bourgeoisie d'avant-garde était alors très mondaine. On apprit par madame Loubens (surtout connue pour le portrait que Degas a fait d'elle) que Degas avait été amoureux d'Edma, et que Manet avait exprimé son admiration pour le travail de cette même jeune fille. Le salon des Morisot était fréquenté par un nombre croissant de célibataires, parmi lesquels se trouvait Jules Ferry auquel Tiburce Morisot dénonça les dangers du baron Haussmann et ses projets urbains grandioses. Les deux sœurs avaient confié des toiles au marchand Alfred Cadart, dont elles attendaient beaucoup et qui se révéla décevant mais madame Morisot s'inquiétait moins, désormais, pour la carrière de ses filles que pour le choix de leurs époux : Yves venait d'épouser en 1866 Théodore Gobillard, un fonctionnaire mutilé d'un bras pendant la campagne du Mexique. Edma épousa deux ans plus tard Adolphe Pontillon, officier de marine, ami de Manet, avec lequel elle partit pour la Bretagne.

Après avoir passé un dernier été avec ses deux sœurs en Bretagne, chez Edma, Berthe commença une carrière indépendante. Elle peignit une vue de la rivière de Pont-Aven à Rozbras, exposée l'année suivante au Salon de 1868, avec les toiles d'Edma, qui exposait encore. La plupart des critiques négligèrent les œuvres de Berthe et Edma Morisot, cette année-là. À cette époque, le mépris pour les femmes-peintres atteignait des sommets, et Manet écrivait à Fantin-Latour .

Mais Berthe poursuivit sa carrière. En 1869, elle ramena d'une visite à sa sœur une "Vue du petit port de Lorient", National Gallery of Art.

De Lorient, en 1869, Berthe rapporta une toile représentant Edma, intitulée "Jeune femme à sa fenêtre (Madame Pontillon)", National Gallery of Art. Berthe adoptait là un style qui rappelait une scène de genre d'Alfred Stevens, tout en faisant preuve d'une bien plus grande liberté. Manet venait alors de commencer une toile semblable de plus grand format, et il éprouvait les plus grandes difficultés à traiter le visage de son modèle Eva Gonzalès, qui s'était également mise en tête de devenir son élève : Manet s'y reprit trente fois. Frustré, il s'acharnait sur le petit portrait d'Edma souhaitant que Berthe le retravaillât. Mais il en faisait les plus grands éloges. Le tableau fut d'ailleurs admis au salon de 1870 en même temps qu'un autre tableau de Berthe, de plus grand format, représentant Madame Morisot-mère et Edma, intitulé "Madame Morisot et sa fille, Madame Pontillon", également intitulé "La Lecture", National Gallery of Art. Manet était intervenu à outrance sur ce tableau, ce qui déplut à Madame Morisot-mère qui écrivait le 20 mars 1870 : Berthe n'appréciait pas les interventions du peintre sur cette toile qu'elle retoucha discrètement avant de l'envoyer au salon. Il semble que les critiques aient été au courant des interventions excessives de Manet, raison pour laquelle ils gardèrent un silence discret, ce qui irrita Manet. Berthe ne lui tint pas rigueur de cet épisode et leur amitié resta intacte. Manet avait une tendance à "s'approprier" Berthe, qu'il avait déjà fait poser pour son tableau "Le Balcon" et qu'il choisit souvent comme modèle, notamment juste après ses fiançailles avec Eugène Manet et juste après leur mariage en 1874.

Le , éclatait la guerre entre la France et la Prusse. Les frères Manet, Degas, Félix Bracquemond et d'autres artistes, étaient engagés dans la Garde nationale. Berthe accepta de partir pour Saint-Germain-en-Laye avec sa mère, mais après avoir rejoint Edma à Cherbourg où elle peignit, elle refusa de quitter la France et revint à Paris quelques mois plus tard alors que les combats s'intensifiaient autour de Paris et que la santé de la jeune fille était mise à rude épreuve. Berthe cessa de peindre pendant un temps. De Cherbourg, elle avait rapporté "Le Port de Cherbourg", 1871, collection particulière, "Femme et enfant assis dans un pré", 1871, "Au Bord de la forêt", 1871.

Il y eut ensuite un chassé-croisé d'influences mutuelles, d'emprunts parfois imperceptibles, de Manet à Morisot et inversement. Entre 1871 et 1872, Berthe réalisa un tableau représentant sa sœur, Yves Gobillard, avec sa fille, Bichette, sous le titre "Femme et enfant au balcon", collection particulière. Yves est de profil et l'enfant, de dos, tourné vers Paris, reprend une idée que Berthe avait déjà traitée dans une des aquarelles de Cherbourg : "Femme et enfant assis dans un pré" 1871, où l'enfant a également le dos tourné. L'année suivante Manet reprit la silhouette de l'enfant vue de dos, qui regarde au loin, à travers une grille dans son Chemin de fer, National Gallery of Art mais la balustrade verte de Berthe Morisot rappelle celle du Balcon de Manet.

Berthe aimait tant son tableau qu'elle en fit une copie à l'aquarelle Institut d'art de Chicago. Le personnage de dos apparaît souvent dans les toiles de Berthe. Par ce procédé, elle donnait aux portraits de famille un aspect moins affecté, qui inaugurait un nouveau genre déjà expérimenté avec la toile "Intérieur", 1871. La femme de profil au premier plan voit l'enfant écarter le rideau de la fenêtre, mais la lumière du jour est si forte que toutes les formes sont dissoutes, ce qui lui vaudra d'être refusé au Salon de 1872.

La même année, Berthe réalisa "Vue de Paris des hauteurs du Trocadéro", Santa Barbara Museum of Art, Californie. Mais elle n'était pas contente de son travail car elle écrivit à Edma que , faisant allusion au tableau que Manet peignit pendant l'exposition universelle de 1867 : "Vue de l'exposition universelle de 1867", Nasjonalgalleriet, Oslo

L'atelier de Berthe à Passy avait été endommagé par la guerre. Elle cessa de peindre un temps et préféra poser pour Manet qui, déprimé par la guerre, n'arrivait plus à travailler. De cette période date "Berthe Morisot au chapeau noir", 1872, collection particulière.

Au début de l'année 1872, par l'intermédiaire d'Alfred Stevens, le marchand Paul Durand-Ruel vint dans l'atelier de Manet et lui acheta vingt deux toiles. Au début juillet, Berthe demanda à Manet de montrer un de ses paysages de bord de mer à Durand-Ruel qui acheta : "L'Entrée du port de Cherbourg", musée Léon-Alègre, Bagnols-sur-Cèze, et trois aquarelles de Berthe dont "La Jeune Fille sur un banc (Edma Pontillon)", 1972, National Gallery of Art, puis en 1873, "Vue de Paris des hauteurs du Trocadéro" qu'il revendit à un prix respectable à Ernest Hoschedé négociant et collectionneur.

Peu à peu, Berthe allait s'écarter des couleurs sombres de Manet pour adopter des couleurs de plus en plus claires.

La maîtrise de Berthe commençait à subjuguer ses camarades qui la reconnaissaient comme une artiste à part entière, en particulier Edgar Degas. Elle commençait à se détacher des couleurs un peu sombres pour adopter des tons de plus en plus clairs, qu'elle tenait de Corot. Parfois ses couleurs étaient éclatantes comme sur la toile "Intérieur" que le jury du salon de 1872 refusa, ce qui indigna Puvis de Chavannes. Manet qui suivait toujours de très près le travail de Berthe se laissa peu à peu influencer par les teintes claires de "La Petite fille aux jacinthes", pastel, 1872, de "Jeune fille assise sur un banc (Edma Pontillon)", 1872, et du "Berceau", 1872, musée d'Orsay envoyé au salon de 1872.

"Le Berceau" marque une étape dans l'évolution de Berthe : 

C'est de cette époque que date le plein épanouissement de Berthe qui allait souvent s'installer dans la propriété de sa sœur à Maurecourt au bord de l'Oise dans les Yvelines pour travailler. Son style évolue notablement : De cette époque naîtront des œuvres comme : "Madame Boursier et sa fille" 1873, huile sur toile, , Brooklyn Museum, "Sur la pelouse", 1874, pastel, , musée du Petit Palais, Paris, "Sur la plage", 1873, Musée des beaux-arts de Virginie, Richmond (Virginie).

À l'été 1874, Berthe passa ses vacances à Fécamp avec Edma, ses enfants, et des amis de la famille qui posèrent pour elle. En vacances non loin de là, Eugène Manet, âgé de quarante et un ans, venait parfois peindre aux côtés de Berthe et surtout la courtisait. Le 22 décembre suivant, Berthe l'épousait à la Marie puis à l'église de Notre-Dame-de-Grâce de Passy. Cette année-là, Édouard fit de Berthe deux magnifiques portraits, "Portrait de Berthe Morisot à l'éventail" (palais des beaux-arts de Lille), où Berthe apparaît en deuil après la mort de son père en janvier. On distingue néanmoins sa bague de fiançailles sur la main gauche et l'éventail est replié. L'autre portrait est intitulé "Berthe Morisot à l'éventail", musée d'Orsay présente Berthe le visage caché derrière son éventail.

Le Salon de 1873 avait été houleux. Les artistes qui s'étaient vus refuser leurs travaux se plaignaient des choix conservateurs du jury. Berthe n'eut qu'un seul tableau accepté "Blanche", œuvre très conventionnelle qui représentait sans doute Blanche Pontillon bébé. Mais déjà, un groupe d'artistes composé de Monet, Pissarro, Sisley, Degas, avaient signé une charte le 27 décembre 1873, projetant d'organiser une coopérative : "La Société des artistes français", qui allait prendre le nom de Société anonyme des artistes peintres, sculpteurs et graveurs à laquelle Berthe adhéra après la mort de son père. Elle abandonnait le Salon officiel pour les expositions impressionnistes dont elle allait être l'un des éléments marquants. Ceci en dépit des conseils de Puvis de Chavannes, et du refus de Manet, qui venait de recevoir une médaille au salon de 1873 et qui ne voulait pas se joindre au groupe, . Les discussions étaient vives.

La Première exposition des peintres impressionnistes eut lieu dans les "Salons Nadar", 35 boulevard des Capucines, là où se trouvaient les anciens ateliers de Nadar. Vingt neuf artistes y participaient, Berthe étant la seule femme. Une semaine avant l'ouverture de l'exposition, Puvis de Chavannes lui envoya une lettre pour la mettre en garde contre le fiasco de cette entreprise. Mais rien n'arrêta la jeune femme. Elle affirmait ainsi son indépendance vis-à-vis de Manet qui s'était détourné de cette exposition contestataire. Parmi les huiles qu'elle envoya chez Nadar, il y avait : "Le Berceau" (musée d'Orsay), "Le Port de Cherbourg", "la Lecture", "Cache-cache", parmi les pastels : "Portrait de mademoiselle Madeleine Thomas", "Le Village de Maurecourt", "Sur la Falaise", pastel, département des arts graphiques, musée du Louvre. D'après le catalogue de l'exposition, Berthe exposa quatorze huiles, trois pastels et trois aquarelles.

Trois mille cinq cents visiteurs se bousculèrent, la critique vint en nombre. La plus remarquée fut celle parue le 25 avril dans Le Charivari signée Louis Leroy, qui, reprenant dans son article le titre d'un des tableaux de Monet "Impression, soleil levant", donna son nom au mouvement impressionniste : 

Eugène soutenait déjà Berthe à l'été 1874, au moment où la presse ridiculisait la jeune fille, l'accusant de se donner en spectacle. Mais Berthe poursuivait avec ardeur dans la voie qu'elle avait choisie. Elle s'affirmait, abandonnant un tableau dont le fond n'était pas terminé : "Portrait de madame Hubbard" Ordrupgaard museum de Copenhague, et le conservant pour le vendre, alors qu'autrefois, elle aurait détruit une œuvre inachevée. Elle participa à une vente aux enchères à Drouot où douze de ses œuvres furent vendues.

Ce fut un scandale. Renoir racontait qu'un détracteur avait qualifié Berthe de "prostituée" et que Pissarro lui avait envoyé son poing dans la figure, ce qui avait déclenché une bagarre. La police fut appelée en renfort.

Manet encourageait les journalistes à apporter leur soutien à cette vente, alors que le journal Le Figaro dénonçait les tendances révolutionnaires et dangereuses de la première exposition impressionniste dans une violente diatribe signée Albert Wolff. Le journaliste traitait les artistes d'aliénés : Eugène avait l'intention de le provoquer en duel, mais Berthe et ses camarades le détournèrent de ce projet.

Des œuvres de cette époque s'appliquent à décrire, dans des formats plus petits, le monde ouvrier que Zola célébrait, et que Monet, Pissarro et Degas choisirent aussi pour sujet à partir de 1875. Berthe elle-même participa de cette tendance avec un de ses tableaux les plus réussis : "Percher de blanchisseuses", 1875, National Gallery of Art, Washington. Cette année-là, Eugène fut contraint d'être le modèle de Berthe (il détestait poser) pour le tableau : "Eugène Manet à l'île de Wight", collection particulière.

Berthe, désormais plus sûre d'elle, chercha à vendre ses toiles. Édouard et Eugène l'encouragèrent à les envoyer à la galerie Dudley de Londres qui n'en exposa aucune. En revanche, Hoschedé acheta chez Durand-Ruel "Femme à sa toilette", scène d'intérieur inondée de lumière et traitée à grands traits, collection particulière. Certains critiques d'art, Arthur Baignières surtout, commentaient l'évolution de son style en regrettant qu'elle poussât aussi loin la recherche impressionniste : 

Les expositions de ceux que Wolff qualifie « d'aliénés » se poursuivent jusqu'en 1886, avec beaucoup de difficultés, mais beaucoup d'enthousiasme. Il y en eut huit, la troisième financée par Gustave Caillebotte. Berthe participe à toutes sauf à la quatrième (1879), car elle a fort à faire avec sa fille Julie née le 14 novembre 1878. Les femmes-peintres sont brillamment représentées cette année-là par Marie Bracquemond et Mary Cassatt.

En 1876, à la deuxième exposition du groupe, à la galerie Durand-Ruel, rue Le Peletier, Berthe expose "Jeune fille au bal", une huile sur toile, , musée d'Orsay. Ainsi que "Le Psyché" huile sur toile , musée Thyssen-Bornemisza, Madrid (ancienne collection Thyssen-Bornemisza de Lugano).

Elle est en passe de devenir une des figures de proue du groupe impressionniste, en même temps que l'américaine Mary Cassatt, venue vivre à Paris en 1874. Mais la critique conventionnelle s'offusque de sa peinture « féminine », sauf Mallarmé qui lui apporte un soutien enthousiaste.

Toutefois, les tableaux de Berthe intéressent moins les critiques d'art que ceux de Renoir, de Caillebotte, ou de Monet. Ils parlent surtout de que l'on trouve dans "Rêveuse", pastel sur toile, , Nelson-Atkins Museum of Art, Kansas city, Missouri, où dans : "La Toilette (Jeune femme de dos à sa toilette)", huile sur toile , 1875, Institut d'art de Chicago.

Les œuvres présentées en 1877 lui valent les compliments relatifs de Paul Mantz : , et ceux de Théodore Duret qui classait la jeune femme dans .

En 1880, lors de la exposition Berthe présente : "Jours d'été", huile sur toile , 1879, National Gallery, Londres, "Hiver", 1880, huile sur toile 7, Dallas Museum of Art. Pendant cette période, les toiles de Berthe engagent un dialogue avec Manet. "Jeune fille de dos à la toilette" de Morisot qui répondait à "Devant la glace" de Manet, "Jour d'été (le lac du Bois de Boulogne)" de Morisot qui répond à "En bateau" de Manet. Les critiques trouvent les toiles de l'un et de l'autre "inachevées".

Dès 1881, Berthe Morisot et Mary Cassatt apparaissent comme les chefs de file de la nouvelle tendance impressionniste aux yeux des critiques : pour la première fois dans toute l'histoire de l'art, des femmes sont considérées comme les maîtres incontestés d'un mouvement d'avant-garde.

Berthe fait preuve d'encore plus d'audace que les années précédentes, ce qui indigne deux critiques qui l'avaient appréciée jusque-là : Paul Mantz et Charles Ephrussi : Charles Ephrussi est scandalisé par les pastels : 

À partir de 1880, Berthe et sa famille passe tous ses étés dans une maison de campagne de Bougival, et, à partir de 1881, ils résident plusieurs hivers à Nice. Ces deux lieux inspirent à Berthe un grand nombre de toiles qu'elle présente aux dernières expositions "révolutionnaires".

De Nice, elle ramène "Le Port de Nice" huile sur toile en deux versions et deux formats et collection particulière, et une troisième format 38 × 46 Dallas Museum of Art; "Plage à Nice" 1881-1882, aquarelle sur papier , Nationalmuseum Stockholm.

Bougival est une source d'inspiration encore plus importante. Son tableau le plus ambitieux "Le Jardin" (1882-1883) huile sur toile, , Sara Lee Corporation est sans doute exposé à Londres par Durand-Ruel. Berthe réalise encore "Le Quai de Bougival" 1883 Nasjonalgalleriet Oslo, "Eugène Manet et sa fille dans le jardin".

De la peinture de Berthe Morisot, Gustave Geffroy dit : 

Vers 1886-1887 Berthe se mit à explorer de nouvelles techniques : sculpture, pointe sèche, qui constituaient un défi pour la coloriste virtuose qu'elle était. Elle réalisa en 1886 un buste en plâtre blanc de sa fille Julie, que Monet et Renoir l'encouragèrent à exposer chez Georges Petit chez qui ils avaient exposé eux-mêmes. Petit était un homme d'affaires avant tout : il demandait aux artistes de lui laisser une partie de leurs œuvres en compensation de ses frais. Berthe accepta ses exigences, mais Petit ne réussit pas à vendre une seule de ses sept œuvres parmi lesquelles se trouvait le buste de Julie, et "Paule Gobillard en robe de bal", un portrait de sa nièce, Paule Gobillard (1869-1946) artiste peintre également son élève, tout dans les tons de blanc. Berthe lui laissa "Le Lever".

En février 1887, Berthe fut invitée à exposer à Bruxelles avec un groupe d'artistes d'avant-garde : le Groupe des XX où Georges Seurat et Pissarro exposaient aussi. L'envoi de Berthe comprenait "Le Corsage rouge", 1885, huile sur toile, , Ordrupgaard museum de Copenhague; "Le Lever" 1886, huile sur toile , collection particulière, "le Port de Nice", 1881-1882, huile sur toile , collection particulière, "Dans la salle à manger", (1875 ou 1885-1886 selon les biographies), huile sur toile , National Gallery of Art, "Intérieur à Jersey", 1886, huile sur toile, , musée d'Ixelles.

Vers 1886-87, Berthe commença à traiter des nus au pastel, au fusain, à l'aquarelle, tous exécutés dans des tons très doux : "Jeune femme aux épaules nues", 1886, pastel sur papier, , collection privée; "Femme s'essuyant", pastel sur papier, , collection privée. Par la suite, elle s'attacha à représenter sa fille, Julie, sous tous les aspects : en joueuse de flûte avec Jeanne Gobillard, dans "Le Flageolet", 1891, huile sur toile, , collection privée, "Julie avec son lévrier", 1893. Elle avait le projet d'en faire une série. Berthe peignit aussi beaucoup de jeunes filles "La Mandoline", 1889, huile sur toile , ou "Sous l'oranger", 1889, huile sur toile, .

Le couple Manet était à ce moment-là dans le sud de la France. De retour à Paris, Berthe loua une maison à Mézy au Nord Ouest de Paris. Elle s'était aperçue que la santé d'Eugène n'était pas bonne et elle peignit très peu pendant un temps. Dans une lettre à Edma, elle exprime dans son testament le désir que Mallarmé soit le tuteur de Julie.

Berthe fit malgré tout aménager une grange en atelier et elle prit les enfants de Mézy comme modèles, mais Renoir la pressait de terminer une toile décorative dans l'esprit du "Printemps" de Botticelli, commencée à Nice en 1888. Berthe fit de nombreuses études préparatoires pour cette toile "Le Cerisier"", 1891-1892, huile sur toile , collection privée. Elle faisait désormais un grand nombre d'études préparatoires pour tous ses tableaux : elle fit trois versions de "Bergère couchée", et, tout en continuant à travailler sur "le Cerisier", elle reprit sa série de Julie Manet : "Julie Rêveuse", 1894, huile sur toile, et "Julie au violon" 1894, , collection privée.

La santé d'Eugène, âgé de 59 ans, déclinait de plus en plus. Il mourut le 13 avril 1892. Stéphane Mallarmé devient le tuteur de Julie.

Berthe avait décliné l'invitation du Groupe des Vingt pour l'exposition de Bruxelles du début 1892, mais Eugène l'avait poussée à organiser une grande exposition individuelle à la galerie Boussod et Valladon. Cette galerie, fondée par Adolphe Goupil n'était pas favorable aux impressionnistes. Elle fit de la résistance assez longtemps, même lorsqu'elle fut reprise par Bousod, le mari de la petite fille de Goupil, et Valadon, son beau-frère. Elle ne commença à s'ouvrir aux impressionnistes que sous l'influence éphémère de Théo van Gogh.

L'exposition rencontra un accueil très favorable. Degas lui dit que sa peinture vaporeuse cachait un dessin de plus en plus sûr, ce qui était le compliment suprême. Gustave Geffroy de "La Vie artistique" lui consacra des pages très élogieuses. L'année suivante, Berthe rendit visite à Monet, à Giverny, pour admirer ses cathédrales et pour conjurer sa tristesse : sa sœur, Yves Gobillard, venait de mourir en 1893, et Chabrier, en 1894 Berthe se consacra à la représentation de sa fille Julie, de ses nièces, Paule et Jeanne Gobillard : "Le Patinage au bois de boulogne" (1894). Caillebotte ayant légué sa collection au musée du Luxembourg pour y faire entrer l'impressionnisme, on s'aperçut qu'il ne possédait pas une seule toile de Berthe Morisot. Sur instance de Mallarmé, l'État français acquit pour le musée du Luxembourg "Jeune femme en toilette de bal" pour que l'une des figures de proue du mouvement impressionniste soit représentée.
Berthe Morisot tomba malade à la mi-février 1895. Elle avait, selon les biographies, une congestion pulmonaire, ou une grippe, contractée en soignant sa fille du même mal. Elle mourut le au 10 rue Weber à Paris, et légua la plupart de ses œuvres à ses amis artistes : Degas, Monet, Renoir. Malgré sa riche production artistique, le certificat de décès mentionnait : « sans profession ». Elle est enterrée dans le caveau des Manet au cimetière de Passy où il est simplement gravé : « Berthe Morisot, veuve d'Eugène Manet ».

La mort de l'artiste n'entraîna cependant pas la dispersion du groupe impressionniste ; ses compagnons de lutte aimaient et protégeaient sa fille, dont Mallarmé était le tuteur et que Renoir emmenait peindre avec lui. Degas la maria en 1900 au fils d'Henri Rouart. Pour le premier anniversaire de sa mort, du 5 au 21 (ou 23) mars 1896, Durand-Ruel, aidé de Degas, Rouart et de sa fille Julie organisèrent une rétrospective de ses œuvres d'environ trois cents à quatre cents toiles

Paul Valéry, qui épousa sa nièce, Jeanne Gobillard, écrivit un essai sur Berthe en 1926 et le dédicaça à Édouard Vuillard. Il dira plus tard 

C'est au cours d'une perquisition, au siège de l'Institut Wildenstein, diligentée en marge d'une des multiples affaires de détournement dont les Wildenstein père et fils sont accusés que les inspecteurs de la brigade financière découvrent, les 11 et 12 janvier 2011 la toile de Berthe Morisot intitulée "Chaumière en Normandie", 1865, huile sur toile .

Lors de l'inventaire de la succession, les académiciens Daulte et Wildenstein avaient décroché les tableaux ornant les murs de l'appartement d'Anne-Marie Rouart et les avaient étalés sur le sol pour qu'ils ne soient pas considérés comme "meubles meublants", et ne soient pas rendus à l'héritier légitime, Yves Rouart.

À la suite de cette manœuvre de spoliation, orchestrée par les exécuteurs testamentaires de la succession d'Anne-Marie Rouart, cette toile avait été détournée au détriment de son neveu, Yves Rouart. "Chaumière en Normandie", avait été déclaré "collection privée" sur le catalogue - qui faisait autorité absolue - de Daniel Wildenstein. Parmi les pièces majeures provenant de la succession d'Anne-Marie Rouart, il y a une très belle collection d'œuvres de Berthe Morisot. Les autres œuvres comprenaient des Gauguin, Degas, et des Manet.

Selon le testament de madame Rouart, la plus grande partie de cette énorme collection allait à l'Académie des beaux-arts, et une autre à Yves Rouart, petit-fils de Julie Manet.
Ce dernier n'avait jusque-là jamais pu obtenir que quelques œuvres mineures répertoriées par les exécuteurs testamentaires ; ces derniers, Jean-François Daulte, Daniel Wildenstein et son fils Guy Wildenstein, étant censés "protéger" la collection dans les coffres de l'Institut Wildenstein.

C'est seulement en 2011, que la "Chaumière en Normandie" est enfin réapparue et qu'Yves Rouart a pu lancer une procédure pour l'obtenir. Cette toile avait été inscrite au catalogue Wildenstein sous l'intitulé vague "collection privée" sans mention du nom de sa propriétaire d'origine, ni du lieu d'où elle a été décrochée, ni de celui de son héritier en droit.

Yves Rouart qui avait dans un premier temps assigné l'Académie des beaux-arts et signé en 2000 un protocole d'accord révisable avec les exécuteurs testamentaires, a contesté ce protocole. . 
La collection d'Anne-Marie Rouart comprenait en outre le célèbre portrait de Berthe Morisot par Manet. 
Il devait être vendu pour payer la succession par les exécuteurs testamentaires. L'État français s'est opposé à la vente de cette œuvre à l'étranger et l'a rachetée pour plusieurs millions d'euros. C'est aujourd'hui une des pièces maîtresse du musée d'Orsay.

En 2013, le musée Marmottan-Monet héberge encore environ 80 tableaux de Berthe Morisot.

Cette sélection est issue de celle de l'ouvrage "Berthe Morisot" de Charles F. Stuckey, William P. Scott, et Suzanne G. Lindsay, elle-même issue du catalogue raisonné établi par Marie-Louise Bataille, Denis Rouaart, et Georges Wildenstein en 1961. Il y a des variations entre les dates d'exécution des œuvres, les dates de leur exposition, ou les dates d'achat des œuvre de Berthe Morisot, et des confusions entre les titres notamment les "Ports". 



À ceux-là s'ajoutent le "Portrait de Berthe Morisot" par Adèle d'Affry, 1875, conservé au musée d'art et d'histoire de Fribourg en Suisse. Adèle d'Affry a réalisé plusieurs autres portraits de Berthe Morisot non localisés.






</doc>
<doc id="16715" url="https://fr.wikipedia.org/wiki?curid=16715" title="Produit phytosanitaire">
Produit phytosanitaire

Un produit phytosanitaire (étymologiquement, « phyto » et « sanitaire » : « santé des plantes ») est un produit chimique utilisé pour soigner ou prévenir les maladies des organismes végétaux. Par extension, on utilise ce mot pour désigner des produits utilisés pour contrôler des plantes, insectes et champignons.

Ces produits font partie, avec les biocides, de la famille des pesticides. En Europe et dans la plupart des pays, ils doivent être homologués, et autorisés pour un ou plusieurs usages (qui peuvent varier selon les époques ou les pays).

Il s'agit d'une substance active ou d'une association de plusieurs substances chimiques ou micro-organismes, d'un liant et éventuellement d'un solvant éventuellement accompagnés d'adjuvants ou d'un tensioactif.

L'expression « produits phytosanitaires » est couramment employée dans un sens proche de produit phytopharmaceutique, défini par la réglementation communautaire, ou de produit antiparasite contre les ennemis des cultures défini par la réglementation française, ou encore de pesticide.

Les substances actives sont minérales (ex. : sulfate de cuivre) ou organiques (ex. : carbamates).
Elles sont d'origine naturelle (ex. : Bacillus thuringiensis), ou issues de la chimie de synthèse (ex. : glyphosate). Dans ce cas, il s'agit parfois de la reproduction par l'industrie chimique de molécules naturellement biocides isolées dans la nature (ex. : les pyréthrines de synthèse, inspirées de molécules produites par des plantes de la famille des Chrysanthèmes et ayant des vertus acaricides, antiparasitaires, anthelminthiques et surtout insecticides).

Les produits phytosanitaires sont originellement et au sens propre destinés à protéger de nombreuses espèces végétales cultivées (y compris des arbres), généralement pour en améliorer les rendements :

On parle notamment de :
Le terme « phytosanitaire » exclut les substances nutritives du type engrais ou oligoéléments sauf quand il s'agit de mélanges d'engrais et de produits phytosanitaires.

Il existe des types très différents de produits phytosanitaires, contenant ou non des minéraux, des métaux, des produits chimiques de synthèse et/ou naturels (microorganismes, macroorganismes, phéromones, substances naturelles).

Les phytosanitaires sont généralement spécifiquement formulés pour tuer des organismes entrant en compétition avec les plantes cultivées ou nuisant à leur croissance ou à leur reproduction (mousses, champignons, bactéries, végétaux concurrents, insectes, rongeurs, acariens, mollusques, vers, nématodes, virus, etc.). Ils sont plus ou moins rémanents, écorémanents ou (bio)dégradables ou biodisponibles.

Une catégorie de produits phytosanitaires est réputée moins nocive ou non-nocive pour l'environnement en agissant de manière différente : par compétition, par prédation, par mimétisme, par stimulation des défenses naturelles, etc. Tous peuvent être nocifs pour l'environnement ou la santé en cas de mauvaise utilisation, (notamment en ne respectant pas les conditions d'emploi co-définies par le fabricant et les autorités responsables de l'homologation).

Les produits jugés les plus toxiques et/ou écotoxiques sont étiquetés comme tels, conformément à la réglementation qui évolue avec les connaissances scientifiques. L'ANSES est chargée de l'évaluation de la toxicité de ces produits. Le respect de bonnes pratiques agricoles, vétérinaires, phytosanitaires et de transports de produits dangereux permet dans une certaine mesure de limiter les contacts directs des animaux et nécrophages avec ces produits dont la toxicité peut être intrinsèque et directe, ou indirecte et différée liée à des effets indirects (perturbation endocrinienne par exemple) ou à un « "effet cocktail" » (synergie).

Le transport, la préparation et l'application des produits phytosanitaires présentent des risques pour l'utilisateur et l'environnement si certaines règles et précautions ne sont pas respectées. Il faut :

Les produits phytosanitaires peuvent avoir des conséquences dommageables sur le manipulateur et l'Environnement. Une façon de limiter les risques est :

En 1991, le marché mondial était estimé à 22 milliards de dollars, répartis entre herbicides (44 %), insecticides (29 %, lesquels arrivaient en premier dans les pays chauds) et fongicides (21 %, utilisés principalement en Europe occidentale) . Les trois premiers marchés de phytosanitaires (tous produits confondus: herbicides, insecticides et fongicides) étaient alors les États-Unis (5 678 millions de dollars de ventes en 1991), le Japon (2 602 millions de dollars de ventes en 1991) puis la France (2 182 millions de dollars de ventes en 1991) . À cette époque, onze pays représentaient 80 % du marché mondial: USA, Japon, France, URSS, Brésil, Angleterre, Italie, Canada, Allemagne, Inde et Espagne . L'Europe (de l'Ouest et de l'Est) représentait alors plus d'un tiers du marché mondial.

En 2013, le chiffre d'affaires mondial était estimé à 26,7 milliards de dollars . Les quatre premiers pays consommateurs étaient alors les États-Unis, le Brésil, le Japon et la France (le Brésil passant ainsi du mondial en 1991 au en 2013) . De façon générale, on remarque une importance accrue des BRIC : quatre pays, à savoir le Brésil, la Chine, l'Inde et l'Argentine représentaient, en 2015, près d'un tiers du marché mondial .

Après avoir atteint en 1999 puis en 2001, la France consommait environ (chiffres de 2004 ) de produits phytosanitaires, ce qui la plaçait au troisième rang des utilisateurs mondiaux derrière les États-Unis en quantité absolue. Elle était, et demeure, la première consommatrice de phytosanitaires en Europe. En 2010, avec un chiffre d'affaires de 1,8 milliard d'euros, la France arrivait ainsi devant l'Allemagne (1,25 milliard), l'Italie (807 millions), la Grande-Bretagne (589 millions) et la Pologne (455 millions), selon les statistiques de l'Union des industries de la protection des plantes (UIPP) .

Toutefois, une fois ramené à l'hectare cultivé (/ha/an) hors surface en herbe, la France n'arriverait plus qu'au quatrième rang européen .

En 1991, dix firmes assuraient les 3/4 du marché mondial: dans l'ordre, CIBA-GEIGY, ICI (Imperial Chemical Industries), Rhône-Poulenc, Bayer, Du Pont, Dow Elanco, Monsanto, Hoechst, BASF et Schering . Depuis, une forte concentration a eu lieu, alors même que début 2016, Monsanto envisageait de racheter soit Bayer, soit BASF, tandis que Du Pont envisageait de racheter Dow et que ChemChina annonçait, en février 2016, le rachat de Syngenta pour 43 milliards de dollars (Syngenta étant issu d'une fusion entre AstraGeneca, Geneca ayant été une filière d'ICI, et Novartis, créée en 1996 par fusion entre CIBA-GEIGY et Sandoz).

Les nouvelles molécules actives on deux origines possibles : soit directement par les laboratoires des firmes agro-pharmaceutiques, soit par des laboratoires prestataires. Le produit phytosanitaire est généralement mis au point par les firmes agro-pharmaceutiques. Dans la majorité des pays, sa mise en vente et son utilisation sont soumises à une autorisation préalable (l'homologation ou autorisation de mise sur le marché) de l'autorité nationale compétente (en France l'ANSES).

Pour être homologué, un produit doit suivre un parcours visant à démontrer son innocuité (absence de toxicité) pour :

Afin de déposer un dossier pour une demande d'homologation d'une spécialité commerciale, un dossier est établi selon les normes européennes (format "draft Registration Report" ou dRR ) avec les parties suivantes :

L'obtention de l'ensemble des données scientifiques nécessaires à l'évaluation des préparations sont sujettes aux règles de l'OEPP, dont le respect conditionne la validité. En vertu des nouvelles règles de l'Union européenne, il faut 2,5 à 3,5 ans à partir de la date de recevabilité de la requête à la publication d'un règlement approuvant une nouvelle substance active. Le produit est soit autorisé, soit reporté, soit refusé. Le produit autorisé l'est pour un ou plusieurs usages précis, définis par une plante cible (par exemple une culture de blé), un parasite cible (le puceron) et un type de traitement (des parties aériennes par exemple).

Néanmoins, il est courant de voir des produits phytosanitaires ou des produits de vinifications sur le marché avant d'avoir fini la réalisation des tests sur sa toxicité (ex. : algue utilisée pour la clarification des vins, avec écrit sur le paquet « peut être cancérigène en attente de recherche »).

Les produits apparaissant sur le marché au début du sont réputés moins persistants que leurs prédécesseurs, mais ils sont souvent bien plus actifs à des doses parfois bien plus faibles ( de matière active/m² pour une sulfonylurée utilisée pour le désherbage du blé). Il en va de même pour de nombreux biocides.

En Europe, la qualité des eaux destinées à la consommation humaine autorise une concentration maximale de produits phytosanitaires (insecticides, fongicides, herbicides) de 0,1 microgramme/L par substance ou de 0,5 microgramme/L pour toutes substances (directive européenne 80/778/CEE, décret du ).

En France, une loi qui vise à mieux encadrer l’utilisation des produits phytosanitaires sur le territoire national a été voté le 23 janvier 2014 (loi Labbé).

Un nombre croissant de plantes, d'insectes et de pathogènes fongiques développent des résistances à un ou plusieurs pesticides.

Dans le cas d’une mutation de cible, une anomalie naturelle a modifié le code génétique de l’enzyme "cible" de l’herbicide. Ce dernier n’agit donc plus sur la plante. Tous les herbicides qui ont le même mode d’action sont alors concernés. Cela conduit à devoir utiliser des doses extrêmes sur les plantes. Pour ce type de résistance, le taux de résistance est très élevé (exprimé par le rapport R/S) et peut atteindre 1000. C’est-à-dire que la dose d’un herbicide qui détruit une plante résistante est 1000 fois supérieure à la dose qui détruit une plante sensible.

Dans ce type de mécanisme, la matière active peut agir sur sa cible (une enzyme) mais elle y accède difficilement soit elle est très mal transportée vers la cible (cellule moins perméable), soit dégradée par des enzymes de la plante. Cette dernière est encore sensible à l’herbicide mais à des doses très élevées par rapport à la normale, pouvant aller de cinq à cent fois la dose habituelle ; avec des doses classiques, l’adventice peut être ralentie.

Il existe des mécanismes encore inconnus à ce jour. Certaines plantes se sont révélées résistantes sans que l’on puisse expliquer par quel mécanisme elles le soient devenues. Par ailleurs, il a été prouvé que certaines plantes présentent plusieurs mécanismes de résistance sur un même individu.



</doc>
<doc id="16716" url="https://fr.wikipedia.org/wiki?curid=16716" title="Pesticide">
Pesticide

Un pesticide est une substance chimique utilisée pour lutter contre des organismes considérés comme nuisibles. C'est un terme générique qui rassemble les insecticides, les fongicides, les herbicides, les parasiticides. Ils s'attaquent respectivement aux insectes ravageurs, aux champignons, aux « mauvaises herbes » et aux vers parasites.

Le terme pesticide comprend non seulement les substances « phytosanitaires » ou « phytopharmaceutiques » utilisées en agriculture, sylviculture et horticulture, mais aussi les produits zoosanitaires, les produits de traitements conservateurs des bois, et de nombreux pesticides à usage domestique : shampoing antipoux, boules antimites, poudres anti-fourmis, bombes insecticides contre les mouches, mites ou moustiques, colliers antipuces, diffuseurs intérieurs de pesticides, etc.

Dans une acception plus large, comme celle de la règlementation européenne, ce peut être des régulateurs de croissance, ou des substances qui répondent à des problèmes d'hygiène publique (par exemple les cafards dans les habitations), de santé publique (les insectes parasites poux, puces ou vecteurs de maladies telles que le paludisme et les bactéries pathogènes de l'eau détruites par la chloration), de santé vétérinaire, ou concernant les surfaces non agricoles (routes, aéroports, voies ferrées, réseaux électriques, etc.).

Selon l'InVS, d'après les analyses faites en 2006-2007 chez dans le cadre du programme national nutrition santé (PNNS), le sang d'un Français moyen contient presque toujours des pesticides organophosphorés et trois fois plus de certains pesticides (pyréthrinoïdes, paradichlorobenzène) que celui des Américains ou des Allemands, alors que leur taux sanguin de métaux lourds et de pesticides organochlorés est comparable aux concentrations observées à l’étranger.

Les pesticides sont également associés aux perturbateurs endocriniens et à des maladies et à l'infertilité.

Le mot « pesticide » vient de l'anglais, sur le modèle de nombreux mots se terminant par le suffixe "-cide" (latin "-cida", du verbe latin "caedo, caedes, caedere, caedi, caedum" : « tuer »), et sur la base du mot anglais "pest" (animal, insecte ou plante nuisible), lequel provient du latin "pestis" qui signifie « maladie contagieuse, épidémie, peste » (comme le français "peste" qui a cependant conservé l'acception du latin, les termes anglais et français sont donc des faux-amis).

La lutte chimique existe depuis des millénaires : l'usage du soufre remonte à la Grèce antique ( ) et l'arsenic est recommandé par Pline l'Ancien, naturaliste romain, en tant qu'insecticide. Des plantes connues pour leurs propriétés toxiques ont été utilisées comme pesticides (par exemple les aconits, au Moyen Âge, contre les rongeurs). Des traités sur ces plantes ont été rédigés ( traité des poisons de Maïmonide en 1135).
Les produits arsenicaux ou à base de plomb (Arséniate de plomb) étaient utilisés au en Chine et en Europe.

Les propriétés insecticides du tabac étaient connus dès 1690. En Inde, les jardiniers utilisaient les racines de "Derris" et "Lonchocarpus" (roténone) comme insecticide. Leur usage s'est répandu en Europe vers 1900.

En 1807 Isaac-Bénédict Prévost promeut l'usage du sulfate de cuivre dans le traitement de la carie du blé. Guère suivies en France, ses préconisations sont adoptées rapidement en Suisse, en Grande-Bretagne et aux Pays-Bas.

La chimie minérale s'est développée au , fournissant de nombreux pesticides minéraux à base de sels de cuivre (encore bien utile en agriculture biologique). Les fongicides à base de sulfate de cuivre se répandent, en particulier la fameuse bouillie bordelaise (mélange de sulfate de cuivre et de chaux) pour lutter contre les invasions fongiques de la vigne et de la pomme de terre, non sans séquelles de pollution sur les sols (cuivre non dégradable).

Des sels de mercure sont employés à partir du début du pour le traitement des semences. En raison de la toxicité du mercure, ils sont interdits dans les pays de l'OCDE depuis 1991 et dès 1982 pour certains pays d'Europe de l'Ouest. Leur usage perdure dans d'autres pays. 
L'ère des pesticides de synthèse débute vraiment dans les années 1930, profitant du développement de la chimie organique de synthèse et de la recherche sur les armes chimiques durant la Première Guerre mondiale.

En 1874, Zeidler synthétise le DDT (dichlorodiphényltrichloroéthane), dont Muller en 1939 établit les propriétés insecticides. Le DDT est commercialisé dès 1943 et ouvre la voie à la famille des organochlorés. Le DDT a dominé le marché des insecticides jusqu'au début des années 1970 où il a été interdit en UE, notamment.

En 1944, l'herbicide 2,4-D, copié sur une hormone de croissance des plantes et encore fortement employé de nos jours, est synthétisé.
La Seconde Guerre mondiale a généré, à travers les recherches engagées pour la mise au point de gaz de combat, la famille des organophosphorés qui, depuis 1945, a vu un développement considérable encore de mise aujourd'hui pour certains de ces produits, tel le malathion.

En 1950-1955 se développe aux États-Unis les herbicides de la famille des urées substituées (linuron, diuron), suivis peu après par les herbicides du groupe ammonium quaternaire et triazines.

Les fongicides du type benzimidazole et pyrimides datent de 1966, suivi par les fongicides imidazoliques et triazoliques dits fongicides IBS (inhibiteurs de la synthèse des stérols) qui représentent actuellement le plus gros marché des fongicides.

Dans les années 1970-80 apparaît une nouvelle classe d'insecticides, les pyréthrinoïdes qui dominent pour leur part le marché des insecticides.

Auparavant, la recherche de matières actives se faisait au hasard en soumettant de nombreux produits à des tests biologiques. Lorsqu'un produit était retenu pour ses qualités biocides, on cherchait à en améliorer l'efficacité à travers la synthèse d'analogues. Cette procédure a permis de développer les techniques de synthèse qui sont de mise aujourd'hui.

Désormais, l'accent est mis sur la compréhension des modes d'action et la recherche de cibles nouvelles. Connaissant les cibles, on peut alors établir des relations structure-activité pour aboutir à l'obtention de matières actives. 

Actuellement, on assiste à une consolidation du marché au niveau des familles les plus récemment découvertes avec la recherche de nouvelles propriétés. Dans le même temps, de nouvelles cibles physiologiques de l'animal ou du végétal sont explorées dans le but de développer des produits à modes d'action originaux, des produits issus de la biotechnologie ou des médiateurs chimiques.

Les pesticides font partie des biocides.

Ils incluent les produits dits "phytosanitaires" ou "phytopharmaceutiques" (qui étymologiquement « soignent » les plantes : ce sont comme des médicaments pour les plantes en culture). En France, le ministère de l'Agriculture et de la Pêche et le ministère de l'Environnement (de l’Écologie, de l’Énergie, du Développement Durable et de l'Aménagement du Territoire) ont conjointement produit un document visant à mieux différentier les phytosanitaires au sein des biocides.

Ils incluent aussi des produits qui soignent les animaux ou l'homme (antiparasitaires externes ou internes par exemple). Ils peuvent désigner des molécules actives seules, ou des formulations associant plusieurs molécules ou des molécules actives et additifs (surfactants par exemple).

Chaque groupe chimique produit des métabolites au sein des organismes vivants ou des résidus en se dégradant spontanément. Ces résidus ou métabolites sont plus ou moins dégradables et susceptibles d'être retrouvé comme polluants de l'environnement ou contaminants de la nourriture ou de la boisson.

Les pesticides peuvent être regroupés selon différents axes : par type d'usage, par origine, par type d'activité, par groupe chimique, par mode d'action, etc.

On distingue les pesticides organiques (contenant du carbone) et inorganiques (sans carbone autre que sous forme autre que carbonate ou cyanure). Parmi les pesticides organiques, une distinction est faite entre pesticides de synthèse (développés en laboratoire et produits en usine), pesticides naturels (d'origine animale, végétale ou microbienne) et micro-organismes. Les pesticides inorganiques sont des dérivés d'éléments minéraux (exemple : sulfate de cuivre).

Depuis 1975, l'Organisation mondiale de la santé propose une classification des pesticides par niveaux de risques.

Ce regroupement s'intéresse à la cible que le pesticide est destiné à combattre. On recense ainsi :

Les catégories de produits suivants, sont plus spécifiquement et commercialement désignés comme « produits phytosanitaires », sont utilisées pour soigner ou prévenir les maladies des végétaux. Ce ne sont donc pas tous des pesticides au sens strict (régulateurs hormonaux de croissance par exemple) :

Autres produits :

Depuis le juillet 2010 la FAO a ouvert gratuitement à tous (sur simple enregistrement) sa base de données Pesticides dans l'outil FAOSTAT (la plus vaste base de données mondiale sur l’alimentation, l’agriculture et la faim).

On distingue souvent les usages agricoles (ex. : 54 % des ventes en 2000 en Wallonie, qui en consomme moins que la Flandre) et non agricoles (ex. : environ 33 % des ventes en 2000 en Wallonie) qui comprennent les produits utilisés par les jardiniers, les collectivités et les gestionnaires de routes, chemins de fer, canaux, zones d'activité, aéroports Dans le cas de la Wallonie, en 2000, 13 % des ventes en 2000 restaient , et pour certains produits, les enquêtes de terrain montrent que .

Les tonnages tendent à diminuer, mais en partie parce que certains pesticides modernes sont beaucoup plus actifs à moindres doses.

Les quantités de pesticides utilisées dans le monde augmentent régulièrement depuis soixante ans. Elles semblent diminuer dans certains pays d'Europe, mais à dose ou poids égal, les matières actives d'aujourd'hui sont généralement beaucoup plus efficaces que celles des décennies précédentes.

Les molécules commercialisées évoluent, pour contourner les résistances (des insectes, champignons ou végétaux), pour remplacer des produits interdits en raison de leur toxicité, ou quand des molécules "a priori" intéressantes viennent en remplacer d'autres.

Les pesticides les plus utilisés (en termes de quantité) sont les désherbants. La molécule active la plus vendue comme désherbant et la plus utilisée dans le monde est le glyphosate.

, quatre fois plus que la moyenne européenne, elle-même supérieure à celle des États-Unis.

La France est, en 2008, le quatrième consommateur mondial de pesticides, loin derrière les États-Unis, et derrière le Japon et le Brésil.

Parmi les pays européens, la France se classe au quatrième rang, derrière notamment les Pays-Bas et d'autres pays chez lesquels les systèmes de production sont d’abord orientés vers l’horticulture et le maraîchage.

La France est située à la troisième place sur le plan international pour l'utilisation de pesticides en 2013. En 2013, le recours aux pesticides a augmenté de 9,2 %. Le coût des pollutions agricoles (engrais azotés et pesticides) sont de 1 milliard à 1,5 milliard d'euros par an au minimum pour les ménages (eau du robinet et bouteille). Le traitement complet (eutrophysation, algues vertes) est évalué entre 54 milliards et 91 milliards d'euros par an.

Les producteurs ne souhaitent pas diffuser de données de vente régionalisées, mais en croisant les données du RICA et du Recensement agricole 2000, une première cartographie de l'emploi de pesticides a pu être faite sur la base du calcul des dépenses rapportées à la surface agricole, par petite région agricole, confirmant que les sols dédiés aux grandes cultures en consomment le plus, avec la vigne et certaines formes d'arboriculture ou de maraichage.

Selon le RICA, en 2006, ce sont qui ont été dépensés en achat de pesticides ( et , pour un total de vente de produit de en France métropolitaine, le différentiel pouvant être expliqué par le jardinage et l'agriculture des DOM. Cette somme est égale 5 % environ du montant du produit brut des exploitations (hors subvention). Selon l'INRA, pour 25,4 millions d'hectares de SAU des exploitations du RICA, 14,4 millions consomment 96 % des pesticides, 11,7 millions d’hectares correspondant à la jachère ou aux surfaces toujours en herbe. Par ailleurs, 11 % des sols cultivés (soit 1,5 million d’hectares) produisent des fourrages qui ne contribuent que pour 4 % aux dépenses phytosanitaires globales.

Le Grenelle de l'environnement (2007) visait une réduction de 50 % des quantités de matière active utilisées, si possible avant 2018. Une réduction de 30 % des pesticides serait possible en France, avec des changements de pratiques importants, mais sans bouleverser les systèmes de production, selon une étude d'Écophyto 2018, commandée par les ministres chargés de l'agriculture et de l'environnement à une équipe coordonnée par l'INRA, à la suite du Grenelle de l'environnement. D'autres études (projet Endure) estiment qu'avec des technologies novatrices on pourrait réduire, pour le maïs, de 100 % les produits de traitement des semences, jusqu'à 85 % les épandages d'insecticides et de 90 % ceux d'herbicides.
Néanmoins, selon l'UIPP (Union des industries de la protection des plantes), avec de matière active vendues dans l'année, le marché a chuté de 19 % en volume, en 2009. Les fabricants invoquent les hausses de prix, une moindre pression parasitaire, de bonnes conditions climatiques (dont un printemps froid) ou la chute des revenus des agriculteurs exploitant de grandes cultures.

La loi Grenelle II prévoit que "". Ce rapport fera un point annuel sur la diffusion des alternatives aux pesticides auprès des agriculteurs, sur la recherche appliquée et la formation, mais aussi sur ".
Des éléments d'évaluation des impacts économiques d'une réduction en France, vue par l'INRA, a été publiée en mai 2011.

Dans un entretien auprès du journal Libération daté du 30 janvier 2015, Stéphane Le Foll, Ministre de l'Agriculture, a expliqué son plan de lutte contre les pesticides. Stéphane Le Foll explique une utilisation massive de pesticides en France par l'importance des surfaces agricoles, viticoles et arboricoles en France. Selon ses affirmations, nous serions en légère baisse par rapport à la moyenne européenne. En effet, en France, nous atteignons /ha alors que la moyenne en Europe est de /ha en 2011-2012. Le ministre explique l 'échec du premier plan écophyto par son objectif trop ambitieux et le manque de moyens pour changer le modèle de production. 

L'objectif du nouveau plan est de fixer un objectif de diminution de moitié à l'horizon 2025 avec un palier intermédiaire de 25 % en 2020. Il souhaite aussi valoriser les expériences au sein des 2000 fermes pionnières écophyto qui ont réussi à baisser de 12 % en moyenne en 2013 l'utilisation de pesticides car elles ont pratiqué la rotation des cultures, la diversification variétale ou le recours au biocontrôle (la lutte biologique). Son objectif est d'atteindre le nombre de 3000 fermes de ce type, chacune entraînant dix exploitations autour d'elles. 

Selon un sondage du 30 janvier 2015, seulement 45 % des agriculteurs se considèrent engagés dans l'agroécologie mais 13 % seraient prêts à le faire. Les plus jeunes agriculteurs, les moins de 35 ans, témoignent dans le sondage de leur intérêt pour améliorer leurs pratiques. 31 % d'entre eux envisagent de s'engager dans l'agroécologie. 

Stéphane Le Foll souhaite trouver une voie intermédiaire entre une agriculture écologique et productiviste qui réconciliera économie, écologie et social. Il prône le passage d'une agriculture intensive en intrants, en chimie, en azote, en énergie fossile à une agriculture intensive en connaissance et innovation. Un logiciel doit être mis à la disposition des agriculteurs pour une estimation de leur engagement dans l'agroécologie avec la possibilité d'un système de certification à terme pour éviter tout risque de dévoiement.

Son plan prévoit la mise en place de CEPP (Certificat d'économie de produits phytosanitaires) avec une obligation pour les distributeurs de ces produits de baisser de 20 % le nombre de doses utilisées sur cinq ans. Les progrès seront mesurés en suivant l'évolution du NODU, indicateur de référence utilisé pour évaluer le nombre de doses de pesticides en agriculture. Lorsqu'un distributeur n'atteint pas l'objectif de 20 % en 5 ans, il est sanctionné par une pénalité de par NODU non économisé soit l'équivalent de sa marge nette. Quand un distributeur a réussi à dépasser l'objectif de 20 %, il aura la possibilité de vendre ses NODU. Des révisions au terme de deux années sont prévues pour déceler « les passagers clandestins », en clair ceux qui sont dans le statu quo. 

Cette démarche s'inscrit dans une volonté de modifier le métier des distributeurs non pas comme des vendeurs de produits mais des vendeurs de services c'est-à-dire apprendre aux agriculteurs à employer la quantité pertinente ou des techniques alternatives. Ce système de sanction/rétribution diffère du marché carbone où le prix est fixé par le marché car dans ce plan, c'est le gouvernement qui définit les prix du mécanisme. 

Des critiques émanent des possibles conflits d'intérêt car il est prescrit de diminuer l'utilisation des pesticides sans séparer la vente des produits phytosanitaires et le conseil. Le Ministre balaie cette critique en affirmant que « si on sépare la vente de phytos et le conseil, on perd un potentiel d'action de personnes (ndlr : salariés des chambres d'agriculture et salariés de coopératives). Avec notre nouveau système favorisant le service et les produits de biocontrôle, on neutralise le conflit d'intérêts. »

En novembre 2014, François Hollande avait promis de mener plus loin la lutte contre l'utilisation de pesticides néonicotinoïdes qui tuent les abeilles et autres pollinisateurs. En effet, 35 % de la production mondiale de nourriture est liée aux pollinisateurs qui sont décimés par les pesticides avec notamment les néonicotinoïdes qui agissent sur le système nerveux. Le gouvernement a demandé à l'Efsa (Autorité européenne de sécurité des aliments) de mener une étude sur les effets sur la faune pollinisatrice dont les résultats seront révélés en 2015. Le gouvernement ne souhaite pas une interruption brutale de ces produits car ces derniers remplacent déjà des produits autrefois plus nocifs et aucune alternative n'est actuellement disponible. Un délai de cinq ans est nécessaire selon le ministre de l'Agriculture. En attendant, la solution gouvernementale est de reporter l'utilisation des pesticides néonicotinoïdes le soir lorsque les abeilles ne butinent plus. 

L'association Générations Futures rappelle qu’une baisse significative de l’usage des pesticides en agriculture ne pourra pas être atteinte uniquement avec l’optimisation technique des équipements ou le recours aux produits de bio-contrôle. Il s’agit bien d’inciter les producteurs à mettre en place des systèmes de production du type « production intégrée », qui produisent déjà des résultats remarquables dans plusieurs réseaux en France[2], afin d’atteindre l’objectif de −50 % du NODU qui reste l’objectif à terme du plan. Il faudrait également développer l’agriculture biologique qui devrait représenter à terme 20 % de la Surface agricole.

L'agent orange (produit à la demande du gouvernement américain par les principales industries chimiques du pays dont les multinationale Monsanto, Dow Chemical…) est le surnom donné au plus utilisé des herbicides employés pour l'armée des États-Unis lors de la guerre du Viêt Nam, en particulier entre 1961 et 1971. Initialement, les effets pathogènes sur l'être humain étaient inconnus. Ce produit était utilisé exclusivement dans le but de dégager les abords des installations militaires et d'assurer une déforestation afin d'empêcher les combattants ennemis de se dissimuler.


Il existait en 2009 de par le monde près de autorisées à la vente, composées à partir de 900 matières actives différentes. 15 à 20 nouvelles matières actives s'y rajoutent tous les ans, qui remplacent souvent des produits interdits ou devenus inefficaces.

Au moins 8 à 10 ans sont nécessaires entre la découverte d'une matière active et sa mise sur le marché : cette durée inclut les tests d'efficacité et les études règlementaires de toxicité pour l'environnement et pour l'humain.

Les propriétés d'un pesticide découlent pour l'essentiel de la structure de sa matière active. Celle-ci présente 3 parties (ce découpage est artificiel, aucune partie ne pouvant être littéralement séparée) : 

Cette notion de solubilité est importante car c'est l'affinité d'un pesticide pour l'eau ou les corps gras qui va conditionner sa pénétration dans l'organisme ciblé.

Les propriétés physico-chimiques du pesticide

Les pesticides regroupent une grande diversité de structures chimiques et chaque molécule constitue une entité qui se caractérise par un ensemble de propriétés spécifiques (taille moléculaire, encombrement stérique, basicité ou acidité, constante de dissociation, coefficient de partage octanol-eau, solubilité dans l'eau, tension de vapeur) qui vont conditionner sa réactivité à l'égard des constituants du sol. Quand un pesticide est adsorbé par le sol il en résulte une difficulté à préjuger de la rétention d'une molécule par le sol, même à l'intérieur d'une famille chimique donnée.

La caractéristique principale à considérer dans un processus de rétention pour les composés acides ou basiques est la constante de dissociation (pK). Le caractère hydrophobe d'un pesticide augmente lorsque sa solubilité dans l'eau diminue et il en résulte une rétention plus intense par la matière organique du sol. La polarité dépend de la répartition des électrons au sein de la structure moléculaire et influence également le degré de solvatation de la molécule en solution et donc l'énergie totale impliquée dans son adsorption. Les caractéristiques d'une molécule pesticide ne doivent pas être considérées de manière individuelle, car elles interagissent simultanément. Ainsi, la nature des atomes constitutifs et les groupements fonctionnels déterminent la structure électronique,la polarité mais également la valeur de la constante d'ionisation, la capacité d'une molécule à former des liaisons inter- et intramoléculaires et sa solubilité.

La "« formulation »" d'un pesticide vise à présenter la matière active sous une forme stable et permettant son application en lui ajoutant des substances destinées à améliorer et faciliter son action. Ce sont les adjuvants. Ils comprennent des tensio-actifs, des adhésifs, des émulsionnants, des stabilisants, des photoprotecteurs, des antitranspirants, des colorants, des substances répulsives, des émétiques (vomitifs) et parfois des antidotes.


Un code international de 2 lettres majuscules, placées à la suite du nom commercial indique le type de formulation. Les principaux types de formulation sont les suivants :



Les fabricants estiment que les pesticides améliorent la qualité des produits, notamment en réduisant le risque de développement de certaines bactéries ou champignons produisant des toxines.

Les détracteurs des pesticides ou de leur utilisation systématique arguent que :

Les relations entre pesticides (environ 900 molécules actives sur le marché en 2012) et environnement sont à double sens : les pesticides modifient l'environnement parce que écotoxiques, en mettant en œuvre une centaine de mécanismes écotoxiques, et inversement l'environnement (Cf. Oxygène, ozone, humidité, pH, métaux, métalloïdes, bactéries, champignons, etc.), modifient les pesticides, leurs impuretés (dioxines dans l'agent orange par exemple) et leurs métabolites. Pour de nombreux produits anciennement mis sur le marché la photo-altération des produits, de leur impuretés, molécules de dégradation ou métabolites dans l'air et ses effets environnementaux ont été peu étudiés.

Les impuretés ; indésirables mais présentes et presque économiquement inévitables dans certains processus de fabrication sont parfois la première cause de toxicité et écotoxicité d'un produit. Par exemple les effets adverses écologiques de l'hexachlorocyclohexane sur les mammifères sont probablement essentiellement dus aux 5 à 14 % d'isomèreβ qui est bioaccumulable à long terme dans les graisses.

Un des problèmes les plus graves a peut-être été le 2,3,7,8-tetrachlorodibenzo-p-dioxin (TCDD), une impureté de l'herbicide 2,4,5-T (acide 2,4,5-trichlorophénoxyacétique) maintenant interdit) dont la DL50 orale (dose létale médiane) était comprise entre 0.6 et 2.1 μg/kg chez le Cochon d'inde, de 1,100à 5,000 μg/kg chez le hamster. L'administration intrapéritonéale de [3H]TCDD à des souris a montré un métabolisme faible, voire nul, une persistance exceptionnellement longue, et une forte localisation dans le réticulum endoplasmique hépatique (Vinopal and Casida 1973). Bien que ces exemples puissent être extrêmes, ils mettent en évidence le besoin pour des produits pesticides de haute pureté.

On en retrouve dans les brumes et pluies, dans les eaux superficielles, dans les eaux de nappe et en mer (dont antifoolings) et pour certaines molécules et dans certaines régions dans l'eau du robinet. Selon leur tension de vapeur, les molécules pesticides ou leur métabolites sont plus ou moins solubles dans la vapeur d'eau ou l'eau liquide. 

Les pesticides comptent par les polluants préoccupants de l'eau notamment quand ils sont rémanents ou largement utilisés dans les régions ou pays très agricoles. En France la pollution par les eaux usées domestiques et industrielles a fortement régressée, mais en dépit des plans nitrates successifs et du plan Ecophyto, le suivi de 602 pesticides différents (594 en métropole et 231 dans les DOM) montre que la plupart des rivières sont concernées par une pollution chronique par des pesticides. En 2010-2011, des pesticides étaient retrouvés sur (pour 56 % des points des départements d’outre-mer, hors Guyane). '. En 2012 ' En métropole en 2012. Sans surprise, ce sont les zones les plus agricoles qui sont les plus concernées (zones céréalières, de maïsiculture et de viticulture) notamment dans le grand Bassin parisien, en Pays de la Loire, dans le Sud-Ouest mais aussi le long du Rhône. En Martinique, le chlordécone interdit depuis 20 ans est encore très présent dans les sols et cours d'eau en 2012. En métropole, 40 points présentent plus de 5 µg/l (pour les pesticides recherchés, c'est-à-dire compte-non tenu de certains produits non recherchés ou de métabolites écotoxiques), tous en zones de grande culture (nord de la France, Bassin parisien et Sud-Ouest). 

Les bandes enherbées font partie des mesures visant à limiter le transfert de pesticides des champs vers les cours d'eau.

Lors d'un traitement, plus de des quantités utilisées de pesticides n'atteignent pas le ravageur visé. L'essentiel des produits phytosanitaires aboutissent dans les sols (directement ou via la pluie après évaporation dans l'air). Dans le sols, ils subissent des phénomènes de percolation et de dispersion. Les risques pour l'environnement sont d'autant plus grands que ces produits sont toxiques, utilisés sur des surfaces et à des doses/fréquences élevées et qu'ils sont persistants et mobiles dans les sols.

Le sol comporte des éléments minéraux et organiques mais aussi des organismes vivants. ces derniers participent également aux transferts, d'immobilisation, modification (biodégradation, métabolisation), bioturbation et dégradation.

Les phénomènes de transfert

Les transferts dans le sol sont les plus importants. Ils y sont entrainés par l'eau de pluie et s'y déplacent selon la circulation de l'eau. Ces déplacements varient beaucoup selon le régime hydrique, la perméabilité des sols, la nature du produit. Par exemple, en sol limoneux, l'aldicarbe est une substance très mobile tandis que le lindane ne migre pas (la limite d'utilisation de l'aldicarbe a été fixée au 31 décembre 2007, et est interdite d'utilisation passé cette date, de même que le lindane dont l'usage est interdit depuis le 20 juin 2002).

Les phénomènes d'immobilisation

Certains pesticides sont en majorité adsorbés rapidement par les matières humiques du sol (colloïdes minéraux et organiques).

Une molécule adsorbée n'est plus en solution dans la phase liquide ou gazeuse. N'étant plus disponible, ses effets biologiques sont supprimés ; elle n'est plus dégradée par les micro-organismes du sol ce qui augmente sa persistance. Elle n'est plus entraînée par l'eau, ce qui empêche la pollution de cette dernière. Sa désorption lui rend toutes ses capacités biotoxiques.

Ces molécules sont plus fortement retenues en général dans les sols argileux ou riche en matières organiques.

Les phénomènes de dégradation

La dégradation est assurée principalement par les organismes biologiques de la microflore du sol (bactéries, actinomycètes, champignons, algues, levures), celle-ci pouvant atteindre une tonne de matière sèche à l'hectare. Son action s'exerce surtout dans les premiers centimètres du sol.

Il existe également des processus physiques ou chimiques de dégradation, tel que la photodécomposition. Ces actions contribuent à diminuer la quantité de matière active dans le sol et donc à réduire les risques de pollution.

La cinétique de dégradation d'une molécule donnée est déterminée en estimant la persistance du produit. Pour cela, on détermine sa demi-vie qui est la durée à l'issue de laquelle sa concentration initiale dans le sol a été réduite de moitié.
Cette demi-vie peut varier avec la température, le type de sol, l'ensoleillement, etc : ainsi, celle du DDT est d'environ 30 mois en région tempérée et de 3 à 9 mois sous climat tropical.

Le lindane, le DDT et l'endrine se dégradent en quelques semaines dans les sols inondés des rizières, au contraire de l'aldrine, de la dieldrine et du chlordane.

Les sols se comportent, selon les cas, comme un lieu de stockage provisoire ou un filtre "passif" ou "actif", selon leur nature plus ou moins « fixatrice » (adsorbante) et selon qu'ils permettent ou non la dégradation ou biodégradation de certains produits phytosanitaires.

Ce « filtre » est plus ou moins sélectif, car les molécules de pesticides ou leurs résidus sont plus ou moins capables de se fixer sur le sol ou d'être métabolisés par la vie du sol (bactéries, champignons…).

Les particules fortement adsorbées sur les particules de sol peuvent redevenir contaminantes via les envols de poussière et d'aérosols ou dans l'eau turbide après érosion hydrique. Un cas particulier, très complexe, est le sol inondé en permanence (en saison de culture) des rizières


Sous leurs formes organoarséniques - actuellement les plus utilisées - les composés de l’arsenic sont réputés peu toxiques pour l’homme ou les animaux à sang chaud, mais leur décomposition dans l'environnement ou parfois dans l’organisme peut donner des sous-produits arsenicaux inorganiques hautement toxiques, et éventuellement susceptibles de bioaccumulation dans la couche racinaire ou de bioconcentration (y compris dans les arbres, via leurs racines par exemple).

De nombreux effets de pesticides sur les animaux ont été observés. Ils sont complexes, immédiats ou différés dans l'espace et dans le temps, et varient selon de nombreux facteurs, dont en particulier : 

Les pesticides peuvent être responsables de pollutions diffuses et chroniques et/ou aigües et accidentelles, lors de leur fabrication, transport, utilisation ou lors de l'élimination de produits en fin de vie, dégradés, inutilisé ou interdits. Leurs résidus diffusés dans l’environnement par les eaux de nettoyage, les fumures de type lisier/fumiers et les cadavres d'animaux empoisonnés peuvent aussi induire l'apparition de souches de plantes résistantes aux herbicides, d'insectes résistant à des insecticides et de microbes antibiorésistants dans la nature.

En termes de risque d'exposition au produit, certaines espèces de la faune sauvage présentent des vulnérabilités particulières (Exemples : animaux se nourrissant dans les champs au moment des pulvérisations, coprophages (tués par des antiparasitaires rémanents) et nécrophages tels que vautours ou sangliers par exemple qui se nourrissent de cadavres éventuellement volontairement ou accidentellement empoisonnés par des pesticides). Si elles ne sont pas tuées par ces « empoisonnements secondaires », ces espèces peuvent diffuser le contaminant (bioturbation) et parfois le bioconcentrer dans le réseau trophique.

Les pesticides, leurs produits de dégradation et leurs métabolites (parfois plus toxiques que la molécule mère) peuvent contaminer tous les compartiments de l'Environnement. Des contrôles réguliers des milieux de vie sont réalisés par des organismes indépendants et spécialisés :

On les trouve sous forme de "« résidus »" (molécule mère, produits et sous-produits de dégradation ou métabolites) dans nos aliments et boissons. 
Des lois ou directives de l'Union européenne imposent des seuils à ne pas dépasser, y compris dans l'eau potable.

Dans les aliments, ces limites sont les LMR (Limite maximale règlementaire, en mg de résidu par kg d'aliment), bien inférieures aux Doses Journalières Admissibles, elles-mêmes au moins 100 fois plus faibles que les Doses Sans Effet observées lors des études de toxicité.

Un rapport (2011) du CGDD a calculé que les coûts externes de gestion "" (calcul fait sur la base des concentrations moyennes du SEQ pour les eaux de surface). Le coût de traitement de ces apports annuels de pesticides aux eaux de surface et côtières se situerait dans une fourchette de 4,4 à . Au total, le coût annuel du traitement de ces flux annuels d’azote et de pesticides serait compris entre 54 et . Ces coûts n'incluent pas ceux des impacts sur la faune, la flore, la fonge, les écosystèmes, la ressources halieutique, mais seulement les couts de dépollution. Toujours selon le CGDD, si l'on voulait aussi dépolluer les nappes, il faudrait encore ajouter une somme comprise entre 32 et (dont seulement pour le respect de la directive eaux souterraines). Au total, le coût de dépollution des eaux souterraines serait compris entre 522 et (hors coûts d’énergie du pompage avant traitement).

Ces coûts sont aujourd'hui en grande partie assumé par les ménages. Ceux des zones les plus polluées qui pourraient voir ce coût atteindre (+ 140 % par rapport à une facture d'eau moyenne) ». Il faut de à pour assainir l'eau des captages situés en zone d'« agriculture conventionnelle » . 

Ceci confirme un rapport de la Cour des comptes publié en 2010, qui notait aussi que des pays comme l'Allemagne (Bavière) ou le Danemark avaient par des écotaxes et des actions préventives significativement réduit (−30 %) les consommations d'azote et de pesticides, les rapprochant plus rapidement de l'objectif de bon état écologique des masses d'eau à atteindre avant 2015 en Europe. Le mode curatif coute 2,5 fois plus au mètre cube traité que la prévention, et n'améliore nullement la qualité de la ressource ajoutait la Cour des comptes.

L'OMS met en garde contre les dangers directs et indirects liés d'une part à l'utilisation de pesticides, d'autre part à l'exposition aux pesticides. En 1990, un rapport de l'OMS identifiait dus aux pesticides, dont 91 % par suicide. À l'échelle mondiale, 30 % des suicides ont lieu par empoisonnement aux pesticides, notamment dans les zones rurales des pays en développement. Selon une revue de littérature de l'université de Lund (Suède) de 2013, qui s'appuie notamment sur la source précédente, environ meurent chaque année d'intoxication aigüe par des pesticides. En 2004, un rapport de l'Organisation mondiale de la santé, de l'Organisation des Nations unies pour l'alimentation et l'agriculture et du Programme des Nations unies pour l'environnement cite un chiffre de 1990 qui estimait la mortalité des agriculteurs à , dont 99 % dans les pays en développement, alors que pourtant ceux-ci n'utilisaient que 25 % des pesticides vendus dans le monde. En 2017, un rapport d'experts auprès du Conseil des droits de l'homme des Nations unies reprend le chiffre de dus aux pesticides et plaide pour un nouveau traité global sur l'utilisation des pesticides, présentés comme non indispensables. Selon les auteurs, l'utilisation excessive des pesticides contamine les sols et la ressource en eau et représente une menace pour l'environnement, la santé et la production agricole elle-même.

Elles surviennent souvent après un contact direct (agriculteurs, entourage) et le délai relativement court (quelques heures à quelques jours) entre l'exposition au produit et l'apparition des troubles permet le plus souvent de relier les effets à la cause.

Dans certains pays pauvres, au début du , l'empoisonnement par les pesticide tue maintenant plus que les maladies infectieuses.

En France, la Mutualité sociale agricole (MSA) et le laboratoire GRECAN, d'après de premières études MSA, ont conclu qu'en France environ 100 à 200 intoxications aiguës (irritations cutanées, troubles digestifs, maux de têtes) par an sont imputées aux pesticides.

Les dérivés organochlorés induisent tout d'abord des troubles digestifs (vomissement, diarrhée) suivis par des troubles neurologiques (maux de tête, vertige) accompagnés d'une grande fatigue. À ceux-ci succèdent des convulsions et parfois une perte de conscience. Si le sujet est traité à temps, l'évolution vers une guérison sans séquelles survient généralement. L'intoxication aiguë avec ce type de produit est relativement rare, à moins d'ingestion volontaire (suicide) ou accidentelle (absorption par méprise, dérive de nuage, jet de pulvérisateur…).

Les dérivés organophosphorés ainsi que les carbamates, en inhibant la cholinestérase, induisent une accumulation d'acétylcholine dans l'organisme débouchant sur une hyperactivité du système nerveux et à une crise cholinergique. Les signes cliniques sont des troubles digestifs avec hypersécrétion salivaire, nausée, vomissement, crampes abdominales, diarrhée profuse. Il y a de plus des troubles respiratoires avec hypersécrétion bronchique, toux et essoufflement. Les troubles cardiaques sont une tachycardie avec hypertension puis hypotension. Les troubles neuromusculaires se traduisent par des contractions fréquentes et rapides de tous les muscles, des mouvements involontaires, des crampes puis une paralysie musculaire générale. La mort survient rapidement par asphyxie ou arrêt cardiaque. Un antidote spécifique existe pour cette catégorie de produit : le sulfate d'atropine qui neutralise rapidement les effets toxiques.

Les rodenticides à base d'anticoagulants agissent en abaissant le taux de prothrombine dans le sang, nécessaire à la formation du caillot sanguin, entrainant des hémorragies internes. Ils ne causent généralement pas de troubles de la coagulation, ni d'hémorragie chez l'adulte mais des hémorragies graves peuvent survenir chez l'enfant. Les symptômes, après quelques jours (pour une dose élevée) ou après quelques semaines (pour des prises répétées) sont : sang dans les urines, saignement de nez, hémorragie gingivale, sang dans les selles, anémie, faiblesse. La mort peut survenir dans les 5 à 7 jours qui suivent.

Ce risque est débattus pour l'adulte et peu mesurable faute de symptômes spécifiques et de données sur le degré d'exposition sauf pour les lymphomes. Chez l'enfant, des cancers (tumeurs cérébrales, leucémies et néphroblatomes...) sont plus fréquemment associés à une exposition chronique aux pesticides ou à celle des parents lors de la grossesse. Les impacts suspectés de l'exposition "in utéro" du fœtus sont ", "encore à confirmer en raison de possibles biais. " et les manifestations neurologiques sont "

Atteintes dermatologiques : rougeurs, démangeaisons avec possibilité d'ulcération ou de fissuration, urticaire sont très fréquents, surtout sur les parties découvertes du corps (bras, visage) ; les roténones causent des lésions sévères des régions génitales.

Atteintes neurologiques : les organochlorés induisent une fatigabilité musculaire et une baisse de la sensibilité tactile. Les organophosphorés entrainent à long terme des céphalées, de l'anxiété, de l'irritabilité, la dépression et l'insomnie, voire des troubles hallucinatoires. Certains paralysent (comme les dérivés mercuriels ou arsenicaux).En 2012, selon une trentaine d’études épidémiologiques, les pesticides pourraient induire des troubles dépressifs et psychiatriques (sans lien proportionnellement clair établi avec le taux de suicide plus élevé chez les agriculteurs que dans la plupart des autres professions).

Troubles du système hématopoïétique : les organochlorés peuvent diminuer le nombre de globules rouges et blancs, avec risque de leucémie.

Atteintes du système cardiovasculaire : les organochlorés peuvent causer des palpitations et perturber le rythme cardiaque.

Atteintes du système respiratoire : elles sont souvent liées aux phénomène d'irritation engendrés par bon nombre de pesticides, favorisant des surinfections et les bronchites, rhinites et pharyngites.
Atteintes des fonctions sexuelles : un nématicide (DBCP) a provoqué chez les employés de l'usine où il est synthétisé un nombre important de cas d'infertilité. D'autres substances semblent impliquées dans la délétion croissante de la spermatogenèse, soit directement comme reprotoxiques soit à faible doses ou via des cocktails de produits comme perturbateur endocrinien. Dans ce cas, l'embryon peut être touché, même par une exposition à de faibles doses (anomalies génitales, et peut-être risque augmenté de certains cancers et de délétion de la spermatogenèse chez le futur adulte).

Risques fœtaux : des pesticides franchissent la barrière placentaire et ont une action tératogène sur l'embryon. C'est le cas du DDT, du malathion, des phtalimides (fongicide proche de la thalidomide). Il peut survenir des accouchements prématurés ou des avortements, ainsi que des malformations de l'appareil génital du garçon. Il est conseillé à la femme enceinte d'éviter tout contact avec des pesticides entre le et le de la grossesse, mais certains produits ont une longue durée de demi-vie dans l'organisme (lindane, DDT par exemple).

Craintes de perturbations hormonales : Certains pesticides se comportent comme des « "leurres hormonaux" ». Chez des 308 femmes enceintes espagnoles, ayant ensuite donné naissance à des enfants jugés en bonne santé entre 2000 et 2002, on a trouvé au moins un type de pesticide dans le placenta (qui en contenait en moyenne 8, et jusqu’à 15, parmi 17 pesticides recherchés, organochlorés, car étant aussi des perturbateurs endocriniens). Les pesticides les plus fréquents étaient dans cette étude le 1,1-dichloro-2,2 bis (p-chlorophényl)-éthylène (DDE) à , le lindane à et l’endosulfan-diol à (Le lindane est interdit, mais très persistant).

Maladies neurodégénératives : une étude publiée en 2006 et d'autres ont conclu à une augmentation probable des risques de maladie de Parkinson à la suite de l'exposition chronique à certains pesticides, notamment…
. L'exposition aux pesticides augmenterait le risque de maladie de Parkinson de près de :
On ne dispose malgré tout d’aucune étude épidémiologique incriminant un produit particulier dans la maladie de Parkinson.

En France, cette maladie ne figure cependant dans aucun tableau de Maladie Professionnelle mais un cas récent pourrait faire jurisprudence. En 2012, le ministre de l'Agriculture a officialisé la reconnaissance du lien entre cette maladie neurodégénératrice (Parkinson) et les pesticides chez les agriculteurs.

Cancers : Le GRECAN a mis en évidence un plus faible nombre de cancers chez les agriculteurs que dans la population générale, mais avec une occurrence plus élevée de certains cancers (prostate, testicules, cerveau (gliomes)…). L'étude AGRICAN commencée en 2005 est en cours jusqu'en 2020 : elle concerne le suivi de personnes affiliées à la Mutualité sociale agricole (MSA). Il existe dans le monde une trentaine d'études qui montrent toutes une élévation du risque de tumeurs cérébrales. Selon l'INSERM il semble exister une relation entre cancer du testicule et exposition aux pesticides.

"L'étude d'Isabelle Baldi" : Une étude a conclu mi-2007 que le risque de tumeur cérébrale est plus que doublé chez les agriculteurs très exposés aux pesticides (tous types de tumeurs confondues, le risque de gliomes étant même triplé). Les habitants utilisant des pesticides sur leurs plantes d'intérieur ont également un risque plus que doublé de développer une tumeur cérébrale L’étude ne permet pas de dire si un produit ou une famille de pesticide serait plus responsable que d’autres, mais l’auteur note que des pesticides utilisés par les vignerons sont des fongicides.

Une autre étude, portant sur la population masculine française, établit des liens statistiques entre les pesticides employés et les lymphomes développés, et montre que l'incidence des lymphomes est deux à trois fois plus élevée parmi les agriculteurs.

Au niveau moléculaire, une étude française a démontré qu'il existait une relation entre l'exposition professionnelle aux pesticides et l'acquisition d'une anomalie chromosomique connue pour être l'une des étapes initiales de certains cancers.

Une étude de l'Observatoire Régional de Santé de Poitou Charente (septembre 2011) a montré une « surmortalité significative » des adultes par lymphomes (19 %) dans certains territoires agricoles. Un rapport du Réseau national de vigilance et de prévention des pathologies professionnelles (rnv3p) a confirmé un risque accru de tumeurs chez les personnes travaillant dans les secteurs Agriculture, pêche, sylviculture et aquaculture. L'exposition aux pesticides correspondrait à 45 cas sur 578 signalés.

En 2007, dans une méta-analyse incluant 83 études, 73 d'entre elles ont montré une association positive entre exposition aux pesticides et cancer.

Le , l'Institut national du cancer publie la version actualisée d'une fiche repère portant sur un état des lieux des connaissances sur les pesticides et les risques de cancers.

Cette notion a été introduite au milieu des années 1970 pour décrire le phénomène de circulation mondialisée de pesticides interdits dans certains pays. Il décrit les situations où des pesticides interdits dans des pays industrialisés continuent à y être produit par l'industrie chimique, mais uniquement pour l'exportation vers les pays en développement.

Ces produits sont ensuite utilisés dans ces pays en développement, mais presque entièrement sur les cultures d'exportation. Une partie de ces pesticides peut contaminer les eaux marines ou de pluies, ou être directement réexportée - sous forme de résidus, ou de contaminants sur ou dans les produits envoyés vers les pays riches. Ils peuvent aussi l'être dans des poissons, crustacés ou de la viande via la chaine alimentaire, éventuellement bioconcentrés.

Dans les années 2000, Ryan E. Galt estime qu'en raison ' et de certaines dynamiques ' nouvelles, ce concept doit être révisé, et que la réglementation des pays industrialisés sur les pesticides, devrait en visant la sécurité intégrer une évaluation des risques multi-critères (multi-characteristic risk assessments), et que la notion de « cycle du poison » ' ; Il parle de « divergence des pesticides par l'orientation des marchés ».

La rémanence de certains produits peu dégradables (ou non dégradables) (cuivre, produits à base d'arsenic et plomb…) dans le sol et les sédiments doit aussi être prise en compte.

Tout pesticide nécessite une autorisation de mise sur marché et cette autorisation est limitée dans le temps de façon à pouvoir prendre en compte des nouveautés scientifiques (de santé ou d'environnement) et des évolutions législatives. Ainsi certains produits antérieurement autorisés sont interdits en raison de leur dangerosité démontrée ultérieurement (pollution rémanente des eaux, apparition de résistance de souches, influence métabolique à long terme…).

L'utilisation de pesticides retirés du marché est interdite et soumise à contrôle. L'article L.253-17 du Code rural prévoit des peines qui peuvent aller jusqu'à d'amende et six mois d'emprisonnement.

En France, depuis plusieurs années, de nombreux produits jusqu'alors autorisés (donc considérés comme efficaces et ne présentant pas de risque inacceptable) ont été interdits à la mise sur le marché et à l'utilisation. Ces produits sont appelés « Produits Phytosanitaires Non Utilisables » (PPNU). En mars 2012, l'ONG Générations Futures, a présenté douze revendications à l'occasion du colloque "Pesticides et Santé" organisé au Sénat : la relance du plan Ecophyto, la fin du système de dérogation pour certaines molécules dans le cadre des impasses techniques, des bandes de de largeur (comme en Argentine) sans traitement à proximité des zones d'habitation (en épandage aérien, la limite prévue par le régime dérogatoire est de ), le retrait du marché des substances classées CMR (« "Cancérogène, mutagène et reprotoxique" ») de catégorie 1 à 3, la délivrance de l'autorisation de mise sur le marché de tout produit étendue aux ministères de l’Écologie et de la Santé en plus du ministère de l'Agriculture.

Le Parlement européen s'intéresse également au sujet. Un rapport datant de janvier 2009 revient sur une directive de 1991 et prévoit l'interdiction de certaines substances entrant dans la composition des pesticides. L'Union européenne dispose d'une réglementation concernant les produits phytosanitaires et les pesticides.

Un autre rapport parlementaire français constate que les dangers et les risques des pesticides pour la santé sont sous-évalués, que le suivi des produits après leur mise sur le marché est imparfait, que l'effet des perturbateurs endocriniens est mal pris en compte, que les protections sont insuffisantes, et que le plan Ecophyto 2018 doit être renforcé.

Des ONG, dont Greenpeace, tout en reconnaissant le caractère multifactoriel (plus de 40 facteurs) de la disparition des abeilles demandent l'interdiction des produits qu'elles jugent les plus dangereux pour les abeilles (imidaclopride, thiaméthoxame, clothianidine, fipronil, chlorpyriphos, cyperméthrine et deltaméthrine) et "".
En avril 2013, la Commission européenne annonce une probable suspension, au décembre 2013 et pour 2 ans, de 3 insecticides de la famille chimique des néonicotinoïdes (imidoclopride, clothianidine et triaméthoxane commercialisés sous divers noms : Gaucho, Cruiser, Poncho) probablement impliqués dans le déclin des abeilles domestiques.

Un exemple typique de changement de classification est celui de l'atrazine, utilisé massivement en France et dans de nombreux autres pays comme un herbicide d'une grande efficacité pour le désherbage du maïs. L'atrazine (comme toute la famille des triazines) est à présent reconnue comme à l'origine de pollutions majeures des nappes souterraines et des eaux de surface qui sont polluées à 50 % en France (par rapport aux normes édictées pour les triazines). Par exemple, en Bretagne, comme dans le Sud-Ouest et l'Île-de-France, il est courant de trouver, dans des prélèvements d'eau potable, des taux de triazine dix fois plus élevés que le seuil autorisé de 0,1 microgramme par litre.

Jusqu'en 2002, la famille des triazines constituait les produits phytosanitaires les plus employés en France, utilisés à 80 % en termes de surface par les producteurs de maïs conventionnel. Ils avaient été introduits en 1962 et étaient caractérisés par une excellente efficacité et un faible coût.
Protégés des UV solaires dans le sol, ils se sont avérés moins dégradables que ce qu'avait annoncé le fabricant. 
9 ans après son interdiction en Allemagne, l'atrazine était encore le pesticide quantitativement le plus présent dans la pluie, et ses produits de dégradation (ex : désisopropyl—atrazine, déséthyl-atrazine) sont encore très présents alors que la molécule-mère commence à disparaître. 

En raison de sa toxicité et de sa pollution rémanente dans les eaux (molécule peu biodégradable), l'atrazine a été bannie en Allemagne puis après quelques années en France en 2001, comme le reste de la famille des triazines (mise en application en juin 2003 pour la France) après des années d'utilisation (1962-2003).

Ce revirement pourrait être lié à une prise de conscience progressive de la dangerosité de certains produits phytosanitaires, ou éventuellement aux deux condamnations de la France par la cour de Justice européenne pour avoir manqué à ses obligations en matière de qualité de l'eau. De nombreux autres produits sont en discussion, tel l'arsénite de soude (produit cancérigène très utilisé en viticulture). Le programme européen global de réforme écologique de l'agriculture prévoit d'interdire d'ici 2008 près de 400 produits jugés dangereux pour la santé de l'homme qui avaient été cependant agréés par la directive de 1991.
L'arsénite de soude est dorénavant inutilisable en viticulture. Tous les résidus (bidons vides ou partiellement vides) ont été récupérés lors de collectes spécifiques organisées par les autorités compétentes. 
Des contrôles du Service Régional de la Protection des Végétaux (SRPV) peuvent être réalisés dans toutes les exploitations agricoles et des sanctions sont prévues en cas de détention de produits phytosanitaires interdits (les PPNU).

Un exemple de cas très débattu au début du est celui du Gaucho, accusé par les apiculteurs d'être à l'origine de la diminution importante de certaines populations d'insectes, notamment les abeilles.

Bien qu'interdit depuis longtemps dans les pays occidentaux, on en trouve encore des traces dans la graisse des animaux, mais aussi dans la nourriture.Bien que l'utilisation du DDT à grande échelle ait été abandonnée depuis trente ans, l'OMS recommande son utilisation dans les habitations pour lutter contre les moustiques vecteurs du paludisme.
L'Europe dispose d'une directive sur les biocides et a annoncé en 2011 un renforcement de la prise en compte de la biodiversité dans ses politiques d'autorisation et contrôle des pesticides "(".).

Elle a adopté en janvier 2009 un "paquet législatif" sur les pesticides, incluant une nouvelle législation durcissant les usages et autorisation en Europe, base d'une directive-cadre sur l"'utilisation durable des pesticides"visant à mieux protéger les consommateurs européens et l'environnement, interdire les pesticides toxiques et encourager le développement d'une agriculture durable. En 2010, un nombre important et inhabituel de dérogations, voire des semblent avoir cependant permis un usage significatif de pesticides normalement interdits par les nouvelles normes européennes.

Les personnels (dont agriculteurs) effectuant les épandages semblent les plus exposées à un impact sur leur santé. Lors des épandages, il leur est couramment recommandé de porter une combinaison et des gants adaptés à ce pesticide, ainsi qu'un masque de protection lors de la préparation.
Cependant, ces combinaisons sont peu portées, car elles présentent des inconvénients ergotoxicologiques : peu adaptées à la diversité des tâches de l'agriculteur, elles constituent une source d'inconfort, notamment thermique, favorisent la sudation et la rémanence des imprégnations. Dans certains cas, même, les porteurs d'une telle combinaison sont plus contaminés que ceux qui ne la portent pas. Enfin, les combinaisons, et plus particulièrement le masque, exigent un entretien peu aisé. Des tabliers, plus pratiques à mettre et retirer, existent depuis 2010. parfois, "", mais l'usage de protection pourrait nuire à l'image de l'exploitation agricole : les habitants voisins peuvent se sentir menacés par les épandages ou les consommateurs peuvent associer cette tenue à une mauvaise qualité de la production. Ce risque "social" constitue un facteur supplémentaire dissuadant souvent l'épandeur d'utiliser cette protection. 

En France, l'Anses, saisie par la Direction générale de l'alimentation (DGAI) à propos des meilleurs caractéristiques des équipements de protection individuelle (EPI) pour l'utilisation de certains produits phytopharmaceutiques a en 2012 recommandé l'inscription, dans la loi d'une demande d'autorisation de mise sur le marché et de ; les sociétés voulant mettre un nouveau pesticide sur le marché devraient alors présenter une liste d'EPI les plus protecteurs. Dans le même avis, l'ANSES encourage la création d'une norme européenne pour pour les applicateurs de pesticides.

Dans les tracteurs, les cabines pressurisées climatisées et munies de filtres, bien que coûteuses, fournissent un complément de protection. Elles présentent cependant elles aussi des défauts d'utilisabilité et ne constituent pas une protection totale.

Pour pallier ces contraintes, des pratiques supplémentaires sont mises en œuvre : La limitation des durées d'exposition est la première précaution. Parfois, l'odeur alerte quant au danger d'exposition, bien que tous les pesticides n'aient pas d'odeur et qu'une substance peut être nocive bien en deçà du seuil de perception. Les personnes sensibles, notamment les femmes enceintes, doivent être mises à l'écart des zones que l'on sait traitées. Depuis l'arrêté du 12 septembre 2006 en France, des délais de ré-entrée dans les zones traitées sont fixés (6 à 48h selon le pesticide) pour toute personne.

Concernant la protection des consommateurs et les contrôles :

Les enfants sont particulièrement vulnérables. Selon l'EPA (2008), beaucoup de bébés ne développent pas de capacité à métaboliser (dégrader) les pesticides qu'ils ont absorbés durant les deux premières années de leur vie, ce qui les expose particulièrement. L'EPA a interdit deux pesticides domestiques aux États-Unis (Diazinon et Chlorpyrifos), ce qui a conduit à une rapide décroissance de ces produits et de l'exposition de ces produits à New York, où les enfants se montrent en meilleure santé depuis l'interdiction de ces produits. De plus, par kilogramme de masse corporelle, comme pour la plupart des toxiques, les enfants en respirent et en absorbent plus (en moyenne) que les adultes.

Ces produits spécifiques n'existaient pas, ou n'étaient pas recherchés par les fabricants, faute de marché rentable. En Europe, un nouveau règlement de 2009 (CE) contient des dispositions visant à accroître la disponibilité des produits phytopharmaceutiques pour les cultures d'importance mineure.

L'Europe autorise certains usages dérogatoires - à certaines conditions et quand il n'y a pas d'alternatives - Il faut par exemple un «"danger imprévu pour la santé humaine et l'environnement"», ou la nécessité de répondre à des "".

Pour l'Europe, ce sont les États-membres qui doivent veiller au respect des limites maximales de résidus (LMR) fixées par le règlement (CE) /2005. La Commission reconnaît . De plus, "."

De nouvelles lignes directrices pourraient être élaborées mi 2011 ""

Cependant, le nombre réel des dérogations ou leurs justifications ne sont pas publiés, et la commission ' et selon un rapport en 2011, produit par PAN-Europe (qui rassemble plus de 600 ONG), daté du 26 janvier 2011 par PAN-Europe, il y a eu une augmentation anormale et exponentielle (de plus de 500 %) du nombre de dérogations pour des pesticides non autorisés sur une période de quatre ans. Ainsi en 2010 les États membres ont demandé 321 dérogations.

De nombreuses plantes produisent naturellement des substances pour se protéger : ainsi le tabac produit l'insecticide nicotine, et le chrysanthème de la pyréthrine. Cette logique a été poussée plus loin par l'introduction de plantes génétiquement modifiées qui produisent elles aussi, généralement tout au long de leur cycle de croissance, leurs propres matières actives (ainsi le "Bt", une protéine insecticide produite à l'origine par une bactérie, qui est produite dans la plante génétiquement modifiée au niveau des racines, tiges, feuilles et pollen, mais pas dans la graine) ou des substances fongicides ou bactéricides. Cependant, la question se pose de savoir s'il faut classer ces organismes artificiellement créés parmi les pesticides.

La résistance des insectes et en particulier leur « résistance aux pesticides » a été identifié comme un enjeu important dès les années 1960. Elle résulte de la sélection d'individus tolérants des doses qui devraient normalement tuer la majorité ou la totalité des organismes normaux. Les individus résistants se multiplient d'autant mieux qu'il ne sont plus en situation de « compétition intraspécifique », devenant alors en très peu de générations les individus majoritaires de la population. Elle a surtout été observée chez les plantes mais aussi chez les insectes, et en particulier et de plus en plus chez de nombreuses souches et espèces de moustiques devenus rapidement résistants au DDT puis aux organophosphorés, aux carbamates et aux Pyréthrinoïdes. De même chez la mouche domestique et d'autres insectes vecteurs de zoonoses ou de maladies humaines et arthropodes. Les acariens ont également développé des adaptations à certains acaricides) génétiquement transmises à leur descendance.

La résistance est définie par l'OMS comme « l'apparition dans une population d'individus possédant la faculté de tolérer des doses de substances toxiques qui exerceraient un effet létal sur la majorité des individus composant une population normale de la même espèce ».

Elle résulte de la sélection, par un pesticide, de mutants qui possèdent un équipement enzymatique ou physiologique leur permettant de survivre à des doses létales de ce pesticide. Les produits anti-poux sont également concernés, avec des souches de poux devenant de plus en plus résistantes par exemple au malathion et à la d-phénothrine.

Dans le cadre de la "sélection naturelle", un pesticide sélectionne des gènes de résistance. Ces gènes peuvent apparaître lors des mutations aléatoires et naturelles ou provoquées par l'exposition à des agents mutagènes, ou être antérieurement présents dans le génome de l'organisme.

Comme dans le cas des maladies nosocomiales impliquant l'antibiorésistance, les scientifiques cherchent à modéliser et mieux comprendre ces phénomènes pour proposer et évaluer les stratégies de lutte contre l'apparition de ces résistances, notamment concernant les vecteurs de maladies humaines

Un insecte ravageur est classé résistant quand plus de 50 % de sa population dans un champ est porteur des gènes de résistance et résiste effectivement au pesticide.

Depuis le premier cas enregistré (résistance du pou de San José aux polysulfures dans les vergers de l'Illinois en 1905) les cas de résistance ont augmenté de manière exponentielle : 5 cas en 1928, 137 en 1960, 474 en 1980. En 1986, 590 espèces animales et végétales présentaient une résistance : 447 espèces d'insectes ou d'acariens, une centaine de pathogènes des végétaux, 41 espèces de mauvaises herbes ainsi que des nématodes et des rongeurs.

Ces résistances semblent parfois anecdotiques, car n'étant que locale, mais d'autres se sont généralisées au monde entier, comme pour la mouche domestique "Musca domestica" résistante aux organochlorés ou le "Tribolium" (ver de la farine) résistant au lindane et au malathion. Le moustique "Culex pipiens" a développé des résistances élevées aux organophosphorés.

Toutes les familles d'insecticides peuvent induire des résistances chez les insectes. Les pyréthrinoïdes et analogues des hormones juvéniles n'échappent nullement à la règle, avec 6 cas de résistance aux pyréthrinoides en 1976, explosant à 54 cas en 1984.

En revanche, au niveau taxonomique, les différents ordres d'insectes expriment des sensibilités variées.
Les résistances sont plus souvent observées chez les Diptères, devant les hémiptères (pucerons et punaises). Les Coléoptères, Lépidoptères et Acariens représentent chacun des cas de résistance. Par contre, les Hyménoptères (abeilles, guêpes) ne semblent pas développer de résistance, peut-être pour des raisons génétiques.

En 1984, on connaissait 17 espèces d'insectes et d'acariens résistants aux 5 principaux groupes de pesticides : "Leptinotarsa decemlineata" le doryphore de la pomme de terre, "Myzus persicae" le puceron du pêcher, "Plutella xylostella" la teigne des crucifères, le ver de la capsule, des noctuelles "Spodoptera" et des espèces d'Anophèles.

La résistance est parfois recherchée : c'est le cas pour l'acarien prédateur "Phytoseiulus persimilis" utilisé contre les Tétranyques des serres.

Les cultures les plus concernées par les phénomènes de résistance sont le coton et l'arboriculture fruitière. On peut citer le cas de la mouche blanche "Bemisia tabaci" (Aleurode) dans les cultures de coton de la plaine de Gézira au Soudan au début des années 1980 ou celui des cicadelles du riz en Extrême-Orient et dans le Sud Est asiatique. En Indonésie, la lutte chimique contre "Nilaparvata lugens" est devenue impossible au milieu des années 1980, obligeant le pays à se tourner vers la protection intégrée des rizières en 1986.

Des phénomènes de résistance sont rapidement apparu chez les insectes s'attaquant aux OGM végétaux produisant du Bt, et malgré les parades mises en place, ce phénomène s'est étendu des années 1990 à 2010.

Dès les années 1980, quelques chercheurs annonçaient de probables résistances, et invitaient à s'y préparer et à gérer ce risque. En 1996 de premiers cas de résistances massive au Bt sont constatés dans le coton Bt américain. Dans les années 2000, on freine (sans l'arrêter) la diffusion de nouvelles résistances en faisant produire deux formes différentes de Bt par la plante. Trois autres méthodes testées et proposées ont été la création de refuges sans OGM près des champs d'OGM, l'insertion par transgenèse de gène codant d'autres molécules insecticides ou encore la diffusion d'insectes parasites stérilisés. En 2008, contrairement à la théorie soutenue par les fabricants, sur le terrain des phénomènes importants de résistance d'insectes-cibles apparaissent.
Une étude publiée dans Nature Biotechnology (2013) a passé en revue 77 études faites dans huit pays et cinq continent, sur le maïs Bt et le coton Bt notamment. Elle a conclu que face aux plantes OGM rendues insecticides par l’insertion d'un gène leur faisant produire la protéine "Bt", la résistance des insectes au BT augmente également.
La rapidité d'apparition (2 à 3 ans après l'autorisation de culture de l'OGM parfois) d'une résistance varie en fonction des pratiques agricoles (la résistance apparait moins vite ou touche moins d'insectes en présence de « zones refuges » (zones plantées de la même plante, mais non-OGM, à proximité du champ OGM). 
Pour 13 types de ravageurs suivis, l'un était devenu résistant en 2005, et quatre autres en 2011. Trois des 5 types de résistance ont émergé aux États-Unis dans le coton et le maïs, pays où les OGM sont les plus cultivés. l'un a émergé en Inde et l'autre en Afrique. Un 6 type de résistance semble émerger aux États-Unis (mais non comptabilisé car pas encore présent chez 50 % des individus d'un champ).
Les chercheurs ayant publié cette étude jugent "". Ceci pose problème pour l'agriculture Biologique et la démoustication qui ont le droit d'utiliser la molécule Bt en pulvérisation.

Les facteurs favorisant l'apparition d'une résistance sont classés en 3 groupes :

Les deux premiers types de facteurs sont inhérents à l'espèce et ne peuvent être -"a priori"- modifiés par l'homme, qui ne pourra intervenir qu'au niveau du troisième groupe.

Il est possible d'établir une hiérarchisation des facteurs prépondérants à l'apparition des phénomènes de résistance. Les plus importants sont :

Rappelons que l'augmentation de dose appliquée ne fait qu'accroitre la pression de sélection. De même, la multiplication des traitements ne conduit qu'à éliminer les migrants sensibles susceptible de diluer les gènes de résistance.
Il faut donc jouer sur les facteurs opérationnels en cherchant à limiter au maximum la pression de sélection. Dans ce but, il faut :


De nombreuses plantes ont été modifiées génétiquement pour être tolérantes à un désherbant total (le glyphosate). Elles contribuent donc à généraliser l'usage de ce désherbant, au risque d'étendre les résistances qui commencent à apparaître chez certains végétaux.

La teneur en résidus de pesticides dans les produits est règlementée au niveau européen (règlement 396/2005 et ses annexes : règlement 178/2006, règlement 149/2008 et modifications). Ces règlements concernent à la fois les denrées alimentaires (alimentation humaine) et les aliments pour animaux. Ils définissent des limites maximales réglementaires (LMR) qui sont fixées sur le respect des bonnes pratiques agricoles et garantissent la sécurité des consommateurs.

La Commission européenne conduit un programme annuel de suivi des résidus de pesticides dans les fruits, légumes et céréales disponibles sur le marché européen. Ce suivi annuel porte sur environ échantillons prélevés dans les 27 états-membres. Les résultats publiés jusqu’en 2008 (produits analysés en 2006) sont disponibles sur le site de l'EFSA et de la DG Sanco (direction générale de la santé des consommateurs de la Commission européenne). Les résultats des années suivantes ont été publiés dans la revue de l’Autorité de sécurité alimentaire européenne.

Sur la période 2001-2009, sur plus de échantillons analysés, 54,5 % des échantillons ne contenaient pas de résidus de pesticides, 41,3 % des échantillons contenaient au moins un résidu dont la teneur était inférieure à la LMR. 4,3 % des échantillons contenaient au moins un résidu dont la teneur était supérieure à la LMR. Certains échantillons contenaient plus d’un résidu. Les dépassements de LMR étaient plus fréquents dans les fruits et légumes que dans les céréales (1 à 2 % de dépassement de LMR). Par ailleurs, les produits importés avaient des dépassements de LMR plus fréquents que les produits européens, sans que les causes aient été publiées (nature, mode de production et de transport du produit, pays d’origine à règlementation différente de celle de l’Union européenne, etc.).

Dans ses rapports sur les années 2008 et 2009, l'EFSA conclut que l'exposition à long terme des consommateurs ne porte pas atteinte à leur santé. La vérification de l'exposition à court terme montre que pour 134 échantillons analysés (0,19 %) la dose de référence aigüe (ARfD) pourrait avoir été dépassée si l'aliment concerné était consommé en quantité élevée.

En France, les résultats du programme de surveillance 2008 de la DGCCRF (Direction Générale de la Concurrence, de la Consommation et de la Répression des Fraudes du Ministère de l'Économie et des Finances) indiquent des dépassements de LMR de :


Les résultats d'analyses des denrées alimentaires issus des laboratoires nationaux permettent à l'Observatoire des résidus de pesticides (ORP) de la Direction de l'évaluation des risques de l'ANSES d'évaluer les expositions alimentaires des consommateurs, afin d'orienter les programmes de surveillance nationaux à venir ainsi que les mesures correctives nécessaires par les directions ministérielles chargées de la gestion du risque pour les consommateurs français.

En 2011, l'ANSES a publié une étude sur les niveaux résiduels des aliments préparés « tels que consommés » (c'est-à-dire dans l'assiette du consommateur) incluant divers contaminants dont les résidus de pesticides : l'Étude de l'Alimentation Totale (EAT2). Cette étude a notamment mis en évidence une fréquence élevée de présence de résidus de pesticides dans les aliments préparés (lavés, cuisinés…) et/ou transformés, avec 37 % des échantillons alimentaires analysés contenant des résidus, tels que par exemple les produits à base de farine de blé (ex : pain, viennoiseries, pâtes, etc.). Cette étude évalue également l’exposition des différentes populations de consommateurs, en fonction de leurs habitudes alimentaires et de la teneur en résidus dans les aliments. L’estimation de cette teneur dépendant de la sensibilité des analyses, il a été retenu une estimation haute et une estimation basse. Le nombre de résidus recherchés et pour lesquels existe une valeur toxicologique de référence (VTR) est de 254. Parmi ces 254 molécules, 244 ne posent pas de risque chronique pour les consommateurs. Neuf molécules présentent un dépassement de VTR avec l’estimation haute, alors qu’elles ne présentent pas de dépassement avec l’hypothèse basse. Pour ces neuf substances, l’ANSES n'écarte pas un risque chronique. Enfin, une molécule présente un dépassement de la VTR sous l’estimation basse (qui minore les teneurs dans les aliments et donc l’exposition), et le risque est réel pour certains consommateurs.

Ces études portant sur l’exposition des consommateurs par la voie alimentaire ne doivent pas faire oublier que les résidus de pesticides concernent d'autres voies que la voie alimentaire (voies respiratoire et cutanée). Or, peu d’études traitent de ces sujets qui concernent particulièrement des professionnels (agriculteurs, horticulteurs, travaux publics…) mais aussi des particuliers (jardiniers amateurs, utilisateurs d’insecticides ménagers, ..). Il est probable que les problèmes de santé publique soient plus importants dans ces cas qu’avec les aliments.

Enfin, la présence de résidus de pesticides ne concerne pas que les effets sur la santé humaine, mais aussi les effets sur d’autres organismes vivants (végétal ou animal notamment). Le problème pour les gestionnaires de risques est d’autant plus difficile que les effets sont plus ou moins marqués selon l’espèce considérée, et que dans un univers multidimensionnel, il n’est pas aisé d’évaluer le couple bénéfices/risques pour chacune des molécules prise individuellement et en interaction avec d’autres molécules.

Chaque produit est assorti d'une homologation pour un ou plusieurs usages spécifiques qui doivent être spécifiés sur l'étiquette. 
La classe de danger doit également figurer sur l'étiquette, représentée par un logo international.

L'étiquetage ici en question est celui sur l'emballage du pesticide. Pour ce qui est des fruits et légumes à destination de l'alimentation humaine, aucune mention des pesticides utilisés pendant les phases de croissance et maturation n'est mise à disposition pour le consommateur final. En effet, tant que les Limites Maximales en Résidus, sans risque avéré pour la santé, sont respectées (voir chapitre sur Résidus), les traces de pesticides sont légalement acceptées.
On sait maintenant mesurer une partie des molécules utilisées, ainsi que de nombreux résidus, métabolites ou produits de dégradation

Pour mieux évaluer et cartographier les risques, les épidémiologues et écoépidémiologues auraient besoin de connaître les données commerciales (ventes, commandes) précises et géo-référencées et de pouvoir relier les quantités achetées à celles réellement appliquées par surface chez les producteurs et chez les particuliers. 
Cependant, en dépit des progrès des systèmes de traçabilité dans l'agroalimentaire, dans le domaine des pesticides, il semble encore difficile de réunir ces données.

Par ailleurs, les pesticides en suspension dans l'air, ou transportés par l'eau et adsorbés sur les particules du sol sont également difficiles à suivre.

Aussi, pour disposer de données et respecter la convention d'Aarhus sur l'accès à l'information environnementale, certains pays construisent-ils des structures de surveillance à long terme, dont la France avec un "Observatoire français des pesticides Observatoire des résidus de pesticides" (ORP) créé par l"'Agence française de sécurité sanitaire de l'environnement et du travail" (AFSSET) qui a dès 2007 commencé à mettre en ligne une carte de France interactive donnant accès aux données disponibles sur la présence de résidus de pesticides dans l'air, l'eau, les sols et certaines denrées alimentaires. L'agence encourage les propriétaires de données sur les pesticides à contribuer volontairement à mettre à jour cet outil. Ce site ne donne pas d'information sur où se vendent, et en quelle quantité les différents types de pesticides.

Denrées alimentaires : 
En Europe, des résultats d'études de résidus sont disponibles sur le site de la DG Sanco (direction générale de la santé des consommateurs de la Commission européenne).

Dans un contexte de réglementation croissante, de régression massive des pollinisateurs et d'interdiction de certains produits (dont nombreux organochlorés), les fabricants présentent de nouveaux pesticides comme plus "verts", c'est-à-dire moins impactants pour l'environnement, plus rapidement (bio)dégradables, en s'appuyant sur les progrès de la bioinformatique (pour le design moléculaire des futurs produits) chimie ("« chimie verte »" (ex : ajout de photosensililisateurs accélérant la dégradation de molécules d'un pesticide exposé au soleil) et de la toxicologie des pesticides, le développement de nouvelles cibles biochimiques, le recours accru aux cultures génétiquement modifiées qui permettraient selon eux de réduire la quantité et la variété des pesticides appliqués.

On étudie les possibilités de biodégrader ou métaboliser certains pesticides par des bactéries cultivées (ex projet LIFE-PHYTOBARRE du Laboratoire de biologie cellulaire du CEA), aidé par Life + dans différents types de sols et de climats..

On a aussi vu apparaître des pesticides d'origine microbienne (Bt) ou microbiens. De nombreux micro-organismes bénéfiques candidats existent, qui pourraient faire partie des futurs biopesticides, d'origine naturelle et microbienne (voire virale). Leur développement nécessite toutefois des précautions particulières et méthodes différentes (choix de souches microbiennes, isolement, mise en culture pure, essais biologiques d'efficacité "in vitro, ex vivo, in vivo", et en serre puis en plein champ (essais pilotes en conditions réelles d'application). La conservation, le transport, la livraison commerciale et la mise en œuvre d'un pesticide microbien peuvent être facilités par des additifs biocompatibles qui font encore l'objet de recherche. Un grand nombre de brevets de biopesticides ont été développés, mais peu sont disponibles pour l'agriculture industrielle ou la sylviculture, souvent en raison d'une spécificité excessive, ou de problèmes de biosécurité ou de préoccupations environnementales (risque élevé d'effets non ciblés, de mutation…) inconnues en termes d'allergénicité, de toxinogénécité (production de métabolites secondaires toxiques pour des plantes, animaux, champignons ou les humains), pathogénicité (pour les plantes ou les animaux), risque d'utilisation bioterroriste. Des plants ou graines autoprotégés par un microbe symbiote sont également envisageables.

Dans un certain nombre de cas, des alternatives existent, avec des avantages et inconvénients variant selon les contextes et le pas de temps envisagé.
Elles diffèrent selon les usages (protection du bois, protection des cultures…)






</doc>
<doc id="16721" url="https://fr.wikipedia.org/wiki?curid=16721" title="Shivaji">
Shivaji

Shivaji Bhonsle (16 février 1630 - 3 avril 1680) est un général Marathe et fondateur du Royaume marathe.

Shivaji Bhonsle, qui deviendra le Chhatrapati Shivaji Maharaj, fils de Shahaji Bhonsle et de Jijabai Jadhav, naît en 1627 ou en 1630 dans le fort de Shivner non loin de Pune au Maharashtra dans une période où une grande partie de l'Inde est aux mains des musulmans. Son père est un brillant général qui a servi le sultan Muhammed Adil Shah de Bijapur, Nijam Shah d'Ahmadnagar et les Moghols. 

Le poète Sant Tukaram maharaj gourou, fait son éducation et lui transmet son idéal patriotique hindou. Très jeune, il fait preuve d'indépendance d'esprit et de dons militaires exceptionnels. Alors que l'Inde subit le joug musulman, il fait le vœu en 1645, au temple de la déesse Bhavani, d'établir un royaume hindou. Il met en œuvre une stratégie de guérilla qui lui permet en 1646 de s'emparer d'un fort près de Pune. Son succès lui permet de rallier d'autres tribus et de s'attaquer aux possessions du sultan de Bijapur et même de s'emparer de plusieurs forts. Le sultan envoie une armée sous les ordres de Afzal Khan mais Shivaji assassine ce dernier par traîtrise au cours de négociations au moyen d'un bagh-nakha ou « griffes de tigre ». Ayant perdu son commandant en chef, l'armée de Bijapur est ainsi défaite sans difficultés.

En 1660, Aurangzeb, inquiet de la puissance grandissante de Shivaji, envoie contre lui une armée qui prend Pune, la capitale marathe, mais qui ne résiste pas à la contre-attaque de ce dernier.

En 1664, Shivaji pille Surat, un port de commerce important pour les Moghols et où sont installés des comptoirs étrangers. La même année, il fonde l'Empire marathe. Aurangzeb attaque à nouveau, puis les deux hommes signent un traité de paix en 1665 par lequel Shivaji rétrocède 23 des 35 places fortes dont il s'était emparé.

Shivaji et son fils Sambhaji se rendent à Agra, en 1666, mais ils sont traités par Aurangzeb de manière très méprisante et mis sous surveillance. Déguisés en sadhu, les deux hommes s'enfuient et retournent à Pune. Aurangzeb lui accorde en 1670 le titre de raja pour l'amadouer, il se révolte de nouveau et pille Surat pour la deuxième fois.

Il se fait couronner roi ("maharaj") le à Raigarh. Il continue à se battre victorieusement contre les Moghols, contre Bijapur, contre les Portugais de Goa et même contre les pirates abyssins qui rendent ses côtes peu sûres. 

Il meurt à Raigarh en 1680 et laisse à son fils aîné Sambhaji un immense empire.


Il est considéré par les nationalistes du Maharashtra comme un héros «national», et à ce titre, son image est utilisée à des fins politiques, notamment par le parti Shiv Sena («Armée de Shiva»). Ce dernier a renommé plusieurs bâtiments de Bombay en son honneur à la suite de sa victoire aux élections locales à la fin des années 1990 :






</doc>
<doc id="16724" url="https://fr.wikipedia.org/wiki?curid=16724" title="Syndicat">
Syndicat

Un syndicat est un groupement de personnes physiques ou morales pour la défense ou la gestion d'intérêts communs.

Le terme syndicat vient du terme "Syndic". Étymologiquement, le Syndic désigne historiquement d'abord une personne ayant à gérer la gestion de la défense des intérêts communs d'une collectivité, puis une personne qui représente un groupe autre que le groupe municipal, un conseiller, un avocat. À l'origine, le syndic représente son groupe dans une action en justice. Dans ce contexte, le terme syndicat désigne la fonction jouée par le syndic, mais aussi le groupe représenté qui s'est aussi appelé chambre syndicale.

En France, le terme a de nombreuses acceptions mais en raison de l'importance de la relation entre employeurs et employés (ouvriers et employés, techniciens, agents de maîtrise, ingénieurs et cadres), il désigne relativement souvent les organisations de défense de l'intérêt des salariés (ouvriers, employés ou cadres), souvent désignées sous le sigle « OS » (organisations syndicales) et protégées par le droit du travail, une législation particulière reconnaissant la liberté syndicale, et le droit de grève.

Le mot syndicat a été formé avec ses deux racines: le radical grec «syn» (συν), marque l'idée de réunion (comme dans sympathie, symphonie); la racine indo-européenne deik signifie «montrer», et donne en grec «diké», la règle, le droit, la justice. Les deux ensemble ont donné sundikos (σύνδικος ), à l'origine des mots syndic, et syndicat.

Le mot grec porte le fait de défendre quelqu'un en jusitce ou d'appartenir en commun.
Le syndic, dans l'antiquité grecque correspond à un défenseur à l'occasion d'une action en justice. 

Le mot latin syndicus décrit l'avocat et représentant d'une ville.

Le mot a été régulièrement utilisé dans le Midi de la France.

Le mot syndicat (graphié sindicat) entre dans le dictionnaire pour désigner l'activité en 1477 dans le ville de Foix constitué par la charge ou fonction de syndic.

En 1514, un dictionnaire définit un "sindicat" comme association qui a pour objet la défense d'intérêts communs.

La découverte officielle de l'Amérique en 1492 et l'histoire du Québec vont conduire à une évolution sensiblement divergente de part et d'autre de l'Atlantique.

Au Canada, la présence de syndicats est attestée au début du , alors qu'au cours de la Guerre de 1812 (Guerre anglo-américaine de 1812), des artisans spécialisés des Maritimes ont mis en place une structure de lignée syndicale.

En 1730, un dictionnaire définit le syndicat comme une association qui a pour objet la défense d'intérêts professionnels.

Le syndicalisme s'inscrit dans la lignée des groupements corporatifs (métiers, compagnonnage...) des sociétés modernes et médiévales. Ces groupements sont interdits en 1791 par la loi Le Chapelier promulguée le 14 juin 1791, et subissent une répression opiniâtre lors de la première révolution industrielle. Mais en 1864, la loi Ollivier abolit le délit de coalition et reconnaît de fait le droit de grève. Les syndicats ne sont cependant légalisés que le 21 mars 1884 avec la loi Waldeck-Rousseau, qui comporte encore plusieurs restrictions. En particulier, le syndicalisme fut interdit dans la fonction publique. D'un point de vue légal, cette situation perdura jusqu'à la Libération. Toutefois le SNI fut fondé en 1920. 

Entre temps, il existe des syndicats de concile ou de paroisse, à la fin du dix-neuvième siècle.

De nos jours, la création de syndicats de salariés en France est codifiée par les articles L.2131-1 à L.2131-6 du Code du travail.

Selon plusieurs études, un taux élevé de syndicalisation permet de réduire les inégalités de revenus. Les organisations syndicales auraient tendance à privilégier des systèmes de rémunération fondés sur des critères objectifs, attachés aux emplois plutôt qu'aux individus, et à agir contre les discriminations. Selon le Fonds monétaire international (FMI), « en réduisant l’influence des salariés sur les décisions des entreprises », l’affaiblissement des syndicats a permis d’« augmenter la part des revenus constitués par les rémunérations de la haute direction et des actionnaires ».

Le mot renvoie à différentes acceptions :

Alors qu'un syndicat doit dans son principe défendre ses membres, la place d'un syndicat d'employé est régie par des modalités spécifiques en Amérique de Nord. Cette dernière ne définit pas de "représentativité syndicale": les syndicats doivent se faire « accréditer ». L’accréditation introduite aux États-Unis, par le National Labor Act de 1935 (ou loi Wagner / New deal de F.-D. Roosevelt ). Elle a été promue pour favoriser de nouvelles relations sociales dans les sociétés, réduire des rapports sociaux alors considérés comme brutaux et favoriser la négociation collective. Pour être des interlocuteurs incontournables, les syndicats sont tenus de convaincre le « conseil national des relations du travail » de porter 50 % des employés de la société ou d’un soutien majoritaire. Ces conditions permet l’accréditation du syndicat par la commission après enquête.

La notion de syndicat peut exister à différent niveaux: société, branche, régional, national. Un groupe syndical pouvant être membre d'un groupe plus large à travers son appartenance à une union, une fédération ou une confédération.

Au Québec, un syndicat ne doit pas défendre seulement ses membres, mais aussi ses non membres.




</doc>
<doc id="16727" url="https://fr.wikipedia.org/wiki?curid=16727" title="Ludwig von Köchel">
Ludwig von Köchel

Ludwig Alois Friedrich Ritter von Köchel ( – ) est un écrivain, compositeur, botaniste et éditeur autrichien.
Né dans la petite ville de Stein en Basse-Autriche, il étudia le droit à Vienne et fut de 1827 à 1842 le précepteur des quatre fils de l'archiduc Charles d'Autriche.

Outre la musique, il s'intéressait à la botanique, la géologie et la minéralogie. Il fit des recherches dans ces disciplines en Afrique du Nord, dans la péninsule Ibérique, les îles Britanniques, la Laponie et la Russie, qui impressionnèrent les spécialistes de son époque.

Passionné de musique et ayant fait le catalogue de l'œuvre de Wolfgang Amadeus Mozart, ses ouvrages concernant la musique viennoise — une histoire de la musique de cour et la biographie de Johann Joseph Fux (1872) — comptent parmi les ouvrages les plu—s importants du dans ce domaine. Membre du Mozarteum de Salzbourg, il publia en 1862 le "Chronologisch-thematisches Verzeichnis sämtlicher Tonwerke Wolfgang Amadé Mozarts" (« catalogue thématique et chronologique des œuvres de Wolfgang Amadeus Mozart »), plus connu sous le nom de « catalogue Köchel ». Par exemple, la sérénade « Une petite musique de nuit » ("Eine Kleine Nachtmusik") est répertoriée sous le numéro « K. 525 » (Köchel 525).

Ludwig von Köchel est décédé en 1877 à Vienne.

"Chronologisch-thematisches Verzeichnis sämticher Tonwerke  Wolfgang Amadeus Mozart" (Leipzig, 1862 ; rev.2/1905 de P. von Waldersee ; rev.3/1937 de A. Einstein, avec suppl. 1947 ; rev. 6/1964 de S. Giegling, A. Weinmann et G. Sievers [ voir seulement MJb 1971-2, 342-401]

"Ueber den Umfang der musikalischen Produktivität W. A Mozarts" (Salzburg, 1862)

"Drei und achtzig neuaufgefundene Original-Briefe Ludwig von Beethoven an den Erzherzog Rudolf" ( Vienne, 1865)

"Die Pflege der Musik am österreichischen Hofe vom Schlusse des XV. bis zur Mitte des XVIII ? Jahrunderts" (imprimé de façon privé, 1866)

"Die kaiserliche Hof-Musikkapelle in Wien von 1543-1867" (Vienne, 1869/R1976)

"J. J. Fux Hofkompositor und Hofkapellmeister der Kaiser Leopold I, Joseph I, und Karl VI, von 1698-1740" (Vienne, 1872)

C. F. Pohl: ‘Köchel, Dr. Ludwig Ritter von ‘, "Allgemeine deutsche Biographie" (Liepzig, 1875-1912)

A.H. King: ‘ Köchel, Breitkopf, and the Complete Edition[of Mozart]’, "Mozart in Retrospect" (Londres, 1955, rev. 3/1970/"R"1976), 55



</doc>
<doc id="16733" url="https://fr.wikipedia.org/wiki?curid=16733" title="Roi Arthur">
Roi Arthur

Le roi Arthur ou Arthur Pendragon est, d'après les romances médiévales, un seigneur breton qui aurait organisé la défense des peuples celtes des îles Britanniques et de Bretagne armoricaine face aux envahisseurs germaniques à la fin du ou au début du . La légende d'Arthur est principalement inspirée par le folklore et l'invention littéraire, et son existence historique n'est pas attestée. Les sources historiques sont recueillies sur de rares textes contradictoires, essentiellement des poèmes et contes en langue galloise, des annales et chroniques décrivant la romanisation et la christianisation de la Grande-Bretagne comme les "Annales Cambriae" et l’"Historia Brittonum" et la vie des premiers saints de l'île bretonne, comme Gildas le Sage. Le nom d'Arthur apparaît également dans d'anciens poèmes tel que le "Y Gododdin". Son histoire se situe à une époque où le terme « Bretagne » désignait la grande moitié sud de l'actuelle Grande-Bretagne.

La figure légendaire d'Arthur s'est développée essentiellement grâce à l’"Historia regum Britanniae" ("Histoire des rois de Bretagne") écrite par Geoffroy de Monmouth au . Toutefois, antérieurement à cette œuvre, certains contes et poèmes gallois ou bretons, ainsi que des chroniques ou annales reprenant des traditions orales, font déjà apparaître Arthur comme un grand guerrier défendant la Bretagne des hommes et d'ennemis surnaturels ou comme une figure magique du folklore, parfois associée à Annwvyn, "l'autre-Monde" celtique. La part du récit de Geoffroy de Monmouth, écrit encore en latin, adaptée des sources antérieures et celle issue de son imagination sont inconnues.

Bien que les thèmes, les événements et les personnages de la légende du roi Arthur varient considérablement de texte en texte, et qu'il n'existe pas de version unique, les événements contés dans l’"Historia regum Britanniae" ont servi de base pour la plupart des histoires postérieures.

Geoffroy de Monmouth dépeint Arthur comme un roi ayant établi un empire rassemblant toute l'île de Bretagne, ainsi que l'Irlande, l'Islande, la Norvège, le Danemark et une bonne partie de la Gaule. En fait, beaucoup d'éléments qui font désormais partie intégrante de l'histoire du roi Arthur apparaissent dans l’"Historia regum Britanniae" : le père d'Arthur Uther Pendragon, Merlin l'Enchanteur, l'épée Excalibur, la naissance d'Arthur à Tintagel, sa dernière bataille contre Mordred à Camlann et sa retraite finale à Avalon. Au , l'écrivain français Chrétien de Troyes y ajoute Lancelot du Lac et le Saint Graal et initie le genre de la romance arthurienne (en puisant dans la matière de Bretagne) qui devient un volet important de la littérature médiévale. Dans ces histoires, la narration se concentre souvent sur d'autres personnages, tels que les différents chevaliers de la Table ronde au lieu de se focaliser sur le roi Arthur. La littérature arthurienne a prospéré pendant le Moyen Âge, avant de perdre de l'importance dans les siècles qui suivent. Elle est redevenue un sujet à la mode depuis le . Au , le roi Arthur est toujours un personnage mis en scène, à la fois dans la littérature mais aussi dans les adaptations scéniques (festivals, spectacles vivants), au théâtre, au cinéma, à la télévision, dans les bandes dessinées, et d'autres médias.

Le prénom Arthur était en rapport étymologique avec le nom celtique de l'ours, "artos" signifiant à la fois "ours" et "guerrier". On rapproche son nom avec celui de la déesse ourse Artio. Arthur s'expliquerait par *Arto-rix "roi-ours" ("roi des guerriers ?") par un intermédiaire latinisé *"Artori"("u")"s".

Le nom lui-même revêt un symbole de force, de stabilité et de protection, caractères bien présents dans sa légende : c'était un homme réputé fort, posé, et, en tant que roi, garant de la sécurité de ses sujets. Dans la civilisation celtique, l'ours est avant tout l'animal emblématique de la royauté.

Notons néanmoins qu'à l'époque où naît la légende arthurienne (), la place de l'ours comme animal emblématique est prise par le lion. Ainsi dans l"'Historia Regum Britanniae", Arthur rêve à un combat entre un ours et un dragon. Mais Arthur est le dragon, et non l'ours.

La transcription latine basée sur cette racine celtique donnerait le nom "Artorius", ce qui appuierait l'hypothèse romaine identifiant le roi Arthur au personnage de Lucius Artorius Castus. Néanmoins l'assimilation d'Arthur à Artorius repose sur des bases très fragiles.

Les défenseurs de l'hypothèse galloise constatent que le roi Arthur apparaît pour la première fois dans les légendes et élégies galloises, bien avant d'être repris dans les romans de chevalerie du .

Arthur serait né vers 470/475 et serait originaire du Pays de Galles, ou de l'ouest de l'Angleterre, mais l'emplacement exact de sa cour, connue sous le nom de Camelot, reste un mystère. Il aurait repoussé l'invasion des Saxons au début du bien qu'il n'ait jamais été couronné roi. En effet, la chronique de Nennius () le désigne comme un "dux bellorum" (chef de guerre) combattant « avec les rois bretons » et les textes médiévaux en gallois ne lui donnent jamais le titre de roi, mais l'appellent "amerauder" (« empereur »). 

Certains auteurs en feraient un grand propriétaire terrien romanisé ayant constitué, comme c'était alors courant, sa propre troupe de "bucellaires", mercenaires à la solde d'une personne riche et payés en nourriture, d'où leur nom ("buccelus" = biscuit), et ayant prêté main-forte aux rois bretons contre les Saxons. En outre, dès le , les corps de bucellaires sont constitués majoritairement de cavaliers. La légende d'un corps de cavaliers d'élite servant Arthur n'est pas loin…

Kemp Malone, a estimé avoir retrouvé le vrai Arthur dans le personnage de Lucius Artorius Castus. La parenté de nom est en effet assez troublante. Ce préfet romain, installé à York, a commandé (l'épigraphie l'atteste) la Légion "Victrix", chargée de combattre les Calédoniens (peuple de l'actuelle Écosse) au-delà du mur d'Hadrien. Il a remporté contre eux (et non contre les Saxons) une suite de victoires entre 183 et 185 Ensuite, il aurait été envoyé en Armorique mater une rébellion; cependant de récentes recherches tendent à prouver qu'il aurait été envoyé en Arménie. Lors de cette expédition, il portait le titre de "dux", ce qui n'est pas sans rappeler le titre de "dux bellorum" rapporté par la chronique de Nennius.

Selon Geoffrey Ashe, reprenant la thèse de Léon Fleuriot, le légendaire Arthur est inspiré du personnage réel de Riothamus, qui aurait porté le titre de « roi des Bretons » entre 454 et 470. Celui-ci aurait fait campagne en Gaule au cours des années 468 et 469 pour appuyer les Gallo-romains contre les Wisigoths, avant d'être battu par ces derniers à la bataille de Déols.

Plus récemment, C. Scott Littleton et Linda A. Malcor ont repris ces deux dernières hypothèses et affirment que le Arthur de Camelot est la synthèse du Romain Lucius Artorius Castus et du Britannique Riothamus. Pour ces deux chercheurs, le nom d'Arthur est la « celticisation » d'Artorius. Mais ce dernier, personnage assez mineur dans l'Histoire de Bretagne, ne peut plus être considéré comme le modèle du roi Arthur.

Ainsi, il apparaît que certains auteurs médiévaux ont voulu réécrire l'histoire transformant en victoire la défaite essuyée par les Bretons lors de la bataille de Déols. Après s'être rendu maître de toute l'île de Bretagne, Arthur aurait ainsi conquis l'Irlande, l'Islande, la Norvège, le Danemark et une bonne partie de la Gaule. Il aurait même vaincu les légions romaines en Burgondie (Bourgogne), au cours d'une expédition qui l'aurait mené jusqu'à Rome…

On peut également évoquer l'hypothèse du décalage chronologique. Dans ce cas, la bataille de Camlann contre l'usurpateur "Mordred" aurait eu lieu vers 490, alors qu'Arthur revenait de son expédition en Gaule, où il serait allé prêter main-forte aux troupes gallo-romaines confrontées à l'invasion franque. Dans le cadre de cette hypothèse, une nouvelle datation peut être proposée : la bataille du Mont Badon doit être placée dans la chronologie aux environs des années 475 et l'arrivée des Saxons en Bretagne aux environs de 428.

Le nom de Jules César apparaît dans la légende arthurienne. Il faut peut-être définir le terme de césar comme un titre, correspondant à la fonction de empereur en second assumée par le dernier empereur d'Occident Julius Nepos reconnu en Orient. Cette mise au point permettrait de mettre un terme au thème de Merlin l'Enchanteur, "capable de traverser le temps et l'espace", parce qu'il est ainsi avéré qu'il existait bien un dénommé Jules, « "césar" » de son état, contemporain de la jeunesse du personnage connu aujourd'hui sous le nom du "Roi Arthur", mais qui reste encore à identifier parmi les responsables britto-romains de son époque.

Selon l'historienne Norma Goodrish, la tombe d'Arthur datant du se trouve dans la Civil parish (la "commune") d' où s'est déroulée la Bataille d'Arfderydd près du mur d'Hadrien, région dans laquelle "Lucius Artorius Castus" défendait le "limes" romain.

Le patronyme « Arthur » pouvait être courant à l'époque celtique et aurait pu ainsi désigner plusieurs chefs. L'amalgame du récit de différentes vies aurait pu servir à constituer celle du personnage mythologique. Ce nom connaît d'ailleurs une vogue très importante dans l'aristocratie celtique dans les années qui suivent la Bataille de Camlann, où serait mort Arthur, entre 537 et 542.

Pour Withaer, auteur d'une histoire des guerres de ce prince, Arthur fut le dernier roi des Bretons siluriens.

Après avoir défendu longtemps son pays avec succès contre les Angles du nord, les Saxons de l'occident et les Danois qu'il vainquit en douze batailles successives, il aurait été complètement défait à Camlann, vers 542.
Blessé mortellement, il se fit transporter en un lieu inconnu où il termina sa glorieuse vie. Ses soldats étonnés de ne pas le voir reparaître allèrent à sa recherche et, comme ils ne trouvèrent nulle part son tombeau, ils se persuadèrent qu'il n'était pas mort. Bientôt, se répandit la croyance populaire qu'Arthur reviendrait un jour régner sur la Bretagne affranchie du joug étranger, et qu'il y ramènerait le siècle d'or. Les chants patriotiques des bardes le représentaient tantôt guerroyant en Palestine contre les Infidèles, et tantôt errant dans les forêts des deux Bretagnes. Cette espérance du retour d'Arthur s'accrut à mesure que le peuple était opprimé.

Henri II, à qui elle inspirait de vives inquiétudes, imagina un moyen pour la faire cesser. Il se rendit à Glassenbury (ou Glastonbury), où des moines de l'abbaye annoncèrent avoir découvert la tombe d'Arthur et de Guenièvre quelques années après l'incendie de leur église en 1184. Sa reconstruction nécessitait des fonds importants, d'où l'idée des moines, selon l'érudit britannique le docteur Robert Dunning, de broder à partir d'une supposée tombe royale toute une légende autour d'Arthur, de Joseph d'Arimathie, du Saint-Graal ou du chevalier Lancelot, en s'inspirant des écrits de leur évêque Geoffroy de Monmouth. Cette légende ne manquerait pas ainsi d'attirer d'importants donateurs et d'accroître sa renommée par rapport à sa rivale l'Abbaye de Saint Denis. Ces fouilles furent faites en un lieu que des vers chantés par un pâtre indiquaient comme l'endroit de la sépulture d'un grand homme. Giraud de Barri, aumônier du roi Henri II Plantagenêt rapporte qu'on en retira, parmi divers débris, un cercueil de pierre décoré d'une petite croix de plomb, sur laquelle était inscrit : 



D'autres pensent qu'Arthur serait un demi-dieu celte incarné, tel que le dieu de la mer Lir (supposé incarné par le Roi Lear), ou même un personnage fictif comme Beowulf ("loup des abeilles", un surnom de l'ours). Cette théorie serait renforcée par le fait que d'autres Britanniques de cette période, comme Ambrosius Aurelianus, ont combattu les Saxons à la bataille du Mont Badonicus.


Le roi Arthur est apparu pour la première fois dans la littérature galloise. Dans le premier poème gallois retrouvé, le "Y Gododdin", Aneirin (vers 575-600) écrit au sujet d'un de ses personnages qu'« il nourrissait des corbeaux noirs sur les remparts, alors qu'il n'était pas Arthur » (en gallois : « "Gochorai brain du fur caer/ Cyn ni bai ef Arthur." », traduit en anglais par « "he fed black ravens on the ramparts, although he was not Arthur" »). Mais ce poème composé d'une série d'épopées élégiaques peut être interprété de différentes manières.

Une autre ancienne référence au roi Arthur se trouve dans l"'Historia Brittonum" attribuée au moine gallois Nennius, qui aurait écrit cette "Histoire galloise" vers 830. Le roi Arthur est décrit comme un « chef de guerre » plutôt que comme un roi.

Le roi Arthur apparaît aussi dans l'histoire galloise "Kulhwch et Olwen", habituellement associé avec les Mabinogion.

Les dernières parties du texte les "Triades galloises" font mention d'Arthur et situent sa cour à Celliwig en Cornouailles. Celliwig serait l'actuelle Callington ou Kelly Rounds, une colline fortifiée près d'Egloshayle.

Le roi Arthur (ou roi Artus) est aussi parfois décrit comme le chef des chasses fantastiques, comme la Chasse-galerie (un groupe de chasseurs mythiques), non seulement dans les Îles britanniques, mais aussi dans toute l'Europe occidentale.

En 1133, Geoffroy de Monmouth écrivit son "Historia Regum Britanniae". Ce livre fut l'équivalent d'un "best seller" médiéval, et attira l'attention d'autres écrivains, tels que Wace et Layamon, sur ces histoires. Ces écrivains en profitèrent pour améliorer les histoires du roi Arthur.

Même si de nombreux érudits s'accordent sur le fait que Geoffroy a suscité l'intérêt médiéval pour le roi Arthur, une autre hypothèse existe. Les histoires concernant Arthur pourraient venir des traditions orales bretonnes, disséminées dans les cours royales et de la noblesse d'Europe grâce aux jongleurs professionnels. L'écrivain médiéval français Chrétien de Troyes raconta des histoires provenant de cette mythologie à la moitié du , de même que Marie de France dans ses "lais", des poèmes narratifs. Les histoires provenant de ces écrivains et de beaucoup d'autres seraient indépendantes de Geoffroy de Monmouth.

Ces histoires, réunies sous le vocable de matière de Bretagne, devinrent populaires à partir du . Dans ces histoires, Arthur rassembla les chevaliers de la Table ronde (en particulier Lancelot, Gauvain et Galaad). Cette assemblée était en général située à Camelot dans les derniers récits. Le magicien Merlin, dit « l'Enchanteur », y participait de temps en temps. Ces chevaliers participèrent à des quêtes mythiques, comme celle du Saint Graal. D'autres histoires du monde celtique s'associèrent à la légende d'Arthur, telle que la légende de Tristan et Iseut. Dans les dernières légendes, la romance entre le champion d'Arthur, Lancelot, et la reine Guenièvre devint la cause principale de la chute du monde arthurien.

Robert de Boron écrivit dans son "Merlin" qu'Arthur obtint son trône en tirant une épée d'un rocher et d'une enclume. Cet acte ne pouvait être effectué que par le Vrai Roi, ce qui signifie le roi choisi par (les) Dieu(x), ou l'héritier d'Uther Pendragon. Cette épée est dans certaines versions la célèbre Excalibur. Dans d'autres récits, Excalibur sort d'un lac, portée par Viviane, la Dame du Lac - une demoiselle sorcière - et est remise à Arthur peu de temps après le début de son règne. L'épée pouvait trancher n'importe quoi, et son fourreau rendait son porteur invincible.

Le dernier combat d'Arthur, la bataille de Camlann, contre les forces de Mordred vit sa perte. Des histoires montrent que Mordred était un chevalier de la Table ronde et le fils incestueux d'Arthur et de sa sœur Morgane ou bien de sa demi-sœur Morgause. Le Roi Arthur fut mortellement blessé lors de cette bataille, et emmené à Avalon. Là, ses mains furent soignées ou son corps enterré dans une chapelle. D'autres textes disent qu'il n'est pas mort, mais qu'il s'est retiré dans Avalon, monde insulaire mystérieux ; le roi Arthur est en "dormition" et reviendra un jour. De nombreux lieux sont revendiqués comme étant l’Avalon dont parle la légende : Glastonbury (dans le Somerset, en Angleterre), l'île d'Avalon (un îlot sur la commune de Pleumeur-Bodou dans les Côtes-d'Armor), Burgh by Sands, ancienne forteresse Aballaka du Mur d'Hadrien, en Cumberland, à l'embouchure de l'Eden… Mais il faut préciser que les peuples celtiques transportent leurs légendes et les transposent au fur et à mesure de leurs émigrations. Ceci explique donc qu'il y ait plusieurs forêts de Brocéliande, plusieurs Cornouailles…

La légende du roi Arthur s'est répandue dans toute l'Europe. Des images d'Arthur ont été retrouvées à de nombreux endroits. En particulier, dans la cathédrale de Modène en Italie, une gravure datée entre 1099 et 1120 représente Arthur et ses chevaliers attaquant un château. Une mosaïque de 1165 dans la cathédrale d'Otrante, près de Lecce, en Italie contient la représentation curieuse d"'Arturus Rex" portant un sceptre et chevauchant une chèvre. Des marchands du baptisèrent un Hall arthurien à Gdańsk, en Pologne. De nombreux lieux évoquent le roi Arthur en Bretagne, notamment la forêt de Brocéliande ou la Grotte Art en forêt de Huelgoat ou encore Glastonbury.
Le roi unique et incontesté n'a jamais existé dans la civilisation celtique. Les divisions tribales (chefs de clans vassaux de rois des provinces eux-mêmes vassaux d'un roi suprême) ont permis à Jules César de prendre le contrôle de la Gaule. En contrepartie, l'imaginaire populaire s'est emparé d'un roi, plus ou moins attesté, paré des atouts les plus nobles de sa charge : un homme fort, bon guerrier mais sage, fédérateur et bien conseillé. Même après sa disparition, il porte encore les espoirs d'un peuple : son sommeil n'est que temporaire, et il reviendra unir les « deux Bretagnes » et sauver les Bretons.

En 1066, Guillaume le Conquérant s’impose en maître de l’Angleterre… Mais comment faire accepter un Normand comme roi, alors qu'il est issu d'un peuple minoritaire ? En s’appuyant sur la légende arthurienne et sur Arthur, sa figure de proue, unificateur de la Grande-Bretagne et du peuple breton. Car sur le continent se trouvent les descendants de Bretons partis de l'île quelques siècles plus tôt. Pour monter son armée, Guillaume a utilisé les services d'un certain nombre de nobles descendants de ces Bretons émigrants. En favorisant la diffusion du mythe de la survivance d’Arthur, de sa dormition dans l’île d’Avalon et de son retour prochain, Guillaume rendait populaire sa lutte contre les Angles et les Saxons et comptait bien se rallier les Gallois. Ce fut le début de « l’espoir breton ».

De même, Henri II Plantagenêt se servit du mythe arthurien pour asseoir son pouvoir, maintenir son autorité et unifier l’île de Bretagne. Couronné en 1154 après moult difficultés (petit-fils d'Henri, désigné comme successeur mais écarté du trône par le neveu du roi défunt), il confisque la légende à son profit. Afin d’estomper les origines non-anglaises de la dynastie des Plantagenêt, Henri II préférera s’appuyer sur la civilisation bretonne en se présentant comme le digne successeur d’Arthur, bel et bien mort lors de l’ultime bataille. Car le monarque doit affirmer son autorité : vassal du roi de France pour le duché de Normandie, il a besoin du soutien breton contre les revendications des Saxons, qui ont du mal à accepter la domination normande sur l’Angleterre. Afin de renforcer cette analogie, il tente même sans succès de conquérir l’Irlande et l’Écosse afin de réunir sous sa bannière l’ensemble du royaume supposé d’Arthur.

Arthur a aussi beaucoup servi pendant la Seconde Guerre mondiale chez les Britanniques pour raviver les efforts de la population face au risque d'invasion de l'Allemagne nazie.

Dans l'imaginaire en Bretagne continentale, il représente l'unité du peuple breton, puisqu'il était roi des deux Bretagnes. Les auteurs du Moyen Âge l'ont actualisé selon les canons courtois de l'époque en en faisant un modèle de noblesse et de vertu chrétienne.

Arthur est le fils d'Uther Pendragon, roi des Bretons, et d'Igraine (ou Ygerne), veuve de Gorlois (ou Gorlais), duc des Cornouailles.
Il est le frère d’Anna (Morgause), épouse du roi Loth d'Orcanie, et aussi de la fée Morgane.
Il épouse Guenièvre, fille de Léodagan, roi de Carmélide.
Certaines œuvres lui attribuent la paternité de Lohot, et de Mordred, né d'une relation incestueuse avec sa demi-sœur.

La vie des chevaliers de la Table Ronde est antérieure à l'apparition de l'héraldique, mais la notoriété du roi Arthur lui a valu l'attribution d'armes qui, du fait de l'anachronisme, relèvent des armoiries imaginaires.

Hiérosme de Bara, dans son ouvrage "Le Blason des Armoiries", énonce ceci : « De Artus, qu'on dit avoir été roy de la grande Bretaigne, quelques-uns ont dit qu'il n'avoit que trois couronnes, les autres six, un autre neuf, tantost mises en triangle maintenant en pal, et ainsi diversement. »

Concernant la variante à 13 couronnes, .

C'est l'un des Neuf Preux des romans de chevalerie du .

Le personnage du roi Arthur a été l'objet de nombreuses fictions, aussi bien au cinéma qu'à la télévision, sous forme de téléfilms ou de séries télévisées. Il a de plus inspiré de nombreux créateurs contemporains, que ce soit sous la forme de comédies musicales, de jeux vidéo ou de bandes dessinées.











</doc>
<doc id="16734" url="https://fr.wikipedia.org/wiki?curid=16734" title="Laser">
Laser

Un laser (acronyme de l'anglais "") est un système photonique. Il s'agit d'un appareil qui produit un rayonnement lumineux spatialement et temporellement cohérent basé sur l'effet laser. Descendant du maser, le laser s'est d'abord appelé maser optique.

Une source laser associe un amplificateur optique basé sur l'effet laser à une cavité optique, encore appelée résonateur, généralement constituée de deux miroirs, dont au moins l'un des deux est partiellement réfléchissant, c'est-à-dire qu'une partie de la lumière sort de la cavité et l'autre partie est réinjectée vers l'intérieur de la cavité laser. Avec certaines longues cavités, la lumière laser peut être extrêmement directionnelle. Les caractéristiques géométriques de cet ensemble imposent que le rayonnement émis soit d'une grande pureté spectrale, c’est-à-dire temporellement cohérent. Le spectre du rayonnement contient en effet un ensemble discret de raies très fines, à des longueurs d'ondes définies par la cavité et le milieu amplificateur. La finesse de ces raies est cependant limitée par la stabilité de la cavité et par l'émission spontanée au sein de l'amplificateur (bruit quantique). Différentes techniques permettent d'obtenir une émission autour d'une seule longueur d'onde.

Au , le laser est plus généralement vu comme une source possible pour tout rayonnement électromagnétique, dont fait partie la lumière. Les longueurs d'ondes concernées étaient d'abord les micro-ondes (maser), puis elles se sont étendues aux domaines de l'infrarouge, du visible, de l'ultraviolet et commencent même à s'appliquer aux rayons X.

Le principe de l’émission stimulée (ou émission induite) est décrit dès 1916 par Albert Einstein. En 1950, Alfred Kastler (lauréat du prix Nobel de physique en 1966) propose un procédé de pompage optique, qu'il valide expérimentalement, deux ans plus tard, avec Brossel et Winter. Mais ce n'est qu'en 1953 que le premier maser (au gaz ammoniac) est conçu par J. P. Gordon, H. J. Zeiger et Ch. H. Townes. Au cours des années suivantes, de nombreux scientifiques tels N. G. Bassov, Alexandre Prokhorov, Arthur Leonard Schawlow et Charles H. Townes contribuent à adapter ces théories aux longueurs d'ondes du visible. Townes, Bassov, et Prokhorov partagent le Prix Nobel de Physique en 1964 pour leurs travaux fondamentaux dans le domaine de l'électronique quantique, qui mènent à la construction d'oscillateurs et d'amplificateurs basés sur le principe du Maser-Laser. En 1960, le physicien américain Théodore Maiman obtient pour la première fois une émission laser au moyen d'un cristal de rubis. Un an plus tard, Ali Javan met au point un laser au gaz (hélium et néon) puis en 1966, Peter Sorokin construit le premier laser à liquide.

Les lasers trouvent très tôt des débouchés industriels. La première application fut réalisée en 1965 et consistait à usiner un perçage de de diamètre et de de profondeur dans du diamant avec un laser à rubis. Cette opération était réalisée en 15 min, alors qu’une application classique prenait 24 heures.

En 1963 des chercheurs américains tels que White et Anderholm montrent qu’il est possible de générer une onde de choc à l’intérieur d'un métal à la suite d'une irradiation laser impulsionnelle. Les pressions exercées sont de l’ordre de 1 GPa, ou 3 FPs.

En 1967, Peter Holcroft découpe une plaque d’acier inoxydable de d'épaisseur à une vitesse de /min, sous dioxygène avec un laser CO de et conçoit la première tête de découpe.

Bien que les procédés soient démontrés, il faut attendre leurs associations à des machines adaptées pour qu’ils soient implantés en milieu industriel. Ces conditions sont remplies à la fin des années 1970. Et les premières plates-formes industrielles sont implantées en France dès les années 1980. Dès lors le laser s'impose comme un outil de production industrielle dans le micro-usinage. Ses principaux avantages sont un usinage à grande vitesse de l'ordre de /min, sans contact, sans usure d'outil.

Le laser devient un moyen de lecture en 1974, avec l'introduction des lecteurs de codes barres. En 1978, les "laserdiscs" sont introduits, mais les disques optiques ne deviennent d'usage courant qu'en 1982 avec le disque compact. Le laser permet alors de lire un grand volume de données.

Pour comprendre comment fonctionne un laser, il est nécessaire d'introduire le concept de "quantification de la matière" : les électrons sont répartis sur des niveaux d'énergie discrets (les « couches »). Cette hypothèse est "fondamentale" et "non intuitive" : si l'on considère l'image selon laquelle les électrons ne peuvent se trouver que sur certaines orbitales bien précises autour du ou des noyaux atomiques.

Dans la suite, on considérera un atome ne possédant qu'un électron (hydrogène), pour simplifier la discussion. Celui-ci est susceptible de se trouver sur plusieurs niveaux. La connaissance du niveau sur lequel se trouve cet électron définit l"'état de l'atome". Ces états sont numérotés par ordre croissant d'énergie avec un nombre entier formula_1, pouvant prendre les valeurs formula_2, formula_3, ... L'état formula_4 est donc l'état d'énergie la plus basse, correspondant à un électron sur l'orbitale la plus proche du noyau.

Venons-en aux principaux processus d'interaction entre la lumière et la matière, à savoir l'absorption, l'émission stimulée et l'émission spontanée.

Considérons un ensemble d'atomes à deux niveaux. Si on envoie un champ sur un ensemble d'atomes dans l'état "haut", le phénomène privilégié sera l'émission stimulée et le champ sera amplifié. Pour réaliser un amplificateur optique, il faut donc trouver le moyen d'exciter les atomes vers l'état d'énergie supérieure.
De façon plus générale, si certains atomes sont dans l'état fondamental "bas", des photons peuvent être également absorbés, ce qui diminue l'intensité du champ. Il n'y aura amplification que si les atomes sont plus nombreux à être dans l'état "haut" (susceptible d'émettre) que dans l'état "bas" (susceptible d'absorber) : il est nécessaire d'avoir une « inversion de population ».

Cependant, à l'équilibre thermodynamique, l'état le plus bas est toujours le plus peuplé. Au mieux, les populations oscillent entre les deux niveaux (Oscillations de Rabi). Pour maintenir une inversion de population, il est nécessaire de fournir constamment un apport d'énergie extérieure aux atomes, pour ramener dans l'état supérieur ceux qui sont repassés dans l'état fondamental après l'émission stimulée : c'est le « pompage ». Les sources d'énergie extérieures peuvent être de différents types, par exemple un générateur électrique, ou un autre laser (pompage optique).
L'amplificateur est donc un ensemble d'atomes ou molécules que l'on fait passer d'un état fondamental ou faiblement excité formula_1 à un état plus fortement excité formula_10, au moyen d'une source d'énergie extérieure (pompage). Ces atomes peuvent alors se désexciter vers l'état formula_1, en émettant des photons de fréquence proche de formula_18. Ainsi un rayonnement de fréquence formula_28 passant à travers ce milieu peut être amplifié par des processus d'émission stimulée.

Pour obtenir les équations détaillées de l'effet Laser puis de la cavité Laser elle-même, il est nécessaire de faire appel de manière plus quantitative à la physique quantique. Il existe alors deux degrés de quantification dans l'interaction lumière (faisceau laser)/matière (atomes de la cavité), qui chacun, permettent de mieux comprendre la physique de l'effet laser : 

Le modèle semi-classique permet à lui-seul de comprendre d'où vient l'effet Laser et d'obtenir les "équations de taux" qui régissent les populations d'atomes au sein de la cavité Laser.

Les atomes étant quantifiés, le formalisme de la mécanique hamiltonienne est nécessaire. Dans l'approximation d'un système à deux niveaux d'énergie pour les atomes, l'effet du champ électrique extérieur (la lumière, considérée comme monochromatique de pulsation formula_7) consiste en des Oscillations de Rabi des atomes entre ces deux niveaux.

Ces oscillations engendrées par la lumière sont la conséquence directe de la compétition entre l'émission stimulée et le phénomène d'absorption décrits plus haut, et sont décrites par la probabilité pour un atome présent en formula_30 de passer de formula_31 au temps formula_32 à formula_33 au temps t :

formula_34
où
formula_35, avec I, l'intensité du champ électrique incident et d, la valeur du dipôle atomique ;

et 
formula_36 avec 
formula_37

Ainsi, ce modèle semi-classique ne permet pas d'obtenir l'inversion de population nécessaire à l'effet laser : ces oscillations sinusoïdales montrent que le système "ne choisit pas" entre l'émission stimulée et l'absorption.

Si l'on veut expliquer l'effet laser tout en gardant ce modèle semi-classique, il faut donc introduire de manière "ad hoc" l'émission spontanée qui ne peut être expliquée sans seconde quantification.

Dans le cas d'un modèle de transition entre 2 niveaux bas et haut, notés respectivement formula_38 et formula_39, la population de l'état haut doit être supérieure à la population de l'état bas pour qu'il y ait emission : formula_40.

L'évolution de la population de l'état haut est donnée par une loi de décroissance exponentielle : formula_41.

Pour un matériau donné, la différence de population entre l'état haut et l'état bas formula_42 donne le caractère du milieu vis-à-vis du pompage optique : si formula_43 le milieu est amplificateur, si formula_44 le milieu est absorbant et dans le cas formula_45 le milieu est transparent. Un laser ne "lase" que dans le cas où le milieu est amplificateur.

Un laser est donc, fondamentalement, un "amplificateur" de lumière dont la sortie est réinjectée à l'entrée. Son alimentation en énergie est la source du pompage, la sortie est le rayonnement laser qui est réinjecté à l'entrée par les miroirs de la cavité résonnante, le mécanisme de l'amplification étant l'émission stimulée.

On peut comparer ce processus à l'effet Larsen, qui se produit lorsqu'un amplificateur (la chaîne HiFi) voit sa sortie (le haut-parleur) « branchée » sur l'entrée (le micro). Le moindre bruit capté par le micro est amplifié, émis par le haut-parleur, capté par le micro, ré-amplifié jusqu'à la saturation du système (quand celui-ci fournit l'énergie maximum possible de par sa conception). Dans un laser, cette énergie maximale est limitée par la puissance de la source de pompage, et par le nombre d'atomes qui peuvent être simultanément excités.

Dans l'effet Larsen, la fréquence du son produit dépend du spectre des fréquences amplifiées correctement par l'amplificateur et du temps que met le son pour parcourir la boucle sonore (qui n'est pas une valeur unique étant donné que le local induit diverses réflexions et des trajets sonores de longueur différente). Dans un laser, il se passe la même chose si ce n’est que le spectre de l'amplificateur n'est pas une plage le plus plate possible mais est restreint aux bandes de fréquences correspondant aux niveaux d'excitation des différents atomes présents, et la boucle correspond à la longueur de la cavité résonante.

Le gain d'un laser à formula_46 miroirs notés formula_47 de coefficients de réflexion respectifs formula_48 contenant un matériau amplificateur pompé de gain formula_49 est donné par l'évolution de l'intensité dans la cavité itération après itération. Si à un instant formula_32 l'intensité dans la cavité vaut formula_51 alors après un tour de cavité l'intensité vaut formula_52

On peut alors distinguer 3 cas selon la valeur de formula_53 : 

Une cavité laser est considérée stable si le front d'onde peut se propager sans déformation.
Dans le cas d'une cavité laser à 2 miroirs, les conditions de stabilité d'un laser sont liées à la distance formula_57 entre les miroirs de la cavité par rapport aux rayons de courbure des 2 miroirs. Pour 2 miroirs de rayons de courbure respectifs formula_58 et formula_59 avec formula_60, pour un faisceau gaussien :

Les équations de taux (Rate equations en anglais) désignent des équations de conservation de population des états haut et bas respectivement. Elles établissent que la variation de la population d'un état correspond à la différence entre la quantité d'atomes qui rejoint cet état et la quantité d'atomes qui changent d'état.

Dans le cas particulier d'un système à 2 niveaux, pour les deux états haut et bas (formula_39 et formula_38), en considérant un terme d'émission spontanée de probabilité formula_67, et un terme de pompe de probabilité formula_68 où formula_69 désigne la section illuminée par le flux de pompe et formula_70 désigne l'intensité du flux de pompe, alors :

formula_71

et formula_72

On classe les lasers selon six familles, en fonction de la nature du milieu excité. Par ailleurs, les lasers peuvent être aussi bien continus que fonctionner dans un régime impulsionnel, auquel cas on pourra les qualifier également selon la durée caractéristique de leurs impulsions (lasers continus / lasers picosecondes / lasers femtosecondes).

Ces lasers utilisent des milieux solides, tels que des cristaux ou des verres comme milieu d'émission des photons. Le cristal ou le verre n'est que la matrice et doit être dopé par un ion qui est le milieu laser. Le plus ancien est le laser à rubis dont l'émission provient de l'ion Cr. D'autres ions sont très utilisés (la plupart des terres rares : Nd, Yb, Pr, Er, Tm..., le titane et le chrome, entre autres). La longueur d'onde d'émission du laser dépend essentiellement de l'ion dopant, mais la matrice influe aussi. Ainsi, le verre dopé au néodyme n'émet pas à la même longueur d'onde () que le YAG dopé au néodyme (). Ils fonctionnent en continu ou de manière impulsionnelle (impulsions de quelques microsecondes à quelques femtosecondes—millionième de milliardième de seconde). Ils sont capables d'émettre aussi bien dans le visible, le proche infrarouge que l'ultraviolet.

Le milieu amplificateur peut être un barreau dans le cas d'un laser Nd-YAG (donc dopé au Nd et la matrice est du YAG : un grenat d'aluminium et d'yttrium), mais il peut aussi se présenter sous la forme d'une fibre dans le cas des lasers à fibre (donc dopé au Yb et la matrice est en silice). Aujourd'hui, le milieu amplificateur le plus utilisé pour générer des impulsions femtosecondes est le saphir dopé titane. Il possède deux bandes d'absorption centrées à 488 et . Il possède un large spectre d'émission centré à .

, ces lasers permettent d'obtenir des puissances de l'ordre du kW en continu et du GW en pulsé. Ils sont utilisés pour des applications tant scientifiques qu'industrielles, en particulier pour la soudure, le marquage et la découpe de matériaux.

Ce type de laser ressemble au laser solide. Ici le milieu amplificateur est une fibre optique dopée avec des ions de terres rares. La longueur d'onde obtenue dépend de l'ion choisi (Samarium ; Ytterbium ; Erbium ; Thulium ; Holmium ). Cette technologie est relativement récente (le premier date de 1964), mais il existe aujourd'hui des lasers monomodes dont la puissance est de l'ordre de la dizaine de kilowatts. Ces lasers ont l'avantage de coûter moins cher, de posséder un encombrement réduit et d'être résistants aux vibrations. Par ailleurs il n'est pas nécessaire de les refroidir en dessous de .

Dans les lasers à liquide, le milieu d'émission est un colorant organique (rhodamine 6G par exemple) en solution liquide enfermé dans une fiole de verre. Le rayonnement émis peut aussi bien être continu que discontinu suivant le mode de pompage. Les fréquences émises peuvent être réglées à l'aide d'un prisme régulateur, ce qui rend ce type d'appareil très précis. Le choix du colorant détermine essentiellement la gamme de couleur du rayon qu'il émettra. La couleur (longueur d'onde) exacte peut être réglée par des filtres optiques.

Le milieu générateur de photons est un gaz contenu dans un tube en verre ou en quartz. Le faisceau émis est particulièrement étroit et la fréquence d'émission est très peu étendue. Les exemples les plus connus sont les lasers à hélium-néon (rouge à ), utilisés dans les systèmes d'alignement (travaux publics, laboratoires), et les lasers pour spectacles.

Les lasers à dioxyde de carbone sont capables de produire de très fortes puissances (fonctionnement en impulsion) de l'ordre de 10 W. C'est le marquage laser le plus utilisé dans le monde. Le laser CO (infrarouge à ) peut être, par exemple, utilisé pour la gravure ou la découpe de matériaux.

Il existe aussi une sous-famille des lasers à gaz : les lasers excimers qui émettent dans l'ultraviolet. Dans la majorité des cas, ils sont composés d'au moins un gaz noble et habituellement d'un gaz halogène.

Le terme « excimer » vient de l'anglais "excited dimer" qui signifie une molécule excitée composée de deux atomes identiques (ex. : Xe). Or certains lasers dits excimères utilisent des exciplexes qui sont des molécules composées de deux atomes différents (par exemple, gaz noble et halogène : ArF, XeCl). On devrait donc les nommer lasers exciplexes plutôt que lasers excimères.

L'excitation électrique du mélange produit ces molécules exciplexes qui n’existent qu'à l'état excité. Après émission du photon, l'exciplexe disparait car ses atomes se séparent, donc le photon ne peut être réabsorbé par l'excimer non excité, ce qui permet un bon rendement au laser.

Exemple : Lasik

Dans une diode laser (ou laser à semi-conducteur), le pompage se fait à l'aide d'un courant électrique qui enrichit le milieu générateur en trous (un trou est une zone du cristal avec une charge positive car il manque un électron) d'un côté et en électrons supplémentaires de l'autre. La lumière est produite au niveau de la jonction par la recombinaison des trous et des électrons. Souvent, ce type de laser ne présente pas de miroirs de cavité : le simple fait de cliver le semi-conducteur, de fort indice optique, permet d'obtenir un coefficient de réflexion suffisant pour déclencher l'effet laser.

C'est ce type de laser qui représente l'immense majorité (en nombre et en chiffre d'affaires) des lasers utilisés dans l'industrie. En effet, ses avantages sont nombreux : tout d'abord, il permet un couplage direct entre l'énergie électrique et la lumière, d'où les applications en télécommunications (à l'entrée des réseaux de fibres optiques). De plus, cette conversion d'énergie se fait avec un bon rendement (de l'ordre de 30 à 40 %). Ces lasers sont peu coûteux, très compacts (la zone active est micrométrique, voire moins, et l'ensemble du dispositif a une taille de l'ordre du millimètre). On sait maintenant fabriquer de tels lasers pour obtenir de la lumière sur quasiment tout le domaine visible, mais les lasers délivrant du rouge ou du proche infrarouge restent les plus utilisés et les moins coûteux . Leurs domaines d'applications sont innombrables : lecteurs optiques (CD), télécommunications, imprimantes, dispositifs de « pompage » pour de plus gros lasers (de type lasers à solide), pointeurs, etc. Noter que la réglementation en vigueur en France interdit d'en fabriquer éclairant au-delà de mètres.

Ils ont quelques inconvénients tout de même, la lumière émise étant en général moins directionnelle et moins « pure » spectralement que celle d'autres types de lasers (à gaz en particulier) ; ce n'est pas un problème dans la majorité des applications.

Un dispositif très proche dans son fonctionnement, mais qui n'est pas un laser, est la DEL : le dispositif de pompage est le même, mais la production de lumière n'est pas "stimulée", elle est produite par désexcitation spontanée, de sorte que la lumière produite ne présente pas les propriétés de cohérence caractéristiques du laser.

Ce type de laser est très particulier, car son principe est tout à fait différent de celui exposé plus haut. La lumière n'y est pas produite par des atomes préalablement excités, mais par un rayonnement synchrotron produit par des électrons accélérés. Un faisceau d'électrons, provenant d'un accélérateur à électrons, est envoyé dans un "onduleur" créant un champ magnétique périodique (grâce à un assemblage d'aimants permanents). Cet onduleur est placé entre deux miroirs, comme dans le schéma d'un laser conventionnel : le rayonnement synchrotron est amplifié et devient "cohérent", c’est-à-dire qu'il acquiert les caractéristiques de la lumière produite dans les lasers.

Il suffit de régler la vitesse des électrons pour fournir une lumière de fréquence ajustée très finement sur une très large gamme, allant de l'infrarouge lointain (térahertz) aux rayons X, et la puissance laser peut être également ajustée par le débit d'électrons jusqu'à des niveaux élevés. On peut également disposer d'impulsions laser d'intervalle court et précis. Tout cela rend ce type de laser très polyvalent, et très utile dans les applications de recherche. Il est cependant plus coûteux à produire car il est nécessaire de construire un accélérateur de particules.

Le laser téramobile est un dispositif mobile qui délivre des impulsions laser ultrapuissantes et ultrabrèves. Le laser téramobile peut servir à détecter et mesurer des polluants atmosphériques ou à frayer à la foudre un chemin rectiligne.

Selon la puissance et la longueur d'onde d'émission du laser, celui-ci peut représenter un réel danger pour la vue et provoquer des brûlures irréparables de la rétine. Pour des questions de sécurité, la législation française interdit l'utilisation de lasers de classe supérieure à 2 en dehors d'une liste d'usages spécifiques autorisés.

La nouvelle norme :

Les classes ont été déterminées en fonction des lésions que peut provoquer un laser, elles varient en fonction de la fréquence du laser. Les lasers infrarouge (IR B et IR C) et ultraviolet (UV) provoquent des lésions de la cornée, du cristallin ou des lésions superficielles de la peau, tandis que les lasers visible et proche infrarouge (IR A) peuvent atteindre la rétine et l'hypoderme.

Dans le domaine visible, pour un laser continu, les classes sont : 

Les applications lasers utilisent les propriétés de cohérence spatiale et temporelle du laser. Elles peuvent être classées plus ou moins en fonction de la réflexion ou de l'absorption du laser. Ainsi, deux grandes familles apparaissent, celle contenant des applications de transfert d'information, et celle traitant d'un transfert de puissance.

"Transfert d'information"

"Métrologie"

"Transfert de puissance"

"Procédés laser et matériaux"

"Interaction laser/matière : phénomènes physiques"

"Applications médicales"

"Nucléaire"

"Applications militaires"


"Applications policières"
"Artistique"





</doc>
<doc id="16742" url="https://fr.wikipedia.org/wiki?curid=16742" title="Holocène">
Holocène

L'Holocène (du grec ancien "holos" - : entier, et "kainos" - : récent) est une époque géologique s'étendant sur les années, toujours en cours de nos jours. Il est fréquemment subdivisé en fonction de palynozones.

L'Holocène est un interglaciaire, c'est une période relativement chaude qui suit le dernier glaciaire du Pléistocène (dénommé Weichselien en Europe du Nord, Wisconsinien en Amérique du Nord ou Würm dans les Alpes). L'Holocène est la deuxième et dernière époque de la période Quaternaire, et l'un de ses nombreux interglaciaires. Certains scientifiques désignent néanmoins une nouvelle époque géologique succédant à l'Holocène : l'Anthropocène. Toutefois, certaines classifications proposent de placer Pléistocène et Holocène au sein de la période Néogène (respectivement comme troisième et quatrième époques) ; celle-ci étant la période qui, officiellement, précède le Quaternaire.

La remontée du niveau des océans (amorcée à la fin du glaciaire à environ -100 mètres avec le début de la fonte des inlandsis de l'hémisphère nord) s'est poursuivie depuis, environ -35 mètres, jusqu'au niveau actuel, atteint il y a environ . La mer Noire s'est remplie il y a environ . Les inlandsis finissent conjointement de fondre et les terres situées dessous ou à la marge des anciens inlandsis, libérées du poids de la glace, remontent (isostasie du manteau supérieur). Conjointement au réchauffement, faune et flore tempérées reconquièrent les moyennes et hautes latitudes et les écosystèmes de climats froids sont isolés dans des niches écologiques.

La remontée des eaux permit une transgression temporaire dans les terres en marges des inlandsis. Des fossiles marins peuvent être trouvés dans l'Ontario, le Vermont, le Québec, le Michigan. En dehors des zones de haute latitude où la mer s'est avancée à la suite de la dépression glaciaire, on trouve ce type de fossiles dans le lit des lacs, les plaines d'inondation, et les dépôts à l'intérieur des cavernes.

Des changements climatiques importants se produisent au cours de l'Holocène. La température s'élève notablement. Les précipitations augmentent en zone tropicale, entraînant une diminution des zones désertiques. Les zones habitables se décalent vers le Nord, alors que le niveau marin remonte, isolant par exemple les îles britanniques du continent européen. Il y a , le Sahara se couvre de végétation et de multiples lacs s'y créent. Les troupeaux de grands herbivores quittent les zones tropicales où les forêts s'étendent, pour se diriger vers les savanes apparues dans les déserts du Nord et du Sud. Ils sont suivis par une population humaine de chasseurs-cueilleurs qui laissent des peintures et des gravures rupestres dans le Sahara. Le retour ultérieur du désert, entre −3000 et −1000, contraint cette population à migrer sur les rives du Nil, donnant naissance à l'Égypte antique. Un phénomène comparable se déroule en Amérique du Sud, à l'origine de la civilisation de Paracas. 

La faune et la flore ne semblent pas avoir significativement évolué, mais la répartition des espèces a été fortement modifiée (remontées vers le nord des biomes et des biocénoses). 

Vers 3400 ans BP, débute une période de refroidissement global, appelée . Cette dégradation est liée à la combinaison des causes habituelles (orbitales et océaniques) et à des variations de l'activité solaire. Le refroidissement global est marqué par des anomalies climatiques : l'optimum climatique médiéval qui suit le et le réchauffement climatique actuel qui suit le Petit Âge glaciaire.

Cette époque est également marquée par une rapide et importante vague d'extinction d'espèces de grands mammifères dans l'hémisphère nord et en Australie ; la Mégafaune s'y est très fortement réduite.

• En Australie : les habitants préhistoriques y ont fait disparaître de grandes espèces de reptiles ainsi que de grands marsupiaux.

• En Amérique du Nord vivaient jusqu'à il y a environ de nombreux très grands animaux (jusqu'au triple des tailles des animaux "correspondants" en Afrique contemporaine). Toute cette mégafaune a brutalement disparu :
Toutes ces espèces (appartenant à 60 genres de grands mammifères, et incluant une vingtaine d'espèces d'équins) ont disparu d'Amérique du Nord sur environ un millénaire, ce qui est une période très brève à l'échelle des temps géologiques. Ceci après avoir survécu aux trois dernières glaciations. Tous les grands animaux terrestres ont été affectés.

• En Eurasie (Europe et Asie), contrairement à l'Amérique, l'Australie, la Nouvelle-Guinée ou l'Océanie, l'extinction de l'holocène fut limitée. Il est probable que la plupart des grands mammifères avaient évolué en même temps que l'homme devenait un prédateur redoutable, la sélection conservant ceux qui se méfiaient le plus du bipède. Les exceptions notables furent les disparitions du mammouth laineux, du rhinocéros laineux, du lion des cavernes et de l'ours des cavernes.

Sur ces trois continents, l'Homme utilisateur du feu, chasseur maîtrisant la pierre taillée, puis l'arc, la sagaie ou le propulseur, voire le boomerang en Australie, chasse avec des chiens, parfois expert en traque et piégeage animal, voire en poison, semble avoir une grande part de responsabilité dans ces disparitions, lesquelles ont peut-être eu d'importantes conséquences en matière d'écosystème et de physionomie des paysages. En Australie, l'introduction des dingos (chien domestique redevenu sauvage) par les premiers habitants de l'île a causé la disparition de nombreuses espèces animales et de la totalité des grands carnivores indigènes. L'hypothèse selon laquelle l'homme serait totalement ou principalement responsable de ces disparitions est parfois dite "théorie du Blitzkrieg".

Elle a notamment été portée et diffusée par Paul S. Martin qui note que de nombreuses pointes de flèches ont été trouvées dans les gisements de fossiles, parfois encore fichées dans certains os. Cette théorie est discutée et d'autres hypothèses (compatibles avec celle-ci) ont été proposées, dont l'introduction de pathogènes ou parasites qui auraient été responsables d'importantes zoonoses qui auraient décimé les grands animaux (non confirmées à ce jour par l'étude des fossiles), les grands animaux moins nombreux s'y seraient peut-être moins bien adaptés que les petits animaux dont la diversité génétique était peut-être plus élevée.

L'Holocène correspond à l'avènement du Mésolithique, du Néolithique et des cultures ultérieures.

C'est le début d'un accroissement démographique important de l'espèce humaine connu sous le nom de transition démographique agricole.



</doc>
<doc id="16743" url="https://fr.wikipedia.org/wiki?curid=16743" title="Éon">
Éon

L'éon est l'intervalle de temps géochronologique correspondant à la plus grande subdivision chronostratigraphique de l'échelle des temps géologiques, l'éonothème.

L'histoire de la Terre est découpée en quatre éons.
Les trois premiers, qui couvrent les 4 premiers milliards d'années de l'histoire de la Terre sont parfois regroupés au sein d'un superéon nommé le Précambrien. 
Pour un même intervalle de temps géologique, les éons et les éonothèmes portent des noms identiques.

Le terme éon est également utilisé dans le cadre de la planétologie pour permettre de décrire l'histoire des planètes.

Les quatre éons terrestres sont les suivants, du plus ancien au plus récent :

Par extension, la planétologie définit également des éons pour décrire l'histoire géologique des planètes autres que la Terre. Pour la planète Mars, par exemple, l'échelle des temps géologiques martiens est divisée en trois « éons » — néanmoins souvent qualifiés d'époques, même dans la littérature scientifique — selon deux systèmes chronostratigraphiques répondant à des définitions différentes (taux de cratérisation "vs" nature minéralogique) par datation relative et qui ne se superposent pas exactement :

Un est la combinaison d'éons. Le Précambrien est le seul superéon, regroupant l'Hadéen, l'Archéen et le Protérozoïque.



</doc>
<doc id="16744" url="https://fr.wikipedia.org/wiki?curid=16744" title="Basse-Californie (État)">
Basse-Californie (État)

L'État libre et souverain de Basse-Californie (en espagnol : "Estado Libre y Soberano de Baja California") est l'État du Mexique situé le plus au nord. La majeure partie de son territoire comprend la moitié de la péninsule de Basse-Californie au nord du Nord. Cette région constitue la presqu'île du Nord-Ouest du Mexique, située entre le golfe de Californie et l'océan Pacifique. Longue de plus de , et d'une largeur variant entre 50 et , elle est constituée de chaînes montagneuses qui s'élèvent dans le nord à plus de d'altitude. Entourée par l'État de Sonora et à l'est et la Basse-Californie du Sud au sud, elle est également frontalière des États d'Arizona et de Californie aux États-Unis.

L'État de Basse-Californie couvre une superficie de et compte environ habitants (2015). Plus de 75 % de la population vit dans les villes situées autour de la capitale de l'État, Mexicali et de Tijuana.

L'État de Basse-Californie a mérité, grâce à sa biodiversité, son surnom décerné par le commandant Cousteau de « Galápagos de l'hémisphère Nord ».

L’arrivée des premiers habitants dans la péninsule date de ans. À cette époque, plusieurs groupes indigènes vivaient dans la région. Dans le nord, des groupes appartenant à la famille linguistique Yuman, à savoir les tribus ; Kiliwa, Paipai, Kumeyaay, Cocopa, et Quechan ; leurs adaptations à la région restaient variées. Les Kiliwa, les Paipai, et les Kumeyaay, vivaient au nord-ouest, ils vivaient de la chasse et de la cueillette, mais dans les régions avec une population plus dense, les gens vivaient dans un mode de vie sédentaire. Les Cocopa et les Quechan au nord-est, pratiquaient l'agriculture, et principalement dans les plaines inondables du cours inférieur du fleuve Colorado. Au centre, les Cochimí vivaient dans les plaines désertiques ; ils vivaient principalement de chasse et de cueillettes, ils se déplaçaient fréquemment, mais le même groupe ethnique, au large de la côte ouest avait aussi développé une économie fortement maritime.
Les premiers européens atteignent la région en 1539.
Eusebio Chini, un jésuite italien fonde plusieurs postes missionnaires dans la région, dont celui de Lorette en 1697. Entre 1751 et 1753, le missionnaire jésuite croate Ferdinand Konščak explore par voie terrestre, le nord dans l'État. D’autres missions jésuites établirent une mission à Santa Gertrudis en 1752, à San Borja en 1762, et à Santa María en 1767.

En 1768, après l'expulsion des jésuites par le roi Charles III, les prêtres dominicains prennent la relève. Ils organisent des missions parmi les Indiens Cochimí au nord et chez les Yumans à l’ouest. Ils créent des missions à El Rosario en 1774 et à Descanso en 1817.

En 1804, la colonie espagnole de Californie est divisée en deux régions, et la ligne de séparation se situe entre les missions franciscains dans le Nord et les missions dominicains dans le Sud.

En 1824, le Mexique devient indépendant la péninsule est transformé en Territoire de la Basse-Californie.

En 1848, la Haute-Californie est annexée par les États-Unis.

En 1853, le Territoire de la Basse-Californie fait sécession et devient pendant quelques mois une "République de Basse-Californie" qui finit par réintégrer le giron mexicain.

En 1952, le Territoire du Nord de la Basse-Californie devient le État du Mexique, mais la partie sud, en dessous de demeure un territoire administré par l’administration fédérale.

En 1974, Le territoire du Sud de la Basse-Californie devient le État du Mexique. En 1989, Ernesto Ruffo Appel devient le premier gouverneur non-PRI de Basse-Californie.

À l’origine, on désignait sous le nom de « Californie » un territoire bien plus vaste que l’État actuel, puisqu’il était composé de la totalité de la péninsule mexicaine aujourd’hui connue sous le nom de Baja California, et des terres qui se trouvent aujourd’hui dans les États de Californie, du Nevada, de l’Arizona, de l’Utah et du Wyoming (Haute-Californie).

Certains pensent que le nom « California » est un dérivé du nom du paradis mythique de Calafia, évoqué dans l’ouvrage de Garci Rodríguez de Montalvo, Las sergas de Esplandián (1510), la suite du roman Amadis de Gaule[6]. Elle est présentée dans le livre comme une terre difficile à atteindre où l'or abonde, habitée par des Amazones vivant dans des cavernes, et d’étranges animaux.

En 1921, le géographe Lucien Gallois émet l'hypothèse que l'origine du nom cité dans le roman pourrait venir de la Chanson de Roland, qui cite l'île mythique de « Califerne »[7].

D’autres suggèrent que l’étymologie du nom California aurait un rapport plus étroit avec les premiers colons espagnols qui, lorsqu’ils y arrivèrent par les régions du Sud, trouvèrent dans la contrée des sources liées à la tectonique locale « chaudes comme un four » (cali = chaud, fornia = four) ou encore comme des « fourneaux chauds » (caliente fornalia en espagnol).

Une autre origine du nom pourrait être calida fornax, « climat chaud », en latin. Le golfe de Californie apparaît sur des cartes datant des années 1560


La Basse-Californie est une longue et étroite péninsule séparée du Mexique continental par le golfe de Californie. Une chaîne de pics déchiquetés, dont certains sont volcaniques, traverse la péninsule sur toute sa longueur parsemée d'énormes coulées de lave. Le climat est dans l'ensemble tropical et désertique, mais c'est la moitié sud du pays qui est la plus aride. Il n'y a pratiquement aucun cours d'eau en surface, et peu de sources susceptibles de créer des oasis. En été, les températures peuvent dépasser . La végétation est en grande partie constituée de rares broussailles xérophiles et, en certains endroits de la péninsule, poussent de gigantesques et nombreux cactus. La végétation de l'État mexicain de Basse-Californie est tributaire du relief et du climat ; les zones montagneuses abritent surtout des pins, des chênes verts, des peupliers et des palmiers alors que les zones désertiques et arides abritent des arbustes, plus particulièrement des mezquites, des ocotillos, ainsi que des cactus de formes et de tailles variées.

L'étroite presqu'île de Basse-Californie, entre le golfe de Californie et l'océan Pacifique, est la région du Mexique la plus reculée. Le relief montagneux et le climat aride ne favorisent guère la végétation, en dehors du maquis de chênes verts ou "chaparral" en espagnol. "Chaparral" désigne spécifiquement la forêt claire d'arbres et arbustes à feuilles persistantes poussant dans le sud-ouest des États-Unis et en Basse-Californie. Les arbrisseaux de ce maquis sont des feuillus à feuilles dures qui couvrent un sol pauvre à cause de la sécheresse du climat.


L'État de Basse-Californie est divisé en cinq municipalités libres.




</doc>
<doc id="16745" url="https://fr.wikipedia.org/wiki?curid=16745" title="Baja California">
Baja California

Les nombreux sites archéologiques et préhistoriques se situant dans la péninsule de Basse-Californie située au Mexique, sont regroupés sous l'appellation internationale Baja California.

La péninsule de Basse-Californie regorge de plusieurs centaines de sites préhistoriques, situés dans la Sierra de San Francisco, la sierra de San Juan, la sierra de San Borja et dans la sierra de Guadalupe. Certains sont constitués de grottes ornées ou d'abris rocheux, d'autres renferment des ossements de différentes époques. La région est très riche en œuvres d'art rupestre. Des centaines de représentations ont déjà été répertoriées. Les peintures représentent des humains mais également toute une faune telle que cerfs communs, moutons des montagnes, lapins, oiseaux et poissons. Certains animaux et même des humains sont représentés traversés avec des flèches. Les couleurs de base employées furent le noir (charbon de bois), le blanc (cendre volcanique), le rouge (lave écrasée) et parfois même de l'ocre (mélange de rouge-orange-jaune).

Depuis le début des années 1990, des spécialistes, sous la conduite de l'archéologue Maria de la Luz Gutiérrez, de l'Institut National d'Anthropologie et d'Histoire du Mexique (INAH) et sous l'égide de l'INAH et du Conseil National de la Science et de la Technologie (CONACYT) mexicains, étudient les peintures rupestres de Baja California. Du fait de son très bon état de conservation, ils en estimaient l'âge à moins de ans. Or en 2002, le géologue australien Alan Watchman, mondialement reconnu en matière de datation de l'art rupestre, leur communique, les résultats qu'il vient d'obtenir sur les échantillons prélevés dans le massif de Guadalupe, au Mexique. Les premières datations les étonnent, car plusieurs d’entre elles atteignent ans. D’autres analyses ultérieures de datation, plus d’une soixantaine en tout, vont révéler des dates allant jusqu’à ans. « Cet art, réalisé par des tribus de chasseurs-cueilleurs et tailleurs de pierre, se caractérise tout d'abord par une concentration exceptionnelle de sites ornés, et un très grand nombre de peintures», explique Maria de la Luz Gutiérrez. Pour le seul massif de la sierra de Guadalupe, plus de 700 sites de peintures ont été répertoriés et étudiés dans le cadre du projet de l'INAH, la plupart du temps réalisées sur de grandes parois situées au pied de falaises, ou dans des abris sous roche. 
En 1992 Justin Hyland du Département d'Anthropologie de l'Université de Berkeley en Californie et sa collègue, María de la Luz Gutiérrez de l’INAH, entreprennent un travail sur le terrain dans la Sierra de San Francisco. Leur projet, appelé : "Proyecto Arte Rupestre Baja California Sur", est un programme de recherche et de conservation et constitue un des douze projets spéciaux d'archéologie inauguré par le Président Carlos Salinas de Gortari. C'est le plus grand projet d'archéologie jamais organisé en Basse-Californie et un des plus grands projets archéologiques de tout le Mexique.
En décembre 1993, face à l’importance exceptionnelle des découvertes archéologiques et à l’excellente conservation des œuvres pariétales millénaires, l'UNESCO décida de classer au Patrimoine mondial de l'Humanité, tout le legs culturel préhistorique de la Sierra de San Francisco.
On évalue à près de le nombre de représentations picturales dans l’ensemble de la Basse Californie. La plupart d’entre elles, étant dans un excellent état de conservation tout à fait exceptionnel malgré leur grand âge de près de ans pour les plus anciennes.

Les colons espagnols furent les premiers à s'intéresser à la richesse picturale de la région dès le .Les premières explorations scientifiques furent entreprises dès 1893 et en 1895 par l'explorateur français Léon Diguet. Il étudia huit emplacements d'arts pariétaux situés en Basse Californie et publia ses recherches dans plusieurs articles d'anthropologie en 1899. Il prit par ailleurs de nombreux clichés photographiques entre 1893 et 1900, qu'il regroupa dans un recueil, réédité en 1991. 
Jean Clottes, Conservateur général du Patrimoine (honoraire) et spécialiste mondial de l'Art Rupestre, auteur d’ouvrages spécialisés dans l’art au temps préhistorique, témoigne de la beauté de cet art rupestre millénaire : "J'ai voulu partager mes expériences, mes surprises, mes émotions et mes enthousiasmes dans des domaines aussi différents que les fouilles dans les grottes profondes de l'Ariège, des journées exceptionnelles passées à Lascaux, ou les voyages dans des pays proches (Espagne) ou lointains (Niger, Australie, Baja California au Mexique) à la difficile découverte d'arts rupestres souvent méconnus et pourtant d'une qualité comparable à ceux des cavernes françaises et espagnoles." Jean Clottes.

Les Pericúes sont un groupe ethnique disparu qui a habité à l'extrémité sud de la péninsule de la Californie mexicaine. Ce peuple s'est éteint ethniquement et linguistiquement à la fin du . Selon l'analyse crâniologique, leurs crânes sont de type dolichocéphale, c’est-à-dire non mongoloïde, mais australoïde ou europoïde. L'analyse de l'ADN révèle un marqueur haplotype "B" qui confirmerait l'hypothèse d'une migration par circumnavigation ou circambulation terrestre autour de l'océan Pacifique. 

Le territoire ethnique des Péricues comprenait la pointe sud de la péninsule californienne, depuis l'Extrémité de San Lucas jusqu'à l'Extrémité de Pulmo, ainsi que les grandes îles du sud du golfe de Californie, comme Cerralvo, Espíritu Santo ou San José.

Les études sur la langue péricue sont très limitées car il n'en subsiste que quelques mots enregistrés par les missionnaires, plus environ une douzaine de toponymes du Sud de la Basse Californie. Les missionnaires jésuites ont reconnu que cet idiome était une langue différente des autres langues amérindiennes. On suppose que le péricue et le guaicura (langue voisine) ont dû constituer une famille linguistique à part.

Le territoire des Péricues fut occupé depuis la fin du Pléistocène et pendant l'Holocène. La présence humaine dans la péninsule de Baja California remonte à quelques dizaines de milliers d'années. Les sites préhistoriques de Baja California, riches en peintures pariétales, ont livré des ossements humains paléoaméricains dont les crânes ont révélé une origine mélanésienne. La mise au jour, en 1996, de nombreux outils (artefacts, bois brûlés, coquillages travaillés) sur le site de la caverne de Babisuri en Basse-Californie a permis d'y dater d'au moins ans la présence humaine.

Plusieurs dizaines de squelettes datés de ans à ans ont par la suite été découverts par plusieurs équipes d'archéologues mexicains, américains, britanniques, espagnols et japonais, dans la même région mexicaine de Baja California. Des spécialistes internationaux (R. Gonzalès-José et M. Hernandez de l’université de Barcelone, A. Gonzales-Martin de l’Académie d’Histoire et d’Anthropologie de Pachuca, Mexique, H. Pucciarelli et M. Sardi du département scientifique d’Anthropologie du musée de La Plata, Argentine, A. Rosales de l’Institut National d’Anthropologie et d’Histoire du centre INAH Baja California, Mexique, et S. Van der Molen de l’Université Autonome de Barcelone) ont longuement étudié les crânes de plusieurs dizaines de squelettes. Les analyses craniométriques ont permis de connaître l’origine de ces paléoaméricains. Les squelettes Péricues, mis au jour, présentent des crânes hyper-dolichocéphaliques suggérant que les ancêtres des Péricues étaient peut-être de type australoïde ou de type aïnou et vinrent par migrations trans-pacifiques à partir de la dernière période du Pléistocène.

Les premiers contacts entre Européens et Péricues remontent à 1530 environ, quand Fortún Ximénez et une expédition envoyée par Hernán Cortès ont atteint la Basse Californie. Par la suite, des rencontres sporadiques, quelques fois amicales et d'autres fois hostiles, se sont produites entre des explorateurs, des missionnaires ou des marins et ce peuple aux caractéristiques bien différentes de celles des tribus amérindiennes habituellement rencontrées dans tous ces nouveaux et vastes territoires, aussi bien sur le plan physique (physionomie) que culturel (céramiques). Les conflits et les épidémies décimeront ce peuple millénaire. L'explorateur français Léon Diguet prit quelques clichés photographiques de quelques rares survivants, descendants des Guaycuras, peuples voisins des Péricues, aux caractéristiques métissées amérindiennes et péricues.

Les jésuites établirent leur première mission permanente à Conchó durant l'année 1697, mais mirent plus de deux décennies à pénétrer à l'extrême Sud de la péninsule. Des missions destinées aux Pericúes furent finalement établies à Airapí (1720), Añiñí (1724), et Añuití (1730). 

Un évènement dramatique pour les jésuites survint en 1734, quand commença un conflit avec les Pericúes, qui se transforma en l'un des plus grands défis pour les missionnaires en Californie. Deux d'entre eux furent assassinés -- Lorenzo Carranco à Santiago Añiñí, le octobre 1734, et deux jours plus tard, Nicolás Tamaral à San José de Añuití. Pendant deux ans, la région échappa au contrôle jésuite. Mais ce sont les Pericúes qui souffrirent le plus, du fait des décès provoqués par les combats contre les Espagnols et les effets des épidémies apportées d'Europe par les conquérants. À cette époque, la Couronne d'Espagne vint en aide aux Jésuites et chassa ce qui restait de survivants péricues (1768). Ces derniers furent assimilés culturellement, mais leurs gènes survivent dans la population métisse du Sud de la Basse Californie.

En 1991, l’archéologue japonaise Harumi Fujita, chercheuse à l’INAH, travailla sur les sites de Baja California, dans le district sud de La Paz. Elle annonça qu’elle avait donné à analyser plusieurs échantillons d’un site préhistorique situé dans une caverne de l’île d’Espiritu Santo, en Basse Californie. Le laboratoire de l’INAH, au moyen de quatorze analyses au radiocarbone 14, va dater ces éléments de ans. (caverne de Babisuri).
De 1994 à 1996 les archéologues vont mettre au jour près de 150 sites préhistoriques sur l’île d’Espiritu Santo. (Habitations dans des cavernes, sites en plein air, grottes mortuaires, sites pictographiques, etc.)
En 2003, Stefan Lovgen du « National geographic » émet l’hypothèse que les populations paléoaméricaines d’origine australoïde ou mélanésienne, sont arrivés bien avant la vague migratoire des peuples amérindiens d’origine mongoloïde.
Ce peuple paléoaméricain d'origine non amérindienne, pourrait avoir eu comme ancêtre, les peuples ayant les mêmes caractéristiques morphologiques comme celles de la Femme de Peñon découverte dans la vallée de Mexico, près d'une ancienne lagune et dont le squelette, daté de ans rappelle ceux des Péricues.




</doc>
<doc id="16747" url="https://fr.wikipedia.org/wiki?curid=16747" title="Phanérozoïque">
Phanérozoïque

Le Phanérozoïque (du grec φανερός, "phaneros" signifiant « visible », et "ζῶον, zôon" signifiant « animal ») est un éon couvrant les 541 derniers millions d'années. Il commence par la période géologique du Cambrien, avec l'apparition des petits animaux à coquilles, puis a vu le développement d'une vie animale abondante jusqu'à nos jours. L'éon précédent est le Protérozoïque, qui fait partie du Précambrien, une large période allant de la formation de la Terre au début du Phanérozoïque.

La frontière entre les deux éons du Protérozoïque et du Phanérozoïque n'est pas clairement définie, les découvertes du ayant repoussé la limite qui était communément admise au . La plupart des géologues et paléontologues placent la frontière à l'apparition des premiers trilobites, des certains ichnofossiles appelés "Trichophycus (Phycodes) pedum", ou aux premières apparitions d'un groupe de petites formes de vie désarticulées. Ces trois points se situent dans la même période, à quelques millions d'années près, dans la période du Cambrien.

Le Phanérozoïque voit l'émergence d'un grand nombre de formes biologiques, l'apparition des plantes sur la terre ferme, puis leur développement, l'évolution des poissons, la conquête de la terre ferme par les animaux et le développement de la faune moderne.

Durant cette période, les continents se déplacent et la Pangée se sépare, donnant les six masses continentales actuelles.

Le Phanérozoïque est divisé en trois ères :




</doc>
<doc id="16751" url="https://fr.wikipedia.org/wiki?curid=16751" title="Nombre entier">
Nombre entier

En mathématiques, la notion de nombre entier peut désigner deux types de nombres :

</doc>
<doc id="16757" url="https://fr.wikipedia.org/wiki?curid=16757" title="Association pour le maintien d'une agriculture paysanne">
Association pour le maintien d'une agriculture paysanne

Une association pour le maintien d'une agriculture paysanne ou une association pour le maintien d'une agriculture de proximité (AMAP) est, en France, un partenariat de proximité entre un groupe de consommateurs et une exploitation locale (généralement une ferme), débouchant sur un partage de récolte régulier (le plus souvent hebdomadaire) composée des produits de la ferme. L'AMAP est un contrat solidaire, basé sur un engagement financier des consommateurs, qui paient à l’avance la totalité de leur consommation sur une période définie. Ce système fonctionne donc sur le principe de la confiance et de la responsabilité du consommateur ; il représente une forme de circuit court de distribution.

Le terme « AMAP » est enregistré depuis mi-2003 à l'INPI en tant que marque française par l'association Alliance Provence. L'usage de la marque passe par l'engagement du respect de la charte des AMAP inspirée de la Charte de l'agriculture paysanne, éditée en mai 2003 par cette même association. En 2014, la Charte des AMAP est ré-actualisée, à la suite d'un chantier et d'une réflexion participative inter-régionale coordonnée par le Mouvement Inter-Régional des AMAP (MIRAMAP).

Une AMAP est : 

Une telle association est considérée comme participant de l'économie solidaire, selon les critères suivants :

Le fermier amortit les aléas de l'économie de marché (il a une vision sur plusieurs mois) et garde une indépendance par rapport au système de grande distribution ; quant aux consommateurs, ils peuvent directement suivre et influencer le mode de culture (souvent biologique ou biodynamique ou agriculture durable). 

Les AMAP ont également les ambitions suivantes :

Le partenariat est assuré par un comité de bénévoles qui participe au fonctionnement de l'AMAP.

Ses fonctions sont d'assurer le lien avec le producteur pour la communication et l'information, de rechercher d'autres consommateurs, de prendre les inscriptions, d'organiser la distribution, etc.

Producteur et consommateurs sont liés par un contrat dans lequel l’agriculteur s'engage à fournir aux consommateurs un panier par semaine de produits à un prix équitable défini en toute transparence, tandis que les consommateurs s'engagent, en toute connaissance des diverses contraintes, à effectuer à tour de rôle une permanence à l'accueil de la distribution, voire à effectuer ponctuellement sur l'exploitation des tâches en groupe (désherbage, récoltes…).

Un des exemples le plus ancien du concept a émergé dans les années 1960 au Japon. À l'époque, des mères de familles japonaises s’inquiètent de voir l’agriculture s’industrialiser avec un recours massif aux produits chimiques (en 1957, les premières victimes de Minamata, empoisonnées au mercure, sont déclarées). Ces mères fondent alors en 1965 les premiers "teikei" (, signifiant en japonais « coopération ou collaboration ») qui concernent d'abord des coopératives laitières. Le principe de fonctionnement est le suivant : en échange de l’achat par souscription de la récolte du paysan, ce dernier s’engage à fournir des aliments cultivés sans produits chimiques.

À la même époque en Suisse, des fermes communautaires nommées (ou association alimentaire) développent leur propre partenariat avec les consommateurs locaux en leur fournissant chaque semaine des produits frais (légumes, lait, œufs, et fromages).

Aux États-Unis en 1985, un fermier de retour de Suisse après avoir étudié les rencontre alors Robyn Van Hen, une agricultrice bio du Massachusetts. Avec l’aide d’autres producteurs et de consommateurs, ils fondent la première à la ferme de Robyn. Le concept se répand ensuite rapidement par bouche-à-oreille dans tous les États-Unis, puis gagne le Canada (ASC).
Le concept est un système économique à part entière pour le paysan, et autonome, c'est pourquoi les agriculteurs fondateurs l'ont nommé AMAP.

En 2000, on recensait des CSA dans de nombreux pays : plus de en Amérique du Nord, 100 au Royaume-Uni, et aussi en Australie, Danemark, Pays-Bas, Allemagne, Hongrie, Ghana, Nouvelle-Zélande… En 2003, aux États-Unis, sont en fonctionnement.

Le principe d'une distribution hebdomadaire de légumes à des réseaux d'adhérents-consommateurs a été introduit en France par les Jardins de Cocagne aux débuts des années 1990 (chantiers d'insertion de personnes en difficulté par le maraîchage biologique).
Un annuaire en ligne recense les Amap en France.

En 2000, Denise et Daniel Vuillon, couple de maraîchers pratiquant déjà la vente directe et une production biodiversifiée dans l'ouest du Var à Ollioules, rendirent visite à leur fille en stage d'architecture aux États-Unis. En parcourant une rue à Manhattan, Daniel Vuillon aperçut des gens qui s'affairaient autour de gros paniers de légumes sur le parvis d'une petite église. Intrigués, les Vuillon découvrirent alors un groupe de consommateurs de "Community Supported Agriculture" que leur fermier venait de livrer en légumes fraîchement récoltés. Ils rendirent donc visite à ce fermier à une heure de route de New-York, ils discutèrent de cette expérience, trouvèrent l'idée intéressante et l’étudièrent sur place. 

À la suite d'une réunion organisée par ATTAC à Aubagne en sur le thème de la « malbouffe », durant laquelle les Vuillon présentèrent le concept des CSA et ses avantages, des consommateurs furent motivés. Trois réunions se sont tenues. Le 8 avril avait lieu le pique-nique à la ferme qui reste une étape importante dans la création de la première AMAP de France. Le , le premier panier (sur un total de trente-deux) était livré par « Les Olivades » aux consommateurs d'Aubagne. Cette première distribution s'est déroulée sur un parking mis à disposition par une « amapienne ».

Estimant le contexte favorable au développement de ce type d’initiative, ces maraîchers, membres de la Confédération paysanne du Var, avec d’autres associations, envisagèrent de créer une structure chargée d’accompagner les porteurs de projet AMAP dans la région. "Alliance Provence" fut alors fondée le . En Île-de-France, la première AMAP est créée à Pantin pour un nombre qui atteint 300 en l'an 2014.

En se tint à Aubagne le premier colloque international des teikei (Japon), CSA (pays anglophones), AMAP (France), ASC (Québec), etc. Les colloques suivants ont eu lieu à Aubagne (fin ) et à Kobé (). Un deuxième colloque international se tint ensuite à Palmela (Portugal) en donnant lieu à la création du réseau international URGENCI qui regroupe ces diverses initiatives.

Le mouvement s'est progressivement et partiellement structuré, par la création de réseaux des AMAP, essentiellement à l'échange régional, puis plus récemment, par la formation d'un mouvement inter-régional des AMAP (MIRAMAP) en février 2010. Il a pour objet de "renforcer la cohésion des AMAP à travers le partage d’une éthique commune, de mutualiser les expériences et les pratiques et d’assurer la représentation, la mise en valeur des AMAP au niveau national et à la représentation du mouvement auprès des institutions officielles (Régions, ministères, etc.)"

En 2012 plus de AMAP approvisionnent en France familles, soit personnes. .

Ce type d'organisation est présent dans de nombreux pays sous des formes plus ou moins proches. On peut citer :




</doc>
<doc id="16758" url="https://fr.wikipedia.org/wiki?curid=16758" title="Noyau (algèbre)">
Noyau (algèbre)

En mathématiques et plus particulièrement en algèbre générale, le noyau d'un morphisme mesure la non-injectivité d'un morphisme.

Dans de nombreux cas, le noyau d'un morphisme est un sous-ensemble de l'ensemble de définition du morphisme : l'ensemble des éléments qui sont envoyés sur l'élément neutre de l'ensemble d'arrivée. Dans des contextes plus généraux, le noyau est interprété comme une relation d'équivalence sur l'ensemble de définition : la relation qui relie les éléments qui sont envoyés sur une même image par le morphisme. 

Dans l'une ou l'autre de ces situations, le noyau est trivial si et seulement si le morphisme est injectif. Dans la première situation, « trivial » signifie constitué uniquement de l'élément neutre, tandis que dans le second, cela signifie que la relation est l'égalité.

Le noyau d'un morphisme "f" est noté ("f") ou ("f"). Cette abréviation vient du mot anglais "" qui signifie « noyau » (dans tous les sens du terme : l'analogie s'est propagée d'une langue à l'autre).

Cet article présente diverses définitions du noyau, pour les types les plus couramment utilisés de morphismes.

Le noyau d'un morphisme de groupes "f" d'un groupe "G" vers un groupe "H" se compose de tous les éléments de "G" qui sont envoyés par "f" sur l'élément neutre "e" de "H". Formellement : 
Le noyau est un sous-groupe distingué de "G".

L'un des théorèmes d'isomorphisme énonce que le groupe quotient "G"/("f") est isomorphe à l'image de "f". Cet isomorphisme est induit par "f" lui-même. Une proposition légèrement plus générale est le théorème de factorisation des morphismes.

Le morphisme de groupes "f" est injectif si et seulement si son noyau est le groupe trivial.

D'après les propriétés de l'image réciproque, le noyau d'un morphisme composé formula_2 est égal à :

Si "f" est une application linéaire d'un espace vectoriel "V" dans un espace vectoriel "W", alors le noyau de "f" est défini par
Le noyau est un sous-espace de l'espace vectoriel "V", et l'espace vectoriel quotient "V"/("f") est isomorphe à l'image de "f" ; en particulier, le théorème du rang relie les dimensions :
L'application linéaire "f" est injective si et seulement si ("f") = {0}.

Si "V" et "W" sont des espaces vectoriels de dimension finie sur un corps commutatif "K", de dimensions respectives "n" et "p", et si des bases de ces espaces sont données, alors "f" peut être représentée par une matrice formula_6, et le noyau peut être déterminé en résolvant le système homogène d'équations linéaires "M X" = 0.

Dans cette représentation, les solutions de ce système correspondent aux coordonnées des vecteurs du noyau de "f", mais aussi aux vecteurs du noyau de l'application linéaire canoniquement associée à la matrice "M".

La dimension du noyau est donnée par le nombre de colonnes de "M" moins le rang de "M".

Le résolution d'équations différentielles homogènes mène souvent à la détermination du noyau d'une certaine application linéaire.

Par exemple, si l'on désire déterminer les fonctions deux fois dérivables "f" de R dans R telles que
il faut considérer le noyau de l'application linéaire formula_8, où "V" est l'espace vectoriel de toutes les fonctions deux fois dérivables de R dans R, "W" l'espace vectoriel de toutes les fonctions de R dans R, et l'image par formula_9 d'un élément "f" de "V" définie par la condition

Le noyau d'un morphisme d'anneaux "f" d'un anneau "A" dans un anneau "B" se compose de tous les éléments "x" de "A" pour lequel "f" ( "x" ) = 0. Formellement :
Le noyau est un idéal bilatère de "A".

Le théorème d'isomorphisme mentionné ci-dessus pour des groupes et des espaces vectoriels reste valable dans le cas des anneaux.

Le noyau d'un morphisme de corps (c'est-à-dire un morphisme d'anneaux où les anneaux considérés sont des corps) est toujours réduit à l'élément neutre 0, de sorte que tout morphisme de corps est injectif.

Sur un espace vectoriel réel "E", une forme quadratique est une application polynomiale formula_12 qui est homogène de degré 2. Elle est associée à la forme bilinéaire symétrique
Le noyau de "q" est le sous-espace vectoriel 
La contraction de "B" par "v" désigne l'application linéaire formula_15, et "N" apparaît comme le noyau de l'application linéaire 
L'image est un sous-espace vectoriel de l'espace dual "E"* qui est l'annulateur du noyau "N".

Toutes ces notions de noyaux se généralisent dans le cadre de la théorie des catégories abéliennes.

La fonction exponentielle complexe est un exemple de morphisme de groupes, de (ℂ, +) dans (ℂ*, ×). Son noyau est l'ensemble des nombres complexes "z" tels que .

En notant formula_17, on obtient


</doc>
<doc id="16765" url="https://fr.wikipedia.org/wiki?curid=16765" title="Édouard VI">
Édouard VI

Édouard VI (né "Edward", 12 octobre 1537 – 6 juillet 1553) est roi d'Angleterre et d'Irlande de 1547 à sa mort. Il est couronné le 20 février 1547 à l'âge de neuf ans ce qui en fait l'un des plus jeunes souverains anglais. Il est le fils d'Henri VIII d'Angleterre et de Jeanne Seymour, et devient le troisième souverain de la dynastie des Tudor et le premier souverain anglais anglican à monter sur le trône.

Pendant tout son règne, le pouvoir est exercé par un conseil de régence, car il meurt avant d'avoir atteint sa majorité. Le Conseil est d'abord présidé par son oncle Édouard Seymour, duc de Somerset (de 1547 à 1549), puis par John Dudley, comte de Warwick (de 1550 à 1553), qui devient plus tard duc de Northumberland.

Son règne s'accompagne de problèmes économiques et des troubles sociaux qui, en 1549, aboutissent à des émeutes et à une rébellion. En 1546, la guerre est engagée contre l'Écosse ; elle commence par un succès mais à cause d'une attaque française sur Boulogne-sur-Mer, elle prend fin en août 1549 par un retrait militaire vers la France pour défendre la ville. La transformation de l'Église anglicane en un corps religieux structuré a également lieu sous son règne et le jeune Édouard attache un grand intérêt aux questions religieuses pendant tout son règne. Bien qu’Henri VIII ait rompu le lien entre l'Église d'Angleterre et Rome, il n'a jamais admis de renoncer à la doctrine catholique ou à ses cérémonies. C'est pendant le règne d'Édouard que l'anglicanisme est établi pour la première fois en Angleterre avec des réformes qui comprennent l'abolition du célibat des prêtres et de la messe, la disparition des statues et vitraux des églises et l'imposition de l'anglais lors des offices. L'architecte de ces réformes est Thomas Cranmer, archevêque de Cantorbéry, dont le " (« Livre de la prière commune ») est toujours en vigueur.

Édouard tombe malade en janvier 1553 et, quand son entourage comprend qu'il arrive à la fin de sa vie, le conseil de Régence et lui élaborent un " (un « Testament successoral ») pour tenter d'empêcher le pays de retourner au catholicisme. Édouard nomme pour héritière sa cousine Jeanne Grey et écarte de sa succession ses demi-sœurs, Marie (fille de la première épouse de son père, Catherine d'Aragon) et Élisabeth (fille d'Anne Boleyn). Cependant, ce testament est contesté après sa mort et Jeanne ne monte sur le trône que neuf jours avant que Marie ne soit proclamée reine à son tour. Cette dernière annule bon nombre des réformes religieuses de son frère mais le Règlement élisabéthain de 1559 assure toutefois son héritage anglican.

Édouard naît le 12 octobre 1537 dans les appartements de sa mère, au château de Hampton Court, dans le Middlesex. Il est le fils du roi Henri VIII et de sa troisième épouse, Jeanne Seymour. Dans tout le royaume, le peuple salue la naissance d'un héritier mâle, , avec joie et soulagement. Des "Te Deums" sont chantés dans les églises, des feux de joie allumés, et . Jeanne, qui paraît bien récupérer de l'accouchement, envoie des lettres pré-signées annonçant la naissance d'. Édouard est baptisé le 15 octobre, avec la princesse Marie comme marraine et la princesse Élisabeth portant le saint chrême, et le roi d'armes de l'ordre de la Jarretière le proclame duc de Cornouailles et comte de Chester. Jeanne Seymour, cependant, tombe malade le 23 octobre de possibles complications postnatales et meurt la nuit suivante. Henri VIII écrit alors à François de France que .

Édouard est un bébé en bonne santé qui s'alimente bien. Son père est très heureux avec lui ; en mai 1538, on voit le roi . Au mois de septembre la même année, le "lord" chancelier, Thomas Audley, note une croissance rapide et vigoureuse d'Édouard ; d'autres comptes-rendus le décrivent comme un enfant grand et joyeux. La légende selon laquelle il était un garçon chétif a été contestée par certains historiens. À l'âge de quatre ans, il tombe malade d'une « fièvre quarte » qui met sa vie en danger, mais, en dépit de maladies occasionnelles et d'une mauvaise vue, il jouit d'une bonne santé générale jusqu'aux six derniers mois de sa vie.

Édouard est d'abord confié à Margaret Bryan, « maîtresse dame » de la maison du prince. Puis, celle-ci est remplacée par . Jusqu'à l'âge de six ans, Édouard est élevé, comme il le dit plus tard dans sa chronique, . Le mode de vie et l'environnement d'Édouard sont d'abord confiés à , puis à Richard Page, le beau-père de l'épouse d'Édouard Seymour, Anne Stanhope. Henri demande des normes rigoureuses de sécurité et de propreté dans la maison de son fils, soulignant qu'Édouard est . Les visiteurs décrivent le prince, qui est richement fourni en jouets et moyens, y compris sa propre troupe de jongleurs, comme un enfant heureux.

Dès l'âge de six ans, Édouard commence à recevoir l'enseignement de Richard Cox et de . Comme il le rappelle plus tard, son éducation se concentre alors sur ; il suit les cours du tuteur d'Élisabeth, Roger Ascham, et de Jean Belmain, qui lui apprend le français, l'espagnol et l'italien. On sait qu'il étudie aussi la géométrie et apprend à jouer d'instruments de musique, comme le luth et le virginal. Il rassemble des globes terrestres et des cartes géographiques et, selon l'historien C. E. Challis, acquiert des connaissances des affaires monétaires qui témoignent d'une haute intelligence. Son éducation religieuse est supposée avoir favorisé ses réformes. Son enseignement religieux est d'abord probablement assuré par l'archevêque Thomas Cranmer, un réformateur de premier plan. Cox et Cheke sont des catholiques partisans d'Érasme et sont exilés plus tard sous le règne de Marie . En 1549, Édouard écrit un traité présentant le pape comme l'Antéchrist et prend des décisions éclairées sur les controverses théologiques. De nombreux aspects de la religion d'Édouard sont essentiellement catholiques dans ses premières années, comme la célébration de la messe et la vénération des images et des reliques des saints.

Ses deux sœurs sont très attentionnées envers lui et lui rendent souvent visite – à une occasion, Élisabeth lui donne ainsi une chemise . Édouard a pour Marie, même s'il désapprouve son goût pour les danses étrangères. lui écrit-il en 1546. En 1543, Henri invite ses enfants à passer Noël avec lui, en signe de réconciliation avec ses filles, qu'il a précédemment illégitimées et déshéritées. Au printemps suivant, il les remet à leur place dans l'ordre de succession avec le " (troisième loi de succession), qui prévoit également un conseil de régence pendant la minorité d'Édouard. Cette harmonie familiale inhabituelle doit peut-être beaucoup à l'influence de la nouvelle épouse d'Henri, Catherine Parr, qu'Édouard apprécie énormément. Il l'appelle sa et, en septembre 1546, il lui écrit : .

D'autres enfants sont amenés à jouer avec le prince, dont la petite-fille de son chambellan William Sidney qui, à l'âge adulte, se souvient d'Édouard comme d'. Le prince fait ses études avec des fils de nobles, choisis pour l'assister dans ce qui est une sorte de cour miniature. Parmi ces derniers, (1533-1581), fils d'un pair d'Irlande, qui devient un ami proche et durable. Édouard est plus soucieux de ses devoirs que ses camarades de classe et semble les avoir surpassés, motivé par sa volonté de faire son « devoir » et de rivaliser avec les prouesses académiques de sa sœur Élisabeth. Son cadre de vie est splendide : sa chambre est tapissée de coûteuses tapisseries flamandes et ses vêtements, livres et couverts sont incrustés de pierres précieuses et d'or. Comme son père, Édouard est fasciné par les arts militaires et de nombreux portraits de lui le montrent portant un poignard d'or avec un manche en pierres précieuses, à l'imitation du souverain. Ses chroniques décrivent avec enthousiasme les détails des campagnes militaires anglaises contre l'Écosse et la France et les exploits de l'Angleterre, comme la bataille de Pinkie Cleugh à Musselburgh en 1547.

Le juillet 1543, Henri VIII signe le traité de Greenwich avec les Écossais, faisant la paix en fiançant Édouard avec la reine Marie d'Écosse, alors âgée de sept mois. Depuis leur défaite de Solway Moss, au mois de novembre précédent, les Écossais se trouvent en position de faiblesse pour négocier et Henri, qui cherche à réunir les deux royaumes, fait stipuler que Marie doit lui être remise pour faire son éducation en Angleterre. Lorsque les Écossais répudient le traité en décembre 1543 et renouvellent leur alliance avec la France, Henri VIII est furieux. En avril 1544, il ordonne à son beau-frère, Édouard Seymour, comte de Hertford, d'envahir l'Écosse et de . Seymour répond à la trahison par la campagne la plus sauvage jamais lancée par les Anglais contre les Écossais. La guerre, qui se poursuit sous le règne d'Édouard, est aujourd'hui connue sous le nom de ".

Le 10 janvier 1547, Édouard écrit à son père et à Catherine Parr, depuis Hertford, les remerciant pour leur portrait reçu comme cadeau à l'occasion de la nouvelle année. Quelques jours après, le 28 janvier 1547, Henri VIII meurt. Les proches du trône, dirigés par Édouard Seymour et William Paget, acceptent de retarder l'annonce de la mort du roi afin de pouvoir prendre des mesures pour une succession en douceur. Seymour et Anthony Browne, le maître de cavalerie, vont chercher Édouard à Hertford et l'amènent à Enfield, où vit la princesse Élisabeth. Le prince et la princesse sont alors informés de la mort de leur père et entendent la lecture du testament. Le 31 janvier, le "lord" chancelier, Thomas Wriothesley, annonce officiellement la mort d'Henri au Parlement et lit les proclamations pour mettre en route sa succession par Édouard. Le nouveau roi est amené à la tour de Londres, où il est accueilli à . Le lendemain, les nobles du royaume viennent s'incliner devant le jeune roi à la tour de Londres, et on annonce que Seymour est nommé protecteur du Souverain. Henri VIII est enterré à Windsor le 16 février, dans la même tombe que Jeanne Seymour, comme il l'a souhaité.

Édouard VI est couronné à l'abbaye de Westminster quatre jours plus tard, le dimanche 20 février ; c'est le premier couronnement en Angleterre depuis près de 40 ans. Les cérémonies sont raccourcies en raison de leurs longueurs , et aussi parce que la Réforme a rendu certaines d'entre elles inappropriées. La veille du couronnement, Édouard se rend à cheval de la tour au palais de Westminster, acclamé par la foule qui se presse sur son passage, et le trajet est agrémenté de spectacles dont beaucoup sont basés sur des reconstitutions historiques pour un jeune roi précédent, Henri VI. Il rit au spectacle d'un funambule espagnol qui devant la cathédrale Saint-Paul. Au cours de la cérémonie, Cranmer confirme la suprématie royale et appelle Édouard un second Josué, lui demandant de poursuivre la réforme de l'Église d'Angleterre, . Après la cérémonie, Édouard préside un banquet dans la grande salle de Westminster, où, comme il le rappelle dans ses "Chroniques", il dîne avec sa couronne sur la tête.

Henri VIII a nommé seize exécuteurs testamentaires, qui doivent agir en tant que conseillers d'Édouard jusqu'à ce qu'il atteigne l'âge de 18 ans. Ces exécuteurs testamentaires sont assistés par douze hommes « de conseil » qui doivent seconder les exécuteurs testamentaires en cas de besoin. Les dernières volontés d'Henri VIII sur le sujet font l'objet de controverses. Certains historiens suggèrent que les proches du roi ont manipulé le souverain ou le testament pour pouvoir s'assurer le pouvoir, à la fois matériel et religieux. D'ailleurs, la composition de la Chambre privée a été changée vers la fin de 1546 en faveur des partisans de la Réforme. En outre, deux chefs conservateurs membres du Conseil privé sont retirés du pouvoir central. Étienne Gardiner s'est vu refuser l'accès au roi Henri lors de ses derniers mois. Thomas Howard, duc de Norfolk, s'est vu accuser de trahison. La veille de la mort du roi, ses vastes domaines sont saisis, ce qui les rend disponibles pour une redistribution, et le duc passe la totalité du règne d'Édouard à la tour de Londres. D'autres historiens soutiennent que l'exclusion de Gardiner est basée sur des éléments autres que religieux, que Norfolk n'est pas particulièrement conservateur au niveau religieux, que d'autres conservateurs sont restés au Conseil et que le radicalisme d'hommes tels qu', qui contrôle le timbre sec qui reproduit la signature du roi, est sujet à discussion. Quoi qu'il en soit, la mort d'Henri est suivie par une prodigieuse distribution de terres et d'honneurs au nouveau groupe au pouvoir. Le testament contient une clause de cadeaux illimités, ajoutée à la dernière minute, qui permet aux exécuteurs testamentaires d'Henri de distribuer librement terres et honneurs à eux-mêmes et aux membres de la Cour, en particulier à Édouard Seymour, premier comte de Hertford, qui est devenu le "lord" protecteur du royaume, gouverneur de la personne du roi et duc de Somerset.

En fait, Henri VIII ne prévoyait pas la nomination d'un protecteur. Il a confié le gouvernement du royaume pendant la minorité de son fils à un conseil de régence qui dirige collectivement, par décision majoritaire de ses membres qui ont tous les mêmes droits. Néanmoins, quelques jours après la mort d'Henri, le 4 février, les exécuteurs testamentaires choisissent d'investir Édouard Seymour, qui ne siège pas officiellement au Conseil, du pouvoir royal. Treize des seize conseillers (les autres sont absents) acceptent sa nomination comme protecteur, en justifiant leur décision commune « de par l'autorité » de la volonté d'Henri. Seymour a peut-être en fait passé un accord avec certains de ces exécuteurs testamentaires, qui ont presque tous reçus des pots-de-vin. Le fait est connu pour , secrétaire privé d'Henri VIII et il s'est assuré l'appui d’Anthony Browne, de la chambre privée.

La nomination de Seymour est conforme aux précédents historiques et son rôle est d'autant plus facilement admis qu'il est auréolé de ses succès militaires en Écosse et en France. En mars 1547, il obtient des lettres patentes du roi Édouard lui accordant le droit presque monarchique de nommer lui-même les membres du Conseil privé et de les consulter uniquement quand il le veut. Selon les mots de l'historien , . Seymour gouverne en grande partie par décret, ne laissant au Conseil privé guère plus de pouvoir que d'entériner ses décisions.

La prise du pouvoir par Somerset est efficace et sans heurt. L'ambassadeur impérial, , indique qu'il , Paget lui servant de secrétaire, mais il prédit des problèmes à John Dudley, vicomte Lisle, qui a récemment été nommé comte de Warwick lors de la distribution de titres. En fait, dans les premières semaines de son protectorat, Somerset n'est contesté que par le "lord" chancelier, Thomas Wriothesley, dont l'octroi du titre de comte de Southampton n'a évidemment pas suffi à acheter le silence, et par son propre frère. Religieux conservateur, Wriothesley s'oppose à l'hypothèse du pouvoir monarchique de Somerset sur le Conseil. Il se trouve alors brutalement démis de son poste et obligé de vendre certaines de ses charges aux autres délégués.

Comme dit précédemment, Somerset doit faire face à l'opposition difficilement contrôlable de son frère cadet, Thomas Seymour, qui est décrit comme un . Oncle du roi, Thomas Seymour demande à être nommé gouverneur du souverain et prétend à une plus grande part du pouvoir. Somerset tente d'acheter son frère en lui offrant une baronnie, le poste de "Lord" de l'Amirauté et un siège au Conseil privé mais Thomas continue à comploter pour prendre le pouvoir. Il commence par offrir de l'argent de poche au jeune roi à l'insu de tous et lui explique que son frère tient les cordons de la bourse trop serrés, faisant ainsi de lui un « roi mendiant ». Il invite également le souverain à se débarrasser de son protecteur dans un délai de deux ans et à , mais Édouard, habitué à s'en remettre au Conseil, ne coopère pas. Cependant, en avril, en utilisant l'appui d'Édouard pour contourner l'opposition de son frère, Thomas épouse secrètement Catherine Parr, la veuve d'Henri VIII, qui est notamment la tutrice de Jeanne Grey, âgée de 11 ans à l'époque, et de la princesse Élisabeth, 13 ans.

Au cours de l'été 1548, Catherine Parr, enceinte, découvre Thomas embrassant la princesse Élisabeth. Par la suite, Élisabeth est retirée de chez elle et transférée chez Anthony Denny. En septembre, Catherine Parr meurt en couches et Thomas Seymour reprend rapidement sa cour auprès d'Élisabeth, lui écrivant, lui proposant de l'épouser. Élisabeth répond positivement à ses avances mais se heurte au refus d'Édouard, qui n'est pas prêt à accepter une telle union, sauf autorisation du Conseil. En janvier 1549, le Conseil fait arrêter Thomas Seymour pour des motifs divers, comme des détournements de fonds à la Monnaie Royale de Bristol. Le jeune roi Édouard, que Seymour est accusé d'avoir voulu marier à Jeanne Grey, témoigne au sujet de l'argent de poche. L'absence de preuves claires de trahison excluant toute possibilité de procès, Seymour est condamné à mort par une loi votée par le Parlement ("") que le roi doit signer et est décapité le 20 mars 1549.

Somerset est un habile soldat, comme il l'a démontré au cours de son expédition en Écosse et lors de la défense de Boulogne-sur-Mer, en 1546. Dès le début de ses fonctions, son intérêt en tant que protecteur se porte principalement sur la guerre contre l'Écosse. Après une victoire écrasante à la bataille de Pinkie Cleugh, en septembre 1547, il a créé un réseau de garnisons en Écosse, qui s'étend au nord jusqu'à Dundee. Malgré ses premiers succès, il perd rapidement le contrôle de la situation et son objectif d'union des deux royaumes par la conquête du second apparaît de plus en plus irréaliste. Les Écossais s'allient à la France, qui envoie des renforts pour défendre Édimbourg en 1548, tandis que Marie, reine d'Écosse, est enlevée et envoyée en France où elle est fiancée au dauphin. Le coût du maintien d'importantes troupes et garnisons en Écosse devient également une charge insupportable pour les finances royales. Enfin, une attaque française sur Boulogne en août 1549 contraint Somerset à entamer un retrait d'Écosse.

Au cours de l'année 1548, l'Angleterre connaît des troubles sociaux. À partir du mois d'avril 1549, éclate une série de révoltes armées, alimentées par divers griefs religieux et agraires. Les deux rébellions les plus graves, qui nécessitent une vaste intervention militaire pour y mettre fin, ont lieu dans le Devon et les Cornouailles d'un côté et dans le Norfolk de l'autre. La première, appelée parfois la « révolte du Livre de la prière », est surtout due à l'obligation faite de célébrer les offices religieux en anglais et non plus en latin, et la , dirigée par un commerçant appelé Robert Kett, principalement à l'empiètement de grands propriétaires sur les pâturages communaux. La complexité de cette agitation sociale vient de ce que les manifestants pensent agir légitimement contre les propriétaires avec l'appui du lord protecteur, convaincus que les propriétaires sont les contrevenants.

Le même soutien aux foyers d'agitation s'exprime dans tout le pays et pas seulement dans le Norfolk et à l'ouest. L'origine du fait que la population croit que Somerset la soutient se trouve en partie dans sa série de décrets parfois libéraux, souvent contradictoires, et en partie par le manque de coordination des activités des commissions qu'il a envoyées en 1548 et 1549 pour enquêter sur les griefs liés aux pertes de terrains, empiétement des grands troupeaux de moutons paissant sur des terres communes, et autres questions similaires. Les commissions d'enquête diligentées par Somerset sont dirigées par un député protestant évangélique appelé , dont la rhétorique libérale lie la question des clôtures à la théologie réformée et à la notion de communauté divine. Les groupes locaux supposent souvent que les conclusions de ces commissions leur donnent le droit d'agir eux-mêmes contre les propriétaires en infraction. Le roi Édouard écrit dans sa "Chronique" que les soulèvements de 1549 commencent .

Quel que soit le point de vue de Somerset, les événements catastrophiques de 1549 sont considérés comme une preuve d'un échec colossal de son gouvernement et le Conseil lui en fait porter la responsabilité. En juillet 1549, Paget écrit à Somerset : 

Les événements qui conduisent à la chute de Somerset ont souvent été qualifiés de « coup d'État ». Le octobre 1549, Somerset est averti que son pouvoir est fortement menacé. Il émet une proclamation appelant à l'aide, s'empare de la personne du roi et se retire, par mesure de sécurité, dans l'enceinte fortifiée du château de Windsor, où Édouard écrit, . Pendant ce temps, l'ensemble du Conseil publie le détail des erreurs de gouvernement de Somerset. Les conseillers déclarent clairement que les pouvoirs du protecteur viennent d'eux et pas de la volonté d'Henri VIII. Le 11 octobre, le Conseil fait arrêter Somerset et amène le roi à Richmond. Édouard résume les accusations portées contre Somerset, dans sa "Chronique" : En février 1550, John Dudley, comte de Warwick, émerge comme chef du Conseil et est nommé successeur de Somerset. Bien que ce dernier soit libéré de prison et renommé au Conseil, il est exécuté pour trahison en janvier 1552 après avoir comploté pour renverser le gouvernement de Dudley. Édouard note ainsi la mort de son oncle dans sa "Chronique" : .

Les historiens ont des opinions contrastées sur le gouvernement de Somerset, opposant les capacités d'organisation de certains de ses alliés tels que Paget, son directeur de cabinet, avec l'ineptie de son propre commandement. À l'automne 1549, ses guerres coûteuses ont perdu leur effet d'entraînement, menant la Couronne à la ruine financière et conduisant à des émeutes et des révoltes dans tout le pays. Jusqu'à ces dernières décennies, la réputation de Somerset auprès des historiens était pourtant très bonne, en raison de ses nombreuses proclamations en faveur du peuple contre une classe de propriétaires terriens avides. Plus récemment, cependant, on le dépeint souvent comme un dirigeant arrogant et distant, manquant de compétences politiques et administratives.

Contrairement à Somerset, John Dudley, comte de Warwick, qui est fait duc de Northumberland en 1551, était autrefois considéré par les historiens comme un intrigant qui s'était élevé et enrichi au détriment de la Couronne. Depuis les années 1970, les réalisations administratives et économiques de son gouvernement sont reconnues et il est maintenant crédité de la restauration de l'autorité du Conseil royal et du retour des finances à l'équilibre après le désastre du protectorat de Somerset.
Le chef des conservateurs, Thomas Wriothesley, de Southampton, et ses partisans s'allient aux partisans de Dudley pour créer un Conseil uni, que Wriothesley et des observateurs comme l'ambassadeur de Charles Quint, l'empereur du Saint-Empire, pensent pouvoir utiliser pour inverser la politique religieuse de Somerset. Warwick, pour sa part, pense que les convictions protestantes du roi sont suffisamment solides et explique qu'Édouard est mûr pour se prononcer librement mais, surtout, il profite de la situation pour prendre discrètement avec ses fidèles le contrôle de la Chambre privée. Paget, qui reçoit une baronnie, passe dans le camp de Warwick car il se rend compte que la politique conservatrice ne peut pas faire revenir Charles Quint sur son soutien aux Français dans l'affaire de Boulogne. Southampton, en préparant le dossier à charge pour faire condamner Somerset, cherche à discréditer Warwick en expliquant que toutes les décisions de Somerset ont été prises avec son appui. En réaction, Warwick convainc le Parlement de libérer Somerset, ce qui est fait le 14 janvier 1550. Warwick peut alors exclure Southampton et ses partisans du Conseil après avoir obtenu le soutien d'une majorité de ses membres en échange de titres et se faire nommer "lord" président du Conseil et grand maître de la maison du roi. Même s'il n'a pas le titre de protecteur, il est maintenant clairement à la tête du gouvernement.

En grandissant, Édouard s'intéresse de plus en plus aux affaires du gouvernement. Cependant, son implication effective dans les décisions a longtemps été sujette à débat et les historiens du ont envisagé toute une série de possibilités, faisant de lui , selon Stephen Alford. Un « Conseil d'État » spécial est créé lorsque Édouard a quatorze ans et le roi en choisit lui-même les membres. Édouard assiste aux réunions hebdomadaires du Conseil, . Un des principaux points de contact du roi avec le pouvoir est la Chambre privée et Édouard y travaille en étroite collaboration avec William Cecil et , les deux secrétaires principaux. L'influence du roi est très forte en matière de religion et le Conseil applique la politique très anglicane qu'Édouard encourage.

Le mode de gouvernement du duc de Northumberland est très différent de celui de Somerset. Soucieux de gouverner avec une majorité de conseillers, il encourage le Conseil à travailler et l'utilise pour légitimer son autorité. Sans lien du sang avec le roi, il prend la précaution de faire entrer quelques-uns de ses partisans au Conseil pour le contrôler. Il ajoute également des membres de sa famille à la maison royale. Il comprend que, pour dominer personnellement, il lui faut prendre un contrôle total du Conseil. Selon les mots de l'historien John Guy, .

La politique de Warwick en matière de guerre est plus pragmatique que celle de Somerset, ce qui lui vaut des reproches de faiblesse. En 1550, il signe le traité d'Outreau avec la France, accepte de se retirer de Boulogne et rappelle toutes les garnisons anglaises d'Écosse. En 1551, Édouard est fiancé à Élisabeth de Valois, fille du roi Henri II de France. Pragmatique, il se rend compte que l'Angleterre ne peut plus supporter le coût des guerres. À l'intérieur, il prend des mesures de police contre les troubles locaux. Pour prévenir les révoltes futures, il laisse en permanence des représentants de la couronne dans les localités, dont les "lords" lieutenants, qui commandent les forces militaires et font rapport au gouvernement central.

Avec et , Warwick s'attaque à l'état désastreux des finances du royaume. Son premier gouvernement succombe à la tentation d'une solution rapide et dévalue la monnaie. Le désastre économique qui s'ensuit pousse Warwick à suivre les propositions de Thomas Gresham. En 1552, la confiance dans la monnaie est restaurée, les prix baissent fortement et le commerce se développe. Mais si la reprise économique complète n'est atteinte que sous le règne d'Élisabeth , son origine se trouve dans la politique du duc de Northumberland. Le régime sévit également contre les détournements de fonds généralisés des finances publiques et effectue un examen approfondi des pratiques de recouvrement des recettes, qui est .

En matière de religion, le gouvernement de Northumberland suit la même politique que celui de Somerset, soutenant un programme de plus en plus vigoureux de réforme. Bien que l'influence pratique d'Édouard VI sur le gouvernement soit limitée, son intense anglicanisme rend la mise en place d'une administration réformée obligatoire. Sa mise en œuvre est menée par le parti de la réforme, qui monte en puissance tout au long du règne d'Édouard. L'homme qui a le plus la confiance d'Édouard, Thomas Cranmer, archevêque de Cantorbéry, présente une série de réformes religieuses qui révolutionnent l'Église anglaise la menant – en rejetant la suprématie du pape – d'un culte profondément romain à une institution réformée. La confiscation des biens de l'Église qui avait commencé sous Henri VIII reprend sous Édouard – avec notamment le détournement des recettes des offices au plus grand profit de la couronne et des nouveaux propriétaires des biens saisis. La réforme de l'Église doit donc être autant considérée comme une réforme politique que religieuse sous le règne d'Édouard VI. À la fin de son règne, l'Église catholique est ruinée, la plus grande partie des propriétés des évêques étant transférée dans les mains de l'État.

Les convictions religieuses de Somerset et de Northumberland ne sont pas connues des historiens, qui sont partagés sur la sincérité de leur protestantisme. Il y a moins de doute, cependant, sur la dévotion — certains ont parlé du fanatisme — du roi qui dit lire chaque jour douze chapitres des Écritures et apprécier les sermons, et a été qualifié par John Foxe de « lutin pieux». Édouard est représenté au cours de sa vie et par la suite comme un nouveau Josias, ce roi biblique qui détruisit les idoles de Baal. Il semble excessif dans son anti-catholicisme et demande une fois à Catherine Parr de convaincre la princesse Marie . Le biographe d'Édouard, Jennifer Loach, demande toutefois de ne pas accepter trop facilement l'image pieuse d'Édouard répandue par les réformateurs, comme dans le livre très influent de John Foxe, "", où une gravure représente le jeune roi écoutant un sermon prononcé par Hugh Latimer. Dans la première partie de sa vie, Édouard suivait les pratiques catholiques en vigueur comme assister à la messe ; mais il se laisse convaincre, par Cranmer et d'autres réformateurs figurant parmi ses tuteurs et courtisans, qu'il doit imposer la « vraie » religion en Angleterre.
La Réforme de l'Église anglaise avance sous la pression de deux tendances différentes : les traditionalistes d'une part et les intégristes de l'autre, qui vont jusqu'à mener des opérations de destruction de représentations pieuses et se plaignent que la Réforme ne va pas assez loin. La doctrine réformée devient doctrine officielle, notamment le principe de "Sola fide" et la communion pour les laïcs sous les deux espèces du pain et du vin comme pour le clergé. Le rite de l'ordinal de 1550 dû à Cranmer remplace l'ordination divine des prêtres avec un système de nomination géré par le gouvernement, autorisant les « ministres » à prêcher l'Évangile et administrer les sacrements, plutôt que, comme avant, à . Cranmer se fixe la tâche d'écrire une liturgie valable pour tous en anglais, détaillant tous les offices quotidiens et hebdomadaires et les fêtes religieuses, dont l'application est rendue obligatoire dans l", la loi d'uniformité de 1549. Le ', le « Livre de la prière commune » de 1549, conçu comme un compromis, est attaqué par les traditionalistes car il supprime de nombreux rituels liturgiques catholiques, tels que l'élévation du pain et du vin, alors que d'autres réformateurs se plaignent de voir conserver trop d'éléments « papistes » comme les restes de rites pénitentiels précédant la communion. Le Livre de la prière est également contesté par de nombreux responsables catholiques comme Étienne Gardiner, évêque de Winchester, et Edmund Bonner, évêque de Londres, qui sont tous deux emprisonnés à la tour de Londres et, avec d'autres religieux, privés de leur siège.

Après 1551, la Réforme va plus loin, avec l'approbation et les encouragements d'Édouard qui commence à exercer son rôle de chef suprême de l'Église. Les changements sont également une réponse aux critiques de réformateurs tels que John Hooper, évêque de Gloucester, et l'Écossais John Knox qui est employé comme « ministre » de Newcastle par le duc de Northumberland, et dont les prédications invitent la cour du roi à s'opposer à la communion à genoux. Cranmer est également influencé par les opinions d'origines continentales des réformateurs Martin Bucer, qui meurt en Angleterre en 1551, Pierre Martyr qui enseigne à Oxford et d'autres théologiens étrangers. Les progrès de la Réforme s'accélèrent avec la nomination de nouveaux évêques réformateurs. Au cours de l'hiver de 1551 à 1552, Cranmer réécrit le Livre de la prière commune en termes moins ambigus, révise le droit canonique et prépare un énoncé de la doctrine, les « Quarante-deux articles », pour préciser les pratiques de la religion réformée, en particulier sur le problème de la communion qui divise la communauté religieuse. La formulation de Cranmer suivant la religion réformée et privant la communion de toute notion de présence réelle de Dieu dans le pain et le vin, supprime en fait la messe. Selon Elton, la publication du Livre de la prière révisé en 1552, soutenu par une deuxième loi d'uniformité, . Le Livre de la prière de 1552 reste le livre fondamental des offices de l'Église d'Angleterre. Cependant, Cranmer ne peut mettre en œuvre toutes ces réformes lorsqu'il devient évident, au printemps 1553, que le roi Édouard, de qui dépend toute la Réforme en Angleterre, est mourant.

En janvier 1553, Édouard VI tombe malade et, en juin, après plusieurs périodes d'améliorations et de rechutes, se trouve dans un état désespéré. Or, si, après sa mort, le roi est remplacé par sa sœur catholique Marie, la Réforme anglaise risque d'être en péril et les conseillers du souverain ont de nombreuses raisons de craindre une telle situation. Pourtant, Édouard est également opposé à voir son autre sœur Élisabeth lui succéder : il ne veut pas légitimer la montée d'une femme sur le trône. Dans son projet de testament politique intitulé ", « Ma conception de la succession », Édouard entreprend de modifier sa succession, il écarte les princesses Marie et Élisabeth et veut transmettre la Couronne à sa cousine, Jeanne Grey, 16 ans, qui s'en était vue écartée. Le 21 mai 1553, au cours d'un triple mariage « avec un faste royal», Jeanne épouse Guilford Dudley, fils cadet du duc de Northumberland, tandis que les sœurs de Jeanne et de Guildford, prénommées toutes les deux Catherine, épousent respectivement le comte de Pembroke et l'héritier du comte de Huntingdon.

Au début du mois de juin, Édouard supervise personnellement l'élaboration de la version finale de son testament par les avocats et il appose sa signature à . Puis, le 15 juin, sous l'œil vigilant du duc de Northumberland, il convoque les hauts magistrats et les avocats autour de son lit, leur ordonnant de faire allégeance et de traduire ses volontés sous forme de lettres patentes qu'il veut voir adoptées par le Parlement. Puis, il oblige ses conseillers et les avocats à signer un document en sa présence, par lequel ils s'engagent à exécuter fidèlement ses volontés après sa mort. Plus tard, lorsqu'il présente ses excuses à la reine Marie pour son rôle dans l'affaire quelques mois plus tard, le juge en chef Édouard Montagu rappelle que, lorsque ses collègues et lui ont soulevé des objections juridiques à la validité du document, Northumberland les avait menacés . Plus tard, Édouard leur demande personnellement leur obéissance et Montagu entend un groupe de seigneurs debout derrière lui conclure . Enfin, le 21 juin, le document est signé par plus d'une centaine de notables, comme les conseillers du roi, les pairs, des archevêques, des évêques et les shérifs. Beaucoup d'entre eux prétendent plus tard qu'ils ont été victimes d'intimidation de la part de Northumberland et trois comtes acceptent de donner leur accord contre des concessions substantielles de terre.

Tout le monde sait maintenant qu'Édouard est mourant et certains commencent à vouloir éliminer Marie. La France, pour qui la perspective de voir la cousine de l'empereur sur le trône d'Angleterre apparaît désagréable, soutient Northumberland. Les diplomates étrangers, même si certains pensent que l'écrasante majorité du peuple anglais soutient Marie, sont néanmoins convaincus que Jeanne sera nommée reine sans problèmes : , écrit ainsi Simon Renard à Charles Quint.
Henri VIII a créé un précédent lorsqu'il a lui-même nommé et exclu ses héritiers de son propre chef, indépendamment des règles de succession traditionnelles. Édouard s'est souvent exercé à élaborer des documents politiques. Dans sa dernière année d'existence, il met de plus en plus en pratique cette méthode pour participer à l'activité réelle du gouvernement. Ce document est son premier projet pour . Édouard prévoit, s'il n'a pas lui-même de descendant, d'avoir une succession faite uniquement d'héritiers mâles, c'est-à-dire des fils de Jeanne Grey ou de ses sœurs. À l'approche de sa mort et peut-être convaincu par Northumberland, il accepte de modifier le testament pour que Jeanne et ses sœurs puissent elles-mêmes être reines. Pourtant, Édouard considère ce cas comme une exception à la règle du sexe masculin, exigée par la réalité, un exemple à ne pas suivre si elle ou ses sœurs n'ont que des filles. Logiquement, en suivant ce raisonnement, c'est Frances Grey, duchesse de Suffolk, mère de Jeanne et nièce d'Henri VIII, qui devrait être désignée comme héritière d'Édouard, mais écartée au profit de ses enfants par la volonté d'Henri, elle semble avoir renoncé à ses droits après une visite à Édouard. Les lettres patentes du 21 juin excluent ses deux demi-sœurs, que son père a déclaré bâtardes, de sa succession ; il ne peut écarter Marie la catholique, sans faire de même pour Élisabeth la protestante. Les dispositions modifiant la succession sont une violation du "" de 1543 et certains les considèrent comme le résultat d'un raisonnement précipité et illogique.

Pendant des siècles, cette tentative de modifier la succession a surtout été considérée comme l'œuvre d'un seul homme : le duc de Northumberland. Depuis les années 1970, cependant, de nombreux historiens attribuent cette modification et l'insistance de sa mise en œuvre au seul roi. a établi qu'Édouard adolescent , tandis que David Starkey déclare : . Parmi les autres membres de la Chambre privée, John Gates, ami intime de Northumberland, est soupçonné d'avoir suggéré au souverain de changer le testament de telle sorte que Jeanne Grey puisse hériter de la Couronne, et pas seulement ses fils. Quelle que soit sa part de contribution au texte, Édouard est convaincu que sa parole est la loi et souscrit pleinement à la décision de déshériter ses demi-sœurs : .

Édouard tombe malade en janvier 1553 après avoir disputé une partie de raquettes par un froid glacial. Il a de la fièvre, tousse et son état général se dégrade progressivement. L'ambassadeur de Charles Quint, , indique : Début avril, Édouard se sent suffisamment bien pour aller prendre l'air dans le parc de Westminster et se rendre jusqu'à Greenwich mais, à la fin du mois, il est de nouveau très faible. Le 7 mai, il a beaucoup changé et ses médecins ne doutent pas qu'il va guérir. Quelques jours plus tard, le roi regarde les bateaux sur la Tamise, assis à sa fenêtre. Cependant, il rechute et, le 11 juin, Scheyfve, qui a un informateur dans la maison du roi, indique que . Maintenant, ses médecins croient qu'il souffre d'une et admettent qu'il est au-delà de toute possibilité de guérison. Bientôt, ses jambes sont tellement œdématiées qu'il doit rester allongé sur le dos et il a perdu toute volonté de résister à la maladie. Pour son tuteur, John Cheke, il aurait murmuré : .

Édouard fait sa dernière apparition en public le juillet, quand il se montre à la fenêtre de son palais de Greenwich, terrifiant ceux qui le voient dans un tel état. Les deux jours suivants, une foule nombreuse vient au palais dans l'espoir de voir le roi à nouveau, mais le troisième jour, on leur dit que le temps est trop froid pour qu'il puisse apparaître. Édouard décède à l'âge de 15 ans, au palais de Greenwich, le 6 juillet 1553. Selon le récit légendé de sa mort par John Foxe, ses derniers mots sont : . Il est enterré dans la chapelle de la Vierge à l'abbaye de Westminster le 8 août 1553 selon le rite réformé célébré par Thomas Cranmer. Le cortège est conduit par un et les Londoniens regardent passer le char funèbre, couvert d'un drap d'or, surmonté d'une effigie du souverain, de la couronne, du sceptre et de la jarretière. Au même moment, la reine Marie assiste à une messe pour son âme à la tour de Londres où Jeanne Grey est déjà emprisonnée.

La cause du décès d'Édouard VI n'est pas connue avec certitude. Comme de nombreux autres décès royaux au , les rumeurs d'empoisonnement abondent, mais aucune preuve n'est trouvée à cet appui. Le duc de Northumberland, dont l'impopularité va apparaître dans les événements qui suivent la mort du roi, est largement soupçonné d'avoir ordonné l'empoisonnement. Une autre théorie estime qu'Édouard a été empoisonné par les catholiques qui cherchent à porter Marie sur le trône. Le chirurgien qui a ouvert la poitrine d'Édouard après sa mort a constaté que . L'ambassadeur de Venise prétend que le roi est mort de « consomption », en d'autres termes de tuberculose : un diagnostic accepté par de nombreux historiens. Skidmore estime qu'Édouard a contracté la tuberculose après son épisode de rougeole et de variole en 1552 qui a supprimé son immunité naturelle à la maladie. Loach suggère plutôt que ses symptômes étaient typiques d'une bronchopneumonie aiguë, conduisant à une , une septicémie, et une insuffisance rénale.

La princesse Marie, qui a vu son frère pour la dernière fois en février, est tenue informée de son état de santé par Northumberland et par ses contacts avec les ambassadeurs impériaux. Charles Quint lui conseille d'accepter le trône, même si elle doit changer de religion pour y accéder. Consciente de la mort imminente d'Édouard, elle quitte son domicile de , près de Londres, et se retire sur ses terres près de Kenninghall dans le Norfolk, où elle sait qu'elle peut compter sur le soutien de la population. Northumberland envoie des navires sur la côte du Norfolk pour l'empêcher de s'échapper ou de permettre l'arrivée de renforts du continent. Il retarde l'annonce de la mort du roi, alors qu'il organise ses troupes et Jeanne Grey est conduite à la tour le 10 juillet. Le même jour, elle est proclamée reine alors que dans les rues de Londres la foule murmure son mécontentement. Le Conseil privé reçoit un message de Marie affirmant ses « droit et titre » au trône et ordonnant au Conseil de la proclamer reine, comme elle s'est déjà elle-même proclamée. Le Conseil lui répond que Jeanne est reine de par la volonté d'Édouard et Marie, en revanche, n'a pas droit au titre et n'est soutenue que par .

Northumberland se rend vite compte qu'il avait mal calculé sa façon d'agir, au moins pour avoir omis de placer Marie sous son contrôle avant la mort d'Édouard. Même si beaucoup de ceux qui se rallient à Marie sont des conservateurs souhaitant la défaite de l'anglicanisme, ses partisans, pour qui elle est l'héritière légitime du trône en dehors de toutes considérations religieuses, sont également nombreux. Northumberland est obligé de renoncer au contrôle d'un Conseil nerveux à Londres et de se lancer à la poursuite imprévue de Marie dans l'Est-Anglie, d'où arrivent des nouvelles du soutien croissant qu'elle reçoit, y compris d'un certain nombre de nobles, de gentilshommes et d'. Le 14 juillet, Northumberland quitte Londres avec trois mille hommes, pour atteindre Cambridge le lendemain. En attendant, Marie rassemble ses forces au château de Framlingham, dans le Suffolk, où elle a levé une armée de près de vingt mille hommes le 19 juillet.

Le Conseil privé comprend qu'il a commis une grave erreur. Dirigé par le comte d'Arundel et le comte de Pembroke, le Conseil proclame publiquement Marie reine. Le règne de neuf jours de Jeanne a pris fin. La proclamation déclenche une joie délirante dans tout Londres. Bloqué à Cambridge, Northumberland doit proclamer lui-même à son tour Marie reine, obligé de le faire par une lettre du Conseil. William Paget et le comte d'Arundel se rendent à Framlingham demander pardon à Marie et Arundel arrête Northumberland le 24 juillet. La nouvelle souveraine commence par faire emprisonner Jeanne Grey et son époux dans les geôles de la tour de Londres sous l'inculpation de haute trahison puis décapiter le 22 août, le duc de Northumberland, peu de temps après qu'il a renoncé à l'anglicanisme. Son exécution consterne sa bru, Jeanne, qui le suit sur l'échafaud le 12 février 1554, après l'implication de son père dans la rébellion de Wyatt.

Bien qu'Édouard n'ait régné que six ans et soit mort à l'âge de 15 ans, son règne a apporté une contribution durable à la Réforme anglaise et à la structure de l'Église d'Angleterre. La dernière décennie du règne d'Henri VIII avait vu l'abandon d'une partie de la Réforme et le retour à des valeurs plus conservatrices. En revanche, le règne d'Édouard a connu des progrès radicaux. Au cours des six années de son règne, l'Église anglaise passe d'une liturgie essentiellement catholique à ce qui est habituellement considéré comme un culte à la fois catholique et réformé. En particulier, l'introduction du Livre de la prière commune, l'ordinal de 1550, et les Quarante-deux articles de Cranmer constituent toujours la base des pratiques de l'Église d'Angleterre. Édouard lui-même a pleinement approuvé ces changements, et s'ils sont l'œuvre de réformateurs tels que Thomas Cranmer, Hugh Latimer et Nicholas Ridley, soutenus par un Conseil du roi résolument évangélique, le roi a été un catalyseur dans l'accélération de la réforme au cours de son règne.

La reine Marie, qui a tenté de défaire le travail de réforme de son frère, a dû faire face à des obstacles majeurs. En dépit de sa croyance en la suprématie du pape, elle a approuvé la Constitution qui l'a faite chef suprême de l'Église d'Angleterre, une contradiction qui va la gêner. Elle s'est trouvée incapable de récupérer un grand nombre de propriétés ecclésiastiques qui ont été données ou vendues à des propriétaires privés. Même si elle fait périr sur le bûcher un certain nombre des principaux chefs de l'Église anglicane, de nombreux réformateurs, soit en exil, soit restés subversivement actifs en Angleterre au cours de son règne, vont mener une propagande qu'elle est incapable d'enrayer. Néanmoins, l'anglicanisme n'est pas encore » du peuple anglais et, si elle avait vécu plus longtemps, il est probable que sa tentative de reconstruction de l'Église catholique aurait pu réussir, faisant du règne d'Édouard, plutôt que du sien, une aberration historique.

À la mort de Marie en 1558, la Réforme anglaise repart de l'avant et la plupart des réformes mises en place pendant le règne d'Édouard sont réintégrées dans le règlement élisabéthain. La reine Élisabeth remplace les conseillers et les évêques nommés par sa sœur par ceux de son frère, comme William Cecil, ancien secrétaire de Northumberland, et Richard Cox, ancien précepteur du roi, qui prononce un discours contre l'Église catholique à l'ouverture du Parlement en 1559. Le Parlement adopte une nouvelle loi d'uniformité au printemps suivant qui restaure, avec quelques modifications, le Livre de la prière de Cranmer de 1552 ; et les Trente-neuf articles de 1563 sont en grande partie basés sur les Quarante-deux articles de Cranmer. Les développements théologiques du règne d'Édouard fournissent une source essentielle de référence pour la politique religieuse d'Élisabeth, bien que l'internationalisme de la Réforme d'Édouard n'ait jamais été repris.

Édouard VI a été joué à l'écran par :




</doc>
<doc id="16767" url="https://fr.wikipedia.org/wiki?curid=16767" title="Loi de Hubble">
Loi de Hubble

En astronomie, la loi de Hubble énonce que les galaxies s'éloignent les unes des autres à une vitesse approximativement proportionnelle à leur distance. Autrement dit, plus une galaxie est loin de nous, plus elle semble s'éloigner rapidement. Cette loi ne concerne que la partie de l'univers accessible aux observations. L'extrapolation de la loi de Hubble sur des distances plus grandes est possible, mais uniquement si l'univers demeure homogène et isotrope sur de plus grandes distances.

Il s'agit là d'un mouvement d'ensemble des galaxies de l'univers. À celui-ci se superposent les mouvements propres acquis par les galaxies du fait de leurs interactions gravitationnelles avec leurs voisines. Par exemple, la Voie lactée forme un système gravitationnellement lié avec la galaxie d'Andromède qui ont toutes deux une orbite elliptique très allongée qui fait qu'actuellement, la galaxie d'Andromède s'approche de nous. De même, la Voie lactée et la galaxie d'Andromède se rapprochent peu à peu du superamas de la Vierge. Néanmoins, au-delà d'une certaine distance, le mouvement général d'expansion l'emporte sur les mouvements propres, et toutes les galaxies lointaines s'éloignent de nous.

La loi de Hubble tire son nom de l'astronome américain Edwin Hubble qui la publia en 1929. Elle fut la première preuve de l'expansion de l'Univers, un phénomène générique prédit par la relativité générale, et du Big Bang, le modèle cosmologique qui en résulte le plus naturellement. Hubble découvrit cette loi en observant un décalage vers le rouge presque systématique dans les galaxies dont il avait découvert auparavant la nature exacte à l'aide de l'observation d'un certain type d'étoiles variables, les céphéides. Ces étoiles sont sujettes à des variations de luminosité dont la période est reliée à la luminosité absolue suivant une loi établie par l'astronome Henrietta Leavitt au début du . L'observation de la période de variation des céphéides dans une autre galaxie permettait ainsi de déduire leur distance relative. La vitesse de fuite de ces mêmes galaxies était, elle, mesurée par l'observation d'un décalage vers le rouge de leur spectre, effet interprété comme étant dû à leur mouvement de fuite (voir effet Doppler-Fizeau).

C'est en comparant ce décalage à la distance de ces galaxies, qu'il trouva une relation linéaire entre les deux, annoncée en 1929. Pour cette raison, la paternité de la loi de Hubble est attribuée à Edwin Hubble, d'où son nom. Cependant, deux ans plus tôt, Georges Lemaître avait prédit l'existence de cette loi en étudiant un type de modèle issu de la relativité générale. Dans son article écrit en français et publié dans les Annales de la société scientifique de Bruxelles, il indique clairement que cette loi qu'il a prédite est vérifiée par les observations dont il dispose (pour la plupart œuvres de Hubble et Gustaf Strömberg). Étant publié en français, et traduit en anglais par Arthur Eddington après la publication des résultats de Hubble (en 1931), ce résultat de Lemaître est resté inaperçu, d'autant que la traduction anglaise de son article par Eddington est étrangement amputée de la phrase clé qui énonce la relation. Il a cependant été prouvé depuis, par Mario Livio, que Lemaître avait traduit lui-même sa publication en anglais, et s'était donc auto-censuré pour éviter une polémique liée à cette découverte, en laissant donc l'honneur de la découverte à Hubble.

La vitesse de récession apparente "v" des galaxies étant déduite de la formule Doppler et sa distance "d" mesurée par les céphéides, la loi de Hubble s'écrit simplement
où formula_2 est la constante de Hubble, la lettre "H" étant bien sûr utilisée en l'honneur de Hubble. L'indice 0 est utilisé pour indiquer la valeur de la constante à l'instant présent. Celle-ci en effet n'est pas constante dans le temps. Elle diminue très rapidement dans le temps. Cependant, depuis quelques milliards d'années, le facteur d'échelle d augmente plus vite que H diminue, donc il y a accélération de l'expansion.

On peut le cas échéant remplacer la vitesse "v" par sa valeur déduite du décalage vers le rouge "z" et la vitesse de la lumière "c" pour obtenir 

Ces deux lois ne sont valables que pour de faibles valeurs de la vitesse, donc pour des distances relativement faibles. On sait de nos jours que l'interprétation du décalage vers le rouge en termes d'effet Doppler n'est pas correcte physiquement puisque l'augmentation de distance au cours du temps entre deux galaxies n'est pas due à la vitesse des galaxies dans un espace fixe mais plutôt à un étirement de l'espace lui-même, les galaxies restant "fixes" dans cet espace. Il faut donc faire une analyse différente. On trouvera plus loin des détails sur l'interprétation physique de la loi de Hubble et les modifications à la loi de Hubble qui en résultent.

Si l'on se restreint à l'application de la loi de Hubble dans l'univers local (quelques centaines de millions d'années lumière), alors il est tout à fait possible d'interpréter la loi de Hubble comme un mouvement des galaxies "dans" l'espace. Néanmoins, la loi énonçant une vitesse de récession apparente proportionnelle à la distance, son extrapolation conduit à conclure que des galaxies suffisamment lointaines s'éloignent de nous à une vitesse plus grande que la vitesse de la lumière, en contradiction apparente avec la relativité restreinte. De fait, ce n'est pas dans le cadre de la relativité restreinte que l'on doit appliquer la loi de Hubble, mais celui de la relativité générale. Celle-ci stipule entre autres que le concept de vitesse relative entre deux objets (deux galaxies distantes, par exemple), est un concept purement local : on ne peut mesurer la différence de vitesse entre deux objets que si leur trajectoires sont « suffisamment proches » l'une de l'autre. Il convient bien sûr de préciser ce dernier terme, qui en l'occurrence dit essentiellement que la notion de vitesse relative n'a de sens que dans une région de l'espace-temps qui peut être correctement décrite par une métrique de Minkowski. Il est en effet possible de montrer (voir Expansion de l'Univers) que l'échelle de longueur au-delà de laquelle on ne peut plus décrire localement un espace en expansion par une métrique de Minkowski est précisément le rayon de Hubble, soit la distance au-delà de laquelle les vitesses de récession apparentes sont précisément relativistes.

L'interprétation en termes de mouvement dans l'espace décrit par la relativité restreinte devient donc précisément invalide au moment où surgit le paradoxe d'une vitesse de récession supérieure à la vitesse de la lumière. Ce paradoxe est résolu dans le cadre de la relativité générale qui permet d'interpréter la loi de Hubble non pas comme un mouvement "dans" l'espace, mais une expansion "de" l'espace lui-même. Dans ce cadre-là, le postulat d'impossibilité de dépassement de la vitesse de la lumière fréquemment (et improprement) employé en relativité restreinte se reformule de façon plus exacte en énonçant qu'aucun signal ne peut se déplacer à une vitesse supérieure à celle de la lumière, les vitesses étant "localement" mesurées par des observateurs dans des régions où l'espace peut être décrit par la relativité restreinte (soit à petite échelle).

La valeur actuelle de la constante de Hubble est aujourd'hui (2013) mesurée à (70 kilomètres par seconde et par mégaparsec), avec une incertitude d'environ 10 % (soit ). Ce résultat est obtenu de façon consistante par de nombreuses méthodes : 
La valeur actuelle est considérablement plus basse que la valeur initiale trouvée par Hubble (de l'ordre de ). L'erreur commise par Hubble était due à une mauvaise estimation de la magnitude absolue des céphéides, aujourd'hui considérablement mieux connue (voir mesure des distances en astronomie).


Tant que l'on considère des galaxies dont la vitesse de récession est faible, leur distance à un observateur varie peu entre le moment où elles émettent leur lumière et le moment où celle-ci est reçue par l'observateur. De même, tant que le temps de propagation du signal lumineux est petit devant le temps caractéristique de l'expansion, le temps de Hubble, la vitesse de récession et le taux d'expansion varient peu sur cet intervalle. Ainsi, il n'y a pas d'ambiguïté dans la définition des quantités "v", formula_2, et "d". À grande distance, il convient de préciser ce que l'on entend par distance, et vitesse de récession. De plus, rien ne garantit "a priori" que la relation linéaire mentionnée plus haut reste valable. Il existe en fait des corrections à la loi de Hubble. Celles-ci jouent un rôle crucial en cosmologie car elles permettent en principe de reconstituer directement l'histoire récente de l'expansion.

Si l'on appelle "d" la distance qui nous sépare actuellement de la galaxie observée, on peut montrer que pour des décalages vers le rouge modérés, ces deux quantités sont reliées par la formule
où la quantité formula_6 est le paramètre de décélération de l'expansion, proportionnelle à la dérivée seconde du facteur d'échelle. 

Cette relation est importante car elle permet de mesurer le paramètre de décélération et par suite déduire la pression moyenne des différentes formes de matière qui composent l'univers.

En pratique, la quantité "d" n'est pas mesurable directement. Ce que l'on mesure, c'est soit la distance obtenue en comparant la luminosité apparente d'un astre à sa luminosité intrinsèque supposée connue, formula_7 (on parle alors de distance de luminosité), soit la distance obtenue en mesurant son diamètre apparent, sa taille réelle étant dans ce cas supposée connue, formula_8(on parle alors de distance angulaire). Dans ce cas, on exprime généralement les distances en fonction du "redshift" et non le contraire, et les formules s'écrivent :
En pratique pour des objets lointains, on n'utilise pas les formules ci-dessus, qui ne sont valides que pour des petits décalages vers le rouge. Voir les articles distance angulaire et distance de luminosité pour plus de détails.

Des réticences, initiées par Albert Einstein lui-même du fait de sa préférence pour un univers statique (voir univers d'Einstein), ont été formulées vis-à-vis de l'interprétation du décalage vers le rouge en termes de fuite des galaxies ou d'expansion de l'espace. Aucune des alternatives proposées n'est considérée comme viable aujourd'hui, en raison du manque de motivations théoriques sous-jacentes (il s'agit essentiellement de phénomènes "ad hoc" invoqués uniquement pour réinterpréter ces résultats, comme la lumière fatiguée) et qui échouent à proposer un modèle cosmologique rendant compte de l'ensemble des observations désormais disponibles (voir l'article expansion de l'Univers). Par exemple, la théorie de la lumière fatiguée échoue à expliquer le fait que le fond diffus cosmologique a un spectre de type corps noir.




</doc>
<doc id="16768" url="https://fr.wikipedia.org/wiki?curid=16768" title="Acaricide">
Acaricide

Un acaricide est une substance active ou une préparation phytopharmaceutique ayant la propriété de tuer les acariens. 

Selon leur mode d'action, les acaricides agissent en perturbant la respiration cellulaire, les phénomènes de croissance et de développement, ou le système nerveux.

Leur type d'action peut être ovicide (sur œufs d'hiver et/ou d'été), larvicide, adulticide, et parfois stérilisant sur les femelles.

Les acaricides sont vendus en pharmacie, directement par des laboratoires s'occupant de produits anti-allergiques, ou hors du circuit médical. Il faut se méfier de certains de ces derniers car ils peuvent avoir une action acaricide, mais sans tuer les œufs et les larves.



</doc>
<doc id="16770" url="https://fr.wikipedia.org/wiki?curid=16770" title="Ensemble de définition">
Ensemble de définition

En mathématiques, l'ensemble de définition d'une fonction dont l'ensemble de départ est noté et l'ensemble d'arrivée est l'ensemble des éléments de qui possèdent une image dans par , autrement dit l'ensemble des éléments de pour lesquels existe :

On dit de qu'elle est . L'ensemble de définition   est encore appelé domaine de définition (ou simplement domaine) de ; quand   est un simple intervalle, on peut l'appeler intervalle de définition.

Il ne faut pas confondre l'ensemble de définition   de la fonction avec son ensemble de départ . Il arrive toutefois que les deux soient égaux : la fonction est alors une application ; elle est alors dite ou .

À titre de contre-exemple, considérons la fonction
Cette fonction n'est pas définie en 0 : « formula_3 » n'existe pas.

L'ensemble de définition de cette fonction est donc formula_4. Il diffère de son ensemble de départ, formula_5 ; cette fonction n'est donc pas une application.

Cependant, il est toujours possible de transformer une fonction en application, par exemple en la "restreignant" à son domaine de définition. Cette restriction est notée habituellement formula_6. C'est une application par construction.

Ainsi, dans notre exemple, la fonction
est bien une application.

Une autre solution pour transformer une fonction en application consiste à la "prolonger", c'est-à-dire choisir une image dans l'ensemble d'arrivée pour chacun des éléments sans image de l'ensemble de départ. En particulier, si une fonction formula_8 n'est pas définie en un point formula_9, il est possible de la prolonger en ce point en la remplaçant par une autre fonction, appelée un prolongement de formula_8 en formula_9 et notée ici formula_12, et telle que :

Ainsi, dans notre exemple, on peut transformer la fonction formula_8 en application en la prolongeant à l'origine par (par exemple) : formula_22.

Remarque : assez souvent, pour alléger les notations, le prolongement est noté de la même manière que la fonction initiale. Cette ambiguïté est sans conséquence si le prolongement est explicité et remplace aussitôt et définitivement la fonction initiale.

Ensemble de définition d'une fonction multivaluée (autrement dit : d'une relation binaire)


</doc>
<doc id="16771" url="https://fr.wikipedia.org/wiki?curid=16771" title="Ovicide">
Ovicide

Un produit phytosanitaire ovicide est une substance active ou une préparation capable d'empêcher l'évolution des œufs, en tuant l'embryon.

En protection des cultures contre les parasites, la connaissance des caractéristiques ovicides des produits phytopharmaceutiques appliqués et de la date de début des pontes permettent de raisonner l'emploi des insecticides et des acaricides.


</doc>
<doc id="16772" url="https://fr.wikipedia.org/wiki?curid=16772" title="Larvicide">
Larvicide

Un produit phytosanitaire larvicide est une substance active ou une préparation ayant la propriété de tuer les larves.

En protection des cultures contre les parasites, la connaissance des caractéristiques larvicides des produits phytopharmaceutiques appliqués, et de la date de début d'apparition des larves, permettent de raisonner l'emploi des insecticides et des acaricides.



</doc>
<doc id="16773" url="https://fr.wikipedia.org/wiki?curid=16773" title="Musique assistée par ordinateur">
Musique assistée par ordinateur

La musique assistée par ordinateur (MAO) regroupe l'ensemble des utilisations de l'informatique comme outil associé à la chaîne de création musicale depuis la composition musicale jusqu'à la diffusion des œuvres, en passant par la formation pédagogique au solfège ou aux instruments.

La MAO fait son apparition auprès d'un public élargi dans les années 1970, lorsque les premières idées de synthétiseur associées à l'ordinateur voient le jour (Synclavier et Fairlight). Elle se démocratise dans les années 1980 grâce à la généralisation de la micro-informatique (ex. Commodore 64, Apple II et surtout Atari ST, premier ordinateur personnel, avec le CX5M de Yamaha, à intégrer une interface MIDI). Aujourd'hui tous les ordinateurs sont livrés avec une carte son et permettent donc potentiellement de composer, traiter, modifier le son ; des interfaces évoluées sont apparues (USB, Firewire, …) qui permettent de communiquer avec tout type d'instruments ou appareils audio et de les piloter. L'augmentation constante de la puissance des ordinateurs favorise l'arrivée de logiciels offrant toujours plus de fonctionnalités, et dans le même temps apparaissent des logiciels libres ou gratuits qui répondent aux besoins les plus courants.

Internet participe également au développement de la MAO par l'intermédiaire, par exemple, des sites qui proposent des formations musicales à distance, mettent à disposition des échantillons sonores ou diffusent sous forme électronique des partitions de musique tombée dans le domaine public, ainsi que grâce à tous les forums spécialisés sur la musique.
Les recherches sur la composition assistée par ordinateur remontent à 1955, et débouchèrent en 1956 sur le fameux quatuor à cordes dit « Illiac Suite », élaboré par Lejaren A. Hiller et Léonard M. Isaacson à l'université de l'Illinois.

Les postulats de base des travaux d'Hiller et Isaacson s'élaborèrent dans la mouvance des théories cybernétiques qui accordaient une grande confiance aux pouvoirs du calcul (intelligence artificielle). Ces auteurs élaborèrent un modèle mathématique d'analyse - construction musicale qui, en adaptant deux traités de contrepoint (en l'occurrence le traité de 1725 de Johann Joseph Fux, "Gradus ad Parnassum", et celui de Palestrina) servait de base à une reconstruction. Hiller était un scientifique chimiste, pour lui la décomposition devait permettre la re-composition. Formaliser certaines règles d'écriture et entrer dans l'ordinateur des schémas compositionnels classiques suffirait à traduire les côtés émotionnels ou passionnels de la musique par des jeux de règles et d'interdits. Pour préserver un certain degré d'expression artistique, le programme simulait l'aspect auto-organisé en introduisant quelques aspects des théories sur la formalisation du hasard (des chaînes de Markov - formulation des processus stochastiques et une méthode aléatoire de tirage des nombres dite de « Monte-Carlo »), très en vogue, elles aussi. Ce qui a fait dire à Hiller que « la musique est un compromis entre la monotonie et le chaos », mais sans jamais se poser le problème de savoir qui effectuait ce compromis.

En France, Pierre Barbaud poursuivit ces expériences et réalisa de nombreuses œuvres, grâce à du temps de calcul gracieusement alloué par la Compagnie des machines Bull, parfois en collaboration avec Janine Charbonnier et Roger Blanchard. Au début, les pièces étaient d'abord calculées par ordinateur puis jouées par des instruments, entre autres :


À partir de 1975, elles furent composées et produites par ordinateur sous la forme d'une bande magnétique directement audible en concert ("Saturnia Tellus", commande de l'ORTF en 1979). Ce projet de production automatique globale avait été théorisé par Pierre Barbaud dès 1960 dans l'article "Musique algorithmique", revue "Esprit", janvier 1960, et suiv. Le compositeur a recueilli ces expériences dans l'ouvrage "Initiation à la composition automatique" (Dunod, Paris, 1965). D'autres compositeurs comme Michel Philippot ont travaillé dans le sens de la composition automatique.

L'arrivée puis la large diffusion des nouvelles technologies de l'information, notamment celles permettant une manipulation aisée de l'image et du son, accélère le développement de la MAO et donne naissance, à la fin des années 1990, à un outil indispensable dans la création phonographique, surtout depuis la généralisation du son numérique (le disque compact qui remplace le disque vinyle).

Les logiciels destinés aux activités musicales répondent à un très large spectre d'utilisations et couvrent pratiquement toute la chaîne de création musicale. Parmi les domaines dans lesquels intervient l'informatique, on trouve :

Les logiciels les plus courants intègrent des fonctionnalités qui recouvrent la notation musicale, l'enregistrement du son (y compris sa modification) et la restitution sonore. Certains sont capables de générer des arrangements, d'autres pilotent des instruments. Ces logiciels permettent d'accéder avec peu de connaissances et de moyens à des domaines musicaux auparavant réservés à des spécialistes (ex. l'écriture de partition ou le travail de traitement du son), de même que les logiciels les plus pointus multiplient les possibilités des professionnels (ex. bibliothèques de sons, gains de productivité dans la production musicale, etc.). En combinant différents logiciels, il est ainsi possible de mener l'ensemble des activités de création musicales sur un seul ordinateur : composition, arrangement, exécution par des synthétiseurs et enregistrement de sources audio, mixage, etc., jusqu'à la gravure sur CD.

Pour l'utilisateur, les logiciels musicaux peuvent s'analyser sous deux angles :

La combinaison de ces deux critères sert souvent d'argument (au moins en marketing) pour distinguer les logiciels professionnels des logiciels .

Les logiciels de M.A.O. sont couramment répartis entre les catégories suivantes :


Parmi les logiciels qui manipulent le son, on distinguait ceux qui gèrent le son de synthèse (ex. le son stocké sur une carte son bon marché) de ceux qui manipulent des sons réels (ex. les « magnétophones numériques » qui enregistrent des instruments analogiques). Le son d'origine analogique est beaucoup plus gourmand en ressources informatiques (traitement et stockage) que le son de synthèse, mais plus proche du son réel des instruments. Avec la multiplication des interfaces sonores, l'amélioration de la qualité des sons de synthèse et la puissance de traitement des ordinateurs récents, les deux types de son tendent à se combiner (ex. typique : le mixage des sons de synthèse d'une boîte à rythme avec des instruments analogiques) ; la plupart des logiciels actuels permettent cette combinaison.


</doc>
<doc id="16775" url="https://fr.wikipedia.org/wiki?curid=16775" title="Insecticide">
Insecticide

Les insecticides sont des substances actives ou des préparations phytosanitaires ayant la propriété de tuer les insectes, leurs larves et/ou leurs œufs. Ils font partie de la famille des pesticides, eux-mêmes inclus dans la famille des biocides, tous deux réglementés en Europe par des directives spécifiques.

Le terme générique « insecticide » inclut aussi les pesticides destinés à lutter contre des arthropodes qui ne sont pas des insectes (ex : araignées ou acariens tels que les tiques) ainsi parfois que des répulsifs.

On distingue des produits agissant par contact, des produits « systémiques », et des produits à mode intermédiaire, dits « translaminaires ».

En termes de pollution et de dégradation de la biodiversité, parmi les produits phytopharmaceutiques et autres biocides, les insecticides semblent les plus impliqués, car biologiquement très actifs, et de plus en plus répandus dans l'environnement. Une étude récente (2015), la première ayant cherché à faire une évaluation globale de leurs impacts sur les milieux aquatiques a montré que plus de 50 % des insecticides détectés dans l'eau (sur la base de 11 300 analyses) l'étaient à des taux ", ce qui fait conclure aux auteurs que " et qu""'. 

De 1955 à 2000, l'intensification de l’agriculture a entraîné une augmentation de plus de 750 % de la production de pesticides, une industrie qui représente aujourd’hui un marché de 50 milliards de dollars dans le monde.

Elles sont liées aux modes d'action des insecticides, fondés par exemple sur la neurotoxicité de certaines molécules, ou sur leur impact sur la respiration cellulaire, la formation de la cuticule chitineuse, ou de la perturbation de la mue.

Ce sont principalement les :

Très utilisés de 1940 à 1970, ils sont en très nette régression. Ce terme n'inclut pas systématiquement tout insecticide comportant dans sa formule un atome de chlore.

Ce sont des insecticides de contact : aucun n'a besoin d'être véhiculé par la sève dans les végétaux pour agir sur les insectes qui les mangent.

Les organochlorés sont des toxines neurotropes qui altèrent le fonctionnement des "canaux sodium" indispensables à la transmission de l'influx nerveux. Leur spectre d'action est large. 
Le DDT, par exemple, agit sur l'insecte par contact et ingestion, induisant un tremblement généralisé (incoordination motrice) puis une paralysie qui met parfois 24 h pour s'installer. 

La toxicité aiguë des organochlorés envers l'homme est relativement faible, dans les conditions normales d'utilisation, mais ce sont des substances très stables et bioaccumulables, donnant des produits de dégradation et de biotransformation (métabolites) encore plus stables, peu solubles dans l'eau, à faible tension de vapeur, d'où des problèmes d'accumulation dans les organismes et les écosystèmes via les chaînes alimentaires. Certains peuvent persister très longtemps dans les sols, les tissus végétaux et les graisses, ce pourquoi ils ont été interdits dans bon nombre de pays.

Outre leur rémanence excessive, leur usage a été freiné par des phénomènes de résistance apparus en particulier chez les Diptères (cas de l'aldrine), dont chez certains moustiques.

Exemples : 

La première commercialisation (parathion) date de 1944. Ils sont actuellement les insecticides les plus variés du marché.
Ces produits n'ont guère de points communs entre eux, si ce n'est leur origine, une certaine liposolubilité et leur mode d'action sur le système nerveux. 
Ce sont des inhibiteurs de la cholinestérase, qui est bloquée sous une forme inactive : l'acétylcholine s'accumule au niveau de la synapse, empêchant la transmission de l'influx nerveux et entraînant la mort de l'insecte.
Ce mode d'action explique leur haute toxicité vis-à-vis de l'homme et des animaux à sang chaud. 

La plupart des organophosphorés pénètrent plus ou moins dans le tissu des plantes, étant semi systémiques, ou sont transportés par le système vasculaire de la plante : ils sont alors systémiques.

Ils se situent à l'opposé des organochlorés, avec une toxicité aiguë élevée mais une faible rémanence. 
Leur faible rémanence nécessite souvent la répétition des traitements pour assurer une longue protection. Ils pénètrent facilement dans l'organisme des insectes par leur liposolubilité élevée.
Certains sont spécifiquement acaricides.

On distingue :
Ils sont généralement hautement toxiques et peu stables.
Ils sont plus stables que le groupe précédent (meilleure rémanence).
Des produits issus de ces 3 groupes sont regroupés ci-dessous selon leur mode d'action :

Ce vaste groupe regroupe les dérivés de l'acide carbamique, comprenant aussi un grand nombre de fongicides et d'herbicides.

Ils agissent comme les organophosphorés ; en inhibant la cholinestérase. Certains ont des actions spécifiques (aphicide, molluscicide). Le propoxur, bendiocarbe et dioxacarbe sont utilisés en lutte anti-paludique pour leur grande rémanence. 

Ils agissent le plus souvent par contact bien que certains aient une action systémique (aldicarbe, benfuracarbeé). Leur rémanence est généralement faible. 

On distingue :

Insecticides dits « de troisième génération », ils sont copiés sur les pyrèthres naturels, en cherchant à augmenter leur toxicité et leur photostabilité. 
Dotés d'une toxicité considérable et agissant par contact, ils tuent presque instantanément les insectes par effet choc neurotoxique, permettant de les utiliser à des doses très réduites (10 à 40 g de matière active par ha). Comme les organochlorés, ils tuent l'insecte en bloquant le fonctionnement des canaux sodium indispensables à la transmission de l'influx nerveux. 

Réputés peu toxiques pour l'homme, on leur attribue le coefficient de sécurité (rapport des toxicités pour les insectes et pour les mammifères) le plus élevé parmi les insecticides chimiques.
Très biodégradables, ils ne persistent pas dans le milieu édaphique, mais ils sont très toxiques pour certains organismes aquatiques (poissons) ainsi que pour les auxiliaires de l'agriculture (dont les abeilles). Ils possèdent des propriétés diverses. 

Une molécule donnée présente de nombreux isomères aux degrés d'activités variés. La synthèse industrielle cherche à ne produire que l'isomère le plus actif de la molécule.

Les néonicotinoïdes sont une classe d'insecticides agissant sur le système nerveux central des insectes avec une toxicité inférieure chez les mammifères. Cette classe présente trois grandes caractéristiques : D'abord, ils sont hautement toxiques pour les insectes. Ensuite, leurs propriétés systémiques qui les rendent présents dans toutes les parties de la plante traitée. Enfin, ils ont une longue persistance dans l'environnement.

Ils ont en commun une liaison soufre. Faiblement toxiques, ils ont des propriétés acaricides (contre œufs et larves), mais ils sont pratiquement inefficaces vis-à-vis des insectes.

Ce sont des insecticides acaricides caractérisés par la présence d'une structure -N=CH-N. Ils tuent les œufs et les jeunes stades larvaires. Ils sont efficaces contre les insectes ayant développé une résistance aux organophosphorés ou aux organochlorés.

C'est un groupe d'insecticides découvert en 1972, le diflubenzuron étant la première matière active commercialisée. Elle se caractérise par son mode d'action qui perturbe la formation de la chitine qui n'est plus sous forme fibrillaire des larves d'insectes. La chitine synthétase est le site actif. Les insectes meurent lors de la mue suivante. Ils sont faiblement toxiques pour l'homme. Le délai d'action est de 2 à 7 jours. Leur demi-vie est de 2 semaines.

Carbinols : Ce sont pratiquement tous des acaricides spécifiques. Proches du groupe du DDT, ils comprennent des dérivés chlorés et bromés.

Toutes les plantes produisent des molécules pour se défendre de leurs prédateurs, et en particulier des insectes. De nombreuses graines (Pois, haricots, grain de café notamment) contiennent des protéines spéciales (globulines) insectifuges). Certaines plantes sont depuis longtemps utilisées pour éloigner ou tuer des insectes, ou pour tuer d'autres invertébrés (comme vermifuge...), etc.

En Europe, les insecticides végétaux ont connu un développement important entre les deux guerres, avant d'être éclipsés par les insecticides de synthèse moins coûteux. 
Des cultures à grande échelle de plantes à propriété insecticide furent menées dans les années 50. Ces insecticides sont extraits de diverses plantes par macération, infusion ou décoction. En voici quelques exemples :

Des composées du genre "Chrysanthemum" accumulent naturellement dans leurs capitules des substances insecticides, les pyréthrines. "Tanacetum cinerariifolium" est l'espèce la plus employée. Les fleurs, rappelant par leur forme les marguerites, sont broyées et séchées. La poudre obtenue est diluée au 1/10ème dans de l'eau. L'effet est augmenté par l'addition d'adjuvants, tel que le piperonyl butoxyde. Peu toxiques, les pyréthrines sont très vite dégradées dans la nature. Elles sont actives contre de nombreux insectes avec un effet choc.

Elles sont extraites de racines, feuilles ou graines de légumineuses ("Derris" spp en Asie du Sud-Est et "Lonchocarpus" spp en Amérique du Sud). Elles sont très toxiques pour les poissons et certains insectes qu'elles paralysent (inhibition du complexe mitochondrial I, c'est-à-dire de la chaîne respiratoire à échelle cellulaire) mais et peu toxiques pour les animaux à sang chaud. Leurs effets résiduels sont réputés faibles.
C'est un insecticide de contact, utilisé contre les insectes suceurs et broyeurs (pucerons, teignes, mouches des fruits, altises, noctuelles).
<br>Préparation : des racines de "Derris elliptica" de 2,6 cm de diamètre sont lavées puis broyées avec un peu d'eau et de savon (1 part de savon, 4 parts de racines et 225 d'eau). La solution obtenue par filtrage est utilisée immédiatement.
Attention ! Les roténones provoquent par contact de sévères lésions des régions génitales.

Extraite au niveau des feuilles et des tiges du tabac, "Nicotiana tabacum" ("Solanaceae"). Cet alcaloïde agit par inhalation, ingestion et contact. La nicotine a des propriétés acaricides, insecticide et fongicide. La nicotine se dégrade en 3-4 jours. C'est une substance très toxique pour l'homme, les mammifères et les poissons. Sa DL 50 est de . Elle peut être inhalée et absorbée directement à travers la peau : il faut donc éviter tout contact lors de sa manipulation. Le traitement est plus efficace s'il se déroule à température élevée (>). Il ne faut pas consommer les cultures traitées avant un délai de 4 jours.
<br>Préparation : la bouillie se prépare en arrosant 1 kg de tiges et de feuilles avec 15 l d'eau plus une poignée de savon (agent mouillant). Après 24 h, ce mélange est filtré et prêt à l'emploi. 

Extraite d'un arbre ("Azadirachta indica" : Margousier) de la famille des "Meliaceae", l'azadirachtine possède des propriétés insecticide et répulsive sur plus de 200 espèces d'insectes de 6 ordres différents et a des propriétés fongicides. Elle agit par contact et ingestion. Le produit se dégradant sous l'action de la lumière, il est conseillé de traiter en fin de journée. Le produit est efficace contre la teigne des crucifères, la coccinelle du melon et certaines cicadelles.
<br>Le neem extrait des graines permet, à une concentration de 3-5 ppm, de protéger les denrées stockées sur une période de 6 mois.
<br>Préparation : les graines sont débarrassées de la pulpe et les noyaux sont séchées à l'ombre. Une fois bien secs, ils peuvent être conservés plusieurs mois. Puis les graines sont réduites en poudre et mises à tremper dans de l'eau (25 à 50 g de graines par litre) pendant une nuit. La solution obtenue à partir des feuilles et graines est utilisée en pulvérisation contre les chenilles défoliatrices des cultures maraîchères. 
<br>Les feuilles et les fleurs sont placées sous la literie contre les poux et les puces. Une usine d'extraction de cette substance existe en Inde (1995) avec une capacité de traitement de 20 t par jour.
<br>À Madagascar, l'espèce végétale utilisée est "Melia azedarach" ("Meliaceae").

Ce produit est un perturbateur endocrinien, carcinogène génotoxiqueet provoque des atrophies sur les jeunes abeilles et des lésions dans le foie et les poumons des rats. 

<br>Par la décision 2008/941/CE du 8 décembre 2008, la Commission Européenne a refusé l'inscription de l'azadirachtine (substance active de l'Huile de neem) à l'annexe I de la directive 91/414/CEE, ce qui revient à interdire aux États membres d'incorporer cette substance active dans les préparations bénéficiant d'une autorisation de mise sur le marché sur leur territoire. Par conséquent son usage comme insecticide est interdit en agriculture, maraîchage, jardinage, espaces verts, serres. Un délai d'utilisation est maintenu jusqu'en 12/2010 (pouvant être prolongé au maximum jusqu'en 12/2011) . L'azadirachtine n'est d'ailleurs pas autorisée en France . Elle figure cependant parmi la liste des substances actives naturelles proposées par la commission "Moyens alternatifs et protection intégrée des cultures" de l'AFPP . Un usage dans des locaux (habitation, bureaux) est logiquement autorisé, car le produit relève alors de la directive biocides.

Ce produit est extrait d'un arbre de 4 à 6 m de haut, le "Quassia amara", de la famille des "Simarubaceae" au Brésil et en Amérique centrale, ainsi que d'un autre arbre atteignant 12 m, "Picraena excelsa". <br>Cette substance est présente essentiellement dans le bois. La quassine est pratiquement inoffensive pour les animaux domestiques et l'homme. Il n'affecte pas les insectes utiles tels que les abeilles et les coccinelles.
<br>Soluble dans l'eau, c'est un produit systémique utilisé essentiellement contre les insectes suceurs (pucerons).
<br>Préparation : on fait macérer 2 à 3 jours 1 kg de copeaux dans 6 litres d'eau. L'eau de trempage est utilisée en pulvérisation additionnée de 1 % de savon blanc. La quassine a une action insecticide et nématicide. 
<br>La longue persistance du goût très amer du produit empêche d'appliquer directement le produit sur les feuilles ou fruits consommables.
<br>Par la décision 2008/941/CE du 8 décembre 2008, la Commission Européenne a refusé l'inscription de la quassine (quassia) (substance active de l'Huile de neem) à l'annexe I de la directive 91/414/CEE, ce qui revient à interdire aux États membres d'incorporer cette substance active dans les préparations bénéficiant d'une autorisation de mise sur le marché sur leur territoire. Par conséquent son usage comme insecticide est interdit en agriculture, maraîchage, jardinage, espaces verts, serres. Un délai d'utilisation est maintenu jusqu'en 12/2010 (pouvant être prolongé au maximum jusqu'en 12/2011) . La quassia n'est d'ailleurs pas autorisée en France . Elle figure cependant parmi la liste des substances actives naturelles proposées par la commission "Moyens alternatifs et protection intégrée des cultures" de l'AFPP . Un usage dans des locaux (habitation, bureaux) est logiquement autorisé, car le produit relève alors de la directive biocides.

Cette substance est extraite de "Ryania speciosa", de la famille des "Flacourtiaceae" et se rencontre en Amérique du Sud. On utilise les tiges, les racines et la sciure de tronc. Le produit agit par contact et l'effet est lent mais très puissant, les insectes cessant de se nourrir, de se déplacer et de se reproduire. C'est un insecticide sélectif par ingestion. Le ryania est peu toxique pour les vertébrés et l'effet dure au champ 5 à 9 jours. On obtient de bons résultats envers les larves de Lépidoptères.
<br>Préparation : les racines, les feuilles ou les tiges sont séchées puis moulues finement. 30 à 40 g de poudre sont mélangés à 7 à 8 litres d'eau puis le liquide obtenu par filtrage est pulvérisé, tous les 10 à 14 jours en arboriculture.

Cette substance provient de diverses espèces d"'Aconitum" ("A. fischeri, A. kuznezoffi, A. autumnale", "A. napellus"). Ces plantes contiennent de l'aconitine et d'autres alcaloïdes très toxiques pour les mammifères, les oiseaux et les invertébrés. Par voie orale, la dose létale chez l’homme varierait de 2 à 5 mg. En Chine, ces plantes sont cultivées pour le traitement de semences.

Le géraniol est obtenu par distillation fractionnée d'extraits naturels de "Cymbopogon winterianus" Jowitt. Il a été démontré que le géraniol en solution aqueuse avait une double action sur les insectes et tous les stades de la métamorphose, par étouffement et déshydratation de l'insecte, des œufs et des larves. C'est même l'un des meilleurs larvicides du marché.

Une décoction de piment a également un effet insecticide.

Une étude brésilienne a montré que les grains de café (non-torréfié) sont riches en globulines insecticides. Ces globulines se sont avérées en laboratoires très efficaces contre la larve du charançon du niébé (insecte modèle couramment utilisé pour tester l'activité insecticide des protéines) ; 50 % des larves exposées étaient rapidement tuées par d'infimes quantités de ces protéines du café. Des scientifiques imaginent créer des OGM exprimant le gène codant cette protéine, par exemple dans des céréales, en espérant que ces protéines ne soient pas directement ou indirectement pas nocives pour l'homme ou l'environnement si produites par des parties consommables des plantes cultivées ou si par croisement génétique elles étaient produites par des cousines sauvages.

Elle varie selon les produits, selon les conditions d'application (vent, hygrométrie, qualité du pulvérisateur). 

L'apparition et/ou diffusion (par le jeu de l'évolution et de la sélection naturelle) de gènes de résistance chez les insectes-cibles ou non-cibles (ou arthropodes) peut réduire ou annuler leur efficacité ou poser de nouveaux problèmes (pullulation d'une espèces devenue résistante par ex., telle que la mouche domestique ou le moustique ou divers déprédateurs des cultures). 

En 2011, le marché mondial des insecticides (y compris acaricides et nématicides) s'élevait à 14 milliards de dollars,
dont 11,6 milliards pour les applications agricoles (traitements foliaires, du sol et des semences) et 2,4 milliards pour les applications non-agricoles.

De par ses objectifs et son mode d'action, tout insecticide utilisé dans le milieu naturel a un impact écologique, plus ou moins important selon son efficacité, sa toxicité plus ou moins ciblée et sa rémanence dans l'environnement, même s'il s'agit d'un produit dit d'origine naturelle ou microbienne (Toxine Bt par exemple)

Parfois l'insecticide tue aussi les prédateurs naturels de l'espèce-cible (ou les fait régresser), ce qui perturbe les réseaux trophiques, y compris des agroécosystèmes, dans les rizières notamment ou la prédation intraguilde peut être modifiée. 
Les insecticides peuvent ainsi dégrader de nombreux services écosystémiques (via la régression des apidés et des papillons pollinisateurs par exemple) et paradoxalement favoriser la diffusion d'insectes parasites devenus résistants, dont "Nilaparvata lugens", devenu le premier parasite du riz dans les rizières . Ils contribuent aussi directement et indirectement au déclin du plancton aérien à faire régresser les populations d'animaux insectivores (dont oiseaux tels qu'hirondelles et martinets) et d'autre part à limiter l'efficacité des pesticides en favorisant des phénomènes de résistances aux insecticides. À titre d'exemple, on a montré qu'en Camargue le fipronil était dans la nature "in fine" inefficace pour limiter certaines espèces de moustiques qui y sont pourtant sensibles en laboratoire : ainsi, le fipronil destiné à tuer les larves de chironomes a aussi dans la nature un impact négatif sur leurs prédateurs invertébrés (et indirectement sur les vertébrés), d'où l'absence paradoxale de différence dans l'abondance des chironomes entre parcelles biologiques et parcelles traitées par le fipronil. Dans les premières diffusions, l'espèce survit normalement, mais sans la seconde les insectes qui échappent aux pesticides ou devenus résistants se reproduisent d'autant mieux qu'ils ont moins de prédateurs (la biomasse en macroinvertébrés prédateurs) diminue dans les zones traitées ; soit par intoxication via l'alimentation, soit via le manque de nourriture pour les insectivores ou prédateurs d'insectivores. Plus haut dans la chaine trophique, certains oiseaux tels les hérons régressent aussi. Les rizières conventionnelles ont donc une moindre valeur trophique que les rizières biologiques.

Parfois l'insecticide tue des insectes qui eux-mêmes contrôlaient des ravageurs herbivores, ce qui peut poser de nouveaux problèmes dans les agrosystèmes 

Plusieurs études scientifiques menées par des chercheurs (INRA, CNRS), et des ingénieurs des filières agricoles et apicoles (ACTA, ITSAP-Institut de l’abeille, ADAPI) ont souligné l'impact négatif d'une classe d'insecticides, les néonicotinoïdes, sur les abeilles et bourdons en laboratoire et lors de tests en conditions contrôlées. Ces insecticides agissent sur le système nerveux central des insectes, et sont parmi les insecticides les plus utilisés à travers le monde. Ces molécules sont mises en cause par de nombreux apiculteurs pour expliquer le syndrome d'effondrement des colonies d'abeilles.

Il est recommandé de ne pas pulvériser en présence de vent ou par temps ensoleillé (déshydratation des gouttelettes et évaporation dans l'air). Une hygrométrie élevée est favorable, mais il ne doit pas pleuvoir pendant ou juste après le traitement.
Il est parfois recommandé d'appliquer les insecticides à usage externe à la tombée de la nuit car, l'humidité de l'air remonte, certains composés sont détruits par les rayons du soleil et donc moins actifs s'ils sont répandus en journée.
D'autre part, cela permettrait d'éviter de tuer certains insectes utiles tels que les abeilles. En effet, les abeilles s'abreuvant notamment de la rosée du matin, elles sont souvent intoxiquées si l'insecticide a été répandu en tout début de matinée. C'est cependant le soir que se forment la rosée, qui peut également être bue par d'autres espèces (espèces nocturnes notamment).

Pour mieux étudier et comprendre le mode d'action des insecticides et répulsifs notamment utilisés dans la lutte anti-vectorielle pour le contrôle de maladies véhiculées et transmises par des moustiques, poux, puces, tiques de nouvelles méthodes pourraient être bientôt disponibles. 




</doc>
<doc id="16776" url="https://fr.wikipedia.org/wiki?curid=16776" title="Adulticide">
Adulticide

Un produit phytosanitaire adulticide est une substance active ou une préparation ayant la propriété de tuer le stade adulte des ravageurs des cultures.

Les produits adulticides peuvent être simultanément ovicides ou larvicides, et parfois stérilisants des femelles, ce qui augmente leur efficacité lorsque les populations de ravageurs présentent plusieurs stades en même temps.

En protection des cultures contre les parasites, la connaissance des caractéristiques adulticides des produits phytopharmaceutiques appliqués, et de la date de début d'apparition des adultes, permettent de raisonner l'emploi des insecticides et des acaricides.



</doc>
<doc id="16777" url="https://fr.wikipedia.org/wiki?curid=16777" title="CGT">
CGT

CGT est un sigle qui peut désigner :



CGT est un code qui peut désigner :

</doc>
<doc id="16778" url="https://fr.wikipedia.org/wiki?curid=16778" title="Confédération française démocratique du travail">
Confédération française démocratique du travail

La Confédération française démocratique du travail (CFDT) est une confédération interprofessionnelle de syndicats français de salariés, la première par le nombre d'adhérents revendiqués (plus de en 2011, mais une étude de 2007 estimait que la CFDT avait moins d'adhérents que la CGT) et la première dans le secteur privé depuis le 31 mars 2017 par son audience électorale aux élections professionnelles. Elle est deuxième organisation syndicale en matière d'audience électorale si on amalgame les résultats de la fonction publique et le secteur privé, derrière la CGT.

La CFDT plonge ses racines dans le syndicalisme chrétien. Elle a été créée en 1919 sous le nom de Confédération française des travailleurs chrétiens (CFTC).

Le mouvement Reconstruction, initialement minoritaire dans la CFTC, militait pour un syndicalisme de gauche, socialiste et démocratique sans être marxiste et contre les traités européenne jugé comme une restauration d'une Europe capitaliste sous couvert de démocratie chrétienne et va imposer sa marque dans la CFTC et devenir majoritaire.

Après la Libération (1944-1945), une minorité de gauche, regroupée dans la tendance « reconstruction », anime un débat interne en faveur de la « déconfessionalisation » de la centrale. Sous la conduite d'Eugène Descamps, qui devient secrétaire général de la CFTC, elle devient majoritaire en 1961. La rupture se produit en 1964 : le congrès extraordinaire qui se tient au Palais des Sports les 6 et 7 novembre transforme la CFTC en CFDT. Une minorité (environ 10 % des effectifs), qui suit, notamment, Joseph Sauty, refonde aussitôt une « CFTC maintenue » dans une autre salle parisienne, le Musée social.

La CFDT déclare désormais placer son action dans le cadre de la lutte des classes. En quête de sa propre identité (notamment vis-à-vis de la CGT), la CFDT se tourne vers le mot d'ordre de l'autogestion qui s'ajoute à celui de "planification démocratique" qu'elle a adopté depuis la fin des années 1950 (au sein de la CFTC). L'idée d'Autogestion est inspirée d'expériences menées dès les années 1950 dans la Yougoslavie et d'expériences menées dans l'Algérie nouvellement indépendante. À partir de 1968, l'Autogestion devient le fondement de la vision du monde de la CFDT, de son projet de société. Jusqu'au milieu des années 1970, le discours de la CFDT est radical. Il emprunte au marxisme et aux courants les plus durs à l'égard du capitalisme. Le socialisme autogestionnaire est vu comme une alternative au capitalisme, mais aussi au socialisme d’État. 

La CFDT se rapproche aussi du Parti socialiste unifié (PSU) mené par Michel Rocard. De , elle passe également des accords d'action avec la CGT. La centrale syndicale adopte une posture progressiste que les questions de son temps : soutien à la décolonisation, à la légalisation de l’avortement, prise en compte des droits spécifiques aux femmes, qui accèdent de plus en plus massivement au marché du travail, et du sort des immigrés tout en admettant que l'immigration peut constituer un outil de maximalisation des profits du patronat. 

En mai 1968, la CFDT appelle à l'action dans les entreprises. Tandis que la CGT concentre ses efforts sur les revendications salariales (obtenant ainsi une augmentation de 35 % du salaire minimum), la CFDT défend principalement l'extension des droits syndicaux afin d'abolir le système « monarchique » qui prévaut en entreprise et le remplacer à terme par une « démocratie d'entreprise ». Avec la CGT et FO elle est à l'origine de la création des sections syndicales d'entreprises lors des accords de Grenelle.

Au congrès de 1970, la CFDT adopte de nouveaux statuts. Elle prône l'autogestion des entreprises. En 1971, Edmond Maire est élu secrétaire général. De nombreux militants cédétistes participent aux « Assises du socialisme » (1974) et rejoignent à cette occasion le Parti socialiste (PS) de François Mitterrand. Ils y animent une minorité de sensibilité « chrétienne de gauche » derrière Michel Rocard dans le cadre de ce qui a été baptisé la deuxième gauche. La CFDT collabore alors avec plusieurs associations de la société civile, dont le Gisti. Dès 1977, elle recommande le passage aux 35 heures pour partager le travail.

Cependant, la rupture de l'Union de la gauche (1977) et l'échec de celle-ci aux élections législatives de 1978 conduit la centrale d'Edmond Maire à un changement de stratégie. Elle s'éloigne de la CGT et entreprend un mouvement de « resyndicalisation » (ou de « recentrage sur l'action syndicale ») qui consiste notamment à prendre de la distance face aux partis politiques. Elle appelle pour la dernière fois à voter socialiste à la présidentielle de 1981. 

La CFDT prend également ses distances à l'égard de l'autogestion : des théoriciens comme Pierre Rosanvallon (conseiller économique de la CFDT puis conseiller politique d'Edmond Maire) théorisent même une nouvelle vision de l'autogestion comme un concept-relais vers une forme de libéralisme.

Si la CFDT soutient le « tournant de la rigueur » mené par le gouvernement Mauroy en 1983, elle se définira ensuite par son autonomie, n'hésitant pas à soutenir des réformes menées par des gouvernements de droite lorsqu'elles lui paraissent justifiées. Une importante minorité conteste ce « recentrage ». Au congrès de Strasbourg, qui se tient du 21 au , Jean Kaspar est élu secrétaire général. La fédération Sud-PTT naît le 16 décembre 1988 au terme d'un conflit politique au sein de la fédération des PTT à la suite duquel plusieurs responsables, principalement de la région parisienne, ont été démandatés lors des Conseils Nationaux Fédéraux de novembre et décembre 1988. Ces militants n'ont pas suivi le virage plus à droite de la CFDT, venus souvent de l'extrême-gauche, ils se déclarent autogestionnaires et mettent l'accent sur une nouvelle forme de démocratie interne.

En 1992, Nicole Notat, issue du SGEN-CFDT, est élue secrétaire générale. Elle est la première femme à occuper un tel poste dans une confédération syndicale de salariés en France.

Lors des grèves de décembre 1995, elle soutient le projet de réforme de la Sécurité sociale du Premier ministre RPR Alain Juppé. Devenu l'interlocuteur privilégié des employeurs et des gouvernements, la CFDT prend la présidence de la Caisse nationale d'assurance maladie des travailleurs salariés (CNAM) et de l'Unédic. Ces choix de la direction nationale, en rupture par rapport aux positions de la CFDT dans les années 1970, sont contestés par une minorité qui sera mise en échec au Congrès de Lille en 1998. La CFDT s'isole des autres syndicats et d'autres opposants internes rejoignent la CGT ou Sud.

En 2002, François Chérèque, ancien secrétaire général de la fédération CFDT Santé-sociaux, est élu secrétaire général. Il est reconduit dans ses fonctions lors du congrès de Grenoble du 12 au et du congrès de Tours le .

En 2003, au nom du « réformisme », la CFDT négocie aux côtés des autres centrales la réforme des retraites de 2003 qui sera mise en place par le gouvernement de Jean-Pierre Raffarin. Mais l'unité syndicale ne tient pas, et la centrale est seule, avec la CFE-CGC, à soutenir un projet de réforme contesté par les autres organisations qui considèrent cette réforme comme une régression sociale.

Cette prise de position a des conséquences internes pour la CFDT qui perd au moins 10 % de ses adhérents en trois ans. Selon la centrale elle-même, les effectifs passent de , fin 2002, à , fin 2005. Cette baisse des effectifs est due à un non-renouvellement d'adhésions individuelles mais aussi au départ de plusieurs syndicats. Le , sur son site internet, la CFDT reconnaît la perte de adhérents pour les années 2003 et 2004, ce qui ramène ses effectifs de fin 2002 à fin 2004 (cette perte est en partie compensée par plus de nouvelles adhésions cette même année).

En mars 2003, afin de développer sa présence sur les universités, la CFDT décide d'appuyer la création d'un syndicat étudiant issue d'une tendance de l'UNEF : la Confédération étudiante. Afin d'aider à son développement, les deux organisations s'associent ce qui assure à la Cé un important appui financier et logistique. Ce partenariat permet à la CFDT d'informer les étudiants salariés de leurs droits lors d'action sur les zones de travail estivales ou les campus.

À la suite du rejet du traité constitutionnel européen, la CFDT, qui a appelé à voter « oui », participe à la création du mouvement Sauvons l'Europe et y adhère en tant qu'organisation membre. Selon un sondage Ifop-"Ouest France Dimanche" du 23 avril 2006, la CFDT est la confédération à laquelle les Français font le plus confiance (61 % contre 51 % pour la moyenne des syndicats). Cependant, elle arrive régulièrement en seconde position derrière la CGT lors des élections professionnelles. Elle recueille environ 25 % des voix (cf. plus bas).
En 2006, la CFDT dénonce le contrat première embauche (CPE) et rejoint les autres syndicats au sein des intersyndicales et des manifestations afin de demander le retrait de celui-ci.

Lors du congrès de Grenoble de juin 2006, François Chérèque défend le bilan de ces quatre dernières années et critique ouvertement le gouvernement et le MEDEF. Les débats principaux concernent notamment le thème de l'insertion des jeunes dans le monde du travail, la sécurisation des parcours professionnels et la priorité à la construction d'une Europe sociale. Ce congrès est également marqué par une volonté de porter une attention privilégiée à l’adhérent et souligne la nécessité de « porter un regard lucide pour mieux travailler avec les jeunes pour leur avenir. » Il s'achève par un appel à construire l’avenir.

La CFDT fait partie des huit syndicats qui ont organisé en six mois huit grandes manifestations de rue lors des grèves contre la réforme française des retraites de 2010. Le 11 janvier 2013, la CFDT fait partie des 3 syndicats (avec la CFTC et la CFE-CGC) à avoir signé l'accord « sécurisation de l'emploi » avec le syndicat patronal MEDEF sous la médiation du gouvernement. Cet accord est vivement critiqué par la CGT et FO.

Pendant les campagnes de l'élection présidentielle de 2002 et des élections régionales de 2015, la CFDT appelle à faire battre le Front national. En 2016, la CFDT fait partie, avec l'UNSA et la CFTC, des syndicats favorables au projet de loi « travail » présenté par la ministre Myriam El Khomri, qui accepte d'intégrer certaines de ses revendications. Le 31 mars 2017, la CFDT devient le premier syndicat de France (dans le privé), passant devant la CGT, fait inédit depuis un siècle. En avril 2017 lors de la campagne de l'élection présidentielle française, tout en affirmant ne donner aucune consigne de vote, Laurent Berger annonce « avoir clairement pris position contre Marine Le Pen et publié un argumentaire pour nos militants ».


La CFDT comprend quelque syndicats de base. Ce nombre est en constante évolution en fonction des fusions réalisées, des nouvelles adhésions et des désaffiliations. Les syndicats de base sont regroupés en 22 unions régionales interprofessionnelles et 15 fédérations nationales professionnelles.

La CFDT compte aussi deux unions confédérales : la CFDT Cadres et l'union confédérale des retraités (UCR). Une union fédérale représente les 3 fonctions publiques : l'union des fédérations des fonctions publiques et assimilés (UFFA) .

Les instances dirigeantes de la centrale sont la commission exécutive (10 membres) et le bureau national (38 membres). Le conseil national confédéral (CNC) réunit trois fois par an les représentants des unions régionales et des fédérations et constitue le « parlement » de la CFDT.



Par principe, la CFDT n'affilie que les syndicats de métropole (Corse comprise). Dans les quatre départements d'outre-mer historiques (DOM) et à Saint-Pierre-et-Miquelon, elle signe des « contrats d'association » avec des unions syndicales qui portent le sigle CFDT. Les « syndicats associés » conservent leur autonomie et l'intégralité de leurs cotisations mais ils participent aux élections professionnelles sous l'étiquette CFDT.


Dans les ex territoires d'outre-mer (TOM), la CFDT entretient des liens de coopération avec les centrales syndicales locales partageant ses valeurs.


L'Association études et consommation (ASSECO-CFDT) est une association nationale de consommateurs agréée par l'État depuis 1981.

Depuis son congrès de Nantes en 1973, la CFDT dispose d'une Caisse nationale d'action syndicale (CNAS) issue de la fusion d'un fonds d'action syndicale et d'une caisse nationale d'action professionnelle issue elle-même de la fusion de 6 caisses de résistance d'unions départementales ou de fédérations. La CNAS est alimentée par une part des cotisations syndicale (8,6 % pour les actifs, 3 % pour les retraités), elle est gérée par un comité de gestion de 11 personnes (le trésorier confédéral et 10 administrateurs élus par les syndicats). La CNAS offre trois types de prestations aux adhérents et aux structures de la CFDT :

En raison de l'ampleur du mouvement social contre la réforme des retraites en France de 2010, la CFDT a demandé exceptionnellement à la CNAS d'utiliser la prestation grève pour indemniser ses adhérents dans le cadre de cette action interprofessionnelle.

Les comptes de la CNAS sont publiés chaque année avec les comptes confédéraux. Pour l'exercice 2009, la réserve de grève accumulée depuis la création de la CNAS se montait à plus de 101 millions d'euros. Elle s'élève à 125 millions d'euros en 2018.

Depuis 1973, la CFDT n'a à sa tête qu'un secrétaire général, sans président.


Le tableau ci-dessous, énumère le nombre d'adhérents revendiqué par le syndicat. Toutefois ces chiffres sur la réalité des effectifs syndicaux représente une polémique ancienne. En date du 5 septembre 2006, le magazine "Liaisons sociales" publie contenu d'une étude financée par la Dares, organisme officiel et statistique du ministère de l'Emploi.

Intitulée « Les syndiqués en France 1990-2006 », cette étude conduite par les chercheurs Dominique Andolfatto et Dominique Labbé revoit à la baisse les chiffres officiels des effectifs annoncés par les syndicats. Il ressort que la CFDT n'aurait que 450 000 adhérents (contre 803 665 officiellement à cette même date). L'étude place également la CGT en tête des organisations syndicales alors que, selon les chiffres de la CFDT, la fin des années 1990 avait vu celle-ci, alors dirigée par Nicole Notat, dépasser une CGT en mutation. Cependant la CGT est toujours restée en tête des résultats aux élections prud'homales et professionnelles et elle devance la CFDT, FO, la CFTC, la CFE-CGC, l'Unsa et Solidaires.

La CFDT déclare son nombre d'adhérents en divisant par 8 le nombre de cotisations mensuelles encaissées dans l'année. Ce mode de calcul ancien conduit à surestimer le nombre d'adhérents alors que le prélèvement automatique des cotisations s'est développé.

La métamorphose idéologique de la CFDT, de l'autogestion au « réformisme », s'est produit à mesure qu'arrivait une nouvelle de militants, peu politisés et moins radicaux, diplômés et moins concernés par l'introduction des techniques modernes de managements dans les entreprises. A mesure aussi que la centrale perdait ses adhérents ouvriers. Désormais, ce ne sont plus ces derniers qui siègent mais les professions intermédiaires, surreprésentés parmi les salariés par rapport à leur place dans l’ensemble de la population salariée.

En 2006, le tiers des effectifs de la CFDT a plus de 50 ans.

La CFDT fait partie des cinq confédérations de syndicats de salariés considérées comme représentatives par l’État, en application de l'arrêté du , et bénéficie de ce fait, jusqu'à la fin de la période transitoire mise en place par la loi du « portant rénovation de la démocratie sociale » d'une présomption irréfragable de représentativité.

Mais, désormais, ce sont les élections en entreprise qui la conditionnent, au niveau des entreprises de plus de 10 salariés depuis l'entrée en vigueur de la loi de 2008, ainsi que dans les branches professionnelles et au niveau interprofessionnel. 

Plus que par le nombre d’adhérents, ou sur des critères anciens, les Lois françaises (loi du 20 août 2008 modifiée par celle du 5 mars 2014) ont consacré la représentativité des organisations syndicales sur une mesure d’audience. Pour la première fois, dans le cadre de la réforme de la représentativité syndicale, l’audience des organisations syndicales auprès des salariés a été mesurée au niveau national et interprofessionnel ainsi qu’au niveau des branches professionnelles, du scrutin dans les entreprises de moins de 10 salariés (TPE) fin 2012 et des élections du secteur agricole début 2013.

Au total, salariés se sont exprimés en faveur des organisations syndicales de leur choix (soit plus de suffrages qu’aux élections prud’homales), ce qui conforte la légitimité de ces dernières en tant qu’acteurs du dialogue social.   

Au niveau national et interprofessionnel en 2013, la CFDT talonne la CGT. Les 5 premières organisations syndicales atteignent le score suivant : CGT : 26,77 % ; CFDT : 26,00 % ; FO : 15,94 % ; CFE-CGC : 9,43 % ; CFTC : 9,30 %.

Selon les chiffres du Haut conseil du dialogue social publiés en mars 2017, la CFDT se hisse pour la première fois en tête des élections professionnelles tenues de 2013 à 2016 avec 26,37 % des voix (+ 0,37 par rapport à 2013) devant la CGT 24,85 % (- 1,92 point), FO 15,59 % (- 0,35), la CFE-CGC 10,67 % (+ 1,24 point), la CFTC 9,49 % (+ 0,19), l’UNSA 5,35 % (+ 1,09) et Solidaires 3,46 % (- 0,01). Le poids relatif (retenu pour les seules organisations représentatives au niveau national est de 30,32 % pour la CFDT, 28,57 % pour la CGT, 17,93 % pour FO, 12,27 % pour la CFE-CGC et 10,91 % pour la CFTC.

Résultats 2008/2002 par collège :

La CFDT est inscrite comme représentant d'intérêts auprès de l'Assemblée nationale. Elle déclare à ce titre en 2012 un budget global de , dont de financement public, mais n'indique pas les coûts annuels liés aux activités directes de représentation d'intérêts auprès du Parlement.

La CFDT est inscrite depuis 2014 au registre de transparence des représentants d'intérêts auprès de la Commission européenne. Elle déclare en 2016 pour cette activité 10 collaborateurs à temps plein et des dépenses d'un montant compris entre et .

Il a été reproché à la CFDT de ne pas entendre l'avis des salariés intermittents et des syndicats de cette branche lors de négociations de 2016. Le 30 mai 2016, les confédérations syndicales CFDT CFTC et CFE-CGC, ainsi que le Medef, ont refusé de reconnaître l’accord signé unanimement par les syndicats du secteur de l’audiovisuel et du spectacle vivant sur le régime de l’intermittence . De son côté la CFDT met en avant le fait de «s'inscire dans une trajectoire de réduction confirmée du rapport dépenses/recettes relative à l'indemnisation» des intermittents .

Le 15 mai 2003, la CFDT signe un compromis avec le gouvernement Raffarin sur la réforme des retraites des fonctionnaires alors que quelques jours avant plus d'un million de manifestants défilaient contre la réforme des retraites. Cette prise de position de la confédération engendre une crise au sein de la CFDT, qui se solde par plus de 30 000 départs d'adhérents . Sur son site la CFDT insiste sur le fait que les réformes ont permis des centaines de milliers de départs anticipés .

En mars 2018, la CFDT est condamnée par le conseil de prud’hommes de Paris pour non-respect de ses obligations envers une salariée. Cette dernière, secrétaire confédérale de la CFDT, avait subi le « harcèlement moral » de son supérieur, un secrétaire national, sans que les dirigeants de la confédération, régulièrement alertés, n'interviennent.





</doc>
<doc id="16779" url="https://fr.wikipedia.org/wiki?curid=16779" title="Herbicide">
Herbicide

Un herbicide est une substance active ou une préparation phytosanitaire ayant la propriété de tuer les végétaux. Le terme « désherbant » est synonyme d'herbicide.

Les herbicides appartiennent à la famille des pesticides, elle-même incluse dans la famille des biocides.

Les herbicides sont employés en défense des cultures, dans les golfs, en sylviculture, pour lutter contre les mauvaises herbes qui concurrencent les plantes cultivées, qu'il s'agisse de plantes herbacées ou ligneuses. La pratique agricole associée est le désherbage chimique.

Selon leur mode d'action, on peut les utiliser en pré- ou en post-levée.

On distingue :

« Phytocide » est un terme générique qui regroupe l'ensemble de ces produits.

Avant l'utilisation généralisée des herbicides chimiques, on avait recours à des méthodes de lutte culturale, par exemple en modifiant le pH du sol, la salinité, ou les niveaux de fécondité, pour lutter contre les mauvaises herbes. La lutte mécanique (qui comprend le travail du sol) a également été utilisée dans ce but et l'est encore.

L'usage des herbicides chimiques a commencé dans les années 1880, uniquement avec des composés inorganiques (minéraux). En 1896, Louis Bonnet réussit en France le désherbage du blé contre la moutarde des champs à l'aide d'une solution de 6 % de sulfate de cuivre. Par la suite, d'autres sels sont utilisés ainsi que l'acide sulfurique dilué. Celui-ci est largement employé en France entre les deux guerres mondiales sous l'impulsion de l'agronome Edmond Rabaté. On l'utilise sur les cultures de céréales malgré ses inconvénients liés à sa forte corrosivité et au fait qu'il n'élimine pas certaines mauvaises herbes (notamment les graminées). Il n'est définitivement supplanté par les herbicides de synthèse que dans les années 1960.

Les premiers herbicides organiques apparaissent avant la Seconde Guerre mondiale, c'est le cas du 4,6-dinitro-ortho-crésol (DNOC) breveté en France en 1932 par Pastac et Truffaut ou du pentachlorophénol (PCP) introduit aux États-Unis en 1936 comme agent de préservation du bois et employé dès 1940 comme désherbant des céréales.

Bien que la recherche sur les herbicides chimiques a commencé à la fin du , la première percée majeure a été le résultat de recherches menées au Royaume-Uni et aux États-Unis au cours de la Seconde Guerre mondiale sur l'utilisation potentielle d'armes biologiques.
Le premier herbicide moderne, l'acide 2,4-dichlorophénoxyacétique, a été découvert et synthétisé chez Imperial Chemical Industries par W. G. Templeman. En 1940, ce dernier a montré que « des substances de croissance appliquées de manière appropriée tuaient certaines mauvaises herbes à feuilles larges dans les céréales sans nuire aux cultures ». En 1941, son équipe a réussi à synthétiser la molécule. La même année, Pokorny obtint le même résultat aux États-Unis.

Indépendamment, une équipe dirigée par a fait la même découverte à la station expérimentale de Rothamsted. Quastel a été chargé par l' (ARC) de découvrir des méthodes pour améliorer le rendement des cultures. En analysant le sol comme un système dynamique, plutôt que comme une substance inerte, il a été en mesure d'appliquer des techniques telles que la perfusion. Quastel a pu ainsi quantifier l'influence de diverses hormones végétales, d'inhibiteurs et d'autres produits chimiques sur l'activité des micro-organismes du sol et évaluer leur impact direct sur la croissance des plantes. Le travail de cette unité est resté totalement secret, mais après la guerre, certaines découvertes, dont celle du 2,4-D ont été développées pour un usage commercial.

Quand le 2,4-D a été commercialisé en 1946, il a déclenché une révolution dans le monde agricole et est devenu le premier herbicide sélectif efficace. Il a permis d'améliorer le désherbage du blé, du maïs, du riz et d'autres cultures de céréales, car il élimine sélectivement les dicotylédones, mais n'affecte pas la plupart des monocotylédones (graminées). Le faible coût du 2,4-D a facilité son utilisation jusqu'à nos jours et il reste l'un des herbicides les plus couramment utilisés dans le monde. Comme c'est le cas pour d'autres herbicides de type acide, les formulations actuelles utilisent soit un sel d'amine (souvent des triméthylamines) ou l'un des nombreux esters du composé parent. Ces dérivés sont plus faciles à manipuler que l'acide.

La famille des triazines, qui comprend l'atrazine, a été introduite dans les années 1950 ; c'est la famille d'herbicides la plus préoccupante au sujet de la contamination des eaux souterraines. L'atrazine ne se décompose pas facilement (en quelques semaines) après avoir été appliquée à des sols au pH légèrement basique. Dans des conditions de sol alcalin, l'atrazine peut être transférée dans le profil du sol jusqu'à la nappe phréatique par l'eau s'infiltrant après les pluies, provoquant la contamination mentionnée plus haut.

Le glyphosate ("Roundup") a été mis sur le marché en 1974 pour le désherbage non sélectif. Grâce à la création de variétés de plantes cultivées résistantes au glyphosate, il est maintenant utilisé très largement pour le désherbage des cultures. L'association de l'herbicide avec la semence résistante a contribué à la consolidation du secteur des semences avec l'industrie de la chimie à la fin des années 1990.

De nombreux herbicides chimiques modernes utilisés dans l'agriculture et le jardinage sont spécifiquement formulés pour se décomposer rapidement après application. Cela permet d'implanter des cultures après l'application, sans que celles-ci soient affectées par l'herbicide.
Cependant, les herbicides à faible activité résiduelle (c'est-à-dire qui se décomposent rapidement) ne permettent pas de maîtriser les mauvaises herbes pendant toute la saison de culture et ne garantissent pas que les racines des mauvaises herbes soient tuées sous les revêtements de surface (au risque que ces plantes réapparaissent ultérieurement de façon destructrice), ce qui laisse la place à des désherbants ayant un niveau élevé de persistance dans le sol.

Le groupe de travail « Terminologie » de la Commission des essais biologiques (CEB) de l'Association française de protection des plantes, recommande d'employer les définitions suivantes pour les différents types d'herbicides :




Les formulations vendues dans le commerce doivent être homologuées (pour un ou plusieurs usages). 
Un produit désherbant contient généralement une ou plusieurs molécules actives (ex : glyphosate pour le Round Up) et des adjuvants (ex : polyoxyéthylène amine ou POEA pour le Roundup ; POEA dont certains considèrent qu'il a une action spécifique sur les végétaux traités) pour stabiliser le mélange ou accélérer ou permettre sa pénétration dans les tissus végétaux pour les tuer (ou en inhiber la croissance dans le cas des inhibiteurs de croissance).

Les modes d'action des herbicides sont fondés sur :

En France, les pollutions de l'eau causées par les produits phytopharmaceutiques sont (au regard du nombre de molécules et des tonnages de produit) principalement dues aux herbicides de synthèse.

Ils furent surtout utilisés au début du vingtième siècle. Les plus utilisés actuellement sont :


Ils constituent la très large majorité des herbicides du marché actuel. Par commodité, on les regroupe suivant leur type de pénétration dans le végétal :

Apparues en 1960, les dinitroanilines sont très peu solubles dans l'eau, ont une forte volatilité et sont souvent photodégradables : ce sont donc des produits a incorporer dans le sol, avant à la mise en place de la culture.

Ils agissent en stoppant la croissance des plantules peu après leur germination. Ils sont désignés sous le terme -impropre- d'« antigerminatifs ». Ce sont plus précisément des antimitotiques. 
Ils s'utilisent en pré-levée contre les graminées. Leur toxicité est faible et leur persistance varie selon la dose employée (quelques semaines à un an). Leur nom se termine par le suffixe « line ».

Exemples : benfluraline, butraline, fluchloraline, nitraline, orysaline, pendiméthaline, trifluraline
Ce sont exclusivement des herbicides. Leur absorption est essentiellement racinaire. Véhiculés par la sève brute, ils s'accumulent dans les feuilles où ils inhibent la photosynthèse. Ils ont une très faible solubilité dans l'eau et présentent une assez longue persistance d'action dans le sol (2 à 3 mois) mais variable selon les conditions écologiques rencontrées (sol, pluie, température).
Ils ont une bonne action sur les graminées et sur certaines dicotylédones. Ils sont utilisés en pré ou post-levée. Leur toxicité est quasiment nulle. Leur nom se termine par le suffixe « uron ».

Exemples : chlortoluron, chloroxuron, cycluron, diuron, éthidimuron, fénuron, isoproturon, linuron, monolinuron, méthabenzthiazuron, métobromuron, métoxuron, monuron, thiazafluron, tebuthiuron, thiazafluron, siduron, néburon…

Ce groupe présente une structure cyclique. Ils agissent par inhibition compétitive au niveau du photosystème II. Les molécules actives prennent la place de la plastoquinone au niveau du site actif la protéine D1 dans les thylakoïdes, ceci inhibant la réduction de la plastoquinone et donc le transport électronique. Il y a alors un surplus d'électrons dans le photosystème II, entraînant un stress oxydant et une diminution de la synthèse des sucres. Ils pénètrent par absorption radiculaire et sont véhiculés par la sève brute. Ils sont appliqués directement sur le sol.
Le maïs est une plante très tolérante à ces composés, en particulier à l'atrazine. Le sorgho est également tolérant mais le blé et le soja y sont sensibles. Leur toxicité est faible et leur sélectivité souvent bonne. Leur solubilité dans l'eau est réduite et sont donc peu entrainés dans le sol. Leur persistance peut ainsi atteindre 6 à 12 mois pour certains.

Exemples : atrazine, cyanazine, méthoprotryne, propazine, terbuthylazine, simazine, simétryne, secbumeton, secbumeton, terbuméton, amétryne, desmétryne, prométryne, terbutryne…

Certains produits de cette famille sont des herbicides totaux, d'autres sont sélectifs. Étant absorbés par voies foliaire et racinaire, ils sont indépendants des conditions climatiques. Ils agissent en bloquant l'activité de l'enzyme AHAS (ou ALS) indispensable à la synthèse de 3 acides aminés essentiels : la valine, la leucine et l'isoleucine. Cela empêche la plante de croître et entraîne une sénescence prématurée.
Ce mode d'action explique le peu de toxicité de ces substances à l'égard des animaux et de l'homme, vu que ces derniers ne peuvent synthétiser ces acides aminés, se les procurant à travers les végétaux. 
Utilisés sur céréales ou en désherbage total, ils sont très souples à l'emploi. Leur persistance est de plusieurs mois.

Exemples : imazaméthabenz, imazapyr…

Ils agissent sur la même enzyme que les imidazolinones, l'acétolactate synthase (ALS)

Exemples : amidosulfuron, azimsulfuron, chlorsulfuron …

Synthétisées à partir de 1964, ces molécules possèdent 2 noyaux benzènes reliés par un oxygène. Ils sont absorbés par les feuilles et les racines. Leur transport dans la plante est très limité, ils ont une action de contact. Ils ont un effet inhibiteur sur la croissance des méristèmes et sont de ce fait généralement utilisés en prélevée ou en post-levée précoce contre les graminées. Ils inhibent également la respiration. Leur solubilité dans l'eau est faible et ils persistent dans les sols de 2 à 4 mois. Leur toxicité vis-à-vis des mammifères est faible. Leur nom se termine généralement par le suffixe « fène ».

Exemples : acifluorfène-sodium, aclonifen, bifénox, bromofénoxime, chlométhoxyfène, diclofop-méthyle, fluorodifène, fomesafen, lactofène, nitrofène, oxyfluorfène.

Connus en 1942, ils sont absorbés par le feuillage et véhiculés par la sève. Leur causticité est nulle. Il en existe 2 grands groupes :
Le plus connu est le 2,4-D (acide dichloro 2,4 phénoxyacétique), très utilisé pour le désherbage sélectif des monocotylédones qui y sont peu sensibles, à la différence des dicotylédones. Le 2,4,5-T est utilisé comme débroussaillant.

Exemples :

Dérivé du benzène, ce groupe comprend des molécules toxiques pour les animaux (insecticide) et les végétaux. Ils sont de couleur jaune. Ils ont été très utilisés contre une large gamme de dicotylédones au stade plantule, pour la protection des céréales en traitement de post-levée. Ce sont des herbicides de contact à action rapide entraînant des nécroses sur les tissus qui se dessèchent et meurent. Ils agissent sur les membranes cellulaires qu'ils perméabilisent aux ions H, abaissant fortement le pH des cellules.
Ils ne se déplacent pas dans la plante, seules les parties touchées seront affectées par l'herbicide par l'apparition de brûlures au point d'impact. 

Ils sont dangereux pour l'homme et l'environnement de par leur toxicité élevée. Le DNOC, à l'état sec, présente de plus des risques d'explosion. Les colorants nitrés sont actuellement remplacés par des produits plus sélectifs.

Exemples : DNBP (dinosèbe), DNOC (dinitro-Oortho-crésol), dinoterbe, PCP (pentachlorophénol).
Conçus en 1945 pour la destruction des graminées, ces herbicides se subdivisent en 4 catégories : 

Ces herbicides ont en commun leur faible toxicité et une volatilité plus ou moins grande. Ils perturbent la division cellulaire (antimitotique) et la physiologie générale de la plante, provoquant le phénomène d'anse en panier, dû aux feuilles ne pouvant pas se déplier. 

Ils s'emploient le plus souvent en pré-levée (thiocarbamates) ou post-semis, parfois en post-levée (phénmediphame, barbame). À l'exception des composés allates, qui persistent plusieurs mois dans le sol, leur persistance est quasiment nulle.

Exemples :

Synthétisés dans les années 1950, ils sont formés par l'association de 2 cycles pyridiniques. Ce sont des accepteurs d'électrons photosynthétiques, actifs sur les réactions lumineuses de la photosynthèse, provoquant l'arrêt de l'assimilation de CO. Ils provoquent également la dégradation des acides gras insaturés, l'ensemble de ces actions débouchant sur la mort. 
Ils se caractérisent par leur rapidité d'action et leur absence de sélectivité (désherbant total), à l'exception du difenzoquat. Ils pénètrent dans les organes aériens mais migrent peu. Ce sont avant tout des produits de contact. Ils sont très solubles dans l'eau et n'ont pas d'effet par traitement de sol car ils sont fortement absorbés par les argiles où, de ce fait, ils ne se dégradent que très lentement. Ils sont très toxiques pour l'homme et les animaux, et on n'en connait pas ou peu d'antidote (selon les molécules).

Exemples : diquat, paraquat, difenzoquat.

Ce sont des herbicides antigraminées qui inhibent l'acétyl-coenzyme A carboxylase (ACCase) dans les chloroplastes. De nombreuses résistances sont apparues, certaines résultent de modifications de l'enzyme chez la plante, d'autres en revanche proviennent d'autres mécanismes (résistance non liée à la cible). 

Exemples : alloxydime-sodium, clodinafop-propargyl.

C'est un désherbant total, c’est-à-dire un herbicide non sélectif. Le mécanisme d'action de ce pesticide est systémique. Il agit en bloquant l'enzyme 5-énolpyruvylshikimate-3-phosphate synthase (EPSPS). C'est un produit irritant et toxique, surtout connu sous la marque Roundup.

La guerre du Viêt Nam a révélé les effets néfastes sur les populations vietnamiennes de « l'agent orange », défoliant formé d'un mélange de 2,4-D et de 2,4,5 T, utilisé au cours de ce conflit par l'armée américaine.
Le 2,4,5-T a montré sa longue rémanence et la haute toxicité d'une dioxine, la 2,3,7,8-tétrachlorodibenzo-p-dioxine* (Dioxine de Seveso) contenue à l'état de traces en mélange avec la matière active. Cette substance est un résidu de synthèse du 2,4,5-T, 100 fois plus toxique que la strychnine. Elle a un effet tératogène prononcé. On estime que 50 millions de litres d'agent orange, soit de matières actives renfermant de dioxine ont été répandu dans la jungle et les rizières du Sud Viêt Nam de 1962 à 1971. Il y a été enregistré de 1974 à 1977 une nette augmentation des cancers du foie dans cette région. Ce produit est accusé d'engendrer une maladie congénitale, la phocomélie (du grec "corps de phoque") : des enfants vietnamiens naissent sans bras et sans jambes. Enfin, il entraine de graves lésions cutanées (chloracné). Le TCDD séjourne longtemps dans l'organisme (30 ans) où sa solubilité dans les graisses en favorise sa concentration et son stockage. Il séjourne longtemps dans le sol, car la dioxine est difficilement assimilée par les plantes.

Le 2,4,5 T est interdit dans 15 pays dont les États-Unis et son usage est sévèrement restreint dans 7 autres.

L'usage d'engrais vert, en évitant la prolifération des adventices, permet de limiter l'utilisation d'herbicides.

Des rotations entre céréales d'hiver et de printemps permettent de « casser » les cycles végétatifs et réduire le « stock » de graines dans le sol.

La lutte contre les adventices passe par une anticipation : on ne traite pas la culture, mais on prépare la parcelle pour la suivante.

Les parties aériennes séchées de différentes plantes (par exemple "Artemisia annua", "Bromus tectorum", "Hordeum murinum", "Origanum vulgare") incorporées dans le sol ont permis de réduire, de manière significative, la croissance des mauvaises herbes. Avec l’apparition de l’acide pélargonique (Finalsan) dans certaines gammes d'herbicides, on peut considérer qu’un premier succès industriel a été obtenu en s’inspirant de l’allélopathie.

Des phénomènes de résistance aux pesticides (comparables aux phénomènes d'antibiorésistance) sont apparus de par le monde, par sélection face à l'usage croissant de désherbants. Certains végétaux se montrent capables de métaboliser et dégrader des pesticides qui tuent d'autres plantes (c'est le cas par exemple des peupliers ("Populus" spp.) pour les herbicides à base de chloroacétanilide, ce qui peut les rendre utiles pour l'épuration d'eaux contaminées par certains désherbants.

Les désherbants peuvent parfois interagir avec les cultures voisines s'ils sont appliqués en période de vent (phénomène de dérive au moment de la pulvérisation, en particulier en cas de pulvérisation par avion ou hélicoptère) ou à la suite d'un lessivage vers des cultures situées vers l'aval de la pente s'il pleut pendant ou peu après la pulvérisation.

On a montré dans les années 1970 que dans le cycle pluri-annuel de la rotation des cultures, les restes de désherbants sélectifs épandus à l'année « n » peuvent persister dans le sol, et à l'année n+1 diminuer les rendements d'autres cultures sensibles à ce désherbant.

Enfin certains désherbants contenant des toxiques non dégradables tels que l'arsenic, notamment utilisé sur les golfs et les cultures de coton aux États-Unis sont responsables d'une lente accumulation de ce polluant dans le sol et parfois dans l'eau (voir l'article Impacts environnementaux des golfs pour plus de détails).

L'usage intensif et généralisé de désherbants a favorisé l'apparition de phénomènes de résistance de la part de certaines adventices dites « mauvaises herbes ». Dans certains cas, on voit même des résistances croisées à plusieurs désherbants (dont désherbants totaux). Ces résistances posent des problèmes de plus en plus complexes pour la gestion des cultures.

On a montré au début des années 2000 qu'une exposition répétée à des doses sublétales (non mortelles) de désherbant présents dans le sol ou l'environnement de la plante-cible favorisait l'apparition de telles résistances.

Ces résistances interagissent avec les cultures au moins de deux manières, en favorisant le maintien d'espèces très compétitives dans les cultures, au détriment des rendements de ces dernières, et parfois en posant des pertes de prix de vente (récoltes moins « propres ») ou sanitaires (certaines adventices semblent favoriser des maladies auxquelles elles résistent mieux que les plantes cultivées, l'ergot du seigle par exemple avec le Vulpin des champs, "Alopecurus myosuroides"). Dès les années 1980, des guides techniques ont été publiés en Angleterre pour essayer de gérer ces résistances qui posent déjà en Angleterre des problèmes permanents pour la protection des cultures d'hiver ; ces guides proposent d'utiliser des désherbants plus efficaces et à leur efficacité maximale (au risque de faire apparaitre de nouvelles formes de résistances chez les mauvaises herbes) ou d'intégrer des pratiques de lutte intégrée dont les cultures semées au printemps, après labour, dans une rotation des cultures, avec des semis d'automne tardif, ou l'agriculture biologique.

L'augmentation de l'efficacité du contrôle des adventices permise par les herbicides peut mettre certaines espèces d'adventices en danger de disparition. Plus généralement, ils entrainent une diminution de la diversité des adventices. La diminution de l'abondance et de la diversité des adventices a également des conséquences négatives sur les populations d'oiseaux granivores.

En France comme dans un grand nombre de pays, l'utilisation d'herbicides pour contrôler le développement des herbes folles a entraîné une contamination largement répandue des eaux de surface et des eaux souterraines par des substances actives de désherbants, en particulier de la famille chimique des triazines : simazine, terbuthylazine Les herbicides de la famille des triazines font l'objet de mesures d'interdiction en France mais pas dans l'Union européenne. La contamination des eaux peut être le fait de la substance active ou de ses produits de dégradation : l'acide aminométhylphosphonique (AMPA), un produit de dégradation du glyphosate, et les produits de dégradation de l'atrazine sont fréquemment observés. Ces pollutions entraînent une hausse des coûts de potabilisation. Des résidus de pesticides sont également détectables dans de nombreux sols, y compris loin de leur zone d'application.

L’utilisation des pesticides est à l’origine de maladies touchant les agriculteurs et leur famille : dépression, cancer, dégénérescence rétinienne, problèmes respiratoires, maladie de Parkinson et malformations congénitales.

L'application des herbicides par pulvérisation doit respecter les recommandations du fabricant et la législation qui peut varier selon les pays. 

En Europe, "".

Il existe de nombreuses marques de désherbants, on peut ainsi citer : Roundup, Charlygranger, Fertiligène, Resolva






</doc>
<doc id="16780" url="https://fr.wikipedia.org/wiki?curid=16780" title="Adventice">
Adventice

Une adventice, appelée également mauvaise herbe, désigne, pour les agriculteurs et les jardiniers, une plante qui pousse dans un endroit (champ, massif...) sans y avoir été intentionnellement installée. Les adventices sont généralement considérées comme nuisibles à la production agricole, bien qu'elles puissent également être bénéfiques. 

Leur contrôle est le principal objectif des pratiques agricoles de désherbage.

En agronomie, ce mot désigne une plante herbacée ou ligneuse qui se trouve dans un agroécosystème sans y avoir été intentionnellement installée. Elle correspond approximativement aux expressions « mauvaises herbes » ou « herbes folles » dans le langage courant. Le terme « adventice » a été introduit par les agronomes à partir de la fin du pour remplacer celui de « mauvaise herbe », considéré comme non-neutre. En effet, les espèces de plantes adventices peuvent s'avérer bénéfiques, neutres ou néfastes pour les activités humaines suivant le contexte dans lequel elles poussent. Le terme « mauvaise herbe » désigne plus spécifiquement une plante dont la présence est indésirable à un endroit donné. 

Le terme est parfois utilisé hors du domaine agronomique: on trouve l'appellation « adventice des cours d'eau » pour des plantes qui entravent par leur développement les activités nautiques ou « adventice des prairies permanentes ».

Le terme « messicole », bien qu’imparfaitement défini, s’applique aux adventices annuelles, à germination hivernale, strictement inféodées aux champs de céréales qui n’entrent pas en compétition avec la culture mais présentent une valeur patrimoniale ou de support pour la diversité faunistique.

Le terme mauvaise herbe nuisible (traduction de l'anglais "noxious weed)" est une notion anglo-saxonne proche du français « mauvaise herbe » qui désigne généralement des adventices introduites et invasives.

La malherbologie désigne l'ensemble des sciences et des techniques permettant d’étudier ces « mauvaises herbes » qui croissent spontanément, pour les combattre.

Dans le cadre de la production agricole, les adventices peuvent être des espèces non cultivées installées dans un champ, mais aussi les repousses d'une culture précédente :
Il existe en France 220 espèces d'adventices importantes, mais 1200 espèces peuvent se rencontrer dans les agroécosystèmes et 26 sont très fréquentes. Les adventices appartiennent à un grand nombre de familles botaniques mais plus de la moitié des espèces fréquemment rencontrées appartiennent à l’une des familles suivantes : Astéracées, Poacées, Cyperacées, Polygonacées, Brassicacées et Apiacées. La famille des Poacées contient le plus grand nombre d'adventices (mais aussi le plus grand nombre de plantes cultivées).

En zone tempérée, dans un bassin de production donné, le nombre des principales espèces d'herbes folles à connaître est de l'ordre d'une trentaine.

Les adventices présentent généralement une stratégie écologique de type rudéral. Cette stratégie est adaptée aux environnements fréquemment perturbés et riches en ressources (lumière, nutriment) comme les agroécosystèmes. La stratégie rudérale est caractérisée par des exigences nutritionnelles élevées, un cycle court (caractérisé par un fort taux de croissance, une petite taille et une floraison précoce) et un fort investissement dans la reproduction (production d’un grand nombre de petites graines, maintenant leur capacité germinative sur une longue période). Néanmoins, les adventices présentent d'autres stratégies, intermédiaire entre une stratégie totalement rudérale et une stratégie totalement compétitive

Les adventices peuvent être :
Les communautés d’adventices sont largement ouvertes aux nouvelles espèces en raison de trois caractéristique du champ cultivé : faible diversité, perturbations fréquentes, ressources abondantes. Elles se reconstituent après chaque perturbations, à partir de la banque de graines, accumulée dans le sol depuis parfois plusieurs années.

Les facteurs qui influent les communautés d’adventices peuvent être divisés en trois catégories : les conditions locales (climat, type de sol, structure du paysage), les facteurs abiotiques liés à la culture (herbicides, travail du sol, fertilisation) et les facteurs biotiques (culture, pathogènes et ravageurs, microorganismes).

Certaines espèces d'adventices étaient présentes dans les toundras européennes pendant la dernière glaciation. D'autres espèces proviennent des habitats habituels des espèces rudérales (écosystèmes fréquemment perturbés et souvent riches en nutriments) : bords de rivières et côtes, éboulis rocheux, dunes, falaises, marais salés...

Les adventices peuvent migrer mélangées aux lots de semences. Les adventices du Proche-Orient se sont ainsi propagées en Europe au fur et à mesure que l'agriculture s'étendait. Déjà adaptées aux pratiques agricoles, elles se sont progressivement adaptées au climat

Elles peuvent également être introduites dans des régions où l'agriculture existe déjà, par les activités humaines : transport de marchandises, de bétail, de graines, déplacement des personnes et des véhicules... Dans ces conditions, leur implantation connait plusieurs phases. Elles constituent tout d'abord une petite population très localisée. dans une deuxième phase, elles se propagent le long des axes de communication ou peuvent s'implanter dans les friches industrielles ou agricoles. À ce stade, elles sont généralement incapables de s'implanter dans les parcelles cultivées. Elles deviendront des adventices lorsque leur évolution ou la mutation des pratiques agricoles leur permettront de pénétrer dans l'agroécosystème.

De nombreuses espèces d'adventices ont subi des processus de mimétisme vavilovien, qui les a menées à développer des caractéristiques morphologiques, physiologiques ou phénologiques proches de celles de la culture qu'elles habitent préférentiellement. C'est le cas du lin bâtard dans le lin cultivé, d"'Echinochloa crus-galli oryzoides" dans le riz, de "Bromus secalinus" dans le seigle ou de la vesce dans la lentille.

Du fait des pressions évolutives particulières aux agroécosystèmes, les espèces d'adventices présentent souvent des modifications importantes de leur génome qui incluent des remaniements chromosomiques, des hybridations entre espèces et des phénomènes de polyploïdie. Ces phénomènes peuvent conduire à l'apparition d'espèces ou de sous-espèces nouvelles (neotaxons). On peut citer parmi ces neotaxons "Lolium temulentum" ou les variants hexaploïdes de "Roemeria hybrida".

Les semences des adventices se caractérisent par :

Selon l'espèce, la période préférentielle de germination varie :

Seule une petite partie (5-10 %) de la banque de graines germe chaque année.

La taux annuel de décroissance indique le pourcentage de graines perdant leur capacité germinative au-cours d'une année. Il est proche de 100 % pour le brome et le tussilage. Le stock semencier de ces plantes a donc pratiquement disparu au bout d'un an. Il est de 10 à 30 % pour le pâturin annuel, les rumex ou le mouron des champs, ce qui signifie que 50 % du stock semencier est encore présent après 7 à 8 ans.

La profondeur à partir de laquelle les adventices sont susceptibles de germer avec succès varie entre les espèces. Elle est de 2 à pour le coquelicot ou le pâturin annuel mais peut atteindre plus de pour le vulpin des champs et le gaillet grateron, et jusqu'à pour la folle avoine.

La capacité de production de semence va de quelques centaines à plus de graines par plante.

Parmi les adventices produisant moins de 500 graines par plante, on peut citer la folle avoine et le brome stérile. Parmi les adventices pouvant produire plus de graines par plante on peut citer le coquelicot.

La "« nuisibilité »" des adventices pour l'homme prend plusieurs formes:

Les adventices sont les organismes responsables de la plus grosse perte potentielle de rendement sur les grandes cultures (blé, orge, maïs...) et sont responsables, très variables à l'échelle locale d’une perte de rendement de 10 % en moyenne à l’échelle mondiale. Toutes les espèces adventices n'ont pas la même nuisibilité. Les adventices de grande taille, de grande étendue latérale, ayant une forte biomasse et, pour les graminées, un grand nombre de talles, sont plus susceptibles de capturer les ressources avant la culture. Dans le cas de la compétition pour l'eau, la profondeur d'enracinement est également un facteur de nuisibilité. Un fort taux de croissance en début de cycle et une phénologie similaire à celle de la culture sont également des facteurs qui permettent aux adventices de s'installer et de se développer dans la culture et donc d'entrer en compétition.

Étant donné que la compétition est définie comme le partage de ressources entre individus d'une ressource limitée, le niveau de compétition exercé par les adventices dépend surtout de la quantité de ressources disponibles par rapport à la demande des adventices (donc à leur nombre (densité) et leur identité et de la culture.

Les adventices ont également des effets sur le fonctionnement de l’agroécosystème qui sont neutres ou positifs pour les humains. Elles servent de nourriture aux invertébrés, aux oiseaux et aux microorganismes, dont certains sont des auxiliaires de culture. Le liseron joue ainsi plusieurs rôles importants, quand on sait le gérer, dont celui de maintenir les mycorhizes pendant l'hiver. Elles peuvent limiter l’érosion ou fixer de l’azote. Elles peuvent avoir une utilité directe comme engrais, fourrages, aliments, teintures ou source de médicaments. Elles sont également des espèces potentielles pour la domestication: plusieurs cultures actuelles, comme l'avoine et le seigle, sont d'anciennes adventices. L’amarante et le chénopode, considérées dans l'Europe contemporaine comme des adventices problématiques, étaient cultivées par certaines cultures précolombiennes et pourraient constituer des sources de nourriture intéressantes.

Gérard Ducerf et Camille Thiry ont montré dans "Les plantes bio-indicatrices" que leur observation permet de déterminer très finement la nature d’un sol, ses carences et ses excès (matière organique végétale ou animale, tassement, dégradation, excès en nitrates, etc.). 

Les naturalistes chinois ont depuis longtemps observé les caractéristiques des adventices en reliant « utilement leurs connaissances minéralogiques et botaniques, donnant jour à ce que l’on appelle la prospection géo-botanique. Ils avaient remarqué que l’occurrence de certaines plantes en un endroit donné pouvait être indicatrice de la présence souterraine de gisements de zinc, de sélénium, de nickel ou de cuivre. »

Certaines espèces d'adventices, comme les messicoles, peuvent avoir une valeur culturelle.

La gestion des adventices peut se raisonner à différentes échelles de temps et combiner des interventions à différents stades du développement de la culture ou des adventices. Toutes les pratiques agricoles influençant la composition et l'abondance des communautés d'adventices, elles peuvent toutes être utilisées pour gérer leur nuisibilité. Parmi les pratiques visant à limiter le stock de graines, on peut citer la composition de la rotation, le travail du sol et la pratique du faux-semis. Le décalage de la date de semis est une stratégie d'évitement des adventices. La gestion de la fertilisation azotée, les mélanges variétaux, le semis sous couvert, le choix de variétés concurrentielles, de densités de semis élevées, ou d'écartement faible des rangs contribuent à l'atténuation en culture. Enfin, le désherbage mécanique et le désherbage chimique comptent parmi les solutions de rattrapage. Le désherbage manuel est ou a été également utilisé pour certaines cultures (cultures potagères, riz).

La lutte intégrée ou les méthodes de culture modernes s'inspirant des mécanismes naturels - la permaculture, l'agroforesterie, l'agriculture naturelle - limitent le besoin de désherbage en utilisant une combinaison de techniques et d'approches (biologiques, chimiques, physiques, culturales variétales) qui peuvent comprendre une couverture du sol permanente, un paillage de matériaux organiques, l'utilisation d'engrais verts, la densification des cultures de manière à ne pas laisser la lumière atteindre le sol et ainsi empêcher la croissance des mauvaises herbes.

L'espèce cultivée a un fort impact sur la composition de la communauté d'adventices : certaines espèces sont fréquemment associées à une culture donnée : chénopode et amarante dans les betteraves, gaillet et véronique dans les céréales, brassicacées dans le colza etc.. En raison de phénomènes de mimétisme vavilovien, il s'agit souvent d'espèces appartenant à la même famille botanique que la culture, ainsi que d'espèces présentant des caractéristiques écologique ou phénologiques proches de celles de la culture. Ainsi, les cultures sont généralement dominées par des adventices dont la saison de germination est similaire à la saison de semis de la plante cultivée (automne, printemps ou été). Les cultures pérennes (luzerne) présentent également une plus grande proportion d'adventices pérennes que les cultures annuelles. 

Les adventices germant principalement à partir de la banque de graines constituée pendant les 2 ou 3 années précédentes, la composition de la rotation de culture est le levier agronomique majeur permettant de contrôler la composition de la communauté d'adventices et d'empêcher l'implantation d'une flore adventice très spécialisée vis-à-vis de la culture et donc très compétitive. Historiquement, dans les systèmes d'assolement triennal, la succession d'une culture d'hiver et d'une culture de printemps était une des techniques de contrôle des adventices. 

La composition de la rotation culturale conditionne également les pratiques agricoles : des successions de cultures différentes, particulièrement les alternances de cultures monocotylédones et dicotylédones, permettent de varier les herbicides utilisés. L'implantation de cultures sarclées (mais, betterave, navet) permet de mettre en œuvre un désherbage mécanique. Cette diversification des pratiques de désherbage permet d'éviter l'implantation d'une flore adventice très spécialisée et donc compétitive. 

Le travail du sol, même superficiel, détruit les parties aériennes des adventices, fragmente et expose à l'air leurs systèmes racinaires. C'est le principe du désherbage mécanique. Il détruit préférentiellement les espèces pérennes et les monocotylédones. Il peut être combiné avec le faux semis : un léger travail du sol permet d'activer les graines en surface, qui peuvent ensuite être détruites mécaniquement ou chimiquement.

Le travail du sol modifie également la disposition des graines dans le sol. Le semis direct ou le travail du sol sans retournement entraînent une accumulation des graines dans les premiers centimètres du sol, elles ont alors une forte probabilité de germer, ce qui peut être un avantage s'il est possible d'effectuer un désherbage. Elles sont également plus exposées à la prédation, ce qui entraîne une décroissance plus rapide du stock semencier. Ceci favorise les graines sans dormance ou de faible longévité. En revanche, en cas de labour avec retournement, les graines sont distribuées de manière homogène dans le sol Leur probabilité de germination est faible mais elles peuvent conserver leur capacité germinative plusieurs années et germer si le sol est à nouveau retourné. Ceci favorise les graines à dormance longue (à Taux Annuel de Décroissance faible).

Un léger travail du sol (déchaumage superficiel, par exemple) permet de provoquer la germination des graines présentes en surface, qui peuvent ensuite être détruites mécaniquement ou chimiquement.

La fertilisation localisée : en ne déposant la fertilisation qu'au plus près de la plante cultivée, on lui donne un avantage sur les adventices. Même si elles lèvent, celle-ci seront moins vigoureuses que la culture.

Il existe des désherbants totaux et des molécules très spécifiques. Les désherbants totaux sont très efficaces pour désherber des champs avant mise en culture. Afin d'éviter des traitements inutiles et ne pas tuer la plante cultivée, l'agriculteur conventionnel ou l'applicateur de pesticide doit pouvoir identifier les herbes folles présentes dans ses parcelles. Les herbicides sont plus ou moins spécifiques. On distingue deux grandes classes d'herbicides, les anti-dicotylédones, utilisables sur les cultures de monocotylédones et les anti-monocotylédones, utilisables sur les cultures de dicotylédones. Néanmoins, il existe des herbicides dont la spécificité est plus fine.

De manière générale, les herbicides favorisent les adventices appartenant à la même famille que la culture, qui y sont moins sensibles.

Les OGM résistants aux herbicides sont un cas particulier du désherbage chimique permettant d'utiliser des désherbants à spectre plus large.

En agriculture, les bioherbicides sont des agents biologiques visant les plantes. En 1971, un bioherbicide était défini comme une substance destinée à réduire les mauvaises herbes et ne provoquant pas de dégradation de l’environnement (Revue semestrielle de terminologie française, 1971). De nos jours, la définition d’un bioherbicide a évolué. D’après Bailey (2014), les bioherbicides sont des produits d'origine naturelle ayant un pouvoir désherbant

Ces produits peuvent être soit :

Le paillis formé par les plantes de couverture, voire le couvert vivant diminue la levée des adventices et leur croissance précoce.

Le non travail du sol concentre les semences dans les premiers horizons du sol. Après grenaison les semences d'adventices se retrouvent en surface et germent moins bien qu'enfouies 

Une lutte efficace contre les adventices peut entraîner une réduction de la biodiversité des adventices et des oiseaux des champs qui s'en nourrissent. En Midi-Pyrénées, le Conservatoire Botanique Pyrénéen (CBP) de Bagnères-de-Bigorre a publié un inventaire de 150 plantes de moissons inféodées aux cultures, dont de nombreuses en voie de disparition. En effet, ces adventices s'interchangeaient naturellement jusqu'à la première moitié du dans les sacs de semences. L'obsolescence de cette méthode a perturbé la dynamique et la diversité génétique des semences d'adventices. En France, un plan national d'action a été mis en place afin de combattre la perte de diversité des messicoles. 

La majorité des effets néfastes sont liés à l'usage des herbicides. Les plus grands utilisateurs de désherbants sont les agriculteurs conventionnels, mais aussi les jardiniers, qu'ils soient professionnels ou amateurs. La quantité de produits vendue dans les jardineries est bien trop importante, par rapport au rendement nécessaire dans un jardin. 

Il importe de prendre conscience que le désherbage systématique des plantes adventices n'est ni une pratique adaptée ni réfléchie, mais plutôt le reflet d'une incompréhension du fonctionnement des écosystèmes et de la place de l'homme au sein de ceux-ci. À long terme, les effets néfastes de ces pratiques, parfois irrémédiables, sur les écosystèmes et la santé humaine, sont plus importants que les bénéfices, même s’il arrive que les nuisances restent invisibles sur le court terme. 

En France comme dans un grand nombre de pays, l'utilisation d'herbicides pour contrôler le développement des herbes folles a entraîné une contamination largement répandue des eaux de surface et des eaux souterraines,par des substances actives de désherbants, en particulier de la famille chimique des triazines : simazine, terbuthylazine... Les herbicides de la famille des triazines font l'objet de mesures d'interdiction en France mais pas dans l'Union Européenne. La contamination des eaux peut être le fait de la substance active ou de ses produits de dégradation: l'AMPA, un produit de dégradation du glyphosate, et les produits de dégradation de l'atrazine sont fréquemment observées. Ces pollutions entraînent une hausse des coûts de potabilisation. Des résidus de pesticides sont également détectables dans de nombreux sols, y compris loin de leur zone d'application. Ils peuvent avoir des conséquences sur les communautés microbiennes des sols. 

Les traitements répétés sur de grandes surfaces ont causé l'apparition de plantes résistantes à plusieurs types de désherbants. L'amarante traitée depuis des années peut être tellement résistante que . 

L’utilisation des pesticides est à l’origine de maladies touchant les agriculteurs et leur famille : dépression, cancer, dégénérescence rétinienne, problèmes respiratoires, maladie de Parkinson et malformations congénitales. 

La possibilité du développement de cultures de maïs transgéniques, présentant une tolérance à des herbicides, comme le glyphosate (Roundup), ou encore le glufosinate ammonium avec l'événement de transformation T25, suscite des interrogations. Employés dans le respect des bonnes pratiques agricoles, ces OGM devraient effectivement réduire la consommation d'herbicides au cours des premières années d'utilisation. Toutefois, toute utilisation de désherbants, raisonnée ou non, peut induire de nouvelles pollutions de l’eau et provoquer une l'apparition d'une résistance, nécessitant éventuellement l'emploi de quantités accrues des produits. C’est ainsi que les plantes transgéniques risquent de se transformer en outils de sélection des espèces vivantes qu'elles prétendent combattre.

En France, le plan Ecophyto 2018 visait à réduire de 50 % l'usage des pesticides entre 2008 et 2018. Son échéance a été reportée à 2025.

Une partie des adventices européennes étaient probablement déjà présentes en Europe avant l'arrivée de l'agriculture, dans la toundra de l'époque glaciaire ou dans les habitats perturbés, naturels ou créés par les chasseurs-cueilleurs. Néanmoins ces espèces sont difficiles à identifier aujourd'hui.

Beaucoup d'adventices ont été apportées en Europe au moment de la diffusion de l'agriculture, mélangées au graines des plantes cultivées (hémérochorie), accrochées aux poils du bétail ou aux affaires des humains. Elles provenaient majoritairement du proche-orient, lieu d'origine de l'agriculture européenne et centre de domestication majeur, mais certaines venaient également de Grèce et d'Anatolie. Au cours de leur diffusion, elles ont subi une sélection naturelle qui leur a permis de s'adapter à de nouveaux climats. Parmi ces espèces on peut citer "Nigella arvensis, Valerianella echinata, Centaurea cyanus, Papaver argemone", "Camelina sativa", "Avena sativa, Neslia paniculata, Silene noctiflora, Thlaspi arvense".

En Europe, l'agriculture s'est propagée à la fois le long de la côte méditerranéenne et en remontant la vallée du Danube. Ces deux courants ont apporté des espèces ou des génotypes d'adventices différents, adaptés aux climats rencontrés. C'est le cas des sous-espèces "Avena sativa sterilis", présente en Europe du Nord, et "Avena sativa fatua", présente en Europe du Sud.

Certaines espèces remontent de l'Espagne vers la France, suivant la diffusion de l'agriculture méditerranéenne: "Delphinium verdunense, Nigella gallica, Glaucium corniculatum, Hypecoum pendulum, Roemeria hybrida."

Le mouvement d'introduction en provenance du proche orient continu ensuite à un rythme plus faible: "Bifora radians", "Conringia orientalis" et "Myagrum perfoliatum" arrivent au cours du Moyen Âge.

L'ensemble de ces espèces, arrivées avant 1500, sont nommées archeophytes. Les adventices arrivées après 1500 sont nommées neophytes.

La découverte de l'Amérique par les Européens, puis le développement du commerce avec les autres continents, provoque un afflue important de nouvelles espèces en Europe, principalement américaines. Le rythme est d'environ 3 espèces par an jusqu'au puis d'environ 30 espèces par an au . Pour des raisons climatiques, elles s'implantent principalement en Europe du Sud et leur fréquence diminue à mesure que l'on monte vers le nord de l'Europe. En France, environ 200 sur les 1200 espèces d'adventices potentielles sont des neophytes.

Parmi les espèces provenant d'Amérique, on peut citer: "Amaranthus retroflexus, Ambrosia artemisiifolia, Conyza canadensis, Panicum capillare, Setaria parviflora, Conyza bonariensis, Datura stramonium, Dichanthium saccharoides, Galinsoga quadriradiata,  Xanthium stumarium, Aster squamatus, Bidens subalternans, Bromus catharticus, Paspalum dilatatum."

"Artemisia verlotiorum" et "Matricaria discoidea" proviennent d'Extreme-Orient, "Senecio inaequidens" et "Oxalis pes-caprae" d'Afrique du Sud, "Chenopodium pumilio" d'Australie.

En parallèle, les mutations des pratiques agricoles entrainent une diminution de la diversité et de l'abondance des adventices, qui commence à la fin du puis s'amplifie après 1950. Cette perte de biodiversité est due à l'amélioration des pratiques de désherbage: tri des semences, désherbage mécanique des cultures sarclées, allongement des rotations puis, après 1945, utilisation des herbicides.

À partir des années 1950 s'ajoute à cette augmentation de la pression de désherbage, une simplification des rotations de culture et une meilleure maitrise du milieu (chaulage, fertilisation, drainage) qui conduisent à une homogénéisation des sols.

"Linaria arvensis, Filago neglecta, Filago arvensis" et "Nigella arvensis" ne sont plus observées en France après 1920. En 2001, 300 espèces étaient en régression, et 100 étaient menacées. Le nombre moyen d'espèces d'adventices par champ a diminué de 20 % entre 1945 et 2000 en Europe et de 42 % entre 1970 et 2000 en France.

En conséquences, les espèces présentant des stratégies écologiques intermédiaires entre les stratégies stress-tolérantes et rudérales ont disparu ou régressé au profit des espèces rudérales, qui ont un taux de croissance et une capacité reproductive plus élevées. Les espèces spécialistes de types de sols particuliers (calcaires, pauvres, acides, sableux), qui présentent souvent des caractéristiques stress-tolérantes, ont particulièrement régressé en raison de l'homogénéisation des conditions pédologiques. C'est le cas de "Gnaphalium uliginosum, Misopates orontium" et "Stachys arvensis", spécialistes des sols acides et sableux. En revanche, les espèces spécialistes d'une culture donnée, caractérisées par un syndrome de mimétisme vavilovien, ont augmenté leur fréquence en raison de la simplification des rotations de culture. Les espèces généralistes, capables de s'implanter dans plusieurs types de culture et sur plusieurs types de sol, se sont maintenues ou ont augmenté leur fréquence, car elles sont moins sensibles aux variations des pratiques agricoles. C'est le cas de "Senecio vulgaris, Matricaria perforata" et "Cirsium arvense".

Il semble que depuis 1980, le nombre moyen d'espèces d'adventices par parcelle tende à nouveau à augmenter, peut-être en lien avec le développement de l'agriculture biologique et avec la diminution de l'usage des herbicides. Les espèces en augmentation sont principalement des espèces préférant les sols riches, neophytes ou monocotylédones mais rarement des espèces menacées.

Dans les premiers siècles qui suivirent la découverte de l'Amérique par les européens, peu de nouvelles espèces d'adventices pénètrent sur le continent. Cependant, à partir du , plusieurs espèces d'adventices européennes et africaines s'implantent en Amérique, principalement en lien avec les importations de bétail. Pour des raisons climatiques, on retrouve surtout des espèces européennes dans les zones tempérées, des espèces méditerranéennes en Californie et des espèces africaines dans les zones tropicales.

En français, par extension, le terme mauvaise herbe ou mauvaise graine désigne également un adolescent proche de la délinquance. Cette acceptation du terme se retrouve par exemple dans la chanson « "La mauvaise herbe" » de Georges Brassens.

Certaines espèces ont une valeur symbolique importante dans les cultures humaines. Ce sont principalement des espèces messicoles. Le coquelicot est associé à la mémoire des combattants de la première guerre mondiale dans les pays du Commonwealth. Le bleuet remplit ce rôle en France.




</doc>
<doc id="16781" url="https://fr.wikipedia.org/wiki?curid=16781" title="Molluscicide">
Molluscicide

Un produit molluscicide est une substance active ou une préparation ayant la propriété de tuer les mollusques (limaces, ou escargots, y compris aquatiques). Les hélicides sont les produits qui en théorie ne ciblent que les escargots.

En protection des cultures, les molluscicides sont employés principalement pour tuer les limaces et les escargots. 
Ils sont aussi utilisés pour des raisons sanitaires en zone tropicale, pour lutter contre certaines parasitoses telles que les schistosomiase pour lesquelles l'escargot est un hôte intermédiaire obligatoire.

Le plus utilisé est le métaldéhyde présenté sous forme de poudre blanche qui est hautement inflammable. C'est un déshydratant qui provoque chez les mollusques l'émission d'une bave très abondante. L'animal épuise ses réserves hydriques et meurt déshydraté. Ces produits sont également toxiques pour les mammifères.
Des carbamates possèdent des propriétés hélicides (mercaptodiméthur, méthiocarbe).

Quand il s'agit de produits chimiques biocides, ils fonctionnent suivant trois modes :

Des alternatives moins écotoxiques sont recherchées ; 

Elle varie selon le produit, les conditions météorologiques, et le respect des consignes d'utilisation (les granulés doivent être posés en surface et non enterrés). Dans certains cas. 

La recherche essaye de trouver des moyens d'augmenter la durée de libération de la matière active molluscicide, par exemple en la liant dans une matrice gélatineuse réticulée (hydrogel) . Dans un cas, l'efficacité (mesurée en termes de mortalité quotidienne atteignant 100 % des escargots) a été portée de 52 à 73 jours, mais ceci expose aussi les espèces non cibles plus durablement.

Certains molluscicides peuvent produire chez l'homme des allergies cutanées, par exemple les produits à base de clonitralide (Bayluscide) 

Parmi les problèmes toxicologiquement préoccupants induits par cette classe de pesticides figurent 


 


</doc>
<doc id="16783" url="https://fr.wikipedia.org/wiki?curid=16783" title="Nématicide">
Nématicide

Les nématicides ou nématocides sont des biocides contenant une ou plusieurs substance(s) active(s) ou une préparation ayant la propriété de tuer les nématodes. 
Il peut parfois s'agir d'un médicament humain ou vétérinaire (bien qu'on parle alors plutôt d'antiparasitaire) ; 
il peut enfin s'agir d'un traitement alternatif visant à limiter ou contrôler les nématodes, sans faire appel à des substances issues de la chimie de synthèse (utilisé en Agriculture bio par exemple).

Les nématicides, principalement utilisés par l'agriculture industrielle relèvent de trois familles chimiques : 

La plupart des substances nématicides sont en réalité des biocides à large spectre et souvent à la fois insecticide et nématicide (bien que les insectes et les nématodes soient fort éloignés d'un point de vue taxonomique), voire également désherbant et fongicides, tuant la plupart des formes de vie.

Les produits spécifiquement nématicides intéressant les agriculteurs, les serriculteurs ont deux particularités : 

Ces produits sont par nature toxiques et écotoxiques, plus ou moins selon leur spectre biocide et selon leur vitesse de dégradation dans l'environnement, ainsi que selon la nature et toxicité de leurs molécules de dégradation ou biodégradation (métabolites). Ils sont rarement sélectifs, mais plutôt à très large spectre.


Chez l'homme (Parasitologie médicale) et chez l'animal qui peuvent aussi être infestés par des nématodes parasites, on parle plutôt d'antiparasitaires. 
Les nématicides sont alors par exemple


Des produits alternatifs de biocontrôle existent, non issus de la chimie de synthèse et utilisant les moyens développés par la nature pour contrôler les nématodes. Ce sont par exemple :

Différentes méthodes alternatives permettent de lutter contre les infestations de nématodes mais ont aussi l'incovénient de détruire de nombreux organismes vivants utiles. Ce sont par exemple :


L'agriculture biologique cherche à restaurer et entretenir les équilibres écologiques du sol en ne favorisant pas la pullulation de nématodes, en permettant la survie de nématodes prédateurs des nématodes parasites agricoles et en cherchant à renforcer la résistance naturelle des plantes contre les nématodes.

 


</doc>
<doc id="16784" url="https://fr.wikipedia.org/wiki?curid=16784" title="Rodenticide">
Rodenticide

Un produit rodenticide (du latin, "rodere", ronger) ou raticide est une substance active ou une préparation ayant la propriété de tuer des rongeurs.

Les rodenticides sont des biocides classés parmi les méthodes chimiques et létales par rapport au piégeage (non létal) de lutte contre les rongeurs
Au sein de la catégorie des rodenticides chimiques, les anticoagulants sont les plus pratiques et les plus utilisés, car ne présentant pas d'aversion alimentaire, et semble-t-il peu d'« "aversion gustative conditionnée" » de la part des rongeurs (mais de telles aversions sont possibles, et démontrées en laboratoires pour certains anticoagulants absorbés à dose non létale par le rat brun), et parce qu'il existe un antidote en cas d'empoisonnement humain ou d'animaux domestiques (chiens, chat) à la suite de la consommation accidentelle de grains empoisonnés, d'appâts empoisonnés, ou d'un animal empoisonné. Mais des souches de rats et souris résistantes semblent de plus en plus nombreuses.

Les rodenticides sont utilisés dans l'environnement domestique, agricole (élevages, granges, etc), en milieu rural et dans l'industrie agroalimentaire ;
Ils servent à contrôler ou éliminer des populations de souris, surmulots, ragondins, campagnols et par extension taupes (qui ne sont pas des rongeurs mais des insectivores).

Pour le grand public, ils sont ou ont été commercialisés sous plusieurs formes :
Dans la plupart des pays dont France, Belgique, Canada, Suisse, les raticides fumigants (pour fumigations des galeries creusées par les rongeurs) ne peuvent être achetés et utilisés que par des entreprises agréées.

Parfois de l'eau empoisonnée remplace les appâts (il convient alors de l'exposer aux rongeurs sans que d'autres petits animaux, les animaux domestiques, le bétail, les volailles ou les enfants puissent y avoir accès).

Les molécules actives rodenticides sont issues de familles chimiques très diverses, dont :

Autrefois utilisé, il est aujourd'hui interdit (non biodégradable, non dégradable et toxique pour tous les animaux). Il l'a notamment été sous forme de vert de Paris ou vert de Schweinfurt (ancienne dénomination commerciale de l'acéto-arsénite de cuivre, ou "Pigment vert CI 21", très toxique).

La chloropicrine a autrefois été très utilisée contre le rat, mais avec un danger pour l'applicateur lui-même. C'est un gaz de combat utilisé lors de la Première Guerre mondiale qui induit une mort par œdème pulmonaire aigu. Ces produits sont aujourd'hui interdits.

Ce sont les molécules qui sont et qui ont été les plus utilisées comme raticides et souricides (depuis les années 1950-1960).
Avant cela, elles commençaient déjà à être utilisées comme médicament à usage humain pour empêcher le sang des blessures de coaguler, après certaines opérations chirurgicales ou dans le traitement de quelques maladies. Cet usage comme médicament les a fait mieux connaitre et maîtriser, préparant leur utilisation comme rodenticide.

Les anticoagulants utilisés pour tuer les rongeurs sont des dérivés de la 4-hydroxycoumarine : ("warfarine", appelée "coumaphène" en France, coumachlore, coumatétralyl, bromadiolone, difenacoum…) et de l'indane-1,3-dione (chlorophacinone, diphacinone, difenacoum… qui sont des « "antivitamines K" » auxquels les rongeurs sont particulièrement sensibles en raison d'un faible volume sanguin et d'un rythme cardiaque élevé. La mort survient par anémie aiguë, provoquée par les hémorragies accidentelles survenant quelques jours à 2 semaines après l'ingestion. L'antidote est la vitamine K1 (ou K3, mais qui agit moins vite et nécessite une dose un peu plus élevée). Ce type de rodenticide est un poison d'accumulation présenté en mélange avec un appât et il est nécessaire que le rongeur consomme plusieurs doses durant plusieurs jours. La mort survient habituellement 3 à 10 jours après la première prise. Ces produits sont notamment utilisés dans le cadre de campagnes de dératisation.

Les anticoagulants dits "« de seconde génération »" (difenacoum, bromadiolone, brodifacoum et flocoumafen) ont eux un effet plus persistant (plusieurs mois, même après une seule prise alimentaire) ; ainsi un enfant ayant accidentellement ingéré un anticoagulant de seconde génération a dû être traité durant sept mois. Et un adulte ayant fait une tentative de suicide avec un anticoagulant de seconde génération a eu besoin de 8 mois de traitement à la vitamine K.

Ce sont des neurotoxiques (alpha-Chloralose, crimidine, strychnine) qui entraînent rapidement la mort après un coma ou des spasmes musculaires. Pas d'antidote; traitement symptomatique : anti-convulsivants et ventilation. Ces produits sont aujourd'hui interdits.

Les glucosides cardiaques (scilliroside) provoquent la mort par arrêt cardiaque. Pas d'antidote; traitement classique de l'intoxication digitalique.

(calciférol, cholécalciférol), souvent associés au coumaphène.
La dératisation durable est une combinaison entre des technologies innovantes et la connaissance de la vie et du comportement du rongeur en surface et dans les canalisations. La lutte se fait sans effets négatifs sur l’environnement mais aussi avec les plus hautes préoccupations éthiques possibles envers les animaux. Cette méthode s'appuie sur des solutions à long terme et durables respectueuse de l'environnement telles que les méthodes mécaniques et électroniques sans usage de produits phytosanitaires.

Généralement plusieurs techniques de luttes doivent être combinées au sein d'une stratégie réfléchie de lutte, en s'appuyant sur des bases écologiques et notamment sur une bonne connaissance de l'écologie des populations des espèces ""nuisibles"" qu'on cherche à contrôler.

L'association de deux molécules ayant un mécanisme de toxicité différent peut être source de synergies toxiques.
De même, une molécule biocide raticide peut être potentialisée par une autre molécule (non toxique seule). Par exemple la warfarine est potentialisée (c'est-à-dire que son effet sera multiplié) par le triméthoprime-sulfaméthoxazole (fait découvert par hasard chez l'homme, à l'occasion de problèmes induits par une association médicamenteuse).

Chez l'humain adulte, la prise accidentelle de produits rodenticides à base d'anticoagulants n'entraîne généralement pas - à moins d'absorption massive - de troubles de la coagulation, ni d'hémorragie.
Chez l'enfant, des hémorragies graves peuvent survenir.

Ces produits agissent en abaissant le taux de prothrombine dans le sang, nécessaire à la formation du caillot sanguin, entraînant ainsi des hémorragies internes. Les symptômes apparaissent après quelques jours pour une dose élevée, après quelques semaines pour des prises répétées : sang dans les urines, saignement de nez, hémorragie gingivale, sang dans les selles, anémie, faiblesse. La mort peut survenir dans les 5 à 7 jours qui suivent.

Les anticoagulants sont toxiques à faible dose (ils sont dosés 0,005 % à 0,0025 % de l'appât raticide en général, selon les molécules utilisées), comme les autres rodenticides, ils doivent être utilisés avec précaution, et les appâts non consommés doivent être éliminés avec précaution conformément aux notices et à la réglementation, de même que les cadavres d'animaux empoisonnés quand ils sont retrouvés. D'autres espèces que des rongeurs peuvent accidentellement être empoisonnées, dont des ruminants (les jeunes y étant plus sensibles)

En France, des agents d'amertume sont obligatoires (y compris pour les blocs paraffinés), à des doses réglementées pour les produits destinés à être utilisés dans les jardins, afin que les enfants ne les mangent pas.

Ils sont surtout liés aux risques suivants

À titre d'exemple, le difénacoum est considéré (2012) par l'Europe comme '. De plus, une ' (Allemagne);

Apparue dès les années 1960, cette résistance peut combiner deux aspects, héréditaire (génétique) liée à une pression de sélection face à un usage très ou trop général de raticides, et/ou une résistance acquise comportementale (l'animal évite l’appât) et/ou physiologique. Cette résistance semble pour partie comparable aux phénomènes d'antibiorésistance et de maladies nosocomiales observées dans les contextes où des microbes sont constamment exposés aux antibiotiques (de même pour certaines résistances d'insectes aux insecticides).

Une résistance généralisée serait catastrophique pour la sécurité agroalimentaire (5 à 15 % des récoltes céréalières mondiales sont encore pillées ou souillées par des rongeurs, par les rats surtout (20 millions de t/an)) et une résistance génétiquement acquise risque de conduire les dératiseurs à l'utilisation de poisons encore plus dangereux, sans garanties que de nouvelles résistances ne puissent apparaitre.

Par ordre alphabétique (certains de ces produits peuvent être interdits pour certains usages ou dans certains pays)
On trouve aussi dans certains pays des poisons violents (dose unique)




</doc>
<doc id="16786" url="https://fr.wikipedia.org/wiki?curid=16786" title="Nadar">
Nadar

Félix Tournachon, dit Nadar, né le ou le , au 195, rue Saint-Honoré(ancien 4e arrondissement de Paris, 1 arrondissement actuel) et mort le dans la même ville, est un caricaturiste, écrivain, aéronaute et photographe français.

Il publie à partir de 1854 une série de portraits photographiques d'artistes contemporains, parmi lesquels Daniel-François-Esprit Auber, Michel Bakounine, Théodore de Banville, Charles Baudelaire, Hector Berlioz, Sarah Bernhardt, Jean-Baptiste Camille Corot, Gustave Courbet, Gustave Doré, Alexandre Dumas, Jules Favre, Loïe Fuller, Victor Hugo, Émile Zola, Zadoc Kahn, Charles Le Roux, Franz Liszt, Édouard Manet, Guy de Maupassant, Gérard de Nerval, Jacques Offenbach, les frères Élie Reclus et Élisée Reclus, Gioachino Rossini, George Sand, Hector de Sastres, Jules Verne, Richard Wagner et Gustave Bourdin.

Trop souvent réduit à son rôle de photographe, il était aussi un écrivain prolifique dans des genres aussi variés que le roman, la nouvelle, le poème en prose, la brève de comptoir, le témoignage, la plaidoirie ou (sa spécialité) le portrait littéraire.

Le pseudonyme "Nadar" a également été utilisé par une société constituée autour de son demi-frère Adrien Tournachon sous les formes "Nadar jeune" et "Nadar jne", provoquant parfois la confusion. Un arrêt de la Cour impériale de Paris lui a restitué en 1857 la propriété exclusive de ce pseudonyme, sous lequel il signera ses écrits et qui sera utilisé par son atelier photographique sous la gouverne de son fils Paul.

Grand, les cheveux roux, les yeux effarés, fantasque à la jeunesse vagabonde, il se définit lui-même comme .

Ses parents étaient d'origine lyonnaise. Son père, Victor Tournachon, commence son activité à Lyon chez Molin dont il épouse la fille (mère de Nadar). Il change son nom en Tournachon-Molin et s'installe dans la capitale comme éditeur. Le jeune Félix fréquente différents internats de la région parisienne, alors que son père connaît des revers de fortune. Il étudie notamment au Collège Bourbon devenu le lycée Condorcet.

Contrairement à ce qu'affirment plusieurs sources, il n'a jamais fréquenté l'École des mines de Saint-Étienne. Félix commence en réalité des études de médecine à Lyon ; cependant sans soutien financier, à la mort de son père en 1837, il se voit obligé d'y renoncer pour "gagner le pain quotidien" de la famille, dont il a désormais la charge et qui comprend son demi frère, Adrien Tournachon, plus jeune de cinq ans et sa mère Thérèse Maillet deuxième épouse de son père.

Ayant travaillé dans différentes rédactions de journaux lyonnais avant de revenir s'installer à Paris, il effectue divers travaux dans les « petites feuilles » de la presse parisienne. Il collabore à la fondation par Polydore Millaud d'un journal judiciaire, "L'Audience", et fréquente le milieu de la jeunesse artistique popularisé par le roman de Murger : "Scènes de la vie de bohème". Il commence à y côtoyer Gérard de Nerval, Charles Baudelaire et Théodore de Banville. Ses amis artistes le surnomment "Tournadar" à cause d'une habitude répandue dans la jeunesse rebelle vers 1840 de rajouter à la fin de certains mots la terminaison "dar". Vers 1838 une abréviation transforme ce nom de guerre en pseudonyme "Nadar".

La vie est très dure et il subsiste en utilisant divers expédients ; il écrit des romans, dessine des caricatures. Grâce à l'aide financière d'un ami, il se lance, à dix-neuf ans, dans l'aventure de la création d'une revue se voulant prestigieuse, "Le Livre d'or", dont il devient le rédacteur en chef. Grâce à ses relations, il s'assure la collaboration de personnalités, dont Balzac, Alexandre Dumas, Théophile Gautier, Gérard de Nerval, Gavarni et Daumier. L'aventure est obligée de s'arrêter au troisième numéro.

Après cet échec, Félix reprend du service dans les gazettes comme caricaturiste, tout en continuant à publier des nouvelles et des billets fantaisistes. À la veille de la révolution de 1848, il obtient la consécration avec son premier dessin-charge publié dans le journal "Le Charivari".

Le , il s'engage avec son frère dans la légion polonaise, pour porter secours à la Pologne. Son passeport est au nom de "Nadarsky". Il est fait prisonnier et confiné dans une mine, puis il refuse le rapatriement gratuit et revient à pied. Deux mois plus tard, il sera de retour à Paris, coiffé d'une chapka de couleur groseille, après un long voyage lors duquel il fut arrêté en Saxe par des représentants du gouvernement prussien.

Rapidement après son retour, il est engagé comme agent secret par l'éditeur Jules Hetzel, alors chef du cabinet du ministre des Affaires étrangères du gouvernement provisoire. Sa mission est de se renseigner sur d'éventuels mouvements de troupes russes à la frontière prussienne.

De retour à Paris, il reprend ses activités de caricaturiste auprès de "petits journaux", tandis que sa renommée s'établit peu à peu. À partir de 1851, il s'attelle à un grand projet de "Musée des gloires contemporaines", pour lequel, avec l'aide de plusieurs collaborateurs, il rencontre les grands hommes du moment afin de les dessiner. L'ensemble de ce travail concerne plus de trois cents grands hommes de l'époque sur un total de plus de vignettes et constitue un panthéon qui lui apportera la notoriété, sous le nom de "Panthéon Nadar" en quatre feuillets, dont un seul paraît.

Il n'hésite pas à caricaturer sa propre activité de photographe. Par exemple, dans le numéro 20 du "petit journal pour rire" dont il était rédacteur en chef, en première page, il y a une caricature signé Nadar dont le titre est « une théorie photographique par Nadar » avec pour commentaire : « Monsieur, c'est pour le portrait de mon mari qui est mort il y a deux ans à Buenos-Ayres : je voulais le faire peindre de mémoire, mais on m'a dit que la photographie faisait bien plus ressemblant que la peinture... ». 
Sa nouvelle aisance lui permet d'emménager dans un pavillon mansardé du 113 rue Saint-Lazare, où il peut disposer d'un jardin bénéficiant de la lumière naturelle. C'est dans ce jardin que seront réalisés ses chefs-d'œuvre, continuant l'œuvre des portraits entreprise avec la caricature, désormais continuée avec une nouvelle technique : la photographie.

À partir de cette époque, la technique du portrait est maîtrisée et les travaux sont de qualité. Les prix évoluent à la baisse. De nombreux ateliers photographiques ouvrent et les personnalités — les élites du monde des arts, des lettres, mais aussi de la politique, du théâtre et même de l'Église — peut-être attirés par leur côté narcissique, n'hésitent pas à « se faire tirer le portrait ». Ce sont ces œuvres que l'on retrouve dans les papeteries sous forme d'estampes et de photographies.

Le 11 septembre 1854, il se marie à Paris avec Ernestine Constance Lefèbvre, jeune femme issue d'une riche famille protestante. Malgré le mariage, il continue d'offrir l'hospitalité à ses nombreux amis, comme à l'époque de la Bohème. Nadar se brouille avec son frère cadet, qui s'était lui aussi lancé, avec son appui, dans le métier de photographe-portraitiste, mais voulait aussi utiliser le nom de «"Nadar"». Il s'ensuit un procès gagné par Félix en 1857.

Nadar souhaite que l'appareil de photographie puisse désormais être emporté à l'extérieur et en voyage, aussi facilement que le chevalet du peintre, il va commencer aussi à expérimenter la photographie embarquée dans un ballon, il fut donc, dès 1858 le pionnier de la photographie aérienne, avec ses vues du Petit Bicêtre. Daumier représenta Nadar opérant avec difficulté lors d'une ascension aérienne, avec cette légende prémonitoire : "Nadar, élevant la photographie à la hauteur de l'Art" le 25 mai 1862.

En 1860, manquant de place, Nadar déménage de la rue Saint-Lazare au boulevard des Capucines. Il fait installer au fronton de son immeuble une immense enseigne, dessinée par Antoine Lumière et éclairée au gaz.

Il expérimente l'éclairage à la poudre de magnésium, plus facile à brûler qu’en bloc. Complexe à mettre en œuvre, ce procédé, qui consiste à brûler de la poudre de magnésium, s’avère très dangereux car le magnésium est inflammable et dégage beaucoup de fumée. De plus, le déclenchement du flash se faisant manuellement, il arrivait qu'il ne se produise pas au bon moment (trop tôt ou trop tard). Nadar tente ensuite une nouvelle expérience qu'il décrit dans son livre « Quand j‘étais photographe » :

Il effectue une démonstration pour le journal "La Presse scientifique" et dépose le brevet de photographie à la lumière artificielle en février 1861. Nadar est conscient de la portée de son invention. Désormais, il est possible de révéler au public le monde souterrain. Il le prouve en s'attaquant à un nouveau chantier : la photographie des sous-sols de Paris, c'est-à-dire les catacombes et les égouts.

En avril 1874, la première exposition des peintres impressionnistes se produit dans son ancien studio. On lui en a souvent attribué l'organisation ; en fait, il s'agissait de son ancien studio qu'il louait. Il est aussi possible, mais non prouvé, qu'il ait demandé à son locataire d'abriter les impressionnistes, mais il ne fut pas en tout cas l'organisateur de l'exposition.

Après le déménagement de son atelier rouge, sa femme lance et gère, avec 20 salariés, un nouvel établissement "fort aristocratique" rue d'Anjou-Saint-Honoré dont son fils deviendra très jeune le directeur artistique.

Nadar a fustigé les canons de représentation et, écœuré par l'évolution de la production raille ses concurrents, qui se contentent « d'un format à peu près unique, singulièrement pratique pour l'espace de nos logements bourgeois. Sans s'occuper autrement de la disposition des lignes selon le point de vue le plus favorable au modèle, ni de l'expression de son visage, non plus que de la façon dont la lumière éclaire tout cela. On installait le client à une place invariable, et l'on obtenait de lui un unique cliché, terne et gris à la va-comme-je-te-pousse ».

Très curieux des nouveautés techniques de son temps, il se lança avec passion dans le monde des ballons.

Grâce aux frères Louis et Jules Godard, aéronautiers aguerris (Eugène Godard), il réalise près de Paris la première photographie aérienne en 1858, depuis un « vol captif » à au-dessus du "Petit-Bicêtre" (actuel Petit-Clamart). Il est obligé d'alléger au maximum et ne peut embarquer sa "guillotine horizontale". Leur coopération durera jusqu'en 1863 (grave brouille lors de la construction du ballon « Le Géant »).

Les aventures de Nadar inspireront Jules Verne pour "Cinq semaines en ballon" écrit en 1862. Un des héros de "De la Terre à la Lune" et "Autour de la Lune" — romans parus en 1865 et 1869 — s'appelle d'ailleurs Michel Ardan, anagramme de Nadar.

Jules Verne le décrit ainsi :
C'est un homme de 42 ans, grand, mais un peu voûté déjà, comme ces cariatides qui portent des balcons sur leurs épaules. Sa tête forte, véritable hure de lion, secouait par instants une chevelure ardente, qui lui faisait une véritable crinière. Une face courte, large aux tempes, agrémentée d'une moustache hérissée comme les barbes d'un chat et de petits bouquets un peu égarés, un regard myope, complémentaient cette physionomie éminemment féline.

En 1863, il fonde la "Société d’encouragement de la navigation aérienne au moyen du plus lourd que l’air". Il fait construire un immense ballon, « Le Géant », haut de 40 mètres et contenant 6 000 m³ de gaz, dont les ascensions publiques devaient réunir de quoi financer les travaux de la Société. Le 4 octobre, le premier vol du Géant a lieu à Paris avec 13 personnes à bord dont Jules Verne, mais également plusieurs invités. Le ballon perd rapidement de la hauteur et atterrit à Meaux, à moins de de Paris. Nadar recommence l'expérience le 18 octobre avec son épouse. Dans les environs de Hanovre, le ballon atterrit durement et est entraîné sur . Le récit de cette catastrophe par Nadar est repris par la presse dans toute l'Europe. D'autres ascensions auront lieu mais sans le succès public escompté. Nadar doit donc arrêter l'aventure du "Géant" par manque d'argent.

Il fonde, en 1867, avec d'autres passionnés comme lui, la revue "L'aéronaute".
En 1870-1871, lors du siège de Paris par les Allemands, il constitue de son propre chef une « Compagnie d’Aérostiers » avec Camille Legrand, dit Dartois, et Jules Dufour, dit Duruof, dont le but est la construction de ballons militaires pour les mettre à la disposition du gouvernement. Ils établissent un campement sur la place Saint-Pierre, au pied de la butte Montmartre, où naît la poste aérienne du siège. Les ballons permettaient de surveiller l’ennemi, d’établir des relevés cartographiques et également d’acheminer du courrier. Nadar baptise ses ballons : "le George-Sand", "l’Armand-Barbès" et "le Louis-Blanc". C'est à bord de "l’Armand-Barbès" que Léon Gambetta, ministre de l’Intérieur, quitte Paris le <time>7 octobre 1870</time> pour se rendre à Tours afin d'y organiser la résistance à l’ennemi. Mais le gouvernement se détourne de Nadar, jugé trop « révolutionnaire », et préfère financer d'autres entreprises.

Au total, 66 ballons seront construits entre le <time>23 septembre 1870</time> et le <time>28 janvier 1871</time> qui transporteront 11 tonnes de courrier, soit 2,5 millions de lettres. Cinq des ballons seront capturés par l'ennemi. Cette première fabrication en série d'aéronefs, marque la naissance officielle de l'industrie aéronautique. Deux « usines » avaient été installées dans les gares de chemin de fer réquisitionnées : les frères Godard à la gare de Lyon et Dartois et Yon à la gare du Nord.

Après l'épisode de la Commune, Nadar se retrouve complètement ruiné et recommence brièvement une activité dans la photographie, mais pour réaliser avant tout des travaux qui lui assurent sa subsistance.

En 1886, il accompagne son fils Paul Tournachon pour réaliser une interview du chimiste Eugène Chevreul illustrée par des photos. Ce double travail, paru le 5 septembre dans "le Journal illustré" peut certainement être considéré comme le premier reportage photographique réalisé en même temps que l'entretien journalistique dont Paul assure l'illustration.

En 1887, il s'installe au manoir de l'Ermitage de la Forêt de Sénart où il accueille ses amis dans le besoin, jusqu'en 1894. Il est alors ruiné et malade, sa femme est devenue hémiplégique à la suite d'un choc affectif concernant son fils.

Décidé cette même année 1894 à s'installer avec sa chère femme malade dans le Midi, Nadar tente de nouveau sa chance à l'âge de 77 ans. Il laisse à son fils la gestion de ses affaires à Paris, et fonde à Marseille un atelier photographique. Nadar, «"doyen des photographes français"» devient dans la région de Marseille une véritable gloire et se lie d'amitié avec l'écrivain Frédéric Mistral.

À cette époque, Nadar s'intéresse à la photostérie, application de la photogravure qui donne une image en relief rappelant la sculpture. C'est lui qui fait sortir la photostérie du laboratoire où Lernac, son inventeur, l'eut peut-être laissée dormir. Et c'est lui qui, par sa connaissance approfondie de la technique photographique, parvient à la rendre industriellement pratique.

En 1900, Nadar triomphe à l'Exposition Universelle de Paris, avec une rétrospective de son œuvre, organisée par son fils. Il revient en 1904 à Paris, où il meurt le , à quelques jours de ses 90 ans. Il est enterré au cimetière du Père-Lachaise ( division).

Les relations entre Félix Nadar et son fils, Paul, ont été compliquées. Marchant sur les traces de son père, Paul se lance dans la photographie à son tour et devient très jeune le véritable directeur artistique de l'atelier sous l'égide de son père. Il réalisé une série de photographies du chimiste Eugène Chevreul en 1886 pendant l'interview que le centenaire donne à son père. Toutefois, décalage de générations oblige, des divergences artistiques apparaissent entre le père et le fils. Tandis que Félix Nadar privilégiait les poses solennelles et graves, son fils avait une conception plus fantaisiste de la photographie. Paul utilise parfois des trucages et s'intéresse davantage aux gens du spectacle.

Le pseudonyme "Nadar" fut utilisé pendant quelque temps par son frère Adrien pour ses photographies, sous les formes "Nadar jeune" et "Nadar jne". Cette signature provoqua la confusion et fut la cause d'un procès entre les deux frères, de mars 1856 à décembre 1857, qui permit à Félix d'être le seul utilisateur du pseudonyme. Ce procès fut l'un des premiers de ce genre, sur le statut d'auteur photographe.

Son fils, Paul, réutilisa plus tard le pseudonyme avec la permission de son père.




La Bibliothèque historique de la ville de Paris ainsi que le Musée Carnavalet se partagent un fonds Nadar consacré à l'aérostation, acheté par la Ville de Paris et composé d'environ 2500 manuscrits.

En Belgique, les barrières de police sont aussi nommées barrières Nadar à la suite de sa visite à Bruxelles du 26 septembre 1864 avec le ballon géant où il fit ériger des barrières mobiles afin de garder à distance la foule qui se massait au jardin botanique, qui ont impressionné les Bruxellois.





</doc>
<doc id="16787" url="https://fr.wikipedia.org/wiki?curid=16787" title="Secret bancaire">
Secret bancaire

Le secret bancaire désigne, dans son acception première, l'obligation qu'ont les banques de ne pas livrer des informations sur leurs clients à des tiers. Il relève du secret professionnel. Ce procès se trouve en France, en Suisse, en Autriche et au Liban uniquement. Par extension, le terme désigne parfois les mécanismes qui permettent à des personnes morales ou physiques de détenir des avoirs bancaires de façon plus ou moins anonyme.

La notion de secret professionnel, et donc de secret bancaire, est assez variable selon les pays. Le principe commun est une obligation légale (sous peine de sanction pénale) pour les banquiers de maintenir la confidentialité des informations obtenues sur leurs clients lors de l'exercice de leurs fonctions. Les différences entre les législations se situent principalement dans les mécanismes de divulgation d'information (et donc de rupture du secret).
En Autriche, le secret bancaire est inscrit dans la constitution. Actuellement, c'est le dernier pays de l'Union européenne à résister à sa levée. Un sixième de l'argent placé en Autriche appartient à des étrangers (52 milliards d'euros en tout, un chiffre qui a doublé en dix ans). L'Autriche est devenue le coffre-fort de certains fraudeurs allemands et oligarques russes.

En France, le secret bancaire est un secret professionnel comme un autre (article L. 511-33 du Code monétaire et financier), avec une restriction importante : certaines administrations ont automatiquement accès aux informations qu'elles demandent. Il s'agit de l'administration fiscale, des services des douanes, de la banque de France, de la commission bancaire et de l'autorité des marchés financiers (AMF). En outre, la justice peut avoir accès aux informations concernées par le secret bancaire dans le cadre d'une procédure pénale. Le secret bancaire peut, dans certains cas très précis, être levé lorsque la demande émane du bénéficiaire du compte bancaire, et lorsque la banque est partie à l'instance.

Dès 1990, la France a instauré des lois obligeant les établissements bancaires à signaler de leur propre initiative toute transaction suspecte (loi du 12 juillet 1990, devenues les articles L. 563-1 et L. 562-2 du Code monétaire et financier). Les déclarations sont transmises à la cellule TRACFIN (traitement du renseignement et action contre les circuits financiers clandestins) créée à cet effet. 

En 2008, l'article L. 511-33 est modifié afin d'autoriser les agences de notation à accéder à des informations couvertes par le secret professionnel.

En France, l'administration possède un droit d'accès direct sans contrôle judiciaire aux informations détenues par les banques. De fait, le secret bancaire en France est limité au secret professionnel de ses "agents". Les justifications pour ces limitations sont la lutte contre la fraude fiscale et celle contre le blanchiment d'argent.

Le secret bancaire est régi aux États-Unis par l'US Bank Secrecy Act de 1970. Il a subi des modifications à la suite de la promulgation de l'USA PATRIOT Act.

En Suisse, le secret bancaire est réglé par l'article 47 de la Loi fédérale sur les banques et les caisses d'épargne . Cet article, entré en vigueur le 8 novembre 1934 et révisé depuis, prévoit l'emprisonnement pour trois ans au plus en cas de violation volontaire du secret bancaire et d'une amende de 250.000 CHF au plus si la violation du secret bancaire a été commise par négligence.

Les exceptions au secret sont beaucoup plus rares qu'en France. En fait, seule la justice peut obtenir la levée du secret bancaire dans le cadre d'une procédure pénale. L'administration ne possède pas de droit d'accès aux informations détenues par les banques, à l'exception toutefois de la FINMA, l'autorité fédérale de surveillance des marchés financiers, chargée de la surveillance du système bancaire.

La Suisse peut échanger des informations avec les autres pays par deux voies distinctes.



Les comptes anonymes n'existent plus en Suisse depuis 1991 et il est donc théoriquement possible de retrouver les propriétaires d'un compte en cas de levée du secret bancaire. Cependant, certaines opérations peuvent encore être réalisées par un intermédiaire financier (lui-même soumis au secret professionnel) sans que celui-ci ne doive dévoiler l'identité de son client, excepté sur requête d'un juge. 

Le secret bancaire suisse prend une forme proche de celle en vigueur actuellement avec la loi bancaire de 1934 qui consacre une tradition et étend son champ d'application aux étrangers. La naissance du secret bancaire a répondu à trois enjeux géopolitiques :
En 1934, la « loi fédérale sur les banques et les caisses d'épargne » est promulguée et elle répond aux inquiétudes du milieu bancaire à tous les niveaux : le renforcement du secret bancaire, dont la levée est passible dorénavant de poursuites pénales, est instauré au milieu des nombreux textes précisant les nouvelles règles prudentielles bancaires protégeant les petits épargnants. Le résultat est là : les cantons socialistes ne peuvent plus réformer la fiscalité et faire tomber le secret bancaire, l’État fédéral se doit de respecter la confidentialité des usagers bancaires et les autorités fiscales étrangères, comme celle de la France, ne peuvent plus poursuivre leurs ressortissants qui trouvent refuge en Suisse pour leurs avoirs financiers.

La création du secret bancaire suisse n’a donc aucun rapport avec la défense des victimes du nazisme. Elle est une réponse à un enjeu économique, à savoir la défense de l’attractivité de la place financière suisse, et aussi à des rivalités géopolitiques sur plusieurs niveaux régionaux (canton, fédération, étranger).

Cette représentation d’un secret bancaire bâti sur une volonté « humanitaire » a été bien utile pour faire taire bien des critiques à son égard jusqu’à la crise des « fonds en déshérence » à la fin des années 90, crise qui a mis en lumière la tromperie - rempart contre le nazisme - et l’absence de volonté de la part des banques suisses dans l’identification des fonds déposés par les victimes de la Seconde Guerre mondiale.

Sous la pression de l'administration américaine avec la loi FATCA et de certains pays européens, la Suisse et Singapour concluent le un accord pour la levée du secret bancaire via des échanges automatiques de renseignements bancaires.

Depuis 2005, l'OCDE a mis en place un modèle de convention fiscale visant à limiter le secret bancaire et à favoriser la coopération internationale afin de lutter contre l'évasion fiscale. Celui-ci fut approuvé au G20 de Berlin de 2004 puis, en octobre 2008, par le Comité d'experts de l'ONU sur la coopération internationale en matière fiscale.

Une « liste noire » de pays a été dressée, les pays s'étant engagés à mettre en œuvre cette convention ayant été retirés de celle-ci pour être inscrits sur une « liste grise », qui contenait par exemple, en septembre 2009, l'Uruguay, le Chili, le Costa Rica et le Guatemala. L'Autriche et le Luxembourg furent retirés de cette liste grise en 2009.

Plusieurs États ont par suite été amenés à réformer leur système juridique en matière de finances. En mai 2010, tous les États s'étant engagés à se mettre en règle conformément à ces standards ; la « liste grise » incluait alors les îles Cook, les îles Marshall, Montserrat, Nauru, Niue, Vanuatu, le Bélize, le Brunei, le Costa Rica, le Guatemala, le Liberia, les Philippines, le Panama et l'Uruguay.

Après le Chili, en mai 2010 le gouvernement de l'Uruguay annonçait le dépôt d'un projet de loi devant limiter le secret bancaire conformément à ces standards internationaux.

Étonnamment, les diverses listes existantes ou ayant existé n'incluent jamais les États-Unis, la Grande-Bretagne et la Chine (au travers des territoires dépendants - par exemple, respectivement: Delaware, Jersey et Hong Kong).

Le secret bancaire a été l'objet de nombreux débats et polémiques car certains le considèrent comme une entrave à la poursuite d'investigation sur des comptes occultes (blanchiment d'argent, financement du terrorisme, etc.). D'autres argumentent que ce n'est pas le secret bancaire qui doit être remis en cause mais le contrôle d'origine ou de destination des fonds.

Le secret bancaire est à distinguer du contrôle d'origine ou de destination des fonds. - Il est facile de blanchir de l'argent dans un pays sans secret bancaire, mais avec des mécanismes de contrôle défaillants au niveau du dépôt - Le contrôle de l'origine des fonds, le contrôle de la destination des fonds et le secret bancaire restreignant les accès de l'administration sont trois aspects bien différents. La plupart des comptes occultes ont bénéficié de contrôles défaillants. 

Le blanchiment d'argent par des organisations mafieuses et les fraudes fiscales sont rendus possibles par une défaillance du contrôle au niveau de l'origine des fonds.

Le financement du terrorisme est rendu possible par une défaillance au niveau de la destination des fonds.

Selon les libéraux, le secret bancaire garantit néanmoins un droit fondamental, celui du respect de la propriété et de la vie privée des individus. En effet, l'accès de l'administration aux comptes bancaires permet à celle-ci de voir le détail des transactions (soutien de partis politiques, d'associations, de mouvements politiques, de groupements religieux, une partie de la consommation, etc).

Dans le cas de contrôles déficients ou inexistants, la levée du secret bancaire permettrait à un organisme extérieur de connaitre a posteriori l'origine et la destination des fonds, connaissance utile essentiellement avant le transfert des fonds.

La levée du secret bancaire empêcherait les complicités et la corruption au sein des organes de contrôle sur l'origine et la destination des fonds, à condition toutefois que ce contrôle existe (ou soit imposé par la loi) et d'avoir accès aux processus internes de la banque (et du personnel intervenant).

L'hypothèse étant qu'au sein d'un organisme bancaire déficient ou corrompu, le respect de la vie privée est de toute manière mis en danger, si l'on est concerné par l'organe déficient ou corrompu. Il n'a donc plus lieu d'être.





</doc>
<doc id="16788" url="https://fr.wikipedia.org/wiki?curid=16788" title="CFDT (homonymie)">
CFDT (homonymie)

CFDT est une abréviation pour:


</doc>
<doc id="16789" url="https://fr.wikipedia.org/wiki?curid=16789" title="Ensemble d'arrivée">
Ensemble d'arrivée

En mathématiques, pour une fonction donnée "f" : "A" → "B", l'ensemble "B" est appelé l'ensemble d'arrivée, but ou codomaine de f.

L'ensemble d'arrivée ne doit pas être confondu avec l'image de f, "f"("A"), qui est en général seulement un sous-ensemble de "B".

Soit la fonction "f" sur l'ensemble des nombres réels définie par

L'ensemble d'arrivée de f est formula_2, mais clairement "f"("x") ne prend jamais de valeurs négatives.
L'image est en fait l'ensemble formula_3 des réels positifs, l'intervalle formula_4.

Nous aurions pu définir la fonction "g" ainsi

Tandis que "f" et "g" ont le même effet quand elles sont appliquées à un nombre réel donné, les fonctions sont différentes puisqu'elles ont des ensembles d'arrivée différents.

L'ensemble d'arrivée peut avoir un effet sur la surjectivité d'une fonction ; dans notre exemple, "g" est une surjection alors que "f" ne l'est pas.

Ensemble de définition


</doc>
<doc id="16798" url="https://fr.wikipedia.org/wiki?curid=16798" title="Surjection">
Surjection

En mathématiques, une surjection ou application surjective est une application pour laquelle tout élément de l'ensemble d'arrivée a au moins un antécédent, c'est-à-dire est image d'au moins un élément de l'ensemble de départ. Il est équivalent de dire que l'ensemble image est égal à l'ensemble d'arrivée.

Il est possible d'appliquer l'adjectif « surjectif » à une fonction (voire à une correspondance) dont le domaine de définition n'est pas tout l'ensemble de départ, mais en général le terme « surjection » est réservé aux applications (qui sont définies sur tout leur ensemble de départ), auxquelles nous nous limiterons dans cet article (pour plus de détails, voir le paragraphe « Fonction et application » de l'article « Application »).

Pour désigner les ensembles de départ et d'arrivée d'une surjection, il est usuel de dire « de "A sur B" » au lieu de « de "A dans B" » comme pour une application en général.

Dans le cas d'une fonction réelle d'une variable réelle, sa surjectivité est équivalente au fait que son graphe intersecte toute droite horizontale.

Une application qui est à la fois surjective et injective est une bijection.

Une application "f" de "X" dans "Y" est dite surjective si :
ou encore : pour tout élément "y" de "Y", il existe au moins un élément "x" de "X" tel que "f"("x") = "y".

On considère le cas d'une station de vacances où un groupe de touristes doit être logé dans un hôtel. Chaque façon de répartir ces touristes dans les chambres de l'hôtel peut être représentée par une application de l'ensemble "X" des touristes vers l'ensemble "Y" des chambres (à chaque touriste est associée une chambre).
La fonction définie par
n'est pas surjective car certains réels ne possèdent pas d'antécédent. Par exemple, il n'y a pas de réel "x" tel que "f"("x") = −4. Mais si on change la définition de "f" en donnant comme ensemble d'arrivée ℝ,
alors elle le devient car chaque réel positif y possède au moins un antécédent : 0 possède exactement un antécédent, 0, et tous les réels "y" strictement positifs en possèdent deux, la racine carrée de "y" et son opposé.

La fonction définie par
est surjective puisque, pour tout réel arbitraire "y", il existe des solutions à l'équation "y" = 2"x" + 1 d'inconnue "x" ; une solution est "x" = ("y" − 1) / 2.

La fonction définie par
n'est pas surjective car les réels strictement plus grands que 1 ou strictement plus petits que –1 n'ont pas d'antécédent. Mais la fonction définie par
qui possède la même expression que "g", mais avec un ensemble d'arrivée qui a été restreint à l'ensemble des réels compris entre –1 et 1, est surjective. En effet, pour tout réel arbitraire "y" de l'intervalle [–1, 1], il existe des solutions à l'équation "y" = cos("x") d'inconnue "x" : ce sont les réels "x" = ±Arccos("y") + 2"k" pour tout entier relatif "k".

Sur ces quelques exemples, on voit qu'il est toujours possible de transformer une application non surjective en une surjection à condition de restreindre son ensemble d'arrivée.

Si "f" est une application de "X" dans "Y" et Im("f") = "f"("X") son ensemble image (c'est-à-dire l'ensemble des images par "f" des éléments de "X"), alors l'application
est une surjection.

En d'autres termes, si "f" est corestreinte à Im("f"), c'est-à-dire si on remplace son ensemble d'arrivée par son ensemble image, elle devient surjective.

Toute application "f" peut être décomposée comme "f = i"∘"s" où "s" est une surjection et "i" une injection. Cette décomposition est unique à un isomorphisme près. Une décomposition est fournie dans le paragraphe détaillé. Une autre (équivalente) est de choisir pour "s" la surjection définie ci-dessus, et pour "i" l'injection canonique de l'image de "f" dans son ensemble d'arrivée.

Pour toute application "f" : "X" → "Y", les quatre propriétés suivantes sont équivalentes :

Soit "f" une application de "X" dans "Y".

Soit "f" une application de "X" dans "Y". Si "f" est , c'est-à-dire s'il existe une application "g" de "Y" dans "X" telle que la fonction composée "f"∘"g" soit égale à l'application identité sur "Y", alors "f" est surjective (d'après une propriété vue plus haut).

Une telle application "g" est appelée une "section", ou "inverse à droite" de "f". Elle est nécessairement injective.

Réciproquement, si "f" est surjective alors elle admet une section. Cette propriété s'appuie sur le fait que l'on peut toujours « remonter les flèches » de "Y" vers "X" . Elle est toujours vraie si "Y" est fini. L'affirmation qu'elle est vraie pour tout ensemble "Y" est équivalente à l'axiome du choix.




</doc>
<doc id="16800" url="https://fr.wikipedia.org/wiki?curid=16800" title="Injection (mathématiques)">
Injection (mathématiques)

Une application "f" est dite injective ou est une injection si tout élément de son ensemble d'arrivée a au plus un antécédent par "f", ce qui revient à dire que deux éléments distincts de son ensemble de départ ont deux images par "f" distinctes.

Lorsque les ensembles de départ et d'arrivée de "f" sont tous les deux égaux à la droite réelle ℝ, "f" est injective si et seulement si son graphe intersecte toute droite horizontale en au plus un point.

Si une application injective est aussi surjective, elle est dite bijective.

Une application "f" : "X" → "Y" est injective si pour tout "y" ∈ "Y", il existe au plus un "x" ∈ "X" tel que "f"("x") = "y", ce qui s'écrit :
formula_1.
L'implication précédente équivaut à sa contraposée :
formula_2.

Prenons le cas d'une station de vacances où un groupe de touristes doit être logé dans un hôtel. Chaque façon de répartir ces touristes dans les chambres de l'hôtel peut être représentée par une application de l'ensemble des touristes, "X", vers l'ensemble des chambres, "Y" (à chaque touriste est associée une chambre).

Considérons l'application "f" : ℝ → ℝ définie par "f"("x") = 2"x" + 1.
Cette application est injective (et même bijective), puisque pour tous nombres réels arbitraires "x" et "x′", si 2"x" + 1 = 2"x′" + 1 alors 2"x" = 2"x′", soit "x" = "x′".

En revanche, l'application "g" : ℝ → ℝ définie par "g"("x") = "x" n'est "pas" injective, parce que (par exemple) "g"(1) = 1 = "g"(−1).

D'autre part, si nous définissons l'application "h" : ℝ → ℝ par la même relation que "g",mais avec l"'ensemble de définition restreint" à l'ensemble des réels positifs, alors l'application "h" "est" injective.
Une explication est que, pour des réels positifs arbitraires donnés "x" et "x′", si "x" = "x′", alors |"x"| = |"x′"|, ainsi "x" = "x′".

Le terme « injection » a été créé par MacLane en 1950 tandis que l'adjectif « injectif » apparaît deux ans plus tard, en 1952, dans les Foundations of Algebraic Topology d'Eilenberg et Steenrod.


</doc>
<doc id="16802" url="https://fr.wikipedia.org/wiki?curid=16802" title="Bijection">
Bijection

En mathématiques, une bijection est une "application bijective". Une application est bijective si et seulement si tout élément de son "ensemble d'arrivée" a un et un seul "antécédent", c'est-à-dire est "image" d'exactement un élément (de son domaine de définition), ou encore si elle est "injective" et "surjective". Les bijections sont aussi parfois appelées correspondances biunivoques.

On peut remarquer que dans cette définition, on n'impose pas de condition aux éléments de l'ensemble de départ, autre que celle qui définit une application : tout élément a une image et une seule.

S'il existe une bijection "f" d'un ensemble "E" dans un ensemble "F" alors il en existe une de "F" dans "E" : la bijection réciproque de "f", qui à chaque élément de "F" associe son antécédent par "f". On peut alors dire que ces ensembles sont en bijection, ou équipotents.

Cantor a le premier démontré que s'il existe une injection de "E" vers "F" et une injection de "F" vers "E" (non nécessairement surjectives), alors "E" et "F" sont équipotents (c'est le théorème de Cantor-Bernstein).

Si deux ensembles "finis" sont équipotents alors ils ont le même nombre d'éléments. L'extension de cette équivalence aux ensembles "infinis" a mené au concept de cardinal d'un ensemble, et à distinguer différentes tailles d'ensembles infinis, qui sont des classes d'équipotence. Ainsi, on peut par exemple montrer que l'ensemble des entiers naturels est de même taille que l'ensemble des rationnels, mais de taille strictement inférieure à l'ensemble des réels. En effet, de formula_1 dans il existe des injections mais pas de surjection.

Une application formula_2 est bijective si tout élément de l'ensemble d'arrivée formula_3 a exactement un antécédent par formula_4, ce qui s'écrit formellement :
ou, ce qui est équivalent, s'il existe une application formula_6 qui, composée à gauche ou à droite par formula_4, donne l'application identité :
c'est-à-dire :
Une telle application formula_11 est alors déterminée de manière unique par formula_4. On l'appelle la "bijection réciproque" de formula_4 et on la note formula_14. C'est aussi une bijection, et sa réciproque est formula_4.

Une bijection de formula_16 dans formula_3 est une relation binaire formula_18 de formula_16 dans formula_3 qui est une application et dont la relation réciproque formula_21 est aussi une application. De façon plus détaillée, formula_18 doit posséder les quatre propriétés suivantes :


L'injectivité de formula_18 équivaut à la fonctionnalité de formula_21 et la surjectivité de formula_18 équivaut à l'« applicativité » de formula_21.

Il est usuel de représenter une relation binaire "fonctionnelle" formula_18 par une "fonction" formula_4 en posant : formula_39 si et seulement si formula_40. Si l'on précise que formula_4 est une "application", on suppose que formula_18 est fonctionnelle "et applicative".

La symétrie entre fonctionnalité et injectivité d'une part, et entre « applicativité » et surjectivité d'autre part, donne que si formula_18 est une relation bijective alors formula_21 l'est aussi.

Prenons le cas d'une station de vacances où un groupe de touristes doit être logé dans un hôtel. Chaque façon de répartir ces touristes dans les chambres de l'hôtel peut être représentée par une application de l'ensemble "X" des touristes vers l'ensemble "Y" des chambres (à chaque touriste est associée une chambre).


Théorème de la bijection


</doc>
<doc id="16804" url="https://fr.wikipedia.org/wiki?curid=16804" title="Égalité (mathématiques)">
Égalité (mathématiques)

En mathématiques, l’égalité est une relation binaire entre objets (souvent appartenant à un même ensemble) signifiant que ces objets sont "identiques", c’est-à-dire que le remplacement de l’un par l’autre dans une expression ne change jamais la valeur de cette dernière.

Une égalité est une proposition pouvant s’écrire à l’aide du signe égal « = », séparant deux expressions mathématiques de même nature (nombres, vecteurs, fonctions, ensembles…) ; la proposition contraire s’écrit à l’aide du symbole de différence « ≠ ». Une proposition d'égalité, ou d'inégalité, peut être vraie ou fausse : en ce sens, elle a aussi une valeur logique dite "valeur de vérité".

Une égalité peut apparaître comme une affirmation, une définition de notation ou encore comme une équation :

Le symbole « = » est parfois utilisé en mathématiques pour d'autres usages que l'égalité :

Dans un ensemble, la relation d'égalité est la seule relation binaire à la fois réflexive, symétrique, antisymétrique, et transitive. C'est en effet la seule relation d'équivalence qui soit également une relation d'ordre.

Pour les propositions logiques, on utilise plutôt les symboles d’équivalence ≡, ↔ ou ⇔.

La logique des prédicats contient des axiomes standards pour les égalités qui formalisent les lois de Leibniz, énoncées par le philosophe Leibniz au . L'idée de Leibniz était que deux choses sont identiques si et seulement si (ssi) elles ont les mêmes propriétés. En formalisant

Cependant, dans la logique de premier ordre, on ne peut pas quantifier les prédicats. Nous avons donc besoin d'un schéma d'axiomes :

Cette série d'axiomes valable pour tout prédicat à une variable, ne prend en compte qu'un seul sens de l'implication: si alors et ont les mêmes propriétés.

Pour construire la réciproque, il suffit d'ajouter : pour tout , 

Ainsi, si et ont les mêmes propriétés, pour le prédicat défini par ssi , nous avons ssi . Or est réalisé, donc est vrai : 

Gottlob Frege, s’inspirant de Leibniz, considérait que deux objets sont égaux si et seulement si on peut les substituer l’un à l’autre partout.


Dans la logique du premier ordre, ceci correspond en réalité à un "schéma d'axiome", car nous ne pouvons pas quantifier des expressions comme F (prédicat fonctionnel)

Quelques exemples:

Les deux premiers axiomes sont les notions communes 2 et 3 du premier livre des Éléments d'Euclide.



Remarque : la relation « est approximativement égal à » dans l'ensemble des réels, n'est pas transitive malgré les apparences, car une somme de petites erreurs finit par faire une grosse différence. La relation « est égal presque partout », elle, reste une relation transitive.

Bien que les propriétés de symétrie et de transitivité soient souvent considérées comme fondamentales (avec la réflexivité, elles caractérisent toutes les relations d'équivalence), elles ne sont ici que des conséquences des propriétés de réflexivité et de substitution.


Le signe = a été introduit par Robert Recorde en 1557, dans "Whetstone of Witte" pour épargner à tous ceux qui effectuaient des calculs (lui, en particulier) d'avoir à écrire "est égal" en toutes lettres. Il semblerait que ce signe représentait la gémellité (deux lignes de même longueur), apparemment synonyme, pour lui, d'égalité. Mais de nombreux autres signes sont à cette époque proposés par divers auteurs et, inversement, le signe = est utilisé pour d'autres usages. Il ne s'impose comme signe de l'égalité qu'au cours du .

Dans l'Égypte ancienne, ce signe existait déjà et symbolisait l'amitié, par opposition à deux lignes se croisant, symbole d'inimitié.



</doc>
<doc id="16806" url="https://fr.wikipedia.org/wiki?curid=16806" title="Triangle d'été">
Triangle d'été

Le Triangle d'été, appelé également Triangle des nuits d'été ou les Trois belles d'été, est un astérisme en forme de triangle formé par 3 des étoiles les plus brillantes qui, dans l'hémisphère nord, sont visibles toute la nuit entre juin et août. Malgré son nom, le Triangle d'été est aussi visible tout le reste de l'année depuis l'hémisphère nord, mais pas toute la nuit.

Ces 3 étoiles sont, de la plus brillante à la moins brillante :

Les distances entre Véga et Altaïr ainsi qu'entre Deneb et Altaïr sont semblables, ce qui fait en sorte que le triangle est presque isocèle.

La surface du triangle recouvre les constellations de la Flèche et du Petit Renard ainsi que l'astérisme de la Croix du Nord.

Ayant pour fond la Voie lactée, ce triangle est très facilement identifiable et sert souvent de point de départ pour retrouver d'autres constellations (comme le Petit Renard en son centre) ou repérer quelques objets du ciel profond, comme M27 ou M57.




</doc>
<doc id="16810" url="https://fr.wikipedia.org/wiki?curid=16810" title="Basse-Californie du Sud">
Basse-Californie du Sud

L'État libre et souverain de Basse-Californie du Sud (en espagnol : "Baja California Sur") est un État du Mexique, situé au nord-ouest du pays. Il est entouré par la Basse-Californie dont il est séparé par le , par l'océan Pacifique et par le golfe de Californie, il a une superficie de et une population de 712 029 habitants en 2015. Sa capitale est la ville de La Paz.

On trouve de nombreuses peintures rupestres dans les grottes, ce qui atteste une présence humaine très ancienne et encore largement inconnue.

La capitale de l'État est La Paz dont l'aéroport s'appelle Manuel Márquez de León International Airport (indicatif IATA : LAP).



</doc>
<doc id="16817" url="https://fr.wikipedia.org/wiki?curid=16817" title="Erlang">
Erlang

Erlang peut faire référence à :



</doc>
<doc id="16818" url="https://fr.wikipedia.org/wiki?curid=16818" title="Wings 3D">
Wings 3D

Wings 3D est un logiciel libre de modélisation polygonale en trois dimensions par surface de subdivision. Il s'inspire des logiciels Nendo et Mirai de Izware. Wings 3D est disponible sous de nombreuses plate-formes, y compris GNU/Linux, Mac OS X et Windows. Il est écrit dans le langage de programmation Erlang et utilise une console virtuelle.

Il est conçu pour la modélisation et le texturage de modèles 3D, grâce à son interface relativement simple d'utilisation. À l'heure actuelle, le logiciel ne permet pas de faire des animations. Il dispose d'un moteur de rendu OpenGL et il est capable d'exporter les modèles dans de nombreux formats comme 3D Studio (3DS), Wavefront (OBJ), Nendo (NDO), VRML (WRL), Renderware (RWX), FBX (sur Windows et Mac OS X), Yafray et Toxic et il peut être directement importé par Blender. Il peut importer le format 3D Studio (3DS), Nendo (NDO), Wavefront (OBJ), FBX (sur Windows et Mac OS X) et Adobe Illustrator 8 (AI). De plus, son système de plugin rend possible l'extension des capacités du logiciel : il est par exemple possible d'utiliser un moteur de rendu par lancer de rayon externe tel que Yafray ou PovRay.

Wings 3D est disponible en français et dispose de tutoriels et d'une interface très légère qui permettent d'en assimiler les bases rapidement.



</doc>
<doc id="16820" url="https://fr.wikipedia.org/wiki?curid=16820" title="Climatologie">
Climatologie

La climatologie est l'étude du climat et de l'état moyen de l'atmosphère, c'est-à-dire la succession des conditions météorologiques sur de longues périodes dans le temps. Il s'agit d'une branche combinée de la géographie physique et de la météorologie, l'étude du temps à court terme étant le domaine de la météorologie opérationnelle. Un climatologue, ou climatologiste, est un spécialiste qui fait l'étude des variations locales et temporelles des climats grâce aux statistiques des données provenant de plusieurs domaines qui affectent le climat.

Si la climatologie s'intéresse essentiellement à l'étude et à la classification des climats existants sur terre, une partie de la discipline traite aussi de l'interaction entre climat et société ; que ce soit l'influence du climat sur l'Homme ou de l'Homme sur le climat.

En règle générale, le climat varie peu, en un endroit donné du globe, sur une durée de l'échelle du siècle. Mais sur des temps géologiques, le climat peut changer considérablement. Par exemple, la Scandinavie a connu plusieurs périodes glaciaires dans le dernier million d'années. L'étude des climats passés est la paléoclimatologie. Cette étude en fonction de l'histoire humaine s'appelle climatologie historique.

La climatologie est constituée d'une multitude de disciplines scientifiques. On y retrouve entre autres les astrophysiciens qui s'intéressent à la quantité d'énergie solaire reçue par la terre, les dynamiciens de l'atmosphère qui s'intéressent aux échanges d'énergie entre les différentes couches de l'atmosphère, les chimistes de l'atmosphère qui étudie la composition de l'air, d'océanographes, de glaciologues, de vulcanologues, des géophysiciens, des biochimistes, de biologistes C'est l'addition du savoir de chacune de ces disciplines qui permet d'obtenir une compréhension globale de l'histoire de notre climat, ainsi que de permettre de faire des projections pour prédire statistiquement son évolution. 

La connaissance de nombreux paramètres, comme la température à différentes altitudes, l'influence des gaz à effet de serre, l'humidité relative, l'évaporation océanique, est nécessaire pour produire des modèles climatiques numériques et anticiper les changements du climat que l'on peut prévoir à plus ou moins long terme (30 ans).

Avant l'observation systématique du temps, il existait déjà un sens inné du climat dans le sens d'une moyenne auquel il était possible de comparer les événements climatiques (ex : décrire une tempête comme forte sous-entend de savoir ce qu'est une tempête moyenne). L'agriculture nécessite aussi une connaissance empirique du climat, par exemple, de la mousson. 

En Europe, les premières traces écrites de climatologie datent de la Grèce antique. Par exemple, Xenophon décrit précisément le climat d'Athènes dans "Les Revenus" et s'intéresse aux liens entre plantes et climat, Herodote s'interroge sur le mécanisme des crues du Nil et Aristote fait le lien entre une Terre sphérique et une diminution de la température vers le nord et le sud (à cause de l'angle que font les rayons du Soleil avec la Terre). En 334 , ce même auteur publie aussi Meterologica, un traité sur la météorologie qui fera autorité sur le sujet jusque dans les années 1700. 

En Chine, la première mention connue du climat date de la dynastie Xia (- av. J.-C.) sous la forme d'un texte d'environ 400 mots appelé Xia Xiao Zheng. Ce texte décrit les conditions météorologiques moyennes de chaque mois de l'année

L'invention du thermomètre dans les années 1600 en Italie marque le commencement de mesures de température régulières, indispensables à la climatologie moderne. L'invention du baromètre et du pluviomètre suivent rapidement en 1643 et 1639. Le premier réseau de mesures météorologiques fut créé en 1653 par Ferdinand II de Médicis en Toscane . Puis, en 1664, commence à Paris la plus longue série d'observations météorologiques connue.

En 1683, Edmond Halley publie une carte mondiale des vents basé sur ses expéditions marines. Il décrit en 1686 le principe de la mousson et des Alizés. Ensuite, George Hadley lie en 1735 les Alizés et la rotation de la Terre par ce qui s'appelle aujourd'hui la/les cellule de Hadley. En Amérique, en 1785, Benjamin Franklin publie la première carte du Gulf Stream et lie ce phénomène à l'action du vent. 

En 1838 Claude Pouillet puis Joseph Tyndall attribuent l'effet de serre naturel à la vapeur d'eau et au gaz carbonique. Pouillet affirme qu'une modification de leurs quantité dans l'atmosphère doit se traduire par un changement climatique.

En 1843, Alexandre von Humboldt, dans un effort de réunir des données éparses et de dégager des lois générales, invente le vocable de climatologie : "Depuis un demi-siècle, on a accumulé des observations de température sous les climats divers sans reconnaître les lois dont elles sont l'expression fidèle, lois qui ne peuvent se manifester qu'en groupant les faits d'après des considérations théoriques". 

En 1882, dans un des premiers livres sur le sujet, Julius von Hann définit la climatologie comme étant "la science des états de l’atmosphère". Il sépare ainsi la climatologie de la météorologie qu'il définit comme la science de l’atmosphère (au moment présent). Dans ce traité, la climatologie se présente sous deux formes: l'étude de l'état moyen de l'atmosphère (classification des climats...) et l'étude des écarts à cette moyenne. 

En 1895, Svante Arrhenius lie l'augmentation du dans l'atmosphère et un réchauffement sensible de la Terre. Il se base pour cela sur les observations de la lune faites dans l’infrarouge qui donnent une estimation des capacités d’absorption de la vapeur d'eau et du . Malgré des calculs inexacts, il énonce une des premières lois sur l'effet de serre: "Si la quantité d’acide carbonique augmente en progression géométrique, l’augmentation de la température suivra, presque avec une progression arithmétique". Il donne pour ordre de grandeur en plus pour un doublement du dans l'air.

Malgré les avancées du , la climatologie n'a, au début du , qu'un impact limité. On étudie surtout les moyennes climatiques, par exemple à travers une classification mondiale des climats.

Durant la deuxième guerre mondiale, principalement à cause du développement de l'aviation, le réseau de mesures météorologiques s'améliore. L’intérêt pour la météorologie augmente ce qui pousse aussi à un regain d’intérêt pour la climatologie. Le développement de l'informatique permet aussi la création des premiers modèles climatologiques comme celui de Norman Phillps en 1956.


Atmosphère : du grec atmos -vapeur humide- et sphère
On entend souvent par le terme atmosphère, la première de ses couches, à savoir la troposphère.
L'atmosphère est une enveloppe gazeuse fondamentale à l'existence des êtres vivants et de la vie en milieu terrestre.
Celle-ci joue également un rôle majeur dans le cycle de l'eau. (évaporation=précipitations)

L'air en son sein est défini en termes de température, pression, charge humide et mouvements ou direction(horizontaux et verticaux).

La partie de l'atmosphère la plus proche de la Terre est donc la troposphère, dans laquelle se jouent les principaux phénomènes météorologiques. Cette couche de l'atmosphère n'apparaît pas régulière dans la mesure où l'on observe une épaisseur plus importante au niveau de l'équateur (17 à ).

On notera la présence de gradients thermiques qui varient sur une échelle horizontale de la troposphère jusqu'à la ionosphère.
Jusqu'à environ d'altitude la température diminue (troposphère supérieure)

L'air, dans cette basse couche (8 à ) est soumis à d'importantes turbulences. Cette instabilité a pour origine les reliefs ainsi que les contrastes thermiques générés par les grands ensembles continentaux et océaniques.
La tropopause constitue la limite supérieure de la troposphère. La température moyenne y est de .

La stratosphère

De 15 à .
La température se remet à augmenter doucement.
La cause est simple et provient de l'absorption par l'ozone des rayons Ultra Violet qu'elle contient.
Le courant-jet, courant horizontal majeur trouve sa place dans cette stratosphère.
La stratopause est la limite supérieure de cette couche.

La mésosphère

De 50 à .

Le gradient thermique redevient négatif. Il le devient à d'altitude d'environ .
La mésopause constitue sa limite supérieure.

L'ionosphère ou thermosphère
De 80 à environ.
Les températures augmentent fortement. 
On assiste à l'intérieur de cette couche atmosphérique au phénomène de dissociation des molécules d'hydrogène et de dioxygène. La thermopause, limite supérieure, reste floue.



Plus de pluies en Europe occidentale, multiplication de mini tornades de type tropical dans des zones tempérées, risque de sécheresse et de désertification de la péninsule Ibérique, modification du Gulf stream entraînant une possible rupture de la circulation thermohaline... les effets les moins facilement prédictibles sont les plus significatifs, à savoir : à partir de quel seuil de rupture s'enclenche une boucle de rétroaction qui, une fois lancée, échappera à toute tentative humaine pour la juguler ?

Ces aspects induisent des réflexions pour agir sur le climat, qui reviendraient à expérimenter sur ce terrain ce qui est scientifiquement confiné à une expérience de laboratoire : lire géo-ingénierie.

La question de l'impact de l'évolution du climat sur le milieu dans lequel évoluent nos sociétés est essentielle. Une des réponses des spécialistes est d'étudier le passé climatique de la Terre (les glaciations et les périodes interglaciaires par exemple) pour en tirer des enseignements (voir paléoclimat), et d'utiliser des modèles de simulation du climat pour tenter d'extrapoler les conséquences d'évolution de certains paramètres (typiquement la température moyenne). Les risques identifiés sont principalement les conséquences d'une augmentation rapide de la température (0,5° durant le , à comparer à une augmentation de 1° en 1000 ans lors des périodes de transition interglaciaire). Les conséquences de cette augmentation de température sont l'augmentation du niveau des océans (avec les risques d'inondation des zones côtières), l'accroissement de la désertification, la modification du régime des moussons, l'extinction d'espèces et la diminution de la biodiversité essentiellement.





</doc>
<doc id="16821" url="https://fr.wikipedia.org/wiki?curid=16821" title="Con">
Con

Con est un mot polysémique et un substantif trivial qui désigne à l'origine le sexe de la femme. Au sens figuré, le mot con est aussi un mot vulgaire en général employé comme insulte dans les pays francophones, . Il désigne une personne stupide, naïve ou désagréable, de même que ses dérivés « connard » et « connasse ». Con a aussi un emploi impersonnel, souvent dépréciatif dans les expressions « jouer au con », « bande de cons » Le mot dérivé « connerie » désigne une erreur, une bêtise, la stupidité en général.
Con est également à l'origine du nom des "confréries de Conards", sociétés festives et carnavalesques traditionnelles de Normandie, Auvergne, Bourgogne…

Con provient de l'étymon latin "" (« gaine, fourreau », par analogie le sexe de la femme). Le terme ne semble pas vulgaire, au contraire :

Chez Martial, auteur licencieux, le mot a clairement un sens sexuel :

Quel que soit l'étymon indo-européen (voir ci-dessous), il semble que la dérivation se soit faite comme pour "sexus" qui est passé du sens général « genre, catégorie de gens », « sexe fort ou faible », à celui de « sexe, organe sexuel ». .

En castillan par exemple, ' (« con »), est moins vulgaire que le mot français. En portugais, ' n'est utilisé comme insulte que quand il est adressé à une femme de mauvaises mœurs, et, moins souvent, d'intelligence limitée — ce dernier usage est un gallicisme d'importation récente.

Vers le , le vocable français prend un sens figuré injurieux et se met en place une construction adjectivale. L'emploi était alors misogyne, exploitant l'impuissance et la passivité du sexe féminin de l'imaginaire collectif. Aujourd'hui, l'absence fréquente d'accord en position d'attribut ou d'apposition (par exemple "Elle est con.") rappelle l'origine nominale de l'expression, sans qu'il soit toutefois fait référence, pas plus qu'en latin, à la vulve.

Une tradition populaire rattache "con" au terme "con(n)in" ou "con(n)il", qui désignait en ancien français le lapin, dérivé du latin '. On retrouve cette racine dans le castillan "conejo", le catalan "conill", l'italien ', l'ancien occitan "conilh", le breton "konifl", l'alsacien "Kénjele", le néerlandais ' ou l'allemand '. Le terme cuniculture (ou cuniculiculture) désigne l'élevage des lapins.

La parenté indo-européenne avec ' (néerlandais) et ' (anglais) n'est pas établie.
' proviendrait en effet en proto-indo-européen soit de "*kust-" (« intestin, rein, vessie »), soit de "*skerǝ-" (« couper »), soit de "* (s)keu-" (« cacher ») qui nous donne aussi "cul", via le latin '.

Les origines possibles de l'étymon germanique "*kunton" donnant "cunt" sont : "*gneH/guneH" ("femme", cf. gynécologie, "queen") soit "*gen/gon" ("créer", "devenir", cf. génétique, gamète) ou bien "*geu-" (creux, cavité) d'après la loi de Grimm. Toutefois certains relient le "*kunton" au latin "" (« coin »), un cognat possible de "cunnus"/"con".

Le dérivé "déconner" avait jusqu'à la fin du le sens premier de se retirer, sens qu'il a complètement perdu aujourd'hui. Son contraire "enconner", signifiant pénétrer vaginalement, composé sur le même mode qu"'enculer", est aujourd'hui pratiquement désuet et réservé à la littérature érotique.

"Connard" est formé par suffixation avec l'affixe péjoratif -ard mais il est possible que le mot ait été influencé par "cornard" ; il n'a, lui, qu'un sens uniquement figuré. "Connasse", en revanche, désignait au départ et jusqu'au une prostituée de bas étage ou inexperte. Son sens figuré de femme sotte est attesté dès le . "Conneau" et ses variantes graphiques "connaud" et "connot", synonymes de "connard", sont devenus obsolètes au cours du .

Les autres dérivés modernes, utilisés dans le sens figuré uniquement sont : "déconnage" et "déconne" pour l'action de débiter ou faire des sottises, "déconneur" pour celui qui aime à les dire ou à les faire, "connement" en tant qu'adverbe et "connerie" pour chose stupide ou sans intérêt.

Les patronymes « Conne », « Connard », « Connart » et variantes n'ont aucun rapport étymologique avec le mot « con » : en Europe continentale, ils proviennent du germanique "con(hardt)" signifiant « brave et dur » (à rapprocher du néerlandais "koen", « courageux » et de l'anglais "hard", « dur »). Chez les personnes d'origine irlandaise, Connard et Connart sont des dérivés de Connacht.

Jusqu'aux débuts du le mot avait une connotation particulièrement vulgaire en particulier dans son acception physiologique ; il n'était employé dans des écrits publics que pour mieux enfoncer : 
La bassesse du vocable est toutefois déplorée par Jules Michelet qui indique : 
En 1928, Louis Aragon dut faire publier clandestinement "Le Con d'Irène", un roman érotique, pour s'éviter les foudres de la censure. Ce n'est qu'en 1968 que Régine Deforges le republie sous le titre édulcoré "Irène" ; le livre est tout de même saisi pour son contenu érotique.

Dans son sens figuré, le mot se voit de plus en plus employé après la Seconde Guerre mondiale et apparaît dans des œuvres de nombreux écrivains comme Louis-Ferdinand Céline, Louis Aragon, Raymond Queneau et même Jean-Paul Sartre

, selon le titre d'une chanson de Pierre Perret.

. Le personnage du "con", celui que l'on moque et dont on veut se différencier, est omniprésent dans la littérature, la chanson ou la culture populaires françaises.

On se souvient du méprisant « J'aime voir de mon balcon passer les cons » ("Le Pornographe du phonographe"), du descriptif « Quand on est con, on est con » (refrain du "Temps ne fait rien à l'affaire") ou du répétitif « Avec mon bouquet de fleur/mon revolver/etc. j'avais l'air d'un con, ma mère » ("Marinette") de Georges Brassens.
Jacques Brel, dans "La chanson de Jacky" (1966), aspirait à « Être une heure, rien qu'une heure durant / Beau, beau, beau et con à la fois ».

Le film "Le Dîner de cons" de Francis Veber, avec Thierry Lhermitte et Jacques Villeret, dépeint le personnage du con, dans le sens idiot, celui que l'on invite pour s'en gausser.

L'expression "roi des cons" amplifie le sens du mot, elle est mise en musique par Georges Brassens dans "Le Roi" ; Renaud au contraire prétend qu'en cas d'abdication du roi des cons, « Il y aurait cinquante millions de prétendants » (c'est-à-dire tous les Français) dans "Hexagone".

Une tradition estudiantine légendaire de l'École normale supérieure est de désigner le cuisinier ou l'intendant responsable d'une nourriture particulièrement exécrable de « Quel khon ». Il s'agit d'une cérémonie organisée au réfectoire par le cacique général ( au concours d'entrée de lettres classiques) :
Le con est aussi le ressortissant d'un pays étranger que l'on raille dans les blagues à caractère raciste ou xénophobe : en France, les blagues belges visent les voisins wallons, en Amérique latine, les Galiciens (les Espagnols par synecdoque) sont vus comme des cons ().

Certains ont utilisé à des fins poétiques la polysémie du mot, comme Georges Brassens dans "Le Blason". Il déclare déplorer la bassesse avec laquelle ses contemporains désignent « cet incomparable instrument de bonheur » :
<poem>
Mais le pire de tous est un petit vocable
De trois lettres pas plus, familier, coutumier
Honte à celui-là qui, par dépit, par gageure,
Dota du même terme en son fiel venimeux
Ce grand ami de l'homme et la cinglante injure
Celui-là, c'est probable, en était un fameux.
La male peste soit de cette homonymie !
C'est injuste, Madame, et c'est désobligeant
Que ce morceau de roi de votre anatomie
Porte le même nom qu'une foule de gens.
</poem>

Pierre Perret n'est pas en reste avec "Celui d'Alice" (1974) :
<poem>
Si je me réfère
À mon dictionnaire
Il est temps de faire
La définition
De ce mot espiègle
Qui échappe à la règle
Plus noble qu'un aigle
Dans sa condition
Ce mot vous le dites
Censeurs hypocrites
Établissez vite
Son vrai sens profond
Car si on l'ausculte
Au lieu d'une insulte
On peut faire un culte
Du joli mot con
</poem>

La syllabe « con » étant à la fois un mot et le début de nombreux autres, les paroliers en jouent souvent. La chanson paillarde "Là-haut sur la montagne" annonce par exemple à un couplet que le curé va voir « un con », le couplet suivant complétant « un condamné à mort ».

La chanson "Poupine et Thierry" des Wriggles contient quant à elle huit mots coupés à la syllabe « con » désignant le chasseur, en particulier
Le journal satirique "Le Canard enchaîné" atténue la violence du mot en simulant une coquille typographique. Il intitule « Le mur du çon » (allusion au mur du son) une rubrique signalant les citations les plus ridicules proférées par les personnalités publiques. .

Dans la chanson française, le mot s'est largement banalisé. Serge Gainsbourg l'utilise fréquemment, jusque dans le titre "Requiem pour un con". Le chanteur beaucoup moins provocateur Étienne Daho l'utilise dans "Encore cette chanson".

La chanson la plus célèbre pour le soin avec lequel elle décrit les cons est "Le temps ne fait rien à l'affaire" de Georges Brassens. Elle est reprise dans le film "Le Dîner de cons".

Le rappeur Youssoupha a utilisé l'insulte dans une de ses chansons en désignant nommément Éric Zemmour. La justice française a estimé que cet usage était acceptable dans le cadre de la liberté d'expression, en particulier parce que Zemmour a lui-même un ton très provocateur, et que les expressions violentes sont dans la traditions du rap.

Deux films restent particulièrement célèbres pour leur usage de l'insulte : "Les Tontons flingueurs" et "Le Dîner de cons". De par la popularité de ces films, les citations les plus marquantes sont devenues des moyens de traiter une personne de con sans même utiliser le mot. Dans "Les Tontons flingueurs" « Les cons ça ose tout, c'est même à ça qu'on les reconnaît » ; dans "Le Dîner de cons", c'est la phrase « Il a une belle tête de vainqueur ».

L'exemple le plus célèbre d'utilisation du mot par un homme politique, même s'il peut s'agir d'une citation apocryphe, est celui d'Édouard Daladier après les accords de Munich, qui voyant la foule l'applaudir, dit entre ses dents « Les cons » ou « Quels cons !».
« Mort aux cons ! » est un slogan du jargon militaire français utilisé pour stigmatiser l'ennemi. En 1944, le capitaine Raymond Dronne des Forces françaises libres ( compagnie de combat du régiment de marche du Tchad puis division blindée) baptise sa jeep « Mort aux cons! », expression à laquelle Charles de Gaulle aurait répondu « Vaste programme, Messieurs. Vaste programme. »

Il existe de nombreuses variantes de l'anecdote. Toutes s'accordent sur la réplique de de Gaulle, mais les circonstances varient très largement.

Aujourd'hui, le slogan est fréquemment utilisé par les milieux d'extrême gauche ou alternatifs pour désigner l'ennemi à abattre. Par exemple, un collectif de graffiteurs se fait appeler MAC, acronyme de « Mort aux cons ». En 1980, le chanteur Renaud chante la vie d'un personnage anarchiste dans ces termes : « N'empêche que "Mort aux cons" dans la cage d'escalier, c'est moi qui l'ai marqué, c'est vous dire si j'ai raison ! » (la chanson "Dans mon HLM" sur l'album "Marche à l'ombre"). À cette époque, le « con » du gauchiste était celui qui rentrait dans le cadre du système : « Élections, piège à cons ». L'ennemi peut être de l'autre côté de l'échiquier : le directeur de publication du site internet des Jeunesses identitaires est passé en jugement pour diffamation dont certains termes étaient « Vieux con de gauche, ça se soigne, docteur ? ».

Un autre anecdote célèbre, peu vérifiable, lie le mot au général de Gaulle : selon cette légende, de Gaulle a salué le général Jacques Massu en lui lançant « Alors, Massu, toujours con ? », et Massu a répondu « Oui, mon général, et toujours gaulliste ».

En 1979, à la suite d'un article de Michel Droit dans "Le Figaro" dénonçant la version reggae de La Marseillaise composée par Serge Gainsbourg ("Aux armes et cætera"), celui-ci fit circuler un bon mot qui passa à la postérité : « On n'a pas le con d'être aussi Droit ».

Aujourd'hui le mot peut être employé par un homme politique sans que cela fasse scandale — pourvu qu'il ne soit pas utilisé de manière insultante : l'ancien Premier ministre français, Dominique de Villepin déclara en mars 2006 « Ils vont s'apercevoir que je suis assez con pour aller jusqu'au bout. » au sujet de la crise du contrat première embauche. La vulgarité ne choqua pas vraiment, simplement le rabaissement volontaire qu'il s'infligeait valut à la phrase de figurer dans différents recueils de « perles » d'hommes politiques.

Par contre, en février 2008, le président Nicolas Sarkozy a choqué, lors du Salon de l'agriculture, en employant ce même mot en public. Alors qu'il tendait la main à un visiteur du Salon, celui-ci refusa en disant : « Ah non, touche-moi pas, tu me salis », et le président répliqua vivement : « Casse-toi alors pauvre con ». La scène qui a été filmée à son insu fit l'objet d'un phénomène internet.
En 2013, quand les médias découvrent le « Mur des cons » du syndicat de la magistrature, le ministre Alain Vidalies note que ce cas révèle une banalisation de l'usage du mot « con ».

Dans le Sud de la France, en particulier à Toulouse où il se prononce , "con" ou son dérivé "bouducon" est utilisé de manière impersonnelle comme interjection. Il est utilisé fréquemment dans le registre familier en début ou fin de phrase, à l'instar de "putain" ou "putain con". On résume souvent : « Putain, con, c'est la ponctuation. ». Il existe d'ailleurs cette drôlerie racontée à Toulouse: « Qu'est-ce qui commence par un F et termine avec un N et tombe à l'automne? » - « Des feuilles, con! »

En Provence, l'interjonction « Oh ! Con ! » est utilisée de manière non injurieuse entre amis ou collègues. Certaines expressions composées avec "con", comme les plus connues "Le con de ta/sa mère !", "Le con de ta/sa race !" et "Le con de Manon" (ou "Le con de Madon", pour exprimer de l'agacement, de l'irritation), peuvent s'y employer de manière exclamative, en l'absence de personne à injurier. (Bien sûr certaines d'entre elles conservent un emploi injurieux, comme les deux premières précédemment citées ). On remarquera le parallélisme avec les expressions du genre "enculé de ta race" ou "de ta mère" dans lequel la "race" ou la "mère" sont des compléments sans signification réelle, uniquement destinés à renforcer l'expression.

Dans son sens figuré, le vocable a perdu une grande partie de sa force, dans le registre familier du moins. Aussi un grand nombre d'expressions le renforcent-elles par le biais d'une comparaison avec un objet, un animal ou une situation symbolisant la bêtise : "con comme un balai", "con comme la lune", "con comme ses pieds", "con comme un comptoir sans verre", "con comme un bol", "con comme un jeune chien", "con comme un piston", "con comme une valise sans poignée" (expression chère à Jacques Chirac). À cette fin, il est souvent affublé d'un adjectif : "gros con", "sale con", "petit con", "pauvre con" ou "vieux con".

L'art de la contrepèterie est un des rares domaines à employer encore le mot dans son sens premier, ou ses dérivés. Les plus connus sont les contrepets suivants :






</doc>
<doc id="16831" url="https://fr.wikipedia.org/wiki?curid=16831" title="Bagh-nakha">
Bagh-nakha

Le Bagh-nakh (ou "wagnak"), soit « "griffe de tigre" », est une arme de corps à corps indienne utilisée par les Rajputs, les Moghols et les Marathes. Elle est constituée de griffes d'acier fixées à la main, parfois au moyen d'un gant.

C'est l'arme utilisée par le Marathe Shivaji Bhonsle, fondateur du Royaume Marâthe, le pour abattre Afzal Khan, le général que le sultan de Bijapur Ali Adil Shah II avait envoyé pour le soumettre.


</doc>
<doc id="16832" url="https://fr.wikipedia.org/wiki?curid=16832" title="Argot">
Argot

Un argot est un . C'est un sociolecte qu'il faut distinguer du jargon, qui est propre aux représentants d'une profession ou d'une activité commune se caractérisant par un lexique spécialisé.

Selon certains, la fonction première de tout argot serait de chiffrer la communication, afin qu'un non-initié ne la comprenne pas. Des spécialistes du domaine comme Albert Dauzat ou Gaston Esnault se sont prononcés contre cette thèse. Pour G. Esnault, (définition du "Dictionnaire historique des argots français" de 1965). Autrement dit, s'il arrive qu'un locuteur emploie des mots d'argot pour éviter d'être compris par les non-initiés, cela ne signifie pas pour autant que le recours à des mots argotiques soit essentiellement motivé par une volonté de cryptage.

L'histoire des dictionnaires de jargon et d'argot depuis les livrets populaires facétieux de Pechon de Ruby et d'Ollivier Chereau indique au contraire qu'il faut plutôt voir dans l'argot un désir d'expressivité, qu'il soit commun à un groupe social particulier ou qu'il déborde largement cette notion de groupe (quand le mot "argot" est synonyme de "langue verte") : en quoi les expressions "abbaye de mont(e)-à-regret" « potence » et "huistres de Varanes" (= varennes ou garennes) « fèves », recensées par Chereau (édition lyonnaise de 1630 la plus ancienne conservée), seraient-elles cryptiques ? et en quoi seraient-elles propres à un groupe particulier déterminé ?

Pour les argots de groupes, il faut intégrer dans l'expressivité de ces mots la marque de rattachement des énonciateurs à la vie et aux activités des groupes.

En anglais et en américain, l'argot est appelé "slang" ("cant" plus anciennement quand il s'agissait de malfaiteurs). Selon le "Chambers Dictionary" le terme désigne . Mais cette définition ainsi traduite ne fait pas de distinction claire entre ce qui relève de l'argot et ce qui relève du jargon au sens moderne du terme en linguistique française (termes technolectaux jugés difficiles à comprendre pour les non-spécialistes) ; quant à la notion de « sociolecte des voleurs et de la pègre », est-elle admise par les sociologues et sociolinguistes ?

L'utilisation de l'argot est une façon de contourner les tabous instaurés par la société. Le langage courant témoigne d'une certaine retenue à évoquer certaines réalités explicitement. L'argot, mais aussi le langage familier, permet alors de désigner ces réalités par un langage détourné, dénué des connotations immédiates liées aux mots du registre habituel. Cela explique que le lexique argotique soit particulièrement riche dans certains domaines comme la sexualité, mais aussi la violence, les crimes et la drogue. Cette fonction de contournement des tabous est utilisée par l'argot commun dans le premier cas, par la pègre dans le second.

Il n'existe pas "un", mais "des" argots (ou des "parlures argotiques", pour reprendre l'expression de Denise François-Geiger et Jean-Pierre Goudaillier). Différents groupes sociaux ont développé, à des époques différentes, leur propre parler. L'importance des fonctions cryptique et identitaire varie entre les argots. On remarque que la tendance actuelle privilégie l'identitaire sur le cryptique : le français contemporain des cités en particulier a moins besoin de masquer son message que de marquer l'appartenance à son groupe et, par opposition, son rejet de la société préétablie. Mais l'usage de la langue corse, qui n'est pas un argot, reste pratiqué dans le milieu organisé insulaire à Paris, Marseille etc de façon à ne pas être compris des non corses tout en resserrant le lien d'intimité entre les locuteurs sans que la société extérieure soit particulièrement rejetée. Les fonctions de l'argot peuvent donc être polymorphes.

Pour que les tiers soient maintenus dans l'incompréhension de la communication, l'argot doit constamment renouveler ses procédés d'expression, spécifiquement son lexique. L'existence de dictionnaires d'argot annule bien sûr toute l'efficacité des mots définis. De nombreux termes originaires de l'argot sont d'ailleurs passés dans le registre familier, voire dans le langage courant (par exemple, "cambrioler" et ses dérivés sont issus de l'argot "cambriole" « chambre »). Ainsi, certains mots ou expressions possèdent une foule de traductions argotiques, la palme revenant à des termes comme « argent », « femme » ou « faire l'amour » qui possèdent plus d'un millier d'équivalents en argot.

À l'origine (peu avant 1630), le mot "argot" a désigné le monde des mendiants, puis vers 1700, le « jargon » des gueux, puis le parler des voleurs. Des argots se sont également développés dans d'autres groupes sociaux, et chaque quartier possède son propre « argot ».

En France, le concept apparaît au et est identifié en provençal sous le nom de « jargon ». L'un des premiers textes connus concernant un jargon de bandits est le dossier judiciaire du procès des Coquillards à Dijon en 1455. Le premier livre imprimé (Levet, 1489) des poèmes de François Villon utilise au l'expression « jargon et jobelin », puis au siècle suivant apparaissent « baragouin », « narquois » ou « blesquien », notamment. Le premier texte français entièrement centré sur la vie et le jargon des petits merciers et des gueux est publié à Lyon en 1596 chez Jean Jullieron. Il s'agit de "La vie généreuse des Mercelots, Gueux et Boesmiens" signé par Pechon de Ruby. Ce texte connaîtra cinq rééditions jusqu'en 1627 et sera à l'origine du développement de la littérature argotique. Il contient finalement un lexique de 150 mots de jargon blesquien qui évoluera d'une édition à l'autre. Ce n'est que vers 1630 que le mot "argot" apparaît, mais seulement avec le sens de « monde des mendiants » dans l'ouvrage publié par Ollivier Chereau, "Le Jargon ou Langage de l'Argot reformé". 
En plus d'une abondante production lexicographique, la littérature a contribué à diffuser « la langue verte ». On peut citer : "Essai sur l'argot" (texte philosophique, linguistique et littéraire sur l’argot, les filles et les voleurs), 1834, d’Honoré de Balzac, les "Mémoires" de l'ex-bagnard Vidocq, "Les Mystères de Paris" d'Eugène Sue, Victor Hugo, (), "Les Mohicans de Paris" de Dumas, et sous la Troisième République Émile Zola, Jean Richepin et sa "Chanson des Gueux", Francis Carco, Céline, Édouard Bourdet et Jacques Perret. À travers ces ouvrages, c'est plutôt l'argot parisien qui est mis en lumière.

L'argot, qui renaît et se renouvelle sans cesse, a continué à évoluer dans les romans comme ceux d'Albert Simonin ou de Frédéric Dard dans la série San-Antonio, dans les dialogues de films avec Michel Audiard ou Alphonse Boudard, dans les chansons avec Pierre Perret, de Renaud ou dans les sketches comiques de Coluche. Souvent associé à l'argot, Michel Audiard n'en appréciait pas la notion, préférant parler de et trouvant le vocabulaire généralement qualifié d'argot .

Le verlan est un procédé argotique ancien qui s'est développé dans l'argot parisien des années cinquante et a pris un nouvel essor et de nouvelles formes dans les années 1970, notamment avec le langage contemporain des cités.

L'argot commun, parfois appelé "jargot", est un parler familier dérivé de l'argot mais qui en a perdu les fonctions cryptiques et identitaires. Il n'est plus spécifique à un groupe, et est essentiellement utilisé dans une visée ludique : les locuteurs « jouent » à reproduire un parler largement connoté. Il reprend en général du vocabulaire argotique « dépassé », abandonné par le groupe social qui en est l'origine dès qu'il a été compris par des tiers. Le « français branché » des années 1980 est un exemple typique d'argot commun.

Pour élaborer un parler qui lui est propre, un groupe social a recours à différents moyens. Le plus important est lexical : on associe d'ailleurs généralement l'argot uniquement à un vocabulaire particulier. Cependant, il peut y avoir également une modification de la syntaxe, même si elle est d'une bien moindre importance.

En fait, l'argot est toujours connu pour son vocabulaire, mais cela ne signifie pas qu'il suit les règles syntaxiques, grammaticales, phonétiques, pragmatiques... de la langue standard. La formation des phrases, la prononciation, l'intonation, la gestuelle... sont très différentes de la norme officielle et participent donc à la distinction du groupe. Néanmoins, les procédés autres que lexicaux utilisés par l'argot ne lui sont en général pas propres : il s'agit généralement de caractères du langage familier ou populaire.

Quant aux procédés d'élaboration lexicale, ils sont de deux types : soit sémantiques (modification et jeu sur les sens des mots), soit formels (création ou modification de mots). Lorsque l'élaboration lexicale est formelle, on assiste souvent à une déconstruction du langage courant : l'argot déforme, mélange, déstructure, découpe... les mots et enfreint les règles. Cette déconstruction laisse transparaître la volonté du groupe social de se démarquer en rejetant la société établie.

Les procédés décrits ici concernent l'argot français actuel, et plus particulièrement le français contemporain des cités.


"Cette liste est inspirée de la classification de Marc Sourdot (opus cité ci-dessous)."

L'ensemble de ces procédés relèvent de la relexification.

L'argot est souvent employé en littérature pour créer un climat et un contexte : pour camper des personnages de truands, ou simplement des personnes appartenant aux classes populaires il serait fort peu crédible de mettre dans leur bouche le langage châtié d'un diplomate ou d'un académicien.

C'est un procédé classique dans la littérature policière ou para-policière (Albert Simonin, qui a d’ailleurs adjoint une vingtaine de pages de lexique argot/français à son "Touchez pas au Grisbi," Frédéric Dard (alias San - Antonio), Auguste Le Breton, Alphonse Boudard, mais pas seulement, d'autres auteurs moins spécialisés, comme Louis Ferdinand Céline, Jean Hougron, Vincent Ravalec, entre autres peuvent être cités.

Il en va de même dans le domaine de la chanson, un auteur et interprète comme Pierre Perret en a fait sa marque distinctive, toutefois certaines chansons ont pris l'argot comme un élément plus central; on peut ainsi citer deux chansons centrées sur les deux domaines phares de l'expression argotique, l'acte sexuel et l'argent : Colette Renard, dans sa chanson fétiche "Les nuits d'une demoiselle", énumère malicieusement les expressions les plus inventives de l'argot appliqué à l'amour physique (Cf Supra, paragraphe relation sexuelle), tandis que les Frères Jacques ont basé leur chanson "Le fric" sur les mots d'argot désignant l'argent accolé un adjectif approprié pour la rime : 

Le fric/magique, pognon/mignon, le pèze/balèze, l'artiche/fortiche...etc, avec un tempo toujours accéléré évoquant la frénésie de la spéculation boursière.

L'argot (tel que parlé dans les années 40 et 50) est aussi le thème d'une œuvre parodique coécrite par l'écrivain (ancien résistant et ancien voyou) Alphonse Boudard associé à Luc Étienne, connu des lecteurs du Canard enchaîné pour ses contrepèteries rassemblées dans la rubrique "L'Album de la comtesse", c'est "La méthode à Mimile" (ou l'argot sans peine), une parodie de la Méthode Assimil dont le ressort comique est le contraste entre les expressions argotiques (aussi triviales que possible), classées par thème et mises en contexte et leur traduction française qui utilise un registre de langue particulièrement châtié.





</doc>
<doc id="16837" url="https://fr.wikipedia.org/wiki?curid=16837" title="Parsec">
Parsec

Le parsec (prononcer ; symbole pc) est une unité de longueur utilisée en astronomie valant, par définition, exactement unités astronomiques. Son nom est la contraction de « parallaxe-seconde », expression se rapportant à la définition historique, désormais obsolète, du parsec ("cf." figure).

« Parsec » est emprunté à l'anglais ', mot-valise proposé par l'astronome britannique à partir de ' (en français, « parallaxe ») et de "" (« seconde »).

« Parsec » est prononcé ("parsèk") en français.

D'après Frédéric Arenou, le parsec a été utilisé pour la première fois en par l'astronome allemand , sous le nom de ' (littéralement « distance stellaire » en allemand). En 1913, l'astronome britannique propose de nommer l'unité ' et Turner '. Le terme de ' fut rejeté par Frank Watson Dyson, par crainte d'une confusion avec le terme "". 

En , la commission « Notations » de l'Union astronomique internationale suggère l'utilisation de l'année-lumière, « surtout dans les articles populaires », et du parsec, « ou de préférence une unité dix fois plus grande avec un nom distinct ».

Historiquement, le parsec est défini comme la distance à laquelle une unité astronomique (ua) sous-tend un angle d’une seconde d'arc. Autrement dit, la distance à partir de laquelle on verrait la distance terre-soleil, sous un angle d'une seconde d'arc. Cette définition est néanmoins légèrement ambiguë et n'avait par ailleurs jamais été officialisée, ce qui conduisait à des variations, certes faibles mais inutilement présentes, de la valeur (en unités du Système international) adoptée pour cette unité. La définition du parsec a donc été précisée et à la même occasion officialisée en note de la résolution B2 adoptée lors de l'assemblée générale de l'Union astronomique internationale de 2015.

Selon la de la adoptée lors de l'assemblée générale de l'Union astronomique internationale de 2015, .

La définition actuelle (2015) donne une valeur exacte au parsec en termes d'unités astronomiques (l'unité astronomique a été définie exactement en 2012 en unités du Système international) mais la définition officielle ne donne aucune interprétation physique de celle-ci et ne la relie pas à la définition historique. Le lien entre la définition historique et la définition actuelle peut s'exprimer simplement : le parsec est la longueur du rayon d'un cercle dont l'arc soutenu par un angle au centre d'une seconde d'arc mesure exactement une unité astronomique.

Un demi-cercle (arc de cercle d'un support angulaire de ) d'un parsec de rayon a une longueur de , soit exactement selon la définition précédente. étant strictement égaux à , l'arc de cercle soutenu par un angle d'une seconde d'arc mesure exactement astronomique.

L'unité astronomique étant exactement définie dans le Système international comme valant (résolution de l'UAI de 2012), un parsec vaut exactement , soit environ . L'année-lumière étant également définie exactement dans le Système international ( = × × = ), un parsec vaut exactement , ce qui se simplifie en , soit environ .

En résumé :

Le symbole du parsec est pc. Ses multiples et sous-multiples utilisent les préfixes du Système international d'unités : kpc pour le kiloparsec (), Mpc pour le mégaparsec ( de parsecs), Gpc pour le gigaparsec ( de parsecs).

Cette unité résulte de l’application d’une méthode trigonométrique dite « méthode de la parallaxe », servant à déterminer la distance séparant un observateur d’un objet éloigné quelconque, à la mesure de la distance des objets célestes. Pour des raisons pratiques, les astronomes expriment souvent les distances des objets astronomiques en parsecs plutôt qu’en années-lumière. Cette unité permet une conversion directe des valeurs observées en distance : si la parallaxe annuelle d’une étoile est mesurée en secondes d’arc, alors la distance entre cette étoile et le Soleil, exprimée en parsecs, est égale à l’inverse de cette valeur. La magnitude absolue et le module de distance sont deux unités dérivées du parsec, et l'expression des distances en parsecs facilite la manipulation de ces données.

Les premières mesures de distance interstellaire (l’étoile 61 Cygni par Friedrich Wilhelm Bessel en 1838) furent effectuées en utilisant la largeur de l’orbite terrestre comme référence. Le parsec dériva de cette méthode. La détermination des distances des corps célestes est l’objet principal de l’astrométrie.

L’étoile la plus proche du Soleil, α Cen C (Proxima Centauri), se trouve à (). Les distances des autres objets célestes n’appartenant pas au système solaire sont bien plus grandes et se mesurent couramment en kiloparsecs (symbole kpc) ou mégaparsecs (symbole Mpc).

Les parallaxes ont des valeurs faibles : 0,76″ pour Proxima Centauri ; aussi, la méthode parallactique ne permet guère de déterminer des distances stellaires supérieures à environ, ce qui correspond à des mesures de parallaxe inférieures à dix millisecondes d’arc.

Entre 1989 et 1993, le satellite Hipparcos, lancé par l’Agence spatiale européenne, a mesuré la parallaxe d’environ cent mille étoiles avec une précision supérieure à la milliseconde d’arc, ce qui a permis de déterminer la distance d’étoiles éloignées de nous de plus d’un kiloparsec.

Sur la figure 1, (d’échelle très réduite et ne respectant pas les valeurs angulaires), S est le Soleil, T la Terre et P un objet situé à un parsec du Soleil : par définition, l’angle formula_1 est égal à une seconde d’arc (1″) et la distance formula_2 vaut une unité astronomique (). Grâce aux règles de trigonométrie, il est possible de calculer formula_3 :

Comme

on a

et

donc

Le choix d'une définition arbitraire mais désormais fixe de l'unité astronomique explique la précision des valeurs précédentes.

Sur le schéma ci-dessus, l'angle censé être d'une seconde a une valeur bien supérieure, et par conséquent l'hypoténuse est clairement plus longue que le côté adjacent. En réalité, pour un angle aussi petit, la différence de longueur entre les deux est très faible en valeur relative, et finalement l'hypoténuse vaut à peine plus d'un parsec (autrement dit, un parsec est aussi bien la distance du Soleil à l'étoile lointaine que de la Terre à l'étoile lointaine).

Pour les très faibles valeurs d'angles (exprimés en radians), on peut faire l'approximation (développement limité au premier ordre) formula_10 ; de même formula_11, d'où l'affirmation suivant laquelle le côté adjacent et l'hypoténuse sont quasiment égaux. Dans le cas du parsec, formula_12 valant formula_13, l'erreur relative commise en confondant les deux côtés est inférieure à formula_14, donc on commet (en utilisant ces formules) une erreur de l'ordre de la distance Paris-Brest, ce qui peut sembler important, mais est évidemment négligeable aux échelles astronomiques considérées.




</doc>
<doc id="16838" url="https://fr.wikipedia.org/wiki?curid=16838" title="Campeche (ville)">
Campeche (ville)

Campeche, parfois francisé en « Campêche » (abrégé de San Francisco de Campeche) est une ville du Mexique et la capitale de l'État du même nom. Sa population s'élevait à habitants en 2010.

Cette ville vit de ses richesses naturelles, à savoir le pétrole et la pêche à la crevette.

La ville historique, fortifiée au pour la protéger des attaques des pirates, est un bien inscrit sur la Liste du patrimoine mondial de l'UNESCO.

Avant l'arrivée des conquistadors qui fondèrent la ville, la région est occupée par les Mayas depuis le . Cette zone en particulier est sous la domination de la cité d'Edzná, située à de Campeche et édifiée par le peuple Itzá (également fondateur de la cité de Chichén Itzá). Edzná a connu son apogée aux environs de l'an 1000.

En mars 1517, l'expédition espagnole commandée par Francisco Hernández de Córdoba, partie de Cuba, atteint la côte de l'actuelle Campeche. Ayant besoin d'eau, les conquistadors débarquent dans un village maya, "Can Pech", où ils sont accueillis pacifiquement par les villageois, qui leur demandent néanmoins de quitter les lieux après s'être ravitaillés en eau.

En 1526, Charles Quint demande à Francisco de Montejo dit "El Adelantado", qui avait participé à la prise de Mexico-Tenochtitlan avec Hernán Cortés, de conquérir la péninsule du Yucatán. Dans un premier temps, Montejo et ses hommes s'aventurent sur la côte orientale du Yucatan, mais ils sont repoussés par les Mayas. En 1531, Montejo réussit à pénétrer la péninsule par la côte occidentale, et fonde la ville de Salamanca de Campeche. Son fils Francisco de Montejo dit "El Mozo" (soit « le jeune homme »), prend la ville de Chichén Itzá pour y installer la première capitale du Yucatan, mais cet emplacement sera abandonné quelques années plus tard.

En 1540, Francisco de Montejo "El Mozo" crée une nouvelle garnison sur la côte ouest du Yucatan, qu'il appelle San Francisco de Campeche. Ce port sera d'une grande importance pour la suite de la conquête du Yucatan.

Après avoir soumis les Mayas militairement, les Espagnols s'efforcent de les convertir à la religion catholique. Les franciscains, qui étaient arrivés dès 1535 pour repartir peu après à la suite de conflits avec les autochtones et les colons, reviennent à Campeche en 1540. Ils entreprennent la construction de l'église et du couvent San Francisco sous la direction du frère Luis de Villalpando.

La situation de Campeche dans le golfe du Mexique en fait le principal lieu d'échanges de la péninsule du Yucatan, ce qui permet l'essor économique et l'accroissement de la population de la ville. Campeche exporte notamment du bois de Campêche (produit tinctorial) et du sel. Elle comporte également des chantiers navals.

Mais le monopole commercial imposé par l'Espagne à ses colonies a pour conséquence le développement de pratiques illégales visant à contourner ce monopole. Ainsi, contrebande et piraterie se développent largement dans le Golfe du Mexique, et notamment autour de Campeche.

Pour freiner ces pratiques, les autorités créent en 1616 un permis de coupe et de commercialisation du bois de Campêche. Cependant, cette mesure est inefficace puisque la piraterie continue à se développer. En 1629, le roi Philippe IV d'Espagne crée une flotte de garde-côtes pour protéger le commerce maritime, ainsi qu'une garnison pour défendre la cité, mais ni l'une ni l'autre ne sont réellement efficaces. Cependant, accaparé par les pressions militaires des autres nations européennes et par les révoltes dans ses possessions néerlandaises, Philippe IV n'est pas en mesure de renforcer la protection de la cité.
Ainsi, le 27 janvier 1661, une flotte de flibustiers, emmenée par Henry Morgan, attaque et pille deux navires de commerce dans la baie de Campeche, et repart sans être poursuivie. D'autres pirates célèbres ont attaqué Campeche, comme John Hawkins, Francis Drake, Jean Lafitte, Laurent de Graff, Cornelius Jol, Michel de Grandmont, et François l'Olonnais.

Ce n'est qu'en 1686 que commence la construction de fortifications autour de la ville, qui durera jusqu'en 1704. Ces réalisations, supervisées par l'ingénieur français Louis Bouchard, comprennent un mur d'environ de long reliant des bastions défensifs. Ce mur a une épaisseur de à la base et une hauteur moyenne de . Les bastions sont au nombre de huit :

Par ailleurs, deux forts à l'extérieur de la ville viennent compléter le dispositif : San José le Haut (aujourd'hui Musée des armes et des navires), et San Miguel (Musée d'archéologie).

Campeche possède un aéroport ("Ing. Alberto Acuña Ongay International Airport", dont le code AITA est CPE).

En Ligue mexicaine de baseball, les Piratas de Campeche sont basés à Campeche où se trouve leur stade, le Estadio Nelson Barrera Romellón, enceinte de .




</doc>
<doc id="16839" url="https://fr.wikipedia.org/wiki?curid=16839" title="Croissance économique">
Croissance économique

La croissance économique désigne la variation positive de la production de biens et de services dans une économie sur une période donnée, généralement une période longue. En pratique, l'indicateur le plus utilisé pour la mesurer est le produit intérieur brut (PIB). Il est mesuré « en volume » ou « à prix constants » pour corriger les effets de l'inflation. Le taux de croissance, lui, est le taux de variation du PIB. On utilise souvent la croissance du PIB par habitant comme indication de l'amélioration de la richesse individuelle, assimilée au "niveau de vie".

La croissance est un processus fondamental des économies contemporaines, reposant sur le développement des facteurs de production, lié notamment à la révolution industrielle, à l'accès à de nouvelles ressources minérales (mines profondes) et énergétiques (charbon, pétrole, gaz, énergie nucléaire...) ainsi qu'au progrès technique. Elle transforme la vie des populations dans la mesure où elle crée davantage de biens et de services. À long terme, la croissance a un impact important sur la démographie et le niveau de vie (à distinguer de la qualité de vie) des sociétés qui en sont le cadre. De même, l'enrichissement qui résulte de la croissance économique peut permettre de faire reculer la pauvreté de cette même société.

Certaines conséquences de la croissance économique comme la pollution et les atteintes à l'environnement, l'accentuation des inégalités sociales ou l'épuisement des ressources (voir pic pétrolier notamment) sont souvent considérées comme des effets pervers qui obligent à distinguer croissance et progrès.

Les économistes utilisent le terme de croissance conventionnellement pour décrire une augmentation de la production sur le long terme. Selon la définition de François Perroux, la croissance économique correspond à . La définition de Simon Kuznets va au-delà et affirme qu'il y a croissance lorsque la croissance du PIB est supérieure à la croissance de la population.

À court terme, les économistes utilisent plutôt le terme d'« expansion », qui s'oppose à « récession », et qui indique une phase de croissance dans un cycle économique. La croissance potentielle estime l'écart entre la croissance mesurée et celle qui serait obtenue avec une pleine utilisation de tous les facteurs de production ; cet écart est minimal au plus fort d'une expansion.

Au sens strict, la croissance décrit un processus d'accroissement de la seule production économique. Elle ne renvoie donc pas directement à l'ensemble des mutations économiques et sociales propres à une économie en développement. Ces transformations au sens large sont, conventionnellement, désignées par le terme de développement économique. Selon François Perroux, Le terme de « croissance » s'applique alors plus particulièrement aux économies déjà développées.

La du gouvernement britannique souligne qu'il est important de distinguer trois notions qui :

Le croissantisme économique est considéré comme étant l'idéologie de la croissance par opposition à la philosophie décroissantiste.

La croissance économique est généralement mesurée par l'utilisation d'indicateurs économiques dont le plus courant est le produit intérieur brut (PIB). Il offre une certaine mesure quantitative du "volume" de la production. Afin d'effectuer des comparaisons internationales, on utilise également la parité de pouvoir d'achat, qui permet d'exprimer le pouvoir d'achat dans une monnaie de référence. Pour comparer la situation d'un pays à des époques différentes on peut également raisonner à monnaie constante.

L'indicateur du PIB reste cependant imparfait comme mesure de la croissance économique. Il est pour cela l'objet de plusieurs critiques :

Cette contradiction apparente provient probablement du fait que le PIB ne mesure pas réellement le développement, le progrès en lui-même ; il ne mesure pas non plus l'activité économique, pourvoyeuse d'emploi, car l'activité peut fort bien croître sans augmentation de valeur ajoutée, si l'on remplace du capital ou des matières premières par du travail. La croissance ne mesure en fait que l'augmentation de la consommation de facteurs de production : travail, capital et ressources naturelles (matières premières, potentiel productif des terres agricoles, etc). La société peut progresser sans croissance, en modifiant la répartition des facteurs.

Dans son acception classique, le développement économique ne se résume pas à la seule croissance économique et des indicateurs ont été proposés pour mesurer plus finement celui-ci, comme l'indice de développement humain.

Dans un certain nombre de cas, les données de la comptabilité nationale ne sont pas disponibles ou sont de mauvaise qualité. C'est notamment un problème lorsqu'on s'intéresse à des périodes anciennes, à des pays en voie de développement avec une mauvaise comptabilité nationale ou encore lorsqu'on s'intéresse au développement économique à un niveau infra-national, par exemple au niveau d'une ville ou d'une région. Dans ce cas, plusieurs indicateurs ont été proposés.


Grâce au développement des statistiques nationales, les économistes, les historiens et les démographes ont constaté qu'avant la Révolution industrielle, la croissance économique est essentiellement liée à celle de la population : on produit plus parce qu’il y a plus d'individus pour produire, mais le niveau de vie reste le même. À partir du , la croissance économique se déconnecte de celle de la population et l’augmentation du niveau de vie devient exponentielle, mais très irrégulière. Après les très forte croissance mondiale des années 1830 et croissance mondiale des années 1850, la Grande Dépression (1873-1896) donne un sérieux coup de frein. De même, la grande dépression des années 1930 fait suite à la croissance économique de la Belle Époque et à la puissante expansion des années 1920. Plus généralement, les périodes de reconstruction suivant une guerre sont favorables, comme lors de la très forte croissance des années 1950, socle des Trente Glorieuses.

Les historiens s’accordent sur le fait que le niveau de vie sur l’ensemble du globe a peu évolué de l’Antiquité jusqu’au (entre l'an 1 et l'an 1000 l'économie mondiale aurait même décliné), mis à part une embellie en Europe occidentale entre les s, annulée par les épidémies et les famines des s. Ils s'accordent aussi à constater qu'il y a de grandes disparités selon les peuples et selon les époques. Sachant qu'on a affaire à des sociétés où presque toute la population est rurale, il est de toutes façon presque impossible d'obtenir la statistique de leur production, puisque celle-ci est presque complètement locale, voire familiale (bâtiment, mobilier, confection, alimentation, services…), et très marginalement commerciale, de telle sorte qu'il est impossible de reconstituer un standard moyen de consommation et de l'évaluer en monnaie.

La croissance économique, aussi bien comme phénomène que comme donnée objectivable, est donc quelque chose de récent, lié à l'urbanisation des sociétés et à l'apparition de statistiques nationales. Jusqu'aux années 1970, c'était aussi un phénomène géographiquement limité, qui concernait surtout les pays occidentaux et le Japon.

Les Pays-Bas sont la première société à connaître un phénomène de croissance, au . Comme le note Henri Lepage en reprenant les analyses de Douglass North, 

Le phénomène s'est ensuite progressivement étendu. La phase de développement économique depuis la Révolution industrielle n'a aucun précédent historique. Après le , lorsque différentes parties du monde développent des relations commerciales, on constate des périodes de croissance économique, mais éphémères et marginales. Les écarts entre conditions de au étaient réduits, pour certains auteurs comme Paul Bairoch : l'Inde possédait même un niveau de vie supérieur à l'Europe. On estime que la croissance globale de l'économie entre 1500 et 1820 n'est que d'un trentième de ce qu'elle a été depuis (de 247 milliards de dollars internationaux en 1500 à 695 en 1820, puis en 1998). Les revenus en Europe ont été multipliés par 20 depuis 1820. L'Asie accélère aussi son rythme de croissance depuis un demi-siècle : le niveau de vie en Chine a été multiplié par six et celui du Japon par huit.

Cependant, au le développement économique entraîne des bouleversements sociaux comme l'exode rural. Le niveau de vie et le développement n'ayant commencé à être étudiés rigoureusement qu'au , il est cependant difficile, faute de données, de faire une comparaison entre le .

En 1913, le PIB/hab français était de dollars internationaux (base 1990). En 1998, il était de . Le taux de croissance moyen du PIB/hab était donc de 2,0 % sur cette période. S'il avait été de 1,0 %, le niveau de vie aurait été de en 1998, soit un peu moins que le niveau de vie réel de l'Uruguay ().

L'évolution en pourcentage du PIB en volume d'une année à l'autre. Les données sont mesurées en monnaie constante de 2005 d’après les données de l’OCDE

"Source :" Banque Mondiale

Dans "Le Capital au XXIe siècle", Thomas Piketty fait l'hypothèse que la période de forte croissance économique est terminée et qu'il y a toutes les raisons de penser que la croissance devrait revenir à un niveau plus faible dans un régime stationnaire. 

Dans "" (2016), l'économiste Robert J. Gordon défend la thèse que la forte croissance aux États-Unis et dans les pays développés entre 1870 et 1970 a été une exception et que les innovations qui ont eu lieu depuis 1970 génèrent moins de croissance que par le passé.

On peut distinguer plusieurs types de déterminants à la croissance : richesses naturelles, environnement extérieur, population, innovation (concept qui ne concerne pas seulement le progrès technique), investissement, connaissance, cohérence du développement. Les principales conclusions des travaux de Xavier Sala-i-Martin, économiste espagnol spécialiste de la croissance, confirment qu'il n'y a pas qu'un seul déterminant simple de la croissance économique.

Xavier Sala-i-Martin avance par ailleurs que le niveau initial est la variable la plus importante et la plus robuste. C'est-à-dire que, dans la plupart des cas, plus un pays est riche, moins il croît vite. Cette hypothèse est connue sous le nom de convergence conditionnelle. Il considère également que la taille du gouvernement (administration, secteur public) n'a que peu d'importance. Par contre la qualité du gouvernement a beaucoup d'importance : les gouvernements qui causent l'hyperinflation, la distorsion des taux de change, des déficits excessifs ou une bureaucratie inefficace ont de très mauvais résultats. Il ajoute également que les économies plus ouvertes tendent à croître plus vite. Enfin, l'efficience des institutions est très importante : des marchés efficients, la reconnaissance de la propriété privée et l'état de droit sont essentiels à la croissance économique. Il rejoint en cela les conclusions d'Hernando de Soto.

Sur une plus longue période, l'expérience historique, notamment celle du , suggère que l'extension des libertés économiques (liberté d'entreprendre, liberté de circulation des idées, des personnes et des biens) est une condition de la croissance. Au , il existe plusieurs cas où une population partageant les mêmes antécédents historiques, la même langue et les mêmes normes culturelles a été divisée entre deux systèmes, l'un étant une économie de marché et l'autre une économie planifiée : les deux Allemagne, les deux Corée, la République populaire de Chine et Taïwan. Dans chaque cas, les zones ayant pratiqué l'économie de marché ont obtenu une croissance nettement supérieure sur le long terme. Cependant, l'enrichissement de l'Allemagne de l'Ouest s'explique par l'aide des États-Unis, l'enrichissement de la Corée du Sud et de Taïwan par l'aide des États-Unis et du Japon et que Taïwan a attiré les Chinois les plus qualifiés. Les États-Unis et l'Europe de l'Ouest étant beaucoup plus développés que l'URSS, leurs pays alliés ont été beaucoup plus aidés. La très forte croissance de l'URSS avant les années 1960 et la très forte croissance de la Chine depuis les années 1980 sont des exemples de pays dont l'économie planifiée a augmenté la croissance. Aucun pays n'a eu une croissance telle que celle de la Chine et l'URSS sans bénéficier d'aide extérieure ou d'une exploitation massive de ressources naturelles très lucratives, telles le pétrole, par rapport au nombre d'habitants. L'effondrement de l'URSS témoigne également des meilleurs résultats des économies de marché par rapport aux économies de type collectiviste.

Sur le très long terme, Angus Maddison identifie trois processus interdépendants qui ont permis l'augmentation conjointe de la population et du revenu : la conquête ou la colonisation d'espaces fertiles et relativement peu peuplés, le commerce international et les mouvements de capitaux, l'innovation technologique et institutionnelle.

Quant à Daron Acemoglu, dans "An Introduction to Modern Economic Growth" (2008), il distingue quatre causes fondamentales de la croissance : l'environnement naturel, la culture, les institutions et la chance.

Les théories explicatives de la croissance sont relativement récentes dans l'histoire de la pensée économique. Ces théories, sans négliger le rôle de l'ensemble des facteurs de production tendent à mettre en avant parmi ceux-ci le rôle primordial du progrès technique dans la croissance. Sur le long terme, seul le progrès technique est capable de rendre plus productive une économie (et donc de lui permettre de produire plus, c'est-à-dire d'avoir de la croissance). Toutefois, ces théories expliquent encore mal d'où provient ce progrès, et en particulier en quoi il est lié au fonctionnement de l'économie.

La plupart des économistes de l'école classique, écrivant pourtant au commencement de la révolution industrielle, pensaient qu'aucune croissance ne pouvait être durable, car toute production devait, selon eux, inexorablement converger vers un état stationnaire. C'est ainsi le cas de David Ricardo pour qui l'état stationnaire était le produit des rendements décroissants des terres cultivables, ou encore pour Thomas Malthus qui le liait à son « principe de population », mais aussi pour John Stuart Mill.

Toutefois, Adam Smith, à travers son étude des effets de productivité induits par le développement de la division du travail, laissait entrevoir la possibilité d'une croissance ininterrompue. Et Jean-Baptiste Say écrivait (Traité d'économie politique, Livre I, chapitre XII)

Nikolai Kondratiev est un des premiers économistes à montrer l'existence de cycles longs de 50 ans, et Joseph Schumpeter développe la première théorie de la croissance sur une longue période. Il considère que l'innovation portée par les entrepreneurs constitue la force motrice de la croissance. Il étudie en particulier le rôle de l'entrepreneur dans "Théorie de l'évolution économique" en 1913.

Pour Schumpeter, les innovations apparaissent par « grappes », ce qui explique la cyclicité de la croissance économique. Par exemple, Schumpeter retient les transformations du textile et l'introduction de la machine à vapeur pour expliquer le développement des années 1798-1815, ou le chemin de fer et la métallurgie pour l'expansion de la période 1848-1873. De façon générale il retient trois types de cycles économiques pour expliquer les variations de la croissance :

Schumpeter introduit enfin le concept de « destruction créatrice » pour décrire le processus par lequel une économie voit se substituer à un modèle productif ancien un nouveau modèle fondé sur des innovations. Il écrit ainsi :

Après la Seconde Guerre mondiale, les économistes Harrod et Domar, influencés par Keynes, cherchent à comprendre les conditions dans lesquelles une phase d'expansion peut être durable. Ainsi, s'il ne propose pas à proprement parler une théorie de la croissance (expliquant son origine sur une longue période), le modèle de Harrod-Domar permet, néanmoins, de faire ressortir le caractère fortement instable de tout processus d'expansion. En particulier, il montre que pour qu'une croissance soit équilibrée , il faut qu'elle respecte un taux précis, fonction de l'épargne et du coefficient de capital (quantité de capital utilisée pour produire une unité) de l'économie. Or, il n'y a aucune raison que la croissance, qui dépend de décisions individuelles (en particulier des projets d'investissement des entrepreneurs), respecte ce taux. De plus, si la croissance est inférieure à ce taux, elle va avoir tendance non pas à le rejoindre, mais à s'en éloigner davantage, diminuant progressivement (en raison du multiplicateur d'investissement). La croissance est donc, selon une expression d'Harrod, toujours « sur le fil du rasoir ».

Ce modèle, construit après guerre et marqué par le pessimisme engendré par la crise de 1929, a toutefois été fortement critiqué. Il suppose, en effet, que ni le taux d'épargne, ni le coefficient de capital ne sont variables à court terme, ce qui n'est pas prouvé.

Robert Solow propose un modèle néoclassique de croissance. Ce modèle repose essentiellement sur l'hypothèse d'une productivité marginale décroissante du capital dans la fonction de production. Le modèle est dit néoclassique au sens où les facteurs de production sont utilisés de manière efficace et rémunérés à leur productivité marginale. Solow montre que cette économie tend vers un "état stationnaire". Dans ce modèle, la croissance de long terme ne peut provenir que du progrès technique (et non plus de l'accumulation du capital).

Si on pense que tous les pays convergent vers le même état stationnaire, alors le modèle de Solow prédit un phénomène de convergence : les pays pauvres devraient croître plus vite que les pays riches.

L'une des faiblesses théoriques du modèle de Solow vient du fait qu'il considère le progrès technique comme exogène. Autrement dit, il ne dit rien sur la façon dont le progrès technique apparaît.

Les théories de la croissance endogène cherchent endogénéiser le progrès technique, c'est-à-dire à construire des modèles qui expliquent son apparition. Ces modèles ont été développés à partir de la fin des années 1970 notamment par Paul Romer, Robert E. Lucas et Robert Barro. Ils se fondent sur l'hypothèse que la croissance génère par elle-même le progrès technique. Ainsi, il n'y a plus de fatalité des rendements décroissants : la croissance engendre un progrès technique qui permet que ces rendements demeurent constants. Si tel est le cas, la croissance n'a donc plus de limite. À travers le progrès technique, la croissance constitue un processus qui s'auto-entretient.

Ces modèles expliquent que la croissance engendre du progrès technique par trois grands mécanismes. Premièrement, le "learning by doing" : plus on produit, plus on apprend à produire de manière efficace. En produisant, on acquiert en particulier de l'expérience, qui accroît la productivité. Deuxièmement, la croissance favorise l'accumulation du capital humain, c'est-à-dire les compétences possédées par la main d'œuvre et dont dépend sa productivité. En effet, plus la croissance est forte, plus il est possible d'accroître le niveau d'instruction de la main-d'œuvre, en investissant notamment dans le système éducatif. D’une manière générale, la hausse du niveau d'éducation de la population – par des moyens publics ou privés – est bénéfique. Troisièmement, la croissance permet de financer des infrastructures (publiques ou privées) qui la stimulent. La création de réseaux de communication efficaces favorisent, par exemple, l'activité productive.
En particulier ce « retour de l'État » se traduit par le fait qu'il est investi d'un triple rôle : encourager les innovations en créant un cadre apte à coordonner les externalités qui découlent de toute innovation (par exemple grâce à la protection qu'offre aux innovateurs les brevets) ; susciter celles-ci en investissant dans la recherche (notamment fondamentale) et les infrastructures dont les externalités dépassent le profit que peuvent en attendre les acteurs privés ; améliorer le capital humain en investissant dans le système éducatif. D'une manière générale, c'est le rôle des politiques structurelles de l'État, en particulier les investissements dans le capital public, qui est ainsi souligné.

Ces modèles sont toutefois très frustes en ce qu'ils n'expliquent pas les mécanismes précis qui font que la croissance économique stimule le progrès technique. En particulier, chacun des modèles de ces théories ne s'attache qu'à un seul mécanisme liant progrès technique et croissance. Comme le notent Dominique Guellec et Pierre Ralle, 

L'un des principaux critiques du modèle de croissance économique est l'économiste Nicholas Georgescu-Roegen en introduisant dans l'analyse économique la notion d'entropie mise en évidence par Sadi Carnot en 1824 et Rudolf Clausius en 1865. C'est cette analogie qui remettrait fondamentalement en cause la notion de croissance économique pour prôner une bio-économie que la nature nous imposera, en raison de la finitude de certaines ressources (pétrole, gaz, charbon, métaux précieux...) et de l' de tous processus productifs. toutefois que les théories propres aux systèmes complexes initiées par le prix Nobel Ilya Prigogine analysent d' les évolutions, même dans le domaine physique (, , …).

Les études empiriques modernes indiquent que le niveau de l'investissement des entreprises est très dépendant de leurs anticipation sur le niveau de croissance économique attendu pour les dix prochaines années. À la fin du , puis dans les années 1960 et dans la deuxième partie des années 1990, la régularité des statistiques de création d'emploi positives ont donné aux entreprises le sentiment que leurs produits et services pourraient compter à moyen terme sur un grand marché durablement solvable, justifiant l'investissement. Cette constatation milite pour la recherche d'une croissance avant tout prévisible, sans interruption ni cycle économique trop marqué, même si son intensité est moins forte. La théorie de la « fin du cycle économique », à la fin des années 1990 a même généré des taux d'investissement record dans les nouvelles technologies, et une des plus grandes bulles de l'histoire des bourses de valeurs, suivie d'un krach dans ce même secteur des nouvelles technologies lorsqu'il est apparu que la vague d'investissement avait généré des surcapacités.

Les tenants de la décroissance considèrent la croissance infinie comme une impossibilité physique et expriment a minima de sévères réserves sur la possibilité de poursuivre le modèle actuel de croissance, en raison de la nature finie des ressources naturelles. Rien n'indique selon eux que l'on puisse y substituer d'autres ressources, ni que les ressources renouvelables puissent rendre les mêmes services. De même, ils soulignent les éventuelles dégradations de l'environnement qui pourraient remettre en cause la croissance future. Pour les critiques de la croissance, la promesse de « développement économique pour tous » n'est donc qu'une promesse qui ne repose sur rien de tangible. André Gorz souligne ainsi qu'« une croissance illimitée dans un monde fini est une illusion ».

Les premières critiques de la notion de croissance datent du début du .

Au , le rapport commandé en 1970 par le Club de Rome à une équipe du Massachusetts Institute of Technology, intitulé "The Limits To Growth". Ce rapport est encore connu sous le nom de « rapport Meadows », du nom de deux de ses auteurs. Il a fait l'objet de deux mises à jour en 1993 et en 2004, qui ne remettent pas fondamentalement en cause les conclusions du premier rapport. L'étude souligne les dangers, sur les plans écologique, économique, et humain, de différentes hypothèses de croissance économique et démographique. Elle a inspiré de multiples réflexions sur le concept de développement durable, qui s'est progressivement imposé depuis les années 1980 et 1990 dans la communauté internationale.

Une partie de la croissance économique est permise par l'exploitation des ressources naturelles : il convient donc de les gérer au mieux (par exemple par le recyclage), d'optimiser le potentiel d'extractions et de ressources. L'efficacité du système capitaliste est alors parfois remise en cause. Néanmoins, Karl Marx soulignait déjà dans "Le Capital" , faisant tout pour que « rien ne se perde ni ne soit gaspillé ». Les économistes libéraux soutiennent que le libre marché permet la meilleure affectation des ressources et leur gestion la plus efficace.

L'économiste Pascal Salin va jusqu'à soutenir que les problèmes d'efficacité et de gestion liés à l'exploitation des ressources pourraient être résolus par la privatisation de ces ressources. En effet, un propriétaire, responsable d'une ressource naturelle, va l'évaluer et la gérer de façon à maximiser sa richesse et va donc l'entretenir. Pascal Salin prend comme exemple le problème de déforestation des forêts amazoniennes et écrit que . Pascal Salin insiste également sur le progrès technique et sur les .

Contestant la vision optimiste d'un progrès technique capable de répondre aux problèmes et questions qu'il a lui-même engendrés, des penseurs et économistes voient une autre logique à l'œuvre dans l'idéal de croissance, qui obère la saine gestion des ressources de la planète. Ainsi pour Jacques Ellul, contempteur moderne de ce qu'il a appelé le "système technicien", pour une entreprise capitaliste, seul compte le profit indépendamment des effets positifs ou négatifs de son activité.

La croissance mondiale depuis la fin du a été possible grâce au charbon puis au pétrole, qui sont des ressources naturelles non renouvelables. D’autres sources d’énergie sont venues compléter les besoins croissants en énergie électrique comme l'énergie nucléaire qui elle aussi repose sur une ressource, abondante selon l'AIEA, mais non renouvelable, l'uranium, encore que des recherches s'orientent notamment vers le thorium et à plus long terme la fusion nucléaire.

La production économique engendre dans certains cas des perturbations dans les équilibres écologiques, du fait de la surexploitation des ressources naturelles : émissions de gaz à effet de serre (énergies fossiles), surpompage (eau), surlabourage (terres arables), surpâturage (ressources végétales), surpêche (ressources halieutiques). Augmenter la production de biens matériels ou le transport (pour répondre à l’accroissement démographique par exemple) peut aggraver ces perturbations. Ces effets sont particulièrement visibles depuis les années 2000 dans la plaine du Nord de la Chine par exemple, qui manque cruellement d'eau par suite d'une activité économique en très forte croissance depuis les années 1980.

Le réchauffement climatique amène l'ensemble des économies du monde à prendre en compte leurs émissions de gaz à effet de serre et à rechercher au maximum une « croissance propre ». La communauté internationale envisage la mise en place de contraintes collectives, comme le protocole de Kyoto.

Certaines études montrent les conséquences de la croissance économique mesurée par le produit intérieur brut sur l'évolution du capital naturel.

Les critiques de la croissance insistent enfin sur les déséquilibres qui peuvent naître de la croissance : bouleversements sociologiques, politiques et écologiques.

Ainsi, les exodes ruraux ou les nouveaux moyens de transport ont entrainé un exode rural et des transformations urbanistiques majeures, qui changent durablement les rapports sociaux. De plus, 

Pour ses partisans, la croissance économique permet la diminution des inégalités de revenu des individus à l'échelle supranationale. Quand c'est le cas, des enquêtes d'opinion sur la qualité de vie montrent que celle-ci augmente de concert avec le revenu par habitant, du moins jusqu'à un seuil de par an.

La diminution rapide de la pauvreté dans le monde dans la seconde moitié du est établie. Elle est largement due à la croissance économique, selon la Banque mondiale. C'est dans les régions où la croissance a été la plus faible, en particulier en Afrique subsaharienne, que la pauvreté a le moins diminué et qu'elle risque d'augmenter à l'avenir.

Toutes les prédictions de bornes absolues au développement depuis Malthus se sont révélées fausses, en raison de la capacité de l'homme à trouver de nouveaux usages aux ressources : le travail humain a été remplacé par le travail animal, puis mécanique, avec le développement progressif d'énergies nouvelles : bois, charbon, électricité, pétrole. Ainsi, l'économiste Julian Simon affirme dans "The Improving State of the World" que les conditions matérielles de l'humanité s'améliorent rapidement.

Pour lui, l'économie répond aux lois de la thermodynamique et de la biologie. Selon le second principe de la thermodynamique, l’énergie d’un système clos tend inéluctablement à la dégradation thermique. Il en va de même pour l'économie: le processus économique matériel ne peut se répéter et s’accroître indéfiniment dans un monde où l’énergie et les matières premières sont limitées.

Jacques Ellul a analysé dans toute son œuvre la société technicienne dans laquelle nous vivons sous tous ses aspects. Dans la troisième partie de son ouvrage "Le bluff technologique" intitulée « Le triomphe de l'absurde », il considère que la croissance (économique) est l'un des paradigmes de la déraison. Il dénonce l'obsession de la croissance, à tout prix, alors qu'on ne se demande ni : croissance de quoi ? Ni, cette croissance est-elle utile ? Ni : à qui servira cette croissance ? Ni même que fera-t-on de tous ces excédents ? Il rappelle qu'il ne peut pas y avoir une croissance illimitée dans un monde limité.

En 1974, Richard Easterlin publie une étude empirique montrant que le PIB par habitant, au-delà d'un certain seuil de richesse, n'a pas d'effet sur le niveau de satisfaction des individus. Ce paradoxe est connu dans la littérature économique sous le nom de « paradoxe d'Easterlin ».

Il a été remis en cause en 2008 par l'étude de Justin Wolfers et Betsey Stevenson, montrant à l'aide de données individuelles collectées dans un grand nombre de pays qu'il existe bien un lien entre le PIB par habitant et le degré de satisfaction des individus.

Une étude plus approfondie, publiée en 2013 par la revue PLOS ONE, confirme les conclusions d'Easterlin : la satisfaction de vivre s’accroît fortement avec le PIB dans les pays à faible revenu, mais la relation devient beaucoup moins pentue au-delà d’un PIB de , puis elle s’aplatit avec un PIB au-delà de , et tend même à décliner avec le PIB dans les pays les plus riches, suggérant l’existence d’un « point de béatitude » qui se situe dans l’intervalle entre et en parité de pouvoir d’achat.

Dans les années 1950, Simon Kuznets avait supposé l'existence d'une relation générale entre croissance et inégalités (courbe de Kuznets), celles-ci augmentant d'abord, puis diminuant lorsque les revenus sont assez élevés. Les études empiriques successives ont largement invalidé cette hypothèse et, en première approximation, la croissance est neutre par rapport aux inégalités.

Dans une étude empirique publiée pour la Banque mondiale, David Dollar et Art Kraay ont conclu que les revenus des populations pauvres (le quintile inférieur) augmentaient proportionnellement avec le revenu moyen, de manière presque systématique quelles que soient les périodes et les pays concernés. Toutefois, la même année, la Banque mondiale relève dans un ouvrage intitulé "Qualité de la croissance", qu'une croissance instable a des répercussions très négatives sur les pauvres, et que leurs actifs humains et naturels peuvent être tellement affectés en cas de crise, que cela peut les empêcher de profiter de la reprise lorsque celle-ci apparaît. Il en est de même des politiques d'encouragement envers les pauvres, dont l'interruption peut être liée aux à-coups de croissance, et dont la cyclicité a les mêmes effets. D'autre part, la dégradation du capital naturel (environnement, terres, sources, pêcheries), dont l'impact sur la croissance économique est sujet à débats, risque selon elle d'avoir des Enfin, l'inégalité des revenus, résultant d'une répartition inégale des actifs tels que capital physique, ressources éducatives et terres, affecte le bien-être social via deux mécanismes, dont un mécanisme indirect qui réduit le potentiel de croissance et les chances de faire reculer la pauvreté.

La croissance dépend de l'augmentation des facteurs de production, de ressources, naturelles, foncières et humaines pas, peu, difficilement, couteusement ou lentement renouvelables... souvent déjà surexploitée (surpêche, déforestation, érosion des sols, etc.).

Le progrès technique peut limiter ou aggraver l'appropriation des ressources naturelles par l'homme au détriment des autres espèces. Certains auteurs soulignent que la croissance économique mesurée par le PIB tend à détruire le capital naturel. Certains économistes contemporains, comme Paul Romer, intègrent dans leurs réflexions la limitation des ressources naturelles, et le fait que le progrès technologique et la « connaissance » peuvent générer une nouvelle croissance.

Au plan micro-économique, des études ont montré que les stratégies visant à renforcer les capitaux propres des entreprises, ce qui permet d’absorber les pertes en cas d’accident de parcours contribuent à une croissance durable et mieux prévisible.

Une des critiques de l'économie de marché est que l'environnement est mal pris en compte dans les modèles économiques actuels, sauf peut-être à travers le progrès technique dans le modèle de Solow (d'inspiration néoclassique avec deux facteurs de production capital et travail), dans la mesure où celui-ci tient compte des contraintes environnementales. Quand le progrès technique ignore les contraintes environnementales, la croissance et une meilleure productivité peut avoir des effets négatifs sur l'environnement, ce que dénonçait le philosophe Hans Jonas dans "Le Principe responsabilité" dès 1979.

L'un des secteurs où ces déséquilibres apparaissent le mieux est celui de l'agriculture, où le modèle productiviste de l'agriculture intensive pratiquée depuis la Seconde Guerre mondiale a généré des impacts environnementaux négatifs (Cf. pollution diffuse et générale par les pesticides et engrais, perte de biodiversité, dégradation des sols).

L'Organisation de coopération et de développement économiques (OCDE) la définit comme : elle consiste à . Elle nécessite de “découpler” davantage les impacts environnementaux et la croissance économique et d’adopter des modes de consommation et de production plus respectueux de l'environnement tout en réduisant la pauvreté et en améliorant les perspectives des populations en matière de santé et d’emploi. Elle implique de 

En France, un « Observatoire national des emplois et métiers liés à la croissance verte » a été créé en 2010, qui vise à fournir , qui a défini plusieurs périmètres (). Au sens large, pour un , selon l’observatoire (publication 2010), environ emplois (en équivalent temps plein) étaient concernés en 2008. Des métiers et des emplois verts sont maintenant identifiés dans le répertoire de Pôle emploi et dans la nomenclature des professions et catégories socioprofessionnelles de l'Insee ; et 9 professions « vertes » ont été identifiées fournissant emplois en 2007.

L'observatoire Trendeo dénombre seulement nettes d'emplois annoncées en France de 2009 à 2016 dans le secteur du développement durable, allant de l'éolien au solaire en passant par la dépollution, l'alimentation biologique, le jardinage et les espaces verts ou les biocarburants. Le secteur a échoué à se structurer autour d'acteurs majeurs dans les deux filières dont on parlait beaucoup en 2009, le solaire et l'éolien, où de nombreuses annonces de créations d'emplois n'ont pas été suivies de réalisations.





Notions
Théoriciens



</doc>
<doc id="16843" url="https://fr.wikipedia.org/wiki?curid=16843" title="Louchébem">
Louchébem

Le louchébem ou loucherbem, dans son nom complet largonji des louchébems (« jargon des bouchers »), désigne l'argot des bouchers parisiens et lyonnais de la première moitié du . Le louchébem reste de nos jours connu et utilisé dans cet univers professionnel.

Le louchébem est un "largonji" (jargon) que le lexicographe Gaston Esnault définit ainsi : .

Strictement, le louchébem est une .

Le processus de création lexicale du louchébem se rapproche du verlan et du javanais. On « camoufle » des mots existants en les modifiant suivant une certaine règle : la consonne ou le groupe de consonnes au début du mot est reportée à la fin du mot et remplacée par un « l », puis on ajoute un suffixe argotique au choix, par exemple -"em"/"ème", -"ji", -"oc", -"ic", -"uche", -"ès". Ainsi "s-ac" se mue en "l-ac-s-é", "b-oucher" en "l-oucher-b-em", "j-argon" en "l-argon-j-i", etc.

Le louchébem est d'abord et surtout un langage oral, et l'orthographe en est très souvent phonétisée.

Le louchébem ne semble pas avoir été conçu par les bouchers de Paris. Les plus anciens mots attestés du louchébem se trouvent, mêlés à l'argot du bagne de Brest, dans les écrits d'un forçat, Ansiaume, datés de .

Encore aujourd'hui les bouchers se servent du louchébem en communauté. Voici quelques exemples : 

Certains mots de louchébem sont devenus communs et ont aujourd'hui leur place dans le langage familier.

C'est en particulier le cas de "loufoque" que Pierre Dac, fils de boucher, a contribué à populariser, au point d'être parfois présenté comme l'inventeur-même du mot.

Exemples :

Une version exacte du louchébem (aujourd'hui quasiment disparue sauf dans les communautés d'anciens résistants) était parlée par les résistants français pendant la Seconde Guerre mondiale
On retrouve beaucoup de termes louchébem dans la littérature d'Alphonse Boudard : « Je me retrouve ce soir avec les "Lettres à l’Amazone" de Remy de Gourmont. Ça ne m’en apprend pas lerchem [cher] sur l’Inde, les Hindous et les Clancul. Non, et je ne parviens pas à les lire, ses somptueuses bafouilles au cher Maître. » ("La métamorphose des cloportes")

Dans la chanson "Sale Argot" du groupe de rap français IAM, sur la mixtape "IAM Official Mixtape", le rappeur Akhénaton rappe un couplet entier en louchébem.

Dans son album "Méfiez-vous des petites filles", Philippe Marlu interprète "Lansonchouille", première chanson en louchébem du millénaire, écrite par Stéphane 'Léfanstouf' Moreau.
Raymond Queneau a également utilisé le loucherbem dans un texte du même nom dans son recueil "Exercices de style", publié en 1947.

Dans son roman jeunesse "Les Mystères de Larispem", Lucie Pierrat-Pajot met en scène une caste de bouchers ayant pris le pouvoir lors de la Commune de Paris pour former un état populiste, où aristocrates n'existent plus et tous vivent égaux. Dans ce Paris rétro-futuriste, le louchébem est couramment parlé, et l'auteure explique que c'est en apprenant l'existence de cette forme d'argot que lui est venue l'inspiration pour son livre. 


Autres procédés de déformation de mots à but cryptique :



</doc>
<doc id="16854" url="https://fr.wikipedia.org/wiki?curid=16854" title="Autriche-Hongrie">
Autriche-Hongrie

L'Autriche-Hongrie , en forme longue habituelle l’Empire austro-hongrois (en allemand : "Österreichisch-Ungarische Monarchie") ou l'empire d’Autriche-Hongrie, également appelée Double monarchie ou Monarchie danubienne, est un État d’Europe centrale qui a existé de 1867 à 1918.

Cet État repose sur une union personnelle par la maison de Habsbourg-Lorraine de trois ensembles, liés par un « compromis austro-hongrois » (en allemand ', en hongrois ') :

Au milieu du , les empereurs d’Autriche se trouvent exclus de leurs zones d'influence traditionnelles, l’Italie (1860) puis l’Allemagne (1866). Le nouveau projet politique de François-Joseph consiste alors à conforter son assise en associant les élites hongroises au pouvoir. Ce compromis est accepté par l’aristocratie hongroise car il pérennise ses privilèges féodaux dans ce pays. En 1867, , déjà empereur d’Autriche, est couronné roi de Hongrie à Budapest. Autocrate conservateur mais pragmatique, il s'appuie sur les facteurs de cohésion que sont la monarchie et sa bureaucratie, l’Église catholique, l’aristocratie et l’armée. Son petit-neveu lui succède en 1916 sous les noms de d’Autriche et de Hongrie.

L’empire habsbourgeois est devenu ainsi une « Double monarchie », expression que l’Autriche-Hongrie possède en propre. L’aigle à deux têtes est un symbole bien antérieur à la constitution de cette double monarchie, mais lui convient parfaitement. On emploie aussi l’expression « Monarchie danubienne ». Un tout petit affluent du Danube, la Leitha, proche de Vienne, mais séparant alors le domaine de la couronne impériale autrichienne de celui de la couronne royale hongroise, sert à l’esprit frondeur des Viennois pour surnommer le premier « Cisleithanie » (« de ce côté-ci de la Leitha »), et le second « Transleithanie » (« de l'autre côté de la Leitha »). Ce sont des expressions non officielles, mais qui finissent par être employées par l’administration.

Ce compromis politique avec les Hongrois lèse les intérêts des autres peuples de la « Double monarchie », slaves (Tchèques, Slovaques, Polonais, Ukrainiens, Slovènes, Croates, Serbes) ou latins (Italiens, Roumains). À l’issue de la Première Guerre mondiale, cet empire éclate, éclatement que le traité de Saint-Germain confirme au nom du « droit des peuples à disposer d'eux-mêmes » formulé dans le dixième des quatorze points du président américain Woodrow Wilson.

Les décisions majoritaires des assemblées représentatives de ces peuples constituées lors de la défaite, remplacent en décembre 1918 la « Double-Monarchie » par sept « États-nations ». Certains sont nouvellement indépendants : la Tchécoslovaquie ou le royaume des Serbes, Croates et Slovènes - future Yougoslavie, formé par la réunion des Slaves du Sud d’Autriche-Hongrie, de Serbie et du Monténégro. Un autre, la Pologne, est ressuscité après plus de 120 ans de disparition. Deux pays latins déjà existants sont simplement agrandis aux dépens de l’empire : l’Italie et la Roumanie.

Ces profonds changements sont officialisés durant les deux années suivantes par la signature des traités de Saint-Germain et du Trianon qui consacrent la fin de l’Empire, l’interdiction pour les Habsbourg de résider en Autriche, mais aussi l’interdiction pour les Allemands d’Autriche, ainsi exclus du fameux « droit des peuples », de s’unir à la république de Weimar. L’Autriche elle-même devient une république, tandis que la Hongrie reste officiellement un royaume, à la tête duquel est instaurée une régence. Le territoire de la Hongrie est réduit au tiers central de la Grande Hongrie (où les Magyars sont très majoritaires) et laisse une partie des populations hongroises hors de ses nouvelles frontières.

Succédant à l'empire d'Autriche, la dénomination officielle d'Autriche-Hongrie apparaît en 1867, mais désigne un État déjà constitué dès la première partie du . C'est la dernière forme politique qu'ont prise les possessions de la dynastie impériale des Habsbourg-Lorraine en Europe centrale.

L’Autriche-Hongrie est, après la Russie, le plus grand État d’Europe avec une superficie de . Mais tous ses peuples, et notamment les Slaves, n’y sont pas également représentés : l’ancien royaume de Bohême-Moravie, celui de Galicie-Lodomérie, celui de Croatie-Slavonie ainsi que la principauté de Transylvanie ont perdu leurs prérogatives antérieures et la « double-monarchie » n'est formée que de deux entités principales et ultérieurement une troisième :

Cette organisation territoriale et la législation qui en découle favorisent largement les aristocraties et les bourgeoisies germanophones et magyarophones, de confession catholique romaine (religion de l’État et de la dynastie), au détriment des autres groupes sociaux, linguistiques ou religieux.
Les royaumes et pays représentés à la Diète d'Empire ont une superficie de pour une population de (recensement de 1910) ; ils sont constitués de . Chacune de ces provinces dispose d’une autonomie relative, avec des assemblées provinciales ("") élues dans un premier temps au suffrage par classes, censitaire puis universel. Les habitants de langue allemande, qui comptent pour 36 % de la population totale et élisent 43 % des sièges au parlement, s’opposent globalement à la restauration d’un royaume de Bohême autonome, revendiquée par les Tchèques.

Un ensemble austro-bohême datant de 1526 a été consolidé lors des guerres de Trente Ans (1618-1648) et de succession d’Autriche (1740-1748). Cet ensemble sert de socle à la création de l'empire d’Autriche en 1804.
Avant 1867, l'empire d'Autriche comprend donc :

Les autres territoires sous domination de l'Autriche sont alors :

Les pays de la Couronne de saint Étienne ont une superficie de et une population de . Ils sont constitués :
Le royaume de Hongrie possède, contrairement à l'empire d'Autriche, une structure centralisée. Les comitats, unités administratives de base, sont gérés conjointement par un préfet nommé par l'État et un "alispán" élu par l'aristocratie locale. Le royaume de Croatie-Slavonie dispose d'une administration autonome dirigée par un "ban" nommé par l'État qui est responsable devant le "Sabor", assemblée élue qui a la capacité de légiférer dans les domaines de l'enseignement et la justice.

Les pays de la Couronne de Saint-Étienne ont été constitués en 1000 et restaurés en 1699 lors de la reconquête de la Hongrie sur l'Empire ottoman.
Si le nord et l'ouest du royaume et de la Croatie sont sous la domination des Habsbourg dès 1526, le reste de la Hongrie (Hongrie ottomane et Transylvanie) ne le sont que depuis 1699.

Administrée par l'Empire à partir de 1878 et annexée en 1908 sous les auspices du ministre Alois Lexa von Aehrenthal, la Bosnie-Herzégovine a une superficie de et une population de . Elle est constituée de deux provinces :
Comment décider à quelle entité cette province appartiendrait ?
De plus, les catholiques (Croates) sont minoritaires en Bosnie-Herzégovine et ni la Cisleithanie ni la Transleithanie ne souhaitent augmenter leurs minorités orthodoxes (Serbes), et encore moins englober une minorité musulmane (Bosniaques).
De ce fait, la Bosnie-Herzégovine se retrouve ainsi être la « Troisième entité de la double monarchie » : annexée, elle ne dépend spécifiquement d’aucune de ses deux composantes mais est considérée comme commune aux deux couronnes, qui administrent ensemble tout ce qui ne relève pas des pouvoirs locaux bosno-herzégovins. Les pouvoirs locaux sont surtout confiés à la minorité catholique (croate) et à la majorité musulmane (bosniaque), ce qui mécontente la minorité orthodoxe (serbe) : une situation instable qui explique la constitution de sociétés secrètes comme la Jeune Bosnie () dont faisait partie Gavrilo Princip, l’assassin de l’archiduc François-Ferdinand.

Après la défaite de Sadowa en 1866 face à la Prusse, les options fédérales à six (Autriche, Bohême-Moravie, Galicie, Hongrie, Croatie et Transylvanie) ou à trois (Autriche, Hongrie, Croatie) sont abandonnées et, en 1867, l'empire d'Autriche devient une « double monarchie » (impériale et royale) rassemblant l'empire d'Autriche et le royaume de Hongrie. Le compromis de 1867 est négocié entre Beust, président du Conseil de l'empire d'Autriche, et Andrássy, président du Conseil du royaume de Hongrie, l'un des héros de la guerre d'indépendance de 1848-1849. Ce compromis fait accepter François-Joseph par les Hongrois, et il est solennellement couronné roi de Hongrie à Budapest.

La partie autrichienne de l’empire devient peu à peu une véritable monarchie constitutionnelle, avec un système de représentation bicaméral qui comprend :
Eduard Taaffe, ami personnel de l’empereur, est ministre-président de 1879 à 1893.

Dans la partie hongroise, le pouvoir est exercé par l’aristocratie foncière hongroise qui s’assure le contrôle quasi-exclusif de la chambre des députés, grâce à un système électoral censitaire et un découpage approprié des circonscriptions. Kalman Tisza puis son fils Istvan Tisza sont ministre-président de 1875 à 1890, puis de 1903 à 1905 et de 1913 à 1917. La capitale de l’État restauré fait l’objet d’un plan d’urbanisme ambitieux, de la fusion en 1873 des communes de Buda, Obuda et Pest, à l’inauguration en 1896 de la première ligne de métro continentale et de la place des Héros, pour célébrer le millénaire du royaume.

Dans ce cadre, les affaires communes sont réduites à trois ministères. Les affaires étrangères sont une administration unique commune aux deux États. S'il y a une armée commune avec un état major commun, il y a toutefois deux armées territoriales : « Landwehr » en Autriche et « Honvéd » en Hongrie. Le ministère des Finances commun gère en plus à partir de 1879 la Bosnie-Herzégovine : il est financé à 70 % par la Cisleithanie et à 30 % par la Transleithanie.

Les affaires commerciales, douanières et monétaires qui exigent une harmonisation font l'objet de dispositions communes renouvelées tous les dix ans, dans des projets de lois identiques adoptés par les deux parlements. La convention douanière qui fait des territoires autrichien et hongrois un seul territoire douanier est renouvelée sans problème en 1878 et 1887. Elle est remplacée à partir de 1903 par un traitement de faveur avec réciprocité assurée.

Les Hongrois ayant rejeté l'idée d'un parlement commun, le contrôle des instances communes est réalisé par deux délégations. Chacun des deux parlements élit un maximum :
En Hongrie où le système électoral exclut en fait les nationalités autres que hongroise, quatre des quarante délégués de la chambre basse sont croates. Les délégations siègent séparément, communiquent entre elles par écrit et n'ont aucun pouvoir législatif.

Le système né du compromis de 1867 est une tentative de synthèse entre un État fédéré (') et une fédération d'États ('), les hommes d'État de chacune des deux parties de la monarchie développant des lectures différentes :

L'Autriche-Hongrie a globalement développé à la fois son agriculture et son industrie. Cependant, l'évolution de chacune de ses parties est contrastée. La Hongrie reste essentiellement agricole et gouvernée par une aristocratie jalouse de ses privilèges, alors que l'ensemble austro-bohême s'industrialise et se démocratise.

Point de convergence des archaïsmes et des idées progressistes d'une époque, Vienne est en 1900 l'un des principaux phares de la culture européenne.

Le défi majeur de la double monarchie est d'ordre politique. L'Autriche-Hongrie connaît comme les autres États de l'époque les conflits de classes sociales, mais en outre les diverses nationalités demandent leur place à côté des germanophones et des Hongrois, notamment celles qui avaient constitué des États avant d'être intégrées : principalement les Tchèques en Autriche et les Croates en Hongrie.

À la veille de la Première Guerre mondiale, l’Autriche-Hongrie est le troisième État européen par sa population, soit . L’allemand est langue officielle en Cisleithanie, le magyar en Transleithanie. L’Empire compte quatorze groupes culturels et linguistiques dont six non-slaves. Cependant, au sein de l’armée, l’allemand reste la seule langue du commandement même si les officiers doivent être polyglottes afin d’être compris de leurs hommes.

Les « nationalités » (se définissant à l'époque, lors des recensements, par la déclaration d’usage de la langue) sont, en 1910, réparties ainsi :

Places-fortes de la contre-Réforme, l'Autriche et accessoirement la Hongrie ont laissé peu d'espace au protestantisme qui est toutefois resté vivace en Bohême-Moravie et en Transylvanie. En conséquence, plus des trois quarts des Austro-Hongrois sont en 1910 catholiques d'affiliation (). L'orthodoxie est présente en Bosnie-Herzégovine, en Hongrie du Sud-Est, en Galicie orientale, en Transylvanie et en Bucovine. L'islam est présent en Bosnie-Herzégovine. Phénomène tant religieux qu'ethno-culturel, le judaïsme est représenté sous sa forme ashkénaze dans les "" de Galicie, de Bucovine et de Marmatie, ainsi que dans les métropoles autrichiennes ou hongroises, et sous sa forme séfarade en Bosnie (particulièrement à Sarajevo où il est souvent ladinophone). 

Si certains pays sont mono-religieux (en Autriche, Bohême-Moravie, Dalmatie et Croatie il n'y a pratiquement que des catholiques), d'autres se partagent entre trois ou quatre confessions (en Bosnie-Herzégovine, on trouve des catholiques, des orthodoxes, des musulmans et des juifs, en Transylvanie des catholiques, des protestants, des orthodoxes et des juifs…).

Les germanophones et les magyarophones se partagent entre catholicisme (très majoritaire) et protestantisme ; Slovènes, Croates, Tchèques, Slovaques et Polonais sont catholiques ; Ukrainiens et Roumains peuvent être catholiques (de rite grec) ou orthodoxes ; les Serbes sont orthodoxes, et les Bosniaques musulmans. Les Roms enfin sont, pour certains, chamanistes, mais la majorité a déjà adopté l'une des religions des pays où ils vivent.

Toutes ces confessions sont présentes dans la capitale, où se produit un mélange culturel propice au développement de la vie intellectuelle et artistique, que d'aucuns trouvent admirable, tandis que d'autres le trouvaient détestable. Dans une moindre mesure, on retrouve le même ' dans les principales villes de l'Empire, comme en témoignent la littérature, le théâtre, les arts et l'architecture. 

En 1910, 55 % de la population active travaille encore dans l'agriculture, 48 % en Cisleithanie et 62 % en Transleithanie. La Transleithanie fournit la moitié de la farine nécessaire à la production du pain consommé en Cisleithanie. La propriété des terres est très concentrée en Bohême et en Hongrie, elle est davantage morcelée dans les pays alpins où subsiste une classe moyenne paysanne importante.

L'Autriche-Hongrie est devenue la quatrième puissance industrielle en Europe, devançant de peu la Russie. La production aurait triplé en Autriche entre 1880 et 1913, avec en tête la métallurgie et la construction mécanique. Le Ministre-président d'Autriche de 1879 à 1893 Eduard Taaffe met en place une législation sociale relative à la durée du travail et à l'assurance maladie. Les industries se concentrent essentiellement en Basse-Autriche, Bohême ainsi qu'à Budapest côté Hongrie. Elles emploient 23 % de la population active.

La double monarchie constitue un vaste marché intérieur, notamment pour les produits agricoles de Transleithanie. Les produits manufacturés de Cisleithanie commencent à souffrir de la concurrence allemande. Le réseau ferroviaire a été multiplié par 8 entre 1860 et 1900. Il a récupéré son retard par rapport aux autres grands États européens grâce en particulier à l'impulsion de l'État. Trieste est devenu le huitième port européen et voit passer la moitié des exportations de la double monarchie.

Les clivages linguistiques et religieux correspondent souvent à des clivages économiques et politiques : ainsi par exemple les orthodoxes (Ukrainiens, Roumains ou Serbes) sont presque partout pauvres et peu instruits, alors que dans les régions où ils sont majoritaires, l’aristocratie, maîtresse des terres, est catholique de langue allemande, magyare ou polonaise, tandis que la bourgeoisie, maîtresse de l’industrie et du commerce, est catholique, protestante ou juive, de langue allemande, magyare ou yiddish. Dans ces conditions, la déclaration d’appartenance à une nationalité, c’est-à-dire l’affirmation d’une langue et d’une culture, vise moins à l’obtention d’une quelconque souveraineté qu'à exprimer des revendications sociales et politiques au sein de l’Empire.

Dans les deux parties de la Monarchie, la noblesse a un grand poids économique, en raison de l’immensité des terres possédées par elle. En 1914 par exemple, moins de 1 % de la population possède 40 % du territoire. Une des revendications des nationalités est la réforme agraire, qui sera partiellement réalisée après le partage de l'Empire. De ce point de vue, les paysans ont pu voir avec espoir la double monarchie s'effondrer. En termes de classes sociales et de politique, la prédominance de l’aristocratie dans la haute administration et au gouvernement fait de la société austro-hongroise une société inégalitaire.
Les situations économiques et sociales, ainsi que la situation des minorités sont différentes en Autriche et en Hongrie.

L’Autriche, avec une bourgeoisie active et un taux de croissance économique avant 1914 équivalent à celui de l’Allemagne, se rapproche des États de l'Europe occidentale. La grande noblesse n'a pas hésité à investir dans les entreprises industrielles (exemple : Skoda) et François-Joseph n’hésite pas à anoblir la grande bourgeoisie, y compris juive, en reconnaissance de ses mérites. D’ailleurs les Juifs austro-hongrois découvriront après 1918 que les « États successeurs » de l’Empire sont (surtout à partir des années 1930) généralement bien moins tolérants à leur égard que les Habsbourg-Lorraine. Un système d’assurances sociales a été créé au profit des travailleurs. Le suffrage est universel et direct depuis 1907 et les nationalités (Roms exceptés) sont proportionnellement représentées au Parlement de Vienne. 

Les Slaves, pour leur part, participent davantage à la vie politique de l’Empire en Autriche qu’en Hongrie. Le club polonais au Parlement de Vienne fait souvent varier les majorités, au gré de ses intérêts. Edvard Beneš et Hugh Seton-Watson (journaliste anglais ardent défenseur avec Lord Northcliffe de l’indépendance tchèque), reconnaissent que les Tchèques ont une certaine liberté politique sous l’Empire. Le problème des langues en Bohême se pose dès les élections de 1897 : il s’agit de permettre la mise en place d’un bilinguisme administratif en Bohême, donc de revenir partiellement à une situation antérieure à 1620. À cette réforme en faveur du tchèque s’opposent l’ensemble des partis allemands, dont les pangermanistes, qui organisent une forte agitation, relayée par le , dans le Nord de la Bohême. Ce problème se manifeste par une pratique parlementaire utilisée de nombreuses fois par la suite : l’obstruction parlementaire, qui peut prendre plusieurs formes. Durant la période inaugurée par la mise en place du suffrage universel par classes en 1896 et close par la déclaration de guerre en 1914, les problèmes linguistiques paralysent le fonctionnement institutionnel de l’Autriche, ainsi que celui des partis qui se pensent transnationaux, comme les sociaux-démocrates du Parti social-démocrate d'Autriche.

La Hongrie, en revanche, reste très féodale dans sa structure (un héritage de l’écrasement de la révolution de 1848), et un tiers des terres appartient à moins de aristocratiques. La vie politique est essentiellement réservée aux Magyars qui ne représentent que 48 % de la population de la Grande Hongrie. Les Slaves et les Roumains n’ont aucun droit politique, par défaut de représentation en application du système électoral hongrois. Les mouvements autonomistes croate, roumain et slovaque se trouvent très ancrés dans la volonté collective de ces populations, du fait des discriminations et des humiliations de la part des aristocrates hongrois qui considèrent Slovaques et Valaques comme « allogènes » alors qu'ils vivent dans leurs terroirs depuis plus de mille ans.

Pour l’écrivain autrichien de langue allemande Robert Musil (dans son roman "L'Homme sans qualités"), qui a dû être officier dans l’armée « cacanienne », l’Autriche-Hongrie était la "Cacanie", du préfixe apposé partout K. und K. : " (« impérial et royal »).

La terminologie exacte semble avoir été la suivante :

Le puissant rayonnement culturel de la monarchie habsbourgeoise à la fin du et au début du , a été stimulé notamment par sa richesse multinationale et le dynamisme de sa minorité juive : Hermann Broch, Sigmund Freud, Karl Kraus, Gustav Mahler, John von Neumann, Karl Popper, Joseph Roth, Arthur Schnitzler, Arnold Schönberg, Otto Weininger et Stefan Zweig en étaient issus. Vienne fut peut-être la capitale de la modernité. Son influence s'étendit tout au long du dans le domaine des arts (peinture, architecture, musique, littérature), de la science et le domaine médical, avec l’école psychanalytique, qui révolutionna la perception du monde. Tous ces domaines étaient un facteur d'universalisme dans cet empire, véritable mosaïque multinational et multilinguistique marqué par la montée des nationalismes des peuples qui le composaient.

Malgré un conservatisme certain, la Cour de Vienne et notamment l’empereur François-Joseph, ont toujours soutenu les artistes contemporains et le groupe de la Sécession, par la commande officielle. Ainsi, entre autres, Otto Wagner participa à la construction du métro de Vienne, en réalisant diverses de ses stations, réalisa l'immeuble de la Caisse d'Épargne et de la Poste, et Gustav Klimt se vit confier la fresque du hall d’entrée du musée d'histoire de l'art de Vienne à Vienne, ainsi que celles de la villa Hermès offerte par François-Joseph à son épouse Élisabeth comme résidence privée à Vienne.

Les idées révolutionnaires des artistes viennois au début du ne s’appliquaient en aucun cas à la contestation de l'ordre politique et social organisé par la dynastie des Habsbourg-Lorraine, dont la supranationalité convenait à leur contestation de l'historicisme issu des mouvements nationalistes de la révolution autrichienne de 1848. Aucun d'entre eux ne se réclamait des nationalités dont ils étaient issus. La Double monarchie, incarnée par la dynastie, par son absence de référent national, était leur lieu d'expression, assignant à l'art une autre mission que politique.

Il peut être convenu aujourd'hui de voir cette période comme la décadence d'une société. Ce n'est pas en termes de décadence que les artistes viennois parlaient d'eux-mêmes mais plutôt en termes de renouveau, en s'opposant aux goûts et aux diktats d'artistes quasi officiels comme le peintre Hans Makart, le « Rubens viennois » ou l'écrivain Franz Grillparzer, chantre de l'époque Biedermeier. Si l'édification de l'opéra de Vienne et du Ring avaient consacré le goût du pastiche architectural dans les , l'édification et la décoration du Métro, de la Caisse d'Épargne et de la Poste et de bien d'autres édifices publics ou privés surent donner ses bases à l'architecture contemporaine, voire futuriste.

En d'autres termes, cette « querelle des Anciens et des Modernes », de l'art officiel contre l'art nouveau a été extrêmement perceptible dans la Vienne du début du .

La décomposition de l’Empire ottoman amène l’Autriche-Hongrie à rechercher l’alliance de l’Allemagne pour contrer l’expansionnisme russe et les velléités de rassembler tous les peuples slaves du Sud en un même État. Cette politique l’éloigne de la France et du Royaume-Uni, alors que la double monarchie partage avec ce dernier le même souci d’équilibre entre puissances continentales européennes. 

Le dualisme austro-hongrois, issu du compromis de 1867, a singulièrement tendu la situation en donnant aux aristocrates hongrois le pouvoir de bloquer toute modification constitutionnelle et toute évolution politique de l'Empire. Les aristocrates hongrois craignent de voir voter des réformes (notamment foncières et linguistiques) qui ébrécheraient leur pouvoir absolu. François-Joseph reconnaît qu’il est souhaitable de créer une troisième force, slave, et de rendre au grand-duché de Transylvanie sa Diète.

Les Tchèques sont mécontents du compromis de 1867 car ils s'estiment oubliés. Dès 1868, les Tchèques demandent à François-Joseph un statut semblable à celui des Hongrois avec l'octroi de l'autonomie au royaume historique de Bohême. Ce projet aurait remplacé le système dualiste de l'Empire en système trialiste. À la consternation des Tchèques, le projet est cependant enterré en 1871 suite au refus catégorique des Allemands de Bohême et de Moravie, qui redoutent de se retrouver en minorité dans ces régions si l'autonomie voit le jour. Quant au gouvernement de Budapest, il rejette également cette idée car il ne veut pas que les Hongrois perdent leur statut privilégié dans la Double-Monarchie.

Un groupe d’intellectuels réunis autour de l’archiduc héritier et progressiste François-Ferdinand d’Autriche et de son épouse Sophie Chotek, comtesse d’origine tchèque, émettent l'idée de transformer la Double Monarchie en un État subdivisés en plusieurs États autonomes dont les limites suivraient la répartition ethnique sur le territoire austro-hongrois. En 1906, Aurel Popovici conçoit alors l'idée des États unis de Grande Autriche. Cette idée est plutôt mal reçue par les Hongrois, qui dans le cadre de ce redécoupage devraient céder une portion importante de leurs terres et ne sera finalement jamais mise en place.
En fait, la « Double monarchie » tolère l’expression des cultures et identités autres qu’allemande ou hongroise, à condition qu’elles ne soient pas les vecteurs du panslavisme, de l’irrédentisme ou du socialisme. Jusqu’en 1917, seule une minorité conteste aux Habsbourg-Lorraine leur statut de souverains légitimes des États sur lesquels ils règnent. La majorité des sujets réclame l’autonomie dans le cadre de l’Empire, et non la sécession. Les sujets autres que germanophones ou magyars revendiquent le respect des langues, des cultures et des religions de chaque groupe, des écoles, une répartition plus équitable des ressources et des impôts, et en Hongrie une meilleure représentation politique.

François-Joseph inaugure le " (expansion vers le sud-est, vers les Balkans). Bien que l’armée russe ait apporté en 1848 un soutien indispensable à la monarchie autrichienne, la politique de François-Joseph s’oppose aux visées de l’Empire russe sur les Balkans dans le cadre du recul de l’Empire ottoman. Ces ambitions antagonistes des deux puissances impériales sont l’une des causes de la Première Guerre mondiale.

L’Empire allemand et l’Autriche-Hongrie, les « Empires centraux », constituent donc une alliance, sous le nom de Duplice ('), qui devient la Triplice (') ou Triple alliance quand l’Italie vient rejoindre l’alliance. Toutefois, l’Italie ne souscrit à cette alliance que dans une optique défensive : celle-ci ne doit fonctionner que dans la mesure où l’un des signataires est agressé.

À la suite de la guerre russo-turque de 1877-1878, le traité de Berlin confie à l’Autriche-Hongrie l’administration de la Bosnie-Herzégovine ottomane. Cette province a une population linguistiquement homogène (on y parle le serbo-croate), mais religieusement composite (on y trouve des musulmans, des chrétiens catholiques que l’Empire va favoriser, et des chrétiens orthodoxes qu’il va étroitement surveiller). La Bosnie-Herzégovine est géographiquement encastrée dans les territoires austro-hongrois. L’Empire annexe ce territoire en 1908, dernière annexion des Habsbourg-Lorraine, malgré l’opposition de la Russie et de la France.

L’Autriche-Hongrie finit par apparaître agressive. À la suite de l’attentat de Sarajevo (assassinat par un serbe de Bosnie de l’archiduc François-Ferdinand, héritier de l’Empire), et bien que la Serbie ait accepté presque toutes les exigences de l’ultimatum austro-hongrois, l'empire décide d'éradiquer la Serbie par la force. Emil Ludwig a montré comment les décisions des empereurs autrichien, russe et allemand, autocrates fortement influencés par l'aristocratie militaire qui les entoure (Hötzendorf en Autriche), ont amené l'enchaînement de la déclaration de guerre à la Serbie le , la mobilisation générale de l'armée russe puis la mobilisation de l'Allemagne contre la France, entraînant le continent européen dans la guerre.

Il y a une armée commune à l’Autriche et à la Hongrie avec des régiments d’infanterie. Ensuite, il y a une organisation de réserve propre à l’Autriche () et une autre propre à la Hongrie (), ainsi qu’une organisation de défense territoriale () en Autriche et une autre en Hongrie. La Bosnie-Herzégovine fait l’objet d’une autre organisation avec quatre régiments d’infanterie (), un bataillon de chasseurs à pied () et des bataillons autrichiens détachés.
L’armée austro-hongroise est le reflet de la disparité de l’Empire. Les Serbes et les Croates sont envoyés en garnison à Vienne ou à Budapest, les germanophones en Bosnie, les Tchèques en Hongrie, les Roumains en Galicie, les Polonais en Transylvanie, les Hongrois en Bohême ou en Bucovine pour tenter d’unifier cet empire multiculturel. La cohésion au combat n’est pas évidente. Un lieutenant de réserve roumain décrit le début de la guerre dans un régiment "" : les officiers supérieurs sont hongrois et ont besoin de traducteurs roumains pour se faire comprendre. Le livre de Liviu Rebreanu et le film "La Forêt des pendus" font état des désertions qui ont pu survenir après , lorsque la Roumanie rejoint l'Entente. Des soldats tchèques, slovaques, slovènes, serbes ou ukrainiens faits prisonniers par les Russes n’hésitent pas, eux non plus, à former des légions auxiliaires de l’armée russe. 

Au début de la guerre cependant, malgré le rôle ingrat que lui a dévolu l’Allemagne en 1914, l’armée austro-hongroise conserve sa cohésion, grâce à un commandement unique largement germanisé : elle lance seule l’offensive contre l’armée russe, tout en affrontant une armée serbe aguerrie. En 1915 et avec ses alliés, elle repousse l’armée russe et occupe la Serbie. Mais l’Italie, d’abord neutre, choisit finalement d’entrer en guerre aux côtés de l’Entente, en signant un traité dont les clauses secrètes prévoient l’attribution du Tyrol du Sud, du Trentin, de Trieste et de la Dalmatie en cas de défaite de l’Autriche-Hongrie.

Monté sur le trône le , d’Autriche-Hongrie sonde les voies de la paix auprès de la France. La négociation entamée au printemps 1917 avec le gouvernement français par l’intermédiaire des princes de Bourbon-Parme, François-Xavier et Sixte, frères de l’impératrice Zita, n’aboutit pas, l’Entente s’étant déjà engagée vis-à-vis de l’Italie, de la Serbie et de le Roumanie.

D’abord victorieuse de l’armée italienne à Caporetto en , l’armée austro-hongroise est finalement défaite à Vittorio Veneto en .

L’idée de nation comme source de la souveraineté, issue des révolutions américaine et française, avait déjà été exprimée lors de différentes révoltes : transylvaine de 1784 et « Printemps des Peuples » de 1848, notamment les révolutions autrichienne, hongroise et roumaine. Les dirigeants et les assemblées des différentes composantes de l’Empire s’appuient en 1918 sur l’idée de nation comme source de la souveraineté lorsque le président américain Woodrow Wilson, dans ses « quatorze points », propose à l’Europe les principes du « Droit des peuples à disposer d'eux-mêmes » et de l’autodétermination. Le principe de la souveraineté nationale triomphe de l’idée d’un empire supranational dont la souveraineté s’incarnait en une dynastie, les Habsbourg-Lorraine. Il ne le fait ni par révolution sanglante comme en France, ni par une consultation populaire unique comme lors du rattachement de Nice et de la Savoie à la France en 1860, mais par une juxtaposition de proclamations.

La dislocation de l’Empire austro-hongrois se produit à la fin de la Première Guerre mondiale, aboutissant à un partage du territoire entre sept États :


Ces nouveaux et anciens États sont appelés les « États successeurs » de l’Autriche-Hongrie.

En 1945, la Ruthénie subcarpathique et la majeure partie de la Galicie seront annexées par l’Union soviétique et incorporées dans la République socialiste soviétique d'Ukraine.

Durant la guerre froide et jusqu’à la chute du rideau de fer en 1989, les cinq sixièmes du territoire de l’ancienne Autriche-Hongrie appartiennent à des états communistes : seule la république d’Autriche et les territoires italiens en sont exclus.

Le royaume de Hongrie qui couvrait l’ensemble du bassin du moyen Danube, des Alpes aux Carpates, se disloque et sa dislocation est officialisée par le traité de Trianon qui réduit le territoire hongrois des deux tiers. La Hongrie adopte dès lors une politique visant à « réviser » ce traité de Trianon. Gouvernée par l'amiral Horthy à partir de 1920, le pays s’engage par opportunisme aux côtés de l’Allemagne nazie à partir de la fin des pour obtenir en échange :

Fin 1941, la Hongrie avait récupéré la moitié des territoires perdus en 1918 mais était devenue un état satellite du Troisième Reich.

À la suite de l'occupation par l'armée soviétique, le traité de Paris de 1947 rétablit les frontières de 1920.

Ces pertes sont encore aujourd’hui très mal vécues par une partie de l’opinion hongroise, notamment depuis que le gouvernement de Viktor Orbán prend des positions de plus en plus nationalistes. Malgré l’opposition affichée par les instances de la Communauté européenne, des mouvements politiques tels le Jobbik revendiquent des formules pour rattacher de façon extraterritoriale les populations de langue hongroise vivant dans les États voisins. Parmi ces formules, le rattachement des citoyens « un par un » : à compter de , il n’est ainsi plus nécessaire de résider en Hongrie pour obtenir la citoyenneté hongroise.

De leurs côtés, les populations hongroises de Roumanie et de Slovaquie se sont dotées de leurs propres organisations politiques qui participent à la vie politique de ces pays.

Malgré le désir exprimé par de nombreux Autrichiens de bénéficier eux aussi du « Droit des peuples à disposer d'eux-mêmes » en se rattachant à la nouvelle République allemande, l'Entente est résolue à maintenir cet État et consolide son assise territoriale :

À la suite du bouleversement des frontières et donc des circuits économiques, l’activité économique en Autriche éprouve des difficultés à se rétablir aux niveaux d’avant-guerre. Par conséquent, le chômage reste relativement important ce qui favorise les mouvements sociaux. Un clivage entre le mouvement socialiste qui gère la ville de Vienne, capitale maintenant disproportionnée où vit le tiers de la population, et les forces traditionalistes dans les provinces, est exacerbé par les débordements de part et d’autre des milices armées. La faillite du Credit Anstallt offre l’occasion au chancelier Dollfuss de suspendre le parlement et d’instaurer un régime d’inspiration chrétienne et corporatiste, qui ne parvient pas à contrer les effets du krach de 1929, n’emporte pas l’adhésion de la population, et finit balayé par l'Allemagne nazie en 1938.

À l’issue de la Seconde Guerre mondiale, l’Autriche est détachée de l’Allemagne et est d’abord occupée par les quatre puissances victorieuses de la guerre jusqu’en 1955, puis tenue de se tenir à équidistance des deux blocs, ce qui achève de séparer son destin géostratégique de celui de l’Allemagne divisée en deux. Par son obligation de neutralité, elle attire les instances internationales, en particulier l’Agence internationale de l'énergie atomique et le siège de l’OPEP.

En revanche, son destin économique et politique la rattache clairement à l’Occident car elle conserve une économie de marché et peut profiter du plan Marshall américain, qui finance le tiers des investissements de la reconstruction. Elle retrouve un système parlementaire à partis multiples et elle a l’opportunité de prendre un nouveau départ basé sur une pratique interne de la cogestion tant entre partis politiques (le "Proporz") qu’entre les partenaires sociaux (la "Sozialpartnerschaft").
Le souvenir des Habsbourg demeure en Autriche un facteur identitaire qui se manifeste à travers la mise en valeur du patrimoine historique (palais et trésors impériaux notamment) ou encore à l’occasion des funérailles de l’impératrice Zita (1989) et du dernier archiduc héritier de la couronne, Otto de Habsbourg-Lorraine ().





</doc>
<doc id="16863" url="https://fr.wikipedia.org/wiki?curid=16863" title="Théophanie">
Théophanie

Une théophanie (des radicaux grecs "théo-", θεός « dieu », et "phan-", « apparition ») est, dans le domaine religieux, une manifestation divine, au cours de laquelle a normalement lieu la révélation d'un message divin aux hommes ou simplement d'un avertissement.

À l'origine, le terme grec θεοφάνια / "theophánia" désignait, dans la religion antique de ce peuple, une fête pendant laquelle on exposait publiquement la totalité des statues des dieux, surtout à Delphes. 

Avec l'avènement du christianisme, le terme conserve la signification de manifestation divine : la révélation du Buisson ardent à Moïse et la naissance de Jésus-Christ sont des théophanies essentielles de l'Ancien et du Nouveau Testament.

Le 19 janvier, l'Église orthodoxe célèbre la fête de la Théophanie : selon les Évangiles, par son baptême dans le Jourdain, le Christ s'est manifesté comme Fils de Dieu. Cette fête est le pendant orthodoxe de la fête de l'Épiphanie dans l'Église catholique.

C'est à cette occasion qu’a lieu la traditionnelle cérémonie de bénédiction de l’eau, qui se déroule dans toutes les églises russes chaque année. Les Russes se baignent alors dans l'eau glacée. On appelle cette coutume « les bains de la Théophanie » "yordan", en souvenir du baptême de Jésus-Christ dans les eaux du Jourdain. Cependant certains membres du clergé de l'Église russe, y voyant de la superstition, interdisent ces pratiques, suivant l'avis de l'archiprêtre Sergueï V. Boulgakov (in "Livre de chevet des célébrants"). Les arméniens orthodoxes célèbrent à cette date non seulement la théophanie mais aussi la fête de Noël pendant laquelle on bénit aussi les eaux à la fin de la Divine Liturgie.



</doc>
<doc id="16865" url="https://fr.wikipedia.org/wiki?curid=16865" title="Géante rouge">
Géante rouge

Une étoile géante rouge ou géante rouge est une étoile ayant évolué en dehors de la séquence principale, devenant ainsi géante.

Ce sont des étoiles d'une masse allant d'un tiers à huit fois celle du Soleil qui, après avoir épuisé l'hydrogène de leur noyau, commencent à consommer l'hydrogène en couche autour du noyau riche en hélium. 

Deux phénomènes sont responsables de l'augmentation substantielle du rayon de l'étoile (qui peut atteindre un rayon fois supérieur à celui du Soleil). Premièrement, la fusion en couche de l'hydrogène. Et deuxièmement, la contraction du cœur d'hélium, libérant une importante quantité d'énergie gravitationnelle. Ces deux sources d'énergie rayonnées vers l'extérieur induisent une pression interne qui fait augmenter le rayon de l'étoile. L'étoile est alors beaucoup plus lumineuse à cause de l'augmentation de sa surface et, contre-intuitivement, sa température en surface diminue. En résultat, l'étoile devient plus grande, plus « froide » et donc de couleur plus rouge ; d'où le nom de « géante rouge ».

Si l'étoile a une masse inférieure à 2,5 masses solaires (formula_1), l'apport d'hélium dans le noyau provenant de la fusion de l'hydrogène en couche peut provoquer ce qu'on appelle un flash de l'hélium : un sursaut rapide de la fusion de l'hélium dans le noyau quand les conditions de pression et de température sont suffisantes. La luminosité de l'étoile augmente rapidement. L'étoile commence alors une brève période de fusion d'hélium. Puis, elle commence une nouvelle montée de la branche des géantes. Les étoiles plus massives que 2,5 masses solaires entrent dans la phase de fusion de l'hélium de manière plus calme.

La phase durant laquelle une étoile pauvre en métaux consomme de l'hélium s'appelle la « branche horizontale », car dans le diagramme de Hertzsprung-Russell ces étoiles se trouvent placées sur une ligne quasi-horizontale. Les étoiles riches en métaux ne se situent pas sur la branche horizontale, mais plutôt dans un même endroit (le « clump » rouge) du diagramme de Hertzsprung-Russell.

Durant son évolution, une étoile peut se trouver plusieurs fois dans la « phase géante » si elle est suffisamment massive pour pouvoir provoquer la fusion d'éléments plus lourds que l'hélium. Dans ce cas, sur le diagramme de Hertzsprung-Russell, l'étoile se trouve sur ce qu'on appelle la branche asymptotique des géantes, ou peut même atteindre le stade Wolf-Rayet pour les étoiles initialement les plus massives.

Il est estimé que le Soleil deviendra une géante rouge d'ici environ 5,4 milliards d'années. La taille de notre étoile dépassera alors l'orbite de la Terre, le Soleil aura alors un rayon au minimum de 200 fois celui actuel.




</doc>
<doc id="16880" url="https://fr.wikipedia.org/wiki?curid=16880" title="Gérard de Nerval">
Gérard de Nerval

Gérard Labrunie, dit Gérard de Nerval, est un écrivain et un poète français, né le à Paris, ville où il est mort le . Figure majeure du romantisme français, il est essentiellement connu pour ses poèmes et ses nouvelles, notamment son ouvrage "Les Filles du feu", recueil de nouvelles (la plus célèbre étant "Sylvie") et de sonnets ("Les Chimères") publié en 1854.

Fils d'Étienne Labrunie, médecin militaire, et de Marie-Antoinette Laurent, fille d'un marchand linger de la rue Coquillière, Gérard de Nerval naît le 22 mai 1808, vers 20 heures, à Paris, au 96, rue Saint-Martin (actuellement le 168). Baptisé le 23 à Saint-Merri, il est confié quelques mois plus tard à une nourrice de Loisy, près de Mortefontaine. Son père est nommé le 8 juin suivant médecin militaire adjoint à la Grande Armée, il est rapidement promu médecin et attaché, le 22 décembre, au service de l'armée du Rhin. Le 29 novembre 1810, sa mère meurt à Głogów, en Silésie alors qu’elle accompagnait son mari. De 1808 à 1814, Gérard est élevé par son grand-oncle maternel, Antoine Boucher, à Mortefontaine, dans la campagne du Valois, à Saint-Germain-en-Laye et à Paris. Au printemps 1814, son père retrouve la vie civile et s'installe avec son fils à Paris, au 72, rue Saint-Martin. Gérard reviendra régulièrement dans ces lieux évoqués dans nombre de ses nouvelles.

En 1822, il entre au collège Charlemagne, où il a pour condisciple Théophile Gautier. C'est en classe de première (année scolaire 1823-1824) qu'il compose son premier recueil resté manuscrit de cent quarante pages : "Poésies et Poèmes par Gérard L. 1824" qu'il donne plus tard à Arsène Houssaye en 1852. Ce recueil a figuré à l'exposition Gérard de Nerval à la Maison de Balzac à Paris en 1981-1982. Il a déjà écrit, sous le nom de Gérard L. un panégyrique de Napoléon : "Napoléon ou la France guerrière, élégies nationales", publié chez Ladvocat et réédité en 1827 par Touquet. L'année suivante, il écrit deux "Épîtres à Monsieur Duponchel" caché sous le pseudonyme de "Beuglant". Dès juillet 1826, il se lance dans la satire à la suite du scandale de l'Académie française qui a préféré Charles Brifaut à Alphonse de Lamartine. Il compose alors une "Complainte sur l'immortalité de Monsieur Briffaut" (orthographe de l'auteur), puis une pièce dans le même esprit : "L'Académie ou les membres introuvables", ce qui lui valut d'être recalé au concours de l'Académie en 1828.

Le 28 novembre 1827, "le Journal de la Librairie" annonce la parution de sa traduction de "Faust" en volume in-32 qui porte le titre : "Faust, tragédie de Goethe, traduite par Gérard (1828)".

Le , pour faire plaisir à son père, Gérard accepte d'être stagiaire dans une étude de notaire. Mais il pratique le métier mollement. Il a autre chose à faire. En bon soldat du romantisme, il est convoqué par Victor Hugo pour faire partie de la "claque" de soutien à "Hernani", mission dont Gérard s'acquitte volontiers (voir Bataille d'Hernani).

1830 est l'année des deux révolutions : la révolution romantique à laquelle Gérard participe, et la révolution politique, celle des Trois Glorieuses à laquelle il ne participe qu'en badaud. La politique ne l'intéresse pas. Les barricades lui ont cependant inspiré un poème-fleuve : "Le peuple, son nom, sa gloire, sa force, sa voix, sa vertu, son repos" publié en août 1830 dans le "Mercure de France du ". Il publie encore un pamphlet : "Nos adieux à la Chambre des Députés de l'an 1830 ou, Allez-vous-en vieux mandataires, par le Père Gérard, patriote de 1789, ancien décoré de la prise de la Bastille (…)" et "En avant, marche!" publiés dans "Le Cabinet de lecture" le 4 mars 1831.

Gérard a surtout deux importants projets : une anthologie de la poésie allemande et une anthologie de la poésie française, deux ouvrages pour lesquels il lui faut une abondante documentation à laquelle il accède grâce à Alexandre Dumas et Pierre-Sébastien Laurentie qui lui font obtenir une "carte d'emprunt", ce qui lui évite de perdre du temps en bibliothèque.
La première anthologie porte le titre de "Poésies allemandes, Klopstock, Schiller et Bürger, Goethe, précédée d'une notice sur les poètes allemands par M. Gérard". L'œuvre est accueillie avec moins d'enthousiasme que "Faust", dont le compositeur Hector Berlioz s’est inspiré pour son opéra "la Damnation de Faust".

La seconde anthologie est un "Choix de poésie de Ronsard, Joachim Du Bellay, Jean-Antoine de Baïf, Guillaume de Saluste Du Bartas, Jean-Baptiste Chassignet, précédé d'une introduction par M. Gérard".

Ces deux ouvrages ne rencontrent pas un succès éclatant. Mais à l'automne 1830, le Cénacle mis en place par Sainte-Beuve pour assurer le triomphe de Victor Hugo rassemble des écrivains reconnus : Alfred de Vigny, Alfred de Musset, Charles Nodier, Alexandre Dumas, Honoré de Balzac. Les réunions ont lieu rue Notre-Dame-des-Champs, soit chez Hugo, soit chez le peintre Eugène Devéria, frère d'Achille Devéria, mais ce cénacle commence à se disperser. Apparaît un nouveau cénacle : le Petit-Cénacle, dont l'animateur est le sculpteur Jean Bernard Duseigneur qui reçoit dans son atelier, installé dans une boutique de marchand de légumes, où il retrouve Pétrus Borel et Célestin Nanteuil avant de publier "La Main de gloire" en septembre.

Mais c'est surtout à ce moment-là que Nerval a envie d'écrire des pièces de théâtre à la manière d'Hugo. Deux de ses œuvres reçoivent un très bon accueil au théâtre de l'Odéon : "Le Prince des sots" et "Lara ou l'expiation". Toutes n'ont pas le même succès mais Gérard ajoute un nom d'auteur à son prénom.

Il devient Gérard de Nerval, pseudonyme adopté en souvenir d'un lieu-dit, le clos de Nerval près de Loisy, un champ cultivé par son grand-père maternel, à cheval sur la commune de Mortefontaine.

Une des caractéristiques du Petit-Cénacle est la propension de ses membres au chahut, à la boisson, aux farces, aux jeux de mots et au "bousin" ou "bouzingo" (barouf). C'est d'ailleurs à la suite d'une de ces manifestations du groupe que les agents du guet interviennent et arrêtent trois ou quatre Jeunes-France dont Nerval fait partie avec Théophile Gautier. Enfermé à la prison de Sainte-Pélagie, Nerval écrit un petit poème aussitôt publié dans "Le Cabinet de lecture" du 4 septembre 1831. De nouveau dans la nuit du 2 février 1832, les Jeunes-France sont arrêtés, pris pour des conspirateurs, et cette fois leur peine est plus longue.

En 1833, Nestor Roqueplan lui ouvre les colonnes de son journal : "La Charte de 1830". Mais déjà un autre ami (Édouard Gorges) lui propose d'écrire avec lui un roman-feuilleton, dont l'action se déroulerait dans la Bretagne des chouans. Le vif succès remporté en 1829 par Les Chouans de Balzac fait hésiter Nerval. Pourtant, l'envie de visiter la région de Vitré l'emporte et il en revient avec un récit : "L'Auberge de Vitré" qu'il exploitera plus tard dans le prologue de son roman "Le Marquis de Fayolle", roman édité après la mort de Nerval en 1856 par Édouard Gorges, qui l'a remanié et achevé.

Il fut membre de la goguette des "Joyeux" et de la goguette des "Bergers de Syracuse".

En janvier 1834, à la mort de son grand-père maternel, il hérite d'environ . Parti à l'automne dans le Midi de la France, il passe la frontière, à l'insu de son père, et visite Florence, Rome puis Naples. En 1835, il s’installe impasse du Doyenné chez le peintre Camille Rogier, où tout un groupe de romantiques se retrouve, et fonde en mai le "Monde dramatique", revue luxueuse qui consume son héritage et que, lourdement endetté, il doit finalement vendre en 1836. Faisant alors ses débuts dans le journalisme, il part en voyage en Belgique avec Gautier, de juillet à septembre. En décembre, il signe pour la première fois « Gérard de Nerval » dans "Le Figaro".

Le 31 octobre 1837 est créé à l'Opéra-Comique "Piquillo" sur une musique de Monpou ; Dumas signe seul le livret, malgré la collaboration de Nerval ; l’actrice Jenny Colon tient le premier rôle. Nerval se serait épris de cette actrice qui n'aurait pas répondu à ses sentiments. Il fréquente alors le salon de Madame Boscary de Villeplaine, où une rivalité amoureuse l'oppose au financier William Hope pour la conquête de l'actrice.

Selon certains exégètes, il aurait voué un culte idolâtre à Jenny Colon, même après la mort de celle-ci, et elle serait la figure de la Mère perdue, mais aussi de la Femme idéale où se mêlent, dans un syncrétisme caractéristique de sa pensée, Marie, Isis, la reine de Saba, ce qui fait débat parmi les spécialistes de Nerval. Durant l'été 1838, il voyage en Allemagne avec Dumas pour préparer "Léo Burckart", pièce retardée par la censure. Après la première de "L'Alchimiste", écrite en collaboration avec Dumas, le 10 avril 1839, "Léo Burckart" est finalement créé au théâtre de la Porte-Saint-Martin le 16 avril. Dans le même temps, il publie "Le Fort de Bitche" (25-28 juin) dans "Le Messager" et "Les Deux rendez-vous" (15-17 août) – qui deviendra plus tard "Corilla" – dans "La Presse". Puis, en novembre, il part pour Vienne, où il rencontre la pianiste Marie Pleyel à l'Ambassade de France.

De retour en France en mars 1840, il remplace Gautier, alors en Espagne, pour le feuilleton dramatique de "La Presse". Après une troisième édition de "Faust", augmentée d'une préface, et de fragments du "Second Faust" en juillet, il part en octobre en Belgique. Le 15 décembre a lieu la première de "Piquillo" à Bruxelles, où il revoit Jenny Colon et Marie Pleyel.

À la suite d'une première crise de folie le 23 février 1841, il est soigné chez Marie de Sainte-Colombe, qui tient la « maison de correction Sainte-Colombe », créée en 1785 au 4-6 rue de Picpus. Le mars, Jules Janin publie un article nécrologique dans "Les Débats". Après une seconde crise, le 21 mars, il est interné dans la clinique du docteur Blanche, à Montmartre, de mars à novembre.

Le 22 décembre 1842, Nerval part pour l'Orient, passant successivement par Alexandrie, Le Caire, Beyrouth, Constantinople, Malte et Naples. De retour à Paris dans les derniers mois de 1843, il publie ses premiers articles relatifs à son voyage en 1844. En septembre et octobre, il part avec Arsène Houssaye, directeur de "L'Artiste", en Belgique et aux Pays-Bas. De juin à septembre 1845, il remplace Gautier, alors en Algérie, dans "La Presse".

Son "Voyage en Orient" paraît en 1851. Il affirme dans une lettre au docteur Blanche datée du 22 octobre 1853, avoir été initié aux mystères druzes lors de son passage en Syrie, où il aurait atteint le grade de « refit », l’un des plus élevés de cette confrérie. Toute son œuvre est fortement teintée d’ésotérisme et de symboles, notamment alchimiques. Alors qu’on l'accusait d’être impie, il s'exclama : 

Entre 1844 et 1847, Nerval voyage en Belgique, aux Pays-Bas, à Londres… et rédige des reportages et impressions de voyages. En même temps, il travaille comme nouvelliste et auteur de livrets d’opéra ainsi que comme traducteur des poèmes de son ami Heinrich Heine (recueil imprimé en 1848). Nerval vit ses dernières années dans la détresse matérielle et morale. C'est à cette période qu'il écrira ses principaux chefs-d’œuvre, réalisés pour se purger de ses émotions sur les conseils du docteur Émile Blanche pour le premier, pour la dimension cathartique du rêve et contre l'avis du docteur Blanche pour le second : "Les Filles du feu, Aurélia ou le Rêve et la Vie" (1853-1854).

Au bas d'un portrait photographique de lui, Gérard de Nerval écrivit : 

Le 26 janvier 1855, on le retrouva pendu aux barreaux d'une grille qui fermait un égout de la rue de la Vieille-Lanterne (voie aujourd'hui disparue, qui était parallèle au quai de Gesvres et aboutissait place du Châtelet, le lieu de son suicide se trouverait probablement à l'emplacement du théâtre de la Ville), pour , selon la formule de Baudelaire.

Parmi ses amis, certains comme Arsène Houssaye émirent l'hypothèse d'un assassinat perpétré par des rôdeurs, au cours d'une de ses promenades habituelles dans des lieux mal famés ; d'autres, comme Théophile Gautier ou Nadar furent convaincus qu'il s'agissait d'un suicide. Depuis lors, la question a fait l'objet de nombreux débats. Le doute subsiste car il fut retrouvé avec son bolivar sur la tête alors que celui-ci aurait normalement dû tomber du fait de l'agitation provoquée par la strangulation.

On retrouva une lettre dans laquelle il demandait , somme qui, selon lui, aurait suffi pour survivre durant l'hiver.

La cérémonie funéraire eut lieu à la cathédrale Notre-Dame de Paris, cérémonie religieuse qui lui fut accordée du fait de son état mental, malgré son suicide présumé. Théophile Gautier et Arsène Houssaye payèrent pour lui une concession au cimetière du Père-Lachaise.



Nerval a écrit deux romans :

N'ont été publiées au que sept pièces personnelles de Nerval. Les titres, voire le texte, d'autres pièces non publiées, nous sont également parvenus.

Les deux plus anciens titres sont parus sous la forme de plaquettes :

Les trois titres suivants sont issus de la collaboration entre Alexandre Dumas père et Nerval :

Nerval publia ensuite :

Il subsiste des fragments ou des indications, sous forme de manuscrit, des pièces suivantes (toutes ces pièces n'ont pas été forcément achevées) :

Des titres suivants, évoqués à certains moments par Nerval, il ne reste rien, et certains n'ont probablement jamais été écrits :

Nerval a également écrit les adaptations suivantes :






</doc>
<doc id="16890" url="https://fr.wikipedia.org/wiki?curid=16890" title="Degré (angle)">
Degré (angle)

Un degré, généralement représenté par ° (le symbole degré), est une mesure d'un angle plan, qui représente le 1/360 d'un tour complet ; un degré est aussi équivalent à /180 radians. Lorsque cet angle est en rapport avec un méridien de référence, il indique un emplacement le long d'un grand cercle d'une sphère, comme la Terre (voir Coordonnées géographiques), Mars ou la sphère céleste. Le rapport entre 365,25 (nombre de jours moyen de la rotation de la Terre autour du Soleil) et 360° (tour complet) permet d'établir l'approximation suivante : « La Terre tourne d'environ un degré autour du Soleil chaque jour ».

Le degré, divisé en minutes et secondes qui sont des soixantièmes, vient des Babyloniens, qui comptaient en base 60 (sexagésimale) à l'instar des Chinois qui, il y a plus de 4700 ans selon le calendrier chinois, utilisaient déjà 60 en fonction de leur astronomie et astrologie. Pour les Chinois, 60 correspond à un cycle temporel fondamental. Les mathématiciens persans ont poursuivi et mesuré les angles célestes et terrestres de la même manière. La mesure du temps de cette façon, directement issue des angles astronomiques, en a découlé.

Plusieurs explications ont été données sur l'origine du découpage en 360°.

Comme l'année durant laquelle la Terre fait le tour du Soleil dure 365 jours, chaque nuit les étoiles tournent d'une fraction de tour (1/365 environ) par rapport à l'axe. La mesure de temps n'étant pas nécessairement précise à ses débuts, le calendrier babylonien était basé sur une année de répartis en de , comme le montre la tablette Mul Apin. Il est possible que le degré ait été défini comme la fraction d'angle de décalage entre le ciel d'une nuit et celui de la nuit suivante, à une même heure (cf. Cosmologie), les étoiles bougeant ainsi d'environ 30° entre deux lunes successives. Cette définition devait néanmoins être approximative à près.

L'explication généralement répandue est que l’utilité originelle des 360° du système sexagésimal est de faciliter le calcul des fractions (et des multiplications). En effet, 360 étant le multiple de 1, 2, 3 et 5 il se divise par ces nombres ainsi que par leur multiples 6, 8, 9, 10, 12, 15 et toutes leurs combinaisons, ce qui simplifie la plupart des calculs et des conversions.

Finalement, du fait que 360° égale 0°, on se retrouve à calculer en modulo 360 lorsque l’on parle en degrés. On peut souvent opérer les calculs dans les modulos inférieurs que sont les multiplicateurs de 360. Au plus simple, sept demi-tours valent un demi-tour. En langage mathématique : 7 ≡ 1 (mod 2), sept est congru à un, modulo deux ; et 7 × 180° = 1260° ≡ 180° (mod 360°). En pratique, on se contente de dire . De même 120° + 270° = 390° ≡ 30° (mod 360°).

Mais la réalité sur l'origine des 360 degrés est vraisemblablement différente. La figure géométrique la plus simple qui soit n'est pas le cercle, mais le triangle équilatéral, avec ses trois côtés et ses trois angles égaux. Il semble que les Sumériens, pour définir le degré d'angle, aient pris l'angle du triangle équilatéral comme référence et qu'ils l'ont, en application de leur base sexagésimale, divisé en 60 degrés, puis le degré en 60 minutes d'angle, puis la minute en 60 secondes d'angle.

La somme des angles d'un triangle étant égale à un angle plat (ou à deux angles droits), il s'en déduit que l'angle plat, qui est donc égal à 3 angles de triangle équilatéral, vaut 60×3=180 degrés, que l'angle droit qui en est la moitié vaut 90 degrés, et que le tour complet, qui vaut deux angles plats, mesure donc 360 degrés. Le degré serait plutôt la partie d'un angle de triangle équilatéral (angle de référence) et ce ne serait qu'en conséquence de cette définition qu'un tour complet mesurerait 360 degrés.
Par ailleurs, le fait que 360 soit un nombre divisible par beaucoup de nombres entiers ne doit rien au hasard. Il le doit à l'origine même du système sexagésimal utilisé par les Sumériens, puis par les Babyloniens, basé sur une méthode de calcul sur les phalanges (qui serait encore en usage au Viêt Nam). Ces peuples comptaient, sur une main, leurs phalanges avec le pouce ; le pouce défile sur les trois phalanges des quatre autres doigts, soit douze phalanges : on compte ainsi de 1 à douze, d'où la base 12 initiale, nombre qui apparaît dans d'autres circonstances : les 12 apôtres, les 12 représentants des 12 tribus d'Israël, les 12 heures du jour et les 12 heures de la nuit, etc. Ensuite, on utilise les doigts de l'autre main pour les retenues. Le pouce, en opposition à l'un des quatre autres doigts, permet de compter de 1 à 4 douzaines. Avec les deux mains, on compte ainsi jusqu'à 5×12 = 60.

Le nombre 360 est donc le résultat de la multiplication de 3 phalanges × 4 doigts d'une main × 5 douzaines × 6 angles de référence pour un tour complet de cercle. Le fait qu'il y ait 360 degrés dans un cercle apparaît ainsi à la fois en raison du nombre important des diviseurs de 360 et comme résultat d'un calcul cohérent. Le triangle peut aussi évoquer l'astronomie dans l'Égypte antique par l'entremise de son zodiaque de Dendérah ou des multiples tombes au plafond astronomique, notamment celui de la tombe TT353 de Sénènmout qui savait qu'une journée compte 24 heures.

Le degré d’arc (symbole °) est une unité pratique d’angle plan. Un degré vaut /180 radians, 10/9 grades ou 160/9 mils, soit 1/360 d’un tour complet.

Même s'il ne s'agit pas d'une unité du Système international (SI), le degré est en usage avec lui. Les préfixes du SI sont rarement appliqués aux symboles du degré d’arc et de ses subdivisions (uniquement à la seconde d’arc, en fait) ; ces symboles sont également les seuls à ne pas être séparés du nombre les précédant par une espace : on doit écrire « » et non « ».

En astronomie de position, le degré carré est utilisé pour mesurer un angle solide sur la sphère céleste. Un degré carré vaut formula_1 stéradian.

Un degré est subdivisé en 60 minutes d’arc (symbole ′), elles-mêmes divisées en 60 secondes d’arc (symbole ″).
On utilise aussi fréquemment la notation décimale : on notera aussi bien « 12,5° » que « 12° 30′ », ou encore, « 48,59039° » que « 48°35'25,4" ». La préférence dépend ici de l'outil de calcul et/ou de mesure.

Les fonctions trigonométriques sont indépendantes de l’unité angulaire choisie. Mais en analyse, les fonctions sont définies par les valeurs prises par les fonctions pour des variables exprimées en radians.

Pour un angle de mesure , exprimée en degrés, on a donc , et de même pour les autres fonctions trigonométriques.

En astronomie ou en optique, on utilise l’approximation formula_2 pour les faibles angles (inférieurs à 5°).

Le sinus et la tangente d’un angle faible sont quasi-égaux à sa valeur en radians.



</doc>
<doc id="16892" url="https://fr.wikipedia.org/wiki?curid=16892" title="Nombre rationnel">
Nombre rationnel

Un nombre rationnel est, en mathématiques, un nombre qui peut s'exprimer comme le quotient de deux entiers relatifs. Les nombres rationnels non entiers (souvent appelés fractions) sont souvent notés formula_1, où "a" et "b" sont deux entiers relatifs (avec "b" non nul). On appelle "a" le numérateur et "b" le dénominateur.

Chaque nombre rationnel peut s'écrire d'une infinité de manières différentes, comme 1/2 = 2/4 = 3/6 = etc. Mais il existe une forme privilégiée, quand "a" et "b" n'ont pas de diviseurs communs autre que 1 (ils sont premiers entre eux). Tout nombre rationnel non nul possède exactement une seule forme de ce type avec un dénominateur positif. On parle alors de fraction irréductible.

Le développement décimal d'un nombre rationnel est toujours périodique au bout d'une certaine décimale (par exemple dans le cas d'une écriture décimale finie, le rajout de zéros assure la périodicité). Cela est vrai dans n'importe quelle base. Réciproquement, si un nombre possède un développement décimal périodique dans au moins une base, alors c'est un nombre rationnel. 

Un nombre réel qui n'est pas rationnel est dit irrationnel. L'ensemble des nombres rationnels est un corps commutatif, noté Q ou ℚ (baptisé ainsi par Peano en 1895 d'après l'initiale du mot italien "quoziente", le quotient). De par sa définition :
où ℤ est l'anneau des entiers relatifs.

Comme tous les réels, les rationnels admettent une représentation en développement décimal illimité. Le développement décimal des nombres rationnels a la particularité d'être périodique. C'est-à-dire qu'il existe un suffixe constitué d'une séquence finie de chiffres se répétant continuellement. Cette séquence est appelée : « période du développement décimal illimité ».

Le développement décimal illimité d'un nombre réel, et "a fortiori" d'un nombre rationnel, est unique si on s'interdit de finir par une séquence périodique composée de ’9’. En effet, dans ce dernier cas, il existera une écriture équivalente se terminant par une période composée de ’0’, et mieux encore, un développement décimal limité équivalent.

Conventionnellement, lorsque nous écrivons un nombre avec les chiffres arabes dans le système décimal nous traçons, s'il y a lieu, une barre horizontale au-dessous de la séquence périodique. Il est aussi possible de mettre un point au-dessus de chaque chiffre de la période, mais cette notation est beaucoup moins utilisée.

Lorsqu'une période est indiquée nous devons faire référence à un nombre rationnel et c'est pour cette raison que d'une manière rigoureuse :

Le développement décimal illimité d'un nombre rationnel est périodique et, réciproquement, un nombre à développement décimal périodique est toujours rationnel. Ce critère est néanmoins mal commode pour évaluer la rationalité d'un nombre. Un deuxième critère est donnée par la fraction continue. Un nombre est rationnel si et seulement si son développement en fraction continue est fini. Cette méthode est à l'origine des premières démonstrations de l'irrationalité de la base du logarithme népérien et de .

Ainsi, le nombre formula_5 (où l'on a des séquences de ’2’ de plus en plus longues) est irrationnel car il n'y a pas de période.

Soient "a, b, c, d" quatre entiers, avec "b" et "d" non nuls.

Les deux nombres rationnels représentés par "a"/"b" et "c"/"d" sont égaux si et seulement si "ad" = "bc".

L'addition est donnée par : 

On démontre que cette égalité ne dépend pas du choix des représentants "a/b" et "c/d".

La multiplication par :

L'opposé et l'inverse par :

On en déduit que le quotient est donné par :

Tout nombre rationnel positif peut s'exprimer comme somme d'inverses distincts d'entiers naturels distincts. Par exemple, on a :

On peut voir un nombre rationnel comme la classe d'équivalence d'une paire ordonnée d'entiers, par la relation d'équivalence suivante :

On note alors formula_12, c'est-à-dire que l'ensemble des nombres rationnels est le quotient de formula_13 par la relation d'équivalence.

On peut ensuite injecter les entiers dans les rationnels, et définir des lois de composition interne pour se donner une structure de corps.

Cette construction est valable à partir de n'importe quel anneau intègre, on parle alors de corps des fractions.


Muni de la topologie de l'ordre usuel, ℚ est un corps topologique. Cela signifie que les opérations arithmétiques sont continues. L'addition est de plus compatible avec l'ordre (on parle de groupe ordonné).

Par contre, ℚ ne possède pas la propriété de la borne supérieure : l'ensemble des nombres rationnels "x" tels que est majoré mais ne possède pas de plus petit majorant.

D'autre part, ℚ n'est pas un espace complet : il existe des suites de Cauchy de nombres rationnels qui ne convergent pas vers un nombre rationnel, comme la suite ("x") définie par récurrence suivant la méthode de Héron :

Ces deux limitations montrent notamment que des nombres essentiels en mathématiques, comme ou , ne sont pas rationnels. Cela conduit à compléter ℚ en construisant un ensemble plus grand, qui possède la propriété de la borne supérieure et dans lequel toute suite de Cauchy converge : l'ensemble des nombres réels.

On peut munir ℚ d'une autre métrique. 

Soit formula_18 un nombre premier. On pose :
La fonction ainsi définie est complètement multiplicative, ce qui permet de poser sans ambiguïté, pour tout nombre rationnel formula_25 :
Alors formula_27 définit un espace métrique. 

L'espace métrique formula_28 n'est pas complet, et sa complétion est le corps ℚ des nombres "p"-adiques. Le théorème d'Ostrowski montre que toute valeur absolue non triviale sur ℚ est topologiquement équivalente soit à la valeur absolue usuelle, soit à une valeur absolue "p"-adique.

Arbre de Stern-Brocot


</doc>
<doc id="16895" url="https://fr.wikipedia.org/wiki?curid=16895" title="Quintana Roo">
Quintana Roo

L'État de Quintana Roo est un État situé au sud du Mexique dans la péninsule du Yucatán.
Entouré par les États du Yucatán et de Campeche, la mer des Caraïbes et le Belize, Quintana Roo occupe une superficie de . La capitale est Chetumal. Quintana Roo comptait habitants en 2015. Autrefois, la région était habitée par les Mayas comme l'attestent de nombreuses ruines.
Jusqu'en 1974, Quintana Roo a eu le statut de Territoire de la fédération. C'est en cette année qu'il devint le État du Mexique.

La région est peuplée dès l'Antiquité au moins, comme en attestent des sites archéologiques tels que la cité maya de Tulum.

En 1867, le Yucatan connaît de graves troubles avec la guerre des castes.

Le territoire actuel de Quintana Roo devient un territoire du Mexique par décret du président Porfirio Díaz en 1902. Ce territoire est ainsi nommé en hommage à Andrés Quintana Roo, indépendantiste mexicain et législateur ayant fait partie des assemblées constituantes du Mexique.

Quintana Roo accède au statut d'État parmi les États-Unis du Mexique le 8 octobre 1974.

L'État de Quintana Roo est divisé en dix municipalités. À leur création en 1974, elles étaient au nombre de sept, mais de nouvelles municipalités ont été créées depuis : Solidaridad en 1993, Tulum en 2008, Bacalar en 2011. 

Quintana Roo est un des territoires principaux occupés par les Mayas à l'époque précolombienne. C'est donc pour cela que l'on peut retrouver de nombreux temples de ce peuple dont Chacchoben, Chakalal, Coba, Cozumel, Tulum et Xel-Ha .

L'État de Quintana Roo est situé à l’extrémité orientale du Mexique, situé au sud, cet État est frontalier des États du Yucatan au nord-ouest, de Mérida à l'ouest, ainsi que du Belize au sud. Quintana Roo a également accès au golfe du Mexique et à la mer des Caraïbes.

Le climat que connaît Quintana Roo est un climat tropical sec ou humide selon la saison.



</doc>
<doc id="16896" url="https://fr.wikipedia.org/wiki?curid=16896" title="Grade">
Grade

Le mot grade a plusieurs significations.






</doc>
<doc id="16901" url="https://fr.wikipedia.org/wiki?curid=16901" title="Insecte">
Insecte

Voir l'article détaillé
Les insectes (Insecta) sont une classe d'animaux invertébrés de l'embranchement des arthropodes et du sous-embranchement des hexapodes. Ils sont caractérisés par un corps segmenté en trois tagmes (tête possédant des pièces buccales externes, une paire d'antennes et au moins une paire d'yeux composés ; thorax pourvu de trois paires de pattes articulées et deux paires d'ailes plus ou moins modifiées ; abdomen dépourvu d'appendices) protégés par une cuticule formant un exosquelette composé de chitine et pourvu de trachées respiratoires.

Avec près de 1,3 million d'espèces décrites existant encore (et près de nouvelles espèces inventoriées par an), les insectes constituent la plus grande part de la biodiversité animale (définie par le nombre d'espèces). On estime à entre 5 et 80 millions d'espèces possibles, ce qui représenterait plus de 80 % des différentes formes de vie animale. Leur biomasse totale serait 300 fois plus importante que la biomasse humaine, quatre fois supérieure à celle des vertébrés, sachant que les insectes sociaux représentent à eux seuls la moitié de la biomasse des insectes.

Ils sont apparus il y a plus de 400 millions d'années et ils sont les plus anciens animaux à s'être adaptés à la vie terrestre, devenant amphibies. Ils sont également les premiers animaux complexes à avoir développé la capacité de voler pour se déplacer, étant les seuls à posséder ce moyen de locomotion pendant 150 millions d'années. Pourvus d'ailes, d'une petite taille et d'un stade nymphal de la métamorphose, ces caractéristiques favorisant la colonisation de nombreuses niches écologiques expliquent leur réussite évolutive. On les trouve maintenant sous presque tous les climats et dans les milieux continentaux terrestres et aquatiques. Quelques espèces se sont même adaptées à la vie aquatique en eau salée, un habitat majoritairement dominé par le groupe des crustacés.

Ils ont de nombreuses interactions avec les humains. Certains insectes entrent en compétition directe pour nos ressources comme les ravageurs en agriculture et en exploitation forestière (sylviculture). D'autres peuvent causer des problèmes de santé majeurs en tant que vecteurs de pathogènes et de maladies infectieuses graves. À l'opposé, beaucoup d'insectes sont considérés comme écologiquement bénéfiques en tant que prédateurs, pollinisateurs, producteur de commodités (miel, soie, etc.), détritivores, ou encore en tant que source de nourriture pour de nombreuses espèces animales et chez l'Homme.

Le cycle de vie des insectes passe par plusieurs stades de transformations physiques appelés et implique généralement plusieurs métamorphoses.

Les araignées, scorpions et acariens ne sont pas des insectes, mais des arachnides ; entre autres différences, ils ont huit pattes.

L'entomologie est la branche de la zoologie dont l'objet est l'étude des insectes.

Le mot insecte vient du latin ' qui signifie « en plusieurs parties » qui réfère à la segmentation des trois parties principales. L'étymologie latine est un calque du grec ' (éntomos) signifiant « incisé, entaillé ».

Au sein des arthropodes, les insectes ont traditionnellement été rapprochés des myriapodes sur la base de plusieurs caractères : appendices uniramés, présence de trachées et de tubes de Malpighi, mandibules formées d'un appendice complet (et non pas de la base d'un appendice comme chez les crustacés). Cependant, la phylogénie moléculaire, l'arrangement des gènes mitochondriaux , ainsi que l'analyse cladistique des caractères ont conduit à considérer que les insectes devaient en fait être inclus au sein des crustacés (au Moyen Âge, ils étaient classés dans les "vermes", « vers » comprenant aussi les petits rongeurs et les mollusques). Le clade des pancrustacés établi à la suite de cette découverte contient donc les lignées de crustacés marins qui sont probablement paraphylétiques et les insectes proprement dits, qui sont monophylétiques. Les caractères ayant conduit au rapprochement des insectes avec les myriapodes sont donc probablement des convergences associées à l'adaptation au milieu terrestre. Le développement du système nerveux des insectes et des crustacés possède en revanche des similitudes extrêmement frappantes.

La classification des insectes a été proposée par Carl von Linné au sur la base de critères morphologiques propres aux insectes. Ainsi, une trentaine d'ordres d'insectes actuels est recensée sur l'ensemble de la planète. Leur classification n'est pas encore stabilisée, quelques groupes établis par la tradition se révélant récemment hétérogènes. Le sous-embranchement des hexapodes Hexapoda est donc un concept plus vaste que celui des insectes lequel, au sens strict, constitue un groupe frère des entognathes Entognatha.

D'après Roth (1974), la classe des Insectes est subdivisée en deux sous-classes :

D'après Brusca & Brusca (2003) et d'après Ruggiero (2015), incluant Brusca, expert pour ITIS, la classe Insecta comprend trois sous-classes :

Cette sous-classe regroupe des insectes primitifs aptères. On y retrouve peu de diversité et ils sont classés en deux groupes qui sont traités comme des ordres : Archaeognatha et Zygentoma.

Cette sous-classe regroupe les insectes «ailés» ou ptérygotes. Ce groupe représente la lignée principale de la majorité des insectes. Ils se sont abondamment diversifiés depuis leur apparition il y a environ 350 millions d'années (Carbonifère). La classification actuelle sépare les ptérygotes en plus de 25 ordres différents.

Pterygota (ptérygotes) d'après :

Avec près de 1,3 million d'espèces décrites, les insectes représentent plus des deux tiers de tous les organismes vivants. Dans cette classe, quatre ordres dominent dans le nombre d'espèces décrites. Entre et espèces sont incluses dans l'ordre des coléoptères, des diptères, des hyménoptères et des lépidoptères. Les coléoptères représentent 40 % des espèces d'insectes, mais certains entomologistes suggèrent que les mouches et les hyménoptères pourraient être aussi diversifiés.

Ils sont la classe d'organismes vivants la plus diversifiée en terme du nombre d'espèces et par ce fait, ils sont majoritairement dominants dans les milieux terrestres et aquatiques. Cette biodiversité est un facteur important pour la conservation de la nature, l'intégrité de l'environnement et le potentiel invasif de certaines espèces généralistes.

L'état des populations mondiales d'insecte est très mal connu, notamment dans les forêts tropicales et équatoriales.
On sait cependant que beaucoup d'espèces semblent avoir disparu ou sont en forte voie de régression (insectes saproxylophages par exemple dans les zones tempérées).
De manière générale l'ONU a identifié de grandes causes de régression de la biodiversité qui sont les modifications des habitats des espèces (destruction, banalisation, fragmentation, artificialisation, déforestation, drainage, mise en culture, etc.) ; la surexploitation ; la pollution ; l'introduction d'espèces exotiques envahissantes ; et les changements climatiques.

Concernant le dérèglement climatique, on mesure mal les impacts qu'il aura sur les insectes, le taux de pullulation et le caractère invasif (éventuel ou avéré) de certaines espèces ; Il existe un écart entre les évaluations de vulnérabilité des espèces et les stratégies de gestion conservatoire (bien qu'il y ait un consensus sur l'importance de lier ces deux domaines pour la conservation de la biodiversité). Une étude récente (2012) a cherché à étudier la vulnérabilité de 3 espèces de coléoptères aquatiques ibériques endémiques en trois colonisations indépendants d'un même habitat, sur la base de leur métabolisme et physiologie selon la température, des modèles de distribution et de capacité de dispersion. La gestion doit prendre en compte les capacités différentielles à persister et les gammes possibles de réponse au réchauffement. Dans ce cas l'étude a conclu que ces 3 espèces seront affectées très différemment par le réchauffement malgré des traits écologiques et biogéographiques assez similaires

Un groupe d'experts, appartenant principalement à des organismes de recherches publiques d'une quinzaine de pays, a synthétisé les publications de ces dernières années sur le thème du comptage des populations d'insectes, et a conclu au « déclin massif des insectes » depuis les années 1990, ce qui serait dû à l'utilisation et la persistance de pesticides systémiques.

En France métropolitaine, le calcul du nombre d'espèces, basé sur des estimations statistiques, évalue la faune entomologique actuellement connue à (décrites pour la plupart uniquement par la forme adulte), et la faune entomologique totale à . Il reste donc près de à découvrir.

Comme tous arthropodes, les insectes ont un corps segmenté soutenu par un exosquelette qui est composé d'une cuticule chitineuse. Les segments du corps sont organisés en trois parties principales qui sont la tête, le thorax et labdomen. La tête possède une paire d'antennes, une paire d'yeux composés, des ocelles et trois ensembles d'appendices modifiés qui forment les pièces buccales. Ces appendices se sont spécialisés avec l'évolution, si bien que maintenant on en retrouve plusieurs types (broyeur, suceur, suceur-piqueur, suceur-spongieur et suceur-lécheur).

Le thorax est composé de trois segments (prothorax, mésothorax et le métathorax) et porte généralement tous les organes locomoteurs (ailes ou pattes). L'abdomen est composé la plupart du temps de onze segments qui peuvent parfois porter des appendices tels des cerques par exemple. À l'intérieur, il contient une partie des organes importants comme l'appareil digestif, le système respiratoire, le système excréteur et les organes reproducteurs. On retrouve une grande variabilité et de nombreuses adaptations dans la composition des parties du corps de l'insecte, en particulier les ailes, les pattes, les antennes et les pièces buccales.

La respiration de l'insecte se fait grâce à des invaginations du tégument appelées trachées qui constituent un réseau apportant l'oxygène directement aux cellules. Ces trachées s'ouvrent sur l'extérieur par des stigmates respiratoires à ouverture variable, sur les côtés des segments (pleurites) thoraciques et abdominaux. L'appareil circulatoire n'a donc pas ou peu de rôle pour la respiration (à quelques exceptions près comme les larves de chironome — diptère vivant dans des milieux très faiblement oxygénés — qui possèdent de l'hémoglobine).

Le milieu intérieur est constitué d'hémolymphe qui est mis en mouvement par des vaisseaux contractiles dorsaux et les mouvements musculaires généraux de l'insecte. L'appareil circulatoire est ouvert, à faible pression.

L'insecte utilise son système digestif pour extraire des nutriments et d'autres substances à partir de la nourriture qu'il consomme. Ces aliments sont généralement ingérés sous forme de macromolécules complexes composées de protéines, polysaccharides, lipides et d'acides nucléiques. Ces macromolécules doivent être ventilées par des réactions cataboliques pour devenir des molécules plus petites comme des acides aminés et des molécules de sucre simple. De cette manière, les cellules peuvent les assimiler.

L'appareil digestif est constitué d'un long tube clos appelé le canal alimentaire et celui-ci s'étend longitudinalement à travers le corps. Ce tube digestif dirige unidirectionnellement la nourriture de la bouche à l'anus. Il est divisé en trois parties : stomodeum (intestin antérieur), mésentéron (intestin moyen) et proctodeum (intestin postérieur). Le stomodeum et le proctodeum sont recouverts de cuticule puisqu'ils sont issus d'invaginations du tégument. En plus du tube digestif, les insectes ont également des glandes salivaires et des réservoirs salivaires. Ces structures se retrouvent dans le thorax, à côté de l'intestin antérieur.

Le système nerveux central est constitué d'une double chaîne ganglionnaire ventrale, dont les ganglions les plus massifs sont antérieurs et forment le cerveau situé dans la cavité de l'exosquelette de la tête. Les trois premières paires de ganglions sont fusionnés dans le cerveau, tandis que les trois paires suivantes fusionnent pour former un ganglion sous-œsophagien qui innerve les pièces buccales.

Les segments thoraciques ont un ganglion placé de chaque côté du corps, donc une paire par segment. Cette disposition est également présente dans les huit premiers segments abdominaux. Cette constitution peut varier, certaines blattes (blattaria) ont seulement six ganglions abdominaux. La mouche domestique ("Musca domestica") a tous les ganglions fusionnés en un seul et celui-ci se retrouve dans le thorax.

Quelques insectes ont des nocicepteurs, des cellules qui détectent et transmettent des sensations de douleur. Bien que la nociception ait été démontré chez les insectes, il n'y a pas de consensus sur leurs degrés de conscience à la douleur.

Les mâles sont typiquement munis d'un organe phallique ou pénis qui comprend une pièce basale, le phallobase, un édéage (organe d'intromission) distal et des appendices latéro-apicaux, les paramères, qui prennent naissance sur le phallobase. L'oviscapte ou ovipositeur est l'appendice abdominal, généralement long et effilé, à l'aide duquel de nombreuses femelles d'insectes évolués déposent leurs œufs dans les endroits les plus favorables à leur incubation.

Le cycle de vie des insectes passe par plusieurs stades de transformations physiques appelés et implique généralement plusieurs métamorphoses. Ce cycle évolutif est une série de stades (œuf, larve, nymphe, adulte) qui se succèdent au cours d'une génération complète, les insectes étant caractérisés par le stade nymphal de la métamorphose. Ce cycle peut être interrompu annuellement par des conditions climatiques défavorables (température, pluie, manque de nourriture, etc.). La diapause est le terme qui réfère à cet arrêt prolongé au cours du cycle de vie de l'insecte.

Les insectes primitifs de la sous-classe des Apterygota ont un développement dit sans métamorphose ou amétabole. Dès la naissance, le jeune insecte est très semblable à l'adulte, à la taille près (« amétabole » équivaut à « sans changement »). Du côté des insectes ptérygotes, on retrouve deux types de transformations : hémimétaboles (hétérométaboles) et holométaboles.

Le développement est contrôlé par une hormone stéroïde, l'ecdysone, qui est produite dans des glandes prothoraciques et permet la mue. Une autre hormone, l'hormone juvénile, un dérivé terpénoïde, inhibe la métamorphose. Elle est produite dans les corps allates, des organes endocrines près de l'œsophage.

La reproduction des insectes est également contrôlée par l’ecdysone et l’hormone juvénile, qui agissent dans les deux sexes. Ces hormones contrôlent le fonctionnement de l'appareil reproducteur, mais n'influent pas sur la détermination des caractères sexuels, qui sont strictement déterminés de manière génétique. Les hormones de type phéromones jouent aussi un rôle majeur pour l'attraction et la reconnaissance des individus au sein d'une espèce.

Ce type de développement est composé de trois étapes principales : l'œuf, la nymphe (ou larve) et l'adulte (il n'y a pas de stade pupal). La nymphe est similaire à l'adulte. Elle est cependant plus petite, ses ailes ne sont pas développées complètement et ses organes sexuels ne sont pas fonctionnels. Au cours de sa croissance, la nymphe ressemblera de plus en plus à l'adulte et ses ailes se déploieront à sa dernière mue.

On retrouve deux sous-divisions à ce type de métamorphose :

Cette transformation est typique des insectes endoptéygotes et de certains exoptérygotes (exemple: thrips et aleurodes). Ce type de développement est composé de quatre étapes principales : l'œuf, la nymphe (ou larve), la chrysalide (pupe) et l'adulte. Le stade larvaire ne ressemble pas à l'adulte. La larve ne présente aucun signe extérieur du développement de ses ailes. La métamorphose en adulte est concentrée au stade nymphal (pupe).

La reproduction des insectes présente de grande variabilité. Ceux-ci ont un temps de génération relativement court et un taux de reproduction très élevé comparativement aux autres espèces animales. Chez les insectes, on retrouve la reproduction sexuée et la reproduction asexuée. Dans la première, le mâle et la femelle se rencontrent, souvent par l'intermédiaire de phéromones ou d'autres moyens de communication, pour copuler. L'issue de cette reproduction est un embryon résultant de la fusion de l'œuf et du spermatozoïde. Il s'agit d'un mode de reproduction qui est le plus commun chez les insectes. Dans la reproduction asexuée, la femelle est capable de se reproduire sans mâle par le développement des ovocytes en embryons (parthénogénèse). Ce type de reproduction a été décrit dans plusieurs ordres d'insectes.

De plus, la grande majorité des femelles sont ovipares; ainsi elle dépose ses petits sous forme d'œufs. Certains cafards, pucerons et mouches pratiquent l'ovoviviparité. Ces insectes incubent les œufs à l'intérieur de leur abdomen et les pondent au moment de l'éclosion. D'autres insectes sont vivipares et ils complètent leur développement à l'intérieur de l'abdomen de la mère. 

Les comportements de reproduction chez les insectes peuvent être très diversifiés. Pendant la période de reproduction, la communication se réalise principalement par la sécrétion de phéromones. À l'aide de ses antennes, le mâle peut donc trouver l'emplacement d'une femelle réceptive. Les phéromones sont propres à chaque espèce et elles sont constituées de différentes molécules chimiques.

Une autre technique de communication est l'utilisation de la bioluminescence. On retrouve ce type d'appel chez les coléoptères de la famille des Lampyridae et des Phengodidae. Les individus de ces familles produisent de la lumière qui est fabriquée par des organes à l'intérieur de leur abdomen. Les mâles et les femelles communiquent de cette manière durant la période de reproduction. Les signaux sont différents d'une espèce à l'autre (dans la durée, la composition, la chorégraphie aérienne et l'intensité.
Plusieurs insectes élaborent des chants d'appel pour signaler leur présence au sexe opposé. Ces sons peuvent être créés par la vibration des ailes, par la friction des pattes ou par le contact avec le sol, un substrat, etc. Les orthoptères (criquets, sauterelles et grillons), certaines espèces de mouches (drosophiles, moustiques, etc.), les homoptères (comme les cigales), certains coléoptères (comme les tenebrionidae) et bien d'autres sont adeptes de cette technique.

Chez certains groupes, les mâles pratiquent des prouesses aériennes ou des pas de danses complexes pour attirer une partenaire. Certains odonates et certaines mouches courtisent de cette manière.

Les mâles de certaines espèces d'invertébrés (comme les Mécoptères et les mouches Empididae) offrent des cadeaux dans le but de s'attirer les bonnes faveurs d'une femelle. Ils capturent une proie pour ensuite s'approcher d'une femelle. Par message chimique (émission de phéromones), ils indiquent à la femelle leur intention et lui offre le présent. Celle-ci examinera soigneusement la proie. Si elle ne trouve pas le repas à son goût, elle refusera les avances du mâle. Dans le cas contraire, elle s'accouplera avec lui.

La compétition entre mâles est féroce et beaucoup affichent des comportements territoriaux et agressifs. Ils sont prêts à se battre pour conserver un petit territoire ou avoir la chance de se reproduire avec une femelle. Chez certaines espèces, les mâles possèdent des cornes et des protubérances sur leur tête ou leur thorax. Ces ornements servent à combattre d'autres mâles de la même espèce.

L'écologie des insectes est l'étude scientifique des interactions des insectes, individuellement ou en tant que communauté, avec leur environnement ou avec les écosystèmes environnants. Les insectes jouent un rôle des plus importants dans les écosystèmes. Premièrement, ils permettent l'aération du sol et le brassage de la matière organique qui s'y retrouve. Ils entrent également dans la chaîne alimentaire en tant que proies et prédateurs. De plus, ils sont d'importants pollinisateurs et de nombreuses plantes dépendent des insectes pour se reproduire. Finalement, ils recyclent la matière organique en s'alimentant des excréments, des carcasses d'animaux et des plantes mortes, et la rendent ainsi disponible pour d'autres organismes. D'ailleurs, ils sont responsables en grande partie de la création des terres arables. Les insectes sont inféodés aux terres émergées. Quelques-uns vivent en eau douce et à de rares exceptions en mer. On les trouve sous presque tous les climats, du plus chaud au plus froid.

en cours
Les insectes jouent un rôle important dans leur écosystème et ils exploitent une grande diversité de ressources alimentaires. Certains sont herbivores (phytophage) et ils se nourrissent des plantes et des arbres. Le groupe des phytophages inclut les insectes qui s'alimentent des racines, de la tige, des feuilles, des fleurs et des fruits. Les mangeurs de feuilles peuvent se nourrir des tissus extérieurs ou encore être spécialisés à un type précis de cellule végétale. On retrouve aussi des insectes à l'alimentation spécifique qui se nourrissent d'un seul genre ou d'une espèce de plante. D'autres sont très généralistes et peuvent s'alimenter de plusieurs types de plantes. Au sein des différents groupes, on retrouve une grande proportion d'espèces phytophages dans l'ordre des lépidoptères, des orthoptères, des phasmoptères, des hémiptères et des thysanoptères. Chez les papillons (lépidoptère) ce sont les larves qui sont essentiellement phytophages. Chez les adultes, les pièces buccales ont évolué en une trompe multi segmentée qu'on appelle le proboscis. Au repos, ce tube est enroulé sous la tête. Les papillons se nourrissent du nectar des fleurs, des sels minéraux et des nutriments contenus dans d'autres liquides. On retrouve également des papillons qui n'ont pas de pièces buccales et qui vivent essentiellement sur leur réserve de graisse.

Certains insectes se sont spécialisés dans leur alimentation phytophage. Par exemple, ils s'alimentent uniquement de bois. Ce type d'alimentation se nomme xylophagie. Les insectes xylophages, à l'état larvaire ou adulte, s'alimentent des branches, du tronc ou encore des racines des arbres. Certains peuvent devenir des ravageurs et causer des dommages économiques en s'alimentant des arbres ou en véhiculant des pathogènes qui peuvent affecter la qualité et la santé des arbres. Les saproxylophages, quant à eux, ne consomment que le bois en décomposition (arbre mort).

Au sein de cette classe, on retrouve également des insectes prédateurs qui sont principalement carnivores. Ils ont généralement des adaptations physiologiques qui leur permettent de chasser activement (vision spécialisée, pattes adaptées à la course ou à saisir, pièces buccales modifiées pour broyer ou aggriper, etc.) ou à l'affût (camouflage). Ces prédateurs sont utiles pour réguler les populations d'invertébrés et ainsi préserver un certain équilibre dans l'écosystème. D'ailleurs, certains sont utilisés dans le contrôle des ravageurs (lutte biologique). La plupart des insectes prédateurs sont généralistes mais quelques espèces ont une préférence pour des proies plus spécifiques.

L'ordre des odonates (libellules et demoiselles) est essentiellement carnivore. Toutes les espèces, la larve et l'adulte, chassent d'autres animaux. Les adultes attrapent généralement des insectes volants tandis que les larves interceptent un large éventail d'invertébrés aquatiques et même de petits vertébrés (têtards ou petits poissons). Ils possèdent la meilleure vision dans le monde des insectes et ils sont également d'excellents pilotes aériens.Un autre ordre majoritairement carnivore et celui des Mantodea. Les mantes possèdent une très bonne vision, des pattes raptoriales adaptées à la capture et au maintien de leur proie et souvent un camouflage qui leur permet de se fondre dans leur habitat. Au stade adulte, leur régime alimentaire se compose essentiellement d'insectes mais les grandes espèces peuvent s'attaquer à de petits scorpions, des centipèdes, des araignées, des lézards, des grenouilles, des souris et même des oiseaux.

Certains insectes se sont spécialisés dans leur alimentation carnée. Par exemple, les hématophages se nourrissent de sang. Ces organismes sont souvent des ectoparasites (parasites qui n'entrent pas à l'intérieur de leur hôte, mais qui se fixent provisoirement sur sa peau). Leurs pièces buccales ont évolué en parties capables de percer la peau et d'aspirer le sang. Chez certains groupes, comme les Siphonaptera (puces) et les Phthiraptera (poux), les pièces buccales se sont adaptées pour mieux s'ancrer à l'hôte.

Les insectes détritivores se nourrissent des débris d'animaux (carcasses et excréments), de végétaux ou fongiques. En s'alimentant, ils recyclent les composés organiques contenus dans ses détritus et les rendent disponibles pour d'autres organismes. Ils ont une importance primordiale dans la structuration et la santé des sols.

Dans cette catégorie, on retrouve les insectes coprophages qui s'alimentent des excréments et les recyclent par le fait même. Ce sont, pour la plupart des insectes de l'ordre des coléoptères ou des diptères. Ces insectes peuvent être spécifiques aux excréments d'un animal ou généralistes. Le bousier ou encore la mouche verte sont de bons exemples d'insectes coprophages. Lorsque la matière en décomposition est issue d'un cadavre, on parlera plutôt de nécrophagie. Les insectes nécrophages peuvent être spécifiques à un stade de décomposition ou présents dans l'entièreté du processus. D'ailleurs, ils sont utilisés en médecine légale (entomologie médico-légale) pour établir les circonstances d'un décès (détermination de l'heure du décès, du mouvement du corps après la mort, de la présence de traumatismes, présence de drogues ou autres toxines dans l'organisme, etc.). Les insectes saproxylophages se retrouvent également dans la catégorie des détritivores. Dans les écosystèmes forestiers, ils jouent un rôle majeur en contribuant au cycle du carbone et au recyclage de la nécromasse végétale ligneuse qu'ils transforment en un humus forestier particulièrement riche et apte à absorber l'eau.

Certains insectes possèdent une coloration ou une forme qui leur permettent de se fondre dans leur environnement. Le camouflage est répandu dans plusieurs groupes d'insectes, en particulier ceux qui se nourrissent de bois ou de végétation. Certains ont la coloration et la texture du substrat dans lequel ils vivent. La plupart des phasmes sont connus pour imiter efficacement les formes des branches et des feuilles. Certaines espèces ont même des excroissances qui ressemblent à de la mousse ou encore à du lichen. On retrouve également de fins imitateurs, chez les phasmes et les mantes, qui bougent leur corps de manière rythmée pour mieux se fondre dans la végétation qui bouge au gré du vent.

Certaines espèces ressemblent à s'y méprendre à une guêpe ou à insecte toxique. Cette technique de défense se nomme mimétisme batésien. Ils peuvent jumeler leur coloration aux comportements de l'insecte imité et ainsi bénéficier d'une protection contre les prédateurs. Certains longicornes (Cerambycidae), mouche syrphide (Syrphidae), chrysomèles (Chrysomelidae) et certains papillons pratiquent ce type de mimétisme. De nombreuses espèces d'insectes sécrètent des substances désagréables ou toxiques pour se défendre. Ces mêmes espèces présentent souvent de l'aposématisme, une stratégie adaptative qui envoie par une coloration vive ou contrastante un message d'avertissement.

 La pollinisation est le processus par lequel le pollen est transféré vers le pistil (organe femelle) de la fleur soit par autofécondation, soit par fécondation croisée. La plupart des plantes à fleurs ont besoin d'un intermédiaire pour se reproduire et cette tâche est réalisée majoritairement par les insectes. En butinant, ils ont accès au nectar, un liquide sucré riche et énergisant. Pour y avoir accès, ils entrent en contact avec le pollen qui se dépose sur leur corps. Le pollinisateur transportera ensuite celui-ci vers une autre fleur, un bel exemple de relation de mutualisme. Les fleurs arborent différents motifs et colorations pour attirer ces insectes. Le nombre et la diversité des pollinisateurs influent fortement sur la biodiversité végétale et inversement (voir ), et la perte de diversité chez les pollinisateurs pourrait menacer la pérennité des communautés végétales

L'abeille domestique est certainement l'insecte pollinisateur le plus populaire en agriculture mais des milliers d’espèces différentes d’abeilles sauvages, de guêpes, de mouches, de papillons et d'autres insectes jouent également un rôle important dans la pollinisation. En agriculture, ils sont d'une importance primordiale pour la production de nombreuses cultures (pommes, oranges, citrons, brocolis, bleuets, cerises, amandes, etc.). Le domaine scientifique qui étudie les insectes pollinisateurs se nomme anthécologie.

Certains insectes ont besoin d'une autre espèce d'insecte pour réaliser leur développement. On appelle "« parasitoïdes »" les organismes qui, au cours de leur développement, tuent systématiquement leur hôte, ce qui les fait sortir du cadre du parasitisme au sens strict. Chez ces insectes, on retrouve une spécificité vis-à-vis de l'insecte hôte. Ils peuvent se nourrir à l'intérieur de l'organisme (endoparasitoïdes) ou à l'extérieur du corps de l'hôte (ectoparasitoïdes). Ils peuvent être solitaires ou grégaires (plus d'une centaine de larves sur le même hôte). Certaines guêpes et mouches parasitoïdes sont utilisées en lutte biologique.

Lors de l'oviposition, la femelle parasitoïde s'approche de son hôte et lui pénètre l'exosquelette à l'aide de son ovipositeur modifié. Elle déposera ses œufs à l'intérieur de celui-ci. Une autre technique consiste à déposer les œufs sur l'insecte ou à proximité de celui-ci. Les larves pénétreront la larve hôte par les orifices buccaux et respiratoires ou encore en perçant directement sa peau.

Au milieu des années 1990, on avait déjà répertorié et nommé 87 000 espèces d'insectes parasitoïdes, classés dans six ordres :

Les insectes sont parfois distingués en insectes « ravageurs » (ou « nuisibles ») et insectes « bénéfiques », bien que cette distinction soit relative et fortement associée à un modèle d'agriculture productiviste (les ravageurs peuvent en effet être bénéfiques dans certains cas, et participent au biotope et à la biodiversité).

Diverses cultures ont intégré depuis longtemps la consommation d'insectes. Récemment, plusieurs initiatives mondiales essaient de promouvoir cette consommation comme alternative à la consommation de viande afin de limiter les besoins en terres arables (la production de viande nécessitant, de façon directe et indirecte via la consommation par les bêtes d'aliments végétaux, beaucoup plus de terre que celle de légumes).

Par ailleurs, divers projets, dont le lancé par l'Union européenne, vise à développer la production d'insectes en tant qu'aliment pour le bétail.

De nombreux insectes sont considérés comme nuisibles par les humains. Certains peuvent causer des problèmes de santé majeurs en tant que vecteurs de pathogènes et de maladies infectieuses graves (ex: moustiques et certaines mouches) ou engendrer de l'inconfort et des problèmes cutanés en tant que parasites (ex: poux et punaise de lit). On retrouve également des insectes qui causent des dommages aux infrastructures (ex: termites et fourmis charpentière) ou qui s'alimentent des produits agricoles. Ces ravageurs se nourrissent de différents végétaux, des grains (riz, céréales, légumineuses, etc.), des fruits, des légumes et des autres produits à la post-récolte. Il y a également des insectes qui causent des blessures au bétail et aux autres animaux de la ferme comme certaines familles de mouches parasites ( Tachinidae, Sarcophagidae, Oestridae, etc.). À cause des pertes économiques qu'ils engendrent, le contrôle des insectes nuisibles nécessitent parfois l'utilisation de substances chimiques (insecticide) ou d'insectes prédateurs (lutte intégrée).

Bien que les insectes ravageurs attirent plus d'attention, la majorité des insectes sont bénéfiques pour l'environnement. Certains insectes, comme les guêpes, les abeilles, les mouches, les papillons et les fourmis sont les principaux pollinisateurs de nombreuses plantes à fleurs. On retrouve également des insectes prédateurs qui sont d'excellents alliés dans le contrôle des ravageurs (lutte biologique) en agriculture. Par exemple, on peut utiliser des coccinelles pour contrôler les populations de pucerons dans certaines cultures. Les carabes, les staphylins, les chrysopes, les hémérobes, les guêpes parasitoïdes, les mouches parasitoïdes, et plusieurs autres insectes permettent de contrôler les populations d'insectes ravageurs.

Divers insectes ont été exploités depuis l'Antiquité pour la production de commodités alimentaires et textiles. Par exemple, l'élevage du ver à soie ("Bombyx mori") (sériciculture) se pratique depuis près de 5000 ans. La larve fabrique un cocon qui est constitué d'un fil de soie brute de 300 à de long. La fibre est très fine et brillante et une fois tissée, elle crée un tissu d'une grande qualité que l'on appelle soie. Cet élevage a hautement influencé la culture chinoise et le développement du commerce avec les pays européens. Un autre insecte domestiqué qui a grandement influencé l'histoire est l'abeille domestique. Les premières représentations de l'homme collectant du miel datent d'il y a ans. Les abeilles produisent également des commodités alimentaires comme du miel, de la gelée royale et de la propolis. Ces produits peuvent servir à traiter différents problèmes de santé en médecine alternative.

Les insectes sont utilisés en médecine depuis plus de 3600 ans. Certains remèdes thérapeutiques et médicaux sont confectionnés avec les parties du corps, l'hémolymphe ou les toxines produites par l'insecte. Par exemple, l'hémolymphe des cigales (Cicadidae) contient une concentration élevée d'ions de sodium et peut être utilisé comme traitement pour certains problèmes de vessie ou de reins. Certains méloés (Meloidae) sont aussi utilisés en médecine humaine et vétérinaire. L'utilisation d'asticots de mouche est également une pratique médicale courante. En se nourrissant des tissus nécrosés, les larves facilitent la cicatrisation des tissus sains en stimulant la production de tissus cicatriciels et en désinfectant les plaies sans l'usage d'antibiotiques.






</doc>
<doc id="16902" url="https://fr.wikipedia.org/wiki?curid=16902" title="Sikh (homonymie)">
Sikh (homonymie)


</doc>
<doc id="16903" url="https://fr.wikipedia.org/wiki?curid=16903" title="Sikhisme">
Sikhisme

Le sikhisme est une religion monothéiste fondée dans le nord de l'Inde au par le Gurû Nanak.

Le mot "ਸਿੱਖ" (sikkh) est un mot pendjabi qui est dérivé du mot sanskrit "शिष्यः" (śiṣya) signifiant "disciple" ou "étudiant", ou de "शिक्ष" (śikṣa), signifiant "étude" ou "instruction" Khushwant Singh, "The Illustrated History of the Sikhs", Oxford University Press, 2006, Transcrit littéralement cela revient à dire « l'âme de dieu ». La doctrine du sikhisme se fonde sur les enseignements spirituels des Dix gurûs, recueillis dans le Sri Guru Granth Sahib.

Gurû Nanak (1469-1539), fondateur du sikhisme, est né dans le village de Talwandi, nommé maintenant Nankana Sahib, près de Lahore, dans l'actuel Pakistan. Ses parents sont punjabi et appartiennent à une caste marchande : les Khatri du Punjab. Dès son enfance, Guru Nanak est fasciné par la spiritualité et montre des dispositions peu ordinaires pour l'apprentissage. C'est sans doute durant cette période qu'il découvre l'enseignement du poète saint Kabîr (élevé dans une famille musulmane), père de la littérature hindi, un homme révéré à la fois par les hindous et les musulmans. Après une expérience spirituelle de « fusion » avec l'essence de toute chose, Gurû Nanak compose le "Jap Ji Sahib", poème mystique qui résume un enseignement qu'il décide de partager. Il voyage dans toute l'Inde et dans de nombreux pays environnants - Népal, Tibet, Sri Lanka, avant d'entamer un long périple au cœur du monde musulman. En effet, le premier disciple et ami d'enfance de Guru Nanak, Mardana, barde attaché à la famille du Gurû, est de confession musulmane. Mardana décide de suivre Guru Nanak qui effectue son pèlerinage à La Mecque. Ce voyage les conduira notamment dans la péninsule d'Arabie, en Perse et en Afghanistan.

Après plusieurs années de voyage, Guru Nanak réunit une communauté et fonde un village, Kartarpur - la Ville du Créateur. Il enseigne sans relâche et de nombreuses personnes viennent à son enseignement. La religion, pense-t-il, est un lien pour unir des hommes, mais dans la pratique il constate qu'elle monte les hommes les uns contre les autres et est à l'origine de nombreuses discriminations : entre hommes et femmes, entre castes, entre religions, entre origines ethniques, etc. Il regrette en particulier l'antagonisme entre hindous et musulmans, quand lui voit la richesse commune de ces deux religions. Une phrase bien connue de Guru Nanak est : « Il n'y a ni hindou et ni musulman. » À ceux qui demandent alors qui ils sont s'ils ne sont ni hindous, ni musulmans, il répond : « vous êtes des disciples ». C'est ainsi que le mot "Sikh" (disciple), se répand.

Gurû Nanak est opposé au "système des castes". Ses fidèles se réfèrent à lui en tant que gurû (professeur, maître). Avant sa mort, il indique un nouveau gurû pour être son successeur et pour mener la communauté. Le dixième et dernier gurû, Gurû Gobind Singh (1666-1708) introduit la cérémonie de baptême sikh en 1699 donnant par là une identité caractéristique aux Sikhs. Les cinq Sikhs nouvellement baptisés sont appelés Panj Pyare, "Les Cinq Bien-Aimés", qui baptisent à leur tour le gurû à sa demande.

Avant son décès, le gurû complète l'Âdi Granth des œuvres de son prédécesseur, le renommé Siri Guru Granth Sahib, et commande qu'il soit dorénavant l'autorité spirituelle définitive et que l'autorité temporelle passe au Khalsa Panth - la Communauté des Sikhs. Le livre saint des Sikhs est compilé et édité par le cinquième gurû, Gurû Arjun en 1604. Ce sont les premières écritures saintes dans le monde à avoir été compilées par les fondateurs d'une foi au cours de leur vie (les écrits saints de la religion bahá'íe au étant également tous rédigés par le fondateur lui-même ou en sa présence). Elles sont surtout rédigées en punjabi, mais aussi en hindi, en persan, etc.

Guru Arjan construisit également le mondialement célèbre Gurdwârâ - Darbar Sahib, à Amritsar, qui est le centre du Sikhisme.(Et le Maharaja Ranjit singh met de l'or sur ce Gurdwara).

Durant le , les Sikhs firent l'objet de répressions et de persécutions diverses de la part des autorités, poussées par le fanatisme général. Ils durent faire des sacrifices extrêmes pour protéger et préserver leur foi et leur identité. L'empire moghol était en voie de désintégration, les Afghans, sous la conduite d'Ahmed Shah Abdali, avaient commencé à envahir le pays. Les Sikhs profitèrent de ces circonstances pour établir leur propre royaume qu'ils achevèrent de constituer sous le Maharaja Ranjît Singh (1780-1839). L'empire sikh dura un demi-siècle et fut annexé par les anglais en 1849.

La religion sikhe est strictement monothéiste. Ses adeptes croient en un seul Dieu Suprême, Absolu, Infini, l'Éternel, le Créateur, la Cause des causes, sans inimitié, sans haine, à la fois immanent et transcendant. Il est appelé: le Guru Suprême (ou en langage courant, « Quel Dieu ! »).

Le postulat de base du sikhisme est qu'il n'y a pas de péché originel, mais la vie ayant émané d'une Source Pure, le Seigneur de Vérité demeure en elle.

Ainsi Guru Nanak dit:

Non seulement toute la philosophie sikhe, mais aussi toute l'histoire et le tempérament des Sikhs découlent de cette manière de voir.

La position doctrinale de Guru Nanak est assez simple, en dépit de son origine. La cohérence du sikhisme est à mettre au bénéfice de son concept central simple - la souveraineté d'un Dieu unique, le Créateur. Guru Nanak l'appelle « Le Nom Vrai » (Satnam) pour éviter d'utiliser un terme qui soit plus restrictif. Il enseigne que « Le Nom Vrai », qui se manifeste de manières diverses, dans des endroits divers et par des noms divers, est éternellement « Un », Dieu souverain et omnipotent, à la fois transcendant et immanent, créateur et destructeur, intemporel et partout présent.

Selon Guru Nanak, discuter quels composants de sa croyance proviennent de l'hindouisme, quels sont musulmans, c'est discuter comme un idiot qui cherche quelle religion possède le droit de professer des concepts universels tels que la bonté, la charité, l'honnêteté, la vénération du nom de dieu, le respect des autres.

Car Dieu n'est ni musulman, ni hindou, ni de telle ou telle confession : Dieu est UN – "Ek Omkar". En effet, dans le "Guru Granth Sahib" il est écrit :

Le sikhisme considère que toutes les religions peuvent mener vers Dieu. Si la religion sert à se croire supérieur aux autres, il ne s'agit pas de religiosité – mais de vanité humaine, orgueil ou démon que le sikhisme demande de détruire (les symboles physiques sikhs sont là pour rappeler cet ordre à la conscience : combattre, vaincre et tuer l'ego, qui empêche la communion avec Dieu-Un).

Un sikh ne doit pas se perdre en verbiage inutile, idéalisme aveugle, attitude amenant à oublier la présence du Dieu-Un ; l'enseignement du sikhisme rappelle qu'un musulman qui méprise le "Brahman" des Sages hindous, est en fait ignorant du Dieu coranique – « Allah » ; et un hindou qui méprise l'unique "Allah" des sages musulmans, est en réalité ignorant de l'Être suprême védique – « Brahman ». Cela est vrai aussi pour les autres religions.

Car Guru Nanak, le "gourou"/maître fondateur du sikhisme, "n'a jamais voulu opposer l'islam et l'hindouisme, ni dissoudre ou remplacer l'islam et l'hindouisme par sa propre philosophie religieuse".

Guru Nanak a juste exposé sa foi, ses expériences mystiques, par une poésie sensible et remplie de dévotion envers le Dieu-Un tel qu'il se révéla à sa personne : les neuf autres gourous du sikhisme et ses authentiques "disciples" (« sikhs ») n'ont fait que le suivre dans ce cheminement spirituel, voulu universel et protecteur.

Gurû Nanak souscrit également à la croyance en la mâyâ, l'illusion du monde physique. Bien qu'il considère les objets matériels comme des réalités et comme des expressions de la vérité éternelle du créateur, ils tendent à ériger « un mur d'erreurs » autour de ceux qui ne vivent que dans un monde des désirs matériels. Ceci les empêche de voir le Dieu vrai qui a créé la matière comme un voile autour de lui, de sorte que seules les consciences spirituelles, libérées du désir, puissent le pénétrer.

Le monde est immédiatement vrai dans le sens qu'il est rendu manifeste aux sens par la maya, mais il est finalement irréel puisque seul Dieu est finalement vrai. En accord avec la doctrine hindoue de la transmigration des âmes, c'est-à-dire du samsara (cycle où l'âme, sans naissance ni mort, peut transmigrer sous une forme humaine, animale, végétale), ainsi que son corollaire, la loi du karma, Nanak conseille aux fidèles de ne pas prolonger leur cycle de réincarnations par une vie hors de Dieu où l'on opte pour l'égoïsme, les plaisirs transitoires et une vie matérialiste basée sur l'avidité ou la jalousie, amenant à la frustration ou au chagrin, à faire souffrir les autres vies.

Pour faire suivre la voie divine, il faut vivre en faisant des actes charitables, des prières, méditer pour parfaire son propre karma. On doit ne penser qu'à Dieu, répéter sans fin le nom de Dieu (Naam Japna) et ainsi unir son âme avec Dieu. Le salut, dit-il, ne signifie pas entrer au Paradis après le Jugement dernier, mais s'unir à la Divinité-Une et se fondre en Elle, communier avec le Maître infini à jamais.

Un Sikh ne peut avoir foi en aucun autre prophète vivant ou non vivant. En accord avec le Sikhisme, Dieu n’apparaît jamais sous forme humaine. Le paradis et l’enfer n’existent que dans ce monde.

Le Sikhisme est basé sur la théorie du karma et de la réincarnation. Le Gurbani, la parole du gourou, dit:

On évite les réincarnations en renonçant aux vices (chair animale, alcool, tabac, jeux de hasard), en surmontant son propre égoïsme ("haumai"), en menant une vie intègre et honnête, car le but suprême de l'existence est la libération ("mukti"). Dans le Sikhisme, le concept de la Libération n’est pas dans un « autre monde », c’est d’être un "Sachiar", « réalisé par Soi-Même », obtenu par la grâce divine.

Le pèlerinage vers des lieux « saints » ne trouve pas sa place dans le Sikhisme. Pour un Sikh, "Shabad" (la "Parole") est le seul lieu saint et l’eau sacrée des rivières, la compassion envers les créatures, la méditation, et une vie de vérité sont le seul pèlerinage.

Le Sikhisme n’est pas une religion fataliste. Un Sikh se soumet à la volonté de Dieu mais est toujours disposé à se battre pour de meilleurs lendemains.

Les sikhs ne reconnaissent pas le système de castes, ils y sont même farouchement opposés ; le sikhisme s'est créé sur un concept d'égalité de droits pour tous. De même, les sikhs ne croient pas en l'adoration des idoles, dans les rituels ou les superstitions.

Cette religion correspond à une manière d'être, de rendre service à l'humanité et d'engendrer tolérance et fraternité vis-à-vis de tous. Les Gurus du sikhisme ne demandent pas le retrait du monde pour atteindre le Salut. Il peut être atteint par chaque personne qui gagne honnêtement des richesses matérielles et mène une existence enracinée dans la volonté de paix.

Richesse et possessions personnelles ne sont pas des obstacles à la réalisation d'idéaux spirituels :

Le Sikhisme préconise la lucidité et le courage authentiques (au-delà du clivage pessimisme/optimisme) :

Il n'y a pas de personnes de basse extraction à mépriser, à exclure (du fait de leur naissance ou de leur condition), mais des attitudes basses – à éviter absolument :

Les sikhs doivent se marier avec la personne de leur choix. .

L'union physique, sexuelle, n'est que le couronnement de l'union spirituelle préalable. 

Sans cette union spirituelle préalable, il ne peut y avoir mariage et, par conséquent, aucune union physique n'est possible ou souhaitable. 

À ce propos, le Sri Guru Granth Sahib déclare :

Le "Khālsā" (mot d'origine persane qui signifie « pur »), est le nom, initialement donné par Gurû Gobind Singh, à l'ordre chevaleresque des Sikhs qu'il créa en 1699. Par extension, le mot désigne chaque membre de cet ordre, chaque Sikh (homme ou femme) qui a été baptisé ou initié en recevant l'Amrit.

Les sikhs initiés (sikhs amritdaris), doivent suivre la règle des « 5 K » : ils doivent porter les cheveux longs et la barbe (Kesh); porter en permanence un peigne dans les cheveux (Kangha) ils portent aussi un poignard recourbé, un turban, un bracelet en fer, le Kawra, symbolisant l'unité (boucle sans fin) et un caleçon spécifique, le Kacchera.

Les sikhs non initiés ne portent pas tous ces attributs.

Le végétarisme est une norme culturelle dans le sikhisme : le "Guru Granth Sahib", qui enseigne la pitié envers toutes les créatures et le refus d'encourager ou de participer à leur mise à mort, comparent les meurtres d'animaux à l'oubli du "Dieu Un omniprésent".

Le temple sikh s'appelle "Gurdwârâ" (littéralement : « la porte du Guru »). Pour y entrer, il faut se déchausser et se couvrir la tête. Le temple est un lieu ouvert à tous, croyant ou non, ils se doivent de vous accueillir dans le respect tant que vous faites de même. Pour être reconnu comme un temple officiel, il faut remplir ces trois critères : arborer le drapeau orange, contenir le livre sacré et être en mesure d'offrir gîte et nourriture.
La salle principale du temple contient le trône, le Guru Granth Sahib sous un dais. Les sikhs se prosternent devant le livre sacré et déposent un don d'argent, avant de s'asseoir par terre pour prier.
En sortant, on vous proposera d'aller manger quelque chose au Langar (cantine communautaire gratuite créée à l'origine entre autres pour lutter contre la séparation des castes).
C'est un devoir pour un sikh de participer au service communautaire.

Les Sikhs sont installés principalement au Punjab, pour 80 % d'entre eux, mais aussi dans la région de Delhi. En Inde, on estime la communauté Sikh à quelque 20 millions de personnes, soit environ 2 % de la population indienne.

Ailleurs dans le monde, on trouve aussi d'importantes communautés Sikh au Pakistan, Royaume-Uni et dans les anciennes colonies britanniques - Canada, Australie, Singapour, Kenya, etc. - et aux États-Unis, ainsi qu'en Indonésie.

Notons également qu'il existe une communauté de plus en plus importante de Sikhs occidentaux - ou d'origine non indienne - pour la plupart pratiquants du Kundalinî yoga. Cela s'explique par l'appartenance à la spiritualité Sikh de Yogi Bhajan, maître de Kundalini Yoga, et par les nombreuses passerelles qui existent entre l'enseignement spirituel des Sikhs et celui du Kundalinî Yoga tel qu'il a été popularisé par Yogi Bhajan. Notons par exemple que la plupart des mantras du Kundalini Yoga sont extraits du Siri Guru Granth Sahib.

Après les attentats du 11 septembre 2001, nombreux sont les Américains ayant confondu les symboles de croyance religieuse sikh, tels que les turbans et les barbes, avec ceux des terroristes qui ont effectué les attaques. Ces derniers se retrouvent souvent maltraités et confondus avec les musulmans. Dans les mois qui ont suivi l'attaque, pas loin de 300 incidents ont été rapportés sur le sol américain, incluant menaces, actes de violence, et même meurtre (voir meurtre de Balbir Singh Sodhi).

Le 2 mars 2006, un jugement de la Cour suprême du Canada a légalisé le port du kirpān dans les écoles publiques, en se fondant sur le principe de liberté religieuse garanti par la Constitution. La Cour a jugé qu'une autorité scolaire ne pouvait interdire totalement le port du "kirpan" par un élève, dans la mesure où le "kirpan" est porté dans des conditions sécuritaires (lame cousue dans son étui).

Le 5 août 2012, un homme ouvre le feu dans un temple Sikh dans la banlieue de Milwaukee, au nord de Chicago. Le bilan fait état de 7 morts et 3 blessés graves.

Il existe plusieurs communautés sikhs en France, il est estimé à hauteur de habitants de confession Sikh. Il y a deux Gurdwara à Bobigny (Singh Sabha France et Darbar Sri Guru Granth Sahib Ji), un à Bondy Guru Tegh Bahadur Ji, un à La Courneuve Shri Guru Ravidas Ji et un au Bourget Baba Makhan shah lubana.

Cependant la population française ignore la véritable identité des Sikhs. Il arrive également que les Sikhs soient considérés comme musulmans en raison de leur apparence physique.

Aussi dans une décision du , la Cour européenne des droits de l'homme a estimé que l'obligation, pour les Sikhs motocyclistes, de porter un casque (en abandonnant leur turban) n'est pas contraire à l'article 9§2 de la Convention européenne des droits de l'homme. Au Royaume-Uni cependant, les Sikhs sont exemptés de l'obligation de porter un casque sur un deux-roues motorisés, s'ils portent un turban.

En France, la loi du 15 mars 2004 visant à interdire le port « ostensible » de symboles religieux dans les écoles publiques conduit désormais régulièrement à l'exclusion de l'enseignement public de lycéens et de collégiens Sikhs refusant d'ôter leurs turbans. Dans un arrêt du , le Conseil d'État a considéré que l'obligation, pour les Sikhs, de poser tête nue pour la photographie du permis de conduire n'était pas contraire aux articles 9 et 14 de la Convention européenne des droits de l'homme. Les Sikhs portent en effet un turban par tradition mais aussi par commodité. En effet, les Sikhs ne se coupant pas les cheveux ni la barbe, le turban leur permet d'enrouler leurs cheveux. En 2011, le comité des droits de l'homme des Nations unies a sanctionné la France pour avoir demandé à un Sikh de retirer son turban sur sa pièce d'identité, la France n'ayant pas justifié les motifs de la nécessité d'une photo tête nue pour un Sikh, lorsque le port du turban Sikh (Dastaar) n'entrave pas son identification.

Le Conseil Représentatif des Sikhs de France est l'instance representative des organismes religieux et sociaux Sikhs.

 sikhs vivent en Italie, principalement en Lombardie et en Émilie-Romagne, et travaillent dans l'industrie laitière, notamment dans la fabrication du parmesan.

Dans le cadre de son Programme de l'« École libre », le Ministère de l'éducation nationale britannique, () a autorisé, en septembre 2011, l'ouverture d'une école Sikh, comme 24 autres écoles, sur 323 candidatures. Cette école primaire, située à Handsworth, une région économiquement défavorisée de Birmingham, est ainsi financée à 100 % par l’État et accueille 180 élèves provenant de familles de confession Sikh mais aussi d'autres confessions ou de familles athées. Ranjit Singh Dhanda, le directeur de l'école, déclare :
À Londres, le quartier de Southall, dans le district d'Ealing, a la communauté Sikh la plus grande dans le pays, et la gurdwârâ la plus grande en Europe.

Les sikhs marginaux et nomades Nihan Singh mangent de la viande alors que les autres sikhs sont végétariens. Au cours de cérémonies rituelles, des chèvres sont décapitées d'un coup de sabre et leur chair est consommée par les assistants. C'est une manière de montrer qu'ils sont différents des autres sikhs. Et pour ceux-ci, une raison de les tenir à l'écart.

Beaucoup de Sikhs ont pour nom « ». Singh, qui signifie « lion », est rarement un nom de famille à proprement parler mais plutôt un titre ou surnom (« middle name ») porté par les hommes Sikhs ; le nom ajouté pour les femmes est « », qui signifie « princesse».

Cependant, tous les « Singh » ne sont pas Sikhs, ce nom étant aussi porté largement par les hindous. Vijay Singh, écrivain et cinéaste indien et le golfeur fidjien du même nom ne sont pas Sikhs.









</doc>
<doc id="16906" url="https://fr.wikipedia.org/wiki?curid=16906" title="Gurus du sikhisme">
Gurus du sikhisme

Pour le sikhisme, guru vient étymologiquement de gu : obscurité et ru : vers la lumière. Le guru, comme dans l'hindouisme, est celui qui mène le disciple de l'obscurité vers la lumière.

Le Guru Granth Sahib le livre saint des sikhs rappelle à ce propos, page 463:

ਜੇ ਸਉ ਚੰਦਾ ਉਗਵਹਿ ਸੂਰਜ ਚੜਹਿ ਹਜਾਰ ॥

ਏਤੇ ਚਾਨਣ ਹੋਦਿਆਂ ਗੁਰ ਬਿਨੁ ਘੋਰ ਅੰਧਾਰ ॥੨॥ 

soit:

Même si une centaine de lunes devaient apparaitre, et mille soleils,

Même avec une telle lumière, les ténèbres persiteraient sans la présence du guru.

Guru est un mot important qui revêt quatre significations. La première est par rapport à Dieu. Dieu est le guru, l'être en dehors du temps, mais qui rentre dans la dimension temporelle. Dieu, appelé aussi Waheguru, est l'instructeur divin. Pour le sikhisme, le guru se retrouve aussi dans l'hindouisme ou dans l'islam.

La deuxième explication du mot est naturellement le guru humain. Les Sikhs reconnaissent onze gurus, dix ayant vécu en chair et en os, et le dernier incarnant l'âme des dix précédents et prenant la forme d'un livre, le "Siri Guru Granth Sahib" :


Le troisième sens du mot est donc celui utilisé non pas pour un guru de chair et d'os mais pour le Guru Granth Sahib, le gourou intemporel laissé par Guru Gobind Singh.

Une quatrième signification est liée au terme Guru Panth c'est-à-dire la communauté sikhe. Ses décisions bien que temporelles marquent les croyants.


</doc>
<doc id="16915" url="https://fr.wikipedia.org/wiki?curid=16915" title="Alain Delon">
Alain Delon

Alain Delon, né le à Sceaux, est un acteur et homme d'affaires français. Il a aussi été producteur et a réalisé deux films.

Acteur parmi les plus populaires du cinéma français, il a attiré dans les salles plus de de spectateurs, s'établissant ainsi comme un champion du box-office au même titre que Louis de Funès et Jean-Paul Belmondo à la même époque. Ayant joué aux côtés de grands acteurs tels que Jean Gabin, Simone Signoret, Romy Schneider ou Lino Ventura, il a vu un grand nombre de ses films devenir des classiques du cinéma français, comme "Plein Soleil", "L'Éclipse", "Rocco et ses frères", "Le Guépard", "Mélodie en sous-sol", "La Tulipe noire", "Le Samouraï", "La Piscine", "Le Clan des Siciliens", "Borsalino", "Monsieur Klein" ou "Notre histoire".

La longueur et le prestige de sa carrière, commencée à l'âge de , suscitent le respect et l'admiration de nombre de cinéastes contemporains comme Johnnie To, Quentin Tarantino ou Sofia Coppola. Bien qu'il n'ait pas réussi à s'imposer à Hollywood, Alain Delon dispose d'une renommée internationale ; outre l'Europe, il a également connu un grand succès en Asie, où il a développé des activités entrepreneuriales.

En 1985, il obtient le César du meilleur acteur pour "Notre histoire".

Alain Fabien Maurice Marcel Delon naît le à Sceaux, dans le département de la Seine (actuellement les Hauts-de-Seine). Fils de Fabien Delon (1904-1977), directeur d'un petit cinéma de quartier, Le Régina, et d'Édith Arnold (1911-1995), préparatrice en pharmacie. Les Delon sont originaires de Saint-Vincent-Lespinasse, en Tarn-et-Garonne. Sa généalogie remonte à Jean Delon, né au . L'arrière-grand-père paternel d'Alain Delon, Fabien Delon (Saint-Vincent-Lespinasse, - Figeac (Lot), ), décoré de la Légion d'honneur en 1892, était Ingénieur des ponts et chaussées. Sa grand-mère paternelle, Marie-Antoinette Evangelista, était corse originaire de la commune de Prunelli-di-Fiumorbo, elle avait épousé son grand-père Jean-Marcel Delon alors percepteur dans cette commune. La légende familiale dit la famille Evangelista apparentée aux Bonaparte.
En 1939, Alain Delon a quatre ans lorsque ses parents divorcent. Il est alors confié à une famille d’accueil, dont le père est gardien de prison à Fresnes, ce qui lui permet d'affirmer qu'il a assisté à l'exécution de Pierre Laval dans la cour. Il est placé ensuite dans la pension catholique de Saint-Nicolas d'Igny (dans l'Essonne) où il passe toute sa jeunesse avec un de ses meilleurs amis, Gérard Salomé. Il se fait renvoyer six fois des écoles qu'il fréquente. Sa mère qui a épousé en secondes noces Paul Boulogne, un commerçant boucher-charcutier de Bourg-la-Reine, lui ménage une place dans le domicile familial. Alain passe un CAP de boucherie pour reprendre sans aucune conviction le commerce de son beau-père qui compte seize employés.
À 14 ans, il a l'occasion de tourner dans "Le Rapt", un court-métrage réalisé par le père de l'un de ses amis.
À dix-sept ans, devançant l'appel sous les drapeaux, il effectue son service militaire dans la Marine Nationale. Après un passage au Centre de formation maritime de Pont-Réan, il poursuit son service militaire en 1953 à l'École des transmissions des Bormettes. Après avoir été pris pour un vol de matériel de prison, la Marine Nationale lui laisse le choix entre quitter la marine ou prolonger son engagement de trois à cinq ans. Matelot de 1 classe, il est alors affecté à la compagnie de protection de Saïgon, sous les ordres du commandant Constant Colmay. Vers la fin de la guerre d'Indochine, il est mis aux arrêts pour avoir volé une jeep et fait une virée au cours de laquelle le véhicule tombe dans un arroyo. Son brevet de radio lui est retiré et il est exclu de la marine.
De retour à Paris en 1956 où il fait la connaissance de la future Dalida avec qui il aura ensuite une liaison, il enchaîne les petits métiers, notamment comme débardeur aux Halles et à Montmartre où il côtoie le monde de la pègre et des gigolos, dont l'un, selon Bernard Violet, un « homosexuel nommé Carlos », assurera sa protection. Sa rencontre amoureuse avec Brigitte Auber au "Club Saint-Germain" l'éloigne de cet univers et va changer son parcours. Dans le quartier de Saint-Germain-des-Prés, il se fait remarquer par Jean-Claude Brialy qui l'invite au Festival de Cannes, où son physique et sa gueule d'ange ne passent pas inaperçus. Il fait un bout d'essai concluant et aborde ainsi le milieu du cinéma, sans formation particulière de comédien.

À Rome, où Alain Delon vit avec Gian Paolo Barbieri, qui deviendra un photographe célèbre, il est remarqué par le découvreur de talents américain David O. Selznick, qui lui propose un contrat de sept ans aux États-Unis à la condition qu'il apprenne l'anglais. De retour en France, Delon se met donc à l'étude de cette langue mais il rencontre Yves Allégret, qui le convainc de rester en France.
En 1957, Michèle Cordoue, dont il est l'amant, convainc son mari, le réalisateur Yves Allégret, de l'engager pour tourner son premier film "Quand la femme s'en mêle". Il y joue un petit rôle aux côtés de la star Edwige Feuillère. Alain Delon raconte : . Il apparaît ensuite dans la comédie "Sois belle et tais-toi" de Marc Allégret, où il côtoie Mylène Demongeot, Henri Vidal, ainsi qu'un autre jeune acteur, tout comme lui débutant : Jean-Paul Belmondo.
En 1958, il rencontre Romy Schneider sur le tournage du film "Christine", réalisé par Pierre Gaspard-Huit, avec son ami Jean-Claude Brialy et Micheline Presle. Le coup de foudre est réciproque. Il a vingt-trois ans, elle en a vingt ; les « fiancés de l'Europe » se fiancent, le 22 mars 1959, sur le lac de Lugano dans la maison des parents Schneider, sous les feux de la presse. Ils incarnent la beauté, la jeunesse, le succès et deviennent un couple célébré par le show-business et le public.
Après "Christine", où il tenait son premier rôle important, Delon rencontre son premier succès dans "Faibles Femmes" de Michel Boisrond, où il retrouve Mylène Demongeot et partage également l'affiche avec d'autres jeunes premières, Pascale Petit et Jacqueline Sassard. Dans "Le Chemin des écoliers", d'après Marcel Aymé, il joue le fils du personnage interprété par Bourvil.
Son modèle est alors Jean Gabin

En 1960, Alain Delon accède au rang de star sous la direction de René Clément avec "Plein Soleil", adapté du roman "Monsieur Ripley" de Patricia Highsmith, qui est suivi, en 1961, par "Rocco et ses frères" de Luchino Visconti, qui remporte le Prix Spécial du Jury au Festival de Venise et consacre Delon et Annie Girardot. La jeune star joue ensuite dans un sketch romantique face à Brigitte Bardot dans "Les Amours célèbres", un film en costumes inspiré des bandes dessinées de Paul Gordeaux, tourné par Michel Boisrond. La même année, Alain Delon commence une carrière d'homme d'affaires en achetant dans le Vieux-Nice, le restaurant "La Camargue". 

Dans la foulée du "Guépard", Delon s'essaie au théâtre sous la direction de Visconti, dans une pièce de l'élisabéthain John Ford, "", donnant la réplique à Romy Schneider et Daniel Sorano.

L'acteur s'éloigne des compositions légères de ses débuts. De fait, ni la comédie anarchiste de René Clément, "Quelle joie de vivre", ni le sketch de "Le Diable et les Dix Commandements" réalisé par le vétéran Julien Duvivier (où il séduit Danielle Darrieux), pas plus que "Les Amours célèbres" ne figurent parmi ses films marquants. En 1962, il joue aux côtés de Monica Vitti dans "L'Éclipse" de Michelangelo Antonioni, film qui obtient le Prix Spécial du Jury du Festival de Cannes.

En 1963, il joue le rôle de Tancrède dans "Le Guépard" de Luchino Visconti, en compagnie de Claudia Cardinale et de Burt Lancaster ; le film obtient la Palme d'or au festival de Cannes. La même année il tourne, sous la direction de Henri Verneuil, "Mélodie en sous-sol,", récompensé par le Golden Globe du meilleur film en langue étrangère. C'est lors du tournage de ce classique du genre policier que Delon rencontre Jean Gabin. Cette série de films est considérée comme une suite de chefs-d'œuvre. Alain Delon s'impose également en héros de film d'aventures dans "La Tulipe noire", de Christian-Jaque, avec Virna Lisi.

En 1964, il s'essaie à la production, dans le registre du film d'auteur engagé, avec "L'Insoumis" d'Alain Cavalier aux côtés de Georges Géret et Lea Massari. La même année, le 13 août, peu de temps après sa rupture avec Romy Schneider (leur liaison durait depuis cinq ans), il épouse l'actrice Nathalie Canovas alias Barthélémy, dont il attend un enfant. Anthony naît le 30 septembre suivant, à Hollywood, où l'acteur a signé un contrat de longue durée car il veut y faire carrière. Déçu par la qualité des films, il résilie son contrat.

Sa carrière s'internationalise. Il travaille au Royaume-Uni pour un sketch de "La Rolls-Royce jaune" d'Anthony Asquith, avec Shirley MacLaine et George C. Scott, et pour "La Motocyclette" de Jack Cardiff d'après André Pieyre de Mandiargues avec Marianne Faithfull. À Hollywood, il tourne avec Ann-Margret, Van Heflin, Jack Palance le thriller "Les Tueurs de San Francisco", "Texas, nous voilà", un western parodique avec Dean Martin, et le film de guerre "Les Centurions" de Mark Robson avec Anthony Quinn et George Segal. 
En 1966, Clément lui donne le rôle de Jacques Chaban-Delmas dans "Paris brûle-t-il ?". 
Devenu une valeur sûre du cinéma français, l'acteur côtoie ses pairs : Lino Ventura dans "Les Aventuriers" de Robert Enrico, Senta Berger dans le thriller "Diaboliquement vôtre" de Julien Duvivier et Brigitte Bardot pour la seconde fois dans un sketch des "Histoires extraordinaires", d'après Edgar Allan Poe, réalisé par Louis Malle.

En 1967, Alain et Nathalie tournent ensemble dans "Le Samouraï", le classique de Jean-Pierre Melville. L'année suivante, Delon revient au théâtre pour une pièce de Jean Cau, mise en scène par Raymond Rouleau, "Les Yeux crevés". Durant la décennie, Delon retrouve son maître Clément pour le suspense dans "Les Félins", où il est le prisonnier de Jane Fonda et Lola Albright.

En 1968, Delon affronte Charles Bronson dans le policier "Adieu l'ami", écrit par Sébastien Japrisot et réalisé par Jean Herman, connu également comme écrivain sous le pseudonyme de Jean Vautrin. 

Toujours en 1968, Delon monte sa propre société de production, "Adel Productions". Son premier film produit est "Jeff", également réalisé par Herman. Il propose à Mireille Darc de jouer avec lui dans "Jeff". Alain Delon clôt la décennie avec deux classiques du film noir : "La Piscine", qui est l'occasion de retrouvailles spectaculaires avec Romy Schneider devant la caméra de Jacques Deray, et "Le Clan des Siciliens", retrouvailles avec Verneuil, Gabin et Ventura.

Pendant ce temps, éclate l'affaire Marković, du nom de son garde du corps, Stevan Marković, retrouvé mort dans un bois à Élancourt dans les Yvelines. François Marcantoni, un ami de Delon, est accusé de l'assassinat. Alain Delon est interrogé par la police, bien que l'assassinat ait eu lieu à Paris alors qu'il était à Ramatuelle, lieu de tournage de "la Piscine" ; Nathalie est également interrogée.

En 1969, il fonde un haras à Aix-en-Provence, avec Mireille Darc et le parrain du milieu marseillais Jacky Imbert.

En 1970, Delon tourne avec Jean-Paul Belmondo, son unique rival dans le cinéma français, "Borsalino", classique du film de gangsters signé Jacques Deray. En 1970 et 1972, Delon tourne de nouveau avec un de ses maîtres, Jean-Pierre Melville, "Le Cercle rouge", face à Bourvil (son père dans "Le Chemin des écoliers" onze années plus tôt), et "Un flic" qui marque sa rencontre professionnelle avec Catherine Deneuve et Richard Crenna. Durant la décennie, il développe et pousse à l'extrême deux aspects essentiels de son personnage cinématographique : le fétichisme du vêtement (chapeau et imperméable) et le professionnalisme. On retrouve cet aspect dans "Le Cercle rouge", "Un flic" et "Borsalino and Co"… Tournée en 1974, la suite de "Borsalino" se fait sans Belmondo (son personnage étant mort dans le précédent film), mais avec Deray ; la même année Delon accepte le rôle principal de "Zorro".

Dans les années 1970 et au début des années 1980, Alain Delon apparaît dans un grand nombre de films d'action, en majorité des polars, où il interprète des personnages de héros, ou parfois d'anti-héros tragiques : "Doucement les basses" avec Nathalie Delon et Paul Meurisse, "Flic Story" (rôle de Roger Borniche), "Le Gang" d'après Borniche, "Trois hommes à abattre", aux côtés de l'actrice italienne Dalila Di Lazzaro, d'après Jean-Patrick Manchette, tous de Jacques Deray. "Le Gitan" avec Bernard Giraudeau et Renato Salvatori, son frère dans "Rocco", et "Comme un boomerang", aux côtés de Charles Vanel, mis en scène par José Giovanni, "Mort d'un pourri" de Georges Lautner, sur un scénario de Michel Audiard, avec Ornella Muti et Klaus Kinski… À la même époque Delon tourne le western "Soleil rouge" du Britannique Terence Young, où il interprète « Gotch », rivalisant avec Bronson, Toshirō Mifune et Ursula Andress. Il tentera de nouvelles incursions dans le cinéma américain en tenant l'un des rôles principaux du thriller "Scorpio" réalisé par Michael Winner, aux côtés de Burt Lancaster, Paul Scofield et Gayle Hunnicutt, et du film catastrophe "Airport 80 Concorde" aux côtés de Sylvia Kristel et Robert Wagner, qui ne remporte pas un grand succès commercial.

1971 marque sa première rencontre avec Joseph Losey pour "L'Assassinat de Trotsky", où il se confronte à Romy Schneider et Richard Burton. Quelques années plus tard, "Monsieur Klein", chef-d'œuvre de Losey, dont Delon est l'acteur principal et le producteur, repart bredouille du festival de Cannes 1976, mais s'avère un beau succès critique. En 1977, à la des César, il remporte le César du meilleur film.

Delon tourne deux fois avec Simone Signoret dans "La Veuve Couderc" de Pierre Granier-Deferre et "Les Granges Brûlées" de Jean Chapot, et se mesure une dernière fois à Jean Gabin dans le tragique "Deux hommes dans la ville" de José Giovanni. Alain Jessua offre également à l'acteur deux rôles intéressants, dans "Armaguedon" face à Jean Yanne et Renato Salvatori, et surtout dans l'éprouvant "Traitement de choc" où il apparaît nu et frappe Annie Girardot.

Alain Delon et Mireille Darc travaillent ensemble pour "Madly", "Les Seins de glace" de Lautner et "L'Homme pressé" d'Édouard Molinaro d'après Paul Morand. Et en 1973, le séducteur de l'écran donne la réplique à Dalida, dans le duo "Paroles, paroles…", dans lequel lui-même ne chante pas, à la différence de sa partenaire.

Il produit le thriller "Le Jeu de la puissance"/"Power Play" avec notamment les stars britanniques David Hemmings, Peter O'Toole et Donald Pleasence.

Si les choix commerciaux de Delon sont souvent critiqués, force est de reconnaître qu'il n'a jamais quitté le cinéma d'auteur. Outre les films déjà cités, il paraît en 1972 dans "Le Professeur" de l'Italien Valerio Zurlini, qui impose un Delon fatigué. En 1978, l'acteur produit "Attention, les enfants regardent" de Serge Leroy, film atypique et passé inaperçu, dans lequel l'acteur apparaît dans un rôle à contre-emploi.

Dans le grand film soviétique "Téhéran 43" (1981) Alain Delon, Claude Jade et Curd Jürgens étaient les vedettes occidentales dans des rôles aux côtés d'acteurs soviétiques. Mais c'était aussi la participation de Delon, qui a apporté le film 47 millions de spectateurs de cinéma seuls en Union soviétique. En 1981, Delon réalise son premier film, un polar, "Pour la peau d'un flic", d'après Jean-Patrick Manchette, qui révèle Anne Parillaud. Il joue dans "Trois hommes à abattre", où il rencontre Dalila Di Lazzaro. Étant producteur, Delon avouera que tous les films incluant dans leur titre le terme « Flic », qu'il choisira lui-même, s'avéreront être des succès commerciaux. L'année suivante l'acteur retrouve Catherine Deneuve dans "Le Choc" de Robin Davis, d'après Manchette encore, dont il cosigne l'adaptation et les dialogues (ce n'est pas la première fois). Il revient à la réalisation en 1983 pour "Le Battant", avec de nouveau Anne Parillaud et Richard Anconina dans un second rôle. En 1984, il incarne le baron de Charlus dans "Un amour de Swann", adapté de Marcel Proust par Volker Schlöndorff ; le film recueille des critiques mitigées. 
L'année suivante, Alain Delon s'écarte de nouveau de son personnage de héros de polar pour tourner dans "Notre histoire" de Bertrand Blier, qui lui vaut d'être récompensé par le César du meilleur acteur en 1985.

S'ensuit "Parole de flic" de Pinheiro (face à Jacques Perrin et le débutant Vincent Lindon), qui est un succès public. Il tente de renouveler son image avec le film fantastique "Le Passage", qu'il produit et dont il coécrit le scénario (le générique chanté par Francis Lalanne connaît aussi le succès), et en jouant pour la première fois depuis 1962 dans un téléfilm, la mini-série "Cinéma", dont il interprète aussi la chanson générique. Il y retrouve sa « marraine en cinéma », Edwige Feuillère. Après le film "Ne réveillez pas un flic qui dort" où figurent aussi Michel Serrault et Serge Reggiani, Alain Delon cesse d'apparaître en héros de polar. Si "Nouvelle Vague", qu'il tourne sous la direction de Jean-Luc Godard, lui permet de retrouver la faveur de certains critiques, il ne touche pas le grand public, pas plus qu'avec un film plus commercial, le thriller "Dancing Machine". "Le Retour de Casanova", adapté par Jean-Claude Carrière d'un roman d'Arthur Schnitzler, et dans lequel Alain Delon a pour partenaires Elsa et Fabrice Luchini, ne remporte pas non plus le succès espéré. Alain Delon tourne ensuite coup sur coup sous la direction de Jacques Deray deux films noirs, "Un crime" et "L'Ours en peluche", d'après Georges Simenon), dont aucun ne touche un large public.

Dans "Le Jour et la Nuit", sous la direction de l'écrivain et philosophe Bernard-Henri Lévy, il joue avec Lauren Bacall, mais la promotion colossale du film est suivie d'une réception critique effroyable ; fiasco commercial, "Le Jour et la nuit" est l'un des plus lourds échecs de la carrière d'Alain Delon.

L'année suivante, il apparaît dans "Une chance sur deux", réalisé par Patrice Leconte : ce polar de divertissement met en scène, sur un mode nostalgique, des retrouvailles artistiques avec Jean-Paul Belmondo, trente ans après "Borsalino", avec pour présence féminine Vanessa Paradis. Même s'il dépasse le million d'entrées, le film ne remporte pas le succès escompté. En 1999, Delon déclare mettre fin à sa carrière au cinéma. 

Il retourne sur les planches à partir de 1996 en jouant une pièce d'Eric-Emmanuel Schmitt, "Variations énigmatiques".

Bien qu'il ait annoncé mettre un terme à sa carrière cinématographique, Alain Delon accepte, en 1999, de participer au film de Bertrand Blier "Les Acteurs", dans lequel il rend hommage à Gabin, Bourvil, Montand, Signoret et de Funès.

En 2001, Alain Delon incarne le commissaire de police Fabio Montale de Marseille, dans une série policière d'après l'œuvre de Jean-Claude Izzo pour TF1, qui s'avère être un des scores historiques pour la télévision française en termes d'audience avec de téléspectateurs. Il joue ensuite en 2003 et 2004 le rôle de Frank Riva dans la série du même nom pour France 2, où il retrouve Jacques Perrin et Mireille Darc. Toujours pour la télévision, il tourne dans "Le Lion" d'après le roman de Joseph Kessel et sous la direction de Pinheiro, auprès de sa fille Anouchka et d'Ornella Muti.

En octobre 2002, Alain Delon et Rosalie van Breemen se séparent après quinze ans de vie commune. Dépressif, âgé de soixante-sept ans, Delon avoue souvent à la presse son manque d'envie de vivre. En 2003, Claudia Cardinale, sa partenaire dans " Le Guépard" en 1963, lui remet l'Étoile d'Or du Festival international du film de Marrakech. Il est fait commandeur de la Légion d'honneur en 2005 par le président de la République française Jacques Chirac pour « sa contribution à l'art du cinéma mondial ». En 2008, il tient le rôle de Jules César dans "Astérix aux Jeux olympiques", mais ce film à très gros budget, malgré plus de six millions de spectateurs, est très mal accueilli par la critique et ne reçoit pas le succès escompté. Alain Delon continue sa carrière sur les planches, interprétant notamment en 2007 "Sur la route de Madison" et en 2008 "Love Letters", successivement avec Mireille Darc et Anouk Aimée.

En 2010, Alain Delon apparaît dans le téléfilm "Un mari de trop" aux côtés de la chanteuse Lorie. Il reprend le théâtre en 2011 avec la pièce "Une journée ordinaire" sur les relations père-fille qu'il interprète aux côtés de sa fille, Anouchka, et d'Élisa Servier. On le voit tenant la main de Mireille Darc, le 4 mars 2011 à l'église Saint-Roch, aux obsèques d'Annie Girardot. Cette même année, en plus d'être président du jury de l'élection Miss France 2012, il est nommé président à vie du jury (il a déjà été président du jury des élections Miss France en 2001 et 2011). Il est également ambassadeur de la marque de lunettes Krys.

En 2013, le festival de Cannes lui rend hommage ; à cette occasion le film "Plein Soleil" de René Clément est présenté en version remastérisée lors de la sélection Cannes Classics. En octobre, Alain Delon joue de nouveau dans la pièce de théâtre "Une Journée ordinaire", mais cette fois en tournée à travers la France, accompagné de nouveau de sa fille Anouchka.

Début mai 2017, Alain Delon annonce qu'il va tourner à l'automne avec Juliette Binoche son dernier film devant la caméra de Patrice Leconte, déclarant .

De 1959 à 1963, Alain Delon et Romy Schneider ont une histoire d'amour et se fiancent.

La chanteuse allemande, Nico, avec qui il a eu une relation, met au monde un fils, Christian Aaron Boulogne dit Ari Boulogne, le 11 août 1962. Même si l'enfant a été élevé par sa propre mère et adopté par son beau-père, Alain Delon en a toujours contesté la paternité.

Il épouse Francine Canovas le 13 août 1964. Leur fils, Anthony, naît le 30 septembre 1964 à Hollywood. Ils divorcent en 1968. 
Durant 15 ans, entre 1968 et 1983, Alain Delon partage la vie de l'actrice Mireille Darc.

Il a ensuite une idylle avec l'actrice Anne Parillaud, puis une autre, plus courte, avec Catherine Bleynie (née en 1952), divorcée de Didier Pironi. En mars 1985, il pose avec elle en couverture de "Paris Match", ainsi qu'en septembre 1985, en couverture de Ciné Télé Revue lors de la remise d'un prix.

Il rencontre en 1987 Rosalie van Breemen, un mannequin néerlandais, sur le tournage du vidéo-clip de sa chanson "Comme au cinéma". Il a avec elle deux enfants : Anouchka, née le , et Alain-Fabien, né le . Ils se séparent en 2001, après quatorze ans d'union.

En 2001, dans son livre de souvenirs, "L'amour n'oublie jamais", paru chez Jean-Jacques Pauvert, le photographe, Christian Aaron Boulogne, fils du mannequin, actrice et chanteuse allemande, Nico, affirme être le fils caché et non reconnu d'Alain Delon.

En 1972 et 1973, il organise en France les championnats du monde de boxe avec les affiches Jean-Claude Bouttier / Carlos Monzón (17 juin 1972 et 29 septembre 1973) puis Carlos Monzón / José Nápoles (avril 1974). Par ailleurs, il constitue une écurie de chevaux de course et obtient le titre de champion du monde des trotteurs avec ses chevaux Equileo et Fakir du Vivier.

En 1978, il crée à Genève sa société de diffusion de produits de luxe, Alain Delon Diffusion SA ; sous son nom, on trouve des parfums comme AD , suivi en 1981 d'une fragrance pour femme, « Le Temps d'Aimer ». Ces deux lignes, , ont été remplacées par d'autres fragrances, telles que « Samouraï » , « Samouraï Woman », « Shogun » ou encore « Samouraï Woman Pinkberry ». La société de l'acteur vend aussi du champagne, du cognac, des montres, des lunettes, des cigarettes, des vêtements et des accessoires griffés à son nom. Les concepteurs de ces différents produits de luxe ne sont pas connus. lorsque l'acteur Chow Yun-fat les porte dans le film "Le Syndicat du crime" et ses deux suites ; John Woo, le réalisateur, a déclaré par ailleurs être un admirateur de Delon et de son jeu d'acteur.

Alain Delon est aussi devenu collectionneur d'œuvres d'art. Sa collection comprend des œuvres d'Olivier Debré, Rembrandt Bugatti, Jean Degottex, Jean Dubuffet, Hans Hartung, Jean-Paul Riopelle, Pierre Soulages, Nicolas de Staël, Alechinsky, Zao Wou-Ki, Vieira da Silva ainsi que deux bronzes d'Antoniucci Volti, les « Muses ». À la suite d'une exposition organisée par le galeriste Franck Prazan, il a cependant vendu 40 toiles consacrées à l'École de Paris et au mouvement CoBrA lors d'une vente aux enchères à Drouot-Montaigne en octobre 2007. La vente totalisera un peu plus de d'euros. Depuis 2013, Alain Delon est également le parrain de Winn'Art, le supplément artistique du magazine "Winner" dirigé par Véra Baudey.

En 1985, il s'installe en Suisse à Chêne-Bougeries, dans la banlieue de Genève. En 1999, après son arrivée en Suisse, il obtient la citoyenneté genevoise et suisse à côté de la nationalité française.

En 1993, il se sépare de son palais de Sidi Mimoun à Marrakech qu'il a habité pendant quinze ans avec Mireille Darc. 

Éternel solitaire, il vit dorénavant principalement au domaine de la Brûlerie à Douchy, propriété de 55 hectares acquise en 1971 et où il a prévu de se faire enterrer dans une chapelle construite dans le parc à côté du cimetière canin où reposent ses 35 chiens, ses plus fidèles compagnons.

Alain Delon se définit comme gaulliste, expliquant avoir été . Il est ainsi engagé de longue date à droite.

Tout comme beaucoup d'artistes, notamment Brigitte Bardot, il appelle à voter Valéry Giscard d'Estaing lors des élections présidentielles de 1974 et 1981. En 1988, il soutient au premier tour la candidature de Raymond Barre.

À partir de la fin des années 1980, il fait état de son amitié et de sa sympathie pour Jean-Marie Le Pen, tout en précisant que . Il précise que l'extrême droite « c'est quand même la droite », et qu'elle « regroupe quelques millions de Français », dont il faut tenir compte. En octobre 2013, il salue la progression électorale du Front national. Cette prise de position, dénoncée par le Comité Miss France, le conduit à démissionner de sa fonction de président à vie du jury ; il déclare alors : .

Il dit cependant préférer Nicolas Sarkozy au Front national. Dans la perspective des élections européennes de 2014, il exprime sa sympathie pour le mouvement Force Vie de Christine Boutin.

Le , il affirme : .

Alain Delon cultive des amitiés avec des personnalités dont les idées sont éloignées des siennes. Il tourne avec Luchino Visconti, proche du Parti communiste italien, et soutient financièrement le film "Monsieur Klein" de Joseph Losey, banni d'Hollywood pour ses sympathies communistes. En 1986, après la défaite de la gauche aux élections législatives, il insiste pour que ce soit l'ancien ministre de la Culture, Jack Lang (PS), qui lui remette les insignes de commandeur de l'ordre des Arts et des Lettres. Lors des élections municipales de 2014 à Paris, il soutient la candidate PS, Anne Hidalgo, avec qui il est ami.

Dans le cadre de la primaire présidentielle des Républicains de 2016, il préfère Alain Juppé à Nicolas Sarkozy, estimant avoir été « quitté » et « largué » par celui-ci. Le 20 avril 2017, il publie une lettre de soutien au candidat de la droite à l'élection présidentielle, François Fillon, en difficulté dans les sondages. Lors du second tour, qui voit s'opposer Emmanuel Macron et Marine Le Pen, il ne se déplace pas pour voter.

















</doc>
<doc id="16918" url="https://fr.wikipedia.org/wiki?curid=16918" title="Marie Ire (reine d'Angleterre)">
Marie Ire (reine d'Angleterre)

Marie , également connue sous le nom de Marie Tudor, née le et morte le , est reine d'Angleterre et d'Irlande de 1554 à sa mort, et, par son mari, reine d'Espagne, de Sicile et de Naples, duchesse de Bourgogne, de Milan, de Brabant, de Luxembourg et de Limbourg, comtesse de Flandre et comtesse palatine de Bourgogne.

Issue du mariage malheureux du roi Henri VIII d'Angleterre et de Catherine d'Aragon, Marie fut écartée de la succession au trône, en 1534, par la 1 loi de succession au trône, après le remariage de son père avec Anne Boleyn. Elle ne redevint héritière potentielle, après son demi-frère Édouard mais avant sa demi-sœur Élisabeth, qu'en 1543, avec la .

Comme Marie était catholique, Édouard VI, devenu roi en 1547, tenta de l'évincer de sa succession et, à sa mort en 1553, sa cousine Jeanne Grey fut proclamée reine. Marie rassembla une armée en Est-Anglie et déposa Jeanne qui fut décapitée. Elle épousa Philippe II d'Espagne en 1554 et devint ainsi reine consort d'Espagne lorsqu'il devint roi en 1556.

Le règne de Marie fut marqué par ses tentatives visant à restaurer le catholicisme après les règnes protestants de son demi-frère et de son père. Plus de 280 réformateurs et dissidents furent brûlés vifs lors des persécutions mariales et cette brutale répression lui valut le surnom de " (« Marie la Sanglante »). Ce retour au catholicisme fut annulé après sa mort en 1558 par sa demi-sœur cadette Élisabeth.

Marie est née le au palais de Placentia dans le quartier londonien de Greenwich. Elle était la fille du roi Henri VIII d'Angleterre et de sa première épouse Catherine d'Aragon et leur seul enfant à avoir survécu jusqu'à l'âge adulte. Avant Marie, Catherine avait été enceinte à quatre reprises ; deux de ses grossesses s'étaient terminées par des fausses couches tandis que les deux fils qu'elle mit au monde, tous deux appelés Henry, moururent dans les semaines qui suivirent leur naissance. Elle fut baptisée dans la foi catholique trois jours après sa naissance. Parmi ses parrains figuraient sa grand-tante Catherine d'York, le lord chancelier Thomas Wolsey et Agnès de Norfolk. L'année suivante, Marie devint également marraine à l'occasion du baptême de sa cousine Frances Brandon. En 1520, la comtesse de Salisbury, Margaret Pole, qui avait représenté Marie durant sa confirmation, fut choisie pour devenir sa gouvernante.

Marie était une enfant précoce. En juillet 1520, alors qu'elle n'avait que quatre ans et demi, elle joua du clavecin pour la visite d'une délégation française. La reine fut très impliquée dans l'éducation de sa fille et elle prit conseil auprès de l'humaniste espagnol Jean Louis Vivès à qui elle commanda un traité sur l'éducation des filles intitulé ". À l'âge de neuf ans, Marie savait lire et écrire en latin et elle étudia le français, l'espagnol, la musique, la danse et peut-être le grec. Le roi l'adorait et il se vanta auprès de l'ambassadeur vénitien Sebastian Giustiniani qu'.

Malgré son affection pour Marie, Henri VIII était profondément déçu par le fait qu'il n'avait pas de fils. Le temps passant, il devint clair que le couple royal n'aurait pas d'autres enfants et qu'Henri VIII n'aurait pas d'héritier mâle légitime. En 1525, Henri VIII envoya Marie au Pays de Galles pour présider, au moins nominalement, le conseil chargé de gouverner la région et l'Ouest de l'Angleterre. Elle reçut sa propre cour au château de Ludlow et des prérogatives royales habituellement réservées au prince de Galles. Elle était parfois appelée princesse de Galles même si elle ne porta jamais techniquement ce titre. Il semble qu'elle soit restée trois ans dans les marches galloises avant de revenir dans les "" autour de Londres à partir de 1528.

Tout au long de l'enfance de Marie, Henri VIII négocia un possible mariage pour sa fille. Alors qu'elle n'avait que deux ans, elle fut promise au fils de François de France, le Dauphin François mais le contrat fut annulé au bout de trois ans. En 1522, il fut décidé qu'elle épouse son cousin, l'empereur Charles Quint mais l'accord fut rompu au bout de quelques années. Thomas Wolsey, le principal conseiller d'Henri VIII, reprit des négociations avec les Français et le roi suggéra que Marie épouse le roi François qui cherchait à former une alliance avec l'Angleterre. Selon un nouvel accord, Marie épouserait François ou son second fils, le duc Henri d'Orléans mais Wolsey parvint à négocier une alliance avec la France sans avoir besoin d'organiser un mariage. Selon le diplomate vénitien Mario Savorgnano, Marie était devenue une belle et élégante jeune femme.

Pendant ce temps, le mariage de ses parents était en péril. Déçu de ne pas avoir d'héritier mâle et impatient de se remarier, Henri VIII tenta de faire annuler son union mais cette demande fut rejetée par le pape Clément VII. Le roi avança, en citant des passages de la Bible (), que son mariage avec Catherine était impur car elle était la veuve de son frère Arthur mais cette dernière affirma que leur union n'avait pas été consommée. De fait, ce premier mariage avait été annulé par le précédent pape Jules II sur cette base. Clément VII a peut-être été influencé dans sa décision par Charles Quint, le neveu de Catherine d'Aragon, dont les troupes occupaient Rome dans le cadre de la septième guerre d'Italie.

À partir de 1531, Marie était souvent malade avec des menstruations irrégulières et des épisodes dépressifs sans que l'on sache si cela était causé par le stress, la puberté ou une maladie. Le roi lui interdit de voir sa mère qui fut envoyée vivre à l'écart de la cour au château de Kimbolton. Au début de l'année 1533, Henri VIII épousa en secret sa maîtresse Anne Boleyn qui était enceinte de lui et en mai, l'archevêque de Cantorbéry Thomas Cranmer annula officiellement le mariage avec Catherine. Cette dernière perdit son titre de reine et devint princesse douairière de Galles tandis que Marie fut déclarée illégitime et donc incapable de réclamer le trône à la mort de son père. L'héritière de la Couronne d'Angleterre devint sa demi-sœur et la fille d'Anne, Élisabeth. La cour de Marie fut dissoute, ses serviteurs, dont la comtesse de Salisbury, furent renvoyés et en décembre 1533, elle fut nommée dame d'honneur d'Élisabeth à
Hatfield dans l'Hertfordshire.

Le refus de Marie de reconnaître qu'Anne était reine et qu'Élisabeth était princesse ulcéra le roi. Limitée dans ses déplacements et stressée, Marie était fréquemment malade. L'ambassadeur impérial Eustache Chappuis devint son proche conseiller et tenta sans succès d'intercéder en sa faveur à la cour. Les relations entre Marie et son père étaient tendues et ils ne se parlèrent pas pendant trois ans. Même si elle et sa mère étaient malades, le roi l'empêcha de rendre visite à Catherine et elle fut « inconsolable » quand elle mourut le . Catherine fut inhumée dans la cathédrale de Peterborough tandis que Marie fut confinée à dans l'Hertfordshire.

En 1536, Anne Boleyn perdit les faveurs du roi et fut décapitée. Élisabeth perdit son titre de princesse et fut, comme Marie, évincée de l'ordre de succession. Moins de deux semaines après l'exécution d'Anne, Henri VIII épousa Jeanne Seymour qui le pressa de se réconcilier avec sa fille. Le roi insista pour que Marie reconnaisse son statut de chef suprême de l'Église d'Angleterre, répudie l'autorité pontificale, reconnaisse que le mariage de ses parents était impur et accepte sa propre illégitimité. Elle tenta de se réconcilier avec lui en se soumettant à son autorité aussi loin que l'y autorisèrent mais elle fut finalement contrainte de signer un document par lequel elle acceptait toutes les demandes de son père, ce qui lui permit de retrouver sa place à la cour. Henri VIII lui accorda une suite et les enregistrements de ses dépenses à cette période montrent que , le , Richmond et Hundson étaient parmi ses principales résidences de même que les palais de Placentia, de Westminster, d'Hampton Court appartenant à son père. Ses dépenses étaient consacrées aux vêtements et aux jeux d'argent avec des cartes, l'un de ses loisirs préférés. Dans le Nord de l'Angleterre, des rebelles, dont Lord Hussey, l'ancien chambellan de Marie, se soulevèrent contre les réformes religieuses d'Henri VIII et l'une de leurs demandes était la relégitimation de Marie. Cette révolte, appelée Pèlerinage de Grâce, fut violemment réprimée. Lord Hussey et de nombreux rebelles furent exécutés mais rien n'indique que Marie fut impliquée. En 1537, Jeanne mourut en donnant naissance à un fils, Édouard et Marie fut choisie pour être sa marraine.
Marie fut courtisée par Philippe du Palatinat-Neubourg à partir de 1539 mais ce dernier était luthérien et ses demandes de mariage furent rejetées. Durant l'année 1539, le principal conseiller du roi, Thomas Cromwell, négocia une alliance avec le duché de Clèves. Les propositions de mariage entre Marie et le duc de Clèves n'aboutirent pas mais une union entre Henri VIII et sa sœur Anne fut acceptée. Lorsque le roi rencontra Anne pour la première fois à la fin de l'année, une semaine avant la cérémonie, il la trouva peu attirante mais fut incapable, pour des raisons diplomatiques et en l'absence d'un prétexte convenable, d'annuler le mariage. Cromwell perdit les faveurs royales et fut arrêté pour trahison en juin 1540 sur l'accusation qu'il complotait pour épouser Marie. Anne accepta l'annulation du mariage qui ne fut pas consommé et Cromwell fut décapité.

En 1541, Henri VIII fit décapiter la comtesse de Salisbury, l'ancienne gouvernante et marraine de Marie, sous le prétexte d'un complot catholique dans lequel son fils Reginald Pole fut impliqué. Son bourreau était qui . En 1542, après l'exécution de la cinquième épouse d'Henri VIII, Catherine Howard, le roi célibataire invita Marie à assister aux célébrations de Noël où elle joua le rôle d'hôtesse. En 1543, Henri VIII épousa sa sixième et dernière épouse Catherine Parr qui parvint à réconcilier la famille. Le roi réintégra ses filles dans l'ordre de succession même si d'après la , elles se trouvaient après Édouard ; les deux restaient néanmoins légalement illégitimes.

Henri VIII mourut en 1547 et Édouard VI lui succéda. Marie hérita des propriétés du Norfolk, du Suffolk et de l'Essex et reçut Hunsdon et le palais de Beaulieu comme résidences personnelles. Comme Édouard VI n'avait que dix ans, le pouvoir fut exercé par un conseil de régence dominé par les protestants qui tentèrent d'établir leur foi dans toute l'Angleterre. L' rendit obligatoire les rites protestants tels que l'emploi du livre de la prière commune de Thomas Cranmer. Marie resta fidèle au catholicisme et, par défi, continua de faire célébrer la messe traditionnelle dans sa chapelle. Elle demanda à son cousin Charles Quint d'exercer des pressions diplomatiques pour qu'elle puisse pratiquer sa religion.

Durant la plus grande partie du règne d'Édouard VI, Marie resta dans ses propriétés et se rendit rarement à la cour. La religion était un sujet de tension entre Marie et son demi-frère. Lors des célébrations de Noël en 1550, Édouard VI réprimanda Marie devant toute la cour pour son refus de respecter ses lois sur la religion et les deux se mirent à pleurer. Marie continua de refuser d'abandonner le catholicisme et Édouard VI refusa de renoncer à ses demandes.

Le , Édouard VI mourut d'une infection des poumons, probablement la tuberculose, à l'âge de 15 ans. Il ne voulait pas que Marie devienne reine car il craignait qu'elle ne restaure le catholicisme et n'annule ses réformes et celles de son père ; ses conseillers lui indiquèrent néanmoins qu'il ne pourrait pas exclure une seule de ses demi-sœurs de l'ordre de succession et qu'il devrait également évincer Élisabeth même si elle était anglicane. Guidé notamment par John Dudley, il décida d'exclure ses deux sœurs dans son testament.
En violation de la qui réintégrait Marie et Élisabeth dans l'ordre de succession, Édouard VI désigna Jeanne Grey pour lui succéder ; elle était la belle-fille de Dudley et la petite fille de la sœur cadette d'Henri VIII, Marie, tandis que sa mère était Frances Brandon qui était la cousine et la marraine de Marie. Juste avant la mort d'Édouard VI, Marie fut convoquée à Londres pour voir son demi-frère mourant. Elle fut néanmoins avertie que cette convocation était un prétexte pour l'arrêter et ainsi faciliter l'accession au trône de Jeanne. Au lieu de se rendre à Londres depuis sa résidence de Hunsdon, Marie s'enfuit en Est-Anglie où elle possédait de nombreuses propriétés et où Dudley avait violemment réprimé la . Le 9 juillet, elle écrivit depuis Kenninghall au conseil privé pour lui demander de la proclamer reine.

Le , Jeanne fut proclamée reine par Dudley et le même jour la lettre de Marie arriva au conseil privé à Londres. Deux jours plus tard, Marie et ses partisans avaient rassemblé une armée au château de Framlingham. Dudley perdit ses soutiens et Jeanne fut déposée le 19 juillet. Dudley et elle furent emprisonnés à la Tour de Londres. Marie entra triomphalement dans la capitale le aux côtés d'Élisabeth et d'une procession de 800 nobles.

L'une des premières décisions de Marie en tant que reine fut de libérer les conseillers catholiques Thomas Howard, Étienne Gardiner et Édouard Courtenay qui étaient emprisonnés à la Tour de Londres. Elle comprit que Jeanne n'était qu'un pion dans le plan de Dudley et ce dernier fut la seule personne de son rang exécutée pour haute trahison immédiatement après son accession au trône. Jeanne et son époux, Guilford Dudley, bien que reconnus coupables, furent détenus à la Tour de Londres tandis que le père de Jeanne, Henry Grey, fut libéré. Marie était dans une position difficile car presque tous les membres du conseil privé avaient été impliqués dans la conspiration pour placer Jeanne sur le trône. Elle nomma Gardiner au conseil et le fit évêque de Winchester et lord chancelier, des fonctions qu'il occupa jusqu'à sa mort en novembre 1555. Le octobre 1553, Gardiner couronna Marie en l'abbaye de Westminster.

À 37 ans, Marie commença à se concentrer sur la recherche d'un partenaire pour engendrer un héritier et ainsi empêcher la protestante Élisabeth de lui succéder au trône. Édouard Courtenay et Reginald Pole étaient considérés comme des prétendants possibles mais son cousin Charles Quint lui suggéra d'épouser son fils unique, le prince Philippe d'Espagne. Philippe avait un fils issu d'une précédente union avec Marie-Manuelle de Portugal morte peu après avoir accouché. Dans le cadre des négociations, un portrait de Philippe réalisé par Le Titien fut envoyé en Angleterre en septembre 1553.

Gardiner et la Chambre des Communes tentèrent sans succès de la convaincre d'épouser un Anglais car ils craignaient que l'Angleterre ne passe sous le contrôle des Habsbourg. L'union fut impopulaire en Angleterre ; Gardiner et ses alliés s'y opposaient par patriotisme tandis que les protestants ne voulaient pas d'une monarchie catholique. Lorsque la reine insista pour épouser Philippe, Thomas Wyatt le Jeune organisa un soulèvement dans le Kent impliquant Henry Grey pour placer Élisabeth sur le trône. Marie déclara publiquement qu'elle convoquerait le Parlement pour discuter du mariage et qu'elle déclinerait l'union si l'assemblée estimait qu'elle n'était pas à l'avantage du pays. Les rebelles furent battus à leur arrivée à Londres ; Wyatt, Henry Grey, sa fille Jeanne et son mari Guildford Dudley furent exécutés. Pour son implication dans le complot, Courtenay fut emprisonné puis exilé. Même si elle défendit son innocence dans le soulèvement, Élisabeth fut détenue deux mois à la Tour de Londres puis confinée au palais de Woodstock.

Marie fut, à l'exception des règnes contestés de Jeanne Grey et de Mathilde l'Emperesse, la première reine régnante d'Angleterre. De plus, selon la coutume anglaise du "", les propriétés et les titres d'une femme devenaient également ceux de son mari et certains craignaient que l'homme qu'elle épouserait devînt de fait roi d'Angleterre. Si les grands-parents de Marie , Isabelle de Castille et Ferdinand II d'Aragon avaient conservé la souveraineté de leurs propres royaumes durant leur mariage, il n'existait pas de précédent de ce type en Angleterre. Selon l'acte de mariage, Philippe recevrait le titre de « roi d'Angleterre », tous les documents officiels seraient signés avec leurs deux noms et le Parlement serait convoqué sous l'autorité conjointe du couple jusqu'à la mort de Marie . L'Angleterre ne serait pas obligée de fournir un soutien militaire aux guerres du père de Philippe et ce dernier ne pourrait pas agir sans l'accord de son épouse ou nommer des étrangers dans l'administration anglaise ; il ne pourrait également pas revendiquer le trône si Marie mourrait avant lui. Philippe était mécontent de ces conditions mais il les accepta pour que le mariage se concrétise. Il n'avait pas de sentiments pour Marie et voulait se marier uniquement pour des raisons politiques et stratégiques ; son conseiller, Rui Gomes da Silva, écrivit à un correspondant à Bruxelles que .
Pour élever son fils au rang de sa future épouse, Charles Quint lui céda la Couronne de Naples ainsi que ses revendications au royaume de Jérusalem. Marie devint ainsi reine de Naples et reine titulaire de Jérusalem lors de son mariage. La cérémonie fut organisée à la cathédrale de Winchester le , deux jours après leur première rencontre. Philippe ne parlait pas anglais et ils communiquèrent en espagnol, en français et en latin.

En septembre 1554, Marie cessa d'avoir ses règles. Elle prit du poids et souffrait de nausées au réveil. Quasiment toute la cour, y compris ses médecins, pensait qu'elle était enceinte. Le Parlement adopta un texte prévoyant que Philippe devienne régent au cas où Marie mourrait lors de l'accouchement. Dans la dernière semaine d'avril 1555, Élisabeth fut autorisée à quitter sa résidence et elle fut convoquée à la cour pour assister à l'accouchement qui était jugé imminent. Selon l'ambassadeur vénitien Giovanni Michieli, Philippe aurait prévu d'épouser Élisabeth si la reine mourait mais dans une lettre adressée à son beau-frère Maximilien d'Autriche, Philippe exprima ses doutes quant à la réalité de cette grossesse.

Des cérémonies furent organisées par le diocèse de Londres à la fin du mois d'avril après que des rumeurs annonçant la naissance d'un fils se furent propagées dans toute l'Europe. L'accouchement n'ayant toujours pas eu lieu en mai et en juin, l'hypothèse d'une fausse grossesse grandit ; Marie continua de présenter les signes d'une grossesse jusqu'en juillet 1555 quand son abdomen perdit en volume. Il s'agissait probablement d'une grossesse nerveuse peut-être déclenché par son désir d'avoir un enfant. En août, peu après la disgrâce de cette fausse grossesse que Marie estima être le pour sa , Philippe quitta l'Angleterre pour combattre les Français en Flandre. Marie eut le cœur brisé et elle sombra dans une profonde dépression. Michieli fut touché par le chagrin de la reine qui était et fut inconsolable après le départ de son mari.

Élisabeth, apparemment revenue en grâce, resta à la cour jusqu'en octobre. En l'absence d'enfants, Philippe craignait qu'après Marie et Élisabeth, la Couronne ne passe à Marie d'Écosse qui était promise au dauphin de France. Philippe voulut persuader Élisabeth d'épouser son cousin Emmanuel-Philibert de Savoie afin de garantir une succession catholique et préserver les intérêts des Habsbourg en Angleterre mais elle refusa et l'accord du Parlement aurait été difficile à obtenir.

Durant le premier mois de son règne, Marie proclama qu'elle ne forcerait aucun de ses sujets à suivre sa religion mais à la fin du mois de septembre, plusieurs ecclésiastiques réformateurs tels que , , John Hooper, Hugh Latimer et Thomas Cranmer furent emprisonnés. Le premier Parlement convoqué par la reine en octobre 1553, déclara que le mariage de ses parents était valide et abrogea les lois religieuses édictées par Édouard VI. La doctrine de l'Église redevint celle précisée par l'Acte des six articles de 1539 qui interdisait le mariage des ecclésiastiques ; ceux dans ce cas furent privés de leurs bénéfices.

Marie avait toujours rejeté la rupture avec Rome instituée par son père et l'établissement de l'anglicanisme. Son mari et elle voulaient réconcilier l'Angleterre avec Rome et Philippe persuada le Parlement d'abroger les lois religieuses adoptées par Henri VIII, ce qui ramenait l'Église d'Angleterre sous la juridiction du Vatican. Les négociations prirent plusieurs mois et le pape Jules III dut faire une importante concession : les propriétés et les biens confisqués sous Henri VIII ne seraient pas rétrocédés à l'Église romaine. En 1554, l'Acte de Suprématie est supprimé. À la fin de l'année le pape accepta le compromis et les lois sur les hérétiques furent rétablies.

Lors des persécutions mariales, de nombreux protestants furent exécutés à partir de février 1555. Certains des plus aisés comme John Foxe choisirent l'exil et plus de 800 quittèrent le pays. L'archevêque de Cantorbéry, Thomas Cranmer, fut obligé de voir les évêques Nicholas Ridley et Hugh Latimer brûlés vifs le 16 octobre. Il abjura, rejeta la théologie protestante et rejoignit la foi catholique. Selon l'application normale de la loi, il aurait dû être absous mais la reine refusa de le pardonner et lors de son exécution le , il revendiqua son adhésion au protestantisme. Au total, 284 personnes furent exécutés sous le règne de Marie , la plupart par le feu. Les bûchers se révélèrent très impopulaires et même , l'un des conseillers religieux de Philippe, les condamna tandis que Simon Renard, un autre de ses conseillers, l'avertit que des pourraient . Marie continua néanmoins sa politique de persécution jusqu'à sa mort, ce qui exacerba les sentiments anti-catholiques et anti-espagnols. Les victimes de cette répression furent considérés comme des martyrs par les protestants et John Foxe leur consacra une longue partie de son "Livre des Martyrs".

Reginald Pole, dont la mère avait été exécutée par Henri VIII, arriva comme légat pontifical en Angleterre en novembre 1554. Il fut ordonné prêtre et nommé archevêque de Cantorbéry immédiatement après l'exécution de Cranmer en mars 1556.

À la suite de la conquête de l'Irlande, des colons anglais s'installèrent dans les Midlands pour protéger la région de Dublin contre les attaques irlandaises. Les ' et ' (aujourd'hui les comtés de Laois et d'Offaly) furent fondés et leur colonisation commença. Leurs principales villes étaient respectivement Maryborough (aujourd'hui Portlaoise) et Philipstown (aujourd'hui Daingean).

En janvier 1556, le beau-père de Marie abdiqua et Philippe devint roi d'Espagne, et donc Marie reine. Philippe fut proclamé roi à Bruxelles mais Marie resta en Angleterre. Une paix fragile fut signée avec la France en février 1556 mais le mois suivant, l'ambassadeur français en Angleterre, Antoine de Noailles, fut impliqué dans un complot contre la reine quand Henry Dudley, un cousin de John Dudley, tenta de rassembler une armée en France. Noailles quitta l'Angleterre et Dudley fut exilé en France.

Philippe retourna en Angleterre de mars à juillet 1557 pour persuader Marie de soutenir l'Espagne dans une nouvelle guerre contre la France. Marie y était favorable mais ses conseillers s'y opposèrent en avançant que le commerce avec la France serait interrompu, que cela contrevenait aux termes du mariage et que le marasme économique et les mauvaises récoltes limitaient les possibilités militaires de l'Angleterre. La guerre ne fut déclarée qu'en juin 1557 après que le neveu de Reginald Pole, Thomas Stafford, ait envahi l'Angleterre et pris le contrôle du château de Scarborough, avec l'aide française, pour essayer de renverser Marie . Cette guerre dégrada les relations entre l'Angleterre et la Papauté, car le pape Paul IV était allié avec le roi Henri II de France. En janvier 1558, les forces françaises prirent Calais, la dernière possession anglaise sur le continent. La défense de ce territoire était un fardeau financier, mais sa perte affecta le prestige de Marie .

Le règne de Marie fut marqué par une pluviosité intense qui affecta les récoltes et causa des disettes. Un autre problème fut le déclin du commerce textile à Anvers. Malgré le mariage avec Philippe, l'Angleterre ne profita pas du commerce extrêmement lucratif de l'Espagne avec le Nouveau Monde. Les routes commerciales espagnoles étaient étroitement contrôlées et Marie ne pouvait soutenir la flibusterie et la piraterie car elle était mariée au roi d'Espagne. Dans une tentative pour développer le commerce et soutenir l'économie anglaise, les conseillers de la reine poursuivirent la politique de Dudley visant à obtenir de nouveaux débouchés commerciaux. Elle accorda une charte royale à la Compagnie de Moscovie dont le premier directeur fut Sébastien Cabot et commanda un atlas au cartographe portugais Diogo Homem.

Financièrement, l'administration anglaise tenta de concilier une forme moderne de gouvernement ayant des dépenses plus élevées avec un système fiscal largement médiéval. Marie maintint dans ses fonctions le lord trésorier nommé par Édouard VI, , et lui demanda de superviser la collecte des impôts et des taxes. En 1558, le gouvernement publia une révision du " listant les droits de douane s'appliquant à toutes les importations ; ce document resta largement inchangé jusqu'en 1604. La monnaie anglaise fut dévaluée sous Henri VIII et Édouard VI et Marie prépara une réforme monétaire qui ne fut cependant pas appliquée avant sa mort.

Après le retour de Philippe en 1557, Marie pensa qu'elle était à nouveau enceinte et qu'elle devrait accoucher en mars 1558. Elle notifia dans son testament que Philippe devrait être régent durant la minorité de son enfant. La grossesse était cependant inexistante et Marie fut contrainte d'accepter qu'Élisabeth soit son successeur.

Marie tomba malade en mai 1558 et elle mourut le 17 novembre à l'âge de 42 ans au palais Saint James durant une épidémie de grippe qui emporta également Reginald Pole le même jour. Elle était affaiblie et souffrait peut-être d'un kyste ovarien ou d'un cancer de l'utérus. Sa demi-sœur Élisabeth lui succéda. Philippe, qui se trouvait à Bruxelles, écrivit à sa sœur Jeanne d'Autriche : .

Malgré ses dispositions testamentaires, Marie ne fut pas enterrée aux côtés de sa mère et elle fut inhumée dans l'abbaye de Westminster le 14 décembre dans un caveau qu'elle partage avec Élisabeth . Après son accession au trône d'Angleterre en 1603, Jacques VI d'Écosse fit ajouter une plaque sur la tombe portant l'inscription en latin : " ().

Lors de la cérémonie funèbre, l'évêque de Winchester, John White, fit les louanges de Marie : . Elle fut la première femme à occuper durablement le trône d'Angleterre malgré une forte opposition et disposait d'un large soutien populaire au début de son règne notamment auprès des catholiques. Les historiens catholiques, comme John Lingard, ont avancé que ses politiques échouèrent non pas parce qu'elles étaient mauvaises mais en raison de la faible durée de son règne et des problèmes météorologiques hors de sa portée. Son mariage avec Philippe se révéla particulièrement impopulaire et ses politiques religieuses créèrent un profond ressentiment qui fut accru par les défaites contre la France. Philippe passa une grande partie de son temps sur le continent laissant la reine éplorée et déprimée par son absence et son incapacité à procréer. Après la mort de Marie , il envisagea d'épouser Élisabeth mais elle refusa. Trente ans plus tard, il envoya l'Invincible Armada pour la renverser sans plus de succès.

Au , les persécutions des protestants par Marie lui valurent le surnom de "" (« Marie la Sanglante »). John Knox l'attaqua dans son "Premier coup de trompette contre le gouvernement monstrueux des femmes" publié en 1558 et elle fut violemment vilipendée dans le "Livre des Martyrs" de John Foxe édité en 1563, cinq ans après sa mort. Les éditions ultérieures de l'ouvrage restèrent populaires auprès des protestants durant les siècles qui suivirent et contribuèrent à la perception de Marie comme un tyran sanguinaire. Au milieu du , l'historienne H. F. M. Prescott tenta de réévaluer la vision traditionnelle d'une reine intolérante et autoritaire et les études plus récentes considèrent les anciennes évaluations avec un plus grand scepticisme. Même si le règne de Marie fut finalement impopulaire et inefficace, les réformes fiscales et l'expansion navale coloniale qui furent par la suite célébrées comme des accomplissements de l'ère élisabéthaine furent initiées par Marie .

Marie a été jouée à l'écran par :

Lorsque Marie monta sur le trône, elle fut proclamée reine de la même manière qu'Henri VIII et Édouard VI :

Après son mariage avec Philippe, le titre conjoint reflétait les possessions des deux époux :

Ce titre, utilisé depuis 1554, fut modifié quand Philippe II hérita de la Couronne d'Espagne en 1556 :

Les armoiries de Marie étaient différentes de celles de ses prédécesseurs depuis Henri IV : Écartelé en 1 et en 3 d'azur à trois fleurs de lys d'or (qui est de France) en 2 et en 4 de gueule aux trois lions léopardés d'or (qui est Angleterre). Ses armoiries étaient parfois associées côte à côte avec celles de Philippe. Elle adopta la devise "" (« la Vérité est la fille du Temps »).




</doc>
<doc id="16922" url="https://fr.wikipedia.org/wiki?curid=16922" title="Gustave Caillebotte">
Gustave Caillebotte

Gustave Caillebotte, né à Paris le et mort à Gennevilliers le , est un peintre français, collectionneur, mécène et organisateur des expositions impressionnistes de 1877, 1879, 1880 et 1882.

Il lègue sa collection de peintures impressionnistes et de dessins à l'État. Passionné de nautisme, membre du Cercle de la voile de Paris, dont le siège est à Argenteuil, Caillebotte est aussi un architecte naval et un régatier qui a marqué son époque.

Gustave Caillebotte est né le à Paris. Il est issu du troisième mariage de son père Martial Caillebotte deux fois veuf (1799-1874) avec Céleste Daufresne (1819-1878), fille d'un avocat de Lisieux et petite-fille de notaire. Deux autres enfants naissent : René, en 1851, et Martial en 1853. Né d’un précédent mariage, leur demi-frère Alfred Caillebotte (1834-1896) est ordonné prêtre en 1858. La famille Caillebotte, originaire de la Manche puis installée à Domfront, fait commerce de drap depuis le et grâce à Martial Caillebotte établi à Paris au début des années 1830 fit fortune dans la vente de draps aux armées de Napoléon III. La boutique nommée "Le Lit militaire" était située au 152 de la rue du Faubourg-Saint-Denis.

En 1857, Gustave Caillebotte entre au lycée Louis-le-Grand de Vanves. Il obtient en avril 1869 le « diplôme de bachelier en droit ». Après obtention de sa licence en droit le 6 juillet 1870, Caillebotte est mobilisé vingt jours plus tard dans la garde nationale mobile de la Seine et participe à la défense de Paris pendant la guerre franco-prussienne. Son livret militaire précise qu'il mesure . Il est démobilisé le 7 mars 1871. La même année, il entre dans l'atelier du peintre académique réputé Léon Bonnat, où il fait la connaissance de Jean Béraud, pour préparer les concours des beaux-arts. En 1872, il effectue un voyage à Naples chez son ami le peintre Giuseppe De Nittis. Ce dernier l'avait introduit auprès d'Edgar Degas. En mars 1873, Caillebotte est reçu quarante-sixième au concours des beaux-arts, mais il n'y restera qu'un an. C'est à cette époque qu'il fait la connaissance de Marcellin Desboutin, d'Henri Rouart et de Claude Monet, qui habite alors à Argenteuil.
La mort de son père, le 25 décembre 1874, laisse deux millions de francs en héritage à partager entre sa veuve, en troisièmes noces, et ses quatre enfants. Martial Caillebotte lègue en plus plusieurs immeubles de rapport à Paris, des fermes , des obligations et surtout des titres de rente sur l'État. Son demi-frère, l'abbé Caillebotte (1834-1896, curé de la nouvelle église Saint-Georges de la Villette, puis de Notre-Dame-de-Lorette) avec cinquante mille livres de rentes, est considéré comme « le curé le plus riche de Paris ce qui lui permit d’en être aussi le plus généreux », en construction et entretien d'œuvres et d'édifices. Céleste Caillebotte, née Daufresne, sa mère, conserve la propriété d'Yerres où Caillebotte peint dès 1872 de nombreuses vues de la région comme "Saules au bord de l'Yerres". Cet héritage considérable permet à Gustave Caillebotte de vivre à l'abri de toute contingence matérielle et de se consacrer pleinement à ses nombreuses passions notamment pour la peinture. Gustave Caillebotte se lie aux artistes impressionnistes, exposant à leurs côtés. Il achète certaines de leurs toiles, finance et organise des expositions. Habitant avec son frère Martial Caillebotte (l'hôtel particulier familial à l’angle de la rue de Miromesnil et de la rue de Lisbonne, puis un appartement au 31 boulevard Haussmann, derrière l'Opéra, de 1878 à 1887), il partage les mêmes passions (jardinage et horticulture, philatélie ou yachting) et le même cercle d'amis jusqu'en 1887, année du mariage de Martial.

En , son tableau "Les Raboteurs de parquet" est refusé au Salon, le sujet heurtant par son extrême quotidien — c'est aujourd'hui l'une de ses œuvres les plus célèbres présentées au musée d'Orsay. Éric Darragon note que « cet échec a dû heurter les convictions de l'artiste et le confirmer dans une opinion déjà acquise à la cause d'un réalisme indépendant. Il va devenir un intransigeant lui aussi et ne reviendra plus devant les jurés [...] ». Ainsi, ce serait cet échec face au jury du Salon qui l'aurait poussé à exposer aux côtés des impressionnistes. Caillebotte présenta des toiles aux expositions impressionnistes qui eurent lieu en 1876, 1877, 1879, 1880 et 1882. Marqué par le refus du Salon de 1875, il passe l'automne à Naples chez Giuseppe et Léontine De Nittis où les deux amis, malgré le mauvais temps, peignent sur le motif. 1876, il montre six toiles à l'exposition impressionniste chez Durand-Ruel, dont "Les Raboteurs de parquet". 

Le décès inattendu de son frère René Caillebotte, à l'automne 1876, conduit Caillebotte, déjà persuadé, comme le note Marie-Josèphe de Balanda, qu'« on meurt jeune dans notre famille », à rédiger son premier testament, chez maître Albert Courtier, notaire à Meaux, le 3 novembre 1876.

À l'automne 1878, la mère de Gustave Caillebotte meurt. La propriété familiale d'Yerres est vendue en juin 1879. Les frères Caillebotte s'installent boulevard Haussmann et achètent une propriété au Petit Gennevilliers où ils font construire juste au bord de la Seine une maison en meulière de deux étages, puis une petite maison à un étage avec un atelier pour Gustave, un hangar à bateaux et une longue serre (en 1888). En 1881, Gustave Caillebotte renonce à exposer à la sixième exposition impressionniste, celle-ci ayant invité des peintres trop éloignés de l'esprit des débuts selon lui.

Il passe d'habitude ses étés sur la côte normande, où il s'adonne au nautisme, mais aussi à la peinture, comme en 1884 à Trouville d'où il écrit à Monet: En septembre-octobre 1885, il voyage avec son frère en Italie.

À partir de 1886, Caillebotte peint de moins en moins. Il s'adonne à ses passions que sont le bateau et le jardinage notamment à partir de 1887, date à laquelle son frère Martial se marie avec Marie Minoret. Gustave Caillebotte quitte donc l'appartement qu'ils occupaient tous les deux et s'installe définitivement en 1888 au Petit Gennevilliers, dans la propriété que les deux frères avaient achetée en mai 1881 après la vente en juin 1879 du domaine familial d'Yerres. Gustave Caillebotte rachète la part de son frère, agrandit son terrain en faisant l'acquisition des parcelles voisines et peint les alentours du Petit Gennevilliers. Il garde toutefois un pied-à-terre à Paris au 29 boulevard de Rochechouart.

Le 6 février 1888, s'ouvre à Bruxelles la exposition des XX, Gustave Caillebotte y est invité avec Armand Guillaumin. Il se consacre ensuite presque exclusivement à l'horticulture (en plus des régates d'été), prétexte à des recherches picturales d'une grande luminosité et passion qui fait l'objet d'une abondante correspondance avec Monet et à des visites réciproques de leurs jardins. Caillebotte est le témoin de mariage civil et religieux de Monet avec Alice Raingo en juillet 1892. Monet se voit offrir un tableau de Caillebotte à cette occasion, "Les Chrysanthèmes blancs et jaunes, jardin du Petit Gennevilliers" qui s'ajoute à "La Leçon de piano" (musée Marmottan Monet) et à une étude sur "Rue de Paris, temps de pluie" qu'il possédait déjà. Caillebotte peint les fleurs de son jardin et les paysages de Gennevilliers.

Le 21 février 1894, le peintre, frappé par une congestion cérébrale, meurt, après avoir pris froid alors qu'il travaillait dans son jardin à un paysage. Il avait quarante-cinq ans. Ses funérailles sont célébrées le 27 février en l'église Notre-Dame-de-Lorette. Il y a tant de monde dans cette église, qui est déjà grande, que certains des amis du peintre doivent suivre la cérémonie sous le porche de l'église. Le peintre est inhumé au cimetière du Père-Lachaise, non loin de Delacroix, dans la chapelle funéraire familiale. La perte de Caillebotte affecta beaucoup les impressionnistes qui perdirent à la fois un protecteur et un compagnon. Pissarro écrivit à son fils Lucien : « Nous venons de perdre un ami sincère et dévoué... En voilà un que nous pouvons pleurer, il a été bon et généreux et, ce qui ne gâte rien, un peintre de talent ».

La maison et le parc qu'il possédait à Yerres, en bordure de la rivière homonyme, sont aujourd'hui propriété communale, et le parc est ouvert au public. C'est là qu'il a peint entre autres certaines scènes de périssoires.

Le talent de Caillebotte fut longtemps méconnu — sauf aux États-Unis —, au profit de son rôle de « mécène éclairé ». Le peintre fut redécouvert dans les années 1970 à l'initiative des collectionneurs américains et reconnu par le grand public francophone à partir des années 1990. Les rétrospectives de ses œuvres sont désormais fréquentes. Certains de ses tableaux se trouvent maintenant au musée d'Orsay, à Paris.
Une exposition de ses œuvres a lieu peu après sa mort en juin 1894 chez Durand-Ruel et un hommage a lieu au Salon d'automne de 1921 avec plusieurs de ses toiles. Mais il faut attendre les années 1950 avant que l'attention des connaisseurs ne se reporte vers ses travaux. De grands collectionneurs américains commencent à montrer au public américain les toiles de Caillebotte leur appartenant au sein de leurs collection, et il est de plus en plus connu aux États-Unis. C'est l'exposition majeure de Houston et de Brooklyn en 1976 qui remet les projecteurs sur cet impressionniste oublié. Celle du Grand Palais, à Paris, fin , est la première exposition majeure en Europe qui le fait connaître du grand public français. Elle est suivie de celle de la fondation de l'Hermitage, à Lausanne, du au .

Une exposition consacrée aux frères Caillebotte (avec les photos originales de Martial Caillebotte) s'est tenue au Musée Jacquemart-André puis au musée national des beaux-arts du Québec entre mars 2011 et janvier 2012.

Une exposition des œuvres originales que Gustave Caillebotte a peintes dans sa propriété familiale d'Yerres s'est tenue en 2014 à la Ferme Ornée, salle d'expositions au sein de cette propriété devenue communale.

Une exposition s'est tenue du 28 juin au 4 octobre 2015 à la National Gallery of Art de Washington, puis au Kimbell Art Museum de Fort Worth du 28 novembre 2015 au 14 février 2016.

Une rétrospective de ses œuvres autour du thème a lieu du 25 mars au 3 juillet 2016 au musée des impressionnismes Giverny, puis du 19 juillet au 30 octobre 2016 au musée Thyssen-Bornemisza de Madrid. C'est la première fois qu'une exposition consacrée à Caillebotte se tient en Espagne.

Les historiens d'art qualifient volontiers cet artiste « d’original et audacieux ». Son œuvre est originale par ses thèmes, notamment l'ennui et l'extrême solitude des personnages dans le nouveau Paris haussmannien, mais aussi à la campagne et au sein même du cercle familial — même dans ce cadre privilégié, les personnages semblent indifférents les uns aux autres. Son œuvre est également originale par sa technique : elle semble proche de l'art photographique, mais, par de puissants effets de perspectives tronquées, les distances et les premiers plans sont écrasés et l'horizon absent, d'où la perception instable et plongeante de ses toiles (Caillebotte invente la vue en plongée dans la peinture). Au point de vue de la finition et de la composition de ses œuvres, on peut dire que Caillebotte est à la première époque de l'impressionnisme ce que Seurat représente pour la seconde période (néo-impressionnisme et pointillisme). Les effets de vue plongeante s'imposent dans son art à travers les personnages au balcon et ses vues en surplomb des rues et des boulevards.

Contrairement aux impressionnistes, qui peignent en plein air des scènes sur le vif, Caillebotte cherche aussi ses motifs à l'extérieur, mais réalise des croquis, retravaille ses esquisses à l'atelier. Dans les années 1890, il est influencé par le courant japoniste.

Caillebotte est l'un des premiers grands peintres français à être exposé régulièrement aux États-Unis, où il rencontre un vif succès. Durand-Ruel organise une exposition d'impressionnistes à l'American Art Association de New York en 1886, où figurent dix toiles de Caillebotte. C'est dans ce pays que se trouvent aujourd'hui nombre de ses toiles, aussi bien dans des musées que dans de grandes collections particulières. Il est l'un des fondateurs du courant « réaliste », qu'illustrera par exemple au l'Américain Edward Hopper.

Fortuné, il n'a pas besoin de vendre ses toiles pour vivre, si bien que les descendants de sa nièce, héritière de Martial Caillebotte (frère de l'artiste), son père, et de son frère Jean Caillebotte (1888-1917) mort au combat, possèdent encore près de 70 % de ses œuvres. À sa mort, son frère Martial et Auguste Renoir, son exécuteur testamentaire, prennent les dispositions pour que l’État accepte le legs de ses tableaux impressionnistes.

Dès le moment où Caillebotte se lie aux impressionnistes, il ne cesse de les aider et ce toujours discrètement. Il achète des toiles aux artistes, finance les expositions impressionnistes. Mais au-delà du mécène et du collectionneur, une amitié durable le lie à la plupart des peintres impressionnistes, comme en témoigne sa correspondance. Il aide financièrement ses amis qui sont dans le besoin, sans nécessairement acheter des toiles, il loue un appartement à Claude Monet près de la gare Saint-Lazare, lui fournissant l'argent nécessaire à l'achat de matériel pour la peinture. Il ne cesse d'aider Camille Pissarro. Auguste Renoir et le collectionneur sont très proches puisque dès 1876, Caillebotte en fait son exécuteur testamentaire et, en 1885, il est le parrain de baptême du fils aîné de Renoir (Pierre) et d’Alice Charigot, sa future épouse.

Au Petit Gennevilliers, où Caillebotte s'installe définitivement en 1888, il reçoit la visite de ses amis comme Monet, Gustave Geffroy, Octave Mirbeau ou encore Renoir qui est un familier du lieu. À la dissolution du groupe des impressionnistes en 1887, Caillebotte permet de maintenir les liens entre les différents artistes en maintenant la tradition des dîners, qui réunissaient peintres et littérateurs, d'abord au café Guerbois, puis à la Nouvelle Athènes. C'est au Café Riche qu'avaient désormais lieu tous les mois ces réunions, et selon les souvenirs de Pierre Renoir, c'était Caillebotte qui payait pour tout le monde.

La composition exacte de la collection est difficile à préciser, en effet la désignation exacte n'en a pas été faite par le donateur. Gustave Caillebotte commence sa collection dès le début de l'Impressionnisme. Il achète plusieurs toiles le 24 mars 1875 à une vente d'impressionnistes à l'hôtel Drouot. Sa première toile de Monet est une œuvre réalisée en 1875 intitulée "Un coin d'appartement". Il achète d'autres tableaux de Monet en février 1876. Il choisit avec goût et discernement parmi les peintres impressionnistes, ceux qui devaient par la suite être reconnus comme les maîtres de la peinture de la fin du . L'examen des peintures acquises par Caillebotte montre que presque toutes appartiennent à la période impressionniste de chaque peintre et représentent ainsi les différents aspects que prit l’impressionnisme de 1874 à 1886. Une exception est à souligner avec les œuvres de Jean-François Millet et de Paul Gavarni qui sont des dessins, les seuls de la collection, et dans les peintures d'Édouard Manet et Paul Cézanne. Ces artistes sont d’ailleurs moins largement représentés dans la collection.

Ce sont les œuvres de la belle époque impressionniste de Renoir qui le représentent. Renoir, à l’époque de "La Balançoire" (1876, musée d'Orsay) et du" Bal du Moulin de la Galette" (1876, musée d'Orsay), pratique plusieurs techniques. Certaines de ces œuvres sont d’une facture lisse, tandis que d’autres, aux touches séparées, aux empâtements granuleux relèvent de la technique impressionniste. Or c’est bien cette technique que l’on retrouve dans les œuvres choisies par Caillebotte comme "La Liseuse" (1874-1876, musée d'Orsay).

De même avec l’œuvre de Degas, le choix des "Femmes à la terrasse d'un café, le soir" (1877, musée d'Orsay) montre bien que le collectionneur recherche dans les œuvres de ses camarades impressionnistes celles qui sont le plus caractéristiques par la nouveauté de leurs conceptions artistiques. Avec ce pastel, Caillebotte choisit une des premières scènes de Degas représentant ces types de cafés et de café-concert, qui font partie des thèmes favoris du Naturalisme et de l’Impressionnisme. Comme l'a remarqué P. Lemoisne : « vers 1878, il garde dans ses peintures son faire lisse et harmonieux de la belle époque alors qu’il a déjà adopté pour ses pastels une facture plus heurtée » et des oppositions de couleurs plus hardies.

La préférence du collectionneur pour les œuvres impressionnistes est encore mise en évidence par le fait que les nombreuses œuvres de Pissarro se situent entre les années 1871 et 1879. Sa manière néo-impressionniste n’est pas représentée dans la collection. Les mêmes constatations pourraient être faites à propos du choix des œuvres de Monet et de Sisley. Il cesse d'acquérir des œuvres en 1886, date de la dernière exposition impressionniste et date où il n'expose plus lui-même.

Son activité de collectionneur s'est aussi étendue à la philatélie, dont il a été un adepte assidu avec son frère musicien Martial Caillebotte. Il a été l'un des fondateurs, avec le docteur Jacques Legrand et Arthur de Rothschild, de la Société française de timbrologie, le .

Gustave et Martial Caillebotte montent cette collection de timbres de manière commune vers 1877. Les Caillebotte furent parmi les premiers à collectionner toutes les nuances d'impression d'un même timbre; ils furent également les pionniers de l'étude des affranchissements, tant et si bien qu'une partie non négligeable de leur collection était constituée de cachets et de surcharges. 
La plus grande partie de cette collection, intégrée à la collection Tapling, peut encore se voir aujourd'hui à la British Library de Londres.

Les Caillebotte rédigèrent une étude sur les timbres mexicains qui fut publiée à Paris par le Timbre-Poste, puis révisée, élargie et traduite dans le "Philatelic Record".

Quand Martial se marie en 1887, ils arrêtent leur collection et ils offrent à Thomas Keay Tapling, un des philatéliste les plus importants d'Angleterre, d'en acquérir tout ce qui peut l'intéresser. Ses achats, qui représentent certainement la plus grande partie, lui coûtent la somme de livres (la vente des frères Caillebotte représente 400 de l'époque, c'est-à-dire plus de ).

La plupart des timbres mexicains aujourd'hui à la British Library de Londres furent réunis par les Caillebotte; or, avec quelque deux cents feuilles, cette section est une des plus fournies de ce qui est finalement devenu la collection Tapling.

Lorsque Tapling meurt en 1891, il lègue sa collection au British Museum de Londres ainsi qu'une somme de livres afin que l'on termine la réorganisation de la collection selon les principes définis par les Caillebotte. Cette réorganisation dura sept ans et rendit indiscernable ce qui émanait de Tapling et de ce qui émanait des deux frères Caillebotte. La collection Tapling est pratiquement la seule à réunir la quasi-totalité des timbres émis dans le monde entre 1840 et 1890. Elle fut donc utilisée par ceux qui compilèrent les travaux de référence; elle a ainsi influencé les catalogues généraux de timbres et pour finir, le mode de collection des collectionneurs.

Sa passion pour le nautisme débute au cours de ses séjours estivaux, à la propriété familiale d'Yerres et sur la rivière du même nom, il canote à bords de barques et de périssoires. Il est séduit par le jeu de l'eau, des bateaux et des hommes. Ce jeu se retrouve très vite dans ses toiles : "Canotiers ramant sur l'Yerres", "Le Canotier au chapeau haut-de-forme" (1878). Puis, il va s'intéresser à la voile à partir de 1876 en devenant membre du Cercle de la voile de Paris (CVP, fondé en 1868) au Petit Gennevilliers, près d'Argenteuil, que fréquentaient Monet, Renoir et Sisley. C'est Alfred Sisley qui l'initie sérieusement à ce sport et il fait de même avec Martial qui devient membre en 1878. En 1878, Gustave Caillebotte achète son premier voilier de régate, l’"Iris", avec lequel il gagne durant la saison de 1879, deux premiers prix et sept autres accessits. Emporté par ces succès de régatier, il s'implique davantage dans le yachting et commande d'autres bateaux, le "Lapin" en 1879, puis l’"Inès" et le "Condor" en 1880, à un des meilleurs constructeurs du bassin d'Argenteuil, le chantier « Texier fils » au Petit Gennevilliers et devient cette année-là vice-président du CVP. Fin 1881, la maison que les frères Caillebotte se sont fait construire au Petit Gennevilliers est terminée et l'année suivante, en 1882, Gustave, cherchant à perfectionner ses bateaux, va se lancer dans l'architecture navale, avec l'aide de son ami Maurice Brault (sujet de "L'Homme au balcon", coll. part.) et dessiner le "Jack", son premier voilier, puis en 1883 se sera le "Cul-blanc", un clipper d'Argenteuil et en 1885 "La Pioche", un dériveur.

Amateur passionné et progressiste, Gustave Caillebotte expérimente de nombreuses innovations en comparant notamment les formes de coque des voiliers américains ( dériveurs larges et peu lestés, surnommés ) et les bateaux issus de l'école anglaise (quillards étroits à fort tirant d'eau, lourdement lestés, les ou les ). Il innove aussi sur le plan du gréement : pour son voilier "Condor", il confectionne un spinnaker immense et très coûteux en soie artificielle. Il réalise sur cette voile une superbe peinture (hélas aujourd'hui disparue) représentant... une tête de chatte triangulaire, qui indique que le nom du bateau, à prononcer en deux syllabes distinctes, n'a rien à voir avec le vautour des Andes et est en fait... une plaisanterie grivoise typique de l'esprit des canotiers de l'époque, se référant à l'anatomie féminine. 

En 1886, le "Mouquette", un côtre sur plan Chevreux pour Caillebotte, est le premier bateau construit par le chantier Luce. Ce chantier naval voit le jour à la fin de l'année 1885, au Petit Genevilliers, sous le patronage de Gustave Caillebotte qui s'associe à Ferdinand Luce, constructeur de bateaux, à qui il fait construire une maison voisine qu'il lui loue. Le troisième associé est Maurice Chevreux, architecte naval. C'est dans ce chantier, dont il est partie prenante, que seront construits tous les voiliers conçus par Caillebotte. En 1888, il s'installe à demeure au Petit Gennevilliers, il navigue sur le "Thomas", un plan Chevreux avec lequel il remporte de nombreuses régates sur le bassin d'Argenteuil, ainsi qu'au Havre et à Trouville et dessinera l’"Arriet" cette année-là. Caillebotte est l'un des promoteurs, en 1889, de la jauge des du CVP, il dessine et fait construire pour d'autres (le "Moucheron" en 1890, le "Lézard" en 1891) et pour lui l’"Arico" en 1891, etc.) plusieurs voiliers de cette jauge, dont le "Roastbeef" de 1892, que l'on retrouve dans plusieurs de ses toiles : "Bateau à voile sur la Seine" ou "Régates à Argenteuil" . Il est également l'architecte du "Dahud" en 1893, considéré comme son chef-d'œuvre, et du "Mignon", lancé en 1894 après la mort de son concepteur.

Il posséda trente-deux bateaux et dessina les plans de vingt-deux voiliers entre 1882 et 1893. À côté de ses œuvres architecturales, Gustave Caillebotte a possédé quatorze voiliers de courses, qui remportèrent avec lui plus d'une centaine de prix, Martial Caillebotte continuant les régates sur certains d'entre eux jusqu'en 1900.

Personnage aux multiples facettes, Gustave Caillebotte était également un horticulteur émérite. Monet et Caillebotte partagent tous deux la même passion pour le jardinage. Au Petit Gennevilliers, où il réside définitivement depuis 1888, il possède une grande serre, mais, contrairement au jardin de Monet (avec qui il échange graines et conseils) à Giverny, celui de Caillebotte est géométriquement dessiné, tracé au cordeau. Dans sa serre sont enfermées les plantes les plus précieuses, parmi lesquelles ses orchidées d'une rare diversité qui vont être l'objet de ses études picturales. Un tiers de ses œuvres est consacré à la représentation des jardins.

Ayant agrandi sa propriété en rachetant les parcelles voisines, son terrain atteint plus d'un hectare en 1888. Il fait installer un système performant d'arrosage automatique. Il est élu conseiller municipal de Gennevilliers en 1888, jusqu'à sa démission en 1891.
La propriété est bombardée par l'aviation alliée à l'été 1944 et la grande maison et les dépendances sont totalement détruites. Le terrain sert ensuite à la construction d'une usine de la Snecma.

C’est le legs de Caillebotte qui ouvrit aux impressionnistes les portes des musées nationaux. Cette collection a été formée à l’époque même qui vit naître les peintres qui la composent. Au moment où il prenait place dans les rangs des impressionnistes, Gustave Caillebotte avait déjà commencé sa collection. Son premier testament par lequel il léguait à l’État les tableaux qu’il possédait fut rédigé le 3 novembre 1876 ; la liste des tableaux n’était pas encore dressée, mais il est évident, en raison de la date même du testament, qu'il ne pouvait y avoir alors qu’une partie des œuvres qui constituèrent, quelques années plus tard, la collection. Un codicille du testament concernant une exposition à organiser en 1878 nous apprend déjà quels sont les peintres qui bénéficieront de sa sollicitude. Ce sont Degas, Monet, Pissarro, Renoir, Cézanne, Sisley et Berthe Morisot.

C'est le brusque décès de son frère René, à l'âge de vingt-six ans, à l'automne 1876, qui le conduit, déjà persuadé, à rédiger son premier testament en 1876 : 
« Je donne à l’État les tableaux que je possède ; seulement comme je veux que ce don soit accepté et le soit de telle façon que ces tableaux n'aillent ni dans un grenier ni dans un musée de province mais bien au Luxembourg et plus tard au Louvre, il est nécessaire qu'il s'écoule un certain temps avant l'exécution de cette clause jusqu'à ce que le public, je ne dis pas comprenne, mais admette cette peinture. Ce temps peut être de vingt ans ou plus ; en attendant, mon frère Martial et à son défaut un autre de mes héritiers les conservera. Je prie Renoir d'être mon exécuteur testamentaire et de bien vouloir accepter un tableau qu'il choisira ; mes héritiers insisteront pour qu'il en prenne un important ».

Le 11 mars 1894, Renoir informe par une lettre Henri Roujon, de la direction des Beaux-Arts, que Gustave Caillebotte, décédé le 21 février 1894, lègue à l'État sa collection, comprenant soixante-sept œuvres, de Degas, Cézanne, Manet, Monet, Renoir, Pissarro et Sisley.

Plus de dix-sept ans s’étaient écoulés depuis le jour où Caillebotte décidait de léguer ses œuvres à l'État. De vives protestations accompagnèrent le legs de la part d'artistes officiels, mais également de politiques. L'Académie des beaux-arts protesta officiellement contre l'entrée de ces tableaux au musée du Luxembourg, en qualifiant l'événement d'« offense à la dignité de notre école ». Le peintre Jean-Léon Gérôme écrit dans le "Journal des Artistes" : 

Le 19 mars 1894, l'ensemble du Comité étudie les œuvres offertes. Elles sont présentées dans un atelier situé au 11 boulevard de Clichy, loué à cet effet par Renoir, en présence de celui-ci et de Martial Caillebotte. Dans le procès-verbal de la séance du Comité consultatif du 20 mars, il est noté que les deux hommes auraient été informés que l'entrée d'une œuvre au Louvre ne pouvait être examinée qu'au minimum dix ans après la mort de son auteur, et que le manque de place au Luxembourg et la limitation à trois œuvres de chaque artiste représenté rendaient impossible l'exposition de tous les tableaux composants le legs. Dès le lendemain, le Comité consultatif des musées nationaux vote pourtant l'acceptation du legs dans son intégralité « pour les musées nationaux avec placement au musée du Luxembourg ». Le Comité accepte en plus une toile de Gustave Caillebotte, "Les Raboteurs de parquet", donnée par ses héritiers. Léonce Bénédite précise que la place manque au Luxembourg pour exposer même le tiers de la collection, mais « estime qu'il serait possible de construire sur la terrasse du musée un baraquement provisoire où serait réuni le legs Caillebotte ». Le 17 janvier 1895, le directeur des Beaux-Arts organise une réunion dans son cabinet avec les représentants de l'Administration et les notaires. Sont présents Martial Caillebotte et Auguste Renoir. De cette consultation, il est conclu qu'une exécution rigoureuse du testament est difficilement réalisable, et qu'il faut maintenant trouver une solution acceptable par toutes les parties. Il est décidé que l'Administration choisira les tableaux qu'elle veut exposer. Martial Caillebotte deviendra possesseur des autres œuvres. Les raisons données par l’Administration sont les suivantes : tout d'abord, l’étroitesse des locaux du musée du Luxembourg, qui ne permet plus de laisser entrer aucun ouvrage sans en retirer un autre ; et les règlements qui, par un sentiment d'équité, limitent le nombre des ouvrages pour un même artiste.

La proposition est finalement arrêtée en janvier 1895. L'approbation du Conseil d'État met un certain temps, mais un décret ministériel finira, le 25 février 1896, par autoriser le choix des œuvres effectué. On construit alors une annexe au musée du Luxembourg pour y accrocher ces œuvres. Puis le 23 novembre 1896, les œuvres de la collection sont officiellement remises à l'État. La collection, réduite à trente-huit tableaux, est présentée au public au début de l'année 1897 dans une des trois nouvelles salles de l'annexe du Luxembourg consacrée aux impressionnistes et au legs Caillebotte. Les salles furent construites sur la terrasse du musée.

Ainsi, plus de vingt ans après la rédaction du testament, les œuvres entrèrent dans les musées nationaux. Le transfert du legs Caillebotte au musée du Louvre eut lieu en 1929. Entre-temps une rétrospective Caillebotte s'était tenue au Salon d'automne de 1921. Après la guerre, en 1947, le musée de l'Impressionnisme s'ouvre au Jeu de Paume. La collection sera transférée au musée d'Orsay à son ouverture en 1986.

À cette liste il faut ajouter deux dessins de Millet (qui furent acceptés) et un de Gavarni (qui figure dans l'inventaire après décès, mais non dans la liste transmise à l'administration).

On peut voir par ce legs la volonté de Caillebotte de permettre à un courant artistique d'exister et de gagner en reconnaissance. Il veut par ce geste faire entrer les impressionnistes dans les collections des musées nationaux.

Les œuvres conservées au musée d'Orsay et au musée du Louvre sont celles acceptées par l'État :

Paul Cézanne

Edgar Degas


Édouard Manet


Claude Monet


Camille Pissarro


Auguste Renoir


Alfred Sisley


Paul Gavarni


Jean-François Millet


L'œuvre de Caillebotte représente quatre cent soixante-quinze tableaux, dont :





</doc>
<doc id="16926" url="https://fr.wikipedia.org/wiki?curid=16926" title="Amas globulaire">
Amas globulaire

En astronomie, un amas globulaire est un amas stellaire très dense, contenant typiquement une centaine de milliers d'étoiles distribuées dans une sphère dont la taille varie d'une vingtaine à quelques centaines d'années-lumière. Leur densité est ainsi nettement plus élevée que celle des amas ouverts. Les étoiles de ces amas sont généralement des géantes rouges.

On compte globulaires dans notre galaxie, la Voie lactée. Mais il en existe sans doute d'autres, indétectables car masqués par le disque galactique.

Les amas globulaires font partie du halo galactique. Ils orbitent autour du centre galactique à une distance variant de . C'est par leur étude que Harlow Shapley, en 1918, a pu déterminer la position du Soleil au sein de la Galaxie. Comme les amas globulaires contiennent les étoiles les plus âgées d'une galaxie, ils contribuent également de façon importante à l'étude de l'évolution des étoiles et des galaxies.

La plupart des amas globulaires sont très anciens et se sont probablement formés en même temps que leur galaxie hôte. Néanmoins, certains amas globulaires de couleur bleue ont été récemment observés et leur couleur est, normalement, représentative des étoiles chaudes et jeunes.
On ne sait pas encore si des amas globulaires peuvent se former relativement tard dans la vie d'une galaxie.

Certains amas globulaires, comme Omega Centauri de notre Galaxie, peuvent avoir une masse de plusieurs millions de masses solaires.

Certaines étoiles de type particulier, comme les traînardes bleues ('), les pulsars-millisecondes ou les binaires X de faible masse ('), sont beaucoup plus communes dans les amas globulaires.

La densité des étoiles dans les amas globulaires étant très élevée, les collisions ou quasi-collisions entre étoiles y sont parfois possibles, contrairement aux autres régions d'une galaxie.

Lorsqu'on a pris en compte la distance des amas globulaires, il est apparu que leur distribution était fortement asymétrique et que la partie observable du disque galactique n'en constituait qu'une fraction, le reste étant obscurci par le gaz et la poussière du disque galactique.

Ils orbitent autour du centre galactique à une distance variant de .

Le premier amas globulaire M22 a été découvert en 1665 par Johann Abraham Ihle, un astronome amateur allemand. À cause de la faible ouverture des télescopes de cette époque, les étoiles individuelles des amas ne pouvaient être résolues. Le premier à obtenir ce niveau de détail fut Charles Messier quand il observa l'amas M4. Les huit premiers amas observés apparaissent dans le tableau ci-contre. Plus tard, l'abbé Lacaille lista les amas , , M55, M69 et dans son catalogue datant de 1751–1752. Le "M" avant le numéro de l'amas fait référence au catalogue de Charles Messier, tandis que "NGC" vient du catalogue "" établi par John Dreyer.

William Herschel commença un programme d'observation en 1782, utilisant un télescope plus grand capable de séparer les étoiles des globulaires connus à ce moment. Au passage, Il découvrit amas. Le premier à utiliser le terme "amas globulaire" fut Herschel dans son catalogue des objets lointains de 1789.

Le nombre d'amas globulaires découverts augmenta régulièrement, atteignant 83 en 1915, 93 en 1930 et 97 vers 1947. Au total, globulaires sont recensés dans notre galaxie, sur un total estimé de 180 ± 20. On pense que de nombreux amas sont cachés derrière le nuage de gaz et de poussière du noyau galactique.

Au début de l'année 1914, Harlow Shapley débuta une série d'études des amas globulaires, publiées dans une quarantaine d'articles scientifiques. Il observa des étoiles céphéides variables dans les amas, ce qui lui permit de déterminer leur distance (en correspondance avec leur luminosité).

La plupart des amas globulaires de la Voie lactée est observée à proximité du noyau galactique et une majorité apparaît dans la partie du ciel céleste centrée sur le noyau (autour de la constellation du Sagittaire). En 1918, cette distribution très asymétrique a été utilisée par Harlow Shapley pour déterminer les dimensions de notre galaxie dans son ensemble. En prenant l'hypothèse que les amas globulaires suivaient une distribution plus ou moins sphérique autour du centre de la galaxie, il utilisa leur position pour calculer la position du Soleil dans la Voie lactée. Ce faisant, Shapley a ramené le Soleil (dont on sait maintenant qu'il se trouve à environ du centre ) à sa véritable place, la périphérie de notre galaxie et non le centre. Il a ainsi montré que la taille de notre galaxie est bien plus grande que ce qu'on pensait auparavant.
Les distances estimées par Shapley étaient faussées parce qu'il n'avait pas tenu compte de l'absorption de la lumière des astres observés en provenance de certains amas par la poussière galactique, faisant paraître ces amas plus lointains. Son estimation est cependant du même ordre de grandeur que la taille actuellement admise.

Les mesures de Shapley indiquaient aussi que le Soleil était relativement loin du centre de la Galaxie, contrairement aux données de son époque reposant sur la distribution régulière des étoiles ordinaires dans le ciel. En fait, les étoiles ordinaires sont souvent obscurcies par le gaz et les poussières du disque galactique alors que les amas globulaires sont en dehors de ce disque et peuvent être vus de beaucoup plus loin.

Henrietta Swope et Helen Battles Sawyer ont participé plus tard aux travaux de Shapley. Entre 1927 et 1929, Harlow Shapley et Helen Sawyer commencèrent à classer les amas selon leur densité en étoiles. Les amas les plus denses sont ainsi dits de , jusqu'aux amas les moins denses de . Le système de H. Shapley et H. B. Sawyer issu de cette classification est aujourd'hui utilisé.



</doc>
<doc id="16931" url="https://fr.wikipedia.org/wiki?curid=16931" title="Bijection réciproque">
Bijection réciproque

En mathématiques, la bijection réciproque d'une bijection ƒ est l'application qui associe à chaque élément de l'ensemble d'arrivée son unique antécédent par ƒ. On l'appelle parfois l'application inverse de ƒ (voir ).

On considère l'application ƒ de R vers R définie par :

Pour chaque réel "y", il y a un et un seul réel "x" tel que
ainsi pour "y" = 8, le seul "x" convenable est 2, en revanche, pour "y" = –27 c'est –3. En termes mathématiques, on dit que "x" est l'unique antécédent de "y" et que ƒ est une bijection.

On peut alors considérer l'application qui envoie "y" sur son antécédent, qu'on appelle dans cet exemple la racine cubique de "y" : c'est elle qu'on nomme la « réciproque » de la bijection ƒ.

Si on tente d'effectuer la même construction pour la racine carrée et qu'on considère l'application "g" de R vers R définie par :
les choses ne se passent pas si simplement. En effet, pour certaines valeurs de "y", il y a deux valeurs de "x" tels que "g"("x") = "y" ; ainsi, pour "y" = 4, on peut choisir "x" = 2 mais aussi "x" = –2, puisque 2 = 4 mais aussi (–2) = 4. À l'inverse, pour d'autres choix de "y", aucun "x" ne convient ; ainsi pour "y" = –1, l'équation "x" = –1 n'a aucune solution réelle. En termes mathématiques, on dit que "g" n'est ni injective ni surjective. Dans cet exemple, les définitions qui suivent ne permettent pas de parler de « bijection réciproque » (ni même d'« application réciproque ») de "g".

Si ƒ est une bijection d'un ensemble X vers un ensemble Y, cela veut dire (par définition des bijections) que tout élément "y" de Y possède un antécédent et un seul par ƒ. On peut donc définir une application "g" allant de Y vers X, qui à "y" associe son unique antécédent, c'est-à-dire que
L'application "g" est une bijection, appelée bijection réciproque de ƒ.

De façon plus générale, et en utilisant les notations fonctionnelles, si ƒ est une application d'un ensemble X vers un ensemble Y et s'il existe une application "g" de Y vers X telle que :
alors ƒ et "g" sont des bijections, et "g" est la "bijection réciproque" de ƒ.

La bijection réciproque de ƒ est souvent notée ƒ, en prenant garde à la confusion possible avec la notation des exposants négatifs, pour laquelle on a "x" = 1/"x".

La double propriété :
montre que ƒ est aussi la bijection réciproque de ƒ, c'est-à-dire que

La réciproque de la composée de deux bijections est donnée par la formule
On peut remarquer que l'ordre de ƒ et "g" a été inversé ; pour « défaire » ƒ suivi de "g", il faut d'abord « défaire » "g" puis « défaire » ƒ.

Certaines bijections de E vers E sont leur propre réciproque, c'est le cas par exemple de l'application inverse
ou de toute symétrie orthogonale dans le plan.

De telles applications sont dites involutives.

Le théorème des valeurs intermédiaires et son corollaire, le théorème de la bijection, assurent que toute application continue strictement monotone sur un intervalle I détermine une bijection de I sur ƒ(I) = J et que J est aussi un intervalle. Cela signifie qu'une telle fonction possède une application réciproque définie sur J à valeurs dans I.

Cette propriété permet la création de nouvelles fonctions définies comme application réciproque de fonctions usuelles.

À l'aide de ces fonctions, la recherche de l'application réciproque consiste à résoudre l'équation ƒ("x") = "y", d'inconnue "x" :

La fonction formula_8 est une bijection de sur et possède une application réciproque que l'on cherche à déterminer en résolvant, pour "y" dans , l'équation "x" + 3 = "y", ou encore "x" = "y" – 3. Puisque "y" ≥ 3, cette équation possède deux solutions dont une seule appartenant à l'intervalle : "x" = –. Donc la réciproque de ƒ est ƒ définie par ƒ("y") = –.

Cette recherche peut se révéler infructueuse et nécessiter la création d'une fonction nouvelle. Ainsi, la fonction formula_9 est une bijection de vers ; l'équation correspondante formula_10 n'a pas de solution exprimable à l'aide des fonctions usuelles, ce qui oblige, pour exprimer "x" = ƒ("y"), à définir une nouvelle fonction, la fonction W de Lambert.

Lorsque deux fonctions sont réciproques l'une de l'autre, alors leurs représentations graphiques dans un plan muni d'un repère orthonormal sont symétriques l'une de l'autre par rapport à la droite (D) d'équation "y" = "x" (appelée aussi première bissectrice).

En effet, si M("x", "y") est un point du graphe de ƒ, alors "y" = ƒ("x") donc "x" = ƒ("y") donc M'("y", "x") est un point du graphe de ƒ. Or le point M'("y", "x") est le symétrique du point M("x", "y") par rapport à la droite (D), pour les deux raisons suivantes :

Le milieu du segment [M, M'] est sur la droite (D), et d'autre part, le vecteur formula_11 est orthogonal au vecteur de coordonnées (1, 1), qui est un vecteur directeur de la droite (D) (leur produit scalaire canonique est nul).

On sait donc que "s"(M) est un point du graphe de ƒ. Un raisonnement analogue prouve que si M est un point du graphe de ƒ, alors "s"(M) est un point du graphe de ƒ.

En général, la réciproque d'une fonction continue n'est pas continue mais la réciproque d'une fonction continue sur un intervalle I à valeurs dans un intervalle J est une fonction continue sur J. On trouve une démonstration dans l'article "Théorème d'inversion locale".

Si formula_12 est une fonction continue sur un intervalle formula_13 à valeurs dans un intervalle formula_14 et si formula_15 est sa réciproque, la fonction formula_15 est dérivable en tout point formula_17 tant que formula_12 admet en formula_19 une dérivée non nulle.

La dérivée en formula_17 de formula_15 est alors
Un moyen simple de comprendre, mais non de démontrer, ce phénomène est d'utiliser les notations différentielles et de remarquer que :


</doc>
<doc id="16933" url="https://fr.wikipedia.org/wiki?curid=16933" title="Mécanique hamiltonienne">
Mécanique hamiltonienne

La mécanique hamiltonienne, inventée par William Rowan Hamilton en 1833, est une reformulation de la mécanique classique. Son formalisme a facilité l'élaboration théorique de la mécanique quantique.

En mécanique lagrangienne, les équations du mouvement d'un système à "N" degrés de liberté dépendent des coordonnées généralisées formula_1 et des vitesses correspondantes formula_2, où formula_3.

Le lagrangien peut donc s'écrire formellement comme une fonction : formula_4, les variables indexées représentant les formula_5 variables de ce type.

En mécanique hamiltonienne, chaque vitesse généralisée est remplacée par la quantité de mouvement associée, aussi appelée "moment conjugué" ou encore "impulsion généralisée" :

En coordonnées cartésiennes, les quantités de mouvement sont équivalentes aux moments linéaires, alors qu'en coordonnées polaires elles correspondent aux moments angulaires. Lorsque les coordonnées généralisées sont choisies arbitrairement, il n'est plus possible de donner une interprétation intuitive aux moments conjugués.

L'hamiltonien formula_6 est la transformée de Legendre du lagrangien :

Dans le membre de droite de cette formule, les vitesses sont supposées être exprimées en fonction des moments conjugués.

Si les équations qui définissent les coordonnées généralisées sont indépendantes du temps formula_7, on peut montrer que formula_6 est égal à l'énergie totale formula_9, elle-même étant égale à la somme de l'énergie cinétique formula_10 et de l'énergie potentielle formula_11 (formula_12).

Sous forme différentielle, les deux membres de la définition de formula_6 deviennent :

En utilisant la définition des moments conjugués donnée précédemment et les équations d'Euler Lagrange traduisant le principe de l'action minimale du lagrangien, on obtient les équations du mouvement de Hamilton, dites "équations canoniques de Hamilton" :

Note: l'égalité formula_14 se démontre comme suit:

formula_15

Où on a utilisé pour la dernière égalité la définition des moments conjugués et les équations d'Euler Lagrange.

Les équations de Hamilton sont des équations différentielles du premier ordre et donc plus faciles à résoudre que les équations de Lagrange qui sont du second ordre.
Néanmoins, les étapes qui conduisent à ces équations sont plus complexes que celles de la mécanique lagrangienne :
à partir des coordonnées généralisées et du lagrangien, il faut calculer l'hamiltonien, exprimer les vitesses généralisées en fonction des moments conjugués et remplacer celles-ci dans la définition de l'hamiltonien.

La méthode de Lagrange est moins lourde en termes de manipulations mathématiques. L'avantage principal de l'approche hamiltonienne est de fournir, grâce à la simplicité de son formalisme, un fondement théorique en mécanique. Par exemple, la mécanique quantique utilise un formalisme basé sur celui de la mécanique hamiltonienne.

On pourra aussi noter une certaine similitude entre les équations canoniques de Hamilton et les équations de Maxwell.

Soit une particule non relativiste de masse formula_16 se déplaçant sur un axe. On repère la position de cette particule par une coordonnée formula_17. Supposons de plus que la particule est soumise à une force qui dérive de l'énergie potentielle formula_18. Le lagrangien s'écrit alors :

Le moment conjugué vaut alors :

il s'identifie à la quantité de mouvement habituelle. Cette formule peut être inversée :

On obtient alors le hamiltonien par transformée de Legendre :

Les équations canoniques conduisent alors à :

et à l'équation de la dynamique de Newton :

Considérons un système à formula_5 degrés de liberté décrits à l'instant formula_7 par :



À chaque instant, les formula_29 coordonnées formula_30 définissent un point formula_31 dans l'espace des phases formula_32 à formula_29 dimensions.

Considérons un système à formula_5 degrés de liberté dont les formula_5 coordonnées généralisées formula_36 précisent la position d'un point formula_37 sur une variété différentielle formula_38 à formula_5 dimensions. Le moment conjugué formula_40 est alors un élément de l'espace cotangent formula_41 dans la direction formula_42.

À chaque instant, les formula_29 coordonnées formula_44 définissent dans ce cas un point formula_31 dans l'espace des phases formula_46 qui s'identifie à l'espace fibré cotangent à "2N" dimensions. Cet espace des phases est naturellement muni de la forme symplectique formula_47 définie par :

formula_48

L'évolution dynamique du système selon les équations canoniques de Hamilton à partir d'une condition initiale formula_49 engendre le flot hamiltonien formula_50, c’est-à-dire le groupe continu à un paramètre tel que :

La succession des positions formula_31 dans l'espace des phases se traduit par une courbe "continue", appelée "orbite".

Le flot hamiltonien préserve la mesure de Liouville sur l'espace des phases. Lorsque celui-ci est euclidien, cette mesure invariante sous le flot est simplement la mesure de Lebesgue sur formula_52 :

La démonstration de ce théorème repose sur le fait que la divergence de la « vitesse » dans l'espace des phases est nulle :

où on a utilisé les équations canoniques pour conclure. Autrement dit, le « fluide hamiltonien » dans l'espace des phases est incompressible.

Un système hamiltonien "invariant par translation dans le temps" satisfait toujours à la conservation de l'énergie :

de telle sorte que sa dynamique est en fait toujours restreinte à une hypersurface formula_53 à formula_54 dimensions. Dans ce cas, la mesure de Liouville invariante sous le flot dans l'espace des phases induit une mesure invariante sous le flot sur l'hypersurface d'énergie constante, définie par :

</math>

où formula_55 est la mesure sur l'hypersurface formula_56 induite par la métrique sur l'espace des phases.

Il peut exister d'autres constantes du mouvement indépendantes de l'énergie en plus de celle-ci. Lorsqu'un système invariant par translation défini sur formula_57dans le temps possède formula_5 constantes du mouvement indépendantes, on dit qu'il est intégrable. Sa dynamique est alors particulièrement simple.








</doc>
<doc id="16934" url="https://fr.wikipedia.org/wiki?curid=16934" title="Moruroa">
Moruroa

Moruroa, aussi transcrit en Mururoa et historiquement appelé Aopuni, est un atoll de l’archipel des Tuamotu, situé en Polynésie française. Celui-ci a servi, comme un autre site de l’océan Pacifique, l’atoll de Fangataufa distant de , de lieu d’expérimentation à 138 essais nucléaires français. Moruroa appartient en pleine propriété à l’État français depuis 1964.

Moruroa est un atoll de de longueur et de largeur maximales pour une superficie de terres émergées d’environ situé à au sud-est de Tahiti. L’atoll est composé de plusieurs motus de tailles variées.

D’un point de vue géologique, l’atoll est l’excroissance corallienne (de ) du sommet d'un des plus importants monts volcaniques sous-marins de la (d'un volume de ), qui mesure depuis le plancher océanique et qui s'est formé il y a environ 32,9 à 42,6 millions d'années.

La première mention de l’atoll par un Européen est faite par Philip Carteret le 11 juin 1767 quelques jours après sa découverte de l'île Pitcairn. Il baptise l'atoll du nom de "Bishop of Osnaburgh Island" (île de l’évêque d'Osnaburgh). Le 25 février 1792, la baleinière britannique "Matilda" fait naufrage à proximité de Moruroa où les rescapés, commandés par le capitaine Weatherhead, trouveront refuge et à partir de laquelle ils rejoindront Tahiti en canots de fortune. En avril 1823, c'est le navigateur français Louis Isidore Duperrey qui visite Moruroa, à bord du navire "La Coquille", puis c'est au tour du Britannique Frederick William Beechey de l'aborder le 26 janvier 1826.

En 1964, l’Assemblée territoriale de Polynésie cède gratuitement à l’État français Moruroa et Fangataufa, déjà occupé par l’Armée, par une délibération précisant """

Ce site présentait des critères alors jugés adaptés pour y tester des armes nucléaires : lieu éloigné et désertique, n’étant jouxté que par une faible densité de population (moins de dans un rayon de 500 km et moins de dans un rayon de ), venté avec un régime de vents dirigeant le nuage radioactif vers d’océans réputés déserts. Les deux atolls sont classés "terrain militaire" en 1964 puis en "zones protégées de défense nationale".

Le premier des 138 essais effectués au total à Moruroa est réalisé le . 

Les essais nucléaires français suscitent des inquiétudes et des oppositions locales et internationales. En juillet 1973 le voilier Fri (liberté en Danois) parti de Nouvelle Zélande se dirige vers Moruroa pour protester contre les essais nucléaires . Il est arraisonné par la marine française le 17 juillet. Le , le "Rainbow Warrior", un bateau de l’organisation écologiste Greenpeace en route vers l’atoll est coulé à Auckland en Nouvelle-Zélande par des agents des services secrets français, causant la mort du photographe portugais Fernando Pereira et provoquant le scandale de l’affaire du "Rainbow Warrior".
Alors que la France observe depuis plusieurs années un moratoire sur les essais nucléaires, le nouveau président français Jacques Chirac autorise en 1995, une dernière campagne d’essais, avant la ratification du traité d’interdiction complète des essais nucléaires. Ces essais ont pour objectif de valider différents modèles permettant des simulations ultérieures en laboratoire. Ils provoquent une vive campagne internationale de protestations allant jusqu’au boycott, avec en pointe les pays d’Océanie et des organisations internationales, dont Greenpeace. Cette campagne d’essais nucléaires prend fin l’année suivante. L’évolution de la géologie et de la radioactivité de l’atoll est depuis surveillée attentivement par l’armée française. Une étude de l'Inserm menée de 2002 à 2005 affirme dans sa conclusion que ". En outre, selon le Conseil économique, social et culturel de la Polynésie française (en 2006), ils ont ' et '.

Moruroa et Fangataufa appartiennent en pleine propriété à l’État français depuis 1964. Malgré le vote en 2012 par le Sénat d’une proposition de loi proposant leur rétrocession à la collectivité de Polynésie française, le gouvernement Ayrault n’inscrit pas ce texte à l’ordre du jour de l’Assemblée nationale.





</doc>
<doc id="16935" url="https://fr.wikipedia.org/wiki?curid=16935" title="Nébuleuse de la Lyre">
Nébuleuse de la Lyre

La Nébuleuse de la Lyre (en anglais '), cataloguée dans le Catalogue de Messier sous le nom M57, est une nébuleuse planétaire située dans la constellation de la Lyre. Sa forme caractéristique lui vaut également le surnom de nébuleuse de l'Anneau. Elle a entre et .

Elle se situe à environ de la Terre.

M57 est parmi les objets les plus connus du catalogue de Messier. Elle a été découverte en 1779 par Antoine Darquier de Pellepoix. Le diamètre réel de l'anneau est de , soit un diamètre apparent d'environ .

La région la plus intérieure de l'anneau apparaît plus sombre, car elle émet surtout des rayons ultraviolets. La teinte bleu-vert des régions centrales provient des raies interdites dues à l'oxygène doublement ionisé. Dans les régions extérieures de l'anneau, la couleur rouge provient de la raie H-alpha de l'hydrogène et des raies interdites de l'azote ionisé à 654,8 et .

L'étoile centrale est une naine blanche un peu plus massive que le Soleil. C'est une étoile très chaude, puisque sa température atteint les . Enfin, elle a une magnitude apparente de 14,8.

Cette nébuleuse est présente dans le ciel de l'hémisphère nord et observable toute l'année, dans les meilleures conditions entre mai et septembre. Elle se situe dans la Lyre, l'un des sommets du Triangle d'été, ce qui facilite sa recherche (quand elle occupe le zénith).

Sa magnitude n'est que de 8,8, elle est donc invisible à l'œil nu. Pour l'observer (et distinguer l'anneau), il faut s'équiper d'un petit télescope ou d'une lunette astronomique. Son diamètre apparent est assez faible, ce qui diminue sa visibilité. 

Sa recherche est simple :



</doc>
<doc id="16938" url="https://fr.wikipedia.org/wiki?curid=16938" title="Province de Luxembourg">
Province de Luxembourg

La province de Luxembourg ("province do Lussimbork" en wallon, "Provënz Lëtzebuerg" en luxembourgeois) est une province belge de la Région wallonne située à l'extrême sud du pays et avoisinant le Grand-Duché de Luxembourg.

Elle a pour chef-lieu Arlon qui est situé dans le sud-est de la province. Elle est aussi appelée Luxembourg belge et ne doit pas être confondue avec le Grand-Duché de Luxembourg dont le « quartier wallon » a été cédé à la Belgique sur base d'un critère linguistique en 1839, à l'exception du Pays d'Arlon qui a bien pour langue vernaculaire le luxembourgeois.

D'une superficie de , elle est la plus grande province de Belgique, alors qu'elle est la moins peuplée avec habitants au , hommes et femmes, soit une densité de population de habitants au km.
Administrativement, elle est partagée en cinq arrondissements et (depuis les fusions de 1977) en quarante-quatre communes.

La province est délimitée au nord-ouest par la province de Namur, au nord-est par la province de Liège, à l'est par le Grand-Duché de Luxembourg (cantons de Clervaux, Wiltz, Redange, Capellen et d'Esch-sur-Alzette) et au sud par les départements français de Meurthe-et-Moselle, Meuse et Ardennes (Grand Est).

Le territoire couvre les sous-régions naturelles suivantes :

Avec d'altitude, la Baraque de Fraiture (commune de Vielsalm) est le point culminant de la province.

Les cours d'eau principaux sont les rivières suivantes :

L'histoire de la province est naturellement liée à celle de la Belgique et du territoire actuel du Benelux. Les plus anciens vestiges remontent, comme ailleurs, à la préhistoire, notamment les abris sous roche et les occupations de grottes fouillés dans la vallée de la Haute Lesse. La période des âges des métaux est illustrée par les fouilles de tombes à char, des marchets (en Famenne) et celles des éperons barrés de la Semois.

Lors de la conquête romaine, Arlon ("Orolaunum") n'était qu'une bourgade aux portes de la forêt. Les nombreuses découvertes menées dans le centre ancien de la ville montrent une évolution radicale pendant la période d'occupation romaine, plaçant la ville sur un axe important traversant le territoire du nord au sud.

La période mérovingienne est également largement illustrée par les nombreux cimetières découverts (Torgny, Tellin, Wellin, etc.) et par la fondation de l'abbaye de Saint-Hubert. La transition carolingienne verra la création de comtés majeurs, tels que ceux de La Roche et Durbuy, bientôt intégrés à la couronne luxembourgeoise puis impériale germanique.

Le couvert forestier a maintenu la province à l'écart de la tectonique médiévale, sur le plan politique. Toutefois, le territoire, morcelé entre plusieurs grandes maisons, verra des villes se fortifier, dont il reste aujourd'hui d'importants témoins : châteaux (La Roche, Bouillon), abbayes et lieux de pèlerinage (Orval, Saint-Hubert), fermes seigneuriales, tracés antiques, etc.

Sous l'Ancien Régime, la province de Luxembourg et le Grand-Duché de Luxembourg actuels, mais aussi la région de Bitburg et celle de Saint-Vith (devenues « prussiennes » en 1815), et auparavant le pays de Thionville et celui de Montmédy (passés à la France sous Louis XIV), ne formaient qu'une seule entité : le duché de Luxembourg, qui faisait lui-même partie de l'ensemble vaguement confédéral des Pays-Bas méridionaux.
Sous la Révolution française, les deux tiers du duché de Luxembourg et une partie du duché de Bouillon furent unis pour former le département des Forêts.

Lors du Congrès de Vienne en 1815, le duché de Luxembourg (« restauré » avec Bouillon, mais sans ses anciens territoires orientaux) fut inclus dans la Confédération germanique. En effet, Guillaume d'Orange, devenu par ailleurs grand-duc de Luxembourg, avait reçu le Luxembourg en compensation de la perte de ses possessions privées (Dietz, Hadamar, Dillenburg, etc.) au profit de la Prusse. Toutefois, au lieu de traiter son grand-duché comme un État à part, il en fit pour ainsi dire — et malgré le droit de garnison obtenu par la Prusse dans la forteresse « fédérale » de Luxembourg — une province de son royaume, soumise à la loi fondamentale et à l'administration des Pays-Bas unis.

Lors de l'indépendance de la Belgique en 1831, la conférence de Londres se chargea de régler finalement la question des frontières du nouvel État belge. Les grandes puissances européennes décidèrent de partager le Luxembourg en deux, selon des critères linguistiques (sauf les villages « wallons » de Doncols et de Sonlez restés au Grand-Duché, et tout le Pays d'Arlon, bien que de langue luxembourgeoise, attribué à la Belgique), la partie occidentale revenant à la Belgique et la partie orientale restant à Guillaume des Pays-Bas. Les frontières définitives furent fixées dans le Traité de Londres signé le par la Belgique, les Pays-Bas et les cinq grandes puissances européennes de l'époque. C'est à cette époque qu'Arlon devint définitivement le chef-lieu de la province de Luxembourg.
En 1977, lors de la fusion des communes, la province de Luxembourg perdit la commune de Sugny, qui fusionna avec Vresse-sur-Semois et rejoignit la province de Namur. Elle gagna la commune de Fronville qui quitta la province de Namur pour intégrer la commune de Hotton. Bure et Resteigne quittèrent la province de Namur pour intégrer la commune de Tellin.



Les armoiries reconnues de la province de Luxembourg sont "un burelé d'argent et d'azur de dix pièces au lion de gueules, armé, lampassé et couronné d'or, à la queue fourchue et passée en sautoir".

Le blason de la province de Luxembourg est aujourd'hui commun avec celui du Grand-Duché de Luxembourg, ces deux entités étant les deux parties de l'ancien duché de Luxembourg séparées par les traités de 1839.

L'origine de ce blason remonte au . Il est le signe d'Henri le Blond, fils de Waléran III et d'Ermesinde. Cette dernière permit, par son second mariage, d'incorporer au comté de Luxembourg le comté d'Arlon, tenu jusqu'alors par les ducs de Limbourg. Ses restes sont conservés sur le site de son abbaye à Clairefontaine, près d'Arlon.

La province est divisée en cinq arrondissements administratifs (Arlon, Bastogne, Marche-en-Famenne, Neufchâteau, Virton) et un arrondissement judiciaire (Luxembourg).

Le graphique suivant reprend la population résidente au janvier de chaque année pour la province et ses arrondissements administratifs.






La province comprend six zones interpolice :

À l'instar de la réforme des polices, la réforme des services d'incendie a également divisé le territoire belge en zones, appelées zones de secours. La Province de Luxembourg est constituée d'une seule zone couvrant l'entièreté du territoire de ses 44 communes, où se répartissent 16 casernes, autrefois services communaux d'incendie. La zone de secours Luxembourg est paradoxalement, comme la Province, la plus grande zone de secours de Belgique et la moins peuplée.

La province dispose d’une unique chaîne de télévision régionale couvrant toutes ses communes : TV Lux.

Divers documents anciens concernant la province sont consultables dans deux dépôts des Archives générales du Royaume et Archives de l’État dans les Provinces situés sur son territoire : celui d’Arlon pour ses deux tiers sud et celui de Saint-Hubert pour son tiers nord.

Pour ce qui est de la religion dominante catholique, la province fait partie, avec la province de Namur, du diocèse de Namur. Avant 1823, son territoire dépendait des diocèses de Liège et Trèves.

Les points d'intérêt du tourisme sont avant tout d'ordre naturel. La variété géographique du territoire est si grande que l'on peut traverser de profondes vallées, de larges plateaux agricoles ou d'épaisses forêts. Paradis des randonneurs, des photographes animaliers et de nombreux sports d'extérieur (kayak, ski de fond, VTT, etc.), la province de Luxembourg est le poumon vert du pays.

À côté du patrimoine naturel, le patrimoine bâti n'est pas en reste, visible dans les musées de Marche-en-Famenne et d'Arlon notamment. Outre les églises et les abbayes (Saint-Hubert, Orval, etc.), les châteaux et les fermes fortifiées, la province recèle de nombreux villages typiques, bâtis en pierre locale selon des techniques et des architectures très anciennes. On peut d’ailleurs contempler plusieurs bâtiments typiques de la province en un seul lieu, au Fourneau Saint-Michel, où ont été transplantés de nombreux bâtiments caractéristiques du provenant du sud du sillon Sambre-et-Meuse.

La fédération de tourisme du Luxembourg belge (FTLB) compte douze maisons du tourisme :




</doc>
<doc id="16950" url="https://fr.wikipedia.org/wiki?curid=16950" title="Nâlandâ">
Nâlandâ

Nālandā (hindî/sanskrit/pâli : नालंदा) est une ville de l'État du Bihar, en Inde du nord, près du Népal, ancien siège d'un important centre universitaire bouddhiste, comptant à son apogée jusqu'à moines. 

L'université de Nālandā fut un centre majeur de la pensée indienne, dont l'influence s'étendit sur une grande partie de l'Asie : Asie centrale, Himalaya, Asie du Sud-Est, Chine et Japon. Elle fait partie du patrimoine mondial de l'Unesco depuis 2016.

L'empereur Ashoka (v. -304 à -232) avait déjà fait construire un premier temple bouddhiste sur le site. Le premier monastère apparaît au . Le roi Shakraditya (IAST: Śakrādityam parfois identifié à Kumâragupta Ier (r. 415–455)) de Maghada en serait le fondateur; d'autres dynasties royales aideront le monastère. Nagarjuna sera l'un de ses premiers abbés. Son disciple Aryadeva y enseigna. Le monastère était constitué de moines du Mahayana. Par la suite, le monastère est détruit lors d'une incursion de barbares et reconstruit peu de temps après.

L'université prend son essor, sous le nom de Mahāvihāra — de "mahā", grand et "vihāra", monastère. Le râja Kumāragupta I de la dynastie Gupta au milieu du fait construire le temple central. Elle grandit rapidement en importance et connaît une renommée internationale qui attire des moines étudiants du Tibet, de Birmanie, d'Indonésie (Sumatra et Java), mais aussi de Corée ou de Chine comme Xuanzang qui en fait une description enthousiaste dans son compte-rendu de voyage. 

Asanga et Vasubandhu () furent abbés du monastère et firent de l'université un très grand centre de l'école Cittamātra. Les maîtres Madhyamaka Candrakîrti () et Shantideva (685-763) y enseignèrent.

Comportant des bâtiments à plusieurs étages, une bibliothèque et un observatoire, elle compte quelque quatre mille étudiants lorsque Xuanzang y séjourne par deux fois lors de son périple indien. L'accès s'y faisait par une série de tests très difficiles, Xuanzang nous confie que deux tiers des candidats échouent. À côté des textes du bouddhisme mahāyāna dont la connaissance est obligatoire, on y étudie les Védas, les Upaniṣad, qui y sont enseignés par des brahmanes, la cosmologie et la logique nyāya, mais aussi la grammaire, et la physique. Les cours sont de longueur fixée, annoncés par le son d'une trompe et réglés par une clepsydre. La fin des études se termine par la soutenance d'une thèse.

Une des activités des étudiants est la copie de manuscrits dont le plus important est la Prajnaparamita. Les voyageurs purent ensuite emporter des centaines de manuscrits rédigés ici. Les étudiants étaient vêtus de la robe jaune des moines bouddhistes et se choisissaient un maître auquel ils devaient une obéissance scrupuleuse.

Les souverains Pāla vont embellir Nālandā et l'université devient un foyer artistique, en particulier de sculpture de bronze, qui va influencer le monde bouddhiste au travers des styles et sujets que les étudiants étrangers rapportent chez eux après leur séjour d'étude. De même, des souverains étrangers invitent des maîtres de Nālandā à venir enseigner dans leur pays et ainsi une grande partie de ce qui constitue le bouddhisme tibétain ou Chinois s'y est élaboré. Nālandā sert aussi de modèle à d'autres universités - comme Odantapura ou Vikramaśīla - bientôt fondées par des rājas sur leur terre, nouvelles sources participant à la propagation du bouddhisme. Dans la dernière phase du Bouddhisme indien lorsque le tantrisme se développe, Nalanda commence à être éclipsée par Vikramaśīla.

Au , le bouddhisme entre en déclin en Inde et vers 1200, l'université est détruite par les envahisseurs musulmans lors de leurs incursions dans la vallée gangétique. Une tentative de reconstruction échoue, des brahmanes mettant le feu aux nouvelles structures et le site est abandonné. 

En 1951, un centre moderne pour les études bouddhiques y est fondé.

Hormis l'énorme stūpa central de briques et qui avait été agrandi six fois, les fouilles ont mis au jour une dizaine de monastères ou "vihāra" construits sur le même plan.

Le monastère Nalanda de Labastide-Saint-Georges (Tarn), dans le sud de la France (à de Toulouse) est affilié à la Fondation pour la préservation de la tradition du Mahayana (FPTM).

Nālandā est aussi le nom de deux universités modernes situés au Sri Lanka et à Toronto, au Canada.




</doc>
<doc id="16952" url="https://fr.wikipedia.org/wiki?curid=16952" title="Juhani Aho">
Juhani Aho

Juhani Aho (Johannes Brofeldt jusqu'en 1907 ; né le 11 septembre 1861 à Lapinlahti et décédé le 8 août 1921 à Helsinki) est un écrivain finlandais.
Sa carrière d'écrivain a duré quarante ans.

Juhani Aho fut le premier écrivain professionnel en Finlande, plusieurs fois proposé pour le prix Nobel de littérature. Il fut également journaliste et traducteur (de Maupassant, Zola, Daudet, Lagerlöf...). Son œuvre est toujours considérée comme l'une des pièces maîtresses de la littérature des pays du nord, et continue d'être rééditée et lue. Le cent cinquantième anniversaire de sa naissance a donné lieu en 2011 à de nombreux hommages, publications et expositions en Finlande. 

Fils du pasteur Henrik Gustaf Theodor Brofeldt et de Karolina Fredrika Emelie Snellman, il est l’aîné des dix enfants du couple. Aho entre au Lycée de Kuopio en 1872 et obtient son baccalauréat en 1880. Il étudie ensuite à l'université d'Helsinki, la langue finnoise, la littérature et l'histoire. Jeune étudiant, Aho se fait déjà remarquer en remportant le concours d'écriture littéraire de sa nation.

Avec sa femme, la peintre Wendla ”Venny” Irene Soldan-Brofeldt, il eut deux fils : Heikki et Antti, ainsi qu'un fils, Bjorn, avec sa belle-sœur Mathilda "Tilly" Soldan.

Heikki Aho (1895-1961) et Björn Soldan (1902-1953) sont considérés comme les pionniers du film documentaire finlandais. Leur compagnie, Aho & Soldan, fondée en 1924, a produit plus de 400 documentaires. Egalement photographes, Heikki Aho et Björn Soldan ont marqué l'histoire artistique de la Finlande, tout autant qu'ils ont documenté l'histoire et le développement de leur pays. Expressioniste, dans la lignée de l'avant-garde européenne de l'époque, leur œuvre est aujourd'hui saluée dans le monde entier. Dix photographies d'Aho & Soldan font depuis 2013 partie du fonds permanent du Centre Pompidou. La fille d'Heikki Aho, Claire Aho (1925-2015) est elle aussi une photographe, renommée notamment pour ses photos de mode. "Eclats, cent ans d'histoire d'une famille" (Lastuja – Taiteilijasuvun vuosisata), documentaire réalisé en 2011 par Peter von Bagh, relate l'histoire de cette famille d'artistes.

Juhani Aho écrit plusieurs romans et nouvelles, ses premières nouvelles sont publiées en 1883. En 1884 Il interrompt ses études pour devenir écrivain et journaliste indépendant . Aho travaille pour plusieurs journaux (Uusi Suometar, Savo...) et participe à la création du quotidien Päivälehti, dont le successeur sera Helsingin Sanomat. Il collaborera avec Helsingin Sanomat jusqu'à son décès.

Membre du parti jeune finnois, il prend part aux luttes politiques et sociales, s'oppose à la domination russe en Finlande et œuvre pour la promotion de la langue finnoise.
En 1889 et 1890, un séjour à Paris lui permet d'approfondir sa connaissance de la littérature française, dont il subit l'influence. Il participa également au comité de (re)traduction de la bible en finnois.

L'œuvre de Juhani Aho reflète les deux grands courants qui dominent à cette époque la littérature finlandaise: le naturalisme et le néoromantisme.

Dans sa nouvelle « Au temps où père acheta la lampe » (1883) et dans son court roman "Chemin de fer" (1884), il décrit avec humour l'introduction du progrès technique dans les campagnes. Il consacre ensuite deux romans à une exploration de la psychologie féminine : "La Fille du pasteur" (1885) et "La Femme du pasteur" (1893), souvent comparé à "Madame Bovary". Il publie également deux longues nouvelles : "Vers Helsinki" (1889) et "Seul" (1890).

Par le symbolisme et le romantisme qui l'irriguent, "Seul" tient une place singulière dans l'œuvre considérable d'Aho. "Seul" est le monologue cyclothymique d'un homme épris,ou qui croit l'être, qui s'exalte et se désespère, ressassant sans cesse ses sentiments. En proie au désarroi, il peut lui arriver de confondre poétiquement les Grands Boulevards parisiens et la forêt finlandaise. Mais c'est avec la plus grande minutie qu'il décrit ses états d'âmes fluctuants. Selon l'universitaire Jyrki Nummi, on peut tenir le personnage principal de "Seul" comme un flâneur, tel que le définit Baudelaire dans "Le peintre de lavie moderne".

""Seul" reçut un accueil houleux en Finlande. Jugé inadmissible par les autorités finlandaises qui avaient subventionné ce qui devait constituer un voyage d'études permettant à Aho d'élaborer des ouvrages dignes d'une nation en cours de création, le roman s'inspire de différents courants littéraires européens de l'époque, sans tourner le dos aux racines kalevaliennes de sa culture d'origine. Oscillant entre les polarités géographiques, historiques, éthiques et esthétiques, "Seul" se lit également comme l'un des premiers signes avant-coureursdu modernisme dans la littérature finnoise." (Taina Tuhkunen-Couzic," L'exil d'un amoureux : Seul de Juhani Aho", in Etudes finno-ougriennes, vol 33, 2001). Selon un sénateur finlandais, "au moment même où il devrait s'identifier avec les souffrances de son peuple et encourager l'esprit patriotique, Juhani Aho,manifestement aliéné de son peuple, dirige son Pégase dans les égouts de Paris pour y ramasser des saletés qu'il sert ensuite à son peuple."

Récit semi-autobiographique, "Seul" met aussi en scène Aino Järnefelt, la future femme de Jean Sibelius. Après avoir lu "Seul", le compositeur écrivit à Aho et le provoqua en duel, mais décida le lendemain matin de ne pas lui envoyer la lettre...

Juhani Aho est aussi l'auteur de nouvelles qu'il compare à des copeaux tombés sur le sol dans l'atelier d'un charpentier : "Copeaux" (1891). Ce premier volume sera suivi de sept autres, qui comprendront des textes de genres et de tons très divers.

Il change ensuite d'orientation avec un roman historique consacré à la lutte entre paganisme et christianisme dans la Finlande du , "Panu" (1897), où il adopte les thèmes et le style du néo-romantisme. Dans la même veine, il publie en 1906 "Le Printemps et les gelées tardives", roman sur les débuts du réveil national finnois dans les années 1840.

En 1911, avec "Juha" (traduit en français sous le titre "L'Écume des rapides" ; adapté au cinéma par Mauritz Stiller avec "À travers les rapides" en 1921 et par Aki Kaurismäki avec "Juha" en 1999), il renoue avec un style plus réaliste. Le roman décrit les relations entre un vieux mari bienveillant, sa jeune épouse insatisfaite et un étranger qui la séduit et l'enlève, au milieu de la forêt et des rapides de Carélie. "Juha" est l'un des plus grands classiques du roman finlandais. Il a été adapté en livret pour l'opéra "Juha" du compositeur finlandais Leevi Madetoja.


La liste des ouvrages de Juhani Aho est la suivante:




</doc>
<doc id="16960" url="https://fr.wikipedia.org/wiki?curid=16960" title="Image directe">
Image directe

L'image directe d'un sous-ensemble "A" de "X" par une application "f" : "X" 
→ "Y" est le sous-ensemble de "Y" formé des éléments qui ont, par "f", au moins un antécédent appartenant à "A" :
formula_1





</doc>
<doc id="16961" url="https://fr.wikipedia.org/wiki?curid=16961" title="Image réciproque">
Image réciproque

L'image réciproque d'une partie "B" d'un ensemble "Y" par une application "f" : "X" → "Y" est le sous-ensemble de "X" constitué des éléments dont l'image par "f" appartient à "B" :

Dans le cas particulier où "B" se réduit à un singleton, l'image réciproque formula_2 du singleton formula_3 par la fonction "f" est appelé ensemble des antécédents de "y" par "f".

Considérons l'application "f" : {1, 2, 3} → {"a", "b", "c", "d"} définie par "f"(1) = "a", "f"(2) = "c", "f"(3) = "d". L'image réciproque de {"a", "b"} par "f" est "f"({"a", "b"}) = {1}.

Avec cette définition, "f" est l'application « image réciproque (par "f") », dont l'ensemble de définition est l'ensemble des parties de "Y" et dont l'ensemble d'arrivée est l'ensemble des parties de "X".

"Mise en garde" : lorsque "f" est une bijection, il ne faut pas confondre cette application sur les parties avec la bijection réciproque de "f", également notée "f", de "Y" dans "X". L'image réciproque par "f" s'identifie avec l'image directe par cette bijection réciproque "f". Pour éviter toute confusion, Birkhoff et Mac Lane parlent d'une « application d'ensembles » qu'ils notent "f" au lieu de "f".


Relation binaire réciproque


</doc>
<doc id="16963" url="https://fr.wikipedia.org/wiki?curid=16963" title="Land d'Autriche">
Land d'Autriche

Un land, Land en allemand ("Länder" au pluriel) est un État fédéré de la République d'Autriche. Le pays est constitué de neuf États fédérés.

En allemand, les subdivisions portent le nom de ' (« État fédéré »), ' au pluriel. Il est possible de rencontrer également le terme "Land" (« État »), "Länder" au pluriel. La constitution de l'Autriche utilise les deux termes.

Chaque État autrichien possède une législature élue, le "Landtag", un gouvernement, le "Landesregierung", et un gouverneur, le "Landeshauptmann" ou la "Landeshauptfrau". Les élections ont lieu tous les cinq ans (six ans en Haute-Autriche). La constitution des États détermine, entre autres, la répartition des sièges au gouvernement entre les différents partis, la plupart des États ayant un système de représentation proportionnelle basée sur le nombre de délégués au "Landtag". Le "Landeshauptmann" est toujours élu par le "Landtag", ce qui signifie qu'il est nécessaire de former une coalition pour assurer l'élection d'un candidat spécifique. Vienne, la capitale du pays, est à la fois une ville et un État, le maire agissant comme gouverneur et le conseil comme "Landtag".

Comparativement à d'autres États fédéraux comme ceux des États-Unis ou les "Länder" d'Allemagne, les États autrichiens ont des compétences moins étendues. De nombreux aspects (éducation, santé, télécommunications, etc.) sont de la compétence de l'État fédéral. De même, les États autrichiens n'ont aucune compétence judiciaire.

La liste suivante résume les principales caractéristiques des États d'Autriche. Il indique également le nombre de villes et de communes de chacun.

Le tableau suivant résume les composantes politiques de chaque État en 2010. Le "Landeshauptmann" ou la "Landeshauptfrau" est la personne gouvernant l'État.

Cinq des neuf États autrichiens existent déjà sous une forme ou une autre depuis le Moyen Âge, quoique pas forcément sur un territoire identique : la Basse-Autriche, la Carinthie, la Styrie, le Tyrol et Salzbourg. La Haute et la Basse-Autriche correspondent à peu près aux deux parties autonomes de l'archiduché d'Autriche, la principauté qui forme le cœur historique de l'Empire. Salzbourg correspond à l'archidiocèse de Salzbourg. La Carinthie est issue du duché de Carinthie, la Styrie du duché de Styrie et le Tyrol du comté de Tyrol ; ces trois États cèdent des parties importantes de leur territoire à l'Italie et à la Yougoslavie après la Première Guerre mondiale. Le Vorarlberg est une entité semi-autonome du comté du Tyrol jusqu'en 1918.

Le Burgenland est créé en 1921 à partir de zones essentiellement germanophones de Hongrie, cédées à l'Autriche après les traités de Trianon et de Saint-Germain-en-Laye. Vienne est séparée de la Basse-Autriche en 1922.


</doc>
<doc id="16970" url="https://fr.wikipedia.org/wiki?curid=16970" title="Trois lois de Clarke">
Trois lois de Clarke

L'auteur de science-fiction Arthur C. Clarke a formulé les trois lois suivantes : 


Les lois de Clarke ont été proposées par Arthur C. Clarke dans l'essai "Hazards of Prophecy: The Failure of Imagination", dans "Profiles of the Future" (1962), bien après que la première loi fût écrite. La deuxième loi est présentée comme une simple observation dans le même essai ; son statut de deuxième loi de Clarke a été conférée par d'autres personnes.

Dans une révision de 1973 de "Profiles of the Future", Clarke reconnut la deuxième loi et proposa la troisième dans le but d'en arrondir le nombre, ajoutant : « Comme les trois lois étaient suffisantes pour Newton, j'ai modestement décidé de m'arrêter là ». 

Parmi ces trois lois, la troisième est la plus connue et la plus citée : celle-ci codifie en effet ce qui est sans doute la plus significative de ses rares contributions formelles à la fiction spéculative. Modèle pour les autres écrivains de "hard science fiction", Clarke postule des technologies avancées sans se servir de concepts erronés d'ingénierie, ni d'explications fondées sur des hypothèses incorrectes, ni en extrapolant simplement les techniques existantes.


</doc>
<doc id="16973" url="https://fr.wikipedia.org/wiki?curid=16973" title="Triade de Memphis">
Triade de Memphis

Dans la mythologie égyptienne, la Triade de Memphis est un ensemble de trois dieux de la ville antique de Memphis. Ptah y est l'époux de Sekhmet et le père de Nefertoum.


</doc>
<doc id="16976" url="https://fr.wikipedia.org/wiki?curid=16976" title="Arc (affluent de l'Isère)">
Arc (affluent de l'Isère)

LArc est une rivière s'écoulant en France dans la vallée alpine de la Maurienne, dans le département de la Savoie en région Auvergne-Rhône-Alpes, en ancienne région Rhône-Alpes. C'est un affluent gauche de l'Isère, donc un sous-affluent du Rhône.

Le nom Arc viendrait de "ar" qui signifierait « vallée en forme de plaine ».

Selon d'autres chercheurs, il pourrait trouver ses racines dans l'expression "Supra Flumen quod dicitur Arcus" au ; "ar" serait alors lié à l'eau vive.

Elle prend sa source à d'altitude au pied de l'ancien glacier des Trois Becs et au lac des Sources inférieures, non loin de la frontière franco-italienne, et se jette dans l'Isère à la hauteur de la commune d'Aiton, donc rejoint la vallée de la Tarentaise. De , rivière torrentielle à forte pente, l'Arc a de grands atouts énergétiques que les industriels ont commencé à exploiter dès la fin du .

L'Arc arrose un total de quarante-trois communes sur son passage. Toutefois, seule une dizaine de ces communes, principalement dans la partie supérieure de la vallée, sont véritablement traversées par le cours d'eau, celui-ci se contentant le plus souvent de marquer la limite entre ces communes en aval.

Ces communes sont, de la source jusqu'à la confluence :

Sur l'Arc :

1946 : reprise de la gestion par Électricité de France :

Sur les affluents :

L'Arc traverse les huit zones hydrographiques W100,W101, W102, W103, W104, W105, W106, W107 pour une superficie totale de . Ce bassin versant est constitué à 93,47 de , à 4,53 de , à 1,90 de , à 0,12 de .

C'est le SPM ou Syndicat mixte du Pays de Maurienne qui a la gestion de l'Arc et ses affluents.


Le module de l'Arc a été calculé durant une période de 6 ans à Épierre. Il se monte à pour une surface de bassin de , soit 90 % de la totalité du bassin. La rivière présente des fluctuations saisonnières de débit typiques d'un régime nival, avec des hautes eaux de printemps-été dues à la fonte des neiges et portant le débit mensuel moyen au niveau de 75 à de mai à juillet inclus (avec un maximum en juin), suivies d'une baisse progressive aboutissant à un long étiage d'automne-hiver, de novembre à début avril, entraînant une baisse du débit moyen mensuel jusqu'à un minimum de au mois de janvier.
À l'étiage, le VCN3 peut chuter jusque , en cas de période quinquennale sèche, ce qui reste très confortable.
Les crues peuvent être très importantes, voire dévastatrices. En effet, le QIX 2 et le QIX 5 valent respectivement 144 et . Le QIX 10, le QIX 20 et le QIX 50 n'ont pas été calculés. 

Le débit maximal enregistré à Épierre est de , mais ce chiffre n'a guère de signification, étant donnée la très courte période d'observation de 6 ans. 

Mais l'Arc est également connue pour ses crues dévastatrices:

La lame d'eau écoulée dans le bassin versant de la rivière est de 866 millimètres annuellement, ce qui est certes élevé et résulte des précipitations abondantes sur les Alpes du nord, mais est cependant moindre que ce que l'on observe dans les autres bassins versants des rivières de haute montagne de Savoie. Le débit spécifique (Qsp) se monte à 27,3 litres par seconde et par kilomètre carré de bassin.

Ainsi, tout au long de son cours et de celui de ses affluents, EDF a installé nombre de barrages et prises d'eau.

Dès la fin du , des travaux d'endiguement de la rivière et d'aménagement des bassins versants affluents sont réalisés. En 1893, les premières usines électrochimiques et électro-métallurgiques, alimentées par des conduites forcées, s'installent le long du cours d'eau (usines d'aluminium de la Praz, Prémont-Orelle et Calypso).



</doc>
<doc id="16977" url="https://fr.wikipedia.org/wiki?curid=16977" title="Saint-Jean-de-Maurienne">
Saint-Jean-de-Maurienne

Saint-Jean-de-Maurienne ( est une commune française située dans le département de la Savoie, en région Auvergne-Rhône-Alpes.
Saint-Jean-de-Maurienne est située au confluent de l'Arc, rivière qui a modelé la vallée de la Maurienne, et de l'Arvan qui descend de la vallée des Arves (col de la Croix-de-Fer).

Les communes limitrophes de Saint-Jean-de-Maurienne sont Saint-Julien-Montdenis, Jarrier, Hermillon, Villargondran, Albiez-le-Jeune, Albiez-Montrond, Saint-Pancrace, Pontamafrey-Montpascal et Fontcouverte-la-Toussuire.

Le climat y est de type montagnard en raison de la présence du Massif alpin.

Voici un aperçu dans le tableau ci-dessous pour l'année 2007 :


Saint-Jean-de-Maurienne se situe sur le tracé de la future nouvelle liaison ferroviaire Lyon-Turin. Des habitations, des entreprises, la gare SNCF et le centre de secours sont pleinement touchés par ce projet.
Chargée de la phase « études », la société Lyon Turin ferroviaire (LTF) prévoit l'installation d'une nouvelle gare dans le quartier Sous-le-Bourg, desservant la ligne historique et la ligne Lyon-Turin.
Les enquêtes géologiques et topographiques sont en cours. Ce chantier s'annonce encore plus complexe que celui du tunnel sous la Manche.

La ville de Saint-Jean-de-Maurienne est dotée d'un réseau de transports en commun géré par la communauté de communes Cœur de Maurienne.

Un héliport est disponible pour l'hélicoptère de la gendarmerie dont la base est située à Modane ainsi qu'un autre sur le toit de l’hôpital de St Jean de Maurienne, réservé aux urgences.

Le forum, les nouvelles arcades et les Clapeys sont les trois principaux quartiers.

Le nombre total de logements dans la commune est de . Parmi ces logements, 87,9 % sont des résidences principales, 5,4 % sont des résidences secondaires et 6,7 % sont des logements vacants. Ces logements sont pour une part de 17,5 % des maisons individuelles, 79,3 % sont des appartements et enfin seulement 3,2 % sont des logements d'un autre type. Le nombre d'habitants propriétaires de leur logement est de 37,9 %. Ce qui est inférieur à la moyenne nationale qui se monte à près de 55,3 %. Le nombre de locataires est de 56,7 % sur l'ensemble des logements qui est supérieur à la moyenne nationale qui est de 39,8 %. On peut noter également que 5,4 % des habitants de la commune sont des personnes logées gratuitement alors qu'au niveau de l'ensemble de la France le pourcentage est de 4,9 %. Toujours sur l'ensemble des logements de la commune, 3,5 % sont des studios, 11,5 % sont des logements de deux pièces, 28,4 % en ont trois, 34,5 % des logements disposent de quatre pièces, et 22,1 % des logements ont cinq pièces ou plus.

Le toponyme de la ville trouve son origine dans la référence à son saint patron Jean le Baptiste le Précurseur, auquel est ajouté le déterminant complémentaire "-de-Maurienne" en référence à sa situation dans la vallée de la Maurienne.

Les premières mentions du nom "Maurienne" apparaissent vers le avec l'édification de la cathédrale primitive dédiée à saint Jean-Baptiste) dans la ville de Maurienne. Au cours de la période, un "Urbem Mauriennam" est mentionné. Grégoire de Tours désigne d'ailleurs la ville : « urbs Maurienna » ou « locus Mauriennensis ». En 739, le testament patrice Abbon mentionnera quant à lui la vallée qui prend le nom de Maurienne, . Selon le chanoine Adolphe Gros, dans sa recherche étymologique du nom de la ville, indique que la "Maurienne" sous sa forme "Maurogenna" désigne la ville jusqu'au , où on lui accole celui du saint, alors que la vallée est désignée par « territorio Mauriennam ».

En francoprovençal, la graphie de la commune s'écrit "San Dyan ", selon la "graphie de Conflans".

Saint-Jean-de-Maurienne est la capitale de la vallée de la Maurienne depuis le . Après que Sainte Thècle rapporta d'Alexandrie (Égypte) les reliques de saint Jean le Baptiste qui sont les trois doigts représentés sur les armes de la ville, ainsi que sur les lames des couteaux Opinel, la ville a été élevée au rang d'évêché par Gontran, petit-fils de Clovis.

En 753, Griffon se rend en Italie pour rejoindre le roi des Lombards, Aistolf, le plus puissant adversaire de son demi-frère, le roi des Francs, Pépin le Bref, mais il est tué à Saint-Jean-de-Maurienne par les hommes de Pépin.

La ligne de chemin de fer Aix-les-Bains—Saint-Jean-de-Maurienne est ouverte en 1857.

Dans les années 1900, les progrès technologiques de l'hydroélectricité suisse sont à l'origine d'intenses , qui profitent aux implantations industrielles en Maurienne, tandis que le tourisme prend son essor.

La ville de Saint-Jean-de-Maurienne est une sous-préfecture de la Savoie. L'arrondissement de Saint-Jean-de-Maurienne est divisé en six cantons :

La ville de Saint-Jean-de-Maurienne fait partie de la troisième circonscription de la Savoie.

C'est également la ville la plus importante de la communauté de communes Cœur de Maurienne.

Traditionnellement, Saint-Jean-de-Maurienne a été un fief de la gauche socialiste, dès les années 1930, en raison de l'importance de son bassin d'emploi ouvrier. Ainsi, Roland Merloz en a été le maire socialiste de 1977 à 2008. Mais depuis les années 1990 et les mutations sociologiques de la Maurienne (départ des usines, multiplication des stations touristiques, diminution de la population surreprésentant les personnes âgées), le vote de droite a progressé ; un conseiller général UMP, Pierre-Marie Charvoz, est élu en 2001, Nicolas Sarkozy arrive en tête lors des deux tours de l'élection présidentielle de 2007, avec respectivement 33,5 % et 56,62 %, et, en 2008, Pierre-Marie Charvoz remporte les élections municipales.

Le conseil municipal de Saint-Jean-de-Maurienne compte 29 membres ; il est composé d'un maire, de sept adjoints, de quatre conseillers délégués et de dix-sept conseillers municipaux.

Roland Merloz, maire de la ville depuis 1977 annonce en 2008 sa volonté de ne pas se représenter.

Voici ci-dessous le partage des sièges au sein du Conseil municipal de Saint Jean de Maurienne :

Lors des élections municipales de mars 2008, le taux de participation du premier tour fut de 65,46 % sachant que l'on dénombre un total de inscrits sur toute la commune. Le nombre de votants s'est élevé à voix dont se sont exprimées. Lors du premier tour, la liste majorité présidentielle "Ensemble pour Saint Jean" avec à sa tête, Pierre-Marie Charvoz a recueilli 46,95 % des suffrages soit voix. Suivi de la liste « Saint Jean 10 000 » menée par Hervé Bottino, ayant reçu 34,39 % des suffrages soit voix. En troisième position, la liste « Saint Jean à venir », avec à sa tête Christine Merlin a obtenu 13,26 % des suffrages soit 450 voix. Enfin la liste « Vivons Saint Jean », menée par Florence Arnoux Le Bras obtient 5,39 % des suffrages soit 183 voix.

Lors du second tour, le taux de participation fut de 68,57 %. Le nombre de votants s'est élevé à voix dont se sont exprimées. Lors du deuxième tour, la liste majorité présidentielle "Ensemble pour Saint Jean" avec à sa tête, Pierre-Marie Charvoz a recueilli 55,40 % des suffrages soit voix et remporte ainsi 23 sièges. La liste « Saint Jean 10 000 » menée par Hervé Bottino, a reçu 44,60 % des suffrages soit voix et se voit donc attribuer 6 sièges. Les autres listes n'étaient pas présentes au deuxième tour.

Voici la liste des villes ayant passé un jumelage avec la commune de Saint-Jean-de-Maurienne :
Les habitants de la commune sont appelés les "Saint-Jeannais(es)".

Évolution de la pyramide des âges de la ville de Saint-Jean-de-Maurienne, comparaison entre l'année 1999 et 1982 :
Le nombre total de ménages à Saint-Jean-de-Maurienne est de . Ces ménages ne sont pas tous égaux en nombre d'individus. Certains de ces ménages comportent une personne, d'autres deux, trois, quatre, cinq voire plus de six personnes. Voici ci-dessous, les données en pourcentage de la répartition de ces ménages par rapport au nombre total de ménages.

Les Ménages






Saint-Jean-de-Maurienne est située à proximité de certains des plus grands cols alpins, du domaine skiable Les Sybelles et du parc national de la Vanoise. Des activités sont disponibles pour les amateurs de sports-nature, aussi bien les randonneurs et les cyclistes que les skieurs. Saint-Jean-de-Maurienne permet de rejoindre les cols de la Croix-de-Fer, du Télégraphe, du Lautaret, du Grand Cucheron, de la Madeleine, du Glandon, de l'Iseran, du Mont-Cenis et du Galibier. La ville accueille régulièrement des courses cyclistes importantes telles que le Tour de France ou le Critérium du Dauphiné libéré.


Le taux de chômage, en 1999, pour la commune s'élève à 8,8 %, avec un nombre totale de 359 chomeurs. Le taux d'activité entre 20 et 59 ans s'établit à 84 % ce qui est supérieur à la moyenne nationale qui est de 82,2 %. On comptait 46 % d'actifs contre 19,1 % de retraités dont le nombre est légèrement supérieur à la moyenne nationale (18,2 %). Il y avait 21,9 % de jeunes scolarisés et 13 % de personnes sans activité.
Répartition des emplois par domaine d'activité

Actuellement, une importante activité de fabrication d'aluminium par électrolyse de l'alumine existe encore grâce à une usine Trimet France.

En 2014, la capacité d'accueil de la commune, estimée par l'organisme "Savoie Mont Blanc", est de répartis dans . Les hébergements marchands se répartissent comme suit : ; une structure d'hôtellerie de plein air et une chambre d'hôtes.


Le clos Carloz et la zone de loisirs de la Combe sont les principaux espaces verts de la ville. Il existe également le Jardin de l'Europe et le jardin Saint-Ayrald.

La commune possède plusieurs musées :

Associations culturelles notables :

En 2014, la commune de Saint-Jean-de-Maurienne bénéficie du label « ville fleurie » avec « trois fleurs » attribuées par le Conseil national des villes et villages fleuris de France au concours des villes et villages fleuris.

Naissance sur la commune :

Autres personnalités :





</doc>
<doc id="16981" url="https://fr.wikipedia.org/wiki?curid=16981" title="Maurienne">
Maurienne

La Maurienne est une vallée intra-alpine française située dans le département de la Savoie en région Auvergne-Rhône-Alpes. D'une longueur de , elle est traversée par la rivière de l'Arc. Elle correspond à l'une des six régions historiques de la Savoie, qui fut un "pagus" (), puis un comté intégré au comté de Savoie et une province administrative (de 1723 à 1860) du duché de Savoie.

Originellement, la vallée est celle des Médulles ou "Medulli", qui seront intégrées dans la province des Alpes Cottiennes jusqu'à sa disparition au , tandis que la partie basse est occupée par les Graiocèles, dans la province des Alpes grées. Le mot « Maurienne » se substitue peu à peu pour désigner la vallée des Médulles.

L'origine étymologique du mot "Maurienne" possède plusieurs hypothèses d'interprétation. Les premières mentions du nom apparaissent vers le avec l'édification de la cathédrale primitive dédiée à saint Jean-Baptiste dans la ville de Maurienne. On trouve ainsi un "Urbem Mauriennam" à cette période. À cette même période, Grégoire de Tours désigne ainsi la ville : « urbs Maurienna » ou « locus Mauriennensis ». En 739, le testament patrice Abbon mentionne la "vallis Maurigenica". Le chanoine Adolphe Gros relève que la Maurienne sous sa forme "Maurogenna" désigne la ville jusqu'au , date à laquelle on commence à lui accoler celui du saint, alors que la vallée est désignée par « territorio Mauriennam ».

Certains spécialistes voient dans l'origine du mot « Maurienne » un dérivé du latin "Malus Rivus", « mauvais ruisseau », qui a évolué en "mau riou/rien", comme l'alpiniste Coolidge dans un article de la revue Alpine de 1904. En effet, la rivière de l'Arc est connue pour ses crues.

Pour le chanoine Gros, dans son "Dictionnaire étymologique des noms de lieu de la Savoie" (1935), une autre hypothèse doit être envisagée. Il voit dans les formes primitives "Maurogenna" ou "*Maurigenna", désignant la ville, une féminisation du terme "Maurogenos". Ce dernier serait un mot hybride entre le nom d'un certain romain "Maurus", auquel est associé le suffixe celtique "Genna", signifiant « fils de ».

Le chanoine Jean-Louis Grillet, dans son "Dictionnaire historique, littéraire et statistique des départemens du Mont-Blanc et du Léman" rapporte que pour Jean de Pineda (jésuite espagnol du ) « le consul Marius, après avoir défait les Cimbres dans les défilés alors presqu'inaccessibles de la Maurienne, y fit ouvrir une voie militaire le long de la rivière d'Arcq, et qu'en conséquence la vallée fut appelée "Via-Marian", et par corruption Mauriana ».

Une dernière hypothèse, qui tend à perdurer, indique depuis le , avec le Theatrum Statuum Sabaudiæ (v. 1682), que "Maurienne" trouve son origine dans le mot « Maure », relatif aux incursions du des Sarrasins. La plupart des érudits ayant étudié la question rappellent cependant la mention très antérieure du . En effet, en 942, le roi Hugues pour des raisons stratégiques, craignant de voir le roi d'Italie Bérenger accéder à son trône, conclut un traité avec les Sarrasins. Ces guerriers voyageurs devant venir s'installer dans les Alpes pour empêcher toute invasion ennemie. Certains historiens s'accordent à dire qu'à la suite de cet accord, une partie de la communauté sarrasine s'implanta dans la vallée l'Arc, qui allait de facto porter le nom de Maurienne. Nombreuses sont les théories qui s'appliquent à trouver l'origine du nom de cette vallée. Peut-être qu'il s'agit d'un savant mélange de tout ceci. Cela ne vient que renforcer le caractère mystérieux de son origine.

En francoprovençal, appelé parfois arpitan, Maurienne se traduit par "Môrièna".

Longue de plus de 120 kilomètres, la Maurienne est l'une des plus grandes vallées transversales des Alpes. La rivière qui l'a modelée après la dernière glaciation est l'Arc. La Maurienne débute à l'ombre des Levanna, recouvertes par les glaciers des sources de l'Arc et surplombant le hameau de l'Écot (commune de Bonneval-sur-Arc) au pied du col de l'Iseran. Elle suit d'abord un axe nord-est—sud-ouest jusqu'à Modane, avant de descendre direction nord-ouest jusqu'à Aiton, où l'Arc rejoint à l'Isère dans la combe de Savoie au pont Royal.

Une partie de la vallée est intégrée au parc national de la Vanoise, dans la continuité du parc national italien du Grand Paradis.

Il n'y a pas à proprement parler de massif de la Maurienne : la vallée est bordée au nord (rive droite de l'Arc) par le grand massif de la Vanoise et les chaînes de la Lauzière et du Grand Arc, et au sud (rive gauche) par les Alpes grées (au sens restreint), le massif du Mont-Cenis, des Cerces, des Arves, des Grandes Rousses et de Belledonne.

Les grands cols alpin qui partent de la vallée sont :

Les géographes distinguent traditionnellement trois ensembles, la basse, la moyenne et la haute Maurienne. L'historien Jean Dompnier, dans sa présentation générale de la moyenne Maurienne, précise qu' À ces trois ensembles, les vallées affluentes peuvent aussi être distinguées avec notamment celle de la Valloirette (du col du Galibier à Saint-Michel-de-Maurienne), le bassin de l'Arve ou de l'Arvan qui coule jusqu'à Saint-Jean-de-Maurienne), la vallée des Villards, dite aussi du Glandon (Saint-Alban et Saint-Colomban), ou encore celle du Bugeon (du col de la Madeleine jusqu'à La Chambre). De nombreuses vallées secondaires ont été les voies de passage privilégiées depuis la haute Antiquité entre la péninsule italienne et l'Europe de l'Ouest. Certaines telles que les vallées d'Avérole, de la Savine, du Fréjus ou encore de la Rocheure n'ont jamais été véritablement peuplées, mais ont toujours servi de voies de passage.

La Basse Maurienne débute avec le canton d'Aiguebelle pour se terminer au niveau de celui de La Chambre. Cette partie de la vallée traverse les massifs cristallins externes. C'est une portion boisée et verdoyante, au fond large, plat et encaissé. Elle est constituée de parois abruptes avec un très fort dénivelé entre son talweg et ces sommets proches (écart de en moyenne) visibles depuis le fond de la vallée. Par exemple, le dénivelé entre le village d'Épierre situé à et le proche sommet du Grand pic de la Lauzière culminant à offre au visiteur le sentiment de se trouver face à un mur de près de de hauteur. La Basse Maurienne, grâce à son relief escarpé et ses forêts denses et riches en nombreuses essences, offre un écosystème idéal pour de nombreuses espèces animales rares et farouches telles que le lynx

À partir de La Chambre s'amorce la Moyenne Maurienne qui s'étire jusqu'à Modane. Elle est constituée par un fond de vallée très étroit et s'ouvrant après d'impressionnants défilés et verrous comme celui du pas du Roc à l'entrée de Saint-Michel-de-Maurienne, sur des bassins reliés à des vallées latérales où se situent la plupart des villages de montagnes et des stations de ski telles que Valloire ou encore Saint-François-Longchamp. Elles se terminent généralement par un col carrossable tel que la Madeleine, le Galibier ou la Croix-de-Fer, ou bien de simples passages comme le col de la Valette culminant à ou le col de la vallée Étroite. Ces bassins concentrent la majorité des habitations et infrastructures industrielles. Ils sont alimentés par quantités de torrents de haute montagne qui ont été la raison de l'implantation des usines sidérurgiques en Maurienne au début du , tout comme cela a été le cas en Tarentaise ou en Valais. Tout récemment, le principal acteur est devenu le groupe Trimet, après avoir supplanté le groupe Rio Tinto. Ces usines, demandant une puissance électrique considérable, sont approvisionnées grâce aux centrales hydrauliques locales. À leurs débuts, le transport de l'électricité sur de grandes distances n'était pas maîtrisé, raison pour laquelle les industries ont été implantées à proximité des sources d'énergies. Le meilleur exemple est Saint-Jean-de-Maurienne qui demeure un pôle électrométallurgiques majeur.

Le fond de la Moyenne Maurienne ne connaît, proportionnellement à sa longueur considérable, qu'une faible prise d'altitude. Cette portion de la vallée bénéficie sur son versant adret d'un ensoleillement exceptionnel. Ce micro climat, que l'on retrouve également en certains lieux de Haute Maurienne comme le cône de déjection de Sollières-Sardières-Termignon, permet de maintenir une agriculture traditionnelle et vivrière mais également de voir la renaissance de cultures oubliées, comme celle du safran. De la même manière, la viticulture refait son apparition. Ainsi, sur les coteaux les mieux exposés sont replantés des pieds de vignes et particulièrement le persan, un vin rouge natif de la Maurienne et par conséquent adapté aux conditions climatiques de cette dernière.

Modane, dernière ville de la vallée (selon la définition INSEE), est une ville frontière, située au débouché des tunnels ferroviaire et routier du Fréjus et dominée par la station de Valfréjus. Ainsi Modane, ville située aux confins de la Moyenne Maurienne, est la porte d'accès à la vallée supérieure de l'Arc. D'ailleurs à la sortie de la cité, le relief change radicalement et la nationale, qui jusqu'alors était relativement plane, s'élève tout à coup pour se transformer en route de montagne survolant les falaises vertigineuses du verrou glaciaire de la barrière de l'Esseillon.

Enfin, on appelle "Haute Maurienne" la haute vallée de l’Arc, qui longe la frontière italienne sur environ 45 kilomètres dans la région du Mont-Cenis. S'étendant au-delà de la barrière de l'Esseillon, il s'agit de la partie supérieure de la vallée de l'Arc avec un talweg d'altitude élevée démarrant à , et formée principalement de verrous s'ouvrant sur des cônes de déjections. Elle débute en amont de la cicatrice de Chavière marquant la fin des massifs houillers et le début de l'unité des massifs cristallins Grand-Saint-Bernard/Vanoise/Ambin allant du Valais au val de Suse et bordés à l'est par le massif du Grand-Paradis. La région est connue depuis la plus haute Antiquité et on y trouve de nombreux vestiges d’occupation humaine depuis le Paléolithique. Au Moyen Âge, elle était un passage important d'échanges commerciaux grâce notamment à la route du Sel. Cette dernière traversant le massif de la Vanoise par le col du même nom, permettait d'échanger fromages, dont le beaufort réputé depuis l'époque romaine et le sel exploité aux mines de Salins en Tarentaise, contre étoffes et épices, en Italie, via le col du Mont-Cenis.

Ce sentier d'altitude en grande partie pavé a récemment fait l'objet d'une importante restauration par le parc national de la Vanoise. La délimitation de la partie supérieure de la vallée reste controversée. En effet, pour les spécialistes de géographie physique, la Haute Maurienne commence en amont de Modane, et plus précisément au-delà de la barrière de l'Esseillon. Ce verrou glaciaire est un promontoire dominant la Moyenne Maurienne, riche en forteresses militaires, dont le but initial était de protéger la Haute Maurienne et le Piémont des invasions françaises. Pour les économistes, Modane est attachée à la Haute Maurienne arguant que la ville dispose d'une très forte influence sur les villages en amont, au travers d'infrastructures économiques et administratives (centres commerciaux, établissements scolaires ou gare SNCF par exemple). Toutefois pour la grande majorité des analystes, Modane est une ville rattachée à la partie médiane de cette vallée, aussi bien de par le relief (toute la partie avale du canton est creusée dans le sillon houiller qui se prolonge jusqu’à Saint-Michel-de-Maurienne), que par l'histoire industrielle de ce secteur.

Longtemps restée à l'écart du reste de la vallée et de son développement économique, cette portion conserve un caractère authentique et sauvage, avec des traditions et un folklore unique et vivace tel que le Diable de Bessans, ou la Fête du 15 août célébrée dans le bourg de Bramans.

L'architecture est différente du reste de la vallée, l'exemple le plus visible étant celui des toits en lauzes recouvrant les chalets et leurs cheminées décorées en pierres. Ainsi les nouvelles constructions tout comme les rénovations, en Haute Maurienne et ses contreforts situés entre Modane et Aussois, sont soumises à l'approbation du Conseil d’architecture, d’urbanisme et de l’environnement de la Savoie (CAUE), avec le concours des architectes consultants et du Syndicat du Pays de Maurienne.

Les villages, en remontant la vallée, sont :

Environ habitants permanents, et environ personnes en hiver.

La Haute Maurienne est pourvue d'innombrables sommets dépassant les , et nombreux sont proches de la barre des . Cette portion de la Maurienne est pourvue d'une grande couverture glaciaire.

D'un point de vue climatique, la Haute Maurienne est la parfaite illustration de monsieur Verneilh, alors préfet du département du Mont-Blanc sous l'occupation de la Savoie par Napoléon. Verneilh dans son rapport "Statistique générale de la France, Département du Mont Blanc (1807)" indique : « Souvent au fond d'une vallée, le voyageur supporte avec peine l'ardeur d'un soleil brûlant, en même temps qu'il aperçoit sur les monts qui l'environnent, les frimas d'un éternel hiver ; d'autres fois, après avoir traversé des neiges ou des glaces sur les cols des montagnes, il rencontre, en descendant dans la plaine, d'abord des bois, ensuite une riante verdure, plus bas des fleurs ou même des fruits. » (). Ainsi la Maurienne de par sa situation encaissée dans de très hauts massifs, et subissant les puissants effets de foehn, en est une parfaite illustration. Dans cette partie de la vallée les précipitations sont largement en dessous des moyennes. On compare très souvent cette région au climat quasi méditerranéen qui sévit dans la vallée du Rhône en Valais central, aux alentours de Sierre.

La Vanoise dans son acceptation restreinte (Vanoise cristaline) est constituée d'un imposant et profond socle de quartzite (roche métamorphique compacte et très dure), de micaschistes, de gneiss et de nappes de granite qui chevauche le socle du massif cristallin interne du Grand Paradis dans la partie la plus orientale de la vallée. Les massifs formant la barrière sud alternent entre le socle cristallin d'Ambin formant une continuité géologique avec la Vanoise, allant du Valais au val de Suse (unité Grand-Saint-Bernard/Vanoise/Ambin), et les massifs de schistes lustrés tels que ceux de la pointe de Ronce, ou bien encore la pointe de Charbonnel. La Haute Maurienne se compose d'une grande variété de roches métamorphiques, allant du gneiss, schiste bleu, vert et micaschistes, en passant par la serpentinite, le quartzite ou encore les amphibolites et les calcschistes. On trouve également des roches magmatiques et plus particulièrement du porphyre, des gabbros et de la diorite affleurant du socle d'Ambin dans le massif du Mont-Cenis.

Cette richesse minérale entraîne une grande diversité de reliefs, mais aussi de végétation qui se développent grâce aux différents types de sols que ces roches offrent. D'un versant à l'autre on peut donc trouver des espèces végétales totalement différentes du fait de la nature plus ou moins acide des sols. Cette particularité a conduit certains botanistes alpins à nommer certaines espèces en référence à l'un des massifs de la Haute Maurienne. La Laîche des glaciers ("Carex glacialis") présente exclusivement dans les régions boréales a été découverte dans la région du Mont-Cenis.

Les massifs de Haute Maurienne conservent une importante couverture sédimentaire principalement faite de gypse ce qui donne un aspect imposant aux différents massifs. Des carrières de gypses ont été exploitées jusqu'à une date récente, et l'architecture locale a largement utilisé cette matière alors très recherchée pour l'enduit d'une partie des façades des maisons.

La dolomie, plus compacte et résistante que les autres roches sédimentaires, est également très présente sur les pentes situées en amont de la vallée. Imperméable et résistante à l'acide contrairement au calcaire, elle est par conséquent insensible à l'acidité des précipitations, et n'a pas subi les gels du quaternaire lors des phases de cryoclastie. Ces roches préservées se dessinent sous forme de monolithes perçant la couverture sédimentaire du socle cristallin entamée par la lente érosion des sols.

Ainsi, les habitations traditionnelles de cette région reflètent cette richesse, les murs et lauzes de toit étant constitués de quartzite et de gneiss, le bardage de mélèze et d'enduit pour les façades. Cette profusion de roches dans un secteur limité fait le bonheur des géologues et botanistes. Il existe par ailleurs en Haute Maurienne quelques carrières exploitant des roches métamorphiques destinées à l'ornement, telle celle située sur la commune de Sollières-Sardières et produisant des porphyres schisteux. Toutefois ces exploitations restent exceptionnelles du fait de la réglementation environnementale stricte imposée par le parc national.

La Maurienne est le siège de temps à autre de phénomènes d'essaims de séismes, succession de tremblements de terre se produisant au même endroit pendant une période pouvant aller jusqu'à plusieurs années. Ce fut le cas notamment d'un secteur centré sur Montrond au , et du secteur Montgellafrey–La Chapelle–Saint-François-Longchamp au .

La faune et la flore sont composées des espèces suivantes.



La Maurienne possède une histoire en lien avec sa géographie, une vallée intra-alpine permettant le passage et les échanges entre le futur territoire de la France et la péninsule italienne, lui donnant une certaine importance à chacune des périodes de l'histoire plus générale de la Savoie. C'est d'ailleurs dans cet espace qu'est mentionné pour la première fois l'ancêtre de la maison de Savoie, Humbert aux Blanches Mains, portant le titre vers le début du de comte de Maurienne et donnant naissance aux premiers princes de Savoie, futurs rois d'Italie.

Il faut attendre la fonte des grands glaciers alpins et surtout le Néolithique pour que s'installent les premiers individus dans la vallée de Maurienne, en provenance de la péninsule italienne. On peut observer ainsi sur le territoire de la commune de Lanslevillard, des mégalithes comme celui de la "Pierre aux Pieds" (à plus de ) et la "Pierre de Chantelouve". Par ailleurs, dans l'ensemble de la vallée, on retrouve des pierres gravées ou des peintures rupestres comme celles de Bessans ou de l'Arcelle près du Mont-Cenis ou encore sur la commune d'Aussois. Sur le village de Sollières, les archéologues ont trouvé, après une découverte fortuite en 1972, sur le site des Balmes (grotte située à ), une nécropole, indiquant une occupation de - 2900 à la fin de l'âge du fer. Cette découverte a conduit à la création d'un musée consacré à l'archéologie dans le village de Sollières-l'Envers, non loin d'où fut découverte la grotte.

Une remarquable civilisation s'est développée à l'âge du fer () caractérisée par la production de bijoux (bracelets, pendeloques, etc.) par des bronziers locaux. Les nécropoles de tombes en coffres de lauzes sont abondantes (Albiez, Saint-Sorlin et Saint-Jean-d'Arves, Saint-Jean-de-Maurienne, Lanslebourg, Lanslevillard, Montdenis, etc.) Henri Onde (1935-1965), directeur de l'Institut de géographie à l'Université de Lausanne, relève par ailleurs que les .

Avant l'incorporation dans le monde romain, la Maurienne est peuplée de Gaulois. Les Médulles habitent la partie moyenne et basse de la vallée. La Haute Maurienne est quant à elle peuplée par les Graiocèles dont le chef-lieu "Ocellum" pourrait être localisé sur la commune d'Aussois. En - 16, l'ensemble de la vallée est intégrée à la province des Alpes Cottiennes avec pour capitale Suse. Le roi Cottius devient préfet de la province.
Certains historiens font passer vers la vallée de la Maurienne, pour remonter le cours de l'Arc, Hannibal et son armée. Ainsi, une variante de cet itinéraire soutenue par Geoffroy de Galbert traverserait la chaîne de Belledonne au pas de Coche, franchirait le col de la Croix-de-Fer et rejoindrait la Maurienne à hauteur de l'actuel Saint-Jean-de-Maurienne. Ce raccourci de était pratiqué au Moyen Âge à travers une zone assez peuplée. Toutefois, un autre histoire, Serge Lancel, fait observer que le trajet au plus court en zone de montagne n'est pas forcément le meilleur, en raison de l'effort supplémentaire qu'il impose. Il considère que ce trajet détourné par la vallée de l'Arc revêt un intérêt stratégique car l'armée évite ainsi les cols du Petit-Saint-Bernard et de Montgenèvre, connus de ses adversaires, pouvant espérer surprendre les Romains. Trois passages vers l'Italie sont ainsi envisagés par les différents chercheurs en Maurienne : le col du Mont-Cenis (), le col du Petit Mont-Cenis () et le col Clapier ( ou débouchant en val de Suse et le cours de la Doire ripaire.

La cité de Saint-Jean-de-Maurienne reçoit de sainte Thècle, au , des reliques de Jean le Baptiste (trois doigts de la main). À cette occasion, Gontran, roi de Bourgogne, élève une cathédrale dédié à Jean le Baptiste, en 565 ou 574, marquant le début de l'évêché. Au-delà de l'aspect spirituel, Gontran soustrait la vallée à l'autorité de l'évêque de Turin, prenant ainsi le contrôle sur les vallées de Suse et de Briançon. Cette politique lui permet de contrôler les vallées alpines entre son royaume et les territoires lombards, de la plaine du Pô.

Au milieu du un groupe de Sarrasins venus du Fraxinet, dans les environs de l'actuelle Saint-Tropez, s'établit dans les Alpes et notamment dans la vallée de l'Arc. Envoyés par le roi Hugues qui a conclu un traité avec les Sarrasins, ils devaient essentiellement empêcher toute invasion ennemie, principalement en provenance de son rival le roi d'Italie Bérenger. Une partie des Sarrasins quitteront la région, une seconde sera vaincue lors de la bataille de Tourtour et enfin une troisième s'installera dans la région. « Le temps et d'innombrables mélanges de populations firent le reste : lentement, au fil des générations, le contingent sarrasin se dissout ainsi dans la population provençale ».

Lors du passage de Charlemagne en Savoie, celui-ci divise le territoire en comtés, dont celui de Maurienne. Ce dernier est donné, avec le Traité de Verdun (843), à Lothaire jusqu'à son incorporation au nouveau royaume de Burgondie septentrionale. Lors de la traversée de la vallée, une légende veut que l'épée Durandal ait été donnée à Charlemagne par un ange de Dieu, afin qu'il la remette à un comte capitaine. Charlemagne l'offre à son compagnon Roland. Outre la légende, l'épée aurait été forgée grâce aux mines de fer des Hurtières célèbres à l'époque dans toute l'Europe. Vers 1003, un seigneur du nom d’Humbert, surnommé "Blanches-mains" (parent de nombreux hauts dignitaires du clergé), apparaît. Seigneur de fiefs en Genevois, en Tarentaise, en Val d’Aoste (1034), dans le Viennois et la vallée du Rhône, il obtient le titre de comte (sans précisions) puis celui de "comte en Maurienne" en 1043 et 1046. Sa descendance obtient en 1143 le titre de comte de Savoie qui sera une dynastie - la Maison de Savoie - importante au niveau européen au cours de l'histoire.

Bien que la capitale soit transférée à Chambéry en 1232, la cité de Saint-Jean-de-Maurienne maintient le statut de capitale provinciale avec le siège de l'évêché.

Les familles seigneuriales sont nombreuses sur les terres de Maurienne. L'article d'Alexis Billiet sur d'anciens titres de la province (paru en 1837) rappelle quelques noms de grandes familles des :


En 1805, Napoléon établit lors de la traversée des Alpes durant la campagne d'Italie une route nationale qui accentua l'importance commerciale de la vallée.

La même année est inaugurée la nouvelle ligne de télécommunication Paris-Turin qui traversait toute la vallée de l'Arc. À ce jour, de nombreux vestiges de sémaphores subsistent sur les hauteurs de la Maurienne. Le télégraphe de Sollières-Sardières a pour sa part été récemment restauré dans le cadre du bicentenaire de l'invention de ce réseau de communication.

Le 8 mars 1817, une avalanche provenant du massif de la dent Parrachée détruisit partiellement le village de Sollières et emporta la plus grande partie de son église. Elle fut reconstruite quelques années plus tard.

À partir de 1857, le chantier du percement du tunnel ferroviaire du Mont-Cenis entre Modane, alors savoyarde jusqu'au traité de Turin, et Bardonèche, sous l'impulsion de l'administration sarde, permit de relier Turin à la France par la ligne du Fréjus. En raison des retards des travaux de percement du tunnel franco-italien qui étaient prévus sur trente ans, un certain M. Brassey, associé à l’ingénieur anglais Fell, proposa en 1865 à l’empereur Napoléon III, de construire une ligne de chemin de fer entre Saint-Michel et Suse. La ligne passa par le col du Mont-Cenis, avec une locomotive système Fell, suivant pratiquement le tracé de la route. L’exploitation cessa au bout de trois ans en 1871, les travaux de percement du tunnel ferroviaire du Mont-Cenis s’étant accélérés grâce à l’invention de l’ingénieur Sommeiller qui mit en service sa perforatrice à air comprimé. Cet ouvrage accentuant le rôle stratégique de la voie de communication Maurienne-Val de Suse a depuis grandement facilité les échanges entre la plaine du Pô et l'Europe occidentale.

Dans la nuit du 12 au 13 décembre 1917, la Maurienne est le théâtre d'un accident ferroviaire à proximité de Saint-Michel-de-Maurienne, faisant officiellement un peu plus de 427 morts. Les journaux allant quant à eux dénombrer jusqu'à près de victimes.

Durant la Seconde Guerre mondiale, la partie supérieure de la vallée est occupée selon les dispositions de l'armistice du 24 juin 1940 signé à la villa Incisa située dans la région de Rome. La France et le royaume d'Italie sont représentés respectivement par Charles Huntziger et le maréchal Pietro Badoglio. Par cet accord, la Haute Maurienne (canton de Lanslebourg-Mont-Cenis) ainsi que les communes d'Aussois et Avrieux sont annexées au royaume d'Italie et leur administration transférée à Turin. On impose aux habitants d'échanger leur carte d'identité française contre des passeports italiens. À la suite de l'occupation allemande conséquence de la capitulation italienne, le , les villages de la haute vallée de l'Arc subissent de nombreuses représailles et destructions de la part des occupants voulant punir les mouvements de résistances. La région est le théâtre de massacres, les villages tels Lanslebourg ou Bessans sont brûlés. Un camp de concentration est même construit à Modane. Cette dernière est bombardée le 17 septembre 1943 par l'aviation alliée. L'objectif est alors la gare, important centre de transit entre la France et l'Italie au centre des batailles entre les troupes allemandes et les forces alliées. De nombreux obus manquent leur cible et provoquent de lourdes destructions et près d'une centaine de morts parmi les civils. Par la suite, la Haute Maurienne a été au cœur de l'un des plus célèbres combats de la résistance française dans les Alpes. Sur les hauteurs de la commune de Sollières-Sardières s'est déroulée la bataille du Mont-Froid à d'altitude, entre les chasseurs alpins, et des troupes allemandes totalement endoctrinées au cours du mois d'avril 1945. Ces combats livrés dans des conditions extrêmes sont devenus l'un des symboles de la résistance dans les Alpes. À la suite de cette bataille, le traité de Paris est venu rectifier ce qui avait été considéré comme une faiblesse géostratégique et une erreur historique en réintégrant la totalité du plateau du Mont-Cenis jusqu'alors sur le sol italien depuis le découpage de la Savoie durant son annexion en 1860. Ainsi, à la sortie de la guerre, la carte de la Haute Maurienne s'est vue agrandie d'une superficie de . De jure, les communes de Sollières-Sardières, Lanslebourg et Bramans, retrouvaient leurs alpages séculiers qui jusqu'alors étaient en territoire étranger bien qu'ayant toujours été leur propriété, la Maurienne retrouvant finalement ses frontières historiques.

Durant le mois de juin 1957, la Maurienne est dévastée par une crue spectaculaire. Les villes de Modane et de Fourneaux sont les plus touchées. Dès le début du mois, des pluies torrentielles arrosent la vallée et plus particulièrement la Haute Maurienne. Sous l'influence d'un effet de foehn provoqué par la lombarde, la température passe de 8 à plus de 30 degrés. Cela a pour conséquence la multiplication par cinq du débit des cours d'eau, conduisant l'Arc à sortir de son lit. L'alerte générale est donnée dans la nuit du 14 juin aux alentours de 2 heures du matin. La rivière devient incontrôlable, ayant dépassé de plus de sa hauteur normale et charriant dans le courant rondins de bois, pierres et boue ; l'eau s'infiltre dans les habitations et détruit tout sur son passage. Les canalisations explosent sous la pression, et le pont de la Glaire, ne pouvant résister, est emporté. Cette crue détruit les infrastructures de communications, que ce soit le téléphone ou les routes et voies ferrées. Cet incident conduira la population et les autorités à définir une nouvelle politique d'aménagement de la rivière afin d'éviter qu'un tel événement puisse se reproduire. De mars 1962 à 1964, de grands travaux seront entrepris pour endiguer le lit de l'Arc, et ainsi prévenir de nouvelles crues, représentant une menace pour l'ensemble des habitants de la Maurienne.

Dès les années 1900, les progrès technologiques de l'hydroélectricité suisse sont à l'origine d'intenses , qui profitent aux implantations industrielles en Maurienne.



La capitale historique de la Maurienne est Saint-Jean-de-Maurienne, au confluent de l'Arc et de l'Arvan.

La Maurienne correspond à l'arrondissement de Saint-Jean-de-Maurienne rassemblant les cantons de :

Les principales villes, en remontant la vallée vers l’est, sont :

Le projet de liaison ferroviaire transalpine Lyon - Turin pourrait apporter un nouveau souffle économique à la vallée grâce aux lourds travaux prévus pour le percement des différents tunnels. La question de la viabilité économique de cette nouvelle liaison a été mise en cause par la Cour des comptes. Ce projet ne fait pas l'unanimité au sein de la population locale, certaines communes parfois limitrophes ayant des avis diamétralement opposés : la mairie de Villarodin-Bourget s'oppose au projet actuel, alors que .

Bien avant le développement économique et industriel induit par la houille blanche ou plus récemment par le tourisme, la Maurienne a très tôt été un site notable pour l'industrie. En effet, dès 1289, dans sa partie basse au niveau de la vallée de Saint-Georges-d'Hurtières, l'industrie minière a connu ses premières heures de gloire, pour ensuite devenir majeure à partir de 1875.

Dès la fin du , avec le développement de la houille blanche en Maurienne, l'industrialisation a pris un nouvel essor. Cette nouvelle source d'énergie a amené l'installation de grands groupes électrochimiques et électrométallurgiques. Durant l'été 2013, des négociations ont eu lieu pour la cession de l'usine Rio Tinto au profit d'un consortium constitué entre Trimet détenant 65 % du capital et EDF pour les 35 % restants sous les bons auspices du groupe bancaire BNP Paribas. Cette usine produit essentiellement du fil-machine en aluminium utilisé, entre autres, dans le câblage électrique de l'industrie de l'énergie et dans la connectique au sein de l'industrie automobile. Le procédé de fabrication d'aluminium par électrolyse nécessitant beaucoup d'électricité, l'implantation au de l'industrie de l'aluminium s'explique dans cette région par ses nombreux barrages hydroélectriques et son chemin de fer.

Le Laboratoire souterrain de Modane (LMS), centre de recherche fondamentale de l'IN2P3 (CNRS) et du CEA, est situé au milieu du tunnel routier du Fréjus. Il s'agit du laboratoire le plus profond en Europe et son volume disponible est de . Mis en place à partir de 1982, il se situe au km 6,5 du tunnel routier du Fréjus, à sous la pointe de ce dernier ( équivalent eau). Grâce à sa profondeur, lui offrant une isolation totale aux divers rayonnements, le LSM accueille des expériences de recherche fondamentale en physique des particules, astroparticules et physique nucléaire mais aussi des détecteurs d'ultra-faible radioactivité permettant des mesures environnementales, des applications dans le domaine de la datation, la détermination de l'origine géographique de produits, ou bien encore des bancs de tests en microélectronique.

La soufflerie de l'ONERA, située administrativement sur les communes d'Avrieux et de Modane, est le plus grand parc de soufflerie d'Europe. Elle emploie essentiellement des techniciens et chercheurs. Elle regroupe un ensemble de souffleries simulant des écoulements allant des vitesses subsoniques aux vitesses hypersoniques. Ses travaux vont de la recherche spatiale à l'aéronautique, en passant par la défense et la sécurité. Historiquement, le premier matériel de soufflerie, installé en Autriche, a été cédé au titre des réparations de la Seconde Guerre mondiale, en faveur de la France sortie victorieuse ; il provenait de l'Ötztal, dans la région du Tyrol alors sous occupation française à la suite du conflit.

Ces deux installations sont à la pointe de la technologie scientifique mondiale. Les chercheurs voudraient profiter de la construction (en cours) d’une voie de secours dans le tunnel pour creuser un laboratoire dix fois plus grand.

La vallée de Maurienne possède de nombreuses infrastructures hydroélectriques (barrages), comme celles du Mont-Cenis, de Bissorte, une partie de Grand'Maison partagé avec le département de l'Isère, ou bien encore des plans d'Amont et d'Aval dans la vallée surplombant Aussois.

La vallée compte une vingtaine de stations de sports d'hiver, depuis la petite station village tel Albiez-Montrond, où fut créé le célèbre Opinel dans les années 1890, aux "stations de génération", créées de toutes pièces dans les années 1970 tel Le Corbier et Les Karellis. Certaines de ces stations se sont associées pour créer de grands domaines skiables, comme Les Sybelles, Galibier-Thabor ou Val Cenis Vanoise.

Ces stations proposent d'autres activités que le ski, comme des pistes de randonnée en raquettes ou plus originales comme des promenades en traîneau à chiens ou à ski tracté par un cheval (Ski joëring).
Le parc national de la Vanoise est une zone de protection du biotope alpin, créé en 1963 et partagé entre la Tarentaise au nord, et la Maurienne au sud.

La Maurienne, aux côtés du Beaufortain, de la Tarentaise et du val d'Arly, a reçu le label « Villes et Pays d'art et d'histoire » pour le projet "Pays des Hautes vallées de Savoie" (1991), en lien le développement de l'art baroque dans ces vallées.

Depuis quelques années, le syndicat des Pays de Maurienne, organisme chargé de la promotion touristique de la vallée donne un nouvel élan au développement du territoire au travers du cyclisme en mettant en place une marque « La Maurienne, le plus grand domaine cycliste du monde ». En effet, la Maurienne de par sa géographie est dotée de pistes cyclables relativement planes en fond de vallée, mais aussi de routes de montagnes mythiques, donnant accès à des cols alpins légendaires d'altitude variée, tels le Télégraphe culminant à , la Madeleine atteignant , mais également le Galibier rejoignant le parc national des Écrins à une altitude de , sans oublier l'Iseran, col routier le plus haut des Alpes à . Ces cols et paysages sont couverts par le célèbre Tour de France, événement sportif le plus regardé dans le monde après les Jeux olympiques et la Coupe du Monde de football. Cette compétition, comme dans une moindre mesure celles du Critérium du Dauphiné ou du Tour des Pays de Savoie, semble avoir un impact indirect sur la fréquentation touristique, comme l'indique le directeur du parc national de la Vanoise, Emmanuel de Guillebon, à propos de l'espace protégé.

La Maurienne est également située sur la route des Grandes Alpes, du col de l'Iseran au col du Galibier en passant entre-autres par Bonneval-sur-Arc, Lanslebourg-Mont-Cenis, Modane, Saint Michel de Maurienne et Valloire.

La Maurienne possède de nombreux musées qui reflètent son histoire, son artisanat et son agriculture traditionnelle. Ainsi, le musée du Félicien situé à Argentine permet de découvrir le quotidien des paysans d'autrefois, la coutellerie est présente à Saint-Jean-de-Maurienne au travers du musée de l'Opinel.


La quasi-totalité de la Haute Maurienne est incluse dans le parc national de la Vanoise et son aire optimale d'adhésion. Premier parc national en France, crée en 1963, il jouxte le parc national italien du Grand Paradis, offrant le plus grand espace protégé des alpes. Il s'y trouve une population importante de chamois mais également de bouquetins qui, avant la création du parc national de la Vanoise, étaient les derniers survivants de leur espèce dans les Alpes, hormis la harde du Grand Paradis. Le parc offre également refuge à nombre d'espèces de la faune alpine, tels que les lièvres arctiques, les tétras lyres, les lagopèdes alpins, ainsi que le Triton alpestre. L'aigle royal, symbole de la Maurienne et de la Tarentaise ornant leurs blasons respectifs, est solidement implanté en Vanoise avec 29 couples et plus de 24 aiglons selon le dernier recensement de 2010 Le gypaète barbu a fait l'objet d'une réintroduction récente qui est une réussite. Le loup a atteint la région depuis 8 ans au moins. Le lynx est également présent et surtout en Basse Maurienne.. Le retour de ces prédateurs a modifié la gestion du pastoralisme, la plupart des troupeaux d'ovins et de caprins sont dorénavant surveillés par des patous.

De nombreuses zones en Maurienne sont classés Natura 2000.


De 1926 à 1976, toutes les locomotives (telles les CC 7100 et CC 6500) équipées spécifiquement pour capter le courant par un troisième rail sur la ligne Chambéry-Modane étaient surnommées « Maurienne ». Une sous-série de CC 6500 circulait ainsi en livrée dite « Maurienne », d'une couleur vert bleuté foncé 312, bandes blanches et marquages jaunes ou blancs. Aujourd'hui ne subsiste sous cette livrée que la CC 6558.

La rame TGV Sud-Est 68 porte le blason de la ville de Modane.

La locomotive BB 22287 porte le blason de la ville de Saint-Jean-de-Maurienne.
Un escadron d'hélicoptères, autrefois stationné sur la base aérienne de Chambéry, porte le nom de Maurienne.



le site de mutualisation des Archives départementales de la Savoie et de la Haute-Savoie - sabaudia.org a consacré 3 dossiers à la vallée, aujourd'hui consultable sur savoie-archives.fr :


</doc>
<doc id="16982" url="https://fr.wikipedia.org/wiki?curid=16982" title="Robert Ier de Normandie">
Robert Ier de Normandie

Robert de Normandie (v. 1010 – ), dit Robert "le Libéral ou plus couramment Robert "le Magnifique, est duc de Normandie d'août 1027 à sa mort survenue en Terre sainte, lors d'un pèlerinage. Il est le père de Guillaume « le Bâtard » dit Guillaume le Conquérant.

L'historien Lucien Musset dépeint Robert le Magnifique comme « une personnalité violente et difficile ». Il est ainsi parfois considéré comme l'inspirateur du personnage légendaire de Robert le Diable. Sa rencontre avec Arlette (ou "Herlève"), qui deviendra sa « "frilla" » (épouse « à la manière danoise »), ou son passage à Constantinople ont donné lieu à des histoires plus ou moins légendaires. Sa mauvaise réputation provient en partie des conditions douteuses qui lui permirent d'accéder au trône de Normandie.

Robert le Magnifique était le second fils du duc Richard II de Normandie. À la mort de ce dernier en 1026, son fils aîné Richard III lui succéda naturellement tandis que Robert se voyait confier la vicomté d'Hiémois avec pour capitale Exmes. Ce dernier préféra résider au château de Falaise plutôt que dans la motte féodale de bois d'Exmes. Dans la même année ou la suivante, le cadet se révolta contre le duc. L'armée ducale se présenta alors devant Falaise où était retranché le rebelle. Robert capitula et se soumit à son frère. Il rendit l'hommage vassalique à Richard et put conserver le comté d'Hiémois. Mais en 1027 Richard III mourut empoisonné. Aussitôt, Robert écarta de la succession le fils bâtard du défunt, Nicolas, et monta lui-même sur le trône. Si Guillaume de Jumièges ne dévoile pas le nom de l'empoisonneur, les écrivains plus tardifs, comme Wace n'ont pas hésité à accuser Robert du crime. Il apparaît en effet comme le principal bénéficiaire de la mort de Richard III. 

Âgé de dix-sept ans environ, Robert devint donc duc en 1027. Il montra rapidement qu'il entendait tenir la Normandie d'une main de fer.

Vers 1027/1028, Guillaume I de Bellême, un seigneur des confins méridionaux de la Normandie, se révolta. Le duc vint l'assiéger dans Alençon. Le rebelle fut contraint à la reddition lors de laquelle Robert le Magnifique lui imposa la pesante humiliation de se présenter devant lui avec une selle de cheval sur les épaules.

Dans les débuts de son principat, Robert le Magnifique apprit que l'évêque Hugues de Bayeux recrutait des soldats en France pour renforcer la défense de son château d'Ivry. Furieux d'être tenu à l'écart du conseil ducal, il comptait faire de la forteresse normande un pôle de résistance au duc. Robert réagit rapidement : il se présenta devant Ivry avant même que Hugues ne soit revenu de France. L'évêque dut négocier un sauf-conduit pour ses fidèles déjà réfugiés dans le château contre son propre exil. Il ne fut autorisé à revenir en Normandie qu'en 1032, au plus tard, mais il resta plutôt à l'écart de la cour. Le conflit entre le duc et l'évêque de Bayeux est peut-être en lien avec celui entre Robert le Magnifique et son oncle Robert le Danois.

Fils du duc Richard I de Normandie, Robert le Danois était l'un des personnages les plus puissants du duché puisqu'il était comte d'Évreux et archevêque de Rouen. Guillaume de Jumièges nous explique que Robert le Magnifique se déclara son ennemi, sans que l'on sache les raisons de cette soudaine opposition. Peut-être l'archevêque goûtait-il peu la politique ducale envers l'Église. Les historiens ont remarqué en effet qu'au début de son principat, Robert le Magnifique enleva des terres aux abbayes et aux grandes églises pour les distribuer à de jeunes nobles (tel Roger de Montgommery). C'était un moyen de les fidéliser et de les récompenser à moindre frais. Mais le duc rompait ainsi avec l'attitude de ses prédécesseurs, notamment Richard II, qui s'étaient montrés généreux avec l'Église

Robert le Danois lui a-t-il adressé des remontrances pour ces usurpations ? Toujours est-il que le duc s'enflamma contre lui et partit faire le siège d'Évreux en 1027/1028. Après avoir mis en défense la cité, l'archevêque préféra négocier. Il choisit l'exil et se rendit auprès du roi Robert « le Pieux ». Il ne s'avouait pas pour autant battu. Pour faire fléchir son neveu, il lança l'anathème sur la Normandie. La sanction ecclésiastique fit son effet : Robert le Magnifique rappela Robert le Danois et le rétablit dans ses charges comtales et archiépiscopales.

Ce conflit entre l'archevêque et le duc semble constituer une inflexion dans la politique religieuse de Robert le Magnifique. Robert le Danois retrouva une haute position à la cour et il semble avoir convaincu son neveu qu'une bonne entente avec l'Église était indispensable. Plusieurs faits attestent de ce revirement. La réconciliation paraît avoir lieu selon Lucien Musset dès 1028.

Tout d'abord, ils constatent tous deux l'état déplorable des biens de la cathédrale de Rouen. De nombreuses terres sont alors restituées. Robert le Magnifique signe des chartes à plusieurs abbayes pour confirmer leurs biens ou pour les restituer. Les abbayes de Fécamp et de Saint-Wandrille et la cathédrale Notre-Dame de Rouen figurent parmi les bénéficiaires de ces actes. Le duc poussa même quelques-uns de ses vassaux à le suivre dans ce mouvement.
Renouant avec les actions de son père Richard II, Robert fonda deux monastères. En premier lieu, l'abbaye de Cerisy. Cette fondation, le est pionnière puisqu'elle intervint dans l'ouest de la Normandie, une région dépourvue de monastères en dehors du Mont-Saint-Michel. Ensuite, le , Robert le Magnifique refonde l'abbaye de Montivilliers en remplaçant les moines par des moniales, à l'instigation de sa tante Béatrice. C'est le premier établissement féminin en Normandie. Fondée en 684, l'abbaye avait disparu avec les invasions scandinaves. Dans ce domaine, il fut une nouvelle fois accompagné par des seigneurs du duché : ainsi Gosselin, vicomte d'Arques et son épouse Emmeline fondent en 1030 l'abbaye de la Trinité du Mont et plus tard restaurent en 1042 l'abbaye Saint-Amand à Rouen. Leurs donations sont confirmées par Robert qui affranchit l'abbaye de son pouvoir judiciaire. Onfroy de Vieilles installa des moines à Préaux alors qu'un simple chevalier Herluin pose les bases près de la Risle d'un monastère appelé à un grand avenir : Le Bec.

Enfin, peu après, le duc s'apprêtait à partir en pèlerinage à Jérusalem. Beaucoup d'historiens ont vu derrière ce voyage la preuve d'un repentir chez Robert pour avoir empoisonné son frère Richard III. Là encore, ce n'est que spéculations. Le départ du duc était un risque car la Normandie allait se retrouver sans maître. De plus, on ne revenait pas toujours vivant de ce voyage lointain. Avant de partir, le duc conscient de cette difficulté, rassembla les grands du duché à Fécamp.

Lors de l'assemblée tenue à Fécamp le marquée par la refondation de Montivilliers, il demanda à tous ceux présents, Robert, archevêque de Rouen, les évêques de la province et les grands seigneurs de reconnaître comme héritier son jeune fils, Guillaume, âgé d'environ sept ans. Tous prêtèrent serment de fidélité. Les barons semblent avoir accepté la décision ducale sur le moment, mais probablement avec réticence. Ils reprochaient à Guillaume de ne pas être issu d'une union légitime. Robert n'avait cependant pas le choix : c'était son seul enfant masculin. Selon Wace, Guillaume fut placé sous la garde du roi de France Henri. Il arrangea également un mariage entre Herlève avec Herluin, un seigneur de la vallée de la Risle.

Le duc partit en pèlerinage au début de l'année 1035 avec quelques barons comme le sénéchal Turstin, Odon Stigand et Drogon de Vexin, et prit la route terrestre pour rejoindre Rome. L'empereur byzantin Michel IV l'accueillit ensuite à Byzance. Le duc de Normandie parvint jusqu'à Jérusalem mais il mourut, à l'âge de vingt-cinq ans, durant l'été 1035 sur le chemin du retour, à Nicée. .

À l'image de ses prédécesseurs, le duc de Normandie se montra un allié précieux et ennemi redoutable pour les princes voisins.

En 1031, lors de la mort du roi de France Robert le Pieux, son fils aîné et successeur Henri se heurta à une révolte de son frère cadet Robert, appuyé par sa mère Constance d'Arles. Le comte de Blois Eudes II se mêla à l'opposition contre le nouveau roi. Face à une telle coalition, Henri I dut quitter le domaine royal et trouver refuge à Fécamp auprès du duc de Normandie. Ce dernier l'aida dans son entreprise de reconquête. Il demanda notamment à son oncle, le comte Mauger de Corbeil, d'intervenir militairement aux côtés du roi. Le frère rebelle fut vaincu à la bataille de Villeneuve-Saint-Georges et demanda la paix, permettant à Henri I de régner comme roi des Francs. Pour prix de son appui, le duc de Normandie aurait reçu la suzeraineté sur la partie du Vexin entre l'Epte et l'Oise : le Vexin français. C'est en tout cas ce que précise Orderic Vital mais il est le seul narrateur du Moyen Âge à indiquer cette concession. Les historiens David Bates, Jean-François Lemarignier et plus récemment Pierre Bauduin doutent de l'affirmation du chroniqueur anglo-normand.

Le duc de Normandie apporta aussi un soutien décisif au comte de Flandre Baudouin IV. Vers 1030, ce dernier dut faire face à une rébellion de son fils Baudouin. Il trouva en Robert l'aide militaire dont il avait besoin pour reprendre en main le comté. Robert le Magnifique entra en Flandre s'empara du château de Chocques. Effrayés, les grands abandonnèrent le fils, qui à son tour, consentit à rendre le pouvoir à son père.

La cour normande accueillait depuis le principat de Richard II de Normandie les deux fils du roi anglo-saxon Æthelred II. Celui-ci avait dû quitter son royaume en 1013, chassé par les Danois. Après l'avoir reconquis en 1014, Æthelred était mort deux ans après. Depuis 1016, Knut le Grand, le roi de Danemark, régnait donc sur l'Angleterre. Richard II témoigna d'une certaine neutralité vis-à-vis de son voisin d'outre-Manche, d'autant plus que le Danois avait épousé sa sœur Emma, la tante de Robert. Mais cette femme, veuve d'Æthelred II, était aussi la mère des deux enfants d'Æthelred, réfugiés en Normandie : Alfred et Édouard.

Au contraire de son père, Robert le Magnifique s'engagea clairement en faveur des deux cousins exilés. Il envoya à Knut une ambassade pour lui demander de rendre le royaume aux enfants d'Æthelred. Devant son refus, le duc de Normandie convoqua les grands du duché et ordonna de construire une flotte pour envahir l'Angleterre. Les bateaux, chargés en vivres, en armes et en hommes, se rassemblèrent à Fécamp et prirent la mer mais une tempête déporta les navires vers Jersey. Les Normands ne débarquèrent pas en Angleterre.

À la suite de Rollon, les ducs de Normandie interviennent régulièrement en Bretagne. En 1008, la mort du duc breton Geoffroi I laisse le pouvoir à sa femme Havoise de Normandie, sœur de Richard II. Le duché de Bretagne est alors placé sous la tutelle du duc de Normandie. Les rapports entre Normandie et Bretagne sont alors empreints d'une grande proximité. Cependant le fils d'Havoise et Geoffroi, Alain devenu adulte, souhaite s'émanciper de la tutelle normande. Selon Guillaume de Jumièges, le duc de Bretagne refuse l'allégeance à Robert le Magnifique, signal de la guerre entre les deux duchés.

Après avoir consolidé son pouvoir, Robert construit une forteresse sur la frontière, située proche du Couesnon selon Guillaume de Jumièges. Selon la traduction choisie, le lieu de son implantation est Cherrueix (Ille-et-Vilaine) ou Chéruel (Manche). Il lance ensuite une attaque terrestre soutenue par sa flotte qui ravage la côte. Alain riposta en envahissant l'Avranchin mais les Normands Alfred le Géant et Néel II de Saint-Sauveur écrasèrent les Bretons dans une bataille.

Alain s'incline et sollicite la médiation de l'archevêque Robert, parent commun. Il se reconnait vassal de Robert lors d'une rencontre au Mont-Saint-Michel.

Parents :

Pas d'épouse mais au moins deux « "frillas" » (concubine "à la manière danoise") :
On lui attribue parfois comme épouse légitime Estrith, la sœur de Knut II de Danemark mais c'est très peu probable.

Enfants :





</doc>
<doc id="16984" url="https://fr.wikipedia.org/wiki?curid=16984" title="Radioamateur">
Radioamateur

Les radioamateurs sont des personnes qui pratiquent, sans intérêt pécuniaire, un loisir technique permettant d'expérimenter les techniques de transmission et par conséquences d'établir des liaisons radio avec d'autres radioamateurs du monde entier. Beaucoup d'avancées technologiques sont dues aux radioamateurs, c'est par exemple grâce à eux que les fréquences au-dessus de sont aujourd'hui utilisées. L'activité radioamateur permet d'acquérir ainsi des connaissances techniques dans les domaines de la radio et de l'électronique et de développer des liens d'amitié entre amateurs de différents pays.

Chargée de la réglementation et de la planification des télécommunications dans le monde, l'Union internationale des télécommunications donne les définitions concernant la radio d'amateur.

Dans l'expression radioamateur, le mot "amateur" doit être compris dans son sens premier . Le qualificatif "amateur" n'est pas une appréciation péjorative de la qualité des opérateurs qui ont souvent un niveau de technicité et de savoir-faire très élevé. D'autre part il sous-entend que les communications se font à titre privé et ne peuvent en aucun cas faire l'objet d'une quelconque rémunération.

L'Union internationale des télécommunications recommande : 
Ces règles et recommandations sont appliquées dans la plupart des pays dans les mêmes termes.

Les radioamateurs peuvent discuter de technique radio, mais on voit également apparaître dans les conversations d'autres sujets scientifiques connexes comme la météorologie, l'informatique, l'astronomie, etc. Aujourd'hui, l'usage admet qu'on aborde également ce qui concerne la vie associative pour autant que chaque opérateur discute en son nom propre et non pas au nom de tierces personnes. Les radioamateurs ont un devoir de réserve et s'interdisent d'aborder les thèmes politiques et religieux. 
Pour clarifier une confusion fréquente, les radioamateurs, radioécouteurs (SWL) et cibistes partagent tous la passion pour la radio de loisir. Il s'agit cependant de pratiques différentes qui correspondent chacune à une législation spécifique. Il n'est pas rare de voir un amateur passer d’une activité à l’autre.
La licence radioamateur, grâce aux diverses bandes et à la puissance supérieure qu’elle autorise, donne plus de possibilités que la CB ("Citizen-band)", qui reste limitée aux bandes des , et des appareils de faible puissance dans la bande des. Cependant, de nombreux cibistes, sur la bande qui leur est allouée, pratiquent un trafic proche de celui des radioamateurs.
Quant aux radioécouteurs, encore appelés SWL, abréviation de "Short Wave Listener" (écouteurs sur ondes courtes), ce sont les passionnés de l'écoute des communications radio, souvent de futurs radioamateurs en attente de franchir le pas vers un matériel plus complet et une licence d’émission. Les personnes qui écoutent les stations de radiodiffusion sont parfois appelées BCL pour "Broadcast Listener".

Le nombre total de radioamateurs licenciés dans le monde est proche de trois millions, avec une population très inégale selon les pays. Environ 15 % des radioamateurs sont des femmes (« YL », pour "Young Lady"). En France elles sont 2,31 % soit 344 YL en 2011.

Les pays les plus représentés sont les suivants :

À l'opposé, des pays comme la Tunisie, Vanuatu ou le Laos, ne comptent que deux ou trois licenciés.

Seuls la Corée du Nord et le Yémen n'autorisent pas le radioamateurisme.


Après la guerre, les magasins de « surplus » militaires permettent de trouver des émetteurs et récepteurs à bas prix, comme les "command set" américains ou les "FuG" allemands, modifiables pour les bandes amateurs, ainsi que des composants spéciaux en abondance comme les « quartz » FT243. Les récepteurs HF les plus recherchés sont le BC342 des surplus, ou le « HRO » américain.

Dans les années 1960, des fournisseurs américains proposent des équipements spéciaux pour radioamateurs, comme "Hallicrafters" et "Hammarlund". En Europe, l'italien "Geloso" propose récepteurs, émetteurs ou modules. Les kits adaptés aux passionnés comme ceux de "Heathkit" apparaissent. Mais les amateurs construisent toujours leur matériel, en télégraphie ou téléphonie, ou modifient les équipements aéronautiques déclassés pour démarrer en . La modulation BLU , plus performante, apparaît en HF et supplante progressivement l'AM (modulation d'amplitude).

Les années 1970 voient la généralisation des émetteurs-récepteurs BLU . Les amateurs les plus favorisés peuvent acheter la "Collins S-line" (la « Rolls Royce » des amateurs), les débutants se contentent des kits monobande HW32 de "Heathkit". La yagi tribande en HF fait son apparition sur les toits, signalant les amateurs sans ambiguïté.

Ces marques historiques, presque mythiques, des années 1960-70, comme Collins, Swan, Drake, Heathkit, laissent progressivement la place aux trois fournisseurs asiatiques Icom, Kenwood et Yaesu.

, le radioamateurisme connaît un déclin en Europe de l'Ouest, concurrencé par le loisir Internet, alors qu'il explose en Europe de l'Est après la libéralisation des régimes. Les amateurs construisent de moins en moins leur matériel HF ou VHF, mais les expérimentateurs et techniciens s'orientent vers les micro-ondes, le « "packet-radio" », les relais, la télévision amateur...




Le spectre hertzien est une ressource naturelle. Son utilisation est réglementée par l'Union internationale des télécommunications (IUT) et l'Union internationale des radioamateurs (IARU) au sein de l'ONU, chargée de définir la répartition et les modalités d'utilisation des fréquences hertziennes.

Le Règlement des radiocommunications (RR) comprend les règles liées au service radioamateur. Il est révisé tous les trois ans lors de la Conférence mondiale des radiocommunications (CMR). La refonte de l' du Règlement des radiocommunications lors de la Conférence de 2003 a, en particulier, supprimé l’obligation de la connaissance du code Morse pour utiliser les fréquences au-dessous de . Ceci tend à se généraliser, mais certains pays, dont la Russie, continuent de l'exiger.

L'activité radioamateur est accessible à tous, quel que soit le niveau d'instruction, y compris aux enfants, bien que certains pays imposent un âge minimum de 13 ans.

La Conférence Européenne des administrations des Postes et Télécommunications (CEPT) régule et réglemente l’utilisation des radiocommunications au niveau européen. Elle émet des avis, des recommandations et des décisions en se basant sur le Règlement des radiocommunications.
Elle est composée de 44 membres (Union Européenne et pays d’Europe centrale et orientale).

Les recommandations significatives :

Les radioamateurs européens sont autorisés à émettre sous la double condition d'avoir subi avec succès l'examen permettant la délivrance du « certificat d'opérateur des services d'amateur » après quoi, sous réserve de paiement d'une taxe spéciale annuelle, leur est délivrée l'autorisation administrative accompagnée de l'indicatif d'appel.

En France, c’est l'Agence nationale des fréquences (ANFR) qui assure la gestion des indicatifs et des fréquences et organise le passage des examens, sous contrôle de l’Autorité de régulation des communications électroniques et des postes (ARCEP).

En Belgique, l'autorité de régulation est l'Institut belge des services postaux et des télécommunications (IBPT).

En Suisse, le radioamateurisme est supervisé par l'Office fédéral de la communication (OFCOM), dans le cadre des lois régissant les télécommunications.

Les radioamateurs sont les seuls opérateurs de service radio ayant le droit de réaliser leur propre équipement sans homologation technique. Ceci impose un examen de niveau technique dans la plupart des pays pour éviter les gênes ou brouillages aux autres services ainsi que les risques de sécurité.

La connaissance obligatoire du code Morse a cependant été supprimée depuis 2012, sauf dans de rares pays (Russie par exemple).

L'âge moyen des radioamateurs est proche de , beaucoup d'amateurs obtenant leur licence entre 40 et . Pour lutter contre ce vieillissement, un assouplissement de la licence permettant d'opérer en "novice" avec un examen plus simple et des limites de puissance et de bandes plus réduites a vu le jour dans la plupart des pays pour encourager et rajeunir le radio amateurisme. En France moins de 1 % des radioamateurs ont moins de 25 ans !

En France, depuis 2012 entre en vigueur une licence de classe unique radioamateur avec un unique certificat d’opérateur de la classe 2 CEPT . 
Les titulaires des différents certificats d’opérateur des services d’amateur délivrés antérieurement à 2012 conservent les bénéfices de leur classe et de leur indicatif d’appel personnel (donc l'ex classe 1 dite « radiotélégraphiste » et l'ex classe 3 dite « novice »).

Une précision toutefois, ne pas confondre Certificat d'opérateur du service amateur et licence d'émission.

Le Certificat d'opérateur du service amateur s'obtient en réussissant l'examen technique et de législation, mais ne donne pas pour autant le droit d'émettre. Après et seulement après cette obtention on peut demander à l'administration le droit d'émettre, et seulement là, l'administration délivre un indicatif officiel et donc le droit d'émettre matérialisé par une licence d'émission. Cette licence est renouvelée tous les ans par le payement d'une redevance versée au centre des impôts adéquat, qui s'élève à annuel (montant 2017). Par mesure disciplinaire la licence d'émission peut-être supprimée ou suspendue, alors que le certificat d'opérateur est à vie.

L'indicatif, délivré par l'administration, est l'identifiant de la station d'amateur — une sorte de numéro d'immatriculation — et l'opérateur certifié, responsable de l'utilisation qui est faite de sa station, utilisera cet indicatif pour transmettre. L'indicatif étant attaché à la station d'amateur, si un opérateur autre (dit « occasionnel ») souhaite utiliser la station, il devra obtenir l'autorisation de l'opérateur titulaire, puis, dans son trafic, utiliser l'indicatif de la station qu'il opère, suivi de son indicatif propre. C'est le cas lors de l'utilisation d'une station d'amateur de radio-club par exemple. Chaque opérateur doit respecter les prérogatives (bandes de fréquence, puissance, type de transmission) de son certificat personnel lorsqu'il opère sur une station d'amateur. Il utilisera l'indicatif du radio-club suivi de son propre indicatif lors des transmissions.

En jargon radioamateur on utilise le terme de « pirate » pour désigner toute émission hors des réglementations : puissance, limites de bande, usurpation d’indicatif ou trafic sans licence ou autorisation. Ces infractions peuvent être poursuivies et sévèrement sanctionnées, surtout en cas de brouillage d'autres services.

L'histoire des indicatifs se confond avec l'histoire du radioamateurisme.

Au tout début de l'émission d'amateur, vers les années 1906–1907, chaque radioamateur choisissait son indicatif comme il le souhaitait. Dès 1912, la conférence Radiotélégraphique Internationale demande d'établir une liste qui permettra d'identifier tous les opérateurs utilisant le spectre radioéléctrique. 1920 pour les radioamateurs, l'indicatif serait composé d'un chiffre indiquant le pays d'origine (voir tableau ci-contre) suivi de deux ou trois lettres.

En 1921, un réseau d'émission d'amateur fonctionne dans la région Marseille et chacun s'identifie avec un indicatif personnel de son choix : presque tous les nouveaux amateurs utilisent "8AAA", signe de l'influence des anciens du Génie. Sous la pression des amateurs, l'adminisration des PTT délivre le 13 juillet 1921 la première autorisation d'émission d'amateur sous l'indicatif « 8AA » à André Riss de Boulogne-sur-Mer. Ce furent ensuite 8AB (Léon Deloy), 8BF (Pierre Louis), 8CA (Réginald Gouraud), 8GL (Jack Lefebvre)… Les indicatifs étaient réattribués au fur et à mesure qu'ils n'étaient plus utilisés. Étant donné que les liaisons transatlantiques ne s'envisageaient pas à ce moment, radioamateurs des États-Unis ont pris le chiffre « 1-9 » ; les risques de confusions avec l'Europe n'existaient pas encore.

Ce système a été conservé jusqu'en 1923 où la première liaison transatlantique (8AB, Léon Deloy et 1MO, Fred Schnell) a nécessité d'augmenter le nombre de préfixes nationaux en raison du grand nombre de pays pouvant potentiellement être en contact. Pour cela, on a adjoint une lettre en tête de l'indicatif, qui, au départ, était l'initiale du pays. Mais le nombre de pays devenant de plus en plus grand, il a fallu ajouter une deuxième lettre pour certains pays jusqu'à arriver au système actuel. L'ajout des préfixes de nationalité sous forme de lettres a permis d'utiliser le chiffre pour créer des catégories internes à chaque pays ; différenciation chronologique et, plus tard, de la classe de licence.

Cet indicatif obéit à un format standard de la forme PPxSSSS défini par le Règlement des radiocommunications :


Parfois non : 

Ces préfixes sont identiques à ceux utilisés pour identifier les aéronefs ou les bateaux, ou des stations radio autres qu'amateurs ;
Les relais et balises françaises ont un indicatif commençant par F1Z ou F5Z. Exemple la balise de Beaune : F1ZAW.

Exemples : F5ABC est une station radioamateur française fixe disposant de tous les privilèges ;
ON3ABC est une station radioamateur belge utilisée par un amateur disposant d'une licence de base (novice) ;
VE2AAA est une station radioamateur canadienne située au Québec.

Dans certains pays, par exemple au Canada, le préfixe associé à un chiffre est déterminé par la région où se trouve la station.
En Russie, les chiffres dépendent de l'Oblast (région) dans laquelle réside l'amateur.

Le détail des indicatifs selon les pays et provinces peut être obtenu sur les sites radioamateur, comme l'ARRL.

Si on suit cette nomenclature, il peut exister des indicatifs de trois signes du genre « PnS » tel que l'indicatif « C3W » attribué à une station de Chypre (5B). Il s'agit d'un indicatif spécial.

Aux États-Unis on les appelle les indicatifs « 1x1 » et sont réservés à la commémoration d'événements spéciaux.

Des indicatifs plus longs sont autorisés, en respect de la législation.
La plupart des pays utilisent ce format raccourci à 3 ou car, d'une part il permet d'identifier rapidement la station célébrant un événement, ce qui attire les amateurs, et d'autre part, accélère les contacts (QSO).

En parallèle, les administrations nationales peuvent distribuer des indicatifs particuliers (que certains logiciels radioamateurs, et parfois les radioamateurs eux-mêmes, ne reconnaissent pas comme étant valides) tel que « Z3100A » qui célèbre le centième anniversaire d'une institution. En 2005, l'indicatif HE1TELE est attribué aux radioamateurs valaisans de Suisse (HB), pour le Téléthon.

En général, c'est l'administration de tutelle qui délivre l'indicatif au radioamateur en fonction de sa classe de licence ou de son lieu de résidence selon le cas.

Certaines administrations autorisent l'utilisateur à choisir lui-même le suffixe et parfois le chiffre qu'il souhaite utiliser dans son indicatif, mais ce n'est pas le cas en France où le chiffre correspond à la classe d'indicatif.

Certaines opératrices (appelées « YL » pour "Young Lady" dans le jargon radioamateur) peuvent — compte tenu de leur petit nombre — obtenir parfois un indicatif se terminant par YL (exemple : F4XYL ou HB9XYL). De la même façon, des radioamateurs régulièrement en voyage à l'étranger demandent parfois « X » après le chiffre, tel « 5U7X ».

Les radioamateurs utilisent différents modes de transmission pour communiquer. Les communications en téléphonie (transmission de la voix) sont les plus courantes. Elles peuvent être réalisées, par exemple, en modulation de fréquence (FM) pour une bonne qualité de son ou encore en bande latérale unique (BLU) qui permet une bonne efficacité avec une qualité du son et une bande passante réduites.
La radiotélégraphie (CW, abréviation de "Continuous Wave") qui utilise le code Morse est une activité qui date des tout premiers temps de la radio. Aujourd'hui la technologie a fait disparaître le code Morse de presque toutes les communications et son apprentissage n'est plus obligatoire pour la licence de radioamateur dans la plupart des pays. Cependant beaucoup de radioamateurs continuent d'utiliser le code Morse, en particulier sur 
les bandes décamétriques, ou pour des expérimentations comme la réflexion sur la Lune "(Moon Bounce)" en raison de son efficacité. De plus, un code international autorise des contacts avec le monde entier en s'affranchissant de la barrière linguistique. Dans chaque bande radioamateur des plages de fréquences sont réservées par l'IARU à ce type de transmission.

L'apparition des ordinateurs personnels a permis de développer les modes de transmission numériques comme le radiotélétype (RTTY) qui autrefois demandait la mise en œuvre d'un équipement mécanique lourd. Les radioamateurs ont conduit le développement du packet radio qui a utilisé des protocoles comme TCP/IP dès les années 1970. Des modes numériques spécifiques comme le PSK31 permettent des communications en temps réel, à faible puissance, sur les ondes décamétriques.

Les modes couramment utilisés : 

Les radioamateurs peuvent émettre sur une vingtaine de bandes réparties régulièrement sur tout le spectre radioélectrique. Ces bandes sont globalement les mêmes dans le monde entier. Cependant, certaines ne sont allouées aux radioamateurs que dans certaines régions UIT, d'autres voient leurs limites modifiées selon la région. De plus, les législations nationales peuvent induire quelques différences entre pays.

Les règlements de l'UIT définissent trois régions :

Le tableau suivant donne une liste de toutes les bandes autorisées dans le monde. Les limites indiquées correspondent aux maxima constatés ; bien souvent, les législations locales des pays imposent des limites plus restreintes 

Les bandes des , et sont parfois appelées « bandes WARC » par les radioamateurs car elles leur ont été affectées relativement récemment par une conférence du "WARC" ("world administration radio conference" devenue World Radio Conference).

Les ondes décamétriques (HF entre et ) en particulier les bandes historiques des , , , et , permettent des liaisons lointaines par réflexion ionosphérique, selon la densité des couches réflectrices, liée elle-même au cycle solaire d'environ . Il y a donc des pics de propagation et des creux, tous les onze ans, plus ou moins prévisibles.

Les bandes au-dessus de permettent surtout des liaisons à vue optique, avec cependant de nombreuses propagations exceptionnelles, par exemple sur la couche E sporadique, ou sur les essaims de météorites. La plus utilisée est la bande – (bande des deux mètres), où de nombreux répéteurs (relais) amateurs permettent d'effectuer des liaisons lointaines avec un matériel économique.

Les bandes situées au-dessus de sont très favorables à l'expérimentation, tant pour la réalisation du matériel que pour les communications entre points hauts à — relativement — grandes distances.

L'équipement d'une station est très variable, selon le budget, la passion, et les possibilités du terrain ou de l'environnement. Un débutant peut démarrer avec un dipôle et un émetteur-récepteur d'occasion, alors qu'une grosse station de club peut représenter un capital et un volume considérables.

Les premiers amateurs devaient construire leur équipement entièrement, mais leur nombre croissant a amené des constructeurs à proposer les matériels de base pour HF et VHF. De plus l'évolution vers des modes de transmissions plus complexes, bande latérale unique (BLU), RTTY, Packet, où les précisions de fréquence et les filtrages demandent des techniques complexes, a fait que la majorité des amateurs utilisent du matériel commercial. Les mordus des fréquences hautes, au-dessus de , doivent au contraire réaliser en grande partie leur équipement, parfois à partir de récupération.

Le matériel de base est aujourd'hui un "transceiver" combinant émetteur et récepteur, en bandes HF ou VHF, avec une puissance de environ. En HF, il est parfois complété par un amplificateur linéaire de ou .

Les antennes sont encore le domaine le plus ouvert aux réalisations personnelles, quoique beaucoup utilisent des antennes commerciales. Sur les bandes décamétriques on trouve, par exemple, la classique antenne Yagi tribande à 3 éléments sur les bandes , , .
En fréquences basses (, , ), les dipôles filaires sont largement utilisés, alors qu'en VHF, c'est l'antenne Yagi éventuellement en plusieurs nappes ("stacking") qui équipe la plupart des stations.

Le "shack", ou local du radioamateur, peut être un simple placard, mais la station d'un radioamateur bien équipé en trafic « classique », comme la photo ci-dessous en exemple, est assez volumineuse :

L'ensemble des antennes — leur hauteur et leur gain — est plus important pour optimiser les contacts que les équipements radio proprement dits. Un exemple extrême est donné ci-contre.

Les antennes décamétriques de la station finlandaise OH1AF que l'on voit sur la photo à droite comportent, sur un pylône rotatif de :
Elles sont complétées par des yagis VHF et UHF de 15 éléments au sommet et une antenne filaire à la base.
Les radioamateurs utilisent leur station, soit pour faire des contacts individuels au hasard d’un « appel à tous » (CQ), ou en réponse à un appel, ou sur rendez-vous à une heure et fréquence précises "(schedule)", soit à plusieurs sous forme de tables rondes par radio. Certains se joignent à des heures régulières et sur des fréquences connues pour constituer ainsi un réseau (ou "net") dirigé par une station que l'on appelle « "Net Control" ». Ces nets peuvent être informels ou, au contraire, orientés sur des activités spécifiques comme l'aide d'urgence.
Plus que le message lui-même, c'est la façon d'établir le contact qui passionne le radioamateur. Les communications radioamateurs ne doivent pas être comparées aux communications de type Internet ou téléphones portables. Les radioamateurs échangent évidemment des informations et des nouvelles sur les ondes, mais le but reste toujours celui de la radio en elle-même : tout mettre en œuvre pour réussir un contact en utilisant les ondes radio et les phénomènes naturels par le biais desquels elles peuvent se propager. Dans certains cas un contact peut être un véritable exploit personnel.

La plupart des contacts se font en alternat, c'est-à-dire en utilisant la même fréquence en émission et réception. La procédure ressemble donc à la procédure professionnelle de radiocommunication, avec quelques simplifications.

Entre deux stations qui ne se connaissent pas, les règles sont assez strictes :

Entre stations qui se connaissent, ou qui conversent à plusieurs, l'usage est de simplifier un peu la procédure, le code d'alternat est inutile, l'habitude suffit pour s'insérer entre deux mots grâce à la commutation automatique des émetteurs modernes (VOX).

En cas de "pile-up" sur une station rare (DX), les fréquences d'émission et réception peuvent être décalées pour éviter le brouillage (split), et la procédure est très simplifiée pour faire passer le maximum d'amateurs. En concours (contest ou TEST en télégraphie) c'est encore plus succinct, un simple échange avec trois mots suffit.

Les modes numériques modernes comme le packet utilisent un protocole automatique, qui fait ressembler le contact à un échange de mails, sur un logiciel comme "Airmail" ressemblant à un logiciel de messagerie Internet.

Les liaisons par satellites amateurs utilisent le "full duplex", avec émission et réception sur deux bandes VHF ou UHF différentes, ou en "cross-band", c'est-à-dire une voie sur une bande et l'autre voie sur l'autre.

Pour faciliter les communications qui peuvent s'étendre à l'ensemble des pays du monde, les radioamateurs communiquent majoritairement en anglais (avec un vocabulaire souvent restreint) en utilisant le code Q et des abréviations internationales admises par tous.
Exemple de quelques abréviations courantes utilisées en télégraphie et en téléphonie :


Les radioamateurs utilisent le "code Q" universel en attribuant à certains codes une signification adaptée à leur pratique. On rencontre fréquemment les exemples ci-dessous :


Ces codes et abréviations, initialement prévus pour simplifier la transmission de messages en télégraphie, sont aussi utilisés en téléphonie. L'usage de ces codes en télégraphie dispense (dans une certaine mesure) de connaître l'anglais ou la langue du correspondant.

De nombreux plaisanciers navigateurs sont également des radioamateurs. En mer, ils utilisent leur indicatif suivi du suffixe /MM « maritime mobile ». Outre le plaisir des contacts, ils peuvent ainsi se signaler à leurs proches et obtenir un support technique ou météo, parfois même du secours. Selon les pays, les échanges personnels doivent être limités aux appels de détresse ou de simple position. Pour permettre ces contacts, des stations terrestres s'organisent en « réseaux » avec des fréquences et des horaires connus .Une émission Thalassa a été consacrée à « Herb » un amateur canadien qui fournissait en permanence la météo et des conseils de route aux plaisanciers en traversée atlantique.

Le service maritime mobile utilise les fréquences de trafic maritime « navire à navire » , ou encore des fréquences commodes proches des bandes amateurs.

Le trafic en télévision amateur (TVA) consiste à transmettre des images de télévision en temps réel, le plus souvent une simple mire comportant son indicatif et sa position, ou encore des images de sa station ou de soi-même. Aux États-Unis des relais retransmettent régulièrement les émissions de télévision de la NASA.
La télévision d'amateur, en raison de son importante bande passante et de la stabilité nécessaire du signal, utilise les bandes à partir de jusqu'à et même au-delà. Les portées obtenues généralement sont de l'ordre de 50 à , Des relais en réseau permettent d'obtenir des transmissions sur plusieurs centaines de kilomètres.



L'activité presque sportive de la chasse aux contacts rares, aux concours, aux diplômes comporte plusieurs facettes :

Les associations nationales ou les revues internationales, comme , organisent annuellement des concours radioamateurs. Pendant ces compétitions amicales mais parfois intenses qui durent un ou plusieurs jours, les participants essaient d'accumuler un maximum de contacts. Finalement, un décompte de points dont les règles dépendent du règlement du concours définit un classement. Aucun prix n'est à la clé, simplement un diplôme en papier, et la fierté de son score.

Par exemple le CQ-WW dure deux jours, en téléphonie et en télégraphie. Pendant ce concours, considéré comme une sorte de championnat du monde des radioamateurs, les mieux placés accumulent plusieurs milliers de contacts avec des centaines de pays différents.

Certains participants installent temporairement des stations très performantes sur des points hauts ou des îles bien placées, pour bénéficier d'une propagation idéale et de l'intérêt des autres participants.

En France, la coupe du REF est organisée par l'association nationale des radioamateurs, le REF (Réseau des Émetteurs Français) .

La recherche de liaisons lointaines ou rares donc difficiles — les « DX » — est une des facettes de l'activité radioamateur. Elle peut parfois ressembler à un sport, voire une compétition, par la lutte qui en résulte.

Les radioamateurs les plus mordus de trafic organisent des expéditions vers des points isolés pour ajouter temporairement dans la liste internationale un pays DX actif au monde radioamateur et, du coup, bénéficier d'un intérêt accru pendant cette période. Ainsi plusieurs radioamateurs américains dans les années 60 ont fait le tour du monde en bateau, allant d'île en île, pour apporter de nouveaux contacts possibles à la communauté. Des radioamateurs français ont organisé une expédition vers l'îlot isolé et inhabité de Clipperton uniquement dans ce but.

Des expéditions scientifiques récentes ont également emporté une station radioamateur pour augmenter leur sécurité en cas de panne des autres moyens de communication, ainsi que pour l'aspect éducatif, par exemple celle de Jean-Louis Étienne.

Les contacts rares concrétisés par une « carte QSL » de confirmation font la fierté du "shack" et permettent d'accéder à des diplômes. Les diplômes sont de toutes sortes selon la fréquence, le mode utilisé, et son règlement, allant du diplôme amical d'une ville, jusqu'au classement mondial des chasseurs de DX, le DXCC.

Quelques diplômes connus :

Les SWL (écouteurs) peuvent également participer aux diplômes en envoyant une carte spéciale (QSL) aux stations qu'ils ont entendues. Ils permettent ainsi aux stations émettrices de savoir jusqu'où elles ont été captées, et, à leur tour, ces stations émettrices enverront aux SWL leur carte, en remerciement. Ce sont ces cartes QSL reçues en retour qui permettront à l'écouteur de participer aux concours.

Pour faciliter les échanges internationaux urgents par courrier postal le coupon-réponse international permet à l'expéditeur de recevoir une réponse plus rapide lorsque le destinataire est à l'étranger en lui fournissant le timbre du retour.

Le monde radioamateur est un monde associatif sur les plans international, national et local.
De nombreuses associations ont pour vocation de fédérer les radioamateurs autour de projets et d'activités aussi diverses que :

Les Radio-clubs sont des associations locales regroupant les radioamateurs d'une ville ou d'une région. Le club permet aux radioamateurs de se regrouper et de partager leur passion. C'est un excellent endroit pour avoir un premier contact avec le monde de l'émission d'amateur. C'est aussi un lieu d'échange, de formation, d'activités autour de projets.

Les radioamateurs œuvrent également à l'éveil des jeunes dans le domaine de la technique. Certains groupes de radioamateurs viennent animer des ateliers dans les classes où les enfants réalisent des petits montages électroniques.Sur un plan plus spectaculaire, des radioamateurs ont permis à des élèves de réaliser des contacts avec les astronautes de la station spatiale internationale (ISS). Ces contacts se font dans le cadre du projet ARISS. Les questions posées aux astronautes sont préparées par les enfants et les instituteurs. Lors du contact, les radioamateurs entrent en liaison radio avec l'astronaute à bord, lisent les questions et les enfants peuvent alors entendre les réponses. Le passage ne dure que 10 minutes, mais chacun gardera bien longtemps le très bon souvenir de ce moment exceptionnel.

Dès 1961, les radioamateurs ont construit des satellites pour leur usage propre. Pour trafiquer à l'aide de ces satellites les techniques mises en œuvre sont assez sophistiquées aussi bien dans la prévision des passages que dans la poursuite des satellites avec les antennes.

La réalisation de chaque satellite amateur est gérée par un club généralement issu des universités, écoles d’ingénieurs ou de l’industrie aérospatiale, et leur lancement effectué bénévolement par les agences spatiales. Leur usage en relais est ouvert à tous les amateurs disposant du matériel adéquat. 
Depuis 1996, l'AMSAT-France a pour mission de faciliter l'accès aux communications par satellite pour les radioamateurs.

Le 25 avril 2016 a été lancé Oufti1, satellite belge construit par des universitaires et à usage des radio-amateurs.

Ces relais, ou répéteurs, utilisés aussi bien pour la phonie, la TV ou tout autre mode de transmission utilisent des bandes de fréquences à partir de la VHF et au-delà. Ils sont situés d'ordinaire sur des points hauts (montagnes, collines, immeubles de grande hauteur) et permettent à des opérateurs de communiquer sur des centaines de kilomètres carrés avec une simple station portable de faible puissance. Les relais peuvent également être liés entre eux en réseau. Ils sont, en particulier, beaucoup utilisés par les radioamateurs trafiquant depuis leur véhicule. Ces relais sont installés et entretenus par les radio-clubs régionaux.

Les associations radioamateurs fabriquent, installent et entretiennent des balises qui leur permettent d'étudier la propagation des ondes, et cela sur toutes les fréquences disponibles. Il s'agit d'émetteurs automatiques émettant en continu. D'ordinaire, les balises diffusent l'indicatif qui leur est attribué par l'Administration, leur position et un signal continu, plus ou moins long, qui permet d'effectuer des mesures. Une carte des balises est tenue à jour par les associations locales et nationales et est disponible sur l'Internet. Elles sont coordonnées par le coordinateur d'une des trois régions I.A.R.U. (International Amateur Radio Union). Tous les radioamateurs et écouteurs peuvent ainsi se mettre à l'écoute des balises et tenir un journal de leur réception (jour, heure, réception ou pas et force du signal reçu).

Pour assurer des opérations de secours, ou de support aux plaisanciers, les radioamateurs s’organisent en réseau.
C'est Hiram Percy Maxim, cofondateur de l'ARRL ("American Radio Relay League", l'association radioamateur américaine) qui attribua aux activités radioamateurs pour la première fois en 1914 une fonction de service à la population en répartissant des stations radio relais le long des six principales routes qui traversaient les États-Unis. Aujourd'hui, les réseaux MARS et ARES assurent ce service en collaboration avec la protection civile et l'armée qui interviennent chaque fois qu'un désastre est annoncé.

Les sont bien adaptées à une utilisation à bref délai dans les cas d'urgence.

Les secours sont en droit d'établir des contacts radios par le truchement d'une station de radioamateur, dans le cas de catastrophe . Depuis la zone de catastrophe l'organisation intervenante sur dans la zone demande à l'opérateur radioamateur d'appeler n'importe quelle autre station de radioamateur , si possible située dans le pays à contacter, pour établir un contact direct et immédiat par téléphone avec le secrétariat de l'organisation ou avec la station de radio de l'organisation.

L'Angleterre a mis en place le réseau RAYNET tandis que les radioamateurs belges travaillent en collaboration avec la Croix-Rouge de Belgique.

Il existe en France la « Fédération nationale des radioamateurs au service de la sécurité civile » (FNRASEC). Le mot « radiotransmetteurs » a remplacé « radioamateurs » en 2001 puis est redevenu « Radioamateurs » en 2009. Ses membres sont susceptibles d'assurer deux types de mission de soutien aux pouvoirs publics :
La FNRASEC regroupe des « Associations départementales des radioamateurs au service de la Sécurité civile » (ADRASEC). La FNRASEC est affiliée à la « Fédération nationale de protection civile » (FNPC), une association de secouristes bénévoles.
Il existe aussi des associations départementales indépendantes de la FNRASEC.

Le radioamateurisme est un vaste terrain fertile pour l'expérimentation technique.
Même s'il existe un choix important de matériel commercial pour le trafic en HF ou en VHF dans les modes usuels, on trouve encore de nombreux radioamateurs qui prennent plaisir à construire eux-mêmes leur propres antennes et équipements.
L'apport technique et scientifique des radioamateurs est bien réel dans le développement des techniques de transmission ainsi que dans l'étude des différents modes de propagation. Sans doute aujourd'hui cela est-il moins vrai en raison de la rapidité du progrès dans les domaines des télécommunications, et surtout de la nécessité croissante de matériel de mesure et de composants extrêmement sophistiqués et coûteux. Mais ce manque de moyens est en partie compensé par le nombre important de radioamateurs qui expérimentent sans compter leur temps. Le radioamateurisme reste par essence une activité favorisant l'instruction individuelle et le partage de la connaissance pour les sciences et les techniques. De façon générale, la pratique du radioamateurisme peut être la base d'une solide formation technique et représente ainsi une chance de pouvoir aborder le côté « pratique » d'un savoir théorique.

Parmi les domaines les plus ouverts aux recherches et réalisations personnelles, on peut citer :


On entend par là des fréquences de plusieurs dizaines de gigahertz — c'est-à-dire de longueurs d'onde centimétriques — dans un domaine où l'expérimentation est de mise et les réalisations personnelles obligatoires.

Aujourd'hui, des radioamateurs construisent des émetteurs-récepteurs utilisant les fréquences optiques ( > λ > ). De nombreux modes opératoires sont possibles grâce aux LED classiques ou laser que l'on peut moduler comme on le souhaite.

L’écoute et même l’émission dans les bandes LF et au-dessous ne peuvent s’effectuer que par des montages personnels. Par exemple, l'expérimentation sur demande la création d'antennes spéciales, et l'écoute des bandes ELF s'effectue avec des logiciels d'analyse du signal sur ordinateur.

Les radioamateurs utilisent la « lecture au son », c'est-à-dire que le message est compris par l'écoute du signal du correspondant. Pour être capable de soutenir un trafic à des vitesses suffisamment élevées (de 10 à 35 groupes de cinq signes à la minute selon l'adresse des opérateurs) il faut évidemment envisager un apprentissage et un entraînement sérieux. Comme aide, on peut utiliser un ordinateur qui générera et corrigera des messages en code morse, on peut aussi écouter sur les fréquences amateurs des stations qui émettent à heures fixes à des vitesses accessibles aux débutants et leur permettent ainsi d'acquérir petit à petit des vitesses supérieures. Il est possible de s'entraîner à la pratique du code Morse soit seul, soit en communauté dans un radio-club. Cette dernière solution est vivement recommandée pour une plus grande efficacité.

L'apprentissage de la manipulation est en général plus rapide que la lecture au son, la vitesse imposée par l'examen étant modeste. L'ancienne « pioche », encore utilisée par les passionnés, ou les manipulateurs semi-automatiques mécaniques de type « Vibroplex » qui ont toujours leurs partisans, ont laissé place aux manipulateurs électroniques calibrant traits et points et réduisant la fatigue de l'opérateur.




</doc>
<doc id="16986" url="https://fr.wikipedia.org/wiki?curid=16986" title="Magnitude apparente">
Magnitude apparente

La magnitude apparente est une mesure de l'irradiance d'un objet céleste observé depuis la Terre. Utilisée quasi exclusivement en astronomie, la magnitude correspondait historiquement à un classement des étoiles, les plus brillantes étant de « première magnitude », les deuxièmes et troisièmes magnitudes étant plus faibles, jusqu'à la sixième magnitude, étoiles à peine visibles à l'œil nu. Elle est à présent définie suivant une échelle logarithmique inverse, dans laquelle la magnitude augmente d'une unité lorsque l'irradiance est divisée par environ 2,5. Ainsi, plus un objet céleste est brillant, plus sa magnitude est faible voire négative. Il est habituel de définir la magnitude zéro comme étant celle de l'étoile Véga, aux erreurs d'étalonnage près.

La mesure de la magnitude se fait par photométrie dans une ou plusieurs bandes spectrales (ultraviolet, spectre visible, infrarouge) grâce à des systèmes photométriques tels que le système UBV. Généralement, la magnitude est donnée dans la bande spectrale V (visuel) et se voit alors appelée magnitude visuelle, notée m ou simplement V. Les plus grands télescopes parviennent à détecter des objets céleste jusqu'à une magnitude limite de 30.

Le premier classement des étoiles en fonction de leur brillance remonte à l'Antiquité où au l'astronome grec Hipparque aurait réalisé un catalogue d'un millier d'étoiles visibles à l'œil nu. L'échelle comprend alors six « grandeurs » : les étoiles les plus brillantes sont de première grandeur et les étoiles les moins brillantes encore visibles à l'œil nu sont de sixième grandeur. Cette méthode de classement a été ensuite popularisée dans "Almageste" de Ptolémée au .

Au , Galilée, qui observe le ciel avec sa lunette astronomique, est contraint de créer une septième grandeur pour classer les étoiles visibles seulement avec son instrument.Jusqu'au , des nouveaux échelons et des niveaux intermédiaires sont ajoutés peu à peu à l'échelle avec l'amélioration des instruments d'observation. Ainsi, dans les années 1860, le catalogue "Bonner Durchmusterung" comptait classées en neuf grandeurs. Mais ce classement peut fortement varier selon le jugement des observateurs et, face à la multiplication des étoiles et de leurs catalogues, il devient nécessaire de trouver un procédé d'observation moins subjectif. Plusieurs techniques sont mises au point dont l'une des plus utilisées est le photomètre : il convertit la luminosité en courant électrique qui est ensuite comparé à des valeurs étalons. Malgré cela, les différences restent trop grandes et il faut trouver une loi de variation de la luminosité des astres.

En 1856, Norman Robert Pogson propose une nouvelle classification où il remplace le mot « grandeur », qui évoque trop fortement l'idée de taille, par celui de « magnitude » (qui en est un synonyme savant). Il remarque qu'une étoile de première magnitude est plus brillante qu'une étoile de sixième magnitude. Ainsi une baisse d'une magnitude représente une baisse de luminosité égale à soit environ 2,512. Cette nouvelle échelle logarithmique respecte une propriété physiologique de l'œil sur sa sensibilité à la lumière (loi de Weber-Fechner). Cette échelle entraine la reclassification de la vingtaine d'étoiles de première magnitude (Sirius, Véga, Bételgeuse...) dont les éclats sont trop différents et la création de magnitudes négatives. Pogson choisit comme point zéro pour son échelle l'Étoile Polaire (α Ursae Minoris) dont il fixe la magnitude à 2. Mais par la suite les astronomes se rendent compte que l'Étoile Polaire est une étoile variable et ils choisissent comme nouvelle référence l'étoile Véga avec une magnitude de 0. Mais de nouveau, il apparait que Véga est légèrement variable et depuis le , les astronomes utilisent des sources lumineuses stables en laboratoire telles que le système Gunn, le système STMAG ou la magnitude AB.

La magnitude apparente formula_1 s’écrit :

où formula_2 est l'éclairement ou éclat de l'étoile en Jansky () et où formula_3 est une constante permettant de définir l'origine de l'échelle. Cette constante est fixée par l'astronome au moment de l'observation pour accorder ses mesures de la magnitude d'étoiles standards par rapport à leurs magnitudes connues et inscrites dans les catalogues.

Dans le cas où la luminosité intrinsèque de l'étoile est bolométrique, nous appliquons la loi de Stefan-Boltzmann nous permettant d'aboutir à l'expression suivante :

formula_4

où formula_5 est le rayon de l'étoile en mètre, formula_6 la température effective de l'astre en Kelvins et formula_7 la constante de Stefan-Boltzmann. 

La formule est plus couramment utilisée pour comparer les magnitudes apparentes de deux objets célestes et ainsi déduire la magnitude de l'objet inconnu (1) par rapport à celle d'un objet connu (2), comme l'étoile Véga dont la magnitude est fixée à 0.

La magnitude n'est mesurée que dans une petite partie du spectre électromagnétique appelée bande spectrale. La valeur est donc différente selon le choix de la bande : "U" (ultraviolet), "B" (bleu), "V" (visuel), "R" (rouge) ou "I" (infrarouge). Lorsque la mesure s'effectue sur la totalité du spectre électromagnétique, il s'agit d'une magnitude bolométrique. Elle peut être obtenue en appliquant une correction bolométrique BC à la magnitude absolue ou apparente.

La magnitude visuelle, notée m ou directement V, est la magnitude dans la bande spectrale "V" qui correspond le mieux à la sensibilité de l'œil. C'est cette magnitude qui est généralement utilisée lorsqu'il n'est donné aucune précision sur la bande spectrale observée.

La magnitude photographique, notée m, est mesurée à partir d'une plaque photographique plus sensible dans le bleu. Ainsi, la magnitude photographique diffère de la magnitude visuelle : elle montre les étoiles bleues plus brillantes et les étoiles jaunes moins brillantes. Au contraire, la magnitude photovisuelle, notée m, est mesurée à partir d'une plaque orthochromatique plus sensible dans le jaune. Avec des filtres adéquats, on obtient alors une magnitude correspondant à la magnitude visuelle. Ces deux méthodes sont considérées comme obsolètes et ont été remplacées par des systèmes photométriques mesurant les magnitudes sur plusieurs bandes spectrales. Le plus utilisé est le système photométrique UBV (ou de Johnson) créé dans les années 1950 par Harold Johnson et William Wilson Morgan.

Pour les objets célestes étendus comme les galaxies ou les nébuleuses, on parle plutôt de brillance de surface ou brillance surfacique. Elle s'exprime comme une magnitude par unité d'angle solide, par exemple une magnitude par seconde d'arc au carré.

Pour les étoiles variables, c'est-à-dire les étoiles dont l'éclat varie au cours de périodes plus ou moins longues, on donne les magnitudes maximale et minimale et la période de variation.

La mesure de la magnitude des étoiles doubles ou des étoiles multiples renvoie la magnitude totale du système stellaire, qui n'est pas égale à la somme des magnitudes des étoiles le composant. Si on connait le nombre d'étoiles du système, il est possible de différencier les magnitudes. Elles sont liées par la formule :

où formula_8 est la magnitude totale du système et formula_9 et formula_10 les magnitudes des étoiles le composant.

La magnitude limite d'un instrument désigne la plus faible luminosité observable dans une configuration instrumentale et une bande spectrale données. La magnitude limite visuelle est la magnitude limite dans la bande spectrale "V" (visible).

La magnitude limite visuelle de l'œil nu est de 6, celle des jumelles de 10, et celle des grands télescopes terrestres ou des télescopes spatiaux comme Hubble est de 30. Cette limite est sans cesse repoussée et il est prévu que le Télescope géant européen en cours de construction ait une magnitude limite de 34.

La magnitude apparente dépend de la luminosité intrinsèque de l'objet céleste et de sa distance à la Terre. Cependant, un autre phénomène entre en compte : une partie de la lumière est absorbée par les poussières et les gaz du milieu interstellaire. Cette quantité absorbée est appelée extinction ou absorption interstellaire notée A. Ce phénomène est moins important dans les grandes longueurs d'ondes que dans les petites, c'est-à-dire qu'il absorbe plus les bleus que les rouges. Cela crée un effet de rougissement qui fait qu'un objet parait plus rouge que la réalité.

L'indice de couleur d'une étoile désigne la différence entre les magnitudes apparentes de cette étoile obtenues dans deux bandes spectrales différentes. Il existe plusieurs indices en fonction des bandes utilisées : , ...

La magnitude absolue est une mesure de l'irradiance intrinsèque d'un objet céleste, au contraire de la magnitude apparente qui dépend de la distance à l'astre et de l'extinction dans la ligne de visée. Pour un objet situé à l'extérieur du Système solaire, elle est définie par la magnitude apparente qu'aurait cet astre s'il était placé à une distance de référence fixée à (environ ). 

La comparaison de la magnitude absolue avec la magnitude apparente permet une estimation de la distance de l'objet.

où formula_1 est la magnitude apparente, formula_12 la magnitude absolue et formula_13 la distance exprimée en parsecs. La valeur formula_14, appelée module de distance, est d'une certaine manière une unité de mesure de la distance comme l'année-lumière et le parsec.

 


</doc>
<doc id="16992" url="https://fr.wikipedia.org/wiki?curid=16992" title="Astrométrie">
Astrométrie

L'astrométrie, mieux connue autrefois sous le nom d'astronomie de position, est la branche de l'astronomie qui évalue la position, la distance et le mouvement des étoiles et des autres objets célestes. Elle donne aux astronomes un cadre de référence pour leurs observations et sert à l'élaboration du Temps universel.

L'astrométrie est fondamentale dans des domaines comme la mécanique céleste, la dynamique stellaire et l'astronomie galactique. Elle est également la base observationnelle de l'étude de la dynamique des corps du Système solaire, permettant notamment de confirmer le principe de Copernic et l'héliocentrisme.

L'origine de l'astrométrie remonte au moins à l'Antiquité. 

Au , Hipparque compile le premier catalogue d'étoiles et invente l'échelle de magnitude apparente.

Au cours du temps, l'astrométrie a subi différentes évolutions avec l'invention du cadran solaire, de l'astrolabe, du télescope et du sextant.

De nos jours, les mesures des distances des objets très éloignés sont effectuées par des méthodes photométriques ou par l'utilisation d'indicateurs secondaires comme la loi de Tully-Fisher pour les galaxies, qui relie la vitesse maximale d'une étoile à la magnitude absolue de la galaxie.

L'astrométrie peut s'effectuer à l'aide de différents systèmes de coordonnées célestes. 

Le plus simple est le système de coordonnées horizontales, qui fait intervenir la « sphère locale ». Cependant, l'astrométrie moderne utilise le système de coordonnées polaires pour repérer la direction des astres. Chacun des astres doit être représenté par un point sur la surface d'une sphère de rayon unité. Pour repérer la position d'un des points, il faut la reporter sur deux plans perpendiculaires passant par le centre de la sphère à l'aide des deux autres angles. 

Une variété de facteurs introduisent des erreurs dans la mesure de positions stellaires, incluant les conditions atmosphériques, les imperfections dans les instruments et des erreurs faites par l'observateur ou les mesures d'instruments. Plusieurs de ces erreurs peuvent être réduites par une variété de techniques comme l'amélioration des instruments et la compensation des données.

Les premières estimations de la distance qui nous sépare des étoiles les plus proches ont été effectuées par des mesures précises de la parallaxe, une méthode de triangulation utilisant l'orbite terrestre comme référence. 

Entre 1989 et 1993, le satellite artificiel Hipparcos, lancé par l'Agence spatiale européenne, a mesuré la parallaxe d'environ étoiles avec une précision de l'ordre de la milliarcseconde, ce qui a permis de déterminer la distance d'étoiles éloignées de nous de plus de parsecs.

Pour les astronomes amateurs, il existe plusieurs programmes permettant d'effectuer de l'astrométrie. Certains sont plus performants que d'autres. "Astrometica" de Herbert Raab offre beaucoup de fonctions d'analyse et il est idéal pour les besoins des astronomes amateurs. Un autre logiciel très efficace et convivial est "LagoonAstrométrie" de Benjamin Baqué. Mais ce dernier est plutôt destiné à l'identification d'objet.





</doc>
<doc id="16996" url="https://fr.wikipedia.org/wiki?curid=16996" title="Albiez-Montrond">
Albiez-Montrond

Albiez-Montrond est une commune française située dans le département de la Savoie, en région Auvergne-Rhône-Alpes.

La commune accueille par ailleurs sur son territoire la station de sports d'hiver du même nom.

Albiez-Montrond est un village situé entre et d'altitude et voisine des communes d'Albiez-le-Jeune et de Fontcouverte-la-Toussuire, Albiez-Montrond fait partie du massif « Arvan-Villards », espace des Sybelles.

Village situé sur un plateau ouvert, entouré de hauts sommets à fort dénivelé. Albiez se compose d'une douzaine de villages ou hameaux répartis entre d'altitude et : Le Plan ou chef-lieu, la Cochette ; le Collet-d'en-Haut ; le Collet-d'en-bas, la Colonne et Carreley, le Fregny ou Freigny, Gevoudaz (), le Mollard (), La Saussaz, la Villette sur Albiez-le-Vieux, et La Ville (anciennement le chef-lieu) ; le Chalmieux ; le Gouthier ; les Rieux sur Montrond.

La commune située dans la vallée des Arves surplombe Saint-Jean-de-Maurienne distant de .

L'altitude de la commune est donnée dans le tableau ci-dessous :

La rivière de l'Arvan coule en contrebas de la commune.

Le climat y est de type montagnard. L'hiver est plutôt froid et neigeux et l'été est assez chaud de jour et froid de nuit.

Le tableau ci-dessous indique les données climatiques de la commune (toutes les données ne sont pas encore connues).

"Albiez-Montrond" est un toponyme composé de l'ancienne commune d"'Albiez-le-Vieux" et de celle de "Montrond", à la suite de la fusion mise en place par arrêté préfectoral le (publié au J.O. le ).

"Albiez-le-Vieux" est mentionnée dans l'histoire sous les formes "In Albieys vetulum" (1040), "de Albiaco veteri" (1303), "Arbié le Vieux" (1557) ou encore Albié le Vieux (1793). Il dérive de "Albiez" qui rencontre dés 739 sous la forme "Colonica in Albiadis", dans les cartulaires de l'église-cathédrale de Grenoble, dit cartulaire de Saint Hugues. Le toponyme semble dériver du nom d'un domaine gallo-romain "Albiacum".

La séparation des deux paroisses "Albiez-le-Vieux" et "Albiez-le-Jeune". En 1184, un document mentionne "Ecclesias de duobus Albiacis".

Le toponyme "Montrond" se trouve dés 1038 avec la forme "Monte Rotundo", puis plus tard à la find du "Montrion" et on trouve en 1730 la forme actuelle "Montrond". Il dérive de "Mont riond" et trouve son origine dans la forme latine "mons rotondus", « le mont arrondi ». Au niveau local, on utilise la forme "Mont riond".

En francoprovençal, le nom de la commune s'écrit "Arbyié lo Vieuye", selon la "graphie de Conflans".

L'occupation humaine s'est faite depuis la période prèhistorique comme en attestent certains vestiges retrouvés notamment au Plan.

Les habitants de la commune sont appelés les Albiens et les habitantes les "Albienches".

En cyclisme, Albiez-Montrond fut à l'arrivée de la et dernière étape du Tour de l'Avenir 2017. La montée fut classée en première catégorie et Pavel Sivakov remportait cette étape tandis qu' Egan Bernal conservait son maillot jaune.

Albiez est un domaine skiable, de type familial, qui culmine à d'altitude. 67 hectares de pistes alpines. 18 remontées mécaniques, fils neiges et tapis roulant pour les débutants. Pistes de tous niveaux. Boardercross et snowpark. Possibilité de pratiquer le yooner sur les pistes autorisées. Pistes de luges en accès libre et luge sportive. La station met à disposition, en accès libre, un domaine nordique.

La station a obtenu plusieurs labels « Famille Plus Montagne » ; « Station village » et « Nouvelles glisses ». En 2017, la commune est labellisée « Station verte ».

En 2014, la capacité d'accueil de la commune et station, estimée par l'organisme "Savoie Mont Blanc", est de répartis dans . Les hébergements se répartissent comme suit : ; ; /auberges de jeunesse ; ou gîtes d'étape et .





Production de fromage Beaufort, AOC.






</doc>
<doc id="16997" url="https://fr.wikipedia.org/wiki?curid=16997" title="San Luis Potosí">
San Luis Potosí

San Luis Potosí est la capitale de l'État de San Luis Potosí, Mexique. La ville est située à au nord-nord-ouest de Mexico, et à une altitude moyenne de . Sa population s'élevait à habitants en 2005 et, pour l'ensemble de l'agglomération, de habitants. Sa superficie est de .

Son nom évoque saint Louis, roi de France, son saint patron, et fait également référence aux riches mines de la région bolivienne de Potosí.

San Luis Potosí fut fondée à la fin du . Pendant le règne du vice-roi de Nouvelle-Espagne, la ville était considérée comme un des principaux centres miniers, agricoles, commerciaux, culturels, religieux, administratifs et politiques.

La ville se démarqua par sa participation dans la lutte pour l'indépendance du Mexique (1810-1821) et fut pendant tout le et le début du un centre politique, militaire, idéologique et religieux actif.

San Luis Potosí est considérée comme le berceau de la révolution mexicaine, car le plan de San Luis, qui fut l'appel général au soulèvement armé, y fut publié en 1910. Pour son activité civique et démocratique pendant la deuxième moitié du , la ville est également considérée comme pilier de la nouvelle démocratie mexicaine.

Actuellement (en 2006), il s'agit d'une importante ville industrielle située dans une région riche pour l'agriculture, le bétail et le minerai. C'est aussi un point stratégique pour le commerce et l'éducation, grâce à sa localisation géographique et à ses moyens de communication et de transport. Son architecture baroque, néoclassique et éclectique lui permet d'être candidate à la classification au patrimoine de l'humanité de l'UNESCO.

La ville abrite les pouvoirs exécutif, législatif et judiciaire de l'État, et possède une mairie, ainsi qu'un conseil municipal élu au suffrage universel direct, et est divisée en quatre districts électoraux. Son blason (voir image), qui est aussi le blason de l'État, est divisé en deux parties, azur et or, avec deux lingots d'or et d'argent. Saint Louis y est représenté sur une colline ("el cerro de San Pedro"). La ville est aussi le siège d'un archevêché et possède une cathédrale.


Données de INEGI (2000), la population analphabète de 15 ans ou plus dans la ville comptait sur 6,9 % et seulement 13 % de la population pouvait finir leurs études supérieures.

Dans la ville il y a plusieurs institutions éducatives, telles que :


En éducation élémentaire l'État de San Luis Potosí s'occupe des jeunes parmi les 12 et 15 ans qui font leurs études secondaires avec trois options différentes:


La télésecondaire est caractérisé principalement par l'enseignement à distance pour les jeunes qui habitent dans les communes rurales de l'État. Ultérieurement les jeunes diplômés de l'éducation secondaire sont admis à l'éducation moyen superieur pour laquelle l'État compte avec Colegio de Bachilleres, CEBETIS, CONALEP, Écoles Préparatoires Générales Publiques et Privées partout.

La ville compte une usine de fabrication de pneumatiques du groupe allemand Continental AG.




</doc>
<doc id="16999" url="https://fr.wikipedia.org/wiki?curid=16999" title="Matrice identité">
Matrice identité

En algèbre linéaire, la matrice identité ou matrice unité est une matrice carrée avec des 1 sur la diagonale et des 0 partout ailleurs. Elle peut s'écrire
Puisque les matrices peuvent être multipliées à la seule condition que leurs types soient compatibles, il y a des matrices unité de tout ordre. "I" est la matrice unité d'ordre "n" et est donc définie comme une matrice diagonale avec 1 sur chaque entrée de sa diagonale principale. Ainsi :

formula_2

Concernant la multiplication des matrices, les matrices unités vérifient que pour tous "p", "n" entiers naturels non nuls et pour toute matrice "A" à "n" lignes et "p" colonnes,
ce qui montre que la multiplication par une matrice unité n'a aucun effet sur une matrice donnée. On peut le démontrer par calcul direct ou en remarquant que l'application identité (qu'elle représente dans n'importe quelle base) n'a aucun effet par composition avec une application linéaire donnée.

En particulier, "I" est l'élément neutre pour la multiplication des matrices carrées d'ordre "n".

Il est possible aussi de noter les coefficients de la matrice unité d'ordre "n" avec le symbole de Kronecker ; le coefficient de la "i"-ème ligne et "j"-ème colonne s'écrit :
et donc la matrice unité "I" est égale à 

Si l'ordre n'est pas précisé, ou qu'il est implicitement déterminé par le contexte, nous pouvons la noter simplement "I".



</doc>
<doc id="17000" url="https://fr.wikipedia.org/wiki?curid=17000" title="Bharata natyam">
Bharata natyam

Le bharata natyam est une forme de danse classique indienne originaire du sud de l'Inde. 
C'est l'une des plus anciennes danses indiennes. Mélange de danse classique et d'art martial à la base, elle était liée aux pratiques religieuses dès son origine. Avec le temps, elle fut interdite sous la domination anglaise, mais autorisée dans les comptoirs français (Sud du pays). 

Elle a été sauvée, au début du , d'un oubli presque total. Rukmini Devi Arundale (1904-1986), qui lança en 1936 la Fondation Kalakshetra près de Chennai, a ainsi joué un rôle majeur dans la sauvegarde de cet art millénaire. 

Il existe néanmoins différents styles de Bharata Natyam : le Kalakshetra qui a sa fondation à Madras et le style Pandanallur fondé par les quatre frères Pillai qui s'est répandu notamment à la cour du palais de Mysore dont les plus grands interprètes sont Dr. Krishna Rao et Dr. Vasundhara Doraswamy. 

Le bharata natyam eAllaripust une danse de soliste dont l'apprentissage est très difficile et très long. Souvent enseignée aujourd'hui aux jeunes filles, elle est restée ouverte aux garçons.

Une origine possible du nom Bharata pourrait avoir un rapport avec "Bharat", le nom indien de l'Inde (en réalité bharatanatyam signifie « danse musique théâtre » mais ce mot réunissant les trois n'a pas d'équivalent en français). 

Le mot Bharata pourrait aussi être lié à un ancien théâtrologue indien, Bharata Muni. 

Enfin, le mot bharatha (bha-ra-tha) est composé de trois syllabes qui pourraient faire référence respectivement à trois mots tamouls : "bavam "(l'expression du visage), "ragam "(la musique et le rythme) et "thalam "(rythme imprimé par la main ou par le karuvi).

Le mot "natyam "est le mot tamoul pour danse. 

Un spectacle typique comprend :

Une prière traditionnelle d'ouverture au dieu Ganesha, qui écarte les obstacles. Elle comprend une courte partie d"'abinaya" (expressions du visage). 

Une présentation du tala (rythme), suite de syllabes chantées par la danseuse. Cette danse est entièrement dédiée au dieu Nataraja. Entièrement technique, elle représente "l'ouverture" : les postures et les mouvements de plus en plus complexes symbolisent "l'épanouissement" d'une fleur et de l'art.

Allaripu est constitué de pas de base dont le nombre total s'élève à 218, mais pour faire du bharathanatyam il y a une chose importante : l'Aramandi ou Ardha Mandalam (अर्ध मंडलम्).
C'est une danse technique et abstraite où le rythme est scandé par le tambour. La danseuse montre ici sa dextérité dans le travail des pieds et la grâce des mouvements de son corps. Les pas (ou Jatis), composés d"'adavus" (enchaînements de mouvements) sont chorégraphiés en harmonie avec les notes (ou Svara) sur une mélodie (appelée "raga").

La danse est ici accompagnée par un poème ou une chanson sur un thème dévotionnel ou amoureux. Cette danse parle souvent des dieux, racontant une histoire ou un récit épique. 
Dans le déroulement d'un récital, c'est la première danse narrative, développant l"'abinayam qui signifie l'expression du visage ou du corps."

La pièce centrale du spectacle. C'est aussi la partie la plus longue qui montre les mouvements les plus complexes et les plus difficiles. Les positions des mains et du corps racontent une histoire, habituellement d'amour et de désir. Elle varie entre sa partie technique et sa partie d"'abinaya" et dure de 20 à 30 minutes.

Probablement la partie la plus lyrique où la danseuse exprime certaines formes d'amour : dévotion à l'être suprême, amour maternel, amour des amants séparés puis réunis. Tout comme le shapdam ou le jaavali, c'est une danse d'abhbinaya. 

Cette dernière partie est une danse abstraite où la virtuosité de la musique trouve son parallèle dans le travail des pieds et les poses captivantes de la danseuse. C'est la danse la plus technique qui clôture le spectacle.

En sanscrit "thillana" signifie « explosion de joie ».

Le spectacle se termine par la récitation de quelques versets religieux en forme de bénédiction.

Elle est dans le style carnatique du sud de l'Inde, considérée par certains comme une forme plus pure que celle de la musique du nord de l'Inde.

Les instruments utilisés dans l'ensemble "cinna mêlyam" (« petit ensemble ») accompagnant le bharata natyam sont le mridangam (tambour), le nâgasvaram (un hautbois), la flûte venu, le violon et la vînâ (un instrument à cordes, luth indien).

Medha Hari, T.M.Sridevi, Archana Raja, T.Sangeeta, Lakshmi Priya, A.Vandana, Raghunath Manet ont popularisé la danse à travers le monde. Parmi les grands professeurs de bharata-natyam on peut citer : Minakshisundaram Pillai, Chokkalingam Pillai, Ram Goppal.

André Sauvé, humoriste québécois, fait connaître cette danse aux Québécois et parle de son expérience acquise en Inde à la pratique et l'enseignement de cette danse.

Raghunath Manet : « fils spirituel » de Ram Gopal. Il est apprécié pour avoir innové et introduit une certaine notion de chorégraphie dans le bharata-nâtyam.

Manjula de Maricourt : danseuse et chorégraphe de bharata natyam résidant à Paris. Disciple du Vasundhara Doraswamy de Mysore.

Caroline Fix est danseuse de bharata natyam à Strasbourg.


</doc>

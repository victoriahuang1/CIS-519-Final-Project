<doc id="4709" url="https://fr.wikipedia.org/wiki?curid=4709" title="Jean-Paul II">
Jean-Paul II

Karol Józef Wojtyła ( ) (Wadowice, près de Cracovie, en Pologne, – Vatican, ) est un prêtre polonais, évêque puis archevêque de Cracovie, cardinal, élu pape catholique le sous le nom de (en latin ', en italien ', en polonais '). Il est appelé saint par les catholiques depuis sa canonisation en .

Étudiant polonais en philologie, il joue dans un groupe de théâtre antinazi et entre au séminaire clandestin en 1942. Ordonné prêtre en 1946, après des études à Rome et en France, il est prêtre en Pologne communiste en 1948 auprès de la jeunesse. Après sa thèse sur l'amour, particulièrement conjugal, le cardinal Sapieha le nomme à l'université. Il devient, en 1958, le plus jeune évêque polonais. Il s'oppose au matérialisme, notamment en demandant une église à Nowa Huta.

Pendant , sa maîtrise des langues et de la théologie en font le porte-parole de l'épiscopat polonais, ce qui le fait remarquer par le futur . Archevêque, puis cardinal en 1968 (le plus jeune), il défend les ouvriers face au régime communiste, défendant les droits de l'homme et il s'intègre à la curie où, à la demande de , il prêche les exercices spirituels de 1976. Il reçoit des voix lors du conclave d'août 1978. À l'issue du conclave d'octobre 1978, qui fait suite à la mort brutale de , il est élu sur proposition du cardinal König. C’est le premier pape non italien depuis le pape hollandais en 1522 et le premier pape polonais de l’histoire du catholicisme.

En tant que pape, il s'oppose à l'idéologie communiste et par son action, notamment en Pologne, favorise la chute du bloc de l'Est. Sa volonté de défense de la dignité humaine le conduit à promouvoir les droits de l’homme. Il améliore sensiblement les relations du catholicisme avec les juifs, les orthodoxes, les anglicans et les musulmans. Il est à l’origine de la première réunion internationale inter-religieuse d’Assise en 1986, réunissant plus de de religion.

Son pontificat (, et ) est à ce jour le troisième plus long de l’histoire catholique après ceux de saint Pierre et . Il a parcouru plus de pendant son pontificat, plus de cinq cents millions de personnes ayant pu le voir durant cette période, et institué de grands rassemblements, comme les Journées mondiales de la jeunesse. Il a béatifié et canonisé , soit plus que pendant les cinq siècles précédents.

Karol Józef Wojtyła naît à Wadowice, petite ville de Petite-Pologne, le où réside une communauté juive importante qu'il côtoie quotidiennement. Son père, Karol Wojtyła (né en 1879), est militaire de carrière. Sous-officier dans l'armée austro-hongroise, il devient, après l'indépendance de la Pologne en 1918, officier de l'armée polonaise. Il prend sa retraite en 1927 avec le grade de capitaine.

Il épouse en 1906 Emilia Kaczorowska, de cinq ans sa cadette. Le couple a trois enfants : Edmund Antoni (né en 1906), Olga Maria (morte dès sa naissance, en 1914), et enfin Karol Józef (prénoms de son père et de l'ex-empereur ) en 1920. Très tôt, le petit Karol perd sa mère, atteinte d'une infection rénale (1929), et son frère aîné, devenu médecin, emporté par la scarlatine (1932).
Adolescent, Karol Wojtyła est passionné de littérature et de théâtre. Il participe à des représentations théâtrales données par son lycée. Il se lie d'amitié avec deux actrices de sa troupe, et Ginka Beer, joue dans de nombreuses pièces et obtient souvent les rôles principaux, remplaçant même au pied levé un acteur qui ne pouvait être présent. Il rencontre Mieczysław Kotlarczyk, professeur d'histoire au lycée des filles de Wadowice et passionné de théâtre qui, à partir de 1936, le forme à sa propre technique théâtrale, essentiellement fondée sur la force de la parole et du texte. Ils échangent sur la place de la langue dans la culture et l'identité polonaise et Karol lui écrit même après son départ de Wadowice. Karol Wojtyla a alors la volonté de devenir acteur, et souhaite se consacrer au théâtre.

À quinze ans, il devient président d'une association de jeunes qui se consacre à la Vierge Marie. Le 6 mai 1938, Karol Wojtyła reçoit le sacrement de confirmation. En août 1938, il quitte Wadowice, accompagné par son père, pour Cracovie où il suit des études de lettres à l’université Jagellonne. Il approfondit sa connaissance de l'étymologie, de la phonétique polonaise, du théâtre et de la poésie lyrique et se spécialise en philologie polonaise.

La défaite polonaise de 1939 entraîne le démembrement et l'occupation du pays par l'Allemagne nazie et l'URSS. Parmi d'autres mesures, l'occupant allemand impose la fermeture de l'université, et l'interdiction de fêter les saints polonais. Karol Wojtyła rencontre alors Jan Tyranowski, tailleur féru de spiritualité, homme de prière engagé dans sa paroisse. Une fois pape, dit de celui qui était devenu un proche qu'il était Celui-ci lui propose de participer au Rosaire vivant, organisation catholique clandestine. Jan Tyranowski pousse les membres du Rosaire vivant à prier, à se former, à vivre en présence de Dieu et à faire que . Tyranowski conseille à Karol Wojtyła la lecture des écrits de saints de l'Ordre du Carmel, comme Jean de La Croix, Thérèse d'Avila et Thérèse de Lisieux.

Karol Wojtyła continue à être acteur dans des pièces de théâtre. Il écrit aussi trois pièces, David, Job et Jérémie. Dans ces pièces on peut voir des parallèles entre le destin de la Pologne et d'Israël. Le théâtre est conçu par Karol Wojtyła comme un moyen de résistance et de défense de la patrie polonaise contre l'occupant nazi. Karol Wojtyła donne des représentations clandestines avec des amis : c'est le théâtre surnommé Studio 39.

Pendant l'automne 1940 dans la carrière de Zabrziwek, il découvre la réalité du travail manuel, puis, en octobre 1940, il se fait embaucher en tant qu'ouvrier dans l'usine chimique Solvay, ce qui lui permet d'échapper au service obligatoire allemand. Cette expérience marque durablement sa vie : .

Le 16 février 1941 survient le décès de son père, dernier membre vivant de sa famille.

En juin 1941, l'Allemagne nazie déclare la guerre à son alliée l'URSS et toute la Pologne passe sous le joug nazi.

En juillet 1941, son ancien professeur de théâtre Mieczysław Kotlarczyk rejoint Cracovie avec son épouse. Ils sont hébergés dans l'appartement de Karol Wojtyła. Un mois plus tard, avec un groupe d'acteurs incluant Karol Wojtyła, Kotlarczyk fonde le . Ce style théâtral, d'une grande sobriété de moyens, met en exergue le texte à travers un art déclamatoire très travaillé. Pour Kotlarczyk, la tension dramaturgique provient de la exprimée et reçue, plus que d'une mise en scène spectaculaire. Ce travail sur la puissance, en soi, de la parole, influence profondément Karol Wojtyła dans son apostolat de prêtre, puis d'évêque et de pape.

L'éradication de la culture polonaise est un des moyens utilisés par les nazis pour supprimer toute résistance à long terme dans le pays. Le théâtre rhapsodique fait dès lors partie d'un vaste mouvement de résistance culturelle clandestine, baptisé Unia. L'Unia a aussi une branche militaire. Mais Karol Wojtyła refuse d'entrer dans la résistance armée, préférant des moyens plus pacifiques, comme le combat culturel et la prière. La troupe du se produit dans la clandestinité, les acteurs risquant le peloton d'exécution s'ils se font prendre.

Au cours de l'automne 1942, après un long temps de réflexion, il décide de devenir prêtre, et entre au séminaire clandestin de Cracovie.

Karol Wojtyła est accepté au séminaire clandestin que Sapieha, archevêque de Cracovie, a organisé malgré l’interdiction allemande de former de nouveaux prêtres, en octobre 1942. Chaque étudiant est suivi par un professeur ; les cours ont lieu dans des églises ou chez des particuliers. Karol travaille comme ouvrier la journée et étudie le soir. Il lit alors le "Traité de la dévotion à la Très Sainte Vierge Marie", de saint Louis-Marie Grignion de Montfort. La lecture de Louis-Marie Grignon de Montfort a eu un grand impact dans sa vie, (sa devise en tant qu'évêque puis pape , est issue de la lecture du "Traité de la dévotion à la très Sainte Vierge Marie"). Ses armoiries comportent un écu d'azur à la croix d'or accompagné dans le canton en pointe senestre de la lettre M, en hommage à la Vierge Marie. Il s'initie aussi à la philosophie, et notamment à la métaphysique. Celle-ci, dans un premier temps, le déroute. Mais au bout de deux mois de travail intensif, il y trouve les raisons profondes de son existence et la confirmation de ses intuitions sensibles. Il restera toute sa vie passionné de philosophie.

Le 29 février 1944, il frôle la mort. Il est renversé par une voiture et se retrouve pendant quinze jours à l'hôpital, victime d'un traumatisme crânien.

Le 6 août 1944, Hitler décide de réprimer l'insurrection de Varsovie. Karol Wojtyła échappe à une rafle qui a lieu dans son immeuble, restant silencieusement en prière dans son appartement situé en sous-sol. Menacé par la répression, il trouve refuge au palais épiscopal où Sapieha décide de cacher les séminaristes. Il ne sort que très rarement du palais épiscopal et avec de faux papiers. Il ne retrouve sa liberté de mouvement que le 17 janvier 1945, à la suite de la libération de Cracovie par l'Armée rouge. L'armée soviétique salue l'attitude de l'archevêque face aux nazis.

Karol Wojtyła étudie particulièrement la théologie de Jean de La Croix, de Thérèse d'Ávila et de Thérèse de Lisieux. Il pense d'ailleurs un temps à devenir carme, mais y renonce. En 1946, Sapieha, qui vient d'être nommé cardinal, décide de l'envoyer compléter sa théologie à Rome. Il avance la date de son ordination pour faciliter son départ. Karol Wojtyła est ordonné prêtre lors de la Toussaint, le . Il a .

Karol Wojtyła poursuit ensuite sa formation à l’Angelicum de Rome, université alors dirigée par les dominicains, et où les cours sont dispensés en latin. Il y reste deux ans, pour préparer sa thèse de doctorat en théologie sur . Il loge dans le collège belge, où il apprend le français. Pour les besoins de sa thèse, il apprend aussi l'espagnol.

Le cardinal Sapieha lui demande de visiter l'Europe pendant ses vacances afin d'y étudier les méthodes pastorales. Il voyage alors en France et en Belgique. Pendant ce séjour, il découvre la réalité du début de la déchristianisation de la France mais aussi les nouvelles méthodes pastorales. Il rencontre le théologien Henri de Lubac et observe l’expérience des prêtres-ouvriers. En Belgique, il rencontre l’abbé Joseph Cardijn, fondateur de la Jeunesse ouvrière chrétienne. À son retour en Pologne, il publie dans la revue catholique de Cracovie son impression positive sur les nouvelles formes d'évangélisation en France, pays d'une , mais pays de mission ayant de nombreux incroyants. Il voit alors la nécessité de s'adapter aux situations nouvelles liées à la disparition d'une foi plus traditionnelle et observe avec intérêt les nouvelles formes d'évangélisation, qui : une nouvelle évangélisation.

En juin 1948, il est envoyé à Niegowić, un petit village de la campagne galicienne à cinquante kilomètres de Cracovie. Il y découvre le développement du stalinisme en Pologne. Il lit Lénine et Karl Marx, afin de mieux comprendre la logique communiste. Il défend cette conception : . Face aux pressions faites par le régime communiste, Karol Wojtyła conseille de ne jamais résister, affirmant que .

Le cardinal Sapieha le nomme en mars 1949 à la paroisse universitaire Saint-Florian de Cracovie. Pendant cette période il découvre . Il encadre alors un groupe de jeunes, à qui il donne des conférences.

Il apprend avec eux, fait du ski avec eux et organise une nouvelle forme d'évangélisation. Il organise des excursions, composées de temps de réflexion, de prière et de sport, ceci deux fois par an pendant quinze jours. Il célèbre la messe sur un canoë, chose assez rare avant le concile, et s'habille en civil, afin de ne pas se faire repérer par le régime communiste. Au cours de ces excursions, il écoute et discute beaucoup avec des jeunes, souvent fiancés, avec qui il parle des différents aspects de la vie conjugale. Il innove en discutant ouvertement de la sexualité. Il invite hommes et femmes . Il développe une réflexion profonde sur la vocation du mariage, qui restera toute sa vie l'une des grandes thématiques de son enseignement.

Il est nommé à l'université par le cardinal Sapieha contre sa volonté. Il étudie alors pour rédiger une thèse de philosophie. Il se spécialise en éthique et précisément sur la question de l'amour en général et de l'amour conjugal. Il étudie la philosophie de saint Thomas d'Aquin et les phénoménologues, dont Edith Stein, et sa thèse porte sur le phénoménologue Max Scheler. Il apprend l'allemand, afin de mieux comprendre Scheler. Il obtient son doctorat de philosophie en 1953. Il continue cependant ses excursions avec les jeunes pendant l'été.

En 1953, il occupe la chaire de théologie morale et éthique sociale de la Faculté de théologie de l'université Jagellonne. Il écrit des poèmes sous le pseudonyme d'Andrzej Jawień. Le régime soviétique accentue alors sa répression, développant un culte de la personnalité autour de Staline. Des personnalités catholiques comme le cardinal Stefan Wyszyński sont emprisonnées en septembre 1953. Le prêtre responsable du Rosaire Vivant est condamné à mort. L'enseignement catholique est interdit dans les écoles, et la faculté de théologie de l'université Jagellonne, où enseigne Karol Wojtyła, est fermée en octobre 1954. Après la mort de Staline, les relations deviennent plus libres. Des manifestations en faveur de la liberté religieuse ont lieu et le cardinal Wyszyński est libéré en 1956. En 1954, Karol Wojtyła est nommé professeur d'éthique à l’université catholique de Lublin. Il fonde dans cette ville un Institut de morale dont il conserve la direction jusqu’en 1978.

Karol Wojtyła participe alors secrètement autour du doyen et des professeurs de philosophie à des réunions afin de discuter de la situation de l'Église et de la nation. Ensemble, ils développent des moyens subtils afin de saper le communisme de l'intérieur, spirituellement et philosophiquement. Karol Wojtyła critique le communisme, considérant que l'éthique marxiste ne permet pas d'appréhender la réalité de l'homme en tant que tel. Ainsi il estime que les marxistes . Karol Wojtyła estime aussi que l'approche chrétienne de la vie et de la société est extrêmement réaliste, alors que l'approche marxiste finit par . Face à cette opposition, il ne cherche jamais à développer une confrontation armée ou violente avec les communistes. Il cherche ainsi à fuir les problèmes politiques et les conflits, afin de ne pas gaspiller de temps, et concentre son activité au développement de la connaissance, afin de se consacrer à un travail positif. Ainsi on ne trouve pas de réaction officielle du futur pape lors du soulèvement de Poznań en 1956.

Le , le pape le nomme évêque auxiliaire de Cracovie. À , Karol Wojtyła est le plus jeune évêque de la République populaire de Pologne. Cette nomination est validée par le régime communiste, car Karol Wojtyła est considéré comme une personne qui ne s'intéresse pas aux débats politiques, contrairement au cardinal Stefan Wyszyński. Le régime communiste voit dans le nouvel évêque un moyen de contrer et de diviser l'épiscopat polonais.

C’est à cette époque qu’il choisit sa devise (), inspirée
de la spiritualité de Louis-Marie Grignion de Montfort et illustration de sa dévotion à la Vierge Marie.

En tant qu'évêque auxiliaire il est responsable de la pastorale des étudiants. Il continue alors d'enseigner la morale à la faculté de théologie. Il enseigne principalement saint Thomas d'Aquin, Scheler, Husserl, Heidegger, Ingarden. Il tente de concilier dans sa réflexion, mais aussi dans les articles qu'il publie, la philosophie de saint Thomas avec la phénoménologie. Il considère que la phénoménologie propose des outils mais qu'il lui manque une vision générale du monde propre au thomisme.

Il continue ses activités littéraires, donnant même en 1960 une pièce de théâtre, "La Boutique de l’orfèvre", dont le sous-titre est : , puis en 1964, une dernière pièce, "Rayonnement de la paternité", sous-titrée : . Il collabore aux revues "Znak" et "Tygodnik Powszechny", signant ses poèmes du pseudonyme .

En 1962, l'administrateur apostolique de Cracovie, Eugeniusz Baziak, meurt. Karol Wojtyła est alors nommé pour le remplacer le 13 janvier 1964, devenant ainsi le plus jeune administrateur de diocèse en Pologne.

Pendant plus de vingt ans, Karol Wojtyła défend les paroissiens de la ville nouvelle de Nowa Huta, cité communiste modèle, privée initialement de lieu de culte. Il soutient la construction d'une église en célébrant des messes de Noël en plein air. lui offre une pierre de l'ancienne basilique vaticane, qui devient la première pierre de l', inaugurée en 1977.

Peu de temps après sa nomination comme évêque, le nouveau pape décide d'ouvrir le concile œcuménique du Vatican. L'évêque Karol Wojtyla est alors invité à participer au concile. La phase préparatoire se déroule du au . Dans la réponse au questionnaire pour le Concile , Karol Wojtyła demande que le concile se prononce clairement sur . Il souhaite que soit renforcé le rôle des laïcs dans l'Église, mais aussi le dialogue œcuménique et le célibat des prêtres qu'il défend. Même s'il n'a jamais joué un rôle fondamental au cours du Concile, sa position semble s'être progressivement renforcée au fil du concile au sein de la délégation des évêques polonais.

La première session du concile se déroule du au . Au cours de ce concile, Karol Wojtyła, parlant le français, l'anglais, l'allemand, le polonais, le russe, l'espagnol, l'italien et le latin, devient progressivement le porte-parole de la délégation polonaise. Cette délégation étant la plus importante du monde communiste, elle jouit d'une certaine autorité sur les questions concernant la vie de l'Église au sein du bloc de l'Est. Au fil des débats, Karol Wojtyła se lie d'amitié avec des évêques africains, qu'il sent animés d'une foi jeune, vivante, mais aussi avec les évêques allemands. Il croise des théologiens tels que Hans Küng et Joseph Ratzinger. La nomination de Karol Wojtyła comme archevêque en 1964, lui permet d'avoir une plus grande stature au sein de la délégation.

Il participe de manière active au du Concile , contribuant principalement au développement de l'exhortation sur l'Église dans le monde de ce temps. Lors du concile , deux tendances s'affrontent sur la conception de l'athéisme, souvent liée à la représentation existante du marxisme. Karol Wojtyła ne prend jamais ouvertement position pour l'une d'entre elles, mais défend sa conception face à l'athéisme, lors d'une tribune le : . Il invite l'Église à employer la méthode heuristique, exactement . Karol Wojtyła demande alors de considérer l'athéisme, non dans sa composante sociologique ou politique, mais avant tout dans son état intérieur de la personne humaine. Ainsi lors de son intervention du , il déclare : .

Le , reçoit pour la première fois Karol Wojtyła lors d'une audience privée. Le pape avait suivi ses interventions lors du concile, et il lui apparaissait comme la figure la plus marquante parmi la délégation polonaise, celle d'un évêque attaché à la tradition mais recherchant résolument le renouveau de l'Église, défendant l'autorité de l'Église sans étroitesse d'esprit, tout en étant doté d'une volonté de mettre la personne humaine et son salut au cœur des préoccupations.

À la fin du concile, les évêques polonais envoient une lettre aux évêques allemands, appelant à la réconciliation des deux nations. La dernière phrase , est vivement critiquée par le régime politique polonais, qui stigmatise l'attitude des évêques et leur manque de patriotisme. L'objectif était de favoriser la réconciliation entre les deux nations et d'éviter les revendications de territoire entre celles-ci, tout en n'oubliant pas la réalité des tensions historiques entre les deux pays, liées aux guerres et aux camps de concentration.

 le nomme archevêque de Cracovie le au côté du Cardinal Wyszyński, primat de Pologne, et figure de proue de l’épiscopat polonais dans la résistance au communisme. Il entre en fonction le . Cette nomination continue à être soutenue par le régime communiste, qui considère toujours Karol Wojtyła, du fait de son absence d'implication dans les débats politiques, comme un allié face au cardinal Wyszynski. Cette nomination intervient alors même que le cardinal Wyszynski voulait promouvoir d'autres personnes à ce poste. Ce titre posa des problèmes à Karol Wojtyła qui craignait que le pouvoir communiste utilise et développe une concurrence entre les deux archevêques de Pologne. Karol Wojtyla choisit alors de soutenir inconditionnellement le Cardinal Wyszyński. Il est secrètement convoqué par le régime communiste. Il décide en juillet 1965, sans l'en avertir, de reprendre et de défendre les conceptions du cardinal Wyszynski sans montrer la moindre divergence avec lui. Ainsi Karol Wojtyła refuse de participer au premier Synode des évêques, qui a lieu à Rome, car le cardinal Wyszynski n'est pas autorisé par le régime à y participer. Karol Wojtyła est alors mis sous écoute et espionné par le pouvoir en place ; il est parfois suivi lors de ses déplacements.

En 1966, l'archevêque organise la célébration du millénaire de la Pologne, lié à la commémoration du baptême de , le 4 avril 966. Il préside plus de cinquante messes d'anniversaire, dont une messe pontificale au nom du pape , qui n'est pas autorisé à entrer en Pologne, au sanctuaire Jasna Góra de Częstochowa, haut lieu du catholicisme polonais. L'objectif de la célébration du millénaire de la Pologne est aussi de mettre en avant l'héritage profondément chrétien du pays alors même que le gouvernement communiste promeut l'athéisme.

En 1962, il publie "Amour et responsabilité" dans lequel il développe une conception philosophique et chrétienne de l'amour et de la sexualité.

 le nomme cardinal de San Cesareo in Palatio, titre cardinalice de l'église de San Cesareo de Appia à Rome, dédiée au Saint Césaire de Terracina, le . Il est alors, à quarante-sept ans, le plus jeune de tous les cardinaux vivants. À la suite de cette nomination, il passe deux mois par an au Vatican. Il y devient membre de quatre congrégations : celle pour le clergé, pour l'éducation catholique, pour le culte divin, et pour les Églises orientales. le nomme aussi consulteur du Conseil des laïcs.

Au printemps 1968, une révolte des étudiants polonais éclate face à la censure du régime communiste. Celui-ci accuse les Juifs d’être responsables de la révolte. Karol Wojtyła prend alors publiquement la défense des étudiants et invite, à une conférence organisée à l’archidiocèse de Cracovie, le philosophe juif Roman Ingarden, montrant ainsi son soutien à la communauté juive. L'année suivante il visite officiellement une synagogue, affichant une nouvelle fois sa solidarité envers la communauté juive.

Au cours de ces années, Karol Wojtyła organise l'aide secrète à l'Église de Tchécoslovaquie, en grande partie détruite par le régime communiste. Il ordonne alors secrètement des prêtres à Cracovie. Après la mort de l'évêque tchèque Štěpán Trochta en 1974, le pouvoir interdit à Karol Wojtyła de venir célébrer les obsèques. Néanmoins, il salue publiquement la figure héroïque du défunt.

Les ouvriers de Pologne se révoltent en 1970 face à l’augmentation des prix. La répression du régime entraîne la mort d'une quarantaine de personnes. Le cardinal Wojtyła, tout en se défendant de vouloir agir politiquement, prend la défense des ouvriers. Il tente d'éviter le durcissement des conflits.

Une nouvelle révolte éclate le . Des ouvriers manifestent dans la rue. Karol Wojtyła prend la défense des droits de l’homme, affirmant, lors de l’homélie de la veille du jour de l’an, qu’il défendait . Il critique plus tard ouvertement la censure et les obstacles à la pratique du catholicisme. Cette défense des droits de l’homme se fait de plus en plus ouvertement. Il va jusqu’à affirmer en 1977 que . Cette défense des droits de l’homme va de pair pour le cardinal Wojtyła avec la défense et la reconnaissance de la nation. Il rejette la conception d’une nouvelle Pologne rattachée au mouvement communiste international et qui oublierait l’histoire et l’héritage du pays.

Parallèlement à ces prises de positions publiques, le cardinal encourage l’émergence du réseau d’intellectuels clandestins Odrodzenie (Renaissance), dialoguant fréquemment avec eux.

Le cardinal Wojtyła participe aussi à des congrès internationaux, invité par la philosophe américaine Anna-Teresa Tymieniecka, tant à Naples où il débat avec des phénoménologues sur la place de l'auto-détermination (1974), qu'à Harvard où il participe à une conférence en 1976. Ces voyages lui permettent de rencontrer l'épiscopat américain, et d'acquérir progressivement une stature internationale.

Dès la fin du concile, le pape nomme Karol Wojtyła membre de la commission sur les questions de la contraception et de la sexualité. Le cardinal polonais joue un rôle important dans le groupe qui conseille sur le thème de la contraception juste avant l'encyclique , publiée en 1968. Il reprend la conception de la sexualité qu'il avait développée au début de son ministère de prêtre. Il préside une commission d'étude dans son diocèse. Celle-ci est composée de laïcs et de membre du clergé. Il envoie directement au pape le fruit de ses réflexions. Lors de la publication d’"Humanæ Vitæ", Karol Wojtyła se dit très satisfait d'avoir . Un prêtre du diocèse de Cracovie affirme que près de soixante pour cent de l'encyclique provient du rapport de Wojtyła.

Une de ses initiatives originales, en tant qu’archevêque de Cracovie, est l'ouverture, en 1972, d’un synode pastoral visant à partager la collégialité de Vatican- avec les prêtres et fidèles de l’archidiocèse. Plus de d’études, composés de fidèles de toutes conditions, vont approfondir régulièrement les textes de Vatican-. Ce sont en tout plus de onze mille personnes qui étudient ainsi les enseignements du concile. Ce synode de Cracovie se poursuit jusqu’en 1979 et contribue à mettre en pratique les principes du concile dans l’archidiocèse.

Karol Wojtyła participe aux synodes des évêques de 1969 sur la collaboration des épiscopats nationaux avec le siège apostolique, puis à celui de 1971 sur le sacerdoce et la justice dans le monde. Il est, en 1974, le rapporteur du synode sur l'évangélisation dans le monde contemporain.

En 1969 paraît en Pologne une première version de ce qui est considéré comme l'œuvre philosophique majeure du futur , "Osoba i czyn" (« Personne et Acte »). Il y développe sa conception de l'amour et de l'homme. Après sa rencontre avec Anna-Teresa Tymieniecka en 1973, commence une amitié (selon le journaliste qui sous-entend une amitié amoureuse d'après les de la main de Karol Wojtyla retrouvées dans la Bibliothèque nationale de Varsovie) et une longue collaboration qui aboutira en 1979 à la publication en anglais de la version définitive de l'ouvrage, "The Acting Person".

Le développement de sa conception de l'homme donne une place primordiale à l'autodétermination de l'être humain, l'individu devant donner forme à sa vie et décider ce qu'il veut en faire. Cette conception centrée sur la personne constitue le fondement pour le cardinal Wojtyła du rôle des systèmes politiques, qui ont pour vocation d'aider les individus à se déterminer eux-mêmes. Cela le conduit à critiquer les dérives des systèmes politiques : .

Le 26 août 1978, à la mort de , Karol Wojtyła, cardinal, participe à l'élection du futur pape. Albino Luciani, patriarche de Venise, est alors élu, et prend le nom de , en hommage aux deux précédents papes qui ont ouvert et fermé le Concile , et . meurt trente-trois jours plus tard. Au cours de ce conclave, Karol Wojtyła aurait déjà reçu quelques voix de cardinaux.

D'après l'opinion qui s'imposa par la suite, le conclave aurait été divisé entre deux favoris : Giuseppe Siri, archevêque de Gênes, plutôt conservateur et Giovanni Benelli, archevêque de Florence proche de et grand électeur du conclave précédent. Mais aucun ne s'impose et Karol Wojtyła, qui était aussi pressenti, est élu au huitième tour de scrutin, le , pape de l’Église catholique. On sait d'autre part, que König, archevêque de Vienne, était très proche de lui, et paraît avoir été l'un de ses grands électeurs.

Enfin, les cardinaux allemands ont activement fait campagne pour l'archevêque de Cracovie ; parce qu'ils représentaient une église aux moyens financiers considérables, ils passaient beaucoup de temps en déplacements hors d'Europe pour mettre en œuvre une action caritative importante (hôpitaux, écoles, etc.) ; ils disposaient d'une forte notoriété auprès de prélats africains et sud-américains et donc, d'une influence importante ; moins de quarante ans après l'agression nazie sur la Pologne, ce soutien était particulièrement symbolique.

D’après George Weigel, plusieurs facteurs peuvent expliquer son élection. Cardinal depuis onze années, Karol Wojtyła était bien connu des autres électeurs. Ses interventions lors du concile et sa prédication pendant la retraite papale en 1976 avaient été remarquées. Il avait une longue expérience de la résistance culturelle au communisme qui pouvait contribuer à renouveler l’Ostpolitik du Saint-Siège. Mais avant tout, selon Weigel, il avait marqué les esprits dans sa mission d’évêque diocésain, montrant qu’une direction ferme pouvait être possible au milieu des tensions post-conciliaires. De même, pour Bernard Lecomte, le souhait général des cardinaux était .

La surprise n'en est pas moins très grande : il est le premier pape slave de l'histoire et le premier non italien depuis en 1522. La foule croit d'abord avoir affaire à un cardinal africain, et nombre de commentateurs sont pris de court lors de l'annonce, ignorant tout du nouveau pape, le service de presse du Vatican n'ayant lui-même pas prévu de fiche biographique. se démarque dans la succession des papes par sa nationalité, sa relative jeunesse et sa condition d’ancien athlète. Surtout, il vient d’un pays communiste, d’au-delà du rideau de fer. Dans sa première déclaration, ce détenteur de l'infaillibilité suggère avec humour à la foule de le corriger s'il fait des erreurs… en italien. Le pape est polyglotte.

Après avoir, semble-t-il, renoncé à prendre le même nom que le saint patron de la Pologne, Stanislas, sur demande du cardinal-primat de Pologne, il choisit , en continuité avec ses trois prédécesseurs immédiats. Il inaugure son pontificat le 22 du même mois.

Son pontificat est le troisième plus long () de l’histoire bi-millénaire de la papauté. Sur ses , seul (1846-1878) a régné plus longtemps que lui (, et ), mais saint Pierre, le premier des évêques de Rome, aurait régné encore plus longtemps ( ou dont 25 à Rome). Durant son règne, il aura connu trois présidents français, cinq présidents des États-Unis, et sept chefs d’État d’Union soviétique puis de Russie.

Les premiers jours de son pontificat sont marqués par des changements de forme : il prépare personnellement ses premiers discours, et va directement à la rencontre du public, montrant alors sa grande indépendance vis-à-vis du protocole et de la curie.

Les premiers discours de marquent son attachement au concile, à la collégialité dans l'Église, mais aussi au respect de la tradition, de la liturgie et sa volonté de poursuivre le dialogue œcuménique et la recherche de la paix et de la justice.

Le 22 octobre 1978, lors de la messe inaugurale de son pontificat, il prononce le discours qui marque le début de son pontificat, montrant sa détermination, appelant à un christianisme plus engagé et à l'ouverture des frontières, interpellant : 

Il visite alors Assise, et se proclame porte-parole de , représentant l'église sous les régimes communistes. Il défend très vite les droits de l'homme, considérant la liberté de pratiquer sa religion comme le fondement de toutes les autres libertés lors d'un discours pour le trentième anniversaire de la Déclaration des droits de l'homme.

Dès l'année suivante il visite la Pologne, l'Irlande, les États-Unis (il est le premier pape à se rendre à la Maison-Blanche) et la Turquie. Il débute au cours des audiences papales du mercredi une véritable catéchèse sur la destinée humaine, la sexualité ou la théologie du corps. En 1980 il se rend en Afrique, en France et au Brésil. Il défend l'appartenance à l'Église catholique de l'Église uniate, que Staline avait voulu dissoudre et annexer au patriarcat orthodoxe.

Le mercredi , jour de l'audience générale hebdomadaire qui se tient place Saint-Pierre à Rome, et devant une foule de , est victime d’un attentat. Mehmet Ali Ağca, un jeune turc de , déjà condamné dans son pays pour un assassinat commis deux ans plus tôt, fait feu sur le pape avec un pistolet automatique Browning de calibre , à une distance de moins de six mètres. Six semaines plus tôt avait eu lieu à Washington la tentative d'assassinat du président américain Ronald Reagan.

Atteint par trois balles, le pape doit être opéré en urgence, mais aucun organe vital n'est atteint. L'attentat ayant lieu le jour-anniversaire de la première apparition de la Vierge de Fátima, qu'il devait mentionner dans son discours, attribue sa survie à l’intervention de la Vierge de Fátima, et il pense que cet attentat est celui évoqué dans le message de Fátima.
Plusieurs thèses ont été formulées sur un possible commanditaire. Selon certaines sources, cet attentat pourrait être l’œuvre du GRU, les services de renseignements de l’armée soviétique ; . D'autres personnes, du fait de la nationalité de Mehmet Ali Ağca, pensent que des groupes islamistes radicaux pourraient être à l'origine de l'attentat, le jeune Turc étant contre la visite du pape en Turquie, voyant en lui . laisseraient entendre qu'il s'agirait d'une action menée par la mafia turque commanditée par la mafia italienne. Enfin, certains n'y ont vu que la volonté propre de Mehmet Ali Ağca, considérant qu'il souffrait de troubles psychiatriques.

À la suite de cet attentat, qui a manqué de peu de lui coûter la vie et qui lui laissera des séquelles, le pape circule parmi la foule dans une voiture blindée, surnommée la . En 1983, il se rend dans la cellule de Mehmet Ali Ağca pour lui accorder son pardon.

Lech Wałęsa a l'intuition d'afficher des posters de sur les grilles des chantiers navals de Gdańsk en août 1980 qui place en position d'arbitre de la crise et contribue ainsi à la signature de l'accord de Gdańsk du . Wałęsa obtient la permission de rencontrer le pape en 1981. Il affirme alors que en Pologne.

Par cette encyclique, il montre son soutien à la cause polonaise de Solidarność. Il pousse les évêques polonais à défendre les accords qui ont lieu en Pologne. Cette période marque un fort rapprochement entre l'administration Reagan et , qui partagent des informations confidentielles sur la Pologne. Ronald Reagan soutient aussi la position du pape sur les questions liées à l'avortement. Le , face à l'augmentation des protestations en Pologne, le général Wojciech Jaruzelski déclare la loi martiale. cherche alors à apaiser les revendications, craignant un bain de sang, et affirmant qu'il faut promouvoir la paix. Lors de sa visite en Pologne en 1983, il soutient les opposants au régime. Il appelle les Polonais à suivre leur conscience, à . Par la suite il défend la justice sociale, les droits fondamentaux, les salaires équitables et les syndicats interdits par la loi martiale. Au cours de cette visite, il reçoit le titre de docteur honoris causa de l'université jagellonne de Cracovie.

Dans le film "Testimony", portant sur la vie de , le cardinal Stanisław Dziwisz affirme que le souverain pontife a été blessé par un coup de poignard lors d'une visite au sanctuaire marial de Fatima au Portugal en 1982.

Le pape, qui venait remercier, dans ce sanctuaire, la Vierge Marie pour avoir échappé aux coups de feu tirés contre lui par Mehmet Ali Ağca, est attaqué par Juan María Fernández y Krohn, un prêtre intégriste espagnol opposé à la libéralisation de l'Église. Celui-ci se précipite sur le Pape avec un poignard à la main, mais il est rapidement maîtrisé. L'information n'est pas diffusée et le pape termine son voyage sans révéler ses blessures. , déclare Dziwisz dans le documentaire.

 fait un voyage en 1983 en Amérique centrale, au cours duquel il prend position contre la théologie de la libération. Il défend la lutte contre la pauvreté et l'exclusion qui touche ces populations, mais s'oppose aux révolutions armées. Face aux théologiens voulant concilier révolution et christianisme, il appelle à l'unité de l'Église et au dialogue, montrant une opposition à certains aspects de la théologie de la libération. S'il condamne la théologie de la libération et le communisme, il adopte une posture plus ambiguë envers les dictatures militaires implantées en Amérique latine. Au Nicaragua, il refuse de donner la main au père Ernesto Cardenal, agenouillé devant lui, pour son adhésion à la théologie de la libération et sa participation au gouvernement sandiniste en tant que ministre de la Culture. Pourtant, au cours de ce même voyage en Amérique centrale, il rencontre et salue le dictateur guatémaltèque Ríos Montt, ultérieurement condamné à 80 ans de prison pour génocide, et au major salvadorien Roberto d'Aubuisson, chef des paramilitaires responsables de l'assassinat de l’archevêque Óscar Romero

Lors de son séjour en Argentine alors sous dictature militaire, en juin 1982, après avoir été accueilli « avec une profonde affection » par le général Leopoldo Galtieri, il refuse de recevoir les organisations humanitaires et s’abstient de toute critique au sujet des atteintes aux droits de l'homme. En 1987, il est reçu par Augusto Pinochet au Chili. Le 3 avril, des opposants à la dictature se rassemblent afin de tenter d'obtenir le soutien du souverain pontife. La police charge la foule et des carabiniers ouvrent le feu ; une personne est tuée, six cents sont blessées et des dizaines arrêtées mais en dépit de ces violences n'interrompt pas sa messe donnée à proximité et n'y fait pas allusion.

Il rencontre Mère Teresa et lui demande à partir de 1986 d'être son porte-parole pour défendre la position de l'Église concernant la vie, et notamment son opposition à l'avortement. En 1986, il lance les premières Journée mondiale de la jeunesse. Ces journées sont nées de sa volonté de répondre aux préoccupations des jeunes et de les rencontrer. Stanisław Dziwisz affirme que ces journées sont issues des rassemblements qu'il a eus avec les jeunes, et particulièrement celui ayant eu lieu à Paris, au Parc des princes, en 1980. Ces rencontres réunissent des millions de personnes, et ont lieu tous les deux ou trois ans.

L'évènement le plus marquant de son pontificat est peut-être son initiative d'inviter les représentants de toutes les grandes religions à Assise, le , pour participer à une Journée mondiale de la prière. Pour la première fois dans l'histoire, toutes les religions sont représentées ensemble afin de prier pour la Paix. La démarche de n'était pas du syncrétisme : toutes les religions étaient ensemble pour prier, mais ne priaient pas d'une seule voix. Cette démarche inter-religieuse fut critiquée par Marcel Lefebvre qui provoqua un schisme deux ans plus tard. Au cours de cette journée le pape pria avec les autres chefs religieux, et fit acte de repentance, affirmant que les catholiques n'avaient pas toujours été des bâtisseurs de paix.

Au cours de cette journée pour la paix, il n'y eut aucun mort sur les champs de bataille.

En 1987 il visite le Chili et est accueilli par Augusto Pinochet. Cette visite est critiquée, certains y voyant un soutien au dictateur. Cependant le pape ne critique pas lors de cette visite le vicariat à la solidarité, organisé par l'Église chilienne, qui aide les opposants au régime. Au cours de cette visite, il demanda en privé à Augusto Pinochet de démissionner et de rendre le pouvoir à la société civile.

En 1988, il publie l'encyclique "Sollicitudo Rei Socialis". Dans cette encyclique, il défend une vision chrétienne du progrès social, tout en dénonçant les inégalités criantes entre le Nord et le Sud.

La chute du mur de Berlin en 1989 et la fin de l'URSS, l'année suivante, furent considérées comme étant liées à l'action de . Le succès de ses voyages, en Pologne notamment, avaient contribué à déstabiliser le régime. Mikhaïl Gorbatchev affirmera : .

Début 1991, il s'oppose à l'intervention alliée contre l'Irak de Saddam Hussein qui fait suite à l'invasion du Koweït l'année précédente.

Lors de sa visite en Pologne à Lubaczów les 2 et 3 juin 1991, il dénonce avec force la société de consommation. Il réaffirme également dans ses homélies son opposition claire à l'avortement et appelle les Polonais à suivre leur conscience et à ne pas confondre liberté et immoralisme. Il dénonce .

Il proclame l'année 1994 "année de la famille". Il fait de la lutte contre l'avortement l'une de ses priorités, luttant contre sa légalisation lors de la conférence des Nations unies au Caire. Il dénonce alors une , et invite les catholiques à défendre la vie humaine face aux manipulations génétiques, à l'avortement et à l'euthanasie.

Il organise le Jubilé de l'an 2000 qui marque le deux millième anniversaire de la naissance de Jésus. Au cours de cette année, il soutient officiellement la démarche d'annulation de la dette des pays d'Afrique, initiative lancée par Bob Geldof et Bono.

L'historien Philippe Levillain estime que trop malade, durant les cinq dernières années de son pontificat.

En 2005, il contracte une grippe qui se transforme en laryngotrachéite aiguë avec des crises de spasmes du larynx, ce qui l'oblige à être hospitalisé le . Le , il est de nouveau hospitalisé à la suite d'une crise d'étouffement, puis on pratique une trachéotomie. Il s'était entraîné à prononcer la bénédiction "Urbi et orbi" le jour de Pâques mais reste muet à sa fenêtre, sans arriver à dire un mot. Le , il est victime d'un choc septique, d'un collapsus cardio-vasculaire et d'une infection urinaire en même temps. refuse alors l'hospitalisation. Dans la journée du , il dit adieu à ses collaborateurs, un par un, puis écoute l'Évangile de Jean prononcé par une des religieuses qui l'avait servi pendant .

Il entre dans le coma en soirée puis s'éteint au Vatican le , veille du dimanche de la divine Miséricorde, à , heure locale, à l’âge de et après un pontificat de , le deuxième plus long de l’histoire de l’Église. D’après le certificat du décès publié le par le Vatican, sa mort est due à un choc septique et à une insuffisance cardiaque. Il est enterré au Vatican le . Le cardinal Joseph Ratzinger lui succède le sous le nom de .

Trois aéroports accueillent quelque d’États et une soixantaine d’avions civils pour l’arrivée de ces délégations qui comprennent jusqu’à une cinquantaine de membres ; sont notamment présents lors des funérailles George W. Bush, président des États-Unis, Jacques Chirac, président de la République française, le roi d'Espagne Juan Carlos et le roi des Belges . Parmi les dignitaires religieux qui se rendent à Rome, on trouve, entre autres, Rowan Williams, archevêque de Cantorbéry et président du Conseil mondial des évêques anglicans, et , patriarche orthodoxe de Constantinople.

Plus de de personnes viennent à Rome, du 2 au 8 avril 2005. Celles qui vont en la basilique vaticane, saluer la dépouille du pape, défilent au rythme de à l'heure, soit à la minute. L'attente va de , avec une queue maximale de cinq kilomètres.

Le jour des funérailles, se trouvent place Saint-Pierre et Via della Conciliazione, dans les sites urbains dotés d'écrans géants installés par la municipalité. La salle de presse du Saint-Siège et le Conseil pontifical pour les Communications sociales délivrent plus de (journalistes, photographes, reporters de radio-télévision) pour la couverture de l'événement. TV de diffusent la Messe de funérailles. On estime à deux milliards le nombre de personnes qui ont vu la cérémonie d'enterrement de à travers le monde.

La messe de funérailles est concélébrée par , en présence de et évêques, et prêtres.

De nombreux pays décrètent une ou plusieurs journées de deuil à la suite du décès de . Certains à majorité catholique comme le Brésil, l'Italie, les Philippines, la Pologne. D'autres où les chrétiens sont minoritaires, comme l'Inde, le Tchad, l'Albanie, etc. Dans d'autres pays, dont la France, la Suisse et la Turquie, les drapeaux sont mis en berne sur les bâtiments publics.

Il a plus que doublé le nombre des nonciatures (ambassades du Saint Siège) qui passent de 85 en 1978 (à son élection) à 174 à la fin du pontificat.

Au 16 octobre 2004, il a participé à plus de avec des personnalités politiques, comprenant les officielles : avec des chefs d'État et 246 avec des chefs de gouvernement, des affaires étrangères, accrédités près le Saint-Siège. Ces chiffres ne comprennent pas les diverses rencontres qui ont lieu en clôture de cérémonies liturgiques, tant au Vatican que de par le monde.

En février 1984, il fonde l’institut pour le Sahel et, en février 1992, la Fondation "Populorum Progressio" pour les pauvres d’Amérique latine. Il a également fondé l'Académie pontificale pour la vie et l'Académie pontificale des sciences sociales.

De plus, il a institué la journée du malade (célébrée chaque année le 11 février) et les Journées mondiales de la jeunesse (JMJ), la journée mondiale pour la Paix, la journée mondiale pour les migrants et les réfugiés, la journée mondiale pour les communications ainsi que six autres journées mondiales.

En 1989, il rencontre le Chef Raoni afin de discuter des enjeux liés à la préservation de la forêt amazonienne.

Il a été le premier pape à tenir des conférences de presse dans des avions et .

Il a fait construire deux immenses basiliques près de Cracovie : la basilique de Nowa Huta (en tant qu’évêque de Cracovie) et le Sanctuaire de la Miséricorde Divine à Kraków-Łagiewniki (à la consécration il a fait l'Acte de la Confiance du Monde à la Miséricorde Divine).

Il a été reçu onze fois .

L'organisation de l'Église a été profondément remaniée sous le pontificat de . Il a, au cours des , créé et cherché à universaliser la Curie. Dès 1988, la majorité des cardinaux, ceux qui élisent le pape, venait des pays non européens. Il a également convoqué plénières du collège des cardinaux.

Il a nommé plus de des encore vivants lors de son décès. Il intervient directement dans la nomination des évêques, ce qui fut critiqué comme une marque d'autoritarisme du pape. Il n'a pas fait évoluer la pratique des synodes des évêques, et convoqua 15 synodes: : générales ordinaires (sur la famille en 1980, la réconciliation en 1983, les laïcs en 1987, la formation des prêtres en 1990, la vie consacrée en 1994 et en 2001 sur le ministère épiscopal), générale extraordinaire (sur le concile en 1985), spéciales (sur l'Europe en 1991 et en 1999, l'Afrique en 1994, le Liban en 1995, l'Amérique en 1997, l'Asie et l'Océanie en 1998) et un synode particulier (pour les Pays-Bas en 1980). Il réaffirma l'autorité du pape sur les évêques et les églises locales afin de renforcer l'universalité de l'Église.

Il a consacré environ aux évêques venus à Rome.

Il a permis l’ordination d'hommes mariés dans certains cas très précis (par ex. pasteurs protestants mariés qui se convertissent au catholicisme). Il a œuvré à la promotion du diaconat.

Il a également voulu associer davantage les femmes au fonctionnement de l’Église . Il écrit une "lettre aux femmes" datée du . Il nomme le  Mary Ann Glendon (professeur de droit à Harvard, et ancienne représentante de la délégation pontificale à la conférence de Pékin sur la Femme en 1995) présidente de l’Académie pontificale des sciences sociales. Auparavant, il avait déjà nommé : sœur Sara Butler, M.S.B.T., professeur de théologie à l’université de Mundelein (Chicago), et madame Barbara Hallensleben, de l’université de Fribourg, en Suisse à la Commission théologique internationale.

Durant son pontificat, effectue , représentant en dehors du Vatican, en Italie, à Rome ainsi qu'à Castel Gandolfo. Il rend visite à 317 des de Rome. Il visite , la plupart d'entre elles accueillant un pape pour la première fois, et . La distance parcourue lors de ses voyages apostoliques est de soit le tour de la Terre ou presque trois fois la distance Terre-Lune.

Les trois pays les plus visités par sont : la Pologne, son pays natal (neuf fois) ; la France (huit fois, dont sept fois en métropole et une fois à La Réunion) ; et les États-Unis (sept fois). a un attachement particulier pour la France. Il rappelle, lors de son premier voyage en France en 1980, qu'elle est la « fille aînée de l'Église » et demande, à la fin de son homélie au Bourget : « France, Fille de l’Église et éducatrice des peuples, es-tu fidèle, pour le bien de l’homme, à l’alliance avec la sagesse éternelle ? ». Il effectue également deux voyages à Lourdes (1983 et 2004), un voyage « européen » à Strasbourg, Metz, et Nancy (1988), un voyage pour le anniversaire du baptême de Clovis à Reims (1996), et un voyage pour les Journées mondiales de la jeunesse à Paris (1997).

Durant son plus long voyage, le , qui a lieu en novembre-décembre 1986, , qui a déjà visité l'Inde du au 10 février de la même année, parcourt le Bangladesh, les Seychelles, Singapour, les îles Fidji, la Nouvelle-Zélande et l'Australie.

Alors que certains de ses voyages (comme aux États-Unis ou à Jérusalem) le mènent sur les traces de , beaucoup d’autres pays n’ont jamais été visités par un pape. Il devient le premier pape à se rendre au Royaume-Uni où il rencontre la reine , gouverneur suprême de l’Église d'Angleterre. Lui et l’archevêque anglican de Cantorbéry s’embrassent devant les médias dans la cathédrale de Cantorbéry.

Il est le premier pape à descendre dans un hôtel et non à la nonciature du pays visité (l'hôtel Irshad de Bakou en Azerbaïdjan, en mai 2000), à dire la messe dans un avion, à dire la messe pour la communauté catholique la plus septentrionale (à au nord du cercle polaire à Tromsø en Norvège, en 1989). Il reprend la pratique de de baiser la terre à son arrivée sur un sol étranger.

Il préside générales hebdomadaires en présence de plus de provenant du monde entier, et plus de privées. Plus de de personnes se rendent à Rome pour le voir.

Les raisons de ses nombreux voyages sont sa volonté de montrer le caractère universel de la mission du pape, qui doit parler au monde entier, et doit être un signe visible de l'universalité de l'Église. Il veut aussi permettre aux fidèles de voir le pape, en allant lui-même, , à leur rencontre, d'autant que beaucoup parmi eux n'ont pas les moyens de se déplacer à Rome.

Durant ses voyages, il montre une dévotion particulière envers la Vierge Marie, visitant de nombreux lieux lui étant consacrés, dont Lourdes en France par deux fois, Notre-Dame de Banneux en Belgique, Fátima au Portugal, ou encore Guadalupe au Mexique. Ces visites ont trois principales motivations : l'attachement personnel de envers la Vierge Marie, la volonté de renforcer et populariser les pèlerinages vers des sanctuaires mariaux, et le désir de rappeler la dévotion des catholiques pour la mère du Christ, dévotion qui n'est pas partagée, au même titre, par les protestants.

Ses visites ont la particularité de rassembler des foules gigantesques. Lors de grandes manifestations, comme les Journées mondiale de la jeunesse, le nombre du million de personnes présentes est souvent dépassé.

Le pontificat de a été marqué par un profond engagement social. La dignité de l'homme est l'aspect le plus marquant de sa doctrine au cours de son pontificat.

Le système soviétique anticlérical fut l'objet des critiques du pape dès le début de son pontificat, même si le communisme avait déjà été condamné par en 1937. La dignité de l'homme donne le droit, selon le pape, à des droits inaliénables. Ce constat le conduit à critiquer les dangers des idéologies et des totalitarismes qui vont à l'encontre de cette dignité. Cette opposition au communisme sera renforcée par sa conviction que le communisme nie, selon lui, la vérité tant de Dieu, que la nature humaine. Il affirme ainsi que . Au nom de la dignité de l'homme dans le travail, il défendit la création de syndicats libres, qui étaient interdits sous le régime communiste. Il favorisa en Pologne une résistance intransigeante contre le communisme. Son soutien aux dissidents de l’ex-bloc soviétique, en particulier au syndicat Solidarność et à Lech Wałęsa ainsi son élection comme pape venu de derrière le rideau de fer, ont joué un rôle important dans l’effondrement des régimes communistes en Europe de l’Est à la fin des années 1980. Il fut considéré comme l'un des acteurs principaux de la chute du communisme.

 s'est également opposé aux inégalités criantes dans le monde. Il rejette l'impérialisme et toutes formes de négation de l'indépendance des nations. Dans ses discours, il s'oppose à des idéologies et politiques telles que le féminisme, l'impérialisme, le relativisme, le matérialisme, le fascisme (y compris le nazisme), le racisme, l'ultra-libéralisme et le capitalisme. À plusieurs reprises, il a dénoncé l'oppression des plus pauvres.

L'attitude de à l'égard des courants proche du marxisme, et notamment la théologie de la libération, ainsi que sa dénonciation de certains régimes dictatoriaux, tant en Amérique, qu'en Asie, ont favorisé, selon certains, la transition démocratique en Amérique du Sud et en Asie.

À l’occasion de son voyage au Chili, Augusto Pinochet demanda au pape : répondit : . Au cours de cette même visite, le pape demanda à Augusto Pinochet, lors d'un entretien en privé avec lui, de démissionner et de rendre le pouvoir à la société civile.

Le pontificat de s’est caractérisé par une intensification des échanges avec les autres religions. Au cours de ses voyages, il a rencontré bon nombre de leurs dignitaires et a prié dans plusieurs de leurs lieux saints. Le pape a sensiblement amélioré les relations entre le catholicisme et les autres religions. À plusieurs reprises, il a invité les responsables de toutes les religions à une prière commune pour la paix à Assise : , en 1993 pendant la guerre des Balkans et le , quelques mois après les attentats du .

 a grandi dans un contexte de culture juive florissante, son intérêt pour elle datant de son enfance. Il écrit un grand nombre de textes et de discours sur le sujet des relations entre l’Église et les Juifs, rendant hommage aux victimes de la Shoah. Son premier voyage, qui est aussi le premier d’un pape en ce lieu, est à Auschwitz. Il est le premier pape à visiter une synagogue, à la Grande synagogue de Rome en avril 1986. Il déclare que les juifs sont .

En 1993, décide de reconnaître l'État d'Israël, établissant pour la première fois des liens diplomatiques officiels avec l'État hébreu, et ceci malgré l'opposition de membres de la Curie qui souhaitaient le règlement de la question palestinienne avant la reconnaissance des relations diplomatiques. Lors d'un colloque le , affirme qu'un .

En mars 2000, se rend au Mémorial de Yad Vashem, où il retrouve une rescapée qu'il avait secourue, et demande pardon à Dieu pour les actes antisémites commis par les chrétiens. Dans un billet glissé dans une fente du Mur des Lamentations, il demande à Dieu de pardonner pour les torts faits au peuple juif.

La rédaction par une partie des théologiens juifs du document Dabru Emet en 2000, qui affirme qu' et affirme la volonté de dialogue théologique avec les chrétiens, montre, pour certains, l'impact du pontificat de qui a permis de favoriser l'émergence de ce courant juif dans le développement du dialogue inter-religieux.

Des polémiques émaillèrent le pontificat de . Un carmel s'était établi à Auschwitz. Cette fondation fut très critiquée par une partie de la communauté juive. finit, après plusieurs années, par ordonner aux religieuses de déménager, afin de pacifier les relations. De même la canonisation d'Edith Stein, juive convertie au catholicisme, morte à Auschwitz fut décriée, et considérée par certains comme une de la Shoah par l'Église, alors que , lecteur d'Édith Stein, considérait celle-ci comme exemplaire et sainte.

 devint le deuxième pape à avoir visité la Turquie en se rendant dans ce pays en novembre 1979.

Le pape effectue une visite les 18-19 août 1985 à Casablanca au Maroc. Il parle devant . Au cours de cette rencontre le pape affirme . Plusieurs réactions négatives dans les pays arabes suivirent cette rencontre ; l'Iran et l'ayatollah Khomeini ne reconnurent plus le titre de commandeur des croyants au roi . Le pape a effectué une visite d’une journée à Tunis le 14 avril 1996. L'assassinat des moines de Tibhirine en mai 1996 ainsi que celui de l'évêque Pierre Claverie ont cependant rendu les relations entre les deux religions plus difficiles.

Il encourage la construction d'une mosquée à Rome, tout en demandant plus de réciprocité dans la liberté de culte des pays musulmans. Les attentats du , conduisent à condamner toute forme de violence au nom de Dieu, et affirme que ces attentats n'ont rien à voir avec le vrai Islam. Il invita alors à une journée de prière rassemblant toutes les religions et particulièrement les musulmans, voulant éviter de légitimer toute guerre des religions entre chrétiens et musulmans.

En mai 2001, est le premier pape à se rendre dans une mosquée. Désireux de se recueillir sur le lieu où se convertit saint Paul, il entre et prie auprès des reliques de saint Jean le Baptiste à la mosquée des Omeyyades à Damas (Syrie).

 a rencontré le dalaï-lama, Tenzin Gyatso au Vatican en 1980, 1982, 1986, 1988 et 1990. Plus tard, le , après une audience avec le pape, le dalaï-lama a déclaré lors de sa rencontre avec le président du Sénat italien Marcello Pera : .

Le pontificat est marqué par une volonté de rapprochement avec les églises orientales. Dès le début il se pose en avocat des églises orthodoxes en grande partie contrôlées par le régime communiste. En se proclamant le chef de l'Église silencieuse, il affirme sa défense des églises orientales et occidentales lors de sa première visite en Pologne.

En 1985, il publie l'encyclique "Slavorum Apostoli"

Sur le sujet de la primauté du pape, il a proposé aux chrétiens des autres confessions de dans l’encyclique "Ut Unum Sint" (1995).

En 1999, visite la Roumanie avec les personnalités locales de l’Église orthodoxe. Il est d’ailleurs le premier pape à visiter un pays à majorité orthodoxe depuis le schisme de 1054. Au cours de ce voyage, il demande pardon au nom des catholiques pour le sac de Constantinople.

Lors du Jubilé de l'an 2000, il ouvre la Porte Sainte avec le métropolite orthodoxe Athanasios et le primat anglican George Carey, marquant la volonté d'unité des différents chrétiens. Cependant il ne put jamais se rendre en Russie, le patriarche de Moscou, , refusant de le rencontrer.

Le , il signe avec le patriarche œcuménique la déclaration de Venise , une des premières déclarations communes entre catholiques et orthodoxes depuis le schisme de 1054.

En 2004, lors d'un voyage en Grèce, il offre les reliques de Grégoire de Nazianze, conservées jusque-là au Vatican, au Patriarche dans une logique de réconciliation.

Les tentatives de réconciliation avec les orthodoxes ont aussi été entravées par des conflits de juridictions et de frontières, les Églises uniates réclamant les églises confisquées par les Soviétiques au profit des orthodoxes. Le pape fut critiqué du fait du prosélytisme des catholiques en Russie, conduisant au refus de l'épiscopat russe de le recevoir. Enfin la reconnaissance par le Vatican de l'indépendance de la Croatie fut très mal vécue par les orthodoxes serbes qui considéraient ce pays comme lié à la Serbie.

À plusieurs reprises il demande pardon, au nom des catholiques, pour les torts infligés aux autres chrétiens. Ainsi, lors de son voyage en Slovaquie, il se rend devant un monument commémorant l'assassinat de calvinistes par des catholiques.

En 1998, les Églises luthériennes signent avec le Vatican ensemble un texte, la Déclaration commune sur la justification par la foi, sur une conception commune de la . Ils parviennent ainsi à un accord sur l'un des points principaux des divergences issues de la réforme de Luther.

 développa une véritable théologie du corps au cours de de 1979 à 1984. Cet enseignement est considéré comme une théologique. Dans sa catéchèse, affirme, en s’appuyant sur une anthropologie biblique, que le corps, créé à l’image de Dieu, a pour vocation première de permettre la communion entre l’homme et la femme, cette communion étant à l’image de la communion des personnes en Dieu. La sexualité ne peut donc pas se réduire à une relation de plaisir, qui réduit l’homme ou la femme à un objet dont on peut se satisfaire. Cette tendance utilitariste est selon une conséquence du péché originel. Cependant selon , le Christ contribue à restaurer la sexualité à travers le mariage, qui devient donc le lieu indissociable de la sexualité. Le mariage est le lieu de la communion entre deux personnes, à l’image de Dieu. La relation du mariage conduit à une relation de soumission réciproque de l’homme et de la femme, source de sanctification. La sexualité, le don des corps selon , dans l’acte conjugal vient donc exprimer et réaliser le don mutuel que les époux font d’eux-mêmes et de toute leur vie. La sexualité exprime donc l’amour, la fidélité et l’honnêteté entre les époux.

Cette conception conduit à confirmer l’opposition de l’Église à la contraception. Celle-ci va à l’encontre de la dignité du mariage et du don véritable des époux, et empêche une communion véritable à l’image de Dieu. Dans un entretien avec des scientifiques, il affirme qu’il ne veut pas séparer la sexualité de sa , la contraception allant à l’encontre de la vocation de l’homme et de l’ordre dans lequel Dieu l’a créé. Selon l’homme n’est pas et ne doit pas être maître de la vie, mais dépositaire de la vie.

Son opposition alla aussi à l’encontre de l’avortement. La vie humaine étant présente dès la fécondation, tout avortement constitue selon lui un meurtre, constituant une atteinte fondamentale tant aux dix commandements , mais aussi à la dignité de l’homme qui est niée.

À plusieurs reprises, il a rappelé l’enseignement de l’Église concernant l’exigence de fidélité conjugale et la recommandation d’éviter les méthodes artificielles de contraception. Ainsi quand on l’interrogea sur la possibilité d’utiliser la contraception pour éviter des avortements, affirma que la contraception et l’avortement étaient les deux fruits d’une même plante, qui conduit à nier toute la vocation à l’amour présente dans le mariage.

Il n'a jamais prononcé une seule fois le mot préservatif, mais a par contre insisté de nombreuses fois sur l'efficacité absolue de l'abstinence et de la fidélité contre les maladies sexuellement transmissibles. Cette position fut très vivement critiquée, certains accusant le pape d’être responsable du SIDA en Afrique.

Il s’est fait le défenseur inlassable du droit à la vie, rappelant l’opposition de l’Église à l’avortement, l’euthanasie et à toute forme d’eugénisme. Il a également appelé à une plus ferme condamnation de la peine de mort.

Face aux nouvelles questions de bioéthique et notamment la fécondation artificielle, il publia le document Donum Vitæ. Le document la considère comme , ainsi le détachement de la fécondation de l'acte sexuel, tout comme la contraception est là encore critiqué. Il s’opposa à tous les travaux sur les cellules souches embryonnaires, le clonage humain, qu’il considère comme une atteinte à la dignité humaine.

Il a également confirmé la tradition catholique sur le mariage en s'opposant au mariage homosexuel. Il a par ailleurs maintenu l’interdiction de la communion sacramentelle pour les divorcés remariés en raison de leur absence de communion spirituelle préalable avec l'enseignement de l'Église.

Plusieurs observateurs ont relevé que le Saint-Siège avait tardé à réaliser l’ampleur du problème des abus sexuels commis par des prêtres. Ces dossiers étaient traités, la plupart du temps, dans les diocèses, ce qui a pu empêcher une prise en compte globale de ce phénomène. Pour Bernard Lecomte, , sans être indifférent, a pu être négligent sur ce problème. Les accusations en 1998 contre le père Marcial Maciel Degollado, fondateur des Légionnaires du Christ, n'ont pas été traitées avec suffisamment de moyens et de rapidité. Cette confiance excessive dans la personne du père Marcial Maciel constitue, d'après George Weigel, une erreur de gouvernement du pape. Les allégations d'abus sexuels contre le cardinal Hans Hermann Groër, n'ont pas non plus donné lieu à une enquête immédiate. L'habitude de traiter les affaires de mœurs dans la discrétion, une certaine culture du silence qui prévalait sur ces sujets, n'ont pas favorisé l'émergence de la vérité et la reconnaissance publique des souffrances subies par les victimes. Pour plusieurs vaticanistes, un tournant est pris en 2001, avec le motu proprio "Sacramentorum sanctitatis tutela" de et la lettre "De delictis gravioribus" ("Les Délits les plus graves)", envoyée par le cardinal Ratzinger, imposant aux évêques de faire remonter les dossiers d'abus sexuels à Rome. Une plus grande transparence est alors préconisée. En avril 2002, alors que le scandale des abus sexuels de prêtres américains sur des enfants vient d'éclater, convoque onze cardinaux, tous venus des États-Unis. À cette occasion, il déclare : Il ajoute être et tient à exprimer sa .

Il a redonné une impulsion au culte des saints, en célébrant et dont . Il réforme les exigences de la canonisation, en ne demandant qu'un miracle au lieu de deux pour canoniser. La volonté du Pape était de montrer l'universalité de la sainteté, le Concile affirmant que tous les chrétiens étaient appelés à la sainteté. voulait donc revivifier la dévotion aux saints qui avait été un peu oubliée après le Concile , la vie des saints étant souvent considérée comme exceptionnelle et éloignée de la réalité quotidienne. II a recherché par ces nombreuses béatifications et canonisations à démontrer que tous les catholiques étaient appelés à devenir des saints, et ceci quels que soient leurs pays, leurs cultures et leurs origines, montrant par là même l'universalité de l'Église. Ainsi, il béatifia de nombreuses personnes, tant laïcs que prêtres et religieux, montrant que tous les états de vies, le mariage comme la vie religieuse, étaient des formes possibles de la sainteté.

En octobre 1986, il décide de constituer une commission de cardinaux et d’évêques pour préparer un projet de catéchisme universel romain et en confie la présidence au cardinal Ratzinger. Le cardinal autrichien Christoph Schönborn en sera l’un des principaux rédacteurs. Le "Catéchisme de l'Église catholique" est approuvé officiellement, le , par le pape qui le considère comme un acte majeur de son pontificat.

La publication du catéchisme de l'Église catholique avait pour objectif de montrer que le catholicisme pouvait rendre compte de la foi, de l'amour qui sont à la base de la vie chrétienne. Dans cet ouvrage sont expliquées la doctrine et la tradition de l'Église catholique. Il place au cœur de l'enseignement de l'Église l'enseignement de la Vérité.


Le , à l'occasion de la commémoration du centenaire de la naissance d'Albert Einstein, il exprime le désir que des théologiens, des savants et des historiens, animés par un esprit de sincère collaboration, approfondissent l'examen du cas Galilée. Le , il désigne une commission d'étude chargée de réexaminer l'affaire Galilée, afin de reconnaître les erreurs commises par l'Église. Le , il reconnaît les erreurs de la plupart des théologiens dans la condamnation de Galilée en 1633.

Le , il reconnaît dans un message à l’Académie pontificale des sciences que la théorie de l’évolution est , faisant allusion au qualificatif qu'avait employé dans son encyclique de 1950, "Humani Generis". Il précise en revanche que les théories qui verraient et .

Le , il promulgue l’encyclique "Fides et Ratio" sur les rapports entre la foi et la raison.

Jean-Paul II a proclamé François d'Assise patron céleste des écologistes en 1979.

Il a abordé la question écologique sous un angle théologique, en associant la « structure de péché » à ce qui blesse la Création de Dieu. Ses interventions les plus remarquées en matière d'écologie sont :

Dans l'encyclique "Centesimus Annus", en 1991, il évoque encore la question écologique :

En 2001, lors d'une audience générale, il a appelé à une , soulignant que .

Le , il a signé avec le patriarche œcuménique la déclaration de Venise , une des premières déclarations communes entre catholiques et orthodoxes depuis le schisme de 1054. Cette déclaration fixe six objectifs éthiques.

Le 8 avril 2005, lors des funérailles de , présidées par le doyen du collège cardinalice Joseph Ratzinger, une partie de la foule scande en italien , appuyant cette demande par des calicots écrits en grandes lettres rouges. Le futur pape , alors encore cardinal Ratzinger et responsable de l'office religieux, n'a pas répondu immédiatement à ces souhaits d'autant plus que ce mouvement était mûrement préparé et non spontané : l'exécuteur testamentaire de Stanisław Dziwisz a joué un rôle non négligeable et des banderoles étaient faites par le Mouvement des Focolari.

Le cardinal Camillo Ruini, vicaire de l'évêque de Rome, demande que la cause de soit introduite sans attendre la fin du délai de cinq ans après sa mort. Le , soit jour pour jour après l’attentat de la place Saint-Pierre, et seulement après la mort du pape, son successeur , élu le , dispense la cause en béatification de du délai de cinq ans.

C'est lui-même qui avait ramené de trente ans (code de droit canonique de 1917) à cinq ans après la mort du candidat le délai requis pour l’ouverture d’une cause. Mais il avait aussi fait une exception à cette règle en autorisant, en 1999, l'ouverture du procès diocésain de Mère Teresa deux ans seulement après sa mort. Antoine de Padoue a été canonisé un an après sa mort, mais depuis que le pape Sixte Quint a instauré, en 1588, la procédure moderne de canonisation, jamais aucune cause n’a été ouverte aussi vite que celle de . C'est monseigneur Sławomir Oder qui est nommé postulateur de la cause en béatification du pape défunt.

Quelques théologiens s'opposent au processus de canonisation. En octobre 2007, onze théologiens parmi lesquels le jésuite espagnol Jose Maria Castillo et l'italien Giovanni Franzoni, énoncent sept points d'opposition qui incluent les dernières considérations de sur la contraception et le rôle des femmes au sein de l'Église catholique. On relève également des critiques concernant la couverture des affaires de pédophilie de prêtres catholiques, les négociations financières opaques avec la banque Ambrosiano et les sanctions à l'encontre d'une centaine de théologiens catholiques.

En novembre 2009, la congrégation pour les causes des saints valide du défunt pape. Le , le pape proclame le décret reconnaissant son prédécesseur comme "vénérable."

Le 14 janvier 2011, la guérison de la maladie de Parkinson d'une religieuse du diocèse d'Aix-en-Provence, sœur Marie Simon-Pierre, est reconnue comme un miracle, et le Vatican annonce sa décision de béatifier . La béatification a lieu le , place saint Pierre, à l'occasion du dimanche de la divine miséricorde célébré par devant plus d'un million de fidèles, parmi lesquels beaucoup de Polonais. Le cercueil de , retiré de la crypte vaticane le pour être exposé au public dans le chœur principal de la basilique Saint-Pierre de Rome, est ré-inhumé, le , dans la chapelle Saint-Sébastien de cette basilique, à la place précédemment occupée par . La canonisation de peut donc avoir lieu si une autre guérison miraculeuse, postérieure à la béatification, est authentifiée.

Le 23 avril 2013, la commission de sept médecins de la Congrégation pour les causes des saints reconnaît le caractère scientifiquement inexplicable d'une guérison attribuée à . Il s'agit de Floribeth Mora Diaz, avocate costaricienne, atteinte d'une maladie incurable, plus précisément d'une lésion cérébrale, qui aurait été guérie dans la soirée du , le jour de la béatification de .

La commission des théologiens a reconnu le caractère scientifiquement inexpliqué de cette guérison le , selon la presse italienne.

Le 2 juillet 2013, les évêques et cardinaux membres de la Congrégation pour les causes des saints se réunissent en assemblée plénière pour évoquer différents cas de béatifications et de canonisations. Dès le suivant, le pape François autorise la congrégation à promulguer le décret permettant la canonisation des bienheureux et . Lors du consistoire convoqué le , le pape fixe la date de la cérémonie de canonisation de ses deux prédécesseurs au , dimanche de la divine Miséricorde, fête instituée par , fixée par lui au deuxième dimanche de Pâques, et au cours duquel il s'éteint le .

Le , lors de la messe des Rameaux, le pape François le nomme saint patron des Journées mondiales de la jeunesse.

Le , lors de la messe du dimanche de la divine Miséricorde, le pape François préside la cérémonie de canonisation conjointe des papes et . C'est la première fois dans l'histoire de l’Église qu'une double canonisation de papes a lieu en présence de deux papes vivants, François, qui préside la cérémonie, accompagné de son prédécesseur . est fêté le 22 octobre, date de son intronisation pontificale.

En 2005, il est lauréat du Prix International Charlemagne.

Avant son enterrement, la crypte du Vatican recevait par jour. Depuis, le chiffre approche les .

En France, en 2015, 11 établissements scolaires portent son nom.

La place du Parvis-Notre-Dame de Paris s’appelle désormais par décision du maire de Paris Bertrand Delanoë.

Il en va de même pour le parvis des cathédrales de Metz, Nancy (visitées toutes les deux par le pape en 1988) et de Cambrai, et celui de l'église Notre-Dame des Mineurs à Waziers. La place jouxtant la cathédrale d'Évry (qu'il avait visitée le ), initialement appelée , porte le nom de .

Une statue en bronze de de haut du pape a été offerte à la ville de Ploërmel, dans le Morbihan, par l'artiste russe Zurab Tsereteli, nommé citoyen d'honneur de la ville. Cette œuvre d'art, installée au centre-ville, place Saint , a été inaugurée le dimanche après-midi en présence de . La subvention du conseil général du Morbihan pour ce monument a été annulée par le tribunal administratif de Rennes, à la suite d'un recours de membres de la Libre Pensée du Morbihan. La même statue (mais non surmontée d'une croix) du même artiste a été inaugurée le à Paris près de la cathédrale Notre-Dame en présence de la maire de Paris, Anne Hidalgo, de l'ancien maire de Ploërmel, Paul Anselin et de l'artiste russe, Zourab Tsereteli.

Une autre statue en bronze de et de de a été érigée le sur le parvis de la basilique de Fourvière à Lyon en mémoire de son passage le . Elle a été intégralement financée par le mécénat et la fondation Fourvière à hauteur de .

La maison où il se rendait d'habitude en été pour ses vacances, aux Combes d'Introd, en Vallée d'Aoste, est devenue aujourd'hui un musée. Elle témoigne de son amour pour la montagne, qu'il considérait être l'endroit idéal pour la réflexion et la prière.

À Nice, depuis le , la place sise devant le monastère franciscain de Cimiez porte le nom de et est ornée d'un buste le représentant.

De nombreuses reliques du pape (morceaux de soutane maculés de sang provenant de la tentative d'assassinat en 1981) sont données à des villes de pèlerinage comme Paray-le-Monial, Sainte-Anne-d'Auray ou Lourdes. D'autres reliques itinérantes (morceaux de soutane, mèche de cheveux) sont soumises à la vénération ostentatoire.


 a prononcé pendant son seul pontificat dont hors d'Italie. Ses écrits et textes de discours représentent plus de (soit environ le volume de la Bible catholique).

Les seuls écrits officiels de représentent auxquels il faut ajouter des publications à titre personnel et sans doute des milliers de lettres et documents privés divers.

 a écrit :

 a écrit :




Parmi quelques autres, le téléfilm "Karol, l'homme qui devint Pape", de Giacomo Battiato, racontant la vie de Karol Wojtyla à partir de ses dans la Pologne en guerre et jusqu'à sa mort. La prestation de Piotr Adamczyk dans le rôle de est assez étonnante, notamment par les transformations physiques majeures de l'acteur pendant le déroulement chronologique du film (vieillissement du visage et du corps).

Après sa première présentation et projection au Vatican avec le réalisateur et les acteurs, le pape a qualifié le film de et a déclaré en parlant de l'Europe et de la Pologne en guerre.

















</doc>
<doc id="4711" url="https://fr.wikipedia.org/wiki?curid=4711" title="Informatique industrielle">
Informatique industrielle

L'informatique industrielle est une branche de l'informatique appliquée qui couvre l'ensemble des techniques de conception, d'analyse et de programmation de systèmes informatiques à vocation industrielle.

Les appareils concernés contiennent en général au moins un microprocesseur ou un microcontrôleur, ainsi que des coupleurs d'interfaçage entre des machines ou appareillages industriels et de l'informatique.

Une autre définition courante est que l'informatique industrielle regroupe les programmes de supervision dont les variables représentent des mesures de grandeurs physiques comme la température d'une cuve, l'état d'un capteur ou la position d'un bras robotique.





</doc>
<doc id="4712" url="https://fr.wikipedia.org/wiki?curid=4712" title="Instrument intelligent">
Instrument intelligent

Un instrument intelligent est un capteur ou un actionneur doté de fonctionnalités de communication, de configuration et de validation, en plus des fonctionnalités de mesure ou d'action.

Il est généralement constitué d'un processeur ou d'un microcontrôleur, et d'une interface de communication à un réseau de terrain. Son logiciel peut implémenter du simple traitement du signal aux méthodes de l'intelligence artificielle.

Les instruments intelligents sont connectés en réseaux à un système central (ordinateur ou automate programmable). Il est aussi possible de créer une application complète constituée uniquement d'instruments connectés entre eux.



</doc>
<doc id="4718" url="https://fr.wikipedia.org/wiki?curid=4718" title="Liste de saints catholiques">
Liste de saints catholiques

Sans préjuger des autres, l'Église catholique reconnaît publiquement que certains disciples du Christ peuvent être présentés aux autres croyants comme des modèles de vie chrétienne, de doctrine et comme intercesseurs auprès de Dieu. Attention, la sainteté chrétienne n'est pas à confondre avec perfection morale car si de nombreux saints ont eu une vie très vertueuse, d'autres ont eu une vie moins parfaite certaines et certains étant même des "pécheurs repentis", des personnes qui, sous l'influence du Christ, prirent un tournant radical dans leur vie (conversion). Cette reconnaissance de la sainteté d'une personne ne peut débuter qu'à la mort du croyant ou de la croyante supposé "mort en odeur de sainteté". En effet, dans la théologie catholique et orthodoxe, le seul détenteur de la sainteté, le seul Saint, c'est Dieu lui-même (La Trinité soit le Père, le Fils et le Saint Esprit). La personne qui sera déclarée sainte ou bienheureuse ne peut l'être que parce qu'elle se trouve dans le rayonnement du Christ, c'est-à-dire au Paradis. C'est parce que le saint ou la sainte est dans l'intimité de la sainteté de Dieu qu'elle peut être déclarée officiellement sainte. Cette déclaration, qui fait suite à une longue procédure (procès en canonisation), est ce que l'on appelle : "porter sur les autels" pour les vénérables, la "béatification" pour les bienheureuses et bienheureux et la "canonisation" pour les saintes et les saints. Il existe en effet trois niveaux de sainteté dans l'église catholique. Les vénérables (premier niveau) dont l'église reconnaît "l'héroïcité des vertus" chrétiennes sans pour autant qu'un culte soit rendu à cette personne (comme le Vénérable Étienne Douaihy). Les bienheureux (deuxième niveau) dont la vénération est limitée à une région, un pays ou un ordre religieux (comme le Bienheureux Stéphane Nehmé, la Bienheureuse Elisabeth de la Trinité ou la Bienheureuse Marie de l'Incarnation). Et pour finir les saintes et les saints (troisième niveau) dont la vénération est étendue à toute l'église catholique (comme sainte Monique, saint Maurice d'Agaune, saint Charbel Makhlouf, saint Bernard de Clairvaux, sainte Thérèse de l'Enfant Jésus, saint François d'Assise, sainte Jeanne-Françoise de Chantal, saint Jean-Marie Vianney dit le curé d'Ars, saint Jean XXIII, sainte Catherine de Sienne). À cette liste se rattachent également des listes de saints régionaux (ainsi celle des saints bretons ou suisses comme saint Nicolas de Flue) ou particuliers (appartenant à des ordres religieux comme saint Dominique, sainte Marie de l'Incarnation, sainte Louise de Marillac).

Les premiers chrétiens, disciples de Jésus, ont aussi très rapidement été considérés comme des saintes et des saints (comme sainte Marie la mère de Jésus, sainte Marie-Madeleine ou Marie de Magdala, saint Joseph l'époux de Marie, saint Thomas apôtre, les deux apôtres Jacques (saint Jacques fils de Zébédée et saint Jacques fils d'Alphée) tout comme les archanges (saint Gabriel, saint Michel, saint Raphaël) et les évangélistes (saint Matthieu, saint Marc, saint Luc et saint Jean). Et dans les premiers siècles de l'église, ce sont les martyrs (soit les témoins de la foi, ceux et celles qui sont morts pour ne pas avoir renié leur foi chrétienne. Saint Étienne est considéré comme le premier martyr chrétien) qui ont d'abord été considérés comme saintes et saints (comme les apôtres saint Paul de Tarse et saint Pierre mais aussi sainte Blandine, saint Denis de Paris, sainte Cécile de Rome, saint Sébastien, sainte Agnès de Rome). C'est autour de leurs dépouilles que leur culte va se développer comme dans la ville d'Agaune (actuelle ville suisse de Saint-Maurice) où un important groupe de soldats romains - saint Maurice à leur tête - auraient été mis à mort sur ordre de l'empereur. Ensuite, ce sont les « confesseurs de la foi » (ceux qui ont souffert pour leur foi sans en être morts), les premiers théologiens chrétiens (les Pères de l'Église comme saint Augustin d'Hippone, saint Grégoire de Nazianze et saint Irénée de Lyon) ainsi que les premiers évêques, prêtres, ermites et moines (comme saint Ambroise de Milan, saint Patrick d'Irlande, saint Antoine le Grand, saint Benoît de Nursie, sainte Scholastique) qui ont été déclarés saints ou saintes.

Dans les premiers siècles de l'église, ces canonisations naissaient d'un élan populaire par acclamation de l'assemblée (vox populi, vox dei) ou alors par le développement d'une dévotion populaire spontanée liée à la qualité de vie chrétienne d'une personne, sa doctrine ou ses miracles.

Au Moyen Âge, dès le , l'Église catholique va peu à peu organiser et encadrer la procédure de reconnaissance officielle de la sainteté d'une défunte ou d'un défunt. C'est ce que l'on appelle le procès en canonisation. Un des premiers saints qui fut soumis à cet examen était saint Dominique (canonisé en 1234), le fondateur des Dominicains. La même année, le pape Grégoire IX affirme son droit exclusif à procéder aux canonisations. Il faudra tout de même attendre 1634 pour que le pape Urbain VIII précise toute la procédure et les critères nécessaires à cet examen des candidats à la canonisation. Les écrits, paroles, actes, réputation, vertus, miracles des potentiels saintes et saints sont analysés. Au fil des siècles, cette procédure s'est fortement alourdie et ce n'est qu'après le Concile Vatican II, en 1969, que la procédure va être modifiée en vue d'une simplification. Sous le pontificat de Jean-Paul II, ce mouvement de simplification va encore se poursuivre notamment en diminuant l'accent porté aux miracles (1 seul devient nécessaire) pour centrer l'examen de la cause du candidat sur l'étude de la sainteté de sa vie (constitution "Divinus perfectionnis magister" en 1983). Notons que la profusion de miracles du vivant du saint ou des phénomènes extraordinaires comme l'apparition de stigmates ont tendance à retarder considérablement les procédures des procès en canonisation. Cela a été le cas pour le saint Padre Pio. La décision de proclamation de la sainteté d'une défunte ou d'un défunt revient finalement au pape.

Une dernière forme de canonisation existe. Il s'agit d'une « canonisation équipollente » qui permet à une personne dont la sainteté est attestée de longue date par une vénération locale et dont le culte existe déjà, d'être déclarée sainte, même si aucun miracle n'a été rapporté et accepté comme tel par l'Église catholique. Cela a été le cas en 2012 pour sainte Hildegarde de Bingen canonisée par Benoît XVI puis, les canonisation équipollentes suivantes prononcées par le pape François comme dans le cas, le 12 octobre 2014, des deux saints canadiens : sainte Marie de l'Incarnation et saint François de Montmorency-Laval. Mais le pape François a utilisé la même procédure de canonisation équipollent pour les cas de saint Pierre Favre, sainte Angèle de Foligno, saint José de Anchieta, et sainte Catherine Tekakwitha. Il faut encore préciser que cette procédure de canonisation équipollente avait été quasi oubliée durant plusieurs siècles (moins de 40 cas en 500 ans), mais le pape François l'a remise au goût du jour puisqu'il l'a déjà utilisée six fois depuis le début de son pontificat.

Depuis le Concile de Vatican II et surtout depuis le pontificat de Jean-Paul II, un certain nombre de saints ont été retirés du martyrologe romain et l'accent a été porté sur une universalisation de la sainteté soit des hommes et des femmes venant de tous les continents (comme la sainte amérindienne Kateri Tekakwita, la canadienne Marie de l'Incarnation, sainte Rose de Lima première sainte du Nouveau monde, la sainte africaine Joséphine Bakhita) et représentant tous les états de vie et non pas seulement des prêtres, théologiens, religieux et religieuses. Des couples ont ainsi été canonisés comme les saints Louis et Zélie Martin ainsi que des célibataires comme la bienheureuse Marguerite Bays.

On trouve dans ce qu'on appelle « le Propre de l'évêché » ou le « Propre d'une congrégation religieuse », une liste des saints reconnus ou non par Rome, universels ou locaux, avec une biographie et la date de leur mort qui est, en principe, celle de leur fête, le tout formant un calendrier. Ces saints sont célébrés avec un degré variable d'obligation et de solennité selon le diocèse ou l'Ordre religieux : fête, mémoire, ou mémoire facultative et parfois solennité. Notons encore qu'il existe dans l'église catholique une fête de tous les saints, appelée la Toussaint qui prend place le novembre. Elle est célébrée dans toute l'Église depuis le et ne doit pas être confondue avec la Commémoration des fidèles défunts, célébrée le 2 novembre, qui est consacrée à tous les fidèles décédés dont les catholiques espèrent qu'ils vivent auprès de Dieu, donc dans la lumière de la sainteté de Dieu (certains pouvant être saints sans que l'église catholique ne les connaisse et donc sans qu'elle les présente individuellement à la foi des fidèles). Pour trouver la liste complète des vénérables, bienheureux et bienheureuses ainsi que des saintes et saints catholiques, il faut consulter le martyrologe romain qui compte plus de 10'000 saintes et saints.

De par leur omniprésence dans les sociétés chrétiennes catholiques et orthodoxes (dans le monde protestant la sainteté est réservée à Dieu seul qui est le seul Saint, le seul Seigneur), l'importance des saintes et des saints dans la vie de tous les jours, même la vie des non-catholiques, se lit concrètement comme on peut le constater dans le nom des rues, des localités, d'églises, de sanctuaires, de bateaux, de places de villes (comme le Mont-Saint-Michel, la place Saint-Marc à Venise, Saint-Jean-de-Maurienne, Saint-Ursanne, Saintes-Maries-de-la-mer, Notre-Dame de Paris ou la Bonne-Mère de Marseille, le sanctuaire de Notre-Dame de Lourdes ou de Notre-Dame de Lorette ou encore le pèlerinage à Saint-Jacques-de-Compostelle) ainsi que dans les prénoms donnés aux enfants afin de les placer sous la protection d'un saint, dit saint patron (à une époque, dans certaines régions ou certains pays, l'usage existait de donner à un enfant le nom du saint fêté le jour de sa naissance). Chaque église est d'ailleurs placée sous le patronage d'une sainte ou d'un saint comme celui de Marie (voir toutes les églises, abbayes, couvents ou sanctuaires "") de Sainte-Sophie à Constantinople ou encore de Saint-Pierre de Rome. Mais cette présence touche aussi les métiers (l'évangéliste saint Luc patron des médecins, sainte Apolline patronne des dentistes, saint Jérôme de Stridon patron des bibliothécaires et des archivistes), les maladies (comme saint Roch patron des pestiférés ou sainte Rita patronne des causes perdues, saint Damien dit le Père Damien patron des lépreux), les activités comme la chasse (saint Hubert patron des chasseurs) ou le voyage (saint Christophe patron des voyageurs), les dictons (comme ceux qui concernent les saints de glace ou saint Médard), les réflexes parfois encore automatiques de certaines personnes qui invoquent saint Antoine de Padoue lorsqu'elles ont perdu quelque chose et qu'elles la recherchent, ou bien de « coiffer sainte Catherine », tout comme la tradition ancienne des feux de la Saint-Jean, la musique (comme en écoutant un "Stabat Mater"), l'art de la peinture, de la sculpture (avec une "Madone" ou une "Mater dolorosa"), de l'icône, ... La connaissance du fait religieux est alors nécessaire pour rendre accessible et intelligible une partie essentielle du patrimoine culturel de l'humanité. Elle permettant ainsi une lecture raisonnée d'un grand nombre d'œuvres, notamment picturales du Moyen Âge et de la Renaissance en Occident, mais aussi une meilleure connaissance de l'architecture, de la toponymie, de l'histoire, de l'histoire des religions, de l'œcuménisme, de l'histoire des idées, ...

La liste de saintes et de saints présente ci-dessous permet ainsi d'accéder à la biographie réelle ou à l'hagiographie (récits légendaires de vies de saints, l'ouvrage le plus important étant probablement la "Légende dorée") ainsi qu'aux attributs d'un certain nombre de saints (comme la croix de l'apôtre saint André, les roses de sainte Thérèse de Lisieux, la tête sur un plateau de saint Jean-Baptiste, l'aigle de saint Jean l'évangéliste, les extases et la transverbération de sainte Thérèse d'Avila), Soulignons que cette liste ne comprend qu'un nombre restreint de saintes et saints catholiques.










</doc>
<doc id="4719" url="https://fr.wikipedia.org/wiki?curid=4719" title="Chou">
Chou

Chou peut désigner :








</doc>
<doc id="4722" url="https://fr.wikipedia.org/wiki?curid=4722" title="Barbe la grande martyre">
Barbe la grande martyre

Barbe la grande martyre ou sainte Barbe ou Barbara (en grec et en latin) est une sainte fêtée le 4 décembre par les orthodoxes et les catholiques .

Sainte Barbe aurait vécu au milieu du après Jésus Christ en Bithynie (pan nord-ouest de l'Anatolie) sous le règne de l’empereur Maximien. Son père, , aurait été un riche édile païen d'origine phénicienne. Un jour, son père décida de marier Sainte Barbe à un homme de son choix ; elle refusa et décida de se consacrer au Christ. Pour la punir, son père l’enferma dans une tour à deux fenêtres, mais un prêtre chrétien, déguisé en médecin, s’introduisit dans la tour et la baptisa.

Au retour d’un voyage de son père, Barbe lui apprit qu’elle avait percé une troisième fenêtre dans le mur de la tour pour représenter la Sainte Trinité et qu’elle était chrétienne. Furieux, le père mit le feu à la tour. Barbe réussit à s’enfuir, mais un berger découvrit sa cachette et avertit son père. Ce dernier la traîna devant le gouverneur romain de la province, qui la condamna au supplice. Comme la jeune fille refusait d’abjurer sa foi, le gouverneur ordonna au père de trancher lui-même la tête de sa fille. Elle fut d'abord torturée : on lui brûla certaines parties du corps et on lui arracha les seins, mais elle refusa toujours d'abjurer sa foi. Dioscore la décapita mais fut aussitôt châtié par le Ciel. Il mourut frappé par la foudre. Quant au berger qui l'avait dénoncée, il fut changé en pierre et ses moutons en sauterelles. 

Quand les chrétiens vinrent demander le corps de la jeune martyre, ne voulant ni utiliser son prénom païen ni se dévoiler en utilisant son prénom de baptême chrétien, ils ne purent en parler que comme « la jeune femme barbare », d'où le nom de sainte Barbara qui lui fut donné.

Sainte Barbe est généralement représentée en jeune fille, avec la palme de martyre, elle peut porter une couronne, un livre. Une tour à trois fenêtres (en référence à son adoration de la Sainte Trinité), un éclair constituent également d'autres attributs de la sainte. De même, elle peut porter une plume de paon, symbole d'éternité, ou fouler à ses pieds son père qui est aussi son persécuteur. Sainte Barbe est aussi représentée avec un ciboire surmonté d'une hostie, d'un rocher qui s'entrouvre pour la mettre à l'abri et avec un canon .

Elle est représentée depuis le .

Sainte Barbe est souvent associée à trois autres saintes ayant fait vœu de chasteté : sainte Catherine, sainte Marguerite et sainte Geneviève. C'est le cas pour l'autel des Vierges de l'église Notre-Dame de Croaz Batz de Roscoff (Finistère) présentant une statue de sainte Barbe accompagnée de celles de sainte Catherine et de sainte Geneviève.

Son iconographie illustre les scènes les plus populaires de sa tradition :




Les statues de sainte Barbe sont nombreuses dans les églises et les chapelles.



D'aucuns peuvent demander les prières de sainte Barbe pour être protégés de la foudre, mais elle est aussi la patronne, le modèle et la protectrice des architectes, des géologues, des pompiers, des mineurs (et par extension actuellement, des ingénieurs des Mines), des artilleurs, des sapeurs, des canonniers, des artificiers, des ingénieurs de combat, des métallurgistes, des démineurs et autres corporations liées au feu, les pétroliers militaires, les foreurs et les personnels de l'industrie des turbines à gaz, les carillonneurs, les égoutiers. Sainte Barbe est aussi la patronne de l'École polytechnique. Dans les Forces armées canadiennes, sainte Barbe, sous le nom de Santa Barbara, est la patronne du génie militaire et de l'artillerie. Pie XII la déclare patronne de la marine italienne de combat par un bref du 4 décembre 1951.

En particulier, le fort patronage que lui vouaient les mineurs de fond s’est progressivement transmis aux ouvriers et ingénieurs des travaux souterrains (tunnels, cavernes, etc.) avec la disparition progressive de l’industrie minière occidentale. De nos jours, une sainte Barbe trône toujours à l’entrée des tunnels en construction pour protéger les ouvriers-mineurs des accidents de chantier. Dans le tunnel de Cointe à Liège (Belgique), lors de la finition des travaux, a été aménagée une potale en métal vitrée abritant la statue de la sainte qui était censée protéger les ouvriers durant le chantier.

. Une partie fut emmenée en Italie par les Vénitiens, et une autre au par la fille d’Alexis Comnène à Kiev, où elles se trouvent toujours à la cathédrale Saint-Vladimir de Kiev.

Le culte de sainte Barbe se popularise à partir du en Occident. Cette sainte orientale est particulièrement honorée en Normandie et en Bretagne.

Le 4 décembre, jour de la Sainte-Barbe, patronne des sapeurs-pompiers, mineurs, artificiers, ainsi que d'autres métiers en rapport avec le feu, il est de tradition de défiler dans les rues jusqu'aux différents puits de mines. Ce défilé est généralement accompagné d'un spectacle. Le maire donnait traditionnellement un jour de congé à cette occasion, jusqu'à une époque récente.

Le premier samedi de décembre, à Bozel, Savoie la chapelle Sainte-Barbe est exceptionnellement ouverte, et une messe y est célébrée le matin. Puis s’ensuit une dégustation de soupe traditionnelle (haricots et lard), préparée dans la nuit dans un immense chaudron par les habitants et partagée entre toutes les personnes présentes.

Le 4 décembre, en Provence, chacun doit semer du blé ou des lentilles sur du coton imbibé d’eau dans trois coupelles que l'on pose sur la table le soir de Noël. Une bonne pousse est signe d’abondance et de prospérité.

La forme bretonne est "Barba".

Sainte Barbe (Santa Barbara) est la sainte patronne des mineurs de charbon des Asturies. Une chanson syndicale (qui comprend quelques couplets anarcho - syndicalistes, dont un extrêmement grossier) lui est dédiée . Elle fut longtemps chantée par les opposants au régime du général Franco. Cette chanson est également connue sous le titre: " "El Pozo Maria Luiza""

Le 4 décembre, de la vallée du Rhin (Alsace incluse) jusqu'à la mer Noire en passant par l'Allemagne du Sud, dans l'ancienne monarchie austro-hongroise et les principautés danubiennes, on coupait des branches d’arbres fruitiers (en particulier le cerisier) qui étaient placées dans un vase rempli d’eau. À partir de là, il fallait quotidiennement couper un petit bout du pied de la tige et renouveler l’eau. Si on observe bien ces recommandations, les branches fleurissent vers Noël et une belle floraison est signe d’abondance.

Le 4 décembre, au Liban, on commémore la fuite de sainte Barbe (appelée Barbara) de la tour où elle fut emprisonnée. D'après la croyance, sa fuite n’aurait guère réussi sans l’aide de ses amies qui lui donnèrent l’idée de se déguiser. D'où la tradition libanaise qui veut que la veille de la fête de la Sainte-Barbe, soit le 3 décembre, les enfants se déguisent avec toutes sortes de costumes et de masques et vont cogner aux portes du voisinage et ainsi charmer les adultes (parfois eux-mêmes déguisés) en sollicitant des friandises ou de l'argent, non sans leur avoir au préalable chanté une rengaine à la gloire de Barbara, tout en jouant de la darbouka, soit sur leur pas de porte, soit en chemin dans la rue.

La légende veut aussi que durant sa fuite, Barbara se soit cachée dans un champ de blé et qu'elle se soit nourrie de cette céréale. Ainsi, en mémoire de sainte Barbe, la coutume veut que l'on prépare, dès le 3 décembre, une bouillie de grains de blé sucrée, parfumée à l'anis et garnie d'une flopée de graines de fruits secs (amande, noisette, pistache, pignon, noix…). Cette délicieuse collation est consommée dans les foyers et offerte aux visiteurs. Il est aussi fréquent que l'on fasse pousser, symboliquement, des germes de blé dans les foyers en cette saison. Ces traditions sont particulièrement mises en valeur à Amchit où se trouve une belle et antique église dédiée à sainte Barbe. Cet aspect folklorique de la fête rappelle, dans sa forme, celle du Mardi gras en France ou aussi la célèbre fête de la Halloween dans les pays anglophones. La fête de la Sainte-Barbe intervient exactement à trois semaines du 25 décembre, jour de Noël.

Dans la tradition afro-cubaine et afro-brésilienne la sainte est assimilée au dieu yoruba Xango (à prononcer Chango) dont elle était la première épouse et à qui elle doit ses pouvoirs sur le tonnerre.

Dans la tradition afro-brésilienne, sainte Barbe est assimilée à l'Oricha (ancêtre divinisé) Yorouba (peuple Nigeria) "Oya" connue également sous le nom de "Yansan", qui fut l'une des épouses de Chango, d'Oyo. Elle est la patronne du fleuve Niger en Afrique et gouverne les orages et la foudre dans la tradition du candomblé brésilien.

Le syncrétisme entre sainte Barbe et Yansan est au centre de la pièce de théâtre de l'écrivain brésilien Dias Gomes intitulée « O Pagador de promessas » et qui a été adaptée au cinéma par Anselmo Duarte. Ce film a obtenu la palme d'or au festival de Cannes de 1962, sous le titre français de « La Parole donnée ».

Dans la marine "ancienne", la "sainte-barbe" était le nom de la soute à munitions ou réserve de poudre à canon.

Le nom de « sainte-barbe » donné aux soutes à munitions viendrait d’un glissement phonétique. À l’époque de l’"epidum" romain, entouré d’une palissade, les produits dangereux étaient stockés dans une enceinte extérieure dénommée naturellement "cincta barbara", enceinte barbare. Selon un général français, le général Chapel, par déformation de prononciation, cette locution serait devenue "sancta barbara", « sainte barbe ».

La fête de la Sainte-Barbe est l'occasion pour les élèves-ingénieurs des écoles de l'IMT comme Mines Paris Tech ou Mines Albi de célébrer leur sainte patronne.

Chaque année, une femme du monde artistique ou politique est choisie comme marraine de cette nouvelle promotion, en l'honneur de sainte Barbe. Son identité est révélée lors d'un spectacle parodique (la Petite Revue) mis en scène par la promotion sortante. Avant ce spectacle, la marraine adresse traditionnellement un discours à la nouvelle promotion.

Au sein des corps de sapeurs-pompiers, la fête de la Sainte-Barbe est souvent associée à un défilé et à un repas ou un bal dans de nombreuses villes de France. Cette fête traditionnelle se réfère à la sainte réputée protectrice des sapeurs-pompiers.






</doc>
<doc id="4753" url="https://fr.wikipedia.org/wiki?curid=4753" title="Samurai">
Samurai

Samurai est la translittération du mot japonais 侍, synonyme de bushi (武士), francisé en samouraï.

Il s'agit également du nom de :


</doc>
<doc id="4754" url="https://fr.wikipedia.org/wiki?curid=4754" title="Biophotonique">
Biophotonique

La biophotonique concerne l'utilisation de la lumière pour l'analyse d'objets biologiques mais aussi leurs modifications. C'est une nouvelle science qui couple la biologie et la photonique. La photonique est la science qui traite de la génération, la manipulation, et la détection des photons qui sont des "êtres quantiques". Ces êtres quantiques lorsqu'ils se propagent sont représentés par une onde (voir Indice de réfraction), plus exactement une onde électromagnétique. Dans le cas de la biophotonique, on se limite à la fenêtre de l'eau où l'absorption est faible, qui est la partie ultraviolette, visible et proche infrarouge. Cette fenêtre nommée UV-Vis-NIR correspond au spectre électronique. Lorsque ces êtres quantiques sont mesurés, ils sont alors représentés par un corpuscule ou une particule et on parle alors de Dualité onde-particule. Au départ la photonique a été pensée comme une image de l'électronique en considérant que les photons allaient jouer un rôle central dans les technologies comme les électrons qui sont aussi des êtres quantiques mais eux, ont une masse et une charge électrique (et ne vont pas à la vitesse de la lumière). Ce spectre électronique est la partie des ondes électromagnétiques qui interagissent avec les électrons de valence de manière très particulière. Il est bordé par les UV du vide et les rayons X que l'on nomme rayonnements ionisants qui provoquent des ionisations) et sont très nocifs. La fenêtre UV-Vis-NIR correspond aux transitions électroniques et à divers types de déplacement d'électron. Le vivant utilise beaucoup ces formes de conversion d'énergie entre ces deux types d'êtres quantiques, par exemple la photosynthèse, les mécanismes moléculaires de la vision, la synthèse de la vitamine D... Une des problématiques centrales de la biophotonique est de quantifier les "effets secondaires" des rayonnemenst UV-Vis-NIR sur le vivant de l'échelle moléculaire (p. ex. photolyse) jusqu'aux échelles écologiques. 
La biophotonique n'est pas seulement une forme de juxtaposition (pluridisciplinarité) ou d'intersection (interdisciplinarité) de ces deux sciences mais vise la transdisciplinarité. Pour situer la biophotonique dans une large perspective, on peut consulter l'Histoire de l'optique, la Chronologie de la biologie et l'Histoire de l'histoire naturelle notamment les sections sur la microscopie optique. On pourrait croire que la biophotonique a toujours existé comme une partie de la biologie ou une partie de l'optique mais en fait, la prise de conscience de l'émergence d'une nouvelle discipline avec sa propre culture métissée s'est effectuée en juin 1989 par le fait éditorial d'avoir groupé 22 articles pour la première fois dans une revue reconnue. La publication groupée de 22 articles sur les propriétés optiques des tissus des mammifères a été un marqueur décisif.

Dans l'histoire de la biophotonique française, on peut repérer trois étapes:
La biophotonique n'est pas seulement l'étude de phénomènes liés aux sciences du vivant utilisant l'optique et la photonique. Ainsi la biophotonique a aussi apporté de nouvelles voies en optique par exemple via l'utilisation des connaissances d'objets biologiques sur la structuration de la matière vivante générant des interférences (voir par exemple Cristal photonique). On constate donc que le couplage entre les deux anciennes disciplines académiques a lieu dans les deux sens. En final le cœur de la biophotonique est l'étude de l'interaction entre la lumière et le vivant. Dans la liste non exhaustive ci-dessous, les grands domaines applicatifs de la biophotonique apparaissent.

La biophotonique présente trois grands domaines: (i) soit on désire analyser des objets biologiques et donc minimiser les modifications induites par les rayonnements, (ii) soit modifier ces objets biologiques, (iii) soit utiliser les structurations chimiques (i.e. hémoglobine) et/ou physiques (i.e. iridescence) du vivant pour modifier le comportement de la lumière.
En biophotonique les sous-domaines sont souvent classés par technique.
Au-delà du microscope optique, un des champs les plus considérables concerne les microscopies. La biophotonique traite aussi du développement d'outils en microscopie optique pour l'imagerie de cellules vivantes et de tissus. Son avantage, par rapport à d'autres techniques, est l'accès à une imagerie biologique dans des conditions physiologiques.
Les développements actuels 
visent à accéder à de meilleurs composants optiques et à de nouveaux contrastes optiques pour atteindre par exemple une meilleure sensibilité, ou une meilleure résolution optique afin de descendre vers la résolution nanométrique (on parle alors non plus de microscopie mais de Nanoscopie).
Les contrastes optiques utilisés sont principalement l'Absorption (optique), la fluorescence, la fluorescence à deux photons, l'optique non-linéaire (génération de seconde harmonique, de troisième harmonique..), et les différences d'Indice de réfraction...
Un des obstacles de ces techniques est notamment le photoblanchiment.










</doc>
<doc id="4757" url="https://fr.wikipedia.org/wiki?curid=4757" title="Variables d'environnement CGI">
Variables d'environnement CGI

Les variables d'environnement CGI sont des variables transmises à un programme CGI, par le serveur Web l'invoquant, lors de son exécution.

Elles fournissent des informations sur la requête effectuée par le client, sur le serveur et également sur le client lui-même. Par exemple, lorsqu'on effectue une recherche sur un site qui fonctionne avec des CGI, le script récupérera les termes de la recherche avec la variable d'environnement « QUERY_STRING ».



Toutes les variables qui sont envoyées par le client sont aussi passées au script "CGI", après que le serveur a rajouté le préfixe « HTTP_ ». Voici quelques exemples de variables possibles :



</doc>
<doc id="4758" url="https://fr.wikipedia.org/wiki?curid=4758" title="Techniques de gravure égyptienne">
Techniques de gravure égyptienne

Il existait de multiples techniques de gravure employées par les Égyptiens de l'Antiquité.

Dans les bas-reliefs, la gravure se détache légèrement du fond. Pour cela, on enlève de la matière, sauf dans les parties constituant les motifs. Cette technique est tellement courante qu'on associe souvent (à tort) le mot "bas-relief" à toutes les techniques de gravures égyptiennes.
Dans les hauts-reliefs, la gravure se détache nettement du fond (les trois quarts du volume). C'est une technique qui demande donc beaucoup plus de travail que les autres.

Dans l'image ci-contre, le dieu Rê, représenté avec une tête de rapace, est gravé en haut-relief. De part et d'autre, le pharaon Ramsès faisant offrande de Maât est, lui, en relief en creux.
La technique du relief en creux consiste à créer des volumes par le creusement en profondeur. Les parties normalement les plus saillantes sont creusées le plus profondément. Apparus à l'époque amarnienne, les reliefs en creux subsisteront durant la période ramesside. C'est la technique la moins coûteuse. Elle a aussi l'avantage de bien « accrocher » la lumière et est donc souvent préférée aux autres pour les reliefs exposés à la lumière du jour.

On trouve parfois, dans une même scène, des reliefs en creux associés à des bas-reliefs.

On pourrait utiliser le relief comme un moule. Si on verse du plâtre ou de l'or, on obtient un moulage qui ressemble au modèle.
Variante très courante :

Ici, le relief est dans le mur, il n'en dépasse pas. Le relief est donné en marquant les contours des motifs, ce qui est beaucoup plus simple que de dégager toute la surface autour du sujet comme pour les bas-reliefs et hauts-reliefs.


</doc>
<doc id="4759" url="https://fr.wikipedia.org/wiki?curid=4759" title="Tracé des hiéroglyphes">
Tracé des hiéroglyphes

Les techniques de tracé des hiéroglyphes varient en fonction du support utilisé mais également de l'époque.

Les techniques de tracé des hiéroglyphes sur la pierre s'apparentent aux techniques de gravure sur pierre. On retrouve donc les deux techniques à la base de la gravure égyptienne : les bas-reliefs et les hauts-reliefs.

Cependant, la gravure ne fut pas la seule technique de tracé des hiéroglyphes sur la pierre. 

Dans les tombeaux, les hiéroglyphes furent souvent peints. Ainsi, la tombe de Thoutmôsis contient de larges extraits du « Livre de l'Amdouat » tracés à l’encre rouge et noire. Dans la tombe de la reine Néfertari en revanche, les hiéroglyphes sont colorés. Les couleurs sont celles que l’objet représenté possède naturellement : le vert des végétaux, le bleu du ciel ; ou bien elles ont une valeur symbolique (cf. symbolique des couleurs dans l'Égypte antique) dont le sens nous échappe fréquemment, à moins qu’il ne s’agisse d’une simple convention : le rouge ou le bleu pour le signe « ânkh » par exemple, le jaune pour certains animaux, le bleu pour le trône ("st").

Sur les parois et les colonnes des temples, les deux techniques, - gravure et peinture des hiéroglyphes -, étaient associées. Parfois, ces reliefs peints ont gardé toute la fraîcheur et l’éclat de leurs couleurs d’origine.

Les Égyptiens utilisaient le calame pour écrire les hiéroglyphes à l'encre noire ou rouge.



</doc>
<doc id="4762" url="https://fr.wikipedia.org/wiki?curid=4762" title="Clavier">
Clavier

Le mot clavier peut avoir plusieurs significations :

Dérivé avec suffixation -ier* du radical latin "clavis" (signifiant clef).





</doc>
<doc id="4768" url="https://fr.wikipedia.org/wiki?curid=4768" title="World Wide Web">
World Wide Web

Le (WWW), littéralement la « toile (d’araignée) mondiale », communément appelé le Web, et parfois la Toile, est un système hypertexte public fonctionnant sur Internet. Le Web permet de consulter, avec un navigateur, des pages accessibles sur des sites. L’image de la toile d’araignée vient des hyperliens qui lient les pages web entre elles.

Le Web n’est qu’une des applications d’Internet ; distincte d’autres applications comme le courrier électronique, la messagerie instantanée et le partage de fichiers en pair à pair. Le Web a été inventé au CERN par Tim Berners-Lee plusieurs années après Internet. C'est également lui qui a rendu les médias grand public attentifs à Internet. Depuis, le Web est fréquemment confondu avec Internet ; en particulier, le mot "Toile" est souvent utilisé dans les textes non techniques sans dévoiler clairement s'il désigne le Web ou Internet.

Le "" est désigné par de nombreux noms et abréviations synonymes : , , , , "WWW", "W3", "Toile d’araignée mondiale", "Toile mondiale", "Toile". Certains ont disparu. Le nom du projet originel était . Les mots ont été rapidement séparés en pour améliorer la lisibilité. Le nom a également été utilisé par les inventeurs du Web, mais le nom désormais préconisé par le sépare les trois mots sans trait d’union. Bien que « mondial » s’écrive ou en anglais, l’orthographe et l’abréviation sont maintenant bien établies. Le terme "World Wide Web" est une allitération appartenant au domaine des virelangues
En inventant le Web, Tim Berners-Lee avait aussi pensé à d’autres noms, comme (maillage d’informations), ou encore (la mine d’informations, dont le sigle serait "Tim"). Le sigle "WWW" a été largement utilisé pour abréger avant que l’abréviation prenne le pas. "WWW" se prononce souvent "trois double V", "triple double V", "vévévé" ou "wéwéwé" (en Belgique). La prononciation laborieuse (en français comme en anglais) de "WWW" a sans doute précipité son déclin à l’oral. À l’écrit, les lettres "www" restent très utilisées dans les adresses Web et quelques autres conventions techniques. "WWW" est parfois abrégé en "W3", abréviation qu’on retrouve dans le sigle "W3C" du "". Dans la seconde moitié des années 1990, une blague répandue prétendait que "WWW" signifiait , soit « attente mondiale », car le réseau Internet était engorgé par la popularité grandissante du Web.

Pour écrire « le web », l’usage de la minuscule est de plus en plus courant. L’Office québécois de la langue française préconise la majuscule, le Journal officiel français préconise l'usage du terme « toile d’araignée mondiale ».

L’expression signifie « connecté à un réseau », en l’occurrence le réseau informatique Internet. Cette expression n’est donc pas propre au web, mais à Internet dans sa globalité, on la retrouve également à propos des réseaux téléphoniques.

Un hôte est un terme général qui désigne tout ordinateur (ou serveur) connecté à un réseau informatique. C'est à partir d'un hôte qu'un utilisateur se connecte pour accéder au reste du réseau. Lorsque le réseau est le "World Wide Web", un hôte est un ordinateur en ligne : Chaque hôte d'Internet est alors identifié par une adresse IP à laquelle correspondent zéro, un ou plusieurs noms d'hôte.

Une ressource du "World Wide Web" est une entité informatique (texte, image, forum Usenet, boîte aux lettres électronique, etc.) accessible indépendamment d’autres ressources. Une ressource en accès public est librement accessible depuis tout Internet. Lorsqu’une ressource est présente sur l’hôte de l’utilisateur, elle est dite locale ; par opposition elle est distante si elle est présente sur un hôte différent. Lorsque celle-ci est connectée et disponible au réseau, elle est en ligne. On ne peut accéder à une ressource distante qu’en respectant un protocole de communication. Les fonctionnalités de chaque protocole varient : réception, envoi, voire échange continu d’informations.

HTTP (pour ) est le protocole de communication communément utilisé pour transférer les ressources du Web. HTTPS est la variante avec authentification et chiffrement de ce protocole.

Une URL (pour , signifiant littéralement « localisateur de ressource uniforme ») est une chaîne de caractères décrivant l’emplacement d’une ressource. Elle contient généralement dans l’ordre : le nom du protocole, un deux-points (:), deux barres obliques (//), un nom d’hôte, une barre oblique (/), et un chemin composé de texte séparés par des barres obliques. Exemple : codice_1.

Un hyperlien (ou dans le langage courant, un lien) est un élément dans une ressource qui est associé à une URL. Les hyperliens du web sont orientés : ils permettent d’aller d’une source à une destination. Seule la ressource à la source contient les données définissant l’hyperlien. Ainsi, il est possible d’établir un hyperlien sans la moindre intervention, ni coopération, de la ressource cible de l’hyperlien. Ce modèle unilatéral adopté par le Web ne se retrouve pas dans tous les systèmes hypertextes. Il a l’avantage prépondérant, dans un réseau public mondial, de permettre la création d’innombrables hyperliens entre partenaires décentralisés et non coordonnés.

HTML (pour ) et XHTML () sont les langages informatiques permettant de décrire le contenu d’un document (titres, paragraphes, disposition des images, etc.) et d’y inclure des hyperliens. Un document HTML est un document décrit avec le langage HTML.

Dans un mode de communication client-serveur, un serveur est un hôte sur lequel fonctionne un "logiciel serveur" auquel peuvent se connecter des "logiciels clients" fonctionnant sur des "hôtes clients", un seul hôte peut contenir les deux.

Un serveur HTTP (ou service web) est un logiciel utilisé pour servir des ressources à travers le protocole HTTP. Un client HTTP est un logiciel utilisé pour manipuler ces ressources.

Un serveur web est un hôte sur lequel fonctionne un serveur HTTP (ou service web). Un serveur web peut héberger les ressources qu’il dessert, peut les récupérer sur des hôtes distants, délivrer ces ressources telles quelles (on parlera alors de ressources statiques), ou bien les modifier en fonction de différents critères, on parlera alors de ressources dynamiques. Il est courant sur les services de grosse taille que les ressources soient réparties sur différents serveurs web et éventuellement différents types de serveurs HTTP, certains ayant de meilleurs performances pour les fichiers statiques, d’autre plus de souplesse pour délivrer des fichiers dynamiques. Un serveur web peut être un hôte spécialisé, une partie d’une infrastructure spécialisée ou bien encore un hôte plus généraliste. On appelle site web, l’ensemble des ressources HTTP associées à un nom de domaine et un ou plusieurs noms d’hôtes. Une adresse web est une URL de page web, généralement écrite sous une forme simplifiée limitée à un nom d’hôte. Une adresse de site web est en fait l’adresse d’une page du site prévue pour accueillir les visiteurs.

Un client HTTP est un outil permettant d’utiliser les ressources fournies par un serveur HTTP. Il est chargé, en relation avec différents services, de faire traduire le nom d’hôte en adresse IP par un DNS, d'entrer en contact avec le serveur HTTP et de lui fournir différentes informations telles que le nom du site, et la ressource désirée, ainsi que ses capacités (gestion de différents types de ressource, capacité d’affichage, possibilité de compression de données, etc.) ou sa volonté (choix des langues, protection de la vie privée) dans l’utilisation des données fournies.

Un navigateur web est un type de logiciel client HTTP à interface homme-machine et conçu pour accéder aux ressources du web. Sa fonction de base est de permettre la consultation des documents HTML disponibles sur les serveurs HTTP. Le support d’autres types de ressources et d’autres protocoles de communication dépend du type de navigateur.

Un robot d’indexation est, dans le domaine du web, un type de logiciel client HTTP, utilisé pour indexer les différentes ressources de sites web.

Un agrégateur est, dans le domaine du web, un type de logiciel client HTTP, permettant de regrouper les fils de syndication de différents sites web.

Un aspirateur de site web est un client HTTP permettant de récupérer l’intégralité d’un site web, pour le consulter ensuite hors-ligne ou en conserver une archive.

Une page web (ou page) est un document destiné à être consulté avec un navigateur web. Une page web est toujours constituée d’une ressource centrale (généralement un document HTML ou XHTML) et d’éventuelles ressources liées, automatiquement accessibles (par exemple, des images, des feuilles de style, des scripts javascript ou des fontes de caractères). Il n’est pas exceptionnel qu’un seul document HTML fasse appel à entre quelques dizaines et une centaine de ressources annexes.

Un éditeur HTML (ou éditeur web) est un logiciel conçu pour faciliter l’écriture de documents HTML et de pages web en général.

Un site web (ou site) est un ensemble de pages web et d’éventuelles autres ressources, liées dans une structure cohérente, publiées par un propriétaire (une entreprise, une administration, une association, un particulier, etc.) et hébergées sur un ou plusieurs serveurs web.

Un système de gestion de contenu (Content Management System, abrégé CMS) est un logiciel permettant de gérer le contenu d'un site.

Visiter un site web signifie « consulter ses pages ». Le terme visite vient du fait que l’on consulte généralement plusieurs pages d’un site, comme on visite les pièces d’un bâtiment. La visite est menée par un utilisateur (visiteur ou internaute).

L’expression signifie « consulter le web ». Elle a été inventée pour mettre l’accent sur le fait que consulter le web consiste à suivre de nombreux hyperliens de page en page. Elle est principalement utilisée par les médias ; elle n’appartient pas au vocabulaire technique.

On appelle audience, la fréquentation d’un site.

On appelle fil d’Ariane, le parcours effectué par un visiteur sur un site.

On appelle journal d’accès, la liste des accès effectués par les différents utilisateurs sur un serveur HTTP.

On appelle référent, la ressource ayant conduit un visiteur à une ressource servie. Elles sont données par défaut par la majorité des clients HTTP, mais peuvent être bloquées. Le référent est généralement également conservé dans le journal du serveur HTTP en association avec la ressource consultée.

On appelle User-Agent, la référence donnée par le client HTTP au serveur HTTP pour indiquer sa nature. Parmi les informations données, il peut y avoir le nom du client HTTP et sa version, le nom du moteur de rendu HTML utilisé, les langues préférées par l’utilisateur ou l’outil client, ainsi que différents autres critères comme les modules d’extension installés sur le client. Ces critères permettent au serveur HTTP d’évaluer quelles ressources seront plus adaptées à celui-ci. Différentes technologies dynamiques comme javascript ou les CSS, permettent également aux pages de supporter des clients hétérogènes et de s’adapter dynamiquement en utilisant ses ressources plutôt que celles du serveur.

La mesure de l’audience est l’étude des consultations effectuées sur un site, elle vise à comprendre les désirs des utilisateurs, en fonction des ressources les plus utilisées et du fil d’Ariane de ceux-ci.

La mesure d’audience peut se faire de différentes manières :

On appelle hébergeur web une personne physique ou morale hébergeant (rendant accessible en ligne) sur son ou ses serveurs, les ressources constituant les sites web de ses clients. Cela peut aller du simple auto-hébergement d’un particulier à celui d’une association, d’une entreprise ou d’un industriel. L’hébergement peut alors être fait à domicile sur un modem, assurant un minimum de fiabilité, dans pièce climatisée dédiée aux serveurs, dans un local généraliste au centre de données spécialisé, assurant la redondance énergétique et réseau nécessaire au fonctionnement sans interruptions 24/7.

Une agence web est une entreprise de services informatiques réalisant des sites web pour ses clients.

Un service web est une technologie client-serveur fondée sur les protocoles du web.

Un annuaire web est un site web répertoriant des sites web.

Un moteur de recherche est un site permettant d’effectuer des recherches sur l’ensemble des sites web selon différents critères.

Un portail web est un site web tentant de regrouper la plus large palette d’informations et de services possibles dans un site web. Certains portails sont thématiques.

Un agrégateur web est un site web qui sélectionne, organise et, éventuellement, valide des pages (URL) concernant un sujet précis et les met en forme de façon ergonomique et/ou attractive.

Un blog est un journal web, fournissant des informations dépendantes du temps. Un blog BD est un type de blog utilisant le medium de la bande-dessinée comme support de communication.

Un webmail est un lecteur de courrier électronique utilisant une interface web.

Un wiki est un site web éditable par les utilisateurs.

Tim Berners-Lee (TBL) travaille comme informaticien dans le bâtiment 31 () de l’Organisation européenne pour la recherche nucléaire (CERN), situé à la frontière franco-suisse, lorsqu’il propose, en 1989, de créer un système hypertexte distribué sur le réseau informatique pour que les collaborateurs puissent partager les informations au sein du CERN. Cette même année, les responsables du réseau du CERN décident d’utiliser le protocole de communication TCP/IP et le CERN ouvre sa première connexion extérieure avec Internet. L’année suivante, l’ingénieur système Robert Cailliau se joint au projet d’hypertexte au CERN, immédiatement convaincu de son intérêt, et se consacre énergiquement à sa promotion. Tim Berners-Lee et Robert Cailliau sont reconnus comme les deux personnes à l’origine du "World Wide Web". Tim Berners-Lee crée le premier navigateur et éditeur web, appelé WorldWideWeb et le premier serveur HTTP appelé CERN httpd.

Jusqu’en 1993, le web est essentiellement développé sous l’impulsion de Tim Berners-Lee et Robert Cailliau. Les choses changent avec l’apparition de NCSA Mosaic, un navigateur web développé par Eric Bina et Marc Andreessen au (NCSA), dans l’Illinois. NCSA Mosaic jette les bases de l’interface graphique des navigateurs modernes en intégrant les images au texte et cause un accroissement exponentiel de la popularité du web. Certains développeurs de Mosaic, crééront ensuite Netscape Navigator qui apporte le parallélisme, permettant également d’utiliser les pages avec image sur des connexions bas débit. Celui-ci, d’évolutions en évolutions, est devenu aujourd’hui Mozilla Firefox. Le NCSA produit également le NCSA HTTPd, un serveur HTTP qui évoluera en Apache HTTP Server, le serveur HTTP le plus utilisé depuis 1996. Le 30 avril 1993, le CERN verse dans le domaine public toutes les technologies développées autour du www. Pour l’occasion, l’organisation place en ligne ce qui peut être considéré comme étant le premier site en « www », hébergé sur la station NeXT de TBL.

En 1994, Netscape Communications Corporation est fondée avec une bonne partie de l’équipe de développement de NCSA Mosaic. Sorti fin 1994, Netscape Navigator supplante NCSA Mosaic en quelques mois. En 1995, Microsoft essaie de concurrencer Internet avec The Microsoft Network (MSN) et échoue. Fin 1995, après la sortie de Windows 95 sans le moindre navigateur web préinstallé, Microsoft lance avec Internet Explorer la guerre des navigateurs contre Netscape Navigator.

Les premières années de cet historique sont largement issues de ' ("Une petite histoire du World Wide Web").


En créant le logiciel ", Tim Berners-Lee a créé à la fois le premier navigateur web et le premier éditeur web, car il voulait faire du web un média collaboratif, dans lequel tous les acteurs consultent et créent l’information. Cependant, le web s’est immédiatement orienté en un média de diffusion d’information global plutôt que de collaboration.

Dans la première moitié des années 1990, le concept de site web à la racine d’un nom de domaine stable n’était pas établi, à commencer par la première page web dont l’adresse était codice_3. Les sites étaient souvent mis en place dans des départements techniques par des employés et des étudiants, et les URL changeaient au gré des changements de personnes et d’infrastructure. En outre, il n’existait pas de moteur de recherche efficace. Aussi de nombreuses pages étaient des listes de liens sur les pages préférées de l’auteur de la page. Cette propriété du web sera d’ailleurs exploitée plus tard par les créateurs de Google pour calculer la pertinence des pages, puis détournée par les fermes de liens. Mais bien avant cela, en janvier 1994, Yahoo! est créé et devient rapidement le plus grand annuaire web. Comme les créateurs de Yahoo! étaient des étudiants de l’université Stanford, L’URL originale était codice_6, et ce n’est qu’en janvier 1995 que le domaine codice_7 est créé.

Dans la seconde moitié des années 1990, le web devient populaire, et toutes les grandes entreprises, organisations, écoles, administrations, ouvrent un site web. Les moteurs de recherche deviennent efficaces, notamment avec l’apparition d’Altavista en décembre 1995, et pour finir Google en 1998. Dans cette phase de développement du média, un flot d’information "top-down" prédomine : un site web est fait pour diffuser les informations de son propriétaire. Les interactions s’arrêtent souvent à la recherche et au commerce en ligne. Bien sûr, le courrier électronique, la messagerie instantanée, et les forums de discussion existaient depuis plus longtemps que le web, mais ils fonctionnaient principalement avec des protocoles et logiciels spécifiques (voir , , ).

Avec les années 2000 les notions de blog, de wiki (en 2001, lancement de la Wikipédia en anglais) et de réseautage social (Myspace en 2003, Facebook en 2004) deviennent populaires. Le contenu généré par les utilisateurs se répand (Wikipédia, YouTube en 2005, Twitter en 2006). La technologie Ajax (1998, théorisée en 2005) commence à être largement utilisée pour créer des applications complètes qui tiennent dans une seule page web (Google Maps en 2004). L’expression Web 2.0, largement popularisée au milieu des années 2000, désigne cette transition dans le flux de l’information et la manière d’utiliser le web. Le succès de l’expression Web 2.0 a conduit de nombreuses personnes à appeler Web 2.5, 3.0, 4.0, etc. leur vision du Web de l’avenir.

En 2007, le W3C reprend la technologie HTML5 et projette de le normaliser à l'horizon 2014. Dès l’année 2012, la majorité des moteurs de rendu HTML permettent d’exploiter ce standard et en 2013, la majorité des Systèmes de gestion de contenu utilisent une base HTML5. HTML5 apporte la possibilité d’utiliser des vidéos et de l’audio dans les pages sans dépendre d’un format propriétaire, avec les normes MPEG-4, WebM et Ogg Vorbis, d’utiliser davantage de graphiques vectoriels avec l’utilisation de la norme SVG, de faire de la visioconférence avec webRTC. Le développement de WebGL permet désormais d’utiliser des objets en trois dimensions dans les applications web avec une grande interopérabilité. Le format JSON permet d’échanger des données, en relations avec des bases ou des systèmes de fichiers, augmentant au passage les possibilités de référencement, et d’automatisation des taches entre les applications d’Internet.

L’expression Web 3.0 est utilisée en futurologie à court terme pour désigner l’internet qui suit le Web 2.0 et constitue l’étape à venir du développement du World Wide Web. Son contenu réel n’est pas défini de manière consensuelle, chacun l’utilisant pour désigner sa propre vision du futur d’internet.

Selon une étude de la société Incapsula réalisée en mars 2013, 50 % du trafic web était généré par des bots, c’est-à-dire des machines automatisées. Une mise à jour récente de cette étude (décembre 2013) révèle que ces mêmes machines représentent aujourd’hui près de 61 % du trafic avec notamment une hausse du trafic des moteurs de recherche en hausse de 55 %. Les opérations de hacking représentent environ 4,5 % du trafic alors que les humains prennent environ 38,5 %. Autrement dit à peine plus du tiers du trafic est généré par les activités humaines. La cause réside probablement dans l’explosion du marketing qui représente à lui seul la moitié du trafic (bots et humains confondus). 

Le "", en tant qu’ensemble de ressources hypertextes, est modélisable en "graphe orienté possédant des cycles" avec les ressources pour "sommets" et les hyperliens pour "arcs". Comme le graphe est orienté, certaines ressources peuvent constituer des "puits", ou moins formellement des cul-de-sac : il n’existe aucun "chemin" vers le reste du web. À l’inverse, certaines ressources peuvent constituer des "sources" : il n’existe aucun "chemin" depuis le reste du web.

Les analyses ont montré que la structure du web répondait au modèle des réseaux invariants d’échelle présent dans la plupart des réseaux sociaux. Cela se traduit par la présence de moyeux, les hubs, vers lesquels convergent les liens hypertextes : ce sont les sites les plus importants qui constituent le squelette du web.

Techniquement, rien ne distingue le " d’un quelconque autre web utilisant les mêmes technologies. Ainsi, d’innombrables webs privés existent. Dans la pratique, on considère qu’une page d’un site web populaire, comme un annuaire web, fait partie du web. Le web peut alors être défini comme étant l’ensemble des ressources et des hyperliens que l’on peut récursivement découvrir à partir de cette page, ce qui exclut les sources et les webs privés.

L’exploration récursive du web à partir de ressources bien choisies est la méthode de base programmée dans les robots d’indexation des moteurs de recherche. En 2004, les moteurs de recherche indexent environ 4 milliards de ressources.

Le web profond, ou web invisible, est la partie du web qui n’est pas indexée et donc introuvable avec les moteurs de recherche généralistes. Une étude publiée en 2001 indiquait que la partie invisible du web représente plus de 99 % du web. Le web profond comprend notamment les ressources suivantes :
Ces dernières ressources proviennent généralement de bases de données et constituent la partie la plus importante du web profond.

L’exploration récursive n’est pas le seul moyen utilisé pour indexer le web et mesurer sa taille. L’autre solution consiste à mesurer l’infrastructure informatique connectée à Internet pour héberger des sites web. Au lieu de suivre des hyperliens, cette méthode consiste à utiliser les noms de domaine enregistrés dans le et essayer de se connecter à tous les serveurs web potentiels. C’est notamment la méthode utilisée par la société Netcraft, qui publie régulièrement les résultats de ses explorations, dont les mesures de popularité des serveurs HTTP. Cette mesure porte plus sur l’utilisation des technologies du web que sur le web lui-même. Elle permet notamment de trouver des sites publics qui ne sont pas liés au ".

Un site web mis en ligne sur un intranet est privé, car le public ne peut pas accéder à un intranet.

En outre, si l’on met en ligne un site web sur Internet en omettant de créer des liens depuis au moins une page existante du "", alors ce site constitue un web isolé. Il est virtuellement privé, car le public ne peut pas le découvrir en suivant des hyperliens.

Le web change constamment : les ressources ne cessent d’être créées, modifiées et supprimées. Il existe quelques initiatives d’archives du web dont le but est de permettre de retrouver ce que contenait un site à une date donnée. Le projet Internet Archive est l’un d’eux.

Les divers types de ressource du web ont des usages assez distincts :

Le document HTML est la principale ressource d’une page web, celle qui contient les hyperliens, qui contient et structure le texte, qui lie et dispose les ressources multimédias. Un document HTML contient uniquement du texte : le texte consulté, le texte en langage HTML plus d’éventuels autres langages de script ou de style.

La présentation de documents HTML est la principale fonctionnalité d’un navigateur web. HTML laisse au navigateur le soin d’exploiter au mieux les capacités de l’ordinateur pour présenter les ressources. Typiquement, la police de caractère, la longueur des lignes de texte, les couleurs, etc, doivent être adaptées au périphérique de sortie (écran, imprimante, etc).

Les éléments multimédias proviennent toujours de ressources indépendantes du document HTML. Les documents HTML contiennent des hyperliens pointant sur les ressources multimédias, qui peuvent donc être éparpillées sur Internet. Les éléments multimédias liés sont automatiquement transférés pour présenter une page web.

Seul l’usage des images et des petites animations est standardisé. Le support du son, de la vidéo, d’espaces tridimensionnels ou d’autres éléments multimédias repose encore sur des technologies non standardisées. De nombreux navigateurs web proposent la possibilité de greffer des logiciels (plugin) pour étendre leurs fonctionnalités, notamment le support de types de média non standard.

Les flux (audio, vidéo) nécessitent un protocole de communication au fonctionnement différent de HTTP. C’est une des raisons pour lesquelles ce type de ressource nécessite souvent un plugin et est mal intégré aux pages web.

Ce chapitre concerne les images intégrées aux pages web. L’usage du format de données JPEG est indiqué pour les images naturelles, principalement les photographies.

L’usage du format de données PNG est indiqué pour les images synthétiques (logos, éléments graphiques). Il est aussi indiqué pour les images naturelles, mais uniquement lorsque la qualité prime totalement sur la durée du transfert. L’usage du format de données GIF est indiqué pour les petites animations. Pour les images synthétiques, la popularité ancienne de GIF le fait souvent préférer à PNG. Cependant, GIF souffre de quelques désavantages, notamment la limitation du nombre de couleurs et un degré de compression généralement moindre. En outre une controverse a entouré l’usage de GIF de 1994 à 2004 car Unisys a fait valoir un brevet couvrant la méthode de compression. L’usage d’images de format de données XBM est obsolète.

Jusque dans les années 2000, la consultation de musique et vidéo demandait l’installation d’un programme "ad hoc" (un plugin) pour étendre les fonctionnalités du navigateur web. La très grande diffusion du plugin Flash Player a finalement rendu la consultation vidéo aussi simple que celle des images. Finalement, la cinquième version langage HTML (HTML 5) a intégré la vidéo.

Un langage de script permet d’écrire le texte d’un programme directement exécuté par un logiciel. Dans le cadre du web, un script est exécuté par un navigateur web et programme des actions répondant à l’usage que le visiteur fait de la page web consultée. Un script peut être intégré au document HTML ou provenir d’une ressource liée. Le premier langage de script du web fut JavaScript, développé par Netscape. Ensuite Microsoft a développé une variante concurrente sous le nom de JScript. Finalement, la norme ECMAScript a été proposée pour la syntaxe du langage, et les normes DOM pour l’interface avec les documents.

De technologie aux capacités d’actions très limitées à ses débuts, le langage JavaScript est devenu capable d’exécuter toutes les applications imaginables : traitement de texte, jeu vidéo, émulateur, etc.

Encore plus que pour la vidéo, le plugin Flash Player est devenu très largement utilisé pour la diffusion d’animations. Parfois, des sites entiers sont réalisés en Flash.

Le langage CSS a été développé pour gérer en détail la présentation des documents HTML. Le texte en langage CSS peut être intégré au document HTML ou provenir de ressources liées, les feuilles de style. Cette séparation permet une gestion séparée de l’information (contenue dans des documents HTML) et de sa présentation (contenue dans des feuilles de style). On parle aussi de « séparation du fond et de la forme ».

La gestion des autres types de ressource dépend des logiciels installés sur l’hôte client et de leurs réglages.

Lorsque le logiciel correspondant est disponible, les documents et images de tout type sont généralement automatiquement présentés, selon des modalités (fenêtrage, dialogues) dépendant du navigateur web et du logiciel gérant le type. Lorsque le type de la ressource n’est pas géré, il est généralement possible de la sauver dans un fichier local.

Pour gérer les ressources de systèmes différents du web comme le courrier électronique, les navigateurs font habituellement appel à des logiciels séparés. Si aucun logiciel ne gère un type de ressource, un simple message d’erreur l’indique.

Le web a été conçu pour être accessible avec les équipements informatiques les plus divers : station de travail, terminal informatique en mode texte, ordinateur personnel, téléphone portable, etc. Cette universalité d’accès dépend en premier lieu de l’universalité des protocoles Internet. En second lieu, elle dépend de la flexibilité de présentation des pages web, offerte par HTML. En outre, HTTP offre aux navigateurs la possibilité de négocier le type de chaque ressource. Enfin, CSS permet de proposer différentes présentations, sélectionnées pour leur adéquation avec l’équipement utilisé.

Le W3C a pour cela créé des normes dans le but de permettre l’indépendance des outils qui servent à créer du contenu avec ceux qui servent à le lire. On appelle cela l’interopérabilité.

L’accessibilité du web pour les individus handicapés est aussi l’objet d’attentions particulières comme la .

Les technologies du web n’imposent pas d’organisation entre les pages web, ni à fortiori entre les sites web. Toute page du web peut contenir des hyperliens vers toute autre ressource accessible d’Internet. L’établissement d’un hyperlien ne requiert absolument aucune action du côté de la ressource pointée. Il n’y a pas de registre centralisé d’hyperliens, de pages ou de sites. Le seul registre utilisé est celui du DNS ; c’est une base de données distribuée qui répertorie les hôtes, permet de traduire en adresse IP le nom de domaine contenu dans certains hyperliens et qui est utilisée par tous les systèmes accédant à Internet.

Cette conception décentralisée devait favoriser, et a favorisé, une augmentation rapide de la taille du web. Elle a aussi favorisé l’essor de sites spécialisés dans les informations sur les autres sites : les annuaires et les moteurs de recherche. Sans ces sites, la recherche d’information dans le web serait extrêmement laborieuse. La démarche inverse, le portail web, tente de concentrer un maximum d’informations et de services dans un seul site.

Une faiblesse de la décentralisation est le manque de suivi lorsqu’une ressource est déplacée ou supprimée : les hyperliens qui la pointaient se retrouvent "cassés". Et cela n’est visible qu’en suivant l’hyperlien, le résultat le plus courant étant le message d’erreur 404.

Le web repose sur les technologies d’Internet, notamment TCP/IP pour assurer le transfert des données, DNS pour convertir les noms d’hôte en adresses IP et MIME pour indiquer le type des données. Les standards de codage de caractères et les formats d’image numérique GIF et JPEG ont été développés indépendamment.

Trois technologies ont dû être développées pour le "" :

Ces premières technologies ont été normalisées comme les autres technologies d’Internet : en utilisant le processus des . Cela a donné le pour les URL, le pour HTML 2.0 et le pour HTTP/1.0.

Le (W3C) a été fondé en 1994 pour développer et promouvoir les nouveaux standards du web. Son rôle est notamment de veiller à l’universalité des nouvelles technologies. Des technologies ont également été développées par des entreprises privées.

Les principaux standards actuels sont :

Outre les protocoles de communication et formats de données échangés sur le Web, plusieurs techniques propres au Web sont mises en œuvre pour faire fonctionner les serveurs HTTP (ou serveurs web). Comme ces techniques ne concernent pas les logiciels client du Web, elles ne sont pas standardisées par le World Wide Web Consortium.

Le serveur HTTP est chargé d’assurer la communication entre le poste client et les ressources des sites. Il doit être capable d’aiguiller, sur le serveur, en fonction de la requête HTTP, reçue de la part du client, vers les bonnes ressources. Il peut s’agir de ressources statiques, situées sur le système de fichier du serveur ou un système de fichier déporté, comme NFS, ou des contenus dynamiques par différents biais. La fourniture des pages dynamiques est déléguée à des applications autonomes sur le serveur. Il doit également être capable de gérer les erreurs, si les ressources ne sont pas trouvées ou s’il y a une erreur dans la production de la ressource, en retournant le message adéquat au client. Le premier serveur HTTP fut CERN httpd et est rapidement tombé en désuétude. En 2014, les principaux serveurs Web utilisés sont Apache HTTP Server, serveur dominant du marché depuis des années, Nginx utilisé sur les sites à plus forte audience, mais aussi Microsoft IIS, Google Web Server ou encore Lighttpd.

Le standard "Common Gateway Interface" (CGI) est un protocole de communication inter-processus entre le serveur HTTP et des applications externes situées également du côté serveur. Ce standard est géré par l’IETF.

Le standard FastCGI remplace aujourd’hui majoritairement le CGI. Il permet de séparer plus efficacement le serveur HTTP des applications et de mieux contrôler le nombre d’instances des applications dynamiques du côté serveur.

Certains langages couramment utilisés avec les services HTTP, comme PHP, Java et Ruby, utilisent également des technologies particulières pour la communication avec le serveur.

Pour PHP, mod php pour Apache a souvent été utilisé à la place de CGI ; il tend aujourd’hui à être remplacé par le protocole fastCGI, il y a quelques années, via mod fastcgi et aujourd’hui davantage via mod fcgid, soit via php-fpm.

En Java, le projet Apache Jakarta a permis de réunir plusieurs outils. Dans cette architecture, le module Apache mod jk permet de faire la liaison avec le serveur d’application Tomcat qui exécute les Java Servlet et JavaServer Pages. En Ruby, Phusion Passenger, fonctionnant avec Apache et Nginx est un des principaux outils de communication entre le service HTTP et les applications.

En pratique, CGI et fastCGI permettent d’utiliser n’importe quel langage, cependant, on peut citer certains langages de script qui sont devenus les plus populaires, utilisant du CGI ou une autre technique pour communiquer avec le serveur.

Le langage de programmation a été développé pour générer les pages web. Il jouit d’une forte intégration avec le serveur HTTP et les langages HTML. C’est aujourd’hui le langage le plus utilisé côté serveur avec presque 70 % des sites en 2010. Le moteur ASP a été développé par Microsoft pour interpréter du langage de script dans le serveur IIS , il est le second langage le plus utilisé avec un peu moins de 30 % des sites en 2010.

Le langage Java, dont les pages sont servies par Apache Tomcat est ensuite très utilisé, particulièrement dans le domaine bancaire, la tendance est à y inclure du langage Ruby via le module jRuby. Java était utilisé sur un peu moins de 1 % des serveurs en 2010. Le langage Ruby, avec principalement la serveur d’application Ruby on Rails avec 0,5 % en 2010. Devant la popularité grandissante de JavaScript côté client, liée à Ajax puis HTML5, le côté serveur a suivi avec la plateforme logicielle Node.js, dédiée aux applications serveur.

Au début des CGI, Perl était très utilisé côté serveur, en raison de ses affinités avec les administrateurs système et réseau, et de sa dominance dans ce domaine.

Les bases de données sont également une partie importante de la génération des sites à contenu dynamique.

Les principaux types de base de données sont :











</doc>
<doc id="4780" url="https://fr.wikipedia.org/wiki?curid=4780" title="Mel">
Mel

Mel peut désigner :





</doc>
<doc id="4792" url="https://fr.wikipedia.org/wiki?curid=4792" title="Nombre de Fermat">
Nombre de Fermat

Un nombre de Fermat est un nombre qui peut s'écrire sous la forme 2 + 1, avec "n" entier naturel. Le "n"-ième nombre de Fermat, 2 + 1, est noté "F".

Ces nombres doivent leur nom à Pierre de Fermat, qui émit la conjecture que tous ces nombres étaient premiers. Cette conjecture se révéla fausse, "F" étant composé, de même que tous les suivants jusqu'à "F". On ne sait pas si les nombres à partir de "F" sont premiers ou composés. Les seuls nombres de Fermat premiers connus sont donc "F", "F", "F", "F" et "F".

Les nombres de Fermat disposent de propriétés intéressantes, en général issues de l'arithmétique modulaire. En particulier, le théorème de Gauss-Wantzel établit un lien entre ces nombres et la construction à la règle et au compas des polygones réguliers : un polygone régulier à "n" côtés peut être construit à la règle et au compas si et seulement si "n" est une puissance de 2, ou le produit d'une puissance de 2 et de nombres de Fermat premiers distincts. 

En 1640, dans une lettre adressée à Bernard Frénicle de Bessy, Pierre de Fermat énonce son petit théorème et commente : . Ce théorème lui permet d'étudier les nombres portant maintenant son nom. Dans cette même lettre, il émet la conjecture que ces nombres sont tous premiers mais reconnaît : . Cette hypothèse le fascine ; deux mois plus tard, dans une lettre à Marin Mersenne, il écrit : . Dans une lettre à Kenelm Digby, non datée mais envoyée par Digby à John Wallis le 16 juin 1658, Fermat donne encore sa conjecture comme non démontrée. Toutefois, dans une lettre de 1659 à Pierre de Carcavi, il s'exprime en des termes qui, selon certains auteurs, impliquent qu'il estime avoir trouvé une démonstration. Si Fermat a soumis cette conjecture à ses principaux correspondants, elle est par contre absente des "Arithmétiques" de Diophante rééditées en 1670, où son fils retranscrivit les quarante-sept autres conjectures qui furent plus tard prouvées. C'est la seule conjecture erronée de Fermat.

En 1732, le jeune Leonhard Euler, à qui Christian Goldbach avait signalé cette conjecture trois ans auparavant, la réfute : "F" est divisible par 641. Il ne dévoile la construction de sa preuve que quinze ans plus tard. Il y utilise une méthode similaire à celle qui avait permis à Fermat de factoriser les nombres de Mersenne "M" et "M".

Il est probable que les seuls nombres premiers de cette forme soient 3, 5, 17, 257 et , car Boklan et Conway ont prépublié en mai 2016 une analyse très fine estimant la probabilité d'un autre nombre premier à moins d'un sur un milliard.

La suite des nombres de Fermat possède plusieurs relations de récurrence. On peut citer par exemple si "n" est supérieur ou égal à 2 :
ou encore, avec des produits de nombres de Fermat :

On en déduit le théorème de Goldbach affirmant que :

Soit "D"("n", "b") le nombre de chiffres utilisés pour écrire "F" en base "b".
Les crochets désignent la fonction partie entière et log le logarithme de base "b".

)^2 + 1 \ = \ (F_{n-1} \ - \ 1)^2 + 1 \ = \ F_{n-1}^2 - 2F_{n-1} + 2 \ = \ F_{n-1}^2 - 2(F_{n-2}\ -\ 1)^2.</math>

La raison historique de l'étude des nombres de Fermat est la recherche de nombres premiers. Fermat connaissait déjà la proposition suivante :

Il existe deux entiers "a" impair et "b" tels que "k" = "a "2. En posant "c "= 2, on dispose alors des égalités suivantes :
formula_9 
qui montrent que 1 + "c" est un diviseur du nombre premier 1 + 2 et donc lui est égal, si bien que "k "= 2.

Fermat a conjecturé (erronément, comme on l'a vu) que la réciproque était vraie ; il a montré que les cinq nombres

Actuellement, on ne connaît que cinq nombres de Fermat premiers, ceux cités ci-dessus. 

On ignore encore s'il en existe d'autres, mais on sait que les nombres de Fermat "F", pour "n" entre 5 et 32, sont tous composés ; "F" est le plus petit nombre de Fermat dont on ne sait pas s'il est premier ou composé.

En 2013, le plus grand "nombre de Fermat" dont on savait qu'il est composé était : "F" ; l'un de ses diviseurs est le nombre premier de Proth 57×2 + 1.

Euler démontre le théorème :

Ceci lui permet de trouver rapidement :

Le cas général est un problème difficile du fait de la taille des entiers "F", même pour des valeurs relativement faibles de "n". En 2015, le plus grand nombre de Fermat dont on connaisse la factorisation complète est "F", dont le plus grand des cinq diviseurs premiers a 564 chiffres (la factorisation complète de "F", pour "n "entre 5 et 10, est, elle aussi, entièrement connue). En ce qui concerne "F", on sait qu'il est composé mais c'est, en 2015, le plus petit nombre de Fermat dont on ne connaisse pas la factorisation complète. Quant à "F", c'est, en 2015, le plus petit nombre de Fermat composé dont on ne connaisse aucun diviseur premier.

La série des inverses des nombres de Fermat est convergente et sa somme formula_14 est irrationnelle. En 2001, D. Duverney a même démontré qu'elle était transcendante. Ces résultats viennent de ce que cette somme est trop bien approchée par des rationnels.

Gauss et Wantzel ont établi un lien entre ces nombres et la construction à la règle et au compas des polygones réguliers : un polygone régulier à "n" côtés est constructible si et seulement si "n" est le produit d'une puissance de 2 (éventuellement égale à 2 = 1) et d'un nombre fini (éventuellement nul) de nombres de Fermat premiers distincts. 

Par exemple, le pentagone régulier est constructible à la règle et au compas puisque 5 est un nombre de Fermat premier ; de même, un polygone à 340 côtés est constructible à la règle et au compas puisque 340 = 2."F"."F".

Il est possible de généraliser une partie des résultats obtenus pour les nombres de Fermat.

Pour que soit premier, doit être pair et doit être une puissance de 2.

On appelle couramment « nombres de Fermat généralisés » les nombres de la forme formula_15 (avec ), mais Hans Riesel a donné aussi ce nom aux nombres de la forme formula_16. Le plus grand nombre premier de la forme formula_15 connu en 2017 est formula_18, un nombre de plus d'un million de chiffres.



</doc>
<doc id="4802" url="https://fr.wikipedia.org/wiki?curid=4802" title="Forum social européen">
Forum social européen

Le Forum social européen est l'une des déclinaisons continentales du Forum social mondial (FSM), à côté du Forum des Amériques, du Forum social asiatique, etc. La première organisation du FSE eut lieu en 2002 à Florence.

Un Forum social consiste en un espace de rencontres et d'échanges conçu pour permettre des réflexions de fond, des débats d'idées, la mise à jour de propositions et d'alternatives, l'échange d'expériences et de projets d'actions, entre les différentes entités et mouvements de la société civile se disant « engagés dans la construction d'un autre monde possible centré autour de la personne humaine, et non sur le profit économique ». 


Le Forum social mondial a été créé à l'initiative de 8 organisations (MST, CUT, ABONG, I-Base, etc) qui assument toujours le rôle de secrétariat international en lien, après le FSM de Mumbai en 2004, avec le comité organisateur indien. Dès 2001 les organisateurs brésiliens prenaient l'initiative de constituer le Comité International du FSM qui s'élargit par cooptation à diverses organisations et ONG (plus de 100 en 2006).

Le Forum social européen est animé différemment. Dès la première réunion de préparation du FSE de Florence, à Bruxelles au printemps 2002, les différents mouvements européens décidaient la mise en place d'une « Assemblée Européenne de Préparation » qui assurerait l'organisation des différents FSE. Cette assemblée est ouverte à toute association, collectif, syndicat, ONG, réseau ou organisation qui souhaite s'y investir et se réunit au moins 3 fois par an, à chaque fois dans une ville différente dans le souci de permettre la participation la plus large des mouvements issus des différentes parties de l'Europe.
Ce choix d'une AEP ouverte s'explique par des raisons pratiques : les organisations brésiliennes qui ont inventé le « concept » de Forum social pouvaient décider qui devaient en assumer la responsabilité, ce qui était impossible pour des organisations européennes qui auraient eu de grosses difficultés à déterminer les critères de participation à une structure fermée. Mais cette décision renvoie également à la volonté de mettre en œuvre de façon pratique le principe d'horizontalité.

Le slogan de ce forum était « contre la guerre, le racisme et le néo-libéralisme », en référence spécifique au plan de George W. Bush pour l'Irak. 

Avant même son commencement, le FSE de Florence réveilla des polémiques, surtout dans la perspective que ne se reproduisent des événements comme lors du sommet du G8 à Gênes l'année précédente. 

Finalement le FSE, avec ses délégués qui occupèrent la Forteresse de Basso (lieu historique) et d'autres centres de conférences, ne provoqua aucun incident et se termina par une grande manifestation contre la guerre, de 1 million de personnes selon les organisateurs. 
Parmi les thèmes abordés : immigration, constitution de l'union européenne, la taxe Tobin et surtout les débats sur la paix et le pacifisme. Ce thème compta parmi ses orateurs Gino Strada, leader du mouvement pacifiste en Italie et président de "Emergency", une association italienne qui aide les victimes civiles des conflits armés. 
Des grandes ONG telle que Amnesty International joignirent le FSE ainsi que des organisations de citoyens comme ATTAC. 

À la fin de la rencontre même Romano Prodi, à l'époque président de la Commission Européenne, montra de la sympathie pour la modération du mouvement et ses thèmes pacifistes.

Plus de inscrits au forum 2003 dont des ONG, des organisations syndicales, anticapitalistes. Quelques personnalités se présentèrent aussi au forum, dont José Bové, Laurent Fabius, François Hollande. Les groupes politiques les plus visibles furent la Ligue communiste révolutionnaire (LCR) et le Parti communiste français (PCF). Le parti socialiste était présent "(voir controverse)"
À côté des participants officiels il y a aussi eu beaucoup de participants non inscrits.
Signalons aussi qu'un forum des femmes eut lieu les jours précédents le FSE, pour réagir à la sous-représentativité féminine lors du premier FSE. Ce forum des femmes compta participantes.

Les langues de travail au FSE 2003 sont le français, l'anglais, l'allemand, l'italien et l'espagnol. Les orateurs peuvent s'exprimer dans la langue de leur choix et l'interprétation est assurée par "Babels", réseau international d'interprètes et de traducteurs volontaires dont l'objet est de couvrir les besoins en interprétation des forums sociaux et autres événements internationaux. Les bénévoles ne reçoivent que le remboursement de leurs frais de transport. Cette gestion babélienne du plurilinguisme a un coût qui estimé à euros au forum de Paris. Les organisateurs de ce forum ont considéré cette solution au multilinguisme si pertinente qu'ils ont décidé de faire défiler les interprètes de Babels en tête du cortège de leur manifestation le 15 novembre 2003 à Paris.

Toujours à Paris, il y a eu beaucoup de critiques sur la participation du Parti socialiste français, qui, affirment certains altermondialistes, a participé de la « globalisation capitaliste » durant les années précédentes. Quelques groupes anarchistes avaient organisé un événement « concurrent » dans la ville, le "Forum Social libertaire". 
Pendant la manifestation du 15 novembre, de violents affrontements ont eu lieu entre les militants du Parti socialiste et les autonomes.

Plus de personnes de 70 pays différents assistèrent aux réunions, la plupart organisées à l'Alexandra Palace.
Parmi les participants, des partis politiques comme le "Green Party", (parti vert), des anarchistes, des socialistes, des ONG. 
Les membres du Socialist Workers Party (UK) (Parti socialiste des ouvriers) et le Globalise Resistance formèrent le noyau organisateur, ainsi que Socialist Action (UK) (action socialiste) avec la campagne de désarmement nucléaire. Pour l'organisation du FSE deux visions se sont affrontées : les organisateurs anglais privilégiait la centralisation de l'évènement (avec une manifestation contre la guerre) en argumentant que ça lui donnerait plus d'impact, d'autres composantes du mouvement altermondialiste (par exemple le Forum local de Londres) défendaient que le FSE ne devait pas être considéré comme un événement mais plutôt comme une partie de processus, reprochèrent une approche et une organisation trop hiérarchisée et proposèrent par conséquent, « horizontalement », des événements en marge du FSE, plus spontanés dans leur approche.




Le cinquième Forum social européen s'est déroulé à Malmö en Suède, du 17 au 21 septembre 2008. Environ participants étaient attendus.




</doc>
<doc id="4803" url="https://fr.wikipedia.org/wiki?curid=4803" title="Florence">
Florence

Florence (en italien : "Firenze", prononcé ) est la huitième ville d'Italie par sa population, capitale de la région de Toscane et siège de la ville métropolitaine de Florence.

Berceau de la Renaissance en Italie, capitale du royaume d'Italie entre 1865 et 1870, inscrit sur la liste du patrimoine mondial de l'UNESCO au titre du Centre historique de Florence, la ville présente une richesse artistique exceptionnelle (églises, musées, palais), et est devenue un grand centre touristique.

<mapframe zoom=10 latitude =43.771667 longitude =11.253611 align=left width=250 height=250 text="Carte interactive de Florence"> { "type": "ExternalData", "service": "geoshape", "ids": "Q2044", "properties": { "fill": "#fc3", "stroke": "#ac6600" } } </mapframe>

Florence est située à d'altitude, dans la partie orientale d'une plaine, appelée "bassin de Florence", au pied de l'Apennin du nord, au nord et à l'est de la ville. Elle est traversée par l'Arno, fleuve naissant dans les Apennins et se jetant dans la mer Tyrrhénienne, distante de .

Le climat de Florence est de type méditerranéen. Il est influencé par la mer Tyrrhénienne, les Apennins et les collines toscanes, les trois distants de moins de , et assurant une certaine humidité même en été. L'hiver est par contre plus froid que sur la côte, marqué par la continentalité.

Florence a été fondée sous le nom latin de "Florentia" pendant l'époque romaine, en 59 av. J.-C., près du fleuve Arno. Elle n'a été qu'une simple bourgade jusqu'au , début de son essor économique et artistique qui dura jusqu'au .

Du , Florence connaît de profonds bouleversements politiques et sociaux avec l'essor des riches familles de marchands groupées au sein du "popolo", et le conflit entre les guelfes et gibelins qui partage l'Italie et Florence en deux. Ces deux processus accompagnent le développement de la commune qui, comme dans les autres villes de l'Italie septentrionale, désigne l'émergence de gouvernements autonomes qui ont acquis leur souveraineté après une lutte féroce débouchant sur la paix de Constance, octroyée par l'empereur Frédéric en 1183. Les communes italiennes y ont acquis des droits souverains qui en faisaient de véritables cités-États.

La commune florentine, qu'on connaît alors sous le nom de Fiorenza, naît environ un siècle après celle de Pise ; elle est attestée dès 1081. Faut-il voir là l'absence d'une noblesse urbaine qui créait les premières communes partout en Italie ? En tout cas, le "popolo" des marchands cherche rapidement à faire partie des instances de la commune : le conseil exécutif du consulat, puis du podestat, une assemblée délibérative qui aura plusieurs noms. Florence connaît le combat des factions, avec les gibelins qui triomphent dans les années 1240, avec le vicaire impérial Frédéric d'Antioche, bâtard de l'empereur Frédéric II. Les guelfes sont au pouvoir en 1250 puis en 1266, quand ils prennent Florence avec l'appui de Charles d'Anjou, frère du roi de France appelé comme roi de Naples par le pape. Ces guelfes sont soutenus par le "popolo" : ils créent la charge de capitaine du peuple en 1250, puis élaborent une nouvelle forme institutionnelle, la seigneurie ("Signoria"), en 1282 : un conseil de prieurs, appartenant aux corporations des marchands, les fameux majeurs des "Arti" (laine, draps, changeurs, juges et notaires, etc.), auxquels sont juxtaposés un « gonfalonnier de justice » "(Gonfaloniere di Giustizia)" et des gonfalons (étendards) de quartiers, et ce alors que le podestat et le capitaine du peuple continuent d'exister. 

Le florin, principale monnaie du Moyen Âge, est créé en 1252 par la corporation des changeurs et banquiers ("Arte del Cambio") de Florence, l'une des cinq corporations majeures et contribue au succès de la ville, succès qui l'impose en Europe.

La faction guelfe se structure, reçoit même, pour les gérer, les biens des qui ont fui la ville. Dans les années 1290, les lois anti-magnatices entrent en vigueur : c'est la revanche des corporations de marchands qui interdisent aux nobles l'accès aux charges et limitent la taille des tours qu'ils avaient érigées.

Au début du , Florence expérimente, tout comme les autres villes d'Italie (ex. les Della Scala à Vérone à partir de 1273), les seigneuries personnelles : Charles de Calabre la gouverne en 1323, suivi par le duc d'Athènes Gautier VI de Brienne en 1343. Florence connaît une véritable crise au milieu du : révolte du peuple, faillite des Peruzzi (grande banque) en 1343, peste noire qui fait disparaître la moitié de la population de la ville en 1348.

La ville est ensuite dominée par différents clans qui se disputent le pouvoir. En 1434, ce sont les Médicis qui deviennent maîtres de la ville. Puis, à leur chute, nombre de grandes familles s'exilent en France et y font fortune. Les Gadagne quittent Florence vers la fin du , et détiennent la première place des banques lyonnaises, tandis qu'Albisse Del Bene, un autre banquier florentin, contrôle la levée des impôts dans toutes les régions de France. 

En 1569, Florence devient la capitale du grand-duché de Toscane. 

Florence a été le chef-lieu de l'Arno, département français créé le , à la suite de l'annexion du royaume d'Étrurie à l'Empire français par les troupes napoléoniennes. La ville connaît ensuite une période de lent déclin jusqu'en 1865, date à laquelle elle devient capitale du royaume d'Italie. Elle perd ce statut en 1870, au profit de Rome.

Le , l'Arno inonde une grande partie du centre-ville, endommageant de nombreux chefs-d'œuvre. Un grand mouvement de solidarité internationale naît à la suite de cet évènement et mobilise des milliers de volontaires, surnommés "Les anges de la boue".

Les 30 et 31 mars 2017, le premier G7 de la Culture, organisé sur initiative de l'Italie, s'est tenu à Florence.

Les premières académies d'Europe ont été fondées à Florence :


L'université de Florence a été fondée en 1321. C'est l'une des plus anciennes et prestigieuses universités italiennes, avec et . 

L'Institut universitaire européen de Florence est basé à Fiesole depuis son ouverture en 1976. 

Plusieurs universités étrangères ont également une représentation ou une antenne à Florence, notamment pour les études concernant la Renaissance, l'histoire de l'art ou les activités artistiques et créatives. C'est le cas de l'université Harvard, à la Villa I Tatti, de la New York University à la Villa La Pietra ou encore de la California State University située sur la Via Leopardi. 

La ville abrite également le célèbre Institut d'art de Florence, ainsi que l'Institut français de Florence, le plus ancien de tous les Instituts français fondé en 1907, et le British Institute of Florence, institut culturel anglo-italien fondé en 1917. 

A noter aussi que depuis 2009, le palais Portinari Salviati héberge les Archives Historiques de l'Union Européenne.


Seul le "Ponte Vecchio" a échappé aux destructions de la Seconde Guerre mondiale (bombardements et minages allemands). Les autres ont tous été reconstruits depuis, plus ou moins à l'identique.






Bâtiment ouvert, la "loggia" accueille beaucoup des activités des Florentins, marchés couverts ou lieux d'exposition ouverts jour et nuit :















Le lys rouge ("il Giglio"), distinct des lys jaunes de l'Emblème des Rois de France, symbolise la cité de Florence.
Il est nommé « fleur de lys florencée » et est semblable au meuble présent dans les armes de Lille. Ce symbole figure sur l'ancienne monnaie de la cité-État, le florin ("fiorino" à rapprocher tant de "fiore" — fleur — que de "Fiorentia" — ancien nom toscan de la cité), et lui donne son surnom littéraire, la Cité au lys rouge.

Florence a une équipe de football célèbre, la "Fiorentina", surnommée la Viola, d'après la couleur violet de son maillot. Fondé en 1926, le club joue au stade Artemio Franchi (46 000 places), a gagné une Coupe d'Europe en 1961 et participé à plusieurs finales européennes. Le "Nelson Mandela Forum" est la grande salle omnisports de la ville (8200 places).

Florence a une économie diversifiée active surtout dans le secteur tertiaire. Important centre ferroviaire et routier, la ville est aussi le siège d’une activité industrielle mécanique (comme Selex Galileo, Beta Motor ou la Nuovo Pignone), chimique, pharmaceutique (le Groupe Menarini par exemple), le travail du cuir (Braccialini), de l’habillement (souvent dans le secteur du luxe, comme Roberto Cavalli, Gucci, Ermanno Scervino et Ferragamo), du mobilier. Il y a de nombreuses entreprises typographiques et éditoriales ainsi qu’un artisanat florentin, d’antique réputation, surtout dans le secteur mobilier (ébénisterie), de la porcelaine (Richard Ginori), de la carte décorée, du bronze et de l’orfèvrerie.

Une ressource importante de la ville est le tourisme, avec à peu près d’hôtes et hors hôtel (campings, locations de chambre ou gîtes). Le nombre annuel de nuitées s’élève à 10 millions, un tiers des touristes sont italiens, 20 % américains, 13 % allemands, 8 % japonais, 7,8 % anglais, 5,7 % français et 5 % espagnols. La Galerie des Offices reçoit , tandis que la Galleria dell'Accademia reçoit . La fréquentation de Florence liée aux congrès et aux foires s’est largement développée grâce au réaménagement au cours des années 1990 du centre des congrès.

Dans le secteur du service, le secteur bancaire (Banca Toscana, Banca CR Firenze) et des assurances (La Fondiaria) est important.

La ville est un centre important du commerce, avec une activité de haute spécialisation et très diversifiée. Le centre de la ville, outre l’activité liée au tourisme et à l’accueil, est l’hôte de nombreuses activités traditionnelles (travail du cuir), magasins de produits artisanaux et produits typiques. Les dernières années du ont vu l’érosion de l’activité artisanale au profit des grandes chaines internationales actives surtout dans le secteur de la mode, qui par des magasins associés aux marques les plus importantes du secteur, ont créé (surtout via de’ Tornabuoni, via della Vigna Nuova, via degli Strozzi) un quartier de commerce de luxe.

La grande distribution est active dans la zone externe au centre urbain et près de l’aéroport de Florence-Peretola, Osmannoro concentre de nombreuses activités industrielles.

Florence a une longue tradition de la mode. L'industrie de la haute couture est importante: la ville s'enorgueillit de maisons de mode célèbres telles Gucci, Salvatore Ferragamo, Enrico Coveri, Roberto Cavalli, Emilio Pucci, Patrizia Peppe, Conte of Florence, et beaucoup d'autres. La majorité de ces enseignes sont concentrées dans le secteur des commerces de luxe des Via Tornabuoni et Via della Vigna Nuova. C'est à Florence que s'est tenue en 1951 le premier défilé de haute couture italien, via dei Serragli. La ville abrite depuis cette date une série d'évènements de mode prestigieux : "Pitti Imagine," qui se tient chaque année dans différents lieux de la cité. Parmi eux, le "Pitti Uomo," né en 1972, est un salon international de mode masculine qui se tient deux fois par an: c'est l'un des plus importants rendez vous du monde. Tous les ans également se tient le "Percorsi di Moda a Firenze," une série de visites guidées permettant de visiter les lieux de création et les produits liés à la mode à travers la ville. Enfin, Florence possède une importante école de mode, le "Polimoda Istituto Internazionale Fashion Design & Marketing," ainsi que "l'Accademia Italiana," située au palais Pitti, école de mode, graphique et design. 

Florence abrite l'unique musée italien dédié à la mode, la Galerie des Costumes (dans les jardins du palais Pitti), et depuis 1995, s'est installé le musée Salvatore Ferragamo dans le palais Spini Ferroni. La ville comprend également le musée Gucci, Piazza della Signoria à côté du Palazzo Vecchio. Il retrace l'histoire de la célèbre maison de luxe de sa création à nos jours.

La cuisine florentine est caractérisée par quatre éléments fondamentaux : 
Parmi les plats typiquement florentins :

Florence se trouve sur l'axe principal de la liaison nord-sud de l'Italie et elle est raccordée au principal réseau d'infrastructure et de transport.

En ville, il existe des pistes cyclables mais leur entretien laisse à désirer ainsi que les liaisons entre les différentes pistes. Utiliser le vélo en dehors du centre historique s’avère assez dangereux à cause de l’importance du trafic.

À Florence, il existe un service de covoiturage.

Les transports urbains à Florence sont constitués de lignes d’autobus et minibus (utilisés dans le centre de la ville) administrés par l’ATAF qui gère aussi deux lignes touristiques avec des autobus à double étage découverts.

Il existe aussi un réseau d’autobus long parcours, les principales agences sont SITA, Copit, CAP et Lazzi.

Pour lutter contre l’engorgement chronique des rues à cause du trafic, la ville s'est lancée dans la construction d'un réseau moderne de tramway. Cependant un certain nombre de citoyens opposés au projet ont réclamé un référendum qui s’est tenu le . La majorité des votants a toutefois approuvé le projet. Une première ligne reliant la gare centrale à Scandicci a été inaugurée le . Elle a 14 stations sur une longueur de . La ville espère accueillir 9,8 millions de passagers par an à bord de cette première ligne. Après appel d'offre, l'exploitation et la maintenance ont été attribuées pour une durée de 30 ans à "RATP Dev", filiale de la RATP. "RATP Dev" travaille également à la conception de deux autres nouvelles lignes de tramway (lignes 2 et 3).

Le centre historique de la ville est fermé au trafic à l’exception des autobus, des taxis et des résidents en possession d’un permis. Cette zone est appelée « ZTL » (Zone à Trafic Limité) et est divisée en cinq secteurs. L’entrée est protégée par une porte télématique. L’interdiction d’accès de la ZTL est de 7 h 30 à 19 h 30 les jours fériés et le samedi jusqu’à 18 h. L’été, l’interdiction est étendue la nuit de 22 h 30 à 3 h les jeudis, vendredis et samedis. La traversée est possible aux véhicules à traction animale, aux bicyclettes, aux cyclomoteurs et aux motos. À l'intérieur du centre historique, des zones piétonnes sont strictement réservées aux piétons et aux cyclistes.

En dehors du centre historique, "ZCS" (Zone à stationnement contrôlé) se compose de 14 zones correspondant aux autres parties de la ville. La ZCS est gérée par la société "Servizi alla Strada S.P.A." qui s’occupe de contrôler les parkings de la ville. Les résidents peuvent demander à la commune un permis afin de pouvoir stationner leur véhicule dans leur zone résidentielle ; en dehors de la zone, le stationnement est payant pour les Florentins comme pour les étrangers.

La ville est desservie par deux autoroutes, l'A1 et l'A11, qui la relient à la côte toscane et au nord et au sud de l'Italie. De plus, d'autres routes nationales et régionales l'unissent au reste de la Toscane et à l'Émilie-Romagne, la ville est reliée par deux grandes voies respectivement à Sienne et au Valdarno inférieur vers Pise et Livourne.

Important nœud routier, Florence est le point de départ et de passage de plusieurs routes nationales dont la Via Cassia qui conduit à Rome et la Via della Futa qui rejoint Bologne.

Trenitalia est l'unique gestionnaire des transports ferroviaires sur Florence. Les gares sur le territoire communal sont :

Avec l'entrée en fonction du train à grande vitesse TAV, la ville est desservie selon l'axe principal Turin-Milan-Naples. Il existe un projet de liaison souterraine qui atteindra la future gare de Florence Belfiore afin d'éviter de desservir la gare de Santa-Maria-Novella qui est en cul-de-sac.

Florence dispose d'un aéroport qui se trouve sur le territoire de la commune de Sesto Fiorentino à au nord-ouest du centre ville. Il existe des liaisons avec le plus grand aéroport toscan, l'aéroport Galileo Galilei de Pise.

 

Galluzzo, Settignano, Le Piagge, Brozzi, Gavinana, Isolotto, Trespiano, Legnaia, Soffiano, Ponte a Greve, Rovezzano, Novoli, Careggi, Peretola, Sollicciano, Rifredi, Borgo San Frediano, Oltrarno.

La municipalité de Florence est divisée en 5 quartiers (Quarteri) administratifs:

Bagno a Ripoli, Campi Bisenzio, Fiesole, Impruneta, Scandicci, Sesto Fiorentino, Prato.

Comme beaucoup d'autres villes en Italie, la population de retraités est bien supérieure à celles de jeunes (moins de 14 ans) et en constant vieillissement. Les chiffres suivants sont de mai 2006 (font:Ufficio comunale di statistica).

La population de Florence est à peu près à 91,5 % d’origine italienne. La population étrangère comprend dont :

La ville de Florence est jumelée avec :
La ville a signé des pactes d'amitié avec :

Florence entretient aussi un pacte de fraternité avec :




</doc>
<doc id="4811" url="https://fr.wikipedia.org/wiki?curid=4811" title="Commission économique pour l'Amérique latine et les Caraïbes">
Commission économique pour l'Amérique latine et les Caraïbes

La Commission économique pour l'Amérique latine et les Caraïbes (CEPALC) est une commission régionale de l'ONU fondée en 1948 par résolution du Conseil Économique et Social (alors nommée CEPAL, elle sera rebaptisée CEPALC en 1984). En espagnol : Cepal ou Comisión Económica para América Latina; en anglais UNECLAC "United Nations Economic Commission for Latin America and the Caribbean". La Cépalc publie des statistiques économiques de référence sur l'Amérique latine.

Son siège se trouve à Santiago (Chili). Deux sièges sous-régionaux se trouvent à Mexico pour l'Amérique centrale et à Port-d'Espagne pour les Caraïbes. Des bureaux nationaux sont présents à Buenos Aires, Brasilia, Montevideo et Bogota, avec un bureau de liaison à Washington DC.

La CEPAL est à l'origine des stratégies de développement d'industrialisation par substitution aux importations (ISI) dans les pays d'Amérique latine au cours des années 1960. Prônant un protectionnisme sélectif sur les importations du reste du monde, elle a favorisé l'industrialisation de ces pays (le Brésil voit sa part de l'industrie dans le PIB passer de 10% en 1949 à 40% en 1975, bien que ce pays ait entamé le processus d'ISI avant la création de la CEPAL) notamment par l'intégration régionale (Communauté andine des Nations en 1969), pour compenser les effets de réduction de la taille des marchés dus au protectionnisme. Pourtant, du fait de la place trop importante donnée à l'industrie dans ces modèles de développement, ceux-ci se solderont généralement par des échecs, les unités de production étant surdimensionnées par rapport aux pays. Ces stratégies ont été développées à la suite des hypothèses développementales de Raúl Prebish, ainsi que Celso Furtado et Suzan George. Ces derniers promouvaient un nouvel ordre international avec de meilleures distributions des ressources (grâce à la nationalisation) dans le but de diminuer la dette extérieure de ces pays en développement. 


Le , la Martinique et la Guadeloupe sont admises en tant qu'états associées de la CEPALC lors de la de la CEPALC à El Salvador. Cette adhésion a été permise grâce à l'avis favorable émis par Laurent Fabius, le ministre des affaires étrangères de la république française quelques semaines plus tôt.

Annuellement elle publie l'Annuaire statistique de l'Amérique Latine et des Caraïbes , en anglais et en espagnol, contenant les principaux indicateurs macroéconomiques de la région.

Les autres publications de référence sont (liens en anglais et en espagnol):

La CEPAL publie également des études de conjoncture économique, des rapports et des revues scientifiques.

Liste des revues et cahiers : 

La plupart des ouvrages d'économie publiés par la CEPAL (en espagnol, portugais ou anglais) sont mis à disposition dans un catalogue contenant des dizaines de références scientifiques .



</doc>
<doc id="4812" url="https://fr.wikipedia.org/wiki?curid=4812" title="Amérique latine">
Amérique latine

L’Amérique latine ( ou "Latinoamérica" ; ) est généralement définie comme la partie de l'Amérique dans laquelle les pays ont pour langue officielle des langues romanes, c'est-à-dire dérivées du latin (espagnol, portugais et français). L'Amérique latine englobe ainsi l'Amérique hispanique, certaines îles des Caraïbes, ainsi que la quasi-totalité de l'Amérique du Sud et de l'Amérique centrale. Bien que le français soit une langue romane, certains géographes ne retiennent pas les teritoires ayant comme langue officielle le français pour définir l'Amérique latine.

À l'instar de l'Amérique du Nord, la population d'Amérique latine est ethniquement diversifiée. La religion prédominante en Amérique latine est le christianisme, principalement le catholicisme. L'Amérique latine a une superficie d'environ km², soit plus de 3,9 % de la surface de la Terre, et 13,5 % de sa surface émergée. En 2012, sa population est estimée à plus de 600 millions d'habitants.

L'expression « Amérique latine » a été utilisée pour la première fois par le poète colombien José María Torres Caicedo en 1856 et par le socialiste chilien Francisco Bilbao, tous deux proches de Lamennais.

Le concept d'une Amérique catholique et latine s’opposant à une Amérique anglo-saxonne et protestante a été repris par l’entourage de Napoléon III. En 1861, c’est au nom de la défense de ces pays « latins », considérés comme culturellement proches de la France, que l'empereur envoie une expédition au Mexique dans un contexte de panlatinisme. 

Le développement de l'expression « Amérique latine » est donc lié aux visées coloniales de Napoléon III dans cette région, aux alentours de 1860, lors de l'aventure mexicaine. C'est le Français Michel Chevalier qui mit alors en avant un concept de « panlatinité » destiné à promouvoir les ambitions françaises en opposant les régions de langue latine (espagnol, portugais, et français) dans les Amériques, aux régions de langue anglaise. Cette sorte d'« ingérence » est toujours combattue, au nom des droits de la mère patrie, par Madrid où le concept d'Amérique latine n'a toujours pas droit de cité, mais où prévaut au contraire le concept d'hispanité. Les Espagnols ont toujours préféré les expressions "Hispano América" ou encore "Ibero América" pour la désigner. Plus récemment, les géographes utilisent l'expression « Extrême-Occident » pour parler de l'Amérique latine. L'Académie française définit l’Amérique latine comme « l'ensemble des pays d'Amérique anciennement colonisés par l'Espagne et par le Portugal ».

La définition la plus fréquente de l'Amérique latine retient en pratique les dix-huit pays indépendants de l'« Amérique hispanique », dont la langue officielle principale est l'espagnol, en y ajoutant le Brésil, dont la langue officielle est le portugais et de Haïti, le français, son caractère de langue latine, joue de rôle pour définir l'appartenance à l'Amérique latine. L'expression peut aujourd'hui désigner :

Le Québec, même si l'on y parle une langue latine, le français, n'est pas considéré comme appartenant à l'Amérique latine, du fait de son intégration depuis longtemps au Canada, à l'instar des anciens territoires espagnols que sont la Floride, ou la Californie. 


 
L'expression « Amérique latine » peut être diversement comprise, même si la définition du périmètre correspondant n'est pas toujours précise, on considère assez généralement que l'Amérique latine est constituée de 20 pays, listés ci-dessous, sans y inclure systématiquement Porto Rico, « associé » aux États-Unis.

Haïti, pays indépendant qui parle une langue latine. L’effondrement de l’Empire espagnol après 1808 donne naissance au même moment, après une quinzaine d'années de guerres de libération, à de nouveaux pays qui font et la moderniser, mais leurs destins divergent ensuite, le rêve de Simón Bolívar d'une grande fédération ne se réalisant pas. 

D'autre part, l’examen attentif de l’histoire et de la culture de l’Amérique latine montre que cette dernière ne constitue pas un ensemble culturellement homogène : l’expression « Amérique latine » efface le passé précolombien de la région, pour ne s'attacher qu'à la langue des colonisateurs. Une définition souvent rencontrée, fait ainsi de l'Amérique latine l'addition des 18 pays indépendants de langue espagnole d'un côté (« l'Amérique espagnole »), du Brésil (de langue portugaise) et Haïti de l'autre.

Cette définition de l'Amérique latine est cohérente avec le concept du « partage du monde » signé en 1494 par l'Espagne (alors Castille) et le Portugal lors du Traité de Tordesillas, établi pour définir le partage du Nouveau Monde — considéré comme "terra nullius" — entre les deux puissances coloniales émergentes, avec pour ligne de partage un méridien nord-sud localisé à 370 lieues (1770 km) à l'ouest des îles du Cap-Vert, méridien qui se situerait aujourd'hui à 46° 37' ouest.

La situation de l'économie des pays d'Amérique latine est très variable d'un pays à l'autre, tant au niveau du PIB, des partenaires commerciaux, de l'inflation, de la pauvreté/richesse, des produits exportés, etc.

Les quatre plus grandes économies en termes de PIB sont le Brésil, le Mexique, l'Argentine et la Colombie 

L’Amérique latine a enregistré des progrès considérables au cours des dernières années. Sur le plan politique, après les épisodes de dictature militaire des années 1970 et 1980, la transition démocratique s’est faite dans pratiquement tous les pays. Ce continent, très riche en ressources naturelles renouvelables et non renouvelables, a connu une croissance vigoureuse pendant les années 2000. De nombreux pays de la région ont en outre tenté de conjuguer le dynamisme économique avec une plus grande équité à travers de nouvelles politiques sociales. Les pays d’Amérique latine n’ont pas échappé aux effets de la crise économique globale, mais ils y font face avec plus de succès que lors de crises précédentes, grâce à l’amélioration de la gestion macroéconomique et à la réduction de la vulnérabilité financière. 
Cependant, des défis importants demeurent pour la région. Dans un environnement international instable, elle devra surmonter des obstacles tels que la faible productivité, le niveau relativement bas de l’épargne et de l’investissement domestiques, les fortes
inégalités spatiales et sociales ou l’insuffisance des efforts sur le plan de la recherche et de l’innovation, pour s’installer dans un sentier de croissance et développement soutenable.

Dans ce domaine, l'Amérique latine présente de forts contrastes. La région a connu un recul significatif de la pauvreté (le taux le plus bas depuis trente ans)et une explosion de la classe moyenne (la proportion de Latino-Américains vivant avec moins de quatre dollars par jour est passée de plus de 40 % en 2000 à moins de 30 % en 2010). Malgré ces résultats positifs, un tiers de la population de la région est encore menacée par la pauvreté. Certains pays demeurent plus fragiles économiquement alors que d'autres connaissent un développement très rapide qui s'accompagne d'une croissance économique particulièrement dynamique, comme le Pérou, ou près de 70 milliards de dollars d'investissements sont attendus d'ici à 2017. L'exemple de la Colombie est encore plus frappant : 6,4 % de croissance en moyenne en 2013, soit l'un des plus importants taux de croissance du monde, derrière la Chine, le seul pays à avoir fait mieux. L'Amérique du Sud est, avec le Moyen-Orient, le continent qui possède les plus grandes réserves de pétrole de la planète. Ainsi, le Venezuela est la première puissance pétrolière au monde, avec des réserves absolues estimées à 297 milliards de barils, soit plus que n'importe quel autre pays. Les poids lourds de l'économie, le Brésil et le Mexique, font tous les deux figure de géants sur la scène internationale : en effet, le Brésil est la économique mondiale accompagné dans le top 15 du Mexique (). Mesurée en parité de pouvoir d'achat, l'économie mexicaine a récemment devancée l'Italie pour occuper désormais la .

Les pays d'Amérique latine sont peuplés de façon bien différentes, en particulier en termes d'importance de la population de chaque pays. Ainsi le Brésil a 206 millions d'habitants et le Mexique compte un peu plus de 119 millions d'habitants, alors que l'Uruguay compte moins de 4 millions d'habitants. Deux pays possèdent plus de 100 millions d'habitants, deux plus de 40 millions, quatre autres entre 10 millions et 16 millions, cinq pays comptent entre 5 et 10 millions d'habitants.

La population d'Amérique latine est également remarquable par la diversité de ses origines ethniques, car le continent - où l'homme est apparu beaucoup plus tardivement que sur la plupart des autres continents - a vu arriver successivement plusieurs vagues de peuplement d'origines diverses.
La population amérindienne est issue de peuplement pré-coloniaux. Ces populations indiennes, originaires d'Asie, ont été massivement exterminées au moment de la conquête espagnole, en particulier au Mexique, au contact des maladies venues d'Europe (telles que la variole), auxquelles elles n'ont pu opposer aucune défense naturelle. Indépendamment de la maladie, la conquête elle-même et l'exploitation de la population par les Espagnols et les Portugais ont décimé la population : on estime ainsi que l'exploitation des mines de Potosi a causé la mort de 13 millions d'Indiens en 50 ans. Au début du , la population indienne est la majorité seulement en Bolivie (55 % de la population). Elle est très importante au Pérou, avec 45 % de la population, Guatemala 40 %, et en Équateur (25 %), mais est très minoritaire au Mexique (7 %, soit de l'ordre de 7 à 8 millions d'habitants) et en le reste d'Amérique centrale, ou encore plus petite au Venezuela (5 % de la population) et en Colombie (3 % de la population).

Elle est au départ essentiellement d'origine portugaise au Brésil ou espagnole ailleurs. Des vagues d'immigration ultérieures sont venues ensuite : par exemple l'immigration italienne en Argentine. 

Les latino-américains de type caucasien sont les descendants de fondateurs européens venus en Amérique durant les périodes coloniales et post-indépendantes. Ils sont principalement originaires d'Espagne, d'Allemagne, d'Italie et du Portugal. Bien que minoritaire au regard des 18 millions d'Allemands ou des 31 millions d'Italiens du Brésil, on estime à pas moins d'un million la communauté de Français ou de descendants de Français dans les pays latino-américains. 

Les pays comptant la plus forte proportion de population d'origine européenne sont l'Uruguay (88 %), l'Argentine (86 %), Costa Rica (84 %) et Puerto Rico (80 %). Au Chili, les habitants d'origine européenne représentent 67%. Viennent ensuite le Brésil (47 %), le Venezuela (42 %) et Cuba (37 %). En Colombie et au Guatemala, les blancs atteignent un cinquième de la population (20 %), et au Nicaragua 17 %. Dans les pays comme le Mexique et le Pérou environ 15 % de la population est criollo ou blanche, principalement d'origine espagnole. Le pays avec la plus petite population d'origine européenne est le Honduras (1 % de la population).

L'arrivée de ces populations en Amérique latine est due à l'esclavage. En effet, la résistance « insuffisante » des populations locales, en particulier face aux maladies importées de l'Ancien monde, a poussé les exploitants espagnols et portugais à faire venir une population d'esclaves, plus résistante, venue d'Afrique. 

Mais la majorité de la population d'Amérique latine est en réalité le résultat du métissage entre la population amérindienne et les différents apports. . 

L'Amérique latine est cinq fois plus touchée par les homicides que les autres continents. La moyenne annuelle des meurtres s'établit à 27 pour habitants, alors que la moyenne mondiale est de 5 pour .Cette vague de criminalité touche de nombreux pays d'Amérique latine, notamment la Colombie, où le taux d'homicides, y est le plus important au Monde ( supérieur à 80 pour 100 000 habitants par an). La violence est particulièrement urbaine et masculine (affrontement de cartels, de marras...).

Outre cette criminalité ardente, de petites délinquances sont en hausse, donnant un sentiment d'insécurité aux classes moyennes (incivilités, vols à la tire, cambriolages, vols de voiture, etc.).

L'Amérique latine connait une forte violence à l'encontre des femmes. En effet, 50 % des femmes sont maltraitées, dans la sphère intime, au point de vue physique et/ou psychologique. Ces femmes connaissent des formes de violences, bien divers, individuelle et quotidiennes, et dépasse régulièrement le cadre domestique. Les femmes sont les victimes préférées d'une violence sociale généralisée.

L'Amérique latine est le continent le plus inégalitaire du Monde. On retrouve ces inégalités à différentes échelles: infra nationale, infra régionale et locale.

En effet en Amérique latine, 40 % des ménages les plus pauvres perçoivent 15 % des revenus totaux, là où 10 % des plus riches détiennent 30 % des revenus totaux.

Les inégalités touchent principalement les groupes sociaux minoritaires tels que les femmes, les enfants de moins de 17 ans et les populations d'origine africaines et sont la source des violences sociales qui sévissent en Amérique latine

Des disparités au sein des grandes métropoles sont visibles entre quartiers riches et quartiers pauvres. Ces différents quartiers sont généralement coupés par des murs physiques ou naturels (forêts, manque d'infrastructures pour rejoindre le centre ville, etc.).

Le catholicisme est majoritaire dans la plupart des pays d'Amérique latine. Toutefois, le protestantisme (principalement évangélique) connaît depuis plusieurs années une forte croissance.

Part des religions dans les pays d'Amérique latine (2014)




</doc>
<doc id="4813" url="https://fr.wikipedia.org/wiki?curid=4813" title="Kirghizistan">
Kirghizistan

Le Kirghizistan, Kirghizstan ou Kirghizie, en forme longue la République kirghize (en kirghize , et , , en russe , et , ), est un pays d'Asie centrale. Extrêmement montagneuse, peuplée à l'origine par des populations nomades, cette ancienne république de l'URSS a obtenu son indépendance lors de l'effondrement de cette dernière en 1991. Ses habitants s'appellent les Kirghizes et parlent le kirghize, une langue turque.

Sa capitale, qui est également sa plus grande ville, est Bichkek.

Le Kirghizstan possède de frontières, réparties ainsi :

Le Kirghizstan est un pays d’Asie centrale, encadré par le Kazakhstan au nord, l’Ouzbékistan à l’ouest, le Tadjikistan au sud-ouest, et la Chine au sud-est et à l’est. D’une superficie de , le pays est totalement enclavé et ne possède pas d’accès à la mer.

Le pays est presque entièrement montagneux. Ces zones montagneuses divisent le pays en deux, le Nord-Est et le Sud-Ouest ; ces deux parties ne communiquent que par des cols situés à plus de d'altitude. Les principales villes du Kirghizstan se situent dans les zones les plus basses du pays.

Au nord, la capitale Bichkek se trouve à d'altitude, très près de la frontière avec le Kazakhstan. C’est dans cette région nord du pays que sont concédées plusieurs bases militaires internationales, essentiellement aériennes, à la Russie (conservées après que le Kirghizstan est devenu indépendant) et aux États-Unis.

Au nord-est, le lac Yssik Koul forme une petite mer intérieure de à d'altitude. Profond de , le lac est légèrement salé et ne gèle pas en hiver. Son nom, Yssik Koul, signifie d'ailleurs en kirghize « lac chaud », et il est en partie alimenté par des sources chaudes.

Au sud-est, la chaîne des Tian Shan marque la frontière avec la Chine et culmine à avec le pic Pobedy (« victoire » en russe), qui est le point culminant du pays.

À l’ouest et au sud-ouest du pays, la vallée du Fergana (commune au Kirghizstan, à l’Ouzbékistan et au Tadjikistan) est prise en tenaille par les chaînes du Fergana (au nord-est de la vallée) et du Pamir Alay (ou monts Alaï, au sud). La vallée est notamment le réceptacle de la puissante rivière Naryn, qui traverse la totalité du pays depuis les hauteurs des Tian Shan au nord-est du pays.

Le sud (bordé par le Pamir Alay) de la vallée du Fergana constitue l’autre foyer de peuplement. On y trouve les villes d’Och et de Jalal-Abad, ainsi que le point le plus bas du Kirghizstan, la vallée du Kara-Darya (la seconde source ou un affluent du Syr-Darya), à d'altitude.

Dans cette région, le pays entoure également deux enclaves de l’Ouzbékistan et une du Tadjikistan, reliées à ces pays par des droits de passage sur quelques voies de transport internationalisées en direction de la vallée principale au nord. La chaîne du Pamir Alay, qui domine la région, culmine au pic Lénine à et prolonge la frontière sud du pays avec le Tadjikistan, jusqu’à celle avec la Chine.

Passée en Ouzbékistan, la rivière Naryn devient le Syr-Daria, l’un des deux principaux fleuves nourriciers de l'Asie centrale ; le fleuve traverse ensuite une petite région au nord du Tadjikistan et alimente un petit lac (frontalier avec le sud-ouest du Kirghizstan), avant de traverser à nouveau l’Ouzbékistan, puis de se jeter avec peine dans la mer d’Aral en traversant une grande région au sud du Kazakhstan. Ce fleuve est, avec l’Amou-Daria (qui coule au Tadjikistan et au Turkménistan, mais qui ne parvient plus jusqu’à la mer d’Aral en Ouzbékistan car il sert à alimenter le canal du Karakoum construit au Turkménistan jusque vers la mer Caspienne), l’objet de litiges entre tous ces pays et d’un désastre écologique majeur, à cause du détournement massif pour l’agriculture (durant la période soviétique) de leurs eaux. Le Syr-Darya ne permet plus d’alimenter qu’une toute petite partie au nord de la mer d’Aral, uniquement au Kazakhstan (tandis que le bas du cours de l’Amou-Daria a été complètement asséché, condamnant la plus grande partie méridionale de la mer d'Aral autrefois partagée par le Kazakhstan et l’Ouzbékistan).

Le climat de cette région est continental et varie selon les régions. Le climat de la vallée du Fergana au Sud-Est est sub-tropical et extrêmement chaud en été, avec des températures atteignant 40 °C. Au Nord, dans les zones de basse montagne, le climat est tempéré. Dans la zone du Tian Shan, le climat varie de continental à polaire selon l'altitude. Dans les zones les froides, les températures peuvent rester négatives pendant 40 jours en hiver et certaines zones désertiques connaissent des chutes de neige en continu durant cette saison. Dans les zones de moindre altitude, les températeurs varient de − 6 °C (21 °F) en janvier à 24 °C en juillet.

L'histoire ancienne du Kirghizstan est complexe. L'ethnogenèse kirghiz est à étudier en relation avec les Kirghiz de l'Ienisseï, entre autres, mais aussi les Kazakhs, les Ouzbeks, les Kalmouks.

La région correspondant au Kirghizstan est peuplée aux et par les Kirghizes, un peuple nomade turcique évoluant originellement, il y a , dans le nord-est de la Mongolie et qui se serait d'abord déplacé vers le sud de la Sibérie entre le et le , puis vers la région de Touva jusqu'au . L'islam devient la religion principale de la région vers le ; la plupart des Kirghizes sont des musulmans sunnites de l'école hanafite.

Au début du , le sud du territoire tombe sous le contrôle du khanat de Kokand (1710-1876 ou 1709-1883, ouzbek.

La partie sud du territoire kirgiz est formellement incorporé à l'Empire russe en 1876, au cours du Grand Jeu (rivalité coloniale anglo-russe). 

La répression de plusieurs révoltes pousse un certain nombre d'habitants à émigrer vers l'Afghanistan ou la Chine.

En 1918, un soviet est fondé dans la région et l'oblast autonome Kara-Kirghiz est créé en 1924 au sein de l'URSS. En 1926, il devient la République socialiste soviétique autonome de Kirghizie. En 1936, la République socialiste soviétique kirghize est intégrée comme membre à part entière de l'URSS.

Lors des bouleversements politiques survenus à partir de la fin des années 1980 en Union soviétique, des élections libres sont organisées au Kirghizstan en 1991 qui voient la victoire d'Askar Akaïev au poste de président en octobre de la même année. Le pays change de nom et devient la République du Kirghizstan en décembre 1991, tandis que Frounzé, la capitale, est rebaptisé de son nom présoviétique, Bichkek, en février 1991.

Après l'échec du coup d'État à Moscou qui marque la fin de l'URSS, le Kirghizstan se prononce par un vote en faveur de l'indépendance vis-à-vis de l'URSS le 31 août 1991. Il adhère cependant à la Communauté des États indépendants à la fin de la même année.

En mars 2002, des troubles éclatent dans le district d'Aksy lors de manifestations demandant la libération d'un opposant politique. La répression commandée par le premier ministre Kourmanbek Bakiev tue six personnes. Peu après, le président Akaïev accepte la démission de Bakiev de ses fonctions et le remplace par Nikolaï Tanaiev. Le président Askar Akaïev lance ensuite une réforme constitutionnelle, à laquelle sont conviées l'opposition et la société civile ; elle se conclut en février 2003 par un référendum, vraisemblablement entaché d'irrégularités. Les amendements à la Constitution transforment, entre autres, le Parlement bicaméral en un Parlement unicaméral de 75 sièges à compter des élections de février 2005.

Au cours des années suivantes, le pouvoir, toujours détenu par Askar Akaïev, devient de plus en plus autoritaire. Les élections législatives du 27 février et du 13 mars 2005 sont dénoncées comme frauduleuses, particulièrement par les observateurs de l'OSCE. Des troubles débutent vers la fin mars qui se transforment rapidement dans le sud du pays en manifestations appelant à la démission du gouvernement en place. Le 24 mars, manifestants partisans de l'opposition venus du sud du pays se heurtent à la police à Bichkek et prennent d'assaut l'immeuble abritant la présidence après des rumeurs faisant état de la fuite du président Akaïev hors du pays. C'est la « révolution des Tulipes ».

Après la révolution des Tulipes qui suit ces élections, le gouvernement intérimaire promet de développer de nouvelles structures politiques et de régler certains problèmes constitutionnels. La démission forcée de l'ancien président Askar Akaïev est acceptée par le Parlement kirghize le .

Les chefs de l'opposition mettent en place un gouvernement intérimaire et promettent d'organiser rapidement de nouvelles élections afin de prétendre à une légitimité définitive. Les luttes au sein de l'opposition sont réglées rapidement, Kourmanbek Bakiev prenant les postes de président et de premier ministre.

Kourmanbek Bakiev remporte ensuite l'élection présidentielle le organisée trois mois après le soulèvement populaire qui a provoqué la fuite de l'ancien président, Askar Akaïev, en Biélorussie. Le nouveau gouvernement, sous la présidence de Felix Koulov, est définitivement constitué en .

Un an après les événements de , une nouvelle opposition se structure pour exiger la mise en œuvre des réformes annoncées à l'issue de la révolution des Tulipes. Elle réunit d'anciens alliés de Bakiev et d'anciens partisans d'Akaïev.

Début l'opposition manifeste sur la place centrale de Bichkek pour réclamer une réforme constitutionnelle réduisant les compétences du président Kourmanbek Bakiev, accusé de népotisme et de corruption. Elle réclame, en outre, l'indépendance de la compagnie de radio-télévision et le limogeage de plusieurs hauts responsables. Les opposants et les fidèles du président Kourmanbek Bakiev parviennent finalement à un compromis sur une réforme de la Constitution qui rendrait les institutions du pays plus démocratiques.

Signée le par le président Bakiev, la nouvelle constitution instaure une république présidentielle et parlementaire. Le Parlement, composé de 90 sièges, pourra approuver le gouvernement, dont le premier ministre sera nommé par le parti vainqueur aux élections, avec l'accord du président.

Cependant, dès le , à la faveur de la crise institutionnelle ouverte par la démission du gouvernement de Félix Koulov, le président obtient des députés le vote d'une nouvelle version de la Constitution qui lui redonne plusieurs des pouvoirs concédés en novembre.

Le , le Parlement approuve la nomination par le président Bakiev d'Azim Issabekov, ex-ministre de l'Agriculture dans le gouvernement Koulov, au poste de premier ministre.

Le , Azim Issabekov donne sa démission, le président kirghiz ayant refusé de limoger sept personnes du gouvernement dans le but de les remplacer par des membres de l'opposition. Son remplaçant est Almaz Atambaïev, l'un des représentants du mouvement d'opposition Za reformy (Pour les réformes).

Atambaïev démissionne à son tour le 30 mars. Iskenderbek Aidaralïev assure l'intérim jusqu'aux élections législatives de décembre 2007, qui portent Igor Tchoudinov (du parti Ak Jol) au poste de premier ministre.

Jusqu'en 2009, le président Bakiev dirige le pays en maintenant une relative ouverture politique, loin des normes démocratiques occidentales, mais certains observateurs étrangers classaient le Kirghizstan comme le pays le plus démocratique d'Asie centrale. Depuis 2009, on note cependant un net virage présidentialiste. La réforme de l'État conduite à l'automne 2009 voit les ministères importants rejoindre l'Institut du président, et les autres être dépossédés d'une grande partie de leurs attributions. Maxime Bakiev, fils du président, est nommé à la tête d'une Agence pour le développement aux pouvoirs très étendus, 

L'opposition politique et les ONG de défense des droits de l'homme sont harcelées avec une intensité croissante. Plusieurs meurtres de journalistes et d'opposants défraient la chronique et provoquent les protestations de l'Union européenne. Si le Kirghizstan reste pour le moment plus ouvert que ses voisins, comme l'Ouzbékistan qui dérive depuis des années vers la dictature policière, ou le Turkménistan qui n'en est jamais sorti, il connaît donc actuellement une dégradation rapide du climat politique. Il reste classé régulièrement parmi les pays les plus corrompus du monde.

Le , une violente manifestation des opposants au régime du président Bakiev dégénère. On compte 75 morts et 500 blessés dans la capitale, Bichkek. Dans la soirée, des centaines d'opposants assiègent le Parlement situé à proximité de la résidence présidentielle. Le premier ministre, Daniar Oussenov, déclare l'état d'urgence et un couvre-feu est instauré. Trois chefs de l'opposition sont interpellés et inculpés pour crimes graves, puis l'un d'eux, Omourbek Tekebaïev, est libéré plus tard dans la journée.

Toujours en soirée, le premier ministre, Daniar Oussenov, ayant « remis une lettre de démission », après des négociations avec l’un des chefs de l’opposition, celui-ci, Temir Sarïev, l'annonce à la radio kirghize Azattyk, et, dans la foulée, l’opposition forme son « propre gouvernement », avec à sa tête l’ex-ministre des Affaires étrangères, Roza Otounbaïeva.

Les États-Unis s'inquiètent de l'instabilité du pays, dans la mesure où ils disposent d'une base militaire aérienne qui leur sert de pont pour faire transiter leurs hommes et leur matériel vers l'Afghanistan. Parallèlement, la Russie propose son aide au gouvernement par intérim.

Le , Kourmanbek Bakiev, le président déchu, en exil au Kazakhstan, démissionne officiellement dans une lettre télécopiée adressée aux nouveaux dirigeants du pays.

Bakiev étant en exil, Roza Otounbaïeva est chargée de diriger le pays par intérim, et d'écrire cette fois une nouvelle Constitution parlementaire.

Après son élection en octobre 2011 et depuis le décembre 2011, Almazbek Atambaev est le Président du Kirghizstan.

Le Kirghizstan reste un pays assez pauvre, dont l'économie est essentiellement tournée vers l'agriculture. Juste avant la chute de l'URSS en 1991, 98 % des exportations du Kirghizstan étaient destinées à l'Union soviétique ; l'effondrement de cette dernière a considérablement ralenti la production du pays au début des années 1990. Des réformes importantes furent entreprises qui ont certes permis d'améliorer la performance économique du pays au cours des dernières années (le Kirghizstan fut la première république de l'ancien bloc soviétique à être admise à l'Organisation mondiale du commerce en décembre 1998), mais ses revenus ne sont toujours pas constants et la pauvreté reste très présente.
L'agriculture est le principal secteur d'activité du pays, employant en 2002 la moitié de la population et produisant 35,6 % du PIB. Le Kirghizstan est montagneux et convient à l'élevage du bétail, la principale activité agricole. Les productions dominantes incluent le blé, le sucre de betterave, le coton, le tabac, les légumes, les fruits et les noix ; dans une moindre mesure, la laine, la viande et les laitages.

Le Kirghizstan exporte principalement des métaux non ferreux et des minéraux, des produits manufacturés en laine et agricoles, de l'énergie électrique et quelques autres marchandises. La première source de devises est la production d'or de la mine kirghizo-canadienne de Kumtor, dont l'activité représente environ 10 % du PIB. Ses importations comprennent du pétrole, du gaz naturel, des métaux ferreux, des produits chimiques, la plupart de ses outils et machines, du bois, du papier, un peu de produits alimentaires et des matériaux de construction. Ses partenaires commerciaux principaux sont la Chine, la Russie, le Kazakhstan, les États-Unis, l'Ouzbékistan et l'Allemagne.

La Constitution de 1993 définit le statut politique du pays comme une république démocratique. Le pouvoir exécutif est composé d'un président et d'un premier ministre. Le pouvoir législatif est bicaméral. Le pouvoir judiciaire est composé d'une Cour suprême, d'une Cour constitutionnelle, de cours locales et d'un procureur général.

Signée le par le président Bakiev, une nouvelle constitution instaure une république présidentielle et parlementaire. Le Parlement, composé de 90 sièges, pourra approuver le gouvernement, dont le premier ministre est nommé par le parti vainqueur aux élections, avec l'accord du président.

Le Kirghizstan fait partie de l'Organisation de coopération de Shanghai (OCS), dont il fut en 2001 l'un des membres fondateurs.

Le Kirghizstan est divisé en sept régions ("oblastary" ; au singulier, "oblasty") et une ville ("shaary"). Leurs centres administratifs portent généralement le même nom (dans la liste ci-dessous les exceptions sont notées entre parenthèses) :


Chaque "oblasty" est divisé en districts ("raion"), administrés par des officiels nommés par le gouvernement. Les communautés rurales ("aiyl okmotu"), constituées au plus de vingt villages, ont leurs propres maires et conseils élus.

En 2016, la population du Kirghizstan était estimée à habitants. La même année, 30,1 % avaient moins de 15 ans et 5,13 % plus de 65. Le pays est rural à 64,3 %, pour une densité de population assez faible de 29 habitants par km². La capitale, Bichkek, compte officiellement habitants, auxquels il convient sans doute d'ajouter de nombreux saisonniers, ainsi que des occupants illégaux de terrains non répertoriés dans les statistiques. L'espérance de vie est en 2016 de 70,7 ans, avec une espérance de vie de 75 ans pour les femmes et de 66 pour les hommes.

Le taux de natalité de 22.6 ‰ reste élevé, et le taux de mortalité de 6,6 ‰ est faible, avec un solde migratoire de - personnes. Le Kirghizstan a un taux d'alphabétisation de 97 % en 2001.

65 % de la population est d'origine kirghize, groupe ethnique turc présentant des influences mongoles et étant historiquement constitué de bergers semi-nomades, ils sont divisés en quarante clans. La plus grande minorité est celle des Ouzbeks (14,5 % de la population), principalement installés dans le Sud, suivis par les personnes d'origine russe (9,0 %, essentiellement dans le Nord, constamment en baisse du fait de leur émigration), doungane (1,2 %), ouïghoure (1,1 %), tadjike (1,1 %), meskhète (0,9 %), kazakhe (0,7 %), ukrainienne (0,5 %, en baisse) et coréenne (0,3 %). Il existe également de petites communautés d'origine allemande, mais la plupart sont parties dans les années 1990.

Près de 75 % de la population est composée de musulmans sunnites (islam), avec 20 % chrétiens orthodoxes et quelques milliers de catholiques.

Le Kirghizstan est l’une des deux ex-Républiques socialistes soviétiques, avec le Kazakhstan, à avoir conservé le russe comme langue officielle.
Le pays a, en plus de celle-ci, ajouté la langue kirghize peu après l’indépendance en septembre 1991.
Celle-ci appartient au groupe des langues turques. En 1924, un alphabet basé sur l'alphabet arabe fut introduit, remplacé par l'alphabet latin en 1928. En 1941, l'alphabet cyrillique fut définitivement adopté.

D’après un recensement de 2009, le kirghize se place en tête des langues parlées avec 71 % de locuteurs natifs, l’ouzbek en deuxième position avec 15 %, malgré son statut de langue officielle le russe n’occupe que la troisième place avec 9 % de locuteurs, les 5 % restants étant partagés par d’autres langues.

Toutefois, selon le même recensement, la langue russe est maîtrisée par 40 % de la population en raison du fait qu’elle constitue encore la langue des affaires et de la politique.
On constate néanmoins une tendance à une arrivée de la langue kirghize sur la place publique. Longtemps considérée comme une langue domestique, la langue kirghize apparaît dans la vie politique et notamment beaucoup de délibérations parlementaires sont menées en cette langue.

La religion principale est l'islam sunnite, de l'école hanafite. Mais la pratique religieuse musulmane est marquée également par les influences du chamanisme, existant antérieurement à l'islam, et du soufisme, dont les missionnaires ont joué un grand rôle dans l'islamisation de la région. 

La pratique religieuse est faible, du fait du passé communiste de la république, mais surtout du fait de l'apport de pratiques animistes et chamaniques, et de l'esprit de groupe, tribal et patriarcal, où la figure d'un patriarche (le doyen de la famille) est respectée. L'extrémisme religieux est très rare, et les idées Laïques, apportées au temps du communisme, sont très répandues.

Si les chrétiens orthodoxes se retrouvent surtout chez les Russes et autres Slaves, la pratique religieuse est rare. Le protestantisme est récent, et avant l'indépendance du pays, il concernait surtout les Allemands de la Volga, déportés, ou on retrouvait aussi une minorité catholique.

Il y a entre 10 000 et 15 000 Zoroastriens, surtout implantés dans les zones rurales.

Traditionnellement un peuple nomade, les Kirghizes restent attachés à ces traditions. Elles comprennent notamment les artisanats liés à la fabrication et à la décoration de la yourte, faite d'épaisses toiles de feutre. Le cheval occupe une grande place dans l'art, l'imaginaire et la symbolique collective. Ainsi le "Kok borou" et l"'Oulak tartysh", deux variantes du jeu équestre du « bouc écorché », sont restés des sports très populaires.

Les premiers Jeux Mondiaux Nomades, qui mettent en valeur ces sports traditionnels kirghizes, se sont déroulés en 2014 au Kirghizstan; la deuxième édition des Jeux devrait également avoir lieu au Kirghizstan.

L'épopée et le poème mélodique improvisé sont les expressions artistiques par excellence. Ce dernier est accompagné au moyen d'un instrument à trois cordes, le komouz, et peut faire l'objet de joutes entre deux orateurs, les "aïtysh". L'épopée Manas, phénomène littéraire par son volume et son emphase, transmise et enrichie sur plusieurs siècles par la tradition orale, fait la fierté d'un peuple qui connaît actuellement un processus de réappropriation de ses racines historiques et mythologiques.

Dans la seconde moitié du , le Kirghizstan s'est illustré par un cinéma poétique nourri de la clarté et de la rigueur des écoles soviétiques, avec une génération de grands cinéastes (qualifiée de « miracle kirghiz ») dont Tolomouch Okeev et Bolotbek Chamchiev. Depuis l'indépendance, il convient de citer avant tout la trilogie d'Aktan Abdykalikov composée des films "La Balançoire", "Le Fils adoptif" et "Le Singe".

La littérature kirghiz reste principalement orale jusqu'au , avec des récits épiques nationaux, guerriers et lyriques, tels que "Manas", "Kedeï-khan" ou "Kourmanbek".

Dans le domaine de la littérature écrite, se détache la figure de Tchinguiz Aïtmatov, auteur de nombreux livres, traduits en plusieurs langues, notamment "Jamilia", "Le premier maître", "Il fut un blanc navire", "Une journée plus longue qu'un siècle" et "Les rêves de la louve".

Depuis 2012 les voyageurs de quarante pays (dont la France, la Belgique et la Suisse) sont dispensés de visa pour les séjours touristiques et d’affaires de moins de 60 jours. Le Kirghizstan interdit aux personnes étrangères HIV séropositives d'obtenir un permis de travail.
Le Kirghizstan constitue une destination encore peu pratiquée, mais très appréciée pour le tourisme d'aventure et nature : alpinisme, paralpinisme, héliski, trekking, randonnées à cheval, rafting, VTT, et chasse.

Les principaux sites présentant un attrait touristique sont :

Le Kirghizstan a pour codes :





</doc>
<doc id="4817" url="https://fr.wikipedia.org/wiki?curid=4817" title="Équation linéaire">
Équation linéaire

Une équation à coefficients réels ou complexes est dite linéaire quand elle peut être présentée sous la forme
ou, de manière équivalente
où "x" est l'inconnue, "a" et "b" sont deux nombres donnés. Si "a" est différent de zéro, la seule solution est le nombre

Plus généralement, une équation est dite linéaire lorsqu'elle se présente sous la forme
où "u" est une application linéaire entre deux espaces vectoriels E et F, "b" est un élément donné de F. On recherche l'inconnue "x" dans E.

La linéarité permet d'effectuer des sommes et des combinaisons linéaires de solutions, ce qui est connu en physique sous le nom de principe de superposition. Les espaces ont des structures d'espaces vectoriels ou affines. Les méthodes de l'algèbre linéaire s'appliquent et peuvent considérablement aider à la résolution.

Les équations linéaires à coefficients réels sont les équations les plus simples à la fois à exprimer et à résoudre. Elles ont donc un intérêt en pédagogie des mathématiques pour enseigner la mise en place de la méthode de résolution générale : mise en équation, application d'une méthode de résolution.

D'un point de vue concret, un certain nombre de phénomènes physiques peuvent se modéliser par une loi linéaire (ou loi proportionnelle). Une équation linéaire est l'expression d'un problème dont le phénomène peut se modéliser par une telle loi.

Enfin, des lois plus complexes peuvent prendre une forme linéaire,

Soient "u" une application linéaire de E dans F, et "b" appartenant à F. On considère l'équation linéaire

L'équation
dite équation homogène associée a pour solution le noyau de "u", qui est un sous-espace vectoriel de E.

L'équation complète "u"("x") = "b"
Ce qu'on retient souvent sous la forme « la solution de l'équation complète est la somme d'une solution particulière et de la solution générale de l'équation homogène associée ».

La résolution de l'équation revient donc à la détermination des espaces image et noyau de "u". 
Le noyau est souvent plus facile à calculer que l'image, mais celle-ci peut être connue dans de nombreux cas grâce au théorème suivant.

Théorème d'isomorphisme des supplémentaires du noyau

Soit "u" une application linéaire de E dans F. Soit H un supplémentaire du noyau de "u". Alors H est isomorphe à l'image de "u". Plus précisément, la restriction de "u" à H induit un isomorphisme de H sur Im "u".

Corollaire : relation rang-noyau

Avec les mêmes notations si en outre l'espace de départ E est de dimension finie
Cette formule est parfois appelée formule du rang.
Ou même, plus généralement, si le noyau est de codimension finie, cette codimension est égale à la dimension de l'image.

Si on additionne une solution de "u"("x") = "b" et une solution de "u"("x") = "c", on obtient une solution de l'équation "u"("x") = "b" + "c". On peut plus généralement effectuer des combinaisons linéaires de solutions, ce qui porte souvent le nom de superposition en physiques.

Ainsi si on doit résoudre "u"("x") = "b" pour un vecteur "b" général, on constate qu'il suffit d'effectuer la résolution pour les vecteurs "b" d'une base de F.

On peut essayer d'étendre la méthode de superposition à des « sommes infinies », c'est-à-dire des séries. Mais il faut alors justifier qu'on peut effectuer un passage à la limite.

Soient "n "+ 1 scalaires distincts "x", …, "x" et "n "+ 1 scalaires "y", …, "y". La recherche des polynômes P tels que pour tout "i", P("") = "y" est appelée problème d'interpolation de Lagrange.

Il s'agit d'un problème linéaire avec 

Le noyau de "u" est l'ensemble des polynômes nuls en "x", …, "x"
Il admet pour supplémentaire l'espace des polynômes de degré inférieur ou égal à "n".

En conséquence l'image de "u" est de dimension "n" + 1, ce qui prouve que "u" est surjective et que le problème a toujours une solution. En outre 
est un isomorphisme d'espaces vectoriels.

On en déduit les résultats d'existence et d'unicité suivants

Enfin l'isomorphisme "u" peut être utilisé pour obtenir explicitement le polynôme d'interpolation de plus bas degré. Cela revient à déterminer l'antécédent de "b" par "u". Par linéarité, il suffit de déterminer les antécédents des vecteurs de la base canonique de K.

Il est donc naturel de faire intervenir le problème d'interpolation élémentaire : trouver 

On aboutit alors naturellement à l'expression suivante

Et enfin pour le problème d'interpolation complet

Une équation linéaire à une inconnue "x" est une équation de la forme "ax" + "b" = 0 où "a" et "b" sont des réels (ou des complexes). Les réels "a" et "b" sont appelés des coefficients, "a" est le coefficient devant "x" et "b" le coefficient constant. On appelle aussi cette équation, une équation du premier degré à une inconnue.

Une équation linéaire à plusieurs inconnues "x, y, z", … est une équation de la forme
où "a", "b", "c", …, "k" sont des réels (ou des complexes). De même ici, "a" est le coefficient devant "x", "b" le coefficient devant "y", …, "k" le coefficient constant.

L'ensemble des solutions d'une équation linéaire à "n "inconnues dont au moins un coefficient autre que le coefficient constant est non nul, est un sous-espace affine de dimension "n "– 1.

Cas des équations linéaires homogène

Les équations linéaires homogènes sont celles dont le coefficient constant est nul.

Propriété: si ("x", "y", "z", …) et ("x", "y", "z", …) sont deux solutions d'une équation linéaire homogène alors il en est de même de ("kx", "ky", "kz", …) et ("x" + "x", "y" + "y", "z" + "z", …).

L'ensemble des solutions d'une équation linéaire homogène à "n "inconnues dont un coefficient au moins est non nul est un sous-espace vectoriel de dimension "n "– 1.

Voir aussi : Système d'équations linéaires

On parlera ici de fonctions définies sur ℝ ou sur ℂ à valeurs dans ℝ ou dans ℂ.

Une équation différentielle linéaire du premier ordre d'inconnue "y" est une équation de la forme
où "a", "b" et "c" sont des fonctions numériques.

Une équation différentielle linéaire d'ordre n et d'inconnue y est une équation de la forme 

où "a", "a", …, "a" et "a" sont des fonctions numériques et "y" la dérivée d'ordre "k" de "y".

Si "a", "a", …, "a" et "a" sont des constantes, on parle d'équation linéaire à coefficients constants.

Cas des équations homogènes

Si "a" = 0, on parle d'équation linéaire homogène.

Par exemple l'équation différentielle "y"" + "y" = 0 est une équation différentielle linéaire homogène à coefficients constants.

Si "y" et "y" sont solutions d'une équation différentielle linéaire homogène alors il en est de même de "ky" et de "y" + "y" ;

Si on connaît une solution particulière d'une équation différentielle linéaire, la solution générale est formée de la somme de cette solution particulière avec la solution générale de l'équation linéaire homogène associée.

Méthode de la fausse position


</doc>
<doc id="4818" url="https://fr.wikipedia.org/wiki?curid=4818" title="Microscopie à fluorescence">
Microscopie à fluorescence

La microscopie en fluorescence (ou en "épifluorescence") est une technique utilisant un microscope optique en tirant profit du phénomène de fluorescence et de phosphorescence, au lieu de, ou en plus de l'observation classique par réflexion ou absorption de la lumière visible naturelle ou artificielle . 
On peut ainsi observer divers objets, substances (organiques ou inorganiques) ou échantillons d'organismes morts ou vivants. 

Elle fait désormais partie des méthodes de recherche classiques et de la biologie et continue à se développer avec l'imagerie moléculaire.

La fluorescence est la propriété que possèdent certains corps d'émettre de la lumière après avoir absorbé des photons de plus haute énergie. La microscopie en fluorescence repose sur la formation d'une image par détection de cette lumière émise.
Le déplacement de Stokes décrit la différence entre la longueur d'onde absorbée par l'objet (émise par la source lumineuse du microscope) et émise par l'objet. Plus la différence entre les deux longueurs d'onde est grande plus il est facile d'observer la fluorescence. 
En fluorescence on distingue deux types d'objets : 

En microscopie de fluorescence, on peut donc visualiser directement des substances fluorescentes. Pour des substances, des cellules, des molécules non fluorescentes, il est nécessaire de les marquer par des substances appelées fluorochromes, comme le DAPI qui marque l'ADN et fluoresce en bleu.

Certains marqueurs génétiques comme la protéine fluorescente verte, (en anglais "Green Fluorescent Protein" ou GFP) sont aussi très utilisés en biologie dans des organismes génétiquement modifiés pour en produire de manière endogène. 
Dans ce cas, le fluorochrome est une protéine produite directement par la cellule elle-même et ne nécessite pas l'ajout de substrat. La fluorescence peut alors être visualisée directement dans les cellules vivantes, c'est un des domaines développés par l'imagerie moléculaire.

De nombreuses techniques de marquage peuvent être utilisées :

On peut exciter les substances fluorescentes par une excitation monophotonique. On utilise pour cela une lumière d'excitation dont la longueur d'onde excite directement le fluorophore. Donc, la fluorescence émise peut provenir de toute l'épaisseur de l'échantillon traversée par le faisceau d'excitation. L'élément clé de ce microscope confocal est alors représenté par une "fenêtre" (un sténopé ou un iris confocal) placée devant le détecteur qui élimine la fluorescence provenant des régions non focales. L'observation de signaux de fluorescences repose sur cinq éléments :


Cette excitation consiste en l'absorption quasi simultanée de plusieurs photons d'excitation d'une longueur d'onde proche d'un multiple de l'excitation optimale à un photon. On utilise pour cela un laser pulsé dans des fréquences proches de l'infrarouge. Dans ce cas, seul le point de focalisation du faisceau laser est excitateur (densité de photon suffisante pour coupler l'énergie d'excitation). Bien souvent les applications sont limitées à la microscopie biphotonique (excitation du fluorophore par deux photons). 

Ce système est considéré comme une évolution technologique importante pour trois raisons majeures : 

En pratique, le rendement d'émission de fluorescence est moins bon qu'un confocal simple photon et le rapport signal/bruit est plus faible. Ainsi, il ne montre que peu d'avantage pour l'observation de cellules en culture ou de coupes de tissu (50- d'épaisseur).

Cette technique est naturellement utilisée pour l'excitation simultanée de plusieurs fluorophores à spectres d'émission différents.

Une variante de cette technique est la microscopie à fluorescence par excitation multiphotonique multifocale. Le principe est identique, mais le faisceau laser est divisé en plusieurs faisceaux ce qui permet de balayer simultanément plusieurs points. Ceci permet de diminuer le temps d’acquisition des images.

Les techniques de fluorescence peuvent être utilisées avec différents types de microscope :


Ces appareils possèdent des pièces interchangeables en forme de cube disposées sur une tourelle rotative ou sur une tirette selon les fabricants. 

Ces « cubes » filtrent la lumière allant vers l'objectif et allant vers l'observateur ou le capteur, selon les fluorescences recherchées. 

Ils comportent 2 filtres et un miroir particulier, « dichroïque » qui réfléchit certaines longueurs d'onde et qui est traversé par d'autres.
Vers la source se trouve le filtre d'excitation.

Côté observateur se trouve le filtre barrière.

Les cubes sont répertoriés selon les lumières d'excitation: ultraviolet, violet, bleu, vert...

On utilise cette technique, notamment pour observer des monocouches lipidiques auxquelles on a ajouté une sonde lipidique fluorescente. On observe une fluorescence selon la phase dans laquelle se trouve le lipide principal. En général, la sonde est soluble dans la phase liquide-expansé (LE) mais pas dans les phases gazeuse (G) ou solide (S). Ceci permet d'observer les macrostructures formées par la monocouche. Ci-contre, on observe une monocouche lipidique à l'équilibre par microscopie à épifluorescence. La sonde fluorescente lipidique est soluble dans la phase LE mais pas dans la phase G. Le résultat est qu'on observe, à l'équilibre, des sortes de bulles (mais une « bulle » est un concept tridimensionnel) dont les parois sont constituées du lipide en phase LE et l'intérieur en phase G. Si l'on prend un exemple tridimensionnel, cela reviendrait à prendre de la mousse de savon et à en couper une très fine tranche.





</doc>
<doc id="4826" url="https://fr.wikipedia.org/wiki?curid=4826" title="Mansi">
Mansi

Le mansi (ou vogoul) est une langue appartenant (avec le khanty) au groupe ob-ougrien de la famille des langues finno-ougriennes. Le mansi et le khanty sont étroitement apparentés au hongrois, avec lequel ils forment la branche ougrienne de la famille finno-ougrienne. Le mansi est la langue maternelle d'une partie des Mansis, un peuple du Nord-Ouest de la Sibérie, installé principalement au bord des rivières Sosva et Konda et de leurs affluents. Le nombre de locuteurs est évalué à .

Le mansi se subdivise en quatre dialectes principaux. Le plus parlé est le dialecte du nord, sur lequel est fondé la langue écrite. On distingue également les dialectes de l'est (ou de la Konda), de l'ouest et du sud (ou de la Tavda). Les deux derniers sont aujourd'hui éteints.

La déclinaison du mansi comprend six cas : le nominatif (marque zéro), le locatif (suffixe "-t"), le latif (suffixe "-n"), l'ablatif (suffixe "-nel"), le translatif (suffixe "-iγ/-γ/-jiγ") et l'instrumental (suffixe "-l").

Les noms possèdent trois nombres : le singulier, le duel (suffixe "-iγ/-γ/-jiγ") et le pluriel (suffixe "-t"). Au pluriel et au duel, les suffixes casuels sont les mêmes qu'au singulier, mais le translatif n'est utilisé qu'au singulier.

L'adjectif attribut s'accorde en nombre avec le sujet. Mais l'adjectif épithète ne s'accorde ni en nombre ni en cas avec le substantif auquel il se rapporte.

De même que plusieurs autres langues finno-ougriennes, le mansi exprime les rapports de possession au moyen de suffixes possessifs. Ceux-ci varient selon le nombre et la personne du possesseur et se construisent sur le mot désignant l'objet possédé. Exemple : "kol" « maison » → "kole" « sa maison », "kolmen" « notre maison (à nous deux) », "koluw" « notre maison (à nous qui sommes plus de deux) », "kolen" « votre maison (à vous deux) », "kolan" « votre maison (à vous qui êtes plus de deux) », etc.

La conjugaison du mansi comporte trois nombres (singulier, duel, pluriel), deux temps (le présent et le prétérit), quatre modes (indicatif, conditionnel, impératif, précatif) et deux voix (active et passive). De même qu'en hongrois, il existe deux conjugaisons : la conjugaison indéfinie (ou subjective) et la conjugaison définie (ou objective) dans laquelle la forme verbale comporte un suffixe faisant référence au complément d'objet.



</doc>
<doc id="4829" url="https://fr.wikipedia.org/wiki?curid=4829" title="Toshiba">
Toshiba

Cette entreprise est présidée par Hisao Tanaka . En mars 2012 elle employait environ .

Hisashige Tanaka fonde en 1875 une manufacture de télégraphes dans le quartier de Ginza à Tokyo. Son héritier, Daikichi Tanaka, crée à Shibaura en 1882 , premier fabricant du Japon de matériel télégraphique. En 1893, le nom de la société est modifié en . Durant la première moitié du , l'entreprise devient un fabricant majeur de machinerie lourde au Japon, et se modernise pendant l'Ère Meiji, pour devenir une entreprise d'envergure internationale.

La seconde entreprise, nommée originellement , est fondée en 1890. Elle devient le premier producteur japonais de lampes à incandescence. L'entreprise se diversifie ensuite dans la fabrication d'autres produits de grande consommation, et elle est renommée en 1899.

La fusion en 1939 de Shibaura seisakujo et Tōkyō denki donne naissance à une nouvelle société baptisée . Elle est bientôt surnommée Toshiba, mais ce n'est qu'en 1984 que l'entreprise est officiellement appelée .

Le groupe connut une très forte expansion, à la fois grâce à sa croissance interne mais aussi de par de nombreuses acquisitions, achetant des firmes du secteur primaire et de l'industrie lourde dans les années 1940 et 1950 et les transformant en filiales du groupe à partir des années 1970. Le groupe ainsi créé rassemble "Toshiba EMI" (1960), "Toshiba Équipements Électriques" (1974), "Toshiba Chemins de Fer" (1974), "Toshiba Éclairage et Technologie" (1989) et "Toshiba Transports" (1999).

En 1987, l'entreprise fut accusée de vente illégale de fraiseuses automatiques utilisées par l'Union soviétique pour produire des propulseurs sous-marins très silencieux, violant ainsi l'accord du CoCom.
L'incident mit sous tension les relations entre les États-Unis et le Japon et eut comme conséquence la poursuite et l'arrestation de deux cadres supérieurs, ainsi que la prise de sanctions sur la société par les deux pays.

En 2001, Toshiba signa un contrat avec Orion Electric, un des premiers OEM mondiaux dans le domaine de la fabrication d'équipements électroniques visuels, pour fabriquer et fournir le consommateur en matériel vidéo afin que Toshiba puisse satisfaire la demande croissante du marché Nord-américain.

En décembre 2004, Toshiba a annoncé qu'il arrêterait progressivement la fabrication de téléviseurs à tubes cathodiques.

En 2006, l'entreprise met un terme à sa production d'écrans plasma. La société transféra rapidement à "Orion" la production des téléviseurs cathodiques et plasma de marque Toshiba. Cependant, pour être présent et assurer sa future compétitivité sur le marché des écrans plats numériques, Toshiba a fait un investissement considérable dans une nouvelle génération de technologie d'affichage appelée SED.

Toshiba a racheté en 2007 la firme américaine Westinghouse Electric Company, spécialiste dans le domaine nucléaire (et qui avait cédé sa licence concernant les réacteurs à eau pressurisée en 1974 à Framatome, permettant aux Français de construire le parc nucléaire actuel).

En mai 2011, Toshiba acquiert Landis+Gyr pour l'équivalent de 2,3 milliard de dollars. 

Le 31 août 2011, Sony annonce un accord avec Hitachi, Toshiba et INCJ portant sur la mutualisation des moyens de productions de dalles LCD de petites et moyennes tailles sous la forme d'une nouvelle entreprise appelée Japan Display.

En décembre 2012, Toshiba annonce être entré en négociations avec trois firmes pour la vente d'une participation allant jusqu'à 16 % du constructeur américain de centrales nucléaires Westinghouse Electric dont elle détient 67 % des parts.

Le 21 janvier 2014, Toshiba rachète la majorité des actifs de OCZ Technology et crée la société OCZ Storage Solutions avec ces actifs.

En février 2015, des informations sont secrètement divulguées à la Commission de surveillance des échanges et des titres (CSCE), le gendarme de la bourse japonaise. Toshiba fait mention de problèmes comptables le 3 avril, puis nomme une commission indépendante composée d’avocats et de comptables pour enquêter sur le sujet. Le 20 juillet 2015, le rapport de la commission conclut que la direction du groupe a tout fait pour gonfler artificiellement ses profits de 152 milliards de yens (1,1 milliard d’euros) pendant sept ans. Norio Sasaki, vice-président et qui dirigea le groupe entre 2009 et 2013, le PDG puis Atsutoshi Nishida, dirigeant de 2005 et 2009 et membre du conseil d'administration, annoncent leur démission.

Les employés du groupe auraient subi d’énormes pressions pour atteindre les objectifs de profits à court terme. Cette politique, baptisée « The Challenge », aurait été mise en place du temps d’Atsutoshi Nishida, après notamment l’onéreuse acquisition, en 2006, de la branche nucléaire de l’américain Westinghouse Electric. Le rapport a été transmis à la CSCE, qui va mener ses propres investigations et éventuellement recommander à l’agence des services financiers (FSA) d’imposer une amende à Toshiba. La FSA va également ouvrir une enquête sur EY Shinnihon, le cabinet d’audit qui signait les bilans de Toshiba. La bourse de Tokyo pourrait, de son côté, placer Toshiba sous surveillance, voire l’exclure des cotations. Les actionnaires pourraient engager des poursuites, au Japon et aux États-Unis, sous la forme d’un recours collectif.

En septembre 2015, le groupe annonce pour la période d’avril 2014 à mars 2015, une perte nette de 37,83 milliards de yens, soit 284 millions d’euros.

En octobre 2015, Toshiba vend ses activités dans les capteurs d'images à Sony pour 165 millions de dollars.

En décembre 2015, Toshiba se sépare de employés et annonce une perte nette annuelle record de plus de quatre milliards d'euros. Dans le même temps, Toshiba serait en discussion avec Fujitsu et VAIO pour fusionner leur activités de fabrications d'ordinateurs. En mars 2016, Canon entre en négociation exclusive pour l'acquisition des activités médicales de Toshiba pour environ 6 milliards de dollars. Les activités médicales de Toshiba concernent notamment les IRM et les appareils à rayon X, alors Canon possède des activités dans les appareils à rayon X et les appareils d'ophtalmologie.

En juin 2016, Toshiba vend une participation de 80 % de sa division d'équipement électroménager à Midea.

En décembre 2016, Toshiba annonce la découverte de pertes importantes, de plusieurs milliards de dollars, lié à l'acquisition d'activités de Chicago Bridge & Iron, en décembre 2015.

En 2017, Toshiba annonce une dépréciation de 5,9 milliards de dollars découlant de l'acquisition, fin 2015, de "CB&I Stone & Webster" par Westinghouse. Cette acquisition avait pour but d'éteindre les contentieux juridiques sur la construction des réacteurs AP1000 en chantier aux États-Unis, mais fin 2016 Westinghouse a revu en très forte hausse la charge de travail restante. Ces déboires dans la construction des premiers réacteurs de troisième génération sont très similaires à ceux eeFRAMATOME n Finlande et d'EDF à Flamanville.

Le 29 mars 2017, Toshiba approuve la mise en faillite de Westinghouse ; lesté de 9,8 milliards de dollars de dette et confronté à un enlisement de ses deux chantiers de construction de centrales nucléaires aux États-Unis, Westinghouse s'est formellement placé sous la protection du Chapitre 11 de la loi sur les faillites des États-Unis dans l'espoir de négocier une restructuration avec ses grands créanciers.

En avril 2017, Engie vend sa participation de 40 % dans Nugen, filiale spécialisée dans la construction de centrales nucléaires au Royaume-Uni, à Toshiba pour 139 millions d'euros.

Fin juillet, Toshiba est visé au total par près de 30 plaintes d'actionnaires estimant avoir été lésés par les irrégularités commises entre 2008 et 2014. Le montant cumulé réclamé est de de yens ( d'euros). Le 10 août, les commissaires aux comptes certifient les résultats annuels de Toshiba tout en émettant un avis défavorable sur la gouvernance d'entreprise passée.

En janvier 2018, Toshiba vend sa filiale Westinghouse à Brookfield Asset Management pour 4,6 milliards de dollars. Cependant cette somme va servir à payer les créanciers de Westinghouse et non Toshiba.

Fin janvier 2017, Toshiba annonce que le groupe va vendre une partie de son activité mémoire flash, incluant son business NAND flash appartenant à sa division "Storage & Electronic Device Solutions", Western Digital semblant le mieux placé pour la reprise. Début février, Toshiba indique avoir reçu des offres allant de 200 à de yens (1,7 à d'euros) pour une participation de 19,9 % dans son activité de mémoires flash.

Fin mars 2017, le journal "Nikkei" révèle que deux des candidats, le fonds d'investissement et le spécialiste des semi-conducteurs Broadcom, ont soumis une offre d'environ de yens ( d'euros). Western Digital, partenaire de Toshiba, s'oppose alors à cette vente, jugeant que cette cession viole les clauses contractuelles et demande l'arbitrage de la Chambre internationale de commerce. En effet, Western Digital n'a pas donné son aval à l'opération, alors qu'il est copropriétaire avec Toshiba de plusieurs des usines de la filiale produisant des mémoires NAND 2D et 3D, créés au début des années 2000 au Japon par la société SanDisk, acquise en 2016 par Western Digital.

Fin mai 2017, le "Nikkei" annonce que le fonds semi-public "Innovation Network Corporation of Japan" devrait s'associer avec un consortium pour reprendre les opérations de conception et de production du grand conglomérat japonais, et négocier un éventuel accord avec Western Digital pour la cession de l'activité mémoire flash. Le 14 juin, Western Digital annonce avoir déposé une requête auprès d'un tribunal californien pour bloquer la cession à un tiers des parts de Toshiba dans leur activité conjointe de puces-mémoires.

Le 21 juin 2017, Toshiba choisit un consortium mené par le fonds d'investissement américain Bain Capital, "Innovation Network Corporation of Japan" et la "" comme acquéreur privilégié de sa division de semi-conducteurs, Bain recevant du groupe sud-coréen de semi-conducteurs SK Hynix la moitié de son investissement. Western Digital annonce alors qu'il mettrait son veto à cette opération si elle comprend son concurrent SK Hynix, puis soumet une nouvelle offre de dernière minute avec le fonds d'investissement américain KKR. En conséquence, Toshiba attaque Western Digital en justice pour interférence, réclamant une indemnisation de de yens ( d'euros) auprès d'un tribunal de Tokyo, et empêche les salariés de son partenaire, extérieurs à leur usine commune de Yokkaichi, d'accéder aux informations relatives à leur coentreprise. Toshiba demande ensuite à un tribunal américain de rejeter la requête de Western Digital visant à empêcher la vente de sa division, estimant que la cour saisie n'a pas la compétence pour se prononcer et qu'une injonction lui causerait un préjudice irréparable. Le 23 août 2017, Toshiba entame de nouvelles discussions avec le consortium mené par Western Digital pour lui céder son activité au plus vite, laissant de côté l'offre jusqu'à présent privilégiée. Western Digital, associé à KKR, verrait se joindre à lui "Innovation Network Corporation of Japan" et la "Development Bank of Japan", initialement alliés avec l'offre concurrente, pour une offre de de yens ( d'euros).

En septembre 2017, Toshiba annonce la vente de ses activités de semi-conducteurs au consortium mené par Bain Capital, pour 18 milliards de dollars. Le consortium mené par Bain inclut notamment Dell, Apple, Kingston Technology et Seagate Technology. Western Digital indique dans la foulée qu'il va demander en référé que cette cession soit bloquée.

En novembre 2017, Toshiba annonce une augmentation de capital de l'équivalent de 4,5 milliards d'euros, pour faire face à ses difficultés et aux retards dans la vente de ses activités de semi-conducteurs.

Le 12 décembre 2017, Toshiba et Western Digital annoncent qu'ils mettent fin à leur contentieux au sujet de la vente de la filiale mémoires de Toshiba. L'accord prévoit que Western Digital renonce à toute procédure d'arbitrage et qu'en échange Toshiba lui permette d'investir dans une nouvelle ligne de production de mémoires de pointe. Les accords passés par les deux entreprises au sujet de leurs coentreprises de Yokkaichi seront étendus à 2027, voire au-delà. Western Digital compte également investir dans une nouvelle usine de mémoires dont Toshiba démarrera le chantier en 2018 dans le nord du Japon.

Le , le journal "L'Humanité" met en cause les pratiques sociales de l’entreprise pour maintenir un contrat avec l’État avec le soutien de ce dernier.

Le , le journal "Libération" approfondit l’enquête en faisant un lien avec l’UMP dans cette affaire économico-politique.

La société, de par sa tradition de l'innovation, a été à l'origine de nombreuses premières technologiques au Japon dont, entre autres, le radar en 1942, le téléviseur à transistors et le four à micro-ondes en 1959, la visiophonie en 1971, l'IRM en 1982, les ordinateurs portables en 1986, la NAND EEPROM en 1991, le DVD en 1995 et le HD DVD en 2005.

Avant la Seconde Guerre mondiale, Toshiba était un membre du zaibatsu Mitsui.
Aujourd'hui, Toshiba appartient au keiretsu Mitsui (ensemble d'entreprises, de domaines variés, entretenant entre elles des participations croisées), et a toujours bénéficié de rapports privilégiés avec la banque Mitsui et les autres membres du keiretsu. L'adhésion à un de ces groupes signifie traditionnellement fidélité envers les autres membres du même keiretsu ou à keiretsu allié. Cette loyauté peut même s'étendre, par exemple, au choix de la marque de bière consommée par les employés de l'entreprise, ce qui était le cas de Toshiba avec la marque japonaise Kirin.




</doc>
<doc id="4830" url="https://fr.wikipedia.org/wiki?curid=4830" title="Acide ribonucléique">
Acide ribonucléique

L'acide ribonucléique (ARN) est une molécule biologique présente chez pratiquement tous les êtres vivants, et aussi chez certains virus. L'ARN est très proche chimiquement de l'ADN et il est d'ailleurs en général synthétisé dans les cellules à partir d'une matrice d'ADN dont il est une copie. Les cellules utilisent en particulier l'ARN comme un support intermédiaire des gènes pour synthétiser les protéines dont elles ont besoin. L'ARN peut remplir de nombreuses autres fonctions et en particulier intervenir dans des réactions chimiques du métabolisme cellulaire.

Chimiquement, l'ARN est un polymère linéaire constitué d'un enchaînement de nucléotides. Chaque nucléotide contient un groupe phosphate, un sucre (le ribose) et une base nucléique, ou base azotée. Les nucléotides sont liés les uns aux autres par des liaisons phosphodiester. On trouve quatre bases nucléiques dans l'ARN : l'adénine, la guanine, la cytosine et l'uracile. L'ARN a de nombreuses similitudes avec l'ADN, avec cependant quelques différences importantes : d'un point de vue structurel, l'ARN contient des résidus de ribose là où l'ADN contient du désoxyribose, ce qui rend l'ARN chimiquement moins stable, tandis que la thymine de l'ADN y est remplacée par l'uracile, qui possède les mêmes propriétés d'appariement de base avec l'adénine. Sur le plan fonctionnel, l'ARN se trouve le plus souvent dans les cellules sous forme monocaténaire, c'est-à-dire de simple brin, tandis que l'ADN est présent sous forme de deux brins complémentaires formant une double-hélice. Enfin, les molécules d'ARN présentes dans les cellules sont plus courtes que l'ADN du génome, leur taille variant de quelques dizaines à quelques milliers de nucléotides, contre quelques millions à quelques milliards de nucléotides pour l'acide désoxyribonucléique (ADN).

Dans la cellule, l'ARN est produit par transcription à partir de l'ADN situé dans le noyau. L'ARN est donc une copie d'une région de l'un des brins de l'ADN. Les enzymes qui effectuent la copie s'appellent des ARN polymérases. Les ARN ainsi produits peuvent avoir trois grands types de fonctions : ils peuvent être supports de l'information génétique d'un ou plusieurs gènes codant des protéines (on parle alors d'ARN messagers), ils peuvent adopter une structure secondaire et tertiaire stable et accomplir des fonctions catalytiques (par exemple l'ARN ribosomique), ils peuvent enfin servir de guide ou de matrice pour des fonctions catalytiques accomplies par des facteurs protéiques (ce qui est par exemple le cas des microARN).

L'ARN est un acide nucléique, c'est-à-dire une molécule constituée d'un enchaînement de nucléotides. Chaque nucléotide de l'ARN est constitué d'un pentose, le ribose, dont les atomes de carbone sont numérotés de 1′ à 5′, d'une base azotée, ou base nucléique, et d'un groupe phosphate. La base nucléique est reliée par un atome d'azote au carbone 1′ du ribose. Les nucléotides sont liés les uns aux autres par des groupes phosphate, par l'intermédiaire de liaisons phosphodiester au niveau des carbones 3′ et 5′. L'ARN possède quatre bases nucléiques différentes : l'adénine (notée A), l'uracile (noté U), la cytosine (notée C) et la guanine (notée G). La thymine de l'ADN est remplacée par l'uracile dans l'ARN. La différence entre ces deux bases est le remplacement d'un groupe méthyle en position 5 de la thymine par un atome d'hydrogène dans l'uracile. Cette modification de structure ne change pas les propriétés d'appariement avec l'adénine.

Sur le plan structurel, la présence d'un atome d'oxygène sur la position 2′ du ribose influence la conformation du cycle furanose du ribose. Cet hétérocycle à cinq atomes n'est pas plan, ce qui conduit à deux conformères principaux du sucre, appelés C2′-endo et C3′-endo. Dans l'ARN, qui comporte un atome d'oxygène en position 2′, la position C3′-endo est privilégiée, ce qui modifie profondément la structure des doubles hélices comportant des brins ARN. Ces duplex d'ARN forment une hélice de type A, différente de celle qui est observée de façon majoritaire dans l'ADN classique, qui est une hélice de type B, où le désoxyribose est en conformation C2′-endo.

L'hélice de type A qu'adopte l'ARN lorsqu'il forme un duplex a des propriétés géométriques assez différentes de celles de l'hélice de type B. Tout d'abord le nombre de paires de bases par tour d'hélice est de 11 au lieu de 10 pour l'ADN en forme B. Le plan des paires de bases n'est plus perpendiculaire à l'axe de l'hélice, mais forme un angle d'environ 75° avec celui-ci. Il en résulte un déplacement de l'axe de l'hélice qui ne passe plus par le centre de l'appariement des bases, mais à l'intérieur du grand sillon. Ceci induit une augmentation du diamètre de l'hélice qui passe d'environ pour l'ADN en forme B à environ pour l'ARN en forme A. Enfin, la géométrie des deux sillons est profondément affectée : le petit sillon devient très accessible, tandis que le grand sillon devient très profond, étroit et pincé. Ceci a un impact sur la manière dont l'ARN double brin peut interagir avec des protéines, car l'étroitesse du grand sillon est une barrière à l'accessibilité de ligands protéiques.

La plupart des ARN naturels sont présents sous forme monocaténaire (simple brin) dans la cellule, contrairement à l'ADN qui est sous forme d'un double brin apparié. Les brins d'ARN se replient le plus souvent sur eux-mêmes, formant une structure intramoléculaire qui peut être très stable et très compacte. La base de cette structure est la formation d'appariements internes, entre bases complémentaires (A avec U, G avec C et, parfois, G avec U). La description des appariements internes entre les bases d'un ARN s'appelle la structure secondaire. Cette structure secondaire peut être complétée par des interactions à distance qui définissent alors une structure tridimensionnelle ou structure tertiaire.

La formation de la structure des ARN est très souvent dépendante des conditions physicochimiques environnantes et en particulier de la présence, dans la solution, de cations divalents, comme l'ion magnésium Mg. Ces cations interagissent avec les groupes phosphate du squelette et stabilisent la structure, en particulier en faisant écran à la répulsion électrostatique entre les charges négatives de ces phosphates.

La structure tertiaire des ARN est à la base de la richesse de leurs fonctions et en particulier de leur capacité à catalyser des réactions chimiques (ribozymes).

La structure secondaire d'un ARN est la description de l'ensemble des appariements internes au sein d'une molécule simple brin. Cet ensemble d'appariements induit une topologie particulière, composée de régions en hélice (tiges) et de régions non-appariées (boucles). Par extension, la structure secondaire recouvre également la description de cette topologie.

La formation de structures secondaires au sein d'un ARN simple brin résulte de l'existence de régions contenant des séquences palindromiques, qui peuvent s'apparier pour former localement une structure en double hélice. Par exemple, si l'ARN contient les deux séquences suivantes : --GUGCCACG------CGUGGCAC--, celles-ci forment une séquence palindromique, les nucléotides du second segment étant les complémentaires de ceux du premier, après inversion de leur sens de lecture ; ces deux segments peuvent alors s'apparier de manière antiparallèle pour former une région localement en duplex. La région entre les deux segments forme une « boucle » reliant les deux brins appariés, cet appariement formant une « tige ». On parle alors de structure en « épingle à cheveux », ou tige-boucle.

Dans des ARN de longueur plus importante, il peut exister des structures plus complexes qui résultent de l'appariement de plusieurs régions complémentaires ou palindromiques. En fonction de la manière dont sont « emboîtées » ces différentes régions, on obtient des éléments topologiques variés, avec des tiges ou régions appariées, et divers types de boucles :


Il n'existe pas toujours une structure unique stable pour une séquence donnée et il arrive que certains ARN puissent adopter plusieurs conformations alternatives en fonction de la liaison d'un ligand (protéine, petite molécule…) ou des conditions physico-chimiques (force ionique, pH). On peut en général suivre la formation ou la fusion de la structure secondaire d'un ARN par des mesures spectroscopiques. Ainsi, par exemple, l'absorption dans l'ultraviolet des bases de l'ARN est plus importante à l'état déplié qu'à l'état replié (phénomène d'hyperchromicité).

Au-delà de la topologie des boucles et des hélices composées de paires de bases standard, un ARN peut adopter une structure tridimensionnelle compacte, ou "structure tertiaire", comme une protéine. À l'intérieur de cette structure, les hélices canoniques sont complétées par des appariements non canoniques, c'est-à-dire distincts des appariements classiques, de type Watson-Crick ( et ) et bancals (, ). On a observé une grande variété de ces appariements dans les structures tridimensionnelles d'ARN résolues par cristallographie aux rayons X ou par résonance magnétique nucléaire. On trouve par exemple des appariements Hoogsteen et des appariements « en cisaille » ("sheared"). Il existe également des interactions base-ribose, notamment avec l'hydroxyle 2', qui peut former des liaisons hydrogène. Une nomenclature systématique de toutes ces interactions a été proposée par Éric Westhof et ses collaborateurs. Plus de 150 types d'appariements ont été observés et ont été regroupés en douze grandes familles. Ces appariements non canoniques impliquent toujours des liaisons hydrogène entre les bases, qui sont coplanaires, comme dans les paires Watson-Crick.

Des appariements canoniques ou non canoniques peuvent intervenir entre des régions distantes de la structure secondaire, souvent localisées dans des boucles, ce qui permet de stabiliser un repliement compact de la structure.

Parmi ces interactions non canoniques à grande distance figurent :




Les principales différences entre les deux molécules sont que :


Les trois premières différences donnent à l'ARN une stabilité bien moindre que celle de l'ADN :




D'un point de vue évolutif, certains éléments permettent de penser que l'ARN serait antérieur à l'ADN comme support de l'information génétique, ce qui expliquerait ses fonctions plus étendues et sa généralisation. L'ADN serait apparu plus tard et n'aurait supplanté l'ARN que pour le rôle de stockage à long terme, en raison de sa plus grande stabilité.

La synthèse d'une molécule d'ARN à partir de l'ADN s'appelle la transcription. C'est un processus complexe qui fait intervenir une enzyme de la famille des ARN polymérases ainsi que des protéines associées. Les différentes étapes de cette synthèse sont l'initiation, l'élongation et la terminaison. Le processus de synthèse des ARN est sensiblement différent chez les organismes procaryotes et chez les cellules eucaryotes. Enfin, après la transcription proprement dite, l'ARN peut subir une série de modifications post-transcriptionnelles dans le cadre d'un processus de maturation au cours duquel sa séquence et sa structure chimique peuvent être modifiées (voir plus loin).

Le démarrage de la transcription d'un ARN par une ARN polymérase s'effectue au niveau d'une séquence spécifique sur l'ADN, appelée promoteur. Ce promoteur comporte un ou plusieurs éléments de séquence conservés sur lesquels se fixent en général des protéines spécifiques, les facteurs de transcription. Juste en amont du site d'amorçage de la transcription, l'élément proximal est en général riche en nucléotides T et A, et est pour cette raison appelé boîte TATA chez les eucaryotes ou boîte de Pribnow chez les bactéries. Les facteurs de transcription favorisent le recrutement de l'ARN polymérase sur le promoteur et l'ouverture du duplex d'ADN. Il se forme alors ce qu'on appelle une "bulle de transcription" avec l'ADN ouvert, dont l'un des brins (la matrice) est hybridé avec l'ARN en cours de synthèse.

Une fois l'ARN polymérase fixée sur le promoteur et la bulle de transcription formée, elle synthétise les premiers nucléotides de manière statique sans quitter la séquence du promoteur. Les facteurs de transcription se détachent et l'ARN polymérase devient processive. Elle transcrit alors l'ARN dans le sens 5' vers 3', en utilisant l'un des deux brins de l'ADN comme matrice et des ribonucléotides triphosphates (ATP, GTP, CTP et UTP) comme précurseurs.

"In vivo", chez "Escherichia coli", la vitesse d'allongement de l'ARN polymérase est d'environ 50 à 90 nucléotides par seconde.

La terminaison de la transcription de l'ARN procède selon des mécanismes complètement différents chez les bactéries et chez les eucaryotes.

Chez les bactéries, le mécanisme principal de terminaison fait intervenir une structure particulière de l'ARN, le terminateur, composé d'une tige-boucle stable suivie d'une série de résidus d'uridine (U). Lorsque l'ARN polymérase synthétise cette séquence, le repliement de la tige d'ARN provoque une pause de la polymérase. L'ARN, qui n'est plus apparié à l'ADN matrice que par une série d'appariements A-U faibles, se détache, sans intervention d'autres facteurs protéiques. La terminaison peut aussi se faire via l'intervention d'un facteur protéique spécifique, le facteur Rho.

Chez les eucaryotes, la terminaison de la transcription par l'ARN polymérase II est couplée à la polyadénylation. Deux complexes protéiques, et reconnaissent les signaux de polyadénylation (5′-AAUAAA-3′) et de coupure de l'ARN. Ils clivent l'ARN, induisent le détachement de la polymérase de l'ADN et recrutent la poly-A polymérase qui ajoute la queue poly(A) (voir plus bas).

La maturation des ARN comprend un ensemble de modifications post-transcriptionnelles principalement observées chez les eucaryotes et jouent un rôle important dans le devenir de l'ARN maturé. Les principales modifications sont l'adjonction d'une coiffe en 5′, la polyadénylation en 3′, l'épissage, l'introduction de modifications chimiques au niveau de la base ou du ribose et enfin l'édition.

La coiffe, ou en anglais, est un nucléotide modifié qui est ajouté à l'extrémité 5' de l'ARN messager dans les cellules d'eucaryotes. Elle se compose d'un résidu de guanosine méthylé lié par une liaison 5′-5′ triphosphate au premier nucléotide transcrit par l'ARN polymérase. Cette modification est introduite dans le noyau de la cellule, par l'action successive de plusieurs enzymes : polynucléotide 5'-phosphatase, ARN guanylyltransférase, méthyltransférases.

La coiffe joue plusieurs rôles : elle accroît la stabilité de l'ARN en le protégeant de la dégradation par des exonucléases 5′-3′ et permet également le recrutement de facteurs d'initiation de la traduction nécessaires à la fixation du ribosome sur les ARN messagers cellulaires. La coiffe est donc essentielle à la traduction de la plupart des ARNm.

La polyadénylation consiste en l'addition d'une extension à l'extrémité 3′ de l'ARN composée exclusivement de ribonucléotides de type adénosine (A). Pour cette raison, l'extension est appelée "queue poly(A)". Bien que composée de nucléotides standard, cette queue poly(A) est ajoutée de façon post-transcriptionnelle par une enzyme spécifique appelée poly(A) polymérase et n'est pas codée dans l'ADN génomique. La queue poly(A) est trouvée principalement à l'extrémité des ARN messagers. Chez les eucaryotes, la polyadénylation des ARNm est nécessaire à leur traduction par le ribosome et participe à leur stabilisation. La queue poly(A) est en particulier reconnue par la PABP ("poly(A)-binding protein", « protéine de liaison de la poly(A) »).

Chez les bactéries et dans certaines mitochondries, la polyadénylation des ARN est au contraire un signal de dégradation.

L'épissage est une modification post-transcriptionnelle qui consiste en l'élimination des introns et la suture des exons dans les ARN messagers et dans certains ARN structurés comme les ARNt. Présents chez les organismes eucaryotes, les introns sont des segments d'ARN qui sont codés dans le génome et transcrits dans l'ARN précurseur, mais qui sont éliminés du produit final. Dans la plupart des cas, ce processus fait intervenir une machinerie spécifique complexe appelée le spliceosome. L'épissage se produit dans le noyau des cellules eucaryotes, avant l'export de l'ARN maturé vers le cytoplasme.

Après leur transcription par l'ARN polymérase, certains ARN subissent des modifications chimiques sous l'action d'enzymes spécifiques. Les principaux ARN subissant des modifications sont les ARN de transfert et les ARN ribosomiques. On peut également considérer que les méthylations intervenant dans la synthèse de la coiffe sont des modifications de nucléotides particulières. Dans le cas général, les modifications peuvent porter soit sur la base, soit sur le ribose. Les principales modifications rencontrées sont :



Dans les ARN de transfert, l'introduction de nucléotides modifiés contribue à augmenter la stabilité des molécules.

L'édition des ARN consiste en une modification de la séquence de l'acide ribonucléique, postérieure à la transcription par l'ARN polymérase. À l'issue du processus d'édition, la séquence de l'ARN est donc différente de celle de l'ADN. Les changements opérés peuvent être la modification d'une base, la substitution d'une base ou encore l'ajout d'une ou plusieurs bases. Ces modifications sont effectuées par des enzymes qui agissent sur l'ARN, comme les cytidine désaminases, qui transforment chimiquement les résidus de cytidine en uridine.

Dans les cellules, les ARN remplissent quatre rôles distincts et complémentaires :





Une classe particulière d'ARN, les ARN de transfert, se trouve à l'interface de plusieurs de ces fonctions en guidant les acides aminés lors de la traduction.

Enfin, le génome de certains virus est exclusivement constitué d'ARN et non d'ADN. C'est en particulier le cas des virus de la grippe, du SIDA, de l'hépatite C, de la poliomyélite ou encore du virus Ebola. Suivant les cas, la réplication de ces virus peut passer par un intermédiaire ADN (rétrovirus), mais peut aussi se faire directement d'ARN en ARN.

L'ARN est donc une molécule très polyvalente, ce qui a conduit Walter Gilbert, co-inventeur du séquençage de l'ADN, à proposer en 1986 une hypothèse selon laquelle l'ARN serait la plus ancienne de toutes les macromolécules biologiques. Cette théorie, dite "RNA world hypothesis" (« hypothèse du monde à ARN »), permet de s'affranchir d'un paradoxe de l'œuf et de la poule qui survient lorsqu'on cherche à savoir qui des protéines (catalyseurs) et de l'ADN (information génétique) sont apparus en premier. Dans ce modèle, l'ARN, capable de combiner à la fois les deux types de fonctions, serait le précurseur universel.

L'information génétique contenue au sein de l'ADN n'est pas utilisée directement par la cellule pour synthétiser des protéines. Celle-ci utilise pour cela des copies transitoires de l'information génétique que sont les ARN messagers ou ARNm. Chaque ARN messager porte un ou, parfois, plusieurs cistrons, c'est-à-dire les instructions pour former une seule protéine. Il correspond donc à la copie d'un seul des gènes du génome (on parle alors d'ARNm monocistronique) ou parfois de quelques-uns (ARNm polycistronique).

L'ARN messager ne contient la copie que d'un seul des deux brins de l'ADN, celui qui est codant, et non la séquence complémentaire. Par rapport à la séquence du gène contenue dans l'ADN du génome, celle de l'ARNm correspondant peut contenir des modifications, en particulier dues à l'épissage (voir plus haut) qui élimine les régions non codantes. L'ARN messager synthétisé dans le noyau de la cellule est exporté dans le cytoplasme pour être traduit en protéine. Contrairement à l'ADN, qui est une molécule pérenne, présente pendant toute la vie de la cellule, les ARN messagers ont une durée de vie limitée, de quelques minutes à quelques heures, après quoi ils sont dégradés et recyclés.

Un ARN messager comporte trois régions distinctes : une région 5′ non traduite dite 5′-UTR, située en amont du ou des cistrons qu'il porte ; une région codante correspondant à ce ou à ces cistrons ; et enfin, une région 3′ non traduite dite 3′-UTR. Les ARN messagers sont traduits en protéines par les ribosomes. La région 5′ non traduite contient en général les signaux de traduction permettant le recrutement du ribosome sur le cistron. Le processus de traduction fait également intervenir les ARN de transfert qui apportent au ribosome les acides aminés nécessaires à la biosynthèse des protéines. Au sein du ribosome, par leur anticodon, les ARNt s'apparient successivement aux triplets de bases, ou codons, de la séquence de l'ARNm. Lorsque l'appariement codon-anticodon est correct, le ribosome ajoute l'acide aminé porté par l'ARNt à la protéine en cours de synthèse. Les correspondances entre codons et acides aminés constituent le code génétique.

La fonction des ARN messagers est multiple. Ils permettent d'une part de préserver la matrice d'ADN originale, qui n'est pas directement utilisée pour la traduction, la cellule ne travaillant que sur la copie qu'est l'ARNm. L'existence d'ARN messagers offre surtout à la cellule un mécanisme crucial de régulation du cycle de production des protéines à partir du génome. Le besoin cellulaire en telle ou telle protéine peut varier en fonction de l'environnement, du type de cellule, du stade de développement. La synthèse protéique doit donc être activée ou arrêtée en fonction des conditions cellulaires. La régulation de la transcription de l'ADN en l'ARNm répond à cette nécessité et est contrôlée par des facteurs de transcription spécifiques agissant sur les promoteurs des gènes cibles. Lorsque la quantité d'une protéine donnée est suffisante, la transcription d'ARNm est inhibée, celui-ci est progressivement dégradé et la production protéique cesse. Il est donc important que l'ARNm soit une molécule transitoire, afin de pouvoir réaliser cette régulation essentielle.

Les ARN de transfert, ou ARNt, sont de courts ARN, longs d'environ 70 à 100 ribonucléotides, impliqués dans l'adressage des acides aminés vers les ribosomes lors de la traduction.

Les ARN de transfert ont une structure caractéristique en feuille de trèfle, composée de quatre tiges appariées. L'une de ces tiges est terminée par une boucle qui contient l'anticodon, le triplet de nucléotides qui s'apparie au codon lors de la traduction d'un ARNm par le ribosome. À l'autre extrémité, l'ARNt porte l'acide aminé correspondant attaché par une liaison ester à son extrémité 3′-OH. Cette estérification est catalysée par des enzymes spécifiques, les aminoacyl-ARNt synthétases. En trois dimensions, la structure en feuille de trèfle se replie en « L », avec l'anticodon à une extrémité et l'acide aminé estérifié à l'autre extrémité.

Toutes les cellules vivantes contiennent un ensemble d'ARNt différents portant les différents acides aminés et capable de lire les différents codons.

Les ARN de transfert sont parfois désignés comme des « adaptateurs » entre la séquence génétique et la séquence protéique. C'est Francis Crick qui a proposé l'existence de ces adaptateurs, avant même leur découverte en 1958.

La découverte d'ARN possédant des capacités catalytiques a été faite dans les années 1980, en particulier par l'équipe de Thomas Cech, qui travaillait sur les introns du gène de l'ARN ribosomique du protozoaire cilié "Tetrahymena", et celle de Sidney Altman, qui étudiait la ribonucléase P, l'enzyme de maturation de l'ARNt. Cech et Altman ont été récompensés par le prix Nobel de chimie en 1989 pour cette découverte.

Dans ces deux cas, l'ARN seul est capable de catalyser une réaction de clivage (coupure) ou de transestérification spécifique en l'absence de protéine. Ces ARN catalytiques ont été appelés "ribozymes", car ce sont des enzymes constituées d'acide ribonucléique. Dans le cas de l'intron de "Tetrahymena", il s'agit d'un auto-épissage, l'intron étant son propre substrat, tandis que la ribonucléase P est une enzyme agissant en "trans", sur des substrats multiples.

Depuis ces découvertes initiales, d'autres ribozymes naturels ont été identifiés :






De manière générale, dans tous ces ribozymes, c'est leur repliement spécifique qui leur permet d'effecteur la reconnaissance de leur substrat et la catalyse, comme dans le cas des enzymes protéiques.

Les ARN guides sont des ARN qui s'associent à des enzymes protéiques et servent à en guider l'action sur des ARN ou des ADN de séquence complémentaire. L'ARN guide s'apparie à l'acide nucléique substrat et permet de cibler l'activité de l'enzyme. On a identifié plusieurs types :





Certains ARN jouent un rôle de régulateurs directs de l'expression génétique. C'est en particulier le cas d'ARN non codants possédant des régions complémentaires d'ARN messagers cellulaires et qui peuvent donc s'y apparier pour former localement un double brin d'ARN. Ces ARN antisens peuvent être issus du même locus génétique que leur ARN cible, par transcription du brin complémentaire, on parle alors d'ARN "cis"-régulateurs. Ils peuvent aussi être issus de la transcription d'une autre région du génome, ce sont alors des ARN "trans"-régulateurs.

L'appariement de l'ARN régulateur avec son ARN messager cible peut agir sur la capacité de ce dernier à être traduit par le ribosome ou sur sa stabilité, ce qui aboutit à une régulation de la traduction du ou des gènes portés par l'ARN messager. Chez les bactéries, il existe ainsi de nombreux exemples d'ARN antisens "cis"- ou "trans"-régulateurs qui bloquent le site de démarrage de la traduction. Par exemple, le gène codant la porine OmpF est régulé par un ARN antisens appelé MicF.

Chez les eucaryotes, il existe aussi de grands ARN régulateurs, qui interviennent dans des processus de régulation épigénétique. L'exemple le mieux connu est celui de l'ARN Xist chez les mammifères. Celui-ci inactive non pas un gène, mais un chromosome entier. Xist recouvre l'un des deux chromosomes X de chaque cellule chez les individus femelles qui devient ainsi inactif. Un seul des deux chromosomes de la paire XX est ainsi actif, ce qui permet d'avoir le même taux d'expression des gènes portés par le chromosome X que chez les individus mâles, qui n'en ont qu'un. L'inactivation du X est un processus aléatoire, ce qui peut conduire à l'expression de différents phénotypes par différentes cellules, chez la même femelle. C'est par exemple le cas pour la couleur du pelage chez les chattes.

L'ARN est utilisé aujourd'hui dans un certain nombre d'applications en biologie moléculaire, en particulier grâce au processus d'interférence par ARN, qui consiste en l'introduction dans des cellules eucaryotes de courts fragments d'ARN double-brin appelés « petits ARN interférents ». Longs d'une vingtaine de paires de bases, ces petits ARN interférents (pARNi) sont utilisés par une machinerie cellulaire capable de dégrader les ARNm de manière spécifique. Seuls les ARNm contenant une séquence correspondant à celle du pARNi sont dégradés, ce qui permet de diminuer sélectivement l'expression d'une protéine donnée. Cette approche technologique est beaucoup plus simple et rapide que l'établissement de lignées murines inactivées ("knock-out") et s'appelle un "knock-down".

Des essais d'utilisation de cette technique à des fins thérapeutiques sont envisagés, par exemple en ciblant des gènes viraux pour lutter contre des infections, ou des oncogènes, dans le cas de cancers. Ils nécessitent cependant de stabiliser les petits ARN interférents (pARNi) pour éviter leur dégradation par des ribonucléases et de cibler leur action vers les cellules concernées.

Les acides nucléiques ont été découverts en 1868 par Friedrich Miescher. Miescher appela la nouvelle substance « nucléine » car elle se trouvait dans le noyau des cellules. La présence d'acides nucléiques dans le cytoplasme de la levure fut identifiée en 1939 et leur nature ribonucléique fut établie, contrairement aux chromosomes qui contenaient de l'ADN avec des désoxyriboses. 

Vers 1940, le biologiste belge Jean Brachet étudie des molécules jusque-là peu caractérisées, que l'on appelle encore à l'époque les "acides thymonucléiques et zymonucléiques" (respectivement l'ADN et l'ARN). Il découvre que l'acide thymonucléique est un composant des chromosomes et qu'il est synthétisé lorsque les cellules se divisent après la fécondation. Il met en évidence l'existence d'acides zymonucléiques (ARN) dans tous les types cellulaires: dans le noyau, le nucléole et le cytoplasme de toutes les cellules (alors que l'on pensait à l'époque que ces molécules étaient caractéristiques des cellules végétales et des eucaryotes inférieurs tels que les levures). Enfin, il montre que ces acides sont particulièrement abondants dans les cellules (plus particulièrement dans l'ergastoplasme) qui sont très actives en termes de synthèse protéique. Les bases fondamentales de la biologie moléculaire étaient établies. Nous étions en 1940. Dans l'après-guerre, Brachet est rejoint par le biologiste moléculaire belge Raymond Jeener qui participera activement aux recherches sur le rôle de l'ARN dans la biosynthèse des protéines.

À la fin des années 1950, Severo Ochoa parvint à synthétiser "in vitro" des molécules d'ARN au moyen d'une enzyme spécifique, la polynucléotide phosphorylase, ce qui permit l'étude des propriétés chimiques et physiques de l'ARN.

Le rôle de l'ARN comme « messager » intermédiaire entre l'information génétique contenue dans l'ADN et les protéines fut proposé en 1960 par Jacques Monod et François Jacob à la suite d'une discussion avec Sydney Brenner et Francis Crick.
La démonstration de l'existence de l'ARN messager a été faite par François Gros.
Ensuite, le déchiffrage du code génétique a été réalisé par Marshall Nirenberg dans la première moitié des années 1960. Il utilisa pour cela des ARN synthétiques de séquence nucléotidique connue dont il étudia les propriétés de codage.

Les ribosomes furent observés pour la première fois par le biologiste belge Albert Claude au début des années 1940. Par des techniques de fractionnement subcellulaire et de microscopie électronique, il mit en évidence des « petites particules » de nature ribonucléoprotéique, présentes dans tous les types de cellules vivantes. Il les baptisa « microsomes », plus tard renommés ribosomes.

La structure secondaire des ARNt a été établie par Robert Holley, qui est parvenu à purifier et à analyser la séquence de l'ARNt spécifique de l'alanine en 1964. Ce fut un progrès majeur dans la compréhension du déchiffrage du message génétique porté par les ARN messagers. La structure tridimensionnelle d'un ARNt a été résolue en 1974, indépendamment, par les équipes de Aaron Klug et Alexander Rich, montrant pour la première fois la structure complexe d'un ARN. L'existence de propriétés catalytiques des ARN a été établie indépendamment par Sidney Altman et Tom Cech en 1982, sur la ribonucléase P d'une part et sur les introns autoépissables d'autre part. La résolution de la structure des sous-unités individuelles du ribosome en 2000 par les équipes de Tom Steitz, Ada Yonath et Venki Ramakrishnan, puis celle du ribosome entier par l'équipe de Harry Noller en 2001, constituèrent un progrès essentiel dans la compréhension du mécanisme central de la biologie qu'est la traduction des ARNm en protéines. De plus, cela permit de montrer, entre autres, que le ribosome était aussi un ribozyme.

Durant les années 1970, Timothy Leary, dans son ouvrage "La Politique de l'Extase", verra dans l'ARN la promesse d'une future modification de la conscience (possiblement via de nouvelles drogues, et/ou d'exercices spirituels), dont il serait une composante agrandissant les capacités d'apprentissage de celui qui se livrerait à de telles expériences.

L'hypothèse du monde à ARN est une hypothèse suivant laquelle l'ARN serait le précurseur de toutes les macromolécules biologiques et particulièrement de l'ADN et des protéines qui auraient permis dans un environnement abiotique (caractérisé par une chimie prébiotique en partie hypothétique) l'apparition de premières cellules vivantes, c'est-à-dire, formant un compartiment, et comportant de l'information et des sous-systèmes métaboliques. 

Dans le cadre de l'étude des origines de la vie, cette hypothèse permet une explication de l'apparition des différentes fonctions biologiques via la constitution de certains blocs biomoléculaires à partir d'intermédiaires prébiotique plausibles et de molécules basées sur le carbone. Il a été démontré en 2009 par l'équipe de que des précurseurs plausibles des ribonucléotides, des acides aminés et des lipides peuvent tous être obtenus par homologation réductrice du cyanure d'hydrogène et de certains de ses dérivés. Chacun des sous-systèmes cellulaires connus pourrait donc être expliqué par la chimie du carbone, avec des réactions catalysées par la lumière ultraviolette "a priori" très présente avant l'apparition de la couche d'ozone, à partir du sulfure d'hydrogène comme agent réducteur. Le cycle photoréducteur pourrait lui-même être accéléré par le cuivre [Cu(I)-Cu(II)].




</doc>
<doc id="4841" url="https://fr.wikipedia.org/wiki?curid=4841" title="John Ruskin">
John Ruskin

John Ruskin, né le à Bloomsbury à Londres, mort le à Coniston (Cumbria), est un écrivain, poète, peintre et critique d'art britannique.

Fils unique d'une riche famille, il est éduqué à domicile, avec une insistance particulière sur l'art et la religion. Il poursuit son éducation en dilettante, en tant qu'auditeur libre à Oxford. Malgré des problèmes de santé, il y obtient son MA en 1843. Surtout, il s'y lie d'amitié avec nombre d'intellectuels.

Il est publié dès son adolescence. Grâce à la fortune de sa famille, il peut consacrer sa vie à l'écriture. Il devient rapidement célèbre dans les années 1840 grâce à son travail de critique "Modern Painters" (1843 à 1860) où il propose une nouvelle façon d'appréhender l'art. Il écrit ensuite "The Seven Lamps of Architecture" en 1849 et surtout "The Stones of Venice" en 1853. Il fait aussi passer ses idées par l'enseignement. Il participe à la création de l'"University Museum", donne des cours de dessin au Working Men's College, un établissement de formation continue fondé par ses amis socialistes chrétiens. Il en donne aussi dans une école pour jeunes filles et par correspondance. En 1870, il devient le premier titulaire de la chaire Slade pour l'enseignement des beaux-arts à Oxford.

Son mariage avec Effie Gray, annulé pour non-consommation, continue à alimenter de nos jours des légendes et des suppositions variées. Effie épouse très vite le peintre John Everett Millais, qui est membre du mouvement préraphaélite dont Ruskin est le mécène et le soutien après s'être engagé pour Turner.

John Ruskin était le fils unique de John James Ruskin (1785 – 1864) et de Margaret Cox ou Cock (1781 – 1871). Les époux étaient cousins germains. Les deux familles pratiquaient le commerce de l'alcool. Le grand-père maternel de John Ruskin avait un pub à Croydon. Son grand-père paternel, John Thomas Ruskin (1761 – 1817), originaire d'Édimbourg avait migré d'Écosse à Londres pour s'installer comme marchand. Le père de John Ruskin, quant à lui, était importateur de sherry. Il commença par solder les dettes de l'entreprise familiale avant de faire fortune dans la société "Ruskin, Telford, and Domecq".

Les parents de John Ruskin s'étaient fiancés en 1809, mais les dettes familiales et l'opposition parentale avaient retardé le mariage. En 1817, John James Ruskin était riche et ses parents étaient morts (il semblerait que son père se fût suicidé peu de temps après le décès de son épouse). La noce se déroula en 1818. John Ruskin naquit l'année suivante, dans la maison familiale donnant sur Brunswick Square dans Bloomsbury, un des beaux quartiers de Londres. En 1823, la famille déménagea pour Herne Hill. Dans ses "Præterita", John Ruskin décrit une enfance assez solitaire, mais heureuse.

Jusqu'à ses quatorze ans, John Ruskin fut éduqué à domicile, soit par ses parents, soit par des précepteurs. Son père lui transmit son goût pour le romantisme (Walter Scott, Lord Byron ou Wordsworth). John James Ruskin avait dû arrêter ses études avant l'université où il avait désiré faire du droit, pour se mettre à travailler. Il semblerait qu'il ait tenté de se consoler de la frustration qu'il avait alors ressentie en permettant à son fils de faire ce qui lui plaisait. Le jeune John était encouragé par son père à dessiner et à écrire. Chacun de ses poèmes lui était ainsi payé un demi-penny le vers. Dès ses douze ans, il avait entrepris d'écrire un dictionnaire, manuscrit, de minéralogie.

Sa mère lui donna une stricte éducation religieuse, de tendance évangélique. Dès ses trois ans, elle lui faisait lire des passages de la Bible tous les matins. Il en apprit aussi par cœur. Cette éducation eut des conséquences sur le reste de la vie de John Ruskin. Elle lui fournit la base de ses réflexions aussi bien littéraires que juridiques. Il semble que le puritanisme fut à l'origine de son attrait sensuel pour l'art et de son rejet des choses du corps.

Dès le début, cette éducation fut complétée de deux façons. D'abord, ses parents lui firent régulièrement visiter les hauts-lieux culturels de Grande-Bretagne : paysages ou demeures célèbres. À partir de 1833, ces voyages furent élargis au continent (France, Suisse puis Italie). Dès lors, entre deux voyages, il passa ses matinées dans une école proche de chez lui, tenue par le révérend Thomas Dale. Au début de 1836, alors que ce dernier était devenu professeur de littérature britannique au King's College de Londres, Ruskin commença à y suivre des cours. En octobre de la même année, il s'inscrivit au Christ Church (Oxford) en tant qu'auditeur libre. Il suivit les cours à partir de janvier de l'année suivante. Il ne quitta cependant pas le giron familial car sa mère vint s'installer à Oxford, rejointe par son époux tous les week-ends. Elle disait qu'elle était venue pour veiller sur la santé fragile de son fils.

À la différence de nombre de ses condisciples, John Ruskin passait son temps dans les livres, ce qui lui valut l'animosité de certains. Il devint cependant rapidement proche des spécialistes de lettres classiques, comme Charles Thomas Newton, mais aussi des géologues comme ou Henry Liddell. Ruskin attira l'attention du géologue et théologien William Buckland pour les cours duquel il fournit des dessins. Il adhéra aussi à l’"Oxford Society for the Preservation of Gothic Architecture" et se présenta au prix Newdigate de poésie qu'il finit par remporter lors de sa troisième tentative en 1839. Ce fut Wordsworth lui-même qui lui remit son prix. Cependant, ce furent ses dernières productions poétiques.

À l'automne 1839, ses professeurs lui suggérèrent de se présenter en candidat libre aux examens de baccalauréat. Il était amoureux d'Adèle Domecq, fille d'un des partenaires dans la firme paternelle. Quand il apprit son mariage en avril 1840, il se mit à tousser du sang et dut renoncer à passer ses examens. Il ne put se présenter qu'en avril 1842. L'université lui accorda alors un diplôme honoraire. Il obtint cependant son MA en octobre 1843 ce qui lui permit de signer ses premiers ouvrages d'un « "Graduate of Oxford" » (« diplômé d'Oxford »).

Pendant sa convalescence, en 1840-1841, John Ruskin voyagea avec ses parents en Italie, principalement à Naples et Rome. Dans cette dernière ville, il fit la connaissance du peintre , un ami de John Keats. Severn épouserait plus tard Joan Agnew Ruskin, une cousine de John Ruskin. Il veillerait sur les derniers jours de celui-ci. Il rencontra aussi le peintre George Richmond qui lui fit découvrir les peintres italiens et que Ruskin consulterait à de nombreuses reprises lors de sa rédaction de ses "Modern Painters".

L'année suivante, la famille Ruskin se rendit en Suisse avant de descendre le Rhin. Au cours de ce séjour, John Ruskin eut l'idée d'écrire un pamphlet de critique artistique. Le premier tome de "Modern Painters: their Superiority in the Art of Landscape Painting to the Ancient Masters" parut en mai 1843, le second en 1846, le quatrième en 1856. Le cinquième tome parut en 1860. En 1845, il voyagea pour la première fois sans ses parents, en Suisse, en Italie : Florence, Pise et Venise où il découvrit les primitifs italiens, Fra Angelico et Le Tintoret (dont ses œuvres à la Scuola Grande de San Rocco), ainsi qu'en France où il passa beaucoup de temps au Louvre. À Venise, Ruskin observa que la ville subissait les assauts délétères de deux forces opposées : la restauration et le délabrement. Ce voyage nourrit le deuxième tome de ses "Modern Painters".

Les deux premiers tomes furent appréciés par Charlotte Brontë, Wordsworth ou Elizabeth Gaskell, mais la critique établie, comme dans "The Athenaeum" fut moins favorable. Malgré tout, la carrière littéraire de Ruskin était lancée. Il entra dans les cercles littéraires de ou Samuel Rogers.

Son voyage en Italie lui avait fait découvrir la beauté et le délabrement des monuments romans et gothiques de ce pays. De retour en Grande-Bretagne, il se tourna vers l'étude de l'architecture, principalement celle du "Gothic Revival". Dès 1844, il avait travaillé avec l'architecte George Gilbert Scott à la restauration d'une église de Camberwell. Avec un de ses anciens condisciples, Edmund Oldfield, Ruskin en dessina un des vitraux. À l'été 1848, il visita la cathédrale de Salisbury puis à l'automne les églises de Normandie. Cependant, ce voyage, qui était aussi son voyage de noces, n'alla pas plus au sud à cause des événements parisiens et surtout vénitiens. De ses réflexions et voyages, naquit en mai 1849 "The Seven Lamps of Architecture", le premier ouvrage à être ouvertement signé John Ruskin. En 1849, Edmund Oldfield était présent avec Ruskin à la fondation de l'. En 1853, George Gilbert Scott fit appel aux lumières de Ruskin lors de son réaménagement dans le style roman d'une église de Camden.

À la fin de 1851, le célèbre artiste aquarelliste William Turner mourut. Ruskin qui en avait été très proche devint son exécuteur testamentaire. Cependant, la tâche se révéla rapidement insurmontable. Il découvrit aussi des aspects sombres de l'artiste qu'il ne soupçonnait pas. Lorsque la succession fut définitivement réglée, toutes les œuvres de William Turner rejoignirent la National Gallery en 1856. L'atelier du peintre recelait plus de aquarelles. Ruskin obtint le droit d'en exposer 400, de son choix, dans des salles qu'il dessina et fit aménager lui-même dans la National Gallery. Il se chargea aussi de publier des catalogues de ces œuvres. Certains des dessins et esquisses de l'artiste avaient un caractère pornographique dont la possession même aurait pu être illégale. Ruskin donna son accord pour qu'elles soient détruites.

John Ruskin épousa Euphemia Chalmers Gray, dite Effie Gray, le , à Perth en Écosse. Elle était la fille de George Gray, avocat ami de la famille. Les futurs époux s'étaient rencontrés quand elle avait douze ans et lui vingt et un. En 1841, elle lui avait demandé de lui écrire un conte de fées. "The King of the Golden River" fut la seule œuvre de fiction et un des principaux succès littéraires de Ruskin, après sa parution en 1850 avec des illustrations de Richard Doyle. Ce ne fut que lorsque la jeune fille eut dix-neuf ans que Ruskin la remarqua au cours d'un de ses voyages en Écosse pour soigner une nouvelle dépression. Il se remettait de ses sentiments pour Adèle Domecq, puis pour Charlotte Lockhart, petite-fille de Walter Scott et fille de John Gibson Lockhart. Ruskin décida qu'il était amoureux d'Effie Gray et lui fit un peu la cour. Cependant, sa demande en mariage et la réponse positive se firent au cours d'un échange de lettres après son retour à Londres. Les parents de Ruskin ne s'opposèrent pas au mariage, mais n'y assistèrent pas. Il se déroula en effet à Bowerswell, résidence de la famille Gray, à Perth. Cette maison avait été auparavant celle des Ruskin où le grand-père s'était suicidé. La nuit de noces se déroula à Blair Atholl et le voyage de noces, prévu à Venise s'arrêta en Normandie à cause des événements politiques de 1848.

Dans une lettre à son père en 1854, Effie Ruskin, décrit le fiasco de la nuit de noces. Elle y confie son ignorance quant aux relations sexuelles et écrit que Ruskin « avait été dégoûté par mon corps le premier soir ». Cette phrase donna lieu à de nombreuses spéculations et légendes. La principale est que Ruskin aurait été écœuré par la découverte des poils pubiens de son épouse, car, esthète, il n'aurait jamais vu que des nus artistiques. Or, il semblerait qu'il ait eu accès, grâce à ses condisciples d'Oxford, à des images érotiques et pornographiques l'ayant informé sur cet aspect. Une autre hypothèse aurait été qu'Effie aurait eu ses règles ce soir-là. Ce qui est sûr, en revanche, c'est que les deux époux seraient convenus de repousser la consommation de leur mariage jusqu'aux vingt-cinq ans d'Effie, au moins, afin d'être libres de voyager, ce qu'ils désiraient ardemment tous deux, sans être dérangés par une ou plusieurs maternités.

Après le voyage de noces, les époux s'installèrent chez Ruskin, qui vivait toujours chez ses parents. Il se replongea dans ses travaux intellectuels et ne fut pas un époux très attentionné. Les relations entre Effie Ruskin et ses beaux-parents se dégradèrent rapidement. Elle tomba malade et retourna chez ses propres parents au début de l'année 1849. Les époux ne se virent pas pendant neuf mois. Il finit par aller la rechercher en Écosse et ils partirent, enfin, pour Venise. Ils y séjournèrent longuement en 1849-1850, passèrent onze mois à Londres, mais pas dans la maison familiale, puis retournèrent à Venise en 1851-1852. Cette période fut la plus heureuse de la vie du couple. Éloignés de leurs familles respectives, il pouvait travailler et elle entretenir une véritable vie mondaine, autant à Venise qu'à Londres. Ils fréquentaient la bonne société dans l'une et l'autre ville. À Londres, ils allaient régulièrement chez Charles Lock Eastlake, président de la Royal Academy. Ruskin n'appréciait pas ses tableaux, mais les deux femmes étaient amies. À cette époque, Ruskin fit la connaissance du socialiste chrétien F. D. Maurice. Ce fut enfin le poète Coventry Patmore qui présenta le couple Ruskin au cercle préraphaélite où ils rencontrèrent John Everett Millais. John Ruskin se fit rapidement le mécène et le champion de celui-ci, ainsi que de John Frederick Lewis.

Un nouveau séjour à Venise se termina mal : les bijoux d'Effie avaient été dérobés. Un officier britannique de l'armée autrichienne fut soupçonné. Il semblerait que John Ruskin ait alors dû refuser de se battre en duel. De retour à Londres à l'été 1852, le couple s'installa chez lui, d'abord à Herne Hill puis à Mayfair. Si les parents de Ruskin n'habitaient pas avec eux, leur présence se faisait cependant pesante : ils ne cessaient de critiquer le train de vie que, selon eux, Effie imposait. Et puis Effie se rendit à l'évidence : son époux ne consommerait jamais leur mariage. Elle en conçut une frustration de plus en plus grande. Au printemps 1853, Millais présenta à l'exposition annuelle de la Royal Academy son "Order of Release" pour lequel Effie avait posé. Ruskin réitéra l'invitation qu'il avait déjà faite à Millais de passer des vacances avec eux. Le groupe d'amis séjourna tout l'été en Écosse. Millais commença le portrait en pied que son mécène lui avait commandé. Il fit aussi diverses esquisses d'Effie en vue d'un tableau qu'il ne réalisa jamais. Ruskin continuait son travail solitaire : la préparation d'une série de conférences pour l'automne à Édimbourg. Livrés à eux-mêmes, Millais et Effie finirent par succomber à l'amour.

Le , John Ruskin fut cité à comparaître devant la du Surrey. L'audience se tint le , en son absence (il était à Chamonix avec ses parents) et sans qu'il y fût représenté et défendu. Elle prononça l'annulation du mariage pour « non-consommation » en raison d'une « impuissance incurable ». La non-consommation avait été constatée : un examen médical avait confirmé qu'Effie était toujours vierge. Cependant, Ruskin défendit, en privé, sa virilité, se proposant même de la prouver. La preuve physique ne fut cependant pas exigée. À la même occasion, il expliqua qu'il était parfaitement capable de consommer son mariage, mais qu'il n'aimait pas assez Effie pour en avoir envie. Effie Gray épousa Millais le et ils eurent huit enfants.

Le premier engagement de Ruskin en faveur des préraphaélites remontait à l'été 1851 quand leurs tableaux exposés à la "Royal Academy" furent vivement attaqués par la critique. Millais se tourna alors vers son ami Coventry Patmore, qui connaissait Ruskin, pour lui demander d'essayer d'obtenir l'aide de ce dernier. Ruskin répondit favorablement et envoya deux lettres au "Times". La défense n'était cependant pas exempte de critiques : Ruskin n'appréciait pas les aspects un peu trop « "high church" » du christianisme exprimé dans leurs tableaux, et le disait. Ruskin et Millais devinrent dès ce moment-là amis et le critique invita déjà le peintre à venir passer des vacances avec lui et sa femme. Un pamphlet intitulé "Pre-Raphaelitism" suivit dès août 1851. S'il parlait plus de Turner, son propos présentait cependant les préraphaélites comme les héritiers et continuateurs du vieux peintre, car comme lui ils poursuivaient la vérité visuelle et imaginaire.
Le cycle de conférences donné à Édimbourg début 1854, et publié l'année suivante sous le titre "Lectures on Architecture and Painting", porta principalement sur l'architecture gothique et sur le courant préraphaélite. À l'été 1854, Ruskin défendit les deux tableaux présentés à la Royal Academy par William Holman Hunt dans deux lettres au "Times". Son soutien n'était cependant pas qu'intellectuel, en tant que critique d'art : il était aussi acheteur ou mécène (il acheta ou commanda dès 1853 des dessins à Dante Gabriel Rossetti ou à Elizabeth Siddal) et conseiller auprès d'autres acheteurs de la bonne société britannique. John Ruskin apporta un soutien financier parfois direct aux artistes préraphaélites : en 1855, il fit une rente à Elizabeth Siddal et l'envoya consulter son ami Henry Acland devenu professeur de médecine à Oxford. Comme les œuvres préraphaélites incarnaient l'idéal esthétique prôné par Ruskin, il se considéra aussi très vite membre à part entière de la « "PRB (PreRaphaelite Brotherhood)" » (confrérie préraphaélite). Il fut d'ailleurs admis au et aida à monter une exposition. Cependant, comme il était plus âgé que les préraphaélites (à peu près dix ans), ils le considéraient plutôt comme un oncle (riche et finançant) que comme un frère à part entière. Sa rupture avec Millais à la suite de ses problèmes conjugaux avait en plus divisé le groupe. Il se rapprocha un temps de Dante Gabriel Rossetti, au point d'envisager d'habiter le même immeuble que celui-ci après la mort de son épouse Elizabeth Siddal. Il semblerait cependant que le mode de vie « de bohème » de Rossetti ait déplu à Ruskin qui trouvait aussi ses tableaux de plus en plus « morbides ». Leur amitié n'existait plus au milieu des années 1860.

Ruskin joua aussi un rôle dans l'« idéologie » préraphaélite. Ses "Stones of Venice" furent déterminantes pour William Morris et Edward Burne-Jones qui les découvrirent alors qu'ils n'étaient encore qu'étudiants à Oxford, mais aussi pour Millais ou William Holman Hunt.

L'annulation de son mariage avait permis à Ruskin de se replier chez lui, n'ayant plus à accompagner son épouse dans le monde. Il continua cependant à fréquenter quelques amis comme Carlyle, Alfred, Lord Tennyson, Coventry Patmore, William Allingham, Robert et Elizabeth Browning ou James Anthony Froude et surtout à entretenir une abondante correspondance. Ainsi, il correspondit longuement avec le critique américain Charles Eliot Norton, qui diffusa ses idées aux États-Unis. Celui-ci devint même responsable de la gestion de son œuvre littéraire après sa mort. Il brûla la quasi-totalité de ce qui avait trait à .

Dans les années 1850, John Ruskin apporta un soutien direct à diverses initiatives pédagogiques, voire d'éducation populaire. Son ami Henry Acland avait développé l'"University Museum" comme il était alors appelé, à Oxford. Ruskin rencontra l'architecte et fut en partie responsable du style néo-gothique adopté. Il fut aussi essentiel dans le choix de l'ornementation pour laquelle il proposa des croquis et suggéra de faire appel aux sculpteurs préraphaélites et Thomas Woolner. Enfin, il organisa la levée de fonds pour financer la construction. Ruskin vint aussi régulièrement faire des conférences sur l'esthétique aux ouvriers sur le chantier. Après la mort de l'architecte et le retard pris dans l'achèvement des décors, il finit cependant par se désintéresser du projet.

Son amitié avec F. D. Maurice le fit s'intéresser à l'initiative de celui-ci et d'autres socialistes chrétiens, le Working Men's College, un établissement de formation continue créé à Londres. Il y donna même des cours de dessin de 1854 à 1858. Ruskin considérait qu'il n'aidait peut-être pas « faire d'un charpentier un artiste, mais à le rendre plus heureux dans son métier de charpentier ». Il réussit à convaincre Rossetti à venir lui aussi enseigner. Ce fut au Working Men's College que ce dernier fit la connaissance de Burne-Jones.

La plupart des biographes de Ruskin s'accordent pour dire que la fin des années 1850 et le début des années 1860 fut pour lui une période charnière : sa foi évolua tout comme son attitude vis-à-vis des peintres de la Renaissance italienne. Il se libéra un peu de l'emprise parentale et éprouva du désir sexuel.

En 1858, il séjourna en Suisse et Italie, seul. À Turin, il fut frappé par l'énorme différence entre l'étroite simplicité du service et de la chapelle protestante où il suivait la messe et la grandeur des Véronèse qu'il étudiait. Il renonça même à son sabbatarianisme en dessinant le dimanche. Dans son autobiographie, il écrivit plus tard qu'il avait alors mis définitivement de côté son évangélisme. Il perdit même un temps sa croyance en une vie après la mort. Il ne devint cependant pas athée. Il évolua aussi dans ses goûts artistiques. Il délaissa le néogothique et réévalua les peintres vénitiens du . Il réintégra même la Grèce antique dans son histoire de l'art occidental.

Il prit alors sous son aile le jeune Edward Burne-Jones qui vint en Italie étudier les artistes de la Renaissance, financé par Ruskin dans les affections duquel il remplaça peu à peu Rossetti. Ruskin devint le parrain de Philip, le fils aîné des Burne-Jones. La famille l'accompagna lors d'un nouveau voyage en Italie en 1862. Burne-Jones fut aussi impliqué par Ruskin dans l'expérience de Winnington Hall School : il fournit des dessins pour les tapisseries à broder. Ruskin lui commanda aussi en 1863 des gravures pour illustrer son essai d'économie politique "Munera pulveris". Cependant, l'amitié se refroidit à la fin des années 1860 quand le critique attaqua Michel-Ange qu'adorait l'artiste.

En 1859, Ruskin et ses parents firent leur dernier voyage ensemble, en Allemagne. Il fut pénible à tous points de vue : physiquement, la santé des parents déclinait ; moralement, les différences religieuses entre la mère et le fils créèrent des tensions. Le père de Ruskin, dont la santé déclinait, insista pour que son fils terminât "Modern Painters" avant sa mort. En 1860, la mère de Ruskin se brisa le col du fémur. Il s'éloigna alors autant qu'il le pouvait de la résidence familiale et passa de plus en plus de temps à Winnington Hall School, une école moderne pour jeunes filles fondée par Margaret Bell à Northwich, ce qui lui fut reproché, principalement à cause de l'argent qu'il dépensait à financer cette expérience éducative. Il passa aussi beaucoup de temps à voyager sur le continent, principalement dans les Alpes. Il envisagea même d'acheter une propriété à Brizon. Seule la mort de son père en mars 1864 mit un terme à ses voyages.

Il hérita de £, d'une collection de tableaux estimée à £ et de nombreuses propriétés (maisons et terres). Il en dépensa une partie dans divers projets philanthropiques, dont ceux d'Octavia Hill. Cette fortune allait lui permettre de continuer à vivre et à écrire sans soucis. La pression paternelle ayant disparu, il se sentait intellectuellement plus libre. Il continua à vivre avec sa mère, et une cousine, Joan (ou Joanna) Agnew vint s'installer avec eux comme dame de compagnie, lui facilitant la vie.

Ruskin donnait aussi des cours de dessin par correspondance. Parmi ses élèves se trouvaient Octavia Hill dont il finança les projets philanthropiques ou la marquise de Waterford, , une artiste proche des préraphaélites qui lui présenta la famille La Touche en janvier 1858. Riches banquiers irlandais d'origine huguenote, ils désiraient attirer Ruskin dans leur cercle social et lui demandèrent de donner des cours de dessin à leurs deux filles : Emily quatorze ans et dix ans. Il devint rapidement un ami de la famille et fut régulièrement invité, soit dans la résidence londonienne, soit dans celle d'Harristown dans le comté de Kildare en Irlande. Maria La Touche, la mère, devint une confidente très proche. Ce fut à elle qu'il avoua en premier ses évolutions religieuses en août. Cependant, ce fut aussi à la même période qu'il devint évident qu'il était beaucoup plus attiré par la plus jeune des filles, Rose. Celle-ci de son côté montra ses premiers signes d'anorexie mentale. Cette situation créa des tensions. Les longs voyages continentaux de Ruskin au début des années 1860 sont souvent considérés comme une volonté d'éviter les La Touche. De 1862 à 1865, il ne revit pas Rose La Touche et celle-ci s'enfonça dans son anorexie.

Quand Rose atteignit ses dix-huit ans en janvier 1866, Ruskin la demanda en mariage. Elle ne refusa pas, mais souhaita attendre encore trois ans. Les parents s'alarmèrent de ses sentiments qui s'avéraient réciproques. Les liens ne furent pas coupés, mais Ruskin dut avoir recours à des intermédiaires pour communiquer avec la jeune fille : Georgiana Cowper, l'épouse de , une amie qu'il avait rencontrée à Rome en 1840 ; George MacDonald ; ainsi que sa cousine Joan Agnew qui était fiancée à Percy La Touche, le frère de Rose. Ainsi, il lui était interdit de la voir. Les difficultés s'accentuèrent après que Maria La Touche rencontra Effie Millais pour se renseigner sur Ruskin. En fait, elle craignait qu'une consommation du mariage entre Rose et Ruskin ne rendît caduc l'arrêt d'annulation du premier mariage, faisant de Ruskin un bigame. Celui-ci consulta de son côté des avocats pour connaître ses droits. Pendant les trois ans de séparation, l'anorexie de Rose empira. Même si Rose assurait Ruskin de son amour pour lui, il semble que les doutes religieux qu'il exprimait eurent un effet négatif sur elle qui était dévote. En octobre 1870, sa mère lui montra les lettres qu'elle avait échangées avec Effie Millais. L'effet désiré fut atteint : Rose rompit avec Ruskin. Elle s'enfonça dans son anorexie. Quant à lui, il fit une grave dépression nerveuse et s'enfuit à Venise.

L'année suivante, elle demanda une réconciliation via les intermédiaires habituels et il revint de Venise en juillet 1872. Ils passèrent ensemble quelques jours qui semblent avoir été très heureux. Cependant lorsque Ruskin reparla de mariage, elle refusa. Elle demanda à le revoir en 1873, mais ce fut à son tour de refuser. En 1874, elle vint se faire soigner à Londres et ils se virent régulièrement de septembre à décembre, malgré l'opposition des parents de Rose. Ils se rencontrèrent une dernière fois le 15 février 1875, tandis qu'elle était en fin de vie. Elle mourut de son anorexie le 25 mai 1875, plongeant Ruskin dans le désespoir.

Ruskin, fort de son expérience au Working Men's College ou à Winnington Hall School, l'école moderne pour jeunes filles fondée par Margaret Bell dans le quartier de Winnington, à Northwich, dans le Cheshire ainsi que de ses cours de dessin par correspondance, synthétisa sa méthode dans "The Elements of Drawing" (1857) puis "Elements of Perspective" (1859). Ses "Laws of Fiesole" restèrent inachevées. Il s'opposait à la méthode mécanique traditionnelle, insistant sur le fait que savoir voir était plus important que savoir dessiner. Il alla jusqu'à fonder sa propre école de dessin à Oxford à partir du moment où il y occupa la chaire Slade, tout juste fondée, en 1870. En 1874, il rencontre le jeune l'architecte Arthur Heygate Mackmurdo, part avec lui à Florence et le pousse à créer son agence ; il eut une grande influence sur la création de la Century Guild of Artists et sur le mouvement "Arts & Crafts".

Sa chaire fut si suivie qu'elle est encore surnommée la « chaire John Ruskin ».

Il mourut dans sa résidence de Brantwood à Coniston près du Lake District, et, conformément à son souhait, fut inhumé là, ayant refusé la place qui lui avait été offerte dans l'abbaye de Westminster.

John Ruskin et son père furent d'ardents collectionneurs d'art. Ils acquirent de nombreuses aquarelles de Samuel Prout d'abord et à partir de 1839 de J. M. W. Turner. Les deux artistes devinrent d'ailleurs des amis de la famille et furent régulièrement reçus. Les Ruskin furent même à partir de 1842 des mécènes de Turner à qui ils passèrent nombre de commandes. En 1861, John Ruskin put donner 48 Turner à l'Ashmolean Museum d'Oxford et 25 au Fitzwilliam Museum de Cambridge. Il acheta aussi aux préraphaélites de nombreux tableaux, dessins ou gravures.

John Ruskin fut très tôt publié. Ses premiers poèmes parurent dès août 1829 dans le "Spiritual Times". En 1834, plusieurs de ses travaux géologiques furent publiés par John Claudius Loudon dans son "Magazine of Natural History". De même, une première version de "The Poetry of Architecture" éditée en 1893 parut alors que Ruskin était étudiant à Oxford dans l’"Architectural Magazine" de ce même J. C. Loudon.

L'idée à l'origine de cet ouvrage vint à Ruskin lors de son voyage en Italie, Suisse et Allemagne au début des années 1840. Il exprima dès 1842 la volonté d'écrire un pamphlet de critique d'art afin de défendre l'œuvre de Turner à nouveau attaquée par la presse britannique. Il l'avait déjà fait en 1836, mais son texte, à la demande de Turner lui-même, n'avait pas été publié. Cette fois-ci, il mena le projet à son terme. Le premier tome de "Modern Painters: their Superiority in the Art of Landscape Painting to the Ancient Masters", sans illustrations, parut en 1843. Il fut très vite réédité et connut une troisième édition dès 1846. Le deuxième tome parut la même année, après un nouveau voyage en Suisse, en Italie et en France. Le cinquième et dernier tome parut en 1860.

Le premier tome insiste sur la vérité en art. Selon Ruskin, celle-ci n'est pas matérielle mais morale. L'important pour lui est la véritable perception d'un paysage et non son interprétation via la norme des conventions artistiques du pittoresque mises en place par les maîtres italiens et hollandais des et s. L'idéal pour Ruskin est alors le travail de Turner, le « seul capable de peindre une montagne, ou une pierre ». S'il défend Turner, il ne critique cependant pas encore les peintres de son époque. Le ton change avec le second volume, après son voyage à Venise et la lecture en chemin de l'ouvrage d'Alexis Rio "De la poésie chrétienne dans son principe, dans sa matière, et dans ses formes" paru en 1836. Pour Ruskin alors, la véritable perception de la nature est une expérience mystique de la beauté et donc de Dieu. Il reprend, dans une acception personnelle, le concept de « faculté théorique » (du grec « », « contemplation, observation »). Cette faculté théorique agit au moment de la perception, entre l'œil et l'esprit, permettant une appréhension instinctuelle et morale de la beauté. Il l'oppose à une appréhension consciente et rationnelle.

Il poursuit sa réflexion sur la beauté en la scindant en deux grands types : la « beauté vitale » et la « beauté typique ». La « beauté vitale » est pour lui fondée sur la théologie de la nature, elle est la volonté divine exprimée dans Sa création sous toutes ses formes (le monde et ses habitants dont l'homme). La « beauté typique » par contre est pour Ruskin inscrite dans la théologie évangélique : cette beauté est ressentie par l'homme quand il réagit à des grands « types » qui sont l'expression de l'immanence divine dans la Nature (infini, pureté, unité, symétrie, etc.). Ces « types » présents et dans la nature et dans l'art ne sont pas pour lui que des abstractions. Ils ont aussi une réalité que l'artiste se doit de représenter s'il veut réellement exprimer la vérité.

John Ruskin développe ensuite une théorie sur l'imagination qui permet la création. Il scinde celle-ci en trois grandes formes : l'« imagination pénétrante », l'« imagination associative » et l'« imagination contemplative ». La première voit et donc reproduit la forme externe et l'essence interne (donc la vérité) de ce que la « faculté théorique » observe ; la deuxième exprime à la fois la vérité et la pensée de l'artiste créée par la perception de la vérité ; la troisième transforme la vérité en symboles. Donc, pour Ruskin, l'artiste peut très bien représenter la vérité non pas par un réalisme total, mais symboliquement. La vérité artistique n'est pas naturaliste, elle peut être sublimée dans sa représentation symbolique.

C'est également dans le troisième volume de "Modern Painters" que Ruskin expose le concept de , une figure de style héritée du sentimentalisme du qu'il trouve abondamment dans la poésie.

"The Seven Lamps of Architecture" (les "Sept Lampes de l'Architecture") parut en mai 1849. Il fut le premier ouvrage à être signé John Ruskin. Il fut aussi le premier à être illustré : quatorze gravures de la main même de l'auteur. Dès sa préface, il se montre très clair : il attaque « le restaurateur, le révolutionniste ». Il refuse la restauration des bâtiments anciens qui doivent être protégés afin de servir de modèle aux architectes du temps, dont ceux du "Gothic Revival". Il veut aussi que ce mouvement esthétique se sécularise et se protestantise. Il veut le protéger de la « mauvaise influence » du catholicisme romain, représentée selon lui par Augustus Pugin.

Dans la pensée de Ruskin, les sept lampes qui éclairent l'architecte sont le sacrifice, la vérité, la puissance, la beauté, la vie, la mémoire et l'obéissance. L'ouvrage a été éclipsé par le succès des "Stones of Venice", dont il peut être envisagé comme un prélude.

John Ruskin s'oppose dès 1849 avec ferveur aux conceptions de l'architecte Viollet-le-Duc, pour qui l'architecture doit former un tout homogène, au mépris de l'histoire et de l'intégrité du monument. Dans les "Sept Lampes de l'Architecture", Ruskin définit un monument architectural comme un ensemble organique qu'il faut soutenir (en le restaurant le moins possible), mais qu'il faut aussi laisser mourir. Ainsi s'opposent deux conceptions de la restauration du patrimoine bâti. Ruskin est soutenu dans son approche par William Morris, qui prône la « non-restauration » dans le cadre de la « Société pour la protection des bâtiments anciens ». L'engagement de Ruskin contre la restauration tient souvent de la ferveur militante : on recense plus de lettres concernant ce sujet.

Membre du mouvement des préraphaélites, il est l'auteur d'un ouvrage qui le fait considérer comme le fondateur du mouvement Arts & Crafts : "Les Pierres de Venise" (1853). Cette œuvre a un impact non négligeable sur la société victorienne dans sa tentative de relier l'art, la nature, la moralité et l'homme (William Morris, dont Ruskin a été le mentor, est le chef de file du mouvement). Par ses écrits et son audience, par son combat pour ressusciter l'artisanat moribond au Royaume-Uni, il est un précurseur de l'Art nouveau.

En 1878, il est poursuivi en justice pour diffamation par Whistler pour avoir condamné sa peinture le "Nocturne in Black and Gold: The Falling Rocket" (1874). Whistler obtient une indemnisation symbolique. Son éclectisme l'amène à apprécier aussi bien les peintres primitifs italiens que les préraphaélites britanniques ou Turner.
Sa notoriété fait de lui un remarquable propagandiste des arts. Ses idées se popularisent à travers ses livres et influencent le mouvement "Arts & Crafts" (Arts et Métiers), qui se caractérise par la volonté d'évoquer la nature, par le recours aux formes gracieuses, ondulées, délicates, d'un charme doux, par les motifs décoratifs associés à des végétaux, des fleurs, des insectes, des poissons, des sirènes, des dragons et des oiseaux aux couleurs spectaculaires.

Ruskin arriva à l'économie à partir de ses réflexions sur l'art et l'architecture. Ses premiers grands textes sur ce thème furent les deux conférences qu'il donna à Manchester en 1857, au cours de l', intitulées "The Political Economy of Art" (republiées augmentées en 1880 sous le titre "A Joy for Ever"). Dans ce haut-lieu de la pensée libérale, il déclara : « Le principe du « Laissez-faire » est un principe de mort ». Cette phrase fait écho à cet autre principe, essentiel dans son "Unto This Last" : « Il n'y a pas d'autre richesse que la vie ». Les deux se combinent : « Le gouvernement et la coopération sont en tout temps et toutes choses, les lois de la vie. L'anarchie et la concurrence sont en tout temps et toutes choses, les lois de la mort. ». À l'origine, "Unto this Last" était une série de quatre articles pour le magazine de Thackeray, le "Cornhill Magazine", parus en 1860 et republiés en 1862. Ruskin ne voyait donc pas l'économie de façon utilitariste en termes d'échanges marchands, mais en termes moraux. Son anticapitalisme n'est cependant pas tout à fait socialiste, même s'il influença fortement la pensée de socialistes britanniques comme William Morris. Il admirait plus l'organisation d'une société vitaliste et paternaliste médiévale.

Grâce aux cours de dessin qu'il reçut lors de son enfance, avec James Duffield Harding par exemple, John Ruskin fut un dessinateur de talent. Même s'il ne se considéra jamais comme un artiste en tant que tel ou exposa peu, il produisit quelques toiles et aquarelles. Il fut ainsi élu membre honoraire de la "Royal Watercolour Society" en 1873.

Après sa mort, Marcel Proust donne des traductions de deux de ses livres, "La Bible d'Amiens" et "Sésame et les Lys", et écrit plusieurs articles à son sujet. Ruskin aura sur le jeune écrivain une influence telle que l'on a pu dire qu'il fut le .





</doc>
<doc id="4842" url="https://fr.wikipedia.org/wiki?curid=4842" title="William Morris">
William Morris

William Morris, né le à Walthamstow, Essex (aujourd'hui dans le borough londonien de Waltham Forest) et mort le à Hammersmith, Londres, est un fabricant designer textile, imprimeur, écrivain, poète, conférencier, peintre, dessinateur et architecte britannique, célèbre à la fois pour ses œuvres littéraires, son engagement politique libertaire, son travail d'édition et ses créations dans les arts décoratifs, en tant que membre de la Confrérie préraphaélite, qui furent une des sources qui initièrent le mouvement Arts & Crafts qui eut dans ce domaine une des influences les plus importantes en Grande-Bretagne au .

Tout au long de sa vie, William Morris écrivit et publia de la poésie, des romans et traduisit d'anciens textes du Moyen Âge et de l'Antiquité. Son travail littéraire le plus connu en français est l'utopie "Nouvelles de nulle part" ("News from Nowhere"), écrite en 1890. En contribuant à la fondation de la Socialist League en 1884, il joua un rôle clé dans l'émergence du courant socialiste britannique, bien qu'il ait renié ce mouvement à la fin de la même décennie. Il consacra la fin de sa vie aux travaux de l'imprimerie et maison d’édition Kelmscott Press, qu'il avait fondée en 1891. L'édition Kelmscott de 1896 des œuvres de Geoffrey Chaucer est aujourd'hui considérée comme un chef-d'œuvre de conception éditoriale.

William Morris est né à Elm House, Walthamstow, le 24 mars 1834, troisième enfant et premier fils d'une famille de moyenne bourgeoisie aisée d'origine galloise. Son père, William Morris, travaillait comme agent de change pour la compagnie Sanderson & Co., à la Cité de Londres. Sa mère était Emma Morris, née Shelton, fille de Joseph Shelton, un professeur de musique à Worcester. Sans être un prodige, il reste un enfant délicat et studieux. Il apprend à lire très jeune et, dès l'âge de quatre ans, il est émerveillé par les "Waverley Novels" de Walter Scott qu'il a déjà lus en grande partie et qui furent une impulsion pour ses poèmes d'inspiration médiévale. Il a six ans en 1840, lorsque sa famille s'installe à Woodford Hall, ouvert sur de plus grands espaces. Des cours trop réguliers lui sont épargnés afin de ménager sa santé, ce qui lui permet de mener une vie de plein air qui lui donne force et vigueur. Vêtu parfois d'une panoplie de chevalier en armure, il se promène à cheval et apprend par l'observation de la nature dans la forêt d'Epping.

Lecteur vorace, il lit tout ce qui lui tombe sous la main et se passionne pour "Les Mille et Une Nuits" ou les illustrations de l'herbier de John Gerard. Jusqu'à l'âge de neuf ans, il suit l'enseignement donné par la gouvernante de ses sœurs, avant d'entrer dans une école préparatoire pour de Walthamstow, en 1843, où il travaillera médiocrement pendant quatre années. Il a treize ans en 1847, lorsque son père décède, laissant la famille dans une grande aisance matérielle. Les Morris quittent Woodford, jugée désormais trop grande, et le jeune garçon entre à l'internat de Marlborough College en février 1848, où son père avait payé pour qu'on lui réserve une place. Pendant les trois années où il y reste, il tire peu de profit des leçons de français, de latin ou de mathématiques et ne prit goût qu'à l'architecture (l'archéologie ?), grâce aux ouvrages de la bibliothèque, et un certain penchant pour l'anglo-catholicisme qui lui donne la vocation de devenir prêtre. Ses résultats sont médiocres et, à Noël 1851, sa famille le retire de Marlborough et le confie aux soins d'un tuteur privé, le révérend F. B. Guy, plus tard chanoine de St Albans, qui disposera d'une année pour le préparer à l'entrée à l'université.

Après des études universitaires de théologie à Exeter College (Oxford), il songe à entrer dans les ordres.
Il y fait la connaissance d'Edward Burne-Jones. Les deux hommes se lient d'une amitié qui durera toute leur vie et que cimente une passion commune pour la création artistique.

La lecture de Thomas Carlyle, de Charles Kingsley et de John Ruskin le persuade de se consacrer à l’art. Étudiant en architecture, puis en peinture, il rencontre Dante Gabriel Rossetti et les artistes de la Confrérie préraphaélite en 1856, ce qui le détermine à consacrer sa vie aux arts décoratifs, à la fois comme créateur et comme homme d’affaires. En avril 1859, il se marie avec le modèle Jane Burden, dont il a deux filles : Jane Alice Morris, dite « Jenny », née en janvier 1861, et Mary « May » Morris, née en mars de l'année suivante.

La contradiction entre les aspirations socialistes utopiques de William Morris et ses activités de créateur d’objets de luxe, accessibles uniquement à une clientèle de grands bourgeois victoriens, reste encore problématique aujourd'hui. L'explication peut se trouver dans les théories socialistes elles-mêmes, qui visent à démocratiser l'art et ses savoir-faire sous toutes ses formes, afin que l'ouvrier devienne artisan et artiste. La pleine réalisation de l'être humain ne peut s'effectuer, selon Morris, que dans la création d'objets et de meubles beaux et pratiques. Soustraite aux impératifs impérialistes de rentabilité et de rapidité, la fabrication des éléments nécessaires à la vie quotidienne devient un plaisir en soi et la raison d'être d'une vie libre et épanouissante. Le souhait de Karl Marx, se trouve ainsi réalisé, dans l'abolition du désordre économique que provoque le capitalisme (concurrence, faillites, chômage…).

Il meurt, selon son médecin, , c’est-à-dire un homme d’une énergie peu commune et d’une créativité sans bornes.

Morris connut en son temps la célébrité en tant qu’auteur littéraire. Son premier recueil de poésie, "The Defense of Guenevere", n’obtint pas un grand succès et il ne fut véritablement reconnu comme poète que grâce à "The Earthly Paradise", en 1870. Il fut également l’auteur de traductions de sagas islandaises, telle que "Sigurd the Volsung", et d’autres textes classiques.
Ses principales fictions romanesques, ou « romances en prose », sont "A Dream of John Bull, The Well at the World’s End", et l'utopie socialiste "News from Nowhere", parue en 1890. On le considère souvent comme le père de la fantasy : "The Story of the Glittering Plain", "A Tale of the House of the Wolfings and All the Kindreds of the Mark", "The Wood Beyond the World", "The Well at the World's End" et "The Water of the Woundrous Isles" ont notamment influencé l'œuvre de Tolkien.

Il mit aussi son talent d’écrivain au service de ses convictions politiques, comme dans son ouvrage "Les Arts décoratifs, leur relation avec la vie moderne". Quant à son recueil de poèmes paru en 1885, sous le titre "The Pilgrims of Hope (Les Pèlerins de l’espoir)", que c’est .

C'est en 1876 que William Morris fit son entrée en politique en acceptant le poste de trésorier de l'Eastern Question Association. En 1883, déçu par les libéraux, il rejoint les socialistes de la Social Democratic Federation, puis fait partie du groupe de militants socialistes libertaires qui fonde la Socialist League en décembre 1884 pour s'opposer à l'orientation réformiste de la SDF. L'existence de la Ligue est éphémère et elle disparaît en 1890 après avoir connu des conflits internes.

Pendant les années 1880-1890, Morris n'eut de cesse de parcourir la Grande-Bretagne en tant que militant socialiste, alternant conférences et discours publics. Il prônait l'amélioration de la qualité de la vie des travailleurs manuels, de la classe ouvrière tout entière, grâce à l'éducation et les loisirs, avec, en particulier, l'enseignement des arts appliqués. Il considérait la guerre entre le capital et le travail comme le sujet essentiel de toute réflexion sur la société contemporaine. Il s'insurgeait contre le côté philistin de la société victorienne qui le faisait désespérer d'un possible épanouissement de l'art dans le système capitaliste basé sur le profit et la production de masse dénuée de qualité.

Il fut un ardent défenseur de l'environnement et du patrimoine architectural. Sa défense de la terre et ses attaques contre la répartition pernicieuse des biens anticipaient, à maints égards, les revendications écologistes. C'est en particulier à cause de son écologisme radical qu'il sera re-découvert par une partie de l'ultra-gauche française d'inspiration anarchiste ou situationniste ("L'Insécurité sociale, Interrogations, L'Encyclopédie des Nuisances", qui publiera un recueil de ses articles) : 

Partageant les vues de John Ruskin, qu'il contribue fortement à populariser, William Morris s'engage à ses côtés pour prôner la . Il étend la réflexion de Ruskin aux architectures non-nobles, et diffuse l'idée que la restauration est une perte d'authenticité pour l'œuvre. En 1877, il crée la Society for the Protection of Ancient Building, . 

La première décoration d'intérieur dont Morris se chargea fut celle de sa propre demeure construite en 1859 par Philip Webb pour le jeune couple à Bexleyheath, alors en pleine campagne avant de devenir par la suite un faubourg de Londres. À l’Exposition universelle de Londres en 1851, Morris avait été surpris par la laideur des objets présentés : en effet, selon lui, la révolution industrielle en standardisant la fabrication des objets avait mis en avant la notion de profit, au détriment de l’esthétique et de la qualité du produit.

La firme Morris, Marshall, Faulkner & Co, créée en 1861 avec l’aide de Ford Madox Brown, Charles Falkner, Burne-Jones, Rossetti et Philip Webb, acquit rapidement une excellente réputation pour la fabrication de vitraux ainsi que pour sa production de papiers peints et textiles. Elle devint ultérieurement Morris & Co.

En 1888, la première exposition de l'Arts and Crafts Exhibition Society, société issue de l'Art Workers' Guild (regroupement d'architectes, artisans d'art, peintres et sculpteurs, dont il devient Maître en 1891) ne présentait que neuf créations de Morris & Co. Selon le biographe de Morris, J. W. Mackail, peu de membres de ladite société auraient à l'époque imaginé l'influence à venir de William Morris. 

Ses créations sont indissociables des passions qu'il partageait avec ses amis préraphaélites, en premier lieu avec Burne-Jones, tant pour les primitifs italiens que pour l'art du Moyen Âge, sans compter leur aversion commune pour la laideur du goût bourgeois victorien.

En devenant éditeur et imprimeur, William Morris applique sa même exigence dans la réalisation des 66 livres imprimés par sa Kelmscott Press, et la création de nouveaux caractères d’imprimerie. Recherchant un caractère lisible et élégant, et qui lui permette de se distinguer de la production éditoriale de l'époque, il devient, à près de soixante ans, créateur de caractères. Mais il fréquente depuis sa jeunesse les bibliothèques et les manuscrits médiévaux, il a pratiqué la calligraphie, recopiant incessamment textes et enluminures, et les recueils de sa main qui ont été conservés étonnent toujours. Il étudie les créations du typographe Nicolas Jenson, dessine lui-même des caractères et s'inspirant finalement d'un proche de Jenson, Jacques Le Rouge, pour créer le Golden Type (1891), primitivement destiné à une édition de la "Légende dorée". Puis, désireux de se rapprocher de modèles plus anciens, et mû par son goût pour le médiéval, il crée une gothique arrondie, le Troy Type. Ce caractère se révélant trop massif pour son projet d'éditer les œuvres de Chaucer, il en dessine une version réduite, le Chaucer Type. Il cherche encore à travailler une nouvelle police, là encore d'après les prototypographes venus d'Allemagne en Italie, mais il n'arrive pas à l'achever. Ses travaux, repris par l'Ashendene Press, donneront le caractère Subiaco (1902).

Ce n'est qu'après de nombreuses années que Morris apparaît clairement comme l'initiateur des mouvements Arts & Crafts (arts décoratifs et artisanat d'art) en Grande-Bretagne et outre-Manche. Aux États-Unis, en 1883, Morris expose des tapisseries à la Foreign Fair de Boston. La Morris & Company travaillait déjà depuis une dizaine d’années à Boston dans la fourniture de papiers peints, lesquels furent développés par l'Anglais Charles Voysey avant qu'il ne devienne un architecte de renom. En France et en Belgique, Morris inspire notamment la mouvance Art nouveau. On peut souligner au passage l'anti-sexisme de celui qui promouvait le travail des artisans hommes ou femmes avec un même enthousiasme.
Le compositeur britannique Gustav Holst (1874-1934) écrit en entre 1899 et 1900 une symphonie, "Cotswolds", dont le deuxième mouvement constitue une élégie à la mémoire de William Morris.

Selon Fiona Mc Carthy, ce n'est que bien des années après sa mort que l'influence de Morris et l'impact de son œuvre purent être mesurés. En 1996, pour le centenaire de sa mort, à l'occasion de l'exposition organisée conjointement par la William Morris Society et la Society of Designer Craftsmen, Fiona Mc Carthy exprima son émerveillement de l'épanouissement de l'artisanat d'art en général, un siècle après la disparition de Morris, et son admiration pour la survie inespérée des arts décoratifs et artisanats d'art, compte tenu du déplorable contexte politique, environnemental et commercial actuel.

Hommage britannique à William Morris en mai 2011, à travers l'édition d'une série de timbres par la Royal Mail, à l'occasion du anniversaire de la création de la firme Morris, Marshall, Faulkner & Co.


Ce traductions sont menées à bien avec Eiríkr Magnússon en 1870.
Morris a aussi traduit en anglais "L'Énéide" (1875) et "L'Odyssée" (1887). 








</doc>
<doc id="4843" url="https://fr.wikipedia.org/wiki?curid=4843" title="Siegfried Bing">
Siegfried Bing

Siegfried Bing dit improprement Samuel Bing, (Hambourg, 26 février 1838 - Vaucresson, 6 septembre 1905), est un marchand d'art, collectionneur, critique d'art et mécène français d'origine allemande.

Deuxième fils de Frédérique Renner (1811-1893), Siegfried Bing arrive en France en 1854, rejoignant son père, Jacob Bing (1798-1868), fabricant de porcelaine originaire de Hambourg et installé 12 rue Martel ; il employait une centaine de personnes sous la raison sociale Bing & Renner.

Mentionné comme « marchand de porcelaine » en étant autorisé à jouir des droits civiques français le 15 avril 1869, Siegfried Bing reprend l’entreprise de son père en s'associant en 1863 avec Jean-Baptiste Leullier sous la raison sociale Leullier Fils & Bing, société primée lors de l'exposition universelle de 1867. En 1868, il se marie avec une cousine nommée Johanna Baer (1847-1882), le couple a quatre enfants, dont le futur joaillier Marcel Bing (13 octobre 1875 - 10 octobre 1920).

Après la guerre de 1870-1871 durant laquelle il trouve refuge à Bruxelles, il entreprend une série de voyages en Chine, au Japon, en Inde, et fait venir en Europe un nombre important d'objets, composant des catalogues de ventes fort documentés ; il est aidé par son frère Auguste Heinrich Bing (1852-1918) qui développe un comptoir d'exportation à Yokohama. Après la mort de son frère aîné en 1873, devenu chef de la famille, Siegfried se dit « négociant en porcelaines, objets d'art du Japon et de la Chine ». En 1874, il ouvre un commerce parisien de ventes d'objets artisanaux luxueux venus d’extrême-orient au 19 rue Chauchat, à l'angle de la rue de Provence, qu'il baptise L'Art japonais. 

Il est naturalisé français le 11 décembre 1876, son prénom est alors francisé en « Sigefroy ». Il découvre aussi les productions du mouvement Arts & Crafts et noue des liens avec des importateurs britanniques. 

Entre 1884 et 1888, il fait agrandir ses locaux qui ouvrent désormais 22 rue de Provence, participant activement à la diffusion du japonisme mais pas seulement : des artistes comme Edvard Munch, Antonio de La Gandara, Édouard Vuillard, Maurice Denis, Camille Claudel, Paul Signac exposent chez lui, en général des lithographies et des dessins. Il reçoit également la visite de Paul Gauguin ou Vincent van Gogh (son frère Théo est représentant de Goupil & Cie, en affaire avec Bing). L'Art japonais est un lieu où se rencontrent beaucoup de critiques d'art et de jeunes peintres, un lieu ouvert à la modernité et à l'exotisme. Bing publie une revue, "Le Japon artistique", notamment lue par les Nabis et Gustav Klimt.

En 1888, L'Art japonais présente une exposition sur l'histoire de la gravure au Japon. En 1890, grâce aux collections de ses amis, il organise à l’École des beaux-arts de Paris, une exposition de 760 estampes japonaises. Par ailleurs, il est nommé expert lors des ventes successorales des collections japonaises de Philippe Burty, Edmond de Goncourt ou encore celle de Charles Gillot.

En juillet 1890, il est fait chevalier de la Légion d'honneur en tant que « directeur de l'Art japonais », sous le parrainage de Charles Firmin-Gillot.

En 1894, il est commissionné par le directeur des Beaux-Arts à Paris, Henri Roujon, pour établir un rapport sur l'état de l'art en Amérique. À son retour des États-Unis, où il découvre Louis Comfort Tiffany, les résultats de son enquête paraissent en 1896 sous le titre "La culture artistique en Amérique".

Principal négociant en art japonais au moment de son apogée sur Paris, lors d'une visite à la villa d'Henry Van de Velde à Bruxelles, Bing prend conscience de l'importance du renouveau artistique en cours.

Après d'importants travaux, Il choisit de transformer « l'hôtel Bing » en un grand espace d'exposition-vente qu'il inaugure le 28 décembre 1895 : auparavant, c'est la première exposition, en octobre, qui révèle au public l'Art nouveau. Les locaux sont désormais ouverts au 22 de la rue de Provence et rebaptisés Maison de l'Art nouveau (« Maison Bing ») : sur deux niveaux, sont exposées des œuvres d'artistes. Ce lieu est à l'origine du nom du mouvement Art nouveau, Bing s'inspirant sans doute d'articles parus dans la revue belge "L'Art moderne" (fondée en 1881). 

Il réunit, à travers différentes salles, à la fois un atelier de création, un lieu d'exposition temporaire et une boutique pour diffuser les meubles de Van de Velde, Édouard Colonna, de Georges de Feure, Eugène Gaillard, les bijoux de Morren et Lalique, la verrerie de Louis Comfort Tiffany et Gallé, la céramique de Finch, les panneaux décoratifs d'Albert Besnard, Édouard Vuillard, Paul Ranson, Maurice Denis. Parmi les nouveaux décors de la galerie d'art commandés à plusieurs artistes, le vitrail "Au Nouveau Cirque, Papa Chrysanthème" dessiné par Henri de Toulouse Lautrec est réalisé par Louis Comfort Tiffany dont Bing est le représentant en France pour les bronzes, les céramiques, les bijoux, les tissus d'ameublement et les verreries d'art ; tandis que ce dernier lui rend la politesse aux États-Unis.

C'est ce support d'artistes, de décorateurs et de sculpteurs qui lui permet d'avoir un pavillon à l'exposition universelle de Paris en 1900 : appelé la maison de « l'Art Nouveau Bing », il fut conçu par Georges de Feure soit quatre panneaux vitrés servant de fenêtres et représentant des élégantes. Des commandes de musées suivirent. En 1902, il participe à la première exposition internationale consacrée aux arts décoratifs (Turin).

Moins à cause de difficultés financières que pour raisons de santé, Siegfried Bing se retire du négoce en 1904, transmet son fonds à son fils Marcel, qui, en association avec Louis Majorelle, va faire des lieux un nouveau salon d'exposition temporaire ; plus tard, les objets d'art japonais sont revendus à la galerie Durand-Ruel, mais une partie des collections fut offerte en donation au musée des arts décoratifs de Paris (1908).

Très active, la Société des amis de l'Art japonais, fondée par Siegfried Bing et Charles Gillot, fut dirigée par Henri Vever durant les années 1920 : celui-ci fit appel à Maurice Feuillet pour la conception des programmes et des menus, à raison de cinq à six réunions par an.

Siegfried Bing est inhumé au cimetière du Père-Lachaise.






</doc>
<doc id="4844" url="https://fr.wikipedia.org/wiki?curid=4844" title="Traité international sur les ressources phytogénétiques pour l'alimentation et l'agriculture">
Traité international sur les ressources phytogénétiques pour l'alimentation et l'agriculture

Le Traité international sur les ressources phytogénétiques pour l'alimentation et l'agriculture ou TIRPGAA (en anglais, "International Treaty on Plant Genetic Resources for Food and Agriculture" : "ITPGRFA") est un accord international visant à assurer la sécurité alimentaire par le biais de la conservation de la biodiversité, de l'échange et de l'utilisation durable des ressources phytogénétiques, tout en garantissant le partage des bénéfices. Le nom de ce traité est souvent abrégé en « Traité sur les semences » (en anglais, "International Seed Treaty").

Adopté le 3 novembre 2001 par la Conférence de l’Organisation des Nations unies pour l’alimentation et l’agriculture (FAO), le Traité est entré en vigueur le 29 juin 2004.

Le Traité a pour objectif la conservation et l'utilisation durable des ressources phytogénétiques pour l'alimentation et l'agriculture, ainsi que le partage équitable des avantages dérivant de leur utilisation (y compris les avantages générés par les échanges commerciaux). Il reconnaît également le droit des agriculteurs et met en place un système multilatéral d'accès et de partage des avantages dérivant des cultures concernées par le traité. Les principales cultures et plantes fourragères, considérées comme les plus importantes pour la sécurité alimentaire, sont au nombre de 64.

Cet accord doit fonctionner en harmonie avec la Convention sur la diversité biologique.

Il comprend un mécanisme de financement recevant une cote part des bénéfices liés à l'utilisation commerciale des ressources génétiques végétales incluses dans le système

Le traité est suivi par la "Commission des ressources génétiques pour l'alimentation et l'agriculture" (CRGAA) de la FAO. 

Le traité a nécessité sept années de négociations au sein de la FAO. Une tentative précédente de mettre en place un accord international était le IU (International Undertaking on Plant Genetic Resources for Food and Agriculture). Cependant le IU reposait sur le principe selon lequel les ressources génétiques étaient patrimoine commun de l'humanité. La Convention sur la diversité biologique (1993) ayant amené les ressources génétiques sous la juridiction des gouvernements nationaux, l'IU s'est révélé obsolète.

L'ITPGRFA a été adopté le 3 novembre 2001 lors de la Conférence de la FAO, et avait été approuvé par 116 voix et 2 abstentions, le Japon et les États-Unis. Il était ouvert à la signature du 3 novembre 2002 au 4 novembre 2002, pour tous les membres de la FAO, ainsi que des pays membres de l'ONU, de l'une.de ses institutions spécialisées ou de l'Agence internationale de l'énergie atomique. Les pays intéressés peuvent désormais adhérer sans signature préalable.

Le Traité entrera en vigueur lorsqu'au moins 40 pays (dont au moins 20 membres de la FAO) quatre-vingt-dix jours après le dernier dépôt.

Une fois ratifié, les pays signataires constitueront l'organe directeur qui décidera des conditions d'application du Traité. Par exemple, il pourra considérer le niveau, la forme et les modalités de partages des bénéfices découlant de la commercialisation des ressources génétiques. Il pourra également traiter du transfert technologique.

La recherche publique et les semenciers privés sont les principaux acteurs de cette conservation. Ils assurent la gestion de 27 collections privées regroupant près de 37 000 plantes représentatives de la diversité de ces espèces, dont 2, le maïs et le blé, ont été mises à la disposition de la communauté internationale.

Depuis 2016, l’État français a formalisé les conditions permettant de constituer la collection nationale française. Un travail est également en cours pour créer des collections sur d’autres espèces. 

Ce travail, essentiel pour stopper la perte de biodiversité, demande des moyens financiers. Le 3 novembre 2017, à Kigali, le gouvernement français renouvelle son soutien au Traité international sur les ressources phytogénétiques pour l’alimentation et l’agriculture et versera une contribution annuelle de 175 000 euros au Fonds fiduciaire à compter de cette année grâce à l'interprofession des semences française. Le gouvernement a décidé d'inclure de nouveaux matériaux dans le Système multilatéral du Traité international (MLS) qui contient plus de 1.5 million d’échantillons de matériels phytogénétiques provenant de ses 144 pays membres. 

Ratification : Canada, Érythrée, Ghana, Inde, Jordanie, Malawi, Soudan

Acceptation : Cambodge

Approbation : Guinée

En 2017 le Traité concerne 64 des principales espèces utilisées pour l’Agriculture et l’Alimentation mondiale. Il comprend aujourd’hui 1 500 000 ressources génétiques. Les différents pays bénéficiaires de ce traité, (essentiellement des acteurs des pays en développement), ont pu ainsi accéder gratuitement à 4 millions d’échantillons pour développer des plantes adaptées aux modes de production adaptés à chacun d'entre eux. 




</doc>
<doc id="4852" url="https://fr.wikipedia.org/wiki?curid=4852" title="Tchad">
Tchad

Le Tchad, en forme longue la république du Tchad, en arabe , est un pays d'Afrique centrale situé au sud de la Libye, à l'est du Niger, du Nigeria et du Cameroun, au nord de la République centrafricaine, et à l'ouest du Soudan. Sans accès à la mer, il fait géographiquement et culturellement transition entre Afrique du Nord et Afrique noire.

Cinquième plus vaste pays d'Afrique, le Tchad se divise en trois grands ensembles géographiques, au nord une partie désertique du Sahara, au centre le Sahel semi-aride, au sud la savane soudanaise. Le lac Tchad, qui donne son nom au pays, est son principal plan d'eau. Son point culminant est l'Emi Koussi, volcan du massif du Tibesti.

Différents États et empires, tentant de contrôler le commerce transsaharien et la traite orientale, se sont succédé dans la partie centrale du pays depuis la fin du À partir de 1897, la France affirme sa souveraineté successivement sur le Kanem-Bornou et ses anciens vassaux, le Baguirmi puis le Ouaddaï, qu'elle réunit en 1920 en une unique colonie au sein de l'Afrique-Équatoriale française. Le Tchad obtient son indépendance en 1960, François Tombalbaye étant premier chef d'État, mais conserve une relation privilégiée avec l'ancien colonisateur, qui est depuis intervenu militairement à plusieurs reprises. Il est le théâtre de troubles récurrents liés à des dissensions internes, notamment entre musulmans du Nord et chrétiens du Sud, et plus récemment au débordement du conflit du Darfour.

Pays agricole, producteur de coton, d'arachide, de bovins, le Tchad est devenu en 2003 exportateur de pétrole, accroissant ainsi considérablement les ressources financières de l'État, dont le chef actuel est Idriss Déby. En 2015, il est classé troisième par l'Africa Performance Index (API), outil de notation des institutions du secteur public en Afrique.

"Tsâd" est le terme kanouri désignant le lac Tchad, substantif dont l'étymologie serait "grande masse d'eau". Les insulaires désignaient le même lac sous le nom commun de "koulou", qui a explicitement le même sens.

Quand de l'été 1899 au printemps 1900, les missions Joalland-Meynier, Gentil et Foureau-Lamy, qui sont des colonnes de tirailleurs sénégalais, conquièrent les royaumes tombés durant les vingt années précédentes sous la domination du trafiquant d'esclaves Rabah, ceux de ces territoires devant, selon les accords de la conférence de Berlin, revenir à la France sont regroupés en une entité administrative unique, la zone militaire dite du Tchad, connexe au territoire du Haut-Oubangui. C'est une tradition républicaine héritée de Jean-François Reubell de désigner le ressort d'une administration territoriale par son site géographiquement le plus remarquable et non par son nom historique pratiqué par le régime précédent. Le "territoire militaire du Tchad", rattaché en 1903 au territoire de l'Oubangui-Chari, en est séparé en 1920 pour être érigé en "colonie du Tchad".

Le Tchad est surnommé « le cœur mort de l’Afrique » en raison de son enclavement au centre du continent et de son climat particulièrement désertique.

L'État du Tchad dans ses frontières actuelles est une création de la colonisation européenne, ses frontières résultant des négociations menées entre Français, Anglais et Allemands dans les années 1880 à la suite de la conférence de Berlin, mais l'espace tchadien possède une histoire riche et relativement bien connue. Il est sans doute un des berceaux de l'Humanité en la personne de « Toumaï », "Sahelanthropus tchadensis" daté d'environ sept millions d'années dont le crâne fossilisé ( d'inventaire TM 266-01-060-01) a été découvert en 2001 dans l'erg de Toros Menala.

Il y a six mil ans, le lac Tchad, alors immense mer intérieure de plusieurs dizaines de mètres de profondeur emplissant le bassin inférieur du Chari jusqu'à la dépression du Bodélé, le recouvre presque entièrement et commence de régresser. Dès le troisième millénaire avant notre ère, son territoire est parcouru par des caravanes préhistoriques qui, à travers le pays de Yam, "alias" Kerma, apportent à l'Égypte les produits tropicaux.

Peut être en lien avec le profil archéologique dit Sao qui émerge au , la partie méridionale de l'actuel Tchad forme dès avant le une très vaste région riche de ses troupeaux de rhinocéros que bordent cinq massifs montagneux, l', dont le roi des Garamantes, ancêtres des Toubous, revendique la suzeraineté face à l'expansionnisme de Rome.
Ceux ci, « hommes des montagnes » septentrionales, fondent au début du un royaume du Kanem, littéralement « du sud », qui assoie à partir du son empire sur un territoire à peu près identique à celui du Tchad actuel. Le démantèlement du Kanem fait émergé trois principaux royaumes sahéliens : le Kanem-Bornou, le Baguirmi et le Ouaddaï. Au , ces États finissent, comme tout le Soudan, par être submergés par un trafic d'esclaves séculaire destiné aux pays arabes, la traite orientale.

La France étend son protectorat sur le Baguirmi en 1897, le Bornou en 1900, le Ouaddaï en 1909. Le territoire militaire du Tchad est érigé en colonie en 1920 dans le cadre de l'Afrique-Équatoriale française, . Sous l'impulsion du gouverneur Félix Éboué, il est en 1940 le premier territoire français à se rallier à la France libre.

Devenu république autonome en 1958, le Tchad accède à l'indépendance le sous la présidence de François Tombalbaye. Celui-ci doit bientôt faire face à la révolte de peuples du Nord, en majorité musulmans, ce qui l'amène à solliciter l'aide des troupes françaises en 1968. Après l'assassinat de Tombalbaye en 1975, le pouvoir échoit au général Félix Malloum, qui doit céder la place à Goukouni Oueddei en 1979 à la suite de la Première Bataille de Ndjamena.

En 1980, la Seconde Bataille de Ndjamena permet à Goukouni Oueddei d'évincer son rival, Hissène Habré, avec l'aide décisive du dirigeant libyen Mouammar Kadhafi.

Après l'échec d'un projet de fusion entre le Tchad et la Jamahiriya arabe libyenne en 1981, les troupes libyennes se retirent dans le cadre d'un accord conclu avec le gouvernement français. En 1982, Goukouni Oueddei est renversé à son tour par Hissène Habré, qui doit faire appel l'année suivante au soutien des forces françaises pour l'aider à contenir une nouvelle invasion libyenne (opération "Manta"). En 1987, une contre-offensive des forces tchadiennes contraint finalement les troupes libyennes à évacuer le pays, à l'exception de la bande d'Aozou qui est restituée au Tchad seulement en 1994.

En 1990, Hissène Habré est renversé du pouvoir par Idriss Déby, soutenu notamment par la France. En 1999, le Tchad s'engage dans la Deuxième Guerre du Congo, en soutenant le gouvernement de Kinshasa.

Le , devant un rapport d'ONG mettant en évidence l'insécurité à la frontière entre le Tchad et la province soudanaise en guerre du Darfour, le Premier ministre annonce un renforcement des moyens dans cette région.

Peu avant la réélection d'Idriss Déby, le , de brefs combats ont lieu dans la périphérie de N'Djamena, entre une faction de la rébellion, le FUC (Front uni pour le changement) et les troupes gouvernementales. Idriss Déby accuse le Soudan de soutenir ses adversaires.

Le , les rebelles prennent la capitale du pays N'Djamena, à l'exception du palais présidentiel où le président Idriss Déby semble s'être réfugié. La France évacue une partie de ses ressortissants. Le 4 février, le Conseil de sécurité de l'ONU condamne les attaques contre le gouvernement tchadien. L'armée tchadienne repousse les rebelles avec l'aide logistique de la France. La stabilité régionale au Tchad est alors assurée conjointement par la force de l'Union européenne EUFOR (déployée de mars 2008 à mars 2009, environ soldats) et par les forces françaises de l'opération "Épervier".

Menant une guerre de mouvement, les rebelles venus du Soudan voisin multiplient les attaques rapides dans l'Est du pays avec pour but avoué la chute du gouvernement tchadien. En juin 2008, des combats ont opposé pour la première fois l'EUFOR et ces rebelles autour de la ville de Goz Beida, au sud d'Abéché. Le 17 novembre 2008, deux hélicoptères soudanais visent délibérément des véhicules belges de l'EUFOR.

En mai 2009, une nouvelle rébellion conduit l'offensive à partir du Darfour. En 2009, le président soudanais Omar el-Béchir visite le pays sans être arrêté, alors qu'un mandat d'arrêt international avait été émis à son encontre pour crimes de guerre et génocide.

En janvier 2013, le Tchad envoie des troupes au nord du Mali pour participer à l'opération "Serval". Idriss Déby décrit alors ce qui se passe dans le Nord du Mali comme la conséquence de et de la .

Le Tchad est un pays vaste et de faible densité humaine. Il connaît pourtant d'importants contrastes. Le tiers nord du pays est occupé par le Sahara, et presque vide d'hommes. Plus au sud se trouve le Sahel où les précipitations sont plus importantes, de 300 à . C'est dans le Sud de cette zone que se trouve la capitale Ndjamena ainsi que le lac Tchad. Plus au sud encore, se trouve une zone de savane où les précipitations peuvent dépasser . C'est là, dans le Sud-Ouest, qu'en moyenne les densités sont les plus élevées.

Le Nord et le Centre représentent les deux tiers de la superficie du pays et comptent environ 30 % de la population totale. Ces régions sont peuplées de populations négro-africaines et sahariennes disparates, mais toutes musulmanes, appartenant aux groupes linguistiques saharien, soudanais oriental et central.

Par ailleurs, les Arabes (de souche, métissés et négro-africains arabisés), musulmans, occupent trois grandes zones de peuplement, au nord (Nord-Ouest du Kanem), au centre (Batha, Guéra, Chari-Baguirmi et Nord du Ouaddaï) et au sud-est (Salamat), représentant environ 56,5 % de la population tchadienne.

D'autre part, les Hadjaraïs (8,5 %), nom de désignation ethnique, communément attribué aux divers groupes ethniques (plus en référence à la région administrative qu'à leurs divergences culturelles ou religieuses) qui peuplent le Centre et le Centre-Ouest du pays, groupes linguistiques afro-asiatique, tchadique, nilo-saharien, bongo baguirmi, sara baguirmi, baguirmi, soudanais oriental et central, sont majoritairement musulmans. Néanmoins, il existe une minorité de chrétiens voire des animistes dans cette partie du territoire.

Enfin, le Sud-Ouest, chrétien, musulman et très marginalement animiste, est composé de populations négro-africaines appartenant aux groupes linguistiques tchadien et soudanais central (Saras – 30 %, Ngambayes – 5,5 %, Toupouris, Kotokos, Baguirmis, Massas et autres – 9,5 %).

Des sites consacrés à la conservation de la diversité biologique couvrent environ 20 % du territoire national (en 2014) :



La conservation de la nature est concrétisée par la création de parc nationaux dont le parc national de Zakouma, dans le Sud-Est du Tchad, d'un superficie de .

Le décembre 2015, à l'occasion du sommet « Défi climatique et solutions africaines » en marge de la conférence de Paris sur le climat (COP 21), Idriss Déby a alerté la communauté internationale sur le besoin de financement pour l'avenir du lac Tchad (dont la surface a été divisée par 8 depuis 1973) .

En 2002, le Tchad a été divisé en 18 régions administratives. Le 19 février 2008, le nombre des régions a été porté à 22 ; le 4 septembre 2012 à 23. Elles remplacent les quatorze préfectures qui existaient auparavant. Chaque région est dirigée par un gouverneur nommé par le gouvernement tchadien.

Les régions sont divisées en départements (61), administrés par un préfet, eux-mêmes subdivisés en sous-préfectures. Au plus petit niveau se trouvent les cantons. Chaque entité territoriale devrait être dirigée par une assemblée élue mais aucune élection n'a encore eu lieu.

Les unités administratives sont les relais de l'État à un niveau local. La ville de Ndjamena (qui a un statut spécial) est divisée en 10 arrondissements.

L'agriculture et l'élevage du bétail sont les activités dominantes. Après le pétrole, la première ressource d'exportation du Tchad est le coton de la Cotontchad et le sucre de la CST, Compagnie sucrière du Tchad (anciennement SONASUT).

Alors que dans les années 1930, seulement de coton-graine sont produites, dès 1955 leur production atteint tonnes, pour monter à tonnes en 1971 puis tonnes en 1975 et culminer à tonnes en 1998. Le pays est au palmarès des sept premiers producteurs africains de coton au milieu des années 2010.

L'exploitation commerciale des gisements pétroliers de Doba, dans le Sud du pays à partir des années 2000 a un impact profond sur la vie économique et politique tchadienne. La mise en exploitation des gisements pétroliers a été très encadrée par la Banque mondiale depuis 2003. Dès 2004, le pétrole représentait plus de 80 % des exportations nationales, permettant à la balance commerciale de devenir nettement excédentaire.

L'exploitation a commencé après l'achèvement en 2003 de l'oléoduc Tchad-Cameroun qui permet d'acheminer le pétrole dans le golfe de Guinée. Les gisements sont exploités par un consortium associant ExxonMobil, Chevron, et Petronas. L'oléoduc a été partiellement financé par la Banque mondiale. En échange du prêt, l'État tchadien touche des redevances et des dividendes, soient des recettes de 2 milliards de dollars/an sur 25 ans. Le Tchad s'est engagé auprès de la Banque mondiale à dépenser 80 % des redevances et 85 % des dividendes à la lutte contre la pauvreté. À la suite d'un différend entre la Banque et le gouvernement tchadien, un nouveau protocole d'accord a été signé en juin 2006, le gouvernement tchadien doit désormais consacrer 70 % de son budget total aux programmes prioritaires de réduction de la pauvreté.

Le Tchad espère le triplement de la production du pétrole de son pays à partir de 2015. L’entrée en production d’un champ pétrolier à proximité du lac Tchad nourrit un tel optimisme. Le pays du Sahel pourrait atteindre la barre des barils par jour en 2014, puis dépasser le cap des 300 000 en 2015. En 2015, les ressources pétrolières représente 20 % du PIB.

À la fin des années 1990, les "compagnies juniors" canadiennes, investies dans plus de 8 000 propriétés minières, dans plus de 100 pays, pour la plupart encore à l'état de projet multiplient les contrats avec des pays africains parmi lesquels le Tchad.

En 2012, Idriss Déby développe une politique de diversification de l'économie tchadienne. L'Africa Performance Index (API), qui évalue le développement économique des pays africains membres de la zone, place alors le Tchad au pied de son podium et souligne que . La croissance du pays en 2015 est estimée à 7,6 % selon le FMI, soit l'un des taux les plus importants du continent avec le Nigeria.

Alors que l'exploitation d'hydrocarbures a permis au Tchad de générer en dix ans plus de 10 milliards de dollars, Idriss Déby investit notamment dans le rattrapage du pays en termes d'infrastructures afin d'attirer les investissements. Une dynamique illustrée par la construction de la Cité internationale des affaires, à N'Djamena, pour un coût total de 366 millions d'euros.

Idriss Déby ouvre également le pays aux capitaux étrangers, en particulier ceux venant de Chine. Selon le chargé des affaires de l'ambassade de Chine, qui rappelle que le Tchad et la Chine développent leurs relations économiques depuis dix ans, . Plusieurs grands chantiers sont menés au Tchad par des groupes chinois : la raffinerie de Djarmaya, la cimenterie de Baoré, le palais de la Démocratie (siège de l'Assemblée nationale), l'hôpital de la liberté et la maison de la Femme.

Le 14 décembre 2015, le FMI, désireux , approuve le déblocage de 28,7 millions de dollars d'aide au développement au Tchad.

Trois opérateurs mobiles se partagent le marché : Airtel, Tigo (Millicom) et Salam.

Nombre de clients par opérateur (source OTRT)
En 2015, la population tchadienne est estimée par le CIA World Factbook à environ 11,63 millions d'habitants ; 44,2 % a moins de quinze ans, 52,8 % entre quinze et soixante-quatre ans, et 2,98 % soixante-cinq ans et plus. Le taux de croissance démographique serait de 1,89 %, avec un taux de natalité de 36,6 pour mille et un taux de mortalité de 14,28 pour mille. Le "Recensement général de la population et de l'habitat" de 1993 faisait état de habitants. Plus de 47 % de la population a moins de 15 ans et le taux de fécondité était de 6,08. L'espérance de vie était de cinquante-et-un ans.

27 % des Tchadiens vivent en ville, et près de la moitié des urbains résident à Ndjamena, la capitale. Les densités varient considérablement du nord au sud du pays, avec 0,1 habitant au km² dans les régions du Borkou, de l'Ennedi et du Tibesti, et 52,4 habitants au km² dans le Logone Occidental. La moitié de la population vit dans le cinquième le plus méridional du territoire. Selon le World Refugee Survey 2008 publié par le Comité américain pour les réfugiés et les immigrants, le Tchad abritait réfugiés et demandeurs d'asile en 2007. d'entre eux provenaient du Soudan et le reste de la République centrafricaine

Même si le français et l'arabe sont les seules langues officielles, plus d'une centaine de langues sont utilisées à travers le pays, comme le gourane, le sar, le ngambay, le mbay, le kabalaye, le lélé, le kim, le massa, le toupouri, le baguirmi, le nandjéré, le mboum, le hadjeray, le gorane, le balala, le kanembou, le haoussa, ke kanouri, le zaghawa, le mabak, le kado, le laga, le gor, le kaba, le zimé, le n'gama, le moundang, le labet, le mousseye, le quera, etc.

L'arabe tchadien est pratiqué par plus de la moitié de la population, surtout sur les marchés.

Le français, qui a été introduit dans les écoles tchadiennes à partir de 1911 et qui est langue officielle du pays depuis son indépendance, est la langue de l'administration et de l'éducation ; bien qu'elle ne soit la langue maternelle que d'une minorité, c'est une langue seconde qui jouit d'une grande vitalité et qui est la langue véhiculaire du Sud du pays, plus urbanisé.

D'après le recensement de 1993, les religions les plus pratiquées au Tchad sont : l'islam (53,9 %), le christianisme (34,7 %) et l'animisme (7,4 %). Les personnes sans religion représentent 3,1 % de la population (l'un des taux les plus forts en Afrique), et les autres religions 0,9 %.

Peuples nomades : selon le recensement du pays de 2009, les nomades représentent 3,4 % de la population totale du pays, soit personnes ; ils étaient lors du recensement de 1993, mais 5,7 % de la population totale du pays.

Sous la forme de république, Le Tchad est une dictature critiquée par des observateurs internationaux. Le Président de la République nomme les ministres. Le parlement ne comporte qu'une seule chambre.

Un référendum a eu lieu le pour modifier la Constitution de 1996 sur plusieurs aspects préalablement votés par l'Assemblée nationale le . Le point le plus important est la fin de la limitation des mandats (modification de l'article 61 de la constitution du ). Désormais le président peut se représenter plus de deux fois.

Le 3 mai 2006, Idriss Déby est réélu au suffrage universel avec 64,67 % des votes exprimés. Des groupes d'opposants dénoncent un scrutin truqué avec la complicité de la Commission nationale électorale, d'autres appellent au boycott. Le , il est réélu pour un quatrième mandat dès le premier tour de l'élection présidentielle par 88,7 % des voix, face à Albert Pahimi Padacké (6 %) et Madou Nadji (5,3 %).

En février 2016, Idriss Déby est nommé par son parti pour concourir lors de la future élection présidentielle se déroulant en avril 2016. Il fait alors figurer dans son programme l'instauration d'une limite des mandats dans la constitution, déclarant qu' tandis qu'en 2016, le Tchad .

Au mois de mai 2012, Idriss Déby lance une vaste opération anti corruption dans le pays, baptisée opération "Cobra". L’État perd alors un montant estimé à 300 milliards de F CFA (soit 460 millions d'euros) par an à cause de détournements d'argent public. Le but est de sécuriser les circuits de recettes et de dépenses et de contrôler les procédures de préparation, de passation et d'exécution des marchés publics. Après un an et 23 missions de contrôles effectuées à N'Djamena et 22 en province, environ 25 milliards de F CFA ont été recouvrés.

En 2015, Idriss Déby débloque 4,57 millions d'euros pour venir en aide à la région du lac Tchad, en proie à la désertification et fertile au développement de groupes terroristes tels que Boko Haram. Le Tchad étant une cible stratégique pour le groupe terroriste Boko Haram, Idriss Déby tourne une partie de sa politique de ce mandat vers la lutte contre le terrorisme.

Devant faire face à la menace grandissante de Boko Haram, un groupe terroriste rallié à Daesh dans le Nord du Nigeria, Idriss Déby augmente la participation du Tchad à la Multinational Joint Task Force (MNJTF), une force armée composée du Niger, du Nigeria, du Bénin et du Cameroun. En août 2015, Idriss Déby déclare à ce sujet dans un entretien que la MNJTF a Boko Haram.

En septembre 2017, un rapport d'Amnesty International dénonce une vague de répression au Tchad, sous le mandat d'Idriss Déby. Certes, l'organisation reconnaît que cela n'est pas du même niveau que sous son prédécesseur Hissène Habré (une répression implacable qui fit des dizaines de milliers de morts dans les années 1980) mais elle note que le président Déby, qui effectue son cinquième mandat, n'a pas tenu ses engagements initiaux en matière de respect des droits de l'homme. Elle note qu'Idriss Déby , au moyen d'un harcèlement juridique, administratif et policier continu contre ses opposants. Depuis 2016, 65 manifestations ont été interdites par les autorités et on refuse toute existence légale à plusieurs plateformes d'opposition.

La polygamie est couramment répandue, et concerne environ 39 % des femmes tchadiennes ; elle est encadrée par la loi, qui prévoit que la femme peut refuser cette pratique dans son contrat de mariage. Les mutilations génitales féminines sont interdites par la loi mais couramment pratiquées.

Le Tchad a pour codes :





</doc>
<doc id="4860" url="https://fr.wikipedia.org/wiki?curid=4860" title="Fête nationale du Québec">
Fête nationale du Québec

La Fête nationale du Québec, anciennement appelée la Saint-Jean-Baptiste ou Saint-Jean, est la fête nationale des Québécois. En vertu de la Loi sur la fête nationale, le 24 juin est une journée fériée et chômée au Québec. 

Cette date est d'abord celle de la fête religieuse célébrant la naissance de Jean le Baptiste qui s'est plus tard imposée, à partir de 1834, comme fête nationale des Canadiens français. Reconnue jour férié par la province de Québec en 1926, le gouvernement souverainiste du Parti québécois l'a déclarée « fête nationale du Québec » en 1977. 

Depuis 1984, le Mouvement national des Québécoises et Québécois est officiellement responsable de la coordination des festivités qui se déroulent les 23 et 24 juin de chaque année.

Chez les païens, le solstice d'été qui, selon le calendrier julien, tombait le 24 juin, était célébré par des feux de nuit (voir le Collier des Brísingar) symbolisant la puissance fertilisante du Soleil, ces feux de joie demeurant encore aujourd'hui le symbole le plus ancien de la fête . En plus de son caractère de rite de passage saisonnier, la fête du solstice d’été marquait également un jalon dans le cycle de production agricole, alors que s’entamaient les grands travaux agricoles qui ne s’achèveraient qu’à la fin de l’été 
Pour lutter contre le paganisme, les évêques implantent des sanctuaires de Saint Jean Baptiste dont la fête doit se substituer au culte des divinités barbares (par exemple Koupalo chez les Slaves) qui, avec les invasions successives des Saxons puis des Vikings, durera jusqu'à la fin de la dynastie des Carolingiens. Plus largement, la fête prit, au sein de la cosmogonie chrétienne, une importance non négligeable, marquant, à six mois exactement, le pendant de la naissance du Christ célébrée par la fête de Noël qui symbolise elle-même, avec le solstice d’hiver (le jour le plus court de l’année), le début du triomphe de la lumière sur les ténèbres. 

Ainsi, l'un des sanctuaires les plus caractéristiques concernant cette lutte d'influence est celui de Saint Jean Baptiste d'Audresselles, en France, . Ce sanctuaire surélevé, d'où l'on voit le Soleil se coucher dans la mer, est entouré par ceux des dieux germaniques dont les villages entourant Audresselles portent toujours le nom : Audinghen (Odin, Wotan ou Wedne), Raventhun (le corbeau accompagnait Odin), Ambleteuse, auparavant Amel Thuys (dieu Thuys ou Tues), Tardinghen (dieu Thar, Thor ou Thurst), Loquinghen (dieu Loki), Bazinghen (déesse Basine) etc. 

Encore aujourd'hui, c'est à Saint-Jean-Baptiste-d'Audresselles que se réunissent tous les catholiques de la région pour les grandes fêtes religieuses. Parmi eux se trouvent des familles d'Acadiens revenus dans le Boulonnais après la perte de la Nouvelle-France par la France.

Officielle sous l'Ancien Régime, la fête de la Saint-Jean Baptiste reste une fête très populaire dans les zones catholiques de la France actuelle.

La fête débarque en Amérique avec les premiers colons français. Les premières célébrations de cette fête chrétienne en Nouvelle-France auraient eu lieu dès 1606, des colons français se dirigeant vers ce qui deviendrait l'Acadie faisant escale à Terre-Neuve et célébrant, le 24 juin, la Saint-Jean Baptiste sur les côtes. Une seconde mention de la fête remonte à 1636 selon les "Relations des Jésuites", qui relatent les célébrations prenant place à Québec et commandées par le gouverneur Montmagny.

Les célébrations de la Saint-Jean-Baptiste prennent une tournure très patriotique au Bas-Canada grâce, entre autres, aux actions de Ludger Duvernay, qui deviendra le premier président de la Société Saint-Jean-Baptiste. 

C'est le qu'est chanté pour la première fois le "Ô Canada! mon pays, mes amours" de George-Étienne Cartier lors d'un grand banquet patriotique regroupant une soixantaine de francophones et d'anglophones de Montréal dans les jardins de l'avocat John McDonnell, près de l'ancienne gare Windsor. Plusieurs hommes politiques réformistes dont Edmund Bailey O'Callaghan, Louis Perrault, Thomas Storrow Brown, Édouard-Étienne Rodier, Louis-Hippolyte La Fontaine et le maire de Montréal Jacques Viger sont présents lors de ce banquet.

Après cette première célébration, le journal La Minerve conclut, que « cette fête dont le but est de cimenter l'union des Canadiens ne sera pas sans fruit. Elle sera célébrée annuellement comme fête nationale et ne pourra manquer de produire les plus heureux résultats » . C'est à partir de cette date que la fête nationale des anciens Canadiens en vient à correspondre avec la fête catholique de Saint-Jean-Baptiste, déjà bien ancrée dans la tradition.

À la suite des soulèvements des Patriotes de 1837 et 1838 et des répressions militaires qui suivirent, la fête cessa d'être célébrée pendant plusieurs années. Lorsqu'elle réapparaît, c'est sous la forme d'une célébration essentiellement religieuse, bien que les feux soient toujours présents. À Québec en 1842, elle donne lieu à une grande procession religieuse, inaugurant ainsi la tradition du défilé de la St-Jean-Baptiste, promis à une longue postérité. En 1843, Duvernay établit l'Association Saint-Jean-Baptiste, une société charitable et patriotique, en vue de la célébration de la fête de cette année-là à Montréal en 1843. 

Le , les citoyens de la ville de Québec participant aux festivités de la St-Jean-Baptiste se font chanter un autre "Ô Canada", aujourd'hui hymne national du Canada. Il devient populaire très rapidement et on le désigne même comme « hymne national » des Canadiens français. Les paroles sont d'Adolphe-Basile Routhier et la musique de Calixa Lavallée.

En 1908, le pape Pie X fait de saint Jean-Baptiste le patron spécifique des Canadiens français. La procession de chars allégoriques est introduite en 1874. De 1914 à 1923, les défilés n'ont pas lieu.

En 1925, le gouvernement du Québec fait du 24 juin un jour férié.

Après la Révolution tranquille, la Saint-Jean-Baptiste prend un aspect plus politique, les générations les plus jeunes rejetant la symbolique religieuse associée auparavant aux festivités. Le , le gouverneur-général Georges Vanier cible des souverainistes québécoises et assiste au défilé de Montréal, tandis qu'un groupe de militants distribue des documents proclamant : « Vanier vendu » et « Vanier fou de la Reine ». C'est à ce moment que la représentation traditionnelle de saint Jean-Baptiste en petit garçon frisé accompagné d'un mouton disparaît au profit d'une statue de 10 pieds représentant le saint en homme mature, qu'on veut à l'image du Québec moderne .

En 1968, un incident survient durant le défilé traditionnel du 24 juin auquel assiste entre autres, à la veille d'une élection générale, le premier ministre du Canada de l'époque, Pierre Elliott Trudeau, dont l'hostilité à la thèse indépendantiste est bien connue. Il devient rapidement la cible d'un groupe de manifestants indépendantistes qui scandent : « Trudeau traître, Trudeau vendu, à bas Trudeau » et tentent de le chasser de la tribune à coup de pierres et de bonbonnes d'acide. Trudeau refuse de quitter la scène. Il expliquera plus tard dans son autobiographie : « Je n'avais pas du tout envie d'obéir à une violence aussi saugrenue. Je déteste la violence. Démocrate, je n'admets pas qu'une infime minorité d'agitateurs tente de chasser à coups de pierres les invités de la majorité ». Les forces policières au service de l'intégrité du régime démocratique défendu par le premier ministre procédèrent à la répression des manifestants. L'opération se termine par l'arrestation de 290 personnes et 125 manifestants, spectateurs et policiers, sont blessés (sans compter les chevaux de la police montée de la ville de Montréal). La Télévision de Radio-Canada et CBC rediffusèrent la scène du refus de Trudeau de quitter la scène dans les journaux télévisés du soir. D'après les commentateurs politiques de l'époque, de nombreuses personnes considérèrent le geste de Trudeau comme étant un acte ouvert de courage, impressionnant pour l'électorat canadien-anglais. Les adversaires des indépendantistes ont soutenu que la manifestation avait eu l'effet contraire de celui visé par les indépendantistes et, sans jamais en apporter la démonstration, ils affirmèrent que cela avait contribué à la victoire du Parti libéral lors de l'élection fédérale du lendemain (le PLC obtient 45,37 % des voix des électeurs Canadiens et 53,6 % de celles des Québécois).

En 1969, des manifestants du Front de libération populaire détruisent la statue représentant saint Jean-Baptiste en la précipitant par terre. Comble d'ironie, celle-ci se décapite en tombant et sa tête ne fut jamais retrouvée. Cet incident cause une interruption du défilé, qui n'eut pas lieu l'année suivante non plus.

C'est le , que, par un arrêté ministériel du gouvernement de René Lévesque, le 24 juin devient officiellement le jour de la Fête nationale du Québec. L'année suivante, le comité organisateur de la Fête nationale du Québec est créé. Le comité confia d'abord l'organisation des événements à la Société Saint-Jean-Baptiste. Depuis le 24 juin 1975, la chanson Gens du pays de Gilles Vigneault joue lors des festivités. En 1984, l'organisation est confiée au Mouvement national des Québécoises et des Québécois dont fait partie la Société Saint-Jean-Baptiste.

Bien qu'elle soit toujours la fête des Canadiens français, la Saint-Jean-Baptiste devient, au Québec, la fête de tous les Québécois et non plus uniquement celle des Québécois d'origine canadienne-française et catholique. Par les actions de la Société Saint-Jean-Baptiste et du Mouvement national des Québécois principalement, la fête s'est graduellement laïcisée. Malgré tout, la fête demeure toujours l'occasion d'un grand festival culturel dont les Québécois profitent pour manifester leur existence au monde et leur sentiment d'appartenance au Québec. La tradition d'allumer des feux durant la nuit est toujours vivante.

La plus grande manifestation se déroulait traditionnellement dans la ville de Québec (la capitale du Québec) sur les Plaines d'Abraham, qui pendant des années réunit des foules de plus de personnes. En 2011, le maire de Québec Régis Labeaume fait restreindre la consommation d'alcool lors du traditionnel spectacle sur les Plaines d'Abraham et dans les rues de la capitale. L'affluence à la fête chute dramatiquement, en partie à cause du mauvais temps en 2011. Le , malgré un temps radieux, moins de personnes se rassemblent sur les Plaines.

Depuis le , la Commission des normes du travail du Québec veille à l'application de Loi sur la fête nationale, qui fait du 24 juin un jour de congé férié et chômé. Si elle tombe un autre jour de congé, un autre congé doit être ajouté. Si en raison de la nature de l'emploi, le travailleur ne peut s'absenter de son travail, il doit recevoir une indemnité.

Aujourd'hui, la fête de la Saint-Jean-Baptiste est célébrée par plusieurs communautés catholiques à travers le monde, entre autres au Danemark, au Portugal, en Espagne et en France. Pour l'Église catholique romaine, le 24 juin est un jour de fête religieuse en l'honneur de saint Jean-Baptiste.

La fête est toujours célébrée par de nombreux francophones, majoritairement issus de la diaspora québécoise ou acadienne, de la Nouvelle-Angleterre, de l'Ontario, des Provinces Maritimes et de l'Ouest canadien. Les plus importantes célébrations de la Saint-Jean-Baptiste dans le Canada hors Québec ont lieu dans le cadre du Festival franco-ontarien, qui se tient chaque année à Ottawa. La Saint-Jean-Baptiste est aussi une célébration importante pour la région du nord de l'Ontario dans diverses petites villes dont Kapuskasing. Bien que la célébration de la Saint-Jean-Baptiste par les Acadiens en tant que fête catholique ne soit pas inconnue, elle est largement surpassée par la Fête nationale de l'Acadie le 15 août (fête de l'Assomption, qui d'ailleurs fut la fête nationale des Français sous le Second Empire), instituée en 1881 lors d'une convention acadienne, alors que cette date était en compétition avec le 24 juin.

Depuis plusieurs années, un défilé de la Fête nationale du Québec est organisé en France par la Délégation générale du Québec à Paris. Les délégations de New York, Londres, Bruxelles, Mexico et Tôkyô soutiennent aussi des activités pour souligner le 24 juin.

La Saint-Jean-Baptiste est aussi largement célébrée en Colombie-Britannique, plus particulièrement dans la vallée de l'Okanagan, étant donné que la majorité des travailleurs saisonniers, présents pour la récolte estivale des cerises et autres fruits, sont des Québécois.

Certaines personnes au sein des communautés francophones du Canada regrettent que le Québec se soit « approprié » cette fête, puisque de leur point de vue, la fête avait comme but d'unir tous les Canadiens français (les Acadiens ayant une fête différente), qu'ils soient au Canada ou aux États-Unis. D'un autre point de vue, les dirigeants de la Société Saint-Jean-Baptiste de Montréal accusent toujours Ottawa d'avoir usurpé les symboles nationaux historiques des Québécois en s'appropriant la feuille d'érable, le castor et l'hymne "Ô Canada".

En , une polémique éclate lorsque deux artistes québécois anglophones (le groupe Lake of Stew et le chanteur Bloodshot Bill) sont retirés de la programmation d'un spectacle de la Fête nationale intitulé « L’autre Saint-Jean » parce que ceux-ci chantent principalement en anglais et deux commanditaires menaçaient de retirer leur commandite du spectacle si ces artistes anglophones montaient sur scène, craignant des manifestations. Les responsables du spectacle reviennent finalement sur leur décision et autorisent la présence de ces artistes, mais ces derniers devront prévoir des chansons en français pour « s'assurer de la prédominance du français et du caractère francophone des activités » lors de ce spectacle dans le quartier Rosemont. Originaire de Châteauguay, Lake of Stew a symboliquement terminé sa prestation en reprenant la chanson "Harmonie du soir à Châteauguay" du groupe québécois Beau Dommage. Toutefois, certains souverainistes et défenseurs de la langue française craignent que l'on ouvre ainsi la porte à des festivités bilingues.





</doc>
<doc id="4861" url="https://fr.wikipedia.org/wiki?curid=4861" title="Jean le Baptiste">
Jean le Baptiste

Jean le Baptiste ou Jean Baptiste ( "Ioánnēs ho baptistēs", Ἰωάννης ὁ βαπτίζων "Ioánnēs ho baptízōn" ou Ιωάννης ὁ πρόδρομος "Ioánnēs ho pródromos", "Yaḥya" ou يوحنا المعمدان "Ywḥnạ ạlmʿmdạn"), est un personnage majeur du christianisme et de l'islam. Sur le plan historique, il fut un prédicateur juif du temps de Jésus de Nazareth.

L'Évangile selon Jean localise l'activité du Baptiste sur les rives du Jourdain et à Béthanie au-delà du Jourdain. Jésus semble avoir vécu un temps dans son entourage et y avoir recruté ses premiers apôtres. Les évangiles synoptiques synchronisent le début de l'activité de Jésus avec l'emprisonnement de Jean.

L'audience de ce prophète apocalyptique n'a cessé de croître, au point de susciter la réaction d'Hérode Antipas, qui, le voyant rassembler ses partisans, craint qu'il ne suscite une révolution. Dans les évangiles synoptiques, le Baptiste est mis à mort pour avoir critiqué le mariage d'Antipas avec Hérodiade.

Dans le christianisme, Jean le Baptiste est le prophète qui a annoncé la venue de Jésus de Nazareth. Il l'a baptisé sur les bords du Jourdain, laissant certains de ses disciples se joindre à lui. Précurseur du Messie, il est présenté dans les synoptiques comme partageant beaucoup de traits avec le prophète Élie, ce qu'il n'est pas dans l'Évangile selon Jean. Il est un saint. Deux fêtes lui sont consacrées dans le catholicisme : le qui commémore sa naissance, fixée six mois avant Noël pour se conformer au récit d'enfance de l'Évangile selon Luc, et le qui célèbre la mémoire de sa décapitation ou sa décollation (cf. art. Décollation de saint Jean-Baptiste)

La religion mandéenne en fait son prophète principal. Il est considéré par l'islam comme un prophète descendant de 'Îmran.

Étienne Trocmé rappelle que Ces récits relatifs à Jean Baptiste 

Jean est le fils de Zacharie un prêtre qui assure des fonctions au Temple de Jérusalem. Il appartient donc à une famille sacerdotale. Ses traits et ses mœurs rappellent d'assez près ceux de Bannos, dont Flavius Josèphe s'était fait l'émule dans sa jeunesse. À partir de son analyse des Mandéens qui se donnent le nom de "nasôrayya" (observants) et ce que disent les Pères de l'Église de la secte des "nasaréens" André Paul estime que le nom "Nasôréens" ("nasôrayya") fut peut-être donné aux disciples de Jean le Baptiste. L'évangile attribué à l'apôtre Jean localise l'activité de Jean Baptiste sur les rives du Jourdain.

Les pratiques de Jean et celles des esséniens n'ont que peu de rapports. Entre l'immersion effectuée par Jean dans les eaux du Jourdain et les rites des esséniens tels que les définissent les manuscrits de la mer Morte, les différences se révèlent fondamentales : « l'idéologie du Jourdain n'occupe aucune place dans les écrits de Qumrân ». Ces deux mouvements demeurent bien distincts. Entre autres, Émile Puech relève que les rites de purification chez les esséniens « n'ont rien de commun avec le baptême d'eau pratiqué par Jean devant l'imminence du Jugement divin et la venue du règne messianique », rites esséniens qui comportent une « confession des péchés de type collectif , [...] contrairement au pardon des péchés lié au baptême personnel administré par Jean ». En accord avec Puech, François Blanchetière écrit que Jean « n'a sans doute pas eu de contacts personnels directs avec l'essénisme ».

Selon François Blanchetière, Jésus semble avoir .

Dans les évangiles, Jean reconnaît Jésus, son disciple, comme plus grand que lui, mais selon le théologien Étienne Trocmé, En effet, . De plus, si l'on en croit l'évangile selon Matthieu (11, 2-19), de sa prison, Jean aurait envoyé quelques-uns de ses disciples pour ; la réponse de Jésus a-t-elle convaincu Jean Baptiste que son ancien disciple jouait un rôle messianique ? 

De même, pour Pierre Geoltrain, la de Jean produite dans les évangiles constitue un , et donc un 

Les spécialistes discutent de savoir dans quels lieux les deux hommes se rencontraient. L'évangile selon Jean localise l'activité de Jean le Baptiste sur les rives du Jourdain (Jn 10, 40) ou à Béthanie au-delà du Jourdain (Jn 1, 28 ; 3, 26). Le lieu appelé Aenon est identifié au lieu-dit « Uyum » à Ain Fa'rah. L'autre, où est située la première rencontre de Jésus et de Jean est localisée en Pérée au-delà du Jourdain par la tradition chrétienne. Toutefois, selon les travaux de Murphy O'Connor repris par François Blanchetière, cette localisation se révèle impossible à soutenir.

Si la mort de Jean le Baptiste est située par le consensus historien vers 28-29, on ne sait rien en revanche sur la date de sa naissance et encore moins sur ses circonstances. Un seul texte traite de la naissance de Jean le Baptiste : les récits de l'enfance de Jésus que l'évangile selon Luc a ajoutés à la trame de évangile selon Marc dans les années 90. Jean serait né seulement six mois avant Jésus (Lc 1:26), qui lui-même serait né au temps du « premier » recensement, alors que . Un recensement a eu lieu en après J.-C.. Toutefois, le même évangile place la naissance de Jean Baptiste qui est mort en - av. J.-C.. De plus, la version slavonne de la "Guerre des Juifs" évoque une rencontre entre le Baptiste et l'ethnarque Hérode Archélaos qui est destitué et exilé en ap. J.-C, ce qui voudrait dire que Jean était déjà un adulte au moment où Jésus est né. Pour un ensemble de raisons, des historiens comme Gilbert Picard ou Étienne Nodet estiment que Jean appartenait à la génération qui a précédé Jésus, mais la plupart des critiques préfèrent dire que l'on ne sait rien de précis à ce sujet.

Selon la majorité des historiens et exégètes, la mort de Jean Baptiste est à situer vers 28/29, ou, à l'instar de la formulation d'André Paul « vers la fin des années 20 », avant l'arrestation et la crucifixion de Jésus qui aurait eu lieu vers 30 ou 33. Quelques chercheurs optent pour une date un peu plus tardive - vers 32 - dans une chronologie qui fixe en 33 la mort de Jésus de Nazareth, également admise par une partie non négligeable de la recherche.

Dans l'Évangile selon Luc, 

Cet évangile est le seul à évoquer la naissance de Jean le Baptiste :

Il est le fils du prêtre Zacharie et d’Élisabeth, que le verset "Luc" 1, 36 qualifie de « parente », de Marie, la mère de Jésus, alors que la tradition musulmane indique que les deux femmes sont sœurs, ce qui peut s'entendre comme demi-sœurs. Comme celle de Jésus, la naissance de Jean est annoncée à Zacharie par l’archange Gabriel, qui lui dit que son fils à naître, Jean, sera rempli de l’Esprit saint et aura la puissance d’Élie. Une tradition qui fait de Marie la cousine d'Élisabeth, mère de Jean le Baptiste, fixe aussi le lieu de naissance de ce dernier à Sepphoris.

Jean mène une vie d'ascète caché dans le désert, se nourrissant de « sauterelles et de miel sauvage » (Matthieu III:4), et pratiquant le jeûne. Si on suit l'Évangile selon Luc pour dater vers l'an 29 le moment où Jean Baptiste commence à prophétiser, il est à cette époque installé sur les bords du Jourdain, où il pratique le « baptême de repentance » par immersion dans l'eau, ce qui est légèrement différent de la description de son baptême par Flavius Josèphe. Jean réunit autour de lui de nombreux disciples, leur annonçant la venue d'un personnage plus important que lui, que la tradition chrétienne interprète comme le Messie : « Moi, je vous baptise avec de l'eau, pour vous amener à la repentance, mais vient celui plus fort que moi, et je ne suis pas digne de porter ses sandales. Lui vous baptisera dans l'Esprit saint et le feu » (Matthieu III:11).

Dans l'évangile selon Matthieu (III:13-17), Jésus vient voir Jean pour être lui aussi baptisé. Jean lui dit : « C'est moi qui ai besoin d'être baptisé par toi », et Jésus lui répond : « Laisse faire maintenant, car il est convenable que nous accomplissions ainsi tout ce qui est juste. » Jean baptise donc Jésus et c'est au sortir de l'eau que ce dernier voit , tandis Dans l'évangile selon Jean, le baptême de Jésus par Jean disparaît, ce dernier se contente de reconnaître Jésus comme « l'agneau de Dieu ».

Analysant le rôle de Précurseur que le Nouveau Testament confère à Jean, Simon Légasse souligne les distorsions que le texte chrétien doit pratiquer pour préserver l'évocation du Baptiste en écartant toute idée de rivalité possible entre le maître et le disciple. « Les évangélistes évoluent en quelque sorte sur la corde raide lorsqu'ils cherchent à arder Jean Baptiste dans les écrits où Jésus occupe la place centrale ». Ils christianisent Jean en l'incluant dans l'œuvre de salut, avec le souci de ne pas nuire à Jésus. « Luc par exemple prend bien soin de ne pas porter préjudice à Jésus en exaltant le Précurseur. C'est ainsi qu'il élimine ce dernier de son récit avant que Jésus n'y fasse son apparition pour un baptême que le lecteur sait pourtant avoir été administré par Jean » (3, 19-22)".

Les quatre Évangiles citent, au sujet de Jean Baptiste, la prophétie d’Isaïe : (Is 40, 3) « Voix de celui qui crie dans le désert : rendez droit le chemin du Seigneur ».

Dans les évangiles synoptiques, Jean est présenté comme un nouvel Élie ou comme un Élie "redivivus", comme l'ont été d'autres à l'époque y compris Jésus de Nazareth. Même s'il n'a pas eu de contact direct avec les Esséniens, L'audience de ce prophète apocalyptique n'a cessé de croître au point de susciter la réaction « d'Hérode » ainsi que leurs subordonnés, soldats, collecteurs d'impôts et autres publicains. Les autorités religieuses font aussi partie de ses cibles.

À l'époque de Jésus, la tradition juive s'attendait à ce que la venue du « Messie fils de David » soit précédée par le retour du prophète Élie mystérieusement élevé au ciel dans un char de feu (2R 2, 16). Ce nouvel Élie devait frayer un chemin pour préparer le grand jour de YHWH, en invitant les hommes à se convertir et en restaurant Israël. Dans l'évangile selon Marc (Mc 9, 9-13) et celui de Matthieu (Mt 17, 9-13), Jésus le considère comme le précurseur ( "pródromos", ce qui lui vaut d'être nommé le « Prodrome » par les orthodoxes) annoncé sous la figure d'Élie : Dans l'Évangile selon Jean (Jn 1, 19-34), Jean Baptiste nie être Élie car son auteur appartient à une communauté qui dans les années 90-100 considérait que Jésus n'était pas , mais (cf. "Esdras" , Rabbi Dosa dans le Talmud de Babylone, TB Soucca 52a (baraitah), et Manuscrits de la mer Morte).

La colère d'Hérode, tétrarque de Galilée et de Pérée, s'abat sur Jean, lequel lui reproche son union avec Hérodiade, l'épouse de son demi-frère Hérode Boëthos.

Dans l'évangile selon Marc (VI:14-29), Hérode (dont on suppose qu'il s'agit d'Hérode Antipas, malgré le titre de « roi » que lui donne l'évangéliste), excédé par les critiques au sujet de son mariage, ordonne l'arrestation de Jean et « le fait lier en prison ». Sa femme Hérodiade voulait faire tuer Jean mais Hérode Antipas le protégeait, car il le « connaissait pour un homme juste et saint » et « l'écoutait avec plaisir ».

Le Baptiste critique fortement ce mariage en disant à Antipas: . En effet, cette union choquait « en raison de l'interdiction légale du mariage avec la femme de son frère (Lév. 18, 16; 20, 21), que Jean Baptiste rappelait sans ménagement. Selon les évangiles synoptiques, c'est à la suite de ces admonestations de Jean Baptiste qu'Antipas le fait jeter en prison puis décapiter quelque temps plus tard.

Peu après, un récit « plus pittoresque que solide » rapporté par l'Évangile selon Marc, décrit les circonstances de la mise à mort de Jean.

Ce récit est inconnu de l'historien Flavius Josèphe, qui écrit d'après la commande que lui ont fait les empereurs Vespasien et Titus. Pour plusieurs auteurs, cette « séquence évangélique », « n'est pas sans évoquer le livre d'Esther. » Pour Claudine Gautier, « le récit évangélique emprunte à deux sortes de sources. Des sources vétérotestamentaires tout d’abord. Cette jeune fille à qui, parce qu’elle lui a plu au cours d’un banquet, un roi promet : « ce que tu me demanderas, je te le donnerai, jusqu’à la moitié de mon royaume » (Marc 6,23), n’est pas sans rappeler l’héroïne du livre d'Esther, à qui le roi Assuérus, séduit lui aussi au cours d’un banquet, fait mot pour mot la même promesse (Esther 5,3-6 ; 7,2). La première reçoit sur un plat la tête du Baptiste, la seconde obtient la mise à mort de Haman, le conseiller félon. »

Flavius Josèphe de son côté dit simplement que Jean fut exécuté à Machéronte après y avoir été incarcéré, Hérode Antipas craignant que ce prophète n'utilise l'emprise qu'il avait sur la population pour la pousser à la révolte. Outre cette crainte d'Hérode Antipas, Jean le Baptiste est probablement victime de sa prédication qui entend substituer l'immersion baptismale aux sacrifices, relativisant de la sorte l’importance du rôle des élites sacerdotales et celui du Temple, comme il est possible que son jugement des mœurs d'Hérode - fustigeant le souverain et son union scandaleuse avec la femme (Mc 6, 17) de son demi-frère - ait contribué également à sceller son sort.

Le personnage de Jean le Baptiste apparaît fortuitement dans le livre des "Antiquités judaïques" qui évoque une guerre entre le roi Arétas IV de Pétra (roi des Nabatéens) et Hérode Antipas qui résulte d'un conflit de succession après la mort de Philippe le Tétrarque en 33-34.

Selon Flavius Josèphe Jean le Baptiste a été exécuté pour des raisons politiques : personnage populaire, Jean faisait de l'ombre à Hérode Antipas et pouvait user de son influence sur la foule pour provoquer une révolte contre le pouvoir en place :

Pour Pierre Geoltrain, 

L'évangile selon Jean témoigne lors de son écriture dans les années 90-100 qu'il y eut rivalité entre le mouvement des disciples de Jésus et les baptistes ne l'ayant pas reconnu comme Messie. La communauté des mandéens, composée de fidèles de Jean le Baptiste, verra en lui l'ennemi de Jésus-Christ.

L'évangile selon Jean et l'arrière-plan des Actes des Apôtres laissent percevoir une lutte d'influence entre les disciples du Baptiste, qui voient en lui le Messie (Jn 3, 28), et ceux de Jésus.

Selon les Actes, vers 50 à Éphèse (province proconsulaire d'Asie), un Juif de naissance, venant d'Alexandrie et nommé Apollos (ou Apollonios), est considéré par des disciples de Paul de Tarse comme faisant partie de leur mouvement, « bien qu'il connût seulement le baptême de Jean (le Baptiste) ». Ainsi, la prédication de Jean le Baptiste avait fait des adeptes en Égypte. Selon François Blanchetière, Apollos, « formé à Alexandrie dans un milieu qui ressemblait aux Thérapeutes de Philon, avait adopté le baptême de Jean. Les membres du mouvement attendaient avec impatience la venue du Christ, le roi messianique qui les délivrerait de la domination des Romains ». Comme les membres de la communauté d'Éphèse, Apollos devient alors « adepte de la Voie du Seigneur » (ou instruit de la Voie du Seigneur), ce qui est le nom des partisans de Jésus. Les communautés messianistes d'Égypte en ont probablement disparu lors du massacre des Juifs d'Égypte sous Trajan qui a suivi la révolte des exilés (116-117).

Il est vraisemblable que des communautés juives baptistes se réclamant de lui aient continué à exister y compris après la répression des trois grandes révoltes juives (grande révolte (66-73), révolte des exilés (115–117), révolte de Bar Kokhba (132-135)) et notamment après la destruction de Jérusalem (135) et l'interdiction à tout Juif d'y pénétrer.

Selon la tradition des mandéens, leur communauté se serait formée autour de Jean Baptiste, ils pourraient faire partie de ceux qui ne se sont pas ralliés à Jésus. Selon eux, et notamment le "Haran-Gawaita", leur départ de Palestine aurait eu lieu en 37-38. 

Pour André Paul ou Simon Claude Mimouni, les groupes mandéens existant actuellement en Irak et en Iran relèvent du seul courant vraiment baptiste qui ait persisté jusqu'à aujourd'hui.

Lors de son apparition au 

Selon François Blanchetière, au , outre les Esséniens 

Selon André Paul, Jean Baptiste . Devenu un , son surnom . . Le baptême de Jean servait également à « purifier le corps », l'âme étant purifiée au préalable « par la justice ».

Si Jean le Baptiste , .

Saint Jean Baptiste, la vierge Marie et Jésus-Christ sont les trois seules personnes que l'Église orthodoxe célèbre par trois fêtes le jour de leur conception, celui de leur nativité et celui de leur mort.
Les fêtes de Jean Baptiste sont les suivantes :
Les Églises chrétiennes fêtent sa nativité, aussi bien en Orient qu’en Occident, six mois avant Noël, le , au moment du solstice d’été ; c’est une exception à la tradition de fêter les saints le jour de leur mort. Parmi les nombreux rites qui sont associés à cette fête, certains semblent venir directement des anciennes grandes fêtes celtes du solstice d’été, lorsque cette nuit était réputée surnaturelle, et des feux cérémoniels allumés. La pratique des feux de la Saint-Jean, directement hérités des fêtes polythéistes du solstice d'été, reste très vivace dans de nombreuses villes et villages du monde occidental.

Comme la fête de Noël pour la date de naissance de Jésus, la date du pour fêter celle de Jean Baptiste a été choisie au . Les deux naissances sont ainsi placées à six mois d'écart, trois jours après chaque solstice, moment où avec un moyen d'observation rudimentaire, on peut voir que la durée des jours commence à augmenter (), ou à diminuer (). Pour l'Église catholique parvenue au pouvoir, il s'agit à la fois de « recouvrir » deux fêtes païennes par des fêtes devenues chrétiennes, mais aussi d'illustrer tant l'écart de six mois entre les nativités de Jean et de Jésus, que la phrase attribuée au Baptiste parlant de Jésus: « Il faut que lui grandisse et que moi je décroisse. » Pour Alexandre Najjar, « l'église a ainsi christianisé le vieux rite païen qui célébrait l'astre du jour: le soleil qui commence sa descente à partir du symbolise Jean Baptiste; quand il recommence sa montée à partir du , il représente Jésus. » « Entre le Précurseur et le Messie, Luc s’attache d’ailleurs à construire une rigoureuse symétrie : ils naissent à six mois d’intervalle, ce que la tradition chrétienne a conservé en célébrant leurs naissances aux deux solstices opposés. L’un naît d’une femme réputée trop vieille pour enfanter, l’autre d’une vierge ; l’un est rempli du Saint-Esprit, l’autre est conçu du même Saint-Esprit. » Pour Claudine Gauthier, cette opposition si complète les ramène à l’unité et en fait des quasi-jumeaux.

Les reliques attribuées à Jean Baptiste existent dans presque tous les pays chrétiens et à Damas, en pays musulman. Prétendues reliques car elles font l'objet, comme tous les corps des saints, d'un commerce international au Moyen-Âge et que l'invention de reliques, même fausses, est souvent réalisée à des moments cruciaux pour les églises ou communautés monastiques, leur permettant de « sortir de difficultés financières, de réaffirmer le pouvoir d'un évêque, de défendre le bien-fondé d'une réforme, etc ». Des éclats d'os du saint sont même vendus sur des sites web comme eBay.

Plusieurs textes anciens font état au de l'existence du tombeau de Jean Baptiste à Sebaste en Samarie, saint Jérôme témoignant des miracles liés à ce lieu de pèlerinage qui avait vertu de chasser les démons et de guérir les possédés. Théodoret de Cyr en fait aussi état. Rufin d'Aquilée accuse même l'empereur Julien (361-363) d'avoir ordonné la destruction de celui-ci et l'incinération du corps qui s'y trouvait, « les os brulés et jetés au vent ». Cet empereur est connu pour son écrit contre les chrétiens, qu'il appelle « les "Galiléens" ».

Toutefois, les historiens n'accordent que peu de crédit à ces relations polémiques, émanant d'auteurs chrétiens très hostiles à cet empereur qui avait voulu revenir à la tolérance religieuse, avait mis en place une législation anti-chrétienne et tenté d'organiser une « église païenne ».
En 333, le pèlerin anonyme de Bordeaux ne signale pas la présence de ce tombeau. Cette localisation près de Sébaste, impliquerait que la forteresse "Machareous" dont parle Flavius Josèphe ne serait pas Macheronte au fin fond sud de la Pérée, mais la forteresse "Machareous", située par le même auteur au nord de la forteresse de l'Alexandrion dans d'autres de ses volumes.

Alexandre Najjar n'a toutefois aucun doute et raconte cette « terrible profanation ». D'après lui, des moines auraient sauvé une partie des ossements « qu'ils transportèrent à Jérusalem et qu'ils remirent à l'abbé Philippe qui les confia à son tour à saint Athanase, évêque d'Alexandrie. » C'est en tout cas ce que relate Rufin d'Aquilée.

Par contrecoup, la dispersion des cendres et l'anéantissement du corps de saint Jean Baptiste rehausse la valeur de la relique insigne de son chef, possession revendiquée par plusieurs sanctuaires. En suivant ce cheminement, certains pensent donc que le corps du prophète, se trouverait sous le mur Nord de la grande église d'Alexandrie, découvert en 1976. Mais de nombreux édifices religieux pensent, ou ont pensé, détenir ce corps. Ainsi un tombeau qui pourrait contenir la tête de Jean Baptiste ("Yahya") se trouve dans la Grande Mosquée des Omeyyades de Damas, construite à partir de 705 sur l'emplacement de la basilique byzantine dédiée à Saint-Jean-Baptiste. Selon Ibn Asakir, auteur d'un dictionnaire biographique de l'histoire de Damas en , lors de la construction de la mosquée le calife Al-Walīd aurait choisi de conserver à l'intérieur de son enceinte, la tête de Jean Baptiste dans son site original.

En 2004, l'archéologue prétend avoir trouvé la grotte de Jean le Baptiste dans la vallée sauvage du kibboutz près d'Ein Kerem où une tradition locale qui, selon le moine Theodericus du remonte à l'impératrice Hélène, fixe le village de naissance de Jean Baptiste. Toutefois, aucun texte antique ne vient à l'appui de cette tradition. Cette découverte laisse sceptique les historiens. L'essentiel de ce qui a été trouvé est une grande citerne qui, selon Gibson, aurait servi de fonts baptismaux. Toutefois, les immersions des baptistes antiques devaient se faire impérativement dans de l'eau vive. Les fouilles archéologiques débutées en 1999, ont aussi mis en évidence des poteries datées du et des graffitis probablement de moines byzantins du dont un qui représente un personnage vêtu d'une peau de bête et tenant dans sa main gauche un bâton pastoral, rappelant la représentation de saint Jean Baptiste dans l'art byzantin.

L'auteur anonyme (probablement un clerc séculier) de "La Vie Saint Jehan-Baptiste" en 1322 rapporte que dès l'origine, la tête du saint n'a pas été inhumée avec ses os. De plus, l'histoire des trois inventions du chef de Jean le Baptiste est assez confuse à cause du nombre des informations divergentes fournies par des sources de différentes époques. La divergence de ces traditions hagiographiques explique que de nombreuses églises ont revendiqué détenir comme relique insigne tout ou partie de la tête de Jean Baptiste :





Une hypothétique main droite du prophète, constitue, avec l'icône de la Vierge de Philerme, le trésor des Hospitaliers de l'ordre de Saint-Jean de Jérusalem. Elle est aujourd'hui conservée au monastère de Cetinje au Monténégro.

En Bulgarie, lors de fouilles sur l'île Saint-Yvan à côté de la ville de Sozopol, les archéologues ont mis au jour les vestiges d'une église orthodoxe qui date du – . Sous son autel, ont été trouvées, en 2010, dans un sarcophage une partie de la face, une dent et une phalange de la main d'un homme, qui pour certains chrétiens seraient celles de Jean le Baptiste . Les chercheurs se basent sur des inscriptions en grec ancien sur une boîte de tuf près du sarcophage mentionnant « Jean 24 juin » (jour de fête de Jean-Baptiste) et « notre serviteur Thomas », qui selon certaines théories aurait été chargé d'apporter ces reliques sur l'île.

En 2012, Le service de l'université d'Oxford spécialisé dans la datation des reliques effectue une datation au carbone 14 de ces os. La datation situe ces ossements à environ 30 après Jésus Christ ce qui est compatible avec l'hypothèse St Jean-Baptiste.


Comme dans la tradition chrétienne, pour le Coran (VIIe s.), Jean Baptiste (Yahyâ يحيى en arabe) et Jésus (ʿĪsā عيسى) sont cousins. De même, reprenant les traditions des Evangiles apocryphes, Zacharie, le père de Jean le Baptiste, élève Maryam la future mère de Jésus-ʿĪsā dans le Temple, alors que celle-ci est très jeune.

Le Coran fait référence à Maryam (la Vierge Marie chez les chrétiens) comme faisant partie de la maison d’'Îmran (en arabe آل عمران, en hébreu 'Amram). Maryam ("Marie") y est appelée la « fille d’'Îmran », ce qui est plus une référence à son ancêtre que le nom de son père réel, qui n’est pas nommé dans le Coran. La tradition musulmane interprète d’ailleurs le Coran, comme faisant référence à un père ancestral plutôt qu’à un père littéral. Le père ancestral dont il question est Amrân, haut fonctionnaire de pharaon, père de Moïse et de Aaron.

La tradition musulmane ainsi que des érudits et des commentateurs du Coran font un parallèle entre « Maryam fille d'Imran » selon le Coran, c'est-à-dire Marie, mère de Jésus selon les évangiles, et Élisabeth, la mère de Jean le Baptiste ("Yahya"), descendante d'Aaron dans l'Évangile de évangile attribué à Luc. Ce qui n'est qu'un parallèle dans la tradition musulmane est plus précis dans l'évangile attribué à Luc, puisque Élisabeth est décrite comme une « parente » de Marie dans cet évangile.

Dans la tradition chrétienne, Jean le Baptiste est le fils du prêtre Zacharie et d'Élisabeth, qui serait une cousine de Marie, la mère de Jésus (l'évangile attribué à Luc dit qu'Élisabeth est « une parente » de Marie, « cousine » serait une précision apportée par la tradition orale).

Dans la tradition musulmane, Élisabeth s'appelle Îsha (ou Ashâ`) et est l'épouse de Zacharie et la mère de "Yahya" (le Baptiste). Selon le "Dictionnaire du Coran", des historiens anciens indiquent que Îsha et Hannah (Élisabeth et Anne) « seraient deux sœurs, filles de Fâqûdh. »

Dans la tradition musulmane, la mère de Maryam porte le nom de Hannah, l'équivalent arabe d'Anne, qui est le nom par lequel les chrétiens désignent la mère de Marie. Hannah est également honorée par les musulmans comme femme très vertueuse.

De même dans la tradition musulmane, « dès sa conception, Marie est consacrée à Dieu et confiée à sa naissance à Zacharie, le père de Jean le Baptiste. Comme dans les évangiles apocryphes, Marie est élevée au Temple de Jérusalem »

Le Coran se réfère au père de Moïse comme 'Îmran. 'Îmran est également, pour les musulmans, le nom du père de Marie et époux de Anne, qui n'apparaît pas dans le Nouveau Testament mais que les traditions chrétiennes appellent Joachim (Protévangile de Jacques).

La sourate III du Coran porte le nom de la famille de `Imrân ou `Imrân (en arabe : al ʿimrān, ).

La tradition musulmane ainsi que des érudits et des commentateurs du Coran font un parallèle entre « Maryam fille d'Imran » selon le Coran, et Élisabeth, descendante d'Aaron dans la Bible. Ils interprètent ces deux phrases comme faisant référence à un père ancestral plutôt qu'à un père littéral. Dans l'islam, Maryam est aussi appelée la « sœur d'Aaron », ce qui serait une autre référence à l'ancêtre, dont elle descendrait. Le père littéral de Maryam porte aussi le nom d''Îmran dans la tradition musulmane, bien qu'il doive être distingué du père de Moïse et Aaron, et qu'il corresponde à Joachim dans le Nouveau Testament, considéré par les musulmans comme l'un des hommes vertueux présents à Jérusalem à cette époque. La mère de Maryam porte le nom de "Hannah", l'équivalent arabe d'Anne. Elle est également honorée par les musulmans comme femme très vertueuse, à l'instar de sa fille. Anne(en arabe Hannah) et Elisabeth(en arabe Isha) étant des sœurs dans la tradition musulmane.

Ces deux versets renvoient à la mère de Marie mère de Jésus (Anne dans la tradition chrétienne), et dans ce cas `Imran est celui que la tradition chrétienne du Protévangile de Jacques, appelle Joachim.

Le Coran parle de Jean le Baptiste qui se nomme Yahyâ en arabe, le Livre saint des musulmans décrit Jean le Baptiste comme un prophète d'Allah envoyé à son peuple.

Voici le passage concerné (Sourate 19 : Marie, versets 2 à 15) :
Au verset 7, Allah dit : « Son nom sera Yahya [Jean]. Nous ne lui avons pas donné auparavant d'homonyme ». Pour Pierre Lory, « le Coran insiste sur le nom donné à l'enfant, nom qu'il serait le premier à porter. il y a là sans doute un écho à l'évangile de Luc. Faut-il voir dans la tendresse (« "hanân" » ; sourate 19, verset 3) que lui accorde Dieu une allusion à son nom hébraïque Yohanan ? Quoi qu'il en soit, le nom coranique de Jean, Yahyâ, évoque des connotations très particulières, la racine h. y. y. signifiant la vie. »





</doc>
<doc id="4867" url="https://fr.wikipedia.org/wiki?curid=4867" title="Navigateur web">
Navigateur web

Un navigateur web («browser» en anglais) est un logiciel conçu pour consulter et afficher le World Wide Web. Techniquement, c'est au minimum un client HTTP.

Il existe de nombreux navigateurs web, pour toutes sortes de matériels (ordinateur personnel, tablette tactile, téléphones mobiles, etc.) et pour différents systèmes d'exploitation (GNU/Linux, Windows, Mac OS, iOS et Android). Dans les années 2010, les plus utilisés sont Google Chrome, Mozilla Firefox, Internet Explorer/Edge, Safari, Opera.

Le terme "navigateur" est inspiré de Netscape Navigator, le navigateur phare en 1995 et 1996.

D'autres métaphores sont ou ont été utilisées. Le premier terme utilisé était "", comme en anglais. Par la suite, on a vu "fureteur" (surtout utilisé au Québec), "butineur", "brouteur", "arpenteur", "fouineur" ou encore "explorateur" (inspiré d"Internet Explorer"). Le terme "navigateur internet", bien qu'incorrect, est également souvent rencontré. Aujourd'hui, les termes "navigateur" ou "logiciel de navigation" sont recommandés en France par la DGLFLF, ainsi qu'au Canada par l'OQLF.

Les navigateurs web sont des logiciels complexes et en constante évolution, tant au niveau des fonctionnalités, qu'au niveau de la sécurité, car des nouvelles fonctionnalités offrent de nouveaux angles d'attaque.

Le premier navigateur s'appelle WorldWideWeb. Il est développé par l'inventeur du Web, Tim Berners-Lee, en octobre-novembre 1990. C'est un navigateur en mode graphique. Ce premier navigateur était en plus un éditeur HTML, ce qui n'est pas commun. La rapidité de développement a été rendue possible par l'ordinateur choisi pour ce projet, un NeXT. Toutefois, le choix de cet ordinateur rare a sévèrement limité la diffusion du navigateur. Plus tard, le navigateur est renommé Nexus pour éviter la confusion avec le World Wide Web.

Le premier navigateur pour le système X Window sur Unix fut Erwise, créé en 1992 en Finlande par les étudiants Kim Nyberg, Kari Sydänmaanlakka, Teemu Rantanen, et Kati Borgers (née Suominen).

Le troisième navigateur fut ViolaWWW.

En 1993, apparaît NCSA Mosaic. Ce navigateur disponible sur de nombreux systèmes provoqua l'accélération fulgurante et exponentielle du Web. Il est le premier navigateur à afficher les images (GIF et XBM) dans les pages web elles-mêmes, puis à supporter les formulaires interactifs dans les pages.

Dès 1995, Netscape Navigator devint le navigateur dominant, développé par Marc Andreessen, ancien développeur de Mosaic.

En 1995 sortit également Internet Explorer 1 de Microsoft.

À partir de 2000, et après plusieurs années de « guerre des navigateurs » (voir Évolution de l'usage des navigateurs web), c'est Internet Explorer qui devint le navigateur le plus utilisé. La raison en est principalement sa pré-installation au sein de Microsoft Windows, le système d'exploitation le plus utilisé sur les ordinateurs personnels à cette époque.

À compter de 2005, on note une régression de la part de marché d'Internet Explorer, surtout au bénéfice de Mozilla Firefox.

En décembre 2008 sort le navigateur Google Chrome, édité par Google. En 2010 il fait partie des trois navigateurs les plus utilisés et en 2012, il devient un des navigateurs les plus utilisés.

De nombreux autres navigateurs, appelés navigateurs alternatifs, se partagent les miettes, notamment :

Devant le développement croissant des smartphones et tablettes numériques, plusieurs concepteurs de navigateurs sortent des versions mobiles ; Le navigateur Firefox Mobile (nom de code Fennec) est la version mobile de Firefox, Opera Mobile pour Opera, Internet Explorer Mobile la version mobile d'IE Des éditeurs proposent des navigateurs uniquement pour mobiles : UC Browser, Dolphin Browser

De plus, depuis le milieu des années 1990, il a toujours existé une multitude d'autres navigateurs plus ou moins populaires.

Un navigateur web est composé de divers modules logiciels permettant de communiquer suivant des standards des réseaux, d'un moteur de rendu des standards du Web, d'une interface utilisateur adaptée au système d'exploitation qui l'accueille et accessoirement d'un gestionnaire d'extensions appelées "plugins".

Pour la grande majorité des navigateurs sur le marché, l'interface utilisateur est composée d'une zone d'affichage éventuellement gérée sous forme d'onglets, d'une barre de menus déroulants, d'une barre d'outils, d'une barre d'état et surtout d'une barre d'adresse, parfois fusionnée, comme dans le cas de Chrome et de Chromium avec la barre de recherche. Selon les navigateurs il est possible de réorganiser et d'afficher ou non ces différentes barres.

La barre de menus abrite les favoris (ou marque-pages), les commandes des fichiers (ouverture, fermeture), les options de configuration

La fonction principale d'un navigateur web est de permettre la consultation d'informations disponibles (« ressource » dans la terminologie du Web) sur le World Wide Web. Les principales étapes de la consultation d'une ressource sont les suivantes :

Un navigateur web doit être capable, au minimum, d'afficher le texte d'une page web. Un navigateur en mode texte n'affiche souvent rien de plus. Les navigateurs couramment utilisés fonctionnent cependant en mode graphique et sont capables d'utiliser une typographie élaborée, d'ajouter des images dans le texte, de jouer de la musique et des animations et d'interagir avec les actions de l'utilisateur.

Une page web est un texte écrit dans le langage informatique HyperText Markup Language (HTML) qui donne au navigateur le texte à afficher ainsi que la structure générale de la mise en page : titres et paragraphes, listes, tableaux. La mise en page peut être raffinée par l'utilisation de feuilles de style en cascade (CSS) : marges, alignements, espacements, couleurs, bordures, etc.

La position des images dans une page web est donnée par le langage HTML. Les images sont généralement dans un format de données parmi les trois suivant : GIF, JPEG ou PNG.

La plupart des navigateurs permettent d'imprimer les pages web en noir et blanc ou en couleurs. En outre, des dispositifs particuliers peuvent être utilisés pour pallier un handicap visuel ou moteur.

Dans le contexte du Web, un script est un programme informatique intégré à la page web et exécuté par le navigateur. Un script permet des tâches simples comme vérifier les données entrées dans un formulaire, gérer des menus ou réagir aux déplacements du pointeur de souris. Le principal langage de script côté client est le JavaScript. Ce langage est standardisé par l'ECMA sous le nom d'ECMAScript.

Les navigateurs pouvant difficilement gérer l'ensemble des formats de données existants et futurs, les éditeurs de navigateurs ont été amenés à développer un mécanisme de modules d'extension qui prennent le nom de "plugins", ou de contrôles ActiveX pour Internet Explorer.

Ceux-ci permettent d'afficher directement dans la fenêtre du navigateur des données de formats très différents allant de documents PDF à du contenu multimédia. Ce mécanisme permet également d'utiliser des langages de programmation côté client comme les animations Flash, les applets Java ou la technologie Silverlight. La large diffusion d'un plugin étant nécessaire pour qu'un format de données ait une chance de s'imposer sur le Web, on en trouvera bien souvent en libre téléchargement sur les sites de sociétés éditrices de logiciels permettant de créer un type de contenu particulier.

Le protocole HTTP n'est pas le seul qu'un navigateur web puisse utiliser. Il peut gérer aussi les transferts de fichiers à l'aide de FTP ou WebDAV, et établir des connexions sécurisées en utilisant HTTPS (HTTP sur SSL). Certains navigateurs (comme Firefox) supportent le protocole Gopher qui précéda de peu HTTP et devenu confidentiel aujourd'hui. Ces protocoles de communication sont généralement standardisés par l'IETF.

Du fait de sa forte diffusion dans le monde le navigateur web est une cible privilégiée des pirates désireux de nuire aux internautes.

L'exploitation des traces numériques du navigateur web est utilisé par certaines entreprises de commerce électronique.

L'utilisation d'un login sur un navigateur peut dès la première fois ouvrir la possibilité d’être suivi par des sociétés de collecte de données lors des utilisation ultérieures du navigateur.

L’utilisation d'un bloquer de publicité pet permettre à une société tierce d'accéder à des données privées.

L'internaute est menacé via son navigateur d'être victime des attaques suivantes :

Un navigateur est constitué de modules divers réalisant des fonctions bien déterminées et possédant chacun leurs vulnérabilités propres :

Note: ces statistiques ne sont que des estimations, et comprennent une marge d'erreur difficile à estimer. Ces erreurs sont dues à plusieurs raisons : 

Il existe des versions différentes des navigateurs selon le type de machines : téléphone mobile, tablette ou ordinateur. Voici pour les navigateurs les plus populaires les noms de leur version plate-forme bureautique et mobile.



</doc>
<doc id="4869" url="https://fr.wikipedia.org/wiki?curid=4869" title="Navigateur">
Navigateur






</doc>
<doc id="4877" url="https://fr.wikipedia.org/wiki?curid=4877" title="Liste des communes du Gers">
Liste des communes du Gers

Cette page liste les 462 communes du département du Gers par ordre alphabétique, avec leur code Insee et leur code postal principal. Voir aussi les listes des cantons et des anciennes communes du département.

La graphie du nom en occitan (dialecte gascon) est celle de la carte des communes du Gers en gascon (graphie en norme classique occitane).


</doc>
<doc id="4878" url="https://fr.wikipedia.org/wiki?curid=4878" title="Abréviations en informatique Z">
Abréviations en informatique Z



</doc>
<doc id="4879" url="https://fr.wikipedia.org/wiki?curid=4879" title="Abréviations en informatique K">
Abréviations en informatique K



</doc>
<doc id="4880" url="https://fr.wikipedia.org/wiki?curid=4880" title="Abréviations en informatique Q">
Abréviations en informatique Q



</doc>
<doc id="4881" url="https://fr.wikipedia.org/wiki?curid=4881" title="Neutron">
Neutron

Le neutron est une particule subatomique de charge électrique nulle.

Les neutrons sont présents dans le noyau des atomes, liés avec des protons par l'interaction forte. Si le nombre de protons d'un noyau détermine son élément chimique, le nombre de neutrons détermine son isotope. Les neutrons liés dans un noyau atomique sont en général stables mais les neutrons libres sont instables : ils se désintègrent en un peu moins de 15 minutes (880,3 secondes). Les neutrons libres sont produits dans les opérations de fission et de fusion nucléaires.

Le neutron n'est pas une particule élémentaire, étant composé de trois autres particules : un quark up et deux quarks down.

Le neutron est un fermion de spin ½. Il est composé de trois quarks (deux "down" et un "up"), ce qui en fait un baryon de charge électrique nulle. Ses quarks sont liés par l'interaction forte, transmise par des gluons.

La masse du neutron est égale à environ , soit à peu près ou . Le neutron est plus massif que le proton. Sa charge électrique est nulle. Tout comme le proton, le neutron est un nucléon et peut être lié à d'autres nucléons par la force nucléaire à l'intérieur d'un noyau atomique. Le nombre de protons d'un noyau (son numéro atomique, noté "Z") détermine les propriétés chimiques de l'atome et donc quel élément chimique il représente ; le nombre de neutrons (usuellement noté "N") détermine en revanche l'isotope de cet élément. Le nombre de masse (noté "A") est le nombre total de nucléons du noyau : "A" = "Z" + "N".

Le modèle standard de la physique des particules prédit une légère séparation des charges positive et négative à l'intérieur du neutron, conduisant à un moment dipolaire électrique permanent. La valeur prédite est cependant trop petite pour être mesurée avec les instruments actuels.

Le neutron possède une antiparticule, l'antineutron.

Selon les contraintes du modèle standard de la physique des particules, comme le neutron est composé de trois quarks, son seul mode de désintégration possible (sans modifier le nombre baryonique) suppose le changement de saveur d'un quark par l'intermédiaire de l'interaction faible. La désintégration d'un quark down, de charge -1/3, en un quark up, de charge +2/3, est réalisée par l'émission d'un boson W ; de cette façon, le neutron se désintègre en un proton (qui contient un quark down et deux quarks up), un électron et un antineutrino électronique.

À l'extérieur d'un noyau atomique, le neutron libre est instable et sa durée de vie moyenne est de ± (soit un peu moins de 15 minutes ; la demi-vie correspondante est de 880,3 x ln (2) = , soit un peu plus de 10 minutes). Il se désintègre suivant le processus décrit ci-dessus. Ce processus, nommé désintégration bêta, peut également transformer un neutron à l'intérieur d'un noyau atomique instable.

Ces durées de vie (moyenne et demi-vie) sont très supérieures aux durées de vie des neutrons rencontrées dans un réacteur nucléaire, ce qui fait que la disparition des neutrons par désintégration peut être négligée dans le bilan neutronique (production/disparition) du réacteur.

À l'intérieur d'un noyau atomique, un proton peut se transformer en neutron par un processus de désintégration bêta inverse. La transformation provoque également l'émission d'un positron (un antiélectron) et d'un neutrino électronique.

Dans un noyau atomique, l'instabilité du neutron est contrebalancée par celle qui serait acquise par le noyau dans son ensemble si un proton additionnel participait aux interactions répulsives des autres protons déjà présents. De cette façon, si les neutrons libres sont instables, les neutrons liés ne le sont pas forcément. En astrophysique, la stabilité du neutron peut être obtenue non plus par l’interaction forte mais par la gravitation. Une étoile à neutrons est un astre extrêmement dense dont la composition interne est majoritairement faite de neutrons maintenus ensemble par le très fort champ gravitationnel qu'ils génèrent du fait de leur grand nombre et de leur haute densité. La désintégration du neutron est cette fois rendue impossible par le principe d'exclusion de Pauli qui empêche les électrons ainsi produits de coexister en grand nombre.

La radioactivité produit des neutrons libres. Ces neutrons peuvent être absorbés par les noyaux d'autres atomes qui peuvent alors devenir instables. Ils peuvent aussi provoquer une fission nucléaire par collision avec un noyau lourd fissile (plutonium 239, uranium 235...).

Le neutron étant globalement neutre, il ne produit pas directement d'ionisations en traversant la matière. En revanche, il peut avoir de nombreuses réactions avec les noyaux des atomes (capture radiative, diffusion inélastique, réactions produisant des particules α ou d'autres neutrons, fission du noyau), produisant chacune des rayonnements ionisants. À ce titre, les neutrons sont considérés comme un rayonnement ionisant, soit un rayonnement qui produit des ionisations dans la matière qu'il traverse.

 est le premier à prédire en 1920 l'existence du neutron. 

Ayant découvert l'existence du noyau atomique en 1911, Ernest Rutherford émet en 1920 l’hypothèse de l’existence du neutron comme une association proton-électron. James Chadwick, l’assistant de Rutherford et l’un de ses plus brillants disciples, entendit Rutherford, dans le cercle des habitués des "Bakerian Lectures" de la "Royal Society", formuler l’idée d’une sorte d’atome de masse 1 et de charge 0 qui n’était pas l’hydrogène : cet objet n’est pas sujet aux répulsions électriques que subissent les protons et les particules alpha et doit pouvoir s’approcher des noyaux et y pénétrer facilement. Chadwick se souvint douze ans plus tard de cette communication, quand il eut à interpréter les résultats de ses expériences. 

Pendant toutes les années 1920, les physiciens supposent que le noyau atomique est composé des protons et "électrons nucléaires". Par exemple, le noyau de N contenait supposément 14 protons et 7 électrons nucléaires, en plus des 7 "électrons orbitaux" à l'extérieur du noyau. Cependant des difficultés de ce "modèle proton-électron" deviennent évidents. Le modèle est difficile à réconcilier avec le principe d'incertitude de Heisenberg. Le paradoxe de Klein, découvert par Oskar Klein en 1928, soulève encore d'autres objections au confinement d'un électron léger à l'intérieur d'un volume aussi petit qu'un noyau.

En plus, les propriétés observées des atomes et des molécules ne sont pas cohérentes avec le spin nucléaire prévu par le modèle proton-électron. Par exemple, si le noyau N contenait vraiment un total de 21 particules (protons et électrons), chacun de spin ½ "ħ", son spin devrait être un multiple demi-entier de ħ. Cependant les spectres moléculaires de N indiquent que le vrai spin de N est de 1(ħ), ce qui implique un nombre "pair" de particules constituantes.

La découverte du neutron a résulté de trois séries d’expériences, faites dans trois pays différents, l’une entraînant l’autre. En ce sens elle est exemplaire de la recherche de la connaissance.

En 1930, en Allemagne, Walther Bothe et Herbert Becker, spécialistes du rayonnement cosmique observent que les éléments légers lithium, béryllium et bore, bombardés par des particules α, émettent des rayons « ultra pénétrants » qu’ils supposent être des rayons gamma beaucoup plus énergiques que ceux émis par des noyaux radioactifs ou accompagnant les transmutations nucléaires.

En 1931, en France, Irène et Frédéric Joliot-Curie intrigués par ces résultats cherchent à comprendre la nature de ce rayonnement et découvrent qu’il a la propriété de mettre en mouvement des noyaux atomiques et en particulier des protons… Ils supposent qu’il s’agit là d’un effet Compton entre des gamma dont ils estiment l’énergie à environ (une énergie très élevée pour l’époque) et de l’hydrogène.

En 1932, en Angleterre, aussitôt ces résultats parus, James Chadwick fait un test confirmant les résultats et va plus loin et mesurant avec précision l’énergie des noyaux projetés en utilisant la réaction nucléaire He(α) + Be → C + n, il peut affirmer que le rayonnement « ultra pénétrant » ne peut être un rayonnement gamma, d’énergie très élevée, mais doit être composé de particules de masse 1 et de charge électrique 0 : c’est le neutron.

Chacune des trois équipes avait travaillé avec les appareils dont elle disposait, mais aussi avec ses connaissances et avait baigné dans la tradition de son laboratoire. Il n’est pas étonnant que ce soit au laboratoire de Cambridge, dirigé par Ernest Rutherford que le neutron ait été découvert. Depuis 1920, Rutherford, en effet, avait émis l’hypothèse de l’existence du neutron comme une association proton-électron. Cependant l'explication des propriétés nucléaires oblige de reconnaître que le neutron est plutôt une particule aussi élémentaire que le proton.

Werner Heisenberg développe rapidement un "modèle proton-neutron" du noyau constitué des protons et neutrons, ce qui réussit à expliquer les valeurs observées des spins nucléaires. De plus, en 1934 Enrico Fermi explique la radioactivité β comme la transformation d'un neutron par l'émission d'un électron (créé au moment de son émission) ainsi qu'un neutrino (qui restait encore à découvrir une vingtaine d'années plus tard). Aussi Fermi effectue le bombardement des éléments lourds avec les neutrons afin d'induire la radioactivité aux éléments de numéros atomiques élevés.

En 1935, Chadwick et son étudiant Maurice Goldhaber font la première mesure précise de la masse du neutron. La même année Chadwick gagne le Prix Nobel de physique pour la découverte du neutron. En apprenant cette nouvelle, Rutherford dira, selon Emilio Segrè : 

En 1938, Fermi reçoit le prix Nobel en physique pour avoir démontré l'existence de nouveaux éléments radioactifs produits par l'irradiation neutronique, ainsi que pour la découverte des réactions nucléaires induites par des neutrons lents. Cette dernière découverte amène Otto Hahn, Lise Meitner, et Fritz Strassmann à la découverte de la fission nucléaire induite par les neutrons lents.

Ont également étudié les propriétés du neutron : Jean-Louis Destouches, Igor Tamm, Franz N. D. Kurie.

Les particules atomiques et subatomiques sont détectées par la signature qu'elles produisent par interaction avec leur environnement. Ces interactions résultent de leurs caractéristiques fondamentales. Du fait notamment de sa charge globalement nulle, le neutron est généralement détecté par interaction nucléaire, c'est-à-dire par l'utilisation de réactions nucléaires spécifiques.

Les neutrons sont utilisés pour la diffusion neutronique, processus permettant d'étudier de la matière à l'état condensé. Ce rayonnement pénétrant permet de voir les intérieurs des corps, comme des métaux, des minerais, des fluides et permet d'examiner leur structure à l'échelle atomique par diffraction. Un autre avantage des neutrons réside dans leur sensibilité magnétique due à leur spin, ce qui permet d'étudier la structure magnétique des matériaux. La spectroscopie neutronique permet d'étudier d'une manière unique les excitations des corps, comme les phonons, les vibrations atomiques et les magnons. Les neutrons sont également utilisés pour radiographier des objets spéciaux (éléments pyrotechniques de moteurs fusée par exemple, ou encore barres de combustible irradié). On parle dans ce cas de neutronographie. Dans ces utilisations, le rayonnement neutronique est complémentaire des rayons X.

Les neutrons sont également utilisés pour leur aptitude à provoquer des réactions nucléaires (fissions, capture radiative ou diffusion inélastique). Une application en est le contrôle nucléaire de procédé, qui permet de mesurer quantitativement et qualitativement le contenu de mélanges de matière fissile (uranium, plutonium, actinides mineurs) dans le processus de traitement du combustible usé (usine de La Hague notamment).

Les sources de neutrons à haut flux sont soit des réacteurs nucléaires destinés à la production de ce rayonnement, soit des sources de spallation, grands accélérateurs de protons qui envoient un faisceau de protons accélérés sur une cible évaporant des neutrons. Typiquement, les sources de neutrons rassemblent un parc d'instrumentation formant de grands centres d'utilisateurs nationaux ou internationaux.

Australie :
Europe :



</doc>
<doc id="4882" url="https://fr.wikipedia.org/wiki?curid=4882" title="Élargissement de l'Union européenne">
Élargissement de l'Union européenne

L’élargissement de l’Union européenne décrit les vagues successives d’intégration économique et politique des États environnants dans l’Union européenne. L’Union européenne est aujourd’hui composée de , au terme de six élargissements (adhésion de trois nouveaux pays en 1973, un pays en 1981, deux en 1986, trois en 1995, douze en deux vagues en 2004 et 2007 et un en 2013) depuis sa création en 1958 par six pays. Cinq autres pays (l’Albanie, la Macédoine, le Monténégro, la Turquie et la Serbie) ont le statut de pays candidats à l’intégration. L’Islande a été candidate de 2009 à 2015 et a retiré cette candidature le 12 mars 2015. La Bosnie-Herzégovine a déposé début 2016 sa demande de candidature. Le Kosovo, pour sa part, ne s’est pas encore manifesté auprès de l’Union, mais est considéré comme un candidat potentiel.

Alors que l’UE représente aujourd’hui le troisième ensemble de population au monde après la Chine et l’Inde, les futures extensions suscitent des interrogations : l’ambiguïté des limites de l'Europe, la crainte qu’un trop grand élargissement provoque une dilution de l’Union en une simple zone de libre-échange avec des replis ethniques potentiels, et que les démarches d’adhésion occasionnent des conflits sur les points politiques non résolus : le statut du Kosovo (dont l’indépendance vis-à-vis de la Serbie n’est pas reconnue par la Serbie ni par tous les États membres de l’UE), le nom de la République de Macédoine (contesté par la Grèce), la non-reconnaissance de Chypre par la Turquie.

En 1961, le Premier ministre britannique indique sa volonté d’adhésion car les exportations du pays sont de plus en plus orientées vers l’Europe continentale et le gouvernement y voit une opportunité de prospérité. Les candidatures du Danemark et de l’Irlande sont liées à celle du Royaume-Uni car ils entretiennent des liens économiques étroits ce dernier. Les six États fondateurs décident qu’il faut , c’est-à-dire qu’ils doivent resserrer leurs liens pour être assez forts pour un élargissement. Cette volonté se traduit par la mise en place de la politique agricole commune (PAC) en 1962 mais, dans une conférence du , le général de Gaulle pose son veto contre l’élargissement car il voit le Royaume-Uni comme . Les partenaires de la France sont en désaccord avec cette position mais ils sont impuissants en raison de l’obligation d’unanimité.

Finalement, les candidats formulent une nouvelle demande en 1967. La position de De Gaulle n’a pas changé mais après 1969, le président Pompidou va se montrer moins fermé quant aux négociations qui aboutissent le 22 janvier 1972 par la signature des traités d’adhésion qui sont ratifiés sans difficulté à l'exception de la Norvège où les électeurs s’opposent à la ratification pour rejeter la politique commune de la pêche qui provoquerait une concurrence trop importante. En parallèle, cette étape est marquée par la période la plus meurtrière du conflit nord-irlandais, celui-ci ne sera réglé politiquement que deux décennies plus tard.

Finalement, en 1973, le Royaume-Uni, l'Irlande et le Danemark rejoignent ensemble la Communauté économique européenne.

En 1981, la Grèce change de régime politique et rejoint la Communauté économique européenne. À l’exil de la monarchie hellénique succède une période appelée la , les relations avec la CEE qui commençaient à se développer dès 1961 sont gelées, il faudra attendre le retour à la démocratie pour que la possibilité d’un rapprochement soit étudiée de nouveau.

En 1986, une longue période de dictature prend fin dans les deux pays prenant part à cet élargissement (le franquisme en Espagne et le salazarisme au Portugal), la transition vers la démocratie est marquée par des changements politiques, sociaux et économiques importants précédant leur accession.

Le 9 février 1962, le gouvernement espagnol avait déjà adressé à la Communauté économique européenne une lettre demandant l’ouverture de négociations en vue d’une association de l’Espagne à la Communauté, voire d’une éventuelle adhésion à long terme. Cependant, le régime dictatorial de Francisco Franco étant à la tête du pays, cette demande est rejetée en raison du refus des États membres d’accepter la candidature d’un régime non-démocratique. Les négociations durent plus de six ans assorties d’obligations : ratifier les pactes internationaux relatifs aux droits civils, économiques et culturels des Nations unies, rejoindre le Conseil de l'Europe ou encore réformer en profondeur leurs systèmes économiques.

En 1990, l’Allemagne, divisée depuis 1949 en deux États est réunifiée et la Communauté économique européenne intègre l’ancienne République démocratique allemande. Cette intégration est réalisée unilatéralement par les nouveaux Lander allemands, avec l’accord des instances européennes. L’application de la loi fondamentale allemande permet aux Länder de l’Est d’appliquer immédiatement les accords internationaux passés par la RFA et dont font partie les différents traités européens.

Par la suite, des États neutres et riches vont poser leurs candidatures. L’Autriche, la Suède et la Finlande sont, en effet, plus riches que les États membres et ne se revendiquent ni du camp occidental ni du camp soviétique. L’adhésion est concrétisée par le traité de Corfou du 24 juin 1994. Ces États pourront ne pas participer à la politique étrangère et de sécurité commune s’ils estiment qu’elle va à l’encontre de leur neutralité. En outre, une subvention pour les zones arides a été mise en place afin qu’il puissent bénéficier d’une aide économique européenne.

En 1995, l’Autriche, la Finlande ainsi que la Suède rejoignent l’Union européenne.

La décision de principe concernant l’extension de l’Union aux pays associés d’Europe centrale et orientale a été prise en 1993 par le Conseil européen de Copenhague, qui a également défini les critères auxquels les pays candidats devront satisfaire (critères de Copenhague) : 

L’élargissement a été confirmé par une nouvelle décision du Conseil européen en 2002, également à Copenhague et s’est déroulé en deux phases. Ces élargissement, entre 2004 et 2013, ont profondément modifié le visage et l’équilibre de l’Union européenne: sa superficie a augmenté de plus de 25%, sa population de plus de 20% tandis que le PIB moyen par habitant de l’Union européenne a diminué en même temps de plus de 10%.

Le , le Parlement européen a accepté l’adhésion de dix pays supplémentaires (Estonie, Lettonie, Lituanie, Pologne, République tchèque, Slovaquie, Hongrie, Slovénie, Chypre, Malte), qui adhèrent formellement à l’Union par le traité d’Athènes du .

Neuf de ces pays ont consulté leur population par référendum en 2003 :

Du côté des 15 États membres et de , l’extension fut ratifiée par voie parlementaire, aucun gouvernement ne voulant prendre le risque d’un référendum pour valider l’opportunité de cette extension à dix nouveaux États.

Le , juste avant l’adhésion de Chypre à l’Union, le référendum sur le plan de réunification de l’ile est accepté par les Chypriotes turcs mais repoussé par les Chypriotes grecs. L’ile entre cependant entière dans l’Union européenne mais, conformément au Protocole 10 du traité d’Athènes, l’acquis communautaire est suspendu dans les zones qui échappent au contrôle effectif du gouvernement de la République de Chypre. Les aides au développement de la partie Nord sont maintenues.

Ces dix États intègrent l’Union européenne le .

La Roumanie et la Bulgarie ont rejoint l’Union le janvier 2007. Cette intégration tardive des deux pays riverains de la mer Noire résulte de la nécessité de réformer en profondeur la vie politique et la société civile afin de les rapprocher des standards européens, notamment en matière d’économie de marché, de développement de l’État de droit et de lutte contre la corruption. En raison des défis à relever pour ces deux pays, des moyens et un suivi particuliers ont été mis en place par les instances européennes.

En 2003, la Croatie a déposé une demande d’adhésion à laquelle la Commission a rendu un avis favorable le , confirmé par le Conseil le . La Croatie devait alors ouvrir des négociations formelles le .

En février 2005, le Tribunal pénal international s’est plaint d’une coopération insuffisante pour livrer le général Ante Gotovina (retrouvé depuis dans l’archipel des îles Canaries (Espagne) et arrêté le 7 décembre 2005). Le 16 mars, les ministres des affaires étrangères de l’Union ont repoussé la date d’ouverture des négociations et celles-ci n’ont débuté "in fine" que le , en même temps que celles de la Turquie.

La Croatie devient le État de l’Union européenne le , après ratification du traité d’adhésion signé le , et le « oui » obtenu à 66,27 % lors du référendum national qui a été tenu le 22 janvier 2012.

Le référendum sur le maintien du Royaume-Uni dans l'Union européenne organisé le 23 juin 2016 fait suite aux débats sur le retrait du Royaume-uni installés dès l’intégration du royaume dans les Communautés européennes, il se solde par un résultat de 51,9 % des citoyens britanniques choisissant la sortie de l’UE. Ce référendum avait été promis par le premier ministre David Cameron, lui-même ayant fait campagne contre la sortie.

La législation de l’Union prévoit une période de deux ans afin de négocier les modalités du retrait après l’activation de l’Article 50 du traité sur l'Union européenne par le gouvernement du Royaume-Uni. Le gouvernement de Cameron est remplacé juste après le référendum par celui de Theresa May, les débats internes et les problématiques qui n’avaient que peu été anticipées conduisent à de multiples reports de l’activation de l’article 50.

Les pays listés dans la première partie du tableau ont déposé leurs candidatures, elles ont été reconnues et les négociations d’adhésion sont en cours, afin de répondre aux critères requis. Ceux dans la seconde partie n’ont pas encore officiellement déposé leur candidature, cependant ils ont fait connaître officieusement leur intérêt pour une telle candidature. Le Conseil européen du a pris note de la déclaration d’indépendance de l’Assemblée du Kosovo mais n’a pas pris de décision en ce qui concerne ses perspectives d’adhésion éventuelles à l’Union européenne.

Les pays des Balkans devraient adhérer à l’Union au fur et à mesure. Une nouvelle réforme des institutions est à l’étude pour rendre possible l’extension de l’Union à autant de membres. 

Le rapprochement initial de la Turquie vis-à-vis de l’Europe s’est effectué dans le contexte de la Guerre froide. Pour les Américains et les Européens, la Turquie ne devait pas tomber dans le camp soviétique, d’où son intégration à l’OTAN dès 1949.

La Turquie fut intégrée dans le programme d’association à la CEE en 1987, dans la perspective d’une adhésion ultérieure ; elle ouvre officiellement des négociations à Paris le .

Au vu des premières négociations, il apparaît que l’adhésion de la Turquie nécessiterait de sa part un respect des règles en vigueur en Union européenne et notamment les critères de Copenhague. Ce n’était toujours pas le cas à l’ouverture des négociations en 2005, sur les points suivants :


Selon l’article 237 du traité de Rome, puis l’article 49 du traité de Maastricht : . Mais les traités ne définissent pas ce qu’est un . Traditionnellement, les limites de l'Europe sont l’Oural et le Caucase (ou la mer Caspienne) à l’est, la mer Méditerranée au sud, l’océan Atlantique à l’ouest et l’Arctique au nord. Beaucoup critiquent cette définition, décidée par les géographes russes au pour permettre à la Russie d’apparaitre comme un État (elle n’était pas considérée comme tel jusque-là). Ils font remarquer que la création de l’Union européenne avait précisément pour objectif le dépassement des frontières qui ont été à l’origine des conflits nationalistes des siècles passés. Par ailleurs, ce critère pose aujourd’hui le problème de Chypre (de culture hellénique mais située au sud-est de la Turquie, candidate depuis bien plus longtemps) ou encore des régions ultrapériphériques. Cependant, dans les faits, l’Europe s’est d’abord construite sur le sentiment d’appartenir à une même civilisation dite et qu’il existe bien une notion d’espace commun, partagée par tous les membres de l’Union.

En décembre 2002, le Conseil européen réuni à Copenhague a entamé une politique de avec les pays frontaliers de l’Union qui n’ont actuellement aucune perspective d’adhésion. Il s’agit de la Russie, des nouveaux États indépendants d’Europe orientale (Ukraine, Moldavie et Biélorussie), avec qui un partenariat oriental a été inauguré en mai 2009, et des pays du sud méditerranéen (Algérie, Égypte, Israël, Jordanie, Liban, Libye, Maroc, Autorité palestinienne, Syrie et Tunisie), qui entrent dans le cadre du partenariat Euromed. L’objectif de cette politique est d’assurer entre ces États et l’Union des relations étroites et pacifiques fondées sur la coopération. Plus spécifiquement, il s’agit de réaliser à terme une vaste zone assurant la libre circulation des personnes, des biens, des services et des capitaux.

La Suisse, le Liechtenstein, la Norvège et l’Islande ne sont pas membres de l’Union européenne. Ces pays à haut niveau de vie et à longue tradition démocratique répondent globalement aux critères de Copenhague, même si des problèmes techniques subsistent quant aux mécaniques de la démocratie directe en Suisse (la question du secret bancaire a été réglée courant 2009 après l’acceptation par la Suisse des critères de l’OCDE) ou la réglementation de la pêche en Norvège (qui perçoit par ailleurs d’énormes revenus pétroliers). Cette situation résulte donc d’un choix volontaire de leurs habitants. Ainsi la Norvège a refusé par deux fois d’adhérer à l’Union européenne par référendum en 1972, puis en 1994. De même la Suisse a gelé une demande d’adhésion à l’Union européenne déposée en 1992, à la suite de l’échec du référendum la même année. En 2001, consécutivement à une initiative populaire, la Suisse rejette l’idée d’une réouverture rapide des négociations. Il faut attendre fin 2009 pour que la question revienne sur l’avant-scène fédérale sous l’impulsion du ministre Moritz Leuenberger. La position de l’Islande, longtemps réfractaire à toute adhésion à l’Union européenne, a changé avec l’effondrement de son économie consécutif à la crise financière de 2008. Le parlement a approuvé le 17 juillet 2009 une demande d’adhésion.

Les économies de ces trois pays sont cependant fortement intégrées à celle de l’Union européenne : l’Islande, le Liechtenstein et la Norvège sont membres de l’espace économique européen. Bien qu’elle ait refusé en 1992 d’adhérer à l’EEE, de nombreuses conventions bilatérales entre la Suisse et l’UE assurent à ce pays une intégration économique globalement équivalente. La Suisse et l’Union européenne sont tombées d’accord le pour l’intégration du pays alpin dans l’Espace Schengen. Depuis le la Suisse fait pleinement partie de l’Espace Schengen.

La Norvège et l’Islande avaient signé un accord d’association équivalent lors de l’adhésion des autres pays scandinaves : le Danemark, puis la Suède et la Finlande à l’UE. Cet accord avait pour but d’assurer la coexistence de l’Union nordique des passeports de 1954 et de l’espace Schengen. Depuis le changement de gouvernement en Islande en 2009, la perspective d’une adhésion rapide de ce pays à l’Union devient plus précise.

Ces pays sont également intéressés par certains projets de l’UE, ils sont par exemple engagés par des accords de coopération avec Europol.

L’objectif final de la politique de de l’Union est probablement de proposer aux pays concernés une coopération semblable à celle qui existe aujourd’hui avec ces quatre États.

L’Islande a présenté sa candidature d’adhésion pour intégrer l’Union européenne le . Sa demande fut officialisée par le Conseil européen le et transmise pour analyse à la Commission. La Commission annonça en février 2010 qu’elle soutenait l’ouverture des négociations d’adhésion de l’Islande.

Le gouvernement islandais a pour objectif d’adhérer rapidement, adhésion qui sera par ailleurs sujette à un référendum. Le , l’UE accorda le statut de candidat à l’Islande en approuvant formellement l’ouverture des négociations d’adhésion. Les négociations commencèrent le .

Le 12 septembre 2013, le ministre des Affaires étrangères islandais Gunnar Bragi Sveinsson annonce que le Gouvernement islandais suspend pour toute la durée de la législature les négociations d’adhésion à l’UE. Cette candidature est officiellement retirée le 12 mars 2015.

Membre de l’espace économique européen, l’Islande est déjà membre du marché unique et de l’espace Schengen.

L’Europe compte un certain nombre de très petits États qui font moins de et possèdent moins de : 

Ces États sont généralement dépendants d’un État voisin pour une ou plusieurs fonctions régaliennes (la monnaie, la défense). Hormis le Liechtenstein, historiquement plus proche de la Suisse, tous ces pays sont "de facto" fortement intégrés à l’Union à la suite des divers accords de coopération passés avec leurs voisins : par exemple, ils sont tous (sauf le Liechtenstein) associés à la zone euro. Ils ont pu en outre établir des conventions en leur nom propre. Par exemple, le Liechtenstein est membre de l’Espace économique européen et de l’espace Schengen et Saint-Marin a signé un accord de coopération et d’union douanière avec l’UE en 1991.

Au niveau européen, la taille et la population influent sur le poids politique des États, ce qui fait que pour ces micro-États, un mécanisme de pondération plus avantageux serait un préalable pour devenir un jour, membres à part entière de l’Union. Il faut cependant remarquer que la superficie de Malte, le plus petit État membre, est inférieure à celle d’Andorre. Mais sa population est cinq fois plus importante.

Par ailleurs, pour certains d’entre eux, la qualité démocratique de leurs institutions et la transparence de leur système bancaire sont sujets à caution : Andorre, le Liechtenstein et Monaco faisaient partie jusqu’au de la des paradis fiscaux non coopératifs publié par l’OCDE. Mais depuis, les trois pays sont sortis de cette liste noire. Depuis septembre 2009, Monaco figure dans la liste dite des pays qui mènent une politique fiscale conforme aux critères de l’OCDE.

L’Île de Man, Jersey et Guernesey, dépendances de la Couronne britannique et les Îles Féroé, pays constitutif du Royaume du Danemark, ne font également pas partie de l’Union européenne ; a contrario, les îles Åland, état libre associé à la Finlande et Gibraltar font partie de l’Union européenne. Les citoyens de ce territoire britannique d'outre-mer ont d’ailleurs pu voter lors des élections européennes de 2004 au sein de la circonscription d'Angleterre du Sud-Ouest.

La transition des anciens pays socialistes des Balkans vers les critères de Copenhague n’a pas été considérée comme suffisante pour qu’ils puissent rejoindre l’Union en 2004. Seule la Slovénie a pu entrer dans l’UE lors de la première phase du cinquième élargissement du . La Roumanie et la Bulgarie ont adhéré lors de la seconde phase, le et la Croatie a été intégrée au 

La Macédoine et l’Albanie ont reçu le statut officiel de pays-candidats et des accords de stabilisation et d’association (ASA) ont été signés avec toutes les anciennes républiques yougoslaves à l’exception du Kosovo qui n’est pas encore reconnu internationalement. Ce sont globalement les pays les plus pauvres de l’ancien bloc de l'Est et ils reçoivent des aides économiques et techniques de la part de l’Union via les différents instruments de préadhésion.

Les trois États baltes, ayant fait partie de l’URSS de 1940 à 1991, ont adhéré à l’UE en 2004, après s’être portés candidats dès leur indépendance recouvrée.


Ces trois derniers États font partie intégrante du partenariat oriental.
Les pays observateurs sont l’Arménie, la Moldavie et l’Ukraine.

La Russie constitue à elle seule une puissance, tout comme l’UE et les États-Unis. Des sommets Russie-UE assurent un dialogue entre les deux protagonistes en vue d’une future collaboration.

À propos de l’Arménie, l’Azerbaïdjan et de la Géorgie, membres du partenariat oriental, des commissions de coopération parlementaire ont souligné « la nécessité de définir clairement une perspective européenne pour ces pays en n’excluant, par conséquent, pas la possibilité qu’ils deviennent ultérieurement candidats à l’adhésion à l’Union » et invité à prendre pour objectif « la pleine intégration de la Géorgie dans l’Union européenne ».

Le président géorgien Mikheil Saakachvili a déclaré le que l’objectif d’une adhésion était la « priorité numéro un » de la politique étrangère de la Géorgie. C’est ainsi que même si la Géorgie n’est pas membre de l’UE, elle place systématiquement le drapeau du Conseil de l'Europe dont elle est membre et dont le drapeau a été repris par l’Union européenne, à côté de son drapeau national.

L’état économique et politique de ces pays ne permet cependant pas d’envisager une action concrète à court terme. Par ailleurs, l’appartenance de ces pays à l’espace européen est loin de faire l’unanimité.

Par ailleurs, ces trois pays sont aujourd’hui inclus dans la politique de voisinage de l'Union. Le la Commission avait recommandé leur inclusion dans ce programme.

L’Union européenne compte à ce jour un seul pays n’appartenant pas majoritairement au continent européen : Chypre.

Des pays extra-européens ont également été en pourparlers puisque la demande d’adhésion du Maroc a été rejetée en octobre 1987. Bien que Silvio Berlusconi se soit, par exemple, déclaré favorable à une candidature d’Israël, une adhésion de ces pays est extrêmement improbable.

Ils sont en revanche concernés par la politique de voisinage de l’Union. Le , l’UE a signé un accord de partenariat Euromed avec 12 pays du sud-est méditerranéen. L’objectif était de créer une zone de paix et de stabilité avec l’ambition de permettre le libre-échange en 2010. Jusqu’à aujourd’hui les progrès ont été extrêmement ténus et il apparaît évident que les objectifs initiaux ne seront pas atteints.

Ces pays ont vocation à être des ponts culturels au vu de leur histoire et de la proximité géographique avec l’Europe.

La Turquie est dans une situation différente. Bien que seulement 3 % de son territoire soit situé en Europe, elle a signé un accord d’association avec la CEE en 1963 (l’accord d'Ankara), et elle est officiellement candidate à l’entrée dans l’Union européenne depuis 2005. Alors que certains représentants politiques des pays au sein de l’Union européenne sont pour l’intégration (Silvio Berlusconi, José Luis Zapatero, Jean-Claude Juncker…), d’autres (comme Nicolas Sarkozy, ou Angela Merkel…) sont contre l’adhésion de la Turquie à l’UE, et lui préféreraient un « partenariat privilégié ».

Outre les pays européens ou extra-européens, certains partis politiques européens, en particulier l’ALE, préconisent de tenir en compte l’arrivée future de nouveaux membres potentiels à l’intérieur des frontières actuelles de l’Union européenne. L’ALE prend pour exemple des régions qui pourraient devenir indépendantes, en particulier la Flandre, la Wallonie, l’Écosse, le Pays de Galles, la Catalogne, le Pays basque et la Galice. Joan Puigcercos, président de l’ERC, annonce que l’Union européenne ne reconnaîtra de nouveaux États que .





</doc>
<doc id="4889" url="https://fr.wikipedia.org/wiki?curid=4889" title="Proxy">
Proxy

Un proxy est un composant logiciel informatique qui joue le rôle d'intermédiaire en se plaçant entre deux hôtes pour faciliter ou surveiller leurs échanges.

Dans le cadre plus précis des réseaux informatiques, un proxy est alors un programme servant d'intermédiaire pour accéder à un autre réseau, généralement internet. Par extension, on appelle aussi « proxy » un matériel comme un serveur mis en place pour assurer le fonctionnement de tels services.

Le concept de « proxy » (patron de conception) peut être utilisé dans n'importe quel programme informatique.

Si deux personnes qui ne parlent pas la même langue veulent communiquer, elles ont besoin d'un interprète. En informatique, deux entités utilisant des technologies différentes peuvent communiquer entre elles grâce à un intermédiaire qui interprète leurs échanges.

Dans l'environnement plus particulier des réseaux, un serveur proxy (ou « serveur mandataire », en français) est une fonction informatique client-serveur qui a pour fonction de relayer des requêtes entre une fonction cliente et une fonction serveur (couches 5 à 7 du modèle OSI).

Les serveurs proxys sont notamment utilisés pour assurer les fonctions suivantes :

Les fournisseurs d'accès à Internet (FAI) peuvent proposer des proxys pour la connexion de leurs abonnés. Il faut pour cela que l'abonné paramètre correctement son système (via un logiciel d'installation fourni par le FAI).

Mais il est également possible que le fournisseur d'accès utilise un proxy transparent (sans configuration par l'utilisateur). Ce proxy permet par exemple au fournisseur d'accès de connaître les habitudes de navigation de ses abonnés ou de réduire le nombre d'accès effectifs aux sites distants.

L'utilité des serveurs proxys est importante, notamment dans le cadre de la sécurisation des systèmes d'information.

Par exemple, il est presque systématique en entreprise ou dans les établissements scolaires que l'accès internet se fasse à travers un serveur proxy. L'internaute ne voit pas la différence, sauf quand il tente de naviguer sur un site interdit, auquel cas il pourra recevoir un message d'erreur : un tel proxy est appelé proxy filtrant. Il se peut aussi qu'une boite de dialogue s'ouvre et demande un identifiant et un mot de passe avant de pouvoir surfer sur internet.

À l'inverse, un proxy peut aussi servir à contourner les filtrages. Supposons le cas d'un pays qui bloque l'accès à certains sites considérés comme « subversifs », mais qui effectue ce filtrage uniquement en se basant sur l'adresse du site que l'on souhaite visiter. Dans ce cas, en utilisant un proxy comme intermédiaire (situé dans un autre pays donc non affecté par le filtrage), on peut s'affranchir du filtrage (sauf bien sûr si l'adresse du proxy est elle-même interdite).

Le principe fonctionne également dans l'autre sens. Supposons qu'un site web n'accepte que les internautes d'un certain pays (exemple concret : un site de campagne présidentielle américain qui n'accepte que les connexions venant des États-Unis). Dans ce cas, en passant par un proxy situé aux États-Unis, un internaute français pourra visiter le site.

Un troisième rôle du proxy est de compliquer la remontée vers l'internaute (anonymisation). Dans l'exemple précédent, on a trompé le site américain qui n'était pas capable de remonter jusqu'à l'internaute à travers le proxy. Certaines techniques avancées permettent de remonter à travers le proxy. Dans ce cas, un internaute pourra utiliser de nombreux proxys en chaîne comme le réseau The Onion Router (Tor) et stopper la connexion avant que ceux qui le traquent ne soient remontés jusqu'à lui.

Normalement, l'utilisation d'un proxy complet se fait en configurant son navigateur ou son ordinateur. Mais il existe une catégorie de proxy beaucoup plus simple d'utilisation : les proxys web. Il s'agit d'un simple site web dont la page offre un champ permettant de taper l'adresse du site que l'on souhaite visiter. Une fois saisie, la page demandée est affichée à l'intérieur de la première page. Mais l'adresse qui apparaît dans la barre d'adresse est toujours celle du proxy.

Ce type de proxy est moins puissant qu'un proxy normal. En effet, les pages utilisant des techniques avancées pour se mettre à jour elles-mêmes (AJAX) ne « savent » pas qu'elles passent par un proxy, et donc tentent d'atteindre leurs serveurs directement. Si ce serveur était interdit, alors la requête échoue. Par exemple, le proxy web Glype ne permet pas de consulter des sites comme Facebook ou YouTube. La plupart des proxys web ne permettent pas non plus d'utiliser des sites sécurisés utilisant HTTPS, comme les banques par exemple.

De nombreux proxys web sont disponibles gratuitement sur internet, principalement pour permettre à des internautes de contourner les protections de leurs lieux de surf (entreprise, école, comme expliqué précédemment). La plupart se financent en affichant de la publicité en plus de la page demandée. Mais certains peuvent aussi essayer de capturer les mots de passe ou toute autre information sensible (numéro de carte bancaire) ou même rediriger l'internaute vers un faux site. Il faut donc être extrêmement prudent avant d'utiliser un proxy gratuit sur internet.

Le principe des proxys web est utilisé par les bibliothèques universitaires pour permettre à leurs usagers d'accéder à des ressources en ligne pour lesquelles elles disposent d'un abonnement. L'adresse IP du serveur proxy est déclarée auprès du prestataire qui donne accès au contenu aux usagers passant à travers le proxy et se présentant donc avec l'adresse IP autorisée. Le système le plus utilisé pour cet usage est EZproxy.

Programmés en PHP :





Historiquement, la première référence au concept de proxy remonte à Marc Shapiro, à l' (ICDCS) de 1986.


</doc>
<doc id="4893" url="https://fr.wikipedia.org/wiki?curid=4893" title="Titane">
Titane

Le titane est l'élément chimique de numéro atomique 22, de symbole Ti.

C'est un métal de transition léger, résistant, d'un aspect blanc métallique, qui résiste à la corrosion. Le titane est principalement utilisé dans les alliages légers et résistants, et son oxyde est utilisé comme pigment blanc. On trouve cet élément dans de nombreux minerais mais ses principales sources sont le rutile et l'anatase.
Il appartient au groupe des titanes avec le zirconium (Zr), le hafnium (Hf) et le rutherfordium (Rf).

Les propriétés industriellement intéressantes du titane sont sa résistance à la corrosion, souvent associée à la résistance à l’érosion et au feu, la biocompatibilité, mais aussi ses propriétés mécaniques (résistance, ductilité, fatigue, etc.) qui permettent notamment de façonner des pièces fines et légères comme les articles de sport, mais aussi des prothèses orthopédiques.

Le titane a été découvert par le révérend William Gregor en 1791, minéralogiste et pasteur britannique. En analysant des sables de la rivière Helford dans la vallée de Menachan en Cornouailles, il isola ce qu'il nomma du "sable noir", connu aujourd’hui sous le nom d'ilménite. À la suite de plusieurs manipulations physico-chimiques (extraction du fer par des procédés magnétiques et traitement du résidu par de l’acide chlorhydrique), il produisit un oxyde impur d'un métal inconnu. Il nomma cet oxyde menachanite.
Indépendamment de cette découverte, en 1795, Martin Heinrich Klaproth, professeur de chimie analytique à l'université de Berlin, identifia le même métal. Alors qu'il analysait les propriétés du "schörlite rouge", aujourd’hui connu sous le nom de rutile, il conclut que le minerai contenait un métal inconnu identique à celui de Gregor. Il lui donna son nom actuel de « Titane », tiré de la mythologie grecque, en ignorant totalement ses propriétés physico-chimiques. C'est Berzelius qui l'isola en 1825.

Il a fallu attendre plus d'un siècle après la découverte de Gregor pour que l'Américain Matthew Albert Hunter, chercheur au Rensselaer Polytechnic Institute à Troy (New York), soit capable, en 1910, de produire du titane pur à 99 %. Les premières obtentions de titane par Hunter ne furent pas suivies du moindre développement industriel.

En 1939, le procédé industriel de production fut finalement mis au point par Wilhelm Justin Kroll, métallurgiste et chimiste luxembourgeois, consultant au Union Carbide Research Laboratory de Niagara Falls (New York) par réduction du TiCl avec du magnésium.

Caractéristiques physiques remarquables du titane :

Le titane pur est le siège d’une transformation allotropique de type martensitique au voisinage de .
En dessous de cette température, la structure est hexagonale pseudo-compacte ( , , 1,633) et est appelée Ti α (groupe d'espace P6/mmc). Au-dessus de cette température la structure est cubique centrée ( ) et est appelée Ti β. La température de transition est appelée transus β. La température exacte de transformation est largement influencée par les éléments substitutifs et interstitiels. Elle dépend donc fortement de la pureté du métal.

On trouve le titane sous la forme de 5 isotopes dans la nature : Ti, Ti, Ti, Ti, Ti. Le Ti représente l'isotope majoritaire avec une abondance naturelle de 73,8 %. 21 radioisotopes ont été observés, le plus stable le Ti possède une demi-vie de 63 ans.

Le titane peut se trouver sous plusieurs états d'oxydation comme de nombreux métaux de transition. Il possède donc plusieurs oxydes correspondant à ces degrés d'oxydation:

La couche d’oxyde très adhérente et dure explique la longévité de pièces en titane soumises aux chocs de particules en suspension dans les fluides. Cet effet est amplifié par la capacité qu'a cette couche de se régénérer. L'érosion dans l’eau de mer est augmentée par un débit plus élevé ou une granulométrie plus faible.

Le titane est considéré comme un métal ayant une résistance mécanique importante et une bonne ductilité dans les conditions normales de température. Sa résistance spécifique (rapport résistance à la traction / densité) est, par exemple, plus élevée que celle de l’aluminium ou l’acier.
Sa résistance est décroissante à la température avec un replat entre et . En dessous de , dans les domaines de températures cryogéniques, sa résistance augmente et sa ductilité diminue grandement.
Jusqu’à ce jour aucune solution satisfaisante n’a encore été mise au point. On a essayé principalement l’oxydation, la nitruration, la boruration et la carburation. On se heurte à de nombreuses difficultés technologiques de réalisation et d’adhérence. Ajoutons que les traitements de surface du titane, modifiant la nature ou la structure de la surface, ne sont à employer qu’avec la plus grande prudence et après une étude approfondie de leur influence ; ils ont généralement un effet néfaste plus ou moins prononcé sur la résistance et la fatigue.

Le titane est l’un des métaux les plus bio-compatibles, avec l’or et le platine, c’est-à-dire qu’il résiste totalement aux fluides corporels.

De plus, il possède une haute résistance mécanique et un module d’élasticité très bas ( à ), plus proche de celui des structures osseuses () que l'acier inox (). Cette élasticité qui favorise le remodelage osseux en obligeant l'os à travailler (prévention du stress shielding ou ostéoporose péri-implantaire) fait du titane un bio-matériau particulièrement intéressant. Il faut cependant noter qu'une élasticité excessive peut aussi compromettre la fonction du bio-matériau qui aurait subi une déformation inacceptable.

Le Centre international de recherche sur le cancer (CIRC) a classé le dioxyde de titane dans le groupe 2B « susceptible d’être cancérigène pour l’humain » : les études menées ne permettent pas de conclure.

Sa résistance au feu, notamment d’hydrocarbures, est très bonne. Il a été démontré qu’un tube de d’épaisseur pouvait sans dommage ni risque de déformation ou d’explosion supporter une pression de dix atmosphères tout en étant soumis à un feu d’hydrocarbures à une température de . Cela est dû en premier lieu à la résistance de la couche d’oxyde qui évite la pénétration de l’hydrogène dans le matériau. En outre, la faible conductivité thermique du titane protège plus longtemps les éléments internes d’une élévation de température.

Le titane est un métal extrêmement oxydable. Dans la série des potentiels électrochimiques standards, il se place au voisinage de l’aluminium, entre le magnésium et le zinc. Il n’est donc pas un métal noble, son domaine de stabilité thermodynamique ne présente, en effet, aucune partie commune avec le domaine de stabilité thermodynamique de l’eau et est situé fortement au-dessous de ce dernier.
L’une des causes de la résistance à la corrosion du titane est le développement d’une couche protectrice passivante de quelques fractions de micromètre, constituée majoritairement d’oxyde Ti, mais il est reconnu qu’elle peut contenir d’autres variétés.
Cette couche est intègre et très adhérente. En cas de rayure de la surface, l’oxyde se reforme spontanément en présence d’air ou d’eau. Il y a donc inaltérabilité du titane dans l’air, l’eau et l’eau de mer. De plus, cette couche est stable sur une large gamme de pH, de potentiel et de température.

Des conditions très réductrices, ou des environnements très oxydants, ou encore la présence d’ions fluor (agent complexant), diminuent le caractère protecteur de cette couche d’oxyde ; les réactifs d’attaque pour relever les micrographies sont le plus souvent à base d’acide fluorhydrique. Lors d’une réaction par cet acide, il y a formation de cation titane (II) et (III).
La réactivité des solutions acides peut néanmoins être réduite par l’adjonction d’agents oxydants et/ou d’ions lourds métalliques. L’acide chromique ou nitrique et les sels de fer, nickel, cuivre ou chrome sont alors d’excellents agents inhibiteurs. Cela explique pourquoi le titane peut être utilisé dans des procédés industriels et des environnements où les matériaux conventionnels se corroderaient.

On peut bien entendu modifier les équilibres électrochimiques par adjonction d’éléments d’addition qui réduisent l’activité anodique du titane ; cela conduit à améliorer la tenue à la corrosion. Selon les desiderata de modifications, on ajoute des éléments spécifiques. Une liste non exhaustive de quelques adjuvants classiques est reprise ci-dessous :

Ces trois méthodes peuvent être combinées.

Le titane est très peu sensible aux modes particuliers de corrosion tels que la corrosion caverneuse ou la corrosion par piqûre. Ces phénomènes ne sont observés qu’en cas d’utilisation dans un domaine proche d’une limite pratique de tenue à la corrosion générale.
Les risques de corrosion sous contrainte apparaissent dans les conditions suivantes :

Les deux structures allotropiques se distinguent au niveau de la résistance à ce dernier type de corrosion ; le titane α y est fort sensible alors que le β quasiment pas.

Ce procédé sert à isoler le titane ou le zirconium par formation réversible d'iodure volatil et dépôt du métal par pyrolyse sur un filament de tungstène.

Ce procédé permet de réduire l'oxyde de titane en titane par du magnésium.
La première étape consiste à opérer une carbochloration sur le dioxyde de titane. Le produit est obtenu par action du chlore gazeux sur l’oxyde vers , le tout sur lit fluidisé selon la réaction :

Le tétrachlorure de titane, dont la température d’ébullition est de , est récupéré par condensation, décanté, filtré et purifié par distillation fractionnée. Le procédé de réduction qui s’ensuit consiste alors à faire réagir ce tétrachlorure en phase gazeuse sur du magnésium liquide selon la réaction :

TiCl + 2 Mg → 2MgCl + Ti

La réaction est réalisée sous vide ou sous gaz inerte (argon). Le chlorure de magnésium est séparé par décantation, puis, dans une seconde étape, par distillation sous vide vers , ou par lavage à l’acide. Le titane obtenu est un solide poreux faisant penser à une éponge, d’où son nom d’éponge de titane.

Depuis le début de sa mise en exploitation industrielle en 1945, le procédé Kroll n’a pas subi d’évolution notable dans son principe physico-chimique mais son rendement a été amélioré.

Une fois l’éponge obtenue, on la broie afin d’obtenir des copeaux de titane. Ce lot est ensuite homogénéisé dans un mélangeur soit sous gaz neutre soit sous aspiration violente, de manière à prévenir toute inflammation des particules fines de titane (particules d’une centaine de micromètres) pouvant conduire à la formation d’oxynitrure de titane fragilisant et insoluble dans le bain liquide. Le lot homogène est ensuite introduit dans la matrice d’une presse où il est comprimé à froid, sous forme de cylindre dense appelé compact. La densité relative du compact autorise alors toute manutention en vue de constituer une électrode par empilement de ces compacts, étage par étage, et soudage entre eux par plasma ou faisceau d’électrons. On fabrique ainsi une électrode primaire.

Le lingot de titane pur à 99,9 % peut finalement être obtenu par différentes techniques de fusion : 

Pour fabriquer un lingot de titane pur, la matière fondue peut être soit exclusivement de l’éponge, soit un mélange d’éponge et de déchet de titane (scrap), soit exclusivement du déchet de titane. Les lingots d’alliage de titane sont obtenus en mélangeant à la matière titane les éléments d’addition, comme le vanadium et l’aluminium, pour obtenir après fusion l’alliage souhaité. L’alliage le plus couramment utilisé est le . En fonction des techniques de fusion utilisées et selon les besoins en termes d’homogénéité des produits obtenus, le cycle de production peut comprendre deux, voire trois fusions successives du même lingot.

Les lingots sont en général transformés par forgeage à chaud et usinage pour obtenir des s sous forme de brames, bloom ou billette. Puis on obtient des produits finis (feuilles, bobines, barres, plaques, câbles, etc.) par différentes étapes de transformation de laminage, forgeage, extrusion, usinage, etc. Les pièces de fonderie sont en général réalisées directement à partir du lingot de fusion auquel est rajoutée une proportion variable de scrap.

Bien que le titane métallique soit assez rare à cause de son prix, le dioxyde de titane est bon marché et largement répandu comme pigment blanc pour les peintures et les plastiques. La poudre de TiO est chimiquement inerte, résiste à la lumière du Soleil et est très opaque. Le dioxyde de titane pur possède un indice de réfraction très haut (2,70 à λ = ) et une dispersion optique plus élevée que celle du diamant.

Lorsqu'il est sous forme métallique divisée le titane est très inflammable, mais on considère généralement que les sels de titane sont sans danger. Les composés chlorés comme le TiCl et le TiCl sont corrosifs. Le titane peut s'accumuler dans les tissus vivants qui contiennent du silicium, mais il ne possède aucun rôle biologique connu.

On trouve du titane dans les météorites, dans le Soleil et dans les étoiles, ses raies sont bien marquées pour les étoiles de type M. Les roches rapportées de la Lune par la mission apollo 17 sont composées à 12,1 % de TiO. On en trouve également dans le charbon, les plantes et même dans le corps humain.

Sur Terre, le titane n’est pas une substance rare. Il est le neuvième élément le plus abondant dans la croûte terrestre, et le cinquième métal le plus abondant, sa teneur moyenne y est de 0,63 %, Seuls les éléments suivants y ont plus d'atomes, par ordre décroissant : l’oxygène, le silicium, l’aluminium, le fer, l'hydrogène, le calcium, le sodium, le magnésium et le potassium.

La plupart des minéraux, roches et sols contiennent de petites quantités de titane. On dénombre 87 minéraux ou roches contenant au moins 1 % de titane. Les minerais riches en titane sont par contre très peu nombreux, à savoir, l’anatase (TiO), la brookite (TiO), l’ilménite (FeTiO) et ses altérations par carence de fer : le leucoxène, la perovskite (CaTiO), le rutile (TiO), la sphène ou titanite (CaTiO(SiO)) et la titanomagnétite (Fe(Ti)FeO).

La majorité du titane sur Terre se trouve sous forme d’anatase ou de titanomagnétite, mais ces derniers ne peuvent être exploités avec les technologies actuelles de manière rentable. Seuls l’ilménite, le leucoxène et le rutile sont intéressants économiquement, étant donné la facilité avec laquelle ils peuvent être traités.

On trouve des gisements de titane à Madagascar et en Australie, Scandinavie, Amérique du Nord, Malaisie, Russie, Chine, Afrique du Sud et Inde.

La réserve mondiale totale, à savoir celle qui n’est pas encore technologiquement et économiquement exploitable, est estimée à 2 milliards de tonnes.
Les réserves prouvées de rutile et d’ilménite, calculées en pourcentage de TiO utilisable et technologiquement extractible en 2005, sont estimées à 600 millions de tonnes.

Principaux producteurs d'oxyde de titane en 2003, Chiffres de 2003, en milliers de tonnes de dioxyde de titane : 
Le nombre de producteurs de titane à haute pureté est très limité et est concentré dans les régions à forte demande intérieure. En effet, le titane étant un matériau stratégique pour les secteurs aéronautique, énergétique et militaire, les gouvernements des pays industrialisés ont organisé leur propre industrie de production. L’émergence récente de production en Chine et en Inde dans le cadre des plans pluriannuels de développement de l’industrie de défense, confirme cette analyse. Le fait que cette industrie soit destinée en premier lieu à satisfaire des besoins intérieurs stratégiques explique en partie le flou de l’information sur les capacités réelles de production.

Le développement de l’industrie dans le monde libéral a permis aux producteurs occidentaux d’accroître leur offre jusqu’à l’arrivée des producteurs des pays de l’ex-URSS. On peut considérer que le niveau des prix du marché, avant 1990, était principalement basé sur les coûts de production des pays occidentaux (États-Unis, Europe de l’Ouest, Japon) et sur le positionnement par spécialisation de produit de ces fournisseurs aboutissant à un certain lobbying.
L’arrivée sur le marché des producteurs russes, ukrainiens et, à plus long terme, chinois marque de nouvelles étapes dans l’évolution du marché du titane.

Ainsi, une pression sur les prix s’exerce pour gagner des parts sur le marché actuellement dominé par les États-Unis et le Japon. Cette pression se caractérise par une baisse des prix que les coûts de production rendent possible.
Et, par le jeu de la concurrence, la diversification de l’offre peut contribuer à briser le positionnement par spécialisation de produit.

La plus grande utilisation du titane (95 %) est faite sous sa forme de dioxyde de titane TiO (anatase), qui est un pigment important utilisé à la fois dans les peintures domestiques et les pigments des artistes, les matières plastiques, le papier, les médicaments… Il a un bon pouvoir couvrant et est assez résistant au temps. Les peintures à base de titane sont de très bons réflecteurs des infrarouges, et sont donc très utilisées par les astronomes.

Autrefois réputé cher à cause de sa valeur d’achat, le titane est de plus en plus considéré comme économique dans les coûts d’exploitation. La clé du succès pour sa rentabilité réside au maximum dans l'utilisation de ses propriétés et caractéristiques uniques dès la conception, plutôt que de les substituer ex abrupto à un autre métal. Les coûts d’installation et d’exploitation des tubes de forage en titane dans des exploitations pétrolières offshore sont jusqu'à deux fois plus bas qu’avec la référence acier.
En effet, d’une part, la résistance à la corrosion évite les opérations de revêtement des tubes et permet des durées de vie trois à cinq fois supérieures à l’acier, et, d’autre part, la valeur élevée de sa résistance spécifique permet de réaliser des tubes fins et ultra légers.
Cet exemple photographique montre à souhait que le titane, initialement employé dans le domaine aéronautique, touche de plus en plus de segments d'utilisation.

Les domaines de l’aéronautique et de l’aérospatiale constituent la première des applications historiques du titane. Dans ce secteur on utilise totalement ses caractéristiques spécifiques.

De nos jours, le titane constitue 6 à 9 % de la masse des avions. On en trouve tout d’abord sous forme de pièces forgées mais aussi sous forme de boulons. Il ne faut pas oublier les éléments de moteurs, à savoir les étages basses et hautes pression à moyennes températures : disques de compresseurs, aubes de compresseurs, carters structuraux, carter Fan, aubes Fan, etc. ; la température maximale d’utilisation étant limitée à .

Le titane peut se former à chaud (température < ). Ses caractéristiques de superplasticité (température de formage ) permettent d'obtenir des formes très complexes. Il est également utilisé comme élément de structure en présence de composites carbone.

Dans le domaine spatial, ce matériau est utilisé pour les éléments du moteur Vulcain d’Ariane 5 en contact avec le mélange H / O et sa combustion ; les rouets centrifuges sont ainsi soumis à des températures cryogéniques d’un côté (température H liquide) et à celles de la combustion de l’autre. Il sert aussi de réservoir aux gaz de propulsion pour les satellites grâce à ses bonnes propriétés cryogéniques et à sa résistance à la corrosion des gaz propulseurs. Enfin, comme c’est un métal faiblement soumis au magnétisme, il est embarqué sur les stations spatiales sous forme d'outil. Ceux-là mêmes qui, en apesanteur, évoluent près des appareillages électriques, électroniques, sans risque d'être générateur d'arcs et de perturbations électromagnétiques.

Le secteur de la chimie, au sens large du terme, constitue le second secteur d'activité où le titane est présent.

Ainsi, on retrouve des tubes en titane dans de nombreux condenseurs, où sa résistance à la corrosion et à l’abrasion permet des durées de vie élevées.

Il sert également, sous forme de réacteurs dans les raffineries (résistance à HS et CO) et pour le blanchiment de la pâte à papier (résistance au Cl).

Au Japon, il est également utilisé dans le traitement des eaux en raison de sa bonne résistance à la corrosion, ainsi qu'aux agents biologiques.

On l’emploie comme blindage (porte-avions américains) où ses propriétés mécaniques et sa résistance à la corrosion et au feu sont mises en avant. Aux États-Unis, on a même été jusqu’à concevoir des véhicules légers, dont la carrosserie en titane possède une résistance spécifique inégalable et facilite le transport par hélicoptère.

Mais la plus spectaculaire des utilisations est bien sûr la réalisation de plusieurs sous-marins nucléaires par les russes comme la classe Alfa dont la coque entière est en titane. L'avantage du titane dans ce cas est double :

Ainsi, le titane est considéré comme l'une des huit matières premières stratégiques indispensables en temps de guerre comme en temps de paix.

Le défaut majeur de ces coques est leur prix, dû au titane ainsi qu'à la difficulté de le souder.

On dispose actuellement d’un retour d’expérience d’une petite cinquantaine d’années d’utilisation dans le domaine médical (premiers implants dentaires en titane posés en 1964 par le Per-Ingvar Brånemark). Son emploi s’est développé en raison de son caractère biocompatible. En effet, l'os adhère spontanément au titane ce qui en fait un matériau privilégié pour la réalisation de prothèses. En plus de cet aspect biocompatible, le titane est mécanocompatible. Son intérêt réel pour la chirurgie et l'ostéosynthèse reste toutefois à démontrer.

Le titane a aussi fait une percée importante dans le domaine de l’odontologie où il sert d’implant dans l'os pour les supports de prothèses ainsi que pour la confection d'infrastructures prothétiques appelées « chapes » ou « armatures » dans le jargon du prothésiste dentaire et du chirurgien-dentiste. Le NiTi est aussi utilisé en endodontie sous forme de petites limes super-élastiques servant à instrumenter les canaux dentaires pour les dévitalisations et en orthodontie où ses propriétés de mémoire de forme et d'élasticité en font un matériau de choix pour la fabrication des arcs qui permettent de corriger la position des dents.

Il faut signaler l’apparition d’outillage en titane pour la chirurgie, comme les forets creux refroidis à l’eau. À l’inverse de l’acier, tout débris d’outil en titane pouvant rester dans le corps n’occasionnera pas d’infection postopératoire, du fait de sa biocompatibilité.

Enfin, le titane rentre dans la composition des bobines supraconductrices des appareils IRM en association avec un autre métal de transition : le niobium.

Le titane est également utilisé, notamment aux États-Unis, dans les circuits secondaires de réacteurs nucléaires afin de minimiser le nombre d’arrêts de tranches qui sont extrêmement coûteux.
Il faut aussi noter son utilisation dans la géothermie sous forme de canalisations et de carters et dans les échangeurs de chaleur (tubes droits ou en U), toujours pour sa tenue à la corrosion et sa résistance à l’érosion. 
Enfin grâce à sa résistance spécifique élevée, on en utilise dans les turbines génératrices de vapeur sous forme d’aubes ; dans ce cas, on réduit fortement les arrêts de centrale dus aux ruptures d’aubes.

Un nouveau secteur d’application semble bien être la construction automobile. Ce sont surtout les marques allemandes, japonaises et américaines qui introduisent des pièces de titane dans les voitures de tourisme. Ce qui est recherché est l’allégement des structures visant à réduire à la fois les émanations du moteur et le bruit ; on trouve ainsi des soupapes, des ressorts et des bielles en titane.

Le cas des ressorts est typique d’une bonne utilisation des propriétés du titane : comme son module de Young est deux fois plus faible que celui de l’acier, il faut deux fois moins de spires ; comme il est deux fois moins dense que l’acier, le ressort est quatre fois plus léger, et il faut deux fois moins de place pour le loger dans la suspension. Si on ajoute à cela qu’il a une durée de vie quasi illimitée, même sur les routes à haut degré de salinité, on comprend l’intérêt de l’industrie automobile.

Le titane est utilisé pour la réalisation du milieu amplificateur de lasers accordables saphir dopé Titane ou laser titane-saphir. L'intérêt du titane pour un laser accordable est que c'est un métal de transition avec une couche 3d électrostatiquement peu écrantée, ce qui conduit a une courbe de gain très large des amplificateurs optiques saphir dopé titane, et donc une grande accordabilité en fréquence des lasers réalisés avec.

Dans un tout autre domaine de l'optique, le titane est utilisé pour réaliser des montures de lunettes haut-de-gamme, pour lesquelles il offre une combinaison idéale de résistance, flexibilité et légèreté extrême, en plus de son côté bio-inerte. La société Nikon a été la première à commercialiser une monture en titane en 1981.


Le titane est le dans la progression de la sarbacane sportive.

Les noces de titane correspondent au anniversaire de mariage.

En 2014, la France est nette importatrice de titane, d'après les douanes françaises. Le prix moyen à la tonne à l'import était de .







</doc>
<doc id="4894" url="https://fr.wikipedia.org/wiki?curid=4894" title="Dioptre">
Dioptre

En optique, un dioptre est une surface séparant deux milieux transparents homogènes et isotropes, d'indices de réfraction différents. 

On parle de "dioptre plan" si la surface de séparation est un plan, de "dioptre sphérique" si c'est une sphère (ou tout au moins une calotte sphérique).

Si la lumière se propage en ligne droite dans un milieu homogène et isotrope, elle est déviée lors du passage d'un dioptre : il y a réfraction.

De façon générale, il y a à la fois réfraction et réflexion : une partie de la lumière est réfléchie à la surface du dioptre (environ 3%) et l'autre partie est réfractée lors de son passage dans l'autre milieu.

Le changement de direction au niveau du dioptre est décrit par les lois de Snell-Descartes qui fondent l'optique géométrique. Ces lois peuvent se représenter graphiquement en les appliquant à un rayon unique - dit incident - interceptant le dioptre en un point dit point d'incidence. Pour comprendre l'effet d'un dioptre sur la lumière, il faut considérer un nombre minimal de rayons de façon à représenter le faisceau de lumière.

Une des conséquences des lois de Snell-Descartes est que le dioptre plan est un système non-stigmatique. L'illustration ci-dessus montre que la lumière issue d'un point placé dans un aquarium, par exemple, donne des rayons réfractés dans l'air qui ont des directions sans point commun.

Pourtant, lorsqu'on regarde un poisson, on le voit bien ! C'est donc que l'œil du poisson, par exemple, constitue un objet lumineux qui forme une image sur la rétine de l'œil de l'observateur. Ceci n'est possible que parce que "le faisceau de lumière est suffisamment étroit" pour que la tache sur la rétine apparaisse comme un point. On est bien alors dans un cas de stigmatisme approché.

C'est ce phénomène qui permet d'expliquer l'expérience du « bâton brisé » que l'on montre en général pour illustrer la réfraction.
On voit que si "n" > "n" (par exemple le passage des rayons de l'eau vers l'air, "n" représentant l'indice de réfraction de l'eau et "n"celui de l'air), alors pour des valeurs de sin(θ) proches de 1, c'est-à-dire pour des incidences rasantes (rayon incident proche de la surface), on obtient par cette formule une valeur de sin(θ) supérieure à 1. Ceci est évidemment impossible, cela correspond à des situations où il n'y a pas de réfraction mais uniquement de la réflexion : on parle de réflexion totale interne, laquelle se produit lorsque l'angle d'incidence dépasse l'angle critique.

L'angle critique de réfraction est donc tel que : 

Cette propriété est mise à profit dans certains systèmes réflecteurs (prisme à réflexion totale) et les fibres optiques.
L'application des lois de Snell-Descartes permet également de traiter le cas des dioptres non plans. Il suffit de considérer localement la normale au dioptre point d'incidence de chaque rayon contribuant au faisceau. 

De nouveau, par construction géométrique, on constate que le dioptre sphérique n'est pas stigmatique, sauf évidemment pour son centre, puisque chaque rayon arrivant perpendiculairement au dioptre n'est pas dévié. L'image du centre est alors le centre lui-même. (En fait, il est également stigmatique pour deux autres points particuliers de l'axe optique, appelés points de Weierstrass).
Lorsqu'une faible partie du dioptre est utilisée ou, autre façon de dire, lorsque le rayon de courbure est très grand devant les dimensions liées à l'objet (taille, distance), on peut se placer dans les conditions dites de Gauss : on ne considère alors que les rayons qui passent près de l'axe et qui sont peu inclinés. La conséquence mathématique est la possibilité d'assimiler les sinus à la valeur des angles (en radian) et la conséquence physique est que l'on est alors dans les conditions d'un stigmatisme approché: dès lors, à un point objet, on peut associer un point image. Ceci est particulièrement important pour la fabrication des lentilles (voir ci-après).

En particulier, on peut définir un foyer, image d'un objet à l'infini, c'est-à-dire autrement, point de convergence (ou de divergence) d'un faisceau incident parallèle à lui-même et parallèle à l'axe. Et plus généralement, on peut écrire une relation de conjugaison entre un point A de l'axe et son image A' donnée par le dioptre.

Les dioptres sphériques sont alors représentés de façon conventionnelle :

La relation de conjugaison avec origine au sommet, écrite ci-dessous, permet de préciser les positions des foyers. Suivant la courbure (concave/convexe) et suivant l'ordre des indices (n'> n ou n' < n) les foyers sont réels ou virtuels.

Pour un point A sur l'axe (orienté), la position du point image A' est donnée par :
formula_2 

Par ailleurs, le grandissement transverse a pour expressions: formula_3

Les applications sont, de façon générale, les instruments d'optique. Ceux-ci sont constitués d'objets réfractants qui ont nécessairement au moins deux faces. Schématiquement, on peut considérer :



</doc>
<doc id="4896" url="https://fr.wikipedia.org/wiki?curid=4896" title="Biogéographie">
Biogéographie

La biogéographie est une branche à la croisée des sciences dites naturelles, de la géographie physique, Pédologie géoscience, pédologie, l'écologie, de la [[biologie qui étudie la vie à la surface du Terre|globe par des analyses descriptives et explicatives de la répartition des êtres vivants, et plus particulièrement des communautés d'êtres vivants.

En effet, les êtres vivants s'organisent pour donner des paysages différents que l'on appelle « formations »dénommées le plus souvent selon leurs profils ou les formation végétale|et les composant. Ce sont les formations végétales qui marquent le paysage, le vivant végétal appartenant à la [[biosphère, et qui a des interactions avec l'Atmosphère terrestre, l'hydrosphère et la lithosphère (le substrat), le tout évoluant dans le temps.

DOMAINE DE LA BIOGEOGRAPHIE

Évolution de la biogéographie 
Le développement d'une discipline scientifique passe généralement par trois ou quatre phases, que l'on retrouve dans la biogéographie.

La première phase est descriptive. Dès la fin du , l'un des premiers essais sur la géographie du vivant fut proposé par Buffon, mais c'est au que naquit vraiment la biogéographie comme discipline scientifique. Ces pères de la biogéographie sont les explorateurs des et s, parmi lesquels [[Augustin Pyrame de Candolle]] (1778-1841), [[Alexander von Humboldt]] (1769-1859), [[Aimé Bonpland]] (1773-1858), [[Alfred Russel Wallace]] (1823-1913), [[Charles Darwin]] (1809-1882), [[Thomas Henry Huxley]] (1825-1895), [[Philip Lutley Sclater]] (1829-1913), [[Adolf Engler]] (1844-1930). En France la biogéographie connaît un destin assez lié à celle de la phytosociologie, aussi retrouve-t-on des grands noms communs aux deux disciplines comme [[Henri Gaussen]] (1891-1981) et [[Paul Rey]] (1918-2016)…

La phase suivante cherche à comprendre l'histoire des [[faune (biologie)|faune]]s (celle des [[flore]]s demeurant alors en suspens), donc leur évolution. Cette recherche a été amorcée de manière essentiellement narrative cependant par Darwin, Wallace et Huxley, mais c'est [[Ernst Mayr]] qui ajoute réellement cette dimension temporelle en 1965 : son objectif étant d'analyser l'origine, la différenciation, le développement et la mise en place des faunes, en relation avec l'histoire spatio-temporelle des milieux.

L'étape suivante à laquelle sont associés les noms de [[George Evelyn Hutchinson]] (1903-1991), [[Robert MacArthur]] (1930-1972) et [[Edward Osborne Wilson]] (1929-) est l'approche hypothético-déductive prévoyant les distributions des organismes et les processus impliqués à partir d'hypothèses, puis à vérifier sur le terrain les prédictions de ces hypothèses. Cette biogéographie prédictive s'efforce d'expliquer des mécanismes fondamentaux tels que l'immigration, la colonisation, l'extinction, la structuration et le renouvellement des peuplements. Un exemple de cette démarche est la [[Théories de distribution des espèces#Théorie de la biogéographie insulaire|théorie de l'équilibre dynamique]] des peuplements insulaires de McArthur et Wilson (1963 et 1967).

La quatrième phase est la biogéographie expérimentale qui consiste à tester des hypothèses sur certains des mécanismes étudiés par la biogéographie prédictive : créer artificiellement des milieux nouveaux, fragmenter des espaces, ériger ou supprimer expérimentalement des barrières à la colonisation, manipuler des nombres d'espèces sur des espaces restreints, faire des substitutions d'espèces, etc.

Ces dernières années, les études [[génétique]]s effectuées sur des marqueurs neutres des [[génome]]s à [[hérédité]] monoparentale ont permis de retracer des routes de migrations des grandes familles d'arbres au [[Quaternaire]]. Ces approches de [[phylogénétique]] couplées avec des approches de paléontologie ([[fossile]]s, données [[palynologie|palynologique]]s et [[anthracologie|anthracologiques]]) sont d'une puissance inégalée jusqu'à présent. Les progrès sont constants dans la description des trajets des espèces, le rôle des événements historiques ont un poids que l'on ne cesse de revoir à la hausse pour expliquer la physionomie des paysages actuels.

Le travail de cartographie des régions biogéographiques n'est pas achevé et continue à évoluer, notamment concernant les aspects sous-marins et parce que les modifications climatiques peuvent modifier certains facteurs écologiques.



[[Catégorie:Biogéographie| ]]

</doc>
<doc id="4897" url="https://fr.wikipedia.org/wiki?curid=4897" title="Cuauhtémoc Cárdenas">
Cuauhtémoc Cárdenas

Cuauhtémoc Cárdenas Solórzano né le à Mexico, Mexique. Il est un homme politique mexicain. Il est le fils du président Lázaro Cárdenas del Río et le père du gouverneur du Michoacán Lázaro Cárdenas Batel.

Cuauhtémoc Cárdenas fut sénateur de l'État du Michoacán de 1974 à 1980, puis gouverneur de ce même état de 1980 à 1986. Il gagna ces deux postes à la suite d'élections sous la bannière du PRI, dont il fut membre jusqu'en 1987.

Ayant contribué à la formation d'un courant dissident (la "Corriente Democrática") au sein du PRI en vue de démocratiser le mode de désignation du candidat à la présidentielle (qui était encore ces années-là désigné, plus ou moins officiellement, par le Président mexicain en poste), il finit par se porter candidat en 1988 à la tête d'une coalition réunissant la gauche institutionnelle et les secteurs dissidents du PRI : le "Frente Democrático Nacional". Contre toute attente sa candidature rassembla plus de 30 % des votes face au candidat priiste Carlos Salinas de Gortari. De très forts soupçons de fraude pèsent néanmoins jusqu'à aujourd'hui sur les résultats officiels : en effet, le président de la Commission Fédérale Électorale de l'époque, Manuel Bartlett Díaz, avait annoncé le report de l'annonce des résultats en raison d'une panne du système de comptage. La victoire fut finalement attribué au candidat officiel Carlos Salinas de Gortari, mais Cuauhtémoc Cárdenas ainsi que tous les candidats de l'opposition dénoncèrent les résultats et appelèrent à annuler le scrutin. Malgré les protestations, Carlos Salinas de Gortari prit ses fonctions de Président du Mexique le .

C'est dans le contexte de cette défaite et des manifestations populaires qui l'ont accompagné qu'il fonda le Parti de la révolution démocratique, qui allait devenir l'un des trois principaux partis politiques du Mexique et dont il est membre jusqu'à aujourd'hui. Cuauhtémoc Cárdenas en devint le premier président, avant de se présenter une nouvelle fois à l'élection présidentielle de 1994. En compétition avec Ernesto Zedillo pour le PRI et Diego Fernández de Cevallos pour le PAN, il arriva en troisième position avec 17 % des voix. Trois ans plus tard en 1997, il se porte candidat à une élection hautement symbolique: la première élection au suffrage universel du maire de Mexico qu'il remporta largement avec 48 % des votes.

Il a été candidat une troisième fois pour le PRD lors des élections fédérales de 2000. Avec seulement 16 % des suffrages, il est battu par Vicente Fox, le candidat du PAN, parti avec lequel il avait jusqu'alors été dans l'opposition au parti officiel. Il s'est depuis mis en retrait de la vie du parti, tout en y conservant une grande autorité morale.

En 2006 il ne s'est pas porté candidat, ayant laissé le leadership au sein du PRD à Andrés Manuel López Obrador. Il démissionne du parti avec éclat en novembre 2014 en raison du soutien apporté par le parti au maire d'Iguala, suspecté d'être à l'origine des enlèvements des étudiants.



</doc>
<doc id="4898" url="https://fr.wikipedia.org/wiki?curid=4898" title="PRD">
PRD

PRD est un sigle qui désigne un parti :

PRD désigne aussi :

</doc>
<doc id="4899" url="https://fr.wikipedia.org/wiki?curid=4899" title="Caroline du Nord">
Caroline du Nord

La Caroline du Nord (en anglais "North Carolina") est un État du Sud des États-Unis. Sa capitale est Raleigh, et sa plus grande ville est Charlotte et son agglomération (Metrolina). Avec habitants en 2010 sur une superficie de , l'État est le du pays par sa population et le par sa taille. Longeant l'océan Atlantique à l'est, elle est bordée par la Caroline du Sud et la Géorgie au sud, le Tennessee à l'ouest, et la Virginie au nord. Territoire colonisé et occupé par les Européens dès le , la Caroline du Nord est l'une des Treize colonies qui fondèrent les États-Unis d'Amérique. Elle est divisée en cent comtés. En grande partie rurale, elle présente des paysages divers, entre les plateaux du Piedmont, les îles des ' et le Cap Hatteras, et les ', partie de la chaîne des Appalaches. Le Mont Mitchell, point culminant de l'est des États-Unis, se trouve dans l'ouest de l'État.

Le territoire est peuplé depuis plus de ans par des tribus amérindiennes, dont les Cherokees et les Tuscaroras. Découverte en 1512 par Juan Ponce de León, la Caroline du Nord est colonisée par des Espagnols, des Français et des Anglais durant le et le . Dans les années 1730, de nombreux immigrants écossais s'installent au pied des Appalaches, et se révoltent contre la domination britannique lors de la Guerre d'Indépendance. Le 21 novembre 1789, la Caroline du Nord devient le État de l'Union. À partir de 1870, elle connaît une forte expansion démographique, et voit sa population doubler entre 1970 (cinq millions d'habitants) et 2010 (9,5 millions). L'agglomération Raleigh-Durham, appelée aussi "", compte à elle seule plus de 1,8 million d'habitants. La ville principale de l'État, Charlotte, dépasse les 2 millions d'habitants dans son agglomération. 

Depuis les années 1950, l'économie de la Caroline du Nord connaît une phase de transition, passant de l'industrie manufacturière (tabac, textile, biens de consommation) à une économie plus diversifiée, notamment dans les secteurs de l'énergie, des biotechnologies et de la finance. L'État est traditionnellement conservateur et soutient généralement le Parti républicain, mais il est désormais considéré comme un '. Marqué par l'importance de la religion évangélique baptiste, il appartient à la '.

En outre, la Caroline du Nord compte de nombreux établissements universitaires, dont deux universités qui figurent parmi les plus prestigieuses du pays: Duke et l'université de Caroline du Nord à Chapel Hill. Enfin, la ville de Charlotte accueille plusieurs équipes sportives, dont les Panthers de la Caroline (football américain) et les Hornets de Charlotte (basket-ball).
Peuplé par l'homme depuis plus de ans au gré des vagues de migration, son territoire est partagé par diverses tribus dont les Cherokees, Tuscaroras, Cheraw, Pamlico, Meherrin, Coree, Machapunga, Indiens du Cape Fear, Waxhaw, Saponi, Tutelo, Waccamaw, Coharies, Woccon et Catawba. Elle fut le second territoire américain colonisé par l'Angleterre. Découvert en 1512 par l'Espagnol Juan Ponce de León et connu longtemps à partir de 1663 sous le nom de colonie d'Albemarle, du nom de l'estuaire d'Albemarle, Aumale en français (ainsi désigné en l'honneur du conseiller George Monck fait duc d'Aumale et bénéficiaire du comté de Moncks Corner par Charles II), le pays fut concédé en 1584 par Élisabeth à Walter Raleigh, qui tenta, mais sans succès, d'y former un établissement. En 1562, le Français Jean Ribault, envoyé par Charles IX, s'établit dans la Caroline du Sud, et donna au pays le nom de Caroline, en l'honneur de son roi (cf. Fort Caroline) ; mais en 1565 les Espagnols surprirent la colonie française et la massacrèrent. Quelque temps après, Dominique de Gourgues fut envoyé avec trois vaisseaux pour venger le massacre des Français, mais il n'essaya pas de relever la colonie.

Ensuite, pendant un siècle, la présence de l'homme blanc est restée proche de zéro, si l'on exclut un timide hameau installé au sud de la Chesapeake par des colons de Virginie.
En 1663, peu après la restauration de la dynastie Stuart et la fin de Première révolution anglaise, Charles II d'Angleterre donna les terres à huit propriétaires en remerciement de leur soutien militaire : le chancelier Edouard Hyde, premier comte de Clarendon, le général George Monck, duc d'Albemarle, lord Craven, lord John Berkeley, lord Anthony Ashley Cooper, comte de Shaftesbury, sir George de Carteret et sir William Berkeley. La plupart d'entre eux importent des esclaves après la création en 1672 de la Compagnie royale d'Afrique, pour leurs vastes plantations de tabac.

En 1729, leurs descendants en cédèrent la propriété au gouvernement britannique (sauf John Carteret qui négocia avantageusement ses droits) et qui divisa tout le pays en deux États, la Caroline du Nord et la Caroline du Sud, celle-ci incluant la future Géorgie, et qui le posséda jusqu'à la déclaration d'indépendance en 1775. En 1670, John Locke avait donné une constitution à la Caroline ; mais cette constitution ne put être appliquée.

La Caroline connut plus tard, dans les années 1730, une forte immigration de familles modestes écossaises qui s'installèrent au pied des Appalaches. Grâce à leur implication, la Caroline sera la treizième des treize colonies qui se sont révoltées contre la domination britannique lors de la guerre d'Indépendance.

La Caroline du Nord fait partie des États confédérés d'Amérique pendant la guerre de sécession et ce à partir du . Elle ne rejoint l'Union qu'en 1868.

La Caroline du Nord a connu trois constitutions :

La Caroline du Nord a des frontières avec la Caroline du Sud au sud, la Géorgie au sud-ouest, le Tennessee à l'ouest, la Virginie au nord, et l'océan Atlantique à l'est. À l'ouest, les Appalaches constituent une frontière naturelle avec le Tennessee.

Ses principales curiosités géographiques sont :

La Caroline du Nord est un État côtier ; de nombreux fleuves nés dans les Appalaches se jettent dans l'océan Atlantique, par exemple le Chowan, le Roanoke, la Pamlico, le Neuse, le Cape Fear, le Pee Dee et la Catawba. D'autres rivières naissent dans les Appalaches pour se jeter ensuite à l'ouest dans les affluents du Mississippi par exemple, la Hiwassee, la Little Tennessee et la French Broad.

Le National Park Service gère en Caroline du Nord les sites suivants :


L'État de Caroline du Nord est divisé en 100 comtés.

Le Bureau de la gestion et du budget a défini dix-sept aires métropolitaines et vingt-quatre aires micropolitaines dans ou en partie dans l'État de Caroline du Nord.

En 2010, 93,2 % des Nord-Caroliniens résidaient dans une zone à caractère urbain, dont 76,9 % dans une aire métropolitaine et 16,3 % dans une aire micropolitaine.

Le Bureau de la gestion et du budget a également défini onze aires métropolitaines combinées dans ou en partie dans l'État de Caroline du Nord.

L'État de Caroline du Nord compte 553 municipalités, dont 18 de plus de .
Les municipalités de Charlotte et Raleigh étaient respectivement les et les plus peuplées des États-Unis en 2013.

Le Bureau du recensement des États-Unis estime la population de l'État de Caroline du Nord à au juillet 2013, soit une hausse de 3,3 % depuis le recensement des États-Unis de 2010 qui tablait la population à . Depuis 2010, l'État connaît la croissance démographique la plus soutenue des États-Unis.

Avec en 2010, la Caroline du Nord était le 10 État le plus peuplé des États-Unis. Sa population comptait pour 3,09 % de la population du pays. Le centre démographique de l'État était localisé dans le sud-est du comté de Randolph.

Avec 75,73 /km en 2010, la Caroline du Nord était le État le plus dense des États-Unis.

Le taux d'urbains était de 66,1 % et celui de ruraux de 33,9 %.

En 2010, le taux de natalité s'élevait à ( en 2012) et le taux de mortalité à ( en 2012). L'indice de fécondité était de 1,91 enfants par femme (1,84 en 2012). Le taux de mortalité infantile s'élevait à ( en 2012). La population était composée de 23,93 % de personnes de moins de 18 ans, 9,84 % de personnes entre 18 et 24 ans, 26,99 % de personnes entre 25 et 44 ans, 26,30 % de personnes entre 45 et 64 ans et 12,94 % de personnes de 65 ans et plus. L'âge médian était de 37,4 ans.

Entre 2010 et 2013, l'accroissement de la population (+ ) était le résultat d'une part d'un solde naturel positif (+ ) avec un excédent des naissances () sur les décès (), et d'autre part d'un solde migratoire positif (+ ) avec un excédent des flux migratoires internationaux (+ ) et un excédent des flux migratoires intérieurs (+ ).

Selon des estimations de 2013, 91,3 % des Nord-Caroliniens étaient nés dans un État fédéré, dont 57,4 % dans l'État de Caroline du Nord et 33,9 % dans un autre État (14,8 % dans le Sud, 10,4 % dans le Nord-Est, 5,7 % dans le Midwest, 3,0 % dans l'Ouest), 1,1 % étaient nés dans un territoire non incorporé ou à l'étranger avec au moins un parent américain et 7,6 % étaient nés à l'étranger de parents étrangers (56,4 % en Amérique latine, 24,4 % en Asie, 10,5 % en Europe, 5,9 % en Afrique, 2,3 % en Amérique du Nord, 0,5 % en Océanie). Parmi ces derniers, 31,9 % étaient naturalisés américain et 68,1 % étaient étrangers.

Selon des estimations de 2012 effectuées par le Pew Hispanic Center, l'État comptait immigrés illégaux, soit 3,6 % de la population.

Selon le recensement des États-Unis de 2010, la population était composée de 68,47 % de Blancs, 21,48 % de Noirs, 2,19 % d'Asiatiques (0,60 % d'Indiens), 2,16 % de Métis, 1,28 % d'Amérindiens, 0,07 % d'Océaniens et 4,34 % de personnes ne rentrant dans aucune de ces catégories.

Les Métis se décomposaient entre ceux revendiquant deux races (1,99 %), principalement blanche et noire (0,65 %), et ceux revendiquant trois races ou plus (0,17 %).

Les non hispaniques représentaient 91,61 % de la population avec 65,27 % de Blancs, 21,18 % de Noirs, 2,17 % d'Asiatiques, 1,63 % de Métis, 1,14 % d'Amérindiens, 0,06 % d'Océaniens et 0,16 % de personnes ne rentrant dans aucune de ces catégories, tandis que les Hispaniques comptaient pour 8,39 % de la population, principalement des personnes originaires du Mexique (5,11 %) et de Porto Rico (0,75 %).

En 2010, la Caroline du Nord avait la forte proportion de Noirs des États-Unis.

L'État comptait également les grands nombres de Noirs () et d'Amérindiens () ainsi que les grands nombres de Blancs () et de Blancs non hispaniques () des États-Unis.

En 2013, le Bureau du recensement des États-Unis estime la part des non hispaniques à 91,2 %, dont 64,2 % de Blancs, 21,2 % de Noirs, 2,4 % d'Asiatiques, 2,0 % de Métis et 1,1 % d'Amérindiens, et celle des Hispaniques à 8,8 %.

En 2000, les Nord-Caroliniens s'identifiaient principalement comme étant d'origine américaine (13,9 %), anglaise (9,5 %), allemande (9,5 %), irlandaise (7,4 %) et scot d'Ulster (3,2 %).

L'État avait la plus forte proportion de personnes d'origine scot d'Ulster et la forte proportion de personnes d'origine américaine.

L'État abrite la juive des États-Unis. Selon le North American Jewish Data Bank, l'État comptait Juifs en 2013 ( en 1971), soit 0,3 % de la population. Ils se concentraient principalement dans les agglomérations de Charlotte-Concord-Gastonia (), Durham-Chapel Hill (), Raleigh () et Greensboro-High Point ().

Les Amérindiens s'identifiaient principalement comme étant Lumbees (43,3 %) et Cherokees (11,4 %).

Les Hispaniques étaient principalement originaires du Mexique (60,9 %), de Porto Rico (9,0 %), du Salvador (4,7 %) et du Honduras (3,9 %). Composée à 38,1 % de Blancs, 6,3 % de Métis, 3,6 % de Noirs, 1,7 % d'Amérindiens, 0,3 % d'Asiatiques, 0,2 % d'Océaniens et 49,9 % de personnes ne rentrant dans aucune de ces catégories, la population hispanique représentait 24,5 % des Métis, 20,4 % des Océaniens, 10,9 % des Amérindiens, 4,7 % des Blancs, 1,4 % des Noirs, 1,1 % des Asiatiques et 96,4 % des personnes ne rentrant dans aucune de ces catégories.

L'État avait la forte proportion de personnes originaires du Salvador (0,40 %).

L'État comptait également les grands nombres de personnes originaires du Honduras () et du Costa Rica (), les grands nombres de personnes originaires de la République dominicaine () et du Venezuela (), les grands nombres de personnes originaires du Salvador (), de Cuba (), de l'Équateur () et du Nicaragua () ainsi que le grand nombre de personnes originaires de la Colombie ().

Les Asiatiques s'identifiaient principalement comme étant Indiens (27,5 %), Chinois (16,4 %), Viêts (13,1 %), Coréens (9,2 %), Philippins (8,9 %), Hmongs (5,0 %) et Japonais (3,1 %).

L'État avait la forte proportion de Hmongs (0,11 %).

L'État comptait également le grand nombre de Hmongs () et le grand nombre de Laotiens ().

Les Métis se décomposaient entre ceux revendiquant deux races (92,1 %), principalement blanche et noire (30,1 %), blanche et amérindienne (16,3 %), blanche et autre (13,5 %), blanche et asiatique (13,5 %), noire et amérindienne (6,6 %) et noire et autre (3,8 %), et ceux revendiquant trois races ou plus (7,9 %).

Selon l'institut de sondage "The Gallup Organization", en 2015, 49 % des habitants de Caroline du Nord se considèrent comme « très religieux » (40 % au niveau national), 29 % comme « modérément religieux » (29 % au niveau national) et 22 % comme « non religieux » (31 % au niveau national).

Selon l"'American Community Survey", en 2010 89,66 % de la population âgée de plus de 5 ans déclare parler l'anglais à la maison, 6,93 % déclare parler l'espagnol, et 3,41 % une autre langue.

La Caroline du Nord est un État sudiste conservateur et un ancien bastion du Parti républicain. Elle est aujourd'hui considérée comme un "swing state".

L'essentiel de l'État est acquis aux républicains, en particulier les Appalaches (à l'exception de la ville universitaire d'Asheville), les banlieues du Piedmont et les comtés ruraux de la côte atlantique. Les grandes villes de l'État (Charlotte, Durham, Greensboro et Raleigh) et les comtés afro-américains, au nord-est, constituent les quelques enclaves démocrates de Caroline du Nord.

Ancien État confédéré, la Caroline du Nord est acquise aux démocrates de la fin de la période de reconstruction jusqu'aux années 1960. Herbert Hoover est le premier candidat républicain à une élection présidentielle à y être élu en 1928.

En 1968, Richard Nixon est le second républicain au à remporter le collège électoral. Il y est réélu en 1972.

Jusqu'en 2008, le seul candidat démocrate à parvenir à s'imposer fut Jimmy Carter en 1976.

Le républicain George W. Bush y avait été facilement élu en 2000 et aussi en 2004 avec 56 % des voix en dépit de la présence de John Edwards, sénateur de l'État, comme candidat à la vice-présidence au côté du démocrate John Kerry.

Lors de l'élection présidentielle américaine de 2008, le candidat démocrate Barack Obama s'impose de justesse, avec 0,33 point d'avance, sur son adversaire républicain John McCain.

En 2016 c'est le républicain Donald Trump qui remporté l'État avec 49,8 % des voix face à 46,2 % des votes pour la démocrate Hillary Clinton.

Au niveau fédéral, lors du (législature 2017-2019), la délégation de Caroline du Nord au Congrès des États-Unis comprend deux sénateurs républicain (Richard Burr) et Thom Tillis), dix représentants républicains et trois représentants démocrates.
Le pouvoir exécutif de Caroline du Nord est détenu par le gouverneur de l'État. Depuis 2017, il s'agit du démocrate Roy Cooper, qui a battu en 2016 le gouverneur républicain sortant Pat McCrory.

L'Assemblée générale de Caroline du Nord ("North Carolina General Assembly" en anglais) est l'organe législatif du gouvernement. Elle est composée d'une Chambre des représentants, comportant cent vingt membres, et d'un Sénat, composé de cinquante membres. Les deux chambres sont à majorité républicaine depuis les élections de 2010. Le Parti républicain dispose même d'une supermajorité depuis 2012, permettant d'outrepasser un éventuel véto du gouverneur.

Le pouvoir judiciaire se nomme "" en Caroline du Nord. Il comprend les tribunaux suivants :

Le , la Caroline du Nord est l'État qui procède à l'exécution du millième condamné à mort depuis le rétablissement, en 1976, de la peine capitale aux États-Unis.

Le Research Triangle Park est un parc de haute technologie et un lieu d'enseignement situé à Raleigh.

L'aéroport international de Raleigh-Durham ainsi que l'aéroport international Charlotte-Douglas desservent cet État.

Collèges et Universités

Équipes professionnelles de sport


</doc>
<doc id="4902" url="https://fr.wikipedia.org/wiki?curid=4902" title="Groupe Proximus">
Groupe Proximus

Le groupe Belgacom-Proximus, composé de Proximus SA (anciennement Belgacom S.A.) et de ses filiales (Belgacom ICS, Telindus, Belgacom Skynet, Scarlet, Tango), est une entreprise belge de télécommunications, la première entreprise de télécommunications en Belgique.

En 1879, les services télégraphiques belges installèrent une ligne téléphonique au parlement et, la même année, divers entrepreneurs privés déposèrent des demandes d’exploitation de réseaux téléphoniques dans différentes villes belges.

L’absence d’une législation dans les premières années d’exploitation minimisa les chances de succès du développement du réseau téléphonique. Cette absence força les autorités belges à mettre au point un cadre législatif réglementant l’exploitation de la téléphonie en Belgique.

À partir de 1896, le secteur entier de la téléphonie passa entre les mains d’une société publique.

En 1913, une grande partie de la Belgique était accessible par téléphone. Le nombre d’abonnés restait limité, mais la plupart des gares, bureaux de poste et de télégraphe étaient équipés de cabines téléphoniques publiques.

La Première Guerre mondiale représenta un arrêt net et définitif en matière de télécommunication en Belgique.

Un des facteurs explicatifs de cet arrêt s’explique par la dépendance financière de l’entreprise publique. Les dommages causés lors du conflit et le démantèlement partiel des réseaux placèrent l’administration des télégraphes et téléphones devant un besoin d’investissements colossaux.

Le est créée la Régie des Télégraphes et Téléphones (RTT). L’entreprise publique gagne en autonomie : elle n’est désormais plus tributaire des budgets annuels de l’État et a la compétence requise pour mener une gestion propre.

Avec la création de la RTT, d’énormes sommes sont investies dans le réseau téléphonique belge. Des couches sociales sans cesse plus larges ont désormais accès à la téléphonie.

Parallèlement à cela, un autre phénomène intervient qui va devenir rapidement une charge importante pour l’entreprise. L’État va, dans le cadre de la crise économique des années 1930, utiliser la RTT dans sa politique industrielle et d’emploi. En forçant la réalisation d’une automatisation complète du réseau téléphonique belge, l’État tente de contrer le haut taux de chômage du secteur.

Ce phénomène diminue fortement l’autonomie de la RTT. La loi de 1930 avait en effet très clairement explicité que l’entreprise pouvait concevoir et appliquer de manière indépendante un programme d’investissements. En imposant sa politique d’emploi, l’État allait donc contre le principe premier de la loi. Ce dernier élément deviendra très vite dans la période d’après-guerre un problème structurel de la RTT.

Au lendemain de la Seconde Guerre mondiale, la RTT se voit confrontée à d’importants dommages et à un démantèlement partiel des réseaux. Afin de relancer rapidement la dynamique du secteur, l’état décide d’intervenir financièrement.

Au cours de cette période, la demande en services de télécommunication s’accroît à un rythme élevé. Le nombre d’abonnés augmente très rapidement: d’environ en 1946 à en 1951 et en 1965. Cette croissance de la clientèle amène un rythme d’investissements très élevé. Grâce à tout cela, la RTT se situe vers la fin des années 1960 à la pointe du développement technologique et social.

Mais cette gestion expansionniste a un côté négatif. À partir de la fin des années 1960, les pertes commencent à s’accumuler, et l’éclosion d’une crise économique mondiale dès 1973 n’arrange rien aux problèmes. La situation financière de l’entreprise ne fait que se dégrader. Cette situation conduira la RTT à mettre en place à partir du milieu des années 1970 d’importants programmes d’assainissement.

Au cours des années 1980 naît la conviction que le secteur des télécommunications sera un des pôles de développements les plus significatifs de cette fin de . Ainsi, à partir de 1981, les dirigeants de la RTT commenceront la restructuration profonde de la RTT afin de résoudre certains problèmes structurels de l’entreprise.

Parallèlement à cela, un autre partenaire fait son entrée en 1987 : La Commission européenne promulgue son Livre Vert pour les télécommunications dont le thème central est la libéralisation.

Le "Livre Vert" de 1987 participe en Belgique au fondement de la loi du , laquelle crée un nouveau type d’entreprise d’État jouissant d’une plus grande autonomie de gestion. Le secteur des télécommunications belge est donc réorganisé et voit la création de Belgacom, entreprise publique autonome.

Cette loi a pour objectif de créer un environnement propice au développement concurrentiel du marché des télécommunications en Belgique. Désormais, un contrat de gestion définit les prérogatives de l’entreprise et des pouvoirs publics de manière à garantir l’offre d’un certain nombre de services publics d’utilité générale et une autonomie de gestion suffisante et bien plus élargie que ce que la loi de 1930 n’avait prévu.

À partir de 1994 se produit une accélération des processus de convergence européens. La Commission européenne déclare dans un nouveau Livre Vert que l’exploitation des réseaux et la téléphonie doivent également être ouverts à la concurrence.
1994 est également l’année où Belgacom crée Proximus, le premier réseau cellulaire en Belgique. Cette activité ainsi que l'ancien système analogique Mob2 sont transférés le vers une filiale : Belgacom Mobile dont l’actionnariat est le suivant : 75 % Belgacom – 25 % , puis Vodafone en 1999.

Parallèlement, Belgacom se prépare à affronter la concurrence en s’alliant à , Tele Danmark et Singapore Telecom. Diverses institutions financières belges réagissent et s’allient également au consortium, lequel est baptisé ADSB. L’état belge conserve 50,1 % des actions et reste donc l’actionnaire principal.

L’année 2001 voit la mise en place du plan BeST qui vise principalement à restructurer l’entreprise en la divisant en quatre « business units ». Belgacom se défait également de certaines activités comme Belgacom France, Ben, ses activités de sécurité ainsi que les activités françaises d’Infosources.

Le volet humain du plan BeST interviendra dans le courant de l’année 2002. Les objectifs sont multiples dans une société qui, à l’époque, occupait un trop grand nombre de travailleurs : offre de cessation des activités, offre de travail à temps partiel et reconversion sont proposés à une grande partie du personnel.

Dans un marché de plus en plus ouvert et où la concurrence se fait chaque jour plus agressive, Belgacom décide en 2003 de faire un pari sur l’avenir en modifiant radicalement son image. Changement de logo, de couleurs et volonté manifeste d’être plus proche des clients sont les bases sur lesquelles travaille l’ex-RTT.

Ces changements radicaux dans la philosophie de l’entreprise préfigurent la mise en bourse de l’opérateur. En effet, le , Belgacom est cotée pour la première fois sur le marché Euronext. L’état belge reste l’actionnaire majoritaire avec 50 % + 1 action tandis que le consortium ADSB se défait de l’ensemble de ses actions.

Cette opération permet à l’opérateur historique belge de dégager d’importants moyens pour financer ses ambitions. En effet, l’heure est à la large bande (broadband) et le financement du projet Broadway (couvrir l’ensemble du territoire national en fibre optique) nécessite de gros investissements.

2004 est aussi l’année où l’opérateur historique réalise ses premiers tests de télévision numérique avec pour ambition de trouver de nouvelles sources de revenus dans un marché de plus en plus concurrentiel.

L’année 2005 est marquée par deux événements majeurs pour Belgacom : le lancement de Belgacom TV et l’offre publique d’achat sur Telindus.
Les prémices du lancement de la télévision numérique en Belgique apparaissent pour la première fois dans le courant de l’année 2004 lorsque Belgacom procède à ses premiers tests de télévision numérique auprès de quelques centaines de foyers.

En mai 2005, l’opérateur belge surprend le marché en décrochant les droits de diffusion du football belge professionnel (D1 & D2) pour les trois prochaines saisons via sa filiale Skynet iMotions Activities.
Cette action préfigure le lancement imminent de Belgacom TV qui voit le jour en juin 2005. Cette offre de télévision numérique via l’ADSL est la première du genre en Belgique et transforme Belgacom en un "quadruple player" offrant la téléphonie fixe, la téléphonie mobile, l’accès à internet à haut débit ainsi que la télévision. Elle permet en outre à l’entreprise belge de garantir de nouvelles sources de revenus dans la mesure où les marges dégagées par ses activités historiques sont de plus en plus faibles.

2005 est aussi l’année de l’offre publique d’achat sur Telindus, entreprise leader dans le secteur de l’intégration des réseaux. Une première offre, considérée comme hostile par la direction de Telindus, est faite dans le courant du mois de septembre. C’est le début d’une saga boursière qui durera pratiquement quatre mois. Les tensions sont vives entre les deux sociétés et les différents intervenants dans ce dossier se font la guerre par voie de presse.
Après une contre-OPA emmenée par France Telecom, c’est finalement Belgacom qui remporte la mise et qui parvient, fin décembre, à un accord conditionnel sur un partenariat.

L’année 2006 est principalement marquée par le rachat au mois d’août des 25 % de Proximus détenus par Vodafone. Cette opération permet à Belgacom de se préparer au mieux à la convergence. En effet, la tendance du marché semble se diriger vers des offres de services groupés. Les fournisseurs de service de télécommunications en Belgique répondent de plus en plus aux besoins des utilisateurs en proposant des solutions allant de services commercialisés séparément à des offres « quadruple play » groupées et complètes.
Également en 2006, Telindus et Belgacom étendent leur portefeuille ICT sous la nouvelle marque Telindus/Belgacom ICT.

Le mois d’avril 2007 est marqué par le lancement des « Packs », premières offres groupées de Proximus et Belgacom.
Côté télévision, Belgacom continue à développer son offre de télévision numérique. Fin décembre 2007, l’entreprise comptait clients Belgacom TV. Le taux de couverture de la télévision numérique est quant à lui de 80 % de la population.

Le début de l’année 2008 est marqué par le rachat de Scarlet et de Tele2 Luxembourg.
Belgacom rachète son concurrent direct Scarlet pour une somme 185 millions d’euros ce qui provoque une polémique car le groupe absorbe un concurrent direct. Le dossier se trouve actuellement auprès des autorités chargées de la concurrence.
Belgacom achète aussi en juin 2008 les opérations de Tele2 au Luxembourg et au Liechtenstein.

En mars 2009, Belgacom crée Pingping comme moyen de paiement mobile.

En avril 2011, Belgacom achète la section belge de la chaine de magasin de téléphone mobile The Phone House à The Carphone Warehouse Group. En septembre de la même année, l'avenir de Belgacom pourrait passer par une privatisation décidée par l'état.

En 2014 même si Belgacom S.A. reste le nom d’entreprise, Proximus devient la marque commerciale. En avril 2015, l'assemblée générale approuve le changement du nom de Belgacom SA en Proximus SA.

Le juin 2015, la dernière cabine téléphonique publique est retirée.

Le groupe Belgacom a construit en 2007 une nouvelle structure opérationnelle basée sur quatre piliers auxquels s'ajoutent les services internationaux de carrier :

L’entité Consumer Business Unit (CBU) commercialise les produits et les services vocaux, internet et télévision tant sur les réseaux fixes que mobiles à destination des clients particuliers.

L’entité Enterprise Business Unit (EBU) répond aux besoins ICT des clients professionnels.

Les réseaux et les services informatiques ainsi que les activités Wholesale sont regroupés au sein d’une unité centrale : Technology (TEC).

Cette nouvelle entité regroupe les activités après-ventes opérationnelles actuellement réparties entre les différentes "Business Units". Le but de cette consolidation est d’augmenter l’efficacité des processus clients de Belgacom ainsi que le rapport coût-efficacité. Les équipes de cette nouvelle entité travaillent sur l’alignement des procédures et tendent à générer des synergies dans les activités, permettant ainsi l’amélioration de l’efficacité en matière opérationnelle.

L’unité Staff & Support réunit les services de soutien des différentes filiales du groupe. Elle rassemble l’ensemble des fonctions transversales qui soutiennent l’activité du groupe.

Au sein du groupe Belgacom, ces activités sont assurées par la filiale Belgacom ICS, une société initialement détenue à 72 % par Belgacom S.A. et à 28 % par Swisscom Fixnet. L'entrée d'un actionnaire est cependant en cours de finalisation.

L’entreprise fournit des services de capacité et de connectivité de voix et de données aux opérateurs de télécommunications dans le monde entier.

Elle est aujourd’hui le cinquième plus grand opérateur au monde en termes de volume de trafic vocal et le leader mondial en termes de services de signalisation pour les opérateurs mobiles.

Belgacom a annoncé l'entrée de MTN dans le capital de BICS à hauteur de 20 %. Après cette transaction, Belgacom détient 57,6 % de BICS et Swisscom 22,4 %

La marque Belgacom voit le jour en 1992, à la suite de la disparition de la RTT. Si l'entreprise change plusieurs fois de statut afin de respecter les dispositions légales nationales et européennes, son logo évolue également. Des flèches orange inversées du début, le logo évolue graphiquement pour "laisser une place plus grande à l'humain".

La marque Proximus voit le jour en 1994, à la suite du lancement du premier réseau mobile éponyme. En 2005, l'étude Interbrand consacre Proximus comme deuxième marque en Belgique. Proximus offre une large gamme de produits et services de télécommunications mobiles à ses clients résidentiels en professionnels dont la plus large couverture HSDPA de Belgique.

La marque Telindus Belgacom ICT a été créée en juin 2006 à la suite de l'alignement entre Belgacom et Telindus. Depuis cette date, les activités ICT du groupe Belgacom sont proposées sous la marque Telindus qui, à cette occasion, a transformé son logo et son style pour devenir Telindus-Belgacom ICT.

La mention Belgacom ICT souligne le fait que Telindus est membre à part entière du groupe Belgacom. La nouvelle marque Telindus est inspirée par l'évolution qu'a suivie Telindus depuis sa fondation : de fournisseur de technologie, Telindus est devenu partenaire de solution et de sourcing en passant par l'intégration. La nouvelle marque de Telindus, qui remplace une marque vieille de 37 ans, met l'accent sur l'aspect entrepreneur et dynamique de la société.

La marque Scarlet voit le jour en 1992 aux Pays-Bas. La marque a pour objectif d'offrir à ses clients des produits basiques et économiques en téléphonie fixe, mobile et internet. Scarlet existe en tant que marque complémentaire à celles existantes au sein du groupe.

La marque Skynet a été créée en 1995 en même temps que la société éponyme. Elle constituait à l'époque la marque du premier fournisseur d'accès à internet belge, pour les particuliers et sociétés privées.

À partir de l'année 2005, et à la suite de la reprise par Belgacom des activités internet du groupe, la marque a continué à exister mais a été dorénavant associée aux activités de contenu web du groupe Belgacom.

Tango constitue la marque sous laquelle le groupe Belgacom commercialise ses offres au Luxembourg. Lancée en 1998, la marque a remporté un franc succès en devenant le deuxième opérateur mobile sur le marché luxembourgeois.

Swing (Surfer avec Win Gratuitement) est un fournisseur d'accès Internet belge (wallon) des années 2000.

En 1999, la société WIN S.A. formée par plusieurs actionnaires dont le principal est Belgacom gère le réseau WIN (Wallonie Intranet) sous contrat de service de la région Wallonne.

Le réseau Win est alors destiné à donner un accès rapide aux réseaux informatiques de clients tel que les administrations, PME, écoles et hôpitaux.

Afin de promouvoir Internet en Wallonie auprès des particuliers et TPE, les services SwinG sont mis sur pied par WIN S.A. un an plus tard.
La solution proposée consistait en une connexion Internet gratuite.
Les services étaient accompagnés de messageries électroniques @swing.be et d'hébergements web en users.swing.be.

En 2000 soit un an après son démarrage, SwinG compte plus de utilisateurs.

SwinG se destinait à une clientèle résidentielle ainsi qu'aux très petites entreprises.

La société WIN S.A. devient rapidement filiale Belgacom à 100%. L'opérateur historique belge devenant de ce fait l'unique propriétaire de SwinG.

À la fin de l'année 2013, Belgacom annonce que le nom de domaine swing.be a été revendu en 2012 à une société belge néerlandophone "Swing", active dans la vente et la location de matériel de musique et invite les utilisateurs du service de messagerie et d'hébergement à migrer vers skynet.be.

Le 20 février 2014, une intervention a lieu au Sénat de Belgique concernant la "revente par Belgacom/Skynet du nom de domaine swing.be".

Le 31 aout 2014, SwinG arrête ce que l'entreprise désigne comme étant son nom de domaine swing.be .

Le 28 octobre 2014, le site proximus.be mentionne swing.be dans une liste d'adresses de messageries électroniques restant inchangées à la suite du choix du nom Proximus comme marque principale utilisée par Belgacom.

PingPing est la marque neutre et indépendante de Mobile-for, rassemblant les activités de micropaiements mobiles.

La marque Euremis représente le pôle d'expertise en solutions CRM mobiles de Proximus. Créée en 2002 et rachetée en septembre 2006 par Proximus, Euremis propose des solutions mobiles de gestion de clientèle dédiées aux forces de vente dans les secteurs FMCG (Fast Moving Consumer Goods) et pharmaceutique.

Proximus TV est la marque de l'offre de télévision numérique en Belgique lancée durant l'été 2005 par Proximus. Il s'agit de la première offre du genre à avoir été lancée dans le pays. La marque Proximus TV table sur des valeurs d'innovation, de flexibilité et de qualité d'image et de son.

Chaîne de 114 magasins de vente de téléphones mobiles.

Créée en novembre 2013, Belgian Mobile Wallet S.A. est une coentreprise pour la mise au point d'une solution de payement mobile, commercialisé sous la marque Sixdots.

Situation au 30 juin 2013 :

Données financières en millions d’euros.
Capitalisation boursière en février 2011 : 9,2 milliards d'euros.




</doc>
<doc id="4909" url="https://fr.wikipedia.org/wiki?curid=4909" title="Réserve fédérale des États-Unis">
Réserve fédérale des États-Unis

La Réserve fédérale (officiellement ', souvent raccourci en ' ou ') est la banque centrale des États-Unis. Elle est créée en décembre 1913, durant les fêtes, par le ' dit aussi "", à la suite de plusieurs crises bancaires, dont la panique bancaire américaine de 1907. Son rôle évolue depuis et elle renforce son indépendance lors de l'instabilité monétaire des années 1975 et 1985.

Le Congrès des États-Unis définit trois objectifs de politique monétaire dans le "Federal Reserve Act" : plein emploi, stabilité des prix, et taux d'intérêt à long terme modérés. Les deux premiers sont souvent appelés le « double objectif » ou « double mandat » de la "Fed". Outre la politique monétaire, la Fed est maintenant chargée de superviser et réguler le système bancaire, de maintenir la stabilité du système financier, et d'offrir des prestations financières aux organismes de dépôt, au gouvernement fédéral, et aux institutions financières étrangères. Elle étudie de surcroît l'économie américaine, et publie de nombreux rapports, tels que le livre beige, un résumé des conditions économiques dans chaque région.

La Réserve fédérale se compose d'un conseil des gouverneurs (dont Jerome Powell est le président depuis 2018), du Federal Open Market Committee (FOMC), de douze banques régionales (Federal Reserve Banks), des banques membres, et de plusieurs conseils consultatifs. Le FOMC est le comité responsable de la politique monétaire ; il se compose des sept membres du bureau des gouverneurs et des douze présidents des banques régionales (dont cinq seulement ont le droit de vote à un moment donné). La Réserve fédérale comporte ainsi des aspects publics et privés : cette structure est unique au monde pour une banque centrale, et correspond à une volonté de répondre à la fois à l'intérêt public et à celui des banques membres. Une autre particularité du système monétaire américain est que ce n'est pas la banque centrale mais le département du Trésor qui crée la monnaie fiduciaire.

La Fed est une banque centrale indépendante : ses décisions ne sont pas sujettes à l'autorisation du Président des États-Unis ou d'une autre partie du gouvernement fédéral, elle ne reçoit pas de budget du Congrès, et les mandats des gouverneurs sont beaucoup plus longs que ceux des élus fédéraux. Le gouvernement peut cependant exercer un contrôle : l'autorité de la Fed est définie par le Congrès et celui-ci peut exercer son droit de surveillance ("congressional oversight"). Les membres du bureau des gouverneurs, y compris le président et le vice-président, sont nommés par le Président des États-Unis et confirmés par le Sénat. Le gouvernement nomme également les hauts fonctionnaires de la banque et fixe leur salaire. Toutes les banques commerciales autorisées à exercer en dehors d'un seul État sont obligatoirement membres de la Réserve fédérale régionale où se trouve leur siège et détiennent des parts dans celle-ci, ce qui autorise ces banques à élire une partie des membres du bureau de chaque Réserve fédérale régionale. Le gouvernement fédéral reçoit tous les profits de la Fed, hormis un dividende de 6 % versé aux banques membres. 

En 1791, le gouvernement fédéral des États-Unis crée la "First Bank of the United States" chargée de l'émission de la nouvelle monnaie américaine et de la régulation du crédit. En 1816, après la seconde guerre contre les Britanniques, elle est remplacée par la Second Bank of the United States surtout pour mettre fin à l'inflation galopante consécutive à la guerre de 1812. Mais, en 1830, elle est dissoute par le président Andrew Jackson qui est hostile aux banquiers, non responsables devant le peuple américain. Les États-Unis ont eu à faire face à un système monétaire très complexe qui reposait sur un troc entre différentes monnaies régionales, les "greenpapers", qui rendait toute régulation impossible.

En 1908, à la suite de la panique bancaire américaine de 1907, le Congrès forme une "National Monetary Commission" menée par le sénateur républicain Nelson Aldrich pour étudier une réforme bancaire et monétaire. Les travaux de cette commission jetteront les bases du "Federal Reserve Act" adopté le par le Congrès et promulgué le par le président démocrate Woodrow Wilson, nouvellement élu. Si l'organisation (banques régionales + bureau des gouverneurs) est la même que celle d'aujourd'hui, le gouvernement place la nouvelle institution sous son autorité en y nommant comme membres le Secrétaire du Trésor et le "Comptroller of the Currency" (« Contrôleur de la monnaie »). Son but est alors de favoriser la gestion de la monnaie et l'économie du pays, de permettre l'escompte des effets de commerce et, de manière plus générale, de surveiller le bon fonctionnement des banques américaines. 

La crise de 1929 montre les limites de ce système, même si à l'époque la solution envisagée par la réserve fédérale new-yorkaise, une relance monétaire, aurait permis de sortir de la crise. Cette solution fut d'ailleurs refusée par les autres membres, de peur de voir cette dernière devenir trop puissante. En 1935, le "Federal Reserve Board" devient le "Gouvernors Board" et, par le "Banking Act", acquiert un pouvoir de contrôle sur les banques régionales. Est créé également le "Federal Open Market Committee" (FOMC), un comité de politique monétaire qui veille à la réglementation et au contrôle des taux d'intérêt. 

Avec le retrait du secrétaire d'État au Trésor et du Contrôleur de la monnaie, la banque devient théoriquement indépendante pour la politique monétaire. Mais, dans la pratique, elle continue à subir des pressions politiques dont elle va se défaire progressivement après la Seconde Guerre mondiale, d'abord pendant la longue présidence de McChesnet Martin (1951-1970), puis en 1978 par le qui redéfinit le mandat de la Fed et amène la présidence de Paul Volcker (1979-1987) à porter, pour en prouver la validité, les taux d'intérêt à 15 % lors de la crise monétaire de 1980.
La banque centrale est aussi indépendante financièrement, ne recevant aucun budget ni du gouvernement, ni du Congrès américain. Elle se finance via les intérêts des emprunts publics auxquels elle souscrit sur les marchés, les commissions perçues pour les prestations aux banques de dépôts et les intérêts sur les changes de monnaies étrangères. En 2005, elle a ainsi versé près de de USD à ses actionnaires et plus de USD d'excédent au Trésor américain.

La crise financière de 2007 à 2011 a obligé la banque centrale américaine à mener une politique expansionniste. En , la Réserve fédérale américaine a baissé drastiquement son taux directeur à 2,25 %. Une décision prise en urgence face aux risques accrus pour la croissance et à la panique des marchés boursiers, qui craignent une récession aux États-Unis. La Fed a réduit son taux de référence pour les prêts interbancaires de trois quarts de point, pour la première fois depuis la mise en place du système des taux actuel au début des années 1990. Fait inhabituel, la Réserve fédérale a pris sa décision en dehors de la réunion habituelle de son comité de politique monétaire, prévue les 29 et 30 janvier. Il faut remonter à 2001, après les attaques du 11 septembre, pour retrouver une baisse d'urgence du taux directeur.

Alors que la situation s'aggrave, le 16 décembre 2008 la Fed décide de baisser son taux directeur à 0 % (marge de fluctuation entre 0 et un 0,25 %), ce qui provoque une rapide chute du dollar contre toutes les autres devises et une ruée vers l'or physique. De plus, la Fed a désormais officiellement "carte blanche" pour intervenir sur les marchés afin de maintenir un système agonisant.

Le 18 mars 2009, pour contrer les effets de la récession aux États-Unis, la Fed a décidé d'acquérir pour USD d'obligations du Département du Trésor des États-Unis, pour USD de "mortgage-backed securities" (MBS), portant ainsi son portefeuille de MBS à USD, et d'acquérir des dettes de Fannie Mae et Freddie Mac pour USD. Selon un bureau d'études économiques, la Fed, en moins d'un an, aura ajouté à son bilan USD de dettes. Ces opérations ont, entre autres, pour but d'augmenter la liquidité sur les marchés de l'emprunt.

En 2010, la Fed a obtenu un profit de de dollars, dont 79 ont été versés au département du Trésor. En mars 2011, la Fed annonce un , alors qu'elle dégageait en moyenne annuellement pendant les 10 ans précédant la crise financière de 2008-2009. En 2011, ont été versés. En 2014, elle reverse 98,7 milliards de dollars au Trésor public.

Le "Federal Reserve Act", modifié en octobre 1978 par le "Humphrey-Hawkins Full Employment Act", définit ainsi la mission de la Réserve fédérale : « Maintenir en moyenne une croissance des agrégats monétaires et de la quantité de crédit compatible avec le potentiel de croissance de la production, de manière à tendre vers les objectifs suivants : 

Le Conseil des gouverneurs est son conseil d'administration, dont le siège est à Washington, D.C.. Il compte sept membres nommés par le président des États-Unis et confirmés par le Sénat américain. Leur mandat est de quatorze ans non renouvelable. Celui du président est de quatre ans renouvelable sans limite. Malgré ce processus de désignation, l'institution est en pratique quasiment indépendante du pouvoir politique. Le gouverneur dépose néanmoins régulièrement devant le Congrès des États-Unis.

La présidente ("chair") du Conseil des gouverneurs est Jerome Powell, depuis le 5 février 2018.

Paul Volcker a, en 1979-1981, et au prix d'une sévère récession, mis fin à l'inflation des années 1970 en faisant monter les taux au jour-le-jour du marché monétaire, les "Fed Funds", à plus de 20 %.

Alan Greenspan, économiste né en 1926, était depuis le président du Conseil des gouverneurs, poste auquel il a été nommé initialement par Ronald Reagan et confirmé par tous les successeurs de ce dernier. Son mandat a pris fin le .

Sous sa direction, la Réserve fédérale, après un début plutôt rigoriste, aura finalement eu une politique globalement très accommodante, n'hésitant pas à ajouter à chaque choc important des liquidités dans le système bancaire et à maintenir longtemps des taux extrêmement bas pour faciliter la reprise économique, au risque d'alimenter des « bulles » spéculatives (actions technologiques en 2000, immobilier et obligations en 2004-2005) et le déséquilibre des comptes extérieurs des États-Unis. La hantise d'une répétition des erreurs de 1929, où la Réserve fédérale avait eu une politique restrictive catastrophique, a manifestement été un thème majeur de son mandat.

Confronté à la crise financière mondiale débutant en 2007 et au risque de blocage total du marché interbancaire, Ben Bernanke fait procéder à de massives injections de liquidité sur le marché interbancaire afin de permettre aux établissements de refinancer leur activité et d'éviter le déclenchement d'une crise systémique. De cette manière, il transfère la dette bancaire vers la Fed et le fisc puis les pays tiers et dégrise momentanément le marché de crédit des prêts toxiques qui avaient conduit à la crise.

Cette politique d'assouplissement quantitatif fait passer le bilan de la Fed de 800 milliards de dollars à plus de 4.000 milliards. Par ce processus, la Fed devient une sorte de « bad bank » qui concentre du papier ayant une valeur très douteuse.


La Réserve fédérale est détenue par douze "Regional Federal Reserve Banks", et est donc à ce titre, une institution fédérale. Ce statut est censé lui assurer l'indépendance de ses choix vis-à-vis du gouvernement. La Réserve fédérale ne reçoit pas de subventions du congrès pour son fonctionnement.

En revanche, les parts détenues par les banques régionales dans la Réserve fédérale ne sont ni échangeables ni vendables, et ne peuvent être mises en gage pour dégager des fonds. De plus, ces parts rapportent un coupon fixe de 6 % annuel. Et finalement, l'excédent de capital généré par les activités de la Réserve fédérale peut être cédé au budget fédéral, mais en aucun cas aux banques régionales actionnaires. Ceci exclut donc le risque d'une instrumentalisation pure et simple des capacités de la Réserve fédérale par ses actionnaires. En 2006 par exemple, la Réserve fédérale a contribué positivement au budget fédéral à hauteur de de dollars.

Chacune des douze "Regional Federal Reserve Bank" possède sa propre zone de chalandise ("District Reserve Banks") regroupant plusieurs États ou portion d'État, parfois représentée au niveau local par l'une des 25 succursales du réseau. Chaque banque commerciale se trouvant dans la zone géographique de l'une des "Regional Federal Reserve Banks" est obligatoirement actionnaire de celle-ci. Les sièges de chaque banque régionale sont respectivement situées, par ordre d'importance, dans les villes de : 

La "Federal Reserve Bank of New York" est donc la plus importante des douze banques du réseau, car c'est à elle qu'échoit la supervision de l'une des plus importantes places bancaires mondiales (que certains situent en deuxième place). Elle concentre plus de 40 % des actifs des banques régionales, et constitue aussi la plus grande réserve d'or du monde avec en dépôt en 2006 dont seulement 2 % appartiennent aux États-Unis, mais dont les principaux propriétaires sont une cinquantaine d'états étrangers, des organismes internationaux et quelques particuliers.

Elle joue également un rôle majeur puisque c'est elle qui exécute les opérations de marché décidées par le comité de politique monétaire pour faire varier les fonds fédéraux. Elle peut aussi intervenir sur le marché des changes à la demande du Trésor américain. Son président est membre permanent du Comité de politique monétaire.

La Réserve fédérale décide de manière définitive de la politique monétaire américaine, l'organe décisionnaire étant le "Federal Open Market Committee" ou FOMC, ce qu'on peut traduire par « Comité fédéral pour les interventions publiques sur les marchés de taux d'intérêt » équivalent d'un « Comité de politique monétaire ». Il siège au moins huit fois par an. Il est composé des membres du bureau des gouverneurs, du président de la "Federal Reserve Bank of New York" et, par rotation, de quatre autres représentants des "Regional Federal Reserve Bank". Un compte-rendu de la réunion est rendu public au bout de trois semaines. 

Le FOMC se fixe des objectifs sur le taux au jour-le-jour du marché monétaire américain, les "Fed Funds". Pour y arriver, les instruments dont il dispose sont :

En revanche, la valeur externe du dollar américain (USD) est, elle, du strict ressort du gouvernement fédéral, et la Réserve fédérale se garde bien de la commenter sauf, éventuellement, et avec de sérieuses précautions oratoires, quant aux effets inflationnistes ou déflationnistes de son évolution.

La Réserve fédérale n'utilise pas d'objectifs intermédiaires clairement déterminés pour guider sa politique monétaire ; elle a abandonné la politique de suivi strict des agrégats monétaires (M3 notamment). Elle pratique une politique de "fine tuning" (réglage fin), ajustant ses taux plus fréquemment que ses homologues, ce qui la rend plus réactive.

La banque centrale américaine se situe à Washington, D.C., sur l’avenue de la Constitution ("Constitution Avenue"). Elle a été construite en 1935, selon les plans de Paul Philippe Cret. Un autre bâtiment édifié en 1974, (le "Martin Building") se trouve sur "C Street" et abrite d'autres services de la Réserve fédérale.
Depuis 2001, elle emploie plus de armés (Sig-Sauer P229, .40 S&W/12 coups) répartis dans 25 stations.

En 1942, la "Federal Reserve Bank" de New York fut le lieu des réunions secrètes entre les états-majors britannique, soviétique et américain, afin de mener la guerre contre l'Allemagne nazie et l'Italie. En mai 1943, le Premier ministre britannique Winston Churchill et le Président américain Roosevelt s'y sont retrouvés.

Charles August Lindbergh, un représentant du Minnesota à la Chambre des représentants des États-Unis, s'opposa au système de la Réserve fédérale dès sa création en 1913 dans un essai qu'il publia la même année.
Le , Louis Thomas McFadden, un représentant de Pennsylvanie, fit un discours de 25 minutes devant la Chambre des représentants des États-Unis, durant lequel il accusa la Réserve fédérale d'avoir délibérément provoqué la Grande Dépression. Le 15 juin 2007, Ron Paul, un représentant du Texas à la Chambre des représentants des États-Unis, a déposé un amendement visant à supprimer la Réserve fédérale.
En 2014 les Vigiles pour la paix focalisent leurs critiques durant leurs manifestations sur la Réserve Fédérale américaine et ses effets présumés sur la politique mondiale la décrivant comme une menace potentielle de Troisième Guerre mondiale à la suite de la crise ukrainienne de 2013-2014.

Avec "Les secrets de la Réserve fédérale" dont la première version fut publiée en 1952, Eustace Mullins dénonça la création de la Réserve Fédérale comme un complot pour asseoir l'économie et la politique américaine dans le giron de la finance internationale.
Nelson Aldrich aurait dévoyé la mission de la "National Monetary Commission" chargée des intérêts économiques du peuple pour assoir ceux du capitalisme financier. La dénonciation du subterfuge, par Charles August Lindbergh notamment, fragilisait le projet en cas de réélection du président Taft. Les grands banquiers auraient alors sabordé ses chances en amenant la candidature concurrente de Théodore Roosevelt. Cette élection tripartite assura la victoire du candidat démocrate Woodrow Wilson porteur d'une fausse alternative fondée sur une relative autonomie des banques régionales. En effet, les interdépendances actionnariales entre les représentants de ces dernières et les actionnaires privés de la FED assureraient leur assujettissement. De plus, le système de nomination des gouverneurs serait en trompe-l'œil assurant dans les faits une prédominance de la finance sur le monde politique.

G. Edward Griffin dans son livre qui fut un bestseller populaire ainsi que dans les milieux des affaires "The Creature from Jekyll Island: A Second Look at the Federal Reserve" et qui en est à sa sixième édition, va dénoncer « les banques privées qui possèdent effectivement la Réserve fédérale ».

Dans son livre publié en 1995 intitulé "Le complot de la réserve fédérale" ("Federal Reserve Conspiracy"), le professeur Antony Cyril Sutton développe une thèse selon laquelle la Réserve fédérale américaine ne serait qu'une institution contrôlée par des banquiers privés afin de s'arroger le droit exclusif de battre monnaie aux États-Unis. Sutton y expose aussi le grand danger que représente une telle organisation pour une nation qui se voudrait démocratique et un peuple qui se croirait souverain.

Plusieurs films développent cette exposition des faits, défendue également par Ron Paul, le représentant du du Texas, candidat à l'élection présidentielle en 2012 : "The Fiat Empire", "" et "The Biggest Scam In The History Of Mankind".






</doc>
<doc id="4910" url="https://fr.wikipedia.org/wiki?curid=4910" title="Alan Greenspan">
Alan Greenspan

Alan Greenspan, né le à New York, est un économiste américain. Il fut président de la Réserve fédérale, la banque centrale des États-Unis, du au .

Spécialiste de la politique monétaire intérieure des États-Unis, sa gestion du krach d'octobre 1987 et de l’inflation pendant son mandat est reconnue. Louant sa grande expérience, les médias l'ont appelé « l'économiste des économistes », ou le « Maestro », au regard de l', qu'il a favorisé. Il a cependant été mis en cause lors de la crise des subprimes pour avoir laissé fortement augmenter la masse monétaire à la fin de son mandat, par une politique de taux d'intérêts très bas suivie d'un redressement important des taux directeurs, et pour avoir ignoré des mises en garde concernant le marché des "subprimes" émanant du conseil de la Réserve fédérale, notamment d'Edward Gramlich.

Alan Greenspan a grandi à Manhattan (New York) dans le quartier de Washington Heights. Fils unique né en 1926 au sein d'une famille juive, ses parents divorcent peu après sa naissance. Sa mère l'élève seul, travaillant comme vendeuse dans un magasin de meuble. Son père est agent de change (courtier en bourse) à Wall Street et écrit un livre en 1935, "La croissance revient", qu'il lui dédie.

En 1943, diplômé du "George Washington High School", il échappe à la conscription pour des raisons médicales et, repoussant l'entrée à l'université, passe deux années comme musicien professionnel dans le groupe de Henry Jérôme.

À l'automne 1945, il entre à l'école de commerce, de comptabilité et de finance de l'université de New York. Il obtient sa licence d'économie au printemps 1948. Au cours de ses études, il est amené à travailler pour joindre les deux bouts dans différentes activités, dont un poste au "National Industrial Conference Board". Il obtint son master d'économie en 1950 et continua ses études comme doctorant à l'université Columbia sous la houlette d'. En 1953, il s'associa et fonda le cabinet de conseil et d'analyse économique Townsend-Greenspan mettant de côté sa thèse
"Les Habitudes des ménages américains en matière de dépense et d'épargne" qu'il était sur le point d'achever.

En 1967, il s'implique dans la vie publique en intégrant l'équipe de campagne de Richard Nixon, candidat républicain qui devient président des États-Unis en 1969. Prenant quelques distances avec le nouveau gouvernement, il accepta néanmoins la présidence du Council of Economic Advisers (CEA), où il prit ses fonctions le 8 août 1974, la veille de la démission de Richard Nixon à la suite du scandale du Watergate. Il tiendra ce poste jusqu'à la fin du mandat présidentiel de Gerald Ford en 1977, puis retourna diriger sa société de conseil.

En 1977, il soutient sa thèse à l'université de New York et obtient son doctorat en sciences économiques.

En 1979, il entre dans l'équipe de campagne présidentielle de Ronald Reagan, qui devient président en 1981 et sera réélu en 1984.

Le 11 août 1987, il est nommé du conseil de la Réserve fédérale des États-Unis par Ronald Reagan, remplaçant Paul Volcker. Deux mois seulement après son arrivée à la tête de cette institution, il doit faire face au krach d'octobre 1987. Il sera ensuite confirmé à son poste par les présidents suivants George H. W. Bush, républicain, et Bill Clinton, démocrate. En 2004, George W. Bush le renomme pour servir un et dernier mandat. Il cède son poste le février 2006 à Ben Bernanke.

En 1999, il abroge avec Robert Rubin et Larry Summers le Glass-Steagall Act, qui maintenait la séparation entre les métiers de banque de dépôt et de banque d'investissement, permettant aux banques « de faire tout et n'importe quoi ». Il promeut les opérations "Over The Counter" (OTC) et les produits dérivés qui accéléreront considérablement la crise financière mondiale débutant en 2007.

Il a été un ami de Ayn Rand et se définit républicain libertarien.

Milton Friedman, père du monétarisme, le considérait comme le meilleur gouverneur de la Fed.

Alan Greenspan est commandeur de la Légion d'honneur. Il est marié depuis 1997 à la journaliste de la NBC Andrea Mitchell.

Il est un ancien membre du Bohemian Club. Il a participé à la conférence Bilderberg de 2002.

À la suite de la crise des subprimes, ayant à s'expliquer devant le Congrès le , il reconnait publiquement que son système consistant à faire du marché libre le meilleur moyen d'organiser l'économie était en fait faillible. Greenspan déclare également avoir été dépassé par les technologies d'automatisation des marchés financiers.



Charles Ferguson, "Inside Job", 2010


</doc>
<doc id="4916" url="https://fr.wikipedia.org/wiki?curid=4916" title="Mercure (planète)">
Mercure (planète)

Mercure est la planète la plus proche du Soleil et la moins massive du Système solaire. Son éloignement au Soleil est compris entre 0,31 et (46 et 70 millions de kilomètres), ce qui correspond à une excentricité orbitale de 0,2 . Elle est visible à l'œil nu depuis la Terre avec un diamètre apparent de 4,5 à , et une magnitude apparente de son observation est toutefois rendue difficile par son élongation toujours inférieure à 28,3° qui la noie le plus souvent dans l'éclat du Soleil.

Mercure a la particularité d'être en résonance 3:2 sur son orbite, sa période de révolution () valant exactement sa période de rotation (), et donc la moitié d'un jour solaire (). L'inclinaison de l'axe de rotation de Mercure sur son plan orbital est la plus faible du système solaire, à peine . Son périhélie connaît une précession autour du Soleil plus rapide que celle prédite par la mécanique newtonienne, une avance de par siècle qui n'a pu être complètement expliquée que dans le cadre de la relativité générale.

Mercure est une planète tellurique, comme le sont également Vénus, la Terre et Mars. Elle est près de trois fois plus petite et presque vingt fois moins massive que la Terre mais presque aussi dense qu'elle, avec une gravité de surface pratiquement égale à celle de Mars, qui est pourtant près de deux fois plus massive. Sa densité remarquable est due à l'importance de son noyau métallique, qui occuperait plus de 40 % de son volume, contre seulement 17 % pour la Terre.

Comme Vénus, Mercure est quasiment sphérique en raison de sa rotation très lente. Dépourvue de véritable atmosphère, sa surface est très fortement cratérisée, et globalement similaire à la face cachée de la Lune. Seules deux sondes spatiales ont étudié Mercure. Mariner 10, qui a survolé à trois reprises la planète en 1974–1975, a cartographié 45 % de sa surface et découvert son champ magnétique. La sonde Messenger, après trois survols en 2008-2009, s'est mise en orbite autour de Mercure en mars 2011 et a entamé une étude détaillée notamment de sa topographie, son histoire géologique, son champ magnétique et son exosphère.

La quasi-absence d'atmosphère combinée à la proximité du Soleil engendre des températures en surface allant de () au fond des cratères polaires (là où les rayons du Soleil ne parviennent jamais) jusqu'à () au point subsolaire au périhélie.

La planète Mercure doit son nom au dieu Mercure du commerce et des voyages, également messager des autres dieux dans la mythologie romaine. La planète a été nommée ainsi par les Romains à cause de la vitesse à laquelle elle se déplaçait. Le symbole astronomique de Mercure est un cercle posé sur une croix et portant un demi-cercle en forme de cornes (Unicode : ☿). C'est une représentation du caducée du dieu Hermès. Mercure laissa également son nom au troisième jour de la semaine, mercredi (« Mercurii dies »).

Mercure a une excentricité orbitale qui fait varier sa distance au Soleil de 46 à 70 millions de kilomètres.

L'excentricité de l'orbite de Mercure varie de manière chaotique de 0 (orbite circulaire) à une valeur très importante de 0,46 sur plusieurs millions d'années.

En 1989, Jacques Laskar, du Bureau des longitudes, a démontré que les planètes intérieures du système solaire avaient toutes des courses chaotiques. Cependant Mercure est celle dont le mouvement est le plus chaotique.

Comme pour l'ensemble des planètes du système solaire, l'orbite de Mercure connaît une très lente précession du périhélie autour du Soleil. Cependant, contrairement aux autres planètes, la période de précession du périhélie de Mercure ne concorde pas avec les prédictions faites à l'aide de la mécanique newtonienne.

En effet, Mercure connaît une précession légèrement plus rapide que celle à laquelle on peut s'attendre en appliquant les lois de la mécanique céleste, et se trouve en avance d'environ 43 secondes d'arc par siècle.

Les astronomes ont donc, dans un premier temps, pensé à la présence d'un ou plusieurs corps entre le Soleil et l'orbite de Mercure dont l'interaction gravitationnelle perturberait le mouvement de cette dernière. L'astronome français Urbain Le Verrier se pencha sur le problème et suggéra la présence d'une seconde ceinture d'astéroïdes entre le Soleil et Mercure. Des calculs effectués, en prenant en compte l'influence gravitationnelle de ces corps, devaient alors concorder avec la précession observée.

Le , Le Verrier fut contacté par le médecin français Lescarbault à propos d'une tache noire qu'il aurait vu passer devant le Soleil deux jours avant et qui était probablement, d'après lui, une planète intramercurienne. Le Verrier postula alors que cette planète était responsable des anomalies du mouvement de Mercure et se mit en tête de la découvrir. À partir des informations de Lescarbault, il conclut que Vulcain tournait autour du Soleil en 19 jours et 7 heures à une distance moyenne de . Il en déduisit également un diamètre d'environ et une masse de 1/ de celle de Mercure. Cette masse était cependant bien trop faible pour expliquer les anomalies, mais Vulcain était une bonne candidate au corps le plus gros de cette hypothétique ceinture d'astéroïdes interne à Mercure.

Le Verrier profita alors de l'éclipse de Soleil de 1860 pour mobiliser tous les astronomes français afin de repérer Vulcain, mais personne ne put la trouver. Le Verrier resta cependant confiant après que le professeur Wolf, du Centre de données des taches solaires à Zurich, eut observé sur le Soleil deux douzaines de taches suspectes. La planète fut recherchée pendant des décennies. Certains astronomes attestèrent l'avoir vue passer devant le Soleil. Parfois plusieurs dirent l'avoir repérée, mais à des endroits différents.

Finalement, l'énigme fut résolue en 1916 avec la théorie de la relativité générale d'Albert Einstein. En appliquant la relativité générale au mouvement de Mercure, on en arrive à la précession mesurée.

Le transit de Mercure peut se produire en mai ou en novembre lorsque Mercure se situe entre la Terre et le Soleil.

Les transits de Mercure vus depuis la Terre sont beaucoup plus fréquents que ceux de Vénus, avec une fréquence d'environ 13 ou 14 par siècle, en raison de la proximité de la planète au Soleil, qui implique une période de révolution plus courte que celle de Vénus. Ils peuvent se produire en mai à des intervalles de 13 ou 33 ans, ou en novembre tous les 7, 13 ou 33 ans. Les trois derniers transits de Mercure datent de 2003, 2006 et 2016 ; le prochain se produira en 2019.

Alors qu'il étudiait Mercure afin d'en dresser une première carte, Schiaparelli avait remarqué après plusieurs années d'observation que la planète présentait toujours la même face au Soleil, comme la Lune le fait avec la Terre. Il en conclut alors en 1889 que Mercure était synchronisée par effet de marée avec le Soleil et que sa période de rotation équivalait à une année mercurienne, soit 88 jours terrestres. Cette durée était cependant erronée et il fallut attendre les années 1960 avant que les astronomes ne la revoient à la baisse.

En 1962, des observations par radar à effet Doppler ont été effectuées par le radiotélescope d'Arecibo sur Mercure afin d'en apprendre plus sur la planète et de vérifier si la période de rotation était bien égale à la période de révolution. Les températures relevées du côté de la planète censé être toujours exposé à l'ombre étaient trop importantes, ce qui suggéra que cette face sombre était parfois exposée au Soleil. En 1965, les résultats obtenus par Gordon H. Pettengill et Rolf B. Dyce révèlent que la période de rotation de Mercure est en fait de 59 jours terrestres, avec une incertitude de 5 jours. Cette période sera ajustée plus tard, en 1971, à 58,65 jours à jours grâce à des mesures plus précises effectuées par R.M. Goldstein. Trois ans plus tard, la sonde Mariner 10 apportera une meilleure précision, mesurant la période de rotation à jours. Il se trouve que cette période est exactement égale aux 2/3 de la révolution de Mercure autour du Soleil ; ce qu'on appelle une résonance spin-orbite 3:2.

En comparaison avec la Terre, Mercure tourne 59 fois moins vite sur elle-même. Mercure ne présente donc pas toujours la même face au Soleil. L'erreur de Schiaparelli était due au fait que la période de rotation réelle de Mercure est presque exactement la moitié de la période synodique de Mercure (c'est-à-dire le temps mis par Mercure pour revenir à la même configuration Terre–Mercure–Soleil) par rapport à la Terre.

Pour garder une telle période de rotation en étant aussi proche du Soleil, Mercure dispose d'une orbite elliptique inclinée de 3,4° (par rapport à l'équateur solaire), ainsi qu'une forte excentricité ce qui en fait la planète la plus excentrique. La raison pour laquelle les astronomes pensaient que Mercure était verrouillée avec le Soleil est qu'à chaque fois que Mercure était la mieux placée pour être observée, elle se trouvait toujours au même point sur son orbite (en résonance 3:2), présentant ainsi la même face à chaque fois ; ce qui serait aussi le cas si elle était totalement synchronisée avec le Soleil. Cette erreur peut être imputée à la difficulté d'observation de la planète avec les moyens de l'époque.

En raison de sa résonance 3:2, bien qu'un jour sidéral (la période de rotation) dure environ 58,7 jours terrestres, le jour solaire (durée entre deux retours successifs du Soleil au méridien local) dure 176 jours terrestres, c'est-à-dire deux années mercuriennes. Ce qui fait qu'une journée, ainsi qu'une nuit, sur Mercure valent exactement une année chacune, soit 88 jours terrestres (presque un trimestre).

Il en résulte une journée mercurienne plutôt « étrange » pour un observateur qui serait situé à la surface de Mercure. À certains endroits, celui-ci verra le Soleil se lever puis se recoucher, puis se relever à l'horizon Est ; et à la fin de la journée à l'Ouest, le Soleil se couchera puis se relèvera, pour se recoucher. Ce phénomène s'explique par la variation de la vitesse orbitale de Mercure. Quatre jours avant le périhélie, la vitesse (angulaire) orbitale de Mercure est exactement égale à sa vitesse (angulaire) de rotation ; le mouvement du Soleil semble s'arrêter. Puis aux alentours du périhélie, la vitesse orbitale de Mercure excède sa vitesse de rotation et le Soleil semble alors avoir un mouvement rétrograde ; il apparaît retourner là d'où il vient, traversant le ciel d'ouest en est, durant environ quatre jours, avant de reprendre un mouvement apparent normal, c'est-à-dire se déplaçant d'est en ouest.

L'atmosphère de Mercure est quasi inexistante ; on n'en décèle que quelques traces. Elle est extrêmement ténue à cause de la température très élevée de la surface et de la faible gravité de la planète, à tel point que les molécules de gaz de l'« atmosphère » entrent plus souvent en collision avec la surface de la planète qu'avec d'autres molécules de gaz. Il est d'ailleurs plus approprié de parler de l"'exosphère", commençant dès la surface de Mercure, directement « ouverte » sur l'espace, que de son « atmosphère ». Dans la plupart des cas, on peut la négliger et considérer Mercure comme privée d'atmosphère.

Cette exosphère est principalement composée de potassium (31 %), de sodium (25 %) et d'oxygène (9,5 %). On y trouve aussi des traces d'argon, de néon, d'hydrogène et d'hélium.

Mariner 10 mit en évidence une ionosphère d'au plus un cent-millième de celle de la Terre.

Le vent solaire et le dégazage du sol expliquent cette exosphère transitoire, d'une très faible pression de et la variation considérable de la composition de l'atmosphère.

Les atomes composant principalement l'exosphère de Mercure (potassium et sodium) ont une durée de vie (de présence) estimée à trois heures avant d'être libérés dans l'espace et une heure et demie lorsque la planète est au périhélie c'est-à-dire au plus proche du Soleil. Les atomes se renouvellent constamment puisque des ions provenant du vent solaire sont capturés par la magnétosphère de la planète. De plus, les météorites qui tombent à la surface aident à l'approvisionnement en atomes de sodium et de potassium.

Le sodium et le potassium, ainsi que l'argon et une bonne part du néon proviennent du dégazage résiduel des roches, alors que l'hydrogène et l'hélium proviennent principalement de la capture des ions du vent solaire par la magnétosphère de Mercure.

Les impacts météoritiques (résiduels), comprenant les poussières du nuage zodiacal, qui éjectent des particules arrachées à la surface de la planète, contribuent aussi à la formation de cette infime « atmosphère ». Ces météorites apportent elles-mêmes de la matière et pourraient d'ailleurs être la source principale du potassium et du sodium détectés dans cette exosphère.

Mercure est une planète très chaude. La température moyenne en surface est de ().
C'est la température de stabilisation en dessous du régolite, où le sous-sol n'est plus soumis à l'alternance des « ondes » thermiques de la journée et de la nuit (égales à l'année mercurienne).

Dans l'hémisphère nocturne la température se stabilise vers () à la surface, mais elle monte jusqu'à () dans l'hémisphère diurne, lorsque le soleil se trouve aux alentours du zénith.

Du fait de l'inclinaison quasi nulle de son axe de rotation, les zones polaires de Mercure ne reçoivent les rayons solaires que rasants, ce qui doit entraîner une température d'équilibre en sous-sol bien inférieure à celles des latitudes plus basses, et induire des températures de surface inférieures à () dans le fond des cratères polaires, où la lumière solaire ne pénètre jamais. De la glace pourrait y être conservée, car à ces températures elle ne se sublime quasiment plus (la pression partielle de vapeur de la glace est très basse). .

Depuis la surface de Mercure le Soleil apparaît, en fonction de l'orbite elliptique, entre 2,1 et 3,3 plus gros que depuis la Terre, et sa lumière est environ 7 fois plus intense, avec un flux de rayonnement solaire de .

La surface de Mercure est couverte d'un tapis poussiéreux de minéraux (silicates), de cassures et de cratères. La planète ressemble beaucoup en apparence à la Lune, ne présentant "a priori" aucun signe d'activité interne. Pour les astronomes, ces cratères sont très anciens et racontent l'histoire de la formation du système solaire, lorsque les planétésimaux entraient en collision avec les jeunes planètes pour fusionner avec elles. Par opposition, certaines portions de la surface de Mercure semblent lisses, vierges de tout impact. Il s'agit probablement de coulées de lave recouvrant un sol plus ancien et plus marqué par les impacts. La lave, une fois refroidie, donnerait lieu à l'apparition d'une surface lisse, blanchâtre. Ces plaines datent d'une époque plus récente, postérieure à la période de bombardements intenses. La découverte des plaines volcaniques sur la surface permet de mettre en cause des chutes d'énormes astéroïdes atteignant le manteau, et pouvant créer en même temps des éruptions volcaniques à l'opposé de la planète.

Le plus remarquable de ces cratères (du moins, sur la portion qui a pu être photographiée) est le bassin Caloris, un impact météoritique d'un diamètre d'environ , qui fut formé après la chute d'un astéroïde d'une taille avoisinant les ; il y a près de 3,85 milliards d'années. Son nom (« Caloris », chaleur en latin) vient du fait qu'il est situé sur l'un des deux « pôles chauds » de la surface de Mercure, pôle qui fait directement face au Soleil lorsque la planète est au plus proche de celui-ci. Un cratère d'un diamètre supérieur à , appelé « bassin Caloris ». Il s'agit d'une grande dépression circulaire, avec des anneaux concentriques. Plus tard, de la lave a certainement coulé dans ce grand cratère, et en a lissé la surface. Seule la partie est a pu être photographiée par la sonde Mariner 10, la partie ouest étant plongée dans l'ombre au moment du survol de Mercure. L'impact aurait « creusé » jusqu'au manteau planétaire de Mercure, créé des ondes de choc qui ont perturbé et traversé le noyau, puis causé des plissements à l'opposé de ce cratère sur la planète.

À l'opposé du bassin Caloris se trouve une région très accidentée, de la taille de la France et de l'Allemagne réunies, formée de blocs rocheux désordonnés. Les scientifiques pensent que ces fractures sont le résultat du choc qui produisit le bassin Caloris. Les ondes de choc produites par l'impact météoritique ont déformé la face opposée, soulevant le sol à une hauteur de à , et déformant la surface de Mercure, produisant cette région chaotique.

Par ailleurs, les photographies prises par Mariner 10 révèlent la présence d'escarpements lobés dus à une contraction de la planète lors de son refroidissement. Ce refroidissement entraîna une diminution du rayon de la planète d'environ , produisant des cassures dans la croûte, pour former des crêtes et des plis. Ces escarpements traversent les cratères, les montagnes et les vallées, et peuvent atteindre une longueur de . Certaines crêtes atteignent des hauteurs d'environ . L'ancienneté de ces escarpements montre que la planète n'a pas connu d'activité tectonique depuis son jeune âge.

Les différentes caractéristiques de la surface de Mercure sont :


L'évolution géologique de Mercure peut être divisée en cinq grandes périodes ou époques.


Il y a peu de chance pour que Mercure ait possédé une atmosphère initiale (juste après l'accumulation de matière), ou alors elle se serait évaporée très tôt avant l'apparition des plus anciens cratères. Si Mercure avait eu une atmosphère, on aurait pu remarquer une érosion des cratères par les vents, comme sur Mars. Les escarpements présents majoritairement dans les régions « inter-cratères » (qui sont des surfaces plus anciennes que les cratères) et qui traversent parfois certains des plus vieux cratères, montrent que le refroidissement du noyau et la contraction de la planète se sont produits entre la fin de la première période et le début de la seconde.


Il existe une autre chronologie semblable, découpée en cinq époques également : Pré-Tolstoïen (de la formation au refroidissement du noyau ), Tolstoïen (formation du bassin Tolstoj et des plaines lisses par coulées de lave ), Calorien (impact du bassin Caloris et fin du bombardement intense ), Mansurien (3,5 à 1 milliard d'années) et Kuiperien (depuis 1 milliard d'années à aujourd'hui). Ici, la période de volcanisme qui produisit une partie des plaines lisses est placée avant l'impact Caloris.

Des observations radar effectuées en 1991 à partir du radiotélescope d'Arecibo et de l'antenne de Goldstone indiquent la présence de glace d'eau aux pôles Nord et Sud de Mercure. Celle-ci est caractérisée par des zones à réflexion radar élevée et une signature fortement dépolarisée, contrairement à la réflexion radar typique du silicate, constituant la majeure partie de la surface de Mercure. Une vingtaine d'anomalies de ce type ont été observées.

Les résultats obtenus avec le radiotélescope d'Arecibo montrent que ces réflexions radar sont concentrées dans des taches circulaires de la taille d'un cratère. D'après les images prises par Mariner 10, la plus grosse d'entre elles, au pôle Sud, semble coïncider avec le cratère Chao Meng-Fu. D'autres, plus petites, correspondent également à des cratères bien identifiés. La corrélation est cependant impossible à faire pour le pôle Nord, puisque non cartographié. On pense que de la glace repose au fond de ces cratères.

La présence de glace sur Mercure peut surprendre ; les températures régnant à la surface de la planète peuvent atteindre , notamment aux « pôles chauds » — régions les plus exposées au Soleil, lorsque Mercure est au plus proche de celui-ci. Cependant, certains cratères présents aux pôles peuvent ne jamais être exposés aux rayons du Soleil, et donc plongés dans une obscurité permanente. Des études ont montré que la température au niveau de ces cratères n'excède pas (). Certaines surfaces plates, même au niveau des pôles toujours, seraient soumises à des températures inférieures à (). Exposée à de telles conditions, l'eau peut rester sous forme de glace.

Deux sources probables de glace sont envisagées : le bombardement météoritique et le dégazage du sol. Les météorites frappant la planète ont pu apporter de l'eau qui serait restée piégée (gelée par les basses températures des pôles) aux endroits où se sont produits les impacts. De même pour les dégazages, certaines molécules ont pu migrer vers les pôles et s'y retrouver piégées.

La présence de plaines plus jeunes (les plaines lisses) est la preuve que Mercure a connu dans son passé une activité volcanique. L'origine de ces plaines a été mise en évidence à la fin des années 1990 par Mark Robinson et Paul Lucey en étudiant les photographies de Mercure. Le principe était de comparer les surfaces lisses avec les autres, non lisses (et plus anciennes). S'il s'agissait bien d'éruptions volcaniques, ces régions devaient être d'une composition différente de celle qu'elles recouvraient, puisque composées de matériaux venant de l'intérieur de la planète.

Les images prises par Mariner 10 ont d'abord été recalibrées à partir d'images prises en laboratoire avant le lancement de la sonde, et d'images prises durant la mission des nuages de Vénus (Vénus présente une texture plutôt uniforme) et de l'espace profond. Robinson et Lucey ont ensuite étudié divers échantillons de la Lune notamment la réflexion de la lumière afin de faire un parallèle entre la composition et la réflexion de ces matériaux.

À l'aide de techniques avancées de traitement d'images numériques (qui n'étaient pas possibles à l'époque de la mission Mariner 10), ils ont appliqué un code de couleurs aux images afin de différencier les matériaux minéraux sombres des matériaux métalliques. Trois couleurs ont été utilisées : le rouge pour caractériser les minéraux opaques, sombres (plus le rouge est prononcé, moins il y a de minéraux sombres) ; le vert pour caractériser à la fois la concentration d'oxyde de fer (FeO) et l'intensité du bombardement de micrométéorites, également appelé « maturité » (la présence de FeO est moins importante, ou la région est moins mature, sur les portions plus vertes) ; le bleu pour caractériser le rapport UV/lumière visible (l'intensité de bleu augmente avec le rapport). La combinaison des trois images donne des couleurs intermédiaires. Par exemple, une zone en jaune peut représenter une combinaison d'une forte concentration en minéraux opaques (rouge) et une maturité intermédiaire (vert).

Robinson et Lucey ont alors remarqué que les plaines étaient marquées de couleurs différentes par rapport aux cratères et ont pu en déduire que ces plaines étaient de composition différente par rapport aux surfaces plus anciennes (caractérisées par la présence de cratères). Ces plaines ont dû, à l'instar de la Lune, être formées par des coulées de lave. De nouvelles questions se posent alors quant à la nature de ces remontées de roche en fusion : s'agit-il de simples épanchements fluides, ou d'éruptions explosives ? Cependant, toutes les plaines n'ont peut-être pas pour origine des coulées de lave. Il est possible que certaines se soient formées à partir de retombées de poussières et de fragments du sol, éjectés lors de gros impacts météoritiques.

Certaines éruptions volcaniques ont pu se produire à la suite de grosses collisions. Dans le cas du Bassin Caloris, le cratère généré par l'impact devait avoir à l'origine une profondeur de , atteignant probablement le manteau qui a dû entrer partiellement en fusion lors du choc (pression et température très importantes). Le manteau est ensuite remonté lors du réajustement du sol, comblant le cratère.

Ainsi, sachant qu'une partie de la surface de Mercure provient de son intérieur, les scientifiques ont pu en apprendre plus sur la composition interne de la planète.

La planète possède un noyau métallique correspondant à un minimum de 55 % de la masse de la planète, proportionnellement plus gros que celui de la Terre (environ 33 % en masse pour la Terre). Des recherches récentes suggèrent que ce noyau est liquide (fer et du nickel), au moins dans sa partie externe. La composition interne de la planète est de 70 % de métaux (principalement dans le noyau) et 30 % de silicates (manteau). La masse volumique moyenne est de , ce qui est comparable à la masse volumique terrestre (). À partir d'observations depuis la Terre, les astronomes savaient avant même d'envoyer Mariner 10 que Mercure était à peu près aussi dense que la Terre. En revanche, ils ne s'attendaient pas à ce que la croûte de Mercure soit d'une si faible densité, d'après les mesures effectuées par la sonde américaine. Ces résultats indiquent que Mercure possède un énorme noyau métallique occupant 42 % du volume planétaire, avec un rayon de 75 % de celui de la planète. En comparaison, le noyau de la Terre, lui, ne remplit que 17 % de son volume. Ceci implique que Mercure possède une quantité de fer deux fois plus importante que tout autre objet du Système solaire. C'est la raison pour laquelle on la surnomme parfois « la planète métallique » ou « la planète de fer ».

La raison pour laquelle Mercure possède un noyau si gros est encore inconnue et l'un des objectifs principaux des prochaines missions vers Mercure est d'étudier et comprendre la structure interne de la planète. Une réponse qui pourra nous en apprendre beaucoup sur la formation du système solaire.

Cet énorme noyau est recouvert d'un manteau de silicate d'une épaisseur de 500 à , puis d'une croûte. L'étude du spectre de la planète montre que la surface semble pauvre en métaux, ce qui intrigue les scientifiques. Sur Terre, le fer est abondant en surface. Cet élément est même présent dans chaque couche de la planète. Mercure a dû connaître un processus différent lors de sa formation.

Selon des données plus récentes, Mercure aurait une couche externe solide de composée d'une croûte d'une cinquantaine de kilomètres surplombant une couche de sulfure de fer cristallisé. En dessous se trouverait le noyau ferreux, liquide au moins dans sa partie externe et probablement solide au cœur.

Du fait de son important noyau ferreux et de son importante densité, Mercure est une planète très massive pour sa petite taille. Par comparaison, Ganymède, un satellite de Jupiter, est légèrement plus grande que Mercure pour une masse deux fois moindre.

La proportion en fer de Mercure — proportion plus importante que celle de tout autre objet du système solaire — intrigue toujours les astronomes. La réponse à cette question permettrait certainement d'en apprendre beaucoup sur la nébuleuse solaire primitive et les conditions dans lesquelles le système solaire s'est formé. Trois hypothèses ont été proposées pour expliquer la haute métallicité de Mercure et son noyau gigantesque.

L'une d'entre elles suggère que Mercure avait à l'origine un rapport métal–silicate semblable à celui des chondrites et une masse d'environ 2,25 fois la masse courante. Tôt dans l'histoire du système solaire, Mercure aurait été frappée par un planétésimal d'environ 1/ de cette masse. L'impact aurait arraché à la planète une grande partie de sa croûte (la croûte terrestre est la partie superficielle et solide du matériau dont est faite Mercure) et de son manteau, ne laissant derrière que le noyau (métallique), qui aurait fusionné avec celui du planétésimal, et un mince manteau. Une hypothèse assez semblable au scénario (très probable) retenu pour expliquer la formation de la Lune.

Une seconde hypothèse propose que le taux d'éléments lourds, comme le fer, présents dans la nébuleuse solaire était plus important au voisinage du Soleil, voire que ces éléments lourds étaient distribués graduellement autour du Soleil (plus on s'en éloignait, moins il y avait d'éléments lourds). Mercure, proche du Soleil, aurait donc amassé plus de matériaux lourds que les autres planètes pour former son noyau.

Alternativement, Mercure aurait pu s'être formée très tôt dans l'histoire, avant même que l'énergie dégagée par le Soleil ne se soit stabilisée. Mercure aurait eu à sa formation le double de sa masse courante, mais à mesure que la proto-étoile se contractait, la température aux alentours de Mercure augmentait et aurait pu atteindre 2500–3500 K, voire . À de telles températures, une grande partie de la surface de Mercure aurait été vaporisée, formant une atmosphère de « vapeurs rocheuses », qui aurait été ensuite transportée ailleurs par les vents solaires. Une hypothèse similaire propose que les couches extérieures de Mercure aient été érodées par les vents solaires, durant une plus longue période.

Révélée par la sonde Mariner 10 lors de sa première approche, en mars 1974, la présence d'un champ magnétique faible surprit les astronomes qui pensaient jusque-là que Mercure était dépourvue de toute magnétosphère. Vingt minutes avant de survoler la planète au plus près, les magnétomètres de Mariner 10 détectèrent l'onde de choc d'étrave produite par la collision de ce champ avec celui du Soleil et le mesurèrent à , ce qui représente un peu moins d'un soixantième du champ magnétique terrestre.

La source du champ magnétique, encore incertaine, a été déterminée durant un second passage de Mariner 10 comme étant intrinsèque à Mercure, et non pas provoquée par l'action des vents solaires. Avant la mission Mariner 10, les astronomes ne pensaient pas que Mercure possédât un champ magnétique du fait de sa vitesse de rotation et de sa petite taille . Il fallut donc admettre que ce noyau est partiellement fondu et connaît des mouvements de convection qui seraient à l'origine de ce champ. Néanmoins, les estimations récentes suggèrent que le noyau de Mercure n'est pas assez chaud pour que le fer–nickel soit présent sous forme liquide. En revanche, il est possible que d'autres éléments soient présents, comme le soufre ; qui se sont concentrés dans la phase liquide résiduelle, abaissant ainsi le point de fusion du mélange. Il se peut également que le champ magnétique de Mercure soit le reste d'un ancien effet dynamo qui a maintenant cessé, devenu « figé » dans les matériaux magnétiques solidifiés du noyau (la période de rotation ayant pu être beaucoup plus courte par le passé).

Par ailleurs, le champ magnétique de Mercure est une version réduite du champ magnétique terrestre. Norman Ness, qui était alors chargé de l'étude du champ magnétique, put prédire les moments précis où la sonde traverserait l'onde de choc, la magnétopause ainsi que la zone où le champ est maximal. Ces prédictions concordaient avec les mesures relevées par les instruments de Mariner 10. Des expériences menées par la sonde ont montré que, tout comme celle de la Terre, la magnétosphère de Mercure possède une queue séparée en deux par une couche neutre.

Mercure peut être observée à l'œil nu : sa magnitude apparente varie entre -2,4 et 5,5. Mais la planète ne s'éloigne jamais de plus de 27° du Soleil, et sa visibilité se réduit à de courtes périodes de l'année, soit avant le lever du Soleil, soit après son coucher. Les observateurs de l'hémisphère nord sont désavantagés par un autre phénomène : les élongations maximales les plus favorables à l'ouest sont en mars, à l'est en septembre, aux moments où l'écliptique est le moins incliné au crépuscule.

Mercure est connue depuis que les hommes s'intéressent au ciel nocturne ; la première civilisation à en avoir laissé des traces écrites est la civilisation sumérienne () qui la nommait « "Ubu-idim-gud-ud" », mais elle était probablement connue depuis bien avant.

Les premiers écrits d'observations détaillées de Mercure nous viennent des Babyloniens. Les Babyloniens donnaient à cet astre, qu'ils associaient au dieu Nabû du savoir dans la mythologie mésopotamienne, le nom de « "gu-ad" » ou « "gu-utu" ». Ils sont également les premiers à avoir étudié le mouvement apparent de Mercure, qui est différent de celui des autres planètes.

Plus tard, dans l'Antiquité, les Grecs considérèrent jusqu'au que Mercure visible avant le lever du Soleil d'une part et Mercure visible après son coucher d'autre part relevaient de deux astres distincts, appelés respectivement du nom des dieux Apollon (Ἀπόλλων) et Hermès (Ἑρμῆς) de la mythologie grecque ; bien que Pythagore aurait « démontré » qu'il s'agissait du même astre. Les Égyptiens procédèrent de même en donnant le nom de Sat à l'étoile du soir et Horus à celle du matin.

Le nom « Mercure » est issu de la mythologie romaine. Mercure, ou Hermès dans la mythologie grecque, est le messager des dieux ainsi que le dieu protecteur des commerçants, des médecins et des voleurs. L'association de la planète au dieu Mercure semble provenir du fait que la planète se déplace rapidement dans le ciel, rappelant la célérité de la divinité gréco-romaine. Auguste Wahlen écrit dans son Dictionnaire : 

Hygin qui, au , a compilé un grand nombre de sources grecques mentionne dans ses "Fables mythologiques" que la planète était aussi connue sous le nom de Stilbon (ou "Stilbos"), « celui qui brille ».

On raconte que sur son lit de mort Copernic regretta de ne jamais avoir pu observer Mercure à cause des brouillards qui s'élevaient sur la Vistule. Cette anecdote a peut-être été inventée mais elle montre qu'il est difficile d'observer cet astre.

La planète est visible au télescope sous forme d'un disque de 4,5 à 13" d'arc de diamètre, et présente des phases comme la Lune et Vénus.

L'observation de nuit (aube ou crépuscule) est difficile du fait de la grande épaisseur d'atmosphère à traverser, source de turbulences accrues qui rendent l'image floue. Les utilisateurs de l'hémisphère nord sont pénalisés par la configuration de l'orbite très désavantageuse.

En revanche, l'observation de jour permet d'obtenir des résultats intéressants, mais hors des éclipses totales de soleil elles nécessitent une grande prudence du fait de la proximité du Soleil.

Le premier astronome à avoir discerné des caractéristiques géologiques de Mercure était Johann Hieronymus Schröter qui, vers la fin du , dessina en détail ce qu'il avait pu observer, dont des montagnes pouvant atteindre de haut. Ses observations furent cependant infirmées par William Herschel qui ne put voir aucune de ces caractéristiques.

Par la suite, d'autres astronomes ont dressé des cartes de Mercure, dont l'italien Giovanni Schiaparelli et l'américain Percival Lowell (en 1896) qui y voyaient des zones sombres en formes de lignes, similaires aux canaux de Mars. Schiaparelli et Lowell avaient également esquissé des cartes de Mars en soutenant qu'il y avait des canaux artificiels.

La meilleure carte d'avant Mariner 10 provient du Franco-grec Eugène Antoniadi, au début des années 1930. Elle fut utilisée pendant près de 50 ans jusqu'à ce que Mariner 10 nous renvoie les premières photos de la planète. Antoniadi montra que les canaux n'étaient qu'une illusion d'optique. Il reconnut que l'élaboration d'une carte précise de Mercure était impossible à partir d'observations effectuées à l'aube ou au crépuscule, à cause des perturbations atmosphériques (l'épaisseur d'atmosphère terrestre que la lumière doit traverser lorsque Mercure se trouve à l'horizon est importante et crée des distorsions de l'image). Il entreprit alors de faire des observations — dangereuses — en plein jour lorsque le Soleil était bien au-dessus de l'horizon. Il gagna ainsi en netteté, mais perdit en contrastes à cause de la lumière du Soleil. Antoniadi parvint tout de même à achever sa carte en 1934, composée de plaines et de montagnes.

Les coordonnées utilisées sur ces cartes ont peu d'importance dans la mesure où elles ont été établies alors qu'on pensait, comme Schiaparelli l'avait affirmé, que la période de rotation de Mercure sur elle-même était la même que la période de révolution autour du Soleil. Il s'agit donc de la face supposée toujours illuminée qui a été cartographiée.

En 1974–75, Mariner 10 rapporta des photographies en haute résolution permettant la cartographie d'environ 40–45 % de sa surface, révélant les détails topographiques jamais vus auparavant : une surface recouverte de cratères avec des montagnes et des plaines, et très ressemblante à celle de la Lune. Il a d'ailleurs été assez difficile de faire une corrélation entre les caractéristiques photographiées par la sonde et les cartes établies par télescope. Certaines des manifestations géologiques de la carte d'Antoniadi se sont révélées inexistantes.

L'Union Astronomique Internationale a défini en 1970 le méridien 0° comme étant le méridien solaire au premier périhélie après le 1950. Le système de coordonnées utilisé par Mariner 10 se fonde sur le méridien 20° qui coupe le cratère Hun Kal (Hun Kal signifie « 20 » en maya), ce qui donne une légère erreur de moins de 0,5° par rapport au méridien 0° défini par l'UAI. Le cratère Hun Kal est en quelque sorte le Greenwich de Mercure. L'équateur se trouve dans le plan de l'orbite de Mercure. Les longitudes sont mesurées de 0° à 360° en allant vers l'ouest.

Mercure est découpée en 15 quadrangles. Plusieurs méthodes de projection ont été utilisées pour cartographier la surface de Mercure, suivant la position du quadrangle sur le globe. Cinq projections Mercator (projection cylindrique tangente à l'équateur) entourant la planète au niveau de l'équateur, entre les latitudes 25° nord et 25° sud ; quatre projections Lambert (projection conique) entre 20° et 70° de latitude pour chaque hémisphère ; et deux projections stéréographiques pour cartographier les pôles (jusqu'à 65° de latitude).

Chaque quadrangle commence par la lettre H (pour « Hermès »), suivie de son numéro (de 1, pôle Nord, à 15, pôle Sud). Leur nom provient d'une caractéristique importante présente sur leur région (bassin, cratère, etc.) et un nom d’albédo (entre parenthèses) leur est attribué. Les noms d’albédos assignés pour cette nouvelle carte proviennent de celle d'Antoniadi, puisque c'était celle utilisée jusque là par tous les observateurs depuis plusieurs décennies. Ils servent pour repérer les quadrangles lors des observations au télescope depuis la Terre, où l'on ne distingue que les variations d'intensité de lumière. Seuls Lowell et Antoniadi avaient annoté leurs cartes. Les quadrangles non cartographiés ne possèdent que leur nom d'albédo.

Avant d'y envoyer des sondes, on ne connaissait que peu de choses de Mercure. Jusqu'à l'arrivée de la sonde Messenger en janvier 2008, seule la sonde Mariner 10 avait survolé Mercure (en 1974) : c'est à elle que l'on doit une grande partie de ce que l'on connaît de cette planète. La mission actuellement en cours (Messenger), et une autre en préparation devraient nous apporter plus d'informations sur Mercure dans les années à venir.

Mariner 10 a été la première sonde à étudier Mercure de près. Elle a survolé la planète à trois reprises, en mars et septembre 1974 et en mars 1975. À l'origine, elle était destinée à survoler et étudier Vénus, mais les astronomes ont pensé qu'ils pourraient en faire usage également pour étudier Mercure, dont on connaissait peu de choses. Mariner 10 est donc la première sonde à avoir utilisé l'assistance gravitationnelle d'une planète — Vénus — pour en atteindre une autre.

La sonde aura pris durant ces trois passages plus de de Mercure dont certaines à haute résolution ( par pixel). Cependant, seuls 45 % de la surface ont pu être cartographiés. En effet, lors de ses trois passages, Mercure présentait la même face au Soleil ; les régions à l'ombre étant impossibles à cartographier.

Mariner 10 permit de découvrir la présence d'une très mince atmosphère, ainsi qu'une magnétosphère. Elle apporta également des précisions sur sa vitesse de rotation, et de nombreuses autres données exploitables par les scientifiques. La mission arriva à terme le lorsque la sonde se trouva à court de carburant.

Une nouvelle mission pour Mercure baptisée MESSENGER ("MErcury Surface, Space ENvironment, GEochemistry, and Ranging") a été lancée le de Cap Canaveral à bord d'une fusée Boeing Delta 2. La sonde MESSENGER a effectué un premier survol de Mercure en janvier 2008, ainsi qu'un second le 6 octobre de la même année. Ce passage permit de dévoiler une grande partie de la face cachée de la planète. MESSENGER a effectué un troisième passage, à d'altitude, en septembre 2009, qui permit de terminer la cartographie (10 % restaient encore inconnus), avant d'entrer en orbite autour de Mercure le 18 mars 2011 vers 1 h UTC. Une fois en orbite, elle a étudié l'atmosphère et la magnétosphère de la planète, sa composition chimique en surface et sa structure.

La sonde devait initialement rester en orbite durant une année terrestre. Elle y sera resté finalement quatre fois plus longtemps, terminant sa mission en impactant Mercure le 30 avril 2015 à UTC. Durant sa mission, MESSENGER aura pris plus de , dont de nouvelles photos à une résolution de par pixel et a permis de produire des cartes de sa composition globale, un modèle en trois dimensions de la magnétosphère, la topographie de l'hémisphère nord et caractériser les éléments volatils présents dans les cratères constamment ombragés des pôles.

L'Agence spatiale européenne est en train de planifier, en collaboration avec l'Agence spatiale japonaise, une mission baptisée BepiColombo, qui prévoit de placer deux sondes en orbite autour de Mercure : l'une pour cartographier la planète ("Mercury Planetary Orbiter"), l'autre pour étudier sa magnétosphère ("Mercury Magnetospheric Orbiter"). Le projet de l'envoi d'un atterrisseur embarqué avec la mission a dû cependant être abandonné, pour des raisons budgétaires. Ces deux sondes devaient être envoyées par un lanceur Ariane 5 en juillet 2016. Elles devraient rejoindre Mercure environ huit ans plus tard, pour l'étudier durant une année.

Début 2017, le lancement n'a toujours pas eu lieu. Pour le moment, il est reporté à 2018.

Le programme BepiColombo a pour objectif de répondre à une douzaine de questions que se posent les astronomes, notamment au sujet de la magnétosphère et de la nature du noyau de Mercure (liquide ou solide), de la possible présence de glace au fond des cratères constamment à l'ombre, de la formation du système solaire et de l'évolution en général d'une planète au voisinage de son étoile. Des mesures très précises du mouvement de Mercure vont également être effectuées, afin de vérifier la théorie de la relativité générale, censée expliquer les anomalies observées dans son orbite.

Un cratère, au pôle nord ou au pôle sud de Mercure, serait peut-être l'un des meilleurs endroits extraterrestres pour l'établissement d'une colonie, là où la température resterait constante à environ . Ceci est dû à une inclinaison axiale quasi nulle de la planète et au vide quasi parfait à sa surface, empêchant l'apport de chaleur depuis les portions éclairées par le Soleil. Ce qui rend ainsi toujours sombre et froid le fond d'un cratère à l'un des pôles, mais surtout éviterait de gros écarts de température. La colonie pourrait se chauffer elle-même, et la faible température ambiante permettrait une évacuation plus facile de la chaleur que sur un autre lieu extraterrestre.

Une base n'importe où ailleurs serait exposée, en « journée », durant un trimestre terrestre, à la chaleur intense du Soleil, puis durant une période nocturne identique, serait privée de la moindre source de chaleur extérieure, et serait donc dangereuse avec des températures diurnes de et des températures nocturnes de . La situation ne serait pas aussi compliquée qu'il n'y paraît à première vue : les installations pourraient être enterrées sous plusieurs mètres de régolithe qui, dans le vide, servirait aussi bien d'isolant thermique que de bouclier antiradiations. Des approches similaires ont été proposées pour l'installation de bases sur la Lune, dont le jour dure deux semaines, suivi d'une nuit de deux semaines également. Par ailleurs, la base pourrait profiter du jour pour stocker la chaleur, et s'en servir ensuite la nuit. Il serait aussi possible de se servir de la chaleur disponible dans les roches du sous-sol, où la température est quasi constante, de l'ordre de (voir chapitre Conditions climatiques) : une sorte de géothermie mercurienne. En revanche, la protection des robots et des véhicules contre la chaleur du Soleil pourrait poser beaucoup plus de difficultés, entraînant une limitation des activités en surface durant le jour.

« Mercure, le messager ailé » est le mouvement de l'œuvre pour grand orchestre "Les Planètes", composée et écrite par Gustav Holst entre 1914 et 1916.





</doc>
<doc id="4917" url="https://fr.wikipedia.org/wiki?curid=4917" title="Mercure (mythologie)">
Mercure (mythologie)

Mercure ( : "Mercurius") est le dieu du commerce, des voleurs, des voyages et messager des autres dieux dans la mythologie romaine, assimilé à l'Hermès grec.
Son nom est lié au mot latin "merx" ( : marchandise), "mercari" ( : commercer), et "merces" ( : salaire).
Ses attributs traditionnels sont la bourse, le plus souvent tenue à la main, le pétase, le caducée, des sandales ailées ainsi qu'un coq et/ou un bouc.
Il était célébré le 15 mai en particulier.

Le mot « mercredi » dérive étymologiquement de « Mercure ».

Mercure n'apparait pas parmi les divinités "di indigetes" de la religion romaine archaïque. Au moment de son assimilation avec le dieu grec Hermès, commençant vers le , il réunit les fonctions des Dei Lucrii, ces anciennes divinités du commerce, de l'échange et du profit. 

Cette fusion aurait eu lieu, comme toutes les assimilations mythologiques, artistiques, culturelles, juridiques de Rome avec la Grèce antique, au cours du avant notre ère. Dès le début, cette assimilation est facilitée par la proximité d'apparence des deux divinités : le port des chaussures Talaria, du chapeau ailé, et du caducée, ce bâton avec deux serpents entrelacés, don d'Apollon à Hermès. Mercure est souvent accompagné d'un coq, symbole de la nouvelle journée, d'un bélier ou d'une chèvre, symboles de la fertilité, ou encore d'une tortue, faisant référence à l'invention légendaire de la lyre avec une carapace de tortue.

Il a été un messager des dieux et un dieu du commerce, en particulier du commerce des grains.

Enfin, Mercure est aussi, un dieu « psychopompe », qui conduit les âmes récemment décédées dans l'au-delà.

Plus tard, le dieu romain a encore subi des métamorphoses, notamment en Gaule romaine où il est modifié par ce que l'on appelle le syncrétisme gallo-romain. C'est-à-dire qu'il est, bien que toujours dénommé et représenté de la même façon – outre certaines variantes mineures – « amalgamé » aux dieux celtiques majeurs ou aux divinités topiques en fonction des peuples gaulois ou des lieux. Ainsi, en Gaule romaine, ce n'était pas tout à fait au Mercure de Rome qu'on vouait un culte – sauf lors de cérémonies officielles romaines, célébrées par des colons expatriés – mais à des Mercure gaulois.
Minerve, divinité – entre autres – des artisans. Ce "Lugus Mercurius" assimile alors la plupart des aspects du dieu celtique "Lug".

Le Mercure gaulois est connu comme le dieu romain Mercurius associé à des épithètes gaulois. Les plus connus sont ceux associés à Lug/Lougous certes, mais il est aussi associé à d'autres dieux celtes :








En Gaule aussi bien qu'à Rome, bien que Mercure représente souvent la troisième fonction, la fonction artisanat/commerce dans le cadre des fonctions tripartites indo-européennes, son rôle ne se laisse donc pas toujours réduire au cadre dumézilien.

Le temple de Mercure a été construit en 495 av. J-C dans le Circus Maximus, entre les collines de l'Aventin et du Palatin. Lieu particulièrement adapté pour adorer un dieu du commerce connu pour sa rapidité, car c'était un centre majeur du commerce et on y trouvait un hippodrome. La situation du temple de Mercure placé entre l'Aventin tenu par la plèbe et le Palatin, centre politique des patriciens, souligne son rôle en tant que médiateur.

Mercure n'appartenant pas au groupe des premières divinités romaines, il n'avait pas reçu de "flamine" (« prêtre »). On l'honorait cependant lors d'une fête importante, le 15 mai, les Mercuralia, fête au cours de laquelle les marchands s’arrosaient la tête et leurs marchandises d'eau tirée de son puits sacré situé près de la "Porta Capena".

Ses attributs sont : les sandales ailées ; le pétase ; le caducée et parfois la bourse (tenue le plus souvent à la main).


Mercure est le dieu du commerce, des voyageurs, des voleurs, des marchands, des médecins et il est également le messager des dieux. Il accompagne aussi les âmes en enfer.

Tel un ectoplasme, la puissance de ce dieu est due à sa capacité de prendre la forme des circonstances. Jamais il n'était figé dans une forme donnée. Ainsi put-il mettre à sac l'Olympe.

Mercure était connu à Rome sous le nom de "Mercurius". On peut aussi trouver, dans des textes anciens, "Merqurius", "Mirqurios" ou "Mircurios". De nombreux épithètes le caractérisent, certains s'attachant à décrire l'un de ses rôles ou aspects, d'autres indiquant un syncrétisme local avec des divinités non romaines. 












Selon la reprise romaine de la légende grecque d'Hermès, Mercure est le fils de Jupiter et de la nymphe Maïa, fille de Atlas.

Il a eu plusieurs relations avec des déesses comme avec Vénus ou Chioné, ou avec Hersé. La plupart de ses enfants ont des caractéristiques sexuelles particulières avec Hermaphrodite, Pan ou Cupidon dans les traditions plus tardives.

Mercure, messager des Dieux, est chargé de transmettre les nouvelles. À la fin de la renaissance et au début de l’ère baroque, les peintres et auteurs érudits en font l’allégorie du messager : dit Falstaff à l’hôtesse dame Quickly qui tarde à lui communiquer le message dont elle est chargée, dans "Les Joyeuses Commères de Windsor" de Shakespeare. 

Par extension, le nom de Mercure est associé aux débuts de la presse périodique, chargée de communiquer les nouvelles, ainsi le "Mercurius Politicus" (1659) anglais consacré à l’actualité des nouvelles étrangères et des événements en cours dans les « trois nations d’Angleterre, d’Écosse et d’Irlande pour l’information du public. » Le "Mercure galant", un des premiers périodiques français (1672), donne à ses lecteurs les nouvelles de la Cour et de Paris (il inspirera la création de "Der Teutsche Merkur" à Weimar en 1773). En Angleterre encore paraissent "The Impartial Protestant Mercury" (1681), "The London Mercury" (1682) ou le "Rutland And Stamford Mercury" en 1695. La multiplication de ces titres font que Mercure devient nom commun sous la plume de Bayle : « le nombre des mercures ou des ouvrages qui mériteraient ce nom s'est si fort multiplié qu'il serait temps qu'on en donnât l’histoire.» 

En 1737 paraît le journal régional britannique "Sherborne Mercury" ; en 1758, l’américain James Franklin, frère de Benjamin, fonde le "The Newport Mercury". Le "Quebec Mercury", hebdomadaire anglophone, voit le jour dans la ville de Québec en 1805. En 1824 c’est le tour du "Staffordshire Mercury", hebdomadaire anglais ; "El Mercurio de Valparaíso" est créé en 1827. La sixième édition du "Dictionnaire de l'Académie", parue en 1835, rajoute cette définition dans l'article consacré à Mercure : « Il a servi et sert encore de Titre à divers écrits périodiques traitant de politique, de littérature, et contenant des annonces, des nouvelles. Le "Mercure français". Le "Mercure galant". Le "Mercure de France" ». 

"The Maitland Mercury" le premier journal local australien, paraît pour la première fois le 7 janvier 1843; "The Weston & Somerset Mercury" anglais date de la même année. Le "Guelph Mercury" est créé en 1854 au Canada, la même année que le "Hobarton Mercury" australien suivi en 1855 par l’"Illawarra Mercury" également australien ; le "Clevedon Mercury" paraît pour la première fois en 1863 en Angleterre, le "St Arnaud Mercury" australien le 13 février 1864.

Il existe toujours aujourd'hui un certain nombre d’organes de presse qui portent le nom de "Mercure", par exemple le quotidien national chilien "El Mercurio".

Hermès, en tant que message des dieux, mais aussi "proprio motu", apparaît tout au long de l’Odyssée : 








La figure de Mercure apparaît dans de nombreuses sculptures et peintures depuis l’antiquité, notamment sous forme de petits "ex-voto". Il existe des portraits du Dieu comme allégorie du commerce ou allégorie planétaire, des scènes historiques tirées de la mythologie, les chapitre de l’"Iliade" consacrés au "jugement de Pâris" ("Mercure et Pâris", de Donatio Creti, 1747, Bologne), la légende de "Mercure et Argus".

On le retrouve dans les scènes des « amours des dieux » inspirées des "Métamorphoses" d'Ovide et peintes par Véronèse :
Sur le même sujet, il est représenté par Nicolas Chaperon (vers 1630) dans "Vénus, Mercure et Cupidon", au musée du Louvre.

Il est le compagnon de Jupiter dans les représentations des amours de Jupiter et Alcmène (voir "Amphitryon" de Molière) ou du conte de « Philémon et Baucis ». Certaines allégories humanistes prêtent à cette figure une dimension plus ésotérique. C’est le cas de Botticelli, dans le "Printemps" conservé à Florence. Dans l’édition de 1555 des "Emblèmes" d’Alciat, Mercure apparaît en allégorie de l'art, de la vertu et de la sagesse capable de contrecarrer les assauts de la Fortune capricieuse. Une gravure de Jacob Matham, d’après Goltzius (1597) le représente dans sa dimension planétaire, associé aux signes zodiacaux des gémeaux et de la vierge et au caractère féminin froid et humide, ainsi qu’en protecteur des arts et des lettres. 

Une des plus célèbres sculptures représentant Mercure est un bronze de Jean de Bologne, "Mercure volant" (après 1565), conservé à Florence. C’est à Adrien de Vries que l’on doit un autre bronze, "Mercure et Psyché". Le panthéon gréco-romain est populaire au , où l’on voit apparaître "Mercure chevauchant Pégase", 1701-1702, d’Antoine Coysevox, commandé en 1699 pour la décoration du parc de Marly, "Mercure attachant ses talonnières", de Jean-Baptiste Pigalle, 1741-44. En 1777, Augustin Pajou le représente en allégorie du commerce (musée du Louvre).

Associé à l’alchimie en raison de l’utilisation du métal, Mercure apparaît fréquemment dans les éditions illustrées consacrées à cette discipline ésotérique.

Berlioz introduit Hermès (nommé Mercure) dans son opéra Les Troyens : à la fin du duo d'amour entre le troyen Énée et la reine de Carthage Didon « Nuit d'ivresse et d'extase infinie »), le dieu frappe de son caducée le bouclier d'Énée pendu à une colonne et, d'une voix grave, désigne la mer en prononçant le mot "Italie !", montrant ainsi à Énée son destin. 

Offenbach a fait la caricature de Mercure dans "Orphée aux Enfers" :
<poem>
Mercure :
« Eh hop ! Eh hop ! Place à Mercure !
Ses pieds ne touchent pas le sol,
Un bleu nuage est sa voiture,
Rien ne l'arrête dans son vol.

Bouillet dans son dictionnaire
Vous dira mes titres nombreux :
Je suis le commissionnaire
Et des déesses et des dieux ;
Pour leurs amours moi je travaille,
Actif, agile, intelligent,
Mon caducée est ma médaille,
Une médaille en vif argent.
Je suis le dieu de l'éloquence,
Les avocats sont mes enfants,
Ils me sont d'un secours immense
Pour flanquer les mortels dedans.
Je dois comme dieu du commerce
Détester la fraude et le dol,
Mais je sais par raison inverse
Les aimer comme dieu du vol,
Car j'ai la main fort indirecte
Et quelquefois le bras trop long :
Quand il était berger d'Admète
J'ai chipé les bœufs d'Apollon.
Tout en étant le dieu des drôles,
Je suis le plus drôle des dieux,
J'ai des ailes sur les épaules
Aux talons et dans les cheveux.
Jupin mon maître sait me mettre
À toute sauce ; il finira
Par me mettre dans un baromètre
Pour savoir le temps qu'il fera. »

« Et Zeus dit : La vie va, la mort vient. »
</poem>

Dans "Les 48 Lois du pouvoir", l'auteur – un machiavelliste – pose que l'un des attributs du pouvoir est d'être à même d'assumer un « "formelessness" », c'est-à-dire une inconsistance ontologique sur le théâtre des opérations. Un Prince doit, dit en substance Robert Greene, avoir le sens "kairos" (grec), autrement dit d'être à même de saisir les occasions et, par là même, de faire preuve d'une imprévisibilité devant l'ennemi, contraint, alors, à s'adapter et à abandonner ses stratégies, trop classiquement rebattues. "A contrario", chaque fois que l'on expose ses habitudes, ses intentions, ses "modi operandi", etc. l'ennemi a beau jeu de tabler sur nos vulnérabilités. Il peut même tailler sur la mesure de notre « forme prise » une stratégie qui ainsi ferait mouche. Témoin la bataille de Thermopyles où l'immense armée perse fut, dit Robert Greene, astreinte à prendre une forme trop prévisible, tant il est vrai que les Spartiates les avaient leurrés dans un port trop petit et dans un sentier trop étroit. Les Perses ayant pris une forme donnée par le port et le sentier, les Spartiates purent les réduire à néant ! Témoin, à l'inverse, Napoléon, que Robert Greene adoube du titre « dieu de la guerre », en tant que le Français se ficha de la tradition, des méthodes classiquement éprouvées de la guerre et, comme tel, sut détruire les Autrichiens trop formatés, trop disciplinés, trop prévisibles sur le théâtre des opérations. 

Napoléon est à la guerre, ce que Mercure est à la stratégie : une inconsistance qui, devant les circonstances changeantes, prend momentanément une forme donnée. Jamais ni Napoléon ni Mercure ne se figèrent dans une forme, les rendant prévisibles à l'ennemi.





</doc>
<doc id="4918" url="https://fr.wikipedia.org/wiki?curid=4918" title="Mercure (chimie)">
Mercure (chimie)

Le mercure est l'élément chimique de numéro atomique 80, de symbole Hg. Il fut aussi appelé vif-argent jusqu'au début du .

Il a longtemps été utilisé dans les thermomètres et les batteries avant d'être interdit en 1999.

Le mercure est un élément du groupe 12 et de la période 6. "Stricto sensu", c'est un métal pauvre, qui ne répond pas à la définition des éléments de transition par l'Union internationale de chimie pure et appliquée (IUPAC) ; en pratique cependant, il est très souvent assimilé aux métaux de transition dans les manuels et de très nombreux ouvrages. Le groupe 12 est également appelé « groupe du zinc », ou groupe B, et comprend, par numéro atomique croissant, Zn, Cd et Hg, éléments caractérisés par deux électrons sur la sous-couche "s" au-delà d'une sous-couche "d" complète. La configuration électronique du mercure est . Dans ce groupe ordonné, la réactivité décroît, le caractère noble et/ou covalent est plus marqué. Le corps simple mercure presque noble peut être mis à part.

Le corps simple mercure est un métal argenté brillant, le seul se présentant sous forme liquide dans les conditions normales de température et de pression sans phénomène de surfusion, conditions dans lesquelles il possède une tension de vapeur non négligeable car au-delà, il se vaporise assez aisément.

Le mercure apparaît comme un puissant neurotoxique et reprotoxique sous ses formes organométalliques (monométhylmercure et diméthylmercure), de sels (calomel, cinabre, etc.) et sous sa forme liquide en elle-même. L'intoxication au mercure est appelée « hydrargisme » (voir également Maladie de Minamata). On le soupçonne également d'être une des causes de la maladie d'Alzheimer, du syndrome de fatigue chronique, de la fibromyalgie et d'autres maladies chroniques. En 2009, le Conseil d’administration du Programme des Nations unies pour l'environnement (PNUE) a décidé d’élaborer un instrument juridiquement contraignant sur le mercure ; le Comité de négociation intergouvernemental chargé d'élaborer cet instrument juridique s'est réuni en janvier 2011 au Japon puis à Nairobi fin octobre 2011.

Un projet de traité international visant à diminuer les usages du mercure et ses conséquences environnementales et sanitaires néfastes est en négociation depuis juin 2010 (à Stockholm), prévu pour 2013 au Japon. Plus de 100 pays ont été réunis par le PNUE à Nairobi (Kenya), du 31 octobre au 4 novembre 2011 pour une de négociation (INC3, pour Intergovernmental Negotiating committee).
Jusqu'au , deux termes synonymes, "vif-argent" et "mercure", furent employés concurremment avant que la normalisation de la nomenclature chimique n'imposa le dernier à partir de 1787.

Le nom en ancien et moyen français de ce corps chimique, liquide dense et remarquablement mobile est le "vif-argent".

Le mercure se trouve dans la nature essentiellement sous forme d'un minerai de sulfure de mercure (α-HgS), nommé cinabre. On en tire une poudre de couleur rouge vermillon qui a été utilisée comme pigment pour la confection de céramiques, de fresques murales, de tatouages et lors de cérémonies religieuses. Les plus anciennes attestations archéologiques se trouvent en Turquie (Çatalhöyük, -7000, -8000), en Espagne (mine Casa Montero et tombes de La Pijota et de Montelirio, -5300) puis en Chine (culture Yangshao -4000, -3500).

En Grèce, Théophraste (-371, -288) a écrit le premier ouvrage savant sur les minéraux "De Lapidus" dans lequel il décrit l'extraction du cinabre (gr:κινναβαρι, "kinnabari") par des lavages successifs et la production de vif-argent (χυτὸν ἄργυρον, "chytón árgyron") en broyant avec un pilon d’airain le cinabre avec du vinaigre . Au premier siècle, Dioscoride décrit la technique de calcination d'une cuillerée de cinabre placée dessous un récipient sur lequel se dépose la vapeur de mercure ("De materia medica", V, 95). Dioscoride qui écrit en grec ancien, nomme le mercure ainsi obtenu ὑδράργυρος, "hydrárgyros", « argent liquide » en raison de son aspect.

À la même époque, le Romain Pline, décrit la même technique de sublimation du minerai pour obtenir de l"'hydrargyrus" (terme latin dérivé du grec ancien), expression qui en français deviendra "hydrargyre". En 1813-1814, Berzelius choisira le symbole chimique Hg, sigle composé de l'initiale des deux morphèmes "Hydrar" et "Gyrus" pour désigner l'élément mercure. Pline distingue l'hydrargyrus de la forme native du métal qu'il nomme "vicem argenti" qui en français donnera "vif-argent" (Pline, H.N., XXXIII, 123). En français, le terme « vif-argent » apparaît dans une chanson de geste mise par écrit vers 1160, "Le Charroi de Nîmes". Cette appellation va être utilisée jusqu'au début du .

Dès l'Antiquité, les philosophes néoplatoniciens et astrologues gréco-romains ont associé les sept métaux aux couleurs, aux divinités et aux astres : l'or au Soleil, l'argent à la Lune, l'étain à Vénus, le fer à Mars etc. Après la découverte de la technique d'extraction du vif-argent, ils attribuèrent ce métal extravagant, mi liquide mi solide, à l'androgyne "Mercure".

Les alchimistes européens du utilisent concurremment les deux appellations en latin. Le Pseudo-Geber dans son ouvrage "Summa perfectionis" parle de "argento vivo" ou "Mercurio". Ce double usage se perpétuera chez les chimistes des siècles suivants jusqu'à la grande réforme de la nomenclature proposée par Guyton de Morveau, Lavoisier et al. dans Méthode de nomenclature chimique de 1787. Ils choisiront "mercure" un terme simple (non composé sur le plan morphologique) associé à un corps simple (non décomposable sur le plan chimique).

Le mercure a , dont plusieurs isotopes stables éventuellement utilisables pour des analyses isotopiques ou un traçage isotopique. 

Il a aussi des isotopes radioactifs instables (31 de ses dont seulement 4 ont une période supérieure à la journée). Seul le Hg a, selon l'IRSN, des applications pratiques (traceur isotopique).

Le mercure 203 (Hg) est produit par les centrales nucléaires ou le retraitement des déchets nucléaires ; il est recherché et dosé par spectrométrie gamma. 

Sa période radioactive est de , pour une activité massique de 5,11 10 Bq.g-1. Son émission principale par désintégration est de (avec 100 % de rendement d’émission)(Nuclides 2000, 1999).

Le mercure radioactif a été évalué dans les effluents gazeux de l'usine de La Hague (de 1966 à 1979) à 2 MBq.an-1 à 4 GBq.an-1). On l'a aussi dosé dans l'atmosphère de réacteurs de recherche au CEA. 

Selon l'IRSN, . Faute de données concernant la cinétique et les effets du Hg dans l’environnement, on estime généralement qu'il se comporte comme le mercure élémentaire stable (sachant que du mercure élémentaire stable a été très utilisé par l'industrie nucléaire, dont pour la production d’armes nucléaires, notamment des années 1950 à 1963 aux États-Unis, où on le retrouve dans les sols et les eaux qu'il a pollués.

Le mercure est un élément assez rare : son clarke est compris entre .

On trouve le mercure sous forme d'un corps simple comme le mercure natif, d'ions et de composés à l'état oxydé, plus fréquemment sous forme de sulfures, tels que le sulfure de mercure (HgS) de couleur rouge vermillon, nommé cinabre en minéralogie, et plus rarement sous forme d'oxydes ou de chlorures. Le cinabre est son principal minerai.

Du mercure est naturellement présent dans l'environnement, mais essentiellement dans les roches du sous-sol. Les principales sources naturelles d'émission dans l'environnement en sont les volcans puis les activités industrielles.

Aujourd'hui, une grande partie du mercure utilisé légalement (ou illégalement pour l'orpaillage illégal) provient de la récupération de mercure interdit pour certains usages, ou d'une production secondaire (condensats de grillages de minerais complexes dont ceux du zinc) (blende ou sphalérite). En Europe, Avilés (Asturies, en Espagne), est une des grandes zones productrices, avec une production annuelle de plusieurs centaines de flacons par an (l'industrie du mercure nomme flacon un container d'acier contenant de mercure).

Le corps simple mercure est un métal blanc et très brillant, liquide à température ordinaire. Ce liquide, très mobile (faible viscosité) et très dense (densité : 13,6), se solidifie à .

Sous les conditions normales de température et de pression, c'est le seul métal à l'état liquide sans phénomène de surfusion (le seul autre corps simple à l'état liquide dans des conditions atmosphériques de pression et de température est le brome, un halogène). Notons également qu'il s'agit du seul métal dont la température d'ébullition est inférieure à .
Le point triple du mercure, à , est un point fixe de l'échelle internationale des températures (ITS-90).

Les vapeurs de mercure sont nocives. Le mercure est le seul élément en dehors des gaz rares à exister sous forme de vapeur monoatomique. Une bonne approximation de la pression de vapeur saturante "p"* du mercure est donnée en kilopascals par les formules suivantes :

Le mercure n'est pas soluble dans les acides aqueux, en particulier les acides oxydants.

Le mercure forme facilement des alliages avec presque tous les métaux communs à l'exception du fer, du nickel et du cobalt. L'alliage est également difficile avec le cuivre, le platine et l'antimoine.
Ces alliages sont communément appelés amalgames. Cette propriété du mercure a de nombreux usages.

Le mercure dit « vierge » (pur à 99,9 %) réagit avec de nombreux métaux en les dissolvant, voire en produisant une flamme ou en dégageant une forte chaleur (s'il s'agit de métaux alcalins).

Certains métaux résistent mieux à la dissolution et à l'amalgamation, ce sont le vanadium, le fer, le niobium, le molybdène, le tantale et le tungstène. Le mercure peut aussi attaquer les plastiques en formant des composés organomercuriels. En outre, il est très lourd.

Il doit donc être manipulé avec soin, et stocké avec certaines précautions ; généralement dans de solides contenants spéciaux (dits flasques ou flacons) de fer ou d'acier. Les petites quantités sont parfois stockées dans des flacons spéciaux de verre, protégées par une coque de plastique ou de métal.

Le mercure très pur (dit « mercure électronique » ; pur à 99,99999 %) doit obligatoirement être conditionné en ampoules scellées de verre blanc neutre dit « de chimie ».

Dans le groupe du zinc, le mercure se distingue par une certaine noblesse ou inertie chimique. L'ionisation est peu notable et plus rare. Les sels de mercure sont souvent anhydres.

Le mercure existe à divers degrés d'oxydation :

Le mercure métallique n'est pas oxydé à l'air sec. Cependant, en présence d'humidité, le mercure subit une oxydation. Les oxydes formés sont HgO à température ambiante, HgO entre 573 K () et 749 K ().
L'acide chlorhydrique (HCl) et l'acide sulfurique (HSO) dilué n'attaquent pas le mercure élémentaire. En revanche, l'action de l'acide nitrique (HNO) sur le mercure Hg produit HgNO. L'eau régale attaque également le mercure : du mercure corrosif HgCl est alors produit.

Le mercure tend à former des liaisons covalentes avec les composés soufrés. D'ailleurs, les thiols (composés comportant un groupe -SH lié à un atome de carbone C) étaient autrefois nommés "mercaptans", du latin "« mercurius captans »". Cette affinité entre le mercure et les soufre peut s'expliquer dans le cadre du principe HSAB car, par exemple, le méthylmercure est un acide très mou, de même que les composés soufrés sont des bases très « "molles" ».

Des composés mercuriques servent comme fongicides et bactéricides, notamment le Thimerosal médiatisé pour sa présence dans les vaccins ou le Panogen qui avait été par hypothèse, incriminé dans l'affaire du pain maudit de Pont-Saint-Esprit. 

La synthèse du chlore en Europe passe souvent par l'utilisation de cellules à cathode de mercure.

En santé/médecine :

Certaines piles contiennent du mercure. Les piles salines et alcalines ont longtemps contenu du mercure à hauteur de 0,6 % pour les piles salines, 0,025 % pour les autres. Quant aux piles boutons, elles mettent parfois en jeu les couples Zn/Zn et Hg/Hg.

La réaction en fonctionnement est : 
Le mercure est utilisé dans les lampes à mercure et à iodure métallique sous haute pression à la forme atome. Les lampes fluorescentes à vapeur de mercure contiennent environ 15 mg de mercure gazeux. La réglementation RoHS impose depuis 2005 une quantité maximale de . En 2009, plusieurs fabricants ont réussi à abaisser la quantité à .
On notera que le mercure est initialement sous forme d'oxyde.
Pour les piles de « type bouton » répondant à ce modèle, 1/3 du poids de la pile est dû au mercure. Dans leur grande majorité cependant, les piles boutons utilisent de l'oxyde d'argent à la place de l'oxyde de mercure ; elles contiennent alors entre 0,5 et 1 % de mercure.

Le mercure a longtemps été utilisé comme fluide dans les thermomètres du fait de sa capacité à se dilater avec la température. Cet usage a été abandonné, et les thermomètres à mercure interdits du fait de la toxicité du mercure.

Le mercure est utilisé dans les contacts des détecteurs de niveau (poire de niveau) dans les fosses qui ont une pompe de relevage ou une alarme de niveau (~ de mercure par contact).

Le mercure est utilisé dans les systèmes rotatifs des lentilles de phares permettant l'absence de frottement et la grande régularité du mouvement de rotation de ces systèmes sur leurs socles tout en permettant l'alimentation électrique (deux cuves concentriques).

Le mercure est couramment utilisé dans l'orpaillage afin d'amalgamer l'or et de l'extraire plus aisément.
Le mercure est encore présent en septembre 2015 dans certains tensiomètres utilisés dans les cabinets médicaux.

Les qualités du mercure pour la chimie nucléaire et les instruments de mesure en font l'une des huit matières premières stratégiques considérées comme indispensables en temps de guerre comme en temps de paix

Le mercure est utilisé dans certaines mines artisanales.


Ce métal, parmi les plus toxiques est très mobile dans l'environnement car volatil à température ambiante (y compris à partir de l'eau ou de sols pollués). Il s'intègre facilement dans la matière organique et les processus métaboliques (sous forme méthylée). 
Certaines sources (naturelles ou anthropiques) de mercure peuvent être – dans une certaine mesure – tracées par des analyses isotopiques.
On cherche des solutions permettant de mieux et plus durablement le "solidifier" et/ou l'inerter. 

Contrairement aux oligo-éléments, le mercure est toxique et écotoxique quelle que soit sa dose, sous toutes ses formes organiques et pour tous ses états chimiques. 

Sa toxicité dépend notamment de son degré d'oxydation.

Sous sa forme vapeur, sa toxicité s'exprime d'abord via les voies respiratoires, puis il se solubilise dans le plasma, le sang et l’hémoglobine. 
Ainsi transporté, il attaque ensuite les reins, le cerveau et le système nerveux. Chez la femme enceinte, il traverse facilement le placenta et atteint le fœtus. Après la naissance un risque perdure puisque le lait maternel humain est aussi contaminé.

Des bactéries (du sédiment ou de l'intestin) convertissent une partie du mercure dissous, essentiellement en monométhylmercure HgCH.

Pour ces raisons, son usage est réglementé et beaucoup de ses anciens usages sont aujourd'hui interdits, dont dans l'Union européenne où depuis les années 2000 des directives limitent de plus en plus la vente d'objets en contenant. Exemple : La France interdit la vente des thermomètres au mercure depuis 1998 et leur utilisation dans les établissements de santé depuis 1999.

Elle est très élevée dans les régions d'orpaillage (Guyane et Surinam notamment) et dans certaines régions industrielles. 
En 2018 en France le « "Volet périnatal" » du "programme national de biosurveillance" a publié une évaluation de l'imprégnation des femmes enceintes dont pour le mercure (et 12 autres métaux ou métalloïdes ainsi que quelques polluants organiques). 
Ce travail a été fait dosage du mercure dans les cheveux maternels de 1 799 femmes enceintes (« "Cohorte Elfe" »), dosage qui révèle principalement le mercure organique, issu du mercure chroniquement ingéré ou inhalé. Ce panel ne comprenait que des femmes ayant accouché en France en 2011 hors Corse et TOM). Le dosage capillaire de ces 1 799 femmes entrantes en maternité a confirmé une légère baisse par rapport aux études françaises précédentes ; La moyenne géométrique était de 0,4 μg de mercure par gramme de cheveux. Moins de 1 % des femmes du panel étudié présentait plus de 2,5 μg de mercure par gramme de cheveux (seuil établi par le JECFA pour les femmes enceintes), cependant ce taux est significativement supérieure à celui relevé au même moment (entre 2011 et 2012) ailleurs, notamment en Europe centrale et de l’Est, et même aux États-Unis où les taux de mercure sont connus pour être souvent problématiques. Un tel écart entre la France et les autres pays avait déjà été observé en 2007 : tout comme pour l'arsenic, ce mercure supplémentaire pourrait provenir d'une consommation plus importante en France de fruits de mer, ce que semble confirmer le fait qu'une consommation plus élevée de produits de la mer (en cohérence avec la littérature scientifique) était associée à un taux de mercure capillaire plus élevé chez la femme enceinte.

En 1997, une étude a été menée par l'Institut de veille sanitaire sur l'exposition alimentaire au mercure de 165 Amérindiens Wayana vivant sur les bords du fleuve Maroni en Guyane dans les quatre villages Wayanas les plus importants (Kayodé, Twenké, Taluhen et Antécume-Pata) ; des dosages de mercure total ont été pratiqués pour 235 habitants de villages environnants ainsi que des relevés anthropométriques de 264 autres individus. On a constaté que certains poissons contenaient jusqu'à /kg. Plus de 50 % de la population de l'échantillon dépassait la valeur sanguine recommandée par l'OMS de /g de mercure total dans les cheveux (/g en moyenne, à comparer à un taux de référence égale à /g). De plus, environ 90 % du mercure était sous forme organique, la plus toxique et bio-assimilable. Les teneurs étaient élevées pour toutes les tranches d'âge, un peu moindre mesure chez les enfants de moins d'un an, mais ils y sont beaucoup plus sensibles. 

L'exposition était la plus élevée dans la communauté de Kayodé où s'exerçaient au moment des prélèvements des activités d'orpaillage. Pour 242 personnes prélevées dans le Haut-Maroni, 14,5 % dépassaient la valeur limite de /kg. Depuis, l'exploitation de l'or s'est fortement développée. Les indiens Wayana sont donc exposés au mercure très au-delà de l'apport quotidien habituel (environ de méthylmercure et de mercure total), mais aussi bien au-delà de la dose tolérable hebdomadaire recommandée ( de mercure total avec un maximum de de méthylmercure, soit environ /j par l'OMS à l'époque). Les adultes consomment de 40 à de mercure total/jour, les personnes âgées de l'ordre de /j. 

Les jeunes enfants en ingèrent environ /j (dont via l'allaitement), ceux de 1 à 3 ans en ingèrent environ /j, ceux de 3 à 6 ans environ /j et ceux de 10 à 15 ans de 28 à /j. 

Ces doses sont sous-estimées car elle ne prennent pas en compte l'apport par les gibiers, l'air et l'eau. 

Des taux équivalents à ceux mesurés au Japon à Minamata au moment de la catastrophe sont détectés en Guyane.
L'AFSSET a poursuivi ce travail.

Le mercure est responsable de maladies professionnelles chez les travailleurs l'utilisant — voir Mercure (maladie professionnelle). Il est responsable chez l'homme de maladies telles que l'érythème mercuriel.

Le mercure est toxique pour toutes les espèces vivantes connues.
Quelques-uns des impacts démontrés sur la vie sauvage sont : 

Le budget global du mercure est encore incomplètement connu, mais on sait par les "enregistrements sédimentaires" et les analyses isotopiques que les émissions anthropiques ont fortement augmenté depuis le début de l'« Anthropocène ». Les évaluations statistiques quantitatives convergent vers les estimations suivantes : 

En termes de modélisation biogéochimique, on estimait en 2017 que 40% des rejets dans l'eau et les sols faits depuis 4000 ans ont depuis été "séquestrés" (stockés dans un état stable). L'atmosphère en 2010 contenait environ 4,57 Gg de mercure soit soit le triple du niveau de 1850.

Le mercure pose un problème environnemental global : sa concentration moyenne augmente chez les poissons et mammifères dans tous les océans, alors que la plupart des autres métaux lourds sont en diminution. Sa répartition dans les océans, sur les continents et dans les pays varie fortement : par exemple, selon une étude récente, le taux de mercure augmente d'Est en Ouest en Amérique du Nord. Un phénomène dit de « pluies de mercure » est actuellement étudié dans l'Arctique.

85 % de la pollution mercurielle actuelle des lacs et cours d'eau proviendraient des activités humaines (centrales thermiques au charbon, et exploitation ou combustion de gaz ou pétrole). 
Ce mercure provient essentiellement du lessivage de l'air et de sols pollués, et des apports terrigènes en mer ou dans les zones humides.

Les sources seraient, par ordre décroissant d'importance : 

Le mercure émis sous forme de vapeur est très mobile dans l’air, et reste pour partie mobile dans le sol et les sédiments. Il l’est plus ou moins selon la température et le type de sol (il l’est moins en présence de complexes argilo-humiques et plus dans les sols acides et lessivables). Ainsi dit-on parfois qu’une simple pile bouton au mercure peut polluer d'un sol européen moyen pour 500 ans, ou pour un an. Les animaux le transportent aussi (bioturbation).
Le mercure n’est cependant pas biodégradable ni dégradable. Il restera un polluant tant qu’il sera accessible pour les êtres vivants.

Il est ce qu'on appelle un contaminant transfrontalier, par exemple de nombreux lacs du Québec sont pollués dû au transport de particules de la région Nord Ouest de l’Amérique du Nord tel le sud de l’Ontario ainsi que le nord des États-Unis. La teneur en Hg aurait doublé depuis les 100 dernières années, de ce fait les pêcheurs sportifs de cette province doivent mesurer leur consommation de poisson venant de cette région.

Nombreux étaient ceux qui pensaient que les pluies diluaient les pollutions et amenaient de l’eau propre régénérant les écosystèmes. On sait maintenant qu’elles lessivent les polluants émis dans l'atmosphère, en particulier les pesticides et les métaux lourds, dont le mercure, qui peuvent agir en synergie. Le mercure, très volatil, pollue le compartiment atmosphérique, lequel est lavé par la pluie et le brouillard qui polluent les eaux superficielles et les sédiments. Il peut ensuite dégazer ou être émis par les incendies et polluer de nouveau l’air.

Des analyses de la pluie et de la neige par l'Environmental Protection Agency (EPA) et des universités américaines ont montré que de nombreuses régions étaient polluées par le mercure : la teneur en mercure est jusqu’à 65 fois supérieure au seuil défini comme sûr par l'EPA autour de Détroit, 41 fois au-dessus de ce seuil à Chicago, 73 fois à Kenosha (Wisconsin, proche de la frontière avec l'Illinois), et près de 6 fois le seuil pour la teneur moyenne sur six ans à Duluth. Même les pluies les moins polluées dépassent souvent le seuil de sûreté. Les régions moins urbaines sont également parfois touchées : 35 fois le seuil EPA dans le Michigan et 23 fois pour le secteur du Devil’s Lake, dans le Wisconsin.

Dans 12 États de l'est américain (Alabama, Floride, Géorgie, Indiana, Louisiane, Maryland, Mississippi, New York, Caroline du Nord, Caroline du Sud, Pennsylvanie et Texas) à la fin des années 1990 et au début des années 2000, la pluie présentait encore des teneurs en mercure dépassant les seuils acceptables pour l'EPA pour les eaux de surface.

Les États-Unis et la Chine sont particulièrement touchés en raison de l’usage massif du charbon.

Le mercure des lampes fluocompactes a diminué, passant en quelques années de à (et souvent à moins de en 2011) mais dans le même temps, le nombre de lampe a beaucoup augmenté. En France, bien qu', la diffusion de ces lampes a reposé la question des risques liés aux vapeurs de mercure, en cas de bris, pour l'air intérieur, et via les filières d'élimination ou incinération pour l'air extérieur. Si les lampes étaient évacuées dans les ordures ménagères et incinérées, en considérant qu'une ampoule contient de mercure, et qu'il y a en environ 30 millions, de mercure seraient rejetés en plus des déjà rejetés dans l'air (en 2007) selon le CITEPA. Or, la réglementation limite le taux de mercure dans les lampes (à ), mais n'a toujours pas produit de norme pour la teneur en mercure de l'air intérieur ou extérieur, tant pour une exposition de courte durée que pour une exposition à long terme.

On se réfère donc aux valeurs guide de l'OMS ( de mercure inorganique sous forme de vapeur à ne pas dépasser sur une année). En France, la Commission de la sécurité des consommateurs a demandé en 2011 que le gouvernement produise des et préconise la révision de la directive européenne relative à l'utilisation de certaines substances dangereuses dans les équipements électriques et électroniques actuellement en vigueur (2002/95/CE du 27 janvier 2003) ce, afin de .

Seul le code du travail fixe en France, pour les travailleurs, une teneur maximale tolérée en mercure dans l'air ( d’air). 

L'Europe, tout en considérant le mercure comme très toxique, a omis dans sa directive de 2004 sur l’arsenic, le cadmium, le mercure, le nickel et les HAP dans l'air de préciser une valeur cible pour le mercure dans l’air (alors qu'elle existe pour les autres éléments et que la directive reconnait explicitement le mercure comme substance très dangereuse pour la santé et l'environnement. 
Pour le mercure, il n'y a pas non plus de valeurs maximales d’exposition à court terme (qui existent pour d'autres neurotoxiques).

Certaines précautions sont préconisées en cas de casse d'une ampoule au mercure : aération prolongée de la pièce, utilisation de gants pour le ramassage des débris et non-utilisation de l'aspirateur (risque de dispersion)

Il suffit de très peu de mercure pour polluer de vastes étendues d’eau (et les poissons à des niveaux dangereux pour la consommation humaine). 

Un cas de pollution importante par le mercure se rencontre près de Bergen, en Norvège. Le , le U-864, un sous-marin allemand de type U-Boot, a été coulé près de l'île de Fedje. Outre son armement conventionnel (torpilles, grenades et autres munitions), le sous-marin contenait de mercure réparties dans d'acier, destinées à soutenir l'effort de guerre du Japon. Depuis 1945, les bouteilles d'acier résistent très mal aux effets conjugués du temps et de l'eau de mer, et ont commencé à suinter puis à relâcher leur contenu dans les sédiments, et contaminer aussi les poissons. L'épave n'a été découverte que le , et depuis lors, la pêche est interdite dans une zone de . Diverses études et projets ont été menés par l' (Kystverket), mais la dépollution de l'épave et du site, n'a toujours pas commencé, à la fin 2015.

Le mercure étant très volatil, il passe dans l'air et contamine les pluies et peut se retrouver dans la neige et les eaux nivéales (de fonte de neige) puis des lacs de montagne. 

Les sédiments : ils finissent par recueillir la part du mercure qui n'a pas été ré-évaporée ou absorbée par les plantes ou stockée (plus ou moins durablement) dans le sol. Là, des bactéries peuvent méthyler le mercure et le rendre très bio-assimilable, notamment pour les poissons et crustacés ou les oiseaux aquatiques. Les plantes et animaux contaminés contaminent à leur tour la chaine alimentaire).

En mer les poissons piscivores et vivant vieux sont les plus touchés (thons, espadons... en particulier) ; Ils sont presque systématiquement au-dessus des normes quand ils sont adultes. 
De nombreux poissons des grands fonds sont aussi contaminés ("Sabre, Grenadier, Empereur..."), à des taux très variés selon leur âge (certains vivent jusqu'à 130 ans) et leur provenance.

Oiseaux marins prédateurs et cétacés sont également victimes d'une bioaccumulation du mercure dans le réseau trophique. À titre d'exemple, en mer du Nord, au début des années 1990, les taux moyens de mercure dans le foie et les muscles de quelques oiseaux marins de mer du Nord étaient de dans le foie chez le Guillemot de Troïl (pour dans le muscle), chez la Mouette tridactyle pour dans le muscle, chez la mouette rieuse pour dans le muscle et chez la Macreuse noire pour dans le muscle), en µg/g de poids sec. Chez le marsouin ("Phocoena phocoena" de cette même région, le taux moyen de mercure était de dans les foies, de dans les muscles et de dans les reins (en poids sec). Ce sont des taux très élevés, et on a mesuré dans ce lot des "records" de de poids sec chez la mouette tridactyle et de de poids sec chez le marsouin. Les deux principaux facteurs de risques semblaient être l'habitat et le régime alimentaire. 
On a constaté que le taux de mercure augmente avec l'âge chez les marsouins, mais que la proportion de méthylmercure diminue avec l'âge au profit du mercure lié à du sélénium, ce qui laisse supposer l'existence d'un processus de détoxification chez ce mammifère (peut être dans le lysosome des cellules du foie).
Pour ces raisons, 44 états américains ont établi des limites de consommation des produits de la pêche dans plusieurs milliers de lacs et de rivières. Les populations autochtones sont particulièrement visées par ces mesures.

Dans les sols : dans les sols pollués, ou quand ils poussent sur du bois contaminé en décomposition, le mercure est notamment bioaccumulé par les champignons. Par exemple, la vesse de loup géante ("Calvatia gigantea"), comestible quand elle est encore à chair blanche, bio-accumule fortement le mercure et un peu le méthylmercure), avec des teneurs atteignant déjà 19,7 ppm (en poids sec) sur un sol "a priori" non pollué. 
Sur terre, certaines plantes, les lichens et champignons peuvent en accumuler des quantités importantes
Dans quelques pays et à plusieurs reprises, des publications officielles ont averti les individus de la possibilité d'empoisonnement provoqué par les métaux lourds dans les champignons, notamment prélevés dans la nature.

Les espèces qui sont en haut de la chaîne alimentaire sont les plus concernées, outre les poissons, requins, cachalots, phoques, épaulards etc., dans les milieux continentaux, la loutre, le vison, le huard, la sterne, les limicoles, les canards etc., peuvent aussi être très touchés. L’homme, de par sa position dans la chaîne alimentaire, fait partie des espèces touchées.

Ampleur du phénomène chez l’Homme

Selon les CDC américains (Centers for Disease Control and Prevention) :

Santé : le mercure est présent dans les vaccins sous le principe actif Thiomersal depuis 1930.

À l'échelle mondiale, le Programme des Nations unies pour l'environnement a mis en place un « Plan mercure ».

Le , après une semaine de négociation, 140 États ont adopté à Genève la convention de Minamata qui vise la réduction des émissions de mercure au niveau mondial. Cette convention a été signée le , par les représentants des 140 États à Minamata au Japon, en hommage aux habitants de cette ville, touchés durant des décennies par une très grave contamination au mercure, on y parle même de la Maladie de Minamata. Cette convention doit désormais être ratifiée par 50 États, pour entrer en vigueur. La convention prévoit l'interdiction du mercure d'ici 2020 dans les thermomètres, instruments de mesure de la tension, batteries, interrupteurs, crèmes et lotions cosmétiques et certains types de lampes fluorescentes. Elle règle également la question du stockage et du traitement des déchets. Des ONG de défense de l'environnement regrettent néanmoins que cette convention ne touche pas les petites mines d'or et les centrales électriques au charbon. Certains vaccins et les amalgames dentaires, ne sont également pas touchés par cette convention. Achim Steiner, secrétaire général adjoint de l'ONU, chargé du Programme des Nations unies pour l'environnement a souligné, qu'il est assez « incroyable comme le mercure est répandu (…) Nous laissons là un terrible héritage » qui affecte « les Inuits du Canada comme les travailleurs des petites mines d'or en Afrique du Sud ».

Le Michigan, l'Ohio et l'Indiana ont institué des réglementations (par état) sur la consommation de poisson.

Le Wisconsin et le Minnesota ont pris des arrêtés interdisant ou limitant la consommation sur des centaines de lacs.

L'Environmental Protection Agency met à jour régulièrement des conseils aux femmes enceintes, enfants et personnes fragiles, recommandant notamment de limiter la consommation de certains poissons (thon, espadon en particulier) et fruits de mer.

Le Canada recommande également de limiter la consommation de certains poissons marins et poissons des grands lacs.

Le mercure est limité ou interdit pour certains usages. 

Il fait partie des métaux devant être contrôlé dans l'eau potable et l'alimentation.

L'Union européenne s'est dotée en 2005 d'une « stratégie communautaire sur le mercure » en 6 objectifs déclinés en actions spécifiques, à la suite d'un rapport de 2003 sur « les risques pour la santé et l'environnement en relation avec l’utilisation du mercure dans les produits », et à un rapport rapport de la Commission au Conseil, du 6 septembre 2002, concernant le mercure issu de l'industrie du chlore et de la soude après une directive (22 mars 1982) sur le mercure du secteur de l'électrolyse des chlorures alcalins. La Commission européenne a confié à la France la rédaction d’un argumentaire en vue d'éventuellement réviser la classification du Mercure dans le cadre de la directive 67/548/CEE (sur la classification, l’emballage et l’étiquetage des substances dangereuses). L’AFSSET a restreint l’étude à la seule classification CMR ("Cancérigène, Mutagène, Reprotoxique"), pouvant se traduire par une interdiction de vente du mercure en Europe pour un usage grand public et une surveillance accrue en milieu professionnel. L'avis de l'AFSSET a été soumis aux responsables de la classification et d’étiquetage pour l'Europe en novembre 2005 qui ont demandé plus de détails sur la toxicologie du mercure et son caractère cancérogenèse et mutagène (travail fait par l’INRS et l’INERIS). La procédure devrait aboutir à une modification du statut du mercure.

Le juillet 2006, la directive RoHS limite son usage dans certains produits commercialisés en Europe ; usage limité à 0,1 % du poids de matériau homogène (cette directive pourra être élargie à d'autres produits et à d'autres toxiques).

En juin 2007, le Parlement à Strasbourg a voté un règlement interdisant l'exportation et l'importation de mercure et réglementant les conditions de stockage.

Mi-2007 les députés ont voté pour l'interdiction des thermomètres au mercure non-électriques (les matériels électriques et contenant du mercure étaient déjà couverts par une directive) et d'autres instruments de mesure d'usage courant contenant du mercure, sans amendement à la position commune du Conseil, c’est-à-dire sans accepter la demande du PE d'une « dérogation permanente pour les fabricants de baromètres », mais acceptant « une exemption de deux ans ». (La pile au mercure reste autorisée dans le thermomètre) ;

Le parlement estime que 80 à 90 % du mercure des outils de mesure et contrôle est présent dans les thermomètres médicaux et domestique (importés pour les 2/3 d'Extrême-Orient souvent), et que les produits de substitution existent et sont même moins chers pour le particulier. Les instruments plus techniques ou scientifiques (manomètres, baromètres, le sphygmomanomètres, ou thermomètres non médicaux) sont eux fabriqués en Europe et leurs substituts peuvent être plus chers. Quelques dérogations sont prévues à la demande du parlement alors que le conseil envisageait une interdiction totale. Elles concernent les antiquités (thermomètres anciens au mercure) et le domaine médical (ex sphygmomanomètres à mercure, qui mesurent le mieux la tension artérielle). L’interdiction, non rétroactive ne touchera que les instruments neufs, la revente autorisée de matériels existant rendra les fraudes plus difficiles à contrôler, d’autant que les instruments vieux de plus de 50 ans, considérés comme des antiquités pourront encore être importés contenant du mercure.

Chaque État membre doit traduire la directive dans son droit national dans un délai d'un an à partir de son entrée en vigueur, et son application effective ne doit pas prendre plus de 18 mois à partir de la transposition (sauf pour les baromètres, pour lesquels le délai est porté à 24 mois) ;

Fin 2007, la Commission européenne envisage de bannir le mercure de toute préparation à usage thérapeutique. Elle doit aussi statuer sur l'avenir du mercure en dentisterie (incorporé à 50 % dans les plombages ou amalgames dentaires).

Depuis le 2008, la Norvège, qui ne fait pas partie de l'Union Européenne, a interdit l'utilisation du mercure pour toutes applications.

Mi-janvier 2008, un comité scientifique européen, mandaté par la Communauté et composé pour moitié de dentistes, publie un rapport déclarant que l'amalgame dentaire est un matériau sain, dépourvu de tout risque sur la santé humaine. Le document n'est édité qu'en anglais.

Le 22 février 2008 ; Selon la Commission, l'UE, le « plus grand exportateur de mercure au monde, doit montrer la voie à suivre dans la réduction de l'utilisation de ce métal». Pour cela, la commission a proposé d'interdire toute exportations européenne de mercure, ceci après une vaste consultation. L'UE étudie des solutions pour gérer les « "énormes surplus" » () attendus d'ici 2020 par l'abandon progressif du mercure par l’industrie du chlore et de la soude. Le stockage dans d'anciennes mines de sel spécialement adaptées est notamment à l'étude.

Le 26 février 2008 le JOUE publie une Position commune du conseil (CE) no 1/2008 du 20 décembre 2007 en vue de l'adoption d'un règlement (sur l'interdiction des exportations de mercure métallique et le stockage en toute sécurité du mercure).

La Direction générale de l'alimentation a publié une "Évolution des recommandations de consommation" en 2008 mais ne dispose pas d'un plan de surveillance des contaminants comme le mercure. 

Un cas particulier est celui des impacts de l'orpaillage en Guyane pour lequel la quantité de mercure illégalement utilisée et dispersée dans l'environnement est mal connue.

En 2017, le règlement européen relatif au mercure intégrant l'union européenne dans la Convention de Minamata (du 10 octobre 2013) est traduit dans le droit français. Il doit combler les lacunes réglementaires de l'UE en fixant "". Les lacunes identifiées sont au nombre de six : 

Les caractères physiques et chimiques du mercure ont influencé leur présence dans plusieurs produits de consommation, par exemple les thermomètres, les manomètres, l’amalgame dentaire, les lampes fluorescentes et autres. Ce sont des sources émettrices qui ajoutent à l’environnement.
Les solutions évoquées impliquent des interventions à différents niveaux. On peut limiter la "diffusion du mercure dans l'environnement" par les mesures suivantes :
Les piles bâton au mercure sont pour partie remplacées par d’autres. Les piles bouton sont obligatoirement récupérées et recyclées.
On peut aussi réduire "l'exposition humaine au méthylmercure" par les mesures suivantes:

Il faut entre autres relever le défi du traitement de la pluie, tel que conclut un rapport et une campagne de sensibilisation aux États-Unis dont les auteurs et la NWF invitent les industriels et les gestionnaires d'incinérateurs à fortement réduire leurs émissions de mercure. Ils incitent aussi les citoyens à économiser l’énergie pour limiter les émissions de mercure à partir des combustibles, et à ne plus acheter de piles ou produits contenant du mercure, ou s'ils les achètent, à s’en débarrasser correctement. 
La campagne invite également le gouvernement fédéral et les États à surveiller plus étroitement les niveaux de mercure dans les précipitations… Avec des scientifiques des Universités du Michigan du Minnesota, la NWF annonce qu’elle fera elle-même ses prélèvements et analyses de la pluie si les autorités responsables ne le font pas. Les premières villes visées pour une surveillance particulière étaient Chicago, Cleveland, Détroit, Duluth, et Gary (Indiana).
Encore sur la question de l'eau de pluie, plus précisément pour les systèmes de récupération des eaux pluviales pour la consommation, l'arrosage des légumes ou la consommation des animaux, il a été suggéré de tamponner l’acidité de la pluie et de la filtrer sur charbon actif. Ce charbon devrait ensuite être brûlé dans des incinérateurs équipés de filtres appropriés.

Une étude récente basée sur le suivi de l'alimentation de femmes d'un village amazonien (sur les berges de la rivière Tapajós, durant un an) laisse penser que la consommation de fruits diminue l'absorption du mercure par l'organisme. Reste à savoir si ce phénomène est lié à un fruit particulier disponible localement, ou aux fruits en général.

On a dressé avec succès des chiens pour repérer des gouttes de mercure par exemple piégées dans la moquette ou dans les fentes d'un plancher, des instruments contaminés, des puits, des égouts.. de manière à les récupérer avant qu'elles ne s'évaporent et après les avoir amalgamé avec un autre métal (poudre à base de zinc par exemple). En Suède, de mercure ont ainsi été collectées après avoir été détectées par deux labradors "renifleurs" de mercure, dans les écoles ayant participé au projet "Mercurius 98". Aux États-Unis, un chien dressé à détecter l'odeur de la vapeur de mercure a ainsi permis de récupérer de mercure dans les écoles du Minnesota. Des chercheurs envisagent aussi de génétiquement modifier des plantes pour augmenter les rendements de phytoremédiation.

La méthode d’analyse du mercure le plus courant est la spectroscopie d'absorption atomique. C’est une bonne technique pour le dosage des eaux telle l’eau potable, l’eau de surface, les eaux souterraines et les eaux usées. La concentration du mercure dans l’eau est mesurée pour différentes raisons, entre autres : les réglementations sur l’eau potable, le contrôle des réseaux d’égouts municipaux, la réglementation sur les matières dangereuses et loi sur la protection des sols et de réhabilitation des terrains contaminés. La préparation de l’échantillon pour le dosage est séparable en deux étapes : en premier lieu, on oxyde toutes les formes de l’Hg au travers d’une digestion acide En second lieu, les ions sont réduits en Hg élémentaire qui est volatil. L’échantillon gazeux est dirigé vers la cellule du spectromètre atomique.
La présence de mercure dans l’eau se retrouve dans les poissons et dans les sédiments sous sa forme organique, à cause de son affinité pour les lipides des tissus gras des organismes vivants et par précipitation pour les sédiments marins contenant aussi ce contaminant. L’analyse de sédiments marins est tout aussi utile pour connaître l'âge d'une pollution au mercure et ainsi retracer les pollutions industrielles ou naturelles passées.

En cas d'échantillons solides, une méthode analytique semblable peut être utilisée pour déterminer le métal trace. Les échantillons solides sont d'abord traités thermiquement (combustion) dans un four fermé où la température est contrôlée et en présence d’oxygène. Les gaz ainsi créés sont ensuite dirigés dans un tube catalytique à haute température afin de réduire les organo-mercures en mercure. Le mercure ainsi généré par la combustion ou traité par le tube catalytique est amalgamé grâce à un support ayant de l'or. Cet amalgame est ensuite chauffé brutalement (autour de ) afin de relarguer le mercure en « paquet ». Le mercure est ensuite mesuré en spectroscopie d'absorption atomique en vapeur froide à et quantifié par comparaison à un standard international (appelé MRC (Matériaux de Référence Certifié) ou CRM (Certified Reference Material)). Elle est appelée ainsi car la température de mesure est « relativement froide » (autour de ) au regard de l'absorption atomique classique qui utilise soit une flamme soit un four graphite. 
Les avantages de cette technique permettent d'éviter les préparations des échantillons qui utilisent souvent des acides ou d'autres produits chimiques. L'échantillon est simplement pesé et analysé ce qui procure aussi un gain de temps. Elles permettent aussi d'avoir un taux de récupération autour de 100 % et enfin de réduire les limites de quantification par réitération de l'amalgamation avant mesure. Ainsi, dans certaines conditions (salle blanche, amalgamation), ces limites de quantifications peuvent descendre à de mercure pour d'échantillon soit ou 5 ppt. La limite de quantification dans des conditions normales (1 analyse simple), par cette technique, reste cependant autour de () ou 500 ppt. Les limites de détection se mesurent en absolu et peuvent atteindre absolu de mercure.

Dans le cadre de la spectroscopie d'absorption atomique, la lampe à cathode creuse est réglée à étant la longueur d'onde d'absorbance pour Hg, l’absorbance mesurée est comparée avec les absorbances de solutions étalons préparées. Le domaine d’étalonnage est entre /l et /l. Il existe une limite de quantification de /l découlant d’une limite de détection d’environ /l. Le taux de récupération de cette méthode est de 101 % depuis la matrice de l’eau, 97,2 % pour les milieux biologiques et 90,1 % pour les sédiments selon les analyses du Centre d’Expertise en Analyse Environnementale du Québec.

Connu depuis l'Antiquité, les alchimistes puis le corps médical du au le désignaient par le nom « vif-argent » et le représentaient grâce au symbole de la planète Mercure, d'où son nom actuel.

Ce métal, en dépit de sa haute toxicité autrefois négligée, a eu de tout temps de nombreuses utilisations :





</doc>
<doc id="4920" url="https://fr.wikipedia.org/wiki?curid=4920" title="Samouraï">
Samouraï

Le (à ne pas confondre avec le ) est un membre de la classe guerrière qui a dirigé le Japon féodal durant près de 700 ans.

Le terme « samouraï », mentionné pour la première fois dans un texte du , vient du verbe "saburau" qui signifie « servir ». L'appellation est largement utilisée dans son sens actuel depuis le début de la période Edo, vers 1600. Auparavant, on désignait les guerriers plutôt par les termes "mono no fu" (jusqu'au ), puis , qui peuvent l'un ou l'autre se traduire par « homme d'armes ». À partir de la période Edo, les termes "bushi" et samouraï ne sont pas tout à fait synonymes, il existe une différence subtile (voir l'article "Bushi").

On trouve aussi parfois le terme "buke" : il désigne la noblesse militaire attachée au "bakufu" (gouvernement militaire), par opposition aux "kuge", la noblesse de cour attachée à l'empereur. Les "buke" sont apparus durant l'ère Kamakura (1185–1333).
La classe de guerriers professionnels du Japon, constituée d'archers montés sur des étalons, trouve son origine dans la volonté impériale de conquérir des terres des Aïnous à la fin de la période Nara.

Jusque-là, le Japon disposait d'une armée fondée sur la conscription, inspirée du modèle chinois. Les hommes âgés de vingt à trente ans étaient conscrits, répartis en autant de "gunki" (corps de mille soldats et officiers) qu'il y avait de provinces et attachés au service du "kokushi" (gouverneur de la province).

Ce système se révéla totalement inefficace pour lutter contre les « barbares » Aïnous, redoutables cavaliers. L'empereur décida en 792 de le dissoudre pour mettre en place un nouveau système appelé "kondeisei". Le "kondesei" avait l'avantage de réduire le poids du service militaire chez les paysans (sur qui reposait l'économie) puisqu'il était constitué de jeunes cavaliers archers issus de milieux plus aisés. Cette milice, formée de hommes, commença à tomber en désuétude au , mais on ne peut affirmer qu'elle soit à l'origine des premiers samouraïs, apparus à cette époque.

Mitsuo Kure, dans son livre , cite plusieurs autres origines possibles pour les samouraïs :

Enfin, Mitsuo Kure énonce une dernière hypothèse selon laquelle les samouraïs seraient à l'origine des gardes du palais impérial au début du , se fondant sur les premiers documents mentionnant le mot "samurai" (ou plutôt "saburai", « en service », qui se déforma plus tard en "samurai"). S'il rejette rapidement cette hypothèse, arguant que les meilleurs élevages de chevaux se trouvaient dans le Kantō et le Tōhoku et que les armures "o-yoroi" furent mises au point sur le champ de bataille et non dans la paix de la cour, cette hypothèse est en revanche la seule origine citée par Stephen Turnbull dans son .

Turnbull indique à leur sujet qu'ils passèrent rapidement du service impérial à celui des riches propriétaires terriens des provinces, qui devaient lutter contre les Emishi, les bandits et les propriétaires terriens rivaux. Il précise aussi que ces premiers clans de samouraïs étaient d'origine modeste, mais se plaçaient sous l'égide de descendants de lignées impériales mineures, partis chercher fortune dans les contrées sauvages. Les deux plus puissants clans de samouraïs de la fin de l'ère Heian, les clans Taira et Minamoto, découlent de cette tradition, descendant respectivement des empereurs Kammu et Seiwa.

Si l'ère Heian est pour la cour impériale une période de paix et de prospérité, les provinces, en revanche, étaient secouées de révoltes paysannes dues aux lourds impôts, réprimées par les "kokushi" (gouverneurs de provinces nommés par le gouvernement impérial). Les petits fermiers se placèrent sous la protection de puissantes familles de propriétaires terriens, qui de ce fait s'enrichirent et furent bientôt en mesure de recruter des armées privées, constituées de guerriers professionnels, mais aussi de simples civils (paysans, artisans, citadins).

Ces armées conféraient une certaine puissance et une indépendance grandissante à ces propriétaires terriens, riches, mais dénigrés par l'aristocratie de Kyōto, et leur permettaient de défendre leurs terres contre les menaces diverses, mais aussi de s'étendre aux dépens de leurs voisins. De plus, certains tentaient de se dégager de la tutelle du gouvernement central, ce qui provoqua des révoltes auxquelles prirent part certains des premiers gouvernements samouraïs.

En 935, Taira no Masakado, gouverneur de la province de Shimosa, tua son oncle Kunika et rallia à lui de nombreux guerriers, gagnant ainsi le contrôle de la quasi-totalité du Kantō et s'autoproclama empereur en 939. La même année, sur les côtes de la mer intérieure, Fujiwara no Sumitomo rassembla des "wakō" (pirates) et se révolta également. 

Le gouvernement n'eut pas de mal à réprimer ces premières révoltes samouraïs, se contentant d'engager d'autres clans pour lutter contre les premiers, lors de ce qui fut désigné comme Rébellion de Jōhei Tengyō.

En 1028, Taira no Tadatsune se révolta également et prit le contrôle du Kantō. La cour tarda alors à réagir, selon Louis Frédéric (, « les forces impériales [étaient] trop faibles pour intervenir efficacement contre lui ». Au bout de quatre mois, cependant, la cour envoya contre lui Taira no Naokata, qui fut vaincu. En 1031, Minamoto no Yorinobu se joignit aux forces de pacification impériale, obligea Tadatsune à se rendre, et prit le contrôle du Kantō.

Par la suite, les familles de samouraïs les plus influentes, notamment les Taira et les Minamoto, furent appelées à la cour pour assurer la sécurité de l'empereur et de l'aristocratie, avec qui ils tissèrent peu à peu des liens, bien que gardant un statut très bas. Les "jōkō", notamment, s'entouraient de gardes du corps samouraïs à demeure dans son palais, les "hokumen no bushi" (ce qu'on peut traduire par « samouraïs du côté nord »).

Dans les provinces du Tōhoku, la partie nord de l'île de Honshū, plus récemment colonisée et loin de la capitale, des seigneurs tentaient d'échapper à l'influence de la cour. En 1051, Abe no Yoritoki se souleva et la province de Mutsu fut secouée par les affrontements de la guerre de Zenkunen, qui dura en réalité jusqu'en 1062, le général des forces impériales, Minamoto no Yoriyoshi (fils de Yorinobu) ayant fait appel au clan Kiyohara de la province de Dewa. La cour attribua les biens du clan Abe à ces derniers, et, lorsqu'en 1083, Minamoto no Yoshiie, fils de Yoriyoshi, fut nommé juge dans une querelle interne des Kiyohara, il en profita pour les détruire au cours de ce qu'on appelle la guerre de Gosannen. Estimant qu'il avait agi pour des raisons personnelles, la cour refusa de lui attribuer une récompense et il dut prélever des parcelles sur son propre domaine pour payer ses hommes. Selon Mitsuo Kure ("Samouraïs", ), cet acte le rendit très populaire et de nombreuses familles de samouraïs se mirent à son service.

Ces premières rébellions samouraïs, actions isolées et menées loin de la cour eurent finalement peu d'impact dans l'arrivée au pouvoir à la fin du . En revanche, les clans de samouraïs présents à la cour tirèrent parti de la lutte de pouvoir entre l'empereur Go-Shirakawa et l'empereur retiré Sutoku en 1156. À l'issue de ce qui est connu comme la rébellion de Hōgen, l'influence des régents Fujiwara diminua considérablement et les clans Taira et Minamoto parvinrent à gagner des positions importantes à la cour.

En 1159, lorsque Minamoto no Yoshitomo et Fujiwara no Nobuyori tentèrent un coup d'État connu sous le nom de rébellion de Heiji, Taira no Kiyomori écrasa les Minamoto, massacrant une bonne partie du clan et entama une ascension qui l'amena en 1167 au poste de "dajō-daijin", premier ministre.

Cependant, en 1180 éclata la guerre de Gempei, une guerre de succession au trône impérial, les Minamoto reconstitués soutenant un candidat différent de celui des Taira. Au terme de cinq ans de guerre, les Taira furent finalement éliminés et Minamoto no Yoritomo mit en place le premier "bakufu", avant d'être nommé shogun en 1192. Pour la première fois, le Japon était dirigé par des samouraïs, et le resta jusqu'en 1868.

En 1185, les clans Taira et Minamoto s'affrontent dans la baie de Dan-no-ura. Lors de cette bataille décisive, le jeune empereur Antoku, âgé de six ans, sentant la défaite finale, plonge dans les eaux avec sa grand-mère pour se donner la mort plutôt que de subir le déshonneur d'une capture. Plusieurs samouraïs imitent son geste. La légende prétend que les guerriers Taira se sont réincarnés en crabes, d'où cet ornement qu'on retrouve quelquefois sur des casques de samouraïs. Encore aujourd'hui, les pêcheurs qui attrapent des crabes dont la carapace évoque un visage les rejettent à l'eau. Il s'agit en fait d'une espèce endémique: le "heikegani".

Avec la pacification de la période Edo, la fonction combattante des guerriers diminue et ceux-ci deviennent des fonctionnaires. Ils vont laisser le côté guerrier pour les cérémonies, et commencer à s'intéresser aux arts (surtout l'écriture). Néanmoins, probablement pour se redonner de la valeur, des règles très strictes sont codifiées, sous le nom de bushidō (« voie du guerrier »). Le suicide rituel du seppuku — aussi connu sous le nom de « hara-kiri » (littéralement « ouvrir le ventre ») — devra être interdit à certaines périodes par le shogun (seigneur militaire du Japon).

En effet, pour sauvegarder son honneur, un samouraï devait se faire seppuku s'il arrivait malheur à son maître, à sa famille, ou simplement s'il avait fait une faute grave, son seigneur pouvait lui commander à n'importe quel moment le seppuku s'il ne s'estimait pas satisfait. Ce rite provoquait parfois des ravages dans les rangs des samouraïs.

À la fin du , les samouraïs représentent environ 7% de la population japonaise.

La période des Tokugawa amène un certain renfermement du Japon sur lui-même, peu ouvert aux pays étrangers. Cet isolement prend fin avec l'intervention du commodore Matthew Perry qui force le pays à s'ouvrir au commerce extérieur à partir de 1854. Des changements majeurs surviennent alors, avec notamment la reprise en main du pays par l'empereur. 

La restauration de Meiji en 1867 entraîne avec elle toute une série de mesures. Les samouraïs sont également frappés par les réformes. Privés de leurs droits, ils se révoltent avant d'être écrasés par l'armée impériale en 1874 et lors de la rébellion de Satsuma en 1877. Le passage à l'ère moderne fit qu'il fut décidé de conserver l'héritage culturel des différents arts utilisés par les samouraïs au sein de la Dai nippon butoku kai créée en 1895.
Le bouddhisme zen a fortement influencé les samouraïs. Voir par exemple le samouraï Suzuki Shōsan, devenu moine zen à 42 ans.

En 1913, le moine Kaiten Nukariya a écrit un ouvrage sur cette influence du zen : "The Religion of the Samurai A Study of Zen Philosophy and Discipline in China and Japan".

Le shintoïsme a eu une certaine influence, ainsi que le confucianisme.

Dans la tradition samouraï, un fils de samouraï était soumis à une discipline très stricte. Le temps des caresses maternelles était douloureusement court. Avant même d'avoir vêtu son premier pantalon, on l'avait soustrait autant que possible aux tendres contacts et on lui avait appris à réprimer les élans affectueux de l'enfance. Tout plaisir oisif était rigoureusement mesuré et le confort lui-même proscrit, sauf en cas de maladie. Ainsi, dès le moment où il savait parler, on lui enjoignait de considérer le devoir comme le seul guide de son existence, le contrôle de soi comme la première règle de conduite, la souffrance et la mort comme des accidents sans importance du point de vue individuel. 

Cette éducation austère n'allait pas sans impératifs beaucoup plus contraignants, destinés à développer une impassibilité totale dont l'enfant ne devait jamais se départir, hormis dans l'intimité de la maison. On accoutumait les garçonnets à la vue du sang en les forçant à assister à des exécutions. Ils ne devaient manifester aucune émotion. De retour chez eux, on les obligeait à manger un grand plat de riz coloré en rouge sang par l'adjonction d'un jus de prunes salées, afin de réprimer tout sentiment d'horreur secret. Des épreuves encore plus pénibles pouvaient être imposées, même aux très jeunes enfants. À titre d'exemple, on les contraignait à se rendre seuls, à minuit, sur les lieux du supplice, et à en rapporter la tête d'un des condamnés pour preuve de leur courage. En effet, la crainte des morts était jugée tout aussi méprisable de la part d'un samouraï que celle des vivants. Le jeune samouraï devait apprendre à se prémunir contre toutes les peurs. Dans toutes ces épreuves, la plus parfaite maîtrise de soi était exigée. Aucune fanfaronnade n'aurait été tolérée avec plus d'indulgence que le moindre signe de lâcheté. 

En grandissant, l'enfant devait se satisfaire, en guise de distractions, de ces exercices physiques qui, très vite et pour le restant de ses jours, préparent le samouraï à la guerre : "kenjutsu", jujutsu, "bajutsu", "kyujutsu", respectivement art du sabre, lutte, art équestre, tir à l'arc. On lui choisissait des compagnons parmi les fils des domestiques, plus âgés que lui et sélectionnés pour leur habileté dans l'exercice des arts martiaux. Ses repas, bien qu'abondants, n'étaient pas très raffinés, ses tenues légères et rudimentaires, sauf à l'occasion des grandes cérémonies. Lorsqu'il étudiait, en hiver, s'il arrivait qu'il eût si froid aux mains qu'il ne puisse plus se servir de son pinceau, on lui ordonnait de plonger dans l'eau glacée pour rétablir la circulation. Si le gel engourdissait les pieds, on l'obligeait à courir dans la neige. Plus rigoureux était encore l'entraînement militaire proprement dit : l'enfant apprenait de bonne heure que la petite épée à sa ceinture n'était ni un ornement, ni un jouet.

Pour l'éducation religieuse du jeune samouraï, on lui apprenait à vénérer les dieux anciens et les esprits de ses ancêtres. On l'initiait à la foi et à la philosophie bouddhiques et on lui enseignait l'éthique chinoise. Ceci est à nuancer, du fait que tel clan ou telle famille ou encore telle "koryu" (école d'arts martiaux) tendaient à une vision shintoïste, bouddhique ou confucianiste. Ainsi la Tenshin shōden katori shintō-ryū incline vers le shintoïsme tandis que la Hyoho niten ichi ryu ouvre son texte majeur sur une invocation à une déité bouddhiste en poursuivant que s'il faut vénérer les dieux, il ne faut pas pour autant attendre d'eux la victoire.

Peu à peu, à mesure qu'il passait de l'enfance à l'adolescence, la surveillance à laquelle il était soumis allait s'amenuisant. On le laissait de plus en plus libre d'agir selon son propre jugement, avec la certitude qu'on ne lui pardonnerait pas la moindre erreur, qu'il se repentirait toute sa vie d'une offense grave et qu'un reproche mérité était plus à redouter que la mort même. 

Le samouraï apprenait son métier au sein d'écoles anciennes dispensant une formation aux armes, à la stratégie, au renseignement et aux divers aspects de l'art de la guerre. Ces "koryu", écoles anciennes, ont été le cadre qui a façonné l'excellence technique et morale du samouraï.

Le bushido ("voie du guerrier") est un ensemble de principes que devait respecter le samouraï. Un ouvrage populaire, vu comme un guide du samouraï est le "Hagakure". Il s’agit d’une compilation des pensées et enseignements de Jōchō Yamamoto, ancien samouraï vassal de Nabeshima Mitsushige.

Un samouraï n'ayant pas de rattachement à un clan ou à un daimyō (seigneur féodal) était appelé un "rōnin". Un samouraï qui était un vassal direct du shogun était appelé "hatamoto".

Cependant, tous les soldats n'étaient pas samouraïs, ceux-ci constituant une élite équivalent en quelque sorte aux chevaliers européens ; l'armée, à partir de la période Kamakura, reposait sur de larges troupes de fantassins de base nommés "ashigaru" et recrutés principalement parmi les paysans.

Le samouraï utilisait environ 40 armes avec une mention spéciale pour le "katana", grand sabre, qu'il était le seul à pouvoir porter. Il étudiait les "kobudo", les arts martiaux japonais d'avant 1868, au sein des "koryu". Il attribuait une grande importance au "katana", suivant ainsi le bushidō pour lequel le "katana" est l'âme du samouraï. Quand un enfant avait atteint l'âge de 15 ans, il pouvait obtenir un "wakizashi" (petit sabre) et un nom d'adulte lors d'une cérémonie appelée "genpuku" (元服). Lors de cette cérémonie, il devenait samouraï. Il obtenait aussi le droit à porter un "katana".

Une cordelette (souvent fabriquée à partir d'une mèche de cheveux) était souvent nouée à travers un trou dans le "tsuba" (habituellement prévu pour faire passer le "kogatana", stylet rangé dans un compartiment du fourreau), une sorte de sécurité pour "katana", permettant de manifester des intentions pacifiques, puisqu'il devenait dès lors impossible de le dégainer sans dénouer d'abord cette sécurité. 

Un "katana" et un "wakizashi" réunis sont appelés un "daisho" (littéralement : « grand » et « petit »).

Le "wakizashi" était « la lame d'honneur » d'un samouraï et il ne quittait jamais son côté. Le samouraï dormait avec l'arme sous son oreiller et l'emmenait avec lui quand il entrait dans une maison et devait laisser ses armes principales dehors.

Le "tanto" était un petit poignard, et il était porté quelquefois à la place du "wakizashi" dans un "daisho". Il était utilisé quand un samouraï devait faire seppuku ou hara-kiri (suicide). Cependant, placé dans le "keikogi" (« vêtement d'entraînement »), le "tanto" se révélait être une arme de poing très utilisée pour les assassinats ou les combats rapprochés. 

L'arme favorite du samouraï était le "yumi" (« arc »). Le "yumi" resta inchangé jusqu'à l'apparition de la poudre à canon et des fusils au . L'arc composite de style japonais avait une puissance pouvant aller jusqu'à 30Kg (environ 60lbs), même si communément sa puissance avoisinait le 20Kg. Sa taille permettait de lancer divers projectiles comme des flèches enflammées et des flèches-signaux d'une portée efficace de , et plus de quand la précision n'était pas importante. Il était ordinairement utilisé à pied derrière un "tedate" (手盾), un grand mur de bambou mobile, mais il pouvait même être utilisé à dos de cheval. La coutume de tirer à dos de cheval, "yabusame" (流鏑馬), est devenue une cérémonie shintoiste.

Le "nodachi" est un sabre d'aspect similaire au "katana", mais qui mesure environ ; il était réservé aux samouraïs les plus forts. On peut voir Kikuchiyo, personnage venant du monde paysan, en manipuler un dans le film "Les Sept Samouraïs". Ce type d'arme est adapté à la lutte contre les unités de cavalerie, et surtout contre les fantassins en armures légères. Elle ne fut toutefois jamais vraiment populaire en raison de la difficulté de son maniement (requérant davantage de force et de dextérité qu'un "katana" de taille moyenne), et du fait que le "naginata" remplissait déjà très bien ce rôle.

Certains samouraïs les utilisaient toutefois, certains pour crâner à l'instar de nombreux "kabuki-mono", et moins souvent en raison de compétences réelles dans son maniement. On notera notamment le célèbre Sasaki Kojirô et sa Monohoshizao, ainsi que Makara Jurōzaemon Naotaka, et son fameux "nodachi", Tarōtachi, mesurant pour (éléments de poignée et autres accessoires exclus). 

Au , le "yari" (lance) est également devenu une arme populaire. Il a remplacé le "naginata" sur le champ de bataille lorsque la bravoure personnelle est devenue moins importante, et les batailles, plus organisées. Le "yari" était plus simple à utiliser et plus mortel qu'un "katana". Une charge, à cheval ou à terre, était plus efficace quand une lance était utilisée, et offrait plus de 50 % de chances de vaincre un samouraï armé d'un "tachi", forme primitive de "katana" adaptée au combat monté, parfois appelé par erreur "daïkatana" dans la culture occidentale.

Dans la bataille de Shizugatake, où Shibata Katsuie fut vaincu par Toyotomi Hideyoshi (ou Hashiba Hideyoshi), les « sept lances » de Shizugatake (賤ヶ岳七本槍) ont joué un rôle crucial dans la victoire.

Jusqu'au , le tranchant des lames de "katana" était testé sur des condamnés vivants par des bourreaux payés par les samouraïs.

Les armes blanches utilisées par les samouraïs ont énormément gagné en qualité au fil des siècles, jusqu'à arriver à une qualité inégalée : les lames forgées selon la tradition japonaise sont encore aujourd'hui les meilleures que l'homme ait faites sur le plan des qualités physiques, grâce aux techniques complexes de forge et de trempe développées par les forgerons d'armes japonais, ainsi que le "tamahagane", acier spécial obtenu à base de sable ferrugineux.

Un équipement protecteur couvre le samouraï de la tête au pied. L'armure est constituée de plusieurs parties et est conçue de manière à favoriser le plus possible la mobilité du combattant.

Durant les guerres féodales, plusieurs dizaines de milliers de samouraïs pouvaient être impliqués dans les combats. Il devenait donc important de trouver un moyen de transmettre les ordres de déplacement. À cette fin, on utilisait un bâton de commandement ("saihai") qui pouvait être aperçu de loin. Il s'agissait d'un bâton orné à une extrémité d'un faisceau de poils de yak, de lamelles de papier laqué, de lanières de cuir ou de bandelettes de tissu. Le bâton était fixé à l'armure à l'aide d'une corde. Son utilisation remonte aux années 1570.

On peut également noter :
Le premier samouraï étranger, choisi par Oda Nobunaga, était un esclave africain.

Yasuke arrive du Mozambique en 1579, au service d'un très renommé jésuite italien du nom de Alessandro Valignano. Il fait aussitôt sensation lorsqu'il arrive à la capitale. De nombreuses personnes qui affirmaient l'avoir vu furent tuées pour ce qui était considéré comme un mensonge dont ils ne voulaient pas démordre. Nobunaga ayant vent de ces rumeurs et imaginant qu'il devait y avoir un subterfuge, suspectant en particulier que l'homme devait être tout simplement peint en noir, décide de le rencontrer et de faire gommer sa peau. À sa grande surprise, la couleur de l'homme était réelle, Nobunaga intéressé lui donne de l'argent en dépit du fait qu'il est toujours esclave de Valignano et le laisse repartir.

Lors des missions du Jésuite au Japon en 1581, Yasuke apprend à parler japonais et rencontre à nouveau Nobunaga qui apprécie beaucoup sa compagnie, jugeant l'homme incroyablement robuste, fort et intelligent. Il demande à Valignano, qui devait quitter le Japon cette même année, de laisser Yasuke vivre au Japon sous sa protection. Nobunaga appréciait tellement Yasuke que les gens pensèrent qu'il serait un jour nommé seigneur. Ce ne fut pas le cas, mais Yasuke devint samouraï.

En 1582, Nobunaga est défait à Kyoto par l'armée de Akechi Mitsuhide. Yasuke part alors à la rencontre de son héritier, Oda Nobutada, au château de Nijo. Nobutada à son tour attaqué, peut compter sur Yasuke qui reste un samouraï fidèle. Lorsque Mitsuhide fait tomber le château de Nijo, Yasuke n'est pas tué mais amené à Akechi Mitsuhide pour qu'il décide de son sort. On déclare qu'il n'est pas un homme, qu'il ne sait rien, n'est pas japonais, qu'il ne doit cependant pas être tué mais porté aux Jésuites indiens dont Valignano, le missionnaire italien, avait été responsable quelques années auparavant. Yasuke doit donc retourner aux jésuites indiens et nul ne sait ce qu'il est advenu de lui par la suite, aucun écrit n'ayant été retrouvé après cet événement.

Description de Yasuke dans les mémoires de Nobunaga Oda (信長公記, "Shinchōkōki") : 

Yasuke mesurait en effet ce qui à l'époque au Japon devait être extrêmement impressionnant.






</doc>
<doc id="4922" url="https://fr.wikipedia.org/wiki?curid=4922" title="Nintendo 64">
Nintendo 64

La , également connue sous les noms de code Project Reality et Ultra 64 lors de sa phase de développement, est une console de jeux vidéo de salon, sortie en 1996 (1997 en Europe), du constructeur japonais Nintendo en collaboration avec Silicon Graphics. Elle fut la dernière des consoles de cinquième génération à être sortie, en concurrence avec la Saturn et la PlayStation.

La Nintendo 64 a plusieurs particularités : c'est une console « 64-bits » contrairement à ses principales concurrentes dites « 32-bits » ; l'entreprise a préféré le support cartouche, plus rentable pour Nintendo mais plus contraignant pour le développement et plus cher que le support CD proposé par ses concurrents ; elle innove en instaurant un stick analogique sur sa manette qui se révélera indispensable pour les jeux en 3D temps réel ; elle était aussi la première console à disposer de quatre ports manettes pour les jeux multijoueurs (ne nécessitant pas d'adaptateur).

Au début des années 1990, avec le succès des consoles NES et Super Nintendo, la société Nintendo domine le marché mondial du jeu vidéo, malgré la compétition effrénée que lui livre Sega. L'émergence du CD-ROM a amené Nintendo à conclure un partenariat avec Sony pour que celui-ci développe un lecteur de CD pour la Super Nintendo, le SNES-CD, afin de concurrencer la PC-Engine de NEC et le Mega-CD de Sega. Mais un différend amène Nintendo et Sony à abandonner leur projet commun, que Sony recycle alors pour développer sa propre console, la PlayStation. Nintendo se tourne alors vers la firme néerlandaise Philips pour poursuivre son projet. L'annonce de la PlayStation en 1991, ajoutée aux échecs du Mega-CD de Sega et du CD-i de Philips, persuade alors Nintendo d'enterrer définitivement l'extension CD-ROM de la Super Nintendo et de se tourner vers le développement d'une toute nouvelle console.

Désirant court-circuiter la nouvelle génération de console 32-bits, Nintendo annonce une collaboration à long terme avec la société américaine Silicon Graphics (SGI), spécialisée dans l'imagerie de synthèse, afin de développer une console 64-bits, dont le nom de code est « "Project Reality" », à l'occasion du salon Shoshinkai le . À ce moment, Silicon Graphics venait de se bâtir une solide réputation grâce à ses stations de travail utilisées pour les effets spéciaux des films "" et "Jurassic Park", et Nintendo promet que les performances de sa future console seront largement supérieures à celles de ses concurrents.

À l'origine, James H. Clark, le fondateur de Silicon Graphics, avait proposé sa technologie à Tom Kalinske, le président de Sega of America. Après le refus de la technologie SGI par les dirigeants de Sega Japon, James H. Clark entra en contact avec Hiroshi Yamauchi, le président de Nintendo, début 1993.

L'année suivante, Nintendo commence à constituer un groupe de studios de développement chargés de concevoir les premiers jeux à sortir sur "Project Reality". Fin mars 1994, Rare et Williams Electronics Games (avec sa filiale Midway) sont les premières sociétés à signer. En guise d'avant-goût, Rare et Midway sortiront deux jeux sur borne d'arcade à la fin de cette même année, "Killer Instinct" et "Cruis'n USA", en présentant la technologie utilisée comme similaire à celle de la future console de salon. Si le public est impressionné, il se révèlera avec le recul que la console disposera d'un matériel bien moins puissant que celui de ces systèmes d'arcade. L'année 1994 est aussi marquée par le succès du jeu "Donkey Kong Country" sur une Super Nintendo vieillissante. Développé par Rare, le jeu est une première concrétisation de la collaboration de Nintendo avec Silicon Graphics, calculé sur les stations de travail SGI et qui repousse les limites de la console.

Le , Nintendo annonce que le support des jeux de sa future console sera la cartouche alors que toutes les autres consoles en développement ont opté pour le CD-ROM dont la capacité de stockage est beaucoup plus importante. La nouvelle refroidit les développeurs et Acclaim est le seul gros développeur à rejoindre l'aventure au cours des mois suivants. Si d'autres studios entrent dans la course par la suite, tels que Paradigm, Sierra, LucasArts, ou Electronic Arts, aucun d'entre eux n'est japonais.

Le projet est renommé « "Nintendo Ultra 64" » (NU64) quelque temps plus tard. Une photo de la console accompagnée d'une cartouche de jeu (mais pas de la manette) est diffusée en mai 1995 au cours du salon Electronic Entertainment Expo (E3).
La console est finalement présentée en version jouable lors du salon Shoshinkai le . L'événement est un succès et la nouvelle manette munie d'un stick analogique est révolutionnaire. Deux jeux sont présentés en version jouable : "Super Mario 64" et "Kirby's Air Ride 64" (qui sortira finalement en 2003 sur Nintendo GameCube). Onze autres jeux sont présentés sous forme de vidéo : "Pilotwings 64", "Super Mario Kart R", "Wave Race 64", "Star Fox 64", "", "Body Harvest", "Goldeneye 007", "Blast Dozer", "Creator", "Buggy Boogie" et "". La version jouable de "Super Mario 64", le prochain "Super Mario" réalisé en trois dimensions et supervisé par Shigeru Miyamoto, fait sensation. Nintendo annonce également un périphérique destiné à lire un nouveau type de disques magnétiques offrant une capacité de stockage bien plus importante que les cartouches.

Des rumeurs laissent entendre que la console pourrait être renommée « "Ultra Famicom" » ou « "Famicom 64" » pour le marché japonais, en continuité avec les précédentes consoles de salon de la firme. Finalement le , Nintendo annonce que sa console portera le nom de « "Nintendo 64" » et ce partout dans le monde. Le nom « "Ultra" » est abandonné afin d'éviter toute confusion avec le label « "Ultra Games" » de Konami.

Au cours de son développement, la Nintendo 64 connait d'importants retards et sa sortie est plusieurs fois repoussée. Initialement annoncée courant 1995, une date de sortie mondiale est ensuite fixée au . Elle est finalement repoussée à juin car la plupart des jeux annoncés ne sont pas prêts et Miyamoto demande plus de temps pour peaufiner "Super Mario 64". De plus, la sortie en Amérique du Nord n'est prévue que pour septembre.

Fin 1995, Nintendo France lança une campagne publicitaire ciblant directement la PlayStation et la Saturn. Celle-ci conseillait au joueur d'attendre la sortie de l' « "Ultra 64" », prévue courant 1996, plutôt que de demander une console 32-bits pour Noël.

La console sort finalement le au Japon, avec un catalogue de seulement trois jeux : "Super Mario 64", "Pilotwings 64" et "Saikyō Habu Shōgi". Malgré cela, unités sont vendues au lancement grâce, essentiellement, à "Super Mario 64" qui s'écoule pratiquement en autant d'exemplaires. Mais jusqu'à la sortie de la console aux États-Unis, aucun autre titre ne vient étoffer le catalogue.

Lorsque la Nintendo 64 sort le en Amérique du Nord, la PlayStation et la Sega Saturn sont déjà disponibles depuis plus d'un an. De plus, pour contrer Nintendo, Sony avait décidé de baisser le prix de sa console à moins de . Pour rester compétitif, Nintendo est contraint de baisser son prix à au lieu de .

La Nintendo 64 n'atteint pas l'Europe et l'Australie avant le . Ce retard entre le lancement américain et européen s'explique par le fait que Nintendo of Europe venait juste de se réorganiser et que l'approvisionnement en consoles fut très mal réalisé.

La sortie française de la console est cependant repoussée au en raison d'un plan social au sein de Nintendo France. Ce report n'a pas empêché plusieurs grandes surfaces d'importer des machines étrangères (allemandes, espagnoles, anglaises...) et de les commercialiser à un tarif moyen de 1490 F, pendant l'été 1997. La console et sa manette, en version officielle, sont vendues en France à .

Début 1997, Square fit part de sa volonté de délaisser Nintendo et de ne développer plus que sur PlayStation. C'est un coup dur pour Nintendo qui se voit ainsi privé de l'une des franchises les plus populaires au Japon : "Final Fantasy". Ce choix est certainement dû au fait que la PlayStation est devenue leader de son segment à cette date, après plus de 10 ans de leadership de Nintendo dans le secteur des consoles de jeu vidéo. De plus, le support cartouche de la Nintendo 64, limité en termes de stockage, ne permet pas à Square d'inclure les nombreuses scènes cinématiques que comportent ses nouveaux jeux.

Bien qu'elle ait bénéficié d'excellentes ventes lors de ses lancements, la Nintendo 64 n'a jamais rencontré le succès escompté : la PlayStation, bien qu'elle fût techniquement inférieure, utilisait un lecteur de CD-ROM, bénéficiant d'une image plus « Hi-Tech » aux yeux du grand public. Enfin, la difficulté de programmation et le support cartouche firent fuir presque tous les éditeurs. Il faut en effet souligner qu'un jeu sur CD-ROM se programme différemment d'un jeu sur cartouche, du fait de techniques de compression très différentes. Cela implique une relative difficulté à programmer des jeux à la fois sur Nintendo 64 et sur PlayStation, du fait des supports de stockage différents. Les éditeurs tiers ont ainsi été tentés de se concentrer sur la PlayStation devenue leader du marché. Nintendo et Rare se retrouvent donc presque les seuls éditeurs de la console et vont en profiter pour sortir les principaux jeux populaires de la console : "Mario Kart 64", "Star Fox 64", "GoldenEye 007", "F-Zero X", "Banjo-Kazooie"…

En décembre 1998, Nintendo lance sur le marché le jeu le plus attendu de sa génération : "" qui réussit à se vendre à 6 millions d'exemplaires en deux mois. C'est le jeu qui va relancer la N64 dont les ventes commençaient à s'essouffler.

En décembre 1999, Nintendo sort au Japon le 64DD, un périphérique qui devait révolutionner le marché grâce à une capacité de stockage accrue et la possibilité de se connecter à Internet. Le 64DD s'avère être un échec commercial, principalement en raison de son grand retard (il a été annoncé en 1995), et Nintendo décide d'annuler les lancements américains et européens.

Vers l'an 2000, le rythme des sorties commence à ralentir malgré quelques succès comme "Perfect Dark", "Pokémon Stadium" et "". En Europe, le dernier jeu de la console est "Mario Party 3" qui sort le 16 novembre 2001. Au Japon, c'est "Bomberman 64 2001", sorti le 20 décembre 2001, qui met un point final à la carrière de la N64. Le dernier jeu destiné au marché nord-américain est "Tony Hawk's Pro Skater 3" qui est disponible le 20 août 2002. Après cette date la production s'arrêta pour faire place à la Nintendo GameCube.

La Nintendo 64 est généralement considérée comme un semi-échec. Avec plus de 32 millions d'unités vendues et plusieurs jeux considérés comme des modèles dans leur genre, la Nintendo 64 fait beaucoup mieux que la Saturn de Sega. Néanmoins, ses ventes n'atteindront jamais celles de la PlayStation et Nintendo perd sa position dominante dans l'industrie vidéoludique au profit de Sony.

En 2003, Nintendo ressort la Nintendo 64 pour le marché chinois. Le "design" ainsi que quelques spécifications techniques sont modifiées pour l'occasion et la console est rebaptisée iQue Player. Nintendo a créé une filiale nommée iQue pour s'implanter sur ce marché. Pour éviter le piratage, la console est vendue en Chine sous la forme d'une manette (différente de la manette originale de la N64), avec déjà plusieurs jeux inclus, qui se branche directement sur la télévision. D'autres jeux peuvent se charger sur une sorte de carte mémoire. Plusieurs manettes peuvent être reliées ensemble pour jouer à plusieurs.

Depuis décembre 2006, des jeux Nintendo 64 peuvent être téléchargés pour sur la Wii grâce au service de Console virtuelle.

Cependant, depuis le début des années 2010, la Nintendo 64 connaît une véritable seconde vie. Elle est en effet très appréciée des amateurs de rétrogaming, et jouit ainsi d'une grande popularité dans ce milieu. Des jeux comme "Super Mario 64", "Pokémon Snap" ou "GoldenEye 007" connaissent ainsi un grand regain de popularité.

Révélée au salon Shoshinkai de 1995, la manette de la Nintendo 64 se distingue nettement de celles des consoles concurrentes. Imaginée par Genyo Takeda, elle se présente sous la forme d'un trident et il est possible de la tenir de trois manières différentes. Elle possède deux boutons d'action (A et B), quatre boutons auxiliaires principalement dédiés à la gestion de la caméra dans les jeux 3D (C), deux boutons latéraux (L et R), une gâchette située en dessous de la manette (Z), un stick analogique, une croix directionnelle et le traditionnel bouton start. La manette possède également un port d'extension sur lequel il est possible de brancher différents accessoires.

La plus grande innovation de la manette de la Nintendo 64 est sans nul doute son stick analogique multidirectionnel qui permet d'offrir une grande nuance dans le déplacement des caméras et des personnages. Si l'Atari 5200 proposait déjà un joystick analogique dès 1982, cette invention n'avait jusqu'alors jamais vraiment décollé et les consoles continuaient de se cantonner à la croix directionnelle.

Nintendo innove à deux niveaux par rapport aux précédents exemples de joystick analogiques. D'une part, pour une question de robustesse, le stick de la manette Nintendo 64 n'est pas vraiment analogique mais numérique. Il propose néanmoins suffisamment de niveau de sensitivité pour que la distinction soit ténue. D'autre part, le stick analogique de la manette de la Nintendo 64 n'est pas un joystick qu'il faut attraper et peut se manier directement avec le pouce.

C'est véritablement la Nintendo 64 qui popularisa ce type de contrôle.

Une autre innovation de Nintendo est d'avoir conceptualisé la manette dans l'optique de proposer en extension un "kit vibration", un module qui permet, comme son nom l'indique, de faire vibrer la manette.

Rapidement Sega sortira également une manette, le "3D Control Pad", comportant un stick analogique pour sa console Saturn à l'occasion de la sortie du jeu "Nights into Dreams".

Sony reprendra ces idées en 1997 avec la "Dual Analog Controller" puis la "DualShock", des manettes, pour la PlayStation, comportant deux sticks analogiques et qui permet (pour la "DualShock"), sans extension, de créer des vibrations. Par la suite, tous les constructeurs intégreront à leurs manettes un ou plusieurs stick analogiques.

En plus de la manette grise standard, cinq couleurs de manette sont disponibles à la sortie de la console : rouge, bleu, vert, noir et jaune. Comme pour la console, de nombreuses variantes vont ensuite s'ajouter. Au total, il existe vingt-neuf variations de la manette de la Nintendo 64.

L'ergonomie de la manette de la Nintendo 64 a suscité également des critiques, comme la fragilité du stick ou l'inaccessibilité de certains boutons sans changer la position des mains. Pour remédier à cela, le constructeur Hori a sorti une manette en 2001 ("pad Hori") à l'apparence plus classique.

La Nintendo 64 utilise la cartouche comme support pour ses jeux, avec une capacité de stockage pouvant aller de 4 à 64 MB. À sa sortie, le CD-ROM, doté d'une capacité de stockage supérieure, se généralise et est déjà adopté par les autres consoles de sa génération, la Saturn de Sega et la PlayStation de Sony.

La cartouche possède quelques avantages par rapport au CD-ROM, comme des temps de chargement quasiment inexistants, la possibilité de sauvegarder sa partie sans avoir recours à une carte mémoire et un support bien plus difficile à pirater. De plus, la cartouche est un support plus robuste que le CD-ROM, ce qui pouvait s’avérer pertinent dans le sens où la Nintendo 64 visait un public plutôt jeune. En revanche, elle présente également d'importants inconvénients, notamment un espace de stockage bien plus limité que le CD-ROM qui limite fortement l'environnement graphique et sonore des jeux, des coûts de production environ dix fois plus élevés et un cycle de production plus long, ainsi qu'une marge bénéficiaire nette moindre pour les développeurs. À cela s'ajoutent les importants frais de licence demandés par Nintendo. Finalement, cela aboutit à une différence de prix supérieure de 10 à des jeux Nintendo 64 par rapport aux jeux PlayStation et Saturn.

Hiroshi Yamauchi, président de Nintendo jusqu'en 2002, aurait imposé le support cartouche lui-même contre toute attente face à la concurrence. Il a toujours voulu garder un contrôle sur les éditeurs tiers en verrouillant toute tentative susceptible de remettre en cause son "business model" qui en a fait l'une des plus grosses fortunes du Japon. À l'origine, le président de Nintendo avait mis en place un label et diverses contraintes économiques aux éditeurs tiers afin d'assurer une qualité des titres disponibles sur leur console, ce qui a permis à Nintendo de s'imposer avec sa Nintendo « 8 bits » après le krach du jeu vidéo de 1983. L'entreprise japonaise devenant ainsi une des plus rentables du pays à la fin des années 1980 avec un quasi-monopole sur le marché mondial.

La Nintendo 64 n'y fit pas exception. Ils fixaient aux éditeurs tiers un nombre minimum de cartouches à commander. Selon certaines sources, l'ajout d'un lecteur CD aurait également rendu la console trop coûteuse par rapport à la limite que se fixait Nintendo.

Pour toutes ces raisons techniques et économiques, beaucoup de développeurs de jeux décidèrent d'abandonner Nintendo et de se tourner vers les consoles concurrentes. Le cas le plus emblématique est le départ de Square, collaborateur de longue date avec Nintendo, dont la célèbre série "Final Fantasy" se poursuivit sur PlayStation.

La Nintendo 64 est la dernière console de salon à avoir utilisé la cartouche comme support. La console suivante de Nintendo, la GameCube, utilise un format spécial de disque optique pour stocker ses jeux.

Un certain nombre d'accessoires ont été conçus pour la Nintendo 64, dont plusieurs ne sont jamais sortis en Europe. Les trois principaux accessoires s'encastrent directement sur la manette : le Kit Vibration ("Rumble Pak"), qui permet de faire vibrer la manette ; le "Controller Pak", qui est une carte mémoire ; et le "Transfer Pak" qui permet de transférer des données entre la Nintendo 64 et une cartouche Game Boy Color.

La plupart des jeux Nintendo 64 permettent de sauvegarder directement sur la cartouche, grâce à une pile, et ne nécessitent donc pas de carte mémoire. Il est possible d'accéder à un gestionnaire des fichiers de sauvegardes en appuyant sur le bouton "start" lors du lancement d'un jeu compatible avec le "Controller Pak".

Deux autres accessoires permettent d'augmenter les capacités de la console, l’"Expansion Pak" et le "64DD", sortis respectivement en 1998 et décembre 1999.

L’"Expansion Pak", insérable dans une trappe directement sur la console, augmente la mémoire de la console de à . Il est indispensable pour faire fonctionner, complètement ou partiellement, certains jeux comme "Donkey Kong 64", "Perfect Dark" et "". D'autres jeux l'utilisent pour seulement améliorer leurs graphismes.

Le "Voice Recognition Unit" (VRU) est un microphone compatible avec les jeux "Hey You, Pikachu!", "Densha de Go! 64" et "Mario Artist".

Le "Nintendo 64DD" ("Nintendo 64 Disk Drive") est une extension qui se connecte en dessous de la console via le port extension. Elle permet notamment à la Nintendo 64 de lire des disques magnétiques de 64 MB qui offrent ainsi une capacité de stockage supérieure aux cartouches de première génération. Un modem permettait également de se connecter au réseau Randnet qui offrait différentes options comme le jeu en ligne, un service de messagerie ou la navigation sur Internet. Une souris et un clavier étaient disponibles pour faciliter la navigation sur Internet. Annoncé dès 1995, le 64DD sort fin 1999 et uniquement au Japon. Très peu de jeux exploitent ses capacités.

Puissante et flexible, l'architecture de la Nintendo 64 en devient également plus complexe et la programmation se révèle difficile pour les développeurs peu familiers du support par rapport aux autres consoles de l'époque comme la PlayStation.

La Nintendo 64 est équipée de deux microprocesseurs. Le processeur, appelé « Reality Engine », appartient à la famille R4000 de MIPS Technologies et est fabriqué par NEC. Le « Reality Immersion Co-Processor » (RCP) est le coprocesseur conçu spécifiquement par Silicon Graphics pour la Nintendo 64. Il est divisé en deux composants : le « Reality Signal Processor » (RSP), qui exécute les graphismes et les sons, et le « Reality Display Processor » (RDP), qui affiche les données graphiques à partir de la liste d'affichage créée par le RSP. Pour que le RSP crée cette liste d'affichage, l'application utilise l'appel système pour charger un microcode. Le RSP est donc entièrement programmable à partir du microcode.

Bien que dotée d'un puissant processeur, la Nintendo 64 avait des limitations techniques dues à des causes multiples. L'une des principales critiques à l'encontre de la Nintendo 64 fut le rendu flou des textures, contrastant avec l'aspect pixelisé des graphismes de la PlayStation. Si les techniques d’"anti-aliasing" et de "MIP mapping" permettaient de lisser les textures quand la vue s'en éloignait ou s'en rapprochait, la capacité limitée des cartouches et la très faible mémoire cache des textures aboutissaient souvent à ce rendu flou qui devint une marque caractéristique de la console. Pour masquer les problèmes de "clipping", de nombreux jeux étaient noyés dans un brouillard permanent. En ajoutant la capacité limitée de stockage audio et l'architecture trop complexe, il en résulte que de nombreux jeux Nintendo 64 ont une qualité sonore et visuelle inférieure à leurs homologues sur PlayStation.

Néanmoins, en modifiant intelligemment le microcode et en utilisant diverses astuces, il était possible de surpasser ces limitations et d'exploiter tout le potentiel de la Nintendo 64. Les studios de développement Factor 5 et Rare furent notamment réputés pour avoir su exploiter toute la puissance de la console.






Bien que la Nintendo 64 se soit beaucoup moins bien vendue que la PlayStation ou même la Super Nintendo, 51 jeux ont dépassé le million d'exemplaires vendus. Les plus grands succès commerciaux ont tous été édités par Nintendo, mais une partie non négligeable d'entre eux ont été développés par Rare. Dans la liste des meilleures ventes, le premier jeu d'éditeur tiers, "WCW/nWo Revenge" de THQ, n'apparait qu'à la .

La Nintendo 64 fut délaissée par les éditeurs tiers ce qui explique que son nombre de jeux soit bien inférieur à celui de la PlayStation. Par ailleurs les jeux sortirent souvent au compte goutte et furent souvent retardés voire annulés.

Les copieurs, malgré le support cartouche peu pratique à pirater, ont dû faire face à la politique anti-copie très active de Nintendo, et n'ont donc pas connu un très grand succès. Ils se connectaient par le port extension sous la console, dont les derniers modèles ("Nintendo 64 Pikachu") sont d'ailleurs dépourvus. L'accessoire permet de copier, gérer des sauvegardes, des codes de triches ("voir Action Replay"), programmer

Cinq copieurs ont été fabriqués : 








</doc>
<doc id="4923" url="https://fr.wikipedia.org/wiki?curid=4923" title="PlayStation 2">
PlayStation 2

La PlayStation 2 (abrégé officiellement PS2) est une console de jeux vidéo de sixième génération commercialisée par Sony Computer Entertainment, filiale de Sony. Elle est sortie le 4 mars 2000 au Japon, le 26 octobre 2000 en Amérique du Nord, le 24 novembre 2000 en Europe et le 30 novembre 2000 en Australie. La console était en concurrence avec la Dreamcast de Sega, la GameCube de Nintendo et la Xbox de Microsoft.

La PlayStation 2 a succédé à la PlayStation dans la gamme du même nom. Elle a connu un succès immédiat avec un million d'exemplaires vendus en quelques jours au Japon. La PlayStation 2 atteint un total de 150 millions d'exemplaires expédiés en date du 31 janvier 2011, ce qui en fait la console de salon la plus vendue de l'histoire des jeux vidéo. Sony affirme qu'il existe titres de jeu vidéo disponibles sur la console et que 1,52 milliard d'exemplaires de ces titres ont été vendus depuis le lancement de la console. Durant fin de l'année 2009, alors que la console est commercialisée depuis près d'une décennie, Sony explique que la PlayStation 2 restera sur le marché tant qu'il y aura des acheteurs pour ses jeux. Celle-ci est suivie par la PlayStation 3 en 2006.

La PlayStation 2 est aujourd'hui la console la plus vendue de l'histoire du jeu vidéo.

Plus de douze ans après son lancement, Sony annonce officiellement l'arrêt de la distribution de la PlayStation 2 au Japon, le 28 décembre 2012 (elle continuait toujours de se vendre honorablement, figurant encore dans le Top 10) et dans le reste du monde, le 4 janvier 2013. 

Une page se tourne dans l'histoire du jeu vidéo et pour le constructeur japonais, qui avait fait sensation avec la sortie de cette console en mars 2000, à une époque où inclure notamment de série la lecture des DVD (format alors tout récent) dans ce type d'appareil était une innovation importante.

Le développement de la PlayStation 2 est mené par Ken Kutaragi. La console est officiellement annoncée au salon E3 le 11 mai 1999. L'apparence de la console et la date de sortie japonaise sont dévoilées au salon Tokyo Game Show en septembre 1999. La console devait initialement sortir en France le 26 octobre 2000 mais son lancement sera repoussé au 24 novembre 2000.

La PlayStation 2 est lancée le 4 mars 2000 au Japon, le 26 octobre 2000 en Amérique du Nord, le 24 novembre 2000 en Europe et le 30 novembre 2000 en Australie. Le lancement est marqué par l'engouement du public et des retards d'approvisionnement. Au Japon, les consoles proposées sont parties en 48 heures. Des difficultés de production amènent Sony à fournir deux fois moins de consoles que prévu pour les lancements américain et européen. En France, unités sont proposées avant Noël et seules la moitié sont disponibles au jour du lancement (dont unités réservées). Au 23 mars 2001, Sony a déjà écoulé 10 millions de machines dans le monde.

Au lancement de la console, dix jeux sont disponibles au Japon et vingt-neuf en Amérique du Nord, dont "Dead or Alive 2", "Dynasty Warriors 2", "FIFA 2001", "Kessen", "Madden NFL 2001", "Midnight Club", "Ridge Racer V", "SSX", "Tekken Tag Tournament", "TimeSplitters" et "Unreal Tournament". 
La console sort à Hong Kong, à Singapour, en Thaïlande et en Malaisie en décembre 2001, à Taïwan en janvier 2002, en Corée du Sud le 22 février 2002 et en Chine en décembre 2003. La « PlayStation 2 "Slim" », un nouveau modèle à la silhouette affinée (dénommé PlayStation 2 "Slim") est introduit le novembre 2004 en Amérique du Nord et en Europe et le 3 novembre 2004 au Japon.

Les ventes de la PlayStation 2 sont estimées à un peu plus de 150 millions d'exemplaires en 2011. La barre des 100 millions d'unités distribuées est atteinte en novembre 2005, cinq ans et neuf mois après le lancement de la console au Japon. La PlayStation, première console de salon à avoir atteint ce cap, a mis neuf ans et six mois en comparaison. La répartition des ventes sur les trois pôles du marché est d'environ 40 % pour l'Amérique du Nord, 38 % pour l'Europe et 22 % pour le Japon (et l'Asie). En 2004, la console occupe 70 % du marché des consoles de jeu vidéo.

Le prix de la console au lancement est fixé à au Japon, à aux États-Unis et à en France. Le prix est ramené à en juin 2001, à en septembre 2001, à en juillet 2003, à en juin 2004, à en août 2005, à en août 2006 et à en avril 2009. Fin 2010, elle est vendue en pack avec "Gran Turismo 4" à en magasins.

Le 7 novembre 2012, Sony a annoncé sur Twitter l'arrêt très prochain de la production de la PlayStation 2. La dernière usine en activité a fermé ses portes quelques heures plus tôt. La production de la console est arrêtée le 30 novembre 2012 et Sony annonce cesser toute distribution sur le territoire japonais à compter du 28 décembre 2012. Le 7 janvier 2013 la production et la distribution de la PS2 est arrêtée dans le reste du monde.

La PlayStation 2 présente de nombreux processeurs, ce qui en fait une console assez difficile à programmer. Il est indispensable d’utiliser ces différents processeurs en parallèle pour tirer les performances maximales de la console. Dans les grandes lignes, la Playstation 2 comprend trois processeurs principaux : l' "Emotion Engine", le "Graphics Synthetiser" et le "I/O Processor". Ceux-ci jouent respectivement le rôle de processeur principal (CPU), de carte graphique (GPU) et de contrôleur d'entrée-sortie.

L' "Emotion Engine" (EE) est le processeur central de la console. Il contient plusieurs sous-processeurs et circuits spécialisées, qui sont reliés par un bus interne de 128 bits, cadencé à 150MHz. Le sous-processeur principal est un processeur SIMD 128 bits, cadencé à 300MHz. Il est secondé par deux unités de calcul vectoriel 128 bits, nommées VU0 et VU1. Le EE, VU0, VU1 et l'IOP sont programmables, sur deux niveaux (microcode et macrocode) pour les unités vectorielles. Le FPU et les VU0 et VU1 permettent à la console d'atteindre 6,2 GFLOPS. La PS2 est capable de calculer 66 millions de polygones nu par seconde. Outre des processeurs programmables, l"'Emotion Engine" comprend aussi des circuits de communication avec les entrées-sorties et la mémoire, avec la présence d'un contrôleur DMA intégré. On peut aussi noter que l' "Emotion Engine" intègre un décodeur de flux MPEG utilisé pour la lecture des vidéos (beaucoup utilisées pour les cinématiques dans les jeux de l'époque). Il s'agit d'un circuit spécialisé, câblé directement en hardware.

Le "Graphic Synthesizer" (GS) est un circuit qui s’occupe du rendu graphique 3D (rastérisation à partir de primitives), et qui est strictement équivalent à une carte graphique. Il contient un processeur graphique (GPU), couplé à une petite mémoire vidéo. Le processeurs graphique comprend 16 unités programmables séparées, appelées "pixels processors", cadencées à 150 MHz. La mémoire vidéo est une petite mémoire DRAM de 4 mébioctets. Le bus qui relie le processeur graphique et la mémoire vidéo a une largeur de 1024 bits en lecture et 1024 bits en écriture.

Le "I/O Processor" (IOP) gère les entrées/sorties et assure la compatibilité avec la PlayStation. Celui-ci communique avec divers contrôleurs esclaves, qui gèrent chacun une entrée-sortie précise. Par exemple, la console intègre une carte son appelée *"Sound Processor Unit" (SPU), un lecteur DVD, un modem et des cartes PCMIA. Fait intéressant, le "I/O Processor" intègre aussi un processeur qui exécute les jeux Playstation 1. Ce processeur, dédié à l'"émulation" est un processeur MIPS cadencé à 34 MHz. Il n'est pas utilisé pour l’exécution de jeux Playstation 2.

La console dispose d'une ludothèque de plus de 8000 jeux, où sont représentés des séries populaires comme "la série Ratchet and Clank", "Dragon Quest", "Jak and Daxter", "God of War", "Sly", "Final Fantasy", "Grand Theft Auto", "Kingdom Hearts", "Tomb Raider," "Metal Gear Solid", "Medal of Honor", "Need for Speed", "SSX", "Pro Evolution Soccer" et "Tekken". Les trois jeux les plus vendus sur le support sont "" (20 millions d'exemplaires), "" et "" (14 millions d'unités). "" est le premier titre de la console à dépasser la barre du million d'exemplaires vendus, un an après la sortie de la console. En mars 2007, 1,2 milliard de copies de jeux PlayStation 2 ont été distribués à travers le monde. Les jeux PlayStation 2 sont gravés au format DVD-ROM ou CD-ROM. Les versions « éditeur » des jeux, réservées aux professionnels et notamment destinées aux démonstrations en magasin, sont communément appelées "blue disc".

La console est rétrocompatible avec le catalogue de jeux PlayStation ainsi que divers accessoires de la machine (manette, carte mémoire, etc).

Un adaptateur réseau permet de connecter la console au réseau afin d'accéder à des parties multijoueur en ligne. Le module matériel se branche sur la baie d'extension à l'arrière de la console : il est doté d'une carte réseau ethernet. Une connexion internet haut débit est requise. L'adaptateur est à l'origine vendu séparément puis directement intégré dans les modèles PlayStation 2 "Slim". Il n'est plus fabriqué depuis 2006. "" et "" sont des exemples de jeux en ligne populaires sur la console. Contrairement à la PS3 qui a son service de jeu en réseau centralisé (le PlayStation network), les jeux PS2 ont chacun leur réseau dédié indépendant.

Pro Evolution Soccer 2014, sorti le est officiellement le dernier jeu, à l'heure actuelle, paru sur cette console. Le dernier jeu japonais commercialisé est la dernière extension de Final Fantasy XI : Explorateurs d'Adoulin qui est disponible au Japon en mars 2013.

Sony a lancé un kit qui permet d’installer le système d'exploitation Linux sur la console. Le kit comprend un disque dur de , une souris, un clavier, une carte ethernet et deux DVD, incluant une distribution Linux. Des chercheurs américains du National Center for Supercomputing Applications de l'Université de l'Illinois ont mené une étude de faisabilité pour créer une grappe de 70 PlayStation 2 afin d'utiliser la puissance brute de calcul à des fins scientifiques.

En novembre 2004, une version compacte de la PlayStation 2 est commercialisée. Il s'agit de la troisième modification importante apportée à la PS2. Elle a pour numéro de version : "V12, SCPH-70000". Sony a définitivement arrêté la fabrication de l'ancien modèle PS2, et seules les versions compactes sont produites jusqu'à la fin officielle de la production, le 30 novembre 2012. À sa sortie, cette version "allégée" fut largement dénommée "PStwo" par la presse spécialisée, mais cette terminologie n'a cependant jamais été utilisée de façon officielle par Sony (contrairement au modèle "slim" de la PS1 qui fut, lui, bien nommé officiellement "PSone" par le constructeur japonais).

Les changements apportés concernent la modification du design pour obtenir une machine plus compacte et nettement plus fine. Ainsi, le chargement des disques qui s'effectuait par le biais d'un tiroir sur la version précédente (chargement frontal, comme sur les platines DVD), s'effectue désormais sur le dessus de la console (comme sur la PS1). De dimensions , le volume a été réduit d’environ 75 % et son poids passe de plus de à . Cette modification est associée à l'externalisation du boitier d'alimentation 220v, l'alimentation délivre une tension continue de 8.5v (pour un courant d'environ ), et de la suppression du bouton ON/OFF (marche/arrêt) situé derrière la machine; celle-ci reste donc constamment en veille si elle n'est pas débranchée. Elle concerne également l'ajout d'un port Ethernet pour le jeu en réseau (le modèle original demandant l'ajout d'une extension). Il devient alors impossible d'installer un disque dur par les voies officielles (même si des adaptateurs à souder ont été créés, les points de connexion étant encore présents sur la carte mère)

Une nouvelle révision de la PlayStation 2 Slim sort en 2007 au Japon (et 2008 en Europe et aux USA) pour numéro de version V18 SCPH-9000x, en plus de quelques changements cosmétiques au niveau du design visant à la rendre plus moderne, elle supprime désormais le boitier d'alimentation 220V en l'intégrant directement à l'intérieur de la console (à l'image de la FAT). Sony en profite pour corriger plusieurs défauts de conceptions tels que des problèmes de surchauffe et la console devient également légèrement plus silencieuse. Cependant, certains utilisateurs constatent que cette révision de modèle n'est plus capable de gérer la protection des jeux PS1 protégés, les bloquant au moment de l'accès à l'écran d'accueil ; seuls les jeux PS1 non protégés restent alors jouables. Il s'agit là de la dernière révision de la PlayStation 2 jusqu'à l’arrêt de la production en janvier 2013.


La PSX est une station multimédia qui fait office de console de jeu PlayStation et PlayStation 2, de graveur DVD, de tuner Télé et de numériscope grâce à un disque dur intégré. La PSX est lancée le 13 décembre 2003 au Japon, au prix initial de (160 GB) et (250 GB). Malgré des baisses de prix successives, les ventes sont restées confidentielles et la production est arrêtée en février 2005.

La manette de jeu officielle, la DualShock 2, reprend l'apparence globale de la DualShock. Les boutons sont désormais analogiques (sauf L3, R3 et Analog) et la manette produit deux niveaux de vibrations supplémentaires. Accompagnée de celle-ci, la carte mémoire "Memory Card" (8 MB), qui intègre la technologie "MagicGate", est vendue séparément. D'autres accessoires sont proposés : une télécommande, un adaptateur réseau, un disque dur, le micro-casque HeadSet, les micros SingStar, les guitares Guitar Hero, des volants à retour de force, le pistolet optique GunCon 2, le clavier-contrôleur NetPlay Controller, le multiplicateur de manette Multitap, des câbles (AV analogique, S-Video, i.Link), le kit Linux, ou encore des socles (horizontal et vertical).

L'EyeToy est une petite caméra développée par Sony qui se connecte au port USB de la console et permet d'interagir avec l'univers de jeu en réalisant des mouvements avec le corps. Lancé en Europe le 9 juillet 2003 en pack avec le jeu "", l'accessoire connait un certain succès (plus d'un million de vente en octobre 2003) et tout une gamme de jeux spécialement conçus pour le périphérique a vu le jour. La caméra peut aussi être utilisée comme webcam sur un ordinateur grâce à sa connectique USB et les pilotes appropriés. Sony a également commercialisé des "buzzer"s dans le cadre de la série "Buzz!". Vendus par quatre, ils sont munis de cinq boutons, dont quatre de couleurs, qui permettent d'interagir avec les jeux comme dans un jeu télévisé.





</doc>
<doc id="4924" url="https://fr.wikipedia.org/wiki?curid=4924" title="Londres">
Londres

Londres ( ; en anglais "London" ), située dans le Sud-Est de la Grande-Bretagne, est la capitale et la plus grande ville de l'Angleterre et du Royaume-Uni. Longtemps capitale de l'Empire britannique, elle est désormais le siège du Commonwealth of Nations.

Fondée il y a presque par les Romains sous le nom de "Londinium", Londres était au la ville la plus peuplée du monde. Bien que largement dépassée dans ce domaine par de nombreuses mégapoles, elle reste une métropole de tout premier plan, en raison de son rayonnement et de sa puissance économique, dû notamment à sa place de premier centre financier mondial. Londres se place dans le rang des grands centres financiers et culturels du monde avec New York et Hong Kong, cette trilogie est appelée par les médias anglophones « Nylonkong ».

La région de Londres, composée de l"Inner London" et de l"Outer London", comptait environ en 2015 et réalise un cinquième du produit intérieur brut du Royaume-Uni. En 2015, l'aire urbaine de Londres comptait et son aire métropolitaine . En Europe, seules les agglomérations de Moscou, Istanbul et Paris ont un poids démographique comparable. Ses habitants s'appellent les "Londoniens" (en anglais : "Londoners").

Londres, la seule ville du monde à ce jour à avoir organisé trois fois les Jeux olympiques (1908, 1948, 2012), est dynamique et très diverse sur le plan culturel. Elle joue un rôle important dans l'art et dans la mode. Elle reçoit de touristes par an et compte quatre sites inscrits au patrimoine mondial ainsi que de nombreux monuments emblématiques : le palais de Westminster, le "Tower Bridge", la tour de Londres, l'abbaye de Westminster, le palais de Buckingham, ainsi que des institutions renommées comme le "British Museum" ou la "National Gallery".

La dénomination courante Londres peut désigner plusieurs ensembles géographiques ou administratifs différents, pouvant parfois porter à confusion.

L'emploi le plus courant fait référence au Grand Londres ("Greater London"), une des neuf subdivisions régionales de l'Angleterre, formé du territoire sous l'autorité du "Greater London Authority" et du maire de Londres. Le Grand Londres est considéré comme une région NUTS-1 au sein de l'Union européenne. C'est cet ensemble d'environ pour d'habitants qui est couramment désigné lorsque l'on parle de la capitale britannique. Le Grand Londres est divisé en deux zones, Inner London et Outer London. Les deux zones sont considérées des régions NUTS-2. Cependant, le Grand Londres n'est pas officiellement une cité, dont le statut, strictement défini au Royaume-Uni, est attribué à une ville par le monarque britannique sur des critères précis. Avant sa création en 1965, le territoire du Grand Londres faisait partie des comtés du Kent, Middlesex, Surrey, Essex et du Hertfordshire.

La cité de Londres ("City of London", abrégé en "City", ou bien "Square Mile" en référence à sa superficie de 1 mile carré), située au cœur du Grand Londres, correspond à la définition historique de Londres. C'est là que la ville moderne est née et c'est aujourd'hui le plus ancien quartier de la capitale. C'est également une circonscription à part entière avec un statut spécial. La cité de Londres et le reste du Grand Londres forment deux régions dites de ("Lieutenancy areas") différentes.
La vaste agglomération londonienne peut être décrite par la région urbaine de Londres, qui correspond à la zone occupée par les banlieues, et qui occupe un territoire à peu près similaire à la région du Grand Londres mais avec une population légèrement supérieure. Au-delà de la région urbaine se trouve l'aire urbaine de Londres ("London commuter belt" ou "London Metropolitain Area") qui regroupe les territoires habités par des personnes se déplaçant quotidiennement ("commuters") pour aller travailler à Londres. La région urbaine de Londres s'est considérablement agrandie durant l'époque victorienne puis de nouveau pendant l'entre-deux-guerres. Son expansion s'est arrêtée dans les années 1940 à cause de la Seconde Guerre mondiale et de la politique dite de la ceinture verte et sa superficie n'a pas beaucoup évolué depuis. Les limites du district de la Metropolitan Police et de la zone desservie par les transports londoniens ont évolué au fil du temps mais correspondent aujourd'hui approximativement à celle du Grand Londres.

D'autres termes tels que "Inner London", "Outer London", "Central London", "North London", "South London", "East London", "East End of London", "West London" ou bien "West End of London" sont parfois utilisés, non traduits, pour désigner des quartiers, des unités statistiques ou des circonscriptions de Londres,

Contrairement à de nombreuses autres capitales, le statut de « capitale du Royaume-Uni » de Londres n'a jamais été officiellement accordé à la ville par décret ou par charte écrite. Sa position actuelle s'est établie par convention constitutionnelle, Londres étant le siège du pouvoir britannique. Son statut de capitale "de facto" en fait un élément de la constitution non écrite du Royaume-Uni. La capitale de l'Angleterre a été transférée de Winchester à Londres après la conquête normande.

Il se peut que les Romains aient marqué le centre de "Londinium" avec la pierre de Londres, toujours visible à Cannon Street. Les coordonnées du centre de Londres (traditionnellement situé à la Croix d'Éléonore à Charing Cross, près de l'intersection de Trafalgar Square et de Whitehall) sont approximativement . Trafalgar Square est également devenu un lieu central de célébration et de manifestation.
Le Grand Londres se situe à à l'ouest de l'estuaire de la Tamise et s'étend sur une superficie de ( mondial).

L'altitude y varie du niveau de la mer jusqu'à (Biggin Hill, au sud de l'agglomération).

Le fleuve, qui traverse la ville d'ouest en est, a eu une influence majeure sur le développement de la ville. Londres a été fondée à l'origine sur la rive nord de la Tamise et n'a disposé, pendant plusieurs siècles, que d'un seul pont, le pont de Londres ("London Bridge"). Le foyer principal de la ville s'est en conséquence cantonné sur cette rive de la Tamise, jusqu'à la construction, au , d'une série d'autres ponts. La ville s'est alors étendue dans toutes les directions, cette expansion n'étant gênée par aucun obstacle naturel, dans une campagne presque dépourvue de reliefs, à l'exception de quelques collines ("Parliament Hill", "Primrose Hill").

La Tamise était autrefois plus large et moins profonde qu'aujourd'hui. Les rives du fleuve ont été massivement aménagées, la plupart des affluents ont été détournés et sont à présent souterrains, parfois transformés en égouts (ainsi, la rivière Fleet dont le nom subsiste dans "Fleet Street", l'ancienne rue des journaux). La Tamise est sujette à la marée et Londres est largement inondable. Les menaces d'inondation augmentent d'ailleurs avec le temps compte tenu de l'élévation régulière du niveau de l'eau à marée haute et de la lente inclinaison de la Grande-Bretagne (relèvement au nord, abaissement au sud) causée par un phénomène de relèvement isostatique. Un barrage, la barrière de la Tamise, a été construit à travers la Tamise à Woolwich dans les années 1970, pour pallier cette menace. En 2005 cependant, il a été suggéré la construction d'un barrage d'une quinzaine de kilomètres de long plus en aval afin de parer les risques futurs d'inondation.

On décrit souvent Londres par quartiers (Bloomsbury, Mayfair, Whitechapel par exemple). Ces noms n'ont pas d'utilisation officielle mais désignent souvent des paroisses ("parishes") ou des circonscriptions ("city wards") et sont restés en usage par tradition, chacun faisant référence à un quartier distinct avec ses propres caractéristiques mais sans délimitation officielle.

Il existe cependant une zone centrale de Londres qui possède une définition et un statut stricts, la Cité de Londres ("City of London"). Souvent appelée simplement la "City", c'est l'un des plus grands quartiers financiers ("central business district") mondiaux. La "City" possède son propre corps gouvernant et ses propres frontières, lui donnant ainsi une complète autonomie politique et administrative. Le nouveau quartier financier et commercial des "docklands" se situe à l'est de la "City" et est dominé par "Canary Wharf". L'autre quartier d'affaires se trouve dans la Cité de Westminster qui abrite également le gouvernement britannique et l'abbaye de Westminster.

Le West End est le principal quartier commerçant et regroupe les principales attractions telles que Oxford Street, Leicester Square, Covent Garden et Piccadilly Circus. West London regroupe des zones résidentielles huppées telles que Notting Hill, Knightsbridge ou le district de Kensington et Chelsea où le prix moyen d'une maison dans certains quartiers est d'environ et où une maison a été vendue de livres. D'après un classement 2007 réalisé par le groupe immobilier Knight Frank et Citi Private Bank, filiale de Citigroup, Londres est la ville la plus chère du monde dans le domaine de l'immobilier résidentiel de luxe : en moyenne par mètre carré dans ce secteur.

Un autre quartier huppé est celui de Hampstead dans le Borough de Camden, où vivent d'ailleurs de nombreuses personnalités londoniennes.

Les zones situées à l'est de Londres regroupent l"East End" et les banlieues de l'Essex. La zone appelée "East London" a vu naître le développement industriel de Londres. Les nombreux terrains abandonnés qu'on y trouve aujourd'hui sont en plein re-développement, notamment grâce au plan "Thames Gateway", qui inclut "London Riverside" et la "Lower Lea Valley", qui a pu accueillir le parc olympique ainsi que le stade des Jeux olympiques d'été de 2012. "North London" et "South London" sont également des termes utilisés pour désigner les deux zones de Londres séparées par la Tamise.

La densité de population varie considérablement à Londres. Le centre regroupe de nombreux emplois tandis que la périphérie de la ville regroupe des zones résidentielles plus ou moins densément peuplées, la densité étant plus élevée dans la proche banlieue (Inner London) que dans les banlieues plus éloignées (Outer London). Les zones densément peuplées regroupent principalement des immeubles de grande hauteur et les gratte-ciel de Londres sont concentrés dans les deux quartiers d'affaires, tels que le 30 St Mary Axe, Tower 42 et l'immeuble de la Lloyd dans la Cité de Londres, One Canada Square, 8 Canada Square et 25 Canada Square à Canary Wharf.

Récemment, la construction de très grands bâtiments a été encouragée par le plan londonien et de nombreux hauts bâtiments devraient voir le jour, particulièrement dans la cité de Londres et à Canary Wharf. Le Shard London Bridge, de pour , près de London Bridge station, la tour Bishopsgate Tower de ainsi que projets de gratte-ciel de plus de de hauteur proposés ou en construction, tels que le One Blackfriars de , pourraient transformer l'apparence de la ville.

Au nombre des bâtiments remarquables de Londres figurent également la mairie à Southwark, le Muséum d'histoire naturelle de Londres, la "British Library" à Somers Town, la grande cour du "British Museum" et le "Dôme du millénaire" près de la Tamise à Canary Wharf. La centrale électrique de Battersea, aujourd'hui désaffectée mais en voie de réhabilitation, est un symbole marquant, tandis que certaines gares, notamment Saint-Pancras et Paddington, sont de bons exemples de l'architecture victorienne.

Il n'existe pas un unique style architectural permettant de décrire Londres. Différents styles et influences se sont accumulés et mélangés au fil des années. De nombreux bâtiments sont construits en briques de couleur rouge-orangé ou brun foncé comme à Downing Street, décorés de ciselures et de moulures. Nombre de quartiers sont caractérisés par des bâtiments en stuc ou blanchis à la chaux. Peu de constructions sont antérieures au grand incendie de 1666 à l'exception de quelques restes romains, de la tour de Londres et de quelques restes de l'époque Tudor. La majorité des constructions datent de l'époque édouardienne ou victorienne.

De nombreux monuments célèbrent des personnalités ou des événements qui ont marqué la ville. Le Monument, situé dans la cité de Londres, commémore le grand incendie de 1666, offrant une vaste perspective sur le cœur historique de la ville, où l'incendie a débuté. "Marble Arch" et "Wellington Arch", situées respectivement à l'extrémité nord et sud de "Park Lane", sont liées à la monarchie britannique de même que l"Albert Memorial" et le "Royal Albert Hall" à Kensington. La colonne Nelson est un monument national situé à Trafalgar Square et sert généralement à marquer le centre de Londres.

Le climat de Londres symbolise parfaitement le climat de type océanique. Les précipitations sont régulières toute l'année souvent sous forme de bruine, contrairement à l'ouest du Royaume-Uni où elles sont d'intensité plus forte. La moyenne annuelle des précipitations s'établit à , février étant le mois le plus sec de l'année. Ce niveau est inférieur à Rome ou Sydney. Londres est en fait une des capitales européennes les plus sèches, disposant de ressources d'eau par personne inférieures à celles d'Israël par exemple, l'impression de temps maussade vient surtout du fait que l'ensoleillement annuel est faible. Des villes aussi pluvieuses mais avec un ensoleillement élevé ne provoquent pas cette impression de temps maussade qu'on trouve à Londres.

Les étés sont tempérés, les jours de fortes chaleurs sont rares et les hivers sont froids mais rarement glaciaux. Le mois le plus chaud est juillet avec une température moyenne à Kew Gardens de 18.0 °C n'excédant que rarement les 33 °, quoique des niveaux plus élevés soient devenus plus fréquents récemment, les températures estivales en journée varient généralement entre 20 et . La plus haute température fut de , mesurée dans les jardins botaniques royaux de Kew, le , pendant la canicule de 2003. Le mois le plus froid est janvier avec des températures moyennes de à . La température la plus froide fut de , le à Northolt.

Les chutes de neige abondantes sont presque inconnues. Au cours des hivers les plus récents, la neige a rarement excédé un pouce d'épaisseur (soit moins de ). Ceci est notamment dû au fait que la vaste agglomération londonienne crée un microclimat, avec une chaleur enfermée par les immeubles de la ville. La nuit, la température y est parfois de 5 à supérieure aux zones environnantes. Le célèbre smog londonien, mélange de brouillard et de fumée, est devenu rarissime dans les rues de la capitale anglaise. En 1954, il avait provoqué la mort de .

Avec ses 40% d'espaces verts et aquatiques, Londres est considérée comme une des capitales les plus vertes au monde. La société d'histoire naturelle de Londres y a recensé plus de deux mille espèces de plantes à fleurs à travers la ville ainsi que d'oiseaux, de papillons et d'araignées.
Les amphibiens sont également très présents sur l’ensemble de la ville, avec les tritons, les grenouilles rousses et les crapauds notamment. Les reptiles, avec les lézards vivipares, les couleuvres et les vipères se trouvent en revanche quasi exclusivement dans l'Outer London.
La ville compte ainsi 38 sites d'intérêt scientifique particulier, naturelles nationales ainsi que naturelles locales.
Parmi la faune présente à Londres, on trouve également une population de . Ceux- ci sont nettement moins craintifs que leurs congénères de la campagne. Ils côtoient les piétons dans la rue, et élèvent leurs petits dans les jardins des maisons.

Les régions aux alentours de Londres (aujourd'hui situées à l'intérieur des frontières du Grand Londres) semblent avoir été habitées par des Bretons insulaires depuis les temps préhistoriques, mais aucune trace archéologique n'a été mise au jour au nord du pont de Londres, lieu où la ville est véritablement née et d'où elle s'est développée. Les plus anciennes traces certaines d'installations durables remontent à l'an 43 et sont dues aux Romains qui, à la suite de leur conquête de la Bretagne, y bâtissent une première ville. Ce premier campement est appelé "Londinium". Le pont de Londres se trouvait au centre du tout nouveau réseau de routes créé par les Romains et était un lieu de passage privilégié pour traverser la Tamise, ce qui a attiré de nombreux commerçants et ainsi contribué à la croissance de la ville. Londres est vite devenue un important centre d'échanges et de commerce, la Tamise permettant d'acheminer facilement des marchandises jusqu'au cœur de la ville.
Seulement 18 ans après la fondation de la ville par les Romains, la reine Boadicée, à la tête du peuple celte des Iceni, se dresse contre l'invasion romaine et prend Londres pour cible. Le gouverneur Suetonius Paulinus, alors occupé à exterminer les druides sur l'île d'Anglesey, ne peut constituer à temps une armée pour contrer l'invasion celte. La ville est partiellement évacuée, mais des milliers de commerçants sont tués. Londres est alors totalement pillée et détruite. Des fouilles archéologiques ont permis de mettre au jour la présence de débris brûlés recouvrant des pièces et des poteries datant de 60, à l'intérieur des limites de la ville romaine.

La ville est rapidement reconstruite et prospère de nouveau, à l'image du commerce en Bretagne, remplaçant Colchester en tant que capitale de la province romaine de Bretagne. Il n'existe cependant pas d'informations permettant de dater et d'expliquer le transfert de la capitale. Vers le , la ville s'entoure de murailles : le "Mur de Londres". Pendant plus d'un millénaire, les frontières de la ville sont marquées par ce mur qui délimite une zone largement englobée aujourd'hui par celle de la "City". À son apogée au , la population de "Londinium" atteint entre et suivant les sources. Lorsque l'Empire romain commence à décliner, les troupes protégeant la ville sont rappelées sur le continent, Londres commence à péricliter et sa population diminue. Il existe peu d'informations sur cette période appelée "Dark Ages of London" (« Les âges sombres de Londres »), mais après le départ des Romains de Grande-Bretagne en 410, il est largement établi qu'au , Londres est en ruine et pratiquement abandonnée.
La position privilégiée de la ville sur la Tamise en fait un lieu stratégique et vers l'an 600, les Anglo-Saxons fondent une nouvelle ville, "Lundenwic", à environ en amont de la ville romaine, à l'endroit où se trouve aujourd'hui Covent Garden. Un port de pêche et de commerce est probablement localisé à l'embouchure de la rivière Fleet. "Lundenwic" prospère jusqu'en 851, lorsque la ville est envahie et complètement rasée par les Vikings. Après cette occupation viking, Alfred le Grand rétablit la paix et fait déplacer la ville dans les murailles de la vieille cité romaine (alors appelée "Lundenburgh") en 886. La ville originale est devenue "Ealdwic" (« vieille ville »), dont le nom a survécu jusqu'à aujourd'hui pour donner Aldwych.

Ensuite, sous le contrôle de plusieurs rois anglais, Londres connaît une nouvelle phase de prospérité, devenant un lieu de pouvoir ainsi qu'un centre d'échanges et de commerce. Cependant, les raids vikings reprennent au et atteignent leur apogée en 1013, lorsque la ville fut assiégée par le Danois Knut le Grand et que le roi Æthelred le Malavisé est contraint de s'enfuir. Lors d'une contre-attaque, l'armée du roi Æthelred remporte une victoire en détruisant le pont de Londres alors que la garnison danoise se trouve dessus. Knut finit cependant par devenir roi d'Angleterre et ses descendants règnent jusqu'en 1042. Un roi saxon, Édouard le Confesseur, leur succède et refonde l'Abbaye de Westminster ainsi que le Palais de Westminster. À cette époque, Londres est devenu la cité la plus grande et la plus prospère d'Angleterre, bien que le siège du gouvernement se trouve toujours à Winchester.

Après la bataille d'Hastings, le duc de Normandie Guillaume le Conquérant est couronné roi d'Angleterre dans la toute nouvelle Abbaye de Westminster, le jour de Noël 1066. Il accorde certains privilèges aux habitants de Londres tout en construisant un château au sud-est de la ville pour maintenir le contrôle sur la population. Ce château, agrandi par les rois suivants, sert de résidence royale puis de prison et est aujourd'hui connu sous le nom de Tour de Londres.

En 1097, Guillaume le Roux commence la construction du Hall de Westminster, près de l'abbaye du même nom. Ce hall est à l'origine du palais de Westminster, la résidence royale tout au long du Moyen Âge. Westminster devient le siège de la cour royale et du gouvernement, tandis que la Cité de Londres voisine forme un centre d'échanges et de commerce prospère sous l'autorité de sa propre administration, la "Corporation of London". Les villes aux alentours se développent et forment la base du cœur de Londres moderne, remplaçant Winchester en tant que capitale du royaume d'Angleterre au .

Le 2 juin 1216, Le prince Louis (futur Louis VIII) s'empare de la ville jusqu'en 1217.

Après la défaite de l'Invincible Armada espagnole en 1588, une certaine stabilité politique en Angleterre permet à Londres de se développer davantage. En 1603, monte sur le trône d'Angleterre et s'efforce d'unifier les deux pays. Ses lois anticatholiques le rendent très impopulaire et il est victime d'une tentative d'assassinat le , la fameuse conspiration des poudres.
Plusieurs épidémies de peste noire touchent Londres au début du , culminant avec la grande peste de Londres de 1665, qui tue environ 20 % de la population. L'année suivante, le grand incendie de 1666 détruit une grande partie des maisons en bois de la ville. La reconstruction de Londres occupe toute la décennie suivante.

De 1825 à 1925, Londres est la ville la plus peuplée au monde. Cette croissance est accélérée par la construction des premières lignes de chemin de fer à Londres, rapprochant considérablement les villes avoisinantes. Porté par un essor boursier exceptionnellement rapide, ce réseau ferroviaire s'étend rapidement et permet à ces villes de croître tout en permettant à Londres de s'étendre et d'englober les villages aux alentours, à l'image de Kensington. L'apparition des premiers embouteillages en centre-ville mène à la création, en 1863, du premier système de transport souterrain au monde, le métro de Londres, accélérant encore le développement de l'urbanisation. Grâce à cette croissance rapide, Londres devient l'une des premières villes à dépasser le million d'habitants et la première à dépasser les cinq millions.

Le gouvernement local de Londres éprouve des difficultés à gérer l'expansion rapide de la ville, surtout au niveau des infrastructures. Entre 1855 et 1889, le "Metropolitan Board of Works" supervise la croissance des infrastructures. Il est remplacé par le comté de Londres, géré par le "London County Council", la première assemblée élue au niveau de la ville, jusqu'en 1965.

Le "Blitz" et les bombardements allemands de la "Luftwaffe" durant la Seconde Guerre mondiale entraînent la mort d'environ et la destruction de nombreuses habitations et bâtiments dans la ville. La reconstruction dans les années 1950, 1960 et 1970 se caractérise par une absence d'unité architecturale, typique du Londres moderne. En 1965, les limites de Londres sont modifiées pour tenir compte de l'expansion de la ville en dehors du comté de Londres. Le nouveau territoire agrandi, administré par le "Greater London Council", prend le nom de Grand Londres.

Dans les décennies qui suivent la Seconde Guerre mondiale, une large immigration provenant des pays du Commonwealth décolonisés fait de Londres une des villes européennes les plus ethniquement cosmopolites. L'intégration des nouveaux immigrants ne se fait pas toujours en douceur, avec par exemple les émeutes de Brixton dans les années 1980, mais elle se déroule mieux que dans d'autres régions britanniques. Après l'abolition du "Greater London Council" en 1987, Londres est privé d'une administration centrale jusqu'à la création, en 2000, de la "Greater London Authority" et du poste du Maire de Londres ("Mayor of London)".
Le renouveau économique des années 1980 rétablit Londres sur le devant de la scène internationale. En 2012, Londres devient la première ville à accueillir les Jeux olympiques modernes pour la troisième fois, tandis qu'en 2015 la population municipale dépasse d'habitants, son plus haut niveau depuis 1939. En 2016, Londres est la première capitale occidentale à élire un maire musulman, le Travailliste Sadiq Khan.

En tant que siège du gouvernement et principale agglomération du Royaume-Uni, la ville connaît de nombreux épisodes terroristes. L'IRA provisoire tente de mettre le gouvernement britannique sous pression au sujet des négociations en Irlande du Nord, interrompant fréquemment les activités de la ville avec des alertes à la bombe ou des attentats jusqu'au cessez-le-feu de 1997. Le , une série d'attentats est perpétrée dans les transports en commun londoniens par des kamikazes islamistes, seulement après que l'organisation des Jeux olympiques de 2012 est confiée à la ville. Le , un attentat revendiqué par l'État islamique est commis par Khalid Masood sur le pont de Westminster et dans l'enceinte du Parlement ; il fait (4 si on compte le terroriste) et une cinquantaine de blessés. La Police métropolitaine a précisé que Masood ne faisait pas partie d'une organisation terroriste et qu'il a agi seul. plus tard, une autre attaque terroriste touche le centre de la ville.

La gestion de Londres s'effectue sur deux niveaux : au niveau de la ville, sous l'autorité du "Greater London Authority" (GLA) et à un niveau plus local au sein des 33 districts londoniens.

Le "Greater London Authority" est responsable du plan londonien définissant la stratégie de développement de Londres, des services de police (" Metropolitan Police Authority"), de lutte contre les incendies ("London Fire Brigade"), de la plupart des transports ("Transport for London") et du développement économique ("London Development Agency"). Le GLA est composé du maire de Londres, qui dispose des pouvoirs exécutifs, et de la "London Assembly" qui examine les propositions du maire et vote ou rejette ses propositions de budget chaque année. Le GLA est une administration relativement récente (2000) créée afin de remplacer le "Greater London Council" (GLC) aboli en 1986. Le siège de la "Greater London Authority" et du maire de Londres (City Hall) se trouvent au bord de la Tamise, près du Tower Bridge.

Depuis le , le poste de maire de Londres est occupé par le travailliste Sadiq Khan.

Les 33 districts sont formés des 32 "boroughs" et de la Cité de Londres et sont responsables des services locaux non pris en charge par le GLA tels que l'aménagement local, les écoles, les services sociaux, les routes locales et le ramassage des ordures. Chacun des districts a à sa tête un conseil ("council") élu tous les quatre ans. La cité de Londres n'est pas dirigée par une autorité locale classique mais par la "Corporation of London" élue par les résidents et les entreprises et qui n'a pratiquement pas changé de forme depuis le Moyen Âge. La "Corporation of London" a à sa tête le "Lord Mayor of London", qui est un poste différent de celui du maire de Londres.

La cité de Londres possède sa propre force de police, la "City of London Police" indépendante du "Metropolitan Police Service" qui est responsable du reste du Grand Londres.

Les services de santé sont gérés par le gouvernement national grâce au "National Health Service", sous la responsabilité, à Londres, d'un seul "NHS Strategic Health Authority".

Un code postal « "postcode" » sert à délivrer le courrier et correspond ainsi à une adresse particulière. En Grande-Bretagne, un code postal se présente de la manière suivante WY11 1ZZ. Les deux premières lettres pour la ville, les numéros pour une région, les lettres et numéros à une zone résidentielle. Un code postal indique la rue de résidence mais aussi de quel côté de la rue on habite.

Les codes postaux de Londres sont divisés en Nord, Nord Ouest, Sud Est, Sud Ouest, Ouest et Est. Chaque code postal débute par N, NW, SE, SW, W ou E et les zones du centre-ville par EC et WC. Chaque zone correspondant à un code postal mesure 1– selon la densité de population, en donnant la première lettre du code postal on indique tout de suite dans quel endroit on habite à Londres. Londres est aussi divisée en arrondissements, qui sont une subdivision administrative de la ville.

Londres est le siège du gouvernement du Royaume-Uni situé au Palais de Westminster à Westminster. Plusieurs annexes du gouvernement sont situées aux alentours du Parlement, particulièrement le long de "Whitehall" où se trouve la résidence du Premier ministre au "10 Downing Street".

Bien qu'utilisée pour la première fois au par John Bright pour décrire l'Angleterre elle-même, l'expression "Mother of the Parliament" (mère des parlements) est souvent utilisée pour faire référence au parlement britannique car il est souvent considéré comme le premier à avoir instauré un système composé d'une chambre haute et d'une chambre basse élues et a été suivi par beaucoup d'autres systèmes politiques, notamment en Europe et dans les pays du Commonwealth.

Dans le cadre des élections à la Chambre des communes, Londres est divisée en 73 circonscriptions électorales qui élisent chacune un député ("", MP). Lors des élections de 2015, le Parti travailliste a emporté 45 des londoniens, le Parti conservateur 27 et les Libéraux-démocrates le dernier.

Des relations sont en construction avec et .

Londres a toujours été un important foyer de population. À la fois, ville, aire urbaine et région urbaine la plus peuplée du Royaume-Uni, elle a également été la plus peuplée d'Europe et du monde avant de connaître un léger déclin.

Le Grand Londres, composé de "Inner London" et "Outer London", compte en 2014. L'aire urbaine de Londres compte près de d'habitants tandis que l'aire métropolitaine, sa zone d'influence directe, compte d'habitants. D'après Eurostat, Londres est la première ville la plus peuplée et la deuxième aire urbaine la plus importante de l'Union européenne après Paris. La ville se classe également au quinzième rang des villes les plus peuplées du monde et au quinzième rang des aires urbaines les plus peuplées.

La région du Grand Londres occupe une superficie de et la densité de population est de par km, soit une densité plus de supérieure à celle de l'Écosse, de l'Irlande du Nord, du pays de Galles ou de n'importe quelle autre région anglaise. Cette densité cache cependant des disparités au sein de . En 2005, le borough royal de Kensington et Chelsea ("Inner London") comptait contre pour Bromley ("Outer London").

La structure de la population de Londres est légèrement différente de celle de l'Angleterre ou du Royaume-Uni. L'attractivité de Londres a entraîné une immigration vers la capitale de personnes en âge de travailler depuis le reste du pays ou l'étranger. La proportion de personnes entre représente 42,8 % contre 35,1 à l'échelle nationale. En contrepartie, la proportion de personnes âgées de et plus (14,4 %) est inférieure à la moyenne nationale (18,4 %).

Londres comptait sans doute un peu plus de en 1500. Elle s'est rapidement développée aux et . Un peu avant 1700, elle dépasse les et devient la ville la plus peuplée d'Europe devant Paris. Elle est environ vingt fois plus peuplée que Bristol, la deuxième ville d'Angleterre à l'époque. En 1801, lors du premier recensement, la ville comptait . Après cette date, dans un contexte d'industrialisation rapide, la population s'accroît fortement et en 1831, la ville atteint . Sa population dépasse celle de Pékin, et la ville devient donc la plus peuplée au monde. Elle le reste jusqu'en 1925, date à laquelle elle est dépassée par New York. La population de Londres a culminé à en 1939 puis a décliné jusqu'à au recensement de 1981 avant de remonter jusqu'à lors du recensement de 2011.

Londres est l'une des villes possédant la plus grande diversité d'origines. D'après le recensement démographique britannique de 2011, 59,8 % des de Londoniens se considèrent comme appartenant au groupe « blanc », 12 % des habitants se considèrent comme indiens, pakistanais ou bengalis, 13,3 % se considèrent comme noirs (environ 7 % de noirs africains et 4,2 % de noirs des Caraïbes), 1,5 % se disent chinois et 5 % se considèrent comme issus de plusieurs origines.

En 2001, 27 % des Londoniens sont nés en dehors du Royaume-Uni et 21,8 % hors de l'Union européenne. Les Irlandais (d'Irlande et d'Irlande du Nord) sont environ , tout comme les Écossais et les Gallois.

Londres est également une des villes les plus actives du monde sur le plan linguistique. Une étude menée en 2005 a montré que plus de trois cents langues différentes y sont parlées et qu'on peut y trouver ethniques comptant plus de .

Les chiffres de l"'Office for National Statistics" montrent que le nombre de Londoniens nés à l'étranger atteignait en 2006 contre en 1997.

Le tableau ci-contre donne le pays de naissance des résidents de Londres en 2011, date du dernier recensement britannique.

D'après le recensement de 2011, on dénombrait à Londres 48,4 % de chrétiens (catholiques, protestants, anglicans ou autres), 12,4 % de musulmans, 5 % d'hindous, 1,8 % de juifs, 1,5 % de sikhs, 1 % de bouddhistes, 0,6 % d'autres religions, 20,7 % de personnes sans religion et 8,5 % de personnes ne déclarant pas leur religion.

Du point de vue de la religion, Londres a été, tout au long de son histoire, dominée par le christianisme et compte un nombre important d'églises, notamment dans la City. La cathédrale Saint-Paul ainsi que la cathédrale de Southwark sont à la tête de l'Église anglicane tandis que les cérémonies officielles et royales se déroulent soit à Saint-Paul soit à l'abbaye de Westminster (à ne pas confondre avec la cathédrale de Westminster qui est un édifice relativement récent ainsi que la plus grande cathédrale romaine catholique d'Angleterre et du Pays de Galles). Malgré ceci, le pourcentage d'anglicans pratiquants est très bas. En revanche, ce taux est beaucoup plus élevé dans les communautés romaines catholiques et chrétiennes orthodoxes.

Londres abrite également d'importantes communautés musulmane, hindoue, sikhe et juive. De nombreux musulmans vivent à Tower Hamlets et à Newham et le plus important édifice musulman est la grande mosquée de Londres près de Regent's Park. On estime à le nombre de musulmans vivant dans la capitale britannique.

La communauté hindoue de Londres réside dans les quartiers nord-ouest de Harrow et de Brent, où se trouve un des plus grands temples hindous d'Europe, le temple Neasden. La communauté sikhe se trouve elle dans l'est et dans l'ouest de Londres, qui abrite également un des plus grands temples sikhs situés hors d'Inde. La majorité des Britanniques de confession juive se trouve à Londres, particulièrement à Stamford Hill et Golders Green dans le nord de Londres.

En 2014, Londres est la cinquième ville du monde en termes de PUB, et la première d'Europe devant Paris intramuros. Le Grand Londres réalise environ un quart du PIB du Royaume-Uni, et l'aire métropolitaine de Londres environ un tiers. La productivité est nettement supérieure à la moyenne nationale. Très fortement tertiarisée, Londres connaît une importante spécialisation dans la finance. La capitale britannique est la première place financière du monde et l'un des principaux centres d'affaires internationaux. D'après une étude de fDi Markets datant de 2016, Londres est la deuxième ville mondiale ayant reçu le plus d'investissements directs étrangers après Singapour. La ville se hisse première mondiale pour sa connectivité et deuxième pour son potentiel économique et son environnement favorable aux affaires.

L'immigration joue un rôle majeur, elle concerne des personnes de qualification très diverses, mais une des caractéristiques de la ville est sa capacité à attirer les hauts revenus et les personnes avec de hautes qualifications.

Les inégalités économiques sont fortes. Londres compte de nombreuses poches de pauvreté et le taux de chômage est plus élevé que la moyenne nationale (4,3 % au Royaume-Uni en 2017 contre 4,9 % à Londres) et 53 % des enfants de ces quartiers vivent dans un état de pauvreté.

L'économie de Londres s'est orientée vers les services beaucoup plus tôt que d'autres villes européennes, surtout après la Seconde Guerre mondiale. Le succès de Londres dans le secteur tertiaire s'explique surtout par plusieurs des facteurs :

Environ 85 % de la population du Grand Londres (soit 3,2 millions de personnes) travaillent dans le secteur des services. travaillent dans l'industrie et la construction (en proportions égales).

Londres concentre ses activités financières et juridiques dans cinq centres différents : la City, Westminster, Canary Wharf, Camden & Islington et Lambeth & Southwark.

La principale activité économique de Londres est le secteur financier dont les exportations financières, (c'est-à-dire les services aux entreprises fournis par des sociétés londoniennes à des entreprises étrangères dans le secteur des services financiers (indépendamment de l'immobilier)), contribuent grandement à la balance des paiements du Royaume-Uni. Plus de travaillent dans le secteur de la finance de Londres en 2015 qui abrite plus de 480 banques, soit plus que n'importe quelle autre ville au monde. La "City" est le plus grand centre d'affaires d'Europe en termes de flux de capitaux et s'impose comme une véritable place financière de premier plan après lois Sarbanes-Oxley qui accroissent les exigences comptables pour les entreprises cotées à la bourse de Wall Street. Lors d'une récente étude publiée par Mastercard, Londres surpasse New York dans quatre des six domaines de l'étude dont la stabilité économique, la facilité de faire des affaires et le volume des flux financiers. Le maire de New York Michael Bloomberg a déclaré que New York risquait de perdre son statut de capitale financière du monde au profit de Londres à cause du droit et des systèmes de régulation et d'immigration moins stricts du Royaume-Uni. En 2016, la "City" de Londres générait un peu plus de 2 % du PIB britannique et 0,38 % du PIB de l'Union Européenne soit une création en moyenne de plus de de dollars par kilomètre carré par an.

Un second centre financier se développe à Canary Wharf, à l'est de la "City", et compte le quartier général des banques HSBC et Barclays, de l'agence Reuters ainsi que de nombre des plus grands cabinets d'avocats au monde. Les quartiers généraux européens de J.P. Morgan, Citi, Bank of America, Morgan Stanley et American Express sont aussi localisés à Canary Wharf. En 2005, Londres a traité 31 % des transactions sur le marché des changes et traite quotidiennement environ de dollars, soit plus qu'à New York.

Plus de la moitié des 100 premières entreprises britanniques (FTSE 100) et plus de 100 des 500 plus grandes entreprises européennes ont leur siège à Londres. Plus de 70 % des entreprises du FTSE 100 ont leur siège dans l'aire urbaine de Londres et 75 % des entreprises du "Fortune 500" ont un bureau à Londres.

Les médias sont particulièrement concentrés à Londres et l'industrie de la distribution des médias en est le deuxième secteur le plus compétitif. La BBC est un employeur clé de la ville tandis que de nombreux autres médias ont leur siège à Londres.

Le port de Londres a été le plus important du monde mais arrive aujourd'hui en troisième position au Royaume-Uni. de tonnes de marchandises y transitent chaque année. La plupart de ces marchandises transitent cependant par Tilbury qui se trouve en dehors des limites du Grand Londres.

La Chambre de commerce et d'industrie de Londres est la plus grande organisation indépendante de réseautage et d'assistance commerciale de la capitale britannique. Elle représente les intérêts de milliers d'entreprises.

Pour sa part la Bourse de Londres, crée en 1801, est un marché boursier située dans la ville. Il s'agit d'un des plus grands marchés de la planète.

Londres est une des principales destinations touristiques au monde. Ce secteur génère entre et emplois selon les sources. En 2008, les revenus du tourisme représentaient £. En 2014, Londres a reçu de touristes étrangers, pour un total d'environ .

Londres bénéficie de son statut de capitale anglophone en Europe et attire ainsi chaque année de très nombreux étudiants du continent venus apprendre la langue anglaise. Une importante économie du tourisme estudiantin s'est développée autour de cette manne, certains n'hésitant pas à en profiter par des pratiques à la limite de la légalité.

Les principaux sites touristiques londoniens sont concentrés dans le "West End", qui comprend les grands magasins d’"Oxford Street", les théâtres, et les quartiers tels que "Soho", "Covent Garden", "Mayfair", "Piccadilly Circus" et la place de "Leicester Square". Les monuments les plus célèbres de Londres sont le "British Museum", la "Tate Gallery", le "Tate Modern", "Madame Tussauds", les palais de Westminster et de Buckingham, l’"Imperial War Museum", le "Science Museum", la "National Gallery", la " National Portrait Gallery", la "Tower Bridge", "Big Ben", la tour de Londres, "London Eye", la cathédrale Saint-Paul et "Arsenal Football Club Museum".

Selon une étude du King's collège de Londres, la pollution à l’hydroxyde d'azote provoque plus de 9 000 décès prématurés à Londres chaque année.

Le palais de Buckingham est la résidence officielle des monarques du Royaume-Uni depuis 1837. Il se situe dans la ville de Londres. C'est aussi le siège administratif du monarque régnant au Royaume-Uni.

Le palais compte dont d'État. Il mesure de long sur l'avant, de profondeur (incluant le quadrilatère central) et de haut.

Les transports sont un des quatre domaines de compétence du maire de Londres. Le réseau de transport public, géré par "Transport for London" (TfL), est un des plus étendus au monde mais subit tous les jours des embouteillages, retards et problèmes de maintenance. Un programme de de livres a été mis en place pour tenter d'améliorer le réseau à l'horizon de 2012, pour l'inauguration de Jeux olympiques. Malgré un coût des plus élevés d'Europe, l'ensemble du réseau londonien a cependant été déclaré meilleur réseau de transport au monde (devant New York et Paris) par 25 % des interrogées lors d'un sondage réalisé par TripAdvisor.

L'élément central du réseau de transport de la capitale britannique est le métro de Londres, "Underground" ou "London Tube" appelé familièrement "The Tube", composé de et interconnectées pour une longueur totale de . Il existe de nombreux projets d'extensions, notamment la Elizabeth line dont l'ouverture est prévue pour fin 2018. Inauguré en 1863, c'est le plus ancien réseau au monde. Il comporte même la toute première ligne de métro électrique, la "City & South London Railway", mise en service en 1890. Trois millions de trajets par jour, soit environ un milliard par an, sont effectués sur l'ensemble du réseau du métro, qui dessert principalement le centre historique de Londres ainsi que les banlieues de la ville situées au nord de la Tamise mais s'étend jusqu'au-delà des frontières du Grand Londres. Les banlieues sud et sud-est sont moins desservies par le métro mais bénéficient d'un important réseau de trains de banlieue. Le "Docklands Light Railway", inauguré en 1987, dessert l'est de Londres et Greenwich sur les deux rives de la Tamise.

Les trains de banlieue ne traversent généralement pas la ville mais s'arrêtent dans une des 14 gares de la ville situées autour du centre historique. "Crossrail" est un projet de réseau express régional qui devrait entrer en fonction en 2018 et qui permettra de relier les banlieues est et ouest en traversant Londres dans un souterrain. Un train urbain, l'"Overground" est entré en service en novembre 2007.
Le service de train Eurostar relie la gare de Saint-Pancras à Lille et Paris (France), par l'Eurotunnel, en et respectivement, Bruxelles (Belgique) en , mais aussi, depuis 2015, Lyon en un peu moins de 6h puis Avignon en 7h et Marseille en 7h30. Il y a aussi des projets de réinsertion du tramway dans le centre de Londres.

Le "London Inner Ring Road" (périphérique situé autour du centre de Londres), les routes A406 et A205 (dans la banlieue) ainsi que l'autoroute M25 (plus éloignée) contournent la ville et relient les nombreuses voies allant vers le centre-ville de Londres (Inner London). Un projet d'autoroutes sillonnant l'agglomération (appelées London Ringways) avait été lancé en 1962 mais a été en grande partie abandonné au début des années 1970-1971 à cause des objections des riverains et des coûts élevés. En 2003, un péage urbain a été introduit afin de réduire le trafic en centre-ville. À quelques exceptions près, les automobilistes doivent payer 8 livres par jour pour pénétrer à l'intérieur d'une zone correspondant au centre de Londres. Les automobilistes résidant au sein de la zone payante payent 10 %, payables soit pour 5 jours au tarif de ou pour quatre semaines.

La plupart des lignes d'autobus du réseau de Londres fonctionnent en journée et en soirée. Certaines lignes fonctionnent même 24 heures sur 24. L'autobus est le moyen de transport principalement utilisé pour les déplacements locaux et transporte plus de passagers que le métro. Chaque jour de la semaine, les bus londoniens transportent de passagers sur plus de différents. Le nombre de voyages a atteint en 2005/2006. Les bus à impériale rouges sont un des symboles de Londres, de même que les taxis noirs et le métro.

Londres, pour soutenir sa politique d'éradication de la voiture, investit très lourdement dans le transport individuel cycliste. C'est ainsi qu'en 2006 Londres a investi d'euros dans les voies cyclables et les parkings à vélo. Le vélo-partage contribue également à accroitre la mobilité urbaine des Londoniens : en 2016, les vélos Santander construits par PBSC Solutions Urbaines au Canada étaient au nombre de répartis sur 570 stations.

Londres est également une plate-forme de correspondance aérienne mondiale. En 2017, plus de 170 millions de passagers ont transité dans un des 6 aéroports de Londres (London Heathrow, Gatwick, Stansted, Luton, London City, London Southend). L'aéroport d'Heathrow est le deuxième plus important au monde en nombre de passagers internationaux et propose une gamme complète de vols intérieurs, européens ou intercontinentaux. Une part importante du trafic international ainsi que nombre de vols de compagnies aériennes à bas prix sont prises en charge par l'aéroport de Gatwick. Les aéroports de Stansted et de Luton sont spécialisés dans les vols court-courriers des compagnies à bas prix. L'aéroport de Londres-City, le plus petit et le plus proche de Londres, est plutôt, de par sa proximité avec les centres financiers de la capitale, spécialisé dans les vols privés et accueille des vols court-courriers ainsi qu'un important trafic de jets privés. L'aéroport de Londres-Southend est le nouveau venu dans le choix des voyageurs. Finalement, il y a l'aérodrome de Biggin Hill, dans le sud-est de Londres, utilisé seulement par les jets privés.

La Tamise est utilisée par les bateaux de tourisme et les bateaux-bus. Les services sont assurés par "London River Services".

En aval, les jetées principales sont :

Londres est un centre mondial de recherche et d'enseignement dans le supérieur. Selon le classement QS des universités de 2015/16, Londres est la ville avec la plus forte concentration des universités de première classe dans le monde et sa population étudiante internationale de est plus grande que toute autre ville dans le monde. Une étude menée en 2014 par PricewaterhouseCoopers annonce Londres comme la capitale mondiale de l'éducation dans le supérieur.

Un certain nombre d'établissements d'enseignement de premier plan sont basés à Londres. Dans le classement 2015 de QS, l'Imperial College London atteint la place mondiale, University College London (UCL) la , and King's College London (KCL) est classée . La London School of Economics est souvent décrite comme une institution phare pour la recherche en sciences sociales. La London Business School est considérée comme l'une des meilleures écoles de second cycle universitaire en économie et administration. En 2015, son programme "MBA" est classé deuxième mondial par le "Financial Times".

Avec plus de à Londres, l'université fédérale de Londres est la plus grande université du Royaume-Uni. Elle inclut cinq universités indépendantes – City, King's College London, Queen Mary, Royal Holloway et UCL – et des institutions plus petites et plus spécialisées telles que Birkbeck, le Courtauld Institute of Art, Goldsmiths, Guildhall School of Music and Drama, la London Business School, la London School of Economics, la London School of Hygiene & Tropical Medicine, la Royal Academy of Music, la Central School of Speech and Drama, le Royal Veterinary College et la School of Oriental and African Studies. Les membres de l'Université de Londres ont leur propres procédures d'admission, et certaines offrent leur propre diplôme.

Il existe d'autres universités n'étant pas affiliées à l'Université de Londres, comme le prestigieux Imperial College London ou encore Brunel University, Kingston University, London Metropolitan University, University of East London, University of West London, University of Westminster, London South Bank University, Middlesex University, et la University of the Arts London (qui est la plus grande université d'arts en Europe). Il existe de plus trois universités internationales à Londres – Regent's University London, Richmond, The American International University in London et Schiller International University.

Londres inclut cinq écoles de médecines de pointe – Barts and The London School of Medicine and Dentistry (Queen Mary), l'école de médecine de King's College London (la plus grande école médicale d'Europe), l'école de médecine de l'Imperial College, l'école de médecine de UCL et St George's, University of London. C'est l'un des plus grands centre de recherche biomédicale avec notamment l'Institut Francis Crick fondé par l'Imperial College, King's College et UCL.

Il y a un grand nombre d'écoles de commerce à Londres, dont la London School of Business and Finance, la Cass Business School, la Hult International Business School, ESCP Europe, l'European Business School London, l'Imperial College Business School, la London Business School et la UCL School of Management.

Au sein de la cité de Westminster, le quartier de West End regroupe un grand nombre d'attractions autour de Leicester Square, où de nombreux films sont joués en avant-première britannique et mondiale, et Piccadilly Circus et ses publicités électroniques couvrant de nombreux bâtiments.

Dans cette zone se trouvent également le quartier des théâtres de Londres qui regroupe de nombreux cinémas, bars, pubs, boîtes de nuit, restaurants ainsi que le quartier chinois de Londres. Un peu plus à l'est se trouve Covent Garden. Shoreditch et Hoxton, situés à Hackney dans l"East End" regroupent également de nombreux bars, restaurants, night-clubs et galeries. 

Oxford Street, souvent citée comme étant la plus longue rue commerçante au monde, regroupe plus de sur environ , depuis Marble Arch, et accueille près de de clients par an. Bond Street à Mayfair abrite de nombreuses boutiques de luxe, de même que le quartier de Knightsbridge où se situe Harrods. Les quartiers de Knightsbridge (Sloane Street), Mayfair (Bond Street, Brook Street) et Chelsea (King's Road) regroupent de nombreux créateurs et boutiques de mode dont Vivienne Westwood, John Galliano, Stella McCartney, Manolo Blahnik et Jimmy Choo. Londres abrite également de nombreux marchés, dont "Camden market" pour la mode, Portobello Road pour les antiquités et "Borough Market" pour les produits bios.

Londres abrite également de nombreuses écoles des arts du spectacle comme la "Central School of Speech and Drama", d'où sont sortis Judi Dench et Laurence Olivier, la "London Academy of Music and Dramatic Art", où ont été formés Jim Broadbent et Donald Sutherland entre autres, ainsi que la "Royal Academy of Dramatic Art", qui compte Joan Collins et Roger Moore parmi ses anciens élèves. Le Festival du film de Londres, organisé par le "British Film Institute", se tient dans la ville tous les ans en octobre.

Londres joue également un rôle important dans l'industrie cinématographique. Quatre grands studios sont situés dans la ville : Pinewood, Shepperton, Elstree et Leavesden, ainsi que de nombreuses entreprises spécialisées dans la postproduction et les effets spéciaux. De nombreux films ont été tournés à Londres même : "Coup de foudre à Notting Hill" (1999) et "28 jours plus tard" (2002), "Match Point" (2005). "Sweeney Todd" (2008) et surtout les célèbres sagas "Harry Potter" (qui s'est déroulée entre autres à Londres gare de King's Cross par exemple) et "James Bond" (comme dans "Skyfall" en 2012).

Londres possède un panel de musées parmi les plus importants au monde. On en compte 240 (contre 173 à Paris). On trouve des musées dans tous les domaines : art, sciences, histoire, loisirs, etc.
En voici quelques exemples, parmi les plus connus : le British Museum (musée d'histoire et de culture humaine), la National Gallery (musée d'art), le Tate Modern (musée d'art contemporain), Victoria and Albert Museum (musée d'histoire britannique), le musée d'Histoire Naturelle…

 La ville abrite le siège d'une des quatre grands majors du disque, EMI Group, ainsi que d'innombrables musiciens, groupes, orchestres et professionnels de la musique.

Cinq orchestres professionnels sont basés à Londres : l'orchestre symphonique de Londres, l'orchestre philharmonique de Londres, l'orchestre philharmonique royal, l'orchestre Philharmonia et l'orchestre symphonique de la BBC. De nombreux autres orchestres sont également situés dans la ville : l'orchestre de l'âge des Lumières, le "London Sinfonietta" et les ensembles "London Mozart Players" et "English Chamber Orchestra". Le point culminant de la saison classique se produit tous les ans en été avec "The Proms", une série d'environ 70 concerts de musique classique au Royal Albert Hall.

Londres possède deux principaux opéras : le "Royal Opera House" et le "Coliseum Theatre". Les ballets "Royal Ballet" et lEnglish National Ballet" se produisent au ', au "Sadler's Wells Theatre" et au "Royal Albert Hall".

Londres abrite de nombreuses salles de concerts pop/rock telles Earls Court et Wembley Arena, la Carling Brixton Academy ou l'Hammersmith Apollo, et d'innombrables salles plus intimistes. De nombreux artistes résidents à Londres et dans les "Home Counties" environnants. La ville a vu s'ouvrir le tout premier Hard Rock Cafe ainsi que les célèbres studios Abbey Road.

En tant que principale agglomération du Royaume-Uni, tels que le "drum and bass", "garage", "grime" et "dubstep". De nombreux artistes de hip-hop britannique habitent également à Londres.

En 2006, "DJ Magazine" a publié une enquête réalisée auprès de 600 DJ internationaux qui a établi que Londres abritait trois des meilleurs night-clubs au monde : le "Fabric", "The End" et le "Turnmills". En 2007, lors d'un nouveau sondage, le "Fabric" a été classé en deuxième position et "The End" en quatrième position ; six clubs londoniens se trouvent dans les cinquante premières places.

Au , Londres a accueilli à de nombreuses occasions des événements sportifs d'envergure mondiale, comme les Jeux olympiques d'été à trois reprises, en 1908, en 1948 et en 2012, ce qui en fait la première ville à recevoir les J.O. à trois reprises. En 1934, les Jeux du Commonwealth se sont également tenus dans la capitale britannique.

Le sport le plus populaire à Londres est le football (tant par le nombre de joueurs que par le nombre de spectateurs). La ville possède quatorze clubs de foot de "Football League" dont six qui évoluent en "Premier League" pour la saison 2016/2017 (Arsenal, Chelsea, Crystal Palace,Tottenham Hotspur, Watford et West Ham United), les autres clubs évoluant dans les trois divisions inférieures (l'AFC Wimbledon, Brentford, Charlton Athletic, Dagenham & Redbridge, Fulham, Leyton Orient, Millwall et Queens Park Rangers). Il existe également de nombreux clubs "non-leagues" ou amateurs. Londres compte quatre clubs de rugby évoluant dans le championnat d'Angleterre (London Irish, Saracens, London Wasps et Harlequins) bien que seuls les Harlequins jouent vraiment à Londres (les autres clubs jouent en dehors du Grand Londres). Le club des Harlequins Rugby League évolue lui en Super League. Les autres clubs londoniens de rugby sont Richmond FC, Blackheath RC, Rosslyn Park et Barnes R.F.C.. Londres accueille aussi tous les ans un tournoi de rugby à sept comptant pour les World Rugby Sevens Series, le London rugby sevens.

Twickenham, dans l'ouest de Londres, est le stade national de rugby et peut accueillir . Le nouveau stade de Wembley peut accueillir désormais jusqu'à pour l'équipe d'Angleterre de football ainsi que pour les finales de la Coupe d'Angleterre de football, la Coupe de la ligue de football et de rugby. Les autres stades de football principaux sont Craven Cottage pour Fulham, Emirates Stadium pour Arsenal, Stamford Bridge pour Chelsea, White Hart Lane pour Tottenham Hotspurs. Le club de West Ham United a quitté son stade d'Upton Park en 2016 pour s'établir dans le stade Olympique de Londres construit à l'occasion des JO 2012.
Le cricket se joue principalement à Londres sur deux terrains de "test cricket", le Lord's Cricket Ground (qui accueille le Middlesex CC) à St. John's Wood et l"'Oval" (qui accueille le Surrey CC) à Kennington. Le baseball devient de plus en plus populaire avec Londres ayant plusieurs ligues et équipes fortes comprenant Croydon Pirates and London Mets. Les autres rendez-vous annuels sportifs à Londres incluent le tournoi de Wimbledon qui se tient au All England Lawn Tennis and Croquet Club à Wimbledon, le marathon de Londres qui accueille et la "Boat Race" qui, depuis , voit s'affronter sur la Tamise, entre Putney et Mortlake, les clubs d'aviron, de l'université de Cambridge et d'Oxford.

Londres a accueilli les Jeux olympiques d'été en 2012. La Lower Lea Valley est choisi pour devenir le parc et le village olympique. Les installations sont reliées entre elles par une navette à haute vitesse, surnommé The Olympic Javelin. Des transports sont créés pour être capables de déplacer par heure. Après la clôture des jeux, la région est transformée en un grand parc urbain, en bureaux et en logements.

Les armes de Londres se blasonnent de cette façon: « d'argent à la croix de gueules, le canton dextre du chef chargé d'une épée du même. »

Les taux de criminalité varient considérablement d'une région à l'autre de Londres. La police métropolitaine publie des statistiques détaillées sur les crimes, ventilées par catégorie au niveau des arrondissements et des quartiers, sur son site Web depuis 2000.

En 2015, il y a eu 118 homicides, soit une augmentation de 25,5 % par rapport à 2014. La criminalité connaît un nouvel accroissement en février et mars 2018, après une augmentation du taux d'homicide de 40 % en trois ans. La capitale britannique dépasse ainsi les chiffres de la ville de New York. Néanmoins, les chiffres pour 2017 sont de 116 meurtres à Londres pour un total annuel de 290 à New York.

La plupart des victimes sont noires et ces homicides seraient essentiellement liés au trafic de drogue et aux règlements de comptes entre gangs de dealers.

Les parcs royaux de Londres sont des parcs qui appartiennent à la Couronne britannique. Ces huit parcs sont des réserves naturelles ainsi que des jardins botaniques. Il s'agit de
Londres est un des premiers centres de communication au monde avec la présence d'un grand nombre d'entreprises de communication. La plupart des grands médias britanniques et tous les grands réseaux de télévisions nationaux, dont BBC News, le plus important service d'information au monde, ont leur siège à Londres. Environ 53 % des emplois britanniques liés à la télévision et à la radio sont concentrés à Londres. Cette concentration a souvent amené certains commentateurs à critiquer le centrage du Royaume-Uni sur Londres. Cela a amené certains grands médias à délocaliser certains de leurs locaux : la BBC a annoncé en juin 2004 que ses services sport et jeunesse seraient transférés à Manchester, au nord de l'Angleterre. Les autres réseaux installés à Londres comptent parmi eux "ITV", "Channel 4", "Channel 5" et "BSkyB". Tout comme la BBC, ces médias produisent parfois leurs programmes ailleurs au Royaume-Uni mais Londres reste tout de même le principal lieu de production. Les programmes locaux sont proposés par les services régionaux des principaux réseaux : "BBC London" sur "BBC One" et "ITV London sur "ITV1".

Il existe de nombreuses chaînes de radio disponibles à Londres. Les radios locales comprennent "Capital Radio", "Heart 106.2", "Kiss 100" et "Xfm". Les radios d'informations et de débats comprennent "BBC London", "LBC 97.3" et "LBC News 1152".

Le marché des journaux à Londres est dominé par les éditions nationales des grands journaux britanniques, tous édités dans la capitale. Jusque dans les années 1970, la plupart des journaux nationaux étaient concentrés sur "Fleet Street" mais dans les années 1980, ils ont été délocalisés dans des entrepôts plus spacieux, susceptibles d'accueillir des imprimeries automatiques. La plupart se trouvent aujourd'hui dans l'est de Londres. À Wapping, en 1986, SOGAT 82, le syndicat des imprimeurs s'est fortement opposé à ces délocalisations, menant à de nombreux affrontements avec les forces de police. La dernière grande agence de presse de "Fleet Street", Reuters, a déménagé à "Canary Wharf" en 2005 mais "Fleet Street" reste un terme toujours fortement associé à la presse nationale.

Il existe deux journaux locaux à Londres, l'Evening Standard et "Metro", tous les deux gratuits. Ils sont disponibles dans la rue ainsi que dans le métro et les gares. "Time Out Magazine", un guide indépendant hebdomadaire fournit la liste des concerts, films, pièces de théâtre et autres activités culturelles depuis 1968. Il existe de nombreux autres journaux locaux dans l'agglomération londonienne, rapportant des informations très locales.

Londres est au centre de l'industrie télévisuelle et cinématographique britannique, avec les principaux studios à l'ouest de la ville et un important secteur de post-production basé à Soho. Londres est, avec New York, un des deux principaux centres d'édition de langue anglaise.

Londres est une capitale gastronomique et propose des plats internationaux éclectiques. Les Londoniens parlent souvent de la cuisine. Les pubs et la nourriture qu'ils proposent sont très à la mode. Enfin des plats britanniques typiques comme le "fish and chips" (poisson-frites), le "haggis" (panse de brebis farcie), les pies (tourtes à l'anglaise de diverses garnitures) ou encore le "Sunday roast" (le rôti du dimanche, y compris le fameux rosbif) font aussi partie de la gastronomie londonienne.

Voir la page «  »Les personnages de Sherlock Holmes, Jack l'Éventreur et de Oliver Twist sont connus et rattachés à l'histoire culturelle de Londres.

Voir les pages « » et « »

Londres a inspiré de nombreux auteurs et été le sujet de multiples œuvres de littérature. William Shakespeare a passé une grande partie de sa vie et a également travaillé à Londres. Son contemporain Ben Jonson a également vécu à Londres et certains de ses écrits, notamment "l'Alchimiste", se déroulent dans la ville. Concernant ceux ayant écrit l'histoire de la ville, on peut citer Samuel Pepys (1633 - 1703), qui a notamment relaté de grands événements comme l'épidémie de peste de Londres et le grand incendie de 1666. Charles Dickens (1812 - 1870) est étroitement associés à la ville, dont la description d'un Londres embrumé, neigeux et crasseux aux rues remplies de balayeurs et pickpockets a eu une influence majeure sur la perception de la ville à l'époque victorienne.

"", la biographie de James Boswell se déroule principalement à Londres et est à l'origine de la fameuse citation de Samuel Johnson : « Quand un homme en a assez de Londres, il en a assez de la vie car il y a à Londres tout ce que la vie peut apporter » ("When a man is tired of London, he is tired of life; for there is in London all that life can afford"). Le "" de Daniel Defoe est une œuvre de fiction basée sur la grande peste de 1665.

À travers Phileas Fogg, héros du roman "Le Tour du monde en quatre-vingts jours" (paru en 1872), son auteur Jules Verne se plait à représenter l'archétype du gentleman anglais victorien, vu par les Français. Cet "homme-horloge", très stoïque, mène une vie réglée à la minute près, exerçant quotidiennement les mêmes activités, se résumant à celles pratiquées au Reform Club, club de gentlemen londonien. Le roman de 1933 de George Orwell "Dans la dèche à Paris et à Londres" décrit la vie des pauvres dans les deux capitales britannique et française. Peter Ackroyd est un écrivain moderne qui a également été influencé par la ville, notamment dans "London: The Biography", "The Lambs of London" et "Hawksmoor". Bloomsbury et le quartier d'Hampstead ont traditionnellement été au cœur du courant de littérature libertarienne de Londres. Le poète Paul Verlaine s'y installa en 1875 durant deux ans et y séjourna à plusieurs reprises. Il évoque sa vie londonienne dans plusieurs de ses œuvres, telles que "Un tour à Londres" (1894), plusieurs de ses lettres (notamment à Edmond Lepelletier), ainsi que quelques dessins.

Londres est une ville privilégiée dans les polars, où elle sert de décor de manière très récurrente. Un incontournable de ce genre est la série des aventures de Sherlock Holmes d'Arthur Conan Doyle, dépeignant la ville au et au début du . Un autre héros célèbre de la littérature policière britannique vit dans cette ville, le détective belge Hercule Poirot, créé par Agatha Christie et apparu en 1920.

Londres inspira aussi beaucoup la littérature fantastique, comme l'illustre le roman de Robert Louis Stevenson "L'Étrange Cas du docteur Jekyll et de M. Hyde" (1886).

Voir la page «  »

Un des peintres anglais les plus célèbres, le romantique William Turner, représenta Londres dans plusieurs de ses œuvres, comme dans une série de ""Vues de Londres depuis Greenwich"", ou bien ""L'Incendie de la Chambre des Lords et des Communes"" (1835). Quant à William Hogarth, il est à l'origine d'une série de six tableaux et gravures, réalisées entre 1731 et 1732 : "La Carrière d'une prostituée".

La capitale anglaise attira beaucoup les artistes européens (en particulier français) de la fin du , qui leur permettait de se dépayser sans aller loin ; ce fut principalement le cas des peintres. Un des meilleurs exemples est certainement Claude Monet, qui y séjourna plusieurs fois, y résida et y exposa. Dans ses œuvres, il confondit dans les mêmes brouillards impressionnistes les villes de Londres, de Venise ou la Normandie. Parmi ses œuvres londoniennes, on peut citer la Série des "Parlements de Londres" ( du Parlement, conçues entre 1900 et 1905)"," celle des "Charing Cross Bridge" (série de peints entre 1899 et 1904, représentant le Hungerford Bridge) et, plus généralement, des représentations de la Tamise. Toujours en matière de ponts, son compatriote Alfred Sisley peignit en 1874 "Sous le pont de Hampton Court". James Tissot, lui, préféra représenter le portique de la National Gallery dans quelques-unes de ses œuvres. Dans les scènes historiques, Édouard Cibot représenta en 1835 "Anne Boleyn à la Tour de Londres", où l'on voit Anne Boleyn (mère de la reine ), accusée d'intriguer, emprisonnée à la Tour de Londres.

Walter Sickert, postimpressionniste anglais d'origine allemande, était fasciné par l'affaire de Jack l'Éventreur. Il était convaincu de loger dans la même chambre que le criminel occupait avant son arrivée. C'est pour cela qu'il nomma cette pièce la "Jack the Ripper's bedroom" et qu'il représenta (vers 1907) sur une peinture de ce titre. Il fut lié à cette affaire, au point qu'il figure sur la liste des suspects ayant pu jouer le rôle de l'assassin ou de son complice.

Le peintre et graveur américain James McNeill Whistler aussi figura la ville dans ses œuvres. Il immortalisa dans "Nocturne en bleu et or - le Vieux Pont de Battersea" (vers 1872-1875) l'ancien pont de Battersea, détruit en 1885 pour être remplacé l'année suivante par la version actuelle.

Voir la page « »
Dans les bandes dessinées ayant pour cadre la capitale britannique, on peut citer celles reprenant des personnages littéraires tels que Sherlock Holmes, dont plusieurs séries de BD reprennent le héros ou des personnages secondaires. Mais il existe aussi beaucoup de séries originales la mettant en scène.

Pour les britanniques, on peut citer série de comics "La Ligue des gentlemen extraordinaires" (depuis 1999), scénarisée par Alan Moore et dessinée par Kevin O’Neill, réunissant plusieurs héros de la littérature du . Du même auteur, la BD américaine dessinée par Eddie Campbell, "From Hell" (1991-1996), suit l'affaire Jack l'Éventreur.

En dehors de la Grande-Bretagne, on pense bien-sûr à la série de "Blake et Mortimer," créée par le bédéiste belge Edgar P. Jacobs en 1946. Les deux héros résident dans Londres et plusieurs épisodes s'y déroulent, dont le principal est "La Marque jaune" (1953-1954), dans lequel un mystérieux criminel y sème la terreur. Toujours dans la BD belge, un double épisode de la série "Largo Winch" (Jean Van Hamme et Philippe Francq) prend également place dans cette ville : "Chassé-croisé / Vingt secondes".

Dans "Astérix chez les Bretons" (1965), épisode de la série française "Astérix" de René Goscinny et Albert Uderzo, les héros passent par le Londres antique, Londinium. Celui-ci parodie la ville moderne, avec ses bardes à succès, sa tour prison lugubre, ses bus impériaux...

Voir la page « »

Voir la page « »




</doc>
<doc id="4925" url="https://fr.wikipedia.org/wiki?curid=4925" title="Angleterre">
Angleterre

L'Angleterre (en anglais ') est une nation constitutive du Royaume-Uni . Elle est bordée par l'Écosse au nord et le pays de Galles à l'ouest. Son littoral est entouré par la mer du Nord à l'est, la mer d'Irlande au nord-ouest, la mer Celtique au sud-ouest, et la Manche au sud qui la sépare de l'Europe continentale. Son territoire couvre la majorité du centre et du sud de l'île de Grande-Bretagne, et il inclut également une centaine de petites îles. Sa capitale est Londres qui est la première aire urbaine du Royaume-Uni et, selon les critères retenus, d'Europe. L'Angleterre est la nation la plus peuplée du Royaume-Uni avec 53 millions d'habitants en 2011, ce qui représente 84 % de la population britannique, et est la plus grande avec une superficie de . Le nom dAngleterre" est fréquemment employé, par synecdoque, pour désigner le Royaume-Uni dans son ensemble.

Le territoire anglais a commencé à être peuplé durant le Paléolithique supérieur et tire son nom des Angles, l'une des tribus germaniques qui s'installa aux et s. L'Angleterre est devenue un État unifié au cours du avec la création du Royaume d'Angleterre en 927. Le Royaume a été un État souverain jusqu'au , date à laquelle l'Acte d'Union l'unifia au Royaume d'Écosse pour créer le Royaume de Grande-Bretagne. En 1801, la Grande-Bretagne s'unifia avec le Royaume d'Irlande par un autre Acte d'Union pour former le Royaume-Uni de Grande-Bretagne et d'Irlande. En 1922, l'Irlande du Sud devient indépendante du Royaume-Uni, le pays fut alors rebaptisé Royaume-Uni de Grande-Bretagne et d'Irlande du Nord.

Depuis l'Âge des découvertes qui commença au , l'Angleterre a eu un impact culturel considérable sur le reste du monde. La langue anglaise, l'anglicanisme et le droit anglais (qui est la base de la common law, un droit basé principalement sur la jurisprudence qui sert aujourd'hui de système juridique dans un grand nombre de pays) se sont développés en Angleterre ainsi que le système de Westminster, un système parlementaire de gouvernance qui a été adopté par un grand nombre de gouvernements dans le monde. C'est également la première démocratie parlementaire au monde. L'Angleterre est le berceau de la révolution industrielle qui débuta au , faisant d'elle la première nation industrialisée du monde.

Deux tiers de la Grande-Bretagne, au sud et au centre, constituent l’Angleterre, ainsi que quelques îles au large comme l’île de Wight ou les îles Scilly. La nation constitutive est frontalière de deux autres nations du Royaume-Uni – l’Écosse au nord et le Pays de Galles à l’ouest, et est plus proche du continent européen que toute autre île britannique. L’Angleterre est séparée de la France par un corridor maritime de , même si le tunnel sous la Manche les relie près de Folkestone. L’Angleterre possède aussi des littoraux sur la mer d’Irlande, la Mer du Nord et l’Océan Atlantique.

Les ports de Londres, de Liverpool et de Newcastle upon Tyne se trouvent respectivement à l’embouchure de la Tamise, de la Mersey et de la Tyne. La Severn est la plus longue rivière d’Angleterre, sur . Elle se jette dans le canal de Bristol et est célèbre pour ses lames de fond de plus de de hauteur (dans le mascaret de Severn). Cependant, la plus longue rivière entièrement en Angleterre est la Tamise, longue de . Il y a de nombreux lacs, le plus grand étant le Windermere dans le bien nommé Lake District.

Les Pennines, dites la « colonne vertébrale de l’Angleterre », est la plus ancienne chaîne montagneuse de la région, apparue à la fin de l’ère paléozoïque il y a environ 300 millions d’années. Elle se compose principalement de grès, de calcaire et de charbon. Il y a des paysages karstiques dans des régions riches en calcite, comme le Yorkshire ou le Derbyshire par exemple. Les Pennines sont couvertes de landes en altitude, dentelées de vallées fertiles grâces aux rivières. Elles contiennent trois parcs nationaux, les Yorkshire Dales, le Northumberland et le Peak District. Le point culminant de l’Angleterre, à , est le Scafell Pike, dans le comté de Cumbrie. À cheval sur la frontière entre l’Angleterre et l’Écosse se trouvent les monts Cheviot.

Les terres basses de l’Angleterre se trouvent au sud des Pennines, sur de nombreuses collines vertes, dont les Cotswolds, les Chilterns, les North Downs et les South Downs – quand elles rencontrent la mer, elles dévoilent des roches blanches comme les falaises blanches de Douvres. La péninsule du sud-ouest, dans le West Country, se compose de hautes landes, telles le Dartmoor et l’Exmoor, tous les deux parcs nationaux, et profite d’un climat océanique.

La faune et flore sont dégradées depuis l'après-guerre, plus de 80 % des haies, prés et marécages qui s'y trouvaient ont disparu.

Par des cartes des profondeurs de la Manche dressées par des sonars ultraperfectionnés, une gigantesque vallée au fond de la mer a pu être reconstituée en 3D, présentant les caractéristiques de l'érosion due à l'écoulement du fluide.
Le cataclysme se serait produit il y a ans à la suite de la rupture d'un barrage naturel existant entre Douvres et Calais. Le lac glaciaire en amont se serait écoulé à un débit supérieur à 1 million de mètres cubes par seconde dans cette vallée séparant l'île du continent. Une seconde inondation se serait reproduite il y a ans.

En 2007, Sanjeev Gupta de l'Imperial College de Londres a conforté la théorie des années 1980 prédisant une séparation de l'île britannique à la suite d'inondations.

L’Angleterre a un climat océanique tempéré. Il y fait doux avec des températures rarement dessous de en hiver et rarement dessus de en été. Le temps est fréquemment pluvieux et changeant. Les mois les plus froids sont janvier et février, ce dernier en particulier sur le littoral anglais, tandis que le mois de juillet est généralement le plus chaud. Les mois avec un temps variable sont mai, juin, septembre et octobre. Les précipitations sont réparties presque uniformément tout au long de l’année.

Plusieurs facteurs ont une influence importante sur le climat de l’Angleterre : la proximité de l’Océan Atlantique, la latitude au nord et le réchauffement des courants marins par le Gulf Stream. C'est une des régions européennes qui connaît le plus grand nombre de dépressions météorologiques. Les précipitations sont plus fortes à l’ouest, et certaines régions du Lake District reçoivent plus de pluie que partout ailleurs dans le reste de l'Angleterre. Le climat est plus humide à l'Est et au Nord, et plus continental au Sud-Est. Les chutes de pluie à Londres restent inférieures à pour l'année entière.

Les archives des températures montrent que le record de la plus haute température est de le 10 août 2003 à Brogdale dans le Kent, et la plus basse est de le 10 janvier 1982 à Edgmond, dans le Shropshire.

La ville la plus peuplée d'Angleterre est Londres, la capitale du Royaume-Uni. Elle est suivie par des villes qui se sont développées au , lors de la révolution industrielle : Manchester, Birmingham, Leeds et Liverpool. D'après l'Office for National Statistics, les dix plus grandes aires urbaines d'Angleterre en 2011 sont les suivantes :

Les os humains découverts en Angleterre les plus anciens ont plus de . Cette découverte de restes d’Homo erectus a eu lieu dans les Norfolk et Suffolk actuels. Les Homo sapiens sont arrivés dans cette région pour la première fois il y a environ , mais à cause des conditions difficiles de la dernière période glaciaire (connue dans cette région sous le nom de glaciation Devensian), ils fuient pour aller dans les montagnes du Sud de l’Europe. Seuls les grands mammifères comme les mammouths, les bisons et les rhinocéros laineux restèrent. Il y a environ , quand les couches de glace commencent à reculer, les humains repeuplent la zone, et des recherches génétiques ont montré qu’ils viennent du Nord de la péninsule Ibérique. Le niveau de la mer était plus bas qu’aujourd’hui, et l’Angleterre était reliée par la terre à l’Irlande et à l’Eurasie. L’élévation des eaux il y a sépare à nouveau les îles Britanniques, et un demi-siècle plus tard, c’est au tour de l’Eurasie.

Une migration massive survenue il y a environ -4 500 ans depuis le continent introduit la culture campaniforme dans l'île. La propagation du complexe campaniforme est associée au remplacement d'environ 90 % du patrimoine génétique existant en quelques centaines d'années. Cette migration se produit dans le prolongement de l'expansion vers l'ouest qui avait amené l'ascendance liée à la steppe pontique en Europe centrale et du nord au cours des siècles précédents et probablement les locuteurs de langues indo-européennes.

C’est pendant cette période que des monuments néolithiques, comme Stonehenge ou Avebury sont construits. En fondant ensemble de l’étain et du cuivre, tous les deux présents en abondance dans la région, les humains fabriquent du bronze, et plus tard du fer grâce au minerai de fer existant. Ils sont capables de tisser de la laine de mouton pour s’en faire des habits. Selon John T. Koch et d’autres historiens, l’Angleterre, à la fin de l’Âge du bronze, faisait partie d’un réseau commercial maritime, appelé l’Âge du bronze atlantique, qui inclut toute la Grande-Bretagne, ainsi que l’Irlande, la France, l’Espagne et le Portugal. Dans ces régions, les langues celtiques se développent: le tartessien est la plus lointaine langue celtique écrite découverte.

Pendant l’âge du fer, la culture celte, dérivant du Hallstatt et de La Tène, se propage jusqu’en Europe centrale. Le développement des fonderies de fer permet la construction de meilleures charrues, améliorant l’agriculture, et l’efficacité des armes. Les langues brittoniques sont parlées à l’époque. La société est tribale : la "Géographie" de Ptolémée recense environ vingt tribus dans la région, cependant les structures plus anciennes ne sont pas connues car les Bretons ne savaient ni lire ni écrire. Comme d‘autres régions des marges de l’Empire romain, de nombreux liens sont tissés avec les Romains. Jules César, de la République romaine, tente d’envahir deux fois la région en 55 av. J.-C.: bien que les invasions soient des échecs, il essaya de créer un royaume-client avec le chef des Trinovantes.

L'Empire romain conquiert l’Angleterre en 43, pendant le règne de l’empereur Claude, et la région est annexée à l’Empire romain, sous le nom de Bretagne. Les plus connus des peuples qui ont tenté de résister à l’invasion sont les Catuvellauni, menés par Caratacus. Plus tard, une révolte menée par Boadicée, reine des Iceni, est écrasée à la bataille de Watling Street. Cette nouvelle ère a vu l’existence d’une culture gréco-romaine, avec l’introduction de la loi et de l’ordre, l’architecture romaine, l’hygiène personnelle, des systèmes de cultures, l’éducation, et la soie. La "Britannia" désignait la province romaine qui couvrait l’Angleterre, le pays de Galles et le sud de l’Écosse du au début du . Au , l’empereur Septime Sévère meurt à York, où Constantin est proclamé empereur par la suite. Le christianisme est pour la première fois introduit au début du bien que cette origine soit contestée : on parle d’une introduction par l’intermédiaire soit de Joseph d'Arimathie, soit de saint Lucius. Vers 410, les Romains se retirent de l’île à mesure de leur perte de puissance, pour défendre leurs frontières en Europe continentale.

À la suite de la retraite romaine, l’Angleterre est laissée à l’abandon, et la région devient propice à des attaques de peuples marins païens, tels les Saxons et les Jutes qui prennent le contrôle de territoires dans le Sud-Est. Leurs avancées sont contenues pendant un temps après la victoire des Bretons insulaires à la bataille du Mont Badon. Les royaumes britanniques post-romains dans le Nord, plus tard réunis sous la dénomination galloise de "Hen Ogledd" (« vieux Nord »), ont peu à peu été conquis par les Angles au cours du . Les Irlandais effectuaient des raids sur la côte ouest de la Bretagne. Les Irlandais finissent par fonder de véritables principautés sur les côtes galloises et écossaises. Si les premières sont finalement écrasées, les secondes ont donné naissance à l'Écosse par la fusion du Dal Riada avec les royaumes britanniques du Nord.

Durant cette période sur laquelle les sources fiables font défaut (c'est l'Âge sombre ou "Dark Ages" de l'historiographie anglaise), des populations bretonnes peu romanisées établirent de nombreux royaumes bretons dans l'île de Bretagne, notamment dans le pays de Galles et d'autres migrèrent en Irlande. De même, là se trouve probablement la cause première d'une émigration en masse de Bretons vers la péninsule armoricaine, celle-ci prenant alors le nom de Bretagne. Il existe plusieurs théories qui s’opposent sur l’étendue et l’histoire de l’installation des Anglo-Saxons en Angleterre. Cerdic de Wessex a peut-être été un Breton insulaire. Un patchwork de royaumes anglo-saxons finit par émerger au sud et au centre de l’Angleterre, les principaux étant la Northumbrie, la Mercie et le Wessex. Le christianisme est réintroduit dans la région à partir de la fin du , au sud par la mission grégorienne d'Augustin, venu de Rome, et au nord par Aidan de Lindisfarne depuis l’Irlande.

Northumbrie et Mercie sont alors les forces dominantes. Toutefois, après les conquêtes vikings au nord et à l’est, et l’imposition du Danelaw, c'est-à-dire la loi des Vikings, le Wessex devient le premier royaume anglais sous Alfred le Grand. L'unification est le fait d'Édouard l'Ancien, roi de Wessex, assisté de sa sœur Æthelflæd, souveraine de Mercie dans les années 910 : l'Est-Anglie est conquise en 917, le royaume d'York en 918 mais à nouveau perdu en 919, la Northumbrie en 918. À la mort de sa sœur, en 918, Édouard annexe également la Mercie. Son fils et successeur Æthelstan cimente cette unification en prenant le titre de « roi des Anglais », une unification parachevée en 954 avec la victoire d’Eadred sur le Norvégien Éric à la Hache sanglante. Au début du , Knut le Grand intègre brièvement l’Angleterre dans un empire qui réunit aussi le Danemark et la Norvège. Plus tard, Édouard le Confesseur restaure la dynastie des Wessex.
En 1066, les Normands de Guillaume le Conquérant, depuis le duché de Normandie, s'emparent de l'Angleterre, chassant Harold II, dernier des rois anglo-saxons. Ceux que l'on appela « souverains anglo-normands » ouvriront le pays aux influences continentales. Ils introduisent le féodalisme et maintiennent l’ordre à travers la figure de barons, qui construisent des châteaux dans toute la région. La langue de cette nouvelle élite aristocratique est le normand, ce qui aura une influence considérable sur la langue anglaise.

Après la mort accidentelle du dernier représentant de la dynastie anglo-normande en 1135, la guerre civile éclate entre les différents prétendants et se répand sur l'ensemble des territoires d'outre-Manche. Geoffroy Plantagenêt finit par triompher. La maison des Plantagenêt d’Anjou hérite du trône d’Angleterre avec Henri II d'Angleterre, ajoutant l’Angleterre au bourgeonnant empire Plantagenêt, formé de fiefs en France dont l’Aquitaine. Ils règnent pendant trois siècles, et fournissent plusieurs monarques tels que Richard, Édouard, Édouard III, et Henri V. Cette période voit des mutations dans le commerce et la législation, avec notamment la signature de la "Magna Carta", une charte destinée à limiter les pouvoirs des souverains par la loi et protéger les privilèges des hommes. Le monachisme catholique prospère, fournit des philosophes et les universités d’Oxford et de Cambridge sont créées sous la protection royale. La Principauté de Galles devient un fief des Plantagenêt pendant le et la seigneurie d'Irlande est offerte à la monarchie anglaise par le pape.

Au cours du , les Plantagenêt et les Valois se réclament tous les deux de la maison des Capet, et par là-même, de la France: les deux puissances s’affrontent lors de la guerre de Cent Ans. L’épidémie de peste noire touche l’Angleterre en 1348, et a tué jusqu’à la moitié de ses habitants. De 1453 à 1487, deux branches de la famille royale (la Maison d’York et la Maison de Lancastre) se battent lors de la guerre des Deux-Roses. Elle mène à la défaite de la maison d’York, qui abandonne le trône à une famille noble galloise, les Tudor, une branche de la Maison de Lancastre, dirigée par Henri Tudor aidé de troupes galloises et de mercenaires bretons, qui remportent la victoire à la bataille de Bosworth, où le roi Richard III est tué.

Le règne des Tudor est mouvementé. La Renaissance parvient en Angleterre grâce aux courtisans italiens, qui réintroduisent les arts, l’éducation et les savoirs de l’Antiquité gréco-romaine. Pendant ce temps, l’Angleterre développe une flotte navale, invente le théodolite, et explore les mers à l'ouest. Ces explorations sont bloquées par l’Empire ottoman, qui contrôle la mer Méditerranée, et empêche le commerce maritime des États chrétiens de l’Europe avec l’Est méditerranéen.
Henri VIII rompt avec l’Église catholique romaine, à cause d’un désaccord sur un énième divorce royal, et proclame l’Acte de suprématie, en 1534, qui fait du monarque le chef de l’Église anglicane. Contrairement au protestantisme européen, les racines de ce schisme sont plus politiques que théologiques. Il incorpore aussi officiellement les terres galloises dans le royaume d’Angleterre par l’Acte d'Union (1536). Des conflits internes naissent durant les règnes des filles d’Henri VIII : Marie et Élisabeth. La première tenta de ramener le pays dans le giron catholique, tandis que la seconde rompra une seconde fois plus profondément encore, pour asseoir la suprématie de l’anglicanisme.

Une flotte anglaise sous le commandement de Francis Drake détruisit l’Invincible Armada durant l’ère élisabéthaine. À la lutte avec l’Espagne, la première colonie anglaise en Amérique est créée par l’explorateur Walter Raleigh en 1585, et l’appelle la Virginie. La Compagnie des Indes Orientales entre en compétition au Moyen-Orient avec les Pays-Bas et la France. La nature de l’île change elle aussi après la mort d'Élisabeth: les Stuart qui règnent alors sur l'Écosse accèdent au trône d'Angleterre. Partisans d'un absolutisme, leurs visées inquiètent certains Anglais qui craignent pour leurs droits. De plus, le catholicisme des Stuart fait craindre une remise en cause des réformes religieuses de la part des puritains. L’Union des Couronnes est proclamée en 1603 sous Jacques d'Angleterre. Le roi se nomme dès lors roi de Grande-Bretagne, bien que cela n’existe pas dans la loi anglaise.

Un conflit politique, religieux et social donne naissance à la Première Révolution anglaise entre les soutiens du Parlement et ceux du roi, respectivement appelés les «Têtes Rondes» et les «Cavaliers». Ce conflit provient d’un enchevêtrement de problématiques diverses, dans le contexte des guerres des Trois Royaumes, impliquant l’Écosse et l’Irlande. Les parlementaires sortent victorieux, Charles est exécuté. Un régime républicain est alors instauré sous le nom de "Commonwealth de l'Angleterre", dirigé par un Lord Protecteur en la personne de Oliver Cromwell, suivi de son fils Richard. 

À la démission de ce dernier, Charles II revient comme monarque en 1660. La Restauration des Stuart en 1660 durera à peine trente ans. Il apparaît toutefois que le roi et le Parlement doivent gouverner ensemble, bien que cela ne soit en pratique le cas qu’à partir du . La création la même année de la Royal Society encourage les sciences et les arts.

Le Grand incendie de Londres en 1666 frappe la capitale, mais elle est reconstruite peu après. Deux factions émergent dans le Parlement: les Tories, royalistes, et les Whigs, libéraux. Alors que les Tories soutiennent initialement le roi catholique Jacques II, plusieurs d’entre eux, avec le parti Whig, renversent le roi en 1688. la Glorieuse Révolution de 1688 porte le prince néerlandais Guillaume III d'Orange au pouvoir et confirme la monarchie protestante en Angleterre. La révolution financière britannique se traduit par un recours important à l'endettement pour financer la Royal Navy et les aménagements de rivière, tandis que se développe un marché boursier et des assurances. Quelques groupes anglais, particulièrement dans le nord avec les Jacobites, continuent de soutenir le roi Jacques et ses fils. En 1707, les royaumes d'Angleterre et d'Écosse, bien que dirigés par les mêmes souverains issus de la dynastie Stuart, ne forment plus qu'un seul Royaume de Grande-Bretagne, à la suite de la signature de l'Acte d'Union, dont la reine Anne de Grande-Bretagne en devient la première souveraine. Pour faciliter le rapprochement, les législations et les systèmes religieux restent séparés.

Sous le nouvellement formé Royaume de Grande-Bretagne, les Lumières anglaises et écossaises, avec la Royal Society, produisent de nombreuses innovations en science et ingénierie. Cela permet à l’Empire britannique de prospérer, pour devenir le plus grand de l’Histoire. La Révolution industrielle apparaît en Angleterre, qui provoque des profonds changements socioéconomiques et culturels. L’agriculture s’industrialise, les usines et les mines apparaissent, tout comme les réseaux routiers, ferroviaires et maritimes, qui facilitent l’expansion et le développement de la révolution industrielle. L’ouverture du Canal de Bridgewater au nord-ouest de l’Angleterre en 1761 introduit la nation dans une véritable frénésie de la construction de canaux.

L’Angleterre maintient une relative stabilité lors de la Révolution française. William Pitt le Jeune est premier ministre sous le règne de George III du Royaume-Uni. Pendant les guerres napoléoniennes, Napoléon Bonaparte prévoit d’envahir l’Angleterre par le sud-ouest. Toutefois, ces projets d’invasion échouent, d’abord après la défaite de Trafalgar face à Horatio Nelson, ensuite après la défaite de Waterloo, contre le général Wellington. Les guerres napoléoniennes ont encouragé le concept de la nation britannique, avec le sentiment d’un peuple britannique uni, partagé avec les Gallois et les Écossais.

En 1825, la première locomotive à vapeur transportant des passagers ouvre au public, sur le chemin de fer de Stockton-on-Tees et Darlington et la nation détient à elle seule la moitié des de rail européen en 1845, lors de l'épisode de la "railway mania".
Pendant la Révolution industrielle, beaucoup de travailleurs quittent les campagnes anglaises pour aller habiter dans des aires industrielles, et travailler dans des usines, par exemple à Manchester ou à Birmingham. L'expansion très tôt et très rapide des banques, fait de l'Angleterre le premier endroit au monde où la majorité de la population utilise des billets de banque. Pendant l’époque victorienne, Londres devient la plus grande et la plus peuplée des capitales du monde, et le commerce avec l’Empire britannique, tout comme la présence militaire et navale britannique, apporte beaucoup de prestige. L’agitation politique domestique menée par les Chartistes et les suffragettes aboutissent à une réforme législative et l’adoption du suffrage universel. Lors de la Première Guerre mondiale, des milliers de soldats anglais meurent dans les tranchées en France, car le pays fait partie des forces alliées. Deux décennies plus tard, pendant la Seconde Guerre mondiale, le Royaume-Uni combat à nouveau pour les forces alliées. Winston Churchill est alors le premier ministre. Les développements dans les technologies de guerre permettent un bombardement aérien massif du pays par l’Allemagne nazie, c’est le Blitz. Après la guerre, l’Empire britannique fait face à une décolonisation rapide. De nouvelles innovations technologiques voient le jour: l’automobile devient le premier moyen de transport du pays, et les recherches de Frank Whittle sur le moteur à réaction permettent un voyage aérien plus poussé. Les comportements sociaux des Anglais sont bouleversés par l’accès privé aux automobiles, et par la création du National Health Service, en 1948. Cette organisation publique fournit des soins gratuits aux habitants, selon les niveaux de vie de chacun. Tous ces changements ont accéléré la réforme des pouvoirs locaux au milieu du .

Le voit de nombreuses migrations vers l’Angleterre, en particulier depuis les îles Britanniques, mais aussi depuis le Commonwealth, dont le sous-continent indien. Depuis les années 1970, le secteur industriel périclite doucement, au profit des emplois du secteur tertiaire. L’Angleterre, qui fait partie du Royaume-Uni, rejoint le marché commun de la Communauté économique européenne, qui devient plus tard l’Union européenne. À la fin du , les pouvoirs politiques se sont de plus en plus décentralisés en Écosse, au Pays de Galles et en Irlande du Nord. L’Angleterre et le Pays de Galles continuent néanmoins d’exister au sein du Royaume-Uni comme une juridiction unique. La décentralisation a accru le sentiment d’appartenance à l’identité anglaise. Il n’y a pas de gouvernement anglais, et une tentative récente de créer un système similaire a été rejetée par référendum.

La loi dite ' de 1746 avait établi que l'« Angleterre » comprendrait le pays de Galles. Mais cette loi fut révisée en 1967 par la loi dite ' et, depuis cette date, l'« Angleterre » légale ne comprend plus le pays de Galles. L'Angleterre légale fut établie définitivement en 1974 en conséquence de l'effet de la loi dite "Local Government Act" de 1972 qui rattacha la ville de Berwick à l'Angleterre et le comté de Monmouthshire au pays de Galles.

Le , lors du référendum sur le maintien du Royaume-Uni dans l'Union européenne, 53,4 % des Anglais se prononcent pour la sortie du Royaume-Uni de l'Union européenne ("Brexit").

Faisant partie du Royaume-Uni, l’Angleterre est une monarchie constitutionnelle doublée d’un système parlementaire. Il n’y a pas de gouvernement d’Angleterre depuis 1707, quand l’Acte d’union, qui promulgue les termes du traité de l’Union, allie l’Angleterre et l’Écosse au sein du royaume de Grande-Bretagne. Avant cette union, l’Angleterre était dirigée par une monarchie et le Parlement d’Angleterre. Aujourd’hui, l’Angleterre est gouvernée par le Parlement du Royaume-Uni, même si certaines nations constitutives du Royaume-Uni ont des gouvernements décentralisés. À la Chambre des communes, qui est la chambre basse du parlement britannique basée au Palais de Westminster, à Londres, 532 membres du parlement représentent des circonscriptions anglaises, sur un total de 650 élus.

Lors des élections législatives de 2010, le parti conservateur, sur les 532 sièges mis en jeu, a remporté la majorité absolue en Angleterre avec 61 sièges d’avance sur le total des sièges des autres partis (le Président de la Chambre des communes n’est pas considéré comme un conservateur). Cependant, en tenant compte des résultats en Écosse, en Irlande du Nord et au Pays de Galles, ce n’est pas suffisant pour avoir une majorité absolue dans le Royaume-Uni. Aussi, afin de s’assurer la majorité absolue, le parti conservateur, dirigé par David Cameron, a créé une coalition avec les démocrates libéraux, menés par Nick Clegg. En conséquence, Gordon Brown a annoncé qu’il démissionnait de son poste de premier ministre et de dirigeant du parti travailliste.

Comme le Royaume-Uni est membre de l’Union européenne, des élections régionales se tiennent désormais pour élire les membres du Parlement européen. Les élections européennes de 2009 ont abouti à l’élection de 33 conservateurs, 10 travaillistes, 9 membres du parti pour l’indépendance du Royaume-Uni, 9 démocrates libéraux, 2 membres du parti vert de l’Angleterre et du Pays de Galles, et 2 membres du parti national britannique.

Depuis la décentralisation où les autres nations constitutives du Royaume-Uni (l’Écosse, l’Irlande du Nord et le Pays de Galles) ont eu un parlement délégué, les débats en Angleterre font rage pour déterminer comment contrebalancer cette réforme. Au départ, il était prévu que plusieurs régions anglaises auraient leurs propres assemblées locales, mais l’opposition de l’Angleterre du Nord-Est a enterré cette proposition.

La « West Lothian question » est l’un des débats en cours : des membres des parlements gallois ou écossais peuvent-ils statuer sur des lois qui concernent uniquement les Anglais ? Dans le contexte d’une Angleterre qui est la seule nation du Royaume-Uni à ne pas avoir de traitement contre le cancer gratuit, ce débat a donné lieu à une montée du nationalisme anglais. Certains ont donc proposé la création d’un parlement anglais dédié, ou bien de limiter les votes des lois qui concernent uniquement l’Angleterre, aux membres anglais du parlement.

Avec plus de 53 millions d’habitants, l’Angleterre est de loin la nation constitutive la plus peuplée du Royaume-Uni, avec environ 84 % de la population totale. Comparé aux autres pays dans le monde, l'Angleterre possède le grand nombre d’habitants dans l’Union européenne, et le dans le monde. Avec une densité de 395 habitants au kilomètre carré, c’est le deuxième État le plus densément peuplé de l’Union européenne, après Malte.

Le peuple anglais est un peuple britannique. Les recherches génétiques suggèrent que 75 à 95 % de la population anglaise descend en ligne paternelle de populations préhistoriques venues de la péninsule Ibérique. Elle inclut également un élément scandinave important, ainsi qu'une contribution de 5 % des Angles et des Saxons, bien que certains généticiens estiment l'élément scandinave et germanique à 50 %. Au cours du temps d'autres cultures ont exercé leur influence : peuples préhistoriques, Bretons insulaires, Romains, Anglo-Saxons, Vikings, Gaëls, sans oublier la profonde influence des Normands. Il existe une diaspora anglaise présente dans les anciennes colonies de l’Empire britannique, en particulier aux États-Unis, au Canada, en Australie, au Chili, en Afrique du Sud et en Nouvelle-Zélande. À la fin des années 1990, des Anglais ont émigré en Espagne.

Au temps du "Domesday Book", compilé en 1086, plus de 90 % des 2 millions d’Anglais vivent à la campagne. En 1801, la population est de 8,3 millions d’habitants, et elle augmente à 30,5 millions en 1901. Grâce notamment à la prospérité économique de l’Angleterre du Sud-Est, il y a de nombreuses migrations économiques depuis toutes les régions de l’Angleterre. L’immigration irlandaise a également été importante. La proportion d’immigrants d’origine européenne est de 87,5 %, en particulier allemands et polonais.

D’autres migrants venant des anciennes colonies britanniques se sont installés depuis 1950. 6 % des habitants viennent du sous-continent indien, notamment d’Inde et du Pakistan. 2,9 % de la population est noire, d’origine caribéenne principalement. Il y a un nombre important de Chinois, et de leurs enfants aujourd’hui Anglais. En 2007, 22 % des enfants en école primaire sont issus de minorités ethniques. Environ la moitié de l’augmentation de la population anglaise entre 1991 et 2001 est due à l’immigration. Le débat sur l’immigration revêt une forte importance en Angleterre, car selon un sondage du Département de l’Intérieur, 80 % des Anglais souhaitent le résoudre. L’ONS a prévu que la population augmenterait de 6 millions d’habitants entre 2004 et 2029.

En 2013, 27,3 % des nouveau-nés en Angleterre ( sur ) ont une mère née à l'étranger. 17,3 % ont une mère née hors du continent européen (9,8 % une mère née au Moyen-Orient ou en Asie, 5,4 % une mère née dans un pays d'Afrique noire et 2,1 % une mère née dans une autre région du monde).

Selon le recensement de 2011, près de 15 % des habitants de l'Angleterre appartiennent à des minorités ethniques (20,4 % chez les moins de 40 ans et 5 % chez les 60 ans et plus). Dans le Grand Londres, les minorités ethniques représentent 40,2 % de la population totale et 51,2 % des moins de 25 ans.

Comme son nom le suggère, l’anglais, aujourd’hui parlé par des centaines de millions de personnes dans le monde, est historiquement la langue de l’Angleterre, et en est toujours la langue principale. C’est une langue indo-européenne dans la branche anglo-frisone de la famille des langues germaniques. Après la conquête normande, le vieil anglais est confiné dans les classes sociales populaires, et le normand, le latin et l'anglo-normand le supplantent dans l’aristocratie. L'anglais utilise l'alphabet latin et les chiffres arabes comme système d’écriture.

Vers le , l’anglais revient à la mode dans toutes les classes sociales, bien que très modifié. Le moyen anglais montre de nombreux signes de l’influence normande, à la fois dans le vocabulaire et dans la prononciation. Pendant la Renaissance anglaise, de nouveaux mots sont forgés grâce au latin et au grec. L’anglais moderne étend cette habitude de flexibilité, en incorporant des mots depuis d’autres langues. En grande partie à l’aide de l’empire colonial britannique, l’anglais devient la "lingua franca" non officielle dans le monde.

L’apprentissage et l’enseignement de l’anglais est une importante activité économique, et implique l’existence d’écoles de langue, de voyages linguistiques en Angleterre, et de publications anglophones à l’étranger. Il n’y a pas de loi au niveau du Royaume-Uni qui reconnaît une langue officielle en Angleterre, mais l’anglais est la seule langue utilisée dans le monde des affaires. En dépit de la relative petite taille de la nation, il y a de nombreux accents régionaux distincts, et des personnes avec un accent particulièrement prononcé peuvent ne pas être facilement comprises ailleurs.

Le cornique, qui a disparu comme langue d’usage au cours du , est en train d’être ressuscité. Il est maintenant protégé par la charte européenne des langues régionales ou minoritaires. Il est parlé par 0,1 % des habitants en Cornouailles, et est enseigné dans plusieurs écoles, à n’importe quel âge. Les étudiants apprennent dans les écoles publiques une deuxième langue, souvent le français, l’allemand ou l’espagnol. De par l’immigration, un rapport montre qu’en 2007, étudiants parlent une langue étrangère à leur domicile, souvent le panjâbî ou l’ourdou.

Le National Health Service (NHS) est le système de santé public de l’Angleterre, qui fournit l’essentiel des soins dans le pays. Le NHS a été créé le 5 juillet 1948, appliquant la loi de 1946 sur le National Health Service. Cette loi se base sur les conclusions du rapport Beveridge, préparé par l’économiste et réformateur social William Beveridge. Le NHS est en majeure partie financé par les impôts, dont les montants de la National Insurance, et dispense la plupart de ses services gratuitement, même si des charges doivent être payées par certaines personnes, pour les soins ophtalmologiques et dentaires, ainsi que certains soins personnels.

Le ministère responsable du NHS est le département de la Santé, dirigé par le secrétaire d’état à la santé, membre du cabinet du Royaume-Uni. La majeure partie des dépenses du département de la santé concerne le NHS – environ 100 millions de livres sterling sont dépensées en 2008-2009. Récemment, le secteur privé est de plus en plus sollicité pour assurer des soins du NHS, malgré l’opposition des médecins et des syndicats. L’espérance de vie moyenne des habitants en Angleterre est de 77,5 ans pour les hommes, et de 81,7 ans pour les femmes, la moyenne la plus élevée des quatre nations constitutives du Royaume-Uni.

Le christianisme est la religion la plus pratiquée en Angleterre, comme c’est le cas depuis le haut Moyen Âge, bien qu’il ait été introduit plus tôt, pendant les ères gaéliques et romaines. La chrétienté a perduré pendant les siècles qui ont suivi, et aujourd’hui, environ 59 % des Anglais se considèrent chrétiens. La confession dominante dans le pays est l’anglicanisme, et date de la Réforme anglaise au , au moment du schisme de 1536 quand l’Église de Rome a refusé de valider le divorce entre Henri VIII et Catherine d'Aragon. La confession se considère à la fois comme catholique et réformée.

Il existe des traditions pour la Haute Église et la Basse Église, et quelques anglicans se considèrent comme anglo-catholiques, après le mouvement d’Oxford. La monarchie du Royaume-Uni est à la tête de l’Église, en tant que gouverneur suprême. L’anglicanisme a le statut de religion d’État. Il y a environ 26 millions de pratiquants et ils forment la Communion anglicane, et l’archevêque de Cantorbéry en est le représentant symbolique aux yeux du monde. De nombreuses cathédrales sont des bâtiments historiques d’une forte importance architecturale, comme l’abbaye de Westminster, la cathédrale d’York, la cathédrale de Durham ou celle de Salisbury.

La seconde confession chrétienne la plus répandue est l’Église latine de l’Église catholique, qui fait remonter son histoire en Angleterre au avec l’évangélisation d’Augustin de Cantorbéry, et a été la religion principale de l’île pendant près d’un millénaire. Depuis sa réintroduction après l’émancipation catholique, l’Église s’est réorganisée sur des bases anglaises et galloises où elle compte 4,5 millions de fidèles (dont la plupart sont anglais). Jusqu’à aujourd’hui, il y a eu un seul pape anglais, Adrien IV, et les saints Bède le Vénérable et Anselme de Cantorbéry sont considérés comme des docteurs de l’Église.

Une forme du protestantisme, connue sous le nom de méthodisme, est la troisième confession de l'Angleterre, et a grandi hors de l’anglicanisme grâce à John Wesley. Elle a gagné en popularité dans les villes-usines du Lancashire et du Yorkshire, et parmi les mineurs d’étain en Cornouailles. Il y a d’autres minorités non-conformistes, comme l’Église baptiste, les quakers, l’Église congrégationaliste, les unitaristes, et l’Armée du salut. L’immigration de l’île de Chypre (une ancienne colonie britannique) et du Moyen-Orient a introduit une minorité orthodoxe.

Le saint patron de l’Angleterre est saint Georges, représenté sur le drapeau national et sur l’Union Jack. Il y a de nombreux autres saints anglais, dont les plus connus sont Cuthbert, Alban, Wilfrid, Aidan, Édouard le Confesseur, John Fisher, Thomas More, Saint Pétroc, Saint Piran, Margaret Clitherow et Thomas Becket.

Des religions non-chrétiennes sont aussi pratiquées. Une minorité juive est présente sur l’île depuis 1070. Ils sont expulsés d’Angleterre en 1290 selon l’Édit d’Expulsion, et autorisés à revenir seulement en 1656. Dans les années 1950 en particulier, des religions orientales provenant des anciennes colonies britanniques font leur apparition due à l’immigration. L’islam est la principale religion importée, avec 4,8 % de pratiquants anglais, et suivent en nombre l’hindouisme, le sikhisme et le bouddhisme, qui rassemblent 2,7 % de la population ensemble, et qui viennent de l’Inde et de l’Asie du Sud-Est.

Environ 25 % de la population n'a pas de religion et 7,2 % ne déclare pas de religion.

Évolution de la part des différentes religions en Angleterre et au pays de Galles entre 2001 et 2011 :

L'économie anglaise est l'une des plus grandes au monde, avec un PIB moyen par habitant de livres sterling. Habituellement considérée comme une économie de marché mixte, elle a adopté de nombreux principes du libre marché, contrairement au capitalisme rhénan de l'Europe continentale. La monnaie officielle en Angleterre est la livre sterling, également connue sous le nom de GBP. La fiscalité en Angleterre est caractérisée à partir de 2009 par un taux de base de l'impôt des particuliers de 20 % du revenu imposable jusqu'à concurrence de livres sterling, et de 40 % sur les revenus supplémentaires au-dessus de ce montant.

L’Angleterre est le contributeur principal de l’économie du Royaume-Uni, qui est, selon la Banque mondiale, le 24 pays en termes de PIB (PPA) au monde. Les entreprises anglaises sont leaders dans les secteurs chimiques et pharmaceutiques, ainsi que dans les industries aérospatiales, de l’armement, et dans la conception de logiciels. Londres, où siège le London Stock Exchange, principale bourse des valeurs du Royaume-Uni et l’une des plus grandes en Europe, est le centre financier de l’Angleterre – 100 des 500 plus grandes entreprises européennes y ont leur siège social. Londres est le plus grand centre financier en Europe, et l’a aussi été en 2009 au niveau mondial.

La Banque d’Angleterre, fondée en 1694 par le banquier écossais William Paterson, est la banque centrale du Royaume-Uni. Pensée à l’origine comme une banque privée à disposition du gouvernement anglais, elle joue ensuite un rôle au sein du Royaume-Uni – l’institution est nationalisée depuis 1946. La Banque a le monopole de l’émission des devises en Angleterre et au Pays de Galles, mais pas dans les autres nations constitutives du Royaume-Uni. Sa Commission de Politique Monétaire a décentralisé la gestion de la politique monétaire et l’établissement des taux d’intérêts de chaque nations du Royaume-Uni.

L’Angleterre est hautement industrialisée, mais connaît depuis les années 1970 un déclin dans les industries lourdes et manufacturières, ainsi qu’une augmentation du secteur tertiaire dans l’économie. Le tourisme est devenu une activité importante, en attirant des millions de visiteurs chaque année dans la région. L'Angleterre exporte principalement des médicaments, des automobiles – même si de nombreuses marques sont aujourd’hui détenues par des entreprises étrangères, comme Rolls-Royce, Lotus, Jaguar, Land Rover ou Bentley Motors – du pétrole depuis les possessions anglaises en Mer du Nord et le champ de Wytch Farm, des moteurs d’avions, et des boissons alcoolisées. L’agriculture est intensive et fortement mécanisée, et produit 60 % des besoins en nourriture de la population avec seulement 2 % de la population active. Les deux tiers de la production sont consacrés au bétail, le reste aux moissons agricoles.

Les plus éminents chercheurs anglais dans le domaine des sciences et des techniques sont Isaac Newton, Michael Faraday, Robert Hooke, Robert Boyle, Joseph Priestley, J. J. Thomson, Charles Babbage, Charles Darwin, Stephen Hawking, Christopher Wren, Alan Turing, Francis Crick, Joseph Lister, Tim Berners-Lee, Paul Dirac, Andrew Wiles et Richard Dawkins. Quelques experts affirment que le premier concept d’un système métrique apparaît avec John Wilkins, alors premier secrétaire de la Royal Society, en 1668. En tant que lieu de naissance de la Révolution Industrielle, l’Angleterre abrite de nombreux inventeurs de la fin du au début du . Parmi les célèbres ingénieurs anglais se trouve Isambard Kingdom Brunel, plus connu pour la création du Great Western Railway, pour la fabrication d’une série de bateaux à vapeur et de nombreux ponts, ce qui provoque une révolution du transport public et des méthodes d’ingénierie. La Révolution Industrielle est née du moteur à vapeur de Thomas Newcomen. Le physicien Edward Jenner met au point le vaccin contre la variole, puis reconnu pour avoir « sauvé plus de vies […] que toutes celles qui ont été perdues dans toutes les guerres de l’Humanité de l’Histoire connue ».

De nombreuses inventions et découvertes ont été réalisées en Angleterre, comme le moteur à réaction, la première machine à filer industrielle, le premier ordinateur et le premier ordinateur moderne, le World Wide Web ainsi que le protocole HTTP et le langage HTML, la première transfusion sanguine réussie, l’aspirateur, la tondeuse à gazon, la ceinture de sécurité, l’aéroglisseur, le moteur à vapeur, le moteur électrique, ou encore la théorie de l'évolution de Charles Darwin et la théorie atomique. Isaac Newton a développé la loi de la gravitation universelle, la mécanique newtonienne, et calcul infinitésimal. Robert Hooke a donné son nom à la loi d’élasticité des solides. L’Angleterre est aussi à l’origine du chemin de fer, du thermosiphon, du tarmac, de l’élastique, du piège à souris, de la lampe à incandescence, de la locomotive à vapeur, du semoir moderne, et de nombreuses nouvelles techniques et technologies développées sont utilisées en ingénierie de précision.

Le département des Transports est le ministère responsable de la supervision des transports en Angleterre. Il y a de nombreuses autoroutes et beaucoup de routes nationales, comme la route A1, qui traverse l’Est de l’Angleterre, de Londres à Newcastle (la plus grande partie de celle-ci est une autoroute), et ce jusqu’à la Frontière entre l'Angleterre et l'Écosse. L’autoroute anglaise la plus longue est la M6, et part de Rugby jusque dans le Nord-Ouest de la région. Les autres routes principales sont la M1 de Londres à Leeds, la M25 qui entoure Londres, la M60 autour de Manchester, la M4 de Londres au pays de Galles du Sud, la M62 de Liverpool au Yorkshire de l’Est "via" Manchester, et la M5 de Birmingham à Bristol vers le sud-ouest.

Le transport en bus dans la région est très répandu, les principales compagnies sont National Express, Arriva et Go-Ahead Group. Les bus rouges à impériale à Londres sont devenus un symbole de l’Angleterre. Il existe deux réseaux ferroviaires à grande vitesse : le métro de Londres, et le métro Tyne and Wear qui dessert Newcastle, Gateshead et Sunderland. Il y a aussi plusieurs réseaux de tramways, comme celui de Blackpool, le Manchester Metrolink, le Sheffield Supertram, le Midland Metro, ou le Tramlink, système basé à Croydon dans le Sud de Londres.
Le réseau ferroviaire anglais est le plus ancien au monde, et a accueilli ses premiers passagers dès 1825. La majeure partie des kilomètres du réseau de chemin de fer britannique se trouve en Angleterre, uniformément répartis, même si une forte proportion des lignes est fermée après la seconde guerre mondiale. L’écartement des rails est en général standard (pour les voies uniques, doubles ou multiples), et il existe aussi quelques voies étroites, principalement les chemins de fer héritages. Il est possible d’aller en train, en France et en Belgique, grâce au Tunnel sous la Manche, terminé en 1994.
L’Angleterre possède un réseau aérien étendu, domestique et international. Le plus grand aéroport est l’aéroport de Londres Heathrow, et il est le premier mondial en nombre de passagers internationaux. Les aéroports de Londres Gatwick, de Manchester, de Londres Stansted, de Londres Luton, ou de Birmingham sont également importants. La mer permet le transport en ferry, localement et internationalement, dont l’Irlande, les Pays-Bas et la Belgique. Il y a environ kilomètres de voies navigables en Angleterre, dont la moitié est la propriété de British Waterways, même si ce type de transport est limité. La Tamise est la voie navigable la plus empruntée en Angleterre, car les exportations et importations sont concentrées sur le Port de Tilbury dans l’estuaire de la Tamise. C’est l’un des trois ports les plus importants du Royaume-Uni.

Le système scolaire britannique se caractérise par des différences entre les régions du Royaume-Uni (Angleterre, Pays de Galles, Écosse, Irlande du Nord) et le fait que le système est semi-privatisé.

Le cursus se déroule généralement ainsi :

Le sondage réalisé par le programme PISA en 2006 situe le système éducatif britannique nettement au-dessus de la moyenne OCDE1.
Le système éducatif anglais est assez proche des systèmes gallois et écossais. Il est obligatoire d’aller à l'école primaire ("primary school") à partir de l’âge de 5 ans. Les deuxième et sixième années comportent chacune un examen, respectivement "Key Stage 1" et "Key Stage 2". Après l'école primaire, on a le choix entre deux types d'établissement : "grammar school" (lycée) et "comprehensive state" (lycée général). Pour entrer dans une "grammar school", il faut passer un examen nommé « 11+ ». Les "Grammar schools" étant pour les étudiants les plus doués, et les places étant limitées, la grande majorité des enfants fréquentent les "state comprehensive". Les familles ayant plus de moyens optent à la grande majorité pour le système privé.
À partir de l’âge de 11 ans, on étudie pour passer des examens "Key Stage 3". Après avoir réussi la neuvième année, les enfants choisissent les matières pour les examens "General Certificate of Secondary Education" (GCSE). 15 à 40 matières sont à choisir en fonction de l'école, dont seulement cinq sont obligatoires (les maths, l'anglais, et les trois sciences : biologie, physique, chimie). Les écoles exigent en règle générale de leurs élèves qu'ils choisissent au minimum six matières, mais la plupart en demanderont entre neuf et douze. On passe les examens pendant l’été de l’année 11 (équivalent à la fin de la seconde en France) après deux ans d’étude. Après les GCSE, on a le choix entre quitter l’école et continuer d'étudier pour les "A-levels" (semblables au baccalauréat). Habituellement, les étudiants sélectionnent 4 matières mais certains optent pour 5.
En parallèle de ce système d'éducation publique subsiste le système d'éducation privé, élitiste et réservé de par son coût élevé à la classe moyenne supérieure, petite et grande bourgeoise et à la noblesse britannique. Les élèves commencent dans ce système par fréquenter une école préparatoire jusqu'à l'âge de treize ans, puis deviennent pensionnaires dans une "public schools" jusqu'à leurs 18 ans. Ces écoles bénéficient d'un statut et d'une renommée internationaux, et l'éducation se fait au prix d'environ livres sterling par an. Eton, surnommée « la "nursery" de l'élite », et qui a accueilli de nombreux futurs premiers ministres, membres de la famille royale britannique et têtes couronnées étrangères, est probablement l'exemple le plus emblématique de ces écoles dites indépendantes, privées malgré leur nom de "public school".

Il y a de nombreuses universités en Angleterre. Les deux les plus célèbres et les plus vielles sont l'Université d'Oxford et l'Université de Cambridge.

Un grand nombre de vieux menhirs a été érigé pendant la préhistoire, parmi les plus connus sont Stonehenge, Devil's Arrows, Monolithe de Rudston et Cromlech de Castlerigg. Avec l'introduction de l'architecture romaine il y a eu un développement des basiliques, thermes, amphithéâtres, arcs de triomphe, villas, temples romains, voies romaines, forts romains, aqueducs, palissades. Ce sont les Romains qui ont fondé les premières cités et villes telles que Londres ("Londinium"), Bath ("Aquae Sulis"), York ("Eburacum"), Chester ("Deva") et St Albans ("Verulamium"). L'exemple le plus connu est peut-être le Mur d'Hadrien qui s'étend dans tout le nord de l'Angleterre. Un autre exemple bien préservé est les thermes de Bath à Bath, ville du comté de Somerset, au Sud-Ouest de l'Angleterre.

Les constructions séculaires de l'architecture anglo-saxonne étaient des constructions simples qui utilisaient principalement le bois avec de la chaume pour les toitures. L'architecture ecclésiastique a varié passant d'une synthèse de monachisme irlando-saxon à un art paléochrétien caractérisé par des pilastres, des arcades, des balustres et des triangulaires. Après la conquête normande en 1066 de nombreux châteaux en Angleterre ont été créés afin que les Law Lords puissent confirmer leur autorité et se mettre à l'abri de l'invasion venant du nord. Certains des plus célèbres châteaux médiévaux incluent parmi tant d'autres la tour de Londres, le château de Warwick, le château de Durham et le château de Windsor.

Tout au long de l'ère Plantagenêt une architecture gothique anglaise n'a cessé de fleurir, les cathédrales médiévales telles que la cathédrale de Cantorbéry, l'abbaye de Westminster et la Cathédrale d'York, en sont de parfaits exemples. S'inspirant de l'architecture normande il y avait aussi des châteaux, des palais, des grandes maisons, des universités et des églises paroissiales. L'architecture médiévale a été achevée avec le style Tudor du , connu maintenant comme l'arche Tudor, qui était une caractéristique définie comme l'étaient les maisons en torchis. Au lendemain de la Renaissance une forme d'architecture rappelant l'Antiquité classique, mélangée avec le christianisme apparut, le style baroque anglais. L'architecte Christopher Wren célèbre pour son rôle dans la reconstruction de Londres après le grand incendie de 1666 en est un des plus notables promoteurs.

L'architecture géorgienne suivit, dans un style plus raffiné, évoquant une simple forme palladienne, le Royal Crescent à Bath est l'un des meilleurs exemples. Avec l'émergence du romantisme au cours de la période victorienne, un style néogothique a été lancé dans le même temps, la révolution industrielle a ouvert la voie pour des bâtiments tels que le Crystal Palace. Depuis les années 1930 diverses formes modernistes ont fait leur apparition dont l'accueil est souvent controversé, même si les mouvements de résistance traditionaliste continuent avec le soutien de lieux d'influence. 

Le folklore anglais s’est développé pendant plusieurs siècles. Quelques-uns des personnages et des histoires existent à travers toute l’Angleterre, mais la plupart proviennent de régions très spécifiques. Les êtres folkloriques communs sont des pixies, des elfes, des croque-mitaines, des trolls, des gobelins, et des nains. Alors que de nombreuses légendes semblent très anciennes, par exemple celles qui mettent en scène Offa d'Angeln ou Völund, d’autres ont été conçues après l’invasion normande. La légende de Robin des Bois et ses Joyeux Compagnons de la forêt de Sherwood qui combattent le shérif de Nottingham, est sûrement la plus connue d’entre toutes.

Pendant le Moyen Âge classique, des légendes originaires de mythes brittoniques intègrent le folklore anglais : le mythe arthurien. Elles proviennent de sources anglo-normandes, françaises ou galloises, et mettent en avant le Roi Arthur, Camelot, Excalibur, Merlin, et les Chevaliers de la Table Ronde, comme Lancelot du Lac. Ces histoires sont rapportées par Geoffroy de Monmouth dans son "Historia regum Britanniae". Une autre figure précoce des traditions britanniques, Coel Hen, semble être basé sur un personnage réel de l’Angleterre post-romaine. Beaucoup de ces légendes et pseudo-histoires font partie de la matière de Bretagne.

Plusieurs personnages folkloriques sont fondés sur des personnes historiques, ou en partie, dont l’histoire a traversé les siècles. Par exemple, on dit de Godiva qu’elle a traversé Coventry nue à dos de cheval, Hereward l'Exilé est une figure anglaise héroïque représentant la résistance à l’invasion normande, Herne est un cavalier fantôme associé à la forêt de Sherwood et au Windsor Great Park, et la Mère Shipton est l’archétype de la sorcière. Dick Whittington était une personne réel, avec la légende de son chat ajoutée. Chaque année, les 5 novembre sont l’occasion de faire des feux de joie, de tirer des feux d’artifices, et de manger des pommes d’amour en commémoration à la Conspiration des poudres, centrée autour du personnage de Guy Fawkes. Le bandit chevaleresque, comme Dick Turpin, est un personnage récurrent, et Barbe Noire est le pirate typique. Il y a de nombreuses activités folkloriques, que ce soit au niveau national ou régional, comme la Morris dance, la Maypole dance, la Rapper sword (une danse de l’épée) dans le Nord-Est, la Long Sword dance dans le Yorkshire, les Mummers Plays, le bottle-kicking dans le Leicestershire, et le Cooper's Hill Cheese-Rolling and Wake à Brockworth. Il n’y a pas de costume officiel national, mais certains ont une certaine renommée comme les Pearly Kings and Queens londoniens, les Scots Guards, et les Beefeaters (gardes de la tour de Londres).

Depuis le début de l'époque moderne, la nourriture de l'Angleterre a toujours été caractérisée par sa simplicité d'approche, l'honnêteté de sa saveur, et le recours à la haute qualité des produits naturels. Pendant le Moyen Âge et pendant la Renaissance, la cuisine anglaise était dotée d'une excellente réputation, malgré un déclin qui a commencé pendant la révolution industrielle avec l'abandon de la terre et l'urbanisation croissante de la population. Mais la nourriture anglaise a souvent, de nos jours, la réputation de ne pas être très sophistiquée ou même rudimentaire. Cependant, la cuisine anglaise a récemment connu un renouveau, qui a été reconnu par les critiques gastronomes avec quelques bonnes évaluations dans le magazine britannique "Restaurant". Le premier livre de recettes anglaises est le "Forme of Cury" de la cour royale de Richard II.

Les exemples traditionnels de la cuisine anglaise incluent le Sunday roast; comportant généralement du bœuf, de l'agneau ou du poulet, servi avec un assortiment de légumes bouillis, du Yorkshire pudding et de la sauce. D'autres repas importants incluent le fish and chips et le petit déjeuner anglais comprenant bacon, tomates grillées, pain frit, boudin noir, baked beans, champignons frits, saucisses et œufs. Plusieurs tourtes à la viande sont consommées telles que le , le shepherd's pie, le cottage pie, le Cornish pasty et le , le dernier étant consommé froid.

Les saucisses sont couramment consommées, soit en tant que bangers and mash ou toad in the hole. Le Lancashire hotpot est un ragoût bien connu. Certains des fromages les plus populaires sont le cheddar, le cheshire, le red leicester, le stilton et le Wensleydale. De nombreux plats hybrides anglo-indiens avec du curry ont été créés tels que le poulet tikka masala et le balti. Les plats anglais sucrés comprennent l'apple pie, la mince pie, le spotted dick, les scones, l'eccles cake, la custard et le sticky toffee pudding. Les boissons commune sont le thé, qui est très consommé grâce à Catherine de Bragance, tandis que des boissons alcoolisées comprennent les vins et les bières britanniques.

Les premiers exemples connus sont les pierres préhistoriques et les pièces d’art pariétal, en particulier dans le Yorkshire du Nord, la Northumbrie et la Cumbrie, et aussi plus loin dans le sud de l'Angleterre, dans les grottes de Creswell. Avec l’arrivée de la culture romaine au , de nouvelles formes d’art apparaissent : des statues, des bustes, le travail du verre et les mosaïques. De nombreux artefacts ont été retrouvés, à la villa romaine de Lullingstone et à Aldborough. Durant le haut Moyen Âge, le style évolue vers les croix sculptées, la peinture de manuscrits, la joaillerie d’or et d’émail, et démontre un goût pour les dessins entrelacés, comme le trésor de Staffordshire découvert en 2009. Ce style fusionne ensuite avec l'art insulaire, comme en témoignent les Évangiles de Lindisfarne et le Psautier Vespasien. Plus tard, l’art gothique devient populaire à Winchester et à Cantorbéry, par exemple avec le Livre des bénédictions de St. Æthelwold ou le psautier de Luttrell.

De nombreux artistes accompagnent l’ère des Tudor au sein même de la Cour, qui font de la peinture de portraits un élément fondamental et durable de l’art anglais. L’Allemand Hans Holbein en est le chef de file, et Nicholas Hilliard continue dans cette voie. Pendant l'ère Stuart, les artistes continentaux, en particulier les peintres flamands, influencent l’Angleterre, comme Anthony Van Dyck, Peter Lely, Godfrey Kneller et William Dobson. Le est marqué par la création de la Royal Academy, et un certain classicisme, fondé sur l’art de la Renaissance, supplante les autres styles. Thomas Gainsborough et Joshua Reynolds deviennent deux des artistes les plus prisés d’Angleterre.

L’école de Norwich perpétue la tradition des paysages, pendant que le préraphaélisme, dans un style détaillé et éclatant, faire revivre le style de la Renaissance : les leaders du mouvement sont Holman Hunt, Dante Gabriel Rossetti et John Everett Millais. Au , les artistes remarquables sont Henry Moore, sculpteur, et les modernistes britanniques en général. Parmi les peintres contemporains, Lucian Freud détient le record mondial de la vente en valeur d’un tableau, pour un artiste vivant, avec "Benefits Supervisor Sleeping", en 2008.

Les premiers auteurs anglais écrivent en latin, comme Bède le Vénérable et Alcuin. La littérature vieil-anglaise fournit le poème épique "Beowulf", la "Chronique anglo-saxonne", ainsi que des écritures chrétiennes comme "Judith", l’"Hymne" de Cædmon et de nombreuses hagiographies. Après la conquête normande de l’Angleterre, le latin survit parmi les classes sociales éduquées, ainsi que dans la littérature anglo-normande.

La littérature en moyen anglais a émergé avec Geoffrey Chaucer (auteur des "Contes de Canterbury"), John Gower, le Pearl Poet et William Langland. Les moines franciscains, Guillaume d'Ockham et Roger Bacon sont les philosophes majeurs du Moyen Âge. Julienne de Norwich avec ses "Revelations of Divine Love", est une éminente écrivaine chrétienne. Avec la Renaissance anglaise, la littérature en anglais moderne naissant apparaît. William Shakespeare, dont les œuvres les plus connues sont "Hamlet", "Roméo et Juliette", "Macbeth", et "Le Songe d’une nuit d’été", reste l’un des auteurs les plus célèbres de la littérature anglaise.

Christopher Marlowe, Edmund Spenser, Philip Sidney, Thomas Kyd, John Donne, Ben Jonson sont d’autres auteurs connus de la littérature élisabéthaine. Francis Bacon et Thomas Hobbes ont écrit sur l’empirisme et le matérialisme, ainsi que sur la méthode scientifique et le contrat social. Robert Filmer a écrit sur le droit divin. Andrew Marvell est le poète le plus fameux du Commonwealth, pendant que John Milton publie Le Paradis perdu pendant la Restauration anglaise.

Quelques-uns des plus remarquables philosophes des Lumières anglaises sont John Locke, Thomas Paine, Samuel Johnson et Jeremy Bentham. Edmund Burke suit plus tard et est considéré aujourd’hui comme le fondateur du conservatisme. Le poète Alexander Pope et ses vers satiriques sont de plus en plus connus. Puis les auteurs anglais jouent un rôle significatif dans le romantisme, et Samuel Taylor Coleridge, Lord Byron, John Keats, Mary Shelley, Percy Bysshe Shelley, William Blake, William Wordsworth et Alfred Tennyson deviennent des figures reconnues.

En réaction à la Révolution industrielle, les auteurs agrariens cherchent une voie entre la liberté et la tradition. William Cobbett, G. K. Chesterton et Hilaire Belloc en sont les principaux représentants, en lien avec Arthur Penty et le socialisme corporatif, et G. D. H. Cole et le mouvement coopératif. L’empirisme continue avec John Stuart Mill et Bertrand Russell, tandis que Bernard Williams s’implique dans la philosophie morale. Les auteurs de l’ère victorienne sont Charles Dickens, la famille Brontë, Jane Austen, George Eliot, Rudyard Kipling, Thomas Hardy, H. G. Wells, Lewis Carroll ou bien encore Evelyn Underhill. Depuis, l’Angleterre a enfanté de nombreux romanciers devenus très célèbres, comme George Orwell, D. H. Lawrence, Virginia Woolf, Enid Blyton, Aldous Huxley, Agatha Christie, Terry Pratchett, J. R. R. Tolkien, et J. K. Rowling.

La musique traditionnelle britannique est vieille de plusieurs siècles et a inspiré plusieurs genres musicaux, parmi lesquels les chants de marins, les gigues, les hornpipes, et des danses traditionnelles. Ces musiques ont de nombreuses variations, et des particularités régionales. Au , Wynkyn de Worde imprime des ballades de Robin des Bois qui ont un fort retentissement, tout comme "The English Dancing Master" de John Playford ou les "Roxburghe Ballads" de Robert Harley. Les chansons les plus connues à cette époque sont "The Good Old Way", "Pastime with Good Company", "Maggie Mae", et "Spanish Ladies", parmi d’autres. De nombreuses comptines sont d’origine anglaise, telles "Twinkle Twinkle Little Star", "Roses Are Red", "Jack and Jill", "Here We Go Round the Mulberry Bush", ou "Humpty Dumpty".

Les premiers compositeurs anglais de musique classique sont des artistes de la Renaissance, Thomas Tallis et William Byrd, suivis de Henry Purcell pendant la période baroque. George Frideric Handel, né allemand, devient un sujet britannique, et passe la majeure partie de sa vie de compositeur à Londres, créant quelques-uns des morceaux les plus célèbres de la musique classique, comme le "Messie", "Water Music" et "Music for the Royal Fireworks". Le paysage des compositeurs classiques anglais se renouvelle complètement au , avec Benjamin Britten, Frederick Delius, Edward Elgar, Gustav Holst, Ralph Vaughan Williams, William Walton. Aujourd’hui, Michael Nyman en est le chef de file, grâce à "La Leçon de piano".

Dans le champ de la pop et du rock, de nombreux artistes et groupes anglais sont régulièrement cités comme les musiciens les plus influents et les plus prolifiques de tous les temps en termes de vente. En font partie, par exemple, The Beatles, Led Zeppelin, The Who, Pink Floyd, Elton John, Genesis, Queen, Black Sabbath, Iron Maiden, Rod Stewart, The Rolling Stones, The Stone Roses, The Libertines, Blur, Oasis, et plus récemment, Adele, Ed Sheeran, Arctic Monkeys et One Direction. De très nombreux genres musicaux ont leurs racines, ou un fort lien de parenté, avec l’Angleterre, comme la British invasion, le hard rock, le glam rock, le heavy metal, le mod, la Britpop, le drum and bass, le rock progressif, le punk rock, le rock indépendant, le rock gothique, le shoegazing, l’acid house, l’UK garage, le trip hop et le dubstep.

Les grands festivals de musique en plein air en été et à l’automne attirent beaucoup de monde, tels le festival de Glastonbury, le V Festival, et les Reading and Leeds Festivals. L’opéra le plus remarquable en Angleterre est le Royal Opera House, à Covent Garden. The Proms, qui est une saison de musique classique au Royal Albert Hall, est un événement culturel majeur annuel. The Royal Ballet est au premier rang des compagnies de ballet classique. Sa réputation provient de deux grandes figures de la danse du , "prima ballerina" Margot Fonteyn et le chorégraphe Frederick Ashton.

English Heritage est un organisme gouvernemental avec une large attribution en matière de gestion du patrimoine historique de l'Angleterre. Il est actuellement parrainé par le département de la Culture, des Médias et du Sport. L'organisme associatif National Trust for Places of Historic Interest or Natural Beauty tient un rôle contrasté. Dix-sept des vingt-cinq sites du patrimoine mondial de l'UNESCO au Royaume-Uni mondial relèvent des lieux historiques de l'Angleterre. Certains des plus connus incluent le mur d'Hadrien, Stonehenge, Avebury and Associated Sites, la tour de Londres, le littoral du Dorset et de l'est du Devon, Saltaire, Ironbridge Gorge, le parc de Studley Royal et plusieurs autres.

Il y a de nombreux musées en Angleterre, mais le plus remarquable est le British Museum de Londres. Sa collection de plus de sept millions d'objets provenant de tous les continents est l'un des plus grands et le plus complet dans le monde, illustrant et documentant l'histoire de la culture humaine depuis son commencement jusqu'à nos jours. La British Library de Londres est la bibliothèque nationale et est l'une des bibliothèques de recherche les plus importantes au monde, maintenant plus de 150 millions d'articles dans toutes les langues connues et tous les formats, incluant environ 25 millions de livres. La galerie d'art la plus ancienne est la National Gallery de Trafalgar Square, qui abrite une collection de plus de 2300 peintures datant du milieu du à 1900. Les galeries de la Tate abritent des collections nationales d'art britannique moderne et international; elles sont également les hôtes du controversé prix Turner.

L'Angleterre a un héritage sportif très fort, et pendant le la nation a codifié de nombreux sports qui se jouent maintenant dans le monde entier. Les sports originaires de l'Angleterre sont le football, le cricket, le rugby à XV, le rugby à XIII, le tennis, le badminton, le squash, le rounders, la boxe, le snooker, le billard, les fléchettes, le tennis de table, le boulingrin, le netball, courses de chevaux pur-sang et la chasse à courre. Il a aidé le développement de la voile et la Formule 1. Le football est le plus populaire de ces sports. L'équipe d'Angleterre de football, qui joue dans le mythique stade de Wembley, a remporté la Coupe du monde de football de 1966, année où la nation a accueillie la compétition.

Au niveau du football, l'Angleterre est reconnue par la FIFA comme le lieu de naissance de ce sport, avec notamment le club de football de Sheffield FC fondé en 1857 et qui demeure le plus ancien club de football. La Fédération d'Angleterre de football formée en 1863, est la plus ancienne association nationale de football au monde et joua un rôle important dans la mise en place des règles de son sport. La FA Challenge Cup et la Football League ont été respectivement les toutes premières compétitions. Dans l'ère moderne la Premier League est la ligue de football la plus lucrative du monde et fait partie de l'élite internationale. La Ligue des champions a été remporté par Chelsea, Liverpool, Manchester United, Nottingham Forest et Aston Villa, tandis que Arsenal et Leeds United ont atteint la finale.

Le cricket est généralement considéré comme ayant été élaboré en début de la période médiévale parmi les communautés agricoles et de la métallurgie du Weald. L'Équipe d'Angleterre de cricket est l’équipe nationale de cricket de l’Angleterre et du Pays de Galles. Une des rivalités au sommet de ce jeu est la série de test-matchs appelée The Ashes entre l'Angleterre et l'Australie, qui se déroule depuis 1882. La finale de l'édition 2009 a été regardée par près de 2 millions de personnes, bien que le point culminant fut l'édition de 2005 qui a été vue par 7,4 millions. L'Angleterre est actuellement la détentrice du trophée et est classée à la fois des compétitions de cricket Test et One-day International.

L'Angleterre a accueilli quatre Coupe du monde de cricket (1975, 1979, 1983, 1999) et l'ICC World Twenty20 en 2009. Il existe plusieurs compétitions de niveau national, y compris le County Championship dans lequel l'équipe de Yorkshire est de loin le club le plus brillant et avoir remporté la compétition à 31 reprises. Le Lord's Cricket Ground situé à Londres, est parfois appelé la « Mecque du cricket ». William Penny Brookes a été important dans l'organisation du cricket pour les Jeux olympiques modernes. Londres a accueilli les Jeux olympiques d'été en 1908 et 1948, et les accueillera de nouveau en 2012. L'Angleterre prend part aux Jeux du Commonwealth, qui se déroule tous les quatre ans. Sport England est l'organe directeur responsable de la distribution des fonds et fournit des orientations stratégiques pour l'activité sportive en Angleterre. Le Grand Prix crée en 1926 se tient chaque année à Silverstone.

L'équipe d'Angleterre de rugby à XV, considérée comme l’une des meilleures sélections nationales au monde, a remporté la Coupe du monde en 2003. Par ailleurs, l’Angleterre a été l'une des nations hôtes de la compétition en 1991 et en 1999. Elle l’accueille de nouveau en 2015. Les clubs de l'élite anglaise, surtout basés dans le sud du pays, participent à la Premiership et à la coupe anglo-galloise. Les clubs des Leicester Tigers, des London Wasps, de Bath et des Northampton Saints ont remporté la coupe d’Europe.

Dans une autre forme de rugby qui est né à Huddersfield en 1895, l'équipe d'Angleterre de rugby à XIII est classée en tant que nation dans le monde et première en Europe. L'Angleterre par l'intermédiaire du Royaume-Uni accueille en 2013 la Coupe du monde de rugby à XIII. Les clubs du rugby à XIII, surtout dans le nord, rencontrent un fort succès: Wigan Warriors, St Helens, Leeds Rhinos et Huddersfield Giants, les trois premiers ayant gagné le World Club Challenge.

En tennis, le Tournoi de Wimbledon qui se déroule chaque année à Londres est le plus ancien tournoi de tennis au monde.

Le drapeau national de l'Angleterre, connu sous le nom de la Croix-Saint-Georges, a été le drapeau national depuis le . À l'origine, le drapeau a été utilisé par la République de Gênes. Le monarque anglais a rendu un hommage au doge de Gênes à compter de 1190, de sorte que les navires anglais pouvaient battre le pavillon comme un moyen de protection lors de l'entrée en mer Méditerranée. Une croix rouge a agi comme un symbole pour de nombreux croisés aux . Il est devenu associé à Georges de Lydda, qu'ils revendiquaient comme leur saint patron et utilisaient sa croix comme un étendard. Depuis 1606 la Croix de St George a fait partie de la conception du Union Jack, drapeau britannique conçu par le roi James I.

Il existe de nombreux autres symboles et objets symboliques, à la fois officiels et non officiels, y compris la rose Tudor, l'emblème floral national, le Dragon Blanc et les "Trois Lions" présentés sur les Armoiries royales de l'Angleterre. La rose Tudor a été adoptée comme l'emblème national de l'Angleterre à l'époque de la guerre des Deux-Roses (1455–1485) entre les maisons d'York et de Lancastre avec la victoire d'Henri Tudor (futur Henri VII) sur Richard III, lors de la bataille de Bosworth, comme un symbole de paix. Il est un symbole syncrétique dans lequel ont fusionné la rose blanche des York et la rose rouge des Lancasters-cadets des Plantagenêt qui s'affrontèrent pour le contrôle de la maison royale. Le symbole est également connu sous le nom de la "Rose d'Angleterre". Le chêne est le symbole de l'Angleterre, représentant force et courage. Le terme « Chêne royal » ("Royal Oak") est utilisé pour désigner l'évasion du roi Charles II qui, pourchassé par les parlementaires après l'exécution de son père, s'est caché dans un chêne pour éviter d'être repéré avant de s'exiler en toute sécurité.

Les armoiries royales d'Angleterre, un blason national avec trois lions, remontent à leur adoption par Richard d'Angleterre de 1198 à 1340. Il est décrit « gueules à trois léopards d'or », c'est-à-dire rouge avec trois lions jaunes regardant le spectateur. L'origine de ce blason des rois d'Angleterre reste sans explication définitive: , il s'agit de l'union de l'héraldique du duché de Normandie (« de gueules à deux léopards d'or ») et de l'Aquitaine (« de gueules à un léopard d'or »).

L'Angleterre n'a pas d'hymne national officiel à la différence du Royaume-Uni et son hymne "God Save the Queen". Toutefois, les éléments suivants sont souvent considérés comme des hymnes nationaux anglais non officiels : "Jerusalem", "Land of Hope and Glory" (utilisé pour l'Angleterre pendant les Jeux du Commonwealth de 2002), et "I Vow to Thee, My Country". La fête nationale anglaise est la Saint George, ce dernier étant le saint patron de l'Angleterre, elle se tient chaque année le 23 avril.





</doc>
<doc id="4930" url="https://fr.wikipedia.org/wiki?curid=4930" title="Liste des chefs d'État du Mexique">
Liste des chefs d'État du Mexique

Certains présidents n'ont assumé leurs fonctions que durant quelques jours, l'un d'entre eux, même, pendant 45 minutes seulement (Pedro Lascuráin Paredes). Avant son indépendance, le Mexique faisait partie du vice-royaume de Nouvelle-Espagne.
L'indépendance sera acquise le , lorsque le vice-roi de Nouvelle-Espagne, Juan O'Donojú contresigne l'acte d'indépendance du Mexique les conservateurs dont le général Agustín de Iturbide se rallièrent à la cause indépendantiste après la proclamation d'une Constitution libérale en Espagne.






</doc>
<doc id="4932" url="https://fr.wikipedia.org/wiki?curid=4932" title="Nara">
Nara

Nara fut pendant le la capitale du Japon, sous le nom de Heijō-kyō (ou Heizei-kyō), depuis sa fondation en 710 (lors de l'accès au trône de l'impératrice Gemmei), jusqu'en 784, c'est-à-dire durant l'époque de Nara.

Elle représenta la première véritable capitale fixe du pays. Avant 710, les capitales se déplaçaient de royaume en royaume. En effet, selon les anciennes conceptions du shintoïsme, la mort constituait l'impureté la plus grave. Lorsqu'il s'agissait de la mort du souverain, alors l'impureté frappait la capitale ; il fallait donc détruire les palais et les reconstruire ailleurs. Au début du , on comprit qu'il fallait créer un centre plus durable pour le gouvernement et l'administration de l'État.

Selon son plan originel, la ville mesurait sur . Une très large avenue, qui partait de la porte Rasomon au Sud, coupait l'agglomération en deux et menait tout droit aux palais impériaux. C'est l'avenue Suzaku, large de et bordée de saules. Les deux secteurs ainsi formés sont découpés en quartier par les rues qui se coupent à angle droit. Ce plan semble être inspiré de celui de la ville de Xi'an, la capitale chinoise de l'époque. La ville de Nara aurait aussi été conçue par des immigrants du royaume de Paekche.

À droite et à gauche, des palais devaient figurer deux temples bouddhistes de grandes dimensions : le Tōdai-ji et le Saidai-ji (le « grand temple du Levant et le grand temple du Couchant »). On ne construisit en fait que le Tôdai-ji, qui, plusieurs fois reconstruit et de dimensions plus réduites, existe encore aujourd'hui au milieu du Parc de Nara.

Après la fin de l'époque de Nara, la ville fut renommée et perdit de son importance. Elle eut à subir de nombreuses destructions pendant les diverses guerres civiles, et fut incendiée par les Taira en 1180 au terme du siège de Nara, au cours de la guerre de Gempei.

La ville de Nara, située à l'extrême nord de la préfecture de Nara, mesure du nord au sud et de l'est à l'ouest.

Le point le plus élevé de la ville est le Kaigahira-yama, une colline culminant à d'altitude.

En 2010, la ville avait une population de et une densité de population de . En 2005, il y avait d'habitations dans la ville. La plus forte concentration en habitations et en population se trouve le long de la ligne Kintetsu qui relie Nara à Osaka. Il y avait dans la ville ; les Chinois () et les Coréens () sont les deux groupes les plus représentés.

Les deux gares principales de la ville sont : la gare de Nara (desservie par JR West) et la gare de Kintetsu Nara (desservie par la société de chemins de fer Kintetsu).

De ces gares, des trains partent régulièrement vers Kyoto, Osaka et le sud du Kansai.

Les monuments historiques de l'ancienne Nara ont été inscrits au patrimoine mondial de l'Unesco en 1998.

En raison de ses nombreux temples et bâtiments, Nara est une destination touristique majeure.

Nara abrite le second hôtel de style occidental le plus vieux du Japon. Le , construit en bois et ouvert en 1909, accueille les membres de la famille impériale lorsqu'ils viennent à Nara. Le musée de Nara présente une collection permanente d'art ancien japonais et chinois, ainsi que des collections temporaires présentant des cérémonies traditionnelles japonaises.

Fumiyuki Yoshida, graveur sur ivoire Trésor national vivant du Japon est né à Nara le .


Selon une légende attachée au sanctuaire Kasuga, la divinité Takemikazuchi, monté sur un cerf Sika blanc ailé, vint prendre position sur le mont Mikasa pour assurer la protection de Heijo-kyo, la capitale impériale nouvellement construite. Depuis lors, les cerfs sont considérés comme des animaux divins, protecteurs de Nara et de tout le Japon. Anciennement, les gens devaient s'incliner sur leur passage, et, jusqu'en 1637, tuer un daim était passible de la peine de mort.

Le nombre des cerfs chute pendant la guerre du Pacifique car beaucoup sont mangés. En 1945, à la suite de la séparation de la religion et de l'État, les cerfs perdent leur statut divin et deviennent trésor naturel en 1957.

De nos jours, plus d'un millier de cerfs sika apprivoisés errent en liberté dans la ville en quémandant de la nourriture aux touristes, en particulier dans le parc de Nara, au cœur de la ville. Ce parc, qui existe depuis 1880, s'étend sur .



</doc>
<doc id="4933" url="https://fr.wikipedia.org/wiki?curid=4933" title="Liste des présidents de la République française">
Liste des présidents de la République française

Cet article dresse la liste des présidents de la République française de 1848 à 1852, de 1871 à 1940 et depuis 1947. Elle nomme également les personnes qui ont exercé la fonction de chef d'État de la France républicaine de 1792 à 1804 et de 1940 à 1947.

Le , en pleine Révolution française, les députés de la Convention, réunis pour la première fois, décident à l'unanimité de l'abolition de la royauté. Le , la décision est prise de dater les actes de l'an I de la République et le , la République est déclarée « une et indivisible ». 

La Première République passe par trois formes de gouvernement mais aucune ne connait, à proprement parler, de :

La Première République prend fin lors de la proclamation de Napoléon Bonaparte comme le . Le mot subsiste sur les pièces jusqu'en 1806 avant d'être remplacé par « Empire français ».

Les cinq directeurs sont les détenteurs du pouvoir exécutif sous le Directoire, régime qui gouverne la France entre 1795 et 1799. Ils sont désignés, selon la Constitution de l'an III, par le Conseil des Anciens dans une liste produite par le Conseil des Cinq-Cents, et restent cinq ans en fonction.

Les cinq premiers Directeurs sont Barras (seul à rester en fonction jusqu'à la fin du régime), La Révellière-Lépaux, Reubell, Carnot et Le Tourneur, les trois premiers formant rapidement un , leurs intérêts convergeant. Plusieurs autres Directeurs connaissent des carrières éphémères au gré des coups d’État suivant les différentes élections, certains comme Carnot et Barthélémy allant jusqu'à être proscrits. D'autres arrivés en cours de route connaissent une plus longue carrière au sein de l'exécutif, comme Merlin de Douai et Sieyès, ce dernier étant l'un des artisans du coup d’État mettant fin au régime. Il y a eu, au total, treize directeurs.

Le Consulat est un régime politique français issu du coup d'État du 18 Brumaire an VIII (), qui renverse le régime du Directoire (1795-1799). La constitution de l'an VIII établit alors un régime politique autoritaire dirigé par trois consuls et en réalité par le seul Premier consul Napoléon Bonaparte, qui deviendra consul à vie en 1802. Le Consulat a duré jusqu'au (28 floréal an XII), date de la fin de la Première République française et de la proclamation du Premier Empire.



Vingt-cinq personnes ont exercé la fonction de président de la République française depuis 1848.
Dans les premiers mois de la République, de février à décembre 1848, soit jusqu'à l'élection présidentielle, les fonctions de chef de l'État sont exercées dans les faits, successivement, par :

L'élection présidentielle de 1848, organisée pour désigner le président de la Deuxième République française, s'est tenue les 10 et 11 décembre 1848.

Le 4 septembre 1870, en pleine guerre franco-allemande, la République est proclamée, avec à sa tête : 

Après l'invasion du pays par l'armée allemande, le président Albert Lebrun nomme le maréchal Philippe Pétain, alors âgé de 83 ans, président du Conseil, avant de se retirer chez son gendre à Vizille (Isère). Dès juillet 1940, bien que n'ayant pas démissionné de son mandat, Albert Lebrun n'exerce plus aucun pouvoir.

Le gouvernement Pétain signe le traité d'armistice du 22 juin 1940. L'Assemblée nationale se réunit à Vichy et vote le 10 juillet 1940 les pleins pouvoirs constituants au maréchal Pétain. Le lendemain, par deux actes constitutionnels, Philippe Pétain abroge l'article 2 de la loi constitutionnelle du 25 février 1875 concernant l'élection du président de la République : il prend le le titre « chef de l'État français », qu'il conserve jusqu'au .

Le projet constitutionnel de Vichy du , qui, entre autres, prévoyait de conserver le titre de « président de la République » pour le chef de l'État, ne sera jamais promulgué. 
Le , le maréchal Pétain est emmené contre son gré par l'armée allemande à Belfort puis, le 8 septembre, à Sigmaringen en Allemagne où il refuse d’exercer toute fonction.

Parallèlement, à partir du 18 Juin 1940, la France libre en exil, dirigée par le général Charles de Gaulle depuis Londres, conteste la légitimité du régime de Vichy. Le Comité français de Libération nationale lui succède depuis Alger en 1943 puis le Gouvernement provisoire de la République française (GPRF) à partir du .

La fonction présidentielle est officiellement considérée comme ayant été vacante de 1940 à 1947.

Après la libération de Paris en , le général de Gaulle, chef de la France libre en exil depuis l'armistice de 1940, exerce à son tour les fonctions de chef de l'État, cumulées avec celles de chef du gouvernement, en tant que président du Gouvernement provisoire de la République française (GPRF). En désaccord avec le projet de Constitution que proposent les ministres communistes, il démissionne en 1946.

Lui succèdent comme présidents du GPRF :

Après la promulgation de la Constitution de la Quatrième République le et jusqu'à ce que l'ensemble des organes de celle-ci soient mis en place, c'est-à-dire jusqu'à l'élection de Vincent Auriol à la présidence de la République le , on entre dans une période de transition généralement rattachée à la Quatrième République mais où subsistent certaines des institutions provisoires.

Le GPRF disparaît peu de temps après l'adoption de la Constitution de la Quatrième République.



République :

République :

République :

République :
















</doc>
<doc id="4934" url="https://fr.wikipedia.org/wiki?curid=4934" title="Ernesto Zedillo">
Ernesto Zedillo

Ernesto Zedillo Ponce de León (né le à Mexico) est un homme politique mexicain. Membre du PRI (Partido Revolucionario Institucional), il fut président de la République entre 1994 et 2000.

Son père était électricien et Ernesto était le deuxième enfant d'une famille de six enfants. Il était encore jeune lorsque les Zedillo partirent s'installer à Mexicali, ville frontalière avec les États-Unis. Ernesto y réalisa le début de sa scolarité. Il retourna plus tard à Mexico pour poursuivre ses études et obtenir une licence en économie à l'Institut polytechnique national.

Avant son élection, il occupa le poste de ministre de la Planification et du Budget, puis de ministre de l'Enseignement public dans le gouvernement de Carlos Salinas de Gortari. On le désigna comme chef de campagne de Luis Donaldo Colosio, le candidat officiel du PRI. Après l'assassinat de celui-ci, Zedillo se retrouva comme l'un des rares hommes politiques qui puisse légalement se présenter comme candidat. En effet, la loi mexicaine exige que les candidats à la présidence n'occupent pas un poste important dans les derniers mois précédant l'élection.
Sa devise «" Bienestar para tu familia "» (« Bien-être pour ta famille ») a suscité certaines moqueries de la part de ses opposants en raison de la dévaluation du peso mexicain mise en place par Zedillo, et qualifiée d'« "erreur de décembre" ». La dévaluation, d'environ 200 % (le peso perdant les deux-tiers de sa valeur par rapport au dollar américain) plongea le Mexique dans une crise économique profonde, même si la responsabilité de la crise est rejetée sur la gestion économique du gouvernement Salinas comme sur celle de Zedillo. Sur le plan politique, la démocratie-chrétienne (PAN) réussit pour la première fois à faire élire des gouverneurs dans les élections régionales. En 2000, Zedillo devint le premier dirigeant du PRI à avoir connu la défaite de son parti depuis 1929.

Après s'être retiré de la vie politique, il prit part au conseil d'administration de quelques entreprises américaines comme Procter & Gamble, Alcoa ou encore Union Pacific. Par ailleurs, il devint directeur pour le Centre d'étude sur la mondialisation de l'université Yale. Zedillo participa aussi à quelques programmes liés au financement des pays en voie de développement pour l'Organisation des Nations unies.



</doc>
<doc id="4937" url="https://fr.wikipedia.org/wiki?curid=4937" title="Valéry Giscard d'Estaing">
Valéry Giscard d'Estaing

Valéry Giscard d’Estaing ( ; ), surnommé VGE, né le à Coblence (Allemagne), est un homme d'État français, président de la République du au .

Inspecteur des finances de profession, il s'engage en politique dans le Puy-de-Dôme et devient député en 1956. Après avoir été secrétaire d'État aux Finances de 1959 à 1962 et ministre des Finances et des Affaires économiques de 1962 à 1966, il exprime ses réserves envers le pouvoir gaulliste. De 1969 à 1974, lors de la présidence de Georges Pompidou, il exerce à nouveau les fonctions de ministre de l'Économie et des Finances. Il est alors à la tête des Républicains indépendants, qui constituent la deuxième composante de la majorité de droite.

Candidat à l'élection présidentielle de 1974, il défait le gaulliste Jacques Chaban-Delmas et l'emporte au second tour face à François Mitterrand, candidat de l'Union de la gauche, devenant le plus jeune président de la République depuis 1848. Prônant une , il fait notamment voter l'abaissement de la majorité à 18 ans, la dépénalisation de l'avortement, l'élargissement du droit de saisine du Conseil constitutionnel et la fin de la tutelle de la télévision publique. Sa politique internationale est marquée par le renforcement de la construction européenne et par l'implication militaire de la France dans la bataille de Kolwezi (Zaïre) et l'opération Caban (Centrafrique).

Bien qu'il développe le projet de TGV et relance l'industrie nucléaire, il est confronté à des difficultés économiques, les Trente Glorieuses touchant à leur fin. À la tête du gouvernement, il procède en 1976 au remplacement de Jacques Chirac par Raymond Barre, qui mène une politique de rigueur jusqu'à la fin du mandat présidentiel. En matière de sécurité et d'immigration, il se montre conservateur, ce qui contraste avec son image de libéral dans d'autres domaines. Longtemps donné réélu, il est battu par François Mitterrand à l'élection présidentielle de 1981, lors de laquelle le RPR de Jacques Chirac, premier parti de la majorité, exprime ses réticences à le soutenir.

Par la suite, il est notamment député et président du conseil régional d'Auvergne. Élu à la présidence de l'UDF, parti de centre droit qu'il a fondé alors qu'il était à l'Élysée, il est un des principaux dirigeants de l'opposition au président François Mitterrand. Il s'engage au niveau européen en étant député européen et président de la Convention sur l'avenir de l'Europe. En 2004, il se retire de la vie politique pour siéger au Conseil constitutionnel, dont il est membre de droit en tant qu'ancien président de la République.

Auteur de plusieurs essais et romans, Valéry Giscard d'Estaing est membre de l'Académie française depuis 2003.

Valéry René Marie Georges Giscard d’Estaing naît le à Coblence, où son père est directeur des finances du Haut-Commissariat français en Rhénanie, région alors occupée par les forces françaises.

Il est le fils d'Edmond Giscard (1894-1982), devenu Giscard d'Estaing en juin 1922, inspecteur des Finances et économiste, membre de l'Institut de France, grand officier de la Légion d'honneur, et de May Bardoux (1901-2003), fille de l'homme politique Jacques Bardoux et petite-fille d'Agénor Bardoux, lequel a notamment été ministre de l'Instruction publique au début de la . Du même côté, Valéry Giscard d'Estaing descend des comtes Jean-Pierre de Montalivet et Camille de Montalivet, par leur petite-fille et fille Marthe, épouse de Georges Picot ; il est également un descendant d'Adélaïde de Saint-Germain, qui passe pour une fille bâtarde du roi Louis XV et de Catherine Éléonore Bénard, maîtresse royale.

Il a trois sœurs (devenues toutes trois comtesses par la vertu de mariages endogamiques) et un frère : Sylvie (1924-2008), Olivier (né en 1927), Isabelle (née en 1935) et Marie-Laure (née en 1939).

Le , il épouse Anne-Aymone Sauvage de Brantes à la mairie du arrondissement de Paris, avec pour témoin la maréchale de Lattre de Tassigny. Ils se marient religieusement le 23 décembre, dans la chapelle du château d'Authon. Ils ont quatre enfants : Valérie-Anne (née en 1953), éditrice, Henri (né en 1956), homme d'affaires, Louis (né en 1958), homme politique, et Jacinte (1960-2018), vétérinaire.

Quelques mois après sa naissance, en , son père est rappelé à Paris par l'Inspection des finances. La famille s'installe alors 71, rue du Faubourg-Saint-Honoré, dans le de Paris.

Valéry Giscard d'Estaing fait ses études à l'école Gerson, au lycée Blaise-Pascal à Clermont-Ferrand, puis au lycée Janson-de-Sailly et au lycée Louis-le-Grand à Paris pendant l'Occupation. Il obtient son double baccalauréat en philosophie et mathématiques élémentaires en 1942, à 16 ans.

Après une classe préparatoire au lycée Louis-le-Grand, il participe à la libération de Paris à l'âge de 18 ans, en août 1944, faisant partie du service d'ordre chargé de la protection du représentant civil du général de Gaulle en zone encore occupée, Alexandre Parodi. Il refuse de retourner au lycée Louis-le-Grand pour y préparer l'École polytechnique et s'engage dans la française, sous les ordres du général de Lattre de Tassigny. Le , il est élevé au grade de brigadier et obtient cette citation à l'ordre de l'Armée quelques jours plus tard :
Il est par ailleurs dans le premier char qui entre dans Constance, le . Il se rappellera avoir été pris par surprise par la capitulation allemande, alors qu'il était sur un char en Autriche.

Après huit mois de campagne et 28 jours au combat, il est décoré de la croix de guerre 1939-1945, et défile devant le général de Gaulle le 14 juillet 1945.

À la rentrée 1945, il réintègre le lycée Louis-le-Grand et est reçu sixième au concours de l'École polytechnique en . Il en sort en et choisit d'intégrer la toute nouvelle École nationale d'administration (ENA) ; son entrée est facilitée par le décret du , qui permet à un polytechnicien par promotion d'y entrer sans passer de concours. Avant la rentrée, il voyage aux États-Unis et au Canada. Il trouve à Montréal un emploi temporaire de professeur au collège Stanislas. Il intègre l’ENA le 3 janvier 1949. Il effectue notamment un stage de huit mois en Sarre, rédige un mémoire de stage intitulé "Le Rattachement économique de la Sarre à la France" et obtient la note de 19 sur 20, alors que les meilleures notes habituelles tournent autour de 16.

Sorti sixième de l'ENA (promotion Europe), il peut ainsi entrer à l'Inspection générale des finances en 1952.

De juin à décembre 1955, Valéry Giscard d'Estaing, alors âgé de 29 ans, est directeur adjoint au cabinet du président du Conseil, Edgar Faure. En 1956, il se met en disponibilité de son administration après son accès à la députation du Puy-de-Dôme, département dont son arrière-grand-père, Agénor Bardoux, au et son grand-père, Jacques Bardoux, au , furent longtemps les députés. Il est nommé, la même année, membre de la délégation française à la session de l'Assemblée générale des Nations unies. Il accède en 1958 au mandat de conseiller général du Puy-de-Dôme, élu dans le canton de Rochefort-Montagne, mandat qu'il exerce jusqu'en 1974.

Le , six mois après son vote en faveur de l'investiture au gouvernement de Gaulle et quelques jours après l'élection à la présidence de la République de celui-ci, Valéry Giscard d'Estaing, âgé de 32 ans, est nommé secrétaire d'État aux Finances aux côtés du ministre des Finances et des Affaires économiques, Antoine Pinay. Il est principalement chargé d'assister et de représenter le ministre des Finances et, pendant trois ans, travaille en étroite collaboration avec le Premier ministre, Michel Debré, avec qui il est lié depuis plusieurs années et qui est comme lui partisan de l'Algérie française. Contrairement aux autres secrétaires d'État, il intervient régulièrement en Conseil des ministres, ses fonctions gouvernementales touchant de nombreux domaines. Le président de Gaulle confie à son fils, Philippe, qu'« en réalité ce n'était pas le bon M. Pinay qui travaillait le plus, mais son secrétaire d'État, Valéry Giscard d'Estaing », qu'il estime avoir été son « meilleur argentier ».

Michel Debré propose son nom pour remplacer Wilfrid Baumgartner, qui a succédé à Antoine Pinay en 1960. Le , Valéry Giscard d'Estaing est ainsi nommé ministre des Finances et des Affaires économiques dans le gouvernement Debré. Il conserve ses fonctions sous le gouvernement Pompidou, formé trois mois plus tard.

Peu après sa nomination, il refuse de faire partie de la délégation pour les affaires économiques dans le cadre des négociations avec le Front de libération nationale (FLN) en vue de l'indépendance de l'Algérie. Il est en effet longtemps partisan de l'Algérie française et des possibles liens avec l'Organisation armée secrète (OAS) seront par la suite évoqués. Ses diverses correspondances de l'époque montrent qu'il est troublé par l'idée de l'indépendance, à laquelle le général de Gaulle ne semble plus s'opposer, ainsi que par le refus de celui-ci d'aborder la question algérienne en Conseil des ministres. Mais il n'émet pas de critique publique à l'égard de la politique du gouvernement et se montre favorable à « un cadre fédéral assez souple » ou à une partition de l'Algérie. Il dira ensuite avoir été convaincu par la position du Général, notamment après un entretien avec lui. Après l'indépendance de l'Algérie, il est chargé de concevoir les procédures financières et d'instaurer des structures administratives permettant d'indemniser les rapatriés.

Pour restaurer l'équilibre du budget de l'État français et freiner la croissance de la consommation au profit de l'investissement, Valéry Giscard d'Estaing poursuit la politique définie par le plan « Pinay-Rueff », qui a notamment conduit à une dévaluation massive, avant la mise en circulation d'un nouveau franc en 1960, au retour à la convertibilité extérieure du franc, à la libération des échanges. Lancé au début de la présidence du général de Gaulle, ce plan résorbe les principaux déséquilibres macroéconomiques dans une période de forte croissance. Valéry Giscard d'Estaing s'attache à continuer l'œuvre de réorganisation administrative entamée par ses prédécesseurs : dès 1962, il fait intégrer administrativement au sein du ministère les services des affaires économiques, puis, en 1965, la direction du Trésor absorbe celle des finances extérieures et reconstitue la direction du mouvement général des Fonds d'avant guerre, tandis que le Service des études économiques et financières devient la Direction de la prévision.
En 1965, pour la première fois depuis la Seconde Guerre mondiale, le budget de l'État présente un excédent, de 120 millions de francs, en partie néanmoins grâce au transfert de dépenses de l'État vers la Caisse des dépôts et consignations. Valéry Giscard d'Estaing souhaite alors l'élaboration d'une loi organique instaurant l'obligation d'équilibre budgétaire. Cette proposition suscite l'opposition du Premier ministre, Georges Pompidou, qui n'est pas sur la même ligne que le général de Gaulle et Valéry Giscard d'Estaing sur le principe d'équilibre budgétaire. Cette politique budgétaire permet à la France de rembourser aux États-Unis le reliquat de la dette contractée pendant la Seconde Guerre mondiale, à savoir 293 millions de dollars ; à cet effet, une rencontre dans le Bureau ovale de la Maison-Blanche est organisée entre Valéry Giscard d'Estaing et le président américain, John Fitzgerald Kennedy, le 24 juillet 1962.

Sur le plan fiscal, la politique économique de Valéry Giscard d'Estaing se traduit par un alourdissement des taxes et impôts. Il parvient ainsi à contourner les réticences du général de Gaulle, qui ne souhaitait pas que les prélèvements obligatoires dépassent 35 % du PIB. Il étend la taxe sur la valeur ajoutée (TVA), instaurée en 1954 et qui concernait jusqu'alors uniquement les grandes entreprises, au commerce de détail ; cette mesure sera reprise par une directive communautaire en 1967. En 1964, il institue également les sociétés d'investissement à capital variable (SICAV) afin de faciliter l'accès des particuliers aux marchés boursiers, et organise l'épargne-logement par la loi du .

Lors de son premier passage au ministère des Finances, il a de nombreux entretiens privés avec le général de Gaulle, dont il approuve le référendum sur l'élection au suffrage universel du président de la République, contrairement à beaucoup d'indépendants. Concernant ses relations avec le chef de l'État, son collaborateur Jacques Calvet raconte : « Pour plaire à de Gaulle, pour l'intéresser à l'économie, il fallait lui expliquer pourquoi les finances étaient aussi importantes pour la défense nationale que les armées. Giscard savait le faire ». Il parvient à dissuader le président de rapatrier le stock d'or appartenant à la France des États-Unis par le navire de guerre le "Colbert" et cherche à le faire renoncer à l'étalon-or. Il souhaite en effet que le Système monétaire international repose sur un ensemble de monnaies et non plus uniquement sur le dollar, et qu'une monnaie internationale soit émise sur la base des avoirs en or.

Les relations entre Valéry Giscard d'Estaing et le syndicat du patronat, le CNPF, sont assez tendues : alors que ce dernier lui reproche un style autoritaire, le ministre des Finances s'oppose à ses positions protectionnistes et à ses demandes d'aides massives de l'État. Pour lutter contre l'inflation menaçante, il lance, en , un « plan de stabilisation », comprenant, outre des dispositions budgétaires, des mesures d'encadrement des prix. Ce plan amoindrit sa popularité, notamment auprès des commerçants et industriels, affectés par le blocage des prix. Sous le feu des critiques pour avoir maintenu sa politique de rigueur, qui commence néanmoins à produire ses effets, il est remplacé, le , peu après la réélection du général de Gaulle, par Michel Debré.

Valéry Giscard d'Estaing exerce de nouveau à l'Inspection générale des Finances de 1966 à 1967.

Il ne quitte pas pour autant la vie politique. Le juin 1966, il fonde la Fédération nationale des républicains indépendants (FNRI), qui succède aux RI, en proclamant : . Il prône ainsi la création d'un « Sénat européen », élu au suffrage universel et se prononçant sur les textes législatifs ayant une portée communautaire, et d'une « Banque d'Europe », recevant des dépôts des banques centrales nationales et préparant la mise en place d'une monnaie commune. Sont également créés à cette époque les Jeunes républicains indépendants (JRI), futurs jeunes giscardiens. Valéry Giscard d'Estaing cherche alors à acquérir une image de présidentiable et affirme sa ligne politique au sein de la majorité. Il déclare ainsi : .

Le , il lance la campagne de la FNRI en vue des élections législatives de mars. À cette occasion, il émet des réserves quant à la politique économique, sociale et européenne menée par le pouvoir gaulliste, et expose une vision plus libérale des institutions. Cette prise de distance est symbolisée par l'emploi de la formule du « oui, mais ». Cette position irrite le président de Gaulle, qui déclare en conseil des ministres qu'« on ne gouverne pas avec des « mais » », ce à quoi Valéry Giscard d'Estaing réplique que « s'il est exact qu'on ne peut gouverner avec des « mais », il est également vrai que l'on ne peut ni dialoguer ni contrôler avec des « oui » ». Pour les élections législatives, que l'opposition aborde en situation de force après la mise en ballottage du général de Gaulle à la présidentielle de 1965, la majorité présente des candidatures uniques dans chaque circonscription, sous le label « République ». Valéry Giscard d'Estaing parvient à obtenir l'investiture de candidats FNRI dans 83 circonscriptions. À l'issue du second tour, le , le nombre de députés FNRI passent de 35 dans l'Assemblée nationale sortante à 42, tandis que le parti gaulliste perd plusieurs sièges, si bien que la majorité ne tient plus qu'à un siège, le soutien des députés proches de Giscard, qui forment un groupe autonome, devenant indispensable.

Après les élections législatives, le , Valéry Giscard d'Estaing devient président de la commission des finances, de l'économie générale et du plan de l'Assemblée nationale. Dès le mois suivant, il manifeste sa réprobation à l'égard de la procédure des ordonnances adoptée par le gouvernement, mais refuse de voter les motions de censure. Il demeure néanmoins critique envers la politique économique menée (il s'abstient lors du vote du collectif budgétaire, en novembre) et en particulier envers son successeur au ministère des Finances, Michel Debré. Le , il publie un communiqué dans lequel il critique le recours aux ordonnances, l'attitude de la France à l'égard d'Israël dans la guerre des Six Jours et les propos tenus par le général de Gaulle à Montréal le mois précédent. Il se demande si « la manière dont sont élaborées et prises actuellement les décisions essentielles de notre vie publique prépare dans les meilleures conditions l'avenir politique de la France » et fait part de son « angoisse » quant à l'« exercice solitaire du pouvoir », formule perçue comme visant directement le général de Gaulle.

Sur le plan local, Valéry Giscard d'Estaing, déjà conseiller général du Puy-de-Dôme et conseiller municipal de Chamalières, confirme son ancrage en devenant maire de cette ville le , après la démission de Pierre Chatrousse.

Lors des événements de mai 68, il reste relativement silencieux. Le 30 mai, après avoir refusé de voter la motion de censure présentée par l'opposition, il affirme son soutien au général de Gaulle, parti la veille pour Baden-Baden, tout en appelant à la tenue d'élections législatives anticipées et à la formation d'un gouvernement plus représentatif. Contrairement à quelques élus de la majorité, il ne considère pas Georges Pompidou ou Pierre Mendès France comme des recours crédibles. De Gaulle dissout l'Assemblée nationale et les élections législatives des 23 et 30 juin voient l'élection de 64 députés « Républicains indépendants- République », parmi lesquels Michel Poniatowski, contre lequel se présentait un candidat gaulliste, et Olivier Giscard d'Estaing, frère de Valéry.
L'UDR détenant la majorité absolue à elle seule, l'appui des Républicains indépendants n'est plus nécessaire et Valéry Giscard d'Estaing ne parvient pas à retrouver la présidence de la commission des Finances, face à Jean Taittinger. L'écrivain François Mauriac écrit alors à son propos : « Nous le voyons avec plaisir sous notre nez, retoucher, jour après jour, le personnage du plus jeune ministre des Finances qu'il a été et du plus jeune président de la République qu'il sera, s'il plaît à Dieu et s'il n'y a pas d'accident de parcours. Ce n'est qu'un barreau de son échelle que l'UDR vient de scier sous son pied ». Valéry Giscard d'Estaing se fait moins présent dans le débat économique, même s'il dénonce « l'abandon de l'équilibre du budget et de la balance des comptes qui avaient été rétablis et organisés de 1963 à 1965 sous l'autorité du chef de l'État ».

Pour retrouver sa légitimité, Charles de Gaulle annonce la tenue d'un référendum sur la réforme du Sénat et la régionalisation. Alors que la FNRI se prononce pour la liberté de vote, Valéry Giscard d'Estaing annonce, le , qu'il « n'approuvera pas » le référendum. Partisan du bicamérisme, il est en particulier opposé à la perte du pouvoir législatif du Sénat. Cette position lui attire des critiques au sein même de son parti politique. Même s'il ne s'est pas prononcé pour le « non » et bien qu'il affirmera plus tard avoir voté blanc, sa déclaration contribue à l'échec du référendum, le « non » obtenant 52,41 % des suffrages exprimés le , ce qui entraîne, comme annoncé, la démission du général de Gaulle de la présidence de la République. Lors de l'élection présidentielle qui suit, il semble d'abord pencher pour la candidature du modéré Antoine Pinay. Alors que sa candidature est ensuite évoquée et souhaitée par des centristes comme le président par intérim, Alain Poher, Valéry Giscard d'Estaing apporte son soutien à Georges Pompidou le : « J'ai décidé de soutenir la candidature de M. Pompidou […] Dans des circonstances normales, c'est-à-dire en 1972, j'aurais posé ma candidature à la présidence de la République. Dans la situation présente, nous avons cherché la personne qui pouvait le mieux assurer la continuité de la nation. »

Georges Pompidou, élu au second tour face à Alain Poher, prend ses fonctions de chef de l'État le et nomme Jacques Chaban-Delmas au poste de Premier ministre. Deux jours plus tard, Valéry Giscard d'Estaing se voit confier le portefeuille de ministre de l'Économie et des Finances.

L'un de ses premiers actes est, dans le plus grand secret, de contribuer au succès de la première dévaluation du franc depuis 1958, nécessaire au rétablissement de la compétitivité des produits français, notamment après les accords de Grenelle. Le , le franc est dévalué de 11,1 %. Valéry Giscard d'Estaing parle alors de « franc amaigri mais guéri ». Cette dévaluation est la dernière se mesurant à partir du poids en or de l'unité monétaire, puisqu'en , le président américain, Richard Nixon, met fin à la convertibilité du dollar en or, ce qui ouvre la voie à la mise en place d'un système de changes flottants, auquel sont opposés le président Pompidou et le ministre Giscard d'Estaing, qui milite pour un système fondé sur des « parités stables mais ajustables ». En , un accord est conclu avec Nixon sur de nouvelles parités entre les principales monnaies. Giscard reconnaîtra par la suite que « le flottement des monnaies a constitué le moindre mal » pour traverser « la crise de 1974-1981 ».
Au niveau économique, la priorité est donnée à la production industrielle, qui augmente de près de 40 % pendant le mandat de Georges Pompidou, tandis que la croissance dépasse les 5,5 %. Concernant les finances publiques, Valéry Giscard d'Estaing cherche à renouer avec l'équilibre budgétaire, ce qu'il fait avec le budget de 1969, celui de 1970 étant même excédentaire. En revanche, il ne parvient pas à lutter efficacement contre la hausse des prix, renforcée par le premier choc pétrolier. Les plans anti-inflation successifs, qui comprennent essentiellement des mesures de contrôle des prix, se révèlent inefficaces. Les effets de la dévaluation de 1969, renforcés par la réévaluation du mark allemand quelques mois plus tard, conduisent à la perte de la valeur du franc face au mark. Le , Valéry Giscard d'Estaing annonce la sortie de la France du Serpent monétaire européen, laissant ainsi le franc fluctuer en fonction de l'offre et de la demande, ce qui va renforcer l'inflation. Pour financer les quelques mesures de relance, parmi lesquelles la baisse de la TVA, il lance un emprunt, qui sera particulièrement coûteux car indexé indirectement sur l'or.

Louant le « réformisme » du ministre d'État suédois social-démocrate Olof Palme, Valéry Giscard d'Estaing peut, dans un premier temps, paraître proche de la ligne politique défendue par le Premier ministre, Jacques Chaban-Delmas, promoteur de la « Nouvelle société ». Mais ses relations avec le chef du gouvernement vont se dégrader au fil du temps. Le ministre de l'Économie et des Finances s'inquiète notamment d'une certaine forme de dirigisme dans la politique économique, du risque d'accroissement de l'inflation que font courir les mesures préconisées par Jacques Delors ou encore de la conception du dialogue social du Premier ministre. Des tensions apparaissent entre des proches de Valéry Giscard d'Estaing et des partisans de Jacques Chaban-Delmas, perçu comme un successeur possible de Georges Pompidou, ceux-ci accusant le ministre de l'Économie et des Finances d'être à l'origine de révélations embarrassantes pour les gaullistes, en particulier sur la situation fiscale du Premier ministre.

En froid avec l'Élysée, Jacques Chaban-Delmas démissionne le , et Pierre Messmer lui succède. Valéry Giscard d'Estaing conserve ses fonctions de ministre de l'Économie et des Finances dans les trois , gagnant même le titre de ministre d'État le . Pendant cette période, il fait voter la loi du 3 janvier 1973 sur la Banque de France qui limite le financement de l'État par la banque centrale ; cette loi sera critiquée par la suite. Il s'oppose au projet de train à grande vitesse (TGV) de la SNCF, lui préférant le projet d'aérotrain ; en , lors d'un conseil interministériel restreint sur les économies d'énergie, après trois heures de discussions, le président Pompidou coupe court au débat en avalisant le projet. Est également décidé le lancement d'un important parc de centrales nucléaires, ce qui conduit le ministre Giscard d'Estaing à lancer un emprunt international de 1,5 milliard de dollars.

Les élections législatives de 1973 sont marquées par une progression de la gauche et permettent aux Républicains indépendants, qui comptent 54 députés, de retrouver, comme en 1967-1968, un rôle d'arbitre au sein de la majorité, l'UDR disposant de 184 sièges, soit moins que la majorité absolue. Peu avant la mort du président Pompidou, Valéry Giscard d'Estaing est pressenti pour devenir Premier ministre.

Georges Pompidou, atteint de la maladie de Waldenström, meurt le , deux ans avant la fin de son mandat. Le président défunt n'avait pas exprimé de préférence sur un éventuel successeur. Il n'avait en tout cas pas dissuadé les ambitions de Valéry Giscard d'Estaing, qui n'était pas issu du même parti politique que lui, mais à qui il voyait une « vocation nationale ».

Quatre candidats issus de la majorité sont alors pressentis pour concourir à l'élection présidentielle anticipée : le Premier ministre, Pierre Messmer (UDR), le maire de Bordeaux, Jacques Chaban-Delmas (UDR), le président de l'Assemblée nationale, Edgar Faure (UDR), et le ministre des Finances, Valéry Giscard d'Estaing (FNRI). Jacques Chaban-Delmas, qui craint que le Premier ministre Messmer ne rassemble l'ensemble de la droite sur son nom, est le premier à se déclarer, le 4 avril, alors que l'hommage de l'Assemblée nationale au président défunt n'est pas terminé, ce qui lui vaut des critiques.

Valéry Giscard d'Estaing attend le 8 avril pour annoncer sa candidature : depuis la mairie de Chamalières, il explique vouloir « regarder la France au fond des yeux ». Il promet aux gaullistes de ne pas se présenter si Pierre Messmer se porte lui aussi candidat. Peu après, le Premier ministre s'inquiète de la division de la majorité et manifeste sa volonté de se présenter. Mais il renonce finalement face au refus de Jacques Chaban-Delmas de se désister en sa faveur, ce qui ouvre la voie à un affrontement inévitable entre ce dernier et Valéry Giscard d'Estaing. À la suite de cet épisode, l'image du maire de Bordeaux, vu comme un diviseur, continue de se dégrader, tandis que le ministre des Finances peut pleinement compter, avec le retrait d'Edgar Faure, sur le soutien des centristes, ainsi que de plusieurs personnalités de l'UDR, puisque le 13 avril, 39 parlementaires et quatre ministres (Jacques Chirac, Jean-Philippe Lecat, Olivier Stirn et Jean Taittinger) font publier un texte, dit « appel des 43 », qui est considéré comme un soutien implicite à sa candidature.

Au départ, Valéry Giscard d'Estaing, qui n'est pas soutenu par un grand courant de la vie politique française, semble avoir peu de chances d'être élu à la présidence de la République. Sa campagne est beaucoup moins organisée que celle de Jacques Chaban-Delmas, qui se prépare depuis des années et bénéficie de l'appui du puissant parti gaulliste. Il manque également de moyens financiers et de militants, ce qui contraint son équipe à recruter des proches de l'extrême droite pour former son service d'ordre. Son directeur de campagne est le préfet Lucien Lanier.

D'abord donné en troisième position dans les sondages, Valéry Giscard d'Estaing parvient à prendre l'ascendant sur son rival gaulliste, qui multiplie les maladresses et passe de 29 à 18 % d'intentions de vote entre le 9 et le 25 avril, d'après les études de l'Ifop. Le candidat des Républicains indépendants excelle en matière de communication audiovisuelle. Pendant ses fonctions ministérielles, il avait gagné en popularité et innové en matière de communication en présentant l'image d'un homme politique jeune et dynamique, jouant au football ou pratiquant le ski. Fait unique, il pose également aux côtés de sa fille Jacinte, âgée de 13 ans, sur une affiche électorale, qui sera considérée comme une réussite. De leur côté, ses partisans, qui comptent des personnalités du monde artistique comme Brigitte Bardot, Charles Aznavour, Alain Delon, Johnny Hallyday, Sylvie Vartan, Sheila, Mireille Mathieu et Danièle Gilbert, arborent des t-shirts, affiches et autocollants reprenant son slogan de campagne, « Giscard à la barre ». Cette campagne d'un type nouveau, proche de celles qui ont lieu aux États-Unis, lui permet de dégager une image de renouveau et de se démarquer de ses adversaires.

Le , après moins d'un mois de campagne, il arrive en deuxième position avec 32,60 % des voix, derrière François Mitterrand (43,25 %), candidat de l'Union de la gauche, mais largement devant Jacques Chaban-Delmas (15,11 %). Entre les deux tours, le débat qui l'oppose au candidat de la gauche lui donne un avantage décisif, la phrase « Vous n'avez pas le monopole du cœur » ayant marqué les esprits. Il reçoit également l'appui, décisif pour certains, du directeur de "L'Express" et président du Parti radical, Jean-Jacques Servan-Schreiber. Le , Valéry Giscard d'Estaing emporte le second tour avec 50,81 % des suffrages et voix d'avance sur son adversaire. Il recueille 50,67 % en France métropolitaine et 57,31 % en France d'outre-mer. Cette élection reste à ce jour la plus serrée de l'histoire de la et celle pour laquelle la participation a été la plus élevée (87,33 % des inscrits).

Le , Valéry Giscard d'Estaing devient le troisième président de la ; il est alors, à l'âge de 48 ans, le plus jeune président élu. Dans son discours d'investiture, lors duquel il ne porte pas le costume traditionnel en queue-de-pie, il déclare : « De ce jour, date une ère nouvelle de la politique française. […] Ainsi, c'est moi qui conduirai le changement, mais je ne le conduirai pas seul. […] J'entends encore l'immense rumeur du peuple français qui nous a demandé le changement. Nous ferons ce changement avec lui, pour lui, tel qu'il est dans son nombre et dans sa diversité, et nous le conduirons en particulier avec sa jeunesse ». Fait inédit, c'est à pied qu'il remonte les Champs-Élysées, afin d'aller fleurir la tombe du Soldat inconnu et de ranimer la flamme sous l'Arc de triomphe.
Valéry Giscard d'Estaing refuse de dissoudre l'Assemblée nationale, à majorité gaulliste, et nomme au poste de Premier ministre Jacques Chirac, qui forme un gouvernement composé de 15 ministres, dont les titres sont simplifiés, à l'instar de ceux de leurs grands homologues internationaux, et dont la moyenne d'âge est de 52 ans seulement. La plupart sont giscardiens, avec Michel Poniatowski à l'Intérieur, Jean Lecanuet à la Justice, ou Michel d'Ornano à l'Industrie, le reste des ministres étant des gaullistes, comme Robert Galley à l'Équipement, et des personnes issues de la société civile, comme Simone Veil à la Santé ou René Haby à l'Éducation nationale. Le nouveau président s'entoure d'un cabinet plus resserré que ceux de ses prédécesseurs, avec Claude Pierre-Brossolette comme secrétaire général de l'Élysée, fonction qui sera ensuite assurée par Jean François-Poncet, de 1976 à 1978, et par Jacques Wahl, de 1978 à 1981. Le gouvernement Chirac est remanié dès le 9 juin, avec la nomination des secrétaires d'État et la démission du ministre des Réformes, Jean-Jacques Servan-Schreiber, qui protestait contre le début de la huitième campagne française d'essais nucléaires. Le 16 juillet, un secrétaire d'État à la Condition féminine est créé et confié à la journaliste Françoise Giroud.

Le début de sa présidence est marqué par une grande volonté de jeunesse et de modernisation. Il innove avec un portrait officiel pour la première fois réalisé en extérieur, tiré en largeur et non plus en hauteur et sur lequel il ne pose pas avec le collier de grand maître de la Légion d'honneur. Il simplifie le protocole de l'Élysée et rajeunit quelques symboles nationaux — le "bleu drapeau" de l'étendard français est remplacé par un bleu cobalt plus clair, jugé moins agressif, "La Marseillaise" est jouée sur un ton moins fort et un rythme plus lent. Il est également le premier président à donner des entretiens en anglais à la presse internationale. Cherchant à se rapprocher des Français, il conduit sa propre voiture et multiplie les occasions de se montrer à leurs côtés, notamment lors de dîners dans des familles.

Il se différencie par ailleurs de l'ancien pouvoir gaulliste en annonçant, lors du premier Conseil des ministres de sa présidence, la fin des saisies de presse et des écoutes téléphoniques ordonnés par l'exécutif. Afin de dépolitiser le secteur audiovisuel, l'Office de radiodiffusion télévision française (ORTF), qui était en situation de monopole, est démantelé. Sur le plan institutionnel, la révision constitutionnelle du 29 octobre 1974 élargit la possibilité de saisine du Conseil constitutionnel aux parlementaires (60 députés ou 60 sénateurs), permettant ainsi à l'opposition de veiller au respect de la Constitution ; qualifiée de « réformette » lors de son vote par le Congrès, cette mesure a eu un impact important sur le paysage politique et juridique français.

En matière d'urbanisme, un certain nombre de décisions destinées à marquer fortement l'évolution de politique de la ville sont prises. Le , Valéry Giscard d'Estaing met un terme au projet de voie express rive gauche à Paris, invitant le gouvernement à financer les voiries urbaines « qui ne risquent pas de porter atteinte à l'environnement et qui présentent un caractère d'urgence plus marqué ». Le 6 août suivant, il décide de ne plus autoriser à l'emplacement des anciennes halles de Paris la construction d'un centre de commerce international et de le remplacer par un espace vert. Il adresse, le , une lettre au Premier ministre dans laquelle il lui détaille les principaux principes devant inspirer l'aménagement de Paris.

Le , au palais de l'Élysée, Valéry Giscard d'Estaing fait chevalier de la Légion d'honneur le cuisinier Paul Bocuse, décoration qui n'avait pas été remise à un cuisinier depuis 1919 (Auguste Escoffier) ; pour cette occasion, le chef crée la « soupe aux truffes noires VGE ». Le chocolatier Maurice Bernachon crée de son côté le gâteau « Président ».

Rapidement après son ascension à la présidence de la République, Valéry Giscard d'Estaing met en œuvre d'importantes réformes législatives, avec pour objectif d'adapter la législation à l'évolution des mœurs et des réalités sociales, bien qu'une partie de son électorat y soit défavorable.

Il fait ainsi voter la loi instaurant le divorce par consentement mutuel et pour rupture de la vie commune. Largement adoptée par le Parlement et publiée au "Journal officiel" le , elle met fin à une situation dans laquelle seule une faute de la part d'un des deux conjoints rendait le divorce possible.

Valéry Giscard d'Estaing confie le soin de faire voter la loi dépénalisant l'avortement, encadrant l'interruption volontaire de grossesse (IVG) et autorisant l'interruption médicale de grossesse (IMG) sous conditions à la ministre de la Santé, Simone Veil, et non au ministre de la Justice, Jean Lecanuet, plus conservateur. La loi Veil du 17 janvier 1975 rencontre une forte opposition, notamment de la part de la droite, et est finalement adoptée grâce au vote des députés de gauche et du centre. Une visite de Valéry Giscard d'Estaing au Vatican, où l'accueil du pape Paul VI est particulièrement froid, met en crise la relation avec le Saint-Siège, et l'ambassadeur de France près le Saint-Siège, Gérard Amanrich, est limogé. Cet épisode fait prendre conscience à Valéry Giscard d'Estaing qu'il s'est aliéné une partie de l'électorat catholique. Le spécialiste des sondages Michel Pinton estime que cette mesure lui a « fait perdre à voix qu'il ne retrouvera jamais ». 

L'abaissement de la majorité civile et électorale de 21 à 18 ans, profitant à plus de deux millions de jeunes, est également perçue comme une prise de risque sur le plan électoral pour Giscard, cette frange de la population lui étant majoritairement hostile.

Son intention de modifier l'organisation du travail dans les entreprises n'aboutit pas. Néanmoins, les mesures « Giroud », sur l'égalité professionnelle entre les hommes et les femmes, et « Stoléru », sur la revalorisation du travail manuel, sont adoptées. 

La période des réformes sociales et sociétales s'achève à l'automne 1976, à l'exception notable des deux « lois Scrivener » et de la loi du relative à la motivation des actes administratifs et à l'amélioration des relations entre l'administration et le public. La priorité est alors donnée aux questions économiques. Plusieurs mesures jugées plus conservatrices sont adoptées à la fin du septennat. Valéry Giscard d'Estaing, qui nie avoir effectué un tournant conservateur, déclare « qu'après un certain seuil de transformation de la société, il est nécessaire de faire une pause et de souffler » et met en avant l'opposition des gaullistes à ce type de réformes. L'Université est reformée avec pour objectif de mettre fin à l'« utopie totalitaire » d'après-Mai 68, selon les termes de la ministre Alice Saunier-Seïté.

La loi « sécurité et liberté », adoptée trois mois avant l'élection présidentielle de 1981 et qualifiée de liberticide par la gauche, accroît la répression à l'égard des délinquants. Le garde des Sceaux, Alain Peyrefitte, avait initialement envisagé d'introduire l'abolition de la peine de mort dans cette loi, mais y avait renoncé face à l'opposition des députés gaullistes. Déclarant ressentir une « aversion profonde » pour la peine capitale, Valéry Giscard d'Estaing annonce, en début de campagne, qu'il consultera en détail les dossiers des condamnés à mort et se prononcera sur chaque cas avec et . Plus tard, il estime souhaitable que . Durant son septennat, il accorde sa grâce à quatre condamnés à mort dont un mineur, et la refuse à trois reprises (Christian Ranucci en 1976, puis Jérôme Carrein et Hamida Djandoubi en 1977), estimant que la peine de mort a un effet dissuasif lorsque « les victimes sont des enfants ou des femmes faibles, maltraitées, torturées », ce que lui aurait notamment confirmé l'avocat Edgar Faure. Ce seront les dernières exécutions en France. Dans le même temps, en mars 1976, le président de la République met en place un comité d'études sur la violence, la délinquance et la criminalité, présidé par Alain Peyrefitte ; l'année suivante, par six voix contre trois et deux abstentions, le comité se prononce pour l'abolition de la peine capitale en contrepartie de la création d'une « peine de sûreté ». Mais Alain Peyrefitte déclare ensuite, dans "Le Monde" du 25 août 1977, que l'opinion publique n'est pas encore prête à accepter une telle réforme.

Il inaugure la première journée « portes ouvertes » au palais de l'Élysée en 1977, tandis que les premières Journées du patrimoine sont organisées en 1980, année déclarée « année du patrimoine » par le gouvernement français en août 1978.

La politique de Valéry Giscard d'Estaing en matière d'immigration se caractérise par la fin de la politique d'incitation à l'immigration et par le renforcement du contrôle des entrées et des séjours sur le territoire national.

Fait symbolique, un secrétariat d'État aux Travailleurs immigrés, confié à André Postel-Vinay, est créé. C'est sous son impulsion que le gouvernement suspend, en , l'immigration des travailleurs et des familles désireuses de rejoindre un de leurs membres en France, à l'exception notable des ressortissants de la Communauté européenne. Le droit au regroupement familial des immigrés reprend néanmoins à partir du et un décret, signé le par Jacques Chirac, détermine les conditions dans lesquelles un travailleur étranger séjournant régulièrement en France peut être rejoint par les membres de sa famille. Par un décret en date du , le gouvernement suspend pour trois ans l'application du décret du , et, dans un contexte de hausse du chômage, réserve le droit au regroupement familial aux seuls membres de la famille qui ne demandent pas l'accès au marché du travail.

Mais le Conseil d'État, dans une décision du , annule le décret et érige le « droit de mener une vie familiale normale » en principe général du droit. Cette décision du Conseil d'État a des conséquences déterminantes sur le long terme et « ouvre la voie à une immigration de peuplement ».
La loi relative à la prévention de l'immigration clandestine, dite loi « Bonnet », du nom du ministre de l'Intérieur, est promulguée le . Elle renforce les conditions d'entrée sur le territoire, permet l'éloignement des immigrés en situation irrégulière et leur détention dans un établissement pénitentiaire pendant un délai pouvant aller jusqu'à sept jours s'ils ne peuvent quitter immédiatement le territoire. Le , Lionel Stoléru, secrétaire d'État auprès du ministre du Travail et de la Participation (Travailleurs manuels et Immigrés), affirme qu'« il n'est plus question d'accueillir un seul étranger en France ».

En matière d'insertion, le gouvernement tente de résoudre les problèmes d'accueil et de logement, de scolarisation et de formation professionnelle, tandis que le principe d'égalité dans l'accès aux soins et aux prestations sociales est admis. La politique d'immigration de Valéry Giscard d'Estaing est également marquée par l'encouragement aux retours volontaires des immigrés dans leur pays d'origine avec la mise en place, en 1977, d'une aide financière au retour de francs et, en 1978, d'un système de retours forcés d'une partie de la main d'œuvre étrangère installée parfois depuis longtemps en France. Mais l'objectif du gouvernement de faire retourner dans leurs pays d'origine, grâce à l'« aide au retour », étrangers, principalement issus du Maghreb, n'est pas atteint.

En , un pasteur protestant, un prêtre catholique et un immigré algérien menacé d'expulsion se lancent dans une grève de la faim pour protester contre les expulsions de travailleurs immigrés et des jeunes de la seconde génération ; ils cessent leur action après l'annonce, par le ministre de l'Intérieur, de la suspension pour trois mois des expulsions de jeunes immigrés, sauf en cas de délits graves. Le 10 mai suivant, à un an de l'élection présidentielle, le Parti socialiste, le PSU, la LCR, la CFDT et la Ligue des droits de l'homme participent à une marche de protestation contre le projet de loi Stoléru sur le renouvellement des cartes de séjour et de travail et contre le projet d'Ornano codifiant l'accès aux foyers collectifs. Ils critiquent également une circulaire rendant plus difficile l'inscription des étudiants étrangers dans les universités françaises ; cette circulaire est défendue par le Premier ministre, Raymond Barre, qui dénonce, le , à la tribune de l'Assemblée nationale, « l'afflux d'étudiants étrangers fantômes qui ne sont là que pour mener une action publique orientée contre leurs pays d'origine ».

L'historien et politologue Patrick Weil, spécialiste des questions d'immigration et de citoyenneté, révèle en 2015 que Valéry Giscard d'Estaing, devant l'échec de la politique d'aides financières au retour, prône une politique de retour forcé. À partir de 1978, il avait l'intention, selon les travaux de Patrick Weil, de « dénoncer les accords d'Évian, qui permettaient la libre circulation entre la France et l'Algérie » et de « changer la loi pour permettre l'arrêt des titres de séjour, ou la non-reconduite des titres de séjour de ceux qui étaient là depuis dix, quinze ou vingt ans ». Son objectif aurait été d'organiser le retour de Algériens par an pendant cinq ans. Mais face aux réticences de parlementaires de la majorité et de la plupart des membres du gouvernement, notamment Raymond Barre et Simone Veil, ainsi que du Conseil d'État, il renonce à cette idée.

Au début sa présidence, plusieurs mesures sociales sont prises : le minimum vieillesse est majoré de 21 % à partir du , l'âge légal de départ à la retraite est abaissé à 60 ans pour deux millions de personnes au métier pénible et une allocation supplémentaire d'attente (ASA), permettant aux individus licenciés pour motif économique de percevoir 90 % de leur salaire pendant un an, est créée.

Mais le septennat de Valéry Giscard d'Estaing est surtout marqué par les conséquences des deux chocs pétroliers, qui brisent la dynamique des Trente Glorieuses.

En réponse à la nouvelle situation économique, il opte d'abord pour un plan de lutte contre l'inflation, qui s'élève à 16,8 % en 1974. Les mesures prises par le ministre de l'Économie, des Finances et de l'Industrie, Jean-Pierre Fourcade, parmi lesquelles des réductions de dépenses publiques, la mise en place de nouvelles taxes (majoration d'impôt selon les revenus, contribution exceptionnelle d'impôt sur les sociétés) et une politique monétaire restrictive, accroissent la dépression et favorisent l'apparition du phénomène de stagflation, situation dans laquelle la croissance est faible, tandis que le niveau général des prix et le chômage augmentent. Le Premier ministre, Jacques Chirac, est alors favorable à un plan de relance, ce à quoi s'oppose le ministre des Finances. Le président tranche finalement, en 1975, en faveur d'un plan de soutien à l'économie de 30 milliards de francs (15 milliards pour les investissements publics, 10 milliards de mesures fiscales et 5 milliards de crédits accordés aux familles nombreuses et aux personnes âgées).

Lorsque Raymond Barre est nommé Premier ministre, l'inflation et le déficit extérieur s'accroissent, tandis que le franc, mis en difficulté, a dû quitter le Serpent monétaire européen en , huit mois après l'avoir réintégré. Valéry Giscard d'Estaing, guidé par son ambition européenne, charge alors le nouveau chef du gouvernement de rétablir les grands équilibres économiques. Celui-ci, qui a pour objectif affiché de combattre l'inflation et de maintenir la stabilité de la monnaie, décide de mettre un terme à toute politique de "stop and go" (politique restrictive avec baisse de l'inflation, puis politique de relance afin de réduire l'augmentation du chômage provoquée par la politique précédente, puis à nouveau politique restrictive afin de diminuer l'inflation, et ainsi de suite). Il restera fidèle à ces principes jusqu'à la fin du septennat.

Le « plan Barre » du prévoit la limitation des hausses des salaires, le gel des prix à la consommation pour trois mois et des tarifs publics pour six mois, l'abaissement du taux de TVA sur certains produits de 20 % à 17,6 %, l'augmentation de l'impôt sur le revenu de 4 % à 8 % pour les ménages les plus aisés, ainsi que des prix de l'alcool et de l'essence. Ces mesures visent à lutter contre le chômage et à assurer une meilleure compétitivité des entreprises françaises ; le gouvernement mise sur l'appui du patronat pour maintenir le pouvoir d'achat moyen des ménages. Un deuxième plan austérité est lancé en . Il met en place plusieurs mesures sociales pour accompagner la restructuration de l'industrie et l'assouplissement du marché du travail. Cette politique de rigueur est attachée à l'image du Premier ministre, qui n'hésite pas à déclarer que la faible productivité de la sidérurgie dans le bassin lorrain doit se traduire par la baisse du nombre d'emplois ou encore que « la politique du gouvernement ne se détermine pas en fonction de la longueur d'un cortège » de manifestants.

Les politiques économiques menées depuis la fin de la Seconde Guerre mondiale étaient jusque-là marquées par un certain dirigisme, auquel Valéry Giscard d'Estaing avait participé en tant que ministre des Finances, l'État contrôlant le crédit, la masse monétaire, les prix, les salaires. La situation change en 1978, à la suite de la formation du troisième gouvernement Barre, dans lequel le Premier ministre laisse ses fonctions de ministre de l'Économie et des Finances à René Monory. Le , Raymond Barre, inspiré par les résultats de l'économie ouest-allemande, se prononce en faveur d'« un libéralisme social » et des « règles de l'économie de marché ». La concurrence et la liberté des prix sont alors privilégiées : les prix industriels sont libérés dès cette année 1978, suivis de ceux des services l'année suivante. Cette politique libérale, critiquée par les syndicats, se différencie du Programme commun de la gauche et de l'interventionnisme gaulliste. Concernant le financement de l'économie, la majorité met en place une politique favorable à l'épargne avec une réduction d'impôt en cas d'investissement dans des Société d'investissement à capital variable (SICAV).

Sous le septennat de Valéry Giscard d'Estaing, le taux de prélèvements obligatoires en proportion du PIB passe de 33,5 % à 39 %. Le levier fiscal est d'abord utilisé, entre 1974 et 1978, pour réguler la conjoncture, puis, en fin de septennat, pour atteindre l'équilibre budgétaire. L'impôt sur les plus-values est instauré, mais Valéry Giscard d'Estaing avancera par la suite que le Parlement, en l'incorporant dans l'assiette de l'impôt sur le revenu au lieu d'en faire un impôt forfaitaire, dénaturera son initiative. Le barème de l'impôt sur le revenu pour 1980 ne prévoit pas la revalorisation de toutes les tranches à hauteur de l'inflation, ce qui alourdit l'imposition de nombreuses « classes moyennes ».
La politique d'austérité du gouvernement Barre est menacée par le deuxième choc pétrolier, qui se traduit par la multiplication du prix du pétrole par 2,7 entre 1978 et 1981. Valéry Giscard d'Estaing privilégie, en , l'adoption de nouvelles mesures de rigueur à l'hypothèse d'un plan de « refroidissement » de l'économie repoussé après l'élection présidentielle de 1981. Cette décision affecte directement la popularité du président et, plus encore, celle du Premier ministre. La croissance annuelle de la France, qui dépasse les 3 %, est alors l'une des plus élevées au monde, tandis que le pouvoir d'achat des Français continue à croître. L'historien Jean-Charles Asselain souligne que « l'ensemble des indicateurs témoigne d'une diminution effective des inégalités », principalement en raison de l'« alourdissement de la fiscalité sur les tranches supérieures de revenu et sur les successions importantes ». Les déficits budgétaires sont maîtrisés et la dette publique de la France, qui oscille autour de 20 % du produit intérieur brut, est sensiblement la même à la fin qu'au début du septennat.

Mais la politique de lutte contre l'inflation ne produit pas les effets escomptés, en particulier avec les effets du deuxième choc pétrolier, et le chômage de masse apparaît. C'est pendant le mandat présidentiel de Valéry Giscard d'Estaing que le cap du million de chômeurs est franchi. Pour faire face à cette hausse du chômage, qui s'explique en partie par les gains de productivité et l'accroissement de la population active, le gouvernement prend des mesures, comme l'instauration en 1979 du contrat de travail à durée déterminée en France (CDD), qui sont jugées insuffisantes par une partie grandissante de la population.

À l'instar des autres pays européens, la désindustrialisation touche la France pendant le septennat de Valéry Giscard d'Estaing, ce qui conduit des dizaines de sidérurgistes à manifester dans les rues de Paris.

Après avoir mis fin à la patente, qui frappe essentiellement les commerçants, l'exécutif instaure la taxe professionnelle, qui va imposer beaucoup plus lourdement l'industrie.

Valéry Giscard d'Estaing décide de poursuivre et d'intensifier le programme de développement de l'énergie nucléaire civile engagé par son prédécesseur. En 1976, la construction du supergénérateur nucléaire nommé Superphénix est décidée, et la Compagnie générale des matières nucléaires (COGEMA) et l'Institut de protection et de sûreté nucléaire (IPSN) sont créés. En matière industrielle, il engage la modernisation du transport ferroviaire en lançant l'étude sur le train à grande vitesse. En 1974, débute la modernisation du réseau téléphonique avec le triplement des lignes sur sept ans. Devant la nécessité d'économiser l'énergie, le gouvernement restaure, en 1976, l'ancien changement d'heure pendant les mois d'été, mesure permettant une économie d'électricité pour l'éclairage du soir.

Les relations entre le président et le Premier ministre Jacques Chirac se tendent au fil des mois. Celui-ci, conseillé par Pierre Juillet et Marie-France Garaud, est élu, à la fin de l'année 1974, secrétaire général de l'UDR, sur laquelle il affirme progressivement son emprise, notamment grâce aux fonds spéciaux de Matignon. Alors que le président comptait sur lui pour « giscardiser » le parti gaulliste, Chirac en prône l'indépendance. Il démissionne le , estimant ne pas disposer « des moyens qu'[il] estime nécessaires pour assumer efficacement les fonctions de Premier ministre ». Dans le même temps, la volonté de « décrispation » de la société française voulue par Giscard se heurte au refus de François Mitterrand d'entamer tout dialogue avec la majorité, contrairement aux usages établis dans les pays étrangers.

Jacques Chirac est remplacé par le « technicien » Raymond Barre, jusque-là ministre du Commerce extérieur et présenté comme le « meilleur économiste français, en tout cas un des premiers » par Valéry Giscard d'Estaing. À l'instar de Raymond Poincaré ou Antoine Pinay par le passé, le Premier ministre cumule ses fonctions de chef de gouvernement avec celle de ministre de l'Économie. Son gouvernement compte trois figures principales, élevées au rang de ministres d'État : le gaulliste Olivier Guichard, le giscardien Michel Poniatowski et le centriste Jean Lecanuet. À partir de ce moment, le président revient à une lecture plus classique de la Constitution et laisse le Premier ministre exercer ses prérogatives sur les questions intérieures ; les analystes politiques estiment qu'il sera perçu comme s'éloignant de plus en plus des Français. La situation de la majorité sur le plan électoral devient difficile. Les élections cantonales de 1976 avaient déjà été remportées par la gauche, qui était ainsi pour la première fois majoritaire dans le pays, bien que la droite conserve la plupart des départements. Les élections municipales de 1977 constituent également un succès pour l'opposition de gauche, qui prend le contrôle de 155 villes de plus de habitants, soit 57 de plus qu'auparavant. Jacques Chirac, qui a créé trois mois auparavant le Rassemblement pour la République (RPR), est élu maire de Paris contre Michel d'Ornano, soutenu par les giscardiens, ce qui accentue les tensions à droite. À la suite de ces élections, le Premier ministre remet sa démission et forme, le , son deuxième gouvernement, qui voit le départ des trois ministres d'État et l'arrivée des gaullistes Robert Boulin et Alain Peyrefitte, ainsi que du sénateur et maire de Loudun, ancien garagiste, René Monory.

Soucieux de rajeunir son parti, dont la notoriété et la popularité sont faibles, Valéry Giscard d'Estaing soutient la transformation, en mai 1977, de la FNRI en Parti républicain (PR), dont Jean-Pierre Soisson devient le secrétaire général. Le , alors que les sondages prédisent une victoire de la gauche aux élections législatives à venir, Valéry Giscard d'Estaing prononce un discours, à Verdun-sur-le-Doubs (Saône-et-Loire), dans lequel il appelle les Français à faire « le bon choix pour la France » et les met en garde contre les blocages politiques et les conséquences économiques (hausse du chômage, aggravation du déficit budgétaire, baisse de la valeur de la monnaie) que provoquerait une victoire de la gauche aux élections législatives du mois de mars. Il déclare qu'il ne démissionnera pas dans cette hypothèse, mais qu'il n'aura pas les moyens d'empêcher une majorité de gauche d'appliquer le Programme commun. Valéry Giscard d'Estaing, qui a consulté de nombreux constitutionnalistes, évoque ainsi la possibilité d'une cohabitation. Cette hypothèse est inenvisageable pour les gaullistes, pour qui une victoire de la gauche ne peut déboucher que sur une crise institutionnelle.
Quelques jours plus tard, le , il fonde l'Union pour la démocratie française (UDF), qui regroupe les différentes composantes centristes et giscardiennes de la majorité (Parti républicain, Centre des démocrates sociaux, Parti radical, Centre national des indépendants et paysans, Mouvement démocrate socialiste de France) et permet ainsi de limiter les candidatures à droite lors du scrutin législatif. Le nouveau parti présente 405 candidats pour 491 circonscriptions. Au premier tour des élections législatives, la gauche arrive en tête, mais avec une avance moindre que prévu. Dans l'entre-deux tours des élections législatives, conformément à un accord conclu l'année précédente, en cas d'absence de candidat unique de la majorité, le candidat de droite le moins bien placé dans une circonscription se retire en faveur de celui arrivé en tête. Le , à la surprise générale, le RPR et l'UDF remportent une majorité nette à l'Assemblée nationale (290 sièges). Dans un contexte de déliquescence de l'Union de la gauche et de mauvais report de voix des socialistes sur les candidats communistes, le discours du « bon choix » et l'alliance de la droite semblent avoir porté leurs fruits. Le nouveau parti du président réalise un score honorable en obtenant sensiblement moins de sièges que le parti gaulliste, qui perd de son côté 39 députés. Le , lors de l'élection du président de l'Assemblée nationale, Jacques Chaban-Delmas est préféré au sortant Edgar Faure, soutenu par Jacques Chirac et une grande partie des députés gaullistes ; ce résultat est considéré comme étant une victoire politique pour le président de la République.

Les premières élections européennes se tiennent en 1979, au scrutin proportionnel à un tour. Simone Veil est choisie pour être tête de liste de l'UDF, tandis que Jacques Chirac est celle du RPR. Le , celui-ci, hospitalisé à la suite d'un grave accident de voiture, signe l'appel de Cochin, sur les conseils de Marie-France Garaud et Pierre Juillet, dont il se séparera après cet épisode. Dans ce communiqué, il critique le projet européen, parlant de « politique d'asservissement », et critique l'UDF pour ses prises de position pro-européennes : « Comme toujours quand il s'agit de l'abaissement de la France, le parti de l'étranger est à l'œuvre avec sa voix paisible et rassurante. Français, ne l'écoutez pas. C'est l'engourdissement qui précède la paix de la mort. » Ces propos sont vus comme une charge violente à l'égard du président de la République et sont considérés comme excessifs, y compris au sein du RPR. Le , l'UDF arrive en tête du scrutin avec 27,6 % (25 élus), contre 23,5 % pour le PS (22 élus), 20,5 % pour le PCF (19 élus) et seulement 16,3 % pour le RPR (15 élus). Simone Veil est ensuite élue présidente du Parlement européen, malgré l'opposition des députés européens français issus du RPR, qui présentent un candidat contre elle.

Les dissensions au sein de la majorité conduisent le Premier ministre à utiliser, à plusieurs reprises, l'article 49.3 de la Constitution, qui permet de faire passer des textes sans vote si aucune motion de censure n'est adoptée. Après l'utilisation de l'article 49.3 lors du vote du budget 1980, auquel s'opposait le RPR, le président de l'Assemblée nationale, Jacques Chaban-Delmas, saisit le Conseil constitutionnel, qui annule le vote de la loi pour une question de procédure, ce qui conduit à la convocation d'une session extraordinaire à la fin de l'année 1979.

Sous le septennat de Valéry Giscard d'Estaing, trois figures politiques de droite meurent de façon violente ou suspecte. Le député Jean de Broglie (Républicain indépendant) est assassiné le , vraisemblablement victime d'un règlement de comptes. Le , l'ancien ministre gaulliste Joseph Fontanet est également tué ; ce meurtre reste non élucidé. Le , le ministre du Travail, Robert Boulin (RPR), accusé, par des lettres anonymes publiées par "Le Canard enchaîné", d'avoir acquis de manière illégale une garrigue, est retrouvé mort dans un étang de la forêt de Rambouillet. L'enquête judiciaire ne permet pas d'aboutir à des conclusions définitives. Des personnalités du RPR proches de Valéry Giscard d'Estaing accusent alors les gaullistes d'avoir mené une campagne de déstabilisation à l'encontre de Robert Boulin, qui était pressenti pour succéder à Raymond Barre à la tête du gouvernement à l'approche de l'élection présidentielle de 1981. L'hypothèse de l'assassinat politique est envisagée, Robert Boulin ayant notamment fait l'objet de menaces de mort de la part du SAC gaulliste.

Dans un contexte de guerre froide, Valéry Giscard d'Estaing multiplie les rencontres avec les chefs d'État étrangers et cherche à « dédramatiser » la situation internationale. Il tente de dépasser les conflits idéologiques et prône une direction mondiale des grands pays industrialisés. C'est sur son initiative que se tient la première réunion des cinq pays les plus industrialisés de la planète (États-Unis, Japon, France, Allemagne de l'Ouest, Royaume-Uni), au château de Rambouillet, du 15 au . Ce « G5 », qui devient par la suite « G7 » avec la participation de l'Italie et du Canada, se réunira tous les ans afin que les chefs d'État puissent aborder les questions économiques et financières d'actualité.

Dans le but notamment d'assurer les approvisionnements de la France en pétrole, il mène une politique jugée proarabe. En 1975, il refuse de s'allier aux États-Unis contre l'Organisation des pays exportateurs de pétrole (OPEP).

En octobre 1976, en visite officielle en Iran, pays alors en pleine expansion et qui apparaît comme le meilleur rempart aux ambitions soviétiques en Asie occidentale, Valéry Giscard d'Estaing souligne la « stabilité » apportée par cet État « dans une zone d'importance stratégique pour la paix dans le monde ». À cette occasion, il parvient à conclure avec le chah d'Iran des contrats pour 30 milliards de francs, ce qui aurait permis à la France de devenir le quatrième fournisseur de ce pays. Lors de l'exil de l'ayatollah Khomeini en France, le président français lui fait savoir qu'il doit s'abstenir d'inciter à la violence en Iran. Celui-ci ne s'exécutant pas, Valéry Giscard d'Estaing envisage de l'expulser vers l'Algérie, mais le chah s'y oppose. À la fin des années 1970, au moment de la révolution iranienne, les pays occidentaux renoncent à apporter leur soutien au chah, que la France sait condamné par la maladie. L'Iran devient dès lors une république islamique.

Attaché à l'Afrique, Valéry Giscard d'Estaing lance plusieurs actions sur ce continent. En avril 1975, il est le premier président français à se rendre en Algérie depuis l'indépendance de ce pays, en 1962 ; les relations entre les deux États se tendent néanmoins quand la France apporte son soutien à la Mauritanie dans la guerre du Sahara occidental. En 1980, lors des événements de Gafsa, après avoir tenté d'établir de bonnes relations avec le dirigeant libyen Mouammar Kadhafi, le président français apporte son aide à la Tunisie, ce qui provoque l'incendie de l'ambassade de France à Tripoli et l'attaque du centre culturel français de Benghazi. C'est l'action au Zaïre, qui ne fait pas partie de la zone d'influence française, qui marque principalement la politique africaine du septennat. En mai 1978, Valéry Giscard d'Estaing déclenche l'opération « Léopard » en envoyant des troupes parachutistes du REP à Kolwezi pour libérer les Européens pris en otage par des rebelles au gouvernement zaïrois et éliminer la menace qui pèse sur le pays africain. Le président français prend cette décision après d'intenses négociations avec la Belgique et les États-Unis et alors que l'envoi de parachutistes sur un sol contrôlé par des rebelles est jugé périlleux. Les otages sont finalement libérés, les rebelles défaits, et les parachutistes français quittent le Zaïre le .

Le parcours politique de Valéry Giscard d'Estaing se caractérise, dès ses débuts, par son engagement européen : son premier grand discours à l'Assemblée nationale était ainsi consacré à la défense du traité de Rome. Pendant sa présidence, il consacre une grande importance à la construction européenne. Après avoir défendu l'idée d'États-Unis d'Europe dans sa jeunesse, il s'affirme par la suite partisan d'une « troisième voie », entre une Europe supranationale et une Europe des États. Grâce au soutien du chancelier fédéral allemand, Helmut Schmidt, il est à l'origine de la création du Conseil européen en décembre 1974, ce qui est vu comme le prolongement de l'action gaulliste, qui privilégiait la coopération entre les États à l'intégration communautaire. En contrepartie, la France est pressée d'accepter l'élection du Parlement européen au suffrage universel direct, mesure prévue dans le traité de Rome, mais à laquelle s'étaient successivement opposés Charles de Gaulle et Georges Pompidou. Les gaullistes, notamment Michel Debré, s'y opposent et réclament pour certains une renégociation du traité de Rome. Valéry Giscard d'Estaing sollicite l'avis du Conseil constitutionnel, qui estime que l'élection du Parlement européen au suffrage universel est conforme à la Constitution. La loi entérinant cette mesure est adoptée en , le gouvernement ayant eu recours à l'article 49.3 de la Constitution. Les élections européennes de 1979 sont les premières élections du Parlement européen au suffrage universel. Il en résulte une augmentation des pouvoirs du Parlement européen, en particulier en matière budgétaire. Avec le chancelier Helmut Schmidt, Giscard resserre les liens entre la France et l'Allemagne.

Valéry Giscard d'Estaing met l'accent sur le volet économique de la construction européenne. En 1978, deux ans après la nouvelle sortie du franc du Serpent monétaire européen, est lancé, sous l'impulsion de la France et de l'Allemagne, le Système monétaire européen (SME), qui établit un système de taux de change stable et ajustable entre les pays membres de la Communauté économique européenne. Valéry Giscard d'Estaing cherche ainsi à stabiliser et à renforcer la monnaie nationale, afin d'affirmer la position de la France au niveau international. L'ECU, unité de compte européenne, est créé l'année suivante, en 1979 ; le président français a milité pour ce nom, acronyme de "European Currency Unit", qui rappelle l'écu, monnaie française utilisée au Moyen Âge. Ces mesures sont considérées comme des préalables à l'instauration d'une union économique et monétaire en Europe.

Un an avant l'élection présidentielle, dans un entretien accordé à l'hebdomadaire "L'Express" le , le président de la République estime avoir réalisé les trois quarts de ce qu'il souhaitait faire. Un sondage publié le mois précédent dans "Le Point" le donne vainqueur avec 57 % des voix face à Michel Rocard et 61 % face à François Mitterrand. C'est finalement ce dernier qui est désigné candidat du Parti socialiste en .

Rassuré par la victoire surprise de son camp aux élections législatives de 1978 et par les sondages, Valéry Giscard d'Estaing s'est peu à peu désintéressé des stratégies politiques au profit des questions économiques. Il annonce sa candidature à un second mandat, depuis le palais de l'Élysée, le . À quelques semaines du premier tour de l'élection présidentielle, la stratégie et l'organisation de sa campagne ne sont pas définies.

La campagne est notamment marquée par l'affaire des diamants, qui éclate en . "Le Canard enchaîné", puis "Le Monde" l'accusent d'avoir reçu, alors qu'il était ministre des Finances, des diamants en guise de cadeaux de Jean-Bedel Bokassa, alors président de la République centrafricaine. Notamment informé par Bokassa, dont Valéry Giscard d'Estaing a contribué à la chute en 1979, "Le Canard enchaîné" estime la valeur de ces diamants à un million de francs. Présenté par ses opposants comme un monarque républicain, le président de la République traite avec mépris ces accusations, ce qui va accréditer les affirmations des journaux et avoir une influence négative sur sa campagne de réélection. Par la suite, on apprendra que la valeur de ces diamants, qui ont été remis à plusieurs organisations caritatives, a été largement surestimée. La probité de Valéry Giscard d'Estaing pendant son septennat n'a pu être remise en cause : il se montre en particulier soucieux des dépenses publiques, s'opposant notamment à l'affrètement d'avions à des membres du gouvernement et à ses collaborateurs. D'autres facteurs, comme son silence pendant les jours suivants l'attentat de la rue Copernic ou son entretien avec le dirigeant soviétique Léonid Brejnev à Varsovie le , en dépit de la condamnation internationale de l'intervention soviétique en Afghanistan, ont un impact négatif sur sa campagne.

Le 26 avril 1981, il arrive en tête du premier tour de l'élection présidentielle avec 28,32 % des voix, devant François Mitterrand (25,85 %) et Jacques Chirac (18,00 %). Le président du RPR, qui a rencontré secrètement le premier secrétaire du Parti socialiste au domicile d'Édith Cresson en octobre 1980 afin d'élaborer une stratégie visant à faire battre le président sortant, refuse d'appeler ses partisans à soutenir Valéry Giscard d'Estaing pour le second tour et ne se prononce en sa faveur qu'à titre personnel ; des permanences du RPR donnent alors pour consigne de voter pour François Mitterrand.

Lors du débat télévisé de l'entre-deux tours, le , François Mitterrand qualifie Valéry Giscard d'Estaing d'« homme du passif », en réaction à « l'homme du passé » ou « vous n'avez pas le monopole du cœur » dont Giscard l'avait crédité sept ans plus tôt lors du débat télévisé de la présidentielle de 1974. À l'issue de cette confrontation, la comparaison entre les points forts et les points faibles des deux candidats est néanmoins favorable au président sortant.

Peu avant le second tour, "Le Canard enchaîné" publie des documents indiquant que le ministre du Budget, Maurice Papon, a été responsable de la déportation de Juifs sous le régime de Vichy. De son côté, Valéry Giscard d'Estaing refuse de publier une photographie – trouvée par ses services – montrant François Mitterrand pendant la Seconde Guerre mondiale en compagnie du maréchal Pétain ; alors qu'il affirme à ses collaborateurs ne pas vouloir le niveau de la campagne, ses soutiens estiment que la fuite de ce cliché lui aurait permis de l'emporter.

Le , Valéry Giscard d'Estaing perd le second tour de l'élection en recueillant 48,24 % des suffrages exprimés, contre 51,76 % à François Mitterrand. Neuf jours plus tard, il prononce un discours de fin de mandat diffusé au journal télévisé d'Antenne 2, qu'il conclut par la locution , dans une mise en scène demeurée célèbre. Le , après un entretien d'une heure avec François Mitterrand, il quitte à pied le palais de l'Élysée, applaudi par ses partisans et hué par des militants socialistes.

Dans "Le Pouvoir et la Vie", Valéry Giscard d'Estaing écrit : « Pourquoi ai-je échoué ? En raison du chômage ? d'une lassitude des Français ? J'étais crédité de 60 % de bonnes opinions et puis tout à coup une tornade s'est levée. C'est un phénomène étrange ». À l'occasion du trentième anniversaire de sa défaite, en 2011, il déclare que sa campagne était « mauvaise », « bâclée ». Sa campagne, la fin des Trente Glorieuses, l'attitude de Jacques Chirac et la large préférence pour François Mitterrand des électeurs de 18 à 21 ans – à qui Valéry Giscard d'Estaing a donné le droit de vote – semblent avoir été déterminants dans sa défaite.

Vivant sa défaite comme une « injustice », il disparaît pendant plusieurs semaines. Il voyage pendant plusieurs mois à l'étranger, se retirant notamment dans un monastère au mont Athos, en Grèce, et dans le ranch de son ami Jean Frydman, au cœur des montagnes Rocheuses, au Canada. 

En tant qu'ancien président de la République, Valéry Giscard d'Estaing est membre de droit du Conseil constitutionnel, mais il refuse d'y siéger jusqu'en 2004. Il est le premier bénéficiaire, en 1985, d'une lettre du Premier ministre Laurent Fabius « fixant de manière permanente le statut dans la nation des anciens présidents de la République ». À ce titre, l'État français débourse, en 2016, 2,5 millions d'euros pour sa sécurité, la rémunération de ses collaborateurs permanents et un logement de fonction.

En octobre 1981, il publie un communiqué de presse pour réagir à la dévaluation du franc. Il effectue formellement son retour en politique lors des élections cantonales de mars 1982, lors desquelles il est élu conseiller général du Puy-de-Dôme dans le tout nouveau canton de Chamalières, avec 72 % des suffrages au premier tour, un score jugé exceptionnel sur ce territoire. Le , il intervient à la télévision pour la première fois depuis sa défaite à l'élection présidentielle, dans "L'Heure de vérité", émission lors de laquelle les trois quarts des téléspectateurs le jugent convaincant d'après une étude BVA, qui indique aussi que les Français sont plus partagés sur ses chances de revenir au pouvoir. Pendant la présidence de François Mitterrand, il est, aux côtés de Jacques Chirac, le principal dirigeant de l'opposition. Il affiche un avis partagé sur Mitterrand, le considérant en 1986 comme « la fausse valeur de la seconde moitié ce siècle » mais déclarant a posteriori qu'il était le « dernier à avoir eu la dimension présidentielle ».

Le , à l'issue du premier tour d'une élection législative partielle, il est élu député dans la deuxième circonscription du Puy-de-Dôme, avec 63,2 % des voix. Il est réélu député au scrutin proportionnel en 1986, dans le département du Puy-de-Dôme, puis au scrutin majoritaire dans la troisième circonscription du Puy-de-Dôme, en 1988, 1993 et 1997.

Lors de la formation de son gouvernement dans le cadre de la première cohabitation, Jacques Chirac propose le ministère des Affaires étrangères à Valéry Giscard d'Estaing, qui le refuse. Valéry Giscard d'Estaing préside la commission des Affaires étrangères de l'Assemblée nationale de 1987 à 1989 ; il est le seul président de droite d'une commission à être reconduit à la suite des élections législatives de 1988. Après avoir quitté l'Assemblée nationale pour siéger au Parlement européen en application de la réglementation sur le non-cumul des mandats, il occupe de nouveau la présidence de cette commission de 1993 à 1997.

Partisan de l'union de l'opposition face à la majorité socialiste, il émet le vœu de réunir « deux Français sur trois », selon le titre de l'ouvrage qu'il publie en 1984, soit bien au-delà de l'électorat traditionnel de la droite. Pressenti pour être candidat à l'élection présidentielle de 1988, il y renonce le , déclarant : « J'ai déjà servi. C'est à d'autres de servir à leur tour et je souhaite qu'ils réussissent. »<ref name="Le Monde 13/02/1987">« Je ne suis pas candidat à l'Élysée », "Le Monde", 13 février 1987.</ref> Il annonce par la suite son soutien à la candidature de son ancien Premier ministre Raymond Barre, puis se rallie, au second tour, à celle de Jacques Chirac, qu'il soutiendra également en 1995 et 2002. À la suite de la réélection de François Mitterrand, c'est Valéry Giscard d'Estaing qui mène la campagne nationale de la droite aux élections législatives anticipées, qui voient le Parti socialiste l'emporter sans majorité absolue, alors que, pour la première fois, les centristes (UDF et UDC) obtiennent plus d'élus que le RPR. Les médias soulignent les divergences de vue qui opposent Valéry Giscard d'Estaing à Raymond Barre, ce dernier étant considéré comme plus conciliant avec les socialistes et le gouvernement d'« ouverture » de Michel Rocard, qui comprend des personnalités de centre droit.

Le , Valéry Giscard d'Estaing succède à Jean Lecanuet à la tête de l'UDF, qu'il a fondée dix ans plus tôt. Il tente alors de restructurer le parti et d'en concilier les différentes composantes. Comme Jacques Chirac, il doit faire face à la montée en puissance de jeunes personnalités politiques (François Léotard, Charles Millon, Dominique Baudis, François Bayrou, Philippe de Villiers) qui contestent son autorité et prônent le renouvellement de la droite. Mais ceux-ci se divisent et ne parviennent pas à présenter une liste aux élections européennes de 1989, lors desquelles la liste conduite par Valéry Giscard d'Estaing arrive largement en tête, ce qui renforce sa position de chef de l'opposition parlementaire.

L'ancien président de la République, qui propose l'idée d'une fusion du RPR et de l'UDF, est alors considéré comme le candidat probable de la droite à l'élection présidentielle de 1995. Mais les candidatures de Jacques Chirac et du Premier ministre Édouard Balladur finissent par s'imposer. Crédité de 5 % à 10 % d'intentions de vote en leur présence, il renonce à se présenter le , constatant que les mesures auxquelles il croit – notamment une baisse massive des cotisations sociales et un référendum sur la moralisation de la vie publique – « ne rencontrent pas d'échos dans l'opinion publique française ». Alors qu'Édouard Balladur a les faveurs de beaucoup de centristes, Valéry Giscard d'Estaing rallie son rival Jacques Chirac, affirmant par la suite que le Premier ministre de l'époque, d'origine turque, avait une « culture complexe » qui donnait à ses propositions « un côté incertain » alors que Chirac incarnait « le Limousin, la France du Centre », ce qui le rendait compatible avec « la culture et le mode de vie français ».

Valéry Giscard d'Estaing quitte la présidence de l'UDF le . Il met à son crédit le score du parti aux élections législatives de 1993, qui, avec 213 députés, a fait quasiment jeu égal avec le RPR, et l'absence de tout scandale touchant à la probité de l'UDF dans un contexte de révélations sur le financement occulte des partis politiques. François Léotard le remplace à la tête de l'UDF et se montre hostile à son égard. Les principaux partisans de Valéry Giscard d'Estaing se trouvent alors parmi les adhérents directs de l'UDF, mais surtout au Parti populaire pour la démocratie française (PPDF), qui remplace le Club perspectives et réalités en 1995.

En 1999, l'ancien président presse publiquement le chef de l'État, Jacques Chirac, de ramener la durée du mandat présidentiel de sept à cinq ans, mesure qu'il soutient de longue date. Le , Valéry Giscard d'Estaing dépose une proposition de loi en ce sens. Jacques Chirac, qui y était initialement opposé, s'y rallie et annonce la tenue d'un référendum sur cette question (l'idée d'un mandat renouvelable une fois n'est toutefois pas retenue). Le quinquennat est adopté le , par 73,21 % des suffrages exprimés.

Alors que le nouveau président de l'UDF, François Bayrou, marginalise ses partisans, Valéry Giscard d'Estaing appelle à l'union de la droite derrière la liste RPR aux élections européennes de 1999 et ne soutient pas la candidature de François Bayrou à l'élection présidentielle de 2002. S'estimant trop âgé pour briguer un nouveau mandat à l'Assemblée nationale, il ne se présente pas élections législatives de 2002, laissant sa circonscription à son fils Louis. Comme de nombreux cadres de l'UDF, il rejoint l'Union pour un mouvement populaire (UMP), qui entend rassembler les différentes tendances de la droite française.

Lors des élections européennes de 1989, il conduit la liste d'union UDF-RPR, qui arrive en tête du scrutin avec 28,88 % des suffrages et 26 élus, tandis que la liste centriste de Simone Veil obtient 8,43 % et 7 élus. Il entre alors au Parlement européen et démissionne de l'Assemblée nationale.

Valéry Giscard d'Estaing préside le groupe libéral, démocratique et réformateur (LDR) au Parlement européen de 1989 à 1991. Il échoue à former un pôle libéral-conservateur capable de concurrencer les groupes PPE et socialiste, et rallie le PPE en décembre 1991, en compagnie de trois autres députés européens français du groupe libéral. Il préside par ailleurs le Mouvement européen de 1989 à 1997.

Au moment de l'adhésion de l'Autriche, de la Finlande et de la Suède à l'Union européenne, en 1995, il est l'une des rares personnalités politiques à émettre des réserves sur les élargissements successifs de l'UE, considérant qu'un approfondissement préalable de ses institutions est nécessaire. Lors d'un débat parlementaire, en décembre 1994, il déclare ainsi que ce nouvel « élargissement, sans réforme des institutions communautaires ni approfondissement du contenu, change la nature du projet d'union de l'Europe ».

Il est à l'origine d'une réflexion sur un projet d'organisation européenne avec un groupe d'experts vers le milieu des années 1990. Lors du Conseil européen de Laeken, en , il est nommé à la tête de la Convention sur l'avenir de l'Europe, qui a pour but de simplifier les différents traités européens en rédigeant un projet de traité constitutionnel. Toutes les sessions de travail de la Commission sont ouvertes et retransmises par les moyens audiovisuels. Le , Valéry Giscard d'Estaing présente la Constitution européenne, qui est signée par les 25 membres de l'Union européenne le . Il prend dès lors une part active, en avril et , à la campagne pour le « oui » au référendum à propos du traité constitutionnel européen. Alors qu'il prévoit son adoption, le référendum est repoussé avec 54,68 % de « non ». Après cet échec, suivi du même résultat aux Pays-Bas quelques jours plus tard, le traité constitutionnel est en partie abandonné pour laisser place au traité de Lisbonne, signé le .

En 1986, il devient président du conseil régional d'Auvergne, ses listes ayant obtenu 46,5 % des voix lors de l'unique tour de scrutin. Il est réélu en 1992, après que les listes d'union UDR-RPR eurent recueilli 42,6 % des suffrages à l'élection régionale. À l'élection régionale de 1998, les listes UDF et RPR totalisent 41 % des voix, ce qui lui permet d'être réélu pour un troisième mandat le 20 mars 1998.

Ses mandats sont marqués par le désenclavement de l'Auvergne. Il favorise ainsi le réseau routier auvergnat en obtenant la construction de quatre autoroutes (dont l'A89), du Zénith d'Auvergne, de la Grande Halle d'Auvergne, ainsi que la création de l'Institut français de mécanique avancée. Souhaitant développer un pôle de notoriété internationale dans la région, il lance la construction de Vulcania, plus grand centre volcanique d'Europe, qui ouvre ses portes le , à Saint-Ours-les-Roches. Il contribue également à l'implantation d'un second musée des volcans et préside le parc naturel régional des volcans d'Auvergne de 1986 à 2004. Pendant ses mandats, de nombreux lycées sont construits ou restaurés par des architectes qu'il choisit lui-même et dont les travaux sont salués. Pour faire face aux coûts de telles réalisations, il réduit les frais de fonctionnement de la région et s'oppose au projet de reconstruire le bâtiment du conseil régional.

Lors des élections municipales de 1995, il échoue de peu dans la conquête de la mairie de Clermont-Ferrand, détenue par la gauche depuis 1935, sa liste recueillant 49,10 % des voix au second tour ; entre les deux tours, sa liste avait bénéficié du soutien du Front national.

Alors qu'il brigue un quatrième mandat aux élections régionales de 2004 à la tête d'une liste d'union UMP-UDF et au titre de la section départementale du Puy-de-Dôme, le mode de scrutin mis en place en 2003 le contraint à un second tour, qu'il perd avec 47,3 % des voix face à Pierre-Joël Bonté (PS), étant emporté par la vague qui fait basculer la quasi-totalité des régions françaises à gauche. Il quitte, en décembre suivant, la présidence du Conseil des communes et régions d'Europe (CCRE) qu'il occupait depuis 1997.

Après avoir quitté sa fonction de président de région, il décide d'abandonner la vie politique active pour siéger au Conseil constitutionnel, dont il est membre de droit depuis son départ de l'Élysée.

Sortant de son devoir de réserve que lui impose le Conseil constitutionnel, Valéry Giscard d’Estaing soutient Nicolas Sarkozy à l'élection présidentielle de 2007, et désavoue le candidat de l'UDF, François Bayrou, qu'il accuse . En 2012, il indique son intention de voter une nouvelle fois pour Nicolas Sarkozy, qui est selon lui « le plus crédible pour redresser le pays ». À l'occasion du congrès fondateur de l'Union des démocrates et indépendants (UDI), en octobre 2012, il « souhaite bonne chance » au nouveau parti de centre droit.

Il continue à prendre part au débat public, notamment en publiant, dans l'hebdomadaire "Le Point", des chroniques, dans lesquelles il traite des questions économiques et de la politique de l'Union européenne, dont il critique le mode de gouvernance. Il exprime également son soutien au mariage entre couples de personnes de même sexe et ses réticences quant à une participation de la France à une intervention militaire en Syrie visant à renverser le régime de Bachar el-Assad.

En mai 2014, lors du quarantième anniversaire de son accession au pouvoir, une enquête réalisée par l'institut BVA indique que 64 % des Français jugent que Valéry Giscard d'Estaing a été un bon président. Il est considéré comme ayant « des convictions profondes » et étant « compétent », « honnête » mais aussi « distant ». Pour 69 % des sondés, les mesures les plus marquantes de son mandat présidentiel sont la dépénalisation de l'avortement et l'encadrement de l'IVG.

Il soutient François Fillon dans le cadre de la primaire présidentielle des Républicains de 2016, puis à l'élection présidentielle de 2017.

Le , il devient le président de la République française ayant vécu le plus longtemps, dépassant Émile Loubet. Il est également le président ayant survécu le plus longtemps après son mandat (à ce jour, ), là encore devant Émile Loubet. À partir de mai 2017, il n'est plus le plus jeune président élu sous la Cinquième République, Emmanuel Macron étant devenu chef de l'État à l'âge de 39 ans.

Au gouvernement, sous les présidences du général de Gaulle et de Georges Pompidou, Valéry Giscard d'Estaing se montre soucieux de réduire les déficits publics, privilégie l'investissement à la consommation et conduit une politique de lutte contre l'inflation.

Au début de son septennat, sous la pression notamment de Jacques Chirac, il accepte la mise en place de plusieurs mesures de relance. Mais l'arrivée de Raymond Barre à la tête du gouvernement marque le début d'une politique de rigueur assumée. Son septennat voit ainsi le taux de prélèvements obligatoires augmenter sensiblement alors même qu'il met en garde contre une hausse trop importante de ceux-ci, un phénomène selon lui inhérent au socialisme.

Tout en augmentant l'imposition des entreprises, l'exécutif se pose en défenseur de la compétitivité économique avec notamment l'instauration dans le secteur privé du contrat de travail à durée déterminée (CDD). À partir de 1978, dans le cadre de son projet de « libéralisation » de l'économie française, la concurrence et la liberté des prix sont privilégiées. Contrairement à ce qui était pratiqué depuis la fin de la Seconde Guerre mondiale, la politique économique menée n'est plus marquée par le dirigisme (contrôle par l'État du crédit, des salaires, des investissements).

Valéry Giscard d'Estaing fait adopter au début de son septennat plusieurs mesures sociales (revalorisation substantielle du minimum vieillesse, aménagement de l'âge de départ à la retraite pour les personnes ayant un emploi pénible, etc.). Pour lutter contre le chômage, qui augmente de façon importante après les deux chocs pétroliers, des mesures coûteuses sur le plan financier sont mises en place (développement de la formation professionnelle, incitation au travail à temps partiel, développement de la pré-retraite). Mais celles-ci se révèlent relativement inefficaces, ce que soulignera la gauche lors de la campagne présidentielle de 1981.

Valéry Giscard d'Estaing se montre parmi les dirigeants de droite les plus fermes sur la question de l'immigration.

Pendant sa présidence, sous la pression de l'aile centriste de sa majorité, il doit renoncer à son projet de revenir sur les accords d'Évian afin d'expulser de France Algériens chaque année.

En 1990, dénonçant un sentiment grandissant de « francophobie » et estimant que « la France n'est pas un pays d'immigration », il se prononce pour des moyens réglementaires et administratifs permettant de gérer un quota zéro d'immigration ». La même année, il participe aux états généraux du RPR et de l'UDF qui aboutissent à des conclusions radicales concernant l'immigration et l'islam. Le , il déclare : « le type de problème actuel auquel nous aurons à faire face se déplace de celui de l'immigration (« arrivée d’étrangers désireux de s'installer dans le pays ») vers celui de l'invasion (« action d’entrer, de se répandre soudainement », selon la définition donnée par "Littré") ». Il propose ainsi d'abandonner le droit du sol au profit du droit du sang en matière d'acquisition de la nationalité française.

Il s'affirme partisan d'une « troisième voie », située entre une Europe fédérale et une Europe des États. Après l'élargissement de 2004, il se prononce pour une intégration économique et financière accrue de douze États membres de l'Union européenne. À l'automne 2014, il appelle, avec l'ancien chancelier allemand Helmut Schmidt, à la construction d'un ensemble fédéré, comprenant dans un premier temps douze nations de l'Union européenne (France, Allemagne, Italie, Pays-Bas, Belgique, Luxembourg, Espagne, Portugal, Autriche, Pologne, Irlande, Finlande). Ce projet, qui prévoit la création d'institutions spécifiques, l'instauration d'un seul et même budget, une fiscalité et un droit du travail communs, ne demanderait selon lui aucune modification des traités européens. Il détaille ce projet dans son ouvrage "Europa, la dernière chance de l'Europe".



Il est élu sous l'étiquette CNIP en 1956, 1958 et 1962, puis RI en 1967, 1968 et 1973, et enfin sous l'étiquette UDF en 1984, 1986, 1988 (démissionnaire en 1989 pour siéger au Parlement européen), 1993 et 1997.




Valéry Giscard d'Estaing publie, au cours de son septennat, "Démocratie française", essai qui se vend à plus d'un million d'exemplaires. Après sa présidence, il se consacre beaucoup à la littérature. Il publie ses mémoires, intitulés "Le Pouvoir et la Vie", en trois tomes, en 1988, 1991 et 2006. Son premier roman, "Le Passage", paru en 1994, est un texte sentimental racontant l'aventure d'un notaire avec une jeune auto-stoppeuse.

L'ancien président de la République est élu à l'Académie française au premier tour de scrutin, le , au fauteuil numéro 16, laissé vacant par la mort de Léopold Sédar Senghor et précédemment occupé par Armand-Emmanuel du Plessis de Richelieu, Alexandre Ribot ou encore Charles Maurras. Valéry Giscard d'Estaing a obtenu 19 voix sur 34, Michel Tack ayant recueilli deux voix et Robert Pioche une seule. Il est reçu sous la Coupole le . Depuis décembre 2017, il en est le vice-doyen d'âge.

En septembre 2009, Valéry Giscard d'Estaing fait paraître un nouveau roman, "La Princesse et le Président", mettant en scène une relation sentimentale de deux personnages, qui rappellent Lady Diana et lui-même. L'opinion britannique relayée par la presse hésite entre « hilarité et curiosité » selon les termes du journal "The Guardian". Face aux interrogations, l'ancien président de la République affirme avoir « inventé les faits ». Le "Times" explique la parution de ce roman par une volonté d'éclipser la publication des mémoires de Jacques Chirac, son éternel rival politique à l'intérieur de la droite française.

En novembre 2010, sort en librairie "La Victoire de la Grande Armée", dans lequel il imagine triompher de l'armée russe, puis se retirer pacifiquement du pouvoir. Dans ce roman uchronique, l'empereur donne en septembre 1812 l'ordre à la Grande Armée de se retirer de Moscou pour contraindre à l'offensive le général russe Mikhaïl Koutouzov. À la suite de cette manœuvre, les Russes sont défaits par l'armée française, après quoi Napoléon abdique en faveur de son fils adoptif Eugène de Beauharnais et milite ardemment pour la paix en Europe. Pour Laurent Joffrin, il s'agit d'« une fiction réaliste, une invention vraisemblable qui donne un cours nouveau à l'histoire de France » ; selon lui, Valéry Giscard d'Estaing « connaît bien, de toute évidence, la geste napoléonienne, les mœurs d'une époque […], les souffrances de la campagne de Russie, les pensées de l'Empereur et ses habitudes de vie quotidienne jusque dans les détails ».

En octobre 2011 sort le roman "Mathilda", qui retrace le destin tragique de Mathilda Schloss, une Allemande vivant en Namibie.

Tout au long de sa carrière, Valéry Giscard d'Estaing fait part de son admiration pour Guy de Maupassant, allant même jusqu'à participer à un numéro de l'émission de télévision "Apostrophes" consacré à l'écrivain, le . Il fait régulièrement allusion au poème de Charles Baudelaire "Moesta et errabunda", surtout aux deux derniers couplets, qui évoquent « le vert paradis des amours enfantines ».



Valéry Giscard d'Estaing, son frère Olivier Giscard d'Estaing et leur cousin Philippe achètent à la commune d'Estaing le château d'Estaing, dans la haute vallée du Lot, en . Selon eux, l'achat répond à une logique de mécénat pour sa restauration et l'ouverture de plusieurs salles au public. Valéry Giscard d'Estaing précise qu'il envisage « la programmation de concerts, de rencontres et de conférences » et souhaite également « faire une place pour [ses] archives personnelles de président de la Convention européenne ». Certains regrettent un circuit de vente inélégant. En effet, la congrégation des religieuses de Saint-Joseph avait vendu le château à la commune en dessous du prix du marché, afin que celui-ci redevienne patrimoine public et aussi pour empêcher qu'il ne tombe entre les mains d'un acheteur étranger ; or, la commune d'Estaing l'a cédé cinq années plus tard aux Giscard d'Estaing, arguant . D'après le maire d'Estaing, Jean Pradalier, la municipalité est satisfaite de ne plus avoir à gérer ce monument, dont la gestion était lourde, et espère accroître son rayonnement par la célébrité de son propriétaire.

Grand amateur de chasse, Valéry Giscard d'Estaing participait notamment aux chasses présidentielles quand le général de Gaulle était chef de l'État. Il a pris part à de nombreux safaris en Afrique et sera même cité dans le "Rowland Ward", livre des records des grands chasseurs. On estime qu'il a tué une cinquantaine d'éléphants. En 1977, sur ce thème, Gilbert Laffaille a composé une chanson satirique : "Le Président et l'Éléphant".

Durant sa présidence, l'humoriste Thierry Le Luron imite et raille Valéry Giscard d'Estaing sur sa façon de parler et sur sa communication (dîners chez des Français, entretiens au coin du feu). Après son départ de l'Élysée, Valéry Giscard d'Estaing continue à être parodié par "Le Bébête Show" et "Les Guignols de l'Info".

Le président Giscard d'Estaing avait choisi comme emblème personnel un faisceau de licteur entouré d'une couronne de lauriers.

Le même symbole est reporté sur ses armoiries en tant que chevalier de l'ordre de l'Éléphant (Danemark) et chevalier de l'ordre du Séraphin (Suède)




Les archives de la présidence de Valéry Giscard d'Estaing (archives du secrétariat particulier, des collaborateurs du président et des services de l’Élysée comme le service photographique et le service du protocole) sont conservées aux Archives nationales, dans la série 5/AG(3).
Les archives de Valéry Giscard d'Estaing antérieures et postérieures à son septennat (archives privées, archives du ministre de l'Economie et des Finances, archives des campagnes électorales de 1974 et 1981, albums et tirages photographiques) sont également conservées aux Archives nationales, dans le fonds 500/AJ.







</doc>
<doc id="4938" url="https://fr.wikipedia.org/wiki?curid=4938" title="Guadalajara (Mexique)">
Guadalajara (Mexique)

Guadalajara est la capitale de l'État de Jalisco au Mexique. Elle est située à à l'ouest-nord-ouest de la capitale Mexico D. F. (District fédéral) Elle est située dans la vallée d'Atemajac, à au-dessus du niveau de la mer, à proximité du lac de Chapala.
La zone métropolitaine de Guadalajara, avec les municipalités Zapopan, Tlaquepaque, Tonalá, Tlajomulco de Zúñiga, Zapotlanejo et Ixtlahuacán del Río, forme une agglomération de plus de 4,1 millions d'habitants (2005).
Avec c'est la ville du Mexique qui occupe la plus grande surface, et c'est la seconde par le nombre d'habitants.
C'est le pôle économique et culturel de l'ouest du Mexique. C'est également une ville très touristique grâce notamment à son architecture coloniale.

Après la chute de l'Empire aztèque en 1521, Hernán Cortés charge le vainqueur Nuño Beltrán de Guzmán de conquérir les territoires à l'ouest de la Nouvelle-Espagne. Accompagné de 500 soldats espagnols et esclaves aztèques, emmenés pour servir de guides et de travailleurs dans les nouvelles mines découvertes, il rencontre au début peu de difficulté à vaincre les chefs des indiens Caxcanes.

Son capitaine Cristóbal de Oñate fonde une première fois Guadalajara, le 5 janvier 1532 à Nochistlán dans l'actuel État de Zacatecas. La ville est nommée ainsi en l'honneur de la cité natale de Beltrán de Guzmán, Guadalajara en Castille. L'objectif des Espagnols était de consolider leurs conquêtes et de pouvoir se défendre face aux attaques des autochtones.

Mais l'emplacement se révèle peu propice, et le 19 mai 1533, Nuño de Guzmán et Cristóbal de Oñate décident de déplacer l'établissement vers un lieu plus accessible avec un meilleur approvisionnement en eau, et moins de tempêtes de sable. La ville est refondée à Tonalá dans l'actuel Jalisco. Mais les autochtones empêchent Nuño de Guzmán, qui a reçu le titre de Marquis de la Vallée de Tonalá, de développer la colonie : il décide donc fin 1534 de la déplacer de nouveau.

Nuño de Guzmán fonde pour la troisième fois la ville à Tlacotán en 1535, mais le nouvel emplacement se révèle peu propice à l'agriculture et manque de ressources naturelles, qui se trouvent dans les montagnes. De plus les indiens Caxcanes s'y sont retranchés et s'en servent de base pour attaquer les Espagnols.

Les 63 Espagnols survivants finissent par trouver un emplacement protégé contre les attaques des autochtones dans la Vallée de Atemajac : le Cristóbal de Oñate et Nuño de Guzmán fondent définitivement Guadalajara à son emplacement actuel. En août la ville reçoit de Charles Quint le brevet royal lui accordant droit de cité et armes.

Le 10 décembre 1560 Guadalajara devient la capitale de la Nouvelle-Galice avec le déplacement de l'Audiencia royale et de l'évêché depuis Compostela, se transformant en la principale ville de l'ouest de la Nouvelle-Espagne, région qui sert alors de pont commercial entre l'Europe et l'Extrême-Orient. Les principales ressources de la région sont l'élevage et l'agriculture (plus que l'exploitation minière) avec la fondation de grandes haciendas.

La cité s'agrandit au , notamment grâce à l'annexion des villages de Mezquitán, Analco y Mexicaltzingo en 1667.
L'Université de Guadalajara est inaugurée le 3 novembre 1792 dans les anciens locaux du Colegio Santo Tomás. En 1793 la première presse typographique de la région y est installée. La loi proclamée par le roi réformateur Charles III le 4 décembre 1786 crée un système d'intendances pour la Nouvelle-Espagne, qui aboutit au remplacement de la Nouvelle-Galice par une Intendance de Guadalajara aux pouvoirs et au territoire réduits. Ce système perdurera avec quelques modifications jusqu'à l'indépendance au début du siècle suivant.

Guadalajara joue un rôle important durant la Guerre d'indépendance du Mexique puisque le chef des insurgés Miguel Hidalgo y Costilla y proclame l'abolition de l'esclavage et que le célèbre journal "El despertador americano" (Le réveil américain) y est publié. Le chef insurgé José Antonio Torres avait pris la ville le 11 novembre 1810. C'est également près de Guadalajara, à la bataille du pont de Calderón, que les insurgés d'Hidalgo et d'Ignacio Allende sont vaincus le 17 janvier 1811, entraînant la fin de la première phase de la Guerre d'indépendance.

Capitale de l'État du Jalisco, Guadalajara poursuit son expansion et renforce son rôle de métropole de l'ouest du Mexique au . Elle subit néanmoins les conflits qui agitent le pays à cette époque : durant la Guerre de Réforme, le président Benito Juárez transfère la capitale fédérale à Guadalajara et il y échappe à une tentative d'assassinat (grâce à une intervention passée à la postérité de Guillermo Prieto) qui aurait mis fin au camp libéral. Durant l'invasion française, le gouverneur de Guadalajara Pedro Ogazón y mène un mouvement de rébellion jusqu'à la victoire.

Durant le régime de Porfirio Díaz, la ville est reliée par le chemin de fer à Nogales, Manzanillo et Mexico. Le célèbre kiosque à la française du centre-ville est un cadeau fait par l'autocrate à la cité. La ville est peu touchée par la Révolution, les combats se dirigeant vers la ville de Mexico.

Le relief de la ville est plat mais elle est entourée par plusieurs collines dont le "Cerro del Coli " et le volcan de la Primavera, toujours en activité. La région est soumise à une activité sismique modérée.

Le climat est tempéré avec une température moyenne d'environ 18° sur l'année.

La ville est découpée en plusieurs secteurs:



Le centre historique se situe dans la Zone Centre de Guadalajara. C'est la partie la plus ancienne de la ville et compte avec des bâtiments coloniaux, du début du , de l'époque de l'intervention française et du gouvernement de Porfirio Díaz mais aussi des monuments modernes.

Certains quartiers historiques comme :

Dans cette zone se trouvent l'expiatorio en style néogothique, l'ancien rectorat de l'Université de Guadalajara, les temples de San José à Analco, de San Francisco, de Nuestra Señora de Aranzazú, de San Juan de Dios, de San Felipe, de Nuestra Señora del refugio.

La ville est aussi appelée "La ciudad de las Rosas" (La ville des Roses)
La Zona Metropolitana de Guadalajara comprend plusieurs municipalités : Zapopan, Tlaquepaque, Tonala, El Salto, et Tlajomulco de Zunica.

La population est à plus de 90% catholique. La ville est le siège de l'archidiocèse de Guadalajara (avec plus de cinq millions de baptisés) avec sa cathédrale Renaissance dédiée à l'Assomption.





Il y a actuellement (environ) quatre méga-projets en cours de développement à Guadalajara :




</doc>

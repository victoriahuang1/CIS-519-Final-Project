<doc id="19138" url="https://fr.wikipedia.org/wiki?curid=19138" title="Albert Uderzo">
Albert Uderzo

Albert Uderzo, né Alberto Aleandro Uderzo le à Fismes (Marne), est un dessinateur et scénariste de bande dessinée français. Il est le créateur, avec le scénariste René Goscinny, de la série "Astérix".

Albert Uderzo naît à Fismes au sein d'une famille d'immigrés italiens. Il est naturalisé français en 1934. C'est en lisant "Mickey Mouse", publié à l'époque dans "Le Petit Parisien", qu'il découvre la bande dessinée. Son frère Bruno (1920–2004), convaincu de son talent, le pousse à proposer ses services à un éditeur parisien ; c'est ainsi que, pendant la guerre, il est embauché comme par la Société parisienne d'édition (SPE). Engagé pour quelques semaines, il reste finalement un an à la SPE et y apprend les bases du métier : le lettrage, le calibrage d'un texte, la retouche d'image. Il côtoie notamment Calvo, qui fait figure pour lui de maître et l'encourage à persister dans le dessin. 

En 1941, il publie dans le supplément "Boum" de l'hebdomadaire "Junior", édité par la SPE, une illustration pastichant « Le Corbeau et le Renard » : c'est son premier dessin publié. 

En 1945, Uderzo travaille dans un studio d'animation qui produit un court-métrage, "Carbur et Clic-Clac" : déçu par l'expérience et par le résultat final, il décide de ne pas persévérer dans le dessin animé. Dans le même temps, le patron du studio, qui désire se lancer dans l'édition, lui confie l'illustration d'un fascicule en bande dessinée, "Flamberge, gentilhomme gascon" : cette aventure de cape et d'épée, dessinée dans un style humoristique, est la première histoire complète publiée par Uderzo. Le dessinateur considère, "a posteriori", ses débuts comme . 

En 1946, il remporte un concours qui lui permet de publier, aux Éditions du Chêne, un recueil de gags mettant en scène un nouveau personnage, "Clopinard" : grâce au salaire reçu à cette occasion, il réussit à convaincre son père qu'il peut espérer vivre de son métier de dessinateur. Dès la fin des années 1940, Uderzo travaille énormément : il illustre des romans et des pages d'actualité, et publie des histoires dans différents journaux, créant des personnages comme "Zidore l'homme macaque" (une parodie de Tarzan) ou "Clodo et son oie". Pour l'hebdomadaire "OK", il crée une série de héros à la musculature surdéveloppée, qui évoluent dans un Moyen Âge de fantaisie : d'abord le personnage de Arys Buck, puis de son fils le prince Rollin. Très influencé par la bande dessinée américaine, Uderzo signe à l'époque « Al Uderzo ». Arys Buck et Rollin ne vivent, l'un et l'autre, que le temps d'une unique aventure : "Belloy", qu'Uderzo crée dans la même veine, devient au contraire le héros d'une véritable série publiée dans "OK" puis dans d'autres journaux, l'auteur étant cette fois suffisamment satisfait de sa création pour s'y attacher.

"OK" cesse de paraître pendant qu'Uderzo fait son service militaire. Une fois démobilisé, le dessinateur doit proposer ses services à d'autres éditeurs. Ne retrouvant pas de travail dans l'édition pour la jeunesse, il tente sa chance du côté de la grande presse. Uderzo réalise ainsi des dessins de pages d'actualités dans "France Dimanche", pour lequel il réalise également des reportages qui consistent à prendre sur le vif des croquis d'évènements divers ; il collabore quotidiennement à "France-Soir" avec Paul Gordeaux en illustrant des bandes dessinées verticales "Le Crime ne paie pas" et "Les Amours célèbres". Il propose également ses services en Belgique, où paraissent de nombreux illustrés pour la jeunesse. Par l'intermédiaire d'une agence parisienne, Uderzo réalise pour un éditeur belge une aventure de "Capitaine Marvel Jr.", personnage d'une bande dessinée américaine dont il ne connaît pas grand-chose, et dont les Belges ont alors acheté aux Américains les droits pour l'Europe : l'histoire paraît en 1950 dans le journal belge "Bravo !".

En 1950, séjournant à Bruxelles pour y proposer ses dessins, il fait la connaissance de Victor Hubinon, Eddy Paape et Mitacq ainsi que du scénariste Jean-Michel Charlier, avec qui il réalise de nouvelles aventures de "Belloy". C'est à cette époque qu'il rencontre le scénariste avec lequel il travaille le plus durant sa carrière : René Goscinny. Leur première collaboration est une rubrique de savoir-vivre, publiée dans l'hebdomadaire féminin "Bonnes Soirées". Goscinny et Uderzo créent le personnage d"'Oumpah-Pah" le peau-rouge, dont ils tentent, sans succès, de vendre les aventures à des éditeurs américains. Le personnage ne trouve d'acquéreur ni aux États-Unis ni en France et ce n'est que plusieurs années plus tard qu'il est publié, dans une version remise à jour, dans les pages de "Tintin". Les deux auteurs publient ensuite dans "Junior" les séries "Jehan Pistolet" et "Luc Junior".

En 1955, avec Jean-Michel Charlier, Goscinny et Jean Hébrard, Uderzo fonde un syndicat, scindé en deux agences distinctes : Edifrance et Edipresse. Il continue de produire des planches en abondance : "Benjamin et Benjamine" et "Bill Blanchard" (avec Goscinny), pour "La Libre junior", "Clairette" (avec Charlier), "La Diligence de Santa-Fe" (Une aventure de Jim Flokers, avec Jean-Michel Charlier, en 1957)... Les auteurs livrent beaucoup de travaux de commande. Uderzo explique par la suite : .
En 1957, Goscinny, désormais bien introduit à "Tintin" comme scénariste polyvalent, recommande Uderzo à la direction de ce journal. "Poussin et Poussif", éphémère série dont les deux compères publient en 1957 trois épisodes dans "Tintin", permet au duo de passer à la vitesse supérieure : le rédacteur en chef André Fernez apprécie tant les planches qu'il demande à Goscinny de mettre en chantier une histoire à suivre, . Goscinny et Uderzo en profitent pour reproposer leur "Oumpah-Pah" qui devient, en 1958, leur première grande série en commun.

Uderzo s'installe en janvier 1958 dans une HLM neuve au du 3, rue Rameau à Bobigny (actuelle Seine-Saint-Denis), où une plaque a été inaugurée en octobre 2009 pour fêter le cinquantenaire de l'invention d'Astérix. Coïncidence : la rue Rameau est proche de la rue d'Alesia où se trouve une grande nécropole gauloise, alors que les Uderzo habitaient auparavant chaussée Jules César à Eaubonne.

En 1959, Uderzo participe au lancement de "Pilote" en illustrant deux de ses séries-phares : "Tanguy et Laverdure", série d'aviation réaliste scénarisée par Charlier et "Astérix", série humoristique scénarisée par Goscinny. La première histoire de cette nouvelle série, "Astérix le Gaulois", paraît à partir d'octobre 1959 dans "Pilote", avant d'être publiée en album en 1961. "Astérix" attire un nombre croissant de lecteurs, devenant bientôt l'un des plus importants succès de la bande dessinée francophone. Au cours des années 1960, Uderzo abandonne progressivement toutes ses autres séries, pour se concentrer exclusivement à sa principale création. La charge de travail demandée par "Pilote" et "Astérix" étant de plus en plus importante, Goscinny et lui finissent par renoncer en 1962 à leur collaboration à "Tintin", abandonnant "Oumpah-Pah". Uderzo cesse ensuite en 1968 d'illustrer "Tanguy et Laverdure" : Jijé lui succède au dessin de cette dernière série. 

En 1967, Uderzo déménage à Neuilly-sur-Seine. Avec Goscinny, tout en réalisant environ une histoire par an, il supervise attentivement le développement des produits dérivés (des figurines en latex aux spin-offs "Idéfix"). En 1974, les deux auteurs créent les studios Idéfix, afin de contrôler l'adaptation en dessin animé des aventures du Gaulois.

La mort de Goscinny, en 1977, bouleverse profondément Uderzo (il dira plus tard qu'il est resté assis 24 heures ou 48 heures après avoir appris la nouvelle) qui décide pourtant de poursuivre "Astérix". Son premier album réalisé en solo, "Le Grand Fossé", paraît en 1980 chez Albert René, maison d'édition qu'il crée à la suite d'un contentieux avec Dargaud et qui a longtemps été dirigée par sa fille, Sylvie Uderzo (jusqu'en 2007).

Dès le départ, Uderzo se heurte à des critiques bien que ses premiers scénarios se rapprochent de ceux de Goscinny, en conservant ce ton et cet humour propres à la série ; la qualité est plus contestée autour des années 2000. Le public, lui, continue de plébisciter la série, dont les derniers albums parus sont les plus gros tirages de l'histoire de la bande dessinée européenne.

Dans les années 1990, Uderzo et les ayants droit de Goscinny se lancent dans une procédure judiciaire contre les éditions Dargaud, toujours éditrice des premiers albums d'Astérix, l'enjeu principal étant l'attribution des recettes réalisées par la vente des albums à l'étranger. 

En 1998, Dargaud perd définitivement les droits sur les 24 premiers albums d'Astérix. Uderzo confie les droits à Hachette, diffuseur-distributeur des Éditions Albert René. 
À compter des années 2000, un litige oppose Uderzo à sa fille Sylvie, gérante et co-actionnaire d'Albert René ; l'auteur retire finalement à sa fille la gestion de la société éditrice. En 2008, Uderzo et Anne Goscinny vendent leurs parts d'Albert René à Hachette, qui acquiert ainsi le contrôle de l'exploitation de l'univers d'Astérix. Uderzo déclare avoir voulu garantir l'avenir de l’œuvre créée avec Goscinny, en la confiant à un gestionnaire neutre. Un long litige judiciaire autour du patrimoine d'Astérix oppose par ailleurs Uderzo à sa fille Sylvie et à l'époux de cette dernière.

Étant un ami de longue date du dessinateur Cabu, Uderzo est sorti de sa retraite pour réaliser un dessin, rendant hommage aux victimes de l'attentat contre "Charlie Hebdo" (et notamment à ses confrères dessinateurs), dans lequel Astérix, tout en déclarant : « Moi aussi je suis un Charlie ! », propulse hors de la page, d'un coup de poing, un ennemi auquel Uderzo, à la place des cothurnes, a dessiné des babouches. À la suite du drame, Uderzo a aussi publié, par l'intermédiaire des réseaux sociaux, un dessin inédit, initialement réalisé à la suite de la disparition de René Goscinny en 1977 mais qui n'avait alors jamais été diffusé (il est d'ailleurs toujours resté à l'état de crayonné), montrant Astérix et Obélix en deuil. Il a par ailleurs proposé à la vente l'une des planches originales de l'album "Les Lauriers de César" pour soutenir les familles des victimes.

En 2013, après une forte douleur à la main, ne pouvant plus dessiner, il passe le relais à Didier Conrad pour le dessin et à Jean-Yves Ferry pour le scénario d"'Astérix".

Capable de dessiner dans des styles très différents (du réalisme de Tanguy et Laverdure au semi-réalisme d'Astérix), son grand sens du gag visuel complétait parfaitement les talents d'humoriste de René Goscinny.

À l'instar de Franquin, ses personnages sont très expressifs et dotés d'une gestuelle travaillée qui vient en partie de l'intérêt d'Uderzo pour le dessin d'animation. Il reste une référence incontournable pour les dessinateurs et animateurs actuels comme Juanjo Guarnido (Blacksad).

Il a réalisé les dessins du film "L'Avare", sorti en 1980, avec pour vedette Louis de Funès. En 1996, vingt-six auteurs lui rendent hommage dans "Uderzo croqué par ses amis".

Il a donné son nom à un prix de bande dessinée : les Prix Albert-Uderzo.

Albert Uderzo est daltonien (il ne distingue pas le rouge et le vert). La mise en couleurs des albums d'Astérix a été assurée pendant plusieurs années par son frère Marcel Uderzo. Il est né avec douze doigts, six à chaque main, et a été opéré. Avec le temps, des problèmes d'articulations aux mains ne lui permettent plus d'encrer ses planches lui-même, et l'obligent à confier ce travail à des assistants.







</doc>
<doc id="19143" url="https://fr.wikipedia.org/wiki?curid=19143" title="Fulgencio Batista">
Fulgencio Batista

Rubén Fulgencio Batista y Zaldívar (né le à Banes et mort le à Guadalmina, station balnéaire de Marbella en Espagne), est un militaire et homme d’État cubain, d'abord éminence grise de la junte militaire qui dirigea Cuba de 1933 à 1940. Il est élu à la présidence de la République de Cuba en 1940. Il ne se représente pas en 1944, mais le candidat qu'il soutient est battu par Ramón Grau San Martín. En novembre 1948 il est élu sénateur. Il revient au pouvoir à la suite d'un coup d'État en 1952 puis il est élu sans opposition comme chef d'État en 1954.

Il est chassé du pouvoir et du pays en 1959 lors de la révolution cubaine dirigée par Fidel Castro.

Fulgencio Batista y Zaldívar naît à Banes dans la province de Holguín à Cuba le , moins de 3 ans après que sa terre natale fut libérée de l'emprise espagnole et moins de deux ans avant qu'elle devienne une république indépendante. Son père, Belisario Batista Palerma, était un paysan. Ses deux parents sont morts avant qu'il n'atteigne l'âge de 13 ans.

Il quitta alors l'école pour devenir apprenti couturier. Fulgencio travailla dans des productions de cannes à sucre pour ensuite étudier dans le but de devenir barbier. Finalement, il s'engagea dans l'armée à l'âge de 20 ans.

Alors que Fulgencio n'était qu'un simple sergent de l'armée, il joua un rôle décisif dans « la révolte des sergents » de 1933, conséquence d'un profond mécontentement existant dans l’armée concernant leurs salaires, leur évolution de carrière et leurs conditions matérielles. La conspiration militaire menée par le sergent Pablo Rodriguez, concomitante à une forte mobilisation étudiante, débouche sur le renversement du gouvernement dictatorial de Gerardo Machado et la mise en place d'un gouvernement dit « » mené par Ramón Grau San Martín. Celui-ci propulse une série de réformes d'orientation nationaliste et sociale, comprenant la réduction du temps de travail, le droit de vote des femmes, des nationalisations d’intérêts économiques étrangers et l’affirmation de la souveraineté cubaine.

Heurtés par ces mesures, les États-Unis rejettent la légitimité du gouvernement cubain, auquel ils ne concèdent aucune reconnaissance diplomatique, et flattent les ambitions individuelles de certains militaires haut-gradés, parmi lesquels Batista, devenu colonel. Le , une junte militaire présidée par Batista renverse le gouvernement provisoire et met en place un régime pro-américain. Si Batista n'a pas de fonctions dans la hiérarchie organique du gouvernement, il est, en tant que chef d'état-major, l'éminence grise des présidents successifs et Miguel Mariano Gomez et le véritable maître de Cuba. Le journaliste Pablo de la Torriente Brau, membre de l’« Aile gauche étudiante » écrivait de lui à un ami, en 1936 : .

Les syndicats cubains organisent en mars 1935 une importante grève générale avec le soutien du Parti communiste et du Parti authentique pour obtenir le renversement du régime. Les transports et l’économie sont bloqués pendant plus de deux jours mais la grève finit désarticulée par la répression : plusieurs grévistes sont tués, la plupart des syndicats dissous et des figures de l'opposition partent en exil. En mai, Antonio Guiteras, ancien ministre sous le Gouvernement des Cent Jours et dirigeant de l'organisation "Jeune Cuba" est assassiné par des agents du gouvernement.

Il introduit une constitution modelée sur celle des États-Unis et, candidat d'une coalition hétéroclite mêlant sociaux-démocrates, conservateurs et le parti communiste cubain (qui pour ce dernier ne le considérait temporairement que comme un allié des pays en guerre contre l’Allemagne hitlérienne), il est élu lui-même Président de la République de Cuba le face à Ramón Grau San Martín du Parti révolutionnaire cubain. Il appela successivement deux communistes au gouvernement mais sans portefeuille attribué, d'abord le poète Juan Marinello et quand ce dernier se présente à des élections sénatoriales, il est remplacé par Carlos Rafael Rodríguez, un des futurs responsables du pouvoir castriste. Durant ces quatre ans de présidence, il respecte les règles démocratiques.

Il permit aux États-Unis d’utiliser les espaces aérien, maritime et terrestre de Cuba, de disposer de plusieurs bases aériennes et navales à usage exclusif durant la Seconde Guerre mondiale, sans traitement de réciprocité. En 1941, Cuba déclare la guerre à l’Allemagne.

En 1944, Fulgencio Batista ne se représente pas et Carlos Salagrinas, le candidat qu'il soutient, est battu par Ramón Grau San Martín. Par une lettre publique, les communistes cubains saluèrent l’« œuvre » de Batista, lorsqu’il quitta la présidence en 1944 : .

Il opère, durant la période précédant son retrait du pouvoir, un raid systématique sur les ressources financières de l’État, reconnait dans un rapport l’ambassadeur américain Spruille Braden.

Fulgencio Batista se retire en Floride aux États-Unis de 1945 à 1949. Ce serait le chef mafieux Meyer Lansky, qu'il connaissait depuis les années 1930, qui l'aurait sorti de sa retraite à Miami et financé son retour en politique. Le , il est élu sénateur de Las Villas une ville et une province du centre. En 1952, il se présente à l’élection présidentielle mais les sondages alors publiés lui sont nettement défavorables, le plaçant derrière (candidat du Parti orthodoxe et favori de l’élection) et du parti de l'ancien président Grau San Martín. La CIA organise des actions de déstabilisations politiques dans l'ile afin d'instaurer un climat propice à un coup d’État. Celui-ci a lieu le , soutenu par une frange de l'armée à laquelle Batista promet des augmentations salariales. Revenu au pouvoir, il fait quintupler le salaire présidentiel, suspend la constitution, et interdit le droit de grève. Un « "Bureau de répression des activités communistes" » est instauré pour consolider l'autorité de son régime. Pierre Rigoulot affirme que , certifiant que ce dernier .

Pendant deux ans, il est le président par intérim. Si le nouveau gouvernement est rapidement reconnu par plusieurs pays, dont les États-Unis, il est cependant contesté à l'intérieur. Ainsi, le , des rebelles menés par un avocat, Fidel Castro, tentent sans succès de prendre d'assaut la caserne de Moncada à Santiago de Cuba pour entraîner une insurrection. Trois partisans de Castro meurent au combat, 68 autres sont capturés et exécutés sommairement. Castro est lui-même arrêté par un groupe de soldats mais leur sergent désobéit à ses instructions et remet ses prisonniers aux autorités judiciaires, ils bénéficient ensuite d'une intervention de l'archevêque de Santiago qui leur évite la peine de mort. Deux ans plus tard, il est amnistié par Batista et exilé.

En 1954, Batista se fait élire président de la République sans opposition après le retrait de l'ex-président Ramon Grau San Martin qui appelle au boycott, pour protester contre la corruption du régime. Pourtant, l'influence des États-Unis est telle que leur ambassadeur est considéré comme la deuxième personnalité la plus importante du régime.

Le jeu et la prostitution, contrôlés par les gangs nord-américains et notamment la Mafia (grâce aux relations entre Batista et les parrains mafieux Meyer Lansky et Lucky Luciano), se développent. Batista négocie avec Cosa nostra la construction de casinos et d'hôtels de luxe par l’État cubain, mais administrés exclusivement par la mafia. L'aéroport militaire de La Havane est utilisé comme plate-forme de transit pour le trafic de drogue grâce à l’emplacement stratégique de Cuba, et pour alimenter en cocaïne et héroïne les clubs de la capitale. Ces activités procurent au régime des recettes considérables et Batista et ses proches en tirent des bénéfices personnels. La capitale cubaine accueille même en décembre 1946 une conférence au sommet des chefs mafieux nord-américains qui rassemble plus d'un millier de participants et constitue la plus importante du genre.

Selon le journaliste du "Washington Post" Karl E. Meyer, La Havane devient « une sorte de bordel pour les Nord-américains ». L'historien Arthur Schlesinger évoque également une ville . L'universitaire relativise cette analyse considérant que l'importance économique que les États-Unis accordaient aux casinos et au tourisme était exagérée en raison d'une perception coloniale de Cuba.

Castro revient à Cuba dès décembre 1956 et reprend ses activités révolutionnaires pour déposer Batista, soutenu par une partie croissante de la population, notamment dans la province d’Oriente. La répression est brutale : , selon le correspondant du "Chicago Tribune". Le 8 avril 1958, une grève générale est déclenchée dans la capitale mais, peu suivie en raison de son caractère improvisé, est facilement écrasée. De 150 à 200 grévistes sont tués et des centaines d'autres arrêtés. Dans les campagnes, des cadavres de guérilleros torturés sont accrochés aux arbres par les troupes gouvernementales.

En mai 1958, Batista lance hommes contre la guérilla castriste lors de l’ qui échoue trois mois plus tard. Castro mène alors une contre-offensive qui débouche sur une guerre civile le long de la Sierra Maestra dans l’est jusqu’au centre du pays, empêchant la récolte saisonnière de la canne à sucre. Le , une partie de la population de Santa Clara apporte son aide aux guérilleros. Dans les jours qui suivent, ces derniers s'emparent de grandes quantités d’armes et des points stratégiques de la ville.

Le , Fulgencio Batista s'enfuit à Saint-Domingue avec ses proches et 40 millions de dollars. Son départ est suivi par l'entrée à La Havane de quelques milliers de guérilleros partisans de Fidel Castro. Un nouveau président, Manuel Urrutia, est nommé ; Fidel Castro devient commandant en chef de l'armée puis Premier ministre le . La chute du régime n'entraine pas de violences comparables à celles qui avaient suivi le renversement du dictateur Gerardo Machado en 1933, ou de Marcos Pérez Jiménez qui venait de se produire, en 1958, au Venezuela. À La Havane cependant, il y a pillage et destruction des parcmètres, dont il était de notoriété que les revenus allaient non à l’État mais à l'épouse de Batista.

Son régime aurait causé selon les États-Unis la mort de personnes en sept ans. Selon Jeannine Verdès-Leroux, auteur de "La Lune et le Caudillo. Le rêve des intellectuels et le régime cubain (1959-1971)", . Madame Verdès-Leroux déclare avoir tenté une évaluation en utilisant des sources publiées au tout début du castrisme, en les confrontant à la masse des chiffres donnés lors de cérémonies pour les victimes, et n’atteint pas le total de 2 000. De plus, selon elle, ces morts avaient toutes les origines : policiers et militaires tués par des opposants, opposants à Batista, militants politiques pro-Batista ou personnes accusées de trahison, assassinées par le mouvement du 26 Juillet (M-26) de Castro, etc..

Batista passa le reste de sa vie en exil, d'abord au Portugal, puis en Espagne à Guadalmina une station balnéaire de Marbella où il mourut le d'une crise cardiaque.






</doc>
<doc id="19149" url="https://fr.wikipedia.org/wiki?curid=19149" title="Alphabet de l'espéranto">
Alphabet de l'espéranto

L’alphabet de l’espéranto est constitué de 28 lettres, dont 22 proviennent directement de l’alphabet latin de base (q, w, x et y ne sont pas utilisées), et les 6 autres (ĉ, ĝ, ĥ, ĵ, ŝ, ŭ), également issues de cette base, sont agrémentées d’un accent. Chacune de ces 28 lettres, accentuée ou non, est une lettre à part entière, ayant sa place dans l'ordre alphabétique et son entrée dans les dictionnaires. L’orthographe est parfaitement phonologique : chaque lettre représente invariablement un seul phonème.

Les lettres de l’espéranto sont identiques à celles de l’alphabet phonétique international, à l’exception des lettres "c" , "ĉ" , "ĝ" , "ĥ" , "ĵ" , ŝ et "ŭ" . L’alphabet au complet est :

L’orthographe de l’espéranto est entièrement phonémique, c’est-à-dire qu’une lettre correspond à un seul son.

Le nom de chaque voyelle est simplement constitué de la voyelle elle-même : "a", "e", etc. Le nom de la consonne s'obtient simplement en ajoutant un "o" à celle-ci : "bo", "co", … "ŭo", "zo".

Le "ŭ" est une semi-voyelle surtout employée comme deuxième membre d'une diphtongue "aŭ" et "eŭ". 

Les lettres "q", "w", "x" et "y" ne sont pas utilisées en espéranto, sauf dans les expressions mathématiques et les noms étrangers. Dans ce cas, leurs noms se prononcent :

L'existence des lettres accentuées (le "ĝ" en particulier) permet de rattacher graphiquement un mot à plusieurs langues européennes. Par exemple, le mot allemand « Ingenieur » se prononce avec un g dur (transcrite par [ɡ] dans l'Alphabet phonétique international), alors que la même lettre du mot français « ingénieur » se prononce [ʒ] et qu'en anglais, dans "engineer", elle se prononce [d͡ʒ]. Si l'on veut que la prononciation de la lettre g soit toujours la même en espéranto (ce qui est très important pour la facilité de mémorisation des mots), il faut nécessairement choisir l'une ou l'autre des prononciations existantes dans les langues vernaculaires (ou naturelles). La solution choisie en espéranto a été de créer plusieurs versions d'une même lettre en les distinguant par un symbole particulier : La langue dispose donc de la lettre g qui se prononce [g] et de la lettre ĝ qui se prononce [d͡ʒ]. Cela permet une plus grande richesse de sons dans la langue, tout en laissant une écriture relativement proche de celles des langues vernaculaires. Ainsi, le mot « ingénieur » s'écrit « inĝeniero » et se prononce [ind͡ʒeni'ero].

En plus de leur rôle premier de transcription, les lettres diacritées visent à rappeler en espéranto l’orthographe ou la prononciation de plusieurs langues européennes. Par exemple, "poŝto" (« poste »), rappelle graphiquement et phonétiquement le mot "pošta" du tchèque, du slovaque, du slovène, du serbo-croate, mais aussi par la graphie les mots français, anglais, allemand "poste", "post", "Post", et par le son le bulgare поща ("pošta", prononcé ['pɔʃtɐ]). L'espéranto aboutit souvent ainsi à un compromis rappelant plusieurs langues sources : ainsi "ĝardeno" [d͡ʒar'deno] rappelle le français "jardin", l'allemand "Garten" et l'anglais "garden".

Les lettres accentuées (appelées en espéranto , « lettres à chapeau ») ont été introduites par Zamenhof afin d’éviter les digrammes. Ces lettres sont critiquées par certains opposants de l'espéranto, notamment les locuteurs de l’ido.

Le quasi-pangramme (« changement d’écho tous les jeudis »), qui contient les six lettres accentuées, est souvent employé pour tester l’affichage des caractères spéciaux de l’espéranto.

En typographie, si l'on ne dispose pas des lettres accentuées de l'espéranto, il faut utiliser des substitutions. Il n'est pas acceptable d'omettre complètement les accents sans distinguer des lettres normalement différentes. Pour cela on dispose de deux principaux systèmes :

Il existe quelques systèmes moins utilisés, qui remplacent les accents circonflexes par des apostrophes ou un accent circonflexe avant ou après la lettre ("c", "c^", "^c"…).

La méthode de substitution suggérée dès 1889 dans "La Esperantisto" par Ludwik Lejzer Zamenhof, l'initiateur de la langue, était de remplacer les lettres diacritées par des digrammes composés de la lettre de base suivie d'un "h", les éventuelles ambiguïtés étant levées par l'ajout d'un tiret entre les monèmes.

Au cours du , pour la commodité de certains traitements informatiques, le "h" a parfois été remplacé par un "x" pour permettre un remplacement automatique postérieur.

Avec la généralisation désormais de l'Unicode on peut écrire de plus en plus facilement, directement par ordinateur, les lettres accentuées de l'espéranto. L'usage des substitutions commence ainsi à disparaître.
En HTML, ces codes précédés de codice_1 et suivis d'un point-virgule génèrent les lettres accentuées de l'espéranto ; en Visual Basic (utilisé pour les macros de Microsoft Word), ces codes s'utilisent comme argument de codice_2, en OpenOffice.org Basic, ces codes s'utilisent comme argument de codice_3, etc.

Par exemple : "Ŝ" s'obtient par codice_4 en HTML, par codice_5 en Visual Basic, codice_6 en OpenOffice.org Basic, etc.

Certaines dispositions de clavier telles que le BÉPO permettent de saisir directement les lettres accentuées de l'espéranto, tout comme la touche compose de GNU/Linux. Il existe également des programmes tels que EK qui permettent d'ajouter les diacritiques au cours de la frappe.

Comme pour toutes les langues utilisant des lettres diacritées, le braille dispose d'une transcription adaptée à l'espéranto.
Signuno est une adaptation à l'espéranto de la langue des signes internationale pour les Sourds.

Il existe plusieurs convertisseurs en ligne permettant de passer du système X à Unicode et inversement, voire de convertir d’autres systèmes (H, apostrophes, etc.).

La ponctuation en espéranto n’est pas définie de manière précise, et son usage peut varier selon les auteurs. Les règles suivantes sont généralement observées :

Pour les nombres, le séparateur décimal est généralement la virgule, et le séparateur de milliers est souvent le point ou l’espace. Les chiffres romains sont parfois utilisés, mais certains grammairiens (comme Bertilo Wennergren) recommandent de les éviter, parce qu’ils ne sont pas connus dans le monde entier.

L’usage des capitales est semblable au français : les majuscules indiquent le début des phrases et les noms propres. Les abréviations sont aussi écrites en majuscules (par exemple ). Le pronom (« vous ») est parfois écrit avec une majuscule dans les lettres en signe de respect, mais cet usage tend à disparaître.

Les mots dérivés des noms propres peuvent être écrits avec ou sans majuscule, même si l’usage des minuscules est plus fréquent : (« Pays-Bas ») → ou (« néerlandais »). Les noms des mois et des jours s’écrivent habituellement en minuscules.

Puisque les espérantophones proviennent de régions du monde où les normes d’écriture sont différentes, les noms de famille des personnes sont souvent écrits entièrement en majuscules ; c’est d’ailleurs l’usage qui prime sur Wikipédia espérantophone. Cela lève l’ambiguïté qui pourrait avoir lieu à cause de l’ordre « NOM Prénom » (Japon notamment) ou « Prénom NOM » (comme en France).




</doc>
<doc id="19157" url="https://fr.wikipedia.org/wiki?curid=19157" title="Ashoka">
Ashoka

Ashoka ou Açoka (sanskrit : अशोकः, IAST : Aśoka ; né v. 304 av. J.-C. mort en 232 av. J.-C.) est le troisième empereur de la dynastie indienne des Maurya.

Il accède au pouvoir en 273 av. J.-C. et s'efforce tout d'abord de consolider et d'agrandir l'empire hérité de son père Bindusara. À la suite de la conquête meurtrière du Kalinga, il adopte les principes non violents (ahimsa) du bouddhisme. Dès lors l'empire n'est plus troublé par la guerre et, en souverain pacifique, il s'emploie à l'organiser grâce à un corps important de fonctionnaires et une police efficace ainsi qu'au travers d'édits gravés sur des rochers ou des colonnes dispersés dans tout le pays. Il interdit les sacrifices, promeut le végétarisme et encourage la diffusion du bouddhisme en Inde et dans toute l'Asie.

L'empire d'Ashoka s'étend de l'actuel Afghanistan jusqu'au Bengale et aussi loin vers le sud que le plateau de Mysore mais il ne lui survit guère, s'effondrant en cinquante ans. Cependant, Ashoka est le premier souverain à réaliser l'unité de l'Inde sur un aussi vaste territoire et, consciente de son apport dans l'histoire nationale, la République indienne a emprunté au chapiteau qui surmonte le « pilier d'Ashoka » de Sarnath son emblème national aux lions et la roue du "dharma" qui figure sur son drapeau.

Chandragupta (« Protégé par la Lune ») fonde l'empire Maurya en s'emparant tout d'abord du Punjab récemment conquis par Alexandre le Grand puis, il se tourne vers l'est et s'approprie le royaume des Nanda vers 313 , s'installant dans la capitale Pataliputra (actuelle Patna au Bihar). En 305 , il affronte et défait Séleucos Nikator, le satrape macédonien de Babylone qui le menace au nord-ouest. Les deux souverains concluent un accord aux termes duquel Chandragupta obtient la région à l'est des montagnes d'Afghanistan contre des éléphants et une alliance matrimoniale. Après avoir étendu son empire vers le sud, Chandragupta se serait retiré dans un monastère jain, abdiquant en faveur de son fils Bindusara.

Ce dernier nomme vice-roi d'Ujjain l'un de ses fils, Devanampiya Piyadassi (« Le roi ami des dieux au regard bienveillant »), plus connu sous le nom d'Ashoka (« Le sans douleur ») donné par les bouddhistes. Il l'envoie ensuite mater une révolte à Taxila dont il devient également vice-roi, s'initiant ainsi au gouvernement. À la mort de son père en 273 , Ashoka fait éliminer tous ses frères et sœurs, s'empare du pouvoir et se fait couronner quatre ans plus tard.

Comme ses prédécesseurs, Ashoka dispose d'une armée considérable, professionnelle, prête en permanence et qu'il finance lui-même. C'est avec elle, au cours de la treizième année de son règne (261 ), qu'il se lance à la conquête du Kalinga, région située sur la côte est et correspondant à l'actuel Orissa. L'empereur remporte la victoire après une guerre terriblement meurtrière qui aurait fait selon ses dires , et autant de morts de famine et de maladie, chiffres probablement symboliques qui traduisent l'ampleur des massacres perpétrés.

Ce triomphe sanglant provoque chez le souverain une crise morale et politique qui le conduit à adopter les principes non-violents du bouddhisme. Il fait une retraite d'un an dans un monastère, devient végétarien, fait des pèlerinages et de nombreux dons aux bouddhistes mais aussi aux jaïns et aux brahmanes. Il prend les vœux d'upāsaka et réalise au travers des enseignements du Bouddha que ses conquêtes territoriales sont sources de souffrance, et s'oriente vers une conquête intérieure pour cultiver le bien de la société et la non-violence. Il protège les autres religions et érige des piliers où sont gravés en plusieurs langues des textes promouvant la justice et la tolérance.

Débarrassé du souci de la guerre, Ashoka se consacre à l'organisation de l'Empire sur lequel il exerce un pouvoir absolu au moyen d'une administration décentralisée, soutenue par les nombreux édits gravés dans la pierre et dispersés sur tout le territoire.

Les ordres de l'empereur s'appliquent uniformément à tout le territoire, ils émanent de sa volonté et de son autorité personnelle comme en témoigne une de ses inscriptions : 

De plus, bien qu'il ne l'utilise guère après la conquête du Kalinga, Ashoka peut compter sur une armée imposante entièrement à son service car il en est le financeur.

Afin de diffuser son idéal de tolérance, Ashoka promulgue des édits qu'il fait graver dans la pierre et ériger dans tout l'Empire. Nous en connaissons une trentaine, gravés soit sur des rochers, situés à la périphérie du royaume, soit sur des colonnes, dans la vallée du Gange ou plus rarement sur les parois de grottes dans des régions reculées. Ces textes édifiants permettent au souverain non seulement d'éduquer ses sujets en leur inculquant le sens du devoir ("dharma") mais également de conforter son gouvernement par la pression qu'ils exercent sur la population, l'incitant à se plier à la justice, à renoncer à la violence donc à la guerre, à s'imposer une forme d'autodiscipline.

Pour assurer pleinement l'application de ces principes, Ashoka qui se considère comme le « père » de tous ses sujets quelles que soient leur religion ou leur caste, crée un corps de superviseurs de la moralité, le "dhamma-mahamatra", qui, renseigné par de nombreux informateurs, contrôle l'intégrité de ses fonctionnaires et l'observation par tous de ses édits. Ceux-ci constituent un code moral aussi bien acceptable par les pratiquants du bouddhisme, qui en est l'inspiration directe, que par ceux du jaïnisme ou de l'hindouisme.

Cependant, la taille de l'Empire et les moyens de communication de l'époque rendent un contrôle direct impossible ; en effet il faut au moins trente jours pour se rendre de Pataliputra à Kandahar ou à la frontière sud et le double en période de mousson. Le royaume est donc divisé en plusieurs territoires, dont le nombre exact est inconnu, dirigés par des vice-rois ou des gouverneurs. Ceux-ci disposent d'une bureaucratie hétérogène, parfois locale, parfois héritée des précédentes autorités perses ou grecques au Nord-Ouest, ou encore directement issue du cœur de l'Empire dans les régions jugées peu sûres du sud et de l'est.

Par ailleurs, certains souverains vaincus, pourvu qu'ils paient tribut et se conforment au "dharma", conservent leurs trônes. Il en est de même pour plusieurs tribus qui gardent une certaine autonomie dans leur organisation interne. Ceci se manifeste par la présence de différents monnayages malgré une tendance à l'unification dans ce domaine. Ces différentes structures se chargent de la gestion concrète de l'Empire — collecte des impôts, réalisation de grands travaux — les édits d'Ashoka, uniquement préoccupé de principes moraux, restant muets à ce sujet.

Sous le règne d'Ashoka l'Inde est un pays prospère. Les travaux d'irrigation permettent l'augmentation de la production de riz et de coton dont une partie est exportée, de même que des épices (poivre et cannelle), des pierres précieuses (cornaline, rubis, saphir) ou des éléphants. Cela lui permet de faire venir de la soie de Chine, des chevaux d'Asie centrale et de l'or dont on a retrouvé de grandes quantités alors que le pays n'en produit pas.

Le commerce est stimulé par une amorce d'unité monétaire et l'amélioration des routes le long desquelles l'empereur fait creuser des puits et planter des arbres. Ashoka est également à l'initiative de la construction de la « Grande voie royale », longue de , qui relie Taxila à Pataliputra. Cette dernière est une ville et un port fluvial florissant qui compte ce qui en fait l'une des plus grandes cités du monde antique.

Les dons aux nécessiteux ou aux temples et l'entretien des hôpitaux (pour les personnes et les animaux), des routes et surtout d'une immense armée, se font grâce à des taxes prélevées sur les produits agricoles et le commerce.

Ashoka traite également toutes les religions, faisant et recommandant de faire des dons aux moines bouddhistes, jaïns aussi bien qu'aux brahmanes. Cette attitude impressionne durablement les souverains indiens qui pendant longtemps font des dons aux différentes religions présentes dans le pays. Il promeut la non-violence (ahimsa) dans ses édits et dans sa façon de vivre, incitant au végétarisme, interdisant les sacrifices et la torture. Il limite l'abattage des animaux à la cour et remplace la chasse par des pèlerinages. Il encourage la diffusion du bouddhisme pour lequel il convoque un concile dans la capitale et aurait fait édifier dont ceux de Sarnath et Sanchi largement remaniés au cours des siècles suivants. 

La prospérité économique, les échanges et la paix favorisent l'essor culturel. Le règne d'Ashoka voit l'apparition de l'épigraphie et d'une sculpture sur pierre de grande qualité influencée par l'art grec et perse. Elle se caractérise par sa sobriété, des motifs animaliers et des chapiteaux en corolle de lotus renversée comme en témoignent les colonnes aux chapiteaux aux lions retrouvées à Sarnath et Vaishali.

À la mort d'Ashoka en 232 , ses fils se disputent l'Empire ; aucun d'entre eux n'a l'envergure de leurs ancêtres et progressivement les gouverneurs s'émancipent de l'autorité centrale. L'armée, immense mais inactive depuis des décennies, ne réussit pas à éviter la conquête du Punjab par les Gréco-Bactriens en 185 . Quelques années plus tard, le dernier empereur maurya, Brihadratha, est assassiné par un de ses généraux, Pushyamitra, qui fonde la dynastie Shunga et rétablit l'hindouisme avec vigueur. L'Inde se retrouve de nouveau morcelée, les différents royaumes luttent les uns contre les autres pour une domination régionale, situation qui perdure jusqu'à l'avènement des Gupta au milieu du .

L’effondrement de l'empire maurya a des origines multiples : incapacité et rivalité des successeurs d'Ashoka, désir des brahmanes de retrouver leur prééminence, poids financier d'une armée inutile. Mais la cause principale est sa taille même qui ne lui permet pas d'être suffisamment centralisé. Les libertés prises par les administrations régionales, le maintien des royaumes préexistants et les difficultés de communication, expliquent que l'Empire se soit disloqué en à peine cinquante ans.

Ashoka disparaît de la mémoire indienne au Moyen Âge et n'est redécouvert qu'au par les historiens britanniques grâce à ses inscriptions. Les fondateurs de l'Inde moderne reconnaissent l'importance de ce premier unificateur du sous-continent et, désireux de promouvoir son idéal de tolérance, adoptent le chapiteau aux lions de la colonne de Sarnath comme emblème national et insèrent le "dharmachakra" dans le drapeau tricolore.

Après le troisième concile bouddhiste qu'il organise à Pataliputra, Ashoka dépêche des missionnaires dans tout le pays. Cependant, même si le bouddhisme conserve pendant quelque temps une certaine vitalité au Bengale et dans le Deccan et s'il donne lieu à des centres culturels et artistiques importants à Sarnath, Nalanda et Ajanta, il reste toujours minoritaire en Inde puis disparaît presque totalement.

Ashoka envoie aussi des missions au-delà des frontières, son fils (ou son frère), Mahendra, convertit le roi de Ceylan et le bouddhisme se répand progressivement en Asie centrale, en Chine et surtout en Asie du Sud-Est où il est toujours présent. Les édits attestent également de missions de prosélytisme auprès d'Antigone II Gonatas, roi de Macédoine, d'Antiochos II, roi de Syrie et de Ptolémée II, roi d'Égypte. Ce dernier aurait par ailleurs envoyé des ambassadeurs auprès d'Ashoka comme l'atteste Pline l'Ancien.

En 2001, Santosh Sivan réalise "Asoka", film de Bollywood avec Shahrukh Khan, qui relate de façon romancée la vie de l'Empereur maurya.

Ashoka est un des dirigeants indiens dans le jeu vidéo "Civilization IV".






</doc>
<doc id="19158" url="https://fr.wikipedia.org/wiki?curid=19158" title="Atomium">
Atomium

L’Atomium est un monument de Bruxelles, en Belgique, construit à l'occasion de l'Exposition universelle de 1958 et représentant la maille conventionnelle du cristal de fer (structure cubique centrée) agrandie de fois. Il est situé à Laeken sur le plateau du Heysel où eut lieu cette exposition. 

L’Atomium a été imaginé par l’ingénieur André Waterkeyn et érigé par les architectes André et Jean Polak pour l’Exposition universelle de 1958 à Bruxelles en Belgique.

C’est un édifice à mi-chemin entre la sculpture et l’architecture qui culmine à . Son acier de construction pèse . Il est devenu, au même titre que le Manneken Pis et la Grand-Place, un symbole de la capitale de la Belgique.

Il se compose d’une charpente d’acier portant neuf sphères reliées entre elles et revêtues, à l'origine, d’aluminium. 

Les sphères ont un diamètre de et pèsent chacune environ . Symboliquement, l’Atomium incarne l’audace d’une époque qui a voulu confronter le destin de l’Humanité avec les découvertes scientifiques. 

La construction de l’Atomium fut une prouesse technique. Sur les neuf sphères, six sont rendues accessibles au public, chacune comportant deux étages principaux et un plancher inférieur réservé au service. Le Tube central contient l’ascenseur le plus rapide de l’époque () installé par la succursale belge de la firme suisse Schlieren (reprise plus tard par Schindler). Il permet à d’accéder au sommet en . Les escaliers mécaniques installés dans les tubes obliques, comptent parmi les plus longs d’Europe. Le plus grand mesure de long.

Le peintre Roger Hebbelinck (1912 - ?) et le sculpteur Ernest Salu (1909 -1987) filment avec des droits exclusifs la construction de l’Atomium. Le documentaire « Construction de l’Atomium » (1957- 26′- réalisation : E. Salu et R. Hebbelinck , musique : Christian Leroy) est primé à Anvers en 1958. Un reportage de la télévision belge (RTBF) de Francis Drapier réalisé en 1988 reproduit de larges extraits de ce documentaire.

L’Atomium, conçu pour durer six mois, n’était pas destiné à survivre à l’Exposition universelle de 1958, mais sa popularité et son succès en ont fait un élément majeur du paysage bruxellois. Sa destruction fut donc reportée d’année en année jusqu’à ce que l'on y renonce. Mais, pendant trente ans, peu de travaux d’entretien furent réalisés. De 1960 à 1962, un Grand Prix de Bruxelles automobile fut organisé non loin de celui-ci.

En 2013, la chaîne de télévision américaine CNN International a considéré l'Atomium comme monument le plus d'Europe devant la ' et la Casa Milà.

Durant les , l’urgence d’une rénovation en profondeur devint une évidence. Devant l’importance des investissements nécessaires, la question de la conservation d’une construction qui n’était à l’origine destinée qu’à durer quelques mois fut à nouveau posée. D'aucuns souhaitaient une démolition. Une alternative prévoyait de ne réparer que les surfaces extérieures et d’abandonner l’exploitation de l’édifice. Devant les vives réactions suscitées par ces deux éventualités, il fut décidé de procéder à une complète réhabilitation.

L'Atomium, au seuil du , présentait un aspect terni, il était fortement dégradé par l’œuvre du temps, son étanchéité extérieure devenue inefficace et les éléments intérieurs de décoration et de mobilier au charme des très abîmés. Seule la structure pouvait être conservée. 

En 2001, le projet de rénovation est enfin lancé grâce à un partenariat entre l’État fédéral belge, la Région de Bruxelles-Capitale et Bruxelles-ville.

La rénovation a été entamée en et a pris fin en . La couverture originelle en aluminium a été remplacée par une nouvelle, plus résistante, en acier inoxydable, laquelle, d'autre part, est tout indiquée pour un monument symbolique du fer. Chaque sphère était recouverte à l’origine d’environ d’aluminium. Une partie d’entre eux a été mise en vente en guise de souvenir. Tous ont été remplacés, pour chaque sphère, par triangles en acier inoxydable. En outre, ces nouvelles plaques sont plus résistantes que les anciennes. Elles leur ressemblent cependant car elles portent le dessin des triangles originels qu'elles remplacent, cela afin de conserver à l'Atomium l'aspect que l'on avait appris à connaître depuis 1958. Comme à l'origine, sur chacune des neuf sphères, des réseaux électriques animent celle-ci de points lumineux animés de rapides mouvements circulaires reproduisant les électrons qui gravitent autour du noyau atomique. Le travail de montage de ce nouveau revêtement a été effectué par une grue qui amenait les plaques à leur emplacement où elles étaient mises en place par une équipe de cordistes, procédé utilisé pour éviter l'installation d'énormes échafaudages. 

Le pavillon d'accueil a été conçu par l'architecte Christine Conix.

Le financement des travaux de rénovation est estimé à d’euros. 

L’inauguration de la structure rénovée a eu lieu le .

Pour fêter la rénovation de l’Atomium, une pièce commémorative de a été frappée en .

Des six sphères accessibles au public, la sphère de base est réservée à l’exposition permanente consacrée aux , l’Exposition universelle et la construction de l’Atomium. Une autre accueille des expositions temporaires, une troisième à vocation polyvalente peut permettre l’organisation de différentes animations, films, concerts, fêtes ou conférences.

Dans la sphère centrale, un bar et dans la sphère supérieure, en plus du panorama, un restaurant. La sixième sphère est la boule des enfants, destinée à l’organisation d’ateliers de pédagogie urbaine, permettant aux enfants de six à douze ans d’y passer la nuit. 

Le designer Ingo Maurer a dessiné des luminaires intérieurs de l’édifice.

La Société belge des auteurs, compositeurs et éditeurs revendique jusqu'en 2016 la défense du droit d'auteur de l'Atomium et, comme représentant des ayants droit, en interdit alors la publication d'images sans contrepartie financière. 

Bart Somers, ex-président des Libéraux et démocrates flamands (VLD) et bourgmestre de Malines, organise en 2008 un concours de photos dont sont « effacées » les représentations de l'Atomium. Le but revendiqué est de lutter contre les droits d'auteurs jugés trop importants sur les représentations du monument.

La liberté de panorama entrée en application le 15 juillet 2016 permet la diffusion de photographies de l'Atomium à partir de cette date.

Concernant les cas similaires d'architectes interdisant la reproduction photographique de leurs œuvres :



</doc>
<doc id="19160" url="https://fr.wikipedia.org/wiki?curid=19160" title="Pierre Bourdieu">
Pierre Bourdieu

Pierre Bourdieu, né le à Denguin (Pyrénées-Atlantiques) et mort le à Paris, est considéré comme l'un des sociologues les plus importants de la seconde moitié du . Par ailleurs, du fait de son engagement public, il est devenu, dans les dernières années de sa vie, l’un des acteurs principaux de la vie intellectuelle française. Sa pensée a exercé une influence considérable dans les sciences humaines et sociales, en particulier sur la sociologie française d’après-guerre. Sociologie du dévoilement, elle a fait l’objet de nombreuses critiques, qui lui reprochent en particulier une vision déterministe du social dont il se défendait.

Son œuvre sociologique est dominée par une analyse des mécanismes de reproduction des hiérarchies sociales. Bourdieu insiste sur l’importance des facteurs culturels et symboliques dans cette reproduction et critique le primat donné aux facteurs économiques dans les conceptions marxistes. Il entend souligner que la capacité des agents en position de domination à imposer leurs productions culturelles et symboliques joue un rôle essentiel dans la reproduction des rapports sociaux de domination. Ce que Pierre Bourdieu nomme la "violence symbolique", qu’il définit comme la capacité à faire méconnaître l’arbitraire de ces productions symboliques, et donc à les faire admettre comme légitimes, est d’une importance majeure dans son analyse sociologique.

Le monde social, dans les sociétés modernes, apparaît à Pierre Bourdieu comme divisé en ce qu’il nomme des « champs ». Il lui semble, en effet, que la différenciation des activités sociales a conduit à la constitution de sous-espaces sociaux, comme le champ artistique ou le champ politique, spécialisés dans l’accomplissement d’une activité sociale donnée. Ces champs sont dotés d’une autonomie relative envers la société prise dans son ensemble. Ils sont hiérarchisés et leur dynamique provient des luttes de compétition que se livrent les agents sociaux pour y occuper les positions dominantes. Ainsi, comme les analystes marxistes, Pierre Bourdieu insiste sur l’importance de la lutte et du conflit dans le fonctionnement d’une société. Mais pour lui, ces conflits s’opèrent avant tout dans les différents champs sociaux. Ils trouvent leur origine dans leurs hiérarchies respectives, et sont fondés sur l’opposition entre agents dominants et agents dominés. Pour Bourdieu, les conflits ne se réduisent donc pas aux conflits entre classes sociales sur lesquels se centre l’analyse marxiste.

Pierre Bourdieu a également développé une théorie de l'action, autour du concept d’"habitus", qui a exercé une grande influence dans les sciences sociales. Cette théorie cherche à montrer que les agents sociaux développent des stratégies, fondées sur un petit nombre de dispositions acquises par socialisation qui, bien qu'inconscientes, sont adaptées aux nécessités du monde social. L’œuvre de Bourdieu est ainsi ordonnée autour de quelques concepts recteurs : "habitus" comme principe d’action des agents, "champ" comme espace de compétition sociale fondamental et "violence symbolique" comme mécanisme premier d’imposition des rapports de domination. Bourdieu a désigné son approche des structures sociales dans leur dimension de constitution et de transformation sous le terme de structuralisme génétique (ou constructiviste).

Pierre Bourdieu est né le dans les Pyrénées-Atlantiques à Denguin, petit village du Béarn. Il est le fils né du mariage d'Albert Bourdieu et Noëmie Duhau. Son père, issu de la petite paysannerie béarnaise, est d'abord cultivateur journalier, puis devient facteur et, par la suite, facteur-receveur, sans quitter son milieu rural. Sa mère a une origine sociale proche, quoique légèrement supérieure, puisqu'elle est issue d'une lignée de propriétaires à Lasseube. Il est l'unique enfant du couple.

Il est le père de trois enfants : le réalisateur Emmanuel Bourdieu, Laurent Bourdieu, physicien à l'école des neurosciences de Paris, et Jérôme Bourdieu, directeur de recherche à l'INRA.

Interne au lycée Louis-Barthou de Pau, Pierre Bourdieu est un excellent élève et attire l'attention d'un de ses professeurs, ancien élève de l’École normale supérieure, qui lui conseille de s’inscrire en khâgne au lycée Louis-le-Grand de Paris, ce qu'il fait en 1948.

Il est reçu à l’École normale supérieure en 1951. Celui que ses camarades appellent de son deuxième prénom, Félix, y retrouvera peu à peu ses anciens condisciples de classe préparatoire comme Jacques Derrida, Lucien Bianco ou Louis Marin. Alors que la scène philosophique française est dominée par la figure de Jean-Paul Sartre, par le marxisme et par l'existentialisme, Bourdieu réagit comme de nombreux normaliens de sa génération ; ces derniers se sont orientés préférentiellement vers l'étude des « courants dominés » du champ philosophique : le pôle de l'histoire de la philosophie proche de l'histoire des sciences, représenté par Martial Guéroult et Jules Vuillemin, et l'épistémologie enseignée par Gaston Bachelard et Georges Canguilhem.

Pierre Bourdieu soutient en 1953, sous la direction d'Henri Gouhier, un mémoire sur les "Animadversiones" de Leibniz. En plus de son cursus, il suit aussi le séminaire d'Éric Weil à l'École pratique des hautes études sur la philosophie du droit de Hegel.

Reçu septième à l'agrégation de philosophie en 1954, il s'inscrit auprès de Georges Canguilhem pour une thèse de philosophie sur les structures temporelles de la vie affective, qu'il abandonne en 1957 afin de se consacrer à des études ethnologiques de terrain, ce qui représente déjà un déclassement dans la hiérarchie des disciplines du champ académique (il revient sur cette partie de sa trajectoire académique dans son ouvrage "Esquisse pour une auto-analyse").

Georges Canguilhem place son thésard à proximité de Paris, comme professeur au lycée de Moulins en 1954-1955. Mais Pierre Bourdieu doit remplir ses obligations militaires. Après avoir refusé de suivre la formation d’élève officier de réserve, il est affecté au service psychologique des armées à Versailles . Il est trouvé en possession d'un numéro censuré de "L’Express" relatif à la question algérienne. Il aurait ainsi perdu son affectation pour raisons disciplinaires, et, rapidement embarqué avec des jeunes appelés en Algérie dans le cadre de la « pacification », il y accomplit l’essentiel du service militaire, qui dure deux ans.

Il fait d'abord partie d'une petite section qui garde un dépôt d'essence. Grâce à l'intervention de sa famille, il est affecté au Gouvernement Général d'Alger, dans les services administratifs de la Résidence Générale, sous les ordres de Robert Lacoste. De 1958 à 1960, il poursuit ses études sur l’Algérie en accomplissant des études de terrain et il prend un poste d’assistant à la Faculté des Lettres d’Alger.

Cette période algérienne est décisive : c’est là, en effet, que se décide sa carrière de sociologue. Délaissant les , il conduit ainsi, sans formation initiale dans ce domaine, toute une série de travaux d’ethnologie en Algérie, qui aboutissent à l’écriture de plusieurs livres. Ses premières enquêtes le mènent dans les régions de Kabylie et de Collo, bastions nationalistes où la guerre fait rage. Sa "Sociologie de l’Algérie", synthèse des savoirs existants sur ces trois départements français, est publiée dans la collection « Que sais-je ? » en 1958. Après l'Indépendance algérienne, il publie, en 1963, "Travail et travailleurs en Algérie", étude de la découverte du travail salarié et de la formation du prolétariat urbain en Algérie, en collaboration avec Alain Darbel, Jean-Paul Rivet et Claude Seibel. En 1964, il publie "Le Déracinement. La crise de l’agriculture traditionnelle en Algérie", en collaboration avec son ami algérien Abdelmalek Sayad, sur la destruction de l’agriculture et de la société traditionnelle, et la politique de regroupement des populations par l’armée française. Après son retour en France, Bourdieu profite, jusqu’en 1964, des vacances scolaires pour collecter de nouvelles données sur l’Algérie urbaine et rurale de l’époque.

Le terrain ethnologique de la Kabylie ne cessa, même après qu’il eut cessé de s’y rendre, de nourrir l’œuvre anthropologique de Pierre Bourdieu. Ses principaux travaux sur la théorie de l'action "Esquisse d’une théorie de la pratique" (1972) et "Le Sens pratique" (1980) naissent ainsi d’une réflexion anthropologique sur la société kabyle traditionnelle. De même, son travail sur les rapports de genre, "La Domination masculine" (1998), s'appuie sur une analyse des mécanismes de reproduction de la domination masculine dans la société traditionnelle kabyle.

L'enquête algérienne s'appuie entre autres sur la pratique photographique, dont quelques clichés illustrent les livres publiés de son vivant par le sociologue. En 2012, les images prises par Pierre Bourdieu cinquante ans auparavant font l'objet d'une première exposition monographique, organisée à Tours par Christine Frisinghelli et Franz Schultheis sous l'égide du « Jeu de Paume », de Camera Austria et de la Fondation Bourdieu.

En 1960, il regagne Paris, pour devenir l'assistant de Raymond Aron à l’université de Paris. Raymond Aron fait également de lui le secrétaire du Centre de sociologie européenne, institution de recherche qu'il a fondée en 1959, à partir de reliquat de structures d'après-guerre et avec l'aide financière de la fondation Ford.

Le jeune assistant de Raymond Aron obtient un poste de maître de conférences à l’université de Lille, qu'il occupe jusqu’en 1964, tout en continuant d'intervenir à Paris dans le cadre de cours et de séminaires. À Lille, il retrouve Éric Weil et fait connaissance avec l'historien Pierre Vidal-Naquet et surtout l'herméneute, philologue et germaniste, Jean Bollack qui devient un ami fidèle.

En 1962, il épouse Marie-Claire Brizard, avec laquelle il a trois enfants : Jérôme, Emmanuel et Laurent. Au milieu des années 1960, il s'installe avec sa famille à Antony, dans la banlieue sud de Paris. La famille rejoint le Béarn pendant les vacances scolaires. Pierre Bourdieu s'intéresse au Tour de France cycliste et pratique à bon niveau de nombreux sports individuels et collectifs, tels le tennis et le rugby.

En 1964, Bourdieu rejoint la VIe section de l’École pratique des hautes études (EPHE), qui devient en 1975 l’École des hautes études en sciences sociales (EHESS). La même année, sa collaboration commencée plus tôt avec Jean-Claude Passeron aboutit à la publication de l’ouvrage "Les Héritiers", qui rencontre un vif succès et contribue à faire de lui un sociologue « en vue ».

À partir de 1965, avec "Un Art moyen. Essais sur les usages sociaux de la photographie", suivi en 1966 par "L’amour de l’Art", Pierre Bourdieu engage une série de travaux portant sur les pratiques culturelles, qui occupent une part essentielle de son travail sociologique dans la décennie suivante, et qui débouchent sur la publication, en 1979, de "La Distinction : critique sociale du jugement", qui est son œuvre la plus connue et la plus importante pour le champ sociologique, et qui figure parmi les dix plus importantes œuvres sociologiques du monde au dans le classement établi par l"'International Sociological Association".

À l'issue des événements de Mai 68 auxquels il participe en qualité de scientifique, il rompt avec son maître Raymond Aron, penseur libéral, qui désapprouve ce mouvement social. En 1968, il fonde ainsi le Centre de Sociologie de l'éducation et de la culture qui s'émancipe du Centre de sociologie européenne. La même année, il publie avec Jean-Claude Chamboredon et Jean-Claude Passeron "Le Métier de sociologue", un traité dans lequel ils exposent, à partir d’un choix de textes d’auteurs, les méthodes de la sociologie. L’ouvrage devait comporter trois volumes. Le second, qui porte sur le symbolisme dans la société, avait déjà son plan détaillé et ses matériaux.

En 1985, Pierre Bourdieu devient directeur du Centre de sociologie européenne, dont il avait assuré le premier développement au secrétariat au début des années 1960. Le CNRS exige en 1997 une fusion avec le Centre de sociologie de l'éducation et de la culture. La structure préservant les missions des deux entités est dirigée par son élève Rémi Lenoir.

La réception des travaux de Pierre Bourdieu dépasse progressivement le milieu de la sociologie française. Il est en particulier lu dans les milieux historiens francophones, notamment à l'EHESS. Les années 1970 voient émerger une reconnaissance anglo-saxonne. L'Allemagne, grâce à l'action de Joseph Jurt, suit avec plus d'une décennie de retard. La reconnaissance internationale permet à Pierre Bourdieu d'accomplir de nombreux voyages, et des séjours plus ou moins longs, parfois en famille, émaillés de conférences, principalement dans les pays anglo-saxons, au Japon, en Allemagne et en Scandinavie.

Grâce notamment à l'appui d'André Miquel, il devient professeur au Collège de France en 1981.
Il est le premier sociologue à recevoir la médaille d'or du CNRS en 1993. On peut ainsi souligner le paradoxe d’un homme qui n’a cessé de se vivre comme à la marge des institutions académiques dominantes, dont il a même entrepris l’étude critique, par exemple dans "Homo academicus", alors même qu’il a réalisé une des « carrières » universitaires les plus exemplaires qui soient.

Parallèlement à sa carrière universitaire, Pierre Bourdieu a mené une importante activité d'éditeur, qui lui a permis de pleinement diffuser sa pensée. En 1964, il devient directeur de la collection « Le sens commun » aux éditions de Minuit, jusqu’en 1992 où il change d’éditeur, au profit des éditions du Seuil. Dans cette collection, Pierre Bourdieu publie la plupart de ses livres, ainsi que ceux de chercheurs influencés par lui, favorisant ainsi la diffusion de sa pensée. Bourdieu publie également des classiques des sciences sociales (Émile Durkheim, Marcel Mauss, etc.) ou de la philosophie (Ernst Cassirer, Erwin Panofsky, etc.). La collection fait également découvrir aux lecteurs français des sociologues américains de premier plan (traductions d’Erving Goffman). Après son passage aux éditions du Seuil, il y fonde la collection « Liber », en continuité avec la collection « Le sens commun ».

En 1975, il crée, notamment avec le soutien de Fernand Braudel, la revue "Actes de la recherche en sciences sociales", qu'il dirige jusqu'à sa mort. Cette publication est un lieu d’exposition de ses travaux et de ceux de ses élèves. Elle se démarque des revues universitaires traditionnelles par le recours à de nombreuses illustrations (photographie, bande dessinées, etc.), son grand format et sa mise en page.

En 1995, à la suite des mouvements sociaux et pétitions de novembre-décembre en France, il fonde une maison d'édition, Raisons d’agir, à la fois militante et universitaire, publiant des travaux, souvent de jeunes chercheurs qui lui sont liés, procédant à une critique du néolibéralisme.

Pendant la guerre civile en Algérie, Pierre Bourdieu soutient les intellectuels algériens.

À partir du début des années 1980, Pierre Bourdieu s’implique davantage dans la vie publique. Il participe notamment au soutien à Solidarność en partie en raison de la sollicitation de Michel Foucault. En 1981, Bourdieu avec Gilles Deleuze et d'autres intellectuels soutiennent "le principe" de la candidature de Coluche à l’élection présidentielle. Bourdieu voyait dans les accusations de poujadisme portées contre la candidature de Coluche par les hommes politiques, la volonté de ces derniers de préserver leur monopole de la représentation politique, et de se protéger contre la menace d’un « joueur » qui refuse les règles habituelles du jeu politique, montrant ainsi leur arbitraire. Ce n'est que dans les années 1990 qu'il s'engage pleinement dans la vie publique, réinvestissant la figure de l’intellectuel engagé.

Lors du mouvement de novembre/décembre 1995, il défend les grévistes. En 1996, il est l’un des initiateurs des « États généraux du mouvement social ». Il soutient également le mouvement de chômeurs de l’hiver 1997-1998, qui lui apparaît comme un « miracle social ».

L’axe central de son engagement consiste en une critique de la diffusion du néolibéralisme et des politiques de démantèlement des institutions de l’État-providence.

Sans qu’il soit favorable à une solution alternative au capitalisme, sa critique sociale fait de lui une des figures du mouvement altermondialiste, alors naissant. La plupart de ses interventions sont regroupées dans deux ouvrages intitulés "Contre-feux".

De cette période date également "La Misère du monde" (1993), ouvrage d’entretiens, qui cherche à montrer les effets déstructurants des politiques néolibérales, et qui remporte un très important succès public.

L’implication de Bourdieu dans l’espace public lui assure une renommée dépassant le monde universitaire, faisant de lui un des grands intellectuels français de la seconde moitié du , à l’instar de Michel Foucault ou Jacques Derrida. Toutefois, à l’image de ces deux philosophes, sa pensée, bien qu’elle ait exercé une influence considérable dans le champ des sciences sociales n’a pas cessé de faire l’objet de vives critiques, l’accusant par exemple de réductionnisme.

Il est, dans les médias, un personnage à la fois recherché et contesté, selon l’expression d’un magazine, le plus « médiatique des anti-médiatiques ». Cette figure centrale de la vie intellectuelle française est l'objet de nombreuses controverses. On peut y voir le produit de ses critiques du monde médiatique, ainsi que de son engagement antilibéral. Sa participation à l’émission "Arrêt sur images" du constitue un épisode à la fois marquant et révélateur du rapport que Pierre Bourdieu a pu entretenir avec les médias : l’émission, qui faisait suite à la grève de novembre/décembre 1995, devait rendre compte du traitement médiatique de celle-ci ; Bourdieu en était l’invité principal : considérant qu'il a été empêché de développer librement ses analyses et qu'il a fait l’objet de violentes critiques de la part des autres invités, professionnels des médias — Guillaume Durand et Jean-Marie Cavada —, il y voit la confirmation de l’impossibilité de « critiquer la télévision à la télévision parce que les dispositifs de la télévision s’imposent même aux émissions de critique du petit écran ». Peu de temps après, il écrit un petit ouvrage, "Sur la télévision", où il cherche à montrer que les dispositifs des émissions télévisuelles sont structurés d’une manière telle qu’ils engendrent une puissante censure de toutes les paroles critiques de l’ordre dominant.

Il meurt le d’un cancer des poumons généralisé à l'hôpital Saint-Antoine, après avoir souffert d'un intense mal de dos d'origine inconnue. Travaillant durant ses derniers mois à la théorie des champs, il entreprend la rédaction d'un ouvrage, resté inachevé, sur le peintre Édouard Manet, en qui il voit une figure centrale de la révolution symbolique fondatrice de l'autonomie du champ artistique moderne. Peu de temps avant sa mort, Bourdieu termine son "Esquisse pour une auto-analyse", œuvre qu'il se refuse à décrire comme autobiographique mais dans laquelle il s'efforce de rendre compte de sa trajectoire sociale et intellectuelle, à partir des outils théoriques qu'il a forgés. Il entend y donner seulement les « traits pertinents » pour le comprendre et pour comprendre son œuvre. Il envoie le manuscrit achevé de l'ouvrage à son éditeur allemand, qui le publie en 2002. Le livre paraît en 2004 en français.

Sa mort suscite une importante couverture médiatique, qui témoigne de sa reconnaissance internationale.

Sa tombe se situe au cimetière du Père-Lachaise, à Paris, près de celles de Claude Henri de Rouvroy de Saint-Simon et de Jean Anthelme Brillat-Savarin.

Concernant son emploi d’un important jargon et de néologismes, Pierre Bourdieu, dans "Questions de sociologie", défend l’utilisation d’un vocabulaire et d’une syntaxe complexes : un langage spécifique est nécessaire pour être précis, et rompre avec les « automatismes de la pensée ». Les sciences sociales utilisent la langue ordinaire comme un outil : elles doivent donc modifier cette langue, en ce qu’elle transmet, dans son vocabulaire, des représentations et des visions non scientifiques de la société. Il note également que cette exigence d’accessibilité n’est demandée qu’à la sociologie, et non à d’autres disciplines comme la philosophie ou la physique.

Bourdieu est l’héritier de la sociologie classique, dont il a synthétisé, dans une approche profondément personnelle, la plupart des apports principaux.

Ainsi de Max Weber, il a retenu l’importance de la dimension symbolique de la légitimité de toute domination dans la vie sociale ; de même que l’idée des ordres sociaux qui deviendront, dans la théorie bourdieusienne, des "champs". De Karl Marx, il a repris le concept de capital, généralisé à toutes les activités sociales, et non plus seulement économiques, et la théorie des classes sociales. D’Émile Durkheim, enfin, il hérite un certain style déterministe (principe de causalité) et, en un sens, à travers Marcel Mauss et Claude Lévi-Strauss, structuraliste. Il ne faut pas, toutefois, négliger les influences philosophiques chez ce philosophe de formation : Maurice Merleau-Ponty et, à travers celui-ci, la phénoménologie de Husserl ont joué un rôle essentiel dans la réflexion de Bourdieu sur le corps propre, les dispositions à l’action, le "sens pratique", l'activité athéorique : c’est-à-dire dans la définition du concept central d’"habitus". Par ailleurs, Wittgenstein, cité dès "Esquisse d’une théorie de la pratique" en 1971, est une source d’inspiration importante pour Bourdieu, en particulier dans sa réflexion sur la nature des règles suivies par les agents sociaux. Enfin, Bourdieu a placé, à la fin de sa vie, sa sociologie sous le signe de Pascal : « J’avais pris l’habitude, depuis longtemps, lorsqu’on me posait la question, généralement mal intentionnée, de mes rapports avec Marx, de répondre qu’à tout prendre, et s’il fallait à tout prix s’affilier, je me dirais plutôt pascalien […] ».

L’œuvre de Pierre Bourdieu est construite sur la volonté affichée de dépasser, grâce à des innovations conceptuelles comme l"'habitus", le "capital" ou le "champ", une série d’oppositions qui structurent les sciences sociales: subjectivisme/objectivisme, micro/macro, constructivisme/structuralisme. Bourdieu reproche au structuralisme de soumettre l’individu à des règles structurelles, et au constructivisme à l'inverse de faire du monde social le produit de l’action libre des acteurs sociaux; pour lui, le monde social est constitué de structures qui sont certes construites par les agents sociaux, selon la position constructiviste, mais qui, une fois constituées, conditionnent à leur tour l’action de ces agents, selon la position structuraliste. On rejoint ici, par d’autres termes, ce que la sociologie anglo-saxonne appelle l’opposition (agent déterminé entièrement par des structures le dépassant/acteur créateur libre et rationnel des activités sociales) dont la volonté de dépassement caractérise particulièrement le travail conceptuel de Bourdieu.

La reprise du structuralisme génétique (c'est-à-dire dynamique, réinséré dans la dimension temporelle) de Jean Piaget, Lucien Goldmann et surtout de Chomsky, intervient chez Bourdieu dès le milieu des années 1970 lorsqu'il s'intéresse à la rupture introduite dans le structuralisme par la linguistique générative. C'est dans "Choses dites" (1987) que le sociologue explicite sa volonté de donner à sa théorie sociologique le nom de « structuralisme constructiviste » (ou « constructivisme structuraliste »), qu'il appelle aussi « structuralisme génétique », terme qui est depuis fréquemment utilisé pour désigner la démarche bourdieusienne de cette période.

Riche de plus de trente livres et de centaines d’articles, l’œuvre de Bourdieu aborde un nombre très important d’objets empiriques. Elle est toutefois ordonnée autour de quelques concepts directeurs :

L'origine de ce concept est à rechercher dans la pensée scolastique de Thomas d'Aquin, qui a utilisé la notion d'habitus pour traduire le terme aristotélicien d"'hexis". Par le concept d’habitus, Bourdieu vise à penser le lien entre socialisation et actions des individus. L’habitus est constitué en effet par l’ensemble des dispositions, schèmes d’action ou de perception que l’individu acquiert à travers son expérience sociale. Par sa socialisation, puis par sa trajectoire sociale, tout individu incorpore lentement un ensemble de manières de penser, sentir et agir, qui se révèlent durables. Bourdieu pense que ces dispositions sont à l’origine des pratiques futures des individus.

Toutefois, l’habitus est plus qu’un simple conditionnement qui conduirait à reproduire mécaniquement ce que l’on a acquis. L’habitus n’est pas une habitude que l’on accomplit machinalement. En effet, ces dispositions ressemblent davantage à la grammaire de sa langue maternelle. Grâce à cette grammaire acquise par socialisation, l’individu peut, de fait, fabriquer une infinité de phrases pour faire face à toutes les situations. Il ne répète pas inlassablement la même phrase. Les dispositions de l’habitus sont du même type : elles sont des schèmes de perception et d’action qui permettent à l’individu de produire un ensemble de pratiques nouvelles adaptées au monde social où il se trouve. L’habitus est « puissamment générateur » : il est même à l’origine d’un "sens pratique". Bourdieu définit ainsi l’habitus comme des « structures structurées prédisposées à fonctionner comme structures structurantes ». L’habitus est structure structurée puisqu’il est produit par socialisation ; mais il est également structure structurante car générateur d’une infinité de pratiques nouvelles.

Dans la mesure où ces dispositions font système, l’habitus est à l’origine de l’unité des pensées et actions de chaque individu. Mais, dans la mesure où les individus issus des mêmes groupes sociaux ont vécu des socialisations semblables, il explique aussi la similitude des manières de penser, sentir et agir propres aux individus d’une même classe sociale.

Cela ne signifie pas toutefois que les dispositions de l’habitus soient immuables : la trajectoire sociale des individus peut conduire à ce que leur habitus se transforme en partie. D’autre part, l’individu peut partiellement se l’approprier et le transformer par un retour sociologique sur soi.

Les dispositions constitutives de l’habitus ont pour première propriété d’être durables, c’est-à-dire de survivre au moment de leur incorporation. Pour penser cette durabilité des dispositions, Bourdieu introduit le concept d’hystérésis de l’"habitus". Ce concept cherche à désigner le phénomène par lequel un agent, qui a été socialisé dans un certain monde social, en conserve, dans une large mesure, les dispositions, même si elles sont devenues inadaptées à la suite, par exemple, d'une évolution historique brutale, comme une révolution, qui a fait disparaître ce monde.

Un exemple mythique, cité par Marx comme par Bourdieu, est celui de Don Quichotte. Chevalier dans un monde où il n’y a plus de chevalerie, et inapte à faire face à l’effondrement de son univers, il en vient à chasser les moulins à vent qu’il prend pour d’immenses tyrans.

Bourdieu donne un autre exemple dans "Le Bal des célibataires" : les stratégies matrimoniales perdurent comme "habitus" à une époque où elles ont perdu leur sens, provoquant une crise matrimoniale dans la société paysanne béarnaise.

Les dispositions constitutives de l’habitus sont, d’autre part, transposables. Bourdieu veut dire par là que des dispositions acquises dans une certaine activité sociale, par exemple au sein de la famille, sont transposées dans une autre activité, par exemple le monde professionnel.
Le caractère transposable des dispositions est lié à une autre hypothèse : les dispositions des agents sont unifiées entre elles. Cette hypothèse est au centre de l’ouvrage intitulé "La Distinction", où Bourdieu entend montrer que l’ensemble des comportements des agents sont reliés entre eux par un « style » commun.

Dans "La Distinction" — qui porte essentiellement sur la structure sociale — Bourdieu met en évidence l’existence de « styles de vie » fondés sur des positions de classes différentes. Par exemple, il fait ainsi apparaître le lien qui unit l’ensemble des pratiques sociales des ouvriers. Ainsi, le rapport à la nourriture des ouvriers entretient un rapport d’homologie avec leur appréhension de l’art. Pour les ouvriers, la nourriture doit être avant tout nourrissante, c’est-à-dire utile et efficace, et elle est souvent lourde et grasse, c’est-à-dire sans considération hygiénique. De même, la vision de l’art des ouvriers est fondée sur un rejet de l’art abstrait et privilégie l’art réaliste, c’est-à-dire utile, et un peu « pompier », autrement dit, « lourd » et sans « finesse ». Bourdieu retrouve cette insistance sur l’utilité dans le type de vêtements portés par les ouvriers, qui sont avant tout fonctionnels. Ce style de vie est donc unifié par un petit nombre de principes, que sont en particulier la fonctionnalité et l’absence de recherche de l’élégance. Pour Bourdieu, le style de vie des ouvriers se fonde ainsi, fondamentalement, sur le privilège accordé à la "substance" plutôt qu’à la "forme" dans l’ensemble des pratiques sociales. Bourdieu voit dans ce style de vie l’effet des dispositions de l’habitus des ouvriers, qui sont elles-mêmes le produit de leur mode de vie. La vie des ouvriers est, en effet, placée sous le mode de la nécessité, en l’absence de ressources économiques : elle engendre ainsi des dispositions où dominent la recherche de l’utile et du nécessaire.

On retrouve là une idée avancée par Thorstein Veblen dans sa "Théorie de la classe de loisir".

Bourdieu, dans de très nombreux textes, entend souligner le caractère « générateur » de l’habitus. L’habitus, cette « structure structurée prédisposée à fonctionner comme structure structurante », a, en effet, comme propriété d’être à l’origine d’une infinité de pratiques possibles.

À partir d’un nombre restreint de dispositions, l’agent est, ainsi, capable d’inventer une multiplicité de stratégies — un peu à la façon de la grammaire d’une langue, par exemple celle du français, ensemble limité de règles, qui permet à ses locuteurs de créer néanmoins une infinité de phrases, à chaque fois adaptées à la situation.

Ce caractère « générateur » de l’habitus est, enfin, lié à une dernière propriété de l’habitus : celle d’être au principe de ce que Bourdieu nomme le « sens pratique ».

Bourdieu veut dire par là que l’habitus étant le reflet d’un monde social, il lui est adapté et permet aux agents, sans que ceux-ci aient besoin d’entreprendre une réflexion « tactique » consciente, de répondre immédiatement et sans même y réfléchir aux évènements auxquels ils font face.

Ainsi, à la façon d’un joueur de tennis, qui ayant profondément acquis la logique de son jeu, court vers où la balle, lancée par son adversaire, va retomber, sans même y penser (on dit alors qu’il a acquis les automatismes de son jeu), l’agent va agir de même dans le monde social où il vit en développant, grâce à son habitus, de véritables « stratégies » adaptées aux exigences de ce monde. Ainsi, .

Tout comme le fait un joueur de tennis, les stratégies adoptées peuvent être conscientes ou inconscientes. Elles sont des modèles d'action, comprises en termes de profit vers une finalité. Fonction du monde social passé, selon les interactions présentes, elles essaient de fabriquer le futur dans le meilleur bénéfice pour son patrimoine. Ces stratégies ne sont pas forcément délibérément choisies, et même peuvent être d'autant plus efficaces qu'elles ne sont pas intentionnelles.

Avec sa théorie du sens pratique, Bourdieu semble retrouver la théorie de l’acteur rationnel, dominante en économie, en ce qu’il insiste sur le fait que l’habitus est au principe de stratégies par lesquelles les agents accomplissent la recherche d’un intérêt. La différence est pourtant profonde : Bourdieu veut, au contraire, montrer que les agents ne calculent pas, en cherchant intentionnellement à maximiser leur intérêt selon des critères rationnels explicites et conscients. Il critique ainsi fortement la théorie de l’acteur rationnel : il refuse l’idée que les acteurs soient des stratèges minutieux et conscients à la poursuite d’intérêts longuement réfléchis. Pour lui, bien au contraire, les agents agissent - ou plutôt sont dans une pratique – à partir de leurs dispositions sociales, construites au fur et à mesure de leur trajectoire et inscrites dans leur corps, qui rendent possible ce « sens du jeu » – et non par une réflexion consciente. Comme Bourdieu l’écrit, « l’habitus enferme la solution des paradoxes du sens objectif sans intention subjective : il est au principe de ces enchaînements de "coups" qui sont objectivement organisés comme des stratégies sans être le produit d’une véritable intention stratégique ».

Bourdieu prolonge sa critique en refusant l’utilitarisme de la théorie de l’acteur rationnel : l’intérêt ne se résume pas, pour Bourdieu, à un intérêt matériel. Il est la croyance qui fait que les individus pensent qu’une activité sociale est importante, vaut la peine d’être poursuivie. Il existe donc autant de types d’intérêt que de champs sociaux : chaque espace social propose en effet aux agents un enjeu spécifique. Ainsi l’intérêt que poursuivent les hommes politiques n’est pas le même que celui des hommes d’affaires : les uns croient que le pouvoir est la source fondamentale d’utilité, tandis que l’enrichissement économique est la motivation première des "businessmen". Bourdieu a ainsi proposé de substituer au terme d’"intérêt" celui d’"illusio". Par ce mot, Bourdieu entend en effet souligner qu’il n’est pas d’intérêt qui ne soit une croyance, une "illusion" : celle de croire qu’un enjeu social spécifique a une importance telle qu’il faille le poursuivre. Comme le note Bourdieu, . Or, cette "illusio" est acquise par socialisation. L’agent croit que tel enjeu social est important, parce qu’il a été socialisé à le croire. Les intérêts sociaux sont ainsi des croyances, socialement inculquées et validées. Cette croyance est particulièrement forte chez ceux que Bourdieu appelle les "natifs" du champ, c'est-à-dire ceux qui possèdent les propriétés objectives les plus recherchées dans cet univers social et celles qui y favorisent le succès.

Élaboré à la fin des années 1960, thématisé une première fois dans la préface à une publication d’œuvres d’ethnologie kabyle, "Esquisse d’une théorie de la pratique" (1972), complétée dans "Le sens pratique" (1980), le concept d’habitus visait, primitivement, à dépasser les deux conceptions du sujet et de l’action alors dominantes dans l’espace intellectuel français.

S’opposaient ainsi les théories inspirées de la phénoménologie, et en particulier l’existentialisme de Jean-Paul Sartre, qui plaçaient au cœur de l’action la liberté absolue du sujet, aux théories issues du structuralisme, en particulier l’anthropologie structurale de Claude Lévi-Strauss, qui faisait de l’action du sujet un comportement déterminé, au moins en partie, par une conformation structurée de la collectivité (un agencement spontané des relations entre catégories d'individus et fonctions au sein du groupe, agencement dont le sujet n'a que peu ou pas conscience).

Face au structuralisme, Bourdieu a voulu redonner une capacité d’action autonome au sujet, sans toutefois lui accorder la liberté que lui prêtait l’existentialisme. La « solution » que propose Bourdieu est de considérer que l’agent a, lors des différents processus de socialisation qu’il a connus, en particulier sa socialisation primaire, incorporé un ensemble de principes d’action, reflets des structures objectives du monde social dans lequel il se trouve, qui sont devenus en lui, au terme de cette incorporation, des « dispositions durables et transposables », selon l’une des définitions de l’habitus que propose Bourdieu.
Ainsi l’agent en un certain sens agit de lui-même, à la différence du sujet structuraliste qui (dans l'interprétation que Bourdieu et d'autres sociologues après lui en font) actualise des règles : en effet, son action est le produit des « stratégies inconscientes » qu’il développe. Toutefois, ces stratégies sont constituées à partir de dispositions que l’agent a incorporées. Au fondement de l’action, on trouve donc l’ensemble de ces dispositions qui constituent l’habitus. C’est pour cela que Bourdieu préfère au terme d’acteur, généralement employé par ceux qui veulent souligner la capacité qu’a l’individu d’agir librement, celui d’agent, qui insiste, au contraire, sur les déterminismes auxquels est soumis l’individu. Certains travaux insistent néanmoins sur la relation entre habitus, liberté et réflexivité.

L’action des individus est donc, au terme de la théorisation de Bourdieu, fondamentalement le produit des structures objectives du monde dans lequel ils vivent, et qui façonnent en eux un ensemble de dispositions qui vont structurer leurs façons de penser, de percevoir et d’agir.

Dès le milieu des années 1960, Bourdieu s’intéresse au champ de la parenté cher à l’anthropologie classique. Cela sera le premier chantier d’une critique radicale de l’objectivisme dominant alors la théorie anthropologique. En forgeant une théorie qui trouve sa source dans le sens de la pratique, il cherche en effet à marquer une nette rupture avec le structuralisme, qui selon lui privilégierait l’étude des règles et des normes pour expliquer les pratiques de la vie sociale. Ses travaux ethnographiques en Kabylie et, parallèlement, en Béarn (notamment dans son village natal) sont l’occasion alors pour lui de proposer un concept nouveau, celui de « stratégie matrimoniale ».

Selon Bourdieu, l’individu social est un agent mû par un intérêt, personnel ou collectif (son groupe, sa famille), dans un cadre élaboré par l’habitus qui est le sien. Sur la base d’un ensemble réduit de quelques principes normatifs, correspondant à une position sociale et à une condition matérielle, l’agent élabore la stratégie qui sert le mieux ses objectifs. Appliquée au domaine de la parenté, cette idée nous montre des individus opérant des choix cruciaux à l’occasion des mariages dans le but, déterminant à l’avis de l’auteur, de préserver ou améliorer la condition sociale de la famille. C’est le concept de « stratégie matrimoniale » qui complexifie et affine notre regard sur des situations jusqu’ici peu expliquées, par exemple le fait, en Béarn, de confier à une fille plutôt qu’à un garçon la transmission du patrimoine familial pour éviter de le voir morcelé. Il utilise l’analogie du joueur de cartes, qui doit composer son jeu et atteindre son objectif, en fonction des atouts et des fausses cartes qu’il a en main. « Tout se passe comme si ces stratégies matrimoniales visaient à corriger les ratés des stratégies de fécondité » nous dit l’auteur. Finalement, en étudiant justement ces situations particulières (le droit d’aînesse, le primat de la masculinité dans les affaires de succession, la question du mariage du cadet), Bourdieu nous montre un modèle d’analyse où le mariage (l’alliance) et la succession (la filiation) sont avant tout une somme de pratiques dont le sens est construit par l’utilisation réfléchie de chacun.

Pierre Bourdieu définit la société comme une imbrication de champs : champs économique, politique, culturel, artistique, sportif, religieux, etc. Chaque champ est organisé selon une logique propre déterminée par la spécificité des enjeux et des atouts que l’on peut y faire valoir. Les interactions se structurent donc en fonction des atouts et des ressources que chacun des agents mobilise, c’est-à-dire, pour reprendre les catégories construites par Bourdieu, de son capital, qu’il soit économique, culturel, social ou symbolique.

Le champ est un espace social de position où tous les participants ont à peu près tous les mêmes intérêts mais où chacun a en plus des intérêts propres à sa position occupée dans le champ. Chaque champ a ses règles spécifiques mais on peut retrouver des règles générales : lutte entre les anciens et les nouveaux, tous acceptent les enjeux du champ et tous souhaitent sa survie.

La notion de violence symbolique renvoie à l’intériorisation par les agents de la domination sociale inhérente à la position qu’ils occupent dans un champ donné et plus généralement à leur position sociale. Cette violence est infra-consciente et ne s’appuie pas sur une domination intersubjective (d’un individu sur un autre) mais sur une domination structurale (d’une position en fonction d’une autre). Cette structure, qui est fonction des capitaux possédés par les agents, fait violence car elle est non perçue par les agents. Elle est donc source d’un sentiment d’infériorité ou d’insignifiance qui est uniquement subi puisque non objectivé. La violence symbolique trouve son fondement dans la légitimation des schèmes de classement inhérents à la hiérarchisation des groupes sociaux.

Pierre Bourdieu a construit, notamment dans "La Distinction", une théorie de l’espace social, au croisement des traditions marxiste et wébérienne. Cette théorie se propose d’expliquer principalement :

Bourdieu, dans "La distinction" essentiellement, propose une théorie originale de la hiérarchisation de l’espace social, à partir d’une relecture de Max Weber. Cette théorie s’oppose à la théorie marxiste selon laquelle les sociétés se structureraient à partir des processus de production économique. Ainsi, dans ce que les marxistes appellent le mode de production capitaliste, la production économique est structurée autour du rapport de production opposant producteurs directs (les prolétaires) et possesseurs des moyens de production (les capitalistes). Le capitalisme créerait ainsi deux classes sociales, les prolétaires et les capitalistes. Ces deux classes seraient en lutte, les capitalistes exploitant, selon les marxistes, les prolétaires. La production économique structurerait ainsi la société en créant des classes sociales antagonistes.

Bourdieu refuse cette théorie de l’espace social. Il pense, en effet, à la suite de Max Weber que les sociétés ne se structurent pas seulement à partir de logiques économiques. Bourdieu propose ainsi d’ajouter au capital économique, ce qu’il nomme, par analogie, le capital culturel. Il lui semble, en effet, que dans les sociétés modernes, la quantité de ressources culturelles que possèdent les agents sociaux joue un rôle essentiel dans leur position sociale. Par exemple, la position sociale d’un individu est, pour Bourdieu, tout autant déterminée par le diplôme dont il dispose que par la richesse économique dont il a pu hériter.

Bourdieu construit ainsi une théorie à deux dimensions de l’espace social, qui s’oppose à la théorie unidimensionnelle des marxistes. La première dimension est constituée par le capital économique possédé, la deuxième par le capital culturel. Un individu se situe quelque part dans l’espace social en fonction à la fois du volume total des deux capitaux qu’il possède, mais également de l’importance relative de chacun des deux types de capital dans ce volume total. Par exemple, parmi les individus dotés d’une grande quantité de capitaux, et qui forment la classe dominante d’une société, Bourdieu oppose ceux qui ont beaucoup de capital économique et moins de capital culturel (la bourgeoisie industrielle pour l’essentiel), situés en haut à droite du schéma ci-dessous, aux individus qui ont beaucoup de capital culturel mais moins de capital économique, situés en haut à gauche du schéma (les professeurs d’université, par exemple).

Bourdieu insiste sur le fait que sa vision de l’espace social est relationnelle : la position de chacun n’existe pas en soi, mais en comparaison des quantités de capital que possèdent les autres agents. D’autre part, si Bourdieu pense que capital culturel et capital économique sont les deux types de ressources qui structurent le plus en profondeur les sociétés contemporaines, il laisse la place à tout autre type de ressources, qui peuvent, en fonction de chaque société particulière, occuper une place déterminante dans la constitution des hiérarchies sociales.

Bourdieu, à partir de cette théorie de la hiérarchisation de la société, cherche à comprendre comment se construisent les groupes sociaux. À la différence des marxistes, Bourdieu ne croit pas que les classes sociales existent, en soi, objectivement, conformément à la position dite « réaliste ». Au contraire, si le sociologue peut, à partir des différences de comportements sociaux par exemple, construire des classes sociales « sur le papier », il ne va pas de soi que les individus se considèrent comme en faisant partie. De nombreuses études ont ainsi pu montrer que le nombre d’individus se considérant comme faisant partie de la « classe moyenne » est bien supérieur à celui que l’on aurait à partir d’une définition « objective » de cette appartenance. Toutefois, Bourdieu ne pense pas non plus que les classes sociales n’ont aucune réalité, qu’elles ne sont qu’un regroupement arbitraire d’individus, à la façon de la position « nominaliste ». Bourdieu pense qu’une partie essentielle du travail politique consiste à mobiliser les agents sociaux, à les regrouper symboliquement, afin de créer ce sentiment d’appartenance, et de constituer ainsi des classes sociales « mobilisées ». Mais cela a d’autant plus de chance de réussir que les individus que l’on tente ainsi de réunir sont objectivement proches dans l’espace social.

Pour Bourdieu, les styles de vie des individus sont le reflet de leur position sociale. Ainsi, Bourdieu s’efforce de faire apparaître une forte corrélation entre les manières de vivre, sentir et agir des individus, leurs goûts et leurs dégoûts en particulier, et la place qu’ils occupent dans les hiérarchies sociales. Cette corrélation entre positions sociales et pratiques sociales est illustrée par le diagramme au-dessus, qui fait correspondre à un espace des positions sociales, un espace des pratiques sociales, culturelles et politiques.

L’habitus est une des médiations fondamentales de cette corrélation. Les individus, en vivant un certain type de vie sociale, acquièrent également des dispositions culturelles spécifiques. Ainsi, les ouvriers (cf. "supra") condamnés à une vie où la nécessité économique domine, ont une vision fonctionnelle de la nourriture, qui doit être avant tout nourrissante, ou de l’art, qui ne peut être que réaliste. Ils conçoivent de même leur corps comme un instrument qu’il faut affermir et attendent ainsi de la pratique du sport plus de force physique.

Toutefois, Bourdieu pense que dans cet espace des styles de vie se joue un aspect essentiel de la légitimation de l’ordre social. En effet, dans la mesure où les pratiques sociales sont hiérarchisées et que ces hiérarchies reflètent les hiérarchies sociales sous-jacentes, les styles de vies ont de puissants effets de distinction et de légitimation. Par exemple, les groupes sociaux dominants en aimant des musiques plus valorisées socialement que les groupes sociaux dominés trouvent, dans le même temps, une source de distinction dans leurs goûts. Mais cette distinction est aussi légitimation : les groupes sociaux dominants sont distingués car ils aiment des musiques distinguées.

Pierre Bourdieu pense ainsi qu’une partie de la lutte entre groupes sociaux prend la forme d’une lutte symbolique. Les individus des groupes sociaux dominés s’efforcent, en effet, d’imiter les pratiques culturelles des groupes sociaux dominants pour se valoriser socialement. Toutefois, les individus des groupes sociaux dominants, sensibles à cette imitation, ont alors tendance à changer de pratiques sociales : ils en cherchent de plus rares, aptes à restaurer leur distinction symbolique. C’est cette dialectique de la divulgation, de l’imitation et de la recherche de la distinction qui est, pour Bourdieu, à l’origine de la transformation des pratiques culturelles.

Cependant, dans ces luttes symboliques, les classes dominées ne peuvent être que perdantes : en imitant les classes dominantes, elles en reconnaissent la distinction culturelle ; sans pouvoir la reproduire jamais. « La prétention part toujours battue puisque, par définition, elle se laisse imposer le but de la course, acceptant, du même coup, le handicap qu’elle s’efforce de combler ».

L’espace social est pour Bourdieu relationnel. Il n’y aurait pas de goûts en eux-mêmes vulgaires : s’ils le sont, c’est parce qu’on les oppose à d’autres définis comme distingués. Le golf ne pourrait être distingué s’il n’existait pas d’autres sports, comme le football, auquel on puisse l’opposer. De fait, la distinction des pratiques sociales se modifie avec le temps, essentiellement en fonction de leur adoption par les classes sociales les plus basses.

Le diagramme ci-dessus ne représente donc qu’un moment du lien entre positions sociales et pratiques sociales et culturelles. Ce lien change avec les luttes sociales de distinction. Ainsi, le tennis est aujourd’hui bien moins distingué qu’au moment de la réalisation des enquêtes (qui datent des années 1960) dont est tiré ce graphique. Et, de fait, sa pratique s’est largement vulgarisée au sein de la petite bourgeoisie.

Les styles de vie sont ainsi "objectivement" distingués : ils reflètent les conditionnements sociaux qui s’expriment à travers l’habitus. Mais ils sont aussi le produit de "stratégies" de distinction, par lesquelles les individus visent à restaurer la valeur symbolique de leurs pratiques et goûts culturels à mesure de leur imitation par des groupes sociaux moins privilégiés.

La reproduction de l’ordre social passe, pour Bourdieu, à la fois par la reproduction des hiérarchies sociales et par une légitimation de cette reproduction. Bourdieu pense que le système d’enseignement joue un rôle important dans cette reproduction, au sein des sociétés contemporaines. Bourdieu élabore ainsi une théorie du système d’enseignement qui vise à montrer :

Dans "La Reproduction", Pierre Bourdieu, avec Jean-Claude Passeron, s’efforce de montrer que le système d’enseignement exerce un « pouvoir de violence symbolique », qui contribue à donner une légitimité au rapport de force à l’origine des hiérarchies sociales. Comment cela est-il possible ? Bourdieu croit tout d’abord constater que le système éducatif transmet des savoirs qui sont proches de ceux qui existent dans la classe dominante. Ainsi, les enfants de la classe dominante disposent d’un "capital culturel" qui leur permet de s’adapter plus facilement aux exigences scolaires et, par conséquent, de mieux réussir dans leurs études. Cela, pour Bourdieu, permet la légitimation de la reproduction sociale. La cause de la réussite scolaire des membres de la classe dominante demeure en effet masquée, tandis que leur accession, grâce à leurs diplômes, à des positions sociales dominantes est légitimée par ces diplômes. Comme il le note, « les verdicts du tribunal scolaire ne sont aussi décisifs que parce qu’ils imposent la condamnation et l’oubli des attendus sociaux de la condamnation ». Autrement dit, pour Bourdieu, en masquant le fait que les membres de la classe dominante réussissent à l’école en raison de la proximité entre leur culture et celle du système éducatif, l’école rend possible la légitimation de la reproduction sociale.

Ce processus de légitimation est, pour Bourdieu, entretenu par deux croyances fondamentales. D’une part, l’école est considérée comme neutre et ses savoirs comme pleinement indépendants. L’école n’est donc pas perçue comme inculquant un arbitraire culturel proche de celui de la bourgeoisie – ce qui rend ses classements légitimes. D’autre part, l’échec ou la réussite scolaire sont, le plus souvent, considérés comme des « dons » renvoyant à la nature des individus. L’échec scolaire, processus fondamentalement social, sera donc compris par celui qui le subit comme un échec personnel, renvoyant à ses insuffisances (comme son manque d’intelligence, par exemple). Cette « idéologie du don » joue, pour Bourdieu, un rôle déterminant dans l’acceptation par les individus de leur destin scolaire et du destin social qui en découle.

Ces thèses sont reprises et développées dans "La Noblesse d’État" publié en 1989 en collaboration avec Monique de Saint-Martin. Bourdieu met en avant l’emprise de plus en plus grande de ce qu’il nomme le « mode de reproduction à composante scolaire », qui fait du diplôme un véritable « droit d’entrée » dans les entreprises bureaucratiques modernes, même pour la bourgeoisie industrielle qui s’en est longtemps passé pour transmettre ses positions sociales. Aujourd’hui, presque toutes les classes sociales sont condamnées à assurer l’obtention par leurs enfants de diplômes scolaires à même de reproduire leur position sociale, jusque et y compris les propriétaires d’entreprise, dont les enfants doivent avoir un diplôme pour diriger à leur tour l’entreprise. Cela a transformé profondément le système scolaire, en particulier le champ des grandes écoles du pouvoir. Ainsi, Bourdieu s’efforce de montrer que les grandes écoles traditionnelles, où les compétences scolaires traditionnelles dominent, sont aujourd’hui concurrencées par de nouvelles écoles, proche du pôle dominant du champ du pouvoir. L’École normale supérieure a ainsi perdu sa place dominante au profit de l’ENA. Dans le même temps, des « écoles refuges » (souvent des écoles de gestion comme l’European Business School, pour reprendre l’exemple de Bourdieu), aux exigences scolaires faibles, sont apparues, dont la fonction est de permettre à des enfants issus des classes dominantes d’acquérir des diplômes qu’ils ne peuvent obtenir dans les grandes écoles.

Pierre Bourdieu a, à partir de son appareil conceptuel, abordé l’étude de nombreux sous-champs de la sociologie, comme la sociologie du sport, la sociologie politique, la sociologie religieuse, etc.

Au cours des années 1990, Pierre Bourdieu s'est intéressé de plus près aux médias. Sa sociologie des médias s'est construite principalement autour de la question de la télévision sur laquelle il porte un regard très critique. Dans une approche néanmoins moins académique que dans le reste de ses travaux, Bourdieu développe une analyse du rôle de ce médium dans la sphère sociale et politique donnant lieu à plusieurs publications dont un livre "Sur la télévision" (1996).

À l'issue des grèves de 1995-1996, Pierre Bourdieu est invité par Daniel Schneidermann dans l'émission télévisée "Arrêt sur images" aux côtés des journalistes Jean-Marie Cavada et Guillaume Durand face auxquels il se propose de critiquer le système télévisuel aux travers d'extraits de leurs émissions. Se considérant pris au piège par la « mécanique » de ce médium, il reviendra sur son propre passage télévisé dans un article polémique qui donnera lieu à un échange houleux avec l'animateur, Daniel Schneidermann.

On peut ajouter que Pierre Bourdieu s'est intéressé à l'opinion publique. Dans un article intitulé "L'opinion publique n'existe pas" (accessible en ligne), il critique l'usage des sondages d'opinion publique par les spécialistes, ces sondages influant justement sur l'opinion publique, et cherche à démontrer que les opinions seraient issues des techniques de sondage employées.

L’œuvre de Pierre Bourdieu a été l’objet d’une attention critique toute particulière, à la mesure de son influence dans les sciences sociales. Il est difficile de faire apparaître une seule ligne de force dans ce qui est reproché à un travail étalé sur près de quarante ans. Ces critiques sont venues de diverses écoles de pensées en sciences sociales — des marxistes aux partisans de la théorie de l’acteur rationnel — et ont porté sur des aspects très divers de ce travail.

Une critique domine, toutefois : celle-ci porte sur la nature des déterminations sociales dans la théorie de Pierre Bourdieu, qui sont décrites comme rigides et simplificatrices (critique du « déterminisme »). Toutefois Pierre Bourdieu a rappelé dans de nombreux ouvrages que l"'habitus" est un principe "puissamment générateur" et "d'invention". Plus généralement, l'économiste Robert Boyer a montré que la sociologie de Pierre Bourdieu était très bien armée pour penser les changements du monde social (au-delà des reproductions déterministes qui existent de fait). Le philosophe Jacques Bouveresse rappelle que « Bourdieu a été accusé régulièrement de proposer des analyses du monde social qui ne peuvent conduire qu’au nihilisme et à un sentiment d’impuissance plus ou moins radicale » mais souligne « qu’il cherchait [...] exactement le contraire de cela : une forme d’idéalisme réaliste, appuyé sur la connaissance, plutôt que sur les désirs, les rêves, les grandes idées et les bonnes intentions ».

Le philosophe Jacques Rancière a mis en cause, notamment dans "Le philosophe et ses pauvres", le risque de reconduction infinie de la domination dans la sociologie critique de Pierre Bourdieu et la perte de vue de la perspective d'émancipation. La philosophe Charlotte Nordmann et le sociologue Philippe Corcuff ont chacun proposé, sous des modalités différentes, de penser ensemble les instruments critiques de Bourdieu et les questionnements émancipateurs de Rancière.

Un ancien collaborateur de Pierre Bourdieu, Luc Boltanski a, en se détachant de la sociologie critique de Bourdieu à la fin des années 1980 et au début des années 1990, mis l'accent sur les capacités critiques des acteurs, dans le cadre d'une sociologie pragmatique élaborée avec Laurent Thévenot, appelée aussi « sociologie de la critique ». Plus récemment à la fin des années 2000, Luc Boltanski a proposé de construire un cadre d'analyse s'efforçant d'établir une jonction entre une approche critique comme celle de Bourdieu et une approche pragmatique, comme celle qu'il avait élaborée avec Laurent Thévenot.

Nicole-Claude Mathieu, anthropologue et théoricienne du féminisme matérialiste a critiqué "La domination masculine" dans son texte "Bourdieu ou le pouvoir auto-hypnotique de la domination masculine" : « […] le travail du Candidat manque de rigueur technique, méthodologique, et déontologique. Il pèche par pensée, par action, par omission et par distorsion. L'ensemble est à interpréter comme un refus de laisser place à la confrontation entre différentes analyses, ce qui donne à la thèse un statut d'assertion et non de démonstration ».

L’anthropologue, sociologue et philosophe Bruno Latour a remis en question la posture surplombante adoptée par Bourdieu et les sociologues critiques, les acteurs sociaux étant considérés comme de simples informateurs dépourvus de réflexivité. « Le prétexte qui permet aux chercheurs d’occuper le point de vue de nulle part, celui de Dieu, vient généralement de ce qu’ils prétendent faire de façon "réflexive" ce que les acteurs feraient "sans y prêter attention" ».









</doc>
<doc id="19163" url="https://fr.wikipedia.org/wiki?curid=19163" title="Coussin de Lyon">
Coussin de Lyon

Le coussin de Lyon est une confiserie lyonnaise, créée dans les années 1960 par le chocolatier Voisin.

En l'an 1643, lors de la terrible épidémie de peste qui ravageait la cité, les échevins lyonnais firent le vœu d'organiser une procession sur la colline de Fourvière pour implorer la Vierge d'épargner la ville. Ce faisant, ils remirent un cierge de sept livres de cire et un écu d’or présenté sur un coussin de soie. Chaque année depuis, les magistrats de Lyon renouvellent le vœu des échevins en se rendant à Fourvière, tandis que retentissent les trois coups de canon annonçant que le vœu a été respecté.

C'est ce coussin de soierie qui a inspiré la création du coussin de Lyon.

En 1960, certains soyeux imaginèrent une boîte rappelant la forme et l'allure du coussin de soie historique et le chocolatier Voisin, installé à Lyon depuis 1897, reprit l'idée d'utiliser la forme du coussin pour créer sa confiserie. Vite connu dans la ville, il fit peu à peu la conquête de la France et . Aujourd'hui, il n'est pas rare de le rencontrer dans le monde entier lors des manifestations de prestige et de qualité des produits lyonnais.

Le coussin de Lyon est une confiserie à base de chocolat et de pâte d'amande. Cette friandise est un carré de pâte d'amande candie, vert pâle avec un filet vert foncé, fourré d'une ganache de chocolat parfumé au curaçao.

On peut se procurer les coussins à la pièce et également dans des boîtes en velours, rappelant la forme originelle du coussin de soie.

Le coussin de Lyon a été créé par le chocolatier lyonnais Voisin dans les années 1960 et bénéficie depuis de la fabrication exclusive de la spécialité. Le gérant de la société Voisin, Paul Boucaud, indique que la société fabrique quatre-vingt-cinq tonnes de coussins de Lyon par an et que cette fabrication augmente de 10 % chaque année. L'entreprise exclut le recours à la grande distribution pour commercialiser son produit.



</doc>
<doc id="19165" url="https://fr.wikipedia.org/wiki?curid=19165" title="Unification">
Unification

En informatique et en logique, l'unification est un processus algorithmique qui, étant donnés deux termes, trouve une substitution qui appliquée aux deux termes les rend identiques. Par exemple, formula_1 et formula_2 peuvent être rendus identiques par la substitution formula_3 et formula_4, qui donne quand on l'applique à chacun de ces termes le terme formula_5. Dit autrement, l'unification est la résolution d'une équation dans l'algèbre des termes (unification syntaxique) ou dans une algèbre quotient par un ensemble d'identités (unification modulo une théorie) ; la solution de l'équation est la substitution qui rend les deux termes identiques et que l'on appelle l'unificateur.
L'unification a des applications en inférence de types, programmation logique, en démonstration automatique de théorèmes, en système de réécriture, en traitement du langage naturel.

Souvent, on s'intéresse à l'unification syntaxique où il faut que les termes obtenus par application de l'unificateur soient syntaxiquement égaux, comme dans l'exemple ci-dessus. Par exemple, le problème d'unification syntaxique ayant pour données formula_1 et formula_7 n'a pas de solution. Le filtrage par motif (ou "pattern matching") est une restriction de l'unification où l'unificateur n'est appliquée qu'à un seul des deux termes. Par exemple, formula_1 et formula_5 sont rendus égaux par la substitution formula_3.

La fin de l'article présente aussi l'unification modulo une théorie, qui est le cas où on dispose de connaissances supplémentaires sur les fonctions (par exemple, formula_11 est commutatif).

Le premier chercheur à évoquer un algorithme d'unification est Jacques Herbrand dans sa thèse en 1930. Il s'agit d'un algorithme non-déterministe pour unifier deux termes. Herbrand était intéressé par résoudre des équations. En 1960, Prawitz et Voghera ont généralisé le calcul propositionnel à des formules du premier ordre non instanciées, ou tout au moins en les instanciant a minima. L'unification a ensuite été redécouverte en 1963, mais l'article qui la décrit publié seulement en 1965, par dans le cadre de sa méthode de résolution en démonstration automatique. Selon Baader et Snyder, c'est Knuth et Bendix, en 1970, qui ont introduit le concept de "substitution la plus générale" dans leur travail sur la confluence locale d'un système de réécriture. Un an avant la publication de l'article de Robinson, soit en 1964, Guard a indépendamment étudié le problème d'unification sous le nom de "matching".

L'algorithme originel de Robinson est inefficace car il s'exécute en temps exponentiel et demande une quantité de mémoire exponentielle. Robinson écrit alors une note en 1971 où il exhibe une représentation des termes plus concise. L'algorithme utilise alors un espace mémoire polynomial. Boyer et Moore donnent aussi un algorithme qui utilise un espace mémoire polynomial en temps exponentiel dans le pire cas. Venturini-Zilli introduit un système de marquage pour que l'algorithme de Robinson s'exécute en temps quadratique, dans le pire cas, en la taille des termes. Huet travaille sur l'unification d'ordre supérieur et améliore le temps d'exécution de l'algorithme syntaxique du premier ordre ː son algorithme est quasi-linéaire. D'autres chercheurs ont exhibé des algorithmes quasi-linéaires.

Une étape importante est ensuite la découverte d'algorithmes linéaires en 1976 par Martelli et Montanari, Paterson et Wegman et Huet. Comme les articles de Paterson et Wegman sont courts, Dennis de Champeaux a réexpliqué l'idée de l'algorithme en 1986.

En 1982, Martelli et Montanari présente un algorithme presque-linéaire mais plus efficace en pratique.

En 1984, Dwork et al. démontrent que l'unification est P-complet, ce qui signifie qu'a priori l'unification est difficile à paralléliser. Par opposition, le problème de filtrage par motif (pattern matching) est dans NC, c'est-à-dire facile à paralléliser.

Prolog a aussi beaucoup contribué à sa popularisation. Des algorithmes spécifiques pour les termes de certaines théories, appelés aussi unification équationnelle (en particulier associative et commutative par Stickel) ont été proposés (voir ci-dessus).

Un problème d'unification peut être présenté comme la donnée de deux termes, ou alors comme une équation, ou alors comme un ensemble de termes à unifier, ou alors comme un ensemble de couples de termes, ou alors comme un ensemble d'équations. Le problème d'unification formula_12 a une solution formula_13 car substituer (i.e. remplacer) le terme formula_14 à la variable formula_15 et le terme formula_16 à la variable formula_17,
dans les deux termes formula_18 et formula_19 donne le même terme formula_20. En revanche, le problème d'unification formula_21 n'a pas de solution et le problème d'unification formula_22 non plus.

Un problème d'unification peut avoir une infinité de solutions. Par exemple, formula_23 a une infinité de solutions : formula_24, formula_25, formula_26, etc. De toutes ces solutions, on exhibe des solutions dites principales car elles permettent de construire toutes les autres solutions : c'est le concept de solution principale, ou unificateur le plus général (most general unifier, mgu).

Tout d'abord, une solution formula_27 d'un problème d'unification est dite "plus générale" qu'une solution formula_28, si formula_28 est obtenue en substituant des termes à des variables dans formula_27. Par exemple, si on considère le problème d'unificationformula_23, alors la solutionformula_32est plus générale que la solutionformula_33qui est obtenue en substituant le terme formula_14 à la variable
formula_35 dans formula_27. De même, la solutionformula_37est plus générale que la solution formula_27, qui est obtenue en substituant le terme formula_39 à la variable formula_40 dans la solution formula_41.

Une solution formula_42 d'un problème d'unification est dite "principale" si elle est plus générale que toutes les solutions de ce problème, c'est-à-dire si toute solution est obtenue en substituant des termes à des variables dans formula_42. Dans cet exemple, la solution formula_41 est une solution principale du problèmeformula_23.

Le "théorème de la solution principale" exprime que si un problème d'unification a une solution, alors
il a une solution principale. Un algorithme d'unification calcule une solution principale ou échoue si les termes ne sont pas unifiables.

La table suivante donne des exemples de problèmes d'unification. En plus, nous donnons aussi, dans la colonne de gauche, les exemples avec la syntaxe de Prolog ː une variable commence par une majuscule, les constantes et symboles de fonction commencent par une minuscule. Pour les notations mathématiques, "x,y,z" sont des variables, , "f,g" sont des symboles de fonction et "a,b" sont des constantes.

Le "filtrage par motif" est la restriction de l'unification dans laquelle, dans chaque équation, les variables n'apparaissent que dans le terme de gauche. Par exemple, le problèmeformula_46est un problème de filtrage, car le terme formula_47 ne contient pas de variables. Le filtrage est utilisé dans les langages fonctionnels comme Haskell, Caml, LISP.

Dans le cas du filtrage, l'algorithme de Martelli et Montanari se simplifie.

On choisit une équation dans le système.

Si cette équation a la forme

formula_48

on la remplace par les équations
formula_49, ..., formula_50 et on résout le système obtenu.

Si cette équation a la forme

formula_51

où formula_52 et formula_53 sont des symboles différents, on échoue.
Si cette équation a la forme

formula_54

on substitue le terme formula_55 à la variable formula_15 dans le
reste du système, on résout le système obtenu, ce qui donne une solution formula_42 et on
retourne la solution formula_58.

L'unification est ce qui distingue le plus le langage de programmation "Prolog" des autres langages de programmation.

En "Prolog", l’unification est associée au symbole « = » et ses règles sont les suivantes :

En raison de sa nature déclarative, l'ordre dans une suite d'unifications ne joue aucun rôle.

L'algorithme de Robinson de 1965 a été reformulé, par Corbin et Bidoit en 1983
, puis amélioré par Ruzicka et Prívara en 1989,
puis repris par Melvin Fitting en 1990 comme un algorithme non-déterministe, présentation également reprise par Apt en 1996. Nous présentons cette version également. L'algorithme prend en entrée deux termes t1 et t2 à unifier, donnés sous la forme de deux arbres. Il repose sur la notion de "disagreement pair" qui est la donnée d'un sous-terme de t1 et d'un sous-terme de t2, dont les nœuds dans les arbres de t1 et t2 sont à la même position depuis les racines mais avec des étiquettes différentes. Une telle "disagreement pair" est "simple" si l'un des sous-termes est une variable qui n'apparaît pas dans l'autre sous-terme. L'algorithme calcule une substitution, initialement vide. L'algorithme continue tant que la substitution ne rend pas les deux termes égaux. Il choisit de façon non-déterministe un "disagreement pair". S'il n'est pas simple, l'algorithme échoue. Sinon, il enrichit la substitution.
En 1982, Martelli et Montanari décrivent un algorithme sous la forme d'un ensemble de règles qui transforment un ensemble d'équations. L'algorithme est présenté dans des ouvrages pédagogiques. Il est similaire à l'algorithme proposé par Herbrand dans sa thèse. Selon Baader et Snyder, un algorithme sous la forme d'un ensemble de règles dégage les concepts essentiels et permet de démontrer la correction de plusieurs algorithmes pratiques en même temps.

On se donne un ensemble fini d'équations "G" = { "s" ≐ "t", ..., "s" ≐ "t" } où les "s" et "t" sont des termes du premier ordre. L'objectif est de calculer une substitution la plus générale. On applique alors les règles suivantes à l'ensemble G jusqu'à épuisement :

La règle supprimer supprime une équation "t" ≐ "t", c'est-à-dire où les termes de la partie gauche et de la partie droite sont les mêmes. La règle décomposer supprime une équation de la forme "f"("s"...,"s") ≐ "f"("t"...,"t") et la remplace par les équations "s" ≐ "t", ..., "s" ≐ "t". La règle échanger oriente les équations pour que la variable x soit en partie gauche. En présence d'une équation "x" ≐ "t" où la variable x n'apparaît pas dans le terme t, la règle éliminer remplace les occurrences de x par t dans les autres équations.

Les règles suivantes sont également ajoutées en guise d'optimisation :

Si l'ensemble contient une équation de la forme "f"(s⃗) ≐ "g"(t⃗) où f et g ne sont pas les mêmes symboles de fonctions ou alors si le nombre d'arguments n'est pas le même la règle conflit fait échouer le processus d'unification. La règle vérifier (occurs-check), quant à elle, fait échouer l'unification si l'ensemble contient une équation "x" ≐ "f"(s⃗) où x apparaît dans "f"(s⃗).

L'algorithme est en temps exponentiel et demande un espace mémoire au plus exponentiel si l'on représente les termes par leurs arbres syntaxiques. Néanmoins, on peut n'utiliser qu'un espace mémoire linéaire si on représente les termes par des graphes.

En implémentant l'algorithme avec des graphes, l'espace mémoire est linéaire en la taille de l'entrée même si le temps reste exponentiel dans le pire des cas. L'algorithme prend en entrée deux termes sous la forme de graphes, c'est-à-dire un graphe acyclique où les nœuds sont les sous-termes. En particulier, il y a un unique nœud par variables (cf. figure à droite). L'algorithme retourne en sortie une substitution la plus générale ː elle est écrite en place dans le graphe représentant les termes à l'aide de pointeurs (en bleu dans la figure à droite). c'est-à-dire, en plus du graphe décrivant la structure des termes (qui sont des pointeurs aussi), nous avons des pointeurs particuliers pour représenter la substitution. Par si x ː= h(1) est la substitution courante, x pointe vers le nœud correspondant au terme h(1).

où unifierListes unifient les termes de deux listes ː
où find trouve la fin d'une chaîne ː
et où une implémentation naïve de "x apparaît dans t" est donné par ː
L'algorithme présenté dans cette sous-section est dû à Corbin et Bidoit (1983). Pour avoir une complexité quadratique, il y a deux améliorations de l'algorithme précédent.

L'implémentation pour tester si une variable x apparaît dans un sous-terme t est a priori en temps exponentiel. L'idée est d'éviter de parcourir plusieurs fois les mêmes nœuds. On marque les nœuds visités comme dans un parcours en profondeur de graphe. Une fois un test d'appartenance effectué, il faut a priori démarquer les nœuds. Au lieu de cela, on les marque avec le "temps actuel". On ajoute alors un champ aux nœuds du graphe que l'on appelle poinçon qui contient le "temps actuel". Nous disposons d'une variable globale "temps actuel" qui est incrémenté à chaque test d'appartenance.

Pour éviter des appels inutiles à la procédure qui cherche à unifier deux termes, on utilise des pointeurs pour tous les nœuds et pas seulement les variables. On peut montrer que le nombre d'appels à la procédure qui unifie est O(|A|) où |A|`est le nombre d'arcs dans le graphe. Le traitement interne utilise un parcours de pointeurs "find" en O(|S|) où |S| est le nombre de sommets et un test d'appartenance d'une variable dans un terme qui est en O(|S|+|A|) = O(|A|) car le graphe est connexe. Donc l'algorithme est bien quadratique.

et
L'algorithme est inspiré de l'algorithme quadratique de la section précédente. Le test de savoir si x apparaît dans t n'est plus effectué au cours de l'algorithme mais uniquement à la fin ː à la fin, on vérifie que le graphe est acyclique. Enfin, les pointeurs de la substitution et "find" sont implémentés à l'aide d'une structure de données Union-find. Plus précisément, on conserve les pointeurs dans la structure mais on dispose en plus une structure annexe Union-find. Dans la structure de données Union-find, les classes d'équivalence ne contiennent soit que des variables soit que des termes complexes. Pour passer d'une variable à un terme complexe, on utilise les pointeurs éventuels qui sont dans le graphe.

Pour construire un graphe à partir d'un arbre, on parcourt l'arbre et on utilise une table de symboles (implémenté avec une table de hachage ou un arbre binaire de recherche) pour les variables car il faut garantir l'unicité du nœud x pour une variable x.
L'unification modulo une théorie, aussi appelée (unification équationnelle, E-unification, unification dans une théorie) est l'extension de l'unification syntaxique dans les cas où les opérateurs sont assujettis à des axiomes, formant une théorie E. Généralement cette théorie est décrite par un ensemble d'égalités universelles. Par exemple, une théorie E peut contenir l'identité formula_59 où les variables formula_60 et formula_61 sont implicitement quantifiées universellement et qui dit que l'opérateur formula_62 est commutatif. Dès lors, bien que les termes formula_63 et formula_64 ne soient pas syntaxiquement unifiables, ils sont E-unifiables ː

L'E-unification est décidable pour une théorie E s'il existe un algorithme pour cette théorie qui termine sur chaque entrée et résout le problème d'E-unification. Il est semi-décidable s'il existe un algorithme termine pour les entrées qui ont une solution et qui peut boucler pour des équations sans solution.

L'E-unification est décidable pour les théories suivantes ː
L'E-unification est semi-decidable pour les théories suivantes ː

Si on se place en logique d'ordre supérieur, c'est-à-dire si on s'autorise à utiliser des variables comme symboles de fonction ou comme prédicats, on perd la décidabilité et l'unicité de l'unificateur quand il existe. Au pire, deux termes peuvent même avoir une infinité d'unificateurs tous différents, dans le sens suivant: soit "t" et "t´" deux termes qu'on veut unifier, il peut exister un ensemble infini "U" d'unificateurs de "t" et "t´" tel que si σ et ρ sont dans "U", alors σ n'est pas plus plus général que ρ et ρ n'est pas plus général que σ.


</doc>
<doc id="19175" url="https://fr.wikipedia.org/wiki?curid=19175" title="Placebo (groupe)">
Placebo (groupe)

Placebo est un groupe de rock alternatif britannique, originaire de Londres, en Angleterre. Il est formé en 1994 par Brian Molko et Stefan Olsdal, qui sont d'abord accompagnés par Robert Schultzberg jusqu'en 1996, puis Steve Hewitt jusqu'en 2007, et Steve Forrest jusque début 2015.

Le style musical et les textes de Placebo se distinguent par une inclination romantique exacerbée. Autrefois allié à une imagerie provocante et excessive alimentant l'image sulfureuse du groupe, ce romantisme exubérant s'inscrit désormais dans une attitude plus introvertie (voir le concept Placebo). Opérant un changement progressif de son image et de sa musique, le groupe peut être affilié au punk rock pour ses débuts, puis au rock alternatif et à la power pop.

Placebo gagne une reconnaissance internationale, ayant vendu en 2011 plus de onze millions d'exemplaires de ses albums à travers le monde, et est connu du grand public pour ses titres "Nancy Boy", ', ', "", ', "Meds", ' ou encore "", et certaines reprises comme "Johnny and Mary" (Robert Palmer) ou " (Kate Bush). 

Le septième album du groupe, ", est sorti le 16 septembre 2013.

Les tout premiers balbutiements de ce qui deviendra Placebo se font entendre au début des années 1990 à Deptford dans le Grand Londres où les musiciens Brian Molko (écossais et américain) et Steve Hewitt (anglais) se réunissent pour jouer par intermittence dans des pubs. Ces derniers se sont rencontrés au Burger King de Lewisham par l'intermédiaire d'une amie commune. Autodidacte, Brian Molko perfectionne son jeu de guitare depuis ses seize ans et en parallèle de ses études d'art dramatique au "Goldsmith's College" de Londres. Quant à Steve Hewitt, il joue de la batterie depuis l'âge de onze ans et possède un bagage musical conséquent, grâce aux nombreux groupes auxquels il a participé jusqu'alors.

C'est en janvier 1994 que les choses prennent une véritable tournure avec la rencontre entre Brian Molko et Stefan Olsdal (Suédois) à la station de métro "South Kensington" de Londres. Anciens camarades de classe à l'American International School of Luxembourg au Luxembourg, ils n'y ont pourtant jamais eu de véritables liens mais la guitare que transporte Stefan Olsdal, alors étudiant au ' de Londres, est néanmoins un sujet de discussion. Brian Molko finit par l'inviter à assister à la prestation qu'il donne le soir même en compagnie de Steve Hewitt sous le nom d'. Celle-ci a lieu au ' de Deptford et Stefan Olsdal bien que dubitatif au préalable est conquis par la voix du chanteur-guitariste et lui propose de travailler avec lui sur la formation d'un groupe de rock.

Les deux jeunes hommes, qui conservent provisoirement le nom "", montent des premières maquettes puis les enregistrent à l'aide d'instruments et un magnétophone de fortune d'abord en compagnie de Steve Hewitt. Mais ce dernier se voit obligé d'honorer les contrats qui le lient à d'autres groupes. C'est pourquoi en octobre 1994, le suisse Robert Schultzberg, ami d'enfance de Stefan Olsdal, comble la place vacante et le trio se rebaptise Placebo, ce qui signifie en latin « Je te plairai ».

La formation choisit ce nom car le mot sonne bien, qu'il est prononçable dans plusieurs langues et surtout parce qu'un placebo n'est qu'un leurre, une façon pour le groupe de se démarquer de la tendance des années 1990 d'appeler les groupes par des noms de médicaments en référence à « The Cure ». Les premières démos sont enregistrées au mois de mars 1995 aux studios ' de Deptford ; ainsi voient entre autres le jour "Nancy Boy", ' et '. Avec une tendance punk, les compositions restent techniquement simples à l'image des courants des années 1980 prônant le ' et privilégient l'énergie et l'émotion : des caractéristiques qui resteront la marque de fabrique de Placebo qui se trouve alors aux antipodes du courant britpop des années 1990.

Les maisons de disques anglaises subodorent la bonne affaire et se pressent pour faire signer Placebo qui préfère temporiser. Le groupe gagne sa place sur une compilation après un concours et présente ' sur un 45 tours édité par le label Fierce Panda Records dès le 30 octobre 1995. Quatre mois plus tard, le 5 février 1996, Placebo sort son premier véritable single "" chez Deceptive Records avant de signer pour cinq albums avec un autre label, Hut Recordings, sous label de Virgin Records (EMI), en créant au passage son propre label Elevator Music. Dans la foulée, Placebo qui n'en attendait pas tant, est invité par David Bowie pour assurer la première partie de ses concerts sur l", ce qui permet au groupe d'évoluer pour la première fois dans de grandes salles. Quelques mois plus tard, la formation sort son premier album sobrement intitulé "Placebo", le . Le "" compare alors le groupe .

L'album, produit par Brad Wood, connaît un succès d'autant plus rapide que la presse à scandale britannique s'empare des paroles subversives et provocantes ainsi que de la personnalité de leur auteur Brian Molko qui, par une androgynie dessinée au maquillage et une voix nasillarde et traînante, suscite de nombreuses réactions, aussi bien d'admiration que de rejet. "", entrevues irrévérencieuses, ambiances glauques, un cocktail efficace qui, associé à un rock incisif et pressant qualifié de punk rock, conquiert le public. Auteur des paroles, Brian Molko décharge ici toute la frustration emmagasinée pendant deux années de chômage, donnant un aspect émotionnel aux compositions dégageant beaucoup d'urgence et de colère. Les textes apparaissent autobiographiques bien qu'ils soient sans aucun doute romancés. Pour Anne-Claire Norot, des "Inrockuptibles", 

Glamour et décadence sont les ingrédients qui séduisent les nouveaux fans. Dans ce registre, le titre "Nancy Boy" sort en janvier 1997 et atteint la quatrième place du hit-parade britannique ; ce qui en fait aujourd'hui encore le plus grand succès commercial d'un single de Placebo dans ce pays, égalé par la suite par '. Ce morceau devient un hymne symbolisant les débuts du groupe, accompagné de ' et '. Sur scène, ces chansons ont le point commun de dégager un son très ' recouvrant la voix particulière de Brian Molko s'acharnant sur sa guitare surnommée "Bitch", une "Fender Jaguar" rouge et blanche. À ses côtés, Stefan Olsdal ne joue alors qu'exclusivement de la basse, instrument prenant une place prépondérante dans la musique du trio formé sur le concept basse, guitare, batterie. Cependant, le caractère haut en couleur du leader Brian Molko et celui de Robert Schultzberg ne s'accordent plus ; un différend entre les deux hommes existe depuis septembre 1995 mais, soucieux de ne pas porter atteinte à la pérennité du groupe, un consensus tacite assure la formation jusqu'à la sortie de l'album et sa promotion. La situation se dégrade lors de l'été 1996 et Robert Schultzberg est alors prié de quitter le groupe le plus rapidement possible. Steve Hewitt est alors tout désigné pour prendre la relève et reprend ainsi du service à l'aube de la première véritable tournée le . La composition du groupe reste dès lors inchangée jusqu'en 2007.

Après deux années de concerts incessants notamment aux côtés de David Bowie au Madison Square Garden pour fêter son cinquantième anniversaire en janvier 1997 et en première partie de U2 en septembre 1997 au Parc des Princes et au Stade olympique Lluís-Companys, l'album "" (« sans toi je ne suis rien ») produit par Steve Osborne sort le 12 octobre 1998. Contrairement à son prédécesseur qui faisait la part belle aux provocations et à l'exubérance propre à la frustration adolescente, "" met l'accent sur l'intimité, l'amour et l'amitié principalement inspirées des frasques relationnelles du leader Brian Molko, qui explore des sentiments plus adultes tels que ce que Charles Baudelaire appelait spleen notamment sur le titre "". Le meneur démontre son savoir-faire en matière d'écriture et pose une voix bien plus sûre que par le passé. Musicalement, les compositions évoluent vers plus de finesse et entrent dans le rock alternatif, mélange moderne de punk, de pop et de rock, inspiré des groupes « indépendants » des années 1980. L'arrivée de Steve Hewitt a incontestablement recadré le groupe et lui a insufflé une grande bouffée d'expérience. Pour la nouvelle tournée qui s'annonce, William Lloyd, véritable ami, jusque-là homme à tout faire chez Placebo, monte sur scène pour donner de l'ampleur aux nouvelles compositions en tant que guitariste ou bassiste, permettant ainsi à Stefan Olsdal d'alterner entre ces deux instruments sur scène.

Apparaissant comme un lendemain de fête, le second album est l'occasion pour Placebo de faire table rase de ses excès de jeunesse et de dévoiler toutes ses qualités musicales et artistiques avec des morceaux comme "Pure Morning" qui décroche la quatrième place du hit-parade britannique, égalant ainsi le record établi par "Nancy Boy" un an plus tôt. Le morceau ' est l'occasion d'accueillir la voix de David Bowie qui aura soutenu et aidé Placebo depuis les tout débuts de leur carrière et les invite à nouveau à chanter avec lui aux Brit Awards 99. Une filiation toute désignée car Placebo arbore un look résolument provocateur et androgyne : robes, jupes, collants et maquillage sont les attributs du look Placebo sur scène ; ce qui vaut parfois au groupe une affiliation au glam rock. Stefan Olsdal et Brian Molko sont alors au paroxysme de l'époque « travestis » de Placebo (voir le concept Placebo) ; ce dernier ayant fait du théâtre il exploite au maximum l'image pour créer un aspect dramatique et retranscrire sur scène, lors de la nouvelle tournée, l'ambiance douloureuse qui caractérise les chansons de l'album comme ' ou la sensualité d" qui devient un tube ainsi que la bande originale du film "Sexe Intentions". Toujours dans ce registre, Steven Hewitt, Brian Molko et Stefan Olsdal participent au tournage du film ' sur le glam rock et y tiennent chacun un rôle en y interprétant "" des T. Rex. Placebo produit luxure, androgynie, musique et textes chevillés au corps, riches dans le fond et la forme.
En deux albums, Placebo s'est imposé dans le paysage musical et a prouvé que le rock n'était pas mort face à l'hégémonie de la techno en cette fin des années 1990. À cet égard il suscite un grand espoir quant au renouveau de la scène britannique. Dépositaire d'un son et d'une attitude qui a fait scandale, même s'il s'est toujours défendu de nourrir les clichés du rock, Placebo et notamment Brian Molko entretient des relations impudentes avec la presse et le '. Mais fort du succès rencontré par son second album, Placebo s'en va défendre bec et ongle son troisième, ', écrit à la fin de l'année 1999, enregistré en 2000 sous la production de Paul Corkett aux studios Olympic de Londres et sorti le .

Dès 1998, Placebo avait prévenu ses fans qu'il se sentait l'envie d'intégrer de nouveaux styles à sa musique et c'est chose faite dans ce dernier opus. Car ici le groupe n'hésite pas à incorporer de l'electro et du hip-hop voire des ambiances jazz. Poussé par le contexte économique et social de fin de siècle, Placebo oublie un instant son nombril pour jeter un regard sur la société qui l'entoure. L'album est donc bien plus hétérogène que son prédécesseur : ballades pop à textes polémiques pour ' ou ', et titres rock à souhait pour "", se mêlent équitablement aux titres cafardeux et romantiques. Contrairement à l'accoutumée, les textes de Brian Molko ne sont plus dirigés à son encontre mais à celle du genre humain : une nouvelle thérapie pour Placebo. Neuf mois sont nécessaires à l'enregistrement de cet album au son plus métallique que ses prédécesseurs et volontairement provocateur à l'égard de la religion et de la politique. Les fans de la première heure renâclent devant cette nouvelle orientation musicale mais le groupe trouve un nouveau public et l'album est un grand succès commercial avec rapidement 3 millions de disques vendus. Cependant, les critiques rencontrées par ce troisième opus sont bien moins dithyrambiques que pour ".

La tournée débute le au Zodiac Club d'Oxford pour un concert privé réservé au fan club '. S'ensuit la plupart des festivals estivaux et des concerts jusqu'en octobre 2001, de par le monde où, sur scène, Brian Molko et Stefan Olsdal ont troqué leurs robes et collants pour des pantalons et manteaux de cuir assortis à l'ambiance qui s'impose sur '.

L'année 2002 est consacrée à l'écriture et à l'enregistrement du quatrième album. Cependant Brian Molko se fait remarquer en tant que DJ et participe au projet "Trash Palace" de Dimitri Tikovoï : collaboration éclectique d'artistes visant à remettre le « trash » à la mode, dans une ambiance de bordel.
Placebo revient au-devant de la scène le 24 mars 2003 avec ' (« dormir avec des fantômes ») produit par Jim Abbiss. L'électronique expérimentée depuis le ' de ', fait maintenant partie intégrante de presque tous les morceaux et pousse le groupe à engager un musicien supplémentaire afin d'exploiter au mieux les nouveaux aspects de sa musique. C'est Xavior Roide, membre du groupe Dex Dexter, qui avait joué aux côtés de Placebo dans ' qui est engagé pour les claviers et les effets. Cependant, dès les premiers concerts de la tournée, les titres de "" revêtent un timbre beaucoup plus rock que dans leur version studio.

Le rock caustique de Placebo est devenu un rock épuré. Avec des morceaux rapides et dynamiques comme ', ', mais aussi des titres plus doux et plus subtils comme ' ou ' ("Protège-moi" dans la version française, qui deviendra la bande-son du film français ""), Placebo tourne en boucle sur les radios européennes. Il s'agit ainsi d'un continent sur lequel le groupe devient l'un des acteurs majeurs de la scène rock, en cultivant son ambiguïté. Les ventes explosent, dopées par une promotion sans égale dans la carrière du groupe, surtout en France où il rencontre un très net succès depuis ses débuts : "" se classe à la première place du classement de ventes d'albums et est certifié double disque de platine.

C'est également à ce moment que s'effectue un renouvellement du public du groupe: les fans de la première heure désabusés par la commercialisation de Placebo s'en vont et de nouveaux arrivent, aguichés par les passages récurrents des nouveaux tubes de Placebo sur les ondes. Du côté de la presse, on déplore que Placebo fasse du « sur place » depuis son deuxième album et ne parvienne pas à en trouver un digne successeur dans l'innovation plutôt que dans l'imitation.

La tournée qui s'ensuit est un véritable succès et c'est le concert au Palais omnisports de Paris-Bercy du 18 octobre 2003, à l'occasion duquel le groupe invite Frank Black sur scène pour une reprise de ', qui est choisi pour y enregistrer le premier album live du groupe, (« les âmes sœurs se meurent jamais »), commercialisé en DVD en 2004. Saisissant l'opportunité, la maison de disques sort une compilation de singles intitulée ' (« une fois de plus, avec sentiment»), accompagnée de deux nouveaux titres. L'opération pousse Placebo à poursuivre sa tournée jusqu'au mois d'avril 2005 en Amérique du Sud.

Trois mois après la fin de sa tournée sud-américaine, Placebo remonte sur scène le 2 juillet à Paris à l'occasion du Live 8 puis s'accorde une période de repos avant l'enregistrement du cinquième album. Le repos est de courte durée car 2005 et 2006 sont des années très riches en collaborations. Ainsi, Brian Molko a pu apparaître aux côtés de Jane Birkin, Timo Maas et Indochine ; Stefan Olsdal a quant à lui pu s'investir dans son projet personnel, Hôtel Persona. Placebo a également participé à une compilation en souvenir de Serge Gainsbourg, en réinterprétant "The Ballad of Melody Nelson". Lors du show pour le de Canal+, Placebo est au côté de The Cure pour y interpréter le titre de ce groupe, "". Brian Molko a toujours déclaré qu'il a été fortement influencé par ce groupe et a d'ailleurs interviewé Robert Smith en 2001 pour le magazine français "les Inrockuptibles".

Après avoir composé la plupart des nouveaux titres dans le sud de la France, le groupe se réunit aux studios RAK de Londres et entame l'enregistrement du prochain album qui ne durera que quatre mois : une des raisons pour lesquelles ce cinquième album, "Meds", est annoncé comme un réel retour aux sources car enregistré à l'instinct, sans fioriture ni ordinateur qui avaient livré le précédent sous anesthésie. Celui-ci se veut plus accessible et universel, beaucoup moins ésotérique que ses prédécesseurs. Sorti le 13 mars 2006 il est produit par Dimitri Tikovoi avec qui le groupe avait déjà enregistré différentes Face B et qui a produit "Trash Palace", album sur lequel Brian Molko était apparu. 

Finis la provocation et les gadgets, la drogue et le sexe : changement d'orientation ou de stratégie donc, le groupe veut faire oublier son image de « travestis ». L'album se clôt sur le titre ' dans lequel Placebo tire un trait sur son passé sous forme d'une lettre d'adieu. Dans un documentaire intitulé ', le groupe explique comment ce nouvel album marque un nouveau départ en « [tuant] la chose pour laquelle [il] est le plus célèbre ». Encensé par la presse grand public mais ne rencontrant pas le succès commercial escompté, " est loin de faire l'unanimité dans la presse spécialisée ni parmi les fans qui pour certains le trouvent assurément insipide et accompagné de textes bien moins sophistiqués que par le passé. Mais maintenant que le groupe a refermé les portes sur son passé, celui-ci semble fin prêt à se tourner vers l'avenir. À ce sujet, Stefan Olsdal confie au tout début de l'année 2007 : .

Sur scène, Xavior Roide cède sa place à Alex Lee, ancien membre du groupe Suede. Alors que Xavior Roide ne s'occupait que des claviers, Alex Lee joue également de la guitare et s'occupe notamment des effets et parties rythmiques, portant à trois le nombre de guitares sur scène pour certains titres. De par le monde, le public est au rendez-vous d'une tournée dont les prestations sont pourtant données avec de moins en moins d'entrain, révélant une véritable lassitude sur scène et au sein du groupe. Au cours de l'année 2007, Placebo participe au festival itinérant " à travers les États-Unis où on le voit relégué en première partie de premières parties devant des publics clairsemés visiblement peu intéressés par le groupe agacé donnant des concerts expéditifs et qui s'efforce en vain depuis des années à percer outre-Atlantique.

Quelques mois plus tard, alors que des rumeurs courent sur d'éventuels projets solitaires de Steve Hewitt et Brian Molko, le départ de Steve Hewitt est révélé le octobre 2007. Presque onze ans jour pour jour après son arrivée au sein du groupe, le batteur s'en va pour . Brian Molko confie alors qu'.

En 2009, Brian Molko et Stefan Olsdal reviennent sur cette séparation. Brian Molko analyse que c'était car . Et pour Stefan Olsdal, 

D'octobre 2007 à août 2008, les rumeurs vont bon train puisqu'aucune nouvelle officielle n'émane du groupe hormis celles concernant Hôtel Persona, le projet personnel de Stefan Olsdal qui se concrétise par la sortie d'un premier album. Cependant, il n'a jamais été question de mettre un terme à Placebo car comme l'explique Brian Molko : .

Les raisons de ce silence s'expliquent en coulisses, car depuis janvier 2008 Brian Molko et Stefan Olsdal font connaissance avec Steve Forrest (Américain, Californien). Ancien batteur du groupe Evaline, il avait eu l'occasion d'ouvrir les concerts de la tournée américaine de Placebo en octobre 2006. Ayant appris le départ de Steve Hewitt et étant lui-même sans groupe, ce dernier prend contact avec Placebo d'abord par l'intermédiaire du manager du groupe avant d'être invité à Londres par Brian Molko et Stefan Olsdal afin de se découvrir et finalement pour entamer des sessions de répétitions. Mais ce n'est qu'en août 2008, lorsque Placebo annonce l'effectivité de son sixième album enregistré aux studios "" de Toronto sur invitation du producteur David Bottrill, que Steve Forrest est présenté comme le nouveau batteur et troisième membre officiel de Placebo. Choisi non seulement pour son talent mais aussi pour sa jeunesse et son enthousiasme, Steve Forrest, de quatorze et douze ans le cadet de ses partenaires car alors âgé de 22 ans, symbolise la renaissance salvatrice de Placebo.

Le 7 décembre 2008, Steve Forrest officie pour la première fois sur scène, à l'occasion d'un concert caritatif visant à sensibiliser l'opinion au sujet de l'exploitation des humains, donné sur le site du temple d'Angkor Vat au Cambodge. À l'occasion de ce concert exotique, Placebo a entièrement revisité quelques-uns de ses titres et joué en compagnie de la violoniste Fiona Brice qui s'est occupée des cordes pour l'enregistrement de "Meds" et du nouvel album.

Son contrat avec Virgin Records ayant pris fin à la sortie de "Meds", au terme de cinq réalisations, Placebo s'est retrouvé libre de tout engagement et a décidé d'autofinancer son sixième album. Intitulé ', ce dernier se verra diffusé en Europe à travers le label indépendant PIAS et sous le nouveau label propre à Placebo : ', qui tire son nom du refrain de la chanson titre de l'album. Au travers de son sixième opus sorti le 8 juin 2009, Placebo compte définitivement tourner le dos aux troubles de ses trois dernières années de la plus belle des manières. En réaction au précédent album jugé "a posteriori" trop claustrophobe, ", par ses accents « hard pop » et épiques, s'avère effectivement plus coloré que son prédécesseur. Sébastien Delecroix de Waxx Music, explique le 23 septembre 2009 : ,

Etrenné au printemps sur des concerts intimistes au cours desquels Steve Forrest gagne rapidement sa place dans le cœur des fans, " ne perd rien de sa complexité en live puisque Fiona Brice, présente lors de l'enregistrement de l'album, accompagne désormais le groupe sur scène et joue notamment les cordes aux côtés de Nick Gavrilovic qui, lui, succède à Alex Lee. Dorénavant composé de six membres pour ses apparitions, Placebo fut à l'affiche de la plupart des grands festivals de l'été 2009 jusqu'à ce qu'un malaise de Brian Molko sur scène début août au Japon interrompe la virée asiatique. Dans la foulée, Placebo se voit obligé d'annuler la tournée américaine prévue pour le début de l'automne. Au travers d'un documentaire sur la tournée en Asie intitulé "Placebo - ", réalisé par la chaîne anglaise Channel 4 et dont le commentateur n'est autre que Brian Molko, ce dernier confie comme pour s'excuser auprès du public désabusé par ces annulations à quel point la vie sur la route peut être éprouvante, expliquant aussi implicitement par là les raisons qui ont mené Placebo dans les troubles de l'année 2007.

La tournée mondiale de Placebo reprit fin octobre 2009 en France, juste avant que Placebo se voit décerner le prix du meilleur groupe indépendant aux MTV Europe Music Awards 2009, et donne le plus grand concert au Royaume-Uni de son histoire à l'O2 Arena de Londres, rompant avec sa volonté historique de ne jamais jouer dans les temples de la musique dominante (« ») en Angleterre. Placebo reprend sa tournée en 2010 dès le mois de février, en Asie, jusqu'à ce qu'en octobre 2010 les six derniers concerts de la tournée soient annulés en Espagne, au Portugal, en Corée et au Japon, à cause d'un problème de santé de Brian Molko.

Le dernier concert de la tournée est donc celui donné à la "Brixton Academy" de Londres le 28 septembre 2010, lieu mythique des premiers concerts à succès du groupe et du tournage du clip d". L'enregistrement live de ce concert, intitulé ' pour reprendre le slogan de la tournée, sort en formats DVD et Blu-ray le 31 octobre 2011 sous le label "" créé à la suite de la rétrocession par EMI au groupe des droits sur sa discographie passée. En 2011, Placebo ne donne que deux concerts, à Berlin et Stuttgart, dans le courant du mois d'août.

Dès mars 2010, Placebo confirme son intention d'enregistrer un septième album, et quelques dates de festivals sont annoncées début 2012 pour l'été. En mai, un communiqué sur le site officiel annonce la volonté de sortir cet album au printemps 2013 et un premier single avant fin 2012. Le groupe entame une tournée estivale et joue à cette occasion le titre "" qui sort en EP le 12 octobre 2012 avec la reprise de Minxus ' et trois titres inédits. Placebo quitte le label PIAS pour confier la distribution de ses productions à Universal Music, et conserve son nouveau label '.

Le nom et la date de sortie du nouvel album sont dévoilés le 21 mai 2013 : l'album s'intitule ' (« fort comme l'amour ») et sa sortie est programmée pour le 16 septembre 2013. Le premier extrait de l'album est le titre "Too Many Friends", sorti le 8 juillet 2013. Le titre ' fait lui office de premier single pour les États-Unis, le Canada et l'Australie. L'album se classe dans les premières places des classements de ventes d'albums dans plusieurs pays, notamment l'Allemagne, la France et l'Italie, mais obtient des critiques plus mitigées que les albums précédents. Le style de l'album s'inscrit dans la continuité du précédent, avec des titres au son fort, et , et s'en démarque par un retour à des ballades. L'esprit général des textes des principaux morceaux cherchent une célébration de l'amour et de la vie, en quête d'une certaine spiritualité. Le groupe assure une tournée européenne en novembre et décembre 2013. Après une tournée estivale à l'été 2014, ils repartent pour une tournée nord-américaine durant tout le mois d'octobre, pays qu'ils n'avaient plus visités depuis 2007.

Le , Steve Forrest annonce son départ du groupe pour poursuivre des projets personnels. À l'aube de nouveaux concerts à donner au Royaume-Uni, c'est le batteur Matt Lunn, ancien batteur du groupe Colour of Fire qui avait fait des premières parties de Placebo en 2004, qui le remplacera en tant que musicien additionnel. Matt Lunn avait également enregistré avec Brian Molko et Stefan Olsdal une nouvelle version du titre "I Know" en 2008. Désormais, Placebo est un binôme autour duquel s'articulent les quatre musiciens additionnels (le nouveau batteur ne fait pas partie du noyau dur de Placebo, à l'inverse de ses trois prédécesseurs).

En octobre 2015, Placebo se produit à Londres lors d'un concert acoustique à MTV Unplugged au cours duquel de nombreux anciens titres sont rejoués. En novembre 2015, le concert sort en CD, DVD, Blu-Ray et vinyle. 

L'année 2016 annonce les 20 ans de la sortie du premier album du groupe, et le début de deux ans de rétrospective. Le groupe propose des rééditions vinyles limitées des cinq premiers albums, et de nombreuses vidéos d'anciens concerts sont partagées régulièrement sur les réseaux sociaux. En juillet 2015, une compilation d'anciennes faces B, "B-Sides", avait déjà été rendue disponible en téléchargement et streaming, faisant suite à la sortie non désirée par le groupe en 2011 de l'album "". 

En mars 2016, Placebo annonce une grande tournée mondiale pour leur 20 ans de carrière. Le groupe passera en France en novembre 2016, à Lille, Nantes, Amnéville et Paris. En mai 2016, le documentaire Alt. Russia sur la tournée Russe de Placebo en 2014 et la rencontre avec des artistes du pays est diffusé au "Beat Festival Film" de Moscou. En juillet 2016, le groupe annonce la sortie d'un album rétrospectif intitulé ' (titre reprenant des paroles du morceau "Narcoleptic") comprenant la plupart des singles du groupe, ainsi qu'un nouvel EP intitulé ' du nom du titre du groupe Talk Talk et comprenant la reprise de ce morceau et trois nouveaux titres de Placebo, ', ' et '. Le single ' est publié le 19 août 2016.

Des concerts ont lieu en 2017 dont Last Train assure la première partie pour les shows en France. La tournée anniversaire prend fin en Australie à l'automne.

Brian Molko explique avoir Adulé ou conspué, Placebo ne laisse jamais indifférent. Difficile d'accoler à Placebo un genre ou même d'en trouver une origine tant les membres qui le composent et l'ont composé viennent d'horizons différents. Basé à Londres, Placebo n'a pourtant rien d'un groupe anglais. C'est peut-être pour cela que, lorsqu'on demande à Brian Molko s'il a déjà entendu un groupe dont le son s'approche de celui de Placebo, il répond : 

Les membres du groupe ont acquis une certaine notoriété concernant leur orientation sexuelle. Brian Molko s'est souvent dit bisexuel et Stefan Olsdal s'est ouvertement déclaré homosexuel, détails qui ont leur importance concernant la compréhension de certains textes du groupe ou tout simplement son apparence sur scène. Le groupe, ayant toujours été composé de trois membres, s'est souvent décrit comme cinquante pour cent homo et cinquante pour cent hétérosexuel. , explique Brian Molko.

L'image androgyne que Brian Molko s'efforce d'entretenir est expliquée par son attirance pour le théâtre mais aussi par la volonté de renverser les normes établies, notamment à l'égard des genres. Brian Molko et Stefan Olsdal se sont souvent entourés d'une ambiance glam', se produisant sur scène maquillés et vêtus de manière féminine, notamment durant la période de l'album "". , note Stefan Olsdal.

Cependant, depuis l'album ', le groupe s'est voulu moins sexuellement chargé de manière à privilégier la musique, posture qu'il explique dans le documentaire intitulé ' accompagnant l'album "Meds". 

Le style musical de Placebo a beaucoup évolué : du punk rock de son premier album à la hard pop de '. Mais le son Placebo comporte une signature musicale, outre la voix singulière de Brian Molko, c'est la rythmique de guitare à la croche que l'on retrouve notamment sur le titre '. Sur les deux voire trois premiers albums la musique est et . La guitare est jouée en rythmique par Brian Molko et il n'y a pas de place pour les solos. Les compositions se sont faites plus complexes avec le temps et ont inclus samples, pianos, synthétiseurs et violons. Stefan Olsdal s'est de plus en plus consacré à l'écriture et au jeu des parties de guitare meneuses, laissant sur scène la basse à William Lloyd pour de nombreux titres. Brian Molko s'est alors concentré sur la guitare rythmique chevillée aux accords puissances ; configuration beaucoup plus confortable pour assurer le chant en même temps.

Placebo utilise de nombreux accordages de guitare et basse et principalement l'accordage en Fa La# Ré# Sol# Do Do. Cependant, l'accordage en Ré Sol Do Fa La La inauguré sur le titre "Meds" est utilisé sur la majorité des titres de l'album ".

Placebo est connu pour aborder une grande variété d'états émotionnels et principalement les déceptions amoureuses et relations tumultueuses. Nombre de textes de Placebo, écrits par Brian Molko à quelques exceptions près, ont rapport à l'amour, l'amitié et la perte. Souvent écrits à la première personne, ils se caractérisent par leur romantisme et suscitent l'empathie chez l'auditeur. Ils sont en grande partie autobiographiques sur les deux premiers albums et s'ouvrent plus sur l'extérieur à partir du troisième. , explique Stefan Olsdal.

Jusqu'à l'album ', un certain nombre de textes ont explicitement fait référence au sexe et aux drogues (', ", "Commercial for Levi"), alimentant l'image sulfureuse du groupe, mais ils se sont faits de plus en plus rares par la suite ou plus implicites. L'ambiguïté des textes reste la marque de fabrique de Brian Molko qui laisse à l'auditeur le soin d'interpréter à sa manière. Pour Molko, 

Placebo rencontre un formidable succès en France et plus généralement dans les pays francophones (Belgique, Suisse). Outre les parfaites et partielles francophonies respectives de Brian Molko et Stefan Olsdal, ces derniers expliquent ce succès par la culture, puisqu'ils ont tous les deux passé leur adolescence au Luxembourg mais également par une prédisposition du public français pour leur musique : 

Quatre albums de Placebo, de ' à ', ont gravi la première marche du classement des meilleures ventes d'albums en France. Par ailleurs, Placebo a chanté trois de ses titres en français : " français" et "Protège-moi" qui sont les traductions des titres ' et ', ainsi que la face B ", marquante par la trivialité de ses paroles.

Le succès de Placebo dans son pays d'origine a fortement diminué après les trois premiers albums et la direction musicale prise dès ". De même, l'intérêt des États-Unis pour le groupe reste tout à fait mitigé. En revanche d'autres pays latins que la France, comme l'Italie, l'Espagne ou le Mexique sont des terres de prédilection pour Placebo. Stefan Olsdal parle d'ailleurs couramment l'espagnol. L'Allemagne, les pays d'Europe de l'Est ou le Japon et la Corée du Sud sont des pays tombés plus récemment sous le charme du trio international.








</doc>
<doc id="19176" url="https://fr.wikipedia.org/wiki?curid=19176" title="Province de Hainaut">
Province de Hainaut

La province de Hainaut, couramment appelée le Hainaut (en néerlandais "Henegouwen"), est une province de l'ouest de la Belgique située en Région wallonne.

L'origine de la province de Hainaut remonte à 1795, lors de la réunion des Pays-Bas autrichiens à la France révolutionnaire, le département de Jemmapes principalement fondé sur la réunion de la partie nord-est de l'ancien comté de Hainaut, du Tournaisis et d'une partie du comté de Namur (Charleroi), où sont concentrées la plus grande part des mines de charbon de Belgique. À Paris, le décret du (14 fructidor an III) de la Convention nationale française découpe les comtés, duchés et principautés de Belgique et des Pays-Bas en neuf départements français mais ne décide pas l'annexion de la Belgique par la France. Le Comité de salut public approuve le décret. Le , Lazare Carnot et Merlin de Douai persuadent la Convention nationale française de voter le décret du 9 vendémiaire an IV annexant la Belgique et les Pays-Bas.

Le département de Jemmapes doit son nom à la victoire des révolutionnaires contre les troupes impériales autrichiennes à la Bataille de Jemappes en 1792.

Après la chute du Premier Empire, par le traité de Paris du 30 mai 1814, la France conserve les cantons de Dour, Merbes-le-Château, Beaumont et Chimay. Par une ordonnance du 18 août 1814, Louis XVIII réunit les cantons de Dour, Merbes-le-Château et Beaumont au département du Nord (le premier, à l'arrondissement de Douai ; les deux autres, à l'arrondissement d'Avesnes) et celui de Chimay au département des Ardennes (arrondissement de Rocroi).

Ce département de Jemmapes fut dissous en 1814 et remplacé par la province de Hainaut à l'époque du Royaume uni des Pays-Bas. Le , Guillaume Ier des Pays-Bas accepte le trône du Royaume-Uni des Pays-Bas avec une constitution qui fusionne les neuf provinces du Pays-Bas aux huit provinces belges dont la Province de Hainaut. Le , le triumvirat (Driemanschap) du gouvernement provisoire de La Haye proclame la loi fondamentale créant les huit provinces belges du Royaume-Uni des Pays-Bas.

Après la Révolution belge de 1830, la Province de Hainaut devint belge.

Ses limites ne furent définitivement établies qu'avec le rattachement de l'arrondissement de Mouscron lors de la fixation de la frontière linguistique en 1963. Les communes de Mouscron, Luingne, Herseaux et Dottignies, ainsi que le hameau du Risquons-Tout, qui dépendait de la commune de Rekkem, ont été transférés de l'arrondissement de Courtrai au nouvel arrondissement de Mouscron. Les anciennes communes de Houthem, Comines, Bas-Warneton, Ploegsteert (ainsi que le hameau Clef d'Hollande de la commune de Neuve-Église) et Warneton furent transférées de l'arrondissement administratif d'Ypres.


La province a pour chef-lieu Mons. Sa superficie est de pour habitants, hommes et femmes (Hennuyers ou Hainuyers), ce qui fait d'elle la province la plus peuplée de la Région wallonne. Sa densité de population est de habitants au km². Outre Mons, ses principales villes sont Charleroi, La Louvière, Mouscron et Tournai. Le Hainaut compte 69 communes réparties dans sept arrondissements administratifs.

Le Hainaut est constitué de différentes sous-régions, dont le Tournaisis, la Thudinie, la Botte du Hainaut, le Borinage, le Centre et le Pays de Charleroi, etc. Ces trois dernières sont connues pour leur passé charbonnier.

L'altitude de la province est comprise entre (Celles) et (L'Escaillère).

La province est divisée en sept arrondissements : Ath, Charleroi, Mons, Mouscron, Soignies, Thuin et Tournai.

Depuis le avril 2014, la Province de Hainaut ne compte qu'un arrondissement judiciaire, celui de Mons. Avant cette date, elle en comptait trois, ceux de Charleroi, de Mons et de Tournai ayant été fusionnés en un seul arrondissement judiciaire.

Population de droit au premier juillet de chaque année (source : INS) :

Nombre d'habitants × 1000

Pour les services de police, la province est divisée en 23 zones de police.
En ce qui concerne les pompiers, la province est divisée en trois zones de secours : 

La province de Hainaut abrite une des six casernes de la protection civile belge sur son territoire : à Ghlin.

Sept maisons du tourisme couvrent le territoire de la province de Hainaut.



</doc>
<doc id="19177" url="https://fr.wikipedia.org/wiki?curid=19177" title="Ketch">
Ketch

Un ketch est un voilier à deux mâts et à gréement aurique ou gréement Marconi dont le grand mât est situé à l'avant. 

Le plus petit, appelé mât d'artimon, est sur l'arrière mais en avant de la mèche de safran (sans quoi il s'agit d'un yawl et non d'un ketch).

L'intérêt du ketch est la division de la voilure par rapport à un sloop, ce qui permet une plus grande souplesse d'utilisation. Ainsi, par vent frais un bon équilibre sous voile est trouvé en naviguant sous foc et artimon, la grand-voile étant amenée. De même, l'allure de cape est facilitée sous cette configuration. Par contre, le rendement de l'artimon allié à la grand-voile est relativement médiocre aux allures près du vent.

En Europe du Nord et sur de longue distance, le ketch est souvent préféré à un sloop, puisque la voile supplémentaire permet un meilleur équilibre, et un voile plus facile à manipuler plus petit. En cas d'augmentation soudaine de la force du vent, un avantage du ketch est la grand-voile peut être abandonné rapidement, réduisant très fortement la voilure, en laissant des voiles petites pour équilibrer le navire et maintenir la propulsion.

Les ketchs modernes ont presque exclusivement un gréement bermudien.

Les ketchs anciens pouvaient disposer de flèches ou de huniers.

Les ketchs peuvent avoir aucun flèche (voile aurique au-dessus des voiles basses), un flèche sur le grand-mât ("ketch à flèche") ou un flèche par mât ("ketch à double flèche").

Comme pour les goélettes, il est possible de rencontrer des huniers (voile carrée) en sommet de mâts. Cette configuration est rare, elle se rencontre sur les ketchs anciens. 

Le Mentor est un dériveur école gréé en ketch "marconi" (gréement bermudien). Le "Tahiti Ketch" (plan ci-dessous) possède un gréement proche d'un gréement bermudien.
Ci-dessous différentes typologie de gréements pour un ketch

Le yawl est proche du ketch, la différence est liée à la position de la voile d'artimon par rapport au safran. Dans un ketch, l'artimon est positionné en avant du gouvernail (pour stabiliser et augmenter la surface de voile), dans un yawl, l'artimon est en arrière du gouvernail (pour stabiliser le navire).

Une goélette franche est aussi un voilier à voiles auriques et deux mâts. La différence est la position du grand-mât, situé à l’arrière sur une goélette franche et à l'avant sur un ketch.

L'utilisation des premiers ketchs remonterait en 1775, utilisés comme navires de guerre par le Sultanat de Mysore pendant le règne d'Haidar Ali. Au cours des , les ketchs aurait été couramment utilisés comme petits navires de guerre, jusqu'à son remplacement dans ce rôle par bricks au cours de la dernière partie du . Le ketch a continué à être utilisé comme un bateau spécialisé pour le transport des mortiers lors des guerres napoléoniennes, dans cette application, il a été appelé un "ketch de bombe".

Comme pour les autres petits navires à gréement aurique, l'usage historique principal reste la pêche entre le et le début du . Puis comme voiliers de plaisance, ce gréement est aujourd'hui très répandue dans les yacht modernes.
"Ketch" proviendrait du mot anglais ""catch"" ("attraper") relatifs à l'usage de ce type de gréement comme bateau de pêche. Cette configuration de gréement étant très utile pour la manœuvre du navire lors des opérations sur filets.

Le nom des voiles et mâts rencontrés sur un ketch à gréement traditionnel (exemple d'un "ketch à cornes") sont :

Des voiles d'étai entre les mâts sont parfois observés.
En anglais les termes présente un faux ami : en effet "Mizzen" désigne l'artimon, la traduction de mât de misaine correspondant à "Foremast." Le nom des voiles et mât en anglais sont :
Sur les gréements modernes ou gréement bermudien, le nombre de voiles est réduite et la structure du gréement dormant (mâts, vergues, ...) est simplifiée. Ainsi, on n'observe plus de voiles hautes (flèche), ni de mât de beaupré, la forme des voiles est triangulaire, le nombre de focs réduit généralement à un ou deux. Le nom des voiles et mâts rencontrés sur un ketch à gréement bermudien sont :






</doc>
<doc id="19186" url="https://fr.wikipedia.org/wiki?curid=19186" title="Apogée">
Apogée

L'apogée d'un satellite de la Terre (naturel ou artificiel) est le point de son orbite (qui est elliptique) le plus éloigné du centre de la Terre (plus exactement, de leur centre de masse commun). Le terme vient du grec "apogeios", « loin de la terre » (de "apo", « loin » et "gê", « la Terre »).

Il existe une confusion courante entre « apogée » et « aphélie » : l'apogée se réfère à la position d'un satellite en orbite autour de la Terre, par rapport à cette dernière ; alors que l'aphélie se réfère à la position de la Terre sur son orbite circumsolaire, par rapport au Soleil.

Pour plus de détails, voir apoapside.

Le point de l'orbite circum-terrestre où la distance à la Terre est minimum est le périgée.




</doc>
<doc id="19189" url="https://fr.wikipedia.org/wiki?curid=19189" title="Baudouin (roi des Belges)">
Baudouin (roi des Belges)

Baudouin de Belgique ("Boudewijn" en néerlandais, "Baudouin en francais" , Balduin en allemand), né au château du Stuyvenberg le et mort à Motril le est le cinquième roi des Belges. Il règne du jusqu’à sa mort. Deuxième enfant et premier fils de Léopold III et de la reine Astrid, il porte le titre de comte de Hainaut alors que son père est encore duc de Brabant puis devient duc de Brabant à l'avènement de Léopold III au trône de Belgique. Il est le frère aîné et prédécesseur du roi Albert II et le frère cadet de la grande-duchesse Joséphine-Charlotte de Luxembourg.

Son avènement au trône se produit dans une période de crise politique et son long règne est marqué par bien d’autres dont l’une, au moins, a été le résultat de l’expression publique de sa foi catholique.

Son enfance est marquée par les morts accidentelles de son grand-père le très populaire Albert puis l'année suivante par celle de sa mère, il n'a alors que cinq ans, puis par la Seconde Guerre mondiale, vécue d'abord dans un bref exode. Celui-ci commence à La Panne et se poursuit en France, puis, après la défaite française, en Espagne car le roi Léopold III a voulu le soustraire, avec son frère Albert et sa sœur Joséphine-Charlotte, à l'invasion allemande de la Belgique en les évacuant sous la houlette de personnes de confiance, mais sans "" (Mademoiselle), la gouvernante à laquelle Baudouin s’est particulièrement attaché. Rapatriés à Bruxelles, les enfants royaux y passent quatre ans durant lesquels la deuxième épouse du roi, Lilian Baels, faite princesse de Réthy, s’occupe affectueusement d’eux et veille à leur ménager une vie plus conforme à celle des enfants de la bourgeoisie que celle que leurs père, grand-père et grands oncles avaient connue. Baudouin, qu’elle appelle familièrement Baud, étudie soit au château de Laeken, soit au château de Ciergnon.

Juste après le Débarquement, la famille royale, qui s’est agrandie avec la naissance d’Alexandre de Belgique, est emmenée par les nazis en Allemagne dans la forteresse de Hirschstein, puis en Autriche, à Strobl, où elle est libérée le par les troupes américaines.

Commence alors la « Question royale ». Confrontée à l'impopularité du roi Léopold III, la famille royale ne rentre pas en Belgique mais s'installe en Suisse, à Pregny, jusqu'en , attendant que le peuple belge débatte sur l'attitude du roi face à l'Allemagne Nazie : le roi Léopold devait-il quitter la Belgique après la défaite du ou a-t-il eu raison de rester au pays au nom de son statut monarchique pour s'y dresser comme un rempart contre l'éventuelle division du pays que les Allemands pouvaient sans doute vouloir comme ils l'avaient fait en 1914-1918. Baudouin fréquente un collège de Genève ; il accompagne son père et sa belle-mère dans un grand voyage aux États-Unis en 1948.

En attendant la fin des débats, le prince Charles-Théodore, frère de Léopold III, est nommé Régent du royaume jusqu’à nouvel ordre.

En 1950, après la consultation populaire qui donne des résultats fort différents en Flandre et en Wallonie ou, plus exactement, entre les arrondissements électoraux urbains ou ruraux. Devant la violence opposant « léopoldistes » et « anti-léopoldistes » et à la suite de la fusillade de Grâce-Berleur, le roi, rentré au pays avec ses deux fils aînés, fait nommer Baudouin, le , « Prince royal », ce qui correspond à une délégation de pouvoirs. En effet, une régence et le titre de Régent étaient impossibles puisque la loi sur la fin de l'impossibilité de régner du roi Léopold III avait été votée par les Chambres.

Le prince prête serment de respecter la Constitution et les lois du peuple belge devant les Chambres réunies. C’est lors de cette cérémonie que fuse le cri « "Vive la République" ! » attribué à Julien Lahaut qui est assassiné sept jours plus tard.

À la suite de l'abdication de son père le , Baudouin, selon la Constitution majeur depuis ses dix-huit ans, devient le cinquième roi des Belges, le , où fait également rage la deuxième guerre scolaire.

Pendant plus de 10 ans, le jeune roi subit la forte influence politique de son père et de sa belle-mère, allant jusqu’à refuser de serrer la main de ceux qui, comme Hubert Pierlot, s’étaient opposés à Léopold III pendant la guerre.

Un an après son mariage, Baudouin choisit de s’éloigner ; il ne rencontrera plus son père et sa belle-mère qu’en de rares occasions, notamment après le décès de la reine Élisabeth en 1965.

La naissance de Marie-Christine en 1951 et Maria-Esméralda en 1956 étend la fratrie de Baudouin. Son frère Albert se marie avec la ravissante Paola Ruffo di Calabria en 1959 et ils ont leur premier enfant en avril 1960.

Le roi Baudouin apparait en public comme un jeune homme réservé, triste et solitaire, sans aventure amoureuse connue ; la rumeur lui attribue l’intention d’entrer dans les ordres. Au fil des années, le célibat d’un roi qui vit toujours au domicile paternel devient une affaire d’État. Le cardinal Suenens a raconté que Baudouin en parla avec une religieuse irlandaise, Veronica O’Brien et que celle-ci, se mettant en recherche d’une possible épouse, trouva une aristocrate espagnole qui accepta de rencontrer le roi. Les rencontres sont tenues secrètes, ce qui explique l’étonnement des Belges lorsqu’ils apprennent par la voix du premier ministre Gaston Eyskens, en 161 mots prononcés à la radio le 16 septembre 1960 à 12 h 20 et suivi d’un extrait de la Brabançonne, les fiançailles de Baudouin. Les premières photos du couple, lors de la présentation de la jeune femme au château de Ciergnon, montrent des fiancés manifestement amoureux et complices.

Le , Baudouin épouse doña Fabiola de Mora y Aragón qui devient ainsi la « reine Fabiola ». Le mariage civil est célébré au Palais de Bruxelles et le mariage religieux en la cathédrale Saints-Michel-et-Gudule à Bruxelles et est retransmis à la télévision, une première pour un mariage royal en Belgique.

Léopold et Lilian s’installent en janvier 1961 au Château d'Argenteuil avec leurs enfants et la famille royale se divise ; Fabiola et sa belle-mère s’entendant mal, Baudouin et son père se brouillent pour des histoires de meubles emportés ou non de Laeken à Argenteuil.

En juin 1961, l'annonce que Baudouin et Fabiola attendent un enfant est faite à la presse, lors d'un voyage du couple à Rome, par le pape Jean XXIII - ce qui soulève une polémique dans les journaux belges de gauche ; trois semaines plus tard, il est annoncé par la cour que l'heureux événement ne se produira pas. En février 1962, la reine est à nouveau enceinte mais accouche d'un enfant mort-né. Une intervention chirurgicale et deux autres fausses couches ne laissent plus d'espoir de voir naitre un héritier. Aucun héritier ne naît donc de cette union.

Le roi Baudouin investit alors beaucoup dans la formation de son neveu, le prince Philippe, qu’il considère comme son successeur, bien que l’héritier normal du trône soit le prince Albert.

En 1991, les médecins affirment que Baudouin souffre de la maladie de Barlow, et le roi se fait opérer à cœur ouvert à l’hôpital Broussais à Paris.

Deux ans plus tard, le , le roi meurt d'un arrêt cardiaque lors de vacances dans sa villa de Motril, en Espagne. La princesse Lilian est avertie téléphoniquement, le soir même, du décès de Baudouin par son beau-fils Albert. Lilian, vu l’éloignement qui existe depuis de longues années entre elle et le roi, persuadée que sa présence, ou comme son absence, aux funérailles serait critiquée, décide de ne pas assister à la cérémonie mais de se faire représenter par le prince Alexandre et la princesse Marie-Esméralda.

L'annonce publique de la mort de Baudouin provoque une vague d'émotion populaire en Belgique<ref name="7/7">http://www.7sur7.be/7s7/fr/1530/Tetes-Couronnees/article/detail/361593/2008/07/28/15-ans-apres-la-mort-de-Baudouin-les-lettres-affluent-encore.dhtml</ref>. Dès le jour de sa mort, des dizaines de milliers de Belges se rassemblent devant le Palais royal pour déposer des fleurs et des bougies et rendre hommage au Roi défunt . 

La dépouille du roi est ramenée par avion à la Base aérienne de Melsbroek dans la nuit du au 2 août puis transférée au château de Laeken puis, avec un arrêt à la Colonne du Congrès devant la tombe du Soldat inconnu, jusqu’au Palais royal de Bruxelles où les autorités puis la population peuvent lui rendre hommage, plusieurs jours durant. L’émotion est grande et les gens campent jour et nuit, les files d’attente pouvant durer 10 heures. En raison des fortes chaleurs, les services de secours doivent intervenir à de nombreuses reprises.
Au total, Belges sont allés se recueillir au Palais royal. 

Les funérailles se déroulent le en la cathédrale des Saints-Michel-et-Gudule de Bruxelles lors d’une célébration de gloire et d'espérance demandée par son épouse, qui y assiste habillée de blanc, couleur de la résurrection et couleur de deuil des reines catholiques.
La cérémonie est transmise en direct sur un écran géant installé sur la Grand-Place de Bruxelles et par de nombreuses télévisions du monde. Elle rassemble un grand nombre de chefs d'État du monde entier : de la reine Élisabeth II du Royaume-Uni à l'empereur Akihito du Japon, en passant par le président français François Mitterrand.

Un second office est célébré pour la famille en l’église Notre-Dame de Laeken avant que le corps ne soit descendu dans la crypte royale pour reposer près des précédents rois et reines belges.

Le deuil national prend fin le , après la prestation de serment constitutionnel par Albert II. Pendant quelques semaines, les Belges se sont trouvés réunis par la disparition d'un homme .
En décembre 2005, il est élu au du plus Grand Belge de tous les temps par le public de la RTBF.

On rapporte que même 20 ans après sa mort, il demeure toujours aussi populaire en Belgique.

Bien que la Belgique soit une monarchie parlementaire où le roi ne peut exprimer publiquement d'opinion qu'avec l'accord du gouvernement, le roi Baudouin a eu une influence certaine sur les gouvernements qui se sont succédé pendant ses quarante-deux années de règne. La Constitution donne en effet au roi le pouvoir (purement théorique toutefois) de refuser de valider une loi, ce qui explique que le pouvoir qu'il détient n'est pas symbolique comme celui d'autres monarques. D'autant plus que c'est le roi qui nomme et révoque les ministres. Et même s'il lui faut l'approbation d'au-moins un ministre et un vote de confiance par le parlement, cela lui permet de jouer un rôle majeur après chaque élection et lors des crises ministérielles.

Le roi Baudouin acquiert une expérience supérieure à celle de bien des ministres des gouvernements de compromis qui se succèdent pendant son long règne et dont beaucoup ne resteront ministres que quelques années alors que Baudouin aura règné pendant 42 ans. Ce règne constitue, pour l'État belge, une pierre angulaire qui permet au roi d’avoir de l’ascendant sur ses ministres, voire de .

S’il défend toujours l'unité de la Belgique, il ne peut cependant empêcher les querelles linguistiques et la création d'une frontière linguistique, de trois régions, de quatre régions linguistiques et de trois communautés. C'est sous son règne que, de réforme de l'État en réforme de l'État, la Belgique devient un État fédéral.

Sur le plan international, le règne est marqué par la création de la CECA en 1951 et de la Communauté économique européenne en 1957, de l'Exposition universelle de Bruxelles en 1958 et par l'indépendance du Congo belge et du Ruanda-Urundi.

En 1955, accueilli chaleureusement par des populations indigènes, Baudouin effectue une tournée triomphale au Congo belge, visitant toutes les régions d'une colonie qui est alors à son apogée mais où se manifestent des velléités d'autonomie. En 1959, quatre ans après ce voyage qui n’apporte pas de changement fondamental dans la politique belgo-congolaise, il doit annoncer l'intention du Gouvernement d'accorder l'indépendance au Congo. Le 30 juin 1960, le monarque assiste à la transmission des pouvoirs à Léopoldville et prononce un discours, lors des festivités, qui est ressenti internationalement comme ignorant les atrocités commises durant la période de la gestion belge, du même ordre que celles survenues sous l'autorité de toutes les autres puissances colonialistes, et glorifiant l'œuvre coloniale belge. Le Premier ministre Patrice Lumumba, qui sera assassiné plus tard probablement avec l'aide des services secrets belges et américains, réplique par un discours très critique vis-à-vis de la colonisation, discours qui sera qualifié d'insultant ou venimeux tant en Belgique qu'à l'étranger.

Des violences qui surviennent au Congo, immédiatement après la proclamation de l'indépendance ainsi qu'une grande incertitude politique et de nombreux troubles. Dans un échange de lettre entre le chef du cabinet du roi et le major Guy Weber, alors chef de l'armée sécessionniste katangaise, Weber annonce que Mobutu et Tshombé neutraliseraient complètement Lumumba, . D'après Ludo De Witte, recevant la lettre le 26 octobre, le roi rédige un projet de réponse à Tshombé avec la mention « Monsieur le Président », renforçant sa légitimité. Il se dit « très sensible (...) aux sentiments d'attachement que vous continuez à éprouver pour la Belgique et sa dynastie ». Le brouillon de la lettre comporte, faisant référence à Lumumba, la mention de la « politique haineuse d'un seul homme ». La lettre envoyée se conclut par l'accord de ""plausible deniability"" du Roi : Ludo de Witte pense également que Baudouin ayant hérité de l'entourage de son père Léopold III, aurait été fortement influencé par celui-ci, composé de personnalités très déterminées et réactionnaires. Certains ont vu dans l’attitude du roi Baudouin, dans cette affaire d’assassinat politique, une non-assistance à personne en danger, peu compatible avec le respect de la vie qu’il manifeste comme croyant catholique .

Baudouin est connu pour être un homme profondément croyant, catholique,il s'oppose au dépôt dès 1971 d'une proposition de loi de dépénalisation de l’avortement qui échouera. La proposition de loi Lallemand-Michielsen, qui se base notamment sur le livre "Abortus pro/contra" de l’expert en science éthique Hugo Van den Enden, rencontre ensuite une forte opposition et des pressions de diverses personnalités politiques, comme en a témoigné Roger Lallemand, ainsi que du roi.

En 1990, celui-ci refuse donc, obéissant à sa conscience, de sanctionner cette loi qui propose la dépénalisation conditionnelle de l'avortement. Le 30 mars, Baudouin écrit au premier ministre Wilfried Martens : Le Souverain invite dès lors .

Sur la base de l'article 93 de la Constitution, une parade juridique est trouvée : le Conseil des ministres constate que le roi est « dans l'impossibilité de régner » ce qui permet aux ministres réunis en conseil de sanctionner la loi le . Le 5 avril suivant, le roi est rétabli dans ses fonctions après un vote des Chambres réunies constatant que l'impossibilité de régner a pris fin. Certains Belges, dont Guy Verhofstadt considèrent cette astuce comme inconstitutionnelle puisque la Constitution n'envisage que les cas de maladie du roi, que la Constitution ne donne les pouvoirs du Roi aux ministres qu'en cas de décès de celui-ci, qu’en cas d'impossibilité de régner, c'est aux Chambres réunies de pourvoir à la tutelle et la régence.

L’attitude du roi, qui a donc posé un sérieux problème institutionnel, a cependant pour conséquence une augmentation de sa popularité telle qu’il est proclamé « homme de l’année » par les médias.

Ses allocutions à la Nation, annuelles puis deux fois par an, retransmises à la radio et à la télévision sont et expriment une , basée sur les valeurs familiales traditionnelles et une , opposée à l’individualisme, au matérialisme et à l’appât du gain. Cela le conduit à affirmer la nécessité de lutter contre la crise, le chômage, la décadence, le racisme, la pauvreté, l’exploitation sexuelle. .

Sa constance dans l’expression de ses valeurs est appréciée de bien des gens — comme la simplicité de sa vie au quotidien, semblable à celle de la petite bourgeoisie, telle qu’elle transparait dans les photos de presse. 

Baudouin a réussi, au fil des ans, à faire reconnaitre des . Son décès inattendu a choqué une grande partie de la population, suscitant , tant en Belgique qu’à l’étranger.
Les hommages qui lui ont été rendus ont été adressés et le rituel funéraire a reçu une , cristallisant les sentiments de don du roi et de contre-don des Belges et rappelant la mort d’un héros antique. 

On relève cependant un soutien du roi, voire des liens d'amitié, pour des personnalités dont l'action politique ou morale a été décriée. Ainsi a-t-il été proche de Mobutu Sese Seko (il fut le parrain d'une de ses filles et a passé des vacances chez lui) et l'a-t-il soutenu, bien que les méthodes du régime qui incluaient la corruption, le non-respect des droits de l'homme et le détournement d'argent public fussent connues, jusqu'en 1988 lorsque Mobutu compara Léopold II de Belgique à Hitler, établissant ainsi une . Il était proche aussi de Juvénal Habyarimana qui participa à des groupes de prières du Renouveau charismatique organisés au palais de Bruxelles et le soutint notamment en 1990, en demandant par écrit au gouvernement belge l’envoi de soldats belges pour aider Habyarimana menacé par le Front patriotique rwandais. Il manifesta de la complaisance pour le régime de Francisco Franco aux funérailles duquel il fut empêché d’assister par l’intervention du gouvernement belge. Il accueillit officiellement Jean-Paul II en Belgique en des termes qui s’adressaient au représentant de sa religion plutôt qu’au chef d’État, suscitant la critique du milieu laïque belge.

En 1976, lors des célébrations des 25 ans de son règne, le roi Baudouin exprime le souhait de voir une fondation contribuant à l'amélioration des conditions de vie de la population : la Fondation Roi Baudouin est donc créée à l'aide des fonds récoltés à cette occasion. Fondation d'utilité publique indépendante et pluraliste, elle a pour objet d'améliorer les conditions de vie de la population sur les plans économique, social, culturel et scientifique. Elle soutient l'engagement de tous les acteurs de la société afin de générer des changements durables qui contribuent à davantage de justice, de démocratie et de développement. Elle combine la réflexion de fond, la mise sur pied d'initiatives propres et l'aide financière en faveur de projets de tiers. Elle agit tant à court qu'à long terme.

Tous les deux ans, la Fondation Roi Baudouin remet le Prix international Roi Baudouin pour le développement (d'une valeur de ) afin d'appuyer et faire connaître des projets ayant apporté une contribution majeure au développement des pays du Sud ou à la solidarité entre pays industrialisés et ceux en développement. Anciens lauréats : Walter Plawright (développement d'un vaccin contre la peste bovine), Paulo Freire (alphabétisation au Brésil), la Grameen Bank du Bangladesh (micro-crédit), Aids Support Organisation (lutte contre le sida en Ouganda), Mouvement des Paysans Sans Terre au Brésil, la commission des droits de l'homme au Pakistan, etc.

La Fondation Roi Baudouin gère de nombreux fonds d'entreprise et fonds nominatifs, comme le Fonds Reine Fabiola pour la santé mentale, le Fonds Prince Albert, le Fonds Prince Philippe et le Fonds Princesse Mathilde.

La Fondation Astrida est une fondation ayant le statut juridique belge de "Fondation Publique" créée par le testament du du Roi Baudouin, avant qu'il bénéficie de chirurgie cardiaque un an avant son décès et dont l'objet est de "soutenir financièrement ses neveux et nièces (et leurs conjoints) descendants du Roi Albert II et de sa sœur Joséphine Charlotte et les neveux et nièces de la Reine Fabiola, de générations en générations, dans les domaines religieux, artistiques, scientifiques et pédagogiques". 

L’article modifié stipule encore que la fondation pourra aussi aider les neveux et nièces dans leur ‘établissement dans la vie’ sans toutefois soutenir des activités lucratives, quelles qu’elles soient. Cette fondation pourra leur venir en aide en cas de maladie, d’infirmité ou toute autre situation à caractère philanthropique.

Par ailleurs l'objet de la fondation, tel que modifié en 2006, dit que "le Conseil d'Administration veillera dans la mesure de son budget annuel à accorder une aide similaire à toute personne physique ou morale et en particulier (...) à celles qui appartiennent au tiers ou au quart-monde". La fondation est notamment devenue propriétaire du domaine royal d’Opgrimbie.

En 2013, la Reine Fabiola transfère un montant de 2,5 millions d'euros à la fondation Astrida.







</doc>
<doc id="19191" url="https://fr.wikipedia.org/wiki?curid=19191" title="Baudouin">
Baudouin

Baudouin ou Baudoin (plus rarement Bauduin ou encore Bodoin) est un nom propre qui peut désigner :



Empereurs latins de Constantinople
















</doc>
<doc id="19196" url="https://fr.wikipedia.org/wiki?curid=19196" title="Zaïre">
Zaïre

Le Zaïre était le nom porté par l'actuelle République démocratique du Congo sous la Deuxième République, entre 1971 et 1997. Bien que l'appellation ne date que de 1971, le nom de Zaïre est aussi généralement utilisé pour la période 1965-1971 de la Deuxième République, l'ensemble de la période étant marqué par la prise de pouvoir dictatoriale de Joseph-Désiré Mobutu. Cet article concerne l'histoire du pays de 1965 à 1997.

En 1960, le Congo belge accéda à l'indépendance sous le nom de République du Congo mais l'ancienne colonie française, avec laquelle l'ex-Congo Belge partageait une frontière à l'ouest, adopta également le nom de République du Congo (en fait officiellement « République congolaise »). Depuis cette période, ces deux États sont également restés souvent différenciés par les noms de leur capitales : Congo-Kinshasa (alors Congo-Léopoldville) pour l’ex-Congo belge et Congo-Brazzaville pour l’ex-Congo français.

Les rébellions et incertitudes quant au détenteur du pouvoir se succédèrent jusqu'en 1965, date à laquelle le lieutenant général Joseph Mobutu, alors commandant en chef de l'armée, prit le contrôle du pays et s'autoproclama président pour 5 années. Il consolida rapidement son pouvoir et fut bientôt élu président à l'issue d'un scrutin sans opposition.

Rétrospectivement, Mobutu justifia la coupure intervenue en 1965, par un bilan de la première République établi en ces termes : "chaos, désordre, négligence, et incompétence." Le rejet de la légitimité de la Première République allait au-delà des mots. Au cours des deux premières années de son existence, le nouveau régime mit ses priorités sur les tâches de reconstruction et de consolidation politique. Créer les nouvelles bases de la légitimité de l'État, en particulier sous un régime de parti unique, devint la priorité de Mobutu.

Afin de distinguer les deux États voisins du Congo, on ajouta en 1964 le terme « démocratique » à la dénomination de l’ancien Congo belge qui reste désormais connu sous le nom de République démocratique du Congo (RDC) (une désignation qu’il ne gardera que jusqu'en 1971 et qui ne sera reprise qu’en 1997).

Une autre priorité est de reconstruire les structures sociales et politiques du pays, processus qui débute en 1970, continue avec le changement de nom du pays en 1971, et culmine avec l'adoption de la nouvelle Constitution de 1974, la zaïrification forcée avec la centralisation et la concentration croissante du pouvoir dans les mains du « Père de la Nation ». En 1976, cependant, cette politique commença à montrer ses limites, à générer ses propres contradictions, et à préparer le retour d’un système "bula matari" (« celui qui brise les rochers ») fait de brutalités et de répressions, y compris contre les différentes ethnies que le régime avait dans un premier temps tenté de gommer en leur attribuant une nouvelle identité.

Depuis 1965, Mobutu Sese Seko a dominé la vie politique du Zaïre, restructurant l'état à diverses occasions, et se donnant le titre de « Père de la Nation. » Toute analyse relative aux structures politiques de ce pays et à son organisation ne peut être envisagée sans s'intéresser à l'homme qui donna ce nom au pays.

Mobutu est né dans la ville de Lisala à proximité du Congo, le . Malgré ce lieu de naissance, Mobutu n'appartenait pas à l'ethnie majoritaire dans la région, mais aux Ngbandis, une petite ethnie dont le territoire se trouve à proximité de la frontière avec l'actuelle République centrafricaine.

Mobutu se référait constamment à ses origines modestes et à la renommée de son oncle, un guerrier et astrologue du village de Gbadolite. Bien que né sous le nom de Joseph-Désiré Mobutu, il était également appelé du nom de son grand-oncle, "Sese Seko Kuku Ngbendu wa za Banga", ce qui, selon certains, signifie « grand guerrier conquérant, qui va de triomphe en triomphe. »
Mais ce n'est pas exactement cela, même si ces attributs peuvent être donnés à un grand guerrier, qui va de triomphe en triomphe. Voici en fait la vraie signification du nom de cet ancien chef d'État de l'ex-Zaïre :
"Mobutu", c'est-à-dire "Tu es poussière", "Sese seko", "La terre éternelle", "Kuku ngbendu", "Le piment vert" (qui n'est pas mûr), "Wa za ["ka"] Banga", "Le feu/La lumière brûle/brille à l'occident".

Quand, sous l'ère de l'authenticité au début des années 1970, les Zaïrois durent adopter des noms « authentiques », Mobutu reprit le nom de son grand-oncle Mobutu Sese Seko Nkuku Ngbendu wa za Banga, ou plus communément Mobutu Sese Seko (zaïrianisation, radicalisation, et rétrocession).

Mobutu, qui passe quatre années à l'école primaire de Léopoldville, prend sept années pour atteindre le diplôme d'enseignement secondaire, fréquentant différentes écoles. Il a de fréquents conflits avec les missionnaires catholiques des écoles qu'il fréquenta. Il en est définitivement renvoyé en 1950 à l'âge de 19 ans. Il fut alors intégré pour sept années dans la Force publique.

Le service militaire est crucial dans la carrière de Mobutu. Contrairement à d'autres militaires, il maîtrisait très bien le français, ce qui lui valut rapidement un emploi de bureau. En novembre 1950, il fut envoyé à l'école pour officiers congolais, où il fit la connaissance de nombre des militaires qui devaient assurer le contrôle de l'armée après le départ des officiers belges à l'indépendance. Vers la fin de son enrôlement, en 1956, Mobutu avait acquis le grade de Sergent-Major, le plus haut rang accessible aux congolais. Il avait par ailleurs commencé à publier dans des journaux sous un pseudonyme.

Mobutu retourne à la vie civile alors que la possibilité d'une décolonisation se faisait jour. Ses articles de journal attirèrent l'attention de Pierre Davister, éditeur belge du journal "L'Avenir". À cette époque, être parrainé par un européen était une belle opportunité pour un Congolais ambitieux. Sous la tutelle de Davister, Mobutu devint un éditorialiste reconnu pour un nouvel hebdomadaire africain, Actualités Africaines. Davister rendit également plus tard service à Mobutu en donnant un écho favorable au régime de ce dernier via son propre magazine belge, "Spécial".

Mobutu acquit une visibilité parmi l'élite africaine émergente à Léopoldville. Seule une barrière lui restait fermée pour l'obtention d'un statut dans la société coloniale : la reconnaissance complète en tant qu'évolué dépendait de l'approbation des autorités catholiques. Celles-ci lui refusant cette reconnaissance, il la rejettera par la suite.

Au cours des années 1959-60, les jeunes Congolais politiquement ambitieux s'affairaient à monter des réseaux et des alliances. La résidence de Mobutu en Belgique lui épargna nombre des difficultés auxquelles d'autres étaient confrontés, qui se contentaient de relations locales et ethniques. Mais cette approche ne lui aurait été d'aucun secours, les Ngbandi étant une ethnie des plus minoritaires et parmi les Ngala (locuteurs Lingala immigrés à Léopoldville), d'autres tels Bolikango étaient des opposants potentiels de poids. Mobutu emprunta une autre route, la diplomatie belge, les renseignements et les intérêts financiers locaux et internationaux recherchant des relais parmi les étudiants congolais de Bruxelles.

Mobutu croisa par ailleurs la route de Patrice Lumumba, lorsqu'il arriva à Bruxelles. Il s'allia à Lumumba, qui partageait notamment son anticléricalisme, à l'époque de la scission du Mouvement national congolais (MNC) et des divergences d'avec Albert Kalonji. Au début 1960, Mobutu fut nommé à la tête du bureau du MNC-Lumumba à Bruxelles. Il assista à la Table ronde tenue à Bruxelles en janvier 1960, et retourna au Congo seulement 3 semaines avant la date du 30 juin prévue pour la proclamation de l'indépendance. Quand l'armée se mutina contre les officiers belges, la nomination de Mobutu était un choix logique pour combler le vide. Lumumba nomma commandant en chef un membre de sa propre ethnie, Victor Lundula, mais Mobutu était le choix privilégié de Lumumba, et il ne tarda pas à prendre une place importante dans l'armée.

Au cours de la période cruciale de juillet-août 1960, Mobutu construisit "son" armée nationale en liant des alliances extérieures avec les unités qui lui étaient acquises, en exilant les autres unités en des régions reculées, et en absorbant ou dissolvant les unités rivales. Il s'assura de la loyauté des individus en contrôlant leurs promotions et leurs rémunérations. Lundula, âgé et moins alerte, ne s'opposa que peu aux plans de Mobutu.

Après la démission de Lumumba par le Président Kasa-Vubu le 5 septembre, puis la tentative de Lumumba de bloquer cette démission par le Parlement, Mobutu prit les rênes pour la première fois le 14 septembre. Sous sa seule autorité (mais avec l'aide des États-Unis), il installa un gouvernement intérimaire, appelé « Collège des commissionnaires », composé essentiellement de diplômés et d'étudiants universitaires, et qui remplaça le Parlement pendant 6 mois en 1960 et 1961.

Au cours des quatre années suivantes se succédèrent des gouvernements civils faibles, le pouvoir réel s'exerçant en coulisse par le « groupe de Binza », un groupe de partisans mobutistes fortunés, dénommé d'après le quartier de Ngaliema où la majorité d'entre eux résidaient

Quand en 1965, comme en 1960, les tensions se firent jours entre le président et le premier ministre et menèrent à l'instabilité du pays, Mobutu s'empara du pouvoir (de nouveau avec l'aide des États-Unis). Contrairement à la première fois cependant, Mobutu prit le pouvoir au-devant de la scène

De 1965 à 1967, l'État de Mobutu s'attache à renforcer sa légitimité en démantelant graduellement les institutions de la première république et en même temps en accroissant la centralisation du contrôle du pouvoir autour du président. Bien que le parlement continuât à se réunir occasionnellement, ses prérogatives furent sensiblement réduites, les décisions exécutives étant généralement dorénavant prises par ordonnances-lois présidentielles. Tous les partis politiques furent dissous et les activités politiques interdites, Mobutu ayant promis que "pendant 5 années, il n'y aurait pas d'activité des partis politiques". En 1966, les 21 petites provinces furent rassemblées en 12, puis 8 et la capitale, et furent renommées régions en 1972 (voir fig. 1). Elles furent transformées en de simples entités administratives directement responsables devant le gouvernement central, et leurs assemblées étaient plus consultatives que législatives. Après la suppression de la fonction de Premier Ministre en octobre 1966, le président détenait le quasi-monopole du pouvoir exécutif, et contrôlait les pouvoirs législatif et judiciaire.

Nombre de sympathisants de l'opposition tshombiste des années 1960 furent rapidement incorporés au système étatique à travers diverses opérations de patronage. Avec la même célérité, une justice sommaire put disposer des plus farouches opposants au régime. Le , quatre personnages-clef de la première république, dont l'ex-Premier Ministre désigné Évariste Kimba, furent accusés de complot envers l'État, jugés en une parodie de procès et pendus publiquement à Kinshasa. Les menaces contre le régime persistèrent cependant. Des poches d'insurrection existaient encore, notamment au Kivu (dont le maquis de Laurent-Désiré Kabila à Fizi) et au Haut-Zaïre (province Orientale). Des mois s'écoulèrent avant que ces foyers de dissidence puissent être contrôlés

Pendant ce temps, des rumeurs faisaient état d'un retour prochain de Tshombe, l'ancien homme fort de l'État du Katanga, exilé en Espagne. Ces rumeurs s'accentuèrent en juillet 1966 lorsque certains des anciens gendarmes katangais, aidés de mercenaires, se mutinèrent et prirent Kisangani (anciennement Stanleyville). Deux mois plus tard, ceux-ci furent repoussés après une intervention du mercenaire français Bob Denard. En juillet 1967, une autre importante mutinerie éclata à Kisangani, déclenchée par la nouvelle que l'avion de Tshombe survolant la mer Méditerranée avait été détourné vers Alger, où Tshombe était désormais détenu prisonnier. Alors que les rebelles étaient boutés de Kisangani par les forces de l'ANC, ils prirent Bukavu, à proximité de la frontière avec le Rwanda, qu'ils gardèrent pendant 3 mois. Ils tentèrent des attaques infructueuses contre l'ANC, mais en novembre, ils passèrent la frontière rwandaise où ils se rendirent aux autorités locales. La campagne brillante et inattendue de l'ANC donna au régime une aura et une légitimité nouvelle. Le temps était venu pour de nouveau changements.

Déjà en janvier 1966, une étape majeure de la consolidation du régime fut effectuée avec la création du Corps des Volontaires de la République (CVR), une organisation dont les membres furent essentiellement recrutés parmi les étudiants de l'Union Générale des Étudiants Congolais (UGEC). Nombre des idées produites par le CVR provenaient d'un groupe d'étudiants radicaux promouvant les thèmes de nationalisme, d'indépendance économique, et de socialisation. Plus qu'un parti, le CVR était essentiellement un mouvement destiné à mobiliser la population derrière Mobutu, "notre deuxième héros national" (après Lumumba). Les succès variables du CVR comme agent de mobilisation populaire et politique, reflétant en partie les excès des étudiants impliqués, incitèrent Mobutu à lancer un mouvement plus large et plus fédérateur, par lequel, selon les mots de Mobutu, « serait animé par le Chef de l'État lui-même, et dont le CVR ne serait pas l'embryon ».

En 1967, Mobutu avait consolidé son pouvoir et œuvra pour donner au pays une nouvelle constitution qui validerait son système de parti unique. La nouvelle constitution fut soumise à un référendum populaire en juin 1967, et fut approuvée par 98 % des votants. Elle donnait un pouvoir accru au Gouvernement central et au président, qui devenait chef de l'état, de la police et de l'armée, et chargé des affaires étrangères. Le président approuvait ou démissionnait les ministres et leurs membres de cabinet, et fixait leurs attributions. Les ministres étaient chargés de la stricte exécution des ordres et programmes du président. Le président approuvait ou démissionnait également les gouverneurs de province, ainsi que tous les juges, y compris ceux de la Cour suprême.

Le parlement bicaméral fut remplacé par une assemblée unique, dénommée Assemblée nationale. Le président de la République avait le pouvoir de légiférer quant aux dispositions non prévues par les lois, sans préjudice de la Constitution. Sous certaines conditions, le président pouvait gouverner par des arrêtés présidentiels, qui prévalaient sur les lois.

Mais le plus important changement fut celui de la création du Mouvement populaire de la Révolution (MPR) le , marquant l'émergence d'une « nation politiquement organisée ». Plutôt que d'être un parti considéré comme une émanation de l'État, c'est l'État qui fut désormais considéré comme l'émanation du parti. Dès lors, en octobre 1967, les responsabilités des partis et de l'administration furent mises en commun en une structure unique, assurant l'emprise du parti à tous les niveaux de pouvoir, y compris dans les provinces et jusqu'aux syndicats de travailleurs, mouvements de jeunesse et organisations étudiantes. En peu de temps, le parti était devenu l'instrument exclusif et légitime de la vie politique du pays. Comme le déclara l'un de ses dirigeants, "le MPR doit être considéré comme une église, et son fondateur, le Messie".

La fondation doctrinale suivit de peu la fondation du parti, et prit la forme du Manifeste de la Nsele (du nom de la résidence campagnarde de Mobutu à Kinshasa à Nsele, à six kilomètres de Kinshasa) publié en mai 1967. Le nationalisme, la révolution et l’authenticité y furent identifiés comme thèmes majeurs de ce qui serait bientôt qualifié de « mobutisme ». Le nationalisme impliquait la mise en place d'une politique d'indépendance économique.

Les descriptions de la révolution comme « une révolution nationale, essentiellement pragmatique, » c’est-à-dire « répudiant et le capitalisme et le communisme. » et « ni de droite, ni de gauche », devinrent rapidement des slogans légitimant le régime, en même temps que l' « authenticité ». Le concept d’authenticité fut dérivé de la doctrine professée par le MPR d’un « authentique nationalisme zaïrois et une condamnation du régionalisme et du tribalisme ». Mobutu la définissait comme « être conscient de sa propre personnalité et de sa propre valeur ».

L'authenticité donna à Mobutu sa principale originalité philosophique. N'impliquant pas le rejet de la modernité, l'authenticité devait être vue comme un effort pour réconcilier les aspirations des traditions culturelles zaïroises avec les exigences de la modernisation. La façon d'arriver à cette synthèse ne fut cependant pas explicitée. Ce qui ne faisait pas de doute en revanche, c'est que l'usage de ce concept d'authenticité était envisagé par Mobutu comme un moyen d'asseoir son autorité. Comme il le proclamait lui-même, « en nos traditions africaines, il n'y a jamais de place pour plusieurs chefs... Ce pourquoi nous, les Congolais, désirons nous conformer aux traditions du continent, et avons décidé de joindre l'énergie des citoyens de notre pays sous la bannière d'un seul parti national. »

Les détracteurs furent prompts à dénoncer les raccourcis et les explications douteuses pour la légitimation du régime, en particulier les soi-disant qualités inhérentes du parti et de son fondateur. Cependant, le centre de formation idéologique du MPR, l'Institut Makanda Kabobi, endossa sa tâche de formation et de propagande à travers le territoire, et propagea « les enseignements du Président-fondateur qui doivent être donnés et interprétés de la même façon à travers tout le pays ». Les membres du Bureau Politique du MPR furent ainsi sensibilisés à leur responsabilité de « garants du Mobutisme ».

À côté des mérites ou des défauts du mobutisme, le MPR forgea sa légitimité à partir des partis populaires qui firent leur apparition dès la fin des années 1950 à travers l'Afrique, un modèle qui fut aussi une source d'inspiration pour le MNC-Lumumba. C'était un héritage lumumbiste que le MPR essaya de s’approprier en son effort pour mobiliser la population « zaïroise » derrière le Président-fondateur. Étroitement liée avec la doctrine mobutiste, c'est la conception d'un parti unique qui devait régenter tous les secteurs d’activité de la nation.

Traduire le concept « d’"une nation politiquement organisée" » dans la réalité impliquait une expansion du contrôle de l’État sur la société civile. Cela commençait par l'incorporation de la jeunesse et des travailleurs dans des organisations contrôlées par le MPR.

En juillet 1967, le Bureau politique annonça la création de la Jeunesse du Mouvement populaire de la révolution (JMPR), un mois après le lancement de l'Union nationale des travailleurs zaïrois (UNTZA), qui mit ensemble sous une seule organisation trois syndicats de travailleurs pré-existants. Le but était, selon les termes du Manifeste de la Nsele, de transformer le rôle des syndicats de travailleurs de « force de confrontation » en « un organe de support à la politique gouvernementale », devenant ainsi « un lien de communication entre les classes populaires et l'État ». De la même façon, la JMPR devait opérer un lien majeur entre les étudiants et l’État. En réalité, le gouvernement tentait de mettre sous sa coupe les secteurs les plus susceptibles de voir émerger une opposition au régime. En soumettant les dirigeants des travailleurs et de la jeunesse au Bureau politique du MPR, le régime espérait enrôler les forces syndicales et les étudiants dans la machinerie de l’État. Cependant, il fut remarqué par de nombreux observateurs qu’il n’y avait pas de preuves que la cooptation ait réussi à mobiliser un enthousiasme pour le régime au-delà d’un niveau superficiel.

Mobutu fut attentif à supprimer toute institution qui aurait pu favoriser les affinités ethniques. Résolument opposé à une mobilisation politique sur base de l'origine ethnique des personnes, il interdit nombre d’associations telles que l’"association des Lulua Frères", qui existait au Kasaï depuis 1953 en réaction à l’influence dans la région de l’ethnie rivale des Lubas, et "Liboke lya Bangala" (littéralement « paquet de Bangalas »), une association formée dans les années 1950 pour représenter les intérêts des locuteurs Lingala dans les grandes villes. Ceci permettait notamment d’éclipser l’origine ethnique de Mobutu lui-même. Les tensions ethniques réapparurent cependant à mesure que l'insatisfaction des congolais grandissait.

En parallèle avec les efforts visant à neutraliser toute source de pouvoir incontrôlée, d'importantes réformes administratives furent mises en place en 1967 et 1973 pour accroître l’emprise du pouvoir central sur les provinces. L’objectif principal de la réforme de 1967 fut l'abolition des gouvernements provinciaux, en les remplaçant par des fonctionnaires contrôlés par Kinshasa. Le principe de la centralisation fut ensuite étendu aux districts et aux territoires, avec un fonctionnaire dépendant de Kinshasa à leur tête. Les seules entités administratives qui ne tombèrent pas sous la coupe du gouvernement central furent les "collectivités", les "chefferies" et les "secteurs" (comprenant plusieurs "chefferies").

L'État unitaire et centralisé ressemblait de plus en plus à celui mis en place sous le Congo belge, excepté le fait qu'en 1972 les provinces prirent le nom de régions.

La poursuite du contrôle des secteurs-clé sociaux continua. Les associations de femmes furent mises sous contrôle du parti, de même que la presse, et en décembre 1971 Mobutu commença à amputer la puissance des Églises ; trois d’entre elles étaient reconnues à l’époque : l’Église du Christ au Zaïre (alors "Église du Christ au Congo" mais rebaptisée plus tard lorsque le pays changera de nom), l’Église kimbanguiste et l’Église catholique romaine.

Entre 1966 et 1971, de nombreux lieux furent aussi rebaptisés. Quelques-uns de ces changements de dénomination parmi les plus importants sont :

Pour suivre cette voie et toujours en quête de légitimation (sous couvert d’authenticité) de la centralisation du régime (sous le couvert d’une uniformisation), le nom du pays fut changé en « République du Zaïre » en octobre 1971, et celui des forces armées en « Forces armées zaïroises » (FAZ). Cette décision était curieuse, dans la mesure où le nom "Congo", qui se référait à la fois au fleuve Congo et à l’ancien Kongo, était lui-même fondamentalement « authentique » et pré-colonial, alors que « "Zaire" » est un nom portugais adapté maladroitement d’un mot africain, "Nzadi" signifiant "fleuve" (ou de l’expression "Nzadi o Nzere", « la rivière qui engloutit toutes les autres rivières, » autre désignation générique du fleuve Congo).

En 1972, le général Mobutu devient Mobutu Sese Seko et obligea tous les citoyens à adopter des noms africains en lieu et place de noms européens ou chrétiens. Les standards d'habillement occidentaux furent aussi abandonnés au profit de l’abacost, par souci d'authenticité.

Avant que le pays change de nom, une nouvelle monnaie avait été introduite en 1967, baptisée zaïre et destinée à remplacer le franc congolais post-colonial en tant que monnaie nationale.
En définitive, il n'était pas rare de voir exprimées les valeurs monétaires avec trois zéros après la décimale, même après les importantes dévaluations intervenues plus tard.

C'est aussi l'époque des grands travaux et du rapprochement avec la République populaire de Chine, qui viendra édifier dans le pays le Stade des Martyrs, le palais du Peuple et le pont Matadi. Le stade Tata Raphaël accueillera en 1974 l'un des matchs de boxe les plus mythiques, opposant Mohamed Ali à George Foreman. Les palais de Marbre et de Gbadolite datent également de cette époque.

C’est aussi celle de divers "éléphants blancs", dont le barrage d'Inga et la sidérurgie de Maluku, et même un programme spatial. Le réacteur nucléaire de Kinshasa fut géré par les congolais seuls à partir de 1987.

Cette période est aussi celle d’une « zaïrianisation » forcée de tous les anciens symboles non seulement de l’ancienne colonisation (les villes rebaptisées en noms africains, ainsi qu’un certain nombre d’institutions publiques et privées), mais les noms d’identité sont aussi changés de force, et le régime tente de gommer les différences ethniques en leur attribuant une identité commune mais entièrement nouvelle. Pourtant certaines pratiques issues de l’ancien État colonial seront remises en vigueur progressivement, leur changement de nom servant surtout d’alibi africanisé pour masquer leur réalité dans un régime fortement autoritaire et de plus en plus autocratique.

La nationalisation des universités de Kinshasa et de Kisangani, alliée avec l’insistance de Mobutu de supprimer les noms chrétiens et d'installer des sections de la Jeunesse du MPR dans tous les séminaires, amenèrent rapidement des tensions avec l'Église catholique romaine. Celles-ci continuèrent jusqu'en 1975, date à laquelle, à la suite de pressions du Vatican, le régime cessa ses attaques contre l’Église qui récupéra certaines de ses prérogatives en matière d’enseignement. Cependant, conformément à la loi de décembre 1973, qui autorisait l’État à dissoudre « toute église ou secte troublant ou susceptible de troubler l’ordre public», des organisations non autorisées furent démantelées et leurs dirigeants jetés en prison.

Avec la réforme de juin 1973, une étape supplémentaire fut franchie dans la direction d'une centralisation accrue. Le but en était de réaliser une fusion complète entre les structures administratives et politiques en faisant de chaque responsable politique le chef de la section correspondante du parti. Une autre conséquence fut que la réforme entrava sérieusement la puissance des autorités traditionnelles au niveau local. Les autorités traditionnelles héréditaires n'étant désormais plus reconnues, l'autorité échoyait aux seules autorités mises en place par Kinshasa et contrôlée par voie hiérarchique. Dès lors, le processus de centralisation avait formellement éradiqué toute forme d'autonomie locale préexistante.

L'analogie avec l’État colonial devint encore plus flagrante si l'on s'intéresse à l'introduction du « service civil obligatoire » en 1973 (connu aussi sous le nom du terme lingala de "salongo"), et qui prenait la forme d'une après-midi par semaine pour des travaux d'intérêt général, généralement en agriculture ou en projets de développement. Officiellement présenté comme une tentative révolutionnaire de recouvrer les valeurs du communalisme et de la solidarité inhérentes aux sociétés traditionnelles, le Salongo avait pour objectif de mobiliser la population pour des travaux collectif d'intérêt général, "avec enthousiasme et sans contrainte". Mais le Salongo était de fait un labeur forcé. Le manque d'enthousiasme de la population à l'égard du "salongo" mena à de vives résistances et un manque de motivation dans son exécution, ce qui amena de nombreux administrateurs locaux à chercher d'autres moyens de remplir leur objectifs. Le fait de ne pas accomplir les prestations obligatoires pouvait occasionner de un à six mois de prison vers la fin des années 1970, et bien peu de zaïrois s'opposèrent au "salongo". En recréant un des aspects les plus détestés du régime colonial, le service civil obligatoire ne contribua nullement à endiguer l'érosion de la légitimité du pouvoir en place.

Cependant, une paix et une stabilité relatives prévalurent jusqu'en 1978, lorsque des rebelles katangais, basés en Angola, lancèrent une série d'attaques pour envahir le Shaba (Katanga). Ils furent évincés avec l'aide de paras commandos, belges et français, qui furent largués sur Kolwezi.

Au cours des années 1980, le Zaïre demeura un État contrôlé par un parti unique. Bien que Mobutu gardât le contrôle de la situation au cours de cette période, des partis d'opposition étaient actifs, dont le plus connu était Union pour la Démocratie et le Progrès Social. Les tentatives de Mobutu pour contrer ces partis lui attirèrent de nombreuses critiques.

Avec la fin de la Guerre froide, les pressions intérieures et extérieures contre Mobutu s'accrurent. Entre fin 1989 et le début 1990, Mobutu se trouva affaibli par diverses contestations internes, des critiques grandissantes de la communauté internationale contre son régime et ses violations des droits de l'homme, son économie en déroute et la corruption de son gouvernement, et l'assujettissement du budget national aux fins personnelles de Mobutu

En mai 1990, Mobutu marqua son accord au multipartisme et au partage d'un pouvoir issu d'élections libre et à la promulgation d'une constitution. Alors que le processus commençait à s'enliser, des militaires déclenchèrent des pillages à Kinshasa en septembre 1991 pour protester contre le non-paiement de leurs soldes. militaires belges et français furent envoyés, dont certains amenés par des avions américains, pour évacuer les étrangers menacés à Kinshasa.

En 1992, après plusieurs tentatives infructueuses, la Conférence nationale souveraine tant attendue fut finalement mise en place, comprenant quelque représentants répartis dans près de 200 partis politiques le plus souvent basés sur une origine ethnique. La Conférence se donna un mandat législatif, élisant l'archevêque Laurent Monsengwo comme président de l'Assemblée, et Étienne Tshisekedi wa Mulumba, dirigeant de l'UDPS, Premier ministre. Les députés de cette conférence n’hésitent alors pas à critiquer ouvertement Mobutu, sa gestion économique catastrophique, son culte personnel. À la fin de l'année, Mobutu avait créé un gouvernement rival avec ses propres ministres et Premier ministre.

Dès 1991, Mobutu commençait à soutenir Nguz et Kyungu. Un gouvernement de compromis fut finalement mis en place en 1994, intégrant des personnes des deux tendances sous le nom de Haut Conseil de la République-Parlement de Transition (HCR-PT). Mobutu en était le chef de l'État et Kengo Wa Dondo Premier Ministre. Bien que des élections présidentielle et législatives aient été prévues pour deux années plus tard, elles n'eurent jamais lieu.

En 1996, les tensions avec l'État voisin du Rwanda s'accentuèrent, avec le déplacement des affrontements sur le territoire du Zaïre (voir Histoire du Rwanda). Les milices Hutu rwandaises (Interahamwe), qui avaient fui au Zaïre à l'arrivée du gouvernement Tutsi, utilisaient les camps de réfugiés établis dans le Zaïre oriental comme base arrière pour des incursions au Rwanda. Ces milices Hutues se coalisèrent rapidement avec les Forces armées zaïroises (FAZ) pour lancer des attaques contre les Tutsis zaïrois. Ceux-ci ne tardèrent pas à s'organiser pour contrer ces attaques. Quand le gouvernement zaïrois commença à être impliqué dans des massacres en novembre 1996, les milices Tutsies entrèrent en rébellion ouverte contre Mobutu.

La milice Tutsi fut rapidement rejointe par divers groupes d'opposition, et soutenue par plusieurs pays, dont notoirement le Rwanda et l'Ouganda. Cette coalition, dirigée par Laurent-Désiré Kabila, prit le nom d'Alliance des forces démocratiques pour la libération du Congo (AFDL). L'AFDL, dont l'ambition affichée désormais était la prise du pouvoir du pays, prit rapidement la direction de l'ouest, rencontrant peu de résistance, les premiers succès étant acquis début 1997. Des négociations intervinrent en mai 1997 entre Kabila et Mobutu, sous l'égide de Nelson Mandela, qui ne permirent pas à Mobutu de se maintenir au pouvoir. L'AFDL entra à Kinshasa le 17 mai. Kabila s'autoproclama président de la République, transforma l'AFDL de force militaire en organe de gestion du pouvoir et rendit au pays son nom de « République démocratique du Congo. »

Sur Internet, le domaine de premier niveau fut « ".zr" ».




</doc>
<doc id="19198" url="https://fr.wikipedia.org/wiki?curid=19198" title="CSharpBuilder">
CSharpBuilder

C# Builder est un environnement de développement intégré développé par Borland pour le développement d'applications sous le framework Microsoft .NET en langage C#. La première version date de juin 2003.

C#Builder est désormais intégré dans Borland Developpeur Studio.


</doc>
<doc id="19202" url="https://fr.wikipedia.org/wiki?curid=19202" title="TRS-80">
TRS-80

Le TRS-80 est une ligne de micro-ordinateurs et d'ordinateurs de poche construits par Tandy RadioShack dans les années 1980.

Introduit le 3 août 1977, doté d'un microprocesseur Zilog Z80, cadencé à 1,77 MHz, le modèle I dans sa première version dispose de de RAM et de de mémoire morte. Une deuxième version est par la suite dotée de de RAM et de ROM. Il dispose nativement en ROM d'un interpréteur BASIC. Le BASIC « étendu » de la deuxième version était fourni par Microsoft.

Sa mémoire vidéo d'un Ko autorise l'affichage en mode texte monochrome de 64 colonnes pour 16 lignes. Les caractères affichés ne peuvent être que les chiffres et les majuscules (ASCII codé sur 7 bits seulement). Un mode 32 colonnes × 16 lignes est également disponible et un mode « semi-graphique » permet un affichage de 128×48 points : en fait, l'emplacement de chaque caractère du mode texte est divisé en six pixels).

Le modèle de base était fourni sans mémoire de masse à accès direct. Les sauvegardes s'effectuaient sur bande magnétique "via" un lecteur/enregistreur de cassettes audio. Cette interface d'entrée/sortie audio est également utilisée pour sonoriser certains jeux.

La marque a distribué des périphériques divers, dont des lecteurs simples ou doubles de disquettes 5,25 pouces. Ces périphériques se connectent sur une « interface d'extension » qui permet également de porter la mémoire RAM à au total.

Comme la plupart des micro-ordinateurs de cette époque, l'unité centrale et le clavier se présentent sous une forme monobloc. L'alimentation est un transformateur/redresseur externe. Le refroidissement de l'unité centrale ainsi que de l'alimentation sont passifs, c'est-à-dire sans ventilateur, avec seulement des orifices permettant la circulation de l'air.

Ce micro-ordinateur était fourni avec un moniteur spécifique, monochrome (affichage blanc sur fond noir pour les premiers modèles, puis vert sur fond noir).

Ce modèle ayant eu un joli succès dans les années 1980, de nombreux fabricants ont développé des extensions diverses et variées (adaptateurs sonores, cartes couleurs, joystick, etc.). Il était alors en concurrence avec l'Apple II et le PET de Commodore. Il a eu droit aux côtés de l'Apple II à sa rubrique « Trucs & Astuces » de la revue "L'Ordinateur individuel", ainsi qu'à plusieurs revues dédiées (en France, la revue "Trace" éditée par la société Editrace).

Lorsqu'il est équipé de disquettes, le TRS 80 était fourni par Tandy avec le système d'exploitation TRSDOS. Cependant, sa qualité est si médiocre que des éditeurs indépendants ont développé d'autres systèmes d'exploitation, par exemple NEWDOS et LDOS. Une version spécifique de CP/M a également été adaptée au TRS 80.

La plupart de ces systèmes n'étaient pas compatibles avec le passage de l'an 1988. En effet, l'année était codée sur 3 bits à partir de 1980. Cette incompatibilité n'a été levée qu'avec la version 6.3 de LS-DOS qui autorise les dates jusqu'en 2011.

Il y eut quelques clones du TRS 80 modèle I, comme le Video Genie (en Europe) ou le DGT-100 brésilien.

Le modèle II, introduit en 1979, visait une clientèle plus professionnelle. Il intègre deux lecteurs de disquettes 8 pouces. Son microprocesseur Zilog Z80 passe à 4 MHz et il tourne sous TRSDOS II.

Le modèle III, introduit en 1980, est une évolution en monobloc (clavier / moniteur / unité centrale / lecteurs de disquettes 5,25 pouces) du modèle I. Elle intègre un processeur plus rapide, la possibilité d'afficher des caractères minuscules et accentués, et le respect des normes de compatibilité électromagnétique de la FCC.
En version de base, seul est fourni un lecteur de cassettes mais il est possible d'y adjoindre jusqu'à 2 lecteurs de disquettes 5,25 pouces.
Il comporte de RAM d'origine, extensible à .

Le modèle 4 (en chiffres arabes), introduit en 1984, est la dernière évolution de cette famille basée sur le Zilog Z80. Au-delà de son passage à la couleur blanche de sa carrosserie spécifique, elle dispose d'un mode de compatibilité très proche du modèle III. Un autre mode permet entre autres d'obtenir un mode d'affichage de 24 lignes sur 80 colonnes. Il est possible de le passer à de mémoire au total. Il tourne sous une version de TRSDOS qui est en fait dérivée de LDOS.

Une version portative de ce produit a également été créée : le modèle 4P.

Le modèle 4 a, entre autres, été fabriqué en France par Matra.

Le modèle 16 était destiné au marché professionnel, comme son ancêtre le modèle II dont il a gardé la carrosserie, passant cependant du gris au blanc.

Pour des raisons de compatibilité, il a conservé un Zilog Z80 pour tourner sous TRSDOS, mais est également équipé d'un Motorola 68000 sur lequel tourne Xenix, une version Unix de Microsoft.

Il est possible de lui adjoindre un terminal "via" une liaison RS232, terminal qui ressemble extérieurement à un modèle 4.

Surnommé « CoCo » aux États-Unis, ce modèle était clairement orienté grand-public et était alors en compétition avec le Commodore 64. Il était basé sur un Motorola 6809 et tournait sous OS/9 et connut un certain succès, notamment aux États-Unis.

Le ColorComputer a connu des clones brésiliens, comme le .

Les ordinateurs britanniques Dragon 32 et Dragon 64 de Dragon Data ressemblent beaucoup au ColorComputer, et certains de leurs périphériques sont compatibles. Les Dragon peuvent recevoir un lecteur disquette externe 5,25 pouces et tourner sous un système dénommé OS/9, système d'exploitation tirant parti au maximum des capacités du micro-ordinateur.

Le MC 10 est une version économique de l'Ordinateur Couleur. Il est basé sur un Motorola 6803.

Il a été fabriqué, pour la version européenne, en France par Matra Tandy Electronique, précisément à Wintzenheim (près de Colmar), de même que son clone nommé Alice. La différence entre les deux tient à la couleur, blanche pour le TRS-80 et rouge pour Alice, et aussi au fait qu'Alice était accompagné de logiciels éducatifs spécifiques, développés en collaboration avec l'éditeur Hachette, appartenant au groupe Matra. En outre, une carte d'extension mémoire, qui se branchait sur le port d'extension à l'arrière de la machine, était disponible sur Alice.

Ses concurrents à l'époque étaient les ZX81 de Sinclair

Le modèle 100 représente la première gamme d'ordinateurs vraiment portables. Il a été introduit en 1983. Il s'agit d'un produit doté d'un écran LCD de 8 lignes de 40 caractères et est basé sur un microprocesseur Intel 80C85 huit bits. Fabriqué par Kyocera, il était équipé de logiciels Microsoft, dont un BASIC et un éditeur de texte. Il a été aussi commercialisé par Olivetti et NEC.

Il dispose de touches « couper », « copier » et « coller », fonctions qui ont connu un grand succès par la suite. Sa grande autonomie et son modem intégré l'ont rendu assez populaire au début des années 1980 chez les journalistes. Bill Gates a participé personnellement, et pour une bonne part, au développement du firmware de cette machine, et ce fut son dernier travail en tant que programmeur. Il n'est pas compatible an 2000 (voir Y2K).

Le modèle 200 est une évolution du modèle 100 avec un écran de 16 lignes de 40 caractères, rabattable sur le clavier.

Le TRS-80 modèle 2000 est le premier compatible PC de Tandy.

La ligne de TRS-80 "Pocket Computers" (« Ordinateur de poche ») est constituée de versions estampillées Tandy de produits développés par Casio ou Sharp. Voici leurs équivalents :



</doc>
<doc id="19203" url="https://fr.wikipedia.org/wiki?curid=19203" title="Chimie quantique">
Chimie quantique

La chimie quantique est une branche de la chimie théorique qui applique la mécanique quantique aux systèmes moléculaires pour étudier les processus et les propriétés chimiques. Le comportement électronique et nucléaire des molécules étant responsable des propriétés chimiques ne peut être décrit adéquatement qu'à partir de l'équation du mouvement quantique (équation de Schrödinger) et des autres postulats fondamentaux de la mécanique quantique. Cette nécessité a motivé le développement de concepts (notamment orbitale moléculaire, ...) et de méthodes de calculs numériques qui ont permis à la chimie moderne de faire des progrès considérables tant en ce qui concerne la compréhension des phénomènes que des applications.

Comme les études sur la mécanique quantique sont considérées comme étant à la frontière entre la chimie et la physique, elles ne font donc généralement pas partie de la chimie quantique, mais ce que l'on considère comme le premier calcul de chimie quantique fut fait par les scientifiques allemands Walter Heitler et Fritz London sur la molécule de dihydrogène en 1927. La méthode de Heitler et London fut étendue aux États-Unis par les chimistes John C. Slater et Linus Pauling pour devenir la méthode « Valence-Bond (VB) » (ou Heitler-London-Slater-Pauling (HLSP)). Dans cette méthode, l'attention est surtout portée sur l'interaction des paires électroniques, et elle corrèle assez bien avec la représentation classique des liaisons chimiques par les chimistes.

Dans une autre approche développée par Friedrich Hund et Robert S. Mulliken, les électrons sont décrits comme des fonctions mathématiques délocalisées sur l'intégralité de la molécule. L'approche de Hund-Mulliken, plus communément appelée méthode des orbitales moléculaires est celle qui permet le mieux de prédire les propriétés des molécules et est donc la méthode la plus utilisée de nos jours.

Les équations de la chimie quantique devenant très rapidement trop complexes pour être résolues exactement, il est courant d'en faire une résolution numérique approchée à l'aide de supercalculateurs.






</doc>
<doc id="19206" url="https://fr.wikipedia.org/wiki?curid=19206" title="Philippe Kahn">
Philippe Kahn

Philippe Kahn (né le ) est un innovateur en technologie, et un entrepreneur. Il est connu comme l'inventeur de la première solution de partage de photographies pour téléphone appareil-photo, permettant le partage instantané sur les réseaux télécom publics.

Kahn a créé avec succès quatre sociétés du domaine technologique qui sont Borland Software, , , et .

Depuis 2003, il est PDG de Fullpower Technologies, société fournissant des solutions combinant la biologie, les technologies de réseau sans fil, les accéléromètres, les nanotechnologies, les microsystèmes électromécaniques, et le .

Ancien professeur de mathématiques à Grenoble et Cagnes-sur-Mer, il réside aujourd'hui à San Francisco.

Kahn a étudié à l'ETH Zurich, en Suisse (Institut fédéral polytechnique suisse), et à l'université de Nice, en France. Kahn a obtenu une maîtrise en mathématique. 

Lorsqu'il était étudiant, Kahn a développé des programmes pour le Micral, le premier ordinateur personnel utilisant un microprocesseur, fourni complet (c'est-à-dire qui ne soit pas à monter soi-même). Le Micral fut vendu pour des applications verticales, et est maintenant reconnu par le Computer History Museum comme étant le premier ordinateur personnel à base de microprocesseur créé en 1973.

Il a également étudié la musicologie et la flûte classique au Conservatoire de musique de Zurich.

Kahn parle couramment quatre langues : français, anglais, espagnol, allemand.

Kahn est marié à , qui a cofondé et . Il a eu trois enfants d'un premier mariage, Laura, Estelle et Samuel, puis Sophie avec Sonia Lee. Ils gèrent ensemble une organisation caritative pour l'environnement, la fondation Lee-Kahn.

Par ailleurs, il participe à des compétitions de voile et dirige l'équipe Pegasus Racing qui se concentre sur la navigation de bateaux de petite taille très performants et les courses à handicap. Il tient le blog de l'équipe

C'est aussi un pratiquant passionné de surf, de ski hors-piste, et de la musculation CrossFit.

Kahn est connu pour sa passion pour la musique classique et le jazz contemporain. Il joue de la flûte en semi-professionnel.

Considérant qu'un produit comme le Micral, bien qu'innovant, n'aurait pas un succès commercial suffisant en Europe, Kahn part aux États-Unis en tant que touriste en 1982, commercialisant en parallèle le logiciel Sidekick (devenu Borland Sidekick) au travers de petites annonces dans des journaux spécialisés en informatique. Il obtient un emploi chez Hewlett-Packard, avant de le perdre à cause de son statut illégal.
Après une période d'activité de conseil, il fonde Borland International, bien qu'il ne soit toujours pas légalement résident américain à cette époque. Après une période de quatre ans passée aux USA avec un visa de touriste, Kahn obtient une Carte verte en 1986. Il est aujourd'hui naturalisé citoyen américain.

Il arrive dans la Silicon Valley sans biens personnels. Pour assurer les fortes commandes associées aux annonces du logiciel Sidekick, il crée la société Borland. Borland débute dans un local de deux pièces, au-dessus d'un garage de réparation automobile Jaguar à Scotts Valley. Kahn aime plaisanter sur sa manière de démarrer une affaire « à l'américaine, dans un garage ». Kahn est à court d'argent, avec peu de revenus, et il est dans l'incapacité d'acheter une voiture pour sa famille à cause de son statut d'immigrant et son statut de crédit.

Charmaine R. Taylor, ancienne responsable de la publicité chez Zilog, coachète deux véhicules, et y place la publicité que son agence a créée pour les produits Borland. La publicité pour le premier produit Turbo Pascal répand le nom de Borland dans la Silicon Valley, et lance financièrement la société.

Le premier succès commercial de Borland est le premier organiseur de bureau, suivi par Turbo Pascal, lancé en novembre 1983 . Borland se développe pour posséder une ligne de produits d'outils de développement, ainsi qu'un ensemble de produits pour le bureau qui concurrencent directement Microsoft et Lotus/IBM.
Borland est introduit en bourse avec succès à Londres en 1986, suivi d'une introduction en bourse aux USA en 1989 et 1991. Kahn devient un dirigeant couronné de succès dans les affaires de hautes technologies.

Borland concurrence Microsoft dans les années 1980 ainsi qu'au début de la décennie suivante. Président, DG, et PDG de Borland depuis le début et menant Borland de la société en capital risque sans ventes jusqu'à un chiffre d'affaires de 500 millions d'USD, Kahn entre en désaccord stratégique avec le directoire de Borland. Kahn est forcé de démissionner par le directoire en janvier 1995. Toutefois, il siège au directoire en tant que directeur jusqu'en novembre 1996, démontrant jusqu'à sa démission son soutien et sa loyauté malgré la controverse. Kahn a assuré des responsabilités chez Borland pendant douze ans.

Borland est racheté par Micro Focus le 6 mai 2009. Le journal "San Jose Mercury News" rapporte que « Philippe Kahn, qui fonda la compagnie en 1983 et la dirigea jusqu'en 1994, présente l'accord comme un résultat adéquat et comme une synergie pour les deux sociétés et un excellent aboutissement pour les employés, les clients, et les actionnaires. »

Philippe Kahn et Sonia Lee fondent Starfish en 1994, dont la vision fondatrice : « La synchronisation et l'intégration globales des équipements avec fil et sans fil », se traduit par la plateforme TrueSync, afin de : « Saisir et modifier de l'information n'importe où, avec une synchronisation automatique partout ».

Starfish développe l'essentiel de la synchronisation de base pour les équipements, notamment pour l'industrie du sans fil. TrueSync est le premier système de synchronisation sur les ondes (Over-The-Air (OTA)). Starfish est acheté par Motorola pour USD en 1998 et conséquemment devient un département de Nokia après le déclin de Motorola. La technologie de Starfish est une partie intégrante de la plateforme Nokia.




</doc>
<doc id="19210" url="https://fr.wikipedia.org/wiki?curid=19210" title="Champ électromagnétique">
Champ électromagnétique

Un champ électromagnétique est la représentation dans l'espace de la force électromagnétique qu'exercent des particules chargées. Concept important de l'électromagnétisme, ce champ représente l'ensemble des composantes de la force électromagnétique s'appliquant sur une particule chargée se déplaçant dans un référentiel galiléen.
Une particule de charge "q" et de vecteur vitesse formula_1 subit une force qui s'exprime par :
formula_2
où formula_3 est le champ électrique et formula_4 est le champ magnétique. Le champ électromagnétique est l'ensemble formula_5. 

Le champ électromagnétique est en effet la composition de deux champs vectoriels que l'on peut mesurer indépendamment. Néanmoins ces deux entités sont indissociables :

Le comportement des champs électromagnétiques est décrit de façon classique par les équations de Maxwell et de manière plus générale par l'électrodynamique quantique.

La façon la plus générale de définir le champ électromagnétique est celle du tenseur électromagnétique de la relativité restreinte.

La valeur attribuée à chacune des composantes électrique et magnétique du champ électromagnétique dépend du référentiel d'étude. En effet, on considère généralement en régime statique que le champ électrique est créé par des charges au repos tandis que le champ magnétique est créé par des charges en mouvement (courants électriques). Néanmoins, la notion de repos et de mouvement est relative au référentiel d'étude.

Cependant, depuis la définition qu'en donnent les équations de Maxwell et depuis l'interprétation d'Einstein, contrairement aux champs électriques et magnétiques qui peuvent être statiques par rapport à un référentiel correctement choisi, la particularité caractéristique du champ électromagnétique est toujours d'être sujet à propagation, à la vitesse de la lumière, quel que soit le référentiel choisi.

Dans le cadre de la relativité galiléenne, si on considère deux référentiels d'étude galiléens (R) et (R'), avec (R') en mouvement rectiligne uniforme de vitesse V par rapport à (R), et si on appelle v' la vitesse d'une charge "q" dans (R'), sa vitesse dans (R) est v = v' + V.

Si on appelle (E, B) et (E', B') les composantes du champ électromagnétique respectivement dans (R) et dans (R'), l'expression de la force électromagnétique devant être identique dans les deux référentiels on obtient la transformation des champs électromagnétiques grâce à :
formula_6

Cette relation étant vraie quelle que soit la valeur de v' on a :
formula_7 et formula_8

La fréquence d’un champ électromagnétique est le nombre de variations du champ par seconde. Elle s’exprime en hertz (Hz) ou cycles par seconde, et s’étend de zéro à l’infini. Une classification simplifiée des fréquences est présentée ci-après, et quelques exemples d’applications dans chaque gamme sont indiqués. 

Les rayonnements X et gamma peuvent rompre les liaisons moléculaires et être à l'origine d'ionisations, facteur cancérigène.

Les rayonnements ultra-violets, visibles et infra-rouges (300 GHz - 385 THz) peuvent modifier les niveaux d'énergie au niveau des liaisons au sein des molécules.

L’intensité d’un champ est exprimée à l’aide de différentes unités :


La polarisation : orientation du champ électrique dans le rayonnement

La modulation : 

Lorsque l’émission est modulée, il faut différencier la puissance maximale, appelée puissance-crête, et la puissance moyenne résultant de la modulation. Par exemple, dans une émission radar avec des impulsions d’une durée de 1 ms toutes les secondes, la puissance moyenne est 1000 fois inférieure à la puissance-crête dans l’impulsion.

Les champs électromagnétiques peuvent avoir une influence non désirée sur certains équipements électriques ou électroniques (on parlera de compatibilité électromagnétique) et .

Des réglementations spécifiques ont été adoptées dans la plupart des pays pour limiter les expositions aux champs électromagnétiques ; pour les équipements (directive CEM en Europe) et pour les personnes (recommandation 1999/519/CE et directive 2004/40/CE en Europe).

Dans le monde, l'exposition des personnes et de l'environnement et les facteurs de risques font depuis les années 1960 l'objet d'études, contradictoire, portant sur l'étude sur le degré potentiel de nocivité ou non nocivité de certains champs électromagnétiques. À ce jour, il est recommandé, par principe de précaution, de limiter l’exposition des personnes à risque, tels les femmes enceintes, les enfants, ainsi que les personnes « électrosensibles ». Les principales sources à éviter sont les lignes haute-tension, les IRM, et tout émetteur radiofréquence (GSM, 3G, Wi-Fi...).

Pour améliorer la connaissance et le contrôle de l'exposition du public, en France, suite aux lois Grenelle 1 et Grenelle 2, un décret du décembre 2011 impose aux gestionnaires du réseau public de transport d'électricité un contrôle et des mesures des ondes électromagnétiques produites par les lignes électriques à très haute tension (THT), lors de toute mise (ou remise) en service d'une ligne.





</doc>
<doc id="19211" url="https://fr.wikipedia.org/wiki?curid=19211" title="Gaz noble">
Gaz noble

On appelle gaz nobles, ou gaz rares, les éléments chimiques du groupe 18 (anciennement « "groupe VIIIA" », voire « "groupe 0" ») du tableau périodique. Ce sont l'hélium He, le néon Ne, l'argon Ar, le krypton Kr, le xénon Xe et le radon Rn, ce dernier étant radioactif, avec une période de pour le , son isotope le plus stable. Ils forment une famille d'éléments chimiques très homogène de gaz monoatomiques incolores et inodores chimiquement très peu réactifs, voire totalement inertes pour les deux plus légers — hormis dans des conditions très particulières. L'oganesson Og, découvert au début du , prolonge le , mais ses propriétés chimiques sont encore trop largement méconnues pour pouvoir le ranger dans une quelconque famille ; les effets relativistes d'un noyau atomique très chargé sur son cortège électronique pourraient en altérer suffisamment les propriétés, de sorte que cet élément, qui serait probablement solide et non gazeux, ne serait plus nécessairement un gaz noble.

Les propriétés des gaz nobles s'accordent bien avec les théories modernes décrivant la structure des atomes. Leur couche de valence est saturée, de sorte qu'ils n'établissent normalement pas de liaison covalente avec d'autres atomes, d'où leur inertie chimique. On ne connaît que quelques centaines de composés de gaz nobles, essentiellement du xénon. À pression atmosphérique, la différence entre la température d'ébullition et la température de fusion d'un gaz noble n'excède jamais , de sorte qu'ils n'existent à l'état liquide que dans un intervalle de températures très étroit.

On obtient le néon, l'argon, le krypton et le xénon à partir de l'atmosphère terrestre par liquéfaction et distillation fractionnée. L'hélium provient du gaz naturel, dont il est extrait par des techniques de séparation cryogénique. Le radon est généralement isolé à partir de la désintégration radioactive de composés de radium, de thorium ou d'uranium dissous.

La nature chimiquement inerte des gaz nobles les rend utiles pour toutes les applications où les réactions chimiques sont indésirables. L'argon est ainsi utilisé dans les ampoules à incandescence pour éviter l'oxydation du filament de tungstène. Dans un autre registre, l'hélium est utilisé en plongée sous-marine comme gaz respiratoire sous forme d'héliox ou de trimix pour limiter à la fois les turbulences du gaz circulant dans l'équipement respiratoire, la toxicité de l'azote (narcose à l'azote) et la toxicité de l'oxygène (hyperoxie). Les gaz nobles sont par ailleurs utilisés dans des domaines aussi divers que l'éclairage, le soudage, ou encore l'astronautique.

L’appellation "gaz rares" vient de la faible prévalence historique des gaz nobles comme substances chimiques, bien que cette désignation soit techniquement impropre car l'hélium constitue 24 % de la matière baryonique de l'univers, et l'argon 0,94 % de l'atmosphère terrestre au niveau de la mer, de sorte qu'ils ne sont pas rares.

L'appellation "gaz inertes" rencontrée jadis est tombée en désuétude depuis qu'on a synthétisé près d'un millier de composés et d'espèces chimiques contenant chacun des six gaz rares, bien que ces espèces requièrent souvent — mais pas nécessairement — des conditions hors équilibre très particulières pour exister.

Préconisée par l'IUPAC et le Bulletin officiel du ministère français de l'Éducation nationale, l'appellation "gaz nobles", issue de l'allemand "" par analogie avec les métaux nobles (tels que l'or, également peu réactif), apparaît donc de plus en plus comme devant légitimement remplacer à terme celle de "gaz rares" ; c'est celle retenue dans cet article.

Les gaz nobles présentent des interactions interatomiques faibles, de sorte que leur température d'ébullition est très basse. Dans les conditions normales de température et de pression, ce sont tous des gaz monoatomiques : le radon est ainsi gazeux alors que sa masse atomique est supérieure à celle du plomb et du bismuth, par exemple. L'hélium se distingue des autres gaz nobles à différents égards : sa température de fusion et sa température d'ébullition sont plus basses que celles de tous les autres substances connues ; c'est le seul élément connu qui ne peut être solidifié à pression atmosphérique (il faut une pression d'au moins à pour ce faire) ; c'est le seul élément connu présentant le phénomène de superfluidité. Les températures de fusion et d'ébullition des gaz nobles augmentent avec leur numéro atomique, c'est-à-dire en descendant le long de leur colonne du tableau périodique.

La configuration électronique des gaz nobles est caractérisée par le fait que leurs sous-couches "s" et "p" externes sont complètes, avec respectivement deux et six électrons, de sorte qu'il ne leur reste pas d'électron de valence disponible pour établir une liaison chimique avec un autre atome, en vertu de la règle de l'octet. C'est ce qui explique leur inertie chimique. Cette inertie est plus relative pour le krypton et plus encore pour le xénon, dont on a isolé plusieurs centaines de composés, certains étant stables à température ambiante. Le radon semble également assez réactif, mais sa radioactivité en a freiné l'étude. L'oganesson aurait, selon les simulations numériques, une configuration électronique affectée par des couplages spin-orbite lui conférant une réactivité chimique comparable à celle de la plupart des autres éléments ; les données numériques ci-dessous relatives à l'oganesson sont issues d'extrapolations numériques et sont présentées à titre indicatif, cet élément n'étant "a priori" pas rangé parmi les gaz nobles.

Les gaz nobles jusqu'au xénon ont chacun plusieurs isotopes stables ; le radon, en revanche, n'en a aucun : c'est un élément radioactif. Le , son isotope le plus stable, présente une période radioactive de et donne du par désintégration α, qui donne en fin de compte du plomb.

Le rayon atomique des gaz nobles augmente en descendant le long de la en raison du nombre croissant de sous-couches électroniques. La taille de ces atomes détermine plusieurs de leurs propriétés. Ainsi, le potentiel d'ionisation décroît lorsque le rayon atomique augmente car les électrons de valence sont de plus en plus éloignés du noyau et interagissent par conséquent de moins en moins étroitement avec ce dernier. Le potentiel d'ionisation des gaz nobles est le plus élevé des éléments de chaque période, ce qui reflète la stabilité de leur configuration électronique, caractérisée par la saturation de leur couche de valence.

La valeur élevée de leur potentiel d'ionisation est également liée à leur faible réactivité chimique. Les gaz nobles les plus lourds, en revanche, ont une énergie d'ionisation qui devient comparable à celle d'autres éléments chimiques et de certaines molécules. C'est l'observation du fait que l'énergie d'ionisation du xénon est du même ordre que celle de la molécule d'oxygène qui a conduit Neil Bartlett à tenter d'oxyder le xénon avec de l'hexafluorure de platine , connu pour oxyder l'oxygène, ce qui permit de synthétiser l'hexafluoroplatinate de xénon, premier composé du xénon connu.

Les gaz nobles ne sont pas des accepteurs d'électrons susceptibles de former des anions stables : leur affinité électronique est négative.

Les propriétés macroscopiques des gaz nobles sont dominées par leurs faibles forces de van des Waals entre les atomes. Cette force attractive augmente avec la taille des atomes car leur polarisabilité augmente et leur potentiel d'ionisation diminue. Il s'ensuit que, lorsqu'on descend le long de la colonne, les températures de fusion et d'ébullition croissent, de même que l'enthalpie de vaporisation et la solubilité.

Les gaz nobles sont pratiquement des gaz parfaits aux conditions normales de température et de pression, mais les écarts observés par rapport à la loi des gaz parfaits a fourni des éléments clés permettant l'étude des interactions intermoléculaires. Leur potentiel de Lennard-Jones, souvent utilisé pour modéliser les interactions intermoléculaires, a été déduit par John Lennard-Jones à partir de données expérimentales sur de l'argon, avant que le développement de la mécanique quantique fournisse les outils permettant de les comprendre<ref name="10.1098/rsbm.1955.0013">
</ref>. Les analyses théoriques de ces interactions étaient relativement aisées dans la mesure où les gaz nobles sont monoatomiques, avec des atomes sphériques, de sorte que les interactions entre atomes sont indépendantes de leur directions, c'est-à-dire qu'elles sont isotropes.

L'abondance des gaz nobles dans l'univers est inversement proportionnelle à leur numéro atomique. L'hélium est le plus abondant d'entre eux, et le deuxième élément le plus abondant dans l'univers après l'hydrogène, constituant environ 24 % de la masse de la matière baryonique. L'essentiel de l'hélium de l'univers a été formée lors de la nucléosynthèse primordiale à la suite du Big Bang, et sa quantité totale augmente régulièrement en raison de la chaîne proton-proton de la nucléosynthèse stellaire<ref name="10.1086/380121">
</ref>.

Sur Terre, l'abondance relative des gaz nobles est différente. L'hélium n'est que le troisième gaz noble le plus abondant de l'atmosphère terrestre. L'hélium est en effet trop léger pour que l'hélium primordial ait pu être retenu par la gravité terrestre, de sorte que l'hélium présent sur Terre provient de la désintégration α d'éléments radioactifs tels que l'uranium et le thorium de l'écorce terrestre<ref name="10.1111/j.1749-6632.1955.tb35366.x">
</ref>.

L'argon, en revanche, est plus abondant sur Terre que dans l'univers car il provient de la désintégration β du , présent lui aussi dans l'écorce terrestre, et qui donne de l', principal isotope de l'argon terrestre, qui est pourtant assez rare dans le Système solaire : ce phénomène est à la base de la datation par le potassium-argon.

Le xénon est anormalement peu abondant sur Terre, ce qui a longtemps constitué une énigme. Il est possible qu'il soit piégé dans les minéraux<ref name="10.1126/science.1119070">
</ref>, le dioxyde de xénon pouvant se substituer au dioxyde de silicium dans les silicates, comme le quartz.

Le radon se forme dans la lithosphère par désintégration α du radium.

L'hélium est obtenu par distillation fractionnée à partir du gaz naturel, qui peut contenir jusqu'à 7 % d'hélium. Le néon, l'argon, le krypton et le xénon sont obtenus à partir de l'air par liquéfaction puis distillation fractionnée. L'argon est celui dont le coût de revient est le plus bas, tandis que le xénon est le gaz noble le plus cher.

La chimie des gaz nobles est étudiée expérimentalement depuis les années 1960 et les travaux de Neil Bartlett<ref name="10.1039/PS9620000197">
</ref> sur l'hexafluoroplatinate de xénon. Le xénon est en effet le plus réactif des gaz nobles — hormis le radon, trop radioactif pour être étudié en détail — et il forme de nombreux oxydes et fluorures dans lesquels le xénon présente des états d'oxydation 2, 4, 6 et 8 :

Les trioxyde et tétraoxyde de xénon sont solubles dans l'eau, où ils donnent deux oxoacides, respectivement l'acide xénique et l'acide perxénique . Ce dernier donne des perxénates tels que le perxénate de sodium , le perxénate de potassium , ou encore le perxénate de baryum .

La grande majorité des composés du xénon produits jusque dans les années 1980 combinaient le fluor et/ou l'oxygène avec le xénon<ref name="10.1021/ed051p628">
</ref>; lorsqu'ils intégraient d'autres éléments, comme l'hydrogène ou le carbone, c'étaient généralement avec des atomes électronégatifs d'oxygène et/ou de fluor. Néanmoins, une équipe animée par Markku Räsänen de l'Université d'Helsinki a publié en 1995 la synthèse du dihydrure de xénon , puis celle de l'hydroxyhydrure de xénon , de l'hydroxénoacétylène et d'autres composés du xénon<ref name="10.1146/annurev.physchem.55.091602.094420">
</ref>. Par la suite, Khriatchev ont publié la synthèse du composé par photolyse d'eau dans une matrice de xénon cryogénique<ref name="10.1021/ja077835v">
</ref>. Ils ont également fait état des molécules deutérées HXeOD et DXeOD<ref name="10.1021/ja9932784">
</ref>. Le nombre de composés connus du xénon est aujourd'hui de l'ordre du millier, certains présentant des liaisons entre le xénon et le carbone, l'azote, le chlore, l'or ou le mercure<ref name="10.1039/B702109G">
</ref>, tandis que d'autres, observés dans des conditions extrêmes (matrices cryogéniques ou jets gazeux supersoniques) présentent des liaisons entre le xénon et l'hydrogène, le bore, le béryllium, le soufre, l'iode, le brome, le titane, le cuivre et l'argent.

L'un des composés les plus inattendus du xénon est le complexe qu'il forme avec l'or. Le cation tétraxénon-or a en effet été caractérisé par l'équipe allemande de Konrad Seppelt dans le complexe <ref name="10.1126/science.290.5489.117">
</ref>.

Si la chimie des gaz nobles est essentiellement celle du xénon, il existe néanmoins des composés chimiques avec d'autres gaz nobles que le xénon. On connaît ainsi le trioxyde de radon ainsi que le difluorure de radon . Le krypton forme du difluorure de krypton , lequel donne les cations KrF et <ref name="10.1016/S0010-8545(02)00202-3">
</ref>. Les gaz nobles plus légers forment également des exciplexes, c'est-à-dire des molécules qui ne sont stables qu'à l'état excité, notamment utilisés pour faire des lasers (laser à excimère). On connaît également de nombreux ions moléculaires de gaz nobles, même des moins réactifs d'entre eux, comme le néon, qui donne les ions HNe, HeNe, Ne2 et NeAr.

Les gaz nobles présentent une température de fusion et une température d'ébullition particulièrement basses, d'où leur utilisation comme réfrigérants cryogéniques. En particulier, l'hélium liquide, qui bout à à pression atmosphérique, est utilisé avec les comme ceux utilisés en imagerie par résonance magnétique (IRM) et en résonance magnétique nucléaire (RMN)<ref name="10.1109/20.120038">
</ref>. Le néon liquide est également utilisé en cryogénie, malgré le fait qu'il n'atteint pas les températures aussi froides que l'hélium liquide, car il présente une capacité réfrigérante quarante fois supérieure à celle de l'hélium liquide, et plus du triple de celle de l'hydrogène liquide.

L'hélium est utilisé comme constituant des gaz respiratoires à la place de l'azote en raison de sa faible solubilité dans les fluides physiologiques, notamment dans les lipides. Les gaz sont absorbés par le sang et les tissus biologiques sous pression, par exemple en plongée sous-marine, provoquant un effet anesthésiant appelé narcose à l'azote. En raison de sa solubilité réduite, l'hélium est peu absorbé dans les membranes cellulaires, de sorte que l'utilisation de l'hélium à la place de l'azote dans lehéliox ou le trimix réduit l'effet narcotique du gaz respiratoire an cours de la plongée. Cette faible solubilité présente également l'avantage de limiter les risques d'accident de décompression, car les tissus comportent moins de gaz dissous susceptible de former des bulles lorsque la pression diminue lors de la remontée. L'argon, quant à lui, est considéré comme le meilleur candidat pour remplir les combinaisons de plongée.

L'hélium est utilisé, pour des raisons de sécurité, à la place de l'hydrogène dans les dirigeables et les ballons, malgré une perte de portance de 8,6 %.

Les gaz nobles sont également utilisés dans de nombreuses applications requérant une atmosphère chimiquement inerte. L'argon est utilisé pour la synthèse de composés sensibles à l'azote atmosphérique. L'argon solide permet d'étudier les molécules très instables en les immobilisant dans une matrice solide à très basse température qui empêche les contacts et les réactions de décomposition<ref name="10.1039/CS9800900001">
</ref>. L'hélium peut être utilisé en chromatographie en phase gazeuse (CPG), pour remplir les thermomètres à gaz, et dans les appareils de mesure de la radioactivité, comme les compteurs Geiger et les chambres à bulles. L'hélium et l'argon sont couramment utilisés pour isoler les métaux de l'atmosphère lors de la découpe ou du soudage à l'arc électrique, ainsi que pour divers autres procédés métallurgiques et pour la production de silicium par l'industrie des semiconducteurs.

Les gaz nobles sont couramment utilisés pour l'éclairage en raison de leur inertie chimique. L'argon mélangé à l'azote est utilisé pour remplir les ampoules des lampes à incandescence<ref name="10.1002/14356007.a17_485">
</ref>, ce qui prévient l'oxydation de filament en tungstène tout en limitant la redéposition du tungstène sublimé sur les parois de l'ampoule. Le krypton est utilisé pour les ampoules à hautes performance avec une température de couleur plus élevée et un meilleur rendement énergétique car il réduit le taux d'évaporation du filament par rapport aux ampoules à argon ; en particulier, les lampes à halogènes utilisent un mélange de krypton avec de petites quantités de composés d'iode et de brome. Les gaz nobles luisent avec des couleurs particulières lorsqu'ils sont utilisés dans les lampes à décharge, comme les « tubes au néon ». Malgré leur appellation commune, ces lampes contiennent généralement d'autres gaz que du néon comme substance phosphorescente, ce qui ajoutent de nombreuses teintes à la couleur rouge orangé du néon. Le xénon est couramment utilisé dans les en raison de leur spectre quasiment continu qui ressemble à la lumière du jour, avec des applications comme projecteurs de cinéma et phares automobiles.

Les gaz nobles sont utilisés pour réaliser des lasers à excimères, dont le principe repose sur l'excitation électronique de molécules pour former des excimères. Il peut s'agir de dimères tels que , ou , ou plus souvent d'espèces halogénées telles que ArF, KrF, XeF ou XeCl. Ces lasers produisent une lumière ultraviolette dont la faible longueur d'onde ( pour ArF et pour KrF) permet de réaliser des images à haute résolution. Les lasers à excimères ont de nombreuses applications industrielles, médicales et scientifiques. On les utilise en microlithographie et en microfabrication, technologies essentielles à la réalisation des circuits intégrés, ainsi que pour la chirurgie au laser, comme l'angioplastie et la chirurgie oculaire.

Certains gaz nobles ont des applications directes en médecine. L'hélium est parfois utilisé pour faciliter la respiration des personnes asthmatiques, et le xénon est utilisé en anesthésie à la fois en raison de sa solubilité élevée dans les lipides, qui le rend plus efficace que le protoxyde d'azote , et parce qu'il s'élimine facilement de l'organisme, ce qui permet une récupération plus rapide<ref name="10.1093/bmb/ldh034">
</ref>. Le xénon est également utilisé en imagerie médicale des poumons par IRM hyperpolarisée<ref name="10.1016/S0168-9002(97)00888-7">
</ref>. Le radon, qui n'est disponible qu'en petites quantités, est utilisé en radiothérapie.




</doc>
<doc id="19213" url="https://fr.wikipedia.org/wiki?curid=19213" title="Givenchy-en-Gohelle">
Givenchy-en-Gohelle

Givenchy-en-Gohelle est une commune française située dans le département du Pas-de-Calais en région Hauts-de-France. Elle fait partie de la Communaupole de Lens-Liévin (communauté d'agglomération) qui regroupe et comptait en 2010.

La ville se trouve dans l'arrondissement d'Arras mais se situe dans la banlieue de Liévin (arrondissement de Lens).

Le village de Givenchy a été complètement détruit lors de la Première Guerre mondiale, dès 1914.

Par arrêté préfectoral du , la commune est détachée le de l'arrondissement d'Arras pour intégrer l'arrondissement de Lens.

La population de la commune est relativement jeune. Le taux de personnes d'un âge supérieur à 60 ans (19,3 %) est en effet inférieur au taux national (21,6 %) et au taux départemental (19,8 %). 
À l'instar des répartitions nationale et départementale, la population féminine de la commune est supérieure à la population masculine. Le taux (51,6 %) est du même ordre de grandeur que le taux national (51,6 %).

La répartition de la population de la commune par tranches d'âge est, en 2007, la suivante :

La commune de Givenchy-en-Gohelle présente la particularité d'avoir planté de nouveau de la vigne (chardonnay et pinot gris). Les premières vendanges ont eu lieu en 2000.

Ainsi que bien d'autres églises de la région, l'église de Givenchy-en-Gohelle fut fondée à l'époque mérovingienne comme en témoigne son vocable de saint Martin. Contrairement à l'édifice actuel, la première église était située "sur les monts", à flanc de coteau et plus précisément en bordure de l'actuelle rue Renan.

Pillée et ruinée à plusieurs reprises lors des campagnes militaires des s, elle fut restaurée à la fin du . Flanquée d'une tour-clocher aux puissants contreforts, elle assurait la surveillance aux abords de la paroisse et la tradition rapporte que cette tour protégeait l'entrée d'un souterrain (carrière de craie).

À la Révolution, l'église ne cessait de se délabrer, petite, ne comportant qu'une seule nef, mal éclairée, elle ne convenait plus à la population devenue nombreuse. Le à 16 h la nef s'écroula, il n'y eut aucune victime. Le maire décida alors de la raser mais le curé et les paroissiens s'y opposèrent et se regroupèrent dans le petit chœur.

En 1871, la vieille tour-clocher s'effondra, il fallait donc procéder à la reconstruction d'une nouvelle église. Afin de réunir Givenchy et le hameau Givenchisel, l'église fut reconstruite au lieu-dit "la Chapelle" au milieu des terrains agricoles. De style néogothique, elle fut construite entre 1871 et 1873. En 1874, on construisit le cimetière derrière l'église. Elle fut détruite lors de la Première Guerre mondiale.

L'église Saint-Martin fut à nouveau reconstruite au même emplacement et sur des plans identiques à la précédente. La reconstruction débuta en 1924, cependant les travaux furent arrêtés en décembre 1927 à la suite de la faillite de l'entrepreneur Société régionale d'entreprise de la Seine. Ils reprirent seulement en 1931.

L'église fut inaugurée le par monseigneur Dutoit, évêque d'Arras.

Elle possède deux cloches : 

C'est sur le territoire de Givenchy-en-Gohelle que se trouve le plus important monument canadien aux victimes de la Première Guerre mondiale. Il rend hommage au rôle des Canadiens lors de ce conflit, au moyen de figures de pierre symbolisant les valeurs défendues et les sacrifices faits. Érigée entre 1925 et 1936 sur le site de la bataille de la crête de Vimy, cette œuvre d'art est le fruit du travail d'artistes canadiens.
Les deux pylônes, représentant le Canada et la France, culminent 27 mètres au-dessus de la base du monument. En raison de l'altitude du site, la figure la plus élevée - l'allégorie de la paix - domine la plaine de Lens d'environ .

Le terrain d'assise du mémorial ainsi que la centaine d'hectares qui l'entoure ont été donnés au Canada par la France en 1922. Cela en signe de gratitude pour les sacrifices faits par plus de Canadiens au cours de la "Grande Guerre" et notamment pour la victoire remportée par les troupes canadiennes en conquérant la "crête de Vimy" au cours du mois d'.

En s'avançant à l'avant du monument, on peut remarquer une statue de femme voilée, tournée vers l'est, vers l'aube d'un nouveau jour. Elle représente le Canada, une jeune nation, pleurant ses fils tombés au combat. La crête de Vimy est aujourd'hui boisée, chaque arbre a été planté par un Canadien et symbolise le sacrifice d'un soldat.

Les pierres calcaires choisies par Walter Allward viennent de Croatie. Elles sont montées sur une structure en béton. Les pierres d'origine s'étant abîmées avec le temps, des travaux de restauration ont été entrepris en 2005 et se sont achevés en 2007. La reine Élisabeth II a participé à l'inauguration le 7 avril 2007.

Un monument édifié en juin 1925, sur le plateau de Vimy, en face du mémorial canadien, rend hommage à la Division marocaine et aux centaines de milliers de soldats étrangers engagés pour la France pendant la Grande Guerre.





</doc>
<doc id="19214" url="https://fr.wikipedia.org/wiki?curid=19214" title="Approximation de Born-Oppenheimer">
Approximation de Born-Oppenheimer

La première étape dans la simplification de l’équation de Schrödinger est l’approximation de Born et Oppenheimer qui consiste à découpler le mouvement des électrons de celui des noyaux, en s’appuyant sur l’importante différence de masse entre les deux types de particules 2. En effet, à cause de leur masse plus élevée (mp = 1836 me), le mouvement des noyaux est beaucoup plus lent que celui des électrons, et les électrons s’adaptent de manière adiabatique à la position des noyaux.

En d’autres termes, cela signifie que l’échelle de temps associée aux excitations électroniques, qui est proportionnelle à l’inverse de la largeur de bande de transition électronique, est usuellement plus petite que celle caractérisant les ions c’est-à-dire l’inverse des fréquences de phonons. Par conséquent, la configuration électronique peut être considérée comme étant totalement relaxée dans son état fondamental à chaque position que les ions prennent durant leur mouvement. Cette observation offre ainsi la possibilité de découpler les mouvements nucléaires et électroniques de sorte que l’on peut envisager la séparation des variables électroniques et nucléaires. La fonction d’onde totale du système peut, dans ce cas, être écrite comme le produit d’une fonction d’onde décrivant les noyaux formula_1 et d’une autre fonction d’onde décrivant les électrons et ne dépendant que de façon paramétrique des positions ioniques (c’est-à-dire ne dépend que de la position instantanée des noyaux et pas de leur dynamique).formula_2Où formula_3 est le jeu de toutes les coordonnées nucléaires et formula_4 est celui des électrons contenus dans le système.

Dans l’approximation de Born-Oppenheimer on considère donc les noyaux comme immobiles (d’où formula_5) et on étudie le mouvement des électrons dans un réseau cristallin rigide : les noyaux sont “privés de leur statut dynamique” et sont réduits à une charge positive qui est devenue “externe” au nuage électronique. Le problème à formula_6 corps a été simplifié dans la mesure où les seules particules à considérer sont désormais les Ne électrons chargés négativement et se déplaçant dans le potentiel maintenant externe des noyaux. Dans le cadre de cette approximation, on peut alors considérer que les électrons peuvent être traités de façon adiabatique. Le traitement adiabatique consiste à négliger les termes couplés (formula_7) non-adiabatiques (interaction électronphonon) qui proviennent de l’opérateur cinétique des noyaux agissant sur la fonction d’onde électronique formula_8. Les conséquences de cette double simplification peuvent être mesurées en évaluant l’évolution des termes contenus dans l’hamiltonien total du système et le nouvel hamiltonien issu de l’approximation de Born-Oppenheimer :formula_9Le terme d’énergie cinétique nucléaire, indépendant des électrons, s’annule ( formula_5), la corrélation dans l’énergie potentielle attractive électron-noyau est éliminée, et le terme d’énergie potentielle de répulsion noyau-noyau devient une constante évaluée simplement pour une géométrie déterminée. Les parties non constantes de l’hamiltonien issues de cette double approximation de Born-Oppenheimer et adiabatique sont ainsi l’énergie cinétique du gaz d’électrons, l’énergie potentielle due aux interactions électron-électron et l’énergie potentielle des électrons dans le potentiel désormais externe des noyaux formula_11 vérifie l’équationformula_12Avecformula_13L’énergie totale de ce système, appelée énergie de Born-Oppenheimer, est alors la somme de l’énergie du gaz d’électrons formula_14 et de l’énergie électrostatique des ions.formula_15A ce niveau, on voit qu’il est possible de déterminer les positions des noyaux correspondant à l’état fondamental du cristal : ce seront celles qui minimisent formula_16. L’hamiltonien n’est de ce fait constitué que par des contributions de type électronique (monoformula_17 et biélectronique: formula_18). En dehors du nombre d’électrons propre au système, ces parties peuvent être considérées comme étant universelles. L’information spécifique au système – nature des noyaux et des positions atomiques – est contenue entièrement dans formula_19. Dans la majeure partie des systèmes, cette approximation correspond à une simplification raisonnable étant donné que les termes négligés sont de l’ordre du rapport entre la masse électronique effective et la masse ionique, formula_20, et sont par conséquent inférieurs à formula_21. Cet ordre de grandeur est plus faible que les erreurs commises généralement à partir des autres approximations utilisées pour résoudre l’équation de Schrödinger. Nous allons donc nous intéresser seulement à la détermination de formula_14.

Bien que la double approximation de Born-Oppenheimer permette de réduire de façon significative le degré de complexité inhérent à la résolution de l’équation de Schrödinger, “l’équation électronique” formula_23 restant à résoudre demeure un problème à N corps. La nouvelle fonction d’onde totale du système dépend des coordonnées de tous les électrons et ne peut pas être découplée en contributions à une seule particule en raison de leur interaction mutuelle de sorte que le problème reste beaucoup trop complexe pour être résolu dans des calculs utilisant les ressources informatiques actuelles. En raison de cette difficulté, des approximations supplémentaires sont requises pour réaliser de façon effective la résolution de l’équation de Schrödinger pour les matériaux réels.

On peut résumer les deux étapes de la méthode pour une molécule diatomique, dont les noyaux, qui sont considérés comme ponctuels
vis-à-vis de l'étendue du mouvement des électrons, sont distants d'une longueur R.

On considère une molécule formée de deux atomes, A et B, de masse formula_29 et formula_30, de numéro atomique formula_31 et formula_32. Ces deux atomes apportent un total de N électrons, chacun de charge -q, repérés par un indice i. On a formula_33 si la molécule est électriquement neutre. Le calcul mené ci-dessous se place dans ce cas ; par exemple formula_31 est aussi bien le nombre de protons du noyau de A que son nombre d'électrons.
Le hamiltonien doit comporter l'énergie cinétique des noyaux formula_35 et formula_36, l'énergie cinétique des électrons formula_37, l'énergie potentielle d'interaction électrostatique des noyaux entre eux formula_38, des électrons entre eux formula_39, des électrons et des noyaux formula_40. On a donc 

formula_41

où, avec formula_42


On se place maintenant dans le référentiel du centre de masse G des noyaux, et on prend G pour origine des positions. Remarquons qu'il n'est pas exactement confondu avec le centre de masse de la molécule. L'énergie cinétique des noyaux dans le référentiel du laboratoire est, comme en mécanique classique, la somme de l'énergie cinétique du centre de masse dans ce référentiel et de l'énergie cinétique des noyaux dans le référentiel du centre de masse (RCM). On sait (voir un cours de mécanique classique) que dans le RCM, l'étude du mouvement des deux noyaux peut être ramené à celui du mobile fictif dont la position est donnée par formula_48, et dont la masse est formula_49

Dans le référentiel du laboratoire : formula_50

Dans le RCM : formula_51

Les autres termes du hamiltonien ne sont pas modifiés dans le RCM, en se rappelant toutefois que les positions sont maintenant repérées par rapport au centre de masse de A et B.

L'équation de Schrödinger dont la fonction d'onde totale formula_52 de la molécule est solution s'écrit
formula_53

où, d'après BO, les mouvements électronique et nucléaire sont découplés, et au sein du mouvement nucléaire les mouvements de rotation et de vibration le sont également : 

formula_54

et

formula_55


Dans le hamiltonien, il ne reste donc plus que des termes électroniques : c'est un hamiltonien dont la solution est la fonction d'onde électronique formula_57 décrivant le système d'électrons de la molécule :

formula_58

où l'on a rassemblé les termes d'énergie potentielle énumérés ci-dessus dans le terme formula_59. Remarquons que l'on y a inclus (par convention) le terme de répulsion coulombienne des noyaux entre eux formula_38. Ce n'est pas un terme d'énergie électronique, mais il est traité comme une constante puisque ne dépendant que de la distance entre noyaux qui est elle-même traitée comme un paramètre dans l'approximation de Born-Oppenheimer.

On a donc formula_61

formula_64

expression que l'on réinjecte dans l'équation de Schrödinger initiale, avant l'hypothèse BO :

formula_65

donc par action de formula_66 :

formula_67

expression que l'on projette sur une fonction formula_68 donnée de la base adiabatique :
formula_69

puisque la base adiabatique est orthonormée.


On connaît l'action de formula_70 dans le RCM, donc :

formula_71

i.e.

formula_72

L'approximation adiabatique consiste à dire que les variations de la fonction d'onde électronique formula_57 lors d'une petite variation de formula_74 sont négligeables devant celles du coefficient formula_63. Ainsi, les deux premiers termes de la somme ci-dessus sont négligés :

formula_76


formula_78

et puisque la base adiabatique est orthonormée : formula_79, donc :

qu'on peut réécrire comme suit :

qui apparaît comme l'équation de Schrödinger dont formula_82 est solution, dans le cadre de l'hypothèse adiabatique (dans laquelle formula_68 se comporte comme un scalaire multipliant formula_84, sous l'action du laplacien. On peut donc remplacer formula_84 par formula_82).

Ainsi, l'approximation adiabatique a permis de ramener un état de la molécule, initialement décrit comme une superposition de fonctions de la base adiabatique, à une seule de ces fonctions.

Nous amorçons ici l'étude de la rotation et de la vibration de la molécule diatomique. Reprenons la dernière équation ci-dessus. Nous pouvons l'interpréter comme étant l'équation de Schrödinger nucléaire, où le terme d'énergie cinétique est bien celui du mobile fictif correspondant au mouvement relatif des noyaux (c'est l'énergie cinétique totale des noyaux dans le référentiel du centre de masse), et où la valeur propre formula_87 du hamiltonien électronique joue le rôle d'une énergie potentielle dans ce hamiltonien nucléaire.


La fonction formula_90 décrit la rotation de la molécule. La fonction formula_91 décrit la vibration de la molécule. En reportant l'expression de formula_84 dans l'équation de Schrödinger et en écrivant l'action du laplacien (décomposé en partie radiale et partie angulaire), on obtient l'équation radiale dont formula_84 est solution et qui fournit les niveaux d'énergie de vibration, et l'équation angulaire dont formula_90 est solution et qui fournit les niveaux d'énergie de rotation. Les résultats obtenus pour la rotation et la vibration reposent sur le choix du potentiel électronique formula_87.

L'énergie totale de la molécule dans le référentiel du centre de masse apparaît "in fine" comme la somme de l'énergie électronique, de l'énergie de rotation de « l'haltère » formée par les noyaux ("rotateur rigide") et de l'énergie de vibration des noyaux (l'image correspondante est celle de « masses ponctuelles reliées par un ressort ». Attention, en physique nucléaire, on parle aussi d'énergie de rotation et de vibration des noyaux, dans le cadre d'un modèle dit "collectif" ; le noyau y est considéré comme un objet non ponctuel). On a donc découplage de la rotation des noyaux, de la vibration des noyaux, et du mouvement des électrons. "Ce résultat simple et important est la conséquence du l'approximation de Born-Oppenheimer." Ce modèle élémentaire est amélioré en considérant par exemple le couplage de la rotation et de la vibration comme une perturbation au mouvement idéal découplé (distorsion centrifuge).





</doc>
<doc id="19217" url="https://fr.wikipedia.org/wiki?curid=19217" title="Ambre (jeu de rôle)">
Ambre (jeu de rôle)

Ambre est un jeu de rôle édité en France par les Jeux Descartes à partir de 1994. Il a été conçu et écrit par Erick Wujcik, d'après le "Cycle des Princes d'Ambre" de Roger Zelazny. En 1970, Zelazny publie le premier tome du "Cycle". À sa mort, en 1995, la série comptait une dizaine de romans et un jeu de rôles. Bien que le cycle ne fut pas achevé, il fit l'effet d'une petite révolution dans le monde de la fantasy à cause de son système de jeu sans dés.

L'univers d'Ambre tel que développé dans les romans est très présent dans le jeu de rôle, les joueurs incarnant les fils des Princes d'Ambre ou de la Cour du Chaos — les héros des livres. Le monde dans lequel les personnages évoluent est en fait un multi-mondes, vaste "plexus" organisé entre deux pôles de réalité asymétriques : les Cours du Chaos et la cité d'Ambre. La présentation du jeu de rôles d'Ambre fait une grande place aux intrigues de pouvoirs, qui sont la trame principale des livres et qui offrent un support illimité au roleplay. Cela nécessite un effort d'appropriation de l'univers par le maître de jeu et les joueurs ; la liberté de jeu est au prix d'un investissement plus important que dans la plupart des autres jeux de rôles. Une autre particularité de ce jeu est l'absence de dés pour trancher les actions non triviales. Le hasard et le destin sont tributaires des conventions établies par Zelazny et de la bonne entente entre joueurs et maître de jeu. À ce titre, Ambre est souvent déconseillé aux joueurs débutants — mais sa souplesse peut à l'inverse être séduisante pour débuter.

Les inspirations des romans de Zelazny étaient le plus souvent médiévales-fantastiques : ces mêmes inspirations se retrouvent dans le livre de règles, qui sont plus des conventions et des éléments de jeu que des indications intangibles. Elles reprennent la liberté potentielle de naviguer à travers les univers infinis d'Ambre (les « ombres »), ce qui offre des perspectives tout à fait singulières pour un jeu de rôles : là où la plupart des jeux proposent un univers et le développent avec minutie, Ambre propose la seule liberté d'approfondir tout univers souhaité par ses joueurs.

Dans le jeu, le joueur dirige un prince d'Ambre de la troisième génération. En tant que descendant du roi Oberon, les joueurs disposent de certaines capacités : une force et une intelligence surhumaine, l'immortalité temporelle, et surtout la possibilité de voyager dans les mondes (« marcher à travers Ombre »).

Pour se faire une idée du potentiel des personnages, on peut prendre quelques exemples : un prince d'Ambre ne rate jamais sa cible quand il tire une flèche sans handicap, il est en mesure de battre tous les grands joueurs d'échecs tout comme de se battre à l'épée plusieurs jours d'affilée, ou encore de contrôler les pensées d'un humain, etc. Les personnages incarnés par les joueurs commencent donc au plus haut de l'échelle sociale, avec pour seuls ennemis réels les Ambriens des générations antérieures et certains habitants des Cours du Chaos. Les complots et traîtrises sont légion à la cour d'Ambre : ne jamais faire confiance à un autre Prince d'Ambre est la première chose qu'apprend un jeune Prince. Cet élément caractéristique des romans rend parfois épiques les parties, car les joueurs rassemblés autour d'une même table peuvent avoir des intérêts fortement divergents.

Une première singularité de ce jeu est qu'il permet au joueur de se déplacer dans n'importe quelle "ombre" imaginable, et ce à n'importe quelle époque, puisque les Ambriens et les habitants des « Cours du Chaos » ont un contrôle sur les espaces-temps. Tous les mondes possibles existent dans Ambre, et leurs aspects dépendent de l'infinité de « dosages » différents entre Ordre et Chaos. Les possibilités de ce jeu sont donc potentiellement infinies. On pourra y jouer des campagnes très différentes, selon l'orientation choisie par le maître de jeu, les envies ponctuelles des joueurs, etc.

Ambre se pratique sans aucun système de dés ou de jets aléatoires — contrairement à la plupart des autres jeux de rôles. Les joueurs doivent se référer aux décisions du maître de jeu, qui interprète leurs actions : le "rôle play", ou art d'incarner son personnage, est le principal moteur d'Ambre. La magie est très diversifiée et le système sans dés qui la régit s'avère assez fonctionnel. Un joueur débutant appréciant le "role play" trouvera rapidement ses marques tandis qu'un joueur expérimenté pourra mettre à profit la richesse du jeu pour développer les scénarios de manière plus libre que dans un jeu de rôles à règles "figées".

En ce qui concerne la méthode de tirage des personnages, qui s'affranchit elle aussi de l'utilisation du dé, Erick Wujcik a conçu un système d'enchère où chaque joueur dispose d'une centaine de points, qu'il devra répartir entre différents attributs ("Force", "Psyché", "Combat" et "Endurance"). Les points servent également à acheter des armes, des pouvoirs, des protecteurs ou du "Karma".

Généralement, les parties d'Ambre s'appuient surtout sur des conflits inter-ombres ou bien des conflits entre Ambre et les "Cours du Chaos", car les personnages des joueurs ont des capacités physiques et magiques bien plus importantes que celles des habitants des "Ombres" (dont fait partie la Terre). Au fil des parties, les caractéristiques chiffrées des personnages varient de simples indicateurs à valeurs arbitres : si la plupart du temps, Ambre fait la part belle aux dialogues, il se peut que naisse une situation de conflit où un repère stable devient nécessaire. L'expérience des joueurs augmentant, la confiance entre ceux-là et le maître de jeu devenant complète, ces situations deviennent plus rares et les parties peuvent se dérouler entièrement sans feuille de personnages (ou pour simple aide-mémoire).


</doc>
<doc id="19221" url="https://fr.wikipedia.org/wiki?curid=19221" title="Analyse dimensionnelle">
Analyse dimensionnelle

L'analyse dimensionnelle est une méthode pratique permettant de vérifier l'homogénéité d'une formule physique à travers ses équations aux dimensions, c'est-à-dire la décomposition des grandeurs physiques qu'elle met en jeu en un produit de grandeurs de base : longueur, durée, masse, intensité électrique, irréductibles les unes aux autres.

L'analyse dimensionnelle repose sur le fait qu'on ne peut comparer ou ajouter que des grandeurs ayant la même dimension ; on peut ajouter une longueur à une autre, mais on ne peut pas dire qu'elle est supérieure, ou inférieure, à une masse. Intuitivement, il est clair qu'une loi physique ne saurait changer, hormis dans la valeur numérique de ses constantes, au simple motif qu'on l'exprime dans d'autres unités. Le théorème de Vaschy-Buckingham le démontre mathématiquement.

En physique fondamentale, l'analyse dimensionnelle permet de déterminer "a priori" la forme d'une équation à partir d'hypothèses sur les grandeurs qui gouvernent l'état d'un système physique, avant qu'une théorie plus complète ne vienne valider ces hypothèses. En science appliquée, elle est à la base de la modélisation par maquettes et de l'étude des effets d'échelle.

L'analyse dimensionnelle peut trouver des applications dans de nombreux problèmes, en particulier pour déterminer des nombres sans dimension intervenant dans les phénomènes physiques, et qui permettent de modéliser le phénomène par des maquettes, ou encore pour déterminer "a priori" des effet d'échelle. On la retrouve par exemple dans les domaines suivants :

L'analyse dimensionnelle de ces phénomènes fournit d'utiles règles de proportionnalité. Elle permet de spécifier l'étalonnage des modèles expérimentaux, et de guider les études de variation. Dans de nombreux cas, elle permet d'identifier des dépendances fonctionnelles. Dans tous les cas, elle contribue à une meilleure compréhension du problème.

L'analyse dimensionnelle est à la base des systèmes d'unités naturelles.

Dans une formule physique, les variables présentes ne sont pas « que » des nombres, mais représentent des grandeurs physiques.

Une grandeur physique est un paramètre mesurable qui sert à définir un état, un objet. Par exemple, la longueur, la température, l'énergie, la vitesse, la pression, une force comme le poids), l'inertie (masse), la quantité de matière (nombre de moles)… sont des grandeurs physiques. Une mesure physique exprime la valeur d'une grandeur physique par son rapport avec une grandeur constante de même espèce prise comme unité de mesure de référence (étalon ou unité).

On exprime alors la grandeur par un nombre rationnel multipliant l'unité de mesure. De ce fait, les opérations entre grandeurs physiques ne portent pas que sur les nombres, mais également sur les unités. Ces unités présentes dans les formules physiques contraignent la forme que peuvent prendre ces formules, parce que certaines opérations possibles sur de simples nombres deviennent impossibles quand ces nombres sont associés à des unités. Ces contraintes sont celles qui font qu'une formule physique est qualifiée d'« homogène » :

Un tel contrôle est automatisable. Dès 1976, Michel Sintzoff remarquait qu'on peut renforcer la fiabilité des programmes de calculs en physique, en déclarant les variables physiques en tant que telles, et en codant leur dimension par la suite des exposants relatifs aux dimensions de base prises dans un ordre fixe. Il est alors possible de vérifier à la compilation leur homogénéité dimensionnelle par "évaluation symbolique". Pour cela, on remarque notamment que :

Si l'addition d'unités n'a pas de sens, celle de grandeurs physiques de même nature reste possible, à condition toutefois de les ramener à une unité commune.
Ou, de façon équivalente, on peut transformer les minutes en heures avant de pouvoir les additionner :

Dans le premier cas, on aura simplifié les heures au numérateur contre des heures au dénominateur pour obtenir plus que des minutes au numérateur, et dans le second on aura simplifié des minutes au numérateur contre des minutes au dénominateur, pour ne garder plus que des heures au numérateur. 

Une mesure physique étant un nombre "associé à une unité", on a bien d'un côté deux nombres associés à des unités (différentes), et de l'autre le résultat, un nombre associé à une unité.


Il faut se rappeler ici ce qu'est exactement une « unité de mesure » : une mesure physique vise à exprimer la valeur d'une grandeur physique par son "rapport avec une grandeur constante" de même espèce. Si donc l'« heure » est une unité de mesure du temps, c'est parce que l'on peut comparer des grandeurs temporelles avec la grandeur particulière qu'est « une heure » : toute mesure physique ne fait qu'évaluer un rapport entre deux grandeurs de même nature.

Ces unités de mesure sont elles-mêmes des grandeurs physiques mesurables, donc "un nombre associé à une unité", et prendre « une heure » ou « une minute » comme référence est fondamentalement un choix arbitraire. Le caractère arbitraire de ce choix peut être frustrant, parce qu'il ne permet pas de capturer ce qu'est la « nature » d'une unité : bien qu'une mesure soit un nombre associé à une unité (laquelle donne donc à cette mesure sa nature), on ne peut en réalité que faire des rapports, et accéder à des nombres sans dimension.

L'idée d'un système d'unités naturelles répond à cette idée d'éliminer la part d'arbitraire dans la mesure : s'il existe une unité naturelle « T » qui puisse servir de référence universelle pour mesurer le temps, alors la minute et l'heure peuvent se décrire comme respectivement "n.T" et soixante fois "n.T". Si l'unité est naturelle, on peut alors considérer que « T » concentre l'essence de cette grandeur et en est la nature même, ce qui fait qu'un nombre change sa nature et devient une mesure physique : l'unité arbitraire que l'on utilise au quotidien est ainsi dissociée en une grandeur physique essentielle, qui lui donne sa « nature », et un facteur de conversion propre à cette unité, qui en supporte tout l'arbitraire.

Dans cette approche, une mesure d'une grandeur physique implique alors conceptuellement trois entités : une unité naturelle, qui donne la « nature » de la mesure, un facteur de conversion qui découle de la grandeur utilisée comme unité pratique, et un nombre mesuré représentant le rapport entre la grandeur mesurée et l'unité pratique. Que l'unité naturelle ne soit pas clairement définie (la seule unité clairement naturelle est la vitesse de la lumière) n'a pas d'importance pratique. Un facteur de conversion, s'il faut le calculer, prend toujours la forme d'un rapport entre deux mesures de même nature, et ne dépend donc pas de la valeur exacte de l'unité naturelle.

Indépendamment de ce que doit être la valeur d'une unité naturelle, on peut considérer dans cette optique qu'une expression physique traduit des opérations sur des objets complexes, associant un nombre, une unité, et un facteur de conversion.

L’équation aux dimensions d'une formule physique est une « équation de grandeurs », qui a la même forme que la formule physique initiale, mais où ne sont pris en compte ni les nombres, ni les facteurs de conversion, ni les constantes numériques sans dimension : uniquement les grandeurs. On y représente les phénomènes mesurés par un symbole ; par exemple, un temps y est représenté par la lettre « T », une longueur est représentée par la lettre « L ». C'est cette formule qui permet de déterminer la dimension dans laquelle doit être exprimé le résultat d'une formule physique, indépendamment des nombres résultant des mesures.

Les équations physiques relient entre elles des grandeurs physiques, donc des nombres et des unités, et potentiellement des facteurs de conversion dépendant du choix de ces unités.
Déterminer ce facteur de conversion est dans le cas général une opération complexe. En prenant comme référence un système d'unités moderne et rationnel, comme le système international d'unités, le calcul est un peu simplifié :
Et dans ce cas, en transformant les unités autochtones en USI :
Mais comme dans le système international d'unités, qui est un système rationnel, on a bien
On en déduit :
Dans ces unités d'ancien régime, donc, la vitesse (en nœuds) est égale à la distance (en lieues) divisée par le temps (en heures), multiplié par un coefficient de conversion 2,317336792 qui reflète le choix arbitraire des unités.
Inversement, connaissant cette formule "V""D"/"T", et étant données des unités de temps et de longueur (la seconde et le mètre, dans le système métrique), on peut "choisir" l'unité de vitesse de manière à ce qu'il n'y ait pas de facteur de conversion : cette unité « dérivée » est alors le mètre par seconde dans le système métrique.

D'une manière générale, en passant d'une loi physique à une autre, il est possible d'exprimer de proche en proche la dimension de toutes les grandeurs physiques en fonction de sept dimensions de base.

Le système international d'unités fait le choix suivant, et recommande les notations correspondante, largement répandues :

Le choix de ces sept grandeurs est une construction historique, les grandeurs ont été choisies depuis le en fonction des besoins et des étalons que l'on pouvait fabriquer de manière simple et précise. Elles sont "a priori" les plus fondamentales, et « les trois unités fondamentales » étant les seules directement accessible à la mesure pour la physique du , il aurait été difficilement imaginable de faire le choix d'autre grandeurs de base.

On peut cependant choisir d'autres grandeurs de référence, par exemple définir la vitesse comme grandeur de base, et définir l'étalon-longueur en fonction de l'étalon-vitesse et de l'étalon-temps : c'est ce qui est d'ailleurs fait à présent implicitement dans le système métrique, l'étalon-vitesse étant la vitesse de la lumière dans le vide. De même, une alternative à l'intensité électrique pourrait être de retenir la charge électrique comme unité de base. Ces choix alternatifs conduisent alors à des alternatives en matière de système d'unités.

Le choix des grandeurs de base par rapport aux grandeurs dérivées est relativement arbitraire. Dans la plupart des cas, en mécanique, les grandeurs effectivement utilisées se limitent aux « trois unités fondamentales » de Maxwell, le sous-système formula_7. Mais il serait possible de fonder un système sur la force au lieu de la masse (formula_8). De fait, exprimer des unités en ou en revient en quelque sorte à considérer que le newton pourrait être une grandeur de base pour définir ces grandeurs dérivées. On pourrait de même remplacer le temps par une vitesse ou une fréquence, ou s'appuyer sur l'énergie, ou opter pour toute autre combinaison de trois grandeurs mécaniques, du moment que ces trois grandeurs sont indépendantes. Ce choix est uniquement une question de commodité. L'analyse dimensionnelle ne dépend pas des grandeurs retenues comme base.

Comme indiqué ci-dessus, dans le cas général, une loi physique comporte dans le cas général (pour des systèmes d'unité non rationnels) un terme constant reflétant la conversion des unités entre grandeurs d'entrée et grandeur de sortie. Inversement, dans un système rationnel, l'unité de la grandeur de sortie est choisie de telle manière, que son facteur de conversion soit égal à l'unité, c'est-à-dire disparaisse de la formule décrivant la loi physique : ce facteur n'a en effet aucune signification physique.

De proche en proche, de loi physique en loi physique, ce principe permet de déterminer toutes sortes de « grandeurs dérivées », d'en connaître la dimension, et si possible d'en fixer une unité cohérente avec les unités précédemment retenues, pour laquelle le « facteur de conversion » sera égal à un.

Une grandeur dérivée est ainsi une grandeur dont la dimension est liée à au moins une des sept grandeurs de base. Une loi physique exprime le lien entre une grandeur dérivée et les grandeurs de base (ou d'autres grandeurs dérivées). Son énoncé impose une certaine équation aux dimensions.

La dimension d'une grandeur dérivée est dite « simple » lorsqu'elle n'est liée qu'à une des sept grandeurs de base. Par exemple, la dimension de la superficie est simple : elle n'est liée qu'à la longueur et correspond au carré d'une longueur. La dimension d'une grandeur dérivée est dite « composée » lorsqu'elle est liée à au moins deux des sept grandeurs de base. Par exemple, la vitesse est le rapport d'une longueur par une durée.

L'équation aux dimensions est l'équation qui relie la dimension d'une grandeur dérivée à celles des sept grandeurs de base. Dans une équation aux dimensions, la dimension de la grandeur dérivée formula_9 est couramment notée formula_10.

La forme générale d'une équation aux dimensions est :
où :

Ces derniers sont appelés « exposants dimensionnels ». Un tel exposant dimensionnel est un nombre entier relatif. Il peut être (strictement) positif, nul ou (strictement) négatif. Une grandeur sans dimension, ou grandeur de dimension 1, est une grandeur pour lesquels tous les exposants dimensionnels sont nuls.

Ainsi, la "dimension d'une grandeur" est la manière dont elle se compose à partir des sept dimensions de base.
La composition peut devenir plus complexe.

Le radian et son homologue sphérique le stéradian occupent une place à part dans les unités, ni tout à fait unité de base, ni vraiment homologue à une unité dérivée. Pendant longtemps, il a été qualifié d'« unité supplémentaire » ; la générale du Bureau international des poids et mesures a retiré cette notion. Le radian est à présent une .

Le statut particulier de cette unité vient de la dimension réputée « sans dimension » de l'angle plan. Un angle est en effet mesuré par le rapport entre la longueur de l'arc (AB) qu'il découpe sur un cercle de rayon r et le rayon r de ce cercle. Ces deux mesures étant faites dans une unité de longueur, on en conclut que la dimension du radian est nulle, formula_16 (et de même pour le stéradian, rapport de la surface interceptée au carré du rayon, formula_17). Paradoxalement, donc, la quatrième grandeur immédiatement mesurable dans l'expérience quotidienne ne partage pas le statut privilégié des « trois unités fondamentales » : son unité est facultative, et elle n'est même pas considéré comme une grandeur effective.

La « grandeur angulaire » est néanmoins importante pour clarifier la notation de quelques unités, ce qui justifie son emploi facultatif dans le système international d'unités. C'est ainsi que la vitesse angulaire "ω" se note en , et se distingue ainsi des hertz et des becquerels, "a priori" de même dimension formula_18. De même, l'accélération angulaire "α" se note habituellement en .

Bien que ce ne soit pas la pratique habituelle, il est également correct de noter la composante angulaire dans les grandeurs décrivant la rotation, qui peut être simplement identifiée de proche en proche à travers les équations aux dimensions :

Mais fondamentalement, pour l'analyse dimensionnelle, les angles ne peuvent pas être considérés comme une variable du problème, parce que leur définition classique ne leur donne pas de dimension propre. Par exemple, prenons un projectile dont on cherche une expression de la portée "P" en fonction de l'angle et la vitesse du tir, et de l'attraction de la pesanteur "g". Sous cette forme, le problème a quatre variables dépendant de trois grandeurs et devrait donc être bien posé pour résoudre "P" en fonction des trois autres, à une constante près. Mais l'angle étant considéré comme sans dimension, la manière dont il intervient dans un monôme ne peut être qu'arbitraire : cette « variable » s'avère inutilisable dans une approche classique, où elle ne se distingue pas d'une constante arbitraire. Ce problème particulier sera traité ci-dessous par projection, en distinguant les composants v et v de la vitesse initiale suivant deux directions, mais cette solution par projection n'est pas un traitement général, et ne résout pas réellement le problème spécifique des angles.

En thermodynamique ou en mécanique des fluides, il est parfois intéressant de distinguer entre la masse en tant que mesure de l'inertie (masse inertielle), et la masse en tant que mesure de la quantité de matière (masse grave), suivant une suggestion de Huntley
Il existe en effet deux masses pour chaque corps :
La masse grave est à la loi de la gravitation de Newton ce qu'est la charge électrique à la loi de Coulomb : elle est en quelque sorte une "charge gravitationnelle". Bien que masse grave et masse inertielle soient conceptuellement distinctes, on constate en pratique qu'elles sont toujours proportionnelles, ce qui justifie que l'on puisse utiliser la même unité pour les deux (c'est le principe d'équivalence). Cependant, si utiliser la même unité de masse est une possibilité, ce n'est pas une nécessité, et il reste possible de distinguer les deux dans une équation aux dimensions : dans son analyse, Huntley montre qu'une équation physique mettant en jeu les deux types de masse doit être homogène pour chaque type de masse.

Huntley propose une autre extension. Elle consiste à considérer que les trois composantes d'un vecteur doivent être considérés comme relevant de grandeurs distinctes. Dans ce cas, au lieu de n'avoir qu'une longueur L indifférenciée, on aura une longueur Lx dans la direction des x, et ainsi de suite.

Pour illustrer cette idée, on peut chercher à calculer à quelle distance sera le point de chute d'un boulet de canon tiré à partir d'un plan horizontal, avec une vélocité verticale formula_22 et une vélocité horizontale formula_23.

Si l'on ne tient pas compte des dimensions de l'espace, les seules quantités intéressantes seront formula_23 et formula_25, toutes deux en LT, la portée , de dimension L, et l'accélération de la pesanteur, de dimension LT. Ces quatre quantités ne dépendent que de deux grandeurs indépendantes, et il est donc possible de définir deux grandeurs sans dimension.

L'équation recherchée pour la portée est de la forme :
Ou encore, sous forme d'une équation aux dimension :
Ce dont on peut déduire que formula_28 et formula_29, dont on peut déduire que formula_30, mais deux exposants restent indéterminés. C'est normal, puisqu'il y a deux grandeurs indépendantes et quatre quantités pour une seule équation.

Si toutefois on distingue entre les différentes directions de l'espace, alors formula_23 a la dimension LT, formula_25 est en LT, est en L et est en LT. L'équation aux dimensions devient alors :
Ayant à présent trois grandeurs indépendantes et quatre quantités pour deux équations, il est possible de résoudre le système pour trouver formula_34, formula_35 et formula_30 ; et donc :
Si on note θ l'angle de tir, par rapport à la vitesse initiale V on aura formula_38 et formula_39, donc :

On voit immédiatement sur cet exemple le gain qu'apporte l'introduction de longueurs directionnellement distinctes. La justification profonde d'une telle approche est que chaque composant d'une équation dimensionnellement consistante doit lui-même être dimensionnellement consistant, que l'équation soit scalaire, vectorielle ou tensorielle. Par conséquent, en projetant le problème sur l'un ou l'autre de ses axes de symétrie, on peut (parfois) identifier des équations indépendantes, et chaque équation supplémentaire permettra de résoudre une nouvelle variable.

Cette approche consiste à ramener un problème situé dans l'espace de dimension trois à plusieurs problèmes dans des espaces linéaires de dimension un. Bien que souvent utile, donc, cette extension de la méthode proposée par Huntley présente encore quelques insuffisances :

Plutôt que d'introduire seulement trois dimensions de longueur "Lx" d'orientation distinctes, comme le propose Huntley, Donald Siano a proposé pour représenter le caractère vectoriel de certaines grandeurs de retenir comme grandeur à part entière des « grandeurs d'orientation » formula_41, formula_42 et formula_43 dans l'équation aux dimension, le symbole formula_44 représentant de son côté une grandeur scalaire sans orientation. Avec cette approche, la dimension projetée formula_45 que propose Huntley devient une grandeur dérivée composée formula_46, où formula_47 traduit le caractère de « longueur », et formula_41 traduit le caractère d'« orientation » dans une direction particulière, donc le caractère essentiellement vectoriel de cette grandeur.

Dans les formules dimensionnelles, les grandeurs scalaires ont alors une dimension de formula_44 quelle que soit la direction de l'espace où elles sont projetés, mais les grandeurs vectorielles reçoivent une dimension d'orientation non nulle — dont le choix en "x, y, z" est relativement arbitraire tant que ces choix se simplifient dans l'équation aux dimension. La direction peut être par exemple « celle du problème » formula_41 lorsqu'une seule direction est impliquée, mais devient « l'autre direction du plan » formula_42 lorsqu'une deuxième intervient, et « la direction orthogonale aux deux autres » formula_43, en tant que de besoin.

Cette convention conduit notamment à poser que l'écart angulaire traduit une rotation dans un espace à trois dimensions :
Ce même résultat peut s'obtenir directement en remarquant qu'en coordonnées polaires (r, α), une variation élémentaire dα entraîne un déplacement orthogonal dx=r.dα : dx étant d'orientation formula_42 par rapport à la distance r posée d'orientation formula_41, l'homogénéité de la formule impose que dα est d'orientation formula_43, ce qui est donc la dimension du radian.
On peut montrer par ailleurs (à travers le développement en série de Taylor) que sin(θ), comme toute fonction impaire, a la même grandeur d'orientation que son argument θ ; et que cos(x), comme toute fonction paire, a toujours une grandeur d'orientation scalaire — les fonctions ni paires ni impaires ne pouvant prendre que des arguments scalaires.

En exemple d'application, reprenons le problème de la portée d'un projectile en tenant compte de la grandeur d'orientation. Par rapport à la direction formula_41 du point d'impact, la gravité est d'orientation formula_43, et l'angle de tir θ étant dans un plan xz sera de dimension perpendiculaire, c'est-à-dire formula_42. La portée est alors de la forme :
L'homogénéité dimensionnelle impose alors correctement que et ; et par rapport à la grandeur d'orientation, doit alors être un entier impair (donc, peut être pris égal à l'unité). Une analyse complémentaire montre que la fonction recherchée en θ, nécessairement impaire pour des raisons d'homogénéité, est périodique de période 2π (donc de la forme formula_62) et s'annule pour θ=0 et θ=π/2 : donc n=2 et la fonction recherchée est formula_63. On a donc :

La puissance du pouvoir prédictif de l'analyse dimensionnelle en regard de sa simplicité a conduit Wheeler à proposer le principe général suivant :

Cet énoncé, qui peut sembler "a priori" paradoxal, signifie concrètement : ne pas se lancer dans un calcul compliqué sans avoir trouvé au préalable la forme qualitative du résultat, avec l'analyse dimensionnelle.

L'analyse dimensionnelle permet en effet de trouver la forme de la solution de certains problèmes, sans avoir à résoudre d'équations, grâce au théorème de Buckingham (parfois appelé « théorème Pi »). Ce type de calcul n'est valable que si un petit nombre de paramètres contrôlent la solution d'un problème (2 ou 3).

L'analyse dimensionnelle ne permet de trouver l'équation physique qui gouverne le phénomène, qu'à une constante numérique "k" près, sans dimension, et que cette méthode ne peut donc pas déterminer. Il faut faire un calcul explicite complet pour la trouver (ou une mesure expérimentale pour la déterminer). L'expérience montre cependant que, dans un système d'unités adapté au problème étudié, cette constante "k" est toujours de l'ordre de grandeur de 1 (au sens où π ~ "e" ~ 1), d'où la pertinence de l'analyse dimensionnelle pour prévoir la forme du résultat d'un calcul, ainsi que son ordre de grandeur.

Le fait de construire des équations homogènes ne suffit cependant pas à identifier des lois physiques pertinentes. La célèbre équation formula_65 est parfaitement homogène et invariante par changement d'unités ; mais cette homogénéité ne suffisait pas pour autant à la pressentir.

Deux exemples célèbres sont le calcul de la puissance de la première bombe atomique et le modèle de Kolmogorov de la turbulence homogène isotrope, qui a influencé grandement toute la mécanique des fluides.

Galilée avait initialement supposé (à tort) que dans la mesure où la force de gravité exercée sur un corps (son poids) dépend de sa masse, la loi gouvernant la chute des corps, c'est-à-dire la hauteur "h" fonction du temps "t" et de la gravité "g", pouvait également dépendre de la masse "m" de ce corps. On aurait dans ce cas :
La hauteur "h" a évidemment pour dimension formula_67, la masse "m" est en formula_68, et "t" a pour dimension formula_69 ; et l'analyse dimensionnelle donne pour "g" la dimension formula_70. La seule combinaison donnant une grandeur sans dimension est alors :
Une fonction de la masse ne peut pas être rendue sans dimension au moyen des variables "g", "t" et "h", ce qui montre que l'idée de faire dépendre cette loi de la masse est physiquement incorrecte. En réalité, la masse n'intervient dans la description de la trajectoire que lorsque la résistance de l'air est prise en compte, parce que la viscosité de l'air importe alors la dimension d'une masse.

Galilée ne disposait pas du calcul différentiel, et supposait que la vitesse "v" (dont la dimension est formula_72) était proportionnelle à la hauteur de chute "h", c'est-à-dire que formula_73. S'il avait pu avoir recours à l'analyse dimensionnelle, il aurait pu voir que la seule grandeur sans dimension qui peut être obtenue à partir de "v", "h" et "g" est :
Il ne peut donc pas y avoir de dépendance linéaire entre "h" et "v", ce qui peut donc être déterminé sans calcul différentiel.

On cherche à déterminer la période "t" d'oscillation du système masse-ressort en fonction de la raideur "k" du ressort et du poids "p" qui y est suspendu. Ces trois grandeurs physiques ont respectivement pour dimension :

On voit que sous cette forme le problème est insoluble : le poids est la seule grandeur physique ayant une composante en longueur, donc ne peut pas intervenir dans un facteur sans dimension ; et la raideur est alors la seule grandeur ayant une composante en masse, donc ne peut pas intervenir non plus.

Décomposant le poids en produit de la masse par l'accélération de la pesanteur, on cherche alors à déterminer cette période d'oscillation "T" d'une masse "m" attachée à un ressort idéal de raideur "k" dans un champ gravitationnel "g". Ces quatre grandeurs physiques ont respectivement pour dimension :

À partir de ces quatre grandeurs il n'est possible de former qu'un seul composé sans dimension : formula_75. Aucune combinaison ne comporte de facteur "g", parce qu'ici c'est le seul qui ait une composante en longueur.

De fait, l'analyse dimensionnelle peut imposer de fortes contraintes sur la pertinence d'une grandeur physique dans la solution d'un problème, ou sur la nécessité de faire intervenir des paramètres complémentaires. Ici il y a suffisamment de variables pour donner une description correcte du problème, et la conclusion est qu'en réalité la période des oscillations d'une masse accrochée à un ressort ne dépend pas de la gravitation "g" : elle serait la même à la surface de la Terre ou sur la Lune.

Le facteur sans dimension qui a été trouvé est "a priori" une « petite constante », et l'équation peut se réécrire sous la forme équivalente :

L'analyse dimensionnelle ne peut pas déterminer seule la constante formula_77. On trouve par d'autres moyens que formula_78

Considérons un point matériel de masse "m" et de charge électrique "q" soumis à un champ magnétique uniforme formula_79. Le point matériel animé d'une vitesse formula_80 est soumis à la force de Lorentz :
Lorsque formula_82, le point matériel décrit un cercle dans le plan perpendiculaire au champ magnétique à vitesse angulaire "ω" constante. Cette vitesse angulaire doit dépendre des paramètres "m", "q" et formula_79 du problème.

On peut chercher s'il existe une relation simple, comme un produit, entre ces paramètres :
où "k", α, β et γ sont des constantes inconnues, et des nombres sans dimension.

Les équations aux dimensions permettent de déterminer ces nombres. En effet, on a :

d'où l'équation aux dimensions d'un champ magnétique :

On en déduit l'équation aux dimensions de "ω" :

Par ailleurs, la vitesse angulaire ω est le rapport d'un angle divisé par un temps "T" (la période de rotation) :

Un angle étant sans dimensions, il vient :

On en déduit que : γ = 1 ; α+γ = 0 → α = -1 ; et β-γ = 0 → β = 1. D'où la forme de "ω" :
formula_90

On appelle « pulsation cyclotron » la grandeur :
formula_91

Pour cet exemple précis, la résolution de l'équation de la dynamique de Newton montre que "k" = 1 exactement.

Par ailleurs, "B" est la seule grandeur physique de ce monôme à avoir un caractère d'orientation (c'est un pseudovecteur). La relation peut donc s'écrire sous forme vectorielle :
La légende voudrait que l'analyse dimensionnelle ait permis à Geoffrey Ingram Taylor d'estimer en 1950 l'énergie qu'avait dégagée l'explosion d'une bombe atomique, alors que cette information était classée "top secret". Il lui a suffi pour cela d'observer sur un film d'explosion que les militaires américains avaient imprudemment rendu public la dilatation du champignon atomique. L'énergie s'en déduisait suivant la loi expérimentale de proportionnalité.

La légende est la suivante : En 1949, après la déclassification des images d'une explosion nucléaire au Nouveau Mexique, le physicien Taylor analyse les photographies à la lumière de ses travaux passés et en déduit la puissance de la bombe.
Taylor suppose "a priori" que le processus d'expansion de la sphère de gaz dépend au minimum des paramètres suivants :

L'analyse dimensionnelle le conduit alors pour le rayon de la sphère de gaz à l'instant "t" à l'équation :

où "k" est une constante sans dimensions. Taylor retrouve donc bien la loi expérimentale de dilatation du champignon
ce qui semble valider son choix de paramètres. Il détermine alors "r" et "t" à partir du film, et, "k" étant supposée de l'ordre de l'unité et "ρ" étant connue, il obtient finalement :

En réalité, G. I. Taylor n'a pas utilisé ce raisonnement simpliste. Dans sa première publication, longue de 15 pages, G. I. Taylor utilise l'analyse dimensionnelle pour simplifier les équations différentielles qui décrivent l'écoulement. Après de longs et difficiles calculs, il obtient finalement la formule très simple suivante :

formula_96

où intervient la grandeur numérique formula_97 qui dépend de la constante formula_98 qui vaut 1,4 à température ambiante, mais qui diminue à haute température. Taylor s'étonne ainsi dans son second article du très bon accord entre la formule et les valeurs mesurées sur les photos et précise qu'il s'attendait à un moins bon accord.

Ce n'est donc qu"'a posteriori", grâce aux lourds calculs de Taylor et à la constatation expérimentale que la température n'intervient pas, que l'on peut retrouver très élégamment l'expression du rayon du champignon nucléaire en fonction du temps et de l'énergie de la bombe.

Geoffrey Ingram Taylor et John von Neumann ont publié cette solution élégante indépendamment durant la seconde guerre mondiale, ainsi que trois autres après la guerre, L. I. Sedov, R. Latter, et J. Lockwood-Taylor.

L'expression de l'énergie dans l'exemple ci-dessus (bombe nucléaire) peut être obtenue de manière plus générale sans faire référence à l'expansion d'une sphère de gaz. Puisqu'il s'agit de retrouver rapidement le monôme qui intervient dans la relation formula_99, n'importe quelle méthode convient :

Par exemple, formula_100 et formula_101 donc formula_102, d'où formula_95

Méthode généralisable: on cherche formula_104 tels que formula_105 avec formula_106 , formula_107 ,formula_108 et formula_109 (voir tableau en dessous)

formula_110

Une manière de faire l'analyse granulométrique d'un sédiment fin est de le mettre en suspension homogène, puis de mesurer la hauteur sédimentée en fonction du temps. Cette méthode suppose que l'on connaisse la vitesse de sédimentation formula_111 d'une particule en fonction de son diamètre formula_112. De toute évidence, cette vitesse de sédimentation dépend également de l'accélération de la pesanteur formula_113, de la viscosité formula_114 du liquide, et de la densité relative formula_115, différence de densité entre le sédiment et le liquide.
Comme il y a ici cinq paramètres pour seulement trois dimensions fondamentales, il n'est "a priori" possible que de déterminer une dépendance partielle entre les paramètres.

Il est cependant possible de distinguer, dans la dimension de longueur, entre les longueurs mesurées dans le sens vertical en , direction de la vitesse, de l'accélération, et de l'effet de la viscosité, et celles mesurées à l'horizontale en , direction où doit être mesuré le diamètre de la section efficace. Le volume d'une particule combine à la fois une direction verticale et deux horizontales.

Les dimensions de ces variables sont alors :
L'homogénéité de la formule impose alors:
Ce qui correspond à la loi de Stokes, pour laquelle la constante vaut 2/9.

L'origine de l'analyse dimensionnelle fait débat chez les historiens.
On cite généralement les mathématiciens Leonard Euler et Joseph Fourier et le physicien Rayleigh comme y ayant fait d'importantes contributions,
en partant de l'idée que des lois physiques comme formula_122 ne devraient pas dépendre des unités employées pour mesurer les grandeurs physiques qui apparaissent dans la formule. Cette exigence amène à la conclusion qu'une loi physique doit former une équation « homogène » entre ces différentes unités ; résultat finalement formalisé avec le théorème de Vaschy-Buckingham. Mais la première application d'une analyse dimensionnelle semble être due au mathématicien savoyard François Daviet de Foncenex (1734–1799), dans un ouvrage publié en 1761, soit 61 ans avant les travaux de Fourier. En tout cas James Clerk Maxwell établit l'approche moderne de l'analyse dimensionnelle, en posant que la masse, la longueur et le temps étaient des unités fondamentales, et en qualifiant les autres de « dérivées ».

Bien que Maxwell ait défini le temps, la longueur et la masse comme « les trois unités fondamentales », il releva cependant que la masse gravitationnelle pouvait être une grandeur dérivée du temps et de la longueur, conduisant à la dérivation formula_123, à condition de considérer que dans la loi universelle de la gravitation de Newton, la constante gravitationnelle "G" soit prise égale à l'unité. De même, en écrivant la loi de Coulomb dans une forme où la constante formula_124 est posée égale à l'unité, Maxwell détermina que la dimension de l'unité électrostatique devait être formula_125, et compte tenu de ce que par ailleurs il considérait la masse comme une grandeur dérivée formula_123, la charge électrique avait alors la même dimension qu'une masse, c'est-à-dire formula_127.

L'analyse dimensionnelle permet aussi de déduire la forme que doit avoir la relation entre les quantité physiques qui interviennent dans un phénomène que l'on cherche à comprendre et à caractériser. Rayleigh semble l'avoir utilisée dans ce sens le premier, en 1872, en cherchant à expliquer pourquoi le ciel est bleu. Rayleigh publia sa méthode en 1877, dans son livre sur "la théorie du son".

C'est dans son ouvrage "Théorie de la Chaleur" que Joseph Fourier introduit la « dimension », qu'à l'origine il assimile aux valeurs numériques que prennent les exposants des unités de base. Pour lui, par exemple, l'accélération est donc de dimension 1 par rapport à l'unité de longueur, et de dimension -2 par rapport à l'unité de temps. Pour Maxwell, la « dimension » de l'accélération est toute l'expression formula_70, et non la série des exposants ; c'est cette terminologie qui est utilisée aujourd'hui.

Dès la fin du et le début du siècle, avec l'étude plus approfondie des propriétés des fluides et des corps en mouvement dans les fluides, des physiciens comme Ludwig Prandtl, Theodore von Karman, Albert Shields, Johann Nikuradse et Rayleigh ont utilisé l'analyse dimensionnelle pour reproduire en laboratoire et dans des conditions contrôlables le comportement de phénomènes physiques, mais avec des vitesses ou des densités différentes, en se fondant sur les lois de similitude applicables à des maquettes d'échelles différentes. Ce principe de similitude, qui permet d'étudier des phénomènes physiques à des échelles différentes, est la base de la théorie de la similitude, aussi appelée théorie du modèle.

L'analyse dimensionnelle est en effet sous-jacente à la modélisation et la similitude. Le théorème de Vaschy-Buckingham montre que pour toute formule physique où interviennent "n" variables dimensionnelles indépendantes, dépendant de "k" unités fondamentales, la formule peut se transformer en une formule équivalente dépendant de "n-k" variables sans dimension se déduisant des variables initiales. Cette transformation permet d'appliquer la même loi, et donc de reproduire le même phénomène, à des échelles différentes, du moment que ces nombres sans dimension sont identiques dans les deux cas. Dans un cas particulier important, lorsque "n"="k", il n'y a pas de variable libre sans dimension, et le théorème implique que l'expression sans dimension que les variables peuvent former est constante pour le phénomène considéré.

Inversement, dans l'étude d'un phénomène physique, il n'est nécessaire d'étudier le comportement du système que lorsque ces variables sans dimension varient, le reste s'en déduisant par proportionnalité. Une analyse dimensionnelle permet alors de dégager les variables pertinentes pour l'étude du phénomène considéré, ce qui demande un bon sens de la réalité physique, mais permet ensuite de limiter le plan d'expérimentation à ces seules dimensions. Tous les graphiques de résultats où les axes sont des nombres sans dimension découlent d'une analyse dimensionnelle.





</doc>
<doc id="19222" url="https://fr.wikipedia.org/wiki?curid=19222" title="Enthalpie">
Enthalpie

En physique, la fonction enthalpie est une quantité reliée à l'énergie d'un système thermodynamique. Elle comprend l'énergie interne du système, à laquelle est ajouté le produit de la pression par le volume.

L'enthalpie est un potentiel thermodynamique. Il s'agit d'une fonction d'état qui est une grandeur extensive. L'unité de mesure de l'enthalpie dans le Système international d'unités (SI) est le joule, même si d'autres unités historiques sont encore parfois en usage.

L'enthalpie est couramment utilisée lors de l'étude des changements d'état mettant en jeu l'énergie d'un système dans de nombreux processus chimiques, biologiques et physiques. Pour les processus effectués à pression constante, la variation d'enthalpie correspond à la chaleur absorbée (ou dégagée) : formula_1, lorsque le travail n'est dû qu'aux forces de pression. Dans ce cas, la variation d'enthalpie formula_2 est positive dans les réactions endothermiques (qui absorbent de la chaleur) et négative dans les réactions exothermiques (qui libèrent de la chaleur).

Le mot « enthalpie » (du préfixe "en-" et du grec "thalpein" : « chauffer ») serait la création d'Heike Kamerlingh Onnes: .

Considérons une transformation isobare au cours de laquelle le système passe d'un état A à un état B d'équilibre en échangeant de la chaleur formula_3 et du travail uniquement par l'intermédiaire des forces de pression formula_4.

Le premier principe permet d'écrire :

où formula_6 est la fonction d'état "énergie interne".

À pression constante, le travail des forces de pression est égal à :

Donc :

d'où :

On définit ainsi une nouvelle fonction d'état, la fonction "enthalpie" :

Il s'ensuit que :

Par conséquent, à pression constante, la chaleur mise en jeu, qui n'est pas une fonction d'état puisque c'est un transfert d'énergie entre le système et le milieu extérieur, devient égale à la variation de la fonction d'état enthalpie formula_12. La variation de cette fonction ne dépend que de l'état final et de l'état initial du système et est indépendante du chemin suivi par la transformation.

C'est tout l'intérêt de l'application de la fonction enthalpie dans les cas très courants de transformations effectuées à l'air libre, à pression atmosphérique constante.

Cette propriété est à la base de la calorimétrie à pression constante. Par abus de langage, on confond souvent les termes « chaleur » et « enthalpie ».

"Remarque" :



De la définition de la fonction d'état formula_15 on déduit la différentielle :

Appliquons le premier principe :

(où formula_18 représente tout travail "autre que" le travail des forces de pression.) ;

Appliquons le second principe :

si la transformation est réversible, d'où :

Si le système est purement physique (pas de montage électrochimique induisant un travail électrique), on obtient :

Dans le cas d'une réaction chimique effectuée à "T" et "p" constantes, a été définie une grandeur de réaction appelée enthalpie de réaction, associée à l'équation bilan de la réaction : formula_23. "Cette grandeur de réaction permet d'avoir accès à la chaleur mise en jeu au cours de la réaction". Le calcul peut être effectué grâce aux valeurs de l'enthalpie standard de formation de chaque constituant intervenant dans la réaction : formula_24. Les valeurs ont été consignées dans des tables thermodynamiques, établies à la température de référence de .

Dans l'exemple ci-dessous, est établie la relation qui existe entre enthalpie standard de formation et enthalpie de réaction, dans le cas de la combustion d'un alcane : le méthane.

L'eau est considérée ici à l'état liquide parce que l'état final est à 25°C. La vapeur d'eau formée par la combustion est alors condensée en liquide à l'intérieur du calorimètre employé pour la mesure. 

On calcule l'enthalpie de combustion du méthane à sous la pression standard à partir de la loi de Hess :

L'enthalpie standard de formation du dioxygène est nulle car c'est un corps pur simple stable dans les conditions choisies (voir Enthalpie standard de formation).

Le calcul de cette enthalpie de combustion permet le calcul de la chaleur de réaction (voir Enthalpie de réaction). Inversement, si l'on mesure cette chaleur à l'aide d'une bombe calorimétrique, on peut avoir accès à l'enthalpie standard de formation du méthane.

Il existe une relation entre l'enthalpie de réaction et l'énergie interne de réaction :

Pour les réactions chimiques, le terme formula_29 associé au changement de volume est généralement très petit devant formula_30, de sorte que l'enthalpie de réaction peut être assimilée à l'énergie interne de réaction avec une bonne approximation.

Pour la combustion d'une mole de méthane à titre d'exemple, formula_31, mais formula_32 seulement.

L'état standard pour les réactifs et les produits impose les conditions de pression "P"= et de concentration "C" = , la température est choisie arbitrairement. L'état de référence en revanche impose une température de ().

Une détente isenthalpique est une réaction où l'enthalpie ne varie pas. Un bon exemple est la détente de Joule-Thomson.



</doc>
<doc id="19224" url="https://fr.wikipedia.org/wiki?curid=19224" title="Saint-Malo">
Saint-Malo

Saint-Malo est une commune française située en Bretagne, dans le département d'Ille-et-Vilaine, et le principal port de la côte nord de Bretagne. Le secteur touristique y est également très développé.

Ses habitants, les "Malouins" et les "Malouines", étaient en .

La commune de Saint-Malo est située dans le nord-est de la Bretagne, sur le littoral de la Manche et sur la rive droite de l'estuaire de la Rance. Elle se trouve à au nord de Dinan, à au nord de Rennes et à de Paris.

Saint-Malo constitue la partie nord-ouest du Clos-Poulet, une large presqu'île délimitée par la Rance, la Manche et la dépression de Châteauneuf. À l'extrémité nord-est du Clos-Poulet, se trouve Cancale, qui ferme à l'ouest la baie du mont Saint-Michel. Le littoral du Clos-Poulet fait partie de la côte d'Émeraude, qui s'étend de Cancale au cap Fréhel.

Les communes limitrophes de Saint-Malo sont, au nord-est, Saint-Coulomb, à l'est, Saint-Méloir-des-Ondes, au sud-est et au sud, Saint-Jouan-des-Guérets, au sud-ouest et à l'ouest, sur la rive gauche de la Rance, Dinard, La Richardais et Pleurtuit.

Saint-Malo n'est pas limitrophe de Dinard car les deux communes sont séparées par l'estuaire de la Rance.

L'actuelle commune de Saint-Malo résulte de la fusion en 1967 de l’ancienne commune de Saint-Malo (la vieille ville "intra-muros" et les quartiers de Rocabey, de la gare, de Marville, de Courtoisville et de la Découverte) avec celles de Paramé et Saint-Servan.

Le site originel de l’agglomération malouine comprend les îlots rocheux de la Cité (altitude ), reliés par le tombolo de Solidor à Saint-Servan où s’était installée l’antique Alet (de la dénomination ancienne "Aleto" signalée par la "Notitia Dignitatum" ou Notice des Dignités Impériales, manuscrit du premier quart du ) et de Canalchius - du vieux gaulois "Canalch" (altitude 13,80) au nord - devenu au fil du temps le Saint-Malo intra-muros.C'est sur celui-ci, entouré des îlots du Grand Bé, du Petit Bé et du Fort National accessibles à marée basse, doublés de ceux d'Harbour, de Cézembre et de la Conchée dans la rade constellée de multiples rochers, que l’ermite Aaron accueillit un moine originaire de ce qui deviendra par la suite le Pays de Galles : Maclow, Malo ou Maclou (la graphie varie), le futur saint-Malo. Au , l’évêque Jean de Châtillon y transféra le siège épiscopal, dotant la ville de ses premiers remparts. Dénommée à l'époque « Saint-Malo-de-l'Île », la cité connut son premier essor.
Reliée aux falaises de Paramé par un cordon dunaire bordé d’une plage longue de quatre kilomètres s'étendant jusqu’à celle du Minihic en Rothéneuf (cordon renforcé par une digue et loti en villas à la fin du ), ce tombolo protégeait une baie intérieure battue par les marées, permettant ainsi la création du port. Les endiguements progressifs depuis le permirent l’extension de quartiers à partir de la gare au . L’urbanisation s’est poursuivie depuis les années 1960 sur les plateaux de Paramé à l’est et de Saint-Servan au sud. Les trois villes ont été réunies en 1967 et l’agglomération s’étend ainsi jusqu’à la pointe de la Varde et au Havre de Rothéneuf au nord-est et le long de la ria de la Rance au sud.

Le littoral maritime, d'une dizaine de kilomètres, est formé d'ensembles rocheux entrecoupés de plages à l'est de la pointe de la Varde (secteurs du Pont, de Minihic, de Rothéneuf et de la Guimorais) et d'une longue plage entre la base de la pointe de la Varde et la Cité historique (plage longée par la digue de Rochebonne).

Le premier franchissement de la Rance est assuré par le barrage de l'usine marémotrice de la Rance entre Saint-Malo (quartier de la Briantais) et Dinard.

Le site du centre-ville, avec la Cité historique (le vieux Saint-Malo), la Cité (ancien Alet) et le port, est formé par un littoral complexe, avec de nombreux récifs et brisants immergés à marée haute, des tombolos sous-marins, visibles aux marées basses de vives eaux, par des îles ou îlots dont beaucoup ont été fortifiés aux (Cézembre, Fort Harbour, le fort de la Conchée, le Grand Bé et le Petit Bé, l'île du Fort National).

La Cité historique a d'abord été construite sur une île rocheuse située entre la pointe du Naye au sud et les prairies de Cézembre, devenue une presqu'île - légende le présentant comme la conséquence du raz-de-marée de 709, le rocher sert donc de fondation. Les murs de construction traditionnelle en moellons en suivront par la suite le tracé.

Les marées de la baie de Saint-Malo sont parmi les plus importantes en Europe. Elles sont provoquées par la concentration des eaux au cœur d'une baie triangulaire entre Bretagne et Cotentin.
Au maximum, le marnage (amplitude entre marée basse et marée haute) peut y atteindre , soit plus du double du marnage ordinaire en Atlantique. C'est pour cette raison que le barrage de l'usine marémotrice fut construit sur l'estuaire de la Rance, en amont de l'intra-muros de Saint-Malo (l'autre option étant la baie du mont Saint-Michel) au début des années 1960.

La prudence est de rigueur en bord de mer. Avant de s'aventurer sur les bancs de sable ou sur les rochers à marée basse, il convient de se renseigner sur les horaires des marées, au risque de se retrouver piégé par la mer (horaires des marées disponibles dans les offices de tourisme). Les jours où se conjuguent grandes marées et des vents importants imposent un surcroît de précautions. Les risques d'être emporté par la mer et de mourir noyé sont réels. Une signalisation spécifique et des bouées de sauvetage ont été installées tout au long de la chaussée du Sillon.

La baie de Saint-Malo jouit d'un climat tempéré très océanique. Les températures sont adoucies par le Gulf Stream et l'amplitude des températures moyennes entre hiver et été est très faible (de en janvier à en août) avec une moyenne à .

Du point de vue de la richesse de la flore, Saint-Malo est à la cinquième place des communes du département possédant dans leurs différents biotopes le plus de taxons, sa voisine Saint-Coulomb étant en tête, soit 618 pour une moyenne communale de 348 taxons et un total départemental de taxons (118 familles). On compte notamment 81 taxons à forte valeur patrimoniale (total de 207) ; 60 taxons protégés et 30 appartenant à la liste rouge du Massif armoricain (total départemental de 237).
De nombreux parcs et jardins disséminés aux quatre coins de la ville accueillent des espèces riches et variées comme à la roseraie Sainte Anne qui accueille un rosier datant de 1797 (la cuisse de nymphe).

On rencontre l'appellations suivante : "Insula Aaronis" au siècle (« L'île d'Aaron »). Un ermite du nom de "Aaron" y construisit à cette époque un ermitage (selon la tradition, au lieu de la chapelle Saint-Aaron). 

Le nom de la localité est attesté sous la forme latinisée "Macloviensem" en 1162, puis "Saent Mallou" en 1282, "Saint Malou" en 1287 et en 1294, "Saint Malo" en 1304, etc.

Il s'agit d'une formation toponymique médiévale en "Saint-", dont le second élément "-Malo" se réfère à un saint connu autrement sous le nom de "Maclovius".

Durant la Révolution, la commune est rebaptisée "Port-Malo", puis "Commune-de-la-Victoire", puis "Mont-Mamet".

En gallo, la commune se nomme "Saent-Malo" et en breton "".

Quatre anciennes communes ont été absorbées par Saint-Malo et restent présentes dans la toponymie locale : Saint-Servan (souvent dit Saint-Servan-sur-Mer), Paramé, Rothéneuf, Saint-Ideuc.

L'histoire de Saint-Malo remonte à l'époque gauloise : les Coriosolites occupent en premier les lieux. Sous l'influence romaine, la ville de Corseul (dans les terres) se développe aux dépens de la cité d'Alet mais Alet demeure un port important au point qu'à la fin du les Romains choisissent de le fortifier. À cette époque, face à Alet, l'île de la future Saint-Malo est encore inhabitée.

Lors du retrait de l'armée romaine (le 16 janvier 423), Alet subit de nombreuses attaques venues du Nord. C'est ensuite que saint Malo, venant de l'actuel Pays de Galles, s'installe sur le rocher qui prendra le nom de rocher de Saint-Malo en 541.

Alet continue de se développer jusqu'à la fin du premier millénaire où, après plusieurs attaques des Normands, la ville est durablement affaiblie. Au milieu de , le siège épiscopal d'Alet est déplacé sur le rocher de Saint-Malo, mais on ne sait si l'arrivée de l'évêque précède ou suit la première urbanisation de Saint-Malo. Cet événement marque néanmoins la fin de la grandeur d'Alet. Désormais, la position stratégique du port est l'objet de conflits entre les ducs de Bretagne et les rois de France. Saint-Malo sera ainsi rattachée provisoirement au domaine royal de 1395 à 1415, restituée au duc de Bretagne de 1415 à 1488, puis à nouveau intégrée au domaine royal en 1488.

Le , Saint-Malo proclame son indépendance au royaume de France et devient la République de Saint-Malo. L’épisode de quatre ans s’achèvera le avec la conversion au catholicisme du roi Henri IV, la ville revenant à l'issue de cette période dans le giron des rois de France.

C'est avec la découverte des Amériques et le développement des échanges commerciaux avec les Indes (premier navire négrier armé à Saint-Malo en 1669) que Saint-Malo prend son envol économique et s'enhardit considérablement. Les armateurs deviennent plus nombreux et des personnages de cette époque font la renommée de la ville. Jacques Cartier découvre et explore le Canada, les corsaires harcèlent les marines marchandes et militaires ennemies, tels Duguay-Trouin, puis un peu plus tard Surcouf.
D'autres s'illustrent dans les sciences, tel Maupertuis, ou dans les lettres et la politique comme Chateaubriand. Modification du style de vie, les armateurs se font construire de belles demeures particulières appelées malouinières.

L'essor de Saint-Malo est affecté par la Révolution française qui ne l'épargne pas. L'épisode le plus dramatique fut la fusillade dans les dunes du Talard de 60 "contre-révolutionnaires" de l'armée vendéenne en décembre 1793. Le plus jeune avait 16 ans, le plus âgé 19. La pêche errante, la "Grande Pêche", sur les bancs de Terre-Neuve se développe. Le tourisme balnéaire commence très tôt ( de bains en 1838) ainsi que le tourisme littéraire et artistique avec la mise en place du tombeau de Chateaubriand sur l’îlot du Grand Bé, 10 ans avant la mort de l’écrivain.

Durant la Seconde Guerre mondiale, Saint-Malo et ses alentours sont fortifiés par les Allemands comme plusieurs ports de la façade Atlantique. La "Festung" (forteresse) Saint-Malo devient même comme toute la zone côtière d'accès restreint. Lors de la libération de la ville en , les points de résistance allemands sont bombardés par les Américains dont la cité "intra-muros" où, malgré des renseignements communiqués par des officiers de la Marine nationale française encore présents à Saint-Malo, ils pensent à tort que se trouve une importante garnison. Cette garnison allemande se trouve en réalité sur la cité d'Aleth. Ces bombardements et les incendies qu'ils déclenchent détruisent 80% de la vieille ville. Les Allemands sur l'île de Cézembre résisteront encore pendant près d'un mois, ne se rendant qu'après un pilonnage terrestre, maritime et aérien intensif dont l'utilisation de bombes au napalm.
La reconstruction de la vieille ville se fera dans un style « historicisant » mais non « à l'identique » : les remparts n’ayant pas été détruits, la ville est reconstruite au sein de cet espace. Volonté étant de conserver autant que possible à la cité historique sa silhouette traditionnelle, les nouveaux édifices durent adopter le style ancien. Quelques constructions en nombre limité, mais indispensables, purent faire l’objet d’une reconstruction « à l’identique » grâce aux vieilles pierres récupérées, numérotées et remployées.

Saint-Malo est aujourd'hui un important centre touristique estival, également port de commerce, de pêche et de plaisance.

"Les devises sont traditionnellement associées aux armoiries".

Saint-Malo est un quartier maritime dont le code est « SM ».

. L'hermine, symbole des remparts, marche sur la herse, symbole de la ville.

Quant à la présence du dogue dans les armoiries, ces chiens qu'on lâchait le soir sur la grève, pour préserver la ville des pillages de navires, et ce jusqu'en 1770 : Seul Guy Le Borgne indique en 1667 dans son "Armorial breton" , mais dès sa période autonomiste de 1590-1594, la ville de Saint-Malo a fait usage d’un blason avec une hermine. Ce dernier fut confirmé en 1615 par le roi Louis XIII et, en 1696, dans l’"Armorial général" d’Hozier. On peut donc douter que le blason à dogue ait été celui de Saint-Malo.

Le pavillon bleu à croix blanche était celui de la marine marchande française. On y a ajouté un quartier rouge avec l'hermine marchant vers la gauche pour personnaliser les navires de Saint-Malo.

Il n’y a pas de pavillon propre aux corsaires malouins mais un pavillon propre aux navires enregistrés à l’Amirauté de Saint-Malo. Existant dès le , le corsaire en cas d’attaque ne devait arborer que les suivants : "Pavillon du Roi" sous l'Ancien Régime, ou le drapeau national tricolore à partir de la Révolution.

Durant cette période, le maximum de la population est atteint en 1962 avec .

1967 va connaître un essor significatif en raison de la fusion des communes de Saint-Malo, Saint-Servan ( habitants en 1962) et Paramé ( habitants en 1962), fusion intervenue le . Publié au Journal Officiel de la République Française le , le décret précisera que les anciens cantons de Saint-Malo et de Saint-Servan-sur-Mer prennent respectivement les noms de Saint-Malo-Nord et Saint-Malo-Sud. Quant au siège de la nouvelle commune, il est fixé à l'hôtel de ville de Saint-Malo.

La ville compte logements principaux et résidences secondaires.
De nombreux projets immobiliers voient le jour sur la commune. La mairie de Saint-Malo soutient la construction de nouveaux logements en se fixant l'objectif de construire 400 nouveaux logements par an d'ici dix ans.

Saint-Malo est le siège de la Chambre de commerce et d'industrie de Saint-Malo-Fougères. Elle gère le port de commerce, de pêche et de plaisance de Saint-Malo ainsi que le port de pêche de Cancale, la cale du Bec de la Vallée et l’aéroport de Dinard Pleurtuit Saint-Malo. Elle gère aussi l'espace Duguay-Trouin qui permet d'organiser des salons.

Le développement économique de Saint-Malo relève de la compétence de Saint-Malo Agglomération qui a aménagé un parc technopolitain « Atalante Saint-Malo ». Ce parc est destiné à accueillir des entreprises de recherche, de production et de services dans deux filières :

Le pays de Saint-Malo conserve une activité exportatrice développée.

Les plus grosses entreprises de la ville sont :

À Saint-Malo, il existe trois centres villes :

À Saint-Malo, il existe cinq centres commerciaux :

Les marchés ont lieu tous les jours de la semaine de 8 heures à 13 heures :

Saint-Malo est un port de commerce actif, deuxième port de commerce de Bretagne. Il est composé de quatre bassins, le bassin Vauban, le bassin Duguay-Trouin, le bassin Jacques-Cartier ainsi que le bassin Bouvet.
Les engrais et le bois forment une part importante des exportations du port de Saint-Malo, mais de nombreux autres produits y transitent. Le port de Saint-Malo est un des premiers ports français d'importation de granit. Les produits agro-alimentaires sont également très exportés en raison de l'importance de la production alimentaire et agricole en Bretagne.

Le trafic est de plus en plus important et ceci grâce aux liaisons avec la Grande-Bretagne et l'Irlande mais aussi avec les îles Anglo-Normandes.

Chaque année, c'est plus d'un million de passagers qui transitent par le port de Saint-Malo à destination ou en provenance de Grande-Bretagne, d'Irlande, de Jersey ou de Guernesey. L'avant-port de Saint-Malo est doté de deux terminaux ferries récents et performants, le terminal Ferry du Naye puis le terminal de La Bourse. Le nombre de passagers à travers la Manche s'est nettement développé depuis quelques années grâce en particulier aux liaisons régulières avec l'Angleterre : Portsmouth et Poole. Le port de Saint-Malo réalise un trafic important avec les îles de Jersey et Guernesey, autant en passagers qu'en marchandises. C'est l'activité touristique de ces îles qui est très développée et donc qui procure un flux continu. Le port de Saint-Malo, grâce à deux rampes RO/RO bien adaptées, est le principal site français d'acheminement de produits, de groupage et de conteneurisation vers Jersey et Guernesey.

Cependant l'accès au port est aussi rendu difficile par l'amplitude des marées et les fonds rocheux. Quatre phares balisent les différents chenaux d'accès : le Grand Jardin, la Balue, les Bas-Sablons et Rochebonne.

Sous-préfecture d'Ille-et-Vilaine, l'administration municipale de Saint-Malo est dirigée par Claude Renoult depuis avril 2014.

La fusion entre Saint-Malo, Paramé et Saint-Servan fut suggérée sous le terme de « réunification » à plusieurs reprises au cours du , lors des travaux de transformation du port de marée en bassins à flot.

Toutefois ce projet ne se réalisera qu'en 1967 avec pour élément déclencheur la volonté de jeunes entrepreneurs malouins de créer des zones industrielles communes aux trois villes (INDUSMA). L'ensemble des trois localités ainsi réunies, prend le nom de « Saint-Malo »

La ville est le chef-lieu de deux cantons :

Depuis 2001, une communauté d'agglomération nommée Saint-Malo agglomération regroupe 18 communes autour de Saint-Malo. Cette communauté d'agglomération fait partie d’un pays « Voynet » du même nom. Saint-Malo le chef-lieu de l’arrondissement de Saint-Malo regroupant 63 communes. De manière traditionnelle et historique, Saint-Malo est la « capitale » du pays de Saint-Malo.

Saint-Malo est jumelée avec Port-Louis, capitale de l'Île Maurice, en 1999.

Au Canada, Saint-Malo a des liens avec les lieux suivants:

La ville regroupe plus d'une soixantaine de médecins généralistes, plus d'une centaine de médecins spécialisés, une trentaine de pharmacies. Elle possède deux hôpitaux dont l'un est un centre hospitalier, une clinique, alliée de dix maisons de retraite (publiques et privées confondues) et de quatre laboratoires d'analyses médicales.

Le culte catholique est principalement représenté par la cathédrale Saint-Vincent, monument historique de style roman et gothique. Elle a été l'ancien siège de l'évêché de Saint-Malo jusqu'en 1801.

Autres lieux de culte catholique : 



La commune possède plusieurs cimetières : le cimetière de la Vigne au chapitre, le cimetière de Lorette, celui de Paramé, le cimetière du Rosais…

L'enseignement à Saint-Malo est représentatif de l'activité économique régionale, mettant notamment l'accent sur les métiers en relation avec la mer, le tourisme et l'agriculture.

Ainsi, on peut citer dans le domaine maritime l'école de la Marine marchande, un lycée professionnel maritime proposant des formations de marine marchande et d'aquaculture ainsi que le lycée Les Rimains nouvellement installé à côté de la nouvelle gare ferroviaire, préparant aux concours de la marine marchande.

Le tourisme est lui représenté par un lycée hôtelier dans la ville voisine de Dinard, mais aussi par le lycée La Providence à Saint-Malo (intra-muros) qui propose de nombreuses formations dans le domaine de la vente, du commerce ainsi que du service et de l'accueil. On trouve aussi une École nationale de police.

Parmi les enseignements moins liés à l'activité régionale : 

L'enseignement secondaire général est réparti dans trois lycées : deux publics, le lycée Jacques-Cartier et le lycée Maupertuis, ainsi que l'établissement privé l'Institution-La Providence.

À la rentrée 2016, 27 élèves étaient scolarisés dans la filière bilingue publique (soit 0,8 % des enfants de la commune inscrits dans le primaire).

Saint-Malo est relié au réseau ferroviaire depuis 1864. Il disposait de deux réseaux de tramways, les Tramways Bretons et le Tramway de Rothéneuf, qui circulèrent jusqu'en 1947, et qui sont les ancêtres de l'actuel réseau d'autobus urbain.

La Saint-Malo agglomération organise un réseau de transport en commun, actuellement exploité par la société Keolis dans le cadre d'une délégation de service public de six ans, jusqu'en 2012, qui dessert notamment Saint-Malo. personnes ont accès au réseau de transport en commun, qui est composé de 16 lignes de bus ou cars.

En saison, lors des grands week-ends et des grands événements, une navette gratuite est mise en place entre le parking Paul Féval, situé à l'entrée de la ville, et l'Intra Muros (avec parking et services camping-cars à proximité immédiate). Dans quelques années, la mairie prévoit de remplacer cette navette par un Transport en Commun en Site Propre (TCSP).

Dotée d'une nouvelle gare depuis juin 2005, Saint-Malo est reliée à Rennes (environ 20 trains par jour, temps de parcours : 45 min à 1 h) et à Paris (3 à /R par jour ; temps de parcours : 2 h 56 à 3 h 15).
En saison s'ajoutent des liaisons avec Granville (/R par jour, 1 h 15) et Lille Europe (le week-end seulement).

De nombreux Malouins partent chaque matin travailler à Rennes et rentrent le soir, bénéficiant de deux arrêts dans la capitale régionale ; outre la gare Rennes, la gare de Pontchaillou dessert l'hôpital Pontchaillou, des établissements scolaires et le campus de Villejean.

De nombreux lycéens de Dol et de Combourg utilisent le train pour se rendre aux lycées de Saint-Malo qui offrent un large choix de formations dans les métiers du tourisme ou du commerce et dans l'enseignement général et technologique.

À l'inverse, les Rennais viennent parfois l'été ou les dimanches profiter de la mer à Saint-Malo.

La ville voisine de Pleurtuit possède un aéroport sur son territoire.

Il est géré par la CCI de Rennes, la CCI de Saint-Malo et le groupe Vinci Airports.
En 2007, environ passagers ont été transportés par les deux compagnies Ryanair et Aurigny Air Service. L'aéroport propose des liaisons avec Londres Stansted, Nottingham, Bristol, Birmingham et Guernesey.

Saint-Malo est en plein renouvellement urbain sur la période 2008 - 2014. Les projets fleurissent sur toute la commune et dans tous les quartiers. Chaque chantier va contribuer à modifier la physionomie globale de la ville. Le point, chantier par chantier :
Le projet, hormis le pôle culturel, est en cours de finition…
Quelques chiffres : 
Coût du projet : 57 millions d’euros.


Le sport à Saint-Malo est diversifié avec une tendance importante pour les sports nautiques. Étape du Tour de France 2008, événements nautiques et médiatiques, grands rassemblements, compétitions nationales et internationales.

6,8 hectares sont dédiés au sport à Saint-Malo.

Voici la liste du patrimoine sportif :


Les événements ayant pour cadre la ville de Saint-Malo les plus connus sont associés aux courses nautiques : le départ de La Route du Rhum, course de voiliers transatlantique en solitaire, a lieu tous les quatre ans, et l'arrivée de la Transat Québec-Saint-Malo en équipage qui a également lieu tous les 4 ans. Saint-Malo accueille également des évènements nautiques occasionnels comme un départ de la Course des Grands Voiliers (anciennement la Cutty Sark) en 2012 ou d'envergure plus modestes comme le Raid des Corsaires dans la baie de Saint-Malo. Dans le domaine, on notera également un salon du nautisme d'occasion, Saint-Malo à la Hune.

Dans le domaine musical, Saint-Malo présente chaque année le festival de ""La Route du Rock"" (été et hiver), en association avec la proche commune de Saint-Père. La ville organise également le festival de musique ""Classique au large"", au printemps.

La ville de Saint-Malo, le conseil général d'Ille-et-Vilaine et le conseil régional de Bretagne soutiennent financièrement l'organisation du ""Festival de musique sacrée"" qui se tient à la cathédrale Saint-Vincent pendant la période estivale.

Parmi les évènements culturels importants figurent le ""Quai des Bulles"", festival de la bande dessinée et de l'image projetée, qui a lieu à l'automne, ainsi que le festival littéraire des ""Étonnants voyageurs"", au printemps. D'autres évènements animent aussi la ville, tel le ""Festival européen du théâtre lycéen francophone"". Un carnaval haut en couleur a eu lieu le .

Depuis 2009, tous les jeudis de l'été, le festival ""Renc'arts"", permet à des artistes (de rues, de cabarets et de scènes) d'envahir les rues pour proposer gratuitement leurs prestations. Sont également proposés au public en été : les ""Mardis zicos"" (2 concerts au pied du château), les ""Mercredis des Douves"" (animations ludiques également au pied du Château) et les ""Vendredis de la Hoguette"" (animations sportives sur la plage).
Enfin se tient l'été le festival ""Folklores du monde"" en remplacement de la "Fête des Œillets de Paramé" créée après la Seconde Guerre mondiale. Ce festival fut, dans un premier temps, exclusivement orienté vers la culture bretonne. Depuis, il s'est élargi aux cinq continents en accueillant chaque année de nombreux groupes de musique et danse venus des quatre coins du monde. La cité malouine est, lors de cet événement, représentée par les Corsaires Malouins (groupe de chant de marins), l'association folklorique Gwik Alet ainsi que le groupe Quic-en-Groigne, ensemble traditionnel breton, composé d'un bagad classé en de la fédération Bodadeg ar Sonerion et d'un cercle celtique, champion de Bretagne 2004, 2005, 2006, 2008, 2009, 2010 et 2011 de la fédération de danse bretonne War'l leur.

Durant chaque hiver, la fête foraine de la ""Saint Ouine"" prend place pendant environ un mois sur les quais du port de la ville au pied des remparts. Elle se déroulait dans ses débuts sur l'îlot du Grand-Bé près d'une chapelle (Notre-Dame-du-Laurier) dans laquelle était vénérée une statue de saint Ouen d'où son nom. Avec le temps, la fête se transféra sur les quais coïncidant, de 1926 à 1966 avec le Pardon des Terre-Neuvas - ce qui contribua d'ailleurs à lui redonner de l'importance.

La plus grande salle de concerts est la "Nouvelle vague" (anciennement nommée Omnibus). C'est une structure originale composée d'une salle de spectacle de 920 places qui présente toute l'année de nombreux concerts (rock, électro, reggae, musique bretonne, jazz…) ainsi que de salles de répétitions et d'enregistrements.
En octobre 2012, c'est l'association Rock Tympans, organisatrice du festival Route du Rock, qui obtient pour 5 ans la gestion de l'Omnibus, à la suite d'un appel d'offres lancé par la municipalité.

Deux théâtres sont présents dans la ville de Saint-Malo. L'un se situe intra-muros (Chateaubriand) et l'autre à Saint-Servan (place Bouvet). Chacun accueille des spectacles pour adultes ou enfants avec des programmations variées et orientées vers tous les publics.

Il existe en 2013 un seul complexe de cinéma, "Le Vauban", à Rocabey près du quartier de la gare. Cependant, deux autres devraient voir le jour. L'un au cœur de la future médiathèque (3 salles), et l'autre à côté du centre commercial de la Découverte, sur l'ancien site de la Seiffel (7 salles).




Saint-Malo abrite 83 monuments historiques et 169 bâtiments inventoriés. Les plus connus ou les plus emblématiques, sont situés dans la cité historique, "intra-muros" :




Le tour des remparts est sans doute la première attraction touristique de Saint-Malo. Ces remparts ceignent entièrement la ville et on peut en faire le tour virtuellement sur le site de l'office du tourisme malouin.

On pénètre aujourd'hui à l'intérieur de la ville close par huit portes et trois poternes. Au , il n'existe que 
La statue de Notre-Dame de Bon-Secours, placée dans une niche en haut de la porte date d'après les experts du début du et semble être déjà en place en 1439. Le corps de garde placé en haut, détruit par un incendie en 1661, lui fit quelques dégâts nécessitant sa restauration et elle fut reposée et bénite en 1663. Elle est en calcaire blanc, et était polychrome. Elle subit les outrages du temps et des gens, jetée bas et brûlée à la Révolution, elle fut finalement restaurée et remise en place. L'originale se trouve aujourd'hui à l'abri et est remplacée par une copie. On lui prêta de nombreux miracles.

En 1564, on installe à la porte une herse et un hérisson, les deux grosses tours qui l'encadrent datent de 1582 et furent armées de canons. Le corps de garde au-dessus de la porte, fut supprimé en 1590, et remplacé par un beffroi avec une horloge à "Deux visages", on y logea également une cloche répondant au nom de "Noguette" sonnant tous les soirs le couvre-feu et le lâcher des 24 dogues qui montaient la garde. Ceux-ci furent supprimés en 1770, après plus de de bons et loyaux services. ""Noguette"", sonne toujours, mais l'originale fut placée dans le clocher de la cathédrale, à laquelle on accède directement depuis cette porte. Pour la protéger encore plus, on construisit un ravelin en 1644 qui sera supprimé lors de la reconstruction du quai Saint-Louis en 1839. En ce temps là existaient également trois poternes :
Entre ces deux portes :

Puis revenant en arrière le long du quai Saint-Louis :
Elle est encastrée de deux pilastres à bossages, et était ornée, avant la Révolution, des armes du roi, posées en 1721. Des inscriptions en latin, rappellent que le territoire de la ville s'est agrandi et que l'évêque et la chapitre de Saint-Malo ont concédé le terrain. Elle est encadrée du côté ville de deux salles voûtées, aménagées dans l'épaisseur du rempart. Elle était précédée d'un pont-levis.
Le passage très fréquent des bateaux qui descendaient la Rance, depuis Dinan pour ravitailler Saint-Malo, a donné le nom de cette première ville à la porte et au quai, ainsi qu'à la rue en face. Elle est classée au titre des monuments historiques depuis 1886.

La Cité historique est une des plus visitées de Bretagne. En 2013, "Travellers'Choice Destinations" de TripAdvisor a classé la cité malouine en dixième position des villes touristiques françaises. 
Le château de Saint-Malo, qui héberge aujourd'hui la mairie, a été construit par les ducs de Bretagne puis aménagé et modernisé par Siméon Garangeau, disciple de Vauban. Par ailleurs, son donjon abrite actuellement le musée d’Histoire de la Ville et du Pays Malouin. Intégré à la partie nord de l’enceinte par la porte Saint-Thomas, il s'en sépare − au sud-est de la porte Saint-Vincent − par une interruption, cette dernière correspondant à l’ancienne douve qui l'isolait du reste de la ville.

Au centre de la Cité historique se dresse la cathédrale Saint-Vincent de Saint-Malo, dédiée à saint Vincent de Saragosse, repérable à son clocher dominant les toits.

Sur le tour des remparts on trouve disséminées les monuments à Jacques Cartier, Duguay-Trouin, Surcouf , Chateaubriand ou de Mahé de la Bourdonnais.
Comme autres points d'attraction permanents aux abords des remparts, le Fort National, au nord de la Cité historique, est accessible à marée basse, tout comme le fort du Petit Bé, et la tombe de Chateaubriand sur l'île du Grand Bé.




Saint-Malo est: 












</doc>
<doc id="19226" url="https://fr.wikipedia.org/wiki?curid=19226" title="Diathèse">
Diathèse

La diathèse, en linguistique, est un trait grammatical qui décrit comment s'organisent les rôles sémantiques dévolus aux actants, par rapport au procès exprimé par le verbe, en particulier les rôles d'agent et de patient. La diathèse affecte la répartition syntaxique et le marquage morphologique de ces rôles sur le verbe et les différents actants. En revanche, le changement de la diathèse d'un verbe, quand il est possible, ne modifie pas profondément le sens de l'énoncé. On parle, plus spécifiquement, de voix du point de vue de la morphologie verbale pour décrire la forme que prend le verbe pour signifier une diathèse.

Certains verbes n'ont intrinsèquement aucune notion de diathèse : principalement les verbes d'état (comme "être", "paraître", "sembler", "demeurer", "rester", etc. En français, ces verbes sont, en effet, exclus de la notion d'actance.

La grammaire française distingue traditionnellement trois voix : active (qui est la voix non marquée et employée par défaut), passive et voix pronominale, laquelle recouvre sous une même forme plusieurs diathèses : moyen, réfléchi, réciproque, décausatif. Toutefois, le français exprime aussi d'autres diathèses comme le causatif et le factitif. D'autres langues en possèdent d'autres encore, comme le statif, l'antipassif ou l'applicatif.

Dans cette diathèse, le sujet et l'objet coïncident typiquement avec les rôles sémantiques d'agent et de patient d'un verbe d'action. La voix active est la diathèse plus générale, la plus fréquente, souvent morphologiquement la plus simple, et le choix par défaut dans la plupart des langues ; dans certaines, c'est même la seule diathèse disponible.

Dans les langues flexionnelles, le sujet est normalement au nominatif, l'objet à l'accusatif. Le cas est différent dans les langues ergatives.
Exemples :

Bien moins répandue, la voix moyenne est une diathèse surtout indo-européenne, qui ne s'est que rarement conservée dans les langues modernes dérivées. Elle indique que le "sujet-agent" accomplit l'action dans son propre intérêt ; il est en quelque sorte agent et patient du groupe verbal. Certaines utilisations de la voix moyenne recoupent donc celles de la construction réflexive (comme "je me lave"). Il serait cependant erroné de parler de voix moyenne pour le français (d'autant plus que la tournure pronominale ne se limite pas à cette diathèse), sauf peut-être dans des cas tels que :
dont la forme comme le sens se retrouvent d'ailleurs dans certains dialectes germaniques (voir article anglophone sur le cas "bénéfactif").

Le moyen se rencontre principalement en grec ancien, sanskrit, islandais (ancien et moderne) et latin pour une faible part (celui-ci ayant transformé la diathèse moyenne en passif). Il est notable qu'historiquement l'indo-européen opposait l'actif au moyen et ne connaissait pas le passif, qui n'a été qu'un développement tardif, souvent à partir du moyen lui-même (comme en latin). Ainsi, en grec ancien le moyen et le passif sont identiques sauf à deux temps, futur et aoriste : c'est pour cela qu'on parle souvent de voix "médio-passive". En sanskrit, le passif et le moyen ne se distinguent principalement qu'au présent ; ailleurs, c'est le moyen qui est utilisé pour signifier l'une ou l'autre voix. En islandais, enfin, les verbes moyens sont obtenus par la suffixation de "-st" (qui vient de "-sk" en vieil islandais, lui-même issu de "sik", accusatif du pronom réfléchi « soi-même ») et ne sont pas hérités de l'indo-européen. Ils possèdent plusieurs valeurs, parmi lesquelles une valeur réfléchie (voire moyenne), réciproque, passive, ou perfective, entre autres. On le voit, on nomme « voix moyenne » une diathèse qui n'est pas forcément de valeur moyenne mais peut emprunter les valeurs des autres diathèses.

En outre, certains verbes des mêmes langues indo-européennes sont exclusivement conjugués au moyen ; on les nomme "media-tantum" "(seulement moyens)" ou encore "déponents" (ce dernier terme étant surtout réservé au latin et au grec). S'il existe bien quelques verbes "media tantum" de valeur passive, dans la plupart des cas celle-ci est moyenne, active ou intransitive. De plus, vu qu'en latin la diathèse moyenne est devenue passive, les "media tantum" qui ont conservé un sens moyen sont particulièrement notables. 

Exemples : 

Dans beaucoup de langues, la voix active ne se réduit pas aux verbes d'action, mais s'applique aussi à la morphologie des verbes d'état. Toutefois, dans certaines langues, il existe une voix spécifique pour ces derniers. C'est par exemple le cas en akkadien, qui peut former sur tout nom ou adjectif un statif (ou permansif) indiquant l'état impliqué par ce nom ou adjectif. Le statif peut se former sur un adjectif verbal et aboutir ainsi à opposer un statif aux autres voix verbales. Le tableau qui suit présente la conjugaison comparée du présent actif du verbe akkadien "parāsum" « séparer » et du statif formé sur son adjectif verbal "parsum" « séparé ».

Dans cette diathèse, l'objet patient devient sujet grammatical. C'est donc une thématisation du patient par inversion des actants.

Cette diathèse est le plus souvent sentie comme secondaire par rapport à la voix active (qui serait un « degré zéro » d'expression) ; il s'agit de la transformation d'un énoncé actif, dans laquelle l'objet patient de l'actif devient le "sujet patient" (« sujet grammatical qui subit l'action »), tandis que le sujet acteur devient le "complément d'agent" du verbe.

Ce qui montre bien le caractère secondaire de cette diathèse dans certaines langues, c'est la possibilité de construire un énoncé passif agentif (à partir d'un verbe qui s'y prête) tout en créant une phrase qui « ne se dirait pas » de manière isolée ; prenons l'exemple du français : "la choucroute est mangée par moi" est un énoncé valide mais artificiel par rapport à "je mange la choucroute". Une telle phrase ne se comprendrait que dans une situation où l'objet est déjà au centre d'un énoncé préalable, justifiant sa position de sujet d'un énoncé conséquent : « Qui a mangé la choucroute et le gâteau? — La choucroute a été mangée par moi. »

Le verbe lui-même peut changer de forme dans les langues flexionnelles (il se met à la voix passive). Selon les langues, le complément d'agent est souvent introduit par une préposition : 

Exemples de transformation passive des énoncés précédents : 

On a ici envisagé les cas où la phrase passive serait le résultat de la transformation d'une phrase active. Le complément d'agent y est donc obligatoire, puisqu'il reprend le sujet de la phrase active. Par conséquent, seuls les verbes transitifs directs (c'est-à-dire les bi- ou trivalents) peuvent subir cette transformation (on ne peut considérer valides des transformations comme "je lui parle" > "*il est parlé à par moi" ; en revanche, "cette langue est parlée par des milliers de locuteurs" est valide, puisque l'on utilise le verbe "parler" de manière transitive directe). C'est pour cela que l'on parle de "passif agentif".

Il existe cependant des constructions passives qui ne sont pas issues de telles transformations et dans lesquelles l'agent n'est pas nécessaire voire impossible. Si le sujet patient d'un énoncé au passif non agentif continue de « subir l'action », le sujet sémantique de l'énoncé (celui qui agit réellement) n'est pas indiqué. Dans certaines langues (anglais, latin, grec dans une moindre part) un verbe peut être mis au passif non agentif alors qu'il est intransitif ou transitif indirect ; il peut même recevoir un objet patient. Dans ce cas, le passif non agentif sert de forme impersonnelle, indiquée dans les exemples suivants par la traduction au moyen de "on".

Exemples : 
On voit que le passif non agentif est sans doute la forme la plus complexe de passif car son emploi varie énormément d'une langue à l'autre et ses emplois permettent des énoncés parfois intraduisibles directement.

 À déplacer vers Langue ergative.
On parle d'ergativité quand, dans une diathèse active, le sujet grammatical est le patient sémantique ; c'est le cas de phrases comme : 

Ce qui prouve que l'on a affaire à des sujets patients est la possibilité de changer la diathèse du verbe sans inverser les actants : "la branche casse" et "la branche est cassée" sont sémantiquement identiques (au contraire de "bébé mange" et "bébé est mangé"). On peut obtenir le même genre de constructions avec des verbes comme "pendre" (« la montre pend à son poignet » = « ... est pendue à son poignet ») ou "planter" (« le logiciel plante encore » = « ... est encore planté »).

Dans certaines langues (langues caucasiennes, basque, inuktitut, sumérien), l'ergativité ne se limite pas à quelques constructions mais fait partie intégrale du système de la langue.

En effectuant des tests statistiques sur des groupes de sujets, on a pu mettre en évidence des préférences pour la forme active ou passive, selon le cas, au sein de couples de phrases en français, et en inférer les motivations plus ou moins conscientes.

Voici une synthèse des exemples et explications proposés par Jean Costermans dans son ouvrage "Psychologie du langage" :
Il existe des langues qui ne permettent pas le choix entre plusieurs voix. La diathèse est alors une caractéristique lexicale des verbes, fixée une fois pour toutes et qui ne peut être modifiée grammaticalement. Dans ces langues, il n'est donc pas possible de transformer n'importe quelle phrase active en phrase passive : il faut qu'il existe au préalable dans le vocabulaire un couple de verbes dont l'un aura le sens actif, l'autre le sens passif, sans qu'il existe de moyen systématique de dériver l'un de l'autre.

C'est par exemple le cas en créole guadeloupéen, qui ignore généralement les distinctions de voix. Il y existe cependant quelques couples de verbes qui s'opposent par la diathèse, comme "pri" « être pris » de sens passif par opposition à "pwann" « prendre » de sens actif. On peut alors dire : "an ka pwann biten-lasa" « je prends cette chose » ~ "biten-lasa ka pri" « cette chose est prise ». En revanche, on ne peut rendre passif un énoncé comme "an ka vwè biten-lasa" « je vois cette chose » parce qu'il n'existe pas de verbe signifiant « être vu ».


 


</doc>
<doc id="19241" url="https://fr.wikipedia.org/wiki?curid=19241" title="Seine-et-Oise">
Seine-et-Oise

La Seine-et-Oise ( ) est un ancien département français créé en 1790 et dissous en 1968, qui était identifié par le code « 78 », ensuite attribué aux Yvelines.

La Seine-et-Oise fut l'un des 83 départements créés à la Révolution française, le en application de la loi du , à partir d'une partie de la province d'Île-de-France.

Son chef-lieu était Versailles, et il eut pour sous-préfectures Corbeil (devenu Corbeil-Essonnes, remplacée en 1966 par celle d'Évry ), Étampes (jusqu'en 1926), Mantes (devenu Mantes-la-Jolie) (sauf entre 1926 et 1943), Pontoise, Rambouillet (à partir de 1812), et, après 1962, Montmorency, Palaiseau, Le Raincy et Saint-Germain-en-Laye. Deux autres arrondissements s'ajoutèrent en 1966, Argenteuil et Étampes, pour préparer les nouveaux départements. Ainsi, au moment de sa suppression, le département comptait 11 arrondissements, 68 cantons et 688 communes (il y a aujourd'hui 686 communes dans ses limites).

Sa population s'élevait à habitants en 1962 (dernier recensement avant sa suppression), et dépasserait aujourd'hui 4,5 millions d'habitants. Sa superficie était environ 5 .

Il était entouré par les départements de l'Oise au nord, de Seine-et-Marne à l'est, du Loiret au sud, d'Eure-et-Loir et de l'Eure à l'ouest. De plus, le département de la Seine, qui comprenait Paris et sa proche banlieue, était entièrement enclavé en Seine-et-Oise.

Il a été supprimé le 1968 (tout comme le département de la Seine qu'il entourait totalement), en application de la loi du 10 juillet 1964 portant réorganisation de la région parisienne, et a constitué les départements de l'Essonne, du Val-d'Oise et des Yvelines. Quelques-unes de ses communes ont également formé des parties des Hauts-de-Seine (9 communes), de la Seine-Saint-Denis (16 communes) et du Val-de-Marne (18 communes). La réorganisation de la région parisienne en 1964 aboutit à démembrer le Grand Paris pour 3 raisons principales 
D'après Abel Hugo, vers 1835, le langage des habitants de Seine-et-Oise ne différait de celui des Parisiens que dans les campagnes, où le peuple avait naturellement un vocabulaire varié et des locutions qui tenaient à son état, au genre de ses occupations agricoles et industrielles, inconnues à celui de la capitale.

Par suite de l'ancien séjour de la cour à Versailles, les habitants de cette ville et des environs avaient un langage plus riche, plus figuré et plus nuancé d'expressions recherchées que ceux des autres arrondissements du département.


Les arrondissements de Seine-et-Oise étaient composés des cantons suivants :

Le Val-d'Oise et les Yvelines partagent la Chambre de Commerce et d'Industrie de Versailles.




</doc>
<doc id="19242" url="https://fr.wikipedia.org/wiki?curid=19242" title="The Elder Scrolls III: Morrowind">
The Elder Scrolls III: Morrowind

Le joueur dirige un personnage dans la contrée de Morrowind, et plus précisément sur l'île de Vvardenfell (). Son personnage évoluera grâce aux centaines de quêtes que lui proposent les PNJ. La quête principale peut être abandonnée ou reprise à n'importe quel moment ; elle doit être considérée plus comme un fil conducteur narratif que comme le « but du jeu ». L'univers est assez dense pour permettre — tout comme ses prédécesseurs — de se promener longuement dans l'île.

Le jeu se déroule sur l'île de Vvardenfell, appartenant au royaume de Morrowind. Morrowind a un roi, mais il est faible. Chaque région est contrôlée par sa baronnie locale : maison Hlaalu pour le Sud-Ouest avec la ville de Balmora, maison Rédoran pour le Nord-Ouest avec Ald'rhun, maison Telvani pour le Nord-Est avec la ville Sadrith Mora, elle-même divisée en fiefs, le Sud-Est est désert et ne semble contrôlé par personne.

Ces maisons divisées font face à un ennemi beaucoup plus puissant qu'elles, la maison Dagoth, dont le siège est au cœur du volcan du Mont Écarlate. Une barrière a été érigée pour empêcher les monstres de Dagoth de tout envahir, elle s'appelle le Rempart Intangible.

Morrowind est un territoire peuplé de créatures fantastiques, des étranges chiens de Nix aux fourmis géantes que sont les Kwamas en passant par les robots conçus par les Dwemer disparus et les fantômes des ancêtres. L'ambiance est beaucoup plus exotique que dans "", on y trouve des tribus du désert, des architectures folles et biscornues, des références à l'extrême Orient et des maisons faites dans des champignons géants.

L'un des atouts majeurs de Morrowind réside dans son univers riche et varié. Il existe en effet une multitude de personnages et d'objets de la vie quotidienne (ustensiles de cuisine, outils, vêtements, etc.), ce qui donne au jeu de grandes capacités d'interactivité. On trouve également de multiples armes blanches et pièces d'armures de toutes sortes. L'univers en lui-même, son histoire, les diverses tensions sociales etc sont considérés par les fans comme rendant le jeu extrêmement cohérent et réaliste. Ce même point peut assujettir le jeu à une aversion de la part d'autres joueurs car cela demande notamment une attention particulière aux nombreux textes contenus dans le jeu (dans des livres ou des dialogues) qui se détachent nécessairement de l'aspect dynamique (lui aussi considéré comme lacunaire) du jeu.

L'écoulement du temps dans le jeu se fait sans interruption, la nuit venant après le jour. Le décompte du temps se fait selon un calendrier précis. Ajoutons aussi que le climat est sujet à quelques variations (tempête de cendres (c'est une île volcanique), pluie, neige, etc.) qui dépendent parfois de la région où l'on se trouve. Morrowind étant une région plutôt nordique par-rapport aux autres pays du continent, la végétation y est assez pauvre, ce qui est un des nombreux éléments qui rendent le jeu particulièrement triste, voire ouvertement mélancolique.

En effet, le jeu est également partiellement axé sur la contemplation. L'ambiguïté morale de nombreuses quêtes, qui, sans verser directement dans l'immoralité, amène parfois à s'interroger sur le bien-fondé des actions qui nous sont demandées, participe à une indétermination générale quant à l'aboutissement de ce que nous faisons (par exemple, l'ennemi désigné, Dagoth Ur, est-il au fond plus mauvais que le dieu Vivec, chef du peuple colonisé, que l'Empereur, chef du pays colonisateur, ou que la déesse Azura de qui l'on est, fatalement, le bras armé ?). À cela s'ajoutent des problèmes socioéconomiques inhérents au jeu (racisme, pauvreté, corruption). La poésie pratiquée à travers les noms de lieux ou dans les livres est souvent énigmatique et mélancolique finalement, rendant l'esthétique du jeu définitivement triste, doloriste, terne et mystérieuse (Longsanglot (Mournhold en anglais), Côte de la mélancolie, Terres-Cendres ou encore Morrowind (littéralement "le vent du lendemain")).

Alors que "" se déroulait sur l'ensemble du monde de Tamriel, et que "" ne prenait déjà plus place que dans deux ou trois provinces de ce monde, Morrowind ne se déroule même pas dans une province entière. Il s'agit d'ailleurs de la province de Morrowind, patrie des Elfes Noirs, et plus particulièrement l'île volcanique de Vvardenfell. Le jeu démarre à Seyda Nihyn, au sud de l'île, dans une région marécageuse et pluvieuse. Ce type de climat perdure quasiment sur tout le pourtour de l'île, le nord étant aussi composé d'archipels de minuscules rochers. Le centre de l'île est très montagneux et abrite un volcan, le Mont Écarlate. La partie sud-ouest de l'île est carbonisée par la lave, avec des tempêtes de cendres. L'est est principalement composé de plaines. Si Vvardenfell est principalement habitée par les Elfes noirs, toutes les espèces de Tamriel sont cependant représentées en raison de la colonisation impériale.

Terre d'origine des Dunmers, les elfes noirs, Morrowind est un territoire peuplé à cinquante pour cent de Dunmers de souche et à cinquante pour cent d'étrangers. Le racisme y est très fort, le joueur, un immigré, le ressentira au début du jeu, surtout s'il est un Khajiit ou un Argonien, mais ses actions peuvent petit à petit augmenter sa popularité.

Le joueur commence en débarquant d'un bateau, libéré de prison sous ordre de l'empereur de Tamriel. Petit à petit, il comprend avec Caius Cosadès, à force de faire des rapports, que l'Empereur a vu en lui le héros, le messie attendu par les Cendrais, une tribu de Morrowind, pour unir toutes les maisons contre un ennemi commun, la maison Dagoth, et ainsi sauver Morrowind de la menace terrible qui pèse sur elle.

Dagoth Ur, le chef de la maison Dagoth, tient son pouvoir et sa longévité du Cœur de Lorkhan, le cœur d'un dieu découvert par les Nains dans une mine. Ce pouvoir l'a rendu plus fort mais l'a aussi corrompu et rendu fou. Il est impossible de battre Dagoth Ur directement, il faut détruire le cœur pour le faire disparaitre. Tantôt qualifié de dieu, tantôt qualifié de demi-dieu, Dagoth Ur n'a jamais pu être vaincu par les Tribuns, trois personnages au nom de Vivec, d'Almalexia et de Sotha Sil, qui n'ont puisé qu'une petite partie du pouvoir du cœur.

Le messie attendu, le Nérévarine, est la réincarnation d'un héros ancien Nérévar. Le joueur comprend au fil du jeu qu'il n'est autre que le Nérévarine. Son acquisition de l'anneau Astre-Lune, tuant tous ceux qui le portent sauf l'élu, tait beaucoup de doutes, et son but est d'unir toutes les tribus Cendrais et toutes les maisons contre la Maison Dagoth. Une fois les accords (souvent des signatures) obtenus, le Nérévarine parlement avec le demi-dieu Vivec et part à l'assaut du Mont Écarlate avec le gant Garde Spectrale pour tuer Dagoth Ur une bonne fois pour toutes. Le combat final se déroule dans une grotte avec le corps du dieu Akulakhan dont on doit détruire le cœur en évitant les attaques de Dagoth Ur.

L'originalité de "Morrowind" réside en la liberté d'action dont dispose le joueur. Il peut aussi bien effectuer des quêtes qu'explorer le territoire (tout ce qui est visible peut être physiquement parcouru) ou accomplir un objectif qu'il s'est lui-même fixé (sauter par-dessus une montagne par exemple, même cela est possible). Cette grande liberté est appréciée, selon les joueurs, comme un avantage ou un défaut. Le tout se passe dans un monde d'Heroic fantasy (ou Médiéval-Merveilleux) mêlé à quelques avancées technologiques (Steampunk).

La création du personnage se fait selon 5 caractéristiques principales, 5 caractéristiques secondaires et 17 compétences diverses. Ces compétences représentent les capacités dans un domaine précis (ex : la « sécurité » qui reflète les capacités du personnage à crocheter des serrures). La maîtrise de ces compétences est acquise presque exclusivement par la pratique, ce qui ajoute au réalisme du jeu.

Toutes les musiques de "Morrowind" ont été composées par Jeremy Soule. Le thème principal est épique, mais les autres sont plus sombres et mélancoliques. La bande-son est relativement courte (environ quarante minutes).

Concernant les voix, il y en a peu dans le jeu, beaucoup de dialogues étant simplement écrits, et non parlés. Dans la version originale, Lynda Carter (l'interprète de Wonder Woman à la télévision) prête sa voix aux personnages nordiques féminins. Dans la version française, le doubleur des Orques masculins est Christian Pelissier, voix déjà du Capitaine Haddock dans le dessin animé "Les Aventures de Tintin". Quant à celle des Elfes Noirs masculins, il s'agit de la voix de Thierry Mercier, connu entre autres pour le doublage de Teal'c dans la série "Stargate SG-1".


La commercialisation de Morrowind sur le territoire français est passée par deux phases : une version semi-localisée (manuel et boite en français, mais jeu en anglais), compatible avec les mods de la version anglaise, a d'abord été mise en vente. La version française intégrale (compatible avec les mods réalisés avec cette version uniquement) a suivi quelques semaines plus tard.

La version française intégrale étant majoritaire, de nombreux sites de mods pour Morrowind comme Wiwiland travaillent à modifier les mods anglophones pour les rendre compatibles avec la version française. Ce processus a été appelé par ses utilisateurs « la camembérisation ».

Le jeu est sorti en novembre 2002 sur Xbox, directement sorti en version française intégrale en France, par Ubisoft. Contrairement à la version PC, le jeu connaît quelques baisses constantes de frame-rate, des chargements assez longs selon l'avancement de votre partie (une durée moyenne de 2 minutes 15), et un clipping et une distance d'affichage réduits.

Cependant, peu de différences dans la forme substituent, seuls quelques bugs et erreurs de traduction ont été corrigés.

Le jeu est ressorti en "Game of the Year Edition" en février 2004, avec l'ajout des extensions et . La première version GOTY du jeu est connue pour être impossible à terminer, en raison de trois bugs principaux :

De ce fait, Ubisoft subit à une vaste polémique et ce n'est qu'en mai 2004, après de nombreux mails et pétitions, que l'entreprise réagit. La première version GOTY sur Xbox fut ainsi retirée de la vente officiellement, bien que déjà certains magasins avaient déjà pratiqué de nombreux retours environ deux semaines après sa sortie. La société proposa un échange gratuit des exemplaires, avec la correction de la quête principale, et une très sensible retouche technique. Cette seconde version n'a jamais été commercialisée et n'est disponible que par le Service technique Ubisoft, elle est donc extrêmement rare et a été produite à très peu d'exemplaires.

Elle est reconnaissable grâce à son étiquette rajoutée sur le dos de la jaquette. Cette distinction étant discrète et facile à omettre, il n'est cependant pas évident de trouver une véritable version "débuggée", surtout qu'il subsiste une part d'évènements aléatoires même avec cette dernière.

Ce type de bugs peut également se rencontrer sur Windows dans sa version "GOTY", cependant l'architecture PC a permis de vite corriger ce problème, avec le recours de patchs et de mods, officiels et amateurs.

"" est un jeu évolutif, à savoir que le joueur peut importer des plugiciels aux effets les plus variés (ajout de contrées nouvelles, modification des règles, etc.). Ces plugiciels ou mods sont réalisés par les joueurs eux-mêmes et modifient grandement l'immersion dans le monde du jeu, conférant ainsi à Morrowind une durée de vie quasi-illimitée. L'outil de création de plugiciel fourni avec le jeu, "The Elder Scrolls Construction Set", est l'outil utilisé par les développeurs pour réaliser l'immense monde du jeu. L'ajout de plugins nécessite cependant de modifier le plus souvent manuellement les fichiers du jeu, ce qui comporte un risque de voir le jeu partiellement ou entièrement buggé ou dans l'incapacité de démarrer.

"" a été enrichi par deux extensions — officielles : "" et "".

OpenMW est un projet ayant pour but de créer une implémentation libre sous licence GPL V3 de "". Il faudra toutefois utiliser les données du jeu original.

Cette implémentation a pour projet d'être compatible Windows, GNU/Linux et Mac OS X, elle utilise le moteur OpenSceneGraph et le langage C++. Elle doit aussi être compatible avec les extensions officielles ainsi que les mods de la communauté.

Le projet se contente pour l'instant de réimplémenter le jeu, une fois cela fait, des améliorations seront envisageables.

En septembre 2017, le site français jeuxvideo.com le classe meilleur jeu de tous les temps.


</doc>
<doc id="19243" url="https://fr.wikipedia.org/wiki?curid=19243" title="Équation de Schrödinger">
Équation de Schrödinger

L'équation de Schrödinger, conçue par le physicien autrichien Erwin Schrödinger en 1925, est une équation fondamentale en mécanique quantique. Elle décrit l'évolution dans le temps d'une particule massive non relativiste, et remplit ainsi le même rôle que la relation fondamentale de la dynamique en mécanique classique.

Au début du , il était devenu clair que la lumière présente une dualité onde-corpuscule, c'est-à-dire qu'elle pouvait se manifester, selon les circonstances, soit comme une particule, le photon, soit comme une onde électromagnétique. Louis de Broglie proposa de généraliser cette dualité à toutes les particules connues bien que cette hypothèse eût pour conséquence paradoxale la production d'interférences par les électrons — à l'instar de la lumière — ce qui fut vérifié ultérieurement par l'expérience de Davisson-Germer. Par analogie avec le photon, Louis de Broglie associa ainsi à chaque particule libre d'énergie formula_1 et de quantité de mouvement formula_2 une fréquence formula_3 et une longueur d'onde formula_4 :
formula_5

L'équation de Schrödinger, établie par le physicien Erwin Schrödinger en 1925, est une équation fonctionnelle dont l'inconnue est une fonction, la fonction d'onde, ce qui généralise l'approche de de Broglie ci-dessus aux particules massives non relativistes soumises à une force dérivant d'une énergie potentielle, dont l'énergie mécanique totale est classiquement :
formula_6
Le succès de l'équation, déduite de cette extension par utilisation du principe de correspondance, fut immédiat quant à l'évaluation des niveaux quantifiés d'énergie de l'électron dans l'atome d'hydrogène, car elle permit d'expliquer les raies d'émission de l'hydrogène : séries de Lyman, Balmer, Brackett, Paschen, etc.

L'interprétation physique communément admise de la fonction d'onde de Schrödinger ne fut donnée qu'en 1926 par Max Born. En raison du caractère probabiliste qu'elle introduisait, la mécanique ondulatoire de Schrödinger suscita initialement de la méfiance chez quelques physiciens de renom comme Albert Einstein, pour qui .

Le schéma conceptuel utilisé par Schrödinger pour dériver son équation repose sur une analogie formelle entre l'optique et la mécanique :



Ce parallèle avait été noté dès 1834 par Hamilton, mais celui-ci n'avait alors pas de raison de douter de la validité de la mécanique classique. Après l'hypothèse de De Broglie de 1923, Schrödinger s'est dit : l'équation de l'eikonale étant une approximation de l'équation d'onde de l'optique physique, cherchons l'équation d'onde de la "mécanique ondulatoire" (à construire) dont l'approximation soit l'équation de Hamilton-Jacobi. Ce qu'il a fait, d'abord pour une onde stationnaire ("E" = cte), puis pour une onde quelconque.

Remarque : Schrödinger avait en fait commencé par traiter le cas d'une particule relativiste - comme d'ailleurs de Broglie avant lui. Il a alors obtenu l'équation connue aujourd'hui sous le nom de Klein-Gordon, mais son application au cas du potentiel coulombien donnant des niveaux d'énergie incompatibles avec les résultats expérimentaux de l'atome d'hydrogène, il se serait rabattu sur le cas non relativiste, avec le succès que l'on connaît.

En mécanique quantique, l'état à l'instant "t" d'un système est décrit par un élément formula_7 de l'espace complexe de Hilbert — avec l’utilisation de la notation bra-ket de Paul Dirac. Le carré de formula_7 représente les densités de probabilités de résultats de toutes les mesures possibles d'un système.

L'évolution temporelle de formula_7 est décrite par l'équation de Schrödinger :
où

Contrairement aux équations de Maxwell gérant l'évolution des ondes électromagnétiques, l'équation de Schrödinger est non relativiste. Cette équation est un postulat. Elle a été supposée correcte après que Davisson et Germer eurent confirmé expérimentalement l'hypothèse de Louis de Broglie.

L'équation de Schrödinger étant une équation vectorielle on peut la réécrire de façon équivalente dans une base particulière de l'espace des états. Si on choisit par exemple la base formula_17 correspondant à la représentation de position définie par

formula_18

alors la fonction d'onde formula_19 satisfait à l'équation suivante

formula_20

où formula_21 est le laplacien scalaire. En effet l'observable position formula_22 ne dépend pas du temps, donc ses états propres n'en dépendent pas non plus : formula_23.

Sous cette forme on voit que l'équation de Schrödinger est une équation aux dérivées partielles faisant intervenir des opérateurs linéaires, ce qui permet d'écrire la solution générique comme la somme des solutions particulières. L'équation est dans la grande majorité des cas trop compliquée pour admettre une solution analytique de sorte que sa résolution est approchée ou numérique.

Les opérateurs apparaissant dans l'équation de Schrödinger sont des opérateurs linéaires ; il s'ensuit que toute combinaison linéaire de solutions est solution de l'équation. Cela mène à favoriser la recherche de solutions qui ont un grand intérêt théorique et pratique : à savoir les états qui sont propres de l'opérateur hamiltonien.

Ces états sont donc solutions de l'équation aux états et valeurs propres,
formula_24
qui porte parfois le nom d’équation de Schrödinger indépendante du temps. L'état propre formula_25 est associé à la valeur propre formula_26 , scalaire réel, énergie de la particule dont formula_25 est l'état.

Les valeurs de l'énergie peuvent être discrètes comme les solutions liées d'un puits de potentiel (par ex. niveaux de l'atome d'hydrogène) ; il en résulte une quantification des niveaux d'énergie. Elles peuvent aussi correspondre à un spectre continu comme les solutions libres d'un puits de potentiel (par ex. un électron ayant assez d'énergie pour s'éloigner à l'infini du noyau de l'atome d'hydrogène).

Il arrive souvent que plusieurs états formula_25 correspondent à une même valeur de l'énergie : on parle alors de niveaux d'énergie dégénérés.

D'une façon générale, la détermination de chacun des états propres de l'hamiltonien, formula_25, et de l'énergie associée, fournit l'état stationnaire correspondant, solution de l'équation de Schrödinger :
formula_30
Une solution de l'équation de Schrödinger peut alors s'écrire très généralement comme une combinaison linéaire de tels états :
formula_31

Selon les postulats de la mécanique quantique,
L'espace des fonctions d'onde c'est un espace de Hilbert

La recherche des états propres de l'hamiltonien est en général complexe. Même le cas analytiquement soluble de l'atome d'hydrogène ne l'est rigoureusement sous forme simple que si l'on néglige le couplage avec le champ électromagnétique qui va permettre le passage des états excités, solutions de l'équation de Schrödinger de l'atome, vers le fondamental.

Certains modèles simples, bien que non tout à fait conformes à la réalité, peuvent être résolus analytiquement et s'avèrent très utiles :

Dans les autres cas, il faut faire appel aux diverses techniques d'approximation :

La généralisation de l'équation au domaine relativiste mena à l'équation de Klein-Gordon, puis à l'équation de Dirac ; cette dernière établit naturellement l'existence du spin et des antiparticules. Cependant, il n'existe aucune interprétation entièrement cohérente de ces équations d'ondes relativistes dans le cadre d'une théorie décrivant une seule particule ; le cadre pertinent pour le théorique quantique relativiste est la théorie quantique des champs.

Il existe d'autres équations de type Schrödinger, non-linéaires, comme l'Équation de Schrödinger semi-linéaire, ou comme l'Équation de Gross-Pitaevskii, qui interviennent en théorie des atomes ultra froids, des plasmas, des lasers, etc.




</doc>
<doc id="19247" url="https://fr.wikipedia.org/wiki?curid=19247" title="Militantisme">
Militantisme

Le militantisme est une forme d'engagement actif à une cause de nature politique, associative ou syndicale défendant une idéologie. Le but étant de faire valoir ce soutien à une plus grande échelle.

Historiquement, l'usage du mot militant dans un sens moral et religieux se trouvait dans l'expression « Église militante », qui désignait les fidèles sur terre, par opposition à l'Église triomphante (au ciel), et à l'Église souffrante (au purgatoire). Cette notion est toutefois rarement employée dans le catholicisme aujourd'hui, les formes de militantisme ayant beaucoup évolué du fait de la sécularisation, et se retrouvant aujourd'hui dans la société civile d'une manière éclatée.

Le « militantisme moral » est aujourd'hui en plein essor, fondé sur d'autres solidarités que les solidarités de classe, spécialisé dans des causes telles que l'antiracisme, l'humanitaire, la défense des droits de l'homme, la lutte des classes, la lutte contre le SIDA ou la défense de l'environnement, du droit des consommateurs, du développement durable, etc.





</doc>
<doc id="19248" url="https://fr.wikipedia.org/wiki?curid=19248" title="Axelle Red">
Axelle Red

Fabienne Demal, dite Axelle Red, née le à Hasselt dans la province du Limbourg en Belgique, est une auteur-compositrice-interprète belge.

De langue maternelle néerlandaise, elle écrit et chante essentiellement en français et en anglais, mais aussi en espagnol.

Elle a vendu plus de cinq millions de disques dans le monde.

Son père, Roland Demal, est avocat et échevin à Hasselt pour le parti libéral flamand Open-VLD.

En 1993, elle effectue des études de droit à la Vrije Universiteit Brussel.

En 1983, âgée de 14 ans, Fabienne Demal sort son premier 45 tours, "Little girls", qu'elle signe du diminutif « Fabby ». En face B se trouve la version instrumentale du même titre, on peut voir sur la pochette aux couleurs rose saumon la jeune artiste, assise par terre, contemplant d'un air triste, une poupée de chiffons. Ce single lui permet de se faire connaître chez elle, en Belgique ; même s'il ne rencontre qu'un succès d'estime, il entre tout de même au hit parade belge. Deux ans plus tard, en 1985, sort son deuxième single : intitulé "Back to Tokyo", il s’agit d’une chanson d’amour du style populaire courant dans les années 1980, avec en face B "Dancing all over the world". Sur la pochette de ce deuxième disque, on peut voir Axelle Red allongée par terre, entourée de petites peluches, et serrant dans ses bras un ourson. Un clip vidéo, tourné à la terrasse d'un café, dans une gare et un aéroport, fut réalisé à très faible budget pour le titre "Back to Tokyo".

Elle se fait connaître en Belgique francophone dès 1989 avec son single "Kennedy Boulevard", toujours sous le nom d'Axelle, suivi de "Aretha et moi" et "Elle danse seule".

Après l'obtention de son Diplôme en Droit à la Vrije Universiteit Brussel, elle signe avec le label Virgin Benelux et sort en 1993 son premier album, "Sans plus attendre", composé de douze titres, sous le pseudonyme d’Axelle Red. Les singles "Sensualité", "Je t'attends" et "Le Monde tourne mal" permettent au disque de rencontrer le succès en France, en Suisse, au Canada et en Belgique. L'album s'écoule au total à plus de exemplaires.

L’album suivant, "À tâtons", est enregistré à Memphis en 1996 avec Steve Cropper et Isaac Hayes ; il est commercialisé le 21 octobre de cette année-là. Grâce à des titres comme "Ma prière", "À quoi ça sert ?" et "Rester femme", l'album reçoit un « platinum award » pour plus d'un million d'exemplaires vendus en Europe, dont plus de en France. Un an plus tard, elle foule pour la première fois la scène de l'Olympia à Paris.

Depuis 1997, elle est ambassadrice du Fonds des Nations unies pour l'enfance, soutenant les droits des femmes et des enfants dans les régions en guerre et les pays en voie de développement. La même année, elle entame un combat contre les mines antipersonnel lors de la convention d'Ottawa et rencontre dans un Haïti miséreux des enfants mis en prison dans des conditions déplorables sans le moindre procès. Le titre "À quoi ça sert" est choisi par Alain Corneau pour le film "Le Cousin".

En 1998, après s'être mariée avec son producteur/manager Philip Vanes, elle chante "La Cour des grands" avec Youssou N'Dour au Stade de France, pour la cérémonie d'ouverture de la Coupe du monde de football.

Enceinte de sept mois de sa première fille, Janelle, elle s'entoure pour son premier spectacle soul et rhythm and blues ("The Soul of Axelle Red" : deux concerts au Sportpaleis d'Anvers et au Palais des Congrès de Paris) d'artistes renommés tels que Wilson Pickett, Sam Moore, Eddie Floyd, Percy Sledge et Ann Peebles. La même année, elle s'engage en faveur du concert de soutien à Amnesty International à Paris et publie un album espagnol, "Con solo pensarlo".

Le 21 janvier 1999, Axelle Red donne naissance à sa première fille, Janelle Vanes. Elle reçoit la Victoire de la musique en tant qu'artiste féminine de l'année, et sort son troisième album, qu'elle écrit et produit. "Toujours moi" confirme son succès avec plus de vendus dans le monde, dont en France. Elle repart ensuite en Asie, où elle a beaucoup voyagé lors de ses études. Au Laos, l'un des pays d'Asie les plus pauvres, mais aussi en Thaïlande et au Cambodge, elle rencontre des femmes et des enfants soumis à la violence, à la discrimination, à la prostitution et à la pauvreté. Après plus de 400 concerts en 2000, un premier disque live et un DVD sont enregistrés lors de différentes étapes de sa tournée française. Ce disque, "Alive" est certifié disque d'or en France et en Belgique.

En 2002, elle interprète en duo avec Renaud le single "Manhattan-Kaboul". Cette chanson, qui s'écoule à plus de , est le morceau le plus diffusé de l'année en France, et reçoit un NRJ Music Award. 

Quelques mois plus tard, en novembre, sort son quatrième album, "Face A/Face B", une coproduction avec Al Stone. Conséquence de ses nombreux voyages dans des pays défavorisés, cet album est plus engagé : extrémisme, anti-mondialisme, mines antipersonnel, enfants soldats, drogues… L'album déstabilise un peu le public mais atteint les exemplaires vendus et est certifié disque de platine en Belgique.

En 2003, le 24 juin, elle donne naissance à sa deuxième fille, Gloria Vanes. Un coffret contenant notamment des inédits et des duos avec Charles Aznavour, Francis Cabrel, Stephan Eicher, Sylvie Vartan, Arno et Tom Barman est commercialisé. Après avoir dû fuir les troubles au Congo en juin 2004, elle mène le mois suivant campagne au Niger avec l'Unicef contre l'excision des femmes et les mariages d'enfants.

"French Soul", sa première compilation, paraît avec les inédits "Je pense à toi" et "J'ai fait un rêve", dont elle réalise elle-même les clips. Elle termine l'année par une visite éclair au Sri Lanka pour apporter l'aide Unicef aux populations touchées par le tsunami. En 2005, elle part au nord du Sénégal pour la campagne "Make noise till Hong Kong" de l'organisation française Oxfam/Agir Ici, où elle plaide pour un commerce équitable. En mai, elle participe à Genève avec Peter Gabriel et Youssou N'Dour au concert pour le soixantième anniversaire des Nations unies. Kofi Annan y remercie la chanteuse pour son engagement humanitaire auprès de diverses ONG. C'est cette même année qu'elle devient maman pour la troisième fois, en donnant naissance le 15 avril à sa fille Billie Vanes. Aux côtés de Bob Geldof, elle se fait porte-parole de "Live 8 in France" et se produit le devant personnes lors du concert de soutien au château de Versailles. Durant le sommet européen, elle demande officiellement au président de la Commission européenne, Manuel Barroso, d'augmenter le budget consacré au développement des pays du tiers-monde.

Le 6 septembre 2006, Axelle Red est faite chevalier de l’ordre des Arts et des Lettres par le Ministre de la Culture Français. Elle participe aux Concerts 0110 contre l'intolérance et le racisme à Anvers et Bruxelles. Le 2 octobre 2006 paraît "Jardin secret", son cinquième album. Enregistré dans les "Royal Studios" de Willie Mitchell à Memphis, il traite d'espoir, d'optimisme et de positivisme. Alors que l'opus connaît un succès en Belgique avec un triple disque de platine, il ne rencontre qu'un succès d'estime en France où il est malgré tout certifié disque d'or avec copies écoulées.

En 2007, elle se rend pour la campagne Unicef "Together, saving 4 million babies" en Sierra Leone, cinq ans après la guerre civile. En mars, accompagnée du réalisateur cambodgien Rithy Panh, elle prend la parole au Festival du film et forum international sur les droits humains de Genève lors d'un débat consacré à la prostitution. 

En , le roi Albert II de Belgique lui remet la distinction honorifique de commandeur de l'ordre de la Couronne pour son engagement social.

En , l'Université de Hasselt la fait docteur "honoris causa" pour son engagement social en tant qu'artiste et militante des droits de l'homme.
À l'occasion de la journée internationale des femmes, elle est orateur invitée au Conseil de l'Europe, lors d'un débat consacré à la violence domestique. En 2009, elle écrit son premier album en anglais, "Sisters and Empathy" qui sort le 19 janvier. L'album est certifié disque d'or en Belgique.

En 2011, paraît un nouvel album, "Un cœur comme le mien", porté par le single "La claque". Il est certifié disque d'or en Belgique. Après deux concerts en mai à Paris et à Bruxelles, une tournée est prévue, d'octobre 2011 à l'été 2012.

"Rouge Ardent" marque un retour à la soul pop. Présents sur l’album précédent, Gérard Manset, Christophe Miossec et Stephan Eicher contribuent, tout comme Albert Hammond, avec lequel elle avait cocomposé. L’album est dès sa sortie en Belgique et disque d’or.

En parallèle à son actualité musicale, une exposition "Axelle Red Fashion Victim" est présentée jusqu’au 2 juin 2013 au Modemuseum de sa ville natale, Hasselt.

Elle intègre le jury de la troisième saison de "The Voice van Vlaanderen" diffusée de janvier à mai 2014 sur VTM.

Le 30 octobre 2015 paraît "The Songs (Acoustic)", une compilation acoustique de reprises et de quelques titres originaux.

 










</doc>
<doc id="19249" url="https://fr.wikipedia.org/wiki?curid=19249" title="Moyenne arithmétique">
Moyenne arithmétique

En mathématiques, la moyenne arithmétique d'une série de nombres réels est la somme des valeurs divisée par le nombre de valeurs. C'est ce qu'on appelle la "moyenne" en langage ordinaire.

On note généralement la moyenne par le diacritique macron, caractère unicode u+0304, par exemple la moyenne des valeurs de formula_1 est notée formula_2.

Sa formulation mathématique peut se faire comme suit :
formula_3

Par exemple, dans une entreprise de trois employés ayant un salaire de , et , le salaire moyen est
formula_4

Pour une série de valeurs dont le nombre total d’occurrences est inconnu, mais dont les fréquences sont connues pour chaque valeur possible de la série, la formulation mathématique devient :
formula_5

Parfois on souhaite attacher aux valeurs des poids différents (par exemple des coefficients pour les épreuves d'un concours). Dans ce cas à la série de valeurs
formula_6 on associe une série de coefficients ou poids formula_7. La moyenne arithmétique des valeurs
formula_6 pondérée par les poids formula_7 est donnée par
formula_10

L'espérance mathématique d'une variable aléatoire discrète est la moyenne arithmétique de ses valeurs possibles pondérée par leur probabilité de réalisation.

Le terme moyenne empirique est utilisé en statistiques pour parler de la moyenne arithmétique d'un échantillon. La loi des grands nombres affirme que, sous des hypothèses
assez faibles, la moyenne empirique d'un échantillon tend vers l'espérance de la variable aléatoire dont il est issu.

La médiane d'une série de nombres réels est une valeur "m" qui permet de couper l'ensemble des valeurs en deux parties égales :
mettant d'un côté une moitié des valeurs, qui sont toutes inférieures ou égales à "m" et de l'autre côté l'autre moitié des valeurs, qui sont toutes supérieures ou égales à "m".

La médiane est en général différente de la moyenne arithmétique, et moins affectée par les valeurs extrêmes. Par exemple considérons la série {1 ; 1 ; 2 ; 6}.
La médiane est 1,5 alors que la moyenne est 2,5.

Plus généralement considérons une série de valeurs réelles formula_6. Alors la médiane est la valeur "m" qui minimise la somme des valeurs absolues des écarts, i.e.
formula_12
alors que la moyenne arithmétique est la valeur formula_13 qui minimise la somme des carrés des écarts, i.e.
formula_14




</doc>
<doc id="19250" url="https://fr.wikipedia.org/wiki?curid=19250" title="Schaerbeek">
Schaerbeek

Schaerbeek (néerlandais : "Schaarbeek") est l'une des 19 communes bilingues de Bruxelles-Capitale en Belgique.

Elle comptait, au , habitants (Schaerbeekois), hommes et femmes, pour une superficie de , soit . Elle est située dans le nord-est de la Région bruxelloise.

Schaerbeek est composée de nombreux quartiers souvent très populaires et cosmopolites. Elle compte quelques sites remarquables comme le parc Josaphat, l'hôtel communal construit en 1887 par Jules-Jacques Van Ysendyck, l'église royale Sainte-Marie, les Halles, la Maison des Arts, ainsi que de nombreuses maisons art nouveau et art déco particulièrement bien préservées (exemple : la Maison Autrique). C'est également la ville natale de Jacques Brel.

Elle est limitrophe des communes de Bruxelles-ville, Saint-Josse-ten-Noode, Evere, Etterbeek et Woluwe-Saint-Lambert.
Longtemps elle a été la seule commune de la région de Bruxelles à ne pas posséder d'armoiries.
C'est pourquoi elle était représentée par les couleurs de son drapeau « Blanc vert », mais finalement la commune s'est dotée d'un blason.

Coordonnées géographiques extrêmes de Schaerbeek
Les premières activités humaines localisées à Schaerbeek ont été identifiées à l'âge de la pierre par la découvertes d'outils en silex dans la vallée du Josaphat. En outre, des vestiges romains furent mis au jour lors de travaux de terrassement au . On découvrit ainsi des vases et des poteries datant du règne d'Hadrien sous la chaussée de Haecht à l'emplacement de l'actuel lycée Emile Max ainsi que des «"fondations et des voûtes"» à proximité de l'actuelle église royale Sainte-Marie sise place de la Reine, attestant selon des auteurs du de l'existence d'un ouvrage de défense romain, une hypothèse qui n'est plus retenue. Schaerbeek était à une bifurcation de voies romaines venant de Boulogne-sur-Mer par Bavay et allant à Cologne et à Elewijt.

La première mention écrite de Schaerbeek est "Scarenbeca" en 1120 par l'évêque de Cambrai (France) dans un document où il cite aussi "Everna", sa commune voisine Evere pour l'administration et une partie des revenus des églises au chapitre de Soignies.

Au Moyen Âge, le territoire de la commune faisait partie du duché de Brabant. Schaerbeek faisait partie de la « cuve », c'est-à-dire la banlieue, de la ville de Bruxelles : le village fut annexé à la ville en 1301. Cette situation perdura jusqu'en 1795, lorsque l’administration française fit de Schaerbeek une commune. Les ducs de Bourgogne Philippe le Bon et son fils possédaient en 1425 des terrains situés entre la chaussée de Louvain et l'actuelle rue de la Consolation.

Jusqu'au début du , Schaerbeek reste une petite bourgade où la population passe de 600 habitants en 1526 à habitants en 1800 et habitants en 1850.

À partir de 1819 avec la destruction des remparts côté Botanique et le prolongement de la rue Royale vers la place de la Reine, la ville va progressivement s'urbaniser. En 1835, la création de la première voie ferrée au départ de l'allée verte, la construction de la gare du Nord (place Rogier de 1841 à 1846) puis la prolongation des voies vers Anvers vont définitivement lancer l'urbanisation dans le bas de Schaerbeek. La conjonction des prix abordables des terrains, de facilités en matières de services et de transport en commun vont favoriser le développement de la commune. En 1887, l'hôtel de ville est construit place Colignon et la population a gonflé jusqu'à habitants en 1900.

Le début du voit la création de nouvelles avenues bourgeoises : l'avenue Louis Bertrand et plus tard Paul Deschanel qui préfigurent la fracture sociale et géographique entre le haut et le bas de Schaerbeek.

Sous l'Ancien Régime, les Schaerbeekois, qui cultivaient des griottes (ils n'en ont plus, mais il reste une "rue des Cerisiers"), avaient obtenu le privilège d'aller les porter à dos d'âne au marché de Bruxelles pour les y vendre aux brasseurs qui en faisaient de la kriek. En les voyant arriver, les Bruxellois s'exclamaient: "Hè! Doe zèn die èzels van Schoerebeik" (en patois, «Tiens! Voilà les ânes de Schaerbeek»). L'anecdote se raconte encore, et certains Schaerbeekois se disent fiers d'habiter «la cité des Ânes».

Schaerbeek compte une forte communauté marocaine, des endroits sont carrément dominées par la grande présence des Marocains, notamment à Place Pavillon, à la Cage aux Ours, au Rue de Brabant ainsi qu'à la Place Liedts. D'un autre côté, on compte également une communauté turque où elle se fait présente dans tout le long de la Chaussée de Haecht. Une minorité est originaire de l'Albanie, de Pologne et de la République Démocratique du Congo. Schaerbeek est avant tout, la deuxième commune la plus peuplée de la région bruxelloise, après Bruxelles-ville, et la sixième commune de Belgique avec habitants au 1er janvier 2013. Avec une grande présence marocaine et turque à Schaerbeek, une étude démographique datant de 2010 prouve que la proportion de musulmans à Schaerbeek s'élève à 38 %<ref name="Guitterez/Hetogen Le Soir, 20101117"></ref>.

La commune possède une cité-jardin d'intérêt située dans le quartier Terdelt. Elle dispose aussi d'un "quartier des Fleurs" en bordure du parc Josaphat, agrémenté d'alignements florifères de cerisiers du Japon, couvrant les rues d'un tapis rose au printemps.

La cité des fleurs doit aussi son nom au fait qu'y était autrefois pratiquée l'horticulture et que plusieurs rues portent aujourd'hui des noms de fleurs.

Schaerbeek est composé de quartiers sociologiquement très différents :

Malgré cela le contraste entre les quartiers n'est pas si grand et d'une rue à l'autre on peut croiser différentes cultures.

Principaux quartiers et "places" :

Voir aussi Liste des rues de Schaerbeek.

Sur le plan policier, Schaerbeek est divisée en 16 quartiers, chacun doté d'un ou deux agents de quartier, répartis entre quatre commissariats (carte) : 6 quartiers pour le commissariat 1 de la rue Rodenbach, 6 pour le commissariat 3 de l'avenue de Roodebeek, 4 pour le commissariat 4 situé à Saint-Josse, rue de Bériot et 3 pour le commissariat 5, au square Victoria Regina (ex-"Tour IBM").

Voir aussi Zone de police Polbruno.

Schaerbeek possède onze aires de jeu communales :


Voir aussi : Les monuments classés de Schaerbeek



Au cours des premières décennies de l'État belge, la politique schaerbeekoise est dominée par l'opposition classique entre catholiques et libéraux. En 1878, la majorité catholique cède la place à une majorité libérale. À la fin du , Schaerbeek compte une importante population ouvrière et la commune voit la création de la première section du parti ouvrier belge dans la région bruxelloise. En 1896, ils font leur entrée dans le Collège au sein d'une majorité libérale-socialiste. Pendant l'entre-deux-guerres, le paysage politique schaerbeekois est fort morcelé, avec de nombreuses listes. Les années qui suivent la deuxième guerre mondiale voient un renversement d'alliance, les libéraux s'alliant aux catholiques. Les élections de 1970 voient une recomposition du paysage politique local, avec l'entrée en jeu du FDF.

Bastion du libéralisme, Schaerbeek a été conquise en 1970 par le Front démocratique des francophones (FDF), à l'époque nouveau parti régionaliste et pluraliste fondé en 1964, alors que les autres partis belges étaient encore officiellement « nationaux ».

Sous cette direction francophone, la commune adopta une disposition des guichets de l’état civil avec quatre guichets pour les francophones, deux pour les étrangers et un pour les néerlandophones. Cette répartition était cependant illégale et, en 1976, le gouvernement somme la commune de réorganiser les guichets de manière qu’ils soient tous accessibles à tous les habitants. Le bourgmestre Roger Nols et le collège refusent de s’incliner, et le gouvernement délègue un commissaire spécial, le vicomte Ganshof van der Meersch, chargé d'exécuter la décision du gouvernement (ce qu'il fit nuitamment accompagné de gendarmes après une tentative diurne qui s’était heurtée à un mur de militants du FDF).

Au fil du temps, le bourgmestre FDF Roger Nols, à l'initiative de la mise sur pied du Conseil communal consultatif des immigrés en 1973, évolua vers des positions de plus en plus xénophobes, allant jusqu'à inviter Jean-Marie Le Pen à Schaerbeek dans les années 1980, à interdire les enseignes de magasin en d'autres langues que le français ou le néerlandais, à interdire les rassemblements vespéraux de plus de trois personnes sur la voie publique, à interdire les cours de religion musulmane dans les écoles communales, à bloquer l'inscription d'étrangers auprès de l'administration communale…

Pendant cette période, le PRL fera campagne commune avec la liste d'intérêts communaux NOLS ("Nouvelles orientations des libertés schaerbeekoises"). Le FDF finira par l'exclure, après qu'un de ses élus, Georges Verzin, eut claqué la porte du parti pour protester contre sa dérive raciste et mis sur pied une liste dissidente, IDS (Initiatives pour le développement de Schaerbeek). En 1999, alors qu'il n'était plus qu'un simple conseiller communal, Roger Nols passa au Front National et ne se représenta plus en 2000.

En 1994, la liste Duriau (ex-nolsistes) s'allie au FDF (très anti-PRL au niveau local, comme à Etterbeek et à Koekelberg), à Ecolo, au PS et au PSC, rejetant dans l'opposition le PRL encore nolsiste et le Front national. En cours de législature, Francis Duriau s'affilie au FDF et un de ses échevins PSC passe au PRL.

En octobre 2000, pour la première fois, les ressortissants de pays membres de l'Union européenne peuvent s'inscrire en tant qu'électeurs : seuls 971 sur électeurs potentiels accomplissent cette démarche, ils pèsent donc peu parmi les électeurs inscrits. Pas moins de 11 listes se présentaient à ces élections, dont 7 complètes (47 candidats). Quelques petites listes incomplètes étaient également présentes.

Sur 47 sièges, la fédération PRL-FDF (actuel MR) remporte les élections avec 16 sièges, la "Liste du Bourgmestre" (LB) dirigée par le bourgmestre sortant Francis Duriau n'en récolte que 8. Mais l'animosité entre les deux chefs de file et concurrents au poste de bourgmestre aboutit à une large coalition arc-en-ciel réunissant le PRL-FDF-MCC d'une part, Ecolo (11 sièges, dont 1 Agalev) et le PS (5 sièges), rejetant dans l'opposition deux partis de la majorité sortante, la LB et le PSC (3 sièges, actuel CDH). La liste flamande d'extrême-droite Demol (4 élus, regroupant le Vlaams Blok et des transfuges du FN et du PRL) reste dans l'opposition.

Bernard Clerfayt est alors installé comme bourgmestre. Lors des négociations post-électorales sur la répartition des postes exécutifs (bourgmestre, échevins, président du CPAS), Ecolo désigne Tamimount Essaïdi (de parents marocains) comme candidate à un de ses quatre mandats, le PS désigne Alain Hutchinson, déjà secrétaire d'État régional, qui devra être remplacé par un "échevin faisant fonction", Mohamed Lahlali (ancien étudiant marocain), ce qui provoque une polémique ; quant au MR, il installe lui aussi un échevin allochtone, Sait Köse (de parents turcs).

Bernard Clerfayt (MR-FDF) revêtit l'écharpe mayorale pour la mandature 2001-2006, dont les chantiers essentiels allaient être l'assainissement des finances publiques, la poursuite de la rénovation des quartiers anciens et l'amélioration de la sécurité.

À la veille des élections de 2006, la vice-première ministre socialiste Laurette Onkelinx annonce son débarquement dans la cité des Ânes afin de ravir la maïorat au MR Bernard Clerfayt, pourtant crédité d'un bon bilan dans des circonstances difficiles (finances déficitaires, tension dans les quartiers, majorité très large et hétéroclite). La campagne électorale schaerbeekoise devient un enjeu national, traité par tous les grands médias du pays. Contre toute attente, le score donne la liste du bourgmestre sortant grand vainqueur des élections avec près de 42 % des voix (contre 30 % en 2000) pour 25 % pour le Parti socialiste, conduit par Laurette Onkelinx (contre 11 % en 2000). En termes de voix de préférences, Bernard Clerfayt remporte le score de voix de préférence pour seulement pour la vice-première ministre socialiste. Bien qu'un accord électoral secret fût signé entre le PS, le CDH et Ecolo, la chef de file écologiste, l'ancienne vice-première ministre Isabelle Durant, choisit de poursuivre sa collaboration avec la liste du Bourgmestre. L'actuelle majorité (janvier 2007 - janvier 2012) se compose du FDF (le bourgmestre + 3 échevins), du Parti réformateur libéral (4 échevins), d'Ecolo (2 échevins + présidence du Centre public d'action sociale), de Groen! (1 échevin). L'opposition est composée du Parti socialiste, du Centre démocrate humaniste et du groupe Demol (Vlaams Belang).

Le 20 mars 2008, Bernard Clerfayt quitte son poste de bourgmestre pour devenir Secrétaire d'État au Gouvernement fédéral belge. La loi belge interdit de cumuler un poste de Bourgmestre et un poste de Secrétaire d'État, c'est donc Cécile Jodogne ( sur la liste électorale MR) qui fait fonction de Bourgmestre entre 2008 et 2011 .

Schaerbeek a, sur son territoire, des lieux de culte catholique, protestant, orthodoxe, juif, musulman et bahá'í.
L'Église catholique romaine compte dix paroisses dans la commune qu'elle organise ainsi depuis 1961 :


Les quatre bibliothèques publiques schaerbeekoises: 





Voir aussi : et 


Le Crossing de Schaerbeek est un club de football belge affilié à l'URBSFA (numéro d'affilié 4070) né en 2012 de la fusion de la "Royale Union Sportive Albert Schaerbeek" ("RUSAS") et du "RFC Evere".

Le club a connu ses heures de gloire lors des années septante et a évolué au plus haut niveau du football belge. Il évolue actuellement en provinciale et est logé au stade communal de Schaerbeek ainsi qu'au stade Chazal et au parc Saint-Vincent. Ses couleurs sont le vert et blanc en référence aux couleurs des communes d'Evere et de Schaerbeek. 

L'ancien international belge et coach national George Leekens, a évolué sous les couleurs du club bruxellois. 



La ville de Schaerbeek est jumelée avec :


</doc>
<doc id="19252" url="https://fr.wikipedia.org/wiki?curid=19252" title="Région de Bruxelles-Capitale">
Région de Bruxelles-Capitale

La Région de Bruxelles-Capitale, Région bruxelloise ou communément appelée Bruxelles (agglomération), prononcé (en néerlandais ou "Brussel", prononcé ), est l'une des trois régions qui composent la Belgique. Elle ne doit pas être confondue avec la Ville de Bruxelles, qui n'est qu'une commune de la Région de Bruxelles-Capitale.

La région compte au . Elle a ses propres gouvernement et parlement et dispose de pouvoirs internationaux économiques et financiers, ainsi que d'un pouvoir sur les échanges estudiantins.

Le gouvernement fédéral et le parlement fédéral belges, dont la compétence s'exerce sur toute la Belgique, siègent également à Bruxelles-Capitale, de même que les institutions directrices de l'Union européenne et l'administration de l'OTAN. Cela fait de cette région une capitale et cela explique que, dans le monde, on dit couramment « Bruxelles » en identifiant la région avec la Belgique ou avec l'Union européenne. Le gouvernement flamand, qui gère les compétences régionales et communautaires sur le territoire de la Région flamande et les compétences communautaires unilingues pour les néerlandophones de la Région bruxelloise, y a également son siège.

Si on la compare à la plupart des régions d'Europe, Bruxelles-Capitale dispose d'un territoire relativement réduit, sa superficie étant de . La région est enclavée en Région flamande, mais distincte de celle-ci. Bruxelles-Capitale recense par kilomètre carré au . Elle constitue le premier bassin d'emploi de Belgique, en particulier dans le secteur tertiaire, malgré un taux de chômage de 16,9 % en novembre 2017. Bruxelles-Capitale est classée deuxième centre industriel de Belgique après Anvers.

La fleur stylisée représentée sur le drapeau officiel bruxellois est un iris des marais.

À l'origine, la langue de la population était le brabançon, variante locale du domaine linguistique néerlandophone. Mais, au cours des , on constate une montée de la langue française. Bruxelles est devenue bilingue, voire multilingue, avec le français pour langue majoritaire et , langue de convergence entre les différentes communautés linguistiques. À côté d'une immigration française et wallonne, cette progression du français s'explique avant tout par la conversion linguistique de la population flamande au cours des générations.

La francisation démarra graduellement au , mais ne prit toute son ampleur que lorsque la Belgique devint indépendante et que le nombre d'habitants de la nouvelle capitale s'accrut rapidement. Le recul massif du dialecte brabançon, communément appelé dialecte "flamand", ne commença que dans la seconde moitié du . Le français — seule langue officielle en dépit de la majorité flamande — était la langue des tribunaux, de l'administration, de l'armée, de la culture, des médias et de l'enseignement. En tant que langue du pouvoir économique et politique, et des échanges internationaux à l'époque, le français était considéré comme absolument nécessaire pour progresser socialement. Aussi, à partir de 1880, constate-t-on une véritable explosion de bilingues au détriment des néerlandophones unilingues.

Le néerlandais ne se transmettait plus à la génération suivante, ce qui a eu pour effet une augmentation considérable du nombre des francophones unilingues après 1910. Aussi, à partir des années 1960, à la suite de la fixation de la frontière linguistique et de l'essor économique de la Région flamande, la francisation des néerlandophones a stagné . Durant la seconde partie du , Bruxelles devint progressivement une ville d'échanges internationaux, ce qui contribua à un afflux d'immigrants qui favorisèrent l'émergence du français ainsi que d'autres langues étrangères aux dépens du néerlandais. Simultanément, à la suite de l'urbanisation, un nombre supplémentaire de communes précédemment néerlandophones de la périphérie bruxelloise devinrent majoritairement francophones . Ce phénomène, connu en Flandre comme la « tache d'huile », constitue, en même temps que la question du statut de Bruxelles, un des principaux sujets de contentieux de la politique belge.

Lors de la révision constitutionnelle de 1970, un article 107 quater nouveau ajouté à celle-ci prévoit la création de trois régions et confie à la "loi spéciale" le soin de le concrétiser. Cependant, à la suite de l'échec de la mise en pratique du pacte d'Egmont et des accords du Stuyvenberg en 1978, la loi spéciale du 8 août 1980 créant les institutions régionales ne comportait aucune disposition relative à la Région bruxelloise, ceci en raison de l'impossibilité de dégager un accord politique à son propos. 

La Région bruxelloise fut ensuite « mise au frigo » pendant huit ans, période pendant laquelle c'est une loi de juillet 1979 qui continua à s'appliquer. En vertu de cette loi, Bruxelles était dirigée par un comité ministériel de trois membres, un ministre et deux secrétaires d'État, responsables devant le Parlement national, tandis que l'agglomération bruxelloise créée en 1971 continuait à s'occuper de la gestion des dix-neuf communes de la ville, principalement pour la propreté publique et les grandes voiries. Les communes conservaient néanmoins leurs autorités élues, bourgmestres, échevins et conseillers communaux avec des pouvoirs dans l'urbanisme, qui s'enchevêtraient avec ceux du gouvernement.

La « mise au frigo » de Bruxelles, de 1970 à 1978, aura des conséquences importantes, puisque pendant huit ans, la ville ne peut développer une politique propre. Pendant cette période, également, la solution institutionnelle sembla s'éloigner. Les partis néerlandophones étaient réticents à voir dans Bruxelles une région à part entière et semblaient favoriser une gestion conjointe par les deux communautés ou directement par l'État fédéral. Les partis francophones, quant à eux, ne semblaient pas avoir de vision claire des structures à donner à la Région et au fonctionnement de celles-ci, notamment sur la question de la parité linguistique, avec, dans une certaine mesure, une opposition entre les composantes wallonnes et bruxelloises des partis belges francophones.

Ce n'est qu'en 1988 qu'un accord politique est trouvé, accord qui aboutit à la loi spéciale du 12 janvier 1989, relative aux institutions bruxelloises. Cette loi crée la Région de Bruxelles-Capitale, sur le territoire des 19 communes, et met en place ses organes législatif et exécutif. Par ailleurs, elle transfère à la Région l'exercice des compétences de l'Agglomération et règle l'exercice des compétences communautaires en créant notamment les Commissions communautaires (Commission communautaire commune, Commission Communautaire Française et Vlaamse Gemeenschapscommissie).

Le , pour la première fois, les Bruxellois élisaient directement leurs représentants régionaux. Bruxelles constitue depuis lors une Région à part entière, comparable aux Régions flamande et wallonne, à la différence près qu'elle ne bénéficie pas (à l'instar de la Communauté germanophone de Belgique) du principe d'autonomie constitutive, ce qui signifie qu'elle ne peut pas fixer par décision de ses seules instances la composition de son parlement et de son gouvernement, ainsi que certaines règles de fonctionnement. De plus, contrairement aux autres régions et aux communautés, le Parlement de la Région de Bruxelles-Capitale adopte des ordonnances et non des décrets. Dans le droit belge, les ordonnances sont hiérarchiquement "inférieures" aux décrets régionaux, à savoir qu'elles n'ont pas strictement force de loi contrairement aux décrets. Ceci est une indication supplémentaire que le statut d'égalité n'est pas encore parfait entre Bruxelles-Capitale et les autres régions de Belgique : la Région flamande et la Région wallonne.

Bruxelles est desservie par quatre grandes sociétés nationale ou régionales :

La Région se compose de 19 communes, dont la Ville de Bruxelles, la plus vaste d’entre elles, s’étend du centre au nord de l’agglomération et est entourée par les 18 autres.Géographiquement parlant, les 19 communes forment une seule et même agglomération dense en population. L'aire urbaine de Bruxelles n’est pas confinée aux limites administratives de la Région de Bruxelles-Capitale car elle déborde largement sur la Région flamande limitrophe.

Certains francophones ne seraient pas défavorables à une fusion des 19 communes de Bruxelles, soit en une seule entité, soit en un nombre de communes ou districts moins nombreux, mais ils souhaiteraient dans ce cas que des garanties soient données en faveur d'une représentativité plus réelle des groupes linguistiques dans les nouvelles institutions. C'est pourquoi une telle réforme vers une fusion des communes bruxelloise, si elle est parfois jugée nécessaire, est très difficile à mettre en œuvre, car elle bute sur des résistances dans les deux communautés linguistiques, pour des raisons opposées.

Selon une étude récente (La Libre Belgique, la RTBF et l’Université catholique de Louvain (UCL), la part des immigrés non européens dépasse le 1/3 de la population.

Cette région officiellement bilingue est habitée par une majorité d'habitants ayant comme une de ses langues maternelles le français, une minorité de néerlandophones, et une importante minorité d'allophones optant généralement pour le régime linguistique francophone, parmi laquelle les nombreux représentants des fonctionnaires européens et leurs familles en provenance des pays de l'Union européenne ou appartenant aux multinationales et lobbys européens et extra européens installés à Bruxelles. À ces derniers s'ajoutent les communautés de migrants issus non seulement des anciennes colonies belges (République démocratique du Congo (RDC), Rwanda et Burundi en Afrique noire) mais aussi issus de France, du Maghreb (notamment du Maroc), de Turquie, d'Amérique, d'Asie (Iran, Pakistan…), faisant de la Région un ensemble cosmopolite et multiethnique. Ces milieux assimilent le français, bien que l'anglais ait commencé à faire une apparition comme langue des affaires. L'anglais, le turc, l'espagnol et l'arabe parmi nombre d'autres langues, caractérisent une émigration récente d'émigrés.

Le gouvernement et le parlement bruxellois exercent leur pouvoir sur dix-neuf communes. On peut comparer les communes de Bruxelles aux arrondissements ou aux districts d'autres capitales. L'article 194 de la Constitution belge de 1831 désigne la commune centrale nommée Ville de Bruxelles comme capitale de la Belgique. Mais, depuis la naissance de la région de Bruxelles-Capitale inscrite à l'article 4 de la révision constitutionnelle de 1994, on a assisté à l'installation de sièges de ministères fédéraux dans d'autres communes que la commune centrale, consacrant ainsi l'extension aux dix-neuf communes de la fonction de capitale de la Belgique. Ainsi, le Comité interministériel fédéral (CIPS) a son siège à 1060 Bruxelles, c'est-à-dire dans la commune de Saint-Gilles, et l'administration centrale du ministère fédéral de l'emploi et du travail est installée à 1070 Bruxelles, dans la commune d'Anderlecht. Cette évolution est conforme au titre officiel de l'ensemble des dix-neuf communes groupées dans un grand arrondissement électoral unique dénommé par la constitution belge BRUXELLES-CAPITALE. Les administrations du gouvernement de Bruxelles-Capitale proprement dit sont également installées dans plusieurs communes. 

Cependant, les dix-neuf communes ont chacune un bourgmestre, des échevins et un conseil communal qui disposent d'une autonomie interne en matière d'urbanisme, de culture et d'aide sociale, fruit de la tradition historique, alors que, par contre, dans le domaine budgétaire, elles sont soumises au contrôle centralisé du gouvernement et du parlement bruxellois, ce qui contribue à harmoniser la gestion des composantes de l'ensemble urbain. Le gouvernement bruxellois exerce aussi un pouvoir exclusif sur les transports en commun, les grandes voiries, la politique du logement, la lutte contre le chômage et il dispose même d'une capacité dans le domaine du commerce extérieur quand il s'agit de relations entre les entreprises bruxelloises et l'étranger. 

La réforme constitutionnelle de 2012, qui entre en vigueur en 2014, renforce la centralisation bruxelloise en dotant le gouvernement et le parlement bruxellois d'un pouvoir accru en matière sociale et dans le domaine urbanistique.

Le gouvernement et le parlement de la région bruxelloise exercent leurs compétences au sein du territoire de la région, où ils adoptent et appliquent des ordonnances à une population urbaine qui constitue un tout homogène nonobstant la répartition du territoire urbain en communes (que l'on peut comparer aux arrondissements parisiens, mais avec plus de pouvoirs locaux). 

Le Parlement bruxellois est constitué de 89 membres élus au suffrage universel par les habitants des 19 communes formant la Région. Parmi les 89 députés, 72 font partie du "groupe linguistique français" et 17 du "groupe linguistique néerlandais". Chaque groupe siège par ailleurs séparément au sein de son assemblée respective : Assemblée de la Commission communautaire française (Parlement francophone bruxellois) et Assemblée de la Commission communautaire flamande (raad VGC). Le Gouvernement régional bruxellois est choisi par le Parlement pour une durée de cinq ans. Il est composé de cinq ministres et de trois secrétaires d'État.

A Bruxelles-capitale, la nature représente 14 % de la superficie de la région urbaine régie par l'autorité supérieure bruxelloise, soit 2.300 hectares représentés principalement par la grande forêt de Soignes -qui pénètre dans la ville et qui est partagée avec les régions voisines sur près de 5.000 hectares- et par le bois de la Cambre, le parc de Laeken, le parc Roi Baudouin et les nombreux parcs et squares qui constellent la ville. 

De nombreuses espèces animales et végétales habitent ces espaces verts, principalement dans la forêt de Soignes, des cerfs, des daims, des renards, diverses races de petits animaux, lapins, écureuils, belettes et les espèces de chauve-souris et d'oiseaux de l'Europe du nord. 

L'intégrité de l'ensemble des espaces verts bruxellois est classé site Natura 2000 constitué par l'Union européenne comme ensemble de sites désignés dans chaque état membre pour conserver la faune et la flore au sein d'un environnement socio-économique développé.

Bruxellois francophones et Bruxellois néerlandophones disposent de leurs propres institutions politiques et administratives, COCOF, Commission communautaire française pour les Francophones et VGC Vlaamse Gemeenschapscommissie pour les Néerlandophones. Les matières bi-communautaires, c'est-à-dire les matières qui ne peuvent être liées à une communauté linguistique particulière dans la région sont gérées par la Commission Communautaire Commune-Gemeenschappelijke Gemeenschapscommissie (COCOM).

Une école de Bruxelles, en tant qu'institution pédagogique, dépend soit de la Communauté française si l'école est francophone, soit de la Communauté flamande si elle est néerlandophone, et ceci tant pour ce qui est du salaire des enseignants qu'en ce qui concerne les programmes enseignés. C'est un compromis complexe, mais qui permet une cohabitation pacifique des deux cultures de ce pays.

Les compétences communautaires sont au nombre de trois : l'enseignement, la culture et les matières personnalisables. Pour l'enseignement et la culture, les établissements dont les activités sont francophones ou néerlandophones dépendent respectivement des communautés française et flamande. En ce qui concerne la culture, c'est le même raisonnement sauf qu'il existe des institutions dites « biculturelles » qui restent soumises à la compétence du pouvoir fédéral. Ce sont des institutions dont le rayonnement dépasse la Région de Bruxelles-Capitale comme les grands musées : Musée des Beaux-Arts, Musées royaux d'Art et d'Histoire (dits du Cinquantenaire) et leurs différents sites (Porte de Hal, Pavillon Chinois, Tour Japonaise), Institut Royal des Sciences Naturelles de Belgique, Musée royal de l'armée et d'histoire militaire, Palais des Beaux Arts, Théâtre de l'Opéra Royal (dit de la Monnaie), ainsi que le Palais de Justice et les sièges de Justice de Paix, la Justice étant un domaine strictement fédéral.

Pour les matières dites personnalisables dans la langue administrative, c'est-à-dire celles qui concernent la personne des citoyens -la santé et l'action sociale- c'est en fonction de leur organisation fondée sur l'une ou l'autre des deux langues officielles que la distinction s'opère. Les établissements qui ont une organisation néerlandophone dépendent de la Communauté flamande (la Commission communautaire flamande n'ayant pas reçu le pouvoir d'édicter des décrets). Les établissements qui ont une organisation francophone dépendent de la Communauté française ou de la Commission communautaire française (COCOF) pour les matières que la Communauté française a transférées à cette dernière. 

Enfin, certains établissements ou institutions dépendent de la Commission Communautaire commune (COCOM) lorsque, en raison de leur destination, ils ne peuvent être considérés exclusivement francophones ou néerlandophones, comme les hôpitaux publics ou privés qui n'ont pas fait le choix d'être exclusivement francophones ou néerlandophones. Certaines compétences territoriales concernant les citoyens ont été confiées à la COCOM, comme le contrôle des maladies infectieuses, la lutte contre le dopage ou plus récemment les allocations familiales. Au reste, vu le fait qu'il n'y a pas de sous-nationalité, tout citoyen peut s'adresser directement à la Commission Communautaire Commune. Mais il peut aussi librement recourir à des services exclusivement francophones ou néerlandophones, comme l'ONE (Office National de l'Enfance) ou son homologue "Kind & Gezin" (Enfant et Famille), indépendamment de sa propre langue, par exemple si un ressortissant d'une des deux cultures veut inscrire son enfant dans une institution dont la langue est différente de la sienne.

Bruxelles-Capitale est une ville de PME et d'industries tertiaires (commerces, hôtellerie), mais elle possède aussi des entreprises industrielles, Audi et la SABCA. Audi dans la commune de Forest (usine principale en dehors de l'Allemagne de la marque allemande de voitures de luxe) qui est installées dans l'ancienne usine Volkswagen ; la SABCA, du groupe Dassault, société dont le siège de Bruxelles et ses usines de Haren et de Gosselies (Charleroi), ont produit nombre d'avions civils et militaires depuis les années vingt. On peut y ajouter le port catégorisé port de mer et fluvial, qui occupe un nombre de travailleurs, près de en comptant les emplois sous-traités, qui le classe dans la catégorie des entreprises industrielles. L'ensemble de l'industrie bruxelloise génère une constellation de sous-traitants directement rattachés à l'activité industrielle. La totalité du chiffre d'affaires généré par les entreprises de Bruxelles-capitale place la ville en deuxième place derrière Anvers dans la statistique des places industrielles de Belgique . 

Cependant, la vie sociale à Bruxelles-Capitale a révélé, dans le dernier quart du , un phénomène de dualisation et de paupérisation. Les personnes qui atteignent tout juste le taux de pauvreté, ou se situent plus bas, représentent 26 %, alors que le reste de la population est aisée et même riche. Les spécialistes constatent que des difficultés vont se poser dans les domaines du social et de l'éducation et tout indique que la région, en grande difficulté financière, fiscalement exsangue et géographiquement étouffée par le cadre institutionnel belge, ne pourra s'en sortir seule<ref name="Le Monde du 8/10/2012"> Polémique sur un éventuel contrôle des naissances à Bruxelles "Le Monde.fr", 8 octobre 2012</ref>. Mais, depuis 2012, l'évolution de la réforme constitutionnelle -destinée à augmenter les pouvoirs des régions durant la législature 2014-2019- a apporté la perspective de pouvoirs accrus pour le gouvernement bruxellois qui va acquérir des ressources fiscales supplémentaires.

Fin septembre 2014, le taux de chômage à Bruxelles est de 21,2 % alors que, d'autre part, la présence d'une population à haut niveau de revenus constituées par les cadres des institutions européennes et de sociétés privées, notamment des lobbys, influence vers le haut la position de Bruxelles dans l'échelle de la richesse des villes européennes (deuxième derrière la zone centrale de Londres). Cela n'empêche pas les spécialistes de prévoir que des difficultés se poseront dans les domaines du social et de l'éducation et tout indique que la région restera dans une situation financière difficile, comme beaucoup de capitales, si elle ne peut compter sur un soutien suffisant de l'État belge. La contribution annuelle de l'état est justifiée par les frais considérables de police et de représentation officielle entraînés par présence de nombreuses institutions internationale, ainsi que par les frais de fonctionnement et d'entretien des communications (routes, transports en commun) utilisées par des usagers de toute la Belgique, alors que la ville région de Bruxelles est enserrée dans un cadre géographique qui la prive des revenus fiscaux de ceux qui viennent y travailler en profitant des infrastructures urbaines, alors qu'ils payent leurs impôts dans les régions où ils habitent. Tout indique, compte tenu de l'accroissement de la population, qu'une des solutions est à rechercher dans le sens de celle qui prévaut à Berlin où les impôts des navetteurs (qui font la navette quotidienne entre la ville et leur résidence à l'extérieur) sont partiellement payés sur leur lieu de travail dans la ville même. Certains évoquent aussi un contrôle des naissances qui paraît impossible dans un état qui se veut démocratique comme la Belgique, ainsi que le relate un article du journal "Le Monde'

La région de Bruxelles-Capitale ne recouvre qu'une partie de la zone d'influence économique bruxelloise : l'autre partie « déborde » sur le territoire des deux autres Régions du pays, d'une part dans la province du Brabant flamand, d'autre part dans la province du Brabant wallon. Cela lui confère la particularité d'avoir une partie de ses infrastructures gérée par les institutions d'autres régions (c'est le cas pour la section bruxelloise des réseaux de transport en commun « "De Lijn" » et « TEC » par exemple), et des infrastructures importantes se trouvent dans les deux autres régions (cas des aéroports, voir ci-dessous). Il y a donc deux cas à différencier : les infrastructures situées hors du territoire (logiquement gérées par les autres Régions) et celles situées sur le territoire de Bruxelles-Capitale.

La Région bruxelloise ne dispose pas d'aéroports sur son territoire et ses institutions n'en gèrent aucun. La particularité de la Région est qu'elle est servie par deux aéroports situés en dehors de son territoire administratif. Par ailleurs, ce sont les deux principaux aéroports de Belgique :
Cette particularité s'explique par le fait que cette Région est dans les faits un petit territoire totalement urbanisé, qui, à l'instar de toutes les grandes villes du monde, n'a évidemment pas d'aéroport sur son propre territoire.

Bruxelles est dotée, depuis le , d'un port de mer qui est devenu, au fur et à mesure des agrandissements à travers les siècles, le deuxième port intérieur belge grâce au canal maritime de Bruxelles à l'Escaut, appelé le canal de Willebroek, qui relie la capitale à Anvers via l'Escaut. Marchandises principales, les produits lourds en vrac, ciment, sable, pétrole. Les navires de mer jusqu'à et les grands trains de péniches peuvent pénétrer profondément à l'intérieur du pays jusqu'à Bruxelles en évitant les ruptures et les transferts de charge entre Anvers et le centre de Bruxelles. Ainsi, en évitant les transbordements, aussi bien à la réception qu'à l'expédition des marchandises, on réalise une compression des coûts qui assure une position concurrentielle favorable aux entreprises bruxelloises dépendant du trafic maritime international et du trafic intérieur de batellerie. De plus, la liaison, au centre même de la capitale, du canal maritime de Willebroek avec le canal de Charleroi offre aux entreprises bruxelloises des possibilités de développer des liens nord-sud par le moyen de la batellerie circulant entre les Pays-Bas, Anvers, Bruxelles et la zone industrielle du Hainaut où la navigation peut accéder au réseau des canaux français grâce aux importants ouvrages d'art de Ronquières et des ascenseurs de Strépy-Bracquegnies. L'importance du trafic fluvial dans Bruxelles permet d'éviter l'équivalent routier de par an, presque par jour, ce qui, outre un allègement des embarras de circulation, représente une économie de dioxyde de carbone estimée à par an.

Tandis que des infrastructures utilisées aussi par la population bruxelloise sont gérées par d'autres régions (ou par des institutions qui en émanent) quand elles se situent dans d'autres régions selon un principe de cohérence territoriale, cela peut aussi être le cas à l'intérieur même de la Région bruxelloise. La gestion des infrastructures situées en Région bruxelloise montre que ce n'est pas toujours Bruxelles-Capitale qui en a la responsabilité. Ainsi, les « sections bruxelloises » des lignes de transports "De Lijn" et TEC sont utilisées également par des Bruxellois, mais bien qu'elles se trouvent sur le territoire de la région-capitale elles ne sont pas gérée par elle.

La population scolaire bruxelloises comptabilise pour l'année scolaire 2010/2011 un total de élèves (enseignement francophone et néerlandophone) : élèves en maternelle, en primaire et en secondaire. L’augmentation est continue depuis 2004 ; entre 2004/2005 et 2010/2011, la variation est de + 11 % en maternelle, + 5 % en primaire et + 3 % en secondaire. Au vu de l'accroissement régulier de la population à Bruxelles, l'Institut bruxellois de statistique et d'analyse prédit une augmentation de élèves entre 2010 et 2015 et de entre 2010 et 2020.

En plus de l'enseignement francophone et néerlandophone, on y trouve quatre écoles européennes, ainsi que des écoles privées tel le "Lycée français" d'Uccle ou encore des écoles éduquant selon les programmes allemands, anglais ouvertes à tous les habitants des pays membres de l'Union européenne.

L'enseignement bruxellois est couronné par plusieurs institutions universitaires : l'Université libre de Bruxelles et sa sœur la Vrije Universiteit Brussel, l'Université Saint-Louis - Bruxelles, le secteur des sciences de la santé UCL Woluwe (facultés de médecine et dentisterie, de pharmacie et sciences biomédicales et de santé publique) et la faculté d'architecture LOCI de l'Université catholique de Louvain, ainsi que l'École Royale militaire. On y trouve de plus un enseignement supérieur artistique foisonnant, et un vingtaine de hautes écoles. 

Au total, de l'enseignement primaire jusqu'aux écoles européennes et aux universités et grandes écoles, l'enseignement bruxellois comptabilise 425.000 unités (2015-16).

La plus ancienne école secondaire encore en activité de la région bruxelloise est Sacré-Coeur de Jette

Dans la région de Bruxelles, le réseau de gaz et d'électricité est géré exclusivement par Sibelga. En 2009, la consommation de gaz était de et la consommation d'électricité de .

Les 19 communes de Bruxelles sont réparties en 6 zones de Police, toutes bilingues (français/néerlandais):

Le service des pompiers est, comme tout à Bruxelles, bilingue (français/néerlandais) et s'appelle le Service d'incendie et d'aide médicale urgente de la région de Bruxelles-Capitale (en néerlandais : Dienst voor brandbestrijding en dringende medische hulp van het Brussels Hoofdstedelijk Gewest). Il est généralement appelé par son abréviation "SIAMU" ("DBDMH") et est sous la tutelle d'un secrétaire d'État de la Région de Bruxelles-Capitale.
Il dispose d'environ hommes, tous pompiers professionnels, et de 8 casernes reparties sur l'ensemble des 19 communes de Bruxelles.

Il sera intégré prochainement au système de zone de secours au vu de la réforme de la sécurité civile belge.

La Région de Bruxelles-Capitale ne dispose pas de caserne de la protection civile belge sur son territoire. La plus proche est celle de Liedekerke, dans le Brabant flamand.




</doc>
<doc id="19258" url="https://fr.wikipedia.org/wiki?curid=19258" title="Plomberie">
Plomberie

La plomberie est une spécialité de l'ingénieur en mécanique appliquée au bâtiment et du plombier spécialisé, regroupant l'ensemble des techniques utilisées pour faire circuler des fluides (liquide ou gaz) à l'aide de tuyaux, tubes, vannes, robinets, soupapes, pompes aux différents points d'usage d'une installation. Le mot a pour origine le terme latin pour plomb ("plumbum") et provient de l'utilisation de ce métal malléable pour réaliser les installations de plomberie au cours des siècles précédents.

Ces techniques se sont améliorées au fil du temps grâce à l'évolution des connaissances scientifiques et à leur mise en application. Les premières applications systématiques de la plomberie ont été la mise en service de la gravité et des premières pompes, en agriculture, pour l'irrigation au moyen de pentes et de fossés.

Pour obtenir une bonne efficacité du renvoi d'un appareil, des évents sont nécessaires pour permettre le passage de l'air. Quand nous mettons de l'huile dans le moteur d'un véhicule à partir d'une canette métallique, on perce le dessus du contenant à deux endroits opposés sur le couvercle. Le but est de permettre un écoulement régulier par l'introduction d'air dans la boite. La même procédure s'applique pour la tuyauterie afin de faciliter la vidange des appareils sans que le siphon se désamorce à la fin du cycle.

L'installation de la plomberie est régie en Amérique du Nord par des codes d'installation strictes de type directifs et/ou de performance.

Les codes directifs obligent l'installateur à faire ceci ou cela. Le code de performance décrit le rendement minimum que l'installation doit rencontrer. Les nouveaux codes en préparation sont des codes de performance. 

Pour obtenir le droit de travailler dans sa spécialité, l'artisan obtient sa carte de compétence après avoir suivi un cours plus ou moins long dépendant de la spécialité embrassée. Des stages de formation viennent compléter les études : en 2009 l'entrepreneur doit posséder sa carte de compétence pour le gaz naturel, pour la vapeur, les gicleurs, la soudure de pipeline, en fonction de sa spécialité. 

Pour travailler dans le domaine institutionnel, commercial ou industriel le plombier doit savoir lire les plans, connaître les mesures de sécurité, avoir une expérience des matériaux, leur manipulation, leur installation, de leur résistance, connaitre les supports et ancrages appropriés, etc.

Le Dr Roy B. Hunter du National Bureau of Standards, actuellement NIST, a développé des tables pour déterminer les charges de la demande pour les appareils de plomberie. Ces courbes basées sur la probabilité simultanée d'utilisation des appareils, a permis d'élaborer un graphique de la demande pour éviter le sous ou sur-dimensionnement de la tuyauterie. Ce graphique simple à utiliser est reconnu comme fiable par la communauté des concepteurs et ingénieurs.
La courbe de Hunter a été construite à partir de l'appareil unitaire qui s'appuie sur le débit d'eau du plus petit appareil de plomberie (le robinet d'un lavabo vaut un appareil unitaire).

Il s'agit d'un outil auquel le concepteur doit ajouter son jugement, parce que cette courbe ne peut être utilisée pour un réseau municipal ou un gros édifice à logements (un édifice multi-logements). Les théâtres, arénas, lieux de rassemblement publics font partie des exceptions qui sortent de la courbe ; par exemple : quand il y a une partie d'un sport national à la télé, entre les périodes de jeu tout le monde va uriner en même temps, ce qui fausse cette courbe de demande.

La tuyauterie peut être en acier en acier inoxydable, en argile, en bambou en cuivre, en laiton, en verre, en thermoplastique pour l'aqueduc (PVC : polychlorure de vinyle, PE : polyéthylène, PP : polypropylène), en ciment, en ciment amianté pour l'aqueduc ou ciment armé pour le pluvial, en fonte grise centrifugée pour le sanitaire ou en fonte ductile pour l'aqueduc, en aluminium doublé de thermoplastique pour le pluvial, en glaise vitrifiée ou autres matériaux approuvés pour une utilisation spécifique.

La tuyauterie pluviale peut être en béton armé que l'on retrouve surtout dans le domaine du génie civil, domaine qui débute à un mètre à l'extérieur du bâtiment; tandis qu'à l'intérieur de l'édifice, la tuyauterie relève du génie en mécanique du bâtiment et se termine à un mètre à l'extérieur de celui-ci.

Les raccordements peuvent être faits : par joints mécaniques ou par compression pour les tuyaux de fonte, certains tuyaux de plastique et de , par étoupe/plomb pour les tuyaux de fonte avec cloche, par joints taraudés ou joints filetés ou soudage pour les tuyaux d'acier, par brasage pour les tuyaux de cuivre, par du ciment pour les tuyaux de ciment, par fusion pour les tuyaux de plastique, par joints toriques pour les tuyaux PER ou IPEX, etc.

Le choix de la robinetterie est important selon le produit qui est acheminé dans la tuyauterie en fonction de l'application. Quand l'utilisation de pompes s'impose, on doit porter une attention particulière au choix de cet élément ; à cause de la cavitation qui peut se produire dans certaines applications et/ou circonstances.
Ainsi pour contrôler des gaz un robinet à pointeau ou un robinet à boisseau sera utilisé plutôt qu'un robinet vanne. (Un robinet à boulet "Bille" peut avantageusement remplacer le robinet à vanne pour les petits diamètres. Les robinets à soupape ou à jupe se prêtent mieux au contrôle du débit. Le robinet à vanne sert à arrêter le passage du fluide non à en contrôler le débit. Une vanne papillon offrira un service semblable à la vanne pour les grands diamètres 64 mm et plus), la vanne papillon peut servir au contrôle du débit avec une certaine efficacité.

Les raccordements cuivre/acier devront être évités. L'usage d'un raccord de bronze ou d'un joint diélectrique permet d'éviter la corrosion galvanique.

La tuyauterie rigide doit être supportée à intervalles. La tuyauterie flexible doit être supportée sur toute sa longueur. Il est important de noter que pour les édifices en hauteur et les édifices où il y a rassemblement de nombreuses personnes, des ancrages para-sismiques doivent être ajoutés comme dans les hôpitaux, arénas, lieux publics, etc.

Étant donné la grande diversité des matériaux, la disponibilité de ceux-ci, le travail du choix final implique aussi la responsabilité de l'entrepreneur installateur sous la responsabilité de l'ingénieur. Le représentant de la compagnie qui vend et/ou fabrique les nouveaux matériaux sur le marché, fait connaître ceux-ci à l'ingénieur et au contracteur. L'expérience vient des mauvais essais relatifs au non-respect des garanties ou de la non performance des données publiées au départ. Quelquefois, c'est une mauvaise application d'un bon produit qui cause problème.

Dans les grands projets, c'est l'ingénieur en génie mécanique appliqué au bâtiment qui spécifie les matériaux à utiliser, prépare les plans et devis, surveille l'exécution des travaux, et/ou assume la responsabilité de l'installation et/ou partage celle-ci en autant que toutes les règles de l'art aient été respectées par l'entrepreneur en plomberie.

Le travail du plombier s'arrête à un mètre du mur extérieur du bâtiment. À partir de là jusqu'à l'égout public ou à la fosse septique, c'est l'égout de bâtiment. Les travaux pour l'égout de bâtiment sont plutôt exécutés par l'entrepreneur général tout comme le terrassement, le pavage, etc.

Le terme plomberie couvre plusieurs corps de métiers. Tous visent à la mise en pratique de principes scientifiques (ingénierie) comme la thermodynamique, la gravitation, la viscosité des fluides, pour le confort et la salubrité. (les sciences biologiques et médicales sont intimement reliées à la notion de confort. - Le confort est reliée au fait que le corps humain est homéotherme ; c’est-à-dire que la température intérieure du corps doit être constante. - Le confort c'est dissiper notre surplus de chaleur à la même vitesse qu'on la fabrique.)

Les principes scientifiques sont appliqués en réfrigération (voir Sadi Carnot) (avec un gaz, de l'eau), en chauffage avec un caloporteur (l'eau, le glycol, la vapeur), en protection incendie (par des systèmes de gicleurs automatiques), les systèmes d'extinction incendie au CO, les systèmes de vacuum, le transport d'huile ou du gaz naturel, air comprimé, gaz médicaux, industrie chimique et pétrochimique, industrie alimentaire, etc.

La plomberie couvre aussi de nos jours le transport de l'eau pour divers usages dont plus usuellement les applications domestiques et résidentielles.

On sait que pour le traitement des eaux usées il est toujours plus facile de purifier l'eau le plus près possible de sa source polluante.

Parmi les applications nouvelles et les matériaux plus facilement utilisables, il y a le PER ( en anglais)

Égout de bâtiment : 


La plomberie est devenue beaucoup plus accessible aux bricoleurs dans la mesure où il n'est plus nécessaire d'avoir tout l'outillage et le savoir-faire, particulièrement pour le brasage. On trouve actuellement des composants en plastique qui s'assemblent par simple collage (CPVC chlorure de polyvinyle) ou des raccords à visser permettant le raccordement des différents accessoires (robinets, etc.) ou à une ancienne installation en cuivre. Il existe même des tuyaux souples comme le PER (Polyéthylène Réticulé haute densité) pour l'intérieur et le PE (Polyéthylène basse densité) pour l'extérieur (arrosages, fontaines, mares...).

Il reste cependant au bricoleur à savoir schématiser son installation.
Dans cette lignée, il y a aussi le "Duratec Système Airline" qui est dédié à l'air comprimé et aux gaz. Ce tuyau d'aluminium est chemisé de plastique et est doublé à l'intérieur par du plastique. Les raccords sont dotés de joints toriques.
Notez aussi que le PER est appelé dans le monde anglo saxon.

Ce système est réputé pour son application en matière de distribution de l'eau dans une résidence unifamiliale, duplex, maison préfabriquée ou roulotte, du plus elle considérée comme très économique. Un outil spécial doit être utilisé pour réaliser de bons raccordements et de bons joints. Cependant, pour les immeubles d'habitation collective de 6 logement et plus ainsi que pour les édifices publics, on recommande l'utilisation du cuivre à cause des propriétés bactériostatiques de ce métal.

Cependant pour une application à logements multiples on devrait augmenter la tuyauterie d'un diamètre à cause des joints et raccords qui ont un diamètre plus petit que la tuyauterie. Cette restriction crée une turbulence dans l'écoulement du fluide, d'où une perte de pression qui oblige à l'augmentation de diamètre de la tuyauterie. Par contre ce type de tuyau flexible permet de sauver sur les raccords et joints tel que coudes 90 ou 45 et permet ainsi d'effectuer les changements de direction par de grands rayons, ce qui facilite l'écoulement linéaire sans turbulence.

Pour combattre la pollution des lacs et rivières par les eaux usées, des précautions s'imposent. Le traitement le plus économique et le plus efficace doit être fait à la source.

Pour éviter la pollution, l'économie des eaux potables est requise. Il est aussi possible de réutiliser les eaux pluviales, en les stockant, et de les utiliser pour l'arrosage des pelouses, pour l'évacuation des cuvettes des appareils sanitaires.

Les eaux usées (eaux grises) sont évacuées des éviers, des lavabos, des douches, des baignoires et des lessiveuses. Elles sont qualifiées d'eaux grises et peuvent servir à l'arrosage de terrains, pelouses et jardins. Il y a un potentiel d'économie d'énergie en récupérant la chaleur des eaux grises pour l'appliquer au préchauffage de l'eau chaude domestique. Une étude économique au cas par cas doit être faite selon les régions pour justifier ce type de projet.

Les eaux vannes (eaux noires) proviennent des cabinets d'aisance (WC) et urinoirs. Elles doivent être évacuées par les services d'égouts publics ou, quand les services publics sont inexistants, elles doivent passer par des fosses septiques et des champs filtrants avant d'être renvoyées dans la nature.

Les eaux usées techniques (eaux récupérées) proviennent des bâtiments de type industriel rejetant des produits toxiques dans leurs laboratoires. Ces eaux doivent être retraitées au sein de la structure industrielle avant d'être rejetées dans le réseau d'eaux usées. Après un traitement adéquat, elles peuvent cependant être réutilisées pour alimenter les urinoirs et toilettes, ou retournées dans la chaîne de l'industrie. (L'eau des tours d'eau des systèmes de réfrigération en sont un bon exemple).

Les eaux pluviales et les eaux de surface sont en principe évacuées de préférence "via" des fossés et/ou des égouts pluviaux. Il n'est généralement pas nécessaire de traiter ces eaux dans les usines d'épuration quand on les renvoie dans la nature. Moyennant un traitement minimal, ces eaux peuvent être récupérées pour alimenter des douches, toilettes et urinoirs. Elles doivent être emmagasinées dans des réservoirs de plastique sombres à l'abri de la lumière pour empêcher la prolifération d'algues. La tuyauterie de distribution devra être aussi en plastique à cause de l'acidité : leur pH se situe entre 4 et 5, ce qui attaque le cuivre et l'acier.

Cependant dans les villes, les 15 premières minutes d'un orage génèrent des eaux noires à la suite du lavage des toitures et rues de la ville. Ces eaux devraient être traitées ; mais elles sont souvent dérivées dans nos rivières à cause des grands volumes de pluie qui dépassent la capacité maximale que les systèmes d'épuration peuvent traiter.

Dans le cas des bâtiments avec de nombreux utilisateurs, une petite usine d'épuration peut être requise quand les services d'égouts sont absents (voir usine d'épuration).

En principe, il est toujours préférable et plus facile d'effectuer le retrait des substances polluantes le plus près possible de la source. Par exemple, quand on utilise des produits radioactifs dans un hôpital, si ce produit est rejeté à l'égout sans être retiré, il va polluer les boues de l'usine d'épuration. Une fois contaminées, celles-ci deviennent inutilisables. Le même raisonnement s'applique aux médicaments, aux produits chimiques, aux pesticides et à certains produits domestiques non biodégradables, tel le phosphate.

En 2010, grâce aux efforts du World Plumbing Council, le 11 mars est devenu la journée mondiale de la plomberie. Cette journée a pour but de faire mieux connaître l'importance de la plomberie dans la question de l'accès à l'eau potable.



</doc>
<doc id="19264" url="https://fr.wikipedia.org/wiki?curid=19264" title="Liste des Premiers ministres de Belgique">
Liste des Premiers ministres de Belgique

Cette liste présente tous les chefs du gouvernement belge depuis 1830. Le nom de leur fonction a varié au fil des siècles :


Les principaux partis politiques se sont scindés en deux ailes linguistiques indépendantes (l'une francophone, l'autre néerlandophone) :

Premières élections belges au suffrage censitaire () le .


Classement des chefs de cabinets et des premiers ministres selon la durée totale de leur(s) mandat(s) à la tête du gouvernement (en jours) :


</doc>
<doc id="19270" url="https://fr.wikipedia.org/wiki?curid=19270" title="Rembrandt">
Rembrandt

Rembrandt Harmenszoon van Rijn (en néerlandais ), habituellement désigné sous son seul prénom de Rembrandt, né le ou 1607 à Leyde, aux Provinces-Unies et mort le , à Amsterdam également aux Provinces-Unies, est généralement considéré comme l'un des plus grands peintres de l'histoire de la peinture, notamment de la peinture baroque, et l'un des plus importants peintres de l'École hollandaise du . Rembrandt a également réalisé des gravures et des dessins et est l'un des plus importants aquafortistes de l'histoire. Il a vécu pendant ce que les historiens appellent le siècle d'or néerlandais (approximativement le ), durant lequel culture, science, commerce et influence politique des Pays-Bas ont atteint leur apogée.

Rembrandt a réalisé près de 400 peintures, eaux fortes et dessins. La centaine d'autoportraits qu'il a réalisés tout au long de sa carrière permet de suivre son parcours personnel, tant physique qu'émotionnel. Le peintre représente, sans complaisance, ses imperfections et ses rides.

Une des caractéristiques majeures de son œuvre est l'utilisation de la lumière et de l'obscurité (technique du clair-obscur inspirée du Caravage) qui attire le regard par le jeu de contrastes appuyés. Les scènes qu'il peint sont intenses et vivantes. Ce n'est pas un peintre de la beauté ou de la richesse, il montre la compassion et l'humanité, qui ressortent dans l'expression de ses personnages, qui sont parfois indigents ou usés par l'âge. Ses thèmes de prédilection sont le portrait (et les autoportraits) ainsi que les scènes bibliques. Rembrandt représente aussi des scènes de la vie quotidienne, et des scènes populaires. Sa famille proche — Saskia, sa première femme, son fils Titus et sa deuxième concubine Hendrickje Stoffels — apparaissent régulièrement dans ses peintures. Il a exécuté peu de paysages peints (cela est moins vrai pour l'œuvre gravée) et de thèmes mythologiques.

Rembrandt Harmenszoon van Rijn est né le rue Weddesteeg à Leyde, ville universitaire et industrielle des Provinces-Unies (actuels Pays-Bas). Il est le neuvième des dix enfants d'une famille aisée : son père, Harmen Gerritszoon van Rijn, est meunier sur le Rhin et sa mère, Neeltgen Willemsdochter van Zuytbrouck, est fille de boulanger. La religion occupe une place centrale dans l'œuvre de Rembrandt et la période de tension religieuse dans laquelle il a vécu donne à sa foi une certaine importance. Sa mère est catholique et son père appartient à l'Église réformée néerlandaise. Tandis que son œuvre dévoile une profonde foi chrétienne, on ne sait pas si l'artiste appartenait à une église en particulier, bien que ses cinq enfants fussent baptisés dans des églises réformées d'Amsterdam : quatre dans la Vieille église d'Amsterdam et un, Titus, dans la Zuiderkerk.

Enfant, il étudie à l'école latine (institution calviniste donnant un enseignement religieux très poussé et où il prend ses premiers cours de dessin). À l'âge de 14 ans, il s'inscrit à l'université de Leyde mais n'y étudie pas et montre rapidement une forte inclinaison vers la peinture. En 1621, il devient alors pendant trois ans l'apprenti du peintre d'histoire versé dans les descriptions de scènes infernales Jacob van Swanenburgh, chez qui il apprend le dessin à la plume, puis Joris van Schooten.

Mais ce qui est déterminant dans sa formation est son séjour de six mois à Amsterdam en 1624, chez Pieter Lastman et Jan Pynas : Rembrandt y apprend le dessin au crayon, les principes de la composition et le travail d'après nature. Il aborde principalement les mêmes thèmes bibliques et antiques que Lastman et les traite avec la même Lastman lui transmet aussi l'influence d'artistes qu'il avait côtoyés à Rome : Adam Elsheimer et le Caravage, tandis qu'il découvre l'œuvre de Rubens dans son atelier. Rembrandt s'approprie ainsi le clair-obscur pour en faire un langage propre . L'animation et l'art d'Amsterdam marquent son œuvre de jeunesse et il fait la connaissance de peintres flamands, dont Hercule Seghers. Il rentre à Leyde en 1624.
Contrairement à beaucoup de ses contemporains qui faisaient le Grand Tour en Italie lors de leur formation, Rembrandt n'a jamais quitté les Provinces-Unies et s'établit à Leyde en 1625. Son ami Jan Lievens, également élève de Lastman et de Joris van Schooten, rejoint son atelier. À cette époque, leur talent et leur style sont si proches qu'il est difficile pour les historiens de l'art de les distinguer. Concernant les gravures de Rembrandt, André-Charles Coppier parle d'une époque où la production de ce dernier est jusque-là limitée à une surproduction d'estampes à vocation commerciale — des pour lesquels il se contentait d'un style purement linéaire —, associé aux peintres Jan Lievens, Gérard Dou, Hendrick Cornelisz. van Vliet et Jacques des Rousseaux.

En 1625, il signe "La Lapidation de saint Étienne", première toile qui nous soit parvenue de lui. L'année suivante, il réalise ses premières eaux-fortes, "Repos en Égypte" (B. 59) et "La Circoncision" (S. 398). En 1627, Rembrandt enseigne déjà à des apprentis, dont le premier est Gérard Dou qui entre dans son atelier en 1628, et probablement commence avec la préparation des panneaux et toiles et des peintures, qui étaient tous faits à la main dans les ateliers des peintres. Le tout premier commentaire sur Rembrandt connu date de 1628, où l'humaniste Aernout van Buchel écrit : 

Il ouvre un atelier à Leyde vers 1624, qu'il partage avec son ami et collègue Jan Lievenes. En 1627, Rembrandt commence à accepter des élèves, parmi lesquels Gerrit Dou en 1628.

En 1629, l'homme politique Constantijn Huygens, poète et secrétaire du prince d’Orange, lui rend visite dans son atelier et se montre enthousiaste sur Rembrandt dans son autobiographie ; ses commandes lui apportent notoriété et le sortent de ses difficultés financières. Grâce à cette collaboration, le prince Frédéric-Henri achète des peintures à Rembrandt jusqu'en 1646. En 1630, son père meurt. Il délaisse quelque peu la peinture pour se consacrer à la gravure — c'est d'ailleurs son année la plus productive (quinze estampes connues), quoiqu'elles ne soient pas les plus intéressantes : selon Gary Schwartz, ces .
En 1631, après avoir acquis une certaine reconnaissance, il se voit proposer de multiples commandes de tableaux de corporations et de portraits (les « », son premier portrait de groupe, "Leçon d'anatomie du docteur Tulp", étant réalisé en 1632), commandes issues d'Amsterdam qui l'obligent à s'installer dans cette ville. Un important marchand d'art lui offre le gîte, Hendrick van Uylenburgh dont il épouse la nièce Saskia van Uylenburgh le . Ce dernier l'introduit dans le cénacle de la haute société et favorise sa réputation, ce qui lui vaut plus de de portraits de patriciens dans les années 1631-1634 — virage déterminant dans sa carrière. Rembrandt a réalisé plusieurs portraits de sa femme entre 1633 (National Gallery of Art, Washington D.C) et 1634 (musée de l'Ermitage, Saint-Pétersbourg), et reste très prolifique en eaux-fortes, qui gagnent en qualité. Il grave notamment des scènes bibliques qu'il n'a pas le temps de peindre et peint pour la première fois des scènes de genre, des allégories et des portraits sur commande (1633).

Rembrandt et Saskia se marient en 1634 et ont un premier enfant en 1635, Rumbartus, qui meurt seulement deux mois plus tard. Il montre à cette époque une grande concentration dans ses gravures, qui sont désormais , comme "Joseph et la femme de Putiphar". Nouvelle baisse de production picturale en 1636. Le couple perd un nouvel enfant, Cornelia, en 1638. Les commandes de portraits cessent.

En 1639, Rembrandt et Saskia, qui vivent désormais dans une plus grande aisance financière, vont habiter une maison cossue (qui deviendra le Musée de Rembrandt) de Jodenbreestraat, dans le quartier juif. Une maison plus spacieuse et qui permet au peintre de recevoir et d'exposer, mais il doit prendre une hypothèque et s'endetter lourdement pour l'acquérir, ce qui devient la cause principale de ses difficultés financières ultérieures. Un troisième enfant meurt peu après sa naissance en 1640. Le quatrième, Titus, né en 1641, et le seul à atteindre l'âge adulte. Saskia meurt d'une phtisie en 1642 à l'âge de .
Il réalise d'importantes gravures dans tous les genres, mais le sommet artistique de cette période est le tableau "La Ronde de nuit" ("", Rijksmuseum Amsterdam, 1642) : l'œuvre originale mesure (mais sera plus tard diminuée) et fait le portrait de d'une milice civile, d'une façon dynamique, révolutionnaire pour son temps.

Sa production de peintures continue et celle des eaux-fortes se stabilise jusqu'à la fin de sa carrière autour de six estampes par an. 
Entre 1643 et 1649, Rembrandt partage ensuite sa vie avec sa servante , jeune veuve sans enfant, qui prend en charge le bébé Titus. En 1648, elle rédige son testament qui voit toutes les possessions de Rembrandt léguées à son fils Titus. Cette liaison entre un veuf et la nourrice de son fils provoque un scandale au point que Rembrandt décide de la congédier. Geertje entame et gagne un procès contre Rembrandt sur le sujet de promesse de mariage, mais Rembrandt la fait enfermer dans un asile d'aliénés en 1650, la faisant passer pour folle. La vie privée de Rembrandt est très agitée et 1649 est la seule année où Rembrandt n'a réalisé aucun tableau ni estampe — à noter tout de même que bien que non datée, l'une de ses plus remarquables estampes, "La Pièce aux cent florins" a été achevée vers cette année-là, après un long processus. Il vit à cette époque . Il reçoit très peu de commandes, les années 1650 étant moins prolifiques en ce domaine que la seule année 1632, alors que c'est sa principale source de revenus. On ne connaît par exemple qu'une seule eau-forte et un seul tableau de Rembrandt datés de 1653.

Après avoir été condamnée par le synode l'Église pour fornication, Hendrickje donne naissance à Cornelia en 1654. Il se consacre néanmoins à des sujets religieux, aussi bien dans ses estampes que dans ses tableaux, où sont à noter "Le Christ et la Samaritaine" et "Joseph accusé par la femme de Putiphar", peints .

Rembrandt vivant au-dessus de ses moyens, achetant des pièces d'art du monde entier (collection qui lui sert de modèle dans ses peintures), des costumes dont il se sert souvent dans ses œuvres, n'arrive plus à honorer ses dettes en 1656. Il est alors contraint de vendre sa maison et ses possessions — qui font l'objet d'un inventaire — aux enchères et de se contenter d'un logis plus modeste loué au 184 du canal . Sa compagne Hendrickje et son fils Titus fondent en 1658 une association pour continuer le commerce d'œuvres d'art qu'ils avaient commencé avant ces événements et obtiennent l'exclusivité du commerce de celles de Rembrandt en contrepartie de l'obligation d'entretenir Rembrandt toute sa vie. Malgré la renommée de Rembrandt qui continue à croître, les commandes diminuent et une grande toile de 1660, "La Conjuration de Claudius Civilis" (son tableau le plus grand), destiné à la nouvelle mairie d'Amsterdam, est refusée et retournée (maintenant dans le musée national de Stockholm).

En 1663, Hendrickje meurt et son fils se marie, laissant Rembrandt complètement seul. Ces événements marquent ses contemporains, et Joost van den Vondel, le grand poète national, fait comme ceux-ci le rapprochement avec ses œuvres, jugées plus obscures qu'avant, en le décrivant comme .

Les critiques de son époque, tels Joachim von Sandrart ("Teutsche Academie", 1675), Samuel van Hoogstraten ("", 1677), Arnold Houbraken ("Le Grand Théâtre des peintres néerlandais", 1718-1721) et Gérard de Lairesse ("Le Grand Livre des peintres, ou l'Art de la peinture considéré dans toutes ses parties, et démontré par principes ... auquel on a joint les Principes du dessin", 1787) louaient son génie mais réprouvaient . Rembrandt avait suivi l'évolution du baroque international vers une phase plus classique, mais alors que son style personnel arrivait à son paroxysme, il s'éloignait de celui de ses contemporains, plus proche des Van Dyck voire de ses élèves ou anciens compagnons d'ateliers (Govaert Flinck et Jan Lievens).
Malgré une image de solitaire incompris, Rembrandt a continué à recevoir des commandes : de particuliers, notamment Jan Six ; de corporations, comme l'atteste le fameux tableau "Le Syndic de la guilde des drapiers" (1662) ; et même à l'international, puisqu'un noble italien lui commanda un philosophe et reçut "Aristote contemplant le buste d'Homère" (1653) et plus tard "Alexandre le Grand" (1661) et "Homère" (1663), ainsi que 189 eaux-fortes en 1669. Il continuait par ailleurs à avoir des élèves, notamment Philips Koninck et Aert de Gelder.

Lors de ses huit dernières années, Rembrandt n'a produit qu'une seule gravure : un portrait de commande . Cette mise à l'écart de cet art de prédilection ne s'explique pas par un quelconque empêchement lié à la vieillesse, mais, selon Sophie de Brussière (Petit Palais), parce que Rembrandt avait déjà terminé l'exploration des techniques de la gravure — et en parallèle réussi à obtenir ce qu'il avait recherché pendant toute sa carrière de peintre, la « lumière-couleur » —, et n'y accordait plus d'attention.

Il survit aux disparitions de Hendrickje (morte en 1663 de la peste) et Titus, mort en 1668. Sa fille Cornelia, sa belle-fille Marguerite et sa petite-fille Titia sont à ses côtés quand il meurt le à Amsterdam. Désargenté, il est inhumé dans une tombe louée dans l'église Westerkerk, où une plaque commémorative est déposée en 1906 sur une colonne septentrionale de la nef mais plus aucune trace de cette tombe ne subsiste aujourd'hui, la famille Rembrandt n'ayant plus assez d'argent pour y faire construire un tombeau personnel.

Rembrandt a eu plusieurs élèves qui ont connu le succès :


Beaucoup d'œuvres habituellement attribuées à Rembrandt (telles "L'Homme au casque d'or", "Le Cavalier polonais" ou le "Philosophe en méditation") ont une paternité aujourd'hui contestée par les experts, notamment celles du Rembrandt Research Project, une coopération de six professeurs universitaires néerlandais. Le peintre a en effet encouragé ses élèves à copier ses œuvres ou a profité de la vente de leurs tableaux, se rémunérant ainsi en échange de leur instruction et de leur apprentissage. L'expertise est rendue encore plus difficile par le fait que Rembrandt ne signe pas toujours ses tableaux, que sa signature a évolué plusieurs fois et que certains de ses élèves signent leur copie du nom de leur maître.


Voir également la Liste des tableaux de Rembrandt

Près de 300 feuilles sont attribuées actuellement au peintre (contre plus de 1300 dans les années 1950). Il s'agit essentiellement d'exercices de style, la plupart n'ayant pas de rapport direct avec un tableau existant. Les plus riches collections sont conservées à Londres, Amsterdam, Berlin et au Musée du Louvre.

Le peintre a utilisé de nombreuses techniques dont la sanguine, l'encre, la pierre noire. Les thèmes en sont divers mais différents de ceux de ses tableaux : peu de portraits et beaucoup de paysages.

Le peintre reste l'un des grands aquafortiste du et a laissé près de 290 planches qui pour la plupart ne correspondent pas aux originaux car Rembrandt aimait les retravailler. Il a probablement appris la technique auprès de Jan Lievens qui partageait sans doute son atelier à Leyde.

Ses premières eaux-fortes datent de 1626 :"Repos en Égypte" (B 59) et "La circoncision" (s 398). Rembrandt n'a pas coutume de signer et dater les eaux-fortes. Une seule échappe à cette règle : "Jeune homme au buste : autoportrait" (B 338). Il s'est spécialisé dans la technique de l'eau-forte, utilisant une plaque de cuivre recouverte d'un vernis, ce qui permet de travailler sur cette dernière avec le même geste que le dessinateur. De l'acide attaquait ensuite les zones découvertes par le vernis (la "morsure"), formant un relief en creux qui pouvait retenir l'encre. Rembrandt utilisait plusieurs techniques complémentaires : la "double morsure" où il reprenait la plaque en la recouvrant une deuxième fois d'un vernis transparent, l'emploi complémentaire d'un burin pour accentuer certains traits, ou d'un « mordant » directement sur la plaque afin d'obtenir des effets de brume. Il a laissé également des irrégularités d'encrage permettant la constitution de voiles plus ou moins opaques.

L'année 1630 est extrêmement prolifique : "Rembrandt faisant la moue" (B 10), "Rembrandt à la bouche ouverte" (B 13), "Rembrandt au bonnet fourré et habit blanc" (B 24), "Présentation au temple, avec l'ange" (B51), "Jésus-christ au milieu des docteurs de la loi" (B66), "Gueux et gueuse" (B 164), "Gueux assis sur une motte de terre ; ressemblant à Rembrandt" (B 174), "Tête d'homme chauve" (B 292), "Tête d'homme chauve, tourné à droite" (B 294), "Tête d'homme de face" (B 304), "Vieillard à grande barbe" (B 309), "Tête de face riante : autoportrait" (B 316), "Tête d'homme au bonnet coupé" ; "Rembrandt aux yeux hagards" (B 320), "Homme à moustaches relevées et assis" (B 325). Ce sont de petits formats.

Entre 1650 et 1655, Rembrandt a fait quelques rares planches directement en taille-douce.

La peinture de Rembrandt procède par la superposition de couches de glacis (tons de , procédé du clair-obscur), cette matière ou « manière brute » s'opposant à la « manière lisse » de la génération suivante de peintres hollandais.

Rembrandt a peint "La Compagnie de Frans Banning Cocq et Willem van Ruytenburch", entre 1640 et 1642. Au , ce tableau paraissait si sombre et si détérioré qu’on a cru qu’il s’agissait d’une scène nocturne. Il fut donc rebaptisé ou surnommé "La Ronde de nuit". Un nettoyage opéré en 1947 permit de restituer sa lumière et surtout ses couleurs à l’œuvre, qui représente un groupe d'arquebusiers, quittant l'ombre d'une cour et s'avançant dans la lumière du jour.

Le tableau a été commandé pour orner le nouveau hall du "Kloveniersdoelen", la compagnie des arquebusiers, une des milices de gardes civils chargées de défendre la ville en cas de conflit. Rembrandt, s'éloignant des conventions du genre, choisit de montrer la troupe alors qu'elle se met en mouvement. On ne sait d'ailleurs pas à quelle occasion. S'agit-il d'une simple patrouille ou d'un événement particulier ? Une parade, à l'occasion de la visite de Marie de Médicis à Amsterdam, en 1638, ou celle d'Henriette Marie d'Angleterre, en 1642, ont été suggérées. Quoi qu'il en soit, cette approche artistique contraria les commanditaires et certains membres de la milice furent agacés de se voir relégués à l'arrière-plan, presque invisibles. À 1600 florins, le paiement était un record dans l'œuvre de Rembrandt, dans une société où un ouvrier gagnait de par an.

En 1725, pour qu'elle pût trouver sa place sur un mur d'une salle de l'Hôtel de ville, des morceaux de la toile, alors d'environ , furent découpés. Ses dimensions actuelles () sont encore impressionnantes : elle occupe tout un côté d'une des plus grandes salles du Rijksmuseum, dont elle est considérée comme l'œuvre majeure.

Cette toile représente six personnages en costume noir, portant chapeaux et fraises, qui vérifient les comptes de la corporation des drapiers. Elle illustre bien le talent de Rembrandt pour la disposition de ses personnages. Son exécution est sobre et efficace. Un article publié en 2004, par Margaret S. Livingstone, professeur de neurobiologie à l'Université de Harvard Medical School, suggère que Rembrandt, dont les yeux n'étaient pas alignés correctement, souffrait de . Cette conclusion a été faite après l'étude de trente-six autoportraits du peintre. Parce qu'il ne pouvait pas former une vision binoculaire normale, son cerveau se reportait automatiquement sur un seul œil pour de nombreuses tâches visuelles. Cette incapacité pourrait l'avoir aidé à aplatir les images qu'il voyait pour les restituer ensuite sur la toile en deux dimensions.

La distinction entre l'œuvre originale du peintre et celle faite par son atelier est difficile et les attributions ont été variables dans le temps, Rembrandt n'hésitait pas en effet à signer de son nom des tableaux qui étaient l'œuvre de collaborateurs parfois éloignés, le peintre entretenant sciemment cette confusion même pour des autoportraits. De plus, certaines de ses œuvres qui lui furent attribuées étaient réalisées par de ses élèves ou des imitateurs qui prenaient parfois la liberté de signer du nom de Rembrandt.

En 1836 est publié le premier recensement de son œuvre peinte, fait par John Smith, qui compte près de 600 tableaux. Près de 400 peintures supplémentaires lui sont attribuées par la suite, grâce à l'étude de Wilhelm von Bode publiée chez Charles Sedelmeyer (8 volumes, 1897-1907). Un inventaire de 1915 ramène ce nombre à 740 et Abraham Bredius à 600 dans un catalogue publié en 1935.

En 1968, le "Rembrandt Research Project" (RRP) a été créé sous l'égide de l'Organisation néerlandaise pour l'avancement de la recherche scientifique ("Nederlandse Organisatie voor Wetenschappelijk Onderzoek") et publie un nouveau catalogue raisonné qui porte à 420 le nombre de tableaux. Des historiens d'art et des experts de plusieurs disciplines se sont associés pour valider l'authenticité des travaux attribués à Rembrandt et établir une liste complète de ses peintures au sein d'un catalogue raisonné. Certaines œuvres ont été retirées de la liste après expertise, dont "Le Cavalier polonais", conservé par la Frick Collection de New York. La plupart des experts, parmi lesquels le Josua Bruyn du RRP, attribuent maintenant ce tableau à l'un des plus talentueux élèves de Rembrandt, Willem Drost. En 2003, le comité poursuit ses travaux d'investigation. Les enjeux de ces désattributions sont de taille et suscitent de grosses polémiques : un Rembrandt peut se vendre dans les années 2010 à plus de de dollars américains ; la Wallace Collection, qui avait de Rembrandt, ne se retrouve plus qu'avec un seul authentifié. L'expertise de "L'Homme au casque d'or", exposé à la Gemäldegalerie de Berlin, a également abouti à la conclusion que son « attribution à Rembrandt est à présent pratiquement exclue ».

À la suite des désattributions effectuées par les experts (notamment Ernst van de Wetering, directeur du "Rembrandt Research Project"), il n'en resterait actuellement que 357 estimés authentiques.

« Rembrandt » est une modification de l'orthographe du prénom de l'artiste qu'il a présentée en 1633. Ses premières signatures (vers 1625) se composaient d'un premier « R », ou le monogramme « RH » (pour Rembrant Harmenszoon, c'est-à-dire « fils de Harmen »), et à partir de 1629, « RHL » (« L » était, vraisemblablement, pour Leiden). En 1632, il a utilisé ce monogramme au début de l'année, puis a ajouté à son patronyme, « RHL-van Rijn », mais a remplacé cette forme dans la même année et a commencé à utiliser son prénom seul avec son orthographe d'origine, « Rembrant ». En 1633, il a ajouté un « d », et a toujours maintenu cette forme à partir de là, ce qui prouve que cette petite modification avait un sens pour lui. Ce changement est purement visuel, il ne change pas la façon dont son nom est prononcé. Curieusement, malgré le grand nombre de peintures et de gravures signées avec ce changement de prénom, la plupart de ses documents qui sont mentionnés au cours de sa vie ont conservé l'orthographe originelle « Rembrant ». (Note : la chronologie approximative de la signature des formes ci-dessus s'applique aux peintures et, dans une moindre mesure, à la gravure, de 1632, vraisemblablement, il n'y a qu'une seule gravure signée « RHL-v. Rijn », le grand format "La résurrection de Lazare" (B 73) ). Sa pratique de signer son travail de son prénom a probablement été inspirée par Raphaël, Léonard de Vinci et Michel-Ange, qui, hier comme aujourd'hui, ont été appelés par leur prénom seul.


Dans la deuxième édition de "Description de Leiden" (en , 1641), Jan Janszoon Orlers, maire de la ville, inclut la première biographie imprimée (sur une demi-page) de Rembrandt.

Edme-François Gersaint (1694-1750) est le premier à publier un catalogue de gravures de Rembrandt, en 1751 (à titre posthume) : le "Catalogue raisonné de toutes les pièces qui forment l’œuvre de Rembrandt". Gersaint choisit dans cet ouvrage de classer les œuvres non pas dans un ordre chronologique, mais suivant le sujet — et il sera en ceci suivi par la plupart de ses successeurs — qui sont : portraits de Rembrandt ; Ancien Testament ; Nouveau Testament ; sujets pieux, pièces de fantaisie ; mendiants, sujets libres ; paysages ; portraits d'hommes ; têtes de fantaisie ; portraits de femmes ; études.

Adam von Bartsch (1757-1821), également aquafortiste, écrit un ouvrage référence dans ce domaine : "Catalogue raisonné de toutes les Estampes qui forment l'Œuvre de Rembrandt, et ceux de ses principaux Imitateurs". Il y établit ce qui est devenu le système de numérotation définitif, sur son propre nom (par exemple « Bartsch 17 » ou « B. 17 »), pour les gravures à l'eau-forte de Rembrandt et les copies de beaucoup d'autres artistes, système encore employé dans ce domaine.

Ignace Joseph de Claussin (1795-1844), compose en 1824 "Catalogue raisonné de toutes les estampes qui forment l'œuvre de Rembrandt, et des principales pièces de ses élèves" puis en 1828 "Supplément au Catalogue de Rembrandt", le premier faisant référence, notamment pour Charles Henry Middleton qui le cite abondamment dans "" (1878), un autre ouvrage de référence. En gravure, sont également à noter Charles Blanc, Eugène Dutuit, Arthur Mayger Hind et André-Charles Coppier.


Le tableau "L’enfant à la bulle de savon", volé au musée de Draguignan le , dont la valeur était estimée à plus de 20 millions de francs en 1999 (4 millions d'euros), a été retrouvé le , l'auteur du vol s'étant rendu volontairement à la gendarmerie de Marmande. Cependant, depuis que la toile (non signée) a été retrouvée, la , historiens de l'art et conservateurs de musées en France comme ailleurs ont fait part de leurs doutes quant à son authenticité, y voyant la main d’un élève ou d’un imitateur. La conservatrice du musée rappelle que la toile est considérée comme un Rembrandt depuis son acquisition, : propriété du comte de Tourves, la toile a fait l’objet d’une saisie révolutionnaire en 1794, pour intégrer le musée de Draguignan. Le tableau a retrouvé les collections du musée le .

Cet travail a été réalisé en partenariat avec une équipe de Microsoft, le musée de la maison Rembrandt à Amsterdam, l'université de technologie de Delft et la galerie royale de Mauritshuis à La Haye, et a été présentée le 5 avril 2016 à Amsterdam. Résultat d'un travail collaboratif de 18 mois, le rendu de 148 millions de pixels s'appuie sur les technologies d'apprentissage automatique et l'impression 3D. 346 peintures ont été scannées en 3D pour enregistrer les couleurs et le relief de l'huile. Ces images ont ensuite été analysées par un algorithme qui en a extrait toutes les informations. Le programme a permis de faire ressortir les caractéristiques de l’œuvre du peintre pour faire le portrait-robot de ses œuvres soit un portrait, d'un homme de type caucasien, de face, regardant vers la droite, âgé entre 30 et 40 ans, vêtu de noir avec un col, portant la barbe et coiffé d'un chapeau. Grâce à un algorithme de traitement et de reconnaissance d'image, les détails clés des œuvres de Rembrandt ont été mis en évidence : l’espacement des yeux, la position du nez, la forme des visages, etc. Treize couches ont été imprimées successivement avec une encre à UV spéciale pour respecter les multiples couches de peintures superposées les unes aux autres présentes sur les tableaux actuellement authentifiés de Rembrandt.




Plusieurs fictions, au cinéma ou à la télévision, ont retracé la vie de Rembrandt :




</doc>
<doc id="19272" url="https://fr.wikipedia.org/wiki?curid=19272" title="Sttellla">
Sttellla

Sttellla est un groupe musical belge à géométrie variable et dont le seul membre fixe est Jean-Luc Fonck, figure emblématique de l'humour belgo-belge et chanteur intérimaire (en attendant qu'on en trouve un autre). Sttellla allie humour, jeux de mots et délire. Le groupe voit le jour en 1978 autour d'un duo formé par Jean-Luc Fonck et Mimi Crofilm qui quittera l'aventure en 1992.

Sttellla s'écrit avec deux T et trois L. Sur leur site web, on trouve l'explication suivante : . Ils ont voulu ensuite ajouter un T ou un L à chaque improbable nouveau concert qu'ils donneraient, mais ont vite arrêté vu leur succès grandissant de manière exponentielle et les nombreuses demandes pour de nouveaux spectacles.







</doc>
<doc id="19273" url="https://fr.wikipedia.org/wiki?curid=19273" title="Ramsès II">
Ramsès II

Ramsès (en égyptien ancien, "Ousirmaâtrê Setepenrê, Ramessou Meryamon"), est le troisième pharaon de la égyptienne. Né aux alentours de -1304 / mort à Pi-Ramsès vers -1213, et aussi appelé "Ramsès le Grand "ou encore "Ozymandias", Manéthon l'appelle Ramsès (ou Ramesses Miamoun, Rampses). Il règne de -1279 à -1213.

Son règne d'une exceptionnelle durée pour l'époque couvre à lui seul la moitié du nombre d'années que comprend la . En plus des nombreux monuments qu'il a fait bâtir à travers tout le pays (d'où son surnom de « pharaon bâtisseur »), il a fait sculpter de très nombreuses statues à son image et fait graver son nom sur presque tous les temples dont ceux d'autres pharaons, comme s'il les avait fait construire lui-même. Cette quantité extraordinaire d'objets d'art et d'éléments architecturaux à son nom explique que l'on retrouve sa trace dans presque tous les musées du monde ayant un département d'antiquités égyptiennes.

À l'instar d'autres personnages historiques dont la gloire a traversé les siècles, il est réputé pour être un grand guerrier et conquérant ce qui lui vaut en grande partie l'épithète de "Grand" dans les ouvrages historiques traitant de cette période de l'Antiquité égyptienne. Il lutte contre les Hittites et, assurant la domination de l'Égypte sur la Nubie et ses gisements aurifères, il y construit une série de temples dont les plus célèbres sont ceux d'Abou Simbel. Après la bataille de Qadesh en l'an de son règne, contre l'armée de l'empereur des Hittites, Muwatalli (-1310/-1269), la frontière sur l'Oronte est stabilisée.

Son action dans le pays de Koush et surtout dans le couloir syro-caananite dut marquer les esprits de l'époque car l'on racontait encore sous les Ptolémées la légende de l'extraordinaire voyage de « la princesse de Bakhtan » venue s'offrir en mariage au grand roi d'Égypte, écho lointain du fameux mariage avec la fille de Hattousil qui avait alors succédé à Mouwatalli sur le trône du Hatti.

Ramsès est souvent considéré comme le pharaon opposé à Moïse lors de l'Exode, bien qu'il n'existe aucune preuve pouvant l'attester et que son nom ne figure nulle part dans la Torah.

Il est le fils de Séthi et de la reine Mouttouya (ou Touy, ou Touya). Il a un frère qui se nomme Nebchasetnebet, qui meurt jeune, et une sœur aînée, Tia. Certains égyptologues citent aussi une autre sœur nommée Henoutmirê.

Son règne de soixante-six ans est exceptionnellement long et marque la dernière grande période de prospérité de l'Égypte antique. Il est marié à une douzaine d'épouses (presque toutes ayant le titre de « grandes épouses ») :
puis six de ses filles :
ainsi qu'une princesse babylonienne, une princesse syrienne et deux princesses hittites, filles de l’empereur Hattousili (-1264/-1234), dont Maâthornéferourê et sa sœur qu'il épouse en l'an 44, soit vers 1237. Ce dernier mariage est commémoré par deux stèles, l'une à Coptos et l'autre à Abydos, malheureusement le nom de la princesse ne figure pas sur les inscriptions.

Son harem ne comptera pas moins de deux cents concubines. Toutes ces femmes lui donnent une grande quantité d'enfants, on en dénombre cent vingt six.

Ramsès succède à son père Séthi apparemment sans problèmes particuliers. Il pourrait avoir été associé au trône (prince régent ou corégent) vers l'âge de quatorze ans à la fin du règne de ce dernier, selon l'interprétation que l'on fait de l'inscription dédicatoire d'Abydos.

Lorsqu'il monte sur le trône, il hérite d'une situation intérieure et internationale bien plus enviable qu'aux débuts de la . Les actions de son grand-père Ramsès et de son père, tous deux de brillants généraux et chefs d'armées, ont eu pour résultats de restaurer la puissance de l'Égypte et d'éloigner durablement toute menace du Double-Pays.

Cependant, cette politique de conquêtes et d'expansion se heurtait depuis plusieurs décennies à un adversaire de taille, l'empire Hittite qui contrôlait un vaste territoire depuis l'Anatolie jusqu'à l'Euphrate, assurant au passage une certaine domination sur les cités-États de la Syrie et du Liban.

Ces riches cités portuaires et commerciales étaient l'objet de toutes les convoitises et allaient se retrouver une fois de plus au milieu d'une guerre entre Égyptiens et Hittites dont elles représentaient le butin.

Comme son père Séthi, il veut protéger les intérêts de l'Égypte à l'Est contre les Hittites d'Asie. Il doit faire face à la menace dès le début de son règne.

Face à cette situation, Ramsès met sur pied une puissante armée, et établit son camp de base à Pi-Ramsès, qu'il transforme en capitale de son empire. De nouveaux arsenaux y sont construits ainsi que de grandes écuries pouvant accueillir les milliers de chevaux nécessaires au fer de lance de ses troupes : les chars de guerre. Les vestiges de ces écuries ont récemment été identifiés sur le site de Qantir par une équipe d'égyptologues autrichiens dirigée par Manfred Bietak.

Une fois les questions d'approvisionnement réglées, il manœuvre énergiquement en plusieurs campagnes pour s'assurer ses arrières en Canaan et poursuit son avance en attaquant la ville de Qadesh lors de sa de règne, mais ne remporte qu'une semi victoire.

Quittant l'Égypte par les "Chemins d'Horus", voie jalonnée de forteresses protégeant la frontière orientale du pays, l'armée de Ramsès longe le Canaan faisant halte à Gaza, passe par Canaan puis pénètre au Liban, s'assurant au passage l'allégeance de ses vassaux dont Byblos restait toujours l'indéfectible allié. Puis Ramsès et ses troupes s'enfoncent dans les terres et prennent la direction de Damas afin de prendre le chemin menant à Qadesh.

Les Hittites de leur côté avaient rassemblé une puissante armée de coalisés et s'étaient rassemblés dans la plaine de Qadesh, y installant leur camp et attendant l'arrivée de l'ennemi. Ils envoyèrent des éclaireurs qui furent interceptés par les Égyptiens et ramenés au camp de Ramsès. Ils informèrent le roi que les troupes de Mouwatalli se trouvaient au nord et n'osaient s'avancer vers Qadesh par crainte d'une confrontation avec les troupes égyptiennes.

Conforté dans son avance et impatient de reprendre la citadelle autrefois conquise par son père, Ramsès saisit sa chance et ordonne à marche forcée que l'armée se dirige sans plus attendre vers la forteresse convoitée.

Certain que les assiégés ne pourraient tenir longtemps face à sa puissante armée, il prend le risque de se détacher du gros de ses troupes. Le long cortège de soldats répartis en quatre corps d'armée s'étire alors sur la route. En tête de ses troupes Ramsès avec la division d'Amon traverse l'Oronte et il est le premier à arriver sur le site.

La ruse hittite a fonctionné et l'armée de Ramsès offre dangereusement l'occasion que Mouwatalli et ses généraux attendaient d'anéantir à jamais les désirs de conquête des égyptiens. Une victoire écrasante et au mieux la capture de pharaon déstabiliserait toute la région à leur profit et la conquête de l'Égypte ainsi affaiblie serait à portée de main.

Les troupes égyptiennes sont coupées en deux par la charge de l'armée hittite et Ramsès se retrouve seul face au danger. La division de Rê qui franchissait le fleuve est taillée en pièces par les chars hittites qui se retournent vers la division d'Amon et le camp de Ramsès, à peine installés au pied de la citadelle, déjà attaqués de leur côté par les fantassins de Mouwatalli. Le camp royal est investi et les troupes de pharaon battent en retraite, voire s'enfuient. Ramsès et sa garde rapprochée se jettent dans la mêlée et il adresse aux divisions de Ptah et de Seth restées en arrière des messages urgents, leur intimant l'ordre d'entrer dans la bataille.

Grâce à l'intervention conjointe des réservistes, les « Néarins », et de la marche forcée des contingents restés plus en arrière, il parvient à repousser l'attaque et à chasser les troupes de Mouwatalli au-delà de l'Oronte causant de lourdes pertes aux Hittites. Cependant, au contraire de son père et de son illustre ancêtre Thoutmôsis, Ramsès, dont les troupes sont affaiblies au lendemain de la bataille, ne s'empare pas de la citadelle et Qadesh reste aux mains des Hittites.

Ce haut fait d'armes – dont nous possédons plusieurs versions en égyptien, sur papyrus (le poème de Pentaour), mais surtout sur les grands tableaux historiés qu'il fait sculpter sur les murs des principaux temples du pays (Louxor, Karnak, Ramesséum, Abou Simbel...) – est considéré par le roi comme une grande victoire qu'il offre à Amon qui l'aurait alors secouru en plein désarroi et abandon au milieu d'un péril certain. Cette a servi à légitimer son règne, les premiers égyptologues ne remettant pas en cause sa victoire.

Les Hittites se déclarent eux aussi vainqueurs de leur côté, l'issue de la bataille ayant davantage l'aspect d'un "statu quo" que d'une débandade. Ramsès ne pousse d'ailleurs pas plus loin cet avantage annoncé, et préfère renforcer ses positions.

À l'issue de la bataille de Qadesh, un "statu quo" s'installe entre l'empire hittite et l'Égypte et la diplomatie reprend entre les deux rivaux. Cependant la situation ne semble pas à l'avantage des Hittites qui ne cherchent pas à engager un nouveau conflit direct avec Ramsès.

Les Égyptiens de leur côté doivent faire face à de nouvelles difficultés au sein de leur possession de Canaan où les royaumes d'Édom et de Moab se soulèvent, probablement encouragés par l'affaiblissement momentané de l'emprise égyptienne. En effet, la bataille de Qadesh avait momentanément porté un sérieux coup à la puissante armée égyptienne et en tout cas au crédit du pharaon sur la région.

Il est possible en outre que l'or hittite ait financé les désirs d'autonomie locale des deux royaumes. Ces troubles permettaient en tout cas d'éloigner les ambitions de Ramsès des terres hittites.

La réaction de Ramsès est aussi rapide que décisive à l'encontre des insurgés. La de son règne, il confie une partie de son armée à son fils aîné Amonherkhépeshef qui traversant le Neguev et contournant la mer Morte par le sud, se dirige droit contre Édom puis remonte sur Moab. Il met le siège devant la cité de Rabath Batora qu'il conquiert et y installe son camp de base.

De son côté Ramsès qui a quitté la capitale de Pi-Ramsès avec l'autre partie de son armée au même moment que son fils, longe la côte s'assurant le contrôle de Gaza et d'Askalon, puis bifurquant vers Jérusalem il marche contre Jéricho et contournant la mer Morte par le nord, pénètre en Moab. Il dépasse le Mont Nebo, conquiert la cité de Dibon et fait alors la jonction avec l'armée de son fils restée à Rabath Batora.

Grâce à cette technique de la tenaille, la conquête est rapide et le pharaon écrase les troupes des princes locaux qui lui font allégeance. Ramsès laisse des garnisons dans les cités prises, chargées d'organiser le contrôle de la région et de surveiller les mouvements des nombreuses populations nomades qui sévissaient alors, parmi lesquelles on compte les bédouins Shasou, vassaux des Hittites, et les Apirou qui opéraient de fréquentes incursions dans les territoires contrôlés par les égyptiens.

Une fois assuré de ses arrières et de son ravitaillement Ramsès peut alors reprendre la route de la Syrie afin de reprendre les territoires perdus et abandonnés aux Hittites à la suite de la bataille de Qadesh. Pharaon, son fils et leur armée rassemblée remontent alors vers le Mont Nebo et prennent Heshbon en Ammon. Enfin ils marchent sur Damas, l'antique "Temesq", où le roi fonde une nouvelle cité à son nom "Pi-Ramsès de la vallée des Cèdres".

Une fois le contrôle assuré de l'ensemble de cette partie de la Jordanie et de la Syrie actuelles, les troupes égyptiennes se dirigent à nouveau vers l'Oronte et atteignent la ville de Koumidi qui subit un siège et est capturée également.

Grâce à cette tactique de sièges successifs et de mise en coupe réglée des terres conquises, Ramsès a repris le contrôle de la situation au plus proche de ses frontières ainsi que sur toute la zone d'influence égyptienne en Orient. Il s'accorde ainsi un répit qui lui permet de se tourner à nouveau contre l'empire des Hittites.

À peine trois ans après le conflit qui faillit causer respectivement leur perte, les hostilités reprennent donc. Cette fois encore Ramsès cherche à pousser son avantage et à conquérir du terrain.

L'armée égyptienne reprend la route de la Syrie, contourne Qadesh par l'ouest et met le siège devant Dapour une autre forteresse contrôlée par les Hittites.

Il semble que Mouwatalli n'ait pas eu la capacité de contrer cette avancée sur son territoire même si de nombreuses troupes avaient été mises en garnison dans et autour de la citadelle. La bataille s'engage dans la plaine devant la cité et les chars hittites s'affrontent aux chars égyptiens.

Rapidement débordés, les Hittites se réfugient dans la forteresse qui est aussitôt attaquée par les fantassins égyptiens parmi lesquels on compte plusieurs fils du roi qui mènent le siège.

Des représentations de cette nouvelle bataille ont été gravées en relief sur les murs des temples de Ramsès en Égypte dont celui de Louxor et celui du Ramesséum. Ces tableaux présentent en une unité de scène les différentes étapes de la bataille et du siège depuis la bataille dans la plaine jusqu'à la reddition du prince de Dapour qui tend un encensoir en signe d'armistice.

Dapour est conquise et Ramsès y fait ériger une statue à son effigie, y installant également une garnison à demeure.

Cette prise représente pour Ramsès une revanche sur la semi-défaite de Qadesh. En tenant cette position plus septentrionale il démontrait sa capacité à prendre aux Hittites un point stratégique d'importance séparant l'Amourrou de leur emprise.

L'année suivante, pour consolider ses positions il organise une nouvelle campagne qui voit les troupes égyptiennes défiler dans les principales cités de la région prenant au passage Acre.

Tyr, Sidon, Byblos renouvellent leur allégeance et l'armée égyptienne faisant halte à Dapour nouvellement conquise, pénètre encore plus avant en territoire hittite conquérant la cité de Tounip.

Les Hittites ne pouvaient en rester là et quelques années plus tard ils reprennent la forteresse de Dapour, obligeant Ramsès à opérer une nouvelle campagne dans la région lors de la de son règne. La citadelle est à nouveau assiégée et conquise, et à nouveau cette victoire sera légendée en relief sur les murs des temples égyptiens.

Le conflit entre l'Égypte et le Hatti à défaut d'épuiser les belligérants, ne permet donc pas de dégager de nette victoire de l'un sur l'autre. On assiste au contraire à une succession de batailles qui permettent, soit à l'armée hittite, soit à l'armée égyptienne de "grignoter" du terrain, mais aucune grande bataille n'est engagée comme si la crainte d'une défaite et d'un affaiblissement décisif pour l'un ou l'autre des empires l'emportait sur les ambitions d'élargissement des possessions.

De plus, la situation intérieure de l'empire hittite se désagrège avec la mort de Mouwatalli dont la succession est rendue difficile avec l'usurpation du trône par Mursili, fils de l'adversaire de Ramsès. La montée de la puissance assyrienne représente de plus une sérieuse menace pour le Hatti qui cherche alors à faire alliance avec ses anciens ennemis à commencer par Babylone.
Il semble que ce soit les Hittites qui prennent l'initiative de soumettre à l'Égypte une véritable proposition d'alliance et de paix. Hittites et Égyptiens s'engagent à ne plus se faire la guerre, à s'aider mutuellement en cas de catastrophe ou bien d'invasion. Il s'agit sans doute du premier traité de paix connu au monde. Le traité définitif n’aurait été conclu qu’à la du règne de Ramsès, quand l’empire adversaire avait déjà changé de maître : Hattusil , frère de Mouwatalli, qui s’empara du trône, expulsant le fils de l’ancien souverain. Une fois les clauses du traité réglées, elles sont inscrites sur de grandes tablettes en argent massif scellées par Hattusil et remises par l'ambassadeur du Hatti à Ramsès dans sa capitale du delta du Nil. En échange, Pharaon fait parvenir au roi hittite la version égyptienne marquée du sceau de Ramsès.

Chacune des deux tablettes sera déposée aux pieds des principales divinités des deux empires : Teshub pour le Hatti et Rê pour l'Égypte. La version égyptienne de ce traité est reproduite sur les murs de Karnak et la version hittite retrouvée à Hattousa, la capitale du royaume hittite (dans l'actuelle Anatolie en Turquie), est écrite en langue akkadienne sur une tablette d'argile conservée au musée archéologique d'Istanbul.

Les négociations conduisent les deux souverains à s'envoyer un volumineux courrier ainsi que des cadeaux en grand nombre. À ce ballet épistolaire participent non seulement les souverains mais aussi les reines et les ministres, tel le vizir Paser. C'est alors qu'est évoqué un possible mariage entre Ramsès et une fille du roi Hattousili, acte diplomatique venant sceller définitivement la nouvelle alliance des deux anciens ennemis. Cette pratique est courante et Ramsès a déjà épousé une princesse babylonienne.

Cependant, la négociation du mariage est difficile en raison des garanties exigées par la femme d'Hattousili, Puduhepa, qui a, semble-t-il, une influence déterminante sur son époux. En particulier, elle exige que ses messagers puissent joindre la princesse sans entrave.

Ce problème réglé, des envoyés égyptiens viennent à Hattousa, la capitale hittite pour procéder à l'onction de la princesse, acte qui officialise l'union.

La princesse prend alors la route de l'Égypte avec sa dot. Elle rencontre Ramsès à Pi-Ramsès et semble-t-il plaît à son mari. Elle est renommée d'un nom égyptien, Maât-Hor-Néférou-Rê. Nous ignorons si elle eut la moindre influence sur la politique conduite par son mari ; cependant Ramsès fait construire pour elle un palais à Pi-Ramsès. Une fille, Néférourê, naît de cette union, fille dont nous perdons rapidement la trace.

Dans une lettre envoyée par Hattousili à Ramsès, le roi hittite regrette que sa fille n'ait pas conçu un garçon. La princesse termine probablement sa vie dans le harem du roi à Gourob dans le Fayoum. Sa tombe n'a jamais été retrouvée.

Ramsès épouse une seconde princesse hittite des années plus tard mais nous ignorons pratiquement tout du contexte qui préside à cette nouvelle union. Cependant ce fait est révélateur de la normalisation pacifique des rapports entre les deux États.

Originaire d'une famille du delta du Nil, Ramsès installe son palais et le centre administratif de l'Égypte à Pi-Ramsès, mais il a aussi besoin de continuer, comme son père, d'exploiter les ressources de la Nubie (plus au Sud) : l'or pour enrichir les temples, mais aussi pour acheter des alliances en Asie (l'empire hittite est ébranlé par la montée de la jeune Assyrie) ; du bois, dont le cèdre du Liban, mais aussi du cuir, du bétail et surtout des hommes pour l'armée.

Dès les premières années de son règne, —d'aucuns pensent à une corégence avec Séthi —, il intervient en pays de Ouaouat et de Koush, réduisant les désirs traditionnels de révolte des tribus soudanaises. L'exploit est relaté dans l'avant-cour du petit temple de Beit el-Ouali qu'il fit édifier en Basse-Nubie non loin d'Assouan.

Des carrières de la région, qu'il ré-exploite à grande échelle, il tire les grands obélisques et statues qui ornent ses monuments de Haute et Basse-Égypte, mais ne délaisse pas la ville d'Éléphantine et sa région.

Il organise alors un véritable programme architectural pour la région immédiatement au sud de la première cataracte qui est la frontière historique de l'Égypte avec son voisin méridional.

Il restaure bien sûr les forteresses entretenues depuis le Moyen Empire, à Bouhen, Semna et Kouma, mais fonde également une série de sanctuaires, que l'on nomme hémi-spéos, car pour partie creusés dans la roche et pour l'autre construits en maçonnerie, dédiés aux dieux dynastiques et étroitement liés au rôle de l'inondation, notamment :

Ramsès fut aussi un grand théologien, reprenant à son compte l'initiative solaire amorcée par Akhénaton, mais en préservant les cultes traditionnels. Voulant lui aussi développer au travers de sa propre personne une religion transfrontalière permettant de rassembler tous les peuples mis sous sa coupe, il favorisa au contraire les temples des grands dieux de l'Empire : Amon, Rê, Ptah, Osiris.

En effet, plutôt que d'effacer leur culte comme le fit à son péril Akhénaton, il les affirma dans leur rôle central dans la vie économique et spirituelle du pays, et instaura le sien propre, de son vivant, s'associant ainsi encore davantage que ses ancêtres aux dieux dynastiques et tout particulièrement au dieu Rê. L'exemple des temples de Nubie est parlant à ce sujet.

Partout il reprit l'initiative en redonnant aux temples et aux cultes des dieux un faste inégalé. Les innombrables fondations à son nom l'attestent et ses successeurs n'eurent qu'à parachever l'entreprise de leur prestigieux aïeul.

Enfin, conscient de l'emprise du dieu Amon-Rê de Thèbes et de son clergé sur le pays, emprise qui menaçait quelque peu le pouvoir royal, raison qui sans nul doute participa au choix de « l'hérétique » Akhénaton en son temps, il usa de stratégie en favorisant autant que faire se peut les temples de Ptah à Memphis et de Rê à Héliopolis. En retour, il donna des gages de sa bonne foi aux prêtres de Karnak en effaçant le souvenir de celui qui voulut leur perte, ainsi que de sa descendance.

Cette tendance avait déjà été amorcée par son père Séthi qui se fait représenter dans son temple d'Abydos en compagnie de son fils héritier devant une liste de rois représentant leurs ancêtres sur le trône d'Horus, liste de laquelle sont absents les rois d'Amarna, jusqu'à Horemheb, mais aussi Hatchepsout.

C'est de son temps également que les cultes des grandes villes du delta retrouvèrent leur importance, en instituant également de nouveaux, comme ceux des dieux orientaux tels que Baal, qui sera associé par syncrétisme à Seth, ou encore Astarté, Anta, Reshef, etc.

Ces cultes se retrouveront à cette époque dans toute l'Égypte, de Memphis à Thèbes (Deir el-Médineh), prouvant ainsi un brassage des cultures propre à une période de paix assurée.

Ramsès est un grand bâtisseur qui fait de Pi-Ramsès la « capitale » à l'est du delta du Nil, en la dotant de temples grandioses, d'un grand palais, d'un port et d'arsenaux, s'assurant ainsi un poste avancé pour préparer ses expéditions dans le levant, et régner sur un immense empire s'étendant de la quatrième cataracte en pays de Kouch jusqu'aux frontières du et du Mitanni sur l'Oronte.

Il achève ainsi de restaurer la grandeur de l'Égypte des perdue à la suite de l'aventure amarnienne. Grâce à une politique défensive efficace (il construit une série de forts à l'ouest du delta dont on a retrouvé les traces récemment), il offre une période de paix au pays favorisant ainsi le développement des arts et des métiers.

Il achève la grande salle hypostyle du temple d'Amon-Rê à Karnak, ajoute une grande cour à portique au temple d'Amon-Min à Louxor, ainsi qu'un grand pylône précédé de deux obélisques.

Il construit son temple funéraire, le Ramesséum, en face de Louxor, qui comprend deux pylônes précédant deux cours à portiques et une grande salle hypostyle. Diodore de Sicile nous donne une description fidèle de ce monument qu'il nomme alors le "tombeau d'Ozymandias", forme grécisée du nom de couronnement de Ramsès : Ouser-Maât-Rê.

Il fait également édifier un temple cénotaphe à Abydos non loin de celui de son père qu'il achève de décorer. Puisant dans les ruines de l'ancienne capitale d'Amarna, il rebâtit le temple de Thot d'Hermopolis, l'antique Khemenou, en réutilisant notamment les temples et bâtiments du site voisin.

Il construit également à Memphis, agrandissant le grand temple de Ptah avec l'adjonction sur son axe ouest d'une grande salle hypostyle précédée d'un pylône devant lequel il dresse des colosses, mais en édifiant aussi une série de temples et chapelles sur le parvis du sud de l'enceinte où il élève au moins un grand colosse à son effigie qui gît actuellement sur le dos (photo ci-dessous).

De même, il restaure également à Bubastis, où il refait ou décore la salle hypostyle du temple de Bastet. On y a retrouvé récemment un colosse à l'image d'une de ses épouses royales, qui aujourd'hui a été redressé et est visible dans le champ de ruine de la cité antique.

En revanche, il est établi aujourd'hui qu'il fait également enlever ou plutôt remplacer le nom de certains de ses prédécesseurs pour mettre le sien à la place quand il restaure leurs monuments. Ce trait particulier lui donne une réputation d'usurpateur tant nous possédons d'exemples de statues et monuments réinscrits à son nom. Si cette activité est quelque peu abusive, il convient de rappeler que de nombreux monuments et sanctuaires ont souffert dans les années qui précédent l'avènement de la et de ce fait nécessitent une restauration voire une reconstruction complète.

On peut voir ce type de « réaménagement » au temple de Louxor, où dans la cour qu'il fait édifier en l'honneur d'Amon-Min, il intercale des colosses entre les colonnes des portiques qui la bordent, certains sculptés sous son règne, d'autres « usurpés » d'Amenhotep.

Remplissant son rôle de garant de l'équilibre entre les hommes et les dieux, Ramsès se doit de rétablir les cultes et de les doter de biens permettant de les assurer dans tout le pays.

L'un de ses fils, Khâemouaset, grand prêtre de Ptah à Memphis et un temps héritier en titre de la Double Couronne, est chargé de cette mission, parcourant les sites délabrés et inscrivant des stèles commémoratives de cet exploit (voir par exemple la restauration entreprise sur la pyramide d'Ounas de la qui comporte sur son revêtement sud encore visible un texte du prince en l'honneur de son père et de son illustre prédécesseur).

C'est lui qui est chargé également de l'organisation des grandes fêtes jubilaires de Ramsès , les fêtes-"Sed", jusqu'à ce qu'il soit remplacé dans cette fonction par son frère Mérenptah. C'est pour l'occasion de ces jubilés qu'il fit bâtir un grand parvis à Pi-Ramsès qui comportait au moins six obélisques de grande taille.

Ramsès fit ériger des colosses à son effigie dans les "grands temples" construits ou restaurés.

Les plus célèbres sont ceux en façade des temples d'Abou Simbel, ceux qui encadrent l'entrée du pylône du temple de Louxor, le colosse couché à Memphis, ainsi que celui qui trônait depuis quelques décennies en plein centre du Caire, sur la place qui porte son nom devant la gare centrale et qui provient également du grand temple de Ptah.

Attaqué par la pollution, ce dernier a été transféré le vers Gizeh afin d'être installé au cœur du Grand musée égyptien actuellement en cours de construction.

Ramsès eut une fin de règne endeuillée par la disparition successive de ses héritiers et de sa grande épouse royale Néfertari. Il meurt après un règne de soixante-six ans, qui correspond à plus de la moitié de la .

Il est inhumé dans la tombe KV7 dans la vallée des rois qui n'est plus visitable actuellement tant elle est dégradée (car creusée dans une couche marneuse de la vallée, qui ne résista pas bien aux sporadiques mais dévastatrices inondations de l'oued asséché dans lequel fut choisi l'emplacement de la nécropole royale).

Des fouilles et une campagne de restauration sont actuellement en cours pour parfaire notre connaissance de la tombe royale. Le trésor funéraire de Ramsès a disparu depuis longtemps certainement à l'occasion de pillages qui eurent lieu à la fin du Nouvel Empire. Ainsi un braséro au nom de Ramsès a été retrouvé dans le trésor funéraire de Psousennès de la à Tanis, et les musées possèdent des ouchebtis à son nom, preuve caractéristique d'un pillage ancien.

De même, sa momie fut déplacée par les prêtres, d'abord dans la tombe de son père, puis à nouveau dans la tombe de la cachette (DB320) retrouvée à la fin du à la suite d'une enquête rocambolesque du tout jeune service des antiquités égyptiennes conduite par Mariette. En effet, dans les années 1870 à Paris et au Caire, apparaissent des antiquités égyptiennes portant les titulatures royales ; les égyptologues concluent que des trafiquants avaient secrètement découvert une nouvelle tombe. Mariette puis Gaston Maspero et ses collaborateurs remontent la filière des trafiquants jusqu’à deux frères, Ahmed et Mohamed Abd el-Rassul, bédouins sédentarisés probablement en cheville avec Mustapha Aga Ayat, agent consulaire de Grande-Bretagne, de Belgique et de Russie, pour faire passer à Paris les pièces qu'ils avaient pillé. Mohamed Abd el-Rassul accepte de coopérer et révèle la cachette à Deir el-Bahari. Brugsch, conservateur-adjoint du musée de Boulaq et collaborateur de Maspero, découvre cette caverne le : le tombeau contenait dont de divers pharaons du Nouvel Empire (parmi lesquels Séthi, Ahmôsis et Thoutmôsis), funéraires, des meubles et de la vaisselle funéraire... Les pièces furent envoyés au musée de Boulaq le .

Ramsès est retrouvé enveloppé dans des bandelettes posées par les prêtres de la , et réinstallé dans un sarcophage en bois de cèdre qui avait appartenu à Ramsès, son grand-père. Cela illustre combien la vallée des rois fut l'emprise de convoitises lorsque s'effondra l'Empire des . Le Pacha d’Égypte ordonne le déshabillage de la momie de Ramsès le au musée de Boulaq : lors de son débandelettage par Maspero, et le dégagement de ses bras, une tension post-mortem rejette l'un de ses bras soudainement dans un dernier geste, créant l'effroi et la fuite de l'assistance (notamment les ministres du pacha) venue admirer le spectacle, ce qui sera une des origines du mythe de la malédiction des momies égyptiennes. En 1907, Pierre Loti visite de nuit le musée de Boulaq et constate la dégradation de la momie de Ramsès laquelle subit sa première radiographie en 1912.

La dépouille (momifiée) de Ramsès est transférée au musée égyptien du Caire puis « soignée » dans les années 1970 car des champignons s'y étaient développés au contact de l'air moderne. L'égyptologue Christiane Desroches Noblecourt propose son sauvetage grâce à un laboratoire créé pour la momie lors de son exposition à Paris en 1976.

L'étude de cette dépouille au musée de l'Homme à Paris en 1976-77 a révélé que Ramsès était de haute stature, il mesure , roux et « leucoderme, de type méditerranéen proche de celui des Amazighes africains ».

Ramsès est également connu du grand public pour une toute autre raison : les traducteurs de la Bible et longtemps les historiens après eux, l'indiquent comme ayant été le pharaon régnant au moment de l'Exode, la fuite des Hébreux, qui a été évoquée aussi dans de nombreux films, comme "Les Dix commandements".

Cette hypothèse s'appuie sur l'argument suivant : la stèle de victoire de son successeur Mérenptah mentionne un peuple installé en Canaan. De plus, il est attesté selon les sources égyptiennes l'existence d'un haut fonctionnaire, Ben Azèn, de langue sémitique, qui serait intervenu dans un conflit opposant un groupe de nomades à des officiers royaux égyptiens. L'identification à Moïse peut sembler assez évidente.

Par ailleurs, la Bible mentionne que les Hébreux sont astreints à des corvées et construisent les villes de Pithom et Ramsès. Cette dernière ville apparaît ensuite comme le point de départ de l'Exode. Or, Ramsès est un grand bâtisseur et il entreprend au cours de son règne la construction d'une nouvelle capitale : Pi-Ramsès, non loin d'Avaris, l'ancienne capitale des Hyksôs, peuple de langue sémitique venu du Nord ayant pris le pouvoir et donnés plusieurs pharaons. Par conséquent, le règne de Ramsès semble fournir le cadre adéquat correspondant au récit de la Bible sur la sortie des Hébreux d'Égypte.

Cependant l'identification de Ramsès au pharaon de l'Exode se révèle moins évidente lorsqu'on y regarde de plus près. Aucun document provenant de ce règne ne peut être mis en rapport avec l'expulsion, la sortie, d'un peuple de langue sémitique du pays. Enfin le fameux Ben Azèn non seulement n'a jamais quitté l'Égypte mais a fidèlement servi les successeurs du roi jusqu'au règne de Ramsès. Toutefois, l'absence de trace de noyade sur la momie de Ramsès, mort nonagénaire, n'est pas un argument contredisant les versets bibliques. Le texte implique uniquement « "l'armée de Pharaon" » dans la noyade (Exode ch14, v28).

Concernant les localités mentionnées par le récit de la sortie d'Égypte, force est de constater que pour la plupart d'entre elles rien ne permet une identification spécifique à l'époque de Ramsès . L'itinéraire donné par le livre de l'Exode entre le point de départ, la ville de Ramsès, et l'engloutissement de l'armée égyptienne donne les villes suivantes : Sukkoth, Etam, Pi-Hahiroth, Mig-dol et Baal-Cefôn. Des sites comme Etam et Pi-Hahiroth sont inconnus (Etam est peut-être une déformation de Pithom) et Migdol est introuvable dans les textes égyptiens. Mais ce dernier est cité par Ezéchiel et Jérémie, ainsi que l'historien grec Hérodote, comme d'une ville située dans le delta du Nil où séjournent de nombreux juifs après la destruction de Jérusalem par Nabuchodonosor en -587. Le nom de Baal-Cefôn (Baal du Nord) est une divinité populaire dans la partie orientale de la mer Méditerranée vers la fin du y compris en Égypte. Il est plausible que le chapitre quatorze de l'Exode fasse allusion au temple de Tahpanès où selon Jérémie vit une importante communauté juive au avant notre ère. On le voit, la plupart des noms mentionnés s'expliquent dans le contexte plus récent des époques assyrienne, babylonienne et perse globalement du au s, période où les récits de la sortie d'Égypte sont mis par écrit.

Le pharaon de l'Exode ne porte pas de nom. Si les rédacteurs du texte biblique de cette période avaient considéré Ramsès ou un autre pharaon comme celui régnant rien ne s'opposait à ce qu'ils utilisent son nom, ce qui est le cas pour plusieurs autres souverains d'Égypte cités par la Bible. Il semblerait donc qu'ils aient considéré que l'indication de la construction d'une ville nommée Ramsès par une population d'esclaves juifs était suffisamment riche d'information. Il est impossible de s'appuyer uniquement sur les textes bibliques pour faire de Ramsès le pharaon de l'Exode. Quant à Manéthon, historien égyptien à l'époque ptolémaïque, il situe le bannissement des Hébreux sous le règne d'un certain Aménophis difficilement rattachable à un souverain particulier (peut-être Amenhotep).

La tombe de Ramsès se trouve dans Vallée des rois, dans la tombe KV7. Sa Momie a été transférée dans la tombe KV17 puis dans la tombe de la reine Inhapy à Deir el-Bahari (TT 320), découverte en 1881.

La tombe de Ramsès a été découverte en 1737 par Richard Pococke. Par la suite, elle été fouillé en 1825 par James Burton, en 1844/1845 par Karl Richard Lepsius, en 1913/1914 par Harry Burton, en 1938 par Charles Maystre et en 1993/2002 par Christian Leblanc.

La tombe de Ramsès a été ravagée par le temps. Outre les violations qu'elle a eu à subir dès la fin du Nouvel Empire, elle a été périodiquement inondée à la suite de violents orages qui se produisent régulièrement dans la région. Des pluies soudaines font alors se déverser dans l'oued de la vallée des rois de véritables torrents d'eaux mélangées à de la terre, du sable et de la roche formant une boue qui en s'introduisant dans l'hypogée ont peu à peu détruit toute la décoration du tombeau.

Depuis quelques années la "Mission pour la sauvegarde du Ramesséum", dirigée par Christian Leblanc, procède à la restauration de la tombe du roi, la dégageant de sa gangue de boue solidifiée et restituant des pans entiers de sa décoration retrouvés parmi les débris. De rares objets provenant du mobilier du roi ont pu aussi être retrouvés démontrant que la tombe avait été vidée de son contenu bien avant sa dégradation par les éléments naturels.

En face de la tombe de Ramsès, une grande tombe collective a été retrouvée dans la vallée des rois, la KV5, qui comprend de multiples chapelles et tombeaux des enfants royaux. Son exploration n'est toujours pas achevée.

La momie fut examinée en 1886 par Gaston Maspero et le docteur Fouquet, première investigation approfondie de la momie, ces investigations furent menées avec les moyens de l'époque : observation détaillée du corps, mensurations diverses.

En 1974, pour connaître les causes de la mort de Ramsès et d'autres momies, dont celle de Mérenptah, des recherches furent entreprises sous la direction de Maurice Bucaille avec des collaborateurs égyptiens puis une dizaine d'autres collaborateurs français de disciplines médicales diverses. Les résultats furent communiqués entre autres à l'académie de médecine et à la Société française de médecine légale. Son livre "Les Momies des Pharaons et la médecine" présente les résultats définitifs de ses recherches.

De nombreuses techniques modernes furent utilisées, explorations radiologiques et endoscopiques, investigations dans le domaine dentaire, recherches microscopiques, médico-légales, etc. Une trouvaille de grande importance grâce à l'utilisation de films radiologiques de très haute sensibilité permit de mettre en évidence l'existence d'une très grave lésion de la mâchoire de Ramsès , une ostéite étendue de la mandibule. Maurice Bucaille en conclut que ces lésions ont probablement été mortelles à condition que le roi n'eût pas d'autres maladies graves non décelées (à cause de l'impossibilité d'examiner les organes du thorax liée à la momification). La cause de sa mort serait donc une infection d'origine dentaire.

Maurice Bucaille a été par la suite sévèrement critiqué par la communauté scientifique car il partait d'un postulat pour arriver aux faits plutôt que de partir des faits pour "in fine" aboutir à une théorie. En effet celui-ci cherchait avant tout à prouver que Ramsès II était le pharaon de l'époque de Moïse.

Des études plus récentes ont montré que Ramsès II serait mort à plus de quatre-vingt-dix ans et souffrait avant sa mort d'artériosclérose, d'arthrose et d'une maladie rhumatologique, la spondylarthrite ankylosante. Il est aussi probable qu'il soit mort de vieillesse, vu son grand âge.

Le déformation du cou lié à sa maladie aurait obligé les embaumeurs à fracturer volontairement ses vertèbres cervicales afin de mettre sa tête en position horizontale.

La vie de Ramsès a inspiré de nombreux auteurs de fictions, dont Christian Jacq et sa série en cinq volumes "Ramsès" ou Anne Rice dans "The Mummy". La série de bande dessinée "Sur les terres d'Horus" se déroule sous son règne.

Dans le film "Les Dix Commandements" (1956), le personnage de Ramsès fut interprété par Yul Brynner. Ramsès apparaît aussi dans le dessin animé "Le Prince d'Égypte" (1998) qui traite également de la vie de Moïse. Dans le film "Exodus" de Ridley Scott, sorti en 2014, il est interprété par Joel Edgerton.
Dans la comédie musicale "Les Dix Commandements" (2000), Ahmed Mouici tient le rôle de Ramsès.





</doc>
<doc id="19274" url="https://fr.wikipedia.org/wiki?curid=19274" title="March On, Bahamaland">
March On, Bahamaland

<poem></poem>


</doc>
<doc id="19275" url="https://fr.wikipedia.org/wiki?curid=19275" title="Fête">
Fête

Une fête est une période de réjouissance collective destinée à célébrer quelque chose ou quelqu'un. Une fête est limitée dans le temps : il n'y a pas de fête solitaire ; les funérailles ne sont pas considérées comme une fête ; lorsque l'objet de la fête perd de sa motivation, celle-ci peut devenir un devoir ou une obligation sociale. Il y a des fêtes publiques, qui engagent une société tout entière et des fêtes privées limitées à une famille, à une corporation, à des clients, etc.

Étymologie : du latin "festa dies", jour de fête. Famille du mot : festif, férié, festin, festoyer, fêtard…

La plupart des fêtes publiques occidentales sont d'origine chrétienne, ou des fêtes plus anciennes que le christianisme a assimilées ; certaines fêtes sont d'origine civile. La tradition laïque a introduit le terme de "jour férié" pour désigner les jours de fêtes publiques reconnus par la loi, qu'ils soient d'origine chrétienne ou non.

L'héortologie est la discipline des sciences humaines qui étudie les fêtes de tous les points de vue : sociologique, philosophique, historique et théologique.

Selon Jean Duvignaud, la fête n'annonce pas un ordre nouveau, elle n'est pas la révolution. Elle est plutôt une parenthèse à l'intérieur de l'existence sociale et du règne de la nécessité. Elle est aussi ce qui peut conférer une raison d'être à la quotidienneté, d'où la tentation de multiplier les occasions de fêtes, au point, note Jean Duvignaud, que « certaines nations, certaines cultures se sont englouties dans la fête ». 

Elle est plutôt de l'ordre de ce que Sartre appelait l' « adhérence ». Tous sont censés participer d'un même élan, être emportés. La fête est un tourbillon qui semble abolir provisoirement les personnalités, mais donne pourtant à chacun l'occasion d'exprimer des désirs habituellement réprimés, serait-ce sur le mode de la farce. Ce paradoxe se comprend assez bien si l'on admet que la fête est sous le signe, non du Moi, mais du Ça. Il va généralement de soi que ce que l'on fait pendant la fête demeurera sans conséquences, précisément parce que l'on n'est pas censé être alors entièrement soi-même, il arrive que l'ivresse soit manifeste.

Néanmoins, selon Roger Caillois, c'est parce que sous nos climats l'ivresse et le masque ne vont guère de pair que nos fêtes ne prennent pas un tour plus violent. Personne ne peut alors prétendre incarner la violence légitime d'un dieu dont il porterait le masque. Au contraire, nos fêtes sont égalitaires, elles dénudent et démasquent par la dérision. Ailleurs, plus ritualisée, la fête n'est pas étrangère au tremendum, à l'épouvante caractéristique de la confrontation au Sacré que l'homme moderne ne connaît plus guère qu'au travers de certains films d'horreur. 

Roger Caillois, "Les Jeux et les hommes : le masque et le vertige" (1958)






Il existe de très nombreuses fêtes locales, certaines traditionnelles (carnavals), d'autres inventées à une période récente afin de commémorer un évènement (par exemple la libération de la commune en 1944), relancer le lien social, l'attrait touristique ou l'activité économique d'un lieu. Ces fêtes sont souvent organisées autour d'un thème particulier ou d'un produit local, et généralement par les municipalités.

Ces fêtes sont appelées au Japon "matsuri", et souvent liées à une célébration religieuse, shintoïste ou bouddhiste.

Au-delà des fêtes religieuses, il existe des fêtes familiales fixes (« des partys de famille » au Québec), certaines propres à chaque famille, d'autres de caractère universel : anniversaires, anniversaires de mariage, ou fêtes des saints dans les familles chrétiennes.

Certaines fêtes sont à caractère unique destinée à célébrer un évènement de la vie : pendaison de crémaillère, succès à un examen, mariage, naissance (souvent de facto célébrée de façon religieuse), etc.

Les fêtes d'anniversaires revêtent un caractère particulier: elles célèbrent les membres d'une famille ou d'une communauté. Elles sont l'occasion de rassemblements donnant lieu à toutes sortes de décorations. Les fêtes pour enfants sont particulièrement colorées et joyeuses avec toutes sortes d'accessoires et de décorations spécifiques. Sobres de par le passé, la tendance est de donner des fêtes toujours plus colorées, plus accessoirisées.

Des fêtes privées sont également mises en place dans les institutions : entreprises (inaugurations, lancements de produits, fête de l'entreprise, anniversaire de l'entreprise, départs à la retraite, etc.), associations, etc.

Des fêtes privées sont également organisées, sans la recherche de prétexte, pour le seul but de "« faire la fête »". C'est dans cette catégorie que se rangent par exemple, la rave party et la free party.





</doc>
<doc id="19280" url="https://fr.wikipedia.org/wiki?curid=19280" title="Jubilé">
Jubilé

Un jubilé est une fête marquant un intervalle de 50 ans et aussi l'anniversaire joyeux d'un événement dont les effets se prolongent dans le temps (règne, mariage, etc.).

Le mot jubilé vient de l'hébreu , terme dont la signification est incertaine. On considère généralement qu'il désigne un bélier puisque dans la Bible, une corne de bélier ("šôpar") est utilisée comme une trompette pour annoncer l'année du jubilé. Le terme est rendu en latin par (de , « se réjouir ») dans la Vulgate de Jérôme de Stridon. Cette année-là est une année de libération générale, les terres aliénées ou gagées devaient être rendues, les dettes remises et les esclaves libérés. Le Lévitique (25:8–13) déclare ainsi : 
Le livre pseudépigraphe des Jubilés utilise des jubilés pour dater les différents événements de l'histoire biblique.

Un « jubilé », appelé fête-"Sed" était fêté à partir de la de règne d'un pharaon. Ensuite, ces fêtes, aux vertus régénératrices, étaient célébrées avec un intervalle de quelques années en fonction de la situation politique du pays et de l'état de santé du pharaon.

Dans l'Église catholique, un « jubilé » est, depuis l'année 1300 (où le pape Boniface VIII proclama une indulgence plénière pour ceux qui viendraient en pèlerinage à Rome et y feraient 15 visites aux basiliques Saint-Pierre et Saint-Paul, ainsi qu'aux Romains qui visiteraient ces églises 30 fois), une période de pardon, de conversion et d'efforts spirituels ayant lieu, d'abord tous les , puis tous les à partir de 1400 (« pèlerinage panique » pour répondre aux grandes épidémies comme la grande peste), consacrée à la rémission par la pénitence des peines temporelles dues aux conséquences du péché et accompagnée par l'octroi d'indulgences spéciales associées à la visite de lieux saints, à la pratique du jeûne, de l'aumône et de la prière, spécialement la confession et la communion sacramentelle. On appelle aussi ces jubilés des « années saintes ». Au , dans un esprit similaire, les papes ont décrété des années saintes extraordinaires, commémorant la rédemption par la mort et la résurrection de Jésus-Christ. Le dernier jubilé a été célébré en l'an 2000 par le pape Jean-Paul. La dernière "Année Sainte extraordinaire" a été célébrée par le pape François en 2016: le Jubilé de la Miséricorde.

Le mot « jubilé » est aussi employé pour célébrer l'anniversaire d'ordination des diacres, prêtres et évêques.

Dans un contexte sportif, notamment au football, un « jubilé » correspond à une célébration pour rendre honneur à un joueur ayant longtemps rendu service à une équipe (voir l'article anglophone : ")."
Un jubilé a été célébré en l'honneur des footballeurs ou handballeurs suivants (liste non exhaustive) : 

Dans le domaine de la composition échiquéenne, le terme de « jubilé » est utilisé pour un concours de composition organisé à l'occasion de l'anniversaire d'un problémiste.




Le jubilé de l'espéranto a été célébré en 1987 à Varsovie lors du congrès mondial d'espéranto qui a réuni près de 6000 personnes. Il commémore la publication en 1887 par Ludwik Zamenhof de la première version de l'ouvrage Langue Internationale qui a défini les fondements de l'espéranto. À cette occasion a été créé le symbole du jubilé qui est depuis souvent repris comme symbole de l'espéranto.





</doc>
<doc id="19283" url="https://fr.wikipedia.org/wiki?curid=19283" title="Jean-Luc Fonck">
Jean-Luc Fonck

Jean-Luc Fonck, né le à Arlon en Belgique, est un chanteur-poète humoristique belge.

Il est le chanteur et membre le plus connu du groupe Sttellla.

Jean-Luc Fonck est le cadet de trois enfants, il a deux sœurs. De père en fils, depuis le grand-père, on est boulanger chez les Fonck. Jean-Luc vit à Arlon jusque l'âge de douze ans puis débarque à Bruxelles où il rencontre Mimi sur les bancs d'une école bruxelloise. Mimi et Jean-Luc font les quatre cents coups, elle pousse un peu la chansonnette et lui essaie tant bien que mal de jouer de la guitare.

. C'est en 1975 que Sttellla naît officiellement avec des potes de l'école (ils sont parfois neuf sur scène). . Il a acquis depuis une certaine maîtrise de l'art de l'humour par l'absurde.

C'est le contrat avec le label français « Boucherie Productions » qui fera de Sttellla une valeur sûre des festivals. Entre 1975 et 1993, le groupe donne plus de en Belgique, en France et au Québec.

Fin 1993, la rupture avec Mimi entraîne la fin de Sttellla sous forme de duo, et c'est en 1995 que Jean-Luc Fonck entame en Suisse sa première tournée en solo, puis plus tard accompagné d'un groupe, une tournée de deux ans et cent cinquante concerts qui ont réuni au total cent vingt mille personnes.

En 1996, Jean-Luc Fonck a permis la reformation du groupe belge Machiavel, lors des Francofolies de Spa (lors de l'événement « la fête à Jean-Luc Fonck de STTELLLA ») et conjointement à la sortie de leur Best-Of. En 1998, il participe au single "Le bal des gueux" d'Alec Mansion au profit de l’Opération Thermo, qui distribue des repas pour les vagabonds, dans les gares. Cette chanson est interprétée par trente-huit artistes et personnalités dont Toots Thielemans, Stéphane Steeman, Marylène, Armelle, Jacques Bredael, Lou, Alec Mansion, Muriel Dacq, les frères Taloche, Morgane, Nathalie Pâque, Frédéric Etherlinck, Richard Ruben, Christian Vidal, Marc Herman, Jeff Bodart, Jean-Luc Fonck, Benny B et Daddy K.

En 2003, Jean-Luc Fonck publie les "Histoires à délire debout", chez Casterman, dans la collection « C'est pour offrir » dirigée par son ami Philippe Geluck. Ces histoires sont de petites nouvelles, empreintes de poésie et teintées d'un onirisme qui se veut évoquer le monde du dessinateur Fred. Il déclare dans une interview : .

En 2004 et en 2006 paraissent deux autres volumes des histoires à délire debout ("Nouvelles histoires à délire debout" et "Histoires à délire debout, : prochaines histoires à délire debout"). Il signera également, en 2007, "Le Contrat" ; et, en janvier 2011, aux éditions Luc Pire, "Histoires allumées"

Jean-Luc Fonck fut également membre de l'équipe du "Jeu des dictionnaires" ainsi que de la défunte émission "La Télé infernale".

Tous les mardis, il a une chronique dans l'émission le 8/9 sur Vivacité, une radio de la RTBF.




</doc>
<doc id="19287" url="https://fr.wikipedia.org/wiki?curid=19287" title="Alexandre II (pape)">
Alexandre II (pape)

Anselme de Lucques est né vers 1010 ou 1015 à Milan et mort à Rome le , élu pape en 1061 sous le nom d'Alexandre II.

Né à , dans la région de Milan, il est formé à Cluny, aux côtés du cardinal Hildebrand (futur ) puis à l'abbaye du Bec dirigée par Lanfranc. Il commença sa carrière publique par la prédication. En Lombardie, il attaqua les mœurs du clergé, en particulier le mariage des prêtres. L'archevêque Guido, irrité, le dénonça auprès de l'empereur . Au lieu de le condamner, l'Empereur lui confia la prédication en Allemagne. En 1057, Anselme se vit confier l'évêché de Lucques. Par la suite, il fut nommé légat apostolique à Milan, la première fois en 1057 en compagnie d'Hildebrand, la seconde en 1059 en compagnie de Pierre Damien.

À la mort de , en 1061, il fut élu pape par le Sacré Collège (Élection pontificale de 1061), conformément au décret du feu pape. Une notification fut adressée à la cour de l'Empereur, qui l'ignora. Les cardinaux considérèrent que le privilège impérial de confirmation avait été abandonné. Anselme fut couronné pape sous le nom d' le 30 septembre. Furieux d'être dépossédés de leur ancien droit d'élection, les Romains portèrent leurs griefs devant l'impératrice Agnès, régente pour son jeune fils . Celle-ci convoqua une assemblée à Bâle qui, en l'absence de tout cardinal, élut un autre pape, qui prit le nom d'.

Alexandre II condamna la simonie et le nicolaïsme. Il appuya Guillaume le Conquérant dans la conquête normande de l'Angleterre, politiquement et en lui fournissant un étendard consacré ainsi que des reliques sacrées. Il refusa d'autoriser le divorce de et le força à reprendre sa femme, Berthe de Saxe. En 1063, il transforma la Reconquista en guerre sainte, par l'octroi d'une indulgence plénière aux soldats qui participeraient à la prise de Barbastro, ville d'Aragon tenue par les Maures.



</doc>
<doc id="19288" url="https://fr.wikipedia.org/wiki?curid=19288" title="Fête-Sed">
Fête-Sed

Dans l'Égypte antique, la fête-"Sed" ("heb-sed") était la fête de jubilé célébrée traditionnellement à partir de la trentième année de règne d'un pharaon. Elle fait partie de la tradition pharaonique qui débuta avec les premières dynasties (notamment sous Pépi) et perdurera au moins jusqu'à la .

Dès les origines de la monarchie pharaonique, la fête-Sed occupe une place importante dans l'iconographie, l'épigraphie et l'architecture. Sous la , les pharaons Den et Qaâ sont connus pour l'avoir célébrée. Au début de la , à Saqqarah, une partie du complexe funéraire de Djéser permet à l'âme du souverain défunt de bénéficier des bienfaits du jubilé. Dans la cour du Heb-Sed se trouve ainsi une estrade, un pavillon royal et les vestiges des chapelles du Sud et du Nord. Ces dernières étaient sans doute destinées à abriter les statues des dieux provinciaux. Sous la , les ruines du temple solaire du roi Niouserrê ont livré des scènes de ce rituel. Nombre des preuves documentaires sont aussi connues pour la . Sous le Moyen Empire, à Karnak, la chapelle blanche de Sésostris a été édifiée pour y effectuer des rites de substitution sur une statue du roi. Les témoignages les plus nombreux concernent toutefois le Nouvel Empire. Les fêtes jubilaires d' sont connues par les vestiges de son palais de Malqata, par les reliefs du temple d'Amon de Soleb et ceux de la tombe du majordome Khérouef (). Le jubilé précoce d'Akhénaton est renseigné par les blocs épars d'un temple qu'il fit édifier à Karnak. Les jubilés en l'honneur de Ramsès II sont eux aussi largement documentés, telle l'inscription laissée par son fils Khâemouaset à l'occasion de la fête-Sed de la. Le rituel est encore pratiqué sous la pour preuve les scènes du temple jubilaire d'Osorkon II dans le temple de Bastet à Boubastis. Sous la dynastie des Ptolémées le rituel ne semble plus être exécuté car tombé en désuétude. Cependant, ces souverains continuent à s'en réclamer dans leurs titulatures à l'image de Ptolémée V qui se dit « possesseur d'une durée trentenaire ».

Dans l'écriture hiéroglyphique, le nom du dieu Sed est déterminé par le glyphe du chacal debout. Son nom et son culte sont très anciennement attestés. Dès la , les annales du pharaon Den rapportent qu'en la de règne, la fête de la naissance de Sed a été célébrée. Plus tard, sous la , les annales de Khéops semble à nouveau nommer ce dieu mais la citation est malheureusement très obscure et lacunaire. Jusqu'à présent, les sources archéologiques ne font pas un lien direct entre le dieu Sed et la fête-Sed. La connexion entre cette célébration et le dieu chacal Oupouaout est plus solidement nouée. Son emblème apparaît à de nombreuses reprises dans la décoration du complexe funéraire de Niouserrê. À tous les étapes de la fête-Sed, l'emblème précède ou suit le souverain. Selon l'égyptologue Hermann Kees l'utilisation de deux formes d'Oupouaout, une pour la Haute-Égypte et une autre pour la Basse-Égypte, sont intentionnelles et servent à légitimer le pouvoir de pharaon sur les deux parties du royaume. Selon Wolgang Decker, le rôle du dieu est de servir d'éclaireur, son nom signifiant « l'ouvreur de chemin ».

En s'appuyant sur des comparaisons avec des rites jubilaires pratiqués en Afrique et sur l'interprétation des sources égyptiennes, l'origine du rituel serait à rechercher dans une antique chasse de qualification destinée à désigner le nouveau chef de clan, après avoir sacrifié l'ancien, devenu trop âgé pour assurer son rôle de chef de chasse.

Le sacrifice, théoriquement pratiqué au bout de trente ans de chefferie, se serait transformé en rite de régénération royal, succédant à une cérémonie d'inhumation d'une statue du pharaon, substitut symbolique du corps du vieux chef sacrifié.

Quant à la chasse de qualification, elle apparaîtrait dans les nombreux emblèmes et rites cynégétiques qui innervent la fête-Sed. L'omniprésence du dieu chasseur Oupouaout, anciennement nommé "Sed", confirmerait cette hypothèse.

Les documents faisant défaut pour comprendre le déroulement exact de la fête-"Sed", sa reconstitution reste encore du domaine de la simple supposition. Il est vrai que, sur cette cérémonie secrète et mystique, les prêtres ont été avares de renseignements. 

Après la trentième année de règne, cette fête aux vertus régénératrices, était célébrée généralement tous les trois ans (deux à quatre ans dans certains cas). Ainsi, le pharaon Ramsès aurait célébré en tout, à partir du grand jubilé de l'an, quatorze fêtes-"Sed" durant ses soixante-sept années de règne, avec, dans les dix dernières années, une fête-"Sed" tous les deux ans. Mais, en dehors de ce cas d'exception, il n'était déjà pas évident d'atteindre la première fête-"Sed". Certains pharaons enfreindront la règle des trente ans, notamment la reine Hatchepsout qui célébra sa première fête-"Sed" après « seulement » seize ans de règne. À noter que dans le cas de Hatchepsout, l'égyptologue Jürgen von Beckerath a émis l'hypothèse qu'elle aurait célébré sa fête de jubilé en cumulant aux siennes, les années de règnes de son père Thoutmôsis (environ treize ans) pour marquer la continuité (et donc la légitimité) de son règne.

Au-delà de cette fonction de jubilé, la fête-"Sed" était une cérémonie régénératrice que le pharaon pouvait organiser pour montrer à son peuple qu'il était capable de gouverner le pays. À certaines époques, et selon le pharaon, ces fêtes étaient l'occasion de démonstration physique du souverain (course à pied, capture de taureau, chasse au lion ou à l'hippopotame, etc.). Il est tout à fait possible que ces démonstrations n'aient été que symboliques, que le souverain ne les exécutât pas lui-même et qu'un autre les ait faites en son nom (comme c'était déjà le cas pour les cérémonies religieuses).

Le pharaon tirait des flèches en direction des quatre points cardinaux pour montrer que sa domination s'étend sur toute l'Égypte. Mais le rite essentiel de la fête-"Sed" est l'érection du pilier "djed", qui symbolise le dieu Osiris lors de sa résurrection. Seth, son meurtrier, ayant renversé ce pilier mythique, Pharaon a pour devoir de le redresser. Cette victoire sur Seth avait permis à Osiris de déclarer : « Je suis celui qui se tient debout derrière le pilier djed », devenant ainsi le pilier de l'Égypte et du monde.

Comme tous les êtres humains, Pharaon est soumis au vieillissement et à l'amoindrissement de ses forces. Cependant, en raison de sa proximité avec les dieux, il peut surmonter ces aspects néfastes grâce à des rituels de régénération dont il a le privilège. Dans la pensée égyptienne mais aussi dans bon nombre de royautés de l'Afrique pré-coloniale, l'harmonie du monde et la bonne marche du cosmos dépendent de la bonne condition physique du roi. Quelques régicides sont attestés sur des pharaons vieillissants. Cependant, durant la période antique, les Égyptiens n'ont pas fait le choix du meurtre rituel au bout d'une période déterminée du règne. Pourtant, il est généralement avancé que la Fête-Sed est un lointain écho de la mise à mort rituelle du chef de tribu vieillissant. Dès les débuts de la monarchie pharaonique, les souverains égyptiens ont pris pour habitude de célébrer au bout de trente ans cette fête jubilaire, ensuite répétée à des intervalles plus rapprochés ; généralement tous les trois ans.

Dans l'écriture hiéroglyphique, la fête-Sed se note avec un sigle qui représente deux estrades accolées. Chacune d'elles abrite un trône sur lequel Pharaon est censé apparaître, régénéré et glorieux, en tant que roi de Haute et Basse-Égypte. Comme cette fête jubilaire garantit à Pharaon l'éclat permanent de son gouvernement vis-à-vis des hommes et des dieux, elle s'inscrit dans le temps réel du règne mais aussi dans la durée éternelle de l'après décès. Dans de nombreuses scènes pariétales, les temples montrent les dieux offrir à Pharaon des millions de fêtes-Sed. Le souverain est agenouillé et il reçoit dans le creux de la main le hiéroglyphe de la double estrade suspendue à une longue nervure de . De ce fait, de nombreuses représentations ont une portée historique limitée. Le don du jubilé par un dieu n'indique pas forcément que le pharaon a effectivement atteint les trente ans de règne et qu'il a réellement bénéficié des bienfaits du rituel. Dans ce cas précis, il s'agit plus d'une représentation d'un programme rituel dont le théâtre n'est pas de ce bas-monde mais dans les contrées céleste sur lesquelles Pharaon règne éternellement au cours de son existence post-mortem.

Malgré l'importance de la documentation égyptienne, il est très difficile de se faire une idée précise du déroulement de la fête-Sed qui semble s'étendre sur au moins cinq journées consécutives. Sa signification est plus profonde que la simple célébration de la longévité du roi. Dans son essence, il s'agit d'un rituel de régénération dans lequel la puissance magique et la force physique du pharaon en exercice sont renouvelés ainsi que ses relations avec les divinités et avec le peuple. Une série de rites en lien avec l'idéologie royale forme le cœur de la fête. Dans une vaste aire cultuelle sont aménagées des chapelles temporaires afin d'accueillir les statues des divinités provinciales de Haute et Basse-Égypte. Ces statues proviennent de toutes les régions du pays et semblent être rassemblées afin de rendre hommage au roi et de légitimer son autorité sur l'ensemble du royaume. De son côté, le pharaon rend la pareille en rendant hommage aux dieux par nombre de sacrifices et d'offrandes. Par là, le pharaon renouvelle les liens entre la royauté et la sphère divine garante de la fertilité des champs et de la prospérité des troupeaux. 

Une partie du jubilé réaffirme le pouvoir séculier du pharaon par un rite de revendication territoriale connue sous le nom de « dédicace du champ » qui n'est pas sans rappeler la circumambulation autour des murailles de Memphis lors du couronnement. Au sol, deux bornes délimitent un champ de course orienté sud-nord. Cet espace symbolise très clairement les limites territoriales du pays dans lesquelles est exercé le pouvoir pharaonique. Emmitouflé dans un manteau court, le pharaon est coiffé soit de la couronne blanche, soit de la couronne rouge, soit de la double couronne ; dans ses mains, il tient les sceptres "nekhekh" et "mekes". À quatre reprises, il se déplace à grandes foulées entre les deux bornes afin de réaffirmer ses prétentions territoriales sur le pays et sur l'ensemble de la création.

La course de la « dédicace du champ » n'est cependant pas le rituel central du jubilé. Dans l'écriture hiéroglyphique, le jubilé s'écrit avec le sigle de la "tjentjat" qui représente deux estrades accolées. Par ce moyen est signifié que l'acte rituélique central est le double couronnement de Pharaon, une fois en tant que roi de Haute-Égypte, une seconde fois en tant que roi de Basse-Égypte. Tout au long de la fête, une foule considérable assiste de près ou de loin à l'événement ; peuple, prêtres, dignitaires, princes, musiciens, danseurs. À cette occasion, le pharaon se doit de prouver qu'il est capable de nourrir un grand nombre de convives. Pour magnifier sa puissance, Narmer affirme avoir abattu , et capturés . Niouserrê affirme quant à lui avoir servi . Même en laissant de côté l'exagération, il n'est pas douteux qu'il fallait réaliser des préparatifs considérables. Il est aussi possible de penser qu'une partie de l'approvisionnement pouvait provenir de razzias effectuées dans les contrées voisines. Par des étiquettes de jarres trouvées dans les ruines du palais de Malqata, il est prouvé que le pharaon Amenhotep III a organisé de gigantesques banquets lors de ses trois jubilés en offrant vin, bière, viande, graisse, huiles aux participants. 

Lors de la fête-Sed, Pharaon traverse différents états d'être lors d'un parcourt mystique. Chaque étape est symbolisée par le port d'un costume spécifique. La procession inaugurale est un temps qui appartient encore à l'ancienne période du règne ; les trente années qui viennent de s'écouler. Durant ce déplacement, il porte son vêtement cérémoniel habituel, à savoir, un pagne ample. Par la suite, Pharaon connaît un passage semblable à la mort. Durant ce simulacre de décès, il porte un suaire qui le fait ressembler à la momie d'Osiris. Durant une longue phase, celle de la régénération, le souverain porte ensuite un manteau blanc et court qui recouvre l'ensemble du corps jusqu'aux genoux. Lors de la course de la « dédicace du champ », il porte cependant un pagne court, le "chendjyt", plus commode aux déplacements. À la fin du jubilé, Pharaon reprend son costume initial pour montrer que la seconde phase de son règne a débuté, identique à la première. 

Ces différents costumes cérémoniels apparaissent dans ce qu'il est convenu d'appeler les piliers « osiriaques ». Ces éléments décoratifs appartiennent à l'architecture des temples du culte royal édifiés durant les Moyen et Nouvel Empires. Par le moyen de statues colossales (hautes de à ) adossées à des piliers, Pharaon apparaît debout et statique les deux bras croisés sur la poitrine. Parmi les souverains ainsi statufiés figurent Montouhotep II, Sésostris, Hatshepsout, Thoutmôsis III et IV, Amenhotep III, Akhénaton, Ramsès II , III, IV et VI. Cette attitude n'est pas sans rappeler les figurations du dieu Osiris, surtout lorsque le pharaon est vêtu du suaire mortuaire qui le fait ressembler à une momie. Issu de ce rapprochement iconographique, la terminologie de pilier « osiriaque » est un héritage des premiers égyptologues (milieu du ). Elle est cependant incorrecte car ces piliers ne sont pas des représentations du souverain défunt mais des figurations du pharaon en état de léthargie lors de la fête-Sed. Aussi, serait-il plus correct de parler de piliers jubilaires. Outre le suaire osirien, les colosses peuvent montrer le pharaon dans ses autres costumes ; le pagne cérémoniel, le manteau blanc jubilaire ou le pagne "chendjyt". Le souverain peut même être entièrement nu et androgyne mais cette dernière représentation ne concerne que le seul Akhénaton. Les textes gravés sur les piliers attestent clairement du contexte jubilaire : , .


</doc>
<doc id="19290" url="https://fr.wikipedia.org/wiki?curid=19290" title="Flamand (dialecte)">
Flamand (dialecte)

Le terme « flamand » désigne un groupe de dialectes germaniques du bas-francique (les dialectes du néerlandais). Au sens strict, le terme désigne deux dialectes parlés actuellement en grande partie sur le territoire de l'ancien comté de Flandre: le flamand occidental et le flamand oriental. Au sens plus large, on utilise le terme « "flamand" » comme terme générique pour désigner l'ensemble des dialectes néerlandais parlés en Belgique (le brabançon (dont fait partie l'anversois), le Flamand Oriental, le Flamand Occidental et le limbourgeois), ou même comme synonyme, bien que linguistiquement incorrect, du néerlandais tel qu'il est parlé en Flandre (tout comme l'usage populaire du mot « hollandais » pour désigner le néerlandais aux Pays-Bas).

Le flamand est, au sens large et dans le domaine linguistique, un dénominateur de tout dialecte parlé quelque part sur l'ancien territoire du Comté de Flandre. Cela s'étend du sud des Pays-Bas (Zélande) au nord de la France. D'ouest en est, il en existe beaucoup de variétés locales dont la plus répandue est sans doute le flamand occidental, qui serait parlé par plus d'un million de personnes dans l'extrême ouest de la Belgique. Quelques tentatives marginales ont été entreprises pour établir celui-ci en tant que langue « catholique » à part entière sur base de la littérature de Guido Gezelle, mais celles-ci n'ont jamais abouti.

Le flamand occidental de France a cependant été codifié de manière traditionnelle, selon les usages du vieux flamand de la région, dans l'ouvrage de Jean Louis Marteel « Het vlaams dat men ouders klappen ». L'exemple le plus frappant est l'emploi de "ae" (prononcé /ɒː/ ou /ɔː/) qui correspond au néerlandais standard "aa" (prononcé /a:/). Il possède une grammaire propre ainsi qu'un dictionnaire dont les auteurs sont Arthur Fagoo, Joël Sansen et Philippe Simon, édité en 1985.

Il n'est pas et n'a jamais été la langue officielle des Flamands en Belgique, rôle réservé au néerlandais dont l'usage et les règles reposent sur une convention (Nederlandse Taalunie) avec les Pays-Bas et le Suriname.
Cependant, le mot "flamand" est resté d'usage relativement courant en Belgique pour désigner (incorrectement) le néerlandais se référant tout de même à un néerlandais avec une prononciation spécifique "flamande".

Il existe au moins depuis le des dictionnaires et des grammaires en « proto-néerlandais » ou « proto-flamand », ces dialectes parlés dans la Flandre médiévale et qui étaient des prédécesseurs du néerlandais actuel. Différents auteurs utilisent indifféremment les termes "vlaems" (et ses variantes) ou "nederlands" pour la désigner. En français, car il existait aussi des lexiques et des dictionnaires bilingues, cela donnait « langue flamande » ou « thioise ». Le mot « néerlandais » n'apparaît que très tardivement au .

Il y a depuis le Moyen Âge une littérature en « proto-néerlandais », peut-être pas toujours de bonne qualité surtout au , mais toujours vivante à travers le théâtre et la poésie des chambres de rhétorique.

Il n'y a jamais eu dans l'espace néerlandais un centre culturel assez puissant pour imposer sa langue comme norme unique à la différence, par exemple, de Paris pour le domaine français. Néanmoins le néerlandais standard (langue Ausbau) est principalement fondé sur les dialectes de Hollande.

Aujourd'hui, bon nombre de Flamands parlent un parler dénommé "tussentaal", littéralement « langue entre les deux » (entre le dialecte local et le néerlandais standard). Ainsi, la télévision néerlandaise diffuse parfois les séries flamandes avec des sous-titres en néerlandais standard, comme elle le fait pour les séries étrangères diffusées en version originale (les séries anglaises par exemple). D'autre part, la télévision flamande diffuse parfois les séries néerlandaises avec des sous-titres en néerlandais également, afin d'améliorer la compréhension.

Entre le néerlandais parlé aux Pays-Bas et celui parlé en Belgique, certains mots et usages de mot sont différents. Voici une liste : le mot en néerlandais des Pays-Bas, puis le mot en néerlandais de Belgique et la traduction en français :




</doc>
<doc id="19291" url="https://fr.wikipedia.org/wiki?curid=19291" title="The Elder Scrolls II: Daggerfall">
The Elder Scrolls II: Daggerfall

"Daggerfall" est un jeu de rôle sur ordinateur où le joueur incarne un seul personnage en vue 3D subjective. Le joueur évolue dans le monde fictif de Tamriel, plus particulièrement dans la région de la Baie d'Iliac (qui rappelle le pourtour méditerranéen). De même que dans "" ou "", le choix du personnage et de sa conduite sont entièrement laissés à la discrétion du joueur. "Daggerfall" fut surtout influencé par les jeux de rôle sur table comme Donjons et Dragons et . A contrario, d'après Ted Peterson, "lead designer" du jeu, il a à son tour influencé un certain nombre d'autres développeurs, dont Richard Garriott et l'équipe de développement dUltima Online", pionnier du MMORPG.

Une des principales caractéristiques de ' est la richesse de son univers, tant du fait de sa profondeur que de son immensité : km² à parcourir d'après Bethesda, au moins personnages non joueurs (PNJ) avec qui dialoguer, et plus de villes, donjons et lieux divers à explorer.

Si Bethesda avance le chiffre de , et compare la superficie de la Baie d'Illiac à celle du Royaume-Uni (), les joueurs ont pu trouver des chiffres différents en essayant de calculer par eux-mêmes la taille réelle de la Baie. En octobre 2013, le développeur d'un projet de re-création haute-définition de Daggerfall explique que la carte du jeu, océan compris, fait × 500 pixels, chacun représentant une zone de de côté, soit une superficie totale de , c'est-à-dire une surface équivalente à celle de l'Allemagne.

L'histoire se déroule six ans après les évènements d"'Arena". Mandaté par l'Empereur lui-même, le héros est envoyé vers la Baie d'Illiac, bordée par les Provinces de High Rock et de Hammerfell. Sa mission : enquêter sur la mort de feu Lysandus, roi de Daggerfall, qui hante les rues de la capitale depuis qu'il a perdu la vie lors de la Guerre de Bretony. Le joueur doit également retrouver une mystérieuse lettre que l'Empereur avait envoyé à la Reine de Daggerfall, et qui n'est visiblement jamais arrivée à destination. Une seconde mission apparemment anodine qui cache en fait une gigantesque lutte d'influence pour la possession d'un antique artefact dwemer qui pourrait bien décider de l'avenir de l'Empire tout entier.

Selon Ted Peterson, . L'histoire a notamment été influencée par "L'Homme au masque de fer" d'Alexandre Dumas, ainsi que par les campagnes de jeux de rôle sur table que les développeurs menaient alors.

Il est possible de trouver de nombreux livres (environ 90 différents) à travers le Baie d'Illiac, distillant ainsi un "background" particulièrement touffu. La plupart de ces livres seront ensuite repris dans les épisodes ultérieurs de la série.

Le système de création de personnage de "Daggerfall", bien qu'influencé par GURPS, est en grande partie original. En effet, comme dans "Final Fantasy II", les compétences ne s'améliorent pas en accumulant de simples points d'expérience comme dans "Arena" (ou la majorité des jeux de rôle), mais en "utilisant" ces compétences. Par exemple : plus le héros se servira de son épée, plus il saura s'en servir efficacement. Ce système sera ensuite réutilisé quasiment à l'identique dans la plupart des jeux de la série, comme "", "" , "" ou Skyrim

Le joueur choisit son personnage parmi huit races (elfes, humaines, animales), et 18 classes (il peut également créer sa propre classe). Il ou elle sera ensuite défini(e) par huit caractéristiques et 27 compétences allant du combat avec divers types d'armes (épées, hache, arc, etc. chacune dépendant d'une compétence) jusqu'à la connaissance de l'étiquette (plusieurs compétences différentes sont utilisés en fonction de la classe sociale de la personne à qui on parle) en passant par diverses écoles de magie ou le crochetage des serrures. Le joueur dispose de 19 types d'armes différents, la plupart existant en dix matériaux spécifiques. Son armure est composée de sept parties de douze matériaux possibles. Si un voleur décide de porter la même armure qu'un barbare, il devra en subir les conséquences : escalader les toits sera alors dangereux, voire impossible. Divers bonus et malus de ce type permettent une gestion du personnage pointue et réaliste.

Livré à lui-même, le joueur peut se déplacer librement à travers toute la carte (même si des lieux « secrets » tels que des donjons ou les couvents de sorcières restent à découvrir). Il a pour cela le choix entre la marche, la course à pied, la nage, le bateau (petit ou grand), la carriole, le cheval, l'escalade, la lévitation… Pour s'y retrouver, une solution : le dialogue. Il est possible de discuter avec chaque passant, pour demander son chemin, des renseignements ou juste pour prendre des nouvelles. Même le ton choisi pour les conversations influence le déroulement du jeu.

Pour combattre, il est invité à « reproduire » le mouvement de l'arme, tout en maintenant le clic droit de sa souris, tout comme dans ' et plus tard dans '. Ainsi pour porter un coup d'estoc, le joueur doit « pousser » la souris en avant, alors qu'un mouvement de la droite vers la gauche par exemple, portera un coup de taille dans la même direction.

Les quêtes, très nombreuses (quelques extensions réalisées par des joueurs sont disponibles sur le Net), sont totalement indépendantes de la quête principale, elle-même rarement dirigiste. Ces quêtes, généralement délivrées par des notables, des marchands, ou par les nombreuses guildes existantes (officielles ou non, telle la Guilde des Assassins), permettront d'améliorer son personnage, de lui forger une réputation, de le faire grimper dans le rang des Guildes et accéder ainsi à de nouveaux privilèges.

Le joueur peut contracter de nombreuses maladies aux effets variés, comme la peste, mais également la lycanthropie, mi-homme mi-animal (vampire, loup, sanglier). Le commerce, marchander, vendre des objets d'art (tableaux…) n'est pas oublié (un commerçant miteux achètera les trésors à prix d'or par rapport à une boutique de luxe), ni la réparation des armes auprès des forgerons.

À condition de payer ou de faire partie d'une organisation qui les propose, des « itemmakers » sont disponibles, afin d'enchanter soi-même des objets sur mesure, avec la possibilité d'y emprisonner des âmes capturées au combat; de même les « potionmakers » permettent de concocter des recettes magiques et jouer à l'apprenti sorcier en mélangeant au hasard les ingrédients de son choix. Les « spellmakers » offrent quant à eux de créer ses propres sorts en plus de la centaine de sorts affiliés aux six écoles de magies.

Avant de prendre la direction qu'on lui connait, le deuxième épisode des "" était censé s'appeler Mournhold, et devait se dérouler dans la province des Elfes Noirs, Morrowind. Le projet fut ré-orienté, mais des années plus tard, le premier add-on de "", "", allait prendre place à Mournhold. Par ailleurs, on y retrouverait trois des personnages centraux de "Daggerfall" (Barenziah, Morgiah et Helseth) ainsi que la Confrérie Noire.

Au départ, "TES2" a été développé sur un moteur en 2,5D, à la façon d"'" ou de "Doom", avant que Bethesda ne décide finalement d'opter pour son tout nouveau XnGine, l'un des premiers véritables moteurs 3D.

Ces revirements ne sont d'ailleurs peut-être pas étrangers au fait que, comme "Arena", "Daggerfall" fut terminé dans l'urgence. Cela fut la cause d'innombrables bugs. En outre, de nombreux éléments, options, personnages et guildes n'ont finalement pas été utilisés, faute de temps.

À sa sortie, "Daggerfall" fut considéré par le sénateur américain Joseph Lieberman comme l'un des « dix jeux les plus susceptibles de corrompre la jeunesse ». Il contient en effet des scènes de nudité, de cruauté, de combat sanglant, etc.

Au départ, ' était un jeu encore plus ambitieux que ce qu'il fut finalement. Parmi les nombreux éléments abandonnés en cours de route, un certain nombre reste présent sur le CD du jeu, sous la forme de textes ou de graphismes non utilisés. À noter que le mod « DaggerXL » est un projet qui, outre la possibilité de jouer à TES2 sur des configurations récentes en haute-résolution, prévoit de réintégrer ces éléments dans la mesure du possible. Voici une liste des différentes fonctions connues auxquelles les développeurs ont renoncé.

Bethesda a travaillé par la suite sur un add-on pour ', intitulé '. Il fut finalement décidé d'en faire un jeu à part entière. Celui-ci est très semblable à "Daggerfall" dans ses mécaniques, à ceci près qu'il se déroule exclusivement dans un seul donjon, là où son prédécesseur faisait la part belle aux extérieurs et à la liberté de déplacement. Il utilise de plus une version « haute définition » du XnGine, initialement prévue pour la véritable suite de "", "", finalement repoussé de cinq ans et utilisant un moteur graphiquement complètement différent.

En France, "" a obtenu la note de 4 étoiles sur 6, avec la mention « Hit », dans le magazine "Gen4". Il a reçu la note de 89 % dans "Joystick" (n°76, novembre 1996, p.60).

. Mais que la taille de l'univers est inversement proportionnelle à sa richesse : deux villes de tailles et de situations géographiques semblables présenteront de très grandes similitudes, les donjons (à l'exception de ceux concernés par la quête principale) ont été créés aléatoirement, et les extérieurs sont plats, monotones, voire vides. De plus, en dehors de la trame principale, les quêtes sont générées à la volée, à partir d'une base de près de 300 quêtes déclinées à l'infini : il est courant de tomber deux fois sur la même, les seules variables étant la récompense en cas de réussite, le nom des personnes concernées et les lieux où elle prend place.

Lors de sa sortie, de très nombreux bugs ont été remarqués, mais la plupart ont été corrigés depuis par les développeurs grâce au patch 2.13.

Un patch amateur a traduit la quasi totalité du jeu en français.


</doc>
<doc id="19294" url="https://fr.wikipedia.org/wiki?curid=19294" title="Trébuchet">
Trébuchet

Le trébuchet fait partie des pièces d’artillerie médiévales dites à contrepoids. Il s’agit d’un engin de siège qui a été utilisé au Moyen Âge, soit pour détruire la maçonnerie des murs, soit pour lancer des projectiles par-dessus les fortifications. Il est parfois appelé « trébuchet à contrepoids » afin de le différencier d'une arme plus ancienne qu’on appelait « trébuchet à traction », une version primitive de l’engin où la force de propulsion était fournie par des hommes et non par un contrepoids.

Le trébuchet à contrepoids est apparu dans la première partie du dans les pays du pourtour méditerranéen à la fois dans les terres chrétiennes et dans les régions contrôlées par les musulmans. Il pouvait lancer des projectiles de trois cents livres () et les projeter à grande vitesse contre les fortifications ennemies. Dans certaines circonstances, des cadavres infectés par différentes maladies ont été catapultés dans les villes dans le but de propager des épidémies parmi les assiégés, il s’agit d’une variante médiévale de la guerre biologique. Le trébuchet à contrepoids est apparu en Chine vers le en Europe au de notre ère et il ne deviendra obsolète qu'au , bien après l'introduction de la poudre à canon. Le trébuchet est beaucoup plus précis que les autres catapultes du Moyen Âge. Au cours des croisades, Richard Cœur de Lion a donné des noms évocateurs et pittoresques aux deux trébuchets utilisés au cours du siège de Saint-Jean-d'Acre en 1191 : et . Les techniques de construction des trébuchets ont été perdues au début du . La première reconstitution moderne d'un trébuchet, selon des documents d'époque (1324), a été réalisée en France en 1984 par l'ingénieur Renaud Beffeyte (directeur de l’entreprise de charpenterie Armedieval).

En Chine, le trébuchet est déjà connu au , il est décrit dans le Mozi, un ouvrage portant le nom de son créateur, un philosophe de l'État de Lu, également pays et période du philosophe et stratège Confucius. Sous la Dynastie Song, le Wujing Zongyao comporte une illustration de trébuchets au , il s'agit alors d'une arme de siège, soit montée sur roues pour les déplacement terrestre, soit montée sur des bateaux de guerre, utilisés principalement sur les rivières, routes stratégiques dans la Chine de cette époque.

Le trébuchet est mentionné en France au . Les croisades furent vraisemblablement l'occasion qui poussa à développer ce type d’armes de siège. Le trébuchet est une variante du mangonneau en ce sens que son contrepoids, appelé aussi huche, est articulé. Ceci lui confère de nombreux avantages, notamment en ce qui concerne l’équilibrage de l’arme. Le trébuchet nécessite également moins de servants pour le manœuvrer.

L’âge d’or du trébuchet se situe au . Il fut notamment utilisé pendant la croisade des Albigeois, comme l’attestent les fouilles faites à Carcassonne et au château de Montségur par exemple. Le roi d’Angleterre Édouard fit construire le plus grand modèle jamais réalisé, qui servit pour la première fois au siège de Stirling en 1304. Ce trébuchet de grande puissance fut surnommé le "Warwolf", loup de guerre.

On pense communément que le trébuchet pouvait tirer des boulets pesant jusqu'à environ 120 kilogrammes sur une distance d’un peu plus de 200 mètres. Il fallait qu'il eût une portée minimale de 150-200 mètres pour se tenir hors de portée des archers assiégés. Une machine de telles dimensions fut surtout employée comme arme de siège. Elle était pointée sur un point précis des fortifications ennemies qui étaient alors bombardées. Le trébuchet constituait également une arme de dissuasion efficace. En effet, il suffisait parfois que les assiégeants commencent à ériger une machine pour que certaines places fortes capitulent.

On développa également plus tard une variante du trébuchet, plus légère et plus mobile, qui comportait deux contrepoids situés de part et d’autre de la verge et qui, de ce fait, portait le nom de couillard ou également de "biffa".

Le trébuchet fut graduellement remplacé par l’artillerie à poudre jusqu'à la cessation de son emploi au cours du . Les deux systèmes d’armement cohabitent en effet pendant environ trois siècles puisque l’artillerie à poudre fut introduite vers le début de la guerre de Cent Ans opposant les Français aux Anglais, et lors du siège d’Orléans. Il faut dire qu'à l’époque, le trébuchet était d’un maniement plus simple et surtout moins dangereux que l’artillerie à poudre.

L’une des dernières utilisations du trébuchet en tant qu'arme de siège à avoir été consignée par écrit date de 1521. Étant à court de poudre à canon, Hernán Cortés fit monter un trébuchet lors du siège de la capitale aztèque Tenochtitlán. On rapporte qu'il ne servit qu'une fois, à cause d’une défaillance de conception qui causa sa propre destruction lors du premier tir. Les sources ne permettent pas de vérifier s’il s’agit réellement d’un trébuchet ou d’une catapulte.

Ces armes à contrepoids tombèrent dans l’oubli jusqu'à ce que Napoléon, passionné du Moyen Âge en général et des armes de siège en particulier, tente la reconstitution d’une machine de siège en 1851.

La description du trébuchet par Viollet-le-Duc dans son encyclopédie médiévale telle que reproduite plus bas date également du .

Le trébuchet dérive de la fronde antique. Une variante de la fronde comportait une pièce de bois pour allonger le bras et exercer la force de propulsion sur un bras de levier plus grand. Cette évolution du trébuchet à traction qui vient des Chinois, consistait à regrouper un certain nombre de personnes pour tirer sur des cordes attachées à un bras de levier plus court alors qu’une fronde était fixée sur le bras long. Ce type de trébuchet est plus petit et présente une portée plus courte, mais c’est une machine plus facilement transportable et qui possède une cadence de tir plus rapide que les grandes machines, du type de celles qui fonctionnent avec un contrepoids. Le plus petit trébuchet à traction pouvait être animé par le poids et la force de traction d'une personne utilisant une seule corde, mais la plupart étaient conçus et dimensionnés pour être manœuvrés par 15 à 45 hommes, en général deux servants par corde. Ces équipes étaient parfois constituées d’habitants de la cité apportant leur aide pour le siège ou la défense de leur ville. Les trébuchets à traction avaient une portée de 100 à 200 pieds et pouvaient envoyer des projectiles dont le poids pouvait atteindre 250 livres. On pense que les premiers trébuchets à traction ont été utilisés par les mohistes en Chine dès le on peut en trouver des descriptions dans le " Mo Zi "(compilé au ).

Le trébuchet à traction est ensuite apparu à Byzance. Le "Strategikon" de Maurice, composé à la fin du mentionne des « balistes utilisables, dans les deux directions» (Βαλλίστρας έκατηρωθεν στρεφόμενας) et il s’agit probablement de trébuchets à traction (Dennis, 1998, ). L’ouvrage intitulé "Miracles de saint Demetrius", composé par Jean I, archevêque de Thessalonique, décrit clairement les trébuchets à traction de l'artillerie avaro-slave : « Suspendues à l'arrière de ces pièces de bois il y avait des frondes et à l'avant de solides cordes, au moyen desquelles, en tirant vers le bas et en libérant la fronde, ils propulsent les pierres vers le haut et avec un grand bruit ».

Il existe un doute sur la période exacte à laquelle les trébuchets à traction ont été connus et utilisés en Scandinavie. Les Vikings les ont peut-être connus à un stade très précoce, comme le rapporte le moine Abbo de Saint-Germain dans son épopée "De bello Parisiaco" datée de 890, à propos du siège de Paris (885 – 887), où des machines de guerre auraient été utilisées. Une autre source mentionne que les Normands ou les Scandinaves auraient utilisé des machines de guerre lors du siège d'Angers, en 873.

La première trace écrite et incontestable de l’existence d'un trébuchet à contrepoids provient d'un érudit musulman, Mardi bin Ali al-Tarsusi, qui a écrit un manuel militaire pour Saladin vers 1187. Il y décrit un trébuchet hybride dont il dit qu'il avait la même puissance de propulsion qu’une machine à traction tirée par cinquante hommes, en raison de « la force constante [de la gravité], alors que les hommes n’ont pas tous la même force de traction » (apportant la preuve de ses connaissances en mécanique, Tarsusi a conçu son trébuchet, afin que, lorsqu’il avait tiré, on puisse avoir recours à une arbalète de secours, sans doute pour protéger les ingénieurs d’une attaque).

Il aurait écrit « les trébuchets sont des machines inventées par des diables mécréants ». Cela donne à penser qu’à l’époque de Saladin, les musulmans étaient familiarisés avec les machines à contrepoids, mais qu’il ne croit pas qu’ils les aient eux-mêmes inventées. Al-Tarsusi ne précise pas que les « diables mécréants » sont des Européens chrétiens, bien que Saladin ait combattu les croisés pendant la plus grande partie de son règne, et que son manuscrit soit antérieur à l’apparition des armes chinoises et mongoles (Needham ). Il leur fallait une douzaine de jours pour construire ces machines suivant la taille de la structure.

Dans son livre, "Medieval Siege", Jim Bradbury cite abondamment Mardi ibn Ali à propos de divers types de mangonneaux, y compris les machines arabes, perses et turques, en décrivant ce qui pourrait être des trébuchets, mais différents de ceux qui sont cités ci-dessus. Dans "On the Social Origins of Medieval Institutions", on trouve des citations plus détaillées de Mardi ibn Ali sur les différents types de trébuchets, y compris les machines « chrétiennes » du type de celles utilisées par les Croisés.

La première reconstitution moderne d'un trébuchet, selon des documents d'époque (1324) a eu lieu en France en 1984 grâce aux travaux de l’ingénieur français Renaud Beffeyte, passionné par les machines de guerre médiévale et les techniques de siège.
L’entreprise de charpenterie Armedieval qu’il dirige est entièrement consacrée à la reconstitution d’engins de guerre anciens, très spectaculaires ainsi que le montre le trébuchet. Ses travaux comme ses études ont donné lieu à la publication de nombreux ouvrages sur le sujet. Il a fallu à Renaud Beffeyte près de deux ans de recherche et six mois de construction pour aboutir à la reconstitution complète en juin 1987 du trébuchet. Les centaines de tirs effectués par la machine permettent d’en appréhender la puissance. Avec une très grande régularité, celle-ci a catapulté à une distance de plus de des boulets de pierre pesant .
En 1998, Renaud Beffeyte a fabriqué en Écosse le "War Wolf" une machine également précise et performante. Les boulets lancés pèsent et atteignent les . Un documentaire intitulé "Le Trébuchet," qui a été diffusé sur la chaîne de télé américaine Nova et la chaîne France 5 en France, revient sur cette expérience ainsi que l’article publié en janvier 2000 en anglais dans le magazine "Smithsonian".

P.E. Chevedden précise que ses dernières recherches démontrent que les trébuchets ont atteint la Méditerranée orientale à la fin de l’an 500 et qu’ils étaient connus en Arabie et ont été utilisés avec beaucoup d’efficacité par les armées musulmanes. La sophistication technologique pour laquelle la civilisation islamique a été plus tard réputée, était déjà manifeste. Il dit en particulier que la littérature technique islamique a été négligée. Le plus important des traités techniques qui nous soit parvenu sur ces machines est le "Kitab Aniq fi al-Manajaniq" (كتاب أنيق في المنجنيق, "Un livre élégant sur les trébuchets"), écrit en 1462 par Yusuf ibn Urunbugha al-Zaradkash. Il s’agit de l’un des manuscrits arabes les plus abondamment illustrés jamais écrits qui fournit des informations détaillées sur la construction et l’utilisation de ces machines.

Chevedden déclare en outre : « Les ingénieurs ont augmenté l’épaisseur des murs pour résister à cette nouvelle artillerie et remanié les fortifications pour utiliser les trébuchets contre les attaquants. Les architectes travaillant sous les ordres d'al-Adil (1196-1218), frère et successeur de Saladin, ont introduit un système défensif qui utilisait des trébuchets fonctionnant par gravité et montés sur la plate-forme des tours pour empêcher l'artillerie de l'ennemi d’approcher suffisamment près pour que leur tir soit efficace. Ces tours, conçues avant tout comme des emplacements d'artillerie, ont pris des proportions énormes pour être compatibles avec les plus grands trébuchets, et les châteaux ont été transformés, de simples enceintes de murs avec quelques petites tours pour devenir un ensemble de grandes tours reliées par de courts tronçons de murs écrans. Les tours des citadelles de Damas, du Caire et de Bosra sont d’énormes structures, dont la superficie atteint carrés. ».

Au siège de Saint-Jean-d’Acre en 1191, Richard Cœur de Lion a fait construire deux trébuchets qu'il a appelé « God's Own Catapult » et « Bad Neighbour ». Au cours d'un siège du château de Stirling en 1304, Édouard a ordonné à ses ingénieurs de construire pour l'armée anglaise un trébuchet géant, nommé Warwolf (le loup de guerre). La portée et la taille des armes étaient très variables. En 1421, le futur Charles VII commanda un trébuchet ("couillard") qui pouvait lancer une pierre de , tandis qu’en 1188 à Ashyun, on a utilisé des rochers pesant jusqu'à . Le poids moyen des projectiles variait probablement autour de 50 à , avec une portée de . La cadence de tir pouvait être notable : au siège de Lisbonne en 1147 pendant la Reconquista, deux machines étaient capables de lancer une pierre toutes les 15 secondes. Des cadavres humains pouvaient aussi être utilisés dans certaines circonstances : en 1422 le prince Korybut, par exemple, au cours du siège du château de Karlštejn avait catapulté des hommes et du fumier à l’intérieur des fortifications ennemies, apparemment dans le but de propager des maladies chez les assiégés.

Les trébuchets à contrepoids n’apparaissent avec certitude dans les documents historiques chinois qu'en 1268, lorsque les mongols assiégèrent Fancheng et Xiangyang, bien que Joseph Needham ait avancé l'idée que Qiang Shen, un général mandchou de la dynastie Jin pourrait avoir inventé de façon indépendante un premier engin à contrepoids en 1232. Au cours du siège de Fancheng et Xiangyang, l'armée mongole, incapable de prendre ces villes, malgré un siège de plusieurs années imposé aux défenseurs de la Dynastie Song, avait engagé deux ingénieurs perses pour construire des trébuchets à contrepoids articulés et réduire aussitôt les villes en cendres, ce qui a contraint la garnison à la reddition. Ces machines ont été appelées par les historiens chinois Huihui Pao (回回炮) ("huihui" signifiant musulman, et "pao" signifiant canon) ou Xiangyang Pao (襄阳炮), car ils ont d'abord été utilisés au cours de cette bataille.

Les plus grands trébuchets exigeaient pour leur construction une quantité de bois énorme. Au siège de Damiette, en 1249 au cours de la Septième croisade, Louis IX de France a pu construire une palissade pour entourer complètement le camp des croisés avec le bois de 24 trébuchets égyptiens pris à l’ennemi.

Avec l'introduction de la poudre à canon, le trébuchet a perdu sa place de premier engin de siège au profit du canon. Les trébuchets ont été utilisés à la fois au siège de Burgos (1475-1476) et au siège de Rhodes (1480). Les dernières utilisations militaires du trébuchet dont on garde une trace écrite sont attribuées à Cortés, au cours du siège de la capitale aztèque Tenochtitlán en 1521, tentative qui s’est soldée par un échec, l’engin ayant été détruit au premier tir, et lors du siège (1572-1573) de la ville de Sancerre, pendant les Guerres de Religion, où la population à majorité huguenote a tenu bon pendant presque huit mois contre les forces catholiques du roi.

En 1779, les forces britanniques défendant Gibraltar, constatant que leurs canons étaient incapables de tirer assez loin pour atteindre certaines cibles, ont construit un trébuchet. On ne sait pas quelle a été son efficacité : les assaillants espagnols ont finalement été vaincus, mais leur échec est largement dû à une contre-offensive.

Le trébuchet est souvent confondu avec les premiers engins à torsion qui étaient beaucoup moins puissants. La principale différence provient du fait que les engins à torsion (par exemple l’onagre et la baliste) utilisent une corde ou une ficelle torsadée pour fournir l'énergie de propulsion, alors que le trébuchet utilise un contrepoids, en général beaucoup plus proche de l'axe que la charge pour des raisons mécaniques, mais pas obligatoirement. Le trébuchet possède aussi une fronde soutenant le projectile, et un mécanisme pour le relâcher au moment le plus opportun pour un maximum d’efficacité. Les deux types de machines, les trébuchets et les engins à torsion, sont désignés par le terme générique de « catapulte », qui englobe tous les engins destinés à lancer des projectiles sans l'aide de substances explosives.

Le trébuchet à bras flottant est une variante plus récente du trébuchet. La principale différence réside dans le fait que, plutôt que d'être fixé à la structure, l'axe est monté sur des roues qui se déplacent sur une trajectoire parallèle au sol. Cela se traduit par un mouvement du contrepoids selon un trajet plus court, lorsqu’il bascule vers le bas, ce qui augmente la quantité d’énergie communiquée au projectile et rend la propulsion plus efficace. Le plus souvent, on oblige le contrepoids à tomber verticalement en le guidant dans une fente verticale, s’assurant ainsi qu'il n'y aura pas de mouvement de va-et-vient pendant une partie du processus de lancement.

Si les dimensions et la masse de la machine peuvent grandement varier d’un modèle à un autre, il n'en demeure pas moins qu'un trébuchet de taille moyenne devait posséder les caractéristiques techniques suivantes :

Le nom de l’engin vient de l’occitan (langue romane d'Europe) "trebucca" qui signifie : « qui apporte des ennuis ». Il est fait d’un assemblage liant une perche à un contrepoids articulé appelé aussi huche. À l’autre extrémité était attachée une poche dans laquelle était placé le projectile, généralement un boulet de pierre taillée.

Le fonctionnement du trébuchet utilise le principe mécanique du levier pour propulser une pierre ou d'autres projectiles beaucoup plus loin et avec davantage de précision qu’une catapulte qui balaie le terrain. Le bras se balance jusqu'à la position verticale où, généralement à l’aide d’un crochet, l'une des extrémités de la fronde se détache, propulsant le projectile vers la cible avec une grande force.

Beaucoup de progrès ont été réalisés pour perfectionner le trébuchet. Les scientifiques s’opposent encore sur la question de savoir si dans l’antiquité on utilisait des roues pour absorber l'excès d’énergie cinétique et la transmettre au projectile. Il est connu que les poulies, tournaient souvent dans les deux sens pour donner de l’élan au projectile et pour le faire glisser, ce qui augmentait la précision du tir.

Le mangonneau possède une précision plus faible que le trébuchet (qui a été introduit plus tard, peu de temps avant la découverte et la diffusion à grande échelle de la poudre à canon). Le mangonneau lance des projectiles sur une trajectoire plus basse et à une vitesse plus élevée que le trébuchet dans le but de détruire les murs, plutôt que de lancer des projectiles par-dessus les fortifications.

"Note : Cette description reproduit celle de l’"Encyclopédie médiévale" de Viollet-le-Duc qui récapitule les connaissances théoriques sur le trébuchet à la fin du ."

Villard de Honnecourt nous donne le plan d’un de ces grands trébuchets à contrepoids, si fort employés pendant les guerres du et . Quoique l’élévation de cet engin manque dans le manuscrit de notre architecte picard du siècle, cependant la figure qu'il présente et l’explication aident à comprendre ces sortes de machines. Villard écrit au bas de son plan la légende suivante (traduite de l’ancien français) :
Le plan donné par Villard montre deux sablières parallèles espacées l’une de l’autre de huit pieds, et ayant chacune trente-quatre pieds de long. À quatorze pieds de l’extrémité antérieure des sablières est une traverse qui, à l’échelle, paraît avoir vingt-cinq pieds de long ; puis quatre grands goussets, une croix de Saint-André horizontale entre les deux sablières longitudinales ; près de l’extrémité postérieure, les deux treuils accompagnés de deux grands ressorts horizontaux en bois. C'est là un engin énorme, et Villard a raison de recommander de prendre garde à soi au moment où la verge est décochée.

Villard ne donne que le plan des sablières sur le sol, mais nombre de vignettes de manuscrits permettent de compléter la figure. Un des points importants de la description de Villard, c'est le cube du contrepoids.

Ces huches ne sont pas des parallélépipèdes, mais des portions de cylindres dans la plupart des anciennes représentations : or, en donnant à cette huche la forme indiquée dans la figure 1, et les dimensions exprimées dans le texte de Villard, nous trouvons un cubage d’environ cubes et en évaluant le poids du mètre cube de terre à , nous obtenons .
Pour faire mobiliser une pareille masse, il fallait un bras de levier d’une grande longueur : la "verge" était ce levier, elle mesurait quatre toises à six toises de long (de huit à douze mètres), se composait de deux pièces de bois fortement réunies par des frettes de fer et des cordes, et recevant entre elles deux un axe en fer façonné ainsi que l’indique le détail "A".

Les tourillons de cet axe entraient dans les deux pièces verticales "B", renforcées, ferrées à leur extrémité, et maintenues dans leur plan par des contre-fiches. En cas de rupture du tourillon, un repos "C" recevait le renfort "C’", afin d’éviter la chute de la verge et tous les dégâts que cette chute pouvait causer.

Voyons comment on manœuvrait cet engin, dont le profil géométral est donné par la figure 2. Lorsque la verge était laissée libre, sollicitée par le contrepoids "C", elle prenait la position verticale "AB". C'était pour lui faire abandonner cette position verticale qu'il fallait un plus grand effort de tirage à cause de l’aiguité de l’angle formé par la corde de tirage et la verge ; alors, on avait recours aux deux grands ressorts de bois tracés sur le plan de Villard, et reproduits sur notre vue perspective (figure 1).

Les cordes attachées aux extrémités de ces deux ressorts venaient, en passant dans la gorge de deux poulies de renvoi, s’attacher à des chevilles plantées dans le second treuil "D" (figure 2) ; en manœuvrant ce treuil à rebours, on bandait les deux cordes autant que pouvaient le permettre les deux ressorts. Préalablement, la boucle "E", avec ses poulies jumelles "F", dans lesquelles passait la corde de tirage, avait été fixée à l’anneau "G" au moyen de la cheville "H" (cf. détail "X").

La poulie "I" roulait sur un cordage peu tendu "KL", afin de rendre le tirage des deux treuils aussi direct que possible. Au moment donc où il s’agissait d’abaisser la verge, tout en étant ainsi préparé, un servant étant monté attacher la corde double à l’anneau de la poulie de tirage, on décliquait le treuil tourné à rebours, les ressorts tendaient à reprendre leur position, ils faisaient faire un ou deux tours au treuil "D" dans le sens voulu pour l’abattage et aidaient ainsi aux hommes qui commençaient à agir sur les deux treuils, ce qui demandait d’autant moins de force que la verge s’éloignait de la verticale.

Alors on détachait les boucles des cordes des ressorts et on continuait l’abattage sur les deux treuils en "ab" et "a’b’". Huit hommes (deux par levier pour un engin de la dimension de celui représenté figure 1), dès l’instant que la verge était sortie de la ligne verticale, pouvaient amener celle-ci suivant la position "A’B’". Le chargeur prenait la poche en cuir et cordes "M", la rangeait dans la rigole horizontale en "M’", plaçait dedans un projectile, puis, d’un coup de maillet, le décliqueur faisait sauter la cheville "H".

La verge, n'étant pas retenue, reprenait la position verticale par un mouvement rapide, et envoyait le projectile au loin. C'est ici où l’on ne se rend pas, faute de l’expérience acquise par la pratique, un compte exact des forces combinées, de la révolution suivie par le projectile, et du moment où il doit quitter sa poche.

Quelques commentateurs paraissent avoir considéré la poche du projectile comme une véritable fronde se composant de deux attaches, dont une fixe et l’autre mobile, de manière que, par le mouvement de rotation imprimé au projectile, l’une des deux attaches de la fronde quittait son point d’attache provisoire, et le projectile ainsi abandonné à lui-même décrivait dans l’espace une parabole plus ou moins allongée.

D'abord, bien des causes pouvaient modifier le décrochement de l’une des cordes de la fronde : la masse du projectile, son tirage plus ou moins prononcé sur l’une des deux cordes, un léger obstacle, un frottement. Il pouvait se faire ou que le décrochement eût lieu trop tôt, alors le projectile était lancé verticalement et retombait sur la tête des tendeurs, ou qu'il ne se décrochât pas du tout, et qu'alors, rabattu avec violence sur la verge, il ne la brisât.

En consultant les bas-reliefs et les vignettes des manuscrits, nous ne voyons pas figurer ces deux de fronde et l’attache provisoire de l’une d’elles, au contraire les brides de la fronde paraissent ne faire qu'un seul faisceau de cordes ou de lanières avec une poche à l’extrémité, comme l’indiquent nos figures. De plus, nous voyons souvent, dans les vignettes des manuscrits, une seconde attache placée en contrebas de l’attache de la fronde, et qui paraît devoir brider celle-ci, ainsi que le fait même la vignette 3 reproduite dans les deux éditions de Villard de Honnecourt, la française et l’anglaise.

Ici, le tendeur tient à la main cette bride secondaire et paraît l’attacher à la queue de la fronde. C'est cette bride, ce sous-tendeur, que dans nos deux figures 1 et 2 nous avons tracé en "P", le supposant double et pouvant être attaché à différents points de la queue de la fronde ; on va voir pourquoi.

Soit en figure 4 le mouvement de la verge, lorsqu'après avoir été abaissée, elle reprend brusquement la position verticale par l’effet du contrepoids et le projectile devra décrire la courbe "ABC". Or il arrive un moment où la fronde sera normale à l’arc de cercle décrit par la verge, c'est-à-dire où cette fronde sera exactement dans le prolongement de la verge qui est le rayon de cet arc de cercle. Alors, le projectile, mû par une force centrifuge considérable, tendra à s’échapper de sa poche.

Il est clair que la fronde sera plus rapidement amenée dans la ligne de prolongement de la verge suivant que cette fronde sera plus courte et que la masse du projectile ne sera pas lancée du côté des ennemis, mais au contraire sur ceux qui sont placés derrière l’engin.

Il y avait donc un premier calcul à faire pour donner à la fronde une longueur voulue afin qu'ayant à lancer une masse, elle arrivât dans le prolongement de la ligne de la verge lorsque celle-ci était près d’atteindre son apogée. Mais il fallait alors déterminer par une secousse brusque le départ du projectile, qui autrement aurait quitté le rayon en s’éloignant de l’engin presque verticalement.

C'était pour déterminer cette secousse qu'était fait le sous-tendeur "P". Si ce sous-tendeur "P" était attaché en "P’", par exemple, de manière à former avec la verge et la queue de la fronde le triangle "P’OR", la queue "OP’" ne pouvait plus sortir de l’angle "P’OR", ni se mouvoir sur le point de rotation "O".

Mais le projectile "C" continuant sa course forçait la poche de la fronde à obéir à ce mouvement d’impulsion jusqu'au moment où cette poche, se renversant tout à fait, le projectile abandonné à lui-même était appelé par la force centrifuge et la force d’impulsion donnée par l’arrêt brusque du sous-tendeur à décrire une parabole "C’E".

Si, comme l’indique le tracé "S", le sous-tendeur "P" était fixé en "P’’", c'est-à-dire plus près de l’attache de la queue de la fronde, et formait un triangle "P’’O’R' « dont l’angle "O’" était moins obtus que celui de l’exemple précédent, la secousse se faisait sentir plus tôt, la portion de la fronde laissée libre décrivait un arc de cercle "C’’C’’" », par suite du mouvement principal de la verge ; le projectile "C’’’’", abandonné à lui-même sous le double mouvement de la force centrifuge principale et de la force centrifuge secondaire occasionnée par l’arrêt "P’’", était lancé suivant une ligne parabolique "C’’’’E’’", se rapprochant plus de la ligne horizontale que dans l’exemple précédent.

En un mot, plus le sous-tendeur "P" était roidi et fixé près de l’attache de la fronde, plus le projectile était lancé horizontalement ; plus au contraire ce sous-tendeur était lâche et attaché près de la poche de la fronde, plus le projectile était lancé verticalement. Ces sous-tendeurs étaient donc un moyen nécessaire pour régler le tir et assurer le départ du projectile.

S'il fallait régler le tir, il fallait éviter les effets destructeurs du contrepoids qui, arrivé à son point extrême de chute, devait occasionner une secousse terrible à la verge, ou briser tous les assemblages des contre-fiches. À cet effet, non seulement le mouvement du contrepoids était double, c'est-à-dire que ce contrepoids était attaché à deux bielles, avec deux tourillons, mais encore souvent aux bielles mêmes étaient fixés des masses en bascule, ainsi que le font voir nos figures précédentes.

Voici quel était l’effet de ces masses "T". Lorsque la verge se relevait brusquement sous l’influence de la huche chargée de pierres ou de terre, les masses "T" en descendant rapidement exerçaient une influence sur les bielles au moment où la huche arrivait au point extrême de sa chute, et où elle était retenue par la résistance opposée de la verge. Les masses n'ayant pas à subir directement cette résistance, continuant leur mouvement de chute, faisaient incliner les bielles suivant une ligne "gh" et détruisaient ainsi en partie le mouvement de secousse imprimé par la tension brusque de ces bielles. Les masses "T" décomposaient, jusqu'à un certain point, le tirage vertical produit par la huche, et neutralisaient la secousse qui eût fait rompre tous les tourillons sans altérer en rien le mouvement rapide de la verge, en substituant un frottement sur les tourillons à un choc produit par une brusque tension.

Ces engins à contrepoids furent en usage jusqu'au moment où l’artillerie à feu vint remplacer toutes les machines de jet du Moyen Âge.

Le savant bibliophile M. Pichon possède un compte (attachement) de ce qui a été payé pour le transport d’un de ces engins en 1378, lequel avait servi au siège de Cherbourg.

Voici ce curieux document, que son possesseur a bien voulu nous communiquer :
Suit le compte des charpentiers, maçons, tendeurs, charrettes et chevaux. Cet attachement fait connaître l’importance de ces machines qui exigeaient un personnel aussi nombreux pour les monter et les faire agir.

Le chiffre de quarante tendeurs indique assez la puissance de ces engins : car à supposer qu'ils fussent divisés en deux brigades (leur service étant très fatigant, puisqu'ils étaient chargés de la manœuvre des treuils), il fallait donc vingt tendeurs pour abaisser la verge du trébuchet. Les maçons étaient probablement employés à dresser les aires de niveau sur lesquelles on asseyait l’engin.

Pierre de Vaux-Cernay, dans son "Histoire des Albigeois", parle de nombreux mangonneaux dressés par l’armée des croisés devant le château des Termes, et qui jetaient contre cette place des pierres énormes, si bien que ces projectiles firent plusieurs brèches.

Au siège du château de Minerve (en Minervois), dit ce même auteur, "on éleva du côté des Gascons une machine de celles qu'on nomme mangonneaux, dans laquelle ils travaillaient nuit et jour avec beaucoup d’ardeur. Pareillement, au midi et au nord, on dressa deux machines, savoir une de chaque côté. Enfin, du côté du comte, c'est-à-dire à l’orient, était une excellente et immense pierrière, qui chaque jour coûtait vingt-et-une livres pour le salaire des ouvriers qui y étaient employés."

Au siège de Castelnaudary, entrepris contre Simon de Montfort, le comte de Toulouse fit « préparer un engin de grandeur monstrueuse pour ruiner les murailles du château, lequel lançait des pierres énormes, et renversait tout ce qu'il atteignait… Un jour, le comte (Simon de Monfort) s’avançait pour détruire la susdite machine ; et comme les ennemis l’avaient entourée de fossés et de barrières tellement que nos gens ne pouvaient y arriver… » En effet, on avait toujours le soin d’entourer ces engins de barrières, de claies, tant pour empêcher les ennemis de les détruire que pour préserver les hommes qui les servaient.

Au siège de Toulouse, Pierre de Vaux-Cernay raconte que, dans le combat où Simon de Monfort fut tué, « le comte et le peu de monde qui était avec lui se retirant à cause d’une grêle de pierres et de l’insupportable nuée de flèches qui les accablaient, s’arrêtèrent devant les machines, derrière des claies, pour se mettre à l’abri des unes et des autres ; car les ennemis lançaient sur les nôtres une énorme quantité de cailloux au moyen de deux trébuchets, un mangonneau et plusieurs engins… »

C'est alors que Simon de Monfort fut atteint d’une pierre lancée par une pierrière que servaient des femmes, sur la place de Saint-Sernin, c'est-à-dire à cent toises au moins de l’endroit où se livrait le combat. Quelquefois les anciens auteurs semblent distinguer, comme dans ce passage, les trébuchets des mangonneaux. Les mangonneaux sont certainement des machines à contrepoids, comme les trébuchets, mais les mangonneaux avaient une masse fixe placée à la queue de la verge au lieu d’une masse mobile, ce qui leur donnait une qualité particulière.

Villard de Honnecourt appelle l’engin à contrepoids suspendu par des bielles, à contrepoids en forme de huche, trébuchet ; d’où l’on doit conclure que si le mangonneau est aussi un engin à contrepoids, ce ne peut être que l’engin à balancier, tel que celui figuré dans le bas-relief de Saint-Nazaire de Carcassonne et dans beaucoup de vignettes de manuscrits.





</doc>
<doc id="19302" url="https://fr.wikipedia.org/wiki?curid=19302" title="Brigitte Lahaie">
Brigitte Lahaie

Brigitte Vanmeerhaeghe, dite Brigitte Lahaie, est une actrice et animatrice de radio française née le à Tourcoing.

Elle fut l'une des premières stars du cinéma pornographique français, à l'époque de son « âge d’or » et de la libération sexuelle des années 1970 à 1980, durant laquelle des salles confidentielles passaient des films X tournés en 35 mm. Blonde sculpturale, Brigitte Lahaie fut l'une des égéries de cette période.

Après avoir arrêté le X en 1980, elle fait des apparitions dans le cinéma dit « classique » et intervient dans des émissions de télévision sur des sujets concernant la sexualité ou l'érotisme. En 2001, elle devient animatrice de radio, répondant aux auditeurs sur des questions de sexualité.

Elle est l'une des rares actrices X françaises à avoir réussi une reconversion professionnelle dans les médias grand public.

Fille d'un employé de banque et d’une représentante de commerce, Brigitte Lahaie exerce le métier de vendeuse de chaussures avant de déménager à Paris à l’âge de dix-huit ans et de commencer à poser nue pour des magazines.

Brigitte Lahaie commence sa carrière d'actrice pornographique à l'âge de vingt ans en 1976 et exerce ce métier jusqu'en 1980, tournant dans une centaine de films (certains étant seulement érotiques).

Durant cette courte période, elle a travaillé avec les principaux réalisateurs du cinéma X de l'époque : Claude Mulot ("Jouissances", 1976), Gérard Kikoïne ("Parties fines", 1977), Burd Tranbaree ("La Rabatteuse", 1978), José Bénazéraf ("Bordel SS", 1978) et Francis Leroi ("Je suis à prendre", 1978). Ces films ont été en majorité édités chez Alpha France et René Château Vidéo et réédités, pour certains, en DVD dans la collection « Les grands succès du cinéma X français » chez Blue One.

Elle exerçait sa profession à une époque où les réalisateurs savaient faire preuve d’inventivité et d'humour dans l’écriture de leurs scénarios, les scènes X elles-mêmes étant assez peu « anatomiques » et laissant place à une certaine mise en situation érotique. Ainsi, dans son dernier film, "Les Petites Écolières" de Claude Mulot (alias Frédéric Lansac) en 1980, elle joue le rôle d’une tenancière de maison close amenée à se reconvertir dans la direction d’un pensionnat pour jeunes filles. Bien entendu, l’éducation dispensée dans cet établissement porte sur des matières plus proches de la leçon de choses que du programme officiel de l’Éducation nationale.

Avant même l'abandon de sa carrière dans le X, Brigitte Lahaie a tenu des rôles dans des films non pornographiques, notamment des films fantastiques réalisés par Jean Rollin. C'est à l'occasion du tournage de "Vibrations sexuelles", un film X, que le réalisateur remarque sa différence, son charisme. Il lui confie donc un rôle dans "Les Raisins de la mort", tourné fin 1977. Reine du porno hardcore, elle est, dans ce film, opposée à Marie-Georges Pascal qui fut une vedette du cinéma érotique « soft » au début de la décennie. Rollin lui donne ensuite l'opportunité d'être la protagoniste de "Fascination" aux côtés de Franca Maï et de "La Nuit des traquées" avec également Catherine Greiner (Cathy Stewart).

Amorçant une reconversion professionnelle dans le cinéma dit « classique » à l'orée des années 1980, on la voit apparaître dans de petits rôles, souvent créditée du nom de "Brigitte Simonin" : une strip-teaseuse dans "I... comme Icare" d'Henri Verneuil en 1979, « une fille sans culotte » dans "Diva" de Jean-Jacques Beineix en 1980, une infirmière aux côtés d'Alain Delon dans "Pour la peau d'un flic" en 1981, une femme sexy en vidéo dans "Te marre pas ... c'est pour rire !" en 1982, et même une mère de famille dans "N'oublie pas ton père au vestiaire" avec Jean Lefebvre en 1982. Elle participe même à une pièce de l'émission grand public "Au théâtre ce soir".

Dans le même temps, elle continue à apparaître dans quelques films érotiques, dont "Joy & Joan" de Jacques Saurel en 1985 et "Le Diable rose" de Pierre B. Reinhard (avec Roger Carel et Pierre Doris) en 1987, et dans quelques films de genre : "L'Exécutrice" de Michel Caputo en 1986 dans lequel elle tient le rôle-titre d'une femme-flic. Elle joue ensuite dans deux films de Max Pécas ("Brigade des mœurs" en 1985, "On se calme et on boit frais à Saint-Tropez" en 1987). L'année suivante, elle tourne sous la direction de Jess Franco dans "Les Prédateurs de la nuit" puis "" (aux côtés de Christopher Lee). En 1990, elle tient de nouveau un petit rôle de prostituée dans "Henry et June" de Philip Kaufman, l'un des premiers films d'Uma Thurman. Puis, on la revoit dans "Illusions Fatales" en 1993 de Patrick Malakian avec Elise Tielrooy, "Les Deux Orphelines vampires" de Jean Rollin en 1997 et "La dame pipi" en 2000 de Jacques Richard tiré d'une nouvelle de Roland Topor. Elle retrouve ensuite Jean Rollin avec le film "La fiancée de Dracula" en 2002. Elle apparait également, en 2005, au cinéma, dans le film "Calvaire" de Fabrice Du Welz.

Si l'actrice est une des rares vedettes du porno qui soit parvenue à sortir du «ghetto du X », elle est restée pour l'essentiel confinée dans le cinéma bis et n'a fait que de petites apparitions dans des films « grand public ». Elle s'impose en tous cas comme une actrice-fétiche de Jean Rollin avec qui elle a travaillé à sept reprises.

Faisant feu de tout bois au terme de sa carrière d'actrice X, elle se fait remarquer par sa participation à l'émission radio, "Les Grosses Têtes", diffusée sur RTL en France.

Elle s'est également essayée à la littérature avec une autobiographie intitulée "Moi, la scandaleuse", ce qui lui permet d'être invitée a l'émission Apostrophes de Bernard Pivot le 27 mars 1987 et écrit deux romans érotiques publiés chez J'ai lu : "La femme modèle" (1991), qui raconte l'histoire d'une femme mariée entreprenant de poser nue, et "Le sens de la vie" (1994). On lui doit aussi quelques essais sur la sexualité : "Les chemins du mieux aimer" (J'ai Lu, 1999), "D'amour et de sexe" (Marabout, 2004), etc. En 2007 parait "Parlez-nous d'amour", en collaboration avec le Père Patrice Gourrier, proposant deux regards sur le couple, le désir et la sexualité.

Elle tente par ailleurs de percer dans la chanson en 1987, avec le titre "Caresse tendresse", qui passe cependant inaperçu.

À la fin des années 1990, Brigitte Lahaie anime quelques émissions sur des chaînes du câble français. Ces émissions sont le plus souvent centrées sur la sexualité. On la voit notamment sur la chaîne XXL spécialisée dans la pornographie.

C'est à la radio qu'elle retrouve un statut de vedette en tant que présentatrice sur l'antenne de RMC. De 2001 à 2016, la star y anime "Lahaie, l'Amour et Vous", une quotidienne, de 14 heures à 16 heures, qui traite des relations amoureuses et sexuelles, rediffusée à minuit. Durant l'été 2012, elle commente les épreuves d'équitation des JO de Londres pour RMC.

En 2016, après avoir été évincée de RMC, elle continue la radio en allant sur Sud Radio. Elle y anime une quotidienne, de 14 heures à 16 heures.

En , en plein débat sur les violences sexuelles à la suite de l'affaire Weinstein, elle fait partie d'un groupe d'une centaine de femmes signant une tribune dans "Le Monde" pour critiquer les dérives du mouvement #MeToo et de son équivalent français #BalanceTonPorc. Dans ce contexte, durant un débat sur BFM TV, elle déclare, lors d'un échange avec Caroline De Haas : . Plusieurs femmes ayant cosigné la tribune font alors savoir qu'elles désolidarisent de ses propos. Après le tollé suscité par sa déclaration, Brigitte Lahaie déclare le 12 janvier, sur TV5 Monde, que ses propos ont été mal compris et sortis de leur contexte. Elle explique avoir voulu non pas minimiser le viol, mais rappeler que ce qui d'ailleurs peut rendre . Elle déplore à cette occasion avoir fait l'objet d'un lynchage dans les médias et sur les réseaux sociaux, et commente : .

Brigitte Lahaie a été la compagne de l'éditeur René Chateau dans les années 1980.










</doc>
<doc id="19303" url="https://fr.wikipedia.org/wiki?curid=19303" title="Liste de fruits comestibles">
Liste de fruits comestibles

Cette liste énumère les fruits comestibles, classés par habitat et par ordre alphabétique. 

N.B. : Cette liste n'est pas exhaustive. 
Mais pour info elle ne l'est pas

Les fruits tropicaux sont caractérisés par leur intolérance au froid et poussent principalement dans les régions au climat tropical. Ces fruits sont désignés par « fruits exotiques » dans les pays où ils sont importés et consommés. 

Les légumes-fruits, consommés en tant que légumes, mais constituant le fruit de la plante :



</doc>
<doc id="19306" url="https://fr.wikipedia.org/wiki?curid=19306" title="Gaz parfait">
Gaz parfait

Le gaz parfait est un modèle thermodynamique décrivant le comportement des gaz réels à basse pression.

Ce modèle a été développé du milieu du au milieu du et formalisé au . Il est fondé sur l'observation expérimentale selon laquelle tous les gaz tendent vers ce comportement à pression suffisamment basse, quelle que soit la nature chimique du gaz, ce qu'exprime la loi d'Avogadro, énoncée en 1811 : la relation entre la pression, le volume et la température est, dans ces conditions, indépendante de la nature du gaz. Cette propriété s'explique par le fait que lorsque la pression est faible, les molécules de gaz sont suffisamment éloignées les unes des autres pour que l'on puisse négliger les interactions électrostatiques qui dépendent, elles, de la nature du gaz (molécules plus ou moins polaires). De nombreux gaz réels vérifient avec une excellente approximation le modèle du gaz parfait dans les conditions normales. C'est le cas des gaz principaux de l'air, le diazote et le dioxygène.

Sur le plan macroscopique, on appelle gaz parfait tout gaz vérifiant simultanément les :





Sur le plan microscopique, la théorie cinétique des gaz permet de retrouver ce comportement de gaz parfait : un gaz parfait est un gaz dont les molécules n'interagissent pas entre elles en dehors des chocs et dont la taille est négligeable par rapport à la distance intermoléculaire moyenne. L'énergie du gaz parfait est donc la somme de l'énergie cinétique du centre de masse des molécules et de l'énergie interne de chaque molécule (rotation, oscillation). Lorsque ces deux énergies sont proportionnelles, on a le gaz parfait de Laplace.

Comme pour tout gaz, l'état d'équilibre thermodynamique d'un gaz parfait est fixé pour formula_5 moles de molécules, par deux paramètres macroscopiques, au choix. Les autres paramètres peuvent se calculer à partir des deux paramètres choisis par l'équation d'état.

L'équation la plus couramment utilisée est l'équation des gaz parfaits. 

On peut l'écrire différemment, dans une approche plus microscopique où l'on considère le nombre de molécules contenu dans une unité de volume.
En thermodynamique, une autre version est couramment utilisée :
Dans ces expressions, 

Cette équation dérive d'autres lois trouvées auparavant : la loi de Charles, la loi de Boyle-Mariotte et la loi de Gay-Lussac.

Application numérique :
le volume molaire est 
On retient en général la valeur approchée 22,4 L/mol.

Ce qui donne un volume par molécule (volume « libre » autour de la molécule, indépendamment de sa dimension) :
si l'on assimile ce volume libre à un cube, alors l'arête de ce cube est globalement la distance moyenne séparant les molécules à chaque instant, que l'on appelle « longueur de Loschmidt » formula_28. Cette valeur est la racine cubique du volume « libre » :
on utilise en général la valeur approchée 3,33 nm.

Pour une pression valant un millionième de la pression atmosphérique (formula_25/), la distance interparticulaire est 333 nm = 1/3 micromètre et est indépendante de la nature du gaz.

On considère un gaz parfait constitué de N particules identiques. L'état de chaque particule est caractérisé par sa position formula_31 et sa quantité de mouvement formula_32. Chaque particule a une énergie cinétique proportionnelle au carré de sa vitesse formula_33, les particules peuvent échanger de l'énergie entre elles de façon conservative.

Étant donné une énergie totale formula_34 il existe de nombreuses configurations de l'espace des phases formula_35 correspondant à ce niveau d'énergie. 

On définit un volume élémentaire dans l'espace des phases formula_36, avec h égale à la constante de Planck pour être cohérent avec la mécanique quantique. 
Étant donné une plage d'énergie le nombre de configuration microscopique, micro-état, est égal au volume de cette plage d'énergie dans l'espace des phases divisé par le volume élémentaire et le nombre de permutations possibles entre les particules formula_37, les particules étant indiscernables.

Le nombre de micro-états correspondants à une énergie comprise entre formula_34 et formula_39 est 
avec formula_41 et formula_42 est un nombre fonction de N.

Avec formula_43 (énergie du niveau fondamental d'une particule dans une boite cubique)

La densité de micro-états à l'énergie formula_34 est donc

formula_45
On a donc le nombre de micro-états cherché :

On en déduit l'entropie du gaz en fonction de l'énergie :

formula_46

Les grandeurs thermodynamiques se calculent ensuite selon :

formula_47

formula_48

D'où la loi cherchée : 

Si la capacité thermique à pression constante formula_49 d'un gaz parfait ne dépend pas de formula_15, il en est de même de la capacité thermique à volume constant formula_51 en raison de la relation de Mayer. Le quotient formula_52 ne dépend donc pas non plus de la température : dans ce cas, le gaz parfait est dit "de Laplace". Pour un gaz parfait de Laplace, on a pour toute transformation :

avec formula_55 l'énergie interne du gaz et formula_56 son enthalpie, d'où :

Or, la définition de l'enthalpie permet d'écrire :

On en déduit que :

Des valeurs particulières de formula_60 modélisent le comportement de certains gaz. Pour un "gaz parfait monoatomique" formula_61, on obtient :

Le comportement de l'argon est très proche d'un gaz parfait monoatomique. Pour un "gaz parfait diatomique" formula_63, on obtient :

Le comportement du diazote N est proche d'un gaz parfait diatomique.

Dans une transformation adiabatique réversible élémentaire :

Ceci entraîne :

Soit en intégrant :

Compte tenu de l'équation des gaz parfaits, cette relation s'écrit également sous la forme :

Il en résulte, dans une détente adiabatique, un refroidissement considérable, le gaz prenant sur son énergie interne le travail, (formula_71), qu'il fournit.

Pour un gaz parfait monoatomique pour lequel formula_72, on trouve :

Soit pour une diminution de pression de 90 %, un abaissement de température absolue d'un facteur 0,398. De 300 K la température descend à 120 K soit une chute de 180 K. Ce procédé est utilisé dans l'industrie pour obtenir de basses températures. Cependant, la valeur ainsi obtenue ne reflète pas la réalité, car les gaz réels ne sont pas des gaz parfaits à basse température.

Pour les ordres de grandeur, on retient formula_74 2500 J/mol à 300 K. Le travail récupéré dans cette détente est 2500·(180/300) = 900 J/mol.

Une autre loi remarquable est l'échauffement produit quand on laisse pénétrer un gaz parfait de Laplace dans un flacon vide. Le gaz s'engouffre et puis très vite tout redevient chaotique : la température s'uniformise et devient
formula_76 étant la température externe.
Pour une température externe de 300 K et un gaz parfait monoatomique, on obtient :
soit une élévation de 200 K. Dans la soufflerie de Modane, c'est bien ce que l'on peut observer.

Ainsi, on a deux cas d'expansion du gaz.

Un cycle de Carnot moteur d'un gaz parfait a comme le veut le théorème de Carnot, le rendement de Carnot :

Dans le cas présent, tout peut se calculer aisément. Décrivons le cycle — on pourra le dessiner aisément en coordonnées de Clapeyron logarithmiques (formula_79, formula_80) :
Fin du cycle.

Puisque le gaz est revenu à son état initial, le premier principe de la thermodynamique nous dit que :
Le rendement du moteur est le travail récupéré formula_71 (donc égal à formula_92) divisé par la quantité de chaleur délivrée par la source chaude, soit formula_93 :

On démontre que :
d'où :
Ce qui nous donne la formule annoncée.

L'égalité de Clausius :
provient du fait que le cycle a été réversible : l'entropie totale est restée constante, celle du gaz est nulle car il est revenu dans l'état A. 
La source 1 a vu son entropie varier de formula_98, la source 2 de formula_99, d'où l'égalité.

Application numérique : même en prenant une eau de rivière à et une source chaude à , le rendement ne serait que 50 %. Sur un gigawatt électrique fourni par une centrale « réversible », doivent être consommés (en charbon, pétrole, méthane, ou mox nucléaire) dont ira à la rivière (élévation de température) ou dans l'atmosphère (chaleur de fumée, vapeur d'une tour de réfrigération). Si l'on considère que toute la chaleur va dans la rivière et que celle-ci a un débit /s : en 1 seconde, iront chauffer d'eau : soit une élévation de température de 10 (J)/4,18 (J/K) = .

Six centrales produisant donneraient une élévation de 6 × 2,4 = . En France, il est interdit de dépasser dans une rivière (obligation légale, pour la survie de la vie aquatique en zone tempérée) : l'été 2003 fut très chaud, il a fallu arrêter certaines centrales. Cette pollution thermique (formula_100 < 0) calculée par la formule de Carnot est la plus basse possible ; il s'agit en fait d'un minimum, la pollution thermique réelle est plus élevée. Encore avons-nous pris un rendement formula_19 = 0,5 ; la réalité est proche de 0,42.

Voici un troisième exemple, la détente de Joule et Gay-Lussac : le gaz parfait est contenu dans un flacon de volume formula_102 et est brutalement mis en contact avec un récipient vide de volume formula_103, où il s'engouffre partiellement. Quelle est la température finale en admettant que les parois n'absorbent aucune chaleur ?

Dans un premier temps, dans le second récipient, le gaz doit être plus chaud ; si formula_103 est très inférieur à formula_102, on doit retrouver formula_106, d'après l'expérience 2.

Dans le récipient 1, le gaz qui y reste s'est détendu, il doit être plus froid ; c'est ce que l'on constate. Mais après retour à l'équilibre thermique, comme un gaz parfait est un gaz de Joule, la température ne change pas.

L'énergie interne d'une quantité donnée d'un corps pur dépend de deux variables indépendantes (on choisit formula_15 et formula_2 dans ce cas). Donc sa différentielle est égale à:

Or l'énergie interne est constituée des énergies cinétiques microscopiques correspondant à l'agitation thermique et des énergies d'interactions microscopiques correspondant aux énergies de liaisons et d'interactions diverses. Dans le cas d'un gaz parfait, par définition, ces dernières énergies restent constantes (pas de réaction chimique ou à fortiori nucléaire et pas d'interactions entre particules du gaz parfait). Comme l'agitation thermique ne dépend que de formula_15, il s'ensuit que l'énergie interne d'un gaz parfait ne dépend que de formula_15.

Donc, pour un gaz parfait
Et on obtient 

Or
Il s'ensuit :

De même, pour la même raison, l'enthalpie d'un gaz parfait ne dépend que de T :

or pour formula_5 mol d'un gaz parfait

Donc

La différentielle de l'enthalpie serait égale dans le système de variables indépendantes (T,P dans ce cas) à:
avec
formula_121
et
formula_122 (Capacité thermique isobare du gaz parfait).

Il s'ensuit :

Par définition, la différentielle de l'entropie s'exprime par la relation :

Appliquons le premier principe :
d'où :

Dans le cas d'un gaz parfait l'énergie interne ne dépend que de T.
il s'ensuit:

En intégrant entre 2 états formula_132 et formula_133 :

Il est possible d'établir les relations à partir de l'enthalpie au lieu de l'énergie interne :

d'où

Pour un gaz parfait l'enthalpie ne dépend que de T.

d'où :

De façon similaire en intégrant entre 2 états formula_144 et formula_145 :

Pour déterminer la valeur de l'entropie d'un gaz parfait, il suffit alors de connaître une valeur de référence consignée dans les tables thermodynamiques, par exemple (S).

Pour un gaz parfait monoatomique, on préfère retenir la valeur de l'enthalpie libre
avec
où formula_150 est la longueur d'onde de Broglie ; elle est donnée par formula_151 où formula_152 est la constante de Planck et formula_153 est la quantité de mouvement :

On retrouve alors toutes les valeurs données dans les tables (par exemple pour l'argon, le néon…). Les calculs sont à peine plus compliqués pour les gaz diatomiques.

La capacité calorifique à volume constant d'un gaz parfait vaut

De même, la capacité calorifique à pression constante d'un gaz parfait vaut :

La relation de Mayer est la relation qui existe entre les capacités calorifiques molaires C et C, d'un gaz parfait.

D'où

Si l'on applique la formule de Clapeyron donnant la chaleur latente de dilatation "l", de transfert latent de chaleur de dilatation (voir "Coefficients calorimétriques et thermoélastiques"), et on trouve que le coefficient β d'augmentation relative de pression isochore vérifie :
donc :
et l'enthalpie vaut :

Les gaz parfaits font l'objet d'une théorie dite théorie cinétique des gaz expliquant les lois physiques qui les régissent.

Pour ce qui est d’un mélange de gaz parfaits, on dispose de la loi de Dalton :
où formula_167 désigne la pression partielle du gaz "i", c'est-à-dire la pression qu’aurait le gaz "i" s’il occupait seul tout le volume.

Soit un gaz parfait de molécules "A" occupant un volume formula_102, et un gaz parfait de molécules "B" de volume formula_103, séparés par une cloison. Ils sont en équilibre (même température formula_15 et même pression formula_25). Il n'est pas du tout évident que pour une même pression et une même température, le mélange obtenu en enlevant la cloison soit un système de même pression, de même température et de volume formula_172. À titre de comparaison, si l'on mélange 1 L d'eau et 1 L d'alcool, on obtient 1,84 L d'eau alcoolisée ; certes ce ne sont pas des gaz parfaits, mais cela montre que les propriétés ne sont pas toujours additives.

Considérons que les gaz sont chimiquement neutres, c'est-à-dire que les molécules de "A" et de "B" n'interagissent pas : d'après le paragraphe précédent :
Ainsi la température n'a pas changé et la pression sur les parois est due à "A" et "B", soit :
La pression totale est formula_179 : la pression n'a pas changé, la loi de Mariotte reste donc vraie. En revanche, il y a eu « perte d'information par mixage », il s'agit du théorème de Gibbs.

S'il n'y a pas interaction entre "A" et "B", l'énergie interne est simplement celle de "A" plus celle de "B" : la loi de Joule reste vraie. Donc le mélange se comporte comme un gaz parfait.

Le mélange de deux gaz parfaits de Laplace est un gaz parfait de Laplace, mais dont le facteur γ n'est pas la moyenne pondérée des γ : ce sont formula_51 et formula_49 qui sont les moyennes pondérées.

L'entropie d'un mélange est la somme des entropies de chacun des gaz pris sous sa pression partielle (cf. théorème de Gibbs). En conséquence, un mélange de gaz parfaits est une solution idéale. Les propriétés d'un mélange de gaz parfaits se calculent donc facilement à partir des propriétés de ses constituants à l'état de gaz parfaits purs.

À basse pression, tous les gaz peuvent être modélisés par un gaz parfait. Lorsque la pression augmente, on ne peut plus négliger les interactions à courte distance, notamment l'effet de taille des molécules et les interactions de type van der Waals.

Un gaz réel a un comportement voisin d'un gaz parfait dans le cas où les distances intermoléculaires sont grandes par rapport à la taille des molécules et à la portée des forces d'interaction. On appelle gaz parfait « associé au gaz réel » le gaz parfait dont la capacité calorifique à pression constante est celle du gaz réel à pression nulle formula_182. On dresse les tables thermodynamiques d'un gaz réel à partir des corrections apportées à ce gaz parfait associé.

Dans la limite des fortes densités, la nature quantique des atomes ou molécules de gaz doit être prise en compte. On peut donner comme critère que le modèle classique n'est valable que si la distance interatomique est très supérieure à la longueur d'onde de Broglie

Le gaz parfait relativiste est une généralisation du gaz parfait aux cas où les particules de gaz ont des vitesses proches de celles de la lumière. Il faut alors prendre en compte les effets de la relativité dans le calcul des différentes propriétés du gaz. Ainsi, si la loi des gaz parfaits reste valable, l'énergie et la chaleur spécifique sont modifiées par les effets relativistes.




</doc>
<doc id="19307" url="https://fr.wikipedia.org/wiki?curid=19307" title="Eon">
Eon






</doc>
<doc id="19308" url="https://fr.wikipedia.org/wiki?curid=19308" title="Quantité de mouvement">
Quantité de mouvement

En physique, la quantité de mouvement est le produit de la masse par le vecteur vitesse d'un corps matériel supposé ponctuel. Il s'agit donc d'une grandeur vectorielle, définie par formula_1, qui dépend du référentiel d'étude. Par additivité, il est possible de définir la quantité de mouvement d'un corps non ponctuel (ou système matériel), dont il est possible de démontrer qu'elle est égale à la quantité de mouvement de son centre d'inertie affecté de la masse totale du système, soit formula_2 ("C" étant le centre d'inertie). Son unité est le kg⋅m⋅s.

La notion de quantité de mouvement s'introduit naturellement en dynamique : en fait, la relation fondamentale de la dynamique exprime le fait que l'action d'une force extérieure sur un système conduit à une variation de sa quantité de mouvement : formula_3. Par ailleurs elle fait partie, avec l'énergie, des grandeurs qui se conservent pour un système isolé, c'est-à-dire soumis à aucune action extérieure, ou si celles-ci sont négligeables ou se compensent. Cette propriété est utilisée notamment en théorie des collisions.

En mécanique analytique ou quantique la quantité de mouvement apparaît naturellement comme la grandeur liée à l'invariance du hamiltonien ou du lagrangien dans une translation d'espace, c'est-à-dire à la propriété d'homogénéité de l'espace, qui est effectivement vérifiée en l'absence de forces ou champs extérieurs. Sur un plan plus général il s'agit en fait d'une des conséquences du théorème de Noether qui permet de relier symétrie continue d'un système et lois de conservation.

La notion d'impulsion ou "moment linéaire" généralise en mécanique analytique celle de quantité de mouvement, en tant que moment conjugué de la vitesse généralisée formula_4, soit formula_5. Quantité de mouvement et impulsion sont souvent confondues en raison de leur coïncidence dans la majorité des cas. Néanmoins ces deux grandeurs sont distinctes.

L'impulsion coïncide avec la quantité de mouvement en coordonnées cartésiennes ou plus généralement si formula_4 est la dérivée d'une variable linéaire, et non d'un angle et en l'absence de champ magnétique. Dans le cas d'une particule chargée en mouvement dans un champ électromagnétique, impulsion et quantité de mouvement diffèrent en raison d'un terme en formula_7 dû au potentiel vecteur, "q" étant la charge de la particule. L'analogue « angulaire » du moment linéaire est le moment angulaire généralement confondu avec le moment cinétique.

Il est aussi possible de définir la quantité de mouvement, plus souvent alors appelée impulsion, pour le champ électromagnétique. Le plus souvent, il est fait référence à la densité volumique d'impulsion du champ donnée par formula_8.

En mécanique relativiste, les notions de quantité de mouvement et d'énergie sont liées par l'introduction du quadrivecteur énergie-impulsion formula_9, où formula_10 est le facteur de Lorentz.

En mécanique quantique, la quantité de mouvement est définie comme un « opérateur vectoriel », c'est-à-dire comme un ensemble de trois opérateurs (un par composante spatiale) qui respectent certaines relations de commutation (dites "canoniques") avec les composantes de l'opérateur de position.

On trouve une première formulation de la quantité de mouvement chez Jean Buridan (1292 - 1363), dans ses "Questiones" sur la physique d'Aristote : L<nowiki>'</nowiki>"impetus" implanté augmente dans le même rapport que la vitesse. Quand un déménageur met un corps en mouvement, il y implante un certain "impetus". C'est une certaine force qui permet au corps de se déplacer dans la direction dans laquelle le déménageur démarre ce mouvement, qu'il soit vers le haut, vers le bas, vers le côté ou en cercle. C'est à cause de cet "impetus", dit-il, qu'une pierre se déplace après que le lanceur a cessé de la déplacer. Mais en raison de la résistance de l'air (et aussi de la gravité de la pierre) qui s'efforce de la déplacer dans le sens inverse du mouvement causé par l<nowiki>'</nowiki>"impetus", celui-ci faiblira tout le temps. Par conséquent, le mouvement de la pierre sera progressivement plus lent et, finalement, l'impulsion est tellement diminuée ou détruite que la gravité de la pierre prévaut et déplace la pierre vers son lieu naturel. On peut, dit-il, accepter cette explication parce que les autres explications s'avèrent fausses alors que tous les phénomènes sont d'accord avec celle-ci. L'impulsion implantée, on notera, est causée par la vitesse et supposée proportionnelle à celle-ci. Ailleurs, Buridan l'a considérée comme proportionnelle au poids du corps. Dans les unités correctement choisies. L'expression formula_11 reproduite par l'historien des sciences , donne un sens précis à l<nowiki>'</nowiki>"impetus", un concept qui était auparavant assez vague. Du point de vue formel, ce nouveau concept en dynamique est égal à la quantité de mouvement de la physique classique, mais en réalité, les deux sont très différents parce qu'ils jouent différentes parties dans leurs théories dynamiques respectives. Le point important est que dans son sens médiéval, le mot "impetus" est une force avec le même statut physique que la gravité, la légèreté, le magnétisme Néanmoins, la théorie pourrait bien avoir préparé la voie à la notion d'inertie qui le remplacera définitivement au .

Dans le "Discorsi e dimostrazioni matematiche intorno a due nuove scienze" de Galilée, la conservation du mouvement, pourtant pleinement reconnue et utilisée, intervient seulement au cours de l’exposé. René Descartes, mesurant toute sa portée, l’introduit comme une « loi de la nature » au seuil de sa philosophie naturelle. Toutefois les domaines d’application du système de Descartes reste la cosmologie philosophique. Il n'a pas la qualité d’une proposition scientifique, intrinsèquement liée aux conditions qu’exige une théorie géométrisée du mouvement. Par son association avec le concept d’une matière en soi indifférente au repos et au mouvement, Galilée est le précurseur direct du principe classique d’inertie, ouvrant la voie à une première théorie mathématisée du mouvement dont les résultats passeront intégralement dans la synthèse newtonienne.

En mécanique classique, la quantité de mouvement d'un point matériel de masse formula_12 animé dans un référentiel donné d'une vitesse formula_13 est définie comme produit de la masse et de la vitesse :
C'est donc, comme la vitesse, une grandeur vectorielle, dont l'unité SI est le kg formula_15 m formula_15 s.

Cette grandeur est "additive", ainsi pour un système matériel composé de "N" particules, la quantité de mouvement totale (ou "résultante cinétique") du système est définie par :

En introduisant le centre d'inertie "C" du système dont le vecteur position est par définition formula_18 il vient aussitôt par dérivation la relation:
autrement dit la quantité de mouvement totale du système est égale à la quantité de mouvement de son centre d'inertie "C" affectée de la "masse totale" formula_20 du système :

formula_21

Cette relation est valable pour tout type de système matériel, déformable ou non.

En mécanique du solide, la quantité de mouvement est la résultante du torseur cinétique.

La relation fondamentale de la dynamique exprime le fait que l'action d'une force fait varier la quantité de mouvement du point matériel dans un référentiel galiléen :

Cette relation se généralise aisément à un système matériel en ce qui concerne la quantité de mouvement totale du système, c'est-à-dire celle de son centre d'inertie "C" affecté de la masse totale du système :

Ce résultat est connu sous le nom de théorème de la résultante cinétique ou encore "théorème du centre d'inertie": il montre que pour un système matériel l'action des forces extérieures conduit à une variation de la quantité de mouvement du centre d'inertie du système.

En l'absence de forces extérieures, ou si leur résultante est nulle, la quantité de mouvement d'un système matériel est donc une constante du mouvement, puisque alors formula_24. En mécanique analytique cette loi de conservation peut être reliée à l'invariance par translation dans l'espace du Lagrangien, cf. ci-après.

Une illustration classique de la conservation de la quantité de mouvement est fournie par le pendule de Newton, qui est souvent utilisé comme objet décoratif (cf. illustration ci-contre). Une bille à une extrémité est lâchée sans vitesse et acquiert une certaine quantité de mouvement, puis entre en collision avec les autres billes accolées. La bille à l'autre extrémité repart dans le même sens que la bille incidente, ayant acquis sa quantité de mouvement, qui se « transmet » à travers les billes accolées.

De façon générale, la conservation de la quantité de mouvement est très importante dans l'étude des chocs de particules ou de la désintégration (séparation en plusieurs parties) d'un système. En effet dans le cas d'un choc de deux (ou plus) corps matériel, la durée de l'interaction entre les corps est très brève et il est possible de négliger l'effet des interactions extérieures au système constitué par les corps en collision, dont la quantité de mouvement totale peut donc être considérée comme conservée. Il est important de souligner que l'énergie cinétique n'est en général pas conservée dans une collision, car il y a souvent changement de l'état interne des corps durant la collision : par exemple deux particules qui restent accolées au cours d'une collision, ce n'est que si la collision est élastique que l'énergie cinétique est conservée, en plus de la quantité de mouvement (cf. illustrations ci-contre).

Deux exemples classiques permettent d'illustrer l'application de la conservation de la quantité de mouvement dans l'étude des chocs ou de la désintégration d'un système :
où formula_28 est la variation de la vitesse de la première boule pendant le choc. Si le choc est de plein fouet alors formula_28 et formula_30 sont colinéaires et alors la deuxième boule part à la vitesse de valeur formula_31. À la limite il peut y avoir transfert de la totalité de la quantité de mouvement de la première boule sur la deuxième et alors formula_32.
formula_13 et formula_35 désignant respectivement la vitesse de la balle et celle de l'arme juste après le tir.
Par suite, il y a un phénomène de "recul" de l'arme à la vitesse formula_36, ce qui est effectivement constaté. Le même phénomène intervient lorsqu'une personne saute d'une barque vers la berge : la barque « recule ».

De façon générale ce phénomène permet de comprendre le principe du moteur-fusée (cf. figure ci-contre) : l'expulsion d'une masse "dm" de matière à la vitesse d'éjection formula_37 pendant "dt" conduit du fait de la conservation de la quantité de mouvement (en négligeant l'action des forces extérieures) à faire varier la vitesse de la fusée spatiale de formula_38, par suite par intégration sur une durée finie formula_39 la vitesse de la fusée (de masse initiale "m" varie donc de formula_40 avec formula_41 puisque la fusée perd de la masse. Par suite la fusée se déplace dans le sens opposé à la direction de la vitesse d'éjection des gaz de combustion (cf. Équation de Tsiolkovski).

Une variation de quantité de mouvement consécutive à l'action d'une force est donc calculée comme étant l'intégrale de la force pendant la durée d'action de la force. Pour un objet de quantité de mouvement initiale formula_42 à un instant formula_43, qui subit une force formula_44 pendant une durée formula_45, l'intégrale de cette force par rapport au temps, pendant cette durée, est égale à :
En utilisant la relation fondamentale de la dynamique formula_47, on obtient :

L'usage, dérivé de l'appellation anglo-saxonne "impulse", est d'appeler cette grandeur "impulsion". Néanmoins, en toute rigueur, en français "impulsion" désigne le moment conjugué, grandeur de la mécanique lagrangienne. Lorsque la durée d'action de la force est très courte, la grandeur formula_49 précédente est appelée percussion mécanique, en raison de son importance dans la théorie des chocs.

En mécanique lagrangienne, l'état d'un système de "N" particules ("3N" degrés de liberté) est décrit par son Lagrangien noté formula_50, où formula_51 et formula_52 désigne les coordonnées et vitesses généralisées sous formes vectorielles de la "i-ème" particule ("i=1...,N").

Pour chaque particule il est possible de définir le "moment conjugué" (ou impulsion généralisée) de formula_51 par la relation:
Le symbole formula_55 désignant l'opérateur gradient évalué par rapport aux composantes de la vitesse généralisée formula_56 de la "i-ème" particule.

D'après les équations de Lagrange, qui s'écrivent avec les mêmes notations formula_57 il vient aussitôt formula_58, et si la coordonnée formula_59 est "cyclique", c'est-à-dire que le lagrangien "L" ne dépend pas de celle-ci, alors formula_60 et donc le moment conjugué formula_61 est conservé.

La notion de moment conjugué ne correspond pas en général à celle de la quantité de mouvement.

Par exemple, dans le cas du mouvement d'un seul point matériel dans un potentiel central "V(r)", ne dépendant que de la distance "r" à une origine "O", le mouvement est plan (2 degrés liberté) et le Lagrangien du système peut s'écrire aisément en coordonnées cylindro-polaires sous la forme :
et le moment conjugué de formula_63 est donc formula_64 qui est la valeur du moment cinétique de la particule (qui dans ce cas est conservée car "L" ne dépend pas de "θ").

Ce n'est que si les coordonnées généralisées coïncident avec les coordonnées cartésiennes (i.e. formula_65) ET en l'absence de champ électromagnétique que formula_66 et donc que le moment conjugué correspond à la quantité de mouvement de chaque particule. En effet dans ce cas les équations de Lagrange s'identifient avec celles données par la relation fondamentale de dynamique appliquée à chaque particule.

Si les coordonnées cartésiennes sont utilisées et que les particules, qui portent une charge formula_67 sont en présence d'un champ électromagnétique, défini par les potentiels scalaire et vecteur du champ notés formula_68, le Lagrangien du système fait intervenir le potentiel généralisé :formula_69,

et dans ce cas le moment conjugué s'écrit du fait des équations de Lagrange

Le moment conjugué formula_72 est alors dans ce cas désigné sous le nom d'impulsion pour le distinguer de la quantité de mouvement formula_73.

Une translation infinitésimale du système dans l'espace est défini par la transformation formula_74 appliquée à chaque particule, formula_75 étant le vecteur de translation élémentaire. Il est évident puisque formula_76 que cette translation laisse inchangée les vecteurs vitesses formula_77 des particules, qui coïncident avec les vitesses généralisées pour les coordonnées cartésiennes.

Si le Lagrangien du système est invariant par translation dans l'espace, alors nécessairement sa variation élémentaire formula_78 correspondante est nulle au premier ordre en formula_75.

D'après les équations de Lagrange, et en opérant en coordonnées cartésiennes, cette condition s'écrit sous la forme :

or la translation élémentaire formula_81 envisagée étant arbitraire, l'invariance par translation du Lagrangien implique que la quantité de mouvement totale du système formula_82 est conservée.

Ainsi la quantité de mouvement apparaît naturellement en mécanique analytique comme la grandeur conservée associée à l'invariance par translation du Lagrangien (ou du Hamiltonien), c'est-à-dire à la propriété "d'homogénéité de l'espace". Il s'agit d'un cas particulier du théorème de Noether.

Dans le formalisme hamiltonien la description de l'état du système à "N" degrés de liberté se fait en termes des "N" coordonnées et impulsions généralisées formula_83 et formula_84, qui interviennent dans l'expression du Hamiltonien formula_85 du système.

Il est possible d'introduire le crochet de Poisson de deux grandeurs arbitraires formula_86 et formula_87 fonction des coordonnées et impulsions généralisées, défini par :

Dans le cas particulier où formula_89 et formula_90 il vient formula_91: ce résultat permet de généraliser la notion de position et de quantité de mouvement en mécanique quantique, en permettant de définir par le principe de correspondance une relation de commutation canonique entre les deux opérateurs.

Dans le cadre de la description eulérienne des fluides, les équations sont généralement présentées sous forme locale (en un point). On s'affranchit alors de la notion de volume en définissant en tout point du fluide le vecteur quantité de mouvement parformula_92 avec formula_93 la masse volumique du fluide étudié au point formula_94 à l'instant formula_95 et formula_96 la vitesse de la particule de fluide se trouvant au point formula_94 à l'instant formula_95. Si le fluide est incompressible, formula_93 est constant dans le temps et dans l'espace.

Le théorème de la quantité de mouvement pour un fluide s'écrit :formula_100À noter que les forces exercées par l'extérieur sur le fluide sont de deux types : les forces à distance (volumiques) et les forces au contact (surfaciques) :formula_101Un exemple de force volumique est le poids et un exemple de force surfacique sont les forces de friction (on parle plutôt de viscosité).

La quantité de mouvement est une grandeur conservée lors de transformations de translation. Sinon, cela impliquerait une modification sans cause de la position du centre de gravité d'un système de deux corps élastiques qui se percutent.

Aussi, lorsqu'Albert Einstein formula sa théorie de la relativité restreinte, il adapta la définition de la quantité de mouvement afin que celle-ci soit également conservée lors de transformations relativistes.

En mécanique relativiste, il n'est pas possible de séparer les coordonnées d'espace et de temps, aussi il est utile de définir des quadrivecteurs comportant une composante temporelle et trois d'espace : ainsi le vecteur position formula_102 de la mécanique classique se généralise en un quadrivecteur position-temps formula_103.

Le carré de la norme d'un tel quadrivecteur est donné par formula_104 en utilisant la signature formula_105. Cette quantité est invariante par toute transformation de Lorentz.

Il est alors possible de définir le quadrivecteur vitesse en dérivant « par rapport au temps propre » de la particule formula_106 :
où formula_13 représente le vecteur vitesse « ordinaire » de la particule, et formula_109 est un facteur appelé "gamma relativiste" ou facteur de Lorentz, formula_110 étant la vitesse de la lumière. Le carré de la norme de ce quadrivecteur est donné par formula_111.

Le quadrivecteur impulsion-énergie qui généralise en mécanique relativiste la notion de quantité de mouvement s'obtient en considérant formula_112 par analogie avec la définition classique, ce qui donne formula_113, avec :

Le carré de la norme de ce quadrivecteur est la grandeur qui reste invariante lors d'une transformation de Lorentz, et qui est nécessairement égale au carré de la norme de formula_116 soit formula_117, par suite 

Les objets de masse nulle, tels que les photons, possèdent aussi un 4-moment où la pseudo-norme du quadrivecteur formula_119 est nulle. On a dans ce cas :

La notion de quantité de mouvement n'est pas limitée à un corps matériel, mais peut être étendue à un champ comme le champ électromagnétique, pour lequel elle porte plutôt le nom d'impulsion, pour éviter toute confusion. L'impulsion du champ électromagnétique correspondant à un volume "(V)" est donnée par :

La quantité formula_8 correspond à la "densité d'impulsion électromagnétique", c'est-à-dire à l'impulsion du champ électromagnétique par unité de volume. Elle est directement liée au vecteur de Poynting formula_124 puisque formula_125.

Il est possible de montrer que cette quantité correspond bien à la densité d'impulsion liée au champ électromagnétique en considérant son interaction avec les charges et les courants présents dans un volume "(V)" arbitraire, délimité par la surface fermée "(S)" : de par la conservation de l'impulsion du système global {charges + courants + champ e.m}, la variation des densités d'impulsions des charges et courants et du champ doit être égale au flux de densité d'impulsion à travers la surface "(S)".

L'interaction entre le champ et les charges et les courants fait intervenir la densité de force de Lorentz formula_126, or d'après les équations de Maxwell, il vient :
ce qui donne par substitution :
or d'après l'identité formula_130, il vient :
le terme de droite pouvant être rendu plus symétrique en utilisant les deux équations de Maxwell donnant la structure du champ :
ce qui donne finalement :
le terme de droite peut alors se mettre sous la forme de la divergence du tenseur des contraintes de Maxwell :

soit finalement :
cette dernière équation apparaît bien sous la forme d'une équation locale de bilan, le terme de gauche donnant la variation temporelle de la densité locale d'impulsion du système des charges et courants (formula_137) et du champ (terme en formula_138), le terme de droite correspondant aux échanges avec le reste. Ainsi, formula_139 peut être assimilée à la densité d'impulsion du champ électromagnétique.

En mécanique quantique, l'état d'un système à un instant "t" est décrite par un vecteur d'état noté formula_140 appartenant à l'espace des états formula_141 du système (celui-ci possède une structure d'espace de Hilbert). Les différentes grandeurs physiques usuelles (position, énergie, etc.) sont alors des opérateurs hermitiens, donc à valeurs propres réelles, appelé "observables".

La notion de quantité de mouvement d'une particule, plus souvent appelée "impulsion" correspond à un opérateur, en fait un ensemble de trois opérateurs correspondant chacun aux trois composantes d'espaces, dits opérateurs scalaires, qu'il est possible de regrouper, par analogie avec le cas classique en un opérateur dit "vectoriel", dit opérateur impulsion, noté formula_142.

Par définition, l'opérateur de position formula_143 et l'opérateur impulsion formula_142 sont des opérateurs vectoriels, dont les trois opérateurs scalaires agissant sur les différentes composantes correspondent aux diverses directions d'espace et obéissent aux relations de commutation canoniques suivantes :

La première relation de commutation se déduit formellement par analogie avec le crochet de Poisson formula_146 entre coordonnées et impulsions généralisées en mécanique hamiltonienne, en appliquant la prescription (principe de correspondance): formula_147.

La non-commutativité entre formula_148 et formula_149 ("idem" pour les autres composantes) implique "qu'il n'est pas possible de mesurer simultanément la position et la quantité de mouvement (et donc la vitesse) d'une particule". Il existe donc des inégalités, dites de Heisenberg, sur les écart-types moyens notés formula_150 et formula_151 de la mesure de chacune des deux grandeurs: formula_152.

La conséquence de ces relations est que la notion de trajectoire n'existe pas pour une particule quantique, la description est probabiliste.

De façon heuristique, cette situation peut aisément se comprendre. En effet si l'on cherche à localiser avec précision une particule, il faut utiliser une onde de courte longueur d'onde, donc de grande énergie. Or cette énergie va être nécessairement transmise, en tout ou en partie à la particule, modifiant de façon appréciable sa quantité de mouvement. Il sera possible d'utiliser une onde de plus grande longueur d'onde, mais alors l'incertitude sur la mesure de la position augmentera.

En représentation position, où l'état du système peut être décrit par sa fonction d'onde formula_153, l'opérateur position pour une composante "x" donnée correspond simplement à la multiplication de la fonction d'onde par celle-ci :

il est alors facile de vérifier que du fait de la relation de commutation canonique entre formula_155 et formula_156 la quantité de mouvement dans la direction formula_157, pour une particule sans charge électrique et sans spin, est donné par l'opérateur :
l'opérateur vectoriel de quantité de mouvement formula_142 s'écrit ainsi sous forme intrinsèque :

En représentation impulsion l'état du système est décrit par la fonction d'onde « en impulsion » formula_161, l'opérateur impulsion pour une composante "x" donnée correspond simplement à la multiplication de la fonction d'onde par celui-ci:

il est alors facile de vérifier que du fait de la relation de commutation canonique entre formula_148 et formula_149 l'expression de l'opérateur position formula_148, pour une particule sans charge électrique et sans spin, est donné par:
l'opérateur vectoriel de position formula_142 s'écrit ainsi dans cette représentation sous forme intrinsèque :

Les états propres de l'opérateur quantité de mouvement, c'est-à-dire les états pour lequel la quantité de mouvement de la particule a une valeur déterminée, sont donnés en représentation position à une dimension selon "x" par l'équation aux valeurs propres :
La valeur de formula_172 n'est pas quantifiée "a priori", sauf si des conditions particulières sont imposées à la particule, par exemple si elle confinée dans une boîte.
<br>
Ce résultat se généralise aussitôt à trois dimensions sous la forme formula_173, où formula_174 est le vecteur d'onde de la particule. Ces états ne sont pas normalisables au sens usuel (ce ne sont pas des fonctions de carré sommable), mais il est possible de les normaliser « au sens des distributions » :
Avec cette condition de normalisation il est possible de montrer que formula_176, en prenant pour convention de phase "C" réel et les états propres normalisés de l'opérateur impulsion s'écrivent ainsi en représentation position:

Pour un système stationnaire, l'opérateur hamiltonien du système s'exprime en fonction de l'opérateur quantité de mouvement : formula_178 (particule sans spin en l'absence de champ magnétique). En général du fait de la non-commutation entre opérateur impulsion et position, les états propres de l'impulsion ne sont pas états propres du hamiltonien.
Toutefois, dans le cas d'une particule libre formula_179 dans tout l'espace, et les états propres du hamiltonien sont ceux de l'impulsion, car alors formula_142 et formula_181 commutent entre eux. Les états propres d'énergie formula_182 ne sont donc pas quantifiés, et sont qualifiés de continus. Ils correspondent chacun à une valeur donnée de l'impulsion. Cette situation correspond en mécanique quantique à la conservation de la quantité de mouvement classique.

La fonction d'onde « complète » d'un tel système, c'est-à-dire la solution de l'équation de Schrödinger dépendant du temps, est alors donnée par formula_183, avec formula_184, fréquence associée à l'énergie "E". "Les états propres ont donc la forme d'ondes progressives", traduisant sur le plan quantique le déplacement classique de la particule selon la direction de l'impulsion.

Le caractère continu de ces états propres de l'impulsion disparaît si la particule n'est plus strictement "libre", mais "confinée" dans une région donnée de l'espace (« barrière de potentiel infinie »). Du point de vue mathématique cela revient à imposer des conditions aux limites à la fonction d'onde, qui devra s'annuler sur la « frontière » de la « boîte » dans laquelle est confinée la particule, puisque celle-ci a une probabilité de présence nulle en dehors de cette région. Ces conditions aux limites se traduisent physiquement par une quantification de l'énergie et donc de l'impulsion (cf. pour plus de détail l'article Particule dans une boîte). Les états propres correspondants se mettront sous la forme d'une somme des états propres libres, et correspondront à des ondes stationnaires, traduisant sur le plan quantique le confinement de la particule, cf. figure ci-contre.





</doc>
<doc id="19311" url="https://fr.wikipedia.org/wiki?curid=19311" title="Point triple">
Point triple

Un point triple est un point du diagramme de phase température-pression d'une substance chimique pure où peuvent coexister trois phases différentes. Ce point est unique, c'est-à-dire que les trois phases ne peuvent coexister qu'à une température et une pression bien précises, dénommées « la température et la pression du point triple ». Quand les trois phases sont effectivement présentes la variance du système est nulle. La notion de point triple s'applique principalement aux diagrammes de phase des corps purs :


Depuis 1954, l'unité de température du Système international et ses unités dérivées, déterminées par une convention internationale, sont fondées sur la température thermodynamique du point triple de l'eau, "T" :



</doc>
<doc id="19315" url="https://fr.wikipedia.org/wiki?curid=19315" title="Classement en France des cultures par groupes d'usage">
Classement en France des cultures par groupes d'usage

Un groupe de cultures est une méthode de classification des cultures agricoles et horticoles.

En France, les groupes de cultures consacrés par l’usage sont représentés par les catégories ci-dessous. Ces regroupements sont donc distincts des classements fondés sur la systématique, et ils peuvent être différents des groupes de cultures usités dans d’autres pays.

Si les groupes "Cultures fruitières", "Cultures légumières" notamment sont destinés, plus ou moins directement, à l'alimentation humaine, le groupe "Grandes cultures" rassemble ici des espèces ayant des usages beaucoup plus variés tels que : alimentation humaine après transformation, alimentation animale, mais aussi transformation industrielle pour produire du sucre, de l'huile, des tissus, des carburants...ou couverture du sol pour sa préservation... 

D'autres groupes d'usages sont importants tels que les plantes médicinales, les plantes mellifères, les plantes tinctoriales...Ils peuvent comprendre des plantes en partie déjà listées dans cet article.

Les noms entre parenthèses désignent des produits végétaux et des groupes de produits végétaux cités par les arrêtés au Journal officiel sur les limites maximales de résidus de produits phytopharmaceutiques.



</doc>
<doc id="19316" url="https://fr.wikipedia.org/wiki?curid=19316" title="Groupe de Visegrád">
Groupe de Visegrád

Le groupe de Visegrád (aussi appelé Visegrád 4 ou V4 ou triangle de Visegrád) est un groupe informel réunissant quatre pays d’Europe centrale : la Hongrie, la Pologne, la République tchèque et la Slovaquie. Ces pays sont tous des États membres de l'Union européenne et de l'OTAN. La population du groupe s’élève à 63,8 millions d’habitants en 2016 (13 % de la population de l’UE). L’Indice de développement humain moyen est élevé et croissant (l’indice moyen est de 0,8535 d’après le rapport de 2016 sur l’année 2015). Le PIB total du groupe est estimé à 876 milliards de dollars.

Le groupe de Visegrád trouve son origine en automne 1335, lorsque les rois de Bohême, de Pologne et de Hongrie se rencontrèrent dans la ville hongroise de Visegrád pour créer une alliance anti-Habsbourg. Les trois dirigeants se sont mis d’accord sur la création de nouvelles routes commerciales qui permettraient un accès facilité aux marchés européens.

Le V4 moderne a démarré lors d’un sommet des chefs d’État ou de gouvernement de Tchécoslovaquie, Hongrie et Pologne dans la ville de Visegrád le , afin de mettre en place des coopérations entre ces trois États, en vue d’accélérer le processus d’intégration européenne. De là vient le premier nom du groupe, « triangle de Visegrád » ; ces trois États allaient devenir quatre avec la partition de la Tchécoslovaquie le , mais le terme de « triangle » s’utilise encore parfois.

Le V4 renferme une conception politique de l’Europe centrale. En effet, il a facilité leur intégration à l’OTAN et l’Union européenne. Après l’implosion soviétique, ces quatre pays avaient peu sinon pas d’influence dans la sphère européenne. Donc, leur consolidation mutuelle a permis de prouver à l’Europe leur capacité à coopérer en tant qu’États de l’ancien bloc de l'Est.

Le groupe de Visegrád, malgré les mauvaises interprétations, n’est aucunement une organisation ayant comme objectif la concurrence à l’Union européenne. Cette organisation supra-nationale a eu le mérite de permettre à ses membres d’adhérer consécutivement à l’OTAN et l’UE.

Depuis l’adhésion de ses membres à l’OTAN et à l’Union européenne, l’utilité du V4 a été relativisée. Il existe toujours cependant une continuité dans la dynamique des coopérations où ceux-ci tentent d’accroitre leurs intérêts dans leur sphère supra-nationale, par exemple par l’aide à des ONG ou encore une implication commune dans la société civile. Le groupe de Visegrád a permis et réussi le rapprochement intellectuel et culturel.

Lors de la crise migratoire en Europe, en 2015-2016, le groupe de Visegrád se réunit à plusieurs reprises. Il fait alors savoir sa ferme opposition à l’accueil de migrants dans leurs pays, rejetant notamment l’idée des institutions européennes d’un système de répartition obligatoire des migrants et critique la régulation de Dublin.

En mars 2017, le groupe de Visegrád rejette à nouveau les quotas obligatoires de répartition des migrants entre les pays de l’UE dénonçant « le chantage » et « le diktat » européens à leur égard concernant la politique migratoire commune. Une réunion avec le président français en juin 2017 ne rapproche pas les points de vue mais la volonté de maintenir le dialogue est affirmée.

Le 19 juillet 2017, le sommet annuel des chefs de gouvernement du groupe de Visegrád se tient à Budapest en présence du Premier ministre israélien. Ceux-ci décident d’approfondir leur partenariat en matière d’innovation via des startups, et de tenir en 2018 un sommet commun en Israël. Le sommet envoie également une lettre au gouvernement italien en vue « d’endiguer les vagues de migration irrégulière qui partent depuis la Libye et autres endroits d’Afrique du Nord ».

Le groupe de Visegrád est au centre de l’Initiative des trois mers, lancée le 25 août 2016 à Dubrovnik en présence de représentants des États-Unis et de la Chine. Cette organisation rassemble douze pays européens de la Baltique à la mer Noire et à l’Adriatique – groupe de Visegrád plus Lituanie, Estonie, Lettonie, Autriche, Slovénie, Croatie, Roumanie, Bulgarie – autour de projets communs d’infrastructures et en particulier la coopération énergétique, notamment pour favoriser la pluralité des sources d’énergie et diminuer la dépendance énergétique envers qui que ce soit. Le général américain James L. Jones, président de Jones Group International et ancien conseiller à la sécurité nationale du président Obama a déclaré que le développement de l’Initiative des Trois Mers doit être un élément non seulement de développement européen mais aussi de sécurité. Le général Jones a insisté sur l’emploi par la Russie de sa position de fournisseur d’énergie pour augmenter son influence économique et renforcer ses objectifs géopolitiques.

La Slovaquie a réalisé en 2011 une pièce de deux euros commémorative à l’occasion des 20 ans de la formation du groupe le .




</doc>
<doc id="19318" url="https://fr.wikipedia.org/wiki?curid=19318" title="Picomètre">
Picomètre

Un picomètre (symbole : pm) est une unité de longueur du système métrique, équivalent à = (soit ), ou encore .

Cette unité SI remplace l'ångström ( = ) (qui n'est pas SI) pour mesurer les longueurs des liaisons chimiques, qui ont alors typiquement quelques centaines de picomètres de long ; les atomes ont en effet un rayon de pour le plus petit (l'atome d'hélium) jusqu'à pour le césium, le plus gros des atomes connus.

Des unités plus petites telles que le femtomètre peuvent être utilisées pour décrire les noyaux atomiques ou encore les particules subatomiques.



</doc>
<doc id="19320" url="https://fr.wikipedia.org/wiki?curid=19320" title="Langue officielle">
Langue officielle

Une langue officielle est une langue qui est spécifiquement désignée comme telle, dans la constitution ou les textes de loi d'un pays, d'un État ou d'une organisation quelconque.

Elle s'impose à tous les services officiels de l'État (organes de gouvernement, administrations, tribunaux, registres publics, documents administratifs, etc.), ainsi qu'à tous les établissements privés qui s'adressent au public.

La moitié des pays du monde dispose d'une langue officielle.

Certains d'entre eux ne connaissent qu'une seule langue officielle, tels la France et l'Allemagne.

Certains pays ont plusieurs langues officielles, tels la Finlande, la Suisse, le Canada, le Luxembourg, la Belgique et l'Afghanistan.

Dans plusieurs pays, comme l'Italie, les Philippines, l'Espagne, les Pays-Bas ou l'Irak, il y a une langue officielle pour le pays, mais d'autres langues sont coofficielles dans certaines régions. 

Enfin, certains pays n'ont une (ou plus rarement plusieurs) langue officielle qu'en pratique ("de facto"), mais qui n'est pas prévue dans leur constitution : le Mexique, l'Australie, le Belize, le Chili, l'Éthiopie, l'Érythrée, la Gambie, la Guinée-Bissau, la Tanzanie, l'île Maurice, la Suède, le Royaume-Uni ou le Vatican.

Les langues officielles de la Belgique sont le français, le néerlandais et l'allemand, en raison des trois communautés linguistiques du pays. La Flandre est une région unilingue de langue néerlandaise, la Wallonie est une région officiellement bilingue français et allemand (bien que les locuteurs germanophones soient très minoritaires et qu'ils soient concentrés dans les Cantons de l'est qui est aussi le seul endroit du pays où les panneaux sont également affichés en allemand) et la Région de Bruxelles-Capitale est bilingue sur tout son territoire. Quelques communes dont les locuteurs de la langue officielle de la région où elles se trouvent sont minoritaires ont pour cette raison acquis le statut de et sont officiellement bilingues, bien que ce statut ait causé de nombreux litiges conduisant parfois à des heurts entre les communautés linguistiques.

Les langues officielles du Canada sont le français et l'anglais, car une partie de la population, principalement au Québec, est francophone, tandis que le reste du pays est majoritairement anglophone, même si des communautés francophones existent également dans chaque province et territoire.

Aucune langue officielle n'a jamais été adoptée au niveau fédéral.

Toutefois, l'anglais est la langue officielle dans 32 États sur 50. De surcroît, l'État de Hawaï a choisi une deuxième langue officielle (le hawaïen), l'Alaska a également reconnu ses vingt langues indigènes et plusieurs territoires américains ont adopté une deuxième langue officielle (l'espagnol à Porto Rico, le chamorro à Guam, le samoan aux Samoa américaines). Certaines paroisses (comtés) de la Louisiane sont « à facilités » pour les francophones, et il y a des communes et des comtés aux États-Unis pour les hispanophones ; néanmoins, aucun des cinquante États n'a adopté le français ou l'espagnol comme deuxième langue officielle.

En France, le français est langue officielle. En 1539 l'ordonnance du roi François, dite ordonnance de Villers-Cotterêts, rend obligatoire l'usage du français en remplacement du latin comme langue des tribunaux et des chancelleries, et non pour remplacer les langues régionales. Depuis 1992, l'article 2 de la constitution précise que « La langue de la République est le français ».

Dans le Grand-Duché de Luxembourg, le français et l'allemand sont historiquement langues officielles, auxquelles s'est ajouté le luxembourgeois en 1984. Bien que le pays soit trilingue sur tout son territoire, les panneaux ne sont toutefois pas systématiquement affichés dans les 3 langues.

Au Royaume-Uni, l'anglais est "de facto" la langue officielle. Le gallois, l'écossais, l'irlandais, le mannois, le scots, le scots d'Ulster et le cornique sont des langues régionales d'après la Charte européenne des langues régionales ou minoritaires. Le français était langue officielle en Angleterre pendant une partie du Moyen Âge.

En Suisse, l'allemand, le français et l'italien ont le statut constitutionnel de langues officielles depuis la fondation de l'État fédéral en 1848. Tandis que le romanche a le statut de langue nationale depuis 1938, et plus récemment de langue semi-officielle, uniquement pour . Le yéniche est reconnu symboliquement en tant que "langue nationale sans territoire".

L'ukrainien est la langue officielle mais treize autres langues minoritaires sont reconnues, dont le russe, le hongrois et le roumain.

Le russe est majoritaire à l'est et au sud du pays notamment dans de grandes villes comme Odessa et est compris par la plupart des Ukrainiens. La forte présence de la langue russe en Ukraine est due à l'histoire commune durant plusieurs siècles entre l'Ukraine et la Russie, entre autres dans le cadre de l'Empire russe et, plus récemment, de l'Union soviétique.

Dans la République populaire de Donetsk et la République populaire de Lougansk, le russe et l'ukrainien sont les deux langues officielles.

Le français et l'anglais sont les deux langues officielles du Cameroun. Du point de vue de l'espace théorique occupé par chacune des deux langues, ce bilinguisme se réparti comme suit: huit Régions francophones (Centre, Est, Littoral, Ouest, Sud, Adamaoua, Nord, Extrême-Nord) et deux Régions anglophones (Nord-Ouest, Sud-Ouest). Le bilinguisme au Cameroun est reconnu et protégé par les différentes constitutions qu'a connues le pays depuis octobre 1961.

Certaines organisations internationales ont aussi des langues officielles et/ou des langues de travail.

L'ONU s'est dotée de six langues officielles (arabe, anglais, espagnol, français, mandarin, russe), qui sont les langues les plus largement comprises dans le monde même dans les régions où on ne les parle pas spontanément ; tous les travaux et débats sont retranscrits dans ces six langues officielles.

L'Union européenne représente un cas particulier fort intéressant puisqu'en théorie elle ne privilégie la langue d'aucun État. Elle reconnaît 24 langues officielles, soit presque toutes les langues officielles dans au moins un pays membres (seuls le luxembourgeois et le turc sont langues officielles dans un pays membre mais pas au niveau de l'Union européenne). Tous les actes législatifs sont traduits dans ces langues.

Dans son fonctionnement quotidien, l'Union européenne utilise l'allemand, l'anglais et le français comme langues de travail. Dans la pratique, l'anglais occupe une place dominante au détriment des autres langues, en particulier dans les offres d'emploi pour les nouveaux fonctionnaires.

L'existence d'une seule langue officielle est compatible avec une autre langue parlée dans le pays par davantage de personnes. Ainsi dans certains pays d'Afrique ou d'Asie, les langues officielles et d'enseignement (français, anglais ou portugais, par exemple), héritées du colonialisme, ont surtout un statut de langues véhiculaires permettant aux différents groupes ethniques de communiquer entre eux.

De même, dans les pays arabes, la langue officielle est l'arabe littéral alors que la langue maternelle est un arabe dialectal (langue issue de l'arabe mais différente car ayant évolué localement) ou même une langue totalement différente car ne faisant partie du même groupe linguistique (tamazight, kurde).

Autre exemple, la République d'Irlande considère l'anglais, qui est parlé par une majorité d'Irlandais, comme deuxième langue : le pays a adopté le gaélique comme langue nationale officielle alors qu'il n'est parlé que par une faible proportion de la population.

En Chine, le mandarin est la langue officielle du pays bien que la majorité de la population chinoise lui préfère les langues régionales. Seule l'écriture chinoise détient cet avantage d'être intelligible par la grande majorité et permet aux différents groupes du peuple chinois de se comprendre.

En Inde, le pays possède 23 langues officielles dont l'usage et la répartition géographique diffère grandement. Le hindi, parlé par environ de la population dont les locuteurs sont principalement regroupés dans le nord du sous-continent, n'arrive pas à s'imposer comme la seule langue officielle de la fédération. Les autorités ont alors recours le plus souvent à l'usage de l'anglais, langue de l’ancien colonisateur, pourtant qualifiée de , pour pouvoir communiquer avec les populations non hindiphones, qui sont majoritaires dans le sud du pays.

Enfin, en Indonésie, où l'indonésien est utilisée par la plupart des habitants comme langue nationale véhiculaire, les langues régionales (qui, comme l'indonésien, sont dérivées du malais) sont également utilisées quotidiennement par la majorité des Indonésiens.


Pour le Canada et le Québec 


</doc>
<doc id="19324" url="https://fr.wikipedia.org/wiki?curid=19324" title="Métal alcalin">
Métal alcalin

Un métal alcalin est un élément chimique de la première colonne () du tableau périodique, à l'exception de l'hydrogène. Ce sont le lithium Li, le sodium Na, le potassium K, le rubidium Rb, le césium Cs et le francium Fr. Il s'agit de métaux du ayant un électron dans la couche de valence. Ils forment une famille très homogène offrant le meilleur exemple des variations des propriétés chimiques et physiques entre éléments d'un même groupe du tableau périodique.

Les métaux alcalins sont tous des métaux brillants, mous et très réactifs dans les conditions normales de température et de pression. Ils perdent facilement leur électron de valence pour former un cation de charge électrique +1. Ils peuvent être coupés au couteau en raison de leur faible dureté, laissant apparaître une tranche brillante qui ternit rapidement à l'air libre par oxydation sous l'effet de l'oxygène et de l'humidité de l'air ; le lithium réagit également à l'azote atmosphérique. Leur réactivité chimique très élevée fait qu'ils réagissent avec toute trace d'humidité et qu'on ne les trouve jamais comme éléments natifs dans la nature ; ils doivent être conservés dans une huile minérale pour être préservés de l'air, par exemple de l'huile de paraffine. Cette réactivité croît avec leur numéro atomique, c'est-à-dire en descendant le long de la colonne : alcalin, le césium est le plus réactif de tous les métaux. 

Le sodium est le métal alcalin le plus abondant du milieu naturel, suivi par le potassium, le lithium, le rubidium, le césium et le francium, qui est extrêmement rare (il n'y en aurait pas plus d'une trentaine de grammes sur toute la surface de la Terre) en raison de sa radioactivité très élevée (demi-vie de ). Ces éléments sont employés dans diverses applications technologiques et industrielles. Le rubidium et surtout le sont ainsi utilisés dans les horloges atomiques et les méthodes de datation, le Cs offrant la mesure du temps la plus précise. Le sodium connait de nombreuses applications industrielles, par exemple comme fluide caloporteur, ou dans les lampes à vapeur de sodium. Les composés, notamment organiques, du sodium sont très présents dans la vie quotidienne, comme le sel de table, constitué de chlorure de sodium, la soude, ou encore l'eau de Javel. le sodium et le potassium sont des oligoéléments qui jouent un rôle physiologique essentiel. 

Le mot alcalin provient, via le mot d'emprunt alcali, de l'arabe "al-qily" désignant les cendres végétales, riches en potassium.

Les propriétés physiques et chimiques des métaux alcalins peuvent aisément s'expliquer par leur configuration électronique "n"s, "n" étant le numéro de la période, qui leur confère une liaison métallique faible. Ce sont par conséquent des métaux mous de faible masse volumique, dont la température de fusion et la température d'ébullition sont peu élevées, et dont l'enthalpie de sublimation et de vaporisation sont également assez basses. Ils cristallisent dans la structure cubique centrée et présentent chacun une couleur de flamme caractéristique en raison de leur électron s très facilement excitable. La configuration "n"s confère également un rayon ionique élevé aux métaux alcalins ainsi qu'une conductivité électrique et une conductivité thermique également élevées. 

La chimie des métaux alcalins est dominée par la perte de leur électron de valence célibataire dans la sous-couche s externe, ce qui donne un cation à l'état d'oxydation +1 en raison à la fois du caractère labile de cet électron — la première énergie d'ionisation des métaux alcalins est toujours la plus faible de leur période — et de la valeur élevée de la seconde énergie d'ionisation en raison de la configuration électronique de gaz noble des cations M. Seuls les cinq métaux alcalins les plus légers sont chimiquement bien connus, le francium étant trop radioactif pour exister sous forme massive, de sorte que ses propriétés chimiques ne sont pas connues en détail et que les valeurs numériques de ses propriétés sont issues de modèles numériques ; le peu qu'on sait de lui rapproche cependant cet élément du césium, conformément aux simulations numériques.

Les métaux alcalins forment une famille plus homogène qu'aucune autre. Le potassium, le rubidium et le césium sont ainsi difficiles à séparer les uns des autres en raison de la très grande similitude de leur rayon ionique ; le lithium et le sodium sont en revanche plus individualisés. Ainsi, lorsqu'on se déplace vers le bas de la colonne du tableau périodique, le rayon atomique croît, l'électronégativité décroît, la réactivité chimique croît, et températures de fusion et d'ébullition décroissent, de même que l'enthalpie de fusion et l'enthalpie de vaporisation. Leur masse volumique croît globalement du haut vers le bas de la colonne, avec une exception au niveau du potassium, qui est moins dense que le sodium.

Les métaux alcalins sont de couleur argentée, hormis le césium, qui présente une teinte dorée pâle, faisant de ce dernier l'un des trois seuls métaux purs colorés, les deux autres étant le cuivre et l'or ; les métaux alcalino-terreux les plus lourds (calcium, strontium et baryum), ainsi que les lanthanides divalents europium et ytterbium, présentent également une teinte jaune pâle, mais bien moins prononcée que celle du césium. L'éclat des métaux alcalins ternit rapidement à l'air libre en raison de la formation d'une couche d'oxydes.

Tous les métaux alcalins sont très réactifs et n'existent pas à l'état pur dans le milieu naturel. Pour cette raison, ils sont conservés dans une huile minérale ou de l'huile de paraffine. Ils réagissent violemment avec les halogènes en formant des , qui sont des composés cristallins solubles dans l'eau à l'exception du fluorure de lithium LiF. Les métaux alcalins réagissent également avec l'eau pour former des hydroxydes fortement basiques qui doivent par conséquent être manipulés avec précaution. Les métaux alcalins les plus lourds sont plus réactifs que les plus légers : à quantité molaire égale, le césium mis en contact avec l'eau explose plus violemment que le potassium.

Les métaux alcalins réagissent non seulement avec l'eau mais également avec les donneurs de protons comme les alcools et les phénols, l'ammoniac gazeux et les alcynes, ces derniers donnant lieu aux réactions les plus violentes. Ils sont également très utilisés pour réduire les autres métaux à partir de leurs oxydes ou de leurs halogénures.

La première énergie d'ionisation des métaux alcalins est la plus faible de leur période dans le tableau périodique en raison de leur faible et la facilité avec laquelle ils adoptent une configuration électronique de gaz noble en ne perdant qu'un seul électron.

L'énergie de deuxième ionisation des métaux alcalins est toujours très élevée, ce qui s'explique par le fait qu'il s'agit de retirer un électron à une sous-couche saturée plus proche du noyau atomique. C'est la raison pour laquelle les métaux alcalins ne perdent qu'un seul électron pour former des cations. Les font exception : il s'agit de composés dans lesquels un métal alcalin est à l'état d'oxydation –1. Ils peuvent exister dans la mesure où ils présentent une sous-couche "n"s, saturée. L'anion M a été observé pour tous les métaux alcalins hormis le lithium<ref name="10.1021/ja00809a060">
</ref><ref name="10.1021/ja00830a005">
</ref><ref name="10.1002/anie.197905871">
</ref>. Les anions alcalins présentent un intérêt théorique en raison de leur stœchiométrie inhabituelle et de leur faible potentiel d'ionisation. Un exemple particulièrement intéressant est l'hydrure de sodium inversé HNa, les deux ions étant complexé, par opposition à l'hydrure de sodium NaH<ref name="10.1021/ja025655+">
</ref> : de tels composés sont instables en raison de l'énergie élevée résultant du déplacement de deux électrons de l'hydrogène vers le sodium mais plusieurs dérivés pourraient être métastables ou stables<ref name="10.1021/ja021136v">
</ref>.

Le potentiel d'oxydoréduction , en revanche, est l'une des rares propriétés des métaux alcalins qui ne présente pas de tendance uniforme le long de la famille : celui du lithium est anormal, étant sensiblement plus négatif que celui des autres, qui décroît par ailleurs légèrement de haut en bas. Cela s'explique par le fait que le cation Li a une très élevée en phase gazeuse qui compense le fait que Li perturbe fortement la structure de l'eau, ce qui induit une variation d'enthalpie élevée, faisant apparaître cet élément comme le plus électropositif des métaux alcalins.

En solution aqueuse, les métaux alcalins forment des ions de formule générique [], "n" étant le nombre de solvatation. Leur géométrie et leur coordinence s'accorde bien avec celles attendues en fonction de leur rayon ionique. Les molécules d'eau qui se lient directement à l'atome métallique en solution aqueuse sont dites appartenir à la première sphère de coordination, ou première couche de solvatation. Il s'agit d'une liaison covalente de coordination, l'oxygène cédant les deux électrons de la liaison. Chaque molécule d'eau coordonnée est susceptible d'être liée à d'autres molécules d'eau par liaison hydrogène. Ces dernières sont dites de la seconde sphère de coordination. Celle-ci n'est cependant pas bien définie dans le cas des cations de métaux alcalins dans la mesure où ces cations ne sont pas suffisamment chargés électriquement pour polariser les molécules de la première couche de solvatation au point d'induire des liaisons hydrogène avec une seconde couche de molécules d'eau.

Dans le cas du lithium, le nombre de solvatation du cation Li a été déterminé expérimentalement avec la valeur 4, avec un ion tétraédrique []. Le nombre de solvatation du sodium serait probablement 6, avec un ion octaédrique [], tandis que celui du potassium et du rubidium serait vraisemblablement 8 avec des ions [] et [] antiprismatiques, et celui du césium serait peut-être 12 avec des ions []<ref name="10.1351/PAC-CON-09-10-22">
</ref>.

Les vapeurs de métaux alcalins (ou de leurs ions) excités par la chaleur ou l'électricité sont connus pour émettre des couleurs caractéristiques. C'est ainsi que la spectroscopie a vu ses premiers pas se réaliser, grâce aux expériences de Bunsen et Kirchhoff. Les couleurs sont dues au fait que le spectre d'émission atomique est un spectre de raie, et non un spectre continu de type corps noir, preuve de la nature quantique des niveaux d'énergie dans les atomes et ions.
Les couleurs caractéristiques sont :

Les métaux alcalins sont connus pour leur réaction violente avec l’eau. Cette violence augmente quand on descend le long du groupe :

Exemple avec le sodium :

Cette réaction est très exothermique et peut provoquer l’inflammation ou l’explosion de l'hydrogène avec une flamme jaune. Avec le potassium, la flamme est de couleur lilas.

Les réactions des métaux alcalins avec l'eau peuvent être, selon les quantités mises en œuvre, très dangereuses. Si dans les batteries au lithium, lorsqu'elles sont dégradées, comme après un accident, le liquide de refroidissement (de l'eau) entre en contact (même par simple taux d’hygrométrie) avec le lithium, cela peut enclencher une combustion si la température est favorable à cette réaction (20°C).

Les métaux alcalins se dissolvent dans l'ammoniac liquide donnant des solutions bleues qui sont paramagnétiques.

Vu la présence d'électrons libres, la solution occupe plus que la somme des volumes du métal et de l'ammoniac. Les électrons libres font de ces solutions de très bons agents réducteurs.

En réagissant avec l'hydrogène, les métaux alcalins forment des hydrures.

Ces hydrures sont très instables en solution, du fait de leur caractère très basique, et réagissent sur l'eau pour former de l'hydrogène et des hydroxydes.

En réagissant avec l'oxygène, les métaux alcalins forment un oxyde, soluble dans l'eau. La réaction doit cependant être favorisée par le chauffage par exemple, sinon, c'est le peroxyde ou le superoxyde (et non l'oxyde) qui se forme.

La solvatation de l'oxyde conduit à la dissociation des composés sodium et oxyde. L'ion oxyde est instable en solution, et son caractère basique conduit à la déprotonation de l'eau :

Les peroxydes et superoxydes se dismutent en oxygène et en oxyde.




</doc>
<doc id="19325" url="https://fr.wikipedia.org/wiki?curid=19325" title="Liste des langues officielles">
Liste des langues officielles

Cette page recense les 141 langues officielles dans les pays du monde (sont à ajouter les 31 langues officielles à vocation régionale). Les astérisques (*) indiquent les langues qui ne sont officielles que dans une partie du territoire (États, cantons, provinces, etc.). L'Inde reconnait, outre l'anglais et le hindi, vingt-deux autres langues officielles, ces dernières ne figurent pas dans la liste et on se reportera à l'article sur les langues en Inde.

L’anglais est une langue officielle d’ :

L’arabe est une langue officielle d' :

Le chinois ("hanyu" 汉语 ou 漢語), parfois nommé mandarin ("guanhua"官话 ou 官話), est la langue officielle de :

L’espagnol (également appelé castillan) est la langue officielle d’ :

Le français est une langue officielle de : 

Le russe est la langue officielle de :

L’albanais est une langue officielle d’ :

L’allemand est la langue officielle d’ :

L'amazigh est une langue officielle du :
L'amazigh est une langue officielle d :
L'amazigh y est une langue nationale.
L'Amazigh est une langue nationale d:

L’arménien est la langue officielle d’ :

L’aymara est une langue officielle de :

Le bengali est la langue officielle du :

Le catalan est une langue officielle d’ :

Le coréen est la langue officielle de :

Le croate est une langue officielle d’ :

Le danois est une langue officielle d’ :

Le finnois est une langue officielle de :

Le guarani est une langue officielle de :

Le grec est la langue officielle de :

Le hongrois est la langue officielle de :

L’italien est une langue officielle d’ :

Le kiswahili est une langue officielle du :

Le malais est la langue officielle du :

Le mongol est la langue officielle de :

Le néerlandais est une langue officielle de :

L’occitan est une langue officielle de :

L’ourdou est une langue officielle de :

Le persan ("farsi") est une langue officielle d’ :

Le portugais est la langue officielle d’ :

Le quechua est une langue officielle de :

Le roumain est la langue officielle de :

Le samoan est la langue officielle des :

Le serbe est une langue officielle de :

Le sesotho est une langue officielle d’ :

Le slovaque est une langue officielle de :

Le slovène est une langue officielle de :

Le suédois est une langue officielle de :

Le tamoul est une langue officielle de :

Le turc est la langue officielle de :

L’afrikaans est une des onze langues officielles d’ :

L’araona est une langue officielle de la :

L’azéri est la langue officielle d’ :

Le baure est une langue officielle de :

Le bésiro est une langue officielle de :

Le bichelamar est une langue officielle des :

Le biélorusse est une langue officielle de :

Le birman est la langue officielle de :

Le bulgare est la langue officielle de :

Le canichana est une langue officielle de :

Le cavineña est une langue officielle de :

Le cayubaba est une langue officielle de :

Le chácobo est une langue officielle de :

Le chichewa est une langue officielle du :

Le chimane est une langue officielle de :

Le créole de Guinée-Bissau a un statut régional officiel en :

Le créole haïtien est une langue officielle d’ :

Le créole seychellois est une langue officielle des :

Le divehi ("maldivien") est la langue officielle des :

Le dzongkha est la langue officielle du :

Le ese 'ejja est la langue officielle de :

L’estonien est la langue officielle de :

Le fidjien est une langue officielle des :

Le filipino est une langue officielle des :

Le géorgien est la langue officielle de :

Le gilbertin est une langue officielle des :

Le guarasu’we est une langue officielle de :

Le guarayu est une langue officielle de :

L’hébreu est la langue officielle d’ :

Le hindi est une langue officielle d’ :

L’hindi des Fidji est une langue officielle des :

Le hiri motu est la langue officielle de :

L’indonésien, qui est une forme de malais, est la langue officielle de l’ :

L’irlandais est la première langue officielle d’ :

L’islandais est la langue officielle d’ :

L’itonama est une langue officielle de :

Le japonais est la langue officielle "de facto" du:

Le kallawaya est une langue officielle de :

Le kazakh est une langue officielle du :

Le khmer est la langue officielle du :

Le kirghiz est une langue officielle du :

Le kirundi est une langue officielle du :

Le lao est la langue officielle du :

La langue des signes néo-zélandaise est une langue officielle de :

Le latin est la langue officielle du :

Le leko est une langue officielle de :

Le letton est la langue officielle de :

Le lituanien est la langue officielle de :

Le luxembourgeois est langue officielle du :

Le macédonien est la langue officielle de :

Le machineri est une langue officielle de :

Le malgache est la langue officielle de :

Le maltais est la langue officielle de :

Le maori est une langue officielle de :

Le maori des îles Cook est une langue officielle des :

Le maropa est une langue officielle de :

Le marshallais est la langue officielle des :

Le mirandais est la seconde langue officielle du :

Le mojeño-trinitario est une langue officielle des :

Le mojeño-ignaciano est une langue officielle des :

Le monténégrin est la langue officielle du :

Le moré est une langue officielle de :

Le mosetén est une langue officielle de :

Le movima est une langue officielle de :

Le nauruan est la langue officielle de :

Le ndébélé du Sud est une des 11 langues officielles de l':

Le népalais est la langue officielle du :

Le norvégien est la langue officielle de la :

L’ouzbek (ou 'ouzbèke) est la langue officielle de l’ :

Le pacahuara est une langue officielle de :

Le pachto est une langue officielle d’ :

Le paluan est la langue officielle des :

Le polonais est la langue officielle de :

Le puquina est une langue officielle de :

Le sango est une langue officielle de :

Le shikomor ("comorien") est une langue officielle des :

Le shona est une langue officielle du :

Le shuar est une langue officielle de :

Le sindebele est une langue officielle du :

Le singhalais est une langue officielle du :

Le sirionó est une langue officielle de :

Le somali est la langue officielle de :

Le sotho du Nord est une des 11 langues officielles de l' :

Le sotho du Sud est une des 11 langues officielles de :

Le tacana est une langue officielle de :

Le tadjik est la langue officielle du :

Le tamazight est une langue officielle du : 

Le tapiete est une langue officielle de :

Le tchèque est la langue officielle de :

Le tétoum est la langue officielle du :

Le tigrinya est une langue officielle d’ :

Le thaï est la langue officielle de :

Le tok pisin est la langue officielle de :

Le tongien est la langue officielle des :

Le toromona est une langue officielle de :

Le turkmène est la langue officielle du :

Le tuvaluan est la langue officielle des :

L’ukrainien est la langue officielle d’ :
Elle est langue officielle régionale en :

L’uru-chipaya est une langue officielle de :

Le vietnamien est la langue officielle du :

Le wichi est une langue officielle du :

Le yaminahua est une langue officielle de :

Le yuki est une langue officielle de :

Le yuracaré est une langue officielle de :

Le zamuco est une langue officielle de :

Le zoulou est une des 11 langues officielles de :

L'adyguéen est une langue officielle régionale en:

L'agul est une langue officielle régionale en:

L'altaï est une langue officielle régionale en:

Le assamais est une langue officielle régionale d’:

L'avar est une langue officielle régionale en:

Le bachkir est une langue officielle régionale en:

Le bas saxon est une langue officielle régionale d’:

Le basque est une langue officielle régionale en :

Le bouriate est une langue officielle régionale en :

Le cantonais est une langue officielle régionale de :

Le carolinien est une langue officielle régionale des :

Le chamorro est une langue officielle régionale des :

Le darguine est une langue officielle régionale en:

Le galicien est une langue officielle régionale d’ :

Le gallois est une langue officielle régionale du :

L’hawaïen est une langue officielle régionale des :

L’Inuinnaqtun est une langue officielle régionale du :

L’Inuktitut est une langue officielle régionale du :

Le khakasse est une langue officielle régionale de :

Le koumyk est une langue officielle régionale en:

Le lak est une langue officielle régionale en:

Le lezghien est une langue officielle régionale en:

Le mannois est une langue officielle régionale du :

Le mirandais est une langue officielle régionale du :

Le ndébélé est une langue officielle régionale d’ :

Le Nogaï (langue) est une langue officielle régionale en:

L’occitan (appelé aranais dans le val d'Aran) est une langue officielle régionale en :

L’ouïghour est une langue officielle régionale de :

Le romanche est langue nationale depuis 1938, langue officielle régionale de :

Le routoul est une langue officielle régionale en:

Le ruthène est une langue régionale officielle de :

Le sorabe est une langue officielle régionale d’ :

Le sotho du Nord est une langue officielle régionale d’ :

Le sotho du Sud ("sesotho") est une langue officielle régionale d’ :

Le swati est une langue officielle régionale d’ :

Le tabassaran est une langue officielle régionale en:

Le tat est une langue officielle régionale en:

Le tchétchène est une langue officielle régionale en:

Le tchouvache est une langue officielle régionale en:

Le tibétain est une langue officielle régionale de :

Le tobi est la langue officielle régionale de:

Le tsakhur est une langue officielle régionale en:

Le tsonga est une langue officielle régionale d’ :

Le tswana est une langue officielle régionale d’ :

Le venda est une langue officielle régionale d’ :

Le xhosa est une langue officielle régionale d’ :

Le zoulou est une langue officielle régionale d’ :




</doc>
<doc id="19341" url="https://fr.wikipedia.org/wiki?curid=19341" title="Junon">
Junon

Dans la mythologie romaine, Junon, en latin "Juno", est la reine des dieux et protectrice du mariage. Fille de Rhéa et de Saturne, elle est à la fois sœur et épouse de Jupiter. Ses attributs sont le paon, un sceptre surmonté d'un coucou et une grenade, symbole de l'amour conjugal, le lys et la vache. Elle est assimilée à l'Héra des Grecs.

Protectrice des femmes, elle symbolise le mariage lorsqu'elle est représentée recouverte de voiles, et elle est associée à la fécondité lorsqu'elle en tient l'emblème : la pomme de grenade.

Le quatrième mois du calendrier romain aurait été nommé juin en son honneur par les Romains.

Junon n'a pas toujours été vue comme l'épouse de Jupiter. Marcel Renard la rapprochait de Janus, qualifié de "Junonius". Le couple formé de Jupiter et de Junon n'a été créé que par la suite selon l'image du couple souverain du panthéon grec ; ce qui a fait de Junon, sous l'influence d'Héra, une déesse du mariage. Son nom évoque l'idée de "jeunesse" et de "force vitale" . Au regard de la signification de son nom, Junon est originellement une "déesse de la jeunesse". Jean Haudry voit en elle une Aurore indo-européenne et plus précisément une Aurore "jeune" présidant à la durée de la vie et porteuse de richesses.

En accord avec sa fonction première, Junon en tant que "Iuno Sororia", déesse de la jeunesse veillait à l'initiation des jeunes filles à leur rôle de futures mères et intervenait lors du passage de l'état d'enfant à celui de fille nubile. 

Plus tard par assimilation avec Héra, elle préside aux mariages et aux accouchements. Alors, et selon le cas, on l'invoquait sous les noms de Juga, Pronuba, Lucine, etc. Junon prenait un soin particulier des parures et des ornements des femmes : c'est pour cela que, dans ses peintures, ses cheveux paraissaient élégamment ajustés. Elle présidait aussi à la monnaie. De nombreuses monnaies romaines à la légende "Ivno regina" représentent Junon debout, parfois voilée, tenant une haste et sacrifiant à l'aide d'une patère, un paon quelquefois à ses pieds. D'autres, au revers "Ivno victrix", montrent une Junon victorieuse, tenant un casque et une lance, avec un bouclier et parfois un captif à ses pieds. 

Junon intervenait avec Janus aux calendes. Elle facilitait la tâche du dieu qui préside aux commencements et aux passages grâce à "vitalité" dont elle est la dépositaire. Ainsi la collaboration effective entre Janus et Junon, qui concourent parallèlement à la transition d'un mois à l'autre, c'est-à-dire à l'heureuse naissance de la nouvelle lune. Ceci explique pourquoi les anniversaires de presque tous les cultes de Junon tombent sur le des mois respectifs: "Iuno Sospita" le février, "Iuno Lucina" le mars, "Iuno Moneta" le juin, "Iuno Regina" le septembre, "Iuno Sororia" le octobre, et "Iuno Couella" qui était invoquée à chaque premier jour des mois de l'année.

Elle inspirait une vénération mêlée de crainte. C'est à Argos, Samos et dans la Carthage romaine qu'elle était principalement honorée.

À Argos, on voyait sur un trône la statue de cette déesse, d'une grandeur extraordinaire, toute d'or et d'ivoire : elle avait sur la tête une couronne au-dessus de laquelle étaient les Grâces et les Heures. Elle tenait d'une main une grenade, et de l'autre un sceptre, au bout duquel était un coucou, oiseau aimé de la déesse.

À Samos, la statue de Junon portait aussi une couronne : on l'appelait même Junon la reine ; du reste, elle était couverte d'un grand voile de la tête aux pieds.

À Lavinium, en Italie, la Junon tutélaire portait une peau de chèvre, une javeline, un petit bouclier et des escarpins recourbés en pointe sur le devant.

À Rome, après la prise de Véies en 396 av. J.-C., la statue de Junon Reine fut transportée avec beaucoup d'égards de Veies à Rome.

Dans l'Afrique romaine et à Carthage, Junon "Cælestis" (la céleste) se substitua à la punique Tanit.

Ordinairement elle est représentée en matrone majestueuse, quelquefois un sceptre à la main, ou une couronne radiée sur la tête ; elle a auprès d'elle un paon, son oiseau favori.

L'épervier et l'oison lui étaient aussi consacrés : ils accompagnent quelquefois ses statues.
On ne lui sacrifiait pas de vaches, parce que, durant la guerre des géants et des dieux, elle s'était cachée sous cette forme en Égypte. Le dictame, le pavot, la grenade lui étaient donnés en offrande ; ces plantes ornaient ses autels et ses images. La victime immolée ordinairement en son honneur était une toute jeune brebis ; cependant, le premier jour de chaque mois, on lui immolait une truie. Les prêtresses de Junon étaient universellement respectées.

Selon Michael Maier, le paon est censé lui être consacré. En voici le motif : Argus, tué par Mercure à cause d’elle, avait été changé en cet oiseau. Le peintre Antonio de La Gandara réalisa un tableau de cette statue située au jardin du Luxembourg.

Junon était fille de Saturne et de Rhéa, sœur de Jupiter, de Neptune, de Pluton, de Cérès et de Vesta. 







</doc>
<doc id="19343" url="https://fr.wikipedia.org/wiki?curid=19343" title="Match d'improvisation">
Match d'improvisation

Le match d'improvisation théâtrale a été créé le 21 octobre 1977 au Québec, sous la gouverne du Théâtre Expérimental de Montréal, par Robert Gravel, qui souhaitait expérimenter de nouvelles formes théâtrales et approches du public. Afin de casser l'élitisme du théâtre, il a l’idée d’utiliser la forme du sport, en parodiant le populaire hockey sur glace (décorum et règles).

Dans cette aventure, Robert Gravel embarque plusieurs comédiens et amis : Anne-Marie Laprade, Yvan Ponton, qui créera le rôle de l'arbitre, Pierre Martineau qui deviendra le premier maître de cérémonie, ainsi que les premiers comédiens qui se lancèrent dans l'aventure.

Le concept a été déposé par Gravel (non pas Robert, mais son frère) et Yvon Leduc sous le nom de « match d'improvisation » et des championnats du monde d'improvisation furent même introduits.

À l'origine, cet aspect compétition a été décrié par les comédiens : d'une part, il attaquait la création classique ; d'autre part, il révélait la compétition qui existe réellement dans le monde du théâtre. Mais cette notion de conflit, source de danger pour les improvisateurs, intéresse le public et bouscule l'improvisation, poussant à la créativité et la spontanéité.

Ce concept s'est développé avec beaucoup de succès au Québec d'abord, puis dans le monde francophone essentiellement.
Il existe toutefois des ligues d'improvisation partout dans le monde dans différentes langues, que ce soit en anglais aux États-Unis ou en espagnol en Argentine et en Espagne.

Deux équipes composées chacune de 7 personnes (3 joueurs, 3 joueuses et un coach) s'affrontent dans une mini patinoire de hockey (environ 5 m sur 6 m) en bois, dans une tenue réglementaire (à savoir pantalon de sport noir ou jogging noir, T-shirt, baskets et maillot ou dossard numéroté). Le match est présenté et animé par un maître de cérémonie dit MC (prononcez "èmecé" à la française en hommage au caractère francophone québécois). Un musicien, à la base réplique fidèle des organistes des arénas d'Amérique, joue pour chauffer la salle, remplir les temps morts ou ponctuer les moments forts et quelquefois intervenir dans certaines improvisations. Un arbitre et deux arbitres assistants se chargent de faire respecter les règles et de comptabiliser les scores. Ils portent des tenues d'arbitres de hockey.

L’arbitre tire au hasard les thèmes (inconnus des joueurs) dans un barillet et indique la forme que l’improvisation (l'impro) prendra en faisant varier plusieurs éléments :
À la fin de la lecture du carton, les joueurs de chacune des équipes ont 20 secondes pour se concerter (ce qu'on appelle le caucus). En comparée, un assistant arbitre vérifie que l'équipe qui joue en seconde position ne communique pas sur le banc (afin de ne pas créer une inégalité des chances entre les deux équipes). Pour cela il dispose d'un foulard qu'il jette dans l'aire de jeu pour signaler la faute.

À la fin de chaque improvisation, le public, à l'aide d'un carton réversible, aux couleurs des équipes, vote pour la meilleure prestation. Un point est accordé à l'équipe obtenant la majorité des votes du public et remporte l’improvisation.

Le match se déroule en trois périodes (tiers-temps) de 30 minutes chacune et l’équipe qui a le plus de points à la fin du match est déclarée gagnante.

À tout moment, le public peut manifester son mécontentement ou son ennui en jetant une pantoufle (aussi appelé savate) sur la patinoire (avatars des caoutchoucs québécois).

L'arbitre est là pour faire respecter un certain nombre de règles bien établies. Il peut siffler des fautes à l'aide d'un gazou. Ainsi, il existe 16 fautes officielles (Voir ci-dessous). Chaque équipe totalisant trois fautes donne un point à l'équipe adverse.

Le match d'improvisation se doit de rester un spectacle : s'il s'inspire du sport et de son esprit de compétition, c'est pour les parodier, tel que l'a imaginé son fondateur Robert Gravel. S'il est bien compris, le match d'improvisation « joue à jouer un match ». L'aspect le plus remarquable, et ce qui incontestablement explique le succès du match d'improvisation, est le fait que le public lui-même participe au spectacle et joue son propre rôle. Ainsi que le constat de la somme d'exigences pour les joueurs : écoute, rapidité de réaction, humilité (juger quand il est opportun d'intervenir), mise en danger, imaginaire riche, travail d'équipe... que le public découvre indispensables pour que le spectacle soit bon.

Le personnel est généralement composé de cinq personnes : trois arbitres, un MC et un musicien. Le rôle de base du staff est d'encadrer le match afin qu'il se déroule au mieux pour les deux équipes et donc pour le spectacle et le public. Pour ce faire, chaque membre du personnel a quelques devoirs de base:
Il doit garantir une totale impartialité entre les deux équipes. Il ne doit pas focaliser excessivement l'attention du public afin que les joueurs soient l'élément le plus important du match. Il doit se coordonner avec les autres membres du personnel, ce qui assure un spectacle avec un bon rythme.

Il faut souligner que plus le personnel est sobre, plus le jeu des joueurs peut être mis en avant. C'est ce qui fait la qualité du personnel. Souvent, cette qualité est présente lorsque les différents membres du staff se connaissent ou ont déjà collaboré dans le cadre de l'improvisation théâtrale.

Un autre atout majeur pour le personnel est la communication. En effet, un bon personnel sait communiquer très discrètement et simplement. Il doit y avoir un échange de regards ou de quelques mots, selon un code établi et qui doit être discret.

Un membre du personnel doit (de préférence) avoir un personnage. Il ne doit pas jouer à être son personnage, mais il doit être son personnage. Certes, un arbitre n'est pas aimé du public et des joueurs par définition, mais il n'est pas obligé d'avoir un air absolument abominable, il peut se faire détester d'une autre manière. Un MC ou un musicien peut aussi avoir un personnage, même si on ne le perçoit pas souvent. Ce sont souvent des petites touches comme celle-ci qui donnent une identité au personnel. En France et au Luxembourg, le musicien officiel peut intervenir à discrétion lors des improvisations en plus de son rôle de chauffeur de salle. 

Lors d'un match d'improvisation, plus l'arbitre est détesté du public, plus les joueurs et leur jeu seront appréciés du public. Ceci veut dire que si les rapports de pouvoir ont tendance à s'inverser, le match sera ennuyeux car le public n'aura pas cette petite rage intérieure contre l'arbitre, et va donc se lasser d'un match où le rôle des personnages (arbitres, joueurs) n'a plus d'importance. Le rôle de l'arbitre est ici de détourner et de cristalliser sur lui le mécontentement du public quand les joueurs rencontrent des difficultés au cours de leurs improvisations.

L'arbitre peut siffler des fautes, s'il le veut. Les pénalités ne sont pas décernées pour embêter les joueurs mais le but premier d'une pénalité est d’améliorer le jeu. La faute sera sifflée afin de prévenir le joueur qu'il ne gère pas bien l'improvisation et que tel ou tel point est à améliorer (construction, avancement de l'histoire, occupation de l'espace, expression verbale).

Tous ces points confirment bien que le personnel est important pour le bon déroulement et l'encadrement du match. Le staff contribue à poser le cadre de la cérémonie, ce qui met à l'aise les joueurs et leur permet de réaliser un bon match. Il faut rappeler que le match est certes la confrontation de deux équipes mais le spectacle « match d'impro » est tenu par les deux équipes et par le personnel, ensemble.

Durant le match, l'arbitre peut siffler autant de fautes qu'il veut. Chaque faute possède, pour signe, un geste. L'arbitre (debout) siffle dans son gazou en réalisant ce geste et en désignant à qui appartient la faute. Voici les différentes fautes, leur signe respectif et les raisons pour lesquelles elles sont sifflées.

En complément du tableau ci-dessous vous pouvez retrouver ces signes en image 

Une faute peut être sifflée soit à une équipe, soit aux deux équipes, soit à un joueur. L'arbitre, après avoir fait le geste de la faute, désigne une équipe ou les deux équipes en pointant le côté de leur banc, ou un joueur en le pointant directement.

Chaque équipe qui totalise trois fautes offre un point à l'équipe adverse.

Chaque faute peut être majorée si elle est jugée grave. À ce moment-là, l'arbitre siffle la faute normalement en rajoutant un signe : Il tire les deux poings du haut vers le bas. Une faute majorée compte comme deux fautes sur le tableau de score.

Lorsqu'un joueur, au cours d'un match, obtient deux fautes personnelles, l'arbitre siffle une punition de match (en tapant deux fois les poings contre les hanches) et le joueur est expulsé du match.

La Ligue d'improvisation française a été créée en France par Catherine Monnot, metteur en scène et ancienne professeur à l'Université Paris 8.

En 1981 se crée en France après un match de démonstration à Avignon, puis au théâtre d'Aubervilliers la LIF ligue d'improvisation Française, elle fut la première ligue étrangère. Elle popularisa le concept en jouant à partir de 1984 chaque lundi au Bataclan à Paris. l'AFLI (Association Française des Ligues d'Improvisation), créée en 1989, a rassemblé à une époque certaines ligues juniors, amateurs et professionnelles. Elle avait pour but d'assurer le développement des matchs d'improvisation en veillant au respect des règles et du concept initial. L'AFLI a cessé d'exister et les 7 ligues professionnelles pratiquant le match d'impro ont créé la LIF Pro.

De nombreuses rencontres ont lieu chaque week-end en France opposant les différentes équipes de chaque ligue. Des rencontres internationales, et notamment des Coupes du monde, sont également régulièrement organisées.

Des ligues amateurs se sont constitués également en très grand nombre, représentant une ville, une région, un département, sans qu'une fédération ne voit le jour jusqu'à présent, car les pratiques sont variées.

On assiste depuis quelques années à un certain virage qui sépare l'improvisation en deux courants. Il y a les ligues « puristes » qui respectent à 100 % les règles de la LNI. D'autres jouent sans patinoire, certaines n'apprécient pas le concept d'arbitre sévère et le remplacent par un maître de jeu, les 3 tiers temps sont parfois remplacés par deux mi-temps. Ceci étant dit les ligues arrivent toujours à se rencontrer et à s'entendre sur des règles avant de jouer ensemble.

C'est pour cela qu'il existe deux championnats de France amateur :

En 1996 ont été créées en France, des rencontres d'improvisation théâtrales, Les IMPROVISADES, qui réunissent exclusivement des équipes d'entreprises, d'Universités et de Grandes Écoles. Le monde du travail s'intéresse de plus en plus aux techniques d'improvisation .

Le match d'improvisation a donné naissance à de nombreux dérivés, dont notamment les spectacles de Café-théâtre d'improvisation qui se développent de plus en plus.

Le match d'improvisation a été l'objet d'une bataille juridique autour de la notion de droit d'auteur. Cette notion est très différente en France et dans les pays anglo-saxon. Peu après sa création, Robert Gravel avait souhaité que soit reconnu ce concept et demandé que chaque ligue l'utilisant verse un dollar symbolique. À sa mort, se sont créées les éditions Gravel/Leduc constituées des héritiers de Robert Gravel et du directeur de la L.N.I. Et ce sont ces éditions qui, à l'heure actuelle, exigent des droits d'auteur qui n'ont plus rien à voir avec la demande symbolique de Robert Gravel. La LIDY, l'une des premières associations d'improvisation théâtrale en France, dont est issue Jamel Debbouze, et qui regroupe des associations des Yvelines et alentour, a été assignée par les éditions Gravel et Leduc qui prétendait déposer un concept du match d'improvisation en France. Après 10 ans de procédure, un accord a été conclu avec la SACD, la LIDY et les auteurs, accord qui prévoit d'accorder 60 % des droits d'auteur... aux comédiens qui font le spectacle, et 40 % aux auteurs. Le match d'improvisation a été entendu au sens large, y compris pour des spectacles ne respectant pas totalement le concept initial. L'avantage de cet accord est de reconnaître le droit des comédiens. Mais son désavantage est qu'il trahit la demande initiale symbolique de Robert Gravel et qu'il empêche les ligues d'amateurs de fonctionner alors que l'histoire ne dit pas si des droits d'auteurs sont versés à la ligue de hockey !

La version belge a débuté en 1984 avec la Ligue d'improvisation belge. Après un passage par le Mirano et les Halles de Schaerbeek, elle se produit au théâtre Marni depuis 1999. De grands noms de la scène belge ont foulé la patinoire de la LIB, comme Laurence Bibot, Éric De Staercke ou encore Virginie Hocq.

Il faut également signaler l'existence et le développement de la FBIA (Fédération Belge d'Improvisation Amateur), qui forme chaque année de nombreux jouteurs débutants lors des Apis (ateliers de découverte), puis encadre et organise un certain nombre d'évènements avec les équipes plus confirmées.
Un championnat amateur réunit les meilleures équipes non professionnelles de Belgique, dont sont issus les membres de l’Équipe Belge d'Improvisation.

En 2012 la Ligue d'Improvisation Professionnelle Wallonie/Bruxelles est fondée. Elle regroupe des comédiens, issus des différentes écoles francophones d’art dramatique en Belgique. La LIP est une plate-forme de rencontre entre les différentes générations de comédiens, les expérimentés et les jeunes sortants, entre les différentes écoles, entre les différentes disciplines artistiques ; mais elle est et doit être aussi un lieu de rencontres internationales qui permet l’accueil de formateurs spécifiques venant aussi bien d’Italie que de France, du Québec ou de Suisse, pays avec lesquels la Ligue d’improvisation professionnelle Wallonie-Bruxelles partage l’envie d’apprendre et de travailler.

La quarantaine de comédiens qui composent cette Ligue d’improvisation professionnelle Wallonie-Bruxelles ont fait rêver plusieurs milliers de spectateurs depuis de nombreuses années dans la patinoire du Marni, et c’est avec de nouvelles envies et sur une base plus précise et professionnelle qu’ils ont décidé de rendre à l’improvisation ses lettres de noblesse, en lui rendant sa fonction première d’outil.

L'improvisation permet aux comédiens d’explorer les différentes facettes qui composent son métier. Elle offre les moyens de parfaire et de compléter les formations de ces membres. La LIP s’inscrit donc dans un mouvement perpétuel et une constante évolution, tant à l’intérieur que vers l’extérieur.

À Berlin (Allemagne), des ateliers organisés par Marjorie Nadal au sein de l'association La ménagerie proposent depuis 2009 des matchs d'improvisation bilingues. L'arbitre décide dans quelle langue (français, allemand ou mixte) sera jouée chaque improvisation.




</doc>
<doc id="19348" url="https://fr.wikipedia.org/wiki?curid=19348" title="Devise (monnaie)">
Devise (monnaie)

Au sens strict, une devise est une unité monétaire acceptée par un pays étranger, tandis que la « monnaie » est celle de son pays. 

Elle est émise le plus souvent sous le contrôle d’une banque centrale (mais peut aussi être émise à partir d’un fond de panier collectif en multiples devises, géré par une organisation internationale adossée à plusieurs banques centrales indépendantes où ces fonds sont garantis par le dépôt de réserves).

Chaque pays a une monnaie officielle sur son territoire déclinée en moyens de paiement ayant cours légal. Cependant plusieurs pays peuvent :

Pour des raisons pratiques, une devise a souvent des subdivisions, légales ou traditionnelles : 1/1000 (les millimes du dinar tunisien), 1/100 (les eurocents de l’euro), 1/20, 1/10, 1/8, 1/4 (le "quarter" américain)…

La norme ISO 4217 codifie chaque devise avec l'aide de trois lettres : les deux premières précisent le pays et la dernière généralement son initiale (toutefois le code peut changer en cas de redéfinition de la devise sans changement de son nom).

Ainsi les dollars américains et canadiens sont codés respectivement USD (US=United States, D=dollar) et CAD (CA=Canada), la livre britannique GBP (GB=Great Britain, P=pound). L’euro est une exception codé EUR car il concerne plusieurs pays de l‘Union européenne (EU = Union européenne), ainsi que plusieurs autres pays qui bien que non membres officiels de l’Union monétaire, y participent avec l’accord de la Commission européenne, ou par suite d’accords ou traités internationaux, ou de quelques pays européens qui l’utilisent "de facto" (en absence de banque centrale autonome).

Ils ont été rendus obsolètes par la norme ISO 4217 pour un usage international, ils restent utilisés et même recommandés pour un usage national.

Quelques symboles :

Voir : 

Le cours des devises (taux de change entre deux devises) est fixé sur le marché des changes.

D'après un rapport de la Banque des règlements internationaux (BRI), ce sont plus de 5 000 milliards de dollars qui ont été échangés quotidiennement au cours de l'année 2013.




</doc>
<doc id="19368" url="https://fr.wikipedia.org/wiki?curid=19368" title="Gottlob Frege">
Gottlob Frege

Friedrich Ludwig Gottlob Frege (né le à Wismar – mort le à Bad Kleinen) est un mathématicien, logicien et philosophe allemand, créateur de la logique moderne et plus précisément du calcul propositionnel moderne : le calcul des prédicats.

Il est en outre considéré comme l'un des plus importants représentants du logicisme. C'est à la suite de son ouvrage "Les Fondements de l'arithmétique", où il tente de dériver l'arithmétique de la logique, que Russell lui a fait parvenir le paradoxe qui porte son nom. Néanmoins Frege n'entendait nullement réduire le raisonnement mathématique à sa seule dimension logique. Son idéographie visait à associer sur la même page, et de manière explicite, le contenu mathématique (ligne horizontale de la page) et la structure logique (ligne verticale).

Frege est né en 1848 à Wismar, Mecklembourg-Schwerin (aujourd'hui partie du Mecklembourg-Poméranie occidentale). Son père Carl Alexander Frege (1809-1866) a été le cofondateur et le directeur d'un lycée de filles jusqu'à sa mort. Après la mort de Carl, l'école a été dirigée par la mère de Frege, Auguste Wilhelmine Sophie Frege (née Bialloblotzky, 12 janvier 1815 – 14 octobre 1898); Sa mère était Auguste Amalia Maria Ballhorn, descendante de Philipp Melanchthon et son père, Johann Heinrich Siegfried Bialloblotzky, est un descendant d'une famille noble polonaise qui a quitté la Pologne au .Durant son enfance, Frege a rencontré des philosophies qui guideront sa future carrière scientifique. Par exemple, son père a écrit un manuel sur la langue allemande pour les enfants âgés de 9 à 13 ans, intitulé "Hülfsbuch zum Unterrichte in der deutschen Sprache für Kinder von 9 "bis" 13 Jahren" ( ed., Wismar 1850; ed., Wismar et Ludwigslust: Hinstorff, 1862), dont la première section traite de la structure et de la logique du langage.

Frege a étudié dans un "gymnasium" à Wismar et est diplômé en 1869. Son professeur Gustav Adolf Leo Sachse, qui était un poète, a joué le rôle le plus important dans la détermination de la future carrière scientifique de Frege, l'encourageant à continuer ses études à l'université de Jena.

Frege est matriculé à l'Université d'Iéna au printemps de 1869 en tant que citoyen de la Confédération de l'Allemagne du Nord. Durant les quatre semestres de ses études, il a assisté à environ vingt conférences, la plupart sur les mathématiques et la physique. Son professeur le plus important était Ernst Karl Abbe (1840–1905, physicien, mathématicien et inventeur). Abbe a donné des conférences sur la théorie de la gravité, le galvanisme, l'électrodynamique, la théorie de l'analyse complexe des fonctions d'une variable complexe, les applications de la physique, et la mécanique des solides. Abbe était plus qu'un professeur pour Frege: il était un ami de confiance et, en tant que directeur du constructeur optique Carl Zeiss AG, il était en mesure de faire avancer la carrière de Frege. Après l'obtention du diplôme de Frege, ils avaient une correspondance plus étroite.

Ses autres enseignants universitaires notables ont été Christian Philipp Karl Snell (1806–86, utilisation de l'analyse infinitésimale en géométrie, géométrie analytique des plans, mécanique analytique, optique, fondements physiques de la mécanique); Hermann Karl Julius Traugott Schaeffer (1824–1900, géométrie analytique, physique appliquée, analyse algébrique, télégraphe et autres machines électroniques); Et le philosophe Kuno Fischer (1824–1907, philosophie kantienne).

À partir de 1871, Frege a poursuivi ses études à Göttingen, l'université de premier plan en mathématiques en territoires germanophones, où il a assisté aux conférences d'Alfred Clebsch (1833–72, géométrie analytique), Julius Schering (1824–97; Théorie de la fonction), Wilhelm Eduard Weber (1804–91, études physiques, physique appliquée), Eduard Riecke (1845-1915, théorie de l'électricité) et Hermann Lotze (1817–1881, philosophie de la religion). Beaucoup de doctrines philosophiques du Frege mature ont des parallèles avec celles de Lotze; Elles ont fait l'objet d'un débat scientifique, s'il y a eu ou non une influence directe sur les opinions de Frege découlant de ses conférences de Lotze.

En 1873, Frege a obtenu son doctorat sous la direction de Ernst Christian Julius Schering, avec une thèse ayant pour titre « "Ueber Eine geometrische Darstellung der imaginären Gebilde in der Ebene" » (« Sur une Représentation Géométrique des Formes Imaginaires dans le Plan »), dans lequel il visait à résoudre des problèmes fondamentaux de géométrie comme l'interprétation mathématique des points infiniment distants (imaginaires) de la géométrie projective. 

En 1874, Frege est retourné à Iéna et, obtient l'habilitation universitaire des enseignants à la faculté de philosophie, « "Rechnungsmethoden, die sich auf einer Erweiterung des Größenbegriffes Gründen" » ou « "méthodes de calcul, qui sont basées sur la généralisation du concept de la taille" », qui était fondamentalement basé sur la théorie des fonctions complexes. À partir de 1879, Frege devient professeur à la faculté de philosophie d'Iéna, qu'il occupera pendant la majeure partie de sa vie.

Les premiers travaux de Frege montrent une orientation principalement tournée vers la géométrie et l'analyse complexe. Nous savons peu de choses sur son intérêt pour la logique mathématique (qu'il s'agisse d'un tournant ou d'un traitement prolongé). Les problèmes d’arithmétiques et de théorie des nombres sont également présents, notamment dans la section 1 des "Fondements de l'arithmétique"). Cependant, nous ne connaissons pas ses motivations en détail.
L'intérêt de Frege pour les fondements philosophiques des mathématiques a été relativement précoce, et a commencé à rechercher la justification mathématique des entiers naturels. Ce faisant, il devait avoir réalisé avec un certain étonnement que les mathématiques de l'époque n'atteignaient pas du tout, ou n'atteignaient pas suffisamment, le but des mathématiques. C'est pourquoi il a d'abord dû s'occuper de ce problème, et il l'a fait — en dépit du fait qu'il était d'abord un peu à l'écart de son propre aveu des méthodes et des résultats qu'il devait appliquer et accepter. Ce travail aboutit à l'émergence d'un nouveau type de théorie de la logique, que Frege publiera plus tard dans son "Idéographie".

Au cours de ses recherches, il lui est venu à l'idée que l'arithmétique faisait partie de la logique. La capacité d'une personne à se familiariser avec les entiers naturels n'est pas principalement dû à l'expérience, ni à l'espace géométrique, mais au langage et à la capacité d'analyse de la pensée, communément appelée logique. Ce genre de conception philosophique des mathématiques et/ou de l'arithmétique est communément appelé la logique.
En 1879, le premier de ses trois ouvrages principaux, l"'Idéographie", est publié de son vivant (un an après la mort de sa mère).

1884: Cinq ans après la publication de l"'Idéographie", et après quelques défenses de celui-ci, Frege publie son second ouvrage principal, « "Les Fondements de l'arithmétique" » ("Die Grundlagen der Arithmetik"). Frege a probablement déduit les leçons tirées de la réception de son travail précédent, tel qu'expliquer les idées et les justifications du sujet en question sous une forme plus compréhensible au grand public, car « ce serait plus favorable à la réception des deux travaux ». Ces efforts en tout état de cause, ont réussi: "Les Fondements de l'arithmétique" ont été construit avec précision, détails, et concision. Dans ce travail, Frege s'est essentiellement divisé en trois tâches scientifiques:
Frege épousa Margarete Katharina Sophia Anna Lieseberg (15 février 1856 – 25 juin 1904) le 14 mars 1887. Ils ont eu deux enfants qui sont morts très jeunes. Ils ont adopté Paul Otto Alfred Fregét (à l'origine nommé P. O. A. Fuchs). Nous ne connaissons que peu de choses de la vie privée de Frege, sa retraite, son silence et sa nature pensive.

En 1893, Frege publie l'un des travaux les plus importants de sa vie, les « "Lois fondamentales de l'arithmétique" » ("Die Grundgesetze der Arithmetik", Volume I). Dans cet ouvrage, les entiers naturels ont été complètement formalisées, mais Russell a révélé plus tard qu'il existait des contradictions dans celui-ci.
Pendant cette période, il a également publié la plupart de ses articles portant sur la philosophie du langage.

En 1918, Frege prend sa retraite. Entre 1906 et 1918, il ne publie pratiquement rien (à l'exception de quelques discussions dans lesquelles il critiquait ses collègues mathématiciens comme ). En plus de sa crise créatrice, le paradoxe découvert par Russell a été accompagné de tragédies personnelles de sa vie, comme la perte de sa femme en 1904, et la grave détérioration de sa propre santé.

Frege refuse l'invitation que lui fait Bertrand Russell, pour assister au cinquième congrès mathématique international à Cambridge en 1912. Sa réponse négative montre son désespoir.

À partir de 1918, cependant, il a publié des articles importants, traitant de la nature de la pensée selon Frege, la logique philosophique et mathématique y est détaillée. Ces publications et leur élan, laissent penser que sa période dépressive, qui avait été si longue, était, au moins temporairement, terminé.

En 1923, il est arrivé à la conclusion que l'idée (le logicisme) selon laquelle l'arithmétique est entièrement basée sur la logique, est une erreur. Il a alors commencé à considérer la géométrie comme une science possible pour la fondation des mathématiques. Bien qu'il ait commencé à élaborer cette idée, il n'a pas pu approfondir, à cause de sa mort. Il n'a en effet publié aucune de ces idées.

Il est mort d'épigastralgie à Bad Kleinen le 26 juillet 1925, dans sa ville natale, Wismar, où il a été enterré.


Bien que son éducation et son travail mathématique précoce se concentrent principalement sur la géométrie, le travail de Frege s'est vite orienté vers la logique. Son a marqué un tournant dans l'historique de la logique. Le "Begriffsschrift" a ouvert un nouveau terrain, et un traitement rigoureux des idées de fonctions et de variables. Le but de Frege était de montrer que les mathématiques se développaient hors de la logique, et, ce faisant, il a conçu des techniques qui l'ont amené bien au-delà de la logique propositionnelle syllogistique et stoïcienne.

En effet, Frege a inventé la logique des prédicats axiomatique, en grande partie grâce à son invention de variables quantifiées, qui finit par devenir omniprésente en mathématiques et en logique. La logique précédente avait traité les opérateurs logiques "et", "ou", "... si... alors", "non", et "certains" et "tous", mais les itérations de ces opérations, en particulier « "Il existe" » et « pour "tous" », étaient peu comprises : même la distinction entre une phrase comme « chaque garçon aime une fille » et « une fille est aimée par chaque garçon » ne pourrait être représentée que de manière très artificielle, alors que le formalisme de Frege n'avait aucune difficulté à exprimer les différentes lectures de « chaque garçon aime une fille qui aime un garçon qui aime une fille ».

Un exemple fréquemment utilisé est que la logique d'Aristote est incapable de représenter des énoncés mathématiques tel que le théorème d'Euclide, un théorème fondamental de la théorie des nombres qui déclare qu'il existe une infinité de nombres premiers. La « notation conceptuelle » de Frege peut représenter de telles inférences. L'analyse des concepts logiques et la formalisation des concepts qui ont été essentielles pour les "Principia Mathematica" (3 vols., 1910–13) (par Bertrand Russell, 1872–1970 et Alfred North Whitehead, 1861–1947), à la théorie des descriptions de Russell, aux théorèmes d'incomplétude de Kurt Gödel (1906–78), et à la théorie sémantique de la vérité d'Alfred Tarski (1901–83), sont finalement dues à Frege.

L'un des objectifs de Frege était d'isoler des principes d'inférence logiques, de sorte qu'on n'ait nullement besoin de l'intuition. S'il y avait un élément intuitif, il devait être isolé et représenté séparément comme axiome : à partir de là, la preuve devait être purement logique. Après avoir exposé cette possibilité, le but plus large de Frege était de défendre la vue selon laquelle l'arithmétique est une branche de la logique, une vision connue sous le nom de logicisme : contrairement à la géométrie, l'arithmétique devait être démontrée comme n'ayant aucun fondement intuitionniste, et d'axiomes non-logiques.
Cette idée a été formulée dans des termes non-symboliques dans "Les Fondements de l'arithmétique" (1884). Plus tard, dans ses "Lois fondamentales de l'arithmétique" (vol. 1, 1893 ; volume 2, 1903 ; le vol. 2 a été publié à ses frais), Frege a tenté de dériver, en utilisant son symbolisme, toutes les lois de l'arithmétique d'axiomes qu'il a affirmés comme logiques. La plupart de ces axiomes ont été reportés de son "Begriffsschrift", en entraînant quelques changements importants. Le principe vraiment nouveau était celui qu'il appelait la Loi fondamentale V : la « "value-range" » de la fonction "f"(x) est la même que la « "range-value" » de la fonction "g"(x) si et seulement si ∀"x"["f"(x) = "g"(x)].

Cette loi peut être formulée en notation moderne comme suit : soit {"x"|"Fx"} l'extension du prédicat "Fx", c'est-à-dire l'ensemble de tous les Fs, et de manière similaire pour "Gx". Puis, la loi fondamentale V dit que les prédicats "Fx" et "Gx" ont la même extension iff ∀"x" ["Fx" ↔ "Gx"]. L'ensemble de Fs est identique à l'ensemble de Gs dans le cas où chaque F est un G et chaque G est un F. La loi fondamentale V peut simplement être remplacée par le principe de Hume, qui indique que le nombre de Fs est le même que le nombre de Gs si et seulement si les Fs peuvent être mis en correspondance bijective avec les Gs. Ce principe, également, est cohérent si l'arithmétique de second ordre est suffisante pour démontrer les axiomes de l'arithmétique de second ordre. Ce résultat est appelé théorème de Frege.

La logique de Frege, maintenant connue sous le nom de logique du second ordre, peut être affaiblie par la logique dite prédicative de second ordre. La logique prédicative de second ordre ainsi que la loi fondamentale V est formellement compatible avec les méthodes finitistes ou constructives, mais elle ne peut interpréter que des fragments d'arithmétique très faibles.

Le travail de Frege en logique a eu peu d'attention internationale jusqu'en 1903, lorsque Russell a écrit une annexe aux "The Principles of Mathematics" indiquant ses différences avec Frege. La notation schématique utilisée par Frege n'avait pas d'antécédents (et n'a eu aucun imitateur depuis). Jusqu'à ce que Russell et Whitehead avec leurs Principia Mathematica apparaissent en 1910-13, l'approche dominante de la logique mathématique était encore celle de George Boole (1815-64) et de ses descendants intellectuels, en particulier Ernst Schröder (1841-1902). Les idées logiques de Frege se sont néanmoins répandues dans les écrits de son élève Rudolf Carnap (1891-1970) et d'autres admirateurs, en particulier Bertrand Russell et Ludwig Wittgenstein (1889-1951).

Selon Frege, d'une part, la pensée est inséparable du langage ; seul le langage permet à l'attention de se libérer de l'immédiateté sensible, mais il le fait par d'autres éléments sensibles, à savoir les signes; le langage libère donc la pensée comme la technique de navigation contre le vent libère du vent par le vent. Mais, d'autre part, les langues ordinaires pèchent par équivocité des signes, et aussi par le fait qu'elles ne sont pas calquées sur les lois objectives de la pensée, mais sur celles de la psychologie humaine. Il convient donc de mieux distinguer les deux, grâce à l'invention d'une langue spéciale, calquée sur les exigences logiques. L'écriture constitue une étape importante dans la libération de la pensée rigoureuse ; elle permet de s'appuyer sur des signes constants, et aussi de rapporter librement l'énoncé aux lois de la logique. Dans ces conditions, la première tâche de la logique sera d'édifier un langage logique aussi rigoureux que possible, où toute lacune dans l'exposé des raisons sera aperçue d'un coup d'œil. (Que la science justifie le recours à une idéographie, article publié en 1882 dans le "Zeitschrift für Philosophie und philosophische Kritik" (81).)

Frege a développé une conception du langage à la suite de ses recherches logiques. "Über Sinn und Bedeutung" est l'article classique qui expose deux problèmes à propos de la signification des phrases, et où il montre que l'on doit distinguer sens et dénotation :

Frege distingue sens et dénotation ; la dénotation est l'objet auquel on fait référence, le sens est le mode de donation de la dénotation.
Exemple :

Cette distinction, qui sera rejetée par Russell, a pour objet d'expliquer qu'une formule comme a=b ait une utilité, c'est-à-dire qu'elle ne se réduit pas à a=a. Nous apprenons par cette formule que deux concepts distincts renvoient à un seul et même objet. En effet le concept se dit d'un objet, mais ne se confond pas avec lui. Le cheval est en fait un certain objet que nous dénotons par sa propriété d'être un certain cheval. Il y a un cheval veut dire qu'il existe un x (objet dénoté), tel qu'il est un cheval (concept signifié). En effet, le langage désigne le plus souvent moins chaque objet par un nom propre que par une catégorie commune à plusieurs objets.

Notons que Frege explique qu'il ne faut pas psychologiser cette distinction. Le sens n'est nullement la représentation subjective que chacun introduit sous le concept. Il est rigoureux et universel. L'expression "2+2" a la même dénotation que "3+1", mais non le même sens. Elle ne renvoie pourtant en rien à quelque image subjective.

De son vivant, les articles de Frege furent soit refusés, soit négligés, tant par les logiciens que les philosophes.
C'est Russell, qui le premier, reconnut l'importance de cette œuvre.

Rudolf Carnap, un des membres du Cercle de Vienne a suivi les cours que donnait Frege à Iéna.

L'influence de Frege fut double.

Enfin, le père de la phénoménologie, Husserl, critiqué âprement dans un article de Frege, et accusé de psychologisme, modifia ses conceptions.







</doc>
<doc id="19380" url="https://fr.wikipedia.org/wiki?curid=19380" title="Nicola Sirkis">
Nicola Sirkis

Nicolas Henri Didier Sirchis, dit Nicola Sirkis, né le à Antony, est un auteur-compositeur-interprète, chanteur et musicien français. Il est un des membres fondateurs du groupe français Indochine.

Nicola Sirkis est le frère jumeau de l'ancien guitariste du groupe, Stéphane Sirkis, mort le . Il est aussi le frère cadet du réalisateur Christophe Sirchis.

La famille paternelle de Stéphane et Nicola Sirkis est d'origine juive russe venue de Chișinău (ville qui fut roumaine puis russe aujourd'hui capitale de la Moldavie) marquée politiquement à gauche. Leur famille maternelle est une famille de militaires français marquée politiquement à droite. Alors que Nicolas et Stéphane n'ont que deux ans, leur famille déménage à Bruxelles, où leur père a été nommé ingénieur chimiste au sein d'Euratom. C'est donc dans une famille plutôt bourgeoise qu'ils seront élevés, bercés par la musique classique qu'écoutaient leurs parents. Nicola Sirkis affirme toutefois qu'en grandissant, sa jeunesse avait été fortement perturbée par le divorce de ses parents, la drogue et l'alcool, et qu'il a plusieurs fois été renvoyé de ses établissements scolaires (dont le lycée Emmanuel-Mounier de Châtenay-Malabry dans les Hauts-de-Seine et le lycée Jacques Monod de Clamart) avec son frère Stéphane. Lorsqu'il a 12 ans, sa mère quitte la Belgique et s'installe en France avec ses trois enfants à Châtillon.

En mai 1981, il forme avec un ami, Dominique Nicolas, le groupe Indochine, avec comme nom de scène Nicola Sirkis. Son frère Stéphane et Dimitri Bodianski les rejoignent par la suite. "L'Aventurier", premier album du groupe, est un grand succès. Les albums "Le Péril jaune", "3" et "7000 danses" vont suivre, enchaînant les tubes : "Kao Bang", "Canary Bay", "3 nuits par semaine", "", "Tes yeux noirs", "Salômbo", "Les Tzars"...
Mais dans les années 1990, Indochine est délaissé par la presse qui considère le groupe comme démodé (un article virulent parlera de « sinistre ringardise échappée des années 1980 »). Cette décennie marquera une longue traversée du désert pour Nicola Sirkis et Indochine. Mais le chanteur du groupe ne perd pas espoir, et insiste pour qu'Indochine poursuive sa route sous le même nom, malgré le départ en 1995 de Dominique Nicolas.

Nicola Sirkis perd son frère Stéphane Sirkis le et la sortie de l'album "Dancetaria" est retardée en conséquence. Pour lui rendre hommage, Nicola Sirkis organise « Le Stef concert » durant le Dancetaria Tour, et écrit en 2001 (sur l'album "Paradize)" la chanson "Electrastar", composée par Olivier Gérard.

Le 26 juin 2010, Indochine devient le premier groupe de rock français à remplir le Stade de France.

En 1992, il sort son unique album solo, "Dans la lune...", réalisé avec Marie Guillard et le groupe Les Valentins.
Nicola Sirkis a participé le au marathon des mots de Toulouse où il a fait une lecture de "Un jour rêvé pour le poisson banane et autres nouvelles" de J. D. Salinger.

Il participe parfois à la rédaction de certains journaux en tant que "rédacteur en chef d'un jour" (ex : aux Dernières Nouvelles d'Alsace, au Soir, à Ouest-France, au figaroscope).

En 2014, il chante "Hexagone" sur l'album-hommage "La Bande à Renaud", qui est sorti le 9 juin. Puis sur le volume 2, qui sorti le 27 octobre, il interprète "Petite conne".

En 2015, il interprète "L'opportuniste", en duo avec Jacques Dutronc, sur l'album "Joyeux Anniversaire M'sieur Dutronc" qui parut le 9 mars, à l'occasion des 70 ans du chanteur.

Il vit d'abord avec la graphiste-photographe Marion Bataille avec qui il reste sept ans. C'est elle qui a conçu les pochettes d'Indochine de 1982 à 1989. Il épouse ensuite l'actrice Marie Guillard le 22 juillet 1995.

En 1996, il rencontre Gwenaëlle Bouchet - dite Gwen B ou Gwen Blast, bassiste du groupe Madinkà - fan d'Indochine qui participa en tant que figurante dans les clips "Satellite" et "Drugstar" de l'album "Wax". Ils ont une fille, Théa, née le 25 octobre 2001, et se marient le , à Paris. Le couple se sépare en 2009.

Indochine joue le 15 juin 1985 au concert organisé par SOS Racisme Place de la Concorde. 

Indochine a enregistré une reprise de "Road to Nowhere", chanson du groupe Talking Heads sur un album en faveur de la liberté de la presse dans le monde en 2002, ainsi qu'une version de "You Spin Me Round (Like a Record)" du groupe Dead or Alive, en 2008 au profit de Reporters sans frontières, pour le boycott de la cérémonie d'ouverture des JO de Pékin.

Nicola Sirkis est le parrain officiel de plusieurs associations :
Le site officiel du groupe propose une rubrique « Sites associatifs » qui renvoie à différentes organisations luttant contre les discriminations sexuelles, l'homophobie, le racisme, pour la liberté de la presse, du Tibet, ou la protection de la nature, ainsi que des associations à but caritatif, notamment dans des domaines médicaux.
Le chanteur s'est par ailleurs positionné contre le FN, en comparant le parti présidé par Marine Le Pen à un parti néo-nazi. Par ailleurs, Nicola Sirkis a condamné les propos homophobes tenus par certains membres du groupe de rap Sexion d'assaut dans leurs chansons, changeant au passage de maisons de disques.







</doc>
<doc id="19382" url="https://fr.wikipedia.org/wiki?curid=19382" title="Livre égyptienne">
Livre égyptienne

La livre égyptienne (LE, £E, ou E£ ainsi qu'EGP selon la norme ISO 4217-liste des codes des monnaies) est la monnaie officielle de l'Égypte, bien qu'un grand nombre de monnaies, notamment le dollar américain, la livre sterling et l'euro, soient en circulation sur le territoire, une grande part de l'activité économique du pays étant liée au tourisme international.

En 1834, un décret royal promut la loi parlementaire qui prévoit la création d'une monnaie sur une base bimétallique. La livre égyptienne remplace alors le "qirsh" comme monnaie d'échange. Le "qirsh" a néanmoins continué de circuler équivalent au « centime » de la livre, soit , le "qirsh" étant lui-même divisé en 40 "paras". En 1885, le "para" est supprimé et le "qirsh" divisé en dixièmes (, "oshr al-qirsh"). Ces dixièmes de centièmes sont rebaptisés "mallîm" (« millimes ») en 1916.

Le cours légal est fixé par la loi en ce qui concerne les transactions internationales. L’Égypte adoptant "de facto" l’étalon-or entre 1885 et 1914, avec le cours d'une livre pour d’or. Après la Première Guerre mondiale, la livre égyptienne est alignée sur la livre sterling au cours de 0,975 £E pour une livre sterling, cours qui restera en vigueur jusqu’en 1962, quelques années après la crise du canal de Suez, quand elle est légèrement dévaluée et alignée sur le dollar américain, au cours de pour . En 1973, avec la dévaluation du dollar et l'adoption du régime de changes flottants, le cours est actualisé à . En 1978, la livre est dévaluée et un dollar vaut alors 0,7 livre. Elle abandonne, en 1989, le régime d'alignement sur le dollar américain et vaut désormais .

La Banque nationale d'Égypte a émis les premiers billets le . La Banque centrale d'Égypte et la Banque nationale d'Égypte ont fusionné en 1961 pour devenir la Banque centrale d'Égypte. Les billets représentent en général des monuments de l'Égypte antique ou des mosquées. Notons qu'aucun monument chrétien n'est représenté, bien qu'environ 15 % de la population soit chrétienne.

Celle-ci a annoncé en novembre 2016, son intention de laisser flotter la livre égyptienne.



</doc>
<doc id="19384" url="https://fr.wikipedia.org/wiki?curid=19384" title="Stéphane Sirkis">
Stéphane Sirkis

Stéphane Paul Denis Sirchis, dit Stéphane Sirkis, né le à Antony, dans les Hauts-de-Seine (France) et mort le d'une hépatite C à Paris, est un musicien français, connu pour avoir fait partie entre 1982 et 1999 du groupe Indochine. Il est le frère jumeau dizygote de Nicola Sirkis, membre fondateur du groupe.

Stéphane Sirkis est né le 22 juin 1959, quelques minutes après son frère jumeau, Nicola Sirkis à Antony dans les Hauts-de-Seine.
La famille paternelle de Stéphane et Nicola Sirkis est d'origine juive russe marquée politiquement à gauche. Leur famille maternelle est une famille de militaires français marquée politiquement à droite. Il a un frère aîné Christophe. 

Alors que Nicolas et Stéphane n'ont que deux ans, leur famille déménage à Bruxelles, où leur père a été nommé ingénieur chimiste au sein d'Euratom. C'est donc dans une famille plutôt bourgeoise qu'ils seront élevés, bercés par la musique classique qu'écoutaient leurs parents. Les jumeaux sont placés dans un pensionnat catholique à la frontière française. Sa jeunesse est notamment perturbée par le divorce de ses parents. Lorsqu'il a 12 ans, sa mère quitte la Belgique et s'installe en France avec ses trois enfants à Châtillon.

Au début des années 1970, après avoir manifesté un intérêt pour le jazz, Stéphane fonde son propre groupe, Light, influencé par le rock progressif et des formations comme Genesis, Yes ou Soft Machine.

En 1976, il découvre le mouvement punk et est séduit par la rébellion qu'il véhicule. Les notions d'autonomie et de liberté promues par le mouvement punk et dans lesquelles il se reconnaît l'amènent à se détacher des organisations politiques militantes et hiérarchisées au sein desquelles il évoluait jusqu'alors.

En 1979, il quitte Paris et devient agent d'exploitation de remontées mécaniques à La Plagne. En mai 1981, Nicola Sirkis et Dominique Nicolas fondent le groupe Indochine avec l'aide de leur ami Dimitri Bodianski et enregistrent un premier disque en novembre 1981. Stéphane Sirkis observe les débuts du groupe de son frère, tout en s'inscrivant à la faculté de Malakoff pour suivre des cours de psychologie. Il participe à la programmation d'Indochine puis rejoint officiellement le trio en avril 1982. Il devient le guitariste d'Indochine et participe à la composition de plusieurs chansons dans les années 1980.

Son rôle dans la composition s'accroît après le départ du principal compositeur et membre fondateur du groupe Dominique Nicolas en 1995. En effet, après avoir envisagé d'abandonner le nom Indochine et de lancer un duo avec son frère, Stéphane et Nicola décident de maintenir l'existence du groupe et de sortir l'album Wax en 1996. Il est ainsi l'un des compositeurs principaux des albums Wax et Dancetaria, ce dernier album ayant été enregistré après sa mort.

De 1985 à 1999, Stéphane Sirkis compose ainsi la musique de quinze chansons pour six albums du groupe.

Stéphane Sirkis est intéressé par l'audiovisuel. Il est ainsi coprésentateur avec Alain Chabat de l'émission "4C+" sur Canal+ en 1985.

En dehors d'Indochine, il a composé :

Dans les années 1990, il compose, au côté de son frère aîné Christophe Sirchis, des musiques pour des documentaires et des spots publicitaires.

Stéphane Sirkis a envisagé de réaliser un album solo, projet resté à jamais inachevé.

Il meurt d'une hépatite C le , alors que son frère Nicola Sirkis est à Bruxelles pour l'enregistrement d'un morceau de l'album "Dancetaria". Il est enterré dans l'intimité au cimetière parisien de Bagneux, division 70, le .

La vie privée de Stéphane Sirkis est rarement révélée au public. En 1987, il épouse Sophie avec qui il a une fille : Lou Sirkis, née le 8 août 1990. Elle participa au concert d'Indochine au stade de France le 26 juin 2010. Elle joue maintenant et chante dans son propre groupe, Toybloïd. Après son divorce, Stéphane reste très proche de sa fille.

Jusqu'à sa mort, il a vécu avec Éliette, une femme avec qui il ne s'est jamais présenté en public.

Il vécut dans un petit pavillon à Bagneux (Hauts-de-Seine) dans la rue Salvador Allende jusqu'à son décès.

En février 2009, Christophe Sirchis publie "Starmustang", un livre qu'il présente comme un hommage à son frère Stéphane Sirkis. Ce livre relate également le point de vue de Christophe Sirchis sur l'histoire d'Indochine et sur les événements ayant précédé la mort de Stéphane.



</doc>
<doc id="19385" url="https://fr.wikipedia.org/wiki?curid=19385" title="Dominique Nicolas">
Dominique Nicolas

Dominique Nicolas est un musicien et un compositeur français, né le à Paris, dans le (France). Il est connu pour avoir été le cofondateur en 1981 du groupe Indochine, Il fut le tout premier guitariste et le principal compositeur du groupe jusqu'en 1994. Il a ainsi écrit la musique de chansons mythiques comme "L'aventurier", "Sexe", "Trois nuits par semaine", "Tes yeux noirs", "Canary Bay", etc.

Ayant depuis tourné la page de ses années avec Indochine, il a sorti le son premier album solo intitulé "La Beauté de l'idée".

Son tout premier clip, "Underground", réalisé par Peggy m. et Scarlett Owls, est sorti le .

Dominique vit son enfance à proximité de la capitale. Étudiant, sa première passion pour le sport moto qu’il pratique assidûment en compétition. En 1978, Dominique s’intéresse également à la musique, à la culture rock et squatte le Gibus Club et Le Rose Bonbon. Conquis, il investit dans une guitare, un ampli, une boite à rythmes, puis enregistre ses premières maquettes. À la même époque, il sympathise avec un autre habitué du Gibus, Nicola Sirkis. Ils formeront tous les deux et avec quelques autres musiciens le groupe Les Espions.

Le , Nicola Sirkis et Dominique Nicolas fondent le groupe Indochine. Il ne leur faudra que quelques mois pour composer leurs premiers morceaux.

Dominique en est le guitariste et le principal compositeur. En 1982, Indochine assure la première partie de Depeche Mode et très vite tubes et albums s'enchaînent : "L'Aventurier" (1982), "Le Péril jaune" (1983), "3" (1985), "Indochine au Zénith" (1986), "7000 danses" (1987), "Le Baiser" (1990), "Le Birthday Album" (1991), "Un jour dans notre vie" (1993), "Radio Indochine" (1994).

À la suite de divergences de points de vue avec l'un de ses acolytes, il quitte le groupe en 1994.

Après avoir quitté le groupe, il a continué à travailler dans la musique et l'image (musiques de publicité, documentaires, projets personnels).

Passionné par la pêche à la mouche, il a participé à deux émissions de Histoires naturelles en 1988 et 2004 et a réalisé pour la chaîne thématique Seasons plusieurs documentaires sur ce sujet de 2002 à 2004 (dont "Les carnets d’un moucheur" sorti en VHS et DVD).


Un album solo intitulé "La beauté de l'idée", dans un style pop/rock/twang, sort le 25 mai 2015. 

La réalisation de l'album est née tout d'abord, de sa rencontre avec Noël Matteï, en 2010. Les deux musiciens apprennent à se connaitre et très vite des affinités se nouent tant artistiquement que humainement. Ils décident de collaborer ensemble pour l'album. Dominique compose et joue tous les instruments tandis que Noël Matteï écrit les textes qui abordent des thèmes tels que la liberté, l’évasion, les secrets, l’amour, les illusions et désillusions. « Il raconte ma vie en quelque sorte » déclarera Dominique.

Son tout premier clip "Underground" extrait de son futur album, est mis en ligne depuis le 18 novembre 2014. Il a été réalisé par Peggy.m et Scarlett Owls. Il est disponible sur les plateformes de téléchargement légal depuis le .










Sa façon de composer reste la même qu’à ses débuts, son style musical évolue au fil des ans, tout en gardant sa touche personnelle. Dominique compose d'abord à la guitare, trouve une idée, une mélodie et une ambiance, qu'il note sur un enregistreur, puis ensuite en fait une version aboutie avec des arrangements. Il part de sa démo, qu'il produit au fur et à mesure, en y ajoutant des couches d’instruments. Beaucoup de ses guitares démos restent ainsi à la fin et ne sont pas rejouées sur le morceau définitif. Dominique travaille ses sons de guitares avec des effets basiques tels que flanger / delay / compression. Ses guitares sont souvent enregistrées en D.I et aussi réampés si nécessaire.

Pour son son de guitare sur des titres comme "L’Aventurier" ou "Miss Paramount", il utilise un flanger et une chambre d’écho à bande, le tout en D.I.
Il faut le bon dosage d’effet, et se servir du vibrato de la guitare. Il est possible de faire un son approchant avec un delay numérique.
C'est un adepte du son clean. On a souvent comparé ce son à celui des Shadows (Dominique a également expliqué à la télévision avoir eu l'habitude d'accorder sa guitare au son d'Apache, en La). Ses amplis favoris sont des Fender Twin Reverb / Princeton / Champ.

Pour son premier concert au Rose Bonbon en 1981, Dominique utilise une Rickenbacker, mais la guitare la plus utilisée dans la période Indochine, sera une Fender Mustang Competition 1974. Dominique semble avoir une nette préférence pour le son des Fender, et il en utilise plusieurs, dont une Jazzmaster de 1959, une Telecaster 1960, et une Stratocaster Road Worn 60 OWT.







</doc>
<doc id="19387" url="https://fr.wikipedia.org/wiki?curid=19387" title="Interopérabilité">
Interopérabilité

L’interopérabilité est la capacité que possède un produit ou un système, dont les interfaces sont intégralement connues, à fonctionner avec d’autres produits ou systèmes existants ou futurs et ce sans restriction d’accès ou de mise en œuvre.

Il convient de distinguer « interopérabilité » et « compatibilité ». Pour être simple, on peut dire que la compatibilité est une notion verticale qui fait qu'un outil peut fonctionner dans un environnement donné en respectant toutes les caractéristiques et l'interopérabilité est une notion transversale qui permet à divers outils de pouvoir communiquer - quand on sait pourquoi, et comment, ils peuvent fonctionner ensemble.

Autrement dit, on ne peut parler d'interopérabilité d'un produit ou d'un système que si on connaît intégralement toutes ses interfaces.

L'interopérabilité est considérée comme très importante voire critique dans de nombreux domaines, dont l'informatique, le médical au sens large, les activités ferroviaires, l'électrotechnique, l'aérospatiale, le domaine militaire et l'industrie en général. Les différents systèmes, appareils et éléments divers utilisés doivent pouvoir interagir sans heurts.

Compte tenu du fait que ces éléments sont produits par des constructeurs divers, avec des méthodes variées, et qu'ils répondent à des besoins spécifiques, l'idée la plus simple consiste à définir une base explicite, une norme ou un ensemble de normes, que chaque élément va « implanter » dans son propre fonctionnement.

Cette norme joue un double rôle : elle est d'abord un indicateur de la façon dont le dialogue entre les différents éléments doit s'opérer — et cristallise donc les besoins de ce dialogue ; elle est ensuite une passerelle de communication, qui va pouvoir éventuellement s'adapter aux besoins changeants des éléments. La norme est alors la base de conception des interfaces.

Le téléphone est un exemple de système interopérable. Toutes les interfaces sont des normes gérées par l'UIT-T. On peut ainsi téléphoner sans se soucier de la marque de téléphone ou de celui de son correspondant, ni des composantes utilisées par les différents opérateurs.

Le monde anglo-saxon voit l'interopérabilité sous l'angle de l'informatique et des télécommunications, et comme un moyen de puissance et de domination du marché. L'interopérabilité industrielle est traitée par les Anglo-Saxons par l'intermédiaire de l'ingénierie des systèmes, qui est une discipline universitaire.

Le monde universitaire utilise d'ailleurs de plus en plus cette notion comme le démontre une récente étude.

Pour définir plus exactement ce qu'est et n'est pas l'interopérabilité, on peut commencer par la distinguer de la compatibilité. Cette dernière relation est binaire et concerne un ensemble fini de systèmes. A et B sont "compatibles", ou pas, si leurs constructions respectives leur permettent, ou pas, de communiquer et travailler ensemble.

A et B seront dit "interopérables" si, grâce à une ou plusieurs norme(s) externe(s) qu'ils respectent, ils en viennent entre autres à pouvoir être compatibles. L'interopérabilité est générale et ne concerne pas "a priori" des éléments ou systèmes particuliers. Elle existe au travers de normes et formats respectés par tout élément ou système qui souhaite intégrer un plexus interopérable — le réseau des éléments qui communiquent entre eux de façon fluide et normée. On voit que l'interopérabilité ne doit rien au hasard, et résulte d'un accord explicite entre les différents constructeurs d'éléments.

Interopérabilité et uniformité sont souvent mises en relation. Une critique courante est de dire que la course à l'interopérabilité entraîne avec elle un appauvrissement du développement des techniques, en freinant les innovations — qui sont le plus souvent le fait d'un acteur isolé. Il n'est effectivement pas faux de dire que l'interopérabilité amène à une forme d'unicité : le processus par lequel plusieurs systèmes deviennent interopérables se fonde bel et bien sur l'unicité d'une norme à respecter mais pas sur l'unicité des produits qui respectent cette norme, bien au contraire ! La norme peut au contraire favoriser l'apparition de produits concurrents, à la seule condition de porter sur le comportement extérieur, la fonctionnalité ou les interfaces du système ou de l'objet sur lequel elle porte, et non sur sa conception interne.

Il est complètement faux de dire que l'interopérabilité impose un modèle unique de développement des systèmes. Deux systèmes peuvent parfaitement inter-opérer sans pour autant être conçus de la même manière. L'interopérabilité ne concerne que le comportement externe de chaque système, et non ses mécanismes internes. De plus, respecter une norme ne signifie pas ne pas avoir le droit d'en respecter d'autres ou de créer un réseau plus large de systèmes interopérables. Cette décision relève du constructeur ou du prestataire de service. Cela ne signifie pas non plus fermer la porte aux innovations : de simples ajouts peuvent rejoindre une norme existante, et les innovations de plus grande échelle peuvent susciter la mise en place d'une nouvelle norme, qui peut stimuler l'adoption de l'innovation et de ses applications.

Ce problème de l'unicité concerne certains domaines et pas d'autres et pose en fait surtout des problèmes de compatibilité, entre les normes cette fois. La solution la plus courante mais imparfaite consiste à utiliser des normes reposant sur des formats ouverts, et par là rapidement évolutifs. Le domaine de l'informatique illustre plus particulièrement ce point.

L'interopérabilité n'est pas par elle-même un élément concret ou un critère défini. On peut déterminer dans quelle mesure des systèmes sont interopérables en jugeant de leur respect de la norme qui a donné lieu à une interopérabilité. On comprend alors qu'on puisse parler d'interopérabilité partielle : si un logiciel, par exemple, ne respecte qu'une partie d'une norme, il ne pourra peut-être pas dialoguer correctement avec un autre programme, voire pas du tout. Dans l'absolu, seul le respect strict d'une norme donnée conduit à une interopérabilité réelle, mais cette situation est assez éloignée de la réalité.

L'interopérabilité a évidemment de larges implications techniques, mais pas uniquement. Elle peut avoir une incidence sur l'organisation d'une entreprise ou d'un organisme, et pose des questions essentielles. Celles-ci ont trait par exemple aux données et à leur échange : 


La standardisation constitue un élément de réponse pour certaines de ces questions.

Économiquement, l'interopérabilité a des conséquences méconnues du grand public et parfois sous-estimées par les acteurs industriels. Si les produits de plusieurs concurrents ne sont pas interopérables (à cause de brevets exclusifs, de secrets de fabrications ou pour toute autre raison volontaire ou non), on peut aboutir à une situation monopolistique ou bien à un marché fragmenté.

Voir "infra".

Une telle configuration économique se fait au détriment du consommateur. L'informatique, notamment, présente les différents cas : la position de Microsoft par rapport à ses concurrents sur le marché des systèmes d'exploitation illustre bien le premier. Les gouvernements peuvent essayer d'encourager les constructeurs à engager une démarche d'interopérabilité concertée, mais cela se heurte concrètement à des intérêts commerciaux déjà en place. De telles démarches peuvent aussi conduire à des accords semi-ouverts, semi-fermés, c'est-à-dire excluant un ensemble d'acteurs économique au profit d'un petit groupe.

Voir "infra".

Enfin, l'interopérabilité peut renvoyer aux problématiques de la liberté (liberté d'utilisation, liberté de choix...), comme le montre l'USA PATRIOT Act.

L'interopérabilité est une notion absolument cruciale pour les réseaux de télécommunication mondiaux comme le téléphone et l'Internet. Par essence, des matériels divers et variés sont mis en œuvre dans ces réseaux hétérogènes aux côtés d'une panoplie encore plus vaste de matériels informatiques et de logiciels.

Mais elle est cruciale aussi et surtout pour l'ensemble de l'économie, car aujourd'hui, dans presque tous les domaines d'activité, dans l'industrie et dans les services, et même en agriculture, des systèmes informatiques gèrent des données, pilotent des systèmes de contrôle, des systèmes de gestion, et sont interconnectés d'une entreprise à l'autre par des réseaux informatiques (internet, extranet, messageries électroniques, ...).

On peut en quelque sorte affirmer que, du point de vue de l'ingénierie des systèmes, l'interopérabilité informatique pilote l'interopérabilité globale.

Au niveau des gouvernements occidentaux, notons un exercice annuel, le "Coalition Warrior Interoperability Demonstration" visant à vérifier celles-ci et à apporter des solutions industrielles aux difficultés rencontrées.

En télécommunications, le terme peut être défini comme la capacité de fournir et accepter les services d'autres systèmes, et d'utiliser les services échangés afin de permettre un fonctionnement plus efficace pour les deux systèmes.

Dans un environnement radio, l'interopérabilité est composée de trois dimensions :

Différents outils sont utilisés pour assurer l’interopérabilité entre différents matériels informatiques. Parmi ceux-ci, on peut citer les logiciels d'authentification unique (SSO ou Single Sign-On en anglais) qui permettent à plusieurs systèmes d'utiliser une authentification centralisée ou bien les réseaux privés virtuels (Virtual Private Network, VPN en anglais) qui permettent des échanges d'informations sécurisées entre plusieurs réseaux non directement connectés.

Dans l'Union européenne, l'interopérabilité ferroviaire concerne la conception, la construction, la mise en service, le réaménagement, le renouvellement, l'exploitation et la maintenance des éléments des systèmes ferroviaires ainsi que les qualifications professionnelles et les conditions de santé et de sécurité du personnel qui contribue à son exploitation.

Dans l'Union européenne, (dans le domaine ferroviaire), l'interopérabilité est définie comme « l'aptitude du système ferroviaire transeuropéen conventionnel à permettre la circulation sûre et sans rupture de trains en accomplissant les performances requises pour ces lignes. Cette aptitude repose sur l'ensemble des conditions réglementaires, techniques et opérationnelles qui doivent être remplies pour satisfaire aux exigences essentielles ; » 

Dans une certaine mesure, on peut considérer que le code de la route, les gabarits des ponts et des routes correspondent à une certaine forme d'interopérabilité, ainsi que la standardisation des pneumatiques, des équipements dynamiques routiers et les systèmes de paiement électronique sur les autoroutes.

L'interopérabilité des systèmes d'attache aux bornettes est l'un des facteurs cruciaux pour l'extension aux banlieues des réseaux commerciaux de vélopartage comme Vélib'® à Paris, vélo'v® à Lyon et Villeurbanne ou VélôToulouse® à Toulouse.

Les piles et piles rechargeables ont dû être standardisées .

L'électricité distribuée sur les réseaux doit rester dans une certaine plage de tension (220 à ) et de fréquence (), de manière que les équipements puissent être indifféremment branchés sur différents points du réseau, dans différents États.

Pour accompagner le développement des Smart Grids, deux nouvelles normes d'interopérabilité entre les appareils et terminaux et les réseaux électriques intelligents ont été lancées en 2012 par l'ESTI (European Telecommunications Standards Institute) et l’Esna (Energy Services Network Association), pour la zone de l'Union européenne. L’« "Open Smart Grid Protocol" » est une nouvelle couche de protocole de communication standard et le « "BPSK Narrow Band Power Line Channel for Smart Metering Applications" » précise le mécanisme de contrôle du réseau à travers une ligne électrique haute performance à bande étroite.

L'interopérabilité tend à être conçue comme un sujet d'experts, et ses implications dans la vie courante sont quelquefois sous-estimées.

Le cas de Microsoft par rapport à la Commission européenne est révélateur de la façon dont l'interopérabilité concerne des questions importantes sur les relations de puissance. En 2004, la Commission européenne trouva que Microsoft avait abusé de sa position dominante en restreignant délibérément l'interopérabilité entre les micro-ordinateurs Windows et les serveurs de groupes de travail non-Microsoft. En agissant ainsi, Microsoft était capable d'acquérir une position dominante sur le marché pour les systèmes d'exploitation de serveurs de groupes de travail, le cœur de ses réseaux groupe de TIC. On a demandé à Microsoft de publier de la documentation d'interface précise, ce qui devait permettre à des vendeurs rivaux d'entrer en compétition sur un pied d'égalité (« le remède d'interopérabilité »). En juin 2005 la Commission teste sur le marché une nouvelle proposition de Microsoft dans ce sens, comme elle avait rejeté de précédentes propositions comme étant insuffisantes.

Les efforts récents de Microsoft autour de l'interopérabilité pourraient indiquer un changement dans leur approche et leur niveau d'engagement vers l'interopérabilité. Ces efforts incluent la migration des formats de fichiers Microsoft Office vers ECMA Office Open XML, et plusieurs accords avec des partenaires, plus particulièrement leur récent accord de collaboration avec Novell. Cependant, de nombreuses personnes critiquent le format Open XML qui permet l'inclusion d'objets binaires, inclusion incompatible avec la notion d'interopérabilité. Microsoft utilise le mot interopérabilité dans le sens de compatibilité avec ses propres produits.

L'interopérabilité a aussi émergé dans le débat sur le brevet logiciel au Parlement européen (Juin/Juillet 2005). Des critiques prétendent que parce que les brevets sur les techniques nécessitées par l'interopérabilité sont gardés sous conditions RAND ("Reasonable And Non Discriminatory licensing"), les clients auront à payer des licences deux fois : une fois pour le produit et, si nécessaire, une fois pour le programme protégé par le brevet que le produit utilise.

La stratégie précédente est bien connue et classique. Elle concerne généralement un acteur en position de quasi-monopole. C'est grâce à une interopérabilité partielle que des entreprises peuvent acquérir une position hégémonique mondiale. L'interopérabilité implique la connaissance exhaustive des interfaces.

Comme l'Union européenne commençait à organiser un grand marché unique avec la zone euro, les États-Unis ont cherché de la même façon, dès la fin des années 1980 et la chute du mur de Berlin, à mettre en cohérence toute leur économie en encourageant l'interopérabilité : misant en général sur une durabilité faible, laissant de côté (pour l'instant) le protocole de Kyoto, elle cherche à transformer le capital naturel en capital de connaissances. Paul Romer est l'un des principaux théoriciens américains de la croissance économique, qu'il voit tirée par l'innovation technologique, et concerner tous les aspects de la société (changement de paradigme).

Dans cette stratégie, les entreprises des secteurs de l'aéronautique, de l'armement, et de l'informatique intervenant dans la première guerre du Golfe se sont regroupées dans un vaste consortium mondial (NCW), devenu dans sa version civile (énergie, télécommunications, automobile...) Network Centric Operations (opérations réseaux centrées en français, voir Network Centric Operations Industry Consortium), s'appuyant sur des normes mondiales en informatique (W3C, OASIS), et sur les règles du commerce international à travers la Chambre de commerce internationale, le BASD, le WBCSD, l'OCDE... Cette stratégie se met en œuvre dans le cadre d'une politique offensive d'advocacy (soutien aux entreprises) qui comporte un volet important de normalisation dans le système juridique anglosaxon de soft law. L'interopérabilité des données apparaît en évidence dans la stratégie des données en réseau centré du "Department of Defense" américain

Du point de vue de l'influence, les États-Unis jouent sur toutes les vulnérabilités, ou dispositifs qui leur sont favorables, comme la Commission européenne, sur le plan de sa force de proposition et de son fonctionnement.

Concrètement, pour mettre en œuvre ces structures complexes de pilotage, il est nécessaire de disposer d'une infrastructure d'interoperabilité permettant de faire communiquer entre elles un ensemble de communautés comprenant quelquefois plus de employés, appartenant à des organismes différents : entreprises (multinationales, PME/PMI innovantes), centres d'étude et de recherche, universités, appelées clusters aux États-Unis. L'équivalent européen s'appelle pôle de compétitivité.

En fait, pour réaliser les infrastructures d'interopérabilité correspondantes, les méthodes classiques de sécurité informatique (Infrastructure à clés publiques) sont insuffisantes : on doit s'appuyer sur des métadonnées (données des données des ressources informatiques employées par les organismes) qui sont regroupées en ensemble d'éléments dans des registres de métadonnées souvent spécialisés par domaines d'activité. Le Dublin Core donne un canevas général.

En France, le référentiel général d'interopérabilité a un caractère obligatoire pour les administrations et services concernés.

Le droit à la connaissance exhaustive des interfaces d'un produit devrait être un droit accordé sans supplément de prix et sans restriction à celui qui l'acquiert, conférant ainsi la possibilité de faire fonctionner le produit (tant matériel que logiciel) avec d'autres produits existants ou à venir.

Ceci est en parfait accord avec l'article L122-1 du Code de la consommation qui interdit de subordonner l'usage d'un produit à l'achat d'un autre produit. 

La complexité croissante des produits induit une dérive qui doit être contenue par la loi.

Le mot interopérable a fait son apparition dans l'article 4 de la loi 2004-575 du 21 juin 2004 pour la confiance dans l'économie numérique : « On entend par standard ouvert tout protocole de communication, d'interconnexion ou d'échange et tout format de données interopérable et dont les spécifications techniques sont publiques et sans restriction d'accès ni de mise en œuvre ».

La notion d'interopérabilité existe implicitement dans l'article L122-1 du Code de la consommation relatif à la vente liée.

Les normes par domaines :

Voir en particulier : Les normes relatives aux systèmes d'informations.

Il existe différents organismes dont le rôle est de valider des normes que les industries, notamment, utiliseront comme support pour rendre leurs services et produits interopérables, et "a fortiori", compatibles.

ISO : Organisation internationale de normalisation

Pour les systèmes d'information :

Pour l'Internet 

"L'autre guerre des États-Unis, les secrets d'une machine de conquête", Eric Denécé, Claude Revel, 2005, Robert Laffont. 



</doc>
<doc id="19389" url="https://fr.wikipedia.org/wiki?curid=19389" title="Serments de Strasbourg">
Serments de Strasbourg

Les serments de Strasbourg (""), datant du 14 février 842, signent l'alliance militaire entre Charles le Chauve et Louis le Germanique, contre leur frère aîné, Lothaire. Ils sont tous trois les fils de Louis le Pieux, lui-même fils de Charlemagne. Ils précèdent d'un an le traité de Verdun, d'une importance géopolitique considérable.

Louis "le Germanique" déclare son serment en langue romane pour être compris des soldats de Charles le Chauve. Et Charles le Chauve prononce le sien en langue tudesque pour qu'il soit entendu des soldats de Louis. Cette façon de procéder constitue aussi, outre la compréhension par les soldats de l'autre partie, un acte symbolique.

Le texte roman des "Serments" a une portée philologique et symbolique essentielle, puisqu'il constitue pour ainsi dire, « l'acte de naissance de la langue française » dans le cadre d'un accord politique d'envergure historique.

Lothaire, l'aîné des trois frères, revendique le titre d'empereur d'Occident, hérité de leur père Louis le Pieux et restauré par leur grand-père, Charlemagne. Charles et Louis refusent de le reconnaître comme leur suzerain. Lothaire tente d'envahir les États de ses cadets. Ceux-ci se liguent alors contre lui et le battent à Fontenoy-en-Puisaye en juin 841. Pour renforcer leur alliance, Louis le Germanique et Charles le Chauve se rencontrent en février 842 sur le lieu actuel de la Plaine des Bouchers dans le quartier de la Meinau à Strasbourg et y prêtent serment contre leur frère Lothaire. En août 843, le traité de Verdun mettra fin aux hostilités entre les trois frères et dessinera la carte de l'Europe pour les siècles suivants.

Les serments de Strasbourg n'ont pas été conservés dans leur version originale mais sont retranscrits dans l'œuvre de Nithard, "L'Histoire des fils de Louis le Pieux". Même s'ils sont de moindre importance que le traité de Verdun qui les suit de peu, les "Serments de Strasbourg" sont primordiaux du point de vue de l'histoire linguistique, car ils sont une des premières attestations écrites de l'existence d'une langue romane en Francie occidentale (ici l'ancêtre de la langue d'oïl) et d'une langue tudesque. Le médiéviste Philippe Walter écrit :
Le texte de Nithard n’est aujourd’hui connu que par deux manuscrits. Ils se trouvent aujourd'hui tous deux à la Bibliothèque nationale de France.

Le plus ancien manuscrit a été copié vers l’an 1000, sans doute pour l’abbaye de Saint-Médard de Soissons ou celle de Saint-Riquier, toutes deux sises en Picardie. Au , il est en possession de l’abbaye Saint-Magloire de Paris. Vers 1650 il est acheté pour le compte de la reine Christine de Suède et transféré à Rome ; il est acquis après la mort de la reine par la bibliothèque du Vatican. À la suite de la prise de Rome par les Français en 1798, il est transporté à Paris avec tout un lot de manuscrits saisis. Napoléon restituera ensuite la plupart de ces manuscrits, mais pas celui-ci. Déposé enfin à la Bibliothèque nationale de France, il y porte la cote Latin 9768. Le texte des serments se trouve au folio 13.

L’autre manuscrit, également conservé à la Bibliothèque nationale de France (Latin 14663) est une copie du précédent, réalisée au .

La langue romane ici retranscrite est encore à peine séparée du latin vulgaire. C'est un des premiers passages écrits dans une langue romane à être attesté.

Le texte prononcé par Louis le Germanique est :

Soit, en français : 

Les troupes de Charles "le Chauve" promettent :

Soit, en français : 

Cette langue germanique est une forme évoluée de francique parlée dans la région rhénane, dans laquelle Charles "le Chauve" promet :

Soit, en français : 

Les troupes de Louis "le Germanique" jurent :
Soit, en français : (litt. : "contre Charles lui à aide ne devient")

Pour de nombreux philologues qui se sont penchés sur le sujet, la base du document est le latin, alors que pour d'autres spécialistes, il s'agit d'un texte original rédigé en langue maternelle, autonome par rapport au latin.

À partir du , les textes rédigés en langue latine présentent les premiers traits caractéristiques des futures langues française, italienne, espagnole, etc. Vers 800 l'opposition entre latin écrit et langue parlée était reconnue par les contemporains eux-mêmes. Dans ce contexte, la question de la compréhension du latin par les fidèles lors des prédications est posée par les cinq synodes qui se tiennent en 813 en différents lieux. Les évêques se réunissent à Mayence et ils demandent aux prêtres de prêcher de façon que le peuple comprenne, c'est-à-dire en germanique, cependant qu'à Arles et à Chalon-sur-Saône où il se réunissent également, leurs recommandations sont au contraire plus générales puisqu'elles encouragent l'utilisation du latin, d'un latin plus simple certes. Ce qui signifie globalement que la compréhension du latin ne posait guère de problèmes dans les futurs pays de langue d’oc. En revanche, à Reims et à Tours, les évêques recommandent respectivement que le prêche se fasse dans « la langue particulière » de la région et en "rusticam Romanam linguam" (« langue romane rustique »), ce qui indique que la population ne comprenait plus le latin dans ces régions du nord de la France, c'est-à-dire les futures régions de langue d’oïl.

La précocité de la langue d’oïl est illustrée par la "Séquence de sainte Eulalie" qui témoigne d'un usage écrit et littéraire dès le , c'est-à-dire près d'une quarantaine d'années après les "Serments" ; alors que les premiers textes dans les autres langues romanes ne sont pas antérieurs aux , le plus ancien texte en langue occitane (ou en catalan), la "Cançon de Santa Fe", date du .

Il y a une seule phrase de la version romane qui n’a pas d’équivalent dans la version germanique. C’est celle où Louis s’engage à soutenir Charles "". Est-ce une négligence du copiste ? C’est l’explication la plus probable.

Que veut dire exactement cette formule ? On a remarqué son parallélisme avec le latin : « ' », signifiant « en conseil et alliance (armée) ». Et même en grec homérique : « » / ', ni par mes conseils ni par mes forces » ("L'Iliade", IX, 374).

"In" est un latinisme pour "en" qui apparaît au début « "d'ist di en avant" ». Ensuite, "aiudha", issu du latin populaire "*aiuta" (différent du latin classique "adjutat"), d'où l'ancien français "aiue" / "aïe" « aide » ; le digramme "dh" note consonne fricative dentale sonore, même chose dans "cadhuna", issu du latin "*catuna" ("cata + una"), d'où l'ancien français "chaün, cheün" « chaque », avec cependant le maintien du "ca-" initial alors qu'on attendrait déjà à cette époque "cha- / che-", il faut peut-être y voir une forme picarde ou wallo-picarde. "Cosa", en roman du , pouvait avoir un des sens du latin "" « procès, débat », sens qui s’est conservé en français dans l'expression « plaider une cause » (le terme "cause" en français est un emprunt savant au latin classique, avec francisation de la finale "-a" > "-e" [ø]). Le sens général est donc : le soutenir, soit dans les délibérations, soit par force armée.

Quel pouvait être l’équivalent en langue tudesque ? Par un heureux hasard, il a peut-être été conservé dans un texte très différent, mais quasi contemporain des "Serments" : le "Chant de Hildebrand" (""). Ce fragment poétique raconte la discussion, puis le combat mortel entre un père et un fils rangés dans deux armées opposées. Le père, Hildebrand, dit au fils qu’il n’a jamais eu un parent aussi proche que lui pour « "dinc ni geleitos" », assemblée ou escorte (armée). La formule était sans doute traditionnelle pour évoquer une alliance complète, et elle recoupe tout à fait les formules latine et romane.

Un mot n'a pas non plus son équivalent dans le texte germanique, il s'agit de "nunquã" dans « "et ab Ludher nul plaid nunquam prindrai" » (ligne 7). Ce "nunquam" représente une forme archaïque de l'ancien français "nonque" « jamais » (variantes "nonc, nonques"). Il est transcrit ici à la manière latine, puisque l'étymon latin est précisément "nunquam" sans changement.

Ces deux éléments sont l'affirmation, certes discrète, d'une spécificité et du caractère autonome du texte de chacune de ses langues par rapport aux deux autres.

La langue romane des serments appartient globalement au gallo-roman.

Quelques tentatives, généralement anciennes, ont essayé de montrer que le texte roman constituait un témoignage précoce de la langue d'oc, et ce, sur la base de quelques mots du texte qui semblent à première vue plus proches de formes d'oc que de formes d'oïl, par exemple : "sagrament" « serment » (occitan moderne "sagrament" « sacrement ») ; "poblo" « peuple » (occitan moderne "pòble") ; "sendra" (du latin "senior") et l'utilisation de la préposition "ab" (dans « "et ab Ludher" », occitan moderne "amb" « avec » - "ab" en occitan ancien), qui ne s'emploie pas en ancien français au sens comitatif d'« avec ».

En outre, une manière de lire certains éléments du texte peut le laisser penser : le latinisme "nunquam" serait une erreur. . Le tilde sur le "a" de "nunquam" exprime un "me" enclitique. Cette pratique des scribes médiévaux nous est expliquée par la paléographie occitane. Il s’agit d’une construction pseudo-pronominale typique de l’occitan . Nous devrions lire "nul plaid nunqua·m prindrai", de même . Il s’agit également d’une construction pseudo-pronominale. Ces constructions occitanes sont encore vivantes dans la langue d’aujourd’hui. Enfin des considérations historiques semblent aller dans ce sens : les troupes de Charles "le Chauve", comme nous le dit le chroniqueur Nithard, venaient majoritairement non pas du nord de la Gaule, mais d’Aquitaine, région située en grande partie dans le domaine de la langue d’oc.

Cependant, des recherches ultérieures montrent que les termes cités plus haut, peuvent être des archaïsmes de la langue d'oïl, par exemple : "sagrament" est très proche du mot "sacrament" « sacrement », attesté par ailleurs en ancien français au et peut très bien représenter une forme antérieure, intermédiaire entre l'ancien français "sairement" « serment » (attesté vers 1160) et l'ancien français "sacrament" « sacrement » (la lénition de [c] latin devant [r] est observée régulièrement en français dans le groupe /acr/ latin et suppose un stade /agr/, puis /ai(g)r/ comme dans « maigre » issu du gallo-roman , accusatif latin "macrum", dont la forme plus régulière "maire" est trouvée dans certains textes du nord-est de la France, cf. aussi "flagrare" devenu « flairer ») ; "poblo" serait une forme antérieure à "poble", attestée en très ancien français ("Poble ben fist credrë in Deu" dans "La Vie de saint Léger"), ainsi qu'une forme plus évoluée "pueble" ; "sendra" est un masculin au cas sujet singulier, attesté aussi plus tardivement sous la forme "sendre", à laquelle va se substituer "sire" ultérieurement. [ə] final atone est noté "-a" dans le plus ancien français, ainsi parallèlement à "sendra", trouve-t-on "pedra" « père » dans la "Vie de saint Alexis". "Ab" dans « "et ab Ludher" » peut être expliqué par une mauvaise latinisation du vieux français "a" « à », préposition, comme dans l'expression "prendre plait a quelqu'un".

En revanche, des formes plus évoluées phonétiquement sont typiques de la seule et unique langue d'oïl. Ainsi le [p] latin intervocalique est passé à [v] dans "savir" « savoir » (≠ vieil occitan "saber", occitan moderne "saber, saupre"). Le [a] final latin se réduit à [e] devant une palatale "fazet" (≠ ancien occitan "faza", latin "faciat"), [au] latin est passé à [o] dans "cosa" « chose » (≠ occitan "causa" « chose »), la du subjonctif du verbe être ("*siat" en latin vulgaire) est notée "sit" (≠ ancien occitan "sia"). En outre, la graphie "-dh-" rencontrée dans "aiudha, cadhuna" et "Ludher" note une consonne fricative dentale, issue de la lénition d'un [d] intervocalique qui va aboutir à un amuïssement complet en ancien français, ainsi "aiudha" devient "aiue", puis "aïe" « aide » (≠ occitan "ajuda", « aide » est un latinisme < latin classique "adjutat" ≠ latin vulgaire "*aiuta"), "cadhuna" > ancien français "chaün, cheün" « chaque » (≠ occitan "cadun(a)" « chacun(e) »), évolution semblable à "catena" > "*cadena" > "*cadhena" > "chaiene" > "chaine" (≠ occitan "cadena"). Sur le plan grammatical, l'ancien français est la seule langue au sein de la "Romania" à employer de manière systématique le pronom personnel sujet, par exemple : "je" dans les "Serments" sous la forme "eo, io" (il est généralement rendu par la désinence verbale en occitan : "-i", ainsi que les formes irrégulières "fau" « je fais », "vau" « je vais »).

Certains mots de la langue d'oïl se retrouvent tels quels, comme "plaid", encore vivant aujourd'hui quoique rare, et mieux connu par ses dérivés : "plaider", "plaidoyer". Il résulte de l'évolution phonétique du gallo-roman (latin "placitum" « agrément, ordonnance »). D'autres encore sont peu reconnaissables de par leur évolution phonétique, ainsi "dreit" « droit » a subi au stade du moyen français l'évolution en "droit" propre aux dialectes centraux (francien), du nord-est et de Picardie, alors que les dialectes d'oïl du grand ouest ont conservé "dreit" (normand, gallo, angevin, poitevin et saintongeais) ou encore "iv" > français "y" (« "in nulla aiudha contra Lodhuvig nun li iv er." ») issu du latin "ibi" « là, y ». D'autres termes ont disparu comme "salvament" (forme archaïque) que l'on retrouve de manière plus évoluée dans le français médiéval "salvement" « salut, conservation, félicité ». Il existe pourtant sous forme rélictuelle dans le terme de juridiction médiévale "sauvement".

La question du dialecte ou de la langue des "Serments" a continué d’alimenter de nombreuses publications :

Cependant, de nombreux chercheurs s'interrogent sur le bien-fondé de ces conclusions contradictoires et sur la « sincérité » dialectale de la langue romane des serments. Il s'agirait plutôt de la mise par écrit d'une sorte de koinè propre à la France du nord ou plutôt une langue transdialectale et cette forme écrite spécifique présenterait le caractère artificiel d'une langue de chancellerie. En tout cas, B. Cerquiglini constate qu'« aucune œuvre médiévale française (même archaïque) n'est rédigée selon l'usage linguistique d'une seule région dialectale ».

Il existe des textes plus anciens attestant de l'existence d'une langue romane écrite au nord de l'Empire carolingien, comme les "Gloses" de Cassel (environ ou ) ou les "Gloses" de Reichenau () pour les plus célèbres. Ceux-ci, cependant, sont des glossaires, des listes de mots, et ne permettent pas de lire des phrases en langue romane.

Le second texte complet dans l'histoire de la langue française est la "Séquence de sainte Eulalie" qui date vraisemblablement de 880 ou 881. C'est le premier texte littéraire en langue d'oïl et, à ce titre, l'ancêtre de la littérature française.

Photographies du manuscrit à la Bibliothèque nationale de France :




</doc>
<doc id="19392" url="https://fr.wikipedia.org/wiki?curid=19392" title="Liste des arbres fruitiers">
Liste des arbres fruitiers



</doc>
<doc id="19394" url="https://fr.wikipedia.org/wiki?curid=19394" title="Merville (Nord)">
Merville (Nord)

Merville est une commune française située dans le département du Nord (59), en région Hauts-de-France.

Située au sud de l'arrondissement de Dunkerque (Plaine de la Lys) entre Lille (38 km), Hazebrouck (13 km) et Béthune (16 km).

"Mergem" en flamand.

Fin du , Mauront, duc de Douai, fonde un monastère en un lieu appelé Bruël, près de la Lys. Il cède la direction à Aimé de Sion dit Saint-Amé, évêque exilé, en 690.

La population se développe autour du monastère, et ainsi naît une agglomération attestée en latin médiéval sous la forme "Maurontivilla - Broislum" en 697, puis "Menrivilla" en 1076, dans un diplôme de Philippe, roi de France, qui consacre les droits et les biens de la cité à la Collégiale Saint-Amé de Douai. 

En 1431, Meerghem (hameau de la mer, flamand) reçoit le privilège de fabriquer des draps par Philippe le Bon. Ce même roi signe en 1451 la fameuse Ghisle de Menreville, qui constitue la véritable charte de Merville. La cité est rattachée à la France en 1678 par le traité de Nimègue.

De 879 à 1940, la ville a connu 6 destructions totales et 5 partielles. Au cours de la Première Guerre mondiale, Merville est totalement rasée : l'architecture néo-flamande de la ville est caractéristique de la période de reconstruction dans les années 1920.

Au début de la Seconde Guerre mondiale, l'aérodrome est utilisé par les Anglais (Hawker Hurricanes...), puis pris par les Allemands (Messerschmitt Bf 110...) ; ces derniers transforment alors le triangle d'herbe en une structure bétonnée, environnée de blockhaus et de tarmacs disposés en « marguerite », l'aérodrome se situant en effet près de la côte et donc de celle de l'Angleterre (Les hangars situés vers l'aéro-club datent a priori de cette époque). Lors de la libération de la ville, les infrastructures sont alors utilisées par les américains pour réparer et entretenir leurs avions, en particulier les bombardiers lourds (Boeing B-17, Flying Fortress...).

Le 12 juin 1944, un raid de bombardiers de l'USAAF fait de nombreux morts sur l'axe des actuelles « Rue du Général de Gaulle - Rue du docteur Rousseau », le mémorial de 1914-1918 situé aux « deux-ponts » fut endommagé à cette occasion. L'attaque serait due à une erreur d'identification de cible, en effet à cette époque, le débarquement venait de commencer et les aviateurs ont semble-t-il cru à des renforts ou des fuyards allemands lorsqu'il aperçurent nombre d'habitants dans ces rues. Ils provoquèrent la mort de 70 personnes en en blessant 59 autres, uniquement des civils.


 



Désormais, ce rendez-vous sera annuel pour refaire vivre la ville au son de la musique Made in Merville.

La ville ayant été complètement détruite lors de la Première Guerre mondiale, elle ne comporte pas de bâtiments anciens. Par contre, elle recèle de véritables richesses du début du , de l'Hôtel de Ville à l'Église, en passant par ses écluses, moulins, belles demeures et vertes contrées.

Louis Marie Cordonnier en est l'architecte. Si l'extérieur impose de sa superbe, l'intérieur de l'hôtel de ville n'a rien à lui envier : escaliers centraux imposants, salons d'honneurs (2,3,4) et salle du conseil municipal (1) au parquet lustré et vitraux aux couleurs des villes avoisinantes, plaques commémoratives... 
En 2002, la municipalité a confié aux services techniques et à un chantier école, la réfection complète du kiosque. Repeint, reforgé à l'identique, le kiosque s'est vu octroyer un véritable bain de jouvence. 


Dans ses infrastructures - situées sur l'aérodrome de Merville-Calonne, et au Château de la Motte-aux-Bois à (Morbecque) -
cet institut forme des pilotes de lignes, des mécaniciens et autres personnels aéronautiques.

Depuis 1999, le label des Villes Internet, illustré par l’octroi d’une à cinq @, offre à la collectivité locale la possibilité de montrer son implication et sa volonté dans la promotion et la mise en œuvre d’un Internet local citoyen à la disposition de tous pour l’intérêt général. Il est attribué par un jury d’experts engagé pour l’Internet citoyen.




</doc>
<doc id="19405" url="https://fr.wikipedia.org/wiki?curid=19405" title="Noyau de système d'exploitation">
Noyau de système d'exploitation

Un noyau de système d’exploitation, ou simplement noyau, ou (de l'anglais), est une des parties fondamentales de certains systèmes d’exploitation. Il gère les ressources de l’ordinateur et permet aux différents composants — matériels et logiciels — de communiquer entre eux. 

En tant que partie du système d’exploitation, le noyau fournit des mécanismes d’abstraction du matériel, notamment de la mémoire, du (ou des) processeur(s), et des échanges d’informations entre logiciels et périphériques matériels. Le noyau autorise aussi diverses abstractions logicielles et facilite la communication entre les processus.

Le noyau d’un système d’exploitation est lui-même un logiciel, mais ne peut cependant utiliser tous les mécanismes d’abstraction qu’il fournit aux autres logiciels. Son rôle central impose par ailleurs des performances élevées. Cela fait du noyau la partie la plus critique d’un système d’exploitation et rend sa conception et sa programmation particulièrement délicates. Plusieurs techniques sont mises en œuvre pour simplifier la programmation des noyaux tout en garantissant de bonnes performances.

Les premiers concepteurs informatiques avaient l’habitude de décrire les différentes couches logicielles d’un système par une analogie : celle du noyau de la noix.

En anglais, le mot « kernel » désigne le cerneau, la partie comestible de la noix, c'est-à-dire son cœur. A l'inverse, la coque (partie non comestible de la noix) est une enveloppe très dure qui entoure la partie comestible. En anglais cette coque est appelée « shell ». 

Cette analogie permettait de comprendre que l’accès à la partie "comestible et incidemment cachée de la noix implique l'ouverture de la coque avec un instrument spécifique".

La dualité "cerneau / coque" illustre bien le rapport entre le kernel et shell et constitue un paradigme aisément extensible à l'ensemble de l'informatique. Celui-ci peut être aussi compris comme une succession d'étapes permettant de diffuser entre chaque niveau l'ensemble des apports de celui qui le précède pour servir une information de plus en plus enrichies mais également, à chaque niveau, partagée de façon différente : "si l le cerneau est nécessaire, seule la manière dont on peut le briser est d'une grande importance".

En informatique, le noyau d’un système d’exploitation est le logiciel qui assure :

La majorité des systèmes d’exploitation est construite autour de la notion de noyau. L’existence d’un noyau, c’est-à-dire d’un programme unique responsable de la communication entre le matériel et le logiciel, résulte de compromis complexes portant sur des questions de performance, de sécurité et d’architecture des processeurs.

L’existence d’un noyau présuppose une partition virtuelle de la mémoire vive physique en deux régions disjointes, l’une étant réservée au noyau (l’espace noyau) et l’autre aux applications (l’espace utilisateur). Cette division, fondamentale, de l’espace mémoire en un espace noyau et un espace utilisateur contribue beaucoup à donner la forme et le contenu des actuels systèmes généralistes (GNU/Linux, Windows, Mac OS X, etc.), le noyau ayant de grands pouvoirs sur l’utilisation des ressources matérielles, en particulier de la mémoire. Elle structure également le travail des développeurs : le développement de code dans l’espace noyau est "a priori" plus délicat que dans l’espace utilisateur car la mémoire n’y est pas protégée. Ceci implique que des erreurs de programmation, altérant éventuellement les instructions du noyau lui-même, sont potentiellement beaucoup plus difficiles à détecter que dans l'espace utilisateur où de telles altérations sont rendues impossibles par le mécanisme de protection.

Le noyau offre ses fonctions (l’accès aux ressources qu’il gère) au travers des appels système. Il transmet ou interprète les informations du matériel via des interruptions. C’est ce que l’on appelle les entrées et sorties. 

Diverses abstractions de la notion d’"application" sont fournies par le noyau aux développeurs. La plus courante est celle de processus (ou tâche). Le noyau du système d’exploitation n’est en lui-même pas une tâche, mais un ensemble de fonctions pouvant être appelées par les différents processus pour effectuer des opérations requérant un certain niveau de privilèges. Le noyau prend alors en général le relais du processus pour rendre le service demandé et lui rend le contrôle lorsque les actions correspondantes ont été réalisées.

Il peut arriver cependant que le processus puisse poursuivre une partie de son exécution sans attendre que le service ait été complètement réalisé. Des mécanismes de synchronisation sont alors nécessaires entre le noyau et le processus pour permettre à celui-ci, une fois qu'il est arrivé au point où il a besoin que le service ait été rendu, d'attendre que le noyau lui notifie l'exécution effective du service en question.

Un processeur est capable d’exécuter un seul processus, un système multiprocesseur est capable de gérer autant de processus qu’il a de processeurs. Pour pallier cet inconvénient majeur, les noyaux multitâches permettent l’exécution de plusieurs processus sur un processeur, en partageant le temps du processeur entre les processus.

Lorsque plusieurs tâches doivent être exécutées de manière parallèle, un noyau multitâche s’appuie sur les notions de :

Les entrées et les sorties font l’objet d’un traitement spécifique par l’ordonnanceur.

Il existe de nombreux noyaux aux fonctionnalités restreintes tels que les micro-noyaux, les systèmes sans noyau (MS-DOS, CP/M) ou les exo-noyaux.

Ces systèmes sont généralement adaptés à des applications très ciblées mais posent des problèmes variés (de sécurité avec MS-DOS, de performances avec HURD ou QNX). La plupart d’entre eux sont actuellement inadaptés pour une utilisation généraliste, dans des serveurs ou ordinateurs personnels.

Les noyaux ont comme fonctions de base d’assurer le chargement et l’exécution des processus, de gérer les entrées/sorties et de proposer une interface entre l’espace noyau et les programmes de l’espace utilisateur.

À de rares exceptions, les noyaux ne sont pas limités à leurs fonctionnalités de base. On trouve généralement dans les noyaux les fonctions des micro-noyaux : un gestionnaire de mémoire et un ordonnanceur, ainsi que des fonctions de communication inter-processus.

En dehors des fonctions précédemment listées, de nombreux noyaux fournissent également des fonctions moins fondamentales telles que :
Enfin, la plupart des noyaux fournissent également des modèles de pilotes et des pilotes pour le matériel.

En dehors des fonctionnalités de base, l’ensemble des fonctions des points suivants (y compris les pilotes matériels, les fonctions réseaux et systèmes de fichiers ou les services) n'est pas nécessairement fourni par un noyau de système d’exploitation. Ces fonctions du système d’exploitation peuvent être implantées tant dans l’espace utilisateur que dans le noyau lui-même. Leur implantation dans le noyau est faite dans l’unique but d’augmenter les performances. En effet, suivant la conception du noyau, la même fonction appelée depuis l’espace utilisateur ou l’espace noyau a un coût temporel notoirement différent. Si cet appel de fonction est fréquent, il peut s’avérer utile d’intégrer ces fonctions au noyau pour augmenter les performances.

Ces techniques sont utilisées pour pallier des défauts des noyaux tels que les latences élevées. Autant que possible, il est préférable d’écrire un logiciel hors du noyau, dans l’espace utilisateur. En effet, l’écriture en espace noyau suppose l’absence de mécanismes tels que la protection de la mémoire. Il est donc plus complexe d’écrire un logiciel fonctionnant dans l’espace noyau plutôt que dans l’espace utilisateur, les bugs et failles de sécurité étant plus dangereux.

L’ordonnanceur d’un système d’exploitation n’a de sens qu’en système multitâche. Il gère l’ordre dans lequel les instructions de "différentes" tâches sont exécutées et est responsable de la sauvegarde et de la restauration du contexte des tâches (ce contexte est constitué des registres processeurs), appelée également commutation de contexte.

La plupart des ordonnanceurs modernes permettent d’indiquer sur quel processeur sont exécutées les tâches. Certains permettent également de migrer des tâches sur d’autres machines d’une grappe de calcul.

L’algorithme d’ordonnancement détermine quelle tâche doit s’exécuter en priorité et sur quel processeur. Cet algorithme doit permettre d’utiliser efficacement les ressources de la machine. 

L’ordonnancement peut être de type « coopératif » : les tâches doivent être écrites de manière à coopérer les unes avec les autres et ainsi accepter leur suspension pour l’exécution d’une autre tâche. L’ordonnancement peut être également de type préemptif : l’ordonnanceur a la responsabilité de l’interruption des tâches et du choix de la prochaine à exécuter. Certains noyaux sont eux-mêmes préemptifs : l’ordonnanceur peut interrompre le noyau lui-même pour faire place à une activité (typiquement, toujours dans le noyau) de priorité plus élevée.

Le gestionnaire de mémoire est le sous-ensemble du système d’exploitation qui permet de gérer la mémoire de l’ordinateur. Sa tâche la plus basique est d’allouer de la mémoire à des processus lorsqu’ils en ont besoin. Cette mémoire allouée est par défaut propre au processus qui en fait la demande.

Sur les noyaux récents, le gestionnaire de mémoire masque la localisation physique de la mémoire (en mémoire vive ou sur disque dur, dans l’espace de mémoire paginée) et présente au programme une "mémoire globale" uniforme dite mémoire virtuelle. Ainsi, tout processus croit manipuler une mémoire "logique" qui a les propriétés suivantes :

L’intérêt de ne pas indiquer au processus l’emplacement physique des données est de permettre au gestionnaire de mémoire de placer et déplacer à sa convenance les données en mémoire, sans affecter les processus. Ces données peuvent notamment être fragmentées dans la mémoire vive lorsqu’un processus demande un bloc de mémoire d’une taille supérieure au plus grand bloc physique libre. Le contenu de la mémoire peut aussi être migré. Cette migration est faite sur les différents supports mémoires tels que dans la mémoire physique (plus ou moins proche du processeur), dans la mémoire paginée, dans la mémoire accessible par réseaux (grappe de calcul).

La virtualisation de la mémoire permet aussi une gestion optimiste des ressources : la mémoire allouée mais pas encore utilisée peut être virtuellement allouée à plusieurs processus (noyau Linux).

Les programmes dans l’espace utilisateur disposent de pouvoirs restreints sur la mémoire : ils doivent demander au noyau de la mémoire. Le noyau fait appel à son gestionnaire de mémoire pour allouer (ou non) la mémoire au processus qui la demande. Si un processus tente d’utiliser des zones de mémoire ne lui appartenant pas, il est évincé automatiquement. Le mécanisme d’éviction repose sur un mécanisme du processeur, nommément une "unité de gestion de la mémoire", ou MMU, qui signale au noyau l’existence d’un accès fautif. C’est le noyau lui-même qui prend la décision de suspendre ou détruire immédiatement le processus fautif.

Les appels système sont des fonctions :

En plus d’un changement de mode d’exécution, l’appel système suppose au moins deux commutations de contextes :

Le coût d’un appel système est nettement plus élevé qu’un simple appel de fonction intra-processus : alors qu’un appel de fonction ne suppose que quelques instructions primitives (chargement et exécution d’une zone mémoire), le coût d’un appel système se compte en milliers ou dizaines de milliers d’instructions primitives, générant à la fois une charge et des délais d’exécution supplémentaires. Pour ces raisons, les fonctions qui sont utilisées de manière intense sont déplacées dans l’espace noyau. Les programmes utilisateurs font alors un nombre restreint d’appels système de haut niveau. Les nombreuses interactions de bas niveau générées par ces appels système sont effectuées dans l’espace noyau. Cela concerne notamment les pilotes de périphériques.

Les entrées/sorties font également l’objet d’un traitement par l’ordonnanceur.

La gestion du matériel se fait par l’intermédiaire de pilotes de périphériques. Les pilotes sont des petits logiciels légers dédiés à un matériel donné qui permettent de faire communiquer ce matériel. En raison du très grand nombre d’accès à certains matériels (disques durs par exemple), certains pilotes sont très sollicités. Ils sont généralement inclus dans l’espace noyau et communiquent avec l’espace utilisateur via les appels système.

En effet, comme cela a été vu dans le précédent paragraphe, un appel système est coûteux : il nécessite au moins deux changements de contexte. Afin de réduire le nombre des appels système effectués pour accéder à un périphérique, les interactions basiques avec le périphérique sont faites dans l’espace noyau. Les programmes utilisent ces périphériques au travers d’un nombre restreint d’appels système.

Cependant, indépendamment de l’architecture, de nombreux périphériques lents (certains appareils photographiques numériques, outils sur liaison série, etc.) sont/peuvent être pilotés depuis l’espace utilisateur, le noyau intervenant au minimum.

Il existe des couches d’abstraction de matériel (HAL) qui présentent la même interface à l’espace utilisateur et simplifient ainsi le travail des développeurs d’applications. Dans les systèmes de type UNIX, l’abstraction utilisée est le système de fichiers : les primitives , , et sont présentées à l’espace utilisateur pour manipuler toutes sortes de périphériques. On parle dans ce cas de système de fichiers "synthétique".

Il existe toutes sortes de noyaux, plus ou moins spécialisés. Des noyaux spécifiques à une architecture, souvent monotâches, d’autres généralistes et souvent multitâches et multi-utilisateurs. L’ensemble de ces noyaux peut être divisé en deux approches opposées d’architectures logicielles : les noyaux monolithiques et les micro-noyaux.

On considère généralement les noyaux monolithiques, de conception ancienne, comme obsolètes car difficiles à maintenir et moins « propres ». Le noyau Linux était déjà qualifié d’obsolète par Andrew Tanenbaum, dès sa création en 1991. Il ne croyait pas, à l’époque, pouvoir faire un noyau monolithique multiplate-forme et modulaire. La mise en place de micro-noyaux, qui consiste à déplacer l’essentiel des fonctions du noyau vers l’espace utilisateur, est très intéressante en théorie mais s’avère difficile en pratique. Ainsi les performances du noyau Linux (monolithique) sont supérieures à celles de ses concurrents (noyaux généralistes à micro-noyaux), sans compter qu’il fut finalement porté sur de très nombreuses plates-formes et qu’il est modulaire depuis 1995.

Pour ces raisons de performance, les systèmes généralistes basés sur une technologie à micro-noyau, tels que Windows et Mac OS X, n’ont pas un « vrai » micro-noyau enrichi. Ils utilisent un micro-noyau hybride : certaines fonctionnalités qui devraient exister sous forme de mini-serveurs se retrouvent intégrées dans leur micro-noyau, utilisant le même espace d’adressage. Pour Mac OS X, cela forme XNU : le noyau monolithique BSD fonctionne en tant que service de Mach et ce dernier inclut du code BSD dans son propre espace d’adressage afin de réduire les latences.

Ainsi, les deux approches d’architectures de noyaux, les micro-noyaux et les noyaux monolithiques, considérées comme diamétralement différentes en termes de conception, se rejoignent quasiment en pratique par les micro-noyaux hybrides et les noyaux monolithiques modulaires.

Certains systèmes d’exploitation, comme d’anciennes versions de Linux, certains BSD ou certains vieux Unix ont un noyau monolithique. C’est-à-dire que l’ensemble des fonctions du système et des pilotes sont regroupés dans un seul bloc de code et un seul bloc binaire généré à la compilation.

De par la simplicité de leur concept mais également de leur excellente vitesse d’exécution, les noyaux monolithiques ont été les premiers à être développés et mis en œuvre. Cependant, au fur et à mesure de leurs développements, le code de ces noyaux monolithiques a augmenté en taille et il s’est avéré difficile de les maintenir. Le support par les architectures monolithiques des chargements à chaud ou dynamiques implique une augmentation du nombre de pilotes matériels compilés dans le noyau, et par suite, une augmentation de la taille de l’empreinte mémoire des noyaux. Celle-ci devient rapidement inacceptable. Les multiples dépendances créées entre les différentes fonctions du noyau empêchaient la relecture et la compréhension du code. L’évolution du code s’est faite en parallèle à l’évolution du matériel, et des problèmes de portage ont alors été mis en évidence sur les noyaux monolithiques.

En réalité les problèmes de la portabilité de code se sont révélés avec le temps indépendants de la problématique de la technologie des noyaux. Pour preuve, NetBSD est un noyau monolithique et est porté sur un très grand nombre d’architectures, alors que des noyaux tels que GNU Mach ou NT utilisent des micro-noyaux censés faciliter le portage mais n’existent que pour quelques architectures.

Pour résoudre les problèmes évoqués ci-dessus, les noyaux monolithiques sont devenus modulaires. Dans ce type de noyau, seules les parties fondamentales du système sont regroupées dans un bloc de code unique (monolithique). Les autres fonctions, comme les pilotes matériels, sont regroupées en différents modules qui peuvent être séparés tant du point de vue du code que du point de vue binaire.

La très grande majorité des systèmes actuels utilise cette technologie : Linux, la plupart des BSD ou Solaris. Par exemple avec le noyau Linux, certaines parties peuvent être non compilées ou compilées en tant que modules chargeables directement dans le noyau. La modularité du noyau permet le chargement à la demande de fonctionnalités et augmente les possibilités de configuration. Ainsi les systèmes de fichiers peuvent être chargés de manière indépendante, un pilote de périphérique changé, etc. Les distributions Linux, par exemple, tirent profit des modules chargeables lors de l’installation. L’ensemble des pilotes matériels sont compilés en tant que modules. Le noyau peut alors supporter l’immense variété de matériel trouvé dans les compatibles PC. Après l’installation, lors du démarrage du système, seuls les pilotes correspondants au matériel "effectivement" présent dans la machine sont chargés en mémoire vive. La mémoire est économisée.

Les noyaux monolithiques modulaires conservent les principaux atouts des noyaux monolithiques purs dont ils sont issus. Ainsi, la facilité de conception et de développement est globalement maintenue et la vitesse d’exécution reste excellente. L’utilisation de modules implique le découpage du code source du noyau en blocs indépendants. Ces blocs améliorent l’organisation et la clarté du code source et en facilitent également la maintenance.

Les noyaux monolithiques modulaires conservent également un important défaut des noyaux monolithiques purs : une erreur dans un module met en danger la stabilité de tout le système. Les tests et certifications de ces composants doivent être plus poussés.

D’un point de vue théorique, le grand nombre de lignes de code exécutées en mode noyau engendre des problèmes de portabilité. La pratique contredit largement la théorie et les noyaux modulaires sont aujourd’hui les plus portés.

Les limitations des noyaux monolithiques ont amené à une approche radicalement différente de la notion de noyau : les systèmes à micro-noyaux.

Les systèmes à micro-noyaux cherchent à minimiser les fonctionnalités dépendantes du noyau en plaçant la plus grande partie des services du système d’exploitation à l’extérieur de ce noyau, c’est-à-dire dans l’espace utilisateur. Ces fonctionnalités sont alors fournies par de petits serveurs indépendants possédant souvent leur propre espace d’adressage.

Un petit nombre de fonctions fondamentales est conservé dans un noyau minimaliste appelé . L’ensemble des fonctionnalités habituellement proposées par les noyaux monolithiques est alors assuré par les services déplacés en espace utilisateur et par ce micro-noyau. Cet ensemble logiciel est appelé .

Ce principe a de grands avantages théoriques : en éloignant les services des parties critiques du système d’exploitation regroupées dans le noyau, il permet de gagner en robustesse et en fiabilité, tout en facilitant la maintenance et l’évolutivité. En revanche, les mécanismes de communication (IPC), qui deviennent fondamentaux pour assurer le passage de messages entre les serveurs, sont très lourds et peuvent limiter les performances.

Les avantages théoriques des systèmes à micro-noyaux sont la conséquence de l’utilisation du mode protégé par les services qui accompagnent le micro-noyau. En effet, en plaçant les services dans l’espace utilisateur, ceux-ci bénéficient de la protection de la mémoire. La stabilité de l’ensemble en est améliorée : une erreur d’un service en mode protégé a peu de conséquences sur la stabilité de l’ensemble de la machine.

De plus, en réduisant les possibilités pour les services de pouvoir intervenir directement sur le matériel, la sécurité du système est renforcée. Le système gagne également en possibilités de configuration. Ainsi, seuls les services utiles doivent être réellement lancés au démarrage. Les interdépendances entre les différents services sont faibles. L’ajout ou le retrait d’un service ne perturbe pas l’ensemble du système. La complexité de l’ensemble est réduite.

Le développement d’un système à micro-noyau se trouve également simplifié en tirant parti à la fois de la protection de la mémoire et de la faible interdépendance entre les services. Les erreurs provoquées par les applications en mode utilisateur sont traitées plus simplement que dans le mode noyau et ne mettent pas en péril la stabilité globale du système. L’intervention sur une fonctionnalité défectueuse consiste à arrêter l’ancien service puis à lancer le nouveau, sans devoir redémarrer toute la machine.

Les micro-noyaux ont un autre avantage : ils sont beaucoup plus compacts que les noyaux monolithiques. 6 millions de lignes de code pour le noyau Linux 2.6.0 contre en général moins de pour les micro-noyaux. La maintenance du code exécuté en mode noyau est donc simplifiée. Le nombre réduit de lignes de code peut augmenter la portabilité du système.

Les premiers micro-noyaux (comme Mach) n’ont pas tout de suite atteint ces avantages théoriques. L’utilisation de nombreux services dans l’espace utilisateur engendre les deux problèmes suivants :

Le grand nombre d’appels système et la communication sous-jacente sont un défaut inhérent à la conception des micro-noyaux. Dans L4, il a été résolu en plaçant encore plus de services en espace utilisateur. La rapidité de traitement des IPC a pu être améliorée en simplifiant les communications au maximum, par exemple en supprimant toute vérification des permissions, laissant ce soin aux serveurs externes.

Ces modifications radicales ont permis d’obtenir de bonnes performances mais elles ne doivent pas faire oublier qu’un micro-noyau doit être accompagné d’un grand nombre de services pour fournir des fonctionnalités équivalentes à celles des noyaux monolithiques. De plus, la grande liberté dont disposent les services au niveau de la sécurité et de la gestion de la mémoire accroît la difficulté et le temps de leur développement (ils doivent fournir leurs propres interfaces).

La dénomination désigne principalement des noyaux qui reprennent des concepts à la fois des noyaux monolithiques et des micro-noyaux, pour combiner les avantages des deux.

Lorsque, au début des années 1990, les développeurs et concepteurs se sont aperçus des faiblesses des premiers micro-noyaux, certains réintégrèrent diverses fonctionnalités non fondamentales dans le noyau, pour gagner en performance. Les micro-noyaux « purs » semblaient condamnés à l’échec.

Alors que la philosophie générale des systèmes à micro-noyaux est maintenue (seules les fonctions fondamentales sont dans l’espace noyau), certaines fonctions non critiques, mais très génératrices d’appels système, sont réintégrées dans l’espace noyau. Ce compromis permet d’améliorer considérablement les performances en conservant de nombreuses propriétés des systèmes à micro-noyaux.
Un exemple de ce type de noyau hybride est le noyau XNU de Mac OS X. Il est basé sur le micro-noyau Mach 3.0, mais qui inclut du code du noyau monolithique BSD au sein de l’espace noyau.

Cette dénomination est également utilisée pour désigner d’autres types de noyaux, notamment les noyaux monolithiques sur micro-noyaux (temps réel ou non) tels que L4Linux (Linux sur L4), MkLinux (le noyau Linux sur Mach), Adeos, RTLinux et RTAI.

Plus rarement, on peut rencontrer le terme pour remplacer improprement ou .

Étymologiquement, 'exo' signifie en grec 'hors de'. Un exo-noyau est donc un système d'exploitation fonctionnant en espace utilisateur (en 'user-space', au lieu du 'kernel-space' dans le cas des autres noyaux). Les fonctions et services du système d'exploitation sont assurés par de petits modules qui, selon les approches techniques, sont des bibliothèques dynamiques (MIT, LibOSes, voir aussi Unikernel) ou des démons (IntraServices).

Un « méta-noyau » est un ensemble de logiciels qui vise à appliquer la notion de noyau informatique au niveau d’un réseau informatique, en créant une unique couche de gestion des périphériques au niveau d’un réseau.

De cette manière, les logiciels peuvent être déployés et utilisés sur le réseau informatique comme s’il s’agissait d’une machine unique, et l’ensemble des logiciels fonctionnant sur cette plate-forme peuvent se partager les ressources de manière intégrée, comme elle le ferait sur un noyau simple.

Un méta système doit également permettre la personnalisation, la gestion des permissions ainsi que l’utilisation d’informations dépendant de la localisation.

Cette notion rejoint les notions de grappe de calcul, de machine virtuelle, de serveur d'applications et de CORBA.

Les noyaux temps réel sont fonctionnellement spécialisés. Ce sont des noyaux généralement assez légers qui ont pour fonction de base stricte de garantir les temps d’exécution des tâches. Il n’y a pas à proprement parler de notion de rapidité de traitement ou de réactivité dans les noyaux temps réel, cette notion est plutôt implicite à la garantie des temps d’exécution en comparaison aux critères temporels de l’application industrielle (la réactivité d’un système de freinage ABS n’a pas les mêmes critères temporels que le remplissage d’une cuve de pétrole).

Très utilisés dans le monde de l’électronique embarquée, ils sont conçus pour tourner sur des plates-formes matérielles limitées en taille, puissance ou autonomie.

Les noyaux temps réel peuvent adopter en théorie n’importe quelle architecture précédemment listée. Ils fournissent souvent deux interfaces séparées, l’une spécialisée dans le temps réel et l’autre générique. Les applications temps réel font alors appel à la partie temps réel du noyau.

Une des architectures souvent retenue est un noyau hybride qui s’appuie sur la combinaison d’un micro-noyau temps réel spécialisé, allouant du temps d’exécution à un noyau de système d’exploitation non spécialisé. Le système d’exploitation non spécialisé fonctionne en tant que service du micro-noyau temps réel. Cette solution permet d’assurer le fonctionnement temps réel des applications, tout en maintenant la compatibilité avec des environnements préexistants.

Par exemple, on peut avoir un micro-noyau temps réel allouant des ressources à un noyau non temps réel tel que Linux (RTLinux, RTAI, Xenomai) ou Windows (RTX). L’environnement GNU (resp. Windows) peut alors être exécuté à l’identique sur le noyau pour lequel il a été conçu, alors que les applications temps réel peuvent faire directement appel au micro-noyau temps réel pour garantir leurs délais d’exécutions.

VxWorks est un noyau propriétaire temps réel très implanté dans l’industrie bien que les systèmes à base de noyau Linux se déploient énormément et aient un succès grandissant via RTAI et Xenomai (RTLinux étant breveté).

 : source utilisée pour la rédaction de cet article




</doc>
<doc id="19407" url="https://fr.wikipedia.org/wiki?curid=19407" title="Mont-Terrible">
Mont-Terrible

Le département du Mont-Terrible est un ancien département français, dont le chef-lieu était Porrentruy.

Le « mont Terrible » qui lui a donné son nom s'appelle aujourd'hui le « mont Terri » (Canton du Jura, Suisse).

Il fut constitué en 1793 par l'annexion française de la République rauracienne, constituée en 1792 par une partie de l'évêché de Bâle.

Le 5 germinal an I (), la Convention nationale prit un décret « qui réunit à la France le pays de Porrentruy, sous le nom de département du Mont-Terrible ».
Il est rédigé comme suit :

Le territoire du « pays de Porrentruy » correspondait à la partie de l'ancienne principauté épiscopale de Bâle sur laquelle le prince-évêque de Bâle avait conservé l'intégralité de son autorité temporelle, à l'exception de la seigneurie de Schliengen.
Il s'agissait des territoires suivants :

Le 10 octobre 1793, le commissaire Bernard de Saintes avait fait occuper la principauté de Montbéliard.
Par un arrêté du 11 octobre 1793, il l'avait déclarée provisoirement réunie à la France.
L'annexion "de facto" de la principauté de Montbéliard ne fut acceptée par le duc de Wurtemberg, Frédéric-Eugène, que le 5 fructidor an IV (), date à laquelle il ratifia le traité de paix conclu à Paris, le 20 thermidor précédent (), entre Charles-François Delacroix, fondé de pouvoir du Directoire exécutif, d'une part, et le baron Charles de Woellvarth et Conrad d'Abel, fondés de pouvoirs du duc de Wurtemberg et Teck, d'autre part.
Ce traité, arrêté et signé par le Directoire exécutif le 21 thermidor an IV (), et ratifié par le Corps législatif le 28 thermidor suivant (15 août 1796), contenait un article 4 rédigé comme suit :
Dès la fin du mois d'octobre 1793, les quarante municipalités de la principauté de Montbéliard furent distribuées en trois cantons, savoir :
Le 19 pluviôse an VI (), la Ville et République de Bienne émirent le vœu d'être réunies à la République française.
Ces territoires, sur lesquels le prince-évêque de Bâle n'avait conservé, jusqu'en 1792, qu'une autorité nominale, étaient les suivants : 

Les actuelles communes suisses suivantes ne relevèrent jamais du département français du Mont-Terrible ni de celui du Haut-Rhin :
Elles constituaient deux enclaves du canton suisse de Soleure (en allemand : "Kanton Solothurn"), situées entre les départements français du Mont-Terrible et du Haut-Rhin.
La première enclave correspondait à l'actuelle commune de Petit-Lucelle ; la seconde, aux actuelles communes de Bättwil, Hofstetten-Flüh, Metzerlen-Mariastein, Rodersdorf et Witterswil, composant le Leimental soleurois.

La Seigneurie de Schliengen ne fut jamais annexée au département du Mont-Terrible.
Située sur la rive droite du Rhin, elle comprenait : Istein, Huttingen, Mauchen, Schliengen et Steinenstadt.
Par le Recès du , la députation extraordinaire de la Diète impériale de Ratisbonne en décida la sécularisation ainsi que la médiatisation, par son incorporation au Margraviat de Bade.

Le département du Mont-Terrible fut supprimé, sous le Consulat, par la loi du 28 pluviôse an VIII (), « concernant la division du territoire français et l'administration ».
Son territoire fut incorporé au département du Haut-Rhin, dont il forma deux de cinq arrondissements, savoir :

En 1815 suite au Congrès de Vienne, le territoire qui avait constitué ce département fut attribué en grande partie au canton suisse de Berne, l'actuel canton du Jura et le Jura bernois. Le reste fut partagé entre le département français du Doubs (Montbéliard) et le canton de Bâle-Campagne.

Le département du Mont-Terrible était divisé jusqu'à loi du 22 août 1795, en deux districts :

Le district de Delémont était subdivisé en dix cantons, savoir :

Le district de Porrentruy était subdivisé en huit cantons, savoir :





</doc>
<doc id="19408" url="https://fr.wikipedia.org/wiki?curid=19408" title="Meurthe (département)">
Meurthe (département)

Le département de la Meurthe était un département français qui exista entre 1790 et 1871, et dont le chef-lieu était Nancy.

Sa disparition est liée à la défaite française de 1871 et à la première annexion de l'Alsace-Lorraine.

Créé par décret du 27 janvier 1790 d'une partie de la province de Lorraine, il comprenait alors neuf districts : Blâmont, Dieuze, Lunéville, Nancy, Pont-à-Mousson, Sarrebourg, Toul, Vézelise et Vic, qui prendra ensuite le nom de district de Château-Salins.

Du 19 juin au 23 novembre 1793, le canton de Drulingen dépend du district de Sarrebourg et le canton de Wolfskirchen dépend du district de Dieuze. Ces deux cantons sont ensuite rattachés au Bas-Rhin.

Entre 1790 et 1795, l'administration du département siègeait alternativement à Nancy et à Lunéville.

Le département fut divisé en cinq arrondissements en 1800, avec Nancy comme préfecture. Ses sous-préfectures étaient Château-Salins, Lunéville, Sarrebourg et Toul.

Dans un souci de régulariser la ligne séparative avec le Bas-Rhin, le conseil général de la Meurthe prit le 25 juin 1818 une délibération demandant le rattachement au canton de Fénétrange de cinq communes du Bas-Rhin : Baerendorf, Hirschland, Rauwiller, Kirrberg, Gœrlingen. Mais ce souhait n'eut aucune suite favorable car le ministre de la justice y transmit un avis négatif.

En 1806, le département de la Meurthe avait locuteurs germanophones.

Louis Antoine Michel indique en 1822 qu'il y a deux caractères bien distincts dans le langage des habitants de la Meurthe, que la langue de la population de l'Est n'a rien de commun avec celle du reste du département. Soit : un français « cadencé et un peu chantant » pour une part, et un dialecte allemand pour l'autre. Il indique aussi que dans les principales villes, on parle avec assez de pureté, sans accent marqué.

Dans son Dictionnaire de 1836, l'abbé E. Grosse décrit le langage usité dans ce département comme suit : . . .

Ce département avait 714 communes et habitants en 1866.

Il était divisé en 29 cantons, répartis comme suit entre les arrondissements :

En 1871, le traité de Francfort, par lequel l'Empire Allemand annexait la plus grande partie de l'Alsace et un quart de la Lorraine, amputa le nord-est du département, dont les arrondissements de Château-Salins et de Sarrebourg. La partie restante constitua, avec l'arrondissement de Briey, partie non annexée du département de la Moselle, le nouveau département de Meurthe-et-Moselle, rendant ainsi hommage au département perdu par la France.

Après que l'Alsace et la Lorraine furent à nouveau entièrement françaises par le traité de Versailles en 1919, les anciennes limites de départements ne furent jamais reconstituées. La commune de Raon-lès-Leau revendique toujours sa forêt de exigée par Bismarck pour s'assurer le contrôle du sommet stratégique du Donon en échange du maintien de la gare d'Avricourt en territoire français et conservée depuis par la commune alsacienne de Grandfontaine, laquelle dépendait du département des Vosges jusqu'en 1871.




</doc>
<doc id="19409" url="https://fr.wikipedia.org/wiki?curid=19409" title="Cryptozoologie">
Cryptozoologie

La cryptozoologie (du grec ancien , « caché », , « animal », et , « étude », soit « étude des animaux cachés ») désigne la recherche des animaux dont l'existence ne peut pas être prouvée de manière irréfutable. Ces formes animales sont appelées cryptides.

Le terme a été inventé par le biologiste écossais Ivan T. Sanderson. Ce néologisme est selon le GDT une . Lorsque la recherche porte sur des animaux anthropomorphes « cachés » tels que le yéti, on parle plus spécifiquement de cryptoanthropologie.

Il n'existe aucune formation universitaire, ni aucun institut scientifique officiel de cryptozoologie. Le cryptozoologue le plus connu est Bernard Heuvelmans, docteur en sciences d'origine belge, qui a consacré une grande partie de sa vie à chasser des formes animales encore inconnues. Auteur de "Sur la piste des bêtes ignorées" (quatre tomes publiés entre 1955 et 1970), en 1999, Bernard Heuvelmanns a déposé l'entier de sa documentation et de ses archives au Musée de zoologie de Lausanne.

On peut définir la cryptozoologie comme l'étude et la recherche d'animaux de moyenne et de grande taille non encore officiellement répertoriés et dont l'existence controversée pourrait néanmoins être établie sur base de preuves testimoniales (témoignages oculaires), circonstancielles (films, photos, enregistrements de cris), ou même autoscopiques (que chacun peut voir : empreinte de pied, poils, plumes, etc.), mais considérées comme insuffisantes par la communauté scientifique des zoologues. Dans sa méthodologie, elle peut faire appel à diverses disciplines, telles la zoologie, la paléontologie, la paléoanthropologie, etc., mais aussi la psychologie, l'ethnologie, la mythologie, voire la police scientifique.

On peut classer son sujet d'étude en 5 catégories, qui parfois se complètent l'une l'autre :

Le champ d'étude de la cryptozoologie ne se limite pas au Bigfoot, Yéti et autres monstres du Loch Ness, mais s'étend à toute créature vivante non identifiée, pour autant que la taille soit égale ou supérieure à celle d'une grenouille et qu'elle ait laissé une trace dans l'esprit humain. Ainsi les insectes, à quelques exceptions près, n'en font pas partie, car trop petits pour avoir frappé les esprits. Les découvertes fortuites d'animaux ne font pas partie de la cryptozoologie. 


En ce sens, la cryptozoologie s’apparente davantage à l’étude de la mythologie en relation avec la nature, qu’à une recherche d’espèces disparues ou encore inconnues.

L'étude des témoignages a été le point de départ de recherches ayant mené à la découverte d'animaux à la fin du :

Partant de ces exemples, la cryptozoologie étudie témoignages et objets désignés comme preuves. À l'heure actuelle, les résultats de la cryptozoologie ont quelquefois été probants, mais de nombreux éléments présentés comme preuves ont été invalidés par un examen rigoureux : Bernard Heuvelmans a ainsi rejeté des « mains de singes pétrifiées » présentées comme des mains du Yéti et conservées dans un monastère, en montrant qu'elles n'étaient en fait que des molaires fossilisées d'éléphants (les racines étant considérées comme des doigts).

De même, des prétendus poils de Yéti trouvés dans l'Himalaya ont été analysés et proviennent du goral, chèvre de l'Himalaya. Cette analyse a également permis de découvrir que l'aire de répartition du goral était plus étendue vers l'est.

D'autres poils (à la suite d'une longue recherche aussi exhaustive que possible) ont montré, par leur analyse ADN qu'une espèce d'ursidé inconnu existait.

Le Collateral Humanoid Project a été lancé en 2012 par une équipe de chercheurs des universités d’Oxford et de Lausanne sous l'impulsion du généticien anglais Bryan Sykes et du zoologue suisse Michel Sartori. Pour la première fois, des scientifiques ont décidé de procéder à l'analyse génétique de l'ADN mitochondrial attribués au yéti, Bigfoot et autres créatures anthropoïdes inconnues. Ils ont donc lancé un appel à toutes les personnes détenant des échantillons de ce type.

Les conclusions de leur étude ont été publiées en juillet 2014 dans la revue scientifique internationalement connue, "Proceedings of The Royal Society". Le résultat des analyses de 36 échantillons, essentiellement des touffes de poils détenues par des particuliers, ne révèle aucun animal inconnu : coyote, chèvre, grizzli, tapir, raton laveur, humain, chien, etc. À l’exception de deux fragments provenant, l’un d’un animal tué dans les années 1970 à Ladakh en Inde et l’autre, d’un prétendu nid de yétis dans une forêt de bambous au Bhoutan. Le premier est d’une teinte brun doré, le second a des reflets rougeâtres. Ils appartiennent toutefois à la même espèce : un ours préhistorique que l’on croyait disparu depuis . L’ADN de ces poils est très proche de celui du fossile d’un ancêtre de l’ours polaire du Pléistocène découvert au Svalbard, un archipel situé à l’est du Groenland, à la limite de l’océan Arctique et de l’Atlantique.

Selon Bernard Heuvelmans, pour être une science, la cryptozoologie doit répondre à deux impératifs quant à ses acteurs et quant à son objet :

Cependant, la question de fond demeure : si elle approuve l'étude scientifique des (pour les accepter ou les rejeter), la majeure partie de la communauté scientifique s'interroge sur le statut épistémique d'une discipline étudiant des animaux dont on disposerait de traces non pas formelles, mais culturelles (représentations) ou testimoniales. S'il est légitime pour une discipline telle que la sociologie d'étudier les folklores liés aux visions de créatures folkloriques, une discipline ayant pour objet d'étudier non plus les témoignages en tant que témoignages, non pas les représentations, mais bien la probabilité de l'existence d'une créature du fait même qu'elle est représentée, a-t-elle sa place au sein de la zoologie ? La cryptozoologie s'inscrit en effet dans une démarche pseudo-scientifique sans objet scientifique directement observable et .

La principale raison pour laquelle une grande partie de la communauté scientifique considère que l'existence du Bigfoot, du monstre du Loch Ness ou du Mokèlé-mbèmbé est plus qu'improbable est qu'aucune preuve de leur existence n'a jamais été fournie à ce jour, ni aucun spécimen vivant ou mort qui puisse être examiné par la communauté scientifique.

Or, concernant ces animaux, seuls ont été produits des empreintes de pied ou de main, des photos ou des films qui peuvent être potentiellement des contrefaçons. De fait, même des sciences établies, comme la zoologie et la paléontologie, sont confrontées à ce problème (par exemple la contrefaçon de l'Homme de Piltdown présenté comme un fossile d'une espèce inconnue alors qu'il s'agissait de l'assemblage d'un crâne d"'Homo sapiens" et d'une mandibule d'orang-outan).

L'action de la cryptozoologie se borne donc ici à étudier des objets et est purement zoologique. On peut parler d'un autre domaine lors de l'appel à d'autres sciences (psychologie, sociologie par exemple dans le cas de l'analyse de la véracité des témoignages) ne relevant pas de la zoologie. 

En paléontologie, l'identification de espèces fossiles inconnues se fonde à la fois sur des collections de fossiles dont l'analyse critique par les pairs mais aussi sur le contexte phylogénétique des espèces que l'on cherche à identifier. Or si la cryptozoologie s'intéresse le plus souvent à des cas proches d'animaux connus, elle se penche aussi (et est surtout connue pour cela) sur certains animaux (Yéti, Grand Serpent de mer) qui présentent le plus des caractéristiques anatomiques qui sont difficiles à intégrer à l'histoire évolutive des espèces connues. Il n'y a donc pas dans ce dernier cas d'éléments de comparaison.



Quelques bandes dessinées, comme "Kenya", de Léo, "Adèle et la Bête", de Tardi, "Tintin au Tibet" de Hergé, reprennent des thèmes cryptozoologiques, faisant apparaître dans leurs récits des animaux inconnus (yéti) ou disparus (ptérodactyle, mastodonte, etc.).

Dans le monde fictif et parallèle des Cités obscures, inventé par Benoît Peeters et François Schuiten, cette discipline serait (si l'on en croit leur ouvrage "Le Guide des Cités") la plus importante dans le domaine de la zoologie. Les Cités obscures semblent, d'après les auteurs, pauvres en créatures animales, ce qui explique l'importance de cette science.






</doc>
<doc id="19410" url="https://fr.wikipedia.org/wiki?curid=19410" title="Léman">
Léman

Le Léman, appelé aussi par tautologie lac Léman, mais aussi lac de Genève est un lac d'origine glaciaire situé en Suisse et en France ; par sa superficie, c'est le plus grand lac alpin et subalpin d'Europe centrale et d'Europe de l'Ouest.

Son nom, probablement d'origine celtique, nous est parvenu via le latin « Lacus Lemanus ».

Le lac, d'une longueur d'environ et d'une largeur maximale inférieure à , est en forme de croissant (ou d'une virgule) orienté de l'est vers l'ouest. Le rivage nord et les deux extrémités sont suisses et sont partagé entre les cantons de Genève, de Vaud et du Valais, le rivage sud est français dépendant du département de la Haute-Savoie et de la région Auvergne-Rhône-Alpes. La frontière passe au milieu du lac.

Le Léman est traversé d'est en ouest par le Rhône, fleuve franco-suisse qui, avec 75 % des apports, constitue le principal affluent du lac. Sa formation a des origines multiples : plissement tectonique pour la partie du Grand-Lac et action du glacier du Rhône pour le Petit-Lac (entre Yvoire et Genève). Il s'est constitué lors du retrait du glacier du Rhône après la dernière période glaciaire, il y a près de ans. Ses berges ont été fortement artificialisées.

En 2006, selon une étude de la CIPEL (commission franco-suisse chargée de surveiller l’évolution de la qualité des eaux du Léman, du Rhône et de leurs affluents), seulement 3 % de côtes sont encore sauvages. Hors 23 % de prés semi-naturels et de cultures, environ 60 % des berges et abords sont aménagés, enrochés, pavés, privatisés, ce qui limite probablement l'expression de l'écopotentialité du site.

Les théories sur l'origine du Léman peuvent être regroupées en deux grands courants : certains scientifiques privilégient l'origine tectonique comme cause principale, tandis que d'autres insistent sur l'origine érosive. Plusieurs aspects du Léman sont d'origine tectonique, on peut citer la séparation entre le petit lac et le grand lac (entre Yvoire et l'embouchure de la Promenthouse), la dépression du petit lac parallèle aux plis jurassiens.

Mais les recherches les plus récentes insistent sur le creusement de la cuvette du Léman par les avancées du glacier du Rhône lors de grandes phases successives de glaciation. En effet, le glacier du Rhône recouvrait la région de l'actuelle Vevey d'environ un kilomètre de glace et la région genevoise d'environ . Lors de la glaciation de Würm, le glacier du Rhône descendait depuis le Valais et se séparait en deux parties distinctes au contact du massif du Jura ; l'une partant vers le sud en direction de Genève puis Lyon, l'autre en direction du nord sur le plateau suisse en s'intégrant au Rhin. À la fin de cette glaciation, au recul des glaces, un affluent du Petit Lac creuse le seuil d'Yvoire et met en communication les deux bassins versants auparavant séparés; le Léman prend ainsi sa forme actuelle.

Le Léman reçoit les eaux de différentes rivières provenant de cantons suisses romands (Valais, Vaud, Fribourg et Genève) ainsi que des départements français (Haute-Savoie et Ain). Parmi ces nombreux affluents, le Rhône est celui dont le débit est plus important.

Durant l'hiver 2017, les eaux du Léman ont été brassées jusqu'à une profondeur de . Selon la CIPEL, ce brassage n'a pas été complet, le lac ayant une profondeur supérieure à 

Les eaux du Léman sont riches en substances dissoutes, notamment carbonates et sulfates de calcium et magnésium ; les matières en suspension sont décelables dans un milieu calme ; la quantité de ces matières arrivant au lac principalement par le Rhône se monte annuellement à de tonnes.

L'eau provenant des affluents crée un courant localisé à leurs embouchures dans le Léman. De plus, étant de températures différentes avec la température moyenne de l'eau du lac, les eaux de ces affluents (souvent d'origine nivale) s'étalent dans cette eau lacustre où elles finissent par trouver leur équilibre densimétrique. À l'instar des océans, des mers et des grands lacs, le Léman subit des marées infimes mais identifiables (de l'ordre de ).

Le lac reçoit de tonnes d'alluvions par an dont 6,1 de la part du Rhône, 1,1 de la part de la Dranse et 1,1 pour les autres affluents. À Genève, le Rhône n'évacue qu'une masse d'environ par an.

Les affluents du Léman sur la rive nord, ou rive droite (depuis l'embouchure du Rhône), sont essentiellement des cours d'eau vaudois

Les affluents du Léman sur la rive sud, ou rive gauche (depuis l'embouchure du Rhône), sont essentiellement des cours d'eau valaisans et haut-savoyards.

Il s'agit de deux types d'ondes stationnaires observées dans le lac. Les unes concernent la surface libre, interface entre l'air et l'eau, les autres impliquent la thermocline, interface entre les eaux superficielles (épilimnion) et les eaux profondes (hypolimnion).

Les premières études sur ce sujet ont été effectuées par le naturaliste, physiologiste et limnologue vaudois, originaire de Morges, François-Alphonse Forel.

Situé à la bordure nord-ouest des Alpes, le Léman, par la masse d'eau qu'il contient, crée autour de lui un microclimat, en particulier à Montreux et à ses abords immédiats, cette partie de la Riviera lémanique étant protégée de la bise.

En hiver, le lac restitue la chaleur accumulée durant l'été, ce qui adoucit les températures à son voisinage. En été, il rafraîchit tout son pourtour.

En hiver, en cas de conditions climatiques particulières, avec notamment de l'air sec froid et stagnant en haute et moyenne altitude, l'humidité qui s'élève des eaux du lac se transforme en épais stratus qui s'accumule sur une épaisseur de deux ou trois cents mètres, épaisse couche pouvant rester sur place durant deux à trois semaines. Cette mer de nuages déborde souvent du bassin lémanique et envahit les vallées adjacentes jusqu'à une altitude de .

Selon Météo suisse, le climat de la rive nord du Léman (secteur de Pully) présente, pour la période 1981-2001, des normes climatiques suivantes :

À cause du changement climatique, la température moyenne des eaux de surface du lac (à de profondeur) est passée de en 1970 à en 2016 ( en ).

Le Léman est balayé régulièrement par des vents généraux, des vents d'orages et des vents thermiques connus des météorologues locaux, mais aussi des navigateurs et des riverains.




Les brise thermiques nocturnes sont des vents pouvant atteindre . Descendant les vallées par lesquelles elles sont canalisées, elles apparaissent en fin d’après-midi. Un relief abrupt engendre un vent plus fort.

Le Léman a fait l'objet de nombreuses cartographies jusqu'en 1860. Les différentes éditions successives de ces cartographies ont été publiées par l'éditeur Slatkine à Genève.

Une mise à jour a été entreprise par l'Institut F.-A. Forel (Université de Genève) depuis 2003. Ainsi il est possible de trouver une nouvelle carte du Léman entre Genève et Gland (Petit-Lac) ainsi qu'entre Rivaz et l'embouchure du Rhône (Haut-Lac).

Le Léman est bordé par deux pays, la Suisse et la France.

Les villes les plus notables, situées au bord du Léman, depuis l'est, selon le sens horlogique, sont Genève, Nyon, Rolle, Morges, Lausanne, Cully, Vevey, Montreux (en Suisse), Évian-les-Bains et Thonon-les-Bains (en France).

Dans le detail, le lac est bordé par les communes suivantes :




Le lac comprend plusieurs îles : l'île de Peilz, le Château de Chillon, l'île de Salagnon, l'île de la Harpe, l'île de Choisi et l'île Rousseau, l'Île aux Oiseaux (Préverenges).

En Suisse :

Bien que les grandes villes soient épargnées par ce phénomène, de nombreuses parcelles au bord du lac, principalement dans les petites communes du district de Nyon, du district de Morges et les communes limitrophes de la ville de Genève, appartiennent à des propriétaires privés. Les habitants des communes de La Tour-de-Peilz et de Gland, ont néanmoins obtenu par votation populaire, respectivement en novembre 2010 et en février 2012, que leurs rives soient accessibles aux public sur tout le territoire de leur commune. Néanmoins, malgré ces deux votations, le délai pour que le public puisse se promener sur l'ensemble du territoire prendra plusieurs années en raison des oppositions des propriétaires sur les modalités d'expropriation de leurs parcelles.

L'accès aux rives du Léman est souvent sujet à conflits entre les propriétaires et les partisans d’un chemin au bord de l’eau,
en particulier l’association Rives publiques, fondée en 2003, concernant notamment la Loi fédérale sur l’aménagement du territoire (LAT), qui stipule à l'Art. 3, "de tenir libres les bords des lacs et des cours d’eau et de faciliter au public l’accès aux rives et le passage le long de celles-ci", ainsi que la Loi sur le marchepied le long des lacs et sur les plans riverains
(LML), qui date de 1926 et qui stipule à l'Art. 1, que : "Sur tous les fonds riverains du lac Léman, des lacs de Neuchâtel et de Morat, des lacs de Joux et Brenets, et du lac de Bret, il doit être laissé, le long de la rive et sur une largeur de , un espace libre de toute construction ou autre obstacle à la circulation, pour le halage des barques et bateaux, le passage ou marchepied des bateliers et de leurs aides, soit pour tous autres besoins de la navigation ainsi que pour ceux de la pêche." et que : "Lorsqu'il y a une grève le long du fonds riverain, la distance de sera prise sur le dit fonds, dès la limite de la grève."

En France :

C'est la Loi Littoral, par l'intermédiaire du Code général de la propriété des personnes publiques qui énonce les servitudes des berges de cours d'eau domaniaux. Celle-ci est entrée en vigueur le .

Selon un panneau installé par la mairie d'Anthy-sur-Léman, « "Les propriétaires riverains d'un cours d'eau ou d'un lac domanial ne peuvent planter d'arbres ni se clore par haies ou autrement qu'à une distance de . Leurs propriétés sont grevées sur chaque rive de cette dernière servitude de , dite servitude de marchepied" ».

Le terme même de "Léman", pour désigner ce grand lac alpin, apparaît dans la littérature vers 50 sous le nom, écrit en grec, "Lemánē límnē" ou "Lemános límnē" (). Ce nom est même utilisé et popularisé par Jules César qui, en 58, part de "Genava" et du "lacus lemanus" pour combattre les Helvètes. Le nom "Léman" renvoie à une racine indo-européenne, peut-être d'origine celtique, signifiant « lac ». La désignation « lac Léman » constitue donc un pléonasme, voire plutôt une tautologie, commis déjà par les géographes antiques et par César. Avec le développement de la cartographie, les noms se multiplient entre le et le : "lacu lausonio", "lacus losanetes" ou encore "lac de Lozanne".

Au , la renommée internationale de Genève convertie au calvinisme laisse apparaître la dénomination « Lac de Genève ». À cette époque, cette dénomination désigne le « Petit Lac » (), tandis que le « Grand Lac » () est baptisé « Lac de Lausanne ». Au fil du temps, ce dernier terme disparaît au profit de celui de "lac Léman", voire "Léman", et adopté par les Savoyards, les Vaudois et les Valaisans. Cette appellation est généralisée dans les cartes géographiques et complète plusieurs noms de lieux situés le long de la rive savoyarde du lac (Maxilly-sur-Léman, Chens-sur-Léman, etc.). Le nom du Léman, particulièrement à la mode durant le siècle des Lumières, la Révolution française et le Premier Empire, est utilisé par des auteurs comme Jean-Jacques Rousseau, qui utilise également la dénomination "lac de Genève", ou Voltaire et sert de prête-nom au département du Léman qui regroupe alors le nord de la Savoie, le pays de Gex et Genève.

François-Alphonse Forel, médecin et scientifique suisse de la fin du , dit que « l'usage tend à s'établir en géographie, et cela avec raison, de préférer, partout où il en existe, le nom personnel d'un lac au nom de la ville située sur ses bords. Un lac est un individu géographique en lui-même et par lui-même. »

Ainsi, de nos jours, dans les dictionnaires francophones, seule la partie du lac proche de Genève est appelée "Lac de Genève". Néanmoins, cette désignation est souvent utilisée en dehors de la Romandie et du bassin lémanique pour désigner le Léman dans son intégralité, étant donné la renommée internationale de Genève et le fait qu'il s'agit du nom officiel dans la plupart des langues non-romanes ("Genfer See" en allemand (en un mot - "Genfersee" - en suisse allemand), "Lake Geneva" en anglais, etc.). En italien, on dit "Lago di Ginevra" et parfois "Lago Lemano".

En raison des différentes origines (sédimentation, pliage tectonique, érosion glaciaire) les termes de "Haut Lac" (l’extrémité Est du lac soit la partie entre Villeneuve et l'axe Meillerie–Rivaz), "Grand Lac" (le plus grand et le plus profond bassin) et "Petit Lac" (l’extrémité sud-ouest du lac soit la partie entre l'axe Yvoire–Promenthoux chez Prangins et Genève) sont encore utilisés, surtout dans la région proche du Léman. Selon l'Office fédéral de topographie, "Lac de Genève" désigne la partie du "Petit Lac" qui se trouve à l'intérieur des frontières cantonales de Genève (à l'exception de l'exclave lacustre jouxtant la commune de Céligny), donc environ entre l'axe Versoix–Hermance et la sortie du Rhône à Genève.

La qualité de l'eau s'est globalement améliorée depuis les années 1970. Cependant, le , les préfets de Savoie et de Haute-Savoie ont dû interdire la pêche pour consommation et commercialisation de l'omble chevalier ("Salvelinus alpinus") dans le Léman en raison de taux très élevés de polychlorobiphényles (PCB) et de dioxines « supérieurs aux normes réglementaires » pour deux échantillons de ces poissons, , en attendant qu'une enquête de l’Agence française de sécurité sanitaire de aliments (Afssa) précise l'ampleur du problème (la pêche sans consommation du poisson reste autorisée, ainsi que la baignade et les sports nautiques, les PCB étant faiblement solubles dans l’eau).

Deux zones sur le Léman sont des sites Ramsar. Le plus important, le site dénommé "Les Grangettes" occupe sur l'est du Haut-Lac, la réserve naturelle des Grangettes, l'embouchure du Rhône et Le Bouveret. La limite ouest étant une ligne entre le port et le camping de "La Pichette" (commune de Chardonne) sur la rive nord et Saint-Gingolph sur la rive sud. Le site "Rives du Lac Léman", reconnu le , occupe lui une surface de sur la rive française entre les embouchures de la Dranse et du Vion.

De nos jours (2018), une trentaine d'espèces de poissons et de crustacés cohabitent dans le Léman ainsi que, depuis quelques années, une espèce de cnidaire, dont la liste (non exhaustive) suit :




On y trouve des oiseaux sédentaires et nicheurs comme le cygne tuberculé, le canard colvert, la mouette rieuse, la foulque macroule, le fuligule morillon, le harle bièvre, le grèbe huppé, le grand cormoran, le milan noir, le goéland leucophée, le goéland cendré, la nette rousse, le héron cendré, le grèbe à cou noir, le grèbe castagneux ou la gallinule poule-d'eau.

Se situant sur un courant migratoire entre les Alpes et le Jura, le lac est une zone de prédilection pour de nombreux oiseaux. En provenant du nord-est de l'Europe, de Scandinavie ou même de Sibérie, volatiles viennent y prendre leur quartier d'hiver, dont le fuligule morillon, le grèbe huppé, le fuligule milouin, le grand cormoran, le harle bièvre, la foulque macroule, le garrot à œil d’or, le grèbe à cou noir, le grèbe castagneux, le goéland leucophée et occasionnellement le goéland brun, le harle huppé, l'eider à duvet, le canard souchet, la macreuse brune, le fuligule milouinan.

Il y a également des espèces estivantes comme la mouette rieuse, le goéland leucophée et la nette rousse ou le martinet noir et le milan noir sur le rivage et dans les villes.

La pollution était préoccupante dans les années 1980, mais la situation s'est stabilisée avec une diminution des algues et un meilleur apport en oxygène. Toutefois, des déchets chimiques comme les phosphates et les engrais continuent à se déverser dans le lac.

La Commission internationale pour la protection des eaux du Léman (CIPEL) est une commission transfrontalière travaillant depuis 1963 à l'amélioration de la qualité des eaux du Léman, sur la base d'une convention entre la France et la Suisse. L'actuel plan d'action 2011-2020 de la CIPEL, orientant les travaux de la commission, vise à préserver les milieux aquatiques et à garantir certains usages du lac (alimentation en eau potable de la population, pratique d'activités nautiques de loisirs, peuplement piscicole de qualité, etc.).

Le secrétariat permanent de cette organisation est basé à Nyon, quartier de Changins, situé dans le canton de Vaud. Ce service gère l'administration, les services financiers, technique et scientifique ainsi que la coordination des travaux de la commission

Les études en paléo-environnement, faites à partir des restes de végétaux, par la station d'hydrologie lacustre de l'Institut national de la recherche agronomique (INRA), basée à Thonon-les-Bains, avaient révélé que le bassin lémanique a connu de fortes variations climatiques et biologiques depuis un demi-siècle. De nombreuses espèces végétales ont disparu, car les concentrations excessives de phosphore, d'herbicides, de pesticides et de métaux lourds — on trouve encore, au fond des lacs alpins, les traces de la métallurgie au plomb de l'époque romaine — issues des activités urbaines et agricoles — un mètre carré de berge pollué pollue lui-même d'eau — ont permis à l'excès la production des algues qui ont surconsommé l'oxygène contenu dans l'eau : c'est l'eutrophisation.

Cette densification de la matière solide en suspension fragilise le phytoplancton, car il ne reçoit plus assez de lumière — la baisse de la masse de phytoplancton a entraîné, à son tour, la disparition d'espèces de poissons comme l'épinoche, disparue en 1922 mais que l'on trouvait encore en petit nombre jusque dans les années 1970, et la méduse d'eau douce "Craspedacusta sowerbyi" disparue en 1962, mais revue depuis. De plus, la disparition du phytoplancton fournit un terrain propice aux cyanobactéries ou micro-algues ("Planktothrix rubescens"), qui rendent l'eau, par création de toxines hépatiques, nocive à la consommation des poissons, et même à la baignade.

L'observation, le suivi des cycles saisonniers et annuels des écosystèmes limniques, l'étude des incidences du climat et des pollutions (herbicides, pesticides, métaux lourds), le suivi des concentrations de protozoaires ciliés, des rotifères et autres espèces zooplanctoniques herbivores qui filtrent l'eau, la connaissance des espèces nouvelles apparues, a permis au fil des années d'établir des plans de sauvegarde et de prévention, qui passent d'abord par l'amélioration de l'alimentation en eau potable du bassin et donc du lac lui-même.

La raréfaction des brassages complets du lac, qui nécessitent des hivers très froids lors desquels l'eau de surface apporte son oxygène en profondeur, le réchauffement climatique, qui modifie les dates des périodes de frai des poissons, vont être à l'origine de nouvelles adaptations de l'écosystème du lac.

Le Léman s'est installé sur le site d'un ancien fossé d'affaissement de type molassique très profond. Celui-ci a ensuite été façonné par les phases de glaciation qui se sont succédé durant le Quaternaire. Ce n'est qu'entre 15000 et 14000 que le lac est progressivement libéré des glaces. Le niveau de l'eau est alors de plus élevé qu'actuellement et la végétation aux alentours encore pauvre : quelques genévriers et quelques bouleaux nains. La faune est alors composée de rennes, de chevaux et de mammouths. La végétation ne se développe vraiment que lorsque commence la période climatique du Bølling en 12000 ; le niveau du Léman descend alors jusqu'à une hauteur supérieure de au niveau actuel. Celui-ci continue dés lors de baisser régulièrement, atteignant un premier minimum en 4000 , à un niveau inférieur de quelques mètres au niveau actuel, puis le niveau remontera avant la fin de la Préhistoire.

Les premiers humains repérés dans le bassin lémanique ont été des chasseurs de rennes du magdalénien, à la fin du paléolithique supérieur ; des traces de campement ont été retrouvées à proximité de Veyrier au sud de Genève, dans les carrières du Salève. Mais les vestiges ultérieurs, ceux des villages palafittiques établis le long des rives, livrent surtout une extraordinaire documentation sur les conditions de vie à l’époque préhistorique. Ces villages dits «lacustres» ont été classés au patrimoine mondial de l'UNESCO

Aujourd’hui, le Léman est artificiellement maintenu entre les cotes , ce qui efface les variations saisonnières que l’on a connues jusqu’à la fin du . Toutefois, l’archéologie montre qu’il y a eu, sur le très long terme, des changements de niveaux bien plus importants : en raison de modifications majeures du climat, le lac a évolué dans une tranche d’altitude d’environ neuf mètres.

Au cours des millénaires, les humains se sont adaptés à ces conditions variables, s’établissant au voisinage immédiat du rivage lorsque de périodes relativement sèches créaient des conditions de basses eaux, et se retirant plus haut en période de transgression du lac. Les couches archéologiques n’ont été conservées que lorsqu’elles étaient protégées par l’eau, ce qui explique que seules les stations palafittiques sont bien documentées. Les habitats situés plus haut, en terrain sec, ont pour la plupart disparu sous l’effet de l’érosion.

La phase d’occupation la plus ancienne connue est celle du Néolithique moyen, lorsque des groupes d'agriculteurs-éleveurs s'installent sur la terrasse littorale nouvellement libérée par la baisse des eaux, alors que le lac est à un niveau fort bas (vers ). La station de Corsier–Corsier-Port a laissé des vestiges daté de 3856 av. J.-C.

Après une brève remontée des eaux dans l’ensemble de la Suisse (vers 3300-3400 av. J.-C.) les stations littorales connaissent à nouveau un grand développement et cet essor s’étend sur près de huit siècles (3250-2450 av. J.-C.). Sur le Léman, à un niveau de , la station de Vers-l’Église, à Morges, est la seule qui ait conservé un témoignage de cette époque, avec un matériel céramique que l’on peut dater entre 2950 et 2700 av. J.-C..

Au néolithique final et Bronze ancien, entre 2400 et 1800 av. J.-C., on ne trouve plus guère de stations palafittiques, ni sur le Léman, ni ailleurs sur le Plateau suisse. Il s’agit assurément d’une période de forte hausse des niveaux d’eau, une condition qui a obligé les populations à se retirer des rives. Mais, peu après, intervient une sérieuse baisse : la station de Cologny-La Belotte a livré des pilotis datés par dendrochronologie de 1805 à 1778 av. J.-C., accompagnés de matériel archéologique du Bronze ancien. Le lac se situe alors à une altitude de , donc environ deux mètres au-dessous du niveau actuel. La station de Morges–Les Roseaux possède une couche archéologique similaire, à un niveau de et, tout près de là, le site de Préverenges a donné de nombreux pilotis contemporains, qui attestent deux étapes d’occupation, avec une césure brutale de correspondant assurément à une brusque montée des eaux.

À l’âge du Bronze moyen intervient la dégradation climatique du Löbben, entre 1600 et 1500 av. J.-C., dégradation qui entraîne un haut niveau des eaux. Cet état pourrait correspondre à la fameuse terrasse lémanique de , la surface du lac se trouvant à . Ce phénomène a été observé en divers endroits, notamment à Vidy près de Lausanne.

Entre 1085 et 850 av. J.-C., on constate une nouvelle occupation générale des rives lacustres. À cette époque, le Léman connaît un niveau particulièrement bas, vers . Les fouilles de Plonjon ont montré que cet état est continu entre 1067 et 850 av. J.-C., avec tout de même des variations. Le niveau le plus bas a été mesuré sur le site d'Anières-Bassy, à une altitude de , soit au-dessous du niveau actuel. Cette station n’a pas encore livré de datation par dendochronologie, mais le matériel céramique trouvé en surface permet de la dater du Bronze final.

La date de 850 avant notre ère marque la fin des occupations palafittiques, non seulement au bord du Léman, mais sur l’ensemble des lacs du Plateau suisse. La dégradation du climat subatlantique entraîne en effet une importante montée des eaux de tous les lacs du nord des Alpes et marque la fin de l’émersion des terrasses littorales, favorables à l’établissement de constructions sur pilotis.

En 563, durant le haut Moyen Âge l'éboulement du mont Taurus provoque un raz-de-marée sur le Léman . La vague aurait alors atteint à Évian et à Genève, à Lausanne, détruisant de nombreux villages.

Suite à sa libération de la tutelle bernoise et les remous due à la création de la République française en 1792, des patriotes vaudois, le plus connu étant Frédéric-César de La Harpe, avocat dans le pays de Vaud, font la proclamation de la création d'une république sœur de la France, la République lémanique. L'avocat vaudois rédigea en 1797, avec le concours de Vincent Perdonnet, des « Instructions pour l'Assemblée représentative de la République lémanique ». Par la suite, la République helvétique incorpora le Pays de Vaud sous l'appellation canton du Léman, sans qu'il ait eu le temps de proclamer la naissance d'un état indépendant.

De mémoire humaine, le Léman a toujours été sujet à de fortes variations saisonnières. La période estivale était en général marquée par des hautes eaux en raison de la fonte des glaces dans le massif alpin, tandis qu’à l’inverse l’hiver se caractérisait par une situation de basses eaux. Au fil du temps, cependant, les variations ont semblé s’accentuer et à partir du les habitants des rives vaudoises et valaisannes ont commencé à se plaindre de plus en plus souvent d’inondations et de dégâts causés à leurs propriétés riveraines. Ils accusaient les habitants de Genève d’obstruer l’exutoire du Rhône par les nombreuses claies de pêche, palissades, estacades, retenues pour les biefs des moulins et autres établissements industriels, tout particulièrement le barrage de la machine hydraulique assurant l’alimentation en eau des fontaines de la ville, et ainsi d’empêcher un écoulement normal du fleuve.

Il en a résulté un conflit intercantonal de près de deux siècles, les Genevois assurant non seulement qu’ils ouvraient entièrement le barrage de la machine hydraulique en période de hautes eaux, mais aussi qu’ils n’observaient eux-mêmes aucune hausse de niveau, relevés scientifiques à l’appui. L’affaire s’envenima au point de culminer en un retentissant procès de sept ans devant le Tribunal fédéral (1877-1884). La difficulté était en effet d’ordre scientifique et méthodologique. Comment mesurer le niveau réel d’une masse d’eau aussi considérable et en constant mouvement ? Vers la fin du , les écarts saisonniers pouvaient atteindre plus de .

Il y avait donc d’une part les partisans d’une approche pragmatique, en général des non-scientifiques, qui recensaient les signes manifestes d’exhaussement du lac, et d’autre part des ingénieurs cantonaux comme Adrien Pichard, Guillaume-Henri Dufour ou Ignace Venetz, qui préféraient se fier à de savantes mensurations dont les résultats attestaient une relative stabilité du niveau ! Il a fallu attendre le troisième quart du , avec les travaux d’ François-Alphonse Forel, pour mieux comprendre toute la complexité de l’équilibre hydraulique du Léman. Celui-ci est soumis à l’influence du vent, de la pression atmosphérique (déterminant les fameuses seiches lémaniques, de la pluviométrie, de mini-marées (qui ne sont pas négligeables sur une si grosse masse d’eau), mais aussi d’apports d’eau accrus en raison, dès le , de déboisements dans les Alpes valaisannes et d’assèchements de marais dans la vallée du Rhône qui précédemment agissaient comme des régulateurs, ou encore d’un accroissement de la pluviométrie et de la fonte des glaces en raison du réchauffement climatique. Plutôt que de tenter de mesurer les variations du niveau réel du Léman, chose presque impossible, Forel, par une image dont l’évidence s’impose, a montré l’influence incontestable des obstacles qui ont progressivement encombré le Rhône : « Quand le robinet d'un tonneau est fermé partiellement, il s'écoule moins de vin que quand il est tout ouvert ».

En définitive donc, cette meilleure compréhension a permis la signature en 1884 d’une convention intercantonale réglant la régularisation du Léman. Une série de vannes horizontales maintiendront désormais le niveau du lac entre les limites de au-dessus du niveau de la mer.

Actuellement et depuis 1995, ce rôle de régulation a été repris par le barrage du Seujet construit plus en aval et qui sert aussi à produire d'électricité par an en se servant du Léman comme réservoir.

En 1258, les comptes de la châtellenie de Chillon mentionnent l’existence d’une galère appartenant au comte de Savoie. Du jusqu’en 1720, des galères naviguent sur le Léman pour le compte de la Savoie, de Genève et de Berne. En référence à cette période, une réplique de galère a été construite à Morges et navigue actuellement sur le Léman.

Dès le , les archives de Savoie mentionnent la présence de barques sur le Léman. Largement inspirées par l’architecture des galères qui les ont précédées sur le lac, les barques du Léman sont de grands bateaux à voiles latines. Leur principale période d’activité est le et le début du où elles transportent souvent des chargements de pierres depuis les carrières de Meillerie vers Genève. Actuellement cinq barques du Léman naviguent sur le lac : deux barques préservées et trois répliques.

À partir du 18 juin 1823, pour la première fois en Suisse, un bateau à vapeur fait son apparition : le Guillaume-Tell. Construit par une entreprise bordelaise avec un moteur venant d'Angleterre, sa mise en service est l’œuvre du consul américain Edward Church, qui y voyait un moyen de promouvoir cette invention de son compatriote Fulton. Avec ses pour et une vitesse de , il permet de relier Genève à Lausanne en , à comparer avec la journée de voyage nécessaire pour faire le trajet en diligence.

Cette innovation rencontre un grand succès et voit apparaître des concurrents. En 1824, les Genevois mettent en service le Winkelried tandis que les Lausannois lancent le Léman (, ) en 1826. Le Guillaume-Tell quitte le service en 1836 pour laisser la place à des bateaux plus grands et plus rapides avec une coque en fer et non en bois. C'est la grande époque des bateaux à vapeur qui ne commence à s'achever qu'avec l'arrivée progressive des trains à partir de 1855. Ce nouveau concurrent pousse les sociétés de navigation à s'entendre, ce qui aboutit finalement à la fondation de la compagnie générale de navigation (CGN) en 1873.

La CGN continue encore aujourd'hui d'exploiter à la belle saison une flotte de bateaux à vapeurs datant du début du siècle dernier, ainsi qu'en toutes saisons, une flotte moderne remplissant une mission de service public en assurant de multiples liaisons quotidiennes entre les villes suisses de Lausanne et Nyon et les villes françaises d'Évian, Thonon, Yvoire et Chens.

En 1885, la Société internationale de sauvetage du Léman (SISL) est créée. Cette association a pour but de secourir les navigateurs en difficulté. Actuellement, elle compte , réparties autour du Lac, qui disposent toutes de moyens de sauvetage modernes.



À peu près sont amarrées au bord du lac, pour la plaisance, les déplacements et la pêche.

Un service de bateaux à aubes (dont la flotte est appelée "Belle Époque"), dessert depuis le les principales localités entourant le lac. Sa gestion est confiée à la Compagnie générale de navigation (CGN).

La mission de la CGN est l'exploitation commerciale des moyens de transport sur le Léman, l'entretien, la conservation et l'exploitation des bateaux. En plus de sa flotte, la CGN possède un chantier naval à Lausanne, à proximité du port.

La flotte comprend et à roues à aubes, et à roues à aubes, « modernes » sans roues à aubes, et 
On peut également naviguer à l'ancienne avec des barques traditionnelles (dites aussi "barques de Meillerie" du nom d'une carrière et de son port), mais les missions de ces navires historiques sont plus liées à l'activité touristique et au devoir de mémoire locale qu'au transport de biens ou de personnes. Aujourd'hui, cinq barques sont en circulation et se destinent à la plaisance, dont "La Neptune" (construite en 1904, restaurée en 2004), "la Vaudoise" (ex "la Violette", construite en 1932), "la Savoie" (réplique d'un navire en 1896, construite en 2000), l'Aurore (copie d'une cochère gingolaise et construite, elle aussi, en 2000) et "La Demoiselle" (réplique d'un bateau de 1828 et portant le même nom).
L'installation d'une société de bateaux volants électriques assurant un service de taxi dénommés "Sea Bubble" est prévue à Genève entre les deux rives du lac.

À compter d'avril 2018, une ligne pilote sera testée sur une durée de trois à neuf mois. Ce projet a reçu le département des transports de l’Etat de Genève.

Quatre sous-marins ont plongé dans le Léman : le mésoscaphe Auguste Piccard (lors de l'exposition nationale suisse de 1964), le F.-A. Forel (mis à l'eau en 1979) et les sous-marins russes Mir 1 et Mir 2 en 2011

Aucune période de fermeture annuelle de la pêche n'est imposée pour le Léman, il existe cependant des périodes de protection selon les différentes espèces de poisson. L'application d'un concordat entre la France et la Suisse permet que ces limitations soient les mêmes dans ces deux pays. Un permis de pêche est obligatoire pour pécher sur le plan d'eau. La pêche autorisée sans permis peut être autorisée mais à condition de pécher au bouchon fixe limitée à une ligne par personne.
En 2009, il y avait professionnels (151 en 2005, 146 depuis 2006), avec une moyenne d'âge de (2006), et de loisirs ( en 2005, en 2006) sur le Léman. Le total des prises se monte à ( en 2006, en 2008) (soit de poissons pêchés par hectare ce qui correspond à environ de poissons pour de litres d'eau).

En 2007, au total d'ombles chevaliers (de ) et (de ) ainsi que des féras ont été lâchés autant du côté français que suisse, mais, selon l'INRA, « la reproduction naturelle a repris le dessus, grâce à la meilleure santé du lac qui offre une qualité de planctons accrue ». Cependant, « le brochet, grand prédateur du lac, fait des ravages » en particulier chez l'omble chevalier et la truite. Du coup, les captures de cette espèce sont passés en quelque temps de avec un but de par an, mais cela ne semble pas suffire.

En raison de l'abondance des brochets, prédateurs de l'omble chevalier, sa période de protection a été supprimée en 2007 ( au 10 mai).

La Société Internationale de Sauvetage du Léman (SISL) est une organisation franco-suisse à but non lucratif ayant pour but le sauvetage sur le lac. Formée de bénévoles, elle est active depuis 1885. Elle est organisée en , qui ont chacune la responsabilité d'un poste de sauvetage.

En avril 2017, la SISL dispose de d'intervention (embarcations de sauvetage sans cabine) et (embarcations de sauvetage avec cabine).

Le flotte historique et traditionnelle des canots à rames représente , pour la plupart entièrement en bois. Ces canots sont utilisés pour les entraînements et les courses à la rame.
Le lac est équipé d'un réseau de phares alertant les usagers du lac souvent deux heures à l'avance de l'imminence d'un probable coup de vent ou orage.
Au nombre de 22, ils sont disposés tout autour du lac regroupés dans trois zones d'alerte : le Haut Lac (Vevey, La Tour-de-Peiz, Montreux, Villeneuve, Le Bouveret, et Meillerie), le Grand Lac (Rolle, St-Prex, Morges, St-Sulpice, Vidy, Ouchy, Pully, Lutry, Cully, Evian, Thonon et Excenevex) et le Petit Lac (Genève, Versoix, Nyon et Nernier).

Selon l'article 40 "Signaux d’avis de tempête" de l"'ordonnance sur la navigation dans les eaux suisses"
"« L’avis de fort vent (feu orange scintillant à environ 40 apparitions de lumière par minute) attire l’attention sur le danger de l’arrivée de vents dont les rafales peuvent atteindre 25 à 33 nœuds (env. 46 à ), sans indication précise de l’heure. Il est émis aussi tôt que possible. »"

"« L’avis de tempête (feu orange scintillant à environ 90 apparitions de lumière par minute) attire l’attention sur le danger de l’arrivée de vents dont les rafales peuvent dépasser 33 nœuds (env. ), sans indication précise de l’heure. »"

Le lac est lié à l'organisation de nombreuses compétition sportives, notamment dans le domaine nautique, mais aussi des épreuves d'autres sports pratiqués sur ses berges. La motomarine est interdite sur l'ensemble du Léman. Le Comité international olympique possède son siège sur les berges du lac.

Chaque année, au mois de juin a lieu la plus importante compétition de voile au monde sur plan d'eau fermé (lac), le Bol d'or. Près de 600 bateaux y prennent part en moyenne. Le but est de réaliser le plus vite possible l'aller-retour entre Genève (extrémité ouest du lac) et Le Bouveret (extrémité est du lac). À la mi-septembre, une régate similaire est organisée mais en solitaire appelée la Translémanique en Solitaire. En juin, une autre régate importante a lieu : les Cinq jours du Léman. C'est la plus longue course d'endurance en bassin fermé d'Europe. Au cours de l'année, de nombreuses autres régates se disputent sur le lac, les séries multicoques étant très bien représentées.

Depuis 1972, le tour du lac Léman à l'aviron à la rame est organisé chaque année par la Société nautique de Genève. Il s'agit de la plus longue course d'aviron au monde puisque ce ne sont pas moins de qui sont parcourus le long des côtes en une seule étape.

Le troisième samedi d'octobre voit se dérouler le marathon de Lausanne, qui longe le bord du Léman jusqu'à La Tour-de-Peilz. La 'Ronde de la presqu'île du Léman' est une course à pied, organisée par la ville de Messery (Haute-Savoie) depuis 30 ans. Se présentant en plusieurs sections de différentes distances, elle est enregistrée par la Fédération française d'athlétisme dеs courses dites « hors stаdе ». Elle est également ouvertes aux marcheurs depuis 2010.



Bien qu'il paraisse relativement difficile de déterminer les sites à retenir pour donner une approximation de la fréquentation touristique du Léman, l'on sait toutefois d'une étude de 2001 de l'Observatoire national du tourisme (ONT) que la fréquentation touristique liée au lac pour la Suisse était de en 1999. En ce qui concerne la France, il était indiqué pour l’année 2001 une fréquentation dans les pays du Léman représentant 16 % des nuitées du département de la Haute-Savoie. Le nombre de nuités en hôtels et campings cette année-là ayant été d'environ , cela représente donc quelque .

Fondé en 1954 par maître Edgar Pellichet, le Musée du Léman se situe à Nyon, en Suisse dans le canton de Vaud, face au port de plaisance.

Le Musée du Léman présente tout ce qui est trait au lac Léman. Des aquariums géants présentent la faune piscicole du lac. Ce musée couvre une surface de d’expositions permanentes et temporaires auxquelles s’ajoutent des locaux administratifs et techniques.

Situé dans le petit quartier portuaire de Thonon-les-bains, dénommé Rives, a été créé en 1987. sa vocation est d'être un lieu de mémoire à l'égard des professionnels de la pêche au Léman.

Des Barques, mais aussi des moteurs, nasses et filets, outils anciens ou actuels, sont exposés, offrant ainsi une image la plus exhaustive possible de l'organisation de la pêche en pays lémanique. Un diaporama vous présente leur vie et leur activité au fil de l'année, en toute saison.

Outre les villes du bassin lémanique dont Genève (Jet d'eau) et Lausanne (Château Saint-Maire), on trouve sur la Riviera vaudoise le château de Chillon dans son cadre romantique unique popularisé par "La Nouvelle Héloïse" de Jean-Jacques Rousseau et "Le Prisonnier de Chillon" de Lord Byron. D'autres sites moins connus parsèment le bord du lac :



Comme toute étendue d'eau de grande superficie, c'est tout d'abord cet élément qui va occuper la première place dans la mythologie lémanique et dans ses mystères.






Montreux est la principale ville liée à la culture musicale contemporaine située au bord du lac. Un festival de jazz y a été créé en 1967, mais également, des concerts y sont organisés dans le cadre du Super Pop de Montreux, label organisé par le fondateur et directeur du Festival de jazz de Montreux, Claude Nobs (1969 - 1974).

De 1969 à 1974, de nombreux musiciens de genre musical défileront à Montreux, devenue une capitale du rock locale dont notamment Pink Floyd, Led Zeppelin, Santana, Canned Heat, Yes, Chicago, Deep Purple et Frank Zappa.

Deux chansons universellement connues ont rendus célèbre le lac, la première est liée aux manifestations musicales de Montreux et la seconde du fait de l'existence d'un studio d'enregistrement, le Mountain Studios dont le groupe de rock britannique Queen était le propriétaire de 1979 jusqu'en 1993.
Parue en 1972 dans l'album "Machine Head" et largement connue pour son riff de guitare "", interprétée par Deep Purple, évoque l'incendie du Casino de Montreux en 1971. Cette chanson dont le titre signifie en anglais : « "La fumée sur l'eau" » évoque la fumée et les flammes de cet incendie au-dessus du lac. le nom du lac est cité dans la chanson dans la deuxième strophe sous son nom anglais. We all came out to Montreux<br>
On the Lake Geneva shoreline<br>
To make records with a mobile...//... 
chanson du groupe Queen, écrite par Freddie Mercury, La chanson "A Winter's Tale", interprétée par Freddy Mercury est sortie en décembre 1995 et est le second extrait de l'album "Made in Heaven" évoque le Léman, la région de Montreux, ses oiseaux et sa beauté.It's winter-fall<br>
Red skies are gleaming - oh -<br>
Sea gulls are flying over<br>
Swans are floatin' by<br>
Smoking chimney-tops<br>
Am I dreaming ...//... 








Le Léman et son environnement immédiat, ont inspiré des grands auteurs et dessinateurs de bande dessinée de renommée mondiale.







Un numéro du magazine français, "L'Alpe" publié par l'éditeur grenoblois Glénat, paru au printemps 2016, est entièrement consacré au lac sous le titre : « "Lac Léman, petite mer des Alpes" ». Cette revue explique dans cette édition que si le Léman fut souvent évoqué au cinéma, son absence dans les images filmées s'explique généralement par le coût du tournage en Suisse ou et une .

Néanmoins, le Léman est directement une source d'inspiration pour de nombreux films français, suisses et d'autres pays :









Ces personnalités ont contribué par leurs actions scientifiques, sportives, industrielles ou culturelles à la notoriété du Léman et de son bassin.








Lien vers les cartes papier et interactive de la servitude de marchepied : Où marcher sur le littoral du léman des associations française « "le lac pour tous" » et Suisse « "rives publiques" »







"L'Alpe", éditions Glénat et Musée dauphinois (avec le concours du musée du Léman). Ouvrage collectif.



</doc>
<doc id="19411" url="https://fr.wikipedia.org/wiki?curid=19411" title="Noyau">
Noyau

De manière générale, un noyau est la partie centrale située au milieu d'un autre objet.



















</doc>
<doc id="19414" url="https://fr.wikipedia.org/wiki?curid=19414" title="Métal alcalino-terreux">
Métal alcalino-terreux

Les métaux alcalino-terreux ou alcalinoterreux sont les six éléments chimiques du du tableau périodique : béryllium Be, magnésium Mg, calcium Ca, strontium Sr, baryum Ba et radium Ra. Leurs propriétés sont très semblables : ils sont blanc argenté, brillants, et chimiquement assez réactifs à température et pression ambiantes. Leur configuration électronique contient une sous-couche s saturée avec deux électrons, qu'ils perdent facilement pour former un cation divalent (état d'oxydation +2).

Leur nom provient du terme « métaux de terre » utilisé en alchimie et décrivant les métaux qui résistent au feu, les oxydes de métaux alcalino-terreux demeurant solides à température élevée.

Les métaux alcalino-terreux sont caractérisés par un éclat argenté, une masse volumique peu élevée, une température de fusion à peine supérieure à celles des métaux pauvres (et une température d'ébullition inférieure à certains d'entre eux), une grande malléabilité, ainsi qu'une certaine réactivité avec les halogènes, conduisant à des sels ioniques — à l'exception du chlorure de béryllium , qui est covalent — ainsi qu'avec l'eau (hormis le béryllium), moins facilement cependant qu'avec les métaux alcalins, pour former des hydroxydes fortement basiques. La réactivité de ces éléments croît avec leur numéro atomique.

Le béryllium et le magnésium sont plutôt gris car ils se recouvrent d'une pellicule d'oxyde BeO et MgO protectrice passivante, tandis que le calcium, le strontium, le baryum et le radium sont plus brillants et plus mous. La surface de ces métaux se ternit rapidement à l'air libre.

Par exemple, alors que le sodium et le potassium réagissent avec l'eau à température ambiante, le calcium ne réagit qu'avec l'eau chaude, et le magnésium seulement avec la vapeur d'eau :

Le béryllium fait exception à ces comportements : il ne réagit pas avec l'eau liquide ni avec la vapeur d'eau, et ses halogénures sont covalents. Ainsi, le fluorure de béryllium , "a priori" le plus ionique des halogénures de béryllium, est essentiellement covalent, avec une température de fusion d'à peine et une faible conductivité électrique à l'état liquide.

Les ions M issus des alcalino-terreux Ca, Sr et Ba peuvent être caractérisés de manière qualitative par un test de flamme : Lorsqu'on traite un sel d'un alcalino-terreux avec de l'acide chlorhydrique concentré (ce qui donne un chlorure métallique volatil), et qu'on le chauffe fortement dans la flamme non éclairante d'un bec Bunsen, on observe une couleur de flamme caractéristique. Cette flamme est rouge orangée pour Ca (mais vert pâle à travers du verre bleu), pourpre pour Sr (mais violette à travers du verre bleu), et vert pomme pour Ba.

Les éléments de cette série possèdent deux électrons dans leur couche de valence, et leur configuration électronique la plus stable s'obtient par la perte de ces deux électrons pour former un cation doublement chargé.

Le béryllium est utilisé essentiellement dans des applications militaires<ref name="10.1002/14356007.a04_011.pub2">
</ref>. Il est également utilisé comme dopant de type "p" pour certains semiconducteurs, tandis que l'oxyde de béryllium BeO est utilisé comme isolant électrique et conducteur thermique résistant. En raison de sa légèreté et de ses propriétés générales, le béryllium est utilisé dans les applications pour lesquelles rigidité, légèreté et stabilité tridimensionnelle sont requises dans un intervalle de températures étendues.

Le magnésium a été largement utilisé dans l'industrie avec un rôle structurel dans la mesure où ses propriétés dans ce domaine sont meilleures que celles de l'aluminium ; son utilisation a cependant été réduite en raison des risques d'inflammation qu'il présente. Il est souvent allié à l'aluminium ou au zinc pour former des matériaux aux propriétés intéressantes. Le magnésium intervient également dans la production d'autres métaux, comme le fer, l'acier et le titane.

Le calcium intervient comme réducteur dans la séparation d'autres métaux de leurs minerais, comme l'uranium. Il est également allié à d'autres métaux, comme l'aluminium et le cuivre, et peut être utilisé pour la désoxydation de certains alliages. Il est utilisé par ailleurs dans la production de mortier et de ciment.

Le strontium et le baryum ont moins d'applications que les métaux alcalino-terreux plus légers. Le carbonate de strontium est utilisé pour produire des feux d'artifice rouges, tandis que le strontium pur est utilisé pour l'étude de la libération des neurotransmetteurs par les neurones<ref name="10.1038/2121233a0">
</ref>. Le baryum est utilisé pour faire le vide dans les tubes électroniques, tandis que le sulfate de baryum est utilisé dans l'industrie pétrolière, ainsi que dans d'autres types d'applications<ref name="10.1002/14356007.a03_325.pub2">
</ref>.

Le radium a été utilisé jadis dans de nombreuses applications, mais a été remplacé depuis par d'autres matériaux en raison de sa radioactivité, qui le rend dangereux. Il a ainsi été utilisé pour produire des , et fut même ajouté dans les années 1930 à l'eau de table, à des dentifrices et à des cosmétiques en vertu des propriétés rajeunissantes et bienfaisante alors prêtées à la radioactivité. De nos jours, il n'a plus aucun usage, pas même en radiologie, où des sources radioactives plus puissantes et plus sûres sont utilisées à sa place.

Les métaux alcalino-terreux ont un rôle biochimique très variable, certains étant indispensables, d'autres hautement toxiques, ou encore indifférents :



L'isotope Sr est un produit de fission de l'uranium. Lors d'un accident nucléaire (fuite de déchets, explosion nucléaire), il risque de contaminer la nature pour finir par s'incorporer dans les os avec le phosphate de calcium.


Les noms de ces éléments proviennent de leurs oxydes, les "terres alcalines". Les anciens termes qui désignaient ces oxydes étaient "béryllia" (oxyde de béryllium), "magnésia", " chaux vive", "strontia" et "baryta".
L'appellation "alcalino-terreux" est due au fait que les oxydes de ces métaux sont intermédiaires entre ceux des métaux alcalins et ceux des terres rares. L'utilisation du terme « terreux » pour classifier des substances à l'apparence inerte remonte à des temps anciens. Le plus ancien système connu est celui de la Grèce antique, et consistait en un système de quatre éléments classiques, incluant la terre. Des philosophes et alchimistes firent évoluer ce système par la suite, notamment Aristote, Paracelse, John Becher et George Stahl.
En 1789, Antoine Lavoisier dans son "Traité élémentaire de chimie", nota que ces "terres" étaient en fait des composés chimiques. Il les appela alors "substances simples salifiables terreuses". Par la suite il suggéra que les terres alcalines seraient peut-être des oxydes de métaux, mais il admit que ceci n'était qu'une simple conjecture. En 1808, Humphry Davy continua le travail de Lavoisier et fut le premier à obtenir des échantillons de métal par électrolyse de leurs terres en fusion.




</doc>
<doc id="19415" url="https://fr.wikipedia.org/wiki?curid=19415" title="Ranakpur">
Ranakpur

Ranakpur est une petite cité du Rajasthan située dans la chaîne des Ârâvalli à une soixantaine de kilomètres au nord de la ville d'Udaipur. Elle tire son nom du raja Rana Kumbha.

Le site doit sa notoriété au groupe de temples, majoritairement jaïns, et il fait partie des cinq pèlerinages majeurs du jaïnisme.

Le temple le plus important, celui d'Adinatha, est l'une des plus belles et plus vastes constructions jaïns de l'Inde. Il fut élevé par un riche marchand, "Dharna Sah", qui, à la suite de la vision du temple achevé à cet endroit, approcha l'architecte "Depaka" et demanda l'aide du râna Kumbha qui lui accorda le terrain et l'assistance nécessaire.

La construction eut lieu au milieu du et aboutit à un temple immense, formé de 29 salles, comportant 80 coupoles portées par 420 piliers. Le bâtiment est censé compter un total de 1444 piliers tous sculptés avec une ornementation différente. L'ensemble est construit en marbre blanc dont chaque centimètre est gravé, sculpté, ornementé. Le temple est aussi appelé "Chaumukha", ce qui signifie "quatre faces" car la cella abrite un Tîrthankara à quatre faces regardant vers les quatre points cardinaux.

Au côté de la merveille qu'est le temple d'Adinatha, on trouve deux autres temples jaïns du (un dédié à Neminâthaa et l'autre à Parshvanâtha, respectivement et Tîrthankaras) et un temple hindouiste dédié à Sûrya, datant du .



</doc>
<doc id="19417" url="https://fr.wikipedia.org/wiki?curid=19417" title="Année bissextile">
Année bissextile

Une année bissextile est une année comportant 366 jours au lieu de 365 jours pour une année non bissextile. Le jour supplémentaire, le 29 février, est placé après le dernier jour de ce mois qui compte habituellement 28 jours dans le calendrier grégorien. Sauf cas particuliers précisés ci-après, les années sont bissextiles tous les quatre ans. Exemple d'années bissextiles : 2016, 2020, 2024, 2028 ou 2032.

Depuis l'ajustement du calendrier grégorien, l'année sera bissextile (elle aura 366 jours) :
Sinon, l'année n'est pas bissextile (elle a 365 jours).

Ainsi, 2018 bissextile. L'an 2008 était bissextile suivant la première règle (divisible par 4 et non divisible par 100). L'an 1900 n'était pas bissextile car divisible par 4 mais aussi par 100 et non divisible par 400. L'an 2000 était bissextile car divisible par 400.

Le calendrier julien, qui avait cours avant le calendrier actuel, ne distinguait pas les fins de siècles (années divisibles par 100). Une année était bissextile tous les 4 ans, sans autre exception. Le calendrier julien avait ainsi une année moyenne de 365,25 jours, au lieu des 365,2422 jours nécessaires au cycle terrestre. Ce qui a engendré l'accumulation d'une dizaine de jours de retard en quinze siècles. 

L'instauration du calendrier grégorien a permis d'une part de rattraper le retard en supprimant des jours, et d'autre part de ralentir le rythme en supprimant 3 années bissextiles tous les 400 ans. Ce calendrier grégorien offre selon les règles énoncées une année moyenne de 365,2425 jours, ce qui est encore un peu trop long, mais n'engendre qu'une avance de 3 jours en .

L'habitude d'ajouter une journée intercalaire afin de rattraper le retard pris par l'année civile sur l'année solaire remonte aux Romains. Ceux-ci, avant le calendrier julien, utilisaient l'année dite « de Numa » de 355 jours, soit douze mois lunaires. Le retard avec le calendrier solaire était compensé par des mois intercalaires d'une durée variable fixée par le grand pontife. Ce système s'était cependant déréglé au moment des guerres civiles.

Les calendriers luni-solaires de type chinois, encore utilisés dans nombre de pays de l'Asie du Sud-Est et de l'Est pour fixer les fêtes traditionnelles, adoptent aussi ce principe: ajout d'un mois intercalaire 7 fois en 19 ans.

En 45, avant l'ère chrétienne, Jules César, alors dictateur (au sens latin du terme) et grand pontife de la République romaine, fit appel à l'astronome grec Sosigène d'Alexandrie, afin de régler le décalage trop important que l'on constatait entre les années solaires et civiles depuis les guerres civiles. Sosigène d'Alexandrie n'eut qu'à puiser dans le calendrier égyptien et se remémorer le décret de Canope pour proposer une solution.
Ainsi, Jules César fixa notre année de 365 jours, plus une journée intercalaire tous les quatre ans 

Ce jour « additionnel » se plaçait juste avant le 24 février. Il s'agissait donc d'un « 23 février bis ». On nommait le 24 février "a. d. VI Kal. Mart.", soit "ante diem sextum Kalendas Martias", ce qui signifie « le sixième jour avant les calendes de mars » (les Romains comptaient les jours à rebours, bornes incluses, à partir de trois dates de référence présentes dans chaque mois, à savoir les "calendes", le du mois, les "ides", le 13 ou 15 selon les mois, et les "nones", neuf jours bornes incluses avant les ides, comme leur nom l'indique, c'est-à-dire le 5 ou 7) ; le « 23 février bis » se disait donc tout naturellement "a. d. bis VI Kal. Mart.", soit "ante diem bis sextum Kalendas Martias" : « le sixième jour "bis" avant les "calendes" (le premier jour) de mars ». Une année "bissextile" comprend deux fois le sixième jour avant le premier mars ; « deux fois [le] sixième » se dit "bis sextus" en latin ; par l'ajout du suffixe "-ilis", est dérivé l'adjectif "bissextilis", d'où « bissextile » en français.

Plus tard, le jour intercalaire fut positionné le 29 du mois de février, à partir du moment où la méthode latine de décompte des jours fut remplacée par celle que nous employons toujours aujourd'hui.

Les personnes étant nées un 29 février fêtent habituellement leur anniversaire le 28 février les années non bissextiles comme 2011 ou 2013.
Dans certains pays, par exemple à Taïwan, une personne née un 29 février l'est légalement le 28. Par exemple, une personne née le aurait eu 18 ans le .

Depuis 1980 en France, un petit groupe de personnes édite un journal qui paraît seulement les 29 février, appelé "La Bougie du sapeur". En , il publie son numéro .

En 1700, la Suède tenta d'utiliser un calendrier julien modifié pour passer graduellement du calendrier julien au calendrier grégorien. Le processus devait réduire graduellement un jour par an, pendant 11 ans.
Seule l'année 1700 fut ainsi modifiée et en 1712 pour rattraper le calendrier julien il fallut rajouter un jour supplémentaire en février qui devint ainsi doublement bissextile et possédait un 30 février. 
En 1929, l'Union soviétique introduisit un calendrier révolutionnaire dans lequel chaque mois avait 30 jours, et les cinq ou six jours en excès étaient des jours de congé ne faisant partie d'aucun mois, à la manière des sans-culottides du calendrier républicain français. Les années 1930 et 1931 eurent donc un « 30 février » (un 2 mois de 30 jours), mais en 1932 ce calendrier fut partiellement abandonné et les mois retrouvèrent leur longueur antérieure.

La différence minime observée entre le calendrier grégorien 365,2425 jours et la réalité ~365,2422 jours peut être supprimée en continuant les règles d'exceptions aux années bissextiles.

La formule actuelle permet de gérer l'écart de 0,2425 jour sur un cycle de 400 ans :

Une formule plus précise permettrait de gérer l'écart de 0,2422 jour sur un cycle de ans :
Ainsi les années multiples de 2000 devraient ne pas être bissextiles, celles de 4000 le devraient, et celles de ne le devraient pas.



</doc>
<doc id="19420" url="https://fr.wikipedia.org/wiki?curid=19420" title="Gent">
Gent



</doc>
<doc id="19422" url="https://fr.wikipedia.org/wiki?curid=19422" title="Calendes">
Calendes

Les calendes (en latin archaïque : "kǎlendāī", "-āsōm" ; en latin classique : "cǎlendae", "-ārum") étaient le premier jour de chaque mois dans le calendrier romain, celui de la nouvelle lune quand le calendrier suivait un cycle lunaire (années de Romulus et de Numa Pompilius).

Ce jour-là, les pontifes annonçaient la date des fêtes mobiles du mois suivant et les débiteurs devaient payer leurs dettes inscrites dans les "calendaria", les livres de comptes, à l'origine du mot "calendrier".

Pour rendre honneur au dieu Mars et surtout pour faire correspondre le calendrier lunaire avec le cycle solaire, 

Ce terme archaïque proviendrait de l'étrusque, ce qui pourrait expliquer le maintien de la lettre "k" dans l'écriture des dates, lettre dont les Romains s'étaient pourtant rapidement débarrassés au profit de "c" (les rares mots latins en "k" sont en effet souvent d'origine étrangère).

Une autre explication donne une origine purement latine : il proviendrait du latin "calenda" (« ce qui doit être appelé ») du verbe "calare" (« appeler »).

Pour trouver la date calendaire latine, il faut compter le nombre de jours jusqu'à la nouvelle lune ou jusqu'au début du mois suivant et ajouter deux à ce nombre. Par exemple, le 22 avril est le dixième jour des calendes de mai parce qu'il reste huit jours auxquels on ajoute deux.

Ainsi, les calendes de Mars sont les jours intercalaires avant le premier jour du mois de mars qui est le premier mois de l'année romaine.

Chaque mois, les calendes étaient consacrées à Junon, comme les ides l'étaient à Jupiter. Junon était dite Junon calendaire ou mensale.

Les calendes de janvier se disaient Saturnales, que les romains célébraient à la fin du mois de décembre.

Les Matronalia (en latin : "Mātrōnālǐa", "- ǐum") : fêtes célébrées aux calendes de mars par les dames romaines.

Les Fabaries (en latin : "fabariae Kalendae") étaient les calendes de juin, où l'on offrait aux dieux les fèves nouvelles.

Ce mot est à l'origine de plusieurs termes et expressions utilisés en français.

Le calendrier dérive de l'adjectif "calendarium" (« calendaire »), qui désignait un registre de comptes (que l'on apurait le premier du mois ; le "calendarium" était proprement le « registre des échéances ») et, partant, le calendrier est, originellement, le registre sur lequel l'on note les événements liés à une date précise du mois. Le mot français provient directement de l'adjectif latin, avec un sens plus général.

« Renvoyer aux calendes grecques » ("Ad kalendas graecas") signifie « repousser indéfiniment la réalisation d'une action ». En effet, les Grecs n'ayant jamais eu de calendes, l'expression fait référence à une date inconnue. Les calendes grecques, tout comme la Saint-Glinglin, évoquent de manière ironique une date qui semble fixée mais qui en fin de compte n'aura jamais lieu.



</doc>

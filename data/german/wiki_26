<doc id="2832" url="https://de.wikipedia.org/wiki?curid=2832" title="Kolibris">
Kolibris

Die Kolibris (Trochilidae) sind Vögel und stellen nach Ansicht fast aller Autoren die einzige Familie der Ordnung der Kolibriartigen (Trochiliformes) dar. Sie sind allerdings sowohl mit den Seglern (Apodidae) als auch mit den Baumseglern (Hemiprocnidae) so nahe verwandt, dass sie manchmal mit diesen in der Ordnung der Seglervögel (Apodiformes) zusammengefasst werden.

Die Familie der Kolibris umfasst mehr als 100 Gattungen mit mehr als 330–340 Arten. Der Name "Kolibri" wurde im 18. Jahrhundert aus dem Französischen entlehnt (frz. "colibri") und stammt wohl aus einer karibischen Sprache.

Unter den Kolibris findet man die kleinste Vogelart überhaupt; die Bienenelfe ("Mellisuga helenae") misst samt Schnabel und Schwanzfedern nur 6 cm. Der Riesenkolibri ("Patagona gigas") ist mit ca. 25 cm Länge der größte Vertreter der Familie.

Die Schnäbel der Kolibri-Arten variieren stark – oft gattungstypisch – in Größe und Form. Beim Schwertschnabelkolibri ("Ensifera ensifera") z. B. ist der Schnabel fast so lang wie der ganze übrige Körper, der 10 cm misst. Der Kleinschnabel-Kolibri ("Ramphomicron microrhynchum") hat nur eine Schnabellänge von 5 mm. Die Schnäbel der Adlerschnabel-Kolibris ("Eutoxeres") sind stark nach unten gebogen, dagegen die Schnäbel der Schwarzbauch-Avosettkolibris ("Avocettula recurvirostris") an der Spitze nach oben gebogen. Jede Schnabel-Art ist auf einen anderen Blütentyp abgestimmt, sodass jede Gruppe von gleichschnabeligen Kolibris ihre eigene ökologische Nische besetzt.

Die Zunge der Kolibris ist extrem lang, kann weit hervorgestreckt werden und ist an der Spitze gespalten und strohhalmförmig, sodass der Nektar gut aus den Blüten gesaugt werden kann.

Kolibris besitzen acht Rippenpaare. Normalerweise haben Vögel sechs Rippenpaare.

Bei der Unterfamilie Phaethornitinae sind die drei Vorderzehen an der Basis aneinandergeheftet. Bei der Unterfamilie Trochilinae sind die Vorderzehen frei.

Die meisten Kolibris haben ein buntes, schimmerndes Gefieder. Bevorzugt sind Kopf, Kehle und Brust mit schillernden Farben versehen. Die Kehle bei den Männchen ist in der Regel bunt schillernd gefärbt, wobei es auch hier Ausnahmen gibt, zum Beispiel den Adlerschnabel-Kolibri. Die Wirkung des Farbspiels kommt durch Interferenz zustande. Die irisierende Kolibrifeder trägt mehrere Schichten mikroskopisch dünner Hornlamellen. Trifft nun Licht aus einem bestimmten Einfallswinkel auf diese Feder, so wird es von den Lamellen zurückgeworfen. Durch die unterschiedliche Lage der Lamellen wird das Licht in sehr kleinen zeitlichen Differenzen reflektiert.

Bei der Unterfamilie Phaethornitinae sind die Steuerfedern stark verlängert.
Kolibris führen ihren Schwirrflug mit einer sehr hohen Frequenz von 40 bis 50 Flügelschlägen pro Sekunde aus. Mit ihren beweglichen Flügeln können sie auf der Stelle fliegen, um zum Beispiel Nektar zu trinken. Sie können auch seitwärts und sogar rückwärts fliegen. Beim Kolibri ist im Gegensatz zu allen anderen Vogelfamilien die Hand größer als Ober- und Unterarm. Dies sowie eine extreme Beweglichkeit im Schulter- wie im Ellenbogengelenk erlauben dem Kolibri fast jede erdenkliche Flügelstellung. Die hierfür benötigte Brust- und Oberarmmuskulatur macht beim Kolibri gut ein Viertel seines Gesamtgewichts aus.

Bezogen auf ihre Körpergröße sind Kolibris die wohl schnellsten Wirbeltiere der Welt. So erreichen die etwa zehn Zentimeter großen Annakolibris bei ihren Balzflügen Geschwindigkeiten von 385 Körperlängen pro Sekunde (27,3 m/s bzw. 98 km/h), bei Beschleunigungswerten von etwa dem Zehnfachen der Erdbeschleunigung. Zum Vergleich: Wanderfalken kommen im Sturzflug auf Geschwindigkeiten von bis zu 200 Körperlängen pro Sekunde, Kampfjets, wie z. B. die MiG-25 (ein Mach 3 schneller Abfangjäger), erreichen dagegen maximal nur das rund 40-fache ihrer Gesamtlänge.

Das Herz der Kolibris ist im Verhältnis zum Körper sehr groß und schlägt 400- bis 500-mal pro Minute, ihre Atemfrequenz liegt bei bis zu 250 Zügen pro Minute. Während des Schlafes senken viele Kolibris ihre Herzfrequenz stark ab, um Energie zu sparen.

Da Eingang und Ausgang des Kolibrimagens sehr eng nebeneinander liegen, wird die aufgenommene flüssige Nahrung bei gefülltem Magen gleich vom Eingang in den Ausgang übergeleitet. Dies bedeutet, dass der Blütennektar nicht erst über den Muskelmagen in den Dünndarm gelangt, sondern diesen direkt erreichen kann, um dort aufgespalten und dem Energiestoffwechsel zugeführt zu werden.

Der Sauerstoffverbrauch der Kolibris erreicht einen sehr hohen Wert und liegt selbst beim ruhenden Tier fünf bis zehn Mal höher als bei Finkenvögeln.

Kolibris haben die Fähigkeit entwickelt, ihre Körpertemperatur erheblich absenken zu können, um in Notsituationen den Stoffwechsel so zu reduzieren, dass ein Überleben möglich wird. Bei Kolibris wurde auch der Zustand der völligen Teilnahmslosigkeit (Torpidität) beschrieben.

Kolibris leben nur in Amerika und der Karibic. Sie kommen vom Süden Alaskas bis Feuerland vor. Sie leben in Halbwüsten, in den Waldgebieten am Amazonas und in gemäßigten Zonen in den Laubwäldern Chiles. Man trifft sie fast überall im südlichen Nord- und Südamerika an, außer in der subantarktischen und borealen Zone. Von den 330–340 Arten leben fast 130 in der Nähe des Äquators. Nur ein gutes Dutzend Arten lebt in Nordamerika nördlich von Mexiko, die meisten davon im Südwesten der USA. Der Rubinkehlkolibri ("Archilochus colubris") brütet als einziger im Osten Kanadas und der USA. Nur auf Jamaika lebt der Wimpelschwanz ("Trochilus polytmus"), dessen Männchen einen bis zu 17 cm langen Schwanz hat.

Das Taubenschwänzchen ("Macroglossum stellatarum") ist ein in Europa und Asien lebender Schmetterling, der in der Luft stehend mit seinem Rüssel Nektar in Blüten saugt und daher bisweilen für einen Kolibri gehalten wird.

Die Kolibris ernähren sich vorwiegend von Blütennektar. Diese sehr energiereiche Nahrung macht den kraftraubenden Flugstil erst möglich. Insbesondere auffällig rot oder orange gefärbte Blumen ziehen die Kolibris an.

In den Blüten sammeln sich zudem Insekten, die ebenfalls von den Kolibris gefressen werden und eine ausreichende Versorgung mit Eiweiß sicherstellen. Pollen und Fruchtfleisch werden nicht gefressen, da sie gänzlich unverdaulich sind.

Um bei den Weibchen Interesse zu wecken und sie in Paarungsbereitschaft zu bringen, führen die Männchen einen Balztanz auf. Nach der Begattung bauen die Weibchen ein winziges Nest, das aus Spinnweben, Pflanzenwolle, Flechten oder Moos angefertigt wird. Das Nest wird in geringer Höhe in einem Busch oder einem Baum versteckt gebaut. Das Weibchen legt im Abstand von zwei Tagen zwei Eier. Die Brut dauert 14 bis 19 Tage. Die Jungen werden anschließend 3–4 Wochen bis zu 140-mal am Tag gefüttert. Zur Nahrungssuche lassen sich die Weibchen aus dem Nest fallen und gleiten dabei blattähnlich zu Boden. Dadurch wird Nesträubern das Orten des versteckten Nestes erheblich erschwert.

Natürliche Feinde der Kolibris sind Schlangen, Greifvögel, Katzen und Marder.

Der deutsche Paläoornithologe Gerald Mayr vom Frankfurter Senckenberg Forschungsinstitut entdeckte die vermutlich ältesten Kolibrifossilien der Welt in der Grube Unterfeld im baden-württembergischen Frauenweiler (Stadtteil von Wiesloch). Er beschreibt im Fachmagazin "Science" den Fund zweier über 30 Millionen Jahre alter Fossilien, die den heute lebenden amerikanischen Kolibris ähnelten. Es sind die ersten Funde von Kolibris in der Alten Welt.

Die Skelette sind etwa vier Zentimeter lang, haben einen langen Schnabel, um Blütennektar zu saugen, sowie Flügel, die zum Schweben auf der Stelle befähigen. Damit zeigen sie die typischen Merkmale heutiger Kolibris.

Mayr taufte sie auf den Namen "Eurotrochilus inexpectatus" – „unerwarteter, europäischer Kolibri“.

Kolibris gelten als schwierig zu haltende Ziervögel. Zu den wenigen Arten, die auch von Privatpersonen gehalten werden, gehört der Veilchenohrkolibri, bei dem auch bereits die Nachzucht gelungen ist.




</doc>
<doc id="2834" url="https://de.wikipedia.org/wiki?curid=2834" title="Karthago">
Karthago

Karthago (, "Karchēdṓn", etruskisch "Karθazie"; aus dem phönizisch-punischen "Qart-Ḥadašt" „neue Stadt“, wobei gemeint war: „neues Tyros“) war eine Großstadt in Nordafrika nahe dem heutigen Tunis in Tunesien. In der Antike war Karthago zunächst Hauptstadt der gleichnamigen See- und Handelsmacht. Die Einwohner Karthagos wurden von den Römern als Punier (abgeleitet von Phönizier) bezeichnet. Nach der Zerstörung Karthagos durch die Römer ging das karthagische Reich im 2. Jahrhundert v. Chr. im römischen Imperium auf; Karthago wurde unter Gaius Iulius Caesar neu gegründet und stieg bald erneut zu einer bedeutenden Großstadt auf. Erst mit dem Ende der Antike kam auch das Ende der Bedeutung des Ortes.

Heute ist Karthago (, ) ein Vorort von Tunis. Das archäologische Ausgrabungsgelände von Karthago wurde 1979 in die Liste des Weltkulturerbes aufgenommen und ist eine touristische Attraktion.

Karthago liegt an der afrikanischen Mittelmeerküste rund zehn Kilometer östlich des heutigen Tunis im Norden Tunesiens. Karthago befand sich an der wichtigsten phönizischen Handelsroute zwischen der Levante und Gibraltar. Durch seine Lage an der Straße von Sizilien konnte es den Seehandel im Mittelmeer kontrollieren. Dies war ein Hauptgrund für die wirtschaftliche und militärische Dominanz der Stadt. Zugleich bestand von Sizilien aus eine Verbindung zum Tyrrhenischen Meer und zum nördlichen Mittelmeerraum.

Die Stadt selbst lag auf einer Halbinsel, die im Osten vom Golf von Tunis, im Norden von der Lagune Sebkhet Ariana und im Süden vom See von Tunis begrenzt wird. Die Lage war strategisch günstig, weil sich die Stadt so zur Landseite hin leicht verteidigen ließ. Der Byrsa-Hügel war das Zentrum sowohl des punischen als auch des römischen Karthago. Nördlich des eigentlichen Stadtgebiets, aber noch innerhalb der Stadtmauern, befand sich in der Antike das landwirtschaftlich genutzte Gebiet von Megara.

Dieser Abschnitt behandelt die Geschichte der Stadt Karthago.
Karthago wurde im 9. oder 8. Jahrhundert v. Chr. von phönizischen Siedlern aus Tyros gegründet. Im Gegensatz zur älteren Kolonie Utica („Alte (Stadt)“) nannten sie die Stadt „Neue Stadt“, "Qart-Hadašt". Dionysios von Halikarnassos datiert die Gründung Karthagos auf das Jahr 814 v. Chr. Die ältesten archäologischen Funde lassen sich auf die zweite Hälfte des achten Jahrhunderts v. Chr. datieren.

Einzig der Historiker Junianus Justinus nennt die Gründung Karthagos in Verbindung mit „Elissa“ (Dido bei den Römern), punisch „'Išt“. Elissa soll die Tochter des Mutto, König von Tyros gewesen sein. Auf der Flucht vor der Verfolgung durch ihren Bruder Pumjaton gelangte sie über Zypern an den Golf von Tunis. Der ortsansässige Häuptling versprach ihr so viel Land, wie sie mit einer Kuhhaut umspannen könne. Elissa schnitt daraufhin die Kuhhaut in dünne Streifen, legte sie aneinander und konnte somit ein großes Stück Land markieren. Dieser Küstenstreifen bildete die Byrsa, die Keimzelle Karthagos. Nach der Gründung Karthagos habe sich Elissa selbst auf einem Scheiterhaufen geopfert, um der Stadt Wohlstand zu garantieren.

Der Name „'Išt“ (Elissa) ist in der punischen Onomastik mehrfach bezeugt, wobei dessen Bedeutung „die Aktive“ nicht sicher geklärt ist und dass eine Frau eine so weitreichende Expedition leitete, entspricht nicht den damaligen Gegebenheiten und ist daher wenig glaubhaft. Ebenfalls umstritten ist das Bestehen eines „Elissa-Kultes“. Die vorherige Flucht hat außerdem legendenhafte Züge. Weitere Einzelheiten der Geschichte sind aufgrund der griechischen Volksetymologie entstanden. Insgesamt ist daher die „Quelle Justinus“ als wenig zuverlässig zu werten. Sichere Belege für die Gründung Karthagos fehlen damit vollständig. Eine abgewandelte Form dieser Legende findet sich in Vergils "Aeneis".

In den ersten beiden Jahrhunderten seines Bestehens war Karthago von seiner Mutterstadt Tyros abhängig, d. h., es zahlte beständig Abgaben an die Mutterstadt. Als das phönizische Mutterland 539 v. Chr. von den Persern erobert wurde, löste sich Karthago vom tyrischen Einfluss. In der Folgezeit entwickelte es sich zu einer bedeutenden See- und Handelsmacht und gründete Kolonien auf Sizilien, Sardinien, Korsika, den Balearen, an der nordafrikanischen Küste und an der südlichen Mittelmeerküste Spaniens. Auch die langandauernden Konflikte während der Perserkriege, als persische Verbündete des 5. und 4. Jahrhunderts v. Chr. mit den griechischen Kolonien, vor allem Syrakus und seit seiner Gründung auch mit Nikaia (Nizza), taten dem Aufstieg keinen Abbruch. Während dieser Zeit war Karthago einem starken Einfluss der griechischen Kultur ausgesetzt, aber mit den Etruskern und Persern verbündet. Während der Perserkriege spiegeln zwei Ereignisse im westlichen Mittelmeer von historischer Bedeutung die entsprechenden im östlichen:
Die Stadt prosperierte durch den Aufschwung des Seehandels. Im 4. und 3. Jahrhundert v. Chr. war Karthago zur reichsten Stadt des Mittelmeerraums geworden. In ihr wohnten 400.000 Menschen, weitere 100.000 lebten in der angrenzenden landwirtschaftlichen Nutzfläche der Megara.

Die drei "Punischen Kriege" gegen das aufstrebende Rom führten letztlich zum Niedergang Karthagos. Der zweite dieser Kriege hat aber durch den karthagischen Feldherrn Hannibal zu einer ernsten Bedrohung für Rom selbst geführt.

Nach dreijähriger Belagerung eroberten die Römer unter Scipio Aemilianus Karthago 146 v. Chr. zum Ende des Dritten Punischen Krieges. Die Verteidiger unter Hasdrubal leisteten erbitterten Widerstand. Während der sechstägigen Eroberung plünderten und zerstörten die römischen Truppen bereits durch Brandlegung einen Großteil der Stadt. 50.000 der überlebenden Einwohner ergaben sich daraufhin den Römern und wurden in die Sklaverei verkauft. Gemäß der berühmten Forderung des älteren Cato – meist wiedergegeben als "Ceterum censeo Carthaginem esse delendam" („Im Übrigen meine ich, dass Karthago zerstört werden muss“) – wurde daraufhin auch noch der Rest der Stadt inklusive der Burg (Byrsa) systematisch bis auf die Grundmauern geschleift. Die Legende, dass auf Karthagos Boden Salz gestreut wurde, um die Gegend unfruchtbar zu machen, stammt dagegen aus dem 19. Jahrhundert und wird durch antike Quellen nicht belegt. Die Stadtfläche lag jedoch ein Jahrhundert lang brach. Auch die kulturellen Leistungen wurden bei der Zerstörung beseitigt. Nicht zerstörte Dokumente wurden verbündeten numidischen Fürsten übergeben. Einzig das Werk über die Landwirtschaft des Mago wurde auf Befehl des römischen Senats ins Lateinische übersetzt und dürfte teilweise bei späteren römischen Landwirtschafts-Beschreibungen verarbeitet worden sein.

Im Jahr 122 v. Chr. versuchte der Reformer Gaius Sempronius Gracchus im Rahmen seiner Sozialpolitik, Karthago als "Colonia Iunonia Carthago" wiederzugründen. Damit stieß er jedoch auf den Widerstand des Senats. Nach Gracchus' gewaltsamem Tod wurde das Vorhaben wieder aufgegeben. Schließlich war es Julius Caesar, dem Karthago seine Wiederauferstehung verdankte. Nach seinem Sieg über Pompeius im Jahr 46 v. Chr. entschloss sich Caesar, Karthago wieder aufbauen zu lassen. Verwirklicht wurde dieses Vorhaben erst unter Augustus, der 29 v. Chr. 3000 Siedler in Karthago ansiedelte. Die Stadt trug nun den Namen "Colonia Iulia Concordia Carthago". Dabei wurden insbesondere durch den Abtrag des Byrsa-Hügels große Teile der bis dahin noch vor Ort im Boden verbliebenen Bauwerkreste unwiederbringlich zerstört.

In Karthago residierte der römische Statthalter, der senatorische "proconsul" der Provinz "Africa Proconsularis"; dieser Posten war auch in der Kaiserzeit einer der prestigeträchtigsten im ganzen Reich. Es erfolgte – insbesondere aufgrund des Handels mit Getreide und Töpferware – ein rascher Aufschwung. Im 2. Jahrhundert n. Chr. war Karthago mit über 300.000 Einwohnern nach Rom, Alexandria und Antiochia die viertgrößte Stadt des Römischen Reiches. 238 wurde hier der "proconsul" von Rebellen als Gordian I. zum Gegenkaiser ausgerufen – die Revolte wurde zwar niedergeschlagen, markierte aber für Kaiser Maximinus Thrax den Anfang vom Ende.

Karthago war das Zentrum des frühen Christentums in Nordafrika. Bereits im 2. Jahrhundert bestand in Karthago eine große christliche Gemeinde; die Stadt war aufgrund ihrer Größe neben Rom der wichtigste Bischofssitz in der westlichen Reichshälfte. Die Akten der "Scilitanischen Märtyrer", die 180 in Karthago hingerichtet wurden, stellen das älteste christliche Dokument in lateinischer Sprache dar. Im Jahr 203 ließen die heiligen Felicitas und Perpetua in der Arena Karthagos ihr Leben. Wichtige Kirchenväter wie Tertullian und Cyprian wirkten in Karthago und prägten die christliche Literatur in lateinischer Sprache. Cyprian konnte in seiner Zeit als Bischof (248–258) die Christengemeinde von Karthago durch zahlreiche Synoden afrikanischer Bischöfe als führende Gemeinde der Provinz "Africa" etablieren, deren Autorität auch nach Spanien, Gallien und Italien ausstrahlte. Er rivalisierte sogar mit dem Bischof von Rom, konnte sich gegen diesen aber letztlich nicht durchsetzen. Zugleich nahmen die Christenverfolgungen zu: Cyprian, der 250 noch vor Verfolgern geflüchtet war, starb 258 demonstrativ den Märtyrertod.

Im 4. Jahrhundert verlor Karthago zwar etwas an Bedeutung, blieb aber eine blühende Metropole, da "Africa" weiterhin der wichtigste Getreidelieferant der Stadt Rom war. Gegen Ende des 4. Jahrhunderts studierte der Kirchenvater Augustinus von Hippo in Karthago. Der Kommandeur der dortigen römischen Truppen, der "comes Africae", hatte eine wichtige Machtposition inne, da er durch die Kontrolle der Kornzufuhr Italien unter Druck setzen konnte. Nacheinander erhoben sich in den Jahren um 400 die "comites" Gildo, Heraclianus und Bonifatius in Karthago gegen die weströmische Regierung. Es war diese strategische wie ökonomische Bedeutung der Stadt, die auch bei germanischen Heerführern Begehrlichkeiten weckte. Während die beiden westgotischen Anführer Alarich und Athaulf noch mit ihren Versuchen scheiterten, nach "Africa" überzusetzen, hatte Geiserich schließlich Erfolg.

439 wurde Karthago vom Kriegerverband der Vandalen eingenommen, der unter seinem "rex" Geiserich im Zuge der sogenannten Völkerwanderung bereits 429 von Spanien nach Nordafrika übergesetzt war und schließlich ganz "Africa" erobert hatte. Größere Zerstörungen im Zusammenhang mit der Eroberung durch die Vandalen sind in Karthago nicht nachweisbar; auch die Hohe Schule blieb bestehen. Geiserich nutzte das reiche Gebiet als Versorgungsbasis für seine Männer, bedrohte von hier aus viele Gebiete Westroms und versuchte wiederholt, die kaiserliche Regierung in Italien zu erpressen. Ein großangelegter Versuch west- und oströmischer Truppen, das Gebiet zurückzuerobern, scheiterte 468. 474 erkannte Kaiser Zenon Geiserichs Herrschaft über "Africa" an, auch wenn das Gebiet formal Teil des "Imperium Romanum" blieb. Karthago war die Hauptstadt des Vandalenreiches, bis es 533/534 von oströmischen Truppen unter dem Feldherrn Belisar erobert wurde.

In der Folgezeit war Karthago Sitz eines oströmischen Statthalters, eines eigenen Prätorianerpräfekten sowie eines Heermeisters und Sitz der Verwaltung für das kaiserliche Nordafrika, das dann unter Kaiser Maurikios um 590 als Exarchat von Karthago reorganisiert wurde. Die nordafrikanische Kirche erreichte zudem bereits um 535 die Erneuerung ihrer alten Privilegien, und Kaiser Justinian richtete 534 neun Professuren ein: fünf für Medizin und jeweils zwei für lateinische Rhetorik und Grammatik. Karthagos große Zeit war dennoch vorbei, allerdings lässt sich um 600 eine gewisse Nachblüte beobachten: Viele Gebäude im Zentrum wurden noch einmal erneuert und renoviert, während allerdings zugleich das besiedelte Stadtgebiet schrumpfte. Kaiser Herakleios (610–641) war durch einen Putsch seines Vaters, des Exarchen von Karthago, gegen Phokas an die Macht gekommen und zog kurzzeitig in Betracht, die Hauptstadt des Reiches aufgrund der Bedrohung Konstantinopels durch die persischen Sassaniden und die Awaren nach Karthago zu verlegen.

Ab dem Jahr 647 stießen die Araber im Zuge der islamischen Expansion auch nach Nordafrika vor. Der abtrünnige kaiserliche Exarch Gregor, vom Nachschub aus dem Mutterland abgeschnitten, erlag nach kurzem Widerstand der Übermacht der Araber, die bald die Provinz Ifrīqiya mit der Hauptstadt Kairouan gründeten. Das stark befestigte Karthago fiel aber erst 698 nach der byzantinischen Niederlage in der Schlacht von Karthago endgültig an die Angreifer und wurde von den Arabern zerstört. Damit endete für Afrika die Spätantike. Fortan übernahm die nahe gelegene Stadt Tunis die Rolle eines Verwaltungszentrums. Die Ruinen Karthagos dienten jahrhundertelang als Steinbruch für die Bauten in Tunis, Kairouan, Sousse und in anderen arabischen Städten.

Das Wissen über die Verfassung Karthagos beruht neben der punischen Epigraphik auf den antiken Autoren Aristoteles, Polybios, Diodor, Livius und Justinus. Aristoteles untersucht in seinem staatsphilosophischen Werk "Politik" verschiedene bestehende Staatsformen, darunter auch die karthagische. Aristoteles vergleicht die Verfassung Karthagos mit der von Sparta und äußert sowohl Lob als auch Kritik an der karthagischen Staatsstruktur. Ein Problem bei der Rekonstruktion der karthagischen Staatsverfassung ist, dass in den lateinischen und griechischen Quellen die karthagischen Staatsämter oft ungenau bezeichnet werden.

Anfangs standen Könige an der Spitze des karthagischen Staatswesens, ganz nach dem Vorbild der Mutterstadt Tyros. In späterer Zeit war der karthagische Staat eine Oligarchie mit demokratischen Elementen, ähnlich der Römischen Republik.

An der Spitze des karthagischen Gemeinwesens standen zwei Sufeten, von römischen und griechischen Autoren als „Könige“ bezeichnet, die jährlich neu gewählt wurden. Ihre Rolle entsprach etwa der der Konsuln Roms oder der Dogen Venedigs. Sie leiteten den Magistrat, zu dem weitere Ämter gehörten, etwa das Amt des „Großen“ (punisch "rab", möglicherweise verantwortlich für die Staatsfinanzen) und ein eigenes Feldherrenamt. Das wichtigste politische Organ war der Senat, der über politische Fragen zu entscheiden hatte. Er wurde von einem Ausschuss von 30 Senatoren geleitet. Ein zusätzlich gewähltes Richtertribunal setzte sich aus 100 Senatoren zusammen und fungierte als oberster Gerichtshof. Daneben gab es eine Volksversammlung, in der alle Bürger stimmberechtigt waren.

Der Stadtstaat Karthago beherrschte ein großes Reich im westlichen Mittelmeerraum. Dabei gewährten die Bewohner Karthagos den eroberten Gebieten relativ viel Autonomie und beschränkten sich auf die Verwaltung, das Eintreiben von Steuern und die Rekrutierung von Streitkräften. Die karthagisch kontrollierten Gebiete wurden in Verwaltungsbezirke aufgeteilt, die von Beamten kontrolliert wurden. Alte phönizische Gründungen wie Utica sowie griechische Kolonien auf Sizilien durften ihre lokale Verwaltungen beibehalten.

Das karthagische Heer bestand ursprünglich aus den Bürgern der Stadt selbst. Mit der Ausdehnung des karthagischen Staates kamen dann immer größere Anteile der Truppen von den unterworfenen Völkern oder Verbündeten und schließlich auch Söldner hinzu. Hauptsächlich wurden Kämpfer von den Numidern, Iberern, Libyern, Elymern und Sikelern rekrutiert, daneben auch Sarden, Italiker und Kelten sowie Griechen.

Die verschiedenen Völker wurden dabei von den Karthagern jeweils nach der für sie typischen Kampfweise eingesetzt. Gute Beispiele sind hier die Numider, die als leichte Kavallerie dienten, und die Einwohner der Balearen, die hervorragende Schleuderer stellten. Die jeweiligen Kontingente wurden von karthagischen Offizieren befehligt, blieben aber manchmal auch unter dem Kommando ihrer eigenen Anführer.

Die Karthager selbst dienten zum größeren Teil bei der Flotte, es gab aber durchgehend bis zum Schluss auch Landtruppen, die sich aus Karthagern selbst zusammensetzten. Die Kerneinheit dieser Verbände war die sogenannte „Heilige Schar“, eine Elitetruppe von 2.500 Mann, die vermutlich auch als eine Art Offiziersschule diente, aus der sich dann die militärischen Führer der nicht-karthagischen Einheiten rekrutierten. Über diese Einheit hinaus gab es aber auch andere karthagische Landtruppen in nicht unbeträchtlicher Anzahl. Erst im Zweiten Punischen Krieg in der Armee Hannibals überwogen dann die Einheiten der Verbündeten und Söldner.

Im karthagischen Heer existierte eine Art Heeresversammlung, ähnlich, aber doch verschieden von der Heeresversammlung der Makedonen. Beim Tode ihres Feldherrn wählte eine solche Heeresversammlung aus den Offizieren den Nachfolger. Die Wahl wurde aber nur rechtskräftig, wenn die Volksversammlung in Karthago sie bestätigte.

Die karthagischen Feldherrn und höheren Offiziere stammten aus den führenden Familien der Stadt und bildeten immer wieder regelrechte Militärdynastien, in denen die Söhne ebenfalls Feldherren wurden. Im Normalfall wurden Befehlshaber von der Volksversammlung gewählt. Die Haltung des karthagischen Staates zu seinen Feldherrn war ambivalent. Nach Niederlagen oder Versagen kam es vor, dass der Feldherr hingerichtet wurde. Auch gab es eine deutlich stärkere Kontrolle des Staates über das Militär als in anderen vergleichbaren Staaten der damaligen Zeit. Nach dem Ende eines Feldzuges hatten die hohen Offiziere dem Rat der Einhundertvier strenge Rechenschaft abzulegen. Während der Zeit des Feldzuges selbst aber war der Feldherr jedem normalen Recht entzogen und durfte frei agieren.

Die Unterführer und niedrigeren Offiziere konnte der jeweilige Oberkommandierende nach Gutdünken ernennen. Bei fremdländischen Einheiten, insbesondere bei denen Verbündeter, waren die gewöhnlichen Offiziere fast immer aus den Führungsschichten dieser Völker selbst.

Lange Zeit charakterisierte das Heer Karthagos eine gewisse Rückständigkeit. So wurden noch in den Kriegen auf Sizilien lange Zeit Streitwagen verwendet, als diese überall sonst längst außer Gebrauch gekommen waren. Auch die Bewaffnung und Organisation scheint immer wieder rückständig. Auffallend ist hier die hohe Anzahl von Seuchen, von denen karthagische Heere lange Zeit getroffen wurden, was die Frage nach der Lagerorganisation und der Hygiene stellt. Erst in den Kämpfen gegen Pyrrhus modernisierte sich das Landheer. Die berühmten karthagischen Kriegselefanten wurden auch erst um diese Zeit eingeführt.

Der Kern der karthagischen Armee war durchgehend die Phalanx nach griechischem Vorbild. Die Phalanx wurde zur Zeit des Ersten Punischen Krieges von griechischen Militärberatern modernisiert, die als Söldner die Streitkräfte nach den Vorbildern der Diadochenreiche umorganisierten. Im Krieg gegen die Römer wurde die Phalanx jedoch immer mehr mit leichten Truppen ergänzt, bis sie im Zweiten Punischen Krieg dann vielleicht sogar aufgegeben wurde.

Die karthagische Kavallerie wurde ebenfalls erst in der Folge des Ersten Punischen Krieges bedeutsam, vorher spielte sie in den Heeren Karthagos nur eine geringe Rolle. Von den Barkiden wurden immer größere Verbände numidischer und iberischer Kavallerie eingesetzt. Erst im Zweiten Punischen Krieg war dann die karthagische Kavallerie eine bedeutende militärische Größe.

Die Marine Karthagos bestand aus einer beträchtlichen Zahl von Kriegsgaleeren. Die Schiffe selbst waren durchgehend auf dem Stand ihrer Zeit und die Karthager entwickelten auch eigene neue Schiffstypen. Am Anfang war die Trireme mit drei Ruderebenen und je einem Mann an einem Ruder das Standardkriegsschiff, dann entwickelten die Karthager die Quadrireme als neuen Schiffstyp. Hierbei wurde die Zahl der Ruderebenen wieder auf zwei reduziert und an jedem Ruder zwei Mann eingesetzt.
Ebenso wurde die von den Griechen entwickelte Quinquereme in Karthago sehr schnell übernommen und verbessert.

Ein typisches karthagisches Kriegsschiff war zwischen 35 m und 45 m lang und 5 m bis 6 m breit. Die Mannschaft einer karthagischen Quinquereme betrug um die 300 Mann. Die Karthager setzten im Kampf zur See stark auf das Rammen der feindlichen Schiffe und setzten daher weniger Seesoldaten an Deck ein, was wegen der geringeren Last die Schiffe schneller machte, aber auch anfälliger für das Entern.

Die Karthager verwendeten die von den Phöniziern erfundenen Trockendocks und zogen in dem bekannten kreisrunden Kriegshafen in Karthago selbst die Schiffe in spezielle Schiffsschuppen. Von der Anzahl der dort vorhandenen Liegeplätze kann man auf eine Flotte von rund 350 Kriegsschiffen für die Hochzeit Karthagos schließen, die folglich eine Besatzung von ungefähr 100.000 Mann benötigt hätten. Mit der Ausdehnung des karthagischen Staates entlang der Küsten musste die Flotte im Laufe der Zeit immer größer werden, was immer mehr Bürger für die Bedienung der Schiffe dem Heer entzog.

Eine Besonderheit des karthagischen Schiffbaus war die Massenfertigung von Schiffen innerhalb kurzer Zeit, was durch die Verwendung von Fertigteilen und eine Standardisierung dieser Teile möglich war. Die Römer übernahmen dieses Konstruktionsmerkmal von den Karthagern und konnten so ebenfalls in kurzer Zeit große Flotten aufstellen.

Die Punier waren ausgezeichnete Seefahrer. Deshalb verwundert es nicht, dass der Seehandel für die Wirtschaft Karthagos eine zentrale Rolle spielte. Am Kreuzungspunkt der Handelsrouten zwischen dem östlichen und westlichen sowie dem nördlichen und südlichen Mittelmeer gelegen, war Karthago zudem einer der Hauptumschlagplätze für ausländische Güter.

Das wichtigste kommerzielle Interesse der Karthager war der Erwerb von Metall. Silber importierten sie vor allem aus Südspanien, wo sich in der Nähe der Stadt Carthago Nova (heute Cartagena) ertragreiche Bergwerke befanden, daneben auch aus Sardinien und Etrurien. Gold kam wahrscheinlich durch direkten oder indirekten Handel aus Westafrika. Der Bedarf an weniger wertvollen Metallen wie Kupfer und Eisen konnte wohl durch heimische Vorkommen in Nordafrika gedeckt werden. Das zur Bronzeherstellung notwendige Zinn importierte man über die Atlantikküste aus Galicien oder aus Südspanien. Unter dem Seefahrer Himilkon unternahmen die Karthager sogar eine Expedition nach Britannien, um die dortigen Zinnvorkommen zu erschließen.

Nordafrika war in der Antike ein landwirtschaftlich sehr produktives Gebiet, und in der neueren Forschung wird betont, dass Agrarwirtschaft neben Handel bereits früh eine große Rolle spielte. Möglicherweise konkurrierten in der Oberschicht jene, die eher zu einer Händleraristokratie gehörten, mit den Großgrundbesitzern. In römischer Zeit galt die Provinz "Africa" dann bis in die Spätantike neben Ägypten als eine „Kornkammer Roms“; das Gebiet war zu jener Zeit teils noch bewaldet und hielt daher die Bodenkrume. Die Punier hatten früh fortschrittliche landwirtschaftliche Techniken entwickelt. Der karthagische Schriftsteller Mago verfasste im frühen 2. Jahrhundert v. Chr. eine Agrar-Enzyklopädie, die nicht erhalten ist, aber oft von römischen Autoren zitiert wurde. In Byzacena, einem Gebiet, das in etwa dem heutigen tunesischen Sahel entspricht, und im Medjerda-Tal erzielten die Punier hervorragende Ernten. Neben Weizen wurden intensiv Olivenbäume, Weinreben, Feigenbäume und Dattelpalmen angebaut.

Auch die Fischerei war ein lukrativer Wirtschaftszweig. Vor der Küste Karthagos wurden Thunfische gefangen. Vor allem aber betrieb man Fischfang vor der Atlantikküste Spaniens und des heutigen Marokkos, von wo aus der eingesalzene Fisch nach Karthago und in Form von Garum in andere Orte des Mittelmeers exportiert wurde.

Die Produktherstellung in der Stadt Karthago bestand vor allem aus Webereien und Färbereien, daneben auch Betriebe zur Keramikproduktion. Abgesehen von Textilien wurden die Produkte aber nicht exportiert, sondern waren vorrangig auf den heimischen Markt ausgerichtet. Ein weiterer wichtiger Wirtschaftszweig war der Schiffbau. Das dafür benötigte Holz konnte in den damals noch in der Umgebung Karthagos vorhandenen Eichen- und Kiefernwäldern gewonnen werden. Unter Verwendung des Zuschlagstoffes Kalk wurde in einem mehrstufigen Verfahren hochwertiges Eisen erzeugt. Überhaupt war die Metallurgie so weit fortgeschritten, dass erst weitere eingehende Studien Einblicke in die Kausalzusammenhänge der Macht des karthagischen Reiches ermöglichen werden. Aus Zinn und Kupfer wurde Bronze hergestellt und zu Gefäßen und anderen Gegenständen verarbeitet. Vor allem in Kriegszeiten florierte die Waffenproduktion.

Anfangs wurden in Karthago, wie im phönizischen Mutterland üblich, Astarte und Melkart als Hauptgötter verehrt. Ab dem 5. Jahrhundert v. Chr. entwickelten sich Tanit und Baal-Hammon zu den Hauptgöttern Karthagos. Tanit wurde als Schutzpatronin der Stadt verehrt, ihr Gemahl Baal Hammon galt als Fruchtbarkeitsgott. Eine weitere bedeutende Gottheit im Pantheon der Karthager war Eschmun. Auch fremde Kulte wie der der ägyptischen Göttin Isis wurden in Karthago praktiziert.

Es gilt als wahrscheinlich, dass die Karthager Menschenopfer praktizierten. Antike Autoren wie Diodor und Plutarch berichten, dass Kinder, vornehmlich Erstgeborene aus wohlhabenden Familien, einer Molochstatue in die Arme gelegt und durch einen Mechanismus in ein Feuer fallengelassen wurden. Die schriftliche Überlieferung wird durch Funde von Knochen kleiner Kinder auf dem Tofet von Karthago gestützt. Die Interpretation als Menschenopfer ist vor allem durch Gustave Flauberts Roman "Salammbô" (1862) bekannt geworden, sie ist jedoch umstritten – möglicherweise verbrannte man tot geborene und sehr früh verstorbene Kinder.

Die punische Kunst lehnte sich anfangs noch an ihre phönizischen Vorläufer an. Über die griechischen Kolonien auf Sizilien wurden die Karthager schon früh griechischem Einfluss ausgesetzt. Ab dem 4. Jahrhundert v. Chr. war der Einfluss des Hellenismus besonders stark.

Die zahlenmäßig am meisten vertretenen Beispiele punischer Kunst sind die Votivmonumente aus den Grabbezirken (Tofets). Ab dem 5. Jahrhundert v. Chr. treten hohe, mit Reliefs verzierte Kalksteinstelen auf. Das häufigste Motiv ist das Tanit-Zeichen und die Halbmondscheibe. Seltener kommen auch Darstellungen von Menschen oder Tieren vor.

Die Einwohner Karthagos sprachen Punisch, eine Variante der phönizischen Sprache. Damit gehört das Punische zu den semitischen Sprachen und ist mit dem Hebräischen verwandt. Die punische Schrift ist eine kursive Variante des phönizischen Alphabets und war bis ins 1. Jahrhundert n. Chr. in Verwendung.

Überliefert ist die Sprache der Punier mehrheitlich nur durch Inschriften, von denen fast alle einen religiösen Inhalt haben. Meistens handelt es sich um Widmungen auf Gedenk- oder Grabsteinen auf den Kultstätten oder Nekropolen. Die Inschriften sind meist kurz und formelhaft. Zu den wenigen Texten mit nicht sakralem Charakter gehört eine Inschrift aus Karthago, die die Einweihung einer Straße oder eines Tores (die Übersetzung des punischen Begriffs ist nicht gesichert) behandelt. Kürzere Inschriften finden sich auf Keramikfragmenten oder Schmuckstücken.

Die Literatur der Punier, deren Existenz von antiken Autoren bestätigt wird, ist nicht erhalten. Die Bibliothek von Karthago wurde während der Zerstörung der Stadt 146 v. Chr. vernichtet. Der Periplus von Hanno, der Bericht über eine Seereise entlang der afrikanischen Westküste, ist in einer griechischen Übersetzung überliefert. Der Schriftsteller Mago verfasste im 2. Jahrhundert v. Chr. ein bedeutendes Werk über die Landwirtschaft, das nicht erhalten ist, aber von römischen Autoren zitiert wird. Einzig in Plautus’ Komödie "Poenulus" kommen kurze punischsprachige Passagen vor.

Die Kenntnisse vom Karthago der punischen Epoche sind recht beschränkt, da durch die Zerstörung der Stadt im Dritten Punischen Krieg nur wenige Überreste aus dieser Zeit erhalten geblieben sind. Man geht davon aus, dass die Stadt eine Siedlungsfläche von mehr als 240 Hektar und eine Ausdehnung von 800 m entlang der Küstenlinie hatte. Die Einwohnerzahl Karthagos wird auf 400.000 geschätzt. Das Straßennetz von Karthago war rechtwinklig angelegt. Die Stadt war sowohl zum Land als zur See hin von Befestigungsanlagen geschützt. Die 13 m hohe und 40 km lange Mauer umschloss ein größeres Areal, zu dem auch die landwirtschaftliche Nutzfläche von Megara gehörte. Bei Ausgrabungen konnten Fundamente einer Seemauer freigelegt werden.

Das Zentrum des punischen Karthago war der Byrsa-Hügel, die Akropolis von Karthago. Man geht davon aus, dass sich hier eine Zitadelle und ein großer Tempel des Eschmun befand. Da der Hügel aber durch die römische Bautätigkeit eingeebnet wurde, lassen sich diese nicht nachweisen. An der Südostflanke des Hügels wurde ein punisches Wohnviertel ausgegraben. Die aus Lehmziegeln auf Steinfundamenten errichteten Häuser waren mehrstöckig (der Historiker Appianus berichtet von sechsstöckigen Häusern) und waren um einen Innenhof angelegt. Sie verfügten über Mosaikfußböden, Badebecken und unterirdische Zisternen zum Auffangen von Regenwasser. Ein weiteres erhaltenes punisches Wohnviertel ist das aus dem 3. Jahrhundert v. Chr. stammende sogenannte Magonidenviertel ("Quartier Magon") nahe dem Ufer.

An der Küste befand sich die Hafenanlage, die aus einem Handels- und einem Kriegshafen bestand. Der Handelshafen war ein 456 m × 356 m großes rechteckiges Becken, das durch einen Kanal mit dem offenen Meer verbunden war. Ein zweiter Kanal verband den Handelshafen mit dem Kriegshafen oder Kothon, einem runden Becken mit einem Durchmesser von 325 m. Er bot Platz für 220 Kriegsschiffe. In der Mitte des Kriegshafens befand sich eine künstliche Insel mit dem Gebäude der Admiralität. Die Hafenbecken sind bis heute erhalten.

Antike Autoren wie Appianus erwähnen einen unweit vom Hafen gelegenen zentralen Platz (Agora) und öffentliche Gebäude, die jedoch nicht archäologisch nachgewiesen sind.

Der heiligste Ort des punischen Karthago war der Tophet, eine Begräbnis- und Kultstätte und die Stelle, wo der Sage nach Elissa gelandet sein soll. Bei Ausgrabungen legte man zwölf Gräberschichten frei, die aus dem 8. Jahrhundert v. Chr. bis in die frühchristliche Zeit reichen, und fand über 1500 beschriftete und mit religiösen Symbolen verzierte Stelen. Auf dem Tophet wurde zunächst Baal-Hammon, später die Stadtgöttin Tanit verehrt.

Die Topographie des rund ein Jahrhundert nach der Zerstörung der punischen Metropole wieder aufgebauten römischen Karthagos ähnelt der aus der punischen Zeit. Forum und Kapitol lagen auf dem Byrsa-Hügel. Die Stadt wurde um Villenviertel und einen neuen Hafen erweitert.

Eines der prächtigsten römischen Bauwerke in Karthago waren die im Jahr 162 fertiggestellten Antoninus-Pius-Thermen. Das Badehaus lag direkt am Meer und war mit einer Ausdehnung von ca. 200 m die größte Thermenanlage außerhalb Roms. Das Gebäude überstand die arabische Eroberung und wurde erst im 11. Jahrhundert beim Einfall des Nomadenstammes der Banū Hilāl zerstört. Heute gehören die Ruinen zu den wichtigsten Sehenswürdigkeiten Karthagos.

In der Umgebung der Antoninus-Pius-Thermen lag das Magonviertel, in dem vom Deutschen Archäologischen Institut gegraben wurde. In der Nähe befanden sich das römische Theater und die Gallienus-Thermen, in denen 411 das Religionsgespräch von Karthago abgehalten wurde. Landeinwärts befinden sich das heute nur spärlich erhaltene Amphitheater von Karthago, das einst 50.000 Zuschauern Platz bot, und die Zisternen von La Malga, die die Trinkwasserversorgung der Stadt sicherten. Im nördlichen Teil von Karthago stand eine neunschiffige frühchristliche Basilika aus dem 5. Jahrhundert.

Heute ist Karthago ein nobler Villenvorort von Tunis, Standort der größten Universität des Landes und Standort des tunesischen Präsidentenpalastes.

Auf dem Byrsa-Hügel thront die Kathedrale des Heiligen Ludwig. Die Kathedrale wurde 1890 von den französischen Kolonialherren an der Stelle errichtet, die als Ort des Grabes von Ludwig IX. angenommen wurde. Dieser starb 1270 in Karthago im Laufe des siebten Kreuzzuges. Bis 1965 war die größte Kirche Nordafrikas Sitz des Erzbischofs von Karthago, heute dient sie als Kulturzentrum. In dem ehemaligen Kloster neben der Kathedrale befindet sich heute das archäologische Nationalmuseum.

Die elektrische Schnellbahn TGM verbindet Karthago mit der Innenstadt von Tunis. Westlich von Karthago, am Nordufer des Sees von Tunis, liegt der Flughafen Tunis-Carthage. Nördlich von Karthago reihen sich die Vororte Sidi Bou Saïd, La Marsa und Gammarth an der Mittelmeerküste.

Die Ausgrabungen von Karthago gehören zu den bedeutendsten touristischen Attraktionen Tunesiens. Die meisten Reiseveranstalter bieten Tagesausflüge von den Badeorten an der Mittelmeerküste nach Tunis, Karthago und Sidi Bou Said an.




</doc>
<doc id="2835" url="https://de.wikipedia.org/wiki?curid=2835" title="Klingonische Sprache">
Klingonische Sprache

Klingonisch (Eigenbezeichnung: tlhIngan Hol []) ist eine konstruierte Sprache, die 1984 von Marc Okrand im Auftrag der Filmgesellschaft Paramount für die Klingonen, eine außerirdische Spezies im Star-Trek-Universum, geschaffen wurde. Fremde Völker in Science-Fiction-Filmen sprachen zumeist ein sinnloses Kauderwelsch, doch die Produzenten von Star Trek wollten eine Sprache mit realistischem Hintergrund verwenden, damit die Verwendung in den verschiedenen Filmen untereinander stimmig ist. Fans der Serie, aber auch Sprachforscher, griffen die Sprache auf und begannen, sie zu lernen und aktiv zu sprechen. Das "Klingon Language Institute" (KLI) beschäftigt sich mit der Erhaltung, dem Schutz und der Verbreitung der Sprache. Als Standardwerk und Grundlage der Grammatik gilt das von Okrand verfasste Wörterbuch The Klingon Dictionary.

Die ersten klingonischen Begriffe wurden 1979 für den ersten "" von James Doohan, dem Darsteller von Scotty, vorgeschlagen. Als für den Film "" weitere klingonische Dialoge benötigt wurden, wurde Marc Okrand beauftragt, die Sprache auszuarbeiten. Dabei ging er von den ersten Begriffen aus dem Jahr 1979 aus. Um der Sprache einen möglichst fremden Klang zu geben, schmückte Okrand sie mit vielen Zungenbrechern. Da es sich bei den Klingonen um ein kriegerisches Volk handelt, ist auch ihr Sprachgebrauch entsprechend schroff. Zur Begrüßung sagen sie (wenn überhaupt irgendetwas) "nuqneH", was wörtlich ‚Was willst du?‘ heißt. Der einzig bekannte Ausdruck, um sich zu verabschieden, ist "Qapla’", was so viel wie ‚Erfolg‘ bedeutet. Marc Okrand steht in regem Kontakt mit dem KLI und dessen Mitgliedern. Er gibt ihnen in unregelmäßigen Abständen neue Vokabeln bekannt.

CBS bzw. dessen Tochterunternehmen Simon & Schuster halten als Verlag das Urheberrecht an den offiziellen Wörterbüchern und der kanonischen Sprachbeschreibung. Ihr Anspruch auf die Sprache selbst wird angezweifelt, war aber bislang nicht Gegenstand von gerichtlichen Auseinandersetzungen.

Der bei der IANA registrierte Sprachcode für die klingonische Sprache ist codice_1, der ISO-639-2-Code ist codice_2; Klingonisch ist also von der IANA bzw. ISO als Sprache anerkannt.

Auf der klingonischen Heimatwelt, dem Planeten Qo’noS (Kronos), existierten bis zur Gründung des Imperiums durch den mythischen Gründer Kahless verschiedene Volksgruppen und unterschiedliche Sprachen. Im Zuge der Einigung erwuchs aus der Notwendigkeit, sich zu verständigen, das "tlhIngan Hol". Altertümliche Formen der klingonischen Sprache werden als "no' Hol" (Sprache der Vorfahren) in der zeremoniellen Sprache, in Liedern und klassischen Geschichten, in besonderem Maße aber in der klingonischen Oper bewahrt. In einem solchen Fall müssen die Passagen in "no' Hol" von den Zelebranten und Darstellern auswendig gelernt werden, da sie sonst von heute lebenden Klingonen nicht mehr oder nur in einem falschen Sinne verstanden würden.

Nach dem Zwischenfall auf dem Planeten Genesis (siehe "") wurde einer der Klingonen namens Maltz durch Captain Kirk gefangen genommen. Dieser Klingone lebt in Gefangenschaft und verrät der Föderation regelmäßig neue klingonische Wörter. Seine Zurückhaltung ist der Grund, warum viele Wörter noch unbekannt sind.

Da die Wörter ursprünglich nur für die Verwendung in den Filmen erschaffen wurden, gibt es viele Begriffe, die im alltäglichen Leben eher selten Anwendung finden, z. B. für Phaser, Raumschiff oder Planet. Aus demselben Grund herrscht ein großes Defizit bei alltäglichen Begriffen, die im Film nicht gebraucht wurden, wie Kühlschrank, Schlüssel oder Windeln.

Insgesamt enthält die klingonische Sprache 90 Vor- und Nachsilben sowie ca. 1960 Wortstämme. Darin enthalten sind 60 Eigennamen, 25 Planeten, 40 fiktive Tiernamen, knapp 100 Star-Trek-spezifische Begriffe und 42 transkribierte irdische Speise- und Ländernamen. Somit existieren im Grunde lediglich knapp 1700 Grundwörter, die jedoch durch die Möglichkeit der Zusammensetzung und auch der Verwendung von Nachsilben zu insgesamt zirka 3000 möglichen Begriffen definiert werden. 1985 wurden im ersten Wörterbuch etwa 1500, in der erweiterten Neuauflage 1992 zusätzlich 300, 1997 in der Fortsetzung "Klingonisch für Fortgeschrittene" 800 und in den darauffolgenden Jahren in diversen Quellen nochmals ungefähr 400 Wörter bekannt gegeben. Dies beinhaltet nur die als Canon anerkannten Quellen, die direkt von Marc Okrand stammen. Das Ableiten von neuen Bedeutungen lässt sich beispielsweise am Grundwort "chen" „entstehen“ verdeutlichen: "chenmoH" „bilden“ → "chenmoHwI’" „Erschaffer“ → "Hew chenmoHwI’" „Bildhauer“. Eigenkreationen sind hier bis zu einem gewissen Grad auch ohne kanonische Verwendung zulässig und von der Allgemeinheit akzeptiert, solange die Bedeutung klar ist.

Die Wörter basieren auf keiner irdischen Sprache, jedoch hat sich der Autor verschiedener Wortspiele bedient, um ein klingonisches Wort zu erfinden. So hat das Wort für Nachbar "jIl" den Hintergrund, dass Okrands Nachbar "Jill" heißt. Das Wort für Fisch heißt "ghotI’", in Anlehnung an das Wortspiel Ghoti. Das Wort für Stiefel – auf Englisch "boot" – ist "DaS", so wie im Filmtitel "Das Boot" zu lesen.

Da diese Sprache für eine außerirdische Rasse entwickelt wurde, hat der Linguist Marc Okrand sich darum bemüht, sie mit einem besonders exotischen Charakter zu versehen. Dafür wurden seltenere sprachliche Eigenschaften verwendet, wie z. B. die im Englischen ungewöhnliche Satzstellung Objekt – Prädikat – Subjekt und die Verwendung von Vor- und Nachsilben (siehe agglutinierende Sprachen).

Im Klingonischen gibt es nur drei Wortarten: Verb, Substantiv, und eine für alles andere. Adjektive fallen unter Verben und werden als ‚… sein‘ übersetzt, wie z. B. "tIn" ‚groß sein‘.

Der Aufbau des Klingonischen ist sehr stark mit einem Baukastensystem zu vergleichen. Sätze werden aus vielen Einzelteilen in einer (fast immer) unveränderbaren vorgegebenen Reihenfolge zusammengesetzt. Das Klingonische kennt weder Konjugation noch Deklination. Es gibt auch keine verschiedenen Zeitformen und keine Artikel.

Um die Lautschrift des IPA-Systems zu vermeiden und die Darstellung mit üblichen Mitteln zu erleichtern, wird die klingonische Sprache mit Hilfe lateinischer Buchstaben dargestellt. Diese repräsentieren eine speziell für die klingonische Sprache entwickelte Lautschrift, was erklärt, warum manche der Buchstaben mitten im Wort als Großbuchstabe geschrieben werden.

Es gab nie eine offizielle klingonische Schrift, damit Unstimmigkeiten im Filmset vermieden werden können. In "The Klingon Dictionary" heißt es, dass über die klingonische Schrift sehr wenig bekannt sei, außer dass sie "pIqaD" genannt wird. Sehr bekannt ist die sogenannte Mandel-Schrift, die 1980 im Buch "The U.S.S. Enterprise Officer’s Manual" vorgestellt wurde. Diese wurde aber dem englischen Alphabet zugeordnet und ist daher nicht kompatibel mit Okrands Lautschrift.

Das KLI hat seit seiner Gründung 1992 in seinem vierteljährlichen Journal eine klingonische Schrift verwendet, die auf den in den Filmen sichtbaren Zeichen basiert, jedoch nicht identisch ist. Die exakte Quelle dieser Schriftzeichen ist nicht eindeutig bekannt, jedenfalls existierte sie schon vor der Gründung des KLI. Genau diese Schrift wurde 2011 in der Veröffentlichung des Klingonischen Monopoly, in einer klingonischen Sprachlern-Software und in einem 2012 veröffentlichten Buch über den klingonischen "Bird of Prey" (ein Raumschiff-Typ) verwendet. Daher kann dieser Schriftsatz inzwischen als offiziell betrachtet werden, auch wenn er in den Filmen nicht verwendet wird.
Das Logo von Wikipedia enthielt ursprünglich in der oberen Ecke den klingonischen Buchstaben "r". Dieser wurde nach dem Entfernen der klingonischen Wikipedia 2010 durch ein Zeichen aus dem Altäthiopischen ersetzt.

Die während der Filme schlecht ausgesprochenen Szenen und unbeabsichtigten grammatikalischen Fehler werden nachträglich im Buch "Klingonisch für Fortgeschrittene" folgendermaßen als Dialekte und Umgangssprache erklärt: Als Standard-Klingonisch gilt der Dialekt des jeweiligen Herrschers, der als "ta’ tlhIngan Hol" (kurz: "ta’ Hol") bezeichnet wird (wörtlich: ‚Klingonisch des Imperators‘). Da häufige Machtwechsel für die klingonische Kultur charakteristisch sind, wechselt der Dialekt des Herrschers entsprechend oft. Für sprachwissenschaftlich vergleichende Zwecke findet der klingonische Dialekt, welcher in der Hauptstadt von Qo’noS gesprochen wird, als Standard Verwendung.

Die klingonische Sprache ist eine der wenigen fiktionalen Sprachen, die in diversen Normen gefestigt wurden und damit in gewisser Hinsicht einen offiziellen Status erhalten haben: 1999 wurde die Sprache in der Liste der IANA mit dem Kürzel i-klingon eingetragen. Diese Verwendung wurde im Februar 2004 abgelöst durch die Eintragung des Kürzels tlh in die ISO 639-2 und 639-3, die durch den Library of Congress verwaltet wird.
Derselbe Code wird auch in der "MARC Code List for Languages" verwendet.

Es gibt nur wenige Sprachen, die wie im Deutschen ein eigenes Wort für die klingonische Sprache haben, in diesem Fall eben "Klingonisch". Dies liegt meist daran, dass viele Länder die Sendungen im Originalton senden, und sogar dann auch in den Untertiteln von einem "Klingon-Schiff" oder einem "Romulan-Captain" reden. Manche Sprachen haben dennoch eine Anpassung gemacht, indem die landestypische Sprachbezeichnung an den Stamm "Klingon" gehängt wird, so wie das deutsche "Klingonisch" oder das tschechische "Klingonština". Die deutsche Version wurde spätestens mit der Veröffentlichung des Werks "Das Offizielle Wörterbuch Klingonisch-Deutsch" offiziell.

In den meisten Sprachen ist es einfach nur "Klingon", wenige sagen "Klingon-Sprache," um die Sprache von den Klingonen zu unterscheiden.

Sowohl der Vorsitzende des KLI, Lawrence M. Schoen, als auch die Sprachwissenschaftlerin Arika Okrent schätzen, dass es etwa 20 bis 30 Menschen gibt, die Klingonisch fließend sprechen. Es gebe wohl mehr als zweitausend Leute, die zumindest Grundkenntnisse in Klingonisch haben. Das größte Problem bei einer Schätzung ist die Definition, ab wann jemand eine Sprache fließend sprechen kann.

Klingonisch wurde ursprünglich nur für die Kinofilme entwickelt und dort verwendet. Eine alltägliche Konversation auf Klingonisch stößt schnell an die Grenzen des beschränkten Vokabulars von knapp 3.000 Begriffen. Fans verwenden die Sprache gerne im Rahmen des Rollenspiels (siehe Cosplay) und auf Conventions, um ihren Charakteren mehr Authentizität einzuhauchen. In Sprachkursen wird Klingonisch geübt und gesprochen (→ qepHom), aber auch Sprachforscher und Studenten befassen sich mit dem Thema und erstellen z. B. Diplomarbeiten über das Thema Klingonisch oder dessen Nutzer. Es gibt regelmäßig Erwähnungen und kleine Verwendungen in diversen Medien.

Klingonisch wurde in folgenden Kinofilmen gesprochen:




Das eingeschränkte Vokabular der Sprache lässt Übersetzungen literarischer Werke nur eingeschränkt zu. So könnte zum Beispiel eine Übersetzung von Lewis Carrolls "Alice in Wonderland" in das Klingonische nicht erstellt werden, da zahlreiche dafür notwendige Wörter nicht verfügbar sind.
Ebenso ist das häufig fälschlicherweise als abgeschlossen beschriebene Bibel-Übersetzungsprojekt an dieser Hürde gescheitert. Auch andere Projekte erfordern nicht selten Umformulierungen bei der Übersetzung. So wurde zum Beispiel das Wort „Sonne“ im klingonischen Hamlet als „Tages-Stern“ "(pemHov)" umschrieben, weil zu dem Zeitpunkt noch kein Wort dafür bekannt war. Gelegentlich werden für neue Projekte neue Vokabeln durch Okrand bereitgestellt.







Englische Bücher zur Sprache:

Deutsche Bücher zur Sprache:

Übersetzungen der vorgenannten Bücher:

Klingonische Übersetzungen:

Bibliotheksbestände:


Sprachkurs

Das offizielle Klingonisch-Institut (KLI)

Internet-Seiten auf Klingonisch


</doc>
<doc id="2837" url="https://de.wikipedia.org/wiki?curid=2837" title="Kohlenhydrate">
Kohlenhydrate

Kohlenhydrate oder Saccharide bilden eine biologisch und chemisch bedeutsame Stoffklasse. Als Produkt der Photosynthese machen Kohlenhydrate den größten Teil der Biomasse aus. Mono-, Di- und Polysaccharide (u. a. Stärke und Cellulose) stellen zusammen mit den Fetten und Proteinen den quantitativ größten verwertbaren und nicht-verwertbaren (Ballaststoffe) Anteil an der Nahrung. Neben ihrer zentralen Rolle als physiologischer Energieträger spielen sie als Stützsubstanz vor allem im Pflanzenreich und in biologischen Signal- und Erkennungsprozessen (z. B. Zell-Zell-Erkennung, Blutgruppen) eine wichtige Rolle. Die Wissenschaft, die sich mit der Biologie der Kohlenhydrate beschäftigt, heißt Glycobiologie.

Die Monosaccharide (Einfachzucker, z. B. Traubenzucker, Fruchtzucker), Disaccharide (Zweifachzucker, z. B. Kristallzucker, Milchzucker, Malzzucker) und Oligosaccharide (z. B. Raffinose) sind wasserlöslich, haben einen süßen Geschmack und werden im engeren Sinne als Zucker bezeichnet. Die Polysaccharide (Vielfachzucker, z. B. Stärke, Cellulose, Chitin) sind hingegen oftmals schlecht oder gar nicht in Wasser löslich und geschmacksneutral. Alle Kohlenhydrate, die keine Monosaccharide sind, werden auch als Mehrfachzucker bezeichnet.

Da viele Saccharide die Bruttoformel C(HO) aufweisen, wurde fälschlicherweise angenommen, dass es sich um Hydrate des Kohlenstoffs handle, weshalb Carl Schmidt 1844 den Begriff Kohlehydrate prägte, der bis heute als Kohlenhydrate in abgewandelter Form verwendet wird. Vertreter dieser Stoffklasse können jedoch erheblich von dieser Bruttoformel abweichen und weitere funktionelle Gruppen und Heteroatome wie Stickstoff oder Schwefel enthalten, während andere Verbindungen derselben Formel nicht zu den Kohlenhydraten gehören, da sie keine Hydroxyaldehyde oder Hydroxyketone sind. Allgemein liegen Kohlenhydrate vor, wenn in einem Stoff mindestens eine Aldehydgruppe bzw. Ketogruppe und mindestens zwei Hydroxygruppen anzufinden sind. Für unverzweigte Polysaccharide, die aus demselben Monosaccharid mit der Summenformel CHO (Glucose, Fructose, Galactose etc.) aufgebaut sind, gilt die Formel:

Diese Polysaccharide – ein Beispiel ist die Amylose – zählen zu den Homopolymeren.

Bereits im Jahr 1811 machte Constantin Kirchhoff die Entdeckung, dass sich beim Kochen von Stärkemehl mit Säure Traubenzucker bildet. Auf Anregung von Johann Wolfgang Döbereiner wurde 1812 während der Kontinentalsperre eine Stärkezuckerfabrik errichtet. Henri Braconnot entdeckte 1819, dass durch Einwirkung von konzentrierter Schwefelsäure auf Cellulose Zucker entsteht. William Prout gab nach chemischen Analysen des Zuckers, der Stärke durch Joseph Louis Gay-Lussac und Thénard dieser Stoffgruppe den Gruppennamen "Sacharine".

Einfachzucker werden von Pflanzen im Calvin-Zyklus durch Photosynthese aus Kohlenstoffdioxid und Wasser aufgebaut, und enthalten Kohlenstoff, Wasserstoff und Sauerstoff. Zur Speicherung oder zum Zellaufbau werden diese Einfachzucker bei praktisch allen Lebewesen zu Mehrfachzuckern verkettet. Pflanzen synthetisieren in den Plastiden (z. B. Chloroplasten) das Polysaccharid Stärke. Tiere bilden in der Leber aus Glucose den langkettigen Speicherzucker Glycogen.

Die Energieversorgung des Gehirns ist hochgradig von Glucose abhängig, da es Fette nicht direkt energetisch verwenden kann. In Hungersituationen ohne Kohlenhydratzufuhr oder bei verstärkter Muskelarbeit wird daher unter Energieaufwand Glucose in der Gluconeogenese aus den Stoffwechselprodukten Lactat, bestimmten Aminosäuren (u. a. Alanin) und Glycerin synthetisiert. Die Gluconeogenese verwendet zwar einige Enzyme der Glycolyse, dem Abbauweg der Glucose zur Erzeugung von energiereichem ATP und NADH+H, ist aber keinesfalls als deren Umkehrung zu verstehen, da entscheidende Schritte von eigenen Enzymen wie gesagt unter Energieverbrauch stattfinden. Glycolyse und Gluconeogenese sind reziprok reguliert, d. h., sie schließen einander in ein und derselben Zelle "nahezu" aus. Unterschiedliche Organe können jedoch sehr wohl gleichzeitig den einen und den anderen Weg beschreiten. So findet bei starker Muskelaktivität im Muskel Glycolyse und damit Lactatfreisetzung und in der Leber Gluconeogenese unter Verwendung von Lactat statt. Dadurch wird ein Teil der Stoffwechsellast in die Leber verlagert.

Kohlenhydrate sind neben Fett und Eiweiß ein wesentlicher Bestandteil der menschlichen Nahrung. Wichtige Grundnahrungsmittel, die einen hohen Anteil an Kohlenhydraten aufweisen, sind die verschiedenen Getreidesorten, die zu Lebensmitteln verarbeitet werden (Reis, Weizen, Mais, Hirse, Roggen, Hafer) bzw. als Viehfutter genutzt werden (vor allem Gerste, Hafer, Mais, Triticale). Die stärkehaltigen Getreideprodukte sind u. a. Brot, Nudeln, Kuchen u. v. a. m. Die Wurzelknollen der Kartoffel, eines Nachtschattengewächses, und die zu den Hülsenfrüchten gehörenden Erbsen, Bohnen und Linsen weisen einen hohen Kohlenhydratanteil auf.

Pflanzenarten, die vor allem zur Kohlenhydrataufnahme in der Ernährung beitragen, sind in der Liste der Nutzpflanzen zusammengestellt.

Die in der Pflanzenwelt als Stützsubstanz in großen Mengen vorkommende Cellulose ist für den Menschen unverdaulich. Sie ist aber von den Wiederkäuern wie Rindern, Schafen und Ziegen verwertbar, da diese sich in ihren Vormägen (Pansen) den mikrobiellen Aufschluss zu Nutze machen.

Die unmittelbare Energiewährung für biologische Prozesse ist das Adenosintriphosphat (ATP), das zum Beispiel die Muskelkontraktion antreibt und an fast allen energieverbrauchenden Prozessen beteiligt ist. Es liegt jedoch in den Zellen nur in geringer Konzentration vor und muss durch aeroben und anaeroben Abbau energiereicher Verbindungen wie Fette, Kohlenhydrate oder Proteine in den Zellen nachgeliefert werden.

Kohlenhydrate sind der Hauptenergielieferant für den Organismus. Sie sind im Gegensatz zu den Fetten relativ schnell verwertbar, da sie anaerob Energie liefern. Ein wichtiger Kohlenhydratbaustein im Energiehaushalt des Körpers ist die Glucose. Jede Körperzelle kann Glucose durch die Zellmembran aufnehmen bzw. wieder abgeben. In den Zellen der verschiedenen Organe kann sie entweder durch Verstoffwechselung die chemische Energie für Muskelarbeit, anabole Prozesse oder Gehirnaktivität liefern oder in Form von Glucoseketten als Glycogen gespeichert werden.

Die akute Energieversorgung des Körpers wird im Wesentlichen über die im Blut gelöste Glucose gewährleistet. Ihre Konzentration im Blut, der sogenannte Blutzuckerspiegel, wird in engen Grenzen gehalten. Bei der Verdauung wird die Glucose im Dünndarm als Monosaccharid aus dem Nahrungsbrei aufgenommen und in das Blut abgegeben. Nach der Nahrungsaufnahme steigt der Blutzuckerspiegel daher an. Die ins Blut aufgenommene Glucose muss also erst einmal zwischengespeichert werden. Das Signal hierzu gibt das Insulin, ein Peptidhormon. Es signalisiert dem Muskel- und Lebergewebe, verstärkt Glucose aus dem Blut aufzunehmen und zu Glycogen zu verketten. Bei der Aufnahme (Resorption) der Glucose aus der Nahrung ist es nicht egal, wie schnell das geschieht. Da die Glucose in der Nahrung in mehr oder weniger oligomerisierter, bzw. polymerisierter, genauer: polykondensierter Form vorliegt, müssen die Glucoseketten im Verdauungstrakt aufgespalten werden, was je nach Länge der Ketten unterschiedlich schnell geschieht. Werden z. B. stärkehaltige Nahrungsmittel wie Brot oder Kartoffeln gegessen, so zerlegen die Verdauungsenzyme die Glucosekette der Stärke in einzelne Bruchstücke und schließlich bis zu den einzelnen Glucose-Molekülen, die nach und nach in den Blutkreislauf übergehen. Ein Maß für die Geschwindigkeit, mit der die Stärke zerlegt und die Glucose-Bausteine aufgenommen werden, ist der Glykämische Index (GI).

Auch die anderen Nahrungsbestandteile spielen für die Geschwindigkeit des Blutzuckeranstiegs eine Rolle. Dies gibt dem Körper Zeit, die energiereichen Moleküle einzulagern. Je schneller die Glucose im Verdauungstrakt freigesetzt wird, desto schneller steigt der Blutzuckerspiegel an. Wenn sie in Form von Zucker, also direkt als Glucose oder kurzkettigen Di-, Tri- und Oligomeren, aufgenommen wird, so steigt der Blutzuckerspiegel schnell an, welches der Grund für die belebende Wirkung zuckerhaltiger Süßigkeiten und Getränke ist. Bei körperlicher oder geistiger Aktivität wird der Körper so mit schneller Energie versorgt. Wird jedoch „schwer gegessen“, also eine umfangreiche Mahlzeit eingenommen, so wird durch die intensive Durchblutung des Verdauungstraktes dem Muskelgewebe und dem Gehirn Energie entzogen, so dass man eher träge wird. Hierbei spielt das Insulin eine Rolle, da bereits während der Mahlzeit Insulin ausgeschüttet wird, welches den Blutzucker senkt. Der Blutzuckerspiegel steigt also während der Mahlzeit zunächst an, sinkt aufgrund der Einlagerung der Glucose in Muskeln und Leber ab und erreicht wieder einen normalen Wert.

Ein niedriger Blutzuckerspiegel spielt beim Hungergefühl eine entscheidende Rolle. Daher kann es sein, dass man ca. eine halbe bis eine ganze Stunde nach einer Mahlzeit wieder ein leichtes Hungergefühl hat. Es ist daher sinnvoll, das Dessert oder den Kaffee an die Mahlzeit anzuschließen, da Ersteres durch den Zuckergehalt und Letzterer durch die den Blutzuckerspiegel steigernde Wirkung des Koffeins dieses Absinken ausgleicht.

Wenn die Versorgung der Gewebe mit Kohlenhydraten größer ist als ihr Verbrauch, wird der Überschuss in Fett umgewandelt und als Depotfett gespeichert. Fette haben eine höheren physiologischen Brennwert als Kohlenhydrate und haben keine Hydrathülle. Sie sind für die langfristige Energiespeicherung also platzsparender als Kohlenhydrate und bewirken eine bessere Wärmedämmung des Körpers.

Entgegen der landläufigen Meinung werden Fettdepots ständig energetisch verwertet und nicht erst, wenn der Glycogenspeicher im Muskel reduziert ist. Dass der Abbau von Körperfett erst nach ca. 30-minütiger Ausdauerübung einsetzt, ist falsch. Tatsächlich verhält es sich so, dass das Adenosintriphosphat (ATP) für intensive Muskelarbeit durch vier Energiequellen geliefert wird. Daran den größten Anteil hat die Verwertung von Creatinphosphat (aus dem Proteinstoffwechsel), das ein höheres Phosphatgruppenübertragungspotential hat als ATP und dieses daher schnell nachliefern kann; es folgt die anaerobe Zuckerverwertung; etwas geringer ist der Anteil der oxidativen Zuckerverwertung; und den geringsten Anteil hat der Fettabbau. Je intensiver die Anstrengung ist, desto stärker nimmt der Anteil der ersten, insbesondere anaeroben Anteile zu. Folglich nimmt der relative Anteil des Fettabbaus bei erhöhter Pulsfrequenz ab, die absolute Menge des verwerteten Fettes nimmt aber sehr wohl zu, da der Gesamtenergieumsatz ebenfalls zunimmt. Der erhöhte Anteil des anaeroben Stoffwechsels hängt mit dem abnehmenden Sauerstoffangebot im Muskel bei starker Muskelarbeit zusammen, da der Fettabbau im Stoffwechsel ein aerober Prozess ist.

Untrainierten Sportlern wird oft geraten, ausdauernd und leistungsschwach anzufangen („Laufen, ohne zu schnaufen“). Es gibt jedoch die Ansicht, dass allein die Energiebilanz beim Sport entscheidend ist, da z. B. nach der Anstrengung noch ein weiterer Abbau der Fette stattfindet. Wird also während des Trainings viel Glucose umgesetzt, wird in der Erholungsphase umso mehr Fett abgebaut. Weitere Faktoren des Fettabbaus sind die Art und die Frequenz der Nahrungsaufnahme, da mit jedem Insulinausstoß der Fettabbau gehemmt wird. Mit zunehmendem Training vergrößert sich die Muskelmasse, die Sauerstoffaufnahme verbessert sich, wodurch ein erhöhter Fettabbau entsteht.
Leistungssportler trainieren vor einem Wettkampf durch den geeigneten Zeitpunkt der Nahrungsaufnahme die Optimierung ihres Glycogenspeichers, da dieser ein wichtiger Energiespeicher für kurzfristige Leistungsspitzen ist.

Kohlenhydrate gelten nicht als essentiell, da der Körper sie in der Gluconeogenese unter Energieaufwand aus anderen Nahrungsbestandteilen wie Proteinen und Glycerin selbst herstellen kann. Da insbesondere das Gehirn hochgradig von Glucose als Energieträger abhängig ist und keine Fette verwerten kann, muss der Blutzucker­spiegel in engen Grenzen gehalten werden. Dessen Regulation erfolgt durch das Zusammenspiel von Insulin und Glucagon. Bei Kohlenhydratmangel wird das Gehirn durch Ketonkörper versorgt, was sich z. B. bei einer Diät durch Aceton­geruch bemerkbar macht. Eine völlig kohlenhydratfreie Ernährung wurde im Tierversuch bei Hühnern problemlos vertragen. Auch eine Langzeitstudie an Kindern und jungen Erwachsenen mit der sehr kohlenhydratreduzierten ketogenen Diät zeigte gesundheitliche Unbedenklichkeit.
Eine eigenständige Erkrankung des Menschen durch das Fehlen von Kohlenhydraten ist unbekannt. Der Energiegehalt eines Gramms Kohlenhydrat beträgt rund 17,2 Kilojoule (4,1 Kilokalorien).

Die physiologische Energieerzeugung aus Kohlenhydraten erfolgt im Normalfall in der nicht-oxidativen Glycolyse und im oxidativen Citrat-Zyklus. Die Oxidationsschritte im Citrat-Zyklus bestehen in einer Abspaltung von Wasserstoff, der durch Wasserstoffüberträger in die Atmungskette eingespeist und dort mit Sauerstoff zu Wasser oxidiert wird. Das dabei an der Mitochondrienmembran erzeugte Membranpotential liefert mit Abstand die meiste Energie für die ATP-Synthese aus ADP.

Die Gärung ist ein Stoffwechselprozess, bei dem unter Sauerstoffabschluss (Anaerobie) Kohlenhydrate zum Energiegewinn abgebaut werden. Sie wird in der Natur vor allem von Mikroorganismen genutzt, jedoch können Pflanzen unter Sauerstoffmangel auf sie zurückgreifen. In den Muskeln findet unter Sauerstoffmangel Milchsäuregärung statt.

Gärungen werden vielfältig zur Herstellung und Veredelung von Lebensmitteln genutzt. So wird bei der Milchsäuregärung Milchzucker zu Milchsäure umgesetzt und zur Herstellung von Joghurt, Quark und Buttermilch genutzt. Die Herstellung von Sauerteig und Silage beruhen auf der Gärung von Kohlenhydraten zu Milchsäure. Bei der Käse-Herstellung ist die Milchsäuregärung ein wichtiger Zwischenschritt.

Bei der alkoholischen Gärung werden verschiedene Zuckerarten zu Alkohol vergoren. Zu nennen wäre hier u. a. Malzzucker beim Bierbrauen und Traubenzucker beim Keltern von Wein. Stärkehaltige Nahrungsmittel wie Kartoffeln, Getreide und Reis werden z. B. zu Schnäpsen, Früchte zu Obstwässern verarbeitet.

Im Vergleich zur Zellatmung wird bei Gärungen nur eine geringe Menge Energie gewonnen, da statt Citratzyklus und anschließender Atmungskette nur die Substratkettenphosphorylierung genutzt werden kann.

Nach ihrer Funktion im Organismus kann man die Kohlenhydrate in Strukturkohlenhydrate und Nicht-Strukturkohlenhydrate unterteilen:

Strukturkohlenhydrate können von Säugern mit einhöhligem Magen nur bedingt verdaut werden, hingegen weitgehend oder vollständig von Wiederkäuern (Ruminantia), Kamelartigen (Camelidae) (diese sind ebenfalls Wiederkäuer, allerdings nicht im systematischen Sinne, da sich bei ihnen das Wiederkäuen unabhängig entwickelt hat) und Pferdeartigen (Equidae).

Die Einteilung nach chemischer Struktur ist im Absatz "Liste wichtiger Kohlenhydrate" dargestellt.

Die folgende Tabelle zeigt einige Beispiele für die Vielfalt natürlicher Kohlenhydratstrukturen. Pentosen und Hexosen können im Prinzip sowohl Fünf- als auch Sechsringe bilden, wobei ein neues Chiralitätszentrum entsteht, so dass einschließlich der offenkettigen Form bereits für ein Monosaccharid fünf Isomere existieren. Durch glycosidische Bindungen können sich Monosaccharide zu Oligo- und Polysacchariden verbinden. Dadurch potenziert sich die Anzahl möglicher Kohlenhydratstrukturen theoretisch zu einer schier unendlichen Vielfalt. Die Natur beschränkt sich allerdings auf energetisch günstige Strukturen.

Das Beispiel der α--Glucopyranose zeigt verschiedene gleichwertige Darstellungsformen.

Kohlenhydrate sind Hydroxyaldehyde oder Hydroxyketone sowie davon abgeleitete Verbindungen, haben in ihrer offenkettigen Form also neben mindestens zwei Hydroxygruppen auch mindestens eine Aldehydgruppe oder Ketogruppe. Handelt es sich um ein Hydroxyaldehyd (Carbonylgruppe an einem terminalen C-Atom (Aldehyd)), so spricht man von einer Aldose, handelt es sich um ein Hydroxyketon (Carbonylgruppe an einem internen C-Atom (Ketone)), bezeichnet man den Zucker als Ketose. Die Carbonylfunktion ist eine hochreaktive funktionelle Gruppe: Zu nennen sind hier besonders die leichte Oxidierbarkeit zur Carbonsäure, die Reduktion zum Alkohol und der leichte nukleophile Angriff am Kohlenstoffatom der Carbonylgruppe.
Ein genereller Nachweis von Kohlenhydraten kann durch die Molisch-Probe, die Barfoedsche Probe und die PAS-Reaktion erfolgen. Die Unterscheidung von Monosacchariden von Di-, Oligo- oder Polysacchariden ist durch die Barfoed'sche Probe möglich. Aldosen und Ketosen können durch die Seliwanow-Probe mit Resorcin unterschieden werden. Reduzierende Zucker können mit der Fehling-Probe nachgewiesen werden, bei der sich bei Anwesenheit von Aldehyden und reduzierenden Zuckern (Aldosen und Acyloine) rot-braunes Kupfer(I)-oxid bildet. Neben der Fehling-Probe können reduzierende Zucker auch mit Hilfe des Benedict-Reagenz (durch die Farbe des ausfallenden Produkts), mit Nylanders Reagenz, mit 3,5-Dinitrosalicylsäure oder aufgrund der Entfärbung einer Kaliumpermanganat-Lösung nachgewiesen werden. Die Unterscheidung von Pentosen und Hexosen kann durch die "Mejbaum-Probe" mit Orcin oder durch die Bial-Probe (ebenfalls mit Orcin) erfolgen. Durch die Dische-Probe kann mit Diphenylamin Desoxyribose nachgewiesen werden.

Neuere analytische Methoden zum qualitativen und quantitativen Nachweis einzelner Kohlenhydrate in unterschiedlichen Untersuchungsmaterialien verwenden nach hinreichender Probenvorbereitung und gegebenenfalls Derivatisierung, chromatographische Trennverfahren in Kopplung mit der Massenspektrometrie. Für spezielle Kohlenhydrate werden auch ausgewählte TMS-Derivate eingesetzt.

Durch Oxidationsmittel werden Kohlenhydrate zu Aldonsäuren oxidiert. Dies gilt unter basischen Bedingungen nicht nur für die Aldosen, sondern auch für die Ketosen, die durch die Base in einer komplexen Reaktion umgelagert werden (dabei wird die im Zuge der Keto-Enol-Tautomerie auftretende Aldose-Form stabilisiert). Beim Nachweis durch das Fehling-Reagenz wird ein blauer Cu(II)-Weinsäure-Komplex zu unlöslichem, roten Cu(I)oxid reduziert.

Wird die Carbonylfunktion zur Hydroxygruppe reduziert, erhält man ein sogenanntes Alditol.

Durch intramolekularen nukleophilen Angriff einer der Hydroxygruppen auf das Carbonylkohlenstoffatom bildet sich ein zyklisches Halbacetal, welches energetisch meist sehr günstig ist. Hierbei werden überwiegend Sechsringe (pyranose Form) gebildet, die eine sehr niedrige Ringspannung aufweisen, es entstehen aber auch in geringerem Maße Fünfringe (furanose Form). Andere Ringgrößen treten nicht auf, da sie eine zu hohe Ringspannung aufweisen. Es entsteht ferner ein neues Chiralitätszentrum. Die beiden resultierenden Diastereomere werden mit α und β bezeichnet. In wässriger Lösung bilden α- und β-pyranose und -furanose Form eine Gleichgewichtsreaktion miteinander und mit der offenkettigen Form. Eine wässrige Lösung von reiner α-Glucopyranose wird daher nach einiger Zeit zu einer Gleichgewichtsmischung aus α- und β-Glucopyranose und -furanose (38 % α-Glcp, 62 % β-Glcp, 0 % α-Glcf, 0,14 % β-Glcf, 0,002 % offenkettig). Die hierbei messbare Veränderung des Drehwertes wird als Mutarotation bezeichnet.
Während aliphatische Aldehyde bereits von Luftsauerstoff allmählich zur Carbonsäure oxidiert werden, sind Kohlenhydrate durch die Acetalbildung erheblich unempfindlicher, was zweifelsohne für eine so wichtige Biomolekülklasse von enormer Bedeutung ist.

Von zentraler Bedeutung in der Kohlenhydratchemie und Biochemie ist ferner die glycosidische Bindung. Das hierbei gebildete zyklische Vollacetal eines Zuckers bezeichnet man als Glycosid.

Mit Aminen (z. B. in Aminosäuren, Proteinen) reagiert der offenkettige Aldehyd des Kohlenhydrates über ein Imin reversibel zu Amadori-Produkten, welches wiederum ebenfalls mit Aminen oder Aminosäuren kondensieren kann und sich irreversibel umlagert:

Diese nichtenzymatische Reaktion erfolgt im Organismus mit Aminosäuren und Eiweißen relativ häufig und ist einer der zentralen Vorgänge beim Altern (z. B. Altersflecken), da die Reaktionsprodukte vom Körper nicht abgebaut werden können. Ferner spielt sie eine wichtige Rolle bei der thermischen Zubereitung von Lebensmitteln, z. B. beim Braten und Kochen. Es kommt zu der typischen Bräunung, da sich konjugierte Ringsysteme bilden, die farbig sind. Diese Produkte der sogenannten Maillard-Reaktion sind auch für den Geschmack zubereiteter Lebensmittel entscheidend.





</doc>
<doc id="2838" url="https://de.wikipedia.org/wiki?curid=2838" title="Kalender">
Kalender

Ein Kalender ist eine Übersicht über die Tage, Wochen und Monate eines Jahres. Eine veraltete Bezeichnung ist "Jahrweiser".

Das Wort „Kalender“ entstammt dem lateinischen "Calendarium" (Schuldbuch). Dies war ein Verzeichnis der Kalendae, der jeweils ersten, auszurufenden ("calare" „ausrufen“) Tage der antiken Monate. An diesen wurden Darlehen ausgezahlt und Darlehensrückführungen sowie Zins­forderungen fällig.

Es gibt verschiedene Kalendersysteme, heute ist weltweit überwiegend der gregorianische Kalender in Gebrauch.

Kalender gibt es in verschiedenen – gedruckten, bebilderten, elektronischen – Formen (siehe auch Kalendarium).

Die Regeln zur Aufstellung von Kalendern ergeben sich aus astronomischen Gegebenheiten (Mondphasen, Sonnenjahr) und entsprechenden Kalenderberechnungen. Die wissenschaftliche Kalenderkunde ist ein Teilgebiet der astronomischen Chronologie. Die vorwissenschaftliche Kunst, Kalender zu erstellen, nennt man Hemerologie.

Die Kenntnis regelmäßig stattfindender Tierwanderungen war bereits für die frühen Jägerkulturen wichtig.

Ein Bewusstsein für jahreszeitlich und astronomisch sich wiederholende Ereignisse, für entsprechende Zyklen seiner Umwelt, dürfte der Mensch schon sehr früh gehabt haben. Dazu gehörte der Wechsel von Tag und Nacht sowie die Mondphasen. Jahreszeitlich bedingte Klima­schwankungen spielten in der Landwirtschaft der meisten Weltregionen eine bedeutende Rolle und konnten vom Menschen spätestens in der Altsteinzeit wahrgenommen werden. Eine Beobachtung der Veränderungen des Nachthimmels sowie der Eigenbewegungen der Planeten war zu dieser Zeit ebenfalls möglich.

Schon der Turm von Jericho aus dem 9. Jahrtausend v. Chr. deutet auf die Kenntnis der Sommersonnenwende hin und jungsteinzeitliche Bauten wie etwa Stonehenge zeugen von den Bemühungen der sesshaft gewordenen Bevölkerung, die natürliche Jahreslänge und ausgewählte zyklisch wiederkehrende Himmelsereignisse wie Sonnenwende und Tag-und-Nacht-Gleiche exakt bestimmen zu können. Gerade für die Landwirtschaft war wichtig, eine von den konkreten Wetter­bedingungen unabhängige Bestimmung der Zeitpunkte für Aussaat und Ernte vornehmen zu können. Mit der systematischen Himmelsbeobachtung verbunden waren religiöse Fruchtbarkeitskulte – getragen von der Hoffnung auf eine günstige Wiederkehr der Fruchtbarkeitsbedingungen. So wurden bestimmte landwirtschaftliche Termine an Feste gebunden, die wiederum an Himmelsereignisse geknüpft waren.

Für den Übergang von Jägerkulturen zum Ackerbau im Neolithikum (Jungsteinzeit) wird eine Veränderung kalendarischer Vorstellungen vom Mond- zum Sonnenkalender angenommen. Dieser steinzeitliche Kalender, auch "neolithischer Kalender" (von Alexander Thom auch "megalithischer Kalender" genannt) beinhaltet wohl die ältesten kalendarischen Vorstellungen der Menschheit und ist die Grundlage späterer Kalendervarianten. Analog zum Begriff der Neolithischen Revolution (Übergang zum Ackerbau) wird auch von der "Neolithischen Kalender-Revolution" gesprochen.
Die ältesten heute noch bekannten Kalender stammen aus den frühen Hochkulturen Ägyptens und Mesopotamiens. Hier zeigten sich schon zwei grundlegende Kalendertypen, die bis heute die meisten Kalendersysteme prägen: der an den Mondphasen orientierte Mondkalender und der astronomische Kalender, der den Lauf der Himmelskörper widerspiegelt.

Spätestens von den Babyloniern wurde der siebentägige Wochen­zyklus entwickelt, der heute fast weltweit den Ablauf des Alltags regelt. In anderen Kalendern gab es ähnliche Zyklen, zwischen fünf und zehn Tagen.

Die Anpassung von Wochen und Monatsfolgen an die feste Größe des astronomischen Jahres war nicht einfach zu lösen. Es kam zur Herausbildung verschiedener Kalendersysteme.

Frühe Kalendersysteme wurden durch Beobachtung gewonnen ("astronomische Kalender"). Mit dem Eintritt eines bestimmten definierten Himmelsereignisses (z. B. des Neumonds oder der Tag-und-Nacht-Gleiche im Frühling) begann ein neuer Zyklus. Sie mussten regelmäßig nachgeregelt werden.

Diese Methode hatte einen entscheidenden Nachteil: In großen Herrschaftsräumen konnte ein Ereignis an unterschiedlichen Orten eventuell zu unterschiedlichen Zeiten wahrgenommen werden, so dass auch unterschiedliche Daten gezählt wurden. Wenn dagegen der Eintritt eines Ereignisses nur an einem bestimmten Ort (z. B. der Hauptstadt oder dem Haupttempel) maßgeblich war, dann konnten weit entfernt gelegene Gebiete oft erst nach Tagen davon unterrichtet werden. Solche Probleme gab es beispielsweise im früheren jüdischen Kalender, wo der Hohepriester über die erste Sichtung der Mondsichel bei Neumond entschied. Durch die langen Informationswege konnte es passieren, dass ein religiöses Fest in abgelegenen Gebieten am „falschen“ Tag gefeiert wurde. Auch war es kurz vor Monatsende nicht möglich vorherzusagen, welches Datum z. B. in sieben Tagen sein würde, weil der Neumond nicht vorausberechnet, sondern durch tagesaktuelle Beobachtung ermittelt wurde.

Immer mehr Kulturen begannen deshalb ihre Kalender zu berechnen. Der letzte ernsthafte Versuch, einen Beobachtungs-Kalender zu etablieren, wurde in der Französischen Revolution unternommen (Französischer Revolutionskalender).

Die Berechnung von Kalendern (arithmetische Kalender) setzt umfangreiche astronomische und mathematische Kenntnisse voraus. Bei der Entwicklung des frühen ägyptischen astralen Sothiskalenders waren diese Kenntnisse vorhanden. Die Einführung eines ägyptischen Verwaltungskalenders auf 365-Tage-Basis folgte spätestens im dritten Jahrtausend v. Chr. Dieser konnte jedoch das "Durchwandern der Jahreszeiten" nicht verhindern. Die ägyptischen Könige bemängelten die Jahreszeitenverschiebung, doch erst Ptolemaios III. unternahm 238 v. Chr. einen Versuch zur Einführung eines Schalttages. Nach seinem Tod wurde neben dem neuen Schalttageskalender jedoch wieder der alte ägyptische Verwaltungskalender benutzt. Der julianische Kalender, der 45 v. Chr. von Julius Cäsar eingeführt wurde, stützte sich gleichwohl auf die Kalenderform des Ptolemaios III.

Sowohl Mond- als auch Sonnenkalender müssen mit Schalttagen oder unterschiedlichen Monatslängen arbeiten, die nach einer festgelegten mathematischen Regel in den normalen Kalenderlauf eingefügt sind. Ein Sonnenkalender benötigt normalerweise einen zusätzlichen Tag circa alle vier Jahre (im gregorianischen Kalender ist dies der 29. Februar), um die durchschnittliche Tageszahl der Länge des Sonnenjahrs anzupassen. Ein Mondkalender muss die Monatslängen zwischen 29 und 30 Tagen variieren, denn die Zeit zwischen zwei gleichen Mondphasen dauert durchschnittlich circa 29,531 Tage.

Der Einschub eines zusätzlichen Tages, Monats oder Jahres in ein Kalendersystem wird als "Embolismus" ( „einschalten“) bezeichnet.

Lunar- oder Mondkalender orientieren sich an den Mondphasen. Das deutsche Wort Monat leitet sich etymologisch von Mond ab. Allerdings hat der Monat des gregorianischen Kalenders außer dem Namen nichts mehr mit dem Mondzyklus zu tun, da er mit einer durchschnittlichen Länge von 30,437 Tagen fast einen Tag länger dauert als der durchschnittliche synodische Monat.

Der Nachteil eines reinen Lunarkalenders besteht darin, dass er nicht mit dem Sonnenjahr korrespondieren kann, eine Eigenschaft, die in subtropischen und tropischen Breiten oft nicht die Bedeutung hat, die ihm in von den Jahreszeiten abhängigen Kulturen zukommt. So dauert im bekanntesten heute noch gebräuchlichen lunaren Kalender, dem islamischen Kalender, das Jahr mit 12 Monaten durchschnittlich 354,372 Tage. Die islamischen Monate „wandern“ dadurch Jahr für Jahr zirka elf Tage im gregorianischen Kalender nach vorne. Auch das Osterdatum folgt einem Lunaren Kalender (siehe: "Computus (Osterrechnung)")

Die meisten Kulturen orientierten sich bei ihrer Zeitmessung an den durch die Sonne bestimmten Jahreszeiten (Solar- oder Sonnenkalender). Dementsprechend hat der Grundtyp des Solarkalenders die meisten Varianten hervorgebracht. Das "Solarjahr" orientiert sich am tropischen Jahr, dem auf den Frühlingspunkt bezogenen Umlauf der Erde um die Sonne. Dieses ist die Ausgangsbasis für den allgemeinen Jahres­begriff. Der heute weltweit verbreitete gregorianische Kalender ist ein solarer Kalender.

Der Lunisolarkalender stellt den Versuch dar, einen reinen Lunarkalender an das Sonnenjahr anzupassen. Da die Länge der Monate durch die Mondphasen festgelegt ist, können keine Schalttage wie beim Sonnenkalender eingefügt werden. Die Lösung liegt in der Einfügung von Schaltmonaten. Die Jahreslänge der Lunisolarkalender schwankt deshalb zwischen zirka 353 und zirka 385 Tagen. Bekannte Lunisolarkalender sind der jüdische, der traditionelle chinesische und der keltische Kalender.

Es sind nur wenige Kalendersysteme bekannt, die sich weder am Mond noch an der Sonne orientieren. Der astronomische ägyptische Kalender orientierte sich an dem sehr hellen Stern Sirius. Die Maya-Kalender basierten auf einer regelmäßigen Folge von 20 Tagen und einer 52 Jahre dauernden Kalenderrunde.





</doc>
<doc id="2839" url="https://de.wikipedia.org/wiki?curid=2839" title="Koran">
Koran

Der Koran (so die eingedeutschte Form von , []) ist die heilige Schrift des Islams, die gemäß dem Glauben der Muslime die wörtliche Offenbarung Gottes (arab. "Allah") an den Propheten Mohammed enthält. Er ist in einer speziellen Reimprosa abgefasst, die auf Arabisch als Sadschʿ bezeichnet wird. Der Koran besteht aus 114 Suren, diese bestehen wiederum aus einer unterschiedlichen Anzahl an Versen (). Maßgeblich für alle modernen Ausgaben ist die orthographisch standardisierte Edition der Kairoer Azhar-Universität von 1924.

Ein wichtiges Kennzeichen des Korans ist seine Selbstreferentialität. Das bedeutet, dass der Koran sich an vielen Stellen selbst thematisiert. Auch die meisten Glaubenslehren der Muslime hinsichtlich des Korans stützen sich auf solche selbstreferentiellen Aussagen im Koran. Nach dem Glauben der sunnitischen Muslime ist der Koran die unerschaffene Rede Gottes oder zumindest ein Ausdruck davon. Eine Minderheit von Muslimen ist dagegen der Auffassung, dass der Koran erschaffen ist. 

Der Koran ist die Hauptquelle des islamischen Gesetzes, der Scharia, weitere Quelle der Scharia ist unter anderem die Sunna des Propheten Mohammed. Daneben gilt der Koran auch als ästhetisches Vorbild für arabische Rhetorik und Dichtung. Seine Sprache beeinflusste darüber hinaus stark die Entwicklung der arabischen Grammatik. Neben den erhaltenen Fragmenten der vorislamischen Dichter galt und gilt das koranische Arabisch als Richtschnur für die Korrektheit sprachlichen Ausdrucks.

Im Arabischen wird der Koran mit dem Attribut "karīm" (edel, würdig) versehen. Unter deutschsprachigen Muslimen ist der Begriff „der Heilige Qur'an“ gebräuchlich.

Gemäß der Überlieferung nach Mohammeds Cousin Ibn ʿAbbās und seinem Schüler Mudschāhid ibn Dschabr fand die erste Offenbarung in der Höhle im Berg Hira statt. Es sind die ersten fünf Verse der Sure 96. Sie beginnt mit den Worten:

Allgemein wird angenommen, dass Mohammed weder lesen noch schreiben konnte, weshalb die Muslime glauben, dass der Erzengel Gabriel ihm den Befehl gab, das zu rezitieren/vorzutragen, was vorher in sein Herz geschrieben wurde. Daher hat der Koran auch seinen Namen: „Lesung/Rezitation“.

Der islamischen Überlieferung, der Sira-Literatur und der Koranexegese (Tafsir) zufolge trat Mohammed nach der ersten Offenbarung aus der Höhle, und der Erzengel Gabriel baute sich in alle Blickrichtungen vor ihm auf. Von diesem Erlebnis soll Mohammed so erschüttert gewesen sein, dass er zitternd zu seiner Frau Chadidscha heimkehrte, die ihn in eine Decke wickelte, worauf die Sure 74 offenbart wurde:

Der Überlieferung zufolge soll ʿAlī ibn Abī Tālib Augenzeuge der ersten Offenbarung gewesen sein. In den folgenden 22 Jahren wurde Mohammed der gesamte Koran offenbart, wobei viele Verse Bezug auf aktuelle Geschehnisse der Zeit nehmen. Andere Verse erzählen von den Propheten (Adam, Abraham, Noah, Josef, Moses, ʿĪsā ibn Maryam (Jesus) und weiteren) und wieder andere enthalten Vorschriften und allgemeine Glaubensgrundsätze. Dabei wendet sich der Koran an alle Menschen. Es werden auch Nichtgläubige und Angehörige anderer Religionen angesprochen.

Der Koran besteht aus 114 mit Namen versehenen Suren. Während man in der nicht-islamischen Welt bei Koranzitaten üblicherweise die Suren mit ihrer Nummer nennt, wird in Veröffentlichungen von muslimischer Seite bei Koranzitaten meist auf deren arabischen Namen verwiesen. Die Benennung der Sure richtet sich nach einem bestimmten Wort, das in ihr vorkommt, beschreibt jedoch nicht unbedingt ihren Hauptinhalt. Einerseits sind viele Suren inhaltlich als unzusammenhängend zu betrachten – die Sure "An-Nisa" (die Frauen) beispielsweise enthält zwar einen wichtigen Teil der Koranstellen mit Bezug auf Frauen, spricht aber ansonsten auch über das Erbrecht sowie über generelle Glaubensinhalte. Ebenso die zweite Sure (al-Baqara – Die Kuh), welche zwar eine Geschichte mit einer Kuh als Schlachtopfer enthält, jedoch einen Großteil der gesetzlichen Regeln und der Glaubensinhalte vermittelt. Andererseits kommt es vor, dass unmittelbar aufeinanderfolgende Suren, wie beispielsweise "Ad-Duhā" und "Asch-Scharh", dieselbe Thematik behandeln – in diesem Falle die Erinnerung an die Wohltaten Gottes – und deshalb oft zusammen gelesen werden.

Die Anordnung der Suren folgt keinem inhaltlichen Muster; vielmehr sind die Suren, mit Ausnahme der ersten Sure "Al-Fātiha", grob der Länge nach geordnet (beginnend mit der längsten). Beispielsweise ist die Sure 108 mit nur drei Versen und 42 bzw. 43 Buchstaben am kürzesten, wenn ein Hamza hier als Buchstabe gezählt wird. Auch viele andere Suren weichen von der Anordnung nach der Länge ab, was von den Muslimen als Zeichen dafür gesehen wird, dass die Anordnung nicht willkürlich geschah. Die Muslime sind überzeugt, dass die Anordnung der Suren vom Propheten Muhammad so überliefert wurde. Im Gebet ist es deshalb unerwünscht, eine spätere vor einer früheren Sure zu rezitieren. Im Gegensatz zum Tanach der Juden und zur Bibel der Christen, die zu bedeutenden Teilen aus chronologisch geordneten Geschichtsbüchern bestehen, gibt es eine solche Ordnung weder innerhalb der Suren noch in ihrer Anordnung, obwohl die chronologische Folge der Suren als bestimmbar angenommen wird.

Mit Ausnahme von Sure 9 beginnen alle Suren des Korans mit der Basmala-Formel (). Am Anfang von 29 Suren stehen bestimmte abgetrennte Buchstaben, die wegen ihrer ungeklärten Bedeutung auch „geheimnisvolle“ oder „rätselhafte Buchstaben“ genannt worden sind. In modernen Koranausgaben werden am Anfang der Suren neben dem Namen auch die Anzahl der Verse und der Offenbarungsort – Mekka oder Medina – angegeben.

Die Suren bestehen jeweils aus einer unterschiedlichen Anzahl an Versen ("āyāt", sg. āya). Hierbei gibt es grundsätzlich sieben verschiedene Systeme der Verszählung, die schon im 8. Jahrhundert entstanden sind und nach den großen Zentren der Korangelehrsamkeit benannt sind: Kufa, Basra, Damaskus, Homs, Mekka, Medina I und Medina II. Allerdings gibt es nur für zwei dieser Systeme feste, eindeutige Gesamtverszahlen, nämlich 6.236 für Kufa und 6.204 für Basra. Die heutige Verszählung richtet sich üblicherweise nach der 1924 erschienenen ägyptischen Standardausgabe, die der kufischen Verszählung folgt. Neben diesen Verszählungen begegnet in der älteren orientalistischen Literatur noch eine andere Verszählung, die auf Gustav Flügel zurückgeht und keiner islamischen Verszählungstradition verpflichtet ist. Eine Synopse der kairinischen und der Flügel'schen Zählung bietet die vom Reclam-Verlag herausgegebene Koranübersetzung von Max Henning.

Eine weitere Form der Verszählung liegt in der Ahmadiyya-Ausgabe des Korans vor. Hier wird anders als in der ägyptischen Standardausgabe jeweils die Basmala als erster Vers mitgezählt, wodurch sich die Zählung der folgenden Verse jeweils um einen Vers verschiebt (z. Bsp.: 2:30 → 2:31). In den Koranausgaben, die auf der ägyptischen Ausgabe basieren, wird die Basmala dagegen nur bei der ersten Sure Al-Fātiha als eigener Vers mitgezählt. 
Da die Verszahlen nicht als Teil der Offenbarung gelten, werden sie in den oft kunstvoll ornamentierten Koranbüchern mit einer Umrandung von dem Offenbarungstext ausgeschlossen. Auch andere Seitenzahlen bleiben außerhalb des Offenbarungstextes, der klar ersichtlich abgegrenzt wird.

Während Suren und Verse eine sehr unterschiedliche Länge aufweisen, gibt es noch verschiedene andere Einteilungen des Korans, die den Text in gleich lange Abschnitte gliedern. Sie finden vor allem in der Liturgie Verwendung und dienen als Maßeinheiten zur Festlegung von Gebetspensen. Die wichtigsten derartigen Maßeinheiten sind der 30. Teil des Korans, Dschuzʾ genannt (, Plural ), und der 60. Teil des Korans, Hizb genannt (, Plural ). Die Grenzen zwischen den einzelnen Dschuzʾ- und Hizb-Abschnitten befinden sich meistens mitten in einer Sure.

Die Einteilung des Korans in dreißig Teile ist besonders für den Ramadan-Monat wichtig, denn es ist eine beliebte Praxis, verteilt auf die dreißig Ramadan-Nächte, eine Chatma, also eine Komplettlesung des Korans, vorzunehmen. Dschuzʾ- und Hizb-Einteilungen sind üblicherweise an den Rändern der Koranexemplare markiert, manchmal sind sogar die einzelnen Viertel des Hizb gekennzeichnet.

Für die gemeinsame Koranrezitation bei feierlichen Anlässen wurden in vormoderner Zeit die Dschuzʾ-Abschnitte auch häufig einzeln abgeheftet und in einem speziellen Holzkasten, der "Rabʿa" genannt wurde, untergebracht. Verschiedene muslimische Herrscher wie Sultan Kait-Bay oder Sultan Murad III. ließen derartige Rabʿa-Kästen in kostbarer Ausführung anfertigen und stifteten sie den heiligen Stätten in Mekka, Medina und Jerusalem. Dort waren ausgebildete Koranleser damit beauftragt, täglich daraus zu rezitieren.

Der Koran entstand in einem Zeitraum von knapp zwei Jahrzehnten. Nach dem Ort der Offenbarung wird zwischen mekkanischen und medinensischen Suren unterschieden. Die mekkanischen Suren werden noch einmal in früh-, mittel- und spätmekkanische Suren unterteilt.

Der Prozess der Buchwerdung des Korans lässt sich anhand der frühen Entwicklung des arabischen Begriffs "qurʾān", der dem deutschen Wort „Koran“ zugrunde liegt, nachverfolgen. Er kommt etwa 70 Mal im Koran selbst vor. Seine ursprüngliche Bedeutung ist „Vortrag, Lesung, Rezitation“. In diesem Sinne erscheint er zum Beispiel in zwei Passagen aus mittelmekkanischer Zeit, in denen sich Allah an Mohammed wendet:
In der islamischen Lehrtradition wird das Wort dementsprechend als Verbalsubstantiv zum arabischen Verb "qaraʾa" () erklärt. Christoph Luxenberg hat hierzu die Vermutung geäußert, dass es sich um eine Entlehnung vom syrischen Wort "qeryânâ" handelt, das in der christlichen Liturgie eine Perikopenlesung bezeichnet. Tatsache ist, dass das Wort in vorkoranischer Zeit im Arabischen nicht bezeugt ist.

In einigen Versen, die ebenfalls aus mittelmekkanischer Zeit stammen, bezeichnet der Begriff "qurʾān" bereits einen vorgetragenen Text. So wird in Sure 72:1f dem Propheten mitgeteilt, dass eine Gruppe von Dschinn gelauscht und anschließend gesagt habe: „Siehe, wir haben einen wunderbaren "qurʾān" gehört, der auf den rechten Weg führt, und wir glauben nun an ihn“.

Im Laufe der Zeit erhielt dann der Begriff die Bedeutung einer in Buchform vorliegenden Sammlung von Offenbarungen. So wird der "qurʾān" in verschiedenen Passagen, die der frühmedinensischen Zeit zugeordnet werden (Sure 12:1f; 41:2f; 43:2f), als die arabische Version von „dem Buch“ ("al-kitāb") ausgewiesen. In Sure 9:111, einer Passage, die auf das Jahr 630 datiert wird, erscheint der "qurʾān" schließlich als ein heiliges Buch in einer Reihe mit der Tora und dem Evangelium. Zwar ist der Prozess der Buchwerdung damit noch nicht abgeschlossen, doch lässt sich erkennen, dass der Koran bereits als ein Buch aufgefasst wurde. Als einer der letzten geoffenbarten Verse des Korans gilt Sure 5:3. Von diesem Vers wird überliefert, dass Mohammed ihn erstmals wenige Monate vor seinem Tod bei der sogenannten Abschiedswallfahrt den Gläubigen vortrug.

Vor dem Tod des Propheten Mohammed waren bereits verschiedene Teile des Korans niedergeschrieben worden, und nach Abstimmung mit allen, die den Koran sowohl mündlich ("Hafiz") als auch schriftlich bewahrt hatten, entstand nach Mohammeds Tod im Jahre 11 n. H. (632 n. Chr.) zu Zeiten des ersten Kalifen Abū Bakr der erste Koran-Kodex ( "muṣḥaf"), um ihn vor dem Verlorengehen oder Verwechseln mit anderen Aussagen des Propheten Mohammed zu bewahren.

Der dritte Kalif, Uthman ibn Affan (644–656), ließ diese ersten Koran-Kodizes, die auch z. T. in anderen Dialekten als dem quraischitischen Dialekt – dem Dialekt des Propheten Mohammed – abgefasst waren, einsammeln und verbrennen, um dann einen offiziell gültigen Koran herzustellen. Dabei mussten mindestens zwei Männer bei jedem Vers bezeugen, dass sie diesen direkt aus dem Munde des Propheten gehört hatten. Sechs Verse im Koran sind aber nur von einem Zeugen, nämlich Zaid ibn Thabit, dem ehemaligen Diener des Propheten, auf diese Weise bezeugt worden. Dass diese Verse heute doch im Koran stehen, hängt damit zusammen, dass der Kalif ausnahmsweise das alleinige Zeugnis von Zaid akzeptierte.

Nach der islamischen Überlieferung wurden fünf Abschriften des uthmanischen Kodex in die verschiedenen Städte versandt, und zwar nach Medina, Mekka, Kufa, Basra und Damaskus. Gleichzeitig erging die Anordnung, alle privaten Koranaufzeichnungen zur Vorbeugung falscher Überlieferungen zu verbrennen. Man nahm früher an, dass die Abschrift, die nach Medina gesandt wurde, sich heute in Taschkent befindet und ein zweites Exemplar im Topkapi-Museum in İstanbul verwahrt wird. Beide Exemplare sind aber in kufischer Schrift, die sich in das 9. Jahrhundert n. Chr. datieren lässt, aufgeschrieben worden und somit wohl frühestens 200 Jahre nach Mohammed entstanden. In einer Bibliothek in Birmingham, der Cadbury Research Library, entdeckte man 2015 in einer Koranausgabe des späten 7. Jahrhunderts zwei Pergamentblätter, die sich mittels Radiocarbonmethode auf die Zeit zwischen 568 und 645 datieren ließen. Die Blätter enthalten Teile von Sure 18 bis 20, geschrieben mit Tinte in einer frühen Schriftform des Arabischen, des Hijazi. Damit zählen sie zu den ältesten Koranstücken der Welt.

Auch die heutige Anzahl und Anordnung der Suren gehen auf die Redaktion von Uthman zurück. Der Koran-Kodex des ʿAbdallāh ibn Masʿūd, der nach der Einführung des uthmanischen Kodexes noch eine Zeit weiter benutzt wurde, hatte nur 110 oder 112 Suren, die anders angeordnet waren. Charidschitische Gruppen im Iran bestritten, dass Sure 12 und Sure 42 Bestandteil des ursprünglichen Korans gewesen seien. Handschriftenfunde in der "Großen Moschee von San'a" deuten an, dass Korankodices aus dem ersten muslimischen Jahrhundert (7. Jahrhundert n. Chr.) bedeutende Unterschiede in der Orthographie, in den Lesarten (d. h. im Inhalt) und den Anordnungen der Suren aufweisen.

Die arabische Schrift des Uthman'schen Kodex kannte noch keine diakritischen Punkte, wie sie in der heutigen arabischen Schrift verwendet werden, um gleich aussehende Konsonanten zu unterscheiden. Deshalb war das mündliche Beherrschen des Textes wichtig, und die Schriftform des Rasm diente vor allem als Gedächtnishilfe.

In den Orten mit Abschriften des uthmanischen Kodexes entwickelten sich verschiedene Lesarten des Korans. Die islamische Tradition hat später sieben solcher Lesetraditionen als „kanonisch“ anerkannt. Erst Anfang des 8. Jahrhunderts wurden die Buchstaben im Korantext mit diakritischen Zeichen versehen. Die Initiative dazu ging auf al-Haddschādsch ibn Yūsuf zurück, den Statthalter des umayyadischen Kalifen Abd al-Malik im Irak, der auf diese Weise alle Uneindeutigkeiten in der Überlieferung des Korans ausräumen wollte. Aus dieser Zeit stammen auch die ersten Belege für die Bedeutung des Korans im öffentlichen Leben der Muslime, denn Abd al-Malik ließ die von ihm geprägten Münzen sowie die Inschriften des von ihm errichteten Felsendoms mit Koranzitaten (insbesondere Sure 112) versehen.

Im Koran selbst finden sich einige Aussagen über seine himmlische Herkunft. So ist in Sure 85:22 von „einer wohlverwahrten Tafel“ ("lauḥ maḥfūẓ") die Rede, auf der sich der Koran befinden soll und in Sure 43:3f wird ausgesagt, dass es zu dem arabischen Koran ein „Urbuch“ ("Umm al-kitāb") gibt, das sich bei Gott befindet. Es wird außerdem mitgeteilt, dass der Koran im Monat Ramadan (Sure 2:185) bzw. in der „Nacht der Bestimmung“ (Sure 97 „Al-Qadr“) von Gott herabgesandt wurde. Später wurden diese Aussagen im Bereich des sunnitischen Islams so interpretiert, dass der Koran in der „Nacht der Bestimmung“ in die unterste Himmelssphäre herabgesandt und von hier aus Mohammed während seines zwanzigjährigen Wirkens als Prophet jeweils bei den entsprechenden Offenbarungsanlässen in Einzelteilen übermittelt wurde. Aus den genannten Aussagen im Koran wurde allgemein geschlossen, dass dem Koran ein übernatürliches Wesen zukommt.

Um die Mitte des 8. Jahrhunderts kam es allerdings zu heftigen Diskussionen, als verschiedene Theologen aus dem Kreis der Murdschi'a die Präexistenz des Korans in Zweifel zogen und die Theorie der Erschaffenheit des Korans "(ḫalq al-Qurʾān)" aufbrachten. Während die Traditionalisten diese Theorie bekämpften, wurde sie von den Muʿtaziliten und Ibaditen übernommen und weiter ausgearbeitet. Die drei Kalifen al-Ma'mūn, al-Muʿtasim und al-Wāthiq erhoben die Lehre von der Unerschaffenheit des Korans sogar zur offiziellen Doktrin im Abbasidenstaat und ließen alle diejenigen, die sich nicht dazu bekennen wollten, im Zuge der Mihna inquisitorisch verfolgen. In dieser Situation entwickelte der Theologe Ibn Kullāb eine Zwischenposition, indem er zwischen dem Inhalt der Offenbarung und seiner „Ausdrucksform“ "(ʿibāra)" differenzierte. Er lehrte, dass nur Ersteres unerschaffen und anfangsewig sei, während die Ausdrucksform der Rede Gottes in der Zeit variieren könne. Diese Lehre wurde später von den Aschʿariten übernommen.

Zwar bestritten die Muʿtaziliten die Unerschaffenheit des Korans, doch entwickelten sie dafür das Dogma von der „Unnachahmlichkeit des Korans“ ("iʿdschāz al-qurʾān"). Dieses stützte sich auf verschiedene Stellen im Koran, an denen die Ungläubigen aufgefordert werden einzugestehen, dass sie nicht imstande sind, etwas dem Koran Ebenbürtiges hervorzubringen (vgl. Sure 2:223; 11:13), bzw. an denen deutlich ausgesagt wird, dass, selbst wenn Menschen und Dschinn sich zusammentäten, sie nichts Ebenbürtiges hervorbringen könnten (vgl. Sure 17:88). Der Koran gilt damit gleichzeitig als „Beglaubigungswunder“ für den prophetischen Anspruch Mohammeds. Dieses Iʿdschāz-Dogma hat später im Islam allgemeine Verbreitung gefunden.

Um den Koran herum bildete sich ein ganzes Bündel verschiedener Wissenschaften. Aus dem Bedürfnis nach Auslegung (Exegese) des Offenbarungsinhalts entwickelte sich die die Wissenschaft der Koranexegese ("ʿilm at-tafsīr"). Ausführliche, oft Dutzende Bände füllende Kommentarwerke sind vom 2. muslimischen Jahrhundert an (8. Jahrhundert n. Chr.) entstanden; zu den berühmtesten zählen die von at-Tabarī (gest. 923), az-Zamachscharī (gest. 1144),
Fachr ad-Dīn ar-Rāzī (gest. 1209), Qurtubi (gest. 1272), al-Baidāwī (gest. 1290) und Ibn Kathīr (gest. 1373).

Weitere wichtige Themen, mit denen sich die Koranwissenschaften befassen, sind die Asbāb an-nuzūl, die verschiedenen Lesarten des Korans, die abrogierenden und abrogierten Koranverse, und die Koranrezitation. 

Die moderne westliche Forschung befasst sich besonders intensiv mit den Anspielungen auf Erzählstoffe aus dem Umfeld des Alten Testaments und der apokryphen Evangelien, die der Koran enthält. Sie werden als Beleg dafür betrachtet, dass der Koran in vielschichtigen und engen Beziehungen zum „religiösen und geistigen Leben des Nahen Ostens der Spätantike steht“.

Heinrich Speyer hat aufgezeigt, dass der Koran in seiner Darstellung vom Fall des Iblīs stark von der Schatzhöhle, einem syrischen Text des 6. Jahrhunderts, und dem Leben Adams und Evas, einem um die Zeitenwende entstandenen frühjüdischen und vor 400 n. Chr. christlich überarbeiteten Buch, geprägt ist.

Tilman Nagel kommt aufgrund seiner Forschungen zu dem Ergebnis, dass sich nacheinander vier unterschiedliche Themenkreise im Koran niedergeschlagen haben: 

Unter den Hanīfen, deren Bilder und Themen der Koran aufgreift, ist nach Nagels Auffassung der Dichter Umaiya ibn Abī s-Salt besonders wichtig.

Eine wirkliche Übersetzung des Korans gilt in der traditionellen islamischen Theologie als unmöglich, da jede Übersetzung zugleich eine Interpretation enthält. Daher wird das Studium des Korans im arabischen Originaltext empfohlen. Einige Sufis zum Beispiel glauben, es sei segensreicher, sich die arabischen Buchstaben eines Korantextes anzuschauen, auch wenn man kein Arabisch versteht, als eine schlechte Übersetzung zu lesen. Allerdings sind schon im Mittelalter verschiedene persische und türkische Übersetzungen des Korans erstellt worden.

Ab dem Ende des 19. Jahrhunderts wurde der Koran in verschiedene indische Sprachen übersetzt. Den Anfang machte der Brahmo-Gelehrte Girish Chandra Sen mit der Übersetzung in die Bengalische Sprache (1881–1883).

Die erste Übersetzung ins Deutsche stammt vom Nürnberger Pfarrer Salomon Schweigger 1616. Er übersetzte dabei die erste italienische Fassung aus dem Jahre 1547 von Andrea Arrivabene, die ihrerseits auf einer lateinischen Übersetzung aus dem 12. Jahrhundert basierte. Der Orientalist Friedrich Rückert übertrug in der ersten Hälfte des 19. Jahrhunderts weite Teile des Korans in gebundener Sprache ins Deutsche. Rückerts Übersetzung versucht den Klang des koranischen Arabisch im Deutschen wiederzugeben, hat aber Textstellen nach eigenem Ermessen ausgelassen. Rudi Paret, dessen Übersetzung (Erstausgabe 1962) in Fachkreisen als die philologisch zuverlässigste gilt, setzt demgegenüber bei mehrdeutigen Passagen die zusätzlichen Übersetzungsmöglichkeiten bzw. die wörtliche Bedeutung (mit einem w. gekennzeichnet) in Klammern dahinter.

1939 erschien eine von der Ahmadiyya-Gemeinde herausgegebene Koranübersetzung; sie gilt als erste deutsche, durch Muslime herausgegebene Koranübersetzung. Es folgten danach weitere Übersetzungen, u. a. durch den arabisch-christlichen Theologieprofessor Adel Theodor Khoury (traditionsgebunden, vom Islamischen Weltkongress unterstützt), von Lazarus Goldschmidt, von Ahmad von Denffer und von Max Henning (Reclam), die 1968 auch in der DDR erschien.

Die Übersetzung von Henning ist von Murad Wilfried Hofmann überarbeitet und mit Anmerkungen versehen worden. Die Überarbeitung wird von vielen seiner Glaubensgenossen aus dem deutschen Sprachraum geschätzt, von Seite der Islamwissenschaften teils deutlich kritisiert. Eine zeitgenössische Übersetzung, die auch den arabischen Text und gleichzeitig zu jedem Vers eine Auswahl aus wichtigen, ins Deutsche übersetzten Kommentaren bringt, wurde von einer Gruppe deutschsprachiger Muslimas unter Leitung von Fatima Grimm unter dem Titel "Die Bedeutung des Koran" herausgegeben. Eine weitere Übersetzung hat Muhammad Rassoul unter dem Titel "Die ungefähre Bedeutung des Al-Qur’an Al-Karim" in der Islamischen Bibliothek veröffentlicht. Dies ist die Übersetzung, die auf der Website des ZMDs zu finden ist.

Eine Bearbeitung in zeitgenössischer deutscher Sprache, die sich auch an Kinder und Jugendliche wendet, wurde im Jahr 2008 von den Islamwissenschaftlerinnen Lamya Kaddor und Rabeya Müller erstellt. Die Neuübersetzung soll nach Lamya Kaddor „den Respekt vor dem heiligen Buch der Muslime mit einem verständlichen Zugang verbinden“.



siehe Koranübersetzung




</doc>
<doc id="2840" url="https://de.wikipedia.org/wiki?curid=2840" title="Knöterichgewächse">
Knöterichgewächse

Die Knöterichgewächse (Polygonaceae) sind eine Pflanzenfamilie in der Ordnung der Nelkenartigen (Caryophyllales) innerhalb der Bedecktsamigen Pflanzen (Magnoliopsida). Die etwa 48 Gattungen mit etwa 1200 Arten kommen hauptsächlich in den nördlichen gemäßigten Zonen vor, mit wenigen Arten in den Tropen oder Permafrostgebieten.

Häufig sind es krautige Pflanzen, aber es gibt auch verholzende Taxa: Sträucher (beispielsweise "Coccoloba"-Arten), Lianen oder seltener Bäume ("Triplaris"). Manche Arten (beispielsweise bei "Calligonum") sind Rutensträucher bei denen die Sprossachsen den Hauptanteil der Photosynthese übernehmen. Die selbständig aufrechten, niederliegenden, windenden oder kletternden Sprossachsen sind gestreift, gerillt oder stachelig und besitzen oft verdickte Knoten (Nodien). An einem Knoten zweigen mehr als drei Blattspuren ab (vielspurig, multilakunär).

Die Laubblätter sind gut entwickelt oder reduziert. Die meist wechselständig, selten gegenständig oder wirtelig angeordneten Laubblätter sind deutlich oder kaum erkennbar gestielt bis fast sitzend. Die Blattspreiten sind meist einfach. Bei der Unterfamilie Polygonoideae sind Nebenblätter vorhanden und bei der Unterfamilie Eriogonoideae fehlen sie. Die Taxa der Unterfamilie Polygonoideae weisen ein typisches Merkmal auf: die Ochrea (Tute), eine röhrige, meist häutige Scheide am Grund der Blattstiele, die von den verwachsenen Nebenblättern gebildet wird; sie umhüllt den Vegetationspunkt und wird während des Wachstums der Sprossachse durchbrochen.

Manche Arten sind einhäusig (monözisch) oder zweihäusig (diözisch) getrenntgeschlechtig. Die end- oder seitenständigen, ährigen, rispigen, traubigen oder kopfigen Blütenstände enthalten oft viele Blüten. Die Blattstiele sind manchmal gegliedert. 

Die selten eingeschlechtigen, meist zwittrigen Blüten sind relativ klein, radiärsymmetrisch und dreizählig. Es gibt ein oder zwei Kreise mit je drei häutigen Blütenhüllblättern, die sich alle oder nur die inneren bis zur Fruchtbildung vergrößern und auch noch an der Frucht erhalten sind und dann geflügelt, warzig oder bestachelt sein können. Es sind selten ein, meist zwei oder drei Kreise mit je drei Staubblättern vorhanden. Die Staubfäden sind frei oder höchstens an ihrer Basis verwachsen. Die Staubbeutel besitzen zwei Theken und öffnen sich mit Längsschlitzen. Der ringförmige Diskus ist oft gelappt. Zwei oder drei, selten vier Fruchtblätter sind zu einem oberständigen, einkammerigen Fruchtknoten verwachsen. Die zwei oder drei, selten vier Griffel sind vollständig frei oder nur an ihrer Basis verwachsen. 
Die meist dreieckigen, bikonvexen oder bikonkaven Nussfrüchte, bei dieser Familie auch Achänen genannt, sind oft durch die haltbaren Blütenhüllblätter geflügelt. Die Samen enthalten viel Endosperm. Der Embryo ist gerade oder gekrümmt, selten gefaltet.

Die Arten der früheren Familien Calligonaceae , Coccolobaceae nom. nud., Eriogonaceae , Persicariaceae , Rumicaceae sind heute hier enthalten. 

Die Familie der Knöterichgewächse wird in zwei Unterfamilien gegliedert und enthält etwa 43 Gattungen (nach Brandbyge 1993 – wie in folgender Liste – sind es mehr (etwa 53), da dort "Polygonum" in mehrere Gattungen aufgegliedert ist). Sie enthält 1110 bis 1200 Arten.

In China kommen etwa 13 Gattungen mit etwa 238 Arten vor, 65 davon nur dort. Etwa 16 Gattungen mit etwa 160 Arten kommen in Nordamerika, hauptsächlich in gemäßigten Regionen vor.

Unterfamilie Polygonoideae enthält 15 bis 28 Gattungen mit 590 bis 850 Arten:

Sie ist überwiegend neuweltlich und enthält 20 bis 28 Gattungen mit 325 bis 520 Arten. Nur zwei bis vier Arten sind in Afrika beheimatet. Das Zentrum der Artenvielfalt sind die gemäßigten Gebiete im westlichen Nordamerika von Alaska bis Mexiko mit sehr vielen Arten in Kalifornien. Relativ wenige Arten sind in den östlichen USA beheimatet. In Nordamerika gibt es etwa 19 Gattungen mit etwa 281 Arten. In Südamerika gibt es nur wenige Arten in Argentinien und Chile. 
Die meist verholzenden Arten sind zweihäusig getrenntgeschlechtig (diözisch).


Der Buchweizen war früher ein wichtiger Bestandteil der Ernährung und wird auch heute noch in der Küche verwendet. Als Obst werden Sorten von "Rheum rhabarbarum" angebaut. Als Heilpflanzen werden "Rheum rhabarbarum" und "Rheum officinale" verwendet.





</doc>
<doc id="2842" url="https://de.wikipedia.org/wiki?curid=2842" title="Korbblütler">
Korbblütler

Die Korbblütler (Asteraceae oder Compositae), auch Korbblütengewächse, Asterngewächse oder Köpfchenblütler genannt, sind die größte Familie der Ordnung der Asternartigen (Asterales) innerhalb der Bedecktsamigen Pflanzen (Magnoliopsida). Etwa 10 % der Arten der Bedecktsamer gehören zu den Asteraceae. Von der Blütenstandsform sind der deutsche Name Korbblütler und der botanische Name "Compositae" (lat. für ‚Zusammengesetzte‘) abgeleitet.

Die Familie Asteraceae enthält etwa 1.600 bis 1.700 Gattungen mit etwa 24.000 Arten und ist weltweit auf allen Kontinenten, außer Antarktika, in allen Klimazonen vertreten. In Europa gehört sie zu den artenreichsten Pflanzenfamilien.
Es gibt überwiegend ein- bis zweijährige oder ausdauernde krautige Pflanzen-Arten, aber es gibt auch verholzende Arten: Halbsträucher und Sträucher, selten Lianen oder Bäume. Es gibt monokarpische und polykarpische Arten. Es gibt Arten in fast jedem Habitattyp, nur wenige Arten wachsen als echte Epiphyten oder Wasserpflanzen. Bei einigen Taxa enthalten die Pflanzen Milchsaft. 

Die Anordnung der Blätter ist meist wechselständig, selten gegenständig oder quirlständig; sie sind oft zu einer grundständigen Rosette vereinigt. Die gestielten bis sitzenden Laubblätter besitzen selten einfache, oft fiederteilige bis gefiederte Blattspreiten. Sie sind krautig bis ledrig, manchmal sind sie in Dornen umgewandelt. Der Blattrand ist glatt, gewellt, gelappt, gesägt, gezackt oder gezähnt. Es sind meist keine Nebenblätter vorhanden.

In verschiedenen, unterschiedlich aufgebauten Gesamtblütenständen zusammengefasst stehen auf mehr oder weniger unbeblätterten Blütenstandsschäften die Blütenkörbe oder sie stehen einzeln.

Typisch für diese Familie sind die körbchenförmigen Blütenstände. Die Hüllblätter umgeben die Blütenkörbchen und bilden das Involucrum („Hüllkelch“). Die kegelig verlängerte oder abgeflachte Blütenstandsachse, der Blütenboden, das "Blütenlager" (Clinanthium, Phoranthium), der einzelnen Blüten, welche zusammen das Blütenkörbchen (Calathium, Anthodium) bilden, ist kahl und glatt oder behaart. Der Körbchenboden kann sitzende, schuppenförmige Tragblätter, die sogenannten Spreublätter (Palea), besitzen. 

Ein Blütenkorb enthält je nach Art ein bis tausend Blüten. Die Blüten eines Blütenkorbes entwickeln sich und blühen von außen nach innen auf (zentripetal).

Die zwittrigen oder eingeschlechtigen Blüten sind meist fünfzählig. Die Kelchblätter sind teilweise oder ganz reduziert, bei vielen Taxa sind sie zu einem charakteristischen Haarkranz oder seltener zu einem häutigen Saum umgebildet; dieser Flugapparat für die Frucht heißt Pappus. Die Kronblätter sind zu einer Röhre verwachsen. Es ist nur ein Kreis mit drei bis fünf fertilen Staubblättern vorhanden. Die Staubfäden sind nur kurz. Die Staubbeutel (Antheren) sind zu einer Röhre verwachsen und bilden ein typisches Merkmal der Familie. Zwei Fruchtblätter sind zu einem unterständigen Fruchtknoten verwachsen. Die Griffel, mit immer zwei Griffelästen, schieben sich durch die Antherenröhre und schieben dabei den Pollen aus der Röhre mit Fegehaaren, die sich an der Außenseite oder der Spitze der Griffel befinden. Erst danach wird die Narbe empfängnisfähig.

Es gibt zwei grundsätzliche Blütenformen in der Familie: radiärsymmetrische Röhrenblüten (Scheibenblüten) und zygomorphe Zungenblüten (Strahlenblüten). Je nach Unterfamilie sind beide Blütenformen zusammen oder nur eine davon vorhanden.

Die Blütenformel lautet formula_1 oder formula_2. 

Die Frucht ist meist eine Sonderform einer Nuss, die Achäne, meist mit einem Pappus, der in Form von Schuppen, Borsten oder Haaren ausgebildet sein kann.

Die Blütenkörbe sehen wie Einzelblüten aus und fungieren auch blütenökologisch als Gesamtheit zur Anlockung von Bestäubern. Es sind also Blumen, in denen viele, kleine Einzelblüten zusammengefasst sind, sie bilden also eine Scheinblüte (Pseudanthium). Am Rand des Blütenkörbchens angeordnete Zungenblüten verstärken oft den Eindruck, dass es sich bei dem Blütenstand um eine einzige Blüte handelt. 

Die Bestäubung erfolgt überwiegend durch Insekten oder durch den Wind.

Die Ausbreitungseinheit (Diaspore) ist die Achäne. Die Achänen werden entweder durch den Wind durch die Flughaare oder durch Tiere verbreitet. Zur Tierverbreitung bilden die Involukralblätter z. B. bei der Großen Klette ("Arctium lappa") an der Spitze Haken aus, die sich im Fell von Säugetieren oder in der Kleidung von Menschen verhaken, um später an anderer Stelle wieder abzufallen. Dies ist eine spezielle Form der Zoochorie, die man Epizoochorie nennt.

Viele Arten sind reich an ätherischen Ölen, die sich in sehr charakteristischen Drüsenschuppen befinden. Oft wird als Reservestoff Inulin gebildet.

Einzelne Arten der Familie der Asteraceae und besonders ihre Kulturformen werden auf vielfältige Weise genutzt. Hier eine unvollständige Liste mit dem Pflanzenteil, das überwiegend genutzt wird:





Eine große Fülle an Arten und ihre Sorten werden als Zierpflanzen in allen Teilen der Welt genutzt. Sie werden in Parks und Gärten gepflanzt oder dienen als Schnitt- und Trockenblumen.

Fossilfunde der Asteraceae sind meist Pollenablagerungen und Früchte. Aus dem Eozän gibt es nur wenige Pollennachweise, aber ab dem Oligozän und Miozän sind die Pollen der Asteraceae häufig. Die Wichtigkeit der Familie in den Ökosystemen der Erde nimmt vom Mittleren Olizän bis heute zu. 

In letzter Zeit nutzte man die Fossilfunde der Pollen und molekulargenetische Untersuchungen (an ndhF und rbcL Genen), um den Ursprung der Asteraceae aufzudecken. Bremer and Gustafsson 1997 oder Kim et al. 2005 schlossen, dass der Ursprung vor mindestens 38 Mio. Jahren liegt, vermutlich im Mittleren Eozän (vor 42 bis 47 Mio. Jahren). 

Die heutige Verbreitung der am nächsten verwandten Familien Goodeniaceae und Calyceraceae und der basal und isoliert stehenden Unterfamilie Barnadesioideae lassen vermuten, dass der Ursprung der Familie auf Gondwana im heutigen Südamerika, Antarktika und Australien lag.

Die Familie Asteraceae enthält etwa 1600 bis 1700 Gattungen mit etwa 24.000 Arten. Nach phylogenetischen Erkenntnissen wurden zwölf Unterfamilien eingeführt, die insgesamt etwa 43 Tribus enthalten. 

Stammbaum nach Panero & Funk 2008:



</doc>
<doc id="2843" url="https://de.wikipedia.org/wiki?curid=2843" title="Kalmus (Gattung)">
Kalmus (Gattung)

Kalmus ("Acorus") ist die einzige Gattung in der monotypischen Familie der Kalmusgewächse (Acoraceae), der einzigen Familie der Ordnung der Kalmusartigen (Acorales). Sie ist die basalste Gattung der Monokotyledonen. Zu den deutschen Trivialnamen siehe unter dem Artikel zur Art "Acorus calamus".

Kalmus-Arten sind ausdauernde krautige Pflanzen. Als Überdauerungsorgane bilden diese Sumpfpflanzen Rhizome. Es ist ein Aerenchym ausgebildet. Alle Pflanzenteile sind kahl. Wenn man Pflanzenteile verletzt riechen sie stark aromatisch. Die grundständigen und zweizeilig angeordneten Laubblätter sind ungestielt. Die einfache Blattspreite ist flach, parallelnervig, unifazial und schwertförmig.
In einem ährigen Gesamtblütenstand sitzen mehrere kolbige Blütenstände zusammen. In Kolben sind viele Blüten angeordnet. Die Spatha ist stängelähnlich.
Die zwittrigen Blüten sind unscheinbar, dreizählig und pentazyklisch (mit fünf Blütenblattkreisen). Die sechs gleichgestaltigen, freien Blütenhüllblätter sind bräunlich und häutig. Es sind zwei Kreise mit je drei freien, fertilen, gelben Staubblättern vorhanden. Meist drei (zwei bis vier) Fruchtblätter sind zu einem oberständigen Fruchtknoten verwachsen mit zwei bis vier (selten bis fünf) Samenanlagen je Fruchtknotenkammer. Ein Griffel ist nicht erkennbar und so sitzen die Narben direkt auf den Fruchtknoten.

Die braunen bis rötlichen Früchte werden als Kapselfrüchte oder Beeren gedeutet und besitzen ein dünnes, ledriges Perikarp.

Die Chromosomenzahlen betragen 2n = (22), 24, 36, (44), 48.

Die Gattung "Acorus" wurde 1753 in "Species Plantarum", 1, S. 324 aufgestellt. Typusart ist "Acorus calamus" L. Ein Synonym für "Acorus" ist "Calamus" . Der Familienname Acoraceae wurde durch Iwan Iwanowitsch Martynow in "Tekhno-Bot. Slovar.", 1820, Seite 6 veröffentlicht.

Der Gattungsname "Acorus" stammt von griechisch "Ἄκορον (Ákoron)", einer Arzneipflanze, die Dioskurides als der Schwertlilie ähnlich beschreibt; ob er wirklich den Kalmus meint, bleibt unsicher.

Die Kalmusgewächse (Acoraceae) sind die basalste Gruppe der Einkeimblättrigen Pflanzen (Monokotyledonen). Früher wurde die Gattung "Acorus" der Familie der Aronstabgewächse (Araceae) zugeordnet.

Die Kalmus-Arten sind ursprünglich im gemäßigten bis subtropischen Asien sowie Nordamerika und auch in Südostasien heimisch. Sie sind in vielen Gebieten der Welt, beispielsweise Ägypten und Europa verwildert.

Die Gattung "Acorus" enthält je nach Autor nur zwei ("Flora of China" 2010 und Kew) bis zu sechs Arten beziehungsweise einige Unterarten oder Varietäten:


Aus Kalmus-Arten werden das „Kalmusöl“ (‚Calami aetheroleum‘, aus allen Pflanzenteilen) und „Ackerwurz“ (getrocknete Rhizome, auch „Kalmusrot“ oder „Magenwurz“ genannt) gewonnen. Die wichtigsten Inhaltsstoffe sind Asarone, deren Gehalt je nach Art (hier sind wechselnde Ploidiegrade vorhanden) und Pflanzenteil (Blätter oder Wurzeln) variiert. Diploider Amerikanischer Kalmus enthält kein β-Asaron, triploider europäischer Kalmus maximal 10 % und indischer Kalmus (tetraploid) bis zu 96 % β-Asaron. Weitere Inhaltsstoffe sind Farnesene (vorwiegend das β-Farnesen, bis 46 %), Geranylacetat (bis 77 %), der Methylether des "cis"-Isoeugenol (maximal 36 %), Shyobunon (0–53 %), Acorenon (6–9 %) sowie verschiedene Monoterpene (Camphen, Limonen, Myrcen und Pinene).

Kalmusöl wird in der Heilkunde und bei der Parfüm- und Likörherstellung und für Magenbitter verwendet. Kalmus gilt als kräftigend und appetitanregend und wird in der Volksmedizin verschiedentlich als Dekokt gegen Magen- und Verdauungsbeschwerden, Husten (eine krampflösende Wirkung ist belegt) und Erkältungen, Schmerzen und Entzündungen eingesetzt, wobei die Wirksamkeit meist wissenschaftlich nicht belegt ist.

Aufgrund nachgewiesener mutagener, carzinogener sowie reproduktionstoxischer Effekte der Asarone ist der Einsatz von Kalmusöl oder sonstigen Kalmusprodukten in Lebensmitteln nur beschränkt zugelassen; in den USA und Kanada ist die Verwendung von Kalmusprodukten nicht erlaubt.

Kalmus-Arten sind schilfartige Röhrichtpflanzen, die in den gemäßigten Klimazonen der nördlichen Hemisphäre Gräben, die Ufer von Teichen, Bächen und langsamfließenden Flüssen sowie Sumpfgebiete besiedeln.



</doc>
<doc id="2844" url="https://de.wikipedia.org/wiki?curid=2844" title="Kürbisgewächse">
Kürbisgewächse

Die Kürbisgewächse (Cucurbitaceae) sind eine Familie der Bedecktsamigen Pflanzen (Magnoliopsida). Die Vertreter sind meist krautige Pflanzen mit Ranken, eingeschlechtigen Blüten und unterständigem Fruchtknoten. Die Blütenkrone ist meist verwachsen, die Staubblätter sind untereinander verbunden bis verwachsen.

Die Kürbisgewächse sind eine mittelgroße Familie und umfassen rund 800 Arten in rund 130 Gattungen, die in die beiden Unterfamilien Nhandiroboideae und Cucurbitoideae aufgeteilt werden. Sie sind weltweit in den tropischen und subtropischen Klimazonen vertreten, nur wenige Arten kommen auch in gemäßigten Gebieten vor; in Mitteleuropa etwa ist nur die Gattung der Zaunrüben ("Bryonia") heimisch. Zur Familie zählen etliche Nutzpflanzen, die kommerziell bedeutendsten sind Gartenkürbis ("Cucurbita pepo"), Zuckermelone ("Cucumis melo"), Gurke ("Cucumis sativus") und Wassermelone ("Citrullus lanatus").

Die meisten Vertreter haben eine ausgeprägte Pfahlwurzel, die etwa bei Gurke ("Cucumis sativus") einen und bei Kürbissen ("Cucurbita") zwei Meter in die Tiefe reicht. Nahe der Oberfläche bilden sie ein dichtes Netz aus Seitenwurzeln, die mindestens eine so große, wenn nicht größere Bodenfläche bedecken als die oberirdischen Organe. An liegenden Sprossachsen bilden sich häufig Adventivwurzeln, auch ohne direkten Kontakt der Sprossachse mit dem Boden.

Einige xerophytische Vertreter bilden Speicherwurzeln, mit denen sie Trockenzeiten überstehen. Bei "Acanthosicyos" reichen sie bis 15 Meter tief.

Die Sprossachsen sind krautig, selten auch leicht verholzt. Meist sind sie niederliegend, kriechend oder kletternd. Im Querschnitt sind sie häufig eckig und besitzen ein hohles Zentrum. Die Seitenzweige erster und zweiter Ordnung können bis 15 Meter lang werden. Bei Kürbissen gibt es strauchförmige Formen mit kurzen Internodien.

Die Leitbündel sind bikollateral, das heißt, das Phloem befindet sich innen und außen am Xylem. Meist gibt es zehn Leitbündel, die in zwei Ringen um die zentrale Höhle angeordnet sind. Die Siebröhren sind relativ groß und sind bei manchen Sippen auch in der Rinde verstreut und verbinden so die einzelnen Leitbündel.

Viele Vertreter sind sowohl an der Sprossachse als auch an den Blättern weich oder rau behaart, nur wenige sind unbehaart. Die Form der Trichome ist sehr variabel: sie können drüsig sein, ein- oder vielzellig, einfach oder verzweigt.

In etlichen nicht näher verwandten Sippen innerhalb der Familie tritt eine Ausbildung dicker Stämme als Speicherorgane (Pachycaulie) auf. Pachycaule Vertreter in der Unterfamilie Nhandiroboideae sind "Gerrardanthus", "Neoalsomitra", "Xerosicyos" und "Zygosicyos". Innerhalb der Unterfamilie Cucurbitoideae in der Tribus Benincaseae "Cephalopentandra", "Coccinia", "Trochomeria", in der Tribus Coniandreae "Corallocarpus", "Doyerea", "Ibervillea", "Kedrostis", "Seyrigia", sowie verstreut "Marah", "Sinobaijiana", "Momordica", "Odosicyos" und "Tricyclandra". Der extremste Vertreter ist "Dendrosicyos".

Die Ranken sind modifizierte Sprossachsen und das deutlichste abgeleitete Merkmal der Familie. Sie kommen bei allen außer einigen stark abgewandelten, holzigen Arten vor. Bei einigen dieser Arten ("Acanthosicyos horridus", "Acanthosicyos naudiana") sind sie zu Dornen umgewandelt, bei anderen ("Citrullus ecirrhosus", "Dendrosicyos socotranus", "Ecballium elaterium", "Melancium campestre", "Myrmecosicyos messorius" und "Trochomeria polymorpha") fehlen sie ganz. Meist wird zwischen drei Rankentypen unterschieden:

Die Laubblätter sind wechselständig an der Sprossachse angeordnet in einer Schraube mit 2/5-Phyllotaxis. Laubblätter sind in Blattstiel und Blattspreite gegliedert. Im Blattstiel sind die Leitbündel halbmond- oder ringförmig angeordnet. Die Leitbündel sind ungleich, die größeren sind bikollateral. Die Blattspreiten sind einfach und leicht bis tief drei- bis siebenfach handförmig gelappt. Auch die Aderung ist handförmig. Sukkulente Blätter sind selten, auch bei den xerophytischen Arten. "Xerosicyos" besitzt große wasserspeichernde Zellen im Mesophyll und ist eine CAM-Pflanze. Bei den meisten Vertretern in trocken-heißen Gebieten wird eine Hitzeschädigung der Blätter durch hohe Transpirationsraten verhindert. Bei einigen Arten finden sich auf den Blättern extraflorale Nektarien. Nebenblätter fehlen in der Regel. Bei "Acanthosicyos" sind Nebenblätter vorhanden und in photosynthetisch aktive Dornen umgewandelt.

Die Spaltöffnungen sind vorwiegend anomocytisch, ihnen fehlen die Begleitzellen.

Die Blüten der meisten Vertreter sind groß und auffällig und werden von Insekten bestäubt. Einige Gattungen wie "Echinocystis" und "Sechium" bilden kleine, unauffällige Blüten. Die Blüten aller Vertreter stehen in den Blattachseln, entweder einzeln oder in Blütenständen. Häufige Farben sind weiß und gelb, seltener sind andere Farben wie rot bei "Gurania".

Die Blüten sind meist eingeschlechtig, besitzen also entweder männliche oder weibliche Organe. Die Blütenanlagen sind jeweils zwittrig angelegt, es entwickeln sich jedoch nur die männlichen beziehungsweise die weiblichen Organanlagen weiter. In der männlichen (staminaten) Blüte ist jeweils ein Stempel-Rudiment (Pistillodium) erhalten, in der weiblichen Staubblatt-Rudimente (Staminodien). Die Pflanzen sind dabei je nach Art oder Gattung einhäusig (beide Geschlechter an einer Pflanze) oder zweihäusig (nur je ein Geschlecht an einer Pflanze). Von den etwa 800 Arten sind rund 460 einhäusig, 340 zweihäusig. Auf weitere, seltene Geschlechtsverteilungen wird weiter unten eingegangen.

Die Blüten besitzen einen ausgeprägten Blütenbecher (Hypanthium), der becher- bis glockenförmig ist und aus der Blütenachse und/oder aus den verwachsenen Basalteilen von Kelch und Krone gebildet wird. Die Blüten sind meist fünfzählig. Die beiden Kreise der doppelten Blütenhülle bestehen meist aus je fünf, selten zwischen drei und sechs, Blütenblättern. Die Kelchblätter sind verwachsen. Die Kronblätter sind frei oder verwachsen, meist radiärsymmetrisch und bilden eine rad- oder glockenförmige Krone.

Die Staubblätter sind monothezisch (besitzen nur eine Theka), setzen am Hypanthium an und stehen alternierend zu den Kronblättern. Die Grundzahl der Staubblätter ist fünf. Manche Gattungen wie "Fevillea" haben fünf freie Staubblätter, bei manchen sind alle Staubblätter verwachsen wie bei "Cyclanthera". Andere Gattungen wie Gurken ("Cucumis") haben zwei verwachsene Paare und ein freies Staubblatt, sodass der Eindruck von drei Staubblättern entsteht. Diese drei sind meist über ihre Antheren miteinander verbunden.

Der Fruchtknoten ist unterständig. Er besteht aus meist drei verwachsenen Fruchtblättern, seltener ein bis fünf. Die Zahl der Griffel ist stark konserviert: bei den Nhandiroboideae sind sie stets frei und meist drei an der Zahl, selten zwei. Die Cucurbitoideae besitzen einen Griffel mit zwei, drei oder fünf Narben. Diese sind meist vergrößert und ähneln oft den Staubblättern, wohl um Bestäuber anzulocken. Der Fruchtknoten ist nicht in Fächer unterteilt, die Scheidewände sind aufgelöst und die Plazenten, an denen die Samenanlagen sitzen, befinden sich an der Fruchtwand (parietal). Bei den Nhandiroboideae ist die Zahl der Samenanlagen meist gering, die Samenanlagen sind stets hängend. Die Cucurbitoideae besitzen meist große Plazenten mit manchmal sehr vielen Samenanlagen, "Sechium edule" jedoch nur eine. Diese stehen meist aufrecht oder waagrecht, selten hängend. Die Samenanlagen in der ganzen Familie sind crassinucellat, anatrop und bitegmisch. Der Nucellus ist flaschenförmig, was ein charakteristisches Familienmerkmal darstellt. Der Embryosack entwickelt sich nach dem Polygonum-Typ.

Die Blüten besitzen häufig Nektarien, durch die die bestäubenden Insekten angelockt werden. Sie befinden sich an der Basis des Hypanthiums: In weiblichen Blüten bildet das Nektarium einen durchgehenden Ring um die Basis des Griffels, während es bei den männlichen Blüten zusammen mit dem Pistillodium einen knopfartigen Hügel im Zentrum der Blüte bildet.

Die Pollenkörner sind innerhalb der Familie recht vielgestaltig. Die Apertur kann tri- bis mehrfach zonocolpat, colporat, tri- bis mehrfach porat oder pantoporat sein. Die Oberfläche der Exine ist netzförmig, regelmäßig granuliert oder mit Stacheln versehen.

Der Pollen wird in Form von Einzelkörnern, seltener in Pollentetraden ausgebreitet. Die Nhandiroboideae besitzen stets tricolpate Pollenkörner mit einem Durchmesser von unter 40 Mikrometern. Ihre äußere Schicht (Exine) ist meist gestreift (striat), selten netzförmig (reticulat). Die Cucurbitoideae besitzen mit bis zu 200 Mikrometern ungewöhnlich große Pollenkörnern, die Exine ist netzförmig oder stachelig (echinat), die Aperturen sind porat oder colpat. Zwei Triben können anhand der Pollenkörner identifiziert werden: die Cucurbiteae besitzen echinate Pollenkörner, die drei bis viele Keimporen besitzen (triporat bis stephanoporat sind); die Sicyeae besitzen fein stachelige Pollenkörner mit Keimfalten (colpat) oder gemischten Poren/Falten (colporat). Bei den übrigen Triben überwiegen tricolporate Pollenkörner.

Die Früchte sind vielfach Beeren, häufig mit einer harten Schale, sogenannte Panzerbeeren. Gestalt und Oberfläche sind dabei sehr variabel. Sie sind kugelig bis länglich. Die Flaschenkürbisse ("Lagenaria") sind rund, birnen-, flaschenförmig oder länglich. Bei Kürbissen kann die Oberfläche gestreift, gefleckt oder einfarbig sein, die Oberfläche glatt, gerunzelt oder warzig. Die Früchte des Riesen-Kürbis ("Cucurbita maxima") sind die größten des Pflanzenreichs: am 29. September 2007 wurde in Massachusetts ein 766 kg schwerer Kürbis gewogen. Die meisten Arten mit Panzerbeeren sind in saisonal trockenen Gebieten heimisch: hier ermöglichen die hartschaligen, wasserspeichernden Früchte den Samen eine längere Reifezeit, auch wenn der Rest der Mutterpflanze bereits verwelkt ist. Die meisten bleiben bei der Reife geschlossen. Bei den Gattungen "Momordica" und "Cyclanthera" öffnen sich die Früchte. Die Früchte der Zaunrüben ("Bryonia") sind rund fünf Millimeter groß, kugelig und grün, rot oder schwarz.

Bei der Spritzgurke ("Ecballium elaterium") und bei der Explodiergurke ("Cyclanthera brachystachya") lösen sich die Früchte vom Stiel und stoßen das Fruchtinnere, das unter Druck steht, explosionsartig aus. "Cucumis humifructus" verbirgt die Blüten nach der Befruchtung in der Erde (Geokarpie), wo die Früchte reifen und vom Erdferkel ("Orycteropus afer") ausgegraben und verzehrt werden, das so die Samen ausbreitet. Auch die amerikanische Gattung "Apatzingania" hat geokarpe Früchte.

Bei den Nhandirobideae kommen vorwiegend Kapselfrüchte vor, die geflügelte Samen entlassen.
Die Samen sind meist zu vielen, oft zu hunderten in einer Frucht enthalten, sehr selten ist nur ein einzelner pro Frucht. Die Samen sind meist flach, in seltenen Fällen geflügelt. Unter der Samenschale befindet sich ein kollabiertes Perisperm. Ein Endosperm fehlt oder ist spärlich vorhanden. Der Embryo ist sehr ölreich, seine beiden Keimblätter füllen den Großteil des Samens aus.

Größe, Form und Farbe sind sehr vielfältig. Den größten ungeflügelten Samen bildet "Hodgsonia" mit 7 Zentimeter Länge, den kleinsten die Zaunrüben ("Bryonia") mit manchmal nur 3 Millimeter Durchmesser.

Die vorherrschende Chromosomengrundzahl beträgt x = 12, sie reicht jedoch von 7 bis 24. Die Cucurbiteae haben eine fixierte Polyploidie mit einem Chromosomensatz von n = 20. Auch in anderen Triben kommen recht häufig Polyploidie und Aneuploidie vor. Die gesamte Familie ist durch eine 35 Basenpaare lange Inversion im Bereich zwischen den Genen für Leucin-tRNA und Phenylalanin-tRNA gekennzeichnet, die bei den übrigen Familien der Ordnung Cucurbitales nicht vorkommt, also eine Synapomorphie darstellt. Bei "Cucurbita digitata" und bei "Neoalsomitra" ist diese Inversion wieder – unabhängig voneinander – umgekehrt worden.

Das Genom der Cucurbitaceae ist im Vergleich zu anderen Familien ungewöhnlich labil. So kommen hier zwei der bekannten sechs Fälle vor, bei denen das ansonsten plastidäre Gen "rbc"L (für die große Untereinheit der RuBisCO) in das Mitochondrien-Genom transferiert ist: bei der Gurke ("Cucumis sativus") und bei den Kürbissen ("Cucurbita pepo", "Cucurbita maxima"), dies jedoch unabhängig voneinander.

Die Gattung "Cucumis" verfügt auch über die größten bekannten Mitochondrien-Genome, die 1500 kb bei der Gurke und 2400 kb bei der Zuckermelone ("Cucumis melo") groß sind. Bei beiden Arten erfolgt die mitochondriale Vererbung über den Vater (paternal), nicht wie sonst üblich über die Mutter.

Die für die Familie charakteristischsten Inhaltsstoffe sind die Bitterstoffe aus der Gruppe der Cucurbitacine. Dies sind tetrazyklische Triterpene und kommen glykosyliert oder als Aglykon vor. Sie sind vor allem in den Früchten vorhanden, mit Ausnahme von Zuchtformen, die auf geringe Bitterkeit selektiert wurden. Samen sind stets frei von Cucurbitacin.<ref name="Frohne/Jensen"></ref>

Neben den Cucurbitacinen kommen ebenfalls bittere Triterpen-Saponine vor, die auf den Grundgerüsten Dammaran, Cucurbitan und Lanostan beruhen. Besonders reich an Saponinen sind die Gattungen "Luffa" und "Trichosanthes".
Die Samen sind generell reich an Ölen. Bei den kommerziell genutzten Arten variiert der Ölgehalt zwischen 38 und 45 Prozent. Die häufigsten Fettsäuren sind Palmitinsäure, Stearinsäure, Ölsäure, Linolsäure und Linolensäure, die zusammen mehr als 80 Prozent der Fettsäuren ausmachen. Daneben sind Phosphatidylcholin, Lysophosphatidyl-Ethanolamin, Phosphatidyl-Inositol und Phosphatidylethanolamin, Sterol-Glykoside, Cerebrosid und Diphosphatidyl-Glycerol häufig. In "Trichosanthes"-Arten ist Punicasäure, eine konjugierte Triensäure, häufig, in Momordica α-Elaeostearinsäure. In Summe wurden in Samen der Kürbisgewächse über 150 Fettsäuren identifiziert.

In Samen und Früchten kommen verschiedentlich nicht-proteinogene Aminosäuren vor, besonders Citrullin. Die Speicherproteine in den Samen sind vorwiegend Globuline.

Im Phloem transportieren die Kürbisgewächse als Kohlenhydrate außer Saccharose auch die Vertreter der Raffinose-Familie, sie werden deshalb zu den symplastischen Phloem-Beladern gerechnet.

Samen können bei einigen Arten, wie bei Chayote ("Sechium edule"), bereits in der Frucht auskeimen (Viviparie). Bei vielen Arten gibt es eine Samen-Dormanz, die jedoch bereits durch einige Wochen bei kühlen Temperaturen gebrochen wird. Für die Keimung sind Dunkelheit und hohe Temperaturen (meist 25 bis 30 °C) nötig. Die Keimung erfolgt epigäisch, das heißt, dass die Keimblätter photosynthetisch aktiv sind. Sie sind meist querelliptisch.

Kürbisgewächse wachsen bei hohen Temperaturen sehr rasch. Ein wildes Exemplar von "Cucurbita foetidissima" bildete in einer Saison 360 Sprosse in einem Kreis von 12 Meter Radius und einer Gesamtlänge von 2000 Meter. Der Flaschenkürbis wächst bis zu 60 Zentimeter in 24 Stunden. Bei den großfrüchtigen, rankenden Pflanzen wächst die Blattfläche exponentiell bis zum Zeitpunkt des Fruchtansatzes.

Die Geschlechtigkeit der Kürbisgewächse ist äußerst vielfältig und variiert teilweise auch innerhalb einer Art. Die ursprüngliche Geschlechtsverteilung innerhalb der Familie ist die Zweihäusigkeit (Diözie). Monözie und andere Formen sind davon abgeleitet. Androdiözie (männliche Pflanzen und zwittrige Pflanzen) kommt bei Pflanzen äußerst selten vor, etwa bei "Schizopepon bryoniifolius". Bei den Kürbisgewächsen äußerst selten sind zwittrige Blüten, die nur von wenigen Varietäten von Nutzpflanzen bekannt sind, etwa 'Satputia' des Schwammkürbis "Luffa acutangula".

Bei den wenigen untersuchten Arten wird die Geschlechtsverteilung durch ein oder zwei Gene gesteuert, von denen es jeweils zwei oder drei Allele gibt. Je nach Allel-Kombination gibt es, etwa bei "Luffa acutangula" monözische, andromonözische (männliche und zwittrige Blüten), andrözische (nur männliche), gynomonözische (weibliche und zwittrige), gynözische (nur weibliche Blüten) und zwittrige Individuen. Bei manchen Arten ändern Individuen ihr Geschlecht im Laufe ihrer Entwicklung, etwa in "Gurania".

In den Gattungen "Coccinia" und "Trichosanthes" gibt es – zum Teil heteromorphe – Geschlechtschromosomen, die das Geschlecht der Einzelpflanzen bestimmen. Bei "Coccinia grandis" sind, ähnlich wie bei Menschen, Individuen mit zwei X-Chromosomen weiblich. Das größere Y-Chromosom bestimmt das männliche Geschlecht und dominiert, sodass auch tetraploide Individuen mit drei X- und einem Y-Chromosom männlich sind.

Die Ausbildung des Geschlechtes unterliegt nicht nur genetischer Regulation, sondern wird auch durch den Hormonhaushalt, durch Tageslänge und Temperatur beeinflusst. Bei hohen Temperaturen werden männliche Blüten gefördert, bei tiefen weibliche. Kurztagsbedingungen (acht Stunden Licht) fördern vielfach weibliche Blüten. Generell werden weibliche Blüten unter Bedingungen gefördert, bei denen Kohlenhydrate akkumuliert werden.

Diese hohe Variabilität wird in der Züchtung ausgenützt. So sind die meisten in Europa angebauten Gurken-Sorten gynözisch, bilden also nur weibliche Blüten, die auch ohne Bestäubung Früchte ansetzen (Parthenokarpie). Zur Samenzucht wird mit Hilfe von Silber-Ionen (einem Ethylen-Inhibitor) die Bildung von männlichen Blüten angeregt.

Die Blüten öffnen sich meist am frühen Morgen, die Bestäubung ist meist bis Mittag erfolgt. Manche weißblütige Arten, wie der Flaschenkürbis, öffnen sich am Abend und bleiben bis zum Morgen geöffnet. Diese Arten werden meist von Schwärmern bestäubt. Die tagsüber blühenden Arten werden überwiegend von Bienen und anderen Insekten bestäubt, die Pollen und Nektar sammeln. Darunter gibt es auch einige Spezialisten, so besuchen die Bienen-Gattungen "Peponapis" und "Xenoglossa" vorwiegend Blüten der Gattung "Cucurbita".

Das Endosperm entwickelt sich nukleär (durch freie Kernteilung ohne Zellbildung). Das junge Endosperm bildet ein Haustorium in Richtung Chalaza. Dieses bleibt coenocytisch (keine Zellbildung, also freie Zellkerne) oder wird später zellulär. Die Zellen des Endosperms sind im späteren, zelligen Stadium zum Teil hochpolyploid, bei "Echinocystis lobata" wurde 3072n gezählt. Im reifen Samen ist das Endosperm stark reduziert und weitgehend degeneriert. Häufig umfasst es nur zwei Zellschichten.

Bei einigen Arten kommt gelegentlich Polyembryonie vor, es werden in einem Samen mehrere Embryos gebildet. Bei "Hodgsonia macrocarpa" kommt sie stets vor.

Die Samenschale wird bei den Kürbisgewächsen nur vom äußeren Integument der Samenanlage gebildet, das innere degeneriert. Sie besteht von außen nach innen aus einer häufig dreischichtigen Epidermis, einer Hypodermis, einem Sklerenchym, einem Aerenchym und einem Chlorenchym. Dieser fünfschichtige Aufbau ist für die Familie charakteristisch, teilweise gibt es Abwandlungen davon.

Für die Entwicklung der Früchte ist in der Regel eine erfolgreiche Befruchtung nötig. Am Ende der Wachstumsperiode können sich bei etlichen Arten jedoch die letzten Blüten ohne Befruchtung entwickeln, wenn bis dahin noch keine Blüte der Pflanze befruchtet wurde. Auf der anderen Seite wird durch sich entwickelnde Früchte vielfach die Bildung weiterer weiblicher Blüten unterdrückt. Die Form der Früchte, die bei den Kürbisgewächsen sehr variabel ist, ist häufig bereits in der Form des Fruchtknotens vorgebildet.

Die Familie der Cucurbitaceae wurde 1789 durch Antoine Laurent de Jussieu in "Genera Plantarum", Seiten 393–394 aufgestellt. Typusgattung ist "Cucurbita" Synonyme für Cucurbitaceae nom. cons. sind: Bryoniaceae , Nhandirobaceae nom. illeg., Zanoniaceae , Cyclantheraceae .

Die nächstverwandten Familien der Kürbisgewächse (Cucurbitaceae) innerhalb der Ordnung Cucurbitales sind die Begoniengewächse (Begoniaceae), Scheinhanfgewächse (Datiscaceae) und Tetramelaceae.
Die traditionelle Systematik der Kürbisgewächse beruht vorwiegend auf Merkmalen des Androeceums, des Gynoeceums, der Ranken, seit Mitte des 20. Jahrhunderts auch auf Pollenstruktur und dem Bau der Samenschale. Eine der neuesten Klassifikationen der Familie ist die von C. Jeffrey (2005). Folgende Aufstellung ist aus Kocyan u. a. (2007) übernommen, die das System von Jeffrey durch neue Gattungen erweitert hatten, so dass die Familie 126 Gattungen umfasst. Änderungen gegenüber Kocyan u. a. sind gekennzeichnet:


Diese auf morphologischen Merkmalen basierende Gliederung konnte von Kocyan u. a. 2007 in einer phylogenetischen Untersuchung, die 123 der 130 Gattungen umfasste, bis auf Tribus-Ebene im Wesentlichen bestätigen, die Subtribus sind allerdings künstliche Unterteilungen. Aus der Arbeit von Kocyan u. a. kann folgendes, auf die Triben vereinfachtes Kladogramm abgeleitet werden, wobei Wellenlinien Triben anzeigen, die paraphyletisch im Hinblick auf die anderen Äste derselben Klade sind:
Unter den von Jeffrey 2005 anerkannten Triben ist Jollifeae hochgradig polyphyletisch. Die Gattungen der Jollifeae bilden eine Gruppe an der Basis der Cucurbitoideae. Innerhalb dieser Gruppe sind einige Gattungen anderer Triben eingeordnet ("Cogniauxia" von den Benincaseae, "Ampelosicyos" und "Tricyclandra" von den Trichosantheae). Die übrigen Trichosantheae bilden eine Gruppe an der Basis der Sicyeae. Einige Gattungen sind nach den Ergebnissen von Kocyan u. a. para- oder polyphyletisch.

Die Nutzung der Kürbisgewächse durch den Menschen ist sehr vielfältig. Die Früchte werden reif oder unreif geerntet und gebacken (Kürbisse), frittiert (Bittermelone), gekocht (Wachskürbis, Schlangenhaargurke), gefüllt ("Cyclanthera pedata"), eingelegt (Gurke), kandiert (Wassermelone, Wachskürbis, "Cucurbita ficifolia") oder frisch als Salat (Gurke) oder Dessert (Zuckermelone, Wassermelone) gegessen. Zur Haltbarmachung können sie zu Konserven verarbeitet, eingefroren oder eingelegt werden. Der Fruchtsaft einiger Arten wird zu alkoholischen Getränken vergoren, in Japan wird etwa aus der Zuckermelone Likör hergestellt.

Die Samen sind aufgrund ihres hohen Ölgehalts sehr nahrhaft. In Afrika können die Samen der Wassermelone, der Naras und anderer Arten einen wesentlichen Bestandteil der Nahrung ausmachen. In Mexiko und Zentralamerika, aber auch Teilen Mittel- und Südosteuropas werden Kürbissamen als Snack verspeist. Die Auslese von „nackten“, also dünnschaligen Samen hat die Beliebtheit auch in anderen Ländern gefördert. Aus den Samen folgender Arten wird Öl als Nahrung, zur Beleuchtung, Seifenherstellung und anderem hergestellt: Gartenkürbis (darunter auch der Steirische Ölkürbis), Wassermelone, Luffa, "Fevillea cordifolia", "Hodgsonia macrocarpa" und "Cucumeropsis mannii". Neben Früchten und Samen werden auch Blüten, Blätter, Sprossspitzen und Wurzeln (Chayote) zubereitet.

Die Früchte des Flaschenkürbis werden ausgehöhlt zu Vorratsgefäßen, Flaschen, aber auch Tabak-Pfeifen, Musikinstrumenten, Penis-Futteralen, Masken, Bojen für Fischnetze, Baby-Rasseln usw. verarbeitet. Auch die Früchte des Wachskürbis und verschiedener "Cucurbita"-Arten werden als Vorratsgefäße verwendet. Der Schwammkürbis und "Momordica angustisepala" werden als Schwamm verwendet.

Die medizinische Verwendung der Kürbisgewächse ist äußerst vielseitig, sowohl bei den verwendeten Arten als auch bei den Anwendungsgebieten. Guha und Sen geben in ihrem auf Indien konzentrierten tabellarischen Überblick 79 Anwendungsgebiete an. Weit verbreitet sind Anwendungen als Abführmittel, Emetikum und Wurmmittel. Die Wirkung ist meist auf den Gehalt an Cucurbitacinen zurückzuführen. Die Wurzeln von "Trichosanthes dioica" werden als Gift verwendet.

Der Artikel beruht überwiegend auf folgenden Quellen:




</doc>
<doc id="2846" url="https://de.wikipedia.org/wiki?curid=2846" title="Kreuzblütler">
Kreuzblütler

Die Kreuzblütler (Brassicaceae oder Cruciferae), auch Kreuzblütengewächse genannt, sind eine Pflanzenfamilie in der Ordnung der Kreuzblütlerartigen (Brassicales). Die Familie enthält weltweit etwa 336 bis 419 Gattungen mit etwa 3.000 bis 4.130 Arten. Sie ist durch viele Kulturpflanzen von großer wirtschaftlicher Bedeutung.

Einige Wildpflanzen wie Acker-Schmalwand, Hirtentäschelkraut, Acker-Hellerkraut sind vielen bekannt und als „Unkräuter“ nicht beliebt.

Zur Familie der Kreuzblütler zählen viele wichtige Kulturpflanzen. So zählen vom Menschen entwickelte Kulturformen des Gemüsekohls ("Brassica oleracea"), wie beispielsweise Weißkohl, Rotkohl, Brokkoli, Blumenkohl, Rosenkohl und Kohlrabi dazu. Schwarzer und Weißer Senf ("Brassica nigra" und "Sinapis alba"), Pak Choi, Chinakohl ("Brassica rapa" subsp. "chinensis"), Weiße Rübe ("Brassica rapa" subsp. "rapa"), Steckrübe ("Brassica napus" subsp. "rapifera"), Rübse ("Brassica rapa" subsp. "oleifera") und Raps ("Brassica napus" subsp. "napus") gehören auch zur Gattung "Brassica". Rettich und Radieschen sind Vertreter der Gattung "Raphanus". Bekannt sind der Meerrettich ("Armoracia rusticana"), auch "Kren" genannt, die Kresse und der Wasabi ("Eutrema japonicum"). Einige Gattungen enthalten also Kulturpflanzen, die Öllieferanten und Gewürzpflanzen sind oder als Gemüse und Salate gegessen werden.

Angebaut werden einige Arten dieser Familie auch als Grün- und Trockenfutter. Einige Arten werden zur Gründüngung verwendet.

Die Familie enthält auch einige bekannte Zierpflanzen wie beispielsweise Goldlack, Blaukissen, Nachtviole, Levkojen.

Die Arten in dieser Familie wachsen meist als ein-, zweijährige und ausdauernde krautige Pflanzen. Nur wenige Arten verholzen und wachsen als Sträucher ("Alyssum spinosum" oder die südafrikanische "Heliophila glauca"); "Heliophila scandens" bildet Lianen.

Die meist wechselständig und spiralig in einer grundständigen Blattrosette oder am Stängel verteilt angeordneten Laubblätter sind ungeteilt oder zusammengesetzt. Nicht selten sind die Blätter borstig behaart. Die Form und Dichte der stets einzelligen Haare (Trichome) sind wichtige Bestimmungsmerkmale; sie sind unverzweigt oder besitzen eine Fülle von unterschiedlichen Verzweigungstypen. Der Blattgrund besitzt Blattöhrchen oder ist stängelumfassend. Nebenblätter fehlen.

Der traubige oder doppeltraubige, selten trugdoldige Blütenstand enthält meist keine Tragblätter.
Die zwittrigen Blüten sind vierzählig und meist mehr oder weniger radiärsymmetrisch. Der Name der Kreuzblütler leitet sich von der Anordnung der vier Kronblätter der Blüte ab. Sie stehen in der Form eines Kreuzes, wobei oft eines der Kronblätter etwas größer als die übrigen drei ist. Die vier freien Kelchblätter sind in zwei Kreisen angeordnet. Es sind meist vier freie Kronblätter vorhanden. Nur sehr wenigen Arten fehlt dieses Merkmal der Vierzähligkeit: Meist sind vor allem bei diesen Arten die Blüten leicht zygomorph und in einem schirmförmigen Blütenstand so angeordnet, dass dieser die Funktion eines Pseudanthiums übernimmt. Dies ist zum Beispiel bei "Iberis" der Fall.

Ein weiteres wichtiges Erkennungsmerkmal der Kreuzblütler sind die zwei Kreise mit insgesamt sechs Staubblättern. Der äußere Kreis besteht aus nur zwei kurzen, der innere Kreis aus vier langen Staubblättern. Der oberständige Fruchtknoten wird vermutlich aus vier Fruchtblättern gebildet: zwei fertile und zwei sterile. Es wird eine „falsche Scheidewand“ gebildet, die bei der reifen Frucht erhalten bleibt; besonders schön ist diese Scheidewand beim Silberblatt ("Lunaria rediviva") zu sehen. Die sterilen Fruchtblätter fallen bei vollreifen Früchten ab. Der Griffel ist mehr oder weniger reduziert und endet in ein oder zwei Narben. Die Bestäubung erfolgt meist durch Insekten (Entomophilie) oder selten durch den Wind (Anemophilie).

Die Blütenformel lautet:
formula_1

Die Früchte werden Schoten genannt, wenn sie mindestens dreimal so lang wie breit sind oder Schötchen, wenn sie gedrungener sind. Bei manchen Taxa werden Gliederschoten, die in einsamige Teilfrüchte zerfallen, oder geschlossen bleibende, einsamige Nussschötchen ausgebildet. Blütenansatz und Samenbildung überlappen sich oft zeitlich: Während unten schon Samen gebildet werden, blüht der obere Teil des Blütenstandes noch.

Die je nach Art viel bis kein Endosperm enthaltenden Samen sind klein bis mittelgroß; beispielsweise bei "Matthiola" und "Isatis" besitzen sie Flügel. Der Embryo ist gut ausdifferenziert.

Es sind sogenannte Myrosinzellen vorhanden, die das Enzym Myrosinase, das ist eine Thioglukosidase, und Senfölglykoside (Glukosinolate) enthalten. Die Spaltung der Senfölglykoside erzeugt Senföle (Alkylisothiocyanate), Rhodanide (Thiocyanate), Nitrile und Goitrine (Oxazolidinthione).

Typisch ist der durch Senfölglykoside verursachte kohlartige Geruch und Geschmack.

Eine weitere Besonderheit ist, dass bekannte Nutzpflanzen wie Kohl, Broccoli und Raps, möglicherweise alle Kreuzblütler Brommethan als Gas an die Umwelt abgeben und somit einen deutlichen Beitrag zur Gesamtemission dieses Giftes beitragen. Ob es sich dabei nur um einen Abwehr- oder um einen Signalstoff handelt, bleibt noch zu klären. Etwa 15 % der weltweiten Emissionen an Brommethan sollen auf das Konto dieser Pflanzenfamilie gehen.

Das fette Öl der Rapspflanze wird zur Herstellung von Biokraftstoffen sowie (wie auch Senföl) als Speisefett verwendet.

Die ersten Arten, die man dieser Familie zurechnen kann, entstanden etwa vor 37 Millionen Jahre in einem warmen und feuchten Klima. Die größte adaptive Radiation innerhalb der Familie erfolgte im Oligozän, während es kühler wurde. Eine Verdopplung des Genoms verbesserte die Voraussetzung, sich an einen solchen Klimawandel anzupassen.

Kreuzblütler können von der Dauerfrostzone bis zu den Tropen weltweit gefunden werden (Kosmopoliten). Ihren Verbreitungsschwerpunkt haben sie in den gemäßigten Zonen der Nordhalbkugel und hier wiederum in den Mittelmeerländern und in Südwest- und Zentralasien. 102 Gattungen mit 412 Arten sind in China beheimatet, davon kommen 115 nur dort vor.

Der Familienname Cruciferae wurde 1789 von Antoine-Laurent de Jussieu in "Genera Plantarum", Seite 237 veröffentlicht. Der Familienname Brassicaceae wurde 1835 von Gilbert Thomas Burnett in "Outlines of Botany", Seite 854, 1093 und 1123 eingeführt. Typusgattung ist ("Brassica" ). Synonyme für Brassicaceae sind: Cruciferae , nom. cons., Raphanaceae , Stanleyaceae und Thlaspiaceae . Am nächsten verwandt sind die Familien Cleomaceae und Kaperngewächse (Capparidaceae).

In der Familie der Kreuzblütler (Brassicaceae) gibt es etwa 336 (bis 419) Gattungen mit 3000 bis 4130 Arten (die größte Anzahl an Gattungen und Arten listet Judd et al. 1999).

Die Familie wird in etwa (25 bis) 44 Tribus gegliedert, hier die Gliederung nach Marcus Koch & Ihsan Ali Al-Shehbaz 2009 und Suzanne I. Warwick, Klaus Mummenhoff, C. A. Sauder, M. A. Koch & Ihsan A. Al-Shehbaz, 2010:




















































</doc>
<doc id="2847" url="https://de.wikipedia.org/wiki?curid=2847" title="Kardengewächse">
Kardengewächse

Die Kardengewächse (Dipsacoideae) sind eine Unterfamilie in der Pflanzenfamilie der Geißblattgewächse (Caprifoliaceae) innerhalb der Ordnung der Kardenartigen (Dipsacales). Früher wurden die Dipsacoideae als eigene Familie der Dipsacaceae Juss. betrachtet.

Es handelt sich um ein- oder zweijährige bzw. ausdauernde krautige Pflanzen sowie seltener um Halbsträucher. Die stets gegenständigen Laubblätter sind meistens ungeteilt. Nebenblätter besitzen sie keine.

Ein besonders charakteristisches Merkmal der Kardengewächse ist der köpfchenförmige Blütenstand, der sehr dem der Korbblütler ähnelt, und wie dieser von Hüllblättern umgeben ist (→ Pseudanthium). Der Aufbau der Blüten unterscheidet sich jedoch erheblich: Die zwittrigen Blüten sind vier- oder fünfzählig, mit doppelten Perianth. Die vier- oder fünfzipfeligen, am Rand der Köpfchen oft stark zygomorphen Blüten sind oft von einem Außenkelch umgeben. Der Außenkelch besteht aus zwei miteinander verwachsenen Vorblättern. Der Kelch und der Außenkelch sind fast stets trockenhäutig und borstig. Die Kronblätter sind miteinander verwachsen. Die zwei Fruchtblätter sind zu einem unterständigen Fruchtknoten verwachsen. Die meist vier (selten zwei bis drei) Staubblätter sind nicht zu einer Röhre verwachsen. Sie bilden einsamige Schließfrüchte, Achänen genannt.

Die Mehrzahl der Arten findet man in trockenen oder zumindest periodisch trockenen, offenen Gebieten wie Steppen oder Trockenrasen.

Die Unterfamilie Dipsacoideae wurde 1836 durch Amos Eaton in "A Botanical Dictionary", 4. Auflage, S. 36.
unter der Bezeichnung „Dipsaceae“ aufgestellt. Typusgattung ist "Dipsacus" Ein Synonym für Dipsacoideae ist Dipsacaceae 

Die Arten sind von den gemäßigten bis subtropischen Zonen Eurasiens und Afrikas sowie im tropischen und südlichen Afrika verbreitet. Verbreitungsschwerpunkt ist der Mittelmeerraum und Kleinasien.

In der Unterfamilie der Kardengewächse (Dipsacoideae ) gab es früher 11, seit 2013 gibt es 14 Gattungen mit etwa 290 Arten:



</doc>
<doc id="2849" url="https://de.wikipedia.org/wiki?curid=2849" title="Liste von Kanälen">
Liste von Kanälen

Die Liste von Kanälen enthält einen tabellarischen und sortierbaren Überblick mit Grundinformationen zu den Kanälen weltweit. Sie führt einen Teil der bestehenden und ehemaligen Kanäle auf und erhebt "keinen" Anspruch auf Vollständigkeit.

Ein Kanal ist dann schiffbar, wenn dies der Definition der Schiffbarkeit entspricht, dies bezieht sich also nicht auf die Möglichkeit, dort kleinere Sportboote gebrauchen zu können.

Der Beginn und das Ende des Kanals werden bevorzugt mit Koordinaten eingegeben.


</doc>
<doc id="2850" url="https://de.wikipedia.org/wiki?curid=2850" title="Kalmus">
Kalmus

Kalmus steht für:

Kalmus ist der Familienname folgender Personen:

Siehe auch:



</doc>
<doc id="2854" url="https://de.wikipedia.org/wiki?curid=2854" title="Kohlrabi">
Kohlrabi

Der Kohlrabi ("Brassica oleracea" var. "gongylodes" L.), auch Oberkohlrabi, Oberrübe, Kohlrübe (Wien), Rübkohl (Schweiz) und Stängelrübe ist eine Gemüsepflanze. Er ist eine der vielen Zuchtformen des Gemüsekohls. Genutzt wird hier die verdickte, oberirdische Sprossachse (Sprossknolle).

Der Name leitet sich von den lateinischen Wörtern „caulis“ (Kohl) und „rapum“ (Rübe/Wurzelknollen) ab, bedeutet somit Kohlrübe. Oberrübe daher, da sie im Gegensatz zu anderen Rüben über der Erde wächst.

Kohlrabi ist eine zweijährige Pflanze, wobei im ersten Jahr die Sprossknolle gebildet wird, und im zweiten Jahr der Stängel mit einem verzweigten Blütenstand entsteht, aus dem sich Schoten bilden, die die Samen enthalten. Reife Samen zeigen schwarze volle Körner und lassen sich auf festem Untergrund nicht zerdrücken.

Die Knolle ist der gestauchte, verdickte Hauptspross der Pflanze. Sie entsteht über dem zweiten oder dritten Laubblatt durch primäres Dickenwachstum der Sprossachse (genaugenommen des Epikotyls). Die Form der Knolle kann kugelig, plattrund oder oval sein, die Farbe der Schale weißlich, weißgrün bis kräftig grün, rötlich oder violett. Der Durchmesser ist je nach Sorte zwischen 5 und 20 cm, oder noch mehr. Das Gewicht liegt je nach Sorte und Verwendung zwischen 100 g und über 8 kg pro Knolle. Einzelwerte können noch wesentlich darüber liegen.

Die Blätter sind lang gestielt, dunkelgrün, länglich eirund und mehr oder weniger stark gezähnt. Sie sind mit einer bläulich-weißen Wachsschicht überzogen. Die Pflanzen bilden eine Pfahlwurzel. 

Die Blütenbildung kann bereits im Zweiblattstadium durch längere Kältereize ausgelöst werden (Vernalisation); ebenso führen hohe Temperaturen zu einer Devernalisation.

100 g Frischsubstanz des essbaren Anteils der Knolle enthalten im Mittel 91,6 g Wasser, 1,9 g Protein, 0,1 g Fett, 3,8 g Kohlenhydrate und 1,4 g Ballaststoffe. An Mineralstoffen sind Kalium (380 mg), Calcium (70 mg), Phosphor (50 mg), Magnesium (45 mg) und Eisen (0,9 mg) zu nennen. An Vitaminen sind Vitamin C (65 mg), Vitamin A (Carotin, 0,2 mg), Vitamin B1 (0,05 mg), Vitamin B2 (0,05 mg) und Niacin (1,8 mg) vorhanden. Der physiologische Brennwert beträgt 103 kJ/100 g (24 kcal/100 g).

Der Geschmack des Kohlrabi beruht auf dem Gehalt an Zucker, Fruchtsäuren und Senfölglykosiden. Bei den Fruchtsäuren dominieren Äpfelsäure und Citronensäure deutlich.

Kohlrabiblätter haben gegenüber der Knolle einen rund doppelt so hohen Gehalt an Vitamin C, der Gehalt an Carotin beträgt das 100-fache, der an Calcium und Eisen das 10-fache.

Es gibt etliche Sorten, in Deutschland 30 weiße und 14 blaue Kohlrabi-Sorten. Es setzen sich vermehrt Hybrid-Sorten durch. Wichtige Eigenschaften sind: Ertrag, geringe Neigung der Knollen zum Verholzen und Platzen, Schnellwüchsigkeit und geringe Neigung zum Schossen. Die Sorten, die die größten Knollen liefern, sind „Gigant“ und „Superschmelz“. Die Sorten unterscheiden sich nach Farbe (weiß und blau) und der Anbauzeit; weiße Kohlrabisorten benötigen eine geringere Kulturzeit als blaue Sorten. Bei zu geringem Pflanzabstand bildet Kohlrabi, hauptsächlich aufgrund von Lichtmangel, zylindrische Knollenformen aus.

Frühsorten lassen sich schlecht lagern, die Lagerdauer beträgt lediglich zwei bis drei Wochen. Herbstkohlrabi können ohne Laub über mehrere Monate gelagert werden.

In der EU gibt es zwei Güteklassen:

Bezüglich des Bodens ist Kohlrabi empfindlich gegen größere Schwankungen der Bodenfeuchte, dies führt zu einem Aufplatzen der Knollen. Klimatisch hat Kohlrabi eine breite Amplitude und wächst in Mittel- und Westeuropa gut. Im Sommer und Herbst reichen niedrige Temperaturen aus, im Jugendstadium braucht er allerdings Wärme.

In der Fruchtfolge sind längere Anbaupausen zu anderen Kreuzblütlern anzustreben. Kurze Fruchtfolgen erhöhen die Gefahr von bodenübertragenen Krankheiten, besonders der Kohlhernie. Andere Ansprüche bezüglich der Vorfrucht stellt Kohlrabi nicht. Ebenso sind als Nachfrucht alle Gemüsearten geeignet. Kohlrabi stellt keine besonderen Anforderungen an die Stickstoff-Versorgung; überhöhte Stickstoff-Düngung kann leicht zu erhöhten Nitrat-Gehalten in der Knolle führen.

Beim Kohlrabi gibt es in Mitteleuropa Früh-, Sommer- und Herbstproduktion im Freiland, die für den Frischmarkt und die Verarbeitungsindustrie produzieren. Kohlrabi hat eine kurze Entwicklungszeit und wird daher häufig als Vor-, Zwischen- oder Nachfrucht angebaut. Mitteleuropäischer Anbau kann den Markt zwischen Mai und November mit Frischware versorgen. Der Anbau für den Frischmarkt erfolgt dabei überwiegend in gärtnerischen Gemüsebaubetrieben, der Anbau für die industrielle Verarbeitung hingegen zumeist in landwirtschaftlichen Betrieben (Feldgemüsebau).

Weit verbreitet ist die Anzucht von Jungpflanzen und die anschließende Pflanzung im Feld. Die Bestandsdichten variieren zwischen 16 und 17 Stück pro m² im Frühjahrsanbau und 8 bis 10 Stück pro m² bei den späteren Sorten. Im Frühjahr werden die Pflanzen zur Verfrühung teilweise einfach oder doppelt mit Folien abgedeckt. Die Erträge liegen im Frühjahrsanbau bei 20 bis 30 Tonnen pro Hektar, im Sommer und Herbst bei 45 bis 70 Tonnen pro Hektar. Die Ernte erfolgt bei Frischware per Hand. Große Kohlrabi für die industrielle Verarbeitung können mit Kopfkohlerntemaschinen mit speziellem Kohlrabi-Schneidwerk geborgen werden.

Der Anbau von Kohlrabi im Gewächshaus ist besonders in den Niederlanden, in Deutschland, der Schweiz und Österreich verbreitet. Im Gewächshaus erfolgt die Kultur auch für Ernten im Winter. An Krankheiten sind lediglich bei schlechter Belüftung Falscher Mehltau und Schwarzbeinigkeit von Bedeutung. Geerntet wird Gewächshauskohlrabi mit Laub, das den Frischegrad anzeigt.

Virenerkrankungen haben beim Kohlrabi keine wirtschaftliche Bedeutung, das Blumenkohlmosaikvirus kommt gelegentlich vor. Die Bakterien- und Pilzkrankheiten sind im Wesentlichen die gleichen wie bei Kopfkohl und Blumenkohl: besonders Kohlhernie und Falscher Mehltau. Vor allem bei Kohlrabi tritt der Pilz "Cylindrosporium concentricum" auf, der Blätter und Knolle befällt. Die Blätter fallen frühzeitig ab, das Gewebe wird graugrün, an der Knolle bilden sich schwarzgraue Flecken, später Kallusgewebe.

Unter den tierischen Schädlingen sind die gleichen wie bei Kopf- und Blumenkohl zu nennen. Bedeutsam im Freilandbau ist der Große Kohltriebrüssler ("Ceutorhynchus napi"). Die abgelegten Eier führen zum Aufplatzen der Knollen.

Die jungen Knollen werden geschält und in Scheiben oder Stücke geschnitten gekocht oder gedünstet – als Beilage, Püree, in Eintöpfen und Suppen, für Füllungen und Aufläufe. Junge Blätter können wie anderes Blattgemüse verwendet werden. In Teilen Frankreichs werden die Knollen gehobelt und wie Sauerkraut konserviert. Auch als Rohkost ist Kohlrabi geeignet.

Die Herkunft des Kohlrabi ist ungeklärt. Häufig genannte Gebiete der möglichen Domestikation sind der Mittelmeerraum und Mittelasien. Ebenso unklar ist der Zeitpunkt der Entstehung. 

Sichere Belege gibt es erst aus Europa im 16. Jahrhundert. Aus dieser Zeit gibt es eindeutig identifizierbare Zeichnungen in Kräuterbüchern. Aus dem 18. Jahrhundert stammen erste Hinweise auf plattrunde Knollen. Im deutschsprachigen Raum wurde der Kohlrabi besonders im 19. Jahrhundert verbreitet. Er gilt vielfach als typisch deutsches Gemüse, sodass der deutsche Name in etliche andere Sprachen übernommen wurde, so etwa ins Englische, Russische und Japanische.

In der "Fortsetzung des Allgemeinen teutschen Garten-Magazins" von 1815 werden von den weißen Kohlrabisorten der "kleine frühe (Wiener) Kohlrabi", der "große Glaskohlrabi" und der "gemeine grüne Kohlrabi" aufgezählt, von den blauen bzw. roten Sorten der "frühe blaue Glaskohlrabi" und der "späte blaue Kohlrabi". Heute sind die in Deutschland bekanntesten Sorten Wiesmoor weißer Trieb, Wiener blauer Glas und Blauer Speck.



</doc>
<doc id="2855" url="https://de.wikipedia.org/wiki?curid=2855" title="Kronos">
Kronos

Kronos () ist in der griechischen Mythologie der jüngste Sohn der Gaia ("Erde") und des Uranos ("Himmel"), Anführer der Titanen und Vater von Zeus. In der römischen Mythologie entspricht ihm Saturn(us).

Sein Name wurde in der antiken Volksetymologie schon sehr früh (in der Orphik) mit dem des Zeitgottes Chronos () gleichgesetzt, was aber etymologisch falsch ist; ursprünglich waren es zwei verschiedene Götter, die dann in manchen Überlieferungen miteinander verschmolzen wurden. Die Etymologie ist umstritten; man hat eine Ableitung von "kraíno" erwogen, dann wäre Kronos der „Vollender“. Auch wurde vermutet, dass der Name vorgriechischen Ursprungs sei und somit Kronos aus einer vorgriechischen Tradition übernommen wurde. Der Autor des 1962 gefundenen Derveni-Papyrus, einer der ältesten bekannten literarischen Schriften der griechischen Antike, bietet eine andere Etymologie, indem er Kronos von herleitet.

In der Frühzeit der Mythologie hatte Kronos noch keinen festen Platz in der Genealogie der Götter; von den verschiedenen Versionen des Mythos hat sich die von Hesiod überlieferte durchgesetzt, die Kronos zu einem Sohn von Uranos und Gaia macht.

Da Uranos seine Kinder – die Kyklopen und Hekatoncheiren – so sehr hasste, dass er sie in den Tartaros verbannte, brachte Gaia ihre weiteren Kinder – die Titanen – im Geheimen zur Welt. Sie stiftete schließlich Kronos an, den Vater mit einer Sichel zu entmannen.

Kronos wurde damit zum Herrscher der Welt und Begründer des Goldenen Zeitalters. Nach der Darstellung Hesiods ("Theogonie" 446ff.) wurde Kronos von seiner Schwester Rhea (Rheia) zum Gatten genommen. Aus Angst, selbst entmachtet zu werden, fraß er jedoch alle Kinder, die aus dieser Verbindung entstanden: Hestia, Demeter, Hera, Hades und Poseidon, die "Kroniden". Den jüngsten Sohn jedoch, Zeus, versteckte Rhea auf Anraten von Gaia und Uranos in der Höhle von Psychro im Dikti-Gebirge auf Kreta, während sie dem Kronos einen in eine Windel gewickelten Stein überreichte, den dieser verschlang, ohne den Betrug zu bemerken. So konnte Zeus ungestört heranwachsen. Später gelang es Zeus, seinen Vater mit List und Gewalt zu überwinden, worauf Kronos erst den Stein und dann seine verschlungenen Kinder ausspuckte. Den Stein stellte Zeus an der Kultstätte Pytho (Delphi) auf, damit er dort von den Sterblichen bestaunt werde.

Die Orphiker erzählten, dass Kronos eines Tages von dem damals aus den Eichen fließenden Honig berauscht dalag und so von Zeus gefesselt werden konnte. Anschließend brachte dieser ihn auf die „Insel der Seligen“, die Elysischen Gefilde, die am Rande des Erdkreises liegen, wo Kronos bis heute weile. Daher halte dort noch immer das Goldene Zeitalter an, das für den Rest der bekannten Welt mit seiner Entmachtung sein Ende gefunden habe. Nach der "Bibliotheke des Apollodor" war Metis, die erste Geliebte des Zeus, diesem bei der Entmachtung des Vaters behilflich, indem sie ihm den Trank reichte, der Kronos betäubte und ihn schließlich dazu zwang, alle zuvor verschlungenen Kinder wieder von sich zu geben.

Kronos war eine relativ schattenhafte Gestalt aus der Mythologie, die nur in sehr geringem Maße kultisch verehrt wurde. Allerdings gab es ein ihm zu Ehren gefeiertes ländliches Fest, die Kronien. Der von ihm ausgespuckte Stein wurde in Delphi verehrt; man salbte ihn täglich mit Öl und umwickelte ihn an Festtagen mit wollenen Binden. Er ist nicht zu verwechseln mit einem anderen ebenfalls in Delphi aufgestellten und verehrten Stein, dem Omphalos. Der Steinkult war in der Antike im Mittelmeerraum verbreitet.




</doc>
<doc id="2867" url="https://de.wikipedia.org/wiki?curid=2867" title="KZ (Begriffsklärung)">
KZ (Begriffsklärung)

KZ steht für:
Kz. steht für:
Kz steht für:
KZ als Unterscheidungszeichen auf Kfz-Kennzeichen:
KŽ steht als Abkürzung für:
KŽ als Unterscheidungszeichen auf Kfz-Kennzeichen:
.kz steht als Abkürzung für:
Siehe auch:



</doc>
<doc id="2868" url="https://de.wikipedia.org/wiki?curid=2868" title="Kaiserstuhl">
Kaiserstuhl

Kaiserstuhl steht für:




Siehe auch:


</doc>
<doc id="2869" url="https://de.wikipedia.org/wiki?curid=2869" title="Korbach">
Korbach

Die Stadt Korbach, seit dem 18. Juni 2013 offiziell Hansestadt Korbach (früher auch "Corbach" geschrieben, bahnamtlich bis zum 1. Februar 1935), ist die Kreisstadt des Landkreises Waldeck-Frankenberg in Hessen.

Korbach liegt am Nordostrand des Rheinischen Schiefergebirges, hier auch als "Waldecksches Upland" bezeichnet. Die westlichen Stadtteile befinden sich in den Ausläufern des Sauerlandes.
Die höchsten Berge im Korbacher Stadtgebiet sind der Widdehagen (635 m) und der für seinen Reichtum an goldhaltigen Erzen bekannte Eisenberg (562 m). Die Kernstadt wird vom Kuhbach (umgangssprachlich "„Die Kuhbach“") durchflossen, welcher südlich des Stadtgebietes in die Itter und dann weiter über Eder und Fulda in die Weser entwässert. Mit einer Altstadthöhe von 366 bis 392 m. ü. NHN ist Korbach die höchstgelegene Kreisstadt Hessens.

In der Korbacher Spalte, einer Erdspalte in der Nähe Korbachs, gab es bedeutende Fossilienfunde aus dem Oberperm. Es sind die einzigen Procynosuchus-Funde (auch „Korbacher Dackel“ genannt) auf der Nordhalbkugel.

Während die meisten der zugehörigen dörflichen Stadtteile an den im Stadtgebiet gelegenen Bergen liegen, befindet sich die eigentliche Kernstadt auf einer früher unbewaldeten Hochebene, der Waldecker Tafel, was den Einwohnern den Spitznamen „Feldhühnerchen“ eingebracht hat.
Die Nachbargemeinden der Stadt Korbach sind Diemelsee, Lichtenfels, Twistetal, Vöhl, Waldeck und Willingen (Upland) alle im Landkreis Waldeck-Frankenberg sowie Medebach (im Hochsauerlandkreis in Nordrhein-Westfalen)

Korbach besteht aus der Kernstadt und 14 weiteren Stadtteilen. Neben der Kernstadt liegen nur die südöstlichen Ortsteile Meineringhausen, Nieder-Ense und Strothe auf der Waldecker Tafel; die übrigen hingegen in den Tälern oder auf den Anhöhen des hier auslaufenden Rothaargebirges.

Über die Herkunft und Bedeutung des Stadtnamens werden verschiedene Ansichten vertreten. Einigkeit herrscht, dass der Name altsächsischen Ursprungs ist. Die älteste Form lautet Curbecki (980). Einer Auffassung zufolge soll die erste Silbe aus dem mittelniederdeutschen „kurren, korren“ gebildet worden sein, was so viel wie das Murmeln eines Baches bedeutet. Nach anderer Ansicht leitet sich die Silbe „Cor“ oder „Cur“ von „Kür“ und „küren“ ab, was „Wahl“ bzw. „wählen“ bedeutet. Demnach handelte es sich bei Korbach um einen am Bache gewählten Platz, möglicherweise auch um einen Versammlungsort an einem Bach, an dem das Volk einen Anführer wählte.

"Curbechi" wurde urkundlich erstmals im Jahre 980 erwähnt, als der damalige König und spätere Kaiser Otto II. Korbach, Lelbach, Rhena und drei weitere Orte im damaligen Ittergau, unter Hinzuziehung des Grafen Asicho vom Ittergau und Nethegau als Zeugen, im Tausch gegen die Gemarkungen Meginrichesdorf und Memleben im Hassegau an das Kloster Corvey abgab. Erste Siedlungsspuren im Bereich der Kilianskirche werden aber mittlerweile auf die Zeit vor Karl dem Großen datiert, so soll hier ein sächsischer Edelhof gestanden haben. Auch auf dem übrigen Stadtgebiet gibt es Hinweise auf frühzeitliche Besiedlungen, so z. B. auf dem Wipperberg bei Lengefeld.

Im Jahr 1188 verlieh der Paderborner Bischof Bernhard Korbach das Soester Stadtrecht. Auf Grund der Lage Korbachs am Schnittpunkt der Handelswege Köln-Leipzig und Frankfurt-Bremen entwickelten sich Handwerk und Handel rasch, und Korbach blühte auf. Das Gebiet der Altstadt reichte bald nicht mehr aus, um alle Einwohner aufzunehmen. Die Kaufleute siedelten sich daher in zwei neuen Städten, der oberen und unteren Neustadt, außerhalb der Stadtgrenzen an, die sich bald zu einer gemeinsamen Stadt vereinigten. Im 14. Jahrhundert wurde in der Altstadt die Kilianskirche erbaut, in der Neustadt die Nikolaikirche.

Im Jahr 1349 besuchte Kaiser (damals noch römischer König) Karl IV. die Stadt. Im Jahr 1377 wurden die Altstadt und die Neustadt miteinander vereinigt, und auf der Grenze zwischen beiden zuvor getrennten Städten entstand das gemeinsame Rathaus, das sich noch heute dort befindet. 1414 wurde ein doppelter Mauerring, der die gesamte Siedlung umgab, vollendet. Fünf Stadttore bewachten den Zugang zur Stadt: das Tränketor, das Dalwigker Tor, das Enser Tor, das Lengefelder Tor und das Berndorfer Tor. Von diesen ist heute nur noch das Enser Tor erhalten. Die Padberger Fehde von 1413 bis 1418 war der Höhepunkt der langen Grenzstreitigkeiten und wiederholten Übergriffe der Herren von Padberg auf das Gebiet der Grafschaft Waldeck und insbesondere der Stadt Korbach. Mit dieser Fehde fanden die Auseinandersetzungen ein vorläufiges Ende.

Korbach trat der Hanse bei und wurde erstmals 1469 als Mitglied im Hansebund erwähnt. Es war damit eine der geographisch südlichst gelegenen Städte der Hanse. Die Korbacher Kaufleute handelten mit Tuchen, Fellen und Bier, auch mit Gold und anderen Metallen aus der näheren Umgebung. Korbach war die einzige Hansestadt im heutigen Bundesland Hessen.

Mit der Reformation wurde die Stadt wie die gesamte Grafschaft Waldeck protestantisch. Auch heute ist Korbach noch größtenteils protestantisch, auch wenn seit dem 19. Jahrhundert wieder vermehrt Katholiken in die Stadt zogen und die beiden katholischen Kirchen, die ältere Marienkirche und die moderne Josephskirche, errichteten. Zudem sind die westlichen Stadtteile an der Grenze zu Nordrhein-Westfalen fast vollständig (Hillershausen) oder zu größeren Teilen (Nieder-Schleidern, Eppe) katholisch.

Im Dreißigjährigen Krieg musste Korbach immer wieder hohe Kontributionen an durchziehende Truppen leisten. Am Ende des Krieges war nur noch die Hälfte der Häuser bewohnbar, und die Zahl der Einwohner war von 2600 auf 1100 zurückgegangen. Ein großer Stadtbrand vernichtete 1664 fast alle Wohnhäuser; heute gibt es in Korbach nur ein Fachwerkhaus „Im Katthagen“, das vor diesem Brand, 1593, erbaut wurde. Die gotischen Steinkirchen sowie die steinernen Lagerhäuser hingegen blieben gut erhalten.

Im Siebenjährigen Krieg kam es am 10. Juli 1760 nördlich der Stadt zum Gefecht bei Korbach zwischen den Franzosen und einer Braunschweigisch-hessisch-britischen Truppe.

Erst gegen Ende des 18. Jahrhunderts herrschte wieder bescheidener Wohlstand; eine erste wirtschaftliche Blüte erreichte die Stadt wieder Ende des 19. Jahrhunderts. Hierzu trug vor allem die Eröffnung der Twistetalbahn im Jahr 1893 bei, die mit der Bahnstrecke Volkmarsen–Vellmar ab 1897 eine Verbindung nach Kassel bot. Der in Alleringhausen geborene Louis Peter, Gründer der "Peters Union AG", Frankfurt/Main (früher "Mitteldeutsche Gummiwarenfabrik L. Peter AG"), baute 1907 das Zweigwerk Korbach. Die hannoversche Continental AG übernahm 1929 die "Peters Union".

Der Stadt blieben in beiden Weltkriegen größere Schäden erspart. Nach dem Zweiten Weltkrieg nahm die Einwohnerzahl durch Flüchtlinge und Vertriebene aus den ehemaligen deutschen Ostgebieten stark zu.

Am Stadtrand unweit der Bahnstrecke Korbach–Berndorf und der Flechtdorfer Straße stürzte am 29. März 1961 ein F-104 „Starfighter“ der Luftwaffe ab. Dabei kamen Menschen nicht zu Schaden. Pilot und Beobachter konnten sich mit dem Schleudersitz retten. Es handelte sich um den ersten Absturz einer F-104 der Bundeswehr.

Am 1. Juli 1970 wurden die bis dahin selbständigen Gemeinden Alleringhausen, Eppe, Goldhausen, Helmscheid, Hillershausen, Lengefeld, Meineringhausen, Nieder-Schleidern, Rhena und Strothe eingegliedert. Am 31. Dezember 1970 kamen Nieder-Ense, Nordenbeck und Ober-Ense hinzu. Lelbach folgte am 1. Oktober 1971.


Die Kommunalwahl am 6. März 2016 lieferte folgendes Ergebnis, in Vergleich gesetzt zu früheren Kommunalwahlen:
Seit 1963 unterhält Korbach eine Städtepartnerschaft mit dem französischen Avranches im Département Manche. Als weitere Partnerstadt folgte nach der Wiedervereinigung 1990 das thüringische Waltershausen. Weiterhin pflegt Korbach Partnerschaften mit dem polnischen Pyrzyce (Pyritz) und dem tschechischen Vysoké Mýto (Hohenmauth).

Korbach liegt an den Bundesstraßen 251 (Kassel–Brilon) und 252 (Marburg–Paderborn), die sich auf der Korbacher Umgehungsstraße kurzzeitig überschneiden. Ein direkter Anschluss an das Autobahnnetz besteht auf dem Stadtgebiet nicht; die nächsten Autobahnanschlüsse sind an der A 44 bei Diemelstadt bzw. Zierenberg, jeweils etwa 30 km entfernt. Die Stadt ist zudem über Landesstraßen mit Medebach und Diemelsee verbunden.

Der Bahnhof Korbach ist ein Eisenbahnknotenpunkt, an dem sich heute drei Eisenbahnstrecken treffen: eine nach Nordwesten, die als Uplandbahn über Willingen nach Brilon-Wald führt; eine nach Süden, die über Frankenberg nach Marburg (Untere Edertalbahn und Burgwaldbahn) verläuft; und eine nach Nordosten, die als Teil der Bahnstrecke Warburg–Sarnau und eines Abzweiges über Volkmarsen nach Kassel führt.
Eine vierte Strecke führte früher nach Südosten über Waldeck nach Wabern. Sie ist als Ederseebahn im Abschnitt Korbach−Bad Wildungen stillgelegt; zwischen Korbach und Buhlen befindet sich nun der Ederseebahn-Radweg, der von 2008 bis 2012 angelegt wurde.
Die Strecke nach Kassel wurde am 4. Oktober 1998 wiedereröffnet (damals eines der ersten Beispiele für Streckenreaktivierung). Die 31 Kilometer lange Strecke Korbach–Frankenberg wurde Ende der 1980er Jahre stillgelegt. Der kurze Abschnitt von Korbach bis Korbach Süd wurde bereits ab 29. September 1999 wieder befahren. Seit dem 11. September 2015 ist die gesamte Untere Edertalbahn von Korbach bis Frankenberg, die auch den Nationalpark Kellerwald-Edersee anbindet, wieder in Betrieb.

Bis in die 1980er Jahre bestanden durchgehende Fernzugverbindungen unter anderem bis nach Amsterdam, Hamburg und Frankfurt am Main.

Korbach hat, für eine Stadt dieser Größe bemerkenswert, zwei Bahnhöfe an derselben Eisenbahnstrecke, die beide in Betrieb sind. Um die Altstadt und die Südoststadt besser per Bahn erreichbar zu machen, entstand 1,5 km weiter südlich des Hauptbahnhofs der Haltepunkt „Korbach Süd“. Trotz Modernisierungsmaßnahmen entlang der Bahnstrecke nach Kassel sind die Folgen der früheren Stilllegung noch spürbar. Zudem stellt die Streckenführung über Bad Arolsen und Volkmarsen einen beachtlichen Umweg dar.

Im Jahr 1997 wurde in Korbach ein Stadtbus-Konzept mit mittlerweile fünf sich teilweise überschneidenden Ringlinien eingeführt. Diese Busse fahren im 40-Minuten-Takt und treffen sich dabei alle 20 Minuten am Hauptbahnhof. Wichtige Fahrziele, wie zum Beispiel die Innenstadt, das Stadtkrankenhaus oder das Schulzentrum, sind an mehrere Linien angeschlossen.

In Korbach gibt es einen Flugplatz mit Gastronomie. Neben einer 600 m langen Graspiste für Sportflugzeuge steht eine 200 m lange Bahn für den Gleitschirmbetrieb zur Verfügung. Der Flugplatz, einer der ältesten in Deutschland, ist ein beliebtes Ausflugsziel und wird jährlich von hunderten Piloten aus ganz Europa angesteuert.










</doc>
<doc id="2871" url="https://de.wikipedia.org/wiki?curid=2871" title="Korbacher Spalte">
Korbacher Spalte

Die Korbacher Spalte ist eine 20 Meter tiefe, und bis zu 4 Meter breite verfüllte Spalte im Kalkstein eines ehemaligen Steinbruches am Südrand der nordhessischen Kreisstadt Korbach. Sie gilt als bedeutende Fossillagerstätte und ist ein Bodendenkmal aufgrund des Hessischen Denkmalschutzgesetzes und damit als Kulturdenkmal geschützt.

Die Spalte setzt sich südwestlich und östlich des Steinbruchs im Untergrund fort und besitzt eine Gesamtlänge von ca. einem Kilometer. Das Material, mit dem die Spalte verfüllt ist, enthält zahlreiche Fossilien von Landwirbeltieren (Tetrapoda) aus der Zeit des Oberperm vor ca. 255 Millionen Jahren. Die Gesteine, in denen sich die Spalte befindet, sowie das Material der Spaltenfüllung gehören dem tieferen (geologisch älteren) Teil der Zechstein-Serie an.

Die Spalte wurde 1964 entdeckt. Nach ersten Fossilienfunden finanzierte die amerikanische National Geographic Society systematische Grabungen an der Austrittsstelle der Spalte in dem aufgelassenen Steinbruch. Der Fund eines Unterkiefers des bis dahin nur aus den Karoo-Ablagerungen des südlichen Afrikas bekannten Cynodontiers (Vertreter einer Gruppe „säugetierähnlicher Reptilien“) "Procynosuchus", führte zu einer Veröffentlichung im Wissenschaftsmagazin Nature. Dieses Tier wurde weithin als „Korbacher Dackel“ bekannt. Die Stadt Korbach kaufte daraufhin die Fundstelle, überdachte sie und errichtete eine Besucherplattform. Weitere identifizierbare Wirbeltierfossilien stammen von Protorosauriern, Captorhiniden, Pareiasauriern und Dicynodontiern. Einige Funde sind im Wolfgang-Bonhage-Museum Korbach ausgestellt, ein anderer Teil der Funde befindet sich im Naturkundemuseum Karlsruhe.




</doc>
<doc id="2872" url="https://de.wikipedia.org/wiki?curid=2872" title="Natriumchlorid">
Natriumchlorid

Natriumchlorid (Kochsalz oder einfach „Salz“) ist das Natriumsalz der Salzsäure mit der chemischen Formel NaCl.

Natriumchlorid ist in der Natur in großer Menge vorhanden, größtenteils gelöst im Meerwasser mit einem Gehalt von ca. 3 %, insgesamt 3,6 · 10 Tonnen, außerdem als Mineral Halit mit einem Gehalt von bis zu 98 % in den häufigen Steinsalzlagerstätten, die in erdgeschichtlicher Zeit in austrocknenden Meeresbuchten sedimentierten. Die insgesamt unter Deutschland vorkommenden Lagerstätten werden auf ein Volumen von 100.000 Kubikkilometer geschätzt.

Steinsalzschichten sind plastisch und werden deshalb von geologischen Prozessen, denen sie unterliegen, vielfach verformt, u. a. zu leichter abbaubaren Salzstöcken und Salzkissen. Wenn eine Salzlagerstätte im Gebirge an die Oberfläche austritt, kann sogar ein "Salzgletscher" entstehen.

Natriumchlorid ist für Menschen und Tiere der wichtigste Mineralstoff. Der Körper eines erwachsenen Menschen enthält etwa 150–300 g Kochsalz und verliert davon täglich 3–20 Gramm, die ersetzt werden müssen. Dafür wurde es schon in vorgeschichtlicher Zeit gewonnen und blieb lange Zeit ein teures Handelsgut.

Natriumchlorid wird in großem Umfang aus den beiden Hauptvorkommen Steinsalz und Meersalz gewonnen. Salz aus oberirdischen Lagerstätten, z. B. Salzseen, hat nur geringe Bedeutung. Die Weltsalzproduktion betrug 2006 über 250 Millionen Tonnen, die Anteile an Stein- und Meersalz werden auf ca. 70 % bzw. 30 % geschätzt. Die sechs größten Produzenten des Jahres 2006 mit zusammen über 60 % der Produktion sind in der unten stehenden Tabelle angegeben. China steigerte seine Produktion in den letzten Jahren erheblich, die USA hatten noch bis 2005 die größte Förderung. Die EU produziert insgesamt ähnliche Mengen wie die USA.

In Salzbergwerken, unter denen Tagebaue heute von quantitativ geringer Bedeutung sind, wird Steinsalz entweder bohrend-sprengend, schneidend oder nass abgebaut. In den ersten beiden Fällen wird das noch unter Tage mit Brechern zerkleinerte Material in verschiedenen Korngrößen zu Tage gefördert. Beim nassen Abbau (engl. "Liquid Mining") wird ebenso wie bei der ausschließlich von über Tage betriebenen Bohrlochaussolung das Salz mittels Bohrspülwerken, historisch auch in Sinkwerken, in Wasser gelöst und als gesättigte Sole von 26,5 % zu Tage gefördert. Dies erlaubt auch die Nutzung stark verunreinigter Vorkommen. Bei natürlichem Wasserdurchtritt durch salzführende Schichten kann auch Sole zu Tage treten, die meistens jedoch nicht gesättigt ist. Solche Solequellen waren im Binnenland die ersten vom Menschen genutzten Salzvorkommen. Die Konzentration wurde früher mittels Gradierwerken durch Wasserverdunstung erhöht, heute wird bergmännisch trocken abgebautes Salz zugesetzt. Aus der gereinigten Sole, die auch aus trocken abgebautem Steinsalz hergestellt werden kann, wird durch Eindampfen Siedesalz von großer Reinheit erzeugt. Heute geschieht dies mittels in Kaskaden angeordneter geschlossener Vakuumverdampfer, wodurch ein Großteil der eingesetzten Wärme zurückgewonnen wird. In klimatisch geeigneten Gebieten wird zunehmend auch Sonnenenergie zur Verdunstung genutzt, das Verfahren ist vergleichbar dem der Meersalzgewinnung.

Die Gewinnung von Salz aus Meerwasser ist nur in Küstengebieten mit hoher Sonneneinstrahlung bei gleichzeitig geringen Niederschlägen wirtschaftlich. Das Meerwasser wird hierzu durch flache Beckenkaskaden (Salzgärten) geführt, in denen die Salzkonzentration durch natürliche Verdunstung zunimmt. Schließlich wird das ausgefällte Salz zusammengeschoben und getrocknet, nur für die teure Speisesalzqualität "fleur de sel" wird es an der Oberfläche schwimmend abgeschöpft. Auch in Salzgärten kann Sole entnommen werden.

Die vier Grundprodukte der Salzindustrie sind Sole, Steinsalz, Meersalz und Siedesalz. Für die USA schätzte der U.S. Geological Survey für 2007 mittlere Preise ab Werk von 10 USD/t für Sole, 25 USD/t für Steinsalz, 57 USD/t für Salz aus solarer Verdunstung und 150 USD/t für Siedesalz.

Natriumchlorid bildet farblose Kristalle, die eine kubische Natriumchlorid-Struktur ausbilden. Sie sind, im Gegensatz zu vielen anderen Kristallen, nicht doppelbrechend. Hierbei ist jeder Natrium- sowie jeder Chlorkern oktaedrisch vom jeweils anderen Kern umgeben. Es ist sehr gut wasserlöslich, wobei die Löslichkeit nur wenig temperaturabhängig ist.

Entsprechend dem Raoultschen Gesetz kommt es bei wässrigen Lösungen zu einer Dampfdruckerniedrigung gegenüber von reinem Wasser. Der Effekt verstärkt sich mit steigender Natriumchloridkonzentration bzw. Temperatur. Die gesättigte Lösung siedet bei 108,7 °C.

Bei einem Gehalt von 23,4 % Natriumchlorid in wässriger Lösung bildet es ein eutektisches Gemisch. Dieses erstarrt am eutektischen Punkt von −21,3 °C homogen und ohne Entmischung. Diese Lösung wird "Kryohydrat" genannt. Unterhalb von 0,15 °C wird ein stabiles Dihydrat NaCl·2HO gebildet. Das Salz löst sich sehr gut in wässriger Ammoniaklösung. Bei tieferen Temperaturen kann ein Pentaammoniaksolvat NaCl·5NH in Form von farblosen Nadeln ausfallen. Es sind auch Additionsverbindungen mit Harnstoff, Glucose und Sucrose bekannt.

Die wässrige Lösung sowie die Schmelze leiten auf Grund der (elektrolytischen bzw. thermischen) Dissoziation von Natriumchlorid in seine Ionen elektrischen Strom. Die Leitfähigkeit der Schmelze steigt mit der Temperatur. Reines kristallines Natriumchlorid ist dagegen praktisch nicht leitfähig.

Die Standardbildungsenthalpien und Entropien sind in folgender Tabelle gegeben.

Als Speisesalz ist Natriumchlorid ein wichtiger Bestandteil der menschlichen Ernährung. Es wird zum Würzen von fast allen Speisen benutzt. Seit der Zeit der Industrialisierung spielt jedoch mengenmäßig die industrielle Verwendung die weitaus größere Rolle. Je nach der Anwendung werden unterschiedliche Zusatzstoffe beigemischt.

Nach der Verwendung wird unterschieden zwischen Industriesalz als Rohstoff der chemischen Industrie, Auftausalz für den winterlichen Straßendienst, Gewerbesalz für die verschiedensten industriellen und gewerblichen Zwecke und Speisesalz zum menschlichen Genuss. Die Anteile dieser Verwendungen sind für Deutschland und die USA in der folgenden Tabelle angegeben:

Als Industriesalz wird nur das in der chemischen Grundstoffindustrie eingesetzte Natriumchlorid bezeichnet. Es handelt sich um Steinsalz, bei entsprechenden lokalen Marktverhältnissen, wie etwa in Indien, auch um Meersalz sowie in großem Umfang um Sole, die teilweise in Pipelines transportiert wird. Für die USA wird der Soleanteil mit 90 % angegeben. Es ist vielseitiger Rohstoff der chemischen Industrie und Grundlage für viele Produkte, wobei die Produktion zu wichtigen Grundstoffen im Wesentlichen mit zwei verschiedenen Verfahren beginnt:


Weitere bedeutende Folgeprodukte sind Kunststoffe, medizinische Präparate und Schädlingsbekämpfungsmittel.

Steinsalz wird im Winter bei mäßigen Frosttemperaturen als "Auftausalz" ("Streusalz") verwendet, teilweise mit Zusätzen zur Erhaltung der Rieselfähigkeit. In Deutschland wurde Auftausalz eingefärbt, um dessen Verwendung in der Zubereitung von Lebensmitteln zu unterbinden. Mit der Abschaffung der Salzsteuer zum 1. Januar 1993 wurde dies jedoch hinfällig. Vor dem Streuen wird zur Erzeugung von besser geeignetem Feuchtsalz auch Sole zudosiert.

Als Gewerbesalz wird praktisch alles gewerblich, technisch oder industriell verwendete Salz bezeichnet, das nicht in eine der drei anderen Kategorien fällt. Salz zum Zweck der Nahrungsmittelkonservierung zählt, obwohl in geringer Menge verzehrt, zum Gewerbesalz. Das Spektrum des Gewerbesalzes ist daher sehr groß und reicht von grobem ungereinigten Steinsalz bis zu hochreinem Natriumchlorid und sterilen Zubereitungen für chemische, pharmazeutische und medizinische Zwecke. Je nach Verwendung sind außerdem unterschiedlichste Zusätze zugefügt.

Dem Futter von Nutztieren wie Rindern, Schafen und Ziegen wird Viehsalz beigemengt (ungereinigtes, vergälltes Steinsalz, gegebenenfalls mit anderen Mineralsalzen als Zusatz). Dies steigert ihren Appetit und trägt zur allgemeinen Gesundheit bei.

Bekannt sind auch Salzlecksteine, die in zoologischen Gärten, in der Viehwirtschaft, der Haus- und Heimtierhaltung sowie für Wildtiere verwendet werden.

Salz wird traditionell zur Konservierung von Lebensmitteln wie Fleisch (Pökeln, Suren), Fisch (etwa Salzhering), Gemüse (Sauerkraut) usw. verwendet. Dabei entzieht das Salz durch osmotische Wirkung dem Gut die Feuchtigkeit. So wird die Grundlage Wasser für schädliche Organismen entzogen, aber auch Keime und Krankheitserreger abgetötet.

Nitritpökelsalz besteht aus einer Mischung aus Kochsalz und Natriumnitrat, Natriumnitrit oder Kaliumnitrat.

Das Einlegen von Gemüse (etwa Salzgurken, Oliven) in "Salzlake" nutzt den keimtötenden Effekt.

Käse wird vor der Reifung in Salzwasser vorbereitet und während der Reifung mit einer Salzlake gepflegt, damit die Kruste trocken bleibt.

Salz wird als Regeneriersalz zur Wasserenthärtung in Geschirrspülmaschinen und Wasseraufbereitungsanlagen verwendet. In Kältemischungen wird es mit Wasser versetzt.

Auch bei der Lederverarbeitung und in der Färberei ist Salz ein unverzichtbarer Rohstoff.

Bei der Herstellung von Geschirr und anderer Keramik wird durch Zugabe von feuchtem Salz unter hoher Temperatur die traditionelle Salzglasur erzeugt. Dabei verbindet sich das Natrium-Ion des in der Ofenatmosphäre dissoziierten Kochsalzes mit den Silikaten des Tons zu einer dauerhaften Glasur. Das Salz ist allerdings bei den Anwendungstemperaturen (1.200 °C und höher) aggressiv und beschädigt das Ofeninnere (die feuerseitige Wärmedämmung), die deshalb öfter ausgewechselt werden muss.

Die ionisierende Wirkung des Salzes wird in der Metallverarbeitung eingesetzt.

In der modernen Medizin wird nach starkem Blutverlust, etwa bei einer Operation oder einem Unfall, eine 0,9%ige Lösung von Natriumchlorid in Wasser zur Auffüllung des Blutvolumens intravenös verabreicht (isotonische Kochsalzlösung, auch physiologische Kochsalzlösung genannt). Sie ist isoosmotisch mit dem Blutplasma. Des Weiteren wird NaCl-Lösung zum Offenhalten von Venenzugängen und als Trägerlösung für Medikamente verwendet.

In der Antike und im Mittelalter galten Medikamente auf Salzbasis als wirksame Heilmittel. Die Haut Neugeborener wurde zu deren Stärkung mit Salz abgerieben. Es wurde in Wundverbänden, Pflastern, Salben, Pudern und Bädern eingesetzt. Besondere Bedeutung maß man der trocknenden und wärmenden Wirkung des Salzes bei.

Das Salz wurde im Mittelalter auch zur äußerlichen Behandlung von Geschwüren und Wunden benutzt, da es als zusammenziehend, reinigend und lindernd galt.

Man streute Salz in Wunden, um Entzündungen zu verhindern – eine mitunter sehr schmerzhafte Prozedur, die in einer entsprechenden Redewendung („Salz in offene Wunden streuen“) Einzug in die deutsche Sprache gefunden hat. Reines Salz zerstört über Osmose alle Zellen – also auch krankmachende Mikroorganismen wie Bakterien und Pilze, allerdings die Zellen des Verwundeten ebenso. Diese Art der Desinfektion ist also zweischneidig wie das ebenso praktizierte Ausbrennen von Wunden. Der gleiche Wirkmechanismus verhinderte, dass in Kochsalz konservierte Lebensmittel verdarben – also von Mikroorganismen zersetzt wurden.

Noch heute werden Solebäder als Heilmittel eingesetzt. Kuraufenthalte am Meer oder in salzhaltiger Luft bei Salinen und früher auch in Salzbergwerken dienen der Behandlung von Atemwegserkrankungen. Wo dies nicht möglich oder zu teuer ist, werden Inhalationsgeräte eingesetzt, bei denen Salz-Aerosol eingeatmet wird.

Kochsalzlösung wird auch zur Nasenspülung und zum Gurgeln verwendet. Für die Nasenspülung verwendet man isotonische Kochsalzlösung, da normales Wasser aufgrund der Osmose die Schleimhäute aufquellen lassen würde.

Kristallines Natriumchlorid ist für Licht mit Wellenlängen zwischen 0,21 und 25 µm ausreichend transparent, um es für optische Elemente zu verwenden. Aus Natriumchlorid werden daher Linsen, Prismen und Fenster hergestellt, die für den sichtbaren Bereich sowie im nahen bis hinein ins mittlere Infrarot geeignet sind. Aufgrund der Wasserlöslichkeit von Natriumchlorid werden optische Elemente aus diesem Material durch Luftfeuchtigkeit beschädigt. Dies lässt sich jedoch verhindern, wenn ihre Temperatur über der Umgebungstemperatur gehalten wird. Aus diesem Grund sind in die verwendeten Halterungen oft Heizungen integriert. Natriumchlorid-Fenster werden häufig als Brewster-Fenster für leistungsstarke, gepulste CO-Laser verwendet, aber auch in anderen Bereichen.
In der Infrarotspektroskopie wird es als Träger für Untersuchungsproben zu Tabletten gepresst.

Natriumchlorid ist Urtitersubstanz nach Arzneibuch zur Einstellung von Silbernitrat-Maßlösungen. Um eine Maßlösung mit Natriumchlorid herzustellen, muss es zuerst getrocknet werden.

Salze sind für den Menschen lebensnotwendig (siehe physiologische Bedeutung). Früher war Kochsalz sehr kostbar. Die Zubereitung von Speisen wird vereinfacht und durch die Funktion als Gewürz im Geschmack verbessert. Die letale Dosis, die zu einer Hypernatriämie führt, wird beim Erwachsenen mit 0,5 bis 5 Gramm je Kilogramm Körpergewicht angegeben.

Heute ist das im Handel erhältliche Salz oft jodiert, also mit lebensmittelverträglichen Iodverbindungen versetzt und soll einem möglichen Jodmangel vorbeugen. Im Handel wird es als "Jodsalz" bezeichnet. Daneben sind Speisesalz-Kristalle zur Erhaltung der Rieselfähigkeit meist mit einem Trennmittel umhüllt.

Es sind auch Produkte mit Zusätzen von Natriumfluorid (zur Verbesserung der Zahngesundheit) sowie Folsäure im Handel.



</doc>
<doc id="2874" url="https://de.wikipedia.org/wiki?curid=2874" title="Kochen">
Kochen

Kochen (von lateinisch "coquere", „kochen, sieden, reifen“ entlehnt) ist im engeren Sinne das Erhitzen einer Flüssigkeit bis zum und am Siedepunkt, im Weiteren das Garen oder Zubereiten von Lebensmitteln allgemein, unabhängig von der Zubereitungsart wie z. B. Backen (Garen von Teig oder in einem Teigmantel), Braten oder Grillen (trockenes Erhitzen). Abgeleitet davon wird die Berufsbezeichnung Koch. 

Aufgrund der unterschiedlichen Siedepunkte verschiedener Stoffe, zum Beispiel von flüssigem Stickstoff bei 77,36 K (−195,79 °C) in der Molekularküche oder von Speisefett beim Frittieren bei 140 bis 190 °C, kann das Kochen in einem breiten Temperaturspektrum stattfinden.

Je nach Kontext werden die Begriffe "kochen" und "sieden" synonym verwendet.

Das Kochen gehört zu den ältesten und wichtigsten Kulturtechniken des Menschen. Die frühesten Spuren von Nahrungszubereitung mit Werkzeugen sind 1,5 Millionen Jahre alt: in Kenia gefundene, mit Steinklingen abgeschabte Antilopenknochen und zwischen Steinen geöffnete Markknochen. Der entscheidende Schritt wurde mit der Beherrschung des Feuers durch den "Homo erectus" gemacht ("siehe Prähistorische Feuernutzung"). Die frühesten bekannten Spuren, deren Alter auf etwa 1,5 Millionen Jahre geschätzt werden, sind im heutigen Äthiopien gefunden worden. In Asien wurden Herdstellen von H. erectus mit verkohlten Knochen in der Höhle von Zhoukoudian bei Peking (Alter ca. 700.000 Jahre) und in Europa Reste von Hütten mit Herdstellen gefunden Terra Amata bei Nizza (Alter ca. 400.000 Jahre). 

Die Bedeutung des Kochens liegt vor allem in den chemischen Reaktionen, die dabei ablaufen: das Zellgewebe wird gelockert, Eiweiße gerinnen, Bindegewebe geliert, Fette verflüssigen sich, Stärke verkleistert, Mineralstoffe werden freigesetzt und Geschmacksstoffe gebildet. Neben der leichteren Verdaulichkeit von Fleisch und Früchten, die bis dahin den Hauptteil der Ernährung ausmachten, bewirkte das Kochen vor allem eine enorme Ausweitung des Nahrungsangebots: Bis dahin schwerverdauliche, ungenießbare oder auch giftige Tiere und Pflanzen konnten durch Kochen genießbar gemacht werden, darunter auch stärkehaltige Gräser und Wurzeln, von deren Nachkommen einige schließlich zu Grundnahrungsmitteln wurden. Weiter wirkt Kochen sterilisierend und konservierend, was unmittelbaren Einfluss auf die Gesundheit hat und die Möglichkeiten der Vorratshaltung verbessert. Im Gefolge der Erfindung des Kochens veränderte sich die menschliche Anatomie, vor allem das Gebiss: Es verkleinerte sich stark, was auch der Entwicklung des Sprechapparats und damit der Sprache förderlich war.

Die frühen Kochtechniken vor Erfindung der Töpferei und der Metallurgie lassen sich zum großen Teil nur indirekt rekonstruieren, aus Beschreibungen traditioneller Verfahren in geschichtlicher Zeit und aus der Beobachtung noch heute oder bis vor kurzem existierender steinzeitlicher Gesellschaften. Die ursprünglichsten Verfahren sind sicherlich das Grillen, das Garen in heißer Asche und das Rösten auf im Feuer erhitzten Steinen, das für Fleisch, Wurzeln und Getreide geeignet ist. Für das Kochen im engeren Sinn, also das Erhitzen in Flüssigkeit, dienten Erdgruben und natürliche Gefäße wie Muschelschalen, Straußeneier, Schildkrötenpanzer usw. oder enggeflochtene Körbe, deren Inhalt durch Hineinlegen glutheißer Kochsteine gegart wurde. Erdöfen sind noch heute in Gebrauch, in denen Lebensmittel in Blätter gewickelt, mit heißen Steinen belegt und Gras und Erde isoliert, langsam schmoren. In Osttimor wird noch heute in dicken Bambusstangen Essen im offenen Feuer gekocht. Im frühzeitlichen China wurden Lebensmittel mit Lehm oder Ton umhüllt und so im Feuer geschmort. 

Eine andere Methode war, Fleisch (auch mit Getreide oder Gemüse) in Tierhäuten, Mägen oder Därmen über dem Feuer zu garen. Herodot beschreibt es so: "„Im Skythenland verfährt man beim Kochen folgendermaßen. Dem gehäuteten Tier wird das Fleisch von den Knochen gelöst und in den Kessel geworfen, falls ein solcher zur Stelle ist. Ist kein Kessel zur Stelle, so wird das ganze Fleisch in den Magen des Tieres gesteckt, Wasser hinzugegossen und mit Hilfe der Knochen gekocht. Die Knochen brennen sehr gut, und der Magen nimmt bequem das von den Knochen gelöste Fleisch auf. So kocht also das Rind, oder was für ein Tier es sonst ist, sich selbst.“" Wie alt derartige Verfahren sind, kann nicht bestimmt werden, da sie keine Spuren hinterlassen, sie stellen aber vermutlich die Urform der Wurst dar, die sich in Gerichten wie Haggis oder Pfälzer Saumagen niedergeschlagen hat.




</doc>
<doc id="2876" url="https://de.wikipedia.org/wiki?curid=2876" title="Konsul (Begriffsklärung)">
Konsul (Begriffsklärung)

Konsul bezeichnet:

Siehe auch:



</doc>
<doc id="2878" url="https://de.wikipedia.org/wiki?curid=2878" title="Kuwait">
Kuwait

Das Emirat Kuwait (, deutsch auch: "Kuweit") ist ein Staat in Vorderasien auf der Arabischen Halbinsel. Es grenzt im Norden und Westen an den Irak (240 Kilometer gemeinsamer Grenzverlauf), im Süden an Saudi-Arabien (222 Kilometer) und im Osten an den Persischen Golf (499 Kilometer).

Der Großteil des Landes gehört zur Wüste ad-Dibdiba. Abgesehen von dem küstennahen Höhenstreifen der Zaur-Berge an der Bucht von Kuwait und vereinzelten Hügeln ist das Gelände fast eben. Im Inneren befinden sich einige Oasen. Die 40 Kilometer lange Bucht von Kuwait teilt die Küste in zwei Teile. Nach Norden ziehen sich die Schwemmlandablagerungen zum Schatt al-Arab. An der Südspitze der Bucht liegt die Hauptstadt mit ihrem geschützten Naturhafen. Südlich davon befinden sich die großen Erdölfelder, die mit den saudischen Vorkommen in Verbindung stehen. Nach Südwesten steigt das Land allmählich an. Die höchste Erhebung (290 Meter) liegt im Sandsteinplateau an der Westspitze des Staatsgebiets. Die Westgrenze zum Irak zieht sich entlang des Wadi al-Batin, eines nur zeitweilig wasserführenden Trockenflusstales. Zu Kuwait gehören neun Inseln. Bubiyan, die größte Insel, ist durch eine 2400 Meter lange Brücke mit dem Festland verbunden. Nach der Befreiung im Jahre 1991 wurde die Insel in eine Militärbasis umgewandelt. Zivilisten haben zurzeit keinen Zugang zu Bubiyan. Die restlichen acht Inseln heißen: Auha, Failaka, Kubbar, Miskan, Qaruh, Umm al-Maradim, Umm an-Namil und Warba.

Kuwait möchte auf dem Südzipfel einer Halbinsel etwa 30 Kilometer nördlich von Kuwait-Stadt die Planstadt Madinat al-Hareer (Stadt der Seide) für 700.000 Einwohner auf einer Fläche von 250 Quadratkilometern bauen. Spektakulärer Mittelpunkt der durch viele Wasserflächen aufgelockerten Urbanisation soll der 1001 Meter hohe Wolkenkratzer Burj Mubarak al-Kabir werden; die Planung hat Kuhne & Associates übernommen, die Stadt soll bis 2030 vollendet sein.

Abgesehen von einigen Oasen, in denen Dattelpalmen gedeihen, ist das Land Wüste ohne nennenswertes Tierleben und mit nur spärlicher Strauchvegetation. Nur nach den winterlichen Regenfällen wächst für kurze Zeit auch Gras. Die reichen Fischgründe und die Krabbenbänke in den Küstengewässern waren durch die Ölpest Anfang der 1990er Jahre bedroht.

In der heißen Jahreszeit (Mai bis September) herrschen mittlere Temperaturen um 30 °C; es kann aber bis zu 50 °C heiß werden. In den Wintermonaten ist es dagegen milder (13 bis 15 °C), tagsüber bis zu 25 °C, nachts bis unter 0 °C. In dieser Zeit fallen auch die kargen Niederschläge von 10 bis 220 Millimetern im Jahr. Die Wassertemperaturen in der Bucht betragen im Sommer um 30 °C, im Winter 20 °C.

Die Bevölkerungsstruktur Kuwaits ist von dem sehr hohen Anteil ausländischer Arbeitskräfte und ihrer Angehörigen geprägt (rund 60 % aller Einwohner). Der Anteil der Männer übersteigt mit 57 % den der Frauen erheblich; 24,5 % der Bevölkerung war 2004 unter 15 Jahre alt. Kuwait ist eines der Länder mit dem höchsten Verstädterungsgrad (2004: 96 %); die Lebenserwartung lag 2016 bei 78 Jahren. Das Bevölkerungswachstum lag zwischen 1994 und 2004 im Durchschnitt bei 4,1 %. Amtssprache ist Arabisch, Umgangssprache der Kuwaitis untereinander ist Irakisch-Arabisch. Handelssprache ist Englisch.

Von den rund vier Millionen Einwohnern sind nur etwa 33 % Kuwaiter. 150.000 bis 180.000 Beduinen und die übrigen Einwohner, die aus dem Iran, Indien, Pakistan, und vielen anderen – hauptsächlich arabischen und südostasiatischen – Ländern zugewandert sind, besitzen nicht die kuwaitische Staatsbürgerschaft. Die Beduinen und die Zuwanderer sind den Staatsbürgern gegenüber gesellschaftlich benachteiligt.

Die Staatsbürger sind durch ein umfassendes Sozialversicherungssystem abgesichert; das kostenlose Gesundheitssystem gilt als eines der besten der Welt. Es besteht allgemeine Schulpflicht für 6- bis 14-Jährige bei kostenlosem Unterricht. Die Analphabetenrate ist rückläufig (1980: 40 %, 2015: 3,7 %). Die erste der zwei Universitäten des Landes wurde 1954 gegründet.

Die Bevölkerung ist überwiegend muslimisch, davon sind 65 % Sunniten und 35 % Schiiten. Der Islam ist Staatsreligion. Der römisch-katholischen Kirche gehören 6 % der Bevölkerung an. Die restlichen 9 % umfassen vorwiegend andere christliche Konfessionen, Hindus und Parsen.

Kuwait ist in die sechs Gouvernements al-Ahmadi, al-Asima, al-Farwaniyya, al-Dschahra, Hawalli und Mubarak al-Kabir gegliedert.

Die größten Städte sind (Volkszählung 2005): Dschalib asch-Schuyuch 179.264 Einwohner, as-Salimiyya 145.328 Einwohner und Hawalli 106.992 Einwohner. Die Hauptstadt Kuwait-Stadt liegt mit ca. 63.000 Einwohnern nur auf Platz 18 der größten Städte des Landes. Insgesamt konzentrieren sich 94 Prozent der Bevölkerung Kuwaits in der Agglomeration Kuwait-Stadt.

Bis 1991 war Kuwait eine absolute Monarchie unter der Herrschaft der Familie as-Sabah.

Gemäß der Verfassung von 1962, zuletzt geändert 1997, ist Kuwait jetzt eine konstitutionelle Erbmonarchie. Der Emir ist sowohl weltliches als auch geistliches Staatsoberhaupt. Er ernennt und entlässt die Regierung und kann außerdem das Parlament auflösen. Im Demokratieindex 2016 der britischen Zeitschrift "The Economist" belegt Kuwait Platz 121 von 167 Ländern und gehört damit zu den „autoritären Systemen“. Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „teilweise frei“ bewertet.

Das Rechtssystem orientiert sich am islamischen Recht (Scharia) und an britischen Vorbildern.

Erst zwischen 1991 und 1996 wurde ein Parlament (Nationalversammlung) gebildet. Das Parlament besteht aus 50 Mitgliedern, die alle vier Jahre gewählt werden. Wahlberechtigt sind alle Kuwaiter im Alter ab 21 Jahren, ausgenommen sind Angehörige des Militärs und der Sicherheitskräfte. Die Regierung ist auf ein gewisses Vertrauen im Parlament angewiesen.

Kuwait ist Mitglied der UN, der Arabischen Liga, des GATT, der OPEC und der OAPEC.

Im Alter von 77 Jahren verstarb am 15. Januar 2006 Emir Dschabir al-Ahmad al-Dschabir as-Sabah, der schon seit einigen Jahren nach einem Schlaganfall seinen Amtspflichten nicht mehr nachkommen konnte. In Übereinstimmung mit der Verfassung wurde zuerst der 76-jährige Kronprinz Scheich Sa'ad al-Abdallah as-Salim as-Sabah, ein Cousin des Verstorbenen, von der Regierung zum neuen Emir proklamiert. Da dessen Gesundheit ebenfalls stark angeschlagen war, hatte die Mehrheit der Herrscherfamilie vorgeschlagen, dass Scheich Sabah al-Ahmad al-Dschabir as-Sabah, ein Halbbruder des verstorbenen Emirs, die Herrschaft übernehmen sollte. Sa'ad war sogar zu schwach, um seinen Amtseid vor dem Parlament abzulegen. Auf Antrag der Regierung trat das Parlament am 24. Januar 2006 zusammen und beschloss im Konsens, Scheich Sa'ad seines Amtes zu entheben. Am 29. Januar wurde Scheich Sabah vom Parlament als Emir bestätigt und legte seinen Amtseid ab. Dieser Vorgang war ein Bruch des bisherigen Usus in der Dynastie as-Sabah, dass das Amt zwischen den beiden Zweigen (al-Dschabir und as-Salim) der Herrscherfamilie wechseln sollte.

Darüber hinaus war die Absetzung eines amtsunfähigen Herrschers durch das Parlament ein Novum nicht nur in der Geschichte Kuwaits, sondern in der ganzen Golfregion. Am 20. Februar bestätigte das Parlament das Kabinett des Emirs. Dieser löste dann am 21. Mai das Parlament auf und setzte Neuwahlen für den 29. Juni an. Diesem Schritt war ein heftiger Streit zwischen Regierung und Parlament um die Neueinteilung der Wahlkreise vorangegangen. Eine Gruppe oppositioneller Abgeordneter hatte sich der von der Regierung vorgeschlagenen Reduzierung der Wahlkreise von 25 auf 10 widersetzt und stattdessen eine Reduzierung auf nur noch fünf vorgeschlagen. Dadurch sollte die Bevorzugung bestimmter Bevölkerungsteile, die aus dem Missverhältnis zwischen der Zahl der Einwohner und der Abgeordneten je Wahlkreis erwachsen war, etwas vermindert werden. Doch weder die Vorlage der Regierung noch jene der Opposition fand im Parlament die notwendige Mehrheit, so dass die Regierung den Emir um die Auflösung gebeten hatte.

Einen solchen Präzedenzfall, der den Vorrang des Herrscherhauses gegenüber den Parlamentariern geschmälert hätte, wollte man in jedem Fall vermeiden. Bei den Wahlen gelang es den oppositionellen Kräften, die Zahl ihrer Mandate von bisher 26 auf 33 in der 50 Sitze zählenden Kammer zu erhöhen. Von diesen werden rund 15 den Islamisten zugerechnet, sieben gelten als Liberale, zehn als Reformisten. 17 Abgeordnete gelten als der Regierung gegenüber loyal gesinnt. Da die Kabinettsmitglieder im Parlament stimmberechtigt sind, ist das Kräfteverhältnis dort günstiger für die Regierung als es auf den ersten Blick scheinen will. Am 16. Mai 2005 hatte das Parlament mit 35:23 Stimmen bei einer Enthaltung beschlossen, dass Frauen das aktive und passive Wahlrecht erhalten. "„Ich will, dass unsere Frauen uns beim Bau unseres Landes und unserer Zukunft helfen.“" sagte Ministerpräsident Scheich Sabah al-Ahmad as-Sabah. Bei den Wahlen im Juni 2006 zog keine der 28 Kandidatinnen ins Parlament ein, obwohl Frauen 57 Prozent der Wählerschaft stellen.

Bei den darauffolgenden Parlamentswahlen im Mai 2009 zogen schließlich vier Frauen in das Parlament ein.

Die Außenpolitik des Landes wird von der Geschichte und der geopolitischen Lage des Landes zwischen Saudi-Arabien, dem Irak und dem Iran bestimmt. Eine enge Sicherheitspartnerschaft besteht mit den Vereinigten Staaten, die als Garant für die staatliche Unabhängigkeit Kuwaits fungieren. 2004 wurde Kuwait zum Major non-NATO ally ernannt.

Die diplomatischen Beziehungen zum nördlichen Nachbarn Irak wurden am 2. August 2004 wieder aufgenommen, im Juli 2008 entsendete Kuwait erstmals seit 1990 wieder einen Botschafter nach Bagdad, der irakische Botschafter nahm im März 2010 seine Arbeit auf. Das Verhältnis der beiden Länder bleibt jedoch weiterhin belastet, Hauptursachen hierfür sind die noch ausstehenden irakischen Reparationszahlungen von 23,3 Milliarden US-Dollar sowie der von den Vereinten Nationen festgelegte Grenzverlauf. Diesen hatte Saddam Hussein zwar 1994 anerkannt, die Anerkennung wurde von der neuen Regierung allerdings nicht erneut bestätigt.

Die Rechte auf freie Meinungsäußerung und Versammlungsfreiheit werden laut amnesty international in Kuwait empfindlich eingeschränkt. Besonders betroffen sind tausende staatenloser Bidun mit Wohnsitz in Kuwait. Sie erhalten keine Staatsbürgerschaft und haben damit keinen gleichberechtigten Zugang zum Gesundheits- und Bildungssystem sowie zum Arbeitsmarkt. Teilweise geht die Bereitschaftspolizei mit exzessiver Gewalt gegen diese Minderheit vor.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Kuwait Platz 104 von 180 Ländern. Bei der Situation der Pressefreiheit im Land gibt es laut der Nichtregierungsorganisation "erkennbare Probleme", sie ist jedoch die drittbeste innerhalb der Länder der Arabischen Liga.

In Kuwait sitzt ein Journalist in Haft.

Nach dem Korruptionswahrnehmungsindex ("Corruption Perceptions Index") von Transparency International lag Kuwait 2016 von 176 Ländern zusammen mit Tunesien, Bulgarien und der Türkei auf dem 75. Platz, mit 41 von maximal 100 Punkten.

Frauen werden laut Amnesty durch Gesetze sowie im täglichen Leben diskriminiert. 2011 hatten Jura-Absolventinnen Klagen gegen das Justizministerium auf Einstellung eingereicht. Das Ministerium hatte Stellen mit dem Hinweis ausgeschrieben, sie seien Männern vorbehalten. Im September 2012 kündigte der Oberste Justizrat an, Frauen könnten sich auf eine Reihe von Stellen bei der kuwaitischen Staatsanwaltschaft und im Justizwesen bewerben und reagierte damit auf die anhängigen Klagen der Frauen.

Kuwait verhängt die Todesstrafe. Jedoch wurden von neun im Jahr 2012 verhängten Todesurteilen vier in Haftstrafen umgewandelt. Mindestens ein Häftling starb 2012 in Gewahrsam, nachdem er offenbar gefoltert und anderweitig misshandelt worden war. Todesurteile werden bei einem Berufungsgericht verhandelt und dann dem Höchsten Gerichtshof vorgelegt, ehe der Emir über die Urteile zu entscheiden hat. Im Falle einer Zustimmung und einer Nicht-Umwandlung des Urteils in eine lebenslange Haftstrafe, ist das Urteil nicht mehr verhandelbar und ist rechtskräftig.
Menschen, die nicht zurechnungsfähig und unter 18 Jahren sind, werden nicht zum Tode verurteilt. Es können aber auch Ausländer und Frauen hingerichtet werden. 
Bei Vergehen wie Mord, Vergewaltigung, Drogenhandel, Terrorismus oder Kidnapping wird die Todesstrafe zwingend verhängt. Die Todesstrafe wurde allerdings 2007 vorübergehend ausgesetzt. Im Jahr 2013 richtete der Staat Kuwait, erstmals seit 2007, wieder fünf Mörder hin. Danach wurden im Januar 2017, zusammen mit einem Mitglied der kuwaitischen Königsfamilie, insgesamt sieben Menschen hingerichtet. In Kuwait werden Hinrichtungen öffentlich durch den Strang vollstreckt.

Besonders die Situation von Arbeitsmigranten aus Asien ist in Kuwait unzureichend. Immer wieder kommt es in Kuwait zum Übergriff auf weibliche Haushaltshilfen. Sie werden teilweise von ihren kuwaitischen Arbeitgebern geschlagen, seelisch terrorisiert oder sexuell missbraucht. Die Philippinen, Nepal und Indonesien haben 2011 und 2012 es ihren Staatsbürgerinnen verboten, sich von Kuwait und anderen arabischen Staaten anwerben zu lassen. Nach Einschätzung des US-Außenministeriums ist die Misshandlung von Haushaltshilfen in Kuwait so verbreitet, dass dies den Tatbestand des Menschenhandels erfüllt.

Der "UN-Ausschuss für die Beseitigung der Rassendiskriminierung" empfahl Kuwait 2012 Gesetze zu erlassen, um ausländische Arbeitskräfte und Hausangestellte zu schützen und ihre Rechte gemäß internationalen Standards zu gewährleisten. Kuwait ist Mitglied der Internationalen Arbeitsorganisation (ILO), erfüllt aber deren Übereinkommen nicht.

Die Geschichte des Wüstenstaates liegt bis zum Beginn der Neuzeit weitgehend im Dunkeln. Archäologische Funde lassen jedoch vermuten, dass die Insel Failaka schon im 3. Jahrtausend v. Chr. ein Handelsstützpunkt war und vom 3. Jh. v. Chr. an in der Einflusssphäre des Seleukidenreiches lag. Im 3. Jh. n. Chr. wurde das heutige Kuwait dem Persischen Reich einverleibt; ab 630 gehörte es zum Kalifat der Umayyaden, später der Abbasiden und wurde islamisiert. Mitte des 13. Jh. zerfiel das Reich nach dem Mongoleneinfall. Im frühen 16. Jh. gelangten Portugiesen in die Region.

Im selben Jahrhundert wurde das Gebiet ein wegen seiner Randlage kaum beachteter Teil des Osmanischen Reiches. 1756 gelangte die bis heute herrschende Familie As-Sabah aus dem Inneren der Arabischen Halbinsel an die Herrschaft; Kuwait wurde ein von den Türken zunehmend unabhängiges Scheichtum. Aufgrund seiner geopolitischen Lage gewann das Emirat in der Folgezeit eine von Türken und Briten umworbene Stellung am Persischen Golf. Als die Bedrohung durch die Osmanen zu groß wurde, stellte Scheich Mubarak as-Sabah 1899 sein Land unter britischen Schutz. Der Preis war die Zusicherung, Beziehungen zu anderen Staaten nur mit britischer Zustimmung anzuknüpfen. Damit erreichte London, dass das deutsch-türkische Projekt der Bagdad-Bahn (1903 begonnen) nicht bis zum Golf weitergeführt werden konnte.

Nach der Niederlage der Osmanen im Ersten Weltkrieg erklärten die Briten Kuwait zu "einem selbständigen Emirat unter Britischer Schutzherrschaft". Ab 1919 gab es mehrere Feldzüge der Wahhabiten gegen Kuwait, zudem gab es immer wieder Grenzkonflikte mit dem Nadschd. Die endgültigen Grenzen wurden 1922 im Abkommen von Uqair festgelegt, im Jahre 1940 erkannte Saudi-Arabien Kuwait als unabhängigen Staat an. Ab 1922 schirmte die neugeschaffene Neutrale Zone, die auf Veranlassung der Briten zustande gekommen war, das Emirat von Saudi-Arabien und vom Irak ab. 1938 wurde das erste Erdöl gefunden, und von 1946 an entwickelte sich Kuwait zu einem der größten Erdölproduzenten am Persischen Golf. Große Teile der enormen Einnahmen wurden zur Modernisierung, z. B. zum Ausbau sozialer Einrichtungen, verwendet. 1960 war Kuwait Gründungsmitglied der OPEC.

Am 19. Juni 1961 wurde das Land unabhängig. Gebietsansprüche des benachbarten Irak führten zum Abschluss eines Militärabkommens mit Großbritannien (von Kuwait 1971 gekündigt) und 1973 zur Einführung der allgemeinen Wehrpflicht. Der Irak erkannte die Unabhängigkeitserklärung Kuwaits nicht an und erklärte das Emirat zu einem Teil des Irakischen Staates. Der irakische Ministerpräsident Abd al-Karim Qasim drohte im Sommer 1961 mit einem Einmarsch, weshalb Großbritannien wieder Soldaten nach Kuwait entsandte, um eine irakische Invasion zu verhindern. Diese Truppen wurden später durch Truppen arabischer Staaten ersetzt. Die Aufnahme des Emirats in der Arabischen Liga erfolgte unter heftigem Protest der Iraker, welche bis 1963 mit Hilfe der Sowjetunion einen UN-Beitritt verhinderten. Die Anerkennung Kuwaits durch den Irak erfolgte erst nach dem Sturz Qasims im Jahre 1963, im selben Jahr wurde ein Abkommen zur Normalisierung der Beziehungen unterzeichnet. 1966, 1969, 1973 und 1976 kam es zu verschiedenen Grenzkonflikten, bei dem irakische Truppen kuwaitisches Territorium besetzten, 1977 wurde zwischen beiden Ländern ein neues Grenzabkommen unterzeichnet. Vom Beginn der 1960er Jahre an erstarkten auch in Kuwait panarabische Gruppierungen, deren politische Aktivitäten in der Folgezeit jedoch ebenso unterbunden werden konnten wie die anderer Gruppen, z. B. der Palästinenser. Viele von ihnen wurden ausgewiesen. Nach dem Sechstagekrieg 1967 wandte das Emirat große Summen zur Unterstützung der israelischen Kriegsgegner Ägypten und Jordanien auf.

Der iranisch-irakische Krieg, der 1980 ausgebrochen war, bedrohte im Laufe der Zeit zunehmend den lebenswichtigen Ölexport Kuwaits. Nach wiederholten iranischen Luftangriffen auf kuwaitische Schiffe ließ das Land 1987 seine Tanker „umflaggen“: Sie fuhren bis zum Ende des Ersten Golfkrieges, in dem Kuwait den Irak unterstützte und weshalb das Land im Oktober 1981 Ziel mehrerer iranischer Luftangriffe war, unter der Flagge der USA. 1981 war das Land Gründungsmitglied des Golf-Kooperationsrates.

Nach einem politischen Streit um ein an der irakisch-kuwaitischen Grenze gelegenes Ölfeld marschierten am 2. August 1990 irakische Truppen in Kuwait ein. Nach der Absetzung des Emirs und der Regierung wurde das kuwaitische Staatsgebiet, ungeachtet der einhelligen Verurteilung durch den UN-Sicherheitsrat, annektiert. Vorrangiger Grund für die Besetzung des ölreichen Staates war der – durch die aufgrund des Ersten Golfkriegs bestehende hohe Verschuldung des Iraks (80 Milliarden US-Dollar) entstandene – Konflikt zwischen beiden Staaten. Zunächst wurde eine Marionettenregierung unter Alaa Hussein Ali gebildet, jedoch wurde Kuwait wenige Tage später vom Irak annektiert.

Als Reaktion auf die irakische Besetzung beschlossen die Vereinten Nationen ein Wirtschaftsembargo gegen den Irak, dem vor allem amerikanische Marineeinheiten durch eine Seeblockade Nachdruck verliehen. Trotz Lösungsvorschlägen seitens des Irak (beispielsweise dem Rückzug aus Kuwait und im Gegenzug Wiederaufnahme der Verhandlungen zwischen Israel und den Palästinensern) scheiterte der diplomatische Weg, woraufhin im Namen der UN die Aufstellung einer multinationalen Streitmacht beschlossen und diese in Saudi-Arabien stationiert wurde. Als ein bis zum 15. Januar 1991 befristetes UN-Ultimatum zum irakischen Rückzug ergebnislos verstrich, wurde Kuwait im Verlauf des fünfwöchigen Zweiten Golfkriegs am 27. Februar 1991 befreit. Nach dem Einmarsch der Iraker waren in Kuwait zahlreiche Todesopfer zu beklagen; die Infrastruktur und das industrielle Potential des Emirats wurden weitgehend zerstört. Das Ausmaß der durch die Brände auf den Ölfeldern und die Ölpest im Golf entstehenden Umweltschäden war zeitlich begrenzt. Der Irak erkannte im November 1994 die Unabhängigkeit des Landes an.
Die Unterstützung des Iraks durch die Palästinenser während des Zweiten Golfkrieges hatte die Vertreibung der Palästinenser aus Kuwait 1991 zur Folge. Binnen weniger Tage wurden etwa 450.000 Palästinenser aus Kuwait vertrieben. Die mit der Nakba vergleichbare aber deutlich weniger beachtete Vertreibung hatte erhebliche Folgen für die PLO wie die Bevölkerung Kuwaits.

2003 war Kuwait der Ausgangspunkt für die Invasion der US-Truppen in den Irak.

Das aufgrund seines Ölreichtums sehr finanzstarke Kuwait (Staatsreserven geschätzt auf 550 Mrd. USD) hatte in den 16 Jahren vor dem Ölpreisverfall erheblich von hohen und stabilen Ölpreisen profitiert. Das Land verfügt über Erdölreserven von 101,5 Mrd.  Fass Öl (= 8 % der weltweiten Vorkommen), die bei gegenwärtiger Produktionshöhe noch etwa 90 Jahre halten. Bis 2020 will Kuwait die Produktionskapazität bis auf 4 Mio. Barrel/Tag erhöhen. Kuwait lag 2015 an neunter Stelle der weltweiten Ölförderung mit einer täglichen Produktion von 2,85 Mio. Fass Öl.

Das Bruttoinlandsprodukt (BIP) lag 2016 nach Schätzungen des Internationalen Währungsfonds bei 33,5 Mrd. Kuwait Dinar (KWD, ca. 103 Mrd. EUR), das BIP pro Kopf wurde mit ca. 26.000 EUR veranschlagt. Angesichts der großen Zahl (etwa 2,5 Mio.) gering verdienender Ausländer liegt das Pro-Kopf-Einkommen der kuwaitischen Staatsangehörigen wesentlich höher.

Die hohen Einnahmen aus dem Ölgeschäft werden dabei für ebenfalls hohe Ausgaben zur Befriedigung von  Ansprüchen der kuwaitischen Bevölkerung an Leistungen des Wohlfahrtsstaates verwendet.

Bedingt durch die Einnahmeausfälle in Folge des gesunkenen Ölpreises ist die Finanzlage nun aber angespannter als in der Vergangenheit. Der IWF hatte schon vor dem Ölpreisverfall darauf hingewiesen, dass bei unveränderter Fortentwicklung die Einnahmen die Ausgaben schon in naher Zukunft nicht mehr decken werden. Im Haushaltsjahr 2015/2016 standen Einnahmen von 12,2 Mrd. KWD (rund 37 Mrd. EUR) Ausgaben von 19,2 Mrd. KWD (rund 58 Mrd. EUR) gegenüber. Ende des Haushaltsjahres (Ende März 2016) wurde damit ein Defizit von rund 7 Mrd. KWD (rund 20 Mrd. EUR) verzeichnet.

Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Kuwait Platz 52 von 137 Ländern (Stand 2017–2018). Im Index der Wirtschaftlichen Freiheit belegte das Land 2017 Platz 61 von 180 Ländern. 

Der Reichtum des Landes basiert auf Erdöl, das seit 1946 gefördert wird. Kuwait ist Mitglied der Organisation erdölexportierender Länder (OPEC). 94 Prozent der Einnahmen sind von Erdöl abhängig, von dem täglich etwa 2,6 Millionen Barrel gefördert werden. Ein Großteil davon kommt aus dem Burgan-Feld, welches das zweitgrößte Ölfeld der Welt ist. Der Süden Kuwaits liegt im Nordteil der seit 1969 zwischen Kuwait und Saudi-Arabien aufgeteilten Neutralen Zone. Alle Kosten und Einnahmen aus Bodenschätzen in der gesamten ehemaligen Neutralen Zone, also sowohl auf kuwaitischer als auch auf saudischer Seite, werden zu gleichen Teilen mit Saudi-Arabien geteilt.

Etwa eine Million Barrel werden in den drei Raffinerien des Landes täglich verarbeitet. Am 6. März 2006 gab die Regierung bekannt, dass im Norden des Landes neue bedeutende Erdöl- und Erdgasvorkommen entdeckt worden seien. In zwei Lagerstätten werden insgesamt eine Billion Kubikmeter Gas vermutet; außerdem wurden neue Erdölfelder mit Ressourcen zwischen 10 und 13 Milliarden Barrel erkundet. Damit würde Kuwait erstmals zu einem bedeutenden Erdgasproduzenten werden und könnte zudem seine Erdölreserven um zehn Prozent steigern. Kuwait ist eines der Länder, die in der so genannten strategischen Ellipse liegen.

Im Dezember 2013 wurde ein Programm zum Ausbau der Erneuerbaren Energien Windenergie und Photovoltaik angekündigt. Bis 2030 sollen sie 15 % des Strombedarfes des Landes decken. Ziel ist es, das Erdöl vorwiegend für den lukrativen Export zu bewahren, zudem soll die Ressourcenbasis diversifiziert und klimaschädliche Treibhausgasemissionen vermindert werden. Ein alternativer Plan, in die Kernenergie einzusteigen und ein Kernkraftwerk zu bauen, wurde hingegen aufgegeben, da Wind- und Solarenergie nach Angaben des Elektrizitätsministeriums günstiger seien.

Ein Prozent aller Erwerbstätigen war 2003 in der Landwirtschaft beschäftigt. Die Landwirtschaft ist aufgrund der klimatischen Verhältnisse, der Bodenbeschaffenheit und des Wassermangels nur bedingt expansionsfähig. Nur 0,2 Prozent der Staatsfläche werden derzeit über künstliche Bewässerung kultiviert. Angebaut werden für den heimischen Markt Datteln, Melonen und Futterklee. Zur Fleischversorgung werden vor allem Schafe, Ziegen und Rinder gehalten.

Die Importe betrugen 2014 27,38 Milliarden US-Dollar und kamen größtenteils aus den USA (12,7 %), ferner aus China (11,9 %), Saudi-Arabien (7,1 %), Südkorea (6,9 %), Japan (6,6 %), Deutschland (4,8 %) und Indien (4,2 %). Unter den Gütern waren zu 42 Prozent Maschinen und Transportausrüstungen, zu 18 Prozent Vorerzeugnisse, zu 14 Prozent Fertigerzeugnisse und zu 13 Prozent Nahrungsmittel.

Die Exporte betrugen 2014 103,40 Milliarden US-Dollar, Hauptabnehmer waren Südkorea (16,7 %), Indien (14,9 %), Japan (12,3 %) die USA (11,3 %) und China (9,9 %). Wichtigste Exportgüter waren mit über 90 % Anteil Erdöl und Erdölprodukte.

Dem mittel- und langfristigen Umbau der Wirtschaftsstruktur widmet sich Kuwait seit 2010 im Rahmen der Kuwait Vision 2035 (seit 2017 unter dem Titel "New Kuwait") bzw. in jeweils für 4-5 Jahre geltende Entwicklungspläne. Damit will Kuwait dem erklärten Ziel näher kommen, seine Wirtschaft zu diversifizieren und zum Handels- und Finanzzentrum in der Region aufzusteigen. Dem stehen aber Schwächen  der Verwaltung und langsame Entscheidungsprozesse sowie der strukturelle Konflikt Regierung/Parlament entgegen. Der Entwicklungsplan für die Jahre 2015–2020 hat einen Umfang von 34 Mrd. KWD (> 100 Mrd. EUR) und enthält zahlreiche lang diskutierte Großprojekte u.a. aus den Bereichen Infrastruktur, Tourismus, wie der Bau einer U-Bahn und einer Eisenbahn, der Ausbau des Flughafens, Kraftwerke, etc. Zuweisungen erfolgten 2014 in Höhe von 7,5 Mrd. KWD (22,5 Mrd. EUR), 2015 in Höhe von 12 Mrd. KWD (36 Mrd. EUR). Nach Aussage der Regierung soll es Kürzungen oder Verschiebungen bei den Großprojekten, die im Entwicklungsplan enthalten sind, trotz Haushaltsdefizits nicht geben.

Katar und Saudi-Arabien haben vergleichbare Entwicklungspläne vorgelegt.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 65,3 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 47,1 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 16,6 % des BIP.
Die Staatsverschuldung betrug 2016 20,2 Milliarden US-Dollar oder 18,6 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Das sehr gut ausgebaute Straßennetz umfasst etwa 3.600 Kilometer. Kuwait verfügt über einen Industriehafen sowie vier Erdölhäfen. Der internationale Flughafen liegt nahe der Hauptstadt. Mit den milliardenschweren Einnahmen aus dem Ölsektor und ermutigt durch den Sturz Saddam Husseins hat das Land eine Diversifizierung der Wirtschaft in Angriff genommen. Ein moderner Containerhafen soll gebaut und eine Insel in einen Touristenkomplex umgewandelt werden.

Es gibt Pläne zum Bau einer U-Bahn in Kuwait, deren Strecken sich auf vier Linien aufteilen und 171 Kilometer lang sein sollen. Das jährliche Fahrgastaufkommen wird auf 69,1 Millionen Passagiere geschätzt.
Geplant ist der Baubeginn für den Herbst 2011, die Fertigstellung für das Frühjahr 2016.

Die Gesamtkosten werden auf 11,3 Milliarden US-Dollar beziffert, die zu 24 % vom kuwaitischen Staat, 26 % vom Auftragnehmer und die restlichen 50 % durch einen Börsengang aufgebracht werden sollen. Dieser Ansatz ist Teil der von Kuwait angedachten Strategie, für Innovationen bei öffentlichen Projekten privates Kapital zu nutzen.

Kuwait U-Bahn-Vorschläge:

In späteren Ausbauphasen soll dann die Linie 1 um 23,6 auf 57,3 Kilometer und die Linie 2 um 16,4 auf 37,4 Kilometer verlängert werden.

Außerdem gibt es Pläne von Saudi-Arabien und den Vereinigten Arabischen Emiraten ein etwa 2000 Kilometer langes Eisenbahnnetz in der Golfregion zu errichten.
Davon würden um die 505 Kilometer zweigleisiger Strecke, elektrifiziert mit 50 kV Wechselstrom, in Kuwait verlaufen.




</doc>
<doc id="2879" url="https://de.wikipedia.org/wiki?curid=2879" title="Gemüsekohl">
Gemüsekohl

Gemüsekohl ("Brassica oleracea") ist eine formenreiche Pflanzenart der Gattung Kohl ("Brassica") in der Familie der Kreuzblütengewächse (Brassicaceae), deren Zuchtformen etliche Gemüsesorten umfassen.

Der Gemüsekohl ist eine ein- bis zweijährige (Kulturformen) oder ausdauernde, krautige Art, die jedoch am Grund verholzt sein kann. Die Wuchshöhe beträgt 40 bis 120 Zentimeter. Die Pflanzen sind abstehend-sparrig, jedoch wenig verzweigt und aufrecht. Die Blätter sind oft abwischbar blau bereift und meist kahl. Die oberen Stängelblätter sind am Grund abgerundet oder verschmälert und sitzend. 

Die Kelchblätter sind aufrecht, die Kronblätter sind schwefelgelb. Auch die Staubblätter sind aufrecht. Blütezeit ist von Mai bis September. Die Früchte sind die für die Familie typischen Schoten.

Alle Formen und Varietäten haben dieselbe Chromosomenzahl 2n = 18. Sie besitzen ca. 100.000 Gene verteilt auf ein Genom von 599–868 Mb (Millionen Basenpaare), und somit 4- bis 10-mal so viele wie die Modellpflanze "Arabidopsis thaliana".

Die Bestäubung erfolgt durch Insekten oder durch Selbstbestäubung. Die Verbreitung der Samen erfolgt durch den Wind, durch Selbstausbreitung oder durch den Menschen (Anemochorie, Autochorie, Hemerochorie).

Ursprünglich ist die Wildform in den meridionalen bis temperaten, ozeanisch geprägten Küstenbereichen Europas heimisch. Sie wächst auf Felsen, zum Teil auf Küstenkliffen, aber auch in Gebirgen. In Deutschland kommt die Wildform nur auf Helgoland vor und wird hier „Klippenkohl“ genannt. Er wächst hier in der Pflanzengesellschaft Brassicetum oleraceae (Crithmo-Armerion maritimae) an den Felshängen der Insel und teilweise an Ruderalstandorten, die den Schafen nicht zugänglich sind.

Die Zuchtformen kommen außer in Gärten und auf Äckern selten auch verwildert vor und siedeln dann auf Schutt und auf frischen, nährstoffreichen Böden.

Im Jahr 2016 wurden laut Ernährungs- und Landwirtschaftsorganisation (FAO) der Vereinten Nationen weltweit 71,3 Millionen Tonnen Gemüsekohl (einschließlich anderer Kohlarten) geerntet. Die zehn größten Produzenten ernteten zusammen 78,5 % der Welternte. Die größten europäischen Produzenten waren Polen, Rumänien und Deutschland.

Der Wildkohl kommt in fünf größeren, nicht aneinandergrenzenden Gebieten vor. Die Pflanzen dieser Gebiete unterscheiden sich hinsichtlich ihrer Morphologie (Wuchshöhe, Verzweigungsgrad, Blattmerkmale, Blütenfarbe), lassen sich jedoch frei untereinander kreuzen. Sie werden daher je nach Autor als Art oder als geografische Rasse geführt:


Die vielen Zuchtformen des Gemüse-Kohls werden als Varietäten geführt. Man unterscheidet in der Regel (in Klammern die genutzten Pflanzenteile):



Chinakohl ("Brassica pekinensis") gehört zur Art "Brassica rapa" (Rübsen).

Wann und wo die Wildformen in Kultur genommen wurden, lässt sich nicht nachvollziehen. Noch 1980 wurde etwa auf Samos die dort wild vorkommende "Brassica cretica" von den Einheimischen auf den Äckern gezogen. Alle Wildformen und Kulturformen sind miteinander kreuzbar. Der Grüne Krauskohl lässt sich zumindest für das Griechenland des 3. Jahrhunderts v. Chr. nachweisen, ebenso für Italien. In Deutschland findet er sich in den Kräuterbüchern des 16. und 17. Jahrhunderts. Kohlrabi und Markstammkohl werden von Plinius dem Älteren erwähnt, in Deutschland lässt er sich ab dem 16. Jahrhundert nachweisen. Die festen Kohlköpfe sind auch erst aus dieser Zeit nachgewiesen, dürften aber schon zur Zeit Hildegards von Bingen im 11. Jahrhundert existiert haben. Brokkoli und Blumenkohl dürften aus Südgriechenland stammen. Über Genua (um 1490) dürften sie nach Frankreich, Flandern und Deutschland gekommen sein. Die ersten Abbildungen stammen von 1542. Der Rosenkohl ist eine sehr junge Form und stammt aus dem 18. Jahrhundert, wo er zuerst in Belgien auftrat.

Neben Vitaminen und Mineralstoffen, beispielsweise Vitamin C, Vitamine des B-Komplexes, Betacarotin, Folsäure und Kalium, Calcium sowie Eisen, sind in Kohlgemüsen reichlich Ballaststoffe und sekundäre Pflanzenstoffe enthalten. Sie unterstützen die natürlichen Abwehrfunktionen des Immunsystems und können helfen, das Risiko für Krebserkrankungen zu senken.

Im Kohlgemüse sind besonders reichhaltig Glucosinolate vorhanden. Mit Ballaststoffen und weiteren Inhaltsstoffen können sie unter anderem einer Bildung von Magengeschwüren vorbeugen. Weiterhin kann der Verzehr von Pflanzen aus der Kohlfamilie dabei helfen, den Cholesterin- und Blutzuckerspiegel günstig zu beeinflussen und die Verdauung auf natürliche Weise zu regulieren. 




</doc>
<doc id="2881" url="https://de.wikipedia.org/wiki?curid=2881" title="Kohlenwasserstoffe">
Kohlenwasserstoffe

Die Kohlenwasserstoffe sind eine Stoffgruppe chemischer Verbindungen, die nur aus Kohlenstoff und Wasserstoff bestehen. Diese Stoffgruppe ist vielfältig, da Kohlenwasserstoffe Kohlenstoffketten, -ringe oder Kombinationen daraus enthalten können. Es gibt mehrere Untergruppen wie Alkane, Alkene, Alkine und Aromaten (Arene). 
Die Kohlenwasserstoffe haben vor allem durch ihre Verwendung als fossile Brennstoffe und in der organischen Synthese eine große technische Bedeutung erlangt.

Kohlenwasserstoffe sind in der Natur insbesondere in Erdöl und Erdgas in sehr großer Menge enthalten, weshalb diese beiden Bodenschätze auch unter dem Überbegriff "fossile Kohlenwasserstoffe" zusammengefasst werden. Sie sind zudem verbreitet in vielen Pflanzen z. B. als Terpene, Carotinoide und Kautschuk, und so kommen sie fossil auch in Kohle vor. Einfache Kohlenwasserstoffe, insbesondere Methan, sind Stoffwechselprodukte einiger Mikroorganismen. Im Weltall sind Kohlenwasserstoffe, meist in Form von Methan und Ethan, auf Kometen, Planeten und Monden sowie in interstellarer Materie nachgewiesen.

Kohlenwasserstoffe lassen sich in gesättigte, ungesättigte und aromatische Kohlenwasserstoffe unterteilen. 

Bei "gesättigten" Kohlenwasserstoffen handelt es sich um chemische Verbindungen, die ausschließlich C-C-Einfachbindungen enthalten. Sie werden in kettenförmige und ringförmige Verbindungen unterteilt. Kettenförmige werden systematisch als Alkane bezeichnet. Die einfachsten und bekanntesten Alkane sind Methan (CH), Ethan (CH), und Propan (CH). Allgemein lautet die Summenformel für die homologe Reihe der kettenförmigen Alkane CH. Ringförmige Alkane werden als Cycloalkane bezeichnet. Ihre allgemeine Summenformel für die homologe Reihe lautet CH.

"Ungesättigte" Kohlenwasserstoffe lassen sich in Alkene und Alkine einteilen. 
Bei den Alkenen handelt es sich um Verbindungen, die C=C-Doppelbindungen enthalten. Der einfachste Vertreter dieser Stoffgruppe ist das Ethen, auch Ethylen genannt (CH). Die einfachsten "n"-Alkene mit nur einer Doppelbindung haben allgemein die Summenformel CH. Als Polyene werden Verbindungen mit mindestens zwei C-C-Doppelbindungen, wie zum Beispiel 1,3-Butadien, bezeichnet. Cycloalkene sind cyclische Kohlenwasserstoffe, wie zum Beispiel Cyclopentadien. Hier liegen C-C-Doppelbindungen innerhalb eines Kohlenstoffringes vor. 
Alkine sind Kohlenwasserstoffe, die eine oder mehrere C-C-Dreifachbindungen enthalten. Der bekannteste Vertreter ist das Ethin (Acetylen) mit der Summenformel CH. Dementsprechend haben Alkine mit einer Dreifachbindung allgemein die Summenformel für die homologe Reihe CH. Gesättigte und ungesättigte Kohlenwasserstoffe werden besonders in der Petrochemie unter der Bezeichnung aliphatische Kohlenwasserstoffe gesammelt. 

Die letzte wichtige Gruppe von Kohlenwasserstoffen sind die "aromatischen" Kohlenwasserstoffe. Dabei handelt es sich um Kohlenwasserstoffe, die Aromatizität aufweisen und meist C-Ringe besitzen (Arene). Der bekannteste Vertreter ist das Benzol (CH). Eine Untergruppe der Aromaten sind die polyzyklischen aromatischen Kohlenwasserstoffe. Bei ihnen handelt es sich um Verbindungen, die aus mehreren aneinander hängenden Benzolringen bestehen. Ein bekannter Vertreter ist das Naphthalin (CH).

Kohlenwasserstoffe, die vom Aufbau her Platonischen Körpern entsprechen, bezeichnet man als Platonische Kohlenwasserstoffe. Hierzu gehören Tetrahedran, Cuban und Dodecahedran.

Kohlenwasserstoffe mit gleicher Summenformel können verschiedene Strukturformeln (Verknüpfungen der C-Atome) aufweisen. Es handelt sich dann um Konstitutionsisomere. Es gibt sie bei den Alkanen ab den Butanen und bei den meisten anderen Kohlenwasserstoffen. Die klassische "cis"-"trans"-Isomerie tritt mitunter an C-C-Doppelbindungen auf. Einfache verzweigte Kohlenwasserstoffe können, wie das Beispiel 3-Methylhexan zeigt, chiral sein. Das Kohlenstoffatom in Position Nummer 3 wird hier zum Stereozentrum, die Verbindung ist asymmetrisch und man unterscheidet zwischen den ("R")- und ("S")-Enantiomeren.

In Wasser sind viele unpolare Kohlenwasserstoffe unlöslich, in den meisten organischen Lösungsmitteln jedoch gut löslich. Das heißt, Kohlenwasserstoffe sind hydrophob, also auch lipophil. 

Gasförmige Kohlenwasserstoffe brennen sehr schnell und mit heißer Flamme; die dabei frei werdende Energie ist groß. Flüssige Kohlenwasserstoffe mit niedrigem Siedepunkt verdampfen leicht; infolge ihres auch niedrigen Flammpunktes sind Brände leicht zu entfachen. Bei der optimalen (vollständigen) Verbrennung von Kohlenwasserstoffen entsteht Wasser und Kohlenstoffdioxid, bei unzureichender (unvollständiger) Verbrennung können auch Kohlenstoffmonoxid oder Kohlenstoff (Ruß) entstehen. Die Reaktivität der Alkane ist abhängig von ihrer Kettenlänge. Langkettige Alkane sind relativ inert (wenig reaktiv). Verbrennt Kohlenwasserstoff mit rußender Flamme, kann dies auch ein Anzeichen für einen höheren Kohlenstoffanteil in der Verbindung (größere Kettenlänge) sein.

Alkane gehen aber – neben Redoxreaktionen bei ihrer Verbrennung – Substitutionsreaktionen ein, wobei Wasserstoffatome gegen andere Atome und Atomgruppen, aber hauptsächlich Halogenen ausgetauscht werden können. Alkene und Alkine hingegen sind recht reaktionsfreudig und reagieren mit vielen Substanzen unter Anlagerung an die C-C-Mehrfachbindung (Additionsreaktion).

Die vergleichsweise hohe Neigung von Kohlenstoffatome zur Ausbildung von Kohlenstoffketten (engl. catenation (Katenisierung)) ist für die Vielfalt der Kohlenwasserstoffe entscheidend und beruht auf der höheren Bindungsenergie der kovalenten C–C Bindungen (356 kJ/mol) im Vergleich zu höheren Homologen der 14. Gruppe (z. B. Si–Si (226 kJ/mol), Ge–Ge (186 kJ/mol)). Weiterhin ist die C–H-Bindung ebenfalls thermodynamisch stabiler als die Si–H-, Ge–H- oder Sn–H-Bindung. So kann erklärt werden, warum es eine höhere Vielfalt an Kohlen-, als an Silicium- oder Germaniumwasserstoffen gibt.

Alkane werden häufig als fossiler Energieträger in Gemischen wie Biogas, Flüssiggas, Benzin, Dieselkraftstoff, Heizöl, Kerosin, Petroleum verwendet. Die bedeutendsten Alkane sind die niedermolekularen Alkane Methan, Ethan und Propan. Alkane wie "n"-Butan, Isopentan, verschiedene Hexane und das Cycloalkan Cyclohexan sind Bestandteile in Motorenbenzin.

Die Kohlenwasserstoffe dienen als Ausgangsstoff für eine Vielzahl von industriell bedeutenden chemischen Synthesevorgängen. Von technischer Bedeutung sind Alkene wie Ethen und Cyclohexen, Alkine wie Ethin und Polyene wie 1,3-Butadien, Isopren und Cyclopentadien. Viele Arene besitzen technische Bedeutung, darunter Benzol, Toluol, Xylol und Styrol. 

Wichtig sind die Polymerisationsprodukte der Kohlenwasserstoffe wie zum Beispiel Polystyrol, Polyethylen, Polypropylen, Polyethin, viele Copolymere sowie die halogenierten Kohlenwasserstoffpolymere wie Polyvinylchlorid und Polytetrafluorethylen. Darüber hinaus dienen Kohlenwasserstoffe als lipophile Lösemittel.

Kohlenwasserstoffe machen – nach Definition der Weltgesundheitsorganisation "WHO" – den Hauptteil der "flüchtigen organischen Verbindungen" (abgekürzt auch VOC) aus; diese gelten bei Emission als umweltschädigend. Zwecks Emissions-Reduktion wird z. B. in der Schweiz auf diese Emissionen eine Lenkungsabgabe erhoben. Weiter besitzt Methangas einen erheblichen Treibhauseffekt.




</doc>
<doc id="2883" url="https://de.wikipedia.org/wiki?curid=2883" title="Kletterpflanze">
Kletterpflanze

Kletterpflanze ist die Bezeichnung für eine Pflanze, die statt stützender Strukturen als Wuchsform eine Klettertechnik (Kletterstrategie) ausbildet.
Kletterpflanzen können ein- bis mehrjährige (ausdauernde), krautige oder verholzende Pflanzen sein. Verholzende Kletterpflanzen heißen auch Lianen. Die meisten Kletterpflanzen können keinen Stamm ausbilden, um sich freistehend selber zu tragen, sondern finden an anderen Pflanzen, Felsen oder Klettergerüsten Halt. Dadurch erreichen sie rasch eine optimale Ausrichtung ihres Blattwerkes zum Sonnenlicht, ohne selbst tragende Stängel zu entwickeln. Aus etwa 90 Pflanzenfamilien gehören über 2500 Arten zur Lebens- und Wuchsform der Kletterpflanzen.

Kletterpflanzen werden nach ihrer Klettertechnik unterschieden:
Selbstklimmer können Flächen wie Mauern und Fassaden direkt bewachsen.

Die Mehrheit der Selbstklimmer sind Haftwurzelkletterer ("Haftwurzler"):

Auch Rankpflanzen mit solcherart spezialisierten Ranken, dass sie auf Flächen Halt finden, werden als selbstklimmend bezeichnet.

Beispiele dafür sind haftscheibenrankende Arten/Sorten von Wildem Wein wie "Parthenocissus" oder "Cobea scandens" mit extrem zart verästelten Ranken.

Die Jungtriebe der Selbstklimmer wachsen lichtfliehend orientiert aufwärts und die Haftorgane (Haftwurzeln, Haftscheiben) werden bevorzugt an der lichtabgewandten Seite ausgebildet. Dabei sucht die Sprossspitze einen Weg, während immer mehr Haftscheiben die statische Sicherung übernehmen. Bei einem Berührungsreiz scheiden die Haftscheiben eine Art Klebstoff aus, der die Triebe sogar an Fensterglas haften lässt. Das Anhaften kann behindert werden durch hohe Temperaturen der Wandfläche, sandenden Putz und giftige Anstriche, bei Schattenpflanzen wie Efeu auch durch sehr hell gefärbte und sonnenbeschienene Wandflächen.

Gerüstkletterpflanzen nennt man zusammenfassend Kletterpflanzen, die Hilfseinrichtungen (zum Beispiel Rankgerüste) benötigen.




Verschiedene Kletterpflanzen wachsen auch in schattiger Lage und an schlecht belichteten Nordfassaden:

Einige Kletterpflanzen können in begrenztem Umfang auch freistehend wachsen. So ändert der Efeu in höherem Alter seine Wuchsform und ist dann in der Lage seinen Wirtsbaum zu überwachsen, falls dieser absterben sollte. Auch die Kletter-Hortensie bildet einen bis 2 Meter hohen, halbkugeligen sommergrünen Busch, wenn sie keine Klettermöglichkeit vorfindet.

Kletterpflanzen können durch eine Fassadenbegrünung einen wichtigen Beitrag zu einer Bauwerksbegrünung darstellen. Hierbei schützen sie die Fassade vor mechanischen (Hagel, Regen), optischen (UV-Strahlung) sowie thermischen Witterungseinflüssen, indem sie während der Vegetationsphase im Wesen nach eine zusätzliche Fassadenschicht bilden. Die Lebensdauer der eigentlichen Fassade kann sich dadurch beträchtlich erhöhen.

Die zusätzliche Luftschicht vor der Fassade sowie die Transpirationsabkühlung und Konvektion durch Verdunstung von den Blattoberflächen leistet einen positiven Beitrag zur Temperaturpufferung zwischen Gebäude und Umgebung sowie zum lokalen Mikroklima.

Darüber hinaus bietet die grüne Fassadenhülle vielen Tierarten (Vögel, Insekten) Schutz und Lebensraum.

Lediglich erneuerungsbedürftige Fassaden mit losem und hohlliegendem Putz sowie Mauerwerk mit absandendem Fugenmörtel können durch Kletterpflanzen zusätzlich geschädigt werden.

An und in Bauwerken, auch auf Balkonen und Dachgärten sowie in Gärten und Parkanlagen werden nicht selbstklimmende Kletterpflanzen an Kletterhilfen oder Rankgerüsten gezogen. Am Übergang vom Garten zu einem Bauwerk stellt eine Pergola eine nützliche Kletterhilfe dar.

Unter den Kletterpflanzen gibt es etliche Nutzpflanzen, darunter die bereits angeführten Weinreben und Brombeeren. Weitere Beispiele sind u. a. Kiwisorten, Bohnen, Erbsen, Kürbisgewächse (darunter u. a. Gurken und Melonen). Zu den kletternden Nutzpflanzen gehören ferner Gewürz- und Heilpflanzen, darunter Pfeffer, Vanille, "Schisandra" und Arten der Familie Dioscoreaceae.

Winterharte verholzende Kletterpflanzen sind ein wichtiger Bestandteil eines Baumschulsortiments und werden in der Baumschule durch Stecklinge, Wurzelschnittlinge, Steckholz oder Aussaat vermehrt und in einem Container weiterkultiviert.

Beim Einsatz von Kletterpflanzen zur Fassadenbegrünung ist auf Vereinbarkeit von Bewuchs und Bauwerk zu achten. Kletterstrategie, Fassade und eventuell erforderliche Kletterhilfen müssen ebenso aufeinander abgestimmt sein, wie Klima, Licht und Größenverhältnisse berücksichtigt werden müssen. Vor allem unangepasst starkwüchsige Kletterpflanzen verursachen in der Regel hohen Pflegeaufwand und unter Umständen sogar Schäden.

Nicht winterharte, meist krautige Kletterpflanzen werden von Zierpflanzengärtnereien angeboten. Es gibt Sortimente für Wintergärten sowie für Innenräume.

Bekannte verholzende Kletterpflanzen sind:

Kletterpflanzen gibt es in vielen Pflanzenfamilien (Auswahl):

Sogar Kletterfarne gibt es (Auswahl):



</doc>
<doc id="2884" url="https://de.wikipedia.org/wiki?curid=2884" title="Kraut">
Kraut

Das Kraut (von ahd. "krut" ‚nutzbares Gewächs‘, nur teilweise Mehrzahl "Kräuter") steht für:


Kraut steht zudem für:
Kraut ist der Familienname folgender Personen:


Siehe auch:


</doc>
<doc id="2885" url="https://de.wikipedia.org/wiki?curid=2885" title="Kulturpflanze">
Kulturpflanze

Eine Kulturpflanze ist eine Pflanze, die durch das Eingreifen der Menschen zielgerichtet als Nutz- oder Zierpflanze angebaut, kultiviert und züchterisch bearbeitet wird.

Eine Kulturpflanzensorte, die sich von anderen, verwandten Sorten anhand morphologischer, physiologischer, zytologischer, chemischer oder anderer Merkmale unterscheidet, nennt man auch Kulturvarietät oder Cultivar. Der Unterschied ist die Rangstufe im internationalen Code der Nomenklatur der Kulturpflanzen, der die einheitliche Benennung von Kulturpflanzensorten regelt. 

Kulturpflanzen weisen im Gegensatz zu Wildpflanzen typische Veränderungen auf. Beispielsweise kann die Keimruhe der ausgereiften Samen reduziert sein. Bei Getreidearten fallen die später zu erntenden Samen nicht so schnell aus ihrem Sitz. Einige Kulturpflanzen wie etwa manche Zitrusfrüchte bilden gar keine Samen mehr aus. Typisch für die meisten Kulturpflanzen ist auch ein Riesenwuchs im Vergleich zur Wildpflanze.




</doc>
<doc id="2889" url="https://de.wikipedia.org/wiki?curid=2889" title="Krieg">
Krieg

Krieg ist ein organisierter und unter Einsatz erheblicher Mittel mit Waffen und Gewalt ausgetragener Konflikt, an dem oft mehrere planmäßig vorgehende Kollektive beteiligt sind. Ziel der beteiligten Kollektive ist es, ihre Interessen durchzusetzen. Der Konflikt soll durch Kampf und Erreichen einer Überlegenheit gelöst werden. Die dazu stattfindenden Gewalthandlungen greifen gezielt die körperliche Unversehrtheit gegnerischer Individuen an und führen so zu Tod und Verletzung. Neben Schäden an am Krieg aktiv Beteiligten entstehen auch immer Schäden, die meist eher unbeabsichtigt sind. Sie werden heute euphemistisch als Kollateralschäden bzw. Begleitschäden bezeichnet. Krieg schadet auch der Infrastruktur und den Lebensgrundlagen der Kollektive. Eine einheitlich akzeptierte Definition des Krieges und seiner Abgrenzung zu anderen Formen bewaffneter Konflikte existiert nicht.

Kriegsformen sind vielfältig und nicht unbedingt an Staaten oder Staatssysteme gebunden: Sie können auch innerhalb von Staaten stattfinden, etwa als Bürgerkrieg, Unabhängigkeitskrieg oder bewaffneter Konflikt, und zum Weltkrieg oder zum Völkermord führen. Trotz intensiver Diskussionen konnte keine einheitliche völkerrechtliche Definition gefunden werden, die den Begriff des Krieges eingrenzend beschreibt. Die Genfer-Fünf-Mächte-Vereinbarung vom 12. Dezember 1932 ersetzte deswegen den unspezifischen Ausdruck „Krieg“ durch den eindeutigen der „Anwendung bewaffneter Gewalt“ (Artikel III). Die Charta der Vereinten Nationen verbot schließlich die Anwendung von oder Drohung mit Gewalt in internationalen Beziehungen grundsätzlich (Artikel 2, Ziffer 4) und erlaubte sie nur als vom Sicherheitsrat beschlossene Sanktionsmaßnahme (Artikel 42) oder als Akt der Selbstverteidigung (Artikel 51).

In der historisch belegten Menschheitsgeschichte sollen knapp 14.400 Kriege stattgefunden haben, denen ungefähr 3,5 Milliarden Menschen zum Opfer gefallen sein sollen. Da bisher schätzungsweise 100 Milliarden Menschen gelebt haben, würde dies bedeuten, dass jeder dreißigste Erdenbürger sein Leben durch kriegerische Handlungen lassen musste. Jedoch erwähnt eine kritische Beurteilung dieser Schätzung, dass einer der Vertreter dieser Schätzung die Zahl von ca. 3.640.000.000 Kriegsopfern auf ca. 1.240.000.000 reduziert hat. Die kleinere Zahl ist plausibler, könnte jedoch ebenfalls zu hoch gegriffen sein, zumal die 100 tödlichsten Gewaltakte der Weltgeschichte insgesamt etwa 455 Millionen Todesopfer forderten.
Das Wort „Krieg“ (von althochdeutsch "chreg" > mittelhochdeutsch "kriec" bedeutet ursprünglich „Hartnäckigkeit“, „Anstrengung“, „Streit“, „Kampf“, „Bewaffnete Auseinandersetzung“.) In diesem etymologischen Umkreis angesiedelt sind auch mittelniederdeutsch "krich" und mittelniederländisch "crijch". Eine akademische Rekonstruktion führt neuhochdeutsch „Krieg“ auf die indogermanische Wurzel "*grei-" zurück. Diese hat ihre Entsprechung in griechisch "brímē" mit der Bedeutung „Gewalt, Wucht, Ungestüm“ und "hýbris" mit der Bedeutung „Überheblichkeit, Gewalttätigkeit“.

In einem weiteren sprachgeschichtlichen Zusammenhang wird auch das neuhochdeutsche Wort „Kraft“ hier eingeordnet, das möglicherweise aus der gleichen indogermanischen Wurzel entstanden ist. Die große Bandbreite der Bedeutungen spiegeln das altfriesische "halskrīga" mit der Bedeutung „Halssteifheit“ sowie die vermutlich in Verbindung stehenden Begriffe altirisch "bríg" mit der Bedeutung „Kraft, Macht“ und für „Härte, Strenge“ wider.
Der Kollektivsingular, der alle Kriege subsumiert, entstand um 1800. Ältere Enzyklopädien behandeln unter „Krieg“ einzelne Kriege oder spezifische Fragen der Kriegsführung. Ein veraltetes Wort für Krieg ist "Orlog" (noch heute niederländisch und afrikaans: "oorlog").

Das Verbum „jemanden bekriegen“ heißt einerseits „gegen ihn Krieg führen“, andererseits hat das Grundwort "kriegen" die Bedeutung „etwas bekommen, erhalten“, „jemanden erwischen“ Beide Bedeutungen sind geeignet, Herkunft und Charakter dieser kollektiven Gewaltanwendung anzuzeigen. Auch wo andere Kriegsanlässe im Vordergrund stehen, fehlt selten ein ökonomischer Hintergrund.

Während individuelles oder kollektives Rauben und absichtliches Töten von Menschen heute generell als Verbrechen gilt und in einem Rechtsstaat strafbar ist, wird „Krieg“ nicht als gewöhnliche Kriminalität betrachtet, sondern als bewaffnete Auseinandersetzung zwischen Kollektiven, die sich dazu legitimiert sehen. Damit hebt ein Krieg die zivilisatorische Gewaltbegrenzung auf eine Exekutive, wie sie der Rechtsstaat als Regelfall voraussetzt, partiell oder ganz auf: Es stehen sich bewaffnete Armeen gegenüber, die ganze Völker oder Volksgruppen repräsentieren. Diese sind damit Kriegspartei.

Kriegsparteien beurteilen ihre eigene Kriegsbeteiligung immer als notwendig und gerechtfertigt. Ihre organisierte Kollektivgewalt bedarf also einer Legitimation. Krieg als Staatsaktion erfordert daher ein Kriegsrecht im Innern eines Staates sowie ein Kriegsvölkerrecht zur Regelung zwischenstaatlicher Beziehungen. Dieses unterscheidet vor allem Angriffs- von Verteidigungskrieg.

Notwendige Bedingung für eine Klassifikation der Kriege in Kriegstypen ist eine Definition des Begriffes „Krieg“ mittels Bestimmung dessen Wesens. Im Bemühen um den Versuch einer solchen Definition hat sich der preußische General, Militärtheoretiker und-historiker Carl von Clausewitz verdient gemacht. Zuerst entdeckte er den Zusammenhang zwischen Politik und Krieg. Er erkannte, dass die politischen Beziehungen (z.B. zwischen Staaten) zunächst zum politischen Konflikt führen, der dann durch Einschaltung eines besonderen Mittels, und zwar dem Krieg, gelöst wird. 

Weiterhin begriff er, dass die zum Kriege führenden politischen Beziehungen nicht mit dessen Beginn enden, sondern durch den Krieg fortgeführt werden, diesen bis einschließlich des Friedensschlusses beeinflussen. So gelangte er zu der Definition: „Der Krieg ist eine bloße Fortsetzung der Politik mit anderen Mitteln.“ Und er führt weiter aus: Der Krieg hat „nicht seine eigene Logik“ und „das Unterordnen des politischen Gesichtspunktes unter den militärischen wäre widersinnig, denn die Politik hat den Krieg erzeugt; sie ist die Intelligenz, der Krieg aber bloß das Instrument, und nicht umgekehrt.“ Behauptet man eine Teilung der Gesellschaft in Klassen, wäre die folgende Definition zu hinterfragen, die zwar vom Begriffsinhalt nach von Clausewitz ausgeht, die Begriffsintension aber wesentlich erweitert. „Der Krieg ist die Fortsetzung und das wirklich oder scheinbar letzte Mittel der Innen- und Außenpolitik von Klassen sowie politischen Gruppen mittels beidseitiger Anwendung organisierter bewaffneter Gewalt zur Herstellung und Erhaltung beziehungsweise Beseitigung und Milderung von Ausbeutungsverhältnissen.“ 

Kriege lassen sich demnach in verschiedene Grundtypen einordnen:

Ein zwischenstaatlicher Krieg findet zwischen zwei oder mehreren Staaten statt. Dazu gehört der Koalitionskrieg: Mehrere Staaten verbinden sich zu einer gemeinsam agierenden Kriegspartei. Ist ein Land bereits besetzt und seine Regierung entmachtet, kann der Kampf zwischen Staaten als Partisanen- oder Guerillakrieg zwischen Bevölkerung und feindlicher Staatsarmee fortgesetzt werden: Nichtreguläre Streitkräfte kämpfen militärisch gegen die Armee einer Besatzungsmacht.

In einem Bürgerkrieg dagegen kämpfen verschiedene Gruppen innerhalb eines Staates, teilweise auch über Staatsgrenzen hinweg, oft nicht staatlich organisiert. Auch dieser kann mit nichtregulären Streitkräften, „Privatarmeen“ und/oder Söldnern gegen die Armee der eigenen Staatsregierung geführt werden.

In einem Unabhängigkeitskrieg kämpft ein Volk um einen eigenen Staat: z. B. als "Dekolonisationskrieg" gegen eine Kolonialmacht, als "Sezessionskrieg" für die Loslösung eines Teilgebiets vom Staatsverband oder als "Krieg um Autonomie" für eine regionale Autonomie innerhalb eines Staates. Bei diesen Arten handelt es sich oft um die Folge eines Nationalitätenkonflikts.

Ob es sich um einen "Bürgerkrieg" oder einen "Unabhängigkeitskrieg" handelt, hängt oftmals vom Standpunkt der jeweiligen Kriegspartei ab. So wird die Partei, die sich abspalten möchte, eher von einem "Sezessionskrieg" sprechen, während die Partei, die auf einem einheitlichen Staat beharrt, denselben Konflikt als (innerstaatlichen) "Bürgerkrieg" ansehen wird.

Als bewaffneter Konflikt gilt ein sporadischer, eher zufällig und nicht strategisch begründeter bewaffneter Zusammenstoß zwischen kämpfenden Parteien. Die bloße Anzahl von Verletzten und Getöteten ist kein verlässliches Kriterium. Trotzdem nehmen große Forschungsprojekte das Maß von 1.000 Toten als groben Indikator dafür, dass ein bewaffneter Konflikt sich zum Krieg gesteigert hat. Manche Kriegsdefinitionen verlangen auch ein Minimum an kontinuierlichem planerischem und organisatorischem Vorgehen bei mindestens einem der Kontrahenten. Als weiteres Kriterium wird manchmal angesehen, dass mindestens eine der kämpfenden Parteien ein Staat sein muss, der sich mit seinen Streitkräften an der Auseinandersetzung beteiligt.

Ein bewaffneter Konflikt, der durch den Gegensatz konventionell überlegenen Militärs auf der einen Seite, und ihre Schwäche mittels Guerillatechniken ausgleichenden Gegnern auf der anderen Seite, geprägt ist, gilt als asymmetrischer Konflikt. Beispiel für einen solchen Konflikt ist auch der heutige „Krieg gegen den Terror“, den die USA nach den Anschlägen vom 11. September 2001 ausgerufen haben. In ihm kämpft eine Staatenkoalition (Koalitionskrieg) gegen eine bzw. mehrere als weltweite Kriegspartei(en) auftretende terroristische Gruppierung(en). Ob es sich bei dem vermehrten Auftreten asymmetrischer Konflikte um ein neues oder altes, nur stärker auftretendes Phänomen handelt, ist Gegenstand von Diskussionen.

Ob eine bewaffnete Auseinandersetzung – u. a. in den Medien – als „Konflikt“ oder als „Krieg“ bezeichnet wird, ist oft von politischen oder propagandistischen Erwägungen abhängig. Eine Auseinandersetzung, die schon den politikwissenschaftlichen Kriterien eines Krieges entsprechen würde, kann z. B. in der Sprachregelung von Drittstaaten bewusst weiterhin als Konflikt bezeichnet und behandelt werden, um sich damit besser einem Beistandsversprechen „im Kriegsfalle“ oder anderem angemessenem Druck auf die Konfliktparteien entziehen zu können. Bei der Höherstufung eines einfachen bewaffneten Konflikts zu einem Krieg gilt analog das Gleiche.

Subformen des Krieges oder analog so bezeichnete Konflikte sind unter anderem die Fehde, Bandenkriege, Blumenkriege und Wirtschaftskriege.

Kriege werden immer auf drei Ebenen mit unterschiedlicher Entscheidungsgewalt organisiert und geführt:

Beim Krieg sind die vordergründigen Kriegsanlässe von den tieferen Kriegsursachen zu unterscheiden. Die meisten Kriege lassen sich auf einige Hauptursachen zurückführen. Dazu gehören vor allem:

Krieg ist jedoch selten monokausal zu erklären: Viele der hier genannten ökonomischen, politischen, ideologischen, religiösen und kulturellen Kriegsgründe spielen in der Realität zusammen, bedingen sich gegenseitig und gehen ineinander über. Darum lässt sich der Kriegsbegriff auch nicht auf militärische Aggressionshandlungen einengen. Diese durchlaufen fast immer eine Vorbereitungsphase: Krieg beginnt in der Regel im „Frieden“. Wirklicher Frieden ist also mehr als die Abwesenheit von Krieg.

Im modernen Völkerrecht wird der Begriff „Krieg“ nicht mehr verwendet. Die Genfer Konventionen unterscheiden bewaffnete internationale Konflikte von anderen Formen gewaltsamer Konfliktaustragung wie etwa innerstaatlichen Konflikten. Der internationale bewaffneten Konflikt wird geregelt durch die Genfer Abkommen I–IV, sowie über das Zusatzprotokoll I über den Schutz der Opfer internationaler bewaffneter Konflikte. Angriff und Verteidigung, Zivilisten und Militärpersonal sind dabei wesentliche Kriterien. Sie unterscheiden legitime von illegitimen Kriegshandlungen. Was ein internationaler bewaffneter Konflikt ist, definieren die Genfer Abkommen aber nicht. Den nicht-internationalen Konflikt regelt der gemeinsame Artikel der Genfer Abkommen I–IV sowie das Zusatzprotokoll II über den Schutz der Opfer nicht internationaler bewaffneter Konflikte.

Der zwischenstaatliche Krieg soll gemäß seinen Regeln mit einer "Kriegserklärung" beginnen. Diese war im Mittelmeerraum schon seit der Antike vorgesehen. Sie wird seit der Neuzeit aber sehr oft übergangen und durch den Angriff selbst ersetzt.

Ein erklärter Kriegszustand, bei dem jedoch die Waffen schweigen, heißt Waffenstillstand, ein formales Eingeständnis der Niederlage Kapitulation. Diese beendet regulär die Kriegshandlungen, aber noch nicht den Krieg selbst.

Gegenbegriff zum „Krieg“ ist der „Frieden“. Dieser setzt völkerrechtlich wiederum einen wie auch immer gearteten Friedensabschluss zwischen ehemaligen Kriegsgegnern voraus. Wird eine Kriegspartei im Krieg jedoch weitgehend oder vollständig zerstört, so dass sie nicht mehr Vertragspartner sein kann, spricht das Völkerrecht von "Debellation" (Lateinisch: „Besiegung“).

Historisch häufiger aber sind Zwischenzustände wie der einer dauerhaften Besetzung ohne geltenden Friedensvertrag oder ein Zustand, bei dem sich die Gegner ständig auf einen offenen Krieg vorbereiten, dessen Verlauf planen und einüben. Paradebeispiel dafür ist der Kalte Krieg.

Zugleich zeigt die Verbindung von Staat und Krieg sowie die Schwierigkeiten bei der Unterscheidung von Krieg, Raub und Mord das Fehlen einer allgemein akzeptierten Rechtsinstanz an. Die UN-Charta und der Internationale Strafgerichtshof können als Schritte zur verbindlichen Durchsetzung des Völkerrechts angesehen werden.

In der politikwissenschaftlichen Teildisziplin der Internationalen Beziehungen sind Kriege naturgemäß Gegenstand vielfältiger Betrachtungen (s. Strategische Studien, Friedensforschung). Dabei haben sich verschiedene theoretische Erklärungsansätze für ihre Entstehung herausgebildet.

Die realistischen Schulen (Realismus und Neorealismus) sehen in den Staaten die eigentlichen Akteure des Geschehens, ihre Interessen und daran ausgerichteten Handlungen entscheiden über Krieg und Frieden, oft auch über Bürgerkriege in Drittstaaten (Stellvertreterkriege). Realistisch orientierte Theoretiker (Edward Hallett Carr, Hans Morgenthau) sehen im Machtinteresse des einzelnen Staates Grund für Instabilitäten der Staatenkonstellationen. Von einem pessimistischen Menschenbild ausgehend weisen sie den aggressiven Handlungen politischer Führungspersönlichkeiten eine große Rolle bei der Kriegsentstehung zu. Neorealisten wie Kenneth Waltz hingegen diagnostizieren weniger in einem auf aggressiv angestrebte Veränderung des Status quo ausgerichteten Staatshandeln das Problem, sondern in der Staatenkonstellation selbst. Denn da jeder Staat den anderen Staaten prinzipiell mißtraut, besteht ein grundsätzliches Sicherheitsdilemma: Die Furcht vor den anderen Staaten führt zu eigener Absicherung durch Rüstung, diese wiederum wird von diesen als Bedrohung der eigenen Position wahrgenommen und führt zur Gegenrüstung, welche wiederum als Bestätigung der Eingangsbefürchtungen genommen wird. Im Ergebnis kann so auch zwischen faktisch kriegsunwilligen Staaten ein kriegerischer Konflikt entstehen, bspw. ein Präventivkrieg. Je multipolarer die Konstellation, desto multifaktorieller die Kausalität, desto größer das Risiko für eine unerwartete Eskalation.

Der liberale Ansatz in den Internationalen Beziehungen hingegen verweist hingegen primär auf die interne Willensbildung der Staaten. Deren von Einzel- und Gruppenentscheidungen entlang gesellschaftlicher Konfliktlinien formulierte Politikgestaltung beeinflusst die jeweilige Außenpolitik der Staaten. Interne Faktoren wiegen so in der Regel schwerer als externe, und die Akteure bleiben gemeinhin auch an der Innenpolitik orientiert. Die innere Verfasstheit von Staaten spielt bei der Formulierung der Außenpolitik somit die entscheidende Rolle. Gemäß dem liberalen Ansatz sind demokratische Staaten mindestens untereinander signifikant friedlicher als andere Staaten, bis hin zur weitreichenden Aussage, dass demokratische Staaten gegeneinander keine Kriege führen (vgl. Demokratischer Frieden). In Ergänzung oder Konkurrenz dazu existiert die gleichfalls liberale Vorstellung eines Kapitalistischen Friedens, die davon ausgeht, dass ökonomisch eng verbundene und für einander insofern wichtige Staaten aus Eigeninteresse untereinander Kriege vermeiden.

Zweifel an der Kausalität der vorgebrachten realistischen, neorealistischen und liberalen Erklärungsgründe werden von der konstruktivistischen Schule vorgebracht: Für Krieg entscheidend seien weder aggressive Staaten, noch die grundsätzlich anarchische Staatenkonstellation, sondern psychologische und massenpsychologische Mechanismen, die kollektive Feindbilder innerhalb der Staaten, ihrer Eliten und Gesellschaften erst konstruierten und Kriege so ermöglichen würden. Erst ihre Aufdeckung und Hinterfragung bei Bevölkerungen und Akteuren diene wirksam der Kriegsvermeidung; nicht gegebene Situationen seien der Kern des Problems, sondern die Art und Weise wie diese wahrgenommen und bewertet werden.

Vertrauensbildende Maßnahmen zwischen den Staaten und Teilnahme an internationalen Regimen wie den Vereinten Nationen können nach neorealistischer und liberaler Auffassung eine auf Konfliktvermeidung ausgerichtete Staatengesellschaft formen helfen; liberale Politikwissenschaftler gehen dabei davon aus, dass dann auch gemeinsame und gemeinsam weiterentwickelte Werte eine Rolle spielen, Neorealisten verweisen nicht auf Werte, sondern auf das Interesse der Staaten an Regeln für einen nichtkriegerischen Konfliktaustrag. Dementsprechend haben stärkere Staaten eine größere Möglichkeit, ihre Interessen durch die Aushandlung von Normen auszudrücken.

Eine Abkehr des Krieges von seiner Bindung an Staaten oder institutionell verfestigte Akteure könnte die in den IB vorgebrachten Analysen entwerten. Das vermehrte Aufkommen asymmetrischer Konflikte wird tendenziell mit Sorge betrachtet, da es zu eine Schwächung der Rolle der Staaten führe. Mit leichten und billig zu beschaffenden Waffen geführte Bürgerkriege sich flexibel bildender Gruppen mit Guerillataktiken, die sich durch Raub aus dem Krieg selbst ernährten, ihn mittels krimineller Aktivitäten finanzierten (illegaler Rohstoffhandel, Drogenhandel etc.) oder politisch kaum eingrenzbar formulierte Ziele verfolgten, könnten zu einer Zerrüttung der Staatenwelt führen, so dass der im Kern kontrollierbare und durch politische Maßnahmen beendbare Staatenkrieg zugunsten eines potentiell unbeendbaren Krieges zahlreicher (potentiell sehr kleiner) Parteiungen religiöser, politischer oder rein krimineller Natur zurückträte. Befürchtet wird, dass zahlreiche niedrigschwellige Konflikte so anders als früher nicht mehr durch Erschöpfung alleine endeten oder zu einer pazifierenden Staatenbildung mit Gewaltmonopol führten, sondern durch den allseits möglichen Rückgriff auf Ressourcen der Weltwirtschaft (und ihrer Schwarzmärkte) die Gründe ihrer Fortsetzung in sich selbst fänden. während die etablierten Staaten aus moralischen Gründen oder mit Rücksicht auf mangelnde Verlust- und Kampfbereitschaft ihrer eigenen Bevölkerungen ihr potentiell übermächtiges militärisches Befriedungspotential nicht ausspielen könnten.

Eine verbreitete Vorstellung sieht den Ursprung des Kriegs in der Naturgeschichte der Aggression (Sigmund Freud, Konrad Lorenz, Irenäus Eibl-Eibesfeldt). Eine Debatte über Kriege unter Tieren, in erster Linie unter nicht-menschlichen Primaten, schloss sich in der Verhaltensforschung und Primatologie an. Gemeine Schimpansen – nicht aber Bonobos – kennen sowohl die koordinierte Jagd zum Nahrungserwerb wie intraspezifische Konkurrenz in Form innerartlicher Kämpfe, in denen einzelne Angehörige anderer Horden überfallen und getötet werden, bis hin zur allmählichen Vernichtung der anderen Gruppe. Da zwischen Gemeinen Schimpansen und den Vorfahren der heutigen Menschen ein enges Verwandtschaftsverhältnis besteht, wird ein Bezug zwischen den Kämpfen von Schimpansen und dem Verhalten heutiger Menschen gesehen. Archäologisch eindeutige Befunde für Kämpfe früher Menschenformen wie Australopithecen fehlen allerdings, ähnlich wie auch die Kämpfe heutiger Schimpansen archäologisch nicht nachweisbar wären, und nur durch direkte Beobachtung nachgewiesen wurden. Der menschliche Aggressionstrieb kann sich parallel auch aus der Abwehr gegen Raubtiere entwickelt haben. Mit der Entwicklung einfacher Waffen und der Verwendung von Feuer wurden Raubtiere als grundsätzliche Gefahr für die menschliche Spezies ausgeschaltet, die Methoden zur Abwehr und Jagd können prinzipiell auch auf den Kampf mit anderen Menschen übertragen werden. Menschen auf der Stufe des Homo erectus kannten das Feuer und verfügten über sorgfältig hergestellte Waffen (siehe Schöninger Speere), ob sie diese über die Jagd hinaus verwendeten, ist ungewiss. Inwieweit es Konflikte zwischen modernen Menschen und Neanderthalern gab und ob diese zum Aussterben der letzteren beitrugen, ist gleichfalls bislang unbeantwortbar.

Kriegerische Auseinandersetzungen zwischen Dörfern und Sippenverbänden beobachteten Ethnologen noch bei heute lebenden Steinzeitvölkern wie den Yanomami oder den Maring in Papua-Neuguinea. So dokumentierte der Spielforscher Siegbert A. Warwitz im Rahmen eines Forschungsprojekts zum Spielen von Urvölkern das Ausarten eines Völkerballspiels zwischen zwei Stämmen im Hochland von Papua-Neuguinea zu einem mit Dreschflegeln, Mistgabeln und Sensen ausgetragenen blutigen Stammeskrieg. Auch archäologische Befunde verdeutlichen, dass organisierte Gewalt bereits in frühen Gesellschaften zu massiven Auseinandersetzungen geführt hat, die man als Kriege bezeichnen könnte. Im gewissen Gegensatz dazu steht die These, zum Krieg gehöre außer dem physischen Kampf notwendig auch ein Kriegsdiskurs in öffentlichen Medien. Erweitert man den Kriegsbegriff auf diese Weise, kann Krieg im eigentlichen Sinn erst mit der Entwicklung der öffentlichen Kommunikation in urbanisierten Zivilisationen entstehen. Frühere Formen der organisierten Gewalt wären lediglich als "Vorgeschichte" des Kriegs zu verstehen.

Versteht man Krieg hingegen primär als Vorhandensein gewalttätiger und tödlicher Auseinandersetzungen mit starken Auswirkungen für die beteiligten Gemeinschaften, lässt sich aus archäologischen Befunden in verschiedenen Erdteilen auf das Vorhandensein zahlreicher Konflikte bereits vor Entstehung von Hochkulturen und Staaten schließen. Kriege in diesem Sinne begleiten die menschliche Kulturgeschichte also nicht erst seit der Hochkulturphase. Archäologische und anthropologische Befunde lassen vielmehr darauf schließen, dass bereits vorstaatliche Gesellschaften kriegerische Konflikte bis hin zur Vernichtung gegnerischer Familien, Clans oder sonstiger Gruppen kannten (s. Massaker von Talheim). Vorstaatlichen Stammesgesellschaften wurden zwar in der Kriegsforschung Charakteristika planvoller Kriegführung abgesprochen, da sie sich eher auf „Überfälle“ und „Hetzjagden“ konzentriert hätten, während ihre Schlachten ritualisiert und mit geringen Opfern abgelaufen seien, jedoch wies eine solche fortgesetzte Art des Vorgehens – darauf deutet die Auswertung von archäologischen und anthropologischen Ergebnissen hin – vermutlich sogar eine dauerhaft höhere Todesrate auf, als sie selbst in modernen Kriegen auftritt. Generell schwierig und abhängig von jeweiligen Interpretationen archäologischer Funde allerdings ist die Abgrenzung kriegerischer von alltäglicher Gewalt im Sinne von Morden und sonstiger Tötungen und die daraus resultierende Einordnung der Opfer. Bereits Jäger und Sammler kannten kriegerische Auseinandersetzungen, diese steigerten sich aber noch während des allmählichen und stufenweisen Übergangs zur (mit starken Bevölkerungssteigerungen verbundenen) Landwirtschaft, da Bauern zum einen notgedrungen ortsfest sind und Angreifern kaum ausweichen können, andererseits aber durch ihre Vieh- und Vorratshaltung über wertvolle mobile Güter verfügen; abgesehen davon, dass auch ihre Felder und Häuser selbst für bäuerliche Nachbarn interessant sein können. Eine im neolithischen Jericho nachweisliche und mit erheblichem Aufwand erbaute Stadtmauer aus Stein wird als Verteidigungsanlage gedeutet, aus der auf das Vorhandensein gut organisierter Angreifer und Verteidiger geschlossen werden kann. Belege ähnlicher neolithischer Bauwerke finden sich auch an zahlreichen anderen Stellen der Welt, wie etwa China.

Aber wohl erst mit dem bronzezeitlichen Aufkommen von staatsähnlichen Gebilden, die im Altertum fast immer Monarchien waren, entstanden Kriege mit speziell zum Kämpfen abgestellten Heeren. Die Machthaber bedienten sich der Heere in Konflikten um Ressourcen und Machtausdehnung, sei es untereinander oder in der Abwehr gegen nomadische Räuber oder wandernde Großgruppen wie die Seevölker. Die erste in ihrer eigenen Zeit gut dokumentierte Schlacht war die Schlacht bei Megiddo, in der ein ägyptisches Heer unter Thutmosis III. 1457 v. Chr. eine Koalition gegnerischer syrischer Fürsten besiegte. Die metallurgischen und arbeitsteiligen Erfordernisse u. a. der Waffenproduktion in bronze- und eisenzeitlichen Hochkulturen setzten eine Stratifizierung und zunehmende Komplexität der Gesellschaften voraus, Streitwagen waren schlachtenprägend.

Antike Großreiche wie das Assyrische Reich entstanden auch aus organisierten Raubzügen. Sie stellten die eroberten Gebiete unter Tributzwang, versklavten oder deportierten Teile der Bevölkerung und setzten militärische Siege in eine dauerhafte Herrschaft über eroberte Gebiete um.

Krieg wurde in der Antike eher als unvermeidbare und stets wiederkehrende Gegebenheit gesehen, denn als Ausnahme. Gibt der Dichter Homer (8. Jahrhundert v. Chr.) in seiner Ilias angesichts der Dauerkriege von Göttern und Menschen der Hoffnung Ausdruck „"Schwände doch jeglicher Zwiespalt unter Göttern und Menschen"“, hält die Befriedung streitender Parteien also für wünschenswert, ist für den vorsokratischen Philosophen Heraklit (um 520 bis 460 v. Chr.) der Krieg ein notwendiger, immerwährender, das Dasein konstituierender Prozess, dessen Missachtung als Torheit erscheint: Für Heraklit verkörpert der Krieg den natürlichen Prozess beständigen Werdens und Wandels. Er bezeichnet ihn als „"Vater aller Dinge und […] König aller. Die einen macht er zu Göttern, die andern zu Menschen, die einen zu Sklaven, die andern zu Freien".“

Kriegführung zwischen den griechischen Stadtstaaten der klassischen Zeit war geprägt von kurzen und harten Schlachten mit Fußsoldaten, in denen schwer bewaffnete Hopliten als Bürgersoldaten in enger Formation (siehe Phalanx) aufeinander trafen und in einem blutigen Kampf die Entscheidung suchten; Distanzwaffen und Reiterei spielten hingegen eine nachrangige Rolle. Dieses für den Einzelnen hochgefährliche Vorgehen erforderte ein hohes Kampfethos und ein Vorhandensein aufeinander eingespielter, (d. h. gut ausgebildeter) Kämpfer und Einheiten. Strittig ist, ob darin der Anfang einer möglicherweise spezifischen und bis heute aufzeigbaren europäischen oder westlichen Art der Kriegführung zu sehen ist, die nicht auf Erschöpfung des Gegners setzt, sondern auf seine konzentrierte Niederwerfung (und idealerweise Vernichtung) in kriegsentscheidenden Schlachten durch arbeitsteilige und disziplinierte Truppen, welche ihre Kampfbereitschaft aus einem bürgerlichen Ethos ziehen. Der dem Krieg folgende Friede bedurfte besonderer Vertragsschlüsse. Im Griechenland des 4. vorchristlichen Jahrhunderts gab es infolge der Entwicklung nach dem Peloponnesischen Krieg, der die Instabilität der multipolaren Polis-Ordnung Griechenlands aufgezeigt hatte, jedoch mehrere – erfolglose – Versuche, durch die Idee des Allgemeinen Friedens unter prinzipiell als gleichrangig verstandenen Kleinstaaten eine dauerhafte Friedensordnung zu begründen.

Pazifizierung durch Großreiche (s. Hellenismus) war jedoch vorherrschend und an weitreichende staatliche Organisationsfähigkeiten geknüpft. So beruhte die "Pax Romana" der römischen Kaiserzeit auf ständiger militärischer Präsenz Roms, das nun im Gegensatz zur Römischen Republik vor Gaius Marius ein über das ganze Reich verteiltes stehendes Heer aus Berufssoldaten unterhielt, das in Kastellen garnisonierte und über Militärstraßen schnell verlegbar war. Wichtigste und gefährlichste Gegner Roms waren einerseits germanische Stämme, gegen die man sich nach dem vergeblichen Eroberungsversuch im Zuge der Augusteischen Germanenkriege mit dem Bau des Limes abgrenzte und die sich im Lauf der Jahrhunderte zu militärisch immer gefährlicheren Kooperationsverbänden oder Großstämmen zusammenschlossen (vgl. Markomannenkriege, Franken, Ethnogenese durch Heerkönigtum) und andererseits das Reich der Parther, das wiederum vom hochorganisierten persischen Sassanidenreich abgelöst wurde, welches sich in den römisch-persischen Kriegen Rom gegenüber als so ebenbürtig erwies, dass das seit je infanterieorientierte römische Heer Kataphrakten nach sassanidischem Vorbild übernehmen musste. Der Einfall des nomadischen Reitervolkes der Hunnen löste die für Westrom verheerende Völkerwanderung aus, während der östliche Reichsteil ihre Herausforderung überstand.

Nach dem Untergang des Römischen Reiches und dem Auftreten des initial militärisch höchst erfolgreichen Islams (vgl. Islamische Expansion), der das durch einen Krieg gegen die Oströmer massiv geschwächte Sassanidenreich vernichtet und das Territorium Ostroms massiv beschnitten hatte, entwickelten sich in der Völkerwanderung und im Frühmittelalter die Vorläufer der noch heute bekannten Nationen; die kriegerischen und wandernden Völker der ursprünglich heidnischen Angelsachsen, Wikinger und Magyaren setzten sich in ihren Landnahmen sprachlich oft durch, kulturell wurden sie jedoch vom Christentum absorbiert. Ihre Abwehr forderte militärische Innovationen wie die fränkischen Panzerreiter, die auch gesellschaftliche Folgewirkungen wie die stärkere Herausbildung eines ständisch abgrenzbaren Schwertadels hatten. Nach dem Verschwinden der römischen Berufssoldaten wurden im europäischen Mittelalter Heere nur dann aufgeboten und flexibel zusammengestellt, wenn ein Kriegszug geplant war. Begründet wurde die Verpflichtung zum Heeresdienst durch die feudalen Abhängigkeiten innerhalb einer vom höheren und niederen Erb- und Militäradel (→ Lehnswesen, Ritter) dominierten Gesellschaft. Innerhalb dieser Gesellschaft fanden zahlreiche kleinere Konflikte als Fehden statt, deren Ausmaß und Dauer jedoch bedeutend sein konnte (beispielsweise die Soester Fehde). Durch Gottesfrieden und Landfrieden versuchte man dergleichen kriegerische Auseinandersetzungen zumindest zu reduzieren, neben Adel und Kirche traten im Verlauf des Mittelalters zunehmend auch die Städte mit ihren Zünften, die ihre Territorien durch Landwehren abgrenzten und ihre Interessen durch milizartige Aufgebote und verfestigte Bündnispolitik (vgl. Hanse) vertraten. Religiöse, ethnische und machtpolitische Gründe vermischten sich im Rahmen der partiell gewaltsamen Christianisierung Europas und deren jahrhundertelangem Fortschreiten in die Peripherie, sowie den Kreuzzügen gegen Moslems und Heiden (siehe Wendenkreuzzug).

Im Hundertjährigen Krieg um die Vorherrschaft in Frankreich gerieten die französischen Könige durch verheerende militärische Niederlagen althergebrachter Ritterheere (Schlacht bei Crécy, Schlacht von Azincourt) gegen mit Langbögen versehene und diese konzentriert einsetzende englische Truppen massiv unter Druck, verstärkt wurde die Problematik durch interne Machtkämpfe des Hochadels wie dem Bürgerkrieg der Armagnacs und Bourguignons. Die Zeit war geprägt durch relativ neue Phänomene wie das Auftreten freier Söldnerkompanien, die – jenseits des sich aufweichenden Monopols des Adels auf Waffen, nur schwer kontrollierbar und ständig unterbezahlt – das Land verheerten und zu einer weiteren Gefahr für Machtposition und Legitimität des Königtums der Valois wurden. Bestehend aus entwurzelten und brutalisierten Kriegsexistenzen führte das Wüten entlassener und als Écorcheurs berüchtigter Söldner zum Aufbau der Ordonnanzkompanien, ständiger und verlässlicher Einheiten des Königs, die die Adelsaufgebote ergänzten. Zusammen mit dem Aufbau eines sich zentralisierenden Steuersystems, das aufgrund der Notwendigkeit ständige militärische Grundkosten aufzufangen dem König das Recht auf dauerhafte Besteuerung auch jenseits eines konkreten Anlasses einräumte, war ihre Aufstellung ein bedeutsamer Zwischenschritt hin zum Aufbau stehender Heere und zur Herausbildung des modernen Staates in Europa.
Parallel zu diesen innereuropäischen Konflikten erwies sich der Aufstieg der Osmanen als Beginn einer langanhaltenden und ernsthaften Bedrohung Europas, wie die verlorene Schlacht bei Nikopolis verdeutlichte. Mit den Janitscharen verfügten sie über ständig einsetzbare, hoch motivierte und gutausgebildete Truppen, die sich in zahlreichen Kämpfen im Zusammenspiel mit Topey und Sipahi als überaus gefährliche Gegner zeigten. 1453 fiel Konstantinopel (→ Eroberung von Konstantinopel), in Spanien und Portugal wurden fast zeitgleich islamische Mächte jedoch in der Reconquista herausgedrängt.

Durch den gemeinsamen eurasischen Kontinent fand die Kriegsgeschichte Asiens immer in einer gewissen Verbindung mit dem Orient und Europa statt, sowohl was Kriege wie militärische Entwicklungen betrifft.

So hatte das persische Reich der Achämeniden sowohl Auswirkungen auf die griechische Staatenwelt (Perserkriege), als auch auf den Norden des alten, nach dem Untergang der Indus-Kultur vedisch geprägten, Indiens, den wohl bereits Kyros II. bis zum Indus eroberte. Gandhara (im heutigen Afghanistan) wurde eine Satrapie des Reiches.

Alexander der Große eroberte erst Persien und rückte auf den Spuren des Kyros – diesen übertreffend – über den Punjab und den Sindh gegen indische Könige kämpfend den Indus entlang bis zu seiner Mündung vor. Nach Alexander bildeten sich in der Gandhara-Kultur eine griechisch geprägte Kulturlandschaft mit buddhistischer Religion, die über die Diadochen-Reiche eine Zeitlang noch mit der hellenistischen Kultur verbunden war und sogar ein zeitweiliges Indo-Griechisches Königreich aufwies, ehe innerindische Entwicklungen und Wanderungsbewegungen der Saken es verschwinden ließen. Das seleukidische Persien wurde vom Reich der Parther und Sassaniden abgelöst. Indien selbst sah bis ins Zeitalter des europäischen Kolonialismus ein kriegerisches Auf und Ab diverser Reiche, ab dem siebten nachchristlichen Jahrhundert mitgeprägt durch das Vordringen des Islams, von denen aber allein das hinduistisch-buddhistische Maurya-Reich und das islamische Mogulreich – nur zeitweilig – fast den gesamten Subkontinent umfassen konnten, das Gupta-Reich beherrschte zumindest den gesamten Norden und Teile des Südens. Der Maurya-König Ashoka vergrößerte sein Reich kriegerisch, ungewöhnlicherweise äußerte er später auf Säulen Bedauern und Schmerz über die Opfer des Krieges.

Über die lang andauernde Verbindung zu Indien konnten Kriegselefanten (sowohl indische wie afrikanische Waldelefanten) von griechischen Königen und karthagischen Feldherrn auch in Europa verwendet werden, wo sie etwa in der Schlacht von Heraclea oder nach Hannibals Alpenüberquerung gegen die Römer eingesetzt wurden, die sie aber – anders als Seleukiden und Ptolemäer – selber nicht auf Dauer übernahmen.

In China bildeten sich über diverse Stufen wie die Longshan-Kultur und die Erlitou-Kultur frühe Reiche wie die (nicht nachgewiesene) Xia-Dynastie und die (sicher nachgewiesene) Shang-Dynastie heraus, die spätestens ab den Zhou-Dynastien kulturelle Grundlagen späterer chinesischer Staatswerdung schufen. Die Bezeichnung Zeit der Streitenden Reiche steht dabei für eine kriegerische Konzentration der diversen Kleinreiche und Fürstentümer zu Staaten mit hochentwickelten Kriegswaffen aus Eisen und schlagkräftigen Heeren, denen weder Streitwagen, der Einsatz von Kavallerie, noch Distanzwaffen wie die Armbrust unbekannt waren, eine charakteristische Waffe jener Zeit ist die Ge. Innerhalb der konkurrierenden sieben Staaten, die letztendlich übrigblieben, setzte sich der Staat Qin in mehreren Eroberungsfeldzügen durch. Als totalitär anmutender Militärstaat von der Denkschule der Legalisten geprägt, schaffte es dieser Staat unter Qin Shihuangdi das erste – noch kurzlebige – Kaiserreich (→ Qin-Dynastie) zu gründen, dessen Beispiel eines zentralisierten und bürokratischen Gesamtstaates mit von oben nach unten vereinheitlichter Kultur für die chinesische Geschichte konstitutiv wurde, obgleich die dominante Denkschule der Konfuzianer Qin wegen seiner amoralischen Staatsdoktrin verdammte. Militärisch war Qin hochgerüstet, Aufschluss über Struktur, Aussehen und Bewaffnung der Elitetruppen seines Heeres gibt deren detailgetreues Abbild, die Terrakottaarmee. Durch eine von der Verwaltung unbarmherzig, aber höchst effektiv durchgeführte Form der Wehrpflicht war es gelungen, eine massive Überlegenheit speziell an Fußsoldaten aufzubieten, der Gegner wenig entgegenzusetzen hatten, daneben dienten im Heer zahlreiche professionelle Einheiten. Nach Wiederbelebung des in Aufständen zusammengebrochenen Reiches durch die Han-Dynastie dehnte das chinesische Kaiserreich allmählich und über viele Jahrhunderte die Kultur der Han-Chinesen auf das Gesamtgebiet des heutigen Chinas aus. Diese Entwicklung war verbunden mit Staatsbildungen auf den Gebieten Japans, Koreas und Indochinas, die kulturell und in direkten Auseinandersetzungen von China beeinflusst wurden. Ständiges Problem der chinesischen Kaiser seit Anbeginn war das Verhältnis zu nördlichen Nomaden Zentralasiens. Die Chinesische Mauer steht für ein jahrtausendealtes System hochorganisierter Abwehr, das nicht immer erfolgreich war, und zwischen defensiver Verteidigung (z. B. in der Sui-Dynastie) und präventiven Feldzügen zur ausdehnenden Vorfeldverteidigung (wie in der Tang-Dynastie) schwankte.

Dieses System hatte möglicherweise Auswirkungen bis nach Europa, indem es – nicht unumstritten – Wanderungsbewegungen und Kriegszüge nomadischer Völker Richtung Europa lenkte (s. Debatte um die Xiongnu und Hunnen). Eine gesicherte Verbindung zwischen den Kriegen Europas, dem Orient und Ostasiens wurde aber durch die Reichsgründung Dschingis Khans geschaffen, dessen Mongolensturm die Eroberung Chinas und die Zerschlagung und Schwächung islamischer Reiche einschloss, und über Russland bis ins ferne Deutschland reichte, wo ein deutsch-polnisches Ritterheer 1241 n. Chr. in der Schlacht von Liegnitz vernichtet wurde, ehe die Mongolen aus nicht sicher herleitbaren Gründen, aber zum mutmaßlich großen Glück Mittel- und Westeuropas, umkehrten. Russland jedoch blieb für etwa drei Jahrhunderte im mongolischen Machtbereich. Für China hatten die Verheerungen des Mongolensturms möglicherweise die weitreichende Folge, dass technische Entwicklungslinien unterbrochen wurden, so dass es mit der Zeit hinter die späteren westlichen Staaten zurückfiel, mit entsprechenden Auswirkungen für die Machtverteilung in der Welt bis heute.
Feuerwaffen waren tatsächlich erstmals während der Song-Dynastie verwendet und fortentwickelt worden, ob sie dann direkt mit den Mongolen nach Europa kamen oder den Europäern vielmehr über den Kontakt mit den muslimischen Arabern vermittelt wurden ist – ebenso wie die Art und Weise der Vermittlung – unklar. Das Wu Jing Zong Yao nennt zahlreiche Waffen, darunter mit dem sog. Pen Huo Qi bereits einen Flammenwerfer, dazu Sprenggranaten, feurige Pfeile etc.

Im arabischen Raum wurde der mongolische Siegeszug – der die islamischen Reiche elementar bedrohte – durch Abwehrerfolge der Mamelucken beendet, die auch die Reste der Kreuzfahrerstaaten vernichteten, spätere Erfolge der Mongolen unter Timur bedrohten den Islam nicht mehr an der Wurzel, weil die Mongolen sich inzwischen zu ihm bekannten; das Osmanische Reich wurde durch die Eroberung arabischer Länder und Kriege gegen den Iran zum dominierenden Faktor des nahen Ostens.

Im Gefolge der Reformation zerfiel die relativ stabile Einheit des Mittelalters, das Heilige Römische Reich unter Führung von Kaiser und Papst. Die Verbindung von konfessionellen und machtpolitischen Gegensätzen führte zu diversen Konflikten und Kriegen, wie den Hugenottenkriegen oder dem Achtzigjährigen Krieg, in dem die militärisch den spanischen tercios unterlegenen Niederlande durch die Oranische Heeresreform zukunftsweisende Änderungen in der Kriegführung vornahmen. Begleitet wurden diese innereuropäischen Konflikte durch die sich intensivierende Europäische Expansion nach der Entdeckung Amerikas und der Seewege nach Asien, die durch deutliche Weiterentwicklungen im Schiffbau und der Navigation ermöglicht wurden. Kulturell fast nahtlos von der Reconquista in die Conquista übergehend, ermöglichte die Eroberung Mexikos und Perus durch ihre Profite den Aufstieg Spaniens zur zeitweiligen europäischen Vormacht, – und langfristig setzten zahlreiche Auswanderungs- und Siedlungsbewegungen ein, die erst zu europäischen Kolonien, dann aber auch zu neuen Staaten wie den späteren USA führen sollten.

Eine weitreichende Folge des Vordringens der Europäer auf den amerikanischen Kontinent war das durch eingeschleppte Krankheiten verursachte Massensterben der einheimischen Bevölkerung, deren ethnische Zusammensetzung so elementar und auf Dauer verändert wurde. Dies geschah in der Regel unbeabsichtigt. Es gibt jedoch einige verbürgte Fälle, bei denen Seuchen absichtlich verbreitet wurden, etwa durch die Verteilung pockeninfizierter Decken. Insgesamt wurde der Doppelkontinent nicht primär über die Indianerkriege, sondern über Krankheitskeime erobert. da die indianischen Gesellschaften zu konzentrierter Abwehr nur eingeschränkt noch in der Lage waren. Insgesamt erlagen mindestens drei Viertel aller indigenen Bewohner Amerikas den von den Europäern mitgebrachten Krankheiten.

Im Dreißigjährigen Krieg von 1618 bis 1648 mischten sich konfessionelle, ständische und gliedstaatliche Spannungen innerhalb des Heiligen Römischen Reiches mit den machtpolitischen Interessen der Nachbarländer, befeuert und genährt wurde der Krieg durch den habsburgisch-französischen Gegensatz. Geführt wurde der Krieg vornehmlich durch Söldnerheere, die ihren verheerenden Ruf bestätigten. Angeführt und aufgestellt von exzentrischen (→ Christian von Braunschweig-Wolfenbüttel) bis nüchtern-pragmatischen (→ Albrecht Wallenstein) adligen Kriegsunternehmern im oft wechselnden Dienst von Fürsten zeigte sich sowohl ihre Unabhängigkeit wie ihre stets prekäre Finanzierung als mit schrecklichen Nebenwirkungen für die neuzeitliche Gesellschaft verbunden. Getragen von verhältnismäßig eigenständigen Regimentern – denen im Tross Marketender, Soldatenfrauen und -familien, sowie Huren folgten – gingen im Krieg eher seltene Feldschlachten mit kontinuierlichen Raubzügen, Plünderungen und Massakern an der Zivilbevölkerung einher; im Konfliktverlauf starb etwa ein Drittel der mitteleuropäischen Bevölkerung, sei es durch unmittelbare Kriegswirkungen, sei es durch Kriegsfolgen wie Missernten, Hunger und eingeschleppte Seuchen.

Diese Geschehnisse bewirkten tendenziell einen Gesinnungswandel. Der Westfälische Frieden 1648 brachte zum ersten Mal das Prinzip der Nichteinmischung in die Angelegenheiten fremder Staaten in die Diskussion. Der Krieg entwertete den Anspruch, religiöse Standpunkte mit Waffengewalt durchzusetzen und ließ eine straffere Kontrolle der Kämpfenden geraten erscheinen. Der Westfälische Friede leitete in Europa die Trennung von Politik und Religion ein, im nun folgenden Zeitalter des Absolutismus wurden zentralisierte Staaten mit stehenden Heeren üblich. Schweden war bereits unter Gustav Adolf durch Reformierung seines Militärwesens von einem kleineren Staat zur Großmacht im Ostseeraum aufgestiegen, seine Machtpolitik bewegte das Kurfürstentum Brandenburg zur Neuformierung seiner Armee, womit auch sein Aufstieg begann, Peter der Große wiederum kopierte europäische Militärreformen und machte Russland dadurch zur Großmacht. Einteilung in feste und mit klaren Pflichtbereichen versehene Dienstgrade, rigide Disziplinierung und Ausbildung durch Drill und Exerzieren steigerte sowohl die Effektivität des Militärs, wie dessen Kontrolle durch die Staaten, – der Söldner wurde durch den einer ständigen und harten Militärgerichtsbarkeit unterworfenen Soldaten abgelöst. Elemente des Söldnerwesens wurden jedoch noch längere Zeit beibehalten (Kauf von Offiziersstellen, Kompaniewirtschaft). Mit der Schlacht am Kahlenberg wurde die türkische Expansion 1683 gebrochen und eine Wende der Türkenkriege erreicht, militärisch waren die Europäer von nun an überlegen.

Die (nur vergleichsweise) friedliche Periode der sogenannten Kabinettskriege begünstigte die Aufklärung. Aus der Idee der allgemeinen Menschenrechte entwickelte sich die Idee des gehegten Krieges im zivilen Rahmen. Hatte seit Augustinus von Hippo die kirchliche Lehre vom gerechten Krieg die Kriterien zur Legitimation geliefert, so übernahmen dies nun aufgeklärte Juristen wie Hugo Grotius. Jedoch zeigte der Krieg parallel dazu Züge seiner Entgrenzung, am Siebenjährigem Krieg waren alle europäischen Großmächte beteiligt, ihre Kampfhandlungen waren nun nicht mehr auf Europa beschränkt, sondern fanden auch an den Schauplätzen kolonialer Expansion in Indien und Nordamerika (und auch in Afrika) statt, – in gewisser Weise trug bereits dieser Krieg Züge eines Weltkrieges. In der sogenannten Levée en masse mobilisierte dann die um ihr Überleben kämpfende französische Revolution Massenheere, die aus ideologischer Begeisterung kämpften und die außerordentliche Machtstellung Frankreichs in den Koalitionskriegen und den ihnen folgenden Napoleonischen Kriegen begründen halfen, taktisch ermöglichten sie den Wechsel von der Lineartaktik zur Kolonnentaktik. Staaten wie Preußen reagierten mit einer eigenen Form der Wehrpflicht, dem sogenannten Krümpersystem als Teil breiter angelegter Heeresreformen, durch die enorme Vergrößerung der Heere stiegen auch die Opferzahlen spürbar an. In Spanien scheiterte Frankreich an einer britischen Intervention, – aber vor allem an einem Volksaufstand, der in einen beidseitig grausam geführten Guerillakrieg überging, in dem die Trennung von Kombattanten und Zivilisten verschwamm. Anders als in den Kabinettskriegen wurden die Völker materiell und ideologisch in den Krieg einbezogen, der preußische König wandte sich explizit an seine Untertanen, der Dichter Ernst Moritz Arndt verfasste 1813 mit seiner Schrift "Über Volkshass und über den Gebrauch einer fremden Sprache" ein chauvinistisches Pamphlet, das zum offenen Hass nicht allein gegen Napoleon, sondern gegen Frankreich insgesamt aufrief, – der Nationalismus war seit der französischen Revolution Teil europäischer Identitäten. Nach den Verheerungen dieser Kriege wurde Frieden als Ziel der Politik wieder denkbar und in Europa streckenweise auch erreicht: etwa in der verhältnismäßig stabilen Epoche nach dem Wiener Kongress 1815. Außerhalb Europas führten europäische Staaten weiterhin Kolonialkriege, in denen ihre immer weiter zunehmende technische Überlegenheit zum Tragen kam. Institutionell fand eine weitergehende Verwissenschaftlichung des Krieges statt, Preußen gründete 1808 seinen Generalstab, die Ausbildung von Führungspersonal in Militärschulen aller Art wurde ausgebaut.

In der stabilitätsorientierten Restaurationszeit nahm allerdings auch der Wunsch der Bevölkerungen nach Demokratie und Selbstregierung zu, mehrere gescheiterte Revolutionsversuche belegen dies. Problematisch daran war, dass geforderte Veränderungen nur auf Kosten der etablierten Staaten und damit der Stabilität in Europa möglich waren. Mit der Emanzipation der Gesellschaften von den Vorstellungen der monarchischen Regierungen nahm auch der Nationalismus wieder zu, erst noch verbunden mit dem Ideal der Demokratie, später neben und unabhängig von ihm. Die italienischen Vereinigungskriege gaben ein geglücktes Beispiel einer erfolgreichen Nationalbewegung ab, diesmal noch ohne das Gleichgewicht in Europa zu zerstören.

Mit dem Krimkrieg und dem Amerikanischen Bürgerkrieg außerhalb des europäischen Kerngebiets bekamen die Kriege ein bereits modern anmutendes Gesicht: Stellungskrieg und nun auch industriell ausgerüstete Massenheere sorgten für einen Anstieg der Opferzahlen und – im Civil War – eine stärkere Beteiligung eigentlich ziviler Familien, die über freiwillige Meldung und Wehrpflicht den Krieg in der Mitte der Gesellschaft erlebten. Innovationen wie das Minié-Geschoss erhöhten durch vergrößerte Reichweite und Zielgenauigkeit die Gefahr auf dem Schlachtfeld, so dass Soldaten nach ihren Erfahrungen mit klassischen Schützenlinien gegen Ende des Amerikanischen Bürgerkrieges nach Möglichkeit aus der Deckung heraus kämpften. Im Amerikanischen Bürgerkrieg zeigten sich auch die Mängel in der Verwundetenversorgung schmerzlich, noch im Krieg wurden dadurch durch den Militärarzt Johnathan Letterman mobile Feldlazarette nach festgelegten Standards errichtet und mit dem U.S. Ambulance Corps ein Vorläufer heutiger Sanitätsdienste gegründet. Die United States Sanitary Commission betrieb im Hinterland gleichfalls Hospitäler, dort taten auch Frauen als Krankenschwestern und in der Verwaltung Dienst; relativ eigenständig organisiert und teilweise an hervorgehobener Position (s. Dorothea Lynde Dix). Mary Edwards Walker war die erste offizielle Militärärztin der U.S Geschichte.

In Europa endeten die deutschen Einigungskriege mit der Errichtung des zweiten deutschen Kaiserreiches, in ihnen zeigten Innovationen wie das rauchschwache Pulver, das preussische Zündnadel- und das französische Chassepotgewehr ihre Effektivität, neuentwickelte Hinterlader-Artillerie wie das C/64/67 bewies verheerende Wirksamkeit und mit der Mitrailleuse wurde eine Maschinenwaffe verwendet. Gutausgebildete und zahlenmäßig weit überlegene deutsche Wehrpflichtarmeen unter Führung der preußischen Armee setzten sich gegenüber der gleichfalls gutausgebildeten französischen Berufsarmee deutlich durch.

Die moderne Form des Krieges setzte Nationalstaaten voraus, die über ein Steueraufkommen und Verteidigungsetat verfügen und damit eine stehende Armee aufstellen können. Die Entwicklung führte zu immer größeren Armeen mit immer stärkeren Waffen und tendenziell entsprechend höheren Opferzahlen (die jedoch wie erwähnt relativ gesehen deutlich niedriger waren und sind als in prähistorischen und vormodernen Stammeskriegen.)

Im 19. Jahrhundert finden sich auch erste Ansätze zur Begrenzung und Regulierung von bewaffneten Konflikten, die sich als modernes Völkerrecht etablierten. Daraus abgeleitet wurde auch das kodifizierte Kriegsrecht und das Kriegsvölkerrecht. Seine bedeutendsten Errungenschaften vor 1914 waren:

Die Kriegsgründe blieben bei dieser Kodifizierung des Kriegsverlaufs ausgeklammert, und die Wahl der Mittel wurde ebenfalls noch nicht verbindlich geregelt.

Im Ersten Weltkrieg führte der Einsatz von Maschinengewehren, Panzern, Flugzeugen, U-Booten, Schlachtschiffen, Giftgas sowie die totale Kriegswirtschaft zu einem neuen Gesicht des Krieges. Feld- und Seeschlachten forderten Millionen Todesopfer und Abermillionen von Schwerverletzten.

Die bisherige europäische Bündnis-, Gleichgewichts- und Vertragspolitik mit ihrer Doppelstrategie von Hochrüstung und Diplomatie war nicht zuletzt am Konkurrenzkampf um Kolonien gescheitert. Darum wurde vor allem auf Initiative des US-Präsidenten Woodrow Wilson nach 1918 versucht, eine internationale Konfliktregelung zu institutionalisieren. Die Gründung des Völkerbunds stellte den Frieden als gemeinsames Ziel der Staaten heraus und gab dem Völkerrecht eine organisatorische Basis.

Der Briand-Kellogg-Pakt zur Ächtung des Angriffskrieges war ein weiterer Schritt, um nicht nur den Kriegsverlauf, sondern die Staatssouveränität bei der Entscheidung zum Krieg zu begrenzen und den Verteidigungskrieg international akzeptierten Kriterien zu unterwerfen.

Angesichts der neuen Kriegsqualität, die die Massenvernichtungsmittel bedeuteten, wurde ferner versucht, bestimmte als unnötig grausam verstandene Waffen zu ächten und zu verbieten. Dies gelang bis 1939 jedoch noch nicht, obwohl die prinzipielle juristische Handhabe dafür mit der Haager Landkriegsordnung gegeben war.

Der Aufstieg des Nationalsozialismus beendete diese Bemühungen. Systematisch ignorierte Adolf Hitler von 1933 bis 1939 die völkerrechtlichen Obligationen Deutschlands und bereitete seinen Eroberungs- und Vernichtungskrieg vor. Die Appeasement-Politik Großbritanniens scheiterte 1938 trotz der durch Großbritannien, Frankreich, Italien und Deutschland der Tschechoslowakei aufgezwungenen Abtretung des Sudetenlandes und der deutschen Besetzung von Böhmen und Mähren 1939. Der Weg in den Zweiten Weltkrieg war damit frei.

Dieser begann wie der erste als konventioneller Krieg, wurde aber rasch und unaufhaltsam zum "totalen Krieg". Staatlich gelenkte Kriegswirtschaft, Kriegsrecht, allgemeine Wehrpflicht und Propagandaschlachten an der Heimatfront bezogen die Völker ganz und gar in die Kampfhandlungen ein. Die Mobilisierung aller nationalen Reserven für Kriegszwecke hob die Unterscheidung zwischen beteiligten Zivilisten und Kombattanten auf. Die Kriegsführung ignorierte, insbesondere in Osteuropa, in hohem Maße das Kriegs-Völkerrecht.
So kam es im Kriegsverlauf
Die Nürnberger Prozesse schufen den neuen Straftatbestand des „Verbrechens gegen die Menschlichkeit“: dies war der erste Versuch, Menschen nach dem Völkerrecht aufgrund von Kriegsverbrechen zu verurteilen.

Die ungeheure Steigerung der Vernichtungskapazitäten und Verselbstständigung der Kriegsführung verstärkte nach 1945 die Bemühungen, Kriege generell zu vermeiden. In Europa, besonders in Deutschland herrschte bei weiten Teilen der Zivilbevölkerung die Einstellung vor: "Nie wieder Krieg!"

Erneut wirkten nun vor allem die USA auf die Einrichtung einer Weltorganisation zur diplomatischen Konfliktlösung und Kriegsverhütung hin: die Vereinten Nationen (UNO). Die Erfahrung der Ohnmacht des Völkerrechts in den Weltkriegen fand ihren Niederschlag in ihrer Charta, hier vor allem in Kapitel II, Absatz 4:

Dies verbot erstmals allgemeinverbindlich jeden Angriffskrieg und jede militärische Erpressung. Die Charta bekräftigt das Prinzip der Nichteinmischung und das natürliche Recht zur Selbstverteidigung im Fall eines feindlichen Angriffs. Sie verpflichtet alle Mitglieder zu gemeinsamen friedenserhaltenden oder wiederherstellenden Maßnahmen und machte diese von einem Mandat des UN-Sicherheitsrats abhängig. Dabei stand auch die Sorge vor einem neuen weltumspannenden Konflikt Pate, die durch den Zerfall der Anti-Hitler-Koalition bereits auf der Konferenz von Potsdam im Juli 1945 am Horizont auftauchte.

Auch die Bemühungen zur Ächtung bestimmter Waffengattungen wurden seit 1945 verstärkt. Doch während das Verbot von B- und C-Waffen weithin akzeptiert wurde, misslang das universale Verbot der Atomwaffen. Bis 1949 besaßen die USA das Atommonopol; bis 1954 erreichte die Sowjetunion ein strategisches „Atompatt“, das vor allem auf der Bereithaltung von Wasserstoffbomben und Fernlenkwaffen beruhte. Beide weltpolitischen Kontrahenten waren von nun an zum atomaren Zweitschlag mit unkalkulierbaren Folgen im Feindesland fähig.

Seit dem Beinahe-Zusammenstoß der Supermächte in der Kubakrise von 1962 wurden ergänzend aber erste Schritte zur gemeinsamen Rüstungskontrolle gemacht. Die KSZE wurde 1973 eingerichtet und erlaubte den Europäern gewisse eigenständige Abrüstungsinitiativen mit der Sowjetunion. Hinzu kam die seit 1979 wachsende Friedensbewegung, die den innenpolitischen Druck zu Abrüstungsvereinbarungen vor allem in Westeuropa und den USA verstärkte. Mit Gorbatschows Angeboten gelang 1986 in Reykjavík ein Durchbruch zum vollständigen Rückzug aller Mittelstreckenraketen aus Europa, der eine Reihe Folgeverträge nach sich zog.

Unterhalb der Atomkriegsschwelle fanden jedoch zwischen 1945 und 1990 laufend so genannte konventionelle Kriege vor allem in Ländern der so genannten Dritten Welt statt. Eine Reihe davon waren Stellvertreterkriege, z. B. der Koreakrieg (1950 bis 1953), der Vietnamkrieg (1964–1975) sowie zahlreiche Konflikte in Afrika und Lateinamerika. Dort verhinderte der Kalte Krieg und das gegenseitige Abstecken von Einflusszonen der Supermächte häufig regionale Konfliktlösungen und begünstigte verlängerte Bürgerkriege mit vom Ausland finanzierten Guerillakämpfern.

Die Auflösung der Sowjetunion und Jugoslawiens führte Anfang der 1990er Jahre zu neuen Kriegen. Seit 1992 hat sich die Zahl der pro Jahr laufenden Kriege jedoch nahezu halbiert. Jedoch wird Krieg seit dem Ersten Golfkrieg der USA und dem Falklandkrieg Großbritanniens nun auch in Europa wieder als Mittel zum Erreichen legitimer Ziele wie der Durchsetzung von Menschenrechten oder der Prävention gegen tatsächliche oder vermutete Rüstungs-, Terror- und Angriffspläne angesehen.

Als Reaktion auf die Terroranschläge vom 11. September 2001 rief US-Präsident George W. Bush den Krieg gegen den Terror aus. Auch Deutschland unterstützt diesen Sonderfall in Teilbereichen mit dem Einsatz der Bundeswehr in Afghanistan und anderen Auslandseinsätzen.

Auffallend an den Konflikten des 21. Jahrhunderts ist, dass sie nur noch in seltenen Fällen zwischen Staaten stattfinden. Der typische Krieg ist ein interner Konflikt, so gab es 2013 keinen Krieg über Staatsgrenzen hinweg. Verschiedene Institute grenzen interne Konflikte unterschiedlich ab, so dass sie zu abweichenden Einschätzungen über die Häufigkeit und kurzfristiger Schwankungen kommen.
Nicht beendete Kriege, sogenannte „eingefrorene Konflikte“ sind vor allem im ehemaligen Machtbereich der Sowjetunion zu finden.

Im Zeitalter von Digitalisierung und Internet entstehen neue Formen der Kriegsführung. Bezeichnungen wie Cyberwar, Infowar, Netwar oder Lawfare beziehen sich auf entstehende Kriege ohne Schlachtfeld und Heere. Unbemannte Drohnen werden für militärische Zwecke gerüstet und führen mit dem „gezielten Töten“ eine neue Form des Krieges ein, deren ethische Probleme zu öffentlichen Kontroversen führen.

Das Grundgesetz der Bundesrepublik Deutschland bestimmt im Artikel 26 (1):

Seit der Neuzeit wird Krieg eng mit der Politik souveräner Nationalstaaten verknüpft, die innenpolitisch über ein Gewaltmonopol verfügen. Der preußische Militärtheoretiker Clausewitz sah Krieg als "„Akt der Gewalt, um den Gegner zur Erfüllung unseres Willens zu zwingen“". Weil diese Gewalt von einem souveränen Staatswesen ausgeht, definierte er sie als "„Fortsetzung der Politik mit anderen Mitteln“":

Eine politische Orientierung, die Krieg für natürlich, unvermeidbar, sogar fortschrittsfördernd hält und Rüstungsanstrengungen prinzipiell bejaht, nennt man Militarismus.

Die entgegengesetzte Haltung will Kriege nicht nur vermeiden, sondern langfristig als Mittel der Konfliktaustragung ausschließen, abschaffen und überflüssig machen: Der "Pazifismus" (von Lateinisch "pacem facere": „Frieden schaffen“).
Für ihn ist Krieg "„eine Geißel der Menschheit“" (UN-Charta).

Zwischen diesen Polen bewegt sich die so genannte „Realpolitik“ des Großteils aller Staaten, die militärische Gewalt als "ultima ratio" – „letztes Mittel“ – nie ganz ausschließt und von Fall zu Fall als unvermeidlich anwendet. Dabei ist in heutigen Gesellschaften vor, in und nach einem Krieg meist heftig umstritten, ob und wann dieses Mittel tatsächlich das letzte, der Krieg also wirklich unvermeidbar war und ist.

Hierzu werden mitunter kriegsauslösende Einzeltaten inszeniert (Erster Weltkrieg, Zweiter Weltkrieg) oder wirtschaftliche Konflikte provoziert (beispielsweise mittels Zöllen, Patentrechten, Einfuhrbeschränkungen).

Da sowohl Attentate als auch Terrorakte die moralische Rechtfertigung für einen Krieg bilden können, kommt der Inszenierung eines Krieges oft höhere Bedeutung zu als der späteren Durchführung. Dies ergibt sich aus der Tatsache, dass Kriegsführung neben logistischen und humanitären Gesichtspunkten vor allem wirtschaftliche Zwänge birgt.

In ärmeren Ländern dienen Kriege oft innenpolitischem Kalkül. Dabei rechnet die Regierung eines solchen Landes damit, dass das Volk durch das durch den Krieg erzeugte Härteklima mit unmittelbaren Lebensfunktionen wie Nahrung, Kleidung, Wohnung so beschäftigt ist, dass es keine Zeit mehr hat, sich Themen wie Regierung, Politik oder Wirtschaft zu widmen. Eine Regierung kann so versuchen, Kritik zu unterdrücken.

Wohlstandsnationen führen Kriege meist abseits der eigenen Heimat. Eine drastische Einengung der Lebensgrundlage ist in diesen, eher höher gebildeten Bevölkerungen meist nicht vermittelbar und würde nicht breit akzeptiert. Dennoch wird in der Heimat eine „psychologische Militarisierung“ auf das gesamte Volk übertragen, welche auf Patriotismus und Duldung der Beschneidung von Grundrechten, beispielsweise im Wege der Terrorismusbekämpfung, abzielen.

In beiden Fällen handelt es sich um eine Art Flucht nach vorn, im Zusammenhang mit bereits unabhängig vom Krieg bestehenden Strukturproblemen im eigenen Land beziehungsweise drohendem Machtverlust der Regierung. Der Krieg kann als Rechtfertigung für unterschiedliche Einschränkungen (zum Beispiel der Menschenrechte oder der Sozialversorgung) verwendet werden.

Da eine Bevölkerung sich zumeist in relativer Akzeptanz mit ihrer Regierung befindet (gestützt durch staatlich gelenkte Medien oder durch echte Akzeptanz von aggressiven Expansionsabsichten beziehungsweise durch stillschweigendes Erdulden der Staatsführung), stellt die Wechselwirkung zwischen der Volksmeinung einerseits und der Legitimation einer Regierung Krieg zu führen andererseits, ein besonders wichtiges Instrument der Militarisierung im Vorfeld der Kriegsführung dar.

Zu diesen kleinen Kriegen zählen Krawalle, Aufstände, der Staatsstreich, Bürgerkriege usw. Sie bilden die überwältigende Mehrzahl aller Kriege; die „regulären“ Kriege zwischen Staaten und regulären Truppen demgegenüber die Ausnahme.
Einige Autoren (Agamben, Hardt und Negri) hinterfragen diese Ansicht mittlerweile, so werde Ausnahmezustand zum Normalzustand erklärt:
Die Politik sehe Krieg nicht mehr als letztes Mittel, sondern als Werkzeug zur Kontrolle und Disziplinierung.

Wegen der extremen Belastung, die der Krieg den beteiligten Parteien auferlegt, ist eine positiv gestimmte eigene Öffentlichkeit für eine kriegführende Institution oder Nation von kriegsentscheidender Bedeutung.

Die militärische Strategie ist der Plan, um den Zweck des Krieges zu erreichen. Zweck des Krieges ist nach Clausewitz immer der Friede, in dem die eigenen Interessen dauerhaft gesichert sind.

Militärische Strategien ändern sich mit der Waffenentwicklung. In der Geschichte wurden häufig dominante Mächte zurückgeworfen, weil neuere, wirksamere Waffen entwickelt wurden. Aber auch ohne Neuentwicklung von Waffen können bessere strategische Planungen einen Krieg entscheiden, u. U. auch aus der Unterlegenheit heraus.

In der Militärstrategie geht es immer darum, durch geschickte räumliche und zeitliche Anordnung der Gefechtssituationen den Erfolg herbeizuführen. Als Krönung gilt es allgemein, wenn man ohne einen Kampf den Sieg davonträgt. Kriegslisten sind daher ein wesentliches Element des Krieges. Die wohl berühmteste Kriegslist der Geschichte ist die des trojanischen Pferdes.

Militärstrategie lässt sich nach Edward Luttwak in zwei Dimensionen aufspannen. Einer Horizontalen und einer Vertikalen. Die Horizontale Ebene entspricht der temporären Abfolge jeder strategischen Operation inklusive Clausewitzs Kulminationspunkt.
Die Vertikale Dimension gliedert sich in mehrere Ebenen. Die unterste ist die technische Ebene, diese umfasst die Effektivität, als auch die Kosten von Waffensystemen, und damit auch den Ausbildungsstand und Leistungsfähigkeit der einzelnen Soldaten.

Als Nächstes folgt die taktische Ebene. Sie umfasst die untere Militärische Führung also alles bis Bataillons- oder Brigadeebene, sowie die Moral der Truppe und beinhaltet vor allem die Geländeausnutzung.

Als Nächstes folgt die operative Ebene. In dieser findet sich die militärische Strategie von Divisionsebene und aufwärts. Hier werden größere militärische Manöver unter anderen Gesichtspunkten als in der taktischen Ebene geplant und ausgeführt. Hier entscheiden weniger das Gelände als beispielsweise die zur Verfügung stehenden Ressourcen inklusive die Einbeziehung wirtschaftlicher Kapazität.

Als oberste Ebene gilt die Gefechtsfeldstrategie. In ihr entscheiden einzig und alleine die politischen Ziele und Eigenheiten der kriegführenden Parteien. Auf einem Kriegsschauplatz wird die Strategie im Rahmen von Feldzügen durch Operationen umgesetzt. Für Operationen werden Weisungen und Operationspläne erstellt, die die übergeordneten strategischen Ziele in praktische, militärische Aufträge und Handeln umsetzen.

Zu den berühmtesten strategischen Denkern gehören Sun Zi ("Die Kunst des Krieges") und Carl von Clausewitz ("Vom Kriege").

Die ethische Bewertung des Krieges als gewalttätige zwischenmenschliche Handlung unterliegt im Wesentlichen drei zeitlichen Kriterien. Seit dem Mittelalter ist das Recht zum Krieg und seit der frühen Neuzeit das Recht im Krieg als Betrachtungsdomäne etabliert, während seit dem Ende des Kalten Krieges die Verantwortlichkeit einer Besatzungsmacht oder eines konfliktlösenden politischen Akteurs als Nachkriegsrecht verstanden wird.

Der Politikwissenschaftler A.J. Coates identifiziert den Realismus, den Militarismus, den Pazifismus und die Theorie vom gerechten Krieg als die vier wesentlichen ethischen Grundhaltungen zum Krieg.

Diese haben weitestgehend den Charakter von Ideologien. Das erweist sich z. B. an der literaturkritischen Einordnung des Frühwerks „In Stahlgewittern“ des Schriftstellers Ernst Jünger, das aus der Position des „Realismus“ dem persönlichen Erleben und den Tagebuchaufzeichnungen des Autors während des Ersten Weltkriegs erwuchs und die ethische Bewertung dem Leser überlässt. Darstellungsmäßig eine realitätsgerechte, aber weitestgehend emotionslose Beschreibung der Abläufe und Zustände in den Schützengräben der Materialschlachten im französischen Kampfgebiet, fehlen den Vertretern der anderen Richtungen gerade die ethische Stellungnahme und Bewertung der drastisch-nüchtern wiedergegebenen Vorgänge. Der Biograf Steffen Martus versteht die Enthaltung des Autors von einer politischen und moralischen Parteinahme aber als Möglichkeit für jede Seite, die Kriegsdarstellung sowohl als „affirmativ“ als auch als „neutral“ oder als „Antikriegsbuch“ zu lesen.

Jeder Krieg ist, neben dem Verlust von Infrastruktur oder Arbeitsplätzen, immer auch mit Tod und menschlichem Leid verbunden. Diese entstehen einerseits als gewollte oder hingenommene Folgen des Waffeneinsatzes gegen Menschen, andererseits aus strategischen Gründen (zum Beispiel beim Sprengen von Brücken oder durch Vergiftung von Grundnahrungsmitteln); zum Teil wird die Zerstörung von Gebäuden bzw. der allgemeinen Infrastruktur des Kriegsgegners aber auch bewusst herbeigeführt, um die Zerstörungskraft einer Armee zu demonstrieren und den Gegner einzuschüchtern (z. B. „Shock and awe“-Strategie im Irakkrieg).

In vielen Kriegen wurden und werden Kriegsverbrechen begangen (z. B. Folterungen, Übergriffe auf die Zivilbevölkerung etc.). Das große Machtgefälle in Kriegsgebieten und die weitgehende Freiheit vor Strafverfolgung können in Verbindung mit der Allgegenwart des Todes natürliche Hemmschwellen abbauen.

Bei Kriegen ist mit dem großen Aufkommen von Flüchtlingen zu rechnen, für deren Betreuung und Versorgung Flüchtlingslager benötigt werden. Die Überlebenden eines Krieges leiden oft unter schwerwiegenden psychischen und körperlichen Verletzungen. Folgen entstehen auch für die nächste Generation, die Kriegskinder.

Der organisierte Einsatz von Waffen in größerem Umfang bedeutet fast immer die massenhafte Tötung von Menschen. Schon die ständige Rüstung zum Krieg erfordert Aufwendungen und verschlingt Mittel, die für andere Aufgaben fehlen. Auch wenn eine kriegführende Partei Todesopfer nicht anstrebt, werden sie immer als unvermeidbar in Kauf genommen. Wer diese Wirkung betrachtet, nennt diese Form der gewaltsamen Konfliktaustragung daher meist „staatlich organisierten Massenmord“ (Bertha von Suttner, Karl Barth). Darin kommt zum Ausdruck, dass das Phänomen des Krieges kaum wertneutral zu betrachten ist, weil es dabei immer auch um das Leben vieler und die langfristigen Perspektiven aller Menschen geht.

Kriege waren für die betroffenen Gesellschaften von entscheidender Bedeutung. Durch die offensive Kriegsführung des Römischen Reichs verbreitete sich die lateinische Zivilisation in weiten Teilen Europas, während die Kulturen der eroberten Völker sich entweder anpassten oder weitgehend verschwanden. Durch die mit der Völkerwanderung verbundenen Kriege wurden wiederum das Ende des Weströmischen Reiches und durch die Kriege im Zuge der Islamisierung das Ende des Oströmischen Reiches herbeigeführt. Die Auswirkungen des Zusammenbruchs des Weströmischen Reiches waren so gravierend, dass das zivilisatorische Niveau Süd- und Mitteleuropas Jahrhunderte benötigte, um wieder auf den Stand während der Hochkaiserzeit Roms zurückzufinden.

Vielen mesoamerikanischen Kulturen des Mittelalters diente die Kriegsführung zur Erlangung von Ansehen vor ihren eigenen Göttern sowie der Gefangennahme von Kriegen und Sklaven zur Opferung, so dass die unterworfenen Kulturen neben Tributzahlungen auch bevölkerungsseitig dezimiert wurden. Die permanente Kriegsführung unterband wirksam eine gesellschaftliche und kulturelle Weiterentwicklung, so dass alle mesoamerikanischen Kulturen beim Eintreffen der Europäer in Mittelamerika technologisch stark unterlegen waren und ihrerseits von diesen besiegt wurden.

Durch die Revolutionskriege wurde der demokratische Gedanke in Europa verbreitet, durch die Bauernkriege der Protestantismus. Durch den Faschismus in Deutschland wurden in Europa im Zweiten Weltkrieg fast 50 Millionen Menschen getötet und ganze Länder verwüstet. Hier benötigte es Jahre bzw. Jahrzehnte, um die Folgen dieses globalen Krieges zu bewältigen. Als direkte Folge des Zweitens Weltkrieges entstand die Montanunion, deren Nachfolger die heutige Europäische Union ist.

Neben den politischen Auswirkungen hat ein Krieg immer eine Vielzahl an negativen Folgen: So kann er die Bevölkerung eines Landes stark dezimieren. Durch den Zweiten Weltkrieg wurden bspw. ganze Jahrgänge und eine Vielzahl an Bevölkerungsschichten nahezu ausgelöscht. Ebenso drastisch sind die vielseitigen wirtschaftlichen Folgen. Die sozialen und psychischen Folgen eines Krieges, etwa durch extreme Verwerfungen in den Moralvorstellungen, durch das Zerreißen sozialer Bindungen, durch Spätfolgen von Misshandlungen und Vergewaltigungen, können bis in spätere Generationen nachwirken. Als ebenfalls schwerwiegende Auswirkung eines Krieges sind ggf. jahrzehntelanges Leid der Kriegsversehrten und die langdauernden Folgekosten für diese aufzuführen. Auch haben Kriege immer stark negative Auswirkungen auf die Umwelt, da Landstriche durch Kriegshandlungen selbst zerstört und Ressourcen für die Kriegsführung ausgebeutet werden.

Da als eine der rationalen Kriegsursachen der Kampf um Ressourcen gilt, werden Kriege umso unwahrscheinlicher, je günstiger Ressourcen einer Region für eine andere Region verfügbar werden, ohne in einer kriegerischen Auseinandersetzung unter Lebensgefahr erobert werden zu müssen. Damit sind Kriege wirtschaftlich umso uninteressanter, je besser die bestehenden Ressourcen im Wege von Vereinbarungen genutzt werden.

Alternativen zum "militärischen" Widerstand („Krieg“) sind, wenn man angegriffen wird, die Konzepte des „zivilen Widerstands“.

Da Volkswirtschaften (ebenso wie Regionen, Städte und Familien) in erster Linie ihre eigenen Interessen vertreten und Ressourcen zurückhalten, erscheint dieses „Idealbild“ der Welt utopisch.

Die menschliche Sehnsucht nach einem Frieden, der die „Geißel der Menschheit“ überwindet, ist uralt. Politische Friedensarbeit kann sich daher auf breite und heterogene Traditionen stützen. Nach verlorenen Kriegen neigt die Bevölkerung der besiegten Staaten dazu, Krieg generell abzulehnen. So kamen in Deutschland nach 1918 Formeln wie „Nie wieder Krieg“ auf (bekannt ist das Plakat von Käthe Kollwitz mit diesem Titel). Nach Siegen hingegen wird der Krieg oft verherrlicht. So gibt es zahlreiche Siegesdenkmale, Triumphbögen und anderen Erinnerungen an große militärische Erfolge.

In der chinesischen Kosmologie des Taoteking und der Philosophie des Laotse spielte die Kriegsvermeidung durch harmonischen Interessenausgleich eine wichtige Rolle.

In Indien, China und Japan breiteten Jainismus und Buddhismus eine Ethik der Gewaltlosigkeit, Toleranz und Friedensliebe aus, die seit 500 v. Chr. die Gestalt einer Weltreligion gewann.

In der griechischen Philosophie der Antike stellten Sokrates und die Skeptiker die Selbstverständlichkeit in Frage, mit der Wahrheitsbesitz beansprucht und angeblich ewige Rechte gegen andere verteidigt werden. Die Stoiker Zenon und Chrysippos wandten sich gegen das Kriegführen und stellten Überlegungen an, ob Kriege notwendig seien oder wie man sie vermeiden könne.

In allen europäischen Staatsutopien von Platon bis Thomas Morus spielte die Gewaltminderung durch ideale Gesetzgebung und Menschenbildung eine Rolle. Eine eindrucksvolle Anti-Kriegsschrift stammt von Erasmus von Rotterdam: "Die Klage des Friedens".

Das Gottesbild des Judentums hat den weithin üblichen Einsatz der eigenen Religion zur Rechtfertigung der eigenen Kriege erschwert. In den Visionen der biblischen Heilsprophetie erscheint Gott als kommender Weltrichter, der die Völker zu endgültiger Abrüstung anweist: 

Diese Weisung zur universalen Abrüstung hat Jesus Christus durch das prophetische Zeichen des Gewaltverzichts (/) und die Selbsthingabe zur Versöhnung im Neuen Testament bekräftigt. Darum ist der tätige Einsatz für weltweiten Frieden für Christen wie für Juden integraler Bestandteil ihres Glaubens. 

In der Neuzeit wurde der Gewaltverzicht im Westen von den Religionen entkoppelt. Immanuel Kant, Jean-Jacques Rousseau und andere Aufklärer strebten den „ewigen Frieden“ an und entwarfen rechtsstaatliche und demokratische Konzepte, um ihn herbeizuführen. Ludwig van Beethoven hat diesem Traum am Ende der 9. Sinfonie mit seiner Vertonung von Schillers Gedicht "An die Freude" („alle Menschen werden Brüder“) ein musikalisches Denkmal gesetzt. Arthur Schopenhauer sagt: "Der Ursprung alles Krieges aber ist Diebsgelüst!"

Im Zeitalter der europäischen Nationalkriege gewann das Völkerrecht, nach den verheerenden Erfahrungen des Ersten Weltkriegs der Gedanke eines Völkerbunds zur Kriegsverhinderung Akzeptanz. Der Briand-Kellogg-Pakt galt der Ächtung des Krieges als eines Mittels der Politik. Die UNO hat den Angriffskrieg verboten, den Weltfrieden zum Ziel aller Politik erhoben und erstmals ansatzweise wirksame Formen der Konfliktvermeidung und Konfliktlösung ermöglicht.

Diese Tendenzen wurden durch die ungeheure Steigerung der Vernichtungsmöglichkeiten im Krieg notwendig und gestärkt. Die UNO konnte Kriegsursachen wie ökonomische und politische Interessengegensätze jedoch nicht aufheben und viele Kriege nicht verhindern. Auch die Verbreitung von Massenvernichtungswaffen ließ sich bis heute nicht wirksam stoppen. Der am Ende des Kalten Kriegs eingeleitete Abrüstungsprozess kam seit den „neuen Kriegen“ zum Erliegen und wurde durch neue Aufrüstungstendenzen abgelöst. Internationaler Terrorismus und Antiterrorkrieg lassen die Gewaltbereitschaft weltweit noch weiter wachsen.

Eine Alternative zum Frieden gibt es im Zeitalter der Massenvernichtungsmittel nicht mehr. Spätestens seit Erfindung der Atombombe ist er „die Überlebensbedingung des technischen Zeitalters“ geworden (Heidelberger Thesen der EKD 1959).

Oft wird der Krieg heroisiert. Kant beispielsweise schreibt:

Als Kriegsfetischismus bezeichnet man eine übersteigerte Begeisterung für den Krieg. Der Krieg wird dabei zumeist idealisiert. Kriegsfetischismus fand sich beispielsweise im Ersten Weltkrieg wie auch später im Nationalsozialismus. Der Krieg wurde dort als ehrenvoll, männlich und bewundernswert stilisiert.

Der Islam sieht Frieden erst nach der kriegerischen Eroberung des gesamten Dār al-Harb (Gebiet der Nichtmuslime) vor. Danach soll die gesamte Erde unter der Scharia in einer „pax islamica“ leben. Die Welteroberung geschieht mittels bewaffnetem Dschihad mit dem Ziel eines „Paradieses unter dem Schwert des Islam“.

Die Instrumentalisierung religiöser Ideale für politische Interessen fand einen Höhepunkt mit den Kreuzzügen des Mittelalters, die die heiligen Stätten „befreien“ und christliche Staaten errichten wollten. Die Kreuzzugs-Ideologie des ewigen Kampfes des „Guten“ gegen das „Böse“ spielt noch heute eine bedeutende Rolle – nicht nur im Islamismus oder bei US-amerikanischen Neokonservativen.

Immer wieder wurde in der Geschichte versucht, die Kriegsführung bestimmten Regeln oder moralischen Vorgaben zu unterwerfen, also zu einer Art Verhaltenskodex zu finden (siehe zum Beispiel Haager Landkriegsordnung). Die sich im Krieg Bahn brechende Aggression wird „höheren Werten“ unterworfen – und letztlich damit in Augen vieler Kritiker auch relativiert.

In der europäischen Literatur wird häufig so zwischen dem „geordneten“ und dem nicht geordneten Krieg unterschieden. Auf der anderen Seite stehen die, die – im Prinzip mit der gleichen Grundüberlegung – wirtschaftlichen Wohlstand als beste Kriegsprävention ansehen. Hier neigt man dazu, die Perversionen des ungehegten Krieges als Normalzustand des Krieges darzustellen. Daraus folgen Überlegungen, wie Krieg vermieden werden kann und wie man versuchen kann, einen ewigen Frieden zu erreichen. Der Krieg wird so als das absolute Böse angesehen, als das Werk von moralisch verkommenen Machthabern, die aus niederen Motiven ihr Land in einen Krieg stürzen.

Es gibt auch Ansichten, dass sich der Charakter des Krieges geändert habe und folglich heute ein „gehegter Krieg“ nicht mehr möglich sei. Dass sich die Formen des Krieges ändern, ist aber eine Feststellung, die so alt ist wie die Geschichte der Menschheit. Neue Kriegsformen wurden zu allen Zeiten als ordnungswidrig geachtet, häufig als Verstöße gegen eine göttliche Ordnung. Heute werden in der abendländischen Kultur bestimmte Kriegsformen als zulässig dargestellt (etwa Bombenabwürfe auf Städte, die Militär treffen sollen, aber auch Zivilpersonen gefährden), während andere Kriegsformen (etwa Selbstmordattentate, die nicht militärische Einrichtungen treffen) als unzulässig interpretiert werden. In der islamischen Welt halten viele Menschen dagegen Selbstmordattentate für legitim, wie nach den Anschlägen auf das World Trade Center 2001 erkennbar wurde.

Krieg ist nicht nur ein Mittel staatlich organisierter und gelenkter Politik. Neben den Staaten, die als kriegführende Seite ein Heer hatten, spielten offenbar zu allen Zeiten „nichtreguläre“ Gruppen im Krieg eine erhebliche Rolle: Kosaken, Jäger, Husaren, Rōnin, Partisanen, in der neuerer Zeit die Guerilla, Freischärler, Milizen und Taliban. Was "nicht regulär" ist, wird politisch diskutiert. Bei noch genauerem Hinsehen allerdings merkt man, dass die Theorie des irregulären Kämpfers (Partisanen) eine Weiterentwicklung der Clausewitzschen Theorie ist, wie sie die Clausewitz-Kenner Lenin und Carl Schmitt vorgenommen haben.

Somit scheitert auch der Versuch, zwischen einem Konflikt und einem formal erklärten Krieg zu unterscheiden und die Bezeichnung „Krieg“ auf jene Konflikte einzuschränken, die mit einer formalen Kriegserklärung einhergehen.

Ein Krieg verursacht hohe Kosten in der Planung, Vorbereitung und Durchführung (siehe dazu Kriegsökonomie). Der finanzielle Aspekt spielt somit eine bedeutende Rolle in der Art und Weise der Kriegsführung. Je mehr Ressourcen einer Partei zur Kriegsführung zur Verfügung stehen, umso mehr Möglichkeiten hat sie den Gegner zu bezwingen.

Die Rüstungsindustrie entwickelt und produziert Waffen. Sie erhält die Aufträge überwiegend im Auftrag einer Regierung oder Staatengemeinschaft. Die Rüstungsindustrie ist ein Wirtschaftszweig, der in Europa und den USA um etwa 1850 eine eigenständige Industrie wurde. Die Rüstungsindustrie ist in Friedenszeiten an den Kapitalmarkt gekoppelt.

Die größten Waffenlieferanten der Welt sind die Vereinigten Staaten von Amerika, gefolgt von Russland, Deutschland, Frankreich und Großbritannien. All diese Länder besitzen hochentwickelte Rüstungsbetriebe und stehen im gegenseitigen Konkurrenzkampf um die neuesten und wirkungsvollsten Waffensysteme.

Geographische Aspekte spielen bei der Kriegsführung eine entscheidende Rolle. Mit der Erforschung geographischer Umstände bei der politischen Entscheidungsfindung und bei der Kriegsführung befassen sich vor allem die Geopolitik und die Geostrategie.

Die Kriegsführung hat sich im Laufe der Menschheitsgeschichte zunehmende geographischen Dimensionen erschlossen. Traditionell fand sie vor allem zu Land und auf dem Wasser statt. Im 20. Jahrhundert reifte der Luftkrieg aus, und die Militarisierung des Weltraums und des Internet, deren militärische Nutzung von Beginn an einen Anreiz zu ihrer Nutzung dargestellt hatte, schritt voran.

Der Bodenkrieg ist von allen geographisch definierten Dimensionen der Kriegsführung die bedeutsamste, weil der Mensch dauerhaft ausschließlich zu Lande überleben kann. Darüber hinaus sind politisch verfasste Gemeinwesen nur zu Lande aufzufinden. Eine militärische Lösung eines Interessenkonflikts kann daher nur an Land stattfinden. Gegenüber anderen Militärgeographien unterscheidet sich der Krieg zu Lande vor allem darin, dass dieser trotz der jüngsten Mechanisierung weiterhin personalintensiv bleibt.

Der Seekrieg ist von den physikalischen Eigenschaften der weltweiten Wasservorkommen und von ihrer menschlichen Nutzung geprägt. Zwischen 70 % und 75 % der Erdoberfläche besteht aus Wassermassen, die bis auf wenige Ausnahmen miteinander verbunden sind. Der Seekrieg ist vor allem plattformzentriert und strategisch dem Krieg zu Lande untergeordnet, da der Mensch mangels relevanter Wassertauglichkeit über keine natürliche Seekriegsfähigkeit verfügt. Die Weite und die Unbewohnbarkeit der Weltmeere verschafft der Aufklärung und Ausweichmanövern eine wesentlich höhere Bedeutung als an Land.

Der Luftkrieg ist ebenfalls der Lebensfeindlichkeit seiner Umgebung durch Höhe, Temperatur und Sauerstoffmangel unterworfen und daher plattformzentriert. Obwohl bereits zuvor Luftkriegsmittel zum Einsatz kamen, ermöglichten technische Neuerungen eine systematische Erschließung der Luft erst im 20. Jahrhundert. Das entscheidende Merkmal der Luft, ihre Unbeständigkeit, ordnet den Luftkrieg dem Krieg zu Lande unter. Obwohl der systematische Luftkrieg die Gestalt des Krieges entscheidend verändert und Luftstreitkräfte hervorgebracht hat, ist dies keine hinreichende Bedingung für militärisches Fortkommen.

Kriegsspiele bilden die als Kriege definierten „"bewaffneten Auseinandersetzungen zwischen größeren Menschengruppen"“ in symbolischen Handlungen ab. Das Spiel vollzieht sich nach vorher vereinbarten Regeln, die strikt einzuhalten sind. Dazu gehört etwa, dass niemand bei dem Spiel absichtlich geschädigt werden darf. Es handelt sich um ein sogenanntes „Als-Ob-Handeln“, das erfahrene oder erdachte Wirklichkeiten imitiert.

Pauschale Kritik am Kriegsspielen erwächst meist aus einer persönlichen Betroffenheit angesichts der Gräuel der Kriege und einer vorschnellen Gleichsetzung der völlig unterschiedlichen Denk- und Handlungsebenen „Krieg“ und „Kriegsspiel“. Sie übersieht dabei in der Regel die Vielfalt dieser Spielgattung, die von einer abstrakten Symbolhandlung wie einer Fingerbewegung mit dem begleitenden Ruf „Peng, du bist tot, fall um!“, über historische Indianer- oder Ritterspiele, über Brettspiele wie das Schachspiel oder Bewegungsspiele wie das Völkerballspiel bis zu den Computerspielen mit galaktischen Fantasiegestalten reicht. Im weiteren Sinne zählen auch die großen Sportspiele wie etwa das Fußballspiel, bei dem jedermann problemlos von „Schießen“ und „Bomben“, von „Angriff“ und „Verteidigung“ spricht, zu den Symbolspielen mit kriegerischem Hintergrund. Unreflektierte Kritik übersieht, dass Spiel dort endet, wo aus dem Spiel blutiger Ernst wird. Sie verwechselt dabei die symbolische Handlungsebene des Spiels mit der Realität des brutalen tatsächlichen Krieges oder unterstellt unbewiesene und statistisch völlig abwegige Transfers zwischen den beiden unterschiedlichen Lebenswelten. Ähnlich anderen Imitationsspielen wie dem Arzt- oder Schulespielen folgt das Kriegsspielen nach Siegbert A. Warwitz der beobachteten oder fantasierten Wirklichkeit, nicht umgekehrt. Er weist ihm aus pädagogischer Sicht sogar eine mögliche wertvolle Perspektive zu, wo es gelingt, im Spiel Ängste zu verarbeiten oder dem Spielverhalten und Spielausgang in kreativer Weise positive Impulse zu geben. Dies verdeutlicht er etwa an der didaktisch aufgearbeiteten Version des Völkerballspiels, historisch eigentlich ein Genozid-Spiel, bei dem die symbolisch durch die „Waffe Ball“ auszurottenden Menschen des anderen „Volkes“ nach entsprechend veränderten Regeln sich durch eine Eigenleistung wieder selbst „verlebendigen können“. Gisela Wegener-Spöhring stellt fest, dass den Kriegsspielen mit der Chance, Aggressionen schadlos abzuleiten, auch eine wichtige psychologische Funktion zukommen kann. Mit kreativen Umwandlungen der Spielgedanken werden Eltern und Erzieher nach Warwitz wie Wegener-Spöring dem – ohnehin nicht verbietbaren, über die ganze Welt verbreiteten – Kriegsspiel und der Mentalität von Kindern besser gerecht als mit unüberlegten Verboten.

Kriegsähnliche Verhaltensweisen lassen sich auch im Tierreich beobachten. So führen rivalisierende Staaten (vor allem Ameisenstaaten) Kriege um Gebiete und Nahrung. Manche Ameisen-, Wespen-, Bienen- und Hornissenarten überfallen andere Staaten, um sie ihrer Rohstoffe und Nahrungsmittel zu berauben. Hierbei wird genau abgewogen, ob sich der Überfall auch lohnt – also der Verlust eigener Individuen durch den zu erwartenden Gewinn an Ressourcen in einem günstigen Verhältnis steht. Auch attackieren sich Ameisen, wenn ein Teil der Kolonie nicht mehr genetisch so homogen ist, wie der Rest (durch Gendrift oder schwindender Verwandtschaft zwischen den Königinnen).

Auch bei Schimpansen wurden schon wiederholt kriegerische Auseinandersetzungen zwischen verfeindeten Gruppen beobachtet. In manchen Fällen versuchte eine Gruppe Schimpansen, das eigene Gebiet auf Kosten der Nachbarn zu vergrößern. Schon Jane Goodall berichtete über solche Beobachtungen. (siehe auch → Schimpansenkrieg von Gombe). In anderen Fällen fühlten sich Schimpansen vermutlich durch Holzfäller bedroht und flohen auf das Territorium einer benachbarten Gruppe, die ihr Revier gegen die Flüchtlinge mit Gewalt verteidigte.


Überblick

Anthropologische Untersuchungen

Militärgeschichte

Strategietheorie

Krieg und Medien

Krieg in der moralischen und theologischen Reflexion



</doc>
<doc id="2890" url="https://de.wikipedia.org/wiki?curid=2890" title="Kim Peek">
Kim Peek

Laurence Kim Peek (* 11. November 1951 in Salt Lake City, Utah; † 19. Dezember 2009 ebenda) war ein US-amerikanischer Inselbegabter (Savant-Syndrom). Ihn zeichnete ein außergewöhnliches Gedächtnisvermögen aus. Sein Gehirn wies seit seiner Geburt eine Anomalie auf – beide Gehirnhälften waren nur minimal miteinander verbunden, was möglicherweise zu seiner Inselbegabung geführt haben könnte. Peek war das Vorbild für die Figur des autistischen "Raymond Babbitt" im 1988 erschienenen Film Rain Man, durch den er weithin bekannt wurde. Zuletzt galt Peek als einer der bekanntesten Inselbegabten. Der Autismus-Forscher Darold Treffert bezeichnete ihn als „Mega-Savant“.

Kim Peek kam mit so schweren geistigen Behinderungen zur Welt, dass die Ärzte seinen Eltern rieten, ihn in ein Heim zu geben. Sein Schädel war bereits bei der Geburt um ein Drittel größer als bei gesunden Kindern, die Nackenmuskeln konnten das Gewicht kaum halten. Als Kind fiel er dadurch auf, dass er beim Laufen und Sprechen um Jahre zurücklag, dafür aber sonderbare Gewohnheiten wie das Sortieren von Papierschnipseln zeigte, wobei er auf Störungen bei seiner Tätigkeit hysterisch reagierte. Er fing mit 16 Monaten an zu lesen und kannte mit vier Jahren acht Lexikon-Bände Wort für Wort auswendig.

Bis Ende 1963, als er zwölf Jahre alt war, wurde seiner Behinderung keine weitere Beachtung geschenkt. Als er jedoch zur Bescherung an Weihnachten ein Gedicht aufsagen sollte, rezitierte er die Weihnachtsgeschichte aus dem Lukas-Evangelium von Kaiser Augustus bis zu den Hirten mit etwa 40 Zeilen aus der Bibel. Er hatte die Geschichte zuvor nie gelesen, sondern sie am selben Tag in der Kirche gehört und sich eingeprägt.

Im Alter von 33 Jahren traf Peek auf einer Tagung der National Association for Retarded Citizens, des amerikanischen Behindertenverbandes, in Arlington (Texas) den Drehbuchautor Barry Morrow, der, fasziniert von Peeks Geschichte, diese im Film „Rain Man“ (erschienen 1988) verarbeitete.

Peek setzte sich seit seinem Durchbruch in der Öffentlichkeit für behinderte Menschen ein. Ein Teil seines Engagements bestand in Öffentlichkeitsarbeit vor Studenten und Journalisten. Sein Vater, der ihn liebevoll „Kim-Puter“ nannte – und bei dem er bis zuletzt wohnte, war dabei immer an seiner Seite. Dadurch hatten sich Peeks soziale Fähigkeiten überraschend verbessert. In den letzten Jahren kam zu den alten Interessen eine Liebe zur Musik hinzu. Peek konnte die Melodien der Platten seiner Mutter, die er in seiner Kindheit gehört hatte, zum Teil mit einem Finger am Klavier nachspielen. Er soll sich an jede Melodie erinnert haben können, die er einmal gehört hatte.

Peek starb im Alter von 58 Jahren an den Folgen eines Herzinfarktes.

Peek hatte ein außergewöhnliches Erinnerungsvermögen. Er kannte laut eigener Aussage den Inhalt von 12.000 Büchern nahezu auswendig. Dabei genügte es, wenn er das Buch nur ein einziges Mal gelesen hatte. Jede Seite eines Buches brauchte er sich nur ca. sieben Sekunden anzusehen, um sich den vollständigen Inhalt zu merken. Er erfasste dabei mit jedem Auge eine Seite zur gleichen Zeit. Danach war er nicht bereit, das Buch noch einmal zu lesen. Nach dem Lesen konnte er den Inhalt zu 99 % korrekt wiedergeben. Er beschränkte sich konsequent auf Sachbücher, im Besonderen auf solche, die Fakten zusammenstellen.

Neben dem Auswendiglernen von Büchern beherrschte Peek auch das Kalenderrechnen. Er kannte diverse Geschichtsdaten, Busverbindungen sowie das Straßennetz in den USA und Kanada und die Telefonvorwahlen und Postleitzahlen dieser Länder auswendig.

Neueste Untersuchungen von Peeks Gehirn zeigten neben der Vergrößerung und der kaum vorhandenen Verbindung der beiden Gehirnhälften auch, dass der Übergang vom Großhirn zu den inneren Gehirnschichten kaum ausgeprägt war. Peeks Kleinhirn war zudem auffallend klein.

Eine fehlende Verbindung der beiden Gehirnhälften zieht gemäß der modernen Neurobiologie einen ungebremsten Fluss von Informationen ins Bewusstsein nach sich, da die rechte und die linke Gehirnhemisphäre einander nicht intervenieren und so zum Beispiel Zugangsrechte zu Informationen der jeweils anderen Gehirnhälfte steuern können.

Bisher kann aber kein Zusammenhang zwischen den neuroanatomischen Veränderung des Gehirns Kim Peeks und seinen außergewöhnlichen Fähigkeiten nachgewiesen werden. Auch wenn das seltene Fehlen einer Verbindung zwischen beiden Gehirnhälften vorkommt, muss das nicht zwangsläufig zu Funktionsstörungen führen.




</doc>
<doc id="2894" url="https://de.wikipedia.org/wiki?curid=2894" title="Kraftstoff">
Kraftstoff

Ein Kraftstoff (auch Treibstoff) ist ein Brennstoff, dessen chemische Energie durch Verbrennung in Verbrennungskraftmaschinen (Verbrennungsmotor, Gasturbine, …) und Raketentriebwerken in mechanische Energie umgewandelt wird.

Kraftstoffe werden überwiegend zum Antrieb von Fortbewegungsmitteln (Kraftfahrzeug, Flugzeug, Schiff, Rakete) verwendet. Da sie jeweils mittransportiert werden müssen, werden häufig Stoffe mit einer hohen Energiedichte eingesetzt. Aber auch stationäre Verbrennungsmotoren werden mit ihnen betrieben.

Bei der Verbrennung wird als Oxidator meist der Luft-Sauerstoff verwendet, teils, vor allem bei Raketen, aber auch ein eigener Oxidator wie verflüssigter Sauerstoff, Lachgas oder Salpetersäure.

Die Abgrenzung des Begriffes "Kraftstoff" zu dem Begriff "Treibstoff" ist nicht durchgängig einheitlich geregelt:

In den meisten anderen Sprachen gibt es die Unterscheidung so nicht. So bedeutet z. B. im Englischen der Begriff "fuel" allgemein "Brennstoff". Dies schließt "Kraftstoffe" (manchmal "motor fuel" genannt) und "Treibstoffe" (manchmal "propellant" genannt) mit ein.

"Nicht" als Kraftstoff bezeichnet werden üblicherweise Stoffe, die zwar als Energieträger für einen Antrieb dienen, bei denen aber keine chemische Energie freigesetzt wird, z. B. Wasser für eine Wasserturbine oder Uran für den Kernreaktor eines Nuklearantriebs.





Für die Reichweite eines Fahrzeugs sind neben dem Wirkungsgrad seiner Aggregate u. a. das Volumen des Tanks und die darin gespeicherte Energie ausschlaggebend. Der physikalische Vergleich der Heizwerte (kWh pro m³) zeigt, dass flüssige Treibstoffe hinsichtlich ihrer Energiedichte optimal sind. Bei Gasen hängt der Energiegehalt stark vom Druck ab.

Die Möglichkeit, einen Treibstoff in einem Motor einzusetzen, hängt nicht nur von dessen Brennwert ab, sondern auch von der Auslegung des Motors und seiner Treibstoffzufuhr, den jeweiligen chemischen und physikalischen Eigenschaften des Treibstoffes und der ihm beigemischten Additive. Beispielsweise können sich Ventile und Ventilsitze, die für die Verbrennung von Benzin ausgelegt worden sind, bei Betrieb mit Erdgas oder Autogas (keine Beimischung von Additiven) schneller abnutzen, weshalb Fahrzeughersteller ihre Erdgasfahrzeuge mit speziellen für Erdgasbetrieb ausgelegten Motoren ausstatten.

Darüber hinaus muss nach dem Zündungsprinzip unterschieden werden, ob also Selbstzündung (Dieselmotor) oder Fremdzündung (Ottomotor) verwendet wird . 
Ein weiterer wichtiger Diskussionspunkt bei der Verwendung alternativer Kraftstoffe ist die Frage, 
ob er sich in die bestehende Infrastruktur eingliedern lässt oder eine neue Infrastruktur erfordert.
Besonders günstig in dieser Hinsicht sind alternative Kraftstoffe, die sich den bestehenden konventionellen Kraftstoffen aus fossilen Kohlenwasserstoffen beimischen lassen. 
Der Beimischungsanteil lässt sich dann "Infrastrukturneutral" allmählich anheben. 

Ein entscheidender Aspekt bei der Bewertung der Kraftstoffe (konventioneller wie alternativer Kraftstoffe) ist die Kostensituation für den Verbraucher.
Die Kraftstoffkosten sind stark von der nationalen Besteuerung abhängig und variieren je nach Land erheblich (siehe unten "Kraftstoffpreisentwicklung")

Als "Alternative Kraftstoffe" werden Kraftstoffe bezeichnet, die herkömmliche aus Mineralöl hergestellte Kraftstoffe ersetzen können. Hierbei wird unterschieden zwischen Kraftstoffen aus fossilen Energieträgern und solchen, die aus biogenen Energieträgern hergestellt sind.


Unter dem Begriff "Elektrokraftstoffe" (engl. Electrofuels) wird eine Reihe von alternativen Kraftstoffen zusammengefasst, die mit Hilfe von elektrischer Energie hergestellt werden. Um das Konzept wirtschaftlich zu machen, wird diese Energie sinnvollerweise auf regenerativem Weg, also z. B. in Solar-, Wind- oder Wasserkraftwerken, erzeugt. Mittels Elektrolyse von Wasser wird Wasserstoff gewonnen, der entweder direkt als Kraftstoff (z. B. für Brennstoffzellenfahrzeuge) verwendet werden kann oder mit CO zu unterschiedlichen gasförmigen („Power-to-gas“) oder flüssigen („Power-to-liquid“) Kohlenwasserstoffen reagieren kann; auf diesem Weg lassen sich beispielsweise Methan oder Alkohole herstellen, die in konventionellen Verbrennungsmotoren eingesetzt werden können. Dies bietet die Möglichkeit, auch Nutzfahrzeuge, Schiffe und Flugzeuge weitgehend CO-neutral anzutreiben, für die aktuell noch keine tragfähigen Elektrifizierungskonzepte bestehen.

Die bei der Verbrennung von Kraftstoffen freigesetzten Abgase bewirken Gesundheits- und Umweltschäden wie Sauren Regen und den Treibhauseffekt und somit die globale Erwärmung. Insbesondere CO, CO und Stickoxide spielen dabei wichtige Rollen. Weiterhin ist das im Benzin befindliche Benzol erwiesenermaßen karzinogen. Art und Umfang der freigesetzten Schadstoffe sind im Wesentlichen von der Zusammensetzung des Kraftstoffes, der Bauart des Motors und (wenn genutzt) der Abgasnachbehandlung abhängig.
In Deutschland regelt die 10. BImSchV die Zusammensetzung und Qualität der Kraftstoffe, um die Luftbelastungen zu mindern. Die Verordnung regelt die Beschaffenheit von Otto- und Dieselkraftstoffen, Gasöl, Biodiesel, Ethanol, Flüssiggas, Erdgas, Biogas und Pflanzenölkraftstoff.

"Siehe auch: Motorenbenzin, Abschnitt Preise, Erdgasfahrzeuge, Abschnitt Kraftstoffpreise und Markttransparenzstelle für Kraftstoffe"

Kraftstoffpreise weltweit (Auswahl) in Euro (ohne Berücksichtigung von Lohnniveau und Lebenshaltungskosten):





</doc>
<doc id="2895" url="https://de.wikipedia.org/wiki?curid=2895" title="Konjunktion">
Konjunktion

Konjunktion (aus lat. ' ‚zusammen‘; ' ‚verbinden‘) steht für:

Siehe auch:


</doc>
<doc id="2897" url="https://de.wikipedia.org/wiki?curid=2897" title="Kontinuumshypothese">
Kontinuumshypothese

Die Kontinuumshypothese wurde 1878 vom Mathematiker Georg Cantor aufgestellt und beinhaltet eine Vermutung über die Mächtigkeit des Kontinuums, das heißt der Menge der reellen Zahlen. Dieses Problem hat sich nach einer langen Geschichte, die bis in die 1960er Jahre hineinreicht, als nicht entscheidbar herausgestellt, das heißt, die Axiome der Mengenlehre erlauben in dieser Frage keine Entscheidung.

Die sogenannte einfache Kontinuumshypothese CH () besagt:

Anders ausgedrückt:

Bezeichnet man, wie üblich, die Kardinalzahl (Mächtigkeit) der natürlichen Zahlen mit formula_1 (siehe Aleph-Funktion), die darauf folgende Kardinalzahl mit formula_2 und die Kardinalzahl der reellen Zahlen mit formula_3, so heißt die Kontinuumshypothese formal:

Weiter kann man zeigen, dass die Mächtigkeit des Kontinuums mit der mit formula_5 bezeichneten Mächtigkeit der Potenzmenge von formula_1 übereinstimmt. Eine häufig anzutreffende Formulierung der Kontinuumshypothese lautet daher

Die verallgemeinerte Kontinuumshypothese (GCH, ) besagt, dass für jede unendliche Menge formula_8 folgendes gilt:

Hat man auch das Auswahlaxiom zur Verfügung, so hat jede Menge eine Kardinalzahl als Mächtigkeit, und die verallgemeinerte Kontinuumshypothese besagt, dass für jede unendliche Menge formula_8 gilt:

Verwendet man die Aleph-Notation, so bedeutet dies:

Mittels der Beth-Funktion lässt sich das noch kompakter schreiben:

Da die erste Formulierung kein Auswahlaxiom verwendet, sind die nachfolgenden scheinbar schwächer. Tatsächlich folgt in der Zermelo-Fraenkel-Mengenlehre (ZF) aus der zuerst genannten Formulierung der verallgemeinerten Kontinuumshypothese nach einem Satz von Sierpiński das Auswahlaxiom. Daher sind die gegebenen Formulierungen vor dem Hintergrund der ZF-Mengenlehre äquivalent.

Das Problem ist heute gelöst, wenn auch nicht in dem Sinne, wie die Mathematiker dies erwartet hatten:

Kurt Gödel bewies 1938, dass die Kontinuumshypothese (CH) zur Zermelo-Fraenkel-Mengenlehre mit Auswahlaxiom (ZFC) relativ widerspruchsfrei ist, das heißt, wenn ZFC widerspruchsfrei ist, was allgemein angenommen wird, aber nach dem Gödelschen Unvollständigkeitssatz nicht mit Hilfe von ZFC bewiesen werden kann, dann ist auch „ZFC + CH“ widerspruchsfrei. Dazu hatte Gödel innerhalb der ZFC-Mengenlehre die Teilklasse formula_23 der sogenannten konstruierbaren Mengen untersucht und konnte zeigen, dass in formula_23 ebenfalls alle Axiome der Mengenlehre gelten, aber darüber hinaus auch die Kontinuumshypothese erfüllt ist. Das bedeutet:

In den 1960er Jahren zeigte Paul Cohen mit Hilfe der :

Anders ausgedrückt: Auch die "Negation" der Kontinuumshypothese ist zu ZFC relativ widerspruchsfrei; die Kontinuumshypothese ist also insgesamt unabhängig von ZFC. Für diesen Beweis erhielt Cohen 1966 die Fields-Medaille.

Daher kann die Kontinuumshypothese im Rahmen der Standardaxiome der Mengenlehre weder bewiesen noch widerlegt werden. Sie kann, ebenso gut wie ihre Negation, als neues Axiom verwendet werden. Damit ist sie eines der ersten relevanten Beispiele für Gödels ersten Unvollständigkeitssatz.

Die verallgemeinerte Kontinuumshypothese ist ebenfalls unabhängig von der Zermelo-Fraenkel-Mengenlehre mit Auswahlaxiom (ZFC). Dies folgt sofort aus der Beobachtung, dass die Verneinung von CH ja erst recht eine Verneinung von GCH ist und in Gödels konstruierbarem Universum formula_23 sogar GCH gilt. Der Satz von Silver schränkt die Möglichkeiten für die kleinste Kardinalzahl, für die die verallgemeinerte Kontinuumshypothese zum ersten Mal verletzt ist, ein. Der Satz von Easton zeigt, dass die verallgemeinerte Kontinuumshypothese für reguläre Kardinalzahlen in nahezu beliebiger Weise verletzt werden kann.

In der berühmten Liste von 23 mathematischen Problemen, die David Hilbert dem Internationalen Mathematikerkongress 1900 in Paris vortrug, steht die Kontinuumshypothese an erster Stelle.
Viele Mathematiker hatten im Umfeld dieses Problems bedeutende Resultate beigetragen, weite Teile der heute sogenannten deskriptiven Mengenlehre ranken sich um die Kontinuumshypothese.

Da die reellen Zahlen eine für viele Wissenschaften grundlegende Konstruktion darstellen und da Mathematiker platonischer Ausrichtung den Anspruch erheben, die Wirklichkeit zu beschreiben, war das Unentscheidbarkeitsergebnis unbefriedigend. Nach dem Beweis der Unabhängigkeit wurden die Versuche fortgesetzt, durch Hinzunahme möglichst natürlicher Axiome zur ZFC die Kontinuumshypothese doch noch zu entscheiden, zum Beispiel durch Axiome, die die Existenz großer Kardinalzahlen postulieren. Auch Gödel war davon überzeugt, dass sich die Hypothese so widerlegen ließe. In den 2000er Jahren meinte der Mengentheoretiker William Hugh Woodin, Argumente gegen die Gültigkeit der Kontinuumshypothese gefunden zu haben. Später wandte er sich von dieser Auffassung ab und konstruierte ein Modell für Kardinalzahlen, das er Ultimate L nannte, in Anlehnung an Gödels konstruierbares Universum formula_23. In diesem Universum ist die verallgemeinerte Kontinuumshypothese wahr.

Gelegentlich werden Aussagen unter der Annahme gemacht, dass die Kontinuumshypothese wahr sei. So ergeben sich beispielsweise bei der Potenzierung von Kardinalzahlen mit der GCH als Voraussetzung erhebliche Vereinfachungen. Es ist jedoch üblich, diese Voraussetzung dann explizit zu erwähnen, während die Verwendung des ZFC-Axiomensystems oder äquivalenter Systeme in der Regel unerwähnt bleibt.

Im Folgenden sei die Kontinuumshypothese (und das Auswahlaxiom) als wahr angenommen und es wird mit ihrer Hilfe eine nicht messbare Teilmenge der Ebene formula_27 konstruiert. Man beachte, dass dies auch ohne Kontinuumshypothese (aber mit Auswahlaxiom) möglich ist.

Sei formula_28 die kleinste überabzählbare Ordinalzahl. Nach der Kontinuumshypothese gibt es dann eine Bijektion formula_29. Die ordinale Ordnung formula_30 auf formula_28 werde mit Hilfe dieser Bijektion auf formula_32 übertragen: Für formula_33 gelte: formula_34.

Es sei formula_35. Mit formula_36 bezeichnen wir die Indikatorfunktion der Menge formula_37, also formula_38 mit formula_39 genau dann, wenn formula_40.

Für jedes formula_41 sei formula_42. Diese Menge ist für jedes formula_43 abzählbar, da formula_44 als abzählbare Ordinalzahl nur abzählbar viele Vorgänger hat. Insbesondere ist daher formula_45 immer eine Lebesgue-Nullmenge: formula_46.

Weiter definieren wir für jedes formula_47 die Menge formula_48; das Komplement jeder dieser Mengen ist abzählbar, somit gilt formula_49.

Nimmt man an, dass formula_36 messbar ist, so gilt unter Verwendung des Lebesgue-Integrals und des Lebesgue-Maßes formula_51
aber
Die Funktion formula_36 ist also eine Funktion, die nach dem Satz von Tonelli nicht Lebesgue-messbar sein kann, die Menge formula_37 ist damit auch nicht messbar.

Wir betrachten Familien formula_56 ganzer Funktionen, also solcher Funktionen formula_57, die sich auf ganz formula_58 durch eine konvergente Potenzreihe darstellen lassen. Mit Hilfe des Identitätssatzes kann man folgende Aussage zeigen:
Man beachte, dass in der Wertemenge die Funktion formula_62 variiert und der Punkt formula_63 fest ist, die Wertemenge und auch die Anzahl ihrer Elemente hängt von formula_63 ab. Wir stellen nun die Frage, ob diese Aussage richtig bleibt, wenn wir endlich durch abzählbar ersetzen. Wir fragen nach der Gültigkeit von
Paul Erdős fand folgende überraschende Antwort:

Waclaw Sierpinski zeigte die Äquivalenz der Kontinuumshypothese zu Sätzen der elementaren Geometrie:




</doc>
<doc id="2900" url="https://de.wikipedia.org/wiki?curid=2900" title="Cayman Islands">
Cayman Islands

Die Cayman Islands ( Caymaninseln, auch Kaimaninseln oder Kaiman-Inseln) sind eine Inselgruppe in der Karibik und britisches Überseegebiet des Vereinigten Königreichs.

Christoph Kolumbus entdeckte die Inselgruppe während seiner vierten Entdeckungsreise am 10. Mai 1503, nachdem seine Schiffe vom vorgesehenen Kurs abgetrieben waren. Wegen der vielen dort vorgefundenen Schildkröten gab er der Inselgruppe den Namen „Las Tortugas“. Dem portugiesischen Kartografen im Dienste Spaniens Diego Ribero fielen die zahlreichen Echsen auf, sodass er auf seiner Turiner Karte von 1523 die Inseln „Lagartos“ nannte. Im 17. Jahrhundert erhielt die Inselgruppe schließlich den Namen „Las Caymanas“ nach den ebenfalls dort vorkommenden Salzwasserkrokodilen. Während dieses Jahrhunderts dienten die Inseln verschiedenen europäischen Flotten zur Süßwasseraufnahme und der Proviantergänzung durch den Fang von Schildkröten. Der erste englische Ankömmling war Sir Francis Drake, der 1586 mit einer Flotte von 23 Schiffen auf den Cayman Islands landete.

Little Cayman und Cayman Brac waren die ersten Inseln des Archipels, auf denen zwischen 1661 und 1671 von Jamaika aus Siedlungen entstanden. Sie wurden jedoch schnell wieder wegen der zahlreichen Plünderungen durch spanische Freibeuter aufgegeben. Im Vertrag von Madrid von 1670 erkannte Spanien die englische Oberhoheit über Jamaika und die Kaimaninseln an. Bis in das 18. Jahrhundert hinein blieben die Caymans weiter Stützpunkt von Piraten, unter ihnen auch Edward „Blackbeard“ Thatch, einer der bekanntesten englischen Seeräuber dieser Zeit.

In den 1730er Jahren entstanden die ersten dauerhaften Siedlungen, deren Bewohner vorwiegend Landwirtschaft betrieben. Schwerpunkt waren Anbau von Baumwolle und Zuckerrohr, bei dem bereits Sklaven eingesetzt wurden. 1773 erfasste die Royal Navy 39 Familien, von denen die meisten in Bodden Town ansässig waren. Im Jahr 1788 erlitten zehn jamaikanische Handelsschiffe auf ihrem Weg nach England vor der Küste von Grand Cayman Schiffbruch.
Auf einem der Schiffe befand sich ein Angehöriger des englischen Königshauses.
Aus Dankbarkeit über die Rettung sämtlicher Schiffbrüchiger befreite König Georg III. die Inselgruppe von sämtlichen Steuern und Abgaben.

Ihre erste Selbstverwaltung erhielten die Cayman Islands 1831 mit der Einrichtung von fünf Distrikten und einer eigenen Legislative. Als erster Regierungssitz fungierte Bodden Town, an der Südküste von Grand Cayman gelegen. Die Sklaverei wurde 1835 abgeschafft, als es noch über 950 Sklaven auf den Inseln gab. Zu Beginn des 20. Jahrhunderts wurde die Bevölkerung auf 5000 Einwohner geschätzt. Neben der Landwirtschaft wurde die Schifffahrt eine wichtige Einnahmequelle, die etwa 20 % der Arbeitsplätze bot. Das an der Westküste von Grand Cayman gelegene George Town wurde Anfang des Jahrhunderts neuer Regierungssitz.

Als 1937 mit der "Atlantis" das erste Kreuzfahrtschiff die Caymans ansteuerte, wurde die Epoche des Tourismus eingeläutet. 1950 wurde von dem Engländer Benson Greenall das erste Hotel eröffnet, 1953 nahm der erste Flugplatz auf Grand Cayman den Betrieb auf. Bis 1962 bildeten die Cayman Inseln mit Jamaika ein gemeinsames britisches Kolonialgebiet. Während Jamaika 1962 unabhängig wurde, entschieden sich die Caymans weiterhin für den kolonialen Status als britische Kronkolonie. Seit 1946 steht das Territorium auf der UN-Liste der Hoheitsgebiete ohne Selbstregierung.

1971 wurde erstmals ein britischer Gouverneur eingesetzt und 1972 trat eine neue Verfassung in Kraft. Seit Mai 2002 sind die Cayman Islands Mitglied der CARICOM. Der Hurrikan Ivan richtete 2004 schwere Schäden auf den Inseln an, die erst ein Jahr später mit einem speziellen Programm zum Wiederaufbau beseitigt werden konnten.

Die Gipfel eines unterseeischen Gebirges, des bis nach Kuba reichenden Kaimanrückens, bilden die Inselgruppe. Ihren Namen verdanken die Inseln den hier lebenden Echsenarten, den Kaimanen, die man zu Beginn mit Krokodilen verwechselt hatte.
Die Inselgruppe besteht aus den drei Inseln Grand Cayman, Little Cayman und Cayman Brac und liegt etwa 350 km südlich von Kuba. Die Inseln haben zusammen etwa eine Fläche von 262 km², wobei Grand Cayman mit 197 km² Fläche die größte ist.

Im Norden der Insel Grand Cayman befindet sich zwischen Rum Point und Conch Point die 80 km² große Bucht North Sound. Die Bucht ist seicht, mit Tiefen von zumeist 1,8 bis 3,8 Metern.

Das Territorium wird in sieben Distrikte gegliedert, die von Distriktmanagern geleitet werden und unter anderem als Wahlbezirke und als Regionaleinheiten für die Statistik fungieren. Fünf der Distrikte entfallen auf die Hauptinsel Grand Cayman, während die beiden kleineren Inseln Little Cayman und Cayman Brac jeweils einen Distrikt bilden:
Bei den Einwohnerzahlen von 2009 sind Little Cayman und Cayman Brac zu "Sister Islands" zusammengefasst.

Die Bevölkerung konzentriert sich in den drei südwestlichen Distrikten der Hauptinsel Grand Cayman George Town (Hauptstadt), West Bay und Bodden Town, die eine vielfach höhere Bevölkerungsdichte aufweisen als alle übrigen Distrikte. Hier leben auf 22 % der Fläche 89 % der Bevölkerung; in diesem Bereich beträgt die Bevölkerungsdichte 618 Einwohner/km². Der Rest des Territoriums weist eine durchschnittliche Bevölkerungsdichte von 22 Einwohnern/km² auf. Etwa 90 % der 45.436 Einwohner leben auf der größten Insel Grand Cayman.

Die Jahrhunderte brachten eine bunt gemischte Bevölkerung hervor, die sehr stolz auf die Harmonie unter den Bewohnern unterschiedlicher Herkunft ist. Ungefähr 20 % der Bevölkerung sind Schwarze, 20 % sind Weiße und 40 % sind vermischt. Die restlichen 20 % der Bevölkerung sind Immigranten aus aller Welt. Sitten und Gebräuche sind nach wie vor von den ersten Siedlern im 18. Jahrhundert geprägt, die von den britischen Inseln kamen. Die dominierende Religion ist das Christentum.

Die Lebenserwartung auf der Insel beträgt 2016 insgesamt 81,2 Jahre (Männer: 78,5 Jahre/Frauen: 84,0 Jahre). Das Median-Alter der Bevölkerung lag im selben Jahr bei 39,9 Jahren. Eine Frau bekam im Laufe ihres Lebens im Durchschnitt 1,85 Kinder. Auf 1000 Einwohner kamen im Jahr 2017 12,0 Geburten und 5,8 Todesfälle. Die Bevölkerung wächst mit 2 % pro Jahr aufgrund von Einwanderung aus anderen Karibikinseln. Die Cayman Islands sind eine wichtige Durchgangsstation für Kubaner die in die Vereinigten Staaten migrieren wollen. Ein Teil dieser Personen bleibt auf den Cayman Islands.

Quelle: UN

Frühere Hauptexportgüter waren Schildkröten und Muscheln.

Heute gilt die Hauptstadt George Town als Steuerparadies und fünftgrößter Finanzplatz der Welt. Circa 200.000 Firmen sind auf den Inseln registriert (Stand 2013). Die meisten international tätigen Banken, auch die größten deutschen, sind hier mit Filialen präsent. Zudem sind rund 40 % aller Hedge-Fonds auf den Cayman Islands angesiedelt, womit die Cayman Islands der größte Hedge-Fonds-Standort weltweit sind.
Begünstigt wird dieser Wirtschaftszweig durch günstige Rahmenbedingungen wie die hier herrschende Steuerfreiheit. Die Cayman Islands gelten als Steueroase. Sie tauchen auf der "Grey List" auf, welche die OECD im Vorfeld des G20-Treffens im Jahr 2009 erstellt hat. Aufgrund von Zusagen bezüglich der Einhaltung von diversen Steuerstandards tauchen die Cayman Islands nicht auf der Schwarzen Liste der Steuerparadiese auf.
Die deutsche Bundesregierung stuft die Inseln nicht als Steueroase ein (Januar 2010).

Ihrem Ruf wollen die Inseln durch bilaterale Abkommen entgegenwirken, die sie beispielsweise mit Irland, Japan, den Niederlanden und Südafrika abgeschlossen haben.

Am 28. Januar 2016 legte die EU-Kommission ein Maßnahmenpaket "zur Bekämpfung von Steuerflucht" vor, bei dem unter anderem die Kaiman-Inseln auf der "schwarzen Liste" der Steueroasen auftauchen.

Die Staatsausgaben werden ausschließlich über Verwaltungsgebühren und eine 20-prozentige Zollabgabe auf alle Importgüter finanziert. Auf Kraftfahrzeuge wird nach Wert gestaffelt Zoll in Höhe von 27,5 % bis 40,0 % erhoben.

In letzter Zeit reichen diese Einnahmen zur Haushaltsfinanzierung allerdings nicht mehr aus, so dass das britische Außenministerium auf die Einführung weiterer Abgaben drängt.

Als Zahlungsmittel werden neben der Landeswährung "Cayman Island Dollar" (CI$) weitestgehend auch der US-Dollar und das Britische Pfund akzeptiert.

Das Bruttoinlandsprodukt pro Kopf von 62.132 US-Dollar ist eines der höchsten weltweit (Stand 2015).

Auf den Cayman Islands herrscht Linksverkehr. Grand Cayman und Cayman Brac haben zusammen 785 km befestigte Straßen.

Im Jahr 2000 waren 1400 Schiffe auf den Inseln registriert.

2008 gab es 683 Festnetztelefone, 668 Mobiltelefone und 413 Internetnutzer pro 1000 Einwohner.

Es gibt drei Flughäfen auf den Cayman Islands:

2008 besuchten 303.000 Touristen die Inseln. Circa 1,7 Millionen weitere Besucher kommen jährlich mit Kreuzfahrtschiffen für einige Stunden nach George Town.

Bürger aus der EU bzw. aus Schengen-Mitgliedsstaaten benötigen für die Einreise kein Visum, vorgeschrieben ist lediglich ein bis zur Rück- oder Weiterreise gültiger Reisepass und ein Rück- oder Weiterreiseticket.

Vor allem die Hauptinsel Grand Cayman ist für ihre Riffe bekannt. Die Unterwasserwelt rund um die Inseln ist eine besondere Touristenattraktion und wird seit 1978 durch mehrere Gesetze umfangreich geschützt.

Zu den Sehenswürdigkeiten der Hauptstadt George Town gehören unter anderem der "Glockenturm für König Georg V.", die "Legislative Assembly", das "Courts Building" und das "General Post Office". An der Uferpromenade zeigt das "Cayman Maritime and Treasure Museum" unterschiedlichste Ausstellungsstücke. Ferner sind im "McKee's Museum" Schätze aus Wracks aus dem 16./17. Jahrhundert zu bestaunen.
George Town ist auch der bevorzugte Ort für zollfreien Einkauf: Internationale Luxusgüter sowie Schmuck, in den zum Teil einheimische Materialien, wie Schwarze Korallen, eingearbeitet sind, sind hier günstig zu erstehen. Eine weitere Spezialität der Inseln ist der Rum-Kuchen, der von verschiedenen Anbietern hergestellt wird und sich in den letzten Jahren zu einem der erfolgreichsten Exportartikel entwickelt hat.

In "Bodden Town", der früheren Hauptstadt der Inselgruppe, liegt das Höhlensystem "Pirate’s Caves", in dem im 18. Jahrhundert Piraten Zuflucht und Schutz fanden.

Rum Point im Norden der Insel bietet typisches Karibikflair. Auf der Sandbank "Stingray City" leben halbzahme Stachelrochen (southern stingrays), die man füttern kann. Die "Cayman Turtle Farm" am "Northwest Point" ist die einzige ihrer Art mit grünen Schildkröten.

Vor Cayman Brac liegt seit 1996 ein gesunkenes russisches Kriegsschiff, die umbenannte "MV Capt. Keith Tibbetts", mit vier Deckkanonen. Sie ist das einzige per Tauchgang erkundbare russische Kriegsschiff der westlichen Hemisphäre.

Die kleinste der Inseln, Little Cayman, ist einer der letzten unberührten Orte der Karibik mit einem der größten Steingärten der Region für Pelikane und Kormorane, außerdem ist sie eines der besten Gebiete der westlichen Welt zum Lachsangeln.

Die Schulpflicht beginnt mit einem Alter von fünf Jahren und dauert zwölf Jahre.

Fußball auf den Cayman Islands ist sehr beliebt. Die Inseln haben 1976, 1984, 1988, 1992, 1996, 2000, 2004, 2008 und 2012 bei Olympischen Sommerspielen teilgenommen. In den Jahren 2010 und 2014 nahmen sie an den Olympischen Winterspielen teil.

2012 fand die Squash-Weltmeisterschaft der Frauen auf den Inseln statt.



</doc>
<doc id="2903" url="https://de.wikipedia.org/wiki?curid=2903" title="Kompendium">
Kompendium

Kompendium (von lateinisch "compendium" ‚Ersparnis‘, ‚abgekürzter Weg‘) bezeichnet:
Compendium (lateinische Schreibweise) steht für:
Siehe auch:


</doc>
<doc id="2904" url="https://de.wikipedia.org/wiki?curid=2904" title="Coffein">
Coffein

Coffein oder Koffein (auch Tein, Teein oder Thein, früher auch "Caffein") ist ein Alkaloid (Purinalkaloid) aus der Stoffgruppe der Xanthine. Es gehört zu den psychoaktiven Substanzen mit stimulierender Wirkung. 

Coffein ist der anregend wirkende Bestandteil von Genussmitteln wie Kaffee, Tee, Cola, Mate, Guaraná, Energy-Drinks und (in geringeren Mengen) von Kakao. In chemisch reiner Form tritt es als weißes, geruchloses, kristallines Pulver mit bitterem Geschmack auf.

Coffein ist weltweit die am häufigsten konsumierte pharmakologisch aktive Substanz.

Auf Anregung Goethes untersuchte der Apotheker und Chemiker Friedlieb Ferdinand Runge Kaffeebohnen mit dem Ziel, die wirksame Substanz im Kaffee zu finden. 1819 gelang es Runge erstmals, aus den Kaffeebohnen reines Coffein zu isolieren. Er kann somit als Entdecker des Coffeins angesehen werden. Unabhängig von Runge gelang im Jahre 1821 den französischen Apothekern Pierre Joseph Pelletier, Joseph Bienaimé Caventou und Pierre-Jean Robiquet gemeinsam ebenfalls die Isolation des Coffeins. 1832 konnten Christoph Heinrich Pfaff und Justus von Liebig mit Hilfe von Verbrennungsdaten die Summenformel CHNO bestimmen. Die chemische Struktur wurde 1875 von Ludwig Medicus als 1,3,7-Trimethylxanthin angenommen. Die vorerst nur angenommene Struktur konnte Emil Fischer 1895 durch die erste Synthese des Coffeins bestätigen. Der Wirkungsmechanismus wurde erst im 20. Jahrhundert erfolgreich erforscht.

Der in Grüntee und Schwarztee enthaltene Wirkstoff, in der Umgangssprache oft als „Tein“, „Thein“ oder „Teein“ bezeichnet, ist ebenfalls Coffein. Diese früher übliche Unterscheidung zwischen Coffein aus Kaffee und Tein aus Tee beruht auf der unterschiedlichen Freisetzung des Alkaloids im menschlichen Organismus: Coffein aus Kaffee ist an einen Chlorogensäure-Kalium-Komplex gebunden, der nach der Röstung und Kontakt mit der Magensäure sofort Coffein freisetzt und damit schnell wirkt. Coffein aus Tee hingegen ist an Polyphenole gebunden, wobei das Alkaloid erst im Darm freigesetzt wird. Die Wirkung tritt dann später ein und hält länger an.

Die ersten medizinischen Anwendungen waren der Einsatz als Aufputschmittel (Stimulans) und Diuretikum sowie (wie schon Kaffee seit dem 18. Jahrhundert) als Medikament zur Behandlung von Atemstörungen bei Asthma bronchiale (Die atemanregende bzw. atemanaleptische und bronchienerweiternde Wirkung wurde 1912 von Jakob Pál beschrieben).

Coffein ist der Hauptwirkstoff des Kaffees. Außer in den Samen des Kaffeestrauchs kommt es auch in über 60 anderen Pflanzen vor, wie zum Beispiel dem Teestrauch, Guaraná, dem Mate-Strauch und der Kolanuss. Die chemisch mit Coffein eng verwandten Wirkstoffe Theophyllin und Theobromin finden sich ebenso in zahlreichen Pflanzenspezies. Ungeröstete Kaffeebohnen enthalten je nach Sorte etwa 0,9–2,6 % Coffein; nach der Röstung verbleiben 1,3–2,0 %. Dabei enthalten die "Coffea-arabica"-Sorten weniger Alkaloid als die "Coffea-robusta"-Typen. Fermentierte und getrocknete Teeblätter, sogenannter Schwarzer Tee, enthalten – ebenso wie unfermentierter Grüner Tee – etwa 3–3,5 % Coffein.

In den Pflanzen (insbesondere in ungeschützten Keimlingen) wirkt es als Insektizid, indem es bestimmte Insekten betäubt oder tötet.

Coffein kann mittels Extraktion aus Teeblättern oder Kaffeebohnen, zum Beispiel mit einem Soxhlet-Aufsatz, gewonnen werden. Es fällt in großen Mengen bei der industriellen Entkoffeinierung von Kaffee an, wobei als Extraktionsmittel entweder Dichlormethan, Essigsäureethylester oder überkritisches Kohlenstoffdioxid verwendet wird. Daneben wird Coffein hauptsächlich mittels Traube-Synthese industriell hergestellt.

Coffein ist ein Trivialname, der der Substanz wegen des Vorkommens in Kaffee gegeben wurde, der aber nichts über die chemische Zusammensetzung aussagt. Nach der systematischen IUPAC-Nomenklatur lautet die vollständige Bezeichnung 1,3,7-Trimethyl-2,6-purindion, eine Kurzform 1,3,7-Trimethylxanthin – nach der chemischen Ableitung des Coffeins vom Xanthin. Es gehört zur Gruppe der natürlich vorkommenden Purine ("Purinalkaloide"), genauso wie die strukturähnlichen Dimethylxanthine Theophyllin und Theobromin.

Die Struktur des Coffeins besteht aus einem Doppelring, an dem sich außen mehrere Substituenten befinden. Dieser Doppelring im Kern entspricht der Grundstruktur des Purins. Er besteht aus zwei Ringen, einem 6er- und einem 5er-Ring, die jeweils zwei Stickstoff-Atome enthalten. Außen findet man an C-2 und C-6 jeweils ein doppelt gebundenes Sauerstoff-Atom. Beim Coffein befindet sich an N-1, N-3 und N-7 noch jeweils eine Methylgruppe (-CH). Daneben gibt es noch das Isocoffein, bei dem eine der Methylgruppen nicht am N-7, sondern am N-9 hängt. Dem Theophyllin fehlt von den drei Methylgruppen die an N-7, dem Theobromin fehlt die an N-1.

Reines Coffein ist unter normalen Bedingungen ein weißes, geruchloses, kristallines Pulver mit bitterem Geschmack. Coffein tritt in zwei enantiotrop polymorphen Kristallformen auf. Die bei Raumtemperatur stabile β-Form (Tieftemperaturform) wandelt sich bei 141 °C in die α-Form (Hochtemperaturform) um.<ref name="DOI10.1016/0040-6031(94)01958-J">M. Epple, H. K. Cammenga, S. M. Sarge, R. Diedrich, V. Balek: "The phase transformation of caffeine: Investigation by dynamic X-ray diffraction and emanation thermal analysis." In: "Thermochimica Acta." 250, 1995, S. 29, .</ref> Diese schmilzt bei 236 °C. Die Rückumwandlung von α- zur β-Form ist kinetisch gehemmt, so dass die α-Form über Wochen bei Raumtemperatur metastabil sein kann. Die Verbindung ist leicht sublimierbar (ab 178 °C). Die Löslichkeit ist temperaturabhängig:

Coffein bildet bei Kristallisation aus Wasser ein kristallines Hydrat in Form langer Nadeln. Stöchiometrisch enthält das Hydrat im Kristallgitter 0,8 mol Wasser pro Mol Coffein.<ref name="DOI10.1016/0040-6031(80)87173-5">H. Bothe, H. K. Cammenga: "Composition, properties, stability and thermal dehydration of crystalline caffeine hydrate." In: "Thermochimica Acta." 40, 1980, S. 29, .</ref>

Xanthinderivate wie das Coffein werden als schwache Basen bezeichnet, da sie Protonen über ihre Stickstoffatome aufnehmen können. Dennoch sind Lösungen von Xanthinderivaten nicht alkalisch. Xanthinderivate werden zu den Alkaloiden gezählt. Als Alkaloide werden generell alle physiologisch wirksamen, niedermolekularen stickstoffhaltigen Verbindungen, insbesondere pflanzlicher Natur, bezeichnet.

Das neben der Coffeinbase pharmazeutisch ebenfalls verwendete Coffeincitrat, ein Coffein-Citronensäure-Gemisch (ASK, Nomenklatur nach IUPAC: 1,3,7-Trimethyl-3,7-dihydro-2"H"-purin-2,6-dion + 2-Hydroxypropan-1,2,3-tricarbonsäure) hat die Summenformel CHNO, eine molare Masse von 386,31 g·mol und die CAS-Nummer 69-22-7. Es ist ein weißes kristallines Pulver, löslich 1:4 in heißem Wasser (Dissoziation), 1:25 in Ethanol 96 %.
Die wesentlichen Wirkungen des Coffeins sind:


Coffein hat zwar ein relativ breites Wirkungsspektrum, doch ist es in geringen Dosen in erster Linie ein Stimulans. Darunter versteht man im Allgemeinen eine Substanz mit anregender Wirkung auf die Psyche, die Antrieb sowie Konzentration steigert und Müdigkeitserscheinungen beseitigt. Es wird eine "anregende" von einer "erregenden" Wirkung des Coffeins unterschieden, wobei für letztere eine höhere Dosis erforderlich ist. Bei niedriger Dosierung tritt fast ausschließlich die zentral anregende Wirkung des Coffeins hervor, es werden also vor allem psychische Grundfunktionen wie Antrieb und Stimmung beeinflusst. Durch eine höhere Dosis kommt es auch zu einer Anregung von Atemzentrum und Kreislauf.

Während höhere Coffeinkonzentrationen auch die motorischen Gehirnzentren beeinflussen, wirkt das Coffein in geringeren Konzentrationen hauptsächlich auf die sensorischen Teile der Hirnrinde. Aufmerksamkeit und Konzentrationsvermögen werden dadurch erhöht; die Steigerung von Speicherkapazität und Fixierung (mnestische Funktionen) erleichtert den Lernprozess. Mit der Beseitigung von Ermüdungserscheinungen verringert sich das Schlafbedürfnis. Die Erhöhung des Blutdrucks ist gering und verschwindet bei längerfristiger Einnahme; ein Effekt kann erst wieder beobachtet werden, nachdem die Coffeineinnahme mindestens 24 h abgesetzt wurde. Verursacht wird die milde Blutdruckerhöhung durch die zentralnervöse Stimulierung (Erregung des vasomotorischen Zentrums); dem wirkt eine gleichzeitige Senkung durch die Herabsetzung des peripheren Widerstandes kompensatorisch entgegen. Die Stimmung kann sich bis zu leichter Euphorie steigern. Infolge von Assoziationsbahnung verkürzen sich die Reaktionszeiten, was zu einer Beschleunigung des psychischen Tempos führt. Gleichzeitig kommt es zu einer – nur minimalen – Verschlechterung der Geschicklichkeit, speziell bei Aufgaben, die exaktes Timing oder komplizierte visuomotorische Koordination erfordern.
Das breite Wirkungsspektrum verdankt Coffein mehreren Wirkungskomponenten, die auf molekularer Ebene in bestimmte Zellvorgänge eingreifen.
Coffein kann die Blut-Hirn-Schranke fast ungehindert passieren und entfaltet seine anregende Wirkung hauptsächlich im Zentralnervensystem.

Einer neuen Studie nach sollte Coffein nicht nur die Konzentrationsfähigkeit steigern, die Vigilanz und Aufmerksamkeit verbessern und die Geschwindigkeit von Denkprozessen erhöhen, sondern auch das Langzeitgedächtnis verbessern.<ref name="DOI10.1038/nn.3623">Daniel Borota, Elizabeth Murray, Gizem Keceli, Allen Chang, Joseph M Watabe, Maria Ly, John P Toscano, Michael A Yassa: "Post-study caffeine administration enhances memory consolidation in humans." In: "Nature Neuroscience." 17, 2014, S. 201, .</ref>

Coffein in Genussmitteln, wie z. B. in Schwarztee oder Cola, kann insbesondere für Kinder problematisch sein: so können z. B. drei Dosen Cola (je nach Quelle 65–250 mg bzw. 150–350 mg in 990 ml) ungefähr soviel Coffein wie zwei Tassen Kaffee enthalten (je nach Quelle 100–240 mg oder 160–240 mg Coffein in 250 ml Filterkaffee). Ein dreißig Kilogramm schweres Kind kann so auf eine Konzentration von 5–12 Milligramm Coffein pro Kilogramm Körpergewicht kommen; eine Dosis, die ausreicht, Nervosität und Schlafstörungen zu verursachen.

Coffein stand von 1984 bis 2004 auf der Dopingliste des Internationalen Olympischen Komitees, allerdings waren die Grenzwerte so hoch, dass Sportler durchaus Kaffee zum Frühstück trinken konnten. Dennoch wurde am 25. Juli 2000 der spanische Radprofi Óscar Sevilla (Team Kelme) „positiv“ auf Coffein getestet und daraufhin von seinem Verband von der Straßen-Weltmeisterschaft ausgeschlossen. Die World Anti-Doping Agency hat mit Wirkung zum 1. Januar 2004 das Stimulans Coffein von der Liste der verbotenen Substanzen gestrichen. Pasman u. a. (1995) verglichen die Auswirkungen von 0, 5, 9 bzw. 13 Milligramm pro Kilogramm Körpergewicht eine Stunde vor Belastung und stellten fest, dass alle Dosierungen größer als 0 eine signifikante leistungssteigernde Wirkung im Radfahrtest (80 % Wmax) hatten. Die niedrigste Dosierung lag unterhalb der Festsetzungsgrenze als Doping.<ref name="DOI10.1055/s-2007-972996">W. Pasman, M. van Baak, A. Jeukendrup, A. de Haan: "The Effect of Different Dosages of Caffeine on Endurance Performance Time." In: "International Journal of Sports Medicine." 16, 1995, S. 225, .</ref>

Die orale LD für eine Ratte liegt bei 381 Milligramm pro Kilogramm. Bei Menschen liegt die letale Dosis bei ungefähr 10 Gramm Coffein (5–30 g), was etwa 100 Tassen Kaffee entspricht.

Die Wirkung des Coffeins begründet sich auf zellulärer Ebene wie folgt: Im Wachzustand tauschen Nervenzellen Botenstoffe aus und verbrauchen Energie. Dabei entsteht Adenosin als Nebenprodukt. Eine der Aufgaben des Adenosins besteht darin, das Gehirn vor „Überanstrengung“ zu schützen. Es setzt sich an bestimmte Rezeptoren auf den Nervenbahnen (die Adenosinrezeptoren vom Subtyp A2a). Ist Adenosin gebunden, ist das ein Signal für die Zelle, etwas weniger zu arbeiten. So entsteht ein negativer Rückkopplungseffekt: je aktiver die Nervenzellen, desto mehr Adenosin wird gebildet und desto mehr Rezeptoren werden besetzt. Die Nervenzellen arbeiten langsamer. Das Coffein ist dem Adenosin in seiner chemischen Struktur ähnlich und besetzt dieselben Rezeptoren, aktiviert sie jedoch nicht. Adenosin kann nicht mehr andocken, und die Nervenbahnen bekommen kein Signal – deshalb arbeiten sie auch bei steigender Adenosinkonzentration weiter. Die Adenosinrezeptoren werden kompetitiv durch Koffein gehemmt.

Analgetische, also schmerzhemmende Effekte des Coffeins werden diskutiert. Als Mechanismus werden auch hier die antagonistischen Effekte an den Adenosinrezeptoren und die damit verminderte Wirkung des Adenosins auf das zentrale Nervensystem angenommen. Adenosin wirkt an den sensorischen Nervenendungen schmerzerzeugend, indem es direkt auf die spezifischen A2-Rezeptoren einwirkt und eine Schmerzüberempfindlichkeit (Hyperalgesie) verursacht.<ref name="DOI10.1016/0306-4522(90)90068-F">Y. O. Taiwo, J. D. Levine: "Direct cutaneous hyperalgesia induced by adenosine." In: "Neuroscience." 38, 1990, S. 757, .</ref>

In höheren Dosen verhindert Coffein den enzymatischen Abbau von cAMP (cyclischem Adenosin-3’,5’-monophosphat). Dieses spielt im menschlichen Organismus als second Messenger eine wichtige Rolle in der Regulation zellulärer Vorgänge. Coffein hemmt jene Enzyme, spezifische Phosphodiesterasen, die für den Abbau von cyclischem zu acyclischem AMP verantwortlich sind. So kommt es durch den gehemmten Abbau zu einem Anstieg der cAMP-Konzentration in den Zellen.

Eine Studie am Duke University Medical Center in Durham, North Carolina, aus dem Jahre 2004 zeigte auf, dass die Zufuhr von Koffein in Kombination mit dem Konsum einer kohlenhydrathaltigen Mahlzeit den Blutzuckerwert sowie den Insulinwert bei Personen mit Diabetes Typ 2 erhöht.<ref name="DOI10.2337/diacare.27.8.2047">J. D. Lane, C. E. Barkauskas, R. S. Surwit, M. N. Feinglos: "Caffeine Impairs Glucose Metabolism in Type 2 Diabetes." In: "Diabetes Care." 27, 2004, S. 2047, .</ref>

Wenn ein Mensch über längere Zeit hohe Dosen Coffein zu sich nimmt, verändern sich die Nervenzellen. Sie reagieren auf das fehlende Adenosin-Signal und bilden mehr Rezeptoren aus, so dass wieder Adenosin-Moleküle an Rezeptoren binden können. Die Nervenzellen arbeiten langsamer. Die anregende Wirkung des Coffeins ist also stark eingeschränkt. Bereits nach 6 bis 15 Tagen starken Coffeinkonsums entwickelt sich eine derartige Toleranz.

Wird der Coffeinkonsum stark verringert, können Entzugserscheinungen auftreten (siehe unten), die aber meistens nur von kurzer Dauer sind. Coffein ist preisgünstig und legal verfügbar und das weltweit am häufigsten konsumierte Stimulans. Aus der wissenschaftlichen Literatur geht nicht klar hervor, ob Coffein als Suchtmittel anzusehen ist, es hat jedenfalls einige Gemeinsamkeiten mit typischen Suchtmitteln. Zu den wichtigsten gehören die Entwicklung von Toleranz sowie psychischer und körperlicher Abhängigkeit mit Entzugserscheinungen. Toleranz tritt bei nicht unbedingt übermäßigem, aber bei regelmäßigem Coffeingenuss auf.

Als Entzugssymptome wurden in einer empirischen Studie beobachtet: Kopfschmerzen, Erschöpfung, Energieverlust, verminderte Wachsamkeit, Schläfrigkeit, herabgesetzte Zufriedenheit, depressive Stimmung, Konzentrationsstörungen, Reizbarkeit und das Gefühl, keine klaren Gedanken fassen zu können. In einigen Fällen kamen auch grippe-ähnliche Symptome hinzu. Die Symptome setzen zwölf bis 24 Stunden nach dem letzten Coffein-Konsum ein, erreichen nach 20 bis 51 Stunden das Symptommaximum und dauern etwa zwei bis neun Tage. Bereits eine geringe Menge Coffein führt zur Rückfälligkeit.

Zu den Symptomen des Entzugs gehören auch Veränderungen der Theta-Wellen im Gehirn.

Bei Überdosierung (Dosen über 1 g) treten Erregungserscheinungen, stark beschleunigter Puls und Extrasystolen auf; als Gegenmittel können Kohletabletten, Verapamil und Diazepam gegeben werden.

In sehr hoher Konzentration (ab etwa 10 mM im Zellaußenraum) setzt Coffein Calcium-Ionen aus dem endoplasmatischen Retikulum frei. Das geschieht durch seine spezifische Bindung an Ryanodin-Rezeptoren. Aufgrund dieser Eigenschaft wird Coffein in der physiologischen Forschung verwendet. Die benötigte Dosis übersteigt die letale Dosis von Säugetieren bei weitem, deshalb wird Coffein nur bei "in vitro"-Experimenten eingesetzt.

Coffein verstärkt die herzfrequenzsteigernde Wirkung von Sympathomimetika.
Es wirkt beruhigenden Wirkstoffen wie Antihistaminika und Barbituraten entgegen.
50 mg Coffein können eine relative analgetische Wirkungsstärke von 1,3 bis 1,7 bei gleichzeitiger Einnahme von Acetylsalicylsäure oder Paracetamol (mögliche Einsparung von Schmerzmitteln) aufweisen.
Disulfiram und Cimetidin reduzieren den Coffeinabbau im Körper. Rauchen sowie Barbiturate beschleunigen den Coffeinabbau im Körper.
Die Ausscheidung von Theophyllin wird durch Coffein verringert.
Bei gleichzeitiger Einnahme von Antibiotika der Gruppe Gyrasehemmer (Chinolon-Antibiotika) ist eine mögliche Verzögerung der Ausscheidung von Coffein und seinem Abbauprodukt Paraxanthin gegeben.
Coffein kann eine mögliche Abhängigkeit von Substanzen vom Typ Ephedrin erhöhen.

Personen mit Leberzirrhose (mögliche Coffein-Anreicherung), Personen mit Herzarrhythmien, wie Sinustachykardien/Extrasystolen (mögliche Verstärkung), Personen mit Hyperthyreose (mögliche Verstärkung der Nebenwirkungen von Coffein) und Personen mit Angstsyndrom (mögliche Verstärkung) sollten Coffein nur in geringen Dosen (etwa 100 mg/Tag) einnehmen.

Von regelmäßiger Einnahme hoher Dosen wird wegen des möglichen Auftretens von Coffeinismus abgeraten. Während einige Forscher aufgrund von Studien an Mäusen dazu raten, in der Schwangerschaft auf Coffein zu verzichten, hält das American College of Obstetricians and Gynecologists in einer 2010 herausgegebenen Empfehlung eine Tagesdosis von 200 Milligramm Coffein für unbedenklich. Eine brasilianische Studie ergab, dass moderater Koffeinkonsum in Schwangerschaft und Stillzeit in den ersten drei Lebensmonaten keinen Einfluss auf den Schlaf von Säuglingen zu haben scheint.
Eine neue Studie der EFSA (Europäische Behörde für Lebensmittelsicherheit) belegt, dass eine Koffeinzufuhr von bis zu 400 mg pro Tag (das entspricht ungefähr 5.7 mg/kg Köpergergewicht bei einer 70 kg schweren Person) als sicher einzustufen ist. Auch für Schwangere, Stillende und Kinder wurden Richtwerte ermittelt: Für schwangere und stillende Frauen gilt, dass eine über den gesamten Tag verteilte Koffein-Aufnahme aus allen Quellen von bis zu 200 mg pro Tag für den Fötus unbedenklich ist. Für Kinder und Jugendliche pendelt sich der Richtwert bei 3mg/kg Körpergewicht ein und gilt bei dieser Dosierung als unbedenklich.

Zur Pharmakoepidemiologie des Coffeins liegen Untersuchungen zum Einfluss des Coffeins auf den Blut-Lipidstatus aus nationalen Untersuchungen in der Bevölkerung der Bundesrepublik Deutschland vor. Unter anderem konnte eine Erhöhung der Triglyceride im Blutserum bei Probanden mit einem Gebrauch coffeinhaltiger Arzneimittel nachgewiesen werden. Auch zum Coffein-Einfluss auf den Glucose- und Magnesium-Gehalt des Serums wurden Ergebnisse veröffentlicht. Danach wurden höhere Glucosespiegel und erniedrigte Magnesiumspiegel in Seren von Probanden gemessen, die coffeinhaltige Arzneimittel verwendeten.

Auf Bakterien, Pilze und Algen kann Coffein mutagen wirken; dies wird vermutlich durch Hemmung von Reparaturmechanismen der DNA bei diesen Lebewesen verursacht. Bei höheren Tieren oder dem Menschen konnte eine solche Wirkung bisher nicht nachgewiesen werden.

Der Metabolismus von Coffein ist speziesspezifisch. Bei Menschen werden etwa 80 % des aufgenommenen Coffeins durch das Enzym Cytochrom P450 1A2 zu Paraxanthin demethyliert und weitere etwa 16 % werden in der Leber zu Theobromin und Theophyllin umgesetzt. Durch weitere partielle Demethylierung und Oxidation entstehen Urate- und Uracil-Derivate. Aus dem Urin können etwa ein Dutzend unterschiedlicher Coffein-Metaboliten extrahiert werden, aber weniger als 3 % des ursprünglich aufgenommenen Coffeins. Die Hauptausscheidungsprodukte im Urin sind Di- und Monomethylxanthin sowie Mono-, Di- und Trimethylharnsäure.

Die Pharmakokinetik von Coffein hängt von vielen inneren und äußeren Faktoren ab. Die Resorption von Coffein über den Magen-Darm-Trakt in die Blutbahn erfolgt sehr rasch und nahezu vollständig: etwa 45 Minuten nach der Aufnahme ist praktisch das gesamte Coffein aufgenommen und steht dem Stoffwechsel zur Verfügung (Bioverfügbarkeit: 90–100 %). Mit kohlensäurehaltigen Getränken wird Coffein sogar noch rascher aufgenommen. Die maximale Plasmakonzentration wird 15 bis 20 Minuten nach der Aufnahme des Coffeins erreicht. Die Verabreichung von 5 bis 8 mg Coffein/kg Körpergewicht resultiert in einer Plasma-Coffeinkonzentration von 8–10 mg/l. Die biologische Halbwertszeit von Coffein im Plasma beträgt zwischen 2,5 und 4,5 Stunden (andere Quellen sprechen von 3 bis 5 h) bei gesunden Erwachsenen. Dagegen erhöht sich die Halbwertszeit auf im Mittel 80 Stunden (36–144 h) bei Neugeborenen und auf weit über 100 Stunden bei Frühgeburten. Bei Rauchern "reduziert" sich die Coffein-Halbwertszeit um 30–50 %, während sie sich bei Frauen, die orale Verhütungsmittel einnehmen, "verdoppelt". Bei Frauen, die sich im letzten Trimenon der Schwangerschaft befinden, steigt sie auf 15 Stunden an. Ferner ist bekannt, dass das Trinken von Grapefruitsaft vor der Coffeinzufuhr die Halbwertszeit des Coffeins verlängert, da der Bitterstoff der Grapefruit die Metabolisierung des Coffeins in der Leber hemmt. Bei stillenden Müttern hat ein Kaffeekonsum von weniger als zwei Tassen am Tag keinen Einfluss auf das nächtliche Schlafverhalten des Säuglings.

Zur Analytik des Coffeins werden chromatographische Verfahren bevorzugt. Insbesondere die Gaschromatographie, die HPLC und Kopplungen dieser Trenntechniken mit der Massenspektrometrie sind in der Lage, die geforderte Spezifität und Sensitivität bei der Analytik komplexer Matrices in der physiologischen Forschung und in der lebensmittelchemischen Analytik zu gewährleisten. In der pharmazeutischen Analytik wird auch die Dünnschichtchromatographie zur qualitativen und quantitativen Bestimmung von Coffein eingesetzt. Auch Enzymimmunoassays (EIA) für die Routineanalytik von Serum- oder Harnproben stehen zur Verfügung. Die damit erzielten Ergebnisse können in Zweifelsfällen durch GC-MS- oder HPLC-MS-Verfahren überprüft werden.

Isoliertes natürliches oder synthetisches Coffein wird wegen seiner anregenden Wirkung manchen Erfrischungsgetränken (Cola-Getränke), Energy-Drinks und Süßwaren zugesetzt.

Coffein erhöht die analgetische Wirkstärke von Acetylsalicylsäure oder Paracetamol um den Faktor 1,3 bis 1,7, so dass deren Dosis in Kombinationsarzneimitteln entsprechend reduziert werden kann. Solche coffeinhaltigen Kombinationsschmerzmittel sind besonders auch in der Behandlung des Migränekopfschmerzes angezeigt.

In Kombination mit dem Mutterkornalkaloid Ergotamin wird Coffein ebenfalls zur Behandlung des Migräneanfalls verwendet.

"Coffeincitrat" wird unter dem Handelsnamen "Peyona" zur Behandlung der primären Apnoe (Atemstillstand ohne offensichtliche Ursache) bei Frühgeborenen angewendet. Apnoe bei Frühgeborenen bezeichnet ein Aussetzen der Atmung über mehr als 20 Sekunden. Da es nur wenige Patienten mit primärer Apnoe gibt – 32.000 Betroffene in der EU – gilt die Krankheit als selten und Coffeincitrat wurde am 17. Februar 2003 in dieser Indikation als Arzneimittel für seltene Leiden („Orphan-Arzneimittel“) ausgewiesen. Coffeincitrat wird als Infusionslösung (20 mg/ml) verabreicht. Die Lösung kann auch eingenommen werden und ist auf ärztliche Verschreibung erhältlich.

Coffein ist in Dosen von 50 bis 200 mg zur kurzfristigen Beseitigung von Ermüdungserscheinungen angezeigt.

"Coffein-Natriumsalicylat", ein Salz des Coffeins, das im menschlichen Körper besser resorbiert wird als Coffein, wurde früher als Kreislauf- und Atemstimulans und Diuretikum verwendet. Heute ist diese Anwendung obsolet.

Seit April 2014 hat "Coffeincitrat" zur Vorbeugung gegen die bronchopulmonale Dysplasie den Status eines Orphan-Arzneimittels.

Coffein soll den Haarwuchs fördern, wie an der Friedrich-Schiller-Universität Jena entdeckt wurde, was eine Verwendung bei Haarausfall denkbar macht. Die heute auf dem Markt erhältlichen Koffein-Shampoos und Tinkturen stehen allerdings im Ruf, voreilige und wissenschaftlich nicht gerechtfertigte Versprechungen zu ihrer Wirksamkeit abzugeben.

Coffeinhaltige Hautcremes werden zur Hautstraffung und -glättung, z. B. bei Cellulite, beworben.

Produkte mit natürlichem Coffeingehalt:


Den folgenden Produkten wird üblicherweise synthetisch erzeugtes Coffein beigemischt. Teilweise wird aber auch natürliches Coffein, gewonnen bei der Kaffee-Entkoffeinierung, verwendet. Besonders sogenannten Wellness-Produkten wird häufig natürliches Coffein als Guaraná-Extrakt zugesetzt.


Im Jahre 1997 erklärten Wissenschaftler in einem Appell an die Food and Drug Administration, es sei bedeutend, die Deklaration des Coffein-Gehalts in Lebensmitteln zur Pflicht zu machen.




</doc>
<doc id="2905" url="https://de.wikipedia.org/wiki?curid=2905" title="Konvention von Montevideo">
Konvention von Montevideo

Die Konvention von Montevideo über Rechte und Pflichten der Staaten ist ein Vertrag, der am 26. Dezember 1933 in Montevideo im Rahmen der "Seventh International Conference of American States" („Siebte Internationale Konferenz Amerikanischer Staaten“) von 20 amerikanischen Staaten unterzeichnet wurde. Es handelt sich bei dem Vertrag um regionales Völkervertragsrecht, das Bindungen nur für die Vertragsstaaten entfaltet.

Auf dieser Konferenz in Montevideo erklärten der US-Präsident Franklin D. Roosevelt und der US-Außenminister Cordell Hull ihre Ablehnung bewaffneter Interventionen in inneramerikanischen Angelegenheiten. Damit sollte dem Eindruck des US-Imperialismus widersprochen werden. Dies wird auch "Good Neighbor Policy" genannt.

Artikel 1 der Konvention beinhaltet eine Definition des Begriffes ‚Staat‘:

Die Montevideo-Konvention legt somit die Definition des Staates sowie die Rechte und Pflichten der Staaten fest. Sie erweitert die Zusammengehörigkeit der drei klassischen Voraussetzungen (nach der Jellinekschen Trias der konstitutiven Elemente des Staatsbegriffs) um eine vierte Bedingung: die aus einer äußeren Souveränität ("ausschließliche Völkerrechtsunmittelbarkeit") folgende Fähigkeit zur Aufnahme auswärtiger Beziehungen. Diese kann nach der allgemeinen Staatenpraxis jedoch nicht als notwendiges Erfordernis und als „irrelevant für die Staatlichkeit angesehen“ werden.

Der erste Satz des Artikels 3 legt fest, dass „Die politische Existenz eines Staates unabhängig von seiner Anerkennung durch die anderen Staaten ist.“ (engl. "“The political existence of the state is independent of recognition by the other states.”") Dies wird als die "Deklarative Theorie der Souveränität" bezeichnet. Mehrfach wurde hinterfragt, ob diese Kriterien ausreichend sind, da sie nicht vollständig anerkannten Staaten wie der Republik China "(Taiwan)" oder prinzipiell nicht anerkannten Gebilden wie dem Fürstentum Sealand einen Status als Staat zuerkennen. Entsprechend der alternativen "Konstitutiven Theorie der Souveränität" existiert ein Staat nur dann, wenn dieser von anderen Staaten anerkannt würde.




</doc>
<doc id="2906" url="https://de.wikipedia.org/wiki?curid=2906" title="Kurd Laßwitz">
Kurd Laßwitz

Kurd Laßwitz (* 20. April 1848 in Breslau; † 17. Oktober 1910 in Gotha; eigentlich "Carl Theodor Victor Kurd Laßwitz") war ein deutscher Schriftsteller. Er publizierte zudem unter dem Pseudonym "L. Velatus" und gilt als Begründer der deutschsprachigen Science Fiction. Sein Roman "Auf zwei Planeten" aus dem Jahr 1897 gehört zu den wichtigen deutschen Science-Fiction-Romanen und wurde in zahlreiche Sprachen übersetzt.

Kurd Laßwitz – Sohn des Unternehmers und Politikers Karl Wilhelm Laßwitz – studierte Mathematik und Physik an den Universitäten in Breslau und Berlin. 1873 folgte die Promotion „magna cum laude“ mit einer Arbeit »über Tropfen, welche an festen Körpern hängen und der Schwerkraft unterworfen sind«. Im Folgejahr legte er das Staatsexamen für den höheren Schuldienst in den Fächern Mathematik, Physik, Philosophie und Geographie ab.

1876 siedelte er nach Gotha über. Dort trat er eine Stelle als Gymnasiallehrer am Ernestinum Gotha an, wo unter anderem Hans Dominik sein Schüler war. 1884 erfolgte die laufbahnübliche Ernennung zum Gymnasialprofessor und 1909 zum Hofrat. Letzteres verdankt er besonders seinem Wirken in der bildungsbürgerlichen „Mittwochsgesellschaft zu Gotha“, die mit populären Vorträgen aus dem Bereich von Naturwissenschaft, Literatur und Philosophie zur Volksbildung beitrug. Die "Mittwochsgesellschaft" war 1884 gegründet worden, was wesentlich auf der Initiative von Laßwitz selbst basierte. Im Jahr 1884 wurde er zum Mitglied der Leopoldina gewählt.

Er korrespondierte mit zahlreichen Geistesgrößen seiner Zeit, mit Ludwig Anzengruber und Martin Buber. Bertha von Suttner versuchte ohne Erfolg, Kurd Laßwitz für ihre Friedensbewegung zu gewinnen, da sie nach Lektüre seiner Werke in ihm einen Geistesverwandten sah.

Kurd Laßwitz starb im Alter von 62 Jahren in Gotha, wo er im Krematorium Gotha eingeäschert wurde. Seine Urne ist auf dem Hauptfriedhof in einem von der Stadt gepflegten Ehrengrab beigesetzt.

Laßwitz gilt als einer der Väter der modernen Science Fiction. Er schrieb Bücher über Physik, Erkenntnistheorie sowie Immanuel Kant und bearbeitete eine kritische Ausgabe von Gustav Theodor Fechner, dem Begründer der Psychophysik. Anders als Jules Verne und stärker als Herbert George Wells verwendete Kurd Laßwitz die SF vor allem für belehrende und kritisierende Zwecke. Seine Zukunftsentwürfe sind mutiger als die Werke seiner beiden Kollegen und Zeitgenossen, weil sie weiter in die Zukunft reichen. Daher stößt er nach eigener Aussage immer wieder an die Grenzen . Sein Roman Auf zwei Planeten (1897) mit seinen über tausend Seiten gehört zu den wichtigsten deutschen Science-Fiction-Romanen. Er wurde in zahlreiche Sprachen übersetzt und mehrmals neu aufgelegt. "Auf zwei Planeten" war eine wichtige Inspiration für den Raumfahrtpionier Eugen Sänger und für den Himmelsmechaniker Karl Stumpff Anlass, sich in seiner Jugend der Astronomie zuzuwenden. Das Werk hatte Anfang des 20. Jahrhunderts, trotz zunächst beschränkter Verbreitung, einigen (noch ungenügend erforschten) Einfluss auf andere Autoren – etwa die des Expressionismus. Sichergestellt ist ein Einfluss auf Georg Heym; Arno Schmidt schätzte den Roman. Eine größere Verbreitung fanden Laßwitz’ Werke erst in den 1920er Jahren.

Seine gesellschaftskritischen Texte gerieten größtenteils in Vergessenheit, nachdem sie von den Nationalsozialisten verboten worden waren, deren Anschauungen mit Laßwitz’ humanistischen und pazifistischen unverträglich waren.

Ein besonderes Verdienst von Laßwitz ist die erstmalige Verwendung des Begriffs Fernschule in seinem kurzen Aufsatz "Die Fernschule" aus dem Jahr 1899. Damit nutzt Laßwitz als erster die bis heute gängige Bezeichnung für Unternehmen des Fernunterrichts.


Laßwitz’ Nachlass befindet sich in der Forschungsbibliothek Gotha. Er ist durch ein gedrucktes Verzeichnis erschlossen, welches online zur Verfügung steht.


Eine auf zwanzig Bände angelegte Werkausgabe erscheint als "Kollektion Laßwitz" seit 2008 im Verlag Dieter von Reeken, Lüneburg.






</doc>
<doc id="2907" url="https://de.wikipedia.org/wiki?curid=2907" title="Kenia">
Kenia

Kenia [] (Swahili, englisch "Kenya" []), offiziell die Republik Kenia, ist ein Staat in Ostafrika. Er grenzt im Nordwesten an den Südsudan, im Norden an Äthiopien, im Nordosten an Somalia, im Süden an Tansania, im Westen an Uganda und im Südosten an den Indischen Ozean. Die Hauptstadt und größte Stadt des Landes ist Nairobi, die zweite Millionenstadt ist Mombasa. Die Wirtschaft Kenias ist, gemessen am Bruttoinlandsprodukt, die größte in Südost- und Zentralafrika.

Kenia erlangte im Dezember 1963 die Unabhängigkeit von Großbritannien. Nach der Verabschiedung einer neuen Verfassung im August 2010 ist Kenia in 47 halbautonome Gebietskörperschaften, sogenannte "Counties", unterteilt, in denen jeweils ein gewählter Gouverneur regiert.

Kenia grenzt im Nordwesten an den Südsudan, im Norden an Äthiopien, im Osten an Somalia, im Süden an Tansania und im Westen an Uganda. Im Südosten grenzt der ostafrikanische Staat an den Indischen Ozean.

Folgende Inseln gehören ebenfalls zum Staatsgebiet Kenias:

Zentral-Mittel-Kenia wird vom Rift Valley durchzogen, einem Teil des Ostafrikanischen Grabenbruchs. Die höchste Erhebung – der Batian mit 5199 m – befindet sich im Mount-Kenya-Massiv. Der tiefste Punkt liegt bei 0 m an der 480 km langen Küste des Indischen Ozeans; dort sind teilweise Korallenriffe vorgelagert. Die längsten Flüsse des Landes sind: Tana, Sabaki und Kerio. Im Osten erstreckt sich das Küstentiefland als ein schmaler Saum. Die Küstenlinie wird dabei von Buchten und Lagunen gegliedert. Südlich sind Korallenriffe vorgelagert. Nach Westen schließen sich Hügel- und Tafelländer an.

Kenia kann in zwei Klimazonen unterteilt werden:
Im Hochland, das höher als 1800 m liegt, kommt es von April bis Juni und von Oktober bis November zu Regenperioden. Der Niederschlag fällt meist nachmittags, abends und nachts. Die Nächte sind relativ kühl. Die kälteste Zeit in dieser Region liegt im Juli und August mit etwa 10 °C täglichem Minimum. Die warme Periode liegt im Januar und Februar mit etwa 25 bis 26 °C täglichem Maximum. Die Luftfeuchtigkeit beträgt etwa 65 Prozent. In Nairobi liegen die Temperaturen im Juli bei 11 bis 21 °C und im Februar bei 13 bis 26 °C. Die jährliche durchschnittliche Niederschlagsmenge liegt in Nairobi bei 958 mm. Am Victoriasee sind die Temperaturen viel höher, hier gibt es zum Teil starke Regenfälle.

An der Küste liegen die Temperaturen zwischen 22 und 32 °C, und die mittlere Luftfeuchtigkeit beträgt etwa 75 Prozent. Der meiste Niederschlag fällt von April bis Juni. Die trockensten Monate sind Januar und Februar. Die wärmsten Monate sind Oktober bis Mai.

Die Tier- und Pflanzenwelt Kenias ist sehr vielfältig. Tier- und Pflanzenarten können auf Safari-Touren durch zahlreiche Parks entdeckt werden: Bäume im Nairobi Arboretum, Strauße im Ostrich Park, Giraffen im Lang’ata Giraffe Centre und Elefanten im David Sheldrick Wildlife Trust.

Es gibt eine Vielzahl an Nationalparks in Kenia, die ein wichtiges Standbein für den Tourismus darstellen. Der größte Nationalpark ist der Tsavo-Nationalpark, der in Tsavo-Ost und Tsavo-West gegliedert ist. Die bekannte Masai Mara, der nördliche Ausläufer der Serengeti, ist formell kein Nationalpark, sondern ein Naturschutzgebiet. Bedingt durch die Herdenwanderungen von Gnus, Zebras, Antilopen, Büffeln und Impalas, findet man hier besonders in den Monaten Juli und August einen großen Tierreichtum. Weitere bedeutende Nationalparks sind Amboseli, Lake Nakuru und Meru. Ebenso sehenswert ist der Nationalpark in Nairobi, ein kleineres tierreiches Reservat inmitten der Hauptstadt. Wohl nirgends sonst kann man Giraffen und Zebras so dicht vor einer Großstadtskyline beobachten.

Die zusammengefasste Fruchtbarkeitsziffer lag im Jahr 2008 bei 4,9 Kindern pro Frau. Dieser Wert könnte unter anderem daher rühren, dass nur 32 % der kenianischen Frauen moderne Verhütungsmethoden zur Verfügung standen.

Kenia nimmt mit etwa 46,7 Millionen Einwohnern (2016) Rang 34 der bevölkerungsreichsten Länder der Erde ein. Die durchschnittliche Lebenserwartung bei der Geburt wird für 2015 mit 62,1 Jahren angegeben. Auf dem Höhepunkt der AIDS-Epidemie 2000 lag sie nur noch bei 50 Jahren Ein Bevölkerungsanteil von über 42 % ist weniger als 15 Jahre alt.

Entwicklung der Bevölkerung
Quelle: UN, Zahlen für 2030 und 2050 sind Prognosen

Insgesamt leben in Kenia mehr als 40 verschiedene Volksgruppen, die mehr als 50 verschiedene Sprachen und Dialekte sprechen.

Die meisten Einwohner Kenias gehören bantusprachigen Volksgruppen an. Zu diesen zählen die Kikuyu (mit rund 22 % Bevölkerungsanteil die größte Volksgruppe Kenias), die Luhya (14 %), die Kamba (11 %), die Kisii (6 %), die Mijikenda (5,1 %) und die Meru (4,3 %).

Des Weiteren leben im Nordwesten Kenias nilotische Volksgruppen wie die Kalendjin (mit 12,9 % die drittgrößte Gruppe), die Luo (mit 10,5 %), die Turkana (2,6 %), die Massai (2,2 %) und die Samburu (0,6 %).

Zu den kuschitischsprachigen Völkern im Nordosten des Landes gehören die kenianischen Somali (6,2 %) und die Oromo sowie kleinere Gruppen wie die Rendille (0,2 %) und die El Molo.

Nicht-afrikanische Bevölkerungsgruppen (Europäer vorwiegend britischer Herkunft, Asiaten, Araber) machen etwa 1 % der Bevölkerung aus.

Die kenianische Regierung erkennt aktuell (Stand: 2017) 43 Völker als zur Staatsbürgerschaft berechtigt an. Daneben gibt es eine Reihe von Völkern und Communitys, die vermutlich bereits vor der Unabhängigkeit in Kenia lebten, aber als staatenlos gelten – insgesamt rund 18.500 Personen. Zu ihnen zählen die Shona, die Shirazi, die Galja’el sowie die Pemba.

Nach der geltenden Verfassung gilt seit 1992 Swahili mit Englisch als eine der offiziellen Sprachen des Parlamentes, jeder Kandidat muss Kenntnisse der Sprache nachweisen. Alle Beschlüsse des Parlamentes sind aber auf Englisch zu fassen. Auf der unteren Ebene der Gerichte ist Swahili als Verhandlungssprache zugelassen, Niederschriften und Urteile sind hingegen auf Englisch auszufertigen. Die öffentlichen Verwaltungen dürfen im Verkehr mit dem Bürger Englisch und Swahili verwenden. Im Entwurf der neuen Verfassung sind Englisch und Swahili als die beiden offiziellen Sprachen des Staates vorgesehen, Swahili außerdem als nationale Sprache. Die wichtigsten Sprachen von insgesamt 52 Volksgruppen (die Linguisten führen ganz genau 61 Sprachvarianten inkl. indischer Sprachen und Dialekte auf) sind:

Nach der Volkszählung von 2009 sind 82,6 Prozent der Bevölkerung Christen, davon etwa 26 % Anglikaner, 23,3 % Katholiken, 2,5 % Orthodoxe sowie Anhänger der zahlreichen afrikanischen Kirchen. Insgesamt sind 47,4 % der Bevölkerung Protestanten.

Nur noch knapp 1,6 % der Kenianer werden den traditionellen afrikanischen Religionen zugerechnet. Weiterhin gibt es insbesondere an der Küste Muslime der sunnitischen Richtung, die ungefähr 11,1 % der Gesamtbevölkerung ausmachen und vor allem in den südöstlichen Küstengebieten leben. Im östlichen Viertel des Landes dominieren muslimische Somali, die etwa die Hälfte aller Muslime Kenias ausmachen. Genauere Zahlen sind umstritten, da eine Unterscheidung zwischen kenianischen Somali und zwischen einer halben und einer Million Flüchtlingen aus Somalia schwierig ist. Daher könnten sie inzwischen bis zu 20 % der Bevölkerung ausmachen. Daneben sind 0,1 % der Bevölkerung Hindus und 2,4 % der Bevölkerung sind konfessionslos.

Das Gebiet des heutigen Staates Kenia war bereits vor mehr als vier Millionen Jahren von frühen Vormenschen wie Australopithecus anamensis und Kenyanthropus platyops besiedelt und gehört zu jenen Regionen Afrikas, in denen sich die Gattung Homo entwickelte.

Die Geschichte Kenias als Kolonie beginnt 1885 mit einem deutschen Protektorat über die Besitzung Witu an der Küste des Sultanats von Sansibar. 1888 kam die Imperial British East Africa Company nach Kenia und verwaltete bis 1895 Britisch-Ostafrika. Deutschland übergab Witu gemäß dem sogenannten Helgoland-Sansibar-Vertrag von 1890 an die Briten.

1895 rief die britische Regierung Britisch-Ostafrika als Protektorat aus und gab 1902 das fruchtbare Bergland als Siedlungskolonie für Weiße frei. 1920 wurde Kenia offiziell zur Kronkolonie.

Kenia durchlebte Kolonialgeschichte und war lange Zeit in der Hand der Briten. Von 1952 bis 1960 war Kenia auf Grund von Aufständen im Ausnahmezustand. 1957 fanden die ersten Wahlen statt. Die Kenya African National Union (KANU) bildete die erste Regierung. Am 12. Dezember 1963 wurde Kenia unabhängig. Ein Jahr später wurde Jomo Kenyatta der erste Präsident der Republik Kenia, in der ein Einparteiensystem galt.

2017 erklärte der Oberste Gerichtshof die Präsidentenwahl wegen eines Hackerangriffs für ungültig. Es ist das erste Mal, dass eine Wahl in Afrika gerichtlich annulliert wurde.

Die Säuglingssterblichkeit lag im Jahr 2008 bei 77 je 1000 Geburten, die Müttersterblichkeit 560 je 100.000 Geburten. Nach Angaben von UNAIDS sind 5,9 bis 6,3 % der erwachsenen Bevölkerung (15-49 Jahre) HIV-positiv. Jugendliche sind besonders häufig betroffen. AIDS hat in Kenia zu einem Absinken der Lebenserwartung beigetragen. In den letzten Jahren konnten jedoch Fortschritte im Kampf gegen die Epidemie erzielt werden und die durchschnittliche Lebenserwartung stieg wieder an. 2015 gehörte sie laut Angaben der Weltbank mit 66,6 Jahren zu den höchsten in Subsahara-Afrika. 

Entwicklung der Lebenserwartung in Kenia über Zeit

2002 wurde in Kenia mit dem "Childrens Act" eine moderne Kinderschutzgesetzgebung verankert. Hiernach ist die Genitalbeschneidung an Mädchen unter 17 Jahren gesetzlich verboten und wird in Artikel 14 unter Strafe gestellt. Es ist nicht bekannt, ob dieser Artikel vor Gericht schon einmal zur Anwendung kam.

Die kenianische Regierung hat einen „Nationalen Aktionsplan zur Abschaffung der Genitalbeschneidung von 1999 bis 2019“ (also ein Programm gegen die "weibliche" Genitalbeschneidung) aufgestellt. Dieser Plan deutet darauf hin, dass die Beschneidung von Frauen und Mädchen – wie sie z. B. wieder von Mungiki praktiziert wird – noch nicht überall erfolgreich verhindert werden kann.

Homosexualität in Kenia ist in Teilen der Gesellschaft tabuisiert und homosexuelle Handlungen zwischen Männern sind strafbar. Allerdings gab es seit einigen Jahren keine strafrechtlichen Verurteilungen auf dieser Grundlage. Darüber hinaus gibt es in Kenia weder ein Antidiskriminierungsgesetz noch eine Anerkennung gleichgeschlechtlicher Paare.

Im Januar 2006 litt der Nordosten des Landes unter den Folgen einer Dürre und dem damit einhergehenden Nahrungsmittelmangel. Von der Hungerkrise am Horn von Afrika 2006 waren auch in den angrenzenden Ländern Millionen Menschen bedroht.

Anfang April 2006 kam es nach rekordartigen Regenfällen (Nairobi: 54,5 mm) im ganzen Land zu Überflutungen. Viele Menschen starben in den Fluten. Mehrere Orte im Norden waren von der Außenwelt abgeschnitten, so dass die Armee Hubschrauber zur Versorgung der Bevölkerung einsetzte.

2011 kam es zu einer weiteren schweren Hungerkrise am Horn von Afrika, von der bis zu 11,5 Millionen Menschen in Somalia, Äthiopien, Kenia und Dschibuti betroffen waren.

2015 waren 19,1 % der Bevölkerung unterernährt. Im Jahr 2000 betrug die Rate noch 32,2 %.

Der Lehrplan orientiert sich am sogenannten 8-4-4-System, welches das koloniale Schulsystem mit seinen eurozentristischen Inhalten ablöste, das heißt acht Jahre Grundschule, vier Jahre Gymnasium und vier Jahre Hochschule.

Jedes Jahr findet zwischen den Schulen ein Wettkampf um die höchsten Punktzahlen im nationalen Wettbewerb statt. Die Medien berichten ausführlich und mit Homestorys über die Sieger und Siegerinnen. Die besten Schüler des Landes erhalten vom Präsidenten Preise, etwa einen Ochsen oder ein Universitätsstipendium.

Die Alphabetisierungsrate in Kenia betrug 2015 78,0 %. 

Kindergärten und Vorschulerziehung ("preschool education") sind überwiegend auf die Städte beschränkt und kostenpflichtig. Sie werden meist von bildungsstarken und wohlhabenderen Elternhäusern verlangt. Einige der Kindergärten arbeiten z. B. nach der Montessoripädagogik. Der US-amerikanisch-britische Unternehmen Bridge International Academies (BIA) betreibt einige Vorschulen.

Besonders auf dem Land wurden viele Grundschulen nach dem Harambee-Prinzip unterhalten, das heißt die Eltern finanzierten sie durch Spenden selbst. Diese Schulen waren in jeder Hinsicht arm. Diese Situation verbesserte sich erst, als 2003 die Regierung Kibaki ihr Wahlversprechen einlöste und das Schulgeld für die „Primary Schools“ abschaffte. Damit ermöglichte sie zum ersten Mal den Zugang zur Bildung für Kinder aus ärmeren Familien. Es gingen innerhalb eines Jahres 1,7 Millionen Kinder mehr zur Schule. Jedoch blieben Investitionen im Bildungssektor aus, und das Schulsystem ist kaum im Stande, der steigenden Anzahl von Schülern gerecht zu werden.

Das Lehrer-Schüler-Verhältnis hat sich auf 1:100 verschlechtert, ein qualitativ guter Unterricht ist daher kaum möglich. Zudem nimmt die Zahl der Lehrer kontinuierlich ab. Und wer eine halbwegs akzeptable Lehrer-Schüler-Relation für seine Kinder wünscht mit dem daraus resultierenden besseren Lernerfolg und sich nicht zufrieden gibt, nur dem Papier nach seine Kinder eine Klasse weiter aufsteigen zu lassen, ist weiterhin gezwungen, seine Kinder gegen entsprechendes Schulgeld auf eine der vielen Privatschulen zu schicken. Zu ihnen gehören zahlreiche Schulen von BIA. Der Betrieb dieser Schulen ist umstritten.

Weiterführende Schulen (Klasse 9–12) sind kostenpflichtige Gesamtschulen. Ihre Träger sind der Staat, große Organisationen wie z. B. die Kirchen oder private Unternehmer. Die beiden letzteren werden allgemein als Privatschulen bezeichnet. Aufgrund der Kosten sind diese Schulen für große Teile der Bevölkerung unzugänglich, auch wenn die Privatschulen Stipendien vergeben. Einige Schulen nehmen kostenlos nur begabte Kinder aus den Slums auf.

Eine Berufsausbildung, wie sie in Deutschland etwa nach dem Dualen System oder in Berufsfachschulen flächendeckend bekannt ist, existiert in Kenia nicht. Eine Art Ausbildung gibt es im Betrieb (in-service-training) oder an einem der in den Städten zahlreichen Privatinstitute. Dort werden etwa Kfz-Mechaniker, Frisöre oder Computerfachleute ausgebildet. Alle diese Ausbildungen kosten Geld. Ein Hardware-Fachmann wird zum Beispiel in Nairobi für 2000 Euro in 18 Monaten ausgebildet. Eine solche Ausbildung erhöht die Chancen auf dem freien Markt enorm.

Kenia hat heute sieben staatliche Volluniversitäten und eine Vielzahl von Colleges. Nur die besten Schüler erhalten an den staatlichen Universitäten kostenlose Studienplätze. Wer weniger „gut“ ist, ist auf die kostenpflichtigen (internationalen) Privatuniversitäten angewiesen. An den Universitäten fehlt es öfter an nötigen Geldern, daher sind Streiks der Dozenten oder Studenten häufig.

Noch immer zieht es die Elite des Landes (oder Gemeinschaften, die über Harambee das Geld aufbringen) vor, ihre Kinder in Großbritannien oder den USA studieren zu lassen. Einige kommen zum Studium auch nach Deutschland. Ein Auslandsstudium gibt ihnen in der Regel einen Startvorteil bei der Berufssuche. Manche Studiengänge können in Kenia nicht belegt werden.

Kenia ist nach der Verfassung von 2010 eine Präsidialrepublik. Der Staatspräsident verfügt über weitreichende Exekutivvollmachten. Ihm unterstehen sowohl die Regierung als auch die Streitkräfte. Nach friedlicher Annahme der neuen Verfassung mit großer Mehrheit im Referendum vom August 2010 finden mit der Umsetzung des Grundrechtekatalogs, den Reformen in den Feldern Sicherheit und Justiz sowie der Dezentralisierung politischer Verantwortungen wichtige Änderungen statt. Kenia ist seit den allgemeinen Wahlen vom 4. März 2013 ein dezentral aufgebautes und verwaltetes Land, das in 47 "counties" gegliedert ist. Neben dem Präsidenten und Vizepräsidenten wurden erstmals Gouverneure und Parlamente auf Bezirksebene gewählt.

Im Demokratieindex 2016 der britischen Zeitschrift "The Economist" belegt Kenia Platz 92 von 167 Ländern und gilt damit als eine „Hybridregime“ aus demokratischen und autoritären Elementen. Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „teilweise frei“ bewertet. 

Wegen Korruptionsvorwürfen wurden 2005/06 zehn Minister entlassen oder traten wegen größerer Finanzskandale (Anglo-Leasing-Skandal und Goldenberg-Skandal) selbst zurück. Zu den aus mangelnder Loyalität zur Regierung von Präsident Kibaki 2006 entlassenen Ministern gehörten unter anderem Bauminister Raila Odinga und Außenminister Kalonzo Musyoka.

Am 22. März 2006 trat das (nach der für die Regierung gescheiterten Volksabstimmung über die neue Verfassung) von Präsident Kibaki beurlaubte Parlament zur neunten Periode zusammen. Kibaki äußerte sich zur neuen Verfassung und positiv zum Kampf gegen die Korruption. Er bejahte die Pressefreiheit (hier bezog er sich auf den „Sturm auf den Standard“), mahnte die Presse aber gleichzeitig zur Verantwortung.

Am 28. November 2006 gelang in der KANU ein interner Parteicoup, in dem durch eine nicht durch die Führung der Partei einberufene Delegiertenkonferenz in Mombasa eine neue Parteiführung gewählt und wohl mit Hilfe der Regierung und Ex-Präsident Moi registriert wurde. Damit wurde trotz lautstarker Proteste mit Uhuru Kenyatta fast die gesamte alte Parteiführung entmachtet und Nicholas Biwott neuer Parteichef. Gleichzeitig wurde er damit auch der mit zahlreichen Privilegien versehene offizielle Führer der Opposition im Parlament.

Der Oberste Gerichtshof (High Court) hob diese Entscheidung am 29. Dezember 2006 wieder auf, bis in der Hauptsache am 11. Februar 2007 entschieden würde. Damit war Uhuru wieder Partei- und Oppositionsführer – auf Zeit.

Präsident Mwai Kibaki (vom Stamm der Kikuyu), Raila Odinga (vom Stamm der Luo) und Kalonzo Musyoka traten in der Präsidentenwahl im Dezember 2007 als Kandidaten an. Die Wahlen fanden am 27. Dezember statt. Kibaki wurde hierbei mit einer knappen Mehrheit von 300.000 Stimmen im Amt bestätigt. Die Opposition und internationale Wahlbeobachter sprechen von massiven Wahlfälschungen. Nach der Bekanntgabe der Vereidigung von Mwai Kibaki zum Präsidenten kam es in ganz Kenia zu schweren Auseinandersetzungen zwischen den Sicherheitskräften und Demonstranten. Schwerpunkte der gewalttätigen Auseinandersetzungen waren die Hauptstadt Nairobi, Kisumu, und die Städte Nakuru, Naivasha und Eldoret in der Provinz Rift Valley. Es wurden bis zu 1.500 Menschen getötet und mehr als 600.000 Menschen vertrieben. Angeheizt wurde die Situation durch ethnisch motivierte Gewaltakte in mehreren Landesteilen, die für einen Wechsel an der Spitze des politischen Systems stimmten. Diese richteten sich vor allem gegen Kikuyu, Angehörige der Ethnie von Präsident Kibaki.

Am 4. August 2010 wurde über eine neue Verfassung in einem Referendum abgestimmt, die mehrheitlich von der Bevölkerung angenommen wurde.

Die Bezugspunkte der kenianischen Außenpolitik liegen traditionell innerhalb Afrikas, zunehmend auch im Rahmen der Vereinten Nationen und von Regionalorganisationen. Die Republik Kenia ist seit der Unabhängigkeit im Jahr 1963 von politischer Stabilität geprägt und bemüht sich daher auch um regionale Stabilisierung im ostafrikanischen Raum.

In Ostafrika nimmt Kenia eine regionale Führungsrolle für sich in Anspruch. Das Land tritt als Vermittler in regionalen Konflikten auf. Beleg dafür sind kenianische Friedensbemühungen etwa in Burundi, in Südsudan und in Somalia sowie die Teilnahme an allen für die Region wichtigen Initiativen. So ist Kenia eine treibende Kraft in der East-African Community (EAC), der regionalen wirtschaftlichen Kooperationsgemeinschaft mit dem Fernziel, eine politische Union nach dem Vorbild der EU zu werden. Des Weiteren arbeitet Kenia in der Inter-Governmental Authority on Development (IGAD) mit.

Im IGAD-Rahmen hatte Kenia zu Fortschritten beim Friedensprozess für Sudan beigetragen und war wichtige Stütze der Unabhängigkeit Südsudans. Dies gilt auch für die Stabilisierung des fragilen Nachbarlands Somalia. Flüchtlingsbewegungen aus Somalia sowie die kritische Sicherheitslage vor allem an der kenianisch-somalischen Grenze bleiben Gründe für das aktive Engagement Kenias in diesem Bereich. Kenia bemüht sich darum, das weltgrößte Flüchtlingslager Dadaab in absehbarer Zeit zu schließen und ermutigt dessen Bewohner zur freiwilligen Rückkehr nach Somalia. Ein dreiseitiges Abkommen mit UNHCR und der somalischen Regierung bildet hierfür die Grundlage.

Kenia unterhält insbesondere im Sicherheitsbereich enge Beziehungen zu den USA und der ehemaligen Kolonialmacht Großbritannien und beteiligt sich an der Zusammenarbeit gegen den internationalen Terrorismus. Durch seinen militärischen Einsatz in Südsomalia im Rahmen von AMISOM (African Union Mission in Somalia) trägt Kenia zur Bekämpfung der dortigen islamistischen Al-Shabaab-Miliz bei. Mit den Anschlägen gegen die US-Botschaft in Nairobi im Jahr 1998, gegen ein Ferienhotel in Mombasa 2002, im September 2013 mit dem Überfall von Al-Shabaab auf die Westgate-Mall, und dem Anschlag auf die Garissa University im April 2015 wurde Kenia selbst mehrfach Ziel von massiven Terrorakten.

Kenia ist außerdem ein wichtiger Partner bei den internationalen Bemühungen zur Bekämpfung der Piraterie im Indischen Ozean und kooperiert durch die Strafverfolgung mutmaßlicher Piraten vor kenianischen Gerichten mit der EU-Mission "Atalanta" zur Bekämpfung der Piraterie vor der somalischen Küste.

Bei den allgemeinen Wahlen am 4. März 2013 wurden der Präsident, die Nationalversammlung, der Senat sowie Gouverneure und Repräsentanten der Landkreise ("Counties") neu gewählt.

Bei der Präsidentschaftswahl traten acht Kandidaten an. Am 9. März wurde Uhuru Kenyatta, ältester Sohn von Kenias erstem Präsidenten Jomo Kenyatta, von der Wahlkommission zum Sieger erklärt. In der Folge wurde das Wahlergebnis von mehreren unterlegenen Parteien, vor allem Raila Odinga, vor dem Höchstgericht beeinsprucht. Am 30. März wurden alle Einsprüche abgewiesen. Laut dem am 18. Juli veröffentlichten offiziellen Endergebnis wurden für Uhuru Kenyatta 6.173.433 gültige Stimmen abgegeben, womit mit 50,51 % aller gültigen Stimmen die notwendige absolute Mehrheit erreicht wurde. Raila Odinga erreichte mit 5.340.546 gültigen Stimmen 43,70 %.

Bei den Wahlen zur Nationalversammlung entfielen von 349 Sitzen 167 Sitze auf die Jubilee-Koalition, 141 auf die CORD-Koalition, 24 auf die Amani-Koalition sowie weitere 17 Sitze auf sonstige Kleinparteien.

Im August 2017 gewann Kenyatta erneut die Präsidentschaftswahl gegen Odinga, sie musste aber aufgrund von Unregelmäßigkeiten wiederholt werden. Odinga rief zum Boykott dieser Wahl auf. Kenyatta gewann mit rund 98 % der Stimmen, bei einer Wahlbeteiligung von rund 39 %.

Amnesty International bemängelt in seinem Jahresbericht von 2010, dass es der kenianischen Regierung am politischen Willen fehlt, die Personen, die für Menschenrechtsverstöße bei den gewalttätigen Ausschreitungen nach den Wahlen im Jahr 2007 verantwortlich waren, vor Gericht zu stellen und für eine angemessene Entschädigung der Opfer zu sorgen. Auch am Zustand der Straflosigkeit für Mitarbeiter der Staatssicherheitsdienste, die ungesetzliche Tötungen und Folterungen zu verantworten hatten, änderte sich nichts, so der Bericht. Auch andere Menschenrechtsorganisationen wie zum Beispiel die Kenya National Commission of Human Rights (KNCHR) beklagen weiterhin eine „Kultur der Straflosigkeit“ in Kenia, da es bislang nicht gelungen ist, ein örtliches Strafgericht zur Aburteilung der Verursacher und Hintermänner der gewaltsamen Ausschreitungen nach den Wahlen am 27. Dezember 2007 einzurichten. Eine endgültige Entscheidung, ob der IStGH in Den Haag sich des Falles Kenia offiziell annimmt, steht noch aus.

Menschenrechtsverteidiger sahen sich Bedrohungen und großen persönlichen Risiken ausgesetzt. Bei pogromartigen Zusammenstößen zwischen einzelnen Ethnien kommt es ebenfalls immer wieder zu Menschenrechtsverletzungen. Tausende von Menschen wurden bereits gewaltsam aus ihren Häusern vertrieben. Sexualverbrechen wie Vergewaltigungen und Zwangsbeschneidungen von Frauen und Mädchen sind nach wie vor weit verbreitet. Im März 2010 erschien eine Studie der International Federation of Women Lawyers (FIDA), der zufolge in Kenia für Frauen und Mädchen mit Behinderungen das Risiko, Opfer geschlechtsspezifischer Gewalt zu werden, dreimal so hoch ist wie für nicht behinderte Frauen. Die Studie stellte außerdem fest, dass entsprechende Vorfälle praktisch nie zur Anzeige gelangen.

Die Kinderprostitution in dem ostafrikanischen Land hat gemäß einem UNICEF-Bericht dramatische Ausmaße angenommen. Fast ein Drittel aller Mädchen zwischen 12 und 18 Jahren hätten bereits Sex gegen Geld oder Geschenke gehabt. Bei rund 36 % aller Geschlechtsakte mit Kinderprostituierten seien keine Kondome verwendet worden. Laut dem Bericht bieten bis zu 15.000 Mädchen im Alter von 12 bis 18 Jahren an den kenianischen Küsten gelegentlich Sex gegen Geld oder Sachgüter an. Das seien bis zu 30 % dieser Altersgruppe in der Region. Regelmäßig prostituierten sich dort zwei- bis dreitausend Jungen und Mädchen. Im Human Rights Report 2009 der Vereinigten Staaten von Amerika wird zur Situation der Menschenrechte in Kenia zusätzlich der Menschenhandel und die Rekrutierung von Kindersoldaten erwähnt.

In Kenia herrscht ein Klima der gesellschaftlichen Ablehnung, Diskriminierung und Gewalt gegen Menschen aufgrund ihrer sexuellen Orientierung oder Geschlechtsidentität. 2007 hat der Rat der Imame und Prediger von Kenia Homosexualität öffentlich verurteilt und sich gegen die Legalisierung von gleichgeschlechtlichen Ehen ausgesprochen.

Das Strafgesetzbuch kriminalisiert und verbietet homosexuelle Handlungen im Allgemeinen und sieht ein Höchststrafmaß von 14 Jahren vor. Eine spezielle Klausel nennt explizit homosexuelle Handlungen unter Männern und sieht hier eine Höchststrafe von 21 Jahren vor. Im Februar 2010 wurden drei homosexuelle Männer im Küstenort Mtwapa von einer durch einen Bischof aufgehetzten Menge gejagt, misshandelt und beinahe umgebracht.

Im Turbi-Massaker, bei dem es um Viehdiebstahl, Weideland und Wasserrechte ging, waren 2005 bei Kämpfen zwischen verfeindeten Ethnien 90 Menschen ums Leben gekommen.

Am 10. April 2006 ereignete sich in der Nähe von Marsabit ein Flugzeugunglück, bei dem 14 Menschen ums Leben kamen. Drei Insassen überlebten den Absturz einer Harbin Yunshuji Y 12 II Turbo Panda (chinesische Weiterentwicklung der ukrainischen Antonow An-26), die bei stürmischem Wetter und Nebel in einen Hügel raste. Das Flugzeug brannte sofort völlig aus. Unter den 14 Personen befanden sich hochrangige Politiker. Es handelte sich hierbei um vier (Assistant) Minister, sechs Parlamentarier, einen anglikanischen Bischof und weitere Regierungsangestellte. Die Gruppe war auf dem Weg, um Friedensgespräche mit verfeindeten nomadischen Clans zu führen. Drei der verunglückten Politiker repräsentierten die dort angesiedelten und seit langem verfeindeten Clans. MP Dr. Bonaya Godana war ein Gabbra, MP Abdi Sasura ein Borana und Vize-Minister Titus Ngoyoni ein Rendille. Erst kurz zuvor waren sie übereingekommen, für den Frieden in der Region zusammenzuarbeiten. Da fast alle Toten im Flugzeug aus dieser Gegend stammten, verlor die Provinz mit einem Schlag einen Großteil ihrer politischen Elite.

Präsident Kibaki ordnete drei Tage Staatstrauer an; das Parlament unterbrach seine Arbeit für eine Woche. Die durch den Tod notwendigen fünf Nachwahlen brachten am 24. Juli 2006 in drei Fällen Verwandte der Getöteten ins Parlament. In Nakuru gewann William Kariuki Mirugi, der 28-jährige Sohn des verstorbenen Mirugi Kariuki. Er gehört der neuen – von Präsident Kibaki favorisierten – Partei NARC-K an.

Korruption kann in Kenia in allen Regierungsperioden der drei bisherigen Präsidenten Kenyatta, Moi und Kibaki beobachtet werden. Im Korruptions-Index ("Corruption Perceptions Index; CPI") von Transparency International rangiert Kenia unter 178 Ländern an 145. Stelle. Eine Schätzung besagt, dass der durchschnittliche kenianische Stadtbewohner 16 Mal pro Monat besticht, z. B. Polizisten an Straßensperren. Sicherlich sind die meisten dieser Bestechungsgelder klein und nicht immer im politischen Raum zu suchen. In die großen Korruptionsfälle, seien es Bestechungen, Verschwendungen oder das Abzweigen von Geldern bei völlig überteuerten Geschäften, waren nicht nur Geschäftsleute, sondern immer auch Regierungsstellen, teils in großem Ausmaß, verwickelt.

Zwei der bedeutendsten Korruptionsskandale waren der Goldenberg-Skandal, in dem Kenia Anfang der 1990er-Jahre 700 Millionen Euro durch gefälschte Gold- und Diamantenexporte verlor, sowie der Anglo-Leasing-Skandal.

Die aktuelle Politik ist stark mit der Auseinandersetzung um die Korruption beschäftigt. Zwei Körperschaften bekämpfen neben der Presse und den gesellschaftlichen Gruppierungen offiziell die Korruption:

Die "Kenya Defence Forces" (KDF) sind die Streitkräfte der Republik Kenia. Sie wurden im Jahre 1963, nach der Unabhängigkeit Kenias, aus den Teilen der King’s African Rifles gebildet. Gegenwärtig hat die KDF eine Personalstärke von 24.120 Mann, wobei mit 20.000 Soldaten der größte Anteil auf die Landstreitkräfte ("Kenya Army") entfällt. Die Hauptaufgabe der KDF ist die Grenzsicherung zum Nachbarland Somalia, wo sie seit 2007 ein Kontingent für AMISOM stellt.

Die größten Städte sind (Stand Zensus 2009):

Der Staat Kenia gliedert sich seit der Verfassungsreform 2010 in 47 Countys:

Bis dahin war Kenia in sieben Provinzen und einen Hauptstadt-Distrikt aufgeteilt:

Kenias Bruttosozialprodukt ist in den letzten Jahrzehnten im Vergleich zu anderen afrikanischen Staaten überdurchschnittlich gewachsen. Da auch das Bevölkerungswachstum überdurchschnittlich war, hat sich dies nicht in einer wesentlichen Verbesserung der Lebensverhältnisse der meisten Kenianer niedergeschlagen. Die Arbeitslosenquote lag 2008 bei etwa 40 %, die Inflationsrate bei etwa 26 %. Kenias BIP pro Kopf betrug 2016 1.512 US-Dollar, womit es weltweit auf Rang 147 von 188 Ländern lag. Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Kenia Platz 91 von 137 Ländern (Stand 2017/2018). Im Index der Wirtschaftlichen Freiheit belegte es 2017 Platz 135 von 180 Ländern.

Weit mehr als die Hälfte der Kenianer leben von der Landwirtschaft, doch sind nur etwa 20 Prozent der Fläche des Landes nutzbar. Der Rest ist wegen karger Böden oder zu geringen Niederschlägen meist Brach- oder Bergland. Angebaut werden neben Kaffee und Tee auch Sisal und Pyrethrum, das als Basis vieler Insektenbekämpfungsmittel dient.

Daneben erzeugen die Menschen hauptsächlich für den Eigenbedarf Mais, Weizen, Gerste, Zuckerrohr, Bohnen, Bananen, Reis, Ananas und Baumwolle.

Außerdem werden Rosen angebaut. Dies geschieht rund um den Naivashasee in großen Plantagen.

In der Viehwirtschaft sind die Mast- und Milchrinder vorherrschend. Die größeren Betriebe im Hochland Kenias haben einen guten Entwicklungsstand erreicht. Umfangreiche Rinder-, Schaf-, Ziegen- und sogar Kamel-Herden müssen mit den kargen Mitteln des Landes ernährt werden.

Viele Wälder stehen unter Naturschutz. Demgemäß sind die Bambuswälder für die Papierindustrie und die Rinde der Akazien (als Gerbstoff genutzt) im Freiland von eher untergeordneter Bedeutung.

Der besonders artenreiche Dakatcha-Tropenwald im Südosten des Landes wurde 2011 durch den geplanten Bau einer Jatropha-Plantage durch die Firma Kenya Jatropha Energy zur Gewinnung von Agrarenergie bedroht. Die großflächige Plantage hätte auch Auswirkungen auf die traditionelle Landwirtschaft der mehr als 20.000 in dem Gebiet lebenden indigenen Kleinbauern gehabt, weshalb das Vorhaben allgemein umstritten war und schließlich gestoppt wurde.

Kenia hat nur geringfügige Vorkommen an Bodenschätzen. Man gewinnt in nennenswerter Menge Natriumcarbonat (z. B. im Magadi-See) und Salz, daneben geringe Mengen an Gips, Blei, Gold, Silber, Kupfer, Asbest, Kalkstein, Graphit sowie Flussspat, Kieselgur und Seifenstein.

Kenia lebt vom Kaffee- und Tee-Export, von der Industrie (Maschinen- und Fahrzeugbau, Textil und Bekleidung, Ernährung und Genussmittel) und vom Tourismus (Nationalparks und Wildreservate). Der Handel mit Elfenbein und der Abschuss von Elefanten ist verboten, was größtenteils auf das Engagement von Richard Leakey zurückzuführen ist.

Gegen Ende der 1990er-Jahre konnte im gleichen Maße, wie der Kaffee an Bedeutung verlor, die Blumen-Industrie gewinnen. Kenia hat 2003 Israel als größten Blumenexporteur der Welt abgelöst und konnte 2005 seinen Marktanteil am europäischen Blumenmarkt auf 31 % ausdehnen.

Aus der Viehwirtschaft kommen Butter, Fleisch, Häute und Felle in den Export.

Kenia hat aufgrund mangelnder Produktivität ein hohes Handelsbilanzdefizit. 2016 waren die Importe mehr als doppel so hoch wie die Exporte. Wichtigste Handelspartner sind inzwischen Indien und China.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 17,85 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 12,89 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 7,2 % des BIP.

Die Staatsverschuldung betrug 2016 37,4 Mrd. US-Dollar oder 54,4 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Der Sektor der Mikrofinanz und Genossenschaftsbanken ist in Kenia stark segmentiert und unzusammenhängend. Laut einem Bericht der Weltbank von 2007 besteht er aus mehr als 5000 Institutionen. Viele davon, wie zum Beispiel der "Kenya Women Finance Trust (KWFT)" und die "National Association of Self-Employed Women of Kenya", sind speziell auf Frauen fokussiert. Den meisten dieser Institute fehlt das Kapital, um ihr Angebot zu diversifizieren, so dass sie sich auf einzelne Marktnischen beschränken.

Die San-Marco-Plattform ("San Marco Equatorial Range – SMER, Centro Spaziale Luigi Broglio") ist ein Raketenstartplatz vor der Küste Kenias zum Start von Feststoffraketen. Die San-Marco-Plattform besteht aus zwei ehemaligen Ölplattformen und zwei Versorgungsschiffen, die bei einer Wassertiefe von etwa 20 m verankert sind. Der nächste Ort an der Küste ist Malindi. Die Nähe zum Äquator ist vorteilhaft, da durch die höhere Rotationsgeschwindigkeit der Erde gegenüber äquatorferneren Raketenstartplätzen weniger Treibstoff zum Erreichen der Erdumlaufbahn benötigt wird.

Im Zeitraum von 1964 bis 1988 wurden 18 Höhenforschungsraketen und neun Scout-Trägerraketen gestartet. Z. B. wurde von der San-Marco-Plattform 1972 der Röntgensatellit "Uhuru" („Freiheit“ auf Swahili) mit einer Scout-Rakete in den Orbit gebracht. Am 16. Februar 1980 wurden von dieser Plattform zur Beobachtung einer totalen Sonnenfinsternis einige Höhenforschungsraketen gestartet.

Obwohl seit 1988, insbesondere durch die Einstellung der Scout-Produktion, keine Nutzung bekannt ist, wird der Startplatz in Startplänen russischer Feststoffraketen Start-1 oder dem europäischen Vega-Programm genannt, da die Zulassung der Plattformen bis 2014 galt.

Es gibt im kenianischen Verkehrsnetz insgesamt 3000 Kilometer Eisenbahnstrecken und fast 65.000 Kilometer Straßen; dort herrscht Linksverkehr. Seit Mai 2017 verkehren Züge auf der Neubaustrecke Mombasa–Nairobi. Diese Strecke wurde fast komplett von China finanziert.

Die Fluggesellschaft Kenya Airways beschäftigt fast 3000 Arbeitnehmer und unterhält ein internationales Liniennetz. In Kenia gibt es die zwei internationalen Flughäfen Nairobi und Mombasa, neben vielen kleinen Flugplätzen. Über 30 Fluglinien fliegen Nairobi an.

Zu den in Kenia verbreiteten Verkehrsmitteln zählen Matatus und Busse (z. B. Akamba, Easy Coach).

Mit Beginn des 21. Jahrhunderts hat die Zahl der Mobilfunkanschlüsse stark zugenommen. Es gibt etwa 19,4 Millionen Mobilfunkanschlüsse sowie etwa 664.000 Festnetzanschlüsse (Stand 2009). Durch die zunehmende Verbreitung von Internetcafés und der Nutzung des Mobilfunknetzes steht vielerorts Internet zur Verfügung. Knapp 45 % der Bevölkerung nutzten 2016 das Internet. Kenia verfügt über eine der am weitesten entwickeltensten digitalen Infrastrukturen des Afrikanischen Kontinents. Zahlungen werden in Kenia inzwischen häufig über das Bezahlsystem M-Pesa abgewickelt.

Im Jahr 2010 waren nur etwa 13 % der kenianischen Haushalte an das öffentliche Stromnetz angeschlossen. Die Regierung plant die Versorgung bis 2012 auf 22 %, bis 2022 auf 65 % und schließlich bis 2030 auf 100 % auszubauen. Dementsprechend wird auch mit einem starken Anstieg der Spitzenlast von etwa 1300 Megawatt im Jahr 2010, über 2500 MW im Jahr 2015 auf 15.000 MW im Jahr 2030 gerechnet.

Bis 2030 soll die installierte Leistung auf 19.200 MW ausgebaut werden, dabei sollen 5110 MW von Geothermie, 1039 MW von Wasserkraft, 2036 MW von Windenergie, 3615 MW von Öl, 2420 MW von Kohle, 2000 MW von Importen sowie 3000 MW von "sonstigen" Energieträgern gedeckt werden.

Kenia ist zu etwa 65 % auf Wasserkraft angewiesen. Lang anhaltende Trockenperioden seit 2009 und großflächige Abholzung der Wälder reduzierten jedoch die Verfügbarkeit der Wasserkraftwerke auf etwa 30 %, was den vermehrten Einsatz von Ölkraftwerken erforderlich macht und damit zu einer verstärkten Abhängigkeit von Importen führt.

Wegen der geringen Versorgungssicherheit der Wasserkraftwerke setzt Kenia verstärkt auf andere erneuerbare Energieträger.

Kenia errichtete 1981 als erstes afrikanisches Land ein Geothermiekraftwerk. Die im Besitz des staatlichen Energieerzeugers KenGen befindlichen Kraftwerke Olkaria I–III sowie das in Privatbesitz befindliche Kraftwerk Olkaria IV in der Rift-Valley-Provinz decken mit einer Gesamtleistung von 198 MW 13,2 % des kenianischen Strombedarfs – damit liegt Kenia im internationalen Spitzenfeld. Bis 2017 soll die Kapazität um 576 MW erweitert werden und rund 35 % des Gesamtbedarfs decken. Geologische Untersuchungen ergaben ein Potential für Geothermie zwischen 7000 und 10.000 MW verteilt auf 14 Lagerstätten.

Kenia plant die Ausweitung seiner Erdwärmekraftwerkskapazitäten auf 750 Megawatt.


In der Lake-Turkana-Region soll mit dem Windpark Lake Turkana bis 2016 der größte Windpark im subsaharischen Afrika mit einer Gesamtleistung von 310 MW entstehen. Die Bauarbeiten an dem 617 Millionen Euro teuren Projekt, in dem insgesamt 365 Windkraftanlagen zum Einsatz kommen werden, sollen im Juni 2014 beginnen. Nach der Fertigstellung sollen so jährlich bis zu 700.000 Tonnen CO-Emissionen durch den Wegfall von Ölkraftwerken vermieden werden können.

Im September 2010 gab Energieminister Patrick Nyoike bekannt, dass bis 2017 ein Atomkraftwerk mit einer Leistung von 1000 MW errichtet werden soll. Das Kraftwerk soll mit südkoreanischer Technologie errichtet werden und etwa 2,6 Milliarden Euro (3,5 Mrd. US-$) kosten.

Im Januar 2014 berichtete die britische Tageszeitung The Guardian, dass Kenia bis zum Jahr 2016 mehr als die Hälfte seiner Energieproduktion aus Solarenergie beziehen will. Das Investitionsvolumen bemisst sich auf knapp 885 Millionen Euro (1,2 Milliarden Dollar). Bis Ende des Jahres 2013 wurden in Kenia etwa 370 Millionen Euro (500 Millionen Dollar) in Solarprojekte investiert. Vom Ausbau der Solarenergie verspricht sich das Land sinkende Strompreise um bis zu 80 %.

In Kenia sind lediglich etwa 70 % der städtischen und nur 48 % der ländlichen Bevölkerung mit sauberem Trinkwasser versorgt. Die kenianischen Städte Mombasa und Nairobi werden mit Wasser aus den Mzima Springs versorgt. Aus der Quelle von Mzima Springs im größten Nationalpark Kenias sprudeln täglich 200 Millionen Liter Trinkwasser. Das Quellwasser stammt aus unterirdischen Gängen und wird durch das Lavagestein der Umgebung gefiltert.

In Kenia findet sich eine Vielzahl an unterschiedlichen Landschaften, die alle charakteristisch für den afrikanischen Kontinent sind. Schöne Küstengebiete und ein langes Korallenriff, weite Savannen mit Großwildtieren, schneebedeckte Gipfel, Wüste und ein kleiner Dschungel (Regenwald). Dies alles ist im Wesentlichen für den Tourismus erschlossen, sowohl was den Massentourismus – meist an der Küste – aber auch den Individualtourismus – eher im Landesinneren, z. B. bei der Besteigung des Mount Kenya – angeht. Tragendes Element des Tourismus sind neben den weißen Stränden an der Küste die großen Nationalparks (siehe oben).

Prähistorische Stätten, die besucht werden können, sind z. B. die Olorgesailie Prehistoric Site oder Kariandusi bei Gilgil. Viele Grabungsstätten der Paläoanthropologen, wie etwa des Orrorin tugenensis, können jedoch nicht besucht werden. Die anthropologische Forschung Kenias ist untrennbar mit dem Namen der Familie Leakey verbunden.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Kenia Platz 95 von 180 Ländern. Bei der Situation der Pressefreiheit im Land gibt es laut der Nichtregierungsorganisation "erkennbare Probleme".

"The Standard:" Im März 2006 kam es zu einem recht mysteriösen Überfall einer Spezialeinheit der Polizei auf die Tageszeitung "The Standard" und deren Fernsehsender KTN. Bei der Polizeiaktion liefen die üblichen Überwachungskameras weiter. Die Spezialeinheit hatte vergessen, die Kameras auszuschalten oder wenigstens die Videos zu beschlagnahmen. Diese Bilder wurden am nächsten Tag im Fernsehen gesendet und konnten weltweit von der Website des "Standard" heruntergeladen werden.

Die Attacke wurde von einer Schnellen Eingreiftruppe namens Kanga Squad durchgeführt, die von Offizieren der Geheimpolizei CID und der paramilitärischen Einheit General Service Unit (GSU) befehligt wurden. Die Kanga Squad waren mit russischen AK-47- und deutschen G3-Gewehren bewaffnet. Der Überfall erfolgte simultan auf die Büros des Standard und der KTN-Senderäume in Nairobi. Gleichzeitig wurden die Druckanlagen im Industriegebiet stillgelegt und Stapel frisch gedruckter Zeitungen in Brand gesetzt. In den Büros wurde Equipment wie Computer und Unterlagen beschlagnahmt. Die TV-Sendung wurde sofort unterbrochen.

Als spätere Begründung für die Attacke gab der Innenminister John Michuki einen Verdacht auf angebliche staatsfeindliche Aktivitäten der Zeitungsredaktion an. Die Zeitung hatte zuvor veröffentlicht, dass sich Kibaki mit Kalonzo Musyoka, einem seiner stärksten politischen Gegner, heimlich im Präsidentenpalast getroffen haben sollte. Die Opposition protestierte.

"Radio Hope:" Am 12. Mai 2006 ereignete sich ein ähnlicher Überfall auf Radio „Hope FM“, eine christliche Station der Nairobi Pentecostal Church, bei der bewaffnete Gangster die Senderäume stürmten, einen Wachmann erschossen und zwei weitere Menschen verletzten und anschließend die Räume in Brand setzten.

Das Media Council of Kenya (MCK) gründete daraufhin am 2. Juni 2006 unter der Leitung des altgedienten Journalisten Mitch Odero einen Presserat (Ethics Team), um alle Vorwürfe gegen die Presse selbst regeln zu können.


Die meisten der kenianischen Museen, die über das ganze Land verstreut liegen, werden von den National Museums of Kenya (NMK) verwaltet.

Weitere Museen werden von anderen Organisationen unterhalten, so das Railway Museum oder die National Archives.

Besonders Nairobi bietet häufige nationale oder internationale Musik-, Tanz- und Akrobatik-Shows, sei es in großen Hotels, Theaterspielstätten, Schulen oder Kulturzentren. Der Tradition verschrieben sind die "Gonda Traditional Entertainers", die Bomas of Kenya und die "Original Zengala Band".

Die bekanntesten Band aus Kenia sind die Safari Sound Band und Them Mushrooms, die beide traditionelle Lieder auf Swahili in modernen Fassungen aufgenommen haben. Sie gehören zu den Vertretern des Hotelpops, Bands die in Hotels für ausländische Touristen musizieren.

Kenia hat eine reiche Chormusikszene mit einer starken Betonung auf religiösen Gesang. Einer der bekanntestes Chöre ist der "Muungano National Choir" unter der Leitung von Boniface Mganga, der unter anderem auch die bekannte kongolesische Missa Luba singt.

Bekannte kenianische Sänger sind: Susan Awiyo, Merry Johnson, Alex und Merry Ominde, Kim4Love, Necessary Noize (Sängerin: Nazizi), Longombaz und Redsan, Juacali.

Das Theaterleben wird stark vom Schultheater geprägt. Jedes Jahr finden in den Distrikten, Provinzen und auf nationaler Ebene Wettbewerbe und Festivals statt. Die Stücke, die sich um das Genre des Volkstheaters bewegen – und thematisch z. B. häufig AIDS oder frühe Schwangerschaft aufgreifen – werden häufig im Fernsehen gezeigt.

Klassische Theaterspielstätten gibt es wenige, so das Kenya National Theater, welches jedoch kein festes Ensemble hat, sondern nationalen und internationalen Truppen und Show-Events Raum bietet.

Bekanntere Theatergruppen sind Heartstrings Ensemble, Mbalamwezi Players, Tufani, Hearts Ablaze und Winds of Change. Bekannte Schauspieler sind Winnifred Gitao, Angel Waruinge, Antony Kinuthia und Benta Stephanie Ochieng. Mumbi Kaugwa ist zudem Stückeschreiber und Regisseurin.

Die einzige feste Truppe mit eigener Spielstätte, meist Laien mit guter schauspielerischer Qualität, haben die "Phoenix Players Theatre Company". Der Theaterraum befindet sich im "Professional Centre" in der Innenstadt Nairobis. Die Truppe ist unter seinem Gründer James Falkland 1984 aus dem kolonialen und von der Familie Maule betriebenen "Donovan Maule Theater" hervorgegangen, das im modernen Kenia nicht mehr lebensfähig war. Die "Phoenix Players" haben sich mit einem speziellen Programm dem Thema Anti-HIV gewidmet. Sie leben von ihren Mitgliedern und Spendern, kämpfen aber trotzdem ständig um ihre Existenz.

Ansonsten bieten die Kulturzentren großer Nationen in Nairobi Theatergruppen und Säle, in denen periodisch Theater gespielt wird, so im French Cultural Centre, dem Alliance française oder im Goethe-Institut.

Aufgrund dieser Bedingungen hat sich eine kleine, aber lebhafte Theaterszene entwickelt. Autoren wie Cajetan Boy oder Jimmy Makotsi schreiben moderne authentische Stücke in Englisch oder Swahili.

Das Filmleben wird in den großen Städten vom US-amerikanischen Mainstream und von Bollywood (aufgrund der starken indischen Minderheit im Land) beherrscht. Gemeinsam organisieren das Alliance Française und das Goethe-Institut in Nairobi z. B. Kinderfilm-Festivals.

In Kenia werden relativ viele international ausgezeichnete Filme gedreht, z. B. "Jenseits von Afrika" (1986), "Nirgendwo in Afrika" (2001), "Die weiße Massai" (2004), "Der ewige Gärtner" (2005) oder "Afrika, mon amour" (2007).

Für das Fernsehen, die Kenya Broadcasting Corporation (KBC), werden auch Filme im Land produziert, so z. B. "Reflections" oder "Naliaka" von Brutus Serucho.

Die öffentlichen Feiertage wurden in der Verfassung von 2010 neu festgelegt. Darin wurden Feiertage wie der 10. Oktober ("Moi Day", zu Ehren des früheren Präsidenten Daniel arap Moi) gestrichen. Der 20. Oktober ("Kenyatta Day", zu Ehren des ersten Präsidenten Jomo Kenyatta) wurde umbenannt in "Mashujaa Day" – ein Gedenktag für alle Helden in der kenianischen Geschichte. "Idd ul-Azha" und "Diwali" gelten nur für Angehörige der jeweiligen Glaubensrichtung als öffentliche Feiertage. Sollte ein öffentlicher Feiertag auf einen Sonntag fallen, wird der nächste darauf folgende Werktag zum Feiertag.

Zu bestimmten Anlässen werden immer wieder einzelne Tage zu Feiertagen erklärt; diese gelten jedoch nur in dem entsprechenden Jahr. Zum Beispiel wurde anlässlich des Wahlsieges von US-Präsident Barack Obama der 6. November 2008 als "Obama Day" zum Feiertag erklärt.

Gemeinsam mit den afrikanischen Staaten Äthiopien und Marokko stellt Kenia regelmäßig die besten Langstreckenläufer der Welt, insbesondere in den olympischen Disziplinen 5000-Meter-Lauf, 10.000-Meter-Lauf, dem Hindernislauf und dem Marathonlauf. Bei den olympischen Spielen 2016 in Rio de Janeiro konnten kenianische Sportler zudem auch bei kürzeren Laufdistanzen (1500m, 800m und 400m-Hürden) sowie im Speerwurf der Männer Medaillen gewinnen. Während die Grundlage für den Langstreckenlauf meist schon in der Grundschule gelegt werden, weil die weiten Wege zur Schule gehend/laufend zurückgelegt werden, werden die besten Talente in gemeinsamen Trainingslagern entdeckt. Da die Einkommensverhältnisse in Kenia begrenzt sind, dient vor allem der Langstreckenlauf als eine Einkommensgelegenheit. Seit der Initiierung von Title IX haben sich auch amerikanische Hochschulsportprogramme darauf spezialisiert, weiblichen Läufernachwuchs in Kenia zu rekrutieren. Im Vergleich zum europäischen Trainingssystem wird in Kenia häufiger in großen Gruppen trainiert, wobei an Ort und Stelle eine Auswahl der besten erfolgt, weil (fast) jedes Training ein Ausscheidungsrennen ist. In der "Laufkultur" trifft sich die traditionelle Stammeskultur der Hirten der Savanne mit dem post-kolonialem Lebensgefühl der unbegrenzten Möglichkeiten. Genetik, Tradition, Höhentraining und ein internationaler Markt für Rennen im Dauerlauf begünstigen vor allem die Läuferinnen und Läufer aus dem Rift Valley. Zu den bekanntesten Athleten gehören u. a.: Kipchoge Keino, Ben Jipcho, Abel Kirui, Catherine Ndereba, Samuel Kamau Wanjiru, Wilson Kipsang, Eliud Kipchoge.

Der aktuell bekannteste Fußballspieler des Landes ist McDonald Mariga, der bei Inter Mailand spielte.




</doc>
<doc id="2912" url="https://de.wikipedia.org/wiki?curid=2912" title="Literatur">
Literatur

Literatur ist seit dem 19. Jahrhundert der Bereich aller mündlich (etwa durch Vers­formen und Rhythmus) oder schriftlich fixierten sprachlichen Zeugnisse. Man spricht in diesem „weiten“ Begriffsverständnis im Hinblick auf die hier gegebene schriftliche Fixierung etwa von „Fachliteratur“ oder, im Bereich der Musik, von „Notenliteratur“ (Partituren) bzw. ganz allgemein von „Literatur“ im Sinne der Gesamtheit oder von Teilen schriftlich notierter Musik.

Die öffentliche Literaturdiskussion und -analyse ist demgegenüber seit dem 19. Jahrhundert auf Werke ausgerichtet, denen besondere Bedeutung als Kunst zugesprochen werden kann, und die man im selben Moment von Trivialliteratur und ähnlichen Werken ohne vergleichbare „literarische“, sprich künstlerische Qualität, abgrenzt. Die Literatur zählt zu den Gattungen der Kunst.

Das Wort "Literatur" wurde bis in das 19. Jahrhundert hinein regulär für die Wissenschaften verwendet. Mit Literatur sind üblicherweise veröffentlichte Schriften gemeint. Die Gesamtheit der veröffentlichten Schriften eines Fachgebietes bzw. zu einer bestimmten Thematik oder Zielsetzung bildet ein "Schrifttum". Nur eingeschränkt und nicht über den Buchhandel zugängliche Publikationen werden als "graue Literatur" zusammengefasst.

Die heutige begriffliche Differenzierung, die im weitesten Sinne alle sprachliche Überlieferung umfasst und dabei ein enges Feld „literarischer“ Kunstwerke konstituiert, richtete sich erst im Laufe des 19. Jahrhunderts ein. Das Wort stand zuvor für Gelehrsamkeit, die Wissenschaften, die Produktion der res publica literaria und der frühmodernen scientific community, seltener auch lediglich für Schriften der griechischen und lateinischen Antike.

Die Neudefinition des Wortes geschah im Wesentlichen unter Einfluss neuer Literaturzeitschriften und ihnen folgender Literaturgeschichten, die zwischen 1730 und 1830 sich schrittweise den belles lettres, den schönen Wissenschaften öffneten, dem Bereich modischer und eleganter Bücher des internationalen Marktes und die dabei Werken der Poesie ein zentrales Interesse schenkten.

Es wurde im selben Prozess selbstverständlich, dass Literatur


Besprochen wird in den nationalen Philologien (wie der Germanistik, der Romanistik, der Anglistik), die die Ausgestaltung der nationalen Literaturen im 19. Jahrhundert im Wesentlichen vorantrieben, nahezu ausschließlich „hohe“ Literatur. Welche Werke unter welchen Gesichtspunkten besprochen werden, ist seitdem Gegenstand einer Debatte um die Bedeutung, die Werke in der jeweiligen Gesellschaft gewinnen. Der jeweilige „Kanon“ einer Nationalliteratur wird in der öffentlichen (und angreifbaren) Würdigung der „künstlerischen“ Qualität festgelegt, sowie in kontroversen Textinterpretationen der Fiktionen, die Titeln tiefere Bedeutung zusprechen. In der neuen Ausgestaltung übernahm die Literatur im 19. Jahrhundert in den westlichen säkularen Nationen Funktionen, die zuvor die Religionen und ihre Textgrundlagen als Debatten- und Bildungsgegenstände innehatten.

In neuerer Zeit wurde das Thema der digitalen Schriftlichkeit ein Diskussionsgebiet der Literaturwissenschaft und Medienwissenschaft. Gerade bei dieser Art von Literatur ist es nicht mehr möglich, nach Kriterien zu beurteilen, die man für Literatur vergangener Jahrhunderte entwickelt hatte. Siehe dazu: Digitale Schriftlichkeit.

Das Wort Literatur ist eine erst in der Frühmoderne in Mode kommende Ableitung des lateinischen "littera", der „Buchstabe“. Der Plural "litterae" gewann bereits in der Antike eigene Bedeutungen als „Geschriebenes“, „Dokumente“, „Briefe“, „Gelehrsamkeit“, „Wissenschaft(en)“. Im Französischen und Englischen blieb diese Bedeutung erhalten in "lettres" und "letters" als Synonym für „Wissenschaften“.

Das heutige Sprechen von Literatur entwickelte sich auf einem Umweg über das Deutsche und seine Äquivalente für die französische Wortfügung „belles lettres“. Im Laufe des 17. Jahrhunderts setzte sich die französische Wortkombination für einen neuen Bereich eleganter Bücher auf dem europäischen Markt durch. Die zeitgenössische deutsche Übersetzung war hierfür „galante Wissenschaften“, was dem Publikumsanspruch Rechnung trug wie dem modischen Geschmack: Leser beiderlei Geschlechts lasen diese Ware und bestanden darauf, dass sie eine ganze eigene Wissenschaft benötigte, keine akademische pedantische. Als mit dem frühen 18. Jahrhundert das Wort galant in Kritik geriet, setzte sich ein Sprechen von „schönen Wissenschaften“ durch, das im späten 18. Jahrhundert an Tragfähigkeit verlor, da es hier zunehmend um Poesie und Romane ging, eine unwissenschaftliche Materie. Das Sprechen von „schöner Literatur“ erlaubte es schließlich das engere im weiteren Begriffsfeld zu benennen. Man sprach ab Mitte des 18. Jahrhunderts von „Literatur“ mit der Option, jeweilige Schwerpunkte legen zu können. Mit dem Adjektiv „schöne“ wurde das Zentrum bezeichnet, das Literatur im engeren Sinn wurde. Je klarer das Zentrum definiert wurde, desto entbehrlicher wurde im 20. Jahrhundert die weitere Verwendung des Adjektivs.

Aus dem Wort „belles lettres“ ging im deutschen Buchhandel das Wort „Belletristik“ hervor, das heute eine Nachbarstellung einnimmt. Der Buchhandel führte die Verengung des Literaturbegriffs auf Dichtung der Nation, wie sie im 19. Jahrhundert geschah, am Ende nicht durch. Für Verlage ist der internationale Markt unterhaltender Titel ein unverzichtbares Geschäftsfeld. Man kann innerhalb der Belletristik ein kleineres Feld der Klassiker der Literatur abgrenzen und dieses wiederum international sortieren.

Das Wort Literatur hat seine zentrale Bedeutung in Literaturgeschichten, Literaturzeitschriften, in der Literaturkritik und Literaturtheorie. In all diesen Bereichen geht es deutlich darum, Kontroversen über Literatur zu erzeugen. Mit der Belletristik wird im Deutschen eher ein unkontroverses, uneingeschränktes Feld ohne eigene Geschichte beibehalten. Es gibt bezeichnenderweise keine „Belletristikgeschichte“, keine „Belletristikkritik“ und keine nationalen „Belletristiken“, dafür jedoch „Literaturgeschichte“, und „Literaturkritik“ wie „Nationalliteraturen“.

Der heutige Literaturbegriff spiegelt den Wortgebrauch der letzten zweihundert Jahre wider. Er zeichnet sich dabei gleichzeitig durch die Aufnahme einer Reihe historischer Kontroversen aus, die den modernen Streit darüber, welche Werke es verdienen sollten, als Literatur besprochen zu werden, fruchtbar in ihrer teilweisen Unvereinbarkeit bestimmen. Literaturstudenten wird seit dem 19. Jahrhundert die Beherrschung eines Handwerkszeugs der Textanalyse nach den verschiedenen Traditionen der Poetik, der Rhetorik, und der Textinterpretation abverlangt, die dem literarischen Text tiefere kulturelle Bedeutung beimessen soll. Moderne Schulen der Literaturtheorie nahmen hier einzelne Fragestellungen mit unterschiedlichen Schwerpunkten und divergierenden Wünschen an einen Kanon wichtigster Werke der jeweils zu schreibenden Literaturgeschichte auf.

Die Vorstellung, dass Literatur ein Bereich besonders schöner Texte sein sollte, ist Erbmasse der antiken und frühneuzeitlichen Poesie­diskussion. Der alternative Blick auf kunstvolle Sprachbeherrschung geht dagegen auf die Diskussion antiker Rhetorik zurück. Während sich die Rhetorik als weitgehend unkontroverse, zweckorientierte Kunst handhaben ließ, bestand über die Frage des Schönen in der Poesie ein langer Streit, der im 18. Jahrhundert im Wesentlichen als Kampf zwischen Regelpoetikern (Verfechtern einer nach Gesetzen schönen Poesie) und Verfechtern eines Geschmacksurteils geführt wurde. In der zweiten Hälfte des 18. Jahrhunderts setzte sich in Auflösung dieser Diskussion eine neue wissenschaftliche Debatte der Ästhetik durch, die – so die Hoffnung – am Ende in allen Bereichen der Kunst gelten würde als eine Konstante menschlicher Wahrnehmung, wie sie Schönheit auch in der Natur entdeckte.

Ende des 19. Jahrhunderts geriet der Blick auf die Ästhetik in grundsätzliche Kritik. Das hatte zum einen mit der kontroversen Begriffsaneignung durch die Ästhetizisten zu tun, zum anderen mit Kunstwerken, die sich provokant von der Konzentration auf Schönheit verabschiedeten und einen eigenen Realismus im Umgang mit sozialer Realität einklagten. Die schonungslose Anerkennung von Missständen sollte ein anerkanntes Ziel werden. Optionen im Umgang mit dem Konflikt bestanden in der Erweiterung der ästhetischen Konzepte wie in der Diskreditierung der Forderung eigener ästhetischer Wahrheit.

Dass Literatur sich im gegenwärtigen Begriff durch Fiktionalität und tiefere Bedeutung, eine Relevanz für die Gesellschaft, auszeichnet, ist im Wesentlichen Erbe der Roman­diskussion, die Mitte des 18. Jahrhunderts von der Literaturbesprechung aufgenommen wurde. Weder die Aristotelische Poetik noch die Nachfolgepoetiken der frühen Moderne hatten Poesie über Fiktionalität erklärt. Romane hatten sie samt und sonders nicht als Poesie anerkannt.

Der Vorschlag, Romane und womöglich Poesie generell über Fiktionalität zu definieren, findet sich erstmals klarer mit Pierre Daniel Huets Traktat über den Ursprung der Romane (1670) gemacht – als Möglichkeit, den theologischen Umgang mit Gleichnissen auf eine neue Lektüre von Romanen zu übertragen, bei dem es darum gehen soll, zu ermessen, welche kulturelle Bedeutung ein jeweiliger Titel hat.

Beim Aufbau des modernen Besprechungsgegenstands Literatur war die Frage nach tieferer Bedeutung Anfang des 19. Jahrhunderts praktisch, da sie dem Literaturwissenschaftler neue Tätigkeiten abverlangt, vor allem die der Interpretation. Daneben schuf sie neue Möglichkeiten, Texte zu bewerten und sich speziell diskutierbar rätselhaften, fremdartigen Titeln zuzuwenden und über sie die eigene Nation und Geschichte neu zu erklären. Im 19. und 20. Jahrhundert entfaltete die Frage nach der Bedeutung des Textes in der Kultur zudem politische Dynamik, da sich an sie Forderungen nach aktivem Engagement anschließen ließen.

Die Frage stilistischen Anspruchs ist im Wesentlichen Erbmasse der Diskussion neuester „belles lettres“. Poetiken waren davon ausgegangen, dass zwar einzelne Dichter die Kunst unterschiedlich handhabten, dass jedoch das Persönliche selbst nicht zu erstreben war. Schönheit galt es an sich anzustreben, der Künstler rang um die Schönheit. Mit der Romandiskussion wurde die Frage nach kulturellen Hintergründen akut, die Frage des individuellen Autors war dabei wenig das Ziel. Anders war die Debatte in der Belletristik verlaufen. In ihr stand gerade die Frage nach den Titeln im Vordergrund, die den aktuellen Geschmack am besten befriedigten. Es ging im selben Moment um die Frage nach neuen Autoren, die mit eigenen Sichtweisen den Geschmack prägten.

Die „belles lettres“ sollten insgesamt, so ihre Verfechter sich durch Stil auszeichnen – gegenüber den minderwertigen Volksbüchern wie gegenüber der pedantischen Wissenschaftlichkeit. Romane und Memoiren wurden wesentliche Felder der Produktion modernen persönlichen Stils. Die Diskussion jeweiliger Leistungen der individuellen Perspektive ging im frühen 19. Jahrhundert in der heutigen Literaturdiskussion auf – die Frage nach subjektiver Wahrnehmung der Realität, wie sie sich in Literatur abzeichne, prädestinierte den neuen Bereich, der im 19. Jahrhundert aufgebaut wurde, dazu, ein Debattenfeld im Schulunterricht zu werden. Im modernen Literaturunterricht geht es seitdem zentral darum, Schüler zu subjektiven Stellungnahmen zu Literatur zu bewegen, ihre Subjektivität dabei öffentlich wahrzunehmen, Subjektivität behandelter Autoren zu erfassen.

Im Lauf des 20. Jahrhunderts kam eine eigene, mutmaßlich neutrale, wissenschaftliche Analyse von Komplexität literarischer Werke auf. Auf sie richtete sich vor allem der Strukturalismus der 1960er und 1970er und ihm folgend der Poststrukturalismus der 1980er und 1990er aus. Betrachtet man die Untersuchungen mit historischer Perspektive, so nehmen sie aus allen Debattenfeldern Untersuchungsoptionen auf. Besondere Würdigung erhalten dabei Texte, die komplexer zu analysieren sind, die der Literaturbesprechung mehr Angriffsfläche der auszulotenden Kontexte geben.

Der hochrangige Text ist unter dieser Prämisse der, der reich an – womöglich divergierenden – Bedeutungsebenen ist, sich intensiv mit Traditionen auseinandersetzt, sich komplex auf andere Texte bezieht, erst im Blick auf diese besser verstanden wird. Die Analysen sind insofern wissenschaftlich objektiv, als sie tatsächlich die wissenschaftliche Analysierbarkeit als Eigenschaft von Texten erfassen, die sich dank ihrer Qualitäten in der wissenschaftlichen Analyse halten, uns nachhaltig als Literatur damit beschäftigen.

Hier lag, rückblickend betrachtet gleichzeitig die Option einer Mode von Texten, die sich auf die Literaturbetrachtung ausrichteten. Die Postmoderne ging in Entdeckungen des Trivialen am Ende zunehmend konfrontativ bis ablehnend mit den hier definierten Ansprüchen an Kunst der Literatur um.

Erst ab dem 19. Jahrhundert hat man zur Literatur nicht nur das Wissenschaftliche gezählt, sondern alles, was schriftlich niedergelegt war. Ab dem Jahrhundert unterschied man auch zwischen hoher Literatur, sprich Hochliteratur, und Literatur von wenig künstlerischer Qualität, sprich Trivialliteratur.

Der Prozess, in dem im späten 18. und frühen 19. Jahrhundert Dramen, Romane und Gedichte zu „Literatur“ gemacht wurden (sie hingen vorher unter keinem Wort zusammen), muss unter unterschiedlichen Perspektiven gesehen werden. Ganz verschiedene Interessen waren daran beteiligt, die „Literatur“ zum breiten Debattenfeld zu machen. Auf eine einprägsame Formel gebracht, engten die Teilnehmer der Literaturdebatte ihre Diskussion ein und weiteten ihre Debatte damit aus: Seit Jahrhunderten hatten sie erfolgreich wissenschaftliche Schriften als „Literatur“ diskutiert – Poesie und Fiktionen interessierten sie dabei vor 1750 nur am Rande. In der zweiten Hälfte des 18. Jahrhunderts rückten sie ausgewählte Felder des populären Randgebiets in das Zentrum ihrer Rezensionen mit dem Effekt, dass ihre eigene Diskussion sich nun mit den freier besprechbaren Gegenständen ausweitete. Die Gründung der universitär verankerten Literaturwissenschaft festigte im 19. Jahrhundert den Prozess dieser Einengung des Debattenfeldes (auf Dramen, Romane und Gedichte) sowie die Ausdehnung der Diskussion selbst (vor allem auf die staatlichen Schulen und die öffentlichen Medien).

Das Wort „Literatur“ gilt heute zwar nicht mehr demselben Gegenstand wie vor 1750, es blieb jedoch kontinuierlich das Wort des sekundären Austauschs über Literatur. Es findet sich auf Titelseiten von Literaturzeitschriften, in den Bezeichnungen von Lehrstühlen und universitären Seminaren der Literaturwissenschaft, in den Titeln von Literaturgeschichten, in Wortfügungen wie „Literaturpapst“, „Literaturkritiker“, „Literaturhaus“, „Literaturpreis“. Das Wort „Literatur“ ist dabei (anders als Worte wie „Hammer“, die keine Debattengegenstände bezeichnen) vor allem ein Wort des Streits und der Frage: „Was soll eigentlich als Literatur Anerkennung finden?“ Es gibt eine Literaturdiskussion, und sie legt auf der Suche nach neuen Themen, neuer Literatur und neuen Literaturdefinitionen fortwährend neu fest, was gerade für „Literatur“ erachtet wird. Sie tat dies in den letzten 300 Jahren mit solchem Wandel ihres Interesses, dass man für das Wort „Literatur“ eben durchaus keine stabile inhaltliche Definition geben kann.

Das große Thema des Austauschs über Literatur waren bis weit ins 18. Jahrhundert hinein die Wissenschaften. In der Praxis des Besprechungswesens reduzierte sich der Blick der Literaturrezensenten dabei auf neueste Publikationen, auf Schriften – ein Austausch, der zunehmend Leser außerhalb der Wissenschaften ansprach: Wissenschaftliche Journale erschienen in der zweiten Hälfte des 17. Jahrhunderts mit spannenden Themen in den Niederlanden auf Französisch. Englische kamen hinzu, deutsche boomten zwischen 1700 und 1730 im Geschäft, das die Universitäten Leipzigs, Halles und Jenas bestimmten. Der Reiz der wissenschaftlichen Journale war ihre Diskussionsfreudigkeit, ihre Offenheit für politische Themen, die Präsenz, die hier einzelne Literaturkritiker mit eigenen, sehr persönlich geführten Journalen (im deutschen etwa den "Gundlingiana" des Nikolaus Hieronymus Gundling) entwickelten.

Zwischen 1730 und 1770 wandten sich deutsche literarische Journale bahnbrechend der nationalen Dichtung zu – im territorial und konfessionell zersplitterten Sprachraum war die Poesie der Nation ein Thema, das sich überregional und mit größten Freiheiten behandeln ließ. Die Gelehrsamkeit (die „res publica literaria“) gewann mit Rezensionen der „belles lettres“, der „schönen Wissenschaften“, der „schönen Literatur“ (so die Dachbegriffe, die man wählte, um diese Werke ungeniert in wissenschaftlichen Zeitschriften ansprechen zu können), ein wachsendes Publikum. Aus dem modischen Ausnahmefall des Rezensionswesens wurde im Verlauf des 18. Jahrhunderts der Regelfall.

Zu Beginn des 19. Jahrhunderts musste im Deutschen das Wort „Literatur“ neu definiert werden. „Literatur“ war (hielt man sich vor Augen, was da besprochen wurde) definitiv nicht der Wissenschaftsbetrieb, sondern eine textliche Produktion mit zentralen Feldern in der künstlerischen Produktion. „Literatur“ wurde in der neuen Definition:


Nach der neuen Definition war davon auszugehen, dass sich die Literatur in nationalen Traditionssträngen entwickelte: Wenn sie im Kern sprachliche Überlieferung war, dann mussten die Sprachen und die politisch definierten Sprachräume den einzelnen Überlieferungen Grenzen setzen – Grenzen, über die nur ein Kulturaustausch hinweghelfen kann. Ein Sprechen von „Literaturen“ im Plural entfaltete sich. Für die Nationalliteraturen wurden die nationalen Philologien zuständig. Eine eigene Wissenschaft der Komparatistik untersucht die Literaturen heute in Vergleichen.

Die Definition von Literatur als „Gesamt der sprachlichen und schriftlichen Überlieferung“ erlaubt es den verschiedenen Wissenschaften, weiterhin in „Literaturverzeichnissen“ ihre eigenen Arbeiten als „Literatur“ zu listen (Fachliteratur). Die Definition im „engen Sinn“ ist dagegen gezielt "arbiträr" und "zirkulär" angelegt. Es blieb und bleibt darüber zu streiten, welche Werke als „künstlerische“ Leistungen anzuerkennen sind.

Das, was Literatur werden sollte, hatte vor 1750 weder einen eigenen Oberbegriff noch größere Marktbedeutung. Poesie und Romane mussten erst unter eine einheitliche Diskussion gebracht werden, wobei gleichzeitig große Bereiche der Poesie- wie der Romanproduktion aus der Literaturdiskussion herausgehalten werden mussten, wenn diese ihr kritisches Gewicht bewahren wollte.

Der Prozess, in dem ausgewählte Dramen, Romane und Gedichte „Literatur“ wurden, fand dabei in einem größeren statt: Seit dem 17. Jahrhundert gab es auf dem Buchmarkt die „belles lettres“ (englisch vor 1750 oft mit „polite literature“ übersetzt, deutsch mit „galante Wissenschaften“ und ab 1750 „schöne Wissenschaften“). Dieses Feld besteht heute im Deutschen mit der Belletristik fort.

Die „belles lettres“ waren im 17. Jahrhundert unter den „lettres“, den Wissenschaften, für das Besprechungswesen ein unterhaltsamer Randbereich. Sie erwiesen sich im Lauf des 18. Jahrhunderts als popularisierbares Besprechungsfeld. Ihnen fehlten jedoch entscheidende Voraussetzungen, um staatlichen Schutz erlangen zu können: Die „belles lettres“ waren und sind international und modisch (man kann von „nationalen Literaturen“ sprechen, nicht aber von „nationalen Belletristiken“), sie umfassten Memoires, Reiseberichte, politischen Klatsch, elegante Skandalpublikationen genauso wie Klassiker der antiken Dichter in neuen Übersetzungen (ihnen fehlt mit anderen Worten jede Ausrichtung auf eine Qualitätsdiskussion; man liest die mit Geschmack, es gibt „Literaturkritiker“, aber keine „Belletristikkritiker“). Die Belletristik war und ist vor allem aktuell und das selbst in ihren Klassikern (es gibt keine „Belletristikgeschichte“, wohl aber „Literaturgeschichte“) – das sind die wesentlichen Unterschiede zwischen Belletristik und Literatur, die aufzeigen, wie die Belletristik umgeformt werden musste, um die Literatur im heutigen Sinn zu schaffen.

Staatliches Interesse – Achtung, mit der sie zum Unterrichtsgegenstand werden konnte – gewann die Belletristik durch die Einrichtung einer nationalen Debatte, in der es um hohe Kunst der nationalen Dichter ging. Romane, Dramen und Gedichte wurden in der Einrichtung dieser Diskussion zum zentralen Feld der „belles lettres“, zu „schöner Literatur“, dem Kernbereich der literarischen Produktion.

Der Bereich der „belles lettres“ war vor 1750 klein, aber virulent. Unter 1500–3000 Titeln der jährlichen Gesamtproduktion, die um 1700 in den einzelnen großen Sprachen Französisch, Englisch und Deutsch auf den Markt kam, machten die „belles lettres“ pro Jahr 200–500 Titel aus; 20-50 Romane waren etwa dabei. Der Großteil der Buchproduktion entfiel auf die Bereiche wissenschaftliche Literatur und religiöse Textproduktion von Gebetbüchern bis hoch zu theologischer Fachwissenschaft, sowie, wachsend: auf die politische Auseinandersetzung. Zu den Marktentwicklungen eingehender das Stichwort Buchangebot (Geschichte).

Die Literaturkritik, die Kritik der Wissenschaften, ließ sich zwischen 1730 und 1770 gezielt auf die skandalösesten Bereiche des kleinen belletristischen Marktes ein. Dort, wo es die skandalöse Oper und den ebenso skandalösen Roman gab, musste (so die Forderung der Kritiker) in nationalem Interesse Besseres entstehen. Mit größtem Einfluss agierte hier die deutsche Gelehrsamkeit. Die Tragödie in Versen wurde das erste Projekt des neuen, sich der Poesie zuwendenden wissenschaftlichen Rezensionswesens. Frankreich und England hätten eine solche Tragödie zum Ruhm der eigenen Nation, führte Johann Christoph Gottsched in seiner Vorrede zum "Sterbenden Cato", 1731 aus, die den Ruf nach jener neuen deutschen Poesie begründete, aus der am Ende die neue hohe deutsche Nationalliteratur wurde. Die Attacke richtete sich (auch wenn Gottsched das nur in Nebensätzen klarstellte, und ansonsten das Theater der Wandertruppen angriff) gegen die Oper, die in der Poesie den Ton angab. Die Oper mochte Musik sein. Die neue, der Oper ferne Tragödie würde, so versprach es Gottsched, auf Aufmerksamkeit (und damit Werbung) des kritischen Rezensionswesens hoffen können, falls sie sich an die poetischen Regeln hielt, die Aristoteles formuliert hatte.

Die Rückkehr zur aristotelischen Poetik blieb ein Desiderat der „Gottschedianer“. Mit dem bürgerlichen Trauerspiel gewann Mitte des 18. Jahrhunderts ein ganz anderes Drama – eines in Prosa, das bürgerliche Helden tragödienfähig machte – die Aufmerksamkeit der Literaturkritik. Der Roman, der mit Samuel Richardsons "Pamela, or Virtue Rewarded" (1740) dem neuen Drama die wichtigsten Vorgaben gemacht hatte, fand im selben Moment das Interesse der Literaturrezension. War der Roman bis dahin eher Teil der dubiosen Historien als Poesie, so wurde nun die Poesiedefinition für den Roman geöffnet, so wie sie gegenüber der Oper, dem Ballett, der Kantate und dem Oratorium verschlossen wurde.

Der neue Poesiebegriff gab dem Fiktionalen und seiner diskutierbaren Bedeutung größeren Raum als Regeln und Konventionen. Die Diskutierbarkeit von Poesie nahm damit zu. Sie steigerte sich weiter damit, dass das Besprechungswesen zum nationalen Wettstreit der Dichter aufrief.

Die poetischen Werke, die mit den 1730ern geschaffen wurden, um von der Literaturkritik besprochen zu werden, verdrängten nicht die bestehende belletristische Produktion. Der gesamte Markt der Belletristik wuchs in der zweiten Hälfte des 18. Jahrhunderts zum Massenmarkt. Die neue, auf die Besprechung zielende Produktion versetzte jedoch die öffentliche Literaturkritik in die Lage, nach Belieben bestimmen zu können, was öffentlicher Beachtung wert sein sollte und was nicht. Das Besprechungswesen sorgte mit seiner Entscheidungsgewalt über das Medienecho für eine Ausdifferenzierung des belletristischen Sektors und für eine Entskandalisierung der Öffentlichkeit:


Für die öffentliche Auseinandersetzung bedeutete die neue Differenzierung eine Wohltat. Im frühen 18. Jahrhundert hatte man Romane, die hochrangigen Politikern Sexskandale andichteten, in wissenschaftlichen Journalen besprochen, falls die politische Bedeutung das erforderte. Man hatte die Informationen schlicht als „curieus“ gehandelt (siehe etwa die Rezension der "Atalantis" Delarivier Manleys in den "Deutschen Acta Eruditorum" von 1713). Kein Gespür für die Niedrigkeit der Debatte bestand da – man ging vielmehr davon aus, dass sich solche Informationen nicht anders verbreiten ließen, als in skandalösen Romanen. Mitte des 18. Jahrhunderts – die neue Mode der Empfindsamkeit kam in diesem Geschehen auf – konnte man das „Niedere“ zwar nicht vom Buchmarkt verbannen, aber eben aus der Diskussion nehmen. Es mochte einen skandalösen Journalismus beschäftigen, der eines Tages eine eigene Boulevardpresse entwickelte, nicht aber die gehobenen Debatten der Literatur.

Die Literaturdebatte entwickelte auf dem Weg der von ihr angestrebten Marktreform eine besondere Suche nach Verantwortung für die Gesellschaft – und für die Kunst. Sie fragte nach den Autoren dort, wo der Markt bislang weitgehend unbeachtet und anonym florierte. Sie löste Pseudonyme auf und nannte die Autoren gezielt bei ihren bürgerlichen Namen (das war im 17. und 18. Jahrhundert durchaus unüblich, man sprach vor 1750 von „Menantes“ nicht von „Christian Friedrich Hunold“). Die neue Literaturwissenschaft diskutierte, welche Stellung die Autoren in der Nationalliteratur gewannen und legte damit das höhere Ziel der Verantwortung fest. Sie schuf schließlich besondere Fachdiskussionen wie die psychologische Interpretation, um selbst das noch zu erfassen, was die Autoren nur unbewusst in ihre Texte gebracht hatten, doch eben nicht weniger in der literaturwissenschaftlichen Perspektive verantworteten. Rechtliche Regelungen des Autor­status und des Urheberschutzes gaben demselben Prozess eine zweite Seite.

Geschichten der deutschen Literatur offenbaren die Einschnitte des hier knapp skizzierten Geschehens, sobald man die besprochenen Werke auf der Zeitachse verteilt: Mit den 1730ern beginnt eine kontinuierliche und wachsende Produktion „deutscher Dichtung“. Die Diskussionen, die seit 1730 geführt wurden, schlagen sich in Wellen von Werken nieder, die in diesen Diskussionen eine Rolle spielten. Vor 1730 liegt dagegen eine Lücke von 40 Jahren – die Lücke des belletristischen Marktes, dem die Gründungsväter der heutigen nationalen Literaturdiskussion als „Niedrigem“ und „Unwürdigen“ ihre Betrachtungen verweigerten. Mit dem „Mittelalter“, der „Renaissance“ und dem „Barock“ schuf die Literaturgeschichtsschreibung des 18. und 19. Jahrhunderts für die Vergangenheit nationale Großepochen, die der Literatur, wie sie heute erscheint, eine (lückenhafte, nachträglich produzierte) Entwicklung geben.

Der Streit in der Frage „Was ist Literatur?“, der mit dem 19. Jahrhundert aufkam, und der nach wie vor die Literaturwissenschaft beschäftigt, ist kein Beweis dafür, dass die Literaturwissenschaft nicht einmal dies zuwege brachte: ihren Forschungsgegenstand klar zu definieren. Die Literaturwissenschaft wurde selbst die Anbieterin dieses Streits. Darüber, was Literatur sein soll und wie man sie adäquat betrachtet, muss tatsächlich gesellschaftsweit gestritten werden, wenn Literatur – Dramen, Romane und Gedichte – im Schulunterricht, in universitären Seminaren, im öffentlichen Kulturleben als geistige Leistung der Nation gewürdigt wird. Jede Interessengruppe, die hier nicht eigene Perspektiven und besondere Diskussionen einklagt, verabschiedet sich aus einer der wichtigsten Debatten der modernen Gesellschaft.

Nach dem Vorbild der Literatur (als dem sprachlich fixierten nationalen Diskursgegenstand) wurden mit der Wende ins 19. Jahrhundert die internationaler verfassten Felder der bildenden Kunst und der ernsten Musik definiert – Felder, die zu parallelen Marktdifferenzierungen führten: Auch hier entstanden „hohe“ gegenüber „niedrigen“ Gefilden: Die hohen sollten überall dort liegen, wo gesellschaftsweite Beachtung mit Recht eingefordert wird. Der Kitsch und die Unterhaltungsmusik („U-Musik“ im Gegensatz zur „E-Musik“) konnten im selben Moment als aller Beachtung unwürdige Produktionen abgetan werden. Die Literaturdebatte muss von allen Gruppen der Gesellschaft als Teil der größeren Debatte über die Kultur und die Kunst der Nation aufmerksam beobachtet werden: Sie nimmt mehr als andere Debatten Themen der Gesellschaft auf und sie gibt Themen an benachbarte Diskussionen weiter.

Dass sie zum Streit Anlass gibt, ist das Erfolgsgeheimnis der Literaturdefinition des 19. Jahrhunderts: Literatur sollen die Sprachwerke sein, die die Menschheit besonders beschäftigen – das ist zirkulär und arbiträr definiert. Es liegt im selben Moment in der Hand aller, die über Literatur sprechen, festzulegen, was Literatur ist.

Ordnung und Fixierung gewann die Literaturdebatte nicht mit der Begriffsdefinition „Literatur“, an der sich der Streit entzündet, sondern mit den Traditionen ihres eigenen Austauschs. Was als Literatur betrachtet werden will, muss sich für einen bestimmten Umgang mit literarischen Werken eignen. Die Literatur entwickelte sich im 19. Jahrhundert zur weltlichen Alternative gegenüber den Texten der Religion, die bislang die großen Debatten der Gesellschaft einforderten. Die Literaturwissenschaft drang mit ihrem Debattengegenstand – Dramen, Romane und Gedichte – in die Lücke, die die Theologie mit der Säkularisierung zu Beginn des 19. Jahrhunderts ließ. Dabei bewährten sich bestimmte Gattungen, die „literarischen“, besser als andere –


Das Material, das im Lauf des 18. Jahrhunderts zu Literatur gemacht wurde, war zuvor nur im Ausnahmefall von Literaturzeitschriften (wissenschaftlichen Rezensionsorganen) besprochen worden. Der Austausch über Poesie und Fiktionen, über Dramen, Opern und Romane geschah vor 1750 vor allem in den Theatern und in den Romanen selbst. In den Theatern stritten die Fans über die besten Dramen und Opern. Man veranstaltete in London Wettkämpfe, bei denen man Themen ausschrieb und die beste Oper prämierte. Im Roman attackierten Autoren einander unter Pseudonymen mit der beliebten Drohung, den Rivalen mit seinem wahren Namen auffliegen zu lassen. Hier griff der sekundäre Diskurs der Literaturkritik um 1750 mit neuen Debattenangeboten ein.

Die Literaturdiskussion selbst war zuerst eine rein wissenschaftsinterne Angelegenheit gewesen: Als im 17. Jahrhundert Literaturzeitschriften aufkamen, besprachen in ihnen Wissenschaftler die Arbeiten anderer Wissenschaftler. Das Publikum dieses Streits weitete sich aus, dadurch, dass die Literaturzeitschriften Themen von öffentlichem Interesse intelligent ansprachen und da die Rezensenten sich auf das breitere Publikum mit neuen Besprechungen der „belles lettres“ einließen. Wenn die Wissenschaften Dichter besprachen, gewann ihre Debatte eine ganz neue Freiheit: Fachintern, doch vor den Augen der wachsenden Öffentlichkeit besprach man hier Autoren, die außerhalb der eigenen Debatte standen. Man konnte mit ihnen weit kritischer umgehen als mit den Kollegen, die man bislang im Zentrum rezensierte.

In dem Maße, in dem die Wissenschaften ihren ersten Besprechungsgegenstand (ihre eigene Arbeit) zugunsten des neuen (Poesie der Nation) erweiterten, öffneten sie die Literaturdebatte der Gesellschaft. Die Literaturdiskussion florierte fortan nicht mehr als vor allem internes Geschäft; sie agierte in ihrem Streit zugleich gegenüber zwei externen Teilnehmern: dem Publikum, das die Literaturdebatte verfolgt und vieldiskutierte Titel mit der Bereitschaft kauft, die Diskussionen fortzusetzen und gegenüber den Autoren, die nun als die Verfasser von „Primärliteratur“ dem „sekundären Diskurs“ beliebig distanziert gegenüberstehen können.

Der Austausch gewann an Komplexität, als im 19. Jahrhundert die Nation ein eigenes Interesse an der neuformulierten Literatur entwickelte. Die Nationalliteratur ließ sich an Universitäten und Schulen zum Unterrichtsgegenstand machen. Der Nationalstaat bot der Literaturwissenschaft eigene Institutionalisierung an: Lehrstühle an Universitäten. Die nationalen Philologien wurden eingerichtet. Literaturwissenschaftler wurden berufen, um für Kultusministerien die Lehrpläne zu erstellen, nach denen an den Schulen Literatur zu besprechen ist; sie bilden die Lehrer aus, die Literatur bis in die unteren Schulklassen hinab diskutieren.

Die Verlagswelt stellte sich auf den neuen Austausch ein. Kommt ein neuer Roman auf den Markt, schickt sie komplett vorgefasste Rezensionen mit Hinweisen auf die Debatten, die dieser Roman entfachen wird, an die Feuilleton-Redaktionen der wichtigsten Zeitungen, Zeitschriften und Fernsehsender.

Die Autoren veränderten ihre Arbeit. Mit den 1750ern kamen ganz neue Dramen und Romane auf: schwergewichtige, schwerverständliche, die gesellschaftsweite Diskussionen entfachen müssen. Romane und Dramen wurden in ganz neuem Maße „anspruchsvoll“ – Anspruch auf öffentliche Würdigung ist das neue Thema. Um mehr Gewicht auf Debatten zu gewinnen, wurde es unter den Autoren Mode, Dramen, Romane und Gedichte in epochalen Strömungen zu verfassen, Schulen zu gründen, die einen bestimmten Stil, eine bestimmte Schreibweise (die „realistische“, die „naturalistische“ etc.), eine bestimmte Kunsttheorie (die des „Surrealismus“, die des „Expressionismus“) verfochten. Autoren, die sich auf eine solche Weise verorten, werden, wenn die Aktion gelingt, als bahnbrechende besprochen, wenn sie zu spät auf den falschen Zug aufspringen, werden sie von der Kritik als „Epigonen“ gebrandmarkt. Dieses gesamte Spiel kennt kein Pendant vor 1750. Die meisten Stilrichtungen, die wir (wie das „Barock“ und „Rokoko“) vor 1750 ausmachen, sind erst später geschaffene Konstrukte, mit denen wir den Eindruck erwecken, dass Literatur schon immer Debatten fand, wie sie sie seit dem 19. Jahrhundert findet.

Die Autoren organisierten sich in Assoziationen wie dem P.E.N.-Club international. Sie formierten Gruppen wie die „Gruppe 47“ und Strömungen. Mit Manifesten begannen sie, dem sekundären Diskurs Vorgaben zu machen. Im Einzelfall ließen sie sich auf Fehden mit Literaturpäpsten ein, um auf direktestem Weg die Literaturdiskussion auf sich zu ziehen. Autoren nehmen Literaturpreise an oder schlagen sie, wie Jean-Paul Sartre den ihm verliehenen Nobelpreis für Literatur, im öffentlichen Affront aus. Sie halten Dichterlesungen in Buchhandlungen – undenkbar wäre das im frühen 18. Jahrhundert gewesen. Sie begeben sich in den „Widerstand“ gegen politische Systeme, sie schreiben Exilliteratur aus der Emigration heraus.

Mit all diesen Interaktionsformen gewann der Austausch über Literatur eine Bedeutung, die der Austausch über die Religion kaum hatte (geschweige denn der Austausch über Literatur im alten Wortsinn oder derjenige über Poesie und Romane, wie er vor 1750 bestand).

Das brachte eigene Gefahren mit sich. Die Literaturwissenschaft und der von ihr ausgebildete freiere Bereich der Literaturkritik in den Medien sind erheblichen Einflussnahmen der Gesellschaft ausgesetzt. Die Gesellschaft klagt neue Debatten ein, fordert neue politische Orientierungen, erzwingt von der Literaturkritik Widerstand oder Anpassung. Es gibt in der pluralistischen Gesellschaft in der Folge eine feministische Literaturwissenschaft wie eine marxistische, oder (scheinbar unpolitischer) eine strukturalistische und so fort. Eine Gleichschaltung der Gesellschaft, wie sie das Dritte Reich durchführte, greift konsequenterweise gezielt zuerst in den Literaturbetrieb ein. Die institutionalisierte Literaturwissenschaft lässt sich sehr schnell gleichschalten, Lehrstühle werden neu besetzt, Lehrpläne bereinigt, Literaturpreise unter neuen Richtlinien vergeben. Die Gleichschaltung der Verlagswelt und der Autorenschaft ist die schwierigere Aufgabe der Literaturpolitik, der totalitäre Staaten zur Kontrolle der in ihnen geführten Debatten große Aufmerksamkeit schenken müssen.

Warum die Nation überhaupt ein solches Interesse am pluralistischen und jederzeit kritischen Gegenstand „Literatur“ und den Debatten nationaler „Kunst“ und „Kultur“ entwickelte:

Europas Nationen antworteten mit der Einführung nationalstaatlicher Bildungssysteme und der allgemeinen Schulpflicht – durchaus auch – auf die Französische Revolution. Wer aufsteigen wollte, sollte, so das Versprechen, das jede weitere Revolution erübrigen musste, es in der Nation beliebig weit bringen können – vorausgesetzt, er nutzte die ihm angebotenen Bildungschancen. In der Praxis blieben Kinder unterer Schichten bei aller Chancengleichheit finanziell benachteiligt. Weit schwerer wog für sie jedoch, was sie an Erfahrungen frühzeitig in all den Schulfächern machten, in denen die neuen Themen angesagt waren: Wer in der Gesellschaft aufsteigen wollte, würde seinen Geschmack anpassen müssen. Er würde sich ausschließlich für hohe Literatur, bildende Kunst und ernste Musik begeistern müssen und am Ende mit seinen nächsten Angehörigen keine Themen mehr teilen, ihre Zeitungen verachten wie ihre Nachrichten. Die Frage war nicht, ob man aufsteigen konnte. Die Frage war, ob man bei diesen Aussichten aufsteigen wollte? Erst das ausgehende 20. Jahrhundert brachte hier eine größere Nivellierung der „Kulturen“ innerhalb der Gesellschaft – nicht wie in der linken politischen Theorie gedacht durch eine Erziehung, die Arbeiterkinder an die hohe Kultur heranführte, sondern durch neue Moden der Postmoderne, in denen „niedere“ Kultur, „Trash“, plötzlich „Kultstatus“ gewann.

Der Verlierer im Kampf um gesellschaftliche Diskussionen und Aufmerksamkeit scheint bei alledem die Religion gewesen zu sein. Die Literatur ist gerade an dieser Stelle eine interessant offene Konstruktion. Die Texte der Religion können dort, wo man Literatur diskutiert, jederzeit als die „zentralen Texte der gesamten sprachlichen Überlieferung“ eingestuft werden. Aus der Sicht der Literaturwissenschaft liegen die Texte der Religion nicht "außerhalb", sondern mitten "im" kulturellen Leben der Nation. Die Texte der Religion stehen zur Literatur als dem großen Bereich aller textlichen (nach Nationen geordneten) Überlieferung nahezu so ähnlich wie die Religionen selbst zu den Staaten, in denen sie agieren. Es ist dies der tiefere Grund, warum sich das Konzept der Literatur, wie es heute die Literaturwissenschaft beschäftigt, weitgehend ohne auf Widerstand zu stoßen, weltweit ausdehnen ließ.

Die moderne Literaturdebatte folgt vor allem deutschen und französischen Konzepten des 18. und 19. Jahrhunderts. Deutsche Journale wie Lessings "Briefe die Neueste Literatur betreffend" wandten sich früh dem neuen Gegenstand zu. Sie taten dies gerade im Verweis auf ein nationales Defizit. Mit der Französischen Revolution erreichte Frankreich das Interesse an einem säkularen textbasierten Bildungsgegenstand.

Wer sich durch die englische Publizistik des 19. Jahrhunderts liest, wird dagegen feststellen, dass das Wort „Literatur“ hier noch bis Ende des 19. Jahrhunderts synonym für die Gelehrsamkeit stehen konnte. An Themen des nationalen Austauschs fehlte es in Großbritannien nicht – die Politik und die Religion lieferten sie zur freier Teilnahme an allen Diskussionen. Die Nation, die die Kirche im 16. Jahrhundert dem Staatsgefüge einverleibt hatte, fand erst spät eine eigene der kontinentalen Säkularisation gleichkommende Debatte. Die wichtigste Geschichte der englischen Literatur, die im 19. Jahrhundert erschien, Hippolyte Taines "History of English Literature" brachte die neue Wortverwendung als Anstoß von außen ins Spiel und machte verhältnismäßig spät klar, welche Bedeutung England in der neu zu schreibenden Literaturgeschichte selbst gewinnen konnte.

Das Konzept nationaler Literaturen wurde von Europa aus den Nationen der Welt vorgelegt. Es fand am Ende weltweit Akzeptanz. Der Buchmarkt gestaltete sich im selben Geschehen um: Aus einem im frühen 18. Jahrhundert marginalen Feld des Buchangebots wurde die zentrale Produktion. Es drohen mit dem Konzept nationaler Literaturen allerdings fragwürdige Wahrnehmungen:


Aus einer nationalliterarischen Perspektive wurde dankbar auf das Konzept nationaler Literaturen zurückgegriffen, da es die jeweilige kulturelle Identität nicht antastete. Die Komparatistik entwickelte jedoch schon früh mit dem Konzept der Weltliteratur ein transnationales Literaturmodell, das – jenseits einer nationalen oder ökonomischen Vorstellung von Literatur(markt) – ein kosmopolitisches Miteinander der Literaturen der Welt gegen die verengende nationale Perspektive setzte.

Weitaus mehr Einsprüche rief der enge Literaturbegriff hervor. Sowohl die Schulen der textimmanenten Interpretation, die wie der Strukturalismus die Bedeutung im einzeln vorliegenden Textstück suchen, als auch die Schulen der gesellschaftsbezogenen Literaturinterpretation vom Marxismus bis zu den Strömungen der Literatursoziologie, die einen Blick auf die Gesellschaft einfordern, traten in der zweiten Hälfte des 20. Jahrhunderts für einen „weiten“ Literaturbegriff ein, der es Literaturkritikern erlauben würde, auch politische Texte, Werbung und Alltagstexte ideologiekritisch zu besprechen.

So interpretieren die modernen Kulturwissenschaften literarische Texte nicht nur im literaturtheoretischen und -historischen Kontext, sondern auch als historische Dokumente, als Beiträge zu philosophischen Diskussionen oder (in Form der „Cultural Studies“) als Ausdruck der Dominanz herrschender oder der Unterdrückung marginalisierter (Sub-)Kulturen. Umgekehrt öffnen die Kulturwissenschaften den Blick für literarische Qualitäten der Geschichtsschreibung oder philosophische Aspekte von literarischen Texten.

Die Vertreter des Poststrukturalismus erweiterten in den 1980er und 1990er Jahren ihren Text- wie ihren Sprachbegriff noch entschiedener. Roland Barthes hatte in den 1950er Jahren bereits die Titelcover von Zeitschriften genauso wie das neue Design eines Autos in ihren Botschaften besprochen. Zur Selbstverständlichkeit wurde der erweiterte Sprachbegriff in der Filmwissenschaft. Hier spricht man ganz ohne weiteres von der „Bildsprache“ eines Regisseurs, und auch über eine solche Sprache können Literaturwissenschaftler sich äußern. Wenn die Literaturwissenschaft sich jedoch auf sprachliche Kunstwerke spezialisiert, hat dies durchaus Vorteile. Sie hält andere Wissenschaftler davon ab, in ihrem Forschungsfeld als Experten aufzutreten, kann jedoch letztlich sehr frei festlegen, was ihr Gegenstand ist. Sie kann sich so auf ein gut gehendes Kerngeschäft, Literatur im engen Sinn, ausrichten oder mit einem erweiterten Literaturbegriff auftreten. Der wiederkehrende Warnruf, der Tod der Literatur stehe bevor, ist auch ein Spiel mit der Aufmerksamkeit der Gesellschaft, die den Austausch über Literatur verfolgt und verteidigt.

Neuerdings wird von einer „performativen Wende“ der Literatur unter den Bedingungen des Internet gesprochen, die auch die Grenzen zwischen Literatur und darstellenden Künsten bzw. zwischen Schriftlichkeit und Mündlichkeit relativiert: Das Erscheinen eines Textes im Internet könne als performativer Akt analog einer Theateraufführung verstanden werden. Das Internet sei nicht mehr nur ein Geflecht von Texten; die „Netzliteratur“, z.B. das Schreiben in Chatrooms, sei vielmehr wesentlich durch performative Aspekte, d. h. durch Handlungen bestimmt. Die Kategorie der Performanz, die bisher nur auf Mündliches bezogen war, kann damit auch auf schriftliche Äußerungen übertragen werden: Zwischen ihrem Verfassen, ihrem Erscheinen und ihrer Lektüre muss (fast) keine Zeit mehr verstreichen. Das ähnelt der Sprechsituation von Speaker's Corner.


Interessant ist auch die Digitale Bibliothek:
Volltexte gemeinfreier Publikationen zwischen 1750 und 1930:
E-Texte der Philosophie, Religion, Literatur etc.

Im Internet wird aber nicht nur Literatur zur Verfügung gestellt, sondern auch Literatur geschrieben. Beispiele sind Digitale Poesie, Weblogs oder kollaboratives Schreiben im Netz.

Digitale Literatur folgt anderen Kriterien als herkömmliche Literatur.
Literatur im Internet ist von Aspekten der Technik, Ästhetik und Kommunikation geprägt. Das Internet eignet sich dafür um über zeitliche und räumliche Distanzen hinweg zu kommunizieren und multimediale Aspekte zu vereinen und zu integrieren.
Außerdem unterliegen elektronische Medien einer beständigen Metamorphose.
So haben beispielsweise Neal Stephenson und sein Team mit dem Schreiben eines Romans ("The Mongoliad") im Internet begonnen, bei dem eine Community von Autoren interaktiv mitschreibt. Neben dem eigentlichen Text gibt es eine eigene E-Publishing-Plattform („Subutai“) mit Videos, Bildern, einem Wiki und einem Diskussionsforum zum Roman.

Zur Verwaltung von Literatur gibt es mittlerweile zahlreiche Programme. Mit ihnen lassen sich z. B. eigene Literatursammlungen nach spezifischen Merkmalen kategorisieren. Die Abfragen brauchen teilweise nicht von Hand eingegeben zu werden, es reicht, z. B. den Autor bzw. den Titel einzugeben und daraufhin eine Suche in bestimmten Datenbanken zu tätigen. Die Ergebnisse können dann einfach übernommen werden.

Eine Literaturdatenbank katalogisiert den Bestand aktueller und älterer Literatur. Hier finden vermehrt digitale Kataloge bzw. Online-Literaturdatenbanken ihren Gebrauch.


siehe auch: Literaturlexikon

"Die Autoren dieser Titel legen ein Corpus von in ihren Augen literarischen Werken fest und versuchen dann, in einer wissenschaftlichen und subjektiven Analyse dieser Werke auszumachen, was Literatur grundsätzlich auszeichnet."





</doc>
<doc id="2914" url="https://de.wikipedia.org/wiki?curid=2914" title="Loriot">
Loriot

Loriot [], bürgerlich Bernhard-Viktor Christoph-Carl von Bülow [], kurz Vicco von Bülow (* 12. November 1923 in Brandenburg an der Havel; † 22. August 2011 in Ammerland) etablierte sich von den 1950er Jahren an bis zu seinem Tod in Literatur, Fernsehen, Theater und Film als einer der vielseitigsten deutschen Humoristen. Loriot war zunächst Karikaturist, später arbeitete er auch als Schauspieler, Moderator, Regisseur sowie Bühnen- und Kostümbildner. 2003 wurde er zum Honorarprofessor für Theaterkunst an der Berliner Universität der Künste ernannt.

Der Künstlername „Loriot“ ist das französische Wort für „Pirol“. Der Vogel ist das Wappentier der Familie von Bülow. In der mecklenburgischen Heimat des Adelsgeschlechtes hat sich daher „Vogel Bülow“ als eine gängige Bezeichnung für den Pirol eingebürgert.

Bernhard-Viktor Christoph-Carl von Bülow wurde am 12. November 1923 als Sohn des Polizeileutnants Johann-Albrecht Wilhelm von Bülow (1899–1972) und dessen erster Ehefrau Charlotte Mathilde Luise, geborene von Roeder (1899–1929), Tochter Otto von Roeders (1876–1943), in Brandenburg/Havel geboren. Seine Eltern ließen sich 1928 in Gleiwitz scheiden.

Bei der Familie von Bülow handelt es sich um ein altes mecklenburgisches Adelsgeschlecht mit gleichnamigem Stammhaus im Dorf Bülow bei Rehna. Der Name Bülow wurde erstmals 1154 bei der Grundsteinlegung des Ratzeburger Doms urkundlich erwähnt. Die Stammreihe beginnt mit Godofridus de Bulowe (1229). Viele Mitglieder der Familie brachten es im Staatswesen, beim Militär und in der Kirche zu hohen Ämtern oder machten sich um das Kulturleben verdient.
Zu Vicco von Bülows Verwandten zählt Bernhard von Bülow, Reichskanzler im Deutschen Kaiserreich.

Von Bülow war ab 1951 mit der Hamburger Kaufmannstochter und damaligen Modeschülerin Rose-Marie, Tochter von Peter Schlumbom, genannt Romi (* 1929), verheiratet und war Vater zweier Töchter – Bettina und Susanne – sowie Großvater zweier Enkelkinder; er lebte von 1963 bis zu seinem Tod in Ammerland am Starnberger See.

Von Bülow wuchs mit seinem ein Jahr jüngeren Bruder seit 1927 bei Großmutter und Urgroßmutter in Berlin auf. 1933 zogen die Geschwister wieder zu ihrem Vater, der im Jahr 1932 erneut geheiratet hatte. Von Bülow besuchte von 1934 bis 1938 das Schadow-Gymnasium in Berlin-Zehlendorf. Mit dem Vater zog die Familie 1938 nach Stuttgart. Von Bülow besuchte dort das humanistische Eberhard-Ludwigs-Gymnasium, das er 1941 siebzehnjährig mit Notabitur verließ. In Stuttgart sammelte er auch erste Erfahrungen als Statist in der Oper und im Schauspiel. 1940 spielte er als Statist in dem Film Friedrich Schiller – Der Triumph eines Genies mit.

Er begann entsprechend der Familientradition eine Offizierslaufbahn, war drei Jahre mit der 3. Panzer-Division an der Ostfront im Einsatz und wurde mit dem Eisernen Kreuz zweiter und erster Klasse ausgezeichnet; er erreichte den Dienstgrad Oberleutnant. Sein jüngerer Bruder, der am 27. November 1924 ebenfalls in Brandenburg geborene Johann-Albrecht Sigismund von Bülow, fiel am 21. März 1945 als Leutnant bei Gorgast im Oderbruch. Vicco von Bülows militärische Personalakte enthielt keinen Hinweis auf nationalsozialistische Gesinnung.

Auf die Frage, ob er im Zweiten Weltkrieg ein guter Offizier gewesen sei, antwortete er in einem Interview: 

Nach dem Krieg arbeitete er nach eigener Schilderung für etwa ein Jahr als Holzfäller im Solling, um sich Lebensmittelkarten zu verdienen. 1946 vervollständigte er in Northeim am Gymnasium Corvinianum das Notabitur. Auf Anraten seines Vaters studierte er von 1947 bis 1949 Malerei und Grafik an der Kunstakademie (Landeskunstschule) in Hamburg. Zu seinen Lehrern gehörte Alfred Mahlau.

Nach dem Abschluss legte Bülow erste Arbeiten als Werbegrafiker vor und entwarf das charakteristische "Knollennasenmännchen". Von 1950 an war Bülow als Cartoonist zunächst für das Hamburger Magazin "Die Straße", danach für die Zeitschrift "Stern" tätig. Seit dieser Zeit verwendete er den Künstlernamen "Loriot".

Seine erste regelmäßige Serie im Stern sollte "Auf den Hund gekommen" werden. Einige dieser ersten Cartoons lösten bei den Lesern große Proteste aus:
Henri Nannen, der damalige Chefredakteur, stellte die Serie nach sieben Folgen ein und beendete die Zusammenarbeit: 
Nach der Einstellung im „Stern“ zeigte sich kein einziger Verleger in Deutschland interessiert, die Serie als kleines Buch zu drucken. Unter anderem lehnte Ernst Rowohlt ab. Loriot sandte auf Anraten einer Bekannten dem Schweizer Daniel Keel die Zeichnungen; 1954 präsentierten die beiden auf der Frankfurter Buchmesse das Buch "Auf den Hund gekommen: 44 lieblose Zeichnungen". So begann eine lebenslange Zusammenarbeit: Loriot publizierte fortan fast ausschließlich bei Keel. Für Keel – er hatte 1952 den Diogenes Verlag gegründet – war es das zweite Buch; für Loriot das erste. 

1953 startete der „Stern“ eine Kinderbeilage, das „Sternchen“. Loriot schlug die Serie „Reinhold das Nashorn“ vor und bekam den Auftrag, aus zunächst geplanten zwei Monaten wurden schließlich 17 Jahre. 

Im Dezember 1953 wurden auf der Rückseite von "Weltbild" vier Zeichnungen von Loriot veröffentlicht. Im Mai 1954 schloss Loriot dann einen Vertrag mit der Zeitschrift. Die Zusammenarbeit mit dem Verlag war jedoch von Beginn an schwierig. So gab es erhebliche Kritik an vielen von Loriots Beiträgen, einige wurden auch nicht zur Veröffentlichung angenommen. So wurde Loriot im August 1955 gekündigt. Im Kündigungsschreiben des Verlegers Diedrich Kenneweg hieß es, „dass sich unsere Leser mehr und mehr gegen Ihren Stil ausgesprochen haben.“.

Ab Januar 1956 arbeitete Loriot für die "Quick". Dort veröffentlichte er zwischen Oktober 1956 und Dezember 1957 die Ratgeberserie "Der Gute Ton", von der im Herbst 1957 Teile in Buchform als "Der gute Ton: Das Handbuch feiner Lebensart in Wort und Bild" veröffentlicht wurden. Ab September 1957 veröffentlichte Loriot abwechselnd mit seinem Vorbild und Freund Manfred Schmidt die Kolumne "Der ganz offene Brief". Darin setzten sie sich satirisch mit dem aktuellen Zeitgeschehen, kuriosen Meldungen und privaten Ärgernissen auseinander. Die Schreiben waren dabei immer von einer Zeichnung begleitet. Die Kolumne endete 1961. In seinem hundertsten "ganz offenen Brief" vom 13. August 1961 hatte sich Loriot auf einen Artikel des Spiegels bezogen, in dem berichtet wird, dass es in Deutschland erlaubt sei, Wein mit Zuckerwasser, Kohlendioxid und Kaliumferrocyanid zu versetzen. Loriot trieb dies auf die Spitze, indem er behauptete dass „nach uralter Familientradition“ auch "eine" Traube den Weg ins Fass fände. Daraufhin kam es zu erheblichen Protesten und erbosten Leserbriefen von Winzern an die "Quick". Dies führte zu einer verlagsinternen Kampagne gegen Loriot. Er beendete daraufhin seine Mitarbeit an dieser Kolumne in seinem 102. Beitrag. Darin berichtete er davon, dass ihm regelmäßig dunkle Gestalten auflauerten, darunter auch Winzer und Weinhändler, und das obwohl er unter anderem „seit Jahren täglich eine Flasche Wein leere“. Zu seinem eigenen Schutz wolle er deshalb die Mitarbeit an der Kolumne beenden. Die Affäre war damit jedoch noch nicht beendet. So musste er im Oktober an einer PR-Aktion im "Haus des Deutschen Weines" und einem Weinbaugebiet teilnehmen. Der satirische Reisebericht, in dem Loriot neben einer Richtigstellung auch weiter austeilte, wurde im Dezember in der "Quick" veröffentlicht.

Außerdem nahm Loriot ab Mitte der 50er-Jahre verstärkt Werbeaufträge an, unter anderem für Paderborner Bier, Agfa, den Kräuterlikör Scharlachberg („Nimm's leicht!“) und die Tabakmarke Stanwell („Drei Dinge braucht der Mann.“). In Anzeigen und Trickfilmspots kamen auch hier die Knollennasenmännchen zum Einsatz und gewannen mehr und mehr an Popularität.

Kleinere Rollen als Schauspieler hatte Loriot in Bernhard Wickis Filmen "Die Brücke" (1959) und "Das Wunder des Malachias" (1961). Auch in Andrew Martons Kriegsfilm "Der längste Tag" (1962), bei dem Bernhard Wicki Co-Regisseur war, konnte er in einer kleinen Rolle mitwirken. Im selben Jahr gestaltete er das Titelblatt der ersten Ausgabe der Satirezeitschrift "pardon". 

1963 zog Vicco von Bülow mit seiner Familie nach Münsing-Ammerland in die Nähe des Starnberger Sees. Dort wurde er als angesehenes Mitglied der Dorfgemeinschaft 1993 zum Ehrenbürger erhoben.
Loriot moderierte von 1967 bis 1972 die Fernsehsendung "Cartoon" für den Süddeutschen Rundfunk der ARD, die er auch als Autor und Co-Regisseur verantwortete. Es handelte sich ursprünglich um eine Sendereihe internationaler Zeichentrickfilme, in die er auch eigene Arbeiten einbrachte und damit künstlerisch die engen Rahmenbedingungen, die das Medium Zeitschrift seinen Zeichnungen auferlegt hatte, verließ. Loriots anfänglich reine Moderation von einem roten Sofa aus wurde zunehmend zu einem eigenständigen humoristischen Element der Sendung. Später baute Loriot auch Sketche, in denen er selbst die Hauptrolle übernahm, in die Folgen ein.

1971 schuf Loriot mit dem Zeichentrick-Hund Wum ein Maskottchen für die Aktion Sorgenkind in der ZDF-Quizshow "Drei mal Neun", dem er selbst auch die Stimme lieh. Zu Anfang war Wum noch der treue Freund eines Männchens, des eigentlichen Maskottchens, dem er jedoch mehr und mehr die Show stahl und das er schließlich völlig verdrängte. Zu Weihnachten 1972 wurde Wum dann zum Gesangsstar: Mit dem Titel "Ich wünsch' mir 'ne kleine Miezekatze" war er so erfolgreich, dass er für neun Wochen die Spitze der deutschen Hitparade belegte. Dabei handelte es sich bei "Wums Gesang" um von Bülows Sprechgesang. Wum blieb auch in der Nachfolgesendung "Der Große Preis" bis in die 1990er Jahre hinein als Pausenfüller erhalten, bald schon als Duo zusammen mit dem Elefanten Wendelin und später mit dem "Blauen Klaus", einem Außerirdischen, der mit seiner fliegenden Untertasse einschwebte. Loriot schrieb und zeichnete die Trickfilmgeschichten, die jedes Mal mit einer Aufforderung an die Zuschauer schlossen, sich an der Fernseh-Lotterie zu beteiligen, und lieh allen Figuren seine Stimme. Mit der letzten Folge von "Der große Preis" endeten auch die Abenteuer von Wum und Wendelin. Heute ist das Paar auf der letzten Seite der Fernsehzeitschrift Gong zu sehen.

Nach Ende der Serie "Cartoon" produzierte der Süddeutsche Rundfunk eine Sondersendung anlässlich des Besuchs der britischen Queen 1974 ("Loriots Telecabinet"), die bereits einiges von dem vorwegnahm, was im Laufe des Jahrzehnts noch kommen sollte. 1976 entstand mit "Loriots sauberer Bildschirm" die erste Folge der sechsteiligen Fernsehserie "Loriot" bei Radio Bremen, in der er sowohl Zeichentrickfilme als auch gespielte Sketche (letztere oft zusammen mit Evelyn Hamann) präsentierte. Diese Sketche und Trickfilme wurden in Deutschland sehr populär, werden noch immer regelmäßig im Fernsehen wiederholt und sind inzwischen komplett auf DVD erhältlich. Die Anmoderationen und humoristischen Einlagen von Loriot und Evelyn Hamann zwischen den Filmbeiträgen fanden auf einem grünen Sofa statt. 1983 produzierte Radio Bremen zu seinem 60. Geburtstag für die ARD die Sendung "Loriots 60. Geburtstag".

Eine besondere Liebe entwickelte Loriot zur klassischen Musik und zur Oper. Das Interesse hatten die Großmutter, die ihm als Kind Mozart, Puccini und Bach auf dem Klavier vorspielte, und die Plattensammlung seines Vaters mit Aufnahmen von Tenören und Opern-Soli, geweckt. In seiner Stuttgarter Zeit wohnte Loriot in Laufweite zur Oper Stuttgart und wirkte als Komparse auf der Opernbühne mit.

1982 dirigierte er das „humoristische Festkonzert“ zum 100. Geburtstag der Berliner Philharmoniker, mit deren Geschichte er durch familiäre Beziehungen verbunden war (Hans von Bülow, der erste Chefdirigent der Philharmoniker, war ein entfernter Verwandter von Loriot). Seine Erzählfassung des "Karnevals der Tiere" führte Loriot wiederholt mit dem "Scharoun Ensemble" auf, einem Kammermusikensemble von Musikern der Berliner Philharmoniker. 

Als Regisseur inszenierte Loriot die Opern "Martha" (Stuttgart, 1986) und "Der Freischütz" (Ludwigsburg, 1988). Seit 1992 wird seine Erzählfassung von Wagners „Ring des Nibelungen“ aufgeführt: „Der Ring an 1 Abend“, uraufgeführt im Nationaltheater Mannheim. Loriots „Ring“ bildete auch den einzigen Programmpunkt der 1995 erstmals in Berlin veranstalteten Operngala zugunsten der Deutschen AIDS-Stiftung. Loriot war bis 2006 Moderator dieser jährlich in der Deutschen Oper Berlin ausgerichteten Veranstaltung. Seine Moderationstexte bildeten später den Grundstock für "Loriots kleinen Opernführer". Sein Nachfolger als Moderator der AIDS-Gala war ab 2007 Max Raabe.

Für Leonard Bernsteins Operette "Candide" verfasste Loriot neue Texte für eine konzertante Aufführung, welche die Handlung besser verständlich macht und dem Stück in Deutschland zu neuer Popularität verhalf. Die Neufassung der konzertanten "Candide" wurde 1997 im Prinzregententheater München uraufgeführt.

1988 drehte Loriot als Autor, Regisseur und Hauptdarsteller den Film "Ödipussi", 1991 folgte dann "Pappa ante portas". Dabei spielte Evelyn Hamann jeweils die weibliche Hauptrolle. Produziert wurden alle seine Filme von Horst Wendlandt, der auch die meisten Filme von Otto Waalkes und Hape Kerkeling produzierte.


Am 3. Januar 2011 erschienen vier Wohlfahrtsmarken mit Motiven aus bekannten Zeichentrickfilmen von Loriot: "Das Frühstücksei", "Herren im Bad", "Auf der Rennbahn" und "Der sprechende Hund". Die Zeichnungen hat Vicco von Bülow alias Loriot selbst ausgewählt und als Motive für die Wohlfahrtsmarken zur Verfügung gestellt. Die Marken wurden am 10. Januar 2011 von Bundesfinanzminister Wolfgang Schäuble als Herausgeber und Bundespräsident Christian Wulff als Schirmherr des Sozialwerkes Wohlfahrtsmarken der Öffentlichkeit vorgestellt: „Für mich ist Vicco von Bülow einer der ganz großen lebenden Deutschen im Kulturbereich, einer der ganz großen Kulturschaffenden unseres Landes“, sagte der Bundespräsident.

Im März 2012 entschied das Landgericht Berlin zu Gunsten der Erben von Vicco von Bülow, dass Wikipedia im Gegensatz zu vielen anderen Websites und Zeitungen die Briefmarken nicht zeigen darf. Die Abbildungen waren bereits im Herbst 2011 nach einer einstweiligen Verfügung entfernt worden.

Vicco von Bülow starb am 22. August 2011 im Alter von 87 Jahren in Ammerland am Starnberger See. Er wurde am 30. August 2011 im engsten Familienkreis auf dem Waldfriedhof Heerstraße im Berliner Stadtteil Westend beigesetzt. In der evangelisch-lutherischen St. Gotthardtkirche in Brandenburg/Havel, wo von Bülow am 30. Dezember 1923 getauft worden war, wurde ebenfalls ein Trauergottesdienst für ihn gehalten. Loriot hatte 1986 öffentlich zu Spenden für die Sanierung der Kirche aufgerufen.

Der Art Directors Club trauerte um sein Ehrenmitglied in einer Zeitungsanzeige mit den Worten: „Lieber Gott, viel Spaß!“

Neben dem Grab auf dem Berliner Waldfriedhof an der Heerstraße erinnern in seinem Geburtsort Brandenburg „Loriots Weg“ mit mehreren Stationen, darunter einige seiner Lebens- und Wirkungsstätten, ein sitzendes Knollennasenmännchen und die Figur Müller-Lüdenscheid an Loriot.
Auf dem Eugensplatz in Stuttgart weist seit November 2013 eine Säule darauf hin, dass Loriot dort in Jugendjahren wohnte. Nachdem eine bei einer humoristischen Aktion auf das Denkmal gestellte Mopsfigur für Aufsehen gesorgt hatte und wenig später auf ungeklärte Weise wieder verschwunden war, ziert seit Mai 2014 die Bronzestatue eines Mopses ganz offiziell die Säule. Seit 8. Juni 2015 erinnert auch eine Tafel an der Fassade des Hauses Haußmannstraße 1 daran, dass Loriot „im dritten Stock dieses Hauses von 1938 bis 1941“ wohnte. In Bremen wurde 2013 eine Bronzereplik des Loriot-Sofas – ebenfalls mit Mopsskulptur – vor dem Funkhaus von Radio Bremen postiert. Im selben Jahr fand am Hillmannplatz in der Innenstadt die Einweihung des Loriotplatzes statt.

Die im Münchner "riva Verlag" kurz nach Loriots Tod erschienene Biografie musste Mitte Januar 2013 aufgrund von Urheberrechtsverstößen vom Markt genommen werden. Loriots Tochter Susanne von Bülow hatte vor dem Landgericht Braunschweig dagegen geklagt, dass das Buch zu viele Zitate Loriots enthalte. Die Klägerin erzielte einen Teilerfolg. Der Verlag erklärte, das Buch nun in veränderter Form auflegen zu wollen.

Loriots Werke beschäftigen sich hauptsächlich mit zwischenmenschlichen Kommunikationsstörungen. 
Seine Cartoons leben vom Kontrast zwischen der dargestellten Situation, der dabei zur Schau getragenen Würde seiner Knollennasenmännchen und den Legendentexten. Eines dieser Elemente fällt immer aus dem Rahmen, etwa der Legendentext unter der Darstellung eines sich distinguiert ein Kleinkind an die Brust legenden knollennasigen Herrn. Ein anderer Cartoon zeigt, wie einem Anstreicher, der am oberen Rand eines hohen Schornsteins arbeitet, der Pinsel herunterfällt, wozu der Legendentext trocken anmerkt: 
Themen der Cartoons sind insbesondere das Alltagsleben, Szenen aus der Familie und der bürgerlichen Gesellschaft und oft die sprichwörtliche „Tücke des Objekts“. Loriot machte das Absurde sichtbar, das in normalen Alltagssituationen steckt, und verspottete etwa in seinen „Ratgebern“ die insbesondere in der deutschen Nachkriegsgesellschaft verbreitete Sehnsucht nach festen, erlernbaren Regeln im gesellschaftlichen Umgang, deren Beachtung vor Peinlichkeiten bewahren sollte. Die Komik entsteht bei Loriot nun gerade in den Sketchen oft dadurch, dass die Figuren sich auch in unpassenden und grotesken Situationen (etwa wenn zwei einander unbekannte Herren versehentlich in derselben Badewanne gelandet sind) darum bemühen, sich an diese gesellschaftlichen Regeln und Normen zu halten, wodurch ein oft absurder Humor erzeugt wird. In seinen Filmen und Sketchen zeigte Loriot gewissermaßen die ‚Tapferkeit‘ von Menschen, die in den verschiedensten Situationen durch ihre bürgerlichen Umgangsformen eine Katastrophe (oder zumindest den destruktiven Ausbruch von Aggressionen) zu verhindern und so ihre Würde zu wahren versuchen. Auffallend sind daneben gekonnt eingesetzte schlüpfrige Akzente. Loriots Humor zeichnete sich durch einen meisterhaften Gebrauch der deutschen Sprache aus.

Einige Erfindungen und Formulierungen Loriots wurden im deutschen Sprachraum Allgemeingut. Dazu gehören das Jodeldiplom, die Steinlaus (die sogar mit einem Eintrag im Pschyrembel vertreten ist) und der Kosakenzipfel mit den den Konflikthöhepunkt des zugehörigen Sketches markierenden Beschimpfungen „Jodelschnepfe“ und „Winselstute“, aber auch Sätze wie „Da hab’ ich was Eigenes, [da] hab’ ich mein Jodeldiplom“, „Und Reiter werden ja immer gebraucht!“, „Bitte sagen Sie jetzt nichts…“, „Das ist fein beobachtet“, „Früher war mehr Lametta!“, „Ein Klavier, ein Klavier!“, „Das Bild hängt schief!“, „Es saugt und bläst der Heinzelmann, wo Mutti sonst nur saugen kann“ (sowie die Variante „wo Mutti sonst nur blasen kann“), „Männer und Frauen passen (einfach) nicht zusammen!“, „Frauen haben auch ihr Gutes“ oder das lakonische „Ach (was)!“




Eine Auswahl der klassischen Sketche und Zeichentrickfilme:


Weitere Ausstellungen hatte es in Brandenburg bereits 1985 und 1996 gegeben. Im Panoptikum Mannheim befindet sich eine ihm zu Ehren geschaffene Wachsfigur.






Über Loriot


</doc>
<doc id="2917" url="https://de.wikipedia.org/wiki?curid=2917" title="Lauren Bacall">
Lauren Bacall

Lauren Bacall [] (* 16. September 1924 in New York City als Betty Joan Perske; † 12. August 2014 ebenda) war eine US-amerikanische Schauspielerin.

Sie zählte zu den Leinwandlegenden der „goldenen Ära“ Hollywoods und spielte während ihrer 68 Jahre lang dauernden Karriere an der Seite von Stars wie ihrem Ehemann Humphrey Bogart, John Wayne, Rock Hudson, Gary Cooper, Marilyn Monroe, Tony Curtis oder Kirk Douglas. Zu ihren berühmtesten Filmen gehören ihr Leinwanddebüt "Haben und Nichthaben" (1944), die Thriller "Tote schlafen fest" (1946) und "Gangster in Key Largo" (1948), die Liebeskomödie "Wie angelt man sich einen Millionär?" (1953) sowie der Film "Liebe hat zwei Gesichter" (1996), für den sie den Golden Globe Award und eine Oscar-Nominierung erhielt.

In den 1970er Jahren feierte Bacall außerdem große Erfolge als Theaterschauspielerin am Broadway. Für ihre Darstellungen in den Musicals "Applause" und "Woman of the Year" wurde sie mit dem Tony Award ausgezeichnet. Das American Film Institute wählte sie 1999 auf Platz 20 der 25 größten weiblichen Filmstars. 2009 erhielt sie den Ehrenoscar für ihr Lebenswerk.

Lauren Bacall wurde am 16. September 1924 als einziges Kind von William Perske (* 1889 als Wulf Persky; † 1982) und Natalie Weinstein (* 1901 in Rumänien; † 1972), in der Bronx geboren. Ihre deutschstämmige Mutter war bereits als Kleinkind mit den Großeltern, Maximillian und Sophie Weinstein, in die Vereinigten Staaten gekommen. Die Vorfahren ihres Vaters stammten zum Teil aus Frankreich; Bacalls Cousin väterlicherseits war der israelische Spitzenpolitiker und ehemalige Staatspräsident Schimon Peres.

Die ersten Lebensjahre verbrachte Bacall in Brooklyn. Ihre Eltern ließen sich scheiden, als sie sechs Jahre alt war. Fortan lebte sie mit ihrer Mutter, die als Sekretärin arbeitete, bei den Großeltern in Manhattan. Der Kontakt zum Vater brach endgültig ab. Natalie Perske ließ sowohl ihren als auch den Familiennamen ihrer Tochter in Bacal, die rumänische Version ihres Geburtsnamens Weinstein, ändern. Später wurde daraus Bacall, um die Aussprache zu erleichtern.

Mit finanzieller Unterstützung eines wohlhabenden Onkels besuchte Bacall die private Mädchenschule Highland Manor in Tarrytown, wo sie sich in der Theatergruppe engagierte. Später ging sie auf die Julia Richman High School in Manhattan und arbeitete ab ihrem 16. Lebensjahr nebenbei als Model. Während ihrer Schulzeit träumte sie zunächst davon, professionelle Tänzerin zu werden, entschied sich dann jedoch für die Schauspielerei und schrieb sich für ein Jahr an der New Yorker American Academy of Dramatic Arts ein. Dies ermöglichte es ihr, erste Bühnenerfahrungen in einigen Off-Broadway-Produktionen zu sammeln.

Neben dem Schauspielstudium arbeitete sie als Platzanweiserin im St. James Theater. Der Theaterkritiker George Jean Nathan kürte sie in einem seiner Artikel für das Esquire Magazine zur „hübschesten Platzanweiserin“ Manhattans. „Das hat mir wirklich geschmeichelt. Die Aufmerksamkeit von jemandem zu erringen – irgendjemandem –, der in Theaterkreisen angesehen war, das war schon etwas. Es verschaffte mir keine Rolle, aber es konnte auch nicht schaden, und es war besser, als einfach in der Versenkung zu verschwinden“, erinnerte sich Bacall später.

Von März bis Mai 1942 gab Bacall ihr Broadway-Debüt in Rowland Browns "Johnny 2 × 4" am Longacre Theatre. Im gleichen Jahr gewann sie die Wahl zur „Miss Greenwich Village“ und erhielt ihre erste große Rolle in George Simon Kaufmans Theaterstück "Franklin Street," das jedoch schon bei der Probeaufführung in Washington D.C. floppte und der Schauspielerin deshalb nicht den ersehnten Durchbruch verschaffen konnte.

Einflussreiche Bekannte arrangierten 1942 ein Treffen zwischen Bacall und Diana Vreeland, damalige Moderedakteurin bei Harper’s Bazaar. Vreeland war hingerissen vom Aussehen der jungen Schauspielerin, und so schaffte Bacall es auf den Titel der Ausgabe des Magazins von März 1943. Nachdem der Hollywood-Regisseur Howard Hawks Bacalls Bild entdeckt hatte, lud er sie zu Probefilmaufnahmen ein. Bacall über Hawks: „Howard Hawks war es, der mein Leben verändert hat. Er war auf der Suche nach einem Mädchen, das er nach seinen Vorstellungen neu erfinden konnte, er wollte ein Sex-Symbol erschaffen und mich zu einem großen Star machen.“ Und Bacall war der Frauentyp, der Ende der 1930er und anfangs der 1940er in Hollywood gefragt war: starke Charaktere wie Bette Davis und Barbara Stanwyck definierten die klassische Frauenrolle neu; Frauen durften plötzlich kraftvoll und nicht nur einfach unergründlich sein. Um diesen Typ zu unterstreichen, nannte Bacall sich fortan nicht mehr Betty, sondern Lauren.

Sie wurde von Warner Bros. unter Vertrag genommen, und Howard Hawks besetzte sie in seinem Film "Haben und Nichthaben" (1944), der gleichzeitig Bacalls Leinwanddebüt markierte, als Femme Fatale an der Seite von Hollywood-Größe Humphrey Bogart. Bacall erinnerte sich später an ihre Angst während der ersten Szene, in der sie Bogart um Feuer für ihre Zigarette fragt: „Meine Hand zitterte. Mein Kopf wackelte. Die Zigarette zitterte. Ich fragte mich, was Howard wohl denken musste. Was mussten Bogart und die Crew denken? Es war grauenhaft. Die einzige Möglichkeit, meinen Kopf still zu halten, war ihn zu senken, das Kinn fast bis auf die Brust, und mit den Augen zu Bogart aufzublicken.“ Diese Haltung und dieser Blick wurden – neben Bacalls heiser klingender Stimme – bald zu ihren Markenzeichen auf der Leinwand.

Während der Dreharbeiten verliebten sich die damals 19-jährige Bacall und der 25 Jahre ältere Bogart ineinander, doch die Beziehung musste zunächst geheim gehalten werden, da Bogart noch mit der Schauspielerin Mayo Methot verheiratet war. Hawks, der bald von der Affäre seiner beiden Hauptdarsteller erfuhr, nutzte die Anziehungskraft zwischen Bogart und Bacall für seinen Film, indem er sie in noch mehr Szenen zusammen auftreten ließ. "Haben und Nichthaben" kam im Oktober 1944 in die US-amerikanischen Kinos und lockte das Filmpublikum, das sich von der Chemie zwischen Bacall und Bogart überzeugen wollte, in Scharen in die Vorstellungen. Nach der Scheidung von Methot heirateten Bogart und Bacall am 21. Mai 1945 auf einer Farm in Lucas County, Ohio.

Im selben Jahr erschien Bacalls zweiter Film "Jagd im Nebel" an der Seite von Charles Boyer und Peter Lorre. In dem Film von Regisseur Herman Shumlin spielt Bacall eine englische Spionin, und sie musste sich für diese Rolle einen britischen Akzent aneignen. Später gab sie zu, dass ihr dies nicht besonders gut gelungen sei. Sie habe den Film nur gedreht, weil sie von Jack L. Warner in das Projekt gedrängt worden sei. Später wehrte sich Bacall erfolgreicher gegen Rollen, die ihr vom Filmstudio vorgeschrieben wurden, obwohl sie ihr nicht zusagten. „Wenn man nicht spielte, was das Studio von einem verlangte, wurde man suspendiert. Und ich wurde suspendiert! Ich hatte den Vertrag gebrochen. Studioboss Jack Warner hatte da eine einfache Philosophie: Ich bezahle sie, also wird sie verdammt noch mal machen, was ich von ihr will. Und er schickte mir schreckliche Drehbücher.“

In "Tote schlafen fest" (1946) arbeitete sie erneut mit Bogart und unter der Regie von Howard Hawks. Auch in Delmer Daves’ "Die schwarze Natter" (1947) und in John Hustons "Gangster in Key Largo" (1948) spielten Bacall und Bogart Seite an Seite. Ende der 1940er Jahre wurde Bacall schwanger, weshalb sie beruflich etwas kürzertrat, um sich um ihr und Bogarts erstes Kind, Steven Humphrey, kümmern zu können, der am 6. Januar 1949 geboren wurde.

1950 wirkte Bacall zweimal unter der Regie von Michael Curtiz bei den Filmen "Der Mann ihrer Träume" mit Kirk Douglas und Doris Day sowie "Zwischen zwei Frauen" an der Seite von Gary Cooper und Patricia Neal mit.

Am 23. August 1952 kam Bacalls und Bogarts zweites Kind zur Welt. In ihrem ersten Film nach der Geburt von Tochter Leslie Howard spielte Bacall gemeinsam mit Marilyn Monroe und Betty Grable in "Wie angelt man sich einen Millionär?" (1953) von Regisseur Jean Negulesco. Mit ihm drehte sie ein Jahr später auch das Drama "Die Welt gehört der Frau".

1955 bekamen Bacall und Bogart das Angebot, die Hauptrollen in William A. Wellmans Abenteuerfilm "Der gelbe Strom" zu spielen. Während sich Bacall mit den Produzenten einigte, lehnte Bogart die Rolle des Capt. Wilder ab, da das Filmbudget für seine Gagenforderung in Höhe von 500.000 US-Dollar nicht ausreichte. Den Part an der Seite Bacalls übernahm schließlich John Wayne, und der Film feierte seine Weltpremiere am 1. Oktober 1955. Danach war Bacall mit Rock Hudson in der Romanverfilmung "In den Wind geschrieben" (1956) zu sehen.

Humphrey Bogart starb im Januar 1957 an Speiseröhrenkrebs in ihrem gemeinsamen Haus in Los Angeles. Bacall wurde mit 32 Jahren Witwe und alleinerziehende Mutter zweier Kinder im Alter von acht und vier Jahren. Später erinnerte sie sich: „Ich brauchte Jahre, um damit fertigzuwerden. Eigentlich bin ich bis heute nicht über seinen Tod hinweggekommen. Über solche Dinge kommt man nie hinweg.“

Einige Monate später ging sie eine kurze Affäre mit Frank Sinatra ein. Das Paar war auch kurze Zeit heimlich verlobt, Sinatra löste die Verbindung jedoch, nachdem die Presse davon erfahren hatte. Nach dem Tod ihres Ehemannes war Bacall erstmals mit "Warum hab’ ich ja gesagt?" (1957) wieder auf der Kinoleinwand zu sehen. Es folgten "Geschenk der Liebe" (1958) und der britische Abenteuerfilm "Brennendes Indien" (1959) von J. Lee Thompson.

Danach zog es Bacall zurück an die Ostküste, wo sie von Dezember 1959 bis März 1960 in dem Broadway-Stück "Goodbye, Charlie" am Lyceum Theatre auf der Bühne stand. In New York lernte die Schauspielerin ihren Kollegen Jason Robards kennen, den sie im Jahr 1961 heiratete. Ihr gemeinsamer Sohn Sam Prideaux kam am 16. Dezember 1961 zur Welt.

Auf der Kinoleinwand war Bacall in den 1960er Jahren nur sporadisch zu sehen. 1964 spielte sie in dem wenig beachteten Film "Der Mörder mit der Gartenschere" von Regisseur Denis Sanders und drehte mit Tony Curtis, Natalie Wood und Henry Fonda unter der Regie von Richard Quine die romantische Komödie "… und ledige Mädchen". Schließlich war sie in Jack Smights Thriller "Ein Fall für Harper" (1966) zu sehen.

Von Dezember 1965 bis November 1968 gab Bacall über 1000 Vorstellungen als Stephanie Dickinson in dem Broadway-Stück "Cactus Flower," das später mit Ingrid Bergman und Walter Matthau in den Hauptrollen verfilmt wurde. 1969 wurden Bacall und Robards geschieden.

Am 30. März 1970 feierte das Broadway-Musical "Applause" am New Yorker Palace Theatre seine Premiere. Bis Juli 1972 trat Bacall in über 800 Vorstellungen als Margo Channing auf und erhielt für diese Rolle den Tony Award als beste Hauptdarstellerin in einem Musical. In der Fernsehadaption des Stücks von 1973 übernahm Bacall abermals die Hauptrolle, die ihr eine Emmy-Nominierung einbrachte.

Im Jahr 1974 fanden die Dreharbeiten zur Verfilmung von Agatha Christies Kriminalroman "Murder on the Orient Express" in Europa statt. In dem Film spielte Bacall unter der Regie von Sidney Lumet in einem starbesetzten Schauspielensemble, dem u. a. auch Ingrid Bergman, Vanessa Redgrave, Sean Connery, Albert Finney und John Gielgud angehörten. "Mord im Orient-Expreß" feierte seine Weltpremiere am 24. November 1974 in den Vereinigten Staaten. Mit ihrem Kollegen John Gielgud spielte Bacall 14 Jahre später erneut in einer Agatha Christie-Verfilmung: "Rendezvous mit einer Leiche" (1988).

In Don Siegels Western "The Shootist – Der letzte Scharfschütze" (1976) spielte Bacall an der Seite von John Wayne, James Stewart und Ron Howard. Für die Rolle der Bond Rogers erhielt sie 1977 eine British-Academy-Film-Award-Nominierung als beste Hauptdarstellerin. "The Shootist – Der letzte Scharfschütze" war nicht nur John Waynes letzter Film, es war auch der letzte Kinofilm, in dem Bacall eine Hauptrolle spielte.

1978 war sie mit Ruth Gordon in dem Fernsehfilm "Ladies mit weißer Weste" nach dem Drehbuch von Nora Ephron und unter der Regie von Jackie Cooper zu sehen. An der Seite von James Garner war sie 1979 in einer Doppelfolge der Serie "Detektiv Rockford – Anruf genügt" erneut im Fernsehen zu sehen und erhielt für ihre Darstellung ihre zweite Nominierung für den Emmy.

Auf die Kinoleinwand kehrte sie mit Robert Altmans "Health – Der Gesundheits-Kongreß" (1980) zurück, gefolgt von Edward Bianchis "Der Fanatiker" (1981). Von März 1981 bis März 1983 trat Bacall wieder am Broadway auf. Für ihre Rolle in dem Musical "Woman of the Year" gewann sie 1981 erneut einen Tony Award. Ende der 1980er drehte sie die Kinoproduktionen "Mr. North – Liebling der Götter" (1988) und "Wahn des Herzens" (1989) sowie die Fernsehfilme "Dinner at Eight" (1989) und "Schatten über Sunshine" (1990).

In Rob Reiners Psychothriller "Misery" (1990) war sie neben Kathy Bates und James Caan in der Rolle einer Verlegerin zu sehen, und 1994 übernahm sie eine Nebenrolle in Robert Altmans Episodenfilm "Prêt-à-Porter" über den Pariser Modezirkus.

1996 spielte Lauren Bacall in Barbra Streisands romantischer Komödie "Liebe hat zwei Gesichter" die Mutter von Streisands Filmfigur Rose Morgan. Für ihre Darstellung der eitlen und überheblichen Hannah Morgan wurde Bacall von Filmkritikern gefeiert und mit zahlreichen Preisen geehrt, darunter eine Nominierung für den Oscar, die Auszeichnung mit dem Golden Globe als beste Nebendarstellerin und dem Screen Actors Guild Award.

Mit Jack Lemmon, James Garner und Dan Aykroyd drehte sie unter der Regie von Peter Segal die Komödie "Ein Präsident für alle Fälle" (1996) und stand noch einmal mit Kirk Douglas in "Diamonds" (1999) vor der Kamera. In Lars von Triers "Dogville" (2003) und "Birth" (2004) spielte sie an der Seite von Nicole Kidman. Von Trier holte sie auch für sein Drama "Manderlay" (2005) vor die Kamera. In "The Walker" (2007) von Regisseur Paul Schrader übernahm sie ebenfalls eine Nebenrolle. Gemeinsam mit Schrader, Moritz Bleibtreu und Willem Dafoe stellte sie den Film bei der Berlinale vor, wo er auch im Wettbewerb lief. In der Fernsehserie "Die Sopranos" hatte sie 2006 einen Gastauftritt neben Ben Kingsley, in welchem die beiden sich selbst darstellen. Auch in Natalie Portmans Regiedebüt, dem Kurzfilm "Eve" (2008), war sie zu sehen.

Anfang September 2009 wurde der Schauspielerin der Ehrenoscar zugesprochen, den sie am 14. November 2009 erhielt.

In Interviews sagte Bacall über ihre andauernde Arbeit als Schauspielerin trotz ihres hohen Alters:

Am 12. August 2014 starb sie im Alter von 89 Jahren in ihrer Wohnung im Dakota Building in New York City an den Folgen eines Schlaganfalls.


Academy Award of Merit („Oscar“)

British Academy Film Award

César

Emmy Award

Golden Globe Award

National Board of Review Award

Satellite Award

Screen Actors Guild Award

Tony Award

Weitere Auszeichnungen





</doc>
<doc id="2918" url="https://de.wikipedia.org/wiki?curid=2918" title="Lars von Trier">
Lars von Trier

Lars von Trier (bürgerlich "Lars Trier"; geboren am 30. April 1956 in Kopenhagen) ist ein dänischer Filmregisseur und Drehbuchautor. Er ist unter anderem Gewinner der Goldenen Palme von Cannes.

Lars von Trier ist der zweite Sohn von Inger Høst (1915–1989) und dem dänischen Juden Ulf Trier (1907–1978). Ulf Triers Vorfahren Salomon und Ethel Trier waren im 18. Jahrhundert aus Trier nach Dänemark eingewandert. Inger Høst und Ulf Trier lernten sich während der deutschen Besetzung Dänemarks in der dänischen Widerstandsbewegung kennen, wo sie Juden halfen, über den Øresund ins sichere Schweden zu fliehen. Nach von Triers Angaben waren seine Eltern Kommunisten, gehörten einer Gemeinschaft von Nudisten an und erzogen ihn antiautoritär.

Erst im Alter von 33 Jahren erfuhr von Trier, dass Ulf Trier nicht sein leiblicher Vater und er somit nicht jüdischer Abstammung war. Kurz vor ihrem Tod gestand von Triers Mutter ihrem Sohn, dass sein leiblicher Vater ihr ehemaliger Vorgesetzter im Sozialministerium Fritz Michael Hartmann sei. Die deutschstämmige Familie Hartmann lebt mit Johann Ernst Hartmann seit 1762 in Dänemark. Sie hat einige bedeutende dänische Musiker, wie den Komponisten Johann Peter Emilius Hartmann, hervorgebracht. Nach eigenen Angaben war von Trier tief enttäuscht darüber, keine jüdischen Wurzeln zu haben. Er habe sich in der Rolle eines Außenseiters, der aus einer Gruppe von Verfolgten stammte, wohl gefühlt. In der Synagoge habe er sich immer zugehöriger gefühlt als in evangelischen oder katholischen Kirchen.

Ein Onkel mütterlicherseits war Børge Høst, ein Filmregisseur, der wohl sein Interesse fürs Filmemachen weckte. Bereits als Grundschüler drehte er mit einer Kamera im Super-8-Format kleine Animationsfilme, später dann Kurzfilme mit seinen Freunden. Sein erster dokumentierter Animationsfilm von circa 1967 hieß "Turen til Squashland" (Die Reise ins Zucchiniland) und dauerte eine Minute.

Von Trier litt bereits in seiner Kindheit unter Depressionen und Phobien und konnte einige Zeit nicht die Schule besuchen. Er wurde psychiatrisch betreut. Mit zwölf Jahren besuchte er ein Tagesheilungszentrum. Trotzdem spielte er 1969 eine der zwei Hauptrollen in der dänisch-schwedischen Kinderfernsehserie "Hemmelig sommer".

In seiner ersten Ehe war von Trier bis 1996 mit der dänischen Regisseurin, Drehbuchautorin und Schauspielerin Cæcilia Holbek Trier verheiratet, die wie er an der Dänischen Filmschule studiert hat. Während der zweiten Schwangerschaft von Cæcilia Holbek Trier verliebte er sich in die verheiratete Bente Frøge, eine Erzieherin, die seine Tochter in der Kindertagesstätte betreute. Drei Wochen nach der Geburt seiner zweiten Tochter verließ er offiziell seine Ehefrau, um mit Bente Frøge zusammenzusein.
Dieses Verhalten führte zu einem gewaltigen Medienecho in Dänemark. Bente und Lars von Trier heirateten nach beider Scheidungen 1997. Bente Trier brachte nach einigen Jahren Zwillingssöhne zur Welt. Die Ehe hielt bis 2015. Lars von Trier ist zum zweiten Mal geschieden.

Von Triers psychische Probleme führten zu Alkoholismus und Tablettenabhängigkeit. In einem Interview mit der Tageszeitung "Politiken" gab der Regisseur an, in berauschtem Zustand besonders produktiv zu sein und gab gleichzeitig seiner Sorge Ausdruck, ohne Rauschmittel keine Filme mehr machen zu können.

Nach dem externen Abitur begann von Trier 1976 ein Studium der Filmwissenschaften an der Universität Kopenhagen, bei dem er vor allem Leute kennenlernte, die ihm halfen, seine Filme zu verwirklichen. Von 1979 bis 1982 absolvierte er die Dänische Filmhochschule in der Fachrichtung "Regie". 1980 gewann Lars von Trier mit seinem Kurzfilm "Nocturne" den ersten Preis beim Festival der Filmhochschulen in München.

Seine 55-minütige Regie-Abschlussarbeit an der Dänischen Filmhochschule "Befrielsesbilleder/Bilder der Befreiung" von 1982 wurde auf dem Münchner Filmfest als bester Film des Jahres ausgezeichnet. Der Film wurde aus drei verschiedenen Materialien collagiert: erstens einem in Farbe neu gedrehten „Handlungsteil“, zweitens vorhandenem Dokumaterial in Schwarz-Weiß aus der Zeit des Nationalsozialismus, wie die Vorführfahrt des „Wessel-Mörders“ im Auto oder Filmausschnitten über die Festnahme von Kollaborateuren des nationalsozialistischen Deutschlands in Dänemark, und drittens verschiedenen Filmausschnitten in 1950er-Jahre-Farben von jeweils einem Vogel, der auf der Spitze eines Baumes zwitschert. Im Film wird wenig in Deutsch, Dänisch oder Englisch gesprochen. "Bilder der Befreiung" zeigt keine glücklichen Menschen, wie der Titel vermuten lassen könnte, sondern deutsche Soldaten, Kollaborateure und dänische Frauen, die mit den deutschen Besatzern zusammenwaren, als Verlierer und Opfer.

Viele der filmischen Ideen, wie z. B. die Unterbrechung des Handlungsverlaufs, Unterlegung des Films mit Chorälen oder die Kombination SS-Uniformen, Wasser und Feuer, tauchen in seinen späteren Filmen wieder auf.

Neben seinen Spielfilmen drehte Lars von Trier auch Werbespots und Musikvideos. 1983 drehte er zusammen mit Vladimir Oravsky für das dänische Popduo Laid Back "Elevator Boy" und 1990 das spektakuläre "Bakerman"-Video, das die Musiker beim Musizieren in freiem Fall beim Fallschirmsprung filmte.

2005 schrieb Lars von Trier bei einer Folge der dänischen Comedy-Serie "Klovn" mit. Die entsprechende Folge namens "It's a Jungle Down There" lässt sich auf der Webseite des amerikanischen Filmverleihs drafthouse films gratis herunterladen. Als Christian Ulmen 2017 mit jerks die dänische Serie nach Deutschland adaptierte, diente von Triers Folge der zweiten Episode "Camilla" als direktes Vorbild.

1984 kam sein erster Langfilm, der Krimi "The Element of Crime", ins Kino.

In "The Element of Crime" wird der Faszination für faschistoide Symbole noch mehr Raum gegeben als im Vorgängerfilm. "The Element of Crime" ist der erste Teil einer Europatrilogie, die sich mit der Geschichte Europas im 20. Jahrhundert, Überbleibseln archaischer Gesellschaftsformen und dem Verfall Europas auseinandersetzt. "The Element of Crime" gewann bei den Internationalen Filmfestspielen von Cannes den Prix Vulcain de l’artiste technicien und bedeutete den nationalen und internationalen Durchbruch für von Trier. Die weiteren Teile der Trilogie waren 1987 "Epidemic," der ebenfalls Wettbewerbsfilm in Cannes war, und "Europa" (1991), der dort ebenfalls mit dem Prix Vulcain de l’artiste technicien ausgezeichnet wurde und einen Sonderpreis der Jury sowie den Preis für den besten künstlerischen Beitrag erhielt.

1988 verfilmte Lars von Trier für Danmarks Radio die griechische Tragödie "Medea" nach Euripides. Basis seines selbst geschriebenen Drehbuchs war das bereits vorhandene Skript von Carl Theodor Dreyer und Preben Thomsen.

1991 startete von Trier zusammen mit Niels Vørsel das Filmprojekt "Dimensions," die Langzeit-Verfilmung einer polizeilichen Intrige, die auf jährlich drei Minuten Drehzeit beschränkt sein sollte und die (unter anderem mit dem Schauspieler Udo Kier), an verschiedenen Drehorten in Europa gedreht und im Jahr 2024 fertiggestellt werden sollte. Gedreht wurde ohne Drehbuch und auf Englisch. Der Schauspieler Eddie Constantine starb 1993. Nach Angaben der Zeitung "Die Welt" hat von Trier das Projekt inzwischen aufgegeben, da er mit anderen Projekten ausgelastet und die von ihm ausgesuchte Nachfolgerin für die Regie Katrin Cartlidge zwischenzeitlich verstorben ist. Es gibt eine 27 Minuten lange Version des im Jahr 2010 abgebrochenen Projektes.

Die TV-Serie "Hospital der Geister/Geister/Riget/The Kingdom" von 1994 spielt im zweitgrößten dänischen Krankenhaus Rigshospitalet. Lars von Trier ließ sich dafür von "Twin Peaks" inspirieren. Lars von Trier und Niels Vørsel schrieben das Drehbuch sehr schnell und unter möglichst großem Spaß. Die Serie war in Dänemark ein großer Erfolg, da sie sowohl äußerst lustig als auch sehr spannend war und von Folge zu Folge immer gruseliger wurde. 1997 folgte die zweite Staffel. Eine eigentlich geplante Fortsetzung scheint kaum mehr möglich, da zwei Schauspieler (Rollen: Frau Drusse, Dr. Helmer) verstorben sind und Lars von Trier und Niels Vørsel nicht mehr zusammenarbeiten.
Mit "Breaking the Waves" von 1996 beginnt die sogenannte Golden-Heart-Trilogie. Die bis dahin unbekannte Hauptdarstellerin Emily Watson übernahm die Rolle der jungen Bess McNeill, welche sich in dem Wahn, dadurch ihre große Liebe (dargestellt von Stellan Skarsgård) retten zu können, von einem akzeptierten Mitglied der Gemeinschaft zu einer Dorfprostituierten entwickelt. Der international bekannte dänische Künstler Per Kirkeby gestaltete die Kapitelbilder, die den Film gliedern. Es handelt sich um Landschaftsaufnahmen, die minutenlang zu sehen sind und sich dabei minimal verändern.

1998 nahm Lars von Trier mit dem zweiten Dogma-Film "Die Idioten/Idioterne" am Filmfest von Cannes teil. Der Film provozierte stark durch sein Thema „Irre spielen“, die ersten pornographischen Darstellungen in einem Spielfilm und durch die sehr unruhige und wacklige Kameraführung.

Für das technisch aufwendige Musical "Dancer in the Dark," in dem einzelne Tanzszenen mit unzähligen Kameras gleichzeitig gefilmt wurden, erhielt von Trier 2000 die "Goldene Palme" in Cannes. Björk schrieb nicht nur die gesamte Filmmusik, sondern spielte auch die Hauptfigur Selma. Ähnlich wie in "Breaking the Waves" opfert eine Frau ihr Leben für ihre Liebe. Hier kein Ehemann, sondern der geliebte und von Erbkrankheit bedrohte Sohn.

"Dancer in the Dark" kann sowohl zur Golden-Heart- als auch zur USA-Trilogie gerechnet werden.

Mit "Dogville" begann von Trier seine filmische USA-Trilogie, die dort bei einigen Kritikern bereits deshalb auf Vorbehalte stößt, weil der Regisseur aufgrund seiner Flugangst selbst nie dort gewesen ist. Den Vorwurf kommentierte von Trier in Anspielung auf den Film "Casablanca" mit der Feststellung, dass die Amerikaner auch nicht in Marokko gewesen seien. Vor allem störten die Kritiker sich an der aus ihrer Sicht einseitigen Darstellung der Dorfgemeinschaft in Dogville.

Mit dem Film "Manderlay" von 2005 führte er die mit "Dogville" begonnene Geschichte fort. Die Hauptfigur Grace ist nicht mehr mit Nicole Kidman besetzt, sondern mit Bryce Dallas Howard.

Beide Filme arbeiten mit Brechts Theater, in dem stets klargemacht wird, dass man nur ein „Schau-Spiel“ sieht. Emotionale Distanz ist erwünscht und wird gezielt erzeugt. Gefilmt wurde in schwarz gestrichenen Hallen, in denen nur die nötigsten Requisiten standen. In "Dogville" wurden die gedachten Häuser als zusätzlicher Verfremdungseffekt nur durch weiße Linien auf dem Hallenboden kenntlich gemacht.

2008 drehte von Trier in Nordrhein-Westfalen den Horror-Thriller "Antichrist" mit Charlotte Gainsbourg und Willem Dafoe in den Hauptrollen. Der Film erhielt 2009 eine Einladung in den Wettbewerb der 62. Internationalen Filmfestspiele von Cannes und brachte von Trier eine Nominierung für den Europäischen Filmpreis und den dänischen Robert in den Kategorien "Regie" und "Drehbuch" ein. Er gewann den nordischen Filmpreis 2009. Der heftig umstrittene Film festigte von Triers Ruf als Skandalfilm-Regisseur.

Im Sommer 2010 drehte von Trier den Spielfilm "Melancholia" mit internationaler Besetzung in Schweden. Der Film ist aus dem nicht verwirklichten Projekt entstanden, Jean Genets Stück "Die Zofen" mit Penélope Cruz zu verfilmen.

Er entspricht im Aufbau einer Oper, d. h., er besteht aus einer Ouvertüre, zwei Akten und einem Finale. Die Ouvertüre besteht aus verschiedenen Standbildern ohne Ton und Handlung, die sich minimal bewegen. Diese Einleitung dauert acht Minuten und ist eine Weiterentwicklung der Kapitelbilder in "Breaking the Waves." Die Filmmusik entstammt Richard Wagners "Tristan und Isolde".

"Melancholia," der 2011 fertiggestellt wurde, brachte von Trier seine neunte Einladung in den Wettbewerb der Internationalen Filmfestspiele von Cannes ein.

"Nymphomaniac" kam am 20. Februar 2014 als "Nymphomaniac Volume I" in die deutschen Kinos. In einer Langfassung wurde der Film auf den 64. Internationalen Filmfestspielen Berlin 2014 gezeigt. Dem Film ging eine mehrjährige Werbekampagne voraus.

Die Gruppe der Filme "Antichrist", "Melancholia" und "Nymphomaniac" wird häufig als die "Trilogie der Depression" bezeichnet.

Lars von Trier dreht seit Anfang 2017 einen Thriller über einen Serienmörder in den USA der 1970er-Jahre. Im Cast sind Matt Dillon, Uma Thurman und Bruno Ganz. Gedreht wird wieder auf Englisch. Der Film soll Ende 2018 in die Kinos kommen. Seine Weltpremiere wird "The House That Jack Built" im Rahmen der 71. Internationalen Filmfestspiele von Cannes feiern, wo er außer Konkurrenz läuft.

1996 war Kopenhagen Kulturhauptstadt Europas. Lars von Trier nahm mit einem von Ameisen in Neu-Mexiko in Echtzeit gesteuerten Theater, das in der Kopenhagener Kunstvereinigung stattfand, teil. Die über acht Wochen dauernde Inszenierung wurde von Morten Arnfred geleitet. Dokumentiert wurde das Ganze von Jesper Jargil in dem Film De Udstillede (Die Ausgestellten).

Von Trier gab 2004 bekannt, dass er sich trotz zweijähriger Vorbereitung nicht in der Lage sehe, den "Ring des Nibelungen" wie geplant für die Richard-Wagner-Festspiele 2006 in Bayreuth zu inszenieren, da die Inszenierung des vierteiligen Opern-Zyklus von ca. 16 Stunden Spieldauer seine Kräfte übersteigen würde.

1992 gründete von Trier zusammen mit dem Produzenten Peter Aalbæk Jensen die Filmproduktionsfirma Zentropa, die heute die erfolgreichste und größte Produktionsstätte für Film (TV und Kino) in Dänemark ist. Zentropa wurde mit dem "Douglas-Sirk-Preis" ausgezeichnet. Lars von Trier bezieht ein festes Gehalt und ist nach eigener Aussage nicht im Management. Er trägt aber mit seinen kommerziell erfolgreichen Filmen und seiner hohen Medienpräsenz maßgeblich dazu bei, dass Filme anderer finanziert werden können. Der Name Zentropa stammt aus seinem Film Europa – so heißt die Eisenbahngesellschaft im Film. Die Tochterfirma Zentropas, Innocent Pictures, die „frauenfreundliche“ Pornos produziert hat, gibt es nicht mehr.

Ein Mediencoup gelang Lars von Trier mit seinem "Dogma 95"-Manifest. Am 20. März 1995, dem 100. Geburtstag des Films, warf Lars von Trier einen Haufen Flugblätter mit dem Manifest vor die versammelte Presseschar im Pariser Odeon-Theater. 2008 wurde die Dogma-Bewegung um von Trier, Vinterberg, Levring und Kragh-Jacobsen mit dem Europäischen Filmpreis in der Kategorie "Beste europäische Leistung im Weltkino" bedacht.

Lars von Trier gilt als „Enfant terrible“ der Filmindustrie. Bereits sein Dogma-Film Idioten/Idioterne (1998) sorgte mit einer Kombination aus expliziten sexuellen Darstellungen und dem provozierenden „Irrsein“ der Figuren für einen internationalen Skandal. Auch sein Werk "Antichrist" wurde aufgrund der expliziten und extrem gewalttätigen Darstellung kontrovers diskutiert. "Die Welt" nannte ihn den „meistgehassten Film“ des Jahres. Von Trier gab an, seit längerer Zeit unter Depressionen gelitten zu haben und einen Teil davon in seinen Filmen zu verarbeiten. In Cannes hatte er wiederholt mit pornografischen oder gewalttätigen Szenen in seinen Filmen oder kontroversen Äußerungen provoziert. In einem Interview der "Zeit" sagte er unter anderem: „Meine Familie hatte sehr genaue Vorstellungen von Gut und Böse, von Kitsch und guter Kunst. Mit meiner Arbeit stelle ich all das in Frage. Ich provoziere nicht nur die anderen, ich erkläre mir, meiner Erziehung, meinen Werten, auch ständig selbst den Krieg. Und ich attackiere die Gutmenschen-Philosophie, die in meiner Familie herrschte.“

In seinem Film "Europa" spielt Lars von Trier die Rolle des gekauften Juden, der von einem amerikanischen Oberst angeheuert wird, um dem Besitzer der Eisenbahngesellschaft Zentropa namens Hartmann einen Persilschein auszustellen, also um ihn mit Falschaussagen von seinen begangenen Verbrechen „reinzuwaschen“. Hartmann bringt sich trotz der erkauften Entnazifizierung in der Badewanne um.

Lars von Triers biologischer Vater hieß ebenfalls Hartmann und war deutscher Abstammung, sein rechtlicher und sozialer Vater hatte jüdische Vorfahren.

Im Mai 2011 wurde von Trier von den 64. Internationalen Filmfestspiele von Cannes ausgeschlossen. Auf der Pressekonferenz zu seinem Film "Melancholia" hatte Trier dort zuvor mit Äußerungen, die unter anderem auf ironische Weise vermeintliche Sympathie und Verständnis für Adolf Hitler bekundeten, einen Eklat ausgelöst. Von Trier entschuldigte sich wenig später für seine „falschen“ und „dummen“ Äußerungen. Der Vorfall fand ein internationales Medienecho und führte zu Abbestellungen des Films "Melancholia" seitens israelischer und argentinischer Filmverleiher. Anfang Oktober 2011 wurde von Trier erstmals von der dänischen Polizei wegen seiner umstrittenen Äußerungen vernommen. Ihm drohte nach eigenen Angaben eine Anklage wegen der Verharmlosung von Kriegsverbrechen. Anfang Dezember 2011 wurde die Anklage gegen von Trier jedoch fallengelassen. Die Staatsanwaltschaft äußerte sich dahingehend, dass hinter von Triers Aussagen keine Intentionen zur Verharmlosung von Kriegsverbrechen zu vermuten seien, seine Aussagen seien in erster Linie auf die Stresssituation im Interview zurückzuführen.

Nach dem katastrophalen Ausgang der Cannes-Pressekonferenz verordnete von Trier sich ein mehr als dreijähriges öffentliches Schweigen, das er mit einem großen Interview mit der Tageszeitung "Politiken" Ende 2014 brach.

Im Mai 2018 darf von Trier erstmals nach sieben Jahren wieder an den Filmfestspielen von Cannes teilnehmen, nachdem er 2011 von den Veranstaltern zur "persona non grata" erklärt wurde. Bei seiner neuerlichen Teilnahme an den Filmfestspielen stellt er sein Werk "The House That Jack Built" außerhalb des Wettbewerbs vor.

Drehbuch und Regie:

Nur Drehbuch:
Produktion:
Schauspieler:



Dänisch



</doc>
<doc id="2920" url="https://de.wikipedia.org/wiki?curid=2920" title="Aussage (Logik)">
Aussage (Logik)

Eine Aussage im Sinn der aristotelischen Logik ist ein sprachliches Gebilde, von dem es sinnvoll ist zu "fragen", ob es wahr oder falsch ist (so genanntes Aristotelisches Zweiwertigkeitsprinzip). Es ist nicht erforderlich, "sagen" zu können, ob das Gebilde wahr oder falsch ist. Es genügt, dass die Frage nach Wahrheit („Zutreffen“) oder Falschheit („Nicht-Zutreffen“) sinnvoll ist, – was zum Beispiel bei Fragesätzen, Ausrufen und Wünschen nicht der Fall ist. Aussagen sind somit Sätze, die Sachverhalte beschreiben und denen man einen Wahrheitswert zuordnen kann.

Die einleitend referierte Bedeutung des Ausdrucks "Aussage" ist die herrschende Bedeutung.

Der Terminus "Aussage" wird jedoch mehrdeutig verwendet.

Diese lassen sich auf vier Grundbedeutungen reduzieren:

Von der Bedeutung des Ausdrucks Aussage hängt ab, „was eigentlich genau der Gegenstand der Logik“ und was eigentlich „Träger“ von Wahrheit oder Falschheit ist. Für einen technischen Gebrauch der Logik bedarf es einer Klärung dieser Frage allerdings nicht.

Nach verbreiteter, aber umstrittener Auffassung sind Aussagen nicht Sätze, sondern sind Aussagesätze (nur) der sprachliche Ausdruck von Aussagen. Ein Aussagesatz steht stellvertretend für eine Aussage, ist lediglich ein Zeichen für eine Aussage (Proposition) und nur „das sprachliche Korrelat der Aussage“.

Gegen eine Gleichsetzung von Aussage und Aussagesatz wird eingewandt, dass von dem Satztyp und seiner Äußerung die Aussage zu unterscheiden ist, „die mit dieser Äußerung gemacht wird“.

"Beispiel 1": (gleichbedeutende Sätze): „Das Haus ist dreistöckig.“ – „Dieses Wohngebäude hat drei Geschosse.“ – “This house has three floors.”: "drei" Sätze mit "einer" Aussage für "einen" Sachverhalt.

"Beispiel 2": Wenn Hans und Ina sagen „Ich bin krank“, dann äußern beide denselben Satz (i. S. v. Satztyp) und erzeugen unterschiedliche Satzvorkommnisse und machen mit ihren Äußerungen unterschiedliche Aussagen.

Nach Quine soll es der Annahme von Propositionen nicht bedürfen, sodass sich der Terminus „Aussage“ nicht auf Ausgesagtes, sondern nur auf Aussagesätze beziehen können soll.

Tugendhat spricht in einer Grobeinteilung von einer sprachlichen, psychologischen und ontologischen Grundauffassung der Logik: der sprachliche Aussagesatz korrespondiert dem Urteil als psychischem Akt und ontologisch der Aussage, dem Gedanken (Frege); dem Sachverhalt (Husserl: "Wittgenstein I") oder der Proposition (englische Philosophie).

Zwischen „Satz“ – „Urteil“ – „Aussage“ besteht ein proportionalitäts- und attributionsanaloges Verhältnis. D. h. der objektive Gedanke (die Aussage, Proposition) wird im Denken (psychischer Urteilsakt) erfasst und in einem Aussagesatz zur Sprache gebracht. Aussagen über den Aussagesatz betreffen daher in einem analogen Sinn auch den objektiven Aussageinhalt bzw. den psychischen Aussageakt – und umgekehrt. In den meisten Zusammenhängen kommt es daher auf eine nähere Unterscheidung nicht an. Je nach erkenntnistheoretischer Orientierung kann eine entsprechende Terminologie bevorzugt werden. Für den Rezipienten bedeutet dies, dass sachlich über das Gleiche geredet wird, mit welchen erkenntnistheoretischen Voraussetzungen auch immer. War früher der Terminus „Urteil“ naiv (Aristoteles) oder psychologistisch (Empirismus, Kant) herrschend, dominiert nach dem Linguistic Turn der Ausdruck „Satz“, mit dem der Terminus „Aussage“ konkurriert bzw. vermischt wird. Will man die schillernde Bedeutung des Ausdrucks „Aussage“ vermeiden, empfiehlt es sich, terminologisch zwischen Aussagesatz und Proposition zu unterscheiden. Dies ist im deutschen Sprachraum aber nicht üblich.

Mit Frege ist die Aussage von der Behauptung einer Aussage zu unterscheiden: „In einem Behauptungssatz ist also zweierlei zu unterscheiden: der Inhalt, den er mit der entsprechenden Satzfrage gemein hat, und die Behauptung. Jener ist der Gedanke oder enthält wenigstens den Gedanken. Es ist also möglich, einen Gedanken auszudrücken, ohne ihn als wahr hinzustellen. In einem Behauptungssatze ist beides so verbunden, dass man die Zerlegbarkeit leicht übersieht. Wir unterscheiden demnach

Für die Aussagenlogik ist es unerheblich, ob die Eigenschaft eine Wertung enthält, d. h. die Aussage ein „Werturteil“ ist.

Die Aussage (der Aussagesatz) ist zu unterscheiden von der "Aussageform". Eine Aussageform ist „ein Ausdruck, der eine (oder mehrere) freie Variable (Leerstellen) enthält und durch die Belegung aller freien Variablen in eine (wahre oder falsche) Aussage übergeht.“ Die Aussageform geht in eine Aussage über, sobald die Variable ersetzt wird.

In der mathematischen Logik wird der syntaktische Aufbau einer Aussage basierend auf den Zeichen einer Sprache "L" formal spezifiziert.
Je nach Sprache sind verschiedene atomare Aussageformen erlaubt, aus denen durch Junktoren zusammengesetzte Aussageformen gebildet werden. Bei der Prädikatenlogik kommt die Möglichkeit hinzu, in den atomaren Aussageformen enthaltene Variablen durch Quantoren („es gibt ein x, für das gilt“, „für alle x gilt“) zu binden. Eine durch keinen Quantor gebundene Variable heißt "freie Variable".

Eine logische Aussage ist formal definiert als eine Aussageform (Definition siehe dort) über der Sprache "L", in der keine (freien) Variablen vorkommen.

Ein einzelnes "Wort", das nicht für eine Aussage steht, „teilt nichts mit“, „ist nicht wahr oder falsch“. „Nur wenn ein Wort als Abkürzung für einen Satz steht, können wir von seiner Wahrheit oder Falschheit sprechen, …“.

Das zur Abgrenzung vom Wort Gesagte gilt entsprechend (eigentlich) für den Begriff.

Hinter jedem Begriff stehen eine oder mehrere Aussagen, welche seinen Inhalt definieren und diesen Begriff in eine Relation zu anderen bringen. „Deshalb führt die Feststellung, dass der Begriff in seinem Inhalt eine Einheit von Merkmalen ist, zu der Idee, dass jeder Begriff einen Zusammenhang von Aussagen darstellt.“ Dies wurde insbesondere von Cohn vertreten und klingt auch bei Frege an, wenn dieser sagte, dass das Wort nur im Satz eine Bedeutung habe.

„Jede Aussage, in der einem Gegenstand etwas zugesprochen wird, kann man als eine Art Schlusses betrachten, dessen Prämissen das Subjekt der fraglichen Aussage definieren und dem es definierenden Begriff eine Eigenschaft zu- oder absprechen.“

Aussagen können in einfache Aussagen und zusammengesetzte Aussagen eingeteilt werden.
Einteilungsgrund ist dabei, ob Aussagen aus voneinander unterscheidbaren „separierbaren“ Teilaussagen zusammengesetzt sind oder nicht.

"Beispiel": „Berlin ist eine Stadt“ (einfache Aussage); „Berlin ist eine Stadt mit mehr als 3 Millionen Einwohnern“ (logisch eine zusammengesetzte Aussage mit den Teilaussagen „Berlin ist eine Stadt“ und „Berlin hat mehr als 3 Millionen Einwohner“).

Die "Terminologie" variiert: statt von „einfacher Aussage“ spricht man auch von
„unzusammengesetzter Aussage“, „atomarer Aussage“, „elementarer Aussage“, „Elementaraussage“ oder „Elementarsatz“ (Wittgenstein). Statt von zusammengesetzter Aussage ist auch die Rede von „Aussagenverknüpfung“ oder „molekulare Aussage“. Als "atomare Aussagen" werden in der mathematischen oder formalen Logik Aussagen bezeichnet, die nicht aus anderen Aussagen zusammengesetzt sind. Sie enthalten daher keine aussagenverknüpfenden logischen Operatoren (Junktoren) wie ∧ (und), ∨ (oder) und ¬ (nicht). Der Gegenbegriff ist die zusammengesetzte Aussage oder Aussagenverknüpfung.

Ist z. B. die Aussage "Die Straße ist nass und es regnet" in zwei Aussagen zu trennen, die durch das "und" zu einer Aussage verknüpft sind, ist eine solche Trennung bei den einzelnen Aussagen "Die Straße ist nass" und "Es regnet" nicht mehr möglich. Somit handelt es sich bei diesen Aussagen um atomare Aussagen. Bei einer aussagenlogischen Analyse von Argumenten ist es zentral, die Formulierungen in atomare Aussagen zu untergliedern, da nur so die für die Argumentstruktur wichtigen Junktoren formalisiert werden können.
In einer einfachen Aussage wird einem Gegenstand ein einziges Prädikat zu- oder abgesprochen.

Wenn es heißt, eine einfache Aussage sei nicht weiter strukturiert, so ist dies dahingehend zu verstehen, dass die innere Struktur einer Aussage nicht weiter präzisiert wird.

Die Interpretation der atomaren Aussagen erfolgt durch Zuordnung von Wahrheitswerten.

Die Symbole für einfache Aussagen sind eine Frage der Konvention. Gebräuchlich ist zum Beispiel die Kennzeichnung durch Großbuchstaben A, B, C, ggf. mit indizierten Buchstaben.

Eine zusammengesetzte Aussage ist eine Aussage, die durch Verbindung mehrerer einfacher Aussagen entsteht.

Eine Aussagenverknüpfung kann extensional (extensionale Aussagenverknüpfung) oder intensional (intensionale Aussageverknüpfung) erfolgen.

Extensionale Aussagenverknüpfungen sind zusammengesetzte Aussagen, deren Wahrheitswert von dem Wahrheitswert ihrer Teilaussagen bestimmt ist. Der Wahrheitswert der Gesamtaussage ist daher eine Funktion der Wahrheitswerte der Teilaussagen (Wahrheitsfunktionalität).

Logische Konstanten, die eine wahrheitsfunktionale Aussagenverbindung bewirken, werden Junktoren genannt.

Die klassische Aussagenlogik ist eine Junktorenlogik (Lorenzen), eine „Logik der Wahrheitsfunktionen“ (Quine) von Aussagen. Sie gründet auf dem Extensionalitätsprinzip.

Einen Sonderfall stellt die Negation dar. Dies jedoch aus mehr terminologischen und praktischen Gründen. Bei der Negation werden keine Aussagen verknüpft und sie ist daher auch keine Aussagenverknüpfung. Sie wird gleichwohl aus Gründen terminologischer Vereinfachung einstellige Aussageverknüpfung genannt. Sie liefert beim Eingangswert "wahr" den Wert "falsch" und umgekehrt. Terminologisch zutreffender erscheint hier der Ausdruck einstellige Wahrheitsfunktion.

Für die Kombination zweier Aussagen gibt es sechzehn zweistellige Verknüpfungen (Junktoren). Sie geben für alle möglichen Kombinationen von Wahrheitswerten einen für diese Verknüpfung typischen Ergebniswahrheitswert an. Zum Beispiel ist die mit der Konjunktion verknüpfte Aussage "a UND b" nur dann wahr, wenn sowohl a als auch b wahr ist; in jedem anderen Fall ist die Konjunktion falsch.

Intensionale Aussagenverknüpfungen sind nicht-wahrheitsfunktionale Aussagenverknüpfungen. Bei diesen hängt der Wahrheitswert der Gesamtaussage nicht von dem Wahrheitswert der Teilaussagen ab.

"Beispiel": „Anton liest ein Buch über Logik, weil er Logik unglaublich spannend findet“.

Aussagen werden traditionell in analytische Aussagen und synthetische Aussagen unterteilt. Statt von ‚Aussage‘ ist gleichsinnig auch von ‚Satz‘ bzw. ‚Urteil‘ die Rede (vgl. oben die Tugendhat’sche Dreiteilung).

Analytische Aussagen


Nach Ernst Tugendhat gründen alle analytischen Sätze auf dem Satz vom ausgeschlossenen Widerspruch. Sie haben keinen potentiellen Falsifikatoren.

Synthetische Aussagen


Die Berechtigung der Unterscheidung zwischen analytischen und synthetischen Aussagen (Urteilen) wurde von Quine angegriffen. Er vertrat eine These der "Unbestimmtheit der Bedeutung" und stellte grundsätzlich infrage, dass Begriffsbedeutungen scharf voneinander abgegrenzt werden können.

In der Aussagenlogik ist für solche Aussagen nur ihr formaler und nicht ihr inhaltlicher Wahrheitswert von Bedeutung. Beispielsweise muss man Kenntnis vom beschriebenen Sachverhalt haben, um den Wahrheitswert der Aussage „Berlin ist die Hauptstadt von Deutschland, und Rom die Hauptstadt von Italien“ beurteilen zu können; dies ist nicht erforderlich bei der Aussage „Madrid ist die Hauptstadt von Spanien, oder Madrid ist nicht die Hauptstadt von Spanien“, denn nach Festlegung (Normierung) des Gebrauches des logischen Oder und Nicht handelt es sich hier um eine wahre Aussage unabhängig davon, ob Madrid nun wirklich die Hauptstadt von Spanien ist oder nicht. Eine in diesem Sinn formal wahre Aussage wird allgemeingültig oder auch Tautologie genannt.

Eine Aussage in der Prädikatenlogik ist eine Aussageform ohne freie Variable. (Alle in ihr enthaltenen Variablen sind durch Quantoren gebunden.)

In der Prädikatenlogik ergibt sich der Wahrheitswert einer Aussage aufgrund der Interpretation der in ihr enthaltenen Symbole. Beispielsweise ist die Aussage formula_1 wie folgt ermittelbar: Für jedes x werden die Terme x und x+x berechnet. Wenn es ein x gibt, so dass beide Terme denselben Wert erhalten (z. B. für x = 0), so ist die Aussage wahr, andernfalls falsch. Somit hängt der Wahrheitswert der Aussage von der Grundmenge (auch Universum, Domäne, Wertebereich, Individuenbereich genannt) ab, aus der Belegungen für die Variablen stammen dürfen.

Ist eine Aussage bei jeder Interpretation wahr, z. B. formula_2, so nennt man sie allgemeingültig oder auch Tautologie.

Die Modelltheorie ist die mathematische Teildisziplin, die sich mit der Frage beschäftigt, welche Modelle es für welche Mengen von Aussagen gibt.




</doc>
<doc id="2922" url="https://de.wikipedia.org/wiki?curid=2922" title="Gesetze der Form">
Gesetze der Form

Gesetze der Form (im Original englisch "Laws of Form," kurz: LoF) ist der Titel eines Werks von George Spencer-Brown aus dem Jahr 1969, das Philosophie der Logik, mathematische Grundlagenforschung, Kybernetik und Erkenntnistheorie berührt. Das Buch stellt drei aufeinander aufbauende graphische Kalküle vor, die Spencer-Brown mit dem Ziel einer universellen Algebra entwickelt:


Alle Kalküle beruhen auf der einzigen grundlegenden "Operation des Unterscheidens". Gelegentlich wird auch nur die primäre Algebra als "Gesetze der Form" bezeichnet.

Der Ausgangspunkt von Spencer-Brown ist die logische Form der Unterscheidung. Der als „Triff eine Unterscheidung!“ "(draw a distinction)" umschriebene basale Akt wird im Weiteren mit sich selbst kombiniert und erzeugt auf diesem Wege eine Vielfalt neuer Formen, die als Grundbegriffe (wahr und falsch, Symbol, Signale, Namen, Prozesse, sich selbst verändernde Formen, Operator usw.) angesehen werden können. Mit diesen Begriffen lassen sich formale Kalküle der Logik und der Mathematik darstellen.

Spencer-Brown stellt im Gange seiner Untersuchung die klassische Logik als Grundlage der Mathematik in Frage und interpretiert sie neu. Bedeutend sind die erkenntnistheoretischen Konzepte der "Laws of Form". „Überhaupt nichts kann durch Erzählen gewusst werden“ ist eine der Kernaussagen der "LoF." Wissen erlangt man demzufolge nur in der Erfahrung aus den Ergebnissen praktischen Handelns. Im Unterschied zu klassischen Kalkülen der Logik, die die logische Struktur von Sachverhalten und Aussagen abbilden sollen, versteht George Spencer-Brown die logische Form als etwas, das der Erkenntnis als Prozess und Handlung entspricht. Während es für eine formale Sprache beliebig ist, was ihre nicht-logischen Konstanten bezeichnen, wird in den "Laws of Form" das Unterscheiden und Bezeichnen selbst – als simultaner Akt – zum Ausgangspunkt für formale Operationen. Spencer-Browns Kalkül liefert also nicht nur eine formale Syntax und Semantik, sondern auch eine formale Semiotik. Darüber hinaus bietet das Konzept der "Re-entry of the form (into the form)" die Möglichkeit, eine formale Vorstellung von Zustandswechsel und Gedächtnis zu formulieren.

Die "Laws of Form" werden von ihren Anhängern als Minimalkalkül für mathematische Wahrheiten betrachtet. Vertreter mehrerer Wissenschaftszweige berufen sich daher explizit auf die "Laws of Form," etwa Niklas Luhmann in seiner soziologischen Systemtheorie oder Humberto Maturana in der Theorie des radikalen Konstruktivismus.

Spencer-Browns formaler Kalkül wurde 1969 erstmals unter dem Titel "Laws of Form" veröffentlicht. Das Buch wurde seitdem mehrmals neu aufgelegt und in verschiedene Sprachen übersetzt. Die Idee zum Buch entwickelte Spencer-Brown bereits während seiner Tätigkeit als Ingenieur bei British Railways, in der er Ende der 50er Jahre damit beauftragt war, elektrische Schaltungen für das Zählen von Waggons in Tunneln zu entwickeln. Dabei bestand das – damals fundamentale – technische Problem, die Zähler vorwärts und rückwärts zählen zu lassen und bereits gezählte Waggons zu speichern. Spencer-Brown löste das Problem durch die Verwendung bis dato unbekannter "imaginärer Boolescher Werte" und es „entstand zugleich ein neues Problem: Seine Idee funktionierte, aber es gab keine mathematische Theorie, die diese Vorgehensweise rechtfertigen konnte“. Die Ausarbeitung des Kalküls, der diese neuen imaginären Booleschen Werte zuließ, war der Auslöser für die "Laws of Form".

Das Buch umfasst 141 Seiten, von denen 55 den mathematischen Kalkül umfassen, und gilt als für Laien nur schwer verständlich. Es existieren eine Reihe von „Erläuterungsbüchern“ zu den "LoF."

Dem ersten Kapitel der "Laws of Form" sind sechs chinesische Schriftzeichen vorangestellt, die wie folgt übersetzt werden können: „Der Anfang von Himmel und Erde ist namenlos“. Ohne Bezeichnungen ist die Welt leer und unbestimmt. Das Bezeichnen von etwas setzt jedoch eine Unterscheidung voraus: Das Bezeichnete muss vom Rest unterschieden werden. Die Vorstellung der Bezeichnung und des Unterscheidens bilden für Spencer-Brown den Ausgangspunkt, wobei er der Unterscheidung logischen Vorrang einräumt: „We take as given the idea of distinction and the idea of indication, and that we cannot make an indication without drawing a distinction.“

Die Unterscheidung teilt die anfängliche Unbestimmtheit in Bereiche. Die Unterscheidung („distinction“) ist dann eindeutig, wenn sie die Bereiche vollständig voneinander trennt, sodass ein „Punkt von der einen Seite nur auf die andere gelangt, indem er die gemeinsame Grenze kreuzt“. Dadurch, dass die Unterscheidung eine geschlossene Grenze zieht, konstituiert sie das, was sie umschließt, als bezeichenbares Objekt und somit einen Unterschied von Innen und Außen. Das Objekt wird von der Unterscheidung exakt und vollständig umschlossen. („Distinction is the perfect continence“). Es genügt daher, die Form der Unterscheidung als einziges Symbol einzuführen („We take therefore the form of distinction for the form“). Die so getroffene Unterscheidung kann durch ein Symbol auf der Innen- oder Außenseite "(token)" "markiert" werden.

In der Literatur wird als Beispiel gelegentlich ein Kreis auf einem weißen Blatt Papier angeführt: Der Kreis trennt eindeutig außen von innen in dem Sinn, dass man von außen nach innen oder umgekehrt nur gelangen kann, wenn man die Kreislinie „überschreitet“ "(cross)." Die vollständig eingeschlossene Kreisfläche ist als Innenseite („angezeigte“ Seite) eindeutig vom umgebenden Raum unterschieden – "unmarked space" (engl., wörtlich „unmarkierter Raum“).

Spencer-Brown verwendet für die Markierung einer Unterscheidung das englische Wort „cross“, was als Substantiv (Markierung), aber auch als Aufforderung (kreuze!) gelesen werden kann und soll. Das ist insofern bedeutsam, als zu einem späteren Zeitpunkt in den "Laws of Form" der Begriff der „Markierung“ eingeführt wird. Das Treffen einer Unterscheidung wird durch die o. a. Definition mit dem Kreuzen einer Grenze und den unterschiedlichen Werten der Seiten einer Unterscheidung gleichgesetzt. Durch eine Unterscheidung werden zwei Grundoperationen ausgeführt: Entweder man wechselt "von einem unmarkierten Zustand in einen markierten Zustand" (z. B.: Wir gehen von einem leeren Blatt Papier aus, markieren einen „Kreis“ und gehen damit von „Nicht-Kreis“ zu „Kreis“) oder man wechselt "von einem markierten Zustand in einen unmarkierten Zustand" (z. B.: Wir gehen von einem „Kreis“ aus und gehen durch Unterscheidung über zu „Nicht-Kreis“).

Danach – in einem erkenntnistheoretischen Bezug – beschreiben die "Laws of Form" den Zusammenhang von Unterscheidung, Motiv, Wert und Nennung eines Namens: Eine Unterscheidung setzt ein Motiv (des Unterscheidenden) voraus, und es kann kein Motiv geben, wenn nicht Inhalte unterschiedlich im Wert gesehen werden. Eine Unterscheidung setzt also voraus, dass es jemanden gibt, der die Unterscheidung trifft, und dass dieser Akteur einen Wertunterschied sieht, der ihn zur Unterscheidung veranlasst. Da eine Unterscheidung einen Inhalt bezeichnet, der einen Wert besitzt, kann dieser Wert auch benannt werden, und der Name kann mit dem Wert des Inhalts identifiziert werden („Thus the calling of the name can be identified with the value of the content“).

Gemäß der "Laws of Form" verfügt man über zwei Wege, eine Unterscheidung zu treffen: den des "Kreuzens," also des Treffens einer Unterscheidung durch Überschreitung einer Grenze, und den des "Nennens," also der Verwendung eines Namens stellvertretend für die Unterscheidung.

Die "Laws of Form" führen dann ein Symbol für die Grenzziehung einer Unterscheidung ein, dargestellt durch das "cross:" . Dabei wird das, was links unten unter dem „Winkel“ steht, von allem anderen abgegrenzt (man kann sich das "cross" als geschlossenes Rechteck vorstellen). "cross" und "blank page" (eine leere Seite ohne Zeichen) sind die grundlegenden Ausdrücke für das Bestehen oder Nichtbestehen einer "Spencer-Brown-Form". In Textdarstellungen wird die geschlossene Grenzziehung auch durch Klammern wiedergegeben: etwa durch [ ] oder < >, die leere Seite durch einen Punkt „.“ oder durch „{}“. Die allgemeine durch das "cross" angezeigte Form entspricht einer Grenzziehung, die einen Bereich von einem anderen trennt. Sie besagt so viel wie Hier-So! und dort – jenseits der Grenze – auf jeden Fall Nicht-So! Neben dem Winkel sind daher auch andere Symbole möglich, etwa eine Einkreisung.

In dieser Symbolsprache lassen sich die obigen beiden Grundaxiome wie folgt formalisieren:

Danach führt Spencer-Brown das Konzept der "Tiefe des Raumes" ein, das ineinander verschachtelte Symbole erlaubt, und daher zu strukturell reichen Formen führt. Des Weiteren werden vier fundamentale Kanons vorgestellt, die Regeln für die Behandlung solcher Formen behandeln. Die Formen können nach obigen beiden Grundaxiomen stufenweise substituiert („gekürzt“) werden (vgl. insbesondere den 3. Kanon der Substitution: „In any expression, let any arrangement be changed for an equivalent arrangement“) wie in folgendem Beispiel:
Anmerkung zum Vorgang: Zuerst werden die beiden unteren linken 'crosses' durch Kondensation nach Axiom 1 zu einem 'cross'. Dann werden das dadurch entstandene verschachtelte 'cross' und das schon vorhandene verschachtelte 'cross' nach Axiom 2 aufgehoben. Es verbleibt ein 'cross'. 

Zum Ende des 3. Kapitels der "Laws of Form" stellt GSB klar, was er unter dem vorangegangenen sogenannten Indikationenkalkül versteht – nämlich den Kalkül, der dadurch bestimmt wird, die beiden obigen Grund-Formen als Ausgangspunkt zu nehmen („Call the calculus determined by taking the two primitive equations as initials the calculus of indication“) – und er leitet über zur primären Arithmetik, die alle Aussagen umfassen und auf alle Aussagen beschränkt sein soll, die sich aus dem Indikationenkalkül ergeben („Call the calculus limited to the forms generated from direct consequences of these initials the primary arithmetic“).

In Kapitel 4 der "LoF" führt GSB den o. a. Indikationenkalkül in eine sog. „primäre Arithmetik“ über. Ausgangspunkt sind die aus der Form der Kondensation und der Form des Kreuzens gewonnenen Handlungsanweisungen („Initiale“).

und entwickelt aus diesen beiden Initialen neun Theoreme zur primären Algebra:

Die Theoreme 8 und 9 dienen gleichzeitig als Initiale der Brownschen primären Algebra.

In Kapitel 6 der "LoF" definiert GSB die Theoreme 8 und 9 der "primären Arithmetik" ihrerseits als Initiale der primären Algebra. Auf Basis dieser Initiale demonstriert GSB neun sog. „Konsequenzen“ (Entwicklung von Formen durch folgerichtige Anwendung der erlaubten Rechenschritte): „We shall proceed to distinguish particular patterns, called consequences, which can be found in sequential manipulations of these initials.“

In Kapitel 8 der "LoF" sucht GSB zu zeigen, dass jede Konsequenz in der Algebra auf ein beweisbares Theorem über die Arithmetik hinweisen muss. Darauf folgend steht der Beweis, dass auch tatsächlich jedes Theorem über die Arithmetik in der Algebra demonstriert werden kann (Kapitel 9) sowie die Unabhängigkeit der beiden algebraischen Initialgleichungen (Kapitel 10).

Im Hinblick auf den Gödelschen Unvollständigkeitssatz haben insbesondere die postulierte gleichzeitige Vollständigkeit und Widerspruchsfreiheit der "primären Algebra" für Diskussion in der Literatur gesorgt. „Eine naheliegende Vermutung ist, dass die Sätze von Kurt Gödel hier keine Anwendung finden, weil die "Laws of Form" das Imaginäre zu repräsentieren erlauben […] Wenn das Imaginäre einem formalen System inhärent ist, lassen sich die Sätze von Gödel nicht mehr auf dieses System applizieren.“

Gleichungen 2. Grades in der Bedeutung der "LoF" erhält man, indem unendliche algebraische Ausdrücke durch Selbstbezüglichkeit als endliche Gleichungen dargestellt werden. Es wird der Begriff "imaginärer Wert" eingeführt, der die Oszillation zwischen markiertem und unmarkiertem Zustand ausdrücken soll, und der in späterer Folge zur Anwendung komplexer Werte in der Algebra verwendet wird, die ihrerseits als „Analogien zu den komplexen Zahlen in der gewöhnlichen (numerischen) Algebra“ verwendet werden können. Für die mathematische Form der Darstellung des imaginären Wertes prägt GSB den Begriff der "re-entry" („Wiedereintritt“) der Form in die Form.

Ausgangspunkt ist die Demonstration, dass die u. a. mathematische Form sich durch folgerichtige Umformung gemäß den Regeln der "LoF" in einen unendlichen Term der gleichen Wiederkehr transformieren lässt. Diese unendliche Wiederholung lässt sich in einen endlichen Formalismus (hier ausgedrückt durch formula_1) überführen, der in jeder ganzzahligen Tiefe identisch mit dem ganzen Ausdruck ist. Da die Form in ihrem eigenen Raum wieder auftritt, erhielt sie den Namen "re-entry".

Auf dieser Basis entwickelt GSB zwei derartige rekursive Funktionen: Die Funktion formula_2 („Gedächtnisfunktion“), die sowohl für als auch für den leeren Raum codice_1 erfüllt ist, sowie formula_3 („Oszillationsfunktion“), deren Lösung kein feststehender Ausdruck ist, sondern sich infinit verlängert.

formula_3 wird im Formalismus der "LoF" verwendet, um eine mathematische Form der Zeit auszudrücken. formula_5 ist nur lösbar, wenn sie mit unendlich ineinander geschachtelten formula_6 gleichgesetzt wird, und wenn formula_1 die Gleichung lösen soll, muss formula_1 infinit verlängert werden. Diese Gleichung führt – obwohl im Raum (sprich mit den Mitteln der "primären Arithmetik") nicht lösbar – dadurch zu einer Vorstellung von Zeit, indem man das „Nacheinander“ der Zustände auflöst und nur den "imaginären Zustand der Form" betrachtet.

Während der Formalismus der "LoF" sich in großen Teilen in den der Booleschen Algebra überführen lässt, besteht zwischen beiden ein fundamentaler Unterschied: Während die Boolesche Algebra die Gesetze der Logik, hier insbesondere den Satz vom ausgeschlossenen Dritten als axiomatische Grundlage verwendet, gilt diese Annahme nicht in der Brownschen Algebra. Es wird angenommen, dass die "LoF" die „unentdeckte“ Arithmetik zur Booleschen Algebra darstellen.

Im elften Kapitel der "Gesetze der Form" werden oszillierende Werte für die Formen eingeführt, die auf Selbstbezüglichkeit beruhen. Dabei kann durch einen Reentry eine bestimmte Form innerhalb ihrer selbst wieder aufgerufen werden. Die oszillierenden Werte („<>“ oder „.“) werden nicht als Widerspruch oder Syntaxfehler gedeutet, den es etwa durch eine Typentheorie zu verbieten gälte. Spencer-Brown deutet die Oszillation zwischen zwei Werten vielmehr als „mathematische Zeit“. In der Anmerkung zu Kapitel 11 wird auf die Parallele zur Wurzel aus −1 verwiesen, die sich als imaginäre Zahl auch als Oszillation zwischen 1 und −1 darstellen lässt (vgl. dazu Louis H. Kauffman). Legt man die traditionelle Darstellung der imaginären Zahlen als die Punkte der y-Achse in der komplexen Ebene zugrunde, wird die y-Achse damit zum gedanklichen Platzhalter für die Oszillation. Dieser Ansatz ist für die Physik bedeutsam, insofern diese auf komplexe Zahlen zur Beschreibung von Naturprozessen zurückgreift.

Der chilenische Biologe und Systemwissenschaftler Francisco Varela hat 1975 eine Erweiterung des Brownschen Indikationenkalküls zu einem dreiwertigen Kalkül vorgelegt.

Spencer-Brown selbst hat im Nachgang zu den "LoF" neun mathematische und philosophische Anwendungen vorgeschlagen, darunter für das Vier-Farben-Problem, die Riemannsche Vermutung, die Goldbachsche Vermutung und die Fermatsche Vermutung.

Außerhalb der Mathematik wird aus den "Laws of Form" eine besondere Bedeutung zuteil dem "Beobachterdilemma:"Jede von einem Beobachter getroffene Beobachtung (Unterscheidung) impliziert demnach eine zweite Unterscheidung. Die erste ist die (ggf. auch mehrwertige) Unterscheidung des jeweils beobachteten Gegenstands („Die Zahl der Brillenträger nimmt zu“), die zweite ist die implizit zugrundeliegende Unterscheidung, "was" man beobachtete und was "nicht" (hier etwa die Zahl der Blinden, der Hörgeräteträger, der Handybesitzer, der Gesamtbevölkerung usw.).

Diesem bei jeder Beobachtung ausgesparten Raum des Nicht-Beobachteten gibt Spencer-Brown nun den Namen "unmarked space". Bei jeder – wissenschaftlichen, erkenntnistheoretischen, phänomenologischen – Beobachtung entstehe dieser Raum. Umgekehrt sei bei dem Vergleich etwa zwischen einem Phänomen und seiner Beschreibung stets der "unmarked space" im Spiel.

Eine solche "Beobachtung der Beobachtung" wird auch „re-entry“ genannt und ist als Theoriefigur universell, also auch über die Mathematik hinaus, einsetzbar. Sie wird etwa bei dem Soziologen Niklas Luhmann als "Wiedereintritt in die Unterscheidung" übersetzt und zu einer zentralen Theoriefigur der luhmannschen Systemtheorie.

Insbesondere in der Systemtheorie fanden die "LoF" eine über die Mathematik hinausgehende Beachtung. So wurden immer wieder Parallelen der "LoF" zu grundlegenden Konzepten der Systemtheorie (z. B. Unterscheidung, Beobachtung als Trennung von Objekt und Umwelt, Erkenntnis als Konstruktion und Rekursion, etc.) gezogen. Niklas Luhmann hat darauf hingewiesen, dass er seinen grundlegenden differenztheoretischen Ansatz den "LoF" entnommen hat. Ebenso wurden Parallelen zu Konzepten des radikalen Konstruktivismus und der Soziologie hergestellt. Der deutsche Soziologe Dirk Baecker hat in zwei Aufsatzsammlungen Anwendungen und Interpretationen der "LoF" für die Soziologie zusammengestellt.






</doc>
<doc id="2923" url="https://de.wikipedia.org/wiki?curid=2923" title="Lisp">
Lisp

Lisp ist eine Familie von Programmiersprachen, die 1958 erstmals spezifiziert wurde und am Massachusetts Institute of Technology (MIT) in Anlehnung an den ungetypten Lambda-Kalkül entstand. Es ist nach Fortran die zweitälteste Programmiersprache, die noch verbreitet ist.

Auf Basis von Lisp entstanden zahlreiche Dialekte. Zu den bekanntesten zählen Common Lisp und Scheme. Daher bezieht sich der Begriff "Lisp" oft auf die Sprachfamilie und nicht auf einen konkreten Dialekt oder eine konkrete Implementierung.

Lisp steht für List Processing (Listen-Verarbeitung). Damit waren ursprünglich Fortran-Unterprogramme gemeint, mit denen symbolische Berechnungen durchgeführt werden sollten, wie sie im Lambda-Kalkül gebraucht werden. Steve Russell, einer der Studenten von John McCarthy, kam dann auf die fundamentale Idee, auf Grundlage dessen Formulierung eines "Lisp-Interpreters in Lisp" einen Interpreter für diese Ausdrücke für die IBM 704 zu schreiben. Damit war die Programmiersprache Lisp geboren.

Die Grunddatenstrukturen von Lisp sind Einzelwerte (z. B. Symbole, Zahlen, Zeichenketten), die "Atome" genannt werden, und Cons-Zellen, aus denen Listen gebildet werden. Die Listen können beliebig verschachtelt werden ("Listen von Listen"). Damit lassen sich auch leicht Datenstrukturen wie ein assoziatives Array implementieren. Listen werden zur Darstellung in Lisp in runde Klammern gefasst:
Auch Programmanweisungen sind Listen, wobei das erste Listenelement die auszuführende Funktion identifiziert. Es gibt somit keinen grundsätzlichen Unterschied zwischen Daten und Programmanweisungen; diese Eigenschaft wird Homoikonizität genannt. Der Programmierer kann so beispielsweise neue Kontrollstrukturen oder Objektsysteme (OOP) entwickeln (Metaprogrammierung, Makros). Es ermöglicht aber auch Programmteile zur Laufzeit beliebig zu manipulieren.

Lisp bietet dem Programmierer große Flexibilität und weitreichende Einflussmöglichkeiten, weshalb es manchmal auch als "programmierbare Programmiersprache" bezeichnet wird. Datenstrukturen werden dynamisch aufgebaut, ohne dass der Programmierer explizit Speicherplatz reservieren oder freigeben muss (siehe auch automatische Speicherbereinigung). Deklarationen für Daten sind nicht nötig, und ein Lisp-Symbol kann als Variable beliebige Arten von Objekten bezeichnen. Viele dieser Eigenschaften sind im Laufe der Zeit in weitere Programmiersprachen übernommen worden. Anfang der 1960er waren sie jedoch ihrer Zeit weit voraus.

In den 1970er und 1980er Jahren wurden spezielle Lisp-Maschinen entwickelt und vertrieben. Diese ermöglichten das schnelle Ausführen von Lisp-Programmen, was auf damaligen allgemeinen Computern nur unter dem Verzicht auf Typüberprüfung und automatische Speicherbereinigung möglich war. Dies hat sich durch schnellere Computer jedoch geändert.

Programme in Lisp können interpretiert oder von einem Compiler in effizienten Code übersetzt werden. Typische Compiler sind Batch-Compiler oder inkrementelle Compiler. Inkrementelle Compiler können einzelne Ausdrücke übersetzen. Batch-Compiler übersetzen einzelne Lisp-Dateien oder ganze Lisp-Programme. Compiler übersetzen entweder in einen Bytecode für eine virtuelle Maschine, in andere Programmiersprachen (oft in C) für die weitere Übersetzung oder in Maschinencode für einen Prozessor.

Das Akronym "LISP" wird manchmal scherzhaft als „Lots of Irritating Superfluous Parentheses“ (eine Menge lästiger, überflüssiger Klammern) interpretiert.

Historisch gesehen gehört Lisp zusammen mit Prolog zu den wichtigsten Programmiersprachen der künstlichen Intelligenz. Durch Lisp ist erstmals der Lambda-Kalkül zum Kern einer Programmiersprache gemacht worden. Da dieser das zentrale Element der Semantik nahezu jeder modernen Programmiersprache ist, war Lisp richtungsweisend und keine andere Programmiersprache hat die Entwicklung der Programmiersprachen mehr geprägt.

Im Unterschied zu Europa, wo Programmiersprachen wie Assembler, Fortran oder Pascal als klassische Vertreter der Familie der prozeduralen Programmiersprachen gelehrt wurden, war und ist zum Teil bis heute in den USA Lisp, bzw. einer seiner moderneren Dialekte wie Scheme, die erste gelehrte Programmiersprache. Das hatte einen großen Einfluss, da es sich bei den klassischen Vertretern der prozeduralen Sprachfamilien um Vertreter einer "statischen" Verarbeitungsweise von Daten handelt, während unter anderem Lisp ein strikt "dynamisches" Konzept vertritt.

Lisp benutzt S-Expressions als externes Format, um sowohl Quelltext als auch Daten darzustellen. Funktions- und Makroaufrufe werden als Listen geschrieben, die als erstes Element den Namen der Funktion bzw. des Makros enthalten. Kommentare werden mit codice_1 eingeleitet.

Beispiele in Common Lisp:
(+ 2 3 4)

(setf p 3.1415)

(defun square (x)

(square 3)
LISP-Hallo-Welt-Programm (Mit codice_2 erfolgt ein Zeilenumbruch):

Um ein minimales Lisp-System zu implementieren, sind nur sehr wenige Operatoren und ein allgemeiner Mechanismus zur Funktionsdefinition nötig.
Die folgenden Funktionen sind im ursprünglichen Bericht von McCarthy enthalten:


Bereits mit diesen Sprachmitteln kann ein bemerkenswerter Teil der Funktionen, die übliche Lisp-Systeme mitbringen, definiert werden.

In der Originalversion von Lisp gab es zwei grundsätzliche Datentypen: Atome und Listen. Atome hießen so, weil sie nicht verändert werden konnten. Listen waren Sequenzen von Elementen, wobei diese Elemente Atome oder Unterlisten sein konnten. Ein Atom war entweder eine Zahl oder ein Symbol. Ein Symbol war eine alphanumerische Zeichenkette, die als Variablenname oder Datenelement beim symbolischen Rechnen verwendet wurde.

Intern wurde ein Symbol-Atom nur einmal in der Symboltabelle abgespeichert. Zwei Symbolatome, die gleich geschrieben wurden und an verschiedenen Stellen im Quelltext vorkamen, repräsentierten dasselbe Objekt.

Später wurden in den Lisp-Dialekten weitere Datentypen eingeführt, und das Konzept der Lisp-Atome verlor an Bedeutung.








</doc>
<doc id="2924" url="https://de.wikipedia.org/wiki?curid=2924" title="Hyperlink">
Hyperlink

Ein Hyperlink (e Aussprache [], deutsch wörtlich „Über-Verknüpfung“, sinngemäß elektronischer Verweis), kurz Link, ist ein Querverweis in einem Hypertext, der funktional einen Sprung zu einem anderen elektronischen Dokument oder an eine andere Stelle innerhalb eines Dokuments ermöglicht. Wenn der Hyperlink ausgeführt wird, wird automatisch das darin angegebene Ziel aufgerufen. Im Allgemeinen wird der Begriff auf das World Wide Web bezogen, in dem "Hyperlinks" ein Kernbestandteil sind. Inhaltlich entspricht das Konzept von "Hyperlinks" Querverweisen in der gedruckten Literatur, deren Ziel dort allerdings in der Regel manuell aufgesucht werden muss.

Durch das Hypertextsystem können auch Dateien anderen Typs, die im selben System vorliegen oder daran angeschlossen sind, aufgerufen werden. So können Hyperlinks genutzt werden, um beispielsweise Filme, Bilder und Animationen zu erreichen oder Dateien auf einen Computer "herunterzuladen".

Man spricht von einem "verlinkten Dokument", wenn es mindestens ein weiteres Dokument gibt, das mit einer gültigen Verknüpfung (Hyperlink) auf dieses Dokument verweist und dadurch ein Zugriff auf dieses Dokument möglich wird. Der Link des verlinkenden Dokuments besteht in der Regel aus zwei Teilen, einen für den Benutzer „sichtbaren“ Teil (ein Bild oder ein angezeigter Text, der dem Benutzer anzeigt, was er erwarten kann), sowie einen unsichtbaren Teil, das Linkziel im unsichtbaren Quelltext, das für diesen Link hinterlegt ist. Solche Links werden verwendet, um den Lesefluss nicht zu unterbrechen oder wenn bei Weblinks nicht die vollständige Internetadresse angezeigt werden soll.

In einem Link können weitere Metainformationen hinterlegt sein, z. B. ob die Anzeige zum neuen Inhalt wechseln soll oder ob ein neues Fenster dafür geöffnet wird, oder ob temporär ein weiterer Text (Tooltip oder Quickinfo) angezeigt wird, wenn der Benutzer mit der Maus über den Link fährt. Auch eine Reihenfolge ist definierbar, in der die Links aktiv werden, wenn die Links statt mit der Maus mit der Tastatur gewählt werden.

Hyperlinks sind ein charakteristisches Merkmal des Internets. Sie stellen im heutigen World Wide Web ein fundamentales Element dar; sie entsprechen einer Vernetzung auf Anwendungsebene. Mit ihrer Hilfe lassen sich Webseiten und andere Dateien verbinden, die entweder auf demselben Rechner liegen oder durch einen Webserver zur Verfügung gestellt werden. Eingebunden werden Hyperlinks meist mittels der standardisierten Auszeichnungssprache HTML.

Die Syntax lautet dabei folgendermaßen:

Ziel eines solchen Links kann eine andere Datei (Webseite, Bild, Audio- oder Videodatei etc.) oder ein dynamisch erstelltes Dokument sein. Ein Link enthält die Adresse des Ziels, in der Regel als URL. Meistens definiert ein Link zusätzlich, wie er für den Benutzer dargestellt werden soll. Bei Hypertext-Dokumenten wird dazu fast immer in dem Link ein Linktext angegeben, der dem Benutzer angezeigt wird.

Bei den Linkverweisen im Web handelt es sich um eine sehr einfache Implementierung von Hyperlinks; im Gegensatz zu früheren Systemen sind diese "Weblinks" unidirektional, d. h. das Ziel des Links weiß nichts darüber, dass ein Link auf es zeigt. Wird das Zieldokument umbenannt oder gelöscht, wird der Link nicht automatisch korrigiert, es entsteht ein sogenannter „toter Link“.

Statt des Linktextes kann ein Link auch andere HTML-Elemente enthalten wie Grafiken oder eingebettete Objekte (z. B. eine 'Flash-Animation').

Internet-Links können auch auf andere als die vom Benutzer erwartete Adresse führen. Zu solchen Zwecken erstellte Weiterleitungen führen dann entweder auf eine weitere Webseite oder zu anderen Zielen (Dateien). Dies kann unter Umständen auch so geschehen, dass es dem Anwender verborgen bleibt. Mehrfache Weiterleitungen sind ebenfalls möglich.

Das Verlinken von fremden Dateien (Bilder usw.) bezeichnet man als Hotlinking.

Die Links bestimmen die Dokumentenstruktur. Dadurch kann man festlegen, wie der Besucher sich in einer Webpräsenz bewegen kann.

Es gibt insgesamt vier Strukturen, die durch Verweise entstehen können:


In den meisten heute genutzten Büroprogrammen lassen sich Hyperlinks ebenfalls realisieren. Die Verwendung eines Hyperlinks ist hier allerdings komplexer und grenzt sich von ähnlichen Methoden ab. Beispielsweise ist es möglich, in einem Textdokument mit Hyperlinks auf ein gesondertes Glossar zu verlinken oder in einer Präsentation Hyperlinks etwa in der Art einer Navigationsleiste zu gestalten. Es gibt auch Verlinkungen, die für den Benutzer nicht direkt sichtbar sind. Jede Stil-, Absatz- oder Seitenvorlage ist ein zentral gespeicherter Datensatz, der über Links in Teile eines Dokuments eingebunden wird. Auch Kopf- und Fußzeilen werden nur einmal gespeichert und in jede Seite über einen Link eingebunden. Auch, wenn hierbei normalerweise nicht von einem Link gesprochen wird, ist das Funktionsprinzip dasselbe wie bei klassischen Textlinks. Falls eine Kopfzeile eine Navigationsleiste mit eigenen Linkverknüpfungen enthält, werden zwei unterschiedliche Linksysteme ineinander verschachtelt gespeichert. Nicht alle Officesysteme unterstützen das.

Unterschieden werden allerdings Hyperlinks als anklickbare Verknüpfung und verknüpfte Werte beispielsweise bei einer Vorlage oder in einer Tabellenkalkulation. Der Ausdruck "Verknüpfung" bezeichnet hier eine Variable in Form der Positionsangabe einer Zelle, an dessen Speicherziel ein permanent verwendeter Wert hinterlegt ist. Obwohl auch dies im Wortsinne ein Hyperlink ist, unterscheiden sich der klassische Link und eine Tabellenverknüpfung in ihrem Verhalten. Beispielsweise lässt sich an einer zentralen Stelle ein Mehrwertsteuersatz speichern, der in einer anderen Tabelle (oder eines Tabellenteils) permanent in Echtzeit innerhalb von Formeln aufgelöst wird, beispielsweise um den gespeicherten Nettopreis und den in der Formel errechneten Bruttopreis auszuweisen. Die Änderung des verknüpften Wertes hat im Gegensatz zum klassischen Hyperlink die sofortige Neuberechnung und Anzeige aller errechneten Werte zur Folge, ohne dass der Benutzer etwas anklicken muss.

Außerdem muss zwischen eingebetteten und verknüpften Dokumententeilen unterschieden werden. Ein eingebettetes Dokument (oder Dokumententeil) ist eine einfache Kopie des ursprünglichen Originals und hat nichts mit einem Hyperlink zu tun. Wenn sich Inhalte darin ändern, wirkt sich dies nicht auf andere Dokumente aus, in denen dasselbe Ursprungsdokument in einer früheren Version eingebettet ist. Ist das Dokument dagegen verknüpft, befindet sich lediglich ein Verweis auf die Zieldaten im Quelltext. Wenn vom verknüpften Dokument einzelne Inhalte verändert werden, geschieht die Veränderung nicht im eigentlich geladenen Dokument, sondern in dem Dokument, auf das sich die Verknüpfung bezieht und das für den Benutzer unsichtbar ebenfalls geladen wurde. Diese Änderung wirkt sich auf alle Dokumente aus, die dieselben Verknüpfungen enthalten. Auch Phrasen, die über besondere Listen programmintern verwaltet und über eine definierte Tastenfunktion abgerufen werden, arbeiten nach dem Linkprinzip. Allerdings löst der aktivierte Link hier jedoch einen Kopiervorgang aus, der Link auf den Phrasentext selbst wird in der Regel nicht gespeichert.

Verknüpfungen können in einigen Programmen auch nach vordefinierten Regeln automatisch aktiviert werden. Wenn beispielsweise eine Briefvorlage gespeichert wird, wird der Inhalt des Dokuments zusammen mit allen zugehörigen Vorlagen in jedes darauf beruhende neue Dokument kopiert, zusätzlich wird aber auch eine Linkinformation hinterlegt. Ändert sich später die Briefvorlage, wird beispielsweise in OpenOffice.org beim Öffnen eines älteren Dokuments die Rückfrage gestellt, ob das Dokument an die aktuelle Vorlage angepasst werden soll oder nicht. Wird das Dokument in der täglichen Arbeit weiter verwendet, ist die Anpassung in der Regel sinnvoll, bei archivierten Dokumenten hingegen ist die dynamische Anpassung normalerweise nicht erwünscht.

Eine "Internetverknüpfung" (englisch "‚‘" genannt), mit der in der grafischen Oberfläche in der Voreinstellung ausgeblendeten Dateierweiterung codice_2 (= Uniform Resource Locator), ist das Dateiformat in Windows-Systemen für Weblinks (Hyperlinks in das Internet, „Verknüpfung mit Internet-URL“).

Internetverknüpfungen sind Textdateien mit teilweise hexadezimalen Einträgen. Ihr Inhalt ist wie folgt aufgebaut:

Diese Werte können auch über die „Eigenschaften“ einer Internetverknüpfung (beispielsweise mit einem Rechtsklick im Kontext-Menü, und dort über den Tab „Webdokument“ zu erreichen) angezeigt und geändert werden. Die Oberfläche, um diese Eigenschaften zu ändern, ähnelt der der Dateiverknüpfungen, der innere Aufbau gleicht jedoch den INI-Dateien.

Ein Weblink unter Unix/Linux mit Desktop-Umgebung wird in einer .desktop-Datei abgelegt. Es ist eine Textdatei mit folgendem Aufbau:

Auf macOS-Systemen ist die Syntax im XML-Format:

<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
</plist>

Auch die einzelnen Seiten eines Wiki sind durch Hyperlinks, so genannte Wikilinks, miteinander verbunden. Ihre Implementierung kann je nach verwendeter vereinfachter Auszeichnungssprache unterschiedlich gestaltet sein.

Die in MediaWiki genutzte Sprache Wikitext kennt:

Auch andere Hypertext-Systeme setzen Hyperlinks ein. Die Verwendung von Hyperlinks in diesen anderen Systemen wird im Artikel Hypertext beschrieben.

In der Regel lassen sich kleine oder große verlinkte Netze als gerichtete zyklische Graphen abbilden, in denen Hyperlinks mit Hilfe von "Kanten" und "Ecken" bzw. Knoten von oder zu referenzierten Dokumenten dargestellt werden. Interpretiert man das nebenstehende Bild als verlinktes Netz, enthält beispielsweise das Dokument B einen Hyperlink auf das Dokument C. Der Pfeil von der Ecke B zu der Ecke C (eine gerichtete Kante des Graphen) stellt den Hyperlink dar. Von der Ecke B gelangt man über C, E und D wieder zurück zu B (Zyklus), was in diesem Fall bedeutet, dass man von B ausgehend durch Folgen der entsprechenden Hyperlinks wieder zu dem Ausgangspunkt, dem Dokument B, gelangt.

Tim Berners-Lee, der „Erfinder“ des World Wide Web, geht in Analogie zu Fußnoten und Querverweisen in der wissenschaftlichen Literatur davon aus, dass das bloße Vorhandensein eines Hyperlinks keine Rechtsverletzung darstellen kann; der Autor eines Textes mache sich durch Anbringen einer Fußnote oder eines Querverweises nicht automatisch den Inhalt des referenzierten Dokuments zu eigen. Das Prinzip des wechselseitigen Verweisens sei für wissenschaftliches Arbeiten grundlegend; wäre dieses Verweisprinzip illegal, würde dies jegliches wissenschaftliche Arbeiten in unserem heutigen Verständnis unmöglich machen.

Diese Auffassung teilen nicht alle Gerichte, obwohl die Rechtsliteratur selbst intensiv das Verweisprinzip einsetzt. Bisher hat sich noch keine einheitliche Rechtsprechung herausgebildet.
Verbindlich ist dahingehend europaweit die "E-Commerce- des Europäischen Parlaments und des Rates vom 8. Juni 2000" und deren nationale Umsetzungen.

Auch Hotlinking kann untersagt werden, da damit Bandbreitendiebstahl einhergeht.




</doc>
<doc id="2926" url="https://de.wikipedia.org/wiki?curid=2926" title="Leni Riefenstahl">
Leni Riefenstahl

Helene Bertha Amalie „Leni“ Riefenstahl (* 22. August 1902 in Berlin; † 8. September 2003 in Pöcking) war eine deutsche Filmregisseurin, -produzentin und -schauspielerin sowie Drehbuchautorin, Schnittmeisterin, Fotografin und Tänzerin. Sie gilt als eine der umstrittensten Persönlichkeiten der Filmgeschichte. Einerseits wird sie von vielen Filmschaffenden und -kritikern als „innovative Filmemacherin und kreative Ästhetin“ angesehen, andererseits für ihre Werke im Dienst der Propaganda während der Zeit des Nationalsozialismus kritisiert.

Nachdem Riefenstahl ihre ursprünglich eingeschlagene Tanzkarriere aufgrund einer Knieverletzung beenden musste, etablierte sie sich während der 1920er Jahre in der Weimarer Republik als Schauspielerin im Bergfilm­genre.

Durch ihr 1932 veröffentlichtes Regiedebüt "Das blaue Licht", bei dem sie als Hauptdarstellerin, Regisseurin, Koproduzentin und Drehbuchautorin fungierte, wurden führende NSDAP-Politiker wie der Parteichef Adolf Hitler und Reichspropagandaleiter Joseph Goebbels auf sie aufmerksam. Nach der im Jahr darauf erfolgten Machtübernahme der NSDAP erhielt Riefenstahl den Auftrag, die „Reichsparteitagstrilogie“ zu drehen. Die Propagandaproduktionen "Der Sieg des Glaubens", "Triumph des Willens" und "Tag der Freiheit! – Unsere Wehrmacht" entstanden in den Jahren 1933 bis 1935. Für "Triumph des Willens" erhielt Riefenstahl den Deutschen Nationalen Filmpreis 1934/35, die Auszeichnung für die beste ausländische Dokumentation bei den Internationalen Filmfestspielen von Venedig 1935 im gleichfalls faschistischen Italien sowie den "Grand Prix" auf der Pariser Weltfachausstellung.

1938 veröffentlichte Riefenstahl unter dem Namen "Olympia" eine zweiteilige Dokumentation über die Olympischen Sommerspiele 1936 in Berlin. Der Film wurde vielfach als ästhetisches Meisterwerk gelobt, aber auch für seine propagandistischen und ideologischen Elemente kritisiert. Riefenstahl bekam für ihre Arbeit unter anderem den Deutschen Nationalen Filmpreis 1937/38, den Coppa Mussolini, eine olympische Goldmedaille vom Internationalen Olympischen Komitee und den Kinema-Jumpō-Preis im damals imperialistischen Japan verliehen.

Während des Zweiten Weltkrieges filmte sie mit dem „Sonderfilmtrupp Riefenstahl“ Hitlers Polenfeldzug.

In der Nachkriegszeit wurde sie trotz ihrer Einbindung in die nationalsozialistische Filmpolitik laut eines Spruchkammerbeschlusses lediglich als „Mitläuferin“ eingestuft. Dennoch gestaltete es sich nach dem Ende des Krieges für Riefenstahl ab 1945 schwer, weitere Aufträge als Regisseurin und Produzentin zu erhalten, so dass sie ihr Filmschaffen nach "Tiefland" (gedreht bis 1944), der erst 1954 fertiggestellt und veröffentlicht wurde, für fast ein halbes Jahrhundert einstellte.

Ab den 1960er Jahren betätigte sich Riefenstahl als Fotografin und veröffentlichte mehrere Bildbände. Zu ihren bekanntesten Werken dieser Zeit zählen Fotoreportagen über das Volk der Nuba im Sudan und Unterwasseraufnahmen. In diesen Aufnahmen erkannte ein Teil der Kritiker Parallelen zu ihren Arbeiten aus der Zeit des Nationalsozialismus. 2002 veröffentlichte sie mit dem Dokumentarfilm "Impressionen unter Wasser" ihr letztes Werk.

Helene Bertha Amalie Riefenstahl, genannt „Leni“, kam am 22. August 1902 in Berlin-Wedding zur Welt. Zweieinhalb Jahre später wurde ihr Bruder Heinz Riefenstahl (1905–1944) geboren. Ihr Vater Alfred Riefenstahl (1878–1944) war ein Handwerksmeister, der sich einen eigenen Installateurbetrieb aufgebaut hatte. Ihre Mutter Bertha Ida Riefenstahl (1880–1965, geborene Scherlach), als jüngstes von 18 Kindern im polnischen Włocławek aufgewachsen, war Näherin. Ihre ersten Lebensjahre wohnte Riefenstahl mit ihrer Familie in der Prinz-Eugen-Straße im Wedding, später zogen die Riefenstahls in ein eigenes Haus in Zeuthen bei Berlin.

Riefenstahl wurde 1908 in die Volkschule in Berlin-Neukölln eingeschult. Anschließend besuchte sie das Kollmorgensche Lyzeum, eine private Höhere Knaben- und Mädchenschule in Berlin-Tiergarten, das sie 1918 mit der Mittleren Reife verließ. In ihrer Freizeit erhielt Riefenstahl Klavierunterricht und begeisterte sich schon in jungen Jahren für Sport. Sie war Mitglied im Charlottenburger Damen-Schwimmclub Nixe, trat einem Turnverein bei, fuhr Rollschuh, spielte Tennis, ruderte und segelte auf dem Zeuthener See. Nach ihrem Schulabschluss erhielt Riefenstahl für kurze Zeit Mal- und Zeichenunterricht an der Staatlichen Kunstgewerbeschule in Berlin. Außerdem nahm sie hinter dem Rücken ihres strengen Vaters, jedoch mit der Unterstützung ihrer kunstbegeisterten Mutter, Tanzstunden an der Grimm-Reiter-Schule am Kurfürstendamm. Dort lernte sie unter anderem rhythmisches Turnen und Tanzen, Improvisation sowie Phantasietanz. Nachdem Riefenstahl bei einer Tanzveranstaltung der Schule im Berliner Blüthnersaal für die erkrankte Anita Berber eingesprungen war, erfuhr ihr Vater von den heimlichen Tanzstunden und schickte sie für ein Jahr in ein Pensionat in Thale im Harz. Dort übte sie weiter heimlich Tanzen, spielte Theater und besuchte mit ihrer Internatskameradin Hela Gruel die Aufführungen der Freilichtbühne Thale. Von 1920 bis 1923 arbeitete Riefenstahl als Sekretärin im Betrieb ihres Vaters, wo sie Schreibmaschine, Stenografie und Buchhaltung lernte.

Während dieser Zeit durfte sie ihren Tanzunterricht an der Grimm-Reiter-Schule fortsetzen und nahm außerdem von 1921 bis 1923 Unterricht in klassischem Tanz (Ballett) bei Eugenie Eduardowa. Zusätzlich lernte sie Ausdruckstanz an der Jutta-Klamt-Schule in der Fasanenstraße und nahm Boxunterricht bei Sabri Mahir. 1923 besuchte sie für wenige Monate Mary Wigmans Tanzschule in Dresden, wo sie gemeinsam mit Gret Palucca, Vera Skoronel und Yvonne Georgi unterrichtet wurde. Im selben Jahr ging sie mit dem Tennisprofi Otto Froitzheim ihre erste Beziehung ein und lernte während eines Badeurlaubs an der Ostsee den Banker und späteren Filmproduzenten Harry R. Sokal kennen. Dieser finanzierte ihr Debüt als Solo-Tänzerin am 23. Oktober 1923 in München. Es folgte eine sechsmonatige Tournee mit rund 70 Auftritten im In- und europäischen Ausland. Außerdem wurde Riefenstahl von Max Reinhardt für zwei Solo-Auftritte in den Kammerspielen des Deutschen Theaters in Berlin engagiert.

Fred Hildenbrandt beschrieb Riefenstahls Tanzstil mit den Worten: „Dieses sehr schöne Mädchen ringt wohl inständig um einen Rang neben den dreien, die man ernst nimmt: der Impekoven, der Wigman, der Gert. Und wenn man dieses vollkommen gewachsene hohe Geschöpf in der Musik stehen sieht, weht eine Ahnung daher, dass es Herrlichkeiten im Tanz geben könnte, die keine von jenen dreien zu tragen und zu hüten bekam, nicht der heroische Gongschlag der Mary, nicht der süße Geigenlauf der Niddy, nicht die grausame Trommel der Valeska: die Herrlichkeit der Tänzerin, die alle tausend Jahre wiederkehrt. Aber dann beginnt dieses Mädchen ihren Leib zu entfalten, die Ahnung verweht, der Glanz ergraut, der Klang verrostet […].“ Der Kunstkritiker und Tanzhistoriker John Schikowski urteilte im April 1924 über Riefenstahls Darbietung bei einer Matinée an der Volksbühne Berlin: „Knie- und Hüftgelenke erscheinen zuweilen etwas eingerostet, die früher so wunderbar suggestive Sprache der Arme ist teilweise verstummt; an ihre Stelle trat ein äußerlich effektvolles, aber oft seelenloses Spiel der Hände.“

Im Sommer 1924 zog sich Riefenstahl bei einem Auftritt in Prag eine Knieverletzung zu, die ihre tänzerische Bühnenkarriere beendete. Ein Jahr später trennte sie sich von Froitzheim, mit dem sie zwischenzeitlich verlobt gewesen war.

Ihr Filmdebüt gab Riefenstahl in einer Gymnastikszene in dem Dokumentarfilm "Wege zu Kraft und Schönheit" aus dem Jahr 1925.

Im Frühjahr 1925 besuchte sie in einem Kino am Nollendorfplatz eine Vorstellung des 1924 erschienenen Stummfilms "Der Berg des Schicksals" des Regisseurs und Bergfilm-Pioniers Arnold Fanck. Der Film beeindruckte sie so sehr, dass sie beschloss, Schauspielerin zu werden. In einem Hotel in den Dolomiten traf sie auf Luis Trenker, der in "Der Berg des Schicksals" mitgespielt hatte. Er vermittelte ihr den Kontakt zu Fanck, der sich begeistert von Riefenstahl zeigte und beschloss, sie als Hauptdarstellerin für sein nächstes Filmprojekt zu verpflichten: „Als ich Leni Riefenstahl sah, war mein erster Eindruck: Naturkind. Keine Schauspielerin, keine ‚Darstellerin‘. Diese Frau tanzt sich selbst. Man musste ihr also eine Rolle schreiben, die aus ihrem Wesen geboren ward.“ Und so verfasste Fanck das Drehbuch zu dem Film "Der heilige Berg", der von einer Tänzerin handelt, in die sich zwei junge Bergsteiger, gespielt von Trenker und Ernst Petersen, verlieben. Im wirklichen Leben hatte Riefenstahl eine kurze Affäre mit Trenker.

Für die 18 Monate dauernden Dreharbeiten erhielt sie eine Gage von 20.000 Reichsmark. Während der Außenaufnahmen, die in den Schweizer Alpen gemacht wurden, lernte Riefenstahl Skilaufen. Zudem ließ sie sich von Fanck die Funktionen der Filmkamera erklären. Er zeigte ihr den Umgang mit Objektiven, den Einsatz verschiedener Brennweiten und die Wirkung von Farbfiltern. Nach dem Abschluss der Dreharbeiten unterwies der Regisseur sie außerdem im Entwickeln, Kopieren und Schneiden des Filmmaterials. Die Uraufführung von "Der heilige Berg" folgte am 17. Dezember 1926 im Ufa-Palast am Zoo in Berlin. Knapp ein Jahr später kam er unter dem Titel "The Holy Mountain" auch in die US-amerikanischen Kinos. Riefenstahl erntete für ihre schauspielerische Darbietung gemischte Kritiken. Die "Berliner Morgenpost" befand: „Schauspielerisch konnte Leni Riefenstahl nichts geben. Auch sah sie wenig vorteilhaft aus.“ Oskar Kalbus hingegen meinte: „Zwischen diesen herrlichen Männern steht eine für die Kinoleinwand neue Frau: die junge Tänzerin Leni Riefenstahl, ein beinahe unwahrscheinlich zartes, von feinsten Rhythmen beseeltes Geschöpf, keineswegs nur Tänzerin, sondern auch Schauspielerin, die viel natürliche Innerlichkeit mitbringt.“

Riefenstahl sprach für die Rolle des Gretchens in Friedrich Wilhelm Murnaus Filmepos "Faust – eine deutsche Volkssage" aus dem Jahr 1926 vor. Sie kam in die engere Auswahl, unterlag bei der Rollenvergabe aber schließlich der damals noch unbekannten Schauspielerin Camilla Horn. Von Mai bis November 1927 fanden, ebenfalls unter der Regie von Fanck, die körperlich sehr anstrengenden Dreharbeiten zu dem Sportfilm "Der große Sprung" in den Dolomiten statt. Für die Rolle lernte sie mit Hans Schneeberger, der neben Luis Trenker eine der beiden männlichen Hauptrollen spielte, Bergsteigen und -klettern. Privat gingen Riefenstahl und Schneeberger eine dreijährige Beziehung ein. Die Premiere von "Der große Sprung" fand am 20. Dezember 1927 im Ufa-Palast am Zoo in Berlin statt. Der Film wurde ein Erfolg, legte Riefenstahl aber auf die Rolle der „Frau zwischen zwei Männern“ im Abenteuer- und Bergmilieu fest, so dass andere Angebote ausblieben. Lediglich in Rolf Raffés wenig beachtetem Historiendrama "Das Schicksal derer von Habsburg" von 1928 gelang Riefenstahl mit der Verkörperung von Mary Vetsera ein Rollenwechsel.

Danach kehrte Riefenstahl mit dem von Harry R. Sokal produzierten Drama "Die weiße Hölle vom Piz Palü" ins Bergfilmgenre zurück. Die Außenaufnahmen, die das Filmteam ins Berninamassiv führten, wurden unter der Anweisung von Fanck gedreht, Georg Wilhelm Pabst übernahm die Regie bei den Innendrehs. Riefenstahl hatte Pabst dazu überredet, bei dem Projekt mitzuwirken, denn sie wollte unter seiner Schauspielführung, die schon Greta Garbo in "Die freudlose Gasse" (1925) zum Erfolg verholfen hatte, zu einer ernstzunehmenden Schauspielerin werden. Nach seiner Uraufführung am 11. Oktober 1929 in Wien wurde das Werk ein großer nationaler und internationaler Erfolg und Riefenstahl erhielt für ihre schauspielerische Leistung die erhoffte positive Resonanz. Die "B.Z. am Mittag" schrieb: „Leni Riefenstahl, schauspielerisch so gut wie noch nie zuvor, zeigt bei aller fraulichen Anmut jungenhafte Courage und Gewandtheit, sie ist die wohl sympathischste und brauchbarste Hochtouristin im deutschen Film.“ Das Berliner "8 Uhr Abendblatt" stellte fest: „Leni Riefenstahl […] hat in ihrer Körperlichkeit ganz große Szenen.“

In Berlin lernte Riefenstahl Ende der 1920er Jahre den Regisseur Josef von Sternberg kennen. Sie besuchte ihn öfter bei seiner Arbeit in den UFA-Filmateliers in Babelsberg und soll sich laut dem Filmkritiker Hans Feld Hoffnungen auf die Rolle der Lola Lola in von Sternbergs Heinrich-Mann-Verfilmung "Der blaue Engel" gemacht haben, mit der Marlene Dietrich 1930 zum internationalen Filmstar wurde. Von Sternbergs Angebot, ihn nach Hollywood zu begleiten, lehnte Riefenstahl aufgrund ihrer Beziehung zu Schneeberger ab. Dieser trennte sich kurze Zeit später wegen einer anderen Frau von ihr.

Ihr nächster Film, "Stürme über dem Mont Blanc" aus dem Jahr 1930, wurde stumm gedreht und erst nach Abschluss der Dreharbeiten synchronisiert und musikalisch untermalt. Um den Sprung aus dem Stumm- in den Tonfilm zu schaffen, an dem viele erfolgreiche Stummfilmschauspieler wie Vilma Bánky, Pola Negri oder Lars Hanson scheiterten, nahm Riefenstahl stimmbildenden Unterricht bei Eugen Herbert Kuchenbuch. 1931 erschien ihr zweiter Tonfilm, die Ski-Komödie "Der weiße Rausch".

→ "Hauptartikel: Das blaue Licht (1932)"

Neben der Schauspielerei begann Riefenstahl, Drehbücher und Drehberichte zu schreiben. Den ersten veröffentlichte sie in der Fachzeitschrift "Film-Kurier" über Fancks Sportfilm "Das weiße Stadion". 1931 schrieb sie die erste Fassung des Manuskripts für ihren Film "Das blaue Licht". Dieser handelt von einem geheimnisvollen, blauen Licht, das in Vollmondnächten von einer Bergspitze herab scheint und die jungen Männer eines Bergdorfs magisch anzieht, die dann beim Aufstieg tödlich verunglücken. Das Drehbuch entwickelte sie zusammen mit dem jüdischen Filmtheoretiker und Drehbuchautor Béla Balázs sowie mit Unterstützung von Carl Mayer. Sie gründete ihre erste eigene Filmgesellschaft, die L.R. Studiofilm, und überzeugte ihren Mäzen Harry R. Sokal, in das Projekt zu investieren. Riefenstahl übernahm bei "Das blaue Licht" zudem die weibliche Hauptrolle, Regie, Produktionsleitung und den Schnitt. Die Dreharbeiten, bei denen Sarner Bauern als Laienschauspieler mitwirkten, fanden von Juli bis September 1931 statt. Der Film kam am 24. März 1932 in die Kinos und wurde trotz gemischter Kritiken ein Erfolg: Das New Yorker National Board of Review wählte "Das blaue Licht" 1934 unter die "Top Foreign Films" und bei der Biennale in Venedig wurde der Film 1932 mit der Silbermedaille ausgezeichnet.

Am 27. Februar 1932 besuchte Riefenstahl eine Veranstaltung der Nationalsozialisten im Berliner Sportpalast, auf der Adolf Hitler eine Rede hielt. Bald darauf bat sie ihn in einem Brief um ein persönliches Treffen, zu dem es im Mai 1932 in Horumersiel bei Wilhelmshaven kam. Laut Riefenstahls Memoiren habe ihr Hitler bei dieser Gelegenheit offenbart, dass sie ihn mit "Das blaue Licht" sehr beeindruckt habe und er habe zu ihr gesagt: „Wenn wir einmal an die Macht kommen, dann müssen Sie meine Filme machen.“ In der Folgezeit war Riefenstahl häufig zu Gast bei Feierlichkeiten und offiziellen Empfängen hoher Nazifunktionäre, lernte Joseph und Magda Goebbels, Hermann Göring, Albert Speer und Julius Streicher kennen.

Nach der Machtergreifung forderte der im Exil befindliche Balázs von Riefenstahl das zunächst zurückgestellte Honorar für seine Arbeit am Drehbuch für "Das blaue Licht". Darauf schrieb Riefenstahl am 11. Dezember 1933 ihrem Freund Julius Streicher auf einem Papier des Nazitreffs Hotel Kaiserhof in Berlin eine Vollmacht: „Ich erteile Herrn Gauleiter Julius Streicher aus Nürnberg – Herausgeber des "Stürmer" Vollmacht in Sachen der Forderung des Juden Belá Balacs [sic] an mich. Leni Riefenstahl.“ So entledigte sich Riefenstahl mit Hilfe der Nazis ihres jüdischen Koautors. Balázs’ Name verschwand aus dem Vorspann des Films.

In der deutsch-amerikanischen Koproduktion "SOS Eisberg" spielte Riefenstahl zum letzten Mal unter der Regie von Arnold Fanck. Die Dreharbeiten führten sie im Juni 1932 zunächst nach Grönland und Anfang 1933 in die Schweizer Alpen. Aus einer Artikelserie über die Erlebnisse in Grönland, die sie für die Zeitschrift "Tempo" schrieb, und aus Vorträgen, die sie zum Film hielt, entstand das Buch "Kampf in Schnee und Eis", das 1933 erschien. Die Filmpremiere fand am 30. August 1933 im Ufa-Palast am Zoo in Berlin statt. Zu diesem Zeitpunkt hatte Riefenstahl bereits mit den Dreharbeiten zu "Der Sieg des Glaubens" in Nürnberg begonnen. Damit sie der ersten Vorführung von "SOS Eisberg" in Berlin beiwohnen konnte, stellte Hitler ihr sein Privatflugzeug zur Verfügung, das sie noch am selben Abend zurück nach Nürnberg brachte. Die US-amerikanische Fassung "SOS Iceberg" wurde am 22. September 1933 in New York uraufgeführt.

→ "Hauptartikel: Der Sieg des Glaubens"

Mit "Der Sieg des Glaubens" entstand der erste von insgesamt drei NS-Propagandafilmen, die Riefenstahl in den Jahren 1933 bis 1935 über die Reichsparteitage der Nationalsozialistischen Deutschen Arbeiterpartei in Nürnberg drehte und die auch als „Reichsparteitagstrilogie“ bezeichnet werden. Den Auftrag für "Der Sieg des Glaubens" erteilte ihr das Reichspropagandaministerium unter der Leitung Joseph Goebbels’. Dieser hatte am 17. Mai 1933 in seinem Tagebuch notiert: „Nachmittags. Leni Riefenstahl. Ich mache ihr den Vorschlag eines Hitlerfilms. Sie ist begeistert davon.“

Da Riefenstahl weder NSDAP-Mitglied war noch Erfahrung im Dokumentarfilmgenre hatte und obendrein eine Frau war, stieß die Entscheidung, sie für das Projekt zu verpflichten, zunächst auf Unmut innerhalb der Partei. Genossen wie Arnold Raether und Eberhard Fangauf, die in der Hauptabteilung IV (Film) des Reichspropagandaministeriums tätig waren, versuchten Riefenstahl zu sabotieren, indem sie ihr Filmmaterial und Kameraleute verweigerten und den Nachweis ihrer arischen Abstammung verlangten. Auch Hitlers Stellvertreter Rudolf Heß versuchte, sie aus dem Projekt zu drängen, indem er sie der „Führerbeleidigung“ bezichtigte. Riefenstahl gelang es jedoch, sich gegen ihre Gegner in der Partei zu behaupten, wobei ihr die Protektion Hitlers half. Dieser wollte keine nüchterne Schilderung des fünften Reichsparteitages, sondern eine auf ihn ausgerichtete propagandistische Inszenierung, die den Zuschauer beeindrucken und begeistern sollte. Hierbei vertraute er auf das künstlerische Talent und die Vision Riefenstahls.

Sie übernahm die Regie, schrieb das Drehbuch und wählte Sepp Allgeier, Franz Weihmayr und Walter Frentz als Kameramänner aus. Die Dreharbeiten zu "Der Sieg des Glaubens" fanden in der Zeit vom 27. August bis 5. September 1933 statt, der Parteitag selbst wurde vom 30. August bis 3. September 1933 abgehalten. Anschließend schnitt sie den 60-minütigen Film aus rund 16.000 Metern Filmmaterial zusammen. "Der Sieg des Glaubens" hatte am 1. Dezember 1933 Premiere und wurde begeistert aufgenommen. Riefenstahl selbst war hingegen mit der Endfassung des Films aufgrund einiger ästhetischer Unvollkommenheiten nicht zufrieden. Man habe ihr keine ausreichende Zeit zur Vorbereitung gelassen und sie während des Drehs bei ihrer Arbeit behindert, so dass ihr am Ende nur wenig verwertbares Filmmaterial zur Verfügung gestanden habe. Sie beschwerte sich bei Hitler und verärgerte damit Goebbels.

Nach dem Röhm-Putsch im Sommer 1934 wurde der Film wieder aus dem Verkehr gezogen, da er den SA-Stabschef Ernst Röhm noch als „zweitwichtigsten Mann“ an Hitlers Seite zeigte.

→ "Hauptartikel: Triumph des Willens"
Hitler persönlich beauftragte Riefenstahl, auch den sechsten Reichsparteitag der NSDAP, der vom 5. bis 10. September 1934 in Nürnberg stattfand und zu dem eine halbe Million Menschen erwartet wurden, filmisch zu inszenieren. Er ließ ihr alle künstlerischen Freiheiten und die NSDAP stellte der „Reichsfilmregisseurin“ nahezu unbegrenzte Mittel und ein Team von 170 Mitarbeitern, darunter 36 Kameramänner und neun Flugkameramänner, zur Verfügung. Sie benötigte sieben Monate für die Fertigstellung des rund 110 Minuten dauernden Films, der bei seiner Uraufführung am 28. März 1935 im Ufa-Palast in Berlin ein großer Erfolg wurde. Im darauffolgenden Monat lief er in 70 deutschen Städten im Kino an, wo er für Rekordeinspielergebnisse sorgte.

Riefenstahl hatte mit innovativen Montagetechniken, ungewöhnlicher Kameraführung und suggestiver musikalischer Untermalung einen Film geschaffen, der eines der wichtigsten Propagandamittel der Nationalsozialisten wurde. Mehr als 20 Mio. Deutsche sahen den Film, der auch in Schulen vorgeführt wurde. Zusätzlich veröffentlichte Riefenstahl das Buch "Hinter den Kulissen des Reichsparteitagsfilms". Für "Triumph des Willens" erhielt Riefenstahl den Nationalen Filmpreis 1934/35, den Preis für den besten ausländischen Dokumentarfilm bei den Internationalen Filmfestspielen von Venedig 1935 und den "Grand Prix" bei der Pariser Weltfachausstellung 1937.

Durch die aufwändigen Dreharbeiten zu "Triumph des Willens" wurden zahlreiche andere Produktionen der übrigen NS-Filme vernachlässigt. Dies führte zu weiteren Anfeindungen zwischen Goebbels und der Filmemacherin.

→ "Hauptartikel: Tag der Freiheit! – Unsere Wehrmacht"

Anlässlich der Wiedereinführung der allgemeinen Wehrpflicht und aufgrund der Tatsache, dass im September 1934 wegen schlechter Witterungsverhältnisse keine brauchbaren Aufnahmen von der Wehrmacht für die Dokumentation "Triumph des Willens" gemacht werden konnten, drehte Riefenstahl beim siebten Reichsparteitag der NSDAP im September 1935 den 28-minütigen Kurzfilm "Tag der Freiheit! – Unsere Wehrmacht". Die Premiere des dritten und letzten Teils der Reichsparteitagstrilogie fand am 30. Dezember 1935 in Berlin statt. Der Film demonstriert für die meisten Kritiker der Nachkriegszeit eine Armee, die sich für einen Angriffskrieg rüstet. Im Interview mit Ray Müller für den Dokumentarfilm "Die Macht der Bilder" von 1993 meinte Riefenstahl jedoch, sie habe nur eine Übung, eine Show gefilmt, „Weiter nichts.“

Im Januar 1936 wurde Riefenstahl vom italienischen Diktator Benito Mussolini in Rom empfangen. Der „Duce“ hatte die Reichsparteitagstrilogie gesehen und wollte Hitlers Regisseurin für die Verfilmung der Trockenlegung der Pontinischen Sümpfe gewinnen. Riefenstahl lehnte dieses Angebot jedoch unter Hinweis auf ihr kommendes Projekt – die Verfilmung der Olympischen Sommerspiele in Berlin 1936 – ab.

→ "Hauptartikel: Olympia (Film)"

Riefenstahl wurde nach eigenen Angaben von Carl Diem, dem Generalsekretär des Organisationskomitees, mit der Verfilmung der XI. Olympischen Spiele beauftragt. Das Projekt wurde vom Reichsministerium für Volksaufklärung und Propaganda finanziert. Um die staatliche Beteiligung zu verschleiern, wurde die Olympia-Film GmbH „auf Veranlassung des Reichs und mit Mitteln des Reichs gegründet.“ Als Gesellschafter traten Riefenstahl und ihr Bruder Heinz auf. Für die Produktion stellte das Ministerium ein Budget in Höhe von 1,8 Mio. Reichsmark zur Verfügung, Riefenstahl erhielt ein Honorar von 400.000 RM.

Im Herbst 1935 begann Riefenstahl mit den Vorbereitungen der Dreharbeiten. Sie stellte ein großes Kamerateam zusammen, darunter Walter Frentz, Willy Zielke, Gustav Lantschner und Hans Ertl, das im Mai 1936 die ersten Probeaufnahmen anfertigte. Die eigentlichen Dreharbeiten begannen zwei Monate vor Beginn der Olympischen Spiele mit der Aufzeichnung des Prologs und dem Fackellauf, gefolgt von der Eröffnungsfeier am 1. August 1936. Pro Wettkampftag entstanden rund 15.000–16.000 Meter Filmmaterial, insgesamt summierte sich das Material nach dem Abschluss der Dreharbeiten auf 400.000 Filmmeter. Zehn Monate lang sichtete, archivierte und montierte Riefenstahl das Filmmaterial zu den beiden Olympia-Filmen "Fest der Völker" und "Fest der Schönheit" zusammen.

Da sie verschwenderisch mit ihrem Budget umging und immer höhere Forderungen an das Propagandaministerium stellte, kam es zu erneuten Differenzen mit Goebbels, der am 6. November 1936 in seinem Tagebuch notierte: „Frl. Riefenstahl macht mir ihre Hysterien vor. Mit diesen wilden Frauen ist nicht zu arbeiten. Nun will sie für ihren Film eine ½ Million mehr und zwei daraus machen. […] Sie weint. Das ist die letzte Waffe der Frauen. Aber bei mir wirkt das nicht mehr.“ Hitler persönlich ordnete ein Versöhnungstreffen in Gegenwart von Pressefotografen im Sommer 1937 in der Villa Riefenstahl in Berlin an, welche die Regisseurin kurz zuvor bezogen hatte. Zeit ihres Lebens behauptete Riefenstahl, eine tiefe, auf Gegenseitigkeit beruhende Abneigung gegen Goebbels gehegt zu haben. Er habe ihr sexuelle Avancen gemacht und ihr nicht verzeihen können, dass sie diese abgelehnt habe.

Am 20. April 1938 – Hitlers 49. Geburtstag – wurden beide Teile des Olympia-Films im Ufa-Palast am Zoo in Berlin uraufgeführt und vom Publikum begeistert aufgenommen. Die deutsche Presse, der die freie Kunstkritik aufgrund Goebbels’ „Kunstbetrachter-Erlasses“ seit Ende 1936 untersagt war, berichtete ausnahmslos positiv. Für den internationalen Verleih fertigte Riefenstahl eine englische, eine französische und eine italienische Version von "Olympia" an und reiste anschließend durch Europa, um den Film zu vermarkten. Auch hier wurde "Olympia" zum Erfolg, einzig Großbritannien verweigerte die Aufführung. Neben dem Deutschen Nationalen Filmpreis 1937/38 erhielt Riefenstahl für die Olympia-Filme 1938 den Coppa Mussolini, den schwedischen Polar-Preis, den Ehrenpreis der Regierung von Griechenland sowie 1941 den japanischen Kinema Junpo Award und 1948 das Olympische Diplom zur Olympischen Goldmedaille vom Internationalen Olympischen Komitee beim Filmfestival in Lausanne.

Da sie mit "Olympia" den überragenden Erfolg US-amerikanischer Athleten wie Jesse Owens und Forrest Towns dokumentiert hatte, hoffte Riefenstahl, in den Vereinigten Staaten im Filmgeschäft Fuß fassen zu können. Das US-amerikanische "TIME Magazine" hatte der Regisseurin bereits im Februar 1936 eine Titelseite mit der Bildunterschrift „Hitler’s Leni Riefenstahl“ gewidmet. Als sie jedoch im November 1938 mit den Olympia-Filmen im Gepäck an Bord der Europa nach New York reiste, wurde sie dort mit der Nachricht von der Reichspogromnacht, die sich vom 9. auf den 10. November 1938 ereignet hatte, konfrontiert. Die Anti-Nazi-League und das Motion Picture Artists Committee riefen zum Boykott der Olympia-Filme auf und in Hollywood hingen Anti-Riefenstahl-Plakate. Zu den wenigen, die Riefenstahl empfingen, gehörten der Regisseur King Vidor, der Filmproduzent Walt Disney und die Filmgesellschaft Metro-Goldwyn-Mayer. Die Treffen verliefen jedoch ergebnislos und so kehrte sie im Januar 1939 nach Deutschland zurück.

Im März 1939 sprach Riefenstahl mit Albert Speer über den Bau eines 225.000 Quadratmeter großen und eigens für ihre Bedürfnisse zugeschnittenen Filmstudiogeländes, dessen Kosten vollständig von der NSDAP übernommen werden sollten. Aufgrund des Zweiten Weltkriegs wurde dieses Bauvorhaben jedoch nie vollendet. Seit Jahresanfang 1939 bereitete Riefenstahl außerdem die Verfilmung von Kleists Drama "Penthesilea" vor, bei der sie selbst die Rolle der Amazonenkönigin übernehmen wollte. Sie gründete die Leni Riefenstahl-Film GmbH und zog sich zum Schreiben des Drehbuchs auf Sylt zurück. Im Spätsommer 1939 sollten die Dreharbeiten in Libyen beginnen, dieser Plan wurde jedoch durch den Kriegsausbruch am 1. September 1939 vereitelt.

Auf Anordnung Hitlers wurde der „Sonderfilmtrupp Riefenstahl“ gebildet, dem neben der Regisseurin der Tonmeister Hermann Storr, Walter Traut, die Brüder Gustav und Otto Lantschner, Sepp Allgeier und vier weitere Techniker angehörten. Ausgestattet wurden sie mit zwei sechssitzigen Mercedes-Limousinen, einem BMW-Motorrad mit Beiwagen sowie Tankkarten für 700 Liter Benzin, einem Tonfilmwagen sowie selbst entworfenen Phantasieuniformen mit Gasmasken und Taschenpistolen. Der Filmtrupp machte sich am 10. September 1939 auf den Weg an die Ostfront, um den Polenfeldzug zu dokumentieren. Nach Riefenstahls Angaben habe sie sich nützlich machen wollen.

Mehrere Schwarz-Weiß-Fotos dokumentieren, dass Riefenstahl und ihr Sonderfilmtrupp am 12. September 1939 Zeugen eines Massakers der deutschen Wehrmacht an mehr als 20 wehrlosen Juden in der polnischen Kleinstadt Końskie wurden. Eines dieser Fotos, das ein deutscher Landser von ihr gemacht hatte, ist mit dem Satz „Leni Riefenstahl fällt beim Anblick der toten Juden in Ohnmacht.“ beschriftet. Riefenstahl behauptete später jedoch, nur „in der Ferne“ Schüsse gehört zu haben. „Weder ich noch meine Mitarbeiter haben etwas gesehen!“

Nach der Kapitulation der letzten polnischen Streitkräfte wohnte Riefenstahl am 5. Oktober 1939 der Siegesparade der deutschen Truppen in Warschau als Zuschauerin bei, während Sepp Allgeier und die Brüder Lantschner die Parade unter der Regie von Fritz Hippler filmten. Das Angebot Goebbels’, einen Dokumentarfilm über die „Siegfriedlinie“ zu drehen, lehnte sie ab.

Nach dem Einmarsch deutscher Verbände in Paris am 14. Juni 1940 telegrafierte sie am selben Tag ins Führerhauptquartier: „Mit unbeschreiblicher Freude, tief bewegt und erfüllt mit heißem Dank, erleben wir mit Ihnen mein Führer, Ihren und Deutschlands größten Sieg, den Einzug Deutscher Truppen in Paris. Mehr als jede Vorstellungskraft menschlicher Phantasie vollbringen Sie Taten, die ohnegleichen in der Geschichte der Menschheit sind. Wie sollen wir Ihnen nur danken? Glückwünsche auszusprechen, das ist viel zu wenig, um Ihnen die Gefühle auszusprechen, die mich bewegen.“ Später erklärte sie, dass sie zu diesem Zeitpunkt geglaubt habe, dass der Krieg mit der Eroberung der französischen Hauptstadt ein baldiges Ende finden würde und sie habe mit dem Telegramm lediglich ihre Freude und Erleichterung darüber zum Ausdruck bringen wollen.

→ "Hauptartikel: Tiefland (Film)"

Bereits in den 1930er Jahren hatte es Bestrebungen Riefenstahls gegeben, Eugen d’Alberts Oper "Tiefland" zu verfilmen, aber erst unter einem Vertrag mit der Tobis konnte sie das Projekt realisieren. Sie übernahm die weibliche Hauptrolle und fungierte als Regisseurin, Koproduzentin sowie Drehbuchautorin. Die Dreharbeiten begannen während des Zweiten Weltkriegs am 1. August 1940. Da die Außenaufnahmen aufgrund des Einmarsches italienischer Truppen in Südfrankreich nicht in den Pyrenäen – wo die Handlung des Films spielt – gedreht werden konnten, wurde der Drehort unter anderem nach Krün und Mittenwald verlegt. Die Innenaufnahmen wurden ab 1942 in den Tobis-Filmateliers von Berlin-Johannisthal, der UFA-Stadt in Babelsberg und 1944 – um den Bombardements auf das Reichsgebiet zu entgehen – in den Barrandov-Ateliers in Prag gedreht. Um ihrem Film dennoch Authentizität zu verleihen, besetzte Riefenstahl die Komparsenrollen mit südländisch aussehenden Sinti und Roma, die aus den Zwangslagern Salzburg-Maxglan und Berlin-Marzahn Rastplatz rekrutiert wurden. Nach Abschluss der Dreharbeiten wurden sie ins „Zigeunerlager Auschwitz“ deportiert, wo die meisten von ihnen ums Leben kamen.

Am 21. März 1944 heiratete Riefenstahl in Kitzbühel den Gebirgsjäger-Offizier Peter Jacob, den sie 1940 während der Dreharbeiten in Mittenwald kennengelernt hatte. Kurz nach ihrer Hochzeit kam es auf dem Berghof am Obersalzberg zum letzten persönlichen Treffen von Riefenstahl und Hitler. Im Juli 1944 starb Riefenstahls Vater Alfred an einem Herzleiden, wenige Tage später fiel ihr Bruder Heinz, dessen uk-Stellung Anfang 1943 aufgehoben worden war, an der Ostfront.

Nach dem Kriegsende im Mai 1945 gelangte das ungeschnittene "Tiefland"-Filmmaterial in die Hände der französischen Besatzungsmacht, die es bis 1953 unter Verschluss hielt. Riefenstahl konnte "Tiefland" erst Ende 1953 fertigstellen, seine Uraufführung folgte am 11. Februar 1954 in Stuttgart.

1948 musste sich Riefenstahl vor Gericht der Anklage stellen, die Sinti-und-Roma-Komparsen nicht entlohnt und von ihrer Deportation ins „Zigeunerlager Auschwitz“ gewusst zu haben. Sie wurde freigesprochen. In den 1980er Jahren griff die Freiburger Filmemacherin Nina Gladitz die Vorwürfe in ihrem Dokumentarfilm "Zeit des Schweigens und der Dunkelheit" erneut auf. Riefenstahl sah sich durch den Film in ihrer Ehre verletzt und zog gegen Gladitz vor Gericht. In der zweiten und gleichzeitig letzten Instanz entschied das Oberlandesgericht Karlsruhe, dass der Dokumentarfilm weiterhin die Aussage treffen dürfe, dass Riefenstahl die Sinti und Roma zwangsverpflichtet und nicht entlohnt habe. Herausschneiden musste Gladitz hingegen die Behauptung, Riefenstahl habe von der geplanten Deportation und Ermordung ihrer Komparsen gewusst. Als die Riefenstahl-Film GmbH am 6. April 1943 die für Juden und „Zigeuner“ fällige Sonderausgleichsabgabe für 68 Berliner Sinti zahlte, waren diese schon seit März ins „Zigeunerlager Auschwitz“ deportiert.

In einem Interview mit der "Frankfurter Rundschau" am 27. April 2002 behauptete Riefenstahl: „Wir haben alle Zigeuner, die in "Tiefland" mitgewirkt haben, nach Kriegsende wiedergesehen. Keinem einzigen ist etwas passiert.“ Nachdem der Verein Rom e. V. Strafantrag gestellt hatte, kam es zu einem staatsanwaltlichen Ermittlungsverfahren gegen Riefenstahl wegen Verunglimpfung des Andenkens Verstorbener, das jedoch aufgrund mangelnden öffentlichen Interesses eingestellt wurde. Die Filmemacherin hatte sich zuvor in einer Unterlassungserklärung verpflichtet, derartige Behauptungen nicht länger aufzustellen und in einer öffentlichen Stellungnahme ihr Bedauern über die Verfolgung und das in den Konzentrationslagern erlittene Leid der Sinti und Roma geäußert.

Nach dem Ende des Zweiten Weltkriegs wurde Riefenstahl in ihrem Haus bei Kitzbühel im April 1945 verhaftet und in das Gefangenenlager der 7. US-Armee nach Dachau gebracht, wo sie mehrfach zu ihrer Rolle im Dritten Reich verhört und mit Bildern aus den Konzentrationslagern konfrontiert wurde. Am 3. Juni 1945 wurde sie aus der Haft entlassen und sie kehrte in das inzwischen von den Franzosen besetzte Kitzbühel zurück. Ein knappes Jahr später, am 15. April 1946, wurden Riefenstahl, ihr Ehemann und ihre Mutter aus Österreich ausgewiesen, woraufhin sie sich in Königsfeld im Schwarzwald niederließen. Im Mai 1947 wurde Riefenstahl von der französischen Besatzungsmacht wegen angeblicher Depressionen in eine psychiatrische Anstalt in Freiburg eingewiesen, wo sie nach eigener Aussage mehrere Monate lang mit Elektroschocks behandelt wurde. Im Sommer 1947 wurde ihre Ehe mit Peter Jacob geschieden.

Nach ihrer Entlassung aus der Anstalt zog Riefenstahl mit ihrer Mutter nach München-Schwabing. In den Jahren 1948 bis 1952 wurde sie in vier Spruchkammerverfahren entnazifiziert. In den ersten beiden Verfahren in Villingen im November 1948 und Juli 1949 in Freiburg wurde sie als „nicht betroffen“ eingestuft. Im dritten Spruchkammerverfahren wiederum in Freiburg wurde sie am 16. Dezember 1949 zur „Mitläuferin“ des Naziregimes erklärt. Diese Einstufung, mit der außer dem Verlust des passiven Wahlrechts keine Sanktionen verbunden waren, wurde mit dem Beschluss der Berliner Spruchkammer vom 21. April 1952 endgültig bestätigt. Obwohl ihr kein Berufsverbot erteilt wurde, konnte Riefenstahl nach 1945 – abgesehen von der Fertigstellung des Films "Tiefland" – kein weiteres Filmprojekt realisieren. Ihre Beteiligung an der NS-Propaganda und ihre Nähe zu Hitler blieben als Makel an ihr haften, weshalb sich viele Investoren im deutschen Nachkriegsfilmgeschäft von ihr distanzierten. Harry R. Sokal, der zwischen 1923 und 1932 viele ihrer Projekte unterstützt hatte und der auch nach 1945 den Kontakt zu ihr hielt, verfügte nach dem Krieg nicht mehr über die nötigen finanziellen Mittel. So blieben die von ihr geschriebenen Drehbücher "Der Tänzer von Florenz", "Ewige Gipfel" und "Die roten Teufel" unverfilmt und auch für den Streifen "Friedrich der Große und Voltaire", bei dem Jean Cocteau unter ihrer Regie eine Doppelrolle spielen wollte, fand sich kein Produzent.

Inspiriert von Ernest Hemingways Jagdgeschichte "Die grünen Hügel Afrikas" reiste Riefenstahl Mitte der 1950er Jahre nach Kenia und in den Sudan. Dort wollte sie unter dem Titel "Die schwarze Fracht" einen Film über den modernen Sklavenhandel zwischen Ostafrika und den südarabischen Ländern drehen. Eigens dazu gründete sie im Juli 1956 mit Walter Traut die Stern-Film GmbH und begab sich im Norden Kenias auf die Suche nach geeigneten Drehorten und Darstellern. Ihre Reisen brauchten das ihr zur Verfügung stehende Budget jedoch schon nach kurzer Zeit auf, weshalb auch dieses Filmvorhaben scheiterte. Zwei weitere Afrika-Projekte, der Film "Afrikanische Symphonie" und die Dokumentation "Der Nil", ließen sich ebenfalls aus finanziellen Gründen nicht umsetzen.

Ein Bild des britischen Fotografen George Rodger, das in einer Ausgabe des "Sterns" abgedruckt war und zwei muskulöse, mit weißer Asche bestäubte Nuba-Ringkämpfer zeigte, weckte Riefenstahls Interesse an der sudanesischen Volksgruppe. 1962 begab sie sich auf eine Expedition in den Sudan, wo sie auf die Masakin-Qisar, einen der etwa 100 Nuba-Stämme, traf. Sie blieb bis August 1963 bei dem Stamm und belichtete mehr als 200 Farbfilme. Fortan besuchte sie die Nuba alle zwei Jahre, studierte ihre Lebensweise und erlernte ihre Sprache. Begleitet wurde sie von ihrem Lebensgefährten Horst Kettner, der ihr assistierte und den sie zum Kameramann ausbildete.

Die ersten Nuba-Fotos Riefenstahls wurden 1964 in der Illustrierten "Kristall" veröffentlicht. Es folgten eine Fotoserie mit dem Titel "African Kingdom" im Time Life Verlag sowie Fotostrecken in der französischen "Paris Match", der italienischen Wochenzeitschrift "L’Europeo" und im US-amerikanischen "Life Magazine". Im Dezember 1969 brachte der "Stern" die mit 20 Aufnahmen illustrierte Titelgeschichte "Leni Riefenstahl fotografierte die Nuba – Bilder die noch keiner sah" heraus.

Bei den Olympischen Spielen in München 1972 arbeitete sie als akkreditierte Fotografin für die "Sunday Times". 1973 veröffentlichte sie ihren ersten Bildband mit dem Titel "Die Nuba – Menschen wie vom anderen Stern", mit dem ihr der internationale Durchbruch als Fotografin gelang. Die darin enthaltenen Fotografien dokumentieren vor allem die alltäglichen Abläufe der Nuba wie die Ernte, Körperbemalungen und rituelle Kämpfe zwischen den Männern. Im selben Jahr wurde ihr für ihre Verdienste um den Sudan von Staatspräsident Dschafar Muhammad an-Numairi ehrenhalber die sudanesische Staatsbürgerschaft verliehen. 1974 fotografierte Riefenstahl Mick und Bianca Jagger für eine Bildstrecke in der "Sunday Times". Sieben Abzüge aus dieser Serie wurden von Riefenstahl signiert und im Jahr 2014 für 45.600 Euro bei einer Auktion in Wien versteigert. 1975 erschienen weitere ihrer Nuba-Bilder in der von Rolf Gillhausen konzipierten Fotostrecke "Das Fest der Messer und der Liebe" im "Stern". Ein Jahr später brachte sie ihren zweiten erfolgreichen Bildband "Die Nuba von Kau" heraus, in dem sie den Schwerpunkt auf Porträtaufnahmen und Bilder von Zeremonien mit tanzenden Frauen legte. Für "Die Nuba von Kau" hatte Riefenstahl mit Teleobjektiven und großen Brennweiten fotografiert; damit erreichte sie einen verschwimmenden Hintergrund, während der Vordergrund umso deutlicher hervortritt. Mit "Mein Afrika" folgte 1982 ein dritter Bildband mit Riefenstahl-Fotografien des Schwarzen Kontinents.

Im Alter von 71 Jahren absolvierte Riefenstahl einen Tauchlehrgang in Kenia. Um zur Tauchscheinprüfung zugelassen zu werden, hatte sie sich als 20 Jahre jünger ausgegeben. Ihre Tauchausbildung ermöglichte es ihr, sich als Unterwasserfotografin zu betätigen und die Bildbände "Korallengärten" (1978) und "Wunder unter Wasser" (1990) zu veröffentlichen. Von 1979 an lebte Riefenstahl in einer selbst entworfenen Villa mit 1700qm Grundstück in Pöcking am Starnberger See.

Im Jahr 1987 veröffentlichte Riefenstahl ihre Memoiren, an denen sie bereits seit 1982 arbeitete. Sie wurden in mehrere Sprachen übersetzt und rangierten vor allem im Ausland auf den Bestseller-Listen. In den 1990ern wurden weltweit eine Reihe von Leni-Riefenstahl-Ausstellungen eröffnet, die sich mit der Künstlerin und ihrem Werk befassten. Den Anfang machte die von der japanischen Designerin Eiko Ishioka konzipierte Exposition "Leni Riefenstahl – Life" im Bunkamura Museum in Shibuya, Tokio, bei deren Eröffnungsfeier im Dezember 1991 Riefenstahl anwesend war.

1992 wirkte Riefenstahl an ihrer Filmbiografie "Die Macht der Bilder" des Regisseurs Ray Müller mit. Sie gab ihm ausführliche Interviews, besuchte die Drehorte ihrer Filme und gewährte Einblick in ihren Arbeitsalltag. Außerdem durfte Müller sie bei der Feier zu ihrem 90. Geburtstag und bei Fotoshootings mit Siegfried und Roy in Las Vegas sowie mit Helmut Newton filmen. "Die Macht der Bilder" wurde 1993 veröffentlicht und mit dem Emmy ausgezeichnet. 1996 folgte durch Johann Kresnik am Kölner Schauspielhaus eine Übertragung ihrer Biografie auf die Bühne. Am 30. August 1997 wurde Riefenstahl von der Cineasten-Vereinigung "Cinecon" im kalifornischen Glendale mit einem Preis für ihr Lebenswerk ausgezeichnet, da sie alle Facetten des Filmemachens repräsentiere.

Anfang 2000 wurde bekannt, dass Jodie Foster beabsichtigte, das Leben der Regisseurin auf die Kinoleinwand zu bringen. Das Drehbuch hatte die Oscar-Preisträgerin bereits mit Riefenstahl besprochen, der Beginn der Dreharbeiten wurde jedoch wiederholt verschoben. 2011 erklärte Foster schließlich, dass sie den Plan aufgegeben habe. Auch Madonna hatte Interesse an der Verfilmung von Riefenstahls Memoiren gezeigt, dieses Vorhaben aber letztlich nicht umgesetzt.

Im Februar 2000 wurde Riefenstahl von Ray Müller in den Sudan begleitet, wo er sie für die 2003 erschienene Dokumentation "Leni Riefenstahl: Ihr Traum von Afrika" bei den Nuba filmte. Auf dem Rückflug nach Karthum im März 2000 stürzte sie mit einem Hubschrauber in der Nähe von al-Ubayyid ab und überlebte schwer verletzt. Im Oktober 2000 stellte Riefenstahl auf der Frankfurter Buchmesse das von Angelika Taschen herausgegebene Buch "Leni Riefenstahl. Fünf Leben" vor.
2002 ließ sie sich für die knapp einstündige ZDF-Arte-Dokumentation "Die Maßlosigkeit, die in mir ist – Sandra Maischberger trifft Leni Riefenstahl" erneut von einem Kamerateam begleiten und von Sandra Maischberger über ihr Leben und Werk interviewen. Im selben Jahr lichtete Helmut Newton die Filmemacherin für das "Vanity-Fair"-Magazin ab und es erschien ihr 41-minütiger Dokumentarfilm "Impressionen unter Wasser". Der Film zeigt eine Auswahl an Unterwasseraufzeichnungen aus 25 Jahren, in denen Riefenstahl mit ihrem Kameramann und Lebensgefährten Horst Kettner insgesamt über 2000 Tauchgänge absolviert hatte. Die Dokumentation erschien fast 50 Jahre nach "Tiefland" und war ihr letztes Werk. Mit "Impressionen unter Wasser" verfolge sie, die Anfang der 1990er Jahre nach eigenen Angaben aktives Mitglied bei Greenpeace geworden war, das Ziel, „ins Bewusstsein zu rufen, was die Welt verliert, wenn nichts gegen die Zerstörung der Meere unternommen wird.“ Im August 2002 feierte Riefenstahl ihren 100. Geburtstag mit 160 Gästen in Feldafing; unter den Gästen waren Petra Schürmann, Reinhold Messner, Leo Kirch, Hans-Wilhelm Müller-Wohlfahrt, Uschi Glas und Willy Bogner.

Am späten Abend des 8. September 2003 starb Leni Riefenstahl kurz nach ihrem 101. Geburtstag in ihrem Haus in Pöcking. Sie wurde eingeäschert und ihre Urne am 12. September 2003 auf dem Münchner Waldfriedhof beigesetzt.

Riefenstahls 700 Kartons umfassender Nachlass wurde vom 40 Jahre jüngeren Ehe- und Kameramann Horst Kettner (1942–2016) in ihrer gemeinsamen Villa am Starnberger See aufbewahrt und ging nach dessen Tod 2016 an Riefenstahls frühere Sekretärin und Alleinerbin Gisela Jahn. Sie übergab 2018 den Nachlass an die Stiftung Preußischer Kulturbesitz in Berlin. "Der fotografische Bestand soll im Museum für Fotografie am Bahnhof Zoo untergebracht werden." Neben einem „bahnbrechenden ästhetischem Werk" (übernimmt die Stiftung) "auch eine besondere Verantwortung für die kritische Auseinandersetzung“ (besonders im Kontext des Nationalsozialismus) erläutert Hermann Parzinger, Präsident der Stiftung anlässlich der Übereignung.

Riefenstahl legte bei ihren Filmen großen Wert auf ästhetische, harmonische Aufnahmen und Symbolik. Durch den Einfluss ihres Bergfilm-Mentors Arnold Fanck hatte sie ein Gespür für die Wirkung von Landschaften und Architektur entwickelt und so wählte sie für die märchenhafte Handlung ihres Regiedebüts "Das blaue Licht" die malerische Kulisse der Brenta in Südtirol. Das Mystische der Erzählung unterstrich sie durch den Einsatz von Nebel, Licht und Schatten. Das blaue Licht, das in Vollmondnächten von einer Kristallgrotte ausgeht, symbolisiert die unerreichbaren Ideale des von Riefenstahl gespielten Bergmädchens Junta. Außerdem experimentierte sie mit Farbfiltern. Durch den Einsatz eines roten Filters gelang es ihr, den blauen Taghimmel auf den Aufnahmen fast schwarz erscheinen zu lassen, so dass sie auf Nachtdrehs mit Scheinwerfern verzichten konnte. Lange Szenen zerschnitt sie, damit die Handlung kurzweiliger wirkte – eine Technik, die sie ebenfalls bei Fanck gelernt hatte.

Der "Triumph des Willens" sollte interessanter als die statisch wirkende Wochenschau werden. Die Regisseurin positionierte ihre Kameramänner deshalb an mehreren Orten, damit sie Filmmaterial aus unterschiedlichen Blickwinkeln erhielt, was ihr einen dynamischeren Schnitt ermöglichte. Zum ersten Mal in der Dokumentarfilmgeschichte wurden zudem mobile Aufnahmen gemacht. Die Kameramänner filmten auf Rollschuhen und aus fahrenden Autos, platzierten die Kamera in einem Fahnenstangen-Fahrstuhl oder fuhren mit ihr auf Schienen. Die Übergänge der einzelnen Szenen gestaltete Riefenstahl durch die Abstimmung der Grautöne und einen gezielten Toneinsatz (akustische Klammer) möglichst fließend. Ihre Erfahrungen als Tänzerin ließ sie in die Rhythmik der Bilder und in die Choreografie der marschierenden Truppen einfließen. Durch den Schnitt setzte sie dramaturgische Höhepunkte und erzeugte starke Kontraste zwischen der gesichtslosen Menschenmasse und der Einzelperson Hitlers. Dieser wurde zum ersten Mal in Großaufnahme gezeigt und durch Kameraperspektiven wie die Untersicht überhöht inszeniert. Da sie auf eine Kommentierung des Films verzichtete, ließ sie Bilder, Gesänge, Hakenkreuzfahnen und andere Symbole sprechen.

Beim Dreh von "Olympia" ließ sich Riefenstahl von der griechischen Antike inspirieren. Da sie keine Handlung hatte, inszenierte sie die Athleten wie antike Skulpturen und heroische Archetypen, indem sie die Kameras in Zeitlupe über die halbnackten, gestählten Körper und ihre Bewegungen fahren ließ. Um den Sportlern möglichst nahe kommen zu können, entwickelte das Team viele neue Aufnahmetechniken. Es wurden Gruben ausgehoben, aus denen die Kameraleute die Stabhochspringer vor freiem Himmel filmen konnten. Um die Sprints der Läufer aufzunehmen, entwarf die Crew eine Katapultkamera, die mit den Athleten mitgehen konnte. Des Weiteren kamen Ballon-, Unterwasser- und Schienenkameras zum Einsatz. Die Kameramänner verwendeten erstmals Teleobjektive mit einer Brennweite von 600 mm, um auch von weit entfernten Sportlern Nahaufnahmen anfertigen zu können.

In beiden Olympia-Teilen wechselt die Kameraführung zwischen reportagenhaftem Panorama, Schwenk, Passagen aus der Untersicht, in Zeitlupe, mit subjektiver Kamera und Parallelfahrten. Die Montage legt Schwerpunkte auf symbolische Überhöhung durch optische Überblendungen, auf emotionalisierende Musik oder auf die Spannung zwischen sportlichem Wettkampf und Publikumsanfeuerung. Ein weiteres Gestaltungsmerkmal ist der Wechsel zwischen rein musikalisch illustrierten Passagen und Teilen, die durch Sprecher und Publikumsreaktionen scheinbar authentisch kommentiert wirken. Sequenzen, die nicht im Wettkampf eingefangen werden konnten, ließ Riefenstahl während des Trainings vordrehen. Dieses Material schnitt sie anschließend in die echten Wettkampfaufnahmen hinein, wodurch dramatische Szenen auf Spielfilmniveau entstanden. Um den Marathonlauf nicht monoton erscheinen zu lassen, griff sie ebenfalls zu Elementen des Spielfilms. Die Aufnahmen zeigen die erschöpften Läufer, gleichzeitig vermittelt die eingespielte, antreibende Musik jedoch ihren ungebrochenen Willen, das Ziel zu erreichen. Für die damalige Zeit war auch neu, dass Riefenstahl die Turmsprungszenen in unterschiedlicher Schnelligkeit und teilweise rückwärts ablaufen ließ.

Für viele Filmkenner sind die Reichsparteitagstrilogie und "Olympia" keine reinen Dokumentationen, sondern Propagandafilme und künstlerische Inszenierungen eines Führer- beziehungsweise Körperkults. Die US-amerikanische Autorin Susan Sontag schrieb in ihrem 1975 erschienenen Essay "Faszinierender Faschismus": „Will man noch einen Unterschied machen zwischen Dokumentarfilm und Propaganda, dann ist jeder, der die Filme der Riefenstahl als Dokumentarfilme verteidigt, naiv. In "Triumph des Willens" ist das Dokument (das Bild) nicht nur die Aufzeichnung der Realität; die ‚Realität‘ wurde konstruiert um dem Bild zu dienen.“ Jürgen Trimborn meint in seiner Riefenstahl-Biografie: „Keine Dokumentation über den Nationalsozialismus kommt heute ohne die Bilder aus "Triumph des Willens" aus, kein anderer Film hat unsere visuelle Vorstellung, was Nationalsozialismus war, so tief geprägt wie er.“ Differenzierter sieht es Martin Loiperdinger: „["Triumph des Willens" stellt] eine einzigartige zeitgeschichtliche Quelle dar, jedoch nicht einfach für den Nationalsozialismus, wie er wirklich gewesen ist, sondern als Dokument dafür, wie sich der Nationalsozialismus selbst gern gesehen hat.“

Während "Der Sieg des Glaubens" aufgrund von einigen verwackelten und unscharfen Bildern sowie unfreiwillig komischen Szenen als ästhetisch unvollkommen betrachtet wird, gilt "Triumph des Willens" als perfektionistisches Meisterwerk und einer der besten Propagandafilme, der je gedreht wurde. Zu den Markenzeichen der Parteitagsfilme werden insbesondere die bildliche Überhöhung Hitlers und die anmutig wirkende Massenchoreografie gezählt. "Olympia" wurde im Jahr 1956 – trotz des US-amerikanischen Boykotts Ende der 1930er Jahre – von einer Hollywood-Jury zu einem der „zehn besten Filme aller Zeiten“ gekürt. Charakteristisch für ihn seien die idealisierte Darstellung von Kraft, Eleganz und Macht anhand muskulöser, makelloser Körper. Riefenstahl habe die faschistische Ästhetik zwar nicht erfunden, sie jedoch auf geniale Weise ins Medium Film übertragen. Nachdem Riefenstahls Nuba-Bilder veröffentlicht worden waren, warf ihr Sontag vor, mit den Fotografien im Einklang mit der Nazi-Ideologie physische Stärke und Mut zu glorifizieren und nahtlos an ihre propagandistischen Filme aus der Zeit des Nationalsozialismus anzuknüpfen.

Obwohl ihre Werke umstritten sind, besteht unter Filmwissenschaftlern und -kritikern weitgehend Einigkeit, dass Riefenstahl mit ihrer für die damalige Zeit revolutionären, sehr dynamischen Schnitttechnik und der Verwendung ganz neuer Kameraperspektiven filmische Maßstäbe gesetzt hat. Ihre Filme, allen voran "Triumph des Willens" und "Olympia", haben Generationen von Künstlern nach ihr beeinflusst. George Lucas griff beispielsweise eine Einstellung aus "Triumph des Willens" für die Schlussszene in "Krieg der Sterne" auf, Quentin Tarantino ließ sich während der Vorbereitungen für "Inglourious Basterds" von Riefenstahl inspirieren und Rammstein provozierte im Musik-Video zum Cover des Depeche-Mode-Lieds "Stripped" mit Filmmaterial aus "Olympia". Auch auf Werbefilme und -kampagnen, Dokumentarfilme sowie Sportfotografien üben ihre Werke nachhaltigen Einfluss aus.

Schon zu ihren Lebzeiten galt Leni Riefenstahl als Legende und auch nach ihrem Tod ist das öffentliche Interesse an ihrer Person ungebrochen. Sie wird in zahllosen wissenschaftlichen und nichtwissenschaftlichen Veröffentlichungen thematisiert, analysiert und rezensiert – und polarisiert dabei so stark wie kaum eine andere Persönlichkeit der Filmgeschichte. Die einen sehen in ihr Hitlers „Steigbügelhalterin“ und eine Propagandistin der nationalsozialistischen Ideologie und faschistischen Ästhetik, die anderen halten sie für eine begnadete Künstlerin, der es zum Verhängnis geworden sei, dass das NS-Regime ihre Werke zu Propagandazwecken missbraucht habe.

Ernst Oppler war auf Riefenstahl als Darstellerin des modernen Tanzes in den 1920er Jahren aufmerksam geworden und porträtierte sie. Weitere Maler in den 1920er Jahren waren Eugen Spiro, Leo von König und der eher traditionelle Maler Willy Jaeckel. Ein Foto von 1921 zeigt Oppler, von König sowie Elisabeth Griebe und Riefenstahl.

In den 1930er Jahren stieg Riefenstahl in der öffentlichen Wahrnehmung vom „Bergfilm-Starlet“ zur anerkannten und gefeierten „Reichsfilmregisseurin“ auf und wurde im In- und Ausland für "Das blaue Licht", "Triumph des Willens" und "Olympia" mit Preisen ausgezeichnet. Nach den Novemberpogromen 1938 wurde Hitlers “Lady Friend” in Großbritannien und den Vereinigten Staaten jedoch weitgehend boykottiert. Auch in Deutschland verkehrte sich das hohe öffentliche Ansehen Riefenstahls nach 1945 ins Gegenteil. Im Zusammenhang mit der Berichterstattung über die Entnazifizierungsverfahren, denen sich Riefenstahl von 1948 bis 1952 stellen musste, wurden ihre einst vorbehaltlos gefeierten Werke nun kritisch bewertet und eine rein künstlerische Motivation der Filmemacherin angezweifelt. Am 1. Mai 1949 wurde von der Illustrierten "Revue" erstmals über die Zwangsverpflichtung der später ermordeten Sinti-und-Roma-Komparsen für den Film "Tiefland" berichtet. Riefenstahl legte juristische Schritte gegen die Darstellung ein und erreichte unter anderem, dass der Verleger Helmut Kindler vom Amtsgericht München wegen übler Nachrede verurteilt wurde. Insgesamt führte Riefenstahl nach 1945 rund 50 Prozesse, in denen sie sich – meistens erfolgreich – gegen Verleumdungen und üble Nachreden zur Wehr setzte.

In den 1960er Jahren ging die Berichterstattung über Riefenstahl zurück, jedoch wurden ihre ersten Nuba-Fotografien im In- und Ausland veröffentlicht. Ab den 1970er Jahren setzte vor allem im Ausland die sogenannte „Riefenstahl Renaissance“ ein: Die Künstlerin und ihre Dokumentarfilme wurden wiederentdeckt und zunehmend unkritisch gewürdigt. Die britische "Sunday Times" beauftragte sie als Fotografin bei den Olympischen Sommerspielen 1972 in München und beim Telluride Film Festival 1974 wurde sie neben Gloria Swanson und Francis Ford Coppola mit der Silver Medallion für ihre Verdienste um die Filmkunst geehrt. Andere berühmte Künstler wie Mick Jagger, Andy Warhol, George Lucas, Quentin Tarantino, Jodie Foster, Madonna oder Siegfried und Roy äußerten ihre Bewunderung und förderten damit die Rehabilitierung Riefenstahls. Jedoch gab es auch weiterhin kritische Stimmen und vereinzelte Proteste, wenn öffentliche Auftritte Riefenstahls angekündigt wurden. Für Schlagzeilen sorgte ihr Auftritt in einer von Hansjürgen Rosenbauer moderierten Folge der Talkshow "Je später der Abend" am 30. Oktober 1976. Nachdem sie auf ihre Karriere im Dritten Reich angesprochen worden war, lieferte sie sich während der Live-Sendung eine heftige Auseinandersetzung mit den beiden anderen Talkgästen Knut Kiesewetter und Elfriede Kretschmar.

Als Riefenstahl 1987 ihre Memoiren veröffentlichte, waren die Reaktionen erneut geteilt. Während die "New York Times" das über 900 Seiten lange Werk trotz der Tatsache, dass sie ihr Werk in den Dienst der Nazis gestellt habe, als „fesselnd“ bezeichnete und unter die 100 „bemerkenswerten“ Bücher des Jahres 1993 wählte, beanstandeten andere ihre fehlende Selbstreflexion und die mangelnde kritische Auseinandersetzung mit der Vergangenheit. Die Psychoanalytikerin und Autorin Margarete Mitscherlich bezeichnete Riefenstahl 1994 als „Superverleugnerin“. Zu ihrem 100. Geburtstag am 22. August 2002 sowie anlässlich ihres Todes am 8. September 2003 veröffentlichten zahlreiche Medien Retrospektiven auf ihr Leben und Werk. In ihrem Nachruf fasste die britische "Times" die Rezeption der Künstlerin zusammen: „Leni Riefenstahl war die einzige Frau, die als Filmemacherin uneingeschränkt Anerkennung fand. Aber damit hört die einhellige Zustimmung auch schon auf. Sie wurde als Bösewicht, Heldin, Lügnerin, Betrügerin, Rassistin, Opfer einer patriarchischen Gesellschaft und vorbildliche Künstlerin um der Kunst willen porträtiert. Vielleicht hat der Filmhistoriker Liam O’Leary die Widersprüche am besten auf den Punkt gebracht, als er sagte: ‚Sie war ein künstlerisches Genie und ein politischer Trottel‘.“

Riefenstahl selbst sah in der Reichsparteitagstrilogie und "Olympia" rein dokumentarische Werke und wies die NS-Propaganda-Vorwürfe der Öffentlichkeit bis zuletzt vehement zurück: „"Triumph des Willens" ist ein Dokumentarfilm von einem Parteitag, mehr nicht. Das hat nichts zu tun mit Politik. Denn ich habe aufgenommen, was sich wirklich abgespielt hat und habe es insofern überhöht, als dass ich keinen Kommentar dazu gemacht habe. Ich habe versucht, die Atmosphäre, die da war, durch Bilder auszudrücken und nicht durch einen gesprochenen Kommentar. Und um das ohne Text verständlich zu machen, musste die Bildsprache sehr gut, sehr deutlich sein. Die Bilder mussten das sagen können, was man sonst spricht. Aber deswegen ist es doch keine Propaganda.“

In ihren Memoiren und diversen Interviews behauptete Riefenstahl außerdem, dass sie sich zunächst gesträubt habe, die Reichsparteitagstrilogie zu drehen. Sie habe keine Ahnung von ihrer Begabung als Dokumentarfilmerin gehabt und eigentlich nur als Schauspielerin arbeiten wollen. Hitler habe jedoch so lange auf sie eingewirkt, bis sie sich schließlich dazu bereit erklärt habe. Sie habe Hitler jedoch das Versprechen abgenommen, dass sie danach nie wieder einen Film für ihn oder die NSDAP drehen müsse.

Stets betonte Riefenstahl, dass sie sich nicht für Politik interessiert und sich keine Gedanken über die Wirkung ihrer Werke gemacht habe. In ihrem künstlerischen Wirken sei es ihr immer nur um Ästhetik, nicht um Ideologie gegangen. Gleichwohl war sie bereit, ihren Einfluss zugunsten von Parteiinteressen einzusetzen. So intervenierte sie 1936 bei Goebbels gegen die Ernennung des vom Deutschen Archäologischen Institut ausersehenen Kandidaten zum Leiter des Zweiginstituts in Athen und verschaffte die Stelle einem Bewerber, der Landesgruppenleiter der NSDAP/AO in Griechenland war. Auch soll sie 1934 gegenüber einem britischen Reporter ihre Begeisterung über Hitlers Buch "Mein Kampf" geäußert haben: „Das Buch machte auf mich einen enormen Eindruck“, bekannte sie. „Ich wurde ein überzeugter Nationalsozialist, nachdem ich die erste Seite gelesen hatte.“ Sie sei nur eine Mitläuferin des NS-Regimes gewesen und habe erst nach dem Krieg von dessen Verbrechen erfahren. An den Chefredakteur der deutsch-jüdischen Exilzeitung "Aufbau" in New York, Manfred George, schrieb sie 1949: „Ich bin fast wahnsinnig darüber geworden, und ich fürchte, dass ich niemals mehr frei werden kann von dem Alpdruck dieses ungeheuren Leidens.“ In späteren Interviews beteuerte sie immer, die NS-Verbrechen zu verurteilen. Gleichzeitig wehrte sie sich gegen jeden Schuldvorwurf: „[…] wo liegt denn meine Schuld? Sagen Sie mir doch das. Ich habe keine Atombomben geworfen, ich habe niemanden verleugnet. Wo liegt denn meine Schuld?“

Über das Verhältnis von Leni Riefenstahl zu Adolf Hitler wurde schon in den 1930er und 1940er Jahren viel spekuliert. Dabei wurde der Regisseurin mehrfach unterstellt, eine sexuelle Beziehung mit dem Reichskanzler unterhalten zu haben. Auch nach dem Tod des Diktators wurde die Gerüchteküche immer wieder mit angeblichen Enthüllungen angeheizt. So sollte beispielsweise aus den vermeintlich echten Tagebuchaufzeichnungen von Eva Braun, die Riefenstahls ehemaliger Schauspielkollege Luis Trenker dem Boulevardblatt "Wochenend" zugespielt hatte, hervorgehen, dass die Regisseurin nackt vor Hitler getanzt habe. 1948 legten die Angehörigen Brauns und Riefenstahl rechtliche Schritte gegen den Verlag ein. Vor Gericht erwirkten sie eine einstweilige Verfügung, laut der die Aufzeichnungen eine völlig freie Darstellung aus der Feder eines noch unbekannten Autors im Tagebuch-Ich-Stil seien. Trenkers Erklärung, wie er zu den Aufzeichnungen gekommen sei, stellte sich als fadenscheinig heraus und später räumte er ein, dass er „nur einen Jux mitgemacht“ habe.

Riefenstahl selbst bestritt stets, dass zwischen ihr und Hitler mehr als eine rein freundschaftlich-berufliche Beziehung bestanden habe. Sie habe zwar gespürt, dass Hitler sie durchaus „als Frau begehrte“, aber zu Intimitäten sei es nie gekommen. Der bekanntgewordene Schriftwechsel, in dem sich die Regisseurin und der Reichskanzler siezen, ist herzlich und förmlich zugleich und unterstützt Riefenstahls Aussage. Auch der Pressechef der NSDAP, Otto Dietrich, sprach von einer über Jahre andauernden künstlerischen, kameradschaftlich-freundschaftlichen Verbundenheit der beiden. Riefenstahl erklärte ihre Freundschaft zu Hitler damit, dass sie zwischen dem Politiker und dem „Menschen Hitler“ differenziert habe. Sie leugnete nie, dass sie der Persönlichkeit des Diktators verfallen war. Sie habe an das Gute in ihm geglaubt und das Dämonische aus Verblendung zu spät erkannt.

Hitler seinerseits, der Frauen, die sich in politische und militärische Sachen einmischten, als „Greuel“ empfand, schätzte Riefenstahl für ihre Arbeit und sagte: „Vier Paradefrauen habe ich gehabt: Frau Troost, Frau Wagner, Frau Scholtz-Klink und Leni Riefenstahl.“ Als Grund für die Freundschaft der Filmemacherin und des Diktators nennen viele Autoren die charakterliche Ähnlichkeit der beiden. Riefenstahl und Hitler werden als sehr willensstarke, dominante, narzisstische und egozentrische Persönlichkeiten bezeichnet, deren Beziehung von der Identifikation mit dem jeweils anderen und der Stillung ihrer Sehnsüchte gekennzeichnet gewesen sei. Laut Margarete Mitscherlich entdeckte einer im anderen „sein seelisches Selbstbildnis, das sich mit den eigenen Phantasien über Vollkommenheit, Überlegenheit und Verführungskunst deckte.“




Artikel


Weitere Literaturangaben finden sich auf der .






</doc>
<doc id="2929" url="https://de.wikipedia.org/wiki?curid=2929" title="Ludwig Erhardt">
Ludwig Erhardt


</doc>
<doc id="2930" url="https://de.wikipedia.org/wiki?curid=2930" title="Ludwig Wittgenstein">
Ludwig Wittgenstein

Ludwig Josef Johann Wittgenstein (* 26. April 1889 in Wien; † 29. April 1951 in Cambridge) war einer der bedeutendsten Philosophen des 20. Jahrhunderts.

Er lieferte bedeutende Beiträge zur Philosophie der Logik, der Sprache und des Bewusstseins. Seine beiden Hauptwerke "Logisch-philosophische Abhandlung" (Tractatus logico-philosophicus 1921) und "Philosophische Untersuchungen" (1953, postum) wurden zu wichtigen Bezugspunkten zweier philosophischer Schulen, des Logischen Positivismus und der Analytischen Sprachphilosophie. Sein rund 20.000 Seiten umfassender philosophischer Nachlass wurde Ende Oktober 2017 in die Liste des Weltdokumentenerbes eingetragen.

Wittgenstein entstammt der österreichischen, früh assimilierten jüdischen Industriellenfamilie Wittgenstein, deren Wurzeln in der deutschen Kleinstadt Laasphe im Wittgensteiner Land liegen. Er war das jüngste von acht Kindern des Großindustriellen Karl Wittgenstein und seiner Ehefrau Leopoldine (geb. Kalmus), die aus einer jüdischen Prager Familie stammte. Karl Wittgenstein gehörte zu den erfolgreichsten Stahlindustriellen der späten Donaumonarchie, und das Ehepaar Wittgenstein wurde zu einer der reichsten Familien der Wiener Gesellschaft der Jahrhundertwende. Der Vater war ein Förderer zeitgenössischer Künstler, die Mutter eine begabte Pianistin. Im Palais Wittgenstein verkehrten musikalische Größen wie Clara Schumann, Gustav Mahler, Johannes Brahms und Richard Strauss.

Wittgenstein wurde katholisch erzogen. Er selbst wie auch seine Geschwister zeichneten sich durch außerordentliche musische und intellektuelle Fähigkeiten aus. Ludwig Wittgenstein spielte Klarinette. Sein Bruder Paul verlor im Ersten Weltkrieg den rechten Arm und machte dennoch als Pianist Karriere. Diesen Talenten stand eine problematische psychische Konstitution gegenüber: Drei seiner sieben Geschwister begingen Selbstmord (Hans, Rudolf, Kurt). Auch Wittgenstein zeigte, insbesondere nach den Erfahrungen des Ersten Weltkriegs, depressive Züge. Im Kontakt zu anderen soll er teils autoritär und rechthaberisch, teils auch übersensibel und unsicher gewirkt haben.

Ein entfernter Großcousin von Wittgenstein war der Wirtschaftswissenschaftler Friedrich August von Hayek.

Wittgensteins intellektuelle Erziehung begann mit häuslichem Privatunterricht in Wien. Ab 1903 bis 1906 besuchte er die K. k. Staats-Realschule in Linz. Adolf Hitler hat die Schule davor von 1900 bis 1904 besucht. Am 28. Oktober 1906 immatrikulierte Wittgenstein sich an der Technischen Hochschule Charlottenburg. Ursprünglich hatte er bei Ludwig Boltzmann in Wien studieren wollen. Für Berlin entschied sich Wittgenstein, weil sein Realschulzeugnis ihm die Einschreibung an der Universität erst nach einem weiteren Studium erlaubte. Dort beschäftigte sich Wittgenstein, so seine Schwester Hermine in ihren Familienerinnerungen, 

Nach dem Abschlussdiplom als Ingenieur 1908 ging Wittgenstein nach Manchester, wo er an der Universitätsabteilung für Ingenieurwissenschaften versuchte, einen Flugmotor zu bauen. Diesen Plan gab er jedoch bald auf. Danach arbeitete er an „Verbesserungsvorschlägen für Flugzeugpropeller“, einem Projekt, für das er am 17. August 1911 ein Patent erhielt. Schließlich dominierte die Philosophie: Nicht zuletzt auf Anregung Gottlob Freges, den er 1911 in Jena besuchte, nahm Wittgenstein ein Studium in Cambridge am Trinity College auf, wo er sich intensiv mit den Schriften Bertrand Russells beschäftigte, insbesondere mit den "Principia Mathematica". Sein Ziel war es, wie bei Gottlob Frege die mathematischen Axiome aus logischen Prinzipien abzuleiten. Russell zeigte sich nach den ersten Begegnungen nicht beeindruckt von Wittgenstein: Doch nach nicht einmal zwei Wochen sollte sich Russells Meinung ändern: „Ich fange an, ihn zu mögen; er kennt sich aus in der Literatur, ist sehr musikalisch, angenehm im Umgang (ein Österreicher), und ich glaube, wirklich intelligent.“ Schon bald hielt Russell Wittgenstein für höchst talentiert, und Russell war schließlich der Meinung, Wittgenstein sei geeigneter als er, sein logisch-philosophisches Werk fortzuführen. Russell urteilte über ihn
Unter anderem mit Russells Unterstützung wurde Wittgenstein im November 1911 in die elitäre Geheimgesellschaft Cambridge Apostles gewählt. In David Pinsent fand er dort seinen ersten Geliebten. Sie erwarben gemeinsam ein Holzhaus in Skjolden in Norwegen, wo Wittgenstein 1913 für einige Monate an einem System der Logik arbeitete. Dass Wittgenstein homosexuell war, hatte zuerst sein Biograph William Warren Bartley 1973 auf Grund von Aussagen anonymer Freunde Wittgensteins und zweier in Geheimschrift verfasster Tagebücher öffentlich gemacht.

Ab 1912 begann Wittgenstein mit Arbeiten an seinem ersten philosophischen Werk, der "Logisch-philosophischen Abhandlung", die er bis 1917 in einem Tagebuch als Notizen festhielt. Auch während seiner Zeit als österreichischer Freiwilliger im Ersten Weltkrieg beschäftigte er sich weiter damit, bis er das Werk schließlich im Sommer 1918 vollendete. Es erschien jedoch erst 1921 in einer fehlerhaften Version in der Zeitschrift "Annalen der Naturphilosophie". 1922 wurde schließlich eine zweisprachige Ausgabe unter dem heute bekannten Titel der englischen Übersetzung veröffentlicht: "Tractatus Logico-Philosophicus". Abgesehen von zwei kleineren philosophischen Aufsätzen und einem "Wörterbuch für Volksschulen" blieb die "Logisch-philosophische Abhandlung" das einzige zu Lebzeiten veröffentlichte Werk Wittgensteins.

Im Rahmen seiner Kontakte zu der von Ludwig von Ficker herausgegebenen Kulturzeitschrift "Der Brenner" und dem Innsbrucker „Brenner-Kreis“ lernte Wittgenstein Werke des Dichters Georg Trakl kennen. Im Juli 1914 beschloss Wittgenstein, sein beachtliches Erbe für wohltätige Zwecke zu verwenden. Gefördert wurde unter anderem Trakl mit einer einmaligen Summe von 20.000 Kronen. Auch war Wittgenstein indirekt in das Geschehen um den Tod Georg Trakls involviert. Auf Bitten Trakls, der sich nach einem Selbstmordversuch in einem Krakauer Garnisonsspital befand, reiste Wittgenstein am 5. November 1914 nach Krakau, um Trakl zu besuchen. Trakl war jedoch zwei Tage vor Wittgensteins Eintreffen in Krakau gestorben.

Im Ersten Weltkrieg kämpfte Wittgenstein als österreichischer Soldat an der Ostfront in Galizien. Durch die guten familiären Kontakte nach England – insbesondere zu Bertrand Russell – war Wittgenstein durch den Vatikan, Freunde im neutralen Norwegen und der Schweiz in der Lage, mit Freunden auf der „anderen Seite“ in Briefkontakt zu bleiben. Bei Kriegsende wurde er bei Asiago von den Italienern gefangengenommen und in das Offiziersgefängnis in Monte Cassino gebracht. Sein englischer Freund John Maynard Keynes konnte sich als Mitglied der Friedenskonferenz in Paris für seine Freilassung einsetzen. Auch mit seinem Vetter, dem späteren Nobelpreisträger Friedrich August von Hayek, mit dem er in Österreich und England in Kontakt stand, blieb er in Verbindung. Nach der Lektüre der "Kurzen Darlegung des Evangeliums" von Leo Tolstoi äußerte er gegenüber dem Freund Franz Parak den Wunsch, in Zukunft Kinder das Evangelium zu lehren. Durch die Schrecken des Krieges wurde er vom Logiker zum Mystiker im Sinne der „negativen Theologie“. So reifte in ihm der Plan, Volksschullehrer zu werden.

Mit der "Logisch-philosophischen Abhandlung" (Tractatus) vollzog Wittgenstein den linguistic turn (sprachkritische Wende) in der Philosophie. In der Variante Wittgensteins bedeutet dies unter anderem: Philosophische Probleme kann nur verstehen oder auflösen, wer begreift, durch welche Fehlanwendung von Sprache sie überhaupt erst erzeugt werden. Ziel philosophischer Analysen ist die Unterscheidung von sinnvollen und unsinnigen Sätzen durch eine Klärung der Funktionsweise von Sprache: „Alle Philosophie ist ‚Sprachkritik‘.“ Die Hauptgedanken des Tractatus erwuchsen aus der Auseinandersetzung – und in gegenseitiger Befruchtung – mit Bertrand Russell und werden meist der Philosophie des Logischen Atomismus zugerechnet.

Der Kern von Wittgensteins früher Philosophie ist die Abbildtheorie der Sprache. Danach zerfällt die Wirklichkeit in „Dinge“ (Sachen, die sich zueinander verhalten). Jedes „Ding“ hat einen „Namen“ in der Sprache. Bedeutung erhalten diese Namen erst durch ihr Zusammenstehen im Satz. Sätze zerfallen – wie die Wirklichkeit in Dinge – in deren Namen. Wenn die Anordnung von Namen im Zeichen eines Satzes die gleiche Struktur aufweist wie die Anordnung der von den Namen vertretenen Gegenstände in der Wirklichkeit, also denselben „Sachverhalt“ darstellt, wird ein Satz dadurch wahr. Bilden die Dinge in Wirklichkeit einen anderen Sachverhalt als ihre Namen im Satzzeichen, wird ein Satz dadurch falsch.

„Sinnlos“ sind dagegen Sätze, die unabhängig von Sachverhalten in der Wirklichkeit wahr oder falsch sind, also zum Beispiel Tautologien und Kontradiktionen. Wogegen Sätze „unsinnig“ genannt werden, deren Zeichen überhaupt keine Dingverbindungen in der Wirklichkeit darstellt wie: „Der Satz, den ich hiermit ausspreche, ist falsch“. Dieser Satz bezieht sich nicht auf eine mögliche Dingverbindung oder Wirklichkeit, sondern auf sich selbst, was laut Wittgenstein „Unsinn“ ergibt. Das gilt ebenso für Sätze, die vorgeben, etwas zu sagen, was über die reine Anordnung von Dingen in der Welt hinausgeht, indem sie sich zum Beispiel etwas ausbitten oder das von ihnen Vorgestellte „gut“ oder „schlecht“ nennen; denn solcher Wert, den die im Satzzeichen vorgestellte Wirklichkeit haben soll, erhellt nie nur aus ihrer Struktur und kann folglich auch nichts sein, was in einer Konstellation von Namen erscheint. Ein Wert lässt sich daher nach Wittgenstein (Tractatus 7) nicht aussprechen, höchstens „erschweigen“ (könnte daher vielleicht in durch bestimmte Haltungen informierten Reaktionen oder Taten, nie aber in ihn beschreibenden Sätzen erscheinen).

Sich selbst beschreibt die "Logisch-philosophische Abhandlung" gegen Schluss: Sein Vorwort (Wien, 1918) schließt mit den Worten: „Dagegen scheint mir die Wahrheit der hier mitgeteilten Gedanken unantastbar und definitiv. Ich bin also der Meinung, die Probleme im Wesentlichen endgültig gelöst zu haben. Und wenn ich mich hierin nicht irre, so besteht der Wert dieser Arbeit zweitens darin, daß sie zeigt, wie wenig damit getan ist, daß die Probleme gelöst sind.“

Einen Sinn spricht Wittgensteins Philosophie sich damit selber ab, da von ihr kein „Ding“-Zusammenhang, nichts „Wirkliches“, umrissen wird; vielmehr beinhaltet die gesamte Struktur der "Logisch-philosophischen Abhandlung" den „logischen Raum“ schlechthin – als „unsinnige“ Form oder Möglichkeit jedweder Wirklichkeit oder überhaupt denkbaren Sinnes. Wittgenstein legt nahe, dass das, was Sinn ermöglicht, nicht selbst sinnvoll sein kann. Später veranschaulicht Wittgenstein dies mit dem Bild des Urmeters, das selbst keine Länge habe verglichen mit Gegenständen, die zu Länge gelangten, indem sie so lang „wie“ das Urmeter seien.

Wittgenstein entwickelte in der Nachfolge von Gottlob Frege und vermutlich unabhängig von Charles S. Peirce im "Tractatus logico-philosophicus" die sogenannten Wahrheitstabellen, die heute in den meisten Lehrbüchern der Logik erwähnt werden. . Laut Wittgenstein liegt die Logik aller Einzelerkenntnis zugrunde – und markiert zugleich deren Grenze: „Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt“. In diesem Sinne gibt Wittgenstein im Vorwort der "Logisch-philosophischen Abhandlung" an: 

Mit der Veröffentlichung der "Logisch-philosophischen Abhandlung" glaubte Wittgenstein, seinen Beitrag für die Philosophie geleistet zu haben, und wandte sich anderen Tätigkeiten zu. Noch während der Kriegsgefangenschaft in Italien entschied er sich, vermutlich unter dem Eindruck der Lektüre von Leo Tolstoi, für den Beruf des Lehrers. Sein gewaltiges Erbe teilte er unter seinen Geschwistern auf, einen Teil spendete er im Laufe der Zeit jungen Künstlern, unter anderem Adolf Loos, Georg Trakl und Rainer Maria Rilke.

Zunächst besuchte Wittgenstein 1919/1920 die Lehrerbildungsanstalt in Wien. Danach wurde er für einige Jahre Volksschullehrer "in einem der kleinsten Dörfer, es heißt Trattenbach und liegt etwa eine Stunde südlich von Wien im Gebirge", war jedoch in pädagogischer Hinsicht unzufrieden. Nach zwei Jahren wechselte er in das Dorf Puchberg am Schneeberg, wo wie schon zuvor in Trattenbach immer wieder Spannungen zwischen Wittgenstein und den Eltern seiner Schüler auftraten. Binnen zweier Jahre wechselte Wittgenstein erneut die Stelle und wurde Lehrer in Otterthal, wo er auch ein – für diese Zeit fortschrittliches – "Wörterbuch für Volksschulen" schrieb und herausgab. Nachdem er im April 1926 einem elfjährigen Schüler auf den Kopf geschlagen hatte und dieser bewusstlos wurde, reichte Wittgenstein beim Bezirksschulinspektor ein Entlassungsgesuch ein, bevor offizielle Schritte eingeleitet werden konnten. Wittgenstein arbeitete daraufhin einige Monate als Gärtnergehilfe in einem Kloster in Hütteldorf bei Wien, wo er in einem Werkzeugschuppen des Gartens wohnte, und erwog auch – nicht zum ersten Mal –, als Mönch dem Klosterorden beizutreten, wovon ihm jedoch ein Abt des Klosters abriet.
Von 1926 bis 1928 erstellte er zusammen mit dem Architekten Paul Engelmann, einem Schüler von Adolf Loos, für seine Schwester Margarethe Stonborough-Wittgenstein ein repräsentatives Stadt-Palais in Wien (Haus Wittgenstein). Das im Stil der Moderne erbaute Palais wurde bald zu einem Mittelpunkt kulturellen Lebens in Wien und zu einem Treffpunkt des Wiener Kreises, einer Gruppe von Philosophen und Wissenschaftstheoretikern, mit denen er in Kontakt stand.

Wittgenstein war hauptsächlich für die innenarchitektonische Gestaltung des Hauses zuständig. Daneben war er bildhauerisch tätig und schuf eine Büste im Stile des Wiener Künstlers Michael Drobil. Auch bei diesen praktischen Tätigkeiten zeigte sich die selbstbezogene Arbeitsweise Wittgensteins. Sein Ziel war nicht allgemeiner gesellschaftlicher Nutzen, sondern er strebte intellektuelle und psychische Reinheit und Klarheit an. Später schrieb Wittgenstein rückblickend: 

Ende der 1920er Jahre begann Wittgenstein sich wieder intensiv mit philosophischen Fragen zu beschäftigen. Dabei stand er in Kontakt zu einigen Mitgliedern des Wiener Kreises, deren Diskussionen er maßgebend beeinflusste (wenngleich in einer Weise, die Wittgenstein nicht guthieß, da er der Meinung war, dass er nicht richtig verstanden worden sei). Durch einen Vortrag des intuitionistischen Mathematikers L. E. J. Brouwer wurde er – so zumindest nach einem Bericht von Herbert Feigl – schließlich nachhaltig aufgerüttelt und wandte sich wieder der Philosophie zu. Während dieser „Übergangsphase“ vertrat Wittgenstein kurzfristig eine Auffassung, die sich als eine Form des Verifikationismus beschreiben lässt: Die Kenntnis der Bedeutung von Sätzen geht einher mit der Kenntnis der einschlägigen Verifikations- oder Beweisverfahren.

Im Jahre 1929 kehrte Wittgenstein als Philosoph nach Cambridge zurück, wo er zunächst bei Bertrand Russell und George Edward Moore in einer mündlichen Prüfung über seinen Tractatus promovierte. Nach der mündlichen Doktorprüfung soll Wittgenstein seinen Prüfern auf die Schulter geklopft haben mit den Worten: "Don't worry, I know you'll never understand it." („Nehmen Sie es nicht so schwer. Ich weiß, dass Sie es wohl nie verstehen werden“). Moore schrieb in seinem Bericht zur Prüfung: "I myself consider that this is a work of genius; but, even if I am completely mistaken and it is nothing of the sort, it is well above the standard required for the Ph.D. degree." („Ich persönlich halte dieses Werk für das eines Genies; selbst wenn ich mich vollständig irren sollte, liegt es immer noch weit über den Anforderungen für den Doktorgrad.“) Da Wittgenstein sein Erbe während des Ersten Weltkriegs an seine Geschwister verteilt hatte, war seine finanzielle Lage zunächst schlecht, sodass er auf Stipendien angewiesen war. Anfang der 1930er Jahre erhielt er einen Lehrauftrag. Ab 1936 unternahm Wittgenstein mit seinem Lebenspartner Francis Skinner mehrere Reisen nach Norwegen, Wien und Russland.
Im Jahre 1939 wurde Wittgenstein in der Nachfolge von Moore zum Philosophieprofessor in Cambridge berufen; er behielt die Professur bis 1947. Kurz nach seiner Berufung erwarb er die britische Staatsbürgerschaft. Dies war insbesondere dem Umstand geschuldet, dass nach dem Anschluss Österreichs an Nazi-Deutschland am 12. März 1938 Wittgenstein nun deutscher Staatsbürger war und im Sinne der Nürnberger Gesetze als Jude galt.

Während der 1930er Jahre gab Wittgenstein zahlreiche Kurse und hielt Vorlesungen. Immer wieder versuchte er, seine neuartigen Gedanken, die er unter anderem in Auseinandersetzung mit seinem Erstlingswerk entwickelte, in einem Buch zusammenzufassen und erstellte zahlreiche Manuskripte und Typoskripte. Wichtige Schritte waren "The Blue Book" (Typoskript eines Diktats seiner Vorlesung über die Philosophie der Mathematik), "The Big Typescript" (das rasch verworfene Konzept eines Buches) und "The Brown Book" (Typoskript einer Ausarbeitung zum Thema Sprachspiele mit einer Vielzahl von Beispielen). Weitere Manuskripte waren die "Philosophischen Bemerkungen" und die "Philosophische Grammatik". Trotz seiner intensiven Bemühungen gelang es Wittgenstein nicht, sein Buchprojekt zu beenden. Etwa ab 1936 begann Wittgenstein mit den "Philosophischen Untersuchungen", die sich bis etwa 1948 hinzogen. Dieses zweite große Werk hat er selbst weitgehend fertiggestellt, es erschien jedoch erst posthum 1953. Hierdurch gelangte er schnell zu Weltruhm. Denn dieses Werk beeinflusste die Philosophiegeschichte noch stärker als die "Logisch-philosophische Abhandlung (Tractatus)". Es gilt als eines der Hauptwerke der sprachanalytischen Philosophie. In den 1940er Jahren entstand auch das Manuskript "Philosophische Bemerkungen über die Grundlagen der Mathematik".

Während des Zweiten Weltkriegs wurde Wittgenstein nochmals praktisch tätig. Er arbeitete freiwillig als Pfleger in einem Londoner Krankenhaus, 1943 schloss er sich als Laborassistent einer medizinischen Forschungsgruppe an, die den hämorrhagischen Schock untersuchte, und entwarf Experimente und Laborgeräte. Er entwickelte Apparaturen zur kontinuierlichen Messung von Puls, Blutdruck, Atemfrequenz und Atemvolumen, dabei bediente er sich auch der Erfahrungen, die er während der Entwicklung seines Flugmotors gemacht hatte.

Im Jahre 1944 nahm er seine Vorlesungen in Cambridge wieder auf. Nach dem Zweiten Weltkrieg setzte Wittgenstein seine "Philosophischen Untersuchungen" fort und arbeitete unter anderem an der Philosophie der Wahrnehmung und zu den Themen Gewissheit und Zweifel. Aber auch zu vielen kulturellen und wissenschaftstheoretischen Themen hat Wittgenstein Beiträge geliefert. 1939 schrieb er: 

Im Oktober 1947 beendete Wittgenstein seine Tätigkeit an der Universität, um sich ganz seiner Philosophie zu widmen. Er lebte von da an zurückgezogen und verbrachte einige Zeit in Irland. Der Schwerpunkt seiner Arbeiten lag auf der „Philosophie der Psychologie“, die Gegenstand des II. Teils der „Philosophischen Untersuchungen“ wurde. Es ist umstritten, ob die Aufnahme dieser Gedanken in die "Philosophischen Untersuchungen" dem Willen Wittgensteins entspricht. 1949 konnte er sein zweites Hauptwerk dann abschließen.

Wittgenstein starb 1951 an Krebs. Da Wittgenstein es ablehnte, ins Krankenhaus zu gehen, verlebte er die letzten Wochen im Hause seines Arztes, der ihn bei sich aufgenommen hatte. Als dessen Frau Wittgenstein am Tag vor seinem Tod mitteilte, seine englischen Freunde würden ihn am nächsten Tag besuchen, soll er gesagt haben: Wittgensteins Grab befindet sich auf dem "Ascension Parish Burial Ground"-Friedhof in Cambridge.

Wittgensteins Philosophie wurde zu verschiedenen Zeiten unterschiedlich wahrgenommen und interpretiert. Ein Grund dafür ist neben anderen, dass er nur ein Werk zu Lebzeiten veröffentlicht hat und dass die Herausgeber der Philosophischen Untersuchungen, was den zweiten Teil betrifft, einige zweifelhafte Entscheidungen getroffen hatten. Auch der schwer zu deutende, aphoristische Stil führt dazu, dass Wittgenstein von teilweise sehr unterschiedlichen philosophischen Schulen vereinnahmt werden konnte. So wurde er von den Mitgliedern des Wiener Kreises so gelesen, als stünde er den Gedanken des Logischen Positivismus nahe. In den 1960er Jahren gab es die Tendenz, in Wittgenstein einen Vertreter oder zumindest Vordenker der Philosophie der normalen Sprache zu sehen. Auch die inhaltliche Auseinandersetzung und technische Interpretation unterliegt noch stetigem Wandel. In der Tractatus-Interpretation stand lange die Frage nach der Natur der Wittgensteinschen "Gegenstände" im Vordergrund, in der Interpretation seiner späten Philosophie ging es lange um den Begriff der "Bedeutung", dann um das Konzept der "Sprachspiele" und Lebensform, dann um das Problem der "Privatsprache" und in den 1980er Jahren, sah es, was die Rezeptionsgeschichte betrifft, ausgehend von Saul Kripkes "Wittgenstein über Regeln und Privatsprache" so aus, als seien Wittgensteins Gedanken zum Problem des "Regelfolgens" der Schlüssel zum Verständnis des Gesamtwerkes.

Seit etwa Mitte der 1990er Jahre wird die Diskussion über Wittgensteins Philosophie beherrscht von Vertretern einer sogenannten "resoluten Lesart", die sich gegen eine "Standardinterpretation" richten. Diese Betrachtung kam mit den Arbeiten Cora Diamonds zum Tractatus auf. Besonders in den USA folgten viele Philosophen Diamond und entwarfen ein teilweise radikal von der Standardinterpretation abweichendes Bild Wittgensteins. Die beiden Hauptmerkmale dieser Richtung, die manchmal als Neuer Wittgenstein bezeichnet wird, sind die strikte Interpretation des Unsinn-Begriffs, der zufolge der gesamte Tractatus (außer dem sogenannten "Rahmen", Vorwort und Schlussbemerkungen) im wörtlichen Sinne unsinnig ist, im Gegensatz zu der gewöhnlichen Lesart, nach der die unsinnigen metaphysischen Sätze des Tractatus dennoch tiefe Wahrheiten vermitteln. Wegen dieser strikten Interpretation bezeichnen die Verfechter sich als „resolute“ Leser. Diese Richtung wurde auch „therapeutisch“ genannt, da die Sätze Wittgensteins einen therapeutischen Zweck gehabt hätten. Die zweite Klammer, die die "resoluten" Leser verbindet, ist die Überzeugung der grundsätzlichen Kontinuität von Wittgensteins Gedanken. Dem gegenüber behaupten die Vertreter der „Standardinterpretation“, mehr oder weniger einheitlich, einen Bruch in der philosophischen Entwicklung Wittgensteins.

Die Auseinandersetzung der beiden Lager geht teilweise über den in der Philosophie üblichen Schlagabtausch hinaus. Der von den "resoluten Lesern" als Galionsfigur der Standardinterpretation angesehene Peter Hacker nennt es nicht überraschend, dass die neue Interpretation wegen der heute verbreiteten postmodernen Vorliebe für das Paradoxe Anhänger findet. Während James Ferguson Conant, ein Hauptvertreter der resoluten Lesart, ironisch von einem „Schisma“ spricht und entsprechend die Anhänger der neuen Lehre „Ungläubige“ nennt, geht Rupert Read so weit, von „Tractatus-Kriegen“ zu sprechen.

Vielfältiger noch als die Ansichten zu Wittgensteins Frühwerk sind die zu seinem Spätwerk, die sich stark widersprechen. Dies liegt auch daran, dass Wittgenstein sein Werk kaum erläutert hat und bis zu seinem Tod um Formulierungen rang:

Wittgensteins meist kurze Dialoge in seinem Spätwerk gelten als stilistisch brillant. Als problematisch für das Verständnis wird angesehen, dass sein Zugang traditionslos ist; besonders der späte Wittgenstein hat in der Philosophiegeschichte keine Vorläufer und stiftet eine neue, beispiellose Art zu denken. Viele meinen daher, diese Art zu denken müsse erlernt werden wie eine fremde Sprache.

Nur wenige Philosophen haben so beißend über das Philosophieren geurteilt wie Wittgenstein in seinem späten Denken. Er hielt die „großen philosophischen Probleme“ letztlich für „Geistesstörungen“, die unter anderem entstünden, „indem man philosophiere“. Sie würden dadurch zu fixen Ideen, die einen nicht mehr loslassen – in der Regel, weil wir uns in einen unzuträglichen Sprachgebrauch verrannt haben. heißt es in den "Philosophischen Untersuchungen", der Hauptquelle seiner späten Philosophie.

Die Ähnlichkeit der Sätze „Ich habe einen Stuhl“, „Ich habe einen Eindruck“, „Ich habe Zahnschmerzen“ verführt zur Auffassung, man „habe“ Eindrücke oder Empfindungen in gleicher Weise wie „Stühle“ (raumeinnehmende Gegenstände, deren Besitz man durch Verkauf oder Einäscherung verlieren kann) – wodurch sich das Bild aufdrängt, Wörter wie „Eindruck“, „Empfindung“ oder auch „Gedanke“, „Zahl“ müssten wie „Stuhl“ für irgendwie Raumeinnehmendes – wenn nicht Sichtbares, dann Unsichtbares – stehen: etwa für „Ideen“ oder das, was man durch „Nachschauen“ in seinem „Innersten erblicken“ könne. Wittgenstein zielt darauf ab, solche unwillkürlichen Bilder (die hier etwa einen „inneren Raum“ mit „unsichtbaren Gegenständen“ suggerieren) zu überwinden, indem er zum Beispiel ihre Entstehung ins Bewusstsein hebt. Sein Philosophieren hat, wie er sagt, mit der „Entdeckung“ (und dadurch Entschärfung) „schlichten Unsinns“ zu tun, infolgedessen sich der Verstand „Beulen“ – „beim Anrennen an die Grenzen der Sprache“ – geholt habe.

Bis zu diesem Punkt sind sich die Interpreten einig, neigen dann aber dazu, die Schlussfolgerungen, die Wittgenstein zieht, unterschiedlich zu deuten. Seine „Philosophie“, sagt er, lasse „alles, wie es ist“ – stelle „alles bloß hin“ und folgere nicht. „Da alles offen“ liege, sei folglich „nichts zu erklären.“

Laut dem lösungsorientiert-therapeutischen Zugang wird man Wittgensteins Spätwerk nicht gerecht, wenn man versucht, die unmittelbare Beschreibung von etwas Absolutem daraus abzulesen. Wittgenstein hat, so heißt es hier, dergleichen nirgends beschrieben, sondern – im Gegenteil – Verfahren entworfen (nie vorgeschrieben, immer nur vorgeschlagen) zur Lösung von geisteslähmenden Absolutheitsanmutungen, deren Wurzel er in der unhinterfragten Annahme bestimmter Bilder sah. Unter „Bild“ habe er die Verfestigung einer bestimmten Auffassung zu etwas Selbstverständlichem, Unhinterfragbarem, eben „Absolutem“ verstanden, Vorstellungen wie beispielsweise, Zahlen stünden für Gegenstände – oder man müsse die Zeit wie Raum vermessen können. Die Anwesenheit von Emphase oder Modaloperatoren zeige nach Wittgenstein immer auf ein Bild: etwas Verabsolutiertes.

Wittgensteins Lösungsverfahren entwickelt nun zum Beispiel Vergleichsobjekte, um den Bann eines „Bildes“ zu brechen. Ein philosophisches Problem infolge eines solcherart den Verstand lähmenden Bildes sei etwa das Messen von Zeit. Das seiner Ansicht nach problematische Bild ist hier das des Meterstabes, der das, was er vermisst, bereits einnimmt: Raum. Wie ist es so aber möglich, Zeit zu messen? Mit welchem „Meterstab“, der Zeit – Vergangenheit wie Zukunft – bereits einnähme? Zeit lässt sich also nicht messen! Was ist dann aber eine Stunde? Wittgenstein löst das Gefühl der Unsicherheit, indem er ein anderes „Vergleichsobjekt“ vorstellt: man solle Zeitmessen mit Raummessen nicht durch Meterstab, sondern Abschreiten vergleichen. Wittgenstein sage nicht, betont das Lager der Anhänger der sogenannten therapeutischen Lesart, Zeitmessen sei ein Abschreiten von Raum; er stelle lediglich als Beispiel einen anderen Vergleichsgegenstand vor: man könne Zeitmessen auch analog zum Raummessen mittels Abschreiten – statt Meterstabverwendung – sehen. So löse sich der Krampf. „Die eigentliche Entdeckung ist die, die mich fähig macht, das Philosophieren abzubrechen, wann ich will … Es wird nun an Beispielen eine Methode gezeigt, und die Reihe dieser Beispiele kann man abbrechen. – Es werden Probleme gelöst (Schwierigkeiten beseitigt), nicht "ein" Problem“.

Für die Anhänger der „metaphysischen“ Lesart ist dieser Zugang Wittgensteins eine Weiterung von Fähigkeiten, die erst einmal erworben sein wollen – vor allem die Methode der hinnehmenden Veranschaulichung von Sprachspielen, ihrer „Grammatik“ (z. B. die der „Meterstabverwendung“). Dafür habe Wittgenstein bevorzugt einerseits den Verwendungszusammenhang einiger Zentral-Begriffe dargestellt und so etwa die Bedeutung von „Bedeutung“ oder „Regel“ für seine Herangehensweise erhellt, während er andererseits z. B. mit „Sprachspiel“ oder „Familienähnlichkeit“ auch spezifische Begriffe seiner Methode unter Verwendung teilweise für deren Veranschaulichung erfundener Sprachspiele etabliert und hinreichend bestimmt. Das Wesen überhaupt aller Begriffe erkläre sich laut Wittgenstein durchgängig aus der Darstellung ihres Verwendungszusammenhangs oder Sprachspiels, wozu auch Betrachtungen nach der philologischen oder historisch-kritischen Methode gehörten, respektive Deutungen, Vergleiche von Entwicklungsstadien und Kritik.

Die „Metaphysiker“ sind dementsprechend der Meinung, „Sprachspiel“ sei ein zentraler Begriff der Spätphilosophie Wittgensteins; Lebenswirklichkeit zerfalle nach Wittgenstein unhintergehbar in beschreibbare „Regelkreise“. In der Philosophie gehe es darum, deren „Grammatik“ – paradigmatisch oder im Zusammenspiel heterogener Beispiele – darzustellen. Dies geschehe dann mit manchmal verblüffenden Ergebnissen. So ergäbe sich etwa aus dem verdeutlichten Verwendungszusammenhang von „Traum“, dass damit nichts Privates, sondern nur ein bestimmter zwischenmenschlicher Verlauf gemeint sein könne. Und es erweise sich, dass Äußerungen der ersten Person Singular keinen Wahrheitswert hätten.

Den Metaphysikern geht es ferner um die Verdeutlichung des nach ihrem Wittgenstein-Verständnis begriffsschöpfenden Weltbilds einer jeden Lebensform. „Man könnte sich vorstellen“, zitieren sie Wittgensteins "Über Gewißheit", „dass gewisse Sätze von der Form der Erfahrungssätze erstarrt wären und als Leitung für die nicht erstarrten, flüssigen Erfahrungssätze funktionieren; und dass sich dies Verhältnis mit der Zeit änderte, indem flüssige Sätze erstarren und feste flüssig werden.“ Die metaphysische Haltung blickt auf die gerade erstarrten Sätze, um anhand ihrer akuten Sinn von akutem Unsinn abzugrenzen: Offensichtliches festzustellen wie etwa, dass „Steine nicht denken können“ – aber auch weniger Offensichtliches, etwa wieweit man sinnvoll von „künstlicher Intelligenz“ reden kann. Wittgensteins Spätwerk fasziniert und beschäftigt nicht nur Sprachphilosophen, sondern auch Psychiater und Psychologen. Die Ideen Wittgensteins fordern nach Ansicht mancher geradezu dazu auf, in psychotherapeutischen Verfahren angewendet zu werden.

Aus streng „therapeutischer“ Sicht verkürzen die „Metaphysiker“ die Spätphilosophie Wittgensteins. So gesehen gehe es ihm nicht darum, Richtiges von Falschem, erlaubten von nicht-erlaubtem Sprachgebrauch, Sinn von Unsinn abzugrenzen, indem er nachweisend darstellte, was „richtig“, „erlaubt“ oder „sinnvoll“ ist. Wenn Wittgenstein über die Bedeutung von Wörtern spricht, hat dies gemäß der therapeutischen Auffassung nicht den Zweck, eine korrekte Bestimmung von Begriffen zu schaffen, sondern den, einen intellektuellen Krampf zu lösen, wie er in folgender Aussage zum Ausdruck kommt: „Was ist denn nun das Wesen von 'gut'? Es muss doch eine bestimmende Eigenschaft geben, sonst ist doch alles relativ!“

Die Diskussion des Begriffes „Sprachspiel“ steht in engem Zusammenhang mit der des Begriffs „Bedeutung“: In den "Philosophischen Untersuchungen" heißt es in § 43: „Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache.“ Im vorhergehenden Satz bemerkt Wittgenstein jedoch einschränkend: „Man kann für eine große Klasse von Fällen der Benützung des Wortes Bedeutung – wenn auch nicht für alle Fälle seiner Benützung – dieses Wort so erklären: Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache“. Die unterschiedliche Lesart der oben genannten Textpassage zeigt die unterschiedlichen Zugänge von „Therapeuten“ und „Metaphysikern“.

Die „metaphysische“ Sicht: Wittgenstein nimmt eine Bestimmung von „Bedeutung“ (des Wesens des mit dieser Buchstabenkette gekennzeichneten Begriffes) vor. Dementsprechend ist die Aufgabe nun, eine konsistente Position aus Wittgensteins Werken zu extrahieren. Auch wenn Definitionen bei Wittgenstein fast nie „klassisch“ durch die Angabe bestimmender Merkmale erfolgen, sondern von ihm – oft reihenweise – Veranschaulichendes dargestellt wird, in dessen Ähnlichkeit oder Zusammenklang der bestimmte Begriff dann „erscheint“ (Familienähnlichkeit, ein letztlich offenes Verfahren, das keine scharfen Grenzen vorsieht), wird letztlich auch damit immer etwas – und, das sei sogar Wittgensteins Pointe: auch immer hinreichend – bestimmt. § 43 der "Philosophischen Untersuchungen" wäre also durchaus als Definition aufzufassen; das einschränkende „nicht für alle Fälle“ sei eher als Index auf weitere Bestimmungen von „Bedeutung“ durch den Autor zu lesen, etwa in Teil II der "Philosophischen Untersuchungen", wo Wittgenstein in den Ansätzen einer Philosophie der Psychologie die „sekundäre Bedeutung“ beschreibt als eine haltungsbestimmte Form des Erlebens der primären, welche schlicht im Gebrauch des Wortes besteht. Da es keine über „primäre“ und „sekundäre“ hinausgehende Verwendung des Begriffs „Bedeutung“ im Spätwerke Wittgensteins gibt, neigen die Anhänger der metaphysischen Interpretation zur Auffassung, dass Wittgenstein keine weitere Interpretation vorsieht und „Bedeutung“ insofern erschöpfend bestimmt wurde.

Im Gegensatz dazu vertreten Anhänger des „therapeutischen“ Ansatzes die Meinung, Wittgenstein sei es in § 43 nicht darum gegangen, Kern und Wesen von „Bedeutung“ zu bestimmen. Die Einschränkung „nicht für alle Fälle“ sei kein Verweis auf andere Textstellen im Spätwerk des Autors, sondern hebe vielmehr hervor, dass die folgende Bestimmung nichts Immerwährendes skizziere, sondern vielmehr einen möglichen Gegenstand, welcher das Potenzial besitze, in einem Denkkrämpfe verursachenden Bild, indem es mit ihm verglichen werde, Lösungsaspekte aufzuzeigen. So könnte es z. B. befreiend wirken, die Bedeutung von „Vorstellung“ oder „Zahnschmerzen“ nicht wie die von „Stuhl“ oder „Auto“ in etwas Raumeinnehmendem zu sehen, sondern stattdessen zu versuchen, Parallelen zwischen dem mit „Vorstellung“ oder „Zahnschmerzen“ Gemeinten und geregelten Verlaufsformen („Spielen“: deren Zügen …) zu sehen. Gemäß der lösungsorientiert-therapeutischen Haltung lautet die entscheidende Frage nicht, wie sich verschiedene Bestimmungen des Bedeutungsbegriffs ergänzen oder addieren, sondern ob die von „Therapeuten“ entworfenen Gegenstände – indem man das, was einen verwirrt, mit ihnen vergleicht – in der Lage sind, Lösungen aufzuzeigen.

Der Widerstreit der Spätphilosophie-Deutungen überträgt sich auch auf die Einschätzung der Kontinuität in Wittgensteins Denken überhaupt. Die „Therapeuten“ neigen zur Annahme der Kontinuität zwischen der unbedingten Position der "Logisch-philosophischen Abhandlung (Tractatus, TLP)" und den Entkrampfungsverfahren der "Philosophischen Untersuchungen (PU)". Für die „Metaphysiker“ herrscht zwischen Früh- und Spätphilosophie ein Bruch.

In Wittgensteins Spätwerk zerfallen die Welt und sie abbildende Sprache nicht mehr in unauflösbare Dinge und deren logisch mögliche Verknüpfung in Sachverhalte oder Sätze. Nicht mehr die zeitlosen Kombinationsvorgaben der Logik bestimmen den Sprachbau. Vielmehr vergleicht Wittgenstein die Sprache nun mit einer „alten Stadt“: „Ein Gewinkel von Gässchen und Plätzen, alten und neuen Häusern mit Zubauten aus verschiedenen Zeiten: und dies umgeben von einer Menge Vororte mit geraden und regelmäßigen Straßen und mit einförmigen Häusern.“ Dennoch blieb für ihn die Sprache, ihre „Grammatik“, der Raum des Denkens und der Wirklichkeit. „Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache.“ Gebrauch aber ist die Funktion eines Ensembles von Gepflogenheiten oder einer „Lebensform“, die in „Sprachspiele“ zerfällt. „Das Wort ‚Sprachspiel‘ soll hier hervorheben, dass das Sprechen der Sprache ein Teil ist einer Tätigkeit, oder einer Lebensform.“ Mediziner haben andere Sprachspiele als Handwerker oder Kaufleute, Agnostiker andere als Gläubige. Aufgabe der Philosophie bleibt demnach die Auseinandersetzung dieses oder jenes Sprachgebrauchs. Gegenstand der Philosophie ist die Alltagssprache. Der Zweck der Philosophie ist eine Therapie. Der in einer Sprachverwirrung gefangene Mensch soll wieder befreit werden. Die späte Philosophie Wittgensteins ersetzt den Begriff „Logik“ durch „Grammatik“. Der Unterschied besteht darin, dass im Gegensatz zur Logik die „Grammatik“ als Ensemble von Gepflogenheiten einer Lebensform „Veränderungen unterworfen ist“. Die Gemeinsamkeit besteht darin, dass weder Logik noch „Grammatik“ erklärbar sind, sondern beide sich in dem, was sie ausmachen, lediglich "zeigen".

Schließlich identifizieren die „Metaphysiker“ in Wittgensteins Früh- wie Spätwerk eine anticartesianische Ablehnung des Dualismus von privater „Innenwelt“ und öffentlicher „Außenwelt“ sowie des subjektzentrierten Denkens überhaupt, nicht zuletzt durch das Auslassen jeglicher Erkenntnistheorie oder Transzendentalphilosophie.

Thomas Bernhard greift in seinem nach den beiden Schauspielerinnen und ihrem Kollegen der Uraufführung benannten Theaterstück "Ritter, Dene, Voss" (Uraufführung: 18. August 1986 in Salzburg) die Familiensituation Ludwig Wittgensteins auf und verbindet sie mit dessen Neffen Paul, der mehrmals in der psychiatrischen Klinik "Am Steinhof" bei Wien behandelt wurde und dem er seine Erinnerungen "Wittgensteins Neffe" (1982) widmete. Auch seine eigene Kritik an der österreichischen Gesellschaft des 20. Jahrhunderts legt der Autor dem Protagonisten in den Mund. Das Drama spielt im Speisezimmer der herrschaftlichen Villa der Großindustriellenfamilie Worringer. Ludwig, der nach einem Aufenthalt in Skandinavien und nach philosophischen Studien an einer englischen Universität wegen seiner psychischen Labilität im Krankenhaus „Steinhof“ lebte, kehrt für kurze Zeit ins von seinen beiden Schwestern bewohnte Haus zurück. Unterbrochen von grotesken Wutausbrüchen rechnet er, im Bernhardschen Stil, mit seinen Eltern, dem Großbürgertum und, in einem Rundumschlag, mit dem Medizin-, Kunst- und Wissenschaftsbetrieb ab.

In Libuše Monikovás autobiografisch unterlegtem Roman "Treibeis" ist Wittgensteins Tätigkeit als Volksschullehrer (1920–1926) rezipiert. Jan Prantl, einer der beiden Protagonisten (Prantl und Karla, beide tschechische Exilanten), nimmt in der zweiten Station der Handlung an einem internationalen pädagogischen Kongress am Semmering teil. Zwei Wissenschaftler aus Cambridge nutzen den Aufenthalt, um Wittgenstein-Souvenirs wie Zeugnisse, Schreibhefte und Bücher seiner ehemaligen Schüler in Trattenbach und anderen Dörfern (Puchberg) aufzukaufen (Kp. 3) und dem Seminar ihre Augenzeugenbefragungen über die Lehrertätigkeit des Philosophen vorzutragen (z. B. Affekthandlungen wie Züchtigungen, andererseits Idealismus: finanzielle Unterstützung und Förderung mathematisch begabter Schüler, Erarbeitung einer Grundwortschatzsammlung für die Rechtschreibung), die, parodistisch erzählt, von den Teilnehmern unter unterschiedlichen Aspekten diskutiert werden (Kp. 5): biografisch (Familie, Autismus, Ausgrenzung und Verspottung durch Mitschüler in Linz, Kriegsverletzung), historisch-soziologisch (armselige Gebirgsdörfer Niederösterreichs: Trattenbach, Puchberg und Otterthal), reformpädagogisch (zwar Mitwirkung der Schüler bei Herstellung der Wörterbücher, aber keine inhaltliche Beteiligung, einerseits lebensfremde mathematische Textaufgaben, andererseits sorgfältig vorbereitete Ausflüge). Der Kongressleiter fasst als Ergebnis zusammen: „Er mag ein engagierter, vielleicht sogar guter Lehrer gewesen sein. Ein Pädagoge war er nicht!“ 

Erheblichen Einfluss hatte Wittgensteins Philosophie auf das Werk von David Foster Wallace.











</doc>
<doc id="2931" url="https://de.wikipedia.org/wiki?curid=2931" title="Lanthan">
Lanthan

Lanthan [] ist ein chemisches Element mit dem Elementsymbol La und der Ordnungszahl 57. Es zählt zu den Übergangsmetallen sowie den Metallen der Seltenen Erden, im Periodensystem steht es in der 6. Periode und der 3. Nebengruppe, bzw. der 3. IUPAC-Gruppe oder Scandiumgruppe. Meist wird es auch zu den Lanthanoiden gezählt, auch wenn die f-Schale des Elementes unbesetzt ist.

Lanthan (griech. λανθάνειν, "lanthanein", „verborgen sein“) wurde 1839 vom schwedischen Chemiker und Chirurg Carl Gustav Mosander entdeckt. Aus einem vermeintlich reinen Cernitrat gewann er durch fraktionierte Kristallisation Lanthansulfat.

Lanthan kommt natürlich nur in chemischen Verbindungen vergesellschaftet mit anderen Lanthanoiden in verschiedenen Mineralien vor. Hauptsächlich sind dies:

Nach einer aufwendigen Abtrennung der anderen Lanthanbegleiter wird das Oxid mit Fluorwasserstoff zum Lanthanfluorid umgesetzt. Anschließend wird dieses mit Calcium unter Bildung von Calciumfluorid zum Lanthan reduziert. Die Abtrennung verbleibender Calciumreste und Verunreinigungen erfolgt in einer zusätzlichen Umschmelzung im Vakuum.

Das silberweiß glänzende Metall ist hämmerbar und plastisch verformbar (duktil). Es existieren drei metallische Modifikationen.

Lanthan ist unedel. Es überzieht sich an der Luft rasch mit einer weißen Oxidschicht, die in feuchter Luft zum Hydroxid weiterreagiert.

Bei Temperaturen oberhalb von 440 °C verbrennt Lanthan zu Lanthanoxid (LaO). Unter Bildung von Wasserstoff erfolgt in kaltem Wasser eine langsame, in warmen Wasser eine rasche Reaktion zum Hydroxid.

In verdünnten Säuren löst sich Lanthan unter Wasserstoffentwicklung auf.

Mit vielen Elementen reagiert es in der Wärme direkt, mit Halogenen schon bei Raumtemperatur. Lanthan und Wasserstoff bilden ein schwarzes, wasserempfindliches unstöchiometrisches Hydrid.

Lanthan ist Bestandteil im Mischmetall. Pyrophore Werkstoffe für Zündsteine enthalten 25 bis 45 Gewichtsprozent Lanthan. Darüber hinaus findet es Verwendung als Reduktionsmittel in der Metallurgie. Als Gusseisenzusatz unterstützt es die Bildung von Kugelgraphit, als Legierungszusatz bewirkt es eine Verbesserung der Oxidationsbeständigkeit. Lanthanbeimengungen reduzieren die Härte und Temperaturempfindlichkeit von Molybdän.

Hochwertige Kathoden zur Erzeugung von freien Elektronen bestehen aus Lanthanhexaborid als Ersatz für Wolframdraht. Hochreines Lanthanoxid wird in der Glasindustrie zur Herstellung hochwertiger Gläser mit hohem Brechungsindex für die Optik z. B. für Kameralinsen benutzt.

Die Cobalt-Lanthan-Legierung LaCo wird als Magnetwerkstoff, lanthandotiertes Bariumtitanat zur Herstellung von Kaltleitern (temperaturabhängige Widerstände) verwendet. In Verbindung mit Cobalt, Eisen, Mangan, Strontium u. a. dient es als Kathode für Hochtemperatur-Brennstoffzellen (SOFC). „Verunreinigtes“ Lanthan-Nickel (LaNi) findet als Wasserstoffspeicher in Nickel-Metallhydrid-Akkumulatoren Verwendung. Als Zusatz kommt es in Kohlelichtbogenlampen zur Studiobeleuchtung und in Filmvorführanlagen (historische Anwendung?) vor.


Einem Legierungsmetall mit Materialzusammensetzungen aus Lanthan und Titan wird die Wirkung zugeschrieben, dass bei spanbildender Verarbeitung die Spanlänge reduziert wird. Dadurch soll die Bearbeitung des Metalls erleichtert werden.

Im Bereich der Medizin werden aus dem Legierungsmetall korrosionsbeständige und gut sterilisierbare Instrumente hergestellt. Diese Metalllegierung mit Titan soll für Werkzeuge und Apparate für chirurgische Eingriffe besonders gut geeignet sein, da die Allergie-Neigung bei Verwendung derartiger Metalllegierung mit Titan im Verhältnis zu anderen Legierungen gering sein soll.



Lanthan wird als wenig toxisch eingestuft. Eine toxische Dosis ist bisher unbekannt. Jedoch gilt Lanthan-Pulver als stark ätzend, weil es sehr leicht durch z. B. Hautfeuchtigkeit zu basischem Lanthanhydroxid reagiert (ähnlich den Elementen Calcium und Strontium). Die letale Dosis beträgt bei Ratten 720 mg.

In Verbindungen kann Lanthan als farbloses La vorliegen.



</doc>
<doc id="2932" url="https://de.wikipedia.org/wiki?curid=2932" title="Lithium">
Lithium

Lithium (abgeleitet von "" ‚Stein‘; Aussprache [] oder auch []) ist ein chemisches Element mit dem Symbol Li und der Ordnungszahl 3. Es ist ein Element der 1. IUPAC-Gruppe, der Gruppe der Alkalimetalle, und gehört zur zweiten Periode des Periodensystems der Elemente. Lithium ist ein Leichtmetall und besitzt die geringste Dichte der unter Standardbedingungen festen Elemente.

Lithium kommt in der Natur aufgrund seiner hohen Reaktivität nicht elementar vor. Bei Raumtemperatur ist es nur in völlig trockener Luft über längere Zeit stabil, reagiert aber langsam zu Lithiumnitrid. In feuchter Luft bildet sich an der Oberfläche schnell eine mattgraue Lithiumhydroxid-Schicht aus. Wie alle Alkalimetalle reagiert elementares Lithium schon in Berührung mit der Hautfeuchtigkeit und führt so zu schweren Verätzungen und Verbrennungen. Viele Lithiumverbindungen, die in wässriger Lösung Lithiumionen bilden, sind im Gegensatz zu den entsprechenden Natrium- und Kaliumverbindungen als gesundheitsschädlich gekennzeichnet.

Als Spurenelement ist Lithium in Form seiner Salze ein häufiger Bestandteil von Mineralwasser. Im menschlichen Organismus sind geringe Mengen Lithium vorhanden; das Element ist jedoch nicht essenziell und hat keine bekannte biologische Funktion. Jedoch haben einige Lithiumsalze eine medizinische Wirkung und werden in der Lithiumtherapie bei bipolaren Affektstörungen, Manie, Depressionen und Cluster-Kopfschmerzen eingesetzt (siehe Medizin).

Als Entdecker des Lithiums gilt der Schwede Johan August Arfwedson, der im Jahr 1817 die Anwesenheit eines fremden Elements in Petalit (LiAl[SiO]) und bald darauf auch in Spodumen (LiAl[SiO]) und Lepidolith (K(Li,Al)[(Al,Si)O](F,OH)) feststellte, als er Mineralienfunde von der Insel Utö in Schweden analysierte. Sein akademischer Lehrer Jöns Jakob Berzelius schlug "Lithion", eine Ableitung zu griech. λίθος "líthos" ‚Stein‘, als Namen vor, der entsprechend den Bezeichnungen der andern beiden damals bekannten Alkalimetallen Natrium und Kalium auf das Material hinweist, aus dem es gewonnen wurde, und der sich schließlich in seiner latinisierten Form "Lithium" durchgesetzt hat.

1818 war es der deutsche Chemiker Christian Gottlob Gmelin, der bemerkte, dass Lithiumsalze eine rote Flammenfärbung ergeben. Beide Wissenschaftler scheiterten in den folgenden Jahren mit Versuchen, dieses Element zu isolieren. Dies gelang erstmals William Thomas Brande und Sir Humphry Davy im Jahr 1818 mittels eines elektrolytischen Verfahrens aus Lithiumoxid (LiO). Robert Bunsen und Augustus Matthiessen stellten 1855 durch Elektrolyse von Lithiumchlorid (LiCl) größere Mengen reinen Lithiums her. Im Jahr 1917 synthetisierte Wilhelm Schlenk aus organischen Quecksilberverbindungen die ersten lithiumorganischen Verbindungen. 

Mit der ersten kommerziellen Produktion begann 1923 die deutsche Metallgesellschaft in der Hans-Heinrich-Hütte in Langelsheim im Harz, indem eine Schmelze aus Lithium- und Kaliumchlorid (KCl) elektrolysiert wurde. 

Bis kurz nach dem Zweiten Weltkrieg gab es bis auf die Anwendung als Schmiermittel (Mineralöl, angedickt mit Lithiumstearat) und in der Glasindustrie (Lithiumcarbonat oder Lithiumoxid) kaum Anwendungen für Lithium. Dies änderte sich, als in den Vereinigten Staaten Tritium, das sich aus Lithium gewinnen lässt, für den Bau von Wasserstoffbomben benötigt wurde. Man begann mit einer breit angelegten Förderung, vor allem in Kings Mountain (North Carolina). Durch die auf Grund der kurzen Tritium-Halbwertszeit benötigten großen Lithium-Mengen wurde zwischen 1953 und 1963 ein großer Vorrat von Lithium angehäuft, das erst nach dem Ende des Kalten Krieges ab 1993 auf den Markt gebracht wurde. Neben dem Bergbau wurde nun auch die billigere Gewinnung aus Salzlaugen wichtig. Größere Mengen Lithium werden mittlerweile für Batterien, für die Polymerisation von Elastomeren, in der Bauindustrie und für die organische Synthese von Pharmazeutika und Agrochemikalien eingesetzt. Seit 2007 sind Primärbatterien und Akkumulatoren ("Sekundärbatterien") das wichtigste Segment.

Lithium hat an der Erdkruste einen Anteil von etwa 0,006 %. Es kommt damit etwas seltener als Zink, Kupfer und Wolfram sowie etwas häufiger als Kobalt, Zinn und Blei in der Erdkruste vor. Obwohl Lithium häufiger als beispielsweise Blei ist, ist seine Gewinnung durch die stärkere Verteilung schwierig. Im Trinkwasser und einigen Nahrungsmitteln wie Fleisch, Fisch, Eiern und Milchprodukten ist Lithium enthalten. So enthalten 100 g Fleisch etwa 100 μg Lithium. Verschiedene Pflanzen wie beispielsweise Tabak oder Hahnenfuß nehmen Lithiumverbindungen aus dem Boden auf und reichern sie an. Der durchschnittliche Anteil an der Trockenmasse von Pflanzen liegt zwischen 0,5 ppm und 3 ppm. Im Wasser der Weltmeere liegt die mittlere Konzentration bei 180 ppb und im Flusswasser nur bei etwa 3 ppb.

Mengenmäßig wurden 2015 außerhalb der USA 35.000 Tonnen Lithium gewonnen und überwiegend als Lithiumcarbonat (LiCO) gehandelt; die Reserven in den vorhandenen Minen werden auf rund 14,5 Millionen Tonnen geschätzt (Stand: Januar 2017). Das Weltvorkommen aus kontinentalen Solen, geothermischen Solen, aus dem Hectorit-Mineral, Ölfeld-Solen und aus dem Magma-Gestein Pegmatit wird auf 46,9 Millionen Tonnen geschätzt.

Das 2016 identifizierte Lithiumvorkommen der Erde wird auf mehr als 40 Millionen Tonnen geschätzt. Die größten Ressourcen sind in Bolivien (9 Mio. Tonnen), Chile (über 7,5 Mio. Tonnen), USA (6,7 Mio. Tonnen), Argentinien (6,5 Mio. Tonnen) und China (5,1 Mio. Tonnen) sowie Kanada, Kongo, Russland und Serbien mit jeweils 1 Mio. Tonnen vorhanden. In Brasilien und Mexiko gibt es Ressourcen von jeweils 180 000 Tonnen und in Australien Ressourcen von 130 000 Tonnen.

Lithium kommt in einigen Mineralien in Lithium-Pegmatiten vor. Die wichtigsten Minerale sind dabei Amblygonit (LiAl[PO]F), Lepidolith (K(Li,Al)[(Al,Si)O](F,OH)), Petalit (Kastor; LiAl[SiO]) und Spodumen (Triphan; LiAl[SiO]). Diese Minerale haben einen Lithiumgehalt von bis zu 9 % (bei Amblygonit). Andere, seltenere Lithiumerze sind Kryolithionit (LiNa[AlF]), das den größten Lithiumgehalt aller Mineralien aufweist, Triphylin (Li(Fe,Mn)[PO]) und Zinnwaldit (K(Li,Fe,Al)[(Al,Si)O](F,OH)). Lithiummineralien kommen in vielen Silikat-Gesteinen vor, aber meist nur in geringen Konzentrationen. Es gibt keine großen Lagerstätten. Da die Gewinnung von Lithium aus diesen Mineralien mit großem Aufwand verbunden ist, spielen sie heutzutage bei der Gewinnung von Lithium oder Lithiumverbindungen eine untergeordnete Rolle, dies könnte sich jedoch aufgrund der erwartet hohen Nachfrage ändern. Abbauorte sind vor allem die Greenbushes- und Mt.-Cattlin-Minen in Western Australia, in deren Pegmatit-Gesteinen eine hohe Lithiumkonzentration vorliegt und in denen Lithium als Nebenprodukt der Tantalgewinnung anfällt. Auch in einigen anderen Ländern wie Kanada und Russland, bis 1998 auch in Bassemer City, North Carolina, wird Spodumen zur Lithiumgewinnung abgebaut.

Europa besitzt Li-reiche Pegmatitfelder auf der Kärntner Weinebene im Bezirk Wolfsberg, in der finnischen Region Österbotten, im Erzgebirge sowie zwischen Spanien (Almendra) und Portugal (Guarda).

Die Lagerstätten in Österreich und Finnland werden durch Global Strategic Metals bzw. Keliber entwickelt und könnten ab 2016 den Betrieb aufnehmen. Das Vorkommen bei Zinnwald im Erzgebirge wird durch die SolarWorld exploriert.

Lithiumsalze, insbesondere Lithiumchlorid, kommen verbreitet auch in Salzlaugen, meist Salzseen, vor. Die Konzentration kann bis zu einem Prozent betragen. Neben der Konzentration des Lithiums ist für die Qualität der Salzlauge das Mengenverhältnis von Magnesium zu Lithium wichtig. Derzeit wird Lithium vor allem in Chile (Salar de Atacama, die mit 0,16 % mit den höchsten bekannten Lithiumgehalt aufweist), Argentinien (Salar de Hombre Muerto), den Vereinigten Staaten von Amerika (Silver Peak, Nevada) und der Volksrepublik China (Chabyêr Caka, Tibet; Taijinaier-See, Qinghai) gewonnen. Es gibt weitere lithiumhaltige Salzseen, die derzeit noch nicht zum Abbau genutzt werden, beispielsweise in China, Argentinien, Afghanistan und vor allem in Bolivien, wo in dem Salzsee Salar de Uyuni mit geschätzt 5,4 Millionen Tonnen Lithium die möglicherweise größten Ressourcen lagern. 2016 wurden in Utah (USA) Solen mit 1700 mg/L Li bekannt wo schon in den 1960ern Ölexplorationsbohrungen durchgeführt wurden.

Als Kuppelprodukte bei der Lithiumgewinnung werden häufig Kaliumcarbonat (Pottasche), Borax, Cäsium und Rubidium gewonnen.

Aufgrund der erwarteten starken Nachfrage nach Lithium für Batterien von Elektrofahrzeugen prüfen derzeit einige Unternehmen den Abbau von lithiumhaltigen Mineralien und Salzlaugen in verschiedenen Regionen der Welt inklusive Europa.

Nach dem Urknall ist neben Wasserstoff- und Heliumisotopen auch eine nennenswerte Menge des Isotops Li entstanden. Dieses ist aber zum größten Teil heute nicht mehr vorhanden, da in Sternen Lithium mit Wasserstoff im Prozess der Proton-Proton-Reaktion II fusioniert und so verbraucht wurde. In Braunen Zwergen sind Masse und Temperatur jedoch nicht hoch genug für eine Wasserstofffusion; ihre Masse erreicht nicht die dazu notwendige Größe von etwa 75 Jupitermassen. Das beim Urknall entstandene Lithium blieb somit in größeren Mengen nur in Braunen Zwergen erhalten. Lithium ist aus diesem Grund auch extraterrestrisch ein verhältnismäßig seltenes Element, kann aber zum Nachweis Brauner Zwerge dienen.

Die Verteilung von Lithium in verschiedenen Sternen ist stark unterschiedlich, auch wenn das Alter, die Masse und die Metallizität ähnlich sind. Es wird angenommen, dass Planeten einen Einfluss auf den Lithiumgehalt eines Sterns besitzen. Besitzt ein Stern keine Planeten, so ist der Lithiumgehalt hoch, während Sterne wie die Sonne, die von Planeten umgeben sind, einen nur geringen Lithiumgehalt aufweisen. Als Ursache wird vermutet, dass die Gezeitenkräfte von Planeten zu einer stärkeren Durchmischung von äußeren und inneren Schichten in Sternen beitragen, so dass mehr Lithium in einen Bereich gelangt, der heiß genug ist, um dieses zu fusionieren.

Aus lithiumhaltigen Salzlösungen wird durch Verdunsten des Wassers und Zugabe von Natriumcarbonat (Soda) Lithiumcarbonat ausgefällt. Dazu wird die Salzlake zunächst so lange an der Luft eingeengt, bis der Lithiumgehalt 0,5 % überschreitet. Durch Zugabe von Natriumcarbonat fällt daraus das schwerlösliche Lithiumcarbonat aus:

Zur Gewinnung von metallischem Lithium wird das Lithiumcarbonat zunächst mit Salzsäure umgesetzt. Dabei entstehen Kohlenstoffdioxid, das als Gas entweicht, und gelöstes Lithiumchlorid. Diese Lösung wird im Vakuumverdampfer eingeengt, bis das Chlorid auskristallisiert:

Die Apparate und Anlagen für die Lithiumchlorid-Gewinnung müssen aus Spezialstählen oder Nickellegierung sein, da die Salzlauge sehr korrosiv wirkt. Metallisches Lithium wird durch Schmelzflusselektrolyse eines bei 450-500 °C schmelzenden eutektischen Gemisches aus 52 Massenprozent Lithiumchlorid und 48 Massenprozent Kaliumchlorid hergestellt:

Das Kalium wird bei der Elektrolyse nicht abgeschieden, weil es in der Chlorid-Schmelze ein niedrigeres Elektrodenpotential hat.
Spuren von Natrium werden jedoch mit abgeschieden und machen das Lithium besonders reaktiv (vorteilhaft in der organischen Chemie, schlecht für Li-Batterien).
Das flüssige Lithium sammelt sich an der Elektrolytoberfläche und kann so relativ einfach aus der Elektrolysezelle ausgeschleust werden. Es ist ebenfalls möglich, Lithium per Elektrolyse von Lithiumchlorid in Pyridin zu gewinnen. Diese Methode eignet sich besonders gut im Labormaßstab.

Lithium ist ein silberweißes, weiches Leichtmetall. Es ist bei Raumtemperatur das leichteste aller festen Elemente (Dichte 0,534 g/cm). Nur fester Wasserstoff bei −260 °C ist mit einer Dichte von 0,0763 g/cm noch leichter.

Lithium kristallisiert – wie die anderen Alkalimetalle – in einer kubisch-raumzentrierten Kugelpackung in der mit dem Gitterparameter a = 351 pm und zwei Formeleinheiten pro Elementarzelle. Bei tiefen Temperaturen von 78 K ändert sich die Kristallstruktur durch Spontanumwandlung in eine hexagonale Struktur des Magnesium-Typs mit den Gitterparametern a = 311 pm und c = 509 pm oder nach Verformung in eine kubische Struktur des Kupfer-Typs (kubisch flächenzentriert) mit dem Gitterparameter a = 438 pm um. Die genauen Ursachen, welche Struktur gebildet wird, sind unbekannt.

Lithium hat unter den Alkalimetallen den höchsten Schmelz- und Siedepunkt sowie die größte spezifische Wärmekapazität. Lithium besitzt zwar die größte Härte aller Alkalimetalle, lässt sich bei einer Mohs-Härte von 0,6 dennoch mit dem Messer schneiden. Als typisches Metall ist es ein guter Strom- (Leitfähigkeit: etwa 18 % von Kupfer) und Wärmeleiter.

Lithium weist weitgehende Ähnlichkeit zu Magnesium auf, was sich auch in der Tatsache des Auftretens von heterotypen Mischkristallen aus Lithium und Magnesium, der sogenannten Isodimorphie zeigt. Obwohl Magnesium in der hexagonal dichtesten, Lithium dagegen in der kubisch raumzentrierten Kugelpackung kristallisiert, sind beide Metalle weitgehend "heterotyp" mischbar. Dies erfolgt aber nur in einem beschränkten Konzentrationsbereich, wobei die im Überschuss vorhandene Komponente der anderen ihr Kristallgitter „aufzwingt“.

Das Lithium-Ion weist mit −520 kJ/mol die höchste Hydratationsenthalpie aller Alkalimetallionen auf. Dadurch ist es in Wasser vollständig hydratisiert und zieht die Wassermoleküle stark an. Das Lithiumion bildet zwei Hydrathüllen, eine innere mit vier Wassermolekülen, die sehr stark über ihre Sauerstoffatome an das Lithiumion gebunden sind, und eine äußere Hülle, in der über Wasserstoffbrücken weitere Wassermoleküle mit dem Li[HO]-Ion verbunden sind. Dadurch ist der Ionenradius des hydratisierten Ions sehr groß, sogar größer als diejenigen der schweren Alkalimetalle Rubidium und Caesium, die in wässriger Lösung keine derart stark gebundenen Hydrathüllen aufweisen.

Als Gas kommt Lithium nicht nur in einzelnen Atomen, sondern auch molekular als Dilithium Li vor. Das einbindige Lithium erreicht dadurch ein volles s-Atomorbital und somit eine energetisch günstige Situation. Dilithium hat eine Bindungslänge von 267,3 pm und eine Bindungsenergie von 101 kJ/mol. Im gasförmigen Zustand liegt etwa 1 % (nach Masse) des Lithiums als Dilithium vor.

Lithium ist – wie alle Alkalimetalle – sehr reaktiv und reagiert bereitwillig mit sehr vielen Elementen und Verbindungen (wie Wasser) unter Wärmeabgabe. Unter den Alkalimetallen ist es allerdings das reaktionsträgste. Eine Besonderheit, die Lithium von den anderen Alkalimetallen unterscheidet, ist seine Reaktion mit molekularem Stickstoff zu Lithiumnitrid, die bereits bei Raumtemperatur langsam stattfindet:

Dies wird durch die hohe Ladungsdichte des Li-Ions und damit durch eine hohe Gitterenergie des Lithiumnitrids ermöglicht. Lithium hat mit −3,04 V das niedrigste Normalpotential im Periodensystem und ist somit das unedelste aller Elemente.

Wie alle Alkalimetalle wird Lithium unter Petroleum oder Paraffinöl aufbewahrt, da es sonst mit dem in der Luft enthaltenen Sauerstoff und Stickstoff reagiert.

Da die Ionenradien von Li- und Mg-Ionen vergleichbar groß sind, gibt es auch Ähnlichkeiten in den Eigenschaften von Lithium beziehungsweise Lithiumverbindungen und Magnesium oder Magnesiumverbindungen. Diese Ähnlichkeit in den Eigenschaften zweier Elemente aus benachbarten Gruppen des Periodensystems ist als Schrägbeziehung im Periodensystem bekannt. So bildet Lithium, im Gegensatz zu Natrium, viele metallorganische Verbindungen (Organolithium-Verbindungen), wie Butyllithium oder Methyllithium. Ähnliche Beziehungen bestehen auch zwischen Beryllium und Aluminium sowie zwischen Bor und Silicium.

In der Natur kommen die beiden stabilen Isotope Li (7,6 %) und Li (92,4 %) vor. Daneben sind instabile Isotope, beginnend bei Li über Li bis Li, bekannt, die nur künstlich herstellbar sind. Ihre Halbwertszeiten liegen alle im Millisekundenbereich.

Li spielt eine wichtige Rolle in der Technologie der Kernfusion. Es dient sowohl im Kernfusionsreaktor als auch in der Wasserstoffbombe als Ausgangsmaterial für die Erzeugung von Tritium, das für die energieliefernde Fusion mit Deuterium benötigt wird. Tritium entsteht im Blanket des Fusionsreaktors oder in der Wasserstoffbombe neben Helium durch Beschuss von Li mit Neutronen, die bei der Fusion anfallen, nach der Kernreaktion
Die ebenfalls mögliche Reaktion
ist weniger geeignet "(siehe Blanket)". Aus diesem Grund wird das Isotop Li bei der Lithiumgewinnung abgetrennt. Die Trennung kann beispielsweise über einen Isotopenaustausch von Lithiumamalgam und einer gelösten Lithiumverbindung (wie Lithiumchlorid in Ethanol) erfolgen. Dabei werden Ausbeuten von etwa 50 % erreicht.

Ist in einer Dreistufenbombe neben Li auch Li vorhanden (wie es beispielsweise bei Castle Bravo der Fall war), reagiert dieses mit einigen der bei der Fusion erzeugten schnellen Neutronen. Dadurch entstehen wieder Neutronen, außerdem Helium und zusätzliches Tritium. Dies führt, obwohl die Li-Neutron-Reaktion zunächst Energie verbraucht, im Endergebnis zu erhöhter Energiefreisetzung durch zusätzliche Fusionen und mehr Kernspaltungen im Bombenmantel aus Uran. Die Sprengkraft ist deshalb höher, als wenn nur der Li-Anteil der Isotopenmischung in der Bombe umgewandelt worden wäre. Da vor dem Castle-Bravo-Test angenommen wurde, das Li würde "nicht" mit den Neutronen reagieren, war die Bombe etwa 2,5-mal so stark wie erwartet.

Das Lithiumisotop Li entsteht in geringen Mengen in Kernkraftwerken durch eine Kernreaktion des (als Neutronenabsorber verwendeten) Borisotops B mit Neutronen.

Die Isotope Li, Li werden beide in Experimenten mit kalten Quantengasen verwendet. So wurde das erste Bose-Einstein-Kondensat mit dem (Boson) Isotop Li erzeugt. Li dagegen ist ein Fermion, und im Jahr 2003 ist es gelungen, Moleküle dieses Isotops in ein Suprafluid zu verwandeln.

Die heute wichtigste und am schnellsten wachsende Anwendung für Lithium ist die Verwendung in Lithium-Ionen-Akkumulatoren (oft auch als "wiederaufladbare Batterien" bezeichnet) welche z. B. in Smartphones, Laptops, Akkuwerkzeugen oder elektrisch betriebenen Fahrzeugen, wie Hybridautos, Elektroautos oder E-Bikes verwendet werden (siehe Diagramm rechts). Der größte Teil der produzierten Lithiumsalze wird nicht zum Metall reduziert, sondern entweder direkt als Lithiumcarbonat, Lithiumhydroxid, Lithiumchlorid, Lithiumbromid eingesetzt oder zu anderen Verbindungen umgesetzt. Das Metall wird nur in einigen Anwendungen benötigt. Die wichtigsten Verwendungszwecke von Lithiumverbindungen findet man im Abschnitt „Verbindungen“.

Ein Teil des produzierten Lithiummetalls wird für die Gewinnung von Lithiumverbindungen verwendet, die nicht direkt aus Lithiumcarbonat hergestellt werden können. Dies sind in erster Linie organische Lithiumverbindungen wie Butyllithium, Lithium-Wasserstoff-Verbindungen wie Lithiumhydrid (LiH) oder Lithiumaluminiumhydrid sowie Lithiumamid.

Lithium wird wegen seiner Fähigkeit, direkt mit Stickstoff zu reagieren, zu dessen Entfernung aus Gasen verwendet.

Metallisches Lithium ist ein sehr starkes Reduktionsmittel; es reduziert viele Stoffe, die mit anderen Reduktionsmitteln nicht reagieren. Es wird bei der partiellen Hydrierung von Aromaten (Birch-Reduktion) eingesetzt. In der Metallurgie wird es zur Entschwefelung, Desoxidation und Entkohlung von Metallschmelzen eingesetzt.

Da Lithium ein sehr niedriges Normalpotential besitzt, kann es in Batterien als Anode verwendet werden. Diese Lithium-Batterien haben eine hohe Energiedichte und können eine besonders hohe Spannung erzeugen. Nicht zu verwechseln sind die nicht wiederaufladbaren Lithium-Batterien mit den wiederaufladbaren Lithium-Ionen-Akkumulatoren, bei denen Lithiummetalloxide wie Lithiumcobaltoxid als Kathode und Graphit oder andere Lithiumionen einlagernde Verbindungen als Anode geschaltet sind.

Das für den Betrieb von Kernfusionsreaktoren nötige Tritium soll im Blanket des Reaktors aus Lithium-6 erbrütet werden.

Lithium wird mit einigen Metallen legiert, um deren Eigenschaften zu verbessern. Oft reichen dafür schon geringe Mengen Lithium aus. Es verbessert als Beimischung bei vielen Stoffen die Zugfestigkeit, Härte und Elastizität. Ein Beispiel für eine Lithiumlegierung ist Bahnmetall, eine Bleilegierung mit circa 0,04 % Lithium, die als Lagermaterial in Eisenbahnen verwendet wird. Auch bei Magnesium-Lithium-Legierungen und Aluminium-Lithium-Legierungen werden die mechanischen Eigenschaften durch Zusatz von Lithium verbessert. Gleichzeitig sind Lithiumlegierungen sehr leicht und werden deshalb viel in der Luft- und Raumfahrttechnik verwendet.

In der Atomphysik wird Lithium gerne verwendet, da es mit Li als einziges Alkalimetall ein stabiles fermionisches Isotop besitzt, weshalb es sich zur Erforschung der Effekte in ultrakalten fermionischen Quantengasen eignet (siehe BCS-Theorie). Gleichzeitig weist es eine sehr breite Feshbach-Resonanz auf, die es ermöglicht, die Streulänge zwischen den Atomen nach Belieben einzustellen, wobei die Magnetfelder aufgrund der Breite der Resonanz nicht besonders präzise gehalten werden müssen.

Bereits 1850 wurde Lithium in der westlichen Medizin als Mittel gegen Gicht erstmals eingesetzt. Es erwies sich jedoch als unwirksam. Auch andere Ansätze zur medizinischen Anwendung von Lithiumsalzen, so unter anderem als Mittel gegen Infektionskrankheiten, blieben erfolglos.

Erst 1949 beschrieb der australische Psychiater John Cade (1912–1980) ein mögliches Anwendungsgebiet für Lithiumsalze. Er hatte Meerschweinchen verschiedene chemische Verbindungen, darunter auch Lithiumsalze, injiziert, woraufhin diese weniger stark auf äußerliche Reize reagierten, ruhiger, aber nicht schläfrig wurden. Im Nachhinein stellte sich heraus, dass der bei den Versuchstieren beobachtete Effekt auf eine Intoxikation zurückzuführen war. Nach einem Selbstversuch von Cade wurde 1952–1954 die Verwendung von Lithiumcarbonat als Medikament zur Behandlung manischer-depressiver Patienten in einer Doppelblindstudie am Psychiatrischen Krankenhaus in Risskov (Dänemark) untersucht. Damit war der Grundstein für die Lithiumtherapie gelegt.

Bei dieser wird Lithium in Form von Salzen, wie dem Lithiumcarbonat, gegen bipolare Affektstörungen, Manie, Depression und Cluster-Kopfschmerz eingesetzt. Dabei ist die geringe therapeutische Breite zu beachten, die zwischen 0,6 mmol/l und 1,1 mmol/l liegt. Bereits wenn sich der Lithiumblutspiegel an der oberen Grenze der therapeutischen Breite bewegt, kann es bei empfindlichen Menschen zu beherrschbaren, reversiblen Nebenwirkungen kommen. Liegt der Lithiumblutspiegel jedoch deutlich über der therapeutischen Breite – also über 1,1 mmol/l – steigt die Gefahr deutlicher bis schwerer Nebenwirkungen wie Tremor, Rigor, Übelkeit, Erbrechen, Herzrhythmusstörungen und Leukozytose rasant an. Über 3,0 mmol/l besteht Lebensgefahr. Der Grund ist, dass der Stoffwechsel von Lithium und Natrium ähnlich ist. Ein zu hoher Lithiumspiegel kann durch Schwitzen oder Natrium-ausschwemmende Medikamente (natriuretische Diuretika) mit sinkendem Natriumspiegel entstehen. Der Körper versucht, den Natriumverlust zu kompensieren, indem in den Nieren dem Primärharn Natrium entzogen und in das Blut zurücktransportiert wird (Natriumretention). Neben Natrium wird dabei auch Lithium reteniert, das normalerweise gleichmäßig von den Nieren ausgeschieden wird. Die Folge ist ein erhöhter Lithiumspiegel, was bei der Einnahme von Lithium ein Drug monitoring bedingt, bei dem regelmäßig der Lithiumspiegel bestimmt und die Dosis entsprechend angepasst wird. Auch bei korrekter Dosierung kann es unter Langzeit-Behandlung mit Lithium zu Wasser- und Natrium-Verlusten (Diabetes insipidus), Übersäuerung des Blutes (Azidose) und zur Lithium-Nephropathie mit Einschränkung der Nierenfunktion kommen.

Eine Studie, die 1990 in den USA veröffentlicht wurde, beschreibt eine signifikante Verringerung von Straftaten und Suiziden in Regionen mit erhöhtem Lithiumgehalt im Trinkwasser.

Die Wirkungsweise des Lithium als Psychopharmakon ist noch nicht hinreichend erforscht. Derzeit werden insbesondere die Beeinflussung des Inositol-Stoffwechsels durch Hemmung der myo-Inositol-1-Phosphatase (Enzymklasse 3.1.3.25) und die Hemmung der Glykogensynthasekinase-3 (GSK-3) in Nervenzellen als mögliche Mechanismen diskutiert. Die antidepressive Wirkung von Lithium beruht wahrscheinlich ebenfalls auf einer Verstärkung der serotonergen Neurotransmission, also einer erhöhten Ausschüttung von Serotonin in den Synapsen, während die antimanische Wirkung mit einer Hemmung dopaminerger Rezeptoren erklärt wird. Eine weitere interessante Auswirkung von Lithiumsalzen auf den Menschen und Säugetiere wie Ratten ist die wohl damit zusammenhängende Veränderung der Circadianen Rhythmik. Diese Wirkung konnte sogar bei Pflanzen wie der Kalanchoe nachgewiesen werden. Andere serotonerge Substanzen wie LSD, Meskalin und Psilocybin zeigen ebenfalls solche Auswirkungen beim Menschen. Durch Lithium ist es im Tierversuch an "Drosophila melanogaster" gelungen, Symptome der Alzheimer-Krankheit – wie Vergesslichkeit – zu bekämpfen.

Der Altersforscher Michael Ristow stellte 2011 einen Zusammenhang zwischen dem Gehalt an Lithium in der Umwelt und der Lebenserwartung des Menschen her: Zwischen hohem Gehalt des Spurenelementes und hoher Lebenserwartung besteht ein statistisch signifikanter Zusammenhang; des Weiteren verlängern hohe Lithiumkonzentrationen die Lebenserwartung des Modellorganismus "Caenorhabditis elegans".

Lithiumverbindungen zeigen eine karminrote Flammenfärbung, die charakteristischen Spektrallinien liegen als Hauptlinien bei 670,776 und 670,791 nm; kleinere Linien liegen bei 610,3 nm. Darüber kann Lithium mit Hilfe der Flammenphotometrie nachgewiesen werden.

Ein quantitativer Nachweis mit nasschemischen Methoden ist schwierig, da die meisten Lithiumsalze leicht löslich sind. Eine Möglichkeit besteht über das Ausfällen des schwerlöslichen Lithiumphosphats. Dazu wird die zu untersuchende Probe zum Beispiel mit Natronlauge alkalisch gemacht und mit etwas Dinatriumhydrogenphosphat NaHPO versetzt. Beim Erhitzen fällt bei Anwesenheit von Li ein weißer Niederschlag aus:

Elementares Lithium in Form von Metallstaub entzündet sich an der Luft bereits bei Normaltemperatur. Aus diesem Grund muss metallisches Lithium auch unter Luftausschluss, meist in Petroleum gelagert werden. Bei höheren Temperaturen ab 190 °C wird bei Kontakt mit Luft sofort überwiegend Lithiumoxid gebildet. In reinem Sauerstoff entzündet sich Lithium ab etwa 100 °C. In einer reinen Stickstoffatmosphäre reagiert Lithium erst bei höheren Temperaturen schneller zu Lithiumnitrid. Beim Kontakt mit sauerstoff- oder halogenhaltigen Substanzen kann Lithium explosionsartig reagieren.

Da Lithium mit gängigen Feuerlöschmitteln wie Wasser, Kohlendioxid, Stickstoff oder dem inzwischen verbotenen Tetrachlorkohlenstoff stark exotherm reagiert, müssen Brände mit inerten Gasen wie z. B. Argon oder anderen Metallbrandbekämpfungsmitteln wie Salz (z. B. NaCl) gelöscht werden.

Elementares Lithium verursacht wie alle Alkalimetalle bei Hautkontakt Schäden durch Verbrennungen oder alkalische Verätzungen, weil es mit Wasser unter starker Wärmeabgabe Lithiumhydroxid bildet; dafür genügt schon die Hautfeuchtigkeit.

Lithium ist sehr reaktiv und bildet mit den meisten Nichtmetallen Verbindungen, in denen es immer in der Oxidationsstufe +I vorliegt. Diese sind in der Regel ionisch aufgebaut, haben aber im Gegensatz zu Verbindungen anderer Alkalimetalle einen hohen kovalenten Anteil. Das zeigt sich unter anderem darin, dass viele Lithiumsalze – im Gegensatz zu den entsprechenden Natrium- oder Kaliumsalzen – gut in organischen Lösungsmitteln wie Aceton oder Ethanol löslich sind. Es existieren auch kovalente organische Lithiumverbindungen. Viele Lithiumverbindungen ähneln in ihren Eigenschaften auf Grund der ähnlichen Ionenradien den entsprechenden Magnesiumverbindungen (Schrägbeziehung im Periodensystem). Die folgende Grafik bietet eine Übersicht über die wichtigsten Reaktionen des Lithiums. Auf Stöchiometrie und genaue Reaktionsbedingungen ist hier nicht geachtet:

Wasserstoff bildet mit Lithium Hydride. Die einfachste Lithium-Wasserstoff-Verbindung Lithiumhydrid LiH entsteht aus den Elementen bei 600–700 °C. Es wird als Raketentreibstoff und zur schnellen Gewinnung von Wasserstoff, beispielsweise zum Aufblasen von Rettungswesten, verwendet. Es existieren auch komplexere Hydride wie Lithiumborhydrid LiBH oder Lithiumaluminiumhydrid LiAlH. Letzteres hat in der organischen Chemie als selektiver Wasserstoffspender etwa zur Reduktion von Carbonyl- und Nitroverbindungen eine große Bedeutung.

Für die Erforschung der Kernfusion spielen Lithiumdeuterid (LiD) und Lithiumtritid (LiT) eine wichtige Rolle. Da reines Lithiumdeuterid die Energie der Wasserstoffbombe herabsetzt, wird dafür ein Gemisch aus LiD und LiT eingesetzt. Diese festen Substanzen sind leichter zu handhaben als Tritium mit seiner großen Effusionsgeschwindigkeit.

Mit Sauerstoff bildet Lithium sowohl Lithiumoxid LiO als auch Lithiumperoxid LiO.

Wenn Lithium mit Wasser reagiert, bildet sich Lithiumhydroxid, eine starke Base. Aus Lithiumhydroxid werden Lithiumfette hergestellt, die als Schmierfette für Autos verwendet werden. Da Lithiumhydroxid auch Kohlenstoffdioxid bindet, dient es in U-Booten zur Regenerierung der Luft.

Lithium bildet mit den Halogeniden Salze der Form LiX. Dies sind Lithiumfluorid, Lithiumchlorid, Lithiumbromid und Lithiumiodid. Da Lithiumchlorid sehr hygroskopisch ist, wird es – außer als Ausgangsmaterial für die Lithiumgewinnung – auch als Trockenmittel eingesetzt. Es dient zum Trocknen von Gasen, beispielsweise von Erdgas, bevor es durch die Pipeline geführt wird oder bei Klimaanlagen zur Herabsetzung der Luftfeuchte (bis 2 % relativer Luftfeuchte). Lithiumchlorid dient ferner noch zur Herabsetzung von Schmelztemperaturen, in Schweiß- und Hartlötbädern und als Schweißelektroden-Ummantelung für das Schweißen von Aluminium. Lithiumfluorid findet als Einkristall in der Infrarotspektroskopie Verwendung.

Die technisch wichtigste Lithiumverbindung ist das schwerlösliche Lithiumcarbonat. Es dient zur Gewinnung der meisten anderen Lithiumverbindungen und wird in der Glasindustrie und bei der Herstellung von Email als Flussmittel eingesetzt. Auch in der Aluminiumherstellung wird es zur Verbesserung von Leitfähigkeit und Viskosität der Schmelze zugesetzt.

Lithiumseifen sind Lithiumsalze von Fettsäuren. Sie finden vor allem als Verdickungsmittel in hochwertigen Mineralöl-basierten Schmierfetten und -wachsen sowie zur Herstellung von Bleistiften Verwendung.

Weitere Lithiumsalze sind:

Im Gegensatz zu den meisten anderen Alkalimetallorganylen spielen Lithiumorganyle eine beachtliche Rolle insbesondere in der organischen Chemie. Von besonderer Bedeutung sind "n"-Butyllithium, "tert"-Butyllithium, Methyllithium und Phenyllithium, die in Form ihrer Lösungen in Pentan, Hexan, Cyclohexan beziehungsweise gegebenenfalls Diethylether auch kommerziell verfügbar sind. Man kann sie durch direkte Umsetzung metallischen Lithiums mit Alkyl-/Arylhalogeniden gemäß
oder durch Transmetallierung zum Beispiel aus Quecksilberorganylen gemäß
herstellen.

Mit elementarem Lithium in Tetrahydrofuran (THF) anstelle von Magnesium in Diethylether lassen sich Grignard-analoge Additionsreaktionen von Alkylhalogeniden an Carbonylverbindungen mit meist besserer Ausbeute durchführen.

Auf Grund des deutlich kovalenten Charakters ist die Struktur von Lithiumorganylen nur selten durch eine einfache Li–C-Bindung zu beschreiben. Es liegen meist komplexe Strukturen, aufgebaut aus dimeren, tetrameren oder hexameren Einheiten, beziehungsweise polymere Strukturen vor. Lithiumorganyle sind hochreaktive Verbindungen, die sich an der Luft teilweise von selbst entzünden. Mit Wasser reagieren sie explosionsartig. Infolge ihrer extremen Basizität reagieren sie auch mit Lösungsmitteln, deren gebundener Wasserstoff kaum sauer ist, wie etwa THF, was die Wahl geeigneter Lösungsmittel stark einschränkt. Reaktionen mit ihnen sind nur unter Schutzgas und in getrockneten Lösungsmitteln möglich. Daher ist im Umgang mit ihnen eine gewisse Erfahrung erforderlich und große Vorsicht geboten.

Eine weitere Gruppe organischer Lithiumderivate sind die Lithiumamide des Typs LiNR, von denen insbesondere Lithiumdiisopropylamid (LDA) und Lithium-bis(trimethylsilyl)amid (LiHMDS, siehe auch HMDS) als starke Basen ohne nukleophile Aktivität Verwendung finden.

Lithiumorganyle finden vielseitige Verwendung, so als Initiatoren für die anionische Polymerisation von Olefinen, als Metallierungs-, Deprotonierungs- oder Alkylierungsmittel.

Von gewisser Bedeutung sind die sogenannten Gilman-Cuprate des Typs RCuLi.




</doc>
<doc id="2933" url="https://de.wikipedia.org/wiki?curid=2933" title="Lutetium">
Lutetium

Lutetium ist ein chemisches Element mit dem Elementsymbol Lu und der Ordnungszahl 71. Im Periodensystem steht es in der Gruppe der Lanthanoide und zählt damit auch zu den Metallen der Seltenen Erden. Wie die anderen Lanthanoide ist Lutetium ein silberglänzendes Schwermetall. Wegen der Lanthanoidenkontraktion besitzen Lutetiumatome den kleinsten Atomradius, außerdem hat das Element die höchste Dichte und den höchsten Schmelz- und Siedepunkt aller Lanthanoide.

Das Element wurde 1907 nahezu gleichzeitig, aber unabhängig voneinander von Georges Urbain, Carl Auer von Welsbach und Charles James entdeckt. Obwohl 1909 entschieden wurde, dass Urbain die Entdeckung zusteht und damit auch der von ihm vorgeschlagene Name "Lutetium" festgelegt wurde, war besonders im deutschsprachigen Raum die von Carl Auer von Welsbach vorgeschlagene Bezeichnung "Cassiopeium" (Cp) lange verbreitet.

Lutetium zählt zu den seltensten Seltenerdmetallen und wird darum und infolge der schwierigen Abtrennung von den anderen Lanthanoiden nur in geringem Umfang wirtschaftlich genutzt. Zu den wichtigsten Anwendungen des Elements zählt die Verwendung von Lutetiumoxyorthosilicat für Szintillationszähler in der Positronen-Emissions-Tomographie.

Lutetium wurde 1907 als vorletztes Lanthanoid (nur das radioaktive und damit instabile Promethium wurde später entdeckt) annähernd gleichzeitig und unabhängig voneinander durch drei Chemiker entdeckt. Sowohl der Franzose Georges Urbain, der Österreicher Carl Auer von Welsbach als auch der Amerikaner Charles James untersuchten das 1878 von Jean Charles Galissard de Marignac entdeckte Ytterbium genauer. Urbain berichtete am 4. November 1907 in der Pariser Académie des sciences, dass er durch 800fache Fraktionierung von Ytterbiumnitraten, die er aus Xenotim gewonnen hatte, aus dem Ytterbium von Marignac zwei Elemente erhalten habe. Diese nannte er "Neo-ytterbium" und "Lutecium" nach dem alten Namen von Paris, Lutetia.

Kurze Zeit später, am 19. Dezember 1907, gab Carl Auer von Welsbach als Ergebnis von Forschungen, die er seit 1905 durchführte, bekannt, dass er aus den Funkenspektren verschiedener Proben, die er durch fraktionierte Kristallisation von Ytterbium-Ammoniumoxalat gewonnen hatte, geschlossen habe, dass dieses aus zwei verschiedenen Elementen bestehen müsse. Diese nannte er "Cassiopeium" (Cp, nach dem Sternbild Cassiopeia, entspricht Lutetium) und "Aldebaranium" (Ab, nach dem Stern Aldebaran, entspricht Ytterbium). Er konnte jedoch keine Reinstoffe gewinnen.

Auch Charles James arbeitete an der Trennung von Ytterbium mit Hilfe von Ytterbium-Magnesiumnitrat-Salzen und erhielt 1907 größere Mengen der reinen Salze. Nachdem er von der Entdeckung Urbains erfahren hatte, verzichtete er jedoch auf eventuelle Ansprüche auf die Entdeckung des neuen Elements.

In der folgenden Zeit kam es zwischen Urbain und Welsbach zu einigen – durch die politischen Gegensätze zwischen Frankreich und Österreich-Ungarn verstärkten – Auseinandersetzungen um die Anerkennung als rechtmäßiger Entdecker des neuen Elements und damit auch um das Recht, den Namen des Elements festzulegen. Der internationale Atomgewichts-Ausschuss, bestehend aus Frank Wigglesworth Clarke, Wilhelm Ostwald, Thomas Edward Thorpe und Georges Urbain, entschied sich 1909 schließlich für Urbain und seine Elementnamen. Allerdings wurde der Name Neo-ytterbium zu Ytterbium geändert. Endgültig wurde der Name Lutetium für das Element 1949 von der IUPAC festgelegt. Bis dahin hatten vor allem viele deutsche Chemiker an der Bezeichnung Cassiopeium festgehalten.

Das exakte Atomgewicht wurde 1911 von Theodore William Richards anhand Lutetium(III)-bromid bestimmt, das in 15.000 fraktionierten Kristallisationen gereinigt wurde. Metallisches Lutetium wurde erstmals 1953 hergestellt.

Lutetium ist auf der Erde ein seltenes Element, seine Häufigkeit in der kontinentalen Erdkruste beträgt etwa 0,8 ppm. Es ist das seltenste Lanthanoid nach dem instabilen Promethium und Thulium, aber häufiger als Elemente wie Silber (0,079 ppm), Quecksilber oder Bismut.

Es sind keine Lutetiumminerale bekannt, das Element kommt immer als Beimengung in anderen Seltenerd-Mineralen, vor allem solchen des Yttriums und der schwereren Lanthanoide, wie Xenotim oder Gadolinit, vor. So enthält Xenotim aus Malaysia neben Yttrium, Dysprosium, Erbium und Ytterbium auch 0,4 % Lutetium. Bastnäsit als Mineral der leichteren Ceriterden enthält dagegen nur Spuren des Elements, Monazit bis zu 0,1 %.

Wichtige Quellen für Lutetium sind die Xenotimvorkommen in Malaysia (dort als Begleitmineral von Kassiterit), sowie ionenadsorbierende lateritische Tonminerale in den südchinesischen Provinzen Jiangxi und Guangdong. Aufgrund der schwierigen Gewinnung wird es nur in geringen Mengen hergestellt und eingesetzt und besitzt einen hohen Preis. Aufgrund der geringen Nachfrage wird die Versorgung mit Lutetium nicht als kritisch angesehen.

Die Gewinnung von Lutetium ist vor allem durch die schwierige Trennung der Lanthanoide kompliziert und langwierig. Die Ausgangsminerale wie Monazit oder Xenotim werden zunächst mit Säuren oder Laugen aufgeschlossen und in Lösung gebracht. Die Trennung des Lutetiums von den anderen Lanthanoiden ist dann durch verschiedene Methoden möglich, wobei die Trennung durch Ionenaustausch die technisch wichtigste Methode für Lutetium darstellt, sowie auch für andere seltene Lanthanoide. Dabei wird die Lösung mit den seltenen Erden auf ein geeignetes Harz aufgetragen, an das die einzelnen Lanthanoid-Ionen unterschiedlich stark binden. Anschließend werden sie in einer Trennsäule mit Hilfe von Komplexbildnern wie EDTA, DTPA oder HEDTA vom Harz gelöst, durch die unterschiedlich starke Bindung an das Harz erzielt man somit die Trennung der einzelnen Lanthanoide.

Eine Gewinnung von Lutetiummetall ist durch Reduktion von Lutetiumfluorid mit Calcium bei 1500 bis 1600 °C möglich.

Lutetium ist ein weiches, silberglänzendes Schwermetall. Die Lanthanoidenkontraktion bewirkt, dass Lutetium als das Lanthanoid mit der höchsten Ordnungszahl mit 175 pm den kleinsten Atomradius besitzt. In der Folge besitzt es auch mit 9,84 g/cm die höchste Dichte und den höchsten Schmelz- (1652 °C) und Siedepunkt (3330 °C) aller Lanthanoide.

Unter Standardbedingungen kristallisiert Lutetium in einer hexagonal-dichtesten Kugelpackung mit den Gitterparametern a = 351,6 pm und c = 557,3 pm. Neben dieser Struktur sind auch mehrere Hochdruckmodifikationen bekannt. Ab einem Druck von 32 GPa kristallisiert Lutetium in einer Struktur vom Samarium-Typ, einer kompliziert aufgebauten, trigonalen Kristallstruktur, mit den Gitterparametern a = 317,6 pm und c = 2177 pm. Beim Phasenübergang kommt es zu einem Volumenverlust von 1,6 %. Weitere Phasenübergänge gibt es bei einem Druck von 45 GPa, ab dem eine doppelt-hexagonal-dichteste Struktur am stabilsten ist, und bei 88 GPa mit einem Übergang zu einer verzerrten kubisch-dichtesten Struktur ("h"R24).

Unterhalb von 0,1 K, bei einem Druck von 18 GPa unterhalb von 1,2 K, wird Lutetium zum Supraleiter.

Lutetium ist ein typisches unedles Metall, das, vor allem bei höheren Temperaturen, mit den meisten Nichtmetallen reagiert. Mit Sauerstoff reagiert es bei Standardbedingungen an trockener Luft langsam, schneller bei Anwesenheit von Feuchtigkeit. Metallisches Lutetium ist wie andere unedle Metalle, vor allem bei großer Oberfläche, brennbar. Die Reaktion von Lutetium und Wasserstoff ist nicht vollständig, der Wasserstoff tritt stattdessen in die Oktaederlücken der Metallstruktur ein, und es bilden sich nicht-stöchiometrische Hydridphasen aus, wobei die genaue Zusammensetzung von der Temperatur und dem Wasserstoffdruck abhängt.

In Wasser löst sich Lutetium nur langsam, in Säuren schneller unter Wasserstoffbildung. In Lösung liegen immer dreiwertige, farblose Lutetiumionen vor.

Es sind insgesamt 34 Isotope (Lu bis Lu) und 35 Kernisomere des Lutetiums bekannt. Von diesen ist nur Lu stabil, und Lu ist mit einer Halbwertszeit von 3,8 · 10 Jahren das langlebigste. Diese beiden Isotope kommen natürlich vor, wobei Lu mit einem Anteil von 97,41 % in der natürlichen Isotopenzusammensetzung überwiegt. Daher hat ein Gramm natürliches Lutetium eine geringe Eigenstrahlung von 51,8 Bq. Alle weiteren Isotope besitzen nur kurze Halbwertszeiten, mit einem Maximum von 3,31 Jahren bei Lu.

Der langsame Zerfall von Lu zu Hf kann zur Altersbestimmung sehr alter Gesteine verwendet werden. Dabei werden die unterschiedlichen Verhältnisse der Isotope Hf und Hf bestimmt, und mit dem Verhältnis in Gesteinen bekannten Alters verglichen. Mit dieser Methode gelang eine Altersbestimmung des ältesten bekannten Marsmeteoriten ALH84001 auf 4,091 Milliarden Jahre.

Das Radionuklid Lu wird – komplexiert mit Liganden wie DOTA – als kurzreichweitiger Betastrahler in der Therapie gegen neuroendokrine Tumoren und Prostatakrebs verwendet.

Metallisches Lutetium hat keine wirtschaftliche Bedeutung, es wird nur in geringen Mengen für wissenschaftliche Zwecke verwendet. Als Legierung ist das Element wie die anderen Lanthanoide Bestandteil von Mischmetall.

In Verbindungen kann Lutetium als Katalysator für das Cracken von Erdöl und für Polymerisationsreaktionen, als Szintillatormaterial in der Positronen-Emissions-Tomographie oder als Dotierungsmittel für Magnetblasenspeicher aus Gadolinium-Gallium-Granat genutzt werden.

Lutetium besitzt keine biologische Bedeutung und ist nur in äußerst geringen Mengen im menschlichen Körper enthalten. Es wurde bei Versuchen an Ratten festgestellt, dass aufgenommenes Lutetium vor allem in der Leber, in geringeren Mengen auch in Knochen und Milz gespeichert wird.

Über toxische Effekte von Lutetium und seinen Verbindungen auf Lebewesen ist wenig bekannt. Bei Ratten wurde für Lutetiumchlorid eine akute Toxizität mit einem LD-Wert von 315 mg/kg bei intraperitonealer Gabe und 7100 mg/kg für orale Gabe über jeweils sieben Tage bestimmt. Eine chronische Toxizität konnte nicht festgestellt werden. Gelöste Lutetiumionen wirken toxisch für Bakterien wie "Aliivibrio fischeri". Lu-Ionen besitzen einen EC-Wert von 1,57 μM und sind damit in der Bakterientoxizität toxischer als Zink- oder Cadmiumionen und vergleichbar mit Kupferionen.

In Verbindungen kommt Lutetium stets in der Oxidationsstufe +3 vor.

Mit den Halogenen Fluor, Chlor, Brom und Iod bildet Lutetium jeweils ein Halogenid mit der Verhältnisformel LuX. Es handelt sich dabei um typische Salze mit Schmelzpunkten zwischen 892 °C (Lutetium(III)-chlorid) und 1184 °C (Lutetium(III)-fluorid). Mit Ausnahme von Lutetiumfluorid, das in einer Terbium(III)-chlorid-Raumstruktur kristallisiert, bilden die Lutetiumhalogenide eine Aluminiumchlorid-Schichtstruktur.

Es sind eine Reihe von metallorganischen Verbindungen bekannt. Verbindungen mit einer direkten Bindung zwischen Lutetium und Kohlenstoff sind nur in geringem Umfang bekannt, da es bei diesen wie bei vielen Übergangsmetallen leicht zu Folgereaktionen wie β-Hydrideliminierungen kommt. Sie sind daher mit sterisch anspruchsvollen Resten wie der "tert"-Butylgruppe oder einer größeren Zahl kleiner Reste wie in einem Hexamethyllutetat-Komplex [Lu(CH)] stabil. Die wichtigsten Liganden des Lutetiums sind Cyclopentadienyl und dessen Derivate. Ein Sandwichkomplex des Lutetiums ist jedoch nicht bekannt, die wichtigsten Klassen sind solche mit den Formeln CpLuX, CpLuX und CpLu (X kann dabei ein Halogenid, Hydrid, Alkoxid oder weiteres sein). Bei drei Cyclopentadienyl-Liganden werden zwei Liganden η, einer η als Brücke zu einem weiteren Lutetiumatom gebunden.

Mit Sauerstoff reagiert Lutetium zu Lutetium(III)-oxid, LuO, das wie die anderen dreiwertigen Oxide der schwereren Lanthanoide in der kubischen Lanthanoid-C-Struktur kristallisiert.

Die technisch wichtigste Lutetiumverbindung ist Lutetiumoxyorthosilicat. Diese ist, mit Cer dotiert, ein Szintillator und wird in Szintillationszählern in der Positronen-Emissions-Tomographie eingesetzt. Aufgrund der sehr kurzen Abklingzeit von 40 ns hat es dort andere Materialien wie Bismutgermanat verdrängt.

Lutetium-Aluminium-Granat (LuAG), das beispielsweise mit Europium dotiert ist, wird unter anderem in Infrarot-Lasern und als Leuchtstoff in weißen Leuchtdioden und Feldemissionsbildschirmen verwendet.

Eine Übersicht über Lutetiumverbindungen bietet die .




</doc>
<doc id="2934" url="https://de.wikipedia.org/wiki?curid=2934" title="Lanthanoide">
Lanthanoide

Lanthanoide [] („Lanthanähnliche“; griech.: Endung "-ειδἠς" ("-eides") „ähnlich“) ist eine Gruppenbezeichnung ähnlicher Elemente. Zugerechnet werden ihr das Lanthan und die 14 im Periodensystem folgenden Elemente Cer, Praseodym, Neodym, Promethium, Samarium, Europium, Gadolinium, Terbium, Dysprosium, Holmium, Erbium, Thulium, Ytterbium und Lutetium. Im Sinne des Begriffs gehört Lanthan nicht zu den Lanthanähnlichen. Hier folgt die Nomenklatur der IUPAC aber dem praktischen Gebrauch. Die Verwendung der alten Bezeichnung Lanthanide ist weiterhin erlaubt.
Alle Lanthanoide sind Metalle und werden auch als "Elemente der Lanthanreihe" bezeichnet. Sie sind ein Teil der Gruppe der Metalle der Seltenen Erden.

Die Lanthanoide werden auch als Metalle der seltenen Erden bezeichnet. Dieser Name ist aber insofern verwirrend, weil die Elemente dieser Gruppe mit Ausnahme des instabilen Promethiums keineswegs so selten sind, wie es suggeriert wird. So ist beispielsweise Cer in der Natur häufiger als die Elemente Arsen oder Blei. Am Aufbau der Erdkruste sind sie zu einem Massenanteil von 0,02 % beteiligt. Es handelt sich um insgesamt 15 Elemente der 6. Periode, von denen die 14 auf Lanthan folgenden Elemente als Untergruppe der 3. Nebengruppe aufgefasst werden können.

Aufgrund ihrer chemischen Ähnlichkeit kommen die Lanthanoide in der Natur meist vergesellschaftet vor. Da die Trennung der einzelnen Lanthanoide schwierig ist und ihre chemischen Eigenschaften sehr ähnlich sind, werden diese Elemente oft unter dem (nicht offiziellen) chemischen Symbol Ln (nicht zu verwechseln mit La für Lanthan) zusammengefasst. Viele von ihnen können aus Monazit (auch als sekundäre Ablagerungen – Monazitsande) gewonnen werden. Die häufigsten und ökonomisch wichtigsten lanthanoidführenden Minerale sind:


In fast allen Mineralen findet man eine Häufung entweder der leichten (Ce) oder der schweren Lanthanoide (Y verhält sich mineralchemisch wie ein schweres Lanthanoid). So enthält beispielsweise Monazit überwiegend Ce und La, während der Gehalt der nachfolgenden Lanthanoide mit der Ordnungszahl abnimmt (daher wird die Formel von Monazit auch immer als CePO angegeben). In Xenotim findet man genau den umgekehrten Fall (daher auch YPO). Diese meist sehr effektive Fraktionierung hat ihre Ursache in der Lanthanoiden-Kontraktion und den von Mineral zu Mineral unterschiedlich großen zur Verfügung stehenden Kristallgitterplätzen. Auch andere Mineralgruppen können bisweilen hohe Anteile an Lanthanoiden in ihre Struktur einbauen (z. B. Zirkon, Granat). Weiterhin kommen die Lanthanoide auf dem Mond in Form der sog. KREEP-Erze vor.

Die Lanthanoide sind silbrig-glänzende, relativ weiche und reaktionsfähige Metalle. Fast alle weisen die für Metalle typische dichteste Kugelpackung auf. Die Härte nimmt mit steigender Ordnungszahl zu.

Die Lanthanoide gehören wie die Actinoide zu den "inneren Übergangselementen" oder "f-Block-Elementen", da in diesen Reihen die f-Orbitale nicht vollständig mit Elektronen gefüllt sind.

Die Promethium-Isotope sind alle instabil, also radioaktiv.

Aufgrund der ähnlichen Struktur der Valenzschale verhalten sich die Lanthanoide chemisch wie die Elemente der 3. Gruppe des Periodensystems Scandium und Yttrium und bilden mit diesen zusammen die Gruppe der Seltenen Erden. An der Luft oxidieren sie schnell und werden matt. Mit Wasser reagieren sie mehr oder weniger schnell unter Bildung von Wasserstoff.

Beginnend bei Cer wird das 4f-Orbital nach und nach aufgefüllt. Es ist bei Lutetium schließlich mit 14 Elektronen vollständig besetzt. Da die 4f-Orbitale tief im Innern der Atome liegen, nehmen sie im Gegensatz zu den d-Orbitalen der übrigen Nebengruppenelemente wenig Einfluss auf das chemische Verhalten. Die Lanthanoiden-Elemente sind sich somit in ihren chemischen Eigenschaften relativ ähnlich. Sie gleichen sich so sehr, dass man sie bei der Entdeckung der Yttererde 1794 sogar für das Oxid ein und desselben Elements hielt. Das Gleiche gilt für die zahlreichen Bestandteile der Ceriterde. Gemeinsam ist ihnen die Oxidationszahl +3. Daneben treten bei einigen Elementen noch die Oxidationszahlen +2 und +4 auf.

Als Gadolinium Break wird bei den Lanthanoiden eine Unstetigkeit im Verlauf der Ionenradien zwischen Gadolinium und Terbium bezeichnet. Dieser erklärt, warum sich trotz der Ähnlichkeit der Lanthanoiden das chemische Verhalten der Elemente nach dem Gadolinium ändert. Das chemische Verhalten lässt sich am Gadolinium Break leicht beeinflussen. So reichen Spuren von Americium, damit ein Terbiumkomplex den Strukturtyp der leichteren Lanthanoiden annimmt.<ref name="DOI10.1002/nadc.20184067855">Georg Steinhauser: "Strukturchemie - Ein Hauch von Nichts." In: "Nachrichten aus der Chemie." 66, 2018, S. 118, .</ref>

Aufgrund der Lanthanoiden-Kontraktion nimmt der Atomradius innerhalb der Reihe von Cer (183 pm) bis Lutetium (172 pm) nahezu stetig ab (Ausnahmen sind Europium und Ytterbium). Dies liegt daran, dass die Elemente, die – von der Ordnungszahl ausgehend – vor den Lanthanoiden liegen, bereits die 6s- und 5p-Schale mit Elektronen aufgefüllt haben, jedoch die 4f-Schale nicht. Die Lanthanoide füllen nun die 4f-Schale mit Elektronen auf. Bei einer vereinfachten Vorstellung des Atom als aus räumlich abgetrennten Elektronenschalen bestehend, füllt sich nun eine, räumlich gesehen, näher zum Kern befindliche Elektronenschale mit Ladungsträgern. Nebenbei füllt sich der Kern selbstverständlich mit der gleichen Anzahl Protonen, wie Elektronen auf die 4f-Schale hinzukommen. Durch die dadurch bedingte stärkere Anziehung zwischen Elektronen und Protonen schrumpft der Atomradius, während die Ordnungszahl steigt.

Dieser Effekt ist eigentlich nicht außergewöhnlich, da beim Auffüllen einer Schale innerhalb einer Periode immer der Radius sinkt. Allerdings ergeben sich aus dieser Eigenschaft einige Konsequenzen:

Es gibt zahlreiche Beispiele für die Verwendung der Lanthanoide:






</doc>
<doc id="2935" url="https://de.wikipedia.org/wiki?curid=2935" title="Lawrencium">
Lawrencium

Lawrencium ist ein ausschließlich künstlich erzeugtes chemisches Element mit dem Elementsymbol Lr und der Ordnungszahl 103. Im Periodensystem steht es in der Gruppe der Actinoide (7. Periode, f-Block) und zählt auch zu den Transuranen. Lawrencium ist ein radioaktives Metall, welches aber aufgrund der geringen zur Verfügung stehenden Mengen bisher nicht als Metall dargestellt wurde. Es wurde 1961 entdeckt, als man Californium mit Bor-Kernen beschoss. Dieses Element wurde nach Ernest Lawrence benannt. Er ist der Erfinder des Zyklotrons, eines Teilchenbeschleunigers, der eine wichtige Voraussetzung zur Entdeckung vieler Transuran-Elemente war. Der Name wurde 1994 endgültig von der IUPAC bestätigt.

Lawrencium wurde 1961 erstmals von den amerikanischen Wissenschaftlern Albert Ghiorso, Torbjørn Sikkeland, Almon E. Larsh und Robert M. Latimer hergestellt, indem man Californiumisotope mit Kernen von Boratomen beschoss. Am 14. Februar 1961 gaben sie die erfolgreiche Synthese des Elements bekannt.

Anfangs wählte man als Symbol „Lw“. 1963 wurde es von der IUPAC (Internationalen Union für reine und angewandte Chemie) in „Lr“ geändert.

Im Periodensystem steht das Lawrencium mit der Ordnungszahl 103 in der Reihe der Actinoide und schließt diese ab. Sein Vorgänger ist das Nobelium, das nachfolgende Element ist das Rutherfordium, das aber schon zu den Transactinoiden gehört und ein d-Element ist. Sein Analogon in der Reihe der Lanthanoide ist das Lutetium, das gleichfalls diese abschließt.

Lawrencium ist ein radioaktives und sehr kurzlebiges Metall. Es sind zwölf Isotope bekannt, deren Halbwertszeiten zwischen wenigen Sekunden bis 11 Stunden liegen. Über weitere Eigenschaften des Elements liegen wenige Erkenntnisse vor, da die geringe Halbwertszeit empirische Studien fast unmöglich macht.

Die Position von Lawrencium im Periodensystem ist umstritten, da das Element jüngeren Messungen zufolge eine extrem geringe Ionisierungsenergie besitzt.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.



</doc>
<doc id="2936" url="https://de.wikipedia.org/wiki?curid=2936" title="Leichtmetalle">
Leichtmetalle

Als Leichtmetalle werden allgemein Metalle und Legierungen bezeichnet, deren Dichte unter 5,0 g/cm³ liegt. Alle anderen Metalle sind Schwermetalle, von denen Europium mit einer Dichte von 5,244 g/cm³ das leichteste ist. Im technischen Bereich sind vor allem Aluminium, Magnesium, Titan sowie in geringem Umfang Beryllium und Lithium im Gebrauch – sowie weitere Elemente als Legierungselemente in geringer Konzentration. Die Verarbeitung metallischer Werkstoffe erfolgt bei Leichtmetallen grundsätzlich wie bei anderen Metallen auch.

Eine vollständige Liste der Leichtmetall-Elemente mit ihren Dichten bei 20 °C:

Beim Löschen von Bränden von Leichtmetallen muss beachtet werden, dass dazu kein Wasser verwendet werden darf. Leichtmetalle wie Alkali- und Erdalkalimetalle neigen dazu, mit Wasser sehr heftig unter Bildung von Alkalimetallhydroxiden und Freisetzung von Wasserstoff zu reagieren. Im Falle eines Löschversuchs mit Wasser könnte sich der freiwerdende Wasserstoff entzünden und es käme unter Umständen zu Explosionen (Knallgasbildung mit Luftsauerstoff). Dabei steigt die Reaktivität vom Lithium zum Caesium, bzw. Beryllium zum Barium, stark an. Ab dem Kalium erfolgt Selbstentzündung.

Zudem verbrennen viele Leichtmetalle, z. B. Magnesium und Aluminium, bei sehr hohen Temperaturen.
Wasser zersetzt sich hierbei thermisch zu Wasserstoff und Sauerstoff, was ebenfalls zu einer explosionsartigen Brandausbreitung führt.

Auch wasserhaltige Löschmittel, wie etwa Löschschaum, verbieten sich aus diesen Gründen.
Andere Löschmittel zeigen bei Leichtmetallbränden häufig keine Wirkung, denn Magnesium brennt auch unter Kohlendioxidatmosphäre weiter, indem es dem Kohlendioxid den Sauerstoff entzieht.
Normale Löschpulver eignen sich ebenfalls nicht für Leichtmetallbrände.

Als Löschmittel können dienen:




</doc>
<doc id="2937" url="https://de.wikipedia.org/wiki?curid=2937" title="Luft">
Luft

Als Luft bezeichnet man das Gasgemisch der Erdatmosphäre. Trockene Luft besteht hauptsächlich aus den zwei Gasen Stickstoff (rund 78,08 Vol.-%) und Sauerstoff (rund 20,95 Vol.-%). Daneben gibt es noch die Komponenten Argon (0,93 Vol.-%), Kohlenstoffdioxid (0,04 Vol.-%) und andere Gase in Spuren. Gasförmiges Wasser (Wasserdampf) ist im Mittel zu 1,3 Vol.-% in Bodennähe und zu 0,4 Vol.-% in der gesamten Erdatmosphäre enthalten, bei den obigen Werten aber nicht mitgerechnet.

Zusätzlich enthält Luft auch Staub und andere Teilchen (z. B. Pollen, Pilz- und Farn-Sporen). Im natürlichen Zustand ist sie trotzdem für Menschen geruch- und geschmacklos.
Die in Luft vorkommenden Gasmoleküle (Stickstoff, Sauerstoff usw.) streuen die einfallenden und unterschiedlichen Lichtwellen/-teilchen des Sonnenlichtes verschieden stark (Rayleigh-Streuung). Am stärksten wird das kurzwellige blaue Licht gestreut. Dieser Vorgang gibt der Luft ihre typischerweise natürliche blaue Farbe, ein teil-polarisiertes Muster. Ist der Weg des Lichtes durch die Luft länger, verschiebt sich die Streuung zu einem rötlichen Farbton hin. Dies hängt vom Sonnenlichteinfallswinkel ab. Steht die Sonne im Zenit (direkt über dem Betrachter −90° zum Boden), durchläuft das Licht die Erdatmosphäre auf einer Länge von 90 km. Während eines Sonnenaufgangs /-abgangs steht die Sonne am Horizont (in horizontaler Augenlinie des Betrachters −0° zum Boden) und durchquert die Atmosphäre mit einer ca. 12-fachen Länge (ca. 1075 km). Mit fortschreitender Strecke des Lichtes durch die Atmosphäre wird der blaue vom roten Lichtanteil überlagert, da der Blauanteil immer mehr gestreut wird und der rote Anteil wegen seiner größeren Wellenlänge weniger stark gestreut wird. Dieser atmosphärische Effekt ist nur bei wolkenlosem Horizont morgens kurz vor Sonnenaufgang und abends kurz nach Sonnenuntergang zu beobachten. Er wird als Morgenröte bzw. Abendrot bezeichnet und reicht in den Farbabstufungen von leichtem Rosa bis Lila über Vollrot zu tiefem Orange.

Abgesehen von elastischer Streuung der Photonen ist in der Atmosphäre auch inelastische Streuung zu beobachten: Rotationsramanstreuung führt zu einer Umverteilung in den Energien der eintreffenden Strahlung innerhalb von einigen 10 cm und führt somit zu einem „Auffüllen“ der Fraunhoferlinien, den sogenannten Ring-Effekt. Bei einer spektralen Auflösung von 0,5 nm führt dieser Effekt zu optischen Dicken von typischerweise bis zu 2 %, wenn Himmelsstreulicht verschiedener Sonnenstände verglichen wird. Dieser Effekt muss in verschiedenen DOAS Fernerkundungsmethoden zur Messung von Spurengasen korrigiert werden.

Weiterhin kann Vibrationsramanstreuung an Luftmolekülen die Wellenzahl der eintreffenden Photonen von 1550 cm(O) und 2330 cm(N) verschieben und somit ein wellenlängenverschobenes Abbild des Sonnenlichts über das beobachtete Sonnenlicht legen. Seine Intensität beträgt bis zu 0,04 % der ursprünglichen Intensität.

Die Anteile der Atmosphärengase sind keine Naturkonstanten. In der seit Jahrmilliarden andauernden Entwicklung der Erdatmosphäre veränderte sich die Zusammensetzung ständig und mehrmals grundlegend. Seit 350 Millionen Jahren sind die Hauptbestandteile weitgehend stabil. Die aktuelle Mischung ist für trockene Luft in der Tabelle rechts wiedergegeben, wobei zwischen Hauptbestandteilen und Spurengasen unterschieden wird. Die angegebenen Konzentrationen stellen globale Mittelwerte für die freie Troposphäre dar. Die der chemisch stabilen Komponenten sind abseits von Quellen in der gesamten Homosphäre einheitlich, also bis in eine Höhe von etwa 100 km. Bei reaktiven Spurenstoffen gibt es erhebliche Gradienten.
Tiefkalt verflüssigt kann "Flüssige Luft" durch fraktionierte Destillation in ihre Bestandteile zerlegt werden, dies erfolgt meist mit Hilfe des Linde-Verfahrens.

Der Hauptbestandteil der Luft ist chemisch inert. Er wird durch die natürliche (biotische und abiotische) Stickstofffixierung organisch gebunden und damit für Lebewesen nutzbar. Technisch wird der Luftstickstoff über das Haber-Bosch-Verfahren zur Düngemittelherstellung verwendet. Der entgegengerichtete chemische Prozess – die Denitrifikation verläuft rascher, so dass der Stickstoffkreislauf den Stickstoffanteil in der Atmosphäre kaum verändert.

Aus dem Stickstoff der Luft entstehen durch kosmische Strahlung geringe Mengen radioaktiver Kohlenstoff (C), was mit der Radiokarbonmethode für archäologische Datierungen ausgenutzt wird.

Stickstoff wirkt beim Menschen bei einem signifikant erhöhten Partialdruck (ab ca. 3,2 bar, bei Normalbedingung 0,79 bar) zunehmend narkotisch. Beim Tauchen tritt dieser Stickstoff-Partialdruck in Atemluft ab etwa 30 m Tauchtiefe (in Gewässern auf Meeresniveau) auf und der narkotisch/toxische Effekt ist dort als sogenannter Tiefenrausch bekannt. Deshalb werden spezielle Atemgas-Gemische eingesetzt, bei denen Stickstoff (teilweise) z. B. durch Helium (bei Tauchtiefen über 60 m) oder Sauerstoff (Nitrox, bei geringeren Tauchtiefen) ersetzt wurde.

Der molekulare Sauerstoff der Luft ist hauptsächlich durch Photosynthese aus Wasser gebildet worden, wobei die im Laufe der Erdgeschichte hergestellte Menge etwa das Zwanzigfache der heute in der Atmosphäre vorliegenden Menge beträgt. Er verleiht der Atmosphäre ihren oxidierenden Charakter und stellt das wichtigste Oxidationsmittel dar, das für die biologische Atmung bzw. die chemischen Verbrennungsvorgänge benötigt wird.

Auf den menschlichen Körper wirkt Sauerstoff ab einem Partialdruck von 1,4–1,6 bar giftig, das entspricht bei atmosphärischer Zusammensetzung einem Überdruck von etwa sechs Bar. Dies tritt als praktisches Problem z. B. beim Gerätetauchen mit normaler Luft als Atemgas ab einer Tauchtiefe von ca. 50 m auf.

Bei einer hyperbaren Sauerstofftherapie (HBO) werden die Taucherkrankheit und andere Erkrankungen in einer Druckkammer mit bis zu 1,8 bar Luft behandelt.

Der in der Luft enthaltene Sauerstoff ist für alle aeroben Lebewesen zum Leben notwendig. Durch Atmung führen sie Sauerstoff ihrem Stoffwechsel zur Verbrennung (Katabolismus) zu. Pflanzen nutzen das in der Luft enthaltene Kohlenstoffdioxid zur Photosynthese und spalten dabei den Sauerstoff ab. Für fast alle Pflanzen ist dies die einzige Kohlenstoffquelle für vitale Prozesse und Körpersubstanz (Anabolismus). Bei diesem organischen Prozess wird auch fast der gesamte Luftsauerstoff der Luft regeneriert. Der Sauerstoffkreislauf ermöglicht die Aufrechterhaltung und Verteilung eines dauerhaften Vorrats an Ressourcen für Aerobier und photosynthetisch aktive Pflanzen.

Argon ist als Edelgas äußerst reaktionsträge und mit fast 1 % Gehalt relativ häufig. So ist es kostengünstig und wird als Inertgas etwa beim Metallschweißen und zur Füllung von Glühlampen eingesetzt. Dort und als Füllung von Mehrscheiben-Isolierglas nutzt man die relativ zu Luft etwas geringere Wärmeleitfähigkeit. (Teures, rares Krypton dient in Spezialfällen als noch besseres Wärme-Isoliergas.)

Argon entsteht langsam durch radioaktiven Zerfall von Kalium-40, ist stabil und dichter als Luft und verbleibt daher in der Atmosphäre.

Die Umgebungsluft ist nicht trocken, sondern enthält je nach Luftfeuchtigkeit zusätzlich Wasserdampf. Der Wasserdampfgehalt schwankt zwischen einem zehntel Volumenprozent an den Polen und drei Volumenprozent in den Tropen, mit einem Mittelwert von 1,3 % in Bodennähe. Da Wasserdampf leichter ist als trockene Luft (62,5 % des Trockenluftgewichtes), wird feuchte von der umgebenden Luft nach oben gedrückt, wo dann in kühleren Schichten Kondensation auftritt. Oberhalb der Kondensationsschichten ist der Wasserdampfgehalt sehr gering, sodass über die gesamte Atmosphäre gemittelt nur 0,4 Vol.-% Wasserdampf in der Luft sind.

Größere Schwankungen über teils wenige Jahre und Jahrzehnte sind auch bei den Spurengasen zu verzeichnen. Deren niedrige Konzentrationen können durch vergleichsweise geringe Emissionen beeinflusst werden. Ebenso zeigen Vulkanausbrüche häufig einen kurzfristigen Einfluss.

Nach seinem Anteil ist Kohlenstoffdioxid ein Spurengas, aber als das – unter Berücksichtigung von Wasserdampf – fünfthäufigste Atmosphärengas und aufgrund seiner Bedeutung für Klima und Lebewesen wird es oft zu den Hauptbestandteilen der Luft gerechnet.

Die biologische Hauptbedeutung des Kohlenstoffdioxids (umgangssprachlich oft auch als Kohlendioxid bezeichnet) liegt in seiner Rolle als Kohlenstofflieferant für die Photosynthese. Die atmosphärische Kohlenstoffdioxidkonzentration wirkt stark auf das Pflanzenwachstum. Durch den lichtabhängigen Stoffwechselzyklus der Pflanzen, also die Wechselbeziehung zwischen Atmung und Photosynthese, schwanken die bodennahen CO-Konzentrationen im Tagesgang. Es zeigt sich bei ausreichender Pflanzendecke ein nächtliches Maximum und dementsprechend ein Minimum am Tag. Der gleiche Effekt ist im Jahresverlauf vorhanden, da die außertropische Vegetation ausgeprägte Vegetationsperioden besitzt. Auf der Nordhalbkugel besteht ein Maximum im Zeitraum März bis April und ein Minimum im Oktober oder November. Dazu trägt auch die Heizperiode durch erhöhten Verbrauch fossiler Brennstoffe bei.

Insgesamt hat der Kohlenstoffdioxidgehalt seit Beginn der Industrialisierung um über 40 % zugenommen. Dies ist im Zusammenhang mit dem anthropogenen Treibhauseffekt eine der Ursachen für die globale Erwärmung. 2013 überstieg die CO-Konzentration an der Messstation Mauna Loa erstmals den Wert von 400 ppm.

Während Argon mit rund 1 % zu den Hauptbestandteilen der Luft gehört (siehe oben), zählen die weiteren Edelgase Neon, Helium und Krypton mit Volumenanteilen von jeweils > 1 ppm zu den Spurengasen (vgl. Tabelle). Noch seltener ist Xenon (Volumenanteil < 0,1 ppm). Radon ist das seltenste Edelgas in der Luft (mittlerer Volumenanteil 1:10), kann jedoch – isotopenabhängig – über seine Radioaktivität gut bestimmt werden.

Helium wird bei jedem radioaktiven Alpha-Zerfall frei. Das kleine Atom ist sehr beweglich, sickert aus der Erde, ist viel leichter als Luft und entweicht in den Weltraum. Auch das zweitleichteste Edelgas Neon verflüchtigt sich dorthin, so dass von diesen beiden nur Spuren in der Atmosphäre vorkommen.

Aus manchem Gestein dringt als Glied radioaktiver Zerfallsreihen Radon, das sich in Kellern anreichern kann und strahlend weiterzerfällt.

Für die Stratosphäre werden Ozonwerte oftmals nicht in Anteilen, sondern in der Dobson-Einheit angegeben. Da die Werte zudem von der Höhe (Ozonschicht, bodennahes Ozon) sowie von Wetterlage, Temperatur, Schadstoffbelastung und Uhrzeit abhängen und Ozon sich sowohl schnell bildet als auch wieder zerfällt, ist dieser Wert sehr variabel. Aufgrund der hohen Reaktivität von Ozon spielt es bei chemischen Reaktionen vielfältiger Art in der Atmosphäre eine zentrale Rolle. Ein Beispiel sind die ODEs ("ozone depletion events"), bei denen während des polaren Frühlings regelmäßig starke Einbrüche in der Ozonkonzentration von normalerweise 20–40 ppb auf < 5 ppb beobachtet werden können. Diese Erscheinungen werden beispielsweise durch die Freisetzung von Halogenen durch natürliche Prozesse oder durch Mischung von Luftmassen bewirkt. Typische Ozonkonzentrationen in gemäßigten Breiten und besiedeltem Gebiet sind 30–60 ppb auf der Nordhalbkugel und tendenziell ca. 10 ppb weniger auf der Südhalbkugel aufgrund der Rolle von Ozon in der Stickoxidchemie.

Kohlenstoffmonoxid (umgangssprachlich oft auch als Kohlenmonoxid bezeichnet) ist ein unsichtbares brennbares giftiges Gas, das bei der unvollständigen Verbrennung von kohlenstoffhaltigen Substanzen entsteht. Es blockiert den Sauerstofftransport im Blut (Kohlenstoffmonoxidintoxikation) und kann schon in geringen Dosen zum Tod führen. Auch schädigt es die Photosynthese der Pflanzen. Es bildet sich z. B. beim Tabakrauchen und im Verbrennungsmotor. Autoabgase ohne Abgasnachbehandlung durch einen Fahrzeugkatalysator können bis zu 4 % CO enthalten, der Standardwert für Tabakrauch. Brände der Vegetation sind mit ca. 60 % der Emissionen weltweit Hauptquelle für Kohlenstoffmonoxid.


Die mittlere Molmasse ergibt sich als Summe der Produkte der Molmassen und Stoffmengenanteile der Bestandteile, hauptsächlich Sauerstoff, Stickstoff und Argon. Für trockene Luft ist der exakte Wert 28,949 g/mol. Enthält die Luft noch Feuchtigkeit, ist die mittlere Molmasse geringer, da die Molmasse von Wasserdampf nur ca. 18 g/mol beträgt.

Unter Normbedingungen ist die Luftdichte gleich 1,293 kg/m.

Die Gewichtskraft der Luftsäule erzeugt einen statischen Druck. Dieser Druck hängt gemäß der barometrischen Höhenformel von der Höhe über dem Meeresspiegel ab. Zusätzlich ist der Luftdruck vom Wetter abhängig. Wind und allgemein Änderungen des Wetters bewirken Schwankungen des Luftdrucks. Ein Barometer zur Messung des Luftdrucks gehört daher zur Grundausstattung von Wetterstationen. Über einem Quadratmeter Bodenfläche beträgt die Luftmasse dem Luftdruck entsprechend etwa 10.000 kg.

Als Lufttemperatur wird die Temperatur der bodennahen Luft bezeichnet, die weder von Sonnenstrahlung noch von Bodenwärme oder Wärmeleitung beeinflusst ist. Die genaue Definition in Wissenschaft und Technik ist unterschiedlich. In der Meteorologie wird die Lufttemperatur in einer Höhe von zwei Metern gemessen, wofür häufig weiß gestrichene Wetterhäuschen in freier Umgebung dienen.

Bei der Luftfeuchtigkeit handelt es sich um den Anteil des Wasserdampfes an der Luft. Sie wird über verschiedene Feuchtemaße wie Dampfdruck und Taupunkt sowie relative, absolute und spezifische Luftfeuchte angegeben.

Unter Normalbedingungen ist die Schallgeschwindigkeit in Luft gleich 331,5 m/s.

Der Brechungsindex formula_1 der Luft beträgt unter Normalbedingungen für sichtbares Licht ungefähr 1,00029. Der Wert hängt von Druck, Temperatur und Zusammensetzung der Luft ab, vor allem aber von der Luftfeuchtigkeit. Weil formula_2 ungefähr proportional zum Luftdruck ist, lässt sich der Brechungsindex mit einem Michelson-Interferometer bestimmen, dessen einer Arm durch ein Gebiet mit variablem Luftdruck reicht. Aus der entstehenden optischen Weglängendifferenz bestimmt man mit bekanntem Druckunterschied den Brechungsindex.

Spezifische Wärmekapazität unter Normalbedingungen:

Die Wärmeleitfähigkeit formula_5 von Luft ist unter Normalbedingungen formula_6.

Die Luftverschmutzung ist der auf die Luft bezogene Teilaspekt der Umweltverschmutzung. Gemäß dem Bundes-Immissionsschutzgesetz ist Luftverunreinigung eine Veränderung der natürlichen Zusammensetzung der Luft, insbesondere durch Rauch, Ruß, Staub, Aerosole, Dämpfe oder Geruchsstoffe. Von Bedeutung sind erhöhte Ozonwerte für den Smog und Schwefeldioxidkonzentrationen für den sauren Regen, aber auch Konzentrationen von Stickoxiden und flüchtigen organischen Verbindungen, die ihrerseits wiederum einen großen Einfluss auf die Chemie der Luft haben.

In den meisten Industrieländern ist die lokale Luftverschmutzung aufgrund von gesetzlichen Vorgaben zur Luftreinhaltung in den letzten Jahrzehnten stark zurückgegangen. Gleichzeitig hat der Ausstoß von Treibhausgasen wie Kohlenstoffdioxid weiter zugenommen. Die lokale und regionale Luftverschmutzung ist für Länder der Dritten Welt sowie Schwellenländer wie China noch ein erhebliches Problem.

Die Effekte von Spurengasen sind vielfältig und beeinflussen sich in großem Maße auch gegenseitig. Beispielsweise spielt Ozon durch seine Rolle in der Hydroxylradikalchemie in bodennahen Luftschichten nicht nur die Rolle eines Schadstoffs und Treibhausgases, es ist auch essentiell für die Selbstreinigungsmechanismen der Atmosphäre insgesamt.

Die griechischen Naturphilosophen hielten Luft für eines der vier Grundelemente, aus denen alles Sein besteht. Dem Element Luft wurde der Oktaeder als einer der fünf platonischen Körper zugeordnet.





</doc>
<doc id="2938" url="https://de.wikipedia.org/wiki?curid=2938" title="Ludologie">
Ludologie

Ludologie ( und ) ist die „Lehre“ vom Spielen: Gesellschaftsspiel, analoges Spiel und digitales Spiel.

Die Ludologie bearbeitet einen Teilbereich der Spielwissenschaft. Im Unterschied zu der älteren, breiter angelegten Spielwissenschaft fokussiert sich das Interesse der aus dem angelsächsischen Bereich transferierten Disziplin Ludologie vornehmlich auf das digitale Spiel. Sie bezeichnet den noch jungen, transdisziplinären Forschungszweig, der sich an der Schnittstelle von Kultur- und Strukturwissenschaften mit den ästhetischen, kulturellen, kommunikativen, technischen und strukturellen Aspekten des Phänomens "„Spiel“" auseinandersetzt. Den Schwerpunkt der Betrachtung bilden dabei Geschichte, Entwicklung, Analyse und Theorie digitaler Spiele.

Der Begriff ist vor allem im angelsächsischen Sprachraum gebräuchlich und wird meist synonym zum Terminus (Video-)Spieltheorie verwendet.

Im engeren Kontext der "„Ludologie vs. Narratologie“"-Debatte dagegen bezeichnet Ludologie das Paradigma, in dem das Prinzip der Simulation als das Kernkonzept des Spiels aufgefasst wird. Aus diesem Grund findet alternativ der neutrale Begriff Spieleforschung (engl. Game Studies) Verwendung.

Der engere Spielbegriff der „Ludologie“ zeigt sich auch in der Reduzierung auf das historische Wort „Ludus“, wohingegen die „Spielwissenschaft“ den gesamten Phänomenkomplex umfasst und erforscht, der sich nach der von ihr erarbeiteten Systematik über die Kategorien ludus (Kinderspiel, Brettspiel), Agon (= Kampfspiel, Sportspiel), Alea (Glücksspiel, Hasardspiel), Mimikri (Maskenspiel, Marionettenspiel), Circenses (Zirkusspiele, Schauspiel) und Ilinx-( = Ritual-)Spiele erstreckt.

Nachdem bereits die Philanthropen mit GutsMuths oder Basedow im 18. Jahrhundert systematische spielwissenschaftliche Forschung betrieben und Forscher unterschiedlicher Disziplinen wie Herbert Spencer (1865), Moritz Lazarus (1883), Karl Groos (1899) oder Jean Piaget (1975) sich im 19. und 20. Jahrhundert intensiv mit dem Phänomen Spiel auseinandergesetzt hatten, erreichte die Spielwissenschaft mit den Analysen und Erkenntnissen von Friedrich Schiller, Johan Huizinga oder Frederik Jacobus Johannes Buytendĳk erste Höhepunkte der wissenschaftlichen Spieltheorie. Daran konnte die mit dem Aufkommen der Videospiele und des Computerzeitalters entstandene Ludologie anknüpfen und ihren Schwerpunkt im Bereich des digitalen Spiels finden.

Erst gegen Ende der 1990er Jahre begannen sich Veröffentlichungen zur Thematik zu häufen, und es formierte sich die Basis einer neuen wissenschaftlichen Disziplin. Dies war maßgeblich der zunehmenden Etablierung digitaler Spiele, ihren Auswirkungen auf die Gegenwartskultur sowie ihrer wachsenden wirtschaftlichen Bedeutung zu verdanken. Die Bezeichnung „Ludologie“ wurde 1999 durch einen Artikel von Gonzalo Frasca einem breiteren Fachpublikum bekannt.

Im Wintersemester 2015/2016 führte die Universität Bayreuth erstmals in Deutschland den Masterstudiengang "Computerspielwissenschaften" ein. Lern- und Forschungsschwerpunkte liegen hierbei in den Bereichen Game Studies, Medienwissenschaft, Kulturwissenschaft und Informatik. Im Bachelorstudiengang der "Medienwissenschaft und Medienpraxis" lässt sich der Fokus auf Computerspiele legen.

Die Ludologie als eigenständiger Forschungszweig befindet sich noch im Frühstadium. Sie befasst sich in weiten Teilen zunächst mit Fragen der Kanonbildung und der Einigung auf grundlegende Begrifflichkeiten und Kategorien. Schwierig gestaltet sich dies nicht nur durch die teils hohe Interdisziplinarität, sondern auch aufgrund der vielfältigen Ausprägungen bereits fest im Alltagsgebrauch verschiedener Sprachen verankerter Termini, deren Bedeutung im wissenschaftlichen Kontext neu festgelegt werden muss. Als prägnantes Beispiel dafür mag der Begriff „Spiel“ selbst dienen, über dessen klare Abgrenzung man sich alles andere als einig ist. Wesentliche Beiträge zur Klärung der Definitionsfrage haben 2003 Salen/Zimmerman und Jesper Juul (nicht zu verwechseln mit seinem Namensvetter) geleistet.

Der wissenschaftliche Diskurs wird derzeit überwiegend von Akademikern aus angelsächsischen und den nordischen Ländern dominiert. Insbesondere sind deren Bemühungen, für sowohl Forschung als auch Lehre und die Industrie geeignete Institutionen zu schaffen und zu koordinieren, bereits weit fortgeschritten. Neben der hohen Anzahl spielespezifischer Studiengänge, vornehmlich in Großbritannien und den USA, ist dabei vor allem die Einrichtung des "Center for Computer Games Research" der IT-Universität Kopenhagen (ITU) zu erwähnen, das als Denkfabrik der europäischen Ludologie fungiert.

Im deutschsprachigen Raum dagegen ist die Selbstorganisation der Spieleforscher bisher noch eher unverbindlich und multidisziplinär strukturiert. Auch gibt es weniger Industriekontakte, und die Wirkungsforschung besitzt aus politischen Gründen einen höheren Stellenwert als im Ausland. Die Formierung der "AG Computerspiele" der "Gesellschaft für Medienwissenschaft e.V." ist allerdings ein Anzeichen dafür, dass sich daran in absehbarer Zeit etwas ändern könnte. Erste deutsche Professuren mit entsprechendem Schwerpunkt wurden im Dezember 2002 am "Institut für Simulation und Grafik" der Universität Magdeburg und im März 2006 auch am "Institut für Medien- und Kommunikationswissenschaft" der Technischen Universität Ilmenau besetzt.

Primärer Fokus der Ludologie sind die digitalen Spiele, wofür es neben deren Status als massenkulturellem Phänomen auch prinzipielle Gründe gibt: Der Computer gilt als Universalmedium. Das hat zur Folge, dass nicht nur beliebige traditionelle Spiele auf ihm umgesetzt, simuliert oder computergestützt gespielt werden können, sondern auch eine ganze Reihe von neuartigen Spielen, teils in Kombination mit Inhalten und Techniken anderer Medienformen (z. B. Literatur und Film), nur so möglich wird. Diese Konvergenzentwicklung macht digitale Spiele zum idealen Anschauungsobjekt, um allgemeine Erkenntnisse über das Spielen zu gewinnen. Relevante Fragestellungen befassen sich dabei unter anderem mit folgenden Bereichen:


Im Zuge ihrer Geburtswehen leistete sich die Spielforschung eine hitzig geführte Grundsatzdebatte, deren Auswirkungen in Form von ideologischen Grabenkämpfen teilweise immer noch spürbar sind.

Ausgangspunkt war der Ansatz einiger Literatur- und Medienwissenschaftler, wie Janet Murray und Celia Pearce, ihr traditionelles Instrumentarium zur Analyse von Texten auf diejenige digitaler Spiele zu übertragen. Zu diesem Zweck werden Spiele als eine weitere Form von Text deklariert, die dann den bekannten Gesetzmäßigkeiten folgt. „Text“ wird hierbei als kommunikatives Generalkonzept verstanden, als universelles Mittel zur „Konstruktion von Sinn“ und umfasst somit auch Theater, Film und beliebige andere Erzählformen; selbst ein Spiel wie Schach wird in diesem Paradigma als Erzählung angesehen.

Dies wurde von der wachsenden Gruppe von Ludologen um Espen Aarseth kategorisch als unzutreffend abgelehnt; sie setzten stattdessen auf das Prinzip der Simulation, des Agierens „als ob“, als Kern des Spiels an sich:
Jedes Spiel zeichnet sich danach durch folgende Elemente aus: "Regeln" (das explizite Regelwerk, aber auch implizite Regeln der Spielmechanik), die "Spielwelt" (ein materielles/semiotisches System) und das "Gameplay" (die Ereignisse, welche sich aus der Anwendung der Regeln auf die Spielwelt und die Aktionen der Spieler ergeben).

Die Heftigkeit der ursprünglichen Debatte erklärt sich in erster Linie aus ihrer Wahrnehmung als Schlüsselkonflikt um die Deutungshoheit des Phänomens "Spiel". Die Position der Narratologen wurde von den Ludologen als Okkupationsversuch durch das Überstülpen unzulässig verallgemeinerter Konzepte eines fremden Fachbereichs angesehen, was in der Folge zu einer bewussten Radikalisierung und Polemisierung auch der Argumentation einiger Simulationisten führte. Exemplarisch hierfür ist ein in diesem Kontext zu relativer Berühmtheit gelangtes Zitat von Markku Eskelinen:
Vorherrschend in der internationalen Spieleforschung ist mittlerweile eine gemäßigte ludologische Sichtweise. Diese erkennt neben der Simulation als Grundprinzip des Spiels die Nützlichkeit einer traditionell textuellen Analyse von Spielinhalten zwar durchaus an, allerdings nur dann, wenn auch originär narrative Elemente vorhanden sind, was für viele Spiele, wie z. B. Schach oder Tetris, verneint wird.






</doc>
<doc id="2939" url="https://de.wikipedia.org/wiki?curid=2939" title="Landkreis Böblingen">
Landkreis Böblingen

Der Landkreis Böblingen ist ein Landkreis in Baden-Württemberg. Er gehört zur Region Stuttgart im Regierungsbezirk Stuttgart.

Im Westen des Landkreises liegen das Obere Gäu (hier auch "Korngäu" genannt) und ein Teil des Heckengäus, der bis zu den Ausläufern des Schwarzwalds reicht. Im Süden gehören große Teile des Schönbuchs zum Kreisgebiet, des ersten Naturparks in Baden-Württemberg. Auch im Norden gibt es neben den offenen Landschaften der Gäue Waldgebiete, namentlich als Glemswald zusammengefasst. 

Größere Flüsse oder Seen sind im Kreisgebiet nicht vorhanden. Im südlichen Kreisgebiet entspringen die Aich und die Würm. Die Bäche und kleineren Flussläufe münden alle in den Neckar, direkt oder über die Enz.
Der geographisch höchste Punkt befindet sich auf dem Kühlenberg nahe Oberjettingen auf , der tiefste liegt bei in der Glemsniederung an der nördlichen Kreisgrenze.

Die Liste der Orte im Landkreis Böblingen enthält die ungefähr 150 Orte (Städte, Dörfer, Weiler, Höfe und Wohnplätze) des Landkreises Böblingen im geographischen Sinne.

Der Landkreis Böblingen grenzt im Uhrzeigersinn im Nordosten beginnend an den Landkreis Ludwigsburg, an den Stadtkreis Stuttgart sowie an die Landkreise Esslingen, Reutlingen, Tübingen, Calw und Enzkreis.

Nach Daten des Statistischen Landesamtes, Stand 2014.

Der Landkreis Böblingen besitzt die nachfolgenden Naturschutzgebiete. Nach der Schutzgebietsstatistik der Landesanstalt für Umwelt, Messungen und Naturschutz Baden-Württemberg (LUBW) stehen 732,59 Hektar der Kreisfläche unter Naturschutz, das sind 1,19 Prozent.


Der Landkreis Böblingen geht auf das alte gleichnamige württembergische Oberamt zurück, das schon zu Zeiten des Herzogtums Württemberg errichtet wurde. Im Laufe der Geschichte wurde es mehrmals verändert und 1938 in den Landkreis Böblingen überführt. Damals wurden nahezu alle Gemeinden des aufgelösten Oberamts Herrenberg sowie einige Gemeinden des Amtsoberamtes Stuttgart dem Landkreis Böblingen angegliedert.

Am 1. September 1971 wurde der Landkreis um die Gemeinde Dachtel des Landkreises Calw vergrößert.

Am 1. Januar 1973 kamen bei der Kreisreform 15 Gemeinden des Landkreises Leonberg dazu – darunter die damaligen bzw. späteren Städte Leonberg, Weil der Stadt, Renningen und Rutesheim – (die anderen kamen entweder zum Landkreis Ludwigsburg oder zum Enzkreis im Regierungsbezirk Karlsruhe) sowie die Gemeinde Deckenpfronn des Landkreises Calw. 

Am 1. Januar 1975 wurden die Stadt Leinfelden und die Gemeinde Musberg an den Landkreis Esslingen abgegeben. Damit erreichte der Landkreis seinen heutigen Umfang. 

Seit Abschluss der Gemeindereform am 1. Januar 1975 umfasst der Landkreis Böblingen 26 Gemeinden, darunter neun Städte und hiervon wiederum vier „Große Kreisstädte“ (Böblingen, Herrenberg, Leonberg und Sindelfingen). Größte Stadt des Kreises ist Sindelfingen, kleinste Gemeinde ist Deckenpfronn.

Die Einwohnerzahlen sind Volkszählungsergebnisse (¹) oder amtliche Fortschreibungen des Statistischen Landesamts Baden-Württemberg (nur Hauptwohnsitze).

Im Jahr 2010 war der Landkreis Böblingen – bedingt durch das relativ niedrige Durchschnittsalter der Bevölkerung, die höhere Lebenserwartung und die überdurchschnittliche Geburtenhäufigkeit – einer der wenigen Kreise mit einer positiven Geburtenbilanz in Baden-Württemberg.

Der Landkreis wird vom Kreistag und vom Landrat verwaltet.

Der Kreistag wird von den Wahlberechtigten im Landkreis auf fünf Jahre gewählt. Die Kommunalwahl am 25. Mai 2014 führte zu folgendem vorläufigen Ergebnis. Das amtliche Endergebnis wird vom Statistischen Landesamt gegen Ende des Jahres bekannt gegeben.
Der Kreistag wählt den Landrat für eine Amtszeit von 8 Jahren. Dieser ist gesetzlicher Vertreter und Repräsentant des Landkreises sowie Vorsitzender des Kreistags und seiner Ausschüsse. Er leitet das Landratsamt und ist Beamter des Kreises.
Zu seinem Aufgabengebiet zählen die Vorbereitung der Kreistagssitzungen sowie seiner Ausschüsse. Er beruft Sitzungen ein, leitet diese und vollzieht die dort gefassten Beschlüsse. In den Gremien hat er kein Stimmrecht. Sein Stellvertreter ist der Erste Landesbeamte, der nicht Kreis-, sondern Landesbeamter ist.



Die Oberamtmänner des ehemaligen Oberamts Böblingen sind im Artikel Oberamt Böblingen dargestellt.

Bei Landtagswahlen ist das Kreisgebiet in die Wahlkreise 5 (Böblingen) und 6 (Leonberg) aufgeteilt, wobei ersterer den Osten des Landkreises samt Böblingen und Sindelfingen umfasst, letzterer den Norden, Süden und Westen mit Leonberg und Herrenberg.

Der Wahlkreis 5 (Böblingen) wird im aktuellen Landtag durch Thekla Walker (Grüne), Paul Nemeth (CDU) und Harald Pfeiffer (AfD) vertreten, der Wahlkreis 6 (Leonberg) durch Bernd Murschel (Grüne) und Sabine Kurtz (CDU).

Der Bundestagswahlkreis Böblingen umfasst den gesamten Landkreis Böblingen ohne Waldenbuch und Steinenbronn. Im aktuellen Bundestag der 18. Wahlperiode vertritt Clemens Binninger (CDU) den Wahlkreis Böblingen.

Das Wappen des Landkreises Böblingen zeigt in Gold unter einer liegenden schwarzen Hirschstange eine dreilatzige rote Fahne an drei schwarzen Trageringen. Das Wappen wurde 18. April 1947 angenommen und dem neuen vergrößerten Landkreis Böblingen am 30. August 1974 vom Innenministerium Baden-Württemberg neu verliehen.

Die Fahne ist die Wappenfigur der Pfalzgrafen von Tübingen, die u. a. die Städte Böblingen, Sindelfingen und Herrenberg gründeten, ehe diese an das Herzogtum Württemberg kamen. Die württembergischen Hirschstangen symbolisieren die sehr frühe Zugehörigkeit des Kreisgebiets zu Württemberg.

Von 1927 bis 1947 prägte das Sportflugzeug Kl 25 das Wappen des Oberamts und Landkreises Böblingen.

Der Kreis unterhält Partnerschaften zum Kreis Timiș in Rumänien und zum Bezirk Kaunas in Litauen.

Die erste Strecke, die die Württembergische Staatsbahn in diesem Gebiet baute, war 1868/69 die Schwarzwaldbahn von Stuttgart über Leonberg nach Weil der Stadt, die 1872 bis Calw verlängert wurde. Erst 1879 folgte die Gäubahn von Stuttgart damals bis Eutingen im Gäu. Die Rankbachbahn als Querverbindung zwischen diesen beiden Strecken von Böblingen über Sindelfingen nach Renningen kam 1914/15 hinzu. Eine Querverbindung von der Gäubahn ins Neckartal stellte 1909 die Ammertalbahn Herrenberg–Tübingen her.

Die Gegend südlich der Kreisstadt wurde 1910/11 durch die Schönbuchbahn Böblingen–Dettenhausen erschlossen; sie wird ab 1996 im Auftrag der Kommunen von der Württembergischen Eisenbahn-Gesellschaft (WEG) betrieben. Eine Zweigstrecke von der Station Schönaicher First nach Schönaich wurde 1922 durch die Deutsche Reichsbahn eröffnet, die ab 1928 mit der Siebenmühlentalbahn von Leinfelden nach Waldenbuch auch das Siebenmühlental erschloss.

Weissach, die nördlichste Gemeinde des Kreises, ist seit 1906 Endpunkt der von den Württembergischen Nebenbahnen AG erbauten Strohgäubahn.

Von dem 100 Kilometer umfassenden Schienennetz wurden inzwischen 18 Kilometer stillgelegt:
Allerdings lag der Personenverkehr auf weiteren 20 Kilometern rund 30 Jahre lang ebenfalls still. 
Verantwortlich für den öffentlichen Personennahverkehr ist der Verkehrs- und Tarifverbund Stuttgart.

Durch das Kreisgebiet führt die Bundesautobahn 8 Stuttgart–Karlsruhe sowie die A 81 Stuttgart–Singen (Hohentwiel). Ferner wird es durch Bundes-, Landes- und Kreisstraßen erschlossen. Die wichtigsten sind die B 14 Rottweil–Stuttgart und die B 295 Calw–Stuttgart.

Der Landkreis Böblingen ist Schulträger der folgenden Beruflichen Schulen: Kaufmännische Schule Böblingen und Mildred-Scheel-Schule Böblingen (letztere unter anderem mit Biotechnologischem Gymnasium und Ernährungswissenschaftlichem Gymnasium), Hauswirtschaftliche und Landwirtschaftliche Schule Herrenberg, Gewerbliche, Kaufmännische und Hauswirtschaftliche Schule Leonberg und Gottlieb-Daimler-Schule I und Gottlieb-Daimler-Schule II (mit der Akademie für Datenverarbeitung) im Technischen Schulzentrum Sindelfingen, ferner der Schulen für Geistigbehinderte mit Schulkindergarten in Böblingen (Käthe-Kollwitz-Schule), Herrenberg (Friedrich-Fröbel-Schule), Leonberg (Karl-Georg-Haldenwang-Schule) und Sindelfingen (Bodelschwinghschule), der Schule für Körperbehinderte mit Schulkindergarten in Sindelfingen und der Schule für Sprachbehinderte und Kranke in längerer Krankenhausbehandlung mit Schulkindergarten für Sprachbehinderte in Sindelfingen. Zusammen mit den Städten Böblingen und Sindelfingen ist der Landkreis Böblingen Träger der Volkshochschule.

Die beiden Kreiskrankenhäuser in Herrenberg und Leonberg wurden zunächst seit 2005 in Form einer Eigengesellschaft des Landkreises Böblingen von der Kreiskliniken Böblingen gemeinnützige GmbH betrieben. Das Krankenhaus in Böblingen und das frühere städtische Krankenhaus in Sindelfingen wurden in der Klinikum Sindelfingen/Böblingen gGmbH betrieben. Beide Gesellschaften gehörten dem Klinikverbund Südwest an, an dem die Landkreise Calw und Böblingen beteiligt sind. Nach dem Ausstieg der Stadt Sindelfingen aus dem Klinikverbund Südwest werden alle vier Krankenhäuser im Landkreis Böblingen (Böblingen, Herrenberg, Leonberg und Sindelfingen) in der Kreiskliniken Böblingen gGmbH betrieben, an der der Kreis die Mehrheit hält. Der Klinikverbund Südwest ist ein Zusammenschluss der Krankenhäuser Böblingen, Calw, Herrenberg, Leonberg, Nagold und Sindelfingen. Gemeinsam mit dem Therapiezentrum im Klinikverbund Südwest, dem Medizinischen Gesundheitszentrum und der Service GmbH Schwarzwald zählt er zu den größten kommunalen Gesundheitseinrichtungen in Süddeutschland.
2024 sollen die bestehenden Kliniken Böblingen und Sindelfingen aufgegeben werden und durch einen Klinikneubau auf dem Flugfeld in Böblingen ersetzt werden. Das Projekt wird durch Fördermittel, Kredite und Eigenmittel über den Landkreis Böblingen finanziert (Gesamtkosten von 423 bis 452 Millionen Euro).

Der Abfallwirtschaftsbetrieb des Landkreises Böblingen wird in Form eines Eigenbetriebs geführt und ist mit seinen rund 240 Mitarbeitern ein mittelständischer Komplettanbieter für die Abfallentsorgung und Verwertung. Er bedient im Kreis rund 170.000 private Haushalte sowie eine große Zahl von Gewerbebetrieben bei der Entsorgung und Weiterverwertung von Abfällen und betreibt die Wertstoffhöfe in den einzelnen Kreisgemeinden.
Als einer von drei Landkreisen in Baden-Württemberg setzt der Landkreis Böblingen zur Sammlung von Wertstoffen das Wertstoffhofkonzept („Bringsystem“) anstelle des Gelben Sacks um. Der Landkreis ist mit 51,07 % am "Zweckverband RBB" beteiligt, der das Restmüllheizkraftwerk in Böblingen betreibt.

Der Landkreis weist das höchste Verdienstniveau in Baden-Württemberg auf. Das durchschnittliche Arbeitnehmerentgelt (brutto einschl. Arbeitgeber-Sozialbeiträge) betrug 2009 42.645 Euro.

Der Kreis ist Träger der Kreissparkasse Böblingen.

Vereinbarte Verwaltungsgemeinschaften und Gemeindeverwaltungsverbände:

In der Tabelle stehen die Gemeinden des alten Landkreises Böblingen vor der Gemeindereform. Alle Gemeinden, mit Ausnahme von Leinfelden und Musberg, welche heute Teil des Landkreises Esslingen sind, gehören auch heute noch zum Landkreis Böblingen. 

Am 1. Juli 1956 wurde dem Landkreis bei der Einführung der bis heute gültigen Kfz-Kennzeichen das Unterscheidungszeichen "BB" zugewiesen. Es wird durchgängig bis heute ausgegeben. Seit dem 25. April 2013 ist auch das Unterscheidungszeichen "LEO" (Leonberg) erhältlich.




</doc>
<doc id="2941" url="https://de.wikipedia.org/wiki?curid=2941" title="Lichtjahr">
Lichtjahr

Das Lichtjahr ist ein Längenmaß außerhalb des Internationalen Einheitensystems.

Das Lichtjahr ist eine astronomische Längeneinheit. Sie wird oft zur Angabe kosmischer Entfernungen in der astronomischen Öffentlichkeitsarbeit benutzt. Im wissenschaftlichen Umfeld wird jedoch in Parsec gerechnet.

Wie bei jeder Längeneinheit lässt sich auch vom Lichtjahr durch Potenzierung ein Raummaß zur Angabe von Volumina ableiten. In diesem Fall ergibt sich dadurch die Einheit "Kubiklichtjahr", die dem Volumen eines Würfels mit einer Kantenlänge von einem Lichtjahr entspricht.

Ein Lichtjahr ist die Strecke, die eine elektromagnetische Welle wie das Licht in einem julianischen Jahr im Vakuum zurücklegt. Das sind 9,461 Billionen Kilometer (9,461 · 10 km).

Es gibt mehrere Definitionen für ein Jahr. So gibt es das tropische Jahr, das gregorianische Jahr, das julianische Jahr und das siderische Jahr. Diese weichen bis zu 0,005 Prozent voneinander ab, was bei genaueren Angaben zu Divergenzen führt. Daher hat die Internationale Astronomische Union (IAU) empfohlen, ein „Jahr“ ohne genauere Angaben als julianisches Jahr (= exakt 365,25 Tage) auszulegen. Damit und mit der Lichtgeschwindigkeit im Vakuum ist das Lichtjahr exakt definiert. Da der Meter im SI über die Lichtgeschwindigkeit definiert ist (299.792.458-ster Teil einer Lichtsekunde), entspricht ein Lichtjahr einer exakten ganzen Zahl von Metern.

Analog zum Lichtjahr sind die Einheiten Lichtsekunde, Lichtminute, Lichtstunde und Lichttag definiert. Diese sind über die Lichtgeschwindigkeit (beziehungsweise die Definition des Meters im Internationalen Einheitensystem) exakt festgelegt.

Damit entspricht

Ein Lichtjahr entspricht des Weiteren etwa:

Die Einheit Lichtjahr ist mit verschiedenen dezimalen Vielfachen (SI-Präfixen) in Verwendung, beispielsweise kLj oder MLj.



</doc>
<doc id="2942" url="https://de.wikipedia.org/wiki?curid=2942" title="Link (Einheit)">
Link (Einheit)

link ist eine nicht SI-konforme Einheit der Länge aus dem angloamerikanischen Maßsystem.
Der englische Name „link“ steht für „Kettenglied“.

1 "link" = 0,22 yard = 0,66 feet = 7,92 inch = 0,201168 m = 20,1168 cm

1 statute mile = 8 furlong = 80 chain = 320 rod = 8000 "link"



</doc>
<doc id="2943" url="https://de.wikipedia.org/wiki?curid=2943" title="Liter">
Liter

Der (außerhalb der Schweiz auch: das) Liter ist eine Einheit für das Volumen und wird durch das Einheitenzeichen formula_1, formula_2 oder formula_3 symbolisiert oder mit „Ltr.“ abgekürzt.

Ein Liter entspricht einem Kubikdezimeter (dm³). Ein Würfel mit einer Kantenlänge von 10 cm hat demnach ein Volumen von einem Liter.

Der Liter gehört zwar nicht zum Internationalen Einheitensystem (SI), ist zum Gebrauch mit dem SI aber zugelassen. Durch nationale Gesetze ist der Liter eine gesetzliche Maßeinheit.

Der deutsche Einheitenname „Liter“ [, auch: ] stammt von französisch "litre" [aus mittelfranzösisch "litron" (ein Hohlmaß), mittellateinisch "litra", griechisch "lítra" = Pfund].

Grundsätzlich werden im Internationalen Einheitensystem (SI) die Einheitenzeichen kleingeschrieben, es sei denn, dass der Name der Einheit von dem Eigennamen einer Person abgeleitet wurde. In diesem Fall wird der erste Buchstabe des Zeichens großgeschrieben. Dementsprechend ist das Einheitenzeichen für den Liter ein kleines „l“, da der Einheitenname „Liter“ nicht von einem Eigennamen abgeleitet wurde.

Das große „L“ wurde von der CGPM ausnahmsweise als alternatives Einheitenzeichen für den Liter zugelassen, um Verwechslungen des Buchstabens „l“ mit der Ziffer „1“, die bei manchen Schriftarten auftreten können, zu vermeiden, und ist eher im englischen und französischen Sprachraum gebräuchlich. ISO und IEC verwenden jedoch ausschließlich das ursprüngliche Einheitenzeichen „l“. IUPAC und DIN 1301-1 lassen beide Schreibweisen zu.

Gelegentlich wird zur Unterscheidung des Einheitensymbols von der Ziffer „1“ oder dem Großbuchstaben „I“ auch ein kleines „l“ in Schreibschrift (ℓ) verwendet, der zugehörige Unicode-Codepunkt für das „ℓ“ ist U+2113.

Der Einheitenname „Liter“ war früher nach DIN 1301-1 sächlich. Dies deckte sich aber nicht mehr mit dem gemeinsprachlichen Gebrauch. Der Duden etwa erwähnt die sächliche Form nur an zweiter Stelle. Mit der DIN 1301-1:2010-10 wurde diese Festlegung gegenüber der DIN 1301-1:2002-10 geändert; der Einheitenname „Liter“ ist nun als männlich festgelegt.

Gebräuchliche dezimale Teile und Vielfache des Liters sind:
Noch größere Mengen von Flüssigkeiten (insbesondere von Wasser) werden in Kubikmetern angegeben. 1 m³ entspricht 1000 Litern. Der Rhein in Basel hat eine durchschnittliche Abflussmenge von 1040 m³/s.

1793 wurde der Liter in Frankreich als neue „Republikanische Maßeinheit“ (im Zusammenhang mit dem Metrischen System) eingeführt und einem Kubikdezimeter gleichgesetzt.

1879 übernahm das CIPM die französische Liter-Definition und schrieb die Verwendung des l (Kleinbuchstabe l) als Symbol vor.

1901 wurde der Liter auf der dritten CGPM umdefiniert zu jenem Volumen, das 1 kg reines Wasser bei der Temperatur seiner höchsten Dichte unter Normaldruck (1013,25 hPa) hat. Der Liter war damit etwa 1,000028 dm³ groß (ursprünglich waren 1,000027 dm³ für die Umrechnung angegeben). Aufgrund der neuen Aufgabe für diese Maßeinheit, Masse und Volumen über eine spezielle Messgröße des Wassers zu koppeln, hatte reines Wasser unter diesen Bedingungen zwar immer noch eine Dichte von 999,975 kg/m³, aber nunmehr genau 1,0000 kg/l.

Später legte man die Definition auf die Wassertemperatur von 4 °C fest, die nicht die Temperatur der höchsten Wasserdichte ist (tatsächlich etwa 3,98 °C). Somit hatte reines Wasser wieder eine geringfügig kleinere Dichte als 1,0000 kg/l.

1964 wurde auf der 12. CGPM die Definition von 1793 wiederhergestellt, seither ist ein Liter wieder exakt ein Kubikdezimeter und Wasser hat wie vor 1901 eine Dichte von 0,999975 kg/l.

1979 wurde auf der 16. CGPM für den Liter das alternative Symbol L (Großbuchstabe L) zugelassen, gleichzeitig wurde der Wunsch geäußert, in Zukunft nur eines der beiden Symbole (Kleinbuchstabe oder Großbuchstabe) zu behalten. Zuletzt konnte 1990 nur festgestellt werden, dass es für eine Entscheidung noch zu früh sei.


</doc>
<doc id="2945" url="https://de.wikipedia.org/wiki?curid=2945" title="Lumensekunde">
Lumensekunde

Lumensekunde (Einheitenzeichen lms) ist die SI-Einheit der Lichtmenge. Sie berechnet sich als Integral eines Lichtstroms, der in Lumen angegeben ist, über die Zeit in Sekunden:

Die Einheit wird auch Talbot (nach William Henry Fox Talbot) oder Lumberg genannt, jedoch sind diese speziellen Namen weder von den Organen der Internationalen Meterkonvention für den Gebrauch zusammen mit SI-Einheiten angenommen, noch in Deutschland gesetzliche Einheiten im Messwesen.

Gleichzeitig mit der Einheit Lumberg wurde auch das Lumerg geschaffen, das entsprechend dem Verhältnis zwischen Erg und Joule definiert wurde, als 1 lumerg = 10 lms.

In der Praxis wird oft die Lumenstunde (lmh) als Maß verwendet, welche die Menge von einem Lumen über eine Stunde ist.


</doc>
<doc id="2946" url="https://de.wikipedia.org/wiki?curid=2946" title="Lux (Einheit)">
Lux (Einheit)

</math>
Das Lux ist die SI-Einheit der Beleuchtungsstärke.

Die Beleuchtungsstärke auf einer beleuchteten Fläche gibt an, welcher Lichtstrom (gemessen in Lumen, lm) auf eine Flächeneinheit (gemessen in Quadratmetern, m) fällt. Ihre SI-Einheit ist daher Lumen durch Quadratmeter (lm/m). Diese abgeleitete Einheit trägt auch den Namen Lux, ihr Einheitenzeichen ist lx. Der Name leitet sich von der lateinischen Bezeichnung für "Licht" ab.

Die „senderseitige“ Entsprechung zur Beleuchtungsstärke, die spezifische Lichtausstrahlung, hat auch die SI-Einheit lm/m, für sie ist in der offiziellen Dokumentation der Name Lux jedoch nicht vorgesehen.

Die Beleuchtungsstärke formula_2 auf einer beleuchteten Fläche ist die Flächendichte des einfallenden Lichtstroms formula_3, gibt also an, welcher Lichtstrom auf eine gegebene Fläche formula_4 fällt:

Falls die Beleuchtungsstärke über eine endlich große Fläche formula_4 hinweg konstant ist, erübrigt sich die Verwendung differentieller Größen und die vereinfachte Definition lautet: Die auf der Fläche formula_4 konstante Beleuchtungsstärke ist der Quotient aus dem auf die Fläche formula_4 auftreffenden Lichtstrom formula_3 und der beleuchteten Fläche formula_4:

Die Beleuchtungsstärke ist die photometrische Entsprechung zur radiometrischen Größe Bestrahlungsstärke "E" (gemessen in Watt durch Quadratmeter, W/m). Fällt elektromagnetische Strahlung auf die Fläche und erzeugt dort die Bestrahlungsstärke "E", so lässt sich messtechnisch oder rechnerisch die von dieser Strahlung verursachte Beleuchtungsstärke in Lux ermitteln, indem die einzelnen Wellenlängen der Strahlung mit der jeweiligen Empfindlichkeit des Auges bei der betreffenden Wellenlänge gewichtet werden.

Zur Berechnung der Beleuchtungsstärke aus gegebenen anderen photometrischen Größen (z. B. Lichtstärke) siehe die folgenden oder die im Artikel Beleuchtungsstärke gegebenen Beispiele.

Die Beleuchtungsstärke wird mit einem Luxmeter gemessen. An der Physikalisch-Technischen Bundesanstalt (PTB) können Beleuchtungsstärken zwischen 0,001 lx und 100.000 lx realisiert werden. Dies dient u. a. der Kalibrierung von Beleuchtungsstärkemessgeräten.

Für eine kleine ebene Empfangsfläche und eine im Vergleich zur Entfernung formula_12 hinreichend kleine Lichtquelle, welche Licht der Lichtstärke formula_13 in Richtung der Empfangsfläche aussendet, gilt näherungsweise das photometrische Entfernungsgesetz:
Der Neigungswinkel formula_15 der Empfangsfläche ist der Winkel zwischen der Flächennormalen und der Strahlrichtung.

Die Lichtstärke einer Kerze beträgt etwa ein Candela (cd). Sie erzeugt im Abstand von 2 m auf einer senkrecht zur Strahlrichtung stehenden Empfangsfläche die Beleuchtungsstärke

"Ergebnis:" Von einer Kerze im Abstand von ca. 2 m senkrecht beleuchtete Gegenstände erscheinen ungefähr so hell beleuchtet wie im senkrecht auftreffenden Licht des Vollmonds (siehe auch Abschnitt →Beispiele typischer Beleuchtungsstärken).

Die Bestrahlungsstärke formula_17, die von einer isotrop strahlenden Lichtquelle auf einer in 3 m Abstand senkrecht zur Strahlrichtung stehenden Empfangsfläche erzeugt wird, betrage

Integriert über eine die Lichtquelle umgebende Kugel mit dem Radius formula_19 ergibt sich der von der Lichtquelle erzeugte Lichtstrom formula_20 zu

Die Lichtstärke "I" der Lichtquelle beträgt somit in allen Richtungen

"Anmerkung:" Die Rechnung wird dadurch stark vereinfacht, dass die Lichtquelle als isotrop strahlend vorausgesetzt wurde. Die bei der Berechnung des Lichtstromes im Allgemeinen notwendige Integration einer variablen Beleuchtungsstärke über die Kugelfläche konnte so durch eine einfache Multiplikation der konstanten Beleuchtungsstärke mit der gesamten Kugelfläche ersetzt werden. Die bei der Berechnung der Lichtstärke eigentlich für jede betrachtete Richtung nötige Bildung eines Differentialquotienten aus dem variablen Lichtstrom und dem differentiellen Raumwinkel konnte durch eine für alle Richtungen gültige einfache Bildung des Quotienten aus dem Lichtstrom und dem vollen Raumwinkel ersetzt werden.

Umrechnen der Einheiten Candela, Lumen und Lux in Abhängigkeit vom Strahlungskegel und der Entfernung.

Eine Leuchtdiode sende Licht in einem Lichtkegel mit dem Öffnungswinkel formula_23 (entsprechend einem Raumwinkel von formula_24, formula_25 = Steradiant) aus. Für alle Richtungen innerhalb des Kegels betrage die Lichtstärke formula_26. Außerhalb dieses Lichtkegels werde kein Licht abgestrahlt.

Der in den Kegel abgegebene Lichtstrom formula_20 beträgt

Dies ist gleichzeitig der gesamte von der Diode erzeugte Lichtstrom, da in andere Richtungen kein Licht abgegeben wird.

Die Fläche formula_4, die die Leuchtdiode mit formula_24 im Abstand formula_12 ausleuchtet, beträgt:

Beispielsweise beleuchtet die Diode im Abstand von 20 cm eine Fläche von 0,21 × 0,04 m = 0,0084 m.

Die Beleuchtungsstärke formula_17 auf der Fläche formula_4 in der Entfernung formula_12 beträgt

Beispielsweise wird im Abstand von 0,6 m die Beleuchtungsstärke 17 Lux erzeugt.

Siehe Beleuchtungsstärke#Beispiele




</doc>
<doc id="2947" url="https://de.wikipedia.org/wiki?curid=2947" title="Leistung">
Leistung

Leistung (v. mittelhochdeutsch "leistunge") steht für:


Siehe auch: 


</doc>
<doc id="2951" url="https://de.wikipedia.org/wiki?curid=2951" title="Linux">
Linux

Als Linux (deutsch []) oder GNU/Linux ("siehe" GNU/Linux-Namensstreit) bezeichnet man in der Regel freie, unix-ähnliche Mehrbenutzer-Betriebssysteme, die auf dem Linux-Kernel und wesentlich auf GNU-Software basieren. Die weite, auch kommerzielle Verbreitung wurde ab 1992 durch die Lizenzierung des Linux-Kernels unter der freien Lizenz GPL ermöglicht. Einer der Initiatoren von Linux war der finnische Programmierer Linus Torvalds. Er nimmt bis heute eine koordinierende Rolle bei der Weiterentwicklung des Linux-Kernels ein und wird auch als Benevolent Dictator for Life (deutsch "wohlwollender Diktator auf Lebenszeit") bezeichnet.

Das modular aufgebaute Betriebssystem wird von Softwareentwicklern auf der ganzen Welt weiterentwickelt, die an den verschiedenen Projekten mitarbeiten. An der Entwicklung sind Unternehmen, Non-Profit-Organisationen und viele Freiwillige beteiligt. Beim Gebrauch auf Computern kommen meist sogenannte Linux-Distributionen zum Einsatz. Eine Distribution fasst den Linux-Kernel mit verschiedener Software zu einem Betriebssystem zusammen, das für die Endnutzung geeignet ist. Dabei passen viele Distributoren und versierte Benutzer den Kernel an ihre eigenen Zwecke an.

Linux wird vielfältig und umfassend eingesetzt, beispielsweise auf Arbeitsplatzrechnern, Servern, Mobiltelefonen, Routern, Netbooks, Embedded Systems, Multimedia-Endgeräten und Supercomputern. Dabei wird Linux unterschiedlich häufig genutzt: So ist Linux im Server-Markt wie auch im mobilen Bereich eine feste Größe, während es auf dem Desktop und Laptops eine noch geringe, aber wachsende Rolle spielt.

Linux wird von zahlreichen Nutzern verwendet, darunter private Nutzer, Regierungen und Organisationen wie das Französische Parlament und das US-Verteidigungsministerium, Unternehmen wie Samsung, Siemens, Google, Amazon, Peugeot usw.

1983 rief Richard Stallman das GNU-Projekt ins Leben. Es war das Ziel, ein frei verfügbares Unix-ähnliches, POSIX-kompatibles Betriebssystem zu schaffen. Zwar war bereits Anfang der 90er Jahre eine ansehnliche Menge von Software geschrieben worden, doch steckte der eigentliche Betriebssystem-Kernel noch in einer frühen Phase und entwickelte sich nur langsam. Die ebenso freie Berkeley Software Distribution, die sich in den 80er Jahren entwickelt hatte, war in einen Rechtsstreit mit ungewissem Ausgang verwickelt und war aus diesem Grund ebenso keine Alternative als freies Betriebssystem. Damit stand Anfang der 1990er kein vollständiges, freies System zur Verfügung, welches für Entwickler interessant gewesen wäre.

1991 begann Linus Torvalds in Helsinki (Finnland) mit der Entwicklung einer Terminal-Emulation, um unter anderem seinen eigenen Computer besser zu verstehen. Mit der Zeit merkte er, dass sich das System immer mehr zu einem Betriebssystem entwickelte; daraufhin kündigte er es in der Usenet-Themengruppe für das Betriebssystem Minix, "comp.os.minix", an. Im September desselben Jahres sollte das System dann auf einem Server den Interessierten zur Verfügung gestellt werden. Dem damaligen FTP-Server-Administrator Ari Lemmke gefiel keiner der von Torvalds vorgeschlagenen Namen "Freax" oder "Buggix", deshalb veröffentlichte er es stattdessen in einem Verzeichnis mit dem Namen Linux. Torvalds war mit diesem Namen zunächst nicht einverstanden, gab seinen Widerstand aber schnell auf, weil er nach eigener Aussage eingestehen musste, dass Linux einfach ein besserer Name war.

Linux wurde zu dieser Zeit noch unter einer proprietären Lizenz von Torvalds veröffentlicht, die die kommerzielle Nutzung verbot. Er merkte jedoch bald, dass das den Fortschritt der Entwicklung behinderte. Er wollte allen Entwicklern deutlich mehr Freiraum geben und stellte Linux deshalb im Januar 1992 unter die GNU GPL. Es war nun möglich, Linux in GNU zu integrieren und dies als das erste freie Betriebssystem zu vertreiben. Dieser Schritt machte das System für eine noch größere Zahl von Entwicklern interessanter, da er die Modifizierung und Verbreitung vereinfachte.

Die Bezeichnung Linux wurde von Torvalds anfänglich nur für den von ihm geschriebenen Kernel genutzt. Dieser wurde anfänglich auf Minix verwendet. Torvalds und die anderen Linux-Autoren lizenzierten 1992 Linux unter der GNU GPL, so dass der Kernel in GNU integriert werden konnte. Diese GNU-Variante wurde schnell zur meist genutzten Variante, da es zu dieser Zeit keinen funktionierenden freien Kernel gab. Als Torvalds und seine Anhänger später auch das gesamte Betriebssystem als "Linux" bezeichneten, versuchte der Gründer des GNU-Projekts, Richard Stallman, bald, den Namen GNU/Linux durchzusetzen, um der Rolle von GNU eine in seinen Augen angemessene Geltung zu verschaffen. Diese Forderung stieß auf unterschiedliche Reaktionen. Während das GNU-Projekt und das Debian-Projekt den Namen annahmen, lehnten die meisten Entwickler und anderen Linux-Distributoren dies ab oder widersetzten sich deutlich. Begründet wurde dies einerseits mit Bequemlichkeit, weil der Name "Linux" als einfacher angesehen wurde, und andererseits mit dem Hinweis, dass mittlerweile eine beachtliche Menge der mit Linux ausgelieferten Software nicht aus dem GNU-Projekt stamme.

Die Entwicklung des Linux-Kernels wird noch immer von Torvalds organisiert. Dieser ist dafür bei der gemeinnützigen Linux Foundation angestellt. Andere wichtige Entwickler werden oft von verschiedenen Unternehmen bezahlt. So arbeitet z. B. Andrew Morton im Auftrag von Google am Linux-Kernel und ist dabei im sogenannten "Merge Window" für das Sammeln aller Änderungen und das Weiterleiten an Torvalds zuständig.

Neben der Kernel-Entwicklung haben sich auch andere Projekte um das Betriebssystem gesammelt, die es für eine größere Nutzerzahl interessant machten. So ermöglichen grafische Benutzeroberflächen wie KDE oder Gnome einen hohen Benutzerkomfort beim Einsatz als Desktop-System. Verschiedene auf den Desktop ausgelegte Linux-Distributionen vereinfachten die Installation und Konfiguration von Linux so weit, dass sie auch von Anfängern problemlos gemeistert werden können.

Eine weltweite Entwickler- und Nutzergemeinde erstellt eine Vielzahl an weiterer Software und Dokumentation rund um Linux, die die Einsatzmöglichkeiten enorm ausgedehnt haben. Hinzu kommt, dass Hersteller proprietärer Software zunehmend einen Markt bei Linux-Anwendern erkennen und mit der Zeit vermehrt Programme für Linux anbieten. Dabei läuft die Entwicklung schwerpunktmäßig freier Software sowohl in selbstorganisierten Projekten, bestehend aus ehrenamtlichen und bezahlten Entwicklern, als auch in teilweise von Unternehmen unterstützten Stiftungen. Gemein ist allen Modellen, dass sie sich stark über das Internet vernetzt haben und dort ein Großteil der Organisation und Absprache stattfindet.

Schon früh kam es rund um Linux zum Streit. 1992 griff Andrew S. Tanenbaum Linux wegen eines aus seiner Sicht veralteten Designs und eines zu liberalen Entwicklungsmodells an. Später kam Tanenbaum erneut ins Spiel, als Ken Brown an seinem Buch "Samizdat" schrieb und nach Anhaltspunkten suchte, dass Linux nur eine Kopie von Tanenbaums Minix sei. Tanenbaum nahm Linux diesmal in Schutz. Linux habe ein zu schlechtes Design, als dass es abgeschrieben sein könne.

Anderen Streit gab es mit erklärten Konkurrenten. Schon früh wurden interne Microsoft-Dokumente (Halloween-Dokumente) bekannt, die aufzeigten, dass Microsoft annahm, Linux sei die größte Gefahr für Windows. Später begann Microsoft mit einer Kampagne, um Windows bei einer Gegenüberstellung mit Linux technisch wie wirtschaftlich gut aussehen zu lassen. Während die Community diese Kampagne recht gelassen sah, starteten vor allem Unternehmen im Linux-Umfeld Gegenkampagnen. Im Herbst 2006 aber kündigten Microsoft und Novell an, bei Interoperabilität und Patentschutz zusammenzuarbeiten, um so die Zusammenarbeit der einzelnen Produkte zu verbessern.

Ein anderer Konkurrent, der Unix-Hersteller SCO, erhob wiederum 2003 den Vorwurf, dass bei IBM angestellte Linux-Entwickler Quellcode von SCOs Unix in Linux kopiert hätten. Das Verfahren wurde im Sommer 2007 eingestellt, die SCO Group hat mittlerweile Insolvenz angemeldet und wurde vom Börsenhandel ausgeschlossen. 2013 wurde eine Wiederaufnahme des Verfahrens beantragt. Im Artikel SCO gegen Linux ist der Streit chronologisch dokumentiert.

Ebenfalls machte das Markenrecht Linux schon früh zu schaffen. So ließen einige Privatpersonen Mitte der 1990er Jahre den Namen Linux auf sich eintragen, was Torvalds nur mit viel Hilfe wieder rückgängig machen konnte. Er übertrug die Verwaltung der Markenrechte an das Linux Mark Institute, welches wiederum im Jahr 2005 auffiel, als es die Lizenzen für den Markenschutz auf bis zu 5.000 Dollar pro Jahr festlegte. Diese Summe brachte hauptsächlich viele an Community-Projekten beteiligte Gemüter in Wallung, woraufhin sich Torvalds genötigt fühlte, in einem offenen Brief Stellung zu nehmen und klarzustellen, dass das Geld schlichtweg benötigt werde, damit das gemeinnützig arbeitende Linux Mark Institute seine eigenen Kosten decken könne.

Die Bezeichnung "Linux" wurde von Linus Torvalds anfänglich nur für den Kernel genutzt, dieser stellt der Software eine Schnittstelle zur Verfügung, mit der sie auf die Hardware zugreifen kann, ohne sie genauer zu kennen. Der Linux-Kernel ist ein in der Programmiersprache C geschriebener monolithischer Kernel. Wichtige Teilroutinen sowie zeitkritische Module sind jedoch in prozessorspezifischer Assemblersprache programmiert. Der Kernel ermöglicht es, nur die für die jeweilige Hardware nötigen Treiber zu laden. Weiterhin übernimmt der Kernel auch die Zuweisung von Prozessorzeit und Ressourcen zu den einzelnen Programmen, die auf ihm gestartet werden. Bei den einzelnen technischen Vorgängen orientiert sich das Design von Linux stark an seinem Vorbild Unix.

Der Linux-Kernel wurde zwischenzeitlich auf eine sehr große Anzahl von Hardware-Architekturen portiert. Das Repertoire reicht von eher exotischen Betriebsumgebungen wie dem iPAQ-Handheld-Computer, Navigationsgeräten von TomTom oder gar Digitalkameras bis hin zu Großrechnern wie IBMs System z und neuerdings auch Mobiltelefonen wie dem Motorola A780 sowie Smartphones mit Betriebssystemen wie Android oder Sailfish OS auf dem Jolla. Trotz Modulkonzept blieb die monolithische Grundarchitektur erhalten. Die Orientierung der Urversion auf die verbreiteten x86-PCs führte früh dazu, verschiedenste Hardware effizient zu unterstützen und die Bereitstellung von Treibern auch unerfahrenen Programmierern zu ermöglichen. Die hervorgebrachten Grundstrukturen beflügelten die Verbreitung.

Auf kernel.org werden alle Kernel-Versionen archiviert. Die dort zu findende Version ist der jeweilige Referenzkernel. Auf diesem bauen die sogenannten Distributionskernel auf, die von den einzelnen Linux-Distributionen um weitere Funktionen ergänzt werden. Eine Besonderheit stellt dabei das aus vier Zahlen bestehende und durch Punkte getrennte Versionsnummernschema dar, z. B. "2.6.14.1". Es gibt Auskunft über die exakte Version und damit auch über die Fähigkeiten des entsprechenden Kernels. Von den vier Zahlen wird die letzte für Fehlerbehebungen und Bereinigungen geändert, nicht aber für neue Funktionen oder tiefgreifende Änderungen. Aus diesem Grund wird sie auch nur selten mit angegeben, wenn man beispielsweise Kernel-Versionen vergleicht. Die vorletzte, dritte Zahl wird geändert, wenn neue Fähigkeiten oder Funktionen hinzugefügt werden. Gleiches gilt für die ersten beiden Zahlen, bei diesen müssen die Änderungen und neuen Funktionen jedoch drastischer ausfallen. Ab Version 3.0 (August 2011) wird auf die zweite Stelle verzichtet.

Die Entwicklung von Linux liegt durch die GPL und durch ein sehr offenes Entwicklungsmodell nicht in der Hand von Einzelpersonen, Konzernen oder Ländern, sondern in der Hand einer weltweiten Gemeinschaft vieler Programmierer, die sich in erster Linie über das Internet austauschen. In vielen E-Mail-Listen, aber auch in Foren und im Usenet besteht für jedermann die Möglichkeit, die Diskussionen über den Kernel zu verfolgen, sich daran zu beteiligen und auch aktiv Beiträge zur Entwicklung zu leisten. Durch diese unkomplizierte Vorgehensweise ist eine schnelle und stetige Entwicklung gewährleistet, die auch die Möglichkeit mit sich bringt, dass jeder dem Kernel Fähigkeiten zukommen lassen kann, die er benötigt. Eingegrenzt wird dies nur durch die Kontrolle von Linus Torvalds und einigen speziell ausgesuchten Programmierern, die das letzte Wort bei der Aufnahme von Verbesserungen und Patches haben. Auf diese Weise entstehen täglich grob 4.300 Zeilen neuer Code, wobei auch täglich ungefähr 1.800 Zeilen gelöscht und 1.500 geändert werden (Angaben nach Greg Kroah-Hartman als Durchschnitt für das Jahr 2007). An der Entwicklung sind derzeit ungefähr 100 Verantwortliche („maintainer“) für 300 Subsysteme beteiligt.

Der stabile Kernel 2.6 wurde ab Dezember 2001 auf Basis des damaligen 2.4er-Kernels entwickelt und weist eine Reihe von Neuerungen auf. Die auffälligste Auswirkung dieser Änderungen ist, dass graphische und interaktive Anwendungen deutlich schneller ausgeführt werden.

Eine der wichtigsten Änderungen war dabei die Verbesserung des sogenannten "O(1)-Schedulers", den Ingo Molnár für den 2.6er-Kernel komplett neu konzipierte. Er hat die Fähigkeit, das Zuweisen von Prozessorzeit zu unterschiedlichen Prozessen unabhängig von der Anzahl der Prozesse in konstanter Zeit zu erledigen. Seit Kernel 2.6.23 kommt allerdings stattdessen der sogenannte "Completely Fair Scheduler" zum Einsatz.

Eine andere Neuerung stellt die Einführung von Access Control Lists dar, mit deren Hilfe ein sehr fein abgestimmtes Rechtemanagement möglich ist, was vor allen Dingen in Umgebungen mit vielen Benutzern sehr wichtig ist. Ebenso verfügt der neue Kernel über ein deutlich verbessertes System der Dateiüberwachung. In der neuen Version, "Inotify" genannt, gibt die Überwachung bei jeder Operation an einer Datei eine Nachricht ab, was z. B. für Desktop-Suchmaschinen wichtig ist, die daraufhin ihren Index in Bezug auf diese Datei aktualisieren können.

Da der Linux-Kernel alleine nicht lauffähig bzw. bedienbar wäre, muss man ihn mit Hilfssoftware zusammen verteilen, beispielsweise den GNU core utilities und vielen anderen Anwendungsprogrammen. Solch eine Zusammenstellung nennt man „Linux-Distribution“, sie ist eine Zusammenstellung verschiedener Software, die je nach Bedingung unterschiedlich sein kann. Die so entstehenden Distributionen unterscheiden sich teilweise sehr deutlich. Der Herausgeber einer Linux-Distribution ist der "Distributor".

Die Notwendigkeit von Linux-Distributionen ergab sich durch das Entwicklungsmodell von Linux nahezu sofort. Die Werkzeuge des GNU-Projekts wurden zügig für Linux angepasst, um ein arbeitsfähiges System bereitstellen zu können. Die ersten Zusammenstellungen dieser Art waren 1992 "MCC Interim Linux", "Softlanding Linux System" (SLS) und "Yggdrasil Linux". Die älteste bis heute existierende Distribution, Slackware von Patrick Volkerding, folgte 1993 und stammt von Softlanding Linux System ab.

Mit der Ausbreitung der Linux-Distributionen bekamen mehr Menschen die Möglichkeit, das System zu testen, des Weiteren wurden die Distributionen immer umfangreicher, so dass ein immer größerer Einsatzbereich erschlossen werden konnte, was Linux zunehmend zu einer attraktiven Alternative zu Betriebssystemen etablierter Hersteller werden ließ. Im Laufe der Zeit änderte sich auch der Hintergrund der Distributionen: Wurden die ersten Distributionen noch der Bequemlichkeit halber und von Einzelpersonen oder kleinen Gruppen geschrieben, gibt es heutzutage teilweise sehr große Gemeinschaftsprojekte Freiwilliger, Unternehmens-Distributionen oder eine Kombination aus beidem.

Hinter den meisten, vorrangig kleinen Distributionen stehen heutzutage über das Internet koordinierte Projekte Freiwilliger. Die großen Distributionen werden eher von Stiftungen und Unternehmen verwaltet. Auch die Einsatzmöglichkeiten der einzelnen Distributionen differenzierten sich mit der Zeit stark. Vom Desktop-PC über Server-Installationen und Live-CDs bis hin zu Distributionen zu technischen Forschungszwecken ist alles vertreten. Die Zusammensetzung einer üblichen Linux-Distribution für den Desktop-PC umfasst eine große Zahl von Softwarekomponenten, die das tägliche Arbeiten ermöglichen. Die meisten Distributionen werden in Form fertiger CD- oder DVD-Images im Internet bereitgestellt oder mit Support-Verträgen oder Handbüchern verkauft.

Für besondere Anwendungsgebiete existieren oft keine direkt installierbaren Distributionen. Hier werden Frameworks wie OpenEmbedded z. B. für Router oder Handys verwendet, um eine Distribution für den Einsatz auf dem Gerät vorzubereiten.

Es wird eine große Anzahl an Distributionen angeboten, die dem Benutzer eine sehr feine Abstimmung der Auswahlkriterien auf die eigenen Bedürfnisse ermöglicht. Die Auswahl der geeignetsten Distribution ist für viele unerfahrene Benutzer daher nicht einfach. Die verwendete Software kann mehr Gewicht für Privatanwender haben als für Unternehmen, die wiederum mehr Wert auf die Verfügbarkeit eines offiziellen Kundendiensts („Support“) legen. Auch kann die Politik des Projekts oder die des Unternehmens hinter der Distribution, z. B. in Bezug auf proprietäre Software, ebenso eine Rolle spielen wie die Eigenschaften der Community in diesem Projekt.

Die Liste von Linux-Distributionen enthält eine Aufzählung der wichtigsten oder populärsten Distributionen.

Die Vielfalt der Distributionen, die teilweise verschiedene binäre Formate, eigene Verzeichnisstrukturen und ähnliche Unterschiede aufweisen, führt zu einem gewissen Grad an Inkompatibilität zwischen den Distributionen, der bisher auch durch Richtlinien wie den Filesystem Hierarchy Standard und der Linux Standard Base nicht behoben werden konnte. So kann Software, die für die Distribution A bereitgestellt wird, nicht notwendigerweise auch auf der Distribution B installiert werden. Verschiedene Sichtweisen und Lösungsansätze zu dieser Problematik werden im Hauptartikel Linux-Distribution näher beleuchtet.

Die Einsatzgebiete von Linux sind seit der ersten Version stetig erweitert worden und decken heutzutage einen weiten Bereich ab.

Linux, beziehungsweise eine Linux-Distribution, lässt sich als allein installiertes Betriebssystem betreiben, aber auch innerhalb eines Multi-Boot-Systems einsetzen. Parallel installieren kann man Linux beispielsweise neben Windows oder einem BSD wie FreeBSD oder macOS. Moderne Distributionen wie OpenSUSE, Debian oder Ubuntu führen den Nutzer mit Hilfe von grafischen Benutzeroberflächen durch die Installation auf dem PC und erkennen andere Betriebssysteme nahezu immer selbstständig. Aus weit über tausend kostenlosen Programmen kann eine individuelle Kombination ausgewählt werden. Textverarbeitung, Tabellenkalkulation, Multimedia-Anwendungen, Netzwerktools, Spiele oder wissenschaftliche Anwendungen decken die meisten Anwendungsbereiche ab, die im Büroalltag und im Privatbereich wichtig sind.

Trotz Sicherheitsvorsprungs gegenüber dem am weitesten verbreiteten Betriebssystem Windows und der Möglichkeit der Parallelinstallation und umfangreichen, kostenlosen Softwareangebots wird Linux auf Desktoprechnern zögerlich eingesetzt. Auch wenn sich die verbreitetsten Linux-Desktop-Umgebungen ähnlich bedienen lassen wie Windows oder macOS, unterscheiden sie sich durch diverse Systemfunktionen von ihnen. Daher kann wie bei fast jedem Wechsel des Betriebssystems eine gewisse Einarbeitungszeit nötig sein. Im Gegensatz zur geringen Verbreitung auf dem Desktop ist Linux auf Server-Systemen, bei Embedded-Systemen und auf Smartphones bereits ein etabliertes Betriebssystem.

Die Installation der meisten Distributionen ist einfach und gibt geläufige Einstellungen vor, auch die Installation der Anwendungen läuft meist vollautomatisch ab, da sie üblicherweise von einem Paketmanager übernommen wird. Da das genaue Vorgehen aber nicht bei allen Linux-Distributionen einheitlich geregelt ist, kann ein Wechsel der Linux-Distribution Einarbeitungszeit erfordern. Die Installation von Programmen, die nicht zum Umfang der Distribution gehören, kann unterschiedlich sein: Im Idealfall existiert eine Paketquelle der Programmentwickler, die im Paketmanager eingebunden werden und über diesen dann installiert werden kann. Daneben gibt es für eine Reihe von Programmen Pakete, die auf die Distribution abgestimmt zum Download verfügbar sind. Im ungünstigsten Fall muss die Software als Quellcode bezogen werden und für das jeweilige System kompiliert werden. Anwendungen, die vom Anbieter nur für macOS oder Windows auf den Markt gebracht wurden, kann man i. d. R. unter Linux mittels API-Implementierungen wie Wine, Cedega oder Darling bzw. GNUstep verwenden. In anderen Fällen muss man zu alternativen Anwendungen greifen, die für Linux verfügbar sind.
Die beiden weit verbreiteten Desktop-Umgebungen Gnome und KDE haben unterschiedliche Bedienungskonzepte, weshalb viele Distributoren Standards und Richtlinien veröffentlichen, um sowohl Entwicklern als auch Nutzern den Umgang mit verschiedenen Desktop-Umgebungen nahezubringen und ihn zu vereinheitlichen.

Bekannt geworden sind größere Migrationen von Unternehmen oder Institutionen, die mehrere hundert oder tausend Rechner auf Linux-Desktops umgestellt haben, wie die Stadt München im Rahmen des LiMux-Projekts oder die Umstellung von 20.000 Desktops bei Peugeot Citroën. Durch die Auslieferung vorinstallierter Systeme durch einige Fachhändler sowie die wachsende Beliebtheit einiger Distributionen wie Ubuntu wuchs die Linux-Verwendung auf Desktoprechnern von Anfang 2007 bis Mitte 2008 um fast 30 Prozent. In Großbritannien lag der Marktanteil 2008 bei etwa 2,8 Prozent. Weltweit wurde im April 2009 im "Market-Share-Report" von "Net Applications" erstmals ein Marktanteil von einem Prozent ermittelt. Nachdem er 2010 gemäß NetMarketShare wieder auf 0,9 % gefallen war, stieg der Marktanteil bis Dezember 2011 auf 1,41 %. Ende 2016 lag der Marktanteilanteil bei
2,2 %.

Aufgrund der Kompatibilität von Linux mit anderen unixoiden Systemen hat sich Linux auf dem Servermarkt besonders schnell etabliert. Da für Linux schon früh zahlreiche häufig verwendete und benötigte Serversoftware wie Webserver, Datenbankserver und Groupware kostenlos und weitgehend uneingeschränkt zur Verfügung stand, wuchs dort der Marktanteil stetig.

Da Linux als stabil und einfach zu warten gilt, erfüllt es auch die besonderen Bedingungen, die an ein Server-Betriebssystem gestellt werden. Der modulare Aufbau des Linux-Systems ermöglicht zusätzlich das Betreiben kompakter, dedizierter Server. Außerdem hat die Portierung von Linux auf verschiedenste Hardwarekomponenten dazu geführt, dass Linux alle bekannten Serverarchitekturen unterstützt.

Eingesetzt wird es dabei für praktisch alle Aufgaben. Eines der bekanntesten Beispiele ist die Linux-Server-Konfiguration LAMP, bei der Linux mit Apache, MySQL und PHP/Perl (manchmal auch Python) kombiniert wird. Auch proprietäre Geschäftssoftware wie SAP R/3 ist mittlerweile auf verschiedenen Distributionen verfügbar und hat eine Installationszahl von über 1.000 Systemen erreicht. Das Linux Terminal Server Project ermöglicht es, sämtliche Software außer dem BIOS der Clients zentral zu verwalten.

Da Linux auf einer Vielzahl von verschiedenen Hardwaretypen betrieben werden kann, ist auch die für Linux-Server genutzte Hardware ähnlich umfangreich. Auch moderne Hardware wie die von IBMs eServer p5 wird unterstützt und ermöglicht dort das parallele Ausführen von bis zu 254 Linux-Systemen (Modell p595). Auf IBM-Großrechnern der aktuellen System-z-Linie läuft Linux wahlweise nativ, mittels PR/SM in bis zu 30 LPARs oder in jeder davon unter z/VM in potenziell unbegrenzt vielen, real einigen zehntausend virtuellen Maschinen.

Im Januar 2017 wurden mindestens 34 % aller Websites mittels eines Linux-Servers verfügbar gemacht. Da nicht alle Linux-Server sich auch als solche zu erkennen geben, könnte der tatsächliche Anteil um bis zu 31 Prozentpunkte höher liegen. Damit ist ein tatsächlicher Marktanteil von bis zu etwa 65 % nicht auszuschließen. Der Marktanteil von verkauften Linux-Server-Systemen lag im zweiten Quartal 2013 bei 23,2 %. Da bei Servern nicht selten von einem Kunden selbst ein anderes Betriebssystem installiert wird, gibt diese Zahl nur bedingt Auskunft über die effektive Verwendung von Linux auf Server-Systemen.

Für Smartphones und Tablets gibt es speziell optimierte Linux-Distributionen. Sie bieten neben den Telefonie- und SMS-Funktionen diverse PIM-, Navigations- und Multimedia-Funktionen. Die Bedienung erfolgt typischerweise über Multi-Touch oder mit einem Stift. Linux-basierte Smartphonesysteme werden meist von einem Firmenkonsortium oder einer einzelnen Firma entwickelt und unterscheiden sich teilweise sehr stark von den sonst klassischen Desktop-, Embedded- und Server-Distributionen. Anders als im Embedded-Bereich sind Linux-basierte Smartphonesysteme aber nicht auf ein bestimmtes Gerät beschränkt, vielmehr dienen sie als Betriebssystem für Geräte ganz unterschiedlicher Modellreihen und werden oft herstellerübergreifend eingesetzt.

Die Architektur dieser Smartphone- und Tablet-Distributionen hat neben dem Linux-Kernel teilweise wenig mit den klassischen Distributionen zu tun. So wird von Android nur ein Teil der sonst üblichen GNU-Software-Umgebung genutzt. Die meist auf Linux genutzten UNIX-artigen Dienste und Tools werden teilweise durch eine Java-Laufzeitumgebung ersetzt. Dadurch entstehen neue Programmierschnittstellen, die sich auf beliebigen anderen Plattformen emulieren bzw. umsetzen lassen. Trotzdem wird Android als Linux-Distribution angesehen, die viele Eigenschaften mitbringt, die es mit zahlreichen Embedded-Linux-Distributionen teilt. Andere Smartphone-Distributionen, wie etwa Firefox OS, Ubuntu for phones, Maemo, Tizen, Mer, Sailfish OS und MeeGo nutzen größere Teile der klassischen GNU-Software-Umgebung, so dass diese Distributionen teilweise einfacher mit klassischen Linux-Anwendungen ergänzt werden können und somit eher Linux-Distributionen im klassischen Sinne entsprechen.

Das von HP Palm entwickelte WebOS setzt ebenfalls auf dem Linux-Kernel auf, das Userland jedoch besteht aus einer proprietären Entwicklung unter anderer Lizenz. Auch das ehemals von Samsung entwickelte Bada war neben einem RTOS-Kernel auch auf einem Linux-Kernel nutzbar, was aber von Samsung nie in dieser Kombination verkauft wurde.

Linux-Systeme haben seit Ende 2010 die Marktführerschaft auf dem schnell wachsenden Smartphone-Markt übernommen. Sie weisen in Deutschland seit Februar 2013 durchgehend einen Marktanteil von über 70 % auf mit einem bisherigen Maximum von über 82 % im Juli 2014 (Anteile Linux-basierter Alternativen zu Android wurden in der Statistik nicht explizit angegeben). Vorwiegend Android-Geräte haben Apple iOS, Windows Phone und Symbian OS erfolgreich zurückgedrängt.

Da Linux beliebig angepasst und optimiert werden kann, hat es sich auch in Rechenzentren stark verbreitet, in denen speziell angepasste Versionen auf Großrechnern, Computerclustern (siehe Beowulf) oder Supercomputern laufen.

In der TOP500-Liste der schnellsten Supercomputer (Stand November 2017) werden alle gelisteten Systeme mit Linux betrieben. Der im Desktop-Bereich größte Konkurrent Windows spielt im Bereich der Höchstleistungsrechner keine Rolle. Im Juni 2011 waren es noch 4 Systeme (darunter Platz 40), die mit dem Betriebssystem Windows liefen.

Linux setzt sich aus vielfältigen Gründen auch immer mehr in der Industrie, speziell in der Automobilindustrie, durch. Das weltweit erste von Linux betriebene Infotainment-System wurde von General Motors in Kooperation mit Bosch entwickelt. Die GENIVI Alliance definiert Anforderungen an eine Linux-Distribution speziell für Infotainment-Systeme in Fahrzeugen. Die größte Marktdurchdringung hat Linux in Japan. Zu den bekannten Unternehmen, die Linux verwenden, gehören: Ashisuto, Aisin AW, JVC KENWOOD Corporation, NTT DATA MSE und Turbo Systems.

Ferner können auch NAS-Speichersysteme oder WLAN-Router Linux als Betriebssystem nutzen. Vorteil ist, dass eine sehr aktive Entwickler-Community besteht, auf deren Ressourcen (der Kernel mit den Schnittstellen-, Speicherverwaltungs- und Netzwerkfunktionen, aber z. B. auch umfangreiche Entwicklerprogramme, bereits bestehender Code wie die Benutzeroberflächen OPIE oder GPE Palmtop Environment, Erfahrung etc.) die Hersteller dabei zurückgreifen können.

Die Gründe für die Bewertung von Linux als sicheres System sind verschieden und hängen von dessen Aufgaben und der verwendeten Softwarekonfiguration ab. So verfügt Linux als Desktop-System über eine strenge Unterteilung der Zugriffsrechte, die bei anderen verbreiteten Desktop-Systemen im Normalfall nicht eingehalten wird. Dies führt unter anderem dazu, dass viele Funktionsprinzipien verbreiteter Würmer und Viren bei Linux nicht greifen können beziehungsweise nur den ausführenden Benutzer, jedoch nicht das ganze System, kompromittieren können. Eine Kompromittierung des Nutzers kann gleichwohl zu sensiblen Datenverlusten führen. Bisher traten nur sehr wenige Viren unter Linux auf, beispielsweise Staog und Bliss. Im Vergleich zu anderen Desktop-Systemen hat Linux die erste größere Verbreitung bei Nutzern mit einem sehr technischen und sicherheitsbewussten Umfeld erfahren. Die Entwicklung geschah somit, verglichen mit anderen verbreiteten Desktop-Systemen, unter den Augen eines sehr sicherheitskritischen Publikums. Im Gegensatz zu Desktop-Systemen hängt die Sicherheit bei Serversystemen primär vom Grad der Erfahrung der Administratoren mit dem System selbst ab. Linux punktet dabei durch die freie Verfügbarkeit, die es Administratoren ermöglicht, das System ohne Mehrkosten in verschiedensten Testszenarien zu installieren und dort ausgiebig zu untersuchen. Zudem gibt es eine Reihe von speziell gehärteten Linux-Distributionen, welche besonderen Wert auf Sicherheitsaspekte legen. Initiativen wie SELinux bemühen sich dort um das Erfüllen hoher Sicherheitsstandards.

Vorteilhaft ist, dass Linux nicht auf eine Hardware-Architektur festgelegt ist. Würmer und Viren können sich immer nur auf dem Teil der Linux-Systeme verbreiten, auf deren Hardware sie zugeschnitten sind. Hinzu kommt, dass Linux quelloffene Software ist. Jeder kann also den Quellcode studieren, untersuchen und anpassen. Dies führt unter anderem auch dazu, dass der Quellcode (sei es zum Zwecke der Anpassung, zum Zwecke der Schulung, aus dem Sicherheitsinteresse einer Institution oder eines Unternehmens heraus oder aus privatem Interesse) von mehr Menschen studiert wird, als dies bei proprietären Programmen der Fall sein kann, wodurch Sicherheitslücken schneller auffallen (und dann behoben werden können).

Ein wesentliches Merkmal vieler Linux-Distributionen ist es, dass sie kostenlos und automatisiert Sicherheitsaktualisierungen für alle bereitgestellte Software anbieten. Diese Funktion existiert zwar auch bei anderen gängigen Betriebssystemen, erfasst dort aber nicht alle bereitgestellte Software, funktioniert nicht durchgehend automatisch oder ist nicht kostenlos, weshalb die Hürde, solche Aktualisierungen einzuspielen, bei anderen Betriebssystemen höher ist als bei Linux.

Unter anderem wegen der allgemein verfügbaren Sicherheitsaktualisierungen sind Antivirenprogramme für Linux wenig verbreitet. Anstatt mit einem Antivirenprogramm nach Schadsoftware suchen zu lassen, die bekannte Sicherheitslücken in der installierten Anwendungssoftware ausnutzt, können die bekannten Lücken bereits über Sicherheitsaktualisierungen geschlossen werden. Die existierenden Antivirenprogramme für Linux werden daher hauptsächlich dafür eingesetzt, um Datei- und E-Mail-Server auf Viren für andere Betriebssysteme zu untersuchen.

Vom technischen Gesichtspunkt her verfügt Linux über viele Fähigkeiten, die eine sicherheitstechnisch anspruchsvolle Umgebung erfordert. Dazu gehört sowohl eine einfache Nutzer- und Gruppenrechteverwaltung mittels Role Based Access Control, wie auch eine komplexere Rechteverwaltung mit Hilfe von Access Control Lists. Zusätzlich implementieren viele aktuelle Distributionen auch Mandatory-Access-Control-Konzepte mit Hilfe der SELinux/AppArmor-Technik.

Ebenso bietet fast jede Linux-Distribution auch eine Secure-Shell-Implementierung (zumeist OpenSSH) an, mit der authentifizierte verschlüsselte und deswegen sichere Verbindungen zwischen Computern gewährleistet werden können. Andere Verschlüsselungstechniken wie Transport Layer Security werden ebenfalls voll unterstützt.

Im Rahmen der Verschlüsselung für auf Medien gespeicherte Daten steht das Kryptographie-Werkzeug dm-crypt zur Verfügung, das eine Festplattenverschlüsselung ermöglicht. Es bietet dabei die Möglichkeit der Verschlüsselung nach aktuellen Standards wie dem Advanced Encryption Standard. Transparente Verschlüsselung, bei der nur einzelne Dateien statt ganzer Festplatten verschlüsselt werden, stellen die Verschlüsselungserweiterung EncFS und das Dateisystem ReiserFS zur Verfügung. Zu den Sicherheitszertifikaten, die im Zusammenhang mit Linux erworben wurden, siehe den Abschnitt Software-Zertifikate.

Um den Grad der Kenntnisse von Technikern und Administratoren messbar zu machen, wurden eine Reihe von Linux-Zertifikaten ins Leben gerufen. Das Linux Professional Institute (LPI) bietet dafür eine weltweit anerkannte Linux-Zertifizierung in drei Levels, die ersten beiden Level (LPIC-1 und LPIC-2) mit jeweils zwei Prüfungen und den dritten Level (LPIC-3) mit einer Core-Prüfung (301) und mehreren optionalen Erweiterungsprüfungen. Auch die großen Linux-Distributoren wie Red Hat, openSUSE und Ubuntu bieten eigene Schulungszertifikate an, die aber zum Teil auf die Distributionen und deren Eigenheiten ausgelegt sind.

Um den Grad der Sicherheit von Technikprodukten zu bewerten, gibt es ebenfalls eine Reihe von Zertifikaten, von denen wiederum viele für bestimmte Linux-Distributionen vergeben wurden. So hat z. B. das Suse Linux Enterprise Server 9 des Linux-Distributors Novell die Sicherheitszertifikation EAL4+ nach den Common Criteria for Information Technology Security Evaluation erhalten, Red Hat hat für seine Redhat Enterprise Linux 4 Distribution ebenso die EAL4+-Zertifizierung erhalten. Ein Problem bei der Zertifizierung stellen für viele Distributoren allerdings die hohen Kosten dar. So kostet eine Zertifizierung nach EAL2 etwa 400.000 US-Dollar.

Eine häufige Schwierigkeit beim Einsatz von Linux besteht darin, dass oft keine ausreichende Hardware-Unterstützung gegeben ist. Tatsächlich verfügt Linux zahlenmäßig über mehr mitgelieferte Treiber als vergleichbare Systeme (Windows, Mac OS X). Das führt dazu, dass in der Regel nicht einmal eine Treiber-Installation notwendig ist und dass sogar ein Wechsel von Hardware reibungslos möglich ist. Das bietet dem Anwender deutlich mehr Komfort als bei vergleichbaren Betriebssystemen, da so z. B. ein problemloser Umzug des Betriebssystems auf einen anderen Rechner oder sogar die Installation des Betriebssystems auf Wechseldatenträgern möglich ist, ohne dass hierfür spezielle Anpassungen am System nötig wären.

Oft ist diese reibungslose Hardware-Unterstützung jedoch nicht gegeben. Das gilt insbesondere für aktuellere Hardware. Die Ursache liegt darin begründet, dass nur wenige Hardwarehersteller selbst Linux-Treiber für ihre Hardware zur Verfügung stellen oder diese nur in schlechter Qualität vorliegen. Während für Hardware mit offen dokumentierter, standardisierter Schnittstelle (z. B. Mäuse, Tastaturen, Festplatten und USB-Host-Controller) Treiber zur Verfügung stehen, ist dies für andere Hardwareklassen (z. B. Netzwerkschnittstellen, Soundkarten und Grafikkarten) nicht immer der Fall. Viele Hardwarehersteller setzen auf proprietäre hardwarespezifische Schnittstellen, deren Spezifikation zudem nicht öffentlich zugänglich ist, sodass sie mittels Black-Box-Analyse bzw. Reverse Engineering erschlossen werden muss. Beispiele hierfür sind Intels HD Audio-Schnittstelle und deren Linux-Implementierung "snd-hda-intel" oder der freie 3D-Grafiktreiber nouveau für bestimmte 3D-Grafikchips von Nvidia. Ein anderes Beispiel ist der Energieverwaltungsstandard ACPI, der sehr komplex und auf die jeweilige Hauptplatine zugeschnitten ist, sodass eine Implementierung durch die Linux-Gemeinschaft aus Mangel an Ressourcen oder Hintergrundwissen oft unzureichend ist. Oft kann in diesem Zusammenhang auch das Mitwirken der Anwender hilfreich sein, indem sie auf Probleme hinweisen und idealerweise sogar technische Informationen zu ihrer Hardware ermitteln und der Linux-Gemeinschaft zur Verfügung stellen oder Entwicklerversionen vor der Veröffentlichung testen.

Ein oft genannter Grund für die Nichtbereitstellung von Linuxtreibern ist das Entwicklungsmodell des Linux-Kernels: Da er keine feste Treiber-API besitzt, müssen Treiber immer wieder an Veränderungen in den einzelnen Kernel-Versionen angepasst werden. Direkt in den Kernel integrierte Treiber werden zwar von den Kernel-Entwicklern meist mit gepflegt, müssen aber unter der GNU General Public License (GPL) veröffentlicht sein, was einige Hardware-Hersteller ablehnen. Extern zur Verfügung gestellte Treiber müssen aber ebenfalls ständig angepasst und in neuen Versionen veröffentlicht werden, was einen enormen Entwicklungsaufwand mit sich bringt. Außerdem ist die rechtliche Lage solcher externen Module, die nicht unter der GPL stehen, umstritten, weil sie in kompilierter Form technisch bedingt GPL-lizenzierte Bestandteile des Kernels enthalten müssen.

Das Problem der Hardwareunterstützung durch sogenannte Binärtreiber (Gewähren von Binärdateien ohne Offenlegung des Quellcodes) wird im Linux-Umfeld kontrovers diskutiert: Während manche für einen Ausschluss proprietärer Kernel-Module plädieren, befürworten andere, dass einige Hersteller überhaupt – zur Not auch proprietäre – Treiber bereitstellen, mit dem Argument, dass die Linux-Nutzer ohne sie benachteiligt wären, weil sie sonst von bestimmter Hardware schlicht abgeschnitten wären.

Allerdings können Treiber für viele Geräteklassen (z. B. alle per USB oder Netzwerk angeschlossenen Geräte) auch ganz ohne Kernelcode programmiert werden, was sogar die bevorzugte Vorgehensweise ist.

Linus Torvalds betont, dass sich Linux und digitale Rechteverwaltung (DRM) nicht ausschließen. Auch sind freie DRM-Verfahren zur Nutzung unter Linux verfügbar.

In der Praxis ist die Nutzung von DRM-geschützten Medien unter Linux jedoch seltener möglich als unter anderen Systemen, denn aufgrund des Prinzips digitaler Rechteverwaltung können Rechteinhaber alleine entscheiden, auf welchen DRM-Systemen ihre Medien verwendet werden dürfen. Die dabei eingesetzten Verfahren sind nicht standardisiert, sondern werden von den jeweiligen Herstellern kontrolliert, und die beiden größten Hersteller digitaler Rechteverwaltungssysteme im Endverbraucherumfeld, Microsoft und Apple, haben mit Stand Oktober 2009 keine entsprechenden Programme für Linux veröffentlicht oder auch nur entsprechende Absichten bekannt gegeben.

Allerdings gibt es Windows-DRM-zertifizierte Software, die unter Linux eingesetzt werden kann, wie sie beispielsweise bei der AVM FRITZ!Media 8020 verwendet wird.

Grundsätzlich besteht bei DRM-Verfahren die Notwendigkeit, dass die Daten, an denen der Nutzer nur eingeschränkte Rechte erhalten soll, dem Nutzer zu keiner Zeit in unverschlüsselter Form zur Verfügung gestellt werden dürfen, da er ja sonst in diesem Moment eine unverschlüsselte Kopie anfertigen könnte. Da Linux quelloffen ist, ist es dem Nutzer leicht möglich, den entsprechenden Programmteil eines lokalen, rein softwarebasierten DRM-Systems durch eigenen Code zu ersetzen, der genau dies tut.

Bis 2014 war der LinuxTag die größte jährlich stattfindende Messe zu den Themen Linux und freie Software in Europa. Neben den Ausstellungen aller namhaften Unternehmen und Projekte aus dem Linux-Umfeld wurde den Besuchern auch ein Vortragsprogramm zu verschiedenen Themen geboten. Der LinuxTag selbst existierte von 1996 bis 2014 und zog zuletzt jährlich mehr als 10.000 Besucher an. Neben dem großen LinuxTag gibt es noch eine Vielzahl kleinerer und regionaler Linuxtage, die oft mit Unterstützung von Universitäten organisiert werden. Seit 2015 sind die Chemnitzer Linux-Tage die größte Veranstaltung dieser Art in Deutschland.

Zu den weiteren internationalen Messen gehört der "Linux Kongress – Linux System Technology Conference" in Hamburg. Ein Kuriosum ist die jährlich stattfindende "LinuxBierWanderung", die Linux-Enthusiasten der ganzen Welt eine Möglichkeit zum gemeinsamen „Feiern, Wandern und Biertrinken“ geben will.

Neben den allgemeinen Messen und Kongressen findet jedes Jahr das LUG-Camp statt. Dieses wird seit dem Jahr 2000 von Linux-Benutzern aus dem Raum Flensburg bis hin zur Schweiz organisiert und besucht.

Mit der zunehmenden Verbreitung von Linux hat sich auch ein Angebot an Printmedien entwickelt, die sich mit der Thematik beschäftigen. Neben einer Vielzahl an Büchern zu nahezu allen Aspekten von Linux haben sich auch regelmäßig erscheinende Zeitschriften auf dem Markt etabliert. Bekannteste Vertreter sind hier die einzelnen Hefte der Computec Media Group, die monatlich "(Linux-Magazin, LinuxUser)" oder vierteljährlich "(EasyLinux)" erscheinen. Schon seit einer ganzen Weile produzieren auch andere große Verlage wie IDG mit der zweimonatlich erscheinenden "LinuxWelt" sowie Heise mit der in unregelmäßiger Abfolge erscheinenden "c't Linux" Heftreihen beziehungsweise Sonderhefte zu langjährig bestehenden Computerzeitschriften, nämlich "PCWelt" und "c’t". Darüber hinaus gibt es auch noch für die Distribution "Ubuntu Linux" und ihre Derivate das jährlich viermal erscheinende Magazin "UbuntuUser", das durch den Medienanbieter Computec Media veröffentlicht wird.

Der am 12. Oktober 1994 entdeckte Asteroid (9885) Linux wurde nach dem Linux-Kernel benannt.

Die Thematik rund um Linux wurde auch in einer Reihe von Dokumentationen behandelt. So behandelt der Kino-Dokumentationsfilm "Revolution OS" die Geschichte von Linux, freier Software und Open Source und stützt sich dabei größtenteils auf diverse Interviews mit bekannten Vertretern der Szene. Die TV-Dokumentation "", in Deutschland von Arte ausgestrahlt, geht ähnliche Wege, stellt aber auch einen chronologischen Verlauf der Entwicklung von Linux und Unix dar.





</doc>
<doc id="2954" url="https://de.wikipedia.org/wiki?curid=2954" title="Liedermacher">
Liedermacher

Der Begriff Liedermacher bezeichnet im deutschsprachigen Raum einen Sänger, der Musik und Texte seines Programms überwiegend selbst geschrieben oder originär bearbeitet hat.

Der Vortrag eines Liedermachers basiert im Kern auf eigener Interpretation und musikalischer Begleitung. Auch wenn die Aufführung gelegentlich mit einer Begleitband erfolgt, liegt meist großes Gewicht auf dem anspruchsvollen, oft witzig-kritischen Text.

Der Singer-Songwriter (englisch) im Verständnis Bob Dylans ist vom Begriff des Liedermachers vor allem regional (nordamerikanischer Raum) und stilistisch abzugrenzen. Es gibt darüber hinaus neben dem deutschen Liedermacher den "Chansonnier" (französisch), den "Cantautore" (italienisch) bzw. "Cantautor" (spanisch) sowie den "Bard" (russisch). Die Inhalte haben in der Regel Bezug zur Erfahrungswelt des Liedermachers und sind persönlich oder auch politisch geprägt.

Berühmte "Chansonniers" sind unter anderem Édith Piaf, Juliette Gréco, Charles Aznavour, Jacques Brel, Jacques Dutronc, Maurice Chevalier und Serge Gainsbourg. Zu den bekannten "Cantautores" gehören León Gieco (Argentinien), Pablo Milanés (Kuba), Violeta Parra, Víctor Jara (beide Chile), Joaquín Sabina (Spanien) sowie Daniel Viglietti und Alfredo Zitarrosa (Uruguay). Die wohl bedeutendsten "Bardy" aus Russland bzw. der Sowjetunion waren Wladimir Wyssozki und Bulat Okudschawa.

Das Genre des Liedermachers enthält wegen der individuellen Ausdrucksform eine Vielzahl unterschiedlicher musikalischer und textlicher Stil-Ausprägungen:


Typisch für Liedermacher ist die gleichzeitige Zugehörigkeit zu mehreren dieser Kategorien. So singt Reinhard Mey auch humorvolle sowie gesellschaftskritische Lieder.

Weitere bekannte Liedermacher finden sich in der .

Seit den 1990er Jahren hat sich im deutschsprachigen Raum eine neue Form, das so genannte „Liedermaching“, entwickelt. Die Vertreter dieser Gattung setzen sich textlich sowie musikalisch deutlich vom klassischen Liedermacher ab. Als Begründer der Liedermaching-Szene gilt unter anderem das Bonner Liedermacherduo Joint Venture (1993–2000, bestehend aus Götz Widmann und Martin „Kleinti“ Simon).

Weitere Vertreter dieses Genres sind die Monsters of Liedermaching (seit 2003).

Waren seit den 1960er Jahren Liedermacher fast ausschließlich dem linksintellektuellen Spektrum zuzuordnen, so gibt es seit einigen Jahren auch rechtsextreme Liedermacher. Nach Angaben von Blick nach Rechts listete das Bundesamt für Verfassungsschutz 2003 in einer internen Studie unter anderem die folgenden „rechtsextremistischen Liedermacher“ auf: Jörg Hähnel, Veit Kelterborn, Annett Moeck, Michael Müller und Frank Rennicke. Einige davon sind wegen Volksverhetzung vorbestraft und inhaftiert, manche ihrer Aufnahmen sind indiziert.




</doc>
<doc id="2955" url="https://de.wikipedia.org/wiki?curid=2955" title="Leukämie">
Leukämie

Leukämie (von altgriechisch "leukós" „weiß“ sowie "haima" „Blut“; wörtlich also „Weißblütigkeit“), im Deutschen auch als (weißer) Blutkrebs bezeichnet, ist eine maligne Erkrankung des blutbildenden oder des lymphatischen Systems und gehört im weiteren Sinne zu den Krebserkrankungen. Ein anderer früher verwendeter Ausdruck dafür ist "Leukose".

Leukämien zeichnen sich durch stark vermehrte Bildung von funktionsuntüchtigen Vorläuferzellen der weißen Blutzellen aus. Diese werden auch Leukämiezellen genannt. Sie breiten sich im Knochenmark aus, verdrängen dort die übliche Blutbildung und treten in der Regel auch stark vermehrt im peripheren Blut auf. Sie können Leber, Milz, Lymphknoten und weitere Organe infiltrieren und dadurch ihre Funktion beeinträchtigen. Die Störung der Blutbildung vermindert die normalen Blutbestandteile. Es entsteht eine Anämie durch Mangel an Sauerstoff transportierenden roten Blutkörperchen, ein Mangel an blutungsstillenden Blutplättchen und ein Mangel an reifen funktionstüchtigen weißen Blutzellen.

Je nach Verlauf unterscheidet man akute und chronische Leukämien (vgl. Krankheitsverlauf). Akute Leukämien sind lebensbedrohliche Erkrankungen, die unbehandelt in wenigen Wochen bis Monaten zum Tode führen. Chronische Leukämien verlaufen meist über mehrere Jahre und sind im Anfangsstadium häufig symptomarm.

 Chronische Leukämie 

"Chronische Leukämien" werden oft zufällig durch eine Routineuntersuchung festgestellt und beginnen meist schleichend. Als erste Anzeichen können allgemeine Krankheitssymptome wie Unwohlsein und Ermüdung, Leistungsminderung, aber auch Fieber, Nachtschweiß und Gewichtsverlust genannt werden. Auftreten können weiterhin Milz- und Lymphknotenschwellungen sowie Juckreiz, Ausschläge und Infektionen.

Es gibt sehr vielfältige Symptome für eine "akute Leukämie". Oft können diese aus völliger Gesundheit heraus entstehen und äußern sich wie ein schweres Krankheitsbild, z. B. treten Blässe, Schwäche, Blutungsneigung mit spontanen blauen Flecken oder nach Bagatelltraumen und Petechien auf. Eine Anfälligkeit für Infektionen mit Fieber sowie geschwollene Lymphknoten, Milz- und Lebervergrößerung und manchmal Knochenschmerzen sind ebenfalls charakteristisch. In vielen Fällen klagen die Patienten auch über gehäuftes Nasenbluten und Gingivitis. Weitere Symptome sind Gewichts- und Appetitverlust, Müdigkeit und Nachtschweiß.

Keines dieser Symptome allein ist charakteristisch für eine chronische bzw. akute Leukämie.

Die Klassifikation und Diagnostik der Leukämien basiert auf morphologischen und immunologischen Eigenschaften der Leukämiezellen. In den letzten Jahren gewinnen auch zunehmend zytogenetische und molekularbiologische Merkmale an Bedeutung.

Je nach beteiligtem Zelltyp unterscheidet man zunächst "myeloische" von "lymphatischen" Leukämien. Myeloische Leukämien gehen von den Vorläuferzellen der Granulozyten, im weiteren Sinne auch der Erythrozyten und Thrombozyten aus, lymphatische Leukämien betreffen die Lymphozyten und ihre Vorläuferzellen.

Des Weiteren unterscheidet man anhand des Grades der Unreife der im Knochenmark und im Blut vorkommenden Leukämiezellen zwischen "akuten" und "chronischen" Leukämien. Bei den akuten Leukämien finden sich vor allem Zellen in einem sehr frühen, unreifen Stadium, die nahezu funktionslos sind. Bei den chronischen Leukämien kann man vermehrt Leukämiezellen beobachten, die deutlich weiter entwickelt sind und bereits den reifen Blutzellen ähneln, jedoch noch nicht vollständig funktionstüchtig sind.

Die Verdachtsdiagnose ist häufig bereits aus dem Blutbild und Differentialblutbild zu stellen, die genaue Klassifikation erfordert aber meist eine Knochenmarkpunktion.

Die wichtigsten Leukämieformen sind:


Falls die Leukämie von den "Prolymphozyten" (eine bestimmte Form von Lymphozyten-Vorstufen) ausgeht, spricht man wegen des deutlich aggressiveren Krankheitsverlaufes im Vergleich zur CLL von einer Prolymphozytenleukämie (PLL).

Ebenfalls mit der CLL verwandt ist die Haarzellleukämie (HCL), bei der die Leukämie von sehr weit fortgeschrittenen Lymphozyten-Vorstufen ausgeht. Namensgebend sind die haarförmigen Zytoplasma-Fortsätze der Leukämiezellen.

Die einzelnen Leukämietypen weisen eine typische Altersverteilung auf. Die ALL ist die häufigste Leukämie bei Kindern und kommt bei Erwachsenen seltener vor. Die AML steht bei Kindern an zweiter Stelle und ist bei Erwachsenen die häufigste akute Leukämie mit einem Altersgipfel über 60 Jahren. Die CLL tritt bei Kindern praktisch niemals auf und ist eine typische Leukämieform des älteren Menschen. Die CML ist bei Erwachsenen wesentlich häufiger als bei Kindern.

Leukämien entstehen durch genetische Veränderungen von unreifen blutbildenden Vorläuferzellen, sodass diese sich einerseits nicht mehr vollständig zu funktionstüchtigen Blutzellen weiterentwickeln können und andererseits unkontrolliert vermehren "(siehe auch Krebsentstehung)". Es genügt bereits die Veränderung einer einzigen Vorläuferzelle, die durch das anschließende unkontrollierte Wachstum die gesunden Bestandteile des blutbildenden Systems zurückdrängen kann.

Die Auslöser dieser genetischen Veränderungen sind noch nicht geklärt. Gerade bei akuten Formen sind die Ursachen meist unklar und können nicht in einen kausalen Zusammenhang mit pathogenen Faktoren gebracht werden. Diskutiert werden die nachfolgenden potentiell auslösenden Faktoren:


In Deutschland erkranken jährlich 1800 Kinder neu an Krebs, davon rund ein Drittel an Leukämie. Auch hier sind die Ursachen weitgehend unbekannt.

Eine Fallkontrollstudie zeigte ein etwa zwanzigfach erhöhtes Leukämierisiko für Patienten mit einem Down-Syndrom im Vergleich zur Normalbevölkerung.

Es gibt Hinweise, dass Umweltfaktoren (ionisierende sowie nichtionisierende Strahlung sowie Pestizide) potenzielle Risikofaktoren sein können und ein „gut trainiertes kindliches Immunsystem“ einen schützenden Effekt hat. Bei einer Untersuchung von Kindern der Stadt Basra im Südirak wurde ein Anstieg der Leukämierate um rund das Doppelte von 1993 bis 2007 festgestellt. Als mögliche Auslöser kommen unter anderem Benzol, das durch brennende Ölfelder und improvisierte Tankstellen (Kanisterbetankung) in die Umwelt gelangte oder auch Geschosse aus abgereichertem Uran in Frage.

Die vermutete Ursache Radioaktivität für den Leukämiecluster Elbmarsch bei Hamburg ist umstritten. Ungeklärt ist auch der Einfluss radioaktiver Emissionen auf die temporäre Leukämiehäufung um Jülich.

Grundlage der Behandlung von Leukämien ist die Therapie mit Zytostatika. Weitere Behandlungsprinzipien sind die Hochdosistherapie mit autologer Stammzelltransplantation und die allogene Knochenmark- bzw. Stammzelltransplantation. Dazu wird ähnlich wie bei einer Bluttransfusion ein passender Knochenmarkspender benötigt. Untergeordnete Bedeutung hat die prophylaktische oder therapeutische Strahlentherapie. In den letzten Jahren haben sich neue Therapiemöglichkeiten durch die Anwendung von monoklonalen Antikörpern und neue spezifisch in die Krankheitsprozesse eingreifende Medikamente wie Imatinib und Dasatinib (zwei Tyrosinkinase-Inhibitoren) bei der CML und der Philadelphia-Chromosom-positiven ALL oder ATRA bei der Promyelozyten-Leukämie eröffnet. In der Therapie der Leukämien bestehen zwischen den einzelnen Formen erhebliche Unterschiede, die Einzelheiten der Therapie sind in den entsprechenden Artikeln dargestellt.

In den letzten Jahren gab es auch immer weitere Fortschritte der Gentherapie, die nun auf langfristige Therapieerfolge hoffen lassen. Einige Forschungsgruppen arbeiten zum Beispiel daran, T-Zellen von Leukämiepatienten durch Einschleusen bestimmter Gene so zu manipulieren, dass sie auch noch Jahre später Krebszellen eliminieren. Einige Patienten mit ALL oder CLL blieben durch diese Therapie langzeitig in Remission.

Am 1. November 2013 erteilten die USA und am 29. Juli 2014 die europäischen Behörden einem bei Roche entwickelten Wirkstoff die Zulassung. Bei Patienten mit der chronischen lymphatischen Leukämie (CLL) ist schon nach wenigen Therapietagen mit dem Wirkstoff Obinutuzumab (alter Wirkstoffname war Afutuzumab) in Kombination mit dem milden Chemotherapeutikum Chlorambucil ein rapider Rückgang der Blutkrebszellen feststellbar. Nach Abschluss der Therapie waren bei über 20 % der Patienten keine Krankheitszeichen mehr vorhanden.

Im Unterschied zu anderen Krebsarten, die bereits Galenus im Altertum beschrieben hatte, wurde der Blutkrebs erst im 19. Jahrhundert erkannt und untersucht. Die starke Vermehrung weißer Blutzellen beschrieb erstmals 1845 der schottische Arzt John H. Bennett. Er bezeichnete das Phänomen als vereitertes Blut und vermutete eine Infektion als Ursache. Etwa zur selben Zeit beobachtete Rudolf Virchow bei einer Patientin ebenfalls stark vermehrte weiße Blutzellen, diagnostizierte „weißes Blut“ und führte 1847 das medizinische Fachwort „Leukämie“ ein. Genauer geschildert hatte Alfred Armand Velpeau 1827 einen Fall von Leukämie. Der erste Fall von akuter lymphatischer Leukämie bei einem Kind wurde 1860 von Michael Anton Biermer, einem Schüler Virchows, beschrieben. Ende des 19. Jahrhunderts bezeichneten Pathologen die Leukämie als Neoplasie der weißen Blutzellen; danach konnte man mehrere Erscheinungsformen der Leukämie unterscheiden. Eine Chemotherapie mit Aminopterin gelang 1947 erstmals Sidney Farber, einem pädiatrischen Pathologen. Allerdings hielten die erzielten Remissionen nicht lange an. In den folgenden Jahren wurden über das US-amerikanische National Cancer Institute (NCI) in Studien an Kindern mit akuter Leukämie (ALL) Kombinationstherapien untersucht, insbesondere in den 1960er Jahren VAMP (Vincristin, Amethopterin, Mercaptopurin, Prednison). Zunächst zeichneten sich länger dauernde Remissionen ab, doch dann traten in der Mehrzahl schwerwiegende Rückfälle auf unter Befall des zentralen Nervensystems. Im weiteren Verlauf kombinierte man die VAMP-Therapie mit einer Strahlentherapie. Eine erste Auswertung an 278 Patienten im Jahr 1979 zeigte, dass diese Kombination, als „totale Therapie“ verstanden, zu deutlich länger andauernden Remissionen führte. Hiermit wurde der erste erfolgversprechende Durchbruch erzielt.

1995 wurden die Deutsche José Carreras Leukämie-Stiftung sowie die Deutsche Leukämie- und Lymphom-Hilfe gegründet, die Stiftung Deutsche Leukämie- & Lymphom-Hilfe etablierte sich 2010.

Kinder mit Down-Syndrom (Trisomie 21) haben ein zwanzigfach gesteigertes Risiko, an einer akuten Leukämie zu erkranken. Bei Neugeborenen mit Trisomie 21 tritt bei fünf bis zehn Prozent eine sogenannte transiente Leukämie (TL) auf, die alle Eigenschaften einer akuten megakaryoblastären Leukämie aufweist (akute myeloische Leukämie, megakaryoblastischer Subtyp / AMkL), sich jedoch in den allermeisten Fällen innerhalb der ersten Lebenswoche spontan zurückbildet. Bei etwa 20 % dieser Kinder tritt allerdings in den ersten vier Lebensjahren erneut eine AMkL (auch myeloische Leukämie bei Down-Syndrom (ML-DS), siehe WHO-Klassifikation) auf, die den identischen Phänotyp aufweist. Sowohl bei der TL als auch der ML-DS konnten Mutationen des hämatopoetischen Transkriptionsfaktors GATA1 als ursächlich nachgewiesen werden. Die ML-DS wird mit einer intensiven, angepassten Chemotherapie behandelt. Aufgrund der erhöhten Sensitivität gegenüber Chemotherapie sind die Heilungschancen für die ML-DS mit mehr als 85 % deutlich höher als bei der AML bei Kindern ohne Trisomie 21. Auch die akute lymphoblastäre Leukämie (ALL) kommt bei Kindern mit Down-Syndrom häufiger vor, ist hier aber aufgrund der ungünstigeren Risikofaktoren und der höheren Empfindlichkeit für Nebenwirkungen der Therapie mit einer schlechteren Prognose verbunden.

Abgesehen von dem erhöhten Leukämie-Risiko sind Menschen mit Down-Syndrom unterdurchschnittlich anfällig für andere Formen von Krebserkrankungen. In sechs unabhängig voneinander durchgeführten Studien konnte erwiesen werden, dass z. B. Neuroblastome, Nephroblastome, Unterleibskrebs, Brustkrebs, Magenkrebs und Darmkrebs sehr selten auftreten: „Verglichen nach Alter und Geschlecht ist die Wahrscheinlichkeit für eine Person mit Down-Syndrom, an irgendeiner Form von Gewebekrebs zu sterben, um 50 bis 100 Mal niedriger“ als üblich. Zurückgeführt werden kann dies neben dem durch das zusätzliche Erbmaterial offenbar begünstigten Schutzmechanismus des Körpers auch darauf, dass die mit der Trisomie 21 zusammenhängende Disposition für insbesondere Leukämie bekannt ist und eine Erkrankung aufgrund häufigerer Arztbesuche (z. B. wegen der Anfälligkeit für Atemwegserkrankungen) oft in sehr frühen Stadien erkannt und behandelt werden kann. Darüber hinaus leben die meisten Menschen mit Down-Syndrom deutlich gesünder, insbesondere Alkohol und Nikotin werden selten aktiv konsumiert, was das Risiko, an Krebs zu erkranken, zusätzlich senkt.

Reziproke Translokationen sind für Leukämien und Lymphome typisch, bei soliden Tumoren die Ausnahme. Generell betrachtet sind Translokationen ein Charakteristikum von ca. drei Prozent aller Tumoren. Bei insgesamt 14.000 verschiedenen karyotypischen Veränderungen bei Tumoren sind über 100 wiederkehrende Translokationen beschrieben worden (Stand 1991). Chromosomale Veränderungen bei hämatologischen Erkrankungen sind häufig und vielfältig. Ein tabellarischer Überblick soll einen Eindruck von der Vielfalt der Phänomene geben. Dabei werden zunächst Chromosomen-Translokationen und sodann Chromosomen-Deletionen angeführt. Der weitere Artikel gliedert sich in drei Teile. Zunächst werden die chromosomalen Veränderungen bei myeloischen Leukämien besprochen. Im darauffolgenden Abschnitt werden die lymphatischen Leukämien dargestellt und eine beispielhafte Bruchpunkt-Untersuchung vorgestellt. Zum Abschluss wird noch kurz auf das Burkitt-Lymphom eingegangen.

Die folgende Tabelle gibt einen Überblick über die chromosomalen Deletionen bei verschiedenen menschlichen Leukämien.

Bei der chronischen myeloischen Leukämie kommt es in 95 Prozent aller bisher untersuchten Fälle durch eine chromosomale Translokation zu einer Fusion des c-abl-Gens auf dem Chromosom 9q34 mit dem bcr-Gen auf dem Chromosom 22q11 mit dem Ergebnis eines alterierten Chromosoms, dem Philadelphia-Chromosom, und der Expression eines chimärischen Proteins, dem abl/bcr-Produkt, das in drei Varianten als p190, p210 und p230 vorkommt und Tyrosinkinaseaktivität aufweist. Das Fusionsprotein resultiert in einer konstitutiven Aktivierung der abl-Tyrosinkinase und stimuliert vielfältige Signalwege, z. B. p21 Ras, PI3 Kinase, Jun, myc.

Bei den akuten myeloischen Leukämien findet sich eine Vielzahl unterschiedlicher Mutationen. Bei der AML finden sich in bis zu 50 Prozent der untersuchten Fälle Mutationen im "N-ras"-Lokus, in ca. fünf Prozent der untersuchten Fälle Mutationen in p53, in weniger als drei Prozent der untersuchten Fälle Mutationen im "RB-1"-Gen und in ca. 20 Prozent Veränderungen im "WT-1"-Lokus. Vereinzelt sind Fusionen von "SET/CAN"-, "DEK/CAN"-, "MLL"- und "AML-1"-Genen beschrieben worden. Im Folgenden werden die beteiligten Onkogene näher charakterisiert.

Bei der akuten myelomonocytischen Leukämie (AMML) finden sich häufig Mutationen im RB-1 Lokus. Eine Besonderheit bei den AML stellt die Promyelozytenleukämie dar, bei der in mehr als 95 Prozent der untersuchten Fälle eine Translokation t(15;17) (q21;q21) beschrieben ist mit dem Ergebnis einer Fusion von PML und RARa. Das Humane Trithorax-Homolog findet sich auf dem Chromosom 11q23. Die HRX-Translokationen findet sich bei biphänotypischen Leukämien. Trithorax ist ALL-1.

Die folgende Tabelle gibt eine Übersicht über das Vorkommen von
Translokationen bei akuten T-Zell-Leukämien.

Alle betroffenen protoonkogenen Transkriptionsfaktoren (c-myc, Lyl-1, Tal-1,2) sind helix-loop-helix-Proteine; Rhom-1,2 (Ttg-1,2) sind LIM-Domaine-Proteine, Hox-11 (Tcl-3) ist ein Homeoboxgen und Tan-1 ein notch-Homolog. Involviert sind jeweils immer TCR-beta oder TCR-alpha/delta. Vergleichsweise konsistente Mutationen in T-ALLs finden sich auch bei p53 Jonveaux und im RB-Lokus Ahuja und Ginsberg allerdings ohne Translokationen in den Bereich rearrangierender Loci. Untersucht man die verschiedenen Loci, so findet man folgende Verteilung der translozierenden Regionen.

In die TCR-alpha/delta-Region = 14q11 translozieren:

In die TCR-beta-Region = 7q35 translozieren:

Die aufgelisteten Translokationen bei T-ALL haben eine Reihe von Gemeinsamkeiten. Es sind jeweils zwei typische codierende Regionen betroffen: TCR-Gene und Transkriptionsfaktoren. Stets ist das betroffene Allel des TCR als Strukturgen zerstört und das betroffene Allel des Transkriptionsfaktors als Strukturgen intakt, in seiner Regulation aber gestört. Meistens sind die betroffenen Transkriptionsfaktoren zelllinienfremde Gene. Üblicherweise wird ihre Funktion im Rahmen der Zelldifferenzierung vermutet. Im Bereich von 11p13 sind die Bruchpunkte unabhängig vom translozierenden Partnerchromosom in einem kleinen Bereich geclustert. Außerdem finden die Translokationen bei unreifen Zellen statt, so dass man schlussfolgern muss, dass eine aberrante Expression von an der Zelldifferenzierung von nichtlymphatischem Gewebe beteiligten Transkriptionsfaktoren in primitivem lymphoiden Gewebe einen wesentlichen Anteil an der malignen Transformation haben kann.






</doc>
<doc id="2956" url="https://de.wikipedia.org/wiki?curid=2956" title="Lebensmittel">
Lebensmittel

Lebensmittel sind Substanzen, die konsumiert werden, um den menschlichen Körper zu ernähren. Der Begriff "Lebensmittel" (früher im Süddeutschen auch: Viktualien) umfasst als Oberbegriff sowohl das Trinkwasser als auch die Nahrungsmittel. Trinkwasser besteht aus Wasser und darin gelösten Mineralstoffen. Im Unterschied zu Trinkwasser bestehen Nahrungsmittel im Wesentlichen aus den Makronährstoffen – dies sind die Kohlenhydrate, die Lipide (Fette) und die Proteine – und führen daher dem Menschen chemisch gebundene Energie zu. Zusätzlich sind Mikronährstoffe als Mengen- und Spurenelemente wesentliche Bestandteile von Nahrungsmitteln. Lebensmittel werden vom Menschen zum Zwecke der Ernährung oder des Genusses über den Mund, gegebenenfalls nach weiterer Zubereitung, aufgenommen.

Aus rechtlicher Sicht zählen neben Trinkwasser und Nahrungsmitteln als Hauptgruppen auch die Genussmittel zu den Lebensmitteln, wobei Tabakwaren ausgenommen sind.

Eine lebensmittelrechtliche Definition liefert die (Lebensmittelbasisverordnung) zum Lebensmittelrecht:
Nach EU-Verordnung 178/2002/EG gehören wegen fehlender Verarbeitung oder fehlenden Nährwerts "nicht" zu den Lebensmitteln:

Während das deutsche Lebensmittelrecht nur den Begriff „Lebensmittel“ kennt, wird im Kontext der so genannten Vollwerternährung auf spezielle Weise zwischen Lebens- und Nahrungsmitteln unterschieden. Dabei werden als Lebensmittel nur solche Nahrungsmittel bezeichnet, die nicht konserviert und insbesondere nicht über 43 °C erhitzt wurden. Die Begründung ist, dass durch das Erhitzen wichtige Nahrungsbestandteile (wie Vitamine) zerstört werden können. Das "Leben"smittel „lebt“ dann nach der Einschätzung der Vollwerternährung nicht mehr und wird deshalb mit „Nahrungsmittel“ als geringerwertig eingestuft.

Der Nährwert ist der zentrale Nutzen von Lebensmitteln. Er ist ein Maß, um den physiologischen Brennwert eines Lebensmittels zu qualifizieren und quantifizieren. Meist fasst man unter dem Begriff Nährwert nur den Brennwert, also die dem Körper zur Verfügung gestellte Energie, zusammen.

Nahrungsinhaltsstoffe, die dem Körper Energie und zum Teil nach erfolgtem Umbau im Körper auch Bausteine für Wachstum und Körpererneuerung liefern. Zu diesen Grundnährstoffen gehören Proteine, Fette und Kohlenhydrate. Diese Komponenten der einzelnen Lebensmittel liefern dem Körper in erster Linie Energie. Sie werden deshalb auch als Brennstoffe bezeichnet.


Ebenfalls zu den nicht-energieliefernden Nährstoffen zählt man die Wirkstoffe. Sie sind meist essentiell.
Vitamine und Spurenelemente wirken oft als Coenzyme.


Neben dem Nährwert spielt der Genusswert der Lebensmittel eine wesentliche Rolle. Basis für den Genuss sind neben sensorischen Wahrnehmungen auch kulturelle Faktoren.

Lebensmittel lassen sich je nach Standpunkt und Zweck der Einteilung gliedern nach Inhaltsstoffen, Herkunft, Verarbeitungsprozessen, Verzehranlass, Kühlungsbedarf. Eine häufig anzutreffende Gliederungsart teilt die Lebensmittel nach dem Ursprung der Rohwaren in tierische und pflanzliche sowie sonstige Produkte.






Als Basis für Ernährungsstudien benutzt die Bundesforschungsanstalt für Ernährung und Lebensmittel einen so genannten Bundeslebensmittelschlüssel als Lebensmittelnährwertdatenbank.

Gegen eine Reihe von Lebensmitteln kann der Mensch angeboren oder im Laufe seines Lebens Unverträglichkeiten entwickeln. Gluten, Fructose, Laktose, Milch­eiweiß, Solanin, Eiweiß (Ei) und viele andere Stoffe können von akuten allergischen Reaktionen bis hin zu subakuten Autoimmunerkrankungen eine Reihe von Symptomen provozieren. Durch Wechselwirkung (Kreuzreaktion) potenziert sich möglicherweise die Schädlichkeit der Substanzen.

Das Lebensmittelrecht regelt durch zahlreiche nationale und europäische Gesetze und Verordnungen die Herstellung, die Kennzeichnung und den Verkauf von Lebensmitteln in Deutschland und Europa. Es wird angestrebt, zur Beseitigung nationaler Unterschiede und zur Erleichterung des Handels das Lebensmittelrecht EU-weit zu harmonisieren. Die wichtigsten Regelwerke sind:


Die Einhaltung der Rechtsvorschriften für das Herstellen, Behandeln und Inverkehrbringen von Lebensmitteln in den Lebensmittelunternehmen wird durch die amtlichen Lebensmittelüberwachung kontrolliert.

Mit der Produktion, der Verarbeitung und dem Handel von Lebensmitteln befasst sich der Wirtschaftszweig der Lebensmittelwirtschaft. Hierzu zählt die sogenannte Lebensmittelkette "vom Acker bis zum Teller", das heißt Landwirtschaft, Lebensmittelindustrie, Lebensmittelhandwerk, Lebensmittelgroß- und Lebensmitteleinzelhandel sowie der Außer-Haus-Markt (Gastronomie) und angrenzende Bereiche.

Die Menge an Lebensmittelabfällen, die jährlich in den 28 Mitgliedstaaten der EU anfällt, wird in einer von der EU-Kommission veröffentlichten Untersuchung auf ca. 89 Millionen Tonnen, bis zu 50 % entlang der Lebensmittelversorgungskette, geschätzt. Dies entspricht 179 kg pro Kopf, mit großen Unterschieden zwischen den einzelnen EU-Ländern und den verschiedenen Branchen. Dabei ist die Verschwendung bei der landwirtschaftlichen Erzeugung oder der Rückwurf von Fängen ins Meer noch nicht eingerechnet.
Für Deutschland wurden 81,6 kg/a Lebensmittelabfälle pro Person in Privathaushalten ermittelt. Nach der Studie vom März 2012 der Universität Stuttgart wäre davon 45 % vermeidbar und 18 % teilweise vermeidbar gewesen.

Im Jahre 2012 hat Frankreich beschlossen, die Lebensmittelverschwendung bis 2025 zu halbieren.
Ab Anfang 2016 dürfen Supermärkte keine Lebensmittel mehr wegwerfen. Supermärkte ab 400 m² Verkaufsfläche müssen darüber hinaus ein Abkommen mit einer karitativen Organisation für Lebensmittelspenden abschließen. Italien will sich dieser Regelung anschließen.

Nach einer Studie aus dem Jahr 2012 landen in Deutschland elf Millionen Tonnen Nahrung von Verbrauchern, Handel, Industrie und Gastronomie im Müll. Davon stammen 550.000 Tonnen aus dem Handel. Auf private Haushalte entfallen 6,7 Millionen Tonnen.
Anfang 2016 berichtete die Frankfurter Allgemeine Sonntagszeitung, dass ca. 200.000 Tonnen Lebensmittel über die Tafeln verteilt werden.

Der WWF schätzt, dass von den insgesamt 18 Mio. Tonnen an Lebensmitteln, die pro Jahr verloren gehen, alleine 1,4 Mio Tonnen an vermeidbaren Verlusten auf Speisekartoffeln entfallen. Bei einer Jahresernte von 4,3 Mio Tonnen geht damit ein Drittel der Ernte verloren.






</doc>
<doc id="2958" url="https://de.wikipedia.org/wiki?curid=2958" title="Ländervorwahlliste sortiert nach Nummern">
Ländervorwahlliste sortiert nach Nummern

Liste der internationalen Vorwahlnummern im Telefonnetz
sortiert nach Nummern

– nicht vergeben

– Teile Nordamerikas



– Afrika, Atlantikinseln und Inseln im Indischen Ozean










Projekt der Europäischen Kommission vom 20. November 1996: Europäischer Nummerierungsplan (bislang noch nicht umgesetzt).













– Mexiko, Zentralamerika und Südamerika 




– Südpazifik und Ozeanien





– Kasachstan, Russland

x = 0–9

– Ostasien und Sondernummern










– West-, Zentral- und Süd-Asien, Naher Osten








</doc>
<doc id="2959" url="https://de.wikipedia.org/wiki?curid=2959" title="Ländervorwahlliste sortiert nach Ländern">
Ländervorwahlliste sortiert nach Ländern

Liste der internationalen Vorwahlnummern im Telefonnetz
geordnet nach Ländernamen.



</doc>
<doc id="2960" url="https://de.wikipedia.org/wiki?curid=2960" title="LSD">
LSD

Lysergsäurediethylamid, kurz auch LSD, ist ein chemisch hergestelltes Derivat der Lysergsäure, die als Mutterkornalkaloid natürlich vorkommt. LSD ist eines der stärksten bekannten Halluzinogene. Es ruft schon in sehr geringen Dosen lang andauernde pseudohalluzinogene Wirkungen hervor. Pharmakologisch gehört LSD zur Gruppe der serotoninverwandten psychedelischen Substanzen.

Im Jargon wird LSD auch Acid (englisch „Säure“) genannt. Sowohl das Betäubungsmittelgesetz in Deutschland als auch das Suchtmittelgesetz in Österreich stufen LSD als nicht verkehrsfähig ein.

Der Chemiker Albert Hofmann stellte am 16. November 1938 erstmals im Rahmen seiner Forschung zum Mutterkorn Lysergsäurediethylamid her. Sein Ziel war die Entwicklung eines Kreislaufstimulans. Nachdem diese erhoffte Wirkung von LSD im Tierversuch nicht eintrat, verlor Hofmann zunächst das Interesse und archivierte seine Forschungsergebnisse. Am 16. April 1943 entschied sich Hofmann, mögliche Wirkungen von LSD erneut zu prüfen; er vermutete, bei den ersten Versuchen etwas übersehen zu haben. Bei seinen Arbeiten mit LSD bemerkte Hofmann an sich selbst eine halluzinogene Wirkung, die er zunächst nicht erklären konnte. So vermutete er, LSD sei durch unsauberes Arbeiten durch die Haut von seinem Körper aufgenommen worden.

Er wiederholte dieses Erlebnis am 19. April 1943 durch die Einnahme von 250 Mikrogramm LSD. Verglichen mit der Wirksamkeit der damals bekannten Mutterkornalkaloide entsprach das der kleinsten Menge, bei der man noch eine Wirkung hätte erwarten können. Es stellte sich jedoch heraus, dass diese Menge bereits dem Zehnfachen der normalerweise wirksamen Dosis (ab ca. 20 µg) von Lysergsäurediethylamid entsprach. Dieses Datum gilt heute als Zeitpunkt der Entdeckung der psychoaktiven Eigenschaften des LSD. Der Jahrestag wird von popkulturellen LSD-Anhängern als „Fahrradtag“ (Bicycle Day) gefeiert, da Hofmann am Beginn seines bewusst induzierten Rauscherlebnisses mit dem Fahrrad nach Hause fuhr.

Die Firma Sandoz, in deren Auftrag Hofmann forschte, brachte 1949 das Präparat unter dem Namen „Delysid“ in den Handel. Es wurde als Psychotomimetikum angeboten, das es Psychiatrie-Ärzten ermögliche, sich für eine begrenzte Zeit in die Wahrnehmungswelt psychotischer Patienten zu versetzen.

Chemisch gehört Lysergsäurediethylamid zur Strukturklasse der Ergoline. Die Bezeichnung „LSD-25“ rührt daher, dass es die 25. Substanz in Hofmanns Versuchsreihe der synthetischen Lysergsäure-Abkömmlinge war.

LSD ist eine chirale Verbindung mit zwei Stereozentren an den Kohlenstoffatomen C-5 und C-8. Somit existieren vier verschiedene Stereoisomere des LSDs, die zwei Enantiomerenpaare bilden.
LSD, genauer (+)-LSD, besitzt die absolute Konfiguration (5"R",8"R"). (−)-LSD ist (5"S",8"S")-konfiguriert und ist spiegelbildlich zu (+)-LSD. (+)-LSD epimerisiert unter basischen Bedingungen zu dem Isomer (+)-"iso"-LSD mit (5"R",8"S")-Konfiguration; (−)-LSD epimerisiert basisch zu (−)-"iso"-LSD mit (5"S",8"R")-Konfiguration. Das nicht psychoaktive (+)-"iso"-LSD, das sich während der Synthese (je nach Methode in unterschiedlichem Anteil) bildet, kann mit Hilfe chromatographischer Trennmethoden abgetrennt werden und (etwa durch Wirkung verdünnter methanolischer Kaliumhydroxidlösung) zu aktivem (+)-LSD isomerisiert werden. Von LSD sind mehrere strukturelle Analoga bekannt, die als Leitstruktur das LSD bzw. den Ergolingrundkörper besitzen, z. B. ALD-52, 1P-LSD, AL-LAD, ETH-LAD, 1P-ETH-LAD und PRO-LAD. Dabei wurden Modifikationen in Position 1 und in Position 6 am Ergolinsystem vorgenommen.<ref name="DOI10.1007/BF02245940">V. J. Watts, R. B. Mailman, C. P. Lawler, K. A. Neve, D. E. Nichols: "LSD and structural analogs: pharmacological evaluation at D1 dopamine receptors." In: "Psychopharmacology." 118, 1995, S. 401–409, .</ref>

Unter ultraviolettem Licht (360 nm) weist LSD eine stark blaue Fluoreszenz auf. Mit Dimethylaminobenzaldehyd (Ehrlich-Reagenz, Kovacs-Reagenz) ist ein weiterer Nachweis möglich. Der forensisch sichere Nachweis von LSD in den verschiedenen Untersuchungsmaterialien wie z. B. Haaren oder Urin gelingt nach adäquater Probenvorbereitung durch die Kopplung chromatographischer Verfahren mit der Massenspektrometrie. Da LSD so wirkungsstark ist, besteht kein Bedarf, die Substanz zu verunreinigen. In Labors liegt die Droge selten als Pulver vor, so dass die Reinheit selten gemessen wird.

Es gibt verschiedene Aussagen über die Geschwindigkeit, mit der LSD im Blutplasma abgebaut wird, da die veröffentlichten Messergebnisse voneinander abweichen. Aghajanian und Bing fanden 1964 heraus, dass LSD eine Plasmahalbwertszeit im Körper von 2,9 h besitzt. Papac und Foltz berichteten 1990, dass 1 µg/kg oral-verabreichtes LSD bei einem einzelnen männlichen Freiwilligen eine Plasmahalbwertszeit von 5,1 Stunden hatte. Diese trat mit einer maximalen Konzentration von 5 ng/mL drei Stunden nach Verabreichung auf. 

Untersuchungen von 2017 an 40 gesunden Versuchteilnehmern zeigten, dass bei Dosen von 100 µg und 200 µg maximale Plasmakonzentrationswerte nach 1,4–1,5 h erreicht wurden, wobei die Plasmahalbwertszeit 2,6 h betrug und die subjektiven Effekte 8,2 ± 2,1 h (100 µg) bzw. 11,6 ± 1,7 h (200 µg) andauerten. Die subjektiven Maximaleffekte des LSDs stellten sich bei 2,8 h (100 µg) bzw. 2,5 h (200 µg) nach der oralen Einnahme ein. Die Dauer eines unkomplizierten LSD-Erlebnisses liegt in der Regel zwischen fünf und zwölf Stunden, abhängig von Dosierung, Körpergewicht und Alter. Sandoz’ Beipackzettel von Delysid beschreibt: „[Es] können gelegentlich gewisse Nachwirkungen in Form phasischer Affektstörungen noch während einiger Tage andauern.“

Eines der vier Stereoisomere [(+)-LSD bzw. (5"R",8"R")-LSD] wirkt als Partialagonist mit großer Affinität (Bindungsstärke) am Serotonin-5-HT-Rezeptor. Dieser wird mit dem Wirkungsmechanismus vieler atypischer Neuroleptika in Verbindung gebracht. Auch andere klassische psychedelische Halluzinogene werden von diesem gebunden. Es handelt sich aber um keine selektive Bindung, sondern eine Reihe von weiteren Rezeptorensubtypen der 5-HT-Rezeptoren, der Dopamin-Rezeptoren und der Adrenozeptoren, binden LSD ebenfalls.

Sympathische Wirkungen umfassen eine Beschleunigung der Pulsfrequenz (Tachykardie), Ansteigen des Blutdrucks (Arterielle Hypertonie), Erweiterung der Pupillen (Mydriasis), Verschwimmen der Seheindrücke und Schwierigkeiten bei der Schärfeneinstellung des Auges (Akkommodationsstörung), Absonderung von dickem Speichel, erhöhte Schweißbildung (Hyperhidrosis), Zusammenziehen der peripheren Arterien (Vasokonstriktion), mit der Folge, dass Hände und Füße kalt werden und sich bläulich färben, Aufrichten der Körperhaare (Piloerektion). Die häufigsten parasympathischen Wirkungen sind: Verlangsamung der Pulsfrequenz (Bradykardie), Absinken des Blutdrucks (Hypotonus), übermäßige Speichelbildung (Hypersalivation), Tränenfluss, mögliche Übelkeit und vereinzelt Erbrechen. Mögliche motorische Erscheinungen umfassen: verstärkte Muskelspannung, Zuckungen und Krämpfe, mannigfaltige Formen von Zittern sowie komplizierte Verrenkungsbewegungen.

LSD verändert die Wahrnehmung so, dass sie dem Konsumenten als intensives Erleben erscheint, das Zeitempfinden verändert wird und Umgebungsereignisse deutlicher hervortreten. Dies wird vom Konsumenten als Mehrerleben innerhalb einer kürzeren Zeitspanne empfunden. Hinzu kommen optische, sensorische und akustische Wahrnehmungsveränderungen. Diese müssen nicht unbedingt als Halluzinationen erfahren werden, sondern können auch als Veränderungen gegenüber vergleichbaren Erfahrungen ohne LSD-Wirkung erscheinen. Reale Gegenstände können als plastischer empfunden und wie in Bewegung befindlich erlebt werden. Bei hohen Dosierungen kann das Bewusstsein für den Rausch fehlen und die Kontrolle über die eigenen Handlungen vermindert werden oder ganz ausfallen.

Eine euphorische Grundstimmung – ausgelöst beispielsweise durch eine als schön empfundene Landschaft und Musik – kann den ganzen Rausch über anhalten und den gesamten Verlauf der Erfahrung bestimmen. So können aber bestehende Ängste und Depressionen einen sogenannten „Horrortrip“ hervorrufen, der als äußerst unangenehm und als vom Konsumenten nicht mehr steuerbar empfunden wird. Eine erfahrene und vertraute Person als nüchterne Begleitung („Tripsitter“) kann durch geeignete Maßnahmen solche Erfahrungen verhindern oder abmildern.

LSD wirkt bereits in niedrigen Dosierungen ab 20 µg. Die typische Dosierung liegt bei nichttherapeutischem Gebrauch nach Angaben der Europäischen Beobachtungsstelle für Drogen und Drogensucht (EMCDDA) zwischen 20 und 80 µg. Passie u. a. (2008) gibt bei therapeutischem Gebrauch 75 bis 150 µg als moderate Dosis an; wobei geschätzt wird, dass Dosierungen zwischen 100 und 200 µg das volle Wirkungsspektrum entfalten. Allerdings ist die Wirkung von der Verfassung des Konsumenten sowie von der Umgebung und den damit individuell hervorgerufenen Eindrücken abhängig, sodass nicht allein die Dosierung für die Art des Erlebnisses ausschlaggebend ist. (Siehe Set und Setting.)

LSD bildet eine Toleranz von ein bis zwei Wochen aus. Innerhalb dieser Zeit verliert LSD bei wiederholter Einnahme einen großen Teil seiner Wirkung. Die Toleranzbildung wirkt sich auch auf die Toleranz gegenüber anderen verwandten Substanzen aus. So sind LSD, Psilocybin/Psilocin und Meskalin jeweils zueinander kreuztolerant.

LSD kann bei ungünstigen Voraussetzungen vorübergehende Angstepisoden (Horrortrip) oder eine substanzinduzierte Psychose auslösen. Weitere psychische Störungen wie Missbrauch von Halluzinogenen und die fortbestehende Wahrnehmungsstörung nach Halluzinogengebrauch (HPPD) sind als Diagnosekategorie im DSM-IV aufgenommen.

Bei starker Erregung ist unter anderem medizinische Behandlung indiziert. schlägt hier 20 mg Diazepam peroral vor. Beruhigende Gespräche haben sich als wirksam erwiesen und sind daher als erste Maßnahme angezeigt. Antipsychotika können das Erleben verstärken und sind daher kontraindiziert.

Rund 10.000 Patienten nahmen an der LSD-Forschung der 1950er und 1960er Jahre teil. Die Inzidenz von psychotischen Reaktionen, Selbstmordversuchen und Suiziden während der LSD-Behandlung ist mit der von konventionellen Psychotherapien vergleichbar:

Eine Arbeitsgruppe um den britischen Neuropsychopharmakologen David Nutt befand das Eigenschädigungspotential von LSD im Vergleich zu anderen psychotropen Substanzen als eher gering, während das Fremdschädigungspotential von LSD als nicht vorhanden eingestuft wurde. Die Ergebnisse der Studien wurden 2007 und 2010 im Fachjournal The Lancet veröffentlicht. Eine Nachfolgestudie mit ähnlichen Ergebnissen erschien 2015 im "Journal of psychopharmacology". Das Ranking der Nutt-Studien wurde jedoch im Wissenschaftsjournal "The International journal on drug policy" in Frage gestellt. Das Journal "Addiction (Abingdon, England)" kritisierte die Einstufung von psychotropen Substanzen auf nur eine Dimension (Schädlichkeit).

LSD wird von führenden Naturwissenschaftlern in der Halluzinogenforschung, der Europäischen Beobachtungsstelle für Drogen und Drogensucht (EMCDDA) und dem "National Institute on Drug Abuse" des US-Gesundheitsministerium als nicht-abhängigkeitserzeugende Substanz angesehen, da es kein Suchtverhalten erzeugt.
Viele LSD-Konsumenten verringern ihren Gebrauch mit der Zeit freiwillig oder stellen ihn ganz ein.

Chronische Gaben von MAO-Hemmern und SSRIs schwächen die Effekte von LSD ab, man vermutet eine Herabregulation der 5-HT-Rezeptoren. Es besteht jedoch ein mögliches Risiko in der Kombination mit MAO-Hemmern oder SSRIs, die erst einmalig genommen worden sind, da dort die Herabregulation der 5-HT-Rezeptoren noch nicht fortgeschritten ist. Da die MAO-Hemmung und Serotonin-Wiederaufnahmehemmung die Wirkung von serotonergen Substanzen, zu denen LSD zählt, unvorhersehbar verstärkt, ist das Risiko eines Serotoninsyndroms möglicherweise erhöht. Allerdings stellt Ken Gillman in seinem Review von 2010 fest, dass es in über 50 Jahren LSD-Gebrauch keinen dokumentierten Fall gab, in dem ein Serotoninsyndrom zusammen mit dem Gebrauch von LSD einherging. Lithium und einige trizyklische Antidepressiva verstärken die Effekte von LSD, anekdotische Berichte sprechen von temporären komatösen Zuständen in Kombination mit Lithium.

Laut Datenblatt eines Herstellers ist Lysergsäurediethylamid hochtoxisch, laut einer anderen Quelle weist es eine schwache Toxizität auf. Tierversuche lassen vermuten, dass das Verhältnis von Wirkdosis zu tödlicher Dosis beim Menschen bei etwa 1:1000 liegt, d. h., die tausendfache Dosis einer wirksamen Dosis würde beim Menschen zu tödlichen Vergiftungen führen. Pharmazeuten gehen von einer therapeutischen Breite von 280 aus. Damit wäre LSD ein sicheres Medikament. Direkte Todesfälle sind bisher nur bei Tierversuchen bekannt, bei denen bewusst Tieren eine Überdosis intravenös verabreicht wurde.

Unter klinischen Bedingungen verursacht LSD keine Chromosomenbrüche, auch wird davon ausgegangen, dass LSD in mäßigen Dosen keine Effekte an menschlichen Chromosomen zeigt.<ref name="DOI10.1126/science.172.3982.431">N. I. Dishotsky, W. D. Loughman, R. E. Mogar, W. R. Lipscomb: "LSD and Genetic Damage." In: "Science." 172, 1971, S. 431–440, .</ref><ref name="DOI10.1093/mutage/13.6.557">Jih-Heng Li, Lih-Fang Lin: "Genetic toxicology of abused drugs: a brief review." In: "Mutagenesis." 13, 1998, S. 557–565, .</ref> Fragen hinsichtlich der krebserzeugenden, erbgutverändernden und fortpflanzungsgefährdenden Wirkung von LSD konnten auf Grund von zahlreichen schlecht designten Studien nicht hinreichend beantwortet werden. Man geht jedoch davon aus, dass LSD im Menschen nicht fortpflanzungsgefährdend und schwach mutagen bzw. nicht mutagen ist.

Die unter Einfluss von LSD als verändert erscheinende Umwelt kann für den Konsumenten zur Gefahr werden, da er zur Gefahreneinschätzung oft kein Gefühl mehr hat. Dadurch kann es zu einem Verlust der Selbstkontrolle im häufig psychoseartigen Rauschzustand kommen. Auch Menschen mit nicht durch Drogen ausgelösten Psychosen können eine Gefahr für sich und andere darstellen, wegen der Halluzinationen und weil Ereignisse oft nicht mehr richtig einzuordnen sind und dadurch die Fähigkeit fehlt, angemessen zu reagieren.

Hofmann warnt, dass selbst Menschen mit einer stabilen Persönlichkeit und guter Vorbereitung eine LSD-Psychose erleiden können. Durch gründliche Vorbereitung lassen sich laut Hofmann die vielfältigen Gefahren für Gesundheit und Leben, die vor allem durch die Halluzinationen und den Realitätsverlust bestehen, deutlich vermindern, aber nicht ausschließen:
Von der Bedienung von Maschinen oder der Teilnahme am Straßenverkehr ist abzuraten, weil die oben beschriebenen Wahrnehmungsphänomene eine große Gefährdung darstellen können (→ Fahren unter Einfluss psychoaktiver Substanzen).

In Deutschland wurde im Jahr 2010 kein Todesfall gezählt, der direkt oder indirekt mit dem Konsum von LSD in Verbindung stand. Auch im Jahr 2013 wurde in Deutschland kein LSD-Todesfall registriert. Bezogen auf das Jahr 2014 veröffentlichte die Drogenbeauftragte der Bundesregierung keine entsprechenden Zahlen.

Erst in den 1980er Jahren gewann LSD als Partydroge in der Technoszene wieder an Beliebtheit. Nachdem der Konsum von LSD nach Schätzungen der Drogenbeauftragten der Bundesregierung Anfang des letzten Jahrzehnts zurückgegangen war, ist seit 2008 wieder ein leichter Anstieg bei den Erstkonsumenten zu vermerken.
Die Droge wird normalerweise auf Papierstücke aufgebracht, die Tickets, Pappen oder Trips genannt werden und dann gelutscht oder geschluckt. LSD wird aber unter anderem auch als Lösung in Ethanol (sogenanntes "Liquid" oder auch mit Pipette getropfte "Drops"), auf Würfelzucker, als Kapsel- oder in Tablettenform eingenommen (spezielle Tabletten sind kleine Krümelchen, die eine gewünschte Dosis enthalten und als „Micro“ bezeichnet werden. Die Gelatinekapseln sind leer, nur die Kapselhülle selbst wird mit LSD-Lösung benetzt und getrocknet). Ein einzelnes Mikrokügelchen kann bis zu 1000 µg LSD enthalten, wogegen übliche Pappen nur 100–250 µg LSD enthalten.

Die Europäische Beobachtungsstelle für Drogen und Drogensucht berichtet, dass die Verkaufspreise für LSD in den meisten europäischen Ländern zwischen 5 Euro und 11 Euro pro Einheit liegen.

LSD und Ecstasy („Candyflip“):
Diese Kombination kann zu starken Wahrnehmungsveränderungen mit optischen und akustischen Halluzinationen führen. Die psychoaktive Wirkung beider Substanzen kann sich gegenseitig verstärken. Dabei kann es zu erwünscht angenehmen Erlebnissen kommen, aber auch die Gefahr einer drogeninduzierten Psychose ist erhöht.

Ein weiterer Gefahrenpunkt ist die durch die Illegalität bedingte Schwarzmarktware, deren Zusammensetzung oder Dosierung nie genau zu erkennen ist. So können zwei vom selben Dealer erworbene Trips, die sich optisch gleichen, völlig unterschiedlich dosiert sein. Auch müssen Trägermaterialien nicht unbedingt LSD enthalten, da andere halluzinogene Substanzen wie DOI, DOB, 25I-NBOMe, Bromo-DragonFLY etc. ebenfalls im Submilligrammbereich wirksam sind und auch als Löschblätter verkauft werden. Die Wirkdauer dieser Substanzen ist meist stark erhöht, im Falle von Bromo-DragonFLY bis zu mehrere Tage. Dass Strychnin enthalten sein kann, hat sich jedoch als Mythos erwiesen. Ein solcher Fall ist noch nie bestätigt worden. Trägermaterialien von nur geringer Größe (Beispiel: Löschpapier, Micros) nehmen keine wirkungsrelevante Strychninmenge auf.

Zur psychiatrischen Behandlung und zu Forschungszwecken wurde LSD 1949 unter dem Handelsnamen "Delysid" vom Pharmakonzern Sandoz bereitgestellt. Das LSD-Präparat Lysergamid wurde vom tschechoslowakischen Konzern Spofa hergestellt und vor allem in die Ostblockstaaten, einschließlich der DDR, exportiert.

LSD versetzt viele Anwender in einen Zustand, der Ähnlichkeiten mit bestimmten Symptomen von Psychosen hat. Im Unterschied zur Psychose weiß der Anwender in der Regel, dass die veränderte Wahrnehmung willentlich herbeigeführt wurde. Solche künstlich herbeigeführten Zustände werden Modellpsychose genannt.

Der Beipackzettel von "Delysid" wies auf die Möglichkeit der Anwendung als Psycholytikum und Psychotomimetikum hin. Textauszug Indikation: „(a) In der analytischen Psychotherapie zur Förderung seelischer Entspannung durch Freisetzung verdrängten Materials. (b) Experimentelle Studien über das Wesen der Psychose: Indem der Psychiater selbst Delysid einnimmt, wird er in die Lage versetzt, eine Einsicht in die Welt der Ideen und Wahrnehmungen psychiatrischer Patienten zu gewinnen.“

LSD wurde zunächst in der so genannten „psychedelischen Therapie“ eingesetzt, etwa bei schwer Krebskranken oder bei Alkoholikern. Ihr Ziel war es, die Probanden durch ein erschütterndes ekstatisches, stark religiös bzw. mystisch gefärbtes Erlebnis angstfreier zu machen bzw. vom Alkoholismus abzubringen. In seiner Studie zu diesem Thema spricht der Pionier der therapeutischen LSD-Forschung in Deutschland, Hanscarl Leuner, von einer Art „Heilung durch Religion“. Auch heute noch wird LSD im Rahmen der Psychotherapie als so genannte psycholytische Psychotherapie verwendet. Bei allen durch die Forschung bestätigten Vorzügen hat diese Therapieform jedoch auch Schattenseiten, insbesondere wegen des Machtgefälles zwischen dem Therapeuten und dem durch die Einnahme von LSD hoch suggestibel gemachten Therapie-Klienten. Eine qualifizierte Ausbildung und Supervision der Therapeutinnen und Therapeuten ist schon deshalb kaum möglich, weil die Behandlungen – von Ausnahmegenehmigungen abgesehen – vorwiegend in der Illegalität stattfinden.

Studien in den 1950er Jahren stellten bei der Behandlung von Alkoholismus mit LSD eine Erfolgsrate von 50 Prozent fest. Allerdings wurden einige LSD-Studien wegen methodischer Mängel kritisiert und unterschiedliche Gruppen hatten unterschiedliche Ergebnisse. In einem 1998 veröffentlichten Artikel wurden die Arbeiten zu dem Thema erneut untersucht. Man folgerte, dass die Frage der Effizienz von LSD in der Behandlung von Alkoholismus bisher unbeantwortet ist. Eine 2012 veröffentlichte Metaanalyse bestätigte dagegen die Ergebnisse der ursprünglichen Studien und konstatierte eine heilsame Wirkung.

Angesichts der Möglichkeit, mit nur 10 Kilogramm des hochpotenten Psychedelikums die gesamte Bevölkerung der Vereinigten Staaten zu berauschen, begannen in den frühen 1950er Jahren unter dem Vorzeichen des Kalten Krieges Forschungen zur Verwendung von LSD als chemische Waffe, als Wahrheitsserum oder zu anderen Zwecken. Im Fokus der Forschung, die die CIA sowie die Abteilung für chemische und biologische Kriegführung der amerikanischen Streitkräfte durchführten oder durchführen ließen, stand die Möglichkeit, es als Mittel zur "mind control" u. a. in den Laboratorien des Edgewood Arsenal einzusetzen. Im Rahmen von MKULTRA und anderen Projekten wurde Mitarbeitern, ohne dass sie es wussten, LSD verabreicht, die Droge wurde in so genannten "safe houses" in New York City und San Francisco an Freiwillige, an Drogensüchtige oder an Freier von Prostituierten gegeben; Menschenversuche an Strafgefangenen oder an Insassen psychiatrischer Anstalten beinhalteten, Probanden über mehrere Wochen ständig unter LSD-Einfluss zu halten oder die Wirkung der Droge in Kombination mit Elektroschocks, sensorischer Deprivation oder anderen Drogen zu testen. All diese Versuche führten zu keinen verwendbaren Resultaten. Nachdem die Forschungen Mitte der 1970er Jahre öffentlich bekannt geworden waren, wurden sie eingestellt.

Im Rahmen eines Teilprojektes vom MKULTRA nahm auch Ken Kesey, der nach seiner Militärzeit einige Zeit als Pfleger in einer Nervenklinik arbeitete, dort als Versuchsperson an LSD-Experimenten teil. Ken Kesey ging wie der Psychologe Timothy Leary in Berkeley (wo ebenfalls im Rahmen vom MKULTRA geforscht wurde) davon aus, dass LSD die Persönlichkeit von Menschen durch Bewusstseinserweiterung befreien und verbessern könnte und so auch die Gesellschaft positiv verändern könnte. Er gründete eine Hippie-Gruppe, die Merry Pranksters, die mit einem bunt bemalten Schulbus, dem "FURTHER" (engl. weiter; fördern, unterstützen, vorantreiben), durch die USA fuhren und überall sogenannte Acid-Tests veranstalteten, bei denen zum Testen Lysergsäurediethylamid an das Publikum verteilt wurde. Bei diesen LSD-Happenings traten als Band die Grateful Dead auf. Da LSD damals noch legal war, konnte so die Idee und die Praxis des LSD-Gebrauchs stark die Hippieära der Endsechziger mitprägen. Die Fahrten der Merry Pranksters wurden vom Autor Tom Wolfe, der einige Zeit in dem Bus mitfuhr, in dem Buch "Electric Kool-Aid Acid Test" literarisch verewigt. Sie waren ein wichtiger Faktor bei der Entstehung der Hippie-Bewegung in San Francisco und des Psychedelic Rock der späten 1960er und frühen 1970er Jahre, bereitete aber auch härteren Drogen wie Heroin den Boden.

Als Timothy Leary in den 1960er Jahren den Massenkonsum von LSD in den USA propagierte, übte Albert Hofmann starke Kritik. Nach dem Verbot von 1966 in den USA und der Einstufung als nicht verkehrsfähiger Stoff in Deutschland 1971 kam die Forschung an LSD-haltigen Therapeutika weitgehend zum Erliegen.

Als Droge wurde es aufgrund des nicht vorhandenen Abhängigkeitspotentials und der starken Toleranzbildung ebenfalls weitgehend zurückgedrängt. Da sich LSD im Gegensatz zu den meisten anderen Drogen nicht zum täglichen Konsum eignet, ist die nachgefragte Menge für den Drogenhandel unbedeutend, und da keine Abhängigkeit auftritt, sind Konsumenten auch nicht gezwungen, hohe Preise wie z. B. für Heroin oder Kokain zu bezahlen.

Sektorspinnen (Zygiella x-notata, früher Zilla x-notata) bauen unter LSD-Einfluss Netze mit erhöhter Zunahme der Winkelregelmäßigkeit. Man geht von einer Steigerung des Assoziationstempos bei der Anlage der Radialfäden aus sowie der besseren Verwertung des sensiblen Kontrollreizes. Dabei lag eine qualitative Steigerung einer spontanen Leistung durch LSD vor. Bei der Gabe von Mescalin wurden die Netze unregelmäßig und die Abweichung der Netzwinkelgröße nahm zu. Mit diesen Experimenten wollte man die Wirkungsweise von LSD gegenüber Mescalin abgrenzen, die im Menschen kaum unterschiedliche Wirkungen erzielen.

Seit etwa 1990 erlebt die Halluzinogenforschung eine Renaissance. Im Dezember 2007 wurde dem Schweizer Psychiater Peter Gasser bewilligt, eine doppelblinde, plazebokontrollierte Phase-II Dosis-Wirkungs-Pilotstudie zur psychotherapeutischen Behandlung mit LSD an Patienten mit Krebs im Endstadium durchzuführen. Die Pilotstudie sollte „Hinweise geben können, ob es sich lohnt und ob es vertretbar ist, mit LSD-unterstützter Psychotherapie weiterzuforschen allenfalls auch in grösserem Rahmen mit grösseren Probandenzahlen“. Die Ergebnisse sind vielversprechend, jedoch ist die Versuchsgruppe mit 12 Personen zu klein, um statistisch repräsentativ sein zu können. Die Studie wurde teilweise von der "Schweizerischen Ärztegesellschaft für Psycholytische Therapie" und hauptsächlich von der Lobby-Organisation "Multidisciplinary Association for Psychedelic Studies (MAPS)" finanziert.

Aktuellere Publikationen diskutieren LSD und das nichthalluzinogene 2-Brom-LSD (BOL-148) als mögliches Mittel gegen Cluster-Kopfschmerz.

LSD ist in der Bundesrepublik Deutschland aufgrund seiner Aufführung in der BtMG ein nicht verkehrsfähiges Betäubungsmittel. Der Umgang ohne Erlaubnis ist grundsätzlich strafbar. Weitere Informationen sind im Hauptartikel Betäubungsmittelrecht in Deutschland zu finden.

Mit der vierten Betäubungsmittel-Gleichstellungsverordnung (4. BtMGlV) vom 21. Februar 1967, in Kraft getreten am 25. Februar 1967, wurde LSD in der Bundesrepublik Deutschland den betäubungsmittelrechtlichen Vorschriften des Opiumgesetzes, dem Vorläufer des heutigen BtMG, unterstellt.

1966 wurde Lysergsäurediethylamid in den USA verboten. 
In Österreich wurde Lysergsäurediethylamid 1971 verboten. 

Lysergsäurediethylamid fällt außerdem unter die Kontrolle des Einheitsabkommen über die Betäubungsmittel (1961) und der Konvention über psychotrope Substanzen (1971), welche von den Vereinten Nationen beschlossen wurden.






</doc>
<doc id="2963" url="https://de.wikipedia.org/wiki?curid=2963" title="Laparoskopische Chirurgie">
Laparoskopische Chirurgie

Die laparoskopische Chirurgie (von griechisch ἡ λαπάρη (hē lapárē) – die Weichen, die Bauchhöhle, und σκοπέειν/σκοπεῖν (skopeîn) – betrachten, schauen) ist ein Teilgebiet der Chirurgie, bei der mit Hilfe eines optischen Instruments Eingriffe innerhalb der Bauchhöhle vorgenommen werden. Sie wird dem Komplex minimalinvasive Chirurgie (MIC) zugeordnet.

Die Laparoskopie, auch Bauchspiegelung genannt, bezeichnet eine Methode, bei der die Bauchhöhle und die darin liegenden Organe mit speziellen Stablinsen-Optiken (starren Endoskopen) durch kleine, vom Chirurgen geschaffene Öffnungen in der Bauchdecke sichtbar gemacht werden.
Über einen 0,3–2 cm langen Hautschnitt wird ein Trokar in die Bauchdecke eingebracht, durch den dann mit Hilfe eines speziellen Endoskops (Laparoskop), das an eine Videokamera und an eine Lichtquelle angeschlossen ist, der Bauchraum eingesehen werden kann. Bei einer diagnostischen Laparoskopie wird nach der Inspektion des Bauchraums das Instrument wieder entfernt und die Bauchdeckenwunde mittels Naht verschlossen oder – je nach Befund – eine therapeutische Maßnahme angeschlossen. Bei einem operativen Eingriff werden über weitere, ebenfalls 0,3–2 cm große Hautschnitte zusätzliche Instrumente eingebracht, mit deren Hilfe die Operation durchgeführt werden kann.

Bei der üblichen Methode wird zunächst der Bauchraum mit Gas befüllt, bis ein Pneumoperitoneum geschaffen ist. Dies kann durch unterschiedliche Methoden geschehen. Eine davon besteht darin, dass mit einem chirurgischen Skalpell ein kleiner Hautschnitt im Bereich des Nabels gesetzt wird (u. a. weil dort die Bauchwand am dünnsten und der Abstand zu den Bauchorganen am größten ist). Danach wird mit einer speziellen Insufflationskanüle (Veres-Kanüle oder Veres-Nadel) die Bauchwand nur so weit durchstoßen, dass sich schließlich deren stumpfe Spitze, an der sich die Insufflationsöffnung befindet, frei im Bauchraum befindet. Nun kann an die Veres-Kanüle der Schlauch einer Insufflationspumpe angeschlossen und der Bauchraum mit Kohlendioxid (CO) so weit „aufgepumpt“ werden, dass eine Art „Arbeits- und Untersuchungsraum“ entsteht. Die Insufflationskanüle wird entfernt und ein Trokar wird „blind“ eingeführt. Über diesen Trokar wird dann das Laparoskop eingeführt. Damit kann der Intraabdominalraum betrachtet werden.

Dieser Raum (Intraabdominalraum) muss nun noch für eine endoskopische Operation zugänglich gemacht werden. Zu diesem Zweck werden, je nach Art des geplanten Eingriffs, wie oben bereits beschrieben, weitere kleine Einstiche in der Bauchdecke gemacht, durch welche gasdicht schließende Trokar-Hülsen eingeführt und sicher verankert werden. Durch diese im Fachjargon „Trokarzugänge“ genannten „Schlüsselloch-Öffnungen“ können das Endoskop und die chirurgischen Spezialinstrumente vom Operateur oder den Assistenten von Hand bedient werden. Bei dieser Methode können Nebenwirkungen wie Schulterschmerzen durch Reizung des Nervus phrenicus auftreten. 

Bei einer alternativen, weniger verbreiteten Methode, der "gaslosen Laparoskopie", wird die Bauchdecke mittels eines Lift-Systems mechanisch angehoben. Hierbei soll der Patient eine Vielzahl von Vorteilen gegenüber der CO-Methode haben: Die Schmerzen sollen nach einer Operation deutlich geringer sein (vor allem die für die gashaltige Laparoskopie üblichen Schulterschmerzen), die Erholung nach einer Operation zeitlich verkürzt. Außerdem sollen die Kosten für eine Behandlung geringer sein.

Folgende Eingriffe können laparoskopisch durchgeführt werden:

Selbst laparoskopische Tumoroperationen werden in Deutschland mittlerweile standardmäßig durchgeführt. (Siehe hierzu beispielhaft die Website der Charité Berlin.) Beispielsweise zeigt eine Studie bei laparoskopischer Resektion von Darmkrebs gleichwertige Ergebnisse zu dem offenen Eingriff, bei schonenderer OP für den Patienten.

Die erste Laparoskopie beim Menschen – eine diagnostische Laparoskopie – wurde 1910 von dem Schweden Hans Christian Jacobaeus (1879–1937) durchgeführt, nachdem der Dresdner Georg Kelling (1866–1945) 1901 eine Laparoskopie (er selbst bezeichnete das Verfahren als „Zölioskopie“) bei einem Hund durchgeführt hatte. In den 1930er Jahren wurden erstmals auch therapeutische Laparoskopien, vor allem durch Gynäkologen, durchgeführt. Die erste laparoskopische Blinddarmentfernung fand in der Universitätsfrauenklinik Kiel 1980 durch den Gynäkologen Kurt Semm (1927–2003) statt. Die erste laparoskopische Gallenblasenentfernung über ein Galloskop (Ein-Rohrtechnik) wurde 1985 von dem Böblinger Chirurgen Erich Mühe durchgeführt, der allerdings mit seiner Technik in der Literatur keine Würdigung fand. Die erste laparoskopische Gallenblasenentfernung über mehrere Zugänge, so wie sie heutzutage üblich ist, erfolgte 1987 durch den französischen Chirurgen Phillipe Mouret (* 1937), 1989 die erste Leistenoperation durch D. Bogojavlensky, 1991 die erste Dickdarmoperation durch John Monson. 1998 kam es dann zur Einführung der Lift-Laparoskopie mit dem Abdo-Lift (neues Konzept der gaslosen Laparoskopie) durch Daniel Kruschinski.





</doc>
<doc id="2965" url="https://de.wikipedia.org/wiki?curid=2965" title="Laparoskopische Fundoplicatio">
Laparoskopische Fundoplicatio

Laparoskopische Fundoplicatio ist eine Methode zur Behandlung der Refluxkrankheit (Sodbrennen), entstanden aus der konventionell durchgeführten Fundoplicatio nach Rudolf Nissen im Jahre 1957. Es wird dabei der Magenfundus als Manschette um den Mageneingang gelegt und mit einzelnen Nähten fixiert. Dadurch wird der Magensäurerückfluss in die Speiseröhre reduziert.

"siehe auch: Laparoskopie"


</doc>
<doc id="2966" url="https://de.wikipedia.org/wiki?curid=2966" title="Leonardo DiCaprio">
Leonardo DiCaprio

Leonardo Wilhelm DiCaprio (* 11. November 1974 in Los Angeles, Kalifornien) ist ein US-amerikanischer Filmschauspieler, Produzent und Oscar-Preisträger. Er zählt zu den bestbezahlten und erfolgreichsten zeitgenössischen Schauspielern Hollywoods und arbeitet regelmäßig mit renommierten Regisseuren zusammen.

Einem breiteren Publikum wurde DiCaprio durch die Rolle des geistig behinderten Jungen Arnie Grape in "Gilbert Grape – Irgendwo in Iowa" bekannt, für die er 1994 seine erste Oscar-Nominierung erhielt. Zu weltweitem Ruhm gelangte er 1997 mit der Darstellung des mittellosen Schiffspassagiers Jack Dawson in James Camerons Filmdrama "Titanic". Für seine Rollen als Flugpionier Howard Hughes in "Aviator," als Börsenmakler Jordan Belfort in "The Wolf of Wall Street" und als Trapper Hugh Glass in "The Revenant – Der Rückkehrer" wurde er 2005, 2014 und 2016 mit einem Golden Globe ausgezeichnet. Für seine Rolle als Hugh Glass erhielt er 2016 zudem den Oscar als bester Hauptdarsteller.

Der Sohn des Comicbuchautors und -verkäufers George DiCaprio und der früheren Rechtsanwaltsgehilfin Irmelin Indenbirken-DiCaprio, geborene Indenbirken, wurde als einziges Kind seiner Eltern in Hollywood, Los Angeles, geboren. Seine Mutter war in den 1950er-Jahren aus ihrem Geburtsort Oer-Erkenschwick in Nordrhein-Westfalen in die USA gezogen. Sein Vater ist ein bereits in vierter Generation geborener US-amerikanischer Staatsbürger halb italienischer und halb deutscher Abstammung. Seine Großmutter mütterlicherseits, Helene Indenbirken (1915–2008), wurde als Yelena Smirnova in Russland geboren.

Über DiCaprios Vornamen soll entschieden worden sein, als seine schwangere Mutter in Italien vor einem Gemälde Leonardo da Vincis gestanden und der noch ungeborene Sohn gegen die Bauchdecke getreten habe. Seinen zweiten Vornamen hat er von seinem deutschen Großvater Wilhelm Indenbirken. Seine Eltern trennten sich, als er ein Jahr alt war. Acht Jahre später folgte die Scheidung, DiCaprio lebte überwiegend bei seiner Mutter. Er wuchs in Los Angeles im Stadtteil Echo Park auf und besuchte dort die Grundschule sowie die "John Marshall High School". Seine Großmutter Helene Indenbirken wohnte bis zu ihrem Tode im Alter von 93 Jahren am 5. August 2008 am Geburtsort seiner Mutter. DiCaprio, der sie dort seit früher Jugend wiederkehrend besuchte, erwarb während dieser Zeit Grundkenntnisse der deutschen Sprache. 1984 nahm er an einem Breakdance-Turnier in Oer-Erkenschwick teil, nachdem er bereits einige Turniere in den USA hatte gewinnen können.

Aus der zweiten Ehe seines Vaters mit Peggy Farrar hat er einen Stiefbruder, den Gelegenheitsschauspieler Adam Farrar, der allerdings seit Ende der 1990er-Jahre wiederholt wegen verschiedener Delikte mit dem Gesetz in Konflikt geraten ist.

Seine Karriere begann Ende der 1980er-Jahre im Alter von 14 Jahren mit Auftritten in Werbespots, darunter für Kaugummi der Marke "Bubble Yum" und "Kraft"-Käse sowie als Laiendarsteller in Lehrfilmen. Der Durchbruch gelang ihm 1990 mit der Rolle des Garry Buckman in der Fernsehserie "Eine Wahnsinnsfamilie" (Originaltitel: "Parenthood)", die auf dem gleichnamigen Film aus dem Jahr 1989 basiert. In der Seifenoper "California Clan" übernahm er kurz darauf die Rolle des jungen Mason Capwell. Sein Spielfilmdebüt hatte er in "Critters 3 – Die Kuschelkiller kommen", einem B-Movie aus dem Jahr 1991. Zunehmende Bekanntheit erzielte DiCaprio in der Sitcom "Unser lautes Heim", in der er 1991 und 1992 den Straßenjungen Luke Brower spielte.
1993 agierte DiCaprio als Sohn eines gewalttätigen Stiefvaters neben Robert De Niro und Ellen Barkin in "This Boy’s Life". Aufmerksamkeit erlangte er mit der Rolle des geistig behinderten Jungen Arnie Grape in Lasse Hallströms Familiendrama "Gilbert Grape – Irgendwo in Iowa" (1993), für die er 1994 in der Kategorie „Bester Nebendarsteller“ mit einer Oscarnominierung geehrt wurde.
In den darauffolgenden Jahren stellte er in gleich mehreren Filmen seine schauspielerische Vielseitigkeit unter Beweis. So porträtierte er 1995 den Schriftsteller und Musiker Jim Carroll, dessen Heroinsucht und sozialer Abstieg im Rahmen einer Autobiografie veröffentlicht und schließlich verfilmt wurden. In "Total Eclipse – Die Affäre von Rimbaud und Verlaine" von Agnieszka Holland setzte DiCaprio seine Darstellung authentischer Personen mit der Rolle des homosexuellen französischen Lyrikers Arthur Rimbaud fort. Als Filmpartner fungierte David Thewlis als Paul Verlaine. Gemeinsam mit Claire Danes stellte er in "William Shakespeares Romeo + Julia" (1996) unter der Regie von Baz Luhrmann das gleichnamige Liebespaar Romeo und Julia dar. Die Dialoge entsprachen den Originaltexten von William Shakespeare aus dem 16. Jahrhundert. Für seine Darstellung erhielt DiCaprio im Rahmen der Berlinale 1997 den Silbernen Bären.

Die mit elf Oscars ausgezeichnete Kinoproduktion "Titanic", die gegenwärtig auf Platz Zwei der kommerziell erfolgreichsten Filme aller Zeiten rangiert, bedeutete für DiCaprio 1997 den eigentlichen Karriereschub. Durch die Darstellung des mittellosen Schiffspassagiers Jack Dawson, der sich entgegen der gesellschaftlichen Norm in eine junge wohlhabende Frau namens Rose verliebt, entwickelte sich der damals 22-Jährige zum Superstar, einhergehend mit weltweiter Popularität als Mädchen- und Frauenschwarm. Nicht zuletzt der Umstand zahlreicher Berichterstattungen in Jugendmedien erschwerte es ihm in den folgenden Jahren sehr, sich trotz variierender Filmfiguren in unterschiedlichen Genres von seinem Status als Teenie-Idol zu lösen.

Nach fiktiven Figuren in Filmen wie "Celebrity – Schön. Reich. Berühmt." (1998) und "The Beach" (2000), der ihm eine Nominierung für die Goldene Himbeere als „Schlechtester Schauspieler“ einbrachte, spielte DiCaprio 2002 in der Großproduktion "Gangs of New York" zum ersten Mal unter der Regie von Martin Scorsese. Im selben Jahr übernahm er die Rolle des US-amerikanischen Hochstaplers und Scheckfälschers Frank W. Abagnale in "Catch Me If You Can". Seine Darstellung wurde mit einer Nominierung für den Golden Globe Award gewürdigt. 2005 gelang ihm die Auszeichnung in der Kategorie „Bester Hauptdarsteller“ für seine Verkörperung des Hollywood-Millionärs und Flugpioniers Howard Hughes in dem Martin-Scorsese-Film "Aviator", darüber hinaus wurde DiCaprio für den Oscar nominiert.

Zur Verleihung der Golden Globe Awards 2007 gelang ihm als erstem Schauspieler in der Geschichte dieses Filmpreises, für zwei Rollen – als verdeckter Ermittler Billy Costigan in "Departed – Unter Feinden" und als 32-Bataljon-Soldat Danny Archer in "Blood Diamond" – in gleicher Kategorie (Bester Hauptdarsteller in einem Drama) nominiert zu werden. "Blood Diamond" thematisiert den Konflikt um sogenannte Blutdiamanten in Afrika vor dem Hintergrund des Bürgerkriegs in Sierra Leone 1999, für den DiCaprio 2007 seine dritte Oscar-Nominierung in der Kategorie „Bester Hauptdarsteller“ erhielt. Erstmals seit "Titanic" agierte er im Jahr 2008 in Sam Mendes’ Filmdrama "Zeiten des Aufruhrs" wieder an der Seite von Kate Winslet. Seine Darstellung des frustrierten Ehemanns Frank Wheeler bescherte ihm in der Kategorie „Bester Hauptdarsteller – Drama“ eine weitere Nominierung für den Golden Globe. Unter der Regie von Ridley Scott folgte im gleichen Jahr der Thriller "Der Mann, der niemals lebte", in dem DiCaprio den CIA-Agenten und Anti-Terror-Spezialisten Roger Ferris verkörperte.

Mit "Shutter Island", der im Februar 2010 in den deutschen Kinos anlief, arbeitete der Schauspieler bereits zum vierten Mal unter der Regie von Martin Scorsese, den er für den größten lebenden Regisseur hält. DiCaprio übernahm in diesem auf dem gleichnamigen Roman von Dennis Lehane basierenden Psycho–Thriller die Rolle des zunehmend unter Paranoia leidenden US-Marshals Edward „Teddy“ Daniels. 2009 beendete er die Dreharbeiten zu Christopher Nolans Science-Fiction-Film "Inception", der am 29. Juli 2010 in den deutschen Kinos anlief. Der Film war weltweit ein großer Erfolg, wurde mit vier Oscars ausgezeichnet und ist DiCaprios kommerziell erfolgreichster Film seit "Titanic".
Gemeinsam mit Regisseur Clint Eastwood arbeitete DiCaprio 2011 an der Verfilmung des Lebens von FBI-Gründer J. Edgar Hoover. Für seine darstellerische Leistung erhielt DiCaprio eine Golden Globe-Nominierung.

In Quentin Tarantinos Western "Django Unchained" stellte DiCaprio den sadistischen Plantagenbesitzer und Sklaventreiber Calvin Candie dar, wofür er ein weiteres Mal für den Golden Globe nominiert wurde.

Von September 2011 bis Februar 2012 drehte DiCaprio unter der Regie von Baz Luhrmann in Sydney die Romanverfilmung "Der große Gatsby" in 3-D. Er übernahm darin die Titelrolle des Jay Gatsby. Die zweite Hauptrolle übernahm sein enger Freund Tobey Maguire. Es war bereits die zweite Zusammenarbeit zwischen DiCaprio und Luhrmann nach "William Shakespeares Romeo + Julia" im Jahre 1996. Der Film eröffnete die 66. Internationalen Filmfestspiele von Cannes und lief am 16. Mai 2013 in den deutschen Kinos an.

Die fünfte Zusammenarbeit von Leonardo DiCaprio und Martin Scorsese ist "The Wolf of Wall Street", welcher auf der gleichnamigen Autobiografie des früheren Börsenmarklers Jordan Belfort basiert. DiCaprio wurde für seine Darstellung als bester Hauptdarsteller in der Kategorie Komödie oder Musical mit einem Golden Globe Award ausgezeichnet und erhielt seine vierte und fünfte Oscar-Nominierung, die dritte als Bester Hauptdarsteller und seine erste als Produzent für den besten Film.

Nach Beendigung der Dreharbeiten zu "The Wolf of Wall Street" kündigte DiCaprio im Januar 2013 zunächst an, eine längere Pause von der Schauspielerei zu nehmen, um sich stattdessen stärker seinem Umweltengagement zu widmen. Im Herbst 2014 beendete er seine Pause jedoch und begann die Dreharbeiten für eine Verfilmung der Lebensgeschichte des Trappers Hugh Glass unter dem Titel "The Revenant – Der Rückkehrer", bei der Alejandro González Iñárritu die Regie übernahm. Die Dreharbeiten waren von zahlreichen Schwierigkeiten aufgrund von Wetterbedingungen, Budgetüberschreitungen und Drehplanverzögerungen gezeichnet und endeten schließlich im September 2015. Der deutsche Kinostart fand am 6. Januar 2016 statt. Für seine Rolle als Hugh Glass wurde DiCaprio der Academy Award in der Kategorie Bester Hauptdarsteller verliehen.

Nach Angaben des amerikanischen Forbes Magazine zählt Leonardo DiCaprio seit 2007 zu den am besten verdienenden Schauspielern in Hollywood. Zwischen Juni 2007 und Juni 2008 erhielt er Gagen in Höhe von 45 Mio. US-Dollar und rangierte hinter Will Smith (80 Mio. US-Dollar), Johnny Depp (72 Mio. US-Dollar), Eddie Murphy und Mike Myers (je 55 Mio. US-Dollar) auf Platz fünf. Für seine Rolle in "Catch Me If You Can" (2002) erhielt er eine Gage in Höhe von 20 Millionen Dollar, die achtfache Summe seiner Gage für "Titanic" (1997). Im Dezember 2010 belegte er nach einer neuerlichen Forbes-Erhebung durch den Kinokassenerfolg seiner Filme "Shutter Island" und "Inception" (1,1 Milliarden US-Dollar) Platz eins der finanziell erfolgreichsten Schauspieler Hollywoods vor Johnny Depp und Mia Wasikowska (je 1,03 Milliarden US-Dollar).
Laut Angaben der britischen Tageszeitung The Guardian belegte DiCaprio im Jahre 2010 hinter James Cameron und Steven Spielberg Rang drei der einflussstärksten Menschen in Hollywood.
Zwischen Mai 2010 und Mai 2011 erhielt er Gagen in Höhe von 77 Millionen US-Dollar und rangierte damit vor Johnny Depp (50 Mio. US-Dollar) und Adam Sandler (40 Mio. US-Dollar) auf Platz eins der bestverdienenden Schauspieler Hollywoods. Nach einer erneuten Auflistung des "Forbes Magazine" belegte Leonardo DiCaprio im Jahre 2013 hinter Hugh Jackman und Robert Downey jr. Platz drei der einflussreichsten Schauspieler und Platz 21 der mächtigsten Berühmtheiten der Welt.
Das Vulture Magazin platzierte DiCaprio 2013 in ihrer Auflistung der 100 wertvollsten Stars hinter Robert Downey jr. auf Platz zwei.

DiCaprio besitzt mit Appian Way Productions eine eigene Produktionsfirma, die unter anderem maßgeblich an der Erstellung der Filme "Orphan – Das Waisenkind" (2009) und "Red Riding Hood" (2011) beteiligt war.

Mit seiner 1998 – im Alter von 24 Jahren – gegründeten „Leonardo DiCaprio Foundation“ setzt DiCaprio sich für den Umweltschutz und gegen die globale Erwärmung ein. Ausdruck seines Engagements ist der 2007 von ihm erzählte und mitverfasste Dokumentarfilm "11th Hour – 5 vor 12" über die vielfältigen Umweltkrisen, welche die Menschheit und ihren Planeten bedrohen. Im Rahmen der US-Präsidentschaftswahlen 2004 und 2008 unterstützte er jeweils den Kandidaten der Demokratischen Partei (2004 John Kerry, 2008 Barack Obama).

Im Dezember 2013 stieg DiCaprio beim Automobilhersteller Venturi Automobiles ein und gründete mit dessen Gründer Gildo Pallanca Pastor den Rennstall Venturi Formula E Team. DiCaprio kommentierte seinen Einstieg wie folgt: „Die Zukunft unseres Planeten hängt von unserer Fähigkeit ab, kraftstoffsparende Autos zu bauen, die auf sauberer Energie basieren. Venturi Grand Prix hat mit der Gründung eines umweltfreundlichen Rennteams enorme Weitsicht bewiesen und ich bin glücklich, Teil dieser Bemühung zu sein.“ Der Rennstall geht seit 2014 in der neuen Rennserie FIA-Formel-E-Meisterschaft an den Start. 2015 wurde der Schauspieler zum Vorsitzenden des Nachhaltigkeits-Komitees der Rennserie ernannt. Dieses Gremium befasst sich vor allem mit der Förderung der elektrischen Mobilität und will damit die Verbreitung von Elektroautos vor allem in Städten voranbringen.

2014 wurde DiCaprio zum UN-Friedensbotschafter ernannt und hielt die Auftaktrede auf dem UN-Klimagipfel in New York. Darin bezeichnete er den Klimawandel als größte Herausforderung der Menschheit und forderte von den Regierungen sowie der Industrie, sofort entschiedene und deutliche Maßnahmen zu treffen. 

Aus Umweltgründen ist DiCaprio Vegetarier, und wird als Veganer gehandelt. Er unterstützte die Dokumentation Cowspiracy, welche den Einfluss von tierischen Produkten wie Fleisch oder Milch auf die Umwelt thematisiert. Er nahm dazu die Rolle als ausführender Produzent ein, damit die Dokumentation auf Netflix erscheinen konnte.

Im Januar 2016 war DiCaprio Gast beim Annual Meeting 2016 des Weltwirtschaftsforums in Davos und nahm dort für seinen Einsatz gegen den Klimawandel und für den Schutz bedrohter Tierarten den "Crystal Award" des WEF entgegen. 

Auch in seiner Dankesrede nach Erhalt des „Oscars“ als bester Hauptdarsteller 2016 sprach er das Thema Umwelt- und Klimaschutz an. The Revenant – Der Rückkehrer sei ein Film über die Beziehung der Menschen zur Natur. Die globale Erwärmung sei real und die drängendste Bedrohung für die Menschheit. Daher müsse die Menschheit nun etwas dagegen unternehmen und aufhören, das Problem auf die lange Bank zu schieben.

DiCaprio ist ein enger Freund des Schauspielers Tobey Maguire, den er während des Castings für die Serie "Eine Wahnsinnsfamilie" im Jahr 1990 kennenlernte. Zudem ist er ein langjähriger Freund der Schauspieler Kevin Connolly, Lukas Haas und Kate Winslet, für die er bei ihrer dritten Hochzeit als Trauzeuge fungierte. Den Schauspieler Christopher Pettiet kannte er seit seiner Kindheit.

Nach Beziehungen mit den Models Kristen Zang, Emma Miller und Gisele Bündchen war DiCaprio von 2005 bis 2011 mit dem Model Bar Refaeli liiert. Im Mai 2011 bestätigten beide ihre Trennung. Später war er mit der Schauspielerin Blake Lively und dem Model Erin Heatherton liiert.
Von Sommer 2013 bis Dezember 2014 war er mit dem deutschen Model Toni Garrn zusammen. Von April bis November 2015 war er mit dem Model Kelly Rohrbach liiert, die Beziehung ist angeblich an der geringen Freizeit der beiden gescheitert. Es folgte eine etwa einjährige Liaison mit dem dänischen Model Nina Agdal. 

DiCaprio besitzt ein Haus in Los Angeles und eine Wohnung in Manhattan.
Im Jahr 2005 kaufte er die belizische Insel Blackadore Caye und plant, dort ein umweltfreundliches Resort zu bauen.

Im selben Jahr wurde DiCaprio schwer im Gesicht verletzt, als ihn das ehemalige Fotomodel Aretha Wilson mit einer Glasflasche am Kopf traf. Wilson bekannte sich schuldig und wurde 2010 zu zwei Jahren Haft verurteilt.


Seit "William Shakespeares Romeo + Julia" (1996) wird Leonardo DiCaprio in allen deutschen Synchronfassungen von Gerrit Schmidt-Foß synchronisiert, erstmals zuvor in "This Boy’s Life" (1993). In "Gilbert Grape – Irgendwo in Iowa" und "Total Eclipse – Die Affäre von Rimbaud und Verlaine" lieh ihm David Nathan seine Stimme, der seit 1995 vor allem als Stammsprecher von Johnny Depp bekannt ist.



Leonardo DiCaprio zählt seit vielen Jahren zu den gefragtesten und wandelbarsten Schauspielern. So hat er bislang über 61 nationale und internationale Auszeichnungen gewonnen und war für weitere 187 Preise nominiert. Für seine Darstellungen in "Aviator", "The Wolf of Wall Street" und "The Revenant" wurde DiCaprio jeweils als bester Hauptdarsteller mit dem Golden Globe ausgezeichnet, zudem wurde er acht weitere Male nominiert. Seit 2005 zählt Leonardo DiCaprio immer wieder zum Favoritenkreis bei der Vergabe der Oscars. Diesen konnte er 2016 für seine Darbietung in "The Revenant – Der Rückkehrer" in der Kategorie bester Hauptdarsteller gewinnen. Die folgende Auswahl zeigt die wichtigsten und bekanntesten Auszeichnungen und Nominierungen DiCaprios.

2017 wurde eine in der Karibik heimische Spinne nach ihm benannt: "Spintharus leonardodicaprioi".




</doc>
<doc id="2968" url="https://de.wikipedia.org/wiki?curid=2968" title="Leonberg">
Leonberg

Leonberg ist eine Stadt in der Mitte des Bundeslandes Baden-Württemberg, etwa 13 Kilometer westlich von Stuttgart. Mit etwa 45.000 Einwohnern ist sie nach Sindelfingen und Böblingen die drittgrößte Stadt des Landkreises Böblingen und als Mittelzentrum für die umliegenden Gemeinden ausgewiesen. Leonberg ist seit 1. Oktober 1963 eine Große Kreisstadt und war bis 1973 der Sitz des gleichnamigen Landkreises, der im Zuge der Kreisreform 1973 aufgelöst wurde.

Leonberg liegt über dem rechten Hang des Glemstales, auf einem Sporn, einem Ausläufer des Engelbergs, der wiederum Teil des Glemswaldes ist. Die Glems betritt im Südosten von Stuttgart kommend das Stadtgebiet, fließt dann nach Nordwesten, wendet sich beim Stadtteil Eltingen nach Nordosten, fließt anschließend durch die westliche Kernstadt (die Altstadt liegt rechts des Flusses), dann südlich am Stadtteil Höfingen vorbei und verlässt das Stadtgebiet im Nordosten in Richtung Ditzingen wieder.

Die nördlichen Stadtteile Höfingen und Gebersheim zählen zum Strohgäu.

Folgende Städte und Gemeinden grenzen an die Stadt Leonberg. Sie werden im Uhrzeigersinn beginnend im Norden genannt:
Ditzingen und Gerlingen (Landkreis Ludwigsburg), Stuttgart (Stadtkreis) sowie Sindelfingen, Magstadt, Renningen und Rutesheim (alle Landkreis Böblingen)

Leonberg besteht aus der Kernstadt, der 1938 eingemeindeten Gemeinde Eltingen, die heute mit der Kernstadt zusammengewachsen ist, sowie den im Rahmen der Gebietsreform 1975 eingegliederten Stadtteilen Gebersheim, Höfingen und Warmbronn.

Zur Kernstadt gehören auch die Stadtteile Silberberg (räumlich getrennt) und Ramtel, das Wohngebiet Gartenstadt sowie weitere räumlich getrennte Wohnplätze mit eigenem Namen, wie Eichenhof, Glemseck, Hinter Ehrenberg, Mahdental und Rappenhof. Auch im Stadtteil Höfingen unterscheidet man räumlich getrennte Wohnplätze mit eigenem Namen, wie etwa Tilgshäusleshof und Wannenhof.

Die drei 1975 eingegliederten Gemeinden und heutigen Stadtteile Gebersheim, Höfingen und Warmbronn sind zugleich Ortschaften im Sinne der baden-württembergischen Gemeindeordnung. Das heißt, sie haben jeweils einen von der Bevölkerung bei jeder Kommunalwahl alle fünf Jahre neu zu wählenden Ortschaftsrat, dessen Vorsitzender der Ortsvorsteher ist.

Leonberg ist ein Mittelzentrum in der Region Stuttgart, deren Oberzentrum Stuttgart ist. Zum Mittelbereich Leonberg gehören noch die Städte und Gemeinden im Norden des Landkreises Böblingen, im Einzelnen: Renningen, Rutesheim, Weil der Stadt und Weissach.

Nach Daten des Statistischen Landesamtes, Stand 2014.

Das heutige Stadtgebiet Leonbergs gehörte ursprünglich zu weiten Teilen den Grafen von Calw und den Grafen bzw. späteren Pfalzgrafen von Tübingen, die einen Teil der Besitzungen insbesondere als Hubengüter an die von ihnen gestifteten Klöster Hirsau und Bebenhausen abtraten. Auf der Gemarkung befanden sich die 1100 im „Hirschauer Schenkungsbuch“ erstmals urkundlich erwähnten Dörfer Eltingen und Höfingen, das im 12. Jahrhundert beurkundete Gebersheim sowie das 1273 erstmals erwähnte Warmbronn.

Um 1248/49 begann Graf Ulrich I. von Württemberg, auf dem strategisch vorteilhaften Gelände zwischen einer bestehenden Burg auf dem Engelberg und dem Glemsknie eine anfangs „Levinberch“ genannte Stadt am Westrand seines Herrschaftsbereichs aufzubauen. Am Südwestrand der Stadt, am Standort des späteren Schlosses, ließ er zudem eine Burg errichten. Im Reichskrieg gegen Graf Eberhard I. von Württemberg unterstellte sich Leonberg vorübergehend (1312 bis 1316) der Reichsstadt Esslingen. Nachdem die Leonberger später nochmals die Seite wechselten, mussten sie 1383 Urfehde schwören, nicht mehr von Württemberg abzufallen. Von nun an war Leonberg endgültig württembergisch und wurde Sitz eines Amtes. Im Laufe des 14. Jahrhunderts erwarb Württemberg zudem sukzessive Eltingen, Warmbronn und Höfingen, die dem Amt Leonberg zugegliedert wurden.

1457 tagte in Leonberg einer der ersten urkundlich belegten württembergischen Landtage um die Vormundschaft des unmündigen Eberhard V. zu regeln. Dieser errichtete 1467 ein Franziskanerkloster in Leonberg. 1470 lebten etwa 900 Personen in 208 Haushalten in der Stadt. 1468 wird ein neues Rathaus gebaut – das heutige Alte Rathaus, das anfangs Bürgerhaus heißt. Im Jahr 1485 stifteten Leonberger Bürger zum Zwecke der Altersversorgung ein Spital, in dem auch Arme aufgenommen wurden. Nach der Reformation zog es in das leerstehende Kloster um. 1498 wurden beim ersten großen Stadtbrand 46 Häuser zerstört.

Beim Aufstand des „Armen Konrads“ richteten Leonberger Bürger eine „Kanzlei des Armen Konrads“ ein und drängten die lokale Ehrbarkeit in die Defensive. Der nahe Engelberg wurde zentraler Versammlungsort für viele umliegende Gemeinden. Der Zulauf war so groß, dass Herzog Ulrich Zugeständnisse machen musste, um die nach dem Tübinger Vertrag eingeforderte Huldigung zu erlangen.

Während des Exils von Herzog Ulrich (1519–1534) stand Leonberg unter österreichischer Herrschaft. Nachdem Herzog Ulrich aus dem Exil zurückgekehrt war, begann er, die Reformation durchzusetzen. Die Konfession des Landesvaters hatten alle Untertanen anzunehmen. So wurden auch die Leonberger nach mehrfachem Hin und Her schließlich komplett evangelisch. 1541 wurde in Leonberg eine Lateinschule und 1580 eine Mädchenschule eingerichtet.

Zwischen 1560 und 1565 ließ Herzog Christoph anstelle der Burg das Schloss erbauen. 1566 erhielt der Marktbrunnen eine von Leonhard Baumhauer geschaffene Wäppnerfigur. Zwischen 1570 und 1621 schuf der Leonberger Bildhauer Jeremias Schwartz für Württemberg einmalige Grabmäler der Spätrenaissance an der evangelischen Stadtkirche, sowie die älteste Leonberger Stadtansicht von 1618. Johannes Kepler, der mit seinen Eltern 1575 von Weil der Stadt nach Leonberg zog, besuchte bis 1583 Leonberger Schulen. Von 1609 bis 1614 hatte Herzogin Sybilla ihren Witwensitz im Leonberger Schloss. Sie ließ Baumeister Heinrich Schickhardt den 1980 rekonstruierten Pomeranzengarten anlegen. 1609 erbaute Schickhardt für sie das Seehaus als Jagd- und Landsitz.

Während der Zeit der Hexenverfolgung erhob der Leonberger Vogt Lutherus Einhorn während seiner Amtszeit (1613–1629) gegen 15 Frauen Anklage wegen Hexereiverdachts und ließ acht Todesurteile gegen „überführte“ Hexen vollstrecken. Er handelte in Übereinstimmung mit der Leonberger Stadtobrigkeit und weiten Teilen der Bevölkerung. Einer der bekanntesten württembergischen Hexenprozesse wurde 1620 in Leonberg eröffnet: gegen Katharina Kepler, Mutter des kaiserlichen Astronomen Johannes Kepler. Der bald nach Güglingen verlegte Prozess endete im Oktober 1621 mit dem Freispruch, Ergebnis des persönlichen und finanziellen Einsatzes von Johannes Kepler sowie der Standhaftigkeit der Angeklagten.

Während des Dreißigjährigen Krieges quartierte sich von 1634 bis 1638 General Gallas mit seinem Hofstaat im Leonberger Schloss ein. 1635 dezimierte die Pest die Bevölkerung um die Hälfte. Von 1670 bis 1673 war der Schriftsteller und Komponist Daniel Speer Lehrer an der Lateinschule. 1684 fand der erste Pferdemarkt statt, der heute noch veranstaltet wird. Im Jahr 1703 bestand die Bevölkerung aus 1076 Personen.

Ab 1786 begann die Stadt, sich ihrer spätmittelalterlichen Befestigung zu entledigen. Die Stadtgräben wurden zugeschüttet, die Tore und die Stadtmauer großteils abgerissen und neu bebaut. Bei der Neugliederung des jungen Königreichs Württemberg am Anfang des 19. Jahrhunderts überstand das Leonberger Oberamt im Gegensatz zum benachbarten Markgröninger Oberamt die Verwaltungsreform und blieb bis 1938 bestehen. 1869 wurde die Stadt über die württembergische Schwarzwaldbahn ans Schienennetz angeschlossen.

1846 wurde die Hunderasse Leonberger aus Bernhardiner, Neufundländer und Pyrenäenberghund gezüchtet.

Nach der Machtergreifung der Nationalsozialisten 1933 kam es immer häufiger zu teilweise blutigen Straßenschlachten zwischen zumeist Leonberger SA-Leuten, die in Eltingen auf KPD-Anhänger losgingen. 1938 wurde die kleinbäuerlich-proletarisch geprägte KPD-Hochburg Eltingen schließlich in das eher bürgerliche Leonberg eingemeindet.

Im gleichen Jahr (1938) wurde der Engelbergtunnel als erster Autobahntunnel Deutschlands gebaut und im Zuge der Kommunalreform das Oberamt in den Landkreis Leonberg überführt. In den Röhren des Engelbergtunnels wurden während des Zweiten Weltkrieges Flugzeugteile von bis zu 3.500 KZ-Häftlingen produziert, die im KZ-Außenlager des elsässischen KZ Natzweiler-Struthof interniert wurden. Am Südende der inzwischen stillgelegten Röhren befindet sich heute eine KZ-Gedenkstätte. Hier steht eine am 8. Mai 2005 eingeweihte und vom Tübinger Künstler Johannes Kares entworfene Namenswand.

Die Einwohnerzahl der Stadt Leonberg überschritt 1961 die Grenze von 20.000. Daraufhin stellte die Stadtverwaltung den Antrag auf Erhebung zur Großen Kreisstadt, was die baden-württembergische Landesregierung dann mit Wirkung vom 1. Oktober 1963 beschloss. Bei der Kreisreform 1973 wurde der Landkreis Leonberg aufgelöst. Der südliche Teil und mit ihm die Stadt Leonberg kamen zum Landkreis Böblingen, nördliche Teile des Kreises wurden dem Enzkreis und dem Landkreis Ludwigsburg zugeordnet. Mit der Eingliederung von drei Nachbargemeinden 1975 erreichte das Stadtgebiet seine heutige Ausdehnung.

Die Bevölkerung von Leonberg gehörte ursprünglich zum Bistum Speyer und war dem Landkapitel Grüningen im Archidiakonat Trinitatis zugeteilt (siehe Karte). Da die Stadt von Anfang an württembergisch war, hat Herzog Ulrich nach seiner Rückkehr aus dem Exil (1534) auch hier die Reformation durchgesetzt. Daher war Leonberg über mehrere Jahrhunderte hinweg eine überwiegend protestantische Stadt. 1552 wurde die Stadt Sitz eines Dekanats (siehe Kirchenbezirk Leonberg), dessen Dekanatskirche die Stadtkirche ist. Die evangelische Kirchengemeinde Leonberg wuchs nach dem Zweiten Weltkrieg infolge Zuzugs stark an und wurde daher geteilt. So entstand die Blosenbergkirchengemeinde (Kirche von 1966). Bereits 1959 wurde in der Gartenstadt ein Gemeindehaus für die dortige Bevölkerung gebaut. Die Kirchengemeinde im Stadtteil Eltingen besteht ebenfalls bereits seit der Reformation. Von dieser Gemeinde wurde in den 1960er Jahren die Kirchengemeinde Ramtel (Versöhnungskirche, erbaut 1965) abgetrennt. Alle vier Gemeinden (Stadtkirche, Blosenberg, Eltingen und Ramtel) bilden zusammen die Evangelische Gesamtkirchengemeinde Leonberg. Auch in den Stadtteilen Gebersheim, Höfingen und Warmbronn wurde infolge der frühen Zugehörigkeit zu Württemberg die Reformation eingeführt. Es gibt daher jeweils eine evangelische Kirchengemeinde, die wie die vier Gemeinden der Gesamtkirchengemeinde Leonberg zum Dekanat Leonberg innerhalb der Evangelischen Landeskirche in Württemberg gehören.
Katholiken gibt es in Leonberg erst wieder seit dem späten 19. Jahrhundert. Für sie wurde 1946 eine eigene Pfarrei errichtet und 1950 eine eigene Kirche, St. Johannes Baptista, gebaut. Zur Kirchengemeinde gehört auch der Stadtteil Warmbronn, wo es jedoch eine eigene Kirche St. Franziskus gibt. Im Stadtteil Höfingen wurde 1966 die Kirche St. Michael erbaut und 1967 zur Pfarrei erhoben. Diese betreut auch die Katholiken aus Gebersheim. Beide Kirchengemeinden bilden die Seelsorgeeinheit 6 im Dekanat Böblingen des Bistums Rottenburg-Stuttgart.

Neben den beiden großen Kirchen gibt es in Leonberg auch Freikirchen und freie Gemeinden, darunter die Evangelisch-methodistische Kirche (Pauluskirche), die Adventgemeinde, die BMG Leonberg und die Immanuel-Gemeinde Leonberg e. V. Auch die Neuapostolische Kirche ist in Leonberg vertreten sowie die Zeugen Jehovas.

In der Berliner Straße befindet sich eine Moschee der DİTİB mit 48 Gebetsplätzen.

Die Einwohnerzahlen sind Schätzungen, Volkszählungsergebnisse (¹) oder amtliche Fortschreibungen der jeweiligen Statistischen Ämter (nur Hauptwohnsitze).

¹ 

Der Gemeinderat in Leonberg hat 32 Mitglieder. Er besteht aus den gewählten ehrenamtlichen Gemeinderäten und dem ebenfalls stimmberechtigten Oberbürgermeister als Vorsitzendem. Die Kommunalwahl findet alle fünf Jahre gleichzeitig mit der Wahl des Europäischen Parlaments statt. Die Wahl am 25. Mai 2014 führte zu folgendem amtlichen Endergebnis:

An der Spitze der Stadt Leonberg wird seit 1304 ein vom Landesherrn ernannter Schultheiß erwähnt. Ab 1425 trat an dessen Stelle der Vogt, der zugleich das gesamte Amt Leonberg verwaltete. Ab 1535 gab es einen Obervogt und einen Untervogt. Beide wählten den Richter. Einen Rat gibt es seit 1312. Um 1523 hatte er acht Mitglieder.

Daneben gab es seit dem 15. Jahrhundert zwei Bürgermeister, seit 1582 drei. 1596/97 waren Claus Koch, Michael Beck und Jacob Mochel Rechnung führende Bürgermeister.

Ab 1759 stand ein Oberamtmann an der Spitze von Stadt und Amt, das von nun an Oberamt war.
1819 wurden die beiden Bereiche voneinander getrennt, Stadtoberhaupt war nun der gewählte „Stadtschultheiß“, seit 1930 Bürgermeister und mit der Erhebung zur Großen Kreisstadt am 1. Oktober 1963 lautet die Amtsbezeichnung Oberbürgermeister. Dieser wird von den Wahlberechtigten auf acht Jahre direkt gewählt. Er ist Vorsitzender des Gemeinderats. Seine Stellvertreter sind der Erste Beigeordnete mit der Amtsbezeichnung „Erster Bürgermeister“ und ein weiterer hauptamtlicher Beigeordneter mit der Amtsbezeichnung „Bürgermeister“.

Das Wappen der Stadt Leonberg zeigt in Gold einen rot bewehrten und rot bezungten, schwarzen, aufrecht schreitenden, doppelschwänzigen Löwen. Die Stadtflagge ist schwarz-gelb. Die früheste Überlieferung des Wappens stammt aus dem Jahre 1312.

Leonberg unterhält mit folgenden Städten eine Städtepartnerschaft:

Leonberg ist durch die Bundesautobahnen 8 (Karlsruhe–Stuttgart–Ulm–München) und 81 (Würzburg–Stuttgart–Singen) an das überregionale Straßennetz angeschlossen. Beide Autobahnen werden am Autobahndreieck Leonberg verknüpft, das sich südlich der Kernstadt befindet. In unmittelbarer Nähe des Dreiecks befindet sich die Anschlussstelle Leonberg-Ost. Mit dem Ausbau der A 8 zusammen mit der B 295 kam 2008 die Anschlussstelle Leonberg-West hinzu. Die B 295 verläuft derzeit noch mitten durch Leonberg und hat ein sehr hohes Verkehrsaufkommen. Für Motorradfahrer ist das Leonberger Stadtgebiet mit dem Glemstal ein beliebtes Revier, woraus zeitweise hohe Lärmbelastungen resultieren.

Den öffentlichen Personennahverkehr (ÖPNV) bedient vor allem die seit 1978 bestehende Linie S6 (Weil der Stadt–Leonberg–Stuttgart Schwabstraße) der S-Bahn Stuttgart, die auf der Schwarzwaldbahn verkehrt. Sie ist eine Nahverkehrsverbindung in die Landeshauptstadt. Neben dem Bahnhof Leonberg gibt es Haltepunkte im Stadtteil Höfingen und im Stadtteil Silberberg (Haltepunkt "Rutesheim"). Ferner verkehren im Stadtgebiet zahlreiche Buslinien. Alle Linien verkehren zu einheitlichen Preisen innerhalb des Verkehrs- und Tarifverbund Stuttgart (VVS).

In der Kernstadt von Leonberg gilt seit dem 1. März 2008 (ursprünglich 1. Juli 2007, Startzeitpunkt wegen fehlender Verwaltungsvorschriften verschoben) die Feinstaubplakettenpflicht. Nur Fahrzeuge, die mindestens der Schadstoffgruppe 2 angehören, durften ab diesem Zeitpunkt in oder durch das Stadtgebiet fahren. Fahrzeuge der Schadstoffgruppe 1 (ohne Plakette) unterliegen seitdem einem Fahrverbot.

Seit 1. Januar 2012 wurde das Fahrverbot nach der Kennzeichnungsverordnung auf Fahrzeuge der Schadstoffgruppe 2 (rote Plakette), seit 1. Januar 2013 auf Fahrzeuge der Schadstoffgruppe 3 (gelbe Plakette) ausgeweitet. Am 2. Dezember 2013 wurde eine regionale Umweltzone Leonberg/Hemmingen und Umgebung gebildet (Übersicht).

Neben zahlreich vorhandenen Läden befindet sich im Stadtzentrum das Leo-Center, ein über 90 Geschäfte verfügendes Einkaufszentrum. Es wurde 1973 in Betrieb genommen. Quer gegenüber auf der anderen Seite der Kreuzung Römer-/Eltinger Straße befindet sich die Römergalerie, in der eine große Anzahl Läden, Restaurants sowie Büros und Arztpraxen untergebracht sind. Weitere Einkaufszonen findet man in der Altstadt rund um den Marktplatz, in Eltingen (Carl-Schmincke- und Poststraße), und an der Ecke Post-/Römerstraße (Baumarkt, Lebensmittelmärkte, Babyfachmarkt, Sportgeschäft).

Die Leonberger Bausparkasse wurde 1924 als "Christlicher Notbund zur gegenseitigen Hilfe" von Christian Röckle in Leonberg gegründet. Seit Übernahme durch die Wüstenrot AG 2001 existiert die Leonberger Bausparkasse nicht mehr. Ihre Gebäude wurden abgerissen, wobei die beiden höchsten Gebäude am 30. Mai 2009 gesprengt wurden.

Die Robert Bosch GmbH betreibt in Leonberg ein Entwicklungszentrum mit rund 1700 Arbeitsplätzen, das aus der Übernahme der Firma Motometer in den 1990er Jahren entstand. Es sind dort die Geschäftsbereiche "Automotive Electronics", "Chassis Systems Control" und "Car Multimedia" angesiedelt.

Leonberg ist Stammsitz des Familienunternehmens GEZE, einem Anbieter von Türtechnik sowie Sicherheits- und Lüftungssystemen, das in Leonberg rund 1000, weltweit 2700 Mitarbeiter beschäftigt (Stand 2016).

1952 wurde in Leonberg die LEWA gegründet. Sie ist ein weltweit tätiger Hersteller von Dosierpumpen, Prozess-Membranpumpen und Dosier- und Mischanlagen für die Prozessindustrie. Sie beschäftigt in Leonberg rund 560 Mitarbeiter, weltweit ca. 1000 (Stand 2015).

Leonberg ist seit 1953 Sitz der Brückner Textilmaschinenbau.

Der Automobilhersteller und auf Porsche-Fahrzeuge spezialisierte Tuningbetrieb Gemballa Automobiltechnik beschäftigt etwa 30 Mitarbeiter.

Im Jahre 1994 übersiedelte die traditionelle Klavierfabrik Pfeiffer von Stuttgart nach Leonberg.

In Leonberg erscheint als Tageszeitung die Leonberger Kreiszeitung. Seit Oktober 2005 hat die Leonberger Kreiszeitung den Mantel der Stuttgarter Zeitung (zuvor von den Stuttgarter Nachrichten) übernommen und liefert aus eigener Redaktion den Lokalteil für die Stadt Leonberg, die Teilorte und den Altkreis Leonberg zu.

Leonberg ist ein Weinort des Anbaugebietes Württemberg und zählt zum Bereich Remstal-Stuttgart.
Hauptanbaugebiete sind die südlich des Autobahndreiecks gelegene Feinau sowie der glemsaufwärts gelegene Ehrenberg.

Leonberg hat ein Finanzamt und ein Amtsgericht, das zum Landgerichts- und Oberlandesgerichtsbezirk Stuttgart gehört. Ferner befindet sich hier eine Außenstelle des Landratsamts Böblingen.

Die Stadt ist Sitz des Kirchenbezirks Leonberg der Evangelischen Landeskirche in Württemberg.

In Leonberg sind alle Schultypen vorhanden. Mit dem Albert-Schweitzer-Gymnasium und dem Johannes-Kepler-Gymnasium gibt es zwei Allgemeinbildende Gymnasien, sowie mit Wirtschafts- und Technischem Gymnasium am Beruflichen Schulzentrum auch zwei berufliche Gymnasien. Weiterhin bestehen die Gerhart-Hauptmann-Realschule, die Ostertag-Realschule, die Pestalozzischule (Förderschule), die August-Lämmle- und die Schellingschule (Grund- und Werkrealschulen), sowie die reinen Grundschulen in der Kernstadt (Mörikeschule, Sophie-Scholl-Schule und Spitalschule) und in den Stadtteilen Höfingen, Gebersheim und Warmbronn.

Der Landkreis Böblingen ist Schulträger des Beruflichen Schulzentrums (diverse Ausbildungsberufe, Berufsfachschulen, Berufskolleg und berufliche Gymnasien) sowie der Karl-Georg-Haldenwang-Schule für Geistigbehinderte mit Schulkindergarten.

Die private Evangelische Fachschule für Altenpflege rundet das schulische Angebot in Leonberg ab.

Das alte Eltinger Freibad wurde 1990 umgebaut zum Leobad, einem großen Freizeit- und Sportbad, in dem bis 2010 über vier Millionen Besucher gezählt wurden.

Ein kleines Familienbad wird im Stadtteil Höfingen durch den Verein Bädle e. V. betrieben.

Das 1972 fertiggestellte Sportzentrum mit Sporthalle und Hallenbad musste im August 2011 wegen massiver Sicherheitsmängel in der Gebäudetechnik geschlossen werden. Bei einem Bürgerentscheid am 25. März 2012 wurde entschieden, dass eine Sanierung erfolgen soll. Zuvor hatte der Gemeinderat einen Abriss und Neubau beschlossen. Nach der für rund 12 Millionen Euro durchgeführten Sanierung wurde das Sportzentrum am 2. Februar 2014 wieder eröffnet.

Das Theater im Spitalhof Leonberg, das sich innerhalb der Spitalschule befindet, ist eine Musik- und Theaterbühne für Kleinkunst, Musik sowie Kinder- und Jugendtheater. Regelmäßige Theaterdarbietungen verschiedener Tourneebühnen sind in der Stadthalle Leonberg zu sehen.

Der 1840 gegründete "Liederkranz Leonberg" betreibt Chorsingen und besitzt ein eigenes Sängerheim. Der "Musikverein Lyra Eltingen" wurde 1897 gegründet.
Ein weiterer Musikverein ist die "Stadtkapelle Leonberg". Das "Sinfonieorchester Leonberg" wurde 1970 als Jugendsinfonieorchester gegründet. Heute besteht es aus 70 Musikern und wird vom Dirigenten Alexander Adiarte geleitet.

Musikalische Ausbildungsstätten in Leonberg sind die "Jugendmusikschule", die in Kooperation mit dem Musikverein Lyra Eltingen, dem "Musikverein Höfingen" und dem Musikverein Stadtkapelle Leonberg steht, und die Musikschule "Villa Musica", die im Verbund mit dem Liederkranz Leonberg zusammenarbeitet.


Der Pomeranzengarten ist der einzige in Deutschland erhaltene Terrassengarten aus der Zeit der Hochrenaissance. Er wurde 1609 im Auftrag der Herzogin Sibylla von Württemberg angelegt, als das Schloss zum Witwensitz württembergischer Herzoginnen umfunktioniert wurde. Ab 1742 wurde der Pomeranzengarten für den Obst- und Gemüsebau genutzt, bis man ihn ab 1980 nach den Originalplänen Heinrich Schickhardts restaurierte.

Der "Stadtpark" liegt zwischen Leonberg und Eltingen und wurde auf dem Gelände eines Gipswerks angelegt, das 1977 seinen Betrieb einstellte. Gipssteinbrüche wurden als Seen in die Anlagen integriert. Viele Skulpturen sind im Stadtpark aufgestellt. Im Süden des Parks wurde ein Friedensmahnmal errichtet, an dessen Eingang seit 1990 ein Segment der Berliner Mauer steht, das zuvor an der Grenze des Leonberger Partnerbezirks Berlin-Neukölln stand.


Die Stadt Leonberg hat folgenden Personen das Ehrenbürgerrecht verliehen:





</doc>
<doc id="2969" url="https://de.wikipedia.org/wiki?curid=2969" title="Linda Hamilton">
Linda Hamilton

Linda Carroll Hamilton (* 26. September 1956 in Salisbury, Maryland) ist eine US-amerikanische Schauspielerin. Sie wurde vor allem durch die Rolle der "Sarah Connor" in den Filmen "Terminator" und "Terminator 2 – Tag der Abrechnung" bekannt.

Linda Hamilton studierte in New York City Schauspiel und besuchte das Actors Studio von Lee Strasberg.

Als ausgebildete Schauspielerin spielte sie zunächst in kleineren Low-Budget-Filmen, bevor sie 1980 für eine der Hauptrollen in der Seifenoper "Secrets of Midland Heights" engagiert wurde. Danach spielte sie im Film "TAG: Assassination Game" und in der Stephen-King-Verfilmung "Kinder des Zorns" eine der Hauptrollen. Die Rolle der "Sarah Connor" in James Camerons Erfolgsfilm "Terminator" erhöhte ab 1984 ihren Bekanntheitsgrad beträchtlich. 1986 spielte sie in "Black Moon" neben Tommy Lee Jones die weibliche Hauptrolle.

Danach wirkte sie wieder in Fernsehserien mit und war u. a. auch in "Mord ist ihr Hobby" zu sehen. In der Fernsehserie "Die Schöne und das Biest" spielte sie zwischen 1987 und 1989 die Rolle der "Catherine Chandler". Dies brachte ihr eine Emmy- und eine Golden-Globe-Nominierung ein. 1990 spielte sie in der Filmkomödie "Mr. Destiny" neben James Belushi und Michael Caine. Im Jahr darauf übernahm sie in "Terminator 2 – Tag der Abrechnung" erneut die Rolle der "Sarah Connor". Der Film brach Rekorde und wurde zu einem von Hamiltons größten Erfolgen. 1990 wurde sie zudem vom Magazin People in die Liste der 50 schönsten Menschen der Welt gewählt.

1995 spielte sie in dem Fernsehfilm "A Mother’s Prayer" neben Bruce Dern und Kate Nelligan die Hauptrolle einer an AIDS erkrankten Mutter. Dafür war sie in der Kategorie als beste dramatische Darstellung für einen CableACE Award und 1996 für einen weiteren Golden Globe nominiert. 1997 spielte sie im Actionthriller "Die Verschwörung im Schatten" neben Charlie Sheen und auch im Katastrophenfilm "Dante’s Peak" neben Pierce Brosnan jeweils die weibliche Hauptrolle. Für ihre Rolle in "Dante’s Peak" erhielt sie einen Blockbuster Entertainment Award.

Von da an war Hamilton ausschließlich in Serien wie "Frasier" und "Immer wieder Jim" sowie in zahlreichen Fernsehfilmen zu sehen. 2009 sprach sie im vierten Teil der Terminator-Reihe "" erneut die Rolle der "Sarah Connor". Von 2010 bis 2012 spielte sie in zwölf Folgen der Fernsehserie "Chuck" die Rolle der lang vermissten Mutter von Chuck und Ellie. 2010 hatte sie außerdem in der Serie "Weeds – Kleine Deals unter Nachbarn" einen Gastauftritt.

Linda Hamilton ist die Tochter eines Arztes (Carroll Stanford Hamilton), der bei einem Autounfall verstarb, als sie fünf Jahre alt war. Sie hat eine Zwillingsschwester namens Leslie (die in "Terminator 2 – Tag der Abrechnung" zwei kurze Auftritte als Lindas Doppelgängerin hat), eine ältere Schwester und einen jüngeren Bruder. Sie wuchs nach eigener Aussage in einer „sehr öden angelsächsischen Protestantenfamilie“ auf und besuchte zusammen mit ihrer Zwillingsschwester die Highschool in Salisbury. Danach studierte sie zwei Jahre lang am "Washington College" in Chestertown.

Hamilton war zweimal verheiratet, zunächst von 1982 bis 1989 mit dem Schauspieler Bruce Abbott, mit dem sie einen Sohn, Dalton Bruce, hat. Dieser wurde am 4. Oktober 1989 geboren und verkörperte John Connor als Baby im Film "Terminator 2". Im Jahr 1997 heiratete sie James Cameron, mit dem sie eine Tochter hat. Mit der Scheidung von Cameron 1999 erhielt Hamilton 50 Millionen Dollar zugesprochen.

Im Oktober 2005 war Hamilton zu Gast bei "Larry King Live", wo sie gestand, dass sie an einer Bipolaren Störung leide, die auch der Grund für die Scheidung von Abbott gewesen sei.

2006 berichtete sie in einem Interview mit Marshall Julius von blockbuster.co.uk über die Fallstricke und Gefahren in Hollywood, wie etwa beim Dreh eines Feuergefechts in einem Fahrstuhl für "Terminator 2 – Tag der Abrechnung". Dabei hatte Hamilton vergessen, ihren Gehörschutz zu tragen und erlitt ein Knalltrauma, das ihr Gehör auf einem Ohr bis heute beeinträchtigt. Außerdem erlitt sie bei den Dreharbeiten zu dem Film eine posttraumatische Belastungsstörung wie sie bei so genannten Kriegszitterern auftritt. Von letzterer konnte sie sich mittlerweile vollständig erholen.





</doc>
<doc id="2970" url="https://de.wikipedia.org/wiki?curid=2970" title="Lorraine Bracco">
Lorraine Bracco

Lorraine Bracco (* 2. Oktober 1954 in New York City) ist eine US-amerikanische Schauspielerin.

Bracco wurde in Brooklyn als Tochter von Eileen (geborene Molyneux 1926–2010) und Salvatore Bracco, Sr. geboren und wuchs im angrenzenden Westbury auf Long Island auf. Ihre Mutter war von ihrer Herkunft her Französin, die in England aufwuchs. Ihr Vater war Italoamerikaner, der während des Zweiten Weltkriegs als Soldat in Europa diente und dort Eileen traf und mit in die USA nahm. 1972 schloss sie die Hicksville Highschool ab. Zwei Jahre später, 1974, zog Bracco nach Frankreich, wo sie als Model für Jean-Paul Gaultier arbeitete. Dort machte sie Bekanntschaft mit der Regisseurin und Autorin Lina Wertmüller, die sie für eine Filmproduktion engagierte. Ihre drei Jahre jüngere Schwester Elizabeth Bracco wurde ebenfalls Schauspielerin. Außerdem hat sie noch einen Bruder, Salvatore Jr.

Die US-Amerikanerin wirkte daraufhin in mehreren französischen Filmen mit, auch verdingte sie sich als Discjockey bei Radio Luxembourg. Die Darstellung der Mafia-Ehefrau Karen Hill in Martin Scorseses "Good Fellas – Drei Jahrzehnte in der Mafia" (1990) brachte ihr 1991 sowohl eine Oscar- als auch eine Golden Globe-Nominierung als beste Nebendarstellerin ein. 

Lorraine Bracco gehörte zudem zur Stammbesetzung der erfolgreichen Mafia-Serie "Die Sopranos". Ursprünglich hätte sie Tony Sopranos Ehefrau Carmella spielen sollen. Diese Rolle übernahm schließlich Edie Falco. Bracco verkörperte daraufhin die Psychiaterin Dr. Jennifer Melfi, zu der Tony Soprano eine besondere Beziehung hegt. Aufgrund dieser Rolle erhielt sie zahlreiche Fernsehpreisnominierungen und einige Auszeichnungen.

Bracco war bislang zweimal verheiratet: Die erste Ehe führte sie mit Daniel Guerard in den späten 1970er und frühen 1980er Jahren. Aus dieser Ehe stammt eine gemeinsame Tochter. Aus einer Beziehung mit Harvey Keitel hat sie eine weitere Tochter. Nach der Trennung von Keitel war Bracco von 1994 bis 2002 mit dem Regisseur und Schauspieler Edward James Olmos verheiratet.





</doc>
<doc id="2971" url="https://de.wikipedia.org/wiki?curid=2971" title="Louis de Broglie">
Louis de Broglie

Louis-Victor Pierre Raymond de Broglie [] (* 15. August 1892 in Dieppe, Normandie; † 19. März 1987 in Louveciennes, Département Yvelines) war ein französischer Physiker. Er gehörte zur französischen Adelsfamilie der Broglies und war jüngerer Bruder des Experimentalphysikers Maurice de Broglie.

De Broglie gilt als einer der bedeutendsten Physiker des 20. Jahrhunderts, für seine Entdeckung der Wellennatur des Elektrons (Welle-Teilchen-Dualismus) in seiner Dissertation und der daraus resultierenden Theorie der Materiewellen erhielt er 1929 den Nobelpreis für Physik.

Louis-Victor de Broglie, viertes Kind von Victor de Broglie, 5. Herzog de Broglie und Pauline d'Armaillé wurde 1892 in Dieppe geboren. Louis-Victor besuchte das Lycée Janson de Sailly in Paris. Im Jahr 1960 folgte er seinem kinderlosen Bruder Maurice als Herzog nach.

Während seines Studiums an der Pariser Sorbonne befasste sich Louis-Victor zunächst mit der Philosophie und der Geschichte, insbesondere mit Rechtsgeschichte und politischer Geschichte des Mittelalters. Nebenbei las er Werke von Henri Poincaré wie z. B. "Wissenschaft und Hypothese" und "Der Wert der Wissenschaft". 1910 schloss er sein erstes Studium mit dem Lizenziat ab.

Auf Anregung seines siebzehn Jahre älteren Bruders Maurice, eines promovierten Physikers, studierte Louis de Broglie ab 1911 Mathematik und Physik. Maurice, der sich nach dem Tode des Vaters 1906 um Erziehung und Entwicklung seines jüngeren Bruders gekümmert hatte, versorgte Louis nun mit den Texten der Referate und Diskussionen der ersten Solvay-Konferenz, die 1911 in Brüssel stattfand. Durch diese Aufzeichnungen kam Louis de Broglie das erste Mal in intensiven Kontakt mit der Quantenphysik, die sein späteres physikalisches Schaffen prägen sollte.

Durch den Ersten Weltkrieg musste de Broglie sein Studium mehrere Jahre unterbrechen. Er wurde Nachrichtenoffizier und verbrachte den größten Teil seiner Dienstzeit in der funktelegraphischen Station des Eiffelturms. Während seines Militärdienstes befasste sich de Broglie mit der Elektrotechnik und dem Nachrichtenwesen sowie mit der Ausbildung von elektrotechnischem Personal.

Nach der Entlassung aus dem Heeresdienst 1919 setzte de Broglie seine Studien fort und wurde Mitarbeiter im Privatlabor seines Bruders, in dem er vorrangig über Röntgenspektroskopie und den Photoeffekt arbeitete. Ende des Jahres 1923 erschienen de Broglies erste Abhandlungen zur Wellenmechanik.

1924 schloss de Broglie sein Studium mit der berühmt gewordenen Dissertation ab, in der er vermutete, dass der Welle-Teilchen-Dualismus auf jegliche feste Materie anzuwenden sei.
Diese kühne Idee wurde 1926 und 1927 vom Institut de France ausgezeichnet. 1929 folgten für die Entdeckung der Wellennatur der Elektronen die begehrte Medaille Henri Poincaré der Académie des sciences und der Nobelpreis für Physik.

1927 war de Broglie einer der Teilnehmer des 5. Solvay-Kongresses in Brüssel. 1929 wurde er zum Professor für Theoretische Physik am Institut Henri Poincaré in Paris berufen, wechselte jedoch 1932 an die Sorbonne, wo er bis 1962 lehrte. 1933 wurde de Broglie Mitglied der .

Neben seinen Arbeiten auf physikalischem Gebiet veröffentlichte de Broglie vor allem während seiner Zeit am Institut Henri Poincaré einige philosophische und problemgeschichtliche Aufsätze. 1938 erhielt er die Max-Planck-Medaille.

Während der Kämpfe zwischen Frankreich und Deutschland im Zweiten Weltkrieg wurde de Broglie mit der dokumentarischen Sammlung der in den USA veröffentlichten Arbeiten über Nachrichtenübertragung betraut. 1941 veröffentlichte er in diesem Zusammenhang ein Buch über Hochfrequenztechnik.

Der Patriotismus de Broglies während der deutschen Okkupation kommt in seiner Gedenkvorlesung für den französischen Gelehrten André-Marie Ampère im September 1940 zum Ausdruck:

1944 wurde Louis de Broglie Mitglied der Académie française und nach dem Zweiten Weltkrieg Berater der französischen Atomenergiekommission.

Louis-Victor de Broglie starb am 19. März 1987 in Louveciennes bei Paris.

In seinen frühen Forschungen, vor allem während der Arbeit im physikalischen Labor seines Bruders Maurice, beschäftigte de Broglie sich mit dem lichtelektrischen Effekt von Röntgenstrahlen. 1928 veröffentlichte er zusammen mit seinem Bruder ein Buch über Röntgenphysik. Anfang der 20er Jahre widmete er sich der Quantentheorie. Es gelang ihm, die Quantenformel Max Plancks aus der Teilchentheorie des Lichts abzuleiten.

1924 schloss de Broglie sein Studium mit der berühmt gewordenen Dissertation (Untersuchungen zur Quantentheorie) ab.
Nach gründlicher Analyse der von Albert Einstein gefundenen Äquivalenz von Masse und Energie, die in der Formel formula_1 ihren Ausdruck findet, und der Erkenntnisse der Atomphysik kommt de Broglie zu der Überzeugung, Energie sei wie Masse in Form von Teilchen in kleinen Raumbereichen lokalisiert. Der Quantencharakter der Materie, wie er sich beispielsweise in den Atomspektren zeigt, sei aber nur zu erklären,
wenn jeder Masse formula_2 nach der von Max Planck postulierten Beziehung formula_3 eine Frequenz formula_4
zugeordnet wird. Diese für das Teilchen charakterisierende Frequenz ist nach Ansicht von de Broglie nicht auf das Teilchenvolumen beschränkt, sondern ist in Form einer das Teilchen begleitenden Welle auch in einem großen Raumbereich präsent.
De Broglie nennt diese Begleitwelle Phasenwelle, weil Teilchen und Welle über die Phase am Ort des Teilchen aneinander gekoppelt sind. Unter dieser Bedingung erfüllen sowohl Teilchen als auch Welle die Transformationsgesetze der speziellen Relativitätstheorie.

Der Welle-Teilchen-Dualismus, der damals nur für Photonen bekannt war, ist nach Meinung von de Broglie ein Wesensmerkmal nicht nur der Photonen, sondern auch der Materie. Auch einem klassischen Teilchen – z. B. einem Elektron – können somit Welleneigenschaften zugesprochen werden. Im Ruhesystem des Teilchens ist die Wellenlänge der Phasenwelle unendlich groß. Ist das Teilchen in Bewegung, ergibt sich bei Anwendung der Lorentz-Transformation eine Modulation der Welle mit der Wellenlänge

Der Prüfungsausschuss der Pariser Sorbonne, zu dem auch die bekannten Physiker Jean-Baptiste Perrin und Paul Langevin gehörten, war sich unsicher, wie er auf diesen kühnen und experimentell nicht bestätigten Vorschlag reagieren sollte. De Broglie selbst äußerte in Bezug auf die Skepsis Paul Langevins, dieser sei ("vermutlich ein wenig erstaunt über die Neuheit meiner Ideen".)

Langevin bat de Broglie um ein zweites Exemplar seiner Arbeit und schickte es an Albert Einstein, der wiederum Max Born informierte. Einstein zeigte sich tief beeindruckt und erklärte später, er glaube, dass de Broglies Doktorarbeit den ersten schwachen Lichtstrahl auf dieses leidigste unter den physikalischen Rätseln werfe. Max Planck berichtete später, wie ungewöhnlich er de Broglies neue Gedanken zunächst empfand:

Der Prüfungsausschuss akzeptierte schließlich de Broglies Dissertation. Die Versuche von Clinton Davisson und Lester Germer 1927 mit der Elektronenbeugungsröhre und von George Paget Thomson 1928 bestätigten den Wellencharakter der Elektronen auch experimentell.

Auf der Grundlage seiner Erkenntnis, dass alle Teilchen auch Welleneigenschaften besitzen, arbeitete de Broglie nach seiner Promotion an der Verbesserung des Bohr-Sommerfeld’schen Atommodells. Er ordnete jedem Materieteilchen eine so genannte "Materiewelle" zu, die sich auf den bohrschen Bahnen ausbreitet. De Broglie zeigte auf diesem Weg die Beziehung zwischen der Bahnstabilität und dem Bahnumfang der Elektronen im bohrschen Atommodell auf:

d. h. ein Elektron kann sich nur dann ohne Energieverlust um den Atomkern bewegen, wenn sein Bahnumfang ein ganzzahliges Vielfaches seiner Wellenlänge ist. 1926 machte sich de Broglie an die Formulierung einer Differentialgleichung, die das Verhalten der Elektronen beschrieb. Diese Ansätze lieferten wichtige Anregungen für Erwin Schrödinger, der noch im selben Jahr seine partielle Differentialgleichung (Schrödingergleichung) aufstellte. Diese konnte das Verhalten der Elektronen in den stationären Energiezuständen darstellen.

In weiteren Arbeiten widmete de Broglie sich der Quantenfeldtheorie der Elementarteilchen und Wellengleichungen für Teilchen mit höherem Spin.

Zunächst versuchte Louis de Broglie, die Wellenmechanik der Teilchen deterministisch zu erklären, und somit sämtliche Vorgänge exakt berechenbar darzustellen. Nach dem fünften Solvay-Kongress 1927, auf dem er rege Diskussionen mit anderen berühmten Physikern der Zeit wie Albert Einstein, Niels Bohr, Max Planck u .a. führte, gab er den deterministischen Ansatz auf und näherte sich der Wahrscheinlichkeitsinterpretation. Erst 1951 näherte sich de Broglie durch die Arbeiten von David Bohm und Jean-Pierre Vigier wieder einer "kausalen und konkreten Interpretation der Wellenmechanik". →De-Broglie-Bohm-Theorie

Durch de Broglies philosophische und problemgeschichtliche Aufsätze, die vor allem aus seiner Zeit am Institut Henri Poincaré in Paris stammen, wird deutlich, dass de Broglies Beschäftigung mit physikalischen Grundlagenproblemen oft auf seinem historischen Interesse gründete. So ging z. B. seine Idee der Materiewellen letztlich aus dem intensiven Studium der Geschichte der Lichttheorie hervor.





</doc>
<doc id="2973" url="https://de.wikipedia.org/wiki?curid=2973" title="Lise Meitner">
Lise Meitner

<onlyinclude>

Lise Meitner (* 7. November 1878 in Wien als Elise Meitner; † 27. Oktober 1968 in Cambridge, Vereinigtes Königreich) war eine bedeutende österreichische Kernphysikerin. Unter anderem veröffentlichte sie im Februar 1939 zusammen mit ihrem Neffen Otto Frisch die erste physikalisch-theoretische Erklärung der Kernspaltung, die ihr Kollege Otto Hahn und dessen Assistent Fritz Straßmann am 17. Dezember 1938 entdeckt und mit radiochemischen Methoden nachgewiesen hatten. </onlyinclude>Lise Meitner feierte ihren Geburtstag stets am 7. November, obwohl im Geburtsregister der Israelitischen Kultusgemeinde Wien der 17. November 1878 angegeben worden war.

Meitner wurde am 7. November 1878 in Wien Leopoldstadt (2. Wiener Gemeindebezirk) geboren. Sie war die dritte Tochter des aus der Gegend von Mährisch Weißkirchen stammenden jüdischen Rechtsanwaltes Philipp Meitner (1839–1910) und seiner Frau Hedwig Meitner-Skovran, die 1875 geheiratet hatten. Ihre Eltern wohnten damals in der Kaiser-Joseph-Straße Nr. 27, der heutigen Heinestraße. Ihr Vater betrieb dort, bevor die Familie an „bessere Adressen“ übersiedelte, seine Kanzlei als Hof- und Gerichtsadvokat. Lise Meitner wurde protestantisch erzogen. 1908 wurde sie durch die Taufe in die evangelische Kirche aufgenommen.

Ihre Schullaufbahn absolvierte sie auf einer Bürgerschule, da an den Gymnasien Mädchen nicht zugelassen wurden. Nach dem Schulabschluss legte Lise Meitner das Lehrerinnen-Examen für Französisch ab. Außerdem bereitete sie sich im Selbststudium auf die Matura vor und legte die Reifeprüfung 1901 im Alter von 22 Jahren am Akademischen Gymnasium Wien ab, wo sie als "gewählten Beruf" die "realistischen Studien" der Philosophie angab.

Durch ihr Abschlusszeugnis berechtigt, begann Lise Meitner 1901 ihr Studium der Physik, Mathematik und Philosophie an der Universität Wien. Ihr wichtigster akademischer Lehrer dort wurde Ludwig Boltzmann. Bereits in den ersten Jahren beschäftigte sie sich mit Fragestellungen der Radioaktivität. Sie wurde 1906 als zweite Frau an der Wiener Universität im Hauptfach Physik über "Prüfung einer Formel Maxwells" (veröffentlicht unter dem Titel "Wärmeleitung in inhomogenen Körpern") bei Franz-Serafin Exner promoviert. Anschließend bewarb sie sich bei Marie Curie in Paris, allerdings erfolglos. Das erste Jahr nach ihrer Promotion arbeitete sie am Institut für Theoretische Physik in Wien.

Im Jahr 1907 ging sie zur weiteren wissenschaftlichen Ausbildung nach Berlin, wo sie vor allem Vorlesungen bei Max Planck hören wollte. Dort traf sie erstmals auf den jungen Chemiker Otto Hahn, mit dem sie die folgenden 30 Jahre zusammenarbeiten sollte. Sie arbeitete mit Hahn – wie er auch – als „unbezahlter Gast“ in dessen Arbeitsraum, einer ehemaligen „Holzwerkstatt“, im Chemischen Institut der Friedrich-Wilhelms-Universität in der Hessischen Straße. Da im damaligen Preußen Frauen noch nicht studieren durften, musste sie das Gebäude immer durch den Hintereingang betreten und durfte die Vorlesungsräume und Experimentierräume der Studenten nicht betreten. Dieses Verbot fiel erst 1909, nachdem das Frauenstudium in Preußen offiziell eingeführt worden war.

Otto Hahn entdeckte 1909 den radioaktiven Rückstoß und mit der sich daran anschließenden „Rückstoßmethode“ fanden Hahn und Lise Meitner in den Folgejahren auch diverse radioaktive Nuklide. Durch diese Erfolge machte Lise Meitner sich in der Physik einen Namen und lernte unter anderem Albert Einstein und Marie Curie persönlich kennen. Von 1912 bis 1915 war sie inoffizielle Assistentin bei Max Planck.

1912 verbesserten sich die Arbeitsbedingungen von Hahn und Meitner deutlich, als sie ihre Forschungen in der von Hahn aufgebauten Forschungsabteilung Radioaktivität des neu gegründeten Kaiser-Wilhelm-Instituts für Chemie der Kaiser-Wilhelm-Gesellschaft in Berlin-Dahlem (heutiger Hahn-Meitner-Bau an der Thielallee, Institut der Freien Universität Berlin) fortsetzen konnten. Meitner arbeitete zunächst unentgeltlich weiter, wurde jedoch 1913 wissenschaftliches Mitglied des Kaiser-Wilhelm-Instituts für Chemie. Zumindest zu Beginn des Ersten Weltkriegs zeigte sie sich ebenso von Kriegsbegeisterung ergriffen wie nahezu alle ihre damaligen Kollegen.

So hatte Hahn zusammen mit James Franck und Gustav Hertz im Auftrag durch Fritz Haber am 22. April 1915 persönlich den erstmaligen Einsatz von Chlorgas in der Zweiten Flandernschlacht überwacht. Die Giftgaswolke überraschte damals noch den Gegner, etwa 5000 Soldaten starben und weitere etwa 10.000 wurden kampfunfähig verletzt. Drei Tage darauf schrieb Meitner an Hahn: „Ich beglückwünsche Sie zu dem schönen Erfolg bei Ypern“. Meitner war allerdings selbst nicht an Forschung oder Entwicklung chemischer Kampfstoffe beteiligt. Sie ließ sich zur Röntgenassistentin und Krankenpflegerin ausbilden und war ab Juli 1915 zunächst als Röntgenschwester der österreichischen Armee in einem Lazarett an der Ostfront eingesetzt.

Bereits im Oktober 1916 kehrte sie nach Berlin in das Institut zurück und arbeitete erneut gemeinsam mit Hahn, der im Dezember 1916 nach Berlin versetzt worden war. 1917 entdeckten Hahn und Meitner das chemische Isotop Protactinium 231, die langlebige Form des Elements Nr. 91, das mit dem schon 1913 von Kasimir Fajans und Oswald Helmuth Göhring entdeckten kurzlebigen Pa-Isotop Brevium in Konkurrenz stand. (Im Jahre 1949 wurde das neue Element Nr. 91 von der IUPAC endgültig Protactinium genannt und Hahn und Meitner als alleinige Entdecker bestätigt).

1918 erhielt Lise Meitner erstmals eine eigene radiophysikalische Abteilung mit angemessenem Gehalt und wurde Leiterin der physikalisch-radioaktiven Abteilung des Kaiser-Wilhelm-Instituts für Chemie. 1922 habilitierte sie sich und bekam dadurch das Recht, als Dozentin zu arbeiten. 1926 wurde sie außerordentliche Professorin für experimentelle Kernphysik an der Berliner Universität, Deutschlands erste Professorin für Physik.

Anfang 1933 war Meitner wie viele andere noch zuversichtlich, dass die Folgen der Machtübernahme durch die NSDAP glimpflich bleiben würden. Derartige Zeiten des Umbruchs seien zunächst unvermeidlich mit allen möglichen Wirren verbunden, nun komme es auf vernünftige Zurückhaltung an. Hitlers im Radio übertragene Antrittsrede als Reichskanzler habe doch „sehr moderat geklungen, taktvoll und versöhnlich“. Aber als Folge des Gesetzes zur Wiederherstellung des Berufsbeamtentums von Anfang April 1933 wurde Meitner aufgrund ihrer jüdischen Abstammung die Lehrbefugnis entzogen, sie konnte ihre Arbeit an Bestrahlungsexperimenten mit Neutronen lediglich am (nicht staatlichen) Kaiser-Wilhelm-Institut fortsetzen. 1938, als Deutschland Österreich annektierte, wurde Lise Meitner deutsche Staatsbürgerin und war dadurch als gebürtige Jüdin in besonderer Weise gefährdet.

Otto Hahn hatte große Sorge um ihre Sicherheit und bereitete daher zusammen mit dem niederländischen Chemiker Dirk Coster ihre illegale Ausreise ins Exil vor, die am 13. Juli gelang. Über die Niederlande und Dänemark kam sie nach Schweden, wo sie ihre Forschungen bis 1946 am Nobel-Institut fortsetzte. Hahn und Meitner korrespondierten weiter miteinander. Ende Dezember 1938 schrieb ihr Hahn von einem Vorgang, den er, zusammen mit seinem Assistenten Fritz Straßmann, aufgrund äußerst sorgfältiger radiochemischer Methoden entdeckt hatte und den er als „Zerplatzen“ des Urankerns bezeichnete.

Otto Hahn fragte Lise Meitner in einem Brief zum „Zerplatzen“:

Durch Otto Hahn weiterhin über alle in Berlin vollzogenen Versuche auf dem Laufenden gehalten (er hatte die Physiker in seinem Institut nicht informiert und Lise Meitner als einzige über alle Experimente und Ergebnisse brieflich unterrichtet), konnte Lise Meitner im Februar 1939 mit ihrem Neffen, dem Kernphysiker Otto Robert Frisch, in dem Aufsatz „Disintegration of Uranium by Neutrons: a New Type of Nuclear Reaction“ eine erste physikalisch-theoretische Deutung (siehe auch Ida Noddack-Tacke) für das von Otto Hahn formulierte „Zerplatzen“ des Uran-Atomkerns geben. Frisch prägte dabei den Begriff „nuclear fission“ (Kernspaltung), der in der Folgezeit international anerkannt wurde.

Die beiden Bruchstücke (Atomkerne), die bei der Spaltung entstehen, haben zusammen eine geringere Masse als der ursprüngliche Uranatomkern. Aus dieser Massendifferenz errechneten Lise Meitner und Otto Robert Frisch mit Einsteins Formel E=mc² die bei der Spaltung freiwerdende Energie von etwa 200 Millionen Elektronenvolt pro gespaltenem Atomkern. 

In einer späteren Würdigung schrieb Lise Meitner:
Und in einem Fernsehinterview (ARD, 8. März 1959) ergänzte sie:
Fritz Straßmann erwiderte in demselben Interview präzisierend:
Und in ihrem Artikel "Otto Hahn – der Entdecker der Uranspaltung" (1955) hob Lise Meitner explizit hervor:
Auch Otto Robert Frisch betonte gelegentlich, um Mißverständnissen vorzubeugen:
Meitner, inzwischen überzeugte Pazifistin, weigerte sich, Forschungsaufträge für den Bau einer Atombombe anzunehmen, obwohl sie von den USA immer wieder dazu aufgefordert wurde. Sie zog es vor, während des Zweiten Weltkrieges in Schweden zu bleiben.

Für die Entdeckung und den radiochemischen Nachweis der Kernspaltung wurde Otto Hahn 1945 der Nobelpreis für Chemie für das Jahr 1944 verliehen (überreicht wurde er erst 1946). Lise Meitner und Otto Frisch wurden dabei nicht berücksichtigt, und auch in den darauf folgenden Jahren wurde ihnen diese Ehrung nicht zuteil, obwohl sie von mehreren Physikern – auch von Otto Hahn selbst – für den Physik-Nobelpreis vorgeschlagen wurden. 

Die Nichtvergabe an Lise Meitner und Otto Frisch ist aus heutiger Sicht nicht nachvollziehbar, vor allem weil die beiden in Stockholm die theoretische Erklärung für das Phänomen verfassten. Außerdem baute Otto Hahn die berühmte Versuchsanordnung nach einer Anweisung von Lise Meitner auf. Ein Zitat aus Thomas Seilnachts Biografien bedeutender Chemiker verdeutlicht dies: 

Der niederländische Chemiker Dirk Coster, der Lise Meitner im Juli 1938 auf ihrer Flucht begleitet hatte, schrieb ihr anlässlich der Nobelpreis-Verleihung:
Lise Meitner, die das „Zerplatzen“ des Urankerns exklusiv aus erster Hand von Otto Hahn erfahren hatte und die chemischen Leistungen ihres Kollegen wohl am besten beurteilen konnte, sah jedenfalls die Nobelpreis-Verleihung ganz sachlich. An ihre Freundin Birgit Broomé-Aminoff schrieb sie Ende November 1945:

Carl Friedrich von Weizsäcker, Lise Meitners ehemaliger Assistent, ergänzte später:
Und über den Nobelpreis für Hahn schrieb Otto Robert Frisch im Jahre 1956:
Dennoch wird seit einigen Jahren von der amerikanischen Chemikerin und Feministin Ruth Lewin Sime die Ansicht vertreten, Otto Hahn habe den Nobelpreis nicht oder nicht allein verdient, habe Lise Meitner sogar bewusst ausgebootet, um ihn nicht mit ihr teilen zu müssen. Auch habe er sich ihr gegenüber in der Nachkriegszeit charakterlos verhalten. Diese Unterstellungen entfachten einen Sturm der Empörung unter den mit den historischen Fakten vertrauten Experten, werden aber nach wie vor in der heutigen Literatur immer wieder einmal zitiert und kontrovers diskutiert. Ernst Peter Fischer, Physiker und Wissenschaftshistoriker der Universität Konstanz bezeichnete die Tatsache, dass Lise Meitner keinen Nobelpreis erhielt, sogar drastisch als „Dummheit der schwedischen Akademie“. Lise Meitner hätte allerdings dieser simplifizierenden Einschätzung entschieden widersprochen, da sie Vorurteile und einseitige Interpretationen immer strikt abgelehnt hat.
Ein deutliches Urteil vertrat auch Berta Karlik, die Leiterin des Instituts für Radiumforschung in Wien, die an ihre Kollegin Erika Cremer schrieb:
Als „jüdische Mutter der Atombombe“ und „Frau des Jahres“ wurde Lise Meitner 1946 bei einer Vorlesungsreise in den USA in der amerikanischen Presse zu ihrem Missfallen bezeichnet, ein Jahr nach den Atombombenabwürfen auf Hiroshima und Nagasaki. Für Lise Meitner war es stets undenkbar, ihre Arbeit in den Dienst einer Massenvernichtungswaffe zu stellen.

Ab 1947 leitete Lise Meitner die kernphysikalische Abteilung des "Physikalischen Instituts der Königlich Technischen Hochschule Stockholm" und hatte diverse Gastprofessuren an US-amerikanischen Universitäten inne.

In der Nachkriegszeit erhielt Lise Meitner zahlreiche Ehrungen in aller Welt, in besonderer Weise in der Bundesrepublik Deutschland, so beispielsweise 1955 den ersten „Otto-Hahn-Preis für Chemie und Physik“, 1956 den Orden Pour le mérite für Wissenschaften und Künste und 1962 die "Dorothea-Schlözer-Medaille" der Georg-August-Universität Göttingen. Für alle drei Ehrungen hatte Otto Hahn sie vorgeschlagen. 1959 wurde in Berlin – in Anwesenheit beider Namensgeber – das „Hahn-Meitner-Institut für Kernforschung“ (HMI) offiziell vom damaligen Regierenden Bürgermeister Willy Brandt eingeweiht. Zu allen diesen Anlässen, aber auch zu privaten Besuchen kam Lise Meitner stets gerne nach Deutschland.

So ließ sie es sich ebenfalls nicht nehmen eigens von Stockholm nach Göttingen zu reisen, um ihrem Freund Otto Hahn zu seinem 80. Geburtstag am 8. März 1959 persönlich und öffentlich zu gratulieren:

1960 siedelte Lise Meitner zu ihrem Neffen Otto Robert Frisch nach Cambridge über, wo sie die letzten acht Jahre ihres Lebens verbringen sollte. Bis zu ihrem Tod mit 89 Jahren machte sie sich für eine friedliche Nutzung der Kernspaltung stark. Lise Meitner starb am 27. Oktober 1968, wenige Monate nach Otto Hahn.

Lise Meitners Werk wird sehr häufig auf die erste, Anfang 1939 zusammen mit Otto Frisch formulierte, physikalisch-theoretische Deutung der Kernspaltung reduziert. Diese war zweifellos von großer Bedeutung für die Entwicklung der militärischen und friedlichen Nutzung der Kernenergie, wurde aber bereits im Herbst 1939 durch eine umfassende Theorie der Kernspaltung ("The mechanism of nuclear fission") von Niels Bohr und John Archibald Wheeler ersetzt.

Lise Meitner beobachtete die Verwendung der Kernenergie für Waffensysteme äußerst kritisch. Sie ähnelte darin ihrem langjährigen Partner Otto Hahn und anderen Pionieren der Kernphysik wie etwa Albert Einstein (der jedoch, auf Vorschlag von Leó Szilárd, Präsident Roosevelt dringend zum Bau der US-Atombombe aufforderte). Lise Meitner selbst hat allerdings nie irgendeinen öffentlichen Friedensappell initiiert oder unterzeichnet, obwohl sie mehrfach darum gebeten wurde, und sich mit persönlichen Äußerungen zu den Themen ‚Atombombe, Kernwaffentests, nukleare Verseuchung usw.‘ immer zurückgehalten.

Neben den allgemein bekannten Arbeiten erweiterte Lise Meitner vor allem die Kenntnis über das Wesen der Radioaktivität. Die meisten ihrer Arbeiten waren Untersuchungen der Radioaktivität, insbesondere der Alpha- und Betastrahlung. Dabei konzentrierte sie sich auf die Wirkung dieser Strahlen auf verschiedene Materialien. Sie entdeckte gemeinsam mit Otto Hahn eine Reihe radioaktiver Isotope, darunter Protactinium 231, Actinium C und Thorium D.

Wesentliche Beiträge lieferte Lise Meitner auch zum Verständnis des Aufbaus der Atomkerne sowie der Energiefreisetzung beim radioaktiven Zerfall. Gemeinsam mit Otto Frisch veröffentlichte sie eine Reihe von Werken, die die physikalischen Grundlagen der Kernphysik erklärten und beleuchteten. Besonders in den Jahren nach 1945 konzentrierte sie sich daneben zunehmend auf gesellschaftliche Fragen der Atomphysik und stellte die Entwicklung der Kernwaffen und die militärische Nutzung der Kernenergie in Frage.

Über das Privatleben von Lise Meitner ist wenig bekannt, einigen Aufschluss darüber erhält man immerhin aus den veröffentlichten Briefen an bzw. von Elisabeth Schiemann, Otto Hahn und Max von Laue. Nach Aussagen von Otto Hahn und Max Planck war sie extrem zielgerichtet bei ihren Untersuchungen und arbeitete sehr hart, um Lösungen zu finden und Ergebnisse zu bekommen. Sie liebte die Natur und zog sich zum Nachdenken über theoretische Probleme gerne in den Wald zurück. Neben ihrer Forschung galt ihr persönliches, aber doch sehr zurückhaltendes Engagement vor allem dem Einsatz für den Frieden, der bedachten Nutzung der Kernenergie sowie der Gleichberechtigung der Frauen in den Wissenschaften. Sie selbst sagte einmal:

Einen Eindruck von ihrem vertraulichen Verhältnis zu Otto Hahn gibt ein oft zitierter, allerdings unbewiesener Ausspruch Meitners in einem persönlichen Gespräch mit Hahn:

Bis zu ihrem Tod erhielt Lise Meitner 21 wissenschaftliche (darunter 5 Dr. h. c., 12-mal Mitglied verschiedener Akademien) und öffentliche Auszeichnungen für ihr Werk und ihr Leben. Im Jahr 1926 wurde Meitner zum Mitglied der Leopoldina und der Göttinger Akademie der Wissenschaften gewählt. 1947 erhielt sie den Ehrenpreis der Stadt Wien für Wissenschaft. Sie war das erste weibliche Mitglied der naturwissenschaftlichen Klasse der österreichischen Akademie der Wissenschaften und wurde 1955 auswärtiges Mitglied der Royal Society in London, mit dem Recht die Abkürzung 'FMRS' (Foreign Member of the Royal Society) hinter ihrem Namen anzufügen. 

1949 erhielt sie gemeinsam mit Otto Hahn die Max-Planck-Medaille, 1955 den Otto-Hahn-Preis für Chemie und Physik und 1957 von Bundespräsident Theodor Heuss die bedeutendste deutsche Auszeichnung, den Orden Pour le Mérite für Wissenschaften und Künste. 1960 wurde sie in die American Academy of Arts and Sciences gewählt. 

Ebenfalls 1960 wurde ihr die Wilhelm-Exner-Medaille verliehen, und 1966 erhielt sie zusammen mit Otto Hahn und Fritz Straßmann den Enrico-Fermi-Preis der amerikanischen Atomenergie-Kommission. 

1967 wurde sie mit dem Österreichischen Ehrenzeichen für Wissenschaft und Kunst ausgezeichnet.

Lise Meitner wurde insgesamt 48-mal für den Nobelpreis nominiert, aber eine Auszeichnung blieb ihr versagt. Es gingen von 1937 bis 1965 insgesamt 29 Nominierungen für den Physikpreis ein, in den Jahren 1924 bis 1948 insgesamt 19 Nominierungen für den Chemiepreis. Unter den Einsendern der Nominierungen findet sich 1948 eine von Otto Hahn, der 1945 für die Entdeckung und den radiochemischen Nachweis der Kernspaltung mit dem Nobelpreis für Chemie für das Jahr 1944 geehrt wurde. Am häufigsten nominierte sie Max Planck, der sechs Nominierungen für den Chemiepreis und eine für den Physikpreis einsandte. Zu den Unterstützern, die sie mehr als zweimal nominierten, gehörten ferner James Franck (fünf Nominierungen für Physik), Oskar Klein (drei Nominierungen für Physik, eine für Chemie), Max Born (drei Nominierungen für Physik) und Niels Bohr (zwei Nominierungen für Chemie, eine für Physik).

Das chemische Element Meitnerium wurde 1997 nach ihr benannt.

Zusammen mit Otto Hahn war sie 1959 Namensgeberin des Hahn-Meitner-Instituts für Kernforschung in Berlin. 

Auch weitere öffentliche Einrichtungen wie Schulen und Straßen wurden in zahlreichen Städten nach ihr benannt. 

Die Internationale Astronomische Union ehrte sie durch die Benennung des Asteroiden (6999) Meitner und eines Kraters auf dem Erdmond und auf der Venus.

2008 wurde der ABC-Abwehrschule des Österreichischen Bundesheeres der Traditionsname „Lise Meitner“ verliehen.

Der Lise-Meitner-Preis für Kernphysik der Europäischen Physikalischen Gesellschaft ist nach ihr benannt, ferner gibt es einen Lise-Meitner-Literaturpreis.

Das Land Nordrhein-Westfalen vergibt seit 1991 das Lise-Meitner-Stipendium für habilitierende Frauen. 

Der Fonds zur Förderung der wissenschaftlichen Forschung unterhält das Lise-Meitner-Programm zur Förderung ausländischer Wissenschaftler in Österreich.

Am 12. Juli 2010 wurde in Berlin-Mitte, Hessische Straße 1, eine Berliner Gedenktafel angebracht und am 10. Juli 2014 im Ehrenhof der Humboldt-Universität zu Berlin mit einem Festakt das Meitner-Denkmal enthüllt.

Im Juni 2016 wurde sie mit einer Büste im Arkadenhof der Universität Wien geehrt.

Zu ihren Doktoranden gehört Rudolf Jaeckel.

Lise Meitner veröffentlichte 169 Arbeiten, eine kleine Auswahl davon soll hier vorgestellt werden:





</doc>
<doc id="2975" url="https://de.wikipedia.org/wiki?curid=2975" title="Lauch (Begriffsklärung)">
Lauch (Begriffsklärung)

Lauch steht für:

Lauch ist der Familienname folgender Personen:


</doc>
<doc id="2978" url="https://de.wikipedia.org/wiki?curid=2978" title="Louis Armstrong">
Louis Armstrong

Louis Daniel „Satchmo“ Armstrong (* 4. August 1901 in New Orleans; † 6. Juli 1971 in New York City, New York) war ein amerikanischer Jazztrompeter, Sänger und Schauspieler.

Louis Armstrong gab stets den 4. Juli, also den Unabhängigkeitstag der Vereinigten Staaten, des Jahres 1900 als sein Geburtsdatum an. Dies war insbesondere beim afroamerikanischen Teil der Bevölkerung der Vereinigten Staaten oft üblich, wenn das eigene Geburtsdatum und die Geburtsumstände nicht bekannt waren oder nicht den gesellschaftlichen Vorstellungen entsprachen. Dazu passt ebenfalls, dass er sich ein Jahr älter machte und seine Geburt in das Jahr der Jahrhundertwende vorverlegte, was ihm als Jugendlichem den Zutritt zu den Etablissements von Storyville, dem Vergnügungsviertel von New Orleans, erleichterte. Erst aus seinem 1983 entdeckten Taufschein geht das wirkliche Geburtsdatum – der 4. August 1901 – hervor.

Armstrong wurde in ärmlichsten Verhältnissen geboren und wuchs nur zeitweilig bei seiner Mutter auf. Als Siebenjähriger musste er Zeitungen verkaufen. Anfang 1913 wurde er wegen Unruhestiftung in das "Colored Waif’s Home for Boys" eingewiesen, eine Anstalt für obdachlose schwarze Jugendliche, nachdem er in der Silvesternacht mit dem Revolver seines Onkels in die Luft geschossen hatte. In der streng organisierten Anstalt erlernte Armstrong die Grundlagen des Kornettspiels. Bis 1918 schlug er sich mit kleinen Jobs und ersten Auftritten als Musiker im Rotlichtmilieu der Stadt durch.

Von 1918 bis 1919 spielte Armstrong regelmäßig in der Band von Fate Marable auf einem Mississippi-Dampfer, die die Passagiere auf den langen Fahrten flussaufwärts unterhielt. 1918 soll ihn dabei der 15-jährige Bix Beiderbecke in Davenport gehört haben. 1918 ersetzte Armstrong den Trompeter King Oliver in der Band, die dieser zusammen mit dem Posaunisten Kid Ory leitete. Als Oliver nach Chicago zog, folgte Armstrong ihm 1922 nach und stieß als 2. Trompeter zu King Oliver’s Creole Jazz Band, die im Lincoln Gardens Café in der South Side von Chicago spielte. Aus dieser Zeit gibt es bereits erste Tondokumente (u. a. "Chimes Blues"). Insbesondere bei seinen Live-Auftritten soll das Duo Oliver und Armstrong mit seinen zweistimmigen Break-Improvisationen nach zahlreichen Berichten von Zeitzeugen Musikgeschichte geschrieben haben. 1924 heiratete Armstrong Lilian „Lil“ Hardin, die aus Memphis stammende Pianistin der Band. Kurz darauf wechselte er auf ihr Anraten hin in die Band von Fletcher Henderson, wo er rasch zum Starsolisten avancierte und nicht mehr im Schatten seines Lehrmeisters Oliver stehen musste.

1925 verließ Armstrong die Henderson-Band. Ab diesem Jahr entstanden zahlreiche Aufnahmen, die Lil und er hauptsächlich mit Quintett- und Septett-Formationen machten, die sich Louis Armstrong and His Hot Five bzw. Hot Seven nannten. Viele dieser Aufnahmen gelten heute als Meilensteine der Jazzgeschichte. In diesen Jahren entstanden richtungsweisende Aufnahmen wie "West End Blues" (von Jazzkritikern zur Jazzplatte des Jahrhunderts gewählt), "Potato Head Blues", "Struttin’ with Some Barbecue", "Wild Man Blues", "Fireworks" und "Heebie Jeebies". In einigen dieser Aufnahmen stellte Armstrong auch sein Talent als Sänger unter Beweis, insbesondere beim Scat-Gesang. Bemerkenswert ist auch Armstrongs Zusammenarbeit mit dem Pianisten Earl Hines in den späten 1920er Jahren. 1927 wechselte Armstrong dem allgemeinen Trend folgend vom weicher klingenden Kornett zur härteren Trompete.

Bereits 1926 gelang ihm mit Kid Orys "Muskrat Ramble" sein erster Hit in den Billboard-Charts, dem bis 1966 noch 78 weitere folgten. Im Februar 1932 gelang ihm der erste Nummer-1-Hit mit einer Version von "All of Me". Seit den frühen 1930er Jahren, während denen sich der neue Jazz-Stil des Swing entwickelte, trat Louis Armstrong der neuen Mode folgend vorwiegend in Big Bands auf (u. a. dem Orchester von Luis Russell) und wurde rasch innerhalb und außerhalb der Vereinigten Staaten bekannt. Ab 1932 führten ihn zahlreiche Tourneen nach Europa, später in die ganze Welt. 1947 löste Armstrong seine Big Band auf und kehrte wieder zu seinen Ursprüngen, dem New Orleans Jazz und den kleinen Formationen zurück ("Louis Armstrong and his All Stars" feat. Velma Middleton). In den 1950er und 1960er Jahren war es insbesondere Armstrongs Talent als Sänger und Entertainer, das ihn zum Weltstar machte. Eine weitere Steigerung seiner Popularität erzielte er durch die Hollywoodfilme, bei denen er mitwirkte, wie z. B. "Die Glenn Miller Story", "Die oberen Zehntausend" und "Hello, Dolly!".

Nicht zuletzt wegen seiner weltweiten Berühmtheit wurde Louis Armstrong in der Hochzeit des Kalten Krieges in den 1950er Jahren von der US-Regierung als musikalischer Mobilmacher in den Ost-West-Konflikt entsandt. Ab 1956 bereiste er zusammen mit Künstlern wie Benny Goodman den Ostblock sowie die sowohl von den Vereinigten Staaten als auch der UdSSR umworbenen Staaten in Afrika und Asien. So kamen 1956 im heutigen Ghana 100.000 Menschen in ein Stadion, um ihn zu erleben. Zusammen mit weiteren Stars des Jazz wie Dizzy Gillespie und Duke Ellington nutzte Armstrong seine Popularität auf seinen Tourneen auch, um für die Afro-Amerikaner Menschen- und Bürgerrechte einzufordern. So weigerte sich Armstrong 1957 wegen der Rassentrennung in den Vereinigten Staaten, im Auftrag des US-State-Departments in die UdSSR zu reisen.

Seine unermüdliche Energie und seine vielen Auftritte forderten schon früh gesundheitlichen Tribut. Angesichts mehrerer ernsthafter Krisen rieten die Ärzte Armstrong vom Trompetespielen ab, um seine Gesundheit zu schonen. Dem Publikum und seinem Ehrgeiz verpflichtet, verlegte sich Armstrong seit dieser Zeit mehr auf den Gesang. Im Jahr 1969 interpretierte er den Song "We have all the Time in the World" von John Barry und Hal David zum James-Bond-Film "Im Geheimdienst Ihrer Majestät" mit George Lazenby als 007. In dieser Zeit konnte er jedoch, von Ausnahmen abgesehen (u. a. die Gesangsduette mit Ella Fitzgerald, zum Beispiel auf "Ella and Louis"), wegen seiner körperlichen Schwäche nicht mehr an die bahnbrechenden Leistungen der 1920er und 1930er Jahre als Jazztrompeter und Jazzsänger anknüpfen.

Louis Armstrong starb im Alter von 69 Jahren am 6. Juli 1971 in New York an einem Herzinfarkt. Sein Grab befindet sich auf dem "Flushing Cemetery" in Queens, New York City.

Armstrong hatte seine musikalischen Wurzeln im New-Orleans-Jazz. Er hat maßgeblichen Anteil an der Entwicklung dieser Stilrichtung weg von der Kollektivimprovisation hin zu dem herausgestellten Solo und begründete das „Starsolistentum“ im Jazz. Auch technisch setzte Armstrong insbesondere in den 1920er Jahren praktisch sämtliche Maßstäbe für Jazztrompeter. Er kann als einer der bedeutendsten Instrumentalsolisten des Jazz angesehen werden.

Armstrong hat stilistisch fast alle nachkommenden Trompeter der traditionellen Jazzstile entscheidend beeinflusst. Sein Einfluss ist auch heute noch (oder vielleicht wieder) bei jüngeren Musikern wie etwa Wynton Marsalis spürbar.

Darüber hinaus ist Armstrong neben Billie Holiday und Ella Fitzgerald einer der bekanntesten Sänger des Jazz, dessen unverwechselbare Stimme seine weltweite Popularität begründete.

Armstrong erhielt 1960 einen Stern auf dem Hollywood Walk of Fame. Unter Mitbegründung von Phoebe Jacobs entstand nach Armstrongs Tod die "Louis Armstrong Educational Foundation". Der zweitgrößte Tenniscourt in Flushing Meadows (US Open) ist ebenso nach ihm benannt wie der Louis Armstrong Park in New Orleans sowie der im 19 km entfernten Kenner liegende internationale Flughafen, der Louis Armstrong New Orleans International Airport.

1970 führte Samuel Darragh McGredy eine rote Floribundarose ein, die er Louis Armstrong zu Ehren „Satchmo“ nannte.

Der "St. Louis Blues" von W. C. Handy sowie das romantische "What a Wonderful World" von George David Weiss und Bob Thiele besitzen kaum mehr Jazzanklänge. Armstrong bediente sich auch Musicalmelodien; "Mack the Knife" ("Mackie Messer") aus Bertolt Brechts "Dreigroschenoper" und "Hello Dolly" werden vermutlich häufiger in Armstrongs Interpretation gespielt als in der Originalfassung für die Theaterbühne.

Armstrongs Spitzname „Satchmo“ ist eine Verkürzung von "satchel mouth" (zu deutsch etwa „Taschenmund“), eine Anspielung auf die Größe seines Mundes. Als Kind wurde er auch "gate mouth" genannt. Eine weitere Variante seiner Spitznamen in der Frühzeit war „Dippermouth“ (zu deutsch „Schöpflöffelmund“). Dieser Name inspirierte ihn zu dem Titel "Dippermouth Blues".



Das "Louis Armstrong House Museum" konnte 2016 den bis dahin unbekannten einzigen Film, der Louis Armstrong 1959 bei Studioaufnahmen zu "Satchmo plays King Oliver" zeigt, erwerben. Der 33 Minuten lange Film wurde vom Musikproduzenten Sid Frey nach professionellen Maßstäben gemacht, jedoch nicht weiter verwendet und Frey verschwieg in der Folge auch seine Existenz.




</doc>

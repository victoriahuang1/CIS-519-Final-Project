<doc id="1175" url="https://de.wikipedia.org/wiki?curid=1175" title="Düsseldorf">
Düsseldorf

Im Jahr 1288 erhielt der Ort an der Mündung des Flüsschens Düssel in den Rhein das Stadtrecht. Vom Ende des 14. Jahrhunderts bis zum Beginn des 19. Jahrhunderts war die Stadt Regierungssitz von Ländern des Heiligen Römischen Reichs und des Rheinbundes: des Herzogtums Berg, der Herzogtümer Jülich-Berg und Jülich-Kleve-Berg sowie des Großherzogtums Berg, von 1690 bis 1716 auch Residenz des Pfalzgrafen und Kurfürsten Johann Wilhelm von der Pfalz. Vom 19. bis ins 20. Jahrhundert war sie Parlamentssitz der Rheinprovinz des Königreichs Preußen. Im Kaiserreich entwickelte sich Düsseldorf im Zuge der Hochindustrialisierung in Deutschland zum „Schreibtisch des Ruhrgebiets“ und wurde mit dem Überschreiten der Marke von 100.000 Einwohnern im Jahr 1882 zur Großstadt.

Die Rheinmetropole gehört zu den fünf wichtigsten, international stark verflochtenen Wirtschaftszentren Deutschlands. Düsseldorf ist eine Messestadt und Sitz vieler börsennotierter Unternehmen, darunter der im DAX notierte Konzern Henkel. Zudem ist sie der umsatzstärkste deutsche Standort für Wirtschaftsprüfung, Unternehmens- und Rechtsberatung, Werbung und Kleidermode sowie ein wichtiger Banken- und Börsenplatz. Auch im Kunsthandel Deutschlands ist sie führend.

Düsseldorf besitzt mehrere Rheinhäfen. Sein Flughafen Düsseldorf Airport ist das interkontinentale Drehkreuz Nordrhein-Westfalens. Die Stadt ist des Weiteren Sitz von 22 Hochschulen, darunter die renommierte Kunstakademie Düsseldorf und die Heinrich-Heine-Universität. Überregionale Bekanntheit genießt Düsseldorf außerdem durch seine Altstadt („längste Theke der Welt“), seinen Einkaufsboulevard Königsallee („Kö“), seinen Düsseldorfer Karneval, den Fußballverein Fortuna Düsseldorf und den Eishockeyverein Düsseldorfer EG. Weitere Anziehungspunkte sind zahlreiche Museen und Galerien sowie die Rheinuferpromenade und der moderne Medienhafen. Das Stadtbild wird auch durch zahlreiche Hochhäuser und Kirchtürme, den 240 Meter hohen Rheinturm, viele Baudenkmäler und sieben Rheinbrücken geprägt. Bemerkenswert ist die große Anzahl ostasiatischer Einwohner, darunter die japanische Gemeinde, welche die größte japanische Gemeinde Deutschlands bildet. In einem Vergleich der Lebensqualität von 231 Großstädten in der Welt nimmt Düsseldorf den sechsten Platz ein.

Das überwiegend rechtsrheinisch gelegene Düsseldorf befindet sich im mittleren Teil des Niederrheinischen Tieflands auf einer von zahlreichen Rheinarmen durchzogenen Niederterrassenfläche. Lediglich die Stadtteile Oberkassel, Niederkassel, Heerdt und Lörick liegen am linken Rheinufer.

Die Stadt ist Teil der prosperierenden Rheinschiene und grenzt südwestlich an das Ruhrgebiet. Sie liegt damit im Herzen der Metropolregion Rhein-Ruhr sowie im Übergangsbereich zwischen dem Niederrhein und dem Bergischen Land, zu dem die Stadt, historisch betrachtet, gehört. Die Metropolregion Rhein-Ruhr ist eine Wirtschaftsregion und ein städtischer Ballungsraum im Westen Deutschlands. Sie zählt zu den größten Verdichtungsräumen innerhalb der europäischen Megalopolis und ist der größte in Deutschland. In den 20 kreisfreien Städten und zehn Kreisen der Region leben rund elf Millionen Einwohner auf knapp 10.000 km² (Stand 2005). Allein in einem Umkreis von 50 Kilometern um das Oberzentrum Düsseldorf leben etwa neun Millionen Menschen.

Die zentrale Lage im größten Ballungsraum Deutschlands, die Hauptstadtfunktion für das bevölkerungsreiche Bundesland Nordrhein-Westfalen, die Vielzahl bedeutender Unternehmen sowie die Ausstattung mit Infrastrukturen und Angeboten aller Art verschaffen der Stadt große Fühlungs- und Agglomerationsvorteile, die ihr Wachstum und die Entstehung innovativer Milieus erheblich begünstigen. Erspart bleiben der Stadt allerdings einige Agglomerationsnachteile, etwa ein häufig kollabierendes Verkehrsnetz, ein extrem belastendes Stadtklima oder Immobilienpreise auf europäischem Spitzenniveau, weil Düsseldorf selbst keine Millionenstadt ist, einem polyzentrischen Städtenetz angehört und weil landschaftliche Freiräume sowie Erholungsgebiete sie allseits nah umgeben.

Die Lage am Rhein gibt der Stadt die Möglichkeit, mit ihrer Skyline das Stadtbild im Kontrast zum Fluss, der aufgrund seiner Größe einen weitläufigen Landschaftsraum bildet, imposant zu inszenieren und beides zu einer einprägsamen Stadtlandschaft zu verschmelzen.

Der höchstgelegene Punkt im Stadtgebiet, der Sandberg im Stadtteil Hubbelrath, misst , der niedrigste Punkt, die Mündung des Schwarzbachs in den Rhein bei Wittlaer, .

Der geografische Mittelpunkt Düsseldorfs befindet sich im Düsseltal, der Punkt ist mit einer Bronzetafel gekennzeichnet.

Düsseldorf liegt im Zentrum der Zone der Mitteleuropäischen Zeit.

Das Klima des Düsseldorfer Raumes ist durch die reliefbedingte Öffnung in Richtung Nordsee ozeanisch/atlantisch geprägt. Düsseldorf liegt im niederrheinischen Tiefland und überwiegend westliche Windströmungen tragen feuchte Luftmassen heran. Die Folgen sind milde, schneearme Winter und mäßig warme und feucht-schwüle Sommer mit einer wechselhaften Witterung. So gibt es in der Stadt bei einer Jahresmitteltemperatur von 11,2 °C im Mittel rund 790 mm Niederschlag. Der Raum Düsseldorf gehört zu den Gebieten mit den mildesten Wintern in Deutschland. Auch im Winter fällt die Temperatur selten unter den Gefrierpunkt und aufkommende Fröste bleiben meist im oberen Bereich, knapp unter Null. Nach der USDA-Klassifikation der Winterhärtezonen, liegt Düsseldorf in Zone 8b, bei innerstädtischen Mikroklimaten sogar bei Zone 9a, was bedeutet, dass die durchschnittlich kälteste Jahrestemperatur über −6,6 °C liegt. Mit rund 1550 Sonnenstunden ist Düsseldorf eine der weniger sonnigen Städte Deutschlands, was im Winter durch bedeckten Himmel und somit weniger Temperaturabstrahlung ebenfalls für mildere Temperaturen sorgt. Wegen des milden Klimas werden im Raum Düsseldorf viele exotische und mediterrane Pflanzen wie Palmen, Oliven, Lorbeer, Feigen, Pinien und Zypressen im Freiland kultiviert.

Das Stadtgebiet Düsseldorfs besteht aus zehn Stadtbezirken, die in 50 Stadtteile unterteilt sind. In jedem Stadtbezirk gibt es eine Bezirksvertretung mit 19 Mitgliedern unter Vorsitz eines Bezirksvorstehers. Die Bezirksvertretungen sind zu wichtigen, den Stadtbezirk betreffenden Angelegenheiten zu hören und werden bei jeder Kommunalwahl neu gewählt.

Die Stadtbezirke mit ihren zugehörigen Stadtteilen
Im Unterschied zu anderen nordrhein-westfälischen Großstädten haben die Stadtbezirke in Düsseldorf keine eigenen Namen, sondern sind von 1 bis 10 durchnummeriert. Die meisten Einwohner hat der Stadtbezirk 3 mit ca. 109.000 Einwohnern, mit Bilk (rund 37.000 Menschen leben dort) liegt auch der einwohnerreichste Stadtteil im Stadtbezirk 3. Die geringste Bevölkerung hat dagegen der Stadtbezirk 10 mit etwa 25.000 Bewohnern, bei den Stadtteilen weist der Hafen mit 123 Einwohnern die kleinste Bevölkerungszahl auf.

Die Stadtbezirke und Stadtteile sind im Einzelnen:

Weitere Informationen zum Thema befinden sich in der Liste der Stadtbezirke von Düsseldorf und der Liste der Stadtteile von Düsseldorf.

Die Stadt Düsseldorf grenzt im Norden an die kreisfreie Stadt Duisburg und an die Stadt Ratingen, im Osten an die Städte Mettmann, Erkrath und Hilden, im Süden an die Städte Langenfeld (Rheinland) und Monheim am Rhein (alle Kreis Mettmann) sowie im Westen an die Städte Dormagen, Neuss und Meerbusch (alle Rhein-Kreis Neuss).

Die mittelalterliche Stadt Düsseldorf wurde im 12./13. Jahrhundert zwar in der Nähe von frühmittelalterlichen Altsiedlungen gegründet, ging aber als Neugründung – ähnlich wie beispielsweise auch in Alpen oder Kalkar – nicht unmittelbar aus einer dieser Altsiedlungen hervor. Die Siedlung trug ihren Namen nach dem kleinen Fluss "Düssel", der südlich der Straße "Altestadt" in den Rhein mündete. Der Name "Düssel" entstand wahrscheinlich aus dem germanischen Begriff "thusila" und bedeutet "die Rauschende". Die Landschaft, in der Düsseldorf gegründet wurde, war vor der Entstehung der Grafschaft Berg eine ursprünglich fränkische, zu Ripuarien gehörende Grafschaft, in der neueren Forschung Duisburg-Kaiserswerther Grafschaft genannt, ein Herrschaftsgebiet der Ezzonen.

Die erste schriftliche Erwähnung von "Dusseldorp" in einer Schreinskarte kann nicht sicher datiert werden und stammt frühestens aus dem Jahr 1135. Am 5. Juni 1288 fand die Schlacht von Worringen statt, in deren Folge Graf Adolf V. von Berg Düsseldorf am 14. August 1288 die Stadtrechte verlieh. Die nur 3,8 Hektar große Stadt war bereits früh ein mit einer Stadtmauer und einem Graben gesicherter Ort, der die Westgrenze der Grafschaft Berg markierte.

1380 wurde Graf Wilhelm II. von König Wenzel in den Reichsfürstenstand erhoben. Noch im selben Jahr beschloss der neue Herzog zum Ausdruck seiner reichspolitischen Funktion und Stellung, die relativ abgelegene Burg an der Wupper als Regierungssitz aufzugeben und das am Rhein gelegene Düsseldorf zur neuen Residenz zu entwickeln. Für die geplante bergische Hauptstadt Düsseldorf wurde erstmals 1382 eine Burg urkundlich erwähnt, die in den folgenden Jahrhunderten zum Düsseldorfer Residenzschloss ausgebaut wurde. Seit 1386 residierten der Herzog und seine Gemahlin Anna dort. Zwischen 1384 und 1394 wurde die Stadt erheblich erweitert; der Bau der backsteingotischen Hallenkirche St. Lambertus und ihre reichhaltige Ausstattung mit Reliquien und Pfründen datieren in dieser Zeit. Durch geschickte Heiratspolitik vereinigten die Herzöge von Berg die Herzogtümer Jülich und Kleve mit dem ihren zum gemeinsamen Herzogtum Jülich-Kleve-Berg. In den Jahren 1538 bis 1543 war Düsseldorf die Hauptstadt eines Verbundes von Territorialstaaten, der neben Jülich-Kleve-Berg auch das Herzogtum Geldern, die Grafschaften Mark, Ravensberg und Zutphen sowie die Herrschaft Ravenstein umfasste. Insbesondere unter Wilhelm dem Reichen wurde die Region zu einem Zentrum humanistischer Wissenschaft und liberaler Katholizität. Gegenüber Juden setzte sich unter seiner Herrschaft mit der Polizeiverordnung von 1554, die die Ausweisung aller Juden verlangte, allerdings eine antijudaische Linie durch. 1585 wurde bei der Vermählung des Erbprinzen Johann Wilhelm mit der Markgräfin Jakobe von Baden die wohl prunkvollste dokumentierte Hochzeit des 16. Jahrhunderts ausgerichtet. Unter dem Titel "Orpheus und Amphion" kam dabei zum ersten Mal ein opernartiges theatralisches Schauspiel mit Gesang und Musik zur Aufführung. Wilhelm der Reiche sorgte für den Wiederaufbau und Ausbau des Düsseldorfer Schlosses durch den Renaissance-Baumeister Alessandro Pasqualini. Nach dem Aussterben des jülich-bergisch-klevischen Regentenstammes 1609 und während eines Erbfolgestreits zwischen Brandenburg und Pfalz-Neuburg besetzte der spanische General Ambrosio Spinola als kaiserlicher Kommissar 1614 die Stadt.

Nach der Beilegung des Jülich-Klevischen Erbfolgestreits gehörte Düsseldorf mit dem Herzogtum Jülich-Berg zum damals zunächst noch protestantischen Haus Pfalz-Neuburg, einem Zweig des Adelsgeschlechtes der Wittelsbacher. In der ersten Phase der pfälzischen Herrschaft kam es zu schweren Auseinandersetzungen zwischen römisch-katholischen, lutherischen und reformierten Beamten bei Hof und in der Stadt. Unter dem Einfluss seiner Frau, Magdalene von Bayern, konvertierte Erbprinz Wolfgang Wilhelm 1613 zur römisch-katholischen Konfession, wodurch er sich in den politischen Auseinandersetzungen seiner Zeit die Unterstützung der Katholischen Liga sichern konnte. Mit der Übernahme der Pfalzgrafen- und Herzogswürde im Jahre 1614 führte die Konversion Wolfgang Wilhelms in seinen Territorien zu einer Repression der protestantischen Konfessionen und zu einer Begünstigung der römisch-katholischen Kirche. Bei der nun einsetzenden Gegenreformation hatten die bei Hof verkehrenden Jesuiten eine Schlüsselrolle.

Johann Wilhelm von der Pfalz, von den Niederfränkisch sprechenden Düsseldorfern „Jan Wellem“ genannt, schon als pfälzischer Erbprinz seit 1679 Regent von Jülich-Berg, seit 1690 schließlich Kurfürst von der Pfalz sowie Herzog von Jülich-Berg, hielt auch als Souverän an Düsseldorf als Hauptresidenz fest, zumal die frühere kurfürstliche Hauptresidenz in Heidelberg durch den Pfälzischen Erbfolgekrieg zerstört worden war. In der Regierungszeit Johann Wilhelms erfuhr Düsseldorf durch die Präsenz des glanzvollen Hofes eine beachtliche wirtschaftliche, kulturelle und städtebauliche Entwicklung, die sich unter Kurfürst Karl Theodor von der Pfalz fortsetzte, der Schlösser, Sammlungen, Institute gründete und die Carlstadt anlegen ließ. Herausragend und berühmt war die noch von Johann Wilhelm gegründete, unter Karl Theodor ebenfalls geförderte Gemäldegalerie. Allerdings verlor Düsseldorf den Status einer kurfürstlichen Hauptresidenz schon 1718 wieder an Heidelberg. 1720 ging diese Funktion dann an Mannheim und 1778 an München über, von wo aus Karl Theodor die Territorien Kurpfalz-Bayern und Jülich-Berg regierte. Eine weitere kurze Blüte der Stadt erfolgte unter dem kurfürstlichen Statthalter Johann Ludwig Franz Graf von Goltstein. 1769 wurde Düsseldorf Sitz des Jülich-Bergischen Oberappellationsgerichtes.

Seit 1732 weiter neuzeitlich befestigt, wurde die Stadt im Siebenjährigen Krieg 1757 von den Franzosen besetzt und nach der Schlacht bei Krefeld 1758 von Herzog Ferdinand von Braunschweig durch Kapitulation eingenommen, jedoch bald wieder verlassen. Im Zuge der durch die Französische Revolution entfesselten Koalitionskriege kapitulierte Düsseldorf im Jahre 1795 der französischen Revolutionsarmee und blieb unter französischer Besetzung, bis es im Frieden von Lunéville 1801 an Kurpfalz-Bayern zurückgegeben wurde.

Daraufhin erfolgte die vertraglich bedingte Schleifung der Festungswerke. Doch bereits infolge eines Gebietstausches, der in dem Vertrag von Schönbrunn und im Vertrag von Brünn zwischen Kurpfalz-Bayern, Preußen und Frankreich festgelegt worden war, gelangte die Stadt ab 1806 wieder unter französischen Einfluss. Vor dem Gebietstausch hatte Kurfürst Maximilian IV. die weltberühmte Gemäldesammlung, die ein staatlicher Besitz des Herzogtums Jülich-Berg war, abziehen lassen und widerrechtlich dem bayerischen Kunstbesitz einverleibt. Düsseldorf wurde Landeshauptstadt des Großherzogtums Berg. Das Großherzogtum schied auf der Grundlage der Rheinbundakte als souveräner, mit Frankreich alliierter Staat aus dem Heiligen Römischen Reich aus und bestand faktisch bis Ende 1813. Großherzöge waren Joachim Murat bis 1808, sodann Napoleon selbst, schließlich ab 1809 unter Napoleons Regentschaft sein minderjähriger Neffe Napoléon Louis Bonaparte. Unter der neuen Regierung hielten bedeutende soziale und administrative Reformen Einzug. 1810 führte Napoleon den bergischen Code civil ein, der unter anderem den von Heinrich Heine begrüßten Durchbruch in Richtung einer Gleichstellung der Juden mit sich brachte. Anspruchsvolle Maßnahmen zur städtebaulichen Erneuerung und Verschönerung Düsseldorfs wurden vollzogen, insbesondere nach Entwürfen des Landschaftsarchitekten Maximilian Friedrich Weyhe. So pflanzte man die "Neue Allee", die spätere Königsallee, und bepflanzte den "Boulevard Napoléon", die spätere Heinrich-Heine-Allee erstmals als elegante Esplanaden; der Hofgarten erfuhr einen weiteren Ausbau zu einem Englischen Landschaftsgarten. Gleichwohl war das Großherzogtum für Frankreich im Rahmen seiner imperialistischen Expansion letztlich nur als Satelliten- und Pufferstaat sowie als Ressource für Finanzeinnahmen und Truppenaushebungen von Relevanz. Zudem geriet das Großherzogtum zunehmend in eine schwere Wirtschaftskrise, weil die französischen Zölle, die im Zuge der Kontinentalsperre an seinen westlichen und nördlichen Staatsgrenzen erhoben wurden, es von wichtigen Marktgebieten abschnitten. Die Wende brachte die Völkerschlacht bei Leipzig, in deren Folge die französischen Truppen und Spitzenbeamten das Großherzogtum Berg verließen.

Das von den Franzosen verlassene Großherzogtum Berg wurde ab Ende 1813 von Truppen des Königreichs Preußen besetzt und von preußischen Beamten als Generalgouvernement Berg interimistisch verwaltet. Auf der Grundlage der Neuordnung Europas, die in den Jahren 1814 bis 1815 auf dem Wiener Kongress verhandelt worden war, nahm der preußische König Friedrich Wilhelm III. das Territorium und dessen Hauptstadt Düsseldorf am 5. April 1815 schließlich in Besitz. Rechtlich gehörte es ab dem 21. April 1815 zu Preußen. Düsseldorf wurde 1816 Sitz des Landkreises Düsseldorf. Düsseldorf selbst war dabei aber zunächst kreisfreie Stadt, doch schon 1820 wurde die Stadt in den Landkreis Düsseldorf eingegliedert. Am 22. April 1816 nahm die Bezirksregierung Düsseldorf ihre Arbeit auf. Mit der Schaffung der Rheinprovinz wurde Düsseldorf 1822 Sitz eines "Landeshauptmanns" und 1823 Sitz eines "Provinziallandtags".

Durch die Eingliederung in Preußen hatte Düsseldorf nach über 400 Jahren den Status einer Landeshauptstadt und damit sämtliche Behörden der Landesregierung verloren. Düsseldorf war somit nur noch der Mittelpunkt einer Provinz und eine Beamtenstadt, nach Schleifung der Festungswerke von einem geschlossenen Ring ausgedehnter Parks umgeben, dem sich eine erste Stadterweiterung im klassizistischen Stil anschloss. Nach zeitgenössischen Beschreibungen bot die Stadt in der Zeit des Biedermeier insgesamt ein vergleichsweise harmonisches Stadtbild, bemerkte doch etwa Carl Julius Weber: "„Das heitere Düsseldorf gefällt doppelt, wenn man aus dem finsteren Cöln herkommt.“" Allerdings war die politische und administrative Bedeutung der Stadt aufgrund des Verlustes von Hauptstadtfunktionen nicht so hoch wie der Rang des geistigen und künstlerischen Lebens in jener Zeit, welcher maßgeblich auf der Neugründung der Kunstakademie Düsseldorf (1819) und der aus ihr hervorgehenden Düsseldorfer Malerschule (1819–1918) fußte sowie ihr den Ruf einer "Kunst- und Gartenstadt" eintrug. In der Zeit des Vormärz und der Deutschen Revolution waren die in der Stadt vertretenen bürgerlichen Milieus mit den Persönlichkeiten Lorenz Cantador, Ferdinand Freiligrath, Ferdinand Lassalle und Hugo Wesendonck ein Brennpunkt der sich formierenden demokratischen und Arbeiterbewegung.

Ab Mitte der 1830er Jahre erfasste der durch die Industrialisierung ausgelöste gesellschaftliche und wirtschaftliche Umbruch die kleine preußische Provinzstadt. Die Ablösung des Kölner Stapelrechts durch die Mainzer Akte (1831), die Dampfschifffahrt auf dem zunehmend regulierten Rhein, die Einrichtung eines Freihafens (1831) sowie die Anlage der ersten westdeutschen Eisenbahnstrecken (1838) schufen die Voraussetzungen für die Entwicklung Düsseldorfs zur Industriestadt. Die zwischen Rotterdam und Mannheim verkehrende "Dampfschiffahrts-Gesellschaft für den Nieder- und Mittelrhein" wurde 1836 in Düsseldorf gegründet. 1837 fand die erste Gewerbeausstellung in der Flinger Straße statt, die Grundlage für die spätere Entwicklung zur Messestadt. Ab 1850 siedelten sich die ersten Stahlwerke unter anderem in Oberbilk an. Es folgten zahlreiche weitere Industriebetriebe wie beispielsweise die Gerresheimer Glashütte. Allerdings dominierte bis 1870 noch das Textilgewerbe. Eine Berufsfeuerwehr hat Düsseldorf seit 1872.

1872 wurde Düsseldorf erneut kreisfrei. Um 1880 bestand es aus sechs Stadtteilen: der Altstadt (dem ursprünglichen Düsseldorf) mit engen und unregelmäßigen Straßen sowie den beiden Mündungen der nördlichen und der südlichen Düssel, der Carlstadt an der Südseite der Altstadt (1767 angelegt), der in einiger Entfernung liegenden Neustadt, die 1690–1716 erbaut wurde, der Friedrichstadt am Südostende, der Königstadt und schließlich Pempelfort im Norden und Nordosten. 1880 fand in Düsseldorf die "Rheinisch-Westphälische Gewerbeausstellung" statt, die über eine Million Besucher anzog und der Stadt weitere Wachstumsimpulse gab. Nach der Volkszählung vom 1. Dezember 1880 lebten in der Stadt auf 49 Quadratkilometern Fläche 95.458 Menschen. Die verkehrsgünstig und wirtschaftsgeografisch zentral gelegene preußische Stadt, die 50 Jahre zuvor aus politischer und wirtschaftlicher Sicht nur wenig Bedeutendes vorzuweisen hatte, stand dank fortschreitender Industrialisierung, ausgebauter Verkehrsinfrastrukturen, rasanten Bevölkerungswachstums und des Fortfalls von Zollschranken, der sich mit der Verwirklichung des Deutschen Zollvereins ab 1834 ergeben hatte, an der Schwelle der Entwicklung zu einer der bedeutenden Groß- und Industriestädte des 1871 gegründeten Nationalstaats Deutsches Reich, dessen bundesstaatlicher Rahmen das Königreich Preußen nunmehr als einen Gliedstaat umfasste. In der Zeit von 1880 bis 1900 stieg die Bevölkerung auf mehr als das Doppelte an, 215.000 Einwohner.

An der Wende zum 20. Jahrhundert war Düsseldorf eine geschäftige und aufstrebende Industriestadt. 1902 wurde eine große Gewerbe-, Industrie- und Kunstausstellung mit über 2500 Ausstellern auf einem 70 Hektar großen Gelände am Rheinufer organisiert, die weltweit Beachtung fand. Eine gute Finanzverfassung, niedrige Steuern und städtebauliche Anreize zogen vermögende Leute und Unternehmen aus dem ganzen Reich an. Dank der Konzentration von Verwaltungen und unternehmensnahen Dienstleistungen sowie dank der Ansiedlung einer Börse, großer Bankhäuser und einer Reihe wichtiger Zusammenschlüsse der Industrie etablierte sich die Stadt schon zu Beginn des 20. Jahrhunderts als „Schreibtisch des Ruhrgebiets“. 1909 wurde ein Zeppelinflugfeld auf der Golzheimer Heide eingerichtet. Im gleichen Jahr erfolgten die ersten großen Eingemeindungen seit dem Mittelalter. Dadurch wuchs die Stadt um 62,5 km² und erreichte mit einem Zuwachs von rund 63.000 Einwohnern eine Gesamteinwohnerzahl von 345.000. Ihren neuen Zuschnitt nahm die Stadt zum Anlass, im August 1910 eine "Internationale Städtebau-Ausstellung" abzuhalten, zu deren Gelingen neben deutschen Großstädten auch Chicago, Boston, London, Zürich, Kopenhagen, Stockholm und Helsinki stadtplanerische Exponate beitrugen. In der 1912 folgenden "Städte-Ausstellung Düsseldorf für Rheinland, Westfalen und benachbarte Gebiete" wurden Pläne für die „Millionenstadt Düsseldorf“ vorgestellt. Der US-amerikanische Publizist und Reformer Frederic C. Howe pries Düsseldorfs Stadtentwicklung als vorbildlich. Das Wachstum der Stadt schien den Zeitgenossen unaufhaltsam zu sein. Der Ausbruch des Ersten Weltkrieges traf Düsseldorf vollkommen unvorbereitet.

Am 31. Juli 1914 übernahm das Militär die Exekutive und am folgenden Tag wurde die allgemeine Mobilmachung verkündet. Schon bald veränderte sich das Leben in der Stadt merklich. Die Düsseldorfer Industrie stellte auf Kriegsproduktion um und wurde eine der größten Waffenschmieden des Reiches. Die Stadt wandelte sich zu einem Nachschubzentrum und Lazarettstandort. 1915 waren 46.000 Reservisten in Düsseldorf stationiert, 1917 gab es rund 8000 Lazarettbetten. Durch den wirtschaftlichen Niedergang sank der Hafenumschlag auf unter 30 % des Vorkriegsniveaus. Die Geburtenzahlen verringerten sich um 42 %; es herrschte Mangel an Lebensmitteln und Kleidung; die Sterberaten stiegen massiv an; über 10.000 Soldaten kehrten nicht mehr zurück. Im Juni 1917 kam es wegen des Hungers in der Bevölkerung zu Protesten und zu Plünderungen von Läden. Mehrfach wurde der Belagerungszustand verkündet.

Am 8. November 1918 trugen aus Köln kommende Matrosen die Novemberrevolution in die Stadt. Es bildete sich ein provisorischer Arbeiter- und Soldatenrat, der in Zusammenarbeit mit der Stadtverwaltung zunächst die öffentliche Ordnung aufrechterhalten konnte. Infolge des Waffenstillstandes von Compiègne, der Beendigung des Ersten Weltkrieges, besetzten am 4. Dezember 1918 belgische Truppen die linksrheinischen Stadtteile. Himmelgeist und das damals noch selbständige Benrath waren britisch besetzt. Der Rest der Stadt lag in der entmilitarisierten Zone entsprechend Artikel 42, 43 des Versailler Vertrages. Die Soldaten schieden formell aus dem Arbeiter- und Soldatenrat aus, der Arbeiterrat formierte sich neu.

Vom 7. bis zum 9. Januar 1919 übernahm nach Streiks, Besetzung von Zeitungsredaktionen und einer Massendemonstration gegen die Regierung Ebert-Scheidemann ein "Vollzugsrat des Arbeiterrates" aus Mitgliedern des Spartakusbundes und der USPD die Macht. Ziel dieser Gruppen war eine Revolution nach russischem Vorbild. Der Hauptbahnhof, das Polizeipräsidium und das Fernsprechamt wurden besetzt. Aus dem Gefängnis "Ulmer Höh" wurden rund 150 Insassen befreit. Oberbürgermeister Oehler, Regierungspräsident Kruse und einige andere Personen des öffentlichen Lebens konnten sich ins belgisch besetzte Oberkassel retten, andere angesehene Bürger wurden als Geiseln genommen. Aus Protest legten am 10. Januar die städtischen Beamten die Arbeit nieder. Ein "Vollzugsrat des Arbeiterrates" erklärte die Einsetzung Karl Schmidtchens als Oberbürgermeister. Es kam zu Streiks und blutigen Zusammenstößen mit zahlreichen Toten und Schwerverletzten in der Graf-Adolf-Straße. Nach fünf Wochen, am 28. Februar 1919, wurde die Stadt vom Freikorps Lichtschlag erobert und der Vollzugsrat abgesetzt. Dennoch kam es bis Mitte April 1919 immer wieder zu bewaffneten Auseinandersetzungen zwischen Spartakisten und den reaktionären Freikorpstruppen, insbesondere während der Generalstreiksbewegung an der Ruhr vom 8. bis zum 13. April. Heftig umkämpft war der Stadtteil Oberbilk, der nur mit Artillerieunterstützung erobert werden konnte. Bis 1933 blieb Düsseldorf in weiten Teilen dennoch eine „rote“, von der Arbeiterbewegung geprägte, Stadt in Preußen, das 1918 durch den Sturz der Hohenzollern-Monarchie als Freistaat Preußen eine Republik im Deutschen Reich geworden war.

Am 8. März 1921 rückten gegen Mittag französische und belgische Truppen in Düsseldorf und anderen Ruhrgebietsstädten ein und besetzten sie. Hintergrund war die Weigerung der Reichsregierung, Reparationszahlungen aus dem Versailler Vertrag in Höhe von 269 Milliarden Goldmark anzuerkennen. Zwei Jahre später begannen die Franzosen von ihren Brückenköpfen Duisburg und Düsseldorf aus mit der Besetzung des Ruhrgebiets. Mit Annahme des Dawes-Plans am 1. September 1925 durch die deutsche Regierung endete die Besetzung. Aus diesem Anlass kam Reichspräsident Paul von Hindenburg nach Düsseldorf und hielt im Rheinstadion vor rund 50.000 Zuhörern eine patriotische Rede.

1926 fand mit der "GeSoLei" die mit 7,5 Millionen Besuchern größte Messe der Weimarer Republik im und am dafür konzipierten Ehrenhof statt.

1929 ging der Landkreis Düsseldorf größtenteils im neuen Landkreis Düsseldorf-Mettmann auf, der nördliche Teil wurde den Städten Duisburg und Mülheim zugeschlagen. Düsseldorf-Mettmann wurde bei der Kreisreform 1975 in Kreis Mettmann umbenannt.

Am 13. April 1931 begann in Düsseldorf der Strafprozess zu einem der spektakulärsten Kriminalfälle der Weimarer Republik. Zu Gericht saß der schon von 1894 bis 1921 und seit 1925 wieder in Düsseldorf wohnende Serienmörder Peter Kürten, den die Boulevardpresse wegen seiner Vorliebe für das Blut seiner zahlreichen Opfer den „Vampir von Düsseldorf“ nannte. Der Prozess, der auch große internationale Beachtung fand – an die neunzig Auslandskorrespondenten hatten sich angesagt –, führte am 21. April 1931 zu einem Todesurteil, das am 2. Juli 1931 in Köln vollstreckt wurde. In Deutschland löste das Ereignis eine erneute Debatte über die Zulässigkeit der Todesstrafe aus. Der Kriminalfall inspirierte den Regisseur Fritz Lang zu seinem Streifen "M – Eine Stadt sucht einen Mörder", einem der ersten Tonfilme.

Nach der Machtübergabe an die Nationalsozialisten kam es schon am 11. April 1933 in Düsseldorf zur ersten Verbrennung „unerwünschter Literatur“ durch die Deutsche Studentenschaft, unter anderem von Büchern Heinrich Heines. Der NSDAP-Gauleiter Friedrich Karl Florian förderte das massenwirksame Gedenken an Albert Leo Schlageter am Schlageter-Nationaldenkmal, das bereits 1931 errichtet worden war, sowie die personelle Umstrukturierung von Stadtverwaltung und Behörden. Der bisherige Polizeipräsident Hans Langels (Zentrumspartei) wurde abgesetzt und durch den SS-Gruppenführer Fritz Weitzel ersetzt. Zahlreiche Regimegegner wurden verhaftet, misshandelt oder getötet. Düsseldorf war als Hauptstadt des Gaus Düsseldorf (1930–1945) Sitz zahlreicher NS-Verbände und sicherheitspolizeilicher Institutionen: der Staatspolizeileitstelle Düsseldorf, des Höheren SS- und Polizeiführers West (ab 1938), des Inspekteurs der Sicherheitspolizei und des SD, des SS-Oberabschnitts West, des SD-Oberabschnitts West, der SA-Gruppe Niederrhein, der 20. SS-Standarte, eines HJ-Banns (Nr. 39, Obergebiet West, Gebiet Ruhr-Niederrhein), ab 1936 einer Heeresstandortverwaltung und eines Wehrbezirkskommandos der Wehrmacht. Zu den kulturpolitischen „Höhepunkten“ zählten die Propagandaschauen Reichsausstellung Schaffendes Volk (1937) und Entartete Musik (1938).

Am 10. November 1938 wurden in der Pogromnacht die Synagogen auf der Kasernenstraße und in Benrath niedergebrannt, die jüdische Bevölkerung der Stadt wurde verfolgt und mindestens 18 Personen wurden ermordet. Für die Deportation von fast 6000 Juden aus dem gesamten Regierungsbezirk war das „Judenreferat“ der Staatspolizeileitstelle Düsseldorf verantwortlich. Am 27. Oktober 1941 fuhr der erste Zug mit insgesamt 1003 Düsseldorfer und niederrheinischen Juden vom Güterbahnhof Derendorf in die deutschen Konzentrationslager im besetzten Polen (siehe Jüdisches Leben in Düsseldorf). Über 2200 Düsseldorfer Juden wurden ermordet. 1944 lebten in den etwa 400 Lagern Düsseldorfs rund 35.000 ausländische Zivilarbeiter, mehrere tausend Kriegsgefangene sowie KZ-Häftlinge, die Zwangsarbeit leisten mussten.

An die Opfer des Nationalsozialismus in Düsseldorf erinnert seit 1987 die Mahn- und Gedenkstätte Düsseldorf im ehemaligen Polizeipräsidium an der Mühlenstraße (Stadthaus). Es gibt darüber hinaus zahlreiche Düsseldorfer Gedenkorte für Opfer des Nationalsozialismus.

Im Zweiten Weltkrieg fielen 1940 die ersten Bomben auf Düsseldorf. Die alliierten Luftangriffe forderten bis 1945 mehr als 5000 Tote unter der Zivilbevölkerung. Etwa die Hälfte der Gebäude wurde zerstört, rund 90 Prozent wurden beschädigt. Alle Rheinbrücken, die meisten Straßen, Hochwasserdeiche, Unter- und Überführungen sowie das städtische Entwässerungsnetz waren größtenteils zerstört. Die Trümmermenge wurde auf etwa zehn Millionen Kubikmeter geschätzt. Ab dem 28. Februar 1945 wurde Düsseldorf im Zuge der Bildung des Ruhrkessels für sieben Wochen zur Frontstadt mit amerikanischem Dauerbeschuss vom linken Rheinufer und im März immer mehr eingekreist.

Im April versuchten einige Düsseldorfer Bürger des Widerstands um Rechtsanwalt Karl August Wiedenhofen bei Schutzpolizei-Kommandeur Franz Jürgens die Festsetzung des Polizeipräsidenten August Korreng zu erwirken, um die Stadt kampflos an die Alliierten zu übergeben. Der Putschversuch gelang zunächst, wurde dann aber verraten. Nach der Befreiung Korrengs durch loyale Kräfte von Gauleiter Friedrich Karl Florian, der fünf der Widerstandsmitglieder standrechtlich erschießen ließ (darunter Jürgens), gelang es den beiden letzten Mitgliedern Rechtsanwalt Wiedenhofen und Architekt Aloys Odenthal zu entkommen, die im Osten der Stadt heranrückenden amerikanischen Streitkräfte zu erreichen und die endgültige Zerstörung der Stadt durch einen bereits vorbereiteten großen Luftangriff abzuwenden.

Aus Richtung Mettmann kommende Einheiten der U.S. Army besetzten Düsseldorf am 17. April 1945 nahezu kampflos. Nur noch etwa die Hälfte der Bewohner lebte in der in weiten Teilen zerstörten Stadt, die im Zuge der Einteilung Deutschlands in Besatzungszonen unter britische Militärverwaltung kam. Die britische Militärregierung setzte bereits im Juni 1945 eine deutsche Kommunalverwaltung ein. Zum Ende der Kampfhandlungen befanden sich noch etwa 235.000 Menschen in Düsseldorf, zum Jahresende 1945 lebten bereits wieder 394.765 Einwohner in der Stadt. Nach Vorentscheidungen in London gründeten die Briten am 23. August 1946 als einen Nachfolgestaat des nur noch de jure existierenden Freistaats Preußen das Land Nordrhein-Westfalen mit Düsseldorf als Hauptstadt, um die bedeutenden industriellen Ressourcen des Landes der politischen Einflussnahme der Sowjetunion und Frankreichs zu entziehen. Die geografische Zentralität, insbesondere die gewachsene Funktion als wirtschaftliches Entscheidungszentrum („Schreibtisch des Ruhrgebiets“), und das Bestehen unzerstörter Verwaltungsbauten gaben den Ausschlag für die Bestimmung Düsseldorfs zum politischen Zentrum des neuen Landes. Mit Wohnungsnotprogrammen konnten bis 1947 etwa 70.000 Wohnungen zur Verfügung gestellt werden. 1947 fand bereits wieder eine erste Messe in Düsseldorf statt. 1949, dem Gründungsjahr der Bundesrepublik Deutschland, erreichte die Einwohnerzahl Düsseldorfs schon fast wieder Vorkriegsniveau, der systematische Wiederaufbau setzte Anfang der 1950er Jahre ein. Von 1949 bis 1952 war Düsseldorf Sitz der Internationalen Ruhrbehörde, einer Vorläuferin der Europäischen Gemeinschaft für Kohle und Stahl. Dank des Marketingverbundes Igedo und wegen der Nähe zur Textilindustrie konnte sich das Messe- und Ausstellungsgelände am Ehrenhof in dieser Zeit mit der "Verkaufs- und Modewoche Düsseldorf" als neuer deutscher Standort des Modehandels durchsetzen.

Mit dem Neuordnungsplan von 1950 wurden die Grundlagen für die weitere Stadtentwicklung der nächsten Jahrzehnte geschaffen, die das Stadtbild und die Verkehrsführung entscheidend verändern sollte, weitgehend nach dem Leitbild der "Autogerechten Stadt". Zahlreiche Straßen wurden verbreitert und zerstörte Gebäude um zwei bis drei Geschosse höher wieder aufgebaut. Ab Mitte der 1950er Jahre entstanden die ersten Hochhäuser. Düsseldorf entwickelte sich zur Verwaltungsstadt. Dennoch blieb Düsseldorf bis in die 1980er Jahre ein bedeutender Industriestandort. Aufgrund der Nähe zum Ruhrgebiet sowie zur damaligen Bundeshauptstadt Bonn ließen sich zahlreiche Verbände und Interessensvertretungen aus dem Stahlbereich in der Stadt nieder. Die 1960er und 1970er Jahre brachten große Veränderungen. Die Stadt hatte in dieser Zeit den höchsten Bevölkerungsstand ihrer Geschichte. Ab 1961 entstand mit Garath ein völlig neuer Stadtteil in Form einer Trabantenstadt am südlichen Stadtrand. 1965 wurde Düsseldorf Universitätsstadt. Es folgten 1970 die Eröffnung des neuen Schauspielhauses, 1971 der Neuen Messe und 1978 der neuen Tonhalle. 1975 erfolgte die größte Eingemeindung seit 1929. Es entstanden zwei neue Rheinbrücken und es wurde mit dem Bau einer U-Stadtbahn begonnen, deren erste Strecke 1981 eingeweiht werden konnte.

In den 1980er Jahren wurde mit weiteren städtebaulichen Projekten das Stadtbild abermals nachhaltig verändert, dem Neubau des Landtages, der Entwicklung des Medienhafens und dem Bau des Rheinufertunnels, dessen Fertigstellung sich bis in die 1990er Jahre hinzog. Seit 1993 fließt der Autoverkehr unterirdisch und die Altstadt ist mit der Rheinuferpromenade wieder an den Rhein gerückt. In den 1990er Jahren entwickelte sich im Medienhafen ein neues Büro-, Geschäfts- und Freizeitviertel. 1996 vernichtete ein Großbrand ein Terminal des Düsseldorfer Flughafens. Der Flughafen und die Anbindung an die Stadt wurden komplett umgeplant. Die Arbeiten waren 2003 abgeschlossen.

Bei einem Sprengstoffanschlag am Bahnhof Düsseldorf-Wehrhahn wurden am 27. Juli 2000 zehn Menschen zum Teil lebensgefährlich verletzt, eine schwangere Frau verlor ihr ungeborenes Kind. Nach einem Brandanschlag auf die Neue Synagoge in Düsseldorf am 2. Oktober 2000 wandte sich der damalige Bundeskanzler Gerhard Schröder mit einem Appell an die bundesdeutsche Öffentlichkeit, in dem er zum „Aufstand der Anständigen“ aufforderte. Am 25. Mai 2009 erhielt die Stadt den von der Bundesregierung verliehenen Titel „Ort der Vielfalt“.

Seitens der städtischen Verwaltung wird Düsseldorf als schuldenfrei erklärt, was jedoch vom Land NRW und dessen Statistikamt in Frage gestellt wird. Die unterschiedlichen Auffassungen ergeben sich aus unterschiedlichen Bewertungen und sind auch parteipolitisch auf Landes- und Stadtebene motiviert. Zum Stichtag 31. Dezember 2013 hatte die Stadt Verbindlichkeiten in Höhe von insgesamt 383 Millionen Euro. Damit ist Düsseldorf allerdings die am wenigsten verschuldete kreisfreie Stadt Nordrhein-Westfalens.

Folgende Städte, Gemeinden und Gemeindeteile wurden nach Düsseldorf eingegliedert (die Zahlen in Klammern geben den Flächenzuwachs an):


Fußnoten

Am 31. Dezember 2012 betrug die „amtliche Einwohnerzahl“ für Düsseldorf nach Fortschreibung auf Basis des Zensus 2011 593.682 Einwohner. Davon waren 308.014 Frauen (51,88 %) und 285.668 Männer (48,11 %). Der Ausländeranteil betrug 16,55 %, d. h. 98.235 Einwohner. Nach Zählung der Stadt stellten 2006 die Türken mit 15.191 Personen die größte Gruppe der Nichtdeutschen, gefolgt von den Griechen mit 10.591 und den Italienern mit 6890 Personen. Von den außereuropäischen Herkunftsländern stellen die Asiaten (ohne Türken) mit 14.639 die größte Gruppe, darunter Japaner mit 4951, Iraner mit 1419, Chinesen mit 1375 und Koreaner mit 1003 Personen. Stark ansteigend ist die Zahl chinesischer Einwohner der Landeshauptstadt infolge der Ansiedlung von etwa 300 chinesischen Unternehmen (Stand 2011). Zum Stichtag 31. Dezember 2010 hatte Düsseldorf im Vergleich zu den anderen Städten und Gemeinden in Nordrhein-Westfalen den größten Ausländeranteil. Nach Angaben des Statistischen Landesamts hatten 19,3 Prozent der Düsseldorfer einen ausländischen Pass. In Düsseldorf lebten nicht nur die meisten Japaner (nämlich 59 Prozent aller Japaner in Nordrhein-Westfalen), sondern auch die meisten Schweden, Ghanaer, Südkoreaner, Iren, Franzosen und Marokkaner des Landes.

Mit Beginn der Industrialisierung im 19. Jahrhundert setzte in Düsseldorf ein starkes Bevölkerungswachstum ein. Lag die Einwohnerzahl der Stadt 1834 bei rund 20.000, so überschritt sie schon 1882 die Grenze von 100.000, wodurch Düsseldorf zur Großstadt wurde. 1905 hatte die Stadt 250.000 Einwohner, bis 1933 verdoppelte sich diese Zahl auf 500.000. Im Jahre 1962 erreichte die Bevölkerungszahl mit 705.391 ihren historischen Höchststand. In den folgenden Jahren sank die Einwohnerzahl jedoch wieder stark. Dieser Trend konnte auch durch die kommunale Neugliederung in den 1970er Jahren, in deren Folge einige umliegende Gemeinden nach Düsseldorf eingegliedert wurden, nicht gedreht werden. Der Wegzug in die Umlandgemeinden führte dazu, dass sich die Einwohnerzahlen in den 1980er und auch 1990er Jahren bei 570.000 Einwohnern einpendelten. Erst zur Jahrtausendwende kehrte sich der Trend um. So betrug am 30. Juni 2005 die „amtliche Einwohnerzahl“ für Düsseldorf nach Fortschreibung des Landesbetriebs Information und Technik Nordrhein-Westfalen 573.449 (nur Hauptwohnsitze und nach Abgleich mit den anderen Landesämtern).

Am 20. Juli 2014 wurde die 600.000. Einwohnerin Düsseldorfs geboren. Erstmals seit 1978 gibt es wieder mehr als 600.000 Einwohner in Düsseldorf. Das hat das Amt für Statistik ermittelt. Für das Jahr 2030 werden 611.970, 623.600 bzw. 645.000 Einwohner prognostiziert. Bei der Bevölkerungsdichte rangiert Düsseldorf unter den Städten Nordrhein-Westfalens mit 2730,7 Einwohner pro km² hinter Herne und vor Oberhausen auf Platz 2 (Stand: 31. Dezember 2012).

Nach dem Einwohnermelderegister der Stadt Düsseldorf, das seit 2016 seine Bevölkerungszahl aus dem Statistikabzug des Einwohnermelderegisters statt aus der Fortschreibung der Volkszählung von 1987 generiert, ergab sich zum 31. Dezember 2016 eine Zahl von 635.704 Einwohnern. Zum selben Stichtag ermittelte das statistische Landesamt die amtliche Einwohnerzahl von 613.230, mithin 22.474 weniger als das Einwohnermelderegister ausweist.

Der in Düsseldorf nur noch in wenigen Milieus gesprochene Dialekt zählt zum Limburgischen, das durch die Benrather Linie (maache-maake-Grenze) zum auch in Teilen Düsseldorfs gesprochenen Ripuarischen abgegrenzt wird. Durch die Uerdinger Linie (isch-ick-Grenze) im Norden wird es vom Nord-Niederfränkischen unterschieden. Platt wird heutzutage zumeist nur noch von der älteren Generation gesprochen bzw. verstanden. Anstelle des Originalen Düsseldorfer Platt wird in neuerer Zeit häufig ein so genannter Regiolekt benutzt, Rheinisches Deutsch genannt.

Ende 2016 waren 29,7 Prozent der Einwohner römisch-katholisch, 17,7 Prozent evangelisch. 52,7 Prozent gehörten entweder anderen Konfessionen oder Religionen an oder waren konfessionslos. Düsseldorf hat mit rund 7000 Mitgliedern nach Berlin und München bundesweit die drittgrößte jüdische Gemeinde. 

Düsseldorf gehörte von Anfang an zum Erzbistum Köln und war dem Archidiakonat des Domdechanten unterstellt. Obwohl die Reformation anfangs mehrheitlich Fuß fassen konnte, verblieben auch weiterhin Katholiken in der Stadt. Sie gehörten bis 1627 zum Dekanat Neuss, ehe Düsseldorf selbst Sitz eines Dekanats wurde. Der frühzeitige Untergang des Großherzogtums Berg im Jahre 1813 verhinderte die von Napoléon angeregte Gründung eines Bistums Düsseldorf. Ein solches hatte bereits Herzog Wolfgang Wilhelm von Pfalz-Neuburg für seine Hauptresidenz angestrebt. Heute gehören alle katholischen Pfarrgemeinden der Stadt zum Stadtdekanat Düsseldorf, das aus den Dekanaten Nord, Mitte/Heerdt, Ost, Süd und Benrath besteht. In Düsseldorf lebten 2013 etwa 191.000 Katholiken, was einem Bevölkerungsanteil von etwa 32 % entsprach. Der katholische Stadtverband hat seit 2006 im umgebauten Kloster der Maxkirche ein neues Zentralgebäude in der Carlstadt.

Seit 1394 wird Apollinaris von Ravenna, dessen Reliquien im Apollinarisschrein der Stadtkirche St. Lambertus ruhen, als Schutzheiliger und Stadtpatron Düsseldorfs verehrt. Die Größte Kirmes am Rhein ist das Stadtfest zu seinem Namenstag (23. Juli).

Die Reformation konnte sich ab 1527 teilweise durchsetzen, begünstigt vor allem durch den Reformkatholizismus von Herzog Wilhelm V. Neben dem Psalmengesang wurde die Kommunion in beiderlei Gestalt in der Stiftskirche St. Lambertus eingeführt. Dies war die Gründung der lutherischen Gemeinde. 1571 gab es einen erneuten Umschwung am Hofe, dem zufolge die Protestanten unterdrückt wurden. Die lutherische und die 1573 gegründete reformierte Gemeinde trafen sich danach heimlich, bis die Unterdrückung ab 1590 beendet wurde Ab 1609 konnten die Protestanten zunächst ihre Gottesdienste öffentlich abhalten: die Reformierten in ihrem Predigthaus an der Andreasstraße, die Lutheraner an der Berger Straße. 1614 setzte unter dem römisch-katholischen Herrscher Wolfgang Wilhelm wieder die Unterdrückung ein. Bis Mitte des 17. Jahrhunderts konnten die Protestanten nur heimlich ihre Gottesdienste abhalten. Dann erhielten sie das Recht zur freien Religionsausübung. Die erste überlieferte evangelische Predigt in Düsseldorf wurde im Predigthaus an der Bolkerstraße gehalten, das aus dem Jahr 1651 erhalten ist. 1683 konnte sich die reformierte Gemeinde ihre eigene Kirche bauen, die 1916 den Namen Neanderkirche erhielt. Der Turm wurde 1687 fertiggestellt. Im selben Jahr entstand die lutherische Kirche an der Berger Straße. Gehörte die protestantische Gemeinde Düsseldorfs zunächst zur kölnischen Klasse, später zur Bergischen Synode (1589), so wurde Düsseldorf 1611 Sitz einer eigenen Klasse (Kirchenverwaltungsbezirk).

Nach dem Übergang an Preußen vereinigten sich 1825 (→ über die Vereinigung zur Union in Preußen: Agendenstreit) die beiden protestantischen Kirchengemeinden zur „Evangelischen Gemeinde Düsseldorf“, die zur Superintendentur Düsseldorf gehörte. Bereits 1815 war Düsseldorf Sitz des preußischen Oberkonsistoriums der Provinz Jülich-Kleve-Berg geworden, doch zog dieses schon 1816 nach Köln um. 1827 gab es in Düsseldorf eine Synode.

Die protestantische Gemeinde Düsseldorfs wuchs ständig und weitere Kirchen wurden gebaut, so etwa die Johanneskirche am Martin-Luther-Platz (1881), die Christuskirche (1899), die Friedenskirche (1899) und die alte Matthäikirche (1899) sowie die Kreuzkirche (1910). 1905 entstand aus Teilen der Gemeinden Urdenbach und Gerresheim die Kirchengemeinde Eller-Wersten. Durch Eingemeindungen gab es weitere Kirchengemeinden im Stadtgebiet. Am 1. Oktober 1934 wurde der Sitz des Konsistoriums der rheinischen Provinzialkirche Preußens beziehungsweise der Evangelischen Kirche im Rheinland von Koblenz nach Düsseldorf verlegt. Die heutige Kirchenverwaltung ist in der Hans-Böckler-Straße im Stadtteil Golzheim. Weiterhin gibt es ein „Haus der Kirche“ in der Bastionstraße in der Carlstadt. 1936 wurde für alle Düsseldorfer evangelischen Gemeinden ein Gesamtverband gegründet. 1948 wurde die Kirchengemeinde Düsseldorf aufgeteilt. Auch in den Außenbezirken gab es Veränderungen in den Kirchengemeinden.

1964 wurde der Kirchenkreis Düsseldorf in die Kirchenkreise Düsseldorf-Mettmann, Düsseldorf-Nord, Düsseldorf-Ost und Düsseldorf-Süd aufgeteilt, wobei der Kirchenkreis Düsseldorf-Mettmann vor allem Kirchengemeinden außerhalb der Stadt Düsseldorf umfasst. Die drei Kirchenkreise im Stadtgebiet bildeten bis Mitte 2007 den Kirchenkreisverband Düsseldorf innerhalb der Evangelischen Kirche im Rheinland. Am 16. Juni 2007 trat die Synode des neugebildeten Kirchenkreises Düsseldorf erstmals zusammen. Er ist aus dem Zusammenschluss der Kirchenkreise Düsseldorf-Nord, Düsseldorf-Ost und Düsseldorf-Süd hervorgegangen und repräsentiert 24 evangelische Gemeinden und damit 116.550 Protestanten der Landeshauptstadt, die rund 20 Prozent der Bevölkerung ausmachen.

Als Reaktion auf die Vereinigung der Evangelisch-Lutherischen Kirche in Preußen und einiger reformierter Gemeinden zur unierten Evangelischen Kirche in Preußen durch Kabinettsorder von König Friedrich Wilhelm III. 1817 und 1830 bildete sich die Evangelisch-Lutherische (altlutherische) Kirche Preußens. Die Altlutheraner bestanden auf die Anerkennung des lutherischen Bekenntnisses. Sie forderten uneingeschränkte lutherische Gottesdienste, Verfassung und Lehre. Nach harter Verfolgungszeit seitens des Staates und unter Billigung der neuen evangelischen Kirche der Union konnten sie sich 1841 unter König Friedrich Wilhelm IV. konstituieren und wurden anerkannt. Ab 1844 wurden in Düsseldorf wieder lutherische Gottesdienste gefeiert in einer Gemeinde aus Lutheranern der Gemeinde vor der Zwangsvereinigung sowie Zugewanderten aus Sachsen und Bayern. 1882 weihte die Gemeinde ein eigenes Gotteshaus in der Kreuzstraße, das am 12. Juni 1943 einem Luftangriff zum Opfer fiel. 1884 wurde die Gemeinde vom preußischen Staat als juristische Person anerkannt. Da das Grundstück in der Kreuzstraße nach dem Krieg aus stadtplanerischen Gründen nicht mehr bebaut werden durfte, erwarb die Gemeinde ihr jetziges Grundstück und weihte am 2. April 1956 in der Eichendorffstraße in Stockum ihre Erlöserkirche. Die Kirchengemeinde gehört heute zum Kirchenbezirk Rheinland der Selbständigen Evangelisch-Lutherischen Kirche (SELK). In dieser Kirchengemeinde wurde auch Silvia Sommerlath, die nachmalige Königin Silvia von Schweden, durch Superintendent Nagel konfirmiert.

In Düsseldorf ist die "Kommission der orthodoxen Kirche in Düsseldorf" beheimatet mit Gemeinden der griechisch-orthodoxen Kirche Am Schönenkamp in Reisholz, der georgisch-orthodoxen Kirche in der Fährstraße in Düsseldorf-Hamm, der deutschsprachigen Orthodoxen Parochie zu den heiligen Erzengeln, der russisch-orthodoxen Kirche, der rumänisch-orthodoxen Kirche und der serbisch-orthodoxen Kirche, der koptischen Kirche auf dem Pöhlenweg in Grafenberg und der ukrainisch-orthodoxen Kirche.

Die Anglikanische Kirche, die mit der alt-katholischen Kirche in voller Kirchengemeinschaft "(full communion)" steht, ist mit einer Gemeinde in der Rotterdamer Straße am Nordpark ansässig. Die Pfarrkirche der alt-katholischen Gemeinde Düsseldorf ist die Thomaskirche, die vormalige Klarenbachkapelle in der Steubenstraße in Reisholz.

In Düsseldorf sind neben den anderen großen christlichen Konfessionen auch zahlreiche Freikirchen beheimatet: die Apostolische Gemeinschaft mit ihrem Deutschlandsitz und der Düsseldorfer Hauptgemeinde in der Cantadorstraße sowie der Gemeinde in Eller (Klein Eller); das Christliche Zentrum Düsseldorf (Pfingstbewegung) in der Bruchstraße; die Evangelisch-Freikirchlichen Gemeinden (Baptisten) in der Acker-, Luisen- und Christophstraße; die Evangelisch-methodistische Kirche in der Matthiaskirche im Stadtteil Lichtenbroich; die Freie evangelische Gemeinde in der Bendemannstraße; die Heilsarmee; die Herrnhuter Brüdergemeine in Heerdt; die Jesus-Haus-Gemeinde (Pfingstbewegung) in der Grafenberger Allee. Außerdem gibt es die Mosaik-Gemeinde, derzeit in Derendorf. 1990 und 2001 veranstalteten die Düsseldorfer Freikirchen einen Freikirchentag im Robert-Schumann-Saal und auf dem früheren BUGA-Gelände.

Alle bis hier genannten Kirchen, also die beiden katholischen, die evangelische, einige orthodoxe und die Freikirchen sind Mitglied in der Arbeitsgemeinschaft Christlicher Kirchen (ACK).

Zu den christlichen Sondergemeinschaften gehören auch Jehovas Zeugen, die mit 18 Versammlungen (Gemeinden) und vier Gruppen in Düsseldorf vertreten sind. Die Zusammenkünfte (Gottesdienste) werden in fünf Königreichssälen im Düsseldorfer Stadtgebiet abgehalten. Das größte Saalzentrum (mit vier Sälen) befindet sich in Flingern-Süd. Hier werden neben mehreren Zusammenkünften in Deutsch (2 Versammlungen), auch Zusammenkünfte in Englisch, Russisch, Polnisch, Griechisch, Italienisch, Rumänisch, Kroatisch/Serbisch, Tagalog, Chinesisch und deutscher Gebärdensprache abgehalten. Im Königreichssaalzentrum Eller (mit zwei Sälen) werden die Zusammenkünfte, neben Deutsch, auch in Russisch, Spanisch, Hindi und Twi abgehalten. Weitere Königreichssäle befinden sich in Oberkassel, Pempelfort und Hellerhof. Hier werden Zusammenkünfte in Deutsch (3 Versammlungen), Japanisch und Vietnamesisch abgehalten. Außerdem werden mehrere besondere Veranstaltungen, wie z.B. eine Bibelausstellung, vorzugsweise im Königreichssaal Flingern-Süd öffentlich zugänglich abgehalten.

Ferner sind in Düsseldorf die Christengemeinschaft, die Kirche Jesu Christi der Heiligen der Letzten Tage (Mormonen), die Neuapostolische Kirche mit fünf Gemeinden in Benrath, Derendorf, Eller, Flingern und Gerresheim vertreten. Daneben gibt es in Düsseldorf die russischsprachige jüdisch-messianische Gemeinde "Beit Hesed", die auch die deutschsprachige Zeitschrift "Kol Hesed" herausgibt.

Die jüdische Gemeinde Düsseldorf ist mit ca. 7500 Mitgliedern die größte in Nordrhein-Westfalen und die drittgrößte in Deutschland. Ihre neue Synagoge wurde 1958 gebaut und liegt in der Zietenstraße im Stadtteil Golzheim. Sie wird rund um die Uhr von der Polizei bewacht. Die alte Synagoge stand in der Kasernenstraße in der Carlstadt auf dem heutigen Grundstück des Handelsblattverlages. Sie ist den Novemberpogromen 1938 zum Opfer gefallen.

Die Gemeinde als Körperschaft des öffentlichen Rechts ist gemäß ihrer Satzung eine Einheitsgemeinde. Das bedeutet, dass alle religiösen Richtungen respektiert werden. Die Gottesdienste entsprechen dem orthodoxen Ritus. Rabbiner war bis Juli 2011 Julien Chaim Soussan, einer der jüngsten Gemeinderabbiner in Deutschland. 90 % der Gemeindemitglieder stammen aus der ehemaligen Sowjetunion. Zur Gemeinde gehören u. a. ein Kindergarten und eine Grundschule, die Yitzhak-Rabin-Schule. Sie ist eine staatlich anerkannte Grundschule und eine jüdische Konfessionsschule, die für die koschere Ernährung der Kinder sorgt. Kürzlich stellte sich in einer landesweiten Vergleichsarbeit heraus, dass die Schule zu den 25 besten Grundschulen des Landes Nordrhein-Westfalen gehört. Die Gemeinde verfügt auch über einen Sportverein (Makkabi), ein Jugendzentrum und einen Friedhof.

In der Landeshauptstadt gibt es auch eine Reihe muslimischer Gemeinden. Diese bilden jedoch keinen einheitlichen Verband, sondern sind gemäß der nationalen Zugehörigkeit ihrer Mitglieder als türkische, bosnische, marokkanische und sonstige Moscheevereine organisiert. Die größte türkische Vereinigung, die Türkisch-Islamische Union der Anstalt für Religion e. V., besitzt in Düsseldorf drei Moscheen. Sie befinden sich in Lörick, Eller und Derendorf. Insgesamt gibt es in Düsseldorf rund 20 Moscheen. Die Freitagsgebete werden nach Angaben der verschiedenen Trägervereine von rund 4000 Gläubigen besucht, wobei die beiden größten Moscheen in Derendorf und Flingern bis zu 1000 Teilnehmern Platz bieten. Gepredigt wird u. a. auf Türkisch, Arabisch, Berberisch, Bosnisch, Albanisch, Romani und Deutsch. Auch Aleviten sind in Düsseldorf mit einer Gemeinde im Stadtteil Eller vertreten. Das Herkunftsland fast aller Aleviten ist die Türkei. In der Düsseldorfer Gemeinde werden neben religiöser Arbeit auch kulturelle und musikalische Projekte umgesetzt. In NRW besitzt die Alevitische Gemeinde Deutschland den Status einer nach dem Grundgesetz anerkannten eigenständigen Religionsgemeinschaft und koordinieren den Alevitischen Religionsunterricht als ordentliches Schulfach mit. Auch die Aleviten in Düsseldorf gehören diesem Dachverband an.

Im linksrheinischen Stadtteil Niederkassel liegt der einzige buddhistische Tempel in der Tradition der Jōdo-Shinshū Europas, auf dem Grundstück des japanischen Ekō-Hauses. Er ist im japanischen Stil als Betonkonstruktion errichtet und von einem japanischen Garten umgeben. Dem Ekō-Haus sind außerdem ein traditionelles japanisches Haus für Teezeremonien, eine Bibliothek und ein Kindergarten angeschlossen.

Ferner gibt es eine Anzahl buddhistischer Zentren in Düsseldorf aller namhaften Traditionen des Buddhismus. Eine Auswahl: Rigpa (tibetischer Buddhismus, Lehrer Sogyal Rinpoche), Amitabha-Stiftung (ebenfalls tibetischer Buddhismus) sowie Kanzeon Sangha (Zen-Tradition) und Diamantweg-Buddhismus (Lama Ole Nydahl) sowie weitere buddhistische Gruppen und Zentren. Somit offerieren die buddhistischen Gruppen in Düsseldorf ein großes Angebot in Nordrhein-Westfalen.

An der Spitze der Stadt Düsseldorf standen im 13. Jahrhundert die Schöffen, die bis 1806 die oberste und mächtigste Klasse in der Stadtverwaltung darstellten. Seit 1303 ist ein Bürgermeister genannt, der anfangs ebenso ein Schöffe war. Daneben gab es ab 1358 auch einen Rat, der sich teilweise in einen Alten und einen Jungen Rat aufteilte. Die Mitglieder wurden entweder auf Lebenszeit gewählt (Alter Rat), oder aber auch jährlich bestimmt (Junger Rat). Als herzoglicher Vertreter war ferner ein Schultheiß an der Verwaltung der Stadt beteiligt, der den Titel „Amtmann“ führte. Etwa seit dem 15. Jahrhundert gab es neben den genannten Gremien auch einen Gemeindeausschuss von 12 Personen („Zwölfer“), der an der Wahl des Bürgermeisters teilnahm und zu wichtigen Beschlussfassungen herangezogen wurde, eigentlich aber keine wirkliche Bürgerbeteiligung darstellte. Erst in französischer Zeit gab es einen Munizipalrat, ab 1815 einen Gemeinderat mit 30 Mitgliedern. Seit 1856 waren es die „Stadtverordneten“, später Ratsherren, deren Gesamtzahl sich mehrmals veränderte. Die Leitung der Stadt übernahm in französischer Zeit der Maire, der von drei Beigeordneten unterstützt wurde. Seit preußischer Zeit trug das Stadtoberhaupt den Titel Oberbürgermeister. 1856 wurde die Rheinische Städteordnung eingeführt.

Während der Zeit der Nationalsozialisten wurde der Oberbürgermeister von der NSDAP eingesetzt. Nach dem Zweiten Weltkrieg setzte die Militärregierung der Britischen Besatzungszone einen neuen Oberbürgermeister ein und führte 1946 die Kommunalverfassung nach britischem Vorbild ein. Danach gab es einen vom Volk gewählten „Rat der Stadt“, dessen Mitglieder man als „Stadtverordnete“ bezeichnete. Der Rat wählte anfangs aus seiner Mitte den Oberbürgermeister als Vorsitzenden und Repräsentanten der Stadt, der sein Amt ehrenamtlich ausübte. Des Weiteren wählte der Rat ab 1946 ebenfalls einen hauptamtlichen Oberstadtdirektor als Leiter der Stadtverwaltung. 1999 wurde die Doppelspitze in der Stadtverwaltung aufgegeben. Seither gibt es nur noch den hauptamtlichen Oberbürgermeister. Dieser ist Vorsitzender des Rates, Leiter der Stadtverwaltung und Repräsentant der Stadt. Er wurde 1999 erstmals direkt von den Bürgern gewählt.


Die Gesamtsumme der Verschuldung der Stadt Düsseldorf (des öffentlichen Bereichs) belief sich zum Jahresende 2012 auf 872,2 Millionen Euro. Das sind 1478 Euro pro Einwohner. Von den 103 kreisfreien Städten in Deutschland lag Düsseldorf damit auf Platz 100 bei der Pro-Kopf-Verschuldung; das heißt, nur in drei anderen kreisfreien Städten war die Pro-Kopf-Verschuldung geringer.

Für das Haushaltsjahr 2014 hat die Stadt Düsseldorf im Gesamtergebnisplan einen Haushaltsüberschuss in ordentlichen Erträgen und Aufwendungen (einschließlich Finanzerträgen und -aufwendungen) in Höhe von 3,1 Millionen Euro (5 Euro je Einwohner) veranschlagt. Die Stadt Düsseldorf ist damit (neben Krefeld und Münster) eine von nur drei kreisfreien Städten in Nordrhein-Westfalen, die im Jahr 2014 kein Haushaltsdefizit im Gesamtergebnisplan aufweist.

Aktuell gibt es im Stadtrat folgende Sitzverteilung nach Fraktionen (Stand Feb. 2017):

Im Oktober 2014 haben SPD, Grüne und FDP eine Kooperationsvereinbarung beschlossen. Diese drei Parteien verfügen im Stadtrat über exakt die Hälfte der Sitze (41 von 82). Zusammen mit der Stimme des Oberbürgermeisters haben sie eine Mehrheit von einer Stimme.

Für private und geschäftliche Zwecke entstand im Februar 2002 ein Stadtwappen, das von den offiziellen Stadtfarben abweicht und ohne Genehmigung der Stadtverwaltung benutzt werden kann. In dem rot-weiß geteilten Schild befindet sich ein silberner doppelgeschwänzter, aufgerichteter, silbernbekrönter und -bewehrter Löwe mit gesenktem schwarzem Anker in den Pranken.

Die Stadt Düsseldorf verwendet in offiziellen Schreiben und Publikationen ein Logo, das in einem Quadrat in der linken Hälfte die Rheinschleifen im Düsseldorfer Stadtgebiet andeutet und im rechten oberen Quadranten den Bergischen Löwen mit Anker zeigt. Das Stadtlogo wird in verschiedenen Farben verwendet, wobei jedem der derzeit acht Dezernate jeweils ein Farbton zugeordnet ist.

2011 startete die Stadt zur Verbesserung ihres Marketings und ihrer öffentlichen Wahrnehmung einen Prozess der Entwicklung einer Dachmarke. Nach einer Markenkernanalyse folgte ein öffentlicher Wettbewerb, der über 2000 Foto-, Video- und Textbeiträge ergab. Im März 2012 begann ein Verfahren der Interessenbekundung zu einem Wettbewerb von Kreativagenturen. Den sich anschließenden Wettbewerb zur Entwicklung der Dachmarke gewann die Agentur BBDO Proximity, die das Logo der neuen Dachmarke, das Emoticon ":D", am 26. November 2012 der Öffentlichkeit vorstellte. Das Düsseldorfer "„smiling :D“" ist in Rot, der Farbe des Bergischen Löwen, und in der Schriftart Helvetica ausgeführt. Es hat insbesondere die Aufgabe, „das emotionale, sympathische Düsseldorf“ zu transportieren. Die Verfahren bei der Einführung der neuen Dachmarke und ihres Logos waren begleitet durch eine öffentliche Kontroverse.

Düsseldorf unterhält sieben klassische Städtepartnerschaften:


Freundschaftliche Beziehung gibt es weiterhin zu:

Ansehen genießt Düsseldorf ebenfalls hinsichtlich Kultur, Kunst und moderner Architektur. So gibt es neben der großen Kunstsammlung Nordrhein-Westfalen und einer Menge weiterer Museen und Galerien auch die international renommierte Kunstakademie Düsseldorf, die im 19. Jahrhundert die Düsseldorfer Malerschule und im 20. Jahrhundert die Düsseldorfer Photoschule hervorgebracht hat. Bekannte Bühnen sind mit dem Schauspielhaus und dem Kom(m)ödchen in der Stadt vertreten. Zudem sind einige der populären Musiker und Dichter Deutschlands in der Stadt geboren oder waren dort beheimatet. Bedeutende Architekten haben nicht nur im Medienhafen ihre Projekte verwirklicht.

Düsseldorf hat eine Theatertradition, die sich bis in das 16. Jahrhundert zurückverfolgen lässt. Die ersten theatralischen Veranstaltungen werden auf das Jahr 1585 datiert. Das heutige "Düsseldorfer Schauspielhaus" mit seiner modernen geschwungenen Architektur wurde 1970 fertiggestellt. Es ist am Gustaf-Gründgens-Platz, der nach dem ehemaligen Intendanten benannt ist, gelegen. Die größte Düsseldorfer Bühne hat eine große Bekanntheit im deutschsprachigen Raum.

Weitere größere Theater in der nordrhein-westfälischen Metropole sind das "Forum Freies Theater", bestehend aus "Juta" (Jugendtheater) und "Kammerspielen", das ein breites Spektrum an Bühnenkunst bietet, die "Komödie Düsseldorf", ein klassisches Boulevardtheater, das "Theater an der Kö", das vor allem Komödien und moderne Theaterstücke zu bieten hat und von der bekannten Theaterfamilie "Heinersdorff" geführt wird, das "Theater an der Luegallee" in Oberkassel, das "Theater FLIN" in Flingern sowie das "Savoy-Theater". Im JUTA gastiert häufig das "Theater der Klänge". Es ist ein seit 1987 bestehendes Tourneetheater, das unter der Leitung von Jörg Udo Lensing jährlich mit einer Neuproduktion auf zumeist kurze Tourneen geht.

Für Kinder ist das "Theateratelier Takelgarn" mit Comedy, Kabarett, Figuren- und Kindertheater besonders interessant.
Das "Puppentheater an der Helmholtzstraße" richtet sich ebenso wie das "Düsseldorfer Marionetten-Theater" an Kinder und Erwachsene gleichermaßen. Letzteres wurde 1956 gegründet und befindet sich im Palais Wittgenstein, das daneben noch weitere kulturelle Projekte beheimatet.

Sehr traditionsreich ist auch die "Deutsche Oper am Rhein". Sie zeigt an ihren beiden Standorten Düsseldorf und Duisburg Oper, Operette und Ballett.

Das Apollo Varieté unterhalb der Rheinkniebrücke am carlstädtischen Rheinufer bietet klassisches Varieté-Theater im Stil des frühen 20. Jahrhunderts.

Die 1925 als Planetarium errichtete "Tonhalle Düsseldorf" ist Veranstaltungsort für Konzerte und sonstige musikalische Veranstaltungen aus den Bereichen Klassik, Jazz, Pop und Kabarett.

Das Capitol Theater ist das größte Theater der Landeshauptstadt und bietet eine Bühne für wechselndes Musical- und Live-Entertainment.

Das "Kom(m)ödchen" ist die älteste noch bestehende Kabarettbühne Deutschlands. Gegründet wurde es 1946 von Kay und Lore Lorentz. Viele später bedeutende Kabarettisten konnten sich hier erstmals bewähren.

In Düsseldorf ist das Tanzhaus NRW ansässig, die Institution bietet neben einem Bühnenprogramm auch zahlreiche Kurse an.

Die Stadt verfügt über ein vielfältiges Ausstellungsangebot. Allein die 18 städtischen Museen zogen seit 2001 jährlich regelmäßig über eine Million Besucher an, im Jahr 2006 waren es 1,34 Millionen Menschen. Daneben locken mehrere private Museen, Sammlungen und zahlreiche Galerien Besucher an. Meistbesuchtes Museum ist der Aquazoo im Nordpark.

Düsseldorf hat eine lange Tradition als Kunststadt. Die erste große Gemäldesammlung legten bereits der Kurfürst Johann Wilhelm zu Pfalz-Neuburg, kurz Jan-Wellem, und seine Frau Anna Maria de Medici an. Untergebracht wurde die Sammlung in der 1709 bis 1714 errichteten Gemäldegalerie Düsseldorf, einem der frühesten selbständigen Museumsbauten der Welt. Unter Kurfürst Maximilian IV., dem späteren König Maximilian I. Joseph, wurde die Sammlung 1805 zum großen Teil nach München verbracht und bildete dort den Grundstock zur Alten Pinakothek.

Die Kunstakademie Düsseldorf erlangte bereits im 19. Jahrhundert unter dem Begriff "Düsseldorfer Malerschule" als eine der wichtigsten Ausbildungsstätten von Landschafts- und Genremalern Bedeutung. Der "Kunstverein für die Rheinlande und Westfalen" wurde im Jahre 1829 gegründet, um den in Düsseldorf ausgebildeten Malern eine Möglichkeit der Präsentation ihrer Werke in wechselnden Ausstellungen zu bieten.

Einige in Düsseldorf verbliebenen Exponate der kurfürstlichen Gemäldegalerie sowie etliche Werke der Düsseldorfer Malerschule wurden in das "Museum Kunstpalast" übernommen. Es beinhaltet darüber hinaus Graphiken, Zeichnungen, Gemälde und Skulpturen aus allen Stilepochen von der Antike bis ins 21. Jahrhundert. Neben europäischen Exponaten sind weitere Schwerpunkte der Sammlung japanische Holzschnitte und Netsuke. Es ist im Ehrenhof-Komplex integriert, der auch das NRW-Forum beheimatet.

Der zweite Höhepunkt in der Entwicklung Düsseldorfs als Kunststadt folgte in der zweiten Hälfte des 20. Jahrhunderts, als u. a. Joseph Beuys an der Kunstakademie unterrichtete. Als Landeshauptstadt Nordrhein-Westfalens beherbergt Düsseldorf die "Kunstsammlung Nordrhein-Westfalen". Sie ist gegliedert in das "K20" am Grabbeplatz, das "K21" im Ständehaus und das Schmela-Haus in der Mutter-Ey-Straße. Sie hat sich vor allem auf Kunst des 20. und 21. Jahrhundert spezialisiert, entsprechend der Namensgebung der Teilsammlungen aufgeteilt auf die Standorte. Die Kunsthalle Düsseldorf befindet sich gegenüber dem K20 am Grabbeplatz und hat ihren Schwerpunkt auf zeitgenössischer nationaler und internationaler Gegenwartskunst. Einen sehr ungewöhnlichen Standort hat hingegen das KIT (Kunst im Tunnel), das der Kunsthalle Düsseldorf angegliedert ist. Es befindet sich am Mannesmannufer und ist ein unterirdischer Ausstellungsraum für zeitgenössische Kunst.

Zwei Düsseldorfer Museen widmen sich Dichtern. Mit Johann Wolfgang von Goethe befasst sich das "Goethe-Museum", das sich im Schloss Jägerhof im nordöstlichen Teil des Hofgartens befindet. Die sehr umfangreiche, inzwischen deutlich erweiterte Sammlung aus Privatbesitz ist in mehreren Stockwerken untergebracht. Sie enthält unter anderem die Originalreinschrift von Goethes bekanntem Gedicht Gingo biloba.

Heinrich Heine, dem wohl berühmtesten Sohn der Stadt, gilt das Heinrich-Heine-Institut in der Carlstadt. Es zeigt u. a. Originaldokumente und -schriften von und über Heine, Stücke aus seinem Nachlass sowie seine Totenmaske.

Ebenfalls in der Carlstadt gelegen ist das "Filmmuseum" mit seinem angeschlossenen Kino. Das Theater-Museum mit dem Dumont-Lindemann-Archiv befindet sich im Hofgärtnerhaus im Hofgarten.

Das meistbesuchte Museum Düsseldorfs, mit über 400.000 Besuchern pro Jahr, ist der "Aquazoo". Es befindet sich seit den späten 1980er Jahren im Norden der Stadt im Nordpark. Zuvor war es in einem Bunker gegenüber dem Zoopark untergebracht. Neben Wasserlebewesen werden dort auch Weichtiere und geologische Exponate gezeigt. Der Zoologische Garten Düsseldorf wurde im Zweiten Weltkrieg zerstört und nicht wiederaufgebaut.

Eine weitere Attraktion des Nordparks ist der von der japanischen Gemeinde gestiftete Japanische Garten, der 1976 eröffnet wurde. Allerdings bekam Düsseldorf bereits 1904 als erste deutsche Stadt einen Japanischen Garten, der sich etwa an der Stelle des ebenfalls am Rhein gelegenen Ehrenhofs befand.

Auf das ganze Stadtgebiet verteilt, ist Düsseldorf von Gartenanlagen durchzogen, was auf das Leitbild der Stadtentwicklung während der Industrialisierung zurückzuführen ist, die wiederum die gärtnerischen Anlagen des 18. und 19. Jahrhundert weiterentwickelte und Düsseldorfs Ruf als Gartenstadt begründete. Dem trägt das Museum für Europäische Gartenkunst Rechnung, das zur Stiftung Schloss und Park Benrath gehört, ebenso wie das Naturkundliche Heimatmuseum, in dem in Dioramen die verschiedenen Lebensräume der Region, vom Rhein bis zur Lösshochfläche, dargestellt werden.
Im Jahre 1987 fand die Bundesgartenschau (BUGA) in Düsseldorf statt, deren Gelände heute den Südpark bildet.

Das "Stadtmuseum" in der Altstadt besitzt eine große Ausstellung, die die Entwicklung der Stadt Düsseldorf historisch-chronologisch nachvollzieht. Die "Mahn- und Gedenkstätte Düsseldorf" erinnert an die Opfer der Naziherrschaft im 20. Jahrhundert. Sie befindet sich in historischen Räumen, die in der NS-Zeit als Büros, Vernehmungsräume und Haftzellen der Polizei genutzt wurden, und beinhaltet die Dauerausstellung „Verfolgung und Widerstand in Düsseldorf 1933–1945“.

Zum klassischen Brauchtum in Düsseldorf gehört der Karneval. Ihm widmet sich im Haus des Karnevals das "Karnevalmuseum" in der Altstadt. Ebenfalls in der Altstadt gelegen ist das "Schifffahrtsmuseum" im Schlossturm. Es befindet sich im Schlossturm am Rheinufer und zeigt die Entwicklung der Rheinschifffahrt von der Antike bis in die Neuzeit. Das "Senfmuseum" würdigt die Tradition der fast 300-jährigen Senfherstellung. Das "Hetjens-Museum (Deutsches Keramikmuseum)" hat seinen Sitz im Palais Nesselrode in der Carlstadt und zeigt als einziges Institut weltweit Keramikprodukte verschiedener Kulturen aus sämtlichen Epochen in einer Dauerausstellung. Im Ehrenhof-Komplex ist das "NRW-Forum" beheimatet, das Ausstellungen zu verschiedenen Themen befristet zeigt, insbesondere fotografische Werke.

Zum dritten und vorläufig letzten Mal fand vom 5. April bis 10. August 2014 die Quadriennale statt. Unter dem Leitmotiv "Über das Morgen hinaus" präsentierten mehrere Kulturinstitute ein Kulturfestival mit Sonderausstellungen und Begleitprogramm.

Die Universitäts- und Landesbibliothek (ULB) Düsseldorf entstand 1965 bis 1969 aus der Bibliothek der Medizinischen Akademie und wurde 1970 gegründet. Die ULB verfügt über eine Zentralbibliothek und fünf weitere Standorte, die insgesamt rund 2,4 Millionen Medien vorhalten. Es gibt rund 23.000 regelmäßige Nutzer.

Die Stadt Düsseldorf unterhält eine Stadtbücherei mit einer Zentralbibliothek, 14 Stadtteilbibliotheken und einer Autobücherei. Insgesamt werden rund 800.000 Medien angeboten. Die über 1,4 Millionen Besucher liehen im Jahr über 4,7 Millionen Medien aus.

Düsseldorf hat eine reichhaltige Musiktradition, die bereits vor dem 19. Jahrhundert einsetzte. Bedeutende Musiker wie Johann Hugo von Wilderer, Friedrich August Burgmüller, Felix Mendelssohn Bartholdy und das Ehepaar Clara und Robert Schumann hatten zeitweise ihre Wirkungsstätte in der Stadt gefunden.

Aber auch im 20. Jahrhundert hatte Düsseldorf in so unterschiedlichen Stilrichtungen wie dem Jazz, der Neuen Deutschen Welle oder dem Punkrock eine führende Rolle.

Das bedeutendste Konzerthaus der Landeshauptstadt ist die Tonhalle Düsseldorf mit mehr als 200 Veranstaltungen im Jahr. Die Tonhalle ist auch der Sitz der Düsseldorfer Symphoniker, die als Konzertorchester der Landeshauptstadt und als Opernorchester der Deutschen Oper am Rhein fungieren. Das Konzerthaus, ein ehemaliges Planetarium, wurde im Jahre 2005 aufwändig saniert und weist seither eine gute Akustik auf.

Bereits die Düsseldorfer Höfe des 16., 17. und 18. Jahrhunderts brachten ein reges musikalisches Leben hervor. Es ist verbunden mit den Namen Martin Peudargent, Giacomo Negri, Egidio Hennio, Giovanni Battista Mocchi, Georg Andreas Kraft, Sebastiano Moratelli, Stefano Pallavicini, Valeriano Pellegrini, Giorgio Maria Rapparini, Agostino Steffani und Francesco Maria Veracini. Seit 1807 wirkte in Düsseldorf die Familie des Musikers Friedrich August Burgmüller, die mit Friedrich und Norbert Burgmüller zwei bekannte Komponisten der Romantik hervorbrachte. 1818 gründete sich der Städtische Musikverein zu Düsseldorf und veranstaltete das erste von vielen Niederrheinischen Musikfesten, die im 19. Jahrhundert, auch durch die Mitwirkung von Felix Mendelssohn Bartholdy und Robert Schumann, beides Musikdirektoren in Düsseldorf, internationale Bedeutung hatten. Der Städtische Musikverein kann auf eine ungebrochene und musikhistorisch außerordentliche Geschichte von 1818 bis heute verweisen und gilt als der musikalische Botschafter der Stadt in allen großen Konzertsälen Deutschlands und Europas. In Düsseldorf komponierte Felix Mendelssohn Bartholdy 1833 den Vespergesang, 1834 bis 1836 das Oratorium "Paulus", 1850 Robert Schumann seine berühmte 3. Sinfonie („die Rheinische“).

Außerdem gibt es in Düsseldorf eine lebendige und breitgefächerte Chorszene, darunter sind beispielsweise der Bachverein Düsseldorf mit einer ebenfalls langen Tradition oder Chöre und Chorschule an St. Margareta in Gerresheim.

Mit der Maxkirche befindet sich in der Düsseldorfer Altstadt ein kirchenmusikalischer Standort, dessen Geschichte bis ins 17. Jahrhundert zurückreicht und an dem im 19. Jahrhundert u.a. Norbert Burgmüller, Felix Mendelssohn Bartholdy und Robert Schumann wirkten.

Einer der bekanntesten Jazz-Musiker mit Wurzeln in Düsseldorf ist Klaus Doldinger, der auch als Komponist von Filmmusik (Das Boot, Tatort) bekannt ist. Seit einigen Jahren ist er Schirmherr der Düsseldorfer Jazz-Rally.

1995 gründete sich der "Jazz in Düsseldorf e. V.", der den modernen Jazz durch Konzerte in der Jazz-Schmiede mit Musikern aus der ganzen Welt präsentiert. Mit der Jazz-Schmiede hat der Jazz wieder einen festen Veranstaltungsort in Düsseldorf. Zu den Hofgarten-Konzerten der Jazz-Schmiede im Sommer an vier Samstagen kommen jedes Jahr viele hundert Besucher.

Der Volkssänger Heino ist im Düsseldorfer Stadtteil Oberbilk geboren und aufgewachsen.

Auf dem Sektor der Unterhaltungsmusik war Düsseldorf ab den 1970er Jahren ein führendes Zentrum der elektronischen Popmusik. International bekannt waren und sind vor allem Kraftwerk, deren historisches Kling-Klang-Studio in der Düsseldorfer Friedrichstadt lag, aber auch Neu! und La Düsseldorf. In ihrem Titel "Trans-Europa-Express" und dem dazugehörigen Musikvideo thematisiert die Gruppe Kraftwerk auf avantgardistische Weise das Lebensgefühl Düsseldorfer Geschäftsleute, die dank transnationaler Eisenbahnnetze einen Lebensstil mit wechselnden Aufenthalten in europäischen Metropolen genießen. In der Tradition dieser 'Düsseldorfer Schulen' der Siebziger Jahre steht heute noch die international viel beachtete Band Kreidler. Im Sektor Elektropop macht derzeit die im Raum Düsseldorf beheimatete Band Susanne Blech auf sich aufmerksam, im Genre Deep House der Düsseldorfer Künstler Loco Dice, im Indie-Rock die Band PDR. Ein bekannter DJ aus Düsseldorf ist Lukas Langeheine, alias DJ Rafik.

Anfang der 1980er Jahre war Düsseldorf neben West-Berlin und Hamburg Hochburg der deutschen Punk- und NDW-Musik. Die wichtigsten Bands waren und sind Die Toten Hosen, Broilers, Deutsch Amerikanische Freundschaft, Propaganda, Rheingold, Die Krupps, Fehlfarben, Der Plan, KFC, Male, Mittagspause, Tommi Stumpff, Family 5 und Nachzehrer. Cryssis ist die Band des Schlagzeugers der Toten Hosen, vom Ritchie. Die "Beatlesons" bieten Trash-Polka, ausdrücklich ohne Beatles-Songs.

Marius Müller-Westernhagen ist in Düsseldorf geboren und zur Schule gegangen (Humboldt-Gymnasium). In seinem Titel "Mit 18" wird der "Hühner-Hugo" aus der Düsseldorfer Altstadt genannt. Neben Heino, den Toten Hosen und Kraftwerk gehört er zu den erfolgreichsten und bekanntesten Musikern Düsseldorfer Herkunft.

Eine Reihe teils international bekannter Metal-Bands stammt ebenfalls aus Düsseldorf: Warlock (mit Doro Pesch, die ab 1989 solo unter "Doro" weiter machte), Stormwind, Callejon, Warrant und Falkenbach.

Die Düsseldorfer "Mundart"band "Alt Schuss" ist eine der bekannten im Großraum Düsseldorf. Ebenfalls zu diesem Musikgenre gehört die Band "Halve Hahn". Die Wurzeln beider Gruppen liegen im Unterbacher Karneval.

Mit Farid Bang, Blumio, Antilopen Gang, NMZS, Plattenpapzt, Toony, Al-Gear und vielen anderen hat Düsseldorf eine bedeutende Szene des deutschsprachigen Rap (Gangsta-Rap, Hip-Hop) hervorgebracht. Im Album "Asphalt Massaka 2" erwies Farid Bang seiner Heimatstadt mit dem Titel "Ich bin Düsseldorf" die besondere Ehrerbietung. Auch sein Kollege Kollegah, bekannt durch seinen „Zuhälter-Rap“, ist in Düsseldorf zuhause. Im Sektor des Elektro-Rap ist JayJay ein bekannter, im Milieu Düsseldorfer Fortunafans beheimateter Künstler, der im Titel "Knüppel Klopp" seinen Sprechgesang weitgehend auf Düsseldorfer Rheinisch darbietet. Tbo und Glenn A. von D.I.U gehen über das Schema des Gangsta-Rap hinaus, indem sie soziale Probleme wie Diskriminierung, Gewalt, Hass, Armut, Ungerechtigkeit, Arbeitslosigkeit, Materialismus und Stigmatisierung differenziert behandeln. DTC (D-Town Chillaz) vertreten eine englischsprachige Variante des "Düsseldorf Rap". Nabil M. mixt in seinem Rap Deutsch, Arabisch und Französisch. Wurzeln des deutschsprachigen Rap liegen unweit in Ratingen-West, wo die Gruppe Fresh Familee den Titel "Ahmet Gündüz" um 1990 kreierte und ihn als ersten Rap-Song in deutscher Sprache in den Handel brachte. Mit Selfmade Records, Banger Musik und Alpha Music Empire sind in Düsseldorf drei der wichtigsten HipHop Labels Deutschlands zu Hause.

Thematisiert wurde die Stadt 1968 im Schlager und Evergreen "Wärst du doch in Düsseldorf geblieben", mit dem die deutsch-dänische Sängerin Dorthe Kollo eine Goldene Schallplatte gewann.

Am 10., 12. und 14. Mai 2011 fand der Eurovision Song Contest in der ESPRIT arena statt. Es war die 56. Ausgabe dieses Liedwettbewerbs, der zum dritten Mal seit Bestehen in Deutschland ausgetragen wurde.

Die Stadt Düsseldorf war als Aufenthaltsort und Studienort für Künstler ebenso beliebt wie als Standort für Kunstsammlungen und Museen. Vor allem der Kunst des 19. und 20. Jahrhunderts widmen sich einige Museen in Düsseldorf.

Die vorhandene kleine kurfürstliche Gemäldesammlung in Düsseldorf wurde unter Kurfürst Johann Wilhelm von der Pfalz und seiner Ehefrau Anna Maria Luisa de’ Medici zu einer berühmten Kunstgalerie ausgebaut. Im frühen 19. Jahrhundert wurde die Sammlung, darunter bedeutende Werke von Rubens, jedoch nach München abtransportiert, wo sie den Kernbestand der heutigen Alten Pinakothek bildet.

Im 19. Jahrhundert hatte die Düsseldorfer Malerschule, aus der u. a. Oswald Achenbach hervorging, wichtigen Einfluss auf die Malerei. Gegründet wurde die Akademie 1810 von Peter von Cornelius, der sie zunächst leitete; ab 1826 war Wilhelm von Schadow Direktor. Die Akademie diversifizierte sich und brachte insbesondere sozialkritische Genremalerei und bedeutende Landschaftsmaler hervor. Ihre Schüler kamen nicht nur aus dem Rheinland, aus Westfalen und Altpreußen, sondern auch aus den übrigen deutschen Ländern sowie Polen, Russland, den skandinavischen Ländern und den Vereinigten Staaten von Amerika. In der finnischen Kunst gibt es eine eigene "Düsseldorfer Epoche".

Nach der Revolution von 1848 fanden Künstler und Gelehrte im Kunstverein Malkasten in Düsseldorf zusammen. 1846 wurde der Verein zur Errichtung einer Gemäldegalerie zu Düsseldorf, der vor allem Werke der Düsseldorfer Malerschule ankaufte, gegründet. Aus der Initiative dieses Vereins gingen der Kunstpalast und schließlich das Museum Kunstpalast hervor.

Nach dem Ersten Weltkrieg war die Gruppe Das Junge Rheinland die aktivste Künstlergruppe in der Stadt. Zu ihr gehörten u. a. Otto Dix, Max Ernst und Walter Ophey. Den Mittelpunkt der Künstlergruppe bildete die Altstadt-Galerie Junge Kunst – Frau Ey, die von der noch heute in Düsseldorf bekannten Mutter Ey geführt wurde. Viele der Künstler der Vereinigung waren dem Rheinischen Expressionismus verbunden.

Nach dem Zweiten Weltkrieg war Joseph Beuys prägend und Düsseldorf galt, nicht nur auf Grund seines Wirkens an der Kunstakademie Düsseldorf, in den 1970er und 1980er Jahren als eine „Weltkunsthauptstadt“. Heute haben einige Beuys-Schüler wie Katharina Sieverding und Anselm Kiefer Einfluss auf Entwicklung der internationalen Kunstszene. Der Fotograf Bernd Becher, der 1976 eine Professur an der Kunstakademie Düsseldorf übernahm, bildete zusammen mit seiner Frau Hilla Becher viele fotografische Persönlichkeiten aus, die heute aus internationaler Sicht herausragende Vertreter der deutschen Fotografie sind. Zu dieser „Düsseldorfer Photoschule“ zählen insbesondere Boris Becker, Laurenz Berges, Elger Esser, Andreas Gursky, Candida Höfer, Axel Hütte, Simone Nieweg, Thomas Ruff, Jörg Sasse, Thomas Struth und Peter Wunderlich.

Neben traditionellen Denkmälern und Statuen, wie der Mariensäule oder dem Schadow-Denkmal, hat Düsseldorf zahlreiche weitere Kunstobjekte im Öffentlichen Raum zu bieten. So beherbergt der Südpark viele künstlerische Brunnen, Skulpturen und sonstige Kunstobjekte, von denen das bekannteste das Zeitfeld von Klaus Rinke sein dürfte. Auffällig sind in der Innenstadt auch die so genannten Säulenheiligen, Plastiken realistisch nachgebildeter Alltagsbürger auf Litfaßsäulen von Christoph Pöggeler. Einige Kunstwerke waren lange umstritten, beispielsweise das Heine-Monument von Bert Gerresheim. Zur Kunst im Öffentlichen Raum tragen auch die Bahnhöfe der Wehrhahn-Linie bei, jeweils mit ihrer eigenen künstlerischen Gestaltung.

Die umsatzstärkste Geschäftsstraße und eine der bedeutendsten Einkaufsstraßen Deutschlands ist die Schadowstraße. Ihrer städtebaulichen Anlage und ihrer exklusiven Läden wegen bekannter ist allerdings die Königsallee, kurz „die Kö“. In ihrer Mitte verläuft der Stadtgraben, an dessen Nordende sich als Wahrzeichen der Kö die Tritonengruppe, ein Brunnen aus dem Jahre 1902, befindet.

In der Altstadt findet man viele Häuser, die unter Denkmalschutz stehen. Weitere erhaltene historische Ortskerne können die Stadtteile Kaiserswerth und Gerresheim vorweisen. Der dörfliche Charakter der Ortskerne von Angermund, Kalkum, Oberlörick, Heerdt, Hamm, Himmelgeist und Urdenbach ist weitgehend erhalten geblieben.

Das möglicherweise älteste Gebäude im Stadtgebiet ist die Ruine der Kaiserpfalz in Kaiserswerth.
Sie geht auf eine Burg zurück, die im Jahr 1016 errichtet wurde. Diese wiederum ist auf ein Kloster zurückzuführen, das um das Jahr 700 entstand. Um 1193 zur Festung ausgebaut, wurde sie 1702 von französischen Truppen im Spanischen Erbfolgekrieg zerstört.

Ebenfalls sehr alt ist das ursprüngliche Schloss Kalkum. Die Burg entwickelte sich aus einem Fronhof, der erstmals im 9. Jahrhundert erwähnt wurde. Sie wurde 1810 bis 1819 umgebaut. Die Ursprünge des Schlosses Heltorf in Angermund sollen auf das 11. Jahrhundert zurückgehen. Ein Umbau erfolgte in den Jahren 1822 bis 1827.

Der Schlossturm am Burgplatz in der Altstadt war ursprünglich Teil des Düsseldorfer Schlosses, eines der Wahrzeichen der Stadt, das im 13. Jahrhundert errichtet und bis ins 16. Jahrhundert immer weiter ausgebaut wurde. Der Turm wurde 1845 von dem Düsseldorfer Architekten Rudolf Wiegmann im Stil der italienischen Neorenaissance umgebaut. Der Turm ist der einzige noch stehende Rest des Düsseldorfer Schlosses, das durch einen Brand im Jahr 1872 zerstört wurde.

Im 14. Jahrhundert wurden die Burg Angermund als nördlichste Bastion der Grafen von Berg ebenso wie der Vorgänger des Schlosses Eller erbaut. Das heutige Schloss Eller hingegen wurde 1826 errichtet und 1902 um- und ausgebaut.

Die Zeitalter des Barock und des Rokoko haben in Düsseldorf im Gartenbau sowie in Form von Schlössern ihre Spuren hinterlassen. Zu Beginn des Barock-Zeitalters entstand Schloss Garath. Sein Bau erfolgte im 16. Jahrhundert, Umbauten und Ergänzungen erfolgten bis ins 18. Jahrhundert. Nördlich des Hofgartens entstand zwischen 1752 und 1763 das Schloss Jägerhof. Es wurde erbaut von dem Architekten Johann Joseph Couven. Heute beherbergt es das Goethe-Museum. Einen ähnlichen Baustil hat auch das Hofgärtnerhaus vorzuweisen, das sich im Hofgarten befindet und das Theatermuseum beinhaltet. Architekt des Hofgärtnerhauses war Nicolas de Pigage.

Ebenfalls auf de Pigage geht das Schloss Benrath zurück. Erbaut wurde es 1755 bis 1773 im Auftrag von Kurfürst Karl Theodor von der Pfalz. Das denkmalgeschützte Ensemble von Lustschloss, Jagdpark, Weihern und Kanalsystem gilt als bedeutsamstes architektonisches Gesamtkunstwerk von Düsseldorf und wurde von der Stadt 2012 zur Aufnahme in die UNESCO-Liste des Weltkulturerbes vorgeschlagen, von der Jury allerdings nicht weiter berücksichtigt.

Das 1843 errichtete Schloss Mickeln in Himmelgeist hingegen wurde nach dem Vorbild von Renaissance-Villen erbaut.

Nicht zum Stadtgebiet zählt hingegen das Haus Unterbach, obwohl es die Urzelle von Unterbach ist, seit der Gebietsreform 1975 ein Stadtteil der Landeshauptstadt Düsseldorf. Das Haus Unterbach liegt im Gegensatz zum Stadtteil Unterbach östlich der Einmündung der Erkrather Straße (Kreisstraße 7) in die Gerresheimer Landstraße, an der Nordostseite letzterer. Da entlang dieser beiden Straßen(-abschnitte) die heutige Stadtgrenze verläuft, liegt das Haus Unterbach immer noch auf dem Gebiet der Stadt Erkrath. Das vor der Gebietsreform 1975 zu Unterbach gehörende Unterfeldhaus, das erst einige Jahre zuvor umfassend neu gestaltet und ausgebaut worden war, blieb bei der Stadt Erkrath und wurde zu einem ihrer Stadtteile, zu diesem gehört seitdem auch das Haus Unterbach.

Innerhalb der Düsseldorfer Altstadt ist St. Lambertus am Stiftsplatz 1 die älteste Kirche. Erbaut wurde sie 1288 bis 1394 im gotischen Stil, die Kirchweihe war am 13. Juli 1394. Seit 1974 ist St. Lambertus päpstliche Basilica minor. Wesentlich jünger ist die Kreuzherrenkirche in der Ursulinengasse, die von 1445 bis 1455 erbaut wurde.

Die ebenfalls katholische Kirche St. Andreas in der Andreasstraße ist hingegen ein Barockbau, der im Zuge der Gegenreformation zwischen 1622 und 1629 als Hof- und Jesuitenkirche entstanden ist.
Die erste Kirche der Lutherischen Gemeinde in Düsseldorf war die Berger Kirche in der Berger Straße, die von 1683 bis 1687 errichtet wurde. Zur selben Zeit baute die Reformierte Gemeinde ihre erste Kirche in Düsseldorf, die Neanderkirche in der Bolkerstraße 36. Katholisch ist hingegen St. Maximilian, meist kurz „Maxkirche“ genannt, in der Schulstraße, Ecke Citadellstraße, erbaut in den Jahren 1735 bis 1743.

Die ältesten Kirchen der Stadt befinden sich nicht in der Altstadt, sondern in den alten Stadtteilen Bilk und Kaiserswerth:
Als älteste Kirche Düsseldorfs gilt die im 12. Jahrhundert erbaute Pfarrkirche Alt St. Martin in Bilk, auch Alte Bilker Kirche genannt. Entsprechend ihrer Errichtungszeit ist sie im romanischen Stil erbaut. Alt St. Martin war auch die Pfarrkirche des Dorfes Düsseldorf vor der Errichtung von St. Lambertus.
Als zweitälteste Kirche kann St. Suitbertus in Kaiserswerth, erbaut 11. bis 13. Jahrhundert, gelten, seit 1967 päpstliche Basilica minor. St. Suitbertus ist eine romanische Kirche mit zwei gotischen Erweiterungen, der Apsis und dem seitlichen Eingang. Ebenso alt ist St. Nikolaus in Himmelgeist, ebenfalls erbaut 11. bis 13. Jahrhundert. Die Basilika St. Margareta in Gerresheim ist seit 1982 die dritte päpstliche Basilica minor in Düsseldorf. Errichtet wurde sie 1220 bis 1240 als Stiftskirche des Gerresheimer Stifts. Das Stiftsgebäude stammt aus derselben Epoche. Bei beiden Kirchen handelt es sich ebenfalls um Beispiele romanischer Architektur.

Die größte protestantische Kirche im Stadtgebiet ist die evangelisch-lutherische Johanneskirche in der Stadtmitte. Sie wurde in den Jahren 1875 bis 1881 im neugotischen Stil erbaut.

Linksrheinisch ist die katholische Kirche St. Antonius in Oberkassel mit ihrem neoromanischen Stil besonders interessant, erbaut wurde sie 1909 bis 1911. Architekturgeschichtlich interessant ist ebenfalls die evangelische Oberkasseler Auferstehungskirche von 1913–1914 in rheinischer Backsteinarchitektur mit Jugendstilelementen.

Zentral auf dem Kirchplatz gelegen ist St. Peter (kath.) in Friedrichstadt. Diese Kirche hat neugotische und neoromanische Elemente. Erbaut wurde sie zwischen 1887 und 1898.

Weitere interessante Kirchen finden sich in vielen Stadtteilen, so zum Beispiel St. Paulus (kath.) in Düsseltal, St. Maria Rosenkranz (kath.) von 1908 in Wersten und die St.-Josef-Kirche in Oberrath, alle im neoromanischen Stil errichtet.
St. Paulus und St. Josef gehen auf den Düsseldorfer Architekten Josef Kleesattel zurück, ebenso St. Blasius in Hamm, die Häuserzeilenkirche St. Elisabeth und St. Vinzenz in der Stadtmitte, Heilig Geist in Pempelfort, Herz Jesu in Derendorf und St. Ursula in Grafenberg. Weitere neuromanische Kirchen im Stadtgebiet entwarfen die Architekten Caspar Clemens Pickel (z. B. St. Adolfus und St. Apollinaris), Paul und Wilhelm Sültenfuß. Aus älteren Zeiten stammen St. Agnes in Angermund, St. Hubertus in Itter, St. Lambertus in Kalkum und St. Cäcilia in Hubbelrath.

Als architektonische Besonderheiten aus der Zeit des Wiederaufbaus nach dem Zweiten Weltkrieg sind zwei Kirchen hervorzuheben: die Bunkerkirche Sankt Sakrament in Heerdt, die in einem Luftschutzbunker errichtet wurde, und die Rochuskirche in Pempelfort. Die Rochuskirche war bis zu ihrer Zerstörung im Zweiten Weltkrieg eine sehr große und prächtige neoromanische Kirche, die 1897 von Josef Kleesattel errichtet worden war. Nach verheerenden Bombardierungen war sie eine Ruine. 1950 wurde entschieden, nur den Turm zu retten, das Kirchenschiff jedoch durch einen Neubau zu ersetzen. Dieser hat die Form eines Kuppeldachs und wurde von Paul Schneider-Esleben entworfen.
Weitere herausragende Beispiele moderner Kirchenarchitektur sind die katholischen Kirchen St. Bruno, St. Franziskus Xaverius, St. Matthäus, die Altenheimkapelle St. Hildegardis und die im parabolischen Grundriss angelegte Katharinenkirche in Gerresheim aus den 1960er Jahren sowie die evangelische Matthäikirche aus der Zwischenkriegszeit der Weimarer Republik.

Von Burgen und Kirchen abgesehen, finden sich die ältesten Gebäude Düsseldorfs in der Altstadt:

Das älteste Profangebäude der Stadt ist das Löwenhaus in der Liefergasse in der Altstadt. Es stammt aus dem Jahr der Stadterhebung 1288. Das Düsseldorfer Rathaus hingegen geht auf das 16. Jahrhundert zurück. Der älteste Teil wurde 1570 bis 1573 durch Heinrich Tussmann erbaut. In späteren Jahrhunderten kamen weitere Gebäudetrakte hinzu. Vor dem Rathaus erstreckt sich der Marktplatz mit dem Jan-Wellem-Reiterstandbild, das 1712 von Gabriel de Grupello gegossen wurde.

An der Grenze der Altstadt wurde 1811 bis 1815 das Ratinger Tor durch Adolph von Vagedes errichtet. Südlich der Altstadt, zwischen Carlstadt, Friedrichstadt und Unterbilk gelegen, befindet sich das Ständehaus. Es wurde 1876 bis 1880 von Julius Raschdorff erbaut und diente zunächst dem Preußischen Provinzialparlament als Sitz, von 1949 bis 1988 beherbergte es dann den Landtag des Landes Nordrhein-Westfalen. Heute befindet sich dort die Kunstsammlung Nordrhein-Westfalen K21.

Die Expansion und der ökonomische Aufstieg der Stadt bis zum Ausbruch des Ersten Weltkriegs führten auch zu Kaufhausgründungen, so dass mit Kaufhof an der Kö, ehem. Leonhard Tietz AG, erbaut 1906 bis 1908 von Joseph Maria Olbrich, und dem Carsch-Haus, erbaut von 1914 bis 1916 durch Otto Engler, zwei heute noch genutzte Kaufhäuser, entstanden.

Das ehemalige Verwaltungsgebäude der Mannesmannröhren-Werke AG am Mannesmann-Ufer wurde 1912 durch den Architekten Peter Behrens erbaut.

Direkt daneben am Rheinufer errichtete der Architekt Hermann vom Endt zwischen 1909 und 1911 im Auftrag des Rheinischen Provinzialverbandes das Landeshaus und die nach dem späteren Landeshauptmann benannte Villa Horion.

Eines der ersten Hochhäuser Deutschlands ist das an der heutigen Heinrich-Heine-Allee befindliche Wilhelm-Marx-Haus, das zwischen 1922 und 1924 durch Wilhelm Kreis errichtet wurde. Ebenfalls von Kreis stammt der Ehrenhof-Komplex einschließlich der ursprünglich als Planetarium gedachten Tonhalle und auch die in der Nähe gelegene Rheinterrasse. Der Ehrenhof ist heute Heimstatt mehrerer Museen und Ausstellungsinstitute, u. a. des NRW-Forums und des Museum Kunstpalast. Errichtet wurde der Komplex im Rahmen der "Großen Ausstellung für Gesundheitspflege, soziale Fürsorge und Leibesübungen", kurz „GeSoLei“ im Jahre 1926. Die Skulptur „Aurora“ über dem Nordportal stammt von Arno Breker.

Der Hauptbahnhof wurde 1932 bis 1936 von den Architekten Krüger und Eduard Behne errichtet, nachdem der zentrale Eisenbahnhaltepunkt der Stadt vom Graf-Adolf-Platz weg zu einer am Rande der innerstädtischen Bezirke gelegenen Stelle verlegt worden war.

Düsseldorf wurde nach dem Zweiten Weltkrieg teilweise wiederaufgebaut, an vielen Stellen entschied man sich jedoch für moderne Bauten.

Als Planungsdezernent wirkte Friedrich Tamms maßgeblich an der Neugestaltung der Stadt in den 1950er bis 1960er Jahren mit und war u. a. für die Neuanlage der Berliner Allee verantwortlich. Architektonisch ragt hier einerseits das Hochhaus der Stadtsparkasse Düsseldorf heraus, andererseits das Ensemble aus dem Thyssen-Hochhaus („Dreischeibenhaus“), erbaut 1957 bis 1960 von den Architekten Helmut Hentrich und Hubert Petschnigg, dem Düsseldorfer Schauspielhaus, errichtet 1965 bis 1970 durch den Architekten Bernhard Pfau, und den im April 2013 abgerissenen Tausendfüßler. Die Neuordnung des innerstädtischen Bereichs war seinerzeit hoch umstritten, die Personalpolitik des Dezernenten Tamms führte zudem zum Düsseldorfer Architektenstreit. Heute wird der Bereich wieder überplant, in seinem Zentrum befindet sich nunmehr der Kö-Bogen von Daniel Libeskind.

Mitprägend für das Stadtbild an der Rheinfront wurde das Mannesmann-Hochhaus in der Carlstadt, erbaut 1956 bis 1958 durch Paul Schneider-Esleben, der ebenfalls für die Haniel-Garage, Deutschlands erste Hochgarage nach dem Krieg, erbaut 1949–1950, sowie auch das inzwischen wieder abgerissene ARAG-Terrassenhaus in Mörsenbroich verantwortlich zeichnet.

Neben den historischen Gebäuden, den Schrägseilbrücken und dem Mannesmann-Hochhaus formen weitere Gebäude die Rheinfront, deren Gesamtbild einen hohen Wiedererkennungswert aufweist. Besonders markant sind dabei der Rheinturm (erbaut 1979 bis 1982) des Architekten Harald Deilmann, mit 240,5 m höchstes Wahrzeichen der Stadt und das Landtagsgebäude Nordrhein-Westfalen (erbaut 1980 bis 1988). Die Zeitanzeige des Rheinturms gilt als weltweit größte Digitaluhr. An den Rheinturm schließt seit den 1990er Jahren der Rheinpark Bilk an. Hinter dem Rheinpark entstand ab dieser Zeit unter der Bezeichnung "Medienhafen" im vorderen Teil des alten Rheinhafens an der Lausward eine städtebauliche und architektonische Collage aus Neubauten und umgebauten Altbauten, von denen der "Neue Zollhof" mit den drei "Gehry-Bauten", benannt nach ihrem Architekten Frank Gehry (erbaut 1996 bis 1999), sowie das Colorium, ein 17-geschossiges Bürogebäude (fertiggestellt 2001), besonders ins Auge fallen. Das so entstandene Gesamtensemble, das durch maßstäbliche Sprünge, Formen- und Materialvielfalt sowie funktionale und ästhetische Gegensätze gekennzeichnet ist, bildet nicht nur einen begehrten Büro- und Hotelstandort in der Landeshauptstadt, sondern fungiert auch als neue Tourismus-Destination.

Der Neubau des Sendehauses und Landesstudios des Westdeutschen Rundfunks an der Stromstraße ist ein weiteres markantes postmodernes Gebäude, das an den Rheinpark angrenzt. Die Stromstraße führt dann weiter über den Tunnel Gladbacher Straße, über dem das international ausgezeichnete Stadttor thront, das u. a. als Sitz der nordrhein-westfälischen Staatskanzlei dient.

Weiter Richtung Innenstadt fällt am Graf-Adolf-Platz die ovale Hochhausarchitektur des 89 m hohen GAP 15 ins Auge, das 2005 nach Plänen der Architekten J. S. K. erbaut wurde. In der Nähe des Graf-Adolf-Platzes, an der Friedrichstraße, befindet sich das DRV-Hochhaus. Es ist 120 m hoch und hat eine typische 1970er-Jahre-Architektur (Fertigstellung: 1978). Verantwortlicher Architekt war Harald Deilmann. Die Fassade wurde 2006/2007 neu gestaltet. Einen vollständigen Umbau erlebte hingegen das Gebäude der WestLB am Kirchplatz, Ecke Fürstenwall/Friedrichstraße. Der ursprüngliche Bau aus den späten 1960er Jahren, erbaut für die Bausparkasse Rheinprovinz, überragte die Kirche St. Peter, sein Nachfolger hingegen hat eine geringere Traufhöhe. Das andere in der Nähe befindliche Gebäude der WestLB ist ein Hochhauskomplex, dessen Ausführung für die 1970er Jahre typisch ist.

Nördlich der Oberkasseler Brücke erscheint hinter dem neoklassizistischen Oberlandesgericht deutlich der Victoria-Turm als höchstes Gebäude im postmodernen Gebäude-Komplex der Versicherungsgesellschaft an der Fischerstraße.

Im Norden der Stadt setzte die ARAG-Versicherung ihre Zeichen. Das sogenannte Mörsenbroicher Ei war und ist umgeben von Gebäuden dieser Gesellschaft. Das alte Stufenhaus von Paul Schneider-Esleben musste dem 1998 bis 2000 erbauten, 125 m hohen ARAG-Turm des Architekten Sir Norman Foster weichen, der das höchste Verwaltungsgebäude im Stadtgebiet darstellt.

Weichen musste ebenfalls das 1968 bis 1975 erbaute Rheinstadion. An seiner Stelle steht nun die ESPRIT arena. Sie wurde zwischen 2002 und 2004 nach Plänen der Architekten J.S.K. erbaut. Ebenfalls im Norden der Stadt, in Rath, befindet sich der ISS-Dome. Erbaut wurde er in den Jahren 2005 bis 2006 von den Architekten Rhode Kellermann Wawrowsky (RKW).

Im Süden Düsseldorfs ist das Kuppelgewächshaus im Botanischen Garten Düsseldorf noch erwähnenswert. Die Heinrich-Heine-Universität selbst ist in einer für 1960er und 1970er Jahren typischen Bauweise mit etlichen Hochgebäuden für die Institute in Sichtbeton mit großen Glasfenstern, Hörsaalgebäuden mit Sichtbeton und teilweise roter Verklinkerung und weiteren teils rot verklinkerten, teils mit Sichtbeton errichteten Gebäuden gebaut worden. Etwas untypisch ist die verstreute Lage der Gebäude entlang einer geschwungenen Hauptachse des Geländes.

Die Düsseldorfer Brückenfamilie war ursprünglich ein Sammelbegriff für die drei zentralen Schrägseilbrücken Theodor-Heuss-Brücke, Oberkasseler Brücke und Rheinkniebrücke, die die Entwicklung dieses Brückentyps weltweit für viele Jahre maßgeblich beeinflusst hatten.

Die zwischen Golzheim und Niederkassel liegende Theodor-Heuss-Brücke, früher auch "Nordbrücke" genannt, gilt als die erste Schrägseilbrücke Deutschlands. Sie wurde 1952 im Auftrag des Düsseldorfer Stadtplanungsamtes unter Leitung des Architekten Friedrich Tamms von einer Gruppe um den Bauingenieur und Tragwerksplaner Fritz Leonhardt entworfen. Ihre schlanken, freitragenden Pylonstiele und die harfenförmige, parallele Anordnung der Seile wurden von Tamms veranlasst. Bald nach ihrer Fertigstellung im Jahr 1957 beauftragte Tamms auch die Planung der Oberkasseler Brücke und der Rheinkniebrücke, wobei Fritz Leonhardt für die Rheinkniebrücke, Hans Grassl für die Oberkasseler Brücke federführend war.

Die ein kleines Stück weiter südlich stehende Oberkasseler Brücke zwischen der Innenstadt und Oberkassel war die älteste und lange auch die einzige Düsseldorfer Straßenbrücke. Sie ersetzte ab 1898 die Pontonbrücke von 1839, die bis dahin die einzige Verbindung mit dem linken Rheinufer darstellte, und wurde 1924 erweitert. Nach der Zerstörung 1945 wurde ab 1948 erneut eine Behelfsbrücke errichtet. Aus verkehrstechnischen Gründen konnte erst 1973 die von Hans Grassl entworfene Schrägseilbrücke neben ihr errichtet werden. 1976 erfolgte der vielbeachtete endgültige Verschub der neuen Brücke an die historische Stelle. Sie leistet die Verbindung von der Altstadt nach Oberkassel und trägt die Stadtbahnlinien, die von der Heinrich-Heine-Allee ins Linksrheinische führen.

Die noch weiter südlich stehende Rheinkniebrücke verbindet die Friedrichstadt mit Oberkassel. Sie musste aus verkehrstechnischen Gründen vor dem Neubau der Oberkasseler Brücke gebaut werden. Sie wurde zwischen 1965 und 1969 nach den Plänen von Leonhardt errichtet und ist genauso wie die Theodor-Heuss-Brücke und die nach ihr gebaute Oberkasseler Brücke eine Schrägseilbrücke.

Diese drei Schrägseilbrücken zeichnen sich durch die gleichen Stilelemente aus – ein flaches stählernes Brückendeck, schlanke senkrechte Pylone und wenige, harfenförmig angeordnete Schrägseile. Aufgrund ihrer Lage in kurzer Entfernung voneinander konnten sie außerdem schon vor ihrer Fertigstellung in Modellen und Zeichnungen gemeinsam dargestellt werden. Die Düsseldorfer Brückenfamilie wurde 2007 für die Auszeichnung als Historisches Wahrzeichen der Ingenieurbaukunst in Deutschland nominiert.

Weiter südlich verbinden drei ganz anders konstruierte Brücken Düsseldorf mit der Nachbarstadt Neuss. Zunächst kommt von Nord nach Süd vorgehend die Hammer Eisenbahnbrücke, eine an einem Stahlbogen abgehängte Fachwerkbrücke, deren Vorgänger aus dem Jahre 1870 stammt. Reste dieser alten Brücke sind in Form der Türme an den Ufern noch zu sehen. Die heutige Brücke wurde 1987 direkt neben der historischen Trasse im Zuge des Baues der Ost-West-S-Bahnlinie S 8 errichtet. Im Gegensatz zu ihrem Vorgänger trägt sie vier Eisenbahngleise. Einen Fuß- oder Radweg gibt es ebenso wenig wie eine Fahrbahn für Autos.

In Sichtweite steht die Josef-Kardinal-Frings-Brücke, vormals "Südbrücke", ein Neubau der Jahre 1950 bis 1951, nachdem die alte Brücke von 1929 im letzten Kriegsjahr 1945 zerstört worden war. Sie ist die erste Hohlkasten-Balkenbrücke, die in Schweißtechnik ausgeführt wurde. Über die Josef-Kardinal-Frings-Brücke führen die Bundesstraße 1 und eine Straßenbahnlinie.

Die südlichste Düsseldorfer Rheinbrücke ist die Fleher Brücke, eine Schrägseilbrücke mit dem höchsten Brückenpylon in Deutschland und einer Vielzahl von fächerförmig angeordneten Seilen. Erbaut wurde sie von 1976 bis 1979. Auf ihr verläuft die Bundesautobahn 46.

Im Norden der Stadt steht die Flughafenbrücke, eine Schrägseilbrücke mit zu Dreiecken verkürzten Pylonen, die von Düsseldorf-Stockum nach Meerbusch führt und dabei den Verlauf der Bundesautobahn 44 vorgibt. Sie ist die jüngste Düsseldorfer Rheinbrücke, sie wurde zwischen 1998 und 2002 gebaut.

Düsseldorf ist eine der letzten Städte weltweit, in denen es ein großflächiges und intaktes Netz von Gaslaternen gibt (zurzeit 14.500 – mit stark fallender Tendenz). Seit 1848 prägen die Laternen mit ihrem goldgelben Licht die Atmosphäre der Stadt. Die Gasbeleuchtung ist auf vielfältige Weise mit der Stadtgeschichte verbunden. So schrieben die Gebrüder Mannesmann im 19. Jahrhundert im Rheinland mit dem Patent für nahtlose Stahlrohre ein wichtiges Stück Industriegeschichte. Auch die Technik der hängenden Glühkörper wurde von Mannesmann entwickelt. Bereits Clara Schumann schrieb über die Düsseldorfer Gaslaternen.
Die Gaslaternen sind wichtige Zeitzeugen der historischen Stadtentwicklung und selbst die jüngsten sind bereits über ein halbes Jahrhundert alt. Sie beleuchten bis heute große Teile der Wohngebiete. Bekannte Gaslaternen-Typen sind unter anderem die Alt-Düsseldorfer, die seit den zwanziger Jahren eingesetzten Ansatz- und Aufsatzleuchten sowie Reihenleuchten.

Der berühmte und denkmalgeschützte Düsseldorfer Hofgarten wurde bis zum Sturm Ela im Jahr 2014 durch Gaslaternen beleuchtet. Das Modell Frankfurt, das dort in den 1950ern aufgestellt wurde, gilt heute als vermutlich einzigartig in ganz Europa. Unzählige Bäume wurden durch den Sturm zerstört und auch viele Gaslaternen, aber sie überstanden den Sturm ohne jegliche Gefährdung durch austretendes Gas. Bisher wurden die Gaslaternen im Hofgarten nicht ersetzt, es existiert derzeit noch eine provisorische Elektro-Beleuchtung an Holzpfeilern.

Die Stadtverwaltung Düsseldorf beabsichtigt den Austausch fast aller historischen Gaslaternen gegen LED-Leuchten. Eine Petition der Düsseldorfer Bürger sammelte über 10.000 Stimmen für den Erhalt und war die bislang erfolgreichste Petition der Stadtgeschichte. Eine Bürgerinitiative fordert die Anerkennung der Gasbeleuchtung als „Welt-Kulturerbe“ und Industriedenkmal.

Nach Auffassung der Bürgerinitiative sprechen sowohl kulturelle als auch finanzielle Gründe für den großflächigen Erhalt des Gaslaternenenetzes. Sie wird dabei unter anderem vom Bund der Steuerzahler unterstützt, der sich in einer Presseerklärung dazu äußerte. Selbst der BUND für Umwelt und Naturschutz hält den Ersatz der Gaslaternen nicht für ein wirksames Mittel zur Reduzierung des CO-Ausstoßes.

Mit einem Beschluss vom 10. Dezember 2015 hat der Rat der Stadt Düsseldorf erstmals die Gaslaternen als Kulturgut anerkannt. Für den Hofgarten ist die Verwaltung aufgefordert worden, einen Ausführungs- und Finanzierungsbeschluss zur Wiederherstellung der Gasbeleuchtung vorzubereiten. Im übrigen Stadtgebiet sollen mindestens 4000 Gaslaternen erhalten bleiben. Die Festlegung der Erhaltungsgebiete wurde den Bezirksvertretungen übertragen.

Die Stadtwerke Düsseldorf haben im Herbst 2016 erklärt, dass sie in der Lage sind, den dauerhaften Betrieb der Gaslaternen rechtlich und technisch einwandfrei sicherzustellen.

Düsseldorf, das oft den Beinamen "Gartenstadt" erhält, verfügt heute über 1238 Hektar öffentliche Grünflächen, davon 641 Hektar Parks, die sich über das Stadtgebiet verteilen. Der Rhein bildet mit seinen in großen, in weiten Teilen unbebauten Uferzonen ein grünes Band, das verschiedene Parks in nord-südlicher Richtung miteinander verbindet. Am östlichen Stadtrand befinden sich zudem mit 2180 Hektar ausgedehnte Stadtwaldflächen.

Der Ruf als Gartenstadt geht auf die zweite Hälfte des 18. Jahrhunderts zurück. 1769 ließ der Statthalter des Kurfürsten Karl-Theodor den „Alten Hofgarten“ als ersten Volksgarten Deutschlands nach den Plänen von Nicolas de Pigage im Rahmen von Arbeitsbeschaffungsmaßnahmen anlegen. Die Anlage wurde richtungsweisend für ähnliche Parks in anderen Städten. Nach der Schleifung der Festungswerke 1801 wurde der Park durch Maximilian Friedrich Weyhe umgestaltet und in Form einer offenen Gartenlandschaft weiterentwickelt. Dieser „Neue Hofgarten“ mit seinen Gartenlokalen und Unterhaltungsangeboten war gesellschaftlicher Treffpunkt und eine Besonderheit Düsseldorfs. Der Hofgarten blieb bis 1875 die einzige Grünfläche innerhalb des bebauten Stadtgebietes.

Die durch die Industrialisierung hervorgerufene starke städtische Expansion lieferte die Grundlage für die Anlage neuer Parks. So entstanden in privater Initiative 1875 der Florapark und 1876 der Zoologische Garten, beide mit durch die Düssel gespeisten Teichanlagen. Einen großen Einfluss auf die Gestaltung der Grünflächen hatte Heinrich Hillebrecht, der von 1879 bis 1910 Stadtgärtner von Düsseldorf war. Seit Mitte der 1880er Jahre verfolgte die Stadt Düsseldorf das Ziel, Grundstücke für die Anlage von Parks und Erholungsflächen zu erwerben. 1892 wurde der Oberbilker Volksgarten, heute Teil des Südparks, angelegt. Es folgte 1898 der Ostpark. 1903 übernahm die Stadt den Florapark und 1905 den Zoologischen Garten.

1904 fand die große Gartenbau-Ausstellung in Düsseldorf statt, in deren Fokus die Reform des Gartenbaus hin zu einer architektonischen Gestaltung stand. Verfechter dieser Reformen waren Peter Behrens und Walter von Engelhardt aus dem Umfeld der Düsseldorfer Kunstgewerbeschule. Nach ihren Ideen plante Reinhold Hoemann den Kaiser-Wilhelm-Park am Rhein. Im Rahmen der GeSoLei 1926 wurden Teile der Rheinufers aufgeschüttet und gärtnerisch gestaltet. Aus dem Kaiser-Wilhelm-Park und den neuen, aufgeschütteten Flächen wurde der heutige Rheinpark Golzheim, der über den Komplex des Museum Kunstpalast mit dem Hofgarten verbunden ist. Im Rahmen der Reichsausstellung Schaffendes Volk entstand zwischen Rheinufer und der Kaiserswerther Straße der Nordpark. Der Volksgarten und der Südpark waren Teil des Geländes der Bundesgartenschau 1987. Südlich hiervon befindet sich der Botanische Garten.

Die Parks mehrerer Schlösser und Herrensitze, die heute zum Stadtgebiet gehören, sind für die Öffentlichkeit zugänglich. Der bekannteste ist der Park von Schloss Benrath im Süden der Stadt. Ferner sind dies in den südlichen Stadtteilen der Park Elbroich sowie die Parks der Schlösser Eller, Mickeln und Garath. Im Norden der Stadt liegen der Park von Schloss Kalkum sowie der Lantz’sche Park. Zwischen der Carlstadt und der Friedrichstadt liegen der Ständehauspark mit dem Spee’schen Graben, Kaiserteich und Schwanenspiegel. In Pempelfort schließt sich an den Hofgarten das Gelände des Künstlervereins Malkasten mit dem Jacobigarten an.

Von den Düsseldorfer Parkanlagen wurden allein neun wegen ihrer Qualität und Bedeutung 2004 in die Straße der Gartenkunst zwischen Rhein und Maas aufgenommen. Sie sind in der folgenden Liste durch * hervorgehoben.

Düsseldorf hat im Stadtgebiet 43 Naturdenkmäler und zwölf Naturschutzgebiete mit einer Gesamtfläche von 1435 Hektar.

In der Vergangenheit erstreckte sich im Südosten der Stadt durch das Mäandern des Rheins zwischen dem Altrhein und dem heutigen Flussverlauf ein großes Sumpf- und Moorgebiet, das große Teile der heutigen Stadtbezirke 8, 9 und 10 sowie der Nachbarstadt Hilden als auch einige der dort gelegenen Wälder umschloss. Viele der heutigen Naturschutzgebiete gingen aus den verschiedenen Ausprägungen dieses Sumpfes hervor. Der sandtragende Teil dieses "In den Benden" genannten Gebietes wurde inzwischen ausgekiest und hinterließ den Unterbacher See (vormals "Bendensee"), Elbsee, Menzelsee und Dreiecksweiher in Unterbach. Nässegebiete blieben die "Urdenbacher Kämpe", der "Eller Forst" und der "Himmelgeister Rheinbogen", die ebenso wie Teile des Elbsees und Dreiecksweiher unter Naturschutz gestellt wurden. Durch die Konzentration des Wassers konnten große Flächen des Sumpfgebietes unter anderem durch Torfbruch trockengelegt und kultiviert werden. Zuvor umging der historische Mauspfad dieses Gebiet als Höhenweg über die ersten Ausläufer des Bergischen Landes am Übergang zur niederrheinische Ebene.

Die Urdenbacher Kämpe bildet heute als Flussauenlandschaft mit einer Fläche von 316 ha das größte Naturschutzgebiet in Düsseldorf. Sie ist durch die in der Niederrheinregion häufigen Streuobste im Auwald geprägt und FFH-Gebiet. Im Osten der Stadt, übergehend in den Landkreis Mettmann, befindet sich das "Rotthäuser Bachtal", das ebenfalls FFH-Gebiet ist und in dem Teiche und Hecken sowie wiederum Auwälder die hügelige Landschaft prägen.
Das dritte FFH-Gebiet ist die Überanger Mark im Nordosten Düsseldorfs, das vor allem aus Erlen- und Hainbuchenbewaldung besteht.

Die übrigen Naturschutzgebiete innerhalb der Düsseldorfer Stadtgrenzen sind:

Beliebtestes Naherholungsgebiet der Stadt ist der Unterbacher See. Er ist an der Stadtgrenze zu Erkrath und Hilden gelegen und grenzt an den Eller Forst. An ihm sind zahlreiche Freizeit- und Sportmöglichkeiten von Segeln und Tretbootfahren bis Minigolf und Schwimmen möglich. Darüber hinaus existieren zwei Campingplätze. Das Gebiet um den See herum steht unter Naturschutz. Außerdem dient der Düsseldorfer Stadtwald als Teil der "grünen Lunge" zur Naherholung.

Die Stadt hat drei Freizeitparks für Kinder und Familien in den 1970er und 1980er Jahren eingerichtet, die seitdem modernisiert wurden.
Es handelt sich um den Freizeitpark Ulenbergstraße in Bilk, den Freizeitpark Heerdt im linksrheinischen Heerdt und den Freizeitpark Niederheid, der zudem einen Kinderbauernhof enthält.

In Düsseldorf leben insgesamt rund 110.000 Ausländer und es haben sich etwa 5000 ausländische Unternehmen angesiedelt, die die Stadt prägen. Unter anderem führen die Unternehmensniederlassungen zu einem außergewöhnlich hohen Anteil japanischer Einwohner, sowie zahlreicher Niederländer, US-Amerikaner, Briten, Franzosen, Chinesen und Koreaner. Daneben gibt es, wie in anderen vergleichbaren Städten, große türkische, griechische, marokkanische, serbische, italienische und polnische Gemeinden. In der Stadt befinden sich zahlreiche kulturelle und religiöse Einrichtungen der verschiedenen Nationalitäten und Glaubensrichtungen.

40 von 71 der in Nordrhein-Westfalen angesiedelten konsularischen Vertretungen und Zweigstellen lagen Anfang 2013 in Düsseldorf. Hinzu kommen 33 ausländische Handelskammern und Außenhandelsorganisationen. Die 2014 beim Staatsbesuch des chinesischen Staatspräsidenten Xi Jinping in Nordrhein-Westfalen angekündigte Eröffnung eines vierten chinesischen Generalkonsulats in Deutschland in Düsseldorf wurde am 19. Dezember 2015 in Gegenwart von Ministerpräsidentin Hannelore Kraft und Außenminister Wang Yi vollzogen.

Im Jahr 2015 wurde die englische Sprache neben Deutsch zur Verwaltungssprache erhoben. Die Zugänglichkeit und Attraktivität der Stadt für z. B. Expatriates, hochqualifizierte Einwanderer und internationale Wissenschaftler soll somit erhöht werden.

Düsseldorf hat eine englischsprachige internationale Schule im Norden der Stadt mit 950 Schülern aus 44 Nationen, im benachbarten Neuss befindet sich eine weitere internationale Schule. Im Stadtteil Düsseltal befindet sich eine französische Schule mit ca. 430 Schülern, linksrheinisch eine japanische internationale Schule mit ca. 650 Schülern und ein griechisches Lyzeum mit ca. 820 Schülern. Die Yitzhak-Rabin-Schule ist eine der wenigen jüdischen Grundschulen in Deutschland.

Als Kulturorganisationen sind u. a. das seit 1950 in Düsseldorf bestehende Institut français und das Instytut Polski zu erwähnen. Der Japanische Club ist mit 5000 Mitgliedern einer der größten Vereine der Stadt. An der Heinrich-Heine-Universität ist ein Konfuzius-Institut angesiedelt.

Der jährlich stattfindende Japantag, wie auch das Frankreichfest im Juli, gehören zu den kulturellen Höhepunkten im Stadtleben. Außerdem werden alle vier Jahre die jüdischen Kulturtage im Rheinland mit unterschiedlichen Sparten durchgeführt.

Im Stadtteil Gerresheim gibt es einige Straßenzüge, die sehr stark süditalienisch geprägt sind, die griechische Gemeinde ist in Düsseldorf ebenfalls stark. Rund um die Ellerstraße in Oberbilk hat sich in den letzten Jahren ein marokkanisch-tunesisches Viertel gebildet. In Düsseldorf sind christliche Heimatgemeinden u. a. aus Südkorea, Polen und Vietnam verankert, die Russisch-Orthodoxe Kirche unterhält in Düsseldorf ein Patriarchat. Die Koptisch-Orthodoxe Kirche ist mit einer Gemeinde und einer Kirche in der Stadt vertreten. Ferner findet sich innerstädtisch das sogenannte „Maghreb-Viertel“. Es umfasst das Gebiet der am Hauptbahnhof zusammentreffenden Stadtteile (südöstliche) Stadtmitte, Friedrichstadt, Flingern-Süd und (nördliches) Oberbilk und ist durch eine nordafrikanische Bevölkerungsstruktur geprägt.

Die Düsseldorfer Altstadt wird wegen ihrer vielen Kneipen als die „längste Theke der Welt“ bezeichnet. Die Formulierung geht auf den Werbeschaffenden Carl Schweik in den 1960er Jahren zurück. Neben der Altbierkneipe Uerige gibt es die Häuser „Brauerei im Füchschen“, „Brauerei Schumacher“, „Brauerei Zum Schlüssel“ und viele weitere. Es finden sich Hunderte Bars, Restaurants, Diskos und Kneipen auf einem erstaunlich engen Areal. Das Gebiet umfasst im Wesentlichen den historischen Teil der Altstadt, im Norden begrenzt von der Ratinger Straße, im Westen von der Rheinpromenade, im Osten von der Heinrich-Heine-Allee und im Süden vom Carlsplatz. Im November 2009 ist die bisher geltende Sperrstunde in der Altstadt aufgehoben worden.

Konkurrenz bekommt die Altstadt durch den modernen Medienhafen mit den architektonisch bedeutsamen Gehry-Bauten und den interessanten Clubs und Diskotheken, in denen oft auch Prominente verkehren.

Beliebt in gastronomischer Hinsicht ist auch die linke Rheinseite mit den Stadtteilen Oberkassel und Niederkassel. In Bilk hat sich in den letzten Jahrzehnten zaghaft eine studentische Kneipenkultur entwickelt. In den Stadtteilen Derendorf, Flingern und Pempelfort ist eher Szene-Publikum unterwegs. Insbesondere in Pempelfort hat sich im Umfeld der Tußmannstraße und des ehemaligen Geländes des Güterbahnhofes Derendorf eine rege Kneipen- und Gastronomie-Szene entwickelt. In den genannten Vergnügungszentren und in den zahlreichen Kneipen der Stadtteile wird in erster Linie das obergärige Altbier ausgeschenkt. Fast alle Kneipen bieten aber auch andere Biersorten an. Neben dem Altbier gelten der Senf („Mostert“) der Marken "Löwensenf" und "ABB-Senf" sowie der Beerenlikör "Killepitsch" als weitere Spezialitäten der lokalen Gastronomie.

Eine beliebte Flaniermeile ist die Königsallee "(Kö)". „Sehen und gesehen werden“ heißt hier das Motto. Die zahlreichen Straßencafés des Boulevards laden zudem zum Verweilen ein. Auch die Rheinuferpromenade, die den Medienhafen mit der Altstadt verbindet, bietet eine Fülle an Cafés und Restaurants mit Außengastronomie.

Zu den lokalen Spezialitäten zählt der Düsseldorfer Senfrostbraten.

Die wichtigsten Elemente des Düsseldorfer Brauchtums sind der Karneval mit dem Rosenmontagszug als Höhepunkt, die Schützenfeste in den Stadtteilen und im Juli das große Düsseldorfer Schützenfest mit der größten Kirmes am Rhein. Der St.-Sebastianus-Schützenverein besteht seit spätestens 1435, geht aber vermutlich auf das 14. Jahrhundert zurück. Eine alte Tradition ist auch das Radschlagen. Für „Eene Penning“ führten die Düsseldorfer Radschläger – meist schulpflichtige Knaben – ihre Kunst vor. Weniger touristisch und wirtschaftlich bedeutsam, für die Kinder Düsseldorfs aber umso wichtiger, sind die Martinsumzüge in der Altstadt und in den Stadtteilen. Zu Ehren des Heiligen Martin von Tours ziehen sie in der ersten November-Hälfte mit selbst gebastelten Laternen singend hinter einem den Hl. Martin darstellenden Reiter her. Im Anschluss an die Umzüge „gripschen“ sie Süßigkeiten in Geschäften und an Haustüren als Gegenleistung für ein Ständchen. Neben den Karnevals- und Schützenvereinen pflegt der Heimatverein der Düsseldorfer Jonges in besonderem Maße Brauchtum und Tradition.

Rund 112.000 Menschen in Düsseldorf betreiben Breitensport in 369 Vereinen, deren Dachorganisation der Stadtsportbund Düsseldorf darstellt. 36 Vereine sind in ihrer jeweiligen Sportart mindestens in der Regionalliga vertreten und repräsentieren den Leistungssport. Die bekanntesten Düsseldorfer Profivereine sind im Fußball Fortuna Düsseldorf und im Eishockey die Düsseldorfer EG. Vor dem Hintergrund der letztlich erfolglosen Olympiabewerbung Düsseldorf/Rhein-Ruhr für das Jahr 2012 hat die Stadt Düsseldorf massiv in den Bau neuer Sportstätten sowohl für den Profi-, als auch den Breitensport investiert. Dem Stadtmarketing wurde eine Sportagentur angegliedert, die unter dem Slogan „Sportstadt Düsseldorf“ nationale und internationale Sportereignisse in die Stadt zieht und vermarktet. Seit dem 1. Juli 2009 heißt die LTU Arena ESPRIT Arena.

Bekanntestes sportliches Aushängeschild der Stadt ist der Traditionsverein Fortuna Düsseldorf. Die größten sportlichen Erfolge sind der Gewinn der deutschen Fußballmeisterschaft 1933, die DFB-Pokalgewinne 1979 und 1980 sowie der Finaleinzug im Europapokalturnier der Pokalsieger 1979. Fortuna Düsseldorf spielte seit der Saison 2009/10 in der 2. Fußball-Bundesliga und stieg am 15. Mai 2012 erneut in die 1. Bundesliga auf und am letzten Spieltag der Saison 2012/2013 nach einem Jahr in der 1. Liga wieder ab.

Weitere bekannte Fußballvereine der Stadt sind die Fußballabteilung von TuRU Düsseldorf, die von der Saison 2004/2005 bis zur Saison 2007/08 in der Oberliga Nordrhein spielte, der BV 04 Düsseldorf, der seit 1963 regelmäßig zu Ostern ein internationales Junioren-Fußball-Turnier (U19 Champions Trophy) ausrichtet, sowie der VfL Benrath, der 1957 Deutscher Amateurmeister wurde und in den 1930er Jahren die beiden deutschen Nationalspieler Karl Hohmann und Josef Rasselnberg stellte.

Genauso bekannt wie die Fußballmannschaft von Fortuna ist die Düsseldorfer EG. Als achtfacher Deutscher Meister seit 1967 gehört die DEG zu den erfolgreichsten Clubs in Deutschland. 2006 wurden außerdem erstmals der deutsche Pokalsieg und die Vizemeisterschaft errungen. Erneut wurde die Vizemeisterschaft in der Saison 08/09 geholt.

Im Eisstadion an der Brehmstraße, Spielort der DEG von 1935 bis 2006, fanden mehrere europäische Eishockey-Wettbewerbe und Weltmeisterschaftsspiele statt. Seit September 2006 werden die Spiele der DEG im ISS-Dome ausgetragen.

Seit dem Ausstieg des Hauptsponsors 2012 hat die DEG mit finanziellen Problemen zu kämpfen. Trotzdem konnte der Verbleib in der DEL in der Spielzeit 2012/2013 und auch 2013/2014 durch lokale Kooperationen gesichert werden.

Der älteste American-Football-Verein Düsseldorfs sind die Düsseldorf Panther, die seit ihrer Gründung 1978 sechsmal die deutsche Meisterschaft der German Football League gewannen, damit bis 2008 über lange Jahre deutscher Rekordmeister waren und als das älteste noch existierende deutsche American-Football-Team gelten.

Die Düsseldorf Bulldozer, gegründet 1979, blicken auch auf eine lange Tradition zurück und haben in den Anfangsjahren der deutschen Ligen ebenfalls in den oberen Klassen mitgespielt.

Die Mannschaft von Rhein Fire gehörte von 1994 bis zur Auflösung 2007 zu den sportlichen Glanzlichtern Düsseldorfs. Rhein Fire war 2006, wie schon 1999, 2002, 2004 und 2005, Gastgeber des World Bowls und konnte diesen nach fünf Finalteilnahmen auch in den Jahren 1998 und 2000 gewinnen.

Der Tischtennisverein Borussia Düsseldorf war zuletzt 2016 Deutscher Meister und hat neben 28 nationalen Meistertiteln auch 24-mal den deutschen Pokalsieg errungen. Die Borussia holte sechsmal den Europapokal der Landesmeister sowie zweimal den ETTU-Pokal und gewann 2000, 2009, 2010 und 2011 die Champions League im Tischtennis. Hinzu kommt ein 3. Platz bei der ersten Weltmeisterschaft für Vereinsmannschaften.

Der 1949 gegründete Lokalkonkurrent TuSa 06 Düsseldorf wurde zwischen 1962 und 1967 fünfmal Deutscher Mannschaftsmeister der Herren sowie von 1964 bis 1966 dreimal in Folge Deutscher Pokalmeister. Er gehörte zu den Gründungsmitgliedern der Tischtennis-Bundesliga, in der er bis 1971 verblieb.

Mit Eberhard Schöler, Jörg Roßkopf, Dimitrij Ovtcharov und Timo Boll spiel(t)en vier der erfolgreichsten deutschen Tischtennisspieler Jahre lang in Düsseldorf.

Außerdem finden in Düsseldorf jedes Jahr die „Kids Open“ statt, das größte Tischtennis-Jugendturnier Europas.

Der erfolgreichste Tennisclub der Stadt sind die Herren des Rochusclubs in der 1. Bundesliga, bei den Damen hingegen erzielt der TC Benrath in der 1. Bundesliga die meisten Erfolge. Die Mannschaftsweltmeisterschaft World Team Cup wurde zwischen 1978 und 2012 jedes Jahr im Mai auf dem Gelände des Düsseldorfer Rochusclubs ausgetragen. Nach dem World Team Cup gab es 2013 und 2014 ein ATP 250 Tennis-Turnier.

Der TV Unterbach 1905 wurde fünfmal zwischen 1978 und 1984 in der olympischen Disziplin Trampolinturnen deutscher Mannschaftsmeister. Dazu kamen viele Teilnahmen und Titel in den Einzelmeisterschaften auf Deutschland-, Europa- und Weltebene. Zudem stellte der Verein über Jahrzehnte einen Großteil der deutschen Nationalmannschaft und die Leitung der Trampolin-Bundesliga. Internationale Pionierarbeit leistete der Verein mit der Veranstaltung des "Unterbach-Cup" als erstem und höchststehendem europaweiten Wettkampf für Jugendliche, zuletzt mit weltweiter Beteiligung. Weiter gehört der "Turnerbund Hassels 1925" zu den wenigen Sportvereinen, die in Düsseldorf Trampolinturnen anbieten.

Das Ehepaar Anneliese und Siegfried Krehn betrieb über Jahrzehnte eine der ersten Tanzschulen in Düsseldorf. In den 1950er und 1960er Jahren errangen sie zahllose vordere Plätze bei Welt-, Europa- und deutschen Meisterschaften, zunächst als Amateure für den Boston Club Düsseldorf, später als Profis. Unter anderem gewannen sie die deutschen Meisterschaften in den Standardtänzen siebenmal in Folge sowie in den lateinamerikanischen Tänzen dreimal in Folge Außerdem sicherten sie sich in diesen beiden Disziplinen gleichzeitig die Vize-Weltmeistertitel des Jahres 1966. Im Zuge dieser Erfolge standen sie und ihre Tanzschule im Mittelpunkt des ersten Tanzunterrichts im Deutschen Fernsehen.

Das bekannteste Düsseldorfer Basketballteam sind die , die in der zweiten Basketball-Bundesliga ProA spielen. In den 1980er Jahren war die Mannschaft DJK Agon 08 Düsseldorf im deutschen Basketball der Damen dominierend (neunmal deutscher Meister in Folge von 1980 bis 1988, außerdem 1975 sowie in den Jahren 1990 und 1991) und auch in den europäischen Wettbewerben erfolgreich vertreten (zwei Finalteilnahmen 1983 und 1986). Außerdem wurde DJK Agon 08 Düsseldorf noch siebenmal Deutscher Pokalsieger der Damen.

TuRU Düsseldorf, der HSV Düsseldorf bzw. die HSG Düsseldorf spielten von 1983 bis 2012 in der 1. und 2. Handball-Bundesliga. Nachdem Zusammenschluss des Neusser HV und ART Düsseldorf existiert seit der Saison 2017/2018 mit der HSG Neuss/Düsseldorf wieder eine Mannschaft, die in der 2. Handball-Bundesliga spielt. Die Mannschaft spielt dort unter dem Namen HC Rhein Vikings.

Weitere Sportarten, in denen Düsseldorf mit Mannschaften in den obersten Ligen vertreten ist, sind Hockey (1. und 2. Bundesliga), Lacrosse (1. Bundesliga), Baseball, Tanzsport, Kanusport und Faustball (2. Bundesliga).

Der Sport in Düsseldorf erlebte und erlebt Jahr für Jahr verschiedene Sportveranstaltungen mit bundesweiter und auch weltweiter Beachtung. Hier wären die bereits erwähnte Mannschaftsweltmeisterschaft im Tennis im Rochusclub, der METRO Group Marathon, der Skilanglauf-Weltcup am Rheinufer, das Jugendfußballturnier des BV 04, das Radrennen Rund um die Kö und der Kö-Lauf zu nennen.

Im internationalen Fußball war Düsseldorf mit seinem Rheinstadion Gastgeber mehrerer Spiele der Fußball-Weltmeisterschaft 1974 sowie des Eröffnungs- und mehrerer Gruppenspiele der Fußball-Europameisterschaft 1988. Das Eisstadion an der Brehmstraße war WM-Standort bei den Eishockey-Weltmeisterschaften 1955, 1975 und 1983. 1977 wurde im Rheinstadion der WorldCup, ein Vorläufer der heutigen Leichtathletik-Weltmeisterschaften, ausgetragen.

In der Leichtathletik war die Stadt Schauplatz der Crosslauf-Weltmeisterschaften 1977 und ist seit 2006 Gastgeber des Düsseldorfer Indoor-Meetings.

Zwischen 2000 und 2003 bewarb sich Düsseldorf auf nationaler Ebene um die Ausrichtung der Olympischen Spiele 2012. Im nationalen Vorentscheid wurde Düsseldorf hinter Leipzig und Hamburg Dritter.

Am 1. Juli 2017 startete die Tour de France mit einem Einzelzeitfahren in Düsseldorf. Den Grand Depart sahen trotz des schlechten Wetters nach offiziellen Angaben rund 500.000 Zuschauer an der Strecke. Am Tag darauf startete die zweite Etappe in Düsseldorf und endete in Lüttich. Eine Besonderheit der Etappe war, dass diese zuerst nach Osten durch Mettmann und dann noch einmal zurück nach Düsseldorf führte, ehe der Tour-Tross die Stadt in Richtung Mönchengladbach endgültig verließ. Alleine auf dieser Etappe sollen auf dem Stadtgebiet von Düsseldorf rund 800.000 Zuschauer an der Strecke das Rennen verfolgt haben.

Im Zukunftsatlas 2016 belegte die kreisfreie Stadt Düsseldorf Platz 21 von 402 Landkreisen und kreisfreien Städten in Deutschland und zählt damit zu den Orten mit „sehr hohen Zukunftschancen“.

Düsseldorf ist eine wirtschaftsstarke, diversifizierte und global intensiv verflochtene Stadt in der Mitte der Metropolregion Rhein-Ruhr, in der sie eine funktionale Primatstellung innehat (→ "Global City"). Unter den Metropolfunktionen überragt der Sektor "Entscheidungs- und Kontrollfunktionen" alle Kreise und kreisfreien Städte in Nordrhein-Westfalen deutlich. In diesem Sektor rangiert Düsseldorf in Deutschland nach München und Berlin und vor Frankfurt am Main an dritter Stelle. Auch als "Gateway", also hinsichtlich seiner Ferninfrastrukturen und weltweiten Kontakte, ist Düsseldorf von überragender Funktion in Nordrhein-Westfalen. Einerseits ist hierfür die zentrale Lage im bevölkerungsreichsten Ballungsraum Deutschlands ausschlaggebend. Andererseits stellen der drittgrößte Flughafen Deutschlands, Düsseldorf Airport, sowie die Messe Düsseldorf mit 25 internationalen Leitmessen wichtige Faktoren für die wirtschaftliche Bedeutung der Stadt dar, insbesondere im Hinblick auf ihre internationale Verflechtung. Die starke Stellung des Düsseldorfer Arbeitsmarkts manifestiert sich in dem mit Abstand größten Einpendlerüberschuss unter den Kreisen und kreisfreien Städten Nordrhein-Westfalens (per saldo +151.387 sozialversicherungspflichtige Beschäftigte). Das günstige Standortklima für wirtschaftliche Innovation und Unternehmensgründung ist an der relativ hohen Zahl neugegründeter Unternehmen ablesbar. 2009 war Düsseldorf bundesweit die führende Stadt bei Unternehmensneugründungen, geprägt durch Gründungen vor allem im Bereich forschungsintensiver Industrien. 2011 war die Stadt der deutsche Ort mit den meisten Direktinvestitionen aus dem Ausland. Die Zahl der durch Auslandsinvestitionen geschaffenen Stellen lag 2011 dreimal so hoch wie 2010 und innerhalb Deutschlands am höchsten. Damit war Düsseldorf auch nach diesem Parameter der beliebteste Standort für Auslandsinvestitionen in Deutschland. Nach einer Untersuchung des Beratungsunternehmens Ernst & Young war Düsseldorf 2013 – vor London und Paris – der Großraum mit der höchsten Zahl von Direktinvestitionen aus der Volksrepublik China in Europa. 2014 konnte die Region Düsseldorf nach London und vor Paris die meisten ausländischen Direktinvestitionen in Europa verbuchen. Das starke Wachstum chinesischer Investitionen wird einerseits mit den Standortvorteilen erklärt, die die in Nordrhein-Westfalen bestehenden Technik-Unternehmen, die zentrale Lage in Europa und der Messestandort bieten, andererseits mit der Eigendynamik einer bereits vorhandenen chinesischen Community, die sich einrichtet und engere Netzwerke knüpft.

Düsseldorf ist führender Standort in den Branchen Werbung, europäisches Patentwesen, Telekommunikation, Unternehmensberatung und Kunsthandel sowie Deutschlands „Stadt der Mode“. Mit den "Igedo Fashion Fairs" und den "Collections Premieren Düsseldorf" (CPD) fanden sich hier führende Modemessen Europas. Mit dem neuen Format "The Gallery Düsseldorf" versucht der Messeveranstalter Igedo an frühere Erfolge anzuknüpfen. Über 600 Showrooms verschiedener Hersteller sowie große Textilhandelsfirmen konzentrieren sich in der Landeshauptstadt, über 1.300 in der Agglomeration Düsseldorf. Den räumlichen Schwerpunkt bildet hierbei ein Cluster von Orderbüros rund um die Kaiserswerther Straße im Stadtteil Golzheim. Weiterhin ist Düsseldorf, gemessen am Umsatz, Deutschlands Modestandort Nummer eins: Mit rund 18 Milliarden Euro ist der Modeumsatz in Düsseldorf nach einer Untersuchung des Kölner Instituts für Handelsforschung mehr als doppelt so hoch wie in München und Berlin zusammen. Geschätzt werden Nüchternheit und geschäftliche Effizienz des Modestandorts am Rhein.

Die Kultur- und Kreativwirtschaft der Stadt umfasst rund 4.100 Unternehmen mit einem Jahresumsatz von etwa 7,4 Milliarden Euro, der Anteil der Kreativen Klasse an den sozialversicherungspflichtig Beschäftigten ist nach einer Untersuchung für das Jahr 2008 unter den Kreisen und kreisfreien Städten Nordrhein-Westfalens in Düsseldorf am höchsten. Innerhalb Deutschlands gilt die eng verflochtene Metropolregion Rhein-Ruhr als führendes Zentrum der Kreativwirtschaft.

Düsseldorf ist nach Frankfurt am Main die zweitgrößte Banken- und Börsenstadt (Börse Düsseldorf) – ca. 170 Banken haben eine Filiale oder ihre Zentrale in Düsseldorf, das traditionsreiche Bankhaus HSBC Trinkaus & Burkhardt ist hier beheimatet.

Zahlreiche internationale Firmen haben hier ihren Sitz: L’Oréal Deutschland, Komatsu Mining Germany, Air Liquide Deutschland, Nikon Deutschland, Vodafone Deutschland, die Metro AG, Rheinmetall, Henkel, Tata Steel mit Vallourec & Mannesmann Tubes, NRW.Bank, E-Plus, Qiagen und die ERGO Versicherungsgruppe, zu der wiederum beispielsweise die Victoria und die Hamburg-Mannheimer gehören. Daimler produziert in Düsseldorf die geschlossenen Baureihen des Mercedes-Benz Sprinter sowie für VW den technisch verwandten Crafter. Zahlreiche Mittelstandsfirmen im Bereich Hochtechnologie, Medizintechnik, Sondermaschinen- und Anlagenbau sowie Antriebs- und Produktionstechnik und Nahrungsmittelsproduktion sind seit Jahrzehnten fester Bestandteil der Düsseldorfer Industrielandschaft. Dazu gehören unter anderem die Firmen Gerresheimer AG, Demag Cranes AG, Vossloh AG, GEA Group AG, A.u.K. Müller GmbH & Co. KG, Walter Flender Gruppe und Zamek. Die größte japanische Kolonie in Kontinentaleuropa hat Düsseldorf den Beinamen „Nippon am Rhein“ eingebracht. In der Stadt sind allerdings auch Unternehmen aus anderen Ländern in erheblichem Maße aktiv – besonders aus den Niederlanden, aus Großbritannien, Frankreich, Skandinavien und China. Mit der Provinzial Rheinland hat ein großer öffentlicher Versicherer in Düsseldorf seinen Sitz. Ein weiteres bekanntes Versicherungsunternehmen aus der Landeshauptstadt ist der ARAG-Konzern.

Seit der Jahrtausendwende hat sich in Düsseldorf zudem eine lebendige Startup-Szene im Bereich der Internetwirtschaft entwickelt, die allerdings mit Städten wie Berlin, Hamburg und München noch nicht konkurrieren kann. Zu den bekanntesten Düsseldorfer Start-ups zählt die Hotelsuchmaschine Trivago.

Die wirtschaftliche Stärke Düsseldorfs hat der Stadt zu soliden kommunalen Finanzen mit ausgeglichenen Haushalten seit 1999 verholfen. Seit dem 12. September 2007 ist die Stadt als zweite Großstadt Deutschlands schuldenfrei. Als erste deutsche Stadt hat sich Düsseldorf zudem 2005 einem Kreditrating unterzogen und wurde hierbei von der Ratingagentur Moody’s mit Aa1 bewertet, der zweitbesten möglichen Wertung. Die Kreditwürdigkeit Düsseldorfs wurde damit höher eingeschätzt als etwa jene Nordrhein-Westfalens (Aa2), der Deutschen Bank (Aa3) oder der Commerzbank (A2).

2012 betrug das Bruttoinlandsprodukt in Düsseldorf 41,5 Milliarden Euro. Das ist ein Anteil von 7,1 Prozent am Bruttoinlandsprodukt Nordrhein-Westfalens im Jahr 2012. Das BIP pro Erwerbstätigem in Düsseldorf betrug 2012 82.667 Euro – 125,3 Prozent des Werts in ganz Nordrhein-Westfalen. Im bundesweiten Städtevergleich liegt die Stadt damit zusammen mit Frankfurt am Main an der Spitze in Deutschland. Der Kaufkraftindex für die Landeshauptstadt liegt im langjährigen Vergleich rund 20 Prozent über dem Bundesdurchschnitt. Trotz der differenzierten Wirtschaft und der guten Rahmenbedingungen liegt die Arbeitslosigkeit seit Jahren höher als im Bundesdurchschnitt, was mit dem Wegfall von über 50.000 Arbeitsplätzen in der Industrie und dem verarbeitenden Gewerbe in den vergangenen 30 Jahren zusammenhängt.

Die Wirtschafts-, Büro- und Verwaltungsstandorte der Stadt sind mit den stadtplanerischen Zielen der Entlastung der Innenstadt, der Entflechtung des Verkehrs, der Ausnutzung günstigerer Bodenpreise und der Schaffung städtebaulicher Entwicklungsimpulse über das gesamte Stadtgebiet verteilt worden. Neben der Innenstadt gelten folgende Bereiche als die wichtigsten Büro- und Verwaltungsstandorte:


Der Immobilienstandort Düsseldorf, der innerhalb der Metropolregion Rhein-Ruhr die Spitzenstellung einnimmt, zieht aufgrund seiner Werthaltigkeit sowie aufgrund seiner guten demografischen und wirtschaftlichen Aussichten hochwertige Immobilienentwicklungen und Investitionen an, sowohl im gewerblichen Bereich als auch auf dem Sektor der Wohnimmobilien. Das Beratungsunternehmen "bulwiengesa" stuft die Stadt aufgrund ihrer Bedeutung für den Immobilienmarkt neben sechs weiteren deutschen Metropolen in die Gruppe der sogenannten "A-Städte" ein. Nach einer Untersuchung der CBRE gehörte Düsseldorf im ersten Halbjahr 2013 zu den zehn führenden Investmentmärkten für Gewerbeimmobilien in Europa. 2015 überstieg der Gesamtwert von verkauften Gewerbeimmobilien erstmals die Marke von drei Milliarden Euro. Bevölkerungswachstum (vor allem durch Zuzug von Neubürgern) und eine steigende Belegung von Wohnflächen durch Ältere könnten die Wohnflächennachfrage zwischen 2006 und 2025 um insgesamt 3,1 Prozent steigen lassen. Die durch Individualisierung und Metropolisierung geförderte Gentrifizierung, die sich auf dem Wohnungsmarkt stark nachgefragter Stadtteile als Verdrängung einkommensschwacher Milieus durch Neubürger mit höherem Einkommen realisiert, führt in der Stadt zunehmend zu einer wohnungspolitischen Debatte. Der starken Nachfrage nach Wohnungen stehen eine geringe Zahl freier Wohnungen und eine geringe Zahl von Wohnungsneubauten gegenüber, was die Preise für Wohnraum nach oben treibt. Diese vorerst anhaltende Situation führt nach Ansicht von Wohnungsmarktbeobachtern zu einem optimalen wirtschaftlichen Umfeld für renditeträchtige Investitionen, insbesondere für Investitionen in den modern konzipierten, hochwertigen Geschosswohnungsneubau, zumal infolge der geringen Neubautätigkeiten der letzten Jahrzehnte mittlerweile etwa 81 Prozent des Wohnungsbestandes älter als 30 Jahre ist. Gemessen am durchschnittlichen Nettohaushaltseinkommen der Düsseldorfer liegen die Mietpreise in der Stadt im großstädtischen Vergleich allerdings niedrig. Eine im Oktober 2012 veröffentlichte Untersuchung des Immobilienverbandes Deutschland ergab, in Bezug auf eine Dreizimmer-Mietwohnung mit 70 Quadratmetern in mittlerer Lage als Referenzgröße, dass ein Düsseldorfer Haushalt dafür im Durchschnitt 19,8 Prozent des Einkommens aufwendet, ein durchschnittlicher Berliner Haushalt jedoch 23,0 Prozent. 2013 berichtete die Deutsche Bundesbank, dass die Preisentwicklung auf Wohnungsmärkten in deutschen Großstädten, namentlich auch in Düsseldorf, möglicherweise zu „Übertreibungen“ geführt hätte. Als Folge der Finanz- und Weltwirtschaftskrise überlagerten sich in Düsseldorf in den letzten Jahren mehrere Trends, die bei Wohnimmobilien außergewöhnlich hohe Kauf- und Mietpreisanstiege erzeugten, vor allem in den innerstädtischen Lagen: ein Trend zurück in die Stadt (Reurbanisierung), kommunale Liberalisierungsbestrebungen, negative Realzinsen, die „Flucht“ von Privatanlegern in Sachwerte, schlechte Anlagealternativen für risikoarme Investments institutioneller Investoren und ein gewisses Bevölkerungswachstum.

Düsseldorf hat eine dichte Verkehrsinfrastruktur. Hierzu trägt besonders die gute Ausstattung mit Anlagen des öffentlichen Personennahverkehrs und des motorisierten Individualverkehrs bei. Außerdem reduzieren die geringere Stadtgröße und die Lage in einer verkehrlich gut ausgebauten polyzentrischen Raumstruktur die Wahrscheinlichkeit von Verkehrsstaus erheblich. Im Hinblick auf Erreichbarkeitspotenziale im Straßen- und Schienenverkehr wies die Metropolregion Rhein-Ruhr, in deren Zentrum Düsseldorf liegt, 2001 die Spitzenposition der untersuchten Regionen in Nordwest-Europa auf. Dem gegenüber steht beispielsweise ein Vergleich der Unternehmensberatung Arthur D. Little, die Düsseldorf im Hinblick auf die Abstimmung und Vernetzung der ÖPNV-Systeme deutscher Großstädte auf dem letzten Platz sieht.

Der Düsseldorf Airport ist gemessen an den Passagierzahlen nach dem Frankfurter Flughafen und dem Flughafen München der drittgrößte internationale Flughafen Deutschlands. Im Jahre 2012 flogen rund 20,8 Millionen Menschen mit 60 verschiedenen Fluggesellschaften von und zu weltweit 200 Zielen in über 50 Ländern. Der Flughafen zeichnet sich durch seine Nähe zum Stadtzentrum sowie zum Messegelände in Stockum, seinen direkten Anschluss an das Autobahnnetz und seiner sehr guten Anbindung an das Eisenbahnnetz aus, weshalb kurze Transferzeiten in das Stadtgebiet und die Region möglich sind.

Mit dem innenstadtnahen Hafen im gleichnamigen Stadtteil und dem Reisholzer Hafen, dessen Ausbau schon lange Zeit geplant ist, stehen der Rheinschifffahrt im Stadtgebiet zwei Umschlagplätze für Güter zur Verfügung. Über den Rhein, den damit verbundenen Kanälen und dem Main-Donau-Kanal ist Düsseldorf weitreichend an das europäische Binnenwasserstraßennetz – einschließlich Ems, Weser, Elbe, Oder und Donau – und mit wichtigen Seehäfen an der Nordsee und dem Schwarzen Meer verbunden. Für Wassersportler gibt es am Rheinpark Golzheim einen Sport- und Yachthafen.

Zwischen der Altstadt und Kaiserswerth verkehren regelmäßig Fahrgastschiffe der Weissen Flotte Düsseldorf, die vor 1993 von der Rheinbahn betrieben wurden. Die Köln-Düsseldorfer Rheinschifffahrts AG (kd) besitzt ebenfalls Anlegestellen im Stadtgebiet. Mit der Rheinfähre Langst–Kaiserswerth und der Fähre zwischen Urdenbach und Zons sind noch zwei Autofähren in Betrieb. Eine dritte Autofähre verkehrte bis zur Eröffnung der Fleher Brücke 1979 zwischen Himmelgeist und Uedesheim. Heute verkehrt hier sonntags bei schönen Wetter eine Personenfähre mit Mitnahmemöglichkeit von Fahrrädern. Seit einigen Jahren verkehren zwischen der Rheinkirmes und der Altstadt ebenfalls Personenfähren.

Die Bundesstraßen B 1 und B 8 durchqueren das Stadtgebiet in Nord-Süd-Richtung und die Bundesstraße B 7 in Ost-West-Richtung. Ihren heutigen Verlauf und Charakter verdanken sie
Die B 228 verbindet Benrath mit Hilden, Haan und Wuppertal. Heute dienen alle Bundesstraßen im Stadtgebiet in erster Linie dem städtischen Durchgangsverkehr und dem Verkehr zu und von den Autobahnen, die außerhalb des Stadtgebietes die Rolle der Bundesstraßen im Fernstraßennetz übernommen haben. Bis auf die B 7 und B 228 nach Wuppertal endet deshalb die Kennzeichnung der Bundesstraßen noch im Stadtgebiet oder kurz dahinter. Ein Beispiel ist die ehemalige B 8 zwischen Wersten und Hellerhof und weiter bis Opladen. Parallel zu ihr verläuft mit der "Münchener" und "Frankfurter Straße" eine vierspurige Kraftfahrstraße zwischen Bilk und Garath. Mit ihrem Bau wurde schon in den 1960er Jahren begonnen um den sich vervielfachenden Autoverkehr in die stark wachsenden oder neuen Stadtteile im Düsseldorfer Süden bewältigen zu können.

Die Verkehrsplanung nach dem Zweiten Weltkrieg prägte zunächst Friedrich Tamms, ein Verfechter der autogerechten Stadt. Neben einigen der schon genannten Bauprojekte, der Rheinkniebrücke und der Oberkasseler Brücke plante er eine dritte leistungsfähige Nord-Süd-Verkehrsachse zwischen Golzheim und Wersten. Die Berliner Allee und die sich nördlich anschließende Hochstraße – auch "Tausendfüßler" genannt, im April 2013 wieder abgerissen und durch Tunnelbauwerke unter dem Kö-Bogen ersetzt – waren die zentralen Vorhaben auf dieser Achse und wurden zwischen 1954 und 1962 gebaut.

Zusätzlich zum stationären Carsharing, bei dem die Fahrzeuge nach ihrer Nutzung an ihrem ursprünglichen Standort zurückgebracht werden müssen, ist seit 2012 auch die Anmietung sogenannter "Free Floating Cars" möglich. Diese Fahrzeuge, die innerhalb Düsseldorfs auf jedem regulären Parkplatz abgestellt werden dürfen, bieten Car2go, Greenwheels, Stadtmobil und DriveNow an.

Die A 3 zwischen Frankfurt am Main und Oberhausen verläuft östlich außerhalb des Stadtgebietes und war bis Ende der 1960er Jahre die nächstgelegene Autobahn. Ihr erstes Teilstück zwischen Mettmann und Köln-Mülheim wurde bereits 1936 freigegeben. Erreichbar war sie zunächst nur über die heutige B 7. Der "Nördliche Zubringer" entstand von 1950 bis 1960 mit dem Neu- und Ausbau der B 1 zwischen dem heutigen Mörsenbroicher Ei und Kreuz Breitscheid zur Kraftfahrstraße. Der "Südliche Zubringer" – eine als B 326 gekennzeichnete Kraftfahrstraße zwischen Wersten und dem heutigen Kreuz Hilden – kam 1956 hinzu.

Die A 57 zwischen Köln und Nimwegen ist die zweite Nord-Süd-Verbindung in Reichweite der Stadt, existiert seit 1986, das Teilstück zwischen Neuss und Köln bereits seit 1966 (ab 1970 Autobahn) und war zunächst nur über die heutige Josef-Kardinal-Frings-Brücke erreichbar.

Die A 52 ist im Großraum Düsseldorf in zwei Abschnitte geteilt. Das nördliche Teilstück zwischen der Anschlussstelle Düsseldorf-Rath und dem Dreieck Essen-Ost entspricht bis zum Kreuz Breitscheid dem "Nördlichen Zubringer", der 1971 zur Autobahn hochgestuft wurde. Das westliche Teilstück erstreckt sich heute zwischen der Anschlussstelle Büderich und Roermond, ist die Fortsetzung der B 7 in westlicher Richtung und bindet die Stadt über das Kaarster Kreuz an die A 57 an. Der erste Abschnitt bis zum Kreuz Neersen wurde 1971 freigegeben und 1973 zur Autobahn hochgestuft.

Die A 59 zwischen dem Dreieck Düsseldorf-Süd und Kreuz Leverkusen-West verläuft parallel zur A 3, entlastet diese, bindet aber auch Monheim, Langenfeld und Leverkusen besser an Düsseldorf an und wurde zwischen 1968 und 1973 gebaut.

Die A 46 zwischen Heinsberg und dem Kreuz Wuppertal-Nord tangiert die Innenstadt südlich, bindet die Heinrich-Heine-Universität an das Autobahnnetz an und stellt seit 1986 eine lückenlose Querverbindung zwischen der A 3, A 59 und A 57 her. Teilstücke wurden 1979 nach Fertigstellung der Fleher Brücke und 1983 mit der Eröffnung des Universitätstunnels freigegeben. Bereits 1972 wurde der "Südliche Zubringer" zur Autobahn hochgestuft.

Die A 44 zwischen Aachen und Velbert verläuft durch die nördlichen Stadtteile und stellt seit der Eröffnung der Flughafenbrücke 2002 eine lückenlose Querverbindung zwischen der A 3, A 52 und A 57 her. Der direkte Anschluss der Messe, ESPRIT arena und des Flughafen an das Autobahnnetz erfolgte bereits 1992, als ein wichtiges Teilstück der A 44 mit Anschluss an die A 52 im Kreuz Düsseldorf-Nord freigegeben wurde.

Die zuletzt gebauten Autobahnen A 44 und A 46 sind im Großraum Düsseldorf die einzigen überregionalen Autobahnen in Ost-West-Richtung. Außerdem durchqueren nur sie das Stadtgebiet ganz, weshalb sie die übrigen Rheinbrücken und städtischen Durchgangsstraßen entlasten aber auch vier Autobahntunnel gebaut werden mussten. Zusammen mit der A 3 und der A 57 bilden sie seit 2002 den Autobahnring Düsseldorf.

Durch das Stadtgebiet führen die Bahnstrecken
Im "Hauptbahnhof" – dem zentralen und seit 1891 an seinem heutigen Standort gelegenen Fernbahnhof – sind diese Bahnstrecken bis auf Güterbahnstrecke miteinander, mit der Stadtbahn und dem übrigen öffentlichen Personennahverkehr verknüpft.

Auf der Bahnstrecke Köln–Duisburg in der Nähe des Flughafen liegt der Bahnhof "Düsseldorf Flughafen", an dem außer den Zügen einer S-Bahn- und aller sieben Regional-Express-Linien auch ein Teil der hier verkehrenden "ICE"- und "EC/IC"-Züge halten. Die knapp 2,5 km entfernten Terminals erreichen Passagiere und Besucher des Flughafen mittels des SkyTrains. Dort existiert über den unterirdischen Kopfbahnhof "Düsseldorf-Flughafen Terminal" eine zweite Anbindung des Flughafen an das Schienennetz, über die eine zweite S-Bahnlinie ganztägig verkehrt und nachts einzelne Fahrten mehrerer Regional-Express- und weiterer S-Bahn-Linien verlängert werden.

Ebenfalls an der Bahnstrecke Köln–Duisburg aber im Süden Düsseldorfs liegt der Regionalbahnhof "Düsseldorf-Benrath", an dem zwei Regional-Express-Linien und eine S-Bahn-Linie ganztägig halten. Hervorzuheben ist auch der an der Bahnstrecke Mönchengladbach–Düsseldorf liegende Bahnhof Düsseldorf-Bilk, der eine stark frequentierte Umstiegshaltestelle zwischen drei S-Bahn-Linien, den auf der Wehrhahn-Linie verkehrenden Stadtbahnen und den Busverbindungen zur Heinrich-Heine-Universität ist und in den nächsten Jahren zum Regionalbahnhof ausgebaut wird. 

Einschließlich der vorgenannten Bahnhöfe liegen im Stadtgebiet 25 S-Bahn-Stationen.

Im Eisenbahngüterverkehr ist Düsseldorf jedoch nach Stilllegung und Abbruch seines Rangierbahnhofes Düsseldorf-Derendorf kein Eisenbahnknoten mehr, der größte Güterbahnhof des gesamten Düsseldorfer Eisenbahnkomplexes ist heute im Bahnhof des benachbarten Neuss.

Düsseldorf verfügt über ein dichtes Netz an S-Bahn-, Stadtbahn-, Straßenbahn- und Stadtbus-Linien, das Teil des Verkehrsverbundes Rhein-Ruhr (VRR) ist. Der öffentliche Personennahverkehr (ÖPNV) im Stadtgebiet wird durch die Rheinbahn, die Regiobahn und die Deutsche Bahn betrieben. Alle Linien können hier und auch, wenn Fahrtziele außerhalb der Stadt aber im Gebiet des VRR liegen, mit Tickets des VRR benutzt werden. Darüber hinaus gilt der NRW-Tarif und bei Fahrten in den Großraum Köln auch der Tarif des Verkehrsverbundes Rhein-Sieg (VRS).

Ein Straßenbahnnetz wurde gegen Ende des 19. Jahrhunderts aufgebaut, zunächst mit Wagen, die von Pferden über die Gleise gezogen wurden, ab 1896 dann elektrisch.
Dieses Bahnnetz umfasste auch Überlandstrecken nach Krefeld (K-Bahn) und Duisburg (D-Bahn) sowie von Benrath nach Solingen-Ohligs. Die Verbindungen nach Krefeld und Duisburg bestehen bis heute als Stadtbahnlinien U 70 und U 76 (Krefeld) sowie U 79 (Duisburg). Weitere ortsübergreifende Linien bestehen nach Neuss und Ratingen. Mit dem sukzessiven Ausbau des Stadtbahnnetzes verkleinerte sich das Straßenbahnnetz von 19 (1981) auf sieben (2018) Linien und auf eine Streckenlänge von 70,2 Kilometer. 

Die Stadtbahn Düsseldorf umfasst derzeit elf Linien. Sieben von ihnen verlaufen durch den 1988 eröffneten Innenstadttunnel zwischen U-Bahnhof Heinrich-Heine-Allee und dem Hauptbahnhof. Eine weitere Tunnelstrecke für vier neue Stadtbahnlinien, die Wehrhahn-Linie, wurde am 20. Februar 2016 eröffnet. Der U-Bahnhof Heinrich-Heine-Allee ist der zentrale Umstiegspunkt zwischen allen Stadtbahnlinien. Alle Tunnelstrecken haben oberirdische Zulaufstrecken, die nur teilweise mit unabhängigen oder besonderen Bahnkörpern ausgestattet sind.

Seit 1924 verkehren auch Linienbusse in Düsseldorf. Neben Stadtbuslinien innerhalb des Stadtgebietes stellten Fernlinien (vgl. Regionalbusverkehr) Verbindungen zu anderen Städten her, insbesondere nachdem der Betrieb auf einigen Überlandstraßenbahn-Linien eingestellt wurde. Inzwischen hat die Rheinbahn ihre Linie nach Jülich eingestellt und weitere Linien nach Essen, Velbert, Solingen, Leichlingen, Opladen und Moers verkürzt. Heute bestehen noch Verbindungen nach Mülheim an der Ruhr, Mettmann, Erkrath, Solingen-Ohligs, Langenfeld und Monheim; nach Haan fahren inzwischen sogar Schnellbusse. Heute verkehren im Stadtgebiet 42 Stadtbus- und sieben Schnellbuslinien. Buslinien sind auch ein wesentlicher Bestandteil des Nachtverkehrs in den Nächten von Freitag auf Samstag, Samstag auf Sonntag sowie in den Nächten auf Feiertage. Acht NachtExpress-Linien verkehren zwischen 0 und 5 Uhr im 30- oder 60-Minuten-Takt. Im Sommer 2018 wird der Betrieb auf drei Metrobus-Linien aufgenommen.

Die erste S-Bahnlinie außerhalb der Großräume Berlin und Hamburg wurde 1967 zwischen Garath und Ratingen eröffnet. Es folgten der Anschluss des Flughafen an das S-Bahnnetz 1975 mit der Eröffnung des Bahnhofs unter dem Terminal, die Inbetriebnahme der S8 – auch Ost-West-S-Bahn genannt – zwischen den Hauptbahnhöfen von Hagen und Mönchengladbach am 29. Mai 1988 und der S28 zwischen Kaarst und Mettmann am 26. September 1999. Nach Verlängerungen weiterer Linien verkehren im Stadtgebiet heute sieben S-Bahnlinien.

Düsseldorf ist an einige nationale und internationale Fernradwege angeschlossen, u.a. an den Rheinradweg.

Seit dem Jahr 2008 verfügt die Innenstadt Düsseldorfs über ein Fahrradverleihsystem mit Netzcharakter, das auch für Einwegfahrten geeignet ist. Betreiber ist das Unternehmen nextbike. 2011 stehen 400 Mieträder an 58 markierten Stationen im Stadtgebiet zur Verfügung. Pedelecs werden an der Fahrradstation am Hauptbahnhof verliehen.

Die Stadt Düsseldorf ist Mitglied der Arbeitsgemeinschaft fußgänger- und fahrradfreundlicher Städte, Gemeinden und Kreise in Nordrhein-Westfalen, von der sie 2007 das Prädikat „fahrradfreundliche Stadt“ verliehen bekommen hat, auch wenn das Radwegenetz nach Ansicht vieler Bürger noch als sehr lückenhaft angesehen wird.

Düsseldorf ist eine der wenigen Städte in Deutschland, deren Lichtsignalanlagen für Fußgänger über eine separate Gelbphase verfügen. Hier wird das Gelbsignal durch einen rechteckigen gelben Balken gekennzeichnet. Während dieser Zeit haben die Fußgänger die Möglichkeit, die Kreuzung zu räumen, ohne – wie in anderen Städten – gegen Rot laufen zu müssen. Unmittelbar nachdem das Fußgängersignal von Gelb auf Rot wechselt, wird die Freigabe für den Querverkehr eingeleitet. Auch vor der Grünphase gibt es für Fußgänger eine kurze Rot-Gelb-Phase von weniger als einer Sekunde Dauer.
Ampeln in Düsseldorf sind zu einem erheblichen Teil bereits auf Leuchtdioden-Technik umgestellt, was gegenüber Glühlampen niedrigeren Wartungsaufwand, deutlichere Erkennbarkeit und auch einen geringeren Energieverbrauch gewährleisten soll.

Düsseldorfs stürmische Entwicklung zur Großstadt wurde durch die Ansiedlung von Industriebetrieben im 19. Jahrhundert vorangetrieben. Noch heute ist die nordrhein-westfälische Landeshauptstadt eine industriell geprägte Stadt. Die 1129 Betriebe des verarbeitenden Gewerbes (ohne Baugewerbe und Energie- und Wasserversorgung) erwirtschafteten 2005 etwa 29 % des steuerbaren Umsatzes aller Unternehmen in der Stadt und damit nur etwas weniger als der Handel (ca. 32 %), aber deutlich mehr als das Dienstleistungsgewerbe (ca. 9 %). Allerdings ist die Bedeutung des verarbeitenden Gewerbes in den vergangenen Jahren deutlich zurückgegangen. Während die Düsseldorfer Industriebetriebe 1979 noch 90.000 Arbeitsplätze boten, ist die Beschäftigtenzahl in diesem Bereich bis 2006 auf nur noch 38.791 im Jahresmittel zurückgegangen mit fallender Tendenz.

Der größte und bekannteste Düsseldorfer Industriebetrieb ist der Henkel-Konzern, ein Chemieunternehmen, das Wasch- und Reinigungsmittel, Kosmetik und Körperpflegeprodukte sowie Klebstoffe, Dichtstoffe und Produkte für die Oberflächentechnik herstellt und bis heute im Stadtteil Reisholz produziert. Im Bereich Kosmetik und Hygienemittel sind in Düsseldorf die L’Oréal Deutschland GmbH, die Marbert AG sowie die Hakle-Kimberly Deutschland GmbH (Kimberly-Clark Corporation) tätig. Im Bereich Medizintechnik und Pharma ist die Firma Gerresheimer AG ansässig, ein MDAX-notiertes, weltweit tätiges Unternehmen für Hochleistungsgläser und biokompatible Kunststoffprodukte.

Das metallverarbeitende Gewerbe hat eine lange Tradition. Das bekannteste Unternehmen aus diesem Bereich war Mannesmann, das nach seiner Zerschlagung in Teilen in Düsseldorf weiter produziert, darunter die Vallourec & Mannesmann Tubes (Stahlröhren). Ebenfalls bekannt ist der Rheinmetall-Konzern, der größte deutsche Waffenproduzent mit Sitz in Düsseldorf-Derendorf. Weitere metallverarbeitende Unternehmen sind Schmolz + Bickenbach (Edelstahl-Langprodukte) und Hille & Müller GmbH. Seit 2011 hat die GEA Group AG, ein international tätiges Unternehmen im Bereich des Spezialmaschinenbaus, ihren Sitz in Düsseldorf.

Im Bereich Fahrzeugtechnik, Verkehr und Transport ist der größte Betrieb das Mercedes-Benz-Werk in Derendorf, in dem der Mercedes-Benz Sprinter und der VW Crafter produziert und montiert werden sowie CKD-Fahrzeuge für die US-Mercedes-Marke Freightliner hergestellt werden. Demag Cranes produziert in Benrath Hafen-, Terminal- und Eisenbahnkräne sowie automatisch gesteuerte Transportfahrzeuge und entwickelt dazugehörige Management- und Navigationssoftware. Benachbart stellt die Komatsu Mining Germany GmbH Großhydraulik- und Minenhydraulikbagger her. Demag Cranes und Komatsu Mining Germany verbindet der gemeinsame Ursprung aus der Carlshütte AG. Weiterhin stellt die Vossloh Kiepe GmbH Steuerungs- und Antriebstechnik für Straßenbahnen und O-Busse sowie Hybridantriebe und Spezialfahrzeuge her. Die Walther Flender Gruppe produziert Antriebs- und Fördertechnik. Aus der Vergangenheit sind die Schiess AG und die DUEWAG erwähnenswert.

Die Papierindustrie ist in der Stadt durch die Firmen Julius Schulte Söhne GmbH & Co. und den Stora-Enso-Konzern (ehemals Feldmühle) vertreten, der bis 2008 einen Produktionsstandort in Reisholz betrieb und heute seine Deutschlandzentrale in Düsseldorf unterhält. Guschky & Tönnesmann GmbH & Co. KG stellen Ausrüstungen für die Papier- und Verpackungsindustrie her.

Aus dem Bereich Nahrungsmittel sind Zamek Nahrungsmittel GmbH & Co. KG, die Teekanne GmbH, die Düsseldorfer Löwensenf GmbH sowie BASF Personal Care and Nutrition und die Fortin Mühlenwerke bekannt. Das traditionelle Düsseldorfer Altbier wird heute, außer in Hausbrauereien jedoch nicht mehr in großen Brauereianlagen gebraut. Die großen Brauereistandorte in Derendorf und Heerdt sind stattdessen heute Konversionsflächen. Des Weiteren ist mit der Deutschen Tiernahrung Cremer ist Deutschlands größter Mischfutterhersteller in Düsseldorf beheimatet.

Weitere bekannte Betriebe sind die SMS Siemag, die Hütten- und Walzwerkstechnik herstellt, die TELBA AG im Bereich Telekommunikations- und Sicherheitstechnik, die behr Labor-Technik GmbH, die A.u.K. Müller GmbH & Co. KG, ein Unternehmen für Magnetventile, die MHG Strahlanlagen GmbH sowie die "Carborundum-Dilumit Schleiftechnik GmbH (ehemals Carbo Group)", die Schleifscheiben fertigt.

Mit der Metro-Gruppe und ihrer Tochter Metro Cash & Carry sitzt einer der größten Handelskonzerne der Welt in der nordrhein-westfälischen Landeshauptstadt. Weiterhin befinden sich die Zentralen von Bekleidungshäusern wie Peek & Cloppenburg Düsseldorf (P&C) sowie C&A im Stadtgebiet. Als Standort des Modehandels und durch die Messen "igedo" und "CPD" konnte die Stadt den Ruf einer Modestadt erwerben.

Einen Schwerpunkt der städtischen Wirtschaftsstruktur bilden mit rund 1.500 Unternehmen und 24.000 Beschäftigten die Branchen der Informations- und Kommunikationstechnik, darin besonders die Mobilfunkbranche. Mehr als die Hälfte des deutschen Mobiltelefon- und SIM-Karten-Absatzes wird von Düsseldorf aus gesteuert. Die Vodafone GmbH als Nachfolgerin von Mannesmann Mobilfunk hat ihren Sitz ebenso in der Stadt wie E-Plus. Vodafone hat darüber hinaus seine Deutschland- und seine Europazentrale im Vodafone-Campus im linksrheinischen Düsseldorf-Heerdt angesiedelt. Im Jahre 2008 hat das chinesische Telekommunikationsunternehmen Huawei seine europäische Zentrale von London nach Düsseldorf verlegt. Hier baut das Unternehmen seine Europazentrale und ein Forschungs- und Entwicklungszentrum für die Bedürfnisse seiner europäischen Kunden auf. Schon 2005 hatte der ebenfalls chinesische Telekommunikationsausrüster ZTE Deutschland GmbH sein Hauptquartier in Düsseldorf errichtet. Das schwedische Telekommunikationsunternehmen Ericsson tat dies für sein Deutschlandgeschäft bereits 1955.

Auch hat die Uniper SE sowie mehrere ihrer Tochterunternehmen Ihren Hauptsitz in Düsseldorf. Weitere Energieversorger im Stadtgebiet sind die Stadtwerke Düsseldorf und die Naturstrom AG.

Mister Minit, eines der größten Franchiseunternehmen Deutschlands, hat seine Servicezentrale seit 2011 im Düsseldorfer Norden.

In Düsseldorf befinden sich Studios der öffentlich-rechtlichen Fernsehsender Westdeutscher Rundfunk (WDR-Studio Düsseldorf) und Zweites Deutsches Fernsehen (ZDF-Landesstudio Düsseldorf). Aus Düsseldorf kommen außerdem die Programme von NRW.TV und QVC. Ehemals in Düsseldorf ansässig waren von 1998 bis 2006 NBC GIGA, von 1995 bis zu seiner Einstellung 1998 Nickelodeon sowie von 1996 bis zu seiner Einstellung 1998 Der Wetterkanal. Außerdem wurde aus der Landeshauptstadt bis Ende 2003 ein deutsches Programmfenster auf dem Nachrichtensender CNN produziert.

Die in Düsseldorf ansässige DFA produzierte oder produziert unter anderem für NBC GIGA, Der Wetterkanal, CNN D und für NRW.TV.

Seit 2006 produziert das in Düsseldorf ansässige center.tv lokale Nachrichten und Ereignisse für den Großraum Düsseldorf/Neuss. Des Weiteren befinden sich in Düsseldorf diverse unabhängige Filmproduktionsfirmen, wie zum Beispiel die Public Vision TV OHG und Busse & Halberschmidt.

Ebenso hat die Nachrichtenagentur ISQ.networks Press Agency mit 24.000 Beschäftigten weltweit ihre globale Zentrale im GAP 15 am Graf-Adolf-Platz. Das Unternehmen verfügt zwar über mehr als 100 Studios weltweit, in Düsseldorf selbst ist jedoch kein Studio vorhanden.

Der landesweite TV-Lernsender nrwision bündelt in seiner Mediathek Fernsehsendungen über Düsseldorf bzw. von Fernsehmachern aus Düsseldorf.

Die Stadt ist auch Sitz des 1990 gegründeten "Verbands der Betriebsgesellschaften in Nordrhein-Westfalen e. V. (BGNRW)", der die Interessen von 43 Betriebsgesellschaften des nordrhein-westfälischen Lokalfunks vertritt. Der Verband ist Mitglied in der "Arbeitsgemeinschaft Privater Rundfunk (APR)" mit Sitz in München. In Düsseldorf ist die private Rundfunkstation Antenne Düsseldorf mit Rahmenprogramm von Radio NRW ansässig. Hörfunk für die Düsseldorfer Hochschulen macht hochschulradio düsseldorf, ein Campusradio mit eigener 24-Stunden-Frequenz.

Als Tageszeitungen erscheinen in Düsseldorf die Westdeutsche Zeitung, die Rheinische Post, eine Lokalausgabe des Express sowie von der in Essen erscheinenden Neuen Rhein/Neue Ruhr-Zeitung. Die Regionalseiten in der Welt kompakt erschienen letztmals am 28. August 2015. Als bedeutende überregionale Veröffentlichungen sind das Handelsblatt, die Wirtschaftswoche und die mittlerweile eingestellte Junge Karriere zu nennen. Wöchentlich erscheinen außerdem das Düsseldorfer Amtsblatt und die Anzeigenblätter „Düsseldorfer Anzeiger“ und „Rheinbote“. Zudem ist die Stadt der Sitz des bundesweit erscheinenden Content-Marketing-Magazins Wirtschaftsblatt.

Düsseldorf ist zudem der umsatzstärkste Werbestandort der Bundesrepublik. Neben den Riesen BBDO, Grey und Ogilvy & Mather und Publicis hat eine Vielzahl kleiner Agenturen ihren Sitz oder eine deutsche Niederlassung in Düsseldorf.

Düsseldorf gehört zu den führenden IT-Standorten in Deutschland. Die IT-Infrastruktur der Stadt verfügt über internationale und regionale Internet-Knoten. Ab Oktober 2011 steht in Düsseldorf das LTE-Mobilfunknetz zur Verfügung. Am 9. Januar 2012 soll der Probebetrieb für den "Digitalfunk der Behörden und Organisationen mit Sicherheitsaufgaben (BOS)" in Düsseldorf starten. Die Firma Wall AG bietet an demnächst 50 Hotspots kostenlos öffentliches
WLAN an.

Düsseldorf hat sich als vielseitiger Standort für Einkaufszentren und Geschäfte unterschiedlichster Art und Größe entwickelt. Die Gesamtverkaufsfläche im Stadtgebiet wird seitens der Stadtverwaltung mit 834.215 m² angegeben und liegt damit bezogen auf das Verhältnis von Verkaufsfläche zu Einwohnern höher als in München und niedriger als in Frankfurt am Main. Insbesondere im Bereich der Mode hat die Stadt – nicht zuletzt begünstigt durch die Modemessen und die ansässigen Handelsunternehmen P&C und C&A – eine Vorreiterrolle, beim Textileinzelhandel ist sie deutschlandweit führend. Auch im Segment der Luxusbekleidung ist Düsseldorf führend. Die Kö ist laut Jones Lang LaSalle nach wie vor Deutschlands meistbesuchte Luxusmeile. Im Jahr 2012 stellte Jones Lang LaSalle in einer Untersuchung fest, dass die Kö ihren Vorsprung nochmals ausbauen konnte. Mit 5.935 (2011: 5.800) Passanten pro Stunde liegt sie weit vor der Stiftstraße in Stuttgart (2.310 Passanten) sowie der Goethestraße in Frankfurt am Main (1.520 Passanten). Die Düsseldorfer Schadowstraße ist die europaweit umsatzstärkste Einkaufsmeile. Durch den Bau einer neuen U-Bahn-Linie, der Wehrhahn-Linie, ist es jedoch zu einem Rückgang der Besucherzahlen gekommen, da die Straße abschnittsweise gesperrt ist und der Zugang zu den Geschäften dadurch eingeschränkt wird. Dennoch behält die Schadowstraße ihr Merkmal als umsatzstärkste Einkaufsstraße Europas.

Neben den klassischen Einkaufsstraßen wie der sehr bekannten Kö, der Schadowstraße und der Flinger Straße verfügt Düsseldorf über mehrere Einkaufszentren, die teilweise an die Einkaufsstraßen angrenzen. Die Flinger Straße, die in der günstigen bis mittleren Preiskategorie liegt, schafft mit 10.150 Passanten pro Stunde erstmals den Sprung in die Top 10 der meistbesuchten Einkaufsstraßen in Deutschland.

Auf der Königsallee sind dies vor allem das Kö-Center an der nördlichen Allee, die Kö-Galerie in der Nähe der Kreuzung mit der Steinstraße mit Zu- und Durchgang zur Stadtsparkasse auf der Berliner Allee und das Sevens Center zwischen der Kreuzung mit der Steinstraße und der Kö-Galerie.

In Nähe zur Königsallee befinden sich zudem die Schadow-Arkaden im Block Schadow-, Blumenstraße und Martin-Luther-Platz.

In weniger belebten Teilen der Innenstadt sind ebenfalls Einkaufscenter vorzufinden, insbesondere das Stilwerk auf der Grünstraße und die Düsseldorf Arcaden am Bilker Bahnhof, die auch das Stadtteilzentrum, ein öffentliches Schwimmbad und eine Zweigstelle der Stadtbüchereien Düsseldorf beheimaten.

Außerhalb der Innenstadt gibt es weitere Einkaufszentren wie etwa das Einkaufszentrum Westfalenstraße an der gleichnamigen Straße in Rath, das im Frühjahr 2010 neu eröffnete B8 Center in Flingern und für die sowohl für Reisende als auch andere Besucher konzipierten AirportArkaden im Flughafen Düsseldorf.

"Einrichtungen und Körperschaften des öffentlichen Rechts:"

Düsseldorf entwickelte sich vor allem in der preußischen Ära im 19. und frühen 20. Jahrhundert zu einem wichtigen Verwaltungssitz. Die Ansiedlung des Oberlandesgericht Düsseldorf förderte diese Entwicklung besonders.
Neben dem Oberlandesgericht beherbergt Düsseldorf zahlreiche weitere Gerichte, so das Amts- und das Landgericht Düsseldorf, das Sozialgericht Düsseldorf, das Finanzgericht Düsseldorf, das Verwaltungsgericht Düsseldorf, das Arbeitsgericht Düsseldorf und das Landesarbeitsgericht Nordrhein-Westfalen. Durch Düsseldorfer Gerichte wurden Entscheidungen von bundesweiter Bedeutung getroffen. Im Bereich patentrechtlicher Gerichtsverfahren haben die Zivilgerichte Düsseldorfs mittlerweile eine internationale Bedeutung erlangt.

Die Deutsche Rentenversicherung Rheinland (ehemals LVA Rheinprovinz) hat seit ihrer Gründung im 19. Jahrhundert ihren Sitz in Düsseldorf. Ferner besteht ein regionaler Standort der Sozialversicherung für Landwirtschaft, Forsten und Gartenbau (SVLFG).

Als direkte Folge der Verwaltungsansiedlung durch Preußen wurde das Regierungspräsidium für den Niederrhein und das Bergische Land bei der Neuordnung Preußens im 19. Jahrhundert in Düsseldorf angesiedelt. Noch heute hat die Bezirksregierung Düsseldorf hier ihren Sitz. Bis vor wenigen Jahren war ebenso die Oberfinanzdirektion Rheinland seit der preußischen Herrschaft über das Rheinland in Düsseldorf ansässig.

Die Handwerkskammer Düsseldorf umfasst als Kammerbezirk den Regierungsbezirk Düsseldorf, und hat dort ebenfalls seinen Sitz. Den Kammerbezirk der Industrie- und Handelskammer zu Düsseldorf (IHK Düsseldorf) bilden die Stadt Düsseldorf und der Kreis Mettmann, die früher den Landkreis Düsseldorf-Mettmann bildeten. Die Architektenkammer Nordrhein-Westfalen, die größte Einrichtung dieser Art in Deutschland, betreut von der Landeshauptstadt aus die Architekten und Stadtplaner des Landes. Die Rechtsanwaltskammer Düsseldorf vertritt die Interessen von 11.403 Rechtsanwälten im Bezirk des Oberlandesgerichts Düsseldorf. Sie ist somit die sechstgrößte von 28 Rechtsanwaltskammern in Deutschland.

Der Preußische Provinziallandtag für das Rheinland nahm ebenfalls in Düsseldorf, im Ständehaus, seinen Sitz, ebenso die Evangelische Kirche im Rheinland.

Durch Entscheidung der Militärregierung der britischen Besatzungszone wurde Düsseldorf im Jahre 1946 zum Regierungssitz des Landes Nordrhein-Westfalen bestimmt. Die Stadt führt seither die offizielle, gleichwohl rechtlich nicht normierte Bezeichnung "Landeshauptstadt". Der Landtag, die Staatskanzlei, alle Landesministerien und der Landesrechnungshof sind in Düsseldorf angesiedelt; über die letzten Jahrzehnte hat sich im Bereich der Rheinkniebrücke auf dem rechten Rheinufer ein Regierungsviertel herausgebildet. Dort getroffene Richtungsentscheidungen gelten als Indikatoren für die Entwicklungen in Deutschland. Nordrhein-westfälischen Regierungsbildungen und Regierungskrisen kommen Signalwirkungen für die Bundespolitik zu.

Düsseldorf galt in der Bonner Republik als Stadt der Verbände. So war der Deutsche Gewerkschaftsbund (DGB) lange Zeit in Düsseldorf ansässig. Heute sind die wichtigsten Verbände mit Hauptsitz in Düsseldorf:


Aber auch zahlreiche kleinere Verbände haben ihren Sitz in Düsseldorf, beispielsweise der Deutsche Verband für Schweißen und verwandte Verfahren (DVS), der Architekten- und Ingenieurverein Düsseldorf, der Landesmusikrat Nordrhein-Westfalen und der Verband türkischer Unternehmer und Industrieller in Europa.

Düsseldorf hat als langjährige bergische Residenzstadt und späterer Verwaltungssitz in der Rheinprovinz Preußens neben repräsentativen Verpflichtungen auch stets zentralörtliche Funktionen erfüllt.

Die erste Lateinschule wurde im 14. Jahrhundert erwähnt.
1545 wurde das älteste Gymnasium, dessen Tradition bis heute besteht, gegründet. Für die Ausbildung von Künstlern war Düsseldorf seit dem 18. Jahrhundert ein bedeutender Akademiestandort.

Die Franziskaner boten ab 1673 theologische Kurse in Düsseldorf an. Ab 1728 wurden in Düsseldorf Kurse in Philosophie und Theologie angeboten, die als Bestandteile eines Studiums gewertet werden konnten, eine juristische Akademie erhielt 1755 die kurfürstliche Bestätigung zur Ausbildung höherer Beamter, 1747 entstand das "Collegio anatomico-chirurgicum" zur Ausbildung von Militär- und Wundärzten.
Ab 1779 mussten höhere Beamte der Landesverwaltung mindestens zwei Jahre in Düsseldorf Rechtswesen studiert haben.

Auf Grund der Nähe des Ruhrgebietes wurde 1917 Düsseldorf zum Sitz des Kaiser-Wilhelm-Instituts für Eisenforschung, des heutigen Max-Planck-Instituts für Eisenforschung.

Eine medizinische Akademie kam zu Beginn des 20. Jahrhunderts hinzu. Dennoch wurde Düsseldorf erst 1965 Universitätsstadt.

1964 trat die Stadt dem „Institut zur Erlangung der Hochschulreife für Handwerker, Facharbeiter und andere Berufstätige mit abgeschlossener Ausbildung e.V.“ bei und ist seit dem Träger des Wilhelm-Heinrich-Riehl-Kollegs.

In Düsseldorf befinden sich folgende wissenschaftlich-akademischen Einrichtungen:


Im Bereich der Stiftungen, die Bildung und Forschung fördern, genießt die in Düsseldorf ansässige Gerda-Henkel-Stiftung einen besonderen Ruf.

In Düsseldorf gibt es 110 Grundschulen, 14 Hauptschulen, 13 Realschulen und 21 Gymnasien. Weiterhin gibt es 8 Gesamt- und Waldorfschulen sowie 6 ausländische Schulen und das Wilhelm-Heinrich-Riehl-Kolleg als Institution der Erwachsenenbildung.

Einen Schwerpunkt hat die Stadt Düsseldorf auf die Qualität der Schulgebäude gelegt. So wurde im Jahr 2000 eine Immobilienfirma beauftragt, die Mängel sämtlicher Schulgebäude zu erfassen und einen Plan für deren Beseitigung zu erstellen. Statt der bis dahin jährlichen Investitionen von < 5 Mio. Euro wurde ein Bedarf von 35 Millionen Euro pro Jahr diagnostiziert. Daraufhin wurde eine „Masterplan-Schule“ beschlossen, mit dem in den Jahren 2002–2020 insgesamt 600 Millionen Euro investiert werden sollen.

Deutschlandweit einzigartig ist die nachhaltige musikalische Breitenförderung durch die vom Städtischen Musikverein initiierte SingPause an mehr als der Hälfte der Düsseldorfer Grundschulen.

Durch Beschluss des Stadtrates sind die Düsseldorfer Kindertagesstätten für alle Kinder ab drei Jahren beitragsfrei, während in Nordrhein-Westfalen regulär nur das letzte Kindergartenjahr beitragsfrei gestellt ist.

Bei der Quote von Betreuungsplätzen für Kinder unter drei Jahren liegt Düsseldorf an der Spitze der Großstädte in Nordrhein-Westfalen mit 38,4 % zu Beginn des Jahres 2013. Damit liegt die Betreuungsquote in Düsseldorf bereits vor Inkrafttretens des im Kinderförderungsgesetz verankerten Rechtsanspruchs auf einen Betreuungsplatz zum 1. August 2013 über dem Ziel der Landesregierung von 32 %. Dennoch strebt die Stadt zeitnah eine Betreuungsquote von 50 % und mittelfristig von 60 % an.

Düsseldorf hat in zahlreichen deutschlandweiten wie internationalen Städtevergleichen und Rankings zumeist vordere Plätze belegt. Hierbei wurden unterschiedliche Indikatoren für die Wirtschaftskraft, die Qualität der Infrastruktur und die Lebensqualität herangezogen. Insbesondere aus den internationalen Städtevergleichen ergibt sich, dass die Landeshauptstadt dem transnationalen Netz der Global Cities zugerechnet wird.

Das gute Abschneiden in vielen Rankings beruht auf der Schuldenfreiheit der Stadt bei gleichzeitig hohem Vermögensstand, dem internationalen Flughafen, der zentralen Lage, der reichhaltigen Bildungsinfrastruktur und dem Kulturleben, dem hohen durchschnittlichen Pro-Kopf-Einkommen und den steigenden Einwohnerzahlen. Einzelne Rankings berücksichtigen auch Faktoren wie die Dichte international tätiger Unternehmen oder das reiche Angebot an Grünflächen und Naherholungsgebieten.

Die wichtigsten Rankings sind:

Die Stadt Düsseldorf vergibt neben dem Ehrenbürgerrecht noch andere Ehrungen und Auszeichnungen.

Seit 1972 wird im dreijährigen, seit 1981 im zweijährigen Abstand der Heinrich-Heine-Preis an „Persönlichkeiten, die durch ihr geistiges Schaffen im Sinne der Grundrechte der Menschen, für die sich Heinrich Heine eingesetzt hat, den sozialen und politischen Fortschritt fördern, der Völkerverständigung dienen oder die Erkenntnisse von der Zusammengehörigkeit aller Menschen verbreiten“. Vorgänger des Preises war der Immermann-Preis.

Der Helmut-Käutner-Preis ist eine zweijährlich vergebene Auszeichnung, die an Persönlichkeiten verliehen wird, die „durch ihr Schaffen die Entwicklung der deutschen Filmkultur nachdrücklich unterstützt und beeinflusst, ihr Verständnis gefördert und zu ihrer Anerkennung beigetragen haben“.

Der Förderpreis für Literatur der Landeshauptstadt Düsseldorf wird seit 1972 einmal im Jahr durch den Rat der Landeshauptstadt an Künstler und Gruppen, insbesondere der Bereiche Dichtung, Schriftstellerei, Kritik und Übersetzung vergeben. Der Förderpreis wird sowohl für eine einzige künstlerische Leistung als auch für die bisherige Gesamtleistung eines jungen Künstlers verliehen, deren bzw. dessen weitere Entwicklung eine Förderung verdient.

Der Kunstpreis der Landeshauptstadt Düsseldorf ist eine jährliche vergebene Auszeichnung an einen Bildenden Künstler, dessen Werk „richtungsweisend für die Entwicklung der Gegenwartskunst“ ist.

Weitere Auszeichnungen sind der „Große Ehrenring“, der „Jan-Wellem-Ring“ und die „Verdienstplakette“.







Abgekürzt zitiert sind:


</doc>
<doc id="1178" url="https://de.wikipedia.org/wiki?curid=1178" title="Datei">
Datei

Eine Datei () ist ein Bestand meist inhaltlich zusammengehöriger Daten, der auf einem Datenträger oder Speichermedium gespeichert ist. Diese Daten können somit über die Laufzeit eines Programms hinaus existieren und werden als "persistent" bezeichnet – sie sind bei Programmende nicht verloren.

In der elektronischen Datenverarbeitung ist der Inhalt jeder Datei zunächst eine eindimensionale Aneinanderreihung von Bits, die normalerweise in Byte-Blöcken zusammengefasst interpretiert werden. Erst der Anwender einer Datei bzw. ein Anwendungsprogramm oder das Betriebssystem selbst interpretieren diese Bit- oder Bytefolge beispielsweise als einen Text, ein ausführbares Programm, ein Bild oder eine Tonaufzeichnung. Eine Datei besitzt also ein Dateiformat.

Das Wort „Datei“ ist ein Kofferwort aus "Da"ten und Kar"tei", geschaffen durch das Deutsche Institut für Normung (DIN).

Der deutsche Begriff „Datei“ ist deutlich enger gefasst als die englische Übersetzung "file", welche oft auch eine (Papier-)Akte, eine (Papier-)Kartei oder einen Karteikasten beschreibt. Evtl. ist eine Präzisierung auf ' oder ' notwendig. Der Duden lässt eine Bedeutung von „Datei“ als (Papier-)Sammlung von Informationen zu, diese Verwendung ist aber wohl selten.

Dateien werden in den meisten Betriebssystemen über Dateisysteme verwaltet. Ein Dateisystem verwaltet das Speichermedium, indem in Listen vermerkt wird, welche Bereiche des Mediums durch welche Dateien belegt sind, welche Bereiche frei sind, sowie oft Protokolle zu geplanten und/oder abgeschlossenen Änderungen.

Obwohl eine Aufgabe des Dateisystems darin besteht, vom konkreten Medium zu abstrahieren („alle gleich zu behandeln“), sind doch viele Dateisysteme an die üblichen technischen Eigenschaften der Speichermedien angepasst (z. B. Blockgröße 512 Byte für Festplatten).

Für die meisten Dateisysteme ist 1 Byte die kleinste Verwaltungseinheit, d. h., die Länge des Dateiinhalt-Bitstroms muss auf ganze Bytes aufgehen (wobei im Allgemeinen auch 0 Byte = 0 Bit erlaubt sind).

Das Dateisystem verwaltet neben Verzeichnissen mit Dateinamen und -speicherort fast immer noch weitere Dateiattribute. Zu diesen gehören häufig der Dateityp (Verzeichnis, normale Datei, spezielle Datei), die Dateigröße (Anzahl der Bytes in der Datei), Schreib- und Leserechte, Zeitstempel („Datum“, der Erzeugung, des letzten Zugriffs und der letzten Änderung) sowie gegebenenfalls noch andere Informationen. Eine Datei kann in vielen Dateisystemen durch ein Attribut als versteckte Datei gekennzeichnet werden.

Die in Dateinamen verwendbaren Zeichen sind abhängig von Dateisystem, Betriebssystem und gegebenenfalls Sprachoptionen. Bei Unix-kompatiblen Dateisystemen dürfen in einem Dateinamen kein Schrägstrich („/“) und kein Nullzeichen stehen, ferner ist die Länge des Dateinamens auf 255 Zeichen begrenzt. Die Zeichen können unterschiedlich codiert sein. Neuere Betriebssysteme unterstützen auch Unicode.

Nach ihrem Inhalt unterscheidet man unter anderem:

Moderne Dateisysteme unterstützen auch sogenannte „Sparse-Dateien“: Nur tatsächlich mit Daten gefüllte Abschnitte einer (großen) Datei werden tatsächlich gespeichert; die dazwischen liegenden „freien Bereiche“ werden nicht gespeichert und als „mit Null-Bytes gefüllt“ angenommen/bewertet.

Manche Dateisysteme bieten ferner an, Dateien transparent zu komprimieren oder zu verschlüsseln („transparent“: Das lesende/bearbeitende Programm kann die Datei normal verwenden, als ob die Datei nicht komprimiert/verschlüsselt wäre – es „sieht durch diesen Vorgang ungestört hindurch“).

Unter manchen Betriebssystemen werden auch

wie Dateien gehandhabt (vor allem Betriebssysteme der Unix-Familie).

Möglichkeiten, das Dateiformat zu kennzeichnen, beinhalten

Eine solche Kennzeichnung ist teilweise obligatorisch, teilweise dient sie lediglich der Orientierung des Benutzers. Oft fehlen Kennzeichnungen jeder Art; für solche Situationen gibt es spezielle Programme, die den Typ einer Datei zu bestimmen versuchen. Im Unix-Umfeld ist dafür z. B. der Befehl "file" sehr verbreitet.

In grafischen Dateimanagern wie Finder, Windows-Explorer, Nautilus oder Dolphin werden Dateien gewöhnlich als Liste oder Symbole auf einem Arbeitsblatt (Fenster, Ordner u. a.) dargestellt.




</doc>
<doc id="1179" url="https://de.wikipedia.org/wiki?curid=1179" title="Dateimanager">
Dateimanager

Ein Dateimanager () ist ein Computerprogramm zum Verwalten von Inhalten auf Dateisystemen, die sich auf unterschiedlichen Speichermedien befinden können. Neben der übersichtlichen Darstellung in Form einer (oft grafischen) Benutzerschnittstelle zählen das Auflisten, das Umbenennen und Verschieben, das Kopieren und das Löschen von Dateien und Verzeichnissen zu den Grundfunktionen. Gängig ist auch die Möglichkeit zur Bearbeitung von Metadaten unterstützter Dateisysteme, wie beispielsweise Dateiattribute, Dateiberechtigungen und Verknüpfung.

Anders als im Server-Umfeld, in dem teils heute noch textbasierte Shells vorzufinden sind, entstand auf Personal Computern schon früh eine grafische Bedienoberfläche (, kurz “GUI”), in der die Aufgabe des Dateimanagements ein spezielles Programm übernahm: der Dateimanager. Anfang der 1980er Jahre findet man einfache Dateimanager beim Xerox Star oder bei der Apple Lisa. Weil diese Systeme für damalige Verhältnisse teuer waren, setzten sie sich nicht durch. Erst Mitte der 1990er Jahre findet sich der Dateimanager als Teil des Standardrepertoires fast aller Desktop-Betriebssysteme. Bis dahin gab es einige meist textbasierte Dateimanager für die meistverbreiteten Betriebssysteme, etwa den Norton Commander unter DOS.

In aktuellen Betriebssystemen für PCs und Notebooks ist immer ein Dateimanager enthalten. Auch gibt es eine Vielzahl an Dateimanagern von Drittanbietern für alle gängigen Betriebssysteme, die dem enthaltenen Dateimanager meist in einigen Punkten überlegen sind.

Außer auf PCs findet man sie jedoch auch auf PDAs, eingebetteten Systemen (wie Routern oder Firewalls), Satellitenreceivern und Smartphones, obwohl sie auf vielen dieser Systeme meist nachinstalliert werden müssen. Der Grund hierfür ist einerseits die Computer-Sicherheit, andererseits wünscht der Hersteller eines solchen Geräts oft nicht, dass ein Anwender direkt am Dateisystem arbeitet.

Es gibt mehrere (Darstellungs-)Konzepte von Dateimanagern, die unterschiedliche Metaphern für ihre Darstellung verwenden. Einige Programme unterstützen auch mehrere Konzepte. Prinzipiell wird zwischen navigatorischen, zweispaltigen und spatialen Ansätzen unterschieden:

Navigatorische Dateimanager stellen die Inhalte eines beliebigen Verzeichnisses umschaltbar in einem Fenster dar, wobei noch eine Übersicht der Verzeichnisstruktur und ihrer Dateiinhalte, wie beispielsweise eine Baumansicht neben der Verzeichnisansicht, möglich ist. Bekannte Beispiele sind der Windows-Explorer und Nautilus. NeXTStep und Mac OS X sowie einige weitere Dateimanager wie ranger oder One Commander verwenden mit den "Miller-Spalten" eine Darstellung, in der die Ordnerstruktur horizontal statt vertikal angezeigt wird.

Dateimanager nach Vorbild des PathMinders mit zweispaltiger Ansicht () stellen die Inhalte zweier Verzeichnisse gegenüber dar.

Beim räumlichen Konzept (Spatial) wird für jeden geöffneten Ordner ein neues Fenster erzeugt, was als Entsprechung zum Umgang mit physischen Objekten wirken soll. Dabei ist ein einzelnes Fenster fest einem bestimmten Verzeichnis zugeordnet und umgekehrt.


</doc>
<doc id="1181" url="https://de.wikipedia.org/wiki?curid=1181" title="Dateisystem">
Dateisystem

Das Dateisystem ( oder ) ist eine Ablageorganisation auf einem Datenträger eines Computers. Dateien können gespeichert, gelesen, verändert oder gelöscht werden. Für den Nutzer müssen Dateiname und computerinterne Dateiadressen in Einklang gebracht werden. Das leichte Wiederfinden und das sichere Abspeichern sind wesentliche Aufgaben eines Dateisystems. Das Ordnungs- und Zugriffssystem berücksichtigt die Geräteeigenschaften und ist normalerweise Bestandteil des Betriebssystems.

Dateien haben in einem Dateisystem fast immer mindestens einen Dateinamen sowie Attribute, die nähere Informationen über die Datei geben. Die Dateinamen sind in "Verzeichnissen" abgelegt; Verzeichnisse sind üblicherweise spezielle Dateien. Über derartige Verzeichnisse kann ein Dateiname (und damit eine Datei) sowie die zur Datei gehörenden Daten vom System gefunden werden. Ein Dateisystem bildet somit einen Namensraum. Alle Dateien (oder dateiähnlichen Objekte) sind so über eine eindeutige Adresse (Dateiname inkl. Pfad oder URI) – innerhalb des Dateisystems – aufrufbar. Der Name einer Datei und weitere Informationen, die den gespeicherten Dateien zugeordnet sind, werden als Metadaten bezeichnet.

Für unterschiedliche Datenträger (wie Magnetband, Festplattenlaufwerk, optische Datenträger (CD, DVD, …), Flash-Speicher, …) gibt es spezielle Dateisysteme.

Das Dateisystem stellt eine bestimmte Schicht des Betriebssystems dar: Alle Schichten darüber (Rest des Betriebssystems, Anwendungen) können auf Dateien abstrakt über deren Klartext-Namen zugreifen. Erst mit dem Dateisystem werden diese abstrakten Angaben in physische Adressen (Blocknummer, Spur, Sektor usw.) auf dem Speichermedium umgesetzt. In der Schicht darunter kommuniziert der Dateisystemtreiber dazu mit dem jeweiligen Gerätetreiber und mittelbar auch mit der Firmware des Speichersystems (Laufwerks). Letztere nimmt weitere Organisationsaufgaben wahr, beispielsweise den transparenten Ersatz fehlerhafter Blöcke durch Reserveblöcke.

Historisch gesehen sind schon die ersten Lochstreifen- (auf Film- später auf Papierstreifen) und Lochkarten-Dateien Dateisysteme. Sie bilden ebenso wie Magnetbandspeicher lineare Dateisysteme. Die später für die Massenspeicherung und schnellen Zugriff entwickelten Trommel- und Festplattenspeicher ermöglichten dann erstmals durch wahlfreien Zugriff auf beliebige Positionen im Dateisystem komplexere Dateisysteme. Diese Dateisysteme bieten die Möglichkeit, per Namen auf eine Datei zuzugreifen. Das Konzept der Dateisysteme wurde schließlich soweit abstrahiert, dass auch Zugriffe auf Dateien im Netz und auf Geräte, die virtuell als Datei verwaltet werden, über Dateisysteme durchgeführt werden können. Somit sind Anwendungsprogramme in der Lage, auf diese unterschiedlichen Datenquellen über eine einheitliche Schnittstelle zuzugreifen.

Massenspeichergeräte wie Festplatten-, CD-ROM- und Diskettenlaufwerke haben normalerweise eine Blockstruktur, d. h. aus Betriebssystemsicht lassen sich Daten nur als Folge ganzer Datenblöcke lesen oder schreiben. Ein Speichergerät präsentiert das Speichermedium gegenüber dem Betriebssystem lediglich als große lineare Anordnung vieler nummerierter (und darüber adressierbarer) Blöcke.

Ein Block umfasst heute meistens 512 (= 2) oder 4096 (= 2) Bytes, auf optischen Medien (CD-ROM, DVD-ROM) 2048 (= 2) Bytes. Moderne Betriebssysteme fassen aus Performance- und Verwaltungsgründen mehrere Blöcke zu einem Cluster fester Größe zusammen. Heute sind Cluster mit acht oder noch mehr Blöcken üblich, also 4096 Bytes pro Cluster. Die Clustergröße ist im Allgemeinen eine Zweierpotenz (1024, 2048, 4096, 8192, …).

Eine Datei ist ein definierter Abschnitt eines Datenspeichers, der auf dem Gerät aus einem oder mehreren Clustern besteht. Jede Datei erhält außerdem eine Beschreibungsstruktur, die die tatsächliche Größe, Referenzen auf die verwendeten Cluster und evtl. weitere Informationen wie Dateityp, Eigentümer, Zugriffsrechte enthalten kann (Metadaten).

Für die Zuordnung von Clustern zu Dateien gibt es dabei mehrere Möglichkeiten.

Verzeichnisse enthalten Dateinamen und Referenzen zu den jeweiligen Beschreibungsstrukturen. Da Verzeichnisse auch Speicherflächen sind, werden meist speziell gekennzeichnete Dateien als Verzeichnisse verwendet. Die erste Beschreibungsstruktur kann dabei das Ausgangsverzeichnis enthalten.

Ein weiterer eigener Bereich auf dem Speichermedium dient der Buchführung, welche Blöcke oder Cluster schon belegt und welche noch frei sind. Ein oft dafür genutztes Mittel ist die "" (BAM), in der für jeden Block ein Speicherbit angelegt ist, das anzeigt, ob der Block belegt oder frei ist. Die BAM enthält im Prinzip redundante Informationen und dient der Effizienz der Verwaltung; sollten die dort gespeicherten Informationen verlorengehen, so kann die BAM neu erstellt werden.

Im Allgemeinen ist der erste Block für einen sogenannten "Bootblock" (z. B. Master Boot Record) reserviert, der für das Hochfahren des Systems verwendet werden kann. Auf einem Speichermedium mit mehreren Partitionen steht unmittelbar im Anschluss typischerweise die Partitionstabelle, die Organisationsdaten zu den Partitionen enthält. Weder Bootblock noch Partitionstabelle sind Teil des eigentlichen Dateisystems.

Meist enthält jede Partition ein eigenes, von den Daten auf anderen Partitionen unabhängiges Dateisystem; die Ausführungen oben beziehen sich auf die einzelnen Partitionen, die sich eine nach der anderen an die Partitionstabelle anschließen.

Aus Effizienzgründen, also vor allem zur Erhöhung der Leistung/Zugriffsgeschwindigkeit, wurden diverse Strategien entwickelt, wie diese Organisationsstrukturen innerhalb des zur Verfügung stehenden Speicherbereichs angeordnet werden. Da es beispielsweise in vielen Dateisystemen beliebig viele Unterverzeichnisse geben kann, verbietet es sich von vornherein, feste Plätze für diese Verzeichnisstrukturen zu reservieren, es muss alles dynamisch organisiert werden. Es gibt auch Dateisysteme wie einige von Commodore, die die grundlegenden Organisationsstrukturen wie Wurzelverzeichnis und BAM in der Mitte des Speicherbereichs (statt wie meist bei anderen an dessen Anfang) anordnen, damit der Weg, den der Schreib-Lese-Kopf von dort zu den eigentlichen Daten und zurück zurückzulegen hat, im Mittel verringert wird. Allgemein kann es ein Strategieansatz sein, eigentliche Daten und ihre Organisationsdaten physisch möglichst nah beieinander anzuordnen.

Ein Programm greift auf die Massenspeicher über das Dateisystem zu. Unter Unix und ähnlichen Betriebssystemen werden dazu Systemaufrufe zur Verfügung gestellt. Die wichtigsten Systemaufrufe sind hier:



Außerdem bietet das Betriebssystem Verwaltungsfunktionen, zum Beispiel für das Umbenennen, das Kopieren und Verschieben, Erzeugen eines Dateisystems auf einem neuen Datenträger, für Konsistenzprüfung, Komprimierung oder Sicherung (je nach Betriebssystem und Dateisystem verschieden).

Die Umsetzung der Systemaufrufe eines Programms wird oft vom Kernel eines Betriebssystems implementiert und unterscheidet sich bei den verschiedenen Dateisystemen. Der Kernel übersetzt die Zugriffe dann in die Blockoperationen des jeweiligen Massenspeichers. (Anmerkung: Tatsächlich trifft dies nur auf sogenannte monolithische Kernel zu. Hingegen sind auf einem Microkernel oder Hybridkernel aufgebaute Systeme so konzipiert, dass die Dateisystemoperationen nicht vom Kernel selbst ausgeführt werden müssen.)

Wenn ein Programm eine Datei mittels "open" öffnet, wird der Dateiname im Verzeichnis gesucht. Die Blöcke auf dem Massenspeicher ermittelt das Betriebssystem aus den entsprechenden Beschreibungsstrukturen. Falls eine Datei im Verzeichnis gefunden wird, erhält das Betriebssystem auch ihre Beschreibungsstruktur und damit die Referenzen zu den zugehörigen Clustern und gelangt über diese zu den konkreten Blöcken.

Mit "read" kann das Programm dann auf die Cluster der Datei (und damit auf die Blöcke auf dem Massenspeicher) zugreifen. Wird eine Datei aufgrund von "write" größer, wird bei Bedarf ein neuer Cluster aus der Freiliste entnommen und in der Beschreibungsstruktur der Datei hinzugefügt. Auch die anderen Systemaufrufe lassen sich so in Cluster- bzw. Blockzugriffe übersetzen.

Die historisch ersten Dateisysteme waren lineare Dateisysteme auf Lochband oder Lochkarte sowie die noch heute für die Sicherung von Daten eingesetzten Magnetbandsysteme.

Frühe Dateisysteme (CP/M, Apple DOS, Commodore DOS) hatten nur ein einzelnes Verzeichnis, das dann Verweise auf alle Dateien des Massenspeichers enthielt. Mit wachsender Kapazität der Datenträger wurde es immer schwieriger, den Überblick über hunderte und tausende Dateien zu bewahren, deshalb wurde das Konzept der Unterverzeichnisse eingeführt. So wurde dieses eine Verzeichnis in den meisten modernen Dateisystemen zum Wurzelverzeichnis, das neben normalen Dateien auch Verweise auf weitere Verzeichnisse, die Unterverzeichnisse, enthalten kann. Auch diese dürfen wieder Unterverzeichnisse haben.

Dadurch entsteht eine Verzeichnisstruktur, die oft als Verzeichnisbaum dargestellt wird. Das Festplattenlaufwerk C: unter Windows beinhaltet beispielsweise neben Dateien wie "boot.ini" und "ntldr" auch Verzeichnisse wie "Programme", "Dokumente und Einstellungen" usw. Ein Verzeichnis wie zum Beispiel "Eigene Dateien" kann dann wieder Unterverzeichnisse wie "Eigene Bilder" oder "Texte" enthalten. In "Texte" können dann beispielsweise die normalen Dateien "Brief1.txt" und "Brief2.txt" stehen.

Die Verzeichnisse werden auch Ordner genannt und sind, je nach Betriebssystem, durch "umgekehrten Schrägstrich" (englisch ') „\“ (DOS, Windows, TOS), "Schrägstrich" (englisch ') „/“ (Unix, Linux, macOS, AmigaOS), "Punkt" „.“ (OpenVMS) oder "Doppelpunkt" „:“ (ältere Mac-OS-Versionen) getrennt. Da sich eine Hierarchie von Verzeichnissen und Dateien ergibt, spricht man hier von hierarchischen Dateisystemen. Den Weg durch das Dateisystem, angegeben durch Verzeichnisnamen, die mit den Trennzeichen voneinander getrennt werden, nennt man Pfad. Auf die Datei "Brief1.txt" kann mit

zugegriffen werden. Bei "DOS/Windows" gibt es Laufwerksbuchstaben gefolgt von einem Doppelpunkt, die den Pfaden innerhalb des Dateisystems vorangestellt werden. Jeder Datenträger bekommt seinen eigenen Buchstaben, zum Beispiel meist C: für die erste Partition der ersten Festplatte. Bei "Unix" gibt es keine Laufwerksbuchstaben, sondern nur einen einzigen Verzeichnisbaum. Die einzelnen Datenträger werden dort an bestimmten Stellen im Baum "eingehängt" (Kommando mount), so dass alle Datenträger zusammen den Gesamtbaum ergeben. Windows-Varianten, die auf Windows NT basieren, arbeiten intern ebenfalls mit einem solchen Baum, dieser Baum wird aber gegenüber dem Anwender verborgen.

Unter "AmigaOS" erfolgt eine Mischung der Ansätze von DOS und Unix. Die nach Unix-Nomenklatur bezeichneten Laufwerke werden mit Doppelpunkt angesprochen (df0:, hda1:, sda2:). Darüber hinaus können "logische" Doppelpunkt-Laufwerksbezeichnungen wie codice_21 per codice_22 unabhängig vom "physischen" Datenträger vergeben werden.

Die Verzeichnispfade von "OpenVMS" unterscheiden sich stark von Unix-, DOS- und Windows-Pfaden. Zuerst nennt OpenVMS die Geräteart, z. B. bezeichnet „codice_23“ einen lokalen Datenträger. Der Laufwerksname (bis zu 255 Zeichen lang) wird angefügt und mit einem Doppelpunkt abgeschlossen. Der Verzeichnis-Teil wird in eckige Klammern gesetzt. Die Unterverzeichnisse werden durch Punkte getrennt, z. B. „codice_24“. Am Ende des Pfads folgt der Dateiname, beispielsweise „codice_25“. Dessen erster Teil ist ein sprechender Name und bis zu 39 Zeichen lang. Nach einem Punkt folgt der dreistellige Dateityp, ähnlich wie bei Windows. Am Ende wird die Version der Datei, getrennt durch ein Semikolon „;“, angefügt.

Häufig bezeichnet der Begriff Dateisystem nicht nur die Struktur und die Art, wie die Daten auf einem Datenträger organisiert werden, sondern allgemein den ganzen Baum mit mehreren verschiedenen Dateisystemen (Festplatte, CD-ROM, …). Korrekterweise müsste man hier von einem "Namensraum" sprechen, der von verschiedenen Teilnamensräumen (den Dateisystemen der eingebundenen Datenträger) gebildet wird, da aber dieser Namensraum sehr dateibezogen ist, wird häufig nur vom Dateisystem gesprochen.

Die Systemaufrufe wie "open", "read" usw. können auch über ein Netzwerk an einen Server übertragen werden. Dieser führt dann die Zugriffe auf seine Massenspeicher durch und liefert die angeforderte Information an den Client zurück.

Da dieselben Systemaufrufe verwendet werden, unterscheiden sich die Zugriffe aus Programm- und Anwendersicht nicht von der auf die lokalen Geräte. Man spricht hier von "transparenten" Zugriffen, weil der Anwender die Umlenkung auf den anderen Rechner nicht sieht, sondern scheinbar unmittelbar auf die Platte des entfernten Rechners schaut – wie durch eine transparente Glasscheibe. Für Netzwerkdateisysteme stehen spezielle Netzwerkprotokolle zur Verfügung.

Kann auf ein Dateisystem etwa in einem Storage Area Network (SAN) von mehreren Systemen parallel direkt zugegriffen werden, spricht man von einem Globalen- oder Cluster-Dateisystem. Dabei sind zusätzliche Maßnahmen zu ergreifen, um Datenverlust () durch gegenseitiges Überschreiben zu vermeiden. Dazu wird ein Metadaten-Server eingesetzt. Alle Systeme leiten die Metadaten-Zugriffe – typischerweise über ein LAN – an den Metadaten-Server weiter, der diese Operationen wie Verzeichniszugriffe und Block- beziehungsweise Clusterzuweisungen vornimmt. Der eigentliche Datenzugriff erfolgt dann über das SAN, als ob das Dateisystem lokal angeschlossen wäre. Da der Zusatzaufwand (engl. "") durch die Übertragung an den Metadaten-Server insbesondere bei großen Dateien kaum ins Gewicht fällt, kann eine Übertragungsgeschwindigkeit ähnlich der eines direkt angeschlossenen Dateisystems realisiert werden.

Eine Besonderheit stellt das WebDAV-Protokoll dar, das Dateisystem-Zugriffe auf entfernt liegende Dateien via HTTP ermöglicht.

Das "open"-"read"-Modell lässt sich auch auf Geräte und Objekte anwenden, die normalerweise nicht über Dateisysteme angesprochen werden. Dadurch wird der Zugriff auf diese Objekte identisch mit dem Zugriff auf normale Dateien, was meist Vorteile bringt.

Unter den derzeitigen Linux-Kernels (u. a. Version 2.6) lassen sich System- und Prozessinformation über das virtuelle "proc"-Dateisystem abfragen und ändern. Die virtuelle Datei "/proc/cpuinfo" liefert zum Beispiel Informationen über den Prozessor. Unter Linux gibt es einige solcher Pseudo-Dateisysteme. Dazu zählen "sysfs", "usbfs" oder "devpts"; unter einigen BSDs gibt es ein "kernfs". All diese Dateisysteme enthalten nur rein virtuell vorhandene Dateien mit Informationen oder Geräten, die auf eine „Datei“ abgebildet werden.

Der Kernel gaukelt hier quasi die Existenz einer Datei vor, wie sie auch auf einem Massenspeicher vorhanden sein könnte.

Dateien in "ramfs" oder "tmpfs" und ähnlichen Dateisystemen existieren demgegenüber tatsächlich, werden aber nur im Arbeitsspeicher gehalten. Sie werden aus Geschwindigkeitsgründen und aus logisch-technischen Gründen während der Boot-Phase eingesetzt.

Neben Linux gibt es auch für diverse andere Betriebssysteme sogenannte RAM-Disks, mit denen ein komplettes virtuelles Laufwerk im Arbeitsspeicher realisiert wird, vor allem aus Geschwindigkeitsgründen.

Viele moderne Dateisysteme haben das Prinzip der Datei verallgemeinert, so dass man in einer Datei nicht nur eine Folge von Bytes, einen sogenannten "" (engl. Strom), sondern mehrere solcher Folgen (alternative Datenströme) abspeichern kann. Dadurch ist es möglich, Teile einer Datei zu bearbeiten, ohne eventuell vorhandene andere Teile, die sehr groß sein können, verschieben zu müssen.

Problematisch ist die mangelnde Unterstützung von multiplen Streams. Das äußert sich zum einen darin, dass alternative Daten beim Transfer auf andere Dateisysteme (ISO 9660, FAT, ext2) ohne Warnung verloren gehen, zum anderen darin, dass kaum ein Werkzeug diese unterstützt, weshalb man die dort gespeicherten Daten nicht ohne Weiteres einsehen kann und beispielsweise Virenscanner dort abgespeicherte Viren übersehen.

Aus der Tatsache, dass der Hauptdatenstrom von Änderungen an den anderen Strömen nicht berührt wird, ergeben sich Vorteile für die Performance, den Platzbedarf und die Datensicherheit.

Unter Inode-basierten Dateisystemen sind Sparse-Dateien, Hardlinks und symbolische Verknüpfungen möglich. Auch technisch anders aufgebaute Dateisysteme kennen neuerdings zum Teil diese Eigenschaften.

Für Massenspeicher wie CD-ROM oder DVD gibt es eigene Dateisysteme, die Betriebssystem-übergreifend Anwendung finden, vor allem ISO 9660, weitere siehe unten bei Sonstige.

Dateisysteme aus dem Unix-Bereich kennen besondere "Gerätedateien". Deren Namen sind dabei oft per Übereinkommen festgelegt, sie können nach Belieben umbenannt werden; so haben zum Beispiel auch die Tastatur, Maus und andere Schnittstellen spezielle Dateinamen, auf die mit "open", "read", "write" zugegriffen werden kann, sogar der Hauptspeicher hat einen Dateinamen ("/dev/mem"). (Die Unix-Philosophie dazu lautet: „Alles ist eine Datei, und wenn nicht, sollte es eine Datei sein.“)

In anderen Betriebssystemen (wie unter MS-DOS ab Version 2.0) gibt es Gerätenamen mit besonderen Bedeutungen, die dem System der Gerätedateien ähnlich sind: codice_26, codice_27, codice_28, codice_29 und andere. Diese Geräte können analog zu einer Datei geöffnet und über eine Zugriffsnummer "(Handle)" gelesen und beschrieben werden. Sie haben aber verständlicherweise keinen Dateizeiger. Im Unterschied zu den Blockgeräten (auch „Laufwerke“ genannt: codice_30, codice_31, codice_32 usw.) "enthalten" sie keine Dateien, sondern verhalten sich selbst – mit gewissen Einschränkungen – wie Dateien. Diese Pseudodateien existieren seit DOS 2, das stark von UNIX beeinflusst wurde. Unter Berücksichtigung der DOS-Gerätetreiberspezifikation ist es dem Benutzer möglich, eigene Gerätetreiber zu schreiben, sie per DEVICE-Befehl zu laden und über ebensolche Pseudodateinamen anzusprechen. Diese besonderen Dateinamen waren in der Vergangenheit öfters Anlass von Sicherheitsproblemen, da die entsprechenden Namen zum Teil einigen Applikationen nicht bekannt waren und daher nicht herausgefiltert wurden, aber zum Teil auch weil der Zugriffsschutz auf die damit assoziierten Geräte unzureichend geregelt war.

Darüber hinaus existieren Dateisysteme, die mehrere darunterliegende Speichermedien („“) überspannen können, die eine Versionierung von Dateien schon inhärent ermöglichen (VMS), deren Größe zur Laufzeit geändert werden kann (AIX).

Manche Dateisysteme bieten Verschlüsselungsfunktionen an, Umfang und Sicherheit der Funktionen variieren dabei.

Diese werden häufig fälschlicherweise als Datenbankdateisysteme oder SQL-Dateisysteme bezeichnet, hierbei handelt es sich eigentlich nicht um Dateisysteme, sondern um Informationen eines Dateisystems, die in aufgewerteter Form in einer Datenbank gespeichert und in, für den Anwender intuitiver Form, über das virtuelle Dateisystem des Betriebssystems dargestellt werden.

Hauptartikel: assoziative Dateiverwaltung

Das Dateisystem darf von sich aus keine Daten verlieren oder ungewollt überschreiben. Insbesondere zwei Fälle bringen Gefahren mit sich:

Wenn im Multitasking mehrere Aufgaben gleichzeitig anstehen, muss das Dateisystem die einzelnen Aktionen sauber auseinanderhalten, damit nichts durcheinanderkommt. Wenn die Aufgaben auch noch dieselbe Datei ansprechen, sei es nur lesend oder auch schreibend, werden typischerweise entsprechende Sperrmechanismen (') zur Verfügung gestellt oder automatisch in Kraft gesetzt, um Konflikte zu vermeiden. Gleichzeitige Zugriffe von mehreren Seiten z. B. auf eine große Datenbankdatei sind aber auch der Normalfall, so dass man neben globalen Sperren, die die ganze Datei betreffen, auch solche nur für einzelne Datensätze (') benutzen kann.

Wenn ein Laufwerk gerade auf ein Speichermedium schreibt und die Betriebsspannung in diesem Moment ausfällt, dann besteht die Gefahr, dass nicht nur die eigentlichen Daten unvollständig geschrieben werden, sondern dass vor allem die organisatorischen Einträge im Verzeichnis nicht mehr korrekt aktualisiert werden. Um diese Gefahr zumindest möglichst klein zu halten, wird einerseits per Hardware versucht, genug Energiepuffer (Kondensatoren in der Versorgung) bereitzuhalten, so dass ein Arbeitsvorgang noch zu Ende geführt werden kann, andererseits ist die Software so ausgelegt, dass die Arbeitsschritte möglichst „atomar“ ausgelegt sind, das heißt die empfindliche Zeitspanne mit unvollständigen Dateneinträgen so kurz wie möglich gehalten wird. Wenn dies im Extremfall dann doch nicht hilft, gibt es als neuere Entwicklung sogenannte Journaling-Dateisysteme, die in einem zusätzlichen Bereich des Speichermediums Buch über jeden Arbeitsschritt führen, so dass im Nachhinein rekonstruiert werden kann, was noch erledigt werden konnte und was nicht mehr.

Eigene Gesichtspunkte gibt es bei Flash-Speichern, indem diese beim Löschen und Wiederbeschreiben einem Verschleiß ausgesetzt sind, der je nach Typ nur ca. 100.000 bis 1.000.000 Schreibzyklen zulässt. Dabei können in der Regel nicht einzelne Bytes für sich gelöscht werden, sondern meist nur ganze Blöcke (von je nach Modell variierender Größe) auf einmal. Das Dateisystem kann hier daraufhin optimiert werden, dass es die Schreibvorgänge möglichst gleichmäßig über den gesamten Speicherbereich des Flash-Bausteins verteilt und beispielsweise nicht einfach immer bei Adresse 0 anfängt zu schreiben. Stichwort: "Wear-Leveling-Algorithmen".

Dem Aspekt der Datensicherheit gegenüber Ausspähung durch Unberechtigte dienen Dateisysteme, die alle Daten verschlüsseln können, ohne dass andere Schichten des Betriebssystems dafür Aufwand zu treiben bräuchten.

Eine weitere Gefahrenquelle für die Integrität der Daten besteht in Schreibaktionen, die von irgendwelcher Software unter Umgehung des Dateisystems direkt auf physische Adressen auf dem Speichermedium erfolgen. Bei älteren Betriebssystemen war das ohne weiteres möglich und führte zu entsprechend häufigen Datenverlusten. Neuere Betriebssysteme können diese tieferen Ebenen wesentlich effektiver vor unautorisiertem Zugriff schützen, so dass mit den Rechten eines Normalbenutzers gar kein direkter Zugriff auf physische Medienadressen mehr erlaubt ist. Wenn bestimmte Diagnose- oder Reparatur-Dienstprogramme ("") so einen Zugriff doch benötigen, müssen sie mit Administratorrechten ausgestattet sein.

Bei der Migration von Dateibeständen, etwa auf Grund einer Systemablösung, müssen häufig Dateien von einem Dateisystem auf ein anderes übernommen werden. Das ist im Allgemeinen ein schwieriges Unterfangen, denn viele Dateisysteme sind untereinander funktional nicht kompatibel, d. h. das Zieldateisystem kann nicht alle Dateien mit allen Attributen aufnehmen, die auf dem Quelldateisystem gespeichert sind. Ein Beispiel hierfür wäre die Migration von NTFS-Dateien mit Alternate Data Streams auf ein Dateisystem ohne Unterstützung für solche Streams.



Linux:


</doc>
<doc id="1182" url="https://de.wikipedia.org/wiki?curid=1182" title="Demokratische Republik Kongo">
Demokratische Republik Kongo

Die Demokratische Republik Kongo ( [], [], abgekürzt DR Kongo), von 1971 bis 1997 Zaire (frz. "Zaïre"), auch bekannt als Kongo-Kinshasa oder einfach der Kongo, ist eine Republik in Zentralafrika. Sie grenzt (von Norden im Uhrzeigersinn) an die Zentralafrikanische Republik, den Südsudan, Uganda, Ruanda, Burundi, Tansania, Sambia, Angola, den Atlantik und die Republik Kongo. Die DR Kongo ist an Fläche der zweitgrößte und an Bevölkerung der viertgrößte Staat Afrikas. Das Land wird vom Äquator durchzogen; es herrscht ein tropisches Klima. Große Teile des Staatsgebietes sind von tropischem Regenwald bedeckt.

Die etwa 80 Millionen Einwohner lassen sich in mehr als 200 Ethnien einteilen. Es existiert eine große Sprachvielfalt, die Verkehrssprache ist Französisch. Etwa die Hälfte der Einwohner bekennt sich zur katholischen Kirche, die andere Hälfte verteilt sich auf Kimbanguisten, andere christliche Kirchen, traditionelle Religionen und den Islam. Die Hauptstadt Kinshasa gilt mit über 11 Millionen Einwohnern als drittgrößte Stadt Afrikas, Städte mit über einer Million Einwohnern sind Lubumbashi, Mbuji-Mayi, Kananga und Kisangani.

Das Gebiet des heutigen Staates kam 1885 unter belgische Kolonialherrschaft. Die Herrschaft des belgischen Königs Leopold II. gilt als eines der grausamsten Kolonialregime. Nach der Unabhängigkeit 1960 wurde das Land nach mehrjährigen innenpolitischen Konflikten 32 Jahre lang von Mobutu Sese Seko diktatorisch regiert. 1997 wurde Mobutu von dem Rebellenchef Laurent-Désiré Kabila gestürzt. Auf den Machtwechsel folgte ein weiterer Bürgerkrieg, der aufgrund der Verwicklung zahlreicher afrikanischer Staaten als "Afrikanischer Weltkrieg" bekannt wurde. 2002 wurde ein Friedensabkommen unterzeichnet, im Osten des Landes finden aber bis heute weiterhin Kämpfe statt. Erstmals seit 1965 fanden 2006 freie Wahlen statt. Beim Demokratieindex von 2014 nimmt das Land jedoch nur Platz 162 von 167 ein.

Trotz seines Rohstoffreichtums zählt der Staat, bedingt durch jahrzehntelange Ausbeutung, Korruption, jahrelange Kriege und ständige Bevölkerungszunahme, heute zu den ärmsten Ländern der Welt. In der Reihung gemäß dem Index der menschlichen Entwicklung der Vereinten Nationen nahm die Demokratische Republik Kongo im Jahr 2013 den vorletzten Platz ein, verbesserte ihren Indexwert jedoch seither.

Der Name der Demokratischen Republik Kongo änderte sich in der Vergangenheit mehrfach, zeitweise verwendete das Land die gleiche amtliche Bezeichnung wie der Nachbarstaat Republik Kongo. Die folgende Tabelle gibt einen Überblick über die historischen Bezeichnungen:

Das Gebiet der Demokratischen Republik Kongo umfasst als zweitgrößter Staat Afrikas 2.344.885 km² und ist somit 6,6-mal so groß wie Deutschland und 76,9-mal so groß wie die Fläche der ehemaligen Kolonialmacht Belgien.

Es liegt in Zentralafrika am Äquator. Sowohl Flora als auch Fauna sind sehr vielfältig, damit besitzt es ein sehr hohes naturräumliches Potenzial. Deshalb ist die Meinung vieler Experten, dass die Demokratische Republik Kongo heute einer der führenden afrikanischen Staaten wäre, hätte es keine Kolonialausbeutung und ethnischen Konflikte gegeben.

Rund 60 Prozent des Landes nimmt das Kongobecken mit seinen tropischen Regenwäldern ein. Es ist in allen Richtungen von Bergzügen von 500 bis 1000 Meter Höhe begrenzt. Im Süden wird es vom Shaba- oder Katanga-Bergland begrenzt, das Teil der Lundaschwelle ist. Im Süden und Osten des Landes steigen die Bergzüge zu Hochgebirgen auf, wie die Mitumba-Berge und die Kundelungu-Berge im Süden und die Zentralafrikanische Schwelle und Virunga-Vulkane im Osten. Sie erreichen Höhen von bis zu 4500 Meter und sind reich an Bodenschätzen wie Kupfer und Uran. Die höchste Erhebung ist mit 5109 Meter der Margherita Peak und befindet sich im Ruwenzori-Gebirge an der Grenze zu Uganda.

Der größte und längste Fluss, der durch die Demokratische Republik Kongo fließt, ist der Kongo mit 4374 Kilometern Länge. Er ist nach dem Nil der zweitlängste Fluss des afrikanischen Kontinents. Gemessen an seiner Wasserführung von 39.160 m³/s ist er sogar der größte Fluss Afrikas und der zweitgrößte Fluss weltweit. Der Kongo entspringt im Süden im Mitumbagebirge und fließt etwa 1000 Kilometer nach Norden, von wo er nach Westsüdwesten umgelenkt wird. Hier besteht auch ein Binnendelta. Anschließend bildet er die Grenze zwischen der Demokratischen Republik Kongo und der Republik Kongo, bevor er in den Atlantik mündet. Es gibt zahlreiche Flüsse, die im Kongo münden. Der mit einer Wasserführung von 9.873 m³/s bei weitem größte dieser Nebenflüsse ist der aus Angola kommende Kasai, der ebenfalls mehrere Nebenflüsse aufweist und in den Gebirgen im Süden entspringt. Dies trifft ebenfalls auf den Lomami zu, er hat sein Quellgebiet in der ehemaligen Provinz Katanga. Der größte von Norden kommende Zufluss des Kongos ist der Ubangi, der nahezu über seine gesamte Länge die Grenze zur Zentralafrikanischen Republik und zur Republik Kongo bildet. Die 40 Kilometer lange Küste nördlich der Kongomündung in den Ozean stellt die einzige Öffnung zum Atlantischen Ozean dar. Hier liegen die beiden Hafenstädte Muanda und Banana. An diesem nur 40 km langen Küstenstreifen befinden sich unter anderem Erdölvorkommen. Im Osten des Landes befindet sich die Seenkette des Großen Afrikanischen Grabens, die die Ostgrenze bildet. Dazu gehören unter anderem (von Nord nach Süd) der Albertsee, Eduardsee, Kiwusee und Tanganjikasee. Sie birgt darüber hinaus bedeutende Bodenschätze. Hier wurde beispielsweise Erdgas gefunden, im Osten und Nordosten auch Gold und Zinn.

Die Oxisolböden im Kongobecken sind oft stark verwittert und weisen nur geringe Fruchtbarkeit auf, während die höher gelegenen Gebiete im Norden und Süden fruchtbar sind und zum Ackerbau genutzt werden.

In der Demokratischen Republik Kongo herrscht aufgrund der geographischen Lage ein Äquatorialklima vor. In den meisten Landesteilen gibt es daher ein sehr warmes, tropisches Feuchtklima mit einer Durchschnittstemperatur von rund 20 °C in der Trockenzeit und rund 30 °C in der Regenzeit. Das Klima wird relativ wenig durch Jahreszeiten wie Trocken- und Regenzeit beeinflusst. Dennoch gibt es wegen der sehr großen Landesfläche regionale Disparitäten.

Durch die nördliche Landesmitte, in welcher die Städte Mbandaka und Kisangani liegen, verläuft der Äquator. In diesem rund 300 Kilometer breiten Gebiet gibt es das ganze Jahr über heftige Regenfälle, die durchschnittlich rund 1500–2000 mm betragen, während die Temperatur konstant bei rund 26 °C bleibt.

Kinshasas Klima ist gekennzeichnet durch eine Jahresdurchschnittstemperatur von über 25 °C sowie einer Wechselfolge zwischen den Trockenzeiten (vier Monate insgesamt) und den Regenzeiten, welche ihre extremste Ausprägung in den Monaten November und April haben. Im ganzen Jahr fallen in Kinshasa insgesamt rund 1400 mm Regen.

Im Norden des Landes lässt der große Waldflächenanteil, der typisch für das Äquatorialklima ist, Platz für eine Baumsavanne. Dort beginnt die Trockenzeit, gegensätzlich zum Süden, meist zwei bis drei Monate vor dem Jahreswechsel und endet rund zwei bis drei Monate nach dem Jahreswechsel. Deshalb fallen hier rund 90 % der Jahresniederschläge in der Zeit zwischen März und November.

Im Süden beginnt eine Zone des tropischen Klimas, die mit einer Trockenzeit (drei bis sechs Monate, meist Mai bis September) und einer Regenzeit (sechs bis neun Monate, meist Oktober bis April) ausgeprägte Jahreszeiten aufweist. So gibt es zum Beispiel in Lubumbashi in der Provinz Katanga sogar sechs Monate relativer Trockenheit und sehr ausgeprägte Tages-Nacht-Temperaturschwankungen.

Der gebirgige Ostteil der Demokratischen Republik Kongo ist von Höhenklima geprägt und deutlich kühler im Vergleich zu den anderen Gebieten. Da die Temperatur dort pro 80 Höhenmeter um durchschnittlich 1 °C sinkt, kann man namhafte klimatische und ökologische Unterschiede beim Anstieg der Gebirge im Nationalpark Virunga und der Gefälle des Ruwenzori-Gebirges feststellen. An den höchsten Punkten dieser Gebiete ist sogar Schneefall nicht ungewöhnlich. Hier fallen auch die meisten Niederschläge des Landes.

Es gibt auch eine kleine Zone maritimen Klimas. Im kleinen Küstengebiet im äußersten Westen, wo auch der Kongo-Fluss mündet, senkt der kalte Benguelastrom Temperatur und Niederschlagsmenge deutlich ab, sodass es beispielsweise in der Stadt Boma im Jahr durchschnittlich weniger als 800 mm Niederschlag gibt.

In der Demokratischen Republik Kongo liegen die größten noch existierenden Regenwaldgebiete Afrikas. Rund zwei Drittel der Landesfläche sind von tropischem Regen- und Höhenwald bedeckt. In höheren Lagen in Äquatornähe gibt es vor allem Bergregenwald und Nebelwald. Hier findet man vorrangig Bäume und Pflanzen mit langen Stämmen, dünner Baumrinde und festen Blättern. Beispiele hierfür sind der Gummibaum und Hartholzpflanzen wie der Teakbaum und Mahagonigewächse. Außerdem gibt es dort Ölpalmen, Würgefeigen und Aufsitzerpflanzen wie Orchideen. Nördlich und südlich der Regenwaldregion befinden sich 200 und 500 Kilometer breite Streifen mit Feuchtsavanne. Diese Verteilung ist niederschlagsbedingt und gründet sich auf die innertropische Konvergenzzone (ITC). Eine im Feuchtsavannengebiet vorkommende Pflanzengattung ist die Wolfsmilch. Die Feuchtsavanne geht schließlich in die Trockensavanne mit Miombowaldgebieten über. Die typische Vegetation besteht dort hauptsächlich aus Akazien und Sukkulenten.

Obwohl einige Säugetiere wie der Löwe, der Leopard, das Nashorn, der Elefant, das Zebra, der Schakal, die Hyäne sowie eine Reihe von Antilopenarten bevorzugt in den Savannenregionen leben, gibt es vor allem durch den hohen Waldanteil sehr viele verschiedene Säugetierarten – insgesamt 415 – im Land. Insbesondere die fünf als UNESCO-Welterbe ausgezeichneten Gebiete Garamba, Kahuzi-Biéga, Salonga, Virunga (Nationalparks) und das Okapi-Wildtierreservat stellen einen wichtigen Lebensraum für viele Säugetiere wie Bonobos, Östliche Gorillas, Okapis und Afrikanische Büffel dar. Besonders die Vielfalt an Menschenaffen ist bemerkenswert: Die Demokratische Republik Kongo beherbergt als weltweit einziges Land drei Menschenaffenarten: neben Gorillas und Bonobos auch Schimpansen. Diese sind dort allerdings kaum noch aufzufinden und akut vom Aussterben bedroht. Auch die Lage der Bonobos ist bedrohlich: Der Bestand der Tierart, welche man nirgendwo sonst weltweit in Freiheit beobachten kann, wird derzeit auf rund 3000 Tiere im Staatsgebiet geschätzt. Vor den 1980er Jahren lag diese Zahl bei über 100.000. Hauptgrund des Aussterbens der Menschenaffen sind Wilderer, die das Bushmeat als Delikatesse in den Städten verkaufen.

Auch bei anderen Säugetieren besteht das Problem des Artensterbens. Doch im Gegensatz zu dem Affenfleisch werden andere bedrohte Tierarten vor allem aufgrund des immensen Proteinbedarfs der wegen der sehr hohen Fertilitätsrate der Frauen schnell wachsenden Bevölkerung gejagt. Oftmals ist das Jagen geschützter Tierarten für Landesbewohner überlebenswichtig. Doch auf diese Weise schrumpften die Bestände mancher Wildtierarten so sehr, dass einige Arten laut Forschungsprognosen schon in rund 50 Jahren ausgestorben sein könnten.

Auch die anderen Tierklassen lassen sich in großer Zahl finden, es gibt 268 verschiedene Reptilien und je über tausend Fisch- und Vogelarten. Sehr groß ist auch die Anzahl von Insekten, so gibt es alleine über 1300 verschiedene Arten von Schmetterlingen. In keinem anderen Land weltweit ist dieser Wert größer.

Die Urbevölkerung des heutigen Staates bestand aus Pygmäen, welche heute nur noch eine kleine Minderheit darstellen. Über Jahrhunderte hinweg wanderten Bantuvölker ein, es entstanden verschiedene Gesellschaftsformen, von Jägern und Sammlern über Ackerbaubevölkerungen bis hin zu größeren Staatswesen.

Unter den dortigen Staaten trat insbesondere das im 14. Jahrhundert gegründete Königreich Kongo, eines der größten afrikanischen Staatswesen überhaupt, hervor.
Im 15. Jahrhundert erkundeten portugiesische Seefahrer das Gebiet der Kongomündung und nahmen 1491 diplomatische Beziehungen zum Kongoreich auf. Der König besuchte Portugal und trat zum Katholizismus über, und es begann eine kurze Phase annähernd gleichberechtigten Umgangs zwischen dem Kongo und Portugal. In der Neuzeit lieferte das Kongoreich Sklaven in die amerikanischen Kolonien, die Einnahmen aus dem Sklavenhandel ließen eine reiche Oberschicht in den afrikanischen Hafenstädten entstehen.

Vom 16. Jahrhundert an war das Kongoreich im Niedergang begriffen. Bis zum Ende des 17. Jahrhunderts erfolgte die völlige Zerstörung des Königreiches sowie seine Ausbeutung und Plünderung durch Sklavenjäger, die nach dem Zerfall der portugiesischen Vorherrschaft durch Niederländer und Briten fortgeführt wurde. Am Anfang des 18. Jahrhunderts war das Kongoreich fast vollständig zerfallen. 1866 zogen die letzten Portugiesen ab.

In den 1870er Jahren bereiste der Waliser Henry Morton Stanley als erster Europäer das Hinterland, sein Vorschlag, den Kongo dem britischen Kolonialreich anzugliedern, wurde von der britischen Regierung, die sich vor allem für die Nilquellen interessierte, aber abgelehnt.

Der belgische König Leopold II. jedoch, von dem Gedanken an ein Kolonialreich seit langem fasziniert, wollte die Gelegenheit nutzen. 1885 vereinnahmte Leopold den Kongo im Nachgefolge der Kongokonferenz als seinen „Privatbesitz“. Nominell war der neu geschaffene Staat vollständig selbstständig gegenüber der Kolonialmacht Belgien. Der Kongo-Freistaat besaß eine eigene Regierung in Boma, die nur Leopold Rechenschaft abzulegen hatte, eine eigene Armee (die „Force Publique“), sowie eigene diplomatische Vertretungen in anderen Staaten. Die einheimische Bevölkerung war von den politischen und militärischen Eliten des Staates ausgeschlossen. Dieser Status jenseits allen Völkerrechts war in der ganzen Kolonialgeschichte einzigartig. Da mit dem Kongo zugleich auch alle seine Bewohner als rechtloser Privatbesitz angesehen wurden, kam es bei der wirtschaftlichen Ausbeutung (Kautschukboom) zu solch grausamen Exzessen, dass sie als so genannte Kongogräuel 1908 international für Aufsehen und Empörung sorgten und Leopold zur Übergabe des Kongo als „normale“ Kolonie an den belgischen Staat zwangen.

Zwar verbesserten sich die Verhältnisse nun ein wenig, aber nach wie vor wurden der Kongo und seine Bevölkerung von der autoritären Kolonialmacht Belgien ausgebeutet. Mit den weltweit in den Kolonien zunehmenden Unabhängigkeitsbestrebungen wuchs auch im Kongo der Druck nach staatlicher Selbstbestimmung. Nach ersten Unruhen in der Hauptstadt Léopoldville und unter dem Druck der Weltöffentlichkeit zog sich Belgien Anfang 1959 schlagartig aus dem Kongo zurück und hinterließ ein Chaos.

Am 30. Juni 1960 wurde die „Republik Kongo“ unabhängig. Joseph Kasavubu, Führer der Alliance des Bakongo (ABAKO), wurde Staatspräsident. Der bedeutende Panafrikanist und Führer der kongolesischen Unabhängigkeitsbewegung Patrice Lumumba wurde der erste Ministerpräsident des jungen Landes, das er allerdings aufgrund mangelnder Fachkräfte und angesichts sezessionistischer Bestrebungen, insbesondere in der Provinz Katanga, nicht zusammenzuhalten vermochte. Insbesondere die kontinuierlichen Interventionen Belgiens, der USA, aber auch der Sowjetunion führten zu einem allmählichen Zerreißen der jungen Nation. Lumumba wurde schließlich vom Militär abgesetzt und verhaftet. Zwar konnte er der Haft kurz entfliehen, wurde aber kurze Zeit später wieder ergriffen, seinem Gegner Moïse Tschombé – dem Sezessionistenführer in Katanga – ausgeliefert und anschließend ermordet. Eine Beteiligung der CIA sowie des belgischen Geheimdienstes wurde im Jahr 2000 bestätigt, weswegen die belgische Generalstaatsanwaltschaft 2012 ein Ermittlungsverfahren eröffnete.

1965 putschte der frühere Assistent Lumumbas, Joseph Mobutu, und errichtete in den folgenden Jahrzehnten eine der längsten und korruptesten kleptokratischen Diktaturen Afrikas. Moïse Tschombé konnte zeitweise über Teile des Kongos mit einer Söldnerarmee, die überwiegend aus Europäern bestand, herrschen. Ein Höhepunkt der Söldneraktivität im Kongo stellt die Besetzung von Bukavu durch Tschombés europäische Söldner von August bis November 1967 dar. Mobutu begann eine Afrikanisierung des Landes und versuchte, die europäischen Einflüsse im Land zu eliminieren. Europäische Unternehmen wurden verstaatlicht, 1971 das Land in "Zaire" umbenannt. Mobutu errichtete einen Einparteienstaat mit einem bizarren Personenkult, der erhebliche Unterstützung aus westlichen Ländern erhielt, und bekämpfte dafür den Einfluss der Sowjetunion in Afrika. 1977/78 wurde mit internationaler, unter anderem belgischer und französischer Militärhilfe für die Regierung Mobutu Sese Seko die Shaba-Invasion der Front national de libération du Congo des Rebellenführers Nathaniel Mbumba aus Angola niedergeschlagen (Schlacht um Kolwezi).

Unter dem Eindruck des Niedergangs der zairischen Wirtschaft und dem Ende des Ost-West-Konflikts stimmte Mobutu ab 1990 einer schrittweisen Demokratisierung des Landes zu, die aber zu keinem Erfolg führte. Das Ende der Diktatur Mobutus begann stattdessen mit dem Völkermord in Ruanda, in dessen Folge Hunderttausende der am Völkermord beteiligten Hutu nach Zaire flohen. Einer Allianz der neuen ruandischen Tutsi-Regierung und verschiedener Mobutu-Gegner gelang es schließlich innerhalb weniger Monate, ganz Zaire zu erobern und den schwer kranken und international mittlerweile isolierten Mobutu zu stürzen. Der Rebellenchef Laurent-Désiré Kabila wurde 1997 neuer Präsident und benannte Zaire wieder in "Demokratische Republik Kongo" um.
Die einstigen Verbündeten hatten sich rasch zerstritten, und 1998 versuchten erneut von Ruanda gestützte Rebellenorganisationen von Osten aus das Land zu erobern. Eine Intervention von Angola und Simbabwe auf Seiten Kabilas konnte den Sturz der Regierung aber abwenden, und es entwickelte sich ein jahrelanger Stellungskrieg; das Land wurde schließlich in mehrere Machtbereiche aufgespalten. Langwierige Verhandlungen beendeten 2003 den Krieg, alle Kriegsparteien bildeten eine gemeinsame Übergangsregierung.

Der Kongokrieg hatte schwerwiegende sozioökonomische Auswirkungen auf das Land. Wirtschaft und Sozialsysteme, die bereits vor dem Krieg am Boden lagen, brachen völlig zusammen, ganze Landstriche wurden weitgehend entvölkert. Die Zahl der Opfer ist unbekannt, Hochrechnungen gehen von mehr als drei Millionen indirekter Kriegsopfer aus.

Bereits im Januar 2001 fiel Laurent-Désiré Kabila einem Attentat zum Opfer, und sein Sohn Joseph Kabila erbte seine Stellung als Staatspräsident. Joseph Kabila gewann schließlich die im Friedensvertrag vorgesehene Wahl im Jahre 2006 und ist damit erster frei gewählter Präsident der Demokratischen Republik Kongo seit 1965. Mit Kabila führt zum ersten Mal seit 1960 ein Mann den Staat, der zu Gesprächen zur Befriedung und Stabilisierung der Region bereit ist. Ihm im Wege steht dabei allerdings der fast vollständige Zerfall der Infrastruktur, Verwaltung und Wirtschaft des Landes und insbesondere die Ausplünderung der äußerst rohstoffreichen Ostprovinzen des Kongo, in denen die Zentralregierung fast völlig machtlos ist, durch Uganda, Ruanda und verschiedene lokale Machthaber.

In den Gebieten Kivu und Ituri im Osten findet auch nach Ende des zweiten Kongokrieges weiterhin ein bewaffneter Konflikt statt, weil die dortigen lokalen Milizen nicht an den Friedensverhandlungen beteiligt waren.

Zwischen August 2007 und Januar 2009 eskalierte der Konflikt: Im dritten Kongokrieg kämpften in Nordkivu die kongolesischen Streitkräfte, UN-Truppen der MONUC und Mai-Mai-Milizen gegen die Rebellen des Nationalkongress zur Verteidigung des Volkes (CNDP) unter der Führung des Tutsi Laurent Nkunda, eines ehemaligen Generals der Rebellenorganisation RCD. Nkunda behauptete, die lokale Tutsi-Bevölkerung gegen die Hutu-Extremisten der Demokratischen Kräfte zur Befreiung Ruandas (FDLR) zu verteidigen, die auf kongolesischem Gebiet operieren und von Nkunda der Zusammenarbeit mit der kongolesischen Regierung bezichtigt werden.

Ende 2008 eroberte die CNDP immer größere Gebiete im Nordkivu, Verhandlungen zwischen Regierung und Rebellen unter Vermittlung der UN blieben erfolglos. Im Dezember 2008 schlossen die kongolesische Regierung und Ruanda ein Abkommen über eine gemeinsame Bekämpfung der FDLR. Ruandische Soldaten marschierten in den Kongo ein und verhafteten Nkunda, der wenige Tage zuvor von der CNDP für abgesetzt erklärt worden war. Im März 2009 unterzeichneten Regierung und CNDP ein Friedensabkommen. Hoffnungen, dass nach dem Ende der CNDP und der Zusammenarbeit zwischen kongolesischer Regierung und Ruanda im Kampf gegen die FDLR nun auch eine Befriedung der Ostprovinzen möglich sei, erfüllten sich nicht.

Ab etwa 2010 operierten Dutzende bewaffneter Gruppierungen in den Kivuprovinzen. Deren Stärken reichen von wenigen Dutzend bis zu mehreren tausend Kämpfern. An vielen Gruppen sind desertierte Soldaten der FARDC oder andere Sicherheitskräfte beteiligt. Zu den größten zählen die FDLR, die Raïa Mutomboki, die "Alliance des patriotes pour un Congo libre et souverain" und die Nyatura. Die Bewegung 23. März wurde im April 2012 von ehemaligen Mitgliedern der CNDP aus Unzufriedenheit über die Umsetzung des Friedensabkommens gegründet. Sie erlangte maßgeblich Kontrolle im Territorium Rutshuru und erregte großes Aufsehen durch die zwischenzeitliche Einnahme der Provinzhauptstadt Goma. Nach mehreren gescheiterten Verhandlungsrunden mit der Regierung und der Etablierung einer UN-Eingreiftruppe unterlag sie Anfang November 2013 schließlich militärisch.

Die Demokratische Republik Kongo zählt im Jahr 2017 etwas mehr als 81 Millionen Einwohner und ist damit der viertbevölkerungsreichste Staat Afrikas. Die Bevölkerungsdichte ist mit etwas mehr als 30,2 Einwohner pro km² eher gering. Das Bevölkerungswachstum zählt mit fast 3 % zu den höchsten der Welt; jede Frau bringt durchschnittlich 6,3 Kinder zur Welt. 2016 waren 41,2 % der Bevölkerung unter 15 Jahren alt, das Median-Alter betrug 18,4 Jahre. Im weltweiten Vergleich hat das Land laut "Fund For Peace" die problematischste Demografieentwicklung aller Staaten. Im Jahr 2016 wurde die Bevölkerung bereits auf 81,3 Millionen geschätzt. Das jährliche Bevölkerungswachstum lag bei 2,4 % oder ca. 2 Millionen Menschen. Für Mitte des Jahrhunderts wird deshalb mit 197 Millionen Einwohnern in der DR Kongo gerechnet.

Eine Volkszählung fand zuletzt 1984 statt; seitdem hat sich die Bevölkerungszahl mehr als verdoppelt. Der Kongo hat daher auch eine der jüngsten Bevölkerungen der Welt: 46,9 % der Einwohner sind jünger als 15 Jahre, nur 2,5 % älter als 65 Jahre. Die Lebenserwartung liegt im Zeitraum von 2010 bis 2015 bei 56,7 Jahren für Männer und 59,5 Jahren für Frauen.
Während der Bürgerkriege ab Mitte der 1990er Jahre kam es zu einer bis heute anhaltenden ausgeprägten Landflucht; zwischen 2005 und 2010 wuchs die Stadtbevölkerung jährlich im Mittel um 5,1 %.

Die mit Abstand größte Agglomeration des Landes ist die Hauptstadt Kinshasa mit rund 12 Millionen Einwohnern. Damit konzentrieren sich 14 % der Bevölkerung der Demokratischen Republik Kongo auf dieses Gebiet. Neben der Großregion Kinshasa konzentriert sich die Bevölkerung vor allem auf die Bergbauprovinzen Katanga, Kasai-Occidental und Kasai-Oriental.

Die Stadtbevölkerung steigt in fast allen Großstädten des Staates durch anhaltende Landflucht stark an. 2015 lebten 42,5 % der Einwohner in städtischen Gebieten, die Zuwachsrate betrug zwischen 2010 und 2015 rund 4 % jährlich. Im Landesosten können die Einwohnerzahlen, bedingt durch Flüchtlingsbewegungen, erheblich schwanken, 2008 waren dort nach UN-Angaben zwischen 500.000 und einer Million Menschen auf der Flucht.

Während der Kolonialzeit wurden auch im Kongo Ethnien konstruiert. Einige dieser ethnischen Identitäten beruhen auf prämodernen Stammeszugehörigkeiten, andere, wie zum Beispiel die Baluba, wurden gänzlich neu konstruiert. Heute existieren weit mehr als 200 Ethnien in der DRK. Von den Angehörigen dieser Ethnien verstehen sich etwa 80 % als Bantu. Die meisten Bewohner des Landes werden nur einigen wenigen Ethnien zugerechnet, davon die vier großen Bantuvölker: Die beiden größten Gruppen sind Bakongo (16 %) und Baluba (18 %), daneben sind auch die Mongo (13 %) und die Banjaruanda (10 %) zahlenmäßig stark.

Die restlichen 20 % der Landesbewohner setzen sich zu 18 % aus sudansprachigen Völkern, zu 2 % aus Niloten und aus 20.000 bis 50.000 Pygmäen zusammen. Von den etwa 100.000 Europäern (meist Belgier), die zum Zeitpunkt der Unabhängigkeit im Land lebten, sind bis heute etwa 20.000 geblieben.
Bereits vor der Unabhängigkeit schürte die belgische Kolonialmacht Rivalitäten zwischen den Volksgruppen; diese werden bis heute als maßgebliche Ursache für die Kriege und Konflikte im Land genannt.

In der Demokratischen Republik gibt es eine Sprachenvielfalt, die im Land ähnlich groß ist wie die Vielfalt an Volksgruppen: Insgesamt wird die Anzahl der Sprachen und Dialekte des Kongo mit 214 angegeben. Aufgrund der kolonialen Vergangenheit nimmt das Französische den Rang der Amts-, Literatur- und Bildungssprache ein. Daneben gibt es vier offizielle Nationalsprachen: Lingála, Kikongo, Tschiluba und eine kongolesische Variante des Swahili, deren Rechtschreibung 1974 geregelt wurde. Auch diese wurden in der Kolonialzeit von Belgien festgelegt, um die Sprachenvielfalt zu begrenzen. Kikongo ist die Sprache des früheren Kongo-Reiches und ist auch in den Nachbarländern Republik Kongo und Angola verbreitet, während Tshiluba vor allem in den beiden Provinzen Kasai-Occidental und Kasai-Oriental gesprochen wird. Der Ursprung des Lingála befindet sich in dem Land selbst. Diese Sprache, die der Volksgruppe der Bangala zuzuordnen ist, breitete sich aus der Region Équateur entlang der Flüsse aus. Gefördert wurde diese Ausbreitung durch die Europäer, die es als Kommunikationssprache nutzten, später durch die Diktatur von Mobutu, der sich durch die Medien auf Lingala an sein Volk wandte, und heute durch die Popmusik. Swahili ist eine Verkehrssprache in ganz Ostafrika, welche, wenngleich sie im Kongo wenig Muttersprachler hat, diesen Status auch im Osten des Landes besitzt. Außerdem wurde nach dem Ende des Mobutu-Regimes Swahili offizielle Armeesprache und dadurch im gesamten Landesgebiet zunehmend populärer. Artikel 1 der Verfassung bestimmt neben Französisch als „offizieller Sprache“: «… langues nationales sont le kikongo, le lingala, le swahili et le tshiluba». Laut Artikel 142 sind alle Gesetze binnen 60 Tagen in diesen Sprachen zu veröffentlichen. Im Osten des Landes ist Swahili die vorherrschende Sprache der Kommunikation und wird auch in Schulen und auf Ämtern benutzt. Weitere Sprachen sind beispielsweise das mit Tschiluba nah verwandte Kiluba, Chokwe und Kituba.

In den nationalen Medien herrscht unter den vier Sprachen weitgehende Gleichverteilung; in den Regionalmedien wird jedoch die jeweilige Regionalsprache bevorzugt. Schriftsprache ist weiterhin Französisch, doch in der jüngsten Vergangenheit werden oft französischsprachige Texte mit Wörtern der einheimischen Sprachen verknüpft, denen häufig die Funktion eines Stilmittels zukommt.

Indigene Glaubenssysteme drehen sich meist um die Geister der Vorfahren und um Hexer und Zauberer "(ndoki)", die mit diesen kommunizieren können. Des Weiteren glaubt man an die Existenz von Geistern des Wassers, der Fruchtbarkeit und ähnlichen Mächten "(mbumba)", die entweder unsichtbar sind oder in Form von natürlichen Objekten (besonders geformten Felsen, Bäumen oder auch Menschen mit besonderen Eigenschaften wie Albinos) annehmen und die entsprechend verehrt werden. Die Vorstellung von Dämonen, vor denen man sich schützen muss, verlangt die Herstellung von Fetischen und anderen Objekten.

Die dominierende Religion ist das Christentum und innerhalb dessen die Römisch-katholische Kirche. Bereits nach dem ersten Kontakt mit den portugiesischen Entdeckern unter Diogo Cão 1482 blieben Missionare im Land. Anfang des 16. Jahrhunderts wurden die ersten Schulen gebaut, und man überzeugte den König und seine unmittelbare Umgebung, sich taufen zu lassen. Die Region der Kongo-Mündung gehört somit neben Angola und Mosambik zu jenen Gebieten in Afrika, wo die Missionierungsbemühungen der Portugiesen am erfolgreichsten waren.

Nach dem Zerfall des Königreichs gab es im 19. Jahrhundert eine zweite Phase der Missionierung. 1878 errichteten protestantische Missionare in der heutigen Hafenstadt Matadi einen ersten Posten. Die früheste katholische Mission dieser „zweiten Evangelisierung des Kongo“ entstand 1880 in Boma. Die Kongregation vom Unbefleckten Herzen Mariens (Congregatio Immaculati Cordis Mariae, CICM) übernahm das 1886 gegründete „Apostolische Vikariat Belgisch-Kongo“ und errichtete Missionsstationen in Kwamouth (1888) und Leopoldville (1899). 1892 gründeten Jesuiten in Kwango ihre erste Missionsstation. Andere Orden folgten. 1906 sicherte eine Übereinkunft zwischen dem Vatikan und Leopold II. den katholischen belgischen Missionen je 100 bis 200 Hektar unbefristeten Landbesitz zu. Bedingung war, dass jede Missionsstation eine Schule unter staatlicher Aufsicht zur landwirtschaftlichen und handwerklichen Ausbildung unterhielt. Nach dem Ersten Weltkrieg betrieben 22 Missionsgesellschaften von Belgien aus die Kongo-Mission. Vor allem das Schulsystem war in katholischer Hand. 1926 wurden alle staatlichen Schulen im Kongo den katholischen Missionen anvertraut, wobei die Kolonialregierung beträchtliche Summen für den Betrieb zur Verfügung stellte. Nichtkatholische Schulen erhielten erst ab 1946 staatliche Unterstützung. Die Voraussetzung für den Schulbesuch der Kinder war die Taufe. 1930 gab es 640.000 Katholiken (zehn Prozent der Gesamtbevölkerung). 1959 waren es 5,5 Millionen (40 Prozent). Mit dem System der Missionsstationen, die Kirche, Schule und Krankenhaus an einem Ort zusammenführten, bildete die katholische Kirche im ganzen Land eine Infrastruktur aus, die sich bis heute erhalten hat. Sie wuchs damit zu einer mächtigen Kraft in der Gesellschaft.

Das Verhältnis von Kirche und Staat war bis zur staatlichen Unabhängigkeit von verschiedenen Tendenzen geprägt. Die ersten Missionare sahen durch ihre Nähe zur einheimischen Bevölkerung Unterschiede zwischen dem kolonialen System wirtschaftlicher Ausbeutung und einer Entwicklung gemäß christlich-sozialen Vorstellungen und standen dem Unternehmen König Leopolds II. häufig kritisch gegenüber. Die großen Missionsstationen nach dem Ersten Weltkrieg banden die Missionare jedoch enger in das koloniale System ein. Der Unabhängigkeitsbewegung stimmten führende Kirchenvertreter zunächst nur zögerlich zu.

Anfang der 1970er Jahre stellte sich Mobutu mit seiner Kampagne der „Authentizität“ auch gegen das Christentum und die katholische Kirche. Christliche Vornamen wurden verboten. Die katholischen Schulen und die katholische Universität wurden verstaatlicht. Später wurden die Schulen wieder an die Kirche zurückgegeben, da der staatliche Apparat mit deren Verwaltung und Führung überfordert war. In den 1970er Jahren entstanden einheimische Schwesternkongregationen. Mehr Schwarze wurden zu Priestern geweiht, Führungspositionen in der Kirche mit Afrikanern besetzt. Der Vatikan erkannte einen eigens entworfenen Zairischen Messritus offiziell an.

Bei der beginnenden Demokratisierung zu Beginn der 1990er Jahre spielte die katholische Kirche eine bedeutende Rolle. Laurent Monsengwo Pasinya, der damalige Erzbischof von Kisangani und heutige Erzbischof von Kinshasa, wurde zum Präsidenten der Nationalkonferenz (Conférence Nationale Souveraine) gewählt. Als Mobutu im Januar 1992 die Nationalkonferenz auflöste, protestierten weite Teile der Bevölkerung mit dem berühmten „Marsch der Christen“. Nach dem Sturz Mobutus und den anschließenden Kriegen riefen die Führer religiöser Gemeinschaften zum Frieden auf und forderten Demokratisierungsprozesse ein. Die Bischofskonferenz hat ein ständiges Büro eingerichtet, das den Demokratisierungsprozess unterstützt. Im Konflikt zwischen afrikanischen Staaten engagiert sich die katholische Kirche auf der Ebene der gemeinsamen Bischofskonferenz von Burundi, Ruanda und Kongo für Dialog und Versöhnung. Finanziell und teilweise auch personell ist sie noch immer vom Ausland abhängig. Seit November 2010 stellt das Land mit Laurent Monsengwo Pasinya auch einen zum Konklave berechtigten Kardinal.

1878 kamen die ersten protestantischen Missionare in die Kongo-Region. Während der Existenz des Kongo-Freistaats (1885 bis 1908) veröffentlichten einige von ihnen die missbräuchliche Behandlung und Ausbeutung von einheimischen Arbeitern durch die Kolonialgesellschaften und die Kolonialverwaltung. Dies führte mit dazu, dass Leopold II. seinen „Freistaat“ an Belgien übergeben musste. Im Unterschied zur katholischen Kirche, die enger mit dem Staat und den Kolonialgesellschaften verbunden war, hatten die protestantischen Missionare zunächst weniger Vertrauen von Seiten der Regierung und bekamen staatliche Unterstützung für von ihnen betriebene Krankenhäuser und Schulen erst nach dem Zweiten Weltkrieg.

Der Kolonialstaat hatte den verschiedenen Missionsgesellschaften unterschiedliche Territorien zugewiesen. Zur Zeit der Unabhängigkeitserklärung waren etwa 46 protestantische Gruppen aktiv, zumeist aus Nordamerika, Großbritannien und Skandinavien. Sie waren zunächst locker in einem Komitee verbunden. Später schlossen sie sich zur „Eglise du Christ“ („Kirche Christi“) zusammen. Dieser Verbund wurde stark von Diktator Mobutu kontrolliert. Seit der Unabhängigkeit gingen das Eigentum der Missionsgesellschaften und die internen Führungspositionen zunehmend in die Hände von Einheimischen über. Die Regierung Mobutu suchte durch enge Verbindungen zur Führung der „Kirche Christi“ ein Gegengewicht zur Kritik der mächtigen katholischen Kirche aufzubauen. Im Gegenzug half die Regierung dem protestantischen Kirchenbund, neue religiöse Bewegungen und Splittergruppen durch rechtliche und formale Hindernisse in deren Ausbreitung zu behindern.

Die Kimbanguistenkirche wird zu den afrikanischen Kirchen gezählt. Sie wurde während der Kolonialzeit von Simon Kimbangu gegründet, der sich als Erlöser der Schwarzen von der belgischen Unterdrückung ausgab. Die Kimbanguisten überstanden die Bekämpfung durch die Kolonialmacht und haben heute je nach Quelle zwischen 5 Millionen und 10 Millionen Anhänger.

Der bis nach Europa verbreitete pfingstlerische Combat Spirituel hat sein Zentrum im Kongo und allein in Kinshasa rund 50.000 Mitglieder. Staatspräsident Joseph Kabila ist der prominenteste Anhänger dieser Religionsgruppe. Der Combat Spirituel wird von der Öffentlichkeit kritisch gesehen, seitdem bekannt wurde, dass vereinzelte Mitglieder gewaltsame Exorzismusriten an Kindern ausführen. Die Leitung der Kirche distanziert sich zwar von diesen Vorfällen, bekennt sich allerdings zum grundsätzlichen Glauben an die Hexerei von Kindern.

Seit der Unabhängigkeit haben sich zahlreiche weitere christliche Mikrokirchen und Sekten gebildet, deren Zahl von einem Dutzend in den 1960er Jahren auf über 1000 heute angestiegen ist. Sie bilden sich häufig um charismatische, wirtschaftlich erfolgreiche Personen, wobei magische Praktiken eine bedeutende Rolle spielen (z. B. Unverletzbarkeit von Kriegern mittels Verabreichung von Weihwasser). Vielfach zeigt sich eine enge Verflechtung religiöser und erfolgsorientierter materieller Motive.

Die Bevölkerung des Kongo zählt zu den ärmsten der Welt. Eine Untersuchung der kongolesischen Regierung von 2006 ergab folgende Zahlen: 76 % der Bevölkerung konnten ihre Kinder nicht zur Schule schicken, 79 % waren unterernährt, 81 % hatten keinen ausreichenden Wohnraum und 82 % keinen Zugang zu medizinischer Versorgung. Insgesamt 71 % der Bevölkerung lebten in absoluter Armut. Die Armut ist recht unterschiedlich verteilt, in der ärmsten Provinz Équateur oder in den besonders vom Krieg betroffenen Kivuprovinzen wurden die höchsten Werte festgestellt.

In den Provinzen Ituri, Kivu und Kasai im Osten des Landes leiden im Jahr 2018 rund fünf Millionen Menschen an Hunger; mehr als 13 Millionen Menschen sind insgesamt auf humanitäre Unterstützung angewiesen.

Das Sozialsystem des Landes zählt zu den schlechtesten der Welt. Theoretisch ist das seinerzeit vorbildliche, noch aus der Kolonialzeit stammende Sozialversicherungssystem weiterhin in Kraft. Faktisch ist es aber nicht funktionsfähig, allein schon deshalb, weil es heute kaum feste Arbeitsverhältnisse gibt. Ab 1992 stellte die Regierung jahrelang den Unterhalt der Sozialsysteme komplett ein. Staatsbedienstete erhielten keine Gehälter mehr. Nach dem Sturz Mobutus versuchte die neue Regierung zwar, wieder Gehälter zu bezahlen. Dies geschah aber nur unregelmäßig und reichte nicht aus, um den Lebensunterhalt zu bestreiten. Es bürgerte sich ein, dass jeder Bürger staatliche Dienstleistungen direkt bezahlte. Solche Zahlungen, die sowohl an Lehrer und Ärzte als auch an Beamte oder Polizisten erfolgen, werden im kongolesischen Französisch als "la motivation" bezeichnet. Versuche der Regierung, diese Praxis zu verbieten und den Staatsbediensteten wieder Gehälter zu bezahlen, hatten wenig Erfolg: Weder Bürger noch Angestellte trauen der Regierung zu, dass diese regelmäßig gezahlt werden.
Soziale Dienste werden vor allem von der katholischen Kirche betrieben, die unter anderem deswegen in der Bevölkerung ein hohes Ansehen genießt.

Die medizinische Lage in der Demokratischen Republik Kongo ist sehr schlecht. Ein öffentliches Gesundheitssystem ist kaum vorhanden, viele der ohnehin kaum ausgebauten Einrichtungen wurden infolge des Krieges zerstört. So gibt es nur einen Arzt pro 10.000 Menschen, in anderen Staaten ist dieser Wert teilweise 40-mal so hoch. 2005 betrugen die Gesundheitsausgaben der Zentralregierung weniger als eine Million US-Dollar. Laut den Daten der WHO betrugen die Gesundheitsausgaben im Jahr 2009 rund 2 % des Bruttoinlandsprodukts (circa 220 Mio. US-Dollar) oder umgerechnet etwa 3 US-Dollar pro Einwohner.

Zur mangelhaften Versorgungssituation kommt auch das Problem, dass in den ländlichen Regionen nur 29 % und in den Städten 82 % der Menschen Zugang zu sauberem Trinkwasser haben. Insgesamt beläuft sich die Zahl der Kongolesen ohne Trinkwasserzugang laut einer UN-Studie von 2011 auf rund 51 Millionen, obwohl der Staat über mehr als 50 % der Wasserressourcen Afrikas verfügt. Außerdem besteht nur für ein knappes Drittel der Kongolesen die Möglichkeit, Sanitäreinrichtungen zu nutzen. Durch den dadurch hervorgerufenen Mangel an Hygiene treten häufig verschiedene Durchfallerkrankungen auf, ebenfalls weit verbreitet sind andere Infektionskrankheiten wie Typhus und Hepatitis A. Die Demokratische Republik Kongo hat eine der höchsten Kindersterblichkeitsraten, laut Angaben der Weltbank waren es 2017 ca. 94,3 Tote pro 1.000 Lebendgeborene unter fünf Jahren, laut "The World Factbook" der CIA 68,2. Erhebungen aus dem Jahr 2010 kommen auf jährlich 540.000 gestorbene Kinder unter fünf Jahren. Ebenfalls sehr hoch sind die Werte für Säuglings- (126 Todesfälle je 1.000 Geburten) und Müttersterblichkeit (580 Todesfälle pro 100.000 Geburten). Laut Aussagen des Präsidenten der kongolesischen Kinderarztvereinigung von März 2013 gibt es im Kongo etwa 85 Kinderärzte, davon ca. 50 in der Hauptstadt Kinshasa und 20 in der Provinz Katanga.

Weiterhin herrscht ganzjährig ein sehr hohes Malariarisiko im gesamten Land, während des Krieges soll allein diese Krankheit hunderttausende Tote pro Jahr gefordert haben, über ein Drittel davon Kinder unter fünf Jahren. Sehr verbreitet ist auch die Schlafkrankheit, von der 1999 fast zwei Prozent der Bevölkerung betroffen waren. Im Februar 2005 breitete sich in Bas-Uele im Nordwesten des Landes die Lungenpest aus, die WHO berichtete von 61 Toten. Eine weitere Ausbreitung konnte aber verhindert werden. Im Juni 2006 wurden weitere 100 Pesttote im Distrikt Ituri gemeldet.

In der Demokratischen Republik Kongo wurde das Zaire-Ebolavirus erstmals entdeckt, welches nach dem Fluss Ebola nahe dem Ursprungsort benannt wurde. Mit einer Letalitätsrate von 60–90 % ist dies die gefährlichste Spezies des Ebolavirus, das seit 1976 immer wieder auftritt, im Land selbst aber vergleichsweise wenig Opfer fordert, da die Ausbrüche meist in ländlichen Gegenden erfolgten. Der bisher letzte Ausbruch wurde im August 2014 bekannt, er kostete mehr als 40 Menschen das Leben. Die HIV-Rate lag im Kongo 2012 bei rund 1,1 % der Erwachsenen, was knapp einer halben Million Menschen entspricht. Dieser Wert ist verglichen mit den Daten anderer Staaten Subsahara-Afrikas eher niedrig. Die Krankheit fordert jedes Jahr rund 30.000 Todesopfer.

Quelle: UN

Die Alphabetisierungsrate von rund 77,3 % (Männer 88,9 %, Frauen 66,0 %, Zahlen von 2015) ist im Kongo weitaus besser als in Staaten wie Mali oder Niger. Dennoch ist sie durch den Krieg und die damit verbundene Auflösung vieler staatlicher Strukturen seit Mitte der 1990er Jahre deutlich gesunken um sich mit Ende der Konflikte wieder zu erholen: Im Jahr 1995 konnten 77 % der Menschen lesen und schreiben.

Formal ist zwar eine Grundbildung vorgeschrieben (6. bis 12. Lebensjahr) und staatlich garantiert, faktisch ist aber ein öffentliches Bildungssystem kaum existent. Die meisten Schulen erhalten keine staatliche Unterstützung. Daher müssen die Eltern die Lehrer direkt bezahlen. Bedingt durch den Krieg ging der Anteil der Kinder, die eine Schule besuchen, von rund 70 % auf nunmehr etwa 40 % zurück, weil für große Teile der Bevölkerung das Schulgeld unerschwinglich geworden ist. Die Unterrichtsqualität wird generell als schlecht betrachtet, sodass die erworbenen Kenntnisse zumeist unzureichend sind und viele Schulabsolventen keine angemessenen Lese- und Schreibkompetenzen vorweisen können. Ein weiteres Problem ist der Mangel an Lehrpersonal. 2008 kamen auf einen Lehrer 39 Schüler. Aufgrund der demographischen Entwicklung ist mit einer weiteren Verschlechterung des Bildungswesens zu rechnen.

Bis zur Mitte der 1950er Jahre existierte im damaligen Belgisch-Kongo keine Universitätsausbildung. Es gab einige Ausbildungsstätten für Lehrer, für technische und medizinische Berufe, für Agrarwissenschaft und öffentliche Verwaltung sowie religiöse Seminare. Diese führten jedoch nicht zu anerkannten Studienabschlüssen. 1953 wurde in Kinshasa die Katholische Universität Lovanium gegründet. Sie war eng verbunden mit der Katholischen Universität von Löwen in Belgien. 1955 wurde in Lubumbashi eine staatliche Universität eröffnet. 1962 entstand unter protestantischer Schirmherrschaft eine dritte Universität bei Kisangani. Nach der Unabhängigkeit wurde eine Reihe von Fachhochschulen geschaffen.

Im August 1971 wurden die drei Universitäten zur "Université Nationale du Zaire" vereinigt mit separaten Standorten in Kinshasa, Lubumbashi und Kisangani. 1981 kam es wieder zur Aufteilung in selbständige Universitäten an diesen drei Orten.

Die kongolesischen Universitäten gehörten einst zu den besten Afrikas. Heute ist die Unterrichtsqualität unzureichend. Es fehlt sowohl an Lehrmaterial als auch an qualifiziertem Lehrpersonal. Korruption ist auch hier anzutreffen. So werden Abschlüsse häufig durch Bestechung erworben.

Zum Zeitpunkt der Unabhängigkeit war ein Drittel der arbeitsfähigen Bevölkerung in einem festen Arbeitsverhältnis. Bis in die 1990er Jahre sank dieser Anteil auf 15–20 %, wobei die Einkommen erheblich gefallen waren und nicht mehr zum Leben ausreichten. Seitdem ist fast jeder gezwungen, zusätzliches Geld im informellen Sektor zu verdienen, bereits in den 1980er Jahren war das reale Einkommen im Durchschnitt dreimal so hoch wie offiziell gezahltes Gehalt. Die Zahl fester Arbeitsverhältnisse ging seitdem weiter zurück, während des Krieges betrug ihr Anteil nicht einmal mehr 5 %, ein Drittel der Bevölkerung verfügte über gar kein Geldeinkommen.

Die Verfassung von 2006 definiert den Kongo als einen säkularen, demokratischen Rechtsstaat mit einem semipräsidentiellen Regierungssystem. Der Präsident wird vom Volk in allgemeinen, freien und gleichen Wahlen für fünf Jahre direkt gewählt. Eine einmalige Wiederwahl ist möglich. Er ernennt den Premierminister und dessen Kabinett. Es gibt ein Zweikammersystem, bestehend aus Oberhaus (Senat) und Unterhaus (Nationalversammlung). Die 108 Mitglieder des Senats werden für fünf Jahre von den Provinzparlamenten gewählt, die Nationalversammlung mit 500 Abgeordneten wird vom Volk gewählt. 61 Sitze werden nach Mehrheitswahlrecht bestimmt, die übrigen nach Verhältniswahl in offenen Listen.

In der Praxis erfüllt der Staat, abgesehen von der erfolgreichen Wahl 2006, in keiner Weise die Merkmale einer Demokratie und eines Rechtsstaates. Eine Gewaltenteilung existiert nur in der Theorie, es gibt praktisch keine unabhängige Justiz, und Gesetze werden nicht durchgesetzt. Alle staatlichen Institutionen sind hochgradig korrupt und unzuverlässig und es ist seit Jahrzehnten allgemein üblich, dass Posten in staatlichen Institutionen und Betrieben zur persönlichen Bereicherung ausgenutzt werden. Der Staat steht auf dem Demokratieindex der Zeitschrift "The Economist" auf dem 155. von 167 Plätzen und wird der Kategorie "Autoritäres Regime" zugeordnet. Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „unfrei“ bewertet. In der Kategorie „politische Rechte“ erhält die DR Kongo die Note 7, bei der Wahrung der Bürgerrechte erhält das Land die Note 6 (die Note 1 ist die beste und die 7 die schlechteste).

Die territoriale Souveränität der Regierung ist insbesondere im Osten des Landes nicht mehr gegeben. Aufgrund ihrer Instabilität wird die Demokratische Republik Kongo als gescheiterter Staat bezeichnet, gleichwohl keine der zahlreichen Rebellengruppen, die seit der Unabhängigkeit existierten, je die Legitimität des Staates in Frage oder sezessionistische Forderungen stellten.

Am 16. Mai 2005 beschloss das 2003 ernannte Übergangsparlament den Entwurf einer neuen Verfassung. Die Macht des Präsidenten wird darin eingeschränkt. Der Premierminister ist nun nicht mehr dem Präsidenten verantwortlich, sondern der Mehrheitsfraktion im Parlament. Am 27. Oktober 2005 sollte das Volk über die neue Verfassung abstimmen.

Der Abstimmung ging eine langwierige, von EU und UN unterstützte Wählerregistrierung voraus. Jeder Wähler erhielt einen fälschungssicheren Personalausweis, und trotz diverser Boykottaufrufe ließen sich insgesamt 25.650.751 Wähler registrieren, von geschätzt 28 Millionen prinzipiell Wahlberechtigten. Nachdem die Wählerregistrierung erheblich länger als geplant gedauert hatte (in abgelegenen Gebieten Équateurs und Bandundus wurden die letzten Wähler erst im Februar 2006 registriert), wurde die Abstimmung schließlich verschoben.

Am 18. und 19. Dezember 2005 stimmten 84,3 % der Wähler bei einer Wahlbeteiligung von 62 % in einem Verfassungsreferendum für die Annahme der neuen Verfassung. Die Zustimmung war je nach Landesteil unterschiedlich verteilt, in Kinshasa stimmten aufgrund der dortigen Boykottkampagnen nur etwas mehr als 50 % dafür, in den Kivuprovinzen lag sie bei über 90 %. Am 18. Februar 2006 trat die neue Verfassung in Kraft.

Am 30. Juli 2006 fanden die Wahlen für das Präsidentenamt und das Parlament statt. Es war die erste freie Wahl im Kongo seit 1965. Es gab 43 Bewerber für das Präsidentenamt, darunter zahlreiche frühere Rebellenführer, und über 60 Parteien für das Parlament. Unterschiedliche Programme hatten die Kandidaten nicht zu bieten, es ging lediglich um die Frage, wer das Land zukünftig regieren durfte. Der Wahlkampf war von Gewalt, willkürlichen Verhaftungen und Hetzkampagnen der Presse überschattet. Für den Fall, dass die ehemaligen Kriegsherren die Ergebnisse nicht anerkennen würden, wurden schwere Unruhen bis hin zu einem erneuten Ausbruch des Bürgerkriegs befürchtet. Zur Absicherung der Wahl entsandte die EU zusätzlich zur UN-Mission MONUC eine eigene Militärmission, die EUFOR RD Congo.

Der Wahltag selbst verlief dann weitgehend friedlich. Im ersten Wahlgang erhielt Kabila 44,8 %, Jean-Pierre Bemba 20,0 % und Antoine Gizenga 13,1 %, die Anteile aller anderen Kandidaten lagen bei weit unter 10 %. Die Ergebnisse der Parlamentswahl verhielten sich ähnlich: Die PPRD (Kabila) erhielt 111 von 500 Sitzen, die MLC (Bemba) 64 und die PALU (Gizenga) 34, der Rest ging an zahlreiche kleine Parteien und unabhängige Kandidaten. Die Ergebnisse waren wie schon bei dem Verfassungsreferendum sehr unterschiedlich verteilt, in den Ostprovinzen, die während des Krieges unter Rebellenkontrolle standen, erzielte Kabila sehr hohe Ergebnisse, während im Westen die Stimmen breiter verteilt waren. Die befürchteten Unruhen blieben weitgehend aus, es kam lediglich zu begrenzten Gefechten zwischen den Truppen Bembas und Kabilas in Kinshasa.

Da keiner der Kandidaten für das Präsidentenamt eine absolute Mehrheit erreicht hatte, fand am 29. Oktober 2006 eine Stichwahl statt, die Kabila mit 58,05 % der Stimmen gewann. Die Ergebnisse waren wieder ungleich verteilt, die westlichen Provinzen Équateur, Bas-Congo, Kinshasa und Kasai fielen Bemba zu, der Osten Kabila.

Die nächste Wahl fand am 27. November 2011 statt. Im Vorfeld dieser Wahlen gab es Auseinandersetzungen um eine Verfassungsänderung, die nach Auffassung der Opposition eindeutig die Wiederwahl Kabilas begünstigte. Am 12. Januar 2011 stimmte die Nationalversammlung und am folgenden Tag auch der Senat als zweite Parlamentskammer für eine Abschaffung der Stichwahl um das Präsidentenamt. Demnach reicht die einfache Mehrheit im ersten Wahlgang. In der Wahl, bei der es in Einzelheiten Hinweise auf Unregelmäßigkeiten bzw. Wahlbetrug gab, wurde Kabila mit 48,95 % wiedergewählt, sein wesentlicher Konkurrent Étienne Tshisekedi erhielt 32,33 %.

Nach den Wahlen wurde am 30. Dezember 2006 Antoine Gizenga, der alte Lumumbistenführer der 1960er Jahre, zum Premierminister ernannt, am 7. Februar stand die neue, aus 60 Ministern und Vizeministern bestehende Regierung. Erstmals seit Jahrzehnten gab es wieder eine Regierung, die eine gute Regierungsführung zumindest versuchte. Die Erfolge der neuen Regierung blieben gering, der alte Gizenga war der Situation nicht mehr gewachsen, die Macht im Land blieb bei Präsident Kabila und beim Militär. Am 25. September 2008 reichte Gizenga altersbedingt seinen Rücktritt ein, Nachfolger wurde am 10. Oktober 2008 Haushaltsminister Adolphe Muzito. Er gehört ebenfalls der PALU an, dies war wegen eines Koalitionsabkommens zwischen den Regierungsparteien PPRD, PALU und UDEMO eine der Bedingungen bei der Neubesetzung des Postens. Zusammen mit Muzito wurden 16 Minister neu ernannt. Die Mehrheit der Minister der Koalition hält Kabilas PPRD.

Muzito trat am 7. März 2012 zurück. Nachfolger als Premierminister wurde am 18. April 2012 der bisherige Finanzminister Augustin Matata Ponyo. Er stellte am 28. April sein neues Kabinett vor. Vizepremier blieb Daniel Mukoko Samba, der auch den Posten des Haushaltsministers übernahm. Weitere Kabinettsmitglieder sind Louise Munga Mesozi, Alexandre Luba Ntambo (auch zweiter Vizepremier), Richard Mujey Magez, Raymond Tshibanda, Wivine Mumba Matipa, Célestin Vunabandi, Kinkiey Mulumba, Christostome Vahamwiti, Jean-Claude Kibala, Martin Kabwelulu, Lambert Mende, Remy Musungayi Bampale und Nemoyato Begepole.

Der Kongo ist eines der Länder, in denen die Menschenrechte wenig geachtet werden. Dies trifft insbesondere auf die Kriegsgebiete zu, wo die Kriegsparteien kaum Rücksicht auf die Zivilbevölkerung nehmen.

Vergewaltigung war und ist in der Demokratischen Republik Kongo eine Kriegswaffe. In den Jahren 2006 bis 2009 wurden allein von dem Hilfswerk „Heal Africa“ 12.000 vergewaltigte Frauen betreut. Die Organisation geht von der zehnfachen Zahl an Vergewaltigungen aus. Laut einer Studie sind rund 39 % aller Frauen und 24 % aller Männer im Land mindestens einmal in ihrem Leben Opfer einer Vergewaltigung geworden. Immer wieder gibt es Berichte über Massenvergewaltigungen, etwa 2010 in Luvungi.

Sowohl Angehörige bewaffneter Gruppen als auch staatliche Sicherheitskräfte verübten routinemäßig Folterungen und Misshandlungen, insbesondere gegen vermeintliche politische Gegner. Zu den Foltermethoden gehörten Schläge, Verletzungen durch Messerstiche, Vergewaltigungen und das Aufhängen von Personen an Gitterstäben. In den meisten Hafteinrichtungen und Gefängnissen herrschten derart harte Bedingungen, dass sie grausamer, unmenschlicher oder erniedrigender Behandlung gleichkamen. In Berichten hieß es regelmäßig, dass Gefangene an Unterernährung und behandelbaren Krankheiten starben.

Ein weiterer humanitärer Krisenschwerpunkt ist die Nordost-Region der Provinz Orientale, wo die aus Uganda stammenden Lord’s Resistance Army (LRA) im Gefolge einer gescheiterten gemeinsamen Militäraktion von der Demokratischen Republik Kongo, Sudan und Uganda seit Dezember 2007 wiederholt grausame Attacken auf die Zivilbevölkerung verübt. Die LRA wird für den Tod von über 1.200 Menschen und die Entführung von über 600 Kindern seit September 2008 verantwortlich gemacht.

2008 verurteilten Militärgerichte mindestens 50 Menschen zum Tode, darunter auch Zivilisten. Es wurden allerdings keine Hinrichtungen gemeldet – so Amnesty International. Sicherheitskräfte der Regierung und bewaffnete Gruppen überfielen und entführten Menschenrechtsverteidiger, schüchterten sie ein und bedrohten sie mit Mord. In Nord-Kivu mussten viele, die sich für die Menschenrechte einsetzten, untertauchen oder fliehen. Andere wurden zur Zielscheibe, weil sie an der Aufarbeitung politisch brisanter Menschenrechtsverletzungen beteiligt waren. Im Jahr 2008 befanden sich Schätzungen zufolge immer noch 3000 bis 4000 Kinder in den Reihen bewaffneter Gruppen.

In einem im Dezember 2009 von Human Rights Watch veröffentlichten Bericht wird detailliert die gezielte Tötung von mehr als 1400 Zivilisten zwischen Januar und September 2010 während zwei aufeinander folgender kongolesischer Militäroperationen gegen die ruandische Hutu-Miliz „Demokratischen Kräfte zur Befreiung Ruandas“ (FDLR) dokumentiert. Sowohl kongolesische Regierungssoldaten als auch FDLR-Rebellenmilizen haben Zivilisten angegriffen, ihnen vorgeworfen, mit dem Gegner zu kollaborieren, und sie „bestraft“, indem sie mit Macheten zu Tode gehackt wurden. Beide Seiten haben darüber hinaus Zivilisten bei Fluchtversuchen erschossen oder sie absichtlich in ihren Häusern verbrannt. Einige Opfer wurden gefesselt, bevor ihnen, einem Zeugen zufolge, die Kehlen „wie Hühnern durchgeschnitten“ wurden. Die Mehrheit der Opfer waren Frauen, Kinder und ältere Menschen.

Am 1. Oktober 2010 veröffentlichte das Amt des Hohen Kommissars der Vereinten Nationen für Menschenrechte (OHCHR) einen ausführlichen Bericht über schwerste Menschenrechtsverletzungen und Verletzungen internationalen humanitären Rechts auf dem Territorium der Demokratischen Republik Kongo im Zeitraum von März 1992 bis Juni 2003.
Die Regierung von Ruanda hatte vor der Veröffentlichung dieses Berichts vergeblich Änderungen verlangt. Hintergrund war, dass einige der schwersten dokumentierten Verbrechen von Angehörigen der "Rwandan Patriotic Army" (RPA) und der mit ihnen verbündeten "Alliance des forces démocratiques pour la libération du Congo-Zaïre" (AFDL) begangen worden waren. Sie könnten nach Aussage des Berichts möglicherweise als Genozid bezeichnet werden.

Die Demokratische Republik Kongo gehört einer Reihe von politischen und wirtschaftlichen Vereinigungen an:


Die kongolesischen Streitkräfte () entstanden in ihrer heutigen Form nach dem Zweiten Kongokrieg, als die Regierungsarmee mit den verschiedenen Rebellenstreitkräften zusammengelegt wurde. 2003 meldeten Regierung und Rebellen über 300.000 Soldaten für die Eingliederung in die neuen Streitkräfte, nach einer unabhängigen Schätzung waren es aber allenfalls 200.000 Soldaten. Die Sollstärke der FARDC sollte bei etwa 120.000 Mann liegen. Bis 2008 waren aber erst etwa 45.000 Mann in 15 Brigaden einsatzbereit. Bei Aufstellung der neuen Streitkräfte wurden „gemischte“ Einheiten gegründet, das heißt in der FARDC dienen Soldaten verschiedener Bürgerkriegsparteien in ein und derselben Einheit.

Der Neuaufbau der Armee ist noch lange nicht abgeschlossen, die alten Strukturen der Rebellen bestehen weiter fort, zehntausende Soldaten befinden sich außerhalb der regulären Befehlsstrukturen unter dem Kommando ehemaliger Bürgerkriegsgeneräle. Dies ist vor allem im Osten des Landes, in den Kivuprovinzen der Fall, in der bis heute verschiedene lokale Milizen die Macht ausüben.

Die FARDC hat sowohl mit starken Organisations- als auch Moralproblemen zu kämpfen. Die Soldaten sind unzureichend ausgebildet und ausgerüstet, der Sold wird nur unregelmäßig ausbezahlt und reicht nicht aus, um den Lebensunterhalt zu bestreiten. Die Moral der Truppe ist entsprechend schlecht und die Desertationsrate hoch. Bei Kämpfen im Kivu kam es immer wieder zu Massendesertationen tausender Soldaten. Zahlreiche Menschenrechtsverletzungen gehen auf Kräfte der FARDC zurück, regelmäßig kommt es zu Übergriffen auf Zivilisten mit Plünderungen und Vergewaltigungen seitens der Angehörigen der FARDC.

In der Demokratischen Republik Kongo herrscht traditionell eine streng zentralistische Verwaltung. Das Land war bis 2015 in zehn Provinzen und den Hauptstadtdistrikt gegliedert. Die 2005 beschlossene Verfassung sah eine Dezentralisierung vor, bei der die 11 Gebietskörperschaften in 26 neue Provinzen mit eigenen Parlamenten aufgeteilt werden sollten. 40 % der auf dem Gebiet einer neuen Provinz eingenommenen Steuern sollten künftig dort verbleiben. Diese Verwaltungsreform sollte erst 2011 komplett umgesetzt worden sein. Im Januar 2011 wurde die Neuaufteilung des Landes durch eine Verfassungsänderung abgesagt, allerdings wurde die Neugliederung im Jahr 2015 doch umgesetzt.

Die folgende Tabelle gibt die derzeitigen Provinzen des Landes mit Fläche und ehemaliger Provinzzugehörigkeit an. Kinshasa wird offiziell nicht als Provinz, sondern als Hauptstadtdistrikt bezeichnet.

Jahrzehntelange Misswirtschaft, extreme Korruption und jahrelange Bürgerkriege machten den Kongo, der kurz nach der Unabhängigkeit eines der wirtschaftlich am höchsten entwickelten Länder Afrikas war und über die größten Naturreichtümer des Kontinents verfügt, zu einem der ärmsten Länder der Welt, das in allen Entwicklungsindikatoren weit hinten angesiedelt ist.

Das kaufkraftbereinigte Bruttoinlandsprodukt ("BIP") beträgt etwa 41,61 Milliarden US-Dollar, das BIP pro Einwohner ungefähr 495 US-Dollar (etwa 450 Euro). Die Frauenerwerbsquote liegt bei etwa 71 %. Trotz einem jahrelangen Wirtschaftsaufschwung in der Regierungszeit von Joseph Kabila ist die DR Kongo das zweitärmste Land der Welt.

Die Inflationsrate ist beständig hoch und betrug 2011 13,3 %, seit Jahrzehnten dient daher der US-Dollar als Zweitwährung und Wertaufbewahrungsmittel.

Charakteristisch für das Land ist der große informelle Sektor, der nicht in die Berechnung des BIP einfließt. Bereits in den 1980er Jahren soll die informelle Wirtschaft dreimal so groß wie die offizielle gewesen sein. Grund für diese Entwicklung waren und sind die extreme Korruption und die mangelnde Effektivität staatlicher Organe, die ein solides Wirtschaften enorm erschweren. Von staatlicher Seite werden erst in jüngster Zeit Anstrengungen unternommen, den Zustand zu ändern.

Seit Abschaffung einer Einheitsgewerkschaft 1990 besitzt das Land nun zwar mehrere unabhängige Gewerkschaften, welche aber kaum noch Einfluss auf die Unternehmen haben.

Die Wirtschaft des Landes erlebte in den vergangenen Jahrzehnten eine wechselvolle Entwicklung. In vorkolonialer Zeit war das heutige Staatsgebiet eine bedeutende Quelle für Sklavenhändler. Die von Sansibar aus operierenden islamischen Sklavenhändler, die von lokalen Herrschern und Milizen unterstützt wurden, beuteten das Land weit schwerwiegender aus als die Europäer im Westen des Landes. Die europäische Kolonialisierung ab 1876 setzte sich die Beendigung des Sklavenhandels zum Ziel. Der Widerstand der Sklavenhändler wurde blutig niedergeschlagen.

Nach Errichtung des Kongo-Freistaats durch Belgien begann eine in der Kolonialgeschichte beispiellose Ausplünderung des Landes. Der Bevölkerung wurde Zwangsarbeit auferlegt, um Elfenbein, Palmöl und vor allem Kautschuk zu exportieren. Mit Gründung von Belgisch-Kongo 1908 rückte allmählich der Bergbau zum Hauptwirtschaftszweig auf, es wurden vor allem Kupfer und Diamanten abgebaut. Die Landbevölkerung wurde gezwungen, Exportprodukte wie Baumwolle und Palmöl zu produzieren. Es entstanden ein modernes, dichtes Straßennetz und ein effizientes Gesundheitssystem, welches auch den Lebensstandard der Einheimischen hob. In den letzten Jahren vor der Unabhängigkeit zählte die Kolonie zu den wirtschaftlich am höchsten entwickelten afrikanischen Staaten, der Wohlstand war jedoch extrem zugunsten der immer zahlreicher werdenden belgischen Siedler verteilt: Die Hälfte des Volkseinkommens lag bei den 1 % Europäern, von gesellschaftlicher und politischer Teilhabe blieben die Kongolesen weitgehend ausgeschlossen.

Die Wirren nach der Unabhängigkeit und die Ausreise vieler Belgier hatten zunächst einen wirtschaftlichen Einbruch zur Folge, von dem sich das Land aber innerhalb weniger Jahre erholte. Von hohen Rohstoffpreisen getragene teilweise zweistellige Wachstumsraten Ende der 1960er und Anfang der 1970er Jahre ermöglichten große, aber unrentable Bauprojekte wie den Inga-Staudamm und die HGÜ Inga-Shaba. Es wurde erwartet, dass sich das Land innerhalb weniger Jahre zur Industrienation entwickeln würde.

Als infolge der Ölkrise ab 1973 die Rohstoffpreise zu sinken begannen, begann auch der Niedergang der zairischen Wirtschaft. Die immer weiter ausufernde Korruption Mobutus und seiner Herrschaftclique sorgte dafür, dass Exporteinnahmen nicht mehr reinvestiert wurden und die Wirtschaftsbetriebe verfielen. Während der 1980er und zu Beginn der 1990er Jahre befand sich die Wirtschaft im freien Fall, zwischen 1990 und 1994 hatte sich das Bruttoinlandsprodukt fast halbiert, die Kupferproduktion war um über 90 % gesunken, die Inflationsrate dreistellig. Immer größere Teile der Wirtschaft wanderten in den informellen Sektor ab. Zwar stabilisierte sich die Lage kurzzeitig wieder, aber die Kriege ab 1996 führten zu einem weiteren Rückgang der Wirtschaftsleistung.

Nach Kriegsende begann, getragen durch hohe Rohstoffpreise und internationale Investitionen im bedeutenden Bergbausektor, ein erneuter Aufschwung. Der Ausbruch der Finanzkrise 2008 belastet mit sinkenden Rohstoffpreisen und weniger Investitionen auch die kongolesische Wirtschaft außerordentlich stark.

Der Kongo zählt zu den rohstoffreichsten Ländern der Welt, Bergbauprodukte sind daher seit Jahrzehnten Hauptexportgut, wichtigster Devisenbringer des Landes und Haupteinnahmequelle des Staates. Gefördert werden vor allem Diamanten (Kasai), Gold (Kivu, Ituri), Kupfer (Katanga), Coltan (Kivu), Mangan (Katanga), Blei und Zink (Katanga) sowie Zinn (Katanga). Der Reichtum an mineralischen Rohstoffen führte wiederholt zu politischen und bewaffneten Konflikten im Land.

Südafrika hat nach 1994 sein außenwirtschaftliches Engagement in der DRC unter neuen politischen Prämissen zunehmend ausgebaut. Im Fokus der Bemühungen steht dabei die Entwicklung der Verkehrsinfrastruktur im Süden des Landes. Ein wichtiger Akteur bildet dabei die südafrikanische IDC. Ein weiteres von der IDC unterstütztes Investitionsfeld bildet der Kupferbergbau.

Die Volksrepublik China schloss 2007 mit der DRC ein Abkommen ab, auf dessen Grundlage ein Darlehen von 5 Mrd. US-Dollar, 2008 auf 9 Mrd. erhöht, für Infrastrukturbauten gewährt wurde. Als davon profitierende Sektoren wurden genannt: Rohstofferkundungen sowie der Ausbau der Verkehrs- und Sozialinfrastruktur in der Katangaprovinz. Ein Jointventure mit dem Namen "Sicomines" zwischen Gécamines, Sinohydro und der China Railway Engineering Corporation wurde vereinbart.

Besondere Bekanntheit seit Ende des 20. Jahrhunderts hat hier der Abbau des Erzes Coltan erlangt. Es enthält die wirtschaftlich bedeutsamen Metalloxidminerale "Columbit" und "Tantalit", aus denen Niob und Tantal gewonnen werden. Tantal ist für die Produktion elektronischer Geräte von großer Bedeutung, Niob dient als Legierungszusatz in der Stahlproduktion bei der Herstellung hitzebeständiger Metallbauteile in der Luft- und Raumfahrtindustrie.

Die noch aus der Kolonialzeit und den ersten Jahren nach der Unabhängigkeit entstandenen Förderanlagen an einigen Orten sind mangels Instandhaltung heute weitgehend zerfallen, deren Wiederaufbau kommt nur schleppend voran. Artisanaler Bergbau, der weitgehend ohne maschinelle Unterstützung erfolgt, stellt heute daher einen bedeutsamen Wirtschaftszweig mit vielen Erwerbstätigen, dem größten Anteil am Bruttoinlandsprodukt und an den Exporten dar. Diese Wirtschaftsform entzieht sich weitgehend staatlicher Kontrolle.

Während des Krieges war der Verkauf von Bodenschätzen wichtigste Einnahmequelle sowohl für Regierung als auch die Rebellen, auch die Nachbarstaaten und private Gesellschaften waren an der jahrelangen systematischen Ausplünderung des Landes beteiligt.

Nach wie vor wird der Osten des Landes, in dem sich die meisten Bodenschatzvorkommen befinden, nicht von der Regierung, sondern zu großen Teilen von aufständischen Milizen kontrolliert. Die Schürfer müssen ihre Erze zu Preisen, die weit unter Weltmarktpreisen liegen, an Exporthändler verkaufen, die von den lokalen Machthabern konzessioniert sind. Dieses System beschert den Bewaffneten stetige Einnahmen und ermöglicht damit die Finanzierung des Kriegs. Zukünftig soll ein Zertifizierungssystem für kongolesische Rohstoffe dafür sorgen, dass diese legal gehandelt werden.

Der Import von Coltan aus der DRC steht in den Industriestaaten seit Jahren in der Kritik, weil damit westliche Unternehmen indirekt maßgeblich zur Aufrechterhaltung des Kriegszustandes beitragen. Verschiedene Nichtregierungsorganisationen organisierten immer wieder Boykottkampagnen gegen Coltan aus dem Kongo, ließen dabei aber außer Acht, dass der Coltanexport Haupteinnahmequelle der Bevölkerung des Kivu ist.

Um die Geldquellen der Profiteure dieser Geschäfte trockenzulegen, verhängte die Regierung am 11. September 2010 einen totalen Stopp sämtlicher Bergbauaktivitäten für die Provinzen Nord-Kivu, Sud-Kivu und Maniema im Osten des Kongos, das Schürfer, Händler, Exporteure und Inhaber von Abbaurechten betraf. Dadurch konnten die Bergbauaktivitäten jedoch nicht gestoppt werden, sondern wurden stattdessen in den illegalen Bereich gedrängt. Während kriminelle Unternehmen profitierten, brach die sonstige Wirtschaft in der Kivu-Region fast vollständig zusammen. Deshalb wurde das Bergbauverbot im März 2011 wieder aufgehoben.

Die Erdölreserven im gesamten Staatsgebiet werden auf 180 Millionen Barrel geschätzt, im Jahr 2009 wurden täglich rund 16.360 Barrel Erdöl gefördert. Die Regierung forciert die Erdölförderung, missachtet aber dabei häufig Umwelt- und Sicherheitsbedenken. Im Jahr 2010 erhielten SOCO, Dominion Petroleum und das Staatsunternehmen Cohydro die Konzession für Ölbohrungen im Nationalpark Virunga, der zum Weltnaturerbe zählt und mitten in einem von Rebellen kontrollierten Gebiet liegt. Auf Druck von EU-Kommission, UNESCO und zuständigen UN-Stellen wurde die Genehmigung des Projekts jedoch im März 2011 von der Regierung zurückgenommen.

Das Land besitzt Erdgasreserven von 991,1 Millionen Kubikmetern, derzeit findet jedoch noch keine Förderung statt.

Während der Kolonialzeit wurde die Landbevölkerung zum Anbau von Feldfrüchten für den Export gezwungen, in den Jahren vor der Unabhängigkeit entstanden auch von Europäern geleitete Agrargroßbetriebe. Seit 1960 ging die landwirtschaftliche Produktion stetig zurück, besonders die Verstaatlichung ab 1973, in der viele produktive Betriebe enteignet wurden, verursachte einen deutlichen Einbruch. Seitdem wird die Landwirtschaft vor allem zugunsten des lukrativeren artisanalen Bergbaus vernachlässigt. In den meisten ländlichen Regionen herrscht heute Subsistenzwirtschaft vor, ein Transport der Ernte in die Städte wäre aufgrund fehlender Verkehrswege ohnehin kaum möglich.

Nur knapp drei Prozent der Landfläche wird landwirtschaftlich genutzt, dennoch macht die Landwirtschaft mehr als die Hälfte des Bruttoinlandsprodukts aus und beschäftigt fast zwei Drittel der erwerbstätigen Bevölkerung. Die Produktion von Nahrungsmittel reicht für den Eigenbedarf nicht aus, das Land muss solche importieren.

Typische Agrarprodukte sind Maniok, Zuckerrohr, Kaffee, Palmöl, Kautschuk und Bananen. Ebenfalls besteht eine nennenswerte Holzwirtschaft.

Der industrielle Sektor konzentriert sich heute auf die Verarbeitung der vorhandenen Bodenschätze. Während des Wirtschaftsbooms um das Jahr 1970 wurde zwar mit dem Aufbau einer importsubstituierenden Industrie begonnen, diese war aber gegenüber Importen nicht konkurrenzfähig und verschwand bis in die 1990er Jahre fast völlig. Industrielle Großbetriebe bestehen heute kaum noch. Die verarbeitende Industrie besteht heute aus Kleinbetrieben, die verschiedene Konsumgüter wie Textilien, Schuhe oder Zigaretten produzieren bzw. in der Lebensmittelverarbeitung tätig sind.

2007 exportierte das Land Waren im Wert von 6,1 Milliarden US-Dollar, die Hauptexportprodukte sind Diamanten, Gold, Kupfer sowie Holz und Kaffee. Die Exporte gingen im Jahr 2009 fast zur Hälfte (46,8 %) in die Volksrepublik China, es folgten die USA mit 15,4 % und Belgien mit 10,7 % sowie Sambia (5,8 %) und Finnland (4,4 %).

Den Ausfuhren stehen Importe im Wert von 5,2 Milliarden US-Dollar gegenüber. Es handelt sich bei den Einfuhren zumeist um Maschinen und Fahrzeuge aller Art sowie Nahrungsmittel und Treibstoffe. Die Produkte kommen vorwiegend aus Südafrika (18,2 %), Belgien (10,2 %), China (8,3 %), Sambia (7,8 %), Frankreich (7,3 %) und weiteren afrikanischen und europäischen Staaten.

Die Außenhandelsbilanz des gesamten Landes ist zumeist nahezu ausgeglichen, allerdings bestehen innerhalb des Landes hohe Ungleichgewichte, denn nahezu alle Exportgüter werden in nur wenigen Landesteilen produziert. Lokale Handelsbilanzdefizite werden zumeist durch informellen Handel, der in den Statistiken nicht auftaucht, ausgeglichen.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 5,8 Milliarden US-Dollar, dem standen Einnahmen von umgerechnet 5,4 Milliarden US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsüberschuss in Höhe von 0,9 % des Bruttoinlandsprodukts.

Die Staatsverschuldung betrug 2016 21,5 % des BIP. 2010 wurden der Demokratischen Republik Kongo Staatsschulden in Höhe von ca. 12 Milliarden US-Dollar erlassen; 2009 entsprach die Staatsverschuldung noch 138,3 % des BIP und war damit, gemessen an der Wirtschaftsleistung, eine der höchsten der Welt.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Die Demokratische Republik Kongo steht großen Herausforderungen gegenüber, was die Infrastruktur betrifft. Die bewaffneten Konflikte der näheren Vergangenheit haben dazu geführt, dass die Einrichtungen entweder direkt beschädigt oder ihr Erhalt vernachlässigt wurden. Somit ist mehr als die Hälfte der Anlagen dringend erneuerungsbedürftig. Um auf den Stand eines durchschnittlichen Entwicklungslandes zu kommen, müsste die Demokratische Republik Kongo jährlich etwa 5,3 Milliarden US-Dollar bzw. 75 % ihres Bruttoinlandsproduktes von 2006 aufwenden, gleichzeitig geht aktuell jährlich fast eine halbe Milliarde US-Dollar durch ineffiziente Infrastruktur verloren.

Der Kongo erbte bei seiner Unabhängigkeit ein teils sehr gutes Straßennetz von über 100.000 Kilometern Länge, das sich über das gesamte Land erstreckte. Unzureichende Wartung während der Herrschaft Mobutus sorgte dafür, dass in den 1990er Jahren nur noch etwa 10.000 Kilometer Straße befahrbar waren, die Überlandstraßen waren fast vollständig verschwunden. Die Länge des Straßennetzes wird heute mit rund 150.000 Kilometer angegeben, von denen nur rund 3000 Kilometer asphaltiert sind; es gibt in der Welt kaum ein Land, das ein so dünnes Straßennetz hat wie die Demokratische Republik Kongo. Auf 1000 km² kommen im Schnitt gerade 1 km befestigter und 14 km unbefestigter Straße. Weniger als die Hälfte des Straßennetzes befindet sich in annehmbaren Zustand und die Wiederherstellung vernünftiger Straßenverbindungen zwischen den Ballungsräumen des Landes gehört zu den dringendsten Aufgaben der Regierung. Die niedrige Bevölkerungsdichte, das Klima und die Topographie lassen den Unterhalt eines gut ausgebauten Straßennetzes aber sehr teuer werden, so dass das Land etwa 5 % seines Bruttonationalproduktes jährlich allein für den Unterhalt seiner Verkehrsinfrastruktur ausgeben müsste. Das ist ein Vielfaches dessen, was für öffentliche Investitionen in den letzten Jahren zur Verfügung gestanden ist. Als Konsequenz des Ganzen kostet es dreimal so viel, Güter auf der Straße wie auf dem Wasserweg zu transportieren, der Straßentransport ist in der Demokratischen Republik Kongo dreimal so teuer wie in seinen Nachbarländern.

In der Kolonialzeit wurde der Ausbau eines Eisenbahnnetzes vorangetrieben, vorrangig zur effizienteren Ausbeutung der Rohstoffe, die per Bahn schneller aus dem Landesinneren an die Küste gelangen konnten. Heute verfügt die Demokratische Republik Kongo auf dem Papier über rund 5100 Kilometer Gleis in mehreren voneinander unabhängigen Netzen. Die Chemin de Fer Matadi-Kinshasa (CFMK) betreibt eine 366 km lange Verbindung zwischen Kinshasa und dem Hafen Matadi. Diese eingleisige Strecke stammt in ihrer heutigen Streckenführung aus den 1930er Jahren. Die Société Nationale des Chemins de fer du Congo (SNCC) betreibt ein weitaus größeres Netz mit Zentrum im Südosten des Landes, wobei die wichtigste Verbindung zwischen Kolwezi und der Grenze zu Sambia verläuft und teilweise bereits zu Kolonialzeiten durch die 50-Hz-Arbeitsgemeinschaft elektrifiziert wurde. Über die SNCC verlassen Rohstoffe, vor allem Kupfer, das Land. Die SNCC ist von Ilebo über den Fluss Kongo mit Kinshasa und damit der CFMK verbunden. Nachdem in Angola bis 2014 die Benguelabahn wiedererrichtet wurde, soll sie innerhalb der DR Kongo mit dem Netz der SNCC verbunden werden, was Kupferexporte über den Atlantikhafen Lobito ermöglichen wird. Die Infrastruktur der SNCC ist alt und in sehr schlechtem Zustand, so dass mehr und mehr Rohstoffe über die Straße befördert werden. Die Uelle-Bahnen werden größtenteils schon lange nicht mehr bedient, jedoch wurde der Abschnitt zwischen Bumba und Aketi im Jahr 2005 wiederhergestellt.

Im Vergleich mit den Eisenbahnnetzen seiner Nachbarländer verkehren auf den Schienen des Kongo sehr wenige Züge, die Indikatoren für Effizienz und Zuverlässigkeit sind deutlich schlechter und die Preise für die Güter- wie Personenbeförderung deutlich höher.

Der Hafen von Matadi ist mit 2,5 Millionen Tonnen Kapazität der wichtigste Seehafen der Demokratischen Republik Kongo. Er liegt nahe der Kongo-Mündung, hat jedoch den Nachteil, dass er aufgrund der geringen Tiefe des Flusses nur von kleinen Schiffen erreicht wird, womit er vom Umladen in Pointe-Noire abhängt. Während Matadi für den Westteil des Landes von hoher Bedeutung ist, liegen die Häfen für die Städte im Osten der Demokratischen Republik Kongo an der afrikanischen Ostküste: Mombasa für den Nordosten, Daressalam und Durban für den Südosten.

Der Hafen von Matadi ist auch im afrikanischen Kontext ineffizient bei gleichzeitig hohen Kosten, darüber hinaus muss er regelmäßig ausgebaggert werden. Dies gilt auch für die kleineren Häfen Boma und Banana. Der direkte Zugang zum Tiefseehafen Pointe-Noire ist für die Demokratische Republik Kongo durch den desolaten Zustand der Bahn- und Straßeninfrastruktur in der benachbarten Republik Kongo versperrt.

Angesichts der schlechten Straßen und Gleise hat der Schiffsverkehr auf den Flüssen die größte Bedeutung für das Land. Mehr als 15.000 km des Kongo und seiner Nebenflüsse sind schiffbar. Schlechte Wartung der Schiffe und nicht mehr funktionierende Leitsysteme führen jedoch immer wieder zu Unglücken mit zahlreichen Todesopfern.

Aufgrund des schlechten Straßensystems und der geographischen Größe des Landes kommt dem Luftverkehr erhebliche Bedeutung zu. Während des Krieges waren viele Städte nur per Flugzeug erreichbar, Reisen auf dem Landweg waren durch die Rebellenpräsenz zu gefährlich. Von großer Bedeutung ist der Luftfrachtverkehr, die abgebauten Bodenschätze werden vor allem im Osten des Landes auf dem Luftweg abtransportiert, weil die Straßen unter Rebellenkontrolle stehen. Im Land gibt es fast 200 Flugplätze, aber nur 26 mit befestigter Landebahn. Größter Flughafen ist der Flughafen Ndjili in Kinshasa, weitere internationale Flughäfen befinden sich in den Städten Lubumbashi, Bukavu, Goma und Kisangani.

Aufgrund schlechter Wartung und mangelnder Sicherheitskontrollen kam es in Kongo wiederholt zu Flugzeugunglücken, weshalb alle rund 50 kongolesischen Fluggesellschaften auf der schwarzen Liste der EU-Kommission stehen. Die einstmals größte Linie Hewa Bora musste 2011 nach einem Absturz ihren Betrieb einstellen. Viele Inlandsflüge werden von Kongolesen als Umsteigeverbindungen über das Ausland gebucht, um die einheimischen Luftlinien zu umgehen. Somit ist die Schaffung einer effizienten Aufsichtsbehörde über den Luftverkehr von oberster Dringlichkeit.

Die Demokratische Republik Kongo gewinnt elektrische Energie fast ausschließlich aus Wasserkraft. Die beiden größten Kraftwerke sind die zwei Inga-Staudämme am Unterlauf des Kongo. Sie gingen 1972 (Inga I) bzw. 1982 (Inga II) in Betrieb und versorgen sowohl die Hauptstadt Kinshasa als auch Bergbaubetriebe in Katanga mittels der HGÜ Inga-Shaba mit Strom.

Die Demokratische Republik Kongo hat das größte Wasserkraft-Potenzial Afrikas. Es beträgt 100 GW, ist kostengünstig zu erschließen und könnte neben dem Kongo selbst auch die Exportmärkte im südlichen Afrika versorgen. Bis dato ist das Potenzial jedoch weitgehend ungenutzt, im Jahr 2009 waren nur 2,4 GW Leistung installiert, die Vernachlässigung während der Bürgerkriege hat jedoch dazu geführt, dass nur 1 GW überhaupt einsatzbereit ist.
Der im Mai 2013 angekündigte Ausbau der Inga-Staudämme kann als Schritt in Richtung der Entwicklung des riesigen Potenzials verstanden werden.

In der Demokratischen Republik Kongo haben etwa 30 % der Bevölkerung Zugang zu Leitungswasser, meist öffentlich oder auch im eigenen Haus. Fast ein Viertel der Bevölkerung ist jedoch auf Oberflächen-Wasser angewiesen. Besorgniserregend ist, dass dieser Anteil steigt. Der Anteil der Bevölkerung, die nicht einmal Zugang zu einer Latrine hat, liegt bei einem Sechstel, auch dieser Anteil steigt. Der öffentliche Wasserversorger heißt "Regideso", er agiert bei weitem weniger effizient als seine Pendants in anderen afrikanischen Staaten. 40 % des Wassers gehen in seinem Netz verloren und nur 70 % des konsumierten Wassers wird bezahlt.

Das Telefonnetz des staatlichen Betreibers OCPT ist unzuverlässig und unzureichend, es gibt daher nur rund 10.000 Festnetzanschlüsse im ganzen Land. Trotz schwieriger wirtschaftlicher Rahmenbedingungen hat sich die Mobiltelefonie in der Demokratischen Republik Kongo schnell entwickelt. Im Jahr 2006 waren 65 % der Bevölkerung von einem GSM-Signal abgedeckt; aufgrund der schwierigen Topographie des Landes sind aber etwa 20 % ohne Subvention nicht wirtschaftlich erschließbar. Im Jahre 2009 hatten etwa drei von 100 Einwohnern der Demokratischen Republik Kongo einen Mobilfunk-Vertrag. Der lebhafte Wettbewerb zwischen den vier Anbietern führt zu niedrigen Preisen, wie auch in den Nachbarländern. Die Demokratische Republik Kongo ist erst seit 2012 an ein Unterseekabel angeschlossen, wodurch Internetzugang sehr teuer ist. Als Ergebnis dessen liegt der Anteil der Internet-Benutzer in der Demokratischen Republik Kongo noch weit unter dem afrikanischen Durchschnitt.

Die kulturellen Aktivitäten der Demokratischen Republik Kongo sind geprägt durch Synthesen von Musik, Tanz, Theater, Bildender Kunst und Film. Die traditionelle Tanzmusik wurde weltweit populär; sie wird vor allem von Musikgruppen der Hauptstadt Kinshasa mit experimentellen Musikformen, Video und Performances verknüpft.

Ein bekannter Autor war V. Y. Mudimbe, der die archaischen und gewaltsamen Strukturen der postkolonialen Stammesgesellschaft in der Zeit der politischen Wirren der 1960er Jahre beschrieb. Ins Englische übersetzt wurde sein Buch "Before the Birth of the Moon" (zuerst frz. 1976), ins Deutsche ein Erzählungsband. Als Lyrikerin und durch Kurzgeschichten wurde Clémentine Nzuji bekannt. Verschiedene Autoren emigrierten unter der Herrschaft Mobutus nach Kongo (Brazzaville) und Europa, so auch In Koli Jean Bofane, der seit 1993 in Belgien lebt und auch in Deutschland durch die Bücher "Warum der Löwe nicht mehr König der Tiere ist" und "Bibi und die Enten" bekannt wurde.

Trotz der in der Verfassung des Landes garantierten Informations- und Pressefreiheit ist die Pressefreiheit im Land laut „Reporter ohne Grenzen“ derzeit in einer „schwierigen Situation“. Die Organisation führt die Demokratische Republik Kongo im weltweiten Medienindex 2017 auf dem 154. von 180 Plätzen.

Die Medien im Land sind zum überwiegenden Teil im Besitz oder unter dem Einfluss politischer Gruppierungen. Die Journalisten sind finanziell von ihren Auftraggebern abhängig, eine Situation die unabhängige Berichterstattung auch ohne direkte staatliche Interventionen einschränkt. Die Qualität der Berichterstattung ist allgemein schlecht. Die Journalisten sind unzureichend ausgebildet, schlecht bezahlt, korrupt und durch ihre Auftraggeber in der Berichterstattung eingeschränkt. Kritische Journalisten werden bedroht, erpresst, verhaftet und gelegentlich ermordet, sodass Selbstzensur weit verbreitet ist. Urheberrechte werden selten beachtet.

Das Land hat drei bedeutsame Nachrichtenagenturen:

Das Radio ist das reichweitenstärkste Medium des Landes und ist auch im ländlichen Raum sehr verbreitet. 2007 gab es im Land 2 staatliche und über 200 private, lokale Radiosender. Die UNO betreibt das landesweit empfangbare Radio Okapi; daneben sind die ausländischen Sender BBC World Service und Radio France Internationale zu empfangen. RFI musste 2009 zeitweilig den Betrieb im Kongo einstellen, nachdem der Sender Kritik an der kongolesischen Armee geübt hatte. Ende 2012 wurde zeitweilig die Ausstrahlung von Radio Okapi unterbunden, laut Mutmaßungen infolge eines Interviews mit dem Präsidenten der Bewegung 23. März, offiziell jedoch aus administrativen Gründen.

Das Fernsehen wurde 1978 eingeführt und verbreitete anfangs Mobutus Propaganda, der sich als vom Himmel auf die Erde herabschwebender Halbgott darstellen ließ.
Heute gibt es neben dem staatlichen Radio-Télévision nationale congolaise (RTNC) bis zu 50 weitere, zumeist lokale, Privatsender wie Radio Télévision Groupe L’Avenir (RTG@). Generell ist das Programm aus Geldknappheit qualitativ eher schlecht, so werden zumeist Musik, Wiederholungen oder politische Reden ausgestrahlt.

Zeitungen sind mit einem Preis von etwa einem US-Dollar für die meisten Kongolesen unerschwinglich und daher wenig verbreitet. Grund für die hohen Preis ist der fehlende Anzeigenmarkt, wodurch sich die Zeitungen fast vollständig über den Verkaufspreis finanzieren müssen. Der Zeitungsmarkt konzentriert sich fast nur auf die Landeshauptstadt Kinshasa, der Vertrieb auf dem flachen Land ist mangels Infrastruktur zu teuer. In Kinshasa gibt es neun regelmäßig erscheinende Zeitungen, von denen sechs der Opposition und drei der Regierung zugewandt sind. Im ganzen Land dürfte es über 200 Zeitungen geben, die allerdings mitunter nur sehr unregelmäßig erscheinen.

Während laut einer 2013 veröffentlichten Studie 35 % aller Kongolesen ein Mobiltelefon besitzen, haben 2016 nur 3,9 % der Bevölkerung Zugang zum Internet. Dies liegt hauptsächlich an den extrem hohen Preisen. Eine verlässliche Internetflatrate kann 100 US-Dollar im Monat kosten, für die meisten Menschen mehr als ein Monatsgehalt. Wie viele Personen regelmäßig ein Internetcafé aufsuchen, ist unbekannt. Beobachter rechnen allerdings damit, dass nach der Fertigstellung des "West Africa Cable System", einem durch viele afrikanische Staaten verlaufenden See-Telekommunikationskabel von Südafrika nach Großbritannien, die Internetnutzung einen starken Anstieg erleben könnte.

Das Hauptnahrungsmittel in der Demokratischen Republik Kongo ist Maniok, dessen Wurzeln gekocht, gebraten, zu Brot oder Fufu-Brei verarbeitet oder als Atiéké konsumiert werden, weiters Taro, Mais und Reis; letztere vor allem in Kasai und Katanga. Die Blätter der Maniok-Pflanze werden ebenfalls konsumiert: "Pondu" ist ein im ganzen Land verbreitetes, häufig an Festtagen zubereitetes Gericht, bei dem feingeschnittene Maniok-Blätter gekocht und dann in Palmöl geschmort werden. Dazu isst man häufig gestampfte Erdnüsse.

Weiters sind die Kongolesen relativ große Konsumenten von Fleisch, neben Rindfleisch (vor allem in Kivu) sowie Geflügel-, Schweine- und Hammelfleisch kommen auch häufig Wildtiere wie Krokodil, Büffel, Schlange oder Insekten (Bushmeat) auf den Tisch. Bedingt durch die große Anzahl von Flüssen wird auch viel Fisch konsumiert, häufig getrocknet oder gesalzen. In der Regel ist das Essen scharf gewürzt, wobei Gewürze wie Chili, Ingwer, Knoblauch und Pfeffer, manchmal auch Koriander, Kümmel, Sesam, Muskat oder schwarzer Kardamom zum Einsatz kommen. Als Zwischenmahlzeit dienen oft Früchte wie Ananas, Bananen, Papayas, Mangos und Kokosnüsse.

Fremde Küchen haben auf die Kochkunst des Kongo wenig Einfluss gehabt; zu nennen ist hier jedoch der von den Portugiesen übernommene gesalzene Stockfisch.

Das Kunstzentrum des Landes ist Kinshasa, dort befindet sich Zentralafrikas einzige Kunstakademie universitären Niveaus, die "Académie des Beaux-Arts de Kinshasa". Die bekanntesten Künstler des Landes unterrichten hier. Neben der Galerie der Akademie wird Kunst im französischen und belgischen Kulturzentrum und in der Galerie "Symphonie des Arts" präsentiert, ebenso wie in den privaten Studios der größeren Künstler wie Claudy Khan, Henri Kalama Akulez und Lema Kusa.

Die Musik des Landes war schon immer sehr vielfältig. Sie hat einen langen Entwicklungsprozess zu ihrer heutigen Form hinter sich:

In der präkolonialen Zeit gab es in der Demokratischen Republik Kongo sehr viele verschiedene Arten der traditionellen afrikanischen Musik, welche von Region zu Region variierten und sich meist in religiösen Gesängen ausdrückten. Diese besaßen Tonsysteme mit fünf-, sechs- und siebentönigen Tonleitern.

Während der Kolonialzeit bildete sich dann in den 1920er Jahren eine größere Musikszene in der Koloniehauptstadt Léopoldville (heute Kinshasa). Sie bestand sowohl aus Kongolesen als auch aus westafrikanischen Ausländern wie den Hausa und französischen und US-amerikanischen Soldaten. So bildete sich nach und nach der Soukous-Musikstil heraus, welcher auch heute noch typisch für die Kongoregion ist. Neben dem Gesang waren die damals wichtigsten Instrumente Gitarre, Schlagzeug, Akkordeon und Klarinette. Nach und nach kamen Saxophone, Trommeln und später E-Gitarren hinzu. Es entstand auch ein Soukous-Tanz, welcher vor allem vom Rumba-Tanz inspiriert wurde. Der bekannteste kongolesische Sänger der 1950er Jahre war Wendo. Er veröffentlichte den Hit "Marie-Louise", der von vielen als Ausgangspunkt für die moderne kongolesische Musik gesehen wird.
Nach der Unabhängigkeit des Landes 1960 entstanden immer mehr kleinere Musikgruppen in Léopoldville, die das Musikgeschäft stetig wachsen ließen. Bands wie "African Jazz" und "OK Jazz" erreichten europaweite Bekanntheit und tourten vor allem durch Belgien. In den 1970er Jahren begann aber die Phase der "Zaiko-Generation", welche vor allem gitarrenlastig war und von Musikern wie Papa Wemba oder der Musikgruppe "Madilu System" vertreten wurde.

Auch heute noch treten die bekanntesten Musikgruppen des Landes auch international auf, doch der Musikstil hat sich weiter gewandelt: Neben der besonders in kongolesischen Diskotheken beliebten schnellen Soukous-Variante N’dombolo, zu der sehr körperbetont getanzt wird, gibt es erfolgreiche kongolesische Weltmusik-Gruppen. Zu ihnen zählt die Band "Staff Benda Bilili", die 2009 auf der World Music Expo den Künstler-Preis für Weltmusik gewann. Die Gruppe wurde wie das "Orchestre Symphonique Kimbanguiste" durch einen Dokumentarfilm bekannt. Beim Orchestre Symphonique Kimbanguiste handelt es sich um das einzige Symphonieorchester Zentralafrikas. Einem breiteren Publikum in Europa ist der kongolesische Sänger und Tänzer Jessy Matador bekannt, seit er für Frankreich beim Eurovision Song Contest 2010 auftrat. Er verkörpert die moderne kongolesische Popmusik.

Erste Filmstudios entstanden bereits in der Zeit der belgischen Kolonialherrschaft. Aufgrund des Mangels an finanziellen Mitteln und technischer Ausrüstung sind Filmproduktionen in der DR Kongo gering geblieben. Lediglich zwei kongolesische Regisseure konnten beim Panafrikanischen Film- und Fernsehfestival eine Auszeichnung gewinnen, nämlich Kwamy Mambu Nzinga und Mwenze Ngangura.

Der dominierende Sport in der Demokratischen Republik Kongo ist der Fußball. Obwohl die Stadien häufig in einem sehr schlechten Zustand sind, sind Fußballspiele in der Lage, eine große Anzahl an Zuschauern anzuziehen. Die größten Erfolge der Nationalmannschaft des Landes liegen indes schon weit zurück: Die Auswahl gewann die Afrikameisterschaften von 1968 und 1974; 1974 war das damalige Zaire zudem der erste schwarzafrikanische Teilnehmer bei einer Fußballweltmeisterschaft, blieb dort aber chancenlos. Angesichts der wenigen Aufstiegsmöglichkeiten versuchen die kongolesischen Fußballer, im Ausland bei einem Club anzuheuern. Zu jenen, die dabei Glück und Erfolg hatten, gehörten Muntubile Santos und Eugène Kabongo in den 1980er Jahren. Eine nationale Fußballliga gibt es in der Demokratischen Republik Kongo nicht. Der Versuch, eine landesweite Liga zu etablieren, wurde in den 1980er Jahren zwar unternommen, nach zwei Spielzeiten jedoch aufgegeben. Die Infrastruktur erwies sich als zu schwach, die Distanzen zu groß und die finanziellen Möglichkeiten zu gering. Der Landesmeister wird deshalb in regionalen Ligen ermittelt, deren beste Mannschaften im K.O.-System gegeneinander um die "Coupe du Congo" spielen. Der derzeit mit Abstand erfolgreichste Fußballverein des Kongo ist Tout Puissant Mazembe aus Lubumbashi. Der Club, der Mois Katumbi, dem reichen Gouverneur der Provinz Katanga, gehört, gewann in den Jahren 2009 und 2010 die CAF Champions League und zog 2010 als erste afrikanische Fußballmannschaft überhaupt ins Finale der FIFA-Klub-Weltmeisterschaft ein. Außer Mazembe konnten der AS Vita Club und der Daring Club Motema Pembe die CAF Champions League gewinnen. Andere bekannte Vereine sind der FC Bilima, FC Saint Eloi Lupopo und Lubumbashi Sport. Das mit einer Kapazität von 80.000 Plätzen mit Abstand größte Stadion des Landes ist das Stade des Martyrs. Dort tragen die Hauptstadtvereine Daring Club Motema Pembe und Inter Kinshasa Fußballspiele aus.

Weitere Sportarten spielen eine untergeordnete Rolle. International konnte die Basketballmannschaft der Damen auf sich aufmerksam machen. Des Weiteren genießen Boxen und Catchen eine gewisse Popularität.

1968 nahmen erstmals Athleten Zaires an den Olympischen Sommerspielen teil. Danach kam es 1984 wieder zu einer Teilnahme. Seitdem nehmen Athleten der DR Kongo und ihrer Vorgängerstaaten ununterbrochen an den Sommerspielen teil, ohne allerdings dabei eine Medaille errungen zu haben.

In der Zeit der Diktatur von Mobutu Sese Seko wurden Sportereignisse auch zu Propagandazwecken benutzt, um die Macht Mobutus zu stärken und dem Staat Zaire internationale Anerkennung zu sichern. Hierfür ist vor allem der Boxkampf "Rumble in the Jungle" zwischen George Foreman und Muhammad Ali zu nennen, der 1974 im Stade Tata Raphaël in Kinshasa stattfand. Es war das erste weltweit beachtete Sportereignis auf afrikanischem Boden.

Zur Erinnerung an die Unabhängigkeit von Belgien 1960 wird am 30. Juni der "Jour de l’Indépendance" gefeiert. Dies ist der Nationalfeiertag der Demokratischen Republik Kongo, insgesamt gibt es aber neben diesem eine Reihe weiterer gesetzlicher Feiertage, an denen die meisten öffentlichen Institutionen und Geschäfte geschlossen bleiben:





</doc>
<doc id="1186" url="https://de.wikipedia.org/wiki?curid=1186" title="Digitalfotografie">
Digitalfotografie

Als Digitalfotografie (Pendant zu Analogfotografie) wird die Fotografie mit Hilfe einer digitalen Fotokamera oder einer Kamera mit digitaler Rückwand bezeichnet.

Die technischen Grundlagen der Digitalfotografie weichen von der klassischen, optochemisch basierten Fotografie ab und ähneln, insbesondere bei der Bildwandlung, einerseits der Videotechnik, andererseits den bildgebenden Verfahren.

Nicht-digitale Fotos (Papierbilder, Negative, Dias) gescannt (digitalisiert) werden nicht als digitale Fotografie, sondern als digitale Bildbearbeitung bezeichnet.

Das Bestreben, Fotos elektronisch abzuspeichern, ohne den Umweg über Bild- oder Diascanner machen zu müssen, ist eng mit dem Aufkommen des Fernsehens im ersten Drittel des 20. Jahrhunderts verbunden. Fernsehbilder zeigten, dass es möglich ist, Bilder elektronisch zu übertragen und direkt von der Fernsehkamera auf den heimischen Apparat zu projizieren. Das große Problem stellte jedoch die nichtanaloge Speicherung dieser Bilder dar.

Russell Kirsch von NBS hatte schon 1957 den Digital-Scanner entwickelt. Das allererste derart gescannte Bild war ein Babyfoto seines neugeborenen Sohns Walden, 176 mal 176 Pixel. Auf diesen Ideen baute Steven Sasson in den frühen 1970er Jahren auf. 

Die erste Kamera, die als Vorreiter der Digitalkamera angesehen werden kann, wurde deshalb auch als „“ bezeichnet und war ein 1975 von Steven Sasson bei Kodak entwickelter Prototyp. Das Potential der Entwicklung wurde jedoch nicht erkannt, und so gilt gemeinhin die 1981 von Sony unter dem Namen Mavica vorgestellte erste kommerzielle Kamera nach demselben Funktionsprinzip als „Ur-Digitalkamera“. Allerdings arbeitete diese Kamera, wie der Name schon vermuten lässt, mit einem Magnetband (auch Video Floppy genannt), welches keine digitale Speicherung der Daten zuließ. Vorrangig in den USA brachten Kamerahersteller wie Canon, Nikon, Konica oder Fuji Weiterentwicklungen dieses Modells auf den Markt. In Europa war das Interesse an dieser Technologie eher verhalten.

Die erste wirkliche Digitalkamera stellte 1991 die kalifornische Firma Dycam auf der Computerfachmesse CeBIT unter dem Namen "Model 1" vor. Die Kamera war mit einem lichtempfindlichen CCD-Sensor sowie einem Speichermodul ausgestattet, das die direkte Übertragung der Bilder auf den Computer ermöglichte. Trotz des schwarz-weißen Aufnahmemodus’ und einer – aus heutiger Sicht geringen – Auflösung von 376 × 284 Bildpunkten war die Fachpresse begeistert. Das US-amerikanische Wirtschaftsmagazin Fortune wagte sogar folgende Prognose: „Ein Sturm technologischer Innovationen und neuer Produkte sammelt sich über der Welt der Fotografie an, der viel von dem wegblasen wird, was bis heute altbekannt ist. Filme, Chemikalien und Dunkelkammer werden ersetzt werden durch eine Technologie, die blendend und altbacken zugleich ist: den Computer.“

Auf der photokina, einer internationalen Fachmesse für die Photo- und Bildbearbeitungsbranche in Köln, präsentierten 1992 nahezu alle namhaften Firmen aus den unterschiedlichsten Bereichen ihre Prototypen. Neben klassischen Kameraherstellern wie etwa Kodak und Rollei waren der Videogigant Sony und Leaf ebenfalls mit Digitalkamerastudien vertreten, denn das Schlagwort „“ verkündete für alle die Entstehung eines neuen Marktes. Nur zwei Jahre später lautete das Motto der photokina „digital total“ und machte deutlich, wohin die zukünftige Entwicklung gehen würde. 1994 wird auch als das „offizielle“ Startjahr der Digitalen Fotografie in Deutschland angesehen, da die Vogelsänger-Studios den Einsatz von Digitalkameras bekannt gaben. Diese Mitteilung hatte deshalb eine besondere Relevanz, weil die Vogelsänger-Studios – ein großes, europäisches Fotostudio im Bereich Interieurfotografie – für ihren hohen Qualitätsanspruch an Bilder, Bildermacher und Handwerkszeug bekannt sind. Indem einer der Branchenführer im Bereich der Werbefotografie auf digitale Kameratechnik setzte, machte er hierzulande den Weg für die Digitalkamera frei. Allerdings übten sich die Verbraucher bei einem anfänglichen stolzen Preis für die ersten Modelle von ca. 2.000 DM (nach heutiger Kaufkraft rund Euro) in Zurückhaltung, und so blieb der Kundenkreis für die neuen Kameras in den Folgejahren in überschaubarem Rahmen.

Ebenfalls im Jahre 1994 tätigten PC- und Fotoexperten folgende Analyse: „Für den oft zitierten Otto Normalverbraucher dürfte die Digitale Fotografie erst dann interessant werden, wenn namhafte Einzelhandelketten einfachst zu handhabende Digitalkameras als Massenware in ihren Regalen feilbieten und der Fotohandel gleichzeitig die Möglichkeit bietet, von den elektronischen Aufnahmen preisgünstige Papierbilder herzustellen – und dies wird aller Wahrscheinlichkeit nach noch eine geraume Zeit dauern.“

In der Digitalfotografie werden zur Wandlung der Lichtwellen in digitale Signale Halbleiter-Strahlungsdetektoren in CCD- oder CMOS-Technik als Bildsensoren verwendet. Bei dieser Digitalisierung eines analogen Bildes handelt es sich um eine Bildwandlung, bei der eine Diskretisierung (Zerlegung in Bildpunkte) und Quantisierung (Umwandlung der Farbinformation in einen digitalen Wert) des analogen Bildes durchgeführt wird.

Eine Übergangslösung zwischen analoger und digitaler Fotografie stellt die Fotografie mit dem klassischen „Silberfilm“ dar, bei der anschließend das Negativ oder Positiv zunächst mit einem Scanner digitalisiert wird und dann das gespeicherte Bild digital weiterbearbeitet wird.

Als kostengünstigere Variante können – etwa seit 1999 – sogenannte „hochaufgelöste“ (Eigenwerbung) Scans gemeinsam mit der Filmentwicklung bestellt werden. Auf der gelieferten CD sind die Aufnahmen mit geringerer Auflösung im verlustbehafteten JPG-Format gespeichert. Die Qualität dieser Scans ist nur für die Betrachtung am Monitor, aber nicht für eine Weiterverarbeitung geeignet.

Aufgrund der Architektur der Bildaufnehmer ist zwangsläufig eine Interpolation der Farb- und Helligkeitswerte (sog. ) notwendig um ein Bild anzeigen zu können.
Diese Berechnung und eine Reihe von weiteren Bild verändernden Verarbeitungsprozessen wie das Bestimmen des Weißabgleiches, Erhöhung der Farbsättigung, Anheben des Kontrasts, Durchführung einer Tonwertkorrektur, Filterung (die u. a. eine Rauschreduktion bewirken kann), Verbesserung des Schärfeeindrucks und ggf. eine verlustbehaftete Komprimierung übernimmt die Kameraelektronik und die Firmware der Kamera, wenn direkt auf die Speicherkarte eine JPEG-Bilddatei (oder ein vergleichbares Dateiformat) gespeichert werden soll.

Die kamerainterne Bildverarbeitung kann bei hochwertigen Kameras umgangen werden, indem direkt die Metadaten und die Bild gebenden Sensordaten in einer sog. RAW-Datei abgespeichert werden; dabei handelt es sich um ein Rohdatenformat, das von Hersteller zu Hersteller unterschiedlich aufgebaut ist. Dieses wird oft als „digitales Negativ“ bezeichnet. Bei der RAW-Konversion, die Teil der Postproduktion am Rechner ist, wird dann aus den im Rohdatenformat gespeicherten Messwerten ein Bild interpoliert und die oben beschriebenen Bild verändernden Bearbeitungsschritte (bei Bedarf vom Nutzer „manuell“) durchgeführt.

Bei digitalen Kompaktkameras hat der Sensor ein Seitenverhältnis von 1,33 (4:3), daher werden die Bilder standardmäßig auch mit diesem Seitenverhältnis gespeichert. Teilweise ist auch die Speicherung mit anderen Seitenverhältnissen möglich, dies erfolgt überwiegend durch Speicherung eines Bildausschnitts.

Diese Praxis hatte ursprünglich historische Gründe: Die ersten Digitalkameras waren auf existierende Sensoren angewiesen, und da 4:3 dem Seitenverhältnis der verbreiteten Computermonitore und Fernsehnormen NTSC, PAL und SECAM entspricht (was wiederum von den frühesten Kinofilmen herrührt), waren überwiegend Sensoren mit diesem Seitenverhältnis verfügbar.

Digitale Kamerasysteme dagegen verwenden oft Bildsensoren mit dem Seitenverhältnis 3:2, das dem des Kleinbildfilms entspricht. Ausnahme sind hierbei die Kameras der Four-Thirds- und Pentax-Q-Systeme, die das Seitenverhältnis 4:3 verwenden. Viele Kameras des vom Four-Thirds-Standard abgeleiteten Micro-Four-Thirds-Systems sowie einzelne Kompaktkameras ermöglichen die Auswahl verschiedener Seitenverhältnisse, wobei immer ein Ausschnitt aus einer Sensorfläche genutzt wird, die insgesamt über den Bildkreis der Objektive hinausreicht. Hierdurch wird ein Auflösungsverlust, wie er durch reinen Beschnitt entstehen würde, vermindert.

Die Anzahl der Bildpunkte, Pixel genannt, wird vom Hersteller einerseits als rein technische Eigenschaft des Sensors und andererseits als nutzbare Pixelanzahl angegeben. Letztere entspricht üblicherweise der maximal möglichen Bildauflösung der Kamera. Beim in den meisten Fällen verwendeten Sensor des Bayer-Typs handelt es sich hierbei jedoch um Pixel, die mit unterschiedlichen Farbfiltern versehen sind und daher nur für Ausschnitte des Lichtspektrums empfindlich sind. Die fehlenden Farbinformationen werden aus den umgebenden Pixeln interpoliert. Beim Bayer-Sensor hat die Hälfte der Pixel grüne und je ein Viertel blaue und rote Farbfilter. Varianten, bei denen die Hälfte der grünen Farbfilter durch türkisfarbene ersetzt wurden, haben sich nicht durchgesetzt. Dies gilt ebenso für den Xenia-Sensor, der die Primärfarben Gelb, Cyan und Magenta verwendete.

Bei Bayer-Sensoren mit abweichenden Pixelanordnungen (z. B. rechteckige Pixel bei der Nikon D1X oder diagonal angeordnete Pixel beim Super-CCD-Sensor von Fujifilm) werden die Bilder zwar mit der Pixelanzahl ausgegeben, die der tatsächlichen Anzahl der Pixel entspricht, jedoch besteht hier keine eindeutige Relation von Sensor-Pixel und Bild-Pixel mehr. Super-CCD-Sensoren enthalten teilweise zusätzliche farbunempfindliche Pixel, die nicht zur Bildauflösung beitragen, sondern zur Erhöhung des Dynamikumfangs dienen.

Von der Pixelanzahl her nicht unmittelbar vergleichbar sind die Foveon-X3-Sensoren, da bei diesen die Flächen unterschiedlicher Farbempfindlichkeit übereinander angeordnet sind. Hier hat also jeder Pixel volle Farbempfindlichkeit, das Interpolieren der Farben entfällt. Zu beachten ist hierbei allerdings, dass aus Marketinggründen die Pixelanzahl oft bereits verdreifacht angegeben wird. Zurzeit wird der Sensor nur von Kameras der Marke Sigma verwendet.

Die Pixelanzahl allein erlaubt noch keine Aussage zur erreichbaren Auflösung, da hierfür auch die Qualität des verwendeten Objektivs wichtig ist. Bei Ausgabe des Bilds im JPEG-Dateiformat wirkt sich zudem die Aufbereitung der Bilddaten in der Kamera auf die Auflösung aus. Insbesondere bei digitalen Kompaktkameras und Mobiltelefonen bleibt die tatsächliche Bildauflösung oftmals deutlich hinter der sich aus der Pixelanzahl ergebenden theoretischen Auflösung zurück. Die Ursache hierfür liegt in den geringen Sensorabmessungen und den üblicherweise verwendeten Objektiven einfacher Bauart und daher begrenzter Abbildungsleistung.

Die Auflösung digitaler Bilder ist nur eingeschränkt mit der Auflösung eines Filmnegativs oder Abzugs zu vergleichen, da es je nach Ausgabemedium zu Verlusten kommen kann. Zudem wird die heute erreichbare Auflösung bei üblichen Ausgabegrößen wie dem Druck bis Postkartengröße oder Vollbilddarstellung an Bildschirmen bei weitem nicht ausgenutzt.

Die Pixelanzahl gibt nicht unbedingt die Auflösung feiner Strukturen wieder. Bei der Digitalisierung gilt das Nyquist-Shannon-Abtasttheorem. Danach darf die maximale im Bild auftretende Frequenz formula_1 maximal halb so groß sein wie die Abtastfrequenz formula_2, weil es sonst zu unerwünschten Bildverfälschungen, zum Beispiel zu Moiré-Effekte, kommt und das Originalsignal nicht wiederhergestellt werden kann.

Eine weitere Einschränkung der Vergleichbarkeit konventioneller und digitaler Aufnahmen ergibt sich aus der Tatsache, dass es sich beim Filmkorn - technisch betrachtet - um ein "stochastisches", also ein völlig zufälliges und unregelmäßiges Rauschen handelt, das bei technisch gleicher Auflösung meist weitaus weniger störend wirkt als das Rauschen im strikt regelmäßigen Pixelmuster digitaler Aufnahmen. Visuell wirken somit „analoge“ Bilder mit sichtbarem Korn - bei gleichem Informationsgehalt - entweder erträglicher oder gestört.

In der Praxis bedeutet das, dass man vor der Digitalisierung die maximale Frequenz kennen oder herausfinden muss und dann das Signal zwecks Digitalisierung mit mehr als der doppelten Frequenz abgetastet werden muss. Bei der Digitalfotografie kann man, um die Moiré-Effekte von vornherein zu vermeiden, die Optik leicht unscharf stellen. Das entspricht einer Tiefpass-Filterung. Wenn die Pixelzahl des Sensors erhöht wird, muss die Optik neu angepasst werden, weil sonst die erhöhte Pixelzahl nicht ausgenutzt werden kann. In der Praxis wird auch ein sog. Moiré-Filter benutzt, der im Strahlengang sitzt und somit die Nutzung perfekt abgestimmter Optiken ermöglicht.

Beim Scannen gerasterter Bilder muss man die Auflösung ebenfalls so groß wählen, dass die feinsten Strukturen des Rasters dargestellt werden können. Anschließend kann man entrastern (dazu gibt es unterschiedliche Funktionen) und dann die Auflösung herabsetzen.

Die bei der Digitalfotografie entstehenden Bilder, die in Form digitaler Daten vorliegen, werden in der Regel elektronisch, elektromagnetisch oder optisch gespeichert; jedem Bild entspricht dabei i. d. R. eine Datei, die meist in einem standardisierten Grafikformat abgespeichert ist. Aktuelle Digitalkameras verwenden JFIF (JPEG-Komprimierung), einige besser ausgestattete auch das Rohdatenformat und TIFF. Bei den Hybridverfahren wie der Kodak Photo CD entstehen "ImagePacs". Beim Scannen analoger Vorlagen hat man meist freie Auswahl über das digitale Speicherformat.

Für maximale Bildqualität in der Nachbearbeitung empfiehlt sich das Rohdatenformat. Früher wurden die Bildsensordaten unkomprimiert gespeichert, ab 2005 setzten sich Lossless-Kompressionen infolge stärkerer Prozessoren durch. Dieses Format bedarf jedoch deutlich größerer Mengen Speicherplatz und wird insbesondere im professionellen Umfeld angewendet.

JPEG ist dagegen verlustbehaftet, kann aber je nach Kompressionsgrad sehr speicherökonomisch, unter günstigen Umständen aber auch sehr nah am Original sein. JPEG2000 beherrscht mittlerweile die verlustlose Komprimierung und einen größeren Farbraum, wird aus Lizenzgründen aber kaum unterstützt. Der Fotograf muss also bereits "vor" dem Fotografieren eine Entscheidung über den Kompressionsgrad und damit über den möglichen Detailreichtum fällen. Eine vergleichbare Vorabentscheidung trifft der analog Fotografierende mit der Auswahl des Filmmaterials, und er muss selbiges wechseln, um beispielsweise eine andere Lichtempfindlichkeit oder Filmkörnigkeit zu erreichen.

Es gibt nach wie vor viele proprietäre Dateiformate, die nicht mehr ohne weiteres gelesen werden können, wenn die entsprechende Software nicht verfügbar ist. Rohdatenkritiker merken an, dass man entsprechend darauf reagieren muss, z. B. diese zu konvertieren (Umwandlung in ein offenes oder verbreitetes Dateiformat, wie beispielsweise Digital Negative (DNG)) oder die damalige Bearbeitungs-/Entwicklungssoftware zu sichern.

Zu den Vorteilen der digitalen Bildspeicherung gehört die Möglichkeit, umfangreiche Meta-Informationen (oder auch Metadaten) in der Datei zu speichern; diese Zusatzfunktion ist im Exchangeable Image File Format (Exif) standardisiert und wird zumindest mit Basisdaten von allen Digitalkameras realisiert.

Die Option der Speicherung von GPS-Positionsdaten bei Aufnahme (Georeferenzierung) in den Metadaten ist nur mit entsprechend ausgestatteten Kameras möglich, die hierfür in der Regel auf den Anschluss eines externen GPS-Empfängers angewiesen sind. Die entsprechenden Felder in den Metadaten können aber auch händisch oder durch entsprechende Programme ausgefüllt werden. Einige Kameras verfügen zusätzlich über einen integrierten Kompass, welcher die Blickrichtung der Fotos abspeichert.

Bereits das Hybridsystem APS verfügte über noch vergleichsweise eingeschränkte Möglichkeiten der Speicherung von Meta-Informationen, und auch bei Kleinbildkameras ist das Einfügen von Zeit- und Datumsangaben sowie der Bildnummer auf den Filmstreifen möglich, wenn die Kamera über eine entsprechende Funktion verfügt. Einige Kleinbild-Spiegelreflexkameras verfügen über eine Möglichkeit, zahlreiche Aufnahmeparameter zu speichern und in eine Textdatei ausgeben zu können; allerdings ist die Verknüpfung dieser Daten mit den gescannten Bilddateien ausschließlich händisch möglich.

Bei den in die digitale Bilddatei eingebetteten Exif-Daten ist zu beachten, dass einige Programme diese Daten bei einer Bildbearbeitung nicht erhalten; dies betrifft z. B. ältere Versionen der Bildbearbeitungssoftware Adobe Photoshop.

Analoge Kameras und Kamerasysteme wurden über Jahrzehnte entwickelt und optimiert bevor ihre Weiterentwicklung bei den marktführenden Herstellern in den letzten Jahren eingestellt wurde.

Die Bedienung der meisten analogen Kleinbildkameras war ähnlich – wobei Autofokus, Intervalometer, Belichtungsmessung etc. je nach Hersteller deutlich variierte. Die Benutzung von Tasten und Menüsystemen bei Digitalkameras kann deutlich umfassender und komplexer sein und erfordert weiteres Wissen über die Photochemie hinaus - da viele digitale Kameras zahlreiche Funktionen mehr bieten als ihre mechanischen Vorgänger. Bei der Digitalfotografie ist damit zu rechnen, dass der Fotograf bei jedem Systemwechsel neue Dinge erlernen kann, während die Grundlagen stets gleich bleiben – wie Blende, Brennweite, Verschlusszeit etc.

Die Kompatibilität der Modelle untereinander ist stark unterschiedlich. Sie ist zum einen herstellerabhängig, modellreihenabhängig und – gerade bei einfacheren Nicht-Spiegelreflexmodellen – oft nicht oder kaum gegeben.
Einige Hersteller führten vollkommen neue digitale Kamerasysteme ein.

Digitale Bilder können nicht nur mit nativen Digitalkameras oder durch Digitalisieren analoger Vorlagen, sondern auch mit einer digitalen Kamerarückwand angefertigt werden.

Scanbacks funktionieren nach dem Prinzip eines Flachbettscanners; es wird dabei zwischen Single-shot- und Multi-Shot-Verfahren unterschieden.

Bei heutigen Digitalkameras sind meistens Bildsensoren mit einer gegenüber den klassischen Filmformaten geringeren Aufnahmefläche verbaut. Aufgrund des kleineren Bildformates verkleinert sich bei vorgegebener Brennweite der Bildwinkel eines Objektivs, und die Schärfentiefe wird bei vorgegebener Blendenzahl kleiner. Dies bedeutet, dass ein Objektiv mit einer Brennweite, das bei Kleinbildfilm als Normalobjektiv eingesetzt wird, bei einer Digitalkamera mit kleinerem Aufnahmesensor den Bildwinkel eines Teleobjektivs hat. Bei gleichem Bildwinkel und gleicher Blendenzahl vergrößert sich der Bereich der Schärfentiefe.

Das Verhältnis von Normalbild-Diagonale und tatsächlicher Diagonale des Aufnahmesensors wird als „Formatfaktor“ bezeichnet und wird in der Regel im Datenblatt der Kamera beziehungsweise des Objektivs angegeben. Er beschreibt, mit welcher Zahl man die tatsächliche Brennweite des Objektivs einer Kamera multiplizieren muss, um für das Kleinbildformat ein Objektiv mit gleichem Bildwinkel zu erhalten. Hat der Aufnahmesensor beispielsweise die Größe 12 mm × 18 mm, also die halbe Diagonale des Kleinbildformats 24 mm × 36 mm, beträgt der Formatfaktor 2. Ein Objektiv von 25 mm hat bei dieser Digitalkamera den gleichen Bildwinkel wie ein 50-mm-Objektiv bei Kleinbild, und es resultiert bei gleicher Blendenzahl die gleiche Belichtungszeit. Wird die Blendenzahl durch den Formatfaktor dividiert, resultiert die Blendenzahl, bei der die gleiche Schärfentiefe erreicht wird.

Die digitale Aufnahmepraxis weist gegenüber der konventionellen Fotografie einige Besonderheiten auf.

Als Beispiel sei hier die Veränderung der Schärfentiefe erwähnt, die sich aus dem Formatfaktor ergibt (oft fälschlich Brennweitenverlängerung genannt: Die Brennweite eines Objektivs ändert sich jedoch nicht, nur dessen genutzter Bildwinkel durch das veränderte Aufnahmeformat); Objektive, die in der Kleinbildfotografie als Weitwinkel gelten, treten bei den meisten Digitalkameras als Normalobjektiv auf. Da sich die optischen Gesetzmäßigkeiten nicht verändern, nimmt die effektive Schärfentiefe (genauer: der "Schärfebereich") des Bildes zu. Mit Digitalkameras ist es daher schwerer als in der Kleinbildfotografie, einen in Unschärfe zerfließenden Bildhintergrund zu erzielen, wie er beispielsweise in der Porträt- und Aktfotografie aus gestalterischen Gründen häufig erwünscht ist. Einige moderne Spiegelreflex-Digitalkameras verfügen bereits über einen Vollformatsensor (engl. Full Frame) (24 mm × 36 mm). Diese Kameras verhalten sich genauso wie analoge Kleinbild-Spiegelreflexkameras.

Viele Digitalkameras bieten dreh- oder schwenkbare Displays, mit denen einige Aufnahmetechniken komfortabler als mit herkömmlichen Kameras machbar sind. Hierzu gehören beispielsweise Aufnahmestandpunkte in Bodennähe, wie sie häufig in der Makrofotografie benötigt werden, oder Aufnahmen „über Kopf“, um über eine Menschenmenge hinweg zu fotografieren.
Die Nachteile der Displays liegen im hohen Stromverbrauch und der mangelnden Sichtbarkeit in hellen Umgebungen (helles Tageslicht).

Aktuelle Digitalkameras bieten fast ausnahmslos die Möglichkeit der Aufzeichnung kurzer Videoclips von etwa einer Minute in unterschiedlichen Formaten von QQVGA oder QVGA bis hin zu WUXGA, in der Regel auch mit Ton. Tendenziell ist eine Entwicklung der digitalen Fototechnik zu beobachten, immer weiter mit der Videotechnik zu konvergieren; in Spitzenmodellen ist die Länge der Videoclips nur noch durch die Kapazität des Speichermediums begrenzt; die Bildauflösung liegt dabei im Bereich der Qualität von VHS bis hin zu Blu-ray (VGA, 640 × 480 bzw. PAL, 720 × 576 bzw. Full HD, 1920 × 1080 bis UHD (4K) 4096 × 2160 Pixeln).

Neben der automatisch durch die Kamera durchgeführten Bildverarbeitung eröffnet die Digitalfotografie zahlreiche Möglichkeiten der Bildmanipulation und -optimierung durch die elektronische Bildbearbeitung, die über konventionelle Bildretusche und Ausschnittvergrößerung weit hinausgehen.

Beispielsweise können aus einer Folge von Einzelbildern komfortabel Panoramafotos montiert, Bildhintergründe ausgetauscht oder Personen aus Bildern entfernt oder hineinkopiert werden.

Als Speichermedien werden in der Digitalfotografie üblicherweise Speicherkarten verwendet. Weit überwiegend sind dies SD-Karten (Secure Digital Memory Card, auch als Typen SDHC und SDXC). Nur noch geringe Bedeutung haben firmenspezifische Kartentypen wie Memory Stick (Sony) und xD-Picture Card (Fujifilm und Olympus). Die etwas größeren CompactFlash-Karten (CF) waren lange Zeit Standard, werden inzwischen aber nur noch für wenige hochwertige Spiegelreflexkameras benötigt. Zeitweise waren Microdrives eine kompatible Alternative für größere Speicherkapazitäten zu CompactFlash-Karten. Mobiltelefone mit Kamerafunktion speichern üblicherweise auf microSD-Karten.

In der Anfangszeit der Digitalfotografie wurden PC-Karten verwendet, diese sind ebenso wie Kameras für SmartMedia-Karten jedoch vollständig vom Markt verschwunden.

Digitale Kompaktkameras haben zudem häufig einen internen Speicher, der die Speicherung einer geringen Anzahl von Bildern ohne Speicherkarte ermöglicht.

Bei einigen digitalen Spiegelreflexkameras ist mit entsprechender Software auch die Fernsteuerung von einem Computer aus möglich. Die Speicherung kann dann direkt auf der Festplatte des Computers erfolgen, eine Speicherkarte wird dann nicht benötigt. Die Verbindung zwischen Computer und Kamera erfolgt entweder durch USB- oder SCSI-Kabel oder über WLAN. Bei einigen Kameras ist ebenfalls möglich, die Bilddateien über WLAN auch ohne Fernsteuerung zu versenden.

Speicherkarten werden üblicherweise nur zur vorübergehenden Speicherung bis zur Übertragung der Bilddateien auf einen Computer verwendet. Sie werden anschließend formatiert und stehen dann wieder zur Verfügung. Für den Fall, dass größere Datenmengen anfallen, kann der Inhalt der Speicherkarten zunächst auf Image Tanks übertragen werden, die teilweise auch eine Anzeige der Bilder ermöglichen. Von den Image Tanks werden die Dateien später auf den Computer übertragen.

Durch die Möglichkeit der Fernsteuerung und durch die Möglichkeit der Speicherung großer Bildmengen hat die Digitalfotografie schon früh Einsatz unter extremen klimatischen Bedingungen gefunden, wie beispielsweise im Weltall, Wüsten oder Polargebieten.

Für die langfristige Speicherung von Bilddaten gelten grundsätzlich die gleichen Anforderungen, die generell auf die Archivierung digitaler Daten zutreffen. Als weiteres Problem kommt bei Bilddateien hinzu, dass bei Verwendung von RAW-Formaten die langfristige Lesbarkeit der Dateien nicht sichergestellt ist. Bisher (Stand 2011) sind jedoch noch für alle jemals verwendeten RAW-Formate aktuelle Programme verfügbar, mit denen die Bilddateien geöffnet und weiterverarbeitet werden können.

Während beim Film ein beschädigtes Original verwendet werden kann, ist dies bei digitalen Daten in der Regel nicht oder nur mit hohem technischen Aufwand möglich. Der Hauptvorteil digitaler Daten ist, anders als beim chemischen Film, dass beliebig viele identische Kopien erzeugt werden können. Auch der Transport digitaler Daten ist wesentlich unkomplizierter.

Analog zur konventionellen Fotografie gibt es die Möglichkeit eines Index-Prints, in Form von Thumbnails in einem Ordner. Spezielle Programme zum Auffinden von archivierten Bilddateien erleichtern die Suche nach Bildern, die in der „analogen Welt“ einem gut gewartetem Negativsortiersystem entspricht. Während in der analogen Fotografie Kontaktabzüge noch zum normalen Arbeitsablauf gehörten, sind diese Techniken - auch bei Speicherung im Rohdatenformat - im Betriebssystem integrierbar. Ein Leuchttisch wird damit überflüssig.

Die so genannten Bilddatenbanken erzeugen ein Vorschaubild des Bildes und bieten Felder zur Beschreibung des Bildes und der Aufnahmesituation; ein gewisser Komfort ergibt sich durch die Metadaten, die durch das Exif-Format automatisch aufgezeichnet werden (Datum, Uhrzeit, Brennweite, Blende etc.). Viele dieser Funktionen sind in aktuellen Betriebssystemen bereits enthalten. Für ambitionierte Fotografen oder Berufsfotografen sind Online-Fotoagenturen geeignete Plattformen, um ihre Fotos zu speichern und von dort direkt an die Käufer (Zeitungen, Verlage, Redaktionen etc.) zu vertreiben. Entsprechend große Server und Speicherplätze sind jedoch Voraussetzung. Darüber hinaus ist eine „Verschlagwortung“ mit passenden Schlüsselworten möglich, um aus den Datenbanken entsprechende Bilder zu finden. Bedingt durch den Vorteil der Rechentechnik dauert dies nur einen Bruchteil der für Bildmaterialsuche analoger Aufnahmen benötigten Zeit. Zur Verschlagwortung werden die im Bild gespeicherten IPTC-Felder genutzt.

Digitale Bilder können ebenso präsentiert werden wie konventionelle Fotografien; für nahezu alle Präsentationsformen existieren mehr oder minder sinnvolle Äquivalente. Die Diaprojektion vor kleinem Publikum wird beispielsweise ersetzt durch die Projektion mit einem Videoprojektor ("Video-Beamer"); das Fotoalbum durch die Webgalerie; das gerahmte Foto durch ein spezielles batteriebetriebenes Display usw.

Wird eine erneute Bildwandlung (D/A-Wandlung) in Kauf genommen, können digitale Bilder ausgedruckt oder ausbelichtet werden und anschließend genauso wie konventionelle Papierabzüge genutzt werden; sogar die Ausbelichtung auf Diafilm ist möglich.

Allerdings erfordern alle derzeitigen digitalen Präsentationsformen ausreichende Technikkenntnisse sowie recht kostspielige Technik; der billigste Video-Beamer kostet derzeit noch immer etwa das Fünffache eines guten Diaprojektors. Als weiteres neues Problem stellt sich das der Kalibrierung des Ausgabegeräts, was bei den meisten Monitoren, jedoch nur bei wenigen Flüssigkristallbildschirmen (LCDs) möglich ist und insbesondere bei Beamern einen erheblichen Aufwand verursachen kann.

Durch die enge Verwandtschaft der Digitalfotografie einerseits mit der Videotechnik und andererseits mit der Informations- und Kommunikationstechnik erschienen ab den 1980er Jahren eine Reihe von neuen Anbietern auf dem Fotomarkt, die ihr Know-how aus dem Bereich der Video- und Computertechnik gewinnbringend einsetzen konnten. Traditionelle Fotoanbieter gingen Kooperationen mit Elektronikunternehmen ein, um kostspielige Eigenentwicklungen zu vermeiden.

Der Digitalfotografie kommt in der Fotowirtschaft eine wachsende Bedeutung zu. So wurden nach Branchenschätzungen bereits 1999 neben 83 Milliarden analogen Fotografien schon 10 Milliarden Digitalbilder hergestellt. Der Branchenverband Bitkom berichtet, dass im Jahr 2006 circa 58 Prozent aller Deutschen über 10 Jahren eine Digitalkamera verwendeten.

Nach Angaben des Marktforschungsunternehmens Lyra Research wurden 1996 weltweit insgesamt 990.000 Digitalkameras abgesetzt. In Deutschland wurden im Jahr 2003 erstmals mehr Digitalkameras als analoge Kameras verkauft; nach Aussagen des Einzelhandels wurden 2004 bereits teilweise doppelt so viele digitale Geräte wie analoge Kameras abgesetzt. 2010 wurden nach Angabe des japanischen Branchenverbandes CIPA weltweit rund 121,5 Mio. Digitalkameras verkauft.

Neben der Ausbreitung der Digitalfotografie in den Massenmarkt gibt es einen Trend zum Zurückdrängen der analogen Fotografie. Seit etwa 2004 ist beispielsweise eine großflächige Verdrängung fotochemischer Produkte aus dem Angebot von Fotohändlern und Elektronikmärkten zu beobachten: So ging das Produktsortiment an fotografischen Filmen gegenüber dem Vorjahr deutlich zurück. Die Entwicklung neuer Materialien für die Fotografie auf Silberfilm bleibt dennoch nicht stehen. Insgesamt sind zwischen 2006 und 2008 23 neue oder verbesserte Filmemulsionen auf den Markt gekommen.






</doc>
<doc id="1187" url="https://de.wikipedia.org/wiki?curid=1187" title="Digitaltechnik">
Digitaltechnik

Digitale Schaltungen arbeiten mit digitalen Signalen, d. h. mit Signalen, die diskretisiert (zeitdiskret) wie auch quantisiert (wertediskret) sind. Sie stellen damit den Gegenpol zu analogen Schaltungen dar. Neben diesen gibt es noch gemischte Schaltungen (mixed signal) wie auch Schaltungen, die teilweise Eigenschaften reiner digitaler Schaltungen aufweisen, wie z. B. getaktete Analogschaltungen.

Die in der Praxis bedeutsamste Form stellt die binäre Digitaltechnik dar, die nur zwei diskrete Signalzustände umfasst. Diese werden üblicherweise als logisch null "(0)" und als logisch eins "(1)" bezeichnet.

Nichtbinäre digitale Schaltungen, dabei liegen mehr als zwei Wertzuständen vor, stellen beispielsweise MLC-Speicherzellen dar, wo pro MLC-Speicherzelle mehr als ein Bit an Information dargestellt und gespeichert werden kann. Außerdem findet die nichtbinäre Digitaltechnik im Rahmen der digitalen Signalverarbeitung wie bei digitalen Modulationsverfahren Anwendung.

Elektronische Bauelemente der Digitaltechnik sind beispielsweise Logikgatter, Mikroprozessoren und Datenspeicher. In der Analogtechnik kann ein Analogsignal beliebige viele Wertzustände annehmen, der Übergang findet in sogenannten Mixed-Signal-Schaltkreisen, wie beispielsweise Analog-Digital-Umsetzern bzw. Digital-Analog-Umsetzern, statt.

Die binäre Digitaltechnik, im Folgenden erfolgt eine Einschränkung auf diese, bedient sich des Dualsystems mit zwei möglichen Signalzuständen. Diese beiden Werte sind je nach Zusammenhang verschiedenartig bezeichnet. Beispiele für die Bezeichnung sind logisch null "(0)," "L" (), oder „falsch“. Das zweite Symbol wird üblicherweise als logisch Eins "(1)," "H" (), oder „Wahr“ bezeichnet. Wenn ein High-Pegel mit 1 und ein Low-Pegel mit 0 dargestellt wird, spricht man von positiver Logik, bei umgekehrtem Sachverhalt von negativer Logik.

Die Hauptkomponenten digitaler Schaltungen sind Logikgatter wie NOT und NAND sowie Kondensatoren als flüchtige Speicher, darauf aufbauend werden alle anderen Gatter, Zähler, Flipflops und so weiter aufgebaut. Komplexere Schaltungen stellen dann Speicherschaltkreise und Prozessoren dar. Bei der Digitaltechnik wird meist unter Verwendung der Schaltalgebra das Dualsystem (entsprechend obiger Ja-Nein-Unterscheidung) zugrundegelegt. So lässt sich für jedes Logikelement eine Schaltfunktion erstellen, die ihre Funktionsweise beschreibt. In der Praxis verwendet man meist nur NAND-Gatter, mit denen man die Funktionen der anderen Gatter nachbilden kann.

Digitale Schaltungen können zusätzlich zu logischen Funktionen auch zeitabhängige Bestandteile enthalten und ferner takt- oder zustandsgesteuert (synchron/asynchron) arbeiten. Enthält eine digitale Schaltung lediglich Logikelemente ohne Rückkopplung von Ausgängen auf Eingänge, so spricht man von einem Schaltnetz. Werden zusätzlich Speicher verwendet, oder mindestens ein Ausgang auf einen Eingang zurückgekoppelt, so handelt es sich um ein Schaltwerk oder auch einen Automaten. Ein Mikrocontroller oder Prozessor besteht hauptsächlich aus diesen Logikelementen und wird über einen Datenbus mit Speichern und anderen digitalen Baugruppen erweitert. Eine zeitlich gestaffelte Ausführung von Logikverknüpfungen ist möglich. Diese können festverdrahtet oder programmiert sein.

Weitere Vorteile der digitalen Signalverarbeitung gegenüber der analogen Technik liegen, neben den geringeren Kosten der Bauteile aufgrund hoher Integrationsdichte und vereinfachter Entwicklung, vor allem in der höheren Flexibilität. Mit Hilfe spezieller Signalprozessoren oder Computer können Schaltungen in Software realisiert werden. Dadurch lassen sich Funktionen leichter an veränderte Anforderungen anpassen. Außerdem sind komplexe Algorithmen einfach anwendbar, die analog nur mit hohem Aufwand oder gar nicht realisierbar wären.

In der Digitaltechnik können spezielle Entwicklerwerkzeuge im Rahmen des Computer-aided engineering (CAE) und Beschreibungssprachen wie Very High Speed Integrated Circuit Hardware Description Language (VHDL) oder Verilog bei der schnellen Entwicklung neuer Anwendungen und Schaltungen eingesetzt werden.

Die Anzahl der möglichen Bauelemente auf einem Chip ist in digitalen Schaltungen meist wesentlich größer als bei analogen Schaltungen. So besteht ein aktueller Mikroprozessor aus Milliarden von Transistoren, Operationsverstärker aus etwa 100 Transistoren und ein Leistungsdarlington-Transistor aus 2 Transistoren. Das wird jedoch durch eine hohe Integrationsdichte auf entsprechenden Chips kompensiert.




</doc>
<doc id="1188" url="https://de.wikipedia.org/wiki?curid=1188" title="Douglas Sirk">
Douglas Sirk

Douglas Sirk (* 26. April 1897 in Hamburg; † 14. Januar 1987 in Lugano, Schweiz; bürgerlich Hans Detlef Sierck) war ein deutscher Film- und Bühnenregisseur. Er floh wegen der Verfolgung durch die Nationalsozialisten 1937 aus Deutschland. Anschließend gelang ihm eine erfolgreiche Karriere in Hollywood, wo er in den 1950er-Jahren Film-Melodrame drehte, die stilbildend für das Genre wirkten und zahlreiche Rezensionen und Analysen in der Fachpresse erfahren haben.

Detlef Sierck verbrachte als Sohn eines Lehrers seine Jugend teilweise in Hamburg und in Dänemark. Nach dem Abitur verbrachte er das Ende des Ersten Weltkriegs als Seekadett bei der Reichsmarine. Ab 1917 studierte er unter anderem Rechtswissenschaft an verschiedenen Universitäten und arbeitete anschließend zunächst als Redakteur bei der "Neuen Hamburger Zeitung". Ab 1920/21 war er Hilfsdramaturg am Deutschen Schauspielhaus in Hamburg. Von 1923 bis 1929 war er Oberspielleiter am Bremer Schauspielhaus. Nach weiteren Engagements war er von 1929 bis 1935 Intendant des Alten Theaters in Leipzig. Obwohl als Gegner der Nationalsozialisten bekannt, erhielt er 1934 von der Ufa, die nach der Auswanderung vieler namhafter Künstler gute Regisseure suchte, einen Vertrag als Regisseur. 1935 drehte er seinen ersten Film und war in den darauf folgenden Jahren verantwortlich für den Aufstieg von Zarah Leander. Sein größter Erfolg in Deutschland war der Film "Schlußakkord". In erster Ehe war Sierck mit der Theaterschauspielerin Lydia Brincken († 1947), verheiratet, die auch nach der Trennung seinen Namen behielt, und hatte mit ihr einen Sohn, den Schauspieler Klaus Detlef Sierck. Die Ehe wurde 1934 geschieden. 1934 heiratete er Hilde Jary. Mit Rücksicht auf die als Jüdin verfolgte Hilde Jary verließ das Paar 1937 Deutschland und ging über die Niederlande zunächst nach Frankreich, um dann in die USA überzusiedeln.

In Hollywood, wo sich Detlef Sierck in "Douglas Sirk" umbenannte, versuchte er sich zunächst als Drehbuchautor. Erst 1943 gab das Filmstudio MGM ihm den ersten Regieauftrag. Sein erster Film in Hollywood war der Anti-Nazi-Film "Hitler’s Madman", ein Film über Reinhard Heydrich. Er gewann an Reputation mit dem elegant inszenierten Melodrama "Summer Storm" von 1944, das auf dem Stück "Ein Drama auf der Jagd" von Anton Tschechow basierte. Die Kritiker lobten die intelligente Umsetzung der Vorlage und besonders die sensible Führung der Schauspieler, darunter Linda Darnell in ihrer bislang besten Rolle. 1948 wurde er von Claudette Colbert persönlich ausgewählt, Regie bei dem Film Noir "Sleep, My Love" zu führen, in dessen Verlauf Don Ameche versucht, Colbert als ahnungslose Ehefrau in den Wahnsinn zu treiben. 1949 kehrte Sirk für kurze Zeit nach Deutschland zurück, kehrte jedoch bald zurück nach Hollywood, wo er schließlich bei Universal Pictures seine neue künstlerische Heimat fand. In den 1950er Jahren entwickelte er sich zu einem der erfolgreichsten Regisseure von Melodramen, die zu seinem Markenzeichen wurden. Gemeinsam mit dem Produzenten Ross Hunter drehte er ab 1953 einige der stilvollsten Filme des Genres, oft als Remakes von alten Universal-Klassikern. Nach einigen kleineren Filmen fand er mit "All meine Sehnsucht", der das Schicksal einer Frau mit dubioser Vergangenheit, gespielt von Barbara Stanwyck, schildert, zu seiner eigentlichen Formsprache als Regisseur. In seinen Filmen kämpft das Individuum um einen Platz für seine Gefühle gegen die konformistischen und restriktiven Verhaltenskodizes der Gesellschaft.

In Jane Wyman fand Sirk seine ideale Darstellerin für gefühlvoll geschilderte Frauenschicksale. Die beiden Filme "Die wunderbare Macht" von 1954 und "Was der Himmel erlaubt" von 1954 waren an der Kinokasse erfolgreich und fanden wohlwollende Aufnahme seitens der Kritiker. Daneben sorgten sie für den Aufstieg von Rock Hudson zum Topstar des Studios, der unter Sirks Regie seine besten darstellerischen Leistungen erbrachte. In den Folgejahren drehte Sirk mit "In den Wind geschrieben", "Battle Hymn", "The Tarnished Angels" und "Zeit zu leben und Zeit zu sterben" einige der besten Filme des Genres überhaupt. Besonders durch den letztgenannten Film, eine einfühlsame Adaption des gleichnamigen Romans von Erich Maria Remarque mit Liselotte Pulver, gewann Sirk den besonderen Respekt von Jean-Luc Godard und François Truffaut, die sich begeistert zeigten vom innovativen Einsatz neuer Techniken wie Cinemascope und Technicolor für die Schilderung auch sensibler und intimer Momente. 1959 drehte Sirk mit "Solange es Menschen gibt" mit Lana Turner und Sandra Dee in den Hauptrollen seinen letzten und finanziell erfolgreichsten Film. Der Streifen bot eine zurückhaltende Studie über Rassenvorurteile und die Unfähigkeit, Gefühle und Karriere zu vereinen. Juanita Moore wurde für ihre Darstellung einer aufopferungsvollen Mutter für einen Oscar als beste Nebendarstellerin nominiert.

Auf diesem Höhepunkt seines Erfolges in Hollywood verabschiedete sich Sirk aus Amerika und zog nach Lugano in der Schweiz. In den 1960er Jahren führte er sporadisch Regie bei einigen Theaterstücken in Deutschland, vor allem am Hamburger Thalia-Theater und am Münchener Residenztheater. Verschiedene Angebote, wieder bei einem Film Regie zu führen, lehnte er ab. Von 1974 bis 1978 unterrichtete er als Gastdozent an der Hochschule für Fernsehen und Film München, wo Rainer Werner Fassbinder einen seiner Kurse besuchte. 1978 erhielt er für sein Lebenswerk den Deutschen Filmpreis und 1986 den Bayerischen Filmpreis. Er starb 1987 im Alter von 89 Jahren in Lugano.

Sirk gehört zu den heute am meisten geschätzten Regisseuren der 1950er Jahre. Damals war dies jedoch anders: Zwar waren seine Filme beim Publikum beliebte Kassenerfolge, doch die meisten Kritiker verachteten sie als „schnulzig und kitschig“. Erst später fanden auch andere Seiten in Sirks Werk Würdigung: die hervorragende Kameraarbeit und Farbschönheit seiner Filme sowie die vielen Symbole, die er oft in seine Mise en Scène eingebaut hatte. Zugleich übte Sirk in seinen Werken immer wieder unterschwellig Kritik am repressiven Lebensstil und den strengen Gesellschaftsregeln in Amerika: „Sirks Melodramen handeln von Menschen, die in ihren Häusern und gesellschaftlichen Moralvorstellungen gefangen sind.“

Rainer Werner Fassbinder äußerte sich teilweise ekstatisch über die filmischen Qualitäten von Sirk und gab stets unumwunden zu, von seinem Werk beeinflusst worden zu sein. Auch Pedro Almodóvar und Kathryn Bigelow zählen ihn zu ihren Vorbildern. Wim Wenders nannte Sirk einen „Dante der Soap Operas“, der meisterhaft in der Lage gewesen sei, die mit dem American Dream verbundenen Schattenseiten in dramatischen Bildern zu vermitteln.

Das Filmfest Hamburg vergibt seit 1995 den Douglas-Sirk-Preis jährlich an eine Persönlichkeit, die sich um die Filmkultur und die Filmbranche verdient gemacht hat.




</doc>
<doc id="1189" url="https://de.wikipedia.org/wiki?curid=1189" title="Digital">
Digital

Digital oder digital (aus lateinisch "" „Finger“) steht für:

Siehe auch:


</doc>
<doc id="1191" url="https://de.wikipedia.org/wiki?curid=1191" title="Digital-Analog-Umsetzer">
Digital-Analog-Umsetzer

Ein Digital-Analog-Umsetzer (DAU, (DAC)), auch Digital-Analog-Wandler oder D/A-Wandler genannt, wird verwendet, um digitale Signale oder einzelne Werte in analoge Signale umzusetzen. DAUs sind elementare Bestandteile fast aller Geräte der digitalen Unterhaltungselektronik (z. B. CD-Player) und der Kommunikationstechnik (z.  B. von Mobiltelefonen). In der Regel wird der DAU als integrierter Schaltkreis (IC) ausgeführt.

Ein Analog-Digital-Umsetzer erzeugt aus einem kontinuierlichen Wertevorrat ein gestuftes Signal. Ein Digital-Analog-Umsetzer kann aus dem gestuften Signal nicht wieder ein kontinuierliches Signal erzeugen. Die einmal eingetretene Stufung in Schritten von 1 LSB (least significant bit) ist nicht wieder rückgängig zu machen. Bei einer Folge von veränderlichen Werten wird die Stufung allerdings durch eine notwendige Filterung verschliffen.

Ein Digitalsignal ist ein zeitdiskretes und wertdiskretes Signal, wie es nebenstehende Darstellung zeigt. Der Digital-Analog-Umsetzer setzt die quantisierten Informationen, die als binäre Information vorliegen, in ein Signal um, das kontinuierlich einem analogtechnisch arbeitenden Gerät bereitgestellt werden kann.

Bei einer Umsetzung in ein zeitkontinuierliches (aber noch wertdiskretes) Signal wird der Signalwert bis zum nächsten Abtastpunkt in einem Eingangsregister festgehalten. Bei einzelnen Messpunkten und bei langsam veränderlichen Größen entsteht am Ausgang ein Verlauf wie im zweiten Bild als waagerechte Strecken eingetragen.

Bei einer raschen Folge von Punkten mit unterschiedlichen Signalwerten sind aufgrund der Abtastpunkte für das entstehende analoge (also auch wertkontinuierliche) Signal vielfältige Verläufe möglich. Die punktierte Linie im zweiten Bild folgt den Abtastwerten, ähnelt aber dem Ursprungssignal nicht. Sie enthält höhere Frequenzanteile, welche üblicherweise durch Anti-Aliasing-Filter auf analoger Seite verhindert werden müssen. Die Speicherung der Abtastpunkte wird in diesem Fall vom Filter beherrscht.

Im nächsten Bild ist der Betragsverlauf des Frequenzspektrums eines DAU ohne Anti-Aliasing-Filter dargestellt, welcher eine Sinusschwingung mit der Frequenz "f" ausgibt. Diese Sinusschwingung tritt mehrfach in Oberschwingungen auf. Dabei ist "f" die Abtastfrequenz. Alle Signalanteile mit einer Frequenz oberhalb der halben Abtastfrequenz soll das Filter unterdrücken.

Durch die Quantisierungsstufen weist das Spektrum Verzerrungen auf, welche durch den rot-strichliert gezeichneten und einhüllenden Betragsverlauf der Sinc-Funktion bedingt sind. Dadurch kommt es auch unterhalb der halben Abtastfrequenz, also im erwünschten Frequenzbereich, zu einer Verzerrung und Absenkung der Amplituden. Diese linearen Verzerrungen werden durch zusätzliche Filter üblicherweise auf der digitalen Seite kompensiert, im Bild blau punktiert eingezeichnet. Dabei werden höhere Frequenzanteile unterhalb der halben Abtastfrequenz invers zur Sinc-Funktionsverlauf stärker angehoben.

Ist die Signalfrequenz deutlich niedriger als die Grenzfrequenz des Filters, nähert sich der Verlauf des Ausgangssignals dem gestuften Verlauf an. Die Stufung macht sich als Quantisierungsrauschen bemerkbar.

Da das dem DAU zugeführte Digitalsignal dimensionslos ist, muss es mit einem vorgegebenen Wert "U" multipliziert werden. Hier gibt es prinzipiell zwei Möglichkeiten.


Bei einem idealen Digital-Analog-Umsetzer besteht vorzugsweise ein linearer Zusammenhang zwischen Eingangs- und Ausgangsgröße. Es gibt

wobei daneben auch andere Kodierungen, beispielsweise Zweierkomplement, BCD-Code verwendbar sind.

Ferner gibt es DA-Umsetzer mit nicht linearer Quantisierungskennlinie z. B. nach dem logarithmischen A-law- und µ-law-Verfahren für Telefonnetze.

Zusätzlich zum Quantisierungsfehler sind weitere Fehler zu beachten.

Als Abweichungen der Kennlinien zwischen realem und idealem Umsetzer sind folgende Fehler definiert (siehe Bild):
Der Verstärkungsfehler wird oft als Bruchteil des aktuellen Wertes angegeben, der Nullpunktfehler zusammen mit dem Quantisierungsfehler und der Nichtlinearitätsfehler als Bruchteile des Endwertes oder als Vielfache eines LSB.

Einzelne Stufen können unterschiedlich hoch ausfallen.

Bei Schritt für Schritt steigender Eingangsgröße kann es je nach Realisierungsverfahren vorkommen, dass sich ein Wert der Ausgangsgröße verkleinert, insbesondere dann, wenn es einen Übertrag über mehrere Binärstellen gibt, beispielsweise von 0111 1111 nach 1000 0000. In diesem Falle ist der Umsetzer nicht monoton.

Zeitliche Schwankungen im Takt (Jitter) beeinträchtigen die Konstruktion des Ausgangssignals. Einzelheiten zum maximal erlaubten Jitter siehe unter derselben Überschrift im Artikel ADU.

Hier wird das Ausgangssignal durch so viele Widerstände in einem Spannungsteiler erzeugt wie es Stufen gibt; jeder Widerstand ist gleich gewichtet. Mit dem digitalen Wert wird die zugeordnete Stufe über einen 1-aus-"n"-Schalter (Multiplexer) ausgewählt. Dieses Verfahren ist schnell und garantiert monoton, mit zunehmender Auflösung aber vergleichsweise aufwändig. Ein Beispiel für das Verfahren ist ein 8-Bit-Umsetzer mit 256 Widerständen und 272 Schaltern.

Hier wird das Ausgangssignal durch so viele Widerstände erzeugt wie es Binärstellen gibt; jeder Widerstand ist so gewichtet, wie es der Wertigkeit der zugeordneten Stelle entspricht.

Einfacher in der Herstellung und in der Umsetzung ist das R2R-Netzwerk, das in einer Kette von Stromteilern jeweils eine Halbierung eines elektrischen Stromes vornimmt (nur mit Dualkode möglich).

Man benötigt so viele Schalter, wie Bits zur Darstellung der digitalen Werte verwendet werden. Die unterschiedlich gewichteten Ströme werden je nach Wert (1 oder 0) der zugehörigen Binärstelle auf eine Sammelleitung geschaltet oder ungenutzt abgeleitet. Die Summe der zugeschalteten Ströme wird – heute meist in der Schaltung integriert – mittels eines Operationsverstärkers in eine Spannung umgeformt. Das Parallel-Verfahren bietet einen guten Kompromiss zwischen Aufwand und Umsetzungsdauer und wird häufig verwendet.

Hier wird das Ausgangssignal durch so viele Zeitschritte erzeugt wie es Stufen gibt. Mit dem digitalen Wert werden die Einschaltzeit eines einzigen Schalters und bei periodischer Wiederholung der Tastgrad in einer Pulsweitenmodulation festgelegt. Das endgültige Ausgangssignal ist der Gleichwert einer so ein/ausgeschalteten Spannung. Dieses einfach und preiswert zu realisierende Verfahren benötigt unter den hier vorgestellten Verfahren die größte Umsetzungszeit, weil das Verfahren mit einer Abzählung von Zeitschritten und mit Mittelwertbildung verbunden ist. Dieser garantiert monoton arbeitende DAU lässt sich gut als integrierte Schaltung realisieren und ist besonders in Zusammenhang mit dem Taktsignal bei Mikroprozessoren verbreitet. Für die Mittelwertbildung kann üblicherweise ein einfacher Tiefpass verwendet werden.

Die Deltamodulation, die hier gewisse Ähnlichkeiten zur Pulsweitenmodulation hat, wird in der "Delta-Sigma-Modulation" verwendet. Ähnlich dem Zählverfahren wird mit einem oder mehreren 1-Bit-Umsetzern durch zusätzliche kontinuierliche Differenzbildung und Integration der Ausgangsfehler reduziert und eine Rauschformung erreicht, die das Rauschen in höhere Frequenzbereiche verschiebt. Es ist ein gewisser digitaler Rechenaufwand nötig für Abtastfrequenz-Umsetzung und digitale Filterung. Für gute Ergebnisse werden Delta-Sigma-Modulatoren höherer Ordnung mit hoher Überabtastung verwendet, z. B. 5. Ordnung und 64-facher Überabtastung. Dieses Verfahren erfordert durch eine hohe Überabtastung einen geringen Filteraufwand, ist gut integrierbar, bietet eine hohe Genauigkeit und ist bei Verwendung eines 1-Bit-Umsetzers garantiert monoton. Der wesentliche Vorteil gegenüber dem Zählverfahren liegt in der prinzipbedingten Rauschformung, die höhere Frequenzen ermöglicht. Dieses Verfahren wird heute zunehmend nicht nur in der Audio-, sondern auch in der Messtechnik verwendet.

Dies ist kein eigenständiges Verfahren, sondern es werden Kombinationen aus den obigen Verfahren verwendet. Das hochgenaue Delta-Sigma-Verfahren wird z. B. mit einem einfachen, niedrig auflösenden Parallel-Umsetzer für die niederwertigen Bits kombiniert, um die Vorteile beider Verfahren zu verbinden.

Ein weiteres Klassifizierungsmerkmal ist die Art, wie die digitalen Werte dem Umsetzer zugeführt wird (Interface)
Die Eingangssignale sind meistens elektrische Spannungen mit standardisierter Darstellung der zwei Signalzustände, beispielsweise TTL, ECL, CMOS, LVDS.

Um die Gültigkeit der anstehenden Daten zu signalisieren oder den Baustein weiter zu konfigurieren, sind noch weitere Steuerleitungen erforderlich. Bei seriell angesteuerten Umsetzern muss das Eingaberegister in einer Anzahl von Takten beschrieben werden, ehe die Information zur Umsetzung bereitsteht.

Das generierte Signal steht am Ausgang entweder als
zur Verfügung. Fast immer erfordert die ungünstige Impedanz und Kapazität der Umsetzer-Schaltung eine weitere Aufbereitung des Signals. Eine für diesen Zweck eingesetzte Verstärkungsschaltung bestimmt durch ihre begrenzenden Parameter die dynamischen Eigenschaften der Gesamtschaltung (z. B. Bandbreite) wesentlich mit.

Audiosignale werden für gewöhnlich in digitaler Form gespeichert (z. B. als WAVE oder MP3). Um sie über Lautsprecher hörbar machen zu können, ist eine Umsetzung in analoge Signale erforderlich. DAUs finden sich daher in CD- und digitalen Musikabspielgeräten sowie PC-Soundkarten. DAUs sind auch als Einzelgeräte für mobile Anwendungen oder als Komponenten in Stereoanlagen verfügbar.

Digital generierte Videosignale (z. B. eines Computers) müssen vor der Darstellung auf einem analogen Monitor umgesetzt werden. Hier wird dem DAU meist ein Speicher (RAM) angegliedert, in dem Tabellen für die Gammakorrektur, den Kontrast und Helligkeitseinstellung abgelegt sind. Eine solche Schaltung wird als RAMDAC bezeichnet.

In vielen technischen Geräten werden elektromechanische oder elektrochemische Aktoren mit digital berechneten Werten angesteuert, deren Umsetzung ein DAU besorgt. Ebenso werden DAUs in Akku-Ladegeräten und digital einstellbaren Netzteilen eingesetzt.

Der DAU kann auch einen variablen, analogen Bezugswert mit dem digitalen Eingangssignal multiplizieren. Ein Anwendungsbereich ist das digitale Potentiometer, das als einstellbarer Widerstand (z. B. für die Lautstärkeregelung in Audioverstärkern oder Fernsehgeräten) digital angesteuert werden kann. Digitale Potentiometer mit EEPROM-Speicher "merken" sich den zuletzt eingestellten Wert, auch wenn das Gerät von der Netzspannung getrennt wurde.

Extrem schnelle DA-Umsetzer mit integrierten Misch- und Filterfunktionen, (engl. "Transmit-DACs") werden in der Nachrichtentechnik verwendet, z. B. für die Erzeugung von Sendesignalen bei Mobilfunkgeräten.






</doc>
<doc id="1192" url="https://de.wikipedia.org/wiki?curid=1192" title="Doldenblütler">
Doldenblütler

Die Doldenblütler oder Doldengewächse (Apiaceae oder Umbelliferae) sind eine Pflanzenfamilie in der Ordnung der Doldenblütlerartigen (Apiales). Die meisten Arten sind krautige Pflanzen mit mehrfach geteilten Blättern und Doppeldolden als Blütenstand, wodurch sie leicht der Familie zuzuordnen sind.

Die Familie enthält etwa 434 Gattungen mit etwa 3780 Arten, und ist weltweit in den gemäßigten Zonen vertreten. Zu den Doldenblütlern zählen viele Gewürzpflanzen und Nahrungspflanzen, aber auch einige sehr giftige Pflanzenarten, beispielsweise der Wasserschierling und der Gefleckte Schierling.

Die Vertreter der Doldenblütler sind fast ausschließlich ausdauernde krautige Pflanzen. Einige wenige Taxa, wie etwa in der Unterfamilie Mackinlayoideae, sind verholzt. Die Sprossachse ist in der Regel hohl und knotig. Die Wuchshöhen reichen von mehrere Meter hohen Pflanzen in den Steppen Zentralasiens ("Ferula") bis zu wenigen Zentimeter hohen Polsterpflanzen der Antarktis ("Azorella").

Viele Arten bilden eine Pfahlwurzel aus. Die Seitenwurzeln entstehen an beiden Seiten der Xylempole, da an der Spitze des Xylempols ein Harzgang verläuft. 

Die wechselständigen Laubblätter sind einfach oder mehrfach gefiedert. Nur in Ausnahmen besitzen sie einfache Blätter ("Bupleurum"). Die Blätter besitzen eine Blattscheide.

Der Blütenstand ist meist eine vielstrahlige Doppeldolde, eine Dolde aus meist vielen Döldchen. Dieser Bau der Blütenstände ist sehr charakteristisch für die Doldenblütler und hat ihnen auch ihren alten wissenschaftlichen Namen Umbelliferae (Schirm-Träger) eingebracht. 

Die Tragblätter der Dolden sind dicht zusammengedrängt und bilden die Hülle (Involucrum), häufig sind sie auch nur schwach ausgeprägt oder fehlen. Hier entspringen die Döldchenstiele = Doldenstrahlen. Die Döldchen (Umbellulae) sind wiederum von einem (oft auch fehlenden) Hüllchen (Involucellum) umgeben. Die Blütenstiele werden nicht „Döldchenstrahlen“ genannt. Häufig bildet der Blütenstand eine Kuppel oder sogar eine Fläche, auf der häufig Insekten anzutreffen sind.

Seltener sind einfache Dolden. Es gibt auch Arten mit Einzelblüten ("Azorella"). Bei sehr großen Arten können auch mehrere Doppeldolden zu einem noch größeren Blütenstand zusammengefasst sein (Riesen-Bärenklau, "Heracleum mantegazzianum").

Die meist unscheinbaren Blüten sind mit Ausnahme des Gynoeceums fünfzählig und in der Regel radiärsymmetrisch. Bei einigen Arten sind insbesondere die Randblüten aber auch asymmetrisch und dadurch zygomorph. Kelchblätter sind ursprünglich fünf vorhanden, jedoch sind sie oft verkümmert oder fehlen ganz. Die fünf Kronblätter sind frei und sind meist weiß, seltener gelb, rosa bis violett. Die Kronblätter besitzen häufig an der Spitze ein eingeschlagenes Läppchen (Lobulum inflexum). Seine Gestalt sowie die Gestalt der Vorderkante des Kronblattes (Flexurkante) sind wichtige Bestimmungsmerkmale. 

Es gibt nur einen Kreis mit fünf freien, fertilen Staubblättern, die in der Knospe gekrümmt sind. Zwei Fruchtblätter sind zu einem unterständigen Fruchtknoten verwachsen. Die zwei Griffel (auch als Schnabel bezeichnet) sitzen auf einem scheibenförmigen bis kegelförmigen, glänzenden, Griffelpolster (Stylopodium). Dieses dient als Nektarium, d. h., es scheidet Nektar aus. In jedem der zwei Fruchtknotenfächer befindet sich eine hängende anatrope Samenanlage. Eine zweite verkümmert sehr früh. 

Der Aufbau der Blüte kann in folgender Blütenformel zusammengefasst werden: 
formula_1

Die Blüten sind meist protandrisch. Die Bestäubung erfolgt in der Regel über Fliegen, Käfer und andere kurzrüsselige Insekten (Entomophilie).

Die Frucht ist eine trockene, zweiteilige Spaltfrucht, auch Doppelachäne genannt. Die Gestalt ist häufig zylindrisch mit rundem bis elliptischem Querschnitt. Seltener sind kugelige ("Coriandrum") und doppelkugelige Gestalt ("Bifora"). Die zwei Teilfrüchte (Mericarpien oder Carpiden) bleiben zunächst noch mit der Oberseite an einem Fruchthalter (Karpophor) hängen, der sich in der Mitte befindet. 

Jede Teilfrucht hat an ihrer freien Seite fünf Längsrippen oder Hauptrippen (juga primaria) mit je einem Gefäßbündel. Dazwischen liegen Tälchen (valleculae), in deren Wand sich je ein meist dunkler schizogener (durch das Auseinanderweichen von Zellen entstehender) Ölgang (hier als Ölstrieme bezeichnet) befindet. Bei manchen Arten besitzt jedes Tälchen noch eine Nebenrippe (jugum secundarium, etwa die Karotte mit stacheligen Nebenrippen). Die Ölgänge können auch vermehrt ("Pimpinella") oder reduziert ("Coriandrum") sein oder ganz fehlen ("Conium").

Der Samen besteht aus einem sehr kleinen Embryo in einem großen, fett- und proteinreichen Endosperm. Der Embryo liegt am oberen Ende des Samens mit nach oben gerichtetem Hypokotyl. Die Samenschale ist mit der Fruchtwand verklebt.

Die Ausbreitung erfolgt durch Tiere (Epizoochorie), den Wind (Anemochorie), Wasser (Hydrochorie), durch Selbstausbreitung (Autochorie) und teilweise durch den Menschen (Hemerochorie).

Die Hauptbestandteile der ätherischen Öle können je nach Art überwiegend aus Terpenen oder aus Phenylpropanoiden gebildet werden. Beim Koriander ist es überwiegend (+)-Linalool (Terpen), beim Kümmel (+)-Carvon (Terpen), bei Fenchel und Anis Anethol (Phenylpropanoid).

Die Doldenblütler sind die Familie mit dem größten Spektrum an Cumarinverbindungen. Neben einfachen Cumarinen und Hydroxycumarinen (z. B. Umbelliferon) treten auch eine Vielzahl an prenylierten, geranylierten und farnesylierten Cumarinderivaten auf. Dazu zählen auch die Furano- und Pyranocumarine. Erstere können linear oder angulär sein. Hydroxy- und Furanocumarine wirken abschreckend auf Herbivoren (deterrent), als Phytoalexine und als Keimungsinhibitoren. Dabei steigt die Toxizität von Hydroxy- über lineare zu angulären Furanocumarinen an. Die Furanocumarine sind phototoxisch: Bei Einwirkung von UV-Licht wird die DNA inaktiviert (Photosensibilisierung). Anguläre Furanocumarine sind stärker toxisch als lineare, obwohl ihre Phototoxizität geringer ist. Die meisten der holarktitsch verbreiteten, artenreichen Gattungen der Familie enthalten Furanocumarine (etwa "Bupleurum" und "Pimpinella" mit je 150 Arten), während viele monotypische Gattungen mit eingeschränkter geographischer Verbreitung keine Furanocumarine enthalten. 

Sesquiterpenlactone sind mit über 100 Verbindungen in der Familie vertreten. Es treten die gleichen Grundstrukturen (z. B. Germacranolide, Eudesmanolide, Eremophilanolide und Elemanolide) auf wie bei den Korbblütlern, jedoch stereochemisch unterschiedlich. Außerdem sind sie häufiger hydroxyliert und verestert, insbesondere am C11.

In den Doldenblütlern wurden über 150 Polyacetylen-Verbindungen nachgewiesen. Am häufigsten sind die C-Diin-diene der Falcarinol-Gruppe. Die Giftigkeit des Wasserschierlings ("Cicuta virosa") und der Safranrebendolde ("Oenanthe crocata") beruht auf Polyacetylenen.

Alkaloide sind selten. Coniin und ähnliche Piperidin-Derivate kommen im Gefleckten Schierling ("Conium maculatum") vor. In der Unterfamilie Saniculoideae treten häufig Triterpensaponine auf. Typische Kohlenhydrate sind das Trisaccharid Umbelliferose und der Zuckeralkohol Mannitol.

Das Vorkommen von Petroselinsäure als Hauptfettsäure bezeugt die enge Verbindung zwischen den Apiaceae und den Araliaceae.

Aufgrund der ätherischen Öle werden viele Arten als Gewürz-, Gemüse- und Heilpflanzen verwendet. Verwendung finden dabei die Früchte, Blätter und Wurzeln. Beispiele sind Kümmel ("Carum carvi"), Anis ("Pimpinella anisum"), Koriander ("Coriandrum sativum"), Dill ("Anethum graveolens"), Liebstöckel ("Levisticum officinale"), Fenchel ("Foeniculum vulgare"), Petersilie ("Petroselinum crispum"), und Sellerie ("Apium graveolens").

Eine gewisse Ausnahme bilden die Karotte ("Daucus carota") und der Pastinak ("Pastinaca sativa"), die vor allem aufgrund ihres Kohlenhydrat-Gehaltes angebaut werden.

Einige Arten sind sehr giftig. Der Gefleckte Schierling ("Conium maculatum") lieferte das Gift für den erzwungenen Selbstmord des Sokrates. Ebenfalls sehr giftig ist der Wasserschierling ("Cicuta virosa"). Weniger giftig ist die Hundspetersilie ("Aethusa cynapium"), die jedoch oft mit der Petersilie verwechselt wird, wodurch es häufig zu Vergiftungen kommt. 

Viele Arten sind aufgrund ihrer Furanocumarine photosensibilisierend und phototoxisch. Zu erwähnen ist hier besonders der Riesen-Bärenklau ("Heracleum mantegazzianum"). Das in den phototoxischen Arten enthaltene Psoralen wird jedoch in der Medizin im Rahmen der PUVA-Therapie zur Behandlung von Hauterkrankungen eingesetzt.

Die Familie ist weltweit verbreitet, jedoch liegt der Schwerpunkt in den nördlichen gemäßigten Zonen. In den Tropen sind die Doldenblütler besonders in den montanen Höhenstufen verbreitet. Die Doldenblütler wachsen vorwiegend in Steppen, Sümpfen, Wiesen und Wäldern.

Synonyme für Apiaceae nom. cons. sind: Umbelliferae nom. cons., Actinotaceae , Ammiaceae , Angelicaceae , Daucaceae , Ferulaceae , Saniculaceae .

Das Schwestertaxon der Doldenblütler innerhalb der Ordnung Apiales ist die Gruppe aus Pittosporaceae, Araliaceae und Myodocarpaceae. Die Familie enthält etwa 434 Gattungen mit etwa 3780 Arten. Sie wird seit 2010 nur noch in drei Unterfamilien gegliedert.







</doc>
<doc id="1198" url="https://de.wikipedia.org/wiki?curid=1198" title="Digitale Signalverarbeitung">
Digitale Signalverarbeitung

Die digitale Signalverarbeitung ist ein Teilgebiet der Nachrichtentechnik und beschäftigt sich mit der Erzeugung und Verarbeitung digitaler Signale mit Hilfe digitaler Systeme. Im engeren Sinn liegt ihr Schwerpunkt in der Speicherung, Übermittlung und Transformation von Information im Sinne der Informationstheorie in Form von digitalen, zeitdiskreten Signalen.

In der praktischen Anwendung beruhen heute fast sämtliche Aufzeichnungs-, Übertragungs- und Speicherungsverfahren für Bilder und Film (Foto, Fernsehen, Video) und Ton (Musik, Kommunikationstechnik) auf elektronischer Verarbeitung der entsprechenden Signale. Sie ermöglicht eine Vielzahl von Umwandlungs- und Bearbeitungsarten für solche Daten, z. B. die Kompression von Audio- und Videodaten, Nonlinearen Videoschnitt oder die Bildbearbeitung bei Fotos. Darüber hinaus wird digitale Signalverarbeitung auch – neben vielen anderen industriellen Anwendungsgebieten – in der Mess-, Steuerungs- und Regelungstechnik und in der Medizintechnik eingesetzt, etwa bei der Kernspintomographie.

Die digitale Signalverarbeitung beruht auf elektronischen Bauelementen, wie beispielsweise digitalen Signalprozessoren ("DSP") oder leistungsfähigen Mikroprozessoren, entsprechenden Speicherelementen und Schnittstellen zur Signaleingabe und -ausgabe. Die Algorithmen zur Signalverarbeitung können bei einer programmierbaren Hardware durch zusätzliche Software ergänzt werden, welche den Signalfluss steuert. Die digitale Signalverarbeitung bietet Möglichkeiten und Verarbeitungsmöglichkeiten, welche in der früher üblichen analogen Schaltungstechnik gar nicht oder nur mit hohem Aufwand realisierbar sind.

Die Methoden der digitalen Signalverarbeitung stehen der Mathematik, wie beispielsweise den Teilgebieten der Zahlentheorie oder der Codierungstheorie, viel näher als der klassischen Elektrotechnik. Ausgangspunkt war die allgemeine Bekanntheit der schnellen Fourier-Transformation (FFT) ab dem Jahr 1965 durch eine Veröffentlichung von J. W. Cooley und John Tukey. Zusätzlich verbesserten sich in demselben Zeitraum die praktischen Möglichkeiten der digitalen Schaltungstechnik, so dass die neuentwickelten Verfahren Anwendung finden konnten.

Die digitale Verarbeitung eines Signals folgt immer dem Schema Analog → Digital → Verarbeitung → Analog. Die Veränderungen am Signal werden ausschließlich im digitalen Bereich vorgenommen. Am Beispiel einer Audio-CD soll die Vorgehensweise erklärt werden:

Das Schaubild zeigt den typischen Aufbau eines Signalverarbeitungs-Systems, das immer auch analoge Komponenten an der Schnittstelle zur "Außenwelt" besitzt. Zum digitalen Signalverarbeitungs-System im engeren Sinne gehören nur die rot gefärbten Komponenten im unteren Bildteil.

Verfolgen wir den Weg der Signale in der Grafik: Mittels eines Sensors werden physikalische Größen in ein, häufig schwaches, elektrisches Signal konvertiert. Dieses Signal wird für die weitere Verarbeitung z. B. mit Hilfe eines Operationsverstärkers auf den für die nachfolgenden Schritte nötigen Pegel angehoben. Aus dem verstärkten Analogsignal tastet die Sample-and-Hold-Stufe in bestimmten Zeitintervallen Werte ab und hält sie während eines Intervalls konstant. Aus einer analogen zeitkontinuierlichen Kurve wird so ein zeitdiskretes analoges Signal. Ein für eine gewisse Zeit konstantes Signal wird vom Analog-Digital-Wandler benötigt, um die diskreten digitalen Werte zu ermitteln. Diese können dann vom digitalen Signalprozessor verarbeitet werden. Das Signal nimmt dann den umgekehrten Weg und kann über einen Aktor gegebenenfalls wieder in den technischen Prozess einfließen.

Ein digitales Signal ist, im Gegensatz zu den kontinuierlichen Funktionen der analogen Signalverarbeitung, diskret in Zeit- und Wertebereich, also eine Folge von Elementarsignalen (z. B. Rechteckimpulsen). Diese Folge entsteht meist in einem zeit- oder ortsperiodischen Messprozess. So wird zum Beispiel Schall über die Auslenkung einer Membran oder Verbiegung eines Piezokristalls in eine elektrische Spannung umgewandelt und diese Spannung mittels eines AD-Wandlers zeitperiodisch wiederholt in digitale Daten konvertiert. Solch ein realistischer Messprozess ist endlich, die entstehende Folge besitzt einen Anfangsindex formula_1 und einen Endindex formula_2.

Wir können das Signal also als Datenstruktur formula_3 definieren, mit dem Abstand formula_4 zwischen zwei Datenpunkten, den Indizes formula_5 und der endlichen Folge (Array) formula_6 der Daten.

Die Daten sind Instanzen einer Datenstruktur. Die einfachste Datenstruktur ist das Bit, am gebräuchlichsten sind (1, 2, 4 Byte-)Integer- und Gleitkommazahl-Daten. Es ist aber auch möglich, dass das einzelne Datum selbst ein Vektor oder eine Folge ist, wie zum Beispiel bei der Kodierung von Farbinformation als RGB-Tripel oder RGBA-Quadrupel, oder dass das Signal formula_7 die Spalten formula_8 eines Rasterbildes enthält. Dabei ist die einzelne Spalte wieder ein Signal, das zum Beispiel Grau- oder Farbwerte als Daten enthält.

Um in der Theorie Signale nicht nach Anfang und Ende gesondert betrachten zu müssen, werden die endlichen Folgen in den abstrakten Signalraum formula_9, einen Hilbertraum, eingebettet. Bedingung: Die Basisfunktionen sind orthogonal zueinander, ihre Kreuzkorrelation ergibt demzufolge Null. Ein abstraktes Signal ist also durch ein Paar formula_10, gegeben.

Dabei modelliert der euklidische Vektorraum formula_11 den Datentyp des Signals, zum Beispiel formula_12 für einfache Daten, formula_13 für RGB-Farbtripel. Ein Element in formula_9 ist eine doppelt unendliche Folge formula_15. Die definierende Eigenschaft für den Folgenraum ist, dass die sogenannte Energie des Signals endlich ist ("siehe auch Energiesignal"), das heißt

Die Bearbeitung digitaler Signale erfolgt durch Signalprozessoren.
Das theoretische Modell der elektronischen Schaltung ist der Algorithmus. In der digitalen Signalverarbeitung werden Algorithmen wie Mischer, Filter, Diskrete Fourier-Transformation, Diskrete Wavelet-Transformation, PID-Regelung eingesetzt.
Der Algorithmus ist aus elementaren Operationen zusammengesetzt; solche sind zum Beispiel die gliedweise Addition von Signalwerten, die gliedweise Multiplikation von Signalwerten mit einer Konstanten, die Verzögerung, das heißt Zeitverschiebung, eines Signals, sowie weitere mathematische Operationen, die periodisch aus einem Ausschnitt eines (oder mehrerer) Signals(e) einen neuen Wert generieren und aus diesen Werten ein neues Signal.

Eine Abbildung formula_17 zwischen zwei Signalräumen wird allgemein "System" genannt. Eine erste Einschränkung ist die Forderung der Zeitinvarianz (TI für engl. ) der Abbildung formula_17. Diese entsteht grob betrachtet dadurch, dass ein zeitdiskretes signalverarbeitendes System aus einem Schieberegister, das eine beschränkte Vergangenheit speichert, und einer Funktion formula_19, die aus den gespeicherten Werten einen neuen erzeugt, besteht. Betrachtet man auch ortsabhängige Signale, wie z. B. in der Bildverarbeitung, so stehen neben den vorhergehenden Werten auch nachfolgende zur Verfügung. Um die Allgemeinheit zu wahren, ist also eine zweiseitige Umgebung des jeweils aktuellen Datenpunktes zu betrachten.

Die Umgebung habe einen Radius formula_20, zum Zeitpunkt formula_21 befinden sich die Werte formula_22 eines zeitdiskreten Eingangssignals formula_23 im Umgebungsspeicher. Aus diesen wird mittels der die Schaltung verkörpernden Funktion formula_19 der Wert formula_25 zum Zeitpunkt formula_26 des Ausgangssignals formula_27 bestimmt,

formula_28.

Die Funktion formula_19 kann auch von einigen der Argumente unabhängig sein. Bei zeitabhängigen Signalen wäre es wenig sinnvoll, wenn formula_19 von Werten des Signals zu Zeitpunkten formula_31 in der Zukunft abhinge.
Beispiele für solche Funktionen sind

Man kann zeitinvariante Systeme beliebig kombinieren und hintereinanderschalten und erhält wieder zeitinvariante Systeme.

TI-Systeme formula_17, die von einer linearen Abbildung formula_19 erzeugt werden, etwa

nennt man Faltungsfilter. Sie sind ein Spezialfall der linearen zeitinvarianten Filter (LTI) und können auch als formula_37 geschrieben werden. Dabei bezeichnet formula_38 den Faltungsoperator.

LTI-Systeme können im Orts- bzw. Zeitbereich oder im Frequenzbereich definiert und analysiert werden. Nichtlineare oder gar nicht zeitinvariante Filter wie Regelungen können als Echtzeitsysteme nur im Zeitbereich betrachtet werden.

Ein LTI-System formula_17 kann im Zeitbereich mittels seiner Impulsantwortfunktion formula_40 oder im Frequenzbereich mittels seiner Übertragungsfunktion (engl. , RAO) 

analysiert und realisiert werden. Die Impulsantwort eines Faltungsfilters formula_37 ist gerade formula_43. Man kann LTI-Systeme konstruieren, die bestimmte Frequenzbereiche unterdrücken und andere invariant lassen. Möchte man die frequenzselektive Wirkung eines solchen Systems hervorheben, so nennt man es "Filter".

Eine zentrale Rolle in der praktischen Implementierung von LTI-Systemen spielt der FFT-Algorithmus, der zwischen der Darstellung eines Signals im Zeitbereich und im Frequenzbereich vermittelt. Insbesondere kann eine Faltung im Zeitbereich durch eine Multiplikation im Frequenzbereich realisiert werden.

Filter allgemein:

spezielle Filter:

Zur Realisierung der Filterarten gibt es mehrere Möglichkeiten.

Beispielhafte Anwendungsbereiche der digitalen Signalverarbeitung sind:

Im Gegensatz zu konventionellen Filtersystemen in der Nachrichtentechnik, die einzeln in Hardware realisiert werden müssen, können mit der digitalen Signalverarbeitung beliebige Filter einfach bei Bedarf in „Echtzeit“ (z. B. zur Decodierung) mit Hilfe von Software ein- oder ausgeschaltet werden.

Dabei können je nach Leistungsfähigkeit des Systems beliebig viele Filter und aufwendige Filterkurven und sogar Phasenverschiebungen in Abhängigkeit von weiteren Parametern in „Echtzeit“ erzeugt und so das Ursprungsignal bearbeitet werden.

Deshalb ist mit der digitalen Signalverarbeitung durch DSPs eine wesentlich wirkungsvollere Signalbearbeitung als mit konventionellen Filtersystemen (z. B. bei der Rauschunterdrückung analoger Signale) möglich, siehe Rauschfilter.

Am Beispiel der CD lassen sich einige Vorteile der digitalen gegenüber der analogen Signalverarbeitung erkennen: Die auf einer CD digital gespeicherten Messwerte ändern sich auch nach Jahren nicht, es gibt kein „Übersprechen“ von einer Spur zur anderen, es gehen keine hohen Frequenzen verloren. Auch bei beliebig häufigem Abspielen der CD werden die Daten nicht verändert wie bei einer Schallplatte: Dort „schleift“ die Nadel des Tonabnehmers bei jeder Wiedergabe ein wenig Material weg und glättet die Kanten mit der Folge, dass die hohen Frequenzanteile geändert werden.




</doc>
<doc id="1205" url="https://de.wikipedia.org/wiki?curid=1205" title="Danaergeschenk">
Danaergeschenk

Ein Danaergeschenk (gesprochen Da-na-er-geschenk) ist ein Geschenk, das sich für den Empfänger als unheilvoll und schadenstiftend erweist.

Der Begriff stammt aus der griechischen Mythologie. Benannt ist es in Anlehnung an das hölzerne Trojanische Pferd, mit dessen Hilfe die „Danaer“ (bei Homer eine Bezeichnung für die Griechen/Hellenen überhaupt) die Stadt Troja eroberten. Die Bezeichnung ist aus dem Lateinischen ins Deutsche gelangt. Vergil lässt den Priester Laokoon in der "Aeneis" (Buch II, Vers 48-49) sagen: Das englische Sprichwort "Beware of Greeks bearing gifts" (deutsch: "Hüte Dich vor Griechen mit Geschenken") geht auf denselben Vers in Vergils Aeneis zurück.

Zur stehenden Redewendung wurde auch ein Ausspruch aus Senecas Tragödie "Agamemnon" (624): „Danaum fatale munus“ (lateinisch, „ein verhängnisvolles Danaergeschenk“).


</doc>
<doc id="1206" url="https://de.wikipedia.org/wiki?curid=1206" title="Danunäer">
Danunäer

Danunäer bzw. Danuna (akkadisch "Da-nu-na", phönizisch "Dnn-im", ägyptisch "Dnwn/Dnjn/Dnan") ist die Bezeichnung eines Volks, das in der Region Adana ansässig war. Der Name wird deshalb auch mit dem Ort Adana in Verbindung gebracht.

Das Siedlungsgebiet der Danunäer lag in einer Talebene am Meer, eingeschlossen von hohen Bergketten. Eine Gleichsetzung mit dem Stamm Dan wird kontrovers diskutiert. Nach biblischer Überlieferung besiedelte der Stamm "Dan" ein ähnliches Gebiet, jedoch geografisch weiter entfernt in südlicher Richtung.

Die ägyptische Bezeichnung "Dnwn" erscheint erstmals Mitte des 14. Jahrhunderts v. Chr. im Armana Brief EA 151 von Abi-Milki. Weitere Erwähnungen erfolgen unter anderem im Zusammenhang mit den Seevölkern, die im 8. Regierungsjahr von Ramses III. Ägypten zu Wasser angriffen. Später auch in einer Inschrift des Königs Kilamuwa vom Stadtstaat Ja'udi um 800 v. Chr. als "Volk der Danunäer": 
Die Danunäer werden als Bewohner des Königreiches Qu'e sowohl in der Bilingue von Karatepe als auch in der ebenfalls zweisprachigen Inschrift auf der Statue von Çineköy erwähnt.

W. Max Müller wollte die Rodanim der Völkertafel mit den Danuna der Amarna-Briefe gleichsetzen.
Die diskutierte Gleichsetzung der Danunäer mit der Abstammung von "Jawan" als Volk von Rhodos beruht auf der Völkertafel der Genesis () und der Chronik () mit der Aufzählung der "Dodanim, Tharsisa, Elisa und Chittim". "Jawan" entspricht bei dieser Deutung den "Achi-jawa".

Die Redewendungen von König Azatiwada:
entspricht dem Psalm ():
→ "Hauptartikel: Danaer"

Dass es sich bei den Danaern um die Danunäer handelt, wurde vermutet, konnte aber bislang nicht bewiesen werden. In der Ortsnamenliste des Amenophis III. werden Städte bzw. Landschaften auf dem griechischen Festland (= Tanaja/Danaja), vor allem auf der Peloponnes benannt: 

Mukana (Mykene), Deqajis (Theben oder die Landschaft um Theben), Misana (Messenenien), Nuplija (Nafplio), Kutira (Kythera), Weleja (Elis). Der Ort Amukla (Amyklai), nur wenige Kilometer südlich von Sparta, wurde von dieser Liste gestrichen. Der Grund ist unklar. 

Die zeitgleiche Erwähnung der "Danuna" im Gebiet der Levante widerspricht jedoch einer direkten Ableitung der festlandsgriechischen Danaer in Verbindung mit den Angriffen der "Seevölker", weshalb auch mögliche Gleichsetzungen kontrovers diskutiert wurden.




</doc>
<doc id="1208" url="https://de.wikipedia.org/wiki?curid=1208" title="Donald E. Knuth">
Donald E. Knuth

Donald Ervin „Don“ Knuth [] (* 10. Januar 1938 in Milwaukee, Wisconsin) ist ein US-amerikanischer Informatiker. Er ist emeritierter Professor an der Stanford University, Autor des Standardwerks "The Art of Computer Programming" und Urheber des Textsatzsystems TeX.

Knuth ist der Sohn eines Lehrers für Buchhaltung, der daneben noch eine kleine Druckerei unterhielt. Er besuchte die Milwaukee Lutheran High School und begann sein Physikstudium am California Institute of Technology im September 1956. Aus zweierlei Gründen schlug er ab seinem zweiten Studienjahr jedoch den Weg zur Mathematik ein: Zum einen löste er ein Problem eines seiner Mathematikprofessoren, was ihm eine 1,0 als Note einbrachte, zum anderen fand er wenig Gefallen an den physikalischen Praktika.

Er erhielt einen Bachelor- und gleichzeitig einen Master-Abschluss 1960 an der Case Western Reserve University. 1963 erhielt er seinen Ph.D. vom California Institute of Technology bei Marshall Hall, wo er dann auch nach der Promotion Assistant Professor und 1966 Associate Professor und schließlich Professor wurde. 1968 wurde er Professor für Informatik an der Stanford University. Ab 1977 war er dort "Fletcher Jones Professor of Computer Science" und ab 1990 "Professor of the Art of Computer Programming". Seit 1993 ist er Professor Emeritus.

1960 bis 1968 war er Berater der Burroughs Corporation, wo er unter anderem frühe Compiler schrieb. 1968/69 war er Staff Mathematician in der Communication Research Division des Institute for Defense Analyses.

2006 erfuhr Knuth, dass er an Prostatakrebs im Frühstadium erkrankt war. Er unterzog sich im Dezember des Jahres einer Operation, gefolgt von einer leichten Strahlentherapie als Vorsorgemaßnahme. In seiner Video-Autobiographie nannte er die Prognose "ziemlich gut".

Er ist seit 1961 mit Nancy Jill Carter verheiratet und hat einen Sohn und eine Tochter.

Bereits 1964 erlangte er durch seinen Designvorschlag eines Input/Output-Systems für die Programmiersprache Algol 60 internationale Bekanntheit. Dieses System wurde in den meisten Algol-60-Systemen als Komponente implementiert.

Eigens für sein mehrbändiges Werk "The Art of Computer Programming", an dem er weiterhin arbeitet, schuf er mit TeX und METAFONT Computerprogramme, die druckreifen Textsatz ermöglichen und die besonders im mathematisch-akademischen Bereich eingesetzt werden.

Er prägte den Begriff "literate programming" – die Auffassung, Computerprogramme mit derselben Sorgfalt wie einen literarischen Text zu verfassen und Quelltext und Softwaredokumentation zu vereinen.

In diesem Sinne veröffentlichte er Bücher, in denen der vollständige Quelltext von TeX und METAFONT in Abschnitten zusammen mit Erläuterungen zum Design und zur Wirkungsweise der Algorithmen abgedruckt ist (unter Verwendung dieser Programme). Die außerdem erschienenen Benutzerhandbücher enthalten nicht nur Bedienungshinweise für die Anwender dieser Programme („wie weise ich TeX auf mögliche Worttrennungen hin?“), sondern auch – in technischerer Sprache und kleinerer Schrift – detaillierte Angaben zur Funktionsweise („wie funktioniert der Worttrennalgorithmus?“). Sie umfassen damit zugleich auch die Spezifikation dieser Programme.

Neben Knuths Bemühen um ein ansprechendes ästhetisches Erscheinungsbild beim Textsatz ist ihm Korrektheit ein erstrangiges Anliegen. Deshalb vergibt er für jeden neu gefundenen Fehler in seinen Büchern oder Programmen eine Belohnung von einem „hexadezimalen Dollar“ im Wert von $2,56 (100 hexadezimal entspricht 256 dezimal). Sehr wenige dieser Schecks sind bisher eingelöst worden. Da Knuth Schecks nicht mehr für sicher hält, werden die begehrten Anerkennungsschecks seit 2008 als persönliche Einlagen bei der fiktiven "Bank von San Serriffe" ausgestellt.

1974 beschrieb und popularisierte er in seinem Buch "Surreal Numbers: How Two Ex-Students Turned on to Pure Mathematics and Found Total Happiness" die von John Horton Conway vorgestellten surrealen Zahlen.

Seine Vorliebe für schön gedruckte Texte verband er mit seinem theologischen Interesse (er ist evangelisch-lutherisch) im "3:16-Projekt", als er 1985, aufbauend auf einer Schlüsselstelle der Bibel (Johannes 3,16), aus jedem Buch der Bibel Kapitel 3, Vers 16 studierte und eine eigene englische Übersetzung davon von jeweils unterschiedlichen Künstlern schreiben ließ und diese Kalligrafien mit seinen Überlegungen zu den Versen veröffentlichte.

Am 1. Januar 1990 teilte Knuth mit, ab jetzt keine E-Mail-Adresse mehr zu verwenden, um sich auf seine Arbeit zu konzentrieren.

Seit 1993 befindet sich Knuth im Ruhestand, um sich ausschließlich der Fertigstellung von "The Art of Computer Programming" zu widmen. Seit Februar 2011 liegt Band 4A vor, der sich mit Kombinatorik beschäftigt. Band 4B und 4C sollen folgen, Band 5 (von sieben geplanten) hofft er bis 2025 fertigzustellen.

Im Herbst 1999 hielt er am MIT im Rahmen einer mehrjährigen Vortragsreihe prominenter Wissenschaftler zum Thema „Gott und Computer“ sechs Vorlesungen über Querverbindungen zwischen Informatik und Religion aus seiner persönlichen Sicht, und nahm an einer abschließenden Podiumsdiskussion teil. Deren Mitschriften wurden in seinem Buch "Things a computer scientist rarely talks about" veröffentlicht.

Mehrfach kritisierte er in jüngster Zeit öffentlich die Vergabe von Softwarepatenten in den USA und engagierte sich in der Diskussion über freieren Zugang zu Veröffentlichungen in wissenschaftlichen Zeitschriften.

Knuth hat im Zuge seiner weiteren Forschungen für "The Art of Computer Programming" eine neue Prozessorarchitektur mit zugehörigem Assembler entwickelt und wird diese in einer zukünftigen Ausgabe des ersten Bandes veröffentlichen (die entsprechende Beschreibung liegt bereits als Vorabversion vor). Diese 64-Bit-Architektur (MMIX) unterstützt ein Unix-ähnliches Betriebssystem (genannt NNIX), auf dem dann wiederum der TeX-Interpreter ausführbar wäre. Somit wären "The Art of Computer Programming" und "Computers and Typesetting" in Kombination mit freier Software ein vollständig selbstdokumentierendes System, bestehend aus Hard- und Software.

"The Art of Computer Programming" enthält auch zahlreiche detaillierte mathematikhistorische Anmerkungen; daneben verfasste er auch einige Aufsätze zur Mathematikgeschichte.

Zudem ist Knuth auch bekannt für seine wissenschaftlichen Witze, so schrieb er einen Artikel "The Complexity of Songs" (‚Über die Komplexität von Liedern‘) und entwarf das "Potrzebie-Einheitensystem", in dem die Dicke des 26. "MAD-Magazines" als elementare Längeneinheit dient. Das war auch seine erste Veröffentlichung im "MAD-Magazin" (Heft 33) von 1957.


Er ist vielfacher Ehrendoktor; von 1980 bis 2005 wurden ihm 25 Ehrendoktortitel verliehen, unter anderem von der ETH Zürich (2005) und der Eberhard Karls Universität Tübingen (2001).

Zudem ist Knuth der Namenspate für den seit 1997 jährlich vergebenen Knuth-Preis. Der Asteroid (21656) Knuth ist nach ihm benannt.

1992 wurde er auswärtiges Mitglied der Académie des sciences und 2008 der Russischen Akademie der Wissenschaften, 1973 der American Academy of Arts and Sciences, 1975 der National Academy of Sciences, 2003 auswärtiges Mitglied der Royal Society, 1982 Ehrenmitglied der IEEE, Fellow der Association for Computing Machinery (ACM) und 1981 der National Academy of Engineering. Er ist assoziiertes Mitglied der Norwegischen Akademie der Wissenschaften, seit 1998 korrespondierendes Mitglied der Bayerischen Akademie der Wissenschaften und seit 2012 Mitglied der American Philosophical Society. Er ist Fellow der American Mathematical Society.






</doc>
<doc id="1210" url="https://de.wikipedia.org/wiki?curid=1210" title="Diskrete Mathematik">
Diskrete Mathematik

Die diskrete Mathematik als Teilgebiet der Mathematik befasst sich mit mathematischen Operationen über endlichen oder höchstens abzählbar unendlichen Mengen. Im Gegensatz zu Gebieten wie der Analysis, die sich mit kontinuierlichen Funktionen oder Kurven auf nicht abzählbaren, unendlichen Mengen beschäftigt, spielt die Stetigkeit in der diskreten Mathematik keine Rolle. 

Die in der diskreten Mathematik vertretenen Gebiete (wie etwa die Zahlentheorie oder die Graphentheorie) sind zum Teil schon recht alt, aber die diskrete Mathematik stand lange im Schatten der „kontinuierlichen“ Mathematik, die seit der Entwicklung der Infinitesimalrechnung durch ihre vielfältigen Anwendungen in den Naturwissenschaften (insbesondere der Physik) in den Mittelpunkt des Interesses getreten ist. Erst im 20. Jahrhundert entstand durch die Möglichkeit der raschen digitalen Datenverarbeitung durch Computer (die naturbedingt mit diskreten Zuständen arbeiten) eine Vielzahl von neuen Anwendungen der diskreten Mathematik. Gleichzeitig gab es eine rasante Entwicklung der diskreten Mathematik, die in großem Maße durch Fragestellungen im Zusammenhang mit dem Computer (Algorithmen, theoretische Informatik usw.) vorangetrieben wurde. 

Ein Beispiel für ein Gebiet, das am Schnittpunkt von Analysis und diskreter Mathematik liegt, ist die numerische Mathematik, die sich mit der Approximation von kontinuierlichen durch diskrete Größen beschäftigt sowie mit der Abschätzung (und Minimierung) dabei auftretender Fehler.

Zu den Kerngebieten der diskreten Mathematik zählen:


Darüber hinaus hat die diskrete Mathematik in folgenden Gebieten zusätzliche Beiträge geliefert:


Die Fachgruppe Diskrete Mathematik der Deutschen Mathematiker-Vereinigung vergibt im Zwei-Jahres-Rhythmus den nach dem deutschen Mathematiker Richard Rado benannten Richard-Rado-Preis für die beste Dissertation in Diskreter Mathematik. 

Ein Studium der Diskreten Mathematik ist an verschiedenen Universitäten (u. a. TU Berlin) durch eine entsprechende Schwerpunktsetzung innerhalb des Mathematikstudiums möglich. Unter anderem die Technische Hochschule Mittelhessen, die Philipps-Universität Marburg, die Georg-August-Universität Göttingen, die Hochschule Bremerhaven, die RWTH Aachen, die Technische Universität München, die Fachhochschule Münster, die Fachhochschule Nordwestschweiz, die Hochschule Kempten, die Fachhochschule Hof sowie die Friedrich-Schiller-Universität Jena behandeln die diskrete Mathematik als Pflichtmodul im Grundstudium der Informatik. Unter den Fachhochschulen bietet die Hochschule Mittweida im Rahmen eines spezialisierten Masterstudiums diese Möglichkeit.




</doc>
<doc id="1211" url="https://de.wikipedia.org/wiki?curid=1211" title="Dickdarm">
Dickdarm

Der Dickdarm () ist der letzte Teil des Verdauungstraktes der Wirbeltiere und damit auch des Menschen. Er ist der Teil des Darms, der nach dem Dünndarm beginnt und an der Kloake oder am Anus endet. Seine wesentliche Funktion liegt im Transport und in der Speicherung des Stuhls. Der Dickdarm entzieht dem Stuhl Wasser und dickt ihn dadurch ein. Durch seine Fähigkeit, Natrium-, Kalium- und Chlorid-Ionen aufzunehmen oder auszuscheiden, ist er an der Feinregulation des Elektrolyt-Haushaltes beteiligt. Die Darmflora ist vor allem im Dickdarm zu finden. Erkrankungen des Dickdarms sind beim Menschen häufig: Die akute Appendizitis ist ein gängiges Krankheitsbild der Chirurgie und Darmkrebs gehört zu den häufigsten Krebsdiagnosen.

Der Dickdarm ist der bei den Wirbeltieren vom Dünndarm differenzierte Teil des Mitteldarms (Intestinum), also des mittleren Abschnitts des Darmes zwischen dem Magen und den Anhangsorganen sowie der Kloake bzw. dem Anus. Im einfachsten Fall bilden beide Mitteldarmabschnitte ein einfaches und gestrecktes Rohr, in dem sowohl die enzymatische Zersetzung wie auch die Resorption der Nährstoffe stattfindet. Diese einfache Form findet sich bei den Schleimaalen, den Neunaugen sowie den Knochenfischen. Bei den Knorpelfischen ist der Mitteldarm durch eine spezifische Faltenbildung zur Vergrößerung der Oberfläche gekennzeichnet, die je nach Taxon mehr oder weniger schraubenförmig verläuft und als Spiraldarm bezeichnet wird.

Amphibien und Reptilien besitzen meist nur einen kurzen, in wenige Schlingen gelegten Dickdarm. Der Blinddarm ist klein oder fehlt ganz. Bei beiden Gruppen endet der Dickdarm in einen kurzen Enddarm und danach in der Kloake. Bei Vögeln unterscheidet man zwei Dickdarmabschnitte: den Blinddarm und den Enddarm. Die Bauunterschiede betreffen vor allem den Blinddarm, der bei Vögeln paarig angelegt ist. So besitzen Hühner- oder Straußenvögel große Blinddärme, während sie bei Tauben sehr klein sind und keine Verdauungsfunktion haben und bei Papageien, vielen Greif- und Sperlingsvögeln ganz fehlen.

Mit Ausnahme der Kloakentiere sind die Säugetiere die einzige Tiergruppe, bei der es zu einer Trennung von Geschlechtsöffnung und Darmaustritt in Form eines Anus gekommen ist. Innerhalb der Säugetiere zeigt der Dickdarm erhebliche Unterschiede im Aufbau. So besitzen Raubtiere einen kleinen Blinddarm, ein einfaches U-förmiges Colon und einen kurzen Mastdarm (Rektum), die allesamt keine Bandstreifen (Tänien) besitzen. Der Darm der Primaten entspricht dem in diesem Artikel ausführlich dargestellten Aufbau beim Menschen. Einige Pflanzenfresser wie Pferde oder herbivore Nagetiere haben dagegen einen sehr großen Blinddarm, der bei ihnen als Gärkammer dient. Am Colon zeigt vor allem der aufsteigende Teil ("Colon ascendens") erhebliche Gestaltvariationen, bei Pflanzenfressern ist er stark vergrößert. Die Schlingen des "Colon ascendens" sind beispielsweise bei Pferden in Form zweier übereinandergelegter, nach hinten offener Hufeisen angeordnet, bei Schweinen bienenkorbartig und bei Wiederkäuern scheibenförmig aufgerollt (Einzelheiten siehe den Artikel zum jeweiligen Dickdarmabschnitt). Die Anzahl der Bandstreifen ist ebenfalls tierartlich verschieden. Elefanten besitzen einen sehr langen Darm von insgesamt etwa 25 Metern Länge, von denen 6 Meter auf den Dickdarm und 4 Meter auf das Rektum entfallen. Bei den Walen sind der Dünndarm und der Dickdarm nur anhand der Epithelzellen zu unterscheiden.

Der Dickdarm liegt größtenteils in der Bauchhöhle, wo er die Dünndarmschlingen umrahmt. Er beginnt bei den meisten Menschen im rechten Unterbauch, wo der Dünndarm seitlich einmündet und die Leerdarm-Blinddarm-Klappe (Ileozäkalklappe, Bauhin-Klappe) bildet. Unterhalb der Einmündung endet der Dickdarm blind, entsprechend wird dieser Abschnitt Blinddarm "(Caecum)" genannt. An seinem Ende verengt sich der Blinddarm zum Wurmfortsatz "(Appendix vermiformis)", dessen Lage sehr variabel ist. Oberhalb der Bauhin-Klappe beginnt der Grimmdarm "(Colon)", der bis unter die Leber aufsteigt "(Colon ascendens)", unterhalb der Leber nach links umbiegt "(Flexura coli dextra)" und quer durch die Bauchhöhle in den linken Oberbauch zieht ("Colon transversum", auch Quercolon genannt). Hier biegt er erneut um "(Flexura coli sinistra)" und steigt in das Becken ab "(Colon descendens)", wo er anschließend S-förmig nach hinten "(dorsal)" zum Kreuzbein zieht "(Colon sigmoideum)". Dort biegt er nach unten "(kaudal)" um, verlässt die Bauchhöhle und bildet den Enddarm. Dieser wird in den Mastdarm und den Analkanal unterteilt. Die Gesamtlänge des Dickdarms beträgt beim Menschen etwa 1,5 Meter.

Die einzelnen Abschnitte können intraperitoneal, retroperitoneal und extraperitoneal liegen. Damit ist die Lage zum Bauchfell "(Peritoneum)" gemeint. Ein Organ liegt intraperitoneal, wenn es vollständig von Bauchfell überzogen ist und mit einem breiten Band (Gekröse, "Meso") an der Rumpfwand aufgehängt ist, wodurch es relativ frei beweglich ist. Beim Dickdarm gilt das für Blinddarm, Wurmfortsatz, Quercolon und Colon sigmoideum. Retroperitoneal bedeutet, dass das Organ nicht von allen Seiten von Bauchfell überzogen ist, sondern an einer Seite direkt mit der Rumpfwand verwachsen ist. Das gilt für den auf- und absteigenden Grimmdarm ("Colon ascendens" und "Colon descendens") und das obere Rektum. Ein Organ liegt extraperitoneal, wenn es außerhalb der Bauchhöhle liegt und deswegen nicht vom Peritoneum überzogen ist. Im Falle des Dickdarms trifft das auf das Endstück des Rektums und den Analkanal zu.

Charakteristisch für den Dickdarm ist die Wand des Colons. Sie ist gekennzeichnet durch drei sichtbare Längsmuskelzüge, die Bandstreifen genannt werden, halbmondförmige Einziehungen "(Plicae semilunares)" und Aussackungen (Poschen oder Haustren) zwischen den Einziehungen "(siehe Abschnitt Feinbau)".

Die Abschnitte des Dickdarmes werden von den Ästen dreier großer Arterien versorgt. Blinddarm, Wurmfortsatz, aufsteigendes Colon und der größte Teil des Quercolon erhalten Äste der "Arteria mesenterica superior", der restliche Teil des Quercolon, das absteigende Colon, das Colon sigmoideum und das obere Rektum solche von der "Arteria mesenterica inferior". Das untere Rektum und der Analkanal erhalten Blut aus der "Arteria pudenda interna". Der Blutabfluss erfolgt über Venen, die mit den Arterien verlaufen und gleichlautend benannt sind, also über die "Vena mesenterica superior", "Vena mesenterica inferior" und "Vena pudenda interna". Die beiden erstgenannten münden in die Pfortader der Leber, nur die "Vena pudenda interna" mündet in die "Vena iliaca interna", deren Blut in die untere Hohlvene gelangt, ohne die Leber zu passieren.

Da Lymphgefäße in der Regel mit Arterien verlaufen, entsprechen die Lymphabflussgebiete des Dickdarms in etwa den arteriellen Versorgungsgebieten. Die Lymphe aus dem Stromgebiet der "Arteria mesenterica superior" fließt über die Mesenteriallymphknoten an der Austrittsstelle der Arterie aus der Bauchaorta "(Noduli mesenterici superiores)" in den "Truncus intestinalis", der in die "Cisterna chyli" mündet. Die Lymphe aus dem Stromgebiet der "Arteria mesenterica inferior" gelangt entsprechend über die Lymphknoten neben dem Arterienaustritt "(Noduli mesenterici inferiores)" und über den linken "Truncus lumbalis" in die "Cisterna chyli".

In der Wand des Darms befindet sich ein Netzwerk aus Nervenzellen, das die Bewegungen des Darms koordiniert. Dieses sogenannte enterische Nervensystem arbeitet weitgehend autonom, seine Aktivität wird aber von den beiden Anteilen des vegetativen Nervensystems beeinflusst: der Parasympathikus steigert die Darmaktivität, der Sympathikus setzt sie herab. Ähnlich der arteriellen Versorgung wird der Dickdarm bis kurz vor der linken Colonflexur anders innerviert als der Darm dahinter. Die parasympathischen Fasern für den ersten Abschnitt stammen aus dem Vagusnerv, die für den zweiten Abschnitt entspringen aus dem untersten Teil des Rückenmarks und verlaufen als "Nervi splanchnici pelvici". Als Cannon-Böhm-Punkt wird das Gebiet bezeichnet, in dem sich die Innervationsgebiete überlappen.

Der Dickdarm zeigt den typischen Wandaufbau des Magen-Darm-Traktes mit vier Schichten. Die innerste Schicht ist eine Schleimhaut ("Tunica mucosa", kurz "Mukosa"), die ihrerseits aus drei Schichten aufgebaut ist: die Oberfläche ist mit Epithel "(Lamina epithelialis)" bedeckt, das durch lockeres Bindegewebe "(Lamina propria mucosae)" von einer Schicht aus glatten Muskelzellen "(Lamina muscularis mucosae)" getrennt ist. Die Schleimhaut liegt einer lockeren Bindegewebsschicht ("Tunica submucosa", kurz "Submukosa") auf. Diese führt die Blut- und Lymphgefäße für die Mukosa und beinhaltet ein Nervengeflecht, den "Plexus submucosus". Sie dient zudem als Verschiebeschicht zur dritten Wandschicht, der "Tunica muscularis", die dem Organ mit einer inneren Ringmuskelschicht "(Stratum circulare)" und einer äußeren Längsmuskelschicht "(Stratum longitudinale)" peristaltische Bewegungen ermöglicht. Zwischen den Muskelschichten liegt ein weiteres Nervengeflecht, der "Plexus myentericus", der ebenso wie der "Plexus submucosus" zum enterischen Nervensystem gehört. Die vierte Schicht ist je nach Abschnitt des Dickdarms entweder lockeres Bindegewebe "(Adventitia)" oder das Bauchfell.

Ein wichtiger feinbaulicher Unterschied zum Dünndarm besteht in dem Fehlen von Darmzotten, die Dickdarmschleimhaut hat nur tiefe Krypten, die von zylinderförmigen Zellen "(hochprismatisches Epithel)" ausgekleidet sind. Viele dieser Zellen produzieren Gleitschleim, andere nehmen Wasser auf und dicken so den Stuhl ein. Auch die Dickdarmwand ist wie die Wand des Dünndarms in Falten geworfen. Diese entstehen aber durch örtliche Einziehungen der inneren Ringmuskelschicht, die im Querschnitt halbmondförmig erscheinen (daher der lateinische Name "Plicae semilunares"). Zwischen den Einziehungen bildet die Darmwand Aussackungen, die als "Haustren" (deutsch: Poschen) bezeichnet werden. Bei einigen Säugetieren, auch beim Menschen, ist die äußere Längsmuskelschicht zu drei kräftigen Strängen ("Tänien") verdickt. An diesen Tänien hängen außen Ansammlungen von Fettgewebe "(Appendices epiploicae)". Von diesem Muster weicht die "Appendix vermiformis" ab. Sie hat keine Tänien, sondern wie die anderen Abschnitte des Verdauungstraktes eine durchgehende Längsmuskelschicht. In der "Lamina propria" der Schleimhaut sind große Lymphfollikel zu finden. Das Rektum hat statt der Tänien eine durchgehende Längsmuskelschicht, keine Haustren und keine Fettanhängsel. Am Analkanal geht das Epithel des Rektums in mehrschichtig unverhorntes Plattenepithel über.

Aus dem "Entoderm", dem inneren Keimblatt des Embryos, bildet sich zunächst das primitive Darmrohr aus, an dem Vorder-, Mittel- und Hinterdarm zu unterscheiden sind. Aus der weiteren Entwicklung des Mitteldarms geht der größte Teil des Dünndarms und der Dickdarm einschließlich der ersten zwei Drittel des Quercolons hervor. Der Rest des Dickdarms bildet sich aus dem Hinterdarm, während das letzte Stück des Analkanals durch die Einstülpung von "Ektoderm" entsteht. Die Entwicklung des Darmes erklärt auch die Innervation und die Blutversorgung: die Mitteldarmarterie wird zur "Arteria mesenterica superior", die Enddarmarterie zur "Arteria mesenterica inferior".

Im Laufe der Entwicklung verwachsen "Colon ascendens" und "Colon descendens" mit der rückwärtigen Rumpfwand. Beim "Colon ascendens" kann diese Verwachsung unvollständig sein und im Extremfall gar nicht stattfinden, sodass es wie das Quercolon über ein eigenes Mesenterium verfügt. Das "Colon ascendens" ist dann abnorm beweglich, es kann zum Volvulus kommen oder zur Einklemmung von Dünndarmschlingen. Während der Embryonalentwicklung dreht sich der Darm und „verpackt“ sich in der Bauchhöhle. Auch bei diesem Prozess können Fehler auftreten, die dazu führen, dass sich etwa der gesamte Dickdarm auf der linken Seite befindet oder das Quercolon hinter dem Zwölffingerdarm zu liegen kommt. Als Atresie bezeichnet man den Verschluss von Hohlorganen: Am Dickdarm sind am häufigsten Rektum und Analkanal betroffen, bei der "Rektoanalatresie" fehlt die Verbindung zwischen den beiden Abschnitten und der Dickdarm endet blind. Häufig ist der Mastdarm dann durch Fisteln mit angrenzenden Organen verbunden. Bei Jungen ist das häufig die Harnröhre, bei Mädchen die Vagina. Beim angeborenen Megacolon "(Morbus Hirschsprung)" fehlen meistens im Endabschnitt des Dickdarms Nervenzellen, wodurch sich die Muskulatur im betroffenen Bereich zusammenzieht und den Darm verschließt. Der Darminhalt staut sich an der Engstelle und dehnt den Dickdarm auf („Megacolon“).

Der Dickdarm nimmt den Speisebrei aus dem Dünndarm auf, transportiert ihn weiter, speichert ihn im Mastdarm und scheidet ihn letztlich aus. Dabei entzieht er ihm weiteres Wasser, indem er Natrium-Ionen resorbiert. Daneben ist er auch an der Regulation des Chlorid- und Kalium-Ionen-Haushaltes beteiligt, wobei er im Gegensatz zum Dünndarm auch zur aktiven Sekretion von Kalium in der Lage ist. Abgesehen von kurzkettigen Fettsäuren werden im Dickdarm keine Nährstoffe aufgenommen. Eine bedeutende Rolle bei der Bildung dieser Fettsäuren spielen die Bakterien des Dickdarms, die Darmflora.

Die Ileozäkalklappe trennt den letzten Abschnitt des Dünndarms, das Ileum, vom Blinddarm. In Ruhe ist diese Klappe teilweise geschlossen, sodass ein langsamer Durchtritt des Speisebreis möglich ist. Der Übertritt des Speisebreis vom Ileum in den Blinddarm findet bei Nahrungsaufnahme verstärkt statt: Die Magendehnung führt über einen Reflex zu verstärkter Peristaltik des Ileums und über die Ausschüttung des Hormons Gastrin zur Entspannung des zur Klappe gehörenden Schließmuskels. Der Transport des Speisebreis findet im Dünndarm mit einer recht konstanten Geschwindigkeit statt. So staut sich Speisebrei vor der Ileozäkalklappe und dehnt das Ileum. Ohne die entspannende Wirkung des Gastrins bewirkt die Dehnung des Ileums eine Kontraktion des Schließmuskels, ohne Nahrungsaufnahme wird also der Übertritt des Speisebreis blockiert.

Nach der Passage des Ileozäkalsphinkters sammelt sich der Speisebrei im Blinddarm und im aufsteigenden Colon. Typisch für alle Abschnitte des Dickdarms ist die Haustralbewegung. Dabei füllt sich eine Haustre bis zu einem bestimmten Grad und zieht sich dann zusammen, wobei sie ihren Inhalt in die benachbarte Haustre drückt. Daneben ist eine sehr langsame propulsive Peristaltik zu beobachten, bei der sich die Einschnürungen zwischen den Haustren sozusagen Richtung Anus bewegen. Im aufsteigenden Colon und im Quercolon sind Segmentationsbewegungen zu beobachten, die den Stuhl durchmischen. Im Quercolon tritt gelegentlich eine Antiperistaltik auf, die den Stuhl zurück in den Blinddarm treibt. In Verbindung mit der Nahrungsaufnahme tritt eine sogenannte Massenperistaltik auf: Ausgelöst durch die Dehnung des Magens entsteht im mittleren Quercolon eine peristaltische Welle, die den Stuhl in kurzer Zeit über das absteigende Colon und das "Colon sigmoideum" in den Mastdarm befördert (Gastrocolischer Reflex).

Die Darmentleerung ist ein Reflex, der durch die Dehnung der Rektumwand ausgelöst wird. Dabei kontrahieren sich die Längsmuskeln des Mastdarms, verkürzen ihn und erhöhen so den Druck. Der innere Schließmuskel des Anus wird unwillkürlich entspannt. Durch die willentliche Entspannung des äußeren Schließmuskels kann sich der Mastdarm entleeren.

Die gesamte Passagezeit des Dickdarms ist individuell sehr verschieden und reicht von 12 bis 48 Stunden.

Der Dickdarm nimmt mit unter 2 Litern am Tag weniger Wasser auf als der Dünndarm, kann die Resorption jedoch auf 4 bis 5 Liter steigern. Der Wassertransport erfolgt grundsätzlich über die Resorption von Natrium-Ionen: diese werden aktiv aufgenommen, das Wasser folgt passiv nach "(Osmose)". Die Zellen des Dickdarms sind wie die Zellen des Dünndarms in der Lage, Natrium-, Kalium- und Chlorid-Ionen aufzunehmen und im Fall von Chlorid auch auszuscheiden, wenn auch die zellulären Mechanismen dahinter unterschiedlich sind. Zwei wesentliche Unterschiede liegen darin, dass die Dickdarmzellen Natrium auch gegen einen Konzentrationsgradienten aufnehmen und Kalium nicht nur aufnehmen, sondern auch ausscheiden. Damit spielt der Dickdarm eine wichtige Rolle in der Feinregulation des Kalium-Haushaltes. Kohlenhydrate und Proteine, die in den Dickdarm gelangen, werden dort von Bakterien abgebaut. Der Dickdarm kann nur die dabei entstehenden kurzkettigen Fettsäuren resorbieren.

Bei allen Tieren ist der Darm von Bakterien besiedelt, die in ihrer Gesamtheit die Darmflora bilden. Die Zusammensetzung der Darmflora und die Verteilung der Bakterien unterscheiden sich zwischen Pflanzen-, Fleisch- und Allesfressern. Die Bakterien leben dabei in Symbiose mit ihrem Wirt, indem sie ihm nicht verdaubare Nahrungsbestandteile verdauen und zugänglich machen. Da die Bakterien unter Ausschluss von Sauerstoff, also anaerob arbeiten müssen, handelt es sich um Vergärungsprozesse.

Bei Fleischfressern und beim Menschen ist ein Großteil der Darmflora im Dickdarm beheimatet. Hier produzieren sie bei der Vergärung des Speisebreis in erster Linie kurze Fettsäuren, die vom Dickdarm aufgenommen werden. Hinzu kommt Vitamin K, das ebenfalls resorbiert wird.

Während Wiederkäuer die unverdaulichen Bestandteile der pflanzlichen Nahrung, nämlich Cellulose, Xylan, Pectin und andere Polysaccharide, in ihrem Pansen vergären lassen, finden diese Prozesse bei Pferden, Eseln, den meisten anderen Unpaarhufern und Kaninchen im Blinddarm und Colon statt.

Die Art und Menge, sozusagen das Ökosystem der verschiedenen Bakterien im Dickdarm ist Gegenstand aktueller Forschung. Ernährung, aber auch Übertragung von Mensch zu Mensch spielen hier eine Rolle. Das Immunsystem und Erkrankungen werden davon beeinflusst, eine Interventionsmöglichkeit stellt die Stuhltransplantation dar.

Eine Entzündung des Dickdarms wird allgemein Kolitis genannt. Die Entzündung des Wurmfortsatzes (Appendizitis) ist die häufigste Entzündung im Bauchraum. Ursachen für Dickdarmentzündungen sind Infektionen mit Krankheitserregern, Allergien, bestimmte Medikamente, Strahlung, Minderdurchblutung oder unbekannte Faktoren, die zum Beispiel bei der Entstehung der chronisch-entzündlichen Darmerkrankungen eine Rolle spielen.

Bei einer Gastroenteritis, also der Magen-Darm-Entzündung, ist der Dickdarm in der Regel mitbetroffen. Eine solche Entzündung entsteht durch die Infektion mit Bakterien, Viren und seltener Parasiten. Häufige bakterielle Erreger sind besondere Typen von "Escherichia coli" (EHEC, ETEC, EIEC und EPEC), einige Yersinien- und Campylobacter-Spezies, des Weiteren enteritische Salmonellen und Cholera-Erreger. In manchen Fällen manifestiert sich auch eine Tuberkulose im Dickdarm. Kleinräumige Ausbrüche einer infektiösen Gastroenteritis werden häufig durch Viren hervorgerufen, überwiegend Noroviren, bei Kleinkindern häufiger Rotaviren. Seltenere virale Erreger sind Astroviren, Sapoviren und das Humane Adenovirus F. In Sommermonaten überwiegen in Mitteleuropa die bakteriellen Gastroenteritiden, im Herbst und Winter hingegen virale.

Eine Erkrankung, die nur den Dickdarm betrifft, ist die Dysenterie (Ruhr). Sie wird in Mitteleuropa vor allem durch Shigellen (Bakterienruhr) verursacht. In tropischen und subtropischen Regionen ist die Amöbenruhr weiter verbreitet, deren Erreger "Entamoeba histolytica" sich vor allem in Colon und Leber festsetzt. Bei schlechter Abwehrlage, etwa bei AIDS-Erkrankten, können weitere Erreger krankheitsauslösend sein, darunter einige atypische Mykobakterien (MOTT), Kryptosporidien und Candida-Pilze. Selten kommt es bei Immundefizienten zu einer Kolitis aufgrund einer Reaktivierung des Cytomegalievirus. In tropischen Regionen spielt auch die Infektion mit Schistosomen eine Rolle. Die durch diese Würmer ausgelöste Erkrankung wird Bilharziose genannt.

Zu den chronisch-entzündlichen Darmerkrankungen werden üblicherweise "Morbus Crohn" und "Colitis ulcerosa" gezählt, die in Deutschland mit jeweils 5–6 Neuerkrankungen pro 100.000 Einwohner pro Jahr etwa gleich häufig auftreten. Kennzeichnend ist eine dauerhafte (chronische) Immunreaktion in der Darmwand, die in Schüben auftritt. Bei beiden Erkrankungen sind die Auslöser und Mechanismen der Krankheitsentstehung noch unklar. Franz Alexander zählte sie 1950 zu den sieben psychosomatischen Krankheiten, den „Holy Seven“, mittlerweile wurden aber auch genetische Faktoren identifiziert, die bei der Entstehung der Krankheiten eine Rolle spielen könnten. "Morbus Crohn" und "Colitis ulcerosa" unterscheiden sich hinsichtlich des Krankheitsverlaufs und ihres Erscheinungsbildes (Morphologie).

Der Morbus Crohn ist die Entzündung der ganzen Darmwand mit allen Schichten, weshalb es häufig zu Fisteln (beispielsweise Analfisteln) kommt. Grundsätzlich kann der gesamte Verdauungstrakt befallen sein, typischerweise betrifft die Entzündung aber den Endabschnitt des Dünndarms (Ileum) und den Dickdarm. Die Entzündung breitet sich nicht kontinuierlich vom Entstehungsort aus, sondern „springt“ von Abschnitt zu Abschnitt. Der Morbus Crohn heilt häufig nur unvollständig ab und ist durch eine hohe Rezidiv-Rate, also wiederkehrende Entzündungen, gekennzeichnet.

Bei der Colitis ulcerosa beschränkt sich die Entzündung dagegen auf die Schleimhaut. In der Regel beginnt die Entzündung akut im Rektum und breitet sich von dort kontinuierlich auf die restlichen Dickdarmabschnitte aus. Ist der ganze Dickdarm befallen, ist von einer "Pancolitis" die Rede. Bei etwa 10 bis 20 % der Pancolitiden kommt es zur sogenannten „Backwash-Ileitis“, bei der die Entzündung auf das Ileum des Dünndarms übergreift. Auch die Colitis ulcerosa verläuft rezidivierend, also mit wiederkehrenden Schüben. Zwischen den Schüben heilt der Darm in der Regel aber vollständig ab. Beim akut fulminanten Verlauf ist das toxische Megacolon eine seltene, aber lebensbedrohliche Komplikation mit der Gefahr einer eitrigen Bauchfellentzündung. Colitis ulcerosa erhöht das Risiko, an Darmkrebs zu erkranken.

Eine weitere, schlecht erforschte Erkrankung ist die mikroskopische Colitis, die ebenfalls zu den chronisch entzündlichen Darmerkrankungen gezählt werden kann. Sie verursacht wässrige Durchfälle, aber keine mit dem bloßen Auge oder dem Endoskop sichtbaren Schleimhautveränderungen. Die Diagnose kann nur durch die mikroskopische Untersuchung der Schleimhaut nach der Biopsie gestellt werden. Unterschieden werden zwei Formen: die Lymphozytäre Colitis ist durch eine Vermehrung von bestimmten Immunzellen, den Lymphozyten, im Epithel charakterisiert. Die Kollagene Colitis entspricht der Lymphozytären Colitis, zusätzlich hat sich unter der Basalmembran des Schleimhautepithels eine Schicht aus Kollagenfasern gebildet.

Eine ischämische Kolitis entsteht, wenn die Dickdarmschleimhaut aufgrund von Gefäßverengungen oder Verschlüssen (häufig durch Arteriosklerose) überhaupt nicht mehr oder nicht mehr ausreichend durchblutet und dadurch geschädigt wird "(Mesenteriale Ischämie)". Die Reaktion auf den Gewebeschaden ist die Entzündung im betroffenen Gebiet. Beim Dickdarm ist die Durchblutungsstörung häufig auf kleinere Areale begrenzt und tritt etwas häufiger im Bereich der linken Colonflexur auf, da dieses Gebiet an der Grenze der Versorgungsgebiete von "Arteria mesenterica superior" und "Arteria mesenterica inferior" liegt und die Anastomosen zwischen den Versorgungsgebieten aufgrund von Arteriosklerose nicht mehr in der Lage sind, Durchblutungsstörungen auszugleichen. Das Rektum ist in der Regel nicht betroffen, da es aus den Beckenarterien ausreichend versorgt ist.

Auch die Wirkung vieler Medikamente kann Dünn- und Dickdarm schädigen und eine Entzündung "(Enterocolitis)" verursachen, etwa nichtsteroidale Antirheumatika (NSAR) wie Aspirin oder Ibuprofen, Antibiotika, Zytostatika und blutdrucksenkende Mittel wie Diuretika. Schätzungsweise ist jede zehnte Entzündung des (Dick-)Darms auf den Gebrauch von NSAR zurückzuführen. Die Entzündung heilt ab, wenn das Medikament abgesetzt wird. Antibiotika hemmen die Darmflora und begünstigen dadurch die Vermehrung krankheitserregender Bakterien, vor allem "Clostridium difficile", dessen Enterotoxine die Dickdarmschleimhaut angreifen und zur Entzündung führen. Wegen typischer Schleimhautveränderungen werden diese Entzündungen als pseudomembranöse Kolitiden bezeichnet. Bei einer starken Verminderung der Anzahl der neutrophilen Granulozyten im Blut (Neutropenie), häufig als Nebenwirkung einer Chemotherapie mit Zytostatika, kann es zu einer schweren, nekrotisierenden Entzündung des Blinddarms und des aufsteigenden Colons kommen, der sogenannten neutropenischen Colitis "(Typhlitis)".

Das Reizdarmsyndrom ist ein Komplex mehrerer gastrointestinaler Symptome, das mit psychischen Belastungsfaktoren in Verbindung gebracht wird und auch nach einer Darminfektion auftreten kann. Das Reizdarmsyndrom ist eine Ausschlussdiagnose, die gestellt wird, wenn die lang anhaltenden Beschwerden wie Bauchschmerzen, Blähungen und Stuhlveränderungen mit keiner anderen Diagnose in Einklang gebracht werden können. Rund die Hälfte der Patienten mit gastrointestinalen Beschwerden soll an einem Reizdarmsyndrom leiden.

Divertikel sind allgemein Ausstülpungen der Wand eines Hohlorgans, die am Dickdarm am häufigsten auftreten. Unterschieden werden echte Divertikel und unechte Divertikel "(Pseudodivertikel)". Bei Ersteren sind alle Wandschichten an der Bildung des Divertikels beteiligt, bei den Pseudodivertikeln wird in der Regel nur die Schleimhaut durch die Muskelschichten gedrückt. Pseudodivertikel können entweder noch in der Darmwand "(intramural)" liegen oder sich komplett daraus herausstülpen ("extramurale" Divertikel). Etwa zwei Drittel der Dickdarmdivertikel treten am "Colon sigmoideum" auf und sind typischerweise Pseudodivertikel. Das gehäufte, keine Beschwerden verursachende Auftreten von Divertikeln wird als Divertikulose bezeichnet, die in eine Divertikulitis, also eine eitrige Entzündung der Divertikel, übergehen kann und therapiert werden muss. Im schlimmsten Fall können entzündete Divertikel aufbrechen (perforieren), was zu Abszessen in der Bauchhöhle und Entzündungen des Bauchfells führen kann.

Der Dickdarm ist mit über 60000 jährlichen Neuerkrankungen in Deutschland nach der Prostata und der Brustdrüse der dritthäufigste Entstehungsort von Krebs, dem kolorektalen Karzinom. Die Ursachen sind nicht genau bekannt, als Risikofaktoren gelten Rauchen, Bewegungsmangel, Übergewicht, Alkohol und rotes Fleisch. Darüber hinaus gibt es seltene erbliche Formen wie die familiäre adenomatöse Polyposis oder das hereditäre nicht-polypöse kolorektale Krebssyndrom. Das gängige Modell der Entstehung von Dickdarmkrebs geht von einer Adenom-Karzinom-Sequenz aus. Das bedeutet, dass in einem mehrstufigen Prozess durch genetische Veränderungen das Drüsenepithel des Dickdarms entartet: Dafür reicht eine Zelle, die sich wegen dieser genetischen Veränderungen unkontrolliert teilt. Zunächst entsteht so ein gutartiger Tumor, ein Adenom, der bei der Koloskopie als Dickdarmpolyp auffällt. Die Zellen des Adenoms sind aber anfällig für weitere Genmutationen, sodass irgendwann Krebszellen entstehen, die bösartig in das umliegende Gewebe einwachsen und sich schnell teilen. Über 90 % der kolorektalen Karzinome gehen aus Adenomen hervor, weswegen die Entfernung eines Adenoms immer angezeigt ist.

Tumoren des Bindegewebes sind selten, gerade im Vergleich zu den oben beschriebenen epithelialen Tumoren. Die häufigsten Bindegewebstumoren sind Tumoren des Fettgewebes, der glatten Muskelzellen, der Lymphgefäße und Gastrointestinale Stromatumoren. Maligne Lymphome, insbesondere das Mantelzelllymphom, können sich als "lymphomatöse Polypose" manifestieren, wobei sich zahlreiche Polypen im Colon finden.

Neuroendokrine Tumoren gehen im Magen-Darm-Trakt von den Zellen des Diffusen neuroendokrinen Systems aus. Sie sind am Colon sehr selten, am Rektum finden sich dagegen 13 % aller gastrointestinalen Neuroendokrinen Tumoren, während am Wurmfortsatz fast jeder fünfte dieser Tumoren lokalisiert ist.

Der Dickdarm kann mit den Händen und Fingern, endoskopischen und anderen bildgebenden Verfahren untersucht werden.

Beim Abtasten des Bauches im Rahmen der körperlichen Untersuchung können Tumoren des Dickdarms festgestellt werden. Eine häufige Untersuchung ist die digitale Palpation, das Abtasten mit den Fingern (von lat. "digitus", Finger). Dabei führt der Untersuchende einen (in der Regel behandschuhten) Finger in den Anus ein, tastet den Analkanal ab, prüft den Ruhetonus und den Druck des Analsphinkters bei der aktiven Anspannung, schiebt den Finger bis in die Rektumampulle vor und tastet auch diese aus. Bei Männern kann auf diesem Wege auch die Prostata beurteilt werden. Auf diese Art können Tumoren oder schmerzhafte Stellen ausgemacht werden. Nicht zuletzt können Stuhl, Blut oder Eiter am Finger Hinweise auf Erkrankungen geben.

Zur endoskopischen Untersuchung stehen verschiedene Verfahren zur Verfügung. Das Rektoskop ist ein starres Endoskop, das nur zur Beurteilung von Analkanal und Rektum geeignet ist. Eine Sigmoidoskopie zur Beurteilung des Darms bis zum "Colon sigmoideum" kann mit einem bis zu 60 cm langen, flexiblen Endoskop erfolgen. Die Koloskopie (Darmspiegelung) ist die endoskopische Untersuchung des gesamten Dickdarms mit einem langen Endoskop. Sie gilt als Goldstandard für die Beurteilung der Schleimhaut. Mit diesem Verfahren können nicht nur sichtbare Veränderungen beschrieben, sondern auch Proben entnommen (biopsiert) und kleine therapeutische Eingriffe durchgeführt werden (beispielsweise die Entfernung eines Polypen).

Die klassische Ultraschalluntersuchung des Bauches spielt für die Beurteilung des Dickdarms eine untergeordnete Rolle. Nützlich ist sie zur Diagnostik der akuten Appendizitis und der Divertikulitis. Die Endosonografie ist ein kombiniertes Verfahren, bei dem ein rotierender Schallkopf an einem flexiblen Endoskop in den Darm eingeführt wird. Dabei entsteht ein Querschnittsbild des Darms, mit dem alle Wandschichten beurteilt werden können.
Das konventionelle Röntgen des Bauches bietet eine schnelle Diagnostik. Es eignet sich zur Identifizierung freier Luft im Bauchraum, die einen Hinweis auf die Perforation eines Hohlorgans gibt, zur Diagnostik eines Darmverschlusses (Ileus) durch den Nachweis von Luft-Flüssigkeits-Spiegeln in den Darmschlingen oder dem Nachweis von Fremdkörpern oder eingebrachten Materialien. Die Computertomographie erlaubt die Beurteilung der Wandschichten des Dickdarms sowie anderer Organe und Lymphknoten, weshalb dieses Verfahren für die Stadienbestimmung von Tumoren "(Staging)" verwendet wird. Bei jungen Patienten mit chronisch-entzündlichen Darmerkrankungen kommt wegen der fehlenden Strahlenbelastung die Magnetresonanztomographie (MRT) zum Einsatz. Daneben spielt dieses Verfahren auch für das Staging von Rektumkarzinomen eine Rolle. Kontrastmitteluntersuchungen haben wegen der Verbreitung endoskopischer Verfahren an Bedeutung verloren. Sie kommen zum Einsatz, wenn eine endoskopische Untersuchung nicht möglich ist, etwa durch eine hochgradige Stenose des Darmlumens. Mit dieser Technik können beispielsweise Divertikel dargestellt werden.
Die funktionelle Untersuchung des Vorgangs der Stuhlausscheidung mit Kontrastmittel oder MRT wird Defäkographie genannt.

Auch der Stuhl selbst kann Gegenstand der Untersuchung sein. Insbesondere der Nachweis von sichtbarem (Hämatochezie) oder nicht sichtbarem (Guajak-Test) Blut im Stuhl kann Ausgangspunkt weiterführender Diagnostik sein.

Chirurgische Eingriffe können am Dickdarm von Mensch und Tier vorgenommen werden.

Beim Menschen werden diese Eingriffe relativ häufig durchgeführt. Die folgende Liste soll einen Überblick über die typischen Operationsverfahren beim Menschen geben. Grundsätzlich unterscheidet sich die Operationstechnik bei gutartigen Erkrankungen (wie Entzündungen) und bösartigen Erkrankungen. Bei bösartigen Erkrankungen, wie Tumoren, werden nach den Prinzipien der onkologischen Chirurgie größere Teile des Dickdarms mit umliegendem Gewebe entfernt, um mögliche Metastasen in den Lymphgefäßen und -knoten mit zu entfernen. Alle Verfahren können offen mit Laparotomie oder laparoskopisch durchgeführt werden. Bei der laparoskopisch assistierten Operation wird das Operationsgebiet laparoskopisch präpariert. Die Resektion selbst erfolgt dann offen chirurgisch. Welche Operationstechnik angewandt wird, hängt von der Art der Erkrankung ab: während bei gutartigen Erkrankungen die Laparoskopie einen hohen Stellenwert hat, war sie bei Darmkrebs-Operationen lange umstritten. Mittlerweile wurde aber nachgewiesen, dass die Langzeitergebnisse der Laparoskopie bei lokal begrenzten Tumoren den Langzeitergebnissen der offenen Chirurgie ähnlich sind, die Laparoskopie ist daher auch in diesen Fällen ein etabliertes Verfahren.

In der Tiermedizin werden chirurgische Eingriffe vor allem bei Pferden mit Koliken vorgenommen, die häufig vom Dickdarm ausgehen. Hier sind es vor allem Verdrehungen, Verstopfungen und Einstülpungen des Blinddarms und des „großen Colons“ (Colon ascendens), die meist nur chirurgisch zu beheben sind. Bei Hunden und Katzen werden chirurgische Eingriffe vor allem bei Mastdarmvorfällen, Tumoren oder einem Megacolon durchgeführt. Eine Verankerung des absteigenden Colons an der rückenseitigen Rumpfwand (Colopexie) kann sowohl bei Mastdarmvorfällen als auch bei Perinealhernien angezeigt sein.



</doc>
<doc id="1214" url="https://de.wikipedia.org/wiki?curid=1214" title="DNA (Begriffsklärung)">
DNA (Begriffsklärung)

Die Abkürzung DNA steht für:

Siehe auch:



</doc>
<doc id="1215" url="https://de.wikipedia.org/wiki?curid=1215" title="Düssel">
Düssel

Die Düssel ist ein rund 40 Kilometer langer rechter Nebenfluss des Rheins in Nordrhein-Westfalen. Sie entspringt in Wülfrath-Blomrath an der Stadtgrenze zu Velbert-Neviges im Kreis Mettmann. Nach einem Verlauf durch die Städte Wülfrath, Wuppertal, Mettmann, Haan und Erkrath mündet sie im Stadtgebiet von Düsseldorf vierarmig in den Rhein. Die Düssel ist die Namensgeberin für den Wülfrather Ortsteil Düssel und das dortige Haus Düssel, die Stadt Düsseldorf und deren Ortsteil Düsseltal.

Der Name "Düssel" geht wahrscheinlich auf das germanische "thusila" zurück und bedeutet „brausen, rauschen, tosen“, althochdeutsch "doson". Um 1065 wird der Bach als "Tussale" (die Brausende, Rauschende, Tosende) bezeichnet.

Die Düssel durchquert laut dem Handbuch der naturräumlichen Gliederung Deutschlands in ihrem Lauf mehrere Naturräumliche Einheiten. Der Quellbereich liegt in dem Düsselhügelland (337.18), der Oberlauf berührt kurz das Dornaper Kalkgebiet (337.16) und der Mittellauf mit dem Neandertal durchquert die Mettmanner Lößterrassen (337.00). Es schließt sich bei Alt-Erkrath ein kurzer Abschnitt durch den Naturraum Düsseltalmündung (550.13) an. Das Mündungsdelta befindet sich in der Düsseldorf-Duisburger Rheinebene (575.30).

Die Düssel bildet sich aus vier bis acht Quellgewässern. Die höchstgelegene der Quellen, die der eigentlichen Düssel, befindet sich in Wülfrath bei Gut Blomtrath an der Stadtgrenze zu Neviges. Von hier durchfließt die Düssel die Wülfrather Ortsteile Schlupkothen, Aprath und Düssel, am ehemaligen Schloss Aprath und der Aprather Mühle vorbei, bevor sie in Hahnenfurth und Schöller Wuppertaler Gebiet passiert und ihren Lauf auf dem Gebiet der Stadt Haan fortsetzt. Hier vereint sie sich im Ortskern von Gruiten-Dorf mit der Kleinen Düssel, einem Zufluss, der 4,3 km lang ist und östlich in der Gemarkung Bolthausen im Wuppertaler Stadtteil Vohwinkel entspringt.

Westlich von Gruiten fließt sie durch das Naturschutzgebiet Neandertal und markiert dort die Stadtgrenze zwischen Erkrath und Mettmann. Dort hat sich der Fluss tief in den Untergrund aus devonischen Tonschiefern und Riffkalksteinen eingeschnitten und bildet so ein teilweise enges Tal, das in der ersten Hälfte des 20. Jahrhunderts zwischen Gruiten und Braken nur durch die Trassen einer werkseigenen Kleinbahn für den Kalksteinabbau erschlossen war (heute Wegetrassen). Die schluchtartige Enge der "Flasche" ist mit dem Kalksteinabbau verschwunden. Die zweite Enge am 1672 erwähnten und 1986 restaurierten und zwischen den Höfen Thunis und Bracken befindlichen Kalkofen "Huppertsbracken" ist noch erhalten. Zahlreiche Mühlengebäude wie die Winkelsmühle liegen in diesem Talabschnitt. Teilweise sind Anlagen zur Wiesenbewässerung erhalten "(Flößgräben)".

Als größter Nebenbach fließt der Düssel, aus einem weiträumigeren Tal kommend, der "Mettmanner Bach" zu. An dieser Stelle, einer kleinen Talweitung, querte die mittelalterliche "Kölnische Straße (Strata coloniensis)" das Tal. Danach verengte es sich abrupt zu einer für den nordwestdeutschen Raum außergewöhnlichen Schlucht, "Gesteins" oder "Hundsklipp" genannt.

Diese Schlucht wurde ab etwa 1800 nach dem berühmten, in Düsseldorf lebenden Pastor, Komponisten und Kirchenmusiker Joachim Neander "Neandershöhle" und ab etwa 1850 "Neanderthal" genannt. In der ersten Hälfte des 19. Jahrhunderts war das Tal ein beliebtes Ziel für die Maler der Düsseldorfer Schule, die hier ihre Studien an Felsformationen und Pflanzen betrieben. 1837 regte das Neanderthal den Maler Eduard Steinbrück zum allegorischen Bild "Die Nymphe der Düssel" an, das Prinz Carl von Preußen ihm abkaufte. Durch den Kalksteinabbau ab 1849 ist die Enge des früheren Neanderthals, die die Maler zu romantischen Motiven angeregt hatte, verschwunden und ein weiträumiges Tal entstanden.

An der Einmündung des Mettmanner Baches befinden sich der Kunstweg MenschenSpuren und das 1996 eingeweihte Neanderthal Museum. Etwas weiter dem Düssellauf folgend befindet sich an der pittoresken Felsnase "Rabenstein" die Fundstelle des berühmten Fossils Neandertal 1, das für den Neandertaler namensgebend war, einen Urzeitmenschen des Pleistozäns. Die Fundstelle konnte 1997 wiederentdeckt werden und wurde vor einigen Jahren der Öffentlichkeit zugänglich gemacht.

Aus dem Neandertal kommend durchfließt die Düssel die Stadt Erkrath, wo sie unter anderem zwischen den 1950er und 1970er Jahren wegen der Ausbreitung der Wohngebiete, dem Straßenbau und Neubauten wie der Stadthalle an einigen Stellen in ein neues Flussbett verlegt wurde. Hier münden der Hubbelrather Bach, in den kurz vorher der Stinderbach mündete, und der Rotthäuser Bach in die Düssel. Hinter dem Ortsausgang schlängelt sie sich an Haus Morp vorbei durch die überwiegend landwirtschaftlich genutzte Fläche zwischen Erkrath und Düsseldorf-Gerresheim; dies ist der letzte noch naturbelassene Abschnitt des Flusses.

Für Düsseldorf ist die Düssel namensgebend. Ein Binnendelta mit vier Armen zum Rhein liegt komplett auf dem Stadtgebiet. Erstmals teilt sich der Fluss am Höherhof in Düsseldorf-Gerresheim in die "Nördliche" und die "Südliche Düssel". Beide Bachläufe teilen sich später erneut. Von der Nördlichen Düssel zweigt der Kittelbach ab, von der Südlichen Düssel der Brückerbach. Zur Unterscheidung spricht man ab dort mit Sicht auf das gesamte Delta auch von der Inneren Nördlichen bzw. Südlichen Düssel. Die nur wenig voneinander entfernten unterirdischen Mündungen dieser beiden in der Düsseldorfer Altstadt verlaufenden Arme sind wegen des Rheinufertunnels verrohrt, während sich die Mündungen der beiden anderen Bäche naturbelassen in den Rhein erfließen. Durch Spaltwerke an den drei Scheiden kann der Abfluss geregelt werden. Bei Hochwasser des Rheins wird so der Zufluss zu den inneren Armen gedrosselt, so dass deren kritischere Mündungen in der Innenstadt entlastet werden. Fast alle Wasserarchitekturen der Stadt mit Ausnahme der des Benrather Schlosses werden von Düsselwasser gespeist.

Im Jahre 2012 durchgeführte Ausgrabungen deuten darauf hin, dass das Viereck Rhein, Mündungsgebiet der Südlichen und Nördlichen Düssel sowie ein historisch diese beiden Arme künstlich verbindender Stadtgraben ehemals die Verteidigungslinie der Kernstadt Düsseldorf darstellten.

Nach der Teilung der Düssel am Höherhof in Gerresheim unterquert die Nördliche Düssel die Eisenbahngleise von Düsseldorf nach Wuppertal und das Gelände der ehemaligen Glashütte. Nördlich der Straße "Nach den Mauresköthen" fließt sie wieder oberirdisch aber schnurgerade und kanalisiert an Kleingärten vorbei nach Nordwesten bis sie die Dreherstraße kreuzt. Hiernach fließt sie durch den Ostpark, quert die Grafenberger Allee und die Simrockstraße, dahinter auch die Graf-Recke-Straße und verläuft weiter in Richtung Norden. Am Spaltwerk Heinrichstraße, das sich an deren Beginn befindet, wird der Kittelbach abgezweigt, der weiter nach Norden verläuft, während die Innere Nördliche Düssel ihren Weg nach Westen in Richtung Zoopark im Stadtteil Düsseltal findet. Durch diesen Park, wo sich früher der Düsseldorfer Zoo befand, fließt sie hindurch und ist einer der wenigen Stellen im Düsseldorfer Stadtgebiet wo die Düssel relativ naturbelassen durch eine Grünanlage fließen darf. Hinter dem Zoopark durchquert sie das Gelände des Eisstadions an der Brehmstraße zwischen dem Hauptstadion und der neuerbauten Trainingshalle, unterfließt die Brehmstraße und verläuft dann oberirdisch an der Kühlwetterstraße entlang unter der Grunerstraße hindurch zur Buscher Mühle. Hiernach kreuzt sie die Nord-Süd Bahntrasse, an der Yorckstraße wurde die nördliche Düssel in einem kleinen Park renaturiert (hier ist eine Erweiterung im Zuge der Neuen Stadtquartiere Derendorf im Bau), um dann wieder für mehrere hundert Meter unterirdisch verrohrt quer durch die Stadt zu verlaufen. Hierbei kreuzt sie die Bülowstraße und die Sommersstraße, knickt dann zur Jülicher Straße ab, um dann an der Annastraße wieder kurzzeitig zum Vorschein zu kommen. Danach fließt sie erneut als Tunnel in Richtung Süden an der Eulerstraße entlang, kommt wieder zum Vorschein und fließt an der Prinz-Georg-Straße zunächst als sehr schmaler, kanalisierter Bach zwischen den Straßen entlang, bevor sie abermals unterirdisch verläuft. Nachdem sie die Vagedesstraße gekreuzt hat, tritt sie wieder hervor. Im Malkastenpark, am Jacobihaus und am Malkasten-Haus in Pempelfort bildet sie ein wichtiges landschaftliches Gestaltungselement und speist den „Venusteich“. Als Brunnenskulptur „Düsselnixe“ erwies ihr 1898 dort der Bildhauer Leo Müsch die künstlerische Reverenz. In kanalartiger Strenge flankiert sie dann die Gartenanlagen an der Goltsteinstraße und an der Seufzerallee im Alten Hofgarten, speist dort das Bassin am Ende der Reitallee und fließt in den Neuen Hofgarten, wo sie den Teich an der Landskrone bildet und unter der Goldenen Brücke hindurchfließt. Unter der Heinrich-Heine-Allee wird sie wieder zum Tunnel und fließt, für die Altstadtbesucher unsichtbar, an der Kunstsammlung Nordrhein-Westfalen, am Grabbeplatz und am Kom(m)ödchen unter der Mühlenstraße vorbei. An der Josef-Wimmer-Straße tritt sie dann letztmals wieder hervor und fließt im Bereich der Liefergasse in einem Kanal. Direkt zu Beginn des Burgplatzes fließt sie verrohrt quer und unsichtbar unter dem Platz hindurch, am Schlossturm vorbei, um dann von außen nicht erkennbar und nur wenige hundert Meter vom Einleitungsbereich der Südlichen Düssel entfernt an einer Anlegestelle in den Rhein abgeleitet zu werden. An dieser Stelle verlieh die Düssel schon im Mittelalter dem Dorf an seinem Ufer den Namen "Düsseldorf". Dort werden mit dem Wasser der nördlichen Düssel auch der Neptunbrunnen und damit auch der Stadtgraben auf der Königsallee gespeist. Im Jahre 2012 wurde der historisch mit dieser Aufgabe betraute Kanal wieder gefunden.
Der Kittelbach ist ein Nebenarm der Nördlichen Düssel, der im Verlauf der Heinrichstraße in Richtung Norden abzweigt. Im Vergleich zur Nördlichen Düssel ist er weniger unterirdisch verrohrt. Er verläuft in Richtung Nordwesten zum Mörsenbroicher Ei, am ARAG-Tower vorbei und dann entlang der Grashofstraße. Hier ist der Bach teilweise vertunnelt und ändert seine Richtung nach Norden. Nachdem der Kittelbach die Nord-Süd Bahntrasse und ein großes Industriegebiet in Düsseldorf-Derendorf unterflossen hat, kommt er wieder und fließt durch den Stadtteil Unterrath. Er kreuzt die Straße An der Piwipp und verläuft weiter nahezu geradewegs nach Norden. In Unterrath verläuft der Bach Richtung Nordwesten und kreuzt die Unterrather Straße. Im weiteren Verlauf wurde das Flussbett renaturiert, kreuzt die Autobahn A44 und schlängelt sich dann durch einen Parkplatzbereich des Flughafens. Hiernach fließt der Kittelbach 
quer durch das Gelände des Düsseldorfer Flughafens und seiner Start- und Landebahnen und kommt dahinter wieder zum Vorschein, um nun westlich in Richtung Düsseldorf-Kaiserswerth zu fließen. Dort kreuzt er die Alte Landstraße und die Niederrheinstraße und mündet südlich der Ruine der Kaiserpfalz Kaiserswerth in den Rhein.

Der Kittelbach war Namensgeber für die „Kittelbachpiraten“, einen 1925 gegründeten, „militant rechtsgerichteten Jugendverband“, dessen Mitglieder sich 1933 größtenteils der Hitlerjugend oder der SA anschlossen. Im Zweiten Weltkrieg wurde die Bezeichnung von einer losen Gruppe widerständiger und verfolgter Jugendlicher übernommen, vergleichbar den Edelweißpiraten in Köln.

Nach der Teilung der Düssel am Höherhof in Gerresheim fließt die Südliche Düssel oberirdisch in südlicher Richtung durch Düsseldorf-Vennhausen zunächst parallel zum Reichenbacher Weg. Hierbei kreuzt sie den Sandträgerweg. Anschließend verläuft sie ungefähr parallel zum Neusalzer Weg und Kamper Weg, bis sie die Güterbahnstrecke Duisburg–Köln nördlich des Bahnhofes Eller erreicht. Sie unterquert diese Bahnstrecke und weniger als 100 Meter weiter – inzwischen auf dem Gebiet von Düsseldorf-Eller – auch noch die Gleise der S-Bahn-Linie 1. Nun fließt sie parallel zur Vennhauser Allee, unterquert die Gumbertstraße, in einem sehr spitzen Winkel die Karlsruher Straße und schließlich die Heidelberger Straße. Anschließend verläuft sie zwischen Reiterhof und Schützenplatz zur Unterführung der Bahnstrecke Düsseldorf–Köln etwa 200 Meter südlich des S-Bahn-Haltepunktes Eller Süd. Hinter der Unterführung fließt die Südliche Düssel in südwestlicher Richtung in einem gewissen Abstand parallel zur Straße Am Straußenkreuz bis zu deren Ende. Anschließend am Rand des Friedhofes Eller verlaufend behält sie zunächst ihre Richtung bei. Auf diese Weise erreicht sie die Nähe der Autobahn A 46. Dort, wo auch der "Eselsbach" in die Südliche Düssel einmündet, macht sie eine Rechtskurve und fließt zunächst auf der nördlichen Seite rund 200 Meter parallel zur Autobahn und Friedhofsgrenze.

Vor seiner Einmündung fließt der Eselsbach, als Vereinigung von Sedentaler Bach (Quelle in Millrath), Mahnerter Bach und Hühnerbach (Quellen nördlich von Haan), in Erkrath-Sandheide am Unterbacher See vorbei, durch den Eller Forst sowie den Schlosspark Eller und vereinigt sich rund 700 Meter vor seiner Mündung mit dem Hoxbach, der in Haan entspringt, die Hildener Heide sowie Hilden-Nord durchfließt und den Menzelsee, den Hasseler Forst, den Stadtteil Hassels parallel zur A 59 und Ikea Reisholz tangiert. Der Bereich der südlichen Düssel ab Zulauf des "Eselsbaches" bis zum neuen Spaltwerk wurde ab Oktober 1919 reguliert. Zur gleichen Zeit wurden im Bereich des Spaltwerkes und in Höhe der Straße "Werstener Feld" zwei neue Betonbrücken über die Düssel errichtet.

Seit dem Ausbau der A 46 in 1970er Jahren unterquert die Südliche Düssel in Höhe der Straße Werstener Feld die Autobahn und fließt anschließend oberirdisch auf der südlichen Seite parallel zur Autobahn weiter. Rund 250 Meter vor der Anschlussstelle Düsseldorf-Wersten, wo sich die Autobahn in einem Tunnel befindet, wird sie in einem Düker unter die Autobahn geführt. Hier zweigt am Spaltwerk Wersten der Brückerbach ab. Auf der nördlichen Seite fließt die Innere Südliche Düssel wieder offen zunächst entlang der Nixenstraße und dann hinter den östlichen Grundstücken der Kölner Landstraße. Anschließend unterquert sie die Kölner Landstraße an der Abzweigung Harffstraße, fließt durch das östliche Randgebiet des Südparks, an der Mitsubishi Electric Halle vorbei nach Norden und weiter nach Westen entlang dem nördlichen Rand des Volksgartens in unmittelbarer Nähe der Eisenbahnstrecke. Zur Bundesgartenschau 1987, deren Veranstaltungsgelände den Südpark und den Volksgarten umfasste, wurde die Düssel renaturiert und in Höhe der damaligen Philipshalle, heute Mitsubishi Electric Halle, zu einer Teichlandschaft ausgeweitet. Nach der Querung der Straße Auf’m Hennekamp fließt sie zunächst parallel zur Feuerbachstraße und dann in Düsseldorf-Bilk zwischen den beiden Fahrtrichtungen der Karolingerstraße, wo sie unter anderem die Merowingerstraße und die Aachener Straße kreuzt und ihren Weg wieder in Richtung Norden fortsetzt. Den überwiegenden Teil des nun folgenden Flusslaufes verläuft sie verrohrt unterirdisch durch die Innenstadt. Hierbei quert sie die Bilker Allee und fließt unter der Konkordiastraße und dem Fürstenwall hindurch. Dann kommt sie auf dem Schulgelände der Konkordiaschule wieder zum Vorschein und durchfließt offen den Häuserblock zwischen Konkordiastraße und Kronprinzenstraße bis hinter die Reichsstraße (bis 1871 Krautstraße) in Höhe Nr. 15, wo bis 1867 die Kraut-Mühle betrieben wurde. Unterirdisch quert sie die Zufahrt der Rheinkniebrücke, um dann nördlich der Brückenrampe hinter der NRW-Bank wieder in einem renaturierten Abschnitt im Bereich Wasserstraße dem Schwanenspiegel zuzufließen. Diesen durchfließt sie und mündet anschließend nach Unterquerung der Kreuzung Haroldstraße / Kavalleriestraße / Poststraße den Spee’schen Graben. Unterirdisch fließt sie durch Düsseldorf-Carlstadt und dann unter der Schulstraße (wo auch Heinrich Heine zur Schule ging) entlang. Sie mündet zuerst in den kleinen Binnenhafen am Rathausufer, um dann schließlich verrohrt unter der Rheinuferpromenade geführt zu werden, wo sie, unweit und sehr ähnlich wie die Nördliche Düssel, in den Rhein abgeleitet wird.

Der Brückerbach ist ein Mündungsarm der Südlichen Düssel, der bis Anfang des 20. Jahrhunderts weiter nördlich in Höhe der "Harffstraße" vom südlichen Düsselarm abzweigte und in "Himmelgeist" in den Rhein floss. Anfang des Jahrhunderts wurde die geplante Kiesgewinnung südwestlich von der historischen "Scheidlings Mühle" durch den alten Bachverlauf behindert. 1908 wurde deshalb im Bereich des aktuellen "Werstener Kreuzes" ein "Spaltwerk" errichtet und dadurch das Bachbett nach Süden verlegt.

Der Brückerbach zweigt vor dem Düker im Bereich des Spaltwerkes ab, fließt durch einen eigenen rund 500 Meter langen Tunnel und kreuzt auf diese Weise sowohl die Kölner Landstraße wie auch die BA 46. Anschließend ist er bis zu seiner Mündung beidseitig eingedeicht, um die Wohngebiete vor Überflutung durch Rheinhochwasser zu schützen. Zwischen 2005 und 2008 wurden nicht nur die Deiche erneuert, sondern auch das Bachbett renaturiert, sein Gefälle reduziert und in diesem Zusammenhang drei Fischtreppen gebaut. Zunächst verläuft der Brückerbach parallel zur Straße Am Gansbruch erst in südlicher dann in westlicher Richtung. Danach wendet er sich wieder nach Süden und bildet die Grenze zwischen den Stadtteilen Wersten und Bilk. Auf der Bilker Seite liegt der Botanische Garten der Heinrich-Heine-Universität. Unmittelbar vor der Unterführung wendet sich der Brückerbach wieder nach Westen. Nachdem er auch die Himmelgeister Straße und die Zufahrt zum Wasserwerk Flehe (ehemals Himmelgeister Landstraße) unterquert hat, fließt er in das für Fußgänger gesperrte Wasserschutzgebiet "Fleher Wäldchen", wo er am südlichen Ende unbemerkt in den Rhein mündet.

Der Düssellauf ist abschnittsweise naturgeschützt. Auf Wuppertaler Stadtgebiet bei Hahnenfurth und Schöller ist eine Fläche von rund 32 Hektar des Fließgewässers mit seinen Ufern als NSG „Düsseltal“ ausgewiesen. Bei der Grube 7 vor Haan-Gruiten-Dorf berührt die Düssel nur am Rand das 60 Hektar große NSG „Grube 7 und ehemaliger Klärteich“ während hinter Gruiten-Dorf das komplette Düsseltal mit seinen umgebenden Höhen und Seitentälern im 223 Hektar großen NSG „Neandertal“ liegt. Es schließen sich ab dem Neanderthal Museum die Naturschutzgebiete NSG „Laubacher Steinbruch“ (6 Hektar) und NSG „Westliches Neandertal“ (32 Hektar) an. Hinter Erkrath fließt die Düssel durch das 146 Hektar große NSG „Düsselaue bei Gödinghoven“.

Die Naturschutzflächen im Düsseltal und dem Neandertal zwischen Haan und Erkrath sind zugleich als Fauna-Flora-Habitat gemäß der Richtlinie 92/43/EWG innerhalb des Verbundnetzes Natura 2000 ausgewiesen.





</doc>
<doc id="1216" url="https://de.wikipedia.org/wiki?curid=1216" title="Deimos">
Deimos

Deimos (v. griech. Δεῖμος „Schrecken“) steht für:

Siehe auch:


</doc>
<doc id="1217" url="https://de.wikipedia.org/wiki?curid=1217" title="Disjunktion">
Disjunktion

Disjunktion („Oder-Verknüpfung“, von lat. "disiungere" „trennen, unterscheiden, nicht vermengen“), und Adjunktion (von lat. "adiungere", „anfügen, verbinden“) sind in der Logik die Bezeichnungen für zwei Typen von Aussagen, bei denen je zwei Aussagesätze durch ein ausschließendes "oder" oder durch ein nichtausschließendes "oder" verbunden sind:


Seltener gebrauchte Bezeichnungen für die Disjunktion lauten Alternative, Kontrajunktion, Bisubtraktion und Alternation.
Die mehrdeutige Verwendung von „Disjunktion“ etc. ist auf die verschiedenen Rollen des natürlich-sprachlichen oder rückführbar. Die Teilaussagen einer Disjunktion (Adjunktion) werden Disjunkte (Adjunkte) genannt, das die Teilaussagen verknüpfende Wort („oder“) wird als Disjunktor (Adjunktor) bezeichnet.

Die nicht-ausschließende Disjunktion (Alternative, Adjunktion) ist eine zusammengesetzte Aussage vom Typ „A oder B (oder beides)“; sie sagt aus, dass mindestens eine der beiden beteiligten Aussagen wahr ist.


In der polnischen Notation wird für die Disjunktion der Großbuchstabe "A" verwendet:

In der Notation formula_1 einer Verknüpfung von Aussagen steht das Symbol formula_3 (Unicode: U+2228, ∨) für die nicht-ausschließende Disjunktion als aussagenlogischen Junktor. Es ähnelt dem Zeichen formula_4 für die Vereinigungsmenge und erinnert an den Buchstaben „v“, mit dem das lateinische Wort „vel“ anfängt, das für ein solches nicht-ausschließendes Oder steht. 

Die Wahrheitstabelle für die vel-Funktion (OR-Funktion eines "Gatters") als Wahrheitswertefunktion der nicht-ausschließenden Disjunktion ist damit:
Eine Disjunktion ist ein Boolescher Ausdruck, sie ist assoziativ und kommutativ. 

Aus dem Gesagten folgt:

Die Aussage „Tom hilft beim Streichen" oder" Anna hilft beim Streichen“ besteht aus folgenden Teilen:
Keine der beiden Teilaussagen schließt hier die andere aus.
Die Aussage ist falsch, wenn weder Tom noch Anna beim Streichen helfen, ansonsten wahr. Sie ist insbesondere auch wahr, wenn sowohl Tom als auch Anna beim Streichen helfen.

Die ausschließende Disjunktion (Kontravalenz, XOR) ist eine zusammengesetzte Aussage, bei der zwei Aussagen mit der Formulierung „entweder – oder (aber nicht beides)“ verknüpft werden, zum Beispiel die Aussage „Anna studiert "entweder" Französisch "oder" sie studiert Spanisch (aber nicht beides).“ Damit ausgeschlossen ist der Fall, dass beide Teilaussagen wahr sind – im Beispiel also der Fall, dass Anna "sowohl" Französisch "als auch" Spanisch studiert –, eben hierin besteht der Unterschied zur nicht-ausschließenden Disjunktion. Der lateinische Ausdruck für das ausschließende Oder lautet „aut – aut“.

Die Wahrheitstabelle für die aut-Funktion (XOR-Funktion eines "Gatters") als Wahrheitswertefunktion der ausschließenden Disjunktion ist damit:
Aus einer Aussage "A" kann die Disjunktion "A oder B" geschlossen werden.

Für die durch die Disjunktion zur bereits gegebenen Aussage "A" hinzugefügte Aussage "B" müssen keine vorherigen Voraussetzungen erfüllt sein, wie die folgende Beispielableitung zeigt.

Zur Auflösung einer Disjunktion muss aus beiden Teilen der Disjunktion dieselbe Aussage hergeleitet werden können.



</doc>
<doc id="1218" url="https://de.wikipedia.org/wiki?curid=1218" title="Dysplasie">
Dysplasie

Dysplasie (aus altgriech. δυσ- (dys) 'miss-, un-' und πλάσσειν (plassein) 'formen, bilden', neugriechisch: ) bezeichnet in der Humanmedizin und Veterinärmedizin ganz allgemein eine Fehlbildung. 

Ebenso versteht man unter diesem Begriff noch rückbildungsfähige (reversible) Veränderungen von Zellen, Geweben und Organen, die einerseits durch atypische Wachstumsvorgänge und Verlust der Differenzierung gekennzeichnet sind. Hierbei sind die Übergänge zur Anaplasie fließend. Andererseits ist auch eine Aplasie als Dysplasie anzusehen, die aber im Unterschied zur Agenesie durch die Nichtausbildung eines Organes trotz vorhandener Organanlage gekennzeichnet ist.

Bei der Betrachtung des feingeweblichen Aufbaus eines Organs versteht man unter dem Begriff Dysplasie eine Abweichung der Gewebestruktur vom normalen Bild. Treten Dysplasien gehäuft im mikroskopischen Untersuchungsbefund einer histologischen Untersuchung auf, so können dies Krebsvorstufen sein.

Mittelgradige und schwere Dysplasien werden als Präkanzerosen eingestuft, die Vorstufen eines malignen Tumors darstellen.


Es handelt sich bei den Dysplasien von Skelett und Bindegewebe um systemhafte Störungen des Knochen- und Knorpelgewebes. Somit sind sie keine "Organ"-, sondern "Gewebs"defekte.






</doc>
<doc id="1219" url="https://de.wikipedia.org/wiki?curid=1219" title="Dodo">
Dodo

Der Dodo oder auch die Dronte, seltener "Doudo" oder "Dudu", ("Raphus cucullatus", „kapuzentragender Nachtvogel“, früherer lateinischer Name "Didus ineptus") war ein etwa einen Meter großer, flugunfähiger Vogel, der ausschließlich auf der Insel Mauritius im Indischen Ozean vorkam. Der Dodo ernährte sich von vergorenen Früchten und nistete auf dem Boden. Die Forschung geht davon aus, dass die Spezies um 1690 ausstarb. Sein nächster Verwandter ist der ebenfalls ausgestorbene Rodrigues-Solitär ("Pezophaps solitaria") auf der zu Mauritius gehörenden Maskarenen-Insel Rodrigues.

Aus Berichten weiß man, dass der Dodo ein blaugraues Gefieder, einen etwa 23 Zentimeter langen, schwärzlichen, gebogenen Schnabel mit einem rötlichen Punkt sowie kleine Flügel hatte, die ihn nicht zum Fliegen befähigten. Weiterhin bildete ein Büschel gekräuselter Federn den Schwanz und der Vogel legte gelbe Eier. Dodos waren relativ groß und wogen über 20 Kilogramm. Auch wegen seiner schwachen Brustmuskulatur konnte der Dodo nicht fliegen. Das war auch nicht nötig, da er auf Mauritius keine Fressfeinde hatte.

Traditionell hat man vom Dodo die Vorstellung eines massigen, plumpen und unbeholfenen Vogels. Der Biologe Andrew Kitchen erklärt den Eindruck dadurch, dass die alten Zeichnungen überfettete, in Gefangenschaft lebende Vögel zeigen. Da Mauritius trockene und feuchte Jahreszeiten hat, hat der Dodo sich möglicherweise am Ende der Regenzeit Fett angefressen, um so die Trockenperioden, in denen Nahrungsmangel herrschte, zu überdauern. In Verbindung mit der Gefangenschaft, in der Nahrung das ganze Jahr vorhanden war, wurde der Dodo ständig überfüttert.

Eine der wenigen realistischen Abbildungen eines lebenden Dodo schuf der indische Maler Mansur zu Beginn des 17. Jahrhunderts.

Der erste europäische Bericht über die Art stammt von der zweiten Ostindien-Fahrt einer niederländischen Flotte, unter dem Kommando von Jacob Cornelisz van Neck im Jahr 1598. Die Schiffe waren in einem Sturm getrennt worden, ein Teil der Flotte landete im September des Jahres auf Mauritius (damals noch Ilha do Cerne genannt). Eine Gruppe von Seeleuten, die zur Suche nach Wasser und Vorräten an Land geschickt worden war, kehrte mit einigen flugunfähigen Vögeln zurück. Die Insel war damals von Menschen unbewohnt, die Vögel zeigten Menschen gegenüber keine Scheu. Der Reisebericht der Fahrt von 1599 "Waarachtige Beschryving" genannt (nur in englischer und anderen Übersetzungen erhalten), brachte die Existenz des Vogels den Europäern zur Kenntnis. Nach der Beschreibung wären die Vögel „doppelt so groß wie Schwäne“ gewesen, sie wurden von den Seeleuten „Walghstocks or Wallowbirdes“ (in späteren Berichten auch „Walchvoghel“) genannt, nach dem Dialektausdruck wallow (niederländisch walghe), das kränklich, oder auch geschmacklos, bedeuten kann. Dem Text zufolge war das Fleisch wenig wohlschmeckend und benötigte extrem langes Kochen, um genießbar zu werden, so dass die Seeleute andere Vögel bevorzugten. Trotzdem wird in zahlreichen späteren Berichten von der Jagd auf die Vögel als Proviant berichtet.

Die Abbildungen des Dodos in der "Waarachtige Beschryving" wurden nach Erzählungen, also nach Hörensagen, in Europa komponiert, die Graveure übernahmen offensichtlich andere große Vögel als Muster. Auch die Beschreibung ist in großen Teilen fehlerhaft. Spätere Abbildungen, vor allem in "Quinta Pars Indias Orientalis" der Brüder de Bry von 1601, beruhten teilweise wohl auf Skizzen von mitreisenden Schiffsoffizieren und werden im Wesentlichen als korrekt eingeschätzt, sie waren die Vorlage für die meisten der späteren Abbildungen. 

Lebensechte Abbildungen, die aber erst viel später publiziert wurden, erhielten sich in einem Schiffsjournal des Seglers "Gelderlandt" von 1601 bis 1603, welches auch sieben nach dem Leben gefertigte Vogelskizzen enthielt. Der Ornithologe Alfred Newton publizierte sie 1896. Sie zeigen einen plumpen Vogel mit fast rundem Rumpf und einem kurzen, aus wenigen Federn bestehenden Stummelschwanz. Der Name Dodo, der sich später im englischen Sprachraum durchsetzte, taucht zuerst in einem Bericht des Reiseschriftstellers Thomas Herbert aus dem Jahr 1634 auf, seinen Angaben nach stamme er aus dem Portugiesischen.

1690 berichtete der Engländer Benjamin Harry zum letzten Mal von einem Dodo auf Mauritius. Für andere ist der letzte glaubwürdige Bericht bereits die Erzählung über den Untergang einer holländischen Flotte unter Admiral Arnout de Vlaming im Jahr 1662, von denen einige Überlebende, darunter der Berichtende Volkert Evertsz, in einem kleinen Boot Mauritius erreichten. Hier fingen sie Dodos, allerdings nicht mehr auf der Hauptinsel, sondern auf einem kleinen vorgelagerten Inselchen. Bei zahlreichen der späteren Sichtungsberichte wird vermutet, dass sie sich in Wirklichkeit auf die (ebenfalls flugunfähige und ebenfalls ausgerottete) Mauritius-Ralle bezogen, so dass der genaue Zeitpunkt des Verschwindens nicht genau anzugeben ist. In jedem Falle waren die Vögel bereits wenige Jahrzehnte nach ihrer Entdeckung ausgerottet.

Hauptgrund für das Aussterben der Art dürften eingeschleppte Ratten sowie eingeführte und verwilderte Haustiere gewesen sein und hier vor allem Schweine und Affen, welche die Gelege der bodenbrütenden Vögel zerstörten, indem sie ihre Eier fraßen. Da der Dodo ursprünglich keine Feinde besaß, verfügte er über kein Flucht- oder Verteidigungsverhalten. Die Zutraulichkeit des Dodo und die Flugunfähigkeit machten ihn auch für Menschen zu einer leichten Beute. Er war zwar nicht wohlschmeckend, aber als Frischfleisch für lange Seefahrten geeignet. Auch die Eier wurden von Seeleuten in Massen gegessen. Diese beiden Gefahren haben ebenso die Existenz der Galápagos-Riesenschildkröte stark bedroht und einige ihrer Unterarten ausgerottet.
Weniger als 100 Jahre nach seiner Entdeckung war der Dodo ausgestorben. Davon wurde wenig Notiz genommen, bis der Dodo 1865 in "Alice im Wunderland" von Lewis Carroll erwähnt wurde. Mit der Popularität des Buches wuchs auch die Popularität des Vogels.

Einem Forscherteam der Oxford-Universität um Beth Shapiro gelang es 2002, DNA-Bruchstücke aus Knochen zu isolieren. Der DNA-Vergleich zeigte eine enge Verwandtschaft des Dodo mit dem ebenfalls ausgestorbenen Rodrigues-Solitär und der heute noch lebenden ostasiatischen flugfähigen Kragentaube.

Im Juni 2006 entdeckte eine von dem niederländischen Geologen Kenneth Rijsdijk geleitete Forschergruppe auf Mauritius ein ganzes Depot von Tierknochen und Pflanzensamen in einer Grube in einem ehemaligen Moor. Unter diesen wurden auch viele Skelett-Teile des Dodos gefunden, etwa ein vollständiges Bein und ein sehr selten gefundener Schnabel. Rijsdijk schätzte seinen Dodo-Fund als den umfangreichsten überhaupt ein. Der Fund des Dodo-Massengrabes wird von dem niederländischen Forschungsteam auch als Indiz dafür gewertet, dass eine Naturkatastrophe noch vor Ankunft des Menschen einen signifikanten Teil des Dodo-Ökotops und der Dodo-Population ausgelöscht hat. Bei der Naturkatastrophe könnte es sich um einen Zyklon oder ein plötzliches Ansteigen des Meeresspiegels gehandelt haben.

Obwohl einige Museen eine Kollektion von Dodo-Skeletten ausstellen, gibt es bisher weltweit kein vollständig erhaltenes Skelett. Anfang 2011 wurde im Grant-Museum für Zoologie und vergleichende Anatomie der Universität London bei einem Umzug in einer Schublade die Hälfte eines Dodos entdeckt. Ein Dodo-Ei wird im East London Museum in Südafrika gezeigt.

Der Dodo gilt als ein Paradebeispiel für eine vom Menschen ausgerottete Art und wird als solche in zahlreichen, wissenschaftlichen und populären, Büchern über Aussterbevorgänge als Beispiel aufgeführt, ihm wurden außerdem zwei Sonderausstellungen der zoologischen Museen in Amsterdam und in Zürich gewidmet. Über die Gründe der Popularität gerade dieser, früh ausgestorbenen, vergleichsweise schlecht dokumentierten und für das gewöhnliche Schönheitsempfinden eher unästhetischen Art ist viel spekuliert worden. Als mögliche Gründe werden ihr Auftritt im dritten Kapitel des berühmten Kinderbuchs Alice im Wunderland oder die englische Redensart „Dead as a dodo“ angeführt. Im goldenen Zeitalter der niederländischen Seefahrt im 16. und 17. Jahrhundert stand der Vogel mit für Exotik und die Größe der Entdeckungen der Nation, dies zeigt sich in künstlerischen Darstellungen, etwa durch den Maler Roelant Savery, oft in exotischer Landschaft (vgl. unten) oder inmitten anderer exotischer Vögel. Auch Thomas Pynchon beschreibt die Ausrottung der Dodos durch niederländische Kolonisten in einer Episode seines Großromans "Die Enden der Parabel". Dort dient die sie zur Illustration des Auslöschungsdrangs des Menschen.

Im Staatswappen von Mauritius ist der Dodo einer der Schildhalter; an ihn erinnern Münzen, die 1971 von Mauritius herausgegeben wurden.

Der Samen des Calvariabaumes (Dodobaum) "Sideroxylon grandiflorum", eines früher häufig vorkommenden Baumes auf Mauritius, kann nur schwer zum Keimen gebracht werden. Die Theorie, dass er nur nach Passage des Darmtrakts des Dodo keimt, ist aber nicht ausreichend belegt.

Der früheste schriftliche Beleg für das Wort "Dodo" stammt aus dem Tagebuch von Kapitän Willem van West-Zanen von 1602. Allerdings ist nicht auszuschließen, dass der Begriff Dodo auch früher schon verwendet wurde. Der Ursprung des Wortes Dodo ist unbekannt und wird daher kontrovers beschrieben:


Mit dem Rodrigues-Solitär auf Rodrigues wurde der Dodo (früher wissenschaftlich auch "Didus ineptus" genannt) in der Familie der "Dronten" (Raphidae) innerhalb der Ordnung Taubenvögel zusammengefasst. Nach Gesichtspunkten der Abstammungsgeschichte (Phylogenese) müssen diese zwei Arten in die Familie der Tauben (Columbidae; A. Janoo 2005) gestellt werden.
Alle Dronten waren flugunfähige, große Vögel, die ausschließlich auf je einer der Inseln des Maskarenen-Archipels lebten.

Vom rätselhaften Réunion-Solitär („"Raphus solitarius"“, „Weißer Dodo“) von der Insel Réunion sind nur einige schwer interpretierbare Abbildungen übriggeblieben. Nach einer neueren Theorie ist er identisch mit dem ausgestorbenen Ibis "Threskiornis solitarius". Nach anderen Ansichten handelte es sich um Vögel, die durch Seefahrer von Mauritius nach Réunion gebracht worden waren. Das hellere Gefieder wäre dann dadurch erklärbar, dass es sich um Albinos oder um Jungvögel gehandelt haben könnte.

Der Dodo ist das Wappentier im Wappen von Mauritius. Hier ist er der (heraldisch) rechte Schildhalter.

Der Asteroid (6336) Dodo wurde nach dem Dodo benannt.




</doc>
<doc id="1220" url="https://de.wikipedia.org/wiki?curid=1220" title="Damm">
Damm

Damm steht für:

Damm ist der Name folgender Orte:
DAMM steht als Abkürzung für:
Siehe auch:


</doc>
<doc id="1222" url="https://de.wikipedia.org/wiki?curid=1222" title="Divisionsalgebra">
Divisionsalgebra

Divisionsalgebra ist ein Begriff aus dem mathematischen Teilgebiet der abstrakten Algebra. Grob gesprochen handelt es sich bei einer Divisionsalgebra um einen Vektorraum, in dem man Elemente multiplizieren und dividieren kann.

Eine Divisionsalgebra ist eine nicht notwendigerweise assoziative Algebra formula_1, in der zu je zwei Elementen formula_2 die Gleichungen formula_3 und formula_4 stets eindeutige Lösungen formula_5 besitzen. Dabei bezeichnet "·" die Vektormultiplikation in der Algebra. Das ist gleichbedeutend damit, dass die Algebra frei von Nullteilern ist.

Enthält die Divisionsalgebra ein Element 1, so dass für alle formula_6 gilt, dass formula_7, so spricht man von einer Divisionsalgebra mit Eins.

Beispiel einer Divisionsalgebra ohne Einselement mit den beiden Einheiten formula_8 und formula_9, die mit beliebigen reellen Zahlen multipliziert werden können:

formula_10

Eine endlichdimensionale Divisionsalgebra über den reellen Zahlen hat stets die Dimension 1, 2, 4 oder 8. Das wurde 1958 mit topologischen Methoden von John Milnor und Michel Kervaire bewiesen.

Die vier reellen, normierten, Divisionsalgebren mit Eins sind (bis auf Isomorphie):
Dieses Resultat ist als Satz von Hurwitz (1898) bekannt. Alle außer den Oktaven erfüllen das Assoziativgesetz der Multiplikation.

Jede reelle, endlichdimensionale und assoziative Divisionsalgebra ist isomorph zu den reellen Zahlen, den komplexen Zahlen oder zu den Quaternionen; dies ist der Satz von Frobenius (1877).

Jede reelle, endlichdimensionale kommutative Divisionsalgebra hat maximal die Dimension 2 als Vektorraum über den reellen Zahlen (Satz von Hopf, Heinz Hopf 1940). Dabei wird Assoziativität nicht vorausgesetzt.

Heinz Hopf zeigte 1940, dass die Dimension einer Divisionsalgebra eine Potenz von 2 sein muss. 1958 zeigten dann Michel Kervaire und John Milnor unabhängig voneinander unter Benutzung des Periodizitätssatzes von Raoul Bott über Homotopiegruppen der unitären und orthogonalen Gruppen, dass die Dimensionen 1, 2, 4 oder 8 sein müssen (entsprechend den reellen Zahlen, den komplexen Zahlen, den Quaternionen und Oktonionen). Letztere Aussage konnte bisher nicht rein algebraisch bewiesen werden. Der Beweis wurde von Michael Atiyah und Friedrich Hirzebruch auch mit Hilfe der K-Theorie formuliert.

Dazu betrachtet man nach Hopf die Multiplikation einer Divisionsalgebra der Dimension n über den reellen Zahlen als stetige Abbildung formula_11 oder eingeschränkt auf Elemente der Länge 1 (man teile durch die Norm der Elemente, diese ist ungleich Null für Elemente ungleich Null da eine Divisionsalgebra nullteilerfrei ist) als Abbildung formula_12. Hopf bewies, dass es eine solche ungerade Abbildung (das heisst formula_13) nur gibt, wenn n eine Potenz von 2 ist. Dazu benutzte er die Homologiegruppen des projektiven Raums. Es gibt weitere äquivalente Formulierungen zur Existenz von Divisionsalgebren der Dimension n:





</doc>
<doc id="1223" url="https://de.wikipedia.org/wiki?curid=1223" title="Deckung">
Deckung

Der Ausdruck Deckung bezeichnet
Der Ausdruck Deckung bezeichnet im Zusammenhang mit Schutz, Verteidigung:
Der Ausdruck Deckung bezeichnet im wirtschaftlichen Zusammenhang:
Der Ausdruck Deckungsgrad bezeichnet in der Statistik den Anteil einer mit einem bestimmten Merkmal gekennzeichneten Teilmenge an einer Gesamtmenge.

Siehe auch:


</doc>
<doc id="1226" url="https://de.wikipedia.org/wiki?curid=1226" title="Diogenes">
Diogenes

Diogenes (griechisch: Διογένης, wörtlich „gezeugt von Gott“) war der Name diverser Persönlichkeiten des antiken Griechenlands, insbesondere von Philosophen:


Diogenes bezeichnet:


Siehe auch:


</doc>
<doc id="1227" url="https://de.wikipedia.org/wiki?curid=1227" title="Diogenes von Sinope">
Diogenes von Sinope

Diogenes von Sinope (altgriechisch Διογένης ὁ Σινωπεύς "Diogénēs ho Sinōpeús", latinisiert "Diogenes Sinopeus"; * vermutlich um 410 v. Chr. in Sinope; † vermutlich 323 v. Chr. in Korinth) war ein antiker griechischer Philosoph. Er zählt zur Strömung des Kynismus.

Über den historischen Diogenes sind kaum gesicherte Daten erhalten. Fast alle Informationen wurden in Form von Anekdoten überliefert, deren Wahrheitsgehalt Gegenstand wissenschaftlicher Spekulationen ist. Die früheste Quelle zu Diogenes ist eine kurze Stelle bei Aristoteles, die mit Abstand wichtigste der allerdings erst im 3. Jahrhundert tätige Doxograph Diogenes Laertios, dessen Bericht sich wiederum auf zahlreiche ältere Autoren stützt – deren Angaben sich schon damals widersprachen. Insgesamt sind die antiken Berichte zu Diogenes überdurchschnittlich zahlreich, besonders in popularphilosophischen Schriften und in der Buntschriftstellerei. Die Verlässlichkeit sämtlicher Zeugnisse zu Diogenes ist umstritten; vermutlich bildeten sich bereits zu Lebzeiten Legenden, und es ist anzunehmen, dass seit seinem Tod etliche Anekdoten hinzuerfunden worden sind.

Die Lebensdaten Diogenes’ sind unbekannt, es liegen dazu verschiedene, teils widersprüchliche Angaben vor. Nach Auswertung der betreffenden Zeugnisse geht man davon aus, dass Diogenes gegen Ende des 5. Jahrhunderts v. Chr., möglicherweise um das Jahr 410 v. Chr. in Sinope am Schwarzen Meer (heutiger Teil der Türkei) geboren wurde und gegen Anfang der 320er Jahre v. Chr. in Athen oder Korinth gestorben ist.

Trotz anderslautender Thesen muss Diogenes spätestens in den frühen 360er Jahren v. Chr. nach Athen übersiedelt sein, vielleicht auch noch früher. Die Gründe für die Übersiedlung von Sinope nach Athen sind unklar, auch wenn dazu verschiedene Anekdoten und Geschichten überliefert sind. So berichten Diogenes Laertios und einige andere Autoren die Legende, dass er geflohen oder verbannt worden sei, weil er selbst oder sein Vater als Bankier oder Beamter der Münze von Sinope Münzen gefälscht hätten. In Athen wurde er Schüler des Antisthenes und machte Bekanntschaft mit den berühmten Philosophen seiner Zeit: mit Platon, Aischines von Sphettos, Euklid von Megara. Hingegen ist die Begegnung mit Aristippos von Kyrene möglicherweise erfunden.

In den antiken Berichten ist des Öfteren davon die Rede, dass sich Diogenes in Korinth aufgehalten habe. Wie oft und wie lange ist unklar, jedenfalls soll er dort auch gestorben sein (nach anderen Versionen allerdings in Athen). Auch um diese Übersiedelung ranken sich Legenden. Nach einer soll Diogenes während einer Schiffsreise von Piraten entführt und auf Kreta als Sklave von einem Korinther als Hausverwalter und Erzieher seiner Söhne erworben worden sein. In Korinth soll er dem Tyrannen Dionysios II. von Syrakus und nach der bekanntesten der Anekdoten auch Alexander dem Großen begegnet sein. Ob diese Begegnung tatsächlich und auch in dieser Form stattgefunden hat, ist umstritten. Die Anekdote taucht bei zahlreichen antiken Autoren in oft unterschiedlichen Variationen auf und wurde ein beliebtes Motiv der bildenden Kunst; die älteste erhaltene Version stammt von Cicero, ausführlicher berichtet Plutarch:

Sein Beiname „der Hund“ "(kýōn)" war ursprünglich vermutlich als auf seine Schamlosigkeit bezogenes Schimpfwort gemeint. Diogenes aber fand ihn passend und hat sich seither selbst so bezeichnet. Eine von vielen Anekdoten, die diesen Beinamen betreffen, ist die, dass sich Alexander der Große bei Diogenes so vorgestellt haben soll: „Ich bin Alexander, der große König.“ Worauf Diogenes gesagt haben soll: „Und ich Diogenes, der Hund.“
Diogenes soll freiwillig das Leben der Armen geführt und dies öffentlich zur Schau gestellt haben. Angeblich hatte er keinen festen Wohnsitz und verbrachte die Nächte an verschiedenen Orten, wie etwa öffentlichen Säulengängen. Als Schlafstätte soll ihm dabei gelegentlich ein Vorratsgefäß" (píthos)" gedient haben. Zu Diogenes’ Ausstattung gehörte laut Diogenes Laertios ein einfacher Wollmantel, ein Rucksack mit Proviant und einigen Utensilien sowie ein Stock. Seinen Trinkbecher und seine Essschüssel soll er nach einer Anekdote weggeworfen haben, als er sah, wie Kinder aus den Händen tranken und Linsenbrei in einem ausgehöhlten Brot aufbewahrten. Ernährt habe er sich von Wasser, rohem Gemüse, wild gewachsenen Kräutern, Bohnen, Linsen, Oliven, Feigen, einfachem Gerstenbrot und Ähnlichem.

Zu Diogenes’ Zeit galt es in Griechenland als unanständig, in der Öffentlichkeit zu essen. Er tat aber nicht nur dies, sondern befriedigte auch seinen sexuellen Trieb vor aller Augen, und zwar der Einfachheit halber durch Masturbation. Einer Anekdote zufolge soll er sich gewünscht haben, auch das Hungergefühl durch einfaches Reiben des Bauches stillen zu können.

Diogenes Laertios überliefert zwei unterschiedliche zu seiner Zeit kursierende Verzeichnisse von Schriften des Diogenes. Die erste Liste umfasst 13 Dialoge, 7 Tragödien und Briefe, die zweite, von Sotion von Alexandria stammende, 12 Dialoge, Chrien und Briefe. Laut Sosikrates von Rhodos und Satyros von Kallatis, so Diogenes Laertios, hat Diogenes allerdings überhaupt keine Schriften verfasst.

Da seine Schriften verloren sind und Berichte zu philosophischen Positionen, die Diogenes vertreten hat, weit seltener sind als die zahlreich überlieferten Anekdoten, sind seine philosophischen Ansichten nur in groben Umrissen bekannt. Es ist davon auszugehen, dass Diogenes – wie sein Lehrer Antisthenes – die grundsätzliche Ansicht vertreten hat, dass richtig glücklich nur der sein kann, der sich erstens von überflüssigen Bedürfnissen freimacht und zweitens unabhängig von äußeren Zwängen ist. Ein zentraler Begriff ist dabei auch die daraus resultierende Selbstgenügsamkeit "(autárkeia)": „Es sei göttlich, nichts zu bedürfen, und gottähnlich, nur wenig nötig zu haben.“

Diogenes erkannte ausschließlich die Elementarbedürfnisse nach Essen, Trinken, Kleidung, Behausung und Geschlechtsverkehr an. Alle darüber hinausgehenden Bedürfnissen solle man ablegen, so soll er sogar gegen die verzichtbaren Bedürfnisse trainiert haben: Um sich körperlich abzuhärten, hat er sich im Sommer in glühend heißem Sand gewälzt und im Winter schneebedeckte Statuen umarmt. Und um sich geistig abzuhärten, trainierte er es, Wünsche nicht erfüllt zu bekommen, indem er steinerne Statuen um Gaben anbettelte. Dieses naturgemäße Sichplagen "(pónoi)" unterschied Diogenes von dem öfter vorkommenden unnützen Sichplagen, dessen Ziel die Erlangung von Scheingütern sei. Etlichen Anekdoten ist schließlich auch zu entnehmen, dass Diogenes Bequemlichkeit nicht nur ablehnte, sondern wohl auch als Ursache vieler Übel seiner Zeit ansah.

Lust "(hēdonḗ)" und Lustempfindungen scheint Diogenes nicht als besonders wertvoll und auch nicht als unbedingt notwendig angesehen zu haben, er nahm aber beispielsweise die Lust, die man bei sexueller Betätigung empfindet, als zumindest unvermeidlich hin. Sexuelle Betätigung (wie etwa Masturbation) sei jedenfalls der Natur gemäß und ein elementares Bedürfnis.

Als ein Beispiel für Abhängigkeit von anderen Personen galt Diogenes der Geschlechtsverkehr mit Frauen, so wird ihm in etlichen Anekdoten eine gewisse Frauenfeindlichkeit nachgesagt. Trotzdem erkannte er die Notwendigkeit des Geschlechtsverkehrs zum Überleben des Menschen an. An der Ehe, einer seiner Ansicht nach zu engen Bindung, hat Diogenes deshalb aber nicht festgehalten – wie Platon trat er hingegen für die Einrichtung der Frauen- und Kindergemeinschaft ein.

Als äußeren Zwang erachtete Diogenes gesellschaftliche Konventionen, die er teils auf radikale Art und Weise ablehnte. Von Dingen wie der öffentlichen Masturbation und anderen provokativen Verstößen gegen den guten Ton war schon die Rede. Diogenes soll in seinen Schriften aber noch andere, äußerst anstößige Standpunkte vertreten haben. In einer seiner Schriften, der "Politeia", soll er etwa geäußert haben, dass nichts gegen das Essen von verstorbenen Menschen und als Opfer geschlachteten Kindern spreche und dass sexuelle Beziehungen zu Müttern, Schwestern, Brüdern und Söhnen erlaubt seien. Bereits Herodot berichtete an einigen Stellen von menschenfressenden Völkern, Stämmen, die Frauengemeinschaft gewohnt waren, anderen, bei denen es Brauch war, die verstorbenen Eltern zu essen und wieder anderen, bei denen Menschen geopfert wurden. Auch war bekannt, dass (ob wahr oder nicht) bei den Persern sexueller Verkehr zwischen Söhnen und Müttern üblich war. Diese Tatsachen veranlassten ihn, die betreffenden gesellschaftlichen Verbote und Konventionen als bloßes Produkt verschiedener eingeübter Gewohnheiten zu betrachten, die sich als Gesetze "(nómoi)", Sitten und Bräuche verfestigt hätten. Aus ihnen resultierende Zwänge seien also nicht von Natur aus richtig, sondern hindern vielmehr daran, ein glückliches Leben zu führen. Wie Herakles müsse man sich über diese Zwänge hinwegsetzen. Ob Diogenes allen Ernstes auffordern wollte, die eigenen Eltern zu essen und mit Geschwistern sexuell zu verkehren oder ob er mit seinen Ausführungen lediglich allgemein auf die Nichtigkeit äußerer Zwänge hinweisen wollte, die den Einzelnen an seinem Glück hindern, kann heute nicht mehr geklärt werden. Zu vermuten ist, dass es auch in den nicht erhaltenen Tragödien um ähnliche Tabubrüche ging.

Ebenfalls in der "Politeia" soll er die Abschaffung aller seinerzeit bekannten Staatsformen gefordert haben, da „die einzige wahre Staatsordnung die Ordnung im Kosmos sei.“ So soll sich Diogenes selbst als einer der ersten als Weltbürger "(kosmopolítēs)" bezeichnet und somit einen Kosmopolitismus vertreten haben. Diogenes’ religiöse Ansichten sind unbekannt, anzunehmen ist aufgrund einiger Anekdoten eine spöttisch-ironische Distanz zu religiösen Fragen.

Die Disziplinen der traditionellen Bildung (wie Grammatik, Rhetorik, Mathematik, Astronomie und Musiktheorie) hielt Diogenes für unnütz und überflüssig. Im Gegensatz zu seinem Lehrer Antisthenes hielt er sogar die Beschäftigung mit Fragen der Dialektik (heute in etwa die Disziplin Logik) für sinnlos und setzte ihr den gesunden Menschenverstand entgegen. An einigen Stellen sind logische Argumentationen in Form von Schlüssen überliefert, die aber weniger als ernsthafte Beschäftigung mit Logik, sondern mehr als vielleicht sogar spottendes Spiel mit logischen Operationen und rein logischen Rechtfertigungen gewisser Ansichten aufgefasst werden können:

Von anderen Philosophen dachte Diogenes gering. Die Lehren seines Lehrers Antisthenes hat er zwar hoch geschätzt und daran angeknüpft, über die Person Antisthenes’ hingegen und seine Umsetzung seiner Lehren war er anderer Meinung. Er soll ihn als weich bezeichnet und mit einer Trompete verglichen haben, die zwar laute Töne von sich gibt, sich selbst aber nicht hören kann.

Nach Diogenes Laertios dürfte das Verhältnis Diogenes’ zu Platon nicht das beste gewesen sein. Dessen Ideenlehre habe Diogenes folgendermaßen ins Lächerliche zu ziehen versucht: „Als Platon sich über seine Ideen vernehmen ließ und von einer Tischheit und einer Becherheit redete, meinte Diogenes: ‚Was mich anbelangt, Platon, so sehe ich wohl einen Tisch und einen Becher, aber eine Tischheit und Becherheit nun und nimmermehr.‘ Darauf Platon: ‚Sehr begreiflich; denn Augen, mit denen man Becher und Tisch sieht, hast du allerdings; aber Verstand, mit dem man Tischheit und Becherheit erschaut, hast du nicht.‘“ Auch Platons Bemühungen um Definitionen verschiedener Begriffe, scheint er nicht ganz ernst genommen zu haben: „Als Platon die Definition aufstellte, der Mensch ist ein federloses zweifüßiges Tier, und damit Beifall fand, rupfte Diogenes einem Hahn die Federn aus und brachte ihn in dessen Schule mit den Worten: ‚Das ist Platons Mensch‘; infolgedessen ward der Zusatz gemacht ‚mit platten Nägeln‘.“

Philosophie- und Kulturhistoriker sowie Künstler stellen Diogenes als denjenigen dar, der anders als sein Lehrer Antisthenes nicht in erster Linie Thesen aufstellte, sondern seine ureigenen Erkenntnisse öffentlich und demonstrativ in die Tat umsetzte. In diesem Zusammenhang wird denn gelegentlich auch der Begriff ‚Aktionsphilosoph’ verwendet.

Aus den Reihen der Philosophen erhielt Diogenes sowohl allerhöchste Zustimmung als auch strikte Zurückweisung. So nannte Plato ihn in diffamierender Absicht einen „rasend gewordenen Sokrates.“ Hegel kritisierte an Diogenes nicht nur dessen Volksnähe, er warf ihm auch vor, „unwichtige Dinge zu wichtig“ zu nehmen.
Dazu gehört die öffentlich zur Schau gestellte Bedürfnislosigkeit. Indem Friedrich Nietzsche in ihr bloß ein „Heilmittel gegen alle socialen Umsturzgedanken“ sah, sprach er dem antiken Philosophen die Subversivität ab.
Michel Foucault dagegen sah in den frechen Eskapaden und in der radikalen Freiheit, die sich Diogenes mit ihnen nahm, zugleich die größtmögliche Chance auf Wahrheit ("Parrhesia"): „Der Mut zur Wahrheit seitens desjenigen, der spricht und das Risiko eingeht, trotz allem die ganze Wahrheit zu sagen, die er denkt.“
Während Plato Diogenes im Vergleich mit Sokrates herabzustufen sucht, stellt ihn Peter Sloterdijk auf eine Stufe mit ihm und deutet die „Bizarrerien seines Verhaltens“ als „Versuch, den listigen Dialektiker komödiantisch zu übertrumpfen.“ Ulf Poschardt holt Diogenes in seiner phänomenologischen Studie über Coolness ganz nah an unsere Gegenwart heran und zieht eine direkte Verbindungslinie zur Popkultur: "Diogenes lebte laut wie ein Popstar".
Beider Interpretation zufolge — so Harry Walter — repräsentiere Diogenes „mit seinem öffentlichen Querliegen eine Kultur der gestischen Subversion.“
Den Gedanken der Subversion übersetzt Natias Neutert 1986 in ein Ein-Mensch-Theater-Stück, bei der er Autor und Akteur in einem ist. Er zeigt auf, wie die „quer zur heroischen Geschichtsbildung liegende Körperphilosophie“ auch heutzutage in der Lage sein kann, „die Posen der großen Wahrheit“ ad absurdum zu führen.

Die kulturgeschichtliche Ausnahmeerscheinung des Diogenes spiegelt sich in einer Fülle künstlerischer Darstellungen wider, sowohl Bilder als auch Skulpturen.
So sind drei nicht mehr vollständige Statuetten gleichen Typs erhalten, von denen angenommen wird, dass sie Diogenes darstellen: Sie zeigen einen bärtigen nackten Mann in vornübergebeugter Haltung.
Eine dieser Statuetten wurde in der Villa Albani in Rom gefunden. Original daran sind jedoch nur Kopf, Rumpf, Schultern und rechter Oberschenkel; alles Übrige ist später hinzugefügt worden. Von den anderen beiden Statuetten gibt es nur noch Bruchstücke der Beine, allerdings fanden sich bei ihnen auch Fragmente eines Hundes und eines Rucksacks, also typische Attribute des Diogenes.
Ein guterhaltenes Mosaik aus dem 2. Jahrhundert zeigt Diogenes in seinem Fass, darunter ist sein Name zu lesen. Das Mosaik befindet sich heute im Römisch-Germanischen Museum zu Köln.

Wilhelm Busch hat der antiken Figur in seiner Bildergeschichte "Diogenes und die bösen Buben von Korinth" ein für ihn typisches humoristisches Denkmal gesetzt, das wesentlich zu dessen Popularität beigetragen hat.

Ausgaben
Übersetzungen

Übersichtsdarstellungen

Untersuchungen

Rezeption

Philosophische Essays



</doc>
<doc id="1236" url="https://de.wikipedia.org/wiki?curid=1236" title="Eigentum">
Eigentum

Eigentum (Lehnübersetzung aus dem lat. "proprietas" zu "proprius" „eigen“) bezeichnet die umfassendste Sachherrschaft, welche die Rechtsordnung an einer Sache zulässt. Merkmale moderner Formen des Eigentums sind die rechtliche Zuordnung von Gütern zu einer natürlichen oder juristischen Person, die Anerkennung der beliebigen Verfügungsgewalt des Eigentümers und die Beschränkung des Eigentümerbeliebens durch Gesetze. Eigentum ist in den meisten Verfassungen als Grundrecht geschützt, aber nicht inhaltlich bestimmt. Der materiale Gehalt des Eigentums ergibt sich aus einer Vielzahl von Gesetzen des Privatrechts und Öffentlichen Rechts (Bodenrecht, Mietrecht, Kaufrecht, Denkmalschutz, Umweltrecht, Steuergesetze etc.; als Besonderheit: Tierschutz) oder gerichtlichen Präzedenzfällen. Man spricht daher auch von Eigentum als einem „Bündel von Rechten und Berechtigungen“, das die Beziehungen und das Handeln zwischen Personen symbolisiert. Der Gehalt des Eigentumsbegriffs ist nicht statisch und naturgegeben, sondern entwickelt sich im Laufe der Zeit durch die gewohnheitsrechtliche Praxis, Rechtsprechung und Gesetzgebung.

Das Institut des Eigentums ist außer in der Rechtswissenschaft Gegenstand verschiedener Wissenschaften. Die Rechts- und Sozialphilosophie fragt nach der Begründung und Rechtfertigung von Eigentum; die Soziologie befasst sich mit der Entstehung, der gesellschaftlichen Bedeutung und den Folgen der Institutionalisierung von Eigentum (Macht, Status, Soziale Ungleichheit), die Geschichtswissenschaft mit dem Einfluss auf und die Prägung durch die historische Entwicklung, die Politikwissenschaft mit den Folgen und möglichen Wirkungen der Gestaltung der Eigentumsordnung. Die Ethnologie untersucht Eigentumsverhältnisse in unterschiedlichen menschlichen Gesellschaften. In der Ökonomie sowie in anderen Wirtschaftswissenschaften gilt ein gesetzlich gesichertes und möglichst unantastbares Eigentumsrecht als wichtige Grundlage für ein funktionierendes Wirtschaftssystem.

Vom Eigentum zu unterscheiden ist der Besitz, der sich auf die tatsächliche Herrschaft über eine Sache bezieht. Bei Miete oder Leihe fallen Eigentum und Besitz regelmäßig auseinander. Wenn der Besitzer nicht durch einen formalen Vertrag (z. B. Mietvertrag) geschützt ist, kann der Eigentümer die Herausgabe einer Sache (z. B. von einem Finder oder Dieb) verlangen. Im Mietvertrag wird der Mieter Besitzer, der Vermieter bleibt jedoch Eigentümer. Der Mieter erhält also die tatsächliche Sachherrschaft, kann aber den gemieteten Gegenstand nicht als Aktivposten (Vermögen) in seiner Bilanz verbuchen. Dies kann nur der Vermieter (Eigentümer). Daraus wird deutlich, dass Eigentum ein Vermögensrecht ist. Ökonomischen Wert hat nicht der Gegenstand an sich, sondern nur der Eigentumstitel, der mit dem Besitz (dem tatsächlichen „Haben“) nicht zusammenfallen muss, sondern ein zusätzlich zum Gegenstand bestehender abstrakter Rechtstitel ist. Wo keine solche Eigentumstitel existieren, kann daher auch keine Geldwirtschaft existieren.

Die Dokumentation von Eigentum kann an einen Rechtstitel oder die Eintragung in ein Register (z. B. Grundbuch) gebunden sein. Der Eigentümer von Booten und Schiffen heißt Eigner, deren Zusammenschluss "Eignergemeinschaft".

"Eigentum" und "Besitz" werden sprachlich oft gleichgesetzt, sind jedoch im juristischen und ökonomischen Kontext streng voneinander zu unterscheiden. So kann ein Gegenstand sich vorübergehend oder auf Dauer im Besitz einer anderen Person als des "Eigentümers" befinden (zum Beispiel bei einer Mietwohnung). Daneben wird der Begriff des Eigentums umgangssprachlich auch für das Objekt des Eigentums verwendet („Das ist mein Eigentum“).

Der Begriff Eigentum wird meist nur in Gesellschaften oder Populationen gebraucht, in denen es eine rechtliche Unterscheidung von Eigentum und Besitz gibt. Den früheren Eskimo-Populationen war beispielsweise der Begriff des Eigentums unbekannt. In sogenannten realsozialistischen Ländern hingegen gab und gibt es oft eine sprachliche, aber weder eine rechtliche Unterscheidung von Eigentum und Besitz, noch wiesen die in diesen Ländern als Eigentum ausgewiesenen Sachen die für das Eigentum charakteristischen ökonomischen Operationsmöglichkeiten (Beleihung, Pfändung, Vermietung, Verpachtung etc.) auf, es handelte sich dabei also stets um Besitz im engeren Sinne.

Rechtlich wird zudem zwischen Eigentum und Vermögenswert unterschieden. Auch wenn Eigentum im Alltag oft mit Privateigentum gleichgesetzt wird, werden auch kollektive Verfügungsrechte an Sachen, die exklusiv von einer Gemeinschaft oder vom Staat ausgeübt werden, als Eigentum bezeichnet.

Über die historischen Wurzeln des Eigentums gibt es wenig gesichertes Wissen. Aus der Steinzeit kennt man Grab-Beigaben, die den Toten mitgegeben wurden. Dabei dürfte es sich um persönliche Habseligkeiten gehandelt haben wie Waffen, Schmuck und Gebrauchsgegenstände, für die eine besondere Bindung an die Person bestand. 
Gesellschaftliches Eigentum entstand bereits in der Frühzeit im Zusammenhang mit der damals vorherrschenden Okkupationswirtschaft zunächst durch Abgrenzung von Jagdrevieren einzelner Horden und Stämme, die diese gegeneinander verteidigten. Wie die Eigentumsrechte am Land in typischen Jäger-und-Sammler-Gesellschaften ausgestaltet sind, ist Gegenstand einer wiederkehrenden ethnologischen Debatte. Die von Henry Lewis Morgan vertretene und später von Friedrich Engels übernommene These eines „Urkommunismus“ in der menschheitsgeschichtlichen Entwicklung wurde durch Frank G. Specks Beispiel familienbezogener Jagdreviere der Algonkin in Kanada in Frage gestellt. Ob diese Familienreviere jedoch schon zu präkolumbianischer Zeit bestanden haben und ob sie als eine dem europäischen Privateigentum ähnliche Institution angesehen werden können, ist weiterhin umstritten. Neuere Forschungsergebnisse weisen darauf hin, dass auch in den Familienterritorien der Algonkin Rechte primär größeren sozialen Gruppen zugeordnet sind. Grundbesitz soll zudem auf spiritueller und sozialer Reziprozität beruhen, das heißt auf wechselseitigen, nicht im Sinne eines Tausches direkt miteinander verknüpften Gaben und Gegengaben. Eigentum gab es schon bei den noch nicht sesshaften Hirtenvölkern. Individuelles Eigentum an Grund und Boden entstand erst im Übergang zum Ackerbau und im Zuge der allmählichen Ablösung der Sippen durch kleinere Familienverbände und die Entstehung von Siedlungen (Neolithische Revolution). Bedrohungen von außen, aber auch gemeinsame Projekte wie der Siedlungswasserbau im Zweistromland, im Industal oder in Ägypten führten zur Institutionalisierung von Herrschaftsstrukturen und schließlich zu den bekannten Königreichen. In diesem Zuge entstanden auch Rechtsordnungen, in denen es möglich war, das Eigentum durchzusetzen. Die älteste bekannte Kodifizierung ist der Codex Ḫammurapi, der bereits Kaufrecht und Erbrecht kannte.

Im 3. Jahrtausend vor Christus entstanden in Mesopotamien die Tempelwirtschaft, in der in regionalen Zentren rund um den Tempel die Wirtschaft in der Hand der Priester lag und die Rechte zur Bewirtschaftung des Landes gegen Abgaben von der Tempelverwaltung vergeben wurde. Gleichzeitig ist privater Grundbesitz anhand von Kaufverträgen in Keilschrift dokumentiert. Reichtum entstand durch kriegerische Ausweitung des Machtbereiches, aber auch durch Handel zwischen den Zentren und ersten Fernhandel. Es entstanden einerseits grundbesitzende Oberschichten, andererseits wurde der Wohlstand durch Sklaven gemehrt.

Die überlieferte Reflexion über die Bedeutung von Eigentum beginnt mit den Werken von Platon und Aristoteles im antiken Griechenland. Die Gesellschaft dieser Zeit war noch ganz überwiegend landwirtschaftlich geprägt. Selbst in der Polis von Athen lebten noch mehr als dreiviertel der Bevölkerung von der Landwirtschaft. Die Gesellschaft wurde vom Adel und von Großgrundbesitzern dominiert, wenn auch die Reformen des Kleisthenes den Bürgern eine Beteiligung an den Entscheidungen der Polis ermöglicht hatten. Gesellschaftlicher und ökonomischer Kern war der Familienhaushalt (Oikos). Zu diesem Haushalt gehörten auch Sklaven, die man kaufte oder die im Zuge der Kolonialisierung nach Athen gelangt waren. Die Schuldsklaverei war durch die Gesetze Solons abgeschafft worden. Im Oikos war alles dem Hausvater untergeordnet, der über das Vermögen, die Frau, die Kinder und die Sklaven die Rechte des Eigentümers ausübte, aber auch die Verantwortung für ihr Wohlergehen hatte.

Platon entwarf in der Politeia das Konzept eines idealen Staates, in dem jeder die ihm angemessene Position einnimmt. So gibt es den Nährstand der Handwerker und Bauern, die auch in diesem Staat über Eigentum verfügen. Den Zusammenhalt des Staates gewährleisten die Wächter (Wehrstand). Diese haben kein Eigentum, sondern erhalten ihr Auskommen von der Gesellschaft und im Gegenzug ist ihr gesamter Lebensbereich, auch die Wohnung, der Öffentlichkeit zugänglich. Auch die Philosophen, die für Platon geeignet sind, nach Erziehung und Ausbildung den Staat zu leiten, bleiben ohne Besitz. In seinem Spätwerk, den Nomoi, setzt sich Platon mit der Frage auseinander, wie die staatliche Ordnung einer noch zu gründenden Kolonie aussehen sollte. Hier sah er eine Verteilung des Grundbesitzes vor. Diese ist allerdings gleichmäßig und der Boden kann nicht verkauft, sondern nur vererbt oder an einen anderen ohne Grundbesitz übertragen werden.

Ähnlich wie für Platon ist für Aristoteles das Ziel des menschlichen Lebens das Gute, nicht der Reichtum, der nur ein Mittel zur Erreichung dieses Ziels ist. Das Institut des Eigentums entstammt nicht der natürlichen Ordnung, sondern ist Ergebnis der menschlichen Vernunft. Individuelles Eigentum ist dem gemeinschaftlichen Eigentum vorzuziehen, weil persönliches Eigentum eine größere Sorgfalt gegenüber den Sachen bewirkt. Zum zweiten entspricht Privateigentum dem Prinzip der Leistung. Des Weiteren regelt Eigentum eindeutig die Zuständigkeiten, so dass Streit vermieden werden kann. Persönliches Eigentum dient dem Genuss in der Gemeinschaft und ist Voraussetzung für die Tugend der Freizügigkeit. Gemeineigentum ist deshalb nur dort sinnvoll, wo es gemeinschaftlich genutzt wird oder einer gemeinsamen Finanzierung bedarf.

Die frühe Kodifizierung des Rechts im antiken Rom war das Zwölftafelgesetz, das den Zweck hatte, die Konflikte zwischen den grundbesitzenden Patriziern und den Plebejern zu ordnen. Kaufverträge wurden hier sehr formalisiert als Libralakte geregelt. Ähnlich wie in Griechenland war die römische Gesellschaft in Haushalten (Dominium: Eigentum, Besitzrecht) organisiert. Der Hausherr, der Pater familias, war uneingeschränkter Eigentümer. Auch erwachsene Söhne waren nicht geschäftsfähig, wenn sie im Haus des Vaters lebten, selbst wenn sie verheiratet waren und Kinder hatten. Der Pater familas konnte seine Kinder sogar in die Sklaverei verkaufen. Er konnte durch Testament sein Eigentum uneingeschränkt vererben. Lag kein Testament vor, erfolgte die Erbfolge in männlicher Linie.

Im römischen Recht gab es keine formale Definition des Eigentumsbegriffs, wohl aber verschiedene Formen des Eigentums. Aus der Beschreibung „meum esse aio“ (ich behaupte, dass es mein ist) lässt sich anhand der Praxis ableiten, dass die Definition in Satz 1 BGB weitgehend mit der inhaltlichen Bestimmung zur Zeit Ciceros übereinstimmt. Cicero setzte sich mit der Begründung von Eigentum auseinander. Für ihn entsteht Privateigentum ursprünglich durch Okkupation. Das Land der eroberten Provinzen betrachteten die Römer als Eigentum des römischen Volkes und begründeten hiermit das Recht auf eine Bodensteuer (Tribut). Die Römer kannten bereits ein Immissionsverbot (siehe BGB), d. h. jemand konnte sein Grundstück nicht beliebig nutzen, wenn er damit den Besitz anderer beeinträchtigte, z. B. durch Entwässerungsgräben, deren Wasser auf fremden Grund abfloss.

Eine neue Sicht auf das Eigentum kam in der Patristik durch die Verbreitung christlich-jüdischer Gedanken auf, nach denen das Naturrecht mit dem göttlichen Recht gleichzusetzen ist. Im Tanach („Altes Testament“) wird das Land dem Menschen zur Verwaltung übergeben – es bleibt aber im Eigentum Gottes. Bei den Kirchenvätern wie Clemens von Alexandria stand daher die von der Stoa übernommene Frage des richtigen Gebrauchs von Eigentum im Vordergrund. Sie forderten, das Eigentum, das über den eigenen Bedarf hinausgeht, an die Armen weiterzugeben. Die Reichen in der Gemeinde haben entsprechend der paulinischen Lehre eine Fürsorgepflicht gegenüber den Armen („Der eine trage des anderen Last“, Gal. 6, 2).

Bei den Germanen hatte sich der Stand der Wehrbauern und das Institut der Allmende entwickelt. Diese Struktur wurde im frühen Mittelalter zur Zeit des Karolingerreiches durch die Herausbildung des Ritterstandes abgelöst, durch den zentrale Herrschaft besser zu sichern war. Die mittelalterliche Eigentumsstruktur war geprägt durch Grundherrschaften, die entweder als Lehen (vom Landesherren verliehenes Nutzungsrecht) oder weniger verbreitet als Allodien (vererbbares Eigentum) bestanden. Grundbesitz in den Städten, aber auch der zum Teil sehr große Grundbesitz der Klöster war zumeist Eigentum (Allod). Auch Allodien waren nicht in jedem Fall frei veräußerlich, sondern waren zum Teil Stammgüter, das heißt von Vorfahren ererbte Immobilien, welche die Bestimmung hatten in derselben Familie zu bleiben (vgl. Familienfideikommiss). Die Landwirtschaft war zumeist autark. Es gab freie und unfreie Bauern. Die Masse des Volkes lebte als Knechte oder Tagelöhner. Es gab die an die Person gebundene Form der Hörigkeit als Leibeigenschaft und die an den Boden gebundene Grundhörigkeit. Während in Italien schon früh die Städte ein Gegengewicht zu den Grundbesitzern gewannen, bildeten sich nördlich der Alpen städtische Strukturen erst allmählich heraus. In den Städten entwickelten sich Handel und Marktrecht, es entstanden vor allem in Flandern Messen, Kaufmannsgilden und Zünfte der Handwerker. Ein Höhepunkt im Hochmittelalter war die Gründung der Hanse.

Eigentum wurde bzw. wird oft gekennzeichnet durch so genannte Hausmarken, zum Beispiel Wappen und Brandzeichen. Der Kennzeichnung von Grundbesitz dienen die auf den Hermes-Kult zurückgehenden Grenzsteine. Für Grundstücke führte Wilhelm der Eroberer in England 1086 das wahrscheinlich erste Grundbuch ein, das Domesday Book. Unabhängig davon führten die mittelalterlichen deutschen Städte Stadtbücher, Vorläufer der heutigen Grundbücher.

Für die Rechtsgeschichte im Mittelalter von besonderer Bedeutung war das Wiederaufleben römischen Rechts angestoßen von den Forschungen der Legisten an den Universitäten, allen voran der Universität Bologna. Dieses hatte auch Einfluss auf das von den Dekretisten vertretene kanonische Kirchenrecht, das im Decretum Gratiani systematisch zusammengefasst wurde.

Thomas von Aquin versuchte eine vermittelnde Position zwischen der Lehre des Aristoteles und den Auffassungen der Patristik zu entwickeln. Ein wichtiger Schritt in der Entwicklung der Auffassung über das Eigentum ist die Lehre Wilhelm von Ockhams, der das als Eigentum bestimmte, was sich vor Gericht einklagen lässt. Das einzige Naturrecht, das Ockham anerkennt, ist das Recht auf Erhalt der eigenen Person. Daraus ergibt sich der Anspruch der Armen, von den Reichen wenigstens soviel zu erhalten, wie sie zum Leben benötigen. Zum Naturrecht gehört auch, dass alle Menschen frei sind, auch wenn das Völkerrecht die Sklaverei zulässt. Gerade in Hinblick auf Sklaven und die Position der Frau stellt er sich gegen die Tradition seit Aristoteles, die von Thomas von Aquin noch vertreten wurde.

Das im Spätmittelalter einsetzende Wachstum der Städte, die zunehmende Zahl der Universitätsgründungen, die Erfindung des Buchdrucks, die Entdeckung Amerikas, Renaissance und Humanismus kennzeichnen strukturelle Veränderungen der Gesellschaft zu Beginn der Frühen Neuzeit. Das Denken wird säkularer, die Kirche wehrt sich mit der Inquisition, muss aber im Zuge der Reformation, der Entwicklung der Naturwissenschaften und der Herausbildung der Nationalstaaten ihren Machtverlust hinnehmen. Die dominierende Herrschaftsform im 17. und 18. Jahrhundert ist der Absolutismus. Die Subsistenzwirtschaft beginnt sich aufzulösen. Die Strukturen des Feudalismus werden allmählich durch Stadtrechte, Dorfordnungen und Verlagerung der Gerichtsbarkeit in die Gemeinden aufgeweicht. In ländlichen Gebieten entstehen Nachsiedlerschichten wie Heuerlinge oder Kötter und Bödner. Die Wirtschaft wird komplexer mit vorindustriellen Produktionsweisen wie Heimarbeit und ersten Manufakturen und einer sich ausbreitenden Marktwirtschaft. Es entwickelt sich der Übergang zum Merkantilismus und zum Physiokratismus. In dieser Zeit entstand auch Geistiges Eigentum als neue Eigentumsform, zunächst als Privilegien, dann auch geschützt durch Patentrecht (Venedig 1474, Großbritannien 1623, Frankreich 1790). In den Bereich der Privilegien fallen auch die Bergordnungen des 15. und 16. Jahrhunderts. Fragen des Urheberrechts wurden erstmals im 18. Jahrhundert geregelt.

Thomas Hobbes, der philosophisch den Absolutismus stützte, entwickelte die Idee des Gesellschaftsvertrages, in dem der Einzelne seine Freiheitsrechte an einen zentralen, allmächtigen Herrscher überträgt. Als absoluter Regent legt dieser Gesetze fest und setzt sie durch. Das Recht des Eigentümers kann niemand einschränken als der Souverän. Der Bürger hat aber auch kein Recht, ihn daran zu hindern.

Nach dem englischen Bürgerkrieg war in England das Bürgertum trotz der Stuart-Restauration so stark geworden, dass es nach dem Habeas Corpus Act (1679) in der Glorious Revolution (1688) mit der Bill of Rights die Souveränität des Parlaments gegen den König durchsetzen konnte. In den Zwei Abhandlungen über die Regierung bewertete John Locke das Eigentum als Grundrecht. Jedoch entsteht Eigentum nicht durch einen Vertrag, wie bei Hobbes, sondern beruht auf überpositivem Naturrecht. In der Begründung des Eigentums geht Locke mit seiner Arbeitstheorie einen völlig neuen Weg. Der Mensch ist von Natur aus berechtigt, zum Zweck der Selbsterhaltung sich einen Teil der Natur anzueignen. Indem der Mensch ein Naturgut bearbeitet, bringt er einen Teil seiner selbst in den Gegenstand ein. Naturgüter haben ohne Arbeit einen nur geringen Wert. Wasser in der Natur gehört niemandem. Das Wasser im Krug ist aber unbestritten zu Eigentum geworden (II § 29). Auch der Wert des Bodens entsteht größtenteils durch Arbeit (II § 43). Der Erwerb von Eigentum, das heißt die Aneignung der Natur hat bei Locke aber dort ihre Grenzen, wo der Mensch das von der Natur durch Arbeit Gewonnene nicht mehr verbrauchen kann (II § 32). Für die Bildung von Reichtum sind die Möglichkeit des Tausches und das Institut des Geldes entscheidend. Indem der Mensch das Ergebnis der Arbeit tauscht, zum Beispiel Äpfel gegen Nüsse, so erhält er etwas weniger Verderbliches. Dieses darf er besitzen, auch wenn er es nicht unmittelbar verwertet. Durch die Einrichtung des Geldes wurde zwischen den Menschen ein Übereinkommen getroffen, dass die Aufbewahrung des Eigentums unbegrenzt erfolgen kann. „Das große und hauptsächliche Ziel, weshalb Menschen sich zu einem Staatswesen zusammenschließen und sich unter eine Regierung stellen, ist also die Erhaltung ihres Eigentums.“ (II § 124). Den unterschiedlichen Reichtum erklärt Locke mit unterschiedlichem Fleiß und den unterschiedlichen individuellen Voraussetzungen der Menschen. Eingriffe ins Eigentum durch den Staat bedürfen immer der Zustimmung der Bürger (II § 139).

Nach Jean-Jacques Rousseau führt die Bildung von Eigentum dazu, dass der Mensch den Urzustand verlässt. „Konkurrenz und Rivalität auf der einen Seite, Gegensatz der Interessen auf der anderen, und stets das versteckte Verlangen, seinen Profit auf Kosten anderer zu machen: alle diese Übel sind die erste Wirkung des Eigentums und das untrennbare Gefolge der entstehenden Ungleichheit“. (Diskurs, 209) „Der erste, der ein Stück Land eingezäunt hatte und dreist sagte: ‚Das ist mein‘ und so einfältige Leute fand, die das glaubten, wurde zum wahren Gründer der bürgerlichen Gesellschaft. Wie viele Verbrechen, Kriege, Morde, Leiden und Schrecken würde einer dem Menschengeschlecht erspart haben, hätte er die Pfähle herausgerissen oder den Graben zugeschüttet und seinesgleichen zugerufen: ‚Hört ja nicht auf diesen Betrüger. Ihr seid alle verloren, wenn ihr vergeßt, daß die Früchte allen gehören und die Erde keinem.‘“ Dennoch betrachtet er das Eigentum als „das heiligste von allen Bürgerrechten, in gewissen Beziehungen noch wichtiger als die Freiheit selbst […], weil das Eigentum die wahre Begründung der menschlichen Gesellschaft und der wahre Garant der Verpflichtung der Bürger ist.“

„Was der Mensch durch den Gesellschaftsvertrag verliert, ist seine natürliche Freiheit und ein unbegrenztes Recht auf alles, wonach ihn gelüstet und was er erreichen kann; was er erhält, ist die bürgerliche Freiheit und das Eigentum an allem, was er besitzt. Damit man sich bei diesem Ausgleich nicht täuscht, ist es notwendig, die natürliche Freiheit, die ihre Schranken nur in der Stärke des Individuums findet, deutlich von der bürgerlichen Freiheit zu unterscheiden, die durch den Gemeinwillen begrenzt ist, und den Besitz, der nur eine Folge der Stärke oder des Rechts des ersten Besitznehmers ist, vom Eigentum, das nur auf einen ausdrücklichen Titel gegründet werden kann.“ (CS I 8). Im republikanischen Staat Rousseaus ist die bürgerliche Freiheit durch das Gemeinwohl begrenzt. Entsprechend kann durch demokratischen Beschluss in die Verteilung des Einkommens eingegriffen und durch progressive Steuern eine größere Verteilungsgerechtigkeit hergestellt werden. „Der, welcher nur das einfach Notwendige hat, muß gar nichts beitragen; die Besteuerung desjenigen, der Überflüssiges besitzt, kann im Notfall bis zur Summe dessen gehen, was das ihm Notwendige übersteigt.“

Ähnlich wie Locke ein Einfluss auf die amerikanischen Verfassungen, insbesondere die Virginia Bill of Rights von 1776 zugeschrieben wird, hatten die Schriften Rousseaus Einfluss auf die Französische Revolution. In Artikel 17 der Erklärung der Menschen- und Bürgerrechte heißt es: „Da das Eigentum ein unverletzliches und heiliges Recht ist, kann es niemandem entzogen werden, es sei denn, dass dies die gesetzlich festgelegte öffentliche Notwendigkeit offensichtlich fordert, und dass eine gerechte und vorherige Entschädigung geleistet wird.“

Zur Bestimmung des Eigentums unterschied Immanuel Kant das innere und das äußere „Mein und Dein“. Das innere Mein und Dein ist das Recht an der eigenen Person. Eigentum als das äußere Mein und Dein besteht nicht von Natur aus, sondern wird erworben, denn es bedarf der Zustimmung eines anderen, weil durch Eigentum die Sphäre des anderen betroffen ist (RL, AA VI 245). Eigentum unterscheidet sich von sinnlichem Besitz dadurch, dass es ein intelligibler Besitz ist, den man sich nur durch den Verstand vorstellen kann. Eigentum ohne staatliche Gewalt ist nur provisorisch. Eigentum ist dann nicht legitimiert, wenn es andere in ihrer Freiheit beschränkt, ohne dass diese zugestimmt haben. Hieraus folgt, dass die Bildung von Eigentum denknotwendig zu einem republikanischen Staat führt.

Der Begriff der Moderne und seine Abgrenzung zur frühen Neuzeit sind unscharf. Für die Theorie des Eigentums ist von Bedeutung, dass sich im Wechsel vom 18. zum 19. Jahrhundert nach den USA und Frankreich eine Reihe von Staaten eine republikanische Verfassung mit der Fixierung von Grundrechten gegeben haben. In einer Reihe von Ländern wurde das Zivilrecht auf der Grundlage des römischen Rechts den neuen Bedürfnissen angepasst (Vernunftrecht). In der wirtschaftlichen Entwicklung setzte sich die Industrialisierung stetig fort. Neben der abhängigen Landbevölkerung entstand in den Städten eine Arbeiterschaft, die in Manufakturen, aber auch in Bergwerken und Großbetrieben der Metallverarbeitung tätig waren. Unzureichende soziale Bedingungen führten zu einer Pauperisierung zunehmender Bevölkerungsteile und dem Aufkommen der Sozialen Frage. Aus der feudalen Ständegesellschaft wird eine Klassengesellschaft, in der das Eigentum an Produktionsmitteln einen wesentlichen Einfluss auf die Stellung in der Gesellschaft ausmacht.

Bereits zu Beginn des 19. Jahrhunderts setzte die Kritik der sich entwickelnden Verhältnisse ein. Für den Frühsozialisten Pierre-Joseph Proudhon galt: „Eigentum ist Diebstahl“. Aber auch romantische Philosophen wie Franz von Baader kritisierten die soziale Lage der Arbeiter. Eigentum war für Karl Marx und Friedrich Engels Ursache der Entfremdung und der Ausbeutung des Arbeiters. „Das Kapital hat die Bevölkerung agglomeriert, die Produktionsmittel zentralisiert und das Eigentum in wenigen Händen konzentriert. Die Arbeiter, die sich stückweise verkaufen müssen, sind eine Ware wie jeder andere Handelsartikel und daher gleichmäßig allen Wechselfällen der Konkurrenz, allen Schwankungen des Marktes ausgesetzt.“ Sie sahen daher im Kommunismus vor allem ein Projekt zur „Aufhebung des Privateigentums“ an Produktionsmitteln und der darauf basierenden Ausbeutung.

Erst die in der zweiten Hälfte des 19. Jahrhunderts einsetzende und seitdem fortschreitende Sozialgesetzgebung verminderte in den westlichen Industrieländern die Konfliktsituation zwischen Besitzenden und Besitzlosen allmählich und mit steigendem Wohlstand begann man von Schichten und schließlich von Milieus zu sprechen. Es bildeten sich bürgerliche Mittelschichten heraus, die ihrerseits Vermögen und Eigentum bildeten. In Russland führte hingegen die Revolution von 1917 zur Bildung eines sozialistischen bzw. kommunistischen Staates, der das Privateigentum an Produktionsmitteln zwar unterdrückte, die Lohnarbeit jedoch beibehielt und noch verschärfte. Hinzu kam nach dem Zweiten Weltkrieg die Ausweitung des Machtbereichs der Sowjetunion in eine Reihe osteuropäischer Länder sowie die sozialistische Staatsbildung in China. Diese Regierungsformen, die das Privateigentum an Produktionsmitteln im Allgemeinen unterdrückten, waren zugleich mit erheblichen Einschränkungen individueller bürgerlicher Freiheiten verbunden und konnten sich teilweise nicht gegen die Konkurrenz und Politik der westlichen Industrieländer durchsetzten. Der Streit um die Frage des Privateigentums an Produktionsmitteln wird von reformistischen Kräften mehr als eine Frage der Verteilungsgerechtigkeit und des zulässigen Umfangs von Privateigentum geführt, aber auch radikale, anarchistische und kommunistische Bestrebungen zur Aufhebung des Privateigentums an Produktionsmitteln bestehen weiterhin weltweit.

Max Weber betrachtet das Eigentum aus der Perspektive sozialer Beziehungen, die er als „offen“ bezeichnet, wenn niemand daran gehindert ist am gegenseitigen sozialen Handeln teilzunehmen. Wenn hingegen die Teilnahme beschränkt oder an Bedingungen geknüpft ist, spricht er von „Schließung“. Eine Schließung erfolgt immer dann, wenn die Beteiligten sich hiervon eine Verbesserung ihrer Chancen zur Befriedigung ihrer Bedürfnisse erwarten. Eine Schließung nach innen, das heißt innerhalb einer Gruppe, nennt Weber Appropriation. Rechte sind daher für ihn eine Appropriation von Chancen. „Erblich an Einzelne oder an erbliche oder Gesellschaften appropriierte Chancen sollen: „Eigentum“ (der Einzelnen oder der Gemeinschaften oder der Gesellschaften), veräußerlich appropriierte: „freies“ Eigentum heißen.“ Eigentum ist ein Instrument zur Regulierung von Beschaffungskonkurrenz. Hierdurch wird die Verfügungsgewalt über Güter beschränkt.

Die katholische Soziallehre schließt an Thomas von Aquin an und fasst das Eigentum als notwendigen Faktor zur Verwirklichung der individuellen Freiheit auf. Auf dem Zweiten Vatikanischen Konzil wurde festgestellt, dass das Privateigentum – auch an den Produktionsmitteln – zur „Selbstdarstellung der Person“ beiträgt und „den unbedingt nötigen Raum für eigenverantwortliche Gestaltung des persönlichen Lebens jedes einzelnen und seiner Familie“ schafft; das Recht auf Eigentum müsse gleichsam „als eine Art Verlängerung der menschlichen Freiheit“ betrachtet werden.

Der englische Experte für Römisches Recht und Rechtsphilosoph Tony Honoré betrachtet in seiner einflussreichen Arbeit 1961 Eigentum nicht mehr als einzelnes Recht, sondern als ein Bündel von elf Rechten, wie folgt:

Für John Rawls ist das Recht auf Eigentum in seiner "Theorie der Gerechtigkeit" eine der Grundfreiheiten, die gemäß dem ersten und obersten seiner beiden Prinzipien jedem Menschen uneingeschränkt zustehen, soweit durch diese Freiheiten nicht die Freiheiten anderer eingeschränkt werden. Dies sagt noch nichts über die Verteilung von Eigentum aus. Soziale und ökonomische Ungleichheiten sind nach dem zweiten Prinzip nur soweit zulässig, soweit die am wenigsten Begünstigten einer Gesellschaft hieraus Vorteile ziehen. Aus dem zweiten Prinzip folgt, dass eine Umverteilung dann gerechtfertigt ist, wenn sie den am wenigsten Begünstigten einen Vorteil bringt. In einer offenen Marktwirtschaft kann dies bedeuten, dass von einer Umverteilung insofern abzusehen ist, wenn dadurch Wachstum und damit der allgemeine Wohlstand beeinträchtigt werden. In jedem Fall ist durch die Verteilung das Existenzminimum sicherzustellen.

Neben dem Eigentumsrecht, das sich nur auf körperliche Gegenstände beziehen kann, gewinnen seit der Industrialisierung die Rechte an geistigen Schöpfungen an Bedeutung („geistiges Eigentum“). Dies betrifft in der Gegenwart über die Frage des Urheberrechts hinaus das Eigentum an natürlichen Prozessen in der Gentechnik oder an immateriellen Gütern wie Software.

Die Eigentumsordnung einer Gesellschaft als Teil der Wirtschaftsordnung regelt die Verfügungsrechte über wirtschaftliche Güter. Neben der direkten Bestimmung des Eigentums im Privatrecht zählen zur Eigentumsordnung die Einstufung des Eigentums als Grundrecht in der Verfassung ("Schutz, Garantie" oder "Unverletzlichkeit des Eigentums") und eine Vielzahl von Regelungen im Öffentlichen Recht (Bodenrecht, Waldrecht, Nachbarschaftsrecht, Gemeindeordnungen u.a.m.), durch die der Gebrauch des Eigentums begrenzt wird. Erst das Zusammenspiel dieser gesetzlichen Bestimmungen spiegelt den materiellen Gehalt einer Eigentumsordnung wider. In der Theorie der Verfügungsrechte wird dabei zwischen Recht auf Nutzung, Veräußerung, Veränderung und Vermietung eines Gutes unterschieden.

Die Gesamtheit des Eigentums einer Person (oder einer Gruppe, eines Unternehmens, einer Volkswirtschaft etc.) bezeichnet man auch als deren „Vermögen“. In dem ursprünglichen Sinn des Wortes ist festgehalten, dass Eigentum Macht verleiht, etwa indem jemand andere Menschen dafür bezahlt, dass sie für ihn arbeiten.

Neben dem Privateigentum, bei dem eine bestimmte Sache einem bestimmten Individuum gehört, gibt es in entwickelten Gesellschaften auch gemeinschaftliches Eigentum (zwei oder mehr Individuen sind gemeinsame Eigentümer z. B. einer Zufahrt zu ihrem Grundstück), kommunales Eigentum (z. B. ein Wald gehört einer Stadt) und staatliches Eigentum (z. B. der Festlandssockel vor den Meeresküsten gehört dem betreffenden Land). Auch Organisationen wie Behörden, Gesellschaften oder Vereine können Eigentümer sein, z. B. von Grundstücken oder Gebäuden.

Eigentumsordnungen lassen sich danach unterscheiden, welche Arten von Gütern privates Eigentum sein dürfen und welche nicht:


Außerdem ergeben sich wesentliche Unterschiede durch die unterschiedlich gestalteten Eingriffsrechte der politischen Instanzen (Besteuerung des Eigentums und dessen Vererbung, Regelung von Enteignung und der entsprechenden Entschädigung, Sozialpflichtigkeit des Eigentums).

Mit der Eigentumsordnung ist ein Großteil der möglichen sozialen Konflikte geregelt: Ohne abgegrenztes Eigentum gibt es bei allen Gütern, die nicht im Überfluss vorhanden sind, entweder Streit oder es bedarf einer allgemein anerkannten Regelung, wer wann welches Gut benutzen oder verbrauchen darf.

Durch die Abgrenzung von Eigentumssphären und deren Zuordnung zu bestimmten Personen wird die soziale Entscheidungsfindung erheblich vereinfacht. Wenn alle über alles entscheiden, ist der Informations- und Entscheidungsprozess extrem aufwendig und kostet weit mehr Zeit, als wenn jeder nur über das Seine entscheidet.

Gemäß der Theorie der Verfügungsrechte ist der Vorzug des Privateigentums die Erzeugung einer starken Motivation des Eigentümers zu schonendem und sparsamem Gebrauch von Gütern und zur Schaffung neuer Güter. Kollektiveigentum hingegen führe zu unwirtschaftlichem Verhalten. Dennoch gab es gerade in der Landwirtschaft traditionell kollektives Eigentum. Im vorrevolutionären Frankreich etwa gab es unterschiedliche Formen gemeinschaftlichen Eigentums. Die Teilhaber am kollektiven Grundeigentum wurden von Mirabeau 1769 erstmals als „communistes“ benannt, er sah darin unter anderem soziale Vorteile. Außerdem gab es vor und nach der Revolution von 1789 unter freien Bauern familiale Gütergemeinschaften, die „communauté taisible“.

Es komme zur Tragik der Allmende, dem Phänomen, dass Menschen weniger leisten, wenn sie kollektiv tätig sind, da sie weder die Folgen ihrer Handlungen in vollem Umfang tragen müssen noch den individuellen Einsatz in vollem Umfang zugerechnet bekommen.

Durch die Eigentumsordnung entstehen aber auch ganz neue Probleme.


Neben den Problemen, die sich aus einer ungleichen Einkommensverteilung ergeben, gibt es Probleme, die sich durch die Institution des Privateigentums allein nicht regeln lassen:



In vielen traditionell geprägten Kulturen findet sich eine Zwischenform zwischen Individualeigentum und zentralisiertem Staatseigentum, die sogenannte Allmende. Gemeint ist damit das kollektive Eigentum einer Gemeinschaft, etwa eines Dorfes, an gemeinsam nach bestimmten Regeln genutzten Ressourcen. Nachdem diese Form der Bewirtschaftung von natürlichen Ressourcen aus Perspektive der Tragik der Allmende lange Zeit als ungeeignet angesehen wurde, hat sich in den letzten Jahrzehnten die Bewertung geändert.

Eine Sonderform des Kollektiveigentums ist das „gesellschaftliche Eigentum“, eine Eigentumskonzeption des ehemaligen Jugoslawien. Diese Konzeption entstammt der sozialistischen Ideologie insofern, als es eine Abkehr vom marktwirtschaftlichen Eigentumsverständnis bedeutet. Es ist aber nicht mit dem vermeintlich kommunistischen Staats- oder Volkseigentum gleichzusetzen, bei dem der Staat der Rechtsträger ist und welches nach jugoslawischer Anschauung genau wie das Privateigentum zur Ausbeutung und Entfremdung der Arbeiter durch die Monopolisierung der wirtschaftlichen und politischen Macht führt.

In der jugoslawischen Verfassung von 1974 wird das gesellschaftliche Eigentum negativ definiert. "Niemand", weder eine Gebietskörperschaft, noch eine Organisation der vereinten Arbeit oder der einzelne Arbeiter ist Träger der Eigentumsrechte an den gesellschaftlichen Produktionsmitteln. Demnach erlangt niemand Eigentumstitel über das Produkt der gesellschaftlichen Arbeit oder kann über die gesellschaftlichen Produktivkräfte verfügen oder ihre Verteilung bestimmen.

Die Konkretisierung der Definition und die Interpretation des gesellschaftlichen Eigentums blieb seit seiner Einführung 1953 kontrovers und rechtlich umstritten. Den Kern des Meinungsstreits bildet die Frage, ob es sich beim gesellschaftlichen Eigentum um eine rechtliche oder rein sozioökonomische Kategorie handelt, sowie die Frage nach dem Träger des Eigentumsrechts, so dieses bejaht wird.

Ausgehend vom privatkapitalistischen bzw. marktwirtschaftlichen Verständnis wird auch vertreten, dass das gesellschaftliche Eigentum eher eine ordnungspolitische Kategorie als eine Rechtsform oder Kategorie des Eigentums ist. Beim gesellschaftlichen Eigentum fehlt weitgehend die Zuordnung der Herrschaft über eine Sache zu einer juristischen oder natürlichen Person wie in anderen Eigentumsverfassungen. Dennoch entstanden selbst aus dem gesellschaftlichen Eigentum gewisse Individualrechte und es lässt sich in diesem Sinne wohl von einer Eigentumskategorie sprechen, wenngleich sie eben keine Entsprechung in marktwirtschaftlichen Ordnungen findet.

Dementsprechend ist das gesellschaftliche Eigentum als ein Eigentumssurrogat oder eigentumsähnliches Nutzungsrecht einzustufen. Gleichwohl ist zu berücksichtigen, dass in dieser sozialistischen Eigentumsordnung Privateigentum nach marktwirtschaftlichen Vorstellungen nebenher weiter existierte.
Die Frage nach der rechtlichen Einordnung des gesellschaftlichen Eigentums gewann an Aktualität nach dem Auseinanderbrechen Jugoslawiens und bei dem Versuch der Klärung der Eigentumsverhältnisse Privater sowie bei der Unternehmensprivatisierung. In Bosnien und Herzegowina wurde zur Regelung der offenen Eigentumsansprüche Privater die "Commission for Real Property Claims" (CRPC) und im Kosovo das Wohn- und Eigentumsdirektorat ("Housing and Property Directorate / Claims Commission" – HPD/CC) errichtet.





</doc>
<doc id="1237" url="https://de.wikipedia.org/wiki?curid=1237" title="Evolution (Systemtheorie)">
Evolution (Systemtheorie)

Evolution (vom lateinischen "evolvere" = abwickeln, entwickeln; PPP "evolutum") ist in der Systemtheorie ein Prozess, bei dem durch Reproduktion oder Replikation von einem System Kopien hergestellt werden, die sich voneinander und von ihrem Ursprungssystem durch Variation unterscheiden und bei dem nur ein Teil dieser Kopien auf Grund von Selektion für einen weiteren Kopiervorgang zugelassen werden.

Die Evolution ist an drei notwendige Voraussetzungen gebunden:


Diese Voraussetzungen sind hinreichend trivial, so dass man logisch ableiten kann, dass sie an vielen Orten und Gelegenheiten im Universum gegeben sind. Die Ansichten darüber, ob sich Leben daraus entwickeln "muss", gehen jedoch weit auseinander. Weitgehend Einigkeit hingegen besteht in der Evolutionsbiologie darüber, dass die biologische Evolution nicht zwangsläufig zur Entwicklung von bewusster Intelligenz führt, denn diese ist nur ein Spezialfall, der an weitere, vermutlich sehr selten gegebene Bedingungen geknüpft ist. Der einzige Fall, von dem sicher bekannt ist, dass dies dort eintrat, ist unsere Erde. Aber auch hier wurden die Bedingungen der Evolution von Intelligenz erst nach mindestens 530 Millionen Jahren erfüllt, obwohl die fortschreitende Evolution von Vielzellern schon zuvor eine Reihe notwendiger Voraussetzungen für Intelligenzentwicklung bereitstellte.

Als Evolution bezeichnet man heute allgemein jenen statistischen Vorgang, bei dem die Zusammensetzung einer Replikatoren-Population P2 aus einer Stichprobe einer zuvor bestehenden, anderen Replikatoren-Population P1 bestimmt wird. Wird aus P1 eine Stichprobe unterschiedlicher Replikatoren gezogen und aus ihr die Zusammensetzung von P2 bestimmt, so liegt Evolution vor. Läuft dieser Vorgang wiederholt ab, so weisen spätere Populationen – wie beispielsweise P5 oder P100 – jeweils schwankende Zusammensetzungen auf. Die Evolution kann auch als "kumulierender Stichprobenfehler" bezeichnet werden.

Eine evolutionsfähige Population ist eine Menge von Replikatoren. Letztere sind irgendwelche Objekte, von denen Kopien entstehen.

Die Evolution als statistischer Vorgang ist ein logisch und empirisch jederzeit beweisbares Faktum und in der Wissenschaft "nicht bestreitbar". Evolution läuft niemals an Objekten, sondern immer nur an Häufigkeiten von Objekten ab. Er kann grundsätzlich an allen Mengen ablaufen, die nicht einmal den bekannten physikalischen Gesetzen gehorchen müssen.

Evolution im hier definierten Sinn findet auf der Erde im Reich der Lebewesen statt. Der Begriff „Evolution“ wird außerhalb der Biologie teilweise anders definiert, für Vorgänge, die nach anderen Gesetzmäßigkeiten als „Replikation –> Variation –> Selektion“ verlaufen. Dies betrifft beispielsweise die Entstehung und Entwicklung von Galaxien, Sternen und Planeten inklusive der Erde; in den Gesellschaftswissenschaften unter anderem die soziokulturelle Entwicklung des Menschen und in der Systemtheorie die Entwicklung von Computerprogrammen. Die Gemeinsamkeit aller Vorgänge beruht auf einer geschichtlichen Entwicklung und häufig einer Entwicklung in Richtung höherer Komplexität. Aus biologischer Sicht kann diese synonyme Begriffsverwendung leicht zu Missverständnissen führen und ist insofern misslich.

Dieses Thema beschäftigt sich mit dem Ursprung und der Entwicklung des Universums, dessen Teilchen und Elementen. Folgende Artikel befassen sich mit der Thematik:

Die Evolution der Lebewesen ist ihre Entwicklung im Laufe großer Zeitspannen innerhalb der Erdgeschichte.

Siehe dazu:


Unter bestimmten Bedingungen führt die Evolution zu Organismen, die über ein Bewusstsein verfügen. Dieser Entwicklungsprozess ist Gegenstand der Evolutionären Psychologie.

In der Philosophie über lebende Systeme betrachtet man die wissenschaftliche Entwicklung als eine Fortsetzung der biologischen Evolution und spricht von einer Evolution des Geistes:

Lebewesen seien Träger genetisch gespeicherter Informationen. In der Evolution sammle sich mehr und genauere Information in den Lebewesen an. Der Mensch sei als einziges Lebewesen in der Lage, seine geistigen, im Gehirn gespeicherten Informationen auch außerhalb des Körpers zu speichern, zum Beispiel in Büchern oder auf Disketten. Diese Informationen, unter anderem die wissenschaftlichen Ideen (als „geistige Gene“ betrachtet), könnten an alle Menschen und die Nachwelt „vererbt“ werden. Die Mittel der Evolution, nämlich Vermehrung mit Varianten und deren Selektion, setzten sich fort als wissenschaftliche Hypothesenbildungen und deren Prüfung im Versuch.

Aufgrund zahlreicher empirischer Belege glaubt man heute einheitlich, dass die Evolution auf unserem Planeten nicht immer an denselben Replikatoren abgelaufen sein muss. Die Welt der Lebewesen, wie wir sie heute kennen, basierte zwar auf weiten Strecken auf einem chemischen Replikator, der DNA, sie ist jedoch nicht der einzige Replikator. Als weitere Replikatoren erwiesen sich beispielsweise Kristallstrukturen, die ebenfalls Kopien von sich selbst herstellen können. Auch informationstragende Einheiten, die nicht an eine chemische, sondern an eine (bio-)informatische Grundlage gebunden sind, werden als Replikatoren begriffen und wurden von Richard Dawkins 1976 als Meme bezeichnet.

In Form der evolutorischen Ökonomik haben Gedanken der biologischen Evolution auch Eingang in die Wirtschaftswissenschaften gefunden. Hintergrund ist, dass durch freie Märkte eine Selektion unter konkurrierenden Produkten oder Produktionsverfahren stattfindet, in der sich erwünschtere Produkte und effizientere Verfahren gegen weniger gewünschte und ineffizientere durchsetzen. Ständige Produktinnovationen führen so zu einer ständigen Weiterentwicklung, die – wie in der biologischen Evolution – Untersuchungsgegenstand ist. Während in der Biologie aber die Variationen oder Mutationen nur als zufällig modelliert werden, sind sie in der evolutorischen Ökonomik ebenfalls Untersuchungsgegenstand.

Bei Kettenbriefen, die als E-Mail verbreitet werden, entfällt die Kopierungenauigkeit. Es gibt für diese Art der Kettenbriefe noch keine Untersuchungen darüber, ob Empfänger den Text bewusst ändern, um ihrer Version eine größere Verbreitung zu ermöglichen.





</doc>
<doc id="1239" url="https://de.wikipedia.org/wiki?curid=1239" title="Esoterik">
Esoterik

Esoterik (von "esōterikós" ‚innerlich‘, dem inneren Bereich zugehörig‘) ist in der ursprünglichen Bedeutung des Begriffs eine philosophische Lehre, die nur für einen begrenzten „inneren“ Personenkreis zugänglich ist, im Gegensatz zu Exoterik als allgemein zugänglichem Wissen. Andere traditionelle Wortbedeutungen beziehen sich auf einen inneren, spirituellen Erkenntnisweg, etwa synonym mit Mystik, oder auf ein „höheres“, „absolutes“ Wissen.

Heute gibt es weder im wissenschaftlichen noch im populären Sprachgebrauch eine allgemein anerkannte Definition von "Esoterik" beziehungsweise "esoterisch".

In der Wissenschaft haben sich zwei grundlegend verschiedene Verwendungen dieser Bezeichnungen etabliert:

Im populären Sprachgebrauch versteht man unter Esoterik vielfach „Geheimlehren“. Ebenfalls sehr gebräuchlich ist der Bezug auf „höhere“ Erkenntnis und auf Wege, welche zu dieser führen sollen. Des Weiteren wird das Adjektiv „esoterisch“ häufig abwertend im Sinne von „unverständlich“ oder „versponnen“ verwendet.

Das altgriechische Adjektiv "esoterikos" ist erstmals im 2. Jahrhundert bezeugt: bei Galen, der bestimmte stoische Lehren so bezeichnete, und in einer Satire des Lukian von Samosata, wo sich „esoterisch“ und „exoterisch“ auf zwei Aspekte der Lehren des Aristoteles beziehen (von innen oder von außen betrachtet). Weit älter ist der Gegenbegriff „exoterisch“: Schon Aristoteles (384–322 v. Chr.) nennt seine propädeutischen, für Fachfremde und Anfänger geeigneten Kurse „exoterisch“ (nach außen hin gerichtet) und grenzt sie so vom streng wissenschaftlichen philosophischen Unterricht ab. Erst Cicero (106–43 v. Chr.) bezieht den Begriff „exoterisch“ auf eine bestimmte Gattung von Schriften des Aristoteles und der Peripatetiker, nämlich die „volkstümlich geschriebenen“, für die Öffentlichkeit bestimmten Werke (literarische Dialoge) im Gegensatz zu den nur für internen Gebrauch in der Schule geeigneten Fachschriften; die letzteren nennt er aber nicht „esoterisch“. Im Sinne dieser von Cicero getroffenen, nicht auf Aristoteles selbst zurückgehenden Einteilung des Schrifttums werden noch heute in der Altertumswissenschaft die „exoterischen“ von den „esoterischen“ Schriften des Aristoteles unterschieden. Die „esoterischen“ Schriften enthalten keine Geheimlehren, sondern nur Darlegungen, deren Verständnis philosophische Vorbildung voraussetzt. Schon Aristoteles’ Lehrer Platon war der Überzeugung, ein Teil seiner Lehren sei nicht zur Veröffentlichung geeignet (ungeschriebene Lehre). Daher ist in der modernen Forschungsliteratur von Platons „Esoterik“ oder „esoterischer Philosophie“ die Rede, womit die ungeschriebene Lehre gemeint ist.

Im Sinne von „geheim“ benutzte den Begriff "esoterikos" erstmals der Kirchenvater Clemens von Alexandria. In einem ähnlichen Sinn unterschieden Hippolyt von Rom und Iamblichos von Chalkis zwischen exoterischen und esoterischen Schülern des Pythagoras, wobei letztere einen "inneren Kreis" bildeten und bestimmte Lehren exklusiv empfingen. Ins Lateinische wurde das griechische Wort erst in der Spätantike übernommen; der einzige antike Beleg für das lateinische Adjektiv "esotericus" ist eine Stelle in einem Brief des Kirchenvaters Augustinus, der an Ciceros Angaben anknüpfend mit Bezug auf Aristoteles von „esoterischer Philosophie“ schrieb.

Den Ausgangspunkt für die Entstehung des neuzeitlichen Esoterik-Begriffs bildete die auf die Pythagoreer bezogene Begriffsverwendung des Iamblichos. Man dachte dabei an die von Iamblichos überlieferte, in der modernen Forschung umstrittene Einteilung der Pythagoreer in die zwei rivalisierenden Gruppen der „Akusmatiker“ und der „Mathematiker“, die beide den Anspruch erhoben haben sollen, die authentische Lehre des Pythagoras zu vertreten. Ob es eine Geheimlehre der frühen Pythagoreer tatsächlich gegeben hat, ist in der Forschung umstritten, doch war die Vorstellung davon in der Frühen Neuzeit allgemein verbreitet und prägte den Begriff „esoterisch“. Man bezeichnete mit diesem Wort ein Geheimwissen, das ein Lehrer nur ausgewählten Schülern mitteilt.

Im Englischen kommt das Wort erstmals in der 1655–1662 erschienenen "History of Philosophy" von Thomas Stanley vor. Stanley schrieb, den inneren Kreis der Pythagoreer hätten die "Esotericks" gebildet. Im Französischen ist "ésotérique" erstmals 1752 im Dictionnaire de Trévoux bezeugt, 1755 auch in der Encyclopédie. Im Deutschen ist „esoterisch“ als Fremdwort, wohl aus dem Französischen oder Englischen übernommen, erstmals 1772 belegt; das Adjektiv wird ab dem späten 18. Jahrhundert zur Bezeichnung von Lehren und Kenntnissen verwendet, die nur für einen ausgesuchten Kreis Eingeweihter oder Würdiger bestimmt sind, sowie zur Charakterisierung von wissenschaftlichen und philosophischen Texten, die nur für einen kleinen, exklusiven Kreis von Fachleuten verständlich sind. Seit dem 20. Jahrhundert ist eine abwertende Konnotation verbreitet; „esoterisch“ hat oft die Bedeutung „unverständlich“, „geheimnistuerisch“, „weltfremd“, „versponnen“. (Siehe hierzu auch esoterische Programmiersprachen.) Das Substantiv „Esoteriker“ ist ab dem frühen 19. Jahrhundert gebräuchlich (erster Beleg 1813); anfangs bezeichnete es eine Person, die in die Geheimnisse einer Gesellschaft oder in die Regeln einer Kunst oder Wissenschaft eingeweiht ist.

Der Gebrauch des Substantivs „Esoterik“ (französisch "ésotérisme") beginnt 1828 in einem Buch von Jacques Matter über die antike Gnosis. Nachdem auch andere Autoren diesen Neologismus aufgegriffen hatten, wurde er 1852 erstmals in einem französischen Universallexikon als Bezeichnung für Geheimlehren aufgeführt. Weithin gebräuchlich wurde das Wort dann durch die einflussreichen Bücher von Éliphas Lévi über Magie, von wo aus es in das Vokabular des Okkultismus Eingang fand. Seither wurde es (wie auch das Adjektiv) von vielen Autoren und Strömungen als Selbstbezeichnung verwendet, wobei sie es oft in freier Weise neu definierten.

Heute wird „Esoterik“ weithin als Bezeichnung für „Geheimlehren“ verstanden, wobei es sich laut Antoine Faivre "de facto" allerdings zumeist um allgemein zugängliche „offene Geheimnisse“ handelt, die sich einer entsprechenden Erkenntnisbemühung erschließen. Nach einer anderen, ebenfalls sehr geläufigen Bedeutung bezieht sich das Wort auf eine "höhere" Stufe der Erkenntnis, auf „wesentliches“, „eigentliches“ oder „absolutes“ Wissen und auf die sehr vielfältigen Wege, welche zu diesem führen sollen.

In der Wissenschaft haben sich zwei grundlegend verschiedene Verwendungen der Bezeichnung Esoterik oder esoterisch etabliert:



Erste Zeugnisse von Lehren und Sozialstrukturen, die aus heutiger Sicht der Esoterik zugerechnet werden können, finden sich schon recht früh im antiken Griechenland und im damals griechisch besiedelten Süditalien, wobei Pythagoras (*um 570, † nach 510 v. Chr.) als Gründer der religiös-philosophischen Schule und Bruderschaft der Pythagoreer in Kroton (heute Crotone) besonders herausragt. Pythagoras glaubte – ebenso wie die Orphiker und Anhänger verschiedener Mysterienkulte – an die Unsterblichkeit der Seele. Damit verbanden die Pythagoreer und die Orphiker die Vorstellung der Seelenwanderung (Reinkarnation). Sie betrachteten den Körper als eine vorübergehende Behausung der Seele, ja als einen Kerker, aus dem sie sich befreien müsse. Diese Erlösung von der körperlichen Existenz strebten sie durch ein sittlich einwandfreies Leben an, das zunächst zu einer Wiedergeburt auf höherer Stufe führen sollte, schließlich aber zur endgültigen Befreiung von der Körperwelt durch Beendigung der Reihe der Wiedergeburten. Diese Vorstellungen standen in scharfem Kontrast zu der älteren, von Homer repräsentierten Anschauung, in dessen Ilias der Begriff der Seele ("psyche") zwar erstmals nachweisbar auftaucht, aber nur als Attribut der ganz mit dem Körper identifizierten Person. Zu den Anhängern des Reinkarnationsgedankens gehörten später auch andere bedeutende Philosophen wie Empedokles und Platon sowie alle antiken Platoniker.

Ein weiteres zentrales Motiv der Esoterik, das bei den Pythagoreern erstmals auftrat, ist die Erhebung der "Zahlen" zu den Prinzipien alles Seienden. Sie betrachteten die Welt als eine nach ganzzahligen Verhältnissen harmonisch geordnete Einheit ("Kosmos"), und den Weg der Läuterung der Seele sahen sie in der Unterwerfung unter die allgemeine, mathematisch ausdrückbare Harmonie aller Dinge. Auch die Idee der musikalisch begründeten Sphärenharmonie, basierend auf einem Vergleich der Planetenbewegungen mit den von den Pythagoreern entdeckten Zahlenverhältnissen der musikalischen Intervalle, hat hier ihren Ursprung. Sogar ein moralischer Aspekt wurde den Zahlen zugesprochen, indem man bestimmten Zahlen sittliche Qualitäten wie Gerechtigkeit oder Zwietracht zuordnete.
Platon (427–347 v. Chr.) war der Erste, der die Unsterblichkeit der Seele argumentativ zu beweisen versuchte (in seinem Dialog "Phaidon"). Dabei identifizierte er die Seele mit der Vernunft, die er als prinzipiell vom Körper unabhängig betrachtete. Ihre eigentliche Heimat sei das Reich der unvergänglichen Ideen und der reinen Geister, welcher sie entstamme und in welche sie nach dem Tod zurückkehre. Wie schon bei den Pythagoreern erscheint auch hier der Körper als Gefängnis, dem die Seele in der Reihe der Wiedergeburten durch eine reine Lebensführung entrinnen und in ein rein geistiges Dasein übergehen kann. Unverkörpert kann sie demnach die ewigen Wesenheiten, denen sie selbst angehört, unmittelbar schauen, während dieses Wissen im Körper verdunkelt ist und gewöhnlich nur im Zuge der in sich selbst begründeten Tätigkeit der Vernunft wie eine Erinnerung auftaucht. Neben den Lebewesen schrieb Platon auch den Gestirnen sowie dem Kosmos als ganzem eigene Seelen und damit Leben zu.

Esoterisch war Platons Philosophie auch in dem Sinne, dass sie auf einen "inneren" Weg verwies. Das Eigentliche seiner Lehre sei, so Platon, gar nicht mitteilbar, sondern nur der "eigenen" Erfahrung zugänglich. Er könne als Lehrer nur Hinweise geben, aufgrund derer wenige Auserwählte in der Lage sein würden, sich selbst dieses insofern esoterische Wissen zu erschließen, das in solchen Fällen plötzlich als Idee in der Seele entspringe und sich dann selbst weiter seine Bahn breche.

Das Motiv eines inneren Kreises von „Eingeweihten“ (Grundmann) oder Auserwählten, teils verbunden mit der Aufforderung zur Geheimhaltung (Arkandisziplin), tritt auch in den frühchristlichen Schriften, die später als Evangelien in das Neue Testament aufgenommen wurden, des Öfteren auf, wobei allerdings nicht durchgängig ein bestimmter Menschenkreis gemeint ist. Insofern kann von neutestamentlichen Ansätzen einer christlichen Esoterik gesprochen werden, wie der Esoterikforscher Gerhard Wehr es tut. Diesen von Jesus persönlich Auserwählten steht der Apostel Paulus gegenüber, der Jesus nie persönlich begegnet war und dessen Anhänger sogar vehement bekämpfte, aber durch eine "innere" Offenbarung („Damaskuserlebnis“) zum Christentum bekehrt und schließlich zu dessen erfolgreichstem Missionar wurde. Hier spricht Wehr von „paulinischer Esoterik“ im Sinne des inneren Weges. Paulus erhob den Anspruch, das „Pneuma“ (Geist) Gottes empfangen zu haben und daher das Wesen und den Willen Gottes zu kennen, denn der Geist ergründe (anders als die menschliche Weisheit) alles, „auch die Tiefen Gottes“. Eine Sonderstellung unter den Schriften des Neuen Testaments nehmen noch das Johannes-Evangelium und die Offenbarung des Johannes ein, die etwa der Philosoph Leopold Ziegler als „ein durchaus esoterisches Schrifttum“ bezeichnete. Diese Sonderstellung wurde auch im frühen Christentum schon zum Ausdruck gebracht, indem man das Johannes-Evangelium als das „geistige“ oder „pneumatische“ Evangelium von den anderen unterschied (Clemens von Alexandria, Origenes).

An Platons Seelenlehre schloss in nachchristlicher Zeit der Neuplatonismus an, dessen herausragender Vertreter der in Rom wirkende Plotin (205–270 n. Chr.) war und der als die bedeutendste philosophische Richtung der ausgehenden Antike gilt. Plotin zog die äußerste Konsequenz aus Platons Ansatz, indem er den ekstatischen Aufschwung zum „Einen“, wie er das Göttliche nannte, das Gewahrwerden des Urgrundes aller Dinge in uns selbst, als das „wahrhafte Endziel für die Seele“ bezeichnete. „An seinem höchsten Punkt erweist sich Plotins Denken als Mystik“, wie der Philosoph Wolfgang Röd schreibt, und der Esoterikforscher Kocku von Stuckrad sieht hier den „archimedischen Punkt europäischer Seeleninterpretation“ und den „Dreh- und Angelpunkt auch heutiger esoterischer Anschauungen“ wie etwa der New-Age-Bewegung. Noch stärker als bei Plotin trat dieses mystische Element, verbunden mit magischen Praktiken, bei späteren Neuplatonikern wie Iamblichos (etwa 275–330 n. Chr.) und Proklos (5. Jh.) hervor. Diese Philosophen folgten dem in jener Zeit überhaupt sehr verbreiteten Interesse an mystischer Religiosität, Magie und Wahrsagung. Röd spricht in diesem Zusammenhang von einer Verwandlung der neuplatonischen Philosophie „zu einer Art Theosophie und Theurgie“.

Eine andere in der hellenistischen Antike gestiftete Tradition, die für die Esoterik eine große Bedeutung erlangen sollte, ist die Hermetik, die sich auf Offenbarungen des Gottes Hermes beruft und eine Synthese griechischer Philosophie mit ägyptischer Mythologie und Magie darstellt. Hier trat das bis dahin im griechisch-römischen Denken wenig geläufige Motiv des Mittlers in den Vordergrund, der – ob als Gott oder als „aufgestiegener“ Mensch – höheres Wissen offenbart. Ein weiteres Grundmotiv der Hermetik wie auch der späteren Esoterik allgemein ist die Vorstellung einer alles verbindenden "Sympathie", welche die astrologischen Entsprechungen zwischen Makrokosmos und Mikrokosmos begründen sollte. Später trat das neuplatonische Konzept des Aufstiegs der unsterblichen Seele durch die Planetensphären und der damit verbundenen Erlösung bis hin zum Einswerden mit Gott hinzu, ermöglicht durch Erkenntnis und durch die Erfüllung bestimmter ethischer Anforderungen.

Eine besondere Ausprägung erfuhr der Gedanke der Erlösung der Seele durch höhere Erkenntnis in diversen religiösen Strömungen der Spätantike, die zusammenfassend als Gnosis bezeichnet werden. Diese vielfältige Bewegung entstand im ersten nachchristlichen Jahrhundert im Osten des Römischen Reiches und in Ägypten. Sie trat in heidnischen, jüdischen und christlichen Spielarten auf. In ihr verbanden sich Elemente der griechischen Philosophie mit religiösen Vorstellungen. Grundlegend war dabei zumeist ein schroffer Dualismus, d. h. eine scharfe Trennung zwischen der geistigen Welt, der die menschliche Seele entstammt, und der im Grunde nichtigen materiellen Welt, an die sie vorübergehend gebunden ist, – auch verstanden als Gegensatz von Licht und Finsternis oder von Gut und Böse. Die heiligen religiösen Schriften wurden unter diesem Gesichtspunkt als verschlüsselte Botschaften betrachtet, die von „Pneumatikern“, denen das höhere Wissen über die geistige Wirklichkeit zugänglich war, verfasst worden seien und die auch nur von Pneumatikern wirklich verstanden werden könnten. Speziell in der christlichen Gnosis trat noch die Erlösergestalt des Christus und die damit verbundene Vorstellung eines entscheidenden Wendepunktes der Weltgeschichte hinzu.

Die Gnosis stieß auf zunehmenden Widerspruch sowohl von philosophischer Seite (besonders Plotin) wie auch vonseiten der sich etablierenden und institutionell festigenden christlichen Großkirche, wobei eine scharfe Trennung zwischen der im Entstehen begriffenen kirchlichen Theologie und den heterogenen Spielarten der christlichen Gnosis allerdings kaum möglich war und ist. So standen die einflussreichen Theologen Clemens und Origenes der Gnosis nahe, indem auch sie eine höhere, „geistige“ Erkenntnis propagierten und für sich in Anspruch nahmen, und Origenes wurde von seinem späteren Gegner Epiphanius von Salamis gar als „Oberhaupt der Ketzer“ bezeichnet. Problematisch ist zudem, dass die Bezeichnungen „Gnosis“ und „Gnostizismus“ im Wesentlichen von den kirchlichen Gegnern geprägt wurden, während die so Bezeichneten sich selbst zumeist einfach „Christen“ oder gar „orthodoxe“ Christen nannten. Eine wesentliche Differenz zwischen den kirchlichen Kritikern und den von diesen so genannten Gnostikern bestand darin, dass letztere die eigene Erkenntnis (griech. "gnosis") des Einzelnen betonten und eine „Selbstermächtigung des erkennenden Subjekts“ (Stuckrad) propagierten, während die Kirche großen Wert auf die Begrenztheit des menschlichen Erkenntnisvermögens legte und die höchsten Wahrheiten nur in der göttlichen Offenbarung gegeben sah, die – unter Berufung auf die Amtsnachfolge (apostolische Sukzession) – allein in den von ihr anerkannten (kanonisierten) Schriften sowie in den von ihr vorgegebenen festen Bekenntnisformeln zu finden sei. Im Konkreten entzündeten sich die Auseinandersetzungen besonders an Fragen der Astrologie und der Magie. Ab dem 4. nachchristlichen Jahrhundert hatte sich die Macht der Kirche so weit gefestigt, dass bereits geringfügige Abweichungen vom „rechten Glauben“ mit dem Tod durch Feuer oder Schwert geahndet werden konnten. Die Zeugnisse der Ansichten dieser „Häretiker“ wurden vernichtet und gingen fast restlos verloren, so dass man sich bis weit ins 20. Jahrhundert hinein weitgehend auf die nicht gerade unparteiischen Schilderungen erklärter Gegner wie Irenäus von Lyon stützen musste. Erst 1945 wurde in Nag Hammadi, Ägypten, eine Sammlung gnostischer Texte entdeckt, die den „Säuberungen“ entgangen war und erstmals einen umfassenden und unverfälschten Einblick in dieses nach eigener Einschätzung wahre oder orthodoxe Christentum erlaubte.

Im Mittelalter gerieten große Teile dieser antiken Lehren im christlichen Kulturraum in Vergessenheit, während sie im islamischen Raum bewahrt und vielfach aufgegriffen wurden und teils auch in die jüdische Mystik einflossen. Insbesondere solche Lehren, die eine individuelle Erlösung implizierten oder sich auf religiöse Urkunden beriefen, welche keinen Eingang in den biblischen Kanon gefunden hatten, wurden aus dem orthodoxen Christentum ausgegrenzt. Daneben bestanden allerdings im Mittelmeerraum pagane („heidnische“) Religionen fort, und im Nahen Osten blieben vor allem der Manichäismus, der Zoroastrismus und der Islam neben dem orthodoxen Christentum bestehen.

Auf der anderen Seite boten innerhalb des letzteren die neu entstehenden Klöster – insbesondere die des 529 gegründeten Benediktiner-Ordens – Raum für die Pflege kontemplativer Mystik, die sich nun auch nach Norden ausbreitete. Eine große Bedeutung für die mittelalterliche Mystik erlangten einige im 5. und 6. Jahrhundert aufgetauchte Schriften, als deren Autor Dionysios Areopagita genannt wurde, ein Zeitgenosse des Paulus, den dieser in der Apostelgeschichte erwähnt hatte. Dieser Dionysios vertrat eine stark platonisch geprägte „negative“ Theologie, in welcher er zum Ausdruck brachte, dass Gott aller herkömmlichen Erkenntnis unzugänglich sei. Erst der vollkommene Verzicht auf alles „Wissen“ im herkömmlichen Sinn ermögliche die „Einung“ mit Gott und damit eine Erkenntnis, die alles Wissbare sprenge. Daneben war Dionysios der Erste, der eine strukturierte Hierarchie der Engel, d. h. der zwischen Gott und dem Menschen vermittelnden geistigen Wesen, vorlegte. Erst etwa tausend Jahre später kamen ernsthafte Zweifel auf, ob der Autor dieser Schriften wirklich der von Paulus Erwähnte sein konnte, und heute gilt als erwiesen, dass sie frühestens gegen Ende des 5. Jh. entstanden sein konnten. Der Autor wird daher heute meist als Pseudo-Dionysius Areopagita bezeichnet.

Ab dem 8. Jahrhundert konnten sich in Südspanien unter der Herrschaft der in religiösen Dingen sehr toleranten Mauren in friedlicher Koexistenz allerlei Spielarten islamischer, jüdischer und christlicher Spiritualität entfalten, unter denen hier vor allem der islamische Sufismus zu nennen ist. Auch Platon und andere griechische Philosophen wurden von hier aus im westlichen Europa näher bekannt.

Die herausragende Gestalt der frühmittelalterlichen Mystik und zugleich der bedeutendste Philosoph seiner Epoche war der im 9. Jahrhundert lebende Johannes Scotus Eriugena, der von Kaiser Karl dem Kahlen an die Pariser Hofschule berufen wurde. Seine Lehre war stark von Dionysios Areopagita und dem Neuplatonismus beeinflusst, und er legte die ersten brauchbaren Übersetzungen der Werke des Dionysios ins Lateinische vor, wodurch diese auch im Westen ihre Wirkung entfalten konnten. Ein zentrales Thema seiner Lehre war die Rückkehr des Menschen zu Gott, die „Gottwerdung“ (lat. "deificatio" oder griech. "théosis") durch Erhöhung des Bewusstseins, also ganz im Sinne des Neuplatonismus. Freilich wurden seine Ansichten schon zu seinen Lebzeiten von lokalen Synoden verurteilt, und im 13. Jahrhundert wurden auf Geheiß des Papstes alle greifbaren Exemplare seines Hauptwerks vernichtet.

Eine mit der frühchristlichen Gnosis vergleichbare Bewegung sind die Katharer, über die ab der Mitte des 12. Jahrhunderts Berichte vorliegen, deren Ursprung aber weitgehend im Dunkeln liegt. Der Schwerpunkt dieser sich schnell ausbreitenden spirituellen Bewegung lag in Südfrankreich und Norditalien. Sie wich in wesentlichen Punkten von der römisch-katholischen Lehre ab und wurde daher bald massiv bekämpft. Das Katherertum knüpfte vor allem an die Spiritualität des Johannes-Evangeliums an, während es große Teile des Alten Testaments ablehnte. Des Weiteren betrachtete es Christus nicht als Menschen, sondern als einen vom Himmel gesandten Erlöser. Die Erlösung sahen die Katharer darin, dass die Menschenseele aus der als finster betrachteten materiellen Welt in ihre Lichtheimat zurückkehren würde. Die Gemeinschaft der Katharer war streng hierarchisch geordnet; nur der kleine Kreis der in strenger Askese lebenden „Vollendeten“ wurde in ihre Geheimlehre eingeweiht. Als vor allem in weiten Teilen Südfrankreichs sehr beliebte „Gegenkirche“ entwickelte sie sich zur bedeutendsten Konkurrenz der römischen Kirche im Mittelalter, bis diese zu einem regelrechten Kreuzzug aufrief, in dessen Folge das Katharertum vollständig vernichtet wurde.

Als neue mystische Geheimlehre trat im 12. Jahrhundert in Südfrankreich und Spanien die jüdische Kabbala auf, die zunächst im Judentum eine große Bedeutung erlangte, später aber auch außerhalb desselben in der Geschichte der Esoterik eine bedeutende Rolle spielen sollte. Ursprünglich auf die Deutung der Heiligen Schrift (Tora) beschränkt, entwickelte die Kabbala bald auch eine eigenständige theologische Lehre (siehe Sephiroth), die mit magischen Elementen (Theurgie) verbunden war. Manche Kabbalisten (am prominentesten Abraham Abulafia) vertraten (wie die christlichen Gnostiker) die Ansicht, dass man nicht nur durch Interpretation der Tora, sondern auch durch direkte mystische Erfahrung zu „absolutem“ Wissen gelangen könne.
Bis ins 13. Jahrhundert finden sich auch innerhalb des offiziellen Christentums noch wesentliche Teile dessen, was man später als Esoterik bezeichnen würde, darunter kosmologische Lehren, das Denken in Entsprechungen, die Imagination und die Idee der spirituellen Transformation. Beispiele dafür sind in Deutschland die Mystikerin Hildegard von Bingen, in Frankreich die platonisch ausgerichtete Schule von Chartres (Bernardus Silvestris, Guillaume de Conches, Alanus ab Insulis), in Italien der Visionär Joachim von Fiore und die Franziskaner, in Spanien die neuplatonisch geprägte, der Kabbala nahestehende Lehre des Mallorquiners Ramon Llull und in England die Schule von Oxford (Theosophie des Lichts bei Robert Grosseteste, Alchemie und Astrologie bei Roger Bacon). Um 1300 setzte sich jedoch in der Theologie der Averroismus durch, der den Rationalismus betont und Imaginatives ablehnt.

Speziell die Mystik erfuhr allerdings in der ersten Hälfte des 14. Jahrhunderts einen bemerkenswerten Aufschwung und eine Popularisierung, indem ihre Vertreter zum Gebrauch der jeweiligen Volkssprache anstelle des Lateinischen übergingen. Am bedeutendsten waren hier die deutschen Dominikaner Meister Eckhart, Johannes Tauler und Heinrich Seuse; Vergleichbares gab es jedoch auch in den Niederlanden, in England, Frankreich, Italien und Spanien. Bei aller Vielfalt des von diesen Mystikern geschilderten inneren Erlebens und der von ihnen verwendeten Begriffe war ihnen das Ziel der "Unio mystica", der mystischen Vereinigung oder Kommunion des Menschen mit Gott, gemeinsam, die „Gottesgeburt im Seelengrund“. In Eckharts mystischem Denken erreichte die mittelalterliche Mystik einen Höhepunkt; zugleich bildet es aber den Ausgangspunkt für eine neue Richtung der Mystik, die bis in die frühe Neuzeit hinein wirken sollte. Bei ihm stand „Mystik“ nicht für ekstatische Verzückung, sondern für eine besondere Denkweise, die über das Argumentieren und Schlussfolgern hinausgeht und zu einem unmittelbaren Erfassen des Absoluten, ja zum Einswerden mit diesem führt. Damit knüpfte Eckhart an Johannes Scotus Eriugena, Pseudo-Dionysios Areopagita und den Neuplatonismus an. Da er sich vielfach der deutschen Sprache bediente, wurde er zum wirkungsmächtigsten Vertreter dieser platonischen Richtung innerhalb der christlichen Theologie, obwohl Teile seiner Lehre posthum als Häresie verurteilt wurden und auch seine allgemeinverständliche Verbreitung schwieriger theologischer Erörterungen auf Kritik stieß.

Esoterische Praktiken wie die Magie und die Astrologie waren im Mittelalter verbreitet. Zur Magie gehörte auch die Beschwörung (Invokation) von Dämonen und Engeln, wobei die Existenz von Dämonen als gefallener Engel auch in der Theologie anerkannt war. Die Alchemie erlangte erst im 12. Jahrhundert eine gewisse Bedeutung, ausgehend von arabisch-muslimischen Quellen in Spanien.

In der Renaissance, in der man sich auf die Antike zurückbesann, erlebte auch die Esoterik einen Aufschwung. Maßgeblich dafür waren die Wiederentdeckung bedeutender hermetischer Schriften ("Corpus Hermeticum"), die Erfindung des Buchdrucks mit beweglichen metallenen Lettern, durch den sich ein viel breiteres Publikum erschloss, und auch die Auswirkungen der Reformation. Antoine Faivre, der Altmeister der Esoterikforschung, sieht im 16. Jahrhundert sogar den eigentlichen "„Ausgangspunkt dessen, was man später als Esoterik bezeichnen sollte“", und betrachtet daher vergleichbare Erscheinungen in der Antike und im Mittelalter lediglich als Vorläufer der Esoterik: "„als sich die Naturwissenschaften von der Theologie ablösten und man begann, sie um ihrer selbst willen zu betreiben […], da konnte sich die Esoterik als eigener Bereich konstituieren, der in der Renaissance zunehmend die Schnittstelle zwischen Metaphysik und Kosmologie einnahm“".

Das "Corpus Hermeticum", eine Sammlung von Schriften, die dem nach neuerer Kenntnis fiktiven Autor Hermes Trismegistos zugeschrieben wurden, wurde 1463 in Mazedonien entdeckt und gelangte in den Besitz des Mäzens Cosimo de’ Medici in Florenz. Diese Texte schienen sehr alt zu sein, sogar älter als die Schriften Moses und damit die gesamte jüdisch-christliche Überlieferung, und eine Art „Urwissen“ der Menschheit zu repräsentieren. Cosimo gab deshalb sofort eine Übersetzung ins Lateinische in Auftrag, die 1471 erschien und großes Aufsehen erregte. Das Corpus wurde als „ewige Philosophie“ ("Philosophia perennis") betrachtet, die der ägyptischen, griechischen, jüdischen und christlichen Religion als gemeinsamer Nenner zugrunde liege. Dank des Buchdrucks erlangte es weite Verbreitung, und bis 1641 kamen 25 Neuauflagen heraus; auch wurde es in verschiedene andere Sprachen übersetzt. Im 16. Jahrhundert kamen jedoch Zweifel an der korrekten Datierung dieser Texte auf, und 1614 konnte der Genfer Protestant Isaac Casaubon nachweisen, dass sie erst in nachchristlicher Zeit entstanden sein konnten. Da hatten sie ihre enorme Wirkung jedoch längst entfaltet.
Der Übersetzer des "Corpus Hermeticum", Marsilio Ficino (1433–1499), übertrug auch die Werke Platons und etlicher Neuplatoniker ins Lateinische und verfasste eigene Kommentare und Einführungen in die platonische Philosophie. Die Neuplatoniker wurden dadurch nach langer Vergessenheit überhaupt erst wieder bekannt, und Platon wurde im Wortlaut verfügbar. Auch das hatte enorme Auswirkungen. Platonisches Gedankengut wurde gegen die aristotelisch geprägte Theologie in Stellung gebracht. Ein Aspekt dieser Kontroversen betraf die Frage, wie weit menschliche Erkenntnis reichen kann, womit ein wesentlicher Konflikt aus Zeiten des frühen Christentums wieder auflebte (vgl. oben). Manche Neuplatoniker der Renaissance vertraten sogar pantheistische Positionen, was aus Sicht des monotheistischen Christentums an Häresie grenzte.

Ein dritter wichtiger Einfluss auf die Esoterik der Renaissance ging von der Kabbala aus, indem deren Methoden zur Deutung der religiösen Urkunden auch von Christen übernommen wurden. Die bedeutendsten Vertreter dieser „christlichen Kabbala“ waren Giovanni Pico della Mirandola (1463–1494), Johannes Reuchlin (1455–1522) und Guillaume Postel (1510–1581). Im Zentrum der christlich-kabbalistischen Hermeneutik stand der Versuch, auch auf der Grundlage der originär jüdischen Überlieferung die Wahrheit der christlichen Botschaft (Christus ist der Messias) zu beweisen. Das war teils mit anti-jüdischer Polemik verbunden (die Juden würden ihre eigenen heiligen Schriften nicht richtig verstehen), rief aber auf der anderen Seite die Inquisition auf den Plan, was 1520 in der Verurteilung Reuchlins durch den Papst kulminierte.

In Deutschland entwickelte der Kölner Philosoph und Theologe Heinrich Cornelius Agrippa von Nettesheim (1486–1535) aus Elementen der Hermetik, des Neuplatonismus und der Kabbala eine „okkulte Philosophie“ ("De occulta philosophia", 1531). Darin unterschied er drei Welten: die elementare, die himmlische und die göttliche Sphäre, denen beim Menschen Körper, Seele und Geist entsprechen. Die antike Lehre von den vier Elementen (Erde, Wasser, Luft und Feuer) ergänzte er durch eine „fünfte Essenz“, womit er den Begriff der Quintessenz prägte. Größte Bedeutung maß Agrippa der Magie bei, die er als höchste Wissenschaft und erhabenste Philosophie auffasste. Nicht durch Wissenschaft im herkömmlichen Sinn, die er scharf verurteilte, sondern nur durch den „guten Willen“ könne der Mensch sich in mystischer Ekstase dem Göttlichen annähern.
Die neuplatonische Dreiteilung von Mensch und Welt und die Entsprechung von Mikrokosmos (Mensch) und Makrokosmos liegen auch der medizinischen Lehre des Paracelsus (1493–1541) zugrunde. Neben den vier Elementen maß er besonders den drei Prinzipien der Alchemie (Sal, Sulfur und Mercurius) eine große Bedeutung bei. Der Quintessenz Agrippas entspricht bei ihm der "Archaeus", eine organisierende und formbildende Kraft. Für Paracelsus gehörte auch die Astrologie notwendig zur Medizin hinzu, denn der Mensch trage den ganzen Kosmos in sich, Diagnose und Therapie setzten genaue Kenntnisse der astrologischen Entsprechungen voraus, und die Beurteilung des Krankheitsverlaufs und der Wirkung von Medikamenten müsse unter Berücksichtigung der Planetenbewegungen erfolgen.

Zu den bedeutenden Esoterikern der frühen Neuzeit gehört auch Giordano Bruno (1548–1600). Er schrieb mehrere Bücher über Magie, die er als mit der empirischen Naturwissenschaft vereinbar ansah ("Magia naturalis"), und vertrat die Lehre von der Seelenwanderung. Mit der im Geiste Brunos sich vom kirchlichen Dogmatismus befreienden Naturwissenschaft schienen esoterische Anschauungen dagegen vielfach kompatibel zu sein. So waren die astronomischen „Revolutionäre“ Nikolaus Kopernikus (1473–1543), Galileo Galilei (1564–1642) und Johannes Kepler (1571–1630) überzeugte Anhänger der Astrologie, Kepler und Galilei praktizierten diese sogar, und Isaac Newton (1643–1727), der neben Galilei als Begründer der exakten Naturwissenschaft gilt, verfasste daneben auch Beiträge über Hermetik, Alchemie und Astrologie.
An die Forderung Martin Luthers, neben der Bibel nur auf einen individuellen Zugang zu Gott zu vertrauen, knüpfte im 16. und 17. Jahrhundert die „klassische“ christliche Theosophie an. Deren wichtigster Vertreter war Jakob Böhme (1575–1624), ein Schuster, der im Alter von 25 Jahren nach einer schweren Lebenskrise eine mystische Vision hatte und später darüber schrieb. Nach Böhme ist der Ausgangspunkt allen Seins der „Zorn Gottes“, den er jedoch nicht wie das Alte Testament als eine Reaktion auf menschliche Verfehlungen, sondern als ein willenshaftes Urprinzip beschreibt, das vor der Schöpfung, ja „vor der Zeit“ besteht. Dem Zorn steht die Liebe gegenüber, die als Sohn Gottes oder auch als dessen Wiedergeburt angesehen wird. Diesen „wiedergeborenen Gott“ kann der Mensch nur erkennen, wenn er selbst „wiedergeboren“ wird, indem er mit Gott kämpft und durch einen Gnadenakt von diesem Kampf erlöst und mit absolutem Wissen beschenkt wird. Diese theosophische Lehre Böhmes wurde als häretisch eingestuft, und nachdem ein erstes, nicht zur Veröffentlichung bestimmtes Manuskript in die Hände eines Pfarrers gelangt war, wurde der Autor zeitweilig inhaftiert und schließlich mit einem Publikationsverbot belegt. Jahre später (ab 1619) widersetzte er sich jedoch diesem Verbot, und seine Schriften trugen in hohem Maß zur Ausbildung eines spirituellen Bewusstseins auf der Grundlage des Protestantismus bei.
In den Jahren 1614 bis 1616 erschienen einige mysteriöse Schriften, die großes Aufsehen erregten. Ihre anonymen Autoren beriefen sich auf die mythische Gestalt des Christian Rosencreutz, der von 1378 bis 1484 gelebt haben soll und dessen Hinterlassenschaft sie in seinem Grab entdeckt hätten. Die von diesen ersten Rosenkreuzern propagierte Lehre ist eine Synthese verschiedener esoterischer und naturphilosophischer Traditionen mit der Idee einer „Generalreformation“ der ganzen Welt. Ihre Publikation löste eine Flut von zustimmenden und ablehnenden Kommentaren aus; schon 1620 waren über 200 diesbezügliche Schriften erschienen. Der angeblich dahinter stehende geheime Orden bestand nach heutigem Kenntnisstand jedoch wahrscheinlich nur aus wenigen Personen an der Tübinger Universität, darunter Johann Valentin Andreae (1586–1654).

Einen wichtigen Wendepunkt in der Rezeption esoterischer Lehren markiert die 1699/1700 publizierte "Unparteyische Kirchen- und Ketzer-Historie" von Gottfried Arnold, in der erstmals ein Überblick über „alternative“ Anschauungen innerhalb des Christentums gegeben wurde, ohne diese als Irrlehren zu verdammen. Der Protestant Arnold rehabilitierte insbesondere die Gnosis, indem er sie als Suche nach „ursprünglicher Religiosität“ beschrieb.

Im 18. Jahrhundert entwickelte sich eine im Vergleich zu den „Klassikern“ wie Jakob Böhme weniger visionäre, dafür stärker intellektuell geprägte Theosophie. Deren wichtigster Vertreter, Friedrich Christoph Oetinger (1702–1782), war zugleich auch ein bedeutender Propagator der lurianischen Kabbala im deutschen Sprachraum. Durch die erste deutsche Übersetzung im Jahre 1706 wurde das "Corpus Hermeticum" breiter bekannt und zum Gegenstand wissenschaftlicher Darstellungen. Populär waren Themen wie Vampirismus und Hexerei, und Gestalten wie der Graf von Saint Germain oder Alessandro Cagliostro hatten Konjunktur. Daneben etablierte sich eine institutionalisierte Esoterik in Form von Geheimen Bruderschaften, Orden und Logen (vor allem die Rosenkreuzer und Teile der Freimaurerei).
Eine Sonderstellung im Bereich der Theosophie nimmt der renommierte schwedische Naturwissenschaftler und Erfinder Emanuel Swedenborg (1688–1772) ein, der ähnlich wie Böhme aufgrund von Visionen, die er 1744/45 hatte, zum Mystiker und Theosophen wurde. Nach Swedenborgs Überzeugung leben wir mit unserem Unbewussten in einer jenseitigen geistigen Welt, in welcher wir bewusst „erwachen“, wenn wir sterben. Als Autor etlicher umfangreicher Werke avancierte er bald zu einem der einflussreichsten, aber auch umstrittensten Mystiker im Zeitalter der Aufklärung. Anhänger seiner Lehre gründeten die bis heute bestehende Glaubensgemeinschaft „Neue Kirche“, unter den Theosophen seiner Zeit blieb er jedoch ein wenig geschätzter Außenseiter, und sein bedeutendster Kritiker war kein Geringerer als Immanuel Kant (1724–1804), der ihm 1766 die Streitschrift "Träume eines Geistersehers" widmete.

In der Aufklärung, zu deren wichtigstem Denker Kant durch seine späteren Hauptwerke avancieren würde, war der Esoterik neben den etablierten Kirchen eine weitere mächtige Gegnerschaft erwachsen. Aufgrund seines Verständnisses von Vernunft und Wissen musste Kant, obwohl er in jungen Jahren selbst der Seelenwanderungslehre angehangen hatte, Lehren wie diejenige Swedenborgs ablehnen, und darin folgte ihm bald die große Mehrheit der Gelehrten. Zwar könne man, so Kant, nicht "beweisen", dass Swedenborgs Behauptungen über die Existenz von Geistern und dergleichen falsch seien, ebenso wenig aber das Gegenteil, und wenn man auch nur eine einzige Geistererzählung als wahr anerkennen würde, würde man damit das gesamte Selbstverständnis der Naturwissenschaften in Frage stellen.
Dass Aufklärung und Esoterik nicht notwendigerweise im Gegensatz zueinander stehen müssen, zeigen hingegen die Freimaurer, bei denen ein aktives Eintreten für die rationale Aufklärung und ein verbreitetes Interesse für Esoterik nebeneinander bestanden und „Aufklärung“ vielfach mit einem Streben nach „höherem“ Wissen gleichgesetzt wurde, verbunden mit dem esoterischen Motiv der Transformation des Individuums. Esoterisch ausgerichteten Orden gehörten im 18. Jahrhundert viele bedeutende Personen an, darunter der preußische Kronprinz und spätere König Friedrich Wilhelm II., dessen Orden allerdings nur bis zu seiner Krönung bestand, weil er damit aus der Sicht der Ordensleitung seinen Zweck erfüllt hatte. Obwohl auch einige andere Adlige bedeutende Freimaurer waren, spielte das Freimaurertum insgesamt aber eher eine Rolle bei der Stärkung des sich emanzipierenden Bürgertums gegenüber dem absolutistischen Staat.

In die Naturphilosophie und Kunst der deutschen Romantik floss in erheblichem Maß esoterisches Gedankengut ein. So war Franz von Baader (1765–1841) zugleich ein bedeutender Naturphilosoph und der herausragende Theosoph dieser Epoche. In letzterer Hinsicht knüpfte er stark an Böhme an, allerdings in einer äußerst spekulativen Weise. In der romantischen Dichtung tritt der esoterische Einfluss besonders deutlich bei Novalis (1772–1801) hervor, aber auch etwa bei Johann Wolfgang von Goethe (1749–1832), Justinus Kerner (1786–1862) und etlichen anderen bedeutenden Dichtern. Novalis fasste die Natur als ein großes lebendiges Ganzes auf, mit der der Mensch im Zuge einer Initiation erkennend verschmelzen kann. Dabei griff er auch alchemistische und freimaurerische Symbole auf. In der Musik ist vor allem Mozarts in einem freimaurerischen Umfeld entstandene Oper "Die Zauberflöte" zu nennen, in der Malerei Philipp Otto Runge.

Mit der Begründung der modernen Chemie im späten 18. Jahrhundert (vor allem durch die Schriften Lavoisiers 1787/1789) war der Niedergang der „operativen“ Alchemie eingeleitet, was deren Popularität allerdings zunächst wenig beeinträchtigte, und daneben bestand eine „spirituelle“ Alchemie als eine spezielle Form der Gnosis weiter. Auch Elektrizität und Magnetismus waren in dieser Zeit geläufige Themen esoterischer Diskurse, wobei sich besonders der schwäbische Arzt Franz Anton Mesmer (1734–1815) mit seiner Theorie des „animalischen Magnetismus“ hervortat. Mesmer verband die alte alchemistische Vorstellung eines alles durchströmenden unsichtbaren Fluidums mit dem modernen Begriff des Magnetismus und mit der Behauptung, damit Krankheiten heilen zu können. Nachdem er sich 1778 in Paris niedergelassen hatte, eroberten die von ihm entwickelten „magnetischen“ Heilgeräte vor allem die dortige Kaffeehaus-Szene. Seine „Therapiemethode“, zu mehreren um ein solches Gerät herumzusitzen, dabei in Trance und Ekstase zu geraten und den „Magnetismus“ daran beteiligter gesunder Personen in sich einströmen zu lassen, kann als ein Vorläufer der späteren spiritistischen Séancen gelten.
Ende des 18. Jahrhunderts tauchte die neue Praktik auf, zumeist weibliche Personen in einen „magnetischen Schlaf“ zu versetzen und dann über die übersinnliche Welt zu "befragen". Im deutschen Sprachraum befasste sich der schon genannte Justinus Kerner damit. Eine Abwandlung dieser Praktik ist der Spiritismus, dessen Ursprung 1848 bei zwei Schwestern in den USA liegt, der aber schnell auch auf Europa übergriff und Millionen Anhänger fand. Auch hierbei dient eine Person als „Medium“, und diesem werden Fragen gestellt, welche sich an die Geister von Verstorbenen wenden. Die Geister sollen antworten, indem sie den Tisch, an dem die Sitzung stattfindet, in Bewegung versetzen. In Verbindung mit dem Reinkarnationsgedanken entwickelte sich daraus eine regelrechte Religion.

Als Begründer des Okkultismus im eigentlichen Sinn in der zweiten Hälfte des 19. Jahrhunderts gilt Éliphas Lévi (1810–1875). Obwohl seine Werke nur „wenig geschickte Kompilationen“ (Faivre) waren, war er zeitweilig der bedeutendste Esoteriker überhaupt. Einflussreich war auch das umfangreiche okkultistische Werk von Papus (1865–1916); im deutschen Sprachraum ist vor allem Franz Hartmann (1838–1912) zu nennen. Dieser Okkultismus war eine Gegenströmung gegen die vorherrschende Wissenschaftsgläubigkeit und gegen die Entzauberung der Welt durch den Materialismus. Er verstand sich selbst jedoch als modern (Faivre nennt ihn eine Antwort der Moderne auf sich selbst) und lehnte im Allgemeinen den wissenschaftlichen Fortschritt nicht ab, sondern versuchte, diesen in eine umfassendere Vision zu integrieren.

Ein Kennzeichen der Moderne, welche durch die Aufklärer des 18. Jahrhunderts, aber auch durch den Neukantianismus des 19. Jahrhunderts geprägt wurde, ist die zunehmende Trennung von materiellen und sakral-transzendenten Bereichen der Wirklichkeit. Einerseits werden Natur und Kosmos zunehmend rational begriffen und somit „entzaubert“, andererseits wird dem Transzendenten eine außerweltliche Ebene zugewiesen. Daraus kann ebenso die Gegenreaktion erwachsen, die Natur, den Kosmos und die materielle Wirklichkeit erneut sakralisieren zu wollen und somit die Trennung zwischen weltlicher und außerweltlicher Sphäre wieder aufzuheben. Da der Glaube an die Berechenbarkeit aller Dinge und die prinzipielle Ergründbarkeit des Kosmos selbst das Ergebnis einer religionsgeschichtlichen Entwicklung sind, nämlich der schon in der alttestamentlichen Schöpfungsvorstellung angelegten Entseelung des Kosmos, kann gerade die moderne Abwendung von dieser religiösen Tradition auch eine Abwendung von dem Glauben an die rationale Wissenschaft nach sich ziehen. Dies kann zu der Überzeugung führen, dass weder Religion, wie beispielsweise das Christentum, noch Wissenschaft über die „wahre“ Weltdeutungshoheit verfügen, sondern dass eine Erklärung der Welt nur mit wissenschaftlicher und spiritueller Deutung möglich sei. Verschiedene Gruppierungen, die der modernen Esoterik zugeordnet werden können, vertraten genau diese Überzeugung.
In einem engeren Sinn wird vielfach das Jahr 1875 als Geburtsjahr der modernen westlichen Esoterik angesehen, markiert durch die Gründung der Theosophischen Gesellschaft (TG) in New York. Initiator und dann auch Präsident dieser Gesellschaft war Henry Steel Olcott (1832–1907), ein renommierter Anwalt, der sich schon lange für esoterische Themen interessiert hatte und den Freimaurern nahestand. Zur wichtigsten Person wurde jedoch schnell Olcotts Lebensgefährtin Helena Petrovna Blavatsky (1831–1891). HPB, wie sie später zumeist genannt wurde, war deutsch-ukrainischer Herkunft und hatte lange Jahre auf Reisen in weiten Teilen der Welt verbracht. Schon seit ihrer Kindheit stand sie in medialer Verbindung zu spirituellen „Meistern“ in Indien, von denen sie nun (laut einem Notizbucheintrag "vor" der Gründung der TG) die „Weisung“ erhalten hatte, eine philosophisch-religiöse Gesellschaft unter der Leitung Olcotts zu gründen. Auch Olcott berief sich auf Anweisungen von „Meistern“, die er allerdings in Form von Briefen erhalten habe. Die Ziele der TG wurden folgendermaßen formuliert: Erstens sollte sie den Kern einer universalen Bruderschaft der Menschheit bilden, zweitens eine vergleichende Synthese von Religionswissenschaft, Philosophie und Naturwissenschaft anregen und drittens ungeklärte Naturgesetze und im Menschen verborgene Kräfte erforschen. Die Bezeichnung „theosophisch“ wurde dabei anscheinend kurzfristig einem Lexikon entnommen.

Kurz nach der Gründung der TG machte sich Blavatsky an die Abfassung ihres ersten Bestsellers "Die entschleierte Isis" ("Isis Unveiled"), der 1877 herauskam und dessen erste Auflage bereits nach zehn Tagen vergriffen war. In dieser und in anderen Schriften – das Hauptwerk "Die Geheimlehre" ("The Secret Doctrine") erschien 1888 – bündelte HPB die esoterischen Traditionslinien der Neuzeit und gab ihnen eine neue Form. Von großer Bedeutung war dabei die Verbindung mit östlichen spirituellen Lehren, an denen zwar schon seit der Romantik ein recht reges Interesse bestanden hatte, die nun aber als das reinste „Urweistum“ der Menschheit in den Vordergrund rückten, was die Esoterik des 20. Jahrhunderts entscheidend prägen sollte. Blavatsky selbst gab einerseits an, ihr Wissen zu erheblichen Teilen der beinahe täglichen „Präsenz“ eines „Meisters“ zu verdanken (was man hundert Jahre später einmal „Channeling“ nennen würde). Im Vorwort der "Geheimlehre" hingegen behauptete sie, lediglich ein uraltes und bisher geheim gehaltenes östliches Dokument (das "Buch des Dzyan") zu übersetzen und zu kommentieren. Schon nach dem Erscheinen von "Isis Unveiled" begannen Kritiker jedoch nachzuweisen, dass der Inhalt dieses Buches fast vollständig auch schon in anderer zeitgenössischer Literatur zu finden war, wobei die meisten der betreffenden Bücher für HPB unmittelbar in Olcotts Bibliothek verfügbar waren. Der enormen Wirkung ihres Werks tat das jedoch keinen Abbruch.
Im Umfeld der Theosophischen Gesellschaft entstand gegen Ende des 19. Jahrhunderts eine ganze Reihe neuer initiatischer Gemeinschaften und magischer Orden, überwiegend in freimaurerischer und rosenkreuzerischer Tradition, darunter der Hermetic Order of the Golden Dawn (1888). Dieser Orden war von der christlichen Kabbala und dem Tarot inspiriert, befasste sich mit ägyptischen und anderen antiken Gottheiten und räumte einer zeremoniellen Magie einen erheblichen Raum ein. Für letztere stand vor allem Aleister Crowley (1875–1947), der später dem in Wien gegründeten Ordo Templi Orientis (1901) beitrat und diesem eine sexualmagische und antichristliche Ausrichtung gab. Crowley gilt als der bedeutendste Magier des 20. Jahrhunderts.

In Deutschland gründete Franz Hartmann 1886 eine deutsche Abteilung der Theosophischen Gesellschaft und 1888 einen Rosenkreuzer-Orden. Viel bedeutender war hier aber Rudolf Steiner (1861–1925), der 1902 Generalsekretär der neu gegründeten deutschen Sektion der TG wurde. Steiner übernahm zumindest in den ersten Jahren vieles von Blavatsky, war selbst aber stark von den naturwissenschaftlichen Werken Goethes und deutschen Philosophen wie Max Stirner und Friedrich Nietzsche beeinflusst und entwickelte schließlich eine eigene, christlich-abendländische, an der Mystik anknüpfende Lehre, die er später „Anthroposophie“ nannte, nachdem es zum Bruch mit der durch Annie Besant vertretenen internationalen Theosophischen Gesellschaft gekommen war. Als Vertreter einer christlichen Theosophie in Deutschland in der Tradition von Jakob Böhme und Franz von Baader ist im 20. Jahrhundert außerdem Leopold Ziegler (1881–1958) zu nennen.

Der populärste Zweig der Esoterik im 20. Jahrhundert war zweifellos die Astrologie. Sie bedient das Bedürfnis, mit Hilfe des Prinzips der Entsprechung die verlorengegangene Einheit von Mensch und Universum wiederherzustellen. Dies kann neben der praktischen Anwendung auch einen „gnostischen“ Aspekt haben, indem man „Zeichen“ zu deuten versucht und eine ganzheitliche Sprache entwickelt. Eine ähnliche Dualität von Praxis und Gnosis liegt auch beim Tarot vor sowie bei der Unterscheidung von zeremonieller und initiatischer Magie.
Einen herausragenden Einfluss auf die Entwicklung der populären Esoterik in den letzten Jahrzehnten („New Age“) hatte Carl Gustav Jung (1875–1961). Jung postulierte die Existenz universeller seelischer Symbole, die er „Archetypen“ nannte und durch eine Analyse der Religionsgeschichte und insbesondere auch der Geschichte der Alchemie und Astrologie zu identifizieren suchte. In dieser Sichtweise wurde die innere Transformation des Adepten zum zentralen Inhalt esoterischen Handelns, so u. a. in der „psychologischen“ Astrologie. Dem liegt ein Konzept der Seele zugrunde, wie es in ähnlicher Form schon bei den antiken Neuplatonikern und bei Renaissance-Denkern wie Marsilio Ficino und Giovanni Pico della Mirandola zu finden war. Im Kontrast zur traditionellen Psychologie, die an dem mechanistisch-naturwissenschaftlichen Ansatz der Medizin ausgerichtet ist und die Rede von einer Seele als ein Ergebnis metaphysischer, also unwissenschaftlicher Spekulation betrachtet, wird hier die Seele zum „wahren Kern“ der Persönlichkeit erhoben und geradezu sakralisiert, d. h. ihrem eigentlichen Wesen nach als göttlich angesehen. Der Mensch strebt nach Vollkommenheit, indem er sich in seine eigene Göttlichkeit versenkt, welche im Unterschied zu manchen östlichen Lehren dem "Individuum" zugeschrieben wird.

Jungs aus der Theorie der Archetypen entwickeltes Konzept des kollektiven Unbewussten gehört auch zu den Ursprüngen der transpersonalen Psychologie, welche annimmt, dass es Ebenen der Wirklichkeit gibt, auf denen die Grenzen der gewöhnlichen Persönlichkeit überschritten werden können und eine gemeinsame Teilhabe an einer allumfassenden Symbolwelt möglich ist. Solche Vorstellungen verbanden sich in der von Amerika ausgehenden Hippie-Bewegung mit einem großen Interesse an östlichen Meditationstechniken und an psychoaktiven Drogen. Die wichtigsten Theoretiker dieser transpersonalen Bewegung sind Stanislav Grof und Ken Wilber. Grof experimentierte mit LSD und versuchte dabei, eine Systematik der auftretenden „transpersonalen“ Bewusstseinszustände zu entwickeln.
Für die Kommunikation mit transzendenten Wesen in einem veränderten Bewusstseinszustand (etwa in Trance) etablierte sich in den 1970er Jahren die Bezeichnung „Channelling“. Sehr populär wurden in diesem Bereich die Prophezeiungen von Edgar Cayce (1877–1945). Weitere bedeutende Medien waren oder sind Jane Roberts (1929–1984), Helen Schucman (1909–1981) und Shirley MacLaine. Auch die Lehren von Theosophen wie Helena Petrovna Blavatsky und Alice Bailey sind hierher zu rechnen, und Vergleichbares findet sich im Neo-Schamanismus, in der modernen Hexenbewegung und im Neopaganismus. Allen gemeinsam ist die Überzeugung von der Existenz anderer Welten und von der Möglichkeit, aus diesen Informationen zu erhalten, die in der diesseitigen Welt nützlich sein können.

Ein weiteres zentrales Thema der heutigen Esoterik sind ganzheitliche Konzeptionen der Natur, wobei naturwissenschaftliche oder naturphilosophische Ansätze die Grundlage für eine spirituelle Praxis bilden. Ein Beispiel dafür ist die Tiefenökologie, eine biozentrische und radikal gegen den vorherrschenden Anthropozentrismus gerichtete Synthese ethischer, politischer, biologischer und spiritueller Positionen (Arne Næss, "deep ecology", 1973). Die Tiefenökologie betrachtet die gesamte Biosphäre als ein einziges, zusammenhängendes „Netz“, das nicht nur als solches "erkannt", sondern auch in einer spirituellen Dimension "erfahren" werden soll. Damit verwandt sind James Lovelocks Gaia-Hypothese, die den ganzen Planeten Erde als einen Organismus auffasst, und daran anknüpfende Konzepte von David Bohm, Ilya Prigogine, David Peat, Rupert Sheldrake und Fritjof Capra, die man als „New Age Science“ zusammenfassen kann.
Im Bereich der Freimaurerei und des Rosenkreuzertums wurden im 20. Jahrhundert zahlreiche initiatische Gesellschaften neu gegründet. Eine besonders breite Wirkung entfaltete der 1915 gegründete Rosenkreuzer-Orden AMORC. Im deutschen Sprachraum ist die Anthroposophische Gesellschaft mit ihrem Zentrum in Dornach bei Basel am bedeutendsten, was durch den Erfolg der auf anthroposophischer Grundlage arbeitenden Waldorfschulen noch verstärkt wird. Die Theosophische Gesellschaft zerfiel nach Blavatskys Tod in mehrere Gruppierungen, welche heute in diversen Ländern sehr aktiv sind.

Wie schon in der Romantik, lassen sich auch in der Moderne vielfach esoterische Einflüsse in Kunst und Literatur aufzeigen. Das gilt etwa für die Architektur Rudolf Steiners (Goetheanum), für die Musik Alexander Skrjabins, die Gedichte Andrej Belyis, die Dramen August Strindbergs und das literarische Werk Hermann Hesses, aber auch für Bereiche der neueren Science Fiction wie etwa die "Star-Wars"-Filmtrilogie von George Lucas. Ein bedeutender Einfluss auf die bildende Kunst ging auch von der Theosophischen Gesellschaft aus. Beispiele für künstlerische Gestaltung im Dienst der Esoterik sind manche Tarot-Blätter und die Illustrationen in manchen esoterischen Büchern. Nicht im eigentlichen Sinn esoterisch beeinflusst, aber beliebte Gegenstände esoterischer Interpretationen waren die Musik Richard Wagners und die Gemälde Arnold Böcklins.

Was heute als westliche Esoterik bezeichnet wird, wurde anscheinend erstmals gegen Ende des 17. Jahrhunderts als eigenständiges und zusammenhängendes Feld erkannt. 1690/1691 publizierte Ehregott Daniel Colberg seine polemische Schrift "Das platonisch-hermetische Christenthum", und 1699/1700 folgte Gottfried Arnolds "Unpartheyische Kirchen- und Ketzer-Historie", in welcher er bis dahin als häretisch eingestufte Spielarten des Christentums aus christlich-theosophischer Sicht verteidigte. Diesen theologisch ausgerichteten Arbeiten folgten philosophiehistorisch orientierte, zunächst Johann Jakob Bruckers "Historia critica Philosophiae" (1742–1744), in der verschiedene Strömungen behandelt wurden, welche heute der westlichen Esoterik zugerechnet werden, und schließlich "Die christliche Gnosis oder die christliche Religions-Philosophie in ihrer geschichtlichen Entwicklung" (1835) von Ferdinand Christian Baur, der eine direkte Linie von der antiken Gnosis über Jacob Böhme bis zum deutschen Idealismus zog.

Im weiteren Verlauf des 19. Jahrhunderts wurden derartige Themen weitgehend aus dem wissenschaftlichen Diskurs ausgegrenzt, indem man sie als Produkte irrationaler Schwärmerei betrachtete oder als vor-wissenschaftlich einordnete (die Alchemie als Proto-Chemie oder die Astrologie als Proto-Astronomie). Stattdessen schrieben nun Okkultisten wie Éliphas Lévi oder Helena Petrovna Blavatsky umfangreiche „Historien“ der Esoterik, in denen, wie Hanegraaff schreibt, ihre eigene Fantasie eine kritische Betrachtung historischer Tatbestände ersetzte, was erst recht dazu beitrug, dass ernsthafte Wissenschaftler dieses Themenfeld mieden. Erst 1891–1895 legte Carl Kiesewetter mit seiner "Geschichte des neueren Occultismus" wieder eine bedeutende akademische Studie vor, gefolgt von "Les sources occultes du Romantisme" von Auguste Viatte (1927) und Lynn Thorndikes achtbändiger "History of Magic and Experimental Science" (1923–1958). Eine umfassende Sicht westlicher Esoterik, die etwa der Perspektive heutiger Esoterikforschung entspricht, scheint als Erster Will-Erich Peuckert in seiner 1936 erschienenen "Pansophie – ein Versuch zur Geschichte der weißen und schwarzen Magie" entwickelt zu haben, die mit Marsilio Ficino und Giovanni Pico della Mirandola beginnt und über Paracelsus und die christliche Theosophie zum Rosenkreuzertum führt.

In ihrem aufsehenerregenden Buch "Giordano Bruno and the Hermetic Tradition" versuchte die Historikerin Frances A. Yates 1964 nachzuweisen, dass die Hermetik, wie sie von Pico della Mirandola, Giordano Bruno und John Dee vertreten wurde, bei der Begründung der neuzeitlichen Wissenschaft in der Renaissance eine wesentliche Rolle gespielt habe und dass diese Wissenschaft ohne den Einfluss der Hermetik gar nicht entstanden wäre. Obwohl das „Yates-Paradigma“ sich in dieser starken Form in akademischen Kreisen letztlich nicht etablieren konnte und Yates’ provozierende Thesen hauptsächlich in religiösem und der Esoterik nahestehendem Schrifttum auf Resonanz stießen, wird es wegen der Debatten, die es auslöste, als wichtige „Initialzündung“ für die moderne Esoterikforschung betrachtet.

Antoine Faivre stellte 1992 die These auf, dass man die Esoterik als eine "Denkform" (frz. "forme de pensée") betrachten könne, die im Gegensatz zu wissenschaftlichem, mystischem, theologischem oder utopischem Denken steht.

Faivre versteht Esoterik als bestimmte Art und Weise des Denkens:

Dieser Ansatz Faivres erwies sich als sehr fruchtbar für die vergleichende Forschung, wurde von vielen anderen Esoterikforschern übernommen und trat weitgehend an die Stelle des Yates-Paradigmas, stieß aber auch auf vielfältige Kritik. So wurde bemängelt, dass Faivre seine Charakterisierung hauptsächlich auf Untersuchungen des Hermetismus der Renaissance, der Naturphilosophie, der christlichen Kabbala und der protestantischen Theosophie stützte und damit den Begriff der Esoterik so eng fasse, dass er auf entsprechende Erscheinungen in der Antike, im Mittelalter und in der Moderne sowie außerhalb der christlichen Kultur (Judentum, Islam, Buddhismus) vielfach nicht mehr anwendbar sei. Zweifellos hat das Faivre-Paradigma jedoch entscheidend dazu beigetragen, dass die Esoterikforschung als Teil des ernsthaften Wissenschaftsbetriebs anerkannt wurde.

Ein erster spezieller Lehrstuhl für die „Geschichte der christlichen Esoterik“ wurde 1965 an der Sorbonne in Paris eingerichtet (1979 umbenannt in „Geschichte der esoterischen und mystischen Strömungen im neuzeitlichen und zeitgenössischen Europa“). Diesen Lehrstuhl hatte von 1979 bis 2002 Antoine Faivre inne, seit 2002 als Emeritus neben Jean-Pierre Brach.

Seit 1999 gibt es in Amsterdam einen Lehrstuhl für die „Geschichte der hermetischen Philosophie und verwandter Strömungen“ (Wouter J. Hanegraaff). Drittens wurde an der Universität von Exeter (England) ein Zentrum für Esoterikforschung eingerichtet (Nicholas Goodrick-Clarke, † 2012). Im Jahre 2006 richtete auch der Vatikan an der Päpstlichen Universität Angelicum in Rom einen „Lehrstuhl für nichtkonventionelle Religionen und Spiritualitätsformen“ (Michael Fuß) ein.

Die wichtigste deutschsprachige Fachzeitschrift ist "Gnostika".

Ab dem frühen 19. Jahrhundert hatten diverse esoterische Strömungen einen erheblichen Einfluss auf die intellektuelle Begründung der Demokratie und auf die Ausbildung eines Geschichtsbewusstseins. Dabei handelte es sich einerseits um eine romantische Rückbesinnung auf das Ursprüngliche in Ablehnung der Moderne, andererseits um eine progressive Erwartung des Eintretens vorhergesagter Ereignisse. Beispiele für letzteres sind Frühsozialisten wie Robert Owen, Pierre Leroux und Barthélemy Prosper Enfantin. Umgekehrt lässt sich zeigen, dass das Fortbestehen frühsozialistischer Ideen, insbesondere des Saint-Simonismus und des Fourierismus, nach 1848 essentiell sowohl für die Entstehung des Spiritismus als auch des Okkultismus gewesen ist.

Aleister Crowley neigte dem Stalinismus und dem italienischen Faschismus zu. Noch weiter ging Julius Evola, indem er sich auch dem Nationalsozialismus zuwendete. Während Stalin derartigen Erscheinungen gegenüber relativ tolerant war, wurden sie im NS-Deutschland schnell ausgeschaltet. Ein Esoteriker, der die Moderne radikal ablehnte und sich dem Islam zuwendete, war René Guénon. Teile des New Age griffen die Erwartungshaltung des früheren Okkultismus wieder auf.

Manche Praktiken, die heute der Esoterik zugerechnet werden, insbesondere Wahrsagen und die Magie, werden schon im Tanach, der Heiligen Schrift des Judentums, scharf verurteilt. Im frühen Christentum entzündeten sich dann darüber hinaus grundsätzliche interne Konflikte, die zur Ausgrenzung vieler sogenannter „gnostischer“ Gruppierungen aus der sich institutionell festigenden Kirche führten, weshalb deren abweichende Lehren und Erkenntnis-Ansprüche heute ebenfalls zur Esoterik zählen (vgl. Kapitel „Geschichte“).

Bis heute stellt sich die offizielle Lehre der christlichen Hauptströmungen (Orthodoxie, Katholizismus, Protestantismus) klar gegen jede Form der „Wahrsagerei“ und Magie, so beispielsweise der Katechismus der Katholischen Kirche:
Die Evangelische Kirche in Deutschland schreibt:
Grundsätzliche Kritik an jeglicher Esoterik äußern Vertreter der Skeptikerbewegung. So behauptet der Physiker Martin Lambeck, die Esoterik wolle das "„mechanistisch-materialistische“" Weltbild der Physik schleifen, die Physik erscheine daher "„wie eine belagerte Festung“". "„Ausgangspunkt aller Esoterik“" ist nach Lambeck (der sich dabei offenbar auf ein Buch von Thorwald Dethlefsen stützt) die Lehre des Hermes Trismegistos; „hermetische Philosophie“ sei gleichbedeutend mit Esoterik. Insbesondere bilde das in dem Satz "„Wie oben, so unten“" klassisch formulierte Analogieprinzip die Grundlage aller Esoterik, und die Esoteriker seien davon überzeugt, auf dieser Grundlage die gesamte Welt des Mikro- und Makrokosmos erforschen zu können. Daraus folge aber, so Lambeck, dass aus der Sicht der Esoterik "„alle seit Galilei mit Fernrohr und Mikroskop durchgeführten Untersuchungen überflüssig“" gewesen seien. Zudem stehe das Analogisieren "„im fundamentalen Widerspruch zur Methode der heutigen Wissenschaft“". Aus diesem und anderen, ähnlichen angeblichen Widersprüchen zieht Lambeck nun allerdings nicht die Konsequenz, Esoterik abzulehnen. Ihm geht es um die Widerspruchsfreiheit des Lehrgebäudes der Physik und um ihren Anspruch, für ihr Gebiet allein zuständig zu sein. Die Esoterik mache Aussagen, die in diesen Zuständigkeitsbereich fielen, beispielsweise dass alles in der Welt aus zehn Urprinzipien aufgebaut sei (laut Lambeck ein grundlegendes Postulat der Esoterik). Die Existenz derartiger sogenannter „Paraphänomene“ müsse daher im Sinne des Popperschen Falsifikationismus empirisch getestet werden.

Viele Kritiker, aber auch manche Esoteriker selber beklagen einen „Supermarkt der Spiritualität“: Verschiedene, teils widersprüchliche spirituelle Traditionen, die über Jahrhunderte in unterschiedlichen Kulturen der Welt entstanden, würden in der Konsumgesellschaft zur Ware, wobei sich verschiedene Trends und Moden schnell abwechselten ("„gestern Yoga, heute Reiki, morgen Kabbala“") und als Produkt auf dem Markt ihres eigentlichen Inhalts beraubt würden. Dieser Umgang sei oberflächlich, reduziere Spiritualität auf Klischees und beraube sie ihres eigentlichen Sinnes.

In jüngerer Zeit (insbesondere seit den 1990er Jahren) werden auch rassistische und anderweitig rechtsextremistische Erscheinungen innerhalb der Esoterik kritisch kommentiert (siehe Rechtsextremismus und Esoterik). So bemerkt der Journalist Rainer Fromm in der schon erwähnten Hamburger Broschüre: "„Die völkische Käuferschaft hat die Esoterik für sich entdeckt.“" Fromm kommt zu der Einschätzung: "„Offener Neonazismus, Ariosophie und antisemitische Verschwörungsliteratur decken nur eine Minderheit der esoterischen Bewegung ab. Irrationalismus, Karma, Rassismus, Gurus und dogmatische Heilslehren hingegen sind zentrale Bestandteile des esoterischen Glaubens und machen ihn anfällig für autoritäre und rechtsextremistische Ideologien.“" Dagegen schreibt der Strafrechtler Alexander A. Niggli in einer im Auftrag der Eidgenössischen Kommission gegen Rassismus erstellten Expertise, dass "„Esoterik grundsätzlich nicht mit Rassendiskriminierung verknüpft ist“".

Scharfe Polemik gegen Esoterik übte die ehemalige Grünen-Politikerin Jutta Ditfurth in ihrem zuerst 1992 erschienenen Buch "Feuer in die Herzen". Ditfurth bezeichnet Esoterik als Ideologie, welche „ein übelriechender Eintopf aus geklauten, ihrem sozialen und kulturellen Zusammenhang entrissenen Elementen aus allen traditionellen Religionen“ sei und faschistische Wurzeln habe.





</doc>
<doc id="1240" url="https://de.wikipedia.org/wiki?curid=1240" title="Esperanto">
Esperanto

Esperanto ist die am weitesten verbreitete Plansprache. Ihre heute noch gültigen Grundlagen wurden als "Lingvo Internacia" („internationale Sprache“) 1887 von dem Augenarzt Ludwik Lejzer Zamenhof veröffentlicht, dessen Pseudonym Doktoro Esperanto („Doktor Hoffender“) zum Namen der Sprache wurde. Esperanto besitzt in keinem Land der Welt den Status einer Amtssprache, wird jedoch von Sprechern in über 100 Staaten verwendet; in Polen gehört Esperanto "als Träger der Esperanto-Kultur" seit 2014 zum offiziellen immateriellen Kulturerbe.

1887 veröffentlichte Zamenhof in Warschau eine Broschüre mit den Grundlagen der Sprache. In seinem Unua Libro formulierte er zugleich drei Ziele für seine Sprache:


Die erste Ausgabe des Unua Libro, in Russisch, umfasst 40 Seiten im Format A5. Der Grammatik-Teil darin enthält 16 Regeln auf 6 Seiten.

1889 folgte eine Adressenliste mit den ersten Anhängern, außerdem wurde die auf Esperanto in Nürnberg herausgegebene Zeitschrift "La Esperantisto" gegründet.

1898 gründete Louis de Beaufront eine französische Esperanto-Gesellschaft, aus der später der erste Esperanto-Landesverband wurde. 1908 wurde der Esperanto-Weltbund gegründet. Bis zum Ausbruch des Ersten Weltkrieges gab es Verbände oder zumindest Ortsgruppen auf allen Kontinenten.

Zwischen den beiden Weltkriegen kam es in mehr als einem Dutzend Ländern zu Behinderungen. Im nationalsozialistischen Deutschland wurden neben vielen anderen auch Plansprachenvereinigungen verboten.

Unter Josef Stalins Herrschaft in der Sowjetunion gab es kein öffentlich bekannt gemachtes Verbot, jedoch wurden bereits mit Beginn der Großen Säuberung neben vielen anderen Gruppen auch führende Esperanto-Sprecher verhaftet und deportiert. Der Geheimdienst NKWD listete zunächst u. a. „alle Menschen mit Auslandskontakten“ auf. Ein Befehl von 1940 aus Litauen listet „Esperantisten“ neben Briefmarkensammlern unter den zu registrierenden Personengruppen. Tausende Esperantosprecher wurden verhaftet und in Lager gesperrt; Rytkov schätzte, dass unter den 1,5 Millionen Verhafteten auch 30.000 sowjetische Esperanto-Sprecher waren, von denen einige Dutzend erschossen worden sind; Tausende starben später in Lagern. 

Während des Kalten Krieges dauerte es längere Zeit, bis in den osteuropäischen Staaten Esperanto-Verbände gegründet werden konnten. Eine Ausnahme bildete Jugoslawien, wo bereits 1953 ein Esperanto-Weltkongress stattfand. 1959 fand in Warschau der erste Weltkongress in einem Land des Ostblocks statt. Nach und nach entwickelten sich Kontakte und Zusammenarbeit zwischen den Landesverbänden in Ost und West. 1980 durfte der chinesische Landesverband dem Esperanto-Weltbund beitreten.

Nach dem Zweiten Weltkrieg stieg die Zahl der Landesverbände im Weltbund stetig an. 1948 hatte der Weltbund 19 Landesverbände, 1971 bereits 34, 1989 waren es 47 und 2013 insgesamt 71.

Die Wörter bestehen überwiegend aus unveränderlichen Wortelementen, die aneinandergefügt werden. So wird beispielsweise die Mehrzahl eines Substantivs oder Adjektivs und vieler Pronomen durch das Anhängen eines "-j" gebildet: "domo" ,Haus‘, "domoj" ,Häuser‘, der Objektfall durch das Anhängen eines weiteren "-n:" "domojn" ‚Häuser (Akk. Plural)‘. Der Wortstamm wird nicht verändert, wie es oft im Deutschen vorkommt. Das hier sichtbare agglutinierende Prinzip ist beispielsweise auch aus dem Finnischen, Ungarischen und Türkischen bekannt.

Zamenhof strebte einen regelmäßigen Sprachbau an, um den Lernaufwand zu minimieren, insbesondere in der Morphologie und bei der Wortbildung. Für die Deklination von Substantiven und die Konjugation von Verben gibt es jeweils nur ein Schema. Auch das in vielen Sprachen unregelmäßige Verb „sein“ wird im Esperanto nach demselben Schema konjugiert wie alle anderen Verben:


Zur besseren Erkennbarkeit haben einige Wortarten bestimmte Endungen. "-o" beispielsweise ist die Endung für Substantive: "domo" ,Haus‘; "-a" ist die Endung für Adjektive: "doma" ,häuslich‘ usw. Auch einige Wörter, die weder Substantive noch Adjektive sind, enden auf "-o" oder "-a," sodass der Endvokal allein zur Wortartbestimmung nicht ausreicht.

Die meisten Esperanto-Wörter entstammen dem Latein oder romanischen Sprachen. Ein ziemlich großer Anteil kommt aber auch aus germanischen Sprachen, vor allem dem Deutschen und Englischen (je nach Textkorpus wird dieser Anteil auf fünf bis zwanzig Prozent geschätzt). Dazu gibt es eine Reihe von Wörtern aus slawischen Sprachen, besonders dem Polnischen und dem Russischen. Außerdem wurden Wörter aus dem Griechischen entlehnt.

In der Regel sind die Wörter aber in mehreren indogermanischen Sprachen bekannt, zum Beispiel Esperanto "religio" ‚Religion‘: englisch "religion", französisch "religion", polnisch "religia"; Esperanto "lampo" ‚Lampe‘: englisch "lamp," französisch "lampe", polnisch "lampa" usw. Teilweise existieren im Esperanto bewusste Mischformen, zum Beispiel "ĝardeno" ‚Garten‘: Die Schreibung ähnelt englisch "garden", die Aussprache ähnelt französisch "jardin".

Die Schreibweise ist phonematisch, das heißt, dass jedem Schriftzeichen nur ein Phonem (Sprachlaut) und jedem Phonem nur ein Schriftzeichen zugeordnet ist. Sie verwendet Buchstaben des lateinischen Alphabets, ergänzt durch Überzeichen (diakritische Zeichen). Beispielsweise entspricht "ŝ" dem deutschen "sch" und "ĉ" dem "tsch" (z. B. in "ŝako" ‚Schach‘ und "Ĉeĉenio" ‚Tschetschenien‘). (Siehe auch Esperanto-Rechtschreibung.)

Sprachbeispiel
Allgemeine Erklärung der Menschenrechte, Artikel 1:

Jährlich erscheinen mehr als hundert wissenschaftliche Artikel zu Esperanto und anderen geplanten Sprachen. In Deutschland gibt es die „Gesellschaft für Interlinguistik“, deren Mitglieder sich der Erforschung des Esperanto und anderer geplanter sowie internationaler Sprachen und ihrer Verwendung widmen.

Der mit etwa 15.000 Mitgliedern größte weltweite Dachverband ist der Esperanto-Weltbund mit Sitz in Rotterdam. Ihm obliegt die Ausrichtung und Organisation des jährlich stattfindenden Esperanto-Weltkongresses, der größten und wichtigsten Veranstaltung mit Teilnehmerzahlen zwischen 350 und 3.000.

Die größten Esperanto-Organisationen in Deutschland sind mit etwa 1.600 Mitgliedern der Deutsche Esperanto-Bund sowie dessen Jugendorganisation, die Deutsche Esperanto-Jugend (DEJ), die mit ihren Orts-, Regional- und Landesverbänden etwa 130 Mitglieder hat. Diese ist gleichzeitig Mitglied der weltweiten Jugendorganisation TEJO.

Im österreichischen Landesverband sind 72 Esperantisten organisiert, die Schweizerische Esperanto-Gesellschaft vertritt 170 Mitglieder inklusive der Jugendgruppe.

Nach Darstellung des Esperanto-Aktivisten Renato Corsetti waren 1996 etwa 350 Familien bei der „Familia Rondo“ des Esperanto-Weltbundes registriert, in denen die Kinder mit Esperanto als zweiter Muttersprache aufwuchsen. Schätzungen von 2012 gehen von bis zu 2.000 Muttersprachlern aus; der Esperanto-Weltbund gibt derzeit (April 2017) eine Anzahl von 1.000 Muttersprachlern an.

Die Schätzungen für die Zahl der heutigen Sprecher weichen stark voneinander ab – es finden sich Zahlen zwischen 100.000 und zehn Millionen; es ist zu beachten, dass verschiedene Angaben sich auf unterschiedliche Niveaus der Sprachbeherrschung und -nutzung beziehen.

Schätzungen gehen davon aus, dass in den über 130 Jahren seines Bestehens zwischen 5 und 15 Millionen Menschen Esperanto erlernt hätten.
1889 lebten noch über 90 % der Esperantosprecher in Russland. Eine umfassende Erhebung des deutschen Esperanto-Instituts im Jahre 1926 ergab eine Anzahl von 136.209 Sprechern weltweit, darunter über 120.000 in Europa, etwa 31.000 in Deutschland.
Sprecher lebten vor allem in Europa; Esperanto habe darüber hinaus eine lange Geschichte in Ländern wie China, Japan und Brasilien und aktive Esperanto-Sprecher könnte man in den meisten Ländern der Welt finden, schreiben Byram und Hu. Laut John R. Edwards gab es in China 2004 bei einer Bevölkerungszahl von über einer Milliarde Menschen ca. 10.000 Esperanto-Sprecher, von denen etwa 10 % die Sprache fließend beherrschten, was einem Anteil von 0,0001 % der Bevölkerung entspräche (fließende Sprachbeherrschung). Bei der ungarischen Volkszählung für 2011 gaben 8.397 Personen Esperanto-Kenntnisse an, ohne dass nach der Sprachfertigkeit gefragt wurde. Bei einer Einwohnerzahl von etwa 10 Millionen entspricht das einem Anteil von 0,1 Prozent der Bevölkerung, die angaben, Sprachkenntnisse in Esperanto zu besitzen.

Das linguistische Sammelwerk Ethnologue gibt eine Zahl von zwei Millionen Menschen an, die Esperanto sprechen; diese Zahl basiert auf Schätzungen von 2004 und 2015.

Der 2013 zum Präsidenten der Universala Esperanto-Asocio gewählte Mark Fettes ging im Jahr 2003 von weniger als 150.000 Sprechern weltweit aus; dies dürfte sich auf die Zahl der regelmäßigen Sprecher beziehen. Rudolf Fischer, damals Vorsitzender des Deutschen Esperanto-Bundes, vermutete 2008:
„Weltweit sprechen rund 100.000 Menschen fließend und regelmäßig Esperanto, davon leben etwa 2000 in Deutschland.“

Die Frage nach der Anzahl der heutigen Esperantosprecher „löst leicht Verlegenheit aus“ und „gerät dabei unversehens zum Schätzungsabenteuer“.

Der Esperanto-Weltbund (UEA) hatte 2011 insgesamt 5.321 Einzelmitglieder und etwa 15.000 zusätzliche Mitglieder über seine weltweiten Landesverbände. Bis Ende 2016 fiel die Zahl der Einzelmitglieder auf 4.365 und die der assoziierten Mitglieder auf 8.689. Das ist der niedrigste Stand seit der Neugründung der UEA 1947.

Während zu Zeiten des Kalten Kriegs in den sozialistischen Staaten Esperanto gefördert wurde, spielt Esperanto-Unterricht in Schulen oder Hochschulen des ehemaligen Ostblocks heute faktisch keine Rolle mehr.

Nach Angaben aus dem Jahr 1982 wurde seinerzeit in 36 Ländern Esperanto-Unterricht aufgrund staatlicher Verfügungen erteilt. Dazu gehörten viele sozialistische Staaten, darunter Polen, Ungarn, Bulgarien und die baltischen Sowjetrepubliken. 

Hintergrund war die Tatsache, dass die damaligen sozialistischen bzw. kommunistischen Staaten, Englisch als de-facto-Weltsprache und die damit einhergehende westliche Dominanz nicht akzeptieren wollten und daher Esperanto als Gegengewicht unterstützten. Dazu wurden die staatlich beköstigten Esperanto-Verbände eingesetzt. Lehrveranstaltungen an Universitäten gab es 1970 weltweit an 15 Hochschulen, 1980 an 51 und 1985 an 110 Hochschulen in 22 Ländern. Nach einer Schätzung des Esperanto-Funktionärs Humphrey Tonkin aus dem Jahr 1984 erlernten an 32 chinesischen Universitäten 120.000 Studenten Esperanto, während gleichzeitig etwa 10 Millionen chinesische Studenten Englisch lernten. Der wichtigste Esperanto-Studiengang bestand zwischen 1969 und 2002 an der Eötvös-Loránd-Universität in Budapest.

Anfang des 21. Jahrhunderts ist in Ungarn Esperanto als Prüfungsfach an höheren Schulen zugelassen. Es existieren kleinere Schulprojekte an Grundschulen wie das britische "Springboard to Languages," das an vier Grundschulen durchgeführt wird. Ein dreijähriger esperantosprachiger Studiengang „Interlinguistik“ wird seit 1998 an der Adam-Mickiewicz-Universität Posen angeboten; an der Universität Amsterdam existiert seit 2002 ein vom Esperanto-Weltbund finanzierter, jeweils auf fünf Jahre begrenzter Lehrstuhl für Interlinguistik und Esperanto.

Gegen die Einführung von Esperanto als allgemein zu lernende internationale Sprache wurde unter anderem folgendes als Kritik vorgebracht:


Sprachwissenschaft

Geschichte

Wörterbücher (in Buchform)

Wörterbücher (online)

Lehrbücher und Grammatiken




</doc>
<doc id="1241" url="https://de.wikipedia.org/wiki?curid=1241" title="Edgar G. Ulmer">
Edgar G. Ulmer

Edgar Georg Ulmer (* 17. September 1904 in Olmütz, Österreich-Ungarn, heute Tschechien; † 30. September 1972 in Woodland Hills, Kalifornien) war ein US-amerikanischer Filmregisseur, Drehbuchautor, Bühnenbildner und Produzent mährisch-österreichischer Herkunft.

Ulmer vermochte als Regisseur Produktionen mit niedrigem Budget formell so sehr zu verfeinern, dass sie rückblickend von Filmjournalisten und -historikern hoch geschätzt wurden. Ulmer schuf insgesamt 128 Filme in mehreren Ländern.

Edgar Georg Ulmer wurde während eines Ferienaufenthaltes seiner jüdischen Eltern im mährischen Olmütz geboren. Im Jahre 2005 fand der Literaturwissenschaftler Bernd Herzogenrath nach jahrelanger Archivarbeit das Geburtshaus Ulmers – als Resultat der Bemühungen wurde anlässlich des von Herzogenrath organisierten 'ulmerfest' 2006 eine Gedenktafel am Haus angebracht (siehe Foto). Ulmer studierte an der Akademie für angewandte Kunst Wien mit dem Ziel, Bühnenbildner zu werden.

Seine ersten Filmarbeiten dürften eigenen Angaben zufolge ab 1920 erfolgt sein, sind aber nicht mehr verifizierbar. Ab dem Alter von 16 Jahren habe er demnach, da er sich um vier Jahre älter ausgegeben hatte, an deutschen und österreichischen Großproduktionen wie "Der Golem, wie er in die Welt kam", "Sodom und Gomorrha", "Nibelungen" und "Der letzte Mann" als Szenenbildner mitgearbeitet.

1923 begleitete Ulmer Max Reinhardt als Szenenbildner für die "Mirakel"-Inszenierung nach New York. 1924 kam er erneut in die Vereinigten Staaten und fand dort 1925 bei Universal als Ausstattungs- und Regieassistent bei namentlich nicht bekannten „Two-Reel“ (also etwa 20 Minuten langen) Western-Serienproduktionen Beschäftigung. Anschließend arbeitete er als Assistent von Friedrich Wilhelm Murnau, der mittlerweile nach Hollywood gewechselt hatte. Als "Art Director" von "Sunrise" wird Ulmer erstmals auch im Titelvorspann erwähnt.

1929 war Ulmer wieder in Berlin tätig. Er war Produktionsleiter und Szenenbildner bei "Flucht in die Fremdenlegion" und "Spiel um den Mann" und wirkte darüber hinaus an "Menschen am Sonntag" mit, einem Stummfilmklassiker, der unter Federführung einer Reihe damals noch unbekannter Filmschaffender wie Billy Wilder, Fred Zinnemann und den Gebrüdern Siodmak entstand. Danach, 1930, ging Ulmer erneut und endgültig in die Vereinigten Staaten. In Hollywood übernahm er die Postproduktion von Murnaus "Tabu" (1931), nachdem dieser bei einem Autounfall verstorben war. Es folgten Arbeiten als Bühnenbildner für die "Philadelphia Grand Opera Company" und als Art Director für MGM.

1932 ging Ulmer nach New Jersey, um in den Metropolitan Studios von Fort Lee die Regie von "The Warning Shadow" zu übernehmen. Der Film kam jedoch nie in die Kinos; möglicherweise wurde er nicht fertiggestellt. Zwei Akte des Films wurden jedoch in der von Ed Sullivan geschriebenen Satirekomödie über New York, "Mr. Broadway", eingebaut.

Ein Schwerpunkt in Ulmers Filmen waren eine Zeit lang Minderheiten in den USA. Als „director of minorities“ bezog er Eigenheiten der jeweiligen Volksgruppe mit ein und drehte in Mexiko wie auch an der Ostküste der USA für die ukrainische Minderheit, für osteuropäische jüdische Immigranten oder die schwarze Bevölkerung Harlems. So entstanden besondere „Minderheitenfilme“ wie die jiddischen Melodramen "Grine felder" und "Die Kliatsche", der auf Ukrainisch gedrehte Film "Zaporozhets Za Dunayem" oder das melodramatische Musical "Moon over Harlem". Diese Filme wurden in Biographien über ihn lange Zeit kaum näher betrachtet.

Zwischen 1940 und 1942 stellte Ulmer auch kurze Aufklärungsfilme für die nationale Gesundheitsbehörde und Trainings- und Lehrfilme für die US Army her. Ab 1942 inszenierte er in Zusammenarbeit mit Leon Fromkess als "First Contract Director" 15 Filme für die Producers Leasing Corporation her. Als diese 1946 aus finanziellen Gründen den Betrieb einstellte, gründete Ulmer eine eigene Gesellschaft, die jedoch nach wenigen Monaten ebenfalls pleite war.

1946 inszenierte er die auf Hedy Lamarr zugeschnittene Großproduktion "The Strange Woman", kehrte aber danach wieder zu kleineren „Independent“-Filmgesellschaften zurück.

Ulmer arbeitete meist unter einschränkenden Umständen, mit knappen Budgets, oft mittelmäßigen Drehbüchern, Zeitdruck und für zweitklassige Studios. Die so entstandenen, so genannten B-Movies waren nicht mit den Großproduktionen der großen Hollywood-Studios vergleichbar. Sie brachten aber Werke hervor, die zwar zu Lebzeiten kaum gewürdigt wurden, aber retrospektiv, im Hinblick auf den Stil des Film noir, von Filmjournalisten und -historikern geschätzt werden.

Sein Ruf geht auf jene jeweils innerhalb kürzester Zeit entstandenen Filme zurück, in denen er unter einschränkenden Umständen Möglichkeiten zur atmosphärischen und visuellen Anreicherung geschickt ausnutzte. Erster dieser Filme war der Horrorklassiker "Die schwarze Katze" ("The Black Cat") nach Edgar Allan Poe, in dem die beiden Filmstars Boris Karloff und Bela Lugosi erstmals gemeinsam vor der Kamera standen. Weitere Arbeiten waren "Corregidor", "Bluebeard", der Film noir "Umleitung", die von "Citizen Kane" inspirierte und mit Sozialkritik durchsetzte psychologische Studie "Ruthless", der Western "The Naked Dawn" und der Science-Fiction-Film "Beyond the time Barrier".

Seine Filme wurden zur Zeit ihrer Entstehung von der US-amerikanischen Filmkritik überwiegend ignoriert, in Filmkreisen wurde sein Schaffen kontrovers diskutiert. 1956 machte Luc Moullet in der Zeitschrift "Les Cahiers du cinéma" auf den vernachlässigten Künstler der visuellen Sprache, den "le plus maudite cinéaste" ("verfluchtesten Filmschaffenden"), aufmerksam und verschaffte einer weniger voreingenommenen Betrachtung Platz.

Ulmer war in zweiter Ehe mit der Autorin Shirley Kassler (1914–2000) verheiratet, die einige seiner Drehbücher verfasste. Die gemeinsame Tochter Arianné Ulmer Cipes steht der "Edgar G. Ulmer Preservation Corp." in Sherman Oaks vor und trat als Schauspielerin unter dem Namen "Arianne Arden" in drei Nachkriegsfilmen ihres Vaters in Erscheinung.

Edgar G. Ulmer wirkte an folgenden Filmen als Regisseur, Regieassistent, Produzent, Drehbuchautor, Bühnenbildner oder Kameramann mit. Von der Überschrift abweichende Tätigkeiten sind in Klammer angegeben.








</doc>
<doc id="1242" url="https://de.wikipedia.org/wiki?curid=1242" title="Establishing Shot">
Establishing Shot

Ein Establishing Shot () ist die erste Einstellung einer Sequenz, häufig eine Totale. Er zeigt meist eine Landschaftsaufnahme oder den jeweiligen Ort des Geschehens. Durch den Establishing Shot soll der Ort der Handlung vorgestellt und dadurch etabliert werden. Er dient damit der räumlichen und zeitlichen Orientierung des Zuschauers im Handlungsraum.

Besonders wichtig sind Establishing Shots bei bekannten Schauplätzen wie berühmten Weltstädten. Sie müssen so gestaltet sein, dass der Zuschauer den Handlungsort sofort erkennen kann. Ein sehr populäres Beispiel ist Hongkong, das seit den 1970er Jahren sehr häufig mit immer fast gleichen Aufnahmen eines auf dem berühmten Flughafen Kai Tak landenden Flugzeugs über der Stadt als Establishing Shot eingeführt wurde. Bei Filmen, die in Berlin spielen, wird z. B. der Fernsehturm oder der Bundestag in der Eröffnungssequenz gezeigt, bei Paris meist der Eiffelturm.



</doc>
<doc id="1243" url="https://de.wikipedia.org/wiki?curid=1243" title="Eisenhüttenstadt">
Eisenhüttenstadt

Eisenhüttenstadt ist eine Stadt im Land Brandenburg, am Westufer der Oder. Sie entstand als Planstadt nach einem Beschluss im Juli 1950 als sozialistische Wohnstadt für das Eisenhüttenkombinat Ost (EKO), das noch heute ein bedeutender Arbeitgeber ist. Gebaut wurde nahe dem historischen, seit dem 13. Jahrhundert bestehenden Ort Fürstenberg (Oder), mit dem der seit 1953 "Stalinstadt" genannte Ort 1961 zu "Eisenhüttenstadt" vereint wurde. Das Mittelzentrum gehört zum Landkreis Oder-Spree und bildete bis 1993 einen eigenen Stadtkreis. Seitdem hat sie den Status einer amtsfreien Großen kreisangehörigen Stadt.

Durch seine besondere Geschichte als komplette Stadtneugründung und den städtebaulichen Aufbau mit diversen Baudenkmalen gilt Eisenhüttenstadt als einmaliges Bauensemble.

Eisenhüttenstadt liegt auf einer Talsandterrasse des Warschau-Berliner Urstromtales. Im Süden ist es vom Hügelland einer Endmoräne, den Diehloer Bergen, begrenzt. In Eisenhüttenstadt mündet der Oder-Spree-Kanal in die Oder.

Die Stadt liegt etwa 25 Kilometer südlich von Frankfurt (Oder), 25 Kilometer nördlich von Guben und 110 Kilometer von Berlin entfernt.

Eisenhüttenstadt befindet sich im äußersten Norden der Niederlausitz und ist nach Cottbus und Żary (Sorau) deren drittgrößte Stadt. Im Landkreis Oder-Spree ist Eisenhüttenstadt, nach Fürstenwalde/Spree, die zweitgrößte Stadt.

Die Stadt besteht aus dem nach 1950 entstandenen Stadtzentrum und den eingemeindeten Ortsteilen:

Das Stadtzentrum ist wiederum in sieben Wohnkomplexe unterteilt, die ursprünglich mit Geschäften und Dienstleistungseinrichtungen sowie Schulen und Kindergärten ausgestattet waren.


Bereits nach 1251 wurde auf dem heutigen Stadtgebiet im Rahmen der Territorialpolitik des meißnischen Markgrafen Heinrichs des Erlauchten die Stadt Fürstenberg im Verband der Niederlausitz gegründet. 1286 ist sie als Civitas und Zollstätte bezeugt. Im 14. Jahrhundert veranlasste Kaiser Karl IV. den Bau einer Stadtmauer. Von 1316 bis 1817 stand die Grundherrschaft mit geringen Unterbrechungen dem Kloster Neuzelle zu. Der in der ersten Hälfte des 14. Jahrhunderts gebildete Rat hatte die Niedergerichte inne, gemeinsam mit dem Abt von Neuzelle auch die Obergerichte.

Nach dem Prager Frieden 1635 kam Fürstenberg mit der Niederlausitz zum Kurfürstentum Sachsen, 1815 fiel es an Preußen. Das abseits der Fernstraße Frankfurt (Oder) – Guben an einer wenig bedeutenden Oderfähre gelegene, aber als Zollstätte wichtige Städtchen, in dem auch Fischerei und Schifffahrt betrieben wurden, hatte im Jahr 1830 1686 Einwohner. Mit dem Bau der Bahn von Frankfurt (Oder) nach Breslau 1846 und im Anschluss an den hier in die Oder mündenden Oder-Spree-Kanal (1891) begann eine industrielle Entwicklung mit Glashütten, Werften, Säge-, Öl- und Getreidemühlen. Die jüdische Gemeinde der Stadt nahm 1890 ihren Friedhof in Nutzung, der später von den Nazis zerstört wurde. Zwischen 1871 und 1900 verdoppelte sich die Bevölkerungszahl auf 5.700, bis 1933 stieg sie auf 7.054. Im Jahre 1925 wurde ein Oderhafen angelegt.

Für die Kriegsvorbereitungen der Nationalsozialisten entstand zwischen dem Kanal, der Bahnlinie und der Schönfließer Chaussee (heute Beeskower Straße) das "Chemische Zentralwerk" der DEGUSSA, in dem während des Zweiten Weltkrieges Häftlinge eines Außenlagers des KZ Sachsenhausen und Kriegsgefangene des M-Stammlager III B (Kriegsgefangenen-Mannschafts-Stammlager) Zwangsarbeit verrichteten, bei der Tausende ums Leben kamen. Außerdem wurden sie im "Oder Gerätebau" eingesetzt, einer ausgelagerten Rüstungsfabrik von "Rheinmetall-Borsig", im MEW Kraftwerk an der Oder, im Forst und beim Straßenbau. Zwischen 1940 und 1943 wurde am Oder-Spree-Kanal der GBI-Hafen errichtet, mit einem Granitlager für die geplante Reichshauptstadt, der heutige Hafen Eisenhüttenstadt. Am 24. April 1945 besetzte die Rote Armee die Stadt. Fürstenberg (Oder) wurde Garnisonsstadt der sowjetischen Truppen. Die Industrieanlagen wurden zum großen Teil als Reparationsleistung demontiert.
Auf dem III. Parteitag der SED vom 20. bis 24. Juli 1950 wurde der Beschluss zum Bau des Eisenhüttenkombinats Ost (EKO) und einer sozialistischen Wohnstadt bei Fürstenberg (Oder) gefasst. Die neue Wohnstadt sollte nach den „16 Grundsätzen des Städtebaus“ und im architektonischen Stil des Sozialistischen Klassizismus errichtet werden.

Am 18. August 1950 erfolgte der symbolische erste Axthieb zum Baubeginn des Eisenhüttenkombinats. Am 1. Januar 1951 legte Minister Fritz Selbmann den Grundstein für den ersten Hochofen, der am 19. September 1951 den Betrieb aufnahm. Bis 1955 entstanden fünf weitere Hochöfen. Am 1. Februar 1953 wurde die Wohnstadt als selbstständiger Stadtkreis aus dem Kreis Fürstenberg herausgelöst und am 7. Mai 1953 aus Anlass des Todes von Stalin in Stalinstadt umbenannt. Ursprünglich sollte die Stadt zum 70. Todestag von Karl Marx den Namen "Karl-Marx-Stadt" erhalten, den dann aber stattdessen Chemnitz erhielt. Ende des Jahres 1953 hatte die Stadt 2.400 Einwohner, im Jahre 1960 bereits 24.372. Fürstenberg (Oder) wurde 1952 Kreisstadt und hatte 1960 eine Einwohnerzahl von 6.749.

Am 13. November 1961 wurden die Städte Fürstenberg (Oder) (mit dem Ortsteil Schönfließ) und Stalinstadt zu Eisenhüttenstadt zusammengeschlossen, um im Rahmen der Entstalinisierung den unerwünscht gewordenen Namen zu tilgen. Dabei wurde die Stadt Fürstenberg (Oder) aus dem Landkreis Fürstenberg herausgelöst und der bereits unter dem Namen Stalinstadt bestehenden kreisfreien Stadt zugeschlagen. Eisenhüttenstadt war dann bis zur Bildung des Landkreises Oder-Spree sowohl kreisfreie Stadt als auch Kreisstadt des Kreises Eisenhüttenstadt.

Am 19. September 1986 wurde unter großer politischer Anteilnahme in der Bundesrepublik ein Abkommen über die erste deutsch-deutsche Städtepartnerschaft zwischen Saarlouis und Eisenhüttenstadt unterzeichnet.

Mit dem Ausbau des Hüttenwerks stieg die Einwohnerzahl bis 1988 auf den historischen Höchststand von über 53.000. Im Jahre 1993 erfolgte die Eingemeindung des Ortes Diehlo. 1996 wurde die Neue Deichbrücke über den Oder-Spree-Kanal wiederaufgebaut. Mit dem Strukturwandel nach der Wiedervereinigung hat sich die Einwohnerzahl nahezu halbiert. Um den Schrumpfungsprozess zu beherrschen, wurde ein Stadtumbauprogramm begonnen, das mit dem Abriss und der Sanierung zahlreicher Wohnungen verbunden ist.

Der etwas sperrige Name der Stadt hat immer schon dazu animiert, griffigere Bezeichnungen zu kreieren. In der Umgangssprache wird die Stadt oft verkürzt mit „Hüttenstadt“ oder „Hütte“ bezeichnet. Aufgrund des Verfalls seit 1989 wird die Stadt heute im Volksmund bisweilen „Schrottgorod“ genannt. Schrott verballhornte darin das Eisen als ein zur Wiederverwertung anstehendes Material, die Endung "-gorod" die russische Endung für -stadt.

Die Gemeinde Diehlo wurde im Jahr 1993 Ortsteil von Eisenhüttenstadt.

Bei der folgenden Übersicht der Einwohnerzahlen von Eisenhüttenstadt (vor 1961 Stalinstadt) handelt es sich um amtliche Fortschreibungen der Staatlichen Zentralverwaltung für Statistik (bis 1989) und des Amtes für Statistik Berlin-Brandenburg (ab 1990).

Gebietsstand des jeweiligen Jahres (ab 1995 jeweils 31. Dezember), ab 2011 auf Basis des Zensus 2011

Der starke Bevölkerungszuwachs im Jahr 2015 hängt mit der Aufnahme von Flüchtlingen in der Erstaufnahmeeinrichtung Eisenhüttenstadt zusammen. Der Rückgang 2016 ist darauf zurückzuführen, dass die Zahl der untergebrachten Flüchtlinge wieder deutlich gesunken ist.

Die Stadtverordnetenversammlung (SVV) Eisenhüttenstadt besteht seit der Kommunalwahl am 28. September 2008 aus 32 Stadtverordneten und dem hauptamtlichen Bürgermeister. Bei der Kommunalwahl am 25. Mai 2014 errang bei einer Wahlbeteiligung von 37,5 Prozent (−4,8 Prozentpunkte im Vergleich zu 2008) die AfD Stimmen für drei Sitze, konnte allerdings nur zwei besetzen. Damit blieb ein Sitz unbesetzt und es ergab sich folgende Sitzverteilung:

Die Stadtverordneten von BVFO (Bürgervereinigung Fürstenberg/Oder), Bündnis 90/Die Grünen und Piraten bilden dabei eine gemeinsame Fraktion.

Bürgermeister von Stalinstadt bzw. seit 1961 Eisenhüttenstadt:

Frank Balzer (SPD) wurde in der Stichwahl am 8. Oktober 2017 mit 67,9 % der gültigen Stimmen für eine Amtszeit von acht Jahren zum neuen Bürgermeister der Stadt gewählt.

Das Wappen wurde am 31. Januar 1992 genehmigt.

Blasonierung: „In goldenem Feld über drei blauen Wellenfäden in Rot rechts ein Hochhaus, links ein Hochofensystem überhöht von dem bandförmig blauen Teilumriss einer links gewandten Friedenstaube.“

Eisenhüttenstadt führt seit 1973 ein Wappen, das von Johannes Hansky (1925–2004) entworfen wurde. Im Vordergrund werden ein rotes Hochhaus und daneben ein roter Hochofen dargestellt, die das metallurgische Zentrum symbolisieren. Darüber schwebt stilisiert eine Friedenstaube. Im Schildfuß symbolisieren drei blaue Wellen die Lage an der Oder.


Die evangelische Friedensgemeinde Eisenhüttenstadt nutzte für Gottesdienste in Schönfließ zunächst einen Raum in einer Gaststätte. In der Neustadt waren zunächst ein sogenannter Evangeliumswagen, zwischenzeitlich ein Zelt und ab 1952 eine Baracke vorhanden. Für die geplanten Wohnsiedlungen, damals noch als Stalinstadt, waren seitens Walter Ulbricht keine kirchlichen Einrichtungen und insbesondere "keine Kirchtürme" vorgesehen. Das heutige evangelische Kirchengebäude und Gemeindezentrum in der Neustadt wurde nach 1976 erbaut und geht mit auf den langjährigen Einsatz des späteren Ehrenbürgers Pfarrer Heinz Bräuer zurück.

Im Ortsteil Fürstenberg wurde die im Krieg stark zerstörte Nikolaikirche provisorisch aufgebaut und nach der Wende grundlegend saniert. Die neuapostolische Gemeinde in Eisenhüttenstadt hat eine Kirche im Stadtteil Fürstenberg.

Seit den 1920er Jahren gab es eine baptistische Gemeindearbeit, aus der 1990 die Evangelisch-Freikirchliche Gemeinde als selbstständige Gemeinde hervorging.

In der Liste der Baudenkmale in Eisenhüttenstadt und der Liste der Bodendenkmale in Eisenhüttenstadt stehen die vom Land Brandenburg unter Denkmalschutz gestellten Kulturdenkmale der Stadt.



Im Ortsteil Fürstenberg sind an der Königstraße 61 durch den Künstler Gunter Demnig Stolpersteine für Emma und Siegfried Fellert verlegt worden.



Vermutlich seiner Sperrigkeit wegen, die zahllose Assoziationen weckt und eines gewissen Rhythmus nicht entbehrt, existieren verschiedene Musiktitel mit dem Namen der Stadt:


Am 18. August 1950 gab der Minister für Industrie der DDR, Fritz Selbmann, mit den ersten Axtschlägen zum Fällen einer Kiefer den Start frei für den Bau des Eisenhüttenkombinats Ost (EKO).

Die Wirtschaft in Eisenhüttenstadt wird heute von der ArcelorMittal Eisenhüttenstadt GmbH dominiert. ArcelorMittal Eisenhüttenstadt ist ein integriertes Hüttenwerk und gehört zu ArcelorMittal, dem weltweit größten Stahlkonzern. Das aus dem "VEB Eisenhüttenkombinat Ost" bzw. der "EKO Stahl GmbH" hervorgegangene Unternehmen ist gegenwärtig das größte in Brandenburg.

Die kanadische 5N Plus eröffnete 2008 ein Werk in Eisenhüttenstadt.

Seit dem Frühjahr 2011 produziert die Firma Progroup AG, Wellpappen-Rohpapiere für die Verpackungsindustrie in Europa. Im Zuge der Ansiedelung der neuen Papierfabrik wurden auf dem Gelände ein neues Heizkraftwerk von der Firma EnBW Propower GmbH sowie eine neue Kläranlage des örtlichen Trink- und Abwasserzweckverbandes in Betrieb genommen.

Eisenhüttenstadt wird von der Bundesstraße 112 durchquert, die aufgrund des Ausbaus der Neiße-Trasse seit 2015 eine schnelle Verbindung nach Frankfurt(Oder) ermöglicht. In der Stadt beginnt die Bundesstraße 246 nach Beeskow. Die nächstgelegene Autobahnanschlussstelle ist "Frankfurt (Oder)-Mitte" an der A 12.

Zwar liegt Eisenhüttenstadt direkt an der polnischen Grenze, hat aber keinen direkten Grenzübergang. Die nächsten Grenzübergänge befinden sich in Coschen (15 km), Frankfurt (Oder) (25 km) und in Guben (30 km).

Der Bahnhof Eisenhüttenstadt liegt an der Bahnstrecke Frankfurt (Oder)–Cottbus und befindet sich im Stadtteil Fürstenberg. Stündlich gibt es Verbindungen der RB11 nach Cottbus bzw. Frankfurt(Oder), seltener auch direkte Verbindungen des RE1 nach Brandenburg Hbf bzw. Magdeburg Hbf.
Eisenhüttenstadt liegt an einer Bundeswasserstraße der Ausbauklasse III, der Oder-Spree-Kanal mündet hier in die Oder. Auf dem Wasserweg sind die Küsten der Nord- und Ostsee sowie viele europäische Metropolen zu erreichen. Die Stadt verfügt über mehrere Binnenhäfen mit Bahnanschluss und Straßenanbindung.

Der nächstgelegene Flughafen ist Berlin-Schönefeld. Ein Verkehrslandeplatz liegt am Nordwestrand der Stadt im zur Gemeinde Siehdichum gehörenden Pohlitz.

Heute existieren in Eisenhüttenstadt fünf Grundschulen, eine Gesamtschule mit gymnasialer Oberstufe und ein Gymnasium. Weiterhin gibt es ein Oberstufenzentrum mit angeschlossenem beruflichem Gymnasium, drei berufliche Schulen und Fachoberschulen, zwei Förderschulen und zwei weitere Weiterbildungseinrichtungen. Träger der Schulen sind die Stadt Eisenhüttenstadt, der Landkreis Oder-Spree und private Träger.

1991 entstand das durch den Stadtkreis Eisenhüttenstadt getragene Gymnasium als "Städtisches Gymnasium Eisenhüttenstadt." Mit Neubildung des Landkreises Oder-Spree wechselte 1993 die Trägerschaft. Am 30. Oktober 1996 erhielt die Schule den Namen "Albert-Schweitzer-Gymnasium". Die Namensgebung erfolgte im Beisein des damaligen Bundespräsidenten Roman Herzog und des Ministerpräsidenten Manfred Stolpe.
Seit Mai 2009 kann auf dem Schulgelände eine Albert-Schweitzer-Ausstellung besichtigt werden, die eine Dauerleihgabe des Niederlausitzer Albert-Schweitzer-Freundeskreises ist.

Das Oberstufenzentrum Oder-Spree mit über 3500 Auszubildenden und Schülern ist die größte Bildungseinrichtung im Landkreis Oder-Spree und betreibt den Außenstandort "Gottfried-Wilhelm-Leibnitz" in der Waldstraße 10. Die Einrichtung vereint Bildungsgänge der Berufsschule, der Berufsfachschule, der Fachoberschule und des beruflichen Gymnasiums. Ein wesentlicher Schwerpunkt der Bildungs- und Erziehungsarbeit ist die Pflege vielfältiger internationaler Beziehungen mit Partnerschulen unter anderem in Japan, Schweden, Frankreich, Holland, Dänemark und Polen.
Die Schule fusionierte 2012 mit dem OSZ Palmnicken in Fürstenwalde/Spree, wo die Schulleitung und das Sekretariat ihren Sitz haben.

In Eisenhüttenstadt erscheint als tägliche Regionalzeitung die Märkische Oderzeitung mit einem eigenen Lokalteil. Daneben werden die Anzeigenblätter "Märkischer Markt", "Märkischer Sonntag" und "Der Oderland-Spiegel" herausgegeben.

Außerdem wird in der Stadt mit dem "Oder-Spree-Fernsehen" (OSF) ein lokales Fernsehprogramm produziert, das in Eisenhüttenstadt, Neuzelle und Beeskow über Kabel zu empfangen ist.

In den Sportanlagen Wallstraße befindet sich das "Stadion der Hüttenwerker".

Mit dem Eisenhüttenstädter FC Stahl, dem FSV Dynamo Eisenhüttenstadt, der SG Aufbau Eisenhüttenstadt und dem 1. FC Fürstenberg stellte die Stadt bis zum 30. Juni 2016 vier Vereine im Bereich Fußball. Diese waren von der Brandenburg-Liga bis zur Kreisliga vertreten. Zum 1. Juli 2016 fusionierten der Eisenhüttenstädter FC Stahl, die SG Aufbau Eisenhüttenstadt und der 1.FC Fürstenberg und starteten als FC Eisenhüttenstadt den Spielbetrieb auf der 6. Spielebene (Brandenburg-Liga), der sie auch in der Saison 2017/2018 angehören.






</doc>
<doc id="1244" url="https://de.wikipedia.org/wiki?curid=1244" title="Einstellungsgröße">
Einstellungsgröße

Die Einstellungsgröße bezeichnet in der Filmkunst das Größenverhältnis des abgebildeten Subjekts/Objekts zur Cadrage, also dem vorgegebenen Bildfeld. Die Einstellungsgröße ergibt sich aus der Distanz der Kamera zum aufgenommenen Subjekt/Objekt und den gewählten Abbildungsparametern der Kamera.

Einstellungsgrößen finden neben der Filmkunst und der Fotografie auch in der Comic-Kunst Anwendung. Sie sind ein wichtiges Mittel bildlichen/filmischen Erzählens und können psychologische Akzente setzen. In der Regel werden die Einstellungsgrößen im Storyboard definiert.

Grundsätzlich gibt es viele verschiedene Bezeichnungen und Schemata zur Definition von Einstellungsgrößen. Die Grenzen zwischen den einzelnen Einstellungen sind nicht streng, und es gibt regionale Unterschiede (USA, Europa), wie auch persönliche Handschriften eines Regisseurs, Kameramanns oder Filmemachers. Je mehr Bildinhalt um den fokussierten Hauptinhalt herum zu sehen ist, desto totaler ist die Einstellung. Die Einstellungen werden oft in zwei Hauptgruppen unterteilt: Die "totalen Einstellungen" (engl. "long shots") und die "nahen Einstellungen" "(close-ups)". Im Folgenden sind die wichtigsten kurz vorgestellt: "Supertotale", "Totale", "Halbtotale", "Amerikanische" Einstellung, "Halbnahe", "Nahe", "Groß-", "Detailaufnahme", "Italienische" Einstellung.

Ein Wechsel der Einstellungsgröße bei laufender, stationärer Kamera wird als Zoom bezeichnet. Ein Wechsel kann aber auch durch eine Kamerafahrt erfolgen, was bei szenischen Produktionen wesentlich häufiger der Fall ist als ein Zoom. Da sich beim Zoom die Perspektive nicht verändert (der Kamerastandort bleibt gleich), wird diese Methode des Wechsels der Einstellungsgröße als flach und leblos angesehen. Sie entspricht auch nicht der alltäglichen Seherfahrung des Publikums, die eine Veränderung des Bildausschnittes instinktiv mit einer Bewegung des Betrachters verbindet. Eine Sonderform ist der Dolly-Zoom, eine Kamerafahrt mit gleichzeitig gegenläufigem Zoom. Die Einstellungsgröße bleibt bei einem sauber ausgeführten Dolly Zoom gleich, während die Veränderung in der Perspektive ein Schwindelgefühl vermittelt.

In der "Supertotalen", auch "Weite", "Panorama" oder "Weitwinkel-Ansicht" genannt, ist eine Landschaft der Bildinhalt. Menschen erscheinen darin verschwindend klein. Sie wird zum Beispiel für "Establishing Shots" eingesetzt, um das Geschehen in seine Umgebung eingebettet zu zeigen. Als psychologischer Akzent kann sie Gefühle wie Einsamkeit, Isolation, Fremdheit und/oder Gefahr, aber auch Freiheit und Unendlichkeit ausdrücken. Diese Einstellung eignet sich auch, um die Tiefe der Landschaft zu verdeutlichen, wenn Personen diese betreten oder Fahrzeuge hineinfahren.

Die Einstellung wird verwendet, wenn eine Person oder Gruppe vollständig in ihrer Umgebung, also "total" zu sehen ist, die Landschaft aber nicht den Hauptbildinhalt ausmacht. Der Mensch erscheint zwar größer als in der Supertotalen, aber immer noch relativ unwichtig. Die Totale wird häufig für "Establishing Shots" eingesetzt. In Filmen, die überwiegend oder nur aus Totalen bestehen, wirken die Akteure unnahbar. In Dokumentarfilmen sind totale Einstellungen häufiger als in Spielfilmen zu finden. Sie sind dafür da, um einen Überblick über das Geschehen zu geben.

Die Figuren werden von Kopf bis Fuß gezeigt. Diese Einstellungsgröße lässt sich gut für Menschengruppen einsetzen, oder für körperliche Aktionen, beispielsweise in der Slapstick-Comedy. In der Halbtotalen ist die Körpersprache oft wichtiger als der Dialog.

Der "Full Shot" ist eine besondere Form der halbtotalen Einstellung. Er zeigt nur die Personen und wenig oder gar nichts von deren Umgebung.

Die Figuren werden vom Kopf abwärts gezeigt bis zur Hüfte; man nimmt hier die breiteste Stelle der Hüfte als Anhaltspunkt, um die Person anzuschneiden; die unmittelbare Umgebung ist im Hintergrund erkennbar. Eine Sonderform ist die amerikanische Einstellung (american shot, AS), in der die Darsteller bis etwa zum Knie gezeigt werden. Diese Einstellungsgröße wurde oft im Western verwendet, um die Cowboys mitsamt ihrer Waffe zu zeigen.

Die Figur wird vom Kopf bis zur Hüfte gezeigt. Diese Einstellung entspricht der natürlichen Sehsituation und wird deswegen häufig in Dialogszenen verwendet.

Die halbnahe Einstellung wird aber auch in geschlossenen Räumen verwendet und in der deutschen Filmproduktion als "halbnaher Einer", "Zweier" oder "Dreier" usw. bezeichnet.

Die Figur wird vom Kopf bis zur Mitte des Oberkörpers gezeigt. Diese Einstellungsgröße kommt zum Beispiel in Gesprächsszenen zum Einsatz, wenn es auf die Mimik und Gestik ankommt. Auch sieht der Zuschauer in dieser Einstellung, wohin die Figur schaut, und kann daraus Schlüsse über den Fortgang der Handlung ziehen. In deutschen Filmproduktionen werden traditionell die Begriffe "nahe Einer" oder "Zweier" verwendet.

Der Kopf der Figur und ein Teil der Schultern werden abgebildet. Oft sind Teile des gefilmten Objekts (Hüte usw.) abgeschnitten. Die Mimik steht hier deutlich im Vordergrund. Die Einstellungsgröße kann verwendet werden, um Gefühle im Stadium ihrer Entstehung zu zeigen oder Handlungen, die nur mit den Händen vorgenommen werden.

Es wird nur ein Ausschnitt des Gesamtbildes gezeigt, beispielsweise nur die Augen oder der Mund eines Menschen oder andere wichtige Details der Szene, wie etwa die Worte, die auf einem Computer getippt werden.

Die Aufmerksamkeit des Zuschauers wird auf einen kleinen Bildausschnitt gelenkt. Die intensive Bildwirkung vermittelt Intimität oder erzeugt auch eine abstoßende Wirkung.

Besondere Art der Detailaufnahme, bei der nur die Augen des Protagonisten gezeigt werden. Ein bekanntes Beispiel ist in Sergio Leones Western "C'era una volta il West" "(Spiel mir das Lied vom Tod)" in den Szenen des Duells zwischen Henry Fonda und Charles Bronson zu sehen.

Jede Einstellungsform eignet sich für bestimmte Zwecke besonders gut. Eine wichtige Informationsquelle ist aber auch der Wechsel der Einstellungen. An ihr lassen sich stilistische Merkmale und auch die Handschrift des Regisseurs erkennen. Auch verschiedene Filmtraditionen und -kulturen verwenden traditionell sehr unterschiedliche Wechsel in den Einstellungen.

Actionfilme oder Dokumentarfilme weisen oft einen hohen Anteil "long shots" auf, weil für diese Filme Mimik und nonverbale Ausdrucksformen weniger wichtig sind, sondern gesamte Vorgänge eingefangen werden. Die erforderlichen schauspielerischen Leistungen sind bei diesen Filmtypen vergleichsweise gering. Auch quasidokumentarische Filme enthalten oft viele totale Einstellungen und verraten hiermit ihre Herkunft aus dem Dokumentarfilm. Filme, die Beziehungen der Figuren und das Gefühl in den Vordergrund stellen, enthalten viele Nahe Einstellungen.




</doc>
<doc id="1246" url="https://de.wikipedia.org/wiki?curid=1246" title="Erbium">
Erbium

Erbium ist ein chemisches Element mit dem Elementsymbol Er und der Ordnungszahl 68. Im Periodensystem steht es in der Gruppe der Lanthanoide und zählt damit auch zu den Metallen der Seltenen Erden. Der Name leitet sich von der Grube Ytterby bei Stockholm ab, wie auch der von Ytterbium, Terbium und Yttrium.

Erbium wurde 1843 von Carl Gustav Mosander entdeckt. Allerdings handelte es sich bei dem vermeintlich reinen Oxid um eine Mischung der Oxide aus Erbium, Scandium, Holmium, Thulium und Ytterbium.

Um die spätere Aufklärung machten sich die Chemiker Marc Delafontaine und Nils Johan Berlin verdient. Reines Erbiumoxid stellten 1905 der französische Chemiker Georges Urbain und der amerikanische Chemiker Charles James her.

Erbium ist ein seltenes Metall(3,5 ppm), das in der Natur nicht in reiner Form, sondern vor allem in dem Mineral Monazit vorkommt.

Nach einer aufwändigen Abtrennung der anderen Erbiumbegleiter wird das Oxid mit Fluorwasserstoff zum Erbiumfluorid umgesetzt. Anschließend wird mit Calcium unter Bildung von Calciumfluorid zum metallischen Erbium reduziert. Die Abtrennung verbleibender Calciumreste und Verunreinigungen erfolgt in einer zusätzlichen Umschmelzung im Vakuum.

Das silberweiß glänzende Metall der Seltenen Erden ist schmiedbar, aber auch ziemlich spröde.

In Luft läuft Erbium grau an, ist dann aber recht beständig. Bei höheren Temperaturen verbrennt es zum Sesquioxid ErO. Mit Wasser reagiert es unter Wasserstoffentwicklung zum Hydroxid. In Mineralsäuren löst es sich unter Bildung von Wasserstoff auf.

In seinen Verbindungen liegt es in der Oxidationsstufe +3 vor, die Er-Kationen bilden in Wasser rosafarbene Lösungen. Feste Salze sind ebenfalls rosa gefärbt.
Erbium-dotierte Lichtwellenleiter werden für optische Verstärker verwendet, die in der Lage sind, ein Lichtsignal zu verstärken, ohne es zuvor in ein elektrisches Signal zu wandeln. Gold als Wirtsmaterial dotiert mit einigen hundert ppm Erbium wird als Sensormaterial magnetischer Kalorimeter zur hochauflösenden Teilchendetektion in der Physik und Technik verwendet.

Erbium wird neben anderen Selten-Erd-Elementen wie Neodym oder Holmium zur Dotierung von Laserkristallen in Festkörperlasern eingesetzt (Er:YAG-Laser, siehe auch ). Der Er:YAG-Laser wird hauptsächlich in der Humanmedizin eingesetzt. Er hat eine Wellenlänge von 2940 nm und damit eine extrem hohe Absorption im Gewebewasser von ca. 12000 pro cm. 

Als reiner Beta-Strahler wird Er in der Nuklearmedizin zur Therapie bei der Radiosynoviorthese eingesetzt.

Viele seiner Verbindungen, wie Erbiumchlorid, sind rosa gefärbt und werden deshalb in der Töpferei und Glasbläserei eingesetzt.




</doc>
<doc id="1247" url="https://de.wikipedia.org/wiki?curid=1247" title="Einsteinium">
Einsteinium

Einsteinium ist ein ausschließlich künstlich erzeugtes chemisches Element mit dem Elementsymbol "Es" und der Ordnungszahl 99. Im Periodensystem steht es in der Gruppe der Actinoide (7. Periode, f-Block) und zählt zu den Transuranen. Einsteinium ist ein radioaktives Metall, welches in gerade noch wägbaren Mengen herstellbar ist. Es wurde 1952 nach dem Test der ersten amerikanischen Wasserstoffbombe entdeckt und Albert Einstein zu Ehren benannt, der jedoch persönlich mit der Entdeckung bzw. Forschung an Einsteinium nichts zu tun hatte. Es entsteht in sehr geringen Mengen in Kernreaktoren. Das Metall wie auch seine Verbindungen werden in geringen Mengen in erster Linie zu Studienzwecken gewonnen.

Einsteinium wurde zusammen mit Fermium nach dem Test der ersten amerikanischen Wasserstoffbombe, Ivy Mike, am 1. November 1952 auf dem Eniwetok-Atoll gefunden. Erste Proben erhielt man auf Filterpapieren, die man beim Durchfliegen durch die Explosionswolke mitführte. Größere Mengen isolierte man später aus Korallen. Aus Gründen der militärischen Geheimhaltung wurden die Ergebnisse zunächst nicht publiziert.

Eine erste Untersuchung der Explosionsüberreste hatte die Entstehung eines neuen Plutoniumisotops Pu aufgezeigt, dies konnte nur durch die Aufnahme von sechs Neutronen durch einen Uran-238-Kern und zwei folgende β-Zerfälle entstanden sein.

Zu der Zeit nahm man an, dass die Absorption von Neutronen durch einen schweren Kern ein seltener Vorgang wäre. Die Identifizierung von Pu ließ jedoch den Schluss zu, dass Urankerne viele Neutronen einfangen können, was zu neuen Elementen führt.

Die Trennung der gelösten Actinoid-Ionen erfolgte in Gegenwart eines Citronensäure/Ammoniumcitrat-Puffers im schwach sauren Medium (pH ≈ 3,5) mit Ionenaustauschern bei erhöhter Temperatur. Element 99 (Einsteinium) wurde schnell nachgewiesen; man fand zuerst das Isotop Es, einen hochenergetischen α-Strahler (6,6 MeV). Es entsteht durch Einfangen von 15 Neutronen aus U, gefolgt von sieben β-Zerfällen.

Diese Bildung durch fortgesetzten Neutroneneinfang war möglich, weil im Moment der Detonation die Neutronenflussdichte so hoch war, dass die meisten der zwischenzeitlich gebildeten – radioaktiven – Atomkerne bis zum jeweils nächsten Neutroneneinfang noch nicht zerfallen waren. Bei sehr hohem Neutronenfluss steigt also die Massenzahl stark an, ohne dass sich die Ordnungszahl ändert. Erst anschließend zerfallen die entstandenen instabilen Nuklide über viele β-Zerfälle zu stabilen oder instabilen Nukliden mit hoher Ordnungszahl:

Im September 1953 war noch nicht abzusehen, wann die Ergebnisse der Teams in Berkeley, Argonne und Los Alamos veröffentlicht werden könnten. Man entschied sich dazu, die neuen Elemente durch Beschussexperimente herzustellen; gleichzeitig versicherte man sich, dass diese Ergebnisse nicht unter Geheimhaltung fallen würden und somit veröffentlicht werden konnten. Einsteiniumisotope wurden kurz danach am University of California Radiation Laboratory durch Beschuss von Uran (U) mit Stickstoff (N) hergestellt. Dabei merkte man an, dass es Forschungen zu diesem Element gebe, die bislang noch unter Geheimhaltung stehen. Isotope der beiden neu entdeckten Elemente wurden durch Bestrahlung des Plutoniumisotops Pu erzeugt, die Ergebnisse wurden in fünf kurz aufeinander folgenden Publikationen veröffentlicht. Die letzten Reaktionen ausgehend von Californium sind:

Das Team in Berkeley war zudem besorgt, dass eine andere Forschergruppe die leichteren Isotope des Elements 100 durch Ionenbeschuss entdecken und veröffentlichen könnte, bevor sie ihre unter Geheimhaltung stehende Forschung hätten veröffentlichen können. Denn im ausgehenden Jahr 1953 sowie zu Anfang des Jahres 1954 beschoss eine Arbeitsgruppe des Nobel-Instituts für Physik in Stockholm Urankerne mit Sauerstoffkernen; es bildete sich das Isotop mit der Massenzahl 250 des Elements 100 (Fm).

Das Team in Berkeley veröffentlichte schon einige Ergebnisse der chemischen Eigenschaften beider Elemente. Schließlich wurden die Ergebnisse der thermonuklearen Explosion im Jahr 1955 freigegeben und anschließend publiziert.

Letztlich war die Priorität des Berkeley-Teams allgemein anerkannt, da ihre fünf Publikationen der schwedischen Publikation vorausgingen, und sie sich auf die zuvor noch geheimen Ergebnisse der thermonuklearen Explosion von 1952 stützen konnten. Damit war das Vorrecht verbunden, den neuen Elementen den Namen zu geben. Sie entschieden sich, diese fortan nach berühmten, bereits verstorbenen Wissenschaftlern zu benennen. Man war sich schnell einig, die Namen zu Ehren von Albert Einstein und Enrico Fermi zu vergeben, die beide erst vor kurzem verstorben waren: „We suggest for the name for the element with the atomic number 99, einsteinium (symbol E) after Albert Einstein and for the name for the element with atomic number 100, fermium (symbol Fm), after Enrico Fermi.“ Die Bekanntgabe für die beiden neu entdeckten Elemente "Einsteinium" und "Fermium" erfolgte durch Albert Ghiorso auf der 1. Genfer Atomkonferenz, die vom 8. bis 20. August 1955 stattfand. Das Elementsymbol für Einsteinium wurde später von "E" auf "Es" geändert.

Sämtliche bisher bekannten 17 Nuklide und 3 Kernisomere sind radioaktiv und instabil. Die bekannten Massenzahlen reichen von 241 bis 258. Die längste Halbwertszeit hat das Isotop Es mit 471,7 Tagen, so dass es auf der Erde keine natürlichen Vorkommen mehr geben kann. Es hat eine Halbwertzeit von 275,7 Tagen, Es von 39,8 Tagen und Es von 20,47 Tagen. Alle weiteren radioaktiven Isotope haben Halbwertszeiten unterhalb von 40 Stunden, bei der Mehrzahl von ihnen liegt diese unter 30 Minuten. Von den 3 Kernisomeren ist Es das stabilste mit t = 39,3 Stunden.

"→ Liste der Einsteiniumisotope"

Einsteinium wird durch Beschuss von leichteren Actinoiden mit Neutronen in einem Kernreaktor erzeugt. Die Hauptquelle ist der 85 MW High-Flux-Isotope Reactor am Oak Ridge National Laboratory in Tennessee, USA, der auf die Herstellung von Transcuriumelementen (Z > 96) eingerichtet ist.

Im Jahr 1961 wurde genügend Einsteinium synthetisiert, um eine wägbare Menge des Isotops Es zu erhalten. Diese Probe wog etwa 0,01 mg und wurde zur Herstellung von Mendelevium eingesetzt. Weiteres Einsteinium wurde am Oak Ridge National Laboratory durch Beschuss von Pu mit Neutronen hergestellt. Ungefähr 3 Milligramm wurden in einer vierjährigen Dauerbestrahlung aus einem Kilogramm Plutonium und anschließender Trennung erhalten.

Geringe Mengen an Einsteinium und Fermium wurden aus Plutonium isoliert und abgetrennt, welches mit Neutronen bestrahlt wurde. Vier Einsteiniumisotope wurden gefunden (mit Angabe der damals gemessenen Halbwertszeiten):
Zwei Fermiumisotope wurden gefunden:

Durch Beschuss von Uran mit fünffach ionisierten Stickstoff- und sechsfach ionisierten Sauerstoffatomen wurden gleichfalls Einsteinium- und Fermiumisotope erzeugt.

Das Isotop Es wurde beim Beschuss von Cf mit Deuterium identifiziert. Es zerfällt hauptsächlich durch Elektroneneinfang (ε) mit einer Halbwertszeit von 25 ± 5 Minuten aber auch durch die Aussendung von α-Teilchen (6,87 ± 0,02 MeV). Das Verhältnis (ε / α) von ∼ 400 konnte durch die Menge des durch Elektroneneinfang entstandenen Cf identifiziert werden.

Die Isotope Es, Es, Es und Es wurden durch Beschuss von Bk mit α-Teilchen erzeugt. Dabei können 4 bis 1 Neutronen den Kern verlassen, so dass die Bildung von vier unterschiedlichen Isotopen möglich ist.

Obwohl das Isotop Es die längste Halbwertszeit ausweist, ist das Isotop Es leichter zugänglich und wird überwiegend für die Bestimmung der chemischen Eigenschaften herangezogen. Es wurde durch Bestrahlung von 100 bis 200 μg Cf mit thermischen Neutronen erhalten (Flussdichte: 2 bis 5×10 Neutronen × cm s, Zeitraum: 500 bis 900 h). Zur Trennung wurde Ammonium-α-hydroxyisobutyrat verwendet.

Einsteinium erhält man durch Reduktion von Einsteinium(III)-fluorid mit Lithium oder Einsteinium(III)-oxid mit Lanthan.

Im Periodensystem steht das Einsteinium mit der Ordnungszahl 99 in der Reihe der Actinoide, sein Vorgänger ist das Californium, das nachfolgende Element ist das Fermium. Sein Analogon in der Reihe der Lanthanoide ist das Holmium.

Einsteinium ist ein radioaktives Metall mit einem Schmelzpunkt von 860 °C, einem Siedepunkt von 996 °C und einer Dichte von 8,84 g/cm. Es kristallisiert im kubischen Kristallsystem in der mit dem Gitterparameter "a" = 575 pm, was einem kubisch flächenzentrierten Gitter (f.c.c.) beziehungsweise einer kubisch dichtesten Kugelpackung mit der Stapelfolge ABC entspricht. Die Radioaktivität ist derart stark, dass dadurch das Metallgitter zerstört wird. Das Metall ist divalent und besitzt eine merklich hohe Flüchtigkeit.

Einsteinium ist wie alle Actinoide sehr reaktionsfähig. In wässriger Lösung ist die dreiwertige Oxidationsstufe am beständigsten, jedoch kennt man auch zwei- und vierwertige Verbindungen. Zweiwertige Verbindungen konnten bereits als Feststoffe dargestellt werden; der vierwertige Zustand konnte bereits beim chemischen Transport in Tracermengen postuliert werden, eine endgültige Bestätigung steht aber noch aus. Wässrige Lösungen mit Es-Ionen haben eine blassrosa Farbe.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.

Einsteinium findet vor allem Anwendung bei der Erzeugung höherer Transurane und Transactinoide. Ansonsten werden das Metall wie auch seine Verbindungen in erster Linie in geringen Mengen zu Studienzwecken gewonnen.

"→ Kategorie: "

Die Untersuchung von Einsteiniumverbindungen ist durch mehrere Faktoren begrenzt:

Einsteinium(III)-oxid (EsO) wurde durch Glühen des entsprechenden Nitrats in Submikrogramm-Mengen erhalten. Der Gitterparameter des kubisch-raumzentrierten Kristalls beträgt 1076,6(6) pm. Es sind ferner noch eine monokline und eine hexagonale Lanthan(III)-oxid-Struktur bekannt.

Bekannt sind die Oxihalogenide Einsteinium(III)-oxichlorid (EsOCl), Einsteinium(III)-oxibromid (EsOBr) und Einsteinium(III)-oxiiodid (EsOI). Einsteinium(III)-oxichlorid besitzt eine tetragonale Struktur vom PbFCl-Typ.

Halogenide sind für die Oxidationsstufen +2 und +3 bekannt. Die stabilste Stufe +3 ist für sämtliche Verbindungen von Fluor bis Iod bekannt und auch in wässriger Lösung stabil.

Einsteinium(III)-fluorid (EsF) kann durch Ausfällung aus Einsteinium(III)-chlorid-Lösungen mit Fluorid dargestellt werden, als auch aus Einsteinium(III)-oxid durch Umsetzung mit ClF oder F bei 1–2 Atmosphären Druck und 300–400 °C. Die Kristallstruktur konnte nicht bestimmt werden, es wird aber davon ausgegangen, dass sie wie bei Berkelium(III)-fluorid (BkF) und Californium(III)-fluorid (CfF) dem LaF-Typ entspricht.

Einsteinium(III)-chlorid (EsCl) ist ein orangefarbener Feststoff und bildet eine hexagonale Struktur vom UCl-Typ, wobei das Es-Atom 9-fach koordiniert ist.

Einsteinium(III)-bromid (EsBr) ist ein weißgelber Feststoff und bildet eine monokline Struktur vom AlCl-Typ.

Einsteinium(III)-iodid (EsI) ist ein bernsteinfarbener Feststoff und bildet eine hexagonale Struktur vom BiI-Typ.

Die zweiwertigen Verbindungen des Einsteiniums werden durch Reduktion der dreiwertigen Halogenide mit Wasserstoff dargestellt.

Von Einsteinium(II)-chlorid (EsCl), Einsteinium(II)-bromid (EsBr) und Einsteinium(II)-iodid (EsI) sind keine näheren kristallographischen Daten bekannt, wohl aber Messwerte von Absorptionsbanden.




</doc>
<doc id="1248" url="https://de.wikipedia.org/wiki?curid=1248" title="Europium">
Europium

Europium ist ein chemisches Element mit dem Elementsymbol Eu und der Ordnungszahl 63. Im Periodensystem steht es in der Gruppe der Lanthanoide und zählt damit auch zu den Metallen der Seltenen Erden. Nur Europium und Americium sind nach einem Erdteil benannte Elemente.

Wie die anderen Lanthanoide ist Europium ein silberglänzendes Schwermetall. Die Eigenschaften des Europiums folgen nicht der Lanthanoidenkontraktion. Aufgrund seiner Elektronenkonfiguration weist das Element eine deutlich geringere Dichte sowie einen niedrigeren Schmelz- und Siedepunkt auf als die benachbarten Elemente. Es ist das chemisch reaktivste Seltenerdmetall. Nach ersten Hinweisen auf das Element durch William Crookes und Paul Émile Lecoq de Boisbaudran konnte 1896 Eugène-Anatole Demarçay das Element zunächst spektroskopisch nachweisen und dann isolieren.

Europium hat eine hohe technische Bedeutung in Leuchtstoffen, wie sie etwa in Kathodenstrahlröhrenbildschirmen, welche früher für Computermonitore und Fernseher verwendet wurden, in Leuchtstofflampen sowie in Plasmabildschirmen eingesetzt werden. Sowohl der rote als auch der blaue Leuchtstoff in diesen Bildschirmen und Leuchtmitteln sind Substanzen, die mit Europium dotiert sind und dadurch Fluoreszenz in dem entsprechenden Spektralbereich zeigen.

Einen ersten Hinweis auf das später Europium genannte Element fand 1885 William Crookes. Bei der Untersuchung von Fluoreszenzspektren von Samarium-Yttrium-Mischungen konnte er Signale einer ungewöhnlichen orangefarbenen Spektrallinie messen, die in Mischungen der Elemente stärker war als in den reinen Stoffen. Diese auf ein unbekanntes Element hindeutende Spektrallinie nannte er „anormale Linie“, das hypothetische Element S. Eine weitere Entdeckung auf dem Weg zum unbekannten Element machte 1892 Paul Émile Lecoq de Boisbaudran, als er im Funkenspektrum von Samarium neben der anormalen Linie Crookes auch drei bislang unbekannte blaue Spektrallinien entdeckte. 1896 postulierte Eugène-Anatole Demarçay anhand von Ultraviolett-Spektren die Existenz eines bislang unbekannten Elements zwischen Samarium und Gadolinium, wobei er im Jahr 1900 erkannte, dass dieses Element gleich dem von Crookes und Boisbaudran vermuteten sein muss. 1901 gelang es Demarçay, dieses durch fraktionierte Kristallisation der Samarium/Europium-Magnesium-Nitrat-Doppelsalze zu isolieren. Er nannte das Element nach dem Kontinent Europa "Europium". In Analogie zum Europium benannten Glenn T. Seaborg, Ralph A. James und Leon O. Morgan 1948 das sich im Periodensystem direkt unter dem Europium befindende Actinoid ebenfalls nach einem Kontinent "Americium".

Die erste wichtige technische Anwendung des Elements war die Produktion von mit Europium dotiertem Yttriumvanadat. Dieser 1964 von Albert K. Levine und Frank C. Palilla entdeckte rote Leuchtstoff spielte bald eine wichtige Rolle bei der Entwicklung des Farbfernsehens. Für diese Anwendung wurde daraufhin das erste Bergwerk für die Gewinnung von Seltenen Erden, das seit 1954 im kalifornischen Mountain Pass betrieben wurde, stark ausgebaut.

Europium ist auf der Erde ein seltenes Element, die Häufigkeit in der kontinentalen Erdkruste beträgt etwa 2 ppm.

Europium kommt als Nebenbestandteil in verschiedenen Lanthanoid-Mineralen vor, Minerale mit Europium als Hauptbestandteil sind unbekannt. Das Element ist sowohl in Ceriterden wie Monazit und Bastnäsit als auch in Yttererden wie Xenotim enthalten, der Anteil an Europium beträgt in der Regel zwischen 0,1 und 0,2 %. Das für die Gewinnung von Europium wichtigste Vorkommen war bis 1985 das Bastnäsiterz in Mountain Pass, Kalifornien, danach gewannen chinesische Bergwerke – vor allem das Erzvorkommen in Bayan Obo – große Bedeutung.

In manchen magmatischen Gesteinen ist die Konzentration an Europium höher oder geringer, als nach dem mit Chondriten als Standard bestimmten relativen Häufigkeitsverhältnis der Seltenerdmetalle zu erwarten wäre. Dieses Phänomen wird als Europiumanomalie bezeichnet und beruht darauf, dass unter reduzierenden Bedingungen in Magma Eu zu Eu reduziert werden kann. Dieses besitzt einen größeren Ionenradius als dreiwertiges Europium und wird darum leicht in bestimmte Minerale, etwa an Stelle von Strontium oder Calcium in Kalifeldspat und Plagioklas eingebaut, welche dadurch eine "positive" Europiumanomalie aufweisen. Diese Minerale kristallisieren aus der Magmaschmelze und werden dadurch abgetrennt, während dreiwertiges Europium in der Restschmelze gelöst bleibt. Für den Einbau in mafische Gesteine wie Pyroxen und Olivin anstelle von Eisen, Magnesium und Calcium ist das Eu-Ion dagegen zu groß und es kommt zu einer "negativen" Europiumanomalie. Außer durch Kristallisation von Plagioklas kann eine Europiumanomalie auch beim Aufschmelzen von Gesteinen entstehen. Da der Verteilungskoeffizient zwischen Kristall und Schmelze etwa 10-fach größer als für die anderen Seltenerdelemente ist, wird beim teilweisen Aufschmelzen eines Plagioklas-reichen Gesteins nur wenig Europium in die Schmelze abgegeben und es resultiert beim Wiedererstarren ein Gestein mit negativer Europiumanomalie. Die Europiumanomalie ist ein Indikator für den Fraktionierungsgrad eines magmatischen Gesteins.

Eine ausgeprägte Europiumanomalie wurde in Mondgestein gefunden, wobei die Plagioklas-reichen Felsen des Mondhochlandes eine positive (erhöhte Europiumgehalte), die in Kratern und Maria gefundenen Basaltgesteine eine negative Europiumanomalie aufweisen. Dies lässt Rückschlüsse auf die geologische Geschichte des Mondes zu. Dabei wird angenommen, dass die Hochländer mit ihren Anorthositen vor etwa 4,6–4,4 Milliarden Jahren aus dem Mondmantel differenzierten und dieser somit aus Europium-verarmten Olivin-Pyroxen-Gesteinen besteht. Die jüngeren Basalte in den Maria, die aus basaltischen Teilschmelzen dieses Mantels bestehen, sind darum so arm an Europium.

Aufgrund der Ähnlichkeit zu den Begleitmetallen und der geringen Konzentration in den Erzen ist die Abtrennung von den anderen Lanthanoiden schwierig, gleichzeitig aber wegen der Verwendung des Elements technisch besonders wichtig. Nach dem Aufschluss der Ausgangsmaterialien wie Monazit oder Bastnäsit mit Schwefelsäure oder Natronlauge sind verschiedene Wege zur Abtrennung möglich. Neben dem Ionenaustausch wird vor allem ein Verfahren eingesetzt, das auf Flüssig-Flüssig-Extraktion und der Reduktion von Eu zu Eu beruht. Dabei wird bei Bastnäsit als Ausgangsmaterial zunächst das Cer in Form von Cer(IV)-oxid abgetrennt und die verbleibenden Seltenen Erden in Salzsäure gelöst. Daraufhin werden mit Hilfe einer Mischung von DEHPA (Di(2-ethylhexyl)phosphorsäure) und Kerosin in Flüssig-Flüssig-Extraktion Europium, Gadolinium und Samarium von den übrigen Seltenerdmetallen getrennt. Die Trennung dieser drei Elemente erfolgt über die Reduktion des Europiums zu Eu und Fällung als schwerlösliches Europium(II)-sulfat, während die anderen Ionen in Lösung bleiben.

Metallisches Europium kann durch Reaktion von Europium(III)-oxid mit Lanthan oder Mischmetall gewonnen werden. Wird diese Reaktion im Vakuum durchgeführt, destilliert Europium ab und kann so von anderen Metallen und Verunreinigungen getrennt werden:

2010 wurden etwa 600 Tonnen Europium produziert und 500 Tonnen verbraucht (jeweils gerechnet als Europiumoxid). Durch den steigenden Bedarf an Europium ist jedoch zu befürchten, dass mittelfristig die Nachfrage das Angebot übersteigt und es zu einer Verknappung kommen wird. Daher wird an einer Ausweitung der Europiumproduktion, insbesondere durch Eröffnung weiterer Minen wie der im australischen Mount Weld und einer Wiedereröffnung der "Mountain Pass Mine" gearbeitet. Durch die hohe Nachfrage nach Europium ist auch der Preis des Elements stark gestiegen. Lag er 2002 noch bei 240 US-Dollar pro Kilogramm, stieg er 2011 auf bis zu 1830 Dollar pro Kilogramm (jeweils 99 % Reinheit).

Europium ist wie die anderen Lanthanoide ein silberglänzendes weiches Schwermetall. Es besitzt mit 5,245 g/cm eine ungewöhnlich niedrige Dichte, die deutlich niedriger als diejenige der benachbarten Lanthanoide wie Samarium oder Gadolinium und geringer als die des Lanthans ist. Vergleichbares gilt auch für den verhältnismäßig niedrigen Schmelzpunkt von 826 °C und den Siedepunkt von 1440 °C (Gadolinium: Schmelzpunkt 1312 °C, Siedepunkt 3000 °C). Diese Werte stehen der sonst geltenden Lanthanoidenkontraktion entgegen und werden durch die Elektronenkonfiguration [Xe] 4f 6s des Europiums verursacht. Durch die halb gefüllte f-Schale stehen nur die zwei Valenzelektronen (6s) für metallische Bindungen zur Verfügung; es kommt daher zu geringeren Bindungskräften und zu einem deutlich größeren Metallatomradius. Vergleichbares ist auch bei Ytterbium zu beobachten. Bei diesem Element stehen durch eine vollständig gefüllte f-Schale ebenfalls nur zwei Valenzelektronen für metallische Bindungen zur Verfügung.

Europium kristallisiert unter Normalbedingungen in einem kubisch-raumzentrierten Gitter mit dem Gitterparameter a = 455 pm. Neben dieser Struktur sind noch zwei weitere Hochdruckmodifikationen bekannt. Dabei entspricht die Reihenfolge der Modifikationen bei steigendem Druck wie bei Ytterbium nicht derjenigen der übrigen Lanthanoide. So ist weder eine Europiummodifikation in doppelt-hexagonaler Struktur noch in Samarium-Struktur bekannt. Der erste Phasenübergang im Metall findet bei 12,5 GPa statt, oberhalb dieses Druckes kristallisiert Europium in einer hexagonal-dichtesten Struktur mit den Gitterparametern a = 241 pm und c = 545 pm. Oberhalb von 18 GPa wurde mit Eu-III eine weitere, der hexagonal-dichtesten Kugelpackung ähnliche Struktur gefunden.

Bei hohen Drücken von mindestens 34 GPa ändert sich die Elektronenkonfiguration des Europiums im Metall von zwei- auf dreiwertig. Dies ermöglicht auch eine Supraleitfähigkeit des Elements, die bei einem Druck von etwa 80 GPa und einer Temperatur von etwa 1,8 K auftritt.

Europiumionen, die in geeignete Wirtsgitter eingebaut sind, zeigen eine ausgeprägte Fluoreszenz. Dabei ist die abgestrahlte Wellenlänge von der Oxidationsstufe abhängig. Eu fluoresziert weitgehend unabhängig vom Wirtsgitter zwischen 613 und 618 nm, was einer intensiv roten Farbe entspricht. Das Maximum der Emission von Eu ist dagegen stärker vom Wirtsgitter abhängig und liegt beispielsweise bei Bariummagnesiumaluminat mit 447 nm im blauen, bei Strontiumaluminat (SrAlO:Eu) mit 520 nm im grünen Spektralbereich.

Europium ist ein typisches unedles Metall und reagiert mit den meisten Nichtmetallen. Es ist das reaktivste der Lanthanoide und reagiert schnell mit Sauerstoff. Wird es auf etwa 180 °C erhitzt, entzündet es sich an der Luft spontan und verbrennt zu Europium(III)-oxid.
Auch mit den Halogenen Fluor, Chlor, Brom und Iod reagiert Europium zu den Trihalogeniden. Bei der Reaktion mit Wasserstoff bilden sich nichtstöchiometrische Hydridphasen, wobei der Wasserstoff in die Lücken der Kugelpackung des Metalls eintritt.

Europium löst sich in Wasser langsam, in Säuren schnell unter Bildung von Wasserstoff und des farblosen Eu-Iones. Das ebenfalls farblose Eu-Ion lässt sich durch elektrolytische Reduktion an Kathoden in wässriger Lösung gewinnen. Es ist das einzige zweiwertige Lanthanoid-Ion, das in wässriger Lösung stabil ist. Europium löst sich in Ammoniak, wobei sich wie bei Alkalimetallen eine blaue Lösung bildet, in der solvatisierte Elektronen vorliegen.

Das Eu-Kation gehört neben u. a. Sm, Tb und Dy zu den Lanthanoid-Kationen, die in einem geeigneten Komplex bei Absorption bestimmter Wellenlängen Licht im sichtbaren Bereich emittieren kann. Das dreiwertige Europium-Kation ist in einer wässrigen Lösung farblos, werden aber organische Liganden mit einem ausgedehnten π-Elektronensystem koordiniert sorgt der "Antennen Effekt" dafür, dass die lumineszenten Eigenschaften des Zentralteilchens stark steigen. So leiten die π-Elektronen des Ligandens die absorbierte Energie des einfallenden Lichtes (ca. 355 nm) zu den 5d-Elektronen des Eu, wodurch diese in das 4f-Orbital gelangen und beim Zurückfallen Licht im sichtbaren Bereich (bei ca. 610 nm) emittieren.

Es sind insgesamt 38 Isotope und weitere 13 Kernisomere des Europiums zwischen Eu und Eu bekannt. Von diesen ist eines, Eu, stabil, ein weiteres, Eu, galt lange Zeit als stabil; es wurden 2007 jedoch Hinweise darauf gefunden, dass es mit einer Halbwertszeit von mindestens 1,7 Trillionen Jahren als Alphastrahler zerfällt. Diese beiden Isotope kommen in der Natur vor, wobei Eu mit einem Anteil von 52,2 % an der natürlichen Isotopenzusammensetzung das häufigere ist, der Anteil an Eu beträgt dementsprechend 47,8 %.

Mehrere Europiumisotope wie Eu, Eu und Eu entstehen bei Kernspaltungen von Uran und Plutonium. Dabei ist Eu mit einem Anteil von etwa 0,03 % an der Gesamtmenge der Spaltprodukte das häufigste Europiumisotop unter den Spaltprodukten. Es konnte unter anderem im Rongelap-Atoll drei Jahre nach der Kontaminierung durch den Castle-Bravo-Atomwaffentest nachgewiesen werden.

Europium wird vor allem als Dotierungsmittel für die Produktion von Leuchtstoffen eingesetzt, die etwa in Kathodenstrahlröhrenbildschirmen, welche früher hauptsächlich für Computerbildschirme und Fernseher verwendet wurden sowie für Flugzeuginstrumente benötigt werden, und in Kompaktleuchtstofflampen Verwendung finden. Es werden Leuchtstoffe sowohl mit zwei- als auch dreiwertigem Europium für verschiedene Farben verwendet. Für rote Leuchtstoffe wird vor allem mit Europium dotiertes Yttriumoxid (YO:Eu), früher wurden auch Yttriumoxysulfid oder als erster wichtiger roter Leuchtstoff Yttriumvanadat:Eu genutzt. Eu wird meist als blauer Leuchtstoff in Verbindungen wie Strontiumchlorophosphat (Sr(PO)Cl:Eu, Strontiumchloroapatit SCAP) und Bariummagnesiumaluminat (BaMgAlO:Eu, BAM) eingesetzt. Plasmabildschirme erfordern Leuchtstoffe, die die vom Edelgas-Plasma emittierte VUV-Strahlung in sichtbares Licht umwandeln. Hierfür werden sowohl für das blaue als auch rote Spektrum europiumdotierte Leuchtstoffe genutzt – für blaues Licht BAM, für rotes (Y,Gd)BO:Eu.

In Quecksilberhochdrucklampen, die etwa in der Straßenbeleuchtung eingesetzt werden, wird europiumdotiertes Yttriumvanadat auf das Glas aufgebracht, damit das Licht weiß und natürlicher erscheint.

Europium kann auf Grund seiner Neutronenabsorption in Steuerstäben für Kernreaktoren verwendet werden. Europiumhaltige Steuerstäbe wurden unter anderem in verschiedenen sowjetischen Versuchsreaktoren wie BOR-60 und BN-600 erprobt.

Bei Euro-Banknoten wird die Europium-Fluoreszenz gegen Fälschungen verwendet.

Diese Eigenschaft kann auch in der Fluoreszenzspektroskopie ausgenutzt werden. Dazu wird das Europium beispielsweise in einem geeigneten Komplex gebunden, der an der gewünschten Stelle, etwa mit einem bestimmten Protein, bevorzugt reagiert und sich dort anreichert.

Europium kommt nur in minimalen Mengen im Körper vor und hat keine biologische Bedeutung. Auch durch Pflanzenwurzeln kann das Element nicht aufgenommen werden.

Lösliche Europiumverbindungen sind leicht giftig; so wurde für Europium(III)-chlorid ein LD-Wert von 550 mg/kg für intraperitoneale und 5000 mg/kg für orale Gabe an Mäusen ermittelt. Es konnte keine chronische Toxizität festgestellt werden, was möglicherweise mit der geringen Aufnahme von Europium im Darm und der schnellen Umwandlung von löslichem Europiumchlorid zu unlöslichem Europiumoxid unter basischen Bedingungen zusammenhängt. Unlösliche Europiumverbindungen gelten als weitgehend ungiftig, wie in einer Studie mit Europium(III)-hydroxid-Nanopartikeln an Mäusen ermittelt wurde.

Bei Europium(III)-hydroxid-Nanopartikeln (nicht jedoch bei amorphem Europium(III)-hydroxid) wurde eine pro-angiogenetische Wirkung festgestellt, sie fördern "in vitro" die Zellproliferation von Endothelzellen, "in vivo" an Hühnereiern wurde eine vermehrte Bildung von kleinen Blutgefäßen beobachtet. Ein möglicher Mechanismus für diese Beobachtung ist die Bildung von reaktiven Sauerstoffspezies und die Aktivierung von MAP-Kinasen durch diese Nanopartikel.

Es sind Verbindungen in den Oxidationsstufen +2 und +3 bekannt, wobei wie bei allen Lanthanoiden zwar die dreiwertige Stufe die stabilere, die zweiwertige jedoch ebenfalls ungewöhnlich stabil ist und daher eine Vielzahl von Eu(II)-Verbindungen existieren. Die Ionenradien unterscheiden sich je nach Oxidationsstufe, wobei Eu-Ionen größer als Eu-Ionen sind. Mit der Koordinationszahl sechs betragen sie 131 pm für Eu und 108,7 pm für Eu. Der "effektive Ionenradius" (der als Bezugsgröße ein mit 140 pm um 14 pm größeres O-Ion verwendet) beträgt dementsprechend 117 pm bzw. 94,7 pm für die Koordinationszahl sechs. In höheren Koordinationszahlen sind die Ionenradien größer, so beträgt er für Eu in der Koordinationszahl acht 139 pm.

Europium(III)-oxid, EuO, ist die technisch wichtigste Europiumverbindung und dient als Ausgangsmaterial zur Herstellung anderer Europiumverbindungen sowie als Dotierungsmittel für Fluoreszenzfarbstoffe wie YO:Eu, das eine besonders intensive rote Fluoreszenz bei einem Europium(III)-oxid-Gehalt von etwa 10 % zeigt. Es kristallisiert wie die anderen Lanthanoidoxide in der kubischen Lanthanoid-C-Struktur.

Europium(II)-oxid, EuO, ist ein violett-schwarzer ferromagnetischer Feststoff mit einer Curie-Temperatur von 70 K, der in einer Natriumchlorid-Struktur kristallisiert. Es lässt sich durch Reduktion von Europium(III)-oxid mit Europium gewinnen und ist das einzige zweiwertige Oxid der Lanthanoide, das unter Normalbedingungen stabil ist. Neben diesen beiden Oxiden ist auch das gemischtvalente Oxid Europium(II,III)-oxid, EuO, bekannt.

Ähnliche Eigenschaften wie EuO haben auch die Eu-Chalkogenide (also -Sulfide, -Selenide und -Telluride) sowie ihre ungeordneten Legierungen. EuSrS ist z. B. für x=0 ein Ferromagnet, der für formula_3 zu einem isolierenden Spinglas wird, das u. a. wegen seines nichtmetallischen Verhaltens für Computersimulationen besonders geeignet ist.

Mit den Halogenen Fluor, Chlor, Brom und Iod reagiert Europium zu den Trihalogeniden. Diese zersetzen sich beim Erhitzen zu den Dihalogeniden und elementaren Halogenen.

Europium bildet metallorganische Verbindungen. Anders als bei den anderen Lanthanoiden lässt sich aber keine Cyclopentadienylverbindung des dreiwertigen Europiums synthetisieren. Bekannt ist zwar eine Verbindung, die neben drei Molekülen Cyclopentadienyl zusätzlich ein Molekül Tetrahydrofuran enthält, dieses ist jedoch stark an das Europium gebunden und lässt sich durch Erhitzen oder im Vakuum nicht entfernen, da die Verbindung sich vorher zersetzt. Dagegen sind das Europiumdicyclopentadienyl (Cp)Eu(II) und weitere bekannte Derivate stabil. Vom zweiwertigen Europium sind auch Alkinyl-Europium-Verbindungen bekannt.

Einen Überblick über Europiumverbindungen bietet die .




</doc>
<doc id="1249" url="https://de.wikipedia.org/wiki?curid=1249" title="Eisen">
Eisen

Eisen (von ahd. "īsa(r)n"; aus urgerm. *"īsarnan", wie gall. "īsarnon" wahrscheinlich entlehnt aus dem Illyrischen und in Bezug auf das im Gegensatz zur weicheren Bronze starke, kräftige Metall verwandt mit lateinisch "ira", ‚Zorn, Heftigkeit‘) ist ein chemisches Element mit dem Elementsymbol Fe (, ‚Eisen‘) und der Ordnungszahl 26. Es zählt zu den Übergangsmetallen, im Periodensystem steht es in der 8. Nebengruppe (Eisen-Platin-Gruppe), nach der neuen Zählung in der Gruppe 8 oder Eisengruppe. Es ist, auf den Massenanteil (ppmw) bezogen, nach Sauerstoff, Silicium und Aluminium das vierthäufigste Element in der Erdkruste und nach Aluminium das häufigste Metall.

Moderne Eisenwerkstoffe mit einem Massenanteil des Kohlenstoffs von bis zu 2 % werden als Stahl bezeichnet, bei größerem Gehalt als Gusseisen. Die Unterscheidung beruht darauf, dass Gusseisen nicht plastisch verformbar, insbesondere nicht schmiedbar ist, während Stahl verformbar, also schmiedbar ist. Ältere Werkstoffe (vor etwa 1870) mit geringen Kohlenstoffgehalten werden als Schmiedeeisen bezeichnet und weisen größere Verunreinigungen auf, da sie anders hergestellt wurden als moderner Stahl.

Belege für die Nutzung von Eisen in den verschiedenen Kulturen durch archäologische Funde sind gegenüber den Funden von Bronze relativ selten. Einerseits wurde Eisen in den ältesten Perioden der Geschichte nur in geringem Umfang genutzt, andererseits neigt Eisen an feuchter Luft, im Wasser und in der nassen Erde zur Korrosion, wodurch viele Gegenstände nicht erhalten blieben. Nur besondere Umstände oder große Ausmaße des Gegenstandes verhinderten den Verlust solcher Stücke.

Bevor die Menschen in den verschiedenen Kulturkreisen lernten, Eisen aus Erz zu gewinnen, nutzten sie das bereits vor der eigentlichen „Eisenzeit“ bekannte und an seinem spezifischen Nickelgehalt von etwa 5 bis 18 % erkennbare Meteoreisen oder auch "Meteoriteneisen". Aufgrund seiner Seltenheit war dieses „Himmelseisen“ (altägyptisch: bj-n-pt = „Eisen des Himmels“) entsprechend wertvoll und wurde vorwiegend zu Kultgegenständen und Schmuck verarbeitet. So fand man im Alten Ägypten in zwei Gräbern aus vordynastischer Zeit Schmuckperlen aus Meteoreisen mit einem Nickelgehalt von ca. 7,5 %. Ebenso konnte die schon früh geäußerte Vermutung bestätigt werden, dass ein bei der Mumie des Pharao Tutanchamun gefundener Dolch aus Meteoreisen gefertigt worden war. Die ältesten bekannten Funde aus Meteoreisen stammen allerdings aus Mesopotamien, das von den dort lebenden Sumerern als „urudu-an-bar“ (= "Kupfer des Himmels") bezeichnet wurde. Unter anderem wurde in der Stadt Ur ein Dolch mit einer Klinge aus Meteoreisen (10,8 % Ni) und goldbelegtem Griff entdeckt, dessen Herstellung auf eine Zeit um 3100 v. Chr. datiert ist.

Zu den Anfängen der Eisenverhüttung siehe 

Die Nutzung von nickelfreiem, also terrestrischem Eisen muss in Mesopotamien ebenfalls schon früh bekannt gewesen sein, belegt durch einen nickelfreien Eisendolch mit Bronzegriff aus der Zeit zwischen 3000 und 2700 v. Chr., der in den Ruinen von Ešnunna bei Tell Asmar im heutigen Irak gefunden wurde. Aus den Aufzeichnungen der Hethiter im Archiv von Boğazkale (ehemals "Boğazköy") in Zentralanatolien geht hervor, dass Eisen bereits zur Zeit von König Anitta (ca. 1800 v. Chr.) bekannt war und die Verhüttung von Eisen mindestens seit ca. 1300 v. Chr. Zwischen 1600 und 1200 v. Chr. blieb die Eisenproduktion weitgehend ein Monopol des Hethitischen Reiches und war ein Faktor für dessen Aufstieg. Ab 1200 v. Chr. wurde in der Levante Stahl durch Erhöhung des Kohlenstoffanteils produziert. Die Hethiter stellten aus dem Eisen, das anfänglich mit bis zum achtfachen Gewicht in Gold aufgewogen wurde, vorwiegend Schmuck her.

Im alten Ägypten ist die Verhüttung von Eisen erst seit dem 6. Jahrhundert v. Chr. nachgewiesen. Bereits seit dem Alten Reich wurde aber auf Meteroriteneisen zurückgegriffen. Dieses wurde in späteren Texten als "bj3 n pt" („Eisen des Himmels“) bezeichnet und vor allem zur Herstellung von Amuletten und Modellwerkzeugen für das Mundöffnungsritual verwendet.
Ein bekannter Fund ist eine Dolchklinge als Grabbeigabe Tutanchamuns von ca. 1350 v. Chr., die nach neueren Untersuchungen sehr wahrscheinlich aus Meteoreisen besteht. Ein weiterer Eisenfund in einem Grab bei Abydos aus der 6. Dynastie (2347–2216 v. Chr.) ließ sich zwar als nickelfrei und damit terrestrischen Ursprungs bestimmen, sein früherer Verwendungszweck konnte jedoch nicht ermittelt werden, da das Stück völlig verrostet war. Ein 1837 in den Fugen der Cheopspyramide gefundenes Eisenmesser, das zunächst in die 4. Dynastie datiert wurde, erwies sich hingegen als modernes Stück.

Weiterhin gehörten auch die Chalyber zu den Völkern des Mittelmeerraums und Kleinasiens, die bereits gute Kenntnisse über die Nutzung des Eisens als Hüttenwerkstoff gewonnen hatten. Ihr Name lebte in dem griechischen Wort für Stahl ("chalybs") weiter, im Gegensatz zum gewöhnlichen Eisen ("sideros"). Früheste Spuren von Eisenverhüttung auf griechischem Gebiet fanden sich in Form von Eisenschlacke aus der Zeit um 2000 v. Chr. in Agia Triada auf Kreta.

Im Alten Ägypten und in Gerar (Palästina) war die Eisenverhüttung etwa ab 1000 v. Chr. bekannt (für Gerar belegt durch Eisenschmelzöfen und örtlich hergestellte Ackerbaugeräte) und in China mindestens seit der Han-Dynastie (206 v. Chr. bis 222 n. Chr.).

Zu den ältesten europäischen Stücken gehören die eisernen Zelte und Speere, die Graf Gozzadini 1853 in etruskischen Gräbern bei Bologna entdeckt hat. Sie stammen aus dem 9. bis 10. Jahrhundert vor Christus.

Einer der ältesten bekannten Eisenfunde in Deutschland ist ein eiserner Niet als Verbindung zwischen bronzener Lanzenspitze und hölzernem Schaft, der in Helle (Ostprignitz) gefunden wurde und etwa aus der Zeit um 800 v. Chr. stammt. Im deutschsprachigen Raum markiert allerdings die erst 300 Jahre später beginnende La-Tène-Zeit eine erste Hochkultur mit zahlreichen Eisenverhüttungsplätzen und Eisenfunden.

Neben seiner herausragenden Bedeutung als Werkstoff wurde Eisen in der Alchemie verwendet, wo es mit dem Zeichen für Mars/Männlichkeit ♂ assoziiert wurde.
Bis ins 18. Jahrhundert waren Rennöfen bzw. Rennwerke mit angeschlossenen Schmieden in Europa weit verbreitet. Flüssiges Roheisen entstand mit diesem Verfahren allerdings nicht, da ein Rennofen nur Temperaturen zwischen 1000 und 1200 °C erreichen konnte, der Schmelzpunkt von reinem Eisen jedoch bei 1538 °C liegt (Schmelzpunkt von reinem Zementit, FeC: 1250 °C). Die Entwicklung von Gusseisen fand erst im 13. Jahrhundert in Schweden (Lapphyttan und Vinarhyttan) statt. Mit der gegossenen Kanonenkugel verbreitete sich die Gusseisenverarbeitung schnell wie die Feldzüge über ganz Europa.

Als die schwindenden Wälder den wachsenden Holzkohlebedarf zur Eisengewinnung in Großbritannien nicht mehr decken konnten, wurde Kohle (genauer das Kohleprodukt Koks) von Abraham Darby als Alternative entwickelt. Diese Umstellung, zusammen mit der Erfindung der Dampfmaschine, gilt als Beginn der industriellen Revolution. Die Hüttenwerke produzierten Gusseisen und Schmiedeeisen. Mit der Einführung des Puddelverfahrens konnte man die bisher übliche Holzkohle durch die günstigere Steinkohle ersetzen.

Auch in China wurden die ersten Erfahrungen mit Eisen an Meteoriteneisen gewonnen. Erste archäologische Spuren von Schmiedeeisen finden sich im Nordwesten, nahe Xinjiang, aus dem 8. vorchristlichen Jahrhundert. Man vermutet, dass diese Produkte, die mit den Methoden des Nahen Ostens erstellt wurden, durch Handel nach China gelangt sind. 550 v. Chr. wurde der Hochofen entwickelt: Damit war das Herstellen von Gusseisen möglich.

In den Gräbern von Turan, einer Region, die sich über den Osten Irans, den Süden Afghanistans und den Südwesten Pakistans zog, fanden sich eiserne Gegenstände und größere Eisenlager in den Ruinen von Khorsabad. Entdeckt wurden Ringe und Kettenteile zusammen mit etwa 160.000 kg Eisenbarren. Layard stieß bei seinen Ausgrabungen in Nimrud auf eiserne Waffen wie Helme, Speere und Dolche. Berühmt ist die Eiserne Säule in Delhi, ein sieben Meter hoher schmiedeeiserner Pfeiler aus dem 4./5. Jahrhundert.

In Australien und den umliegenden besiedelten Inseln Polynesiens war dagegen die Nutzung von Eisen bis zur Entdeckung durch europäische Forscher unbekannt. Auch in der ansonsten hochstehenden Kultur der Inkas und Azteken Mittel- und Südamerikas verarbeitete man zwar Gold, Silber, Kupfer und Bronze von guter Qualität und großer Kunstfertigkeit, Eisen jedoch nur in geringer Menge und nur Meteoreisen.

Eisen steht in der Reihe der "relativen Elementhäufigkeit bezogen auf Silicium" im Universum mit 8,3 · 10 Atomen je 1 · 10 Siliciumatomen an 9. Stelle. Die Fusion von Elementen in Sternen endet beim Eisen, da bei der Fusion höherer Elemente keine Energie mehr frei wird, sondern aufgewendet werden muss (siehe Nukleosynthese). Schwerere Elemente entstehen endotherm bei Supernovaexplosionen, die auch für das Verstreuen der im Stern entstandenen Materie verantwortlich sind.

Eisen steht in der Reihe der "Elementhäufigkeit nach dem Massenanteil" an 2. Stelle in der gesamten Erde (28,8 %), an 4. Stelle in der Erdhülle (4,70 %) und an 4. Stelle in der kontinentalen Erdkruste (5,63 %); im Meerwasser ist es nur zu 0,002 mg/L enthalten. Eisen ist zusammen mit Nickel wahrscheinlich der Hauptbestandteil des Erdkerns. Vermutlich angetrieben von thermischen Kräften erzeugen Konvektionsströmungen von flüssigem Eisen im äußeren Kern das Erdmagnetfeld.

Die ersten Vorkommen, die abgebaut wurden, waren Raseneisenstein und offenliegende Erze. Heute wird vor allem 40-prozentiges Magneteisenerz abgebaut. Das wichtigste Mineral zur Eisengewinnung ist Magnetit, welches größtenteils aus FeO besteht. Die größten Eisenerzvorkommen finden sich in den sogenannten "Banded Iron Formations" (BIF, gebändertes Eisenerz oder Bändererz), die auch als Takonit oder Itabirit bezeichnet werden und Eisen hauptsächlich in den Mineralen Hämatit und Magnetit enthalten.

Selten kommt Eisen in der Natur auch gediegen vor, meist in Form kleiner Bläschen oder Verdickungen im umgebenden Gestein, aber auch als massige Mineral-Aggregate mit bis zu 25 t Gewicht, und ist deshalb als Mineral anerkannt. Die International Mineralogical Association (IMA) führt es gemäß der Systematik der Minerale nach Strunz (9. Auflage) unter der System-Nr. „1.AE.05“ (Elemente – Metalle und intermetallische Verbindungen – Eisen-Chrom-Familie) (8. Auflage: "I/A.07-10"). Die im englischsprachigen Raum ebenfalls geläufige Systematik der Minerale nach Dana führt das Element-Mineral unter der System-Nr. „1.1.11.0“.

Weltweit konnte gediegenes Eisen bisher (Stand: 2010) an 120 Fundorten nachgewiesen werden, wobei die überwiegende Mehrheit allerdings aus meteoritischen Eisenfunden der Varietät Kamacit besteht.

Eisen kristallisiert im kubischen Kristallsystem, hat je nach Bildungsbedingungen und Reinheitsgrad eine Mohs-Härte zwischen 4 und 5 und eine stahlgraue bis schwarze Farbe. Auch die Strichfarbe ist grau.

Wegen der Reaktion mit Wasser und Sauerstoff (Rosten) ist gediegenes Eisen nicht stabil. Es tritt daher in Legierung mit Nickel entweder als Kamacit (4 bis 7,5 % Ni) oder Taenit (20 bis 50 % Ni) nur in Eisenmeteoriten auf sowie in Basalten, in denen es manchmal zu einer Reduktion von eisenhaltigen Mineralen kommt. Eisen mit geringeren Nickelanteilen gelten als Varietät desselben und sind unter der Bezeichnung "Josephinit" bekannt, allerdings ist diese Bezeichnung auch ein Synonym des Minerals Awaruit (NiFe).

Eisenerze findet man dagegen vergleichsweise häufig, wichtige Beispiele sind die Minerale Magnetit ("Magneteisenstein", FeO), Hämatit ("Roteisenstein", FeO), Pyrrhotin ("Magnetkies", FeS) und Pyrit ("Eisenkies", FeS), Siderit ("Eisenspat", FeCO) und das als Gestein geltende Limonit ("Brauneisenstein", FeO·n HO). Das Sedimentgestein "Eisen-Oolith", manchmal als "Eisenstein" bezeichnet, besteht aus Eisenhydroxidmineralien, verkittet mit tonigen oder kalkigen Bindemitteln. Industriell weniger von Interesse allerdings in der Natur ziemlich häufig antreffbar sind die Minerale Chlorit, Glaukonit und Pyrit. Insgesamt sind derzeit (Stand: 2010) 1424 Eisenminerale bekannt.

"Siehe auch: Liste der größten Eisenerzförderer und Liste der größten Roheisenerzeuger"
Die Volksrepublik China ist mit 629,7 Millionen Tonnen (58,2 Prozent) das im Jahr 2011 bei weitem bedeutendste Herstellerland für Roheisen, gefolgt von Japan 81,0 Millionen Tonnen (7,5 Prozent) und Russland 48,1 Millionen Tonnen (4,4 Prozent). Die drei Staaten hatten zusammen einen Anteil von 70,1 Prozent an der Weltproduktion von 1082,7 Millionen Tonnen. In Europa waren weitere wichtige Produzenten die Ukraine, Deutschland, Frankreich, Italien und Großbritannien.

Weltweit wurden 2011 etwa 2,8 Milliarden Tonnen Eisenerz abgebaut. Die bedeutendsten Eisenerzlieferanten waren die Volksrepublik China, Australien, Brasilien, Indien und Russland. Zusammen hatten sie einen Anteil von 82,5 Prozent an der Weltförderung. Aus dem Eisenerz wurden neben dem Roheisen auch 63,5 Millionen Tonnen Eisenschwamm gewonnen. Zusätzlich wird aus Schrott noch neues Eisen hergestellt.

Eisenerz wird im Tagebau und im Tiefbau (Untertagebau) gewonnen. Dort, wo die als abbauwürdig erkannten Eisenerzlagerstätten offen zutage treten, kann das Erz im weniger aufwändigen Tagebau gewonnen werden. Heute wird Eisenerz hauptsächlich in Südamerika, besonders Brasilien, im Westen Australiens, in der Volksrepublik China, in Ost-Europa (beispielsweise Ukraine) und Kanada auf diese Weise abgebaut.

Diese Länder verdrängten in den letzten Jahren die ursprünglich bedeutendsten Eisenerz-Förderländer wie Frankreich, Schweden und Deutschland, dessen letzte Eisenerzgrube in der Oberpfalz 1987 geschlossen wurde.

Nur ein kleiner Teil der Erze kann als Stückerz direkt im Hochofen eingesetzt werden. Der Hauptanteil der Eisenerze wird als Feinerz in einer Sinteranlage zu Sinter verarbeitet, denn nur in dieser Form als gesinterte grobe Brocken ist der Einsatz im Hochofen möglich, da das feine Erz die Luftzufuhr (Wind) sehr beeinträchtigen oder sogar verhindern würde. Gröbere Erzkörner werden nach ihrer Größe sortiert und gesintert. Kleine Erzkörner müssen dazu gemeinsam mit Kalkzuschlagsstoffen auf mit Gas unterfeuerte, motorisch angetriebene Wanderroste (Rost-Förderbänder) aufgebracht und durch starke Erhitzung angeschmolzen und dadurch „zusammengebacken“ (gesintert) werden. Sehr feines Erz wird pulverfein aufgemahlen, was oft bereits zur Abtrennung von Gangart nötig ist. Dann wird es mit Kalkstein, feinkörnigem Koks (Koksgrus) und Wasser intensiv vermischt und auf einen motorisch angetriebenen Wanderrost aufgegeben. Durch den Wanderrost werden von unten Gase abgesaugt. Von oben wird angezündet und eine Brennfront wandert von oben nach unten durch die Mischung, die dabei kurz angeschmolzen (gesintert) wird. Ein wesentlicher Anteil der Erze wird jedoch zu Pellets verarbeitet. Hierzu wird mit Bindemitteln, Zuschlägen und Wasser eine Mischung erzeugt, die dann auf Pelletiertellern zu Kügelchen von 10 bis 16 mm Durchmesser gerollt wird. Diese werden auf einem Wanderrost mit Gasbefeuerung zu Pellets gebrannt. Sinter ist nicht gut transportierbar und wird deshalb im Hüttenwerk erzeugt, Pelletanlagen werden meist in der Nähe der Erzgruben betrieben.

Das Eisen wird überwiegend im Hochofen durch chemische Reduktion des Eisenoxids der Eisenerze mit Kohlenstoff gewonnen. Der Hochofen ist ein Schachtofen. Koks und Erz werden abwechselnd in Lagen oben in den Ofen hineingeschüttet. Dazu sind oberhalb des Ofengefäßes i. d. R. zwei Bunker angeordnet, die als Gasschleusen zwischen dem Ofengefäß und der Umgebung dienen. Ganz oben befindet sich innerhalb des Ofengefäßes eine Drehschurre, mit der das Material spiralförmig flächig auf der Beschickungsoberfläche verteilt wird. Die Kokslagen halten im unteren Bereich des Ofens, wenn das Erz plastisch wird, die Durchströmbarkeit der Schüttung mit Prozessgas aufrecht (Koksfenster).

Der Einsatz sinkt im Ofenschacht ab und wird dabei durch das etwa 2000 °C heiße, aus Kohlenstoffmonoxid und Stickstoff bestehende aufsteigende Prozessgas getrocknet, aufgeheizt, die Eisenoxide reduziert und schließlich geschmolzen (Redoxreaktion). Das Prozessgas wird erzeugt, indem unten in den Ofen durch wassergekühlte Kupferdüsen (Blasformen) auf etwa 1200 °C vorgeheizte Luft eingeblasen wird. Der Sauerstoff der Luft verbrennt mit Koks zu Kohlenstoffmonoxid. Der gesamte Vorgang dauert etwa acht Stunden.

Das übrig bleibende Abgas, das am oberen Ende des Ofenschachtes gewonnen wird, ist brennbar und wird zum Vorheizen der Luft verwendet. Mit Überschussgas wird in einem Kraftwerk Strom erzeugt.

Der Ofen erzeugt neben dem flüssigen Eisen auch flüssige Schlacke. Beides ist miteinander vermischt, hat eine Temperatur von etwa 1450 °C und wird durch ein Stichloch abgezogen, das etwa alle zwei Stunden durch Anbohren geöffnet und jeweils nach etwa einer Stunde durch Verstopfen mit einer keramischen Masse verschlossen wird. Eisen und Schlacke werden außerhalb des Ofens getrennt. Das Eisen wird in Transportpfannen gefüllt und ins Stahlwerk gebracht.

Das Eisen ist bei 1450 °C flüssig, da durch den im Eisen gelösten Kohlenstoff eine Schmelzpunktserniedrigung erfolgt. Die Schlacke wird mit Wasser verdüst. Dabei erstarrt sie durch das Abschrecken als feinkörniges Glas (Schlackensand). Dieser Schlackensand wird fein gemahlen und als Betonzusatzstoff (Füller) verwendet. Ein Hochofen erzeugt pro Tonne Eisen etwa 200 bis 300 kg Schlacke.

Erz und Koks enthalten als Hauptverunreinigung Siliciumdioxid (Quarzsand, Silikate) SiO und Aluminiumoxid AlO. Ein kleiner Teil des Siliciumdioxids wird zu Silicium reduziert, das im Eisen gelöst wird. Der Rest bildet zusammen mit dem Aluminiumoxid die Schlacke (Aluminiumsilikate).

Da der Schmelzpunkt eines Gemisches von SiO und AlO zu hoch ist, um eine bei 1450 °C flüssige Schlacke zu bilden, wird Calciumoxid zur Schmelzpunktserniedrigung verwendet. Dies wird meist bereits bei der Herstellung des Eisenerzsinters als Kalkstein zugegeben.

Das Eisen des Hochofens (Roheisen) hat nur einen Eisengehalt von etwa 95 %. Es enthält für die meisten Anwendungen zu viel Kohlenstoff, Schwefel, Silicium und Phosphor. Üblicherweise wird daher im Stahlwerk zunächst durch Einblasen von Calciumcarbid, Magnesium oder Branntkalk reduzierend entschwefelt. Die Entschwefelungsschlacke wird abgezogen und das Roheisen dann in einem Konverter (Sauerstoffblasverfahren) unter Zusatz von Branntkalk oxidierend verblasen. Dabei wird Silicium zu Siliciumdioxid und Kohlenstoff zu Kohlenstoffdioxid verbrannt. Der Phosphor wird als Calciumphosphat gebunden. Das flüssige Eisen hat danach eine Temperatur von etwa 1600 °C. Es enthält soviel Sauerstoff, dass beim Erstarren aus verbliebenem Kohlenstoff Kohlenmonoxidblasen entstehen. Beim heute meist verwendeten Strangguss ist dies unerwünscht. Beim Abstechen des Stahls aus dem Konverter in die Gießpfanne wird daher Aluminium zugegeben, um den Sauerstoff als Aluminiumoxid zu binden. Bei hohen Anforderungen an die Qualität des Stahls folgen auf den Konverterprozess noch weitere Verfahrensschritte, wie z. B. eine Vakuumbehandlung (Sekundärmetallurgie).

Hochöfen haben einen großen Material- und Energiebedarf, der bei ungünstigen Rohstoff- und Energiebedingungen nicht immer bereitgestellt werden kann. Daher wurden verschiedene Verfahren entwickelt, um die vorhandenen Eisenerze ohne oder nur mit geringem Einsatz von Koks bzw. alternativ mit Steinkohle, Braunkohle, Erdöl oder Erdgas zu reduzieren. Bei der überwiegenden Anzahl der Verfahren fällt das erzeugte Roheisen in fester, poriger Form an, das als Eisenschwamm bezeichnet wird.

Bekannte Verfahren sind, sortiert nach dem jeweiligen Reduktionsgefäß, unter anderem:

Das durchschnittliche Eisenatom hat etwa die 56-fache Masse eines Wasserstoffatoms. Der Atomkern des Eisenisotops Fe weist einen der größten Massendefekte und damit eine der höchsten Bindungsenergien pro Nukleon aller Atomkerne auf. Deshalb wird es als Endstufe bei der Energieerzeugung durch Kernfusion in den Sternen betrachtet. Den absolut höchsten Massendefekt hat jedoch Ni, gefolgt von Fe, und erst auf dem dritten Platz folgt Fe.

Bei Raumtemperatur ist die allotrope Modifikation des reinen Eisens das Ferrit oder α-Eisen. Diese Modifikation kristallisiert in einer kubisch-raumzentrierten Kristallstruktur (Wolfram-Typ) in der mit dem Gitterparameter a = 286,6 pm sowie zwei Formeleinheiten pro Elementarzelle. Diese Modifikation ist unterhalb von 910 °C stabil. Oberhalb dieser Temperatur wandelt es sich in die γ-Modifikation oder Austenit um. Diese besitzt eine kubisch-flächenzentrierte Struktur (Kupfer-Typ) mit der Raumgruppe  und dem Gitterparameter a = 364,7 pm. Eine dritte Strukturänderung erfolgt bei 1390 °C, oberhalb dieser Temperatur bis zum Schmelzpunkt bei 1535 °C ist wieder das kubisch-raumzentrierte δ-Ferrit stabil. Bei hohem Druck finden ebenfalls Phasenübergänge statt: bei Drücken von mehr als etwa 10 bis 15 GPa und Temperaturen von höchstens einigen hundert Grad Celsius wandelt sich α-Eisen in ε-Eisen, dessen Kristallgitter eine hexagonal dichteste Kugelpackung (hcp) ist, um; bei höheren Temperaturen bis hin zum Schmelzpunkt findet eine entsprechende Umwandlung von γ-Eisen zu ε-Eisen statt, wobei der Druck des Phasenübergangs mit der Temperatur steigt. Darüber hinaus gibt es möglicherweise einen weiteren Phasenübergang von ε-Eisen nach β-Eisen, der bei etwa 50 GPa und mehr als 1500 K liegt; allerdings ist die Existenz dieser β-Phase umstritten, und auch zu ihrer Kristallstruktur gibt es verschiedene Befunde, u. a. eine orthorhombische oder eine doppelte hcp-Struktur.
Diese Umwandlungen nennt man auch die „Polymorphie des Eisens“.

Das Fehlen einer β-Phase in der Standard-Nomenklatur der Eisenallotrope rührt daher, dass früher angenommen wurde, dass die Änderung des Magnetismus am Curiepunkt bei 766 °C von Ferro- auf Paramagnetismus mit einer Strukturänderung einhergeht und somit eine weitere Modifikation zwischen 766 und 910 °C existiert, die als β-Modifikation oder β-Eisen bezeichnet wurde. Dies stellte sich jedoch nach genaueren Messungen als falsch heraus.

Der Schmelzpunkt des Eisens ist experimentell nur für Drücke von bis zu etwa 50 GPa gut bestimmt. Bei höheren Drücken liefern verschiedene experimentelle Techniken stark unterschiedliche Ergebnisse. So lokalisieren verschiedene Studien den γ-ε-Tripelpunkt bei Drücken, die sich um mehrere Dutzend Gigapascal unterscheiden, und liegen bei den Schmelztemperaturen unter hohem Druck um 1000 K und mehr auseinander. Im Allgemeinen ergeben molekulardynamische Modellrechnungen und Schockexperimente höhere Temperaturen und steilere Schmelzkurven als statische Experimente in Diamantstempelzellen.

Das Spektrum von Eisen zeigt Spektrallinien in allen Spektralbereichen. In der Astronomie, genauer in der Röntgenastronomie, sind die im Röntgenbereich liegenden starken Emissionslinien von neutralem Eisen von großem Interesse. Astronomen beobachten sie in Aktiven Galaktischen Kernen, Röntgendoppelsternen, Supernova und Schwarzen Löchern.

Eisen ist beständig an trockener Luft, in trockenem Chlor sowie in konzentrierter Schwefelsäure, konzentrierter Salpetersäure und basischen Agenzien (außer heißer Natronlauge) mit einem pH-Wert größer als 9. In Salzsäure sowie verdünnter Schwefel- oder Salpetersäure löst sich Eisen rasch unter Entwicklung von Wasserstoff. An feuchter Luft und in Wasser, das Sauerstoff oder Kohlenstoffdioxid enthält, wird Eisen leicht unter Bildung von Eisenoxidhydrat ("Rosten") oxidiert. Wird Eisen an trockener Luft erhitzt, so bildet sich eine dünne Schicht von Eisen(II,III)-oxid (FeO, "Eisenhammerschlag"), die stark gefärbt ist ("Anlassen"). Sehr fein verteiltes, "pyrophores" Eisen reagiert schon bei Raumtemperatur mit Sauerstoff aus der Luft unter Feuererscheinung. Brennende Stahlwolle reagiert in feuchtem Chlor-Gas kräftig unter Bildung von braunen Eisen(III)-chlorid-Dämpfen. Wird ein Gemisch aus Eisen- und Schwefelpulver (im Gewichtsverhältnis 7:4) erhitzt, so entsteht vorwiegend Eisen(II)-sulfid. Auch mit weiteren Nichtmetallen wie Phosphor, Silicium und Kohlenstoff bildet Eisen bei erhöhter Temperatur Phosphide, Silicide oder Carbide.

Reines Eisen ist geruchlos. Der typische, als metallisch klassifizierte Geruch, wenn man Eisengegenstände berührt, entsteht durch eine chemische Reaktion von Stoffen des Schweißes und des Fetts der Haut mit den sich dabei bildenden zweiwertigen Eisenionen.

Einer der wichtigsten Duftträger ist 1-Octen-3-on, das noch in großer Verdünnung pilzartig-metallisch riecht. Vorstufe der Geruchsstoffe sind Lipidperoxide. Diese entstehen, wenn Hautfett durch bestimmte Enzyme oder nichtenzymatische Prozesse (z. B. UV-Anteil des Lichts) oxidiert werden. Diese Lipidperoxide werden dann durch die zweiwertigen Eisenionen zersetzt, wobei die Duftstoffe gebildet werden. Die zweiwertigen Eisenionen entstehen durch Korrosion des Eisens bei Berührung mit dem Handschweiß, der korrosive organische Säuren und Chloride enthält.

Beim Verreiben von Blut auf der Haut entsteht ein ähnlicher Geruch. Blut enthält ebenfalls Eisenionen.

Eisen hat 27 Isotope und zwei Kernisomere, von denen vier natürlich vorkommende, stabile Isotope sind. Sie haben die relativen Häufigkeiten: Fe (5,8 %), Fe (91,7 %), Fe (2,2 %) und Fe (0,3 %). Das Isotop Fe hat eine Halbwertszeit von 2,62 Millionen Jahren, Fe von 2,737 Jahren und das Isotop Fe eine von 44,495 Tagen. Die restlichen Isotope und die beiden Kernisomere haben Halbwertszeiten zwischen weniger als 150 ns und 8,275 Stunden. Die Existenz von Fe zu Beginn der Entstehung des Planetensystems konnte durch den Nachweis einer Korrelation zwischen den Häufigkeiten von Ni, dem Zerfallsprodukt von Fe, und den Häufigkeiten der stabilen Fe-Isotope in einigen Phasen mancher Meteorite (beispielsweise in den Meteoriten "Semarkona" und "Chervony Kut") nachgewiesen werden. Möglicherweise spielte die freigesetzte Energie beim radioaktiven Zerfall von Fe, neben der atomaren Zerfallsenergie des ebenfalls vorhandenen radioaktiven Al, eine Rolle beim Aufschmelzen und der Differenzierung der Asteroiden direkt nach ihrer Bildung vor etwa 4,6 Milliarden Jahren. Heute ist das ursprünglich vorhanden gewesene Fe in Ni zerfallen. Die Verteilung von Nickel- und Eisenisotopen in Meteoriten erlaubt es, die Isotopen- und Elementehäufigkeit bei der Bildung des Sonnensystems zu messen und die vor und während der Bildung des Sonnensystems vorherrschenden Bedingungen zu erschließen.

Von den stabilen Eisenisotopen besitzt nur Fe einen von null verschiedenen Kernspin.

Eisen ist der Hauptbestandteil von Stahl und Gusseisen. Eisen ist mit 95 Prozent Gewichtsanteil an genutzten Metallen das weltweit meistverwendete. Der Grund dafür liegt in seiner weiten Verfügbarkeit, welche es recht preiswert macht, und darin, dass Stahl hervorragende Festigkeit und Zähigkeit beim Eingehen von Legierungen mit anderen Metallen wie Chrom, Molybdän und Nickel, erreicht, die es für viele Bereiche in der Technik zu einem Grundwerkstoff machen. Es wird bei der Herstellung von Landfahrzeugen, Schiffen und im gesamten Baubereich (Stahlbetonbau, Stahlbau) eingesetzt. Weitere Einsatzgebiete sind Verpackungen (Dosen, Gebinde, Behälter, Eimer, Band), Rohrleitungen, Druckbehälter, Gasflaschen und Federn. Industriell sind verschiedene Stähle verbreitet; in Deutschland sind etwa 7.500 Sorten genormt.

Eisen wird in den nachfolgend angeführten Formen als Werkstoff genutzt:

Eisen ist (neben Cobalt und Nickel) eines jener drei ferromagnetischen Metalle, die mit ihrer Eigenschaft den großtechnischen Einsatz des Elektromagnetismus u. a. in Generatoren, Transformatoren, Drosseln, Relais und Elektromotoren ermöglichen. Es wird rein oder u. a. mit Silicium, Aluminium, Kobalt oder Nickel (siehe Mu-Metall) legiert und dient als weichmagnetisches Kernmaterial zur Führung von Magnetfeldern, zur Abschirmung von Magnetfeldern oder zur Erhöhung der Induktivität. Es wird hierzu massiv und in Form von Blechen und Pulver (Pulverkerne) produziert.

Eisenpulver wird auch in der Chemie verwendet und dient in entsprechenden Tonband-Typen zur magnetischen Datenaufzeichnung. Eisendraht diente zur Datenaufzeichnung im Drahttongerät und wird u. a. zur Herstellung von Drahtseilen verwendet.

In der Medizin werden eisenhaltige Präparate als Antianämika eingesetzt, kausal in der Behandlung von Eisenmangelanämien und additiv in der Behandlung von durch andere Ursachen hervorgerufenen Anämien.

Eisen ist ein essentielles Spurenelement für fast alle Lebewesen, bei Tieren vor allem für die Blutbildung. In pflanzlichen Organismen beeinflusst es die Photosynthese sowie die Bildung von Chlorophyll und Kohlenhydraten. Im Körper von Menschen und Tieren liegt es oxidiert als Eisen(II) und Eisen(III) vor. Als Zentralatom des Kofaktors Häm "b" in Hämoglobin und Myoglobin und in Cytochromen ist es bei vielen Tieren und beim Menschen für Sauerstofftransport und -speicherung sowie für die Elektronenübertragung verantwortlich. In diesen Proteinen ist es von einem planaren Porphyrinring umgeben.

Weiter ist Eisen Bestandteil von Eisen-Schwefel-Komplexen (so genannte Eisen-Schwefel-Cluster) in vielen Enzymen, beispielsweise Nitrogenasen, Hydrogenasen oder den Komplexen der Atmungskette. Als dritte wichtige Klasse der Eisenenzyme sind die so genannten Nicht-Häm-Eisenenzyme zu nennen, beispielsweise die Methan-Monooxygenase, Ribonukleotid-Reduktase und das Hämerythrin. Diese Proteine nehmen in verschiedenen Organismen Aufgaben wahr: Sauerstoffaktivierung, Sauerstofftransport, Redoxreaktionen und Hydrolysen. Ebenso wichtig ist dreiwertiges Eisen als Zentralion im Enzym Katalase, das in den Peroxisomen der Zellen das im Stoffwechsel entstehende Zellgift Wasserstoffperoxid abbaut.

Die Speicherung des Eisens erfolgt intrazellulär in dem Enzym Ferritin (20 % Eisenanteil) und dessen Abbauprodukt Hämosiderin (37 % Eisenanteil). Transportiert wird Eisen durch Transferrin.

Einige Bakterien nutzen Fe(III) als Elektronenakzeptor für die Atmungskette. Sie reduzieren es damit zu Fe(II), was eine Mobilisierung von Eisen bedeutet, da die meisten Fe(III)-Verbindungen schwer wasserlöslich sind, die meisten Fe(II)-Verbindungen aber gut wasserlöslich. Einige phototrophe Bakterien nutzen Fe(II) als Elektronendonator für die Reduktion von CO.

Vor allem Frauen vor den Wechseljahren haben häufig Eisenmangel, der Grund dafür ist die Menstruation. Sie sollten circa 15 Milligramm Eisen pro Tag zuführen, während der Tagesbedarf eines erwachsenen Mannes nur etwa 10 Milligramm beträgt. Außerdem verlieren Frauen zusätzlich bei der Geburt eines Kindes circa 1000 Milligramm Eisen. Durch die gleichzeitige Einnahme von Vitamin C wird die Resorptionsquote von Eisen deutlich erhöht. Besonders reichhaltig ist Eisen in Blutwurst, Leber, Hülsenfrüchten und Vollkornbrot enthalten und nur gering in (Muskel-)Fleisch. Gleichzeitiger Verzehr von Milchprodukten, Kaffee oder schwarzem Tee hemmen jedoch die Eisenaufnahme.

Eisen ist ein wichtiges Spurenelement für den Menschen, kann jedoch bei Überdosierung auch schädlich wirken. Davon sind insbesondere Menschen betroffen, die an Hämochromatose, einer Regulationsstörung der Eisenaufnahme im Darm, leiden. Das Eisen reichert sich im Verlauf der Krankheit in der Leber an und führt dort zu einer Siderose und weiteren Organschäden.

Weiterhin steht Eisen im Verdacht, Infektionskrankheiten, z. B. Tuberkulose zu fördern, da die Erreger zur Vermehrung ebenfalls Eisen benötigen. Außerdem kommt es bei einigen neurodegenerativen Erkrankungen wie beispielsweise der Parkinson- oder auch der Alzheimer-Krankheit zu Eisenablagerungen in bestimmten Bereichen des Gehirns. Es ist zurzeit unklar, ob dies eine Ursache oder eine Folge der Erkrankung ist.

Daher sind Eisenpräparate wie auch andere Nahrungsergänzungsmittel nur zu empfehlen, wenn ein ärztlich diagnostizierter Eisenmangel vorliegt.

Auch in pflanzlichen Organismen ist Eisen ein essentielles Spurenelement. Es beeinflusst die Photosynthese sowie die Bildung von Chlorophyll und Kohlenhydraten. Eisenüberladung kann sich jedoch in Form von Eisentoxizität bemerkbar machen. In Böden liegt es bei normalen pH-Werten als Fe(OH) vor. Bei geringem Sauerstoffgehalt des Bodens wird Eisen(III) durch Reduktion zum Eisen(II) reduziert. Dadurch wird das Eisen in eine lösliche, für Pflanzen verfügbare Form gebracht. Nimmt diese Verfügbarkeit unter anaeroben Bedingungen, zum Beispiel durch Bodenverdichtung, zu stark zu, können Pflanzenschäden durch Eisentoxizität auftreten, eine Erscheinung, die besonders in Reisanbaugebieten bekannt ist.

Bei der Nachweisreaktion für Eisen-Ionen werden zunächst die beiden Kationen Fe und Fe unterschieden.

Mit Thioglykolsäure lassen sich Fe- und Fe-Ionen nachweisen:

Bei Anwesenheit von Fe- oder Fe-Ionen entsteht eine intensive Rotfärbung.

Die Fe-Ionen lassen sich mit rotem Blutlaugensalz nachweisen:

Fe-Ionen lassen sich mit gelbem Blutlaugensalz nachweisen:

Bei beiden Nachweisreaktionen entsteht tiefblaues Berliner Blau, ein wichtiger Farbstoff. Es läuft keine Komplexbildungsreaktion ab, sondern lediglich ein Kationenaustausch.

Beide Pigmente sind weitgehend identisch, da zwischen ihnen ein chemisches Gleichgewicht besteht. Dabei geht Fe in Fe über und umgekehrt:

Die besonders intensive blaue Farbe des Komplexes entsteht durch Metall-Metall-Charge-Transfers zwischen den Eisen-Ionen.
Es ist bemerkenswert, dass dieses bekannte Eisennachweisreagenz selbst Eisen enthält, welches durch die Cyanidionen chemisch gut maskiert wird (Innerorbitalkomplex) und somit die Grenzen der chemischen Analytik aufzeigt.

Alternativ kann man Eisen(III)-salze mit Thiocyanaten (Rhodaniden) nachweisen. Diese reagieren mit Eisen(III)-Ionen zu Eisen(III)-thiocyanat:

Es bildet sich das tiefrote Eisen(III)-thiocyanat (Fe(SCN)), welches in Lösung bleibt.
Allerdings stören einige Begleitionen diesen Nachweis (z. B. Co, Mo, Hg, Überschuss an Mineralsäuren), so dass u. U. ein Kationentrenngang durchgeführt werden muss.


Eisen bildet mit Sauerstoff zweiwertige und dreiwertige Oxide:

Da diese Oxide keine feste Schutzschicht bilden, oxidiert ein der Atmosphäre ausgesetzter Eisenkörper vollständig. Die poröse Oxidschicht verlangsamt den Oxidationsvorgang, kann ihn jedoch nicht verhindern, weshalb das Brünieren als schwacher Schutz vor Korrosion dient.
Wenn Eisenkörper vor dem endgültigen Verrosten eingesammelt und dem Recycling zugeführt werden, sind verrostetes Eisen und verrosteter Stahl bei der Stahlproduktion im Elektro-Schmelzofen ein begehrter und wertvoller Sauerstoffträger. Dieser Sauerstoff im Eisenschrott wirkt beim „Stahlkochen“ als Oxidationsmittel, um ungewünschte qualitätsmindernde Beimengungen (z. B. Leichtmetalle) zu oxidieren (verbrennen).

Eisenoxide und Eisenhydroxide werden als Lebensmittelzusatzstoffe verwendet (E 172).

Eisen bildet zweiwertige und dreiwertige Salze:

Alle Eisensalze werden unter anderem verwendet als Flockungsmittel und zur Phosphatelimination, dazu gehören die Vorfällung, Simultanfällung, Nachfällung und Flockenfiltration sowie das Ausfällen von Sulfiden, Faulgasentschwefelung und Biogasentschwefelung.

Einzelne Eisenverbindungen:




</doc>
<doc id="1250" url="https://de.wikipedia.org/wiki?curid=1250" title="Edelmetalle">
Edelmetalle

Edelmetalle sind Metalle, die korrosionsbeständig sind, das heißt die in natürlicher Umgebung unter Einwirkung von Luft und Wasser dauerhaft chemisch stabil sind. Aufgrund dieser Stabilität sind Gold und Silber seit dem Altertum zur Herstellung von Schmuck und Münzen in Gebrauch. In den letzten vier Jahrhunderten wurden zusätzlich die Platinmetalle entdeckt, die ähnlich korrosionsbeständig sind wie Gold. Auf den Weltmärkten spielen heute vor allem Gold, Silber, Platin und Palladium eine Rolle. Alle Edelmetalle und Halbedelmetalle zählen zu den Schwermetallen.

Zu den Edelmetallen im klassischen Sinn gehören Gold, Silber und die Platinmetalle. Teilweise wird auch noch Quecksilber zu den Edelmetallen gezählt, obwohl es in vieler Hinsicht reaktiver als die anderen Edelmetalle ist. Edelmetalle korrodieren bei Raumtemperatur an Luft entweder gar nicht oder nur äußerst langsam und in sehr geringem Umfang, so wie Silber, wenn es mit (Spuren von) Schwefelwasserstoff in Berührung kommt. Der Silbergegenstand wird dabei nicht beschädigt, es bildet sich nur eine extrem dünne Schicht von schwarzem Silbersulfid. Auch von Salzsäure werden die Edelmetalle nicht angegriffen. Sie zeichnen sich ferner dadurch aus, dass viele ihrer Verbindungen thermisch nicht stabil sind. So werden Silberoxid und Quecksilberoxid beim Erhitzen in ihre Elemente zerlegt. Edelmetalle entstehen wie andere Elemente, die schwerer sind als Wasserstoff, durch Nukleosynthese.

Im 19. und 20. Jahrhundert wurde die Theorie der Redoxreaktionen verfeinert. Neue Reaktionswege wurden entdeckt. Des Weiteren entwickelte man die elektrochemische Methode der Potentiometrie, mit der man die Stärke von Reduktionsmitteln und Oxidationsmitteln genau messen und vergleichen konnte. Dies gestattete auch eine verfeinerte Einteilung der Metalle nach ihrem edlen oder unedlen Charakter. Zu den Halbedelmetallen gehören demnach solche, die nicht unter Wasserstoffbildung mit wässrigen Lösungen nichtoxidierender Säuren wie zum Beispiel Salzsäure oder verdünnte Schwefelsäure reagieren. Das liegt an ihrem Standardpotential, welches höher als dasjenige des Wasserstoffs ist. Diese Metalle sind auch gegen Luftsauerstoff weitgehend inert. Aus diesem Grund kommen sie in der Natur gelegentlich gediegen vor. 

Metalle wie Bismut und Kupfer liegen mit ihrem Standardpotential deutlich näher am Wasserstoff als die klassischen Edelmetalle. An Luft korrodieren sie schneller, und in oxidierenden Säuren wie konzentrierter Schwefelsäure oder halbkonzentrierter (30-prozentiger) Salpetersäure lösen sie sich zügig. 
Im chemischen Sinne sind Halbedelmetalle also alle Metalle, die in der elektrochemischen Spannungsreihe ein positives Standardpotential gegenüber Wasserstoff besitzen, ansonsten aber nicht so korrosionsbeständig wie klassische Edelmetalle sind. Nach dieser Definition ist auch das künstliche und radioaktive Technetium als halbedel zu bezeichnen.
Diese Halbedelmetalle nehmen also eine Zwischenstellung zwischen den klassischen edlen und unedlen Metallen ein. Selbst Nickel und Zinn werden von einigen Autoren dazugezählt, obwohl ihr Standardpotential etwas unter dem Wasserstoff liegt.

Theoretische Überlegungen aufgrund quantenmechanischer Berechnungen sprechen dafür, dass auch die künstlichen Elemente Bohrium, Hassium, Meitnerium, Darmstadtium, Roentgenium und Copernicium Edelmetalle sind. Praktische Bedeutung kommt diesen Metallen allerdings nicht zu, da ihre bekannten Isotope äußerst instabil sind und schnell (mit typischen Halbwertszeiten von einigen Sekunden, höchstens von wenigen Minuten) radioaktiv zerfallen.

Klar abzugrenzen sind die unedlen Metalle wie Aluminium, Eisen und Blei. Da ihr Standardpotential kleiner als das von Wasserstoff ist, werden sie von nichtoxidierenden Säuren angegriffen. Das kann, wie beim Blei, auch recht langsam erfolgen. "Nichtoxidierend" bedeutet hierbei, dass sich kein stärkeres Oxidationsmittel als das Wasserstoffion in der Lösung befindet.

Neben den Edelmetallen gibt es auch noch einige Metalle, die infolge ihrer Passivierung mitunter eine hohe Korrosionsbeständigkeit besitzen, die je nach chemischem Milieu auch manche Edelmetalle zum Teil übertrifft. Dies sind die Elemente der 4. Nebengruppe (Titan, Zirconium und Hafnium), die der 5. Nebengruppe (Vanadium, Niob und Tantal) sowie die der 6. Nebengruppe (Chrom, Molybdän und Wolfram). Weitere technisch bedeutende Metalle, die Passivschichten bilden, sind Zink (12. Nebengruppe), Aluminium (3. Hauptgruppe) sowie Silicium und Blei (4. Hauptgruppe).

Mit geeigneten aggressiven Chemikalien kann man alle Edelmetalle in Lösung bringen. Gold und einige Platinmetalle lösen sich zügig in Königswasser. Silber sowie die Halbedelmetalle reagieren lebhaft mit Salpetersäure. Im Bergbau werden Cyanidlösungen in Verbindung mit Luftsauerstoff verwendet, um Gold und Silber aus Gesteinen zu lösen. Der Angriff durch den Luftsauerstoff ist nur möglich, weil sich als Produkte stabile Cyanidokomplexe mit Gold und Silber bilden. Auch im Königswasser ist die Bildung stabiler Komplexverbindungen (Chlorokomplexe) mitentscheidend für die oxidierende Wirkung des Milieus.
Edelmetalle verhalten sich im Übrigen häufig gar nicht „edel“ gegenüber sehr elektropositiven Metallen, sondern bilden hier häufig bereitwillig und unter Energiefreisetzung Intermetallische Phasen.

Im physikalischen Sinn ist die Menge der Edelmetalle noch bedeutend kleiner; es sind nur Kupfer, Silber und Gold. Das Kriterium zur Klassifizierung ist die elektronische Bandstruktur. Die drei aufgeführten Metalle besitzen alle vollständig gefüllte d-Bänder, die damit nicht zur Leitfähigkeit und praktisch nicht zur Reaktivität beitragen. Für Platin gilt dies z. B. nicht. Zwei d-artige Bänder kreuzen das Ferminiveau. Das führt zu einem anderen chemischen Verhalten, weshalb Platin, im Gegensatz zu Gold, auch gern als Katalysator benutzt wird. Besonders auffällig ist der Unterschied bei der Herstellung reiner Metalloberflächen im Ultrahochvakuum. Während z. B. Gold vergleichsweise leicht zu präparieren ist und nach der Präparation lange rein bleibt, bindet sich an Platin oder auch Palladium sehr schnell Kohlenstoffmonoxid.

Wie schon bei den unedlen Metallen angedeutet, sind Edelmetalle und Halbedelmetalle einfach metallische Elemente (und eventuell gewisse Legierungen, wie z. B. korrosionsbeständige Stähle), deren Normalpotential positiv gegenüber der Wasserstoffelektrode ist, die also von verdünnten Säuren nicht angegriffen werden. Die Elemente, die in Betracht kommen, sind somit sortiert nach ihrem Normalpotential gegenüber der H-Elektrode in wässriger Lösung bei pH 7:

Antimon zählt als Halbmetall nicht dazu, und bei Polonium ist es möglicherweise seine starke Radioaktivität und makroskopische Unverfügbarkeit (vor dem Bau von Kernreaktoren), wegen der man es klassisch nicht als Edelmetall angesehen hatte – heutzutage ist es aber in Gramm-Mengen verfügbar. Die Unterteilung, sprich Potentialgrenze, dieser Elemente in Edelmetalle und Halbedelmetalle ist ziemlich willkürlich und wird nicht einheitlich gehandhabt. Sie wird aber meistens zwischen Kupfer und Ruthenium gezogen , da letztere prinzipiell durch feuchte Luft aufgrund der Redoxreaktion O + 2 HO + 4 e ⇄ 4 OH(aq) mit einem Normalpotential von +0.4 V angegriffen werden können.

Bronze ist kein Edelmetall, sondern typischerweise eine Kupfer-Zinn-Legierung. Weil bei den Olympischen Spielen und anderen Wettkämpfen Gold-, Silber- und Bronzemedaillen vergeben werden, wird die Bronze im Sprachgebrauch von Sportreportagen manchmal fälschlicherweise auch als Edelmetall bezeichnet.




</doc>
<doc id="1252" url="https://de.wikipedia.org/wiki?curid=1252" title="Edelgase">
Edelgase

Die Edelgase, auch inerte Gase oder Inertgase bilden eine Gruppe im Periodensystem der Elemente, die sieben Elemente umfasst: Helium, Neon, Argon, Krypton, Xenon, das radioaktive Radon sowie das künstlich erzeugte, ebenfalls radioaktive Oganesson. Die Gruppe wird systematisch auch "8. Hauptgruppe" oder nach der neueren Einteilung des Periodensystems "Gruppe 18" genannt und am rechten Rand des Periodensystems neben den Halogenen dargestellt.

Das einheitliche Hauptmerkmal sämtlicher Edelgasatome ist, dass alle ihre Elektronenschalen entweder vollständig mit Elektronen besetzt oder leer sind (Edelgaskonfiguration): Es gibt nur vollständig gefüllte Atomorbitale, die dazu führen, dass Edelgase nur unter extremen Bedingungen chemische Reaktionen eingehen; sie bilden auch miteinander keine Moleküle, sondern sind einatomig und bei Raumtemperatur Gase. Dieser geringen Reaktivität verdanken sie ihren Gruppennamen, der sich an die ebenfalls nur wenig reaktiven Edelmetalle anlehnt.

Helium ist das mit Abstand häufigste Edelgas. Auf der Erde kommt Argon am häufigsten vor; alle anderen zählen zu den seltenen Bestandteilen der Erde. Als Gase sind sie Bestandteile der Luft; in der Erdkruste findet man sie mit Ausnahme des Heliums, das in Erdgas enthalten ist, nur in sehr geringen Mengen. Entdeckt wurden sie – mit Ausnahme des erst 2006 hergestellten Oganessons – kurz nacheinander in den Jahren 1868 (Helium) bis 1900 (Radon). Die meisten Edelgase wurden erstmals vom britischen Chemiker William Ramsay isoliert.

Verwendung finden Edelgase vor allem als Schutzgas, z. B. in Glühlampen, wichtig sind sie als Füllgas von Gasentladungslampen, in denen sie in der für jedes Gas charakteristischen Farbe leuchten. Trotz der geringen Reaktivität sind von den schwereren Edelgasen, insbesondere Xenon, chemische Verbindungen bekannt. Deren wichtigste ist das starke Oxidationsmittel Xenon(II)-fluorid.

Einen ersten Hinweis, dass in der Luft ein unreaktives Gas enthalten ist, fand 1783 Henry Cavendish. Er mischte Luft und Sauerstoff derart, dass die darin enthaltenen Elemente Stickstoff und Sauerstoff mit Hilfe von Reibungselektrizität komplett zu Stickoxiden reagierten. Dabei blieb ein nicht reagierender Rest zurück. Er erkannte jedoch nicht, dass es sich dabei um ein neues Gas – eine Mischung aus Argon und anderer Edelgase – handelte, und setzte seine Experimente nicht fort.

Als erstes Edelgas entdeckten 1868 Jules Janssen und Norman Lockyer das Helium unabhängig voneinander. Die beiden Astronomen beobachteten – Janssen in Indien, Lockyer in England – das Sonnenspektrum und entdeckten darin eine bislang unbekannte gelbe Spektrallinie bei einer Wellenlänge von 587,49 nm. Das neue Element wurde von Edward Frankland nach "" für die Sonne "Helium" genannt. Der erste Nachweis von Helium auf der Erde gelang 1892 Luigi Palmieri durch Spektralanalyse von Vesuv-Lava.

Cavendishs Experimente zur Untersuchung der Luft wurden ab 1888 von Lord Rayleigh fortgesetzt. Er bemerkte, dass „Stickstoff“, der aus der Luft gewonnen wurde, eine andere Dichte besitzt als aus der Zersetzung von Ammoniak gewonnener. Rayleigh vermutete daher, dass es einen noch unbekannten, reaktionsträgen Bestandteil der Luft geben müsse. Daher versuchten er und William Ramsay, durch Reaktion mit Magnesium den Stickstoff aus einer Luftprobe vollständig zu entfernen und dieses unbekannte Gas zu isolieren. Schließlich gelang ihnen 1894 spektroskopisch der Nachweis eines neuen Elementes, das sie nach dem griechischen "argos", „träge“, "Argon" benannten.

Nachdem die wichtigsten Eigenschaften von Helium und Argon bestimmt worden waren, konnte festgestellt werden, dass diese Gase im Gegensatz zu den anderen atmosphärischen Gasen einatomig sind. Dies wurde dadurch erkannt, dass das Verhältnis der molaren Wärmekapazität "C" bei konstantem Druck im Verhältnis zur Wärmekapazität "C" bei konstantem Volumen bei Edelgasen einen sehr hohen Wert von 1,67 (= "C"/"C") aufweist, während zwei- und mehratomige Gase deutlich kleinere Werte aufweisen. Daraufhin vermutete William Ramsay, dass es eine ganze Gruppe derartiger Gase geben müsse, die eine eigene Gruppe im Periodensystem bilden und er begann nach diesen zu suchen. 1898 gelang es ihm und Morris William Travers, durch fraktionierte Destillation von Luft, Neon, Krypton und Xenon zu isolieren.

Als letztes der natürlich vorkommenden Edelgase wurde 1900 von Friedrich Ernst Dorn als "Radium-Emanation" (Ausdünstung von Radium) das Radon entdeckt und mit dem Symbol Em bezeichnet. Dabei handelte es sich um das Isotop Rn. Weitere Radon-Isotope wurden von Ernest Rutherford und André-Louis Debierne gefunden und zunächst für eigene Elemente gehalten. Erst nachdem William Ramsay 1910 das Spektrum und weitere Eigenschaften bestimmte, erkannte er, dass es sich um ein einziges Element handelt. Er nannte dies zunächst "Niton" (Nt), seit 1934 wird der Name Radon verwendet. Oganesson, das letzte Element der Gruppe, konnte nach mehreren nicht erfolgreichen Versuchen erstmals 2002–2005 am Vereinigten Institut für Kernforschung in Dubna erzeugt werden.

Es wurden schon bald nach der Entdeckung Versuche unternommen, Verbindungen der Edelgase zu synthetisieren. 1894 versuchte Henri Moissan, eine Reaktion von Argon mit Fluor zu erreichen, scheiterte jedoch. Im Jahr 1924 behauptete A. von Antropoff, eine erste Kryptonverbindung in Form eines roten stabilen Feststoffes aus Krypton und Chlor synthetisiert zu haben. Später stellte sich jedoch heraus, dass in dieser Verbindung kein Krypton, sondern Stickstoffmonoxid und Chlorwasserstoff enthalten waren.

Mit Xenonhexafluoroplatinat wurde 1962 durch Neil Bartlett erstmals eine Xenonverbindung und damit die erste Edelgasverbindung überhaupt entdeckt. Nur wenige Monate nach dieser Entdeckung folgten im August 1962 nahezu zeitgleich die Synthese des Xenon(II)-fluorids durch Rudolf Hoppe und die des Xenon(IV)-fluorids durch eine Gruppe um die amerikanischen Chemiker C. L. Chernick und H. H. Claassen. Bald darauf konnte durch A. V. Grosse die erste Kryptonverbindung dargestellt werden, die er zunächst für "Kryptontetrafluorid" hielt, die jedoch nach weiteren Versuchen als Kryptondifluorid identifiziert wurde. Im Jahr 2000 wurde die erste Argonverbindung, das sehr instabile Argonfluorohydrid synthetisiert.

Edelgase finden sich vorwiegend in der Erdatmosphäre, in geringem Maße aber auch in der Erdkruste; ihre Häufigkeiten sind jedoch sehr unterschiedlich. Das mit Abstand häufigste ist Argon, das mit einem Volumenanteil von 0,934 % (9340 ppm) einen nennenswerten Anteil der gesamten Atmosphäre ausmacht. Alle anderen sind mit Anteilen unter 20 ppm sehr viel seltener, sie zählen daher zu den Spurengasen. Krypton, Xenon und Radon zählen zu den seltensten Elementen auf der Erde überhaupt. Helium ist außerdem Bestandteil von Erdgas, an dem es einen Anteil von bis zu 16 % am Volumen haben kann.

Ständig verlässt eine geringe Menge Helium auf Grund seiner niedrigen Dichte die Erdatmosphäre in den Weltraum und ständig werden auf der Erde Edelgase neu gebildet, was ihre Häufigkeiten und auch ihre Isotopenverhältnisse maßgeblich bestimmt. Argon, vor allem das Isotop Ar, wird durch Zerfall des Kaliumisotops K gebildet. Helium entsteht beim Alpha-Zerfall von schweren Elementen wie Uran oder Thorium (Alpha-Teilchen), Xenon beim seltenen Spontanzerfall von Uran. Das kurzlebige Radon-Isotop Rn mit einer Halbwertszeit von 3,8 Tagen ist das häufigste und ein Zwischenprodukt in der Zerfallsreihe von U. Andere, noch kurzlebigere Isotope sind ebenfalls Mitglieder der Zerfallsreihen von Uran-, Thorium- oder Neptuniumisotopen. Auf Grund dieser Zerfallsprozesse findet man die Edelgase auch in Gesteinen eingeschlossen. So findet sich Helium in vielen Uranerzen wie Uraninit und Argon im Basalt der ozeanischen Kruste, erst beim Schmelzen des umgebenden Gesteins gast es aus. 

Die Häufigkeitsverteilung der Edelgase im Universum lässt sich großteils durch die Nukleosynthese<nowiki>wege</nowiki> erklären. Je schwerer ein Edelgas, desto seltener ist es. Helium, das sowohl durch primordiale Nukleosynthese gebildet wird, als auch durch stellare Nukleosynthese aus Wasserstoff entsteht, ist dabei nach Wasserstoff das zweithäufigste Element überhaupt. Auch Neon und Argon zählen zu den häufigsten Elementen im Universum. Krypton und Xenon, die nicht durch stellare Nukleosynthese entstehen und sich nur in seltenen Ereignissen wie Supernovae bilden, sind deutlich seltener. Bedingt durch ihren regelmäßigen Aufbau mit gerader Protonenzahl sind Edelgase gemäß der Harkinsschen Regel häufiger als viele ähnlich schwere Elemente.

Mit Ausnahme eines Großteils des Heliums und der radioaktiven Elemente erfolgt die Gewinnung der Edelgase ausschließlich aus der Luft. Sie fallen als Nebenprodukte bei der Gewinnung von Stickstoff und Sauerstoff im Linde-Verfahren an. In der Haupt-Rektifikationskolonne, in der Sauerstoff und Stickstoff getrennt werden, reichern sich die verschiedenen Edelgase an unterschiedlichen Stellen an. Sie können aber in eine eigene Kolonne überführt und dort von allen anderen Gasen getrennt werden. Während Argon leicht abgetrennt werden kann und nur von Stickstoff und Sauerstoff befreit werden muss, besteht bei Helium und Neon, aber auch bei Krypton und Xenon das Problem, dass diese sich zunächst zusammen anreichern und anschließend getrennt werden müssen. Dies kann über eine weitere Rektifikationskolonne oder auch durch unterschiedliche Adsorption der Gase an geeigneten Adsorptionsmedien erfolgen.

Helium wird zumindest seit 1980 überwiegend aus Erdgas gewonnen. Diese Heliumquelle wurde zuerst in den Vereinigten Staaten entdeckt, später auch in der Sowjetunion genutzt, heute in wenigen weiteren Ländern und Werken, so etwa in Algerien, dessen Ausbeute tiefkalt verflüssigt im 40-Fuß-Container nach Marseille und damit Europa verschifft wird. Von den anderen Bestandteilen des Erdgases kann es als Rohhelium entweder durch Ausfrieren aller anderen Gase oder durch Permeation an geeigneten Membranen getrennt werden. Anschließend muss das Helium noch durch Druckwechsel-Adsorption, chemische oder kryotechnische Verfahren von restlichen störenden Gasen wie Stickstoff oder Wasserstoff befreit werden.

Radon lässt sich auf Grund der kurzen Halbwertszeit nicht in größeren Mengen gewinnen. In kleinerem Maßstab dient Radium als Quelle, Radon entsteht beim Zerfall dieses Elements und gast aus einem entsprechenden Präparat aus. Oganesson konnte als künstliches Element in wenigen Atomen durch Beschuss von Californium mit Calcium-Atomen erzeugt werden.

Alle Edelgase sind unter Normalbedingungen einatomige, farb- und geruchslose Gase. Sie kondensieren und erstarren erst bei sehr niedrigen Temperaturen, wobei die Schmelz- und Siedepunkte umso höher liegen, je größer die Atommasse ist. Der Siedepunkt des Heliums liegt mit 4,224 K (−268,926 °C) nur knapp über dem absoluten Nullpunkt, das schwerste Edelgas Radon siedet bei 211,9 K (−61,25 °C).

Helium besitzt die Besonderheit, dass es als einziges Element unter Atmosphärendruck und auch deutlich darüber nicht erstarrt. Stattdessen geht es bei 2,17 K in einen speziellen Aggregatzustand, die Suprafluidität, über. In diesem verliert die Flüssigkeit die innere Reibung und kann so beispielsweise über höhere Gefäßwände kriechen (Onnes-Effekt). Erst bei Drücken über 25,316 bar erstarrt Helium bei 0,775 K. Diese Temperaturen und Drücke gelten nur für das häufige Isotop He, das seltene zweite, leichtere stabile Isotop He hat dagegen deutlich andere Eigenschaften. Es wird erst bei Temperaturen unter 2,6 · 10 K suprafluid. Auch Schmelz-, Siede- und kritischer Punkt liegen bei anderen Temperaturen und Drücken.

Mit Ausnahme des Heliums, das im hexagonalen Kristallsystem kristallisiert, besitzen alle Edelgase eine kubisch-flächenzentrierte Kristallstruktur. Wie durch die steigenden Atomradien zu erwarten, wird der Gitterparameter "a" von Neon zu Radon immer größer.

Auch die Dichten der Edelgase korrelieren wie zu erwarten mit der Atommasse. Helium ist nach Wasserstoff das Gas mit der geringsten Dichte. Als einziges weiteres Edelgas hat Neon eine geringere Dichte als Luft, während Argon, Krypton, Xenon und Radon dichter sind. Radon ist mit einer Dichte von 9,73 kg/m eines der dichtesten Gase überhaupt.

Die Eigenschaften von Oganesson sind auf Grund der kurzen Halbwertszeit nicht experimentell ermittelbar. Nach theoretischen Überlegungen ist durch relativistische Effekte und die hohe Polarisierbarkeit des Oganesson-Atoms anzunehmen, dass Oganesson deutlich reaktiver ist als Radon. Auch ist es unwahrscheinlich, dass es bei Standardbedingungen gasförmig ist, durch Extrapolation kann ein Siedepunkt zwischen 320 und 380 K angenommen werden.

Bei Edelgasen sind alle Elektronenschalen entweder vollständig mit Elektronen besetzt oder leer. Deshalb wird dieser Zustand auch Edelgaskonfiguration genannt. Helium ist dabei das einzige Edelgas, bei dem lediglich ein s-Orbital vollständig besetzt ist (da es kein 1p-Orbital gibt), bei allen anderen ist das äußerste besetze Orbital ein p-Orbital. Nach den Gesetzen der Quantenmechanik ist dieser Zustand der Orbitale energetisch besonders günstig. Darum tendieren auch Atome anderer Elemente dazu, Edelgaskonfiguration zu erreichen, indem sie Elektronen abgeben oder aufnehmen (Edelgasregel).

Die Eigenschaften der Edelgase sind deutlich davon bestimmt, dass sie Edelgaskonfiguration nicht durch Abgabe oder Aufnahme von Elektronen, sondern bereits im neutralen, nicht-ionisierten Zustand erreichen. Edelgase liegen daher einatomig vor, besitzen eine hohe Ionisierungsenergie und reagieren fast nicht mit anderen Elementen oder Verbindungen.

Trotz des Aufbaus der Edelgasatome sind die schweren Edelgase nicht völlig unreaktiv und können einige Verbindungen bilden. Verantwortlich hierfür sind der größere Abstand der Valenzelektronen vom Kern, wodurch die Ionisierungsenergie sinkt, sowie relativistische Effekte. Die größte Vielfalt an Verbindungen ist vom Xenon und nicht wie zu erwarten vom Radon bekannt, da bei diesem die starke Radioaktivität und kurze Halbwertszeit die Bildung von Verbindungen und deren Untersuchung erschwert.

Das einzige Element, das in der Lage ist, direkt mit Xenon, Radon und unter bestimmten Bedingungen auch Krypton zu reagieren, ist Fluor. Während das bei der Reaktion von Krypton und Fluor gebildete Krypton(II)-fluorid thermodynamisch instabil und daher nur bei tiefen Temperaturen synthetisierbar ist, sind die Xenon- und auch Radonfluoride auch bei Raumtemperatur stabil. Andere Elemente reagieren nicht mit Edelgasen, dennoch sind verschiedene weitere Verbindungen bekannt, die durch Reaktionen der Fluoride zugänglich sind.

Die Reaktivität und Stabilität von Verbindungen der leichten Edelgase Helium, Neon und Argon konnte mit Ausnahme einer bekannten Argonverbindung, HArF, nur theoretisch untersucht werden. Demnach gilt Neon als das am wenigsten reaktive Edelgas. So zeigte sich in Rechnungen, dass das Neonanalogon der einzigen in der Theorie stabilen Heliumverbindung HHeF nicht stabil sein sollte.

Aufgrund des Fehlens chemischer Verbindungen der Edelgase gab es lange Zeit auch keine Zahlenwerte ihrer Elektronegativitäten – bestimmt werden konnten davon bis jetzt nur die Werte der Pauling-Skala für die beiden Elemente Xenon (2,6) und Krypton (3,0), die damit in etwa denen der Halogene entsprechen. In den neueren Elektronegativitätsskalen nach Mulliken und Allred und Rochow dagegen lassen sich auch Zahlenwerte für die übrigen Edelgase berechnen, die in diesem Fall über die der Halogene hinausreichen. Bei Helium betragen sie beispielsweise 5,50 nach Allred-Rochow und 4,86 nach Mullikan.

Edelgase werden auf Grund ihrer geringen Reaktivität, der niedrigen Schmelzpunkte und der charakteristischen Farben bei Gasentladungen genutzt. Vor allem Argon und Helium werden in größerem Maßstab verwendet, die anderen Edelgase können nur in geringeren Mengen produziert werden und sind daher teuer. Die geringe Reaktivität wird in der Verwendung als Inert- bzw. Schutzgas beispielsweise beim Schutzgasschweißen und in der Produktion von bestimmten Metallen wie Titan oder Tantal ausgenutzt. Dafür wird vorwiegend das Argon immer dann eingesetzt, wenn der billigere, aber reaktivere Stickstoff nicht verwendet werden kann.

Bei Gasentladungen gibt jedes Edelgas Licht einer charakteristischen Farbe ab. Bei Neon beispielsweise ist das emittierte Licht rot, bei Argon violett und bei Krypton oder Xenon blau. Dies wird in Gasentladungslampen ausgenutzt. Von besonderer Bedeutung ist dabei das Xenon, da das Spektrum einer Xenon-Gasentladungslampe annähernd dem des Tageslichtes entspricht. Es wird darum auch in Autoscheinwerfern als „Xenonlicht“ verwendet. Auch Leuchtröhren basieren auf diesem Prinzip, nach dem ersten verwendeten Leuchtgas Neon werden sie auch "Neonlampen" genannt. Dagegen nutzen die umgangssprachlich „Neonröhren“ genannten Leuchtstofflampen kein Edelgas, sondern Quecksilberdampf als Leuchtmittel. Auch Glühlampen werden mit Edelgasen, häufig Krypton oder Argon, gefüllt. Dadurch ist die effektive Abdampfrate des Glühfadens geringer, was eine höhere Temperatur und damit bessere Lichtausbeute ermöglicht.

Auf Grund der niedrigen Schmelz- und Siedepunkte sind Edelgase als Kühlmittel von Bedeutung. Hier spielt vor allem flüssiges Helium eine Rolle, da durch dieses besonders niedrige Temperaturen erreicht werden können. Dies ist beispielsweise für supraleitende Magnete wichtig, die etwa in der Kernspinresonanzspektroskopie eingesetzt werden. Müssen für eine Anwendung keine so niedrigen Temperaturen erreicht werden, wie sie flüssiges Helium bietet, können auch die höher siedenden Edelgase wie Neon verwendet werden.

Wie alle Gase wirken auch die Edelgase abhängig vom Druck durch Blockierung von Membranen in Nervenzellen narkotisierend. Die nötigen Drücke liegen aber bei Helium und Neon so hoch, dass sie nur im Labor erreicht werden können; für Neon liegt der notwendige Druck bei 110 bar. Da sie daher keinen Tiefenrausch verursachen können, werden diese beiden Gase gemischt mit Sauerstoff („Heliox“ und „Neox“), auch mit Sauerstoff und Stickstoff („Trimix“) als Atemgase beim Tauchen verwendet. Mit diesen ist es möglich, größere Tiefen zu erreichen als bei der Nutzung von Luft. Xenon wirkt dagegen schon bei Umgebungsdruck narkotisierend und kann daher anstelle von Distickstoffmonoxid als Inhalationsanästhetikum verwendet werden. Wegen des hohen Preises und der geringen Verfügbarkeit wird es jedoch nur selten verwendet.

Helium ist Füll- und Traggas für Gasballone und Zeppeline. Neben Helium kann auch Wasserstoff verwendet werden. Dieser ist zwar leichter und ermöglicht mehr Nutzlast, jedoch kann er mit dem Sauerstoff der Luft reagieren und brennen. Beim unreaktiven Helium besteht diese Gefahr nicht.

Entsprechend ihrer Häufigkeit und Verfügbarkeit werden Edelgase in unterschiedlichen Mengen produziert. So betrug 1998 die Menge des hergestellten Argons etwa 2 Milliarden m, Helium wurde in einer Menge von rund 130 Millionen m isoliert. Die Weltjahresproduktion an Xenon wird dagegen für 1998 auf nur 5.000–7.000 m geschätzt (jeweils Normkubikmeter). Entsprechend unterschiedlich sind die Preise der Gase: Argon kostet etwa 15 Euro pro Kubikmeter (unter Standardbedingungen, Laborqualität), Xenon 10.000 Euro pro Kubikmeter (Stand 1999).

Die größte Vielfalt an Edelgasverbindungen gibt es mit dem Xenon. Die wichtigsten und stabilsten sind dabei die Xenonfluoride Xenon(II)-fluorid, Xenon(IV)-fluorid und Xenon(VI)-fluorid, die durch Reaktion von Xenon und Fluor in unterschiedlichen Verhältnissen synthetisiert werden. Xenon(II)-fluorid ist die einzige Edelgasverbindung, die in geringen Mengen technisch genutzt wird, sie dient als starkes Oxidations- und Fluorierungsmittel in der organischen Chemie.

Mit Sauerstoff erreicht Xenon die höchste mögliche Oxidationsstufe +8. Diese wird in Xenon(VIII)-oxid und dem Oxifluorid Xenondifluoridtrioxid XeOF sowie in Perxenaten der Form XeO erreicht. Weiterhin sind Xenon(VI)-oxid und die Oxifluoride XeOF und XeOF in der Oxidationsstufe +6 sowie das Oxifluorid XeOF mit vierwertigem Xenon bekannt. Alle Xenonoxide und -oxifluoride sind instabil und vielfach explosiv. Auch Verbindungen des Xenons mit Stickstoff, Chlor und Kohlenstoff sind bekannt. Unter supersauren Bedingungen konnten auch Komplexe mit Metallen wie Gold oder Quecksilber synthetisiert werden.

Von den anderen Edelgasen sind Verbindungen nur in geringer Zahl bekannt. So sollten Radonverbindungen zwar thermodynamisch ähnlich stabil wie Xenonverbindungen sein, aufgrund der starken Radioaktivität und kurzen Halbwertszeit der Radon-Isotope ist ihre Synthese und exakte Charakterisierung aber außerordentlich schwierig. Vermutet wird die Existenz eines stabilen Radon(II)-fluorids, da Radon nach dem Durchleiten durch flüssiges Chlortrifluorid nicht mehr nachweisbar ist, somit reagiert haben muss. Löst man die Rückstände dieser Lösung in Wasser oder Säuren, bilden sich als Zersetzungsprodukte Sauerstoff und Fluorwasserstoff im gleichen Verhältnis wie bei Krypton- oder Xenondifluorid.

Alle bekannten Verbindungen leichterer Edelgase sind thermodynamisch instabil, zersetzen sich leicht und lassen sich deshalb, wenn überhaupt, nur bei tiefen Temperaturen synthetisieren. Die wichtigste und stabilste Kryptonverbindung ist Krypton(II)-fluorid, das zu den stärksten bekannten Oxidations- und Fluorierungsmitteln zählt. Krypton(II)-fluorid ist direkt aus den Elementen herstellbar und Ausgangsprodukt einer Reihe weiterer Kryptonverbindungen.

Während Helium- und Neonverbindungen weiterhin allein Gegenstand theoretischer Untersuchungen sind und Rechnungen ergaben, dass allenfalls eine Heliumverbindung (HHeF), dagegen keine einzige Neonverbindung stabil sein sollte, konnte eine erste Argonverbindung inzwischen tatsächlich synthetisiert werden: Durch Photolyse von Fluorwasserstoff in einer auf 7,5 K heruntergekühlten Argonmatrix konnte das sehr instabile Argonfluorohydrid gebildet werden, das schon bei Berührung zweier Moleküle oder Erwärmung über 27 K wieder in seine Bestandteile zerfällt.

Argon, Krypton und Xenon bilden Clathrate, Einschlussverbindungen, bei denen das Edelgas physikalisch in einen umgebenden Feststoff eingeschlossen ist. Typische Beispiele hierfür sind Edelgas-Hydrate, bei denen die Gase in Eis eingeschlossen sind. Ein Argon-Hydrat bildet sich langsam erst bei −183 °C, Hydrate des Kryptons und Xenons schon bei −78 °C. Auch mit anderen Stoffen wie Hydrochinon sind Edelgas-Clathrate bekannt.



</doc>
<doc id="1253" url="https://de.wikipedia.org/wiki?curid=1253" title="Elektron">
Elektron

Das Elektron [] (von ‚ Bernstein‘, an dem Elektrizität zum ersten Mal beobachtet wurde; 1874 von Stoney und Helmholtz geprägt) ist ein negativ geladenes Elementarteilchen. Sein Symbol ist e. Die alternative Bezeichnung Negatron (aus negative Ladung und Elektron) wird kaum noch verwendet und ist allenfalls in der Beta-Spektroskopie gebräuchlich.

Die in einem Atom oder Ion gebundenen Elektronen bilden dessen Elektronenhülle. Die gesamte Chemie beruht im Wesentlichen auf den Eigenschaften und Wechselwirkungen dieser gebundenen Elektronen. In Metallen ist ein Teil der Elektronen frei beweglich und bewirkt die hohe elektrische Leitfähigkeit metallischer Leiter. Dies ist die Grundlage der Elektrotechnik. In Halbleitern ist die Zahl der beweglichen Elektronen und damit die elektrische Leitfähigkeit leicht zu beeinflussen, sowohl durch die Herstellung des Materials als auch später durch äußere Einflüsse wie Temperatur, elektrische Spannung, Lichteinfall etc. Dies ist die Grundlage der Elektronik. Aus jedem Material können bei starker Erhitzung oder durch Anlegen eines starken elektrischen Feldes Elektronen austreten (Glühemission, Feldemission). Als "freie Elektronen" können sie dann im Vakuum durch weitere Beschleunigung und Fokussierung zu einem Elektronenstrahl geformt werden. Dies hat die Entwicklung des Oszilloskops, des Fernsehers und des Computermonitors ermöglicht. Weitere Anwendungen freier Elektronen sind z. B. die Röntgenröhre, das Elektronenmikroskop, das Elektronenstrahlschweißen, physikalische Grundlagenforschung mittels Teilchenbeschleunigern und die Erzeugung von Synchrotronstrahlung für Forschungs- und technische Zwecke.

Beim Beta-Minus-Zerfall eines Atomkerns wird ein Elektron neu erzeugt und ausgesandt.

Der experimentelle Nachweis des Elektrons gelang erstmals Emil Wiechert im Jahre 1897 und wenig später Joseph John Thomson.

Das Konzept einer kleinsten, unteilbaren Menge der elektrischen Ladung wurde um die Mitte des 19. Jahrhunderts verschiedentlich vorgeschlagen, unter anderen von Richard Laming, Wilhelm Weber und Hermann von Helmholtz.

George Johnstone Stoney schlug 1874 die Existenz elektrischer Ladungsträger vor, die mit den Atomen verbunden sein sollten. Ausgehend von der Elektrolyse schätzte er die Größe der Elektronenladung ab, erhielt allerdings einen um etwa den Faktor 20 zu niedrigen Wert. Beim Treffen der British Association in Belfast schlug er vor, die Elementarladung als eine weitere fundamentale Naturkonstante zusammen mit der Gravitationskonstante und der Lichtgeschwindigkeit als Grundlage physikalischer Maßsysteme zu verwenden. Stoney prägte auch gemeinsam mit Helmholtz den Namen "electron" für das „Atom der Elektrizität“.

Emil Wiechert fand 1897, dass die Kathodenstrahlung aus negativ geladenen Teilchen besteht, die sehr viel leichter als ein Atom sind, stellte dann aber seine Forschungen hierzu ein. Im gleichen Jahr bestimmte Joseph John Thomson die Masse der Teilchen (er bezeichnete sie erst als "corpuscules") genauer und konnte nachweisen, dass es sich unabhängig vom Kathodenmaterial und vom Restgas in der Kathodenstrahlröhre immer um die gleichen Teilchen handelt. In dieser Zeit wurde anhand des Zeeman-Effektes nachgewiesen, dass diese Teilchen auch im Atom vorkommen und dort die Lichtemission verursachen. Damit war das Elektron als Elementarteilchen identifiziert.

Die Elementarladung wurde 1909 durch Robert Millikan gemessen.

Das Elektron ist das leichteste der elektrisch geladenen Elementarteilchen. Wenn die Erhaltungssätze für Ladung und Energie gelten – was aller physikalischen Erfahrung entspricht – müssen Elektronen daher stabil sein. In der Tat gibt es bisher keinerlei experimentellen Hinweis auf einen Elektronenzerfall.

Das Elektron gehört zu den Leptonen und hat wie alle Leptonen einen Spin (genauer: Spinquantenzahl) von 1/2. Als Teilchen mit halbzahligem Spin gehört es zur Klasse der Fermionen, unterliegt also insbesondere dem Pauli-Prinzip. Sein Antiteilchen ist das Positron, Symbol e, mit dem es bis auf seine elektrische Ladung in allen Eigenschaften übereinstimmt.

Einige der Grundeigenschaften des Elektrons, die in der oben stehenden Tabelle aufgelistet sind, werden durch das magnetische Moment des Elektronenspins miteinander verknüpft:

Dabei ist formula_2 das magnetische Moment des Elektronenspins, formula_3 die Masse des Elektrons, formula_4 seine Ladung und formula_5 der Spin. formula_6 heißt Landé- oder g-Faktor. Der Term vor formula_7, der das Verhältnis des magnetischen Moments zum Spin beschreibt, wird als gyromagnetisches Verhältnis des Elektrons bezeichnet. Für das Elektron wäre nach der Dirac-Theorie (relativistische Quantenmechanik) formula_6 exakt gleich 2. Effekte, die erst durch die Quantenelektrodynamik erklärt werden, bewirken jedoch eine messbare geringfügige Abweichung von 2. Diese Abweichung wird als "anomales magnetisches Moment des Elektrons" bezeichnet.

Kurz nach der Entdeckung des Elektrons versuchte man seine Ausdehnung abzuschätzen, insbesondere wegen der klassischen Vorstellung kleiner Billardkugeln, die bei Streuexperimenten aufeinanderstoßen. Die Argumentation lief darauf hinaus, dass die Konzentration der Elektronenladung auf eine sehr kleine Ausdehnung des Elektrons Energie benötige, die nach dem Äquivalenzprinzip in der Masse des Elektrons stecken müsse. Unter der Annahme, dass die Energie formula_9 eines Elektrons in Ruhe gleich der doppelten Selbstenergie formula_10 der Elektronenladung im eigenen elektrischen Feld sei, erhält man den klassischen Elektronenradius
formula_4: Elementarladung, formula_13: Kreiszahl, formula_14: Elektrische Feldkonstante, formula_3: Elektronenmasse, formula_16: Lichtgeschwindigkeit, formula_17: Feinstrukturkonstante, formula_18: Bohrscher Radius.

Die Selbstenergie trennt dabei gedanklich elektrische Ladung und elektrisches Feld des Elektrons. Setzt man die Ladung "−e" in das Potential formula_19, wobei man zum Beispiel ein zweites Elektron gleichmäßig auf eine Kugeloberfläche vom Radius formula_20 verteilt denkt, so ist dafür Energie nötig, die "Selbstenergie" eines einzigen Elektrons beträgt hiervon die Hälfte. Es gab jedoch durchaus auch andere Herleitungen für eine mögliche Ausdehnung des Elektrons, die auf andere Werte kamen.

Heute ist die Sichtweise bezüglich einer Ausdehnung des Elektrons eine andere: In den bisher möglichen Experimenten zeigen Elektronen weder Ausdehnung noch innere Struktur und können insofern als punktförmig angenommen werden. Die experimentelle Obergrenze für die Größe des Elektrons liegt derzeit bei etwa 10 m. Dennoch tritt der "klassische Elektronenradius" formula_20 in vielen Formeln auf, in denen aus den feststehenden Eigenschaften des Elektrons eine Größe der Dimension Länge (oder Fläche etc.) gebildet wird, um experimentelle Ergebnisse erklären zu können. Z. B. enthalten die theoretischen Formeln für die Wirkungsquerschnitte des Photo- und des Compton-Effekts das Quadrat von formula_20.

Auch die Suche nach einem elektrischen Dipolmoment des Elektrons blieb bisher ohne positiven Befund. Ein Dipolmoment würde entstehen, wenn bei einem nicht punktförmigen Elektron der Schwerpunkt der Masse nicht gleichzeitig der Schwerpunkt der Ladung wäre. So etwas wird von Theorien der Supersymmetrie, die über das Standardmodell der Elementarteilchen hinausgehen, vorhergesagt. Eine Messung im Oktober 2013, die das starke elektrische Feld in einem polaren Molekül ausnutzt, hat ergeben, dass ein eventuelles Dipolmoment mit einem Konfidenzniveau von 90 % nicht größer als 8,7·10 formula_4 m ist. Anschaulich bedeutet das, dass Ladungs- und Massenmittelpunkt des Elektrons nicht weiter als etwa 10 m auseinanderliegen können. Theoretische Ansätze, nach denen größere Werte vorhergesagt wurden, sind damit widerlegt.

Von der (eventuellen) Ausdehnung des Elektrons zu unterscheiden ist sein Wirkungsquerschnitt für Wechselwirkungsprozesse. Bei der Streuung von Röntgenstrahlen an Elektronen erhält man z. B. einen Streuquerschnitt von etwa formula_24, was der Kreisfläche mit dem oben beschriebenen klassischen Elektronenradius formula_25 entspräche. Im Grenzfall großer Wellenlängen, d. h. kleiner Photonenenergien, steigt der Streuquerschnitt auf formula_26 (siehe Thomson-Streuung und Compton-Effekt).

Viele physikalische Erscheinungen wie Elektrizität, Elektromagnetismus und elektromagnetische Strahlung beruhen im Wesentlichen auf Wechselwirkungen von Elektronen. Elektronen in einem elektrischen Leiter werden durch ein sich änderndes Magnetfeld verschoben und es wird eine elektrische Spannung induziert. Die Elektronen in einem stromdurchflossenen Leiter erzeugen ein Magnetfeld. Ein beschleunigtes Elektron - natürlich auch beim Fall der krummlinigen Bewegung - emittiert Photonen, die sogenannte Bremsstrahlung (Hertzscher Dipol, Synchrotronstrahlung, Freie-Elektronen-Laser).

In einem Festkörper erfährt das Elektron Wechselwirkungen mit dem Kristallgitter. Sein Verhalten lässt sich dann beschreiben, indem statt der Elektronenmasse die abweichende effektive Masse verwendet wird, die auch abhängig von der Bewegungsrichtung des Elektrons ist.

Elektronen, die sich in polaren Lösungsmitteln wie Wasser oder Alkoholen von ihren Atomen gelöst haben, werden als solvatisierte Elektronen bezeichnet. Bei Lösung von Alkalimetallen in Ammoniak sind sie für die starke Blaufärbung verantwortlich.

Ein Elektron ist ein Quantenobjekt, das heißt, bei ihm liegt die durch die Heisenbergsche Unschärferelation beschriebene Orts- und Impulsunschärfe im messbaren Bereich, so dass wie bei Licht sowohl Wellen- als auch Teilcheneigenschaften beobachtet werden können, was auch als Welle-Teilchen-Dualismus bezeichnet wird. In einem Atom kann das Elektron als stehende Materiewelle betrachtet werden.

Das Verhältnis "e/m" der Elektronenladung zur Elektronenmasse kann als Schulversuch mit dem Fadenstrahlrohr ermittelt werden. Die direkte Bestimmung der Elementarladung gelang durch den Millikan-Versuch.

Bei Elektronen, deren Geschwindigkeit nicht vernachlässigbar klein gegenüber der Lichtgeschwindigkeit ist, muss der nichtlineare Beitrag zum Impuls nach der Relativitätstheorie berücksichtigt werden. Elektronen mit ihrer geringen Masse lassen sich relativ leicht auf so hohe Geschwindigkeiten beschleunigen; schon mit einer kinetischen Energie von 80 keV hat ein Elektron die halbe Lichtgeschwindigkeit. Der Impuls lässt sich durch die Ablenkung in einem Magnetfeld messen. Die Abweichung des Impulses vom nach klassischer Mechanik berechneten Wert wurde zuerst von Walter Kaufmann 1901 nachgewiesen und nach der Entdeckung der Relativitätstheorie zunächst mit dem Begriff der „relativistischen Massenzunahme“ beschrieben, der aber inzwischen als überholt angesehen wird.

In der Kathodenstrahlröhre (Braunsche Röhre) treten Elektronen aus einer beheizten Glühkathode aus und werden im Vakuum durch ein elektrisches Feld in Feldrichtung (in Richtung der positiven Anode) beschleunigt. Durch Magnetfelder werden die Elektronen senkrecht zur Feldrichtung und senkrecht zur augenblicklichen Flugrichtung abgelenkt (Lorentzkraft). Diese Eigenschaften der Elektronen haben erst die Entwicklung des Oszilloskops, des Fernsehers und des Computermonitors ermöglicht.

Weitere Anwendungen freier Elektronen sind z. B. die Röntgenröhre, das Elektronenmikroskop, das Elektronenstrahlschweißen, physikalische Grundlagenforschung mittels Teilchenbeschleunigern und die Erzeugung von Synchrotronstrahlung für Forschungs- und technische Zwecke.



</doc>
<doc id="1257" url="https://de.wikipedia.org/wiki?curid=1257" title="Erg">
Erg

Erg steht für:

ERG steht als Abkürzung für


</doc>
<doc id="1258" url="https://de.wikipedia.org/wiki?curid=1258" title="Energie">
Energie

Energie (altgr. "en" „innen“ und "ergon" „Wirken“) ist eine fundamentale physikalische Größe, die in allen Teilgebieten der Physik sowie in der Technik, Chemie, Biologie und der Wirtschaft eine zentrale Rolle spielt. Ihre SI-Einheit ist "Joule". Energie ist die Größe, die aufgrund der Zeitinvarianz der Naturgesetze erhalten bleibt, das heißt, die Gesamtenergie eines abgeschlossenen Systems kann weder vermehrt noch vermindert werden (Energieerhaltungssatz). Viele einführende Texte definieren Energie in anschaulicher, allerdings nicht allgemeingültiger Form als Fähigkeit, Arbeit zu verrichten.

Eine Zufuhr von Energie ist unter anderem nötig, um einen Körper zu beschleunigen oder ihn entgegen einer Kraft zu bewegen, um eine Substanz zu erwärmen, ein Gas zusammenzudrücken, elektrischen Strom fließen zu lassen oder elektromagnetische Wellen abzustrahlen, sowie um im leeren Raum materielle Teilchen entstehen zu lassen. Lebewesen benötigen Energie, um leben zu können. Energie benötigt man auch für den Betrieb von Computersystemen, für Telekommunikation und für jegliche wirtschaftliche Produktion.

Energie kann in verschiedenen "Energieformen" vorkommen, beispielsweise als potentielle Energie, kinetische Energie, chemische Energie, elektrische Energie oder thermische Energie. Energie lässt sich von einem System zu einem anderen übertragen und von einer Form in eine andere umwandeln, jedoch setzt der zweite Hauptsatz der Thermodynamik bei der thermischen Energie eine prinzipielle Grenze: diese ist nur eingeschränkt zwischen Systemen übertragbar oder in andere Energieformen umwandelbar.

Durch die hamiltonschen Bewegungsgleichungen und die Schrödingergleichung bestimmt Energie die zeitliche Entwicklung physikalischer Systeme. Gemäß der Relativitätstheorie sind Ruheenergie und Masse durch die Äquivalenz von Masse und Energie (formula_1) verknüpft.

Das Wort "Energie" geht auf , "energeia" zurück, das in der griechischen Antike eine rein philosophische Bedeutung im Sinne von "lebendiger Wirklichkeit und Wirksamkeit" hatte (siehe auch „Akt und Potenz“). Als naturwissenschaftlicher Begriff wurde das Wort selbst erst 1807 von dem Physiker Thomas Young in die Mechanik eingeführt. Die neue Größe "Energie" sollte die Stärke ganz bestimmter Wirkungen angeben, die ein bewegter Körper durch seine Bewegung hervorrufen kann, und die sich nicht allein durch seinen Impuls formula_2 („Masse mal Geschwindigkeit“) bestimmen lassen. Über den Impuls war seit den Untersuchungen des Stoßes zweier Körper durch Christiaan Huygens, Christopher Wren und John Wallis um das Jahr 1668 herum bekannt, dass er bei elastischen wie bei unelastischen Körpern erhalten bleibt, also das richtige Maß für die verursachten Veränderungen und damit für die unzerstörbare „Größe der Bewegung“ ist. Bei anderen Vorgängen aber verursachen Körper verschiedener Masse, auch wenn sie gleichen Impuls haben, verschieden große Wirkungen. Dazu gehört etwa die Höhe, die ein Körper in Aufwärtsbewegung erreicht, oder die Tiefe des Lochs, das er beim Aufprall in eine weiche Masse schlägt. Hierbei nimmt die Wirkung nicht mit der Geschwindigkeit proportional zu, wie der Impuls, sondern mit dem Quadrat der Geschwindigkeit. Daher bezeichnete Gottfried Wilhelm Leibniz 1686 die Größe formula_3 als das wahre Maß für die Größe der Bewegung und nannte sie vis viva („"lebendige Kraft"“). Dieser Name folgte dem damaligen Sprachgebrauch, in dem ein Körper nur durch die ihm innewohnenden "Kräfte" Wirkungen verursachen konnte. Der Name "lebendige Kraft" hat aber durch „Verwechslung mit dem Newtonschen Kraftbegriff eine unheilvolle Verwirrung der Ideen und eine zahllose Schar von Missverständnissen hervorgerufen“ (so Max Planck 1887 in seiner preisgekrönten Darstellung der Geschichte des Energieerhaltungssatzes.) Leibniz argumentierte wie folgt:

Ein Gewicht von formula_4 auf die Höhe formula_5 zu heben erfordert genauso viel Arbeit wie ein Gewicht formula_6 auf die Höhe formula_7 zu heben (Hebelgesetz). Nach Galileo Galilei ist im freien Fall formula_8, also ist die Endgeschwindigkeit im ersten Fall doppelt so hoch wie im zweiten Fall. Setzt man für die "innewohnende" (lebendige) Kraft an formula_9, mit der man diese Arbeit ("latente Form der lebendigen Kraft") messen will, so ist bei Erhaltung der lebendigen Kraft formula_10, das heißt formula_11 und nicht formula_12 wie die Anhänger von Descartes meinten.

Den korrekten Vorfaktor formula_13 in der kinetischen Energie leitete schon Daniel Bernoulli 1726 ab. Bei ihm wie bei anderen analytischen Mechanikern des 18. Jahrhunderts wie Leonhard Euler (z. B. Behandlung der elastischen Deformation), Joseph Louis Lagrange (Mécanique Analytique 1788) finden sich auch Vorläufer des Konzepts der potentiellen Energie (der Term Potentialfunktion stammt von George Green 1828 und unabhängig wurde sie von Carl Friedrich Gauß 1840 eingeführt, war aber als Potential schon Lagrange und Laplace bekannt). Das Konzept war schon Leibniz (in seiner Ableitung von formula_3) und dessen Anhänger Johann Bernoulli bekannt, der als erster 1735 das Prinzip der Erhaltung der "lebendigen Kräfte" formulierte (die Vorstellung hatte aber auch Leibniz zum Beispiel im 5. Brief an Samuel Clarke), das insbesondere vom Leibniz-Schüler Christian Wolff verbreitet wurde. Von potentieller Energie sprach man damals als der "latenten Form der lebendigen Kraft", die sich zum Beispiel beim inelastischen Stoß auf kleinere Teilchen des Körpers verteile.

Um die genannten Wirkungen der Bewegung des Körpers vorhersagen zu können, definierte Young die Größe "Energie" als die Fähigkeit des Körpers, gegen eine widerstehende Kraft eine gewisse Strecke zurückzulegen. Er bemerkte auch, dass Arbeit, die in Form von Hubarbeit an einem Körper geleistet wird, sich später quantitativ in dessen Energie wiederfindet, kam aber noch nicht auf den Begriff der Umwandlung verschiedener Energieformen und behielt auch die Formel formula_15 von Leibniz bei und war im Großen und Ganzen noch ein Anhänger des Cartesianischen Standpunkts der "Kräfte" .

Im 18. Jahrhundert war man in der Mechanik und Physik an der Energie nicht sonderlich interessiert, wichtige Forscher wie Euler sahen den Streit um die "Vis Viva", das wahre Kraftmaß, als Angelegenheit der Philosophen und man befasste sich mit der Lösung der Bewegungsgleichungen vor allem in der Himmelsmechanik. Der Energiebegriff im heutigen Sinn fand seinen Ursprung nicht bei den analytischen Mechanikern des 18. Jahrhunderts, sondern bei den angewandten Mathematikern der französischen Schule, darunter Lazare Carnot, der schrieb, dass die "lebendige Kraft" sich entweder als formula_3 oder Kraft mal Weg (als latente lebendige Kraft) manifestieren kann. Eine quantitative Definition der Arbeit („Kraft mal Weg“, bzw. formula_17) wurde auch 1829 gleichzeitig von Coriolis und Poncelet gegeben, offenbar unabhängig voneinander und auch von Young. Coriolis fand dabei auch den richtigen Ausdruck formula_18 für die Bewegungsenergie, die 1853 von Rankine erstmals "kinetische Energie" genannt wurde.

Im Zusammenhang mit der Dampfmaschine entwickelte sich die Vorstellung, dass Wärmeenergie bei vielen Prozessen die Ursache für eine bewegende Energie, oder mechanische Arbeit verantwortlich ist. Ausgangspunkt war, dass Wasser durch Hitze in den gasförmigen Zustand überführt wird und die Gasausdehnung genutzt wird, um einen Kolben in einem Zylinder zu bewegen. Durch die Kraftbewegung des Kolbens vermindert sich die gespeicherte Wärmeenergie des Wasserdampfes. Demonstriert wurde der Zusammenhang von mechanischer Energie und Wärme in berühmt gewordenen Experimenten von Benjamin Thompson (Graf Rumford, München 1796, 1798) und Humphry Davy (1799).

Der Physiker Nicolas Carnot erkannte, dass beim Verrichten von mechanischer Arbeit eine Volumenänderung des Dampfs nötig ist. Außerdem fand er heraus, dass die Abkühlung des heißen Wassers in der Dampfmaschine nicht nur durch Wärmeleitung erfolgt. Diese Erkenntnisse veröffentlichte Carnot 1824 in einer viel beachteten Schrift über das Funktionsprinzip der Dampfmaschine. Émile Clapeyron brachte 1834 Carnots Erkenntnisse in eine mathematische Form und entwickelte die noch heute verwendete graphische Darstellung des Carnot-Kreisprozesses.

1841 veröffentlichte der deutsche Arzt Julius Robert Mayer seine Idee, dass Energie weder erschaffen noch vernichtet, sondern nur umgewandelt werden kann. Er schrieb an einen Freund: Die Wärmemenge, die bei einer Dampfmaschine verloren gegangen ist, entspräche genau der mechanischen Arbeit, die die Maschine leistet. Dies ist heute bekannt als „Energieerhaltung“, oder auch „Erster Hauptsatz der Thermodynamik“.
Der Physiker Rudolf Clausius verbesserte im Jahr 1854 die Vorstellungen über die Energieumwandlung. Er zeigte, dass nur ein Teil der Wärmeenergie in mechanische Arbeit umgewandelt werden kann. Ein Körper, bei dem die Temperatur konstant bleibt, kann keine mechanische Arbeit leisten. Clausius entwickelte den zweiten Hauptsatz der Thermodynamik und führte den Begriff der Entropie ein. Nach dem zweiten Hauptsatz ist es unmöglich, dass Wärme eigenständig von einem kälteren auf einen wärmeren Körper übergeht.

Hermann von Helmholtz formulierte im Jahr 1847 das Prinzip „über die Erhaltung der Kraft“ und der Unmöglichkeit eines Perpetuum mobiles (perpetuus, lat. ewig; mobilis, lat.: beweglich) 1. Art. Viele Erfinder wollten damals noch Maschinen herstellen, die mehr Energie erzeugten als hineingesteckt wurde. Helmholtz fand seine Erkenntnisse durch Arbeiten mit elektrischer Energie aus galvanischen Elementen, insbesondere einer Zink/Brom-Zelle. In späteren Jahren verknüpfte er die Entropie und die Wärmeentwicklung einer chemischen Umwandlung zur freien Energie. Sowohl Mayer als auch Helmholtz hatten aber in den 1840er Jahren Schwierigkeiten, ihre Erkenntnisse zu veröffentlichen, da beide zunächst als fachfremde Außenseiter galten und die Physiker in Deutschland in einer Abwehrhaltung gegen die seit Ende des 18. Jahrhunderts einflussreiche Naturphilosophie des Kreises um Schelling waren und man beide verdächtigte Anhänger dieser "spekulativen Physik" zu sein.

Josiah Gibbs kam im Jahr 1878 zu ähnlichen Erkenntnissen wie Helmholtz bei elektrochemischen Zellen. Chemische Reaktionen laufen nur ab, wenn die Freie Energie abnimmt. Mittels der freien Energie lässt sich voraussagen, ob eine chemische Stoffumwandlung überhaupt möglich ist oder wie sich das chemische Gleichgewicht einer Reaktion bei einer Temperaturänderung verhält.

Nachdem schon Wilhelm Wien (1900), Max Abraham (1902), und Hendrik Lorentz (1904) Überlegungen zur elektromagnetischen Masse publiziert hatten veröffentlichte Albert Einstein im Rahmen seiner speziellen Relativitätstheorie 1905 die Erkenntnis, dass Masse und Energie äquivalent sind.

Energie kann in einem System auf unterschiedliche Weise enthalten sein. Diese Möglichkeiten werden "Energieformen" genannt. Beispiele für Energieformen sind die kinetische Energie, die chemische Energie, die elektrische Energie oder die potentielle Energie. Verschiedene Energieformen können ineinander umgewandelt werden, wobei die Summe der Energiemengen über die verschiedenen Energieformen vor und nach der Energieumwandlung stets die gleiche ist.

Eine Umwandlung kann nur so erfolgen, dass auch alle anderen Erhaltungsgrößen des Systems vor und nach der Umwandlung den gleichen Wert besitzen. Beispielsweise wird die Umwandlung kinetischer Energie durch die Erhaltung des Impuls und des Drehimpuls des Systems eingeschränkt. Ein Kreisel kann nur dann abgebremst werden und damit Energie verlieren, wenn er gleichzeitig Drehimpuls abgibt. Auch auf molekularer Ebene gibt es solche Einschränkungen. Viele chemische Reaktionen, die energetisch möglich wären, laufen nicht spontan ab, weil sie die Impulserhaltung verletzen würden. Weitere Erhaltungsgrößen sind die Zahl der Baryonen und die Zahl der Leptonen. Sie schränken die Umwandlung von Energie durch Kernreaktionen ein. Die Energie, die in der Masse von Materie steckt lässt sich nur mit einer gleich großen Menge von Antimaterie vollständig in eine andere Energieform umwandeln. Ohne Antimaterie gelingt die Umwandlung mit Hilfe von Kernspaltung oder Kernfusion nur zu einem kleinen Teil.

Die Thermodynamik gibt mit dem zweiten Hauptsatz der Thermodynamik eine weitere Bedingung für eine Umwandlung vor: Die Entropie eines abgeschlossenen Systems kann nicht abnehmen. Entnahme von Wärme, ohne dass parallel andere Prozesse ablaufen, bedeutet eine Abkühlung. Eine niedrigere Temperatur entspricht jedoch einer verminderten Entropie und steht damit im Widerspruch zum zweiten Hauptsatz. Um dennoch Wärme in eine andere Energieform umzuwandeln, muss im Gegenzug zur Abkühlung ein anderer Teil des Systems erwärmt werden. Die Umwandlung von thermischer Energie in andere Energieformen setzt daher immer eine Temperaturdifferenz voraus. Außerdem kann nicht die gesamte in der Temperaturdifferenz gespeicherte Wärmemenge umgesetzt werden. Wärmekraftmaschinen dienen dazu Wärme in mechanische Energie umzuwandeln. Das Verhältnis der durch den zweiten Hauptsatz gegebenen maximal möglichen Arbeit zur verbrauchten Wärmemenge wird Carnot-Wirkungsgrad genannt. Er ist umso größer, je größer die Temperaturdifferenz ist, mit der die Wärmekraftmaschine arbeitet.

Andere Umwandlungen sind nicht so stark von den Einschränkungen durch Erhaltungssätze und Thermodynamik betroffen. So lässt sich elektrische Energie mit wenig technischem Aufwand nahezu vollständig in viele andere Energieformen überführen. Elektromotoren wandeln sie beispielsweise in kinetische Energie um.

Die meisten Umwandlungen erfolgen nicht vollständig in eine einzige Energieform, sondern es wird ein Teil der Energie in Wärme gewandelt. In mechanischen Anwendungen wird die Wärme meist durch Reibung erzeugt. Bei elektrischen Anwendungen sind häufig der elektrische Widerstand oder Wirbelströme die Ursache für die Erzeugung von Wärme. Diese Wärme wird in der Regel nicht genutzt und als Verlust bezeichnet. Im Zusammenhang mit elektrischem Strom kann auch die Abstrahlung elektromagnetischer Wellen als unerwünschter Verlust auftreten. Das Verhältnis zwischen erfolgreich umgewandelter Energie und eingesetzter Energie wird Wirkungsgrad genannt.

Bei technischen Anwendungen wird häufig eine Reihe von Energieumwandlungen gekoppelt. In einem Kohlekraftwerk wird zunächst die chemische Energie der Kohle durch Verbrennung in Wärme umgesetzt und auf Wasserdampf übertragen. Turbinen wandeln die Wärme des Dampfs in mechanische Energie um und treiben wiederum Generatoren an, die die mechanische Energie in elektrische Energie umwandeln.

In der klassischen Mechanik ist die Energie eines Systems seine Fähigkeit, Arbeit zu leisten. Die Arbeit wandelt Energie zwischen verschiedenen Energieformen um. Die spezielle Form der newtonschen Gesetze gewährleistet, dass sich dabei die Summe aller Energien nicht ändert. Reibung und die mit ihr einhergehenden Energieverluste sind in dieser Betrachtung nicht berücksichtigt.

Das Noether-Theorem erlaubt eine allgemeinere Definition der Energie, die den Aspekt der Energieerhaltung automatisch berücksichtigt. Alle Naturgesetze der klassischen Mechanik sind invariant in Bezug auf Verschiebungen in der Zeit. Sie zeichnen sich dadurch aus, dass sie zu allen Zeiten unverändert in der gleichen Form gelten. Das Noether-Theorem besagt nun, dass es zu dieser Symmetrie in Bezug auf Verschiebung in der Zeit eine physikalische Größe gibt, deren Wert sich nicht mit der Zeit verändert. Diese Größe ist die Energie.

Aus dem Energieerhaltungssatz und unvermeidlichen Energieverlusten durch Reibung folgt, dass es unmöglich ist, eine mechanische Maschine zu bauen, die von sich aus beliebig lange läuft (Perpetuum Mobile). Außerdem erlaubt die Energieerhaltung zusammen mit der Impulserhaltung Aussagen über das Ergebnis von Stößen zwischen Objekten, ohne dass der genaue Mechanismus beim Stoß bekannt sein muss.

Die kinetische Energie formula_19 ist diejenige Energie, die dem Bewegungszustand eines Körpers innewohnt. Sie ist proportional zur Masse formula_4 und zum Quadrat der Geschwindigkeit formula_21 relativ zu dem Inertialsystem, in dem man den Körper beschreibt.

Der Betrag der kinetischen Energie ist also von dem Standpunkt abhängig, von dem aus man das System beschreibt. Häufig verwendet man ein Inertialsystem, das in Bezug auf den Erdboden ruht.

Ein ausgedehnter Körper kann neben einer Translationsbewegung auch eine Drehbewegung durchführen. Die kinetische Energie, die in der Drehbewegung steckt, nennt man Rotationsenergie. Diese ist proportional zum Quadrat der Winkelgeschwindigkeit und zum Trägheitsmoment des Körpers.

Potentielle Energie, auch "Lageenergie" genannt, kommt einem Körper durch seine Lage in einem Kraftfeld zu, sofern es sich um eine konservative Kraft handelt. Dies könnte beispielsweise das Erdschwerefeld oder das Kraftfeld einer Feder sein. Die potentielle Energie nimmt in Kraftrichtung ab und entgegen der Kraftrichtung zu, senkrecht zur Kraftrichtung ist sie konstant. Bewegt sich der Körper von einem Punkt, an dem er eine hohe potentielle Energie hat, zu einem Punkt, an dem diese geringer ist, leistet er genau so viel physikalische Arbeit, wie sich seine potentielle Energie vermindert hat. Diese Aussage gilt unabhängig davon, auf welchem Weg der Körper vom einen zum anderen Punkt gelangt ist.

Die potentielle Energie eines Körpers mit der Masse formula_4 in einem homogenen Gravitationsfeld mit Gravitationsbeschleunigung formula_24 ist proportional zur Höhe formula_7 über dem Ursprung des Koordinatensystems:

Beim freien Fall wird diese potentielle Energie in kinetische Energie umgewandelt, indem der Körper beschleunigt wird.

Da der Koordinatenursprung beliebig gewählt werden kann, ist die Lageenergie des Körpers niemals absolut gegeben und auch nicht messbar. Messbar sind nur ihre Änderungen.

Bei periodischen Bewegungen wird regelmäßig potentielle in kinetische Energie und wieder zurück in potentielle Energie verwandelt. Beim Pendel ist beispielsweise an den Umkehrpunkten die potentielle Energie maximal; die kinetische Energie ist hier null. Wenn der Faden gerade senkrecht hängt, erreicht die Masse ihre maximale Geschwindigkeit und damit auch ihre maximale kinetische Energie; die potentielle Energie hat hier ein Minimum. Ein Planet hat bei seinem sonnenfernsten Punkt zwar die höchste potentielle, aber auch die geringste kinetische Energie. Bis zum sonnennächsten Punkt erhöht sich seine Bahngeschwindigkeit gerade so sehr, dass die Zunahme der kinetischen Energie die Abnahme der potentiellen Energie genau kompensiert.

Elastische Energie ist die potentielle Energie der aus ihrer Ruhelage verschobenen Atome oder Moleküle in einem elastisch deformierten Körper, beispielsweise einer mechanischen Feder. Allgemein bezeichnet man die Energie, die bei der elastischen oder plastischen Verformung in dem Körper gespeichert (oder freigesetzt) wird, als Deformationsenergie.

"Thermische Energie" ist die Energie, die in der ungeordneten Bewegung der Atome oder Moleküle eines Stoffes gespeichert ist. Sie wird umgangssprachlich auch als „Wärmeenergie“ oder „Wärmeinhalt“ bezeichnet. Die Umwandlung thermischer Energie in andere Energieformen wird durch die Thermodynamik beschrieben. Hier wird zwischen der im System enthaltenen Energie (innere Energie, Enthalpie) und der Wärme, der über die Systemgrenze transportierten thermischen Energie, unterschieden.

Die Summe aus thermischer Energie, Schwingungsenergie im Körper und Bindungsenergie bezeichnet man als "Innere Energie". Dabei wird in manchem Quellen auch zwischen der "thermischen inneren Energie", der "chemischen inneren Energie" und der "Kernenergie als innerer Energie" unterschieden, was aber den Rahmen der Thermodynamik verlässt.
Während alle Energieformen unter gewissen Bedingungen (siehe #Energieformen und Energieumwandlung) vollständig in thermische Energie umgewandelt werden können (erster Hauptsatz der Thermodynamik), gilt das in umgekehrter Richtung nicht. Der zweite Hauptsatz der Thermodynamik beschreibt hier eine ganz wesentliche Einschränkung (Bild 1). Abhängig von der Temperatur, bei der die Wärme zur Verfügung steht, lässt sich nur ein mehr oder weniger großer Anteil über einen Kreisprozess in mechanische Arbeit umwandeln, während der Rest an die Umgebung abgegeben wird. In der technischen Thermodynamik werden die umwandelbaren Anteile einer Energieform auch als Exergie bezeichnet. Die Exergie ist keine Zustandsgröße im eigentlichen Sinne, denn sie hängt nicht nur vom Zustand des Systems ab, sondern auch vom Zustand der Umgebung, der im Einzelfall gegeben ist, im Allgemeinen angenommen werden muss. Dann lässt sich anhand von Exergie-Flussbildern einer Energie-Wandlungskette verfolgen, wo vermeidbare Verluste (Reibung oder andere dissipative Vorgänge) zu verzeichnen sind. In Bild 2 erkennt man, dass bei der Umwandlung von chemischer Energie (100 % Exergie) in Wärme bei einer mittleren Temperatur von 1000 °C der Exergie-Anteil nur noch 80 % beträgt. Wird diese Energie als Wärme in einem Dampfkessel auf Wasserdampf mit 273 °C übertragen, so verbleiben nur noch ca. 50 % und bei der Übertragung in einen mit 20 °C beheizten Raum nur noch etwa 7 %. Dabei wurde stets eine Umgebungstemperatur von 0 °C angenommen.

Bei der Berechnung des exergetischen Anteils von thermischer Energie ist zu berücksichtigen, ob die Wärmequelle eine konstante Temperatur besitzt, wie das in einem Siedewasser-Reaktor bei circa 270 °C der Fall ist, oder ob die Wärmeabgabe aus einem sich abkühlenden Medium, Rauchgas, erfolgt. Im ersten Fall kann der exergetische Anteil über den Carnot-Wirkungsgrad aus der oberen Prozess-Temperatur und der Umgebungstemperatur bestimmt werden, andernfalls erhält man die Wärme und die Exergie aus dem Flächenintegral, das aus dem T-S-Diagramm in Bild 3 und aus dem T-s-Diagramm in Bild 4 erkennbar ist.
Die Formel lautet:

Die Beziehung kann auch direkt aus den Diagrammen abgelesen werden. Hierbei sind: T die absolute Temperatur in K, S die Entropie in J/K, H die Enthalpie in J, Index 1: Ausgangszustand, Index U: Umgebungszustand.

Die Enthalpie-Differenz ist im Wesentlichen (in diesem Falle) die aus dem Brennstoff der Verbrennungsluft als Wärme zugeführte Energie. Sie erscheint als Fläche unter der Kurve der isobaren Wärmezufuhr. Der exergetische Anteil liegt oberhalb der Umgebungstemperatur, der andere nicht verwertbare Anteil, der „Anergie“ genannt wird, unterhalb dieser Linie. Bei der Abnahme der Exergie in einer Energie-Umwandlungskette spricht man auch von einer Energieentwertung.

Bei der Übertragung der Wärme aus dem Rauchgas auf das Arbeitsmedium, das Wasser, das dabei verdampft und überhitzt wird, entsteht ein weiterer Exergieverlust. Die maximale aus dem Dampfmassenstrom gewinnbare mechanische Leistung darf für einen Prozess mit Heißdampf von beispielsweise 16 bar und 350 °C keinesfalls über den Carnot-Wirkungsgrad mit dieser Temperatur berechnet werden. Das Ergebnis mit einem Wirkungsgrad von 52 % wäre falsch. Es würde dem zweiten Hauptsatz widersprechen, da die mittlere Temperatur der Wärmezufuhr in den Wasser-Dampf-Kreislauf niedriger ist. Erfolgt keine interne Wärmeübertragung (regenerative Speisewasservorwärmung) aus kondensierendem Dampf auf das Speisewasser, wie bei Dampfmaschinen, bei denen im theoretisch günstigsten Fall der Dampf reversibel auf Wasser mit Umgebungszustand gebracht werden kann, so erreicht man bei 15 °C Umgebungstemperatur nur einen maximalen Wirkungsgrad von 34,4 %. Der reversibel geführte Clausius-Rankine-Prozess in Bild 4 mit einem Dampfdruck von 32 bar und Kondensation bei 24 °C erreicht dagegen 37,2 %. Die realen Prozesse erreichen bei diesen Dampfparametern nur weitaus niedrigere Wirkungsgrade.

In Bild 5 ist ein vereinfachtes Energieflussbild der Stromerzeugung durch ein großes Dampfkraftwerk (Frischdampfzustand 260 bar, 545 °C, Speisewasservorwärmung auf 276 °C) mit der Verteilung bis zum Endverbraucher einem entsprechenden Exergieflussbild gegenübergestellt. Man erkennt daraus, dass ein wesentlicher Teil der Energieentwertung nicht im Kondensator oder im nachgeschalteten Kühlturm des Kraftwerkes erfolgt, wo die Abwärme abgeführt wird, sondern bei der Umwandlung der chemischen Energie des Brennstoffes in thermische Energie (Verbrennung) und bei der Wärmeübertragung vom Rauchgas auf den Wasserdampf. Die Zahlenwerte für die Stromverteilung sind Anhaltswerte, sie können im Einzelfall geringfügig abweichen.

Auch die Sonnenenergie, die durch Strahlung auf die Erde gelangt, erfährt auf dem Weg bis zur Erdoberfläche einen Exergieverlust. Während die innere Energie der Sonne bei rund 15 Millionen K noch praktisch aus reiner Exergie besteht, strahlt die Sonne mit einer Oberflächentemperatur von rund 6000 K auf die Erdoberfläche, deren Temperatur mit ca. 300 K anzusetzen ist. Durch Konzentration der Sonnenstrahlen in einem Kollektor käme man also – auch im Hochgebirge, wo die Absorption durch die Erdatmosphäre kaum eine Rolle spielt – über die Temperatur der Sonnenoberfläche nicht hinaus. Es ergäbe sich über den Carnot-Faktor ein Wirkungsgrad von ca. 95 %. Dann würde allerdings keine Energie mehr übertragen. Das thermodynamische Limit liegt darunter bei einer Absorbertemperatur von 2500 K mit einem Wirkungsgrad von ca. 85 %. In der Praxis kommen dissipative Verluste hinzu, angefangen von der Absorption in der Atmosphäre, über die Materialeigenschaften der kristallinen Zellen bis zum ohmschen Widerstand der Fotovoltaikanlagen, sodass bis heute nur Wirkungsgrade von weniger als 20 % erreicht werden können. Der höchste derzeit erreichte Wirkungsgrad ist 18,7 %.

Zum Heizen wird meist Wärme mit nur einem geringen Exergieanteil benötigt. Deshalb ist das Heizen mit elektrischem Strom über eine Widerstandsheizung „Energieverschwendung“. Überall dort, wo mechanische Energie oder Strom aus Wärme erzeugt wird und gleichzeitig Wärmebedarf existiert, ist die Nutzung der Abwärme zum Heizen sinnvoller als die getrennte Bereitstellung von Wärme. In einem Heizkraftwerk wird, wenn es mit Dampf betrieben wird, Dampf aus der Turbine entnommen, dessen Temperatur gerade noch ausreichend hoch ist, um die Kondensationswärme über ein Fernwärmenetz zum Verbraucher zu leiten. Alternativ wird auch in Blockheizkraftwerken (BHKW) die Abwärme von stationären Verbrennungsmotoren genutzt. Auch die Wärmepumpe ist hier zu nennen. Sie wendet Arbeit auf, um Wärme (Energie) aus der Umgebung aufzunehmen und zusammen mit der Antriebsarbeit als Heizwärme bei entsprechend hoher Temperatur abzugeben. Wenn Grundwasser mit 10 °C als Wärmequelle zur Verfügung steht und ein Raum mit 20 °C zu beheizen ist, könnte eine Wärmepumpe mit Carnot-Prozess durch Einsatz von einer Kilowattstunde Antriebsarbeit 29 KWh Wärme liefern (Arbeitszahl =29). Reale Wärmepumpen, die mit wechselweise verdampfenden und kondensierenden Kältemitteln bei unterschiedlichen Drücken betrieben werden, erreichen Arbeitszahlen von ca. 3 bis 5.

Als chemische Energie wird die Energieform bezeichnet, die in Form einer chemischen Verbindung in einem Energieträger gespeichert ist und bei chemischen Reaktionen freigesetzt werden kann. Sie beschreibt also die Energie, die mit elektrischen Kräften in Atomen und Molekülen verbunden ist und kann unterteilt werden in einerseits kinetischer Energie der Elektronen in den Atomen und andererseits der elektrischen Energie der Wechselwirkung von Elektronen und Protonen.

Sie wird bei exothermen Reaktionen frei und muss für endotherme Reaktionen hinzugefügt werden.

In einem elektrischen Feld kann, sofern kein zeitlich veränderliches Magnetfeld vorliegt, ein elektrisches Potential definiert werden. Ein Ladungsträger besitzt dann eine potentielle elektrische (elektrostatische) Energie, die proportional zum Potential und zu seiner Ladungsmenge ist. Da der Nullpunkt des Potentials frei festgelegt werden kann, ist auch die Energie nicht absolut definiert. Für zwei Punkte im Potentialfeld ist aber die Differenz der Energien unabhängig von der Wahl des Potentialnullpunktes. Potentialdifferenzen entsprechen in der Elektrotechnik Spannungen; als Nullpunkt der Potentialskala wird üblicherweise das Potential der Erde gewählt.

Für Anordnungen zweier elektrischer Leiter ist die elektrostatische Energie proportional zum Quadrat der Differenz der elektrischen Potentiale der beiden Leiter. Das Doppelte der Proportionalitätskonstante nennt man elektrische Kapazität. Kondensatoren sind elektrotechnische Bauelemente, die hohe Kapazität besitzen und daher Energie speichern können.

Äquivalent zu der Sichtweise, dass die elektrostatische Energie von Ladungen getragen wird, ist die Interpretation, dass sich die Energie auf den leeren Raum zwischen den Ladungen verteilt. Die Energiedichte, also die Energie pro Volumenelement, ist bei dieser Betrachtungsweise proportional zum Quadrat der elektrischen Feldstärke. Befindet sich in dem elektrischen Feld ein Dielektrikum, so ist die Energie außerdem proportional zur Dielektrizitätskonstante.

Bewegt sich eine Ladung im Vakuum zu einem Ort, an dem ein geringeres elektrisches Potential herrscht, erhöht sich die kinetische Energie der Ladung gerade so viel, wie die potentielle Energie geringer wird. Dies geschieht beispielsweise mit Elektronen in einer Elektronenröhre, in einer Röntgenröhre oder in einem Kathodenstrahlröhrenbildschirm. Bewegt sich eine Ladung dagegen entlang eines Potentialgefälles in einem Leiter, gibt sie ihre aufgenommene Energie sofort in Form von Wärme an das Leitermedium ab. Die Leistung ist dabei proportional zum Potentialgefälle und zur Stromstärke.

Elektrische Energie kann transportiert werden, indem sich Ladungsträger ohne nennenswertes Potentialgefälle entlang von Leitern bewegen. Dies ist beispielsweise in Freileitungen oder in Stromkabeln der Fall, mit deren Hilfe elektrische Energie vom Kraftwerk bis zum Verbraucher fließt.

Magnetische Energie ist in magnetischen Feldern wie im supraleitenden magnetischen Energiespeicher enthalten.

In einem idealen elektrischen Schwingkreis gespeicherte Energie wandelt sich fortlaufend zwischen der elektrischen Form und der magnetischen Form. Zu jedem Zeitpunkt ist die Summe der Teilenergien gleich (Energieerhaltung). Hierbei hat der reine magnetische respektive elektrische Anteil der Energie die doppelte Frequenz der elektrischen Schwingung.

Nach der speziellen Relativitätstheorie entspricht der Masse formula_4 eines ruhenden Objekts eine Ruheenergie von

Die Ruheenergie ist somit bis auf den Faktor formula_30 (Quadrat der Lichtgeschwindigkeit formula_31) der Masse äquivalent. Die Ruheenergie kann bei bestimmten Vorgängen in andere Energieformen umgewandelt werden und umgekehrt. So haben die Reaktionsprodukte der Kernspaltung und der Kernfusion messbar niedrigere Massen als die Ausgangsstoffe. In der Elementarteilchenphysik wird umgekehrt auch die Erzeugung von Teilchen und damit von Ruheenergie aus anderen Energieformen beobachtet.

In der klassischen Mechanik wird die Ruheenergie nicht mitgerechnet, da sie ohne Belang ist, solange sich Teilchen nicht in andere Teilchen umwandeln.

Die allgemeine Relativitätstheorie verallgemeinert das Konzept der Energie weiter und enthält eine einheitliche Darstellung von Energien und Impulsen als Quellen für Raumkrümmungen über den Energie-Impuls-Tensor. Aus diesem lassen sich durch Kontraktionen die für einen Beobachter messbaren Größen wie Energiedichte gewinnen. Für die Untersuchung der Entwicklung von Raumzeiten ist der Energieinhalt entscheidend. So kann man aus Energiebedingungen den Kollaps der Raumzeit zu einer Singularität vorhersagen.

In der Quantenmechanik bestimmt der Hamiltonoperator, welche Energie an einem physikalischen System gemessen werden kann. Gebundene Zustände des Systems können dabei nur diskreten, also nicht beliebigen Energiewerten entsprechen. Deshalb haben die bei Übergängen zwischen diesen Zuständen emittierten Teilchen oder Strahlen Linienspektren. 

Die Quantelung der Energie tritt bei elektromagnetischen Wellen auf: Eine Welle der Frequenz formula_32 kann Energie nur in Paketen formula_33 abgeben, wobei formula_7 das plancksche Wirkungsquantum ist.

Grundsätzlich ist eine "Energieerzeugung" schon aufgrund des Energieerhaltungssatzes nicht möglich. Der Begriff wird im Wirtschaftsleben aber dennoch verwendet, um die Erzeugung einer bestimmten Energieform (zum Beispiel elektrischer Strom) aus einer anderen Form (zum Beispiel chemischer Energie in Form von Kohle) auszudrücken. Analog gibt es im strengen physikalischen Sinne auch keinen Energieverbrauch, wirtschaftlich gemeint ist damit aber der Übergang von einer gut nutzbaren Primärenergie (zum Beispiel Erdöl, Gas, Kohle) in eine nicht mehr weiter nutzbare Energieform (zum Beispiel Abwärme in der Umwelt). Vom "Energiesparen" ist die Rede, wenn effizientere Prozesse gefunden werden, die weniger Primärenergie für denselben Zweck benötigen, oder anderweitig, zum Beispiel durch Konsumverzicht, der Primärenergieeinsatz reduziert wird.

Die Physik beschreibt den oben salopp eingeführten „Energieverbrauch“ mit dem exakten Begriff der Entropiezunahme. Während in einem abgeschlossenen System die Energie stets erhalten bleibt, nimmt die Entropie mit der Zeit stets zu oder bleibt bestenfalls konstant. Je höher die Entropie, desto schlechter nutzbar ist die Energie. Statt von Entropiezunahme kann man anschaulich auch von Energieentwertung sprechen.

Das Gesetz der Entropiezunahme verhindert insbesondere, Wärmeenergie direkt in Bewegungs- oder elektrische Energie umzuwandeln. Stattdessen sind immer eine Wärmequelle und eine Wärmesenke (= Kühlung) erforderlich. Der maximale Wirkungsgrad kann gemäß Carnot aus der Temperaturdifferenz berechnet werden.

Der Grenzfall einer Energieumwandlung ohne Entropiezunahme wird als "reversibler Prozess" bezeichnet. Als Beispiel einer nahezu reversiblen Energieumwandlung sei ein Satellit auf einer elliptischen Umlaufbahn um die Erde genannt: Am höchsten Punkt der Bahn hat er hohe potentielle Energie und geringe kinetische Energie, am niedrigsten Punkt der Bahn ist es genau umgekehrt. Die Umwandlung kann hier ohne nennenswerte Verluste tausendfach im Jahr erfolgen. In supraleitenden Resonatoren kann Energie millionen- oder gar milliardenfach pro Sekunde zwischen Strahlungsenergie und elektrischer Energie hin- und hergewandelt werden, ebenfalls mit Verlusten von weniger als einem Promille pro Umwandlung.

Bei vielen Prozessen, die in der Vergangenheit noch mit hohen Verlusten ergo erheblicher Entropiezunahme verbunden waren, ermöglicht der technologische Fortschritt zunehmend geringere Verluste. So verwandelt eine Energiesparlampe oder LED elektrische Energie wesentlich effizienter in Licht als eine Glühlampe. Eine Wärmepumpe erzeugt durch Nutzung von Wärme aus der Umwelt bei einer bestimmten elektrischen Leistung oft vielfach mehr Wärme als ein herkömmliches Elektroheizgerät bei gleicher Leistung. In anderen Bereichen liegt der Stand der Technik aber schon seit geraumer Zeit nah am theoretischen Maximum, so dass hier nur noch kleine Fortschritte möglich sind. So verwandeln gute Elektromotoren über 90 Prozent der eingesetzten elektrischen Energie in nutzbare mechanische Energie und nur einen kleinen Teil in nutzlose Wärme.

Energiesparen bedeutet im physikalischen Sinn, die Energieentwertung und Entropiezunahme bei der Energieumwandlung oder Energienutzung zu minimieren.

"Spezifisch" heißt in den Naturwissenschaften „auf eine bestimmte Bemessungsgrundlage bezogen“ ("Bezogene Größe"). Die "spezifische Energie" wird auf eine gewisse Eigenschaft eines Systems bezogen, die durch eine physikalische Größe beschrieben werden kann.

Nach DIN 5485 ist die "spezifische Energie" speziell massenbezogen, und die "volumetrische Energiedichte" die dimensional bezogene Bezeichnung.
Nicht als "spezifisch", sondern als "molar" bezeichnet die Thermodynamik und Chemie stoffbezogene Energiewerte:

Mit Energieversorgung und -verbrauch wird die Nutzung von verschiedenen Energien in für Menschen gut verwendbaren Formen bezeichnet. Die von Menschen am häufigsten benutzten Energieformen sind Wärmeenergie und elektrische Energie. Die menschlichen Bedürfnisse richten sich vor allem auf die Bereiche Heizung, Nahrungszubereitung und den Betrieb von Einrichtungen und Maschinen zur Lebenserleichterung. Hierbei ist das Thema Fortbewegung und der Verbrauch zum Beispiel fossiler Energiequellen in Fahrzeugen bedeutsam.

Die verschiedenen Energieträger können über Leitungen die Verbraucher erreichen, wie typischerweise elektrische Energie, Erdgas, Fernwärme und Nahwärme, oder sie sind weitgehend lagerfähig und beliebig transportfähig, wie zum Beispiel Steinkohle und Braunkohlen, Heizöle, Kraftstoffe (Benzine, Dieselkraftstoffe), Industriegase, Kernbrennstoffe (Uran), Biomassen (Holz).

Der Energiebedarf ist weltweit sehr unterschiedlich und in den Industrieländern um ein Vielfaches höher als zum Beispiel in der Dritten Welt (siehe Liste der Staaten mit dem höchsten Energieverbrauch). In industriell hoch entwickelten Ländern haben sich seit dem 19. Jahrhundert Unternehmen mit der Erzeugung und Bereitstellung von Energie für den allgemeinen Verbrauch beschäftigt. Hierbei steht die zentrale Erzeugung von elektrischer Energie sowie die Übertragung an die einzelnen Verbraucher im Vordergrund. Weiterhin sind die Beschaffung, der Transport und die Verwandlung von Brennmaterial zu Heizzwecken wichtige Wirtschaftszweige.

Etwa 40 Prozent des weltweiten Energiebedarfes wird durch elektrische Energie gedeckt. Spitzenreiter innerhalb dieses Anteils sind mit rund 20 Prozent elektrische Antriebe. Danach ist die Beleuchtung mit 19 Prozent, die Klimatechnik mit 16 Prozent und die Informationstechnik mit 14 Prozent am weltweiten elektrischen Energiebedarf beteiligt.

Neben der abgeleiteten SI-Einheit Joule sind je nach Anwendungsgebiet noch andere Energieeinheiten in Gebrauch. Wattsekunde (Ws) und Newtonmeter (Nm) sind mit dem Joule identisch. 

Das Elektronenvolt (eV) wird in der Atomphysik, der Kernphysik und der Elementarteilchenphysik zur Angabe von Teilchenenergien und Energieniveaus verwendet. Seltener kommt in der Atomphysik das Rydberg vor. Die cgs-Einheit erg wird häufig in der theoretischen Physik benutzt.

Die Kalorie war in der Kalorimetrie üblich und wird heute noch umgangssprachlich und im Warenverkehr zusätzlich zur gesetzlichen Einheit Joule bei der Angabe des physiologischen Brennwerts von Lebensmitteln verwendet. In Kilowattstunden (kWh) messen Energieversorger die Menge der an die Kunden gelieferten Energie. Die Steinkohleeinheit und die Öleinheit dienen zur Angabe des Energieinhaltes von Primärenergieträgern. Mit dem TNT-Äquivalent misst man die Sprengkraft von Sprengstoffen.

In der folgenden Umrechnungstabelle ist jeweils die links angegebene Einheit gleich der Zahl mal der oben angegebenen Einheit:
Energie ist eine Größe, die auch im Alltag einen um viele Größenordnungen unterschiedlichen Wert annehmen kann. Beispiele sind:
















</doc>
<doc id="1259" url="https://de.wikipedia.org/wiki?curid=1259" title="Elektronenvolt">
Elektronenvolt

Das Elektronenvolt, auch Elektronvolt, ist eine Einheit der Energie, die in der Atom-, Kern- und Teilchenphysik häufig benutzt wird. Ihr Einheitenzeichen ist eV.

Wird ein Elektron in einem elektrischen Feld beschleunigt, so ändert sich seine kinetische Energie um genau ein Elektronvolt, wenn die Beschleunigungsspannung 1 Volt beträgt. In der SI-Einheit Joule ausgedrückt ist sein Wert gemäß der CODATA-Empfehlung:

Die eingeklammerten Ziffern geben die Unsicherheit in den letzten Stellen des Wertes als geschätzte Standardabweichung des angegebenen Zahlenwertes vom tatsächlichen Wert an. Diese beträgt somit 9,8 · 10 J.

Das Elektronenvolt gehört zwar nicht wie das Joule zum Internationalen Einheitensystem, ist aber zum Gebrauch mit ihm zugelassen und eine gesetzliche Maßeinheit.

Die Einheit wird in der deutschsprachigen Fachliteratur weit überwiegend als „Elektronenvolt“ bezeichnet, also mit dem Morphem „en“ zwischen „Elektron“ und „volt“. Andererseits sieht die "Ausführungsverordnung zum Gesetz über die Einheiten im Messwesen und die Zeitbestimmung" vom 13. Dezember 1985 die Form „Elektronvolt“ vor.

Die DIN-Norm 1301-1 „Einheiten – Einheitennamen, Einheitenzeichen“ vom Oktober 2010 empfiehlt die Form „Elektronvolt“. In der Norm DIN 66030 „Informationstechnik – Darstellung von Einheitennamen in Systemen mit beschränktem Schriftzeichenvorrat“ vom Mai 2002 wird dagegen die Form „Elektronenvolt“ verwendet.

Das Elektronenvolt wird als „handliche“ Einheit der Energie in der Atomphysik und verwandten Fachgebieten wie der experimentellen Kern- und Elementarteilchenphysik verwendet. Beispielsweise wird die kinetische Energie, auf die ein Teilchen in einem Teilchenbeschleuniger gebracht wird, stets in Elektronenvolt angegeben. Handlich ist das deshalb, weil sich die Änderung der kinetischen Energie formula_2 jedes im elektrischen Feld beschleunigten Teilchens aus seiner Ladung formula_3 und der durchlaufenen Spannung formula_4 als formula_5 berechnen lässt und unabhängig von anderen Einflüssen ist: Die Masse des Teilchens, die Länge des Weges oder der genaue räumliche Verlauf der Feldstärke spielen keine Rolle.

Der Betrag der Ladung eines freien, beobachtbaren Teilchens ist immer die Elementarladung formula_6 oder ein ganzzahliges Vielfaches davon. Anstatt die Elementarladung einzusetzen und die Energie in Joule anzugeben, kann man daher die aus einer elektrischen Beschleunigung resultierende Änderung der kinetischen Energie direkt in der Einheit eV angeben. Dabei gilt die Formel formula_7 nur für einfach geladene Teilchen wie Elektronen, Protonen und einfach geladene Ionen; bei formula_8-fach geladenen Teilchen gilt entsprechend formula_9. So ändert sich beispielsweise die kinetische Energie eines Protons beim Durchfliegen einer Potentialdifferenz von 100 V um 100 eV, die Energie eines zweifach geladenen Heliumkerns ändert sich um 200 eV. Die kinetische Energie eines positiv geladenen Teilchens nimmt um den genannten Betrag zu, wenn die durchlaufene Spannung so gepolt ist, dass das elektrische Potential auf dem Weg des Teilchens abnimmt (umgangssprachlich: „Wenn sich das Teilchen von Plus nach Minus bewegt“), sonst nimmt sie ab. Für negativ geladene Teilchen gilt dasselbe mit umgekehrten Vorzeichen (siehe z. B. Gegenfeldmethode beim Photoeffekt).

Das Elektronenvolt kann auch als Einheit der Masse von Teilchen verwendet werden. Die Umrechnung von Masse in Energie geschieht gemäß der Äquivalenz von Masse und Energie. Diese Energie wird "Ruheenergie" genannt.

wobei

Die entsprechende Masseneinheit ist also formula_14. Die Umrechnung in Kilogramm ist
Beispielsweise beträgt die Masse eines Elektrons 9,11 · 10 kg = 511 keV/c².

In der Teilchenphysik wird oft ein System „natürlicher“ Einheiten verwendet. Dabei wird formula_16 gesetzt. Damit hat die Masse eines Teilchens die gleiche Einheit wie seine kinetische Energie. Beide werden dann üblicherweise in Elektronenvolt angegeben.

Gebräuchliche dezimale Vielfache des Elektronenvolt sind:


Die kinetische Energie von schnell bewegten schwereren Atomkernen (Schwerionen) gibt man häufig "pro Nukleon" an. Als Einheit wird dann AGeV geschrieben, wobei A für die Massenzahl steht. Jeder Kern mit 1 AGeV besitzt die gleiche Geschwindigkeit. Analog gibt es je nach Energieskala das ATeV und das AMeV.

Im Large Hadron Collider am CERN werden Protonen mit einer Energie von 6,5 TeV und Bleikerne mit 574 TeV zur Kollision gebracht. Die Energie eines einzelnen Kerns mit ca. 1 µJ bzw. 90 µJ ist dabei immer noch sehr gering. Berücksichtigt man aber die große Anzahl der Teilchen (1,15 · 10 Protonen pro Teilchenpaket, im Ring des LHC befinden sich bis zu 2808 Teilchenpakete pro Richtung), erhält man als Gesamtenergie der im Ring befindlichen Protonen 720 MJ, dies entspricht grob der kinetischen Energie eines startenden großen Flugzeugs.

In der Chemie wird oft nicht die Energie pro Teilchen, sondern pro Mol (mit der Einheit J/mol) angegeben, die man durch Multiplikation der Energie des einzelnen Teilchens mit der Avogadro-Konstante formula_17 erhält, zum Beispiel:

wobei formula_19 die Faraday-Konstante ist.


</doc>
<doc id="1260" url="https://de.wikipedia.org/wiki?curid=1260" title="Elektrodynamik">
Elektrodynamik

Die klassische Elektrodynamik (auch Elektrizitätslehre) ist das Teilgebiet der Physik, das sich mit bewegten elektrischen Ladungen und mit zeitlich veränderlichen elektrischen und magnetischen Feldern beschäftigt. Die Elektrostatik als Spezialfall der Elektrodynamik beschäftigt sich mit ruhenden elektrischen Ladungen und ihren Feldern. Die zugrundeliegende Grundkraft der Physik heißt elektromagnetische Wechselwirkung.

Die Theorie der klassischen Elektrodynamik wurde von James Clerk Maxwell Mitte des 19. Jahrhunderts mithilfe der nach ihm benannten Maxwell-Gleichungen formuliert. Die Untersuchung der Maxwellgleichungen für bewegte Bezugssysteme führte Albert Einstein 1905 zur Formulierung der speziellen Relativitätstheorie. Im Laufe der 1940er Jahre gelang es, die Quantenmechanik und Elektrodynamik in der Quantenelektrodynamik zu kombinieren, deren Vorhersagen sehr genau mit Messergebnissen übereinstimmen.

Eine wichtige Form von elektromagnetischen Feldern sind die elektromagnetischen Wellen, zu denen als bekanntester Vertreter das sichtbare Licht zählt. Obwohl die physikalischen Grundlagen zur Beschreibung elektromagnetischer Wellen durch die Elektrodynamik gegeben sind, stellt ihre Erforschung ein eigenes Gebiet der Physik dar, die Optik.

Das Zusammenspiel von elektromagnetischen Feldern und elektrischen Ladungen wird grundlegend durch die "mikroskopischen" Maxwell-Gleichungen
und die Lorentzkraft
bestimmt.

Daraus ergeben sich mit Hilfe der Materialgleichungen der Elektrodynamik die "makroskopischen" Maxwell-Gleichungen. Diese sind Gleichungen für die effektiven Felder, die in Materie auftreten.

Weiter spielen (daraus ableitbar) eine wichtige Rolle:

Die homogenen Maxwellgleichungen 
und
können durch die Einführung der elektromagnetischen Potentiale gemäß
und
in einem sternförmigen Gebiet identisch gelöst werden (Poincaré-Lemma). Dabei bezeichnet formula_8 das sogenannte skalare Potential und formula_9 das Vektorpotential.
Da die physikalischen Felder nur durch Ableitungen der Potentiale gegeben sind, hat man gewisse Freiheiten, die Potentiale
abzuändern und trotzdem dieselben physikalischen Felder zurückzuerhalten. Beispielsweise ergeben formula_10 und formula_9
dasselbe formula_12-Feld, wenn man sie durch
miteinander in Beziehung setzt. Fordert man auch, dass sich bei einer solchen Transformation dasselbe
formula_14-Feld ergibt, muss sich formula_8 wie
transformieren. Eine solche Transformation wird Eichtransformation genannt.
In der Elektrodynamik werden zwei Eichungen oft verwendet. Erstens die sogenannte Coulomb-Eichung oder Strahlungseichung
und zweitens die Lorenz-Eichung
Die Lorenz-Eichung hat dabei den Vorteil relativistisch invariant zu sein und sich bei einem Wechsel zwischen zwei Inertialsystemen strukturell nicht zu ändern. Die Coulomb-Eichung ist zwar nicht relativistisch invariant, aber wird eher bei der kanonischen Quantisierung der Elektrodynamik verwendet.

Setzt man die formula_14- und formula_12-Felder und die Vakuum-Materialgleichungen in die inhomogenen Maxwellgleichungen ein und eicht die Potentiale gemäß der Lorenz-Eichung, entkoppeln die inhomogenen Maxwellgleichungen und die Potentiale erfüllen inhomogene Wellengleichungen
Hierbei bezeichnet formula_22 den D’Alembert-Operator.

Die Elektrostatik ist der Spezialfall unbewegter elektrischer Ladungen und statischer (sich nicht mit der Zeit ändernder) elektrischer Felder. Sie kann in Grenzen auch verwendet werden, solange die Geschwindigkeiten und Beschleunigungen der Ladungen und die Änderungen der Felder klein sind.

Die Magnetostatik beschäftigt sich mit dem Spezialfall konstanter Ströme in insgesamt ungeladenen Leitern und konstanter Magnetfelder. Sie kann für hinreichend langsam veränderliche Ströme und Magnetfelder verwendet werden.

Die Kombination aus beiden, Elektromagnetismus, kann beschrieben werden als Elektrodynamik der nicht zu stark beschleunigten Ladungen. Die meisten Vorgänge in elektrischen Schaltkreisen (z. B. Spule, Kondensator, Transformator) lassen sich bereits auf dieser Ebene beschreiben.
Ein stationäres elektrisches oder magnetisches Feld bleibt nahe seiner Quelle, wie zum Beispiel das Erdmagnetfeld. Ein sich veränderndes elektromagnetisches Feld kann sich jedoch von seinem Ursprung entfernen. Das Feld bildet eine elektromagnetische Welle im Zusammenspiel zwischen magnetischem und elektrischem Feld. Diese Abstrahlung elektromagnetischer Wellen wird in der Elektrostatik vernachlässigt. Die Beschreibung des elektromagnetischen Feldes beschränkt sich hier also auf das Nahfeld.

Elektromagnetische Wellen hingegen sind die einzige Form des elektromagnetischen Feldes, die auch unabhängig von einer Quelle existieren kann. Sie werden zwar von Quellen erzeugt, können aber nach ihrer Erzeugung unabhängig von der Quelle weiterexistieren. Da Licht sich als elektromagnetische Welle beschreiben lässt, ist auch die Optik letztlich ein Spezialfall der Elektrodynamik.

Im Gegensatz zur klassischen Mechanik ist die Elektrodynamik nicht Galilei-invariant. Das bedeutet, wenn man, wie in der klassischen Mechanik, einen absoluten, euklidischen Raum und eine davon unabhängige absolute Zeit annimmt, dann gelten die Maxwellgleichungen nicht in jedem Inertialsystem.

Einfaches Beispiel: Ein mit konstanter Geschwindigkeit fliegendes, geladenes Teilchen ist von einem elektrischen und einem magnetischen Feld umgeben. Ein mit gleicher Geschwindigkeit fliegendes, gleichgeladenes Teilchen erfährt durch das elektrische Feld des ersten Teilchens eine abstoßende Kraft, da sich gleichnamige Ladungen gegenseitig abstoßen; gleichzeitig erfährt es durch dessen Magnetfeld eine anziehende Lorentzkraft, die die Abstoßung teilweise kompensiert. Bei Lichtgeschwindigkeit wäre diese Kompensation vollständig. In dem Inertialsystem, in dem beide Teilchen ruhen, gibt es kein magnetisches Feld und damit keine Lorentzkraft. Dort wirkt nur die abstoßende Coulombkraft, so dass das Teilchen stärker beschleunigt wird als im ursprünglichen Bezugssystem, in dem sich beide Ladungen bewegen. Dies widerspricht der newtonschen Physik, bei der die Beschleunigung nicht vom Bezugssystem abhängt.

Diese Erkenntnis führte zunächst zu der Annahme, dass es in der Elektrodynamik ein bevorzugtes Bezugssystem gäbe (Äthersystem). Versuche, die Geschwindigkeit der Erde gegen den Äther zu messen, schlugen jedoch fehl, so zum Beispiel das Michelson-Morley-Experiment. Hendrik Antoon Lorentz löste dieses Problem mit einer modifizierten Äthertheorie (Lorentzsche Äthertheorie), die jedoch von Albert Einstein mit seiner speziellen Relativitätstheorie abgelöst wurde. Einstein ersetzte Newtons absoluten Raum und absolute Zeit durch eine vierdimensionale Raumzeit. In der Relativitätstheorie tritt an die Stelle der Galilei-Invarianz die Lorentz-Invarianz, die von der Elektrodynamik erfüllt wird.

In der Tat lässt sich die Verringerung der Beschleunigung und damit die magnetische Kraft im obigen Beispiel als Folge der Längenkontraktion und Zeitdilatation erklären, wenn man die im bewegten System gemachten Beobachtungen in ein ruhendes System zurücktransformiert. In gewisser Weise lässt sich daher die Existenz von magnetischen Phänomenen letztlich auf die Struktur von Raum und Zeit zurückführen, wie sie in der Relativitätstheorie beschrieben wird. Unter diesem Gesichtspunkt erscheint auch die Struktur der Grundgleichungen für statische Magnetfelder mit ihren Kreuzprodukten weniger verwunderlich.

In der manifest Lorentz-forminvarianten Beschreibung der Elektrodynamik bilden das skalare Potential und das Vektorpotential einen Vierervektor, analog zum Vierervektor von Raum und Zeit, so dass die Lorentz-Transformationen analog auch auf die elektromagnetischen Potentiale angewendet werden können. Bei einer speziellen Lorentz-Transformation mit der Geschwindigkeit formula_23 in formula_24-Richtung gelten für die Felder im gebräuchlichen SI-Einheitensystem die Transformationsgleichungen:

Jedoch liefert die klassische Elektrodynamik keine widerspruchsfreie Beschreibung bewegter Punktladungen, auf kleinen Skalen ergeben sich Probleme wie die der Abraham-Lorentz-Gleichung. Die Quantenelektrodynamik (QED) vereint die Elektrodynamik deshalb mit quantenmechanischen Konzepten. Die Theorie der elektroschwachen Wechselwirkung vereinigt die QED mit der schwachen Wechselwirkung und ist Teil des Standardmodells der Elementarteilchenphysik. Die Struktur der QED ist ebenfalls Ausgangspunkt für die Quantenchromodynamik (QCD), welche die starke Wechselwirkung beschreibt. Allerdings ist die Situation dort noch komplizierter (z. B. drei Ladungsarten, siehe Farbladung).

Eine Vereinheitlichung der Elektrodynamik mit der allgemeinen Relativitätstheorie (Gravitation) ist unter dem Namen Kaluza-Klein-Theorie bekannt, und stellt einen frühen Versuch zur Vereinheitlichung der fundamentalen Wechselwirkungen dar.





</doc>
<doc id="1263" url="https://de.wikipedia.org/wiki?curid=1263" title="E-Mail">
E-Mail

Die (auch das) E-Mail (englisch [], kurz "Mail"; engl. ' für „elektronische Post“ oder E-Post) ist zum einen ein System zur computerbasierten Verwaltung von briefähnlichen Nachrichten und deren Übertragung über Computernetzwerke, insbesondere über das Internet. Zum anderen werden auch die auf diesem elektronischen Weg übertragenen Nachrichten selbst als E-Mails bezeichnet.

E-Mail wird – noch vor dem World Wide Web – als wichtigster und meistgenutzter Dienst des Internets angesehen, nicht zuletzt, weil es durch E-Mails möglich ist, Textnachrichten ebenso wie digitale Dokumente (also z. B. Grafiken oder Office-Dokumente) typischerweise in wenigen Sekunden rund um die Erde zuzustellen.

Im Gegensatz zu Telefon oder Internet Relay Chat, die gleichzeitige (synchrone) Kommunikation ermöglichen, ist die E-Mail – wie die Briefpost – ein asynchrones Kommunikationsmedium:
Der Sender versendet seine Nachricht unabhängig davon, ob der Empfänger sie sofort entgegennehmen kann oder nicht.

Standardsprachlich hat sich in Deutschland die weibliche Form ("die E-Mail") des grammatischen Geschlechts weitgehend durchgesetzt, in der Schweiz hingegen das Neutrum ("das E-Mail"), während in Österreich und in Teilen Südwestdeutschlands beide Formen Verwendung finden. Das Österreichische Wörterbuch nennt sowohl die weibliche als auch die sächliche Form, nennt letztere aber zuerst. Dass die sächliche Form überwiegt, wird dadurch unterstrichen, dass „e-mailen“ mit „ein E-Mail versenden“ erklärt wird.

Gemäß Duden, Wahrig und dem amtlichen Wörterverzeichnis der reformierten deutschen Rechtschreibung ist "E-Mail" die richtige Schreibweise, neben "E-mail". 

Ein weniger gebräuchliches Synonym ist der Begriff "E-Post".

Vor dem Aufkommen von E-Mail wurden Nachrichten als Brief oder Telegramm, später auch – als die ersten beiden digitalen Übertragungsverfahren – Fernschreiben und Teletex sowie Fax übermittelt. Ende der 1980er Jahre begann dann der Erfolgsweg der E-Mail – sie war eine der ersten Anwendungen, die die Möglichkeiten des Arpanets nutzten. Die Einführung von E-Mail wurde nicht gezielt vorangetrieben, sondern eroberte das Netzwerk wegen des Benutzerverhaltens. Das überraschte die Arpanet-Initiatoren, denn noch 1967 hatte Lawrence Roberts, der spätere Leiter von IPTO, gesagt, die Möglichkeit des Austausches von Botschaften unter den Netzwerkteilnehmern sei kein wichtiger Beweggrund, um ein Netzwerk von wissenschaftlichen Rechnern aufzubauen („not an important motivation for a network of scientific computers“).

Ein Vorläufer der E-Mail war das MAIL Systemkommando in der Erweiterung Multics des CTSS Time-Sharing-Systems am Massachusetts Institute of Technology, vorgeschlagen 1964/65 von den Systementwicklern Glenda Schroeder, Louis Pouzin und Pat Crisman und implementiert 1965 von Tom Van Vleck. Möglichkeiten Mails im Arpanet zu versenden regte J. C. R. Licklider schon 1968 an und die Idee wurde unter den Entwicklern diskutiert (RFC 196, „Mail Box Protocol“ von Richard W. Watson vom 20. Juli 1971). Nachdem Multics, in dem ein Mail-Programm zur Kommunikation der Nutzer implementiert war, im Oktober 1971 an das Arpanet angeschlossen war folgte Anfang 1972 die Implementierung eines Mail-Programms über das Arpanet durch die MAC Networking Group unter Mike Padlipsky.

Ray Tomlinson hat im Jahr 1971 den ersten elektronischen Brief verschickt und gilt seitdem als Erfinder der Mail. Er war bei dem Forschungsunternehmen Bolt, Beranek and Newman (BBN) an der Entwicklung des Betriebssystems TENEX beteiligt, das auf vielen im Arpanet verbundenen Rechnern zur Verfügung stand, und beschäftigte sich dabei unter anderem mit dem Programm SNDMSG für die Übermittlung von Nachrichten unter den Benutzern des Großrechners und dem Protokoll CPYNET für die Übertragung von Dateien zwischen Computern. Programme wie SNDMSG gab es wie erwähnt bereits seit den frühen 1960er Jahren. Sie ermöglichten Benutzern, den Mailboxen anderer Benutzer desselben Computers Text hinzuzufügen. Eine Mailbox war seinerzeit nichts weiter als eine einzelne Datei, die nur ein Benutzer lesen konnte. Tomlinson kam 1971 auf die Idee, CPYNET so zu ändern, dass es vorhandene Dateien ergänzen konnte und es dann in SNDMSG einzuarbeiten. Die erste Anwendung dieser Kombination war eine Nachricht von Tomlinson an seine Kollegen, in der er Ende 1971 mitteilte, dass man nun Nachrichten übers Netzwerk senden konnte, indem man dem Benutzernamen des Adressaten das Zeichen „@“ und den Hostname des Computers anfügte.

Parallel zum Internet entwickelten sich zu Beginn der 1980er Jahre in den meisten Netzwerken Systeme, mit denen sich Nachrichten übertragen ließen. Dazu gehörten unter anderem Mailbox-Systeme, X.25, Novell und BTX. Diese Systeme wurden Mitte der 1990er durch die Verbreitung des Internets stark verdrängt. Aus dem Jahr 1982 stammt das Protokoll "RFC 822". "RFC 822" wurde im Jahr 2001 durch RFC 2822 ersetzt, das wiederum im Jahr 2008 durch "RFC 5322" ersetzt wurde.
In Deutschland wurde am 3. August 1984 um 10:14 Uhr MEZ die erste Internet-E-Mail empfangen: Michael Rotert von der Universität Karlsruhe (TH) empfing unter seiner Adresse "„rotert@germany“" eine Grußbotschaft von Laura Breeden "(„breeden@scnet-sh.arpa“)" an der US-amerikanischen Plattform CSNET aus Cambridge (Massachusetts) zur elektronischen Kommunikation von Wissenschaftlern, die einen Tag zuvor (am 2. August 1984, 12:21 Uhr) abgeschickt worden war. Eine Kopie dieser E-Mail wurde als „CC“ gleichzeitig an den Leiter des Projekts, Werner Zorn mit der Adresse "(„zorn@germany“)", geschickt.
Heute werden E-Mails meist per SMTP verschickt. Zum Abrufen der E-Mails vom Zielserver existieren verschiedene Verfahren, etwa das POP3- oder IMAP-Protokoll oder Webmail. X.400 ist ein offener Standard, der hauptsächlich im LAN oder WAN benutzt wird.

Die erste große E-Mail-Diskussionsgruppe, die im Arpanet entstand, war eine Mailingliste namens „SF-LOVERS“, in der sich eine Reihe von DARPA-Forschern an öffentlichen Diskussionen über Science-Fiction beteiligte (Rheingold, 1994). SF-LOVERS tauchte in den späten 1970er Jahren im Arpanet auf. Zunächst wurde versucht, dagegen einzuschreiten, weil derartige Aktivitäten selbst bei liberalster Auslegung mit Forschung wenig zu tun hatten. Für einige Monate wurde die Liste deshalb gesperrt. Schließlich wurden die Verantwortlichen der DARPA aber mit dem Argument überzeugt, dass SF-LOVERS ein wichtiges Pilotprojekt zur Erforschung der Verwaltung und des Betriebs großer Mailinglisten war (Hauben, 1993). Die Systemingenieure mussten das System wiederholt umbauen, damit es das explosionsartig ansteigende Nachrichtenaufkommen bewältigen konnte.

Im Jahr 2014 wurden in Deutschland rund 506,2 Milliarden E-Mails versendet.
Im Jahr 2015 waren weltweit schätzungsweise 4,353 Milliarden E-Mail-Konten von 2,586 Milliarden Nutzern in Gebrauch. 81 % der Deutschen versendeten und empfingen im Jahr 2015 E-Mails.

Zum Schreiben, zum Versand, zum Empfang und zum Lesen von E-Mails gibt es zwei Möglichkeiten (Benutzerschnittstellen).

Zur Nutzung von E-Mail kann ein E-Mail-Programm, auch E-Mail-Client oder Mail-User-Agent (MUA) genannt, verwendet werden.
Ein solches Programm ist lokal auf dem Computer des Benutzers installiert und kommuniziert mit einem oder mehreren E-Mail-Postfächern.

Alternativ kann man via Webmail auf seine E-Mail zugreifen. Hierbei verwaltet der Benutzer seine E-Mails in seinem Web-Browser. Ermöglicht wird dies durch eine Webanwendung auf dem Webserver des E-Mail-Anbieters, die ihrerseits auf das E-Mail-Postfach auf dem Webserver zugreift.

E-Mails sind intern in zwei Teile geteilt: Den "Header" mit Kopfzeilen und den "Body" (Textkörper) mit dem eigentlichen Inhalt der Nachricht. Zusätzlich werden innerhalb des Bodys noch weitere Untergliederungen definiert.

Die "Header" genannten Kopfzeilen einer E-Mail geben Auskunft über den Weg, den eine E-Mail genommen hat, und bieten Hinweise auf Absender, Empfänger, Datum der Erstellung, Format des Inhaltes und Stationen der Übermittlung. Der Benutzer wird viele Details aus den Header-Zeilen im Normalfall nicht benötigen. Daher bieten E-Mail-Programme an, den Header bis auf die Grunddaten wie Absender, Empfänger und Datum auszublenden. Bei Bedarf kann der Header jederzeit wieder komplett sichtbar gemacht werden.

Der Body einer E-Mail ist durch eine Leerzeile vom Header getrennt und enthält die zu übertragenden Informationen in einem oder mehreren Teilen.

Eine E-Mail darf gemäß RFC 5322 Abschnitt 2.3 nur Zeichen des 7-Bit-ASCII-Zeichensatzes enthalten. Sollen andere Zeichen, wie zum Beispiel deutsche Umlaute, oder Daten, wie zum Beispiel Bilder, übertragen werden, müssen das Format im Header-Abschnitt deklariert und die Daten passend kodiert werden. Geregelt wird das durch RFC 2045 ff (siehe auch MIME und Base64). Aktuelle E-Mail-Programme kodieren Text und Dateianhänge (vergleiche unten) bei Bedarf automatisch.

Die Nachricht kann aus einem Klartext, einem formatierten Text (beispielsweise HTML) und/oder Binärdaten (beispielsweise einem Bild oder Fax, s. u. bei Dateianhänge) bestehen. Es können auch mehrere Formate als Alternativen gesendet werden oder weitere beliebige Dateien angehängt werden. Den Abschluss bilden ggf. "Signatur" und "Footer". Alle diese zusätzlichen Teile sind optional, müssen in einer E-Mail also nicht unbedingt vorkommen.


In HTML formatierte Mails werden teils ungewollt und unbewusst durch die Voreinstellung des verwendeten E-Mail-Programms, insbesondere von Microsoft-Programmen, versandt, teils bewusst, um Schriftauszeichnungen verwenden zu können, etwa in E-Mail-Newslettern.

Obwohl das HTML-Format standardisiert ist, war es ursprünglich nicht für den Einsatz in E-Mails gedacht. Das führte unter anderem dazu, dass es in der Vergangenheit viele, auch konzeptuelle Sicherheitslücken in den HTML-Rendering-Engines von E-Mail-Programmen gab, die einerseits zur Verbreitung von E-Mail-Würmern beigetragen haben und andererseits ungewollte Informationen über den Empfänger preisgegeben haben (Zählpixel). Diese Situation hat sich im Lauf der Zeit verbessert und bekannte Probleme, wie die standardmäßige Ausführung aktiver Inhalte (beispielsweise JavaScript) oder das automatische Nachladen externer Bilder, wurden durch andere Voreinstellungen entschärft. Die oft inkonsistente Deaktivierung potentiell gefährlicher HTML-Features in verschiedenen E-Mail-Programmen hat allerdings auch den Effekt, dass optische Effekte oder Formatierungen nicht so dargestellt werden, wie es vom Absender gedacht war.

Prinzipbedingt bieten HTML-formatierte E-Mail-Nachrichten stets wesentlich mehr Angriffsmöglichkeiten und sind daher potentiell unsicherer als reine Text-Nachrichten. Deshalb empfehlen viele EDV-Ratgeber und Softwarehersteller, die HTML-Anzeige von E-Mails zumindest im Vorschaufenster des E-Mail-Programms zu deaktivieren oder ganz auszuschließen und auch selbst keine E-Mail-Nachrichten im HTML-Format zu versenden. Das Bundesamt für Sicherheit in der Informationstechnik (BSI) empfiehlt:

Eine E-Mail-Adresse bezeichnet eindeutig den Empfänger einer E-Mail und ermöglicht damit eine Zustellung an diesen Empfänger. So, wie sie für den Transport per SMTP im Internet verwendet wird, besteht sie aus zwei Teilen: In codice_1 ist codice_2 der "domain-part", codice_3 der "local-part". (Andere Transportmechanismen wie zum Beispiel UUCP oder X.400 verwenden eine andere Adress-Syntax.)
Der "domain-part" benennt den MX Resource Record (meist identisch der Domain) des Mailservers, dem die E-Mail zugestellt werden soll.
Der "local-part" identifiziert eindeutig den Besitzer eines E-Mail-Postfachs auf diesem Mailserver.

In einem typischen Fall nimmt eine E-Mail den folgenden Weg von einem Absender (im Beispiel: Anja)
durch das Internet zu einem Adressaten (im Beispiel: Bertram), siehe Abbildung rechts.


Besonderheiten: Oftmals wird es sich bei Anjas Internetdienstanbieter und Anjas E-Mail-Provider um ein und dasselbe Unternehmen handeln. Wenn Anja und Bertram ihre E-Mail-Konten beim selben E-Mail-Anbieter haben, entfällt Schritt 3.

Je nach Ausführung des verwendeten E-Mail-Programms kann der Absender einer E-Mail eine Zustellbestätigung und/oder eine Lesebestätigung anfordern.

Wurde eine "Zustellbestätigung" angefordert, erhält der Absender (im obigen Beispiel Anja) eine Delivery Status Notification (DSN) in Form einer E-Mail, sobald seine E-Mail erfolgreich im Postfach des Empfängers abgelegt wurde und die beteiligten Architekturen dies unterstützen. Bezogen auf das obige Beispiel geschähe dies zeitlich unmittelbar nach Schritt 4.

Wurde eine "Lesebestätigung" angefordert, erhält der Absender (im obigen Beispiel Anja) eine Message Disposition Notification (MDN) in Form einer E-Mail, wenn der Empfänger (im obigen Beispiel Bertram) die an ihn gerichtete E-Mail öffnet und das Auslösen dieser Bestätigung nicht verhindert. Bezogen auf das obige Beispiel geschähe dies zeitlich unmittelbar im Schritt 6 beim Öffnen der E-Mail. Die Lesebestätigung kann somit "nicht" dahingehend interpretiert werden, dass der Empfänger die E-Mail auch tatsächlich gelesen oder gar verstanden hat.

Insofern haben diese Bestätigungen den – allerdings nicht-juristischen, sondern lediglich informativen – Charakter eines Einwurf-Einschreibens (Zustellbestätigung) bzw. eines Einschreibens mit Rückschein (Lesebestätigung) in Deutschland.

Das Format einer E-Mail wird durch den RFC 5322 festgelegt. Danach bestehen E-Mails nur aus Textzeichen (7-Bit-ASCII-Zeichen). Um auch andere Zeichen übertragen zu können, wurden weitere Internet-Standards definiert, mit deren Hilfe 8-Bit-Zeichen in ASCII kodiert werden. Der Standard Quoted-Printable kodiert zum Beispiel den Buchstaben „ß“ als Zeichenkette „=DF“. Breite Verwendung haben die Standards der MIME-Serie gefunden, mit deren Hilfe nicht nur Sonderzeichen in Texten, sondern auch Binär-Dateien kodiert werden können, zum Beispiel um sie als E-Mail-Anhänge zu verschicken.

Die Gesamtgröße von E-Mails ist prinzipiell nicht begrenzt. In der Realität zeigen sich allerdings Grenzen durch technische oder administrative Beschränkungen der Systeme, die die E-Mail übertragen oder empfangen. E-Mail-Provider, E-Mail-Postfächer und beteiligte Mailserver können die Größe einer E-Mail begrenzen. In solchen Fällen sollte der begrenzende Mailserver dem Absender eine "Bounce Message" (Fehlermeldung) senden.

Wo die Mails permanent gespeichert werden, hängt von der verwendeten Technik des Endanwenders ab. Benutzt er ein Webinterface, so werden die Mails grundsätzlich auf dem Mailserver gehalten. Wenn er ein Mailprogramm einsetzt, das die Mails mit dem Protokoll IMAP liest, dann werden die E-Mails ebenfalls auf einem Mailserver gehalten. Ursprünglich sah das alternative Protokoll POP vor, dass die Mails vom Server geholt und dort gleichzeitig gelöscht werden. Der Client ist also für das Speichern auf seinem lokalen Massenspeicher zuständig. Bei neueren POP-Versionen ist es aber – abhängig von den Einstellungen des Servers – auch möglich, die Mails auf dem Server zu belassen.

E-Mails werden (lokal oder auf dem Mailserver) häufig nicht einzeln als separate Dateien, sondern zusammengefasst in Container-Dateien gespeichert. mbox ist eine unter Unix/Linux häufig verwendete Möglichkeit, eine Alternative ist Maildir.

Für einzelne E-Mails ist unter anderem die Dateiendung codice_4 geläufig, die von Programmen wie Novell GroupWise, Microsoft Outlook Express, Lotus Notes, Windows Mail, Mozilla Thunderbird und Postbox verwendet wird. Die Dateien bestehen aus plain text im MIME-Format und enthalten die Kopfzeilen, den Nachrichteninhalt und Anhänge in einem oder mehreren Formaten.

Das E-Mail-Programm Pegasus Mail (kurz PMail) verwendet eigene Mailordner.

Beispiel eines Ablaufs:


Heutzutage sind hauptsächlich SMTP, POP3 und IMAP in Verwendung, oft in Verbindung mit SSL-Verschlüsselung (siehe SMTPS, POP3S und IMAPS).

Die Laufzeit (Transportzeit einer Postsendung vom Absender zum Empfänger) der E-Mail kann ein Problem darstellen, da sie – anders als zum Beispiel beim Telefax – nicht vorhersehbar ist und unter ungünstigen Voraussetzungen stark schwanken kann. Die Schwankungen der Laufzeit werden durch eine Vielzahl von Parametern beeinflusst, vor allem durch die Auslastung der beteiligten Mailsysteme sowie der für E-Mail bereitstehenden Übertragungskapazität der die Mailsysteme verbindenden Leitungen. Ist der Mailserver des Empfängers länger nicht erreichbar, oder wird die Mail nur in großen Zeitabständen auf den Server des Empfängers übertragen, kann es durchaus zu Laufzeiten von einigen Tagen kommen.

Die Nachteile der nicht fest definierten Laufzeit sind jedoch bei den heutigen modernen E-Mail-Systemen nahezu vernachlässigbar (weltweit selten mehr als eine Minute), da bei gut gepflegten Systemen nur noch relativ selten größere Fehler auftreten, durch die längere Laufzeiten verursacht werden könnten. Verzögerungen können allerdings auch bei modernen E-Mail-Systemen durch diverse Spamschutz-Maßnahmen auftreten (beispielsweise dem Greylistingverfahren).

Das E-Mail-System besitzt einige Vor- und Nachteile, die im Folgenden aufgeführt sind:

Als wesentlicher Vorteil von E-Mails ist zu nennen, dass sie sehr schnell (im Bereich von wenigen Sekunden) übermittelt und vom Empfänger gelesen werden können. Der praktische Aufwand, eine E-Mail zu verschicken und zu empfangen, ist geringer, da kein Ausdrucken, Kuvertieren, Adressieren, Frankieren und Postkasteneinwerfen beim Absender und kein Briefkastenentleeren und Brieföffnen beim Empfänger nötig ist. Auf dem Computer geschriebene Briefe können direkt und einfach per E-Mail verschickt und beim Empfänger direkt auf dem Computer gelesen und ggf. weiterverarbeitet werden.

Auch der finanzielle Einzelaufwand (Kosten für Versand einer E-Mail) ist im Normalfall geringer (keine Material- und Portokosten), sofern viele E-Mails verarbeitet werden oder die nötige Infrastruktur (Computer mit Internetzugang) sowieso schon beim Absender und Empfänger zur weitergehenden Nutzung vorhanden ist. Zudem wird der Aufwands- und Kostenvorteil umso größer, je mehr Empfänger die gleiche E-Mail erhalten sollen (Rundschreiben). E-Mail-Dienste werden im Internet für den Privatgebrauch meist kostenlos angeboten. Sie finanzieren sich im Allgemeinen durch Werbung.

Hinsichtlich der Umweltfreundlichkeit von E-Mails im Speziellen gibt es verschiedene Diskussionen und Ansichten wie auch beim Internet und der Computertechnik im Allgemeinen. Zumindest sind E-Mails insofern umweltfreundlicher als herkömmliche Briefe, als sie unmittelbar kein Papier verbrauchen und keinen materiellen Transport (Lkw, Bahn, Flugzeug, Schiff usw.) benötigen.

E-Mails haben gegenüber normaler Papier-Post den Vorteil, dass ihre Anschriften- und Absendertexte (E-Mail-Adressen) deutlich kürzer sind als bei normalen Papier-Post-Adressen mit Name, Straße/Postfach, Postleitzahl, Ort und ggf. Land. E-Mail-Adressen können weitgehend frei gewählt werden und es besteht auch kein Zwang, den eigenen Namen in Klartext (z. B. "michael.mueller@xyz.org") als E-Mail-Adresse zu verwenden, sofern der Domain-Inhaber (xyz.org) keine Regeln zum Format seiner E-Mail-Adressen aufgestellt hat oder keine Gesetze gebrochen werden. Stattdessen sind ebenso Pseudonyme wählbar, womit eine höhere Anonymität erreicht wird, da die E-Mail-Adresse nicht oder nur begrenzt (über die Domain hergeleitet) Aussage macht bzw. Rückschlüsse erlaubt über Namen, Herkunft, Geschlecht, Anschrift, geosozialen Status usw. Ebenso ist der Besitz mehrerer verschiedener E-Mail-Adressen möglich.

In der praktischen Handhabung bieten E-Mails ebenso Vorteile gegenüber der Papier-Post. Eine E-Mail kann gleichzeitig an mehrere Empfänger verschickt werden, wobei auch mit verdeckten Empfängerlisten () gearbeitet werden kann, damit die komplette Empfängerliste nicht von jedem Empfänger einsehbar ist. E-Mails können auf dem Computer einfach archiviert und die Archive können leicht durchsucht werden, um eine E-Mail schnell wiederzufinden. Auch versendete und gelöschte E-Mails können automatisch archiviert werden.

E-Mail-Systeme bieten des Weiteren einige praktische Automatismen. E-Mails lassen sich auf Wunsch automatisch weiterleiten, entweder zu einer anderen E-Mail-Adresse oder auf anderen Kommunikationskanälen, beispielsweise als SMS oder Fax. Auch der umgekehrte Weg ist möglich, das heißt die Weiterleitung eines Fax oder einer SMS an eine E-Mail-Adresse. Auf Wunsch kann auch bei Eingang einer E-Mail eine automatische Antwort an den Absender verschickt werden (zum Beispiel eine Abwesenheits-Nachricht) oder es erfolgt eine Benachrichtigung, dass eine neue Nachricht eingegangen ist. Ebenso ist eine automatische Aussortierung von unerwünschten E-Mails (Spam-Filter & persönliche Blacklists) oder eine automatische Vorsortierung in verschiedene Ordner nach frei vorgebbaren Kriterien möglich.

Von Vorteil ist auch, dass an E-Mails weitere Dateien beliebiger Art angefügt werden können, die der Empfänger weiterverwenden kann. E-Mails (jedoch jeweils nur der Textkörper, nicht der Kopf) können aus Datenschutzgründen auch verschlüsselt und zur Authentifizierung elektronisch signiert werden. Ebenso können auf Wunsch digitale Visitenkarten mit weiteren Informationen (wie Anschrift oder Telefonnummer) als Anhang einer E-Mail mitverschickt werden, wodurch der Empfänger sein Adressbuch leichter mit E-Mail-Kontakten füllen und pflegen kann.

Auch beim Antworten auf E-Mails zeigen sich praktische Vorteile. Antworten auf E-Mails können einfacher und schneller begonnen werden, indem der Absender und die CC-Empfänger der Ursprungs-E-Mail automatisch als Empfänger der Antwort übernommen werden. Ebenso kann in Antworten der Inhalt der Ursprungs-E-Mail zitiert oder angefügt werden, um in der Antwort besser Bezug nehmen oder antworten zu können oder um den Diskussionsfaden zu dokumentieren.

Als Spam- [spæm] oder Junk-Mails (englisch für ‚Abfall‘ oder ‚Plunder‘) werden unerwünschte E-Mails bezeichnet, die meist Werbung etc. enthalten. Die Effizienz von E-Mail wird durch den massenhaften Verkehr von Spam, also E-Mails, die dem Empfänger unverlangt zugestellt werden und häufig werbenden Inhalt haben, teilweise eingeschränkt, insofern die Bearbeitung von Spam-E-Mails den Empfänger Zeit kostet. Seit ungefähr 2002 sind mehr als 50 % und seit 2007 etwa 90 % des weltweiten E-Mail-Aufkommens Spam. Im Jahr 2010 wurden ca. 107 Billionen E-Mails verschickt, mit einem Spam-Anteil von 89,1 %. Im Oktober 2015 lag der Spam-Anteil bei E-Mails bei 54 %.

Das Landgericht Bonn entschied 2014 mit Bezug auf einen Fall von Anwaltshaftung, dass der Spam-Ordner eines Accounts, der im geschäftlichen Verkehr als Kontaktmöglichkeit zur Verfügung gestellt wird, täglich durchgesehen werden muss, um versehentlich als Werbung aussortierte E-Mails zurückzuholen.

Wie jedes Kommunikationsmittel muss auch die E-Mail verschiedenen Anforderungen genügen, um als sicheres Kommunikationsmittel gelten zu dürfen. Hier sind als wichtigste Kriterien die Authentizität, der Datenschutz und die Integrität einer E-Mail zu nennen.

Mit der Authentizität einer E-Mail ist gemeint, dass sichergestellt ist, dass die E-Mail auch wirklich vom Absender stammt, also ein Original ist und keine betrügerische Fälschung. Datenschutz bezeichnet bei E-Mails im Wesentlichen den Schutz vor Mitlesen durch Dritte auf dem Übertragungsweg. Als Integrität bezeichnet man das Schutzziel, dass der E-Mail-Inhalt bei der Übertragung vollständig und unverändert bleibt.

Zur Erreichung der Authentizität, des Datenschutzes und der Integrität existieren bereits diverse Schutzmechanismen, wie an anderen Stellen bereits beschrieben (Verschlüsselung, Absenderauthentifizierung, Pretty Good Privacy, GNU Privacy Guard, S/MIME). Jedoch werden diese Schutzmechanismen beim Großteil des heutigen E-Mail-Verkehrs noch nicht angewendet. Ohne diese Schutzmechanismen besitzen herkömmliche E-Mails jedoch einen geringeren Schutz als eine normale Postkarte.

Der folgende Unterabschnitt soll dazu möglichst plastisch den recht geringen Sicherheits-Standard einer herkömmlichen E-Mail im Vergleich zu einer Postkarte darstellen.

Herkömmliche (unverschlüsselte) E-Mails sind mit einer Postkarte vergleichbar, weil deren Inhalt offen und einfach lesbar verschickt wird. Verschlüsselte E-Mails entsprechen einem verschlossenen Brief, aber E-Mail-Verschlüsselung ist heute immer noch eher die Ausnahme. Aber auch bei einer verschlüsselten E-Mail ist neben dem Absender und den Empfängern (wie bei einem Brief) zusätzlich die Betreffzeile sowie generell alle Kopfzeilen lesbar.

E-Mails werden wie Postsachen beim E-Mail-Dienstleister wie bei einem Postamt gelagert. Somit sind unverschlüsselte E-Mails wie Postkarten beim E-Mail-Dienstleister lesbar. Zudem lassen sich E-Mails gegenüber normaler Papier-Post einfach und automatisch nach nutzbaren Informationen durchsuchen und auswerten.

Zur Erhöhung der Zuverlässigkeit des E-Mail-Dienstes werden beim E-Mail-Dienstleister von E-Mails Kopien erstellt und eine Weile aufbewahrt, so als würde die Post Fotokopien von Postkarten und Briefen machen und archivieren.

Bei Papier-Post lässt sich auf Wunsch die erfolgte Zustellung dokumentieren (Einschreiben mit Rückschein) oder die Post läuft bei Annahmeverweigerung automatisch zurück zum Absender. Herkömmliche E-Mails besitzen zwar auch den Mechanismus der Annahmebestätigung, aber der Empfänger kann die E-Mail trotzdem lesen, ohne gezwungen zu sein, die Annahme dem Absender gegenüber zu bestätigen. Die Annahmeverweigerung als eigenständiger Mechanismus mit Rückmeldung an den Absender existiert bei herkömmlichen E-Mails nicht.

Eine Postkarte wird üblicherweise bei Inlandspost nur von einem bzw. bei internationaler Post von zwei Post-Unternehmen entgegengenommen, transportiert und an den Empfänger ausgehändigt. Eine E-Mail dagegen passiert auf dem Weg durch das Internet üblicherweise die Rechner verschiedener Unternehmen in verschiedenen Ländern. Theoretisch kann eine E-Mail quasi ihren Weg über den halben Erdball durch viele Länder über viele Zwischenstationen (Rechner) nehmen, und alle Beteiligten können diese mitlesen. Es ist insbesondere durch Edward Snowden bekannt geworden, dass Geheimdienste den E-Mail-Verkehr systematisch nach bestimmten Stichwörtern durchsuchen.

Ein Einbrecher muss bei einem Postamt persönlich erscheinen, aber ein Hacker kann (bei Sicherheitslücken) einfach aus der Ferne in ein E-Mail-Postfach einbrechen, ohne dass er verfolgbare Spuren hinterlässt oder der Einbruch überhaupt bemerkt wird. Einbrecher haben bei E-Mail-Spionage weniger Risiko zu fürchten bei höheren Erfolgschancen und besseren Werkzeugen. Voraussetzung ist jedoch eine hohe fachliche Qualifikation des Einbrechers.

Sicherheitsmaßnahmen sind bei Papier-Post für jedermann einfach und nachvollziehbar umsetzbar (Einschreiben mit Rückschein, Siegel, Tresor, Alarmanlage …). Bei E-Mails sind Sicherheitsmaßnahmen viel diffiziler und nur von fortgeschrittenen Computer-Anwendern halbwegs beherrschbar. Aber auch Nachlässigkeiten der Nutzer, z. B. durch Wahl unsicherer Passwörter, erleichtern die Chancen der Einbrecher.

Ähnlich einfach wie bei einem Brief oder einer Postkarte lassen sich E-Mails mit einer falschen Absenderadresse verschicken, was zum Beispiel bei Spam oder Phishing oft zu beobachten ist. Empfänger-, Kopie- und Blindkopie-Adressen (im E-Mail-Kopf gekennzeichnet mit "TO", "CC" beziehungsweise "BCC") lassen sich gleichermaßen fälschen (E-Mail-Spoofing).

Papier-Post wird üblicherweise handschriftlich unterzeichnet (signiert) und ein Betrüger muss zum Betrug die Handschrift fälschen, jedoch wird bei den allermeisten E-Mails auf die elektronische Unterschrift (Signatur) verzichtet und unsignierte E-Mails werden vom Empfänger trotz fehlender bzw. eingeschränkter Rechtskraft im Allgemeinen akzeptiert.

Zusammenfassend kann gesagt werden, dass bei herkömmlichen E-Mails ein noch viel geringerer Sicherheitsstandard als bei einer Postkarte allgemein akzeptiert ist, obwohl kaum ein Mensch daran denken würde, mit einer Postkarte persönliche sensible Daten zu versenden. Vermutlich ist diese Akzeptanz der mangelnden Transparenz der E-Mail-Technologie geschuldet, weil die Risiken für den Nicht-Computerexperten nicht so offensichtlich, nicht erkennbar oder schlichtweg unbekannt sind, oder die Nachteile werden im Vergleich zu den vielen Vorteilen einfach in Kauf genommen.

Auch mit einfachen E-Mails können rechtserhebliche Erklärungen abgegeben und Verbindlichkeiten begründet werden. E-Mails haben allerdings wenig Beweiskraft, da der Sender bei den herkömmlichen Protokollen und Log-Mechanismen nicht längerfristig die Möglichkeit hat, zu beweisen, wann er was an wen versendet, ob der Empfänger die E-Mail erhalten hat oder ob sie tatsächlich abgesendet wurde. Mit der Zeit werden die im sogenannten Benutzerkonto gespeicherten Daten nämlich gelöscht.

Durch eine digitale Signatur und vor allem durch eine qualifizierte elektronische Signatur können im Rechtsverkehr (Zivilrecht, Verwaltungsrecht) Verbindlichkeiten geschaffen werden, die gerichtlich leichter durchsetzbar sind. Umgangssprachlich wird dann von einer „digitalen Unterschrift“ gesprochen. Das verbindliche Setzen eines Zeitstempels wird unter bestimmten Voraussetzungen ebenfalls anerkannt. Näheres wird beispielsweise im deutschen, österreichischen oder liechtensteinischen Signaturgesetz geregelt. Den Empfang der Nachricht kann eine Signatur allerdings nicht beweisen, hierzu ist beispielsweise eine – idealerweise ebenfalls signierte – Antwort notwendig. Einige Dienstleister bieten Lösungen an, die Signatur, Verschlüsselung und Antwort automatisieren („E-Mail-Einschreiben“).

In Deutschland wird in der juristischen Fachliteratur die Auffassung vertreten, dass eine E-Mail bereits mit dem Eingang auf dem Server des Empfänger-Providers als zugestellt gilt. Das Eintreffen einer E-Mail im persönlichen Benutzerkonto (Account) des Empfängers ist nicht unbedingt notwendig, um den Status des Zugestelltseins zu erreichen. Übermittlungsfehler bei der Übersendung einer E-Mail von Empfänger-Provider an den individuellen E-Mail-Account des Empfängers könnten vom Empfänger nicht geltend gemacht werden, um die Rechtsfolgen einer E-Mail in Frage zu stellen. Jüngere Urteile bestätigen diese Auffassung. So können zum Beispiel Maklerverträge und Abmahnungen rechtswirksam per E-Mail zugesandt werden.

Im Jahre 2004 gab es verschiedene Versuche, das Spam-Problem in den Griff zu bekommen. Dabei konkurrierten die Verfahren Sender ID von Microsoft, Sender Policy Framework (SPF), DomainKeys von Yahoo und RMX um die Gunst der Umsetzung. Eine IETF-Arbeitsgruppe versuchte, einen Standard zu definieren. Die Funktionsweise ist dabei bei allen Verfahren ähnlich. Durch einen Zusatzeintrag im DNS sollte es möglich sein, den sendenden Mailserver zu verifizieren. Die IETF-Arbeitsgruppe scheiterte aber letztendlich an ungeklärten Patentansprüchen von Seiten Microsofts. Die verschiedenen Verfahren sollen nun in eigenen Verfahren als RFCs umgesetzt werden.

Anders als beim Telefonat erhalten Absender und Empfänger von E-Mails automatisch eine schriftliche Dokumentation über den kommunizierten Inhalt. Diese kann im benutzten E-Mail-Programm oder in einem Archivsystem aufbewahrt und später zur Rekapitulation herangezogen werden.

Inzwischen wird in vielen Ländern der E-Mail-Verkehr vom Staat überwacht. In Deutschland sind seit dem Jahr 2005 Internetdienstanbieter verpflichtet, entsprechende Hard- und Software vorzuhalten, um einer Überwachungsanordnung sofort Folge leisten zu können, ohne für die daraus erwachsenden Kosten einen finanziellen Ausgleich zu erhalten.

Ein allgemeines Verbot, E-Mails zu veröffentlichen, gibt es in Deutschland nicht. Lediglich aus dem Inhalt der Mail kann sich ein Recht des Autors ergeben, gegen die Veröffentlichung vorzugehen. Dabei sind verschiedene Rechtsfolgen möglich, die von Unterlassungsanspruch, zivilrechtlichem Schadensersatzanspruch in Geld bis zu strafrechtlicher Haftung reichen können, andere Rechtsfolgen sind möglich.

In zivilrechtlicher Hinsicht kann die Veröffentlichung eines Briefes das Urheberrecht des Autors verletzen, dies ist allerdings nicht der Fall bei „allgemeinem Inhalt“. Weiterhin kann die Veröffentlichung das allgemeine Persönlichkeitsrecht des Autors verletzen, insofern nehmen die Instanzgerichte im Anschluss an ein Urteil des Bundesgerichtshofs aus dem Jahr 1954 in jedem Einzelfall eine umfangreiche Interessenabwägung vor. Diese allgemeine Rechtsprechung dürfte auch auf E-Mails anwendbar sein.

Es ist davon auszugehen, dass die Rechtsprechung (Oberlandesgericht Rostock, Beschluss vom 17. April 2002 – 2 U 69/01), nach der hinsichtlich Geschäftsbriefen, die im Rahmen einer vertraglichen Zusammenarbeit gewechselt werden, eine ungeschriebene vertragliche Nebenpflicht beider Vertragsparteien gilt, die Briefe vertraulich zu behandeln, auch auf geschäftliche E-Mails anwendbar ist, zumindest, wenn diese verschlüsselt versandt worden sind.

Das Landgericht Köln hat im Leitsatz des Urteils zum Aktenzeichen 28 O 178/06 entschieden:

Die Veröffentlichung einer fremden E-Mail an einen Dritten auf einer Internetseite kann ausweislich dieses Urteils einen Eingriff in das allgemeine Persönlichkeitsrecht des Absenders in Gestalt der Geheimsphäre darstellen. Insofern ist die Widerrechtlichkeit jedoch nicht indiziert, sondern im Einzelfall positiv festzustellen, wofür eine umfassende Güter- und Interessenabwägung erforderlich ist. Gegenüber stehen sich der Zweck der Veröffentlichung und der von der veröffentlichenden Partei angestrebte Zweck sowie die Form, die Art und das Ausmaß des Eingriffs. Ein Verstoß löst eine Pflicht zur Leistung von Schadensersatz aus.

Dabei stellt das Landesgericht die E-Mail einem verschlossenen Brief gleich.

Das Urteil bezieht sich auf einen Fall, in dem E-Mails veröffentlicht worden sind, die zum einen an einen Dritten gerichtet waren und die zum anderen von der veröffentlichenden Partei auf unlautere Weise erlangt worden sind. Auf den Fall einer Veröffentlichung von E-Mails, die an den Betroffenen selbst gerichtet sind, ist die Argumentation des Urteils nicht anwendbar.

Gegenüber den spontanen Aussagen während eines Telefongespräches bietet die schriftliche Formulierung die Chance, die zu übermittelnden Inhalte besser zu durchdenken und zu strukturieren. Ebenso verringert sich die Gefahr einer unbedachten und im Nachhinein bereuten Aussage.

Andererseits muss – im Gegensatz zum Telefonat – der Verfasser einer E-Mail damit rechnen, dass seine Äußerungen langfristig beliebig oft nachgelesen werden können und vom Empfänger mit geringstem Aufwand oder gar unbedacht an eine praktisch beliebige Auswahl von Mitlesern weitergeleitet werden können. Sie haben somit einen stärkeren Öffentlichkeitscharakter.

E-Mails werden sprachpsychologisch von ihren Empfängern oftmals als kräftiger und härter empfunden als vom Verfasser beabsichtigt. Im Gegensatz zum Telefonat oder persönlichen Gespräch entfällt die sofortige Rückkopplung noch während des Verfassens der Kommunikation und damit eine wesentliche Regelungsfunktion.

Die Einfachheit ihrer Benutzung führte dazu, dass E-Mail zu einem weltweiten Standard in der elektronischen Kommunikation wurde. In der Unternehmenskommunikation wird allerdings inzwischen nicht nur die Informationsüberflutung durch die Flut der E-Mails als Problem wahrgenommen.

Die Tatsache, dass der Absender keine Kontrolle darüber hat, inwieweit seine E-Mail bearbeitet ist oder dass zu viele Mitarbeiter unnötig oder andere am Geschäftsvorgang Beteiligten unter Umständen gar nicht in Kenntnis gesetzt sind, begrenzt den Nutzen von E-Mail im betrieblichen Umfeld. Analysten gehen davon aus, dass in Zukunft der Kommunikationsanteil, welcher über Social Community Plattformen (mit Aufgabenlisten, Bearbeitungsstatus und Abonnementfunktionen) und Wikis anstelle von E-Mail oder Instant Messaging abgewickelt wird, dort ansteigen wird, wo Transparenz, Strukturierung und Vernetzung von Projektwissen von Bedeutung sind.

Moderne Netzwerke in wissensintensiven Unternehmen organisieren sich eher horizontal. E-Mails fördern aber in der Tendenz hierarchische Strukturen.



Für den klassischen Brief wird im Englischen verschiedentlich zur Unterscheidung der Ausdruck "" (engl. "Schneckenpost") verwendet.

2003 verbot das französische Ministerium für Kultur den Gebrauch des Wortes "E-Mail" in offiziellen Schreiben staatlicher Einrichtungen und setzte stattdessen das französisch klingende Wort „“ ein (von „“). Der Begriff war bereits in den 1990er Jahren im französischsprachigen Québec üblich.

Obwohl die jiddische Sprache noch stärker als die deutsche von der englischen Sprache beeinflusst ist, haben sich dort die nichtfremdsprachlichen Begriffe "בליצפאסט" "(Blitzpost)" und "בליצבריוו" "(Blitzbrief)" durchgesetzt.

Eine E-Mail mit unfreundlichem, abmahnendem und unangenehmem Inhalt wird im populären Englisch als "" bezeichnet. E-Mails in aggressivem Ton heißen dabei "Flame-Mails".

Angemessenes Benehmen in der elektronischen Kommunikation einschließlich der E-Mail-Kommunikation und der sozialen Netze wird als Netiquette bezeichnet.




</doc>
<doc id="1264" url="https://de.wikipedia.org/wiki?curid=1264" title="Europa (Begriffsklärung)">
Europa (Begriffsklärung)

Europa ist der Name folgender Personen:
Europa heißen folgende geographische und astronomische Objekte:
Europa steht für:

Siehe auch:


</doc>
<doc id="1265" url="https://de.wikipedia.org/wiki?curid=1265" title="Euro">
Euro

Der Euro (griechisch ευρώ, kyrillisch евро; ISO-Code: EUR, Symbol: €) ist laut Abs. 4 EUV die Währung der Europäischen Wirtschafts- und Währungsunion, eines in AEUV geregelten Politikbereichs der Europäischen Union (EU). Er wird von der Europäischen Zentralbank emittiert und fungiert als gemeinsame offizielle Währung in 19 EU-Mitgliedstaaten, die zusammen die Eurozone bilden, sowie in sechs weiteren europäischen Staaten. Nach dem US-Dollar ist der Euro die wichtigste Reservewährung der Welt.

Der Euro wurde am 1. Januar 1999 als Buchgeld, drei Jahre später, am 1. Januar 2002, als Bargeld eingeführt. Damit löste er die nationalen Währungen als Zahlungsmittel ab. Die Euromünzen werden von den nationalen Zentralbanken der 19 Staaten des Eurosystems sowie von derzeit vier weiteren Staaten mit jeweils landesspezifischer Rückseite geprägt. Die Euro-Banknoten unterscheiden sich bei der ersten Druckserie nur durch verschiedene Buchstaben an erster Stelle der Seriennummer, anhand deren festgestellt werden konnte, für welches Land der Schein gedruckt wurde. Bei der zweiten Druckserie ab 2013 beginnt die Seriennummer mit zwei Buchstaben.

Am 10. Januar 2013 wurde in Frankfurt am Main die erste Banknote der zweiten Serie von Euro-Banknoten vorgestellt, die höheren Schutz vor Fälschungen bieten soll.

Die Idee einer einheitlichen europäischen Währung, die den Handel zwischen den Mitgliedstaaten der Europäischen Wirtschaftsgemeinschaft erleichtern sollte (Schaffung eines „gemeinsamen europäischen Markt[es]“), entstand schon recht bald in der Geschichte der europäischen Integration. 1970 wurde das Vorhaben im „Werner-Plan“ erstmals konkretisiert; demnach sollte bis 1980 eine europäische Währungsunion realisiert sein. Das Vorhaben führte 1972 zur Gründung des Europäischen Wechselkursverbunds („Währungsschlange“). Dieser konnte nach dem Zusammenbruch des Bretton-Woods-Systems (März 1973) nicht wie geplant umgesetzt werden. Die Jahre darauf waren geprägt von den Folgen der ersten Ölkrise: im Herbst/Winter 1973/74 vervierfachte sich der Ölpreis; in einigen europäischen Ländern setzten Gewerkschaften aus diesem Anlass zweistellige Lohnsteigerungen durch (→Kluncker-Runde). Es ist umstritten, ob es eine Lohn-Preis-Spirale oder eine Preis-Lohn-Spirale gab (was war Ursache, was war Wirkung?). Viele europäische Länder hatten Stagflation (also Stagnation und Inflation); die damalige Krisenphase wurde und wird auch als Eurosklerose bezeichnet.

Bis Ende 1978 traten mehrere Staaten aus dem Wechselkursverbund aus. Die Europäische Gemeinschaft fokussierte ihre Aktivitäten stark auf den Agrarsektor (Gemeinsame Agrarpolitik (GAP)); in vielen Ländern begann eine Nettozahlerdebatte, die jahrzehntelang anhielt. Industrieländer wie Deutschland und Großbritannien wurden Nettozahler; landwirtschaftlich geprägte Länder wie Frankreich, Spanien und Portugal waren Nettoempfänger.

1979 wurde das Europäische Währungssystem (EWS) eingerichtet, das Schwankungen der nationalen Währungen jenseits einer gewissen Bandbreite verhindern sollte. Zu diesem Zweck wurde die Europäische Währungseinheit ECU geschaffen – eine Korbwährung, die man als Vorläufer des Euro bezeichnen kann. Der ECU diente nur als Verrechnungseinheit. Als Bargeld gab es ihn nicht; einige symbolische Sondermünzen wurden ausgegeben. Einige EG-Mitgliedstaaten emittierten Staatsanleihen in ECU (sie wurden, wie andere Staatsanleihen auch, an den Börsen gehandelt) und nahmen Kredite in ECU auf.

Im Jahr 1988 erarbeitete ein Ausschuss unter Leitung des EG-Kommissionspräsidenten Jacques Delors den sogenannten „Delors-Bericht“. Im Zuge der von Deutschland angestrebten Wiedervereinigung verknüpfte laut Zeitungsberichten der damalige französische Staatspräsident François Mitterrand die Zustimmung Frankreichs zur Wiedervereinigung mit der Zustimmung des damaligen Bundeskanzlers Helmut Kohl zur „Vertiefung der Wirtschafts- und Währungsunion“, also mit der Einführung des Euro. Kohl widersprach dieser Darstellung, hätte aber die gemeinsame europäische Währung für einen angemessenen Preis für die deutsche Einheit betrachtet. Er stimmte dem ohne vorherige Rücksprache mit Bundesbankpräsident Hans Tietmeyer zu. Wie im Delors-Bericht vorgeschlagen schuf man in drei Schritten die Europäische Wirtschafts- und Währungsunion:


Am 2. Mai 1998 beschlossen die Staats- und Regierungschefs der Europäischen Gemeinschaft in Brüssel die Einführung des Euro. Bundeskanzler Kohl war sich bewusst, dass er damit gegen den Willen einer breiten Bevölkerungsmehrheit handelte. In einem 2013 bekanntgewordenen Interview vom März 2002 sagte Kohl: „In einem Fall [Einführung des Euro] war ich wie ein Diktator“.


Im Vertrag von Maastricht von 1992 einigten sich die EU-Mitgliedstaaten auf bestimmte „Konvergenzkriterien“, die Staaten erfüllen mussten, um den Euro als Währung einzuführen. Sie umfassen im Einzelnen die Stabilität der öffentlichen Haushalte, des Preisniveaus, der Wechselkurse zu den übrigen EU-Ländern und des langfristigen Nominalzinssatzes. Auf Initiative des damaligen deutschen Finanzministers Theo Waigel wurde das erste dieser Kriterien auf dem Gipfel in Dublin 1996 auch über den Euro-Eintritt hinaus festgeschrieben. Dieser Stabilitäts- und Wachstumspakt erlaubt den Euroländern eine jährliche Neuverschuldung von maximal 3 % und einen Gesamtschuldenstand von maximal 60 % ihres Bruttoinlandsprodukts.

Allerdings kam es sowohl vor als auch nach der Euro-Einführung immer wieder zu Verstößen der Mitgliedstaaten gegen diese Regelungen. So konnte insbesondere Griechenland den Euro nur aufgrund von geschönten Statistiken einführen, und zahlreiche Mitgliedstaaten, darunter auch Deutschland und Frankreich, verstießen mehrfach gegen den Stabilitäts- und Wachstumspakt. Die darin vorgesehenen Sanktionen gegen Euroländer mit überhöhtem Defizit, die von den Finanzministern der übrigen Mitgliedstaaten verhängt werden können, wurden bisher jedoch noch kein einziges Mal angewandt. Insbesondere infolge der Staatsschuldenkrise in einigen europäischen Ländern führte dies ab 2010 zu einer politischen Debatte über die Europäischen Wirtschafts- und Währungsunion als mögliche Fiskalunion.

Nachdem zunächst die Bezeichnung der alten Verrechnungswährung ECU auch für die geplante Gemeinschaftswährung erwartet worden war, wurde Anfang der 1990er-Jahre Kritik daran laut, da sie – als Abkürzung für "European Currency Unit" – zu technisch und unpersönlich sei. Dass die Bezeichnung in Anlehnung an den seit dem Mittelalter bekannten französischen Écu verstanden werden konnte, wurde hierbei weitgehend übersehen. Am 16. Dezember 1995 legte der Europäische Rat in Madrid daher einen anderen Namen der neuen Währung fest: „Euro“. Der Begriff soll regelkonform nur in der Einzahl verwendet werden (siehe unten, Pluralformen).

Zuvor waren auch alternative Vorschläge im Gespräch. Wichtige Kandidaten waren "europäischer Franken" (der in seiner spanischen Übersetzung "Franco" jedoch in unpassender Weise an Francisco Franco erinnert hätte), "europäische Krone" und "europäischer Gulden." Durch die Verwendung eines bereits bekannten Währungsnamens sollte Kontinuität signalisiert und das Vertrauen der Bevölkerung in die neue Währung gefestigt werden. Darüber hinaus hätten einige Teilnehmerstaaten den bisherigen Namen ihrer Währung beibehalten können. Gerade dies stieß allerdings auch auf Kritik, da es einen Vorrang bestimmter Mitgliedstaaten gegenüber anderen angedeutet hätte. Letztlich scheiterten alle Vorschläge an den Vorbehalten einzelner Staaten, insbesondere Großbritanniens. Als Reaktion schlug die deutsche Delegation um den damaligen Finanzminister Theodor Waigel den Namen „Euro“ vor. Im Beschluss des Deutschen Bundestages war noch die Rede davon, den Währungsnamen regional mit den Namen der bisherigen Währungen zu erweitern, also in Deutschland „Euro-Mark“, in Frankreich „Euro-Franc“.

Die symbolische Wertangabe Euro auf einer Medaille ist erstmals für eine Ausgabe aus dem Jahr 1965 nachweisbar. Eine weitere private Prägung mit dieser Nominalbezeichnung ist 1971 in den Niederlanden hergestellt worden. Dabei wird der erste Buchstabe der Bezeichnung Euro als ein "C" mit eingefügtem kurzem, leicht geschlängeltem Strich geschrieben. Der erste Buchstabe der Umschrift "EUROPA FILIORUM NOSTRORUM DOMUS" (lat.: "Europa [ist] das Haus unserer Kinder") wird ebenso geschrieben.

Am 31. Dezember 1998 wurden die Wechselkurse zwischen dem Euro und den einzelnen Währungen der Mitgliedstaaten unwiderruflich festgelegt, am 1. Januar 1999 wurde der Euro gesetzliche Buchungswährung. Er ersetzte die frühere Korbwährung ECU (European Currency Unit) in einem Umrechnungsverhältnis von 1:1. Einen Tag später, am 2. Januar, notierten die europäischen Börsen bereits sämtliche Wertpapiere in Euro.

Eine weitere Änderung im zeitlichen Zusammenhang mit der Euro-Einführung war der Wechsel in der Methode der Preisdarstellung für Devisen. In Deutschland war bis zum Stichtag die "Preisnotierung" (1 USD = x DEM) die übliche Darstellungsform. Seit 1. Januar 1999 wird der Wert von Devisen in allen Teilnehmerländern in Form der "Mengennotierung" dargestellt (1 EUR = x USD). Ferner konnten seit dem 1. Januar 1999 Überweisungen und Lastschriften in Euro ausgestellt werden. Konten und Sparbücher durften alternativ auf Euro oder die alte Landeswährung lauten.

Der Europäische Rat beschloss im Juni 2000 in Santa Maria da Feira auf Empfehlung der Europäischen Kommission, Griechenland in das Euro-Währungsgebiet aufzunehmen. Griechenland trat dem Euro zwei Jahre nach den anderen Mitgliedstaaten zum 1. Januar 2001 bei.

In Deutschland wurde der Euro im Rahmen des sogenannten „Frontloading-Verfahrens“ ab September 2001 an Banken und Handel verteilt. Der Handel sollte durch die Ausgabe von Euro und Annahme von D-Mark in den Umtauschprozess einbezogen werden.

Ab dem 17. Dezember 2001 konnte in deutschen Banken und Sparkassen bereits eine erste Euromünzenmischung, auch „Starterkit“ genannt, erstanden werden. Diese Starterkits beinhalteten 20 Münzen im Wert von 10,23 Euro und wurden für 20 D-Mark ausgegeben, wobei die anfallende Rundungsdifferenz durch die Staatskasse übernommen wurde.

Um nach den Weihnachtsfeiertagen und dem Jahreswechsel 2001/2002 Schlangen an den Schaltern der Banken zu vermeiden, wurde es ermöglicht, auch im Januar und Februar 2002 beim Handel in D-Mark zu bezahlen. Das Wechselgeld wurde vom Handel in Euro und Cent herausgegeben. Zusätzlich kam ab 1. Januar 2002 Euro-Bargeld durch Abhebung an Geldautomaten und an den Schaltern der Banken in Umlauf. Weiter gab es in den ersten zwei Wochen des Januar Schlangen an den Umtauschschaltern der Banken und Sparkassen. Ab Ende Januar 2002 wurden Barbeträge hauptsächlich in Euro gezahlt. Eine Unwägbarkeit bei der Einführung des Euro-Bargeldes war, dass die Beschaffenheit, das Aussehen und die Formate der neuen Banknoten bewusst nicht vorab veröffentlicht wurden, um Fälschungen in der Einführungsphase zu vermeiden. Auch die Sicherheitsmerkmale, z. B. Wasserzeichen, Sicherheitsfaden, Hologrammfolie und Mikroschrift, wurden nicht vorab bekanntgegeben.

Während die Umstellung der Geldautomaten weitgehend unproblematisch verlief, befürchtete die Automatenwirtschaft Umsatzverluste, da die Automaten entweder Euro oder D-Mark akzeptierten (andere Zahlungsvarianten wie etwa die GeldKarte hatten damals keine nennenswerte Bedeutung). Einige Verkehrsunternehmen wie etwa der Rhein-Main-Verkehrsverbund hatten zum Stichtag ungefähr die Hälfte der Automaten auf Euro umgestellt, sodass die Kunden vielerorts einen 'alten' und einen 'neuen' Automaten vorfanden. Der Übergang verlief unproblematischer als befürchtet, sodass viele Automaten früher als zunächst geplant auf Euro umgestellt wurden.

Die Konten bei Banken und Sparkassen konnten auf Wunsch seit dem 1. Januar 1999 in Euro geführt werden. Im Rahmen der Einführung des Euro-Bargeldes wurden die Konten dann zum 1. Januar 2002 automatisch auf Euro umgestellt; einige Institute führten diese Umstellung jedoch schon für alle Kunden im Dezember 2001 durch. Die Umstellung war unentgeltlich. In den Übergangsjahren 1999 bis einschließlich 2001 konnten Überweisungen wahlweise in DM oder in Euro getätigt werden; abhängig davon, in welcher Währung das Zielkonto geführt wurde, erfolgte eine automatische Umrechnung; ab dem 1. Januar 2002 waren Überweisungen und Scheckzahlungen nur noch in Euro möglich.

Bestehende Verträge blieben gültig. Geldbeträge wurden im Regelfall zum 1. Januar 2002 umgerechnet (mit dem Faktor 1,95583), so dass sowohl Forderungen als auch Verbindlichkeiten wertmäßig unverändert blieben. Gleichwohl war es im Rahmen noch vorhandener Bargeldbestände bis zum Ende der Übergangsfrist am 28. Februar 2002 möglich, die alte DM-Forderung auch in DM bar zu begleichen.

In Deutschland endete die Übergangsfrist der parallelen Annahme von D-Mark und Euro durch den Handel mit Ablauf des 28. Februar 2002. Seitdem ist der Umtausch der D-Mark in Euro nur noch bei den Filialen der Deutschen Bundesbank (ehemals Landeszentralbanken) unbegrenzt und kostenfrei möglich.
Im Rahmen von Sonderaktionen nehmen manche deutsche Handelsketten und Einzelhändler hin und wieder die Deutsche Mark als Zahlungsmittel an.

Trotz der einfachen und kostenlosen Umtauschmechanismen waren im Juli 2016 noch immer DM-Münzen und -Scheine im Wert von umgerechnet 12,76 Milliarden Euro im Umlauf. Dabei handelt es sich nach Ansicht der Deutschen Bundesbank jedoch größtenteils um verlorengegangenes oder zerstörtes Geld.

Der Euro ist somit die fünfte Währung in der deutschen Währungsgeschichte seit der Reichsgründung 1871. Vorgänger waren Goldmark, Rentenmark (später Reichsmark), Deutsche Mark sowie die Mark der DDR (vorher "Deutsche Mark" beziehungsweise "Mark der Deutschen Notenbank").

In Österreich begann die Oesterreichische Nationalbank am 1. September 2001 mit der Vorverteilung von Euromünzen und -banknoten an die Kreditinstitute. Diese konnten sofort damit beginnen, die Firmenkunden und den Handel mit dem neuen Zahlungsmittel zu versorgen. Dafür wurden von der Nationalbank Kassetten mit Münzrollen, offiziell "Startpaket Handel" genannt, im Wert von 145,50 Euro mit einem Gegenwert von 2.000 Schilling für die Kassenausstattung im Handel ausgegeben. Unabhängig davon konnte jedes Unternehmen seinen individuellen Eurobedarf bei seinem Kreditinstitut anmelden.

An Privatpersonen wurden die offiziell "Startpaket" benannten Münzbeutel ab 15. Dezember 2001 ausgegeben. Sie enthielten 33 Münzen im Gesamtwert von 14,54 Euro mit einem Gegenwert von 200,07 Schilling und wurden für 200 Schilling ausgegeben. Die allgemeine Geldausgabe – insbesondere auch der neuen Banknoten – begann am 1. Januar 2002.

Wie in Deutschland lief auch in Österreich vom 1. Januar bis zum 28. Februar 2002 die sogenannte Parallelumlaufphase, in der mit beiden Währungen bar gezahlt werden konnte, also entweder mit Schilling oder mit Euro – aber auch mit einer Mischung. Zwar verlor der Schilling mit Wirkung vom 1. März 2002 seine Gültigkeit als offizielles Zahlungsmittel; da aber Schillingbanknoten und -münzen bei der Oesterreichischen Nationalbank und Schillingmünzen bei der Münze Österreich unbefristet und kostenlos in Euro umgetauscht werden können, nahmen viele Geschäfte über die gesetzlich vorgesehene Zeit hinaus noch den Schilling an. Die Umstellung an den "Bankomaten" verlief weitgehend problemlos; die dort ausgegebenen Banknoten waren anfangs nur 10- und 100-Euro-Scheine. Die Begrenzung der täglich möglichen Bargeldbehebung von Bankomaten wurde mit der Umstellung von 5000 Schilling (363,36 Euro) auf 400 Euro erhöht. Im unbaren Zahlungsverkehr erfolgte die Umstellung aller Konten und Zahlungsaufträge automatisch am 1. Januar 2002.

Während andere Warenautomaten wie zum Beispiel für Zigaretten nach und nach von Schilling auf Euro umgestellt wurden, wurden die Zuckerl-, Kaugummi-, Kondom- und Brieflosautomaten des Aufstellers Ferry Ebert vom Markt genommen. Für die Firma war das Umrüsten der allein in Österreich rund 10.000 Automaten nicht zu finanzieren; ihre Automaten sind begehrte Sammelobjekte geworden.

Zum Stichtag 31. März 2010 waren nach Nationalbank-Angaben noch Schillingbestände von 9,06 Milliarden Schilling mit einem Gegenwert von 658,24 Millionen Euro im Umlauf. Davon entfielen unbegrenzt in Euro umtauschbare 3,45 Milliarden Schilling (250,9 Millionen Euro) auf Banknoten und 3,96 Milliarden Schilling (287,5 Millionen Euro) auf Münzen. Die Differenz, rund 18 %, 1,65 Milliarden Schilling (119,8 Millionen Euro), entfällt jedoch auf die letzten beiden zum Teil noch im Umlauf befindlichen Banknoten, die mit einer Präklusionsfrist bis 20. April 2018 versehen sind und die schon lange vor der Euro-Einführung ihre gesetzliche Zahlungskraft verloren hatten. Es handelt sich dabei um die 500-Schilling-Scheine „Otto Wagner“ und die 1000-Schilling-Scheine „Erwin Schrödinger“.

Um den Österreichern, aber auch ausländischen Gästen eine einfache Möglichkeit zu bieten, ihre noch vorhandenen Schillingbestände in Euro umzutauschen, fährt seit 2002 während der Sommermonate der Euro-Bus der Oesterreichischen Nationalbank durch Österreich. Ein Nebenzweck der Aktion liegt darin, die Bevölkerung über die Sicherheitsmerkmale der Euroscheine zu informieren.

Die Umstellung auf den Euro war die sechste Währungsreform oder -umstellung in der österreichischen Währungsgeschichte seit 1816 nach den Napoleonischen Kriegen. Vorgänger des Euros waren in Österreich der Gulden, die Krone (Österreich-Ungarn), der Schilling (Erste Republik), die Reichsmark (nach dem Anschluss ans „Dritte Reich“) und der Schilling (Zweite Republik), 1947 gab es eine Währungsreform mit einer Schillingabwertung auf ein Drittel.

Bei allen bisherigen Teilnehmern wurde das Euro-Bargeld zu Jahresbeginn eingeführt.

In einer kurzen Übergangszeit nach der Einführung des Euro-Bargeldes war in jedem teilnehmenden Staat Bargeld in Euro und der alten Landeswährung in Umlauf. Die ehemaligen Landeswährungen waren allerdings zu dieser Zeit in der Regel keine gesetzlichen Zahlungsmittel mehr, wurden aber zahlungshalber angenommen; die Umrechnung in Euro erfolgte zum offiziell festgelegten Wechselkurs. Die Zeit des parallelen Bargeldumlaufes wurde unterschiedlich festgesetzt, zum Beispiel bis Ende Februar oder bis Ende Juni 2002. Die meisten Währungen können oder konnten auch danach noch bei der jeweiligen nationalen Zentralbank gegen Euro eingetauscht werden.

In den Euroländern ist der Umgang mit den früheren Währungen unterschiedlich geregelt. Auch nachdem diese nicht mehr gesetzliches Zahlungsmittel sind, gibt bzw. gab es die Möglichkeit zum Umtausch. Die Umtauschfristen unterscheiden sich aber:

In Deutschland hat ein Forschungsteam der Fachhochschule Ingolstadt zweieinhalb Jahre nach Einführung des Euros eine Studie zu dessen Akzeptanz in der deutschen Bevölkerung vorgelegt. Danach standen zur Erhebungszeit (2004) fast 60 % der deutschen Bevölkerung dem Euro positiv gegenüber. Viele der Befragten trauerten jedoch um die D-Mark. Auch rechneten viele der Befragten Preise von Euro in D-Mark um, bei höheren Beträgen häufiger als bei niedrigen. Bei allen Preisen rechneten lediglich 48 % der Befragten um, bei Preisen über 100 Euro jedoch noch 74 %. Der Grund hierfür ist der einfache Umrechnungsfaktor (recht genau 1:2, exakt 1:1,95583). Zudem verbindet die Bevölkerung mit der Einführung des Euros aber auch eine allgemeine Preisanhebung, die Teile des Einzelhandels vornahmen. In manchen der Euroländer (zum Beispiel in Frankreich und den Niederlanden) waren Preiserhöhungen im Zeitraum der Euro-Einführung gesetzlich untersagt, in Deutschland hatte man auf eine Selbstverpflichtung des Handels gesetzt. Bei Auslandsreisen und Urlaubsaufenthalten in seinem Geltungsbereich gewinnt der Euro deutlich an Sympathie. Auch der bessere Preisvergleich innerhalb Europas wird positiv vermerkt. Laut der genannten Studie begrüßen viele der Befragten auch, dass durch die gemeinsame EU-Währung ein Gegenpol zu US-Dollar und Yen geschaffen wurde.

Laut Eurobarometer 2006 war eine relative Mehrheit von 46 % der deutschen Bevölkerung der Meinung, „Der Euro ist gut für uns, er stärkt uns für die Zukunft“, während 44 % der Meinung waren, der Euro „schwächt das Land eher“. 2002 waren die Eurobefürworter (39 %) noch in der Minderheit gegenüber den Euroskeptikern (52 %). Eine Studie der Dresdner Bank im Auftrag der Forschungsgruppe Wahlen ergab allerdings Ende 2007 ein Absinken der Euroakzeptanz der Deutschen auf 36 % gegenüber 43 % im Jahr 2004.

Laut Eurobarometer 2014 befürwortet mittlerweile mit 74 % eine deutliche Mehrheit der Deutschen den Euro, eine Minderheit von 22 % lehnt ihn ab.

Laut Eurobarometer sind die Österreicher dem Euro gegenüber positiver eingestellt als die Deutschen. 2006 waren 62 % der österreichischen Bevölkerung der Meinung: „Der Euro ist gut für uns, er stärkt uns für die Zukunft“, während 24 % der Meinung waren, der Euro schwäche das Land eher. In Österreich waren bereits 2002 die Eurobefürworter (52 %) in der Mehrheit gegenüber den Euroskeptikern (25 %).

Im Zuge der Einführung des Euros in Lettland stimmen nach dem Marktforschungsunternehmen SKDS lediglich 22 % der lettischen Bevölkerung zu, die Mehrheit von 53 % ist dagegen.

Der Euro wird von der Europäischen Zentralbank (EZB) in Frankfurt am Main kontrolliert. Diese nahm am 1. Juni 1998 ihre Arbeit auf. Die Verantwortung ging jedoch erst mit dem Start der Europäischen Währungsunion (EWU) am 1. Januar 1999 von den nationalen Zentralbanken (NZB) auf die EZB über. Neben der in Artikel 105 des EG-Vertrags festgelegten Sicherung der Preisstabilität hat die EZB auch noch die Aufgabe, die Wirtschaftspolitik der Mitgliedstaaten zu unterstützen. Weitere Aufgaben der EZB sind die Festlegung und Durchführung der Geldpolitik, die Verwaltung der offiziellen Währungsreserven der Mitgliedstaaten, die Durchführung von Devisengeschäften, die Versorgung der Volkswirtschaft mit Geld und die Förderung eines reibungslosen Zahlungsverkehrs. Um die Unabhängigkeit der EZB zu wahren, darf weder sie noch eine der NZB Anweisungen einer der Regierungen der Mitgliedstaaten erhalten oder einholen. Diese juristische Unabhängigkeit ist notwendig, da die EZB das ausschließliche Recht der Banknotenausgabe innehat und somit Einfluss auf die Geldmenge des Euros hat. Dies ist notwendig, um nicht der Versuchung zu erliegen, eventuelle Haushaltslöcher mit einer erhöhten Geldmenge auszugleichen. Dadurch würde das Vertrauen in den Euro schwinden und die Währung würde instabil werden.

Die Europäische Zentralbank bildet zusammen mit den nationalen Zentralbanken, wie der Deutschen Bundesbank, das Europäische System der Zentralbanken und hat ihren Sitz in Frankfurt am Main. Das Beschlussorgan ist der EZB-Rat, der aus dem Direktorium der EZB und den Präsidenten der nationalen Zentralbanken gebildet wird. Das Direktorium besteht wiederum aus dem Präsidenten der EZB, dessen Vizepräsidenten und vier weiteren Mitgliedern, die allesamt regelmäßig für eine Amtszeit von acht Jahren von den Mitgliedern der EWU gewählt und ernannt werden, eine Wiederwahl ist ausgeschlossen.

Als Eurozone wird im strengen Sinne die Gruppe der 19 EU-Länder bezeichnet, die an der dritten Stufe der Europäischen Wirtschafts- und Währungsunion teilnehmen und den Euro als offizielles Zahlungsmittel nutzen („Euro-19“).

Im weiteren Sinne sind damit auch die Staaten gemeint, die den Kurs ihrer eigenen Währung über ein Wechselkurssystem an den Euro gekoppelt oder als Nicht-EU-Mitgliedstaaten, z. T. einseitig, den Euro eingeführt haben. Zu den Nicht-EU-Ländern, die den Euro verwenden, gehören neben den Kleinstaaten Andorra, Monaco, San Marino und Vatikan auch Montenegro und Kosovo. Weiterhin verwenden die zu Frankreich, aber nicht zur EU gehörenden Gebiete Saint-Pierre und Miquelon und Saint Barthélemy den Euro. In den Militärbasen Akrotiri und Dekelia auf Zypern, die unter britischer Hoheit stehen und ebenso nicht zur EU gehören, wird nur mit dem Euro gezahlt.

Einen festen Wechselkurs zum Euro haben in Europa Bosnien-Herzegowina und Bulgarien sowie in Afrika Kap Verde, São Tomé und Príncipe, die Komoren und die 14 Länder der CFA-Franc-Zone. Auch der CFP-Franc, der in einigen pazifischen französischen Übersee-Territorien verwendet wird, ist fest an den Euro gebunden. Andere Wechselkurssysteme, wie der Wechselkursmechanismus II, dem Dänemark angehört, erlauben eine gewisse Bandbreite an Schwankungen um einen Leitkurs. Manche Staaten wie Marokko wiederum haben ihre Währungen an einen Währungskorb gekoppelt, der zu einem bestimmten Anteil am Euro orientiert ist. Die Schweiz setzte von 2011 bis 2015 ein Wechselkurs-Fluktuationslimit. Insgesamt nutzen über vierzig Staaten den Euro oder eine von ihm abhängige Währung.

Im "de jure" zur Republik Zypern gehörenden Nordzypern gilt "de facto" die Türkische Lira als gesetzliches Zahlungsmittel.

Nach den im Vertrag von Maastricht erstmals festgehaltenen Bestimmungen zur Europäischen Wirtschafts- und Währungsunion sind alle EU-Mitgliedstaaten zur Einführung des Euro verpflichtet, sobald sie die EU-Konvergenzkriterien erfüllen, zu denen unter anderem die zweijährige Zugehörigkeit zum Wechselkursmechanismus II (WKM II) zählt. Befreit davon sind – durch Ausnahmeprotokolle – nur Dänemark und das Vereinigte Königreich. Allerdings duldet die Europäische Kommission bislang, dass Schweden durch den Nichtbeitritt zum Wechselkursmechanismus II absichtlich eines der Konvergenzkriterien verfehlt, um so den Eurobeitritt zu vermeiden.

Nach allgemeiner Währungstheorie ist zu erwarten, dass der Euro zu einem vereinfachten Handel zwischen den Mitgliedern der Eurozone und sinkenden bzw. „keinen Transaktionskosten“ führt. Es wird vermutet, dass dies von Vorteil für die Verbraucher und Unternehmen der Eurozone ist, da Handel in der Vergangenheit eine der Hauptquellen ökonomischen Wachstums war. Es wird geschätzt, dass sich seit der Euro-Einführung bis zum Jahr 2009 der Handel innerhalb der Eurozone um 5–15 % erhöht hat. Europäische Unternehmen sollen von dem Wegfall der „Handelshemmnisse zwischen den Mitgliedsländern“ profitieren: eine Ausdehnung der Unternehmungen über den europäischen Markt sowie die Nutzung zunehmender Skaleneffekte sollen einsetzen. Der Euro kann auch als „Vervollständigung des gemeinsamen europäischen Binnenmarktes (freier Verkehr von Waren, Dienstleistungen, Kapital und Personen) gelten“ – man könnte im Umkehrschluss auch konstatieren, dass dem europäischen Binnenmarkt ohne eine gemeinsame Währung eine wichtige Komponente fehlen würde.

Bei Einführung des Euros ging man davon aus, dass Preisunterschiede für Produkte und Dienstleistungen in den Ländern der Eurozone abnehmen würden („Beseitigung der Preisdifferenzierung“): Infolge der ausgleichenden Wirkung des Arbitrage-Handels sollten bestehende Unterschiede schnell ausgeglichen werden. Dies führe zu verstärktem Wettbewerb zwischen Anbietern, niedrigeren Preisen für private Haushalte und damit zu niedriger Inflation und mehr Wohlstand der Verbraucher. Gänzlich beseitigt wird die Preisdifferenzierung jedoch nicht. Für Güter des täglichen Bedarfs werden die Marktteilnehmer nicht große Transportwege und -kosten auf sich nehmen. Eine Angleichung („Konvergenz“) der Preise findet dann nicht statt.
Besondere Vorteile bringt der Euro für Reisende. Sie müssen kein Geld umtauschen bzw. wieder rücktauschen und sparen sich die damit verbundenen Gebühren. Des Weiteren können sie in ihrem Reiseland die Preise nun ohne Probleme mit den Preisen des Herkunftslandes vergleichen.

Bisher bestehende innergemeinschaftliche Wechselkursrisiken und die dadurch notwendigen Währungsabsicherungen würden für europäische Unternehmen entfallen („Verringerung der Wechselkursschwankungen“). Eine Spekulation gegen den Euro ist nach Auffassung vieler Ökonomen aufgrund seiner Größe sehr viel schwieriger als gegenüber kleineren Währungen. Währungsspekulationen hatten in den 1990er-Jahren zu schweren Verwerfungen im Europäischen Währungssystem (EWS) geführt (beispielsweise zum „Schwarzen Mittwoch“ am 16. Dezember 1992). Währungsspekulationen können zu einer ausgeprägten Unter- oder Überbewertung einer Währung führen, mit entsprechenden Konsequenzen für die Inflationsrate und das Wirtschaftswachstum der Währungsgebiete beider Währungen eines Wechselkurses, und erschweren damit einen effizienten Handel zwischen zwei Währungsgebieten. Außerdem können sie die Währungsreserven eines Staates aufzehren. Durch die „Verringerung der Unsicherheit“ durch Wechselkursschwankungen verändert sich das Investitionsverhalten. Die zukünftige Planung und die Kalkulation von Projekten werden erleichtert. Ein Anstieg der Investitionen führt zu einem höheren wirtschaftlichen Wachstum.

In politischer Hinsicht manifestiert der Euro die Zusammenarbeit der europäischen Staaten und ist ein greifbares Symbol europäischer Identität. Er kann zur Konsolidierung der Europäischen Union beitragen und, wie vor der Gründung der Europäischen Währungsunion vielfach erwartet und gehofft, langfristig zur Schaffung einer „politischen Union“ beitragen.

Im Allgemeinen konnte die Europäische Zentralbank ihre Hauptaufgabe erfüllen, das heißt mit ihrer Geldpolitik für eine "stabile" und weder zu hohe noch zu niedrige "Inflation" sorgen. Das Inflationsziel von „unter, aber nahe bei zwei Prozent“ wurde meist erreicht bzw. eine langfristige Abweichung verhindert.

In der Vergangenheit wurden die EU-Konvergenzkriterien hinsichtlich der Staatsverschuldung von fast keinem Land konstant eingehalten. Politisch ist für Ökonomen, welche die Bedeutung eines ausgeglichenen Staatshaushalts hoch einschätzen, fraglich, ob EZB und Europäische Kommission die Mitgliedstaaten zu hinlänglicher Haushaltsdisziplin anhalten können: Entziehen sich einzelne Länder oder Ländergruppen ihrer unterstellten haushaltspolitischen Verantwortung, werden Inflationsrate und Finanzierungskosten für diese Länder solange niedrig bleiben, wie sich der Großteil der restlichen Euroländer nicht zu stark verschuldet. Dies kann in haushaltspolitisch unverantwortlichen Schuldenländern verspätete oder nicht ausreichende Korrekturen der Haushaltspolitiken fördern und zu Wohlstandseinbußen führen.

In der Praxis hat sich die Geld- und Zinspolitik im heterogenen Wirtschaftsraum als schwierig erwiesen "(„Aufgabe der nationalen Geldpolitik“)":
Wachstumsraten von über 5 % in Irland mussten mit Raten nahe Null in den iberischen Staaten in Einklang gebracht werden: Der irischen Situation wäre nach bisher angewandten, „nationalen“ Methoden mit Leitzinserhöhungen und Geldmengenverknappung zu begegnen gewesen, während im Gegenbeispiel Zinslockerungen üblich gewesen wären. Solche regionalen Unterschiede lassen sich mit der "einheitlichen Geldpolitik" der Eurozone durch die EZB nicht hinreichend abbilden. Den „nationalen Volkswirtschaften“ ist „ein individuell einsetzbares wirtschaftpolitisches Instrument abhanden gekommen.“

Ein wesentliches volkswirtschaftliches Problem stellte zu Beginn die Festlegung der Wechselkurse der an der Einheitswährung beteiligten Währungen dar. Eine Volkswirtschaft, die mit überbewerteter Währung der Einheitswährung beitritt, wird im Vergleich ein höheres Vermögen, jedoch auch ein höheres Preisniveau (höhere Kosten und Preise) aufweisen als Staaten, die unterbewertet oder reell bewertet der Einheitswährung beitreten. Aufgrund des höheren Preisniveaus besteht ein großer Importanreiz und verminderte Exportchancen und in der Folge steigende Arbeitslosigkeit. Um die Wettbewerbsfähigkeit der Wirtschaft zu erhalten, ist eine Absenkung des Preisniveaus (in prozentualer Höhe der Überbewertung) notwendig. Ein volkswirtschaftlicher Ausgleich der Überbewertung ist in der Währungsunion mangels Wechselkursmechanismus nur über innere Abwertung erreichbar.

Ein weiterer Effekt betrifft die internationalen Rohstoffpreise, insbesondere den volkswirtschaftlich bedeutsamen Erdölpreis. Öl wird nach wie vor meist in US-Dollar berechnet, und die OPEC akzeptiert seit den 1970er-Jahren sogar nur noch den US-Dollar. Innerhalb der OPEC wurde allerdings diskutiert, die Preise auf Euro umzustellen, womit auch viele Drittländer gezwungen wären, Teile ihrer Devisenreserven für Ölkäufe von US-Dollar- in Euroguthaben umzuwandeln, was äußerst negative Auswirkungen auf den US-Dollar und die US-Wirtschaft hätte, die durch den stetig weiter wachsenden Handel mit Öl stabilisiert wird. Der Irak hatte seine Ölverkäufe unter Saddam Hussein im Jahre 2000 bereits gänzlich in Euro abgerechnet, was allerdings seitens der USA am 10. Juni 2003, rund einen Monat nach der Eroberung des Landes, wieder rückgängig gemacht wurde. Sowohl der Iran als auch Venezuela unter Hugo Chávez, der ein besonders lautstarker Befürworter des Wechsels war, äußerten sich in der Folge zustimmend zu einer solchen Umstellung. Der Iran eröffnete darüber hinaus am 17. Februar 2008 eine eigene, nicht an den US-Dollar gebundene Ölbörse mit Sitz auf der Insel Kisch. Die Ölmengen, die das Land über diesen Handelsplatz exportiert, sollen allerdings zu gering sein, um die Stellung des US-Dollars als „Ölwährung“ ernsthaft gefährden zu können.

Schon vor, aber insbesondere nach der Bargeldeinführung des Euros im Januar 2002 wurden eventuelle Preissteigerungen durch die Währungsumstellung diskutiert.

Die Statistikbehörden der europäischen Länder ermitteln monatsweise Verbraucherpreisindizes, um den Preisverlauf zu ermitteln. In den deutschsprachigen Euroländern konnten hierbei nur minimale Unterschiede festgestellt werden. In keinem der deutschsprachigen Euroländer stieg die Inflation im Frühjahr 2002 über Werte hinaus, die sie nicht auch schon im Sommer 2001 erreicht hatte. Insgesamt war die Inflationsrate in den Jahren 2002 und 2003 sehr niedrig und unter dem Niveau der vorangegangenen Jahre.

Auch über längere Zeiträume gesehen war die Inflation etwas niedriger als in den Jahren vor dem Euro. So stieg der deutsche Verbraucherpreisindex in den fünf Jahren vor der Einführung um 7,4 %, während er in den fünf Jahren danach um 7,3 % stieg. Auch in Österreich stieg laut Statistik Austria der österreichische Verbraucherpreisindex in den zwölf Jahren von 1987 bis 1998 um durchschnittlich 2,45 % pro Jahr, während die Inflationsrate von 1998 bis 2003 auf durchschnittlich 1,84 % sank.

Diese Inflationsrate war jedoch nicht über alle Produktgruppen gleich. Für Waren und Dienstleistungen des täglichen Gebrauchs führte das Institut der Deutschen Wirtschaft im Jahr 2002 eine detaillierte Untersuchung der Daten des Statistischen Bundesamtes durch und ermittelte einen Preisanstieg im ersten Quartal von 4,8 %. Bei einzelnen Produktgruppen konnten stark überdurchschnittliche Preisanstiege festgestellt werden. Die Forscher kamen zu dem Schluss, dass das in der Bevölkerung verbreitete Gefühl starker Verteuerung nicht unbegründet sei, da Anstiege in diesem Bereich stärker wahrgenommen würden als Fixkosten wie Miete oder Heizung, die unverändert geblieben waren. Diese Studie zeigt zwar, dass die Preise in verschiedenen Bereichen Anfang 2002 erheblich stiegen, aber konnte nicht die weitere Entwicklung des Jahres 2002 abbilden. Die Daten des Statistischen Bundesamtes zeigen einen Preisfall unter das Niveau von 2001 gegen Ende 2002 in verschiedenen Produktgruppen, darunter auch den Lebensmitteln.

Nach der Einführung des Euros empfanden viele Verbraucher eine Verteuerung von Waren und Dienstleistungen über der Inflationsrate. Der Anteil derer, die eine schnellere Inflation wahrnahmen, stieg im ganzen Euroraum ab Januar 2002 rapide an.

Umgangssprachlich kam daher zunehmend die von dem Satiremagazin "Titanic" eingeführte und anschließend von vielen Zeitungen verwendete Bezeichnung „Teuro“ auf. Sie wurde auch zum Wort des Jahres 2002 gewählt.

In Deutschland und den Niederlanden war die Wahrnehmung vermeintlicher Preissteigerungen am größten. In den deutschen Medien und der deutschen Politik wurde eine Debatte über vermeintliche Preisverwerfungen geführt. Auch in Österreich entstand bei einer Mehrheit der Eindruck, der Euro beeinflusse die Preisentwicklung negativ.

Für die Diskrepanz zwischen der gemessenen, gesunkenen Inflation und der subjektiv gefühlten Inflation in der Zeit nach der Euro-Einführung gibt es verschiedene Erklärungsansätze. Das Institut der Deutschen Wirtschaft weist schon in seiner Studie 2002 darauf hin, dass bestimmte alltäglich gekaufte Güter wie Lebensmittel tatsächlich überdurchschnittlich teurer wurden, was deutlich stärker wahrgenommen wurde als eine gegenläufige Entwicklung bei Produkten, die man seltener kauft, oder bei monatlich vom Konto abgebuchten Kosten.

Zur psychologischen Seite der Diskrepanz wurden u. a. von der Psychologin Eva Traut-Mattausch Untersuchungen durchgeführt, bei denen Probanden Preisänderungen bei der Währungsumstellung abschätzen sollten. Es ergab sich, dass durchweg die neuen Preise höher eingeschätzt wurden, als sie real waren. Preissenkungen wurden gar nicht, Preiserhöhungen illusorisch verstärkt wahrgenommen. Das hierfür verantwortlich gemachte psychologische Phänomen ist der schon seit Jahrzehnten bekannte so genannte Bestätigungsfehler, bei dem die Beurteilung von Informationen dadurch beeinflusst wird, welche Erwartungen zuvor bestehen. Den Erwartungen entsprechende Informationen werden als glaubwürdiger und wichtiger erachtet. Im Zusammenhang der Preiseinschätzung wirkt sich dies so aus, dass Umrechnungsfehler dann eher korrigiert werden, wenn sie der Erwartung zuwiderlaufen. In einem sehr ähnlichen Versuch in Österreich waren die Ergebnisse gleich.

Es wurde auch vermutet, dass die Wahrnehmung des Preises durch Rundungsfehler bei der Überschlagsrechnung (in Deutschland etwa 1:2 statt 1:1,95583 oder in Österreich 1:14 statt 1:13,7603) beeinflusst wird. In den psychologischen Studien zum Bestätigungsfehler konnte jedoch kein solcher Effekt festgestellt werden.

Aufgrund der in den letzten Jahren festen Wechselkursentwicklung des Euro zu fast allen anderen bedeutenden Währungen und der anhaltenden fiskalpolitischen Schwierigkeiten der USA erwarten einzelne Ökonomen eine allmähliche Erosion und letztendlich die Ablösung des US-Dollars als Weltreserve- und Weltleitwährung. Dies würde das Ende einer Ära bedeuten, die nach dem Zweiten Weltkrieg mit der Ablösung des bis dahin dominierenden britischen Pfundes durch den US-Dollar begann.

Die meisten Wissenschaftler bewerten die wiederkehrenden Äußerungen aus Entwicklungs- und Schwellenländern bezüglich einer Umgewichtung bei ihren Währungsreserven oder einer Neu-Fakturierung von Rohölpreisen in Euro eher als politisches Druckmittel auf die USA, weniger als konkrete Absicht.

2006 war der Euro – gemessen an den Handels- und Finanzbeziehungen der meisten Drittländer mit der Eurozone – noch deutlich unterrepräsentiert.

Als führende internationale Bargeldwährung hat der Euro den US-Dollar 2006 abgelöst. Seit Oktober 2006 ist der Wert der im Umlauf befindlichen Eurobanknoten mit 592 Milliarden Euro höher als der der US-Dollar-Banknoten (579 Milliarden US-Dollar). Dies hängt jedoch auch damit zusammen, dass in den USA Einkäufe deutlich öfter mittels Kreditkarte bezahlt werden. Dadurch ist pro Person durchschnittlich weniger Bargeld im Umlauf.
Nach Einführung des Euros erlebte Deutschland eine wirtschaftliche Schwächephase. Hierfür sehen Ökonomen mehrere Gründe, die zum Teil mit dem Euro zusammenhängen. So sei Deutschland aufgrund politischer Fehler mit einem überhöhten Wechselkurs in die Euro-Währungsunion eingetreten, wodurch ein zu hohes Preisniveau entstand. Dies habe die Wettbewerbsfähigkeit Deutschlands verringert. Erst durch langjährige Lohnzurückhaltung der Tarifparteien sei es wieder zu einer Verringerung des Preisniveaus und damit zu einer Verbesserung der Wettbewerbsfähigkeit gekommen. Hans-Werner Sinn fasst die Realabwertung (innere Abwertung) wie folgt zusammen: „Wir sind billiger geworden und in gewisser Weise auch ärmer“. Zusätzlich entfielen mit Einführung des Euros die Wechselkursrisiken, die Finanzmarktakteure glichen daraufhin die Kreditzinsen für den gesamten Euroraum auf ein einheitliches Niveau an. Die Zinskonvergenz sorgte dafür, dass Kapital aus Euroländern mit niedriger Inflation abgezogen wurde und in Euroländer mit hoher Inflation floss, wo es zu einer wirtschaftlichen Überhitzung und später zu Zahlungsschwierigkeiten kam. Länder wie Deutschland erlitten in dieser Zeit eine Investitionsschwäche.

Zu den spezifisch für Deutschland positiven Auswirkungen zählt der gemessen an der deutschen Wirtschaftskraft relativ moderate Wechselkurs des Euros. Nach dem Sondergutachten des Sachverständigenrats zur Begutachtung der gesamtwirtschaftlichen Entwicklung vom 5. Juli 2012 würde eine Wiedereinführung der Deutschen Mark zu einer erheblichen Aufwertung (Preisniveauerhöhung gegenüber anderen Währungsräumen) führen und somit dauerhaft die internationale Wettbewerbsfähigkeit der deutschen Wirtschaft nicht nur in Europa, sondern weltweit erheblich beeinträchtigen.

Im Vorfeld der Euro-Einführung in einem Mitgliedstaat der EWU entscheiden die EU-Finanzminister über den endgültigen Umtauschkurs. Der Wechselkurs wird dabei immer auf insgesamt sechs signifikante Stellen (d. h. vor und gegebenenfalls auch nach dem Komma) genau festgelegt, um Rundungsfehler möglichst gering zu halten.

Die Wechselkurse der Währungen der ursprünglich an der Währungsunion teilnehmenden Staaten wurden am 31. Dezember 1998 von den Finanzministern festgelegt. Basis war dabei der Umrechnungswert der zuvor bestehenden ECU. Bei späteren Beitritten zum Euro (Griechenland 2001, Slowenien 2007 sowie Malta und Zypern 2008) wurde der Mittelwert im Rahmen des WKM II als Maßstab genommen.

Seit der Einführung des Euros als Buchgeld dürfen die teilnehmenden Währungen nur über eine "Triangulation" ineinander umgerechnet werden. Dabei muss immer zuerst von der Ausgangswährung in den Euro und dann vom Euro in die Zielwährung umgerechnet werden. Eine Rundung ist dabei ab der dritten Euro-Nachkommastelle sowie in der Zielwährung erlaubt. Durch die Triangulation werden Rundungsfehler verhindert, die bei der direkten Umrechnung auftreten könnten, das Verfahren wurde deshalb von der Europäischen Kommission verbindlich vorgeschrieben.

Bei der Umrechnung von Beträgen nach Euro, die noch in „alten“ Währungseinheiten festgelegt sind, darf erst am Ende der Berechnung der zu zahlende Gesamtbetrag gerundet werden. Eine Rundung von einzelnen Berechnungsfaktoren oder von Zwischenergebnissen würde zu einem anderen Gesamtergebnis führen. Damit würde der Rechtsgrundsatz verletzt, dass die Einführung der neuen Währung die Kontinuität von Verträgen nicht berührt.

Praktisches Beispiel: War in einem Mietvertrag ein monatlich zu zahlender Mietzins vereinbart, der sich als Produkt aus Mietfläche und Quadratmeterpreis berechnet, ist nicht der Quadratmeterpreis in Euro umzurechnen und zu runden, sondern erst der monatliche Zahlungsbetrag. Eine andere Vorgehensweise würde unter Umständen erhebliche Senkungen oder Erhöhungen der monatlichen Zahlungen bewirken (vgl. Urteil des deutschen Bundesgerichtshofs vom 3. März 2005 – III ZR 363/04).
Am 4. Januar 1999, dem ersten Tag des Börsenhandels in Euro an der Frankfurter Börse, hatte die neue Europa-Währung einen Wechselkurs von 1,1789 USD pro Euro. Der Kurs des Euros entwickelte sich in Relation zum US-Dollar zunächst negativ und erreichte über die ersten zwei Jahre des Börsenhandels immer weitere Tiefststände. Am 27. Januar 2000 fiel der Euro unter die Euro-Dollar-Parität; das Allzeittief wurde dann am 26. Oktober 2000 mit 0,8252 USD pro Euro erreicht.

Von April 2002 bis Dezember 2004 wertete der Euro mehr oder weniger kontinuierlich auf; am 15. Juli 2002 wurde wieder die Parität erreicht, am 28. Dezember 2004 erreichte er ein Rekordhoch mit 1,3633 USD. Entgegen den Erwartungen vieler Analysten, von denen manche sogar einen baldigen Anstieg auf über 1,4 USD oder gar 1,6 USD prognostiziert hatten, wertete der Euro wegen der Zinserhöhungspolitik der US-Notenbank im Verlauf des Jahres 2005 wieder deutlich ab und erreichte am 15. November mit 1,1667 USD sein Jahrestief 2005. Diese Zinserhöhungspolitik konnte allerdings wegen der Abschwächung der US-Konjunktur 2006 nicht mehr fortgesetzt werden; erschwerend kam seit der zweiten Jahreshälfte 2007 die Subprime-Krise hinzu, die die US-Notenbank zu mehreren Leitzinssenkungen veranlasste, sodass der Euro erneut aufwertete und der EZB-Referenzkurs am 15. Juli 2008 sein bisheriges Rekordhoch von 1,5990 USD erreichte, wobei der höchste je am Markt gehandelte Kurs bei 1,6038 USD lag. Zum Vergleich: Ihren Höchstwert erreichte die D-Mark am 19. April 1995, als 1 USD 1,3455 DEM kostete – das entspricht umgerechnet 1,45361 USD je Euro. Der an die D-Mark gekoppelte österreichische Schilling erreichte sein Allzeithoch am selben Tag mit einem US-Dollar-Preis von 9,485 Schilling, das sind umgerechnet 1,45074 USD je Euro.

Durch die Dollarschwäche war das Bruttoinlandsprodukt des Euroraums zu Markt-Wechselkursen im März 2008 größer als das der USA.

Ein hoher Eurokurs bringt für die europäische Wirtschaft sowohl Vorteile als auch Nachteile. Vorteilhaft ist die Verbilligung der Rohstoffe, die weiterhin überwiegend in US-Dollar gehandelt werden. Nachteilig ist die Verteuerung der Exporte, die zu Absatzproblemen führen kann. Durch die Größe des Euroraumes haben die Wechselkurse und somit die durch Wechselkursschwankungen hervorgerufenen Wechselkursrisiken jedoch weitaus weniger Bedeutung als zu Zeiten nationaler Währungen. Insbesondere konnte sich Anfang 2007 die europäische Binnenwirtschaft mit einem überdurchschnittlichen Wachstum von der nur moderat wachsenden Weltwirtschaft abkoppeln.

Der niedrige Eurokurs bis in das Jahr 2002 ist vermutlich teilweise auf seine damalige Nichtexistenz als Bargeld zurückzuführen, weswegen der Euro zunächst geringer bewertet wurde, als es allein aufgrund der Fundamentaldaten angemessen gewesen wäre. Die wirtschaftlichen Probleme in der europäischen Gemeinschaft machten Investitionen in Europa für ausländische Anleger unattraktiv, was den Euro weiter schwächte. Kurz nach der Bargeldeinführung kam es zu einer Euro-Aufwertung. Die wirtschaftliche Erholung Europas seit 2005, insbesondere der Exporte, hat die Aufwertung des Euros weiter unterstützt. Es gibt weitere Erklärungen, die auch zu der allgemeinen Annahme einer mittel- und langfristigen Fortsetzung des Euro-Wertzuwachses führen; es werden hierfür vorrangig drei Gründe angegeben:


Im Juli 2008 erreichte der Euro mit einem Kurs von 1,5990 US-Dollar pro Euro sein bisheriges Allzeithoch (siehe Tabelle „Jahreshöchst- und -tiefstwerte“ oben); im Zuge der Griechenland-Finanzkrise 2009/10 fiel der Kurs von 1,35 USD/EUR auf etwa 1,20 USD/EUR (= um etwa 10 %).

Der Euro Currency Index (EUR_I) stellt das arithmetische Verhältnis von vier Leitwährungen im Vergleich zum Euro dar: US-Dollar, britisches Pfund, japanischer Yen und Schweizer Franken. Alle Währungen werden in den Maßeinheiten der Währung pro Euro ausgedrückt. Der Index wurde 2004 vom Börsenportal Stooq.com lanciert. Basiswert sind 100 Punkte am 4. Januar 1971. Vor Einführung der europäischen Gemeinschaftswährung am 1. Januar 1999 wurde ein Wechselkurs von 1 Euro = 1,95583 Deutsche Mark berechnet.

Vergleichbar mit dem arithmetisch gewichteten Euro Currency Index ist der handelsgewichtete Euro Effective Exchange Rate Index der Europäischen Zentralbank (EZB). Der Index der EZB misst im Vergleich zum Euro Currency Index viel akkurater den Wert des Euros, da die Gewichtung der EZB die Wettbewerbsfähigkeit europäischer Güter im Vergleich zu anderen Ländern und Handelspartnern stellt.

Auch andere Unternehmen veröffentlichten Euro Currency Indizes. Die Berechnung wurde aber nach wenigen Jahren wieder eingestellt. Beispiele sind der Dow Jones Euro Currency Index (DJEURO) von Dow Jones & Company von 2005 bis 2009 und der ICE Euro Currency Index (ECX) der Terminbörse ICE Futures U.S., früher New York Board of Trade (NYBOT), von 2006 bis 2011.

Der Euro Effective Exchange Rate Index (Euro EER Index, auch bekannt als Euro Trade Weighted Index) ist eine Kennzahl, welche den Wert des Euros mittels eines Währungskorbs aus verschiedenen Währungen vergleicht. Der Index ist der handelsgewichtete Durchschnitt im Vergleich zu diesen Währungen. Er wurde 1999 von der Europäischen Zentralbank (EZB) erstmals veröffentlicht. Die EZB berechnet die effektiven Wechselkurse im Index für drei Gruppen:


Die EZB bestimmt die Gewichte der einzelnen Partnerländer anhand der Anteile der Fertigerzeugnisse, wie sie in der Standard International Trade Classification (SITC) definiert sind. Für die Gewichte verwendet die EZB die Werte aus den Exporten und den Importen, ohne den Handel innerhalb des Euroraums zu berücksichtigen. Die Einfuhren werden nach dem einfachen Anteil der Partnerländer an den Gesamtimporten in das Euro-Währungsgebiet gewichtet. Die Exporte werden hingegen doppelt gewichtet, wegen der sogenannten „Dritt-Markt-Effekte“. Dies erfasst den Wettbewerb der europäischen Exporteure in ausländische Märkte gegenüber inländischen Produzenten und Exporteuren aus Drittländern.

Der Name „Euro“ wurde auf der Tagung des Europäischen Rates am 15. und 16. Dezember 1995 in Madrid beschlossen und in der "Verordnung (EG) Nr. 974/98 über die Einführung des Euros" festgelegt. In allen Sprachen der Länder, in denen die Währung eingeführt wurde, lautet ihr Name „euro“. Abweichend davon wird im Deutschen die Währung großgeschrieben (Euro), im Griechischen wird das griechische Alphabet verwendet (ευρώ).

Trotz der identischen Schreibweise wird der Name der Gemeinschaftswährung in verschiedenen Sprachen sehr unterschiedlich ausgesprochen:


Die korrekte Bezeichnung der gemeinsamen Währung im Nominativ Singular als „Euro“ findet sich in allen diesbezüglichen Rechtsakten der Europäischen Union und wird sogar von der Europäischen Zentralbank im Rahmen ihrer regelmäßigen Konvergenzberichte als De-facto-Konvergenzkriterium überprüft:

In einer Erklärung zum Vertrag von Lissabon stellten die Regierungen von Lettland, Ungarn und Malta am 9. Mai 2008 fest, dass die vereinheitlichte Schreibweise „keine Auswirkungen auf die geltenden Regeln der lettischen, der ungarischen und der maltesischen Sprache“ habe.

Etymologisch leitet sich das Wort „Euro“ als Abkürzung des Namens des Kontinents Europa und damit letztlich aus dem griechischen "Εὐρώπη" ab.

Die Untereinheit des Euros lautet „Cent“. Laut den interinstitutionellen Regeln für Veröffentlichungen der EU sind national abweichende Bezeichnungen allerdings nicht ausgeschlossen. Dies ist ein Zugeständnis an die Länder, deren Währungsuntereinheit bereits vor der Einführung des Euros mit einer Form des Wortes Cent bezeichnet wurde, so z. B. Frankreich und Belgien "(centimes)", Italien "(centesimi)" oder Portugal "(centavos)." Im Finnischen wird zudem die dort für die Untereinheit des Dollars bereits früher gebräuchliche Form "sentti" verwendet. Im Griechischen wird "λεπτό" (Lepto) gebraucht, was auch schon der Name für die Untereinheit der griechischen Drachme war.

Umgangssprachlich ist − auch zur Unterscheidung von den gleichnamigen Untereinheiten anderer Währungen − auch die Bezeichnung „Euro-Cent“ verbreitet. Auch auf den Münzen selbst werden die Worte "Euro" und "Cent" übereinander geschrieben, wobei allerdings "Euro" in kleinerer Schrift als "Cent" erscheint.

Das Wort „Cent“ stammt von "centesimus" (lat. „der Hundertste“ bzw. „das Hundertstel“) ab. Varianten wurden schon seit langem in der Romania für Währungsuntereinheiten benutzt (vgl. Céntimo, Centime, Centavo und Centesimo). Die Form „Cent“ selbst war schon vor der Euro-Einführung über das Niederländische und das Englische ins Deutsche vermittelt worden, insbesondere als Bezeichnung für die Untereinheit des Dollar.

Das Euro-Zeichen wurde 1997 von der Europäischen Kommission als Symbol für die europäische Gemeinschaftswährung eingeführt. Dass es überhaupt ein Symbol gibt, ist eher dem Zufall zu verdanken. Da es nur wenige Währungen gibt, für die ein Symbol existiert, hatte der Rat auch nie über ein Symbol diskutiert. Erst als Anfang 1996 ein Logo für Informationskampagnen gesucht wurde, fand man den Entwurf. Daraus entstand die Idee, dieses Logo auch als Währungssymbol einzuführen. Am 23. Juli 1997 veröffentlichte die Kommission eine Mitteilung über die Verwendung des Euro-Zeichens. Der Text erläutert:

Es basiert auf einem 1974 als Studie geschaffenen Entwurf des ehemaligen Chefgrafikers der Europäischen Gemeinschaft (EG), Arthur Eisenmenger. Es ist ein großes, rundes "E", das in der Mitte zwei waagerechte, versetzte Striche besitzt (oder auch wie ein C mit einem Gleichheitszeichen kombiniert). Es erinnert an den griechischen Buchstaben Epsilon (ε). Ursprünglich sollte die Abkürzung "ECU" verwendet werden. Das Eurozeichen sollte in dieser Form nicht in Texten eingesetzt werden. Typografisch korrekt ist es, das Eurozeichen der verwendeten Schrift zu verwenden (U+20AC).

Allerdings erschien schon – von der Paneuropa-Union (Union Paneuropéenne) im Jahr 1972 herausgegeben – ein Satz mit sieben Werten zu 1, 2, 5, 10, 20, 50 und 100 Euro mit dem Euro-Symbol „€“, das damals etwas anders aussah, aber auch aus einem großen „C“ mit einem eingefügten Gleichheitszeichen bestand. Anlass der Ausgabe waren der 50. Jahrestag der Paneuropa-Union und der 20. Jahrestag der Europäischen Gemeinschaft für Kohle und Stahl, verbunden mit dem Vertrag über die Norderweiterung der Gemeinschaft. Die Stücke zeigen eine Umschrift mit dem Text „CONFŒDERATIO EUROPÆA“. Auf den Rückseiten sind Karl I., Karl V., Napoléon Bonaparte, Richard Nikolaus Graf von Coudenhove-Kalergi, Jean Monnet, Sir Winston Churchill und Konrad Adenauer abgebildet. Eine weitere Euro-Ausgabe mit zwei Stücken gab es ein Jahr später zum 10. Jahrestag des Freundschaftsvertrages zwischen Deutschland und Frankreich.

Das internationale Währungskürzel lautet „EUR“. In der ISO-Norm weicht es in mehrfacher Hinsicht von der allgemeinen Systematik ab:


Das Amt für Veröffentlichungen der Europäischen Union verwendet in seinen Schreibregeln das Eurozeichen nur zur grafischen Darstellung, populärwissenschaftlichen Veröffentlichungen und für Werbezwecke. In amtlichen Texten wird für Währungsbeträge hingegen grundsätzlich der ISO-Code „EUR“ genutzt.

Für den Cent gibt es offiziell weder ein Zeichen noch eine Abkürzung. In amtlichen Texten werden Beträge im Cent-Bereich daher in Eurobruchteilen angegeben, also zum Beispiel für einen Betrag von 20 Cent „0,20 EUR“. Inoffiziell wird die Untereinheit allerdings häufig abgekürzt "(Ct, Ct., ct, C" oder "c)." Das für den US-Cent verwendete Zeichen "¢" ist für den Eurocent ungebräuchlich.

Ende 2010 waren 862,3 Milliarden Euro als Bargeld in Umlauf, davon 840 Milliarden Euro als Scheine (97,4 %) und 22,3 Milliarden Euro als Münzen (2,6 %).

Es gibt Euromünzen zu 1, 2, 5, 10, 20 und 50 Eurocent sowie zu 1 und 2 Euro. Die Vorderseiten der Münzen aller Euroländer sind gleich, auf der Rückseite haben sie nationale Motivprägungen. Dennoch kann im gesamten Währungsraum damit bezahlt werden. Seit 2007 werden die Vorderseiten der Münzen schrittweise erneuert, um die im Jahre 2004 hinzugekommenen EU-Länder ebenfalls darzustellen. Die deutschen Rückseiten besitzen zusätzlich noch ein Münzzeichen, das den Prägeort angibt. Auf den griechischen Münzen ist der Nennwert auch auf Griechisch aufgeführt, statt Cent steht die Bezeichnung "Lepto/Lepta."
Auf der Vorderseite der Münzen befindet sich ein versetztes Doppel-L; die Initialen des belgischen Designers Luc Luycx.

Die Münzen zu 1 und 2 Euro bestehen aus zwei unterschiedlichen Legierungen (Kupfernickel und Messing). Unter Gebrauchsbedingungen entsteht ein elektrochemisches Spannungsgefälle, das Nickel-Ionen aus der Legierung herauslöst. Dies löst jedoch (entgegen ursprünglichen Befürchtungen) keine allergischen Reaktionen aus.

Da die thailändischen 10-Baht-Münzen den 2-Euro-Münzen in Größe und Gewicht stark ähneln und ebenfalls aus zwei unterschiedlichen Legierungen bestehen, erkennen Automaten im Euroraum, die über eine unzureichende Münzprüfung verfügen, diese Münzen möglicherweise als 2-Euro-Münze. Das kann unter Umständen auch mit anderen Münzen – zum Beispiel der neuen türkischen 1-Lira-Münze, der kenianischen 5-Schilling-Münze oder mit Restbeständen der italienischen 500-Lira-Münze – geschehen.

Seit 2004 werden 2-Euro-Gedenkmünzen für den Umlauf ausgegeben. Sie unterschieden sich nur durch das Motiv auf der nationalen Seite von den Umlaufmünzen und sind im gesamten Euroraum gültig.

Die erste Ausgabe wurde zum Gedenken an die Olympischen Sommerspiele 2004 in Athen von Griechenland ausgegeben. 2005 gab Österreich eine Münze zum fünfzigjährigen Jubiläum des Staatsvertrages heraus. Deutschland startete mit seiner ersten Gedenkmünze der Bundesländerserie 2006, auf der das Holstentor zu Lübeck abgebildet ist. Die Auflage betrug 31,5 Millionen. Dem jährlich wechselnden Vorsitz im Bundesrat gemäß wurden in den Folgejahren und werden bis einschließlich 2022 – mit Ausnahme des Jahres 2019 – jeweils einem der 16 Bundesländer gewidmete Gedenkmünzen mit Auflagen von jeweils rund 31 Millionen ausgegeben. Es war deshalb vorgesehen, dass Deutschland für den Umlauf 16 Jahre lang keine 2-Euro-Münzen mit dem Motiv des Bundesadlers (also die „gewöhnliche“ 2-Euro-Münze) – mit Ausnahme einer geringen Auflage Kursmünzensätze für Sammler – prägt. Dennoch wurden aber immer wieder 2-Euro-Münzen mit dem Bundesadler in erheblicher Stückzahl für Umlaufzwecke geprägt.

Zum fünfzigsten Jahrestag der Unterzeichnung der Römischen Verträge, dem 25. März 2007, gaben alle 13 Euroländer eine Gedenkmünze mit gemeinsamem Bild und Schriftzügen in der jeweiligen Landessprache bzw. in Latein aus. Am 1. Januar 2009 erschien erneut eine Gemeinschaftsausgabe der mittlerweile 16 Euroländer anlässlich des zehnten Jubiläums der Wirtschafts- und Währungsunion. Das Ausgabedatum ist symbolisch zu betrachten, da Neujahr ein offizieller Feiertag ist. Die deutsche Ausgabe erschien am 5. Januar und die italienische Münze als letzte der Serie am 26. März. Anfang 2012 folgte die dritte Gemeinschaftsausgabe von nunmehr 17 Ländern anlässlich des zehnjährigen Jubiläums der Einführung des Euros als Bargeld. Anlässlich des dreißigjährigen Bestehens der EU-Flagge gaben alle 19 EU-Länder, die den Euro als offizielles Zahlungsmittel nutzten, im Jahr 2015 eine gemeinschaftliche 2-Euro-Gedenkmünze aus.

Die Euroländer verausgaben neben den normalen Kursmünzen und den 2-Euro-Gedenkmünzen auch reine Sammlermünzen. Teilweise belaufen sich die Nennwerte auf bis zu mehreren hundert Euro, und die Münzen enthalten Silber oder Gold. Derartige Sammlermünzen werden nur in den jeweiligen Ausgabestaaten als gültiges Zahlungsmittel anerkannt, das heißt, sie gelten nicht in der gesamten Eurozone. Die Prägeauflage ist meistens limitiert. Die Nennwerte sind beliebig, nur die Nennwerte der normalen Euro-Kursmünzen dürfen nicht für Sammlermünzen verwendet werden. Den bislang höchsten Nennwert hat mit 100.000 Euro eine Sonderausgabe des Wiener Philharmonikers.

Am 14. April 2016 wurde in Deutschland, vorerst in kleiner Stückzahl und nur in den Filialen der Deutschen Bundesbank erhältlich, eine 5-Euro-Münze als Sammlermünze mit blauem Ring herausgegeben. Sie sorgt auch deshalb für großes öffentliches Interesse, da die Fälschungssicherheit durch die neue Produktionstechnik verbessert werden soll und die Akzeptanz dieser neuen Stückelung mit Spannung erwartet wird.

Euro-Banknoten gibt es in Stückelungen zu 5 €, 10 €, 20 €, 50 €, 100 €, 200 € und 500 €.

Die Euro-Banknoten der ersten Serie wurden nach einem EU-weiten Wettbewerb von dem Österreicher Robert Kalina gestaltet und sind in allen Euroländern identisch. Die Scheine zeigen verschiedene Motive zu den Themen "Zeitalter und Baustile in Europa." Die Vorderseiten zeigen als Motiv ein Fenster oder eine Fensterfront, die Rückseiten jeweils eine Brücke. Dabei sind keine realen Bauwerke abgebildet, sondern es wurden die Stilmerkmale der einzelnen Epochen in eine typische Abbildung eingebracht: Antike auf dem 5-Euro-Schein, Romanik auf dem 10-Euro-Schein, Gotik auf dem 20-Euro-Schein, Renaissance auf dem 50-Euro-Schein, Barock und Rokoko auf dem 100-Euro-Schein, Eisen- und Glasarchitektur auf dem 200-Euro-Schein und moderne Architektur des 20. Jahrhunderts auf dem 500-Euro-Schein.

2005 begann die Entwicklung der von Reinhold Gerstetter gestalteten zweiten Generation von Euro-Banknoten, die ab 2013 sukzessive eingeführt werden.

Bis Ende 2002 war anhand des Anfangsbuchstabens der Seriennummer auf der Rückseite eines Euroscheines zu ersehen, im Auftrag welcher nationalen Zentralbank er gedruckt wurde. Deutschland hatte in diesem System den Buchstaben X zugewiesen bekommen. Seit 2003 wird im sogenannten „Pooling-System“ jeder Wert nur noch von wenigen Nationalbanken produziert und von den Druckereien ins gesamte Eurogebiet transportiert. Jede Nationalbank spezialisiert sich auf höchstens vier Wertstufen.

Heute lässt sich die Herkunft nur noch mit Hilfe des Druckereicodes feststellen, der sich bei jedem Schein auf der Vorderseite befindet, bei Banknoten der zweiten Serie rechts am oberen Bildrand. Bei Noten der ersten Serie variiert die genaue Position je nach Wert des Scheines, beispielsweise befindet sie sich beim 10-Euro-Schein im Stern an der 8-Uhr-Position. Der erste Buchstabe gibt die Druckerei an, in der er gedruckt wurde. Der Buchstabe R steht zum Beispiel für die Bundesdruckerei in Berlin. Der Druckereicode besteht aus einem Buchstaben, drei Ziffern, einem Buchstaben und einer Ziffer. Siehe mehr dazu im Artikel Eurobanknoten.

Österreich forderte die Einführung eines 2-Euro-Scheins, Italien sogar die eines 1-Euro-Scheins. In beiden Staaten waren vor der Einführung des Euros Geldscheine mit relativ geringen Werten im Umlauf – so zum Beispiel der 20-Schilling-Schein (1,45 Euro) in Österreich oder der 1000-Lire-Schein (52 Cent) in Italien.

Am 18. November 2004 beschloss der EZB-Rat, keine Euroscheine mit niedrigerem Wert einzuführen.
Sie hätten einen ähnlichen Wert wie die seinerzeit selten verwendete 5-DM-Note (2,56 Euro).

In einigen Euroländern sind 1- und 2-Cent-Münzen für den Barzahlungsverkehr nicht gebräuchlich und werden nur in kleinen Stückzahlen für Münzsammler geprägt. In Finnland wurden sie als Zahlungsmittel gar nicht eingeführt; dort werden seither Rechnungen, die nicht auf –,–0 oder –,–5 Euro enden, beim Bezahlen auf diese Beträge gerundet. Zwar kann man auch mit 1- oder 2-Cent-Münzen zahlen; sie werden jedoch nicht als Wechselgeld herausgegeben. Schon vor der Euro-Einführung war die kleinste Nominale der finnischen Mark nicht das 1-Penni-Stück, sondern das 10-Penniä-Stück gewesen und Beträge wurden entsprechend gerundet. In den Niederlanden (seit 1. September 2004; entsprechend auch schon mit dem Gulden nach der Abschaffung der 1-Cent-Münze) wurde dieses System später übernommen – begründet mit dem geringen Geldumlauf solcher Münzen.

Die Gegner der Abschaffung befürchten vor allem einen zweiten „Teuro-Effekt“, weil viele Einzelpreise auf volle fünf Cent aufgerundet werden könnten. Dagegen wird aber eingewendet, dass dies wegen der psychologisch wichtigen Schwellenpreise, die dann eher von –,99 auf –,95 herabgesetzt werden, nicht passieren würde. Zudem existieren in niederländischen und finnischen Geschäften immer noch warenbezogene Schwellenpreise, die oft auf –,99 enden. Erst die Summe an der Kasse wird auf- oder abgerundet.

Die EU-Kommission machte am 14. Mai 2013 Vorschläge für eine Vergünstigung oder eine Abschaffung der 1- und 2-Cent-Münzen. Währungskommissar Olli Rehn stellte fest, die Herstellung und Herausgabe dieser Münzen übersteige ihren Wert. Zugleich müssten die Zentralbanken ausgerechnet von diesen Münzen besonders viele Exemplare herausgeben. Insgesamt seien in den letzten elf Jahren 45,8 Milliarden solcher Kleinstmünzen in Umlauf gebracht worden. Die Ausgabe der Kleinstmünzen habe die Euro-Staaten seit dem Start der Gemeinschaftswährung im Jahr 2002 zusammen etwa 1,4 Milliarden Euro gekostet. Die Kosten für die Cent-Münzen könnten etwa durch eine andere Materialmischung oder ein effizienteres Prägungsverfahren reduziert werden.

Die Fälschungssicherheit der Eurobanknoten wird im internationalen Vergleich hoch angesehen. Um sie zu gewährleisten, sind die Scheine mit mehreren Sicherheitsmerkmalen ausgestattet. Bei der Produktion werden in das Banknotenpapier fluoreszierende Fasern und ein mittig verlaufender Sicherheitsfaden eingebracht, der in Gegenlicht dunkel erscheint und die Wertangabe als Mikrodruck trägt. Außerdem bestehen die Scheine aus Baumwollfasern, die ihnen eine charakteristische Struktur verleihen. Weiterhin werden Teile des Motivs mit fluoreszierender Farbe hergestellt, sodass unter UV-Licht die Fasern und das Motiv leuchten. Bei Nutzung von infrarotem Licht reflektieren die Scheine in unterschiedlichen Farben. Ein Wasserzeichen in den Noten lässt im Gegenlicht das jeweilige Architekturmotiv und die Wertzahl erkennen.

Das Durchsichtsregister in der linken oberen Ecke der Banknotenvorderseite lässt ebenfalls im Gegenlicht zusammen mit dem Rückseitenmotiv die Wertzahl erscheinen. Dies passiert dadurch, dass auf Vorder- und Rückseite jeweils nur Teile der Wertzahl gedruckt sind, die sich erst bei der Durchsicht zusammenfügen. Am Rand der 5-, 10- und 20-Euro-Banknoten ist ein durchlaufender metallisierter Folienstreifen aufgebracht, der je nach Beleuchtungswinkel entweder das Euro-Symbol oder den jeweiligen Wert des Scheines als Kinegramm erscheinen lässt. Die höherwertigen Euroscheine ab 50 Euro besitzen an dieser Stelle ein positioniertes Folienelement, das beim Kippen der Banknote in Form eines Hologramms – je nach Betrachtungswinkel – das jeweilige Architekturmotiv beziehungsweise die Wertzahl zeigt.

Durch das Druckverfahren der Banknoten, ein Stichtiefdruckverfahren kombiniert mit – als Irisdruck ausgeführtem – indirektem Hochdruck, entsteht auf der Geldscheinvorderseite ein ertastbares Relief, das die Fälschung der Banknoten erschwert und zugleich Sehbehinderten die Unterscheidung der Banknoten vereinfacht. Außerdem sind die Abbildungen der Fenster und Tore und die Abkürzungen der Europäischen Zentralbank (BCE, ECB, EZB, griech. ΕΚΤ (lat. EKT), EKP) ertastbar.

Die Scheine niedrigen Wertes haben auf der Rückseite einen goldtransparenten Perlglanzstreifen, während bei den Werten ab 50 Euro die Farbe der Wertziffer beim Kippen variiert (OVI = ). Zudem besitzen die Euroscheine maschinenlesbare Kennzeichen, die eine automatische Überprüfung der Echtheit gewährleisten. Eine Besonderheit ist das sogenannte „Counterfeit Deterrence System“ (CDS), das das Reproduzieren auf Kopiergeräten oder per PC verhindern soll. Die Deutsche Bundesbank empfiehlt generell, sich niemals nur auf ein einziges Sicherheitsmerkmal zu konzentrieren, und weist gleichzeitig darauf hin, dass es weitere Sicherheitsmerkmale gebe, die aber nicht veröffentlicht werden.

Mario Draghi (Präsident der Europäischen Zentralbank) stellte am 10. Januar 2013 in Frankfurt als ersten Schein einer neuen, "„Europa-Serie“" genannten Banknotenserie eine neue 5-Euro-Note vor, die ab dem 2. Mai 2013 in Umlauf gebracht wurde. Sie weist zusätzliche Sicherheitsmerkmale auf, z. B. ein Wasserzeichen mit der Abbildung der mythologischen Gestalt Europa, einen Sicherheitsfaden, eine Ziffer „5“, die beim Kippen von Smaragdgrün nach Tiefblau changiert, einen glänzenden Hologrammstreifen sowie tastbare Linien an den Rändern. Zwecks längerer Haltbarkeit ist die neue Banknote mit einem Schutzlack versehen und fühlt sich deshalb wächsern-glatt an. Die parallel zirkulierenden alten Banknoten werden nach und nach aus dem Verkehr gezogen und verlieren „letztlich den Status als gesetzliches Zahlungsmittel […] behalten jedoch auf Dauer ihren Wert“.
Der Fünfer, die am intensivsten zirkulierende Euro-Banknote, hat in der alten Version eine Haltbarkeit von nur knapp einem Jahr.

Neu ist, dass in der Europa-Serie, die 2014 mit einem neuen 10-Euro-Schein und 2015 mit einem neuen 20-Euro-Schein fortgesetzt wurde, die Währungsbezeichnung nicht nur in lateinischer und griechischer Schreibweise ("EURO" bzw. "EYPΩ") erfolgt, sondern auch in kyrillischer Schrift (ЕВРО), und neun statt bisher fünf Akronyme für die Europäische Zentralbank erscheinen. Als einziges EU-Mitglied verwendet Bulgarien das kyrillische Alphabet.

Euromünzen sind wegen des niedrigeren Wertes nicht so stark von Fälschungen betroffen wie die Geldscheine, trotzdem müssen auch sie vor Fälschern geschützt sein. Sie verfügen über eine bestimmte Größe und eine genau definierte Masse. Die Ein- und Zwei-Euro-Münzen sind durch eine Kombination zweier Metalle bicolor gestaltet. Dies und ein komplexes, dreischichtiges Herstellungsverfahren gewährleisten die Fälschungssicherheit der Münzen.
Der Mittelteil echter Ein- und Zwei-Euro-Münzen ist leicht ferromagnetisch, die Ein-, Zwei- und Fünf-Cent-Stücke sind hingegen stark ferromagnetisch. Der Außenring der Ein- und Zwei-Euro-Münzen ist dagegen nicht ferromagnetisch, genau wie die übrigen drei Euromünzen. Da falsche Centmünzen oftmals aus anderen Metallen hergestellt sind als die echten, lassen sie auch oft einen falschen Klang beim Fall auf eine Tischplatte entstehen. Auch hinterlassen sie oft eine bleistiftähnliche Spur, wenn man sie über ein Blatt Papier streicht.

In Deutschland wurden im ersten Halbjahr 2010 rund 33.700 falsche Eurobanknoten eingezogen, die Schadensumme betrug 1,9 Millionen Euro. Dies bedeutete einen Anstieg gegenüber den vorigen Jahren. Mit acht Fälschungen auf 10.000 Einwohner lag Deutschland aber weiter unter dem EU-Durchschnitt. Bei über 60 % der gefälschten Noten handelte es sich um 50-Euro-Scheine. An falschen Euromünzen wurden rund 33.600 Stück eingezogen, davon über 80 % Zwei-Euro-Münzen.
Europaweit betrafen im gleichen Zeitraum je gut 40 % der Fälschungen 20- und 50-Euro-Scheine.

2011 wurden von der Deutschen Bundesbank 39.000, 2012 41.500 falsche Banknoten registriert, die Schäden von 2,1 Millionen bzw. 2,2 Millionen Euro verursachten. Mit 46 % aller „Blüten“ rangierte der 20-Euro-Schein vor dem 50er mit 34 % Anteil. 5er- und 500er-Noten machen nur jeweils 1 % des registrierten Falschgelds aus.

Europaweit wurden im ersten Halbjahr 2012 251.000 gefälschte Euro-Banknoten aus dem Verkehr gezogen. In Relation zu 14,6 Milliarden in Umlauf befindlichen echten Banknoten gilt der Anteil an gefälschten Scheinen als sehr gering. Die meisten Fälschungen wurden 2009/10 gezählt; seither (Stand: 2012) nimmt deren Zahl ab.

Dem Umlauf entzogene 1- und 2-Euro-Münzen wurden bis 2007 in Deutschland nicht (durch Verbiegen oder Plattwalzen ihrer Oberflächen) verunstaltet, sondern entkernt, also in Ring und Kern getrennt und nach Materialsorte sortiert. Solch sortierter Schrott wurde u. a. nach China verkauft. Betrügerisch sollen diese Münzenteile in großem Umfang wieder maschinell zusammengesetzt worden sein. Diese rückgebauten Münzen wurden z. B. durch Flugbegleiterinnen nach Deutschland eingeführt, als beschädigte Münzen der Bundesbank zur Rücknahme angeboten und von dieser angenommen. Betroffen waren 29 Tonnen bei 263 Transaktionen in drei Jahren, im (Schadens-)Wert von 6 Millionen Euro. Das ist wenig im Vergleich zu fast 70.000 t Münzeinzahlungen bei der Bundesbank pro Jahr und fiel dadurch nicht auf. Nach einjähriger Ermittlung wurde der Fall im April 2011 als gerichtsanhängig publik. Die Münzüberbringer gaben vor, „die Münzen seien in China beim Verarbeiten von Müll, Schrottautos und Altkleidern angefallen“. Ein Teil der Münzen war in die Teile zerfallen, bei einem Teil passten Ring und Kern herkunftsmäßig nicht zusammen, manche Spalten waren optisch durchscheinend oder wiesen Klebstoff auf. Seit 11. Januar 2011 gilt nunmehr eine neue EU-Verordnung, wonach nur noch "durch den normalen Gebrauch beschädigte Münzen" umgetauscht werden. Alle anderen werden ersatzlos eingezogen.

Die nationalen Behörden aller Euroländer stellten 2013 insgesamt 175.900 falsche Euromünzen sicher. Somit kam auf 100.000 echte Münzen eine Fälschung. Zwei von drei sichergestellten Falschmünzen waren 2-Euro-Stücke. Um den Fälschungsschutz des Euros zu verbessern, trat in Deutschland zum 1. Januar 2013 die "Bargeldprüfungsverordnung" in Kraft, die – nach einer Übergangsfrist von zwei Jahren – ab 1. Januar 2015 vorschreibt, wie Geldinstitute sicherzustellen haben, dass alle von ihnen wieder in Umlauf gebrachten Euro-Münzen echt sind.

Im Jahre 2002 wurde der Euro mit dem Internationalen Karlspreis zu Aachen ausgezeichnet, da er „wie kein anderer Integrationsschritt zuvor die Identifikation mit Europa befördert und damit einen entscheidenden, epochemachenden Beitrag zum Zusammenwachsen der Völkerfamilie leistet“.





</doc>
<doc id="1267" url="https://de.wikipedia.org/wiki?curid=1267" title="Ernest Rutherford">
Ernest Rutherford

Ernest Rutherford, 1. Baron Rutherford of Nelson (* 30. August 1871 in Spring Grove bei Nelson; † 19. Oktober 1937 in Cambridge, Vereinigtes Königreich) war ein neuseeländischer Physiker, der 1908 den Nobelpreis für Chemie erhielt. Rutherford gilt als einer der bedeutendsten Experimentalphysiker.

1897 erkannte Rutherford, dass die ionisierende Strahlung des Urans aus mehreren Teilchenarten besteht. 1902 stellte er die Hypothese auf, dass chemische Elemente durch radioaktiven Zerfall in Elemente mit niedrigerer Ordnungszahl übergehen. Er teilte 1903 die Radioaktivität in Alphastrahlung, Betastrahlung sowie Gammastrahlung nach der positiven, negativen oder neutralen Ablenkung der Strahlenteilchen in einem Magnetfeld auf und führte den Begriff der Halbwertszeit ein. Diese Arbeit wurde 1908 mit dem Nobelpreis für Chemie ausgezeichnet.

Sein bekanntester Beitrag zur Atomphysik ist das Rutherfordsche Atommodell, das er 1911 aus seinen Streuversuchen von Alphateilchen an Goldfolie ableitete. Rutherford widerlegte das Atommodell von Thomson, der noch von einer gleichmäßigen Masseverteilung ausgegangen war.

Rutherford wies erstmals 1917 experimentell nach, dass durch Bestrahlung mit Alphateilchen ein Atomkern (in seinem Falle Stickstoff) in einen anderen (in seinem Falle in das nächstschwerere Element Sauerstoff) umgewandelt werden kann. Bei diesen Experimenten entdeckte er das Proton. Unter seiner Anleitung „zertrümmerten“ John Cockcroft und Ernest Walton mit künstlich beschleunigten Teilchen einen Atomkern; mit Protonen beschossenes Lithium spaltete sich auf in zwei Alphateilchen, also Helium-Kerne. Einem weiteren Wissenschaftler in Cambridge, James Chadwick, gelang es 1932, das Neutron experimentell nachzuweisen, welches Rutherford bereits Jahre vorher theoretisch postuliert hatte.

Ernest Rutherford war das vierte von zwölf Kindern von James Rutherford (1838–1928) und dessen Frau Martha Thompson (um 1843–1935). Seine Eltern waren im Kindesalter nach Neuseeland immigriert. Von Spring Grove zog die Familie 1876 nach Foxhill. Dort besuchte Rutherford ab März 1877 die von Henry Ladley geleitete Primary School. 1883 zog die Familie weiter nach Havelock, wo der Vater am Ruakaka-Fluss eine von ihm errichtete Flachsmühle betrieb. Aus wirtschaftlichen Gründen musste die Familie fünf Jahre später noch einmal umziehen, diesmal nach Pungarehu auf der neuseeländischen Nordinsel. Unterstützt durch ein Stipendium des Marlborough Education Boards besuchte Rutherford von 1887 bis 1889 das Nelson College. Dort spielte er unter anderem in der Rugby-Mannschaft und war 1889 Schulsprecher. Sein Interesse für Mathematik und Naturwissenschaften wurde durch seinen Lehrer William Still Littlejohn (1859–1933) gefördert.

Ab Februar 1890 studierte Rutherford am Canterbury College in Christchurch. Dort förderte der Professor für Mathematik und Naturphilosophie Charles Henry Herbert Cook (1843–1910) Rutherfords mathematische Begabung, während der Professor für Chemie Alexander William Bickerton (1842–1929), der ebenfalls Physik unterrichtete, Rutherfords Interesse für die Physik weckte. 1892 bestand Rutherford die Prüfungen für den Bachelor of Arts, 1893 erwarb er den Grad eines Master of Arts und ein Jahr später den Abschluss als Bachelor of Science. Rutherfords erste Forschungsarbeiten beschäftigten sich mit dem Einfluss von hochfrequenten Hertzschen Wellen auf die magnetischen Eigenschaften von Eisen und wurden in den "Transactions of the New Zealand Institute" veröffentlicht.

Während dieser Zeit wohnte Rutherford im Haus der verwitweten Mary Kate De Renzy Newton, einer Sekretärin der Woman’s Christian Temperance Union. Dort lernte er ihre Tochter und seine spätere Frau Mary „May“ Georgina Newton (1876–1945) kennen.

Rutherford bewarb sich 1894 um den neuseeländischen Platz für ein „1851 Exhibition Scholarship“, einem aus den Überschüssen der Great Exhibition von 1851 in London finanzierten Stipendium. Er unterlag mit seiner Bewerbung dem Chemiker James Scott Maclaurin (1864–1939) vom Auckland University College. Als Maclaurin das mit 150 Pfund Sterling dotierte und für einen Studienaufenthalt in Großbritannien gedachte Stipendium nicht annahm, wurde es Rutherford als zweitem Bewerber zugesprochen.

Am 1. August 1895 verließ Rutherford von Wellington aus mit einem Dampfschiff Neuseeland. Bei einem Zwischenaufenthalt führte er William Henry Bragg an der University of Adelaide seinen Detektor für Hertzsche Wellen vor und erhielt von Bragg ein Empfehlungsschreiben. Im Oktober 1895 begann Rutherford seine Tätigkeit am von Joseph John Thomson geleiteten Cavendish-Laboratorium der University of Cambridge. Zunächst beschäftigte er sich mit der Verbesserung der Empfindlichkeit seines Detektors, mit dem er bald Radiowellen in einer Entfernung von etwa einer halben Meile nachweisen konnte. Thomson, der Rutherfords experimentelles Talent schnell erkannte, lud Rutherford zu Beginn des Oster-Semesters 1896 ein, ihn bei seinen Untersuchungen der elektrischen Leitfähigkeit von Gasen zu unterstützen. Sie benutzten die wenige Monate zuvor entdeckten Röntgenstrahlen, um die Leitfähigkeit in den Gasen auszulösen. Rutherford entwickelte die experimentellen Techniken, um die Rekombinationsrate und die Geschwindigkeiten der unter der Einwirkung der Röntgenstrahlen entstehenden Ionen zu messen. In der Folgezeit setzte Rutherford diese Experimente unter Verwendung von Ultraviolettstrahlung fort.

Nach zwei Jahren in Cambridge erhielt Rutherford 1897 den „B. A. Research Degree“. Durch Thomsons Fürsprache wurde ihm 1898 das auf 250 Pfund pro Jahr dotierte Coutts-Trotter-Fellowship des Trinity College zugesprochen, das es Rutherford ermöglichte, ein weiteres Jahr in Cambridge zu verbringen.

1898 erhielt er einen Ruf an die McGill-Universität in Montreal (Kanada), wo er bis 1907 arbeitete. Für die Forschungen, die er in dieser Zeit leistete, erhielt er im Jahre 1908 den Nobelpreis für Chemie.

Danach begann er an der Universität Manchester in England zu lehren, wo er unter anderem mit späteren Nobelpreisträgern wie Niels Bohr und Patrick Blackett arbeitete.

Im Ersten Weltkrieg reiste er 1917 zusammen mit Henri Abraham und Charles Fabry in die USA, um die Frage der U-Boot-Abwehr zu diskutieren.

1919 ging er als Professor nach Cambridge, wo er Direktor des Cavendish-Laboratoriums war. 1921 erschien seine Schrift "Nuclear Constitution of Atoms" (dt.: "Über die Kernstruktur der Atome)". Von 1925 bis 1930 war er Präsident der Royal Society.

1933 unterstützte er William Henry Beveridge bei der Gründung des "Academic Assistance Council" (AAC, heute Council for Assisting Refugee Academics), dessen erster Präsident er wurde.

Rutherford wurde in der Westminster Abbey in London nahe dem Grab von Isaac Newton beigesetzt.

Rutherford zählt zu den weltweit am meisten geehrten Wissenschaftlern. Die britische Krone adelte ihn 1914 als Knight Bachelor, nahm ihn 1925 in den Order of Merit auf und erhob ihn 1931 als "Baron Rutherford of Nelson", of Cambridge in the County of Cambridge, zum erblichen Peer.

Ein 1906 neu entdecktes und von Willy Marckwald beschriebenes Uranylcarbonat-Mineral erhielt ihm zu Ehren den Namen Rutherfordin. Neben dem ihm 1908 verliehenen Nobelpreis für Chemie wurden Rutherford zahlreiche wissenschaftliche und akademische Preise und Ehrungen zuteil. Die Royal Society verlieh ihm 1904 die Rumford-Medaille und ehrte Rutherford 1922 mit ihrer höchsten Auszeichnung, der Copley-Medaille in Gold. Die Accademia delle Scienze di Torino würdigte ihn 1908 mit der Vergabe des Bressa-Preises (Premio Bressa). Die Columbia University verlieh Rutherford die alle fünf Jahre vergebene Barnard-Medaille für das Jahr 1910.

1911 wurde er in die National Academy of Sciences und 1915 in die American Academy of Arts and Sciences gewählt. Er erhielt 1924 die Franklin-Medaille des Franklin Institutes in Philadelphia, 1928 die Albert Medal der Royal Society of Arts und 1930 die Faraday-Medaille der Institution of Electrical Engineers.

1932 wurde ihm die Ehrenmitgliedschaft der Deutschen Akademie der Naturforscher Leopoldina verliehen.

Der Rat der Physical Society of London begründete 1939 die Rutherford Memorial Lecture (Rutherford-Gedenkvorlesung) aus der 1965 der Preis Rutherford Medal and Prize hervorging.

Mitte 1946 schlugen Edward Condon und Leon Francis Curtiss (1895–1983) vom US-amerikanischen National Bureau of Standards vor, eine neue physikalische Einheit Rutherford mit dem Einheitenzeichen "rd" zur Messung von Aktivität einzuführen, die sich jedoch nicht durchsetzte.

Am 3. November 1992 brachte die Bank of New Zealand einen 100-Dollar-Schein mit dem Bildnis Rutherfords in Umlauf. Zu seinen Ehren wurde 1997 das Element 104 endgültig als Rutherfordium benannt.

Bei der 2005 ausgestrahlten Sendereihe New Zealand’s Top 100 History Makers wurde Rutherford zum einflussreichsten Neuseeländer der Geschichte gewählt. In seinem Geburtsort entstand eine Gedenkstätte und seine ehemalige Grundschule in Foxhill pflegt mit der „Rutherford Memorial Hall“ sein Andenken.

Seit 2008 trägt der Rutherford Ridge in der Antarktis seinen Namen.

Englische Originalausgaben

Deutsche Übersetzungen





</doc>
<doc id="1268" url="https://de.wikipedia.org/wiki?curid=1268" title="Erich Honecker">
Erich Honecker

Erich Ernst Paul Honecker (* 25. August 1912 in Neunkirchen (Saar); † 29. Mai 1994 in Santiago de Chile) war ein deutscher kommunistischer Politiker. Vom 3. Mai 1971 bis zum 18. Oktober 1989 war er als Erster Sekretär bzw. Generalsekretär des Zentralkomitees der Sozialistischen Einheitspartei Deutschlands (SED) der mächtigste Politiker der Deutschen Demokratischen Republik (DDR).

Als ein hauptamtlicher Funktionär der Kommunistischen Partei Deutschlands (KPD) war Honecker von 1935 bis 1945 wegen Widerstand gegen den Nationalsozialismus inhaftiert. Nach der Befreiung vom Nationalsozialismus gründete Honecker im Auftrag der KPD 1946 in Ost-Berlin die Jugendorganisation FDJ, deren Vorsitzender er bis 1955 blieb. Er war als "Sicherheitssekretär des ZK der SED" maßgeblicher Organisator des Baus der Berliner Mauer und trug schon in dieser Funktion den Schießbefehl an der innerdeutschen Grenze mit. Als langjähriger Generalsekretär des ZKs der SED, Vorsitzender des Staatsrats der DDR sowie Vorsitzender des Nationalen Verteidigungsrates führte und repräsentierte er die DDR in den 1970er und 1980er Jahren. Als einer seiner größten Erfolge gilt die Anerkennung der DDR als Vollmitglied der UNO 1973.

Ende der 1980er Jahre wurden die wirtschaftliche Lage, die Beziehungen zur Führungsmacht Sowjetunion und die innenpolitische Lage der DDR zunehmend schwieriger. Bei seinem offiziellen Besuch in der Bundesrepublik Deutschland wurde Honecker im September 1987 von Bundeskanzler Helmut Kohl empfangen.

Honecker wurde vom SED-Politbüro am 18. Oktober 1989 zum Rücktritt gezwungen. 1992 wurde Honecker in Berlin wegen seiner Verantwortung für Menschenrechtsverletzungen des DDR-Regimes vor Gericht gestellt, das Verfahren aber aufgrund seiner Krankheit eingestellt. Die Anklage war wegen seiner Rolle als Staatschef der untergegangenen DDR sowie der damit zusammenhängenden schwierigen Rechtslage umstritten. Honecker reiste umgehend zu seiner Familie nach Chile, wo er im Mai 1994 starb.

Sein Vater Wilhelm Honecker (1881–1969) war Bergarbeiter und heiratete 1905 Caroline Catharina Weidenhof (1883–1963). Zusammen hatten sie sechs Kinder: Katharina (Käthe, 1906–1925), Wilhelm (Willi, 1907–1944), Frieda (1909–1974), Erich, Gertrud Hoppstädter (1917–2010) geborene Honecker und Karl-Robert (1923–1947).

Erich Honecker wurde in Neunkirchen (Saar) in der Max-Braun-Straße geboren; seine Familie zog wenig später in den heutigen Neunkircher Stadtteil Wiebelskirchen in die Kuchenbergstraße 88. Er besuchte die evangelische Grundschule. 1922 wurde er noch vor seinem zehnten Geburtstag in der fünfzig Mitglieder zählenden kommunistischen Kindergruppe von Wiebelskirchen untergebracht, die auch seine Geschwister Willi, Frieda und Gertrud besuchten und der später in "Jung-Spartakus-Bund" umbenannt wurde. Nach der dritten Klasse wechselte er in die evangelische Hauptschule, die er 1926 nach der achten Klasse verließ, womit automatisch seine Mitgliedschaft im "Jung-Spartakus-Bund" endete.

Als Bergmannbauernfamilie nahmen die in ihrem Revier des Saarlandes familiär eng vernetzten Honeckers, die als Hausbesitzer und Vermieter, mit Obst- und Gemüsegarten und einer Agrarparzelle zu den wohlhabenderen Bergleuten in Wiebelskirchen zählten, eine materiell vergleichsweise gut gesicherte Position ein, die sich, konträr zu den späteren Darstellungen Erich Honeckers, von der Not der im Deutschen Reich verelendeten Arbeitermassen stark unterschied: Sie konnten ihren kleinen Besitz von Generation zu Generation weitergeben, besaßen hinter dem Haus Stallungen für eine Kuh und hielten Ziegen, Kaninchen und zeitweise ein oder zwei Schweine. Den Steckrübenwinter 1916/17, der zu einer reichsweiten Hungersnot führte, überstand die Familie durch ihre bescheidene Landwirtschaft, die ihre Ernährungslage während der Kriegsjahre verbesserte, während der Vater Wilhelm Honecker als Matrose an der Front kaum eingesetzt wurde. Entgegen den Darstellungen Erich Honeckers war sein Vater nicht am Kieler Matrosenaufstand beteiligt. Er war in Wahrheit bereits Ende Juli 1917 als „Reklamierter“ nach Wiebelskirchen zurückgekehrt, nachdem die Oberste Heeresleitung den Abzug von 40.000 Bergarbeitern von der Front angeordnet hatte, weil deren ziviler Einsatz unter Tage wegen der zwischenzeitlich dramatischen Brennstoffknappheit wichtiger als ihr Dienst als Soldaten geworden war. Ebenso ist Wilhelm Honecker nicht bereits in Kiel, wie sein Sohn behauptete, der USPD beigetreten, sondern wahrscheinlich erst nach seiner Heimkehr ins Saarland, wo die USPD erst Anfang 1918 entstanden war.

Die im Saargebiet paritätisch von SPD- und USPD-Vertretern gebildeten Arbeiter- und Soldatenräte wurden bereits am 24. November von der ins Saargebiet einmarschierenden französischen Armee aufgelöst. Durch das im Versailler Vertrag integrierte Saarstatut wurde ein völkerrechtlich neues Gebilde geschaffen, das fünfzehn Jahre lang wirtschaftlich in das französische Zoll- und Währungsgebiet eingegliedert wurde, während das Saargebiet politisch von einer vom Völkerbund eingesetzten Regierungskommission beherrscht wurde. Die Familie Honecker behielt die deutsche Staatsbürgerschaft bei, stand aber dem katholischen Milieu fern, dem die Mehrheit der Saarbevölkerung angehörte, und wurde vom sich herausbildenden linksproletarischen Milieu angezogen.

Als Honecker nach der Schulzeit wegen der verschlechterten Wirtschaftslage keine Lehrstelle fand, drängten ihn seine Eltern zu Ostern 1926, eine anderweitige Beschäftigung auf dem ihm von der Kinderlandverschickung her bekannten Hof des Bauern Wilhelm Streich, im hinterpommerschen Neudorf, in der Nähe der Kreisstadt Bublitz, anzunehmen. Honeckers Memoiren zufolge habe er sich dort zwei Jahre lang nur für freies Essen und freie Kleidung aufgehalten, „um in der Landwirtschaft zu arbeiten“. Streich behandelte ihn jedoch fast als seinen künftigen Schwiegersohn, machte ihn zum Jungbauern, überantwortete Honecker infolge einer Kriegsverletzung schließlich die gesamte Feldbestellung und entlohnte ihn mit 20 Reichsmark monatlich. Im Frühjahr 1928 verzichtete Honecker auf die materiellen Verlockungen der in Aussicht gestellten Hofübernahme. Seine Gastfamilie kleidete ihn daraufhin neu ein, stattete ihn mit Geld aus und er kehrte nach Wiebelskirchen zurück. Da er als Landwirtschaftsgehilfe keine Anstellung fand, ließ er sich im Dachdeckergeschäft seines Onkels Ludwig Weidenhof, das dieser im Erdgeschoss seines Elternhauses betrieb, als Dachdeckergehilfe anlernen. Im Anschluss nahm er eine Lehre als Dachdecker beim Wiebelskirchener Dachdeckermeister Müller an.

Am 1. Dezember 1928 trat er dem Kommunistischen Jugendverband Deutschland (KJVD) – Bezirk Saar bei. Der KJVD zählte zu dieser Zeit nur noch 200 Mitglieder in elf Ortsgruppen. In seiner späteren DDR-Kaderakte datierte er das KJVD-Eintrittsdatum auf 1926 zurück, um seine zweijährige Tätigkeit als Jungbauer in Hinterpommern in seiner politischen Kampfbiographie zu vertuschen. Er galt in den konkurrierenden Jugendverbänden der Sozialdemokratie und des Zentrums als „der Wortführer der Kommunisten“. 1929 wurde er in die Bezirksleitung des KJVD-Saar gewählt. Parallel absolvierte er diverse innerparteiliche Schulungen, um sich auf die Übernahme leitender Funktionen im KPD-Jugendverband vorzubereiten. Im Dezember 1929 beteiligte er sich in Dudweiler an einem zweiwöchigen Lehrgang der KJVD-Bezirksschule über marxistische Theorie und praktische Jugendarbeit. In seiner Freizeit widmete sich Honecker seinen Mitgliedschaften im örtlichen Spielmannszug und in der Jugendorganisation des Roten Frontkämpferbundes "Roter Jungsturm", der später in "Rote Jungfront" umbenannt wurde. Im Kommunistischen Jugendverband war er zunächst Kassierer und später Leiter der Wiebelskirchener Ortsgruppe. Honecker schloss sich formell der KPD an, nachdem er bereits in verschiedenen Institutionen des kommunistischen Parteimilieus aktiv war. Das genaue Datum seines Parteieintritts konnte bis heute nicht ermittelt werden. Honecker selbst gab für seine Aufnahme in die KPD nach 1945 erst das Jahr 1930 und ein anderes Mal „Herbst 1931“ an. Schließlich verlegte er den Parteieintritt auf 1929, um 1979 von der SED für seine fünfzigjährige Parteimitgliedschaft geehrt werden zu können. Im Juli 1930 meldete sich Honecker mit 27 weiteren Auserwählten aus den verschiedenen KJVD-Bezirken beim Parteivorstand der KPD im Berliner Karl-Liebknecht-Haus, um an einem Vorbereitungslehrgang an der Reichsparteischule der KPD in Fichtenau (heute ein Ortsteil von Schöneiche bei Berlin) teilzunehmen. In einem symbolischen Aufnahmeakt als „Genosse“, der sich völlig der Herrschaft der kommunistischen Lebenswelt und deren Partei unterwirft, bekam Honecker seinen neuen Parteinamen "Fritz Molter" zugeteilt, den er auch während der sich anschließenden konspirativen Kaderschulung in Moskau führte.

Seine Dachdeckerlehre brach Honecker nach zwei Jahren ohne Gesellenprüfung ab, weil er vom KJVD im Sommer 1930 zu einem einjährigen Studium an die Internationale Lenin-Schule nach Moskau delegiert wurde, einer vom Exekutivkomitee der Kommunistischen Internationale (EKKI) errichteten stalinistischen Kaderschmiede, die ihn zu einem von zirka 370 deutschen „Kursanten“ nominierte. Im Sommer 1931 absolvierte er das obligatorische, von der Kommunistischen Jugendinternationale eingerichtete Praktikum des "KIM-Kurses", aus dem zahlreiche Kaderkräfte kommunistischer Machtapparate in Ostmitteleuropa nach 1945 hervorgingen. Während dieser Zeit nahm er mit 27 anderen Kursanten als „Internationale Stoßbrigade“ an einem Arbeitseinsatz in Magnitogorsk teil, wo seit 1929 ein Stahlwerk als künftiges Zentrum der sowjetischen Stahlgewinnung entstand. Honeckers Lehrer an der Lenin-Schule war Erich Wollenberg, der während des Großen Terrors, im Zuge der Wollenberg-Hoelz-Verschwörung durch das NKWD als Gegner Stalins verfolgt wurde. In der Ära der Schulleiterin Kirsanowa, die als „eiserne Stalinistin“ galt, wurde Honecker „Reinigungsritualen“ durch Anklage und Selbstanklage unterzogen. Damit sollten sich seine Ich-Interessen innerhalb eines geschlossenen Weltbildes systematisch dem Kollektiv und den Interessen der Partei unterordnen. In seinen Sechs-Tage-Wochen hatte er ein rigides tägliches Arbeitspensum von zehn Stunden und mehr abzuleisten, das aus Unterricht und Selbststudium bestand und zu politisch-ideologischer Einheitlichkeit und mentaler Folgsamkeit erzog. Das Pensum einer Schulstunde umfasste 4–5 Seiten Marx oder Engels, 6–7 Seiten Lenin, 7–8 Seiten Stalin und 20 Seiten Belletristik. Bis zu seinem Lebensende blieb Stalin Honeckers prägendste politische Bezugsfigur.

Nach der Machtübernahme der NSDAP 1933 war die Arbeit der KPD in Deutschland nur noch im Untergrund möglich. Das Saargebiet jedoch gehörte nicht zum Deutschen Reich. Honecker wurde kurz in Deutschland inhaftiert, jedoch bald entlassen. Er kam 1934 ins Saargebiet zurück und arbeitete mit dem späteren ersten saarländischen Ministerpräsidenten Johannes Hoffmann in der Kampagne gegen die Wiederangliederung an das Deutsche Reich. In dieser Zeit im Widerstand in den Jahren 1934 und 1935 arbeitete er auch eng mit dem KPD-Funktionär Herbert Wehner, später SPD, zusammen. Bei der Saarabstimmung am 13. Januar 1935 stimmten jedoch 90,73 Prozent der Wähler für eine Vereinigung mit Deutschland („Heim ins Reich“).
Der Jungfunktionär floh, wie 4000–8000 andere Menschen auch, zunächst nach Frankreich.

Am 28. August 1935 reiste Honecker unter dem Decknamen „Marten Tjaden“ illegal nach Berlin, eine Druckerpresse im Gepäck, und war wieder im Widerstand tätig. Im Dezember 1935 wurde Honecker von der Gestapo verhaftet und zunächst bis 1937 im Berliner Zellengefängnis Lehrter Straße in Untersuchungshaft genommen. Er wurde im Juni 1937 zu zehn Jahren Zuchthaus verurteilt; der ebenfalls angeklagte Bruno Baum wurde – auch durch Honeckers Aussagen – zu dreizehn Jahren Zuchthaus verurteilt.

Honecker verbüßte seine Haftzeit während der Zeit des Nationalsozialismus im Zuchthaus Brandenburg-Görden. Aufgrund der gestiegenen Zahl der Bombenangriffe auf Berlin ab 1943 teilte man ihn einer Baukolonne zu, die mit LKW zu den beschädigten Gebäuden gefahren wurde, um die Bombenschäden zu reparieren. Als diese Transporte nach einem Jahr zu unsicher wurden, brachte man seine Baukolonne im Frauengefängnis Barnimstraße in Berlin unter. Im März 1945 gelang Honecker gemeinsam mit einem Mitgefangenen während eines Bombenangriffs die Flucht aus dem Frauengefängnis. Er versteckte sich in der Wohnung der Gefängnisaufseherin Charlotte Grund, die in der Landsberger Straße 37 wohnte. Nachdem dort das Vorderhaus ausgebombt wurde, kehrte er, aufgrund der gestiegenen Entdeckungsgefahr, in das Gefängnis zurück, was offenbar durch die dienstverpflichteten Aufseherinnen organisiert wurde. Honecker wurde nach Brandenburg zurückverlegt. Nach der Befreiung des Zuchthauses durch die Rote Armee am 27. April ging Honecker nach Berlin. Seine mit den Mithäftlingen in Brandenburg nicht abgesprochene Flucht, sein Untertauchen in Berlin, die „Rückmeldung“, die Nichtteilnahme an dem geschlossenen Marsch der befreiten kommunistischen Häftlinge nach Berlin und die Verbindung mit einer Gefängnisaufseherin bereiteten Honecker später innerparteiliche Schwierigkeiten und belasteten sein Verhältnis zu ehemaligen Mithäftlingen. Gegenüber der Öffentlichkeit verfälschte Honecker das Geschehen in seinen Lebenserinnerungen und in Interviews.

Im Mai 1945 wurde Honecker eher zufällig von Hans Mahle in Berlin „aufgelesen“ und mit zur Gruppe Ulbricht genommen. Durch Waldemar Schmidt wurde er mit Walter Ulbricht bekannt gemacht, der ihn bis dahin noch nicht persönlich kannte. Bis in den Sommer hinein war über die zukünftige Funktion Honeckers noch nicht entschieden worden, da er sich auch einem Parteiverfahren stellen musste, welches mit einer strengen Rüge endete. Zur Sprache kam dabei auch seine Flucht aus dem Zuchthaus Anfang 1945. 1946 war er dann Mitbegründer der Freien Deutschen Jugend, deren Vorsitz er auch übernahm. Seit der Zwangsvereinigung von SPD und KPD im April 1946 war Honecker Mitglied der SED.

In der im Oktober 1949 gegründeten DDR, einer realsozialistischen Parteidiktatur, setzte Honecker seine politische Karriere zielstrebig fort. Als FDJ-Vorsitzender organisierte er die drei Deutschlandtreffen der Jugend in Berlin ab 1950 und wurde einen Monat nach dem ersten Deutschlandtreffen als Kandidat ins Politbüro des ZK der SED aufgenommen. Er war ein ausgesprochener Gegner kirchlicher Jugendgruppen. In den innerparteilichen Auseinandersetzungen nach dem Volksaufstand vom 17. Juni stellte er sich gemeinsam mit Hermann Matern offen an die Seite Ulbrichts, den die Mehrheit des Politbüros um Rudolf Herrnstadt zu stürzen versuchte. Am 27. Mai 1955 gab er den FDJ-Vorsitz an Karl Namokel ab. Von 1955 bis 1957 hielt er sich zu Schulungszwecken in Moskau auf und erlebte den XX. Parteitag der KPdSU mit Chruschtschows Rede zur Entstalinisierung mit. Nach seiner Rückkehr wurde er 1958 Mitglied des Politbüros, wo er die Verantwortung für Militär- und Sicherheitsfragen übernahm. Als Sicherheitssekretär des ZK der SED war er der maßgebliche Organisator des Baus der Berliner Mauer im August 1961 und trug in dieser Funktion den Schießbefehl an der innerdeutschen Grenze mit.

Auf dem 11. Plenum des ZK der SED, das im Dezember 1965 tagte, tat er sich als einer der Wortführer hervor und griff verschiedene Kulturschaffende wie die Regisseure Kurt Maetzig und Frank Beyer scharf an, denen er „Unmoral“, „Dekadenz“, „spießbürgerlichen Skeptizismus“ und „Staatsfeindlichkeit“ vorwarf. In diese Kritik bezog er auch die kulturpolitisch Verantwortlichen der SED mit ein, ohne sie allerdings namentlich zu nennen: Sie hätten „keinen prinzipiellen Kampf gegen die […] aufgezeigten Erscheinungen geführt.“ Das Plenum beendete die Ansätze einer kulturpolitischen Liberalisierung der DDR, die sich nach dem Mauerbau gezeigt hatten.

Während Walter Ulbricht mit dem Neuen Ökonomischen System der Planung und Leitung die Wirtschaftspolitik ins Zentrum gerückt hatte, deklarierte Honecker die „Einheit von Wirtschafts- und Sozialpolitik“ zur Hauptaufgabe. Nachdem er sich der Unterstützung durch die sowjetische Führung unter Leonid Breschnew vergewissert hatte, sammelte er Unterschriften im Politbüro für die Forderung nach Ulbrichts Absetzung. Als Ulbricht davon erfuhr, warf er Honecker aus dem Politbüro. Daraufhin wandte sich Honecker hilfesuchend an den sowjetischen Botschafter Abrassimov, und auf Breschnews Geheiß musste ihn Ulbricht wieder aufnehmen. Schließlich putschte sich Honecker mit sowjetischem Einverständnis an die Macht: Er wies seine Personenschützer an, Maschinenpistolen mitzunehmen, und fuhr mit ihnen zu Ulbrichts Sommerresidenz in Dölln. Dort ließ er alle Tore und Ausgänge besetzen, die Telefonleitungen kappen und zwang Ulbricht, ein Rücktrittsgesuch an das Zentralkomitee zu unterschreiben. Honecker wurde am 3. Mai 1971 als Nachfolger Ulbrichts "Erster Sekretär" (ab 1976 "Generalsekretär") des Zentralkomitees der SED. Wirtschaftliche Probleme und Unmut in den Betrieben spielten eine große Rolle bei diesem Machtwechsel. Nachdem er 1971 auch im Nationalen Verteidigungsrat als Vorsitzender Ulbrichts Nachfolge angetreten hatte, wählte ihn die Volkskammer am 29. Oktober 1976 schließlich auch zum Vorsitzenden des Staatsrats; Willi Stoph, der diesen Posten seit 1973 innegehabt hatte, wurde erneut, wie vor 1973, Vorsitzender des Ministerrats. Damit hatte Honecker die Machtspitze der DDR erreicht. Von nun an entschied er gemeinsam mit dem ZK-Sekretär für Wirtschaftsfragen, Günter Mittag, und dem Minister für Staatssicherheit, Erich Mielke, alle maßgeblichen Fragen. Bis zum Herbst 1989 stand die „kleine strategische Clique“ aus diesen drei Männern unangefochten an der Spitze der herrschenden Klasse der DDR, der zunehmend vergreisenden Monopolelite der etwa 520 Staats- und Parteifunktionäre. Nach Einschätzung des Historikers Martin Sabrow erlangte Honecker gemeinsam mit diesen beiden eine „Machtfülle wie kein anderer Herrscher in der jüngeren deutschen Geschichte, Ludendorff und Hitler eingeschlossen“, weshalb er ihn als „Diktator“ beschreibt. Unter Honecker entwickelte sich das Politbüro rasch zu einem Kollektiv von kritiklosen, unterwürfigen Vollstreckern und Ja-Sagern. Honecker beantwortete Eingaben von Bürgern immer schnell, weshalb ihn Sabrow in Anlehnung an den aufgeklärten Absolutismus als „obersten Kümmerer seines Staats“ bezeichnet.

Honeckers engster persönlicher Mitarbeiter war der ZK-Sekretär für Agitation und Propaganda, Joachim Herrmann. Mit ihm führte er tägliche Besprechungen über die Medienarbeit der Partei, in denen auch das Layout des Neuen Deutschlands und die Abfolge der Meldungen in der Aktuellen Kamera festgelegt wurden. Auf schlechte Nachrichten über den Zustand der Wirtschaft reagierte er, indem er etwa 1978 das "Institut für Meinungsforschung" schließen ließ.
Große Bedeutung maß Honecker auch dem Feld der Staatssicherheit bei, das er einmal in der Woche jeweils nach der Sitzung des Politbüros mit Erich Mielke durchsprach. Honeckers langjährige Sekretärin war Elli Kelm.

Während seiner Amtszeit wurde der Grundlagenvertrag mit der Bundesrepublik Deutschland ausgehandelt. Außerdem nahm die DDR an den KSZE-Verhandlungen in Helsinki teil und wurde als Vollmitglied in die UNO aufgenommen (→ Deutschland in den Vereinten Nationen). Diese diplomatischen Erfolge gelten als die größten außenpolitischen Leistungen Honeckers.

Am 31. Dezember 1982 versuchte der Ofensetzer Paul Eßling, die Autokolonne Honeckers zu rammen, was in westlichen Medien als Attentat dargestellt wurde.

Innenpolitisch zeichnete sich anfangs eine Liberalisierungstendenz vor allem im Bereich der Kultur und Kunst ab, die aber weniger durch den Personalwechsel von Walter Ulbricht zu Erich Honecker hervorgerufen wurde, sondern Propagandazwecken im Rahmen der 1973 ausgetragenen X. Weltfestspiele der Jugend und Studenten diente. Nur wenig später erfolgten die Ausbürgerung von Regimekritikern wie Wolf Biermann und die Unterdrückung innenpolitischen Widerstands durch das Ministerium für Staatssicherheit. Zudem setzte Honecker sich für den weiteren Ausbau der innerdeutschen Staatsgrenze mit Selbstschussanlagen und den rücksichtslosen Schusswaffengebrauch bei Grenzdurchbruchsversuchen ein. 1974 sagte er dazu, „es sind die Genossen, die die Schußwaffe erfolgreich angewandt haben, zu belobigen.“ Wirtschaftspolitisch wurde unter Honecker die Verstaatlichung und Zentralisierung der Wirtschaft vorangetrieben. Die schwierige wirtschaftliche Lage zwang zur Aufnahme von Milliardenkrediten von der Bundesrepublik Deutschland, um den Lebensstandard halten zu können.

Die Londoner Financial Times sah Honecker 1981 auf der Höhe seiner Popularität und stellt diesen Vergleich zum damaligen Bundeskanzler auf:

1981 empfing er Bundeskanzler Helmut Schmidt im Jagdhaus Hubertusstock am Werbellinsee. Honeckers Einschätzung, die DDR habe „wirtschaftlich Weltklasseniveau erreicht und gehöre zu den bedeutendsten Industrienationen der Welt“, kommentierte Schmidt später mit dem Verdikt vom „Mann von beschränkter Urteilskraft“. Trotz der Wirtschaftsprobleme brachten Honecker die 1980er Jahre vermehrte internationale Anerkennung, insbesondere als er am 7. September 1987 die Bundesrepublik Deutschland besuchte und durch Bundeskanzler Helmut Kohl in Bonn empfangen wurde. Auf seiner Reise durch die Bundesrepublik kam er nach Düsseldorf, Wuppertal, Essen, Trier, Bayern sowie am 10. September in seinen Geburtsort im Saarland. Hier hielt er eine emotionale Rede, in der er davon sprach, eines Tages würden die Grenzen die Menschen in Deutschland nicht mehr trennen. Diese Reise war seit 1983 geplant gewesen, wurde jedoch damals von der sowjetischen Führung blockiert, da man dem deutsch-deutschen Sonderverhältnis misstraute. 1988 war Honecker unter anderem auf Staatsbesuch in Paris. Sein großes Ziel, welches er aber nicht mehr erreichte, war ein offizieller Besuch in den USA. Er setzte deshalb in den letzten Jahren der DDR auf ein positives Verhältnis zum Jüdischen Weltkongress als möglichem „Türöffner“.

Auf dem Gipfeltreffen des Warschauer Paktes in Bukarest am 7. und 8. Juli 1989 im Rahmen des „Politisch-Beratenden Ausschusses“ der RGW-Staaten des Warschauer Paktes gab die Sowjetunion offiziell die Breschnew-Doktrin der begrenzten Souveränität der Mitgliedsstaaten auf und verkündete die „Freiheit der Wahl“: Die Beziehungen untereinander sollten künftig, wie es im Bukarester Abschlussdokument heißt, „auf der Grundlage der Gleichheit, Unabhängigkeit und des Rechtes eines jeden Einzelnen, selbstständig seine eigene politische Linie, Strategie und Taktik ohne Einmischung von außen auszuarbeiten“ entwickelt werden. Die sowjetische Bestandsgarantie für die Mitgliedsstaaten war damit in Frage gestellt. Honecker musste seine Teilnahme an dem Treffen abbrechen; am Abend des 7. Juli 1989 wurde er mit schweren Gallenkoliken in das rumänische Regierungskrankenhaus eingeliefert und dann nach Berlin ausgeflogen. Im Regierungskrankenhaus Berlin-Buch entfernte man ihm am 18. August 1989 die Gallenblase und einen Abschnitt des Dickdarms. Während der Operation wurde ein Nierentumor entdeckt, doch die Ärzte wagten es nicht, Honecker darüber zu unterrichten. Erst im September 1989 tauchte Honecker abgemagert und vergreist wieder im Politbüro auf. Währenddessen leitete Günter Mittag die wöchentlichen Sitzungen des Politbüros. Lediglich im August 1989 nahm Honecker einige Termine wahr. So erklärte er am 14. August 1989 bei der Übergabe der ersten Funktionsmuster von 32-Bit-Prozessoren durch das Kombinat Mikroelektronik Erfurt: "„Den Sozialismus in seinem Lauf hält weder Ochs noch Esel auf.“"

Aber in den Städten der DDR wuchsen Zahl und Größe der Demonstrationen, und auch die Zahl der DDR-Flüchtlinge über die bundesdeutschen Botschaften in Prag und Budapest und über die Grenzen der „sozialistischen Bruderstaaten“ nahm stetig zu, monatlich waren es mehrere Zehntausend. Die ungarische Regierung öffnete am 19. August 1989 an einer Stelle und am 11. September 1989 überall die Grenze zu Österreich. Allein hierüber reisten Zehntausende von DDR-Bürgern über Österreich in die Bundesrepublik aus. Die ČSSR erklärte den Zustrom der DDR-Flüchtlinge für inakzeptabel. Am 3. Oktober 1989 schloss die DDR faktisch ihre Grenzen zu den östlichen Nachbarn, indem sie den visafreien Reiseverkehr in die ČSSR aussetzte; ab dem nächsten Tag wurde diese Maßnahme auch auf den Transitverkehr nach Bulgarien und Rumänien ausgedehnt. Die DDR war dadurch nicht nur wie bisher durch den Eisernen Vorhang nach Westen abgeriegelt, sondern nun auch noch gegenüber den meisten Staaten des Ostblocks. Proteste von DDR-Bürgern bis hin zu Streikandrohungen aus den grenznahen Gebieten zur ČSSR waren die Folge.

Die Beziehung zwischen Honecker und dem Generalsekretär der KPdSU und Präsidenten der UdSSR Gorbatschow war schon seit Jahren gespannt: Honecker hielt seine Politik der Perestroika und Kooperation mit dem Westen für falsch und fühlte sich von ihm speziell in der Deutschlandpolitik hintergangen. Er sorgte dafür, dass offizielle Texte der UdSSR, vor allem solche zum Thema Perestroika, in der DDR nicht mehr veröffentlicht oder in den Handel gebracht werden durften. Am 6. und 7. Oktober 1989 fanden die Staatsfeierlichkeiten zum 40. Jahrestag der DDR in Anwesenheit von Michail Gorbatschow statt, der mit „Gorbi, Gorbi, hilf uns“-Rufen begrüßt wurde.
In einem Vieraugengespräch der beiden Generalsekretäre pries Honecker die Erfolge des Landes. Gorbatschow wusste aber, dass die DDR in Wirklichkeit vor der Zahlungsunfähigkeit stand.

Am Ende einer Krisensitzung am 10. und 11. Oktober 1989 forderte das SED-Politbüro Honecker auf, bis Ende der Woche einen Lagebericht abzugeben, der geplante Staatsbesuch in Dänemark wurde abgesagt und eine Erklärung veröffentlicht, die Egon Krenz gegen den Widerstand Honeckers durchgesetzt hatte. Ebenfalls überwiegend auf Initiative von Krenz folgten in den nächsten Tagen Besprechungen und Sondierungen zu der Frage, Honecker zum Rücktritt zu bewegen. Krenz sicherte sich die Unterstützung von Armee und Stasi und arrangierte ein Treffen zwischen Michail Gorbatschow und Politbüromitglied Harry Tisch, der den Kremlchef am Rande eines Moskaubesuchs einen Tag vor der Sitzung über die geplante Absetzung Honeckers informierte. Gorbatschow wünschte viel Glück, das Zeichen, auf das Krenz und die anderen gewartet hatten. Auch SED-Chefideologe Kurt Hager flog am 12. Oktober 1989 nach Moskau und besprach mit Gorbatschow die Modalitäten der Honecker-Ablösung. Hans Modrow dagegen wich einer Anwerbung aus.

Die für Ende November 1989 geplante Sitzung des ZK der SED wurde auf Ende der Woche vorgezogen, dringendster Tagesordnungspunkt: die Zusammensetzung des Politbüros. Per Telefon versuchten Krenz und Erich Mielke am Abend des 16. Oktober, weitere Politbüromitglieder für die Absetzung Honeckers zu gewinnen. Zu Beginn der Sitzung des Politbüros vom 17. Oktober 1989 fragte Honecker routinemäßig: „Gibt es noch Vorschläge zur Tagesordnung?“ Willi Stoph meldete sich und schlug als ersten Punkt der Tagesordnung vor: „Entbindung des Genossen Honecker von seiner Funktion als Generalsekretär und Wahl von Egon Krenz zum Generalsekretär“. Honecker schaute zuerst regungslos, fasste sich aber rasch wieder: „Gut, dann eröffne ich die Aussprache.“ Nacheinander äußerten sich alle Anwesenden, doch keiner machte sich für Honecker stark. Günter Schabowski erweiterte sogar den Antrag und forderte die Absetzung Honeckers auch als Staatsratsvorsitzender und Vorsitzender des Nationalen Verteidigungsrates. Selbst Günter Mittag rückte von ihm ab. Alfred Neumann wiederum forderte die Ablösung von Mittag und von Joachim Herrmann. Angeblich soll Erich Mielke Honecker für fast alle aktuellen Missstände in der DDR verantwortlich gemacht und Honecker schreiend gedroht haben, kompromittierende Informationen, die er besitze, herauszugeben, falls Honecker nicht zurücktrete.

Nach drei Stunden fiel der einstimmige Beschluss des Politbüros. Honecker votierte, wie es Brauch war, für seine eigene Absetzung. Dem ZK der SED wurde vorgeschlagen, Honecker, Mittag und Hermann von ihren Funktionen zu entbinden. Bei der folgenden ZK-Sitzung waren 206 Mitglieder und Kandidaten anwesend. Lediglich 16 fehlten, darunter Margot Honecker. Das ZK folgte der Empfehlung des Politbüros. Die einzige Gegenstimme kam von der 81-jährigen Hanna Wolf, der früheren Direktorin der Parteihochschule „Karl Marx“. Öffentlich hieß es: „Das ZK hat der Bitte Erich Honeckers entsprochen, ihn aus gesundheitlichen Gründen von der Funktion des Generalsekretärs, vom Amt des Staatsratsvorsitzenden und von der Funktion des Vorsitzenden des Nationalen Verteidigungsrates der DDR zu entbinden.“ Egon Krenz wurde per Akklamation einstimmig zum neuen Generalsekretär der SED gewählt. Am 20. Oktober 1989 musste auch Margot Honecker von ihren Ämtern zurücktreten.

Die Volkskammer der DDR setzte Mitte November 1989 einen Ausschuss zur Untersuchung von Korruption und Amtsmissbrauch ein, dessen Vorsitzender am 1. Dezember 1989 Bericht erstattete. Er warf den bisherigen SED-Machthabern umfassenden Missbrauch öffentlicher Ämter zu privaten Zwecken vor. Honecker habe zudem seit 1978 jährliche Zuwendungen von rund 20.000 Mark durch die Bauakademie der DDR erhalten. Die Staatsanwaltschaft der DDR leitete daraufhin strafrechtliche Ermittlungen gegen 30 ehemalige DDR-Spitzenfunktionäre ein, unter ihnen zehn Mitglieder des Politbüros. Die meisten davon kamen in Untersuchungshaft, so am 3. Dezember 1989 auch Honeckers Wandlitzer Nachbarn Günter Mittag und Harry Tisch wegen persönlicher Bereicherung und Vergeudung von Volksvermögen. Am selben Tag wurde Honecker vom ZK aus der SED ausgeschlossen. Er schloss sich daraufhin der neu gegründeten KPD an, deren Mitglied er von 1992 bis zu seinem Tod war.

Am 30. November 1989 wurde dem Ehepaar Honecker die Wohnung in Wandlitz gekündigt und am 7. Dezember 1989 durchsucht. Wegen der aufgeheizten Stimmung lehnten die Honeckers ein Wohnungsangebot am Bersarinplatz ab, beschwerten sich aber mehrfach, man habe sie obdachlos gemacht.

Am 5. Dezember 1989 wurde auch gegen ihn ein Ermittlungsverfahren eingeleitet. Honecker sei „verdächtig, seine Funktion als Vorsitzender des Staatsrates und des Nationalen Verteidigungsrates der DDR und seine angemaßte politische und ökonomische Macht als Generalsekretär des ZK der SED missbraucht“ und „seine Verfügungsbefugnisse als Generalsekretär des ZK der SED zum Vermögensvorteil für sich und andere missbraucht zu haben“. Federführend war bis Januar 1990 das Amt für Nationale Sicherheit (AfNS) der DDR, also der Nachfolger der Stasi, das hierzu einen „Maßnahmeplan im Ermittlungsverfahren gegen Erich Honecker“ erarbeitet hatte, später betrieb die Abteilung für Wirtschaftsstrafsachen beim Generalstaatsanwalt der DDR das Verfahren.

Am 6. Januar 1990 erfuhr Honecker nach einer erneuten Untersuchung durch eine Ärztekommission aus den Abendnachrichten der Aktuellen Kamera des DDR-Fernsehens, dass er Nierenkrebs hatte. Am 10. Januar 1990 entfernte der Urologe Peter Althaus einen pflaumengroßen Nierentumor. Am Abend des 28. Januar 1990 wurde Honecker in seinem Krankenzimmer der Charité festgenommen, am nächsten Tag in das Haftkrankenhaus des Gefängnisses Berlin-Rummelsburg eingeliefert und nach einem Tag wegen Haftunfähigkeit entlassen.

Rechtsanwalt Wolfgang Vogel wandte sich im Auftrag Honeckers an die Evangelische Kirche in Berlin-Brandenburg und bat um Hilfe. Pastor Uwe Holmer, Leiter der Hoffnungstaler Anstalten in Lobetal bei Bernau, bot daraufhin dem Ehepaar Unterkunft in seinem Pfarrhaus an. Althaus fuhr es noch am Abend des 30. Januar 1990 dorthin. Schon am selben Tag kam es zu Kritik und später zu Demonstrationen gegen die kirchliche Hilfe für das Ehepaar, da beide solche Christen, die sich nicht dem SED-Regime angepasst hätten, benachteiligt hätten. Das Ehepaar wohnte dennoch – abgesehen von einer Unterbringung in einem Ferienhaus in Lindow, die im März 1990 schon nach einem Tag wegen politischer Proteste abgebrochen werden musste – bis zum 3. April 1990 weiter bei Holmers. Dann siedelte das Ehepaar in das sowjetische Militärhospital bei Beelitz über. Bei erneuten Untersuchungen auf Haftfähigkeit stellten dort die Ärzte bei Honecker die Verdachtsdiagnose eines bösartigen Lebertumors. Am 2. Oktober 1990, dem Vorabend der Deutschen Wiedervereinigung, wurden die wirtschaftsstrafrechtlichen Ermittlungsakten im Fall Erich Honecker von der Generalstaatsanwaltschaft der DDR an die der Bundesrepublik übergeben. Am 30. November 1990 erließ das Amtsgericht Tiergarten einen weiteren Haftbefehl gegen Honecker wegen des Verdachts, dass er den Schießbefehl an der innerdeutschen Grenze 1961 verfügt und 1974 bekräftigt habe. Der Haftbefehl war aber nicht vollstreckbar, da Honecker sich in Beelitz unter dem Schutz sowjetischer Stellen befand. Am 13. März 1991 wurde das Ehepaar nach vorheriger Information des Bundeskanzlers Kohl durch den sowjetischen Staatspräsidenten Gorbatschow mit einem sowjetischen Militärflugzeug von Beelitz nach Moskau ausgeflogen.

Das Kanzleramt war durch die sowjetische Diplomatie über die bevorstehende Ausreise der Honeckers nach Moskau informiert worden. Die Bundesregierung beschränkte sich aber öffentlich auf den Protest, es liege bereits ein Haftbefehl vor, daher verstoße die Sowjetunion gegen die Souveränität der Bundesrepublik Deutschland und damit gegen Völkerrecht. Immerhin war zu diesem Zeitpunkt der Zwei-plus-Vier-Vertrag, der Deutschland die volle Souveränität zuerkennen sollte, vom Obersten Sowjet noch nicht ratifiziert. Erst am 15. März 1991 trat der Vertrag mit der Hinterlegung der sowjetischen Ratifizierungsurkunde beim deutschen Außenminister offiziell in Kraft. Von diesem Augenblick an wuchs der deutsche Druck auf Moskau, Honecker zu überstellen.

Zwischen Michail Gorbatschow und Honecker bestand ohnehin ein seit Jahren stetig schlechter werdendes Verhältnis, die UdSSR befand sich in der Auflösung. Den Augustputsch in Moskau überstand Gorbatschow nur geschwächt. Der neue starke Mann, Boris Jelzin, Präsident der russischen Teilrepublik RSFSR, verbot die KPdSU, deren Generalsekretär Gorbatschow war. Am 25. Dezember 1991 trat Gorbatschow als Präsident der Sowjetunion zurück. Die russische Regierung unter Jelzin forderte Honecker im Dezember 1991 auf, das Land zu verlassen, da andernfalls die Abschiebung erfolge. Am 11. Dezember 1991 flüchteten die Honeckers daher in die chilenische Botschaft in Moskau. Nach Erinnerung Margot Honeckers hatten zwar auch Nordkorea und Syrien Asyl angeboten, von Chile erhoffte man sich aber besonderen Schutz: Nach dem Militärputsch von 1973 unter Augusto Pinochet hatte die DDR unter Honecker vielen Chilenen, auch dem Botschafter Clodomiro Almeyda, Exil in der DDR gewährt, und Honeckers Tochter Sonja war mit einem Chilenen verheiratet. In Anspielung auf die DDR-Flüchtlinge in den bundesdeutschen Botschaften in Prag und Budapest wurde das Ehepaar Honecker ironisch „letzte Botschaftsflüchtlinge der DDR“ genannt. Chile allerdings wurde damals durch eine links-bürgerliche Koalition regiert, und die deutsche Bundesregierung äußerte, wenn Russland und Chile ihren Anspruch einlösen wollten, Rechtsstaaten zu sein, müsste Honecker, da mit Haftbefehl in Deutschland gesucht, in die Bundesrepublik überstellt werden. Am 22. Juli begründete der deutsche Botschafter Klaus Blech im russischen Außenministerium: „Nach Auffassung der deutschen Regierung verstößt die widerrechtliche Verbringung von Herrn Honecker gegen den Vertrag über die Bedingungen des befristeten Aufenthalts und die Modalitäten des planmäßigen Abzugs der sowjetischen Truppen aus dem Gebiet der Bundesrepublik Deutschland und gegen allgemeines Völkerrecht, weil sie dazu diente, eine wegen Anstiftung zur mehrfachen vorsätzlichen Tötung durch Haftbefehl gesuchte Person der Strafverfolgung zu entziehen.“

Allerdings war der bei Honecker bereits in Beelitz erhobene Verdacht auf Leberkrebs im Februar 1992 in Moskau durch eine Ultraschall-Untersuchung mit dem Befund „herdförmiger Befall der Leber – Metastase“ bestärkt worden. Drei Wochen später aber soll die grundsätzlich zuverlässigere Untersuchung durch ein Computertomogramm ergeben haben: „Werte für einen herdförmigen Befall der Leber wurden nicht festgestellt“. Nun wurde gegen Honecker verbreitet, er sei ein Simulant. Drei Tage später verkündete der russische Justizminister Fjodorow im deutschen Fernsehen, Honecker werde nach Deutschland überstellt, sobald er die Botschaft verlassen habe. Am 7. März 1992 hieß es, die chilenische Regierung korrigiere ihre Haltung im Fall Honecker, Botschafter Almeyda sei zur Berichterstattung nach Santiago beordert, man sei verärgert über seinen Versuch, mit offenbar manipulierten Berichten über den todkranken Honecker dessen Einreise nach Chile zu erreichen. Almeyda wurde von seinem Posten abberufen. Zwar protestierte am 18. März 1992 eine Gruppe von Ärzten aus dem russischen Parlament und machte geltend, es sei die März-Diagnose, die manipuliert worden sei. Aber für die Öffentlichkeit schien Honeckers altersgerecht guter Allgemeinzustand gegen eine Krebserkrankung zu sprechen. Im Juni 1992 sicherte der chilenische Präsident Patricio Aylwin schließlich Bundeskanzler Helmut Kohl zu, Honecker werde die Botschaft in Moskau verlassen. Die Russen ergänzten, sie sähen „keinen Grund“, von ihrer Entscheidung von Dezember 1991 abzurücken, „wonach Honecker nach Deutschland zurückzukehren hat“. Am 29. Juli 1992 wurde Erich Honecker nach Berlin ausgeflogen, wo er verhaftet und in die Justizvollzugsanstalt Moabit gebracht wurde. Margot Honecker dagegen reiste per Direktflug der Aeroflot von Moskau nach Santiago de Chile, wo sie zunächst bei ihrer Tochter Sonja unterkam und bis zu ihrem Tod am 6. Mai 2016 lebte.

Am 29. Juli 1992 wurde Honecker in Untersuchungshaft im Krankenhaus der Berliner Vollzugsanstalten in Berlin-Moabit genommen.

Die Schwurgerichtsanklage vom 12. Mai 1992 warf ihm vor, als Vorsitzender des Staatsrats und des Nationalen Verteidigungsrates (NVR) der DDR gemeinsam mit mehreren Mitangeklagten, unter anderem Erich Mielke, Willi Stoph, Heinz Keßler, Fritz Streletz und Hans Albrecht, in der Zeit 1961 bis 1989 am Totschlag von insgesamt 68 Menschen beteiligt gewesen zu sein, indem er insbesondere als Mitglied des NVR angeordnet habe, die Grenzanlagen um West-Berlin und die Sperranlagen zur Bundesrepublik auszubauen, um ein Passieren unmöglich zu machen. Insbesondere zwischen 1962 und 1980 habe er mehrfach Maßnahmen und Festlegungen zum weiteren pioniertechnischen Ausbau der Grenze durch Errichtung von Streckmetallzäunen zur Anbringung der Selbstschussanlagen und der Schaffung von Sicht- und Schussfeld entlang der Grenzsicherungsanlagen getroffen, um Grenzdurchbrüche zu verhindern. Außerdem habe er im Mai 1974 in einer Sitzung des NVR dargelegt, der pioniermäßige Ausbau der Staatsgrenze müsse weiter fortgesetzt werden, überall müsse ein einwandfreies Schussfeld gewährleistet werden und nach wie vor müsse bei Grenzdurchbruchsversuchen von der Schusswaffe rücksichtslos Gebrauch gemacht werden. „Die Genossen, die die Schusswaffe erfolgreich angewandt haben“, seien „zu belobigen“.

Diese Anklage ist durch Beschluss des Landgerichts Berlin vom 19. Oktober 1992 unter Eröffnung des Hauptverfahrens zugelassen worden. Mit Beschluss vom gleichen Tage wurde das Verfahren hinsichtlich 56 der angeklagten Fälle abgetrennt, deren Verhandlung zurückgestellt wurde. Die verbliebenen 12 Fälle waren Gegenstand der am 12. November 1992 begonnenen Hauptverhandlung. Ebenfalls am 19. Oktober 1992 erließ die Strafkammer einen Haftbefehl hinsichtlich der verbliebenen zwölf Fälle.

Eine zweite Anklageschrift vom 12. November 1992 legte Honecker zur Last, in der Zeit von 1972 bis Oktober 1989 Vertrauensmissbrauch in Tateinheit mit Untreue zum Nachteil sozialistischen Eigentums begangen zu haben. Es handelte sich hierbei um Vorgänge im Zusammenhang mit der Versorgung und Betreuung der Waldsiedlung Wandlitz. In diesem Zusammenhang erging am 14. Mai 1992 ein weiterer Haftbefehl.

Der von aller Welt mit Spannung erwartete Prozess hatte nach Ansicht vieler Juristen einen ungewissen Ausgang. Denn nach welchen Gesetzen der Staatschef der untergegangenen DDR eigentlich verurteilt werden konnte, war umstritten. Auch mussten die Politiker der alten Bundesrepublik befürchten, ihrem „vormaligen Bankettgesellen“ (so der DDR-Schriftsteller Hermann Kant), den sie noch 1987 in Bonn, München und anderen Städten mit allen protokollarischen Ehren empfangen hatten, im Gerichtssaal gegenübergestellt zu werden.

In seiner am 3. Dezember 1992 vor Gericht vorgetragenen Erklärung übernahm Honecker zwar die politische Verantwortung für die Toten an Mauer und Stacheldraht, doch sei er „ohne juristische oder moralische Schuld“. Er rechtfertigte den Bau der Mauer damit, dass aufgrund des sich zuspitzenden Kalten Krieges die SED-Führung 1961 zu dem Schluss gekommen sei, dass anders ein „dritter Weltkrieg mit Millionen Toten“ nicht zu verhindern gewesen sei, und betonte die Zustimmung der sozialistischen Führungen sämtlicher Ostblockstaaten zu dieser gemeinschaftlich getroffenen Entscheidung und verwies auf die Funktionen, die der DDR in seiner Amtszeit im UN-Weltsicherheitsrat trotz des Schießbefehls an der Mauer zugestanden worden seien. Im Weiteren führte er an, dass der Prozess gegen ihn aus rein politischen Motiven geführt werde, und verglich die 49 Mauertoten, deretwegen er angeklagt war, etwa mit der Anzahl der Opfer im von den USA geführten Vietnamkrieg oder der Selbstmordrate in westlichen Ländern. Die DDR habe bewiesen, „dass Sozialismus möglich und besser sein kann als Kapitalismus“. Öffentliche Kritik an Verfolgungen durch die Stasi tat er damit ab, dass auch der „Sensationsjournalismus“ in westlichen Ländern mit Denunziation arbeite und die gleichen Konsequenzen habe.

Honecker war zu dieser Zeit bereits schwer krank. Eine erneute Computertomographie am 4. August 1992 bestätigte die Moskauer Ultraschall-Untersuchung: Im rechten Leberlappen befand sich ein „fünf Zentimeter großer raumfordernder Prozess“, vermutlich eine Spätmetastase des Nierenkrebses, der Honecker im Januar 1990 in der Charité entfernt worden war. Unter Berufung auf diese Feststellungen stellten Honeckers Anwälte Nicolas Becker, Friedrich Wolff und Wolfgang Ziegler den Antrag, das Verfahren, soweit es sich gegen Honecker richte, abzutrennen, einzustellen und den Haftbefehl aufzuheben. Das Verfahren sei eine Nagelprobe für den Rechtsstaat. Ihr Mandant leide an einer unheilbaren Krankheit, die entweder durch Ausschaltung der Leberfunktion direkt oder durch Metastasierung in anderen Bereichen zum Tode führe. Seine Lebenserwartung sei geringer als die auf mindestens zwei Jahre geschätzte Prozessdauer. Es sei zu fragen, ob es human ist, gegen einen Sterbenden zu verhandeln.

Den gestellten Antrag lehnte die Strafkammer mit Beschluss vom 21. Dezember 1992 ab. Das Landgericht führte in seiner Begründung aus, dass kein Verfahrenshindernis bestehe. Zwar habe sich die Einschätzung der voraussichtlich eintretenden Verhandlungsunfähigkeit aufgrund der aktualisierten schriftlichen Gutachten zeitlich verdichtet. Die Prognose des Eintritts der Verhandlungsunfähigkeit sei jedoch im Hinblick auf die Schwere und Bedeutung des Tatvorwurfs und des sich daraus ergebenden Gewichts der verfassungsrechtlich gebotenen Pflicht zur Strafverfolgung noch immer zu ungewiss, als dass eine sofortige Einstellung des Verfahrens zwingend geboten erscheine.

Die hiergegen eingelegte Beschwerde verwarf das Kammergericht durch Beschluss vom 28. Dezember 1992. Das Kammergericht kam jedoch zu dem Ergebnis, aufgrund der Stellungnahmen und Gutachten der medizinischen Sachverständigen sei davon auszugehen, dass infolge eines bösartigen Tumors im rechten Leberlappen Honeckers eine Verhandlungsfähigkeit mit hoher Wahrscheinlichkeit nicht mehr lange bestehen werde und Honecker mit an Sicherheit grenzender Wahrscheinlichkeit den Abschluss des Verfahrens nicht überleben werde. Das Kammergericht sah sich gleichwohl gehindert, das Verfahren selbst einzustellen, weil dies gemäß Abs. 3 StPO nach Beginn der Hauptverhandlung nur noch vom Landgericht durch Urteil ausgesprochen werden könne. Dementsprechend könne es auch den bestehenden Haftbefehl nicht aufheben, bevor das Landgericht über das Vorliegen eines Verfahrenshindernisses entschieden habe.

Hiergegen erhob Honecker Verfassungsbeschwerde vor dem Verfassungsgerichtshof des Landes Berlin. Honecker führte aus, die Entscheidungen verletzten sein Grundrecht auf Menschenwürde. Die Menschenwürde gelte als tragendes Prinzip der Verfassung auch gegenüber dem staatlichen Strafvollzug und der Strafjustiz uneingeschränkt. Die Fortführung eines Strafverfahrens und einer Hauptverhandlung gegen einen Angeklagten, von dem mit Sicherheit zu erwarten sei, dass er vor Abschluss der Hauptverhandlung und mithin vor einer Entscheidung über seine Schuld oder Unschuld sterben werde, verletze dessen Menschenwürde. Die Menschenwürde umfasse insbesondere das Recht eines Menschen, in Würde sterben zu dürfen.

Mit Beschluss vom 12. Januar 1993 entsprach der Verfassungsgerichtshof der Verfassungsbeschwerde Honeckers. Aufgrund der Feststellungen des Kammergerichts, wonach Honecker den Abschluss des Verfahrens mit an Sicherheit grenzender Wahrscheinlichkeit nicht mehr erleben werde, sei davon auszugehen, dass das Strafverfahren seinen gesetzlichen Zweck auf vollständige Aufklärung der Honecker zur Last gelegten Taten und gegebenenfalls Verurteilung und Bestrafung nicht mehr erreichen könne. Das Strafverfahren werde damit zum Selbstzweck, wofür es keinen rechtfertigenden Grund gäbe. Die Aufrechterhaltung des Haftbefehls verletze den Anspruch Honeckers auf Achtung seiner Menschenwürde. Der Mensch werde zum bloßen Objekt staatlicher Maßnahmen insbesondere dann, wenn sein Tod derart nahe sei, dass ein Strafverfahren seinen Sinn verloren habe.

Noch am selben Tage stellte das Landgericht Berlin das Verfahren nach StPO ein und hob den Haftbefehl auf. Den hiergegen von der Staatsanwaltschaft und den Nebenklägern erhobenen Beschwerden half das Landgericht nicht ab. Der Antrag auf Erlass eines neuen Haftbefehls wurde mit Beschluss vom 13. Januar 1993 abgelehnt.

Am 13. Januar 1993 lehnte das Landgericht Berlin in Bezug auf die Anklageschrift vom 12. November 1992 die Eröffnung des Hauptverfahrens ab und hob auch den zweiten Haftbefehl auf. Nach insgesamt 169 Tagen wurde Honecker aus der Untersuchungshaft entlassen, was Proteste von Opfern des DDR-Regimes nach sich zog.

Honecker flog unmittelbar darauf nach Santiago de Chile zu Frau und Tochter Sonja (* 1952), die dort mit ihrem chilenischen Ehemann Leo Yáñez und ihrem Sohn Roberto wohnte. Die mit ihm Angeklagten wurden dagegen am 16. September 1993 zu Freiheitsstrafen zwischen vier und siebeneinhalb Jahren verurteilt. Am 13. April 1993 wurde ein letzter zur Verfahrensbeschleunigung abgetrennter und in Abwesenheit des Angeklagten fortgesetzter Prozess gegen Honecker vom Berliner Landgericht ebenfalls eingestellt. Am 17. April 1993, dem 66. Geburtstag seiner Frau Margot, rechnete Honecker in einer Rede mit dem Westen ab und bedauerte seine Genossen, die noch im Gefängnis in Moabit saßen und „dem Klassenfeind trotzten“. Er schloss seine Rede mit den Worten: „Sozialismus ist das Gegenteil von dem, was wir jetzt in Deutschland haben. Sodass ich sagen möchte, dass unsere schönen Erinnerungen an die DDR viel aussagen von dem Entwurf einer neuen, gerechten Gesellschaft. Und dieser Sache wollen wir für immer treu bleiben.“

In den letzten Monaten musste Honecker künstlich ernährt werden. Am 29. Mai 1994 starb er im Alter von 81 Jahren in Santiago de Chile. Sein Leichnam wurde im Krematorium des Zentralfriedhofs von Santiago eingeäschert. Nach der Trauerfeier wurde seine Urne von seiner Witwe mit nach Hause genommen, später jedoch auf diesem Friedhof beigesetzt.

Während seiner Kaderschulung ab 1930 auf der Moskauer Lenin-Schule lernte Honecker auf einer der Kultur- und Tanzveranstaltungen des Elektrokombinats "Elektrosawod" seine erste Freundin Natascha Grejewna kennen, womit er gegen die strengen Konspirationsregeln der Kominternschule verstieß, die es ihm strikt untersagten, mit Unbekannten anzubändeln. Die Entdeckung dieser Liebelei hätte zum Abbruch von Honeckers Schullaufbahn führen können. Honecker war dreimal verheiratet. Am 23. Dezember 1946 heiratete er die neun Jahre ältere Gefängnisaufseherin Gertrud Margarete Charlotte Schanuel, geborene Drost, die zu diesem Zeitpunkt im Frauengefängnis Barnimstraße tätig war. In der Literatur wird teilweise angegeben, er habe nicht Charlotte Schanuel, sondern seine vormalige Gefängnisaufseherin Charlotte Grund geheiratet. Dieses Gerücht geht auf den ehemaligen DDR-Staatsanwalt Peter Przybylski zurück, der sich dabei auf die Schriftstellerin Wera Küchenmeister berief, die 1945 Grunds Etagennachbarin in der Landsberger Straße 37 war. In einem Interview mit dem Publizisten Ed Stuhler beschrieb Küchenmeister die Beziehung zwischen Honecker und Grund jedoch lediglich als eheähnliche Gemeinschaft. Nachdem Charlotte Schanuel im Juni 1947 an den Folgen eines Hirntumors verstorben war, heiratete er im Dezember 1949 die FDJ-Funktionärin Edith Baumann; die gemeinsame Tochter Erika wurde 1950 geboren. Nachdem die Volkskammerabgeordnete Margot Feist im Dezember 1952 eine uneheliche Tochter, Sonja, von Honecker bekommen hatte, ließ sich Edith Baumann 1953 von ihm scheiden. Im selben Jahr wurde Margot Feist seine dritte Ehefrau.

Sonja Honecker heiratete den Chilenen Leonardo Yáñez Betancourt. Das Paar hat einen Sohn, den 1974 geborenen Roberto Yáñez Betancourt y Honecker, sowie eine Tochter, die 1988 geborene Vivian. Eine weitere Enkelin, Mariana, starb 1988 im Alter von zwei Jahren, was Honecker laut Martin Sabrow schwer traf. 1993 wurde die Ehe geschieden. Sonja Honecker, ihr Ex-Mann und deren Sohn leben heute in Santiago de Chile, Margot Honecker starb 2016 ebendort.

Honeckers Hobby war die Jagd (vgl. Volksjagd). Er war passionierter Jäger geworden, nachdem Klement Gottwald ihm noch als FDJ-Chef ein Jagdgewehr geschenkt hatte. Bald nach seinem Amtsantritt im Politbüro richtete Honecker die "Inspektion Staatsjagd" ein, eine Arbeitsgruppe, die zentral Bauvorhaben und Einweisungen der Jagdgäste in den Staatsjagd- und Diplomatenjagdgebieten vornahm. Das Jagdhaus Hubertusstock in der Schorfheide wurde Schauplatz von Besuchen westlicher Politiker und Manager. Honeckers Jagdpassion stand in Aufwand und Ausübung der Jagd in einer systemübergreifenden Tradition; er ging zuletzt am 8. November 1989 zur Jagd.

Honecker wird wie sein Vorgänger als Staats- und Parteichef, Ulbricht, von einigen Historikern als wenig charismatisch in seinen öffentlichen Auftritten und als nicht sonderlich redebegabt beschrieben, während ihm andere eine gewisse Rhetorik zugestehen. Vor allem seine Reden auf Parteitagen und bei diplomatischen Anlässen, die Kabarettisten und Satirikern außerhalb der DDR-Öffentlichkeit Vorlagen zu Parodien boten, werden von einigen als im Stil ungelenk und hölzern beschrieben. In seiner Zeit als Generalsekretär wurde seine Haltung einmal als „fast unheimliche, einstudierte Unbeweglichkeit“ skizziert.

Udo Lindenbergs größter kommerzieller Erfolg, der Song "Sonderzug nach Pankow", zur Melodie Chattanooga Choo Choo, richtete sich in ironischer Weise direkt an den damaligen Staatsratsvorsitzenden, thematisierte dessen mangelnde Lockerheit und erreichte in der DDR große Popularität. Um einen Konzertauftritt im Palast der Republik anzubahnen, schenkte er Honecker 1987 eine Lederjacke. Im Gegenzug erhielt er von Honecker, der in seiner Jugend beim Roten Frontkämpferbund Schalmei gespielt hatte, ein solches Instrument. Bei seinem Staatsbesuch 1987 in der Bundesrepublik Deutschland schenkte ihm Lindenberg eine E-Gitarre mit der Aufschrift „Gitarren statt Knarren“.

Honecker wurde zweimal auf Briefmarken abgebildet: In der DDR 1972 von der Deutschen Post gemeinsam mit Leonid Breschnew anlässlich des 25. Jahrestags der Gründung der Gesellschaft für Deutsch-Sowjetische Freundschaft und 1984 von der Post Nordkorea gemeinsam mit Kim Il-sung zu Ehren von dessen Besuch in der DDR.

Dmitri Wrubels Graffito "Mein Gott, hilf mir, diese tödliche Liebe zu überleben" an der Berliner Mauer (Frühjahr 1990), welches einen „Bruderkuss“ zwischen Leonid Breschnew und Erich Honecker thematisierte, wurde weltweit bekannt.

Im Jahre 2017 wurde in mehreren Filmen Erich Honecker dargestellt. Am 3. Oktober 2017 zeigte die ARD die Filmkomödie "Willkommen bei den Honeckers". Kurze Zeit später erschien der Kinofilm "Vorwärts immer!", in welchem Honecker vom Schauspieler Jörg Schüttauf dargestellt wird.

Honecker erhielt alle wichtigen Auszeichnungen der DDR, darunter den Karl-Marx-Orden, den Ehrentitel Held der DDR mit dazugehöriger Goldmedaille, den Vaterländischen Verdienstorden mit Ehrenspange, Banner der Arbeit, Held der Arbeit, und von der Sowjetunion als höchste Auszeichnung den Leninorden.

1981 wurde Honecker während seines Staatsbesuches in Japan die Ehrendoktorwürde der Nihon-Universität Tokio verliehen.
1985 bekam Honecker vom IOC den Olympischen Orden in Gold.






</doc>
<doc id="1269" url="https://de.wikipedia.org/wiki?curid=1269" title="Erika Eleniak">
Erika Eleniak

Erika Maya Eleniak (* 29. September 1969 in Glendale, Kalifornien) ist eine US-amerikanische Schauspielerin und ehemaliges Playmate. Sie wurde unter anderem durch die Rolle der "Shauni McClain" in der Fernsehserie "Baywatch" bekannt.

Eleniaks Vater, der aus Kanada in die USA kam, um Schauspieler zu werden, spornte auch seine Tochter an, in diesem Bereich tätig zu werden. So trat sie bereits als Kind in verschiedenen Werbespots auf und arbeitete in den folgenden Jahren immer wieder als Werbemodel. Im Jahr 1982 trat sie erstmals in einem Film auf. In "E. T. – Der Außerirdische" spielte sie Elliots Klassenkameradin während der Frosch-Sezierungs<nowiki></nowiki>szene, in welcher Elliot sie zum Schluss küsste. Nachdem sie in ihrer Highschool-Zeit Drogen und Alkohol verfallen war, schaffte sie es 1988 ihre Sucht zu besiegen und ergatterte verschiedene kleinere Rollen in Serien wie "Charles in Charge" und "Broken Angel".

1989 bemühte sie sich Playmate zu werden, um ihrer stagnierenden Karriere Schwung zu verleihen. Ihr Plan ging auf, und nachdem sie in zwei Playboy-Ausgaben erschienen war, wurde ihr eine Hauptrolle in der neuen Serie "Baywatch" angeboten. Zwei Staffeln lang arbeitete sie bei der Erfolgsserie, bis sie sie aufgrund ihres Partners und Co-Stars Billy Warlock verließ und somit Platz für Pamela Anderson schuf. Nach der Trennung von Warlock gelang ihr 1992 der bislang größte Erfolg: Neben Steven Seagal spielte sie in "Alarmstufe: Rot" ein Playmate. Ein Jahr darauf wurde die Komödie "Die Beverly Hillbillies sind los!" veröffentlicht, floppte aber beträchtlich. Ihr nächster Film "Chasers – Zu sexy für den Knast" von Regisseur Dennis Hopper war ebenfalls erfolglos.

In den Folgejahren versuchte sich Eleniak an verschiedenen Independent- und Fernsehfilmen. Ein weiterer Erfolg blieb ihr jedoch verwehrt. Eleniak ist Mutter einer 2006 geborenen Tochter.




</doc>
<doc id="1271" url="https://de.wikipedia.org/wiki?curid=1271" title="Eiben">
Eiben

Die Eiben ("Taxus") bilden eine Pflanzengattung in der Familie der Eibengewächse (Taxaceae). Die etwa zehn Arten sind hauptsächlich in den gemäßigten Gebieten der Nordhalbkugel verbreitet; in Europa ist die Europäische Eibe ("Taxus baccata") als einzige Art heimisch.

Eiben-Arten sind immergrüne Sträucher oder kleine bis mittelgroße Bäume. Junge Zweige besitzen anfangs eine grüne bis gelblich-grüne Rinde; an ihrem unteren Bereich kann man einige Knospenschuppen beobachten. Später wird die Rinde rötlich-braun, an älteren Ästen entwickelt sich eine schuppige, rötlich-braune Borke.

Die Nadeln sind spiralig am Zweig angeordnet, sind aber gescheitelt, so dass sie zweireihig angeordnet zu sein scheinen. Die linealischen, biegsamen Nadeln können gerade oder gebogen sein, vorne enden sie mit einer kleinen aufgesetzten, aber nicht stechenden Spitze. Auf der Oberseite der Nadeln tritt die Mittelader hervor, auf der Unterseite befinden sich zwei helle Streifen mit den Stomata.

Eiben-Arten sind meist zweihäusig getrenntgeschlechtig (diözisch): Männliche und weibliche Blüten stehen auf separaten Pflanzenexemplaren, gelegentlich sind sie einhäusig getrenntgeschlechtig (monözisch). Die männlichen Zapfen sind kugelig, gelblich mit vier bis 16 Sporophyllen, die jeweils zwei bis neun Sporangien besitzen.

Die Samen reifen im Jahr der Befruchtung. Weibliche Pflanzen tragen im Herbst rote „Früchte“, die in der Mitte einen einzelnen Samen enthalten. Das den Samen umgebende rote, fleischige Gewebe, der Samenmantel (Arillus) entwickelt sich nicht aus der Samenschale (Testa), sondern aus dem Stielbereich der Samenanlage (Funiculus). Der becherförmige Arillus weist je nach Art unterschiedliche Rottöne auf. Man spricht in diesem Fall nicht von einer Frucht (im botanischen Sinne), sondern von einem Samenmantel (Arillus), da es Früchte per definitionem nur bei Bedecktsamigen Pflanzen geben kann.

Die Verbreitung des Pollens erfolgt über den Wind (Anemophilie). Die Samen werden hauptsächlich von Vögeln verbreitet, die den fleischigen Samenmantel verzehren und den Samen später wieder ausscheiden (Endochorie). Die Keimung erfolgt epigäisch, es sind zwei Keimblätter vorhanden. Hirsche und Elche fressen gelegentlich Eibennadeln.

Die meisten Eibenarten, wie die Europäische Eibe ("Taxus baccata"), enthalten sehr giftige Inhaltsstoffe wie Taxin B. Insbesondere die Pazifische Eibe ("Taxus brevifolia") enthält Paclitaxel (Taxol), das zur Behandlung von Brust- und Eierstockkrebs eingesetzt wird. Giftig sind Rinde, Nadeln und Samen. Der rote Samenmantel enthält jedoch keine Giftstoffe. Fälle von tödlichen Vergiftungen durch Eiben sind von Menschen, Rindern und Pferden bekannt. Das Vorkommen von Ecdysteron wurde mehrfach beschrieben.

Die Eiben-Arten sind hauptsächlich in der gemäßigten Zone der Nordhalbkugel verbreitet. In der Neuen Welt erreichen sie südwärts noch Mexiko, Guatemala und El Salvador. In Südostasien sind sie in tropischen Gebirgswäldern vertreten und überschreiten auf Celebes den Äquator. Während sie im Norden ihres Verbreitungsgebietes in tieferen Lagen vorkommen, erreichen sie in den Tropen Höhenlagen von 3000 Meter.

Eiben waren ursprünglich in Deutschland rund um die Bergregionen weit verbreitet. Im Jahr 1568 unterrichtete Herzog Albrecht den Kaiserlichen Rat in Nürnberg, dass sich in ganz Bayern keine hiebreife Eibe mehr befinde. Der Grund dafür war, dass aus dem Holz der Eiben die englischen Langbögen hergestellt wurden. Von Nürnberg aus wurden sie zu Tausenden als früher Exportschlager nach Antwerpen verschifft. Der Paterzeller Eibenwald hat sich als kleines Eibenwaldrelikt im ehem. Klosterforst von Wessobrunn bis heute erhalten, ebenso ein "Naturwaldreservat Eibenwald" in Gößweinstein. Weitere größere Vorkommen befinden sich in Süd-Niedersachsen nahe Bovenden nördlich von Göttingen sowie in Thüringen im Ibengarten bei Dermbach in der Rhön, am Lengenberg westlich Lutter (Eichsfeld) im Kreis Heiligenstadt und im Naturschutzgebiet "Dissau und Steinberg" bei Rudolstadt.

Eiben wachsen in der Strauchschicht feuchter Wälder oder bilden einen Teil der Kronenschicht.

Ortsnamen mit dem Bestandteil „ib“ weisen auf frühere Eibenbestände hin, zum Beispiel Unteribental oder Unteriberg.

Die systematische Abgrenzung der Arten und Varietäten innerhalb der Gattung ist schwierig und bei den Autoren teils unterschiedlich. Aljos Farjon unterscheidet folgende Arten:


Der wissenschaftliche Name der Gattung, lateinisch "taxus", wird etymologisch über neupersisch "taχš" ‚Armbrust, Pfeil‘ und altgriechisch τόξον ‚Pfeilbogen‘ (für deren Herstellung sich Eibenholz besonders eignet) mit (vielleicht beiden Wörtern zugrundeliegendem) skythisch *"taχša-" verbunden, sowie mit dem nicht näher bestimmbaren altindischen Baumnamen "takṣaka-".

Es gibt zahlreiche Kreuzungen. Die bekannteste Kreuzung ist die Hybrid-Eibe ("Taxus" ×"media" ), eine 1900 in Massachusetts entstandene Kreuzung aus "Taxus baccata" und "Taxus cuspidata". Ihre breit säulenförmig wachsende Zuchtform ‘Hicksii’ wird relativ häufig in Parks und Gärten gepflanzt.



</doc>
<doc id="1272" url="https://de.wikipedia.org/wiki?curid=1272" title="Edwin Hubble">
Edwin Hubble

Edwin Powell Hubble (* 20. November 1889 in Marshfield, Missouri; † 28. September 1953 in San Marino, Kalifornien) war ein US-amerikanischer Astronom.
Er klassifizierte die Spiralgalaxien, befasste sich mit der Expansion des Weltalls und entdeckte die Hubble-Konstante der galaktischen Kosmologie und ist Namensgeber des Hubble-Weltraumteleskops.

Hubble studierte Physik und Astronomie in Chicago und beendete dies 1910 mit dem Abschluss als Bachelor of Science. Anschließend verließ er die USA zum Studium der Rechtswissenschaft in Oxford, das er mit dem Master abschloss. Nach drei Jahren kehrte er in die USA zurück.

Als es Vesto Slipher 1912 am Lowell-Observatorium in Flagstaff (Arizona) gelang, erstmals die Radialgeschwindigkeit eines Spiralnebels zu messen, war auch Hubble als Student mit der Relativgeschwindigkeit des Andromedanebels (M31) zum Milchstraßensystem befasst. Am Mount-Wilson-Observatorium konnte er 1923 nachweisen, dass M31 weit außerhalb unserer Galaxis liegt. Die Ergebnisse seiner Beobachtungen und Berechnungen, "Cepheids in Spiral Nebulae," legte er zur Jahreswende 1924/25 der Jahrestagung der American Astronomical Society (AAS) vor, auf der sie am 1. Januar 1925 vorgetragen wurden.

Aufgrund der räumlichen Verteilung anderer Galaxien sowie ihrer im Spektrum u .a. von Milton Humason nachgewiesenen Rotverschiebung postulierte der belgische Priester Georges Lemaître im Juni 1927 die Expansion des Weltalls im Einklang mit Einsteins Allgemeiner Relativitätstheorie. Hubble veröffentlichte zwei Jahre später mit zusätzlichen Daten denselben linearen Zusammenhang zwischen der Rotverschiebung und der Verteilung extragalaktischer Nebel, zog jedoch nicht die physikalische Schlussfolgerung einer Expansion des Weltalls und vermutete ein bisher unentdecktes Naturprinzip hinter der Rotverschiebung. Dennoch wird in der öffentlichen Wahrnehmung diese Entdeckung Lemaîtres häufig Hubble zugeschrieben.

Hubble und Humason entdeckten auf Basis der Arbeiten Sliphers, dass die Spektren verschiedener Galaxien nicht etwa zu gleichen Teilen ins Rote und Blaue verschoben sind, sondern dass es erheblich mehr rotverschobene Spektren gibt. Interpretiert man die Frequenzverschiebung als Dopplereffekt, so lässt sich ableiten, dass sich fast alle beobachteten Galaxien von uns entfernen. Hubble war auch der erste, der einen direkt proportionalen Zusammenhang zwischen Rotverschiebung und Entfernung der Galaxien aufstellte, was bedeuten würde, dass sich diese fernen "Weltinseln" umso schneller von uns fortbewegen, je weiter sie entfernt sind. Hubble selbst benutzte den Ausdruck „scheinbare Geschwindigkeit“, da er zurückhaltend war in der physikalischen Interpretation der Beobachtungen.

Die Größe, die diese Expansion beschreibt, wird ihm zu Ehren die Hubble-Konstante "H" genannt. Mit ihrer Hilfe lässt sich das Alter des Weltraums abschätzen.

Hubble hat auch die Hubble-Sequenz entwickelt, ein morphologisches Ordnungsschema für Galaxien. Am 30. August 1935 entdeckte er den Asteroiden Cincinnati.

Am 28. September 1953 starb Hubble mit 63 Jahren an einem Schlaganfall, während er die Vorbereitungen für mehrere Beobachtungsnächte auf dem Palomar-Observatorium traf.

Wegen der Bedeutung von Hubbles Forschung auch für die moderne Kosmologie sei im Folgenden eine Originalarbeit in der Übersetzung von Jürgen Hamels "Geschichte der Astronomie" (Magnus-Verlag 2004, S. 325–327) unter Ergänzung von Links und kleineren Ergänzungen in Klammern wiedergegeben.

"Die Erforschung des beobachtbaren Raums als Ganzes hat zu zwei Ergebnissen von besonderer Bedeutung geführt: Das eine ist die Homogenität des Raumes – die gleichförmige Verteilung der Nebel im Großen –, das andere die Geschwindigkeits-Entfernungs-Beziehung."

"Die Verteilung der Nebel im Kleinen ist sehr ungleichmäßig. Man findet einzelne Nebel, Nebelpaare, Nebelgruppen verschiedener Größe und auch Nebelhaufen. Das galaktische System ist Hauptteil eines dreifachen Nebels, von welchem die Magellanwolken die anderen Bestandteile bilden. Das Dreiersystem bildet mit einigen anderen Nebeln die „lokale Gruppe“ […]. Deren Mitglieder lieferten uns die ersten Entfernungen, und das Cepheiden-Entfernungskriterium ist bis heute nur auf diese Gruppe anwendbar."

"Vergleicht man [hingegen] große Himmelsbereiche oder Raumbereiche miteinander, so mitteln sich die kleinen Unregelmäßigkeiten heraus und es bleibt die sehr gleichmäßige Verteilung im Großen. Die Verteilung über den Himmel erhält man, indem man die Nebelzahlen innerhalb einer ausgewählten, in gleichmäßigen Abständen über den ganzen Himmel verstreuten Bezirken, bis zu einer bestimmten Grenzgröße der Mittel miteinander vergleicht."

"Die wahre Verteilung bleibt uns durch örtliche Verdunklung teilweise verborgen. Im [Himmels]gebiet der Milchstraße beobachten wir keine Nebel, und nur wenige an ihrem Rande [… was zusammenhängt mit dem] Vorhandensein großer Staub- und Gaswolken, die über das ganze Sternsystem, besonders über die galaktische Ebene, verstreut sind. Diese Wolken verbergen uns die entfernteren Sterne und Nebel […]"

"Die Verteilung in der Tiefe, d. h. die Nebelzahlen zwischen zwei aufeinanderfolgenden Entfernungsstufen, findet man durch Vergleich der Nebelzahlen mit scheinbaren Helligkeiten zwischen zwei entsprechenden aufeinanderfolgenden Helligkeitsstufen. Es handelt sich dabei um den Vergleich zwischen den Nebelzahlen und dem Raumteil, den die Nebel erfüllen. Da diese Zahlen im gleichen Verhältnis wachsen wie die Raumgrößen […], so müssen die Nebel gleichmäßig verteilt sein […]"

"[…] Die Nebel zeigen im allgemeinen sonnenähnliche Absorptionsspektren, sodass man annehmen kann, dass der Sonnentypus unter den Nebelsternen vorherrscht. Die Spektren sind notwendigerweise kurz, weil das Licht zu schwach ist, als dass man es zu einem langen Spektrum auseinanderziehen könnte. Die H- und K-Linie des Kalziums kann man aber noch trennen. Auch erkennt man die G-Bande des Eisens und einige Wasserstofflinien […]"

"Nebelspektren fallen durch die seltsame Tatsache auf, dass ihre Linien nicht die Lage [bezüglich der Wellenlänge] zeigen, wie man sie bei nahen Lichtquellen beobachtet. Wie man durch geeignete Vergleichsspektren beobachtet hat, sind sie ins Rote verschoben. Die Verschiebungen, die man als Rotverschiebung bezeichnet, nehmen im Durchschnitt mit abnehmender scheinbarer Helligkeit zu. Da die scheinbare Helligkeit [annähernd] die Entfernung misst, so folgt, dass die Rotverschiebungen mit der Entfernung zunimmt. Eingehendere Untersuchungen zeigen, dass die Beziehung linear ist. […] Bei dieser Auffassung nimmt man also an, dass sich die Nebel von unserem Raumteil mit Geschwindigkeiten entfernen, die ihrer Entfernung proportional sind […]"




</doc>
<doc id="1273" url="https://de.wikipedia.org/wiki?curid=1273" title="Echelon">
Echelon

Echelon ist ein weltweites Spionagenetz, das von Nachrichtendiensten der USA, Großbritanniens, Australiens, Neuseelands und Kanadas betrieben wird. Das System dient zum Abhören bzw. zur Überwachung von über Satellit geleiteten privaten und geschäftlichen Telefongesprächen, Faxverbindungen und Internet-Daten. Die Auswertung der gewonnenen Daten wird vollautomatisch durch Rechenzentren vorgenommen. Die Existenz des Systems gilt seit einer Untersuchung des europäischen Parlaments von 2001 als gesichert. Über den genauen Umfang und die Art der Abhörmaßnahmen gibt es wegen der Geheimhaltung seitens der Betreiber keine zuverlässigen Angaben. 

Die Organisationen


sind daran beteiligt. Seit den 1970er Jahren gab es Gerüchte über die Existenz eines geheimen Spionagesystems dieser Organisationen. Bestätigt ist die Existenz des Netzwerks spätestens seit der Veröffentlichung des Europäischen Parlaments vom 5. September 2001.

Vermutlich war die antike Echelon-Schlachtordnung der Namensgeber für dieses Spionagenetz. In der Antike entstand die "Echelonformation" (siehe Schiefe Schlachtordnung). Echelonformation ist heute im englischen Sprachraum eine gestaffelte Kampfanordnung. Auch die Flugformation von Zugvögeln wird mitunter so bezeichnet. Echelon kommt möglicherweise auch von französisch „échelle“ (Leiter, Tonleiter, Skala, Maßstab u. w.) bzw. „échelon“ (Leitersprosse, Stufe, aber auch die militärische Staffel). Ein Échelon ist auch ein optisches Gitter (Beugungsgitter).

Der Begriff bezeichnet dabei eigentlich die Software, die in den entsprechenden Abhörstationen die aufgefangene SIGINT nach bestimmten Stichworten durchsucht.

Die Ziele bewertet das Europäische Parlament wie folgt:

Pressemeldungen mutmaßten zuvor, dass Echelon zunächst nur dazu gedacht sei, die militärische und diplomatische Kommunikation der Sowjetunion und ihrer Verbündeten abzuhören. Heute soll das System zur Suche nach terroristischen Verschwörungen, Aufdeckungen im Bereich Drogenhandel und als politischer und diplomatischer Nachrichtendienst benutzt werden. Seit Ende des Kalten Krieges soll das System außerdem der Wirtschaftsspionage dienen. Diese Vorwürfe wurden durch das Europäische Parlament nicht bestätigt, wenn auch einzelne Ausschussmitglieder in dieser Hinsicht Bedenken äußerten.

Anderen Quellen zufolge dient Echelon der Umgehung nationaler Gesetze. Amerikanischen Geheimdiensten ist es verboten, die Telefongespräche amerikanischer Staatsbürger abzuhören. Gleiches gilt auch in Großbritannien. Indem nun der britische Geheimdienst Amerikaner und der amerikanische Geheimdienst britische Telefongespräche abhört, wird dieses Verbot umgangen.

Alle Mitglieder des Echelon-Systems sind Teil der nachrichtendienstlichen Allianz UKUSA, deren Wurzeln bis zum Zweiten Weltkrieg zurückreichen. Die Mitgliedsstaaten der Allianz stellen Abhörstationen und Weltraumsatelliten bereit, um Satelliten-, Mikrowellen- und teilweise auch Mobilfunk-Kommunikation abzuhören. Es gibt laut Untersuchungsbericht des EU-Parlaments keine Hinweise darauf, dass die Technologie auch das großflächige Abhören drahtgestützter Kommunikation (d. h. Telefon, Internet-Backbones innerhalb Europas, Fax usw.) ermöglicht.
Die Erfassung der Signale erfolgt wahrscheinlich hauptsächlich durch in Radarkuppeln aufgestellte Antennen, eine meist kugelförmige Hülle, die die Inneneinrichtung in erster Linie vor äußeren mechanischen Einflüssen wie Wind oder Regen schützt. Es wird vermutet, dass die eingefangenen Signale, hauptsächlich aus der Satellitenkommunikation, teilweise durch die National Security Agency (NSA) ausgewertet werden, die die hierzu erforderlichen Erkennungssysteme besitzt.

Der Öffentlichkeit bekannt gemacht wurde das Spionagesystem erstmals 1976 durch Winslow Peck. Am 5. Juli 2000 beschloss das Europäische Parlament, einen nichtständigen Ausschuss über das Abhörsystem Echelon einzusetzen, 2001 wurde ein 192 Seiten langer Bericht veröffentlicht, in dem die Existenz bestätigt und die Bedeutung und Auswirkungen dargelegt wurden.

Großanlagen zur Überwachung von Satellitenkommunikation sind wegen der aufwändigen Empfangstechnik schwierig zu verstecken. Die Standorte dieser Anlagen sind daher bekannt. Über die Techniken zur Überwachung der kabelgebundenen und mikrowellenübermittelten Kommunikation ist hingegen wenig bekannt.

Echelon betreibt fünf Großstationen zur Überwachung des Verkehrs via Intelsat. In Europa befindet sich eine in Morwenstow (Cornwall) unter Aufsicht der GCHQ für die Überwachung des Atlantiks und des Indischen Ozeans. Zwei Stationen werden von der NSA betrieben, eine in Sugar Grove (West Virginia) und auf dem Armeestützpunkt Yakima im Bundesstaat Washington. Eine neuseeländische Station in Waihopai und eine australische in Geraldton vervollständigen die Kette.

Die Überwachung der nicht-Intelsat-gestützten Kommunikation erfolgt bzw. erfolgte von mindestens fünf Standorten aus, nämlich aus Bad Aibling (Bayern, 2004 abgebaut), Menwith Hill (Yorkshire), Shoal Bay (Nordaustralien), Leitrim (Kanada) und Misawa (Nordjapan).

Im Verdacht stehen folgende Standorte:

Nach Beendigung des Kalten Krieges im Jahr 1990 fiel der Hauptfeind, der Ostblock, als potentieller Gegner weg. Die neue geheimdienstliche Priorität, die Wirtschaftsspionage, wurde von George Bush sen. durch die Nationale Sicherheitsdirektive 67 – herausgegeben vom Weißen Haus am 20. März 1992 – festgelegt. Die freigewordenen Kapazitäten sollen die Echelon-Beteiligten genutzt haben, um die eigenen Verbündeten auf dem Gebiet der Wirtschaft auszuspionieren.

Medien berichteten seit Ende der 1990er Jahre, der US-Geheimdienst NSA hätte 1994 das deutsche Unternehmen Enercon mit Hilfe von Echelon abgehört. Die so gewonnenen Daten seien dem amerikanischen Mitbewerber "Kenetech Windpower Inc." übermittelt worden. Anderen Berichten zufolge hatte das amerikanische Unternehmen drei Jahre vor der angeblichen Abhöraktion die in Frage stehenden Eigenschaften patentieren lassen, was dieser Theorie widerspricht.

Die Wirtschaftsspionage wird auch durch die Aussage des ehemaligen CIA-Chefs James Woolsey im Wall Street Journal vom 17. März 2000 bestätigt. Woolsey bemühte sich allerdings darzulegen, die USA hätten lediglich Informationen über Bestechungsversuche europäischer Unternehmen im Ausland gesucht, denn „die meiste europäische Technologie lohnt den Diebstahl einfach nicht“. Airbus soll einen milliardenschweren Vertrag mit Saudi-Arabien verloren haben, da die NSA vermutlich durch Echelon herausgefunden hatte, dass Airbus die saudischen Geschäftsleute bei der Auftragsvergabe bestochen hatte.

Die sich in Bad Aibling (Bayern) befindliche Echelonbasis Bad Aibling Station konnte bis 2004 große Bereiche Europas abdecken; abhören konnte man aber wie in allen Fällen nur Kommunikation, die über Richtfunkstrecken oder Satelliten geleitet wurde; kabelgebundene Kommunikation wie z. B. die Internet-Backbones können mit dieser Technik nicht abgehört werden. In dem zitierten EU-Report wurde festgestellt, dass diese Anlage nach dem Ende des Kalten Krieges mehrheitlich der Wirtschaftsspionage diente, und es wurde vorgeschlagen, diese zu schließen. 

Bedingt durch die Terroranschläge des 11. September 2001 wurde dieser Beschluss erst verspätet im Jahre 2004 umgesetzt. In seinem Bericht an das EU-Parlament am 5. September 2001 stellte der „Berichterstatter des nicht ständigen EU-Untersuchungsausschusses zu Echelon“ Gerhard Schmid nochmals fest, dass innereuropäische Kommunikation kaum betroffen ist, sondern hauptsächlich transatlantische Verbindungen über Satellit. Als Ersatz für die Anlage in Bad Aibling stand von 2004 bis 2008 am Rand des ehemaligen August-Euler-Flugplatzes (von den USA auch als Dagger Complex bezeichnet) in Darmstadt ein System mit fünf Radomen, das laut einigen Quellen ebenfalls Abhörzwecken gedient haben soll. Die Anlage wurde im Frühjahr 2004 fertiggestellt; im Sommer 2008 wurden die Radome wieder demontiert. 

Der BND betreibt mit der genannten Fernmeldeverkehrstelle des Bundesnachrichtendiensts die Bad Aibling Abhörstation weiter. (Siehe auch: Zusammenarbeit von Bundesnachrichtendienst und NSA). In Bad Aibling bestehen ein Verbindungsbüro zum US-Geheimdienst NSA ("SUSLAG", Special US Liaison Activity Germany) und zwei Übergabepunkte, die "Joint SIGINT Activity" (JSA) und das "Joint Analysis Center" (JAC).

Am 23. April 2015 berichteten Medien erneut über das Ausmaß der Kooperation zwischen BND und NSA in Bad Aibling. Aufgrund eines Beweisantrags der Bundestagsfraktionen wurde untersucht, wie viele der 800.000 Selektoren (IP-Adressen, E-Mail-Adressen, Telefonnummern, Geokoordinaten, MAC-Adressen) gegen deutsche und europäische Interessen gerichtet waren. Diese Selektoren bekam der BND von der NSA über den Verlauf von 10 Jahren automatisch zugewiesen; mehrmals am Tag hat sich ein BND-Server mit einem NSA-Server verbunden und neue Selektoren heruntergeladen. Die gewonnenen Erkenntnisse wurden dann an die NSA weitergeleitet.

Die BND-Einrichtungen seien im bayerischen Bad Aibling genutzt worden, um hochrangige Beamte des französischen Außenministeriums, des Präsidialstabs und der EU-Kommission auszuspähen. Unternehmen, wie z. B. Airbus seien vor allem betroffen, weil die USA angeblich nach Hinweisen auf illegale Exportgeschäfte gesucht habe. Auch die Zahl der von den USA seit Beginn der Kooperation angelieferten Selektoren wurde bekannt: in den Jahren 2002–2013 waren es 690.000 Telefonnummern und 7,8 Millionen IP-Suchbegriffe, berichtet der Rechercheverbund von Süddeutscher Zeitung, NDR und WDR am 30. April.

Schon 2013, nach Veröffentlichung der Snowden-Dokumente, stellte der BND eine Liste aller möglicherweise problematischen Selektoren zusammen. Sie umfasste 2.000 eingesetzte und nicht aussortierte, rechtswidrige Selektoren. Im Zuge der neuen Untersuchungen von März bis Mai 2015 wurden weitere 459.000 solcher Selektoren gefunden, es handelt sich hier z. B. um europäische Politiker und Unternehmen. Davon wurden nur 400 aussortiert. Derzeit (Stand: Mai 2015) ist unklar, wie viele dieser Selektoren vom BND abgelehnt oder ausgeführt wurden, ob es noch mehr gibt und um welche es sich genau handelt. "Der Spiegel" berichtete am 15. Mai 2015, dass über die Hälfte der 40.000 Selektoren, die im März 2015 gefunden worden sind, auch aktiv waren, d. h. tatsächlich zur Ausforschung von Behörden, Unternehmen und anderen Zielen in Europa verwendet worden sind.

Die heutige Bedeutung des Netzwerkes ist aufgrund mangelnder zugänglicher Informationen unklar.




</doc>
<doc id="1275" url="https://de.wikipedia.org/wiki?curid=1275" title="Edel-Tanne">
Edel-Tanne

Die Edel-Tanne ("Abies procera", Syn.: "Abies nobilis"), standardsprachlich Edeltanne, auch Pazifische Edel-Tanne und Silbertanne genannt, ist eine Pflanzenart aus der Gattung der Tannen ("Abies") in der Familie der Kieferngewächse (Pinaceae). Die Schreibweise mit Bindestrich ist in der botanischen Fachliteratur gebräuchlich. Umgangssprachlich wird auch die in Europa heimische Weiß-Tanne ("Abies alba" Mill.) als „Edeltanne“ oder „Silbertanne“ bezeichnet. Nachdem im Frühjahr 1942 durch das Land-Lease System die Produktion von Sitka-Fichtenholz nicht die Nachfrage der US-amerikanischen und britischen Flugzeugindustrie im Zweiten Weltkrieg decken konnte, fanden sich im Holz der Edel-Tanne sowie der Westlichen Hemlock die einzigen Baumarten, die eine Neukonstruktion im Flugzeugdesign nicht erforderten. Damit gewann die Edel-Tanne im Sommer 1942 als Substitut der Sitka-Fichte große wirtschaftliche und kriegstechnische Bedeutung, die 1943 zur maximalen Ressourcen-Nutzung der Edel-Tannen in Bergwäldern des Pazifischen Nordwestens führte.

Die Edel-Tanne ist ein immergrüner Baum, der die größten Wuchshöhen unter den Tannen erreicht, es werden Wuchshöhen über 80 Meter und Stammdurchmesser (BHD) über 2 Meter erreicht. Sie hat auffallend gerade, säulenförmige Stämme. Die Baumkrone ist symmetrisch kegelförmig. Sie ist nicht so dicht wie bei anderen Tannenarten und erscheint deshalb häufig durchsichtig. Im Gipfel sterben häufig Leittriebe ab; im Alter überragen die Seitenäste den Gipfeltrieb und es bildet sich die sogenannte „Storchennest-Krone“. Die Zweige stehen meist im rechten Winkel vom Stamm ab, können aber im unteren Teil der Krone auch hängen. Die schlanken Zweige sind rötlich-braun gefärbt und behaart. Die Zweigoberseite ist meistens durch die dicht anliegenden Nadeln nicht zu erkennen. Die Edel-Tanne kann bis zu 800 Jahre alt werden und erreicht damit das höchste Alter aller Tannenarten.

Die runden Knospen sind sehr klein und mit rotbraunen Schuppen versehen. Sie werden häufig von den Nadeln verdeckt.
Die blaugrünen schimmernden, dichtstehenden Nadeln haben eine Lebensdauer bis zwölf Jahre. Die, im Querschnitt viereckigen, Nadeln sind 25 bis 35 Millimeter lang und oberseits rinnig vertieft. Sowohl auf der Ober- wie auch auf der Unterseite befinden sich Stomabänder. Nadeln an lichtexponierten Stellen sind meist blaugrün gefärbt und stehen aufwärts gerichtet spiralig um den Zweig. Schattennadeln sind meist dunkelgrün gefärbt und stehen gescheitelt an den Zweigunterseiten. Die Nadeln liegen im ersten Viertel dem Zweig an und krümmen sich auf.

Die Edel-Tanne ist einhäusig getrenntgeschlechtig (monözisch). Sie wird mit 20 bis 30 Jahren mannbar, die volle Samenproduktion setzt allerdings erst mit 50 bis 60 Jahren ein. Die männlichen Blütenzapfen sind dunkelrot gefärbt und sitzen in Gruppen von bis zu 30 Blütenzapfen auf der Zweigunterseite. Die unscheinbaren weiblichen Blütenzapfen sind gelblich gefärbt mit einem schwach rötlichen Ton und stehen meist einzeln, aber auch zu zweit, selten zu fünft auf der Oberseite vorjähriger Triebe.

Die Zapfen weisen eine Länge von 10 bis 24 Zentimeter und einen Durchmesser von 5 bis 7 Zentimeter auf. Sie sind damit die größten Zapfen aller bekannten Tannenarten. Schon an jungen Bäumen von nur 2 bis 3 Meter Wuchshöhe werden Zapfen angesetzt. Die reifen Zapfen sind blass purpurbraun, wirken aber blassgrün, da sie überwiegend durch die grünlichen Deckschuppen überdeckt werden. Die Samenschuppen sind etwa 2,5 Zentimeter × 3 Zentimeter groß. Der rötlich-braune Same weist eine Größe von 13 Millimeter × 6 Millimeter auf und besitzt einen hellbraunen bis strohfarbenen Flügel, der nur wenig länger ist als das Samenkorn. Das Tausendkorngewicht beträgt rund 30 Gramm und die Keimfähigkeit liegt bei 30 bis 70 %. Die Keimlinge besitzen meist fünf bis sechs (vier bis sieben) Keimblätter (Kotyledonen).

Die Rinde von jungen Bäumen ist glatt und gräulich bis leicht rötlich gefärbt. Die Borke der älteren Bäume ist grau, teilweise mit einem lilafarbenen Ton, und bricht in rechteckige Platten auf. Sie ist mit 2 bis 5 Zentimeter relativ dünn weshalb die Bäume durch Waldbrände gefährdet sind. Die Rinde der Zweige ist rötlichbraun und feinbehaart.

Es ist nur sehr wenig über die Wurzeltracht der Edel-Tanne bekannt. Man weiß, dass sie keine Pfahlwurzel ausbildet und deshalb auf Windwurf anfällig ist.

Das weiche, weißliche Holz, elastische Holz der Edel-Tanne gilt als das qualitativ hochwertigste aller nordamerikanischen Tannenarten und insgesamt auch innerhalb der Gattung. Es besitzt eine sehr gutes Steifigkeits-/Gewichtsverhältnis das mit den besten Weichhölzern wie denen von Sitka-Fichte sowie der in ihrem natürlichen Areal sympatrisch vorkommendenWestamerikanischen Hemlocktanne konkurrieren kann. Von der Qualität wird es unter den Koniferen-Hölzern nur von schwereren Holz der Douglasie übertroffen.


Die Edel-Tanne kommt im humiden pazifischen Nordwesten der USA vor. Ihr Verbreitungsgebiet reicht von Washington über Oregon bis nach Nordwestkalifornien. Sie wächst im Küstengebirge und in der Kaskadenkette in Höhenlagen zwischen 650 und 1.680 Meter. Sie kommt meistens in Mischwäldern mit der Douglasie ("Pseudotsuga menziesii"), der Purpur-Tanne ("Abies amabilis") und der Westamerikanischen Hemlocktanne ("Tsuga heterophylla") vor, es werden aber auch Reinbestände gebildet.

Die Edel-Tanne bevorzugt kontinentales oder gemäßigtes Klima mit kühlen Sommern und hohen Niederschlägen (2000 bis 2500 Millimeter/Jahr). An den Boden stellt sie geringe Ansprüche, meidet aber Kalk. Gegen Winterkälte, Spätfröste, Schneedruck und Wind ist sie widerstandsfähig, verträgt aber als Lichtbaumart nur sehr wenig Schatten. Temperaturen von −20 °C und weniger übersteht sie problemlos. Im Jugendstadium reagiert sie allerdings auf Frosttrocknis sehr empfindlich. Sie meidet sehr trockene und staunasse Böden. Der pH-Wert sollte nicht hoch sein, optimal ist ein Wert von 5,5.

Im Unterschied zu den weiteren 8 Tannen der Amerikanischen Westküste ist "Abies procera" ähnlich wie die Douglasie eine auf Licht angewiesene Pionierpflanze. Ihre großen, wenig zahlreichen Zapfen, tragen demnach auch besonders große Samen, die einen Sämling bis zu einem Jahr mit Nährstoffen versorgen können, bis dieser sich am Standort etabliert. Ein Vorteil gegenüber der Douglasie ergibt sich an schneereichen Lagen, wo die Sämlinge unter dem auf der winterlichen Schneedecke ansammelnden pflanzlichen Detritus verschüttet werden können. Während Douglasien sich auf solchen Standorten schlecht verjüngen, leiden Sämlinge und Jungbäume der Edel-Tanne wenig. Zudem sind ihre großen und Schweren Samen an schneereiches Klima viel besser angepasst als die kleinen Samen der Douglasie - die Samen werden über Schnee weit transportiert. Nach Ausbruch des Mount Saint Hellens fanden sich am vom Vulkan verwüsteten Hängen bald Edel-Tannen ein - bis zu 5 km vom nächsten erwachsenen Baum entfernt. Auch nach Feuer regenerieren Edel-Tannen schnell. Sie werden daher heute oft auf Standorten die für die Douglasie ungeeignet sind gepflanzt.

Die Edel-Tanne ist im Urwald-Optimalstadium von bis zu 300 Jahre alten nemoralen Berg-Wäldern des Pazifiks eine dominante Klimaxart. Nach 300 Jahren beginnt ihr rapider Verfall. Während des Urwald-Zerfallstadiums sinkt ihr Prozentsatz. In bis zu 400 Jahre alten Beständen zeigen einzelne überlebende Edel-Tannen stark alterungsbedingte Zerfallsmerkmale. 

In ihrem natürlichen Verbreitungsgebiet wird die Edel-Tanne weder durch Schadinsekten noch durch Schadpilze ernsthaft gefährdet. Sämlinge leiden manchmal unter der Grauschimmelfäule Botrytis cinerea sowie an dem Schneeschimmel Herpotrichia nigra. Rüsselkäfer fressen, insbesondere in neu angelegten Kulturen, an der Rinde. Alte Bäume werden häufig von Borkenkäfern befallen.
Befälle mit Melampsorella caryophyllacearum, dem Erreger des Tannenkrebses, verlaufen meist ohne nennenswerte Folgen.
Die Edel-Tanne erleidet häufig Rindenschäden durch Schwarzbären.
In sehr kalten Wintern kann Frosttrocknis auftreten.

Allgemein wird das Holz der Edel-Tanne mit vier weiteren Tannen der amerikanischen Pazifikküste (Prachttanne, Küsen-Tanne, Felsengebirgs-Tanne und Purpur-Tanne) sowie der Westlichen Hemlock zumeist als Hem-Fir vermarktet, der geläufigen US-Abkürzung der Sägeholz-Produzenten "Western Forest Products" und "Western Woods Product Association". Dabei ist neben der Eignung als Konstruktionsholz Edel-Tanne ein bevorzugtes Material für Leitersprossen. Hierbei wird Holz das dichte Jahresringe aufweist und somit höhere Festigkeit aufweist bevorzugt.
Das Weich-Holz der Edel-Tanne ist leicht und dabei trotzdem von hoher Steifigkeit, leicht zu bearbeiten und eignet sich allgemein sehr gut als Bau- und Konstruktionsholz. Es ist vor allem in Japan sehr begehrt. 

Im Zweiten Weltkrieg spielte es eine Rolle im Flugzeugbau, wobei ursprünglich ausschließlich Sitka-Fichte in Frage kam, und aufgrund ungenügender Verfügbarkeit die Suche nach geeigneten Substitute erforderte. Nachdem die Vereinigten Staaten Vier Arten zur Substitution zugelassen hatten, wurden als Resultat des „Forest Products Research Laboratory“ nur noch Edel-Tanne und Westliche Hemlock empfohlen. Im Land-Lease System wurden somit Edel-Tannenholz Produkte an das Vereinigte Königreich sowie Kanada geliefert und dort besonders im Bau der de Havilland Mosquito eingesetzt.

Die Edel-Tanne wird weltweit als Ziergehölz und als Christbaum angebaut. Weiters finden ihre Zweige Verwendung als Schmuckreisig.

In Deutschland machten Edel-Tannen 2012 etwa 5 Prozent der 29,2 Millionen verkauften Weihnachtsbäume aus. Damit steht diese in den Absatzzahlen hinter der Nordmann-Tanne, für die ein Marktanteil von 75 Prozent angegeben wird, und der Blau-Fichte mit 20 Prozent Marktanteil insgesamt an dritter Stelle der beliebtesten Weihnachtsbäume in Deutschland. In den Angaben des Landesverbandes Sachsen in der Schutzgemeinschaft Deutscher Wald entscheiden sich deutsche Haushalte zu 3 Prozent für die Edel-Tanne als Weihnachtsbaum, womit sie an vierter Stelle stehen würde. In anderen europäischen Ländern ist die Edel-Tanne teilweise beliebter als die Nordmann-Tanne. So stellt sie etwa in Irland den beliebtesten Weihnachtsbaum dar.

In den USA war die Edel-Tanne bis in die 1960er Jahre praktisch nur in den Staaten der Pazifischen Nordwestküste, insbesondere in Oregon, als Weihnachtsbaum von Bedeutung. Danach wurde diese in den westlichen USA immer populärer und stellt heute die wichtigste Weihnachtsbaumart in diesen Gebiet dar. Durch ihre gute Fähigkeit, die Feuchtigkeit der Nadeln länger als andere Arten aufrechtzuerhalten, haben Edel-Tannen und Fraser-Tanne in den USA durch diese als wesentlich für die Posternte-Qualität erachtete Eigenschaft eine dominierende Position bei den Produzenten wie den Konsumenten erlangt. Die Edel-Tanne ist in ihrer Fähigkeit, Prozesse der Nadeltrocknis aufzuhalten, auch der Nordmann-Tanne überlegen.

Aufgrund der langen Nadelhaltbarkeit und der silbergraublauen Farbe der Blätter ist sie für die Produktion von Schmuckreisig eine der interessantesten Baumarten. Die Empfindlichkeit der Edel-Tanne gegenüber Kahlfrösten sowie ihre Neigung zu meist unregelmäßigen Wuchs sorgt dafür das sie am europäischen Markt nur eine eher untergeordnete Rolle spielt. Ihre hohen Ansprüchen an Standort- und Bodenverhältnisse (wenig verdichtete Böden mit niedrigen pH-Werten, eine hohe Luftfeuchtigkeit sowie ausgeglichene Temperaturen) versucht man in der Weihnachtsbaum-Kultur mittels geeigneter Saatgutwahl entgegenzuwirken.




</doc>
<doc id="1276" url="https://de.wikipedia.org/wiki?curid=1276" title="Enrico Fermi">
Enrico Fermi

Enrico Fermi (* 29. September 1901 in Rom, Italien; † 28. November 1954 in Chicago, Illinois) war einer der bedeutendsten Kernphysiker des 20. Jahrhunderts. 1938 erhielt er den Nobelpreis für Physik.

Bereits im Alter von 17 Jahren begann er ein Physikstudium an der Scuola Normale Superiore in Pisa, das er 1922 "magna cum laude" mit einer experimentellen Arbeit über Röntgenstreuung an Kristallen mit dem Laurea-Abschluss beendete. 1923 hatte Fermi dank eines Stipendiums einen mehrmonatigen Forschungsaufenthalt in Göttingen bei Max Born. Göttingen war damals das führende Zentrum der theoretischen Physik, hier entstanden viele wesentliche Arbeiten für die Quantenmechanik. 1924 arbeitete er mehrere Monate in den Niederlanden bei Paul Ehrenfest, ebenfalls ein Mitbegründer der Quantenmechanik. Danach wurde Fermi zunächst als Professor für Mathematik nach Florenz berufen (Januar 1925). Zwei Jahre später (1926) ging er als Professor für theoretische Physik an die Universität Rom (La Sapienza) auf den von Orso Mario Corbino neu gegründeten Lehrstuhl für theoretische Physik, den er bis 1938 innehatte. Fermi war seit 1924 Mitglied des Freimaurerbundes. Seit 1928 war er mit Laura Capon (1907–1977) verheiratet und hatte mit ihr zwei Kinder: Nella (1931–1995) und Giulio (1936–1997). In der Zeit von 1926 bis 1932 entstanden wichtige Arbeiten von Fermi zur Quantenmechanik mit Anwendungen zum Beispiel in der Festkörperphysik und Quantenstatistik (Fermi-Dirac-Statistik für Fermionen, Fermis Goldene Regel, Fermifläche, Fermi-Resonanz, Thomas-Fermi Theorie des Atoms). In Rom entstand um Fermi eine sehr aktive Gruppe theoretischer und experimenteller Physiker. Ihr gehörten Gian-Carlo Wick, Ugo Fano, Giovanni Gentile, Giulio Racah, Ettore Majorana sowie die Experimentatoren Franco Rasetti, Giuseppe Cocconi, Emilio Segrè, Edoardo Amaldi und Bruno Pontecorvo an.

Angeregt durch die Entdeckung des Neutrons durch James Chadwick im Jahr 1932 sowie durch den Nachweis von Kernumwandlungen nach Bestrahlung mit Alphateilchen durch Irène und Frédéric Joliot-Curie wandte sich Fermi 1934 der Experimentalphysik zu. Seine bahnbrechende Entdeckung war, dass Kernumwandlungsprozesse durch Neutronenstrahlung wesentlich effektiver ablaufen. Eine weitere Verbesserung der Ausbeute erhält man, wenn die Neutronen stark abgebremst werden (thermische Neutronen). 1934 veröffentlichte Fermi seine Theorie des Beta-Zerfalls („Fermi-Wechselwirkung“). Schon 1933 hatte er die Bezeichnung Neutrino für eines der am Beta-Zerfall beteiligten Teilchen geprägt, dessen Existenz drei Jahre zuvor von Wolfgang Pauli postuliert worden war.

Durch Neutronenbestrahlung des damals schwersten bekannten Elementes Uran erzielten Fermi und seine Mitarbeiter ebenfalls Veränderungen im Ausgangsmaterial (anderes chemisches Verhalten, geänderte Halbwertszeiten der austretenden Strahlung) und interpretierten diese irrtümlich als Kernumwandlung zu Transuranen ("Nature"-Artikel 1934). Ida Noddack-Tacke kritisierte das und wies schon auf die Möglichkeit einer Kernspaltung hin, was dann vier Jahre später Otto Hahn und Fritz Straßmann mittels chemisch-analytischer Techniken zeigten; die theoretischen Grundlagen wurden von Lise Meitner und Otto Frisch erarbeitet (siehe Geschichte der Kernspaltung). Das erste Transuran konnte erst 1942 nachgewiesen werden, allerdings nach einer gänzlich anderen Synthesevorschrift. 1938 erhielt Fermi für seine Arbeiten den Nobelpreis der Physik (laut offizieller Begründung für die Identifizierung neuer radioaktiver Elemente produziert nach Bestrahlung mit Neutronen und seine Entdeckung von Kernreaktionen, die durch langsame Neutronen bewirkt werden), obschon seine Interpretation des Neutronenexperiments (Erzeugung von Transuranen) nach heutigem Kenntnisstand falsch war. Im selben Jahr emigrierte Fermi aufgrund der 1938 erlassenen antisemitischen Gesetze des Mussolini-Regimes, die seine jüdische Frau Laura, seine beiden Kinder und einige seiner Mitarbeiter betrafen, mit seiner Familie in die USA.

Anfang der 1940er-Jahre arbeitete Fermi mit Isidor Isaac Rabi und Polykarp Kusch an der Columbia-Universität in New York.

Ihm gelang am 2. Dezember 1942 um 15:25 Uhr an der University of Chicago mit dem Kernreaktor Chicago Pile No. 1 erstmals eine kritische Kernspaltungs-Kettenreaktion, eine Leistung, die auf der theoretischen Vorarbeit von Leó Szilárd fußte.

Im April 1943 schlug Fermi Robert Oppenheimer die Möglichkeit vor, mittels der radioaktiven Nebenprodukte aus der Anreicherung die deutsche Lebensmittelversorgung zu verseuchen. Hintergrund war Angst davor, dass das deutsche Atombombenprojekt schon in einem fortgeschrittenen Stadium wäre und Fermi war auch zum damaligen Zeitpunkt skeptisch, ob eine Atombombe schnell genug entwickelt werden konnte. Oppenheimer besprach den „vielversprechenden“ Vorschlag mit Edward Teller, welcher die Verwendung von Strontium-90 vorschlug. Auch James Bryant Conant und Leslie R. Groves wurden unterrichtet, Oppenheimer wollte aber nur dann den Plan in Angriff nehmen, falls mit der Waffe genug Nahrungsmittel verseucht werden könnten, um eine halbe Million Menschen zu töten.

Im Sommer 1944 zog Fermi mit seiner Familie nach Los Alamos (New Mexico) in das geheime Atom-Forschungslabor der USA. Als Berater von Robert Oppenheimer spielte Fermi eine wichtige Rolle bei Entwicklung und Bau der ersten Atombomben. Nach dem Zweiten Weltkrieg beschäftigte sich Fermi wieder mit der Grundlagenforschung im Kernforschungszentrum an der Universität Chicago. Nach einer Europareise 1954 wurde bei Fermi Magenkrebs diagnostiziert. Er unterzog sich am 9. Oktober einer Operation und starb sieben Wochen später.

Fermi war für seine schnellen Abschätzungen und seine physikalische Intuition bekannt – er war ein Meister der „back of the envelope“-Rechnungen (die nicht mehr Platz als die Rückseite eines Briefkuverts benötigen). Sprichwörtlich sind auch die "Fermi questions" (Fermi-Probleme), wie etwa aus wenigen Daten die Anzahl der Klavierstimmer in einer Stadt wie Chicago abzuschätzen.

Nach ihm wurden das Elektronengas (auch "Fermigas") (vgl. hierzu Metallbindung), eine Gruppe von Elementarteilchen (Fermionen), das künstlich hergestellte chemische Element Fermium und ein Energieniveau in Vielteilchensystemen (Ferminiveau) benannt. Die Atomic Energy Commission der USA stiftete zu seinem Gedenken den z. Z. mit 375.000 US-Dollar dotierten "Enrico-Fermi-Preis". Das Fermi National Accelerator Laboratory bei Chicago ist nach ihm benannt und die regelmäßigen Kurse der "International School of Physics Enrico Fermi" der italienischen physikalischen Gesellschaft und deren Premio Enrico Fermi.

Bücher:

Einige Aufsätze:


Er war auswärtiges Mitglied der Royal Society (1950), der Royal Society of Edinburgh, Mitglied der National Academy of Sciences und der Leopoldina (1935). 

1936 wurde er Ehrendoktor der Universität Heidelberg, 1946 der Yale University und der Washington University, 1948 der Harvard University und 1952 der University of Rochester. 1953 war er Präsident der American Physical Society.

In Italien war er Mitglied der Accademia d’Italia.

Physikalische Konzepte:

Philosophische und methodische Konzepte:

Sonstiges:




</doc>

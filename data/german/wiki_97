<doc id="14573" url="https://de.wikipedia.org/wiki?curid=14573" title="Märchen">
Märchen

Märchen (Diminutiv zu mittelhochdeutsch "mære" = „Kunde, Bericht, Nachricht“) sind Prosatexte, die von wundersamen Begebenheiten erzählen. Märchen sind eine bedeutsame und sehr alte Textgattung in der mündlichen Überlieferung (Oralität) und treten in allen Kulturkreisen auf. Im Gegensatz zum mündlich überlieferten und anonymen Volksmärchen steht die Form des Kunstmärchens, dessen Autor bekannt ist. Im deutschsprachigen Raum wurde der Begriff "Märchen" insbesondere durch die Sammlung der Brüder Grimm geprägt.

Im Unterschied zur Sage und Legende sind Märchen frei erfunden und ihre Handlung ist weder zeitlich noch örtlich festgelegt. Allerdings ist die Abgrenzung vor allem zwischen mythologischer Sage und Märchen unscharf, beide Gattungen sind eng verwandt. Ein bekanntes Beispiel hierfür ist das Märchen "Dornröschen", das etwa von Friedrich Panzer als märchenhaft ‚entschärfte‘ Fassung der Brünnhilden-Sage aus dem Umkreis der Nibelungensage betrachtet wird. Dabei kann man die Waberlohe als zur Rosenhecke verniedlicht und die Nornen als zu Feen verharmlost ansehen.

Charakteristisch für Märchen ist unter anderem das Erscheinen phantastischer Elemente in Form von sprechenden und wie Menschen handelnden Tieren, von Zaubereien mit Hilfe von Hexen oder Zauberern, von Riesen und Zwergen, Geistern und Fabeltieren (Einhorn, Drache usw.); gleichzeitig tragen viele Märchen sozialrealistische oder sozialutopische Züge und sagen viel über die gesellschaftlichen Bedingungen, z. B. über Herrschaft und Knechtschaft, Armut und Hunger oder auch Familienstrukturen zur Zeit ihrer Entstehung, Umformung oder schriftlichen Fixierung aus. Nach der schriftlichen Fixierung der Volksmärchen setzte eine mediale Diversifikation ein (Bilder, Illustrationen, Übersetzungen, Nacherzählungen, Parodien, Dramatisierungen, Verfilmungen, Vertonungen usw. usf.), die nun an die Stelle der mündlichen Weitergabe trat. Insofern ist die ‚Rettung‘ der Märchen etwa durch die Brüder Grimm zwar einerseits begrüßenswert, aber andererseits setzt dies auch der mündlichen Weitergabe eines mono-medialen Texttyps ein jähes Ende.

"Märchen erzählen" ist als Immaterielles Kulturerbe in Deutschland anerkannt worden. Die Deutsche UNESCO-Kommission hat das Märchenerzählen im Dezember 2016 in das Bundesweite Verzeichnis des immateriellen Kulturerbes aufgenommen.

Die vergleichende Märchenforschung wurde von den Brüdern Grimm begründet und von Theodor Benfey später im 19. Jahrhundert weitergeführt. Antti Aarne kategorisierte 1910 die Märchen nach ihren wesentlichen Erzählinhalten; daraus entstand der heute noch in der internationalen Erzählforschung gebräuchliche Aarne-Thompson-Index. (Im Deutschen wird oft die Abkürzung AaTh verwendet, um Verwechslungen mit AT für Altes Testament zu vermeiden). Der russische Philologe Wladimir Jakowlewitsch Propp leistete 1928 mit seiner strukturalistischen Untersuchung über die Morphologie des Märchens einen wichtigen Beitrag zur literaturwissenschaftlichen Märchenforschung. Dem fügte Eleasar Meletinsky wichtige Einsichten zur Abgrenzung von Märchen und Mythos hinzu.

Allen Märchen liegt eine feste Handlungsstruktur zu Grunde, unabhängig von ihrem Inhalt. Diese Struktur erfüllt bestimmte Funktionen, die mit „archetypischen“ Akteuren verbunden sind (Held, Gegenspieler, Helfer usw.) und ist schon in der Antike aufzufinden.

In jüngerer Zeit werden Märchen auch mit unterschiedlichen theoretischen Ansätzen aus der Anthropologie, Oral History, der Psychologie (Analytische Psychologie, Psychoanalyse) und weiteren Einzeldisziplinen untersucht. Zu den wichtigsten psychologischen Märchenforschern zählen Marie-Louise von Franz und ihre Schülerin Hedwig von Beit. Von Franz publizierte zahlreiche Monografien zur psychologischen Märcheninterpretation. Ihre zentrale These ist in Anlehnung an C.G. Jung, dass in Märchen archetypische Inhalte des kollektiven Unbewussten mit ihrem prozesshaften Zusammenwirken in der menschlichen Psyche dargestellt werden. Somit enthielten Märchen in symbolischer Form psychologisches Orientierungswissen und sie hätten historisch oft auch die Funktion gehabt, die Einseitigkeiten kollektiv herrschender Werte und Ansichten zu kompensieren.

Gut und Böse werden im Märchen meist in Form gut oder böse erscheinender Figuren getrennt; Ausnahmen hierzu sind z. B. ambivalente Trickstergestalten oder Tiere, welche „gut mit den Guten und bös mit den Bösen“ umgehen. Inhaltlich steht meist ein Held oder eine Heldin im Mittelpunkt, der/die Auseinandersetzungen mit guten und bösen, natürlichen und übernatürlichen Kräften bestehen muss. Oft ist der Held eine vordergründig schwache Figur wie ein Kind oder der jüngste Sohn. Oft enden Märchen damit, dass das Gute extrem belohnt und das Böse extrem bestraft wird. Als Beispiele hierfür je ein Zitat aus zwei der bekanntesten Märchen der Brüder Grimm:

Märchen sind sehr alt und reichen weiter als alle anderen literarischen Formen in der Menschheitsgeschichte zurück; sie können nach verschiedenen Typen klassifiziert und versuchsweise verschiedenen Zeitaltern zugeordnet werden. Zu den ältesten Märchen gehören die Zaubermärchen. Sie weisen Erzählstrukturen auf, wie wir sie auch aus antiken griechischen und lateinischen Mythenerzählungen kennen (Götter- und Heldensagen), deren Erzählmaterial ebenfalls weit vor den Gebrauch der Schrift als Überlieferungsweg zurückreicht.

In den Schriftzeugnissen aller frühen Hochkulturen finden sich märchenhafte Züge, so bereits im Gilgamesch-Epos, das Motive enthält, die auch in Märchen vorkommen. Das alte Ägypten war reich an Zauber- und Tiergeschichten. Indien wird eine vermittelnde Rolle zwischen den Erzähltraditionen Asiens und des Vorderen Orients zugeschrieben. Allzu vereinfachende Thesen wie die von Benfey, die europäischen Märchen seien indischen Ursprungs, gelten jedoch als überholt, da sich identische Märchenmotive in extrem weit auseinander liegenden und einander ganz fremden Kulturen finden. Gerade dieser Umstand ist eine der faszinierendsten Beobachtungen in der Märchenforschung. Carl Gustav Jung versuchte, das mit der Annahme eines „Kollektiven Unbewussten“ der Menschheit zu erklären. Ebenso überholt sind die Thesen von Bruno Bettelheim, dass es sich bei Märchen ursprünglich stets um Geschichten für Kinder gehandelt habe, die zeitlos seien und in allen Gesellschaften erzählt und verstanden werden könnten. Tatsächlich beziehen sie sich immer auch deutlich auf soziale Realitäten in ihren Entstehungskontexten, auch wenn Motive von Land zu Land entlehnt wurden.

Als erster großer europäischer Volksmärchensammler und -nacherzähler gilt der Italiener Giambattista Basile, dessen Sammlung "Il Pentamerone" in der ersten Hälfte des 17. Jahrhunderts erschien. Selbstverständlich finden sich zahlreiche Motive seiner Märchen auch in den Märchen der Grimms. Ähnliches gilt für die erste größere französische Volksmärchensammlung von Charles Perrault aus dem Jahr 1697 mit dem Titel "Histoires ou contes du temps passé, avec des moralités: contes de ma Mère l’Oye". Freilich tragen beide Sammlungen stark die Handschrift ihrer Herausgeber, vor allem die Perraults. Aber auch die Volksmärchen der Brüder Grimm sind sprachlich überarbeitet und alle weiteren europäischen Märchensammlungen des 19. Jahrhunderts in der internationalen Nachfolge der Grimms ebenfalls. Völlig unberührt gelassene Fassungen aus mündlicher Überlieferung gibt es erst im 20. Jahrhundert.

Bei Volksmärchen lässt sich kein bestimmter Urheber feststellen. Die mündliche Weitergabe war für lange Zeit die ausschließliche und ist bis heute die natürliche Form der Überlieferung. Dennoch hat auch die schriftliche Überlieferung seit ihren Anfängen auf die traditionelle mündliche Erzählweise miteingewirkt, denn schon im Mittelalter fanden Märchen Eingang in die schriftliche Literatur. Mit der Möglichkeit des Buchdrucks seit dem Ende des 15. Jahrhunderts hat die schriftliche Verbreitung naturgemäß eine größere Bedeutung bekommen. Aufgrund der mündlichen Erzähltradition treten Volksmärchen in vielen teils sehr unterschiedlichen Varianten auf, die desto zahlreicher sind, je älter und je weiter verbreitet ein Märchen - d. h. eine im Wesentlichen gleiche Kombination von Handlungseinheiten - ist. Umgekehrt zeigen auch ganz unterschiedliche Märchen, selbst in weit voneinander entfernten Erzähltraditionen und über Sprachgrenzen hinweg, auffällig viele Gemeinsamkeiten in den einzelnen Handlungsmotiven, das heißt in den kleinsten isolierbaren Handlungseinheiten. Dies hat zum Aarne-Thompson-Index geführt, einer umfangreichen Liste von Märchentypen und einer systematischen Katalogisierung der Handlungseinheiten. Das Märchen ist somit (wie beispielsweise auch das Volkslied) eine Literaturform, die weder Originale noch Nachahmungen kennt; selbst ein Begriff wie „Veränderung“ bezogen auf ein Märchen, das einem anderen ähnelt, ist inadäquat; die Gesamtheit der Volksmärchenliteratur ist prinzipiell von der Permutation der Handlungseinheiten gelenkt, kein konkretes Märchen X ist von einem andern konkreten Märchen Y „abhängig“, wie das in der schriftlich tradierten Literatur in der Regel der Fall ist. (In der Dichtung "Faust" bezieht sich Goethe erkennbar auf das "Puppenspiel vom Doktor Faust", aber das arabische Märchen "Der Fischer und der Geist" ist nicht Vorlage des deutschen Volksmärchens "Der Geist in der Flasche"; beide Märchen verwenden nur eine identische zentrale Handlungseinheit.) Siehe auch Intertextualität.

In Deutschland wird mit dem Begriff Märchen in erster Linie die Volksmärchensammlung "Kinder- und Hausmärchen" der Brüder Grimm (1812) assoziiert, jedoch gibt es zahlreiche weitere Sammlungen deutscher Volksmärchen, wie die Sammlung von Ludwig Bechstein.

In Frankreich wurde die erste Märchensammlung 1697 von Charles Perraults "Histoires ou Contes du temps passé avec des moralités" angelegt und der Ausdruck „contes de fée“ (Feengeschichten) geprägt, von dem sich das englische „fairy tales“ ableitet. Das Element des Zauber- und Fabelhaften tritt hier schon in der Namensgebung zum Vorschein. Es sind jedoch nicht nur Zauberwesen (göttlichen oder teuflischen Ursprungs), welche die Märchenwelt so phantastisch machen, sondern auch Gegenstände mit magischer Wirkung, die den Märchenhelden von großem Nutzen sind - offenbar ein Erbe der keltischen Mythologie - oder das Verzaubertwerden in ein Tier, eine Pflanze, deren Symbolgehalt man hinterfragen kann. Desgleichen spielen hin und wieder Versteinerungen eine Rolle, die sich ebenso tiefenpsychologisch deuten lassen wie die Erlösung durch die Tränen eines mitfühlenden Menschen.
In den aus der Zeit der Wende zum 18. Jahrhundert überlieferten französischen Märchen spiegeln sich jedoch die Probleme einer malthusianischen Gesellschaft, in der eine seit 1690 bestehende Hungerkrise zum Geburtenrückgang, zur Kindestötung und zur Vernachlässigung und Aussetzung sowie zum Verkauf von Kindern durch Eltern und besonders durch Stiefeltern führt. Auch die Verkrüppelung durch Krankheit, Unfall oder Verstümmelung von als unproduktiv geltenden Familienmitgliedern, die als Bettler tätig werden, ist ein Thema. Die phantastischen Elemente der französischen Märchen sind weit weniger ausgeprägt als im deutschen Märchen; seltener spielen sie im Wald, vielmehr oft im Haushalt, im Dorf oder auf der Landstraße. Somit weisen sie deutliche sozialrealistische Züge auf: Sie zeigen, was vom Leben zu erwarten ist. Wichtige Themen sind immer wieder der Hunger, der Zwang zum faktisch vegetarischen Leben, bei dem Fleisch einen seltenen Luxus darstellt, oder die Unterschichtutopie des Sattessens. Die „Suche nach dem Glück“ auf der Landstraße ist dabei nur ein Euphemismus für Bettelei. Allerdings blühen auch Verwandlungsphantasien (in Tiere, Prinzen usw.), die den Zwang zum Eskapismus ausdrücken.

Die mündliche Überlieferung von Volkserzählungen in England zeigt weitaus optimistischere und fröhlichere Züge als in Frankreich oder Deutschland. Es sind selten komplette Erzählungen, häufig nur Reime oder Lieder überliefert, die jedoch von denselben Figuren handeln wie die deutschen oder französischen Märchen. Immerhin war in der englischen Agrargesellschaft des 18. Jahrhunderts bis zur Frühindustrialisierung Hunger kaum jemals allgemein verbreitet. In neuerer Zeit wurden keltische und englische Volksmärchen von dem Australier Joseph Jacobs gesammelt.

Auch hier tauchen dieselben Figuren und Handlungen auf wie im französischen Märchen, allerdings werden sie oft ins adlige oder kaufmännische Milieu verschoben und auf eher komisch-machiavellistische Weise behandelt wie in der Art der Commedia dell’arte, so etwa im Pentamerone des Giambattista Basile, aus dem Clemens Brentano einige Geschichten nacherzählte.

Die russischen Volksmärchen wurden von Alexander Nikolajewitsch Afanassjew gesammelt, die norwegischen Volksmärchen von Peter Christen Asbjørnsen. Die bis heute populäre tschechische Schriftstellerin Božena Němcová (1820–1862) ist besonders durch ihre Märchensammlung (amerikanische Ausgabe 1921) berühmt geworden. Mit ihrem Werk legte sie außerdem bewusst Grundlagen der heutigen tschechischen Sprache. Zahlreiche ihrer Märchen wurden auch verfilmt und besonders der Märchenfilm "Drei Haselnüsse für Aschenbrödel" gehört seit 1973 zum Standardprogramm deutschsprachiger TV-Sender.

Die indischen Märchen können auf eine sehr lange und vielgestaltige Tradition zurückblicken. Zu den bedeutendsten indischen Märchensammlungen gehört die ungefähr 2000 Jahre alte Märchensammlung namens Panchatantra. Der Indologe Johannes Hertel hat Anfang des 20. Jahrhunderts wichtige wissenschaftliche Beiträge zur Erschließung der Panchatantra geleistet. Die Panchatantra floss vermutlich in eine mittelpersische Sammlung von Erzählungen ("Tausend Erzählungen") und damit in die arabische Erzähl- und Märchensammlung Tausendundeine Nacht ein, in der auch Einflüsse griechischer Sagen identifiziert wurden.

Viele chinesische Volksmärchen enthalten Elemente der traditionellen chinesischen Mythologie; eine Abgrenzung ist schwierig. Typisch ist ihre eher episodenhafte, nicht durcherzählte Struktur. Sie wurden von Richard Wilhelm gesammelt ("Chinesische Volksmärchen", Jena 1914).

Die Märchen der Sibirier (z. B. der Tungusen und Jakuten) verweisen teilweise noch auf ihre mongolisch-türkischen Ursprünge, reflektieren aber auch die Nordwanderung dieser Völker in ein subarktisches Umfeld.

Zu den ersten Sammlungen der Märchen der nordamerikanischen Ureinwohner gehörte Karl Knortz’ "Märchen und Sagen der Indianer Nordamerikas" (Jena 1871). Die Märchen der Völker Meso- und Südamerikas sammelte Walter Krickeberg. Heinz Barüske publizierte Märchen der Inuit.

Paul Hambruch nahm 1909/1910 an einer deutschen Südsee-Expedition teil und publizierte Märchen der Südsee und der malaiischen Völker.

Bei den Kunstmärchen (auch als "Moderne Märchen" bezeichnet) handelt es sich um bewusste Schöpfungen von Dichtern und Schriftstellern. Bisweilen greifen sie Motive der Volksmärchentradition auf, meist werden aber neuartige fantastische Wundergeschichten erfunden, die mit dem Volksmärchen aber dennoch durch den Aspekt des Wunderbaren und Unwirklichen verbunden bleiben. Ihr Inhalt wird überwiegend durch die Weltanschauung und die Ideen einer individuellen Person getragen und unterliegt den Einflüssen der Literaturströmungen. In der Romantik erreichte das Kunstmärchen einen frühen Höhepunkt und erhielt entscheidende Impulse für seine weitere Entwicklung. In der Frühromantik lag der Akzent auf sehr künstlichen Schöpfungen, die die Grenzen der herkömmlichen Märchen hinter sich ließen und sich somit dem unbefangenen Märchenleser nicht mehr so leicht erschlossen. Das änderte sich jedoch wieder mit den Dichtern der Spätromantik, die den einfachen Märchenton bevorzugten.

Der am meisten gelesene Verfasser von Kunstmärchen im 19. Jahrhundert war Wilhelm Hauff (1802–1827). Seine Märchenbücher "Die Karawane", "Der Scheich von Alexandria" und "Das Wirtshaus im Spessart" erschienen in drei aufeinanderfolgenden Jahren und spielen, wie die Titel schon verraten, vor unterschiedlichem Hintergrund. Während er in den ersten beiden Bänden die Handlung in den Orient verlegt, dient im letzteren der rauere Norden als Schauplatz. All seine Märchen kennzeichnet das Abenteuer, was aus seiner eigenen Begeisterung für die Fremde zu erklären ist.

Zu den beliebtesten Märchendichtern zählt der Däne Hans Christian Andersen (1805–1875). Angeregt wurde er durch die Brüder Grimm und die deutschen Kunstmärchen. Zunächst ist in seinen Märchen noch eine deutliche Anlehnung an das Volkstümliche zu erkennen, doch schon bald entwickelte er seinen eigenen, unverwechselbaren Stil. Im Gegensatz zu den Volksmärchen, die grundsätzlich an einem unbestimmten Ort spielen, beschrieb er sorgfältig den Schauplatz seiner Geschichten und achtete auf die Nähe zur kindlichen Weltauffassung. Seine Erzählungen weisen eine einfache und ungekünstelte Sprache auf und wirken durch einen eindringlichen Erzählton. Es ging ihm darum, das Wunderbare in die Wirklichkeit des Alltags hineinzuholen, ohne dass eine Kluft zwischen beidem entsteht, wie es bei den Romantikern oft der Fall war.
In Dänemark wie in Deutschland sah man in Andersens Erzählungen in erster Linie Märchen für Kinder. Das allerdings widersprach seinem eigenen Selbstverständnis, denn er selbst verstand sich als Autor für alle Altersklassen.

Sozialkritischen Hintergrund haben die Märchen von Oscar Wilde (1854–1900), die ganz im Sinne der Romantik Idealbilder im Widerstreit zu grausamen Realitäten entwerfen oder aus der Sicht des ausgebeuteten Opfers den Egoismus und die Oberflächlichkeit der Herrschenden anprangern.

Edith Nesbit (1858–1924) entführt ihre kindlichen Leser aus einer realen Situation in eine Zauberwelt und am Ende wieder zurück in die Realität. In ihrem letzten Werk „Meereszauber“ verfolgte sie friedenspädagogische Absichten.
Im gleichen Stil entführt Gerdt von Bassewitz (1878–1923) in "Peterchens Mondfahrt" seine Leser aus der Kinderstube in eine himmlische Welt mit Fantasy-Charakter. Beeindruckend sind die Begrüßungsballaden der allegorischen Naturgeister im Schloss der Nachtfee.

Selma Lagerlöf (1858–1940) erhielt für ihren Märchenroman "Die wunderbare Reise des kleinen Nils Holgersson mit den Wildgänsen" 1909 den Nobelpreis für Literatur. Einen ähnlichen Hintergrund haben Carlo Collodis (1826–1890) Abenteuer von "Pinocchio". In beiden Fortsetzungsgeschichten wird ein unartiges Kind durch schmerzliche Erfahrungen erzogen.

Rafik Schami (* 1946) schachtelt seine orientalischen Märchen "Der ehrliche Lügner", "Erzähler der Nacht" und "Der Wunderkasten" ähnlich jenen aus 1001 Nacht in Rahmengeschichten.

Heinz Körner (* 1947) und Roland Kübler (* 1953) sind ab den 1980ern Autoren und Herausgeber sehr erfolgreicher „alternativer“ Märchen bzw. Märchenanthologien für Erwachsene: U. a. "Die Farben der Wirklichkeit", "Wie viele Farben hat die Sehnsucht". Unter ähnlichen Vorzeichen und ebenfalls in den 1980ern bis Mitte der 1990er hat der Hamburger "Metta Kinau Verlag" einige Märchenanthologien jeweils in mehreren Auflagen herausgegeben.

Bei Märchenparodien handelt es sich um Parodien bekannter Märchen. Die Handlung weicht dabei mehr oder weniger stark von der des ursprünglichen Märchens ab. Manchmal bezieht sich die Parodie auf ein einziges Märchen und manchmal auf mehrere gleichzeitig. Bekannte Märchenparodien sind:

Literatur

Filme

Hörspiele

Meisterliche Märchenerzähler, die Märchen sammeln, gibt es vermutlich, seit es Märchen gibt. Sie trugen zur Entstehung, Überlieferung von Märchen und Märchensammlungen maßgeblich bei.

Zu den bekanntesten Märchensammlern gehören der Italiener Giambattista Basile (1575–1632), der Franzose Charles Perrault (1628–1703), Johann Karl August Musäus (1735–1787), Benedikte Naubert (1756–1819), die Brüder Grimm (1785–1863)/(1786–1859), Ernst Moritz Arndt (1769–1860), Ludwig Bechstein (1801–1860), der Norweger Peter Christen Asbjørnsen (1812–1871), der Russe Aleksandr Nikolajewitsch Afanassjew (1826–1871) sowie der Schweizer Pädagoge Otto Sutermeister (1832–1901). Eine Sammlung orientalischer Märchen enthalten die Geschichten aus 1001 Nacht. Die von Friedrich von der Leyen begründete Reihe "Die Märchen der Weltliteratur" stellt Märchen aus aller Welt vor.

Zu den frühen Märchenerzählern können unter anderem die Barden gezählt werden. Sie setzen damit eine Erzählkultur fort, die bereits in einem frühen indogermanischen Sprach- und Kulturraum angelegt gewesen sein muss.

Bei den Berbern in Nordafrika gibt es die kulturell bedeutsame Erzähltradition bis heute.

Die meisten Märchenerzähler der Gegenwart sammeln alte Volksmärchen und setzen sich für deren Erhaltung und die Tradition des Erzählens ein. Bekanntheit im deutschsprachigen Raum haben insofern u. a. die Deutschen Klaus Adam, Mario Eberlein, Frank Jentzsch, Frieder Kahlert, Elsa Sophia von Kamphoevener, Christian Peitz und Michaele Scherenberg, die Österreicher Eva Jensen, Norbert Julian Kober, Michael Köhlmeier, Erwin Stammler, Folke Tegetthoff, und Helmut Wittmann und die Schweizer Jürg Steigmeier und Hasib Jaenike erlangt. Im internationalen Bereich sind Naceur Charles Aceval (Algerien), Radha Anjali (Indien), Eth Noh Tec (Japan), Heather Forest (USA), Huda al-Hilali (Irak), Jankele Ya'akobson (Israel), Saddek El Kebir (Algerien), Laura Kibel (Italien) und Antonio Sacre (Kuba) zu nennen. Im süddeutschen Raum wurde 1999 ein Bildungsträger mit dem Namen Goldmund e.V. gegründet, der Geschichtenerzähler ausbildet. Schulen für Märchenerzähler gibt es mittlerweile einige, z. B. die Märchenschule "RosenRot" in München oder das Märchenzentrum "Dornrosen" in Nürnberg oder die "Mutabor Märchenstiftung" in Lützelflüh. In Deutschland wurde 1956 in Rheine/Westfalen die Europäische Märchengesellschaft e.V. (EMG) gegründet, die mit ihren mittlerweile 2.500 Mitgliedern zu den größten literarischen Gesellschaften zählt und seit einigen Jahrzehnten u. a. Kurse zur Märchenkunde und zum Märchenerzählen anbietet.

Eine Sonderform des Märchenerzählers ist der fahrende Mundwerker bzw. Bänkelsänger, ein Vertreter des „fahrenden Volkes“. Fahrendes Volk war in Deutschland bis in die 1930er Jahre anzutreffen. Diese Mundwerker zogen umher und erzählten gegen Entgelt Moritaten und/oder sangen Bänkellieder.

Von 1988 bis 2006 fand in Graz alljährlich Europas größtes Erzählkunstfestival "Die lange Nacht der Märchenerzähler" (GRAZERZÄHLT) statt. Seit 2007 findet diese Veranstaltung in Niederösterreich unter dem Namen "fabelhaft" statt.

In Berlin finden seit 1990 jährlich im November die Berliner Märchentage statt.

Außerdem gibt es Märchenwälder und Märchenzoos, das sind Ansammlungen von Dioramen mit kleinen Figuren und Lautsprechern, die (meist gegen Münzeinwurf) Märchen erzählen. Ein traditionsreicher Märchenzoo ist zum Beispiel der Märchenzoo Blauer See (Ratingen).

Seit 1985 finden im Park von Schloss Philippsruhe in Hanau, der Geburtsstadt der Gebrüder Grimm, die Brüder Grimm Märchenfestspiele statt. Die Besucherzahlen der Festspiele überschritten 2006 zum ersten Mal die Millionengrenze.

Die Festspiele Balver Höhle veranstalten seit 1991 alljährlich die Reihe Balver Märchenwochen.

Eine Veranstaltung für Kinder und Familien ist das "Festival der besten deutschsprachigen Märchen- und Geschichtenerzähler", das seit 2005 immer am ersten Wochenende im Juli in Neukirchen-Vluyn am Niederrhein unter der Schirmherrschaft Ursula von der Leyens ausgetragen wird. 16 Erzählerinnen und Erzähler aus dem deutschsprachigen Europa stellen sich im Wettbewerb dem Publikum und einer Jury.

2009 hatte Marburg für einige Monate einen "Grimm-Dich-Pfad" mit überdimensionalen Märchenfiguren und Informationen zu den dazugehörenden Märchen in der Stadt installiert. Diese sollen in erweiterter Form zur 200-Jahr-Feier der Kinder- und Hausmärchen wieder angebracht werden.

Auf der dänischen Insel Bornholm findet seit über zehn Jahren zwischen Juni und September das Bornholmer Märchenfestival statt. Jeweils 14 Märchenerzähler erzählen in der Alten Strandvogtei deutschsprachige Märchen, Mythen und Sagen. Initiator des Festivals ist Eduard Dahlmann.

2012 gründete Eduard Dahlmann den Rügener Märchensommer, der nun alljährlich auf der Insel Rügen stattfinden wird. Die Erzählstätte 2012 war das Grundtvighaus in Sassnitz, 2013 geht der Rügener Märchensommer nach Bergen auf Rügen. Aufführungsstätte dort ist das Sagen- und Märchenhotel.

2015 ist eine weitere Märchenspielstätte auf Rügen beim Dörfchen Puddemin hinzugekommen. Im Rahmen des Rügener Märchensommers erzählen zehn Erzähler/innen auf dem Rückweg vom Bornholmer Märchenfestival im kleinen "Galeriecafé Friedrich", das dem "Museumshof Puddemin Rügen" angegliedert ist und von Familie Zeitz betrieben wird.





</doc>
<doc id="14574" url="https://de.wikipedia.org/wiki?curid=14574" title="Grimms Märchen">
Grimms Märchen

Grimms Märchen nennt man volkstümlich die berühmte Sammlung Kinder- und Hausmärchen, in der Forschungsliteratur auch als "KHM" abgekürzt, die Jacob und Wilhelm Grimm, genannt die Brüder Grimm, von 1812 bis 1858 herausgaben.

Die Brüder sammelten auf Anregung der Romantiker Clemens Brentano, Achim von Arnim und Johann Friedrich Reichardt ursprünglich für deren Volksliedersammlung "Des Knaben Wunderhorn" ab 1806 Märchen aus ihrem Bekanntenkreis und aus literarischen Werken. Sie waren ursprünglich nicht nur für Kinder gedacht, sondern entstanden vor allem aus volkskundlichem Interesse und erhielten entsprechende märchenkundliche Kommentare. Wilhelm Grimms sprachliche Überarbeitungen schufen daraus einen Buchmärchenstil, der bis heute das Bild von Märchen prägt.

Clemens Brentano erhielt auf der Suche nach volkstümlichen Liedern für die Sammlung "Des Knaben Wunderhorn" über Friedrich Carl von Savigny Kontakt zu dessen ehemaligem Studenten Jacob Grimm, der in der Kasseler Bibliothek arbeitete. So kamen die Brüder Grimm ab 1806 dazu, für ihn Lieder und bald auch Märchen zunächst aus literarischen Werken zu exzerpieren. Als musterhaft präsentierte Brentano ihnen seine Redaktionen "Von dem Mäuschen, Vögelchen und der Bratwurst" und "Von dem Tode des Hühnchens" sowie Runges Märchen "Vom Fischer und seiner Frau" und "Vom Wacholderbaum". Weiterhin empfahl er als Gewährsleute mündlicher Erzähltradition Friederike Mannel sowie die Geschwister Hassenpflug, Wild und Ramus. Sein Vorschlag, Erzählungen einer alten Frau im Elisabeth-Hospital in Marburg abzuhören, blieb unberücksichtigt. Solche Feldforschung war höchst selten und auch eigene Kindheitserinnerungen der Brüder Grimm spielten keine Rolle.

Jacob Grimm schickte Brentano am 17. Oktober 1810 48 Texte. Insgesamt war die Sammlung etwas größer, da er Brentano bereits vorliegende Texte nicht erneut abschrieb. Jacob Grimm hatte die Texte sortiert und 25 selbst niedergeschrieben, Wilhelm 14 und verschiedene Gewährsleute sieben. Von der sogenannten handschriftlichen "Urfassung" stammten wohl 18 Stück aus literarischen Quellen (einschließlich der zwei Texte Runges), 16 von den Geschwistern Hassenpflug, 14 von Familie Wild, sechs von Friederike Mannel, zwei von der Frau des Marburger Hospitalvogts und eins von den Geschwistern Ramus. Mündliche Beiträger waren etwa gleichaltrige junge Frauen aus dem bürgerlichen Milieu, bis auf zwei von der Apothekersfrau Wild nachgewiesene Texte ("Strohhalm, Kohle und Bohne", "Läuschen und Flöhchen"). Die Urschrift erwarb der Sammler Martin Bodmer. Sie befindet sich heute in der von ihm gegründeten Bibliotheca Bodmeriana in Cologny bei Genf.

Clemens Brentano nutzte das angeforderte Material nicht. Jacob und Wilhelm Grimm führten die Sammlung in eigener Regie weiter, wobei sie Notizen zu Gewährspersonen und Aufnahmedaten nun genauer führten. Die Geschwister Hassenpflug und Wild waren weiterhin die ergiebigsten Quellen. Dem Bild hessischer Volksüberlieferung am nächsten kommt wohl der pensionierte Dragonerwachtmeister Johann Friedrich Krause als ältester Beiträger überhaupt. Nun war es Brentanos Freund Achim von Arnim, der die Brüder Grimm auf weitere Texte hinwies, u. a. "Die Sterntaler", und sie 1812 zur Publikation animierte. Das Buch sollte preiswert sein und zur Mitarbeit anregen. So wurde auch fragmentarisches Material abgedruckt mit Anmerkungen direkt unter den Texten. Die ersten Exemplare erschienen am 20. Dezember 1812, der größte Teil im März 1813 in einer Auflage von 900 Stück bei Verleger Georg Andreas Reimer in Berlin. Es war zu Verzögerungen gekommen, da der Text von "Der Fuchs und die Gänse" verloren gegangen war. Außerdem führten Reimers Eingriffe in Runges Texte zu Spannungen.

Der Druck des zweiten Teils 1814 (vordatiert auf 1815) verlief unkomplizierter. Wilhelm Grimm entdeckte als Quellen die westfälischen Adelsfamilien von Haxthausen und von Droste Hülshoff. Da diese ihre Märchen letztlich von Mägden, Bauern, Schäfern, u. a. übernahmen, gelang ihm tatsächlich der Zugriff auf eigentliches Volksgut, das gleichwohl durchwegs den intellektuellen Filter belesener Frauen des Bürgertums und des Adels durchlief. Der Erzähler getraute sich nicht alles zu erzählen, die Aufzeichnerinnen gaben nicht jede Geschichte weiter, und die Brüder Grimm wählten wiederum aus und überarbeiteten. Heinz Rölleke bemerkt: „Für fragmentarische, in sich widersprüchliche, oft auch zotenhafte Aufzeichnungen hätte sich seinerzeit weder ein Verleger noch das Lesepublikum interessiert.“ Insbesondere enthielt der zweite Band nun Beiträge der ab Mai 1813 neugewonnenen Erzählerin Dorothea Viehmann, die auch einige des ersten Teils ersetzten. Ihre Kontakte als Wirtstochter und ihr Erzähltalent machten sie zum Idealbild einer Märchenfrau, deren Texte auch zur Vervollständigung anderer verwendet wurden und dem Anmerkungsteil als Vergleichsfassungen dienten. Sie erzählte „bedächtig, sicher und ungemein lebendig mit eigenem Wohlgefallen daran, erst ganz frei, dann, wenn man will, noch einmal langsam, so daß man ihr mit einiger Übung nachschreiben kann“ (Wilhelm Grimm). Ihre Texte wurden auch für spätere Auflagen kaum verändert.

Der Verkauf, vor allem des zweiten Bandes, verlief schleppend, weshalb es zu Unstimmigkeiten zwischen den Grimms und ihrem Verleger Reimer kam. 1819 kam eine zweite Auflage beider Bände heraus, die als die wichtigste in der Editionsgeschichte angesehen wird. Eine Vielzahl von Texten wurde darin neu aufgenommen, darunter einige, die heute zum Grundbestand der KHM zählen ("Die Bremer Stadtmusikanten", "Hans im Glück", "Tischlein deck dich"), zahlreiche Texte der ersten Auflage wurden grundlegend bearbeitet. Die Grimms reagierten so auf Kritik von Freunden und Rezensenten.

Ab der 2. Auflage übernahm Wilhelm Grimm das Sammeln und Überarbeiten der Texte. Jacob besorgte nur noch einige Texte für die 2. und 3. Auflage, nahm aber wohl weiter Einfluss auf die wissenschaftlichen Anmerkungen. Brieflich dokumentiert ist noch sein Rat, die allzu fragmentarischen Märchen "Die drei Schwestern", "Der Löwe und der Frosch" und "Der Soldat und der Schreiner" wegzulassen. Wilhelm Grimm reagierte anscheinend, wenn auch stillschweigend auf zeitgenössische Kritik, die auf gefälligere erzählerische Bearbeitung des Materials gedrängt hatte. Hier zeigte sich die Unvereinbarkeit des Grimm’schen Anspruchs einer literaturhistorischen Sammlung mit Erwartungen an ein Kinderbuch. Dem früheren Rat von Arnims entsprechend, fügte Wilhelm Grimm der 2. Auflage zwei Titelkupfer seines Bruders Ludwig Emil Grimm bei und trennte den Kommentarteil ab. Es schärfte sich nachträglich der Sinn für Gattungsgrenzen, so dass "Die himmlische Hochzeit" dem neuen Abschnitt "Kinderlegenden" zugeordnet wurde, "Die Kinder in Hungersnot" entfiel und "Die heilige Frau Kummernis" stattdessen in "Deutsche Sagen" erschien. Offenbar erkannte Wilhelm auch die Verwandtschaft einiger Texte Hassenpflugs zu französischen Originalen, u. a. von Charles Perrault (z. B. "Der gestiefelte Kater", "Blaubart", "Der Okerlo"), andere waren ohnehin von Jacob Grimm übersetzt worden ("Von der Nachtigall und der Blindschleiche", "Die Hand mit dem Messer", "Das Mordschloß"). Feststellbar ist auch eine zunehmende Sentimentalisierung, Entsexualisierung (z. B. "Rapunzel") und Verchristlichung (z. B. "Das Mädchen ohne Hände", "Der Gevatter Tod", "Allerleirauh", "Die Nelke", "Die Sterntaler"). Von Ausgabe zu Ausgabe arbeitete Wilhelm oft subtil ein Ideal romantischer oder oft genug biedermeierlicher Komposition heraus. Einer literarischen Tradition folgend, ging die Rolle des Bösen in "Hänsel und Gretel" und "Schneewittchen" an Stiefmütter, um wohl das biedermeierliche Familienidyll zu wahren. Fremdwörter wurden ersetzt, so Feen durch Zauberinnen, Prinzen durch Königssöhne. Wilhelm Grimm durchsetzte die Texte ab der 2. Auflage exzessiv mit volkstümlichen Wendungen, die er oft aus Büchern hatte. So ist die Mahnung des "Froschkönigs" nun "in den Wind gesprochen", "Das tapfere Schneiderlein" geht "immer seiner spitzen Nase nach", und "Schneewittchens" Königin wird "gelb und grün vor Neid". Besonders eine Reihe in meist westfälischer Mundart geschriebener Texte sollte wohl die Volkstümlichkeit unterstreichen, blieb aber vom Leser eher unbeachtet. Die ursprüngliche Idee, eine breite Öffentlichkeit zum Mitsammeln anzuregen, erfüllte sich nicht. Ab der 3. Auflage hinzugefügte Texte gehen fast nur auf literarische Quellen zurück (Ausnahme: "Die Lebenszeit"). Diese wurden stilistisch überarbeitet, meist anschaulicher und mit mehr wörtlichen Reden erzählt, aber von direkten Moralisierungen befreit (z. B. "Der kluge Knecht"). Die dritte Auflage erschien 1837, die vierte 1840, die fünfte 1843, die sechste 1850, die siebte Auflage letzter Hand 1857.

Waren den Märchen der Erstauflage noch Kommentare direkt beigegeben, so erschienen diese für die Zweitauflage 1822 separat und wurden erst 1856 erneut aufgelegt. Diese Anmerkungen zu den einzelnen Märchen liefern oft Literaturangaben zu vielen Vergleichstexten, von denen einzelne auch wiedergegeben werden. Die Herkunft der mündlichen Fassungen wird nach Landstrichen angegeben. So erhielten die Beiträge der in ihrer Kindheit von Hanau nach Kassel umgezogenen Hassenpflugs den Vermerk "aus Hessen", "aus den Maingegenden" oder auch "aus Hanau", solche der Dorothea Viehmann stets "Aus Zwehrn". Auch einige schriftliche Quellen werden in dieser Weise verschleiert. So steht "Braunschweiger Sammlung" für die dort 1801 anonym erschienene Sammlung "Feen-Märchen", ebenso wie "Erfurter Sammlung" für Günthers "Kindermährchen" von 1787.

Die 2. Auflage von 1819 wurde auch zur Grundlage für die ersten Übersetzungen (u. a. ins Englische) und für die „Kleine Ausgabe“ mit 50 Titeln, die für Kinder gedacht war und ab 1825 erschien. Sie brachte den Publikationserfolg, der erst später auch auf die große Ausgabe überging. Die „Kleine Ausgabe“ kam als erste deutsche Ausgabe der KHM mit Illustrationen (7 Stahlstichen von Ludwig Emil Grimm) im Text heraus, was von vielen Kritikern (u. a. von Achim von Arnim) zuvor als unabdingbar für einen Bucherfolg angesehen worden war. Von der „Kleinen Ausgabe“ erschienen zu Lebzeiten der Grimms zehn Auflagen (1825, 1833, 1836, 1839, 1841, 1844, 1847, 1850, 1853, 1858). Sie enthielt nur die Titel: KHM 1, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 19, 21, 24, 25, 26, 27, 34, 37, 45, 46, 47, 50, 51, 52, 53, 55, 58, 59, 65, 69, 80, 83, 87, 89, 94, 98, 102, 104 (bis 1853 KHM 104a), 105, 106, 110, 114, 161 (nur 1825 KHM 124), 129, 130, 135, 151, 153.

Heinz Rölleke stellt fest, dass Wilhelm Grimm im Wesentlichen ab der 2. Auflage seinen Stil gefunden hatte, der zukünftig die "Gattung Grimm" ausmachte und unsere Vorstellung von Märchen bis heute prägt. Maßgeblich waren dafür Runges Märchen "Vom Fischer und seiner Frau" und "Vom Wacholderbaum", die sie auch später immer wieder als maßgeblich für Märchen ansahen. Eine ähnliche Rolle spielte vielleicht auch Jung-Stillings "Jorinde und Joringel". Ein Kriterium der Textauswahl waren vermutetes Alter und mündliche Überlieferung (z. B. "Der Froschkönig oder der eiserne Heinrich"), sowie Forschungsinteressen der Brüder Grimm wie Themen des älteren Tierepos (z. B. "Der Hund und der Sperling", "Der Wolf und der Fuchs"). Von Anfang an bestand das mythologische und volkskundliche Interesse der Brüder Grimm zugleich mit ihrem Wunsch, ein Kinderbuch zu schaffen. Ihr ganzes Philologenleben lang feilten sie (ab der zweiten Auflage fast ausschließlich Wilhelm) an den Texten, tauschten ganze Märchen aus, nahmen neue auf, verschmolzen mehrere Textfassungen oder fügten Redensarten und Sprichwörter ein. Jacob Grimm bekannte, dass dies nichts mit Exaktheit im mathematischen Sinn zu tun habe. Deshalb gleicht keine Auflage in ihrem Textbestand der anderen. Dies geschah im Bemühen, den verborgenen Kern zu rekonstruieren, wobei ein neuer Stil von Buchmärchen entstand, die man zwischen "Kunst-" und mündlichen "Volksmärchen" ansiedeln kann. "Schneeweißchen und Rosenrot" kann weitgehend als Kunstmärchen Wilhelm Grimms gelten. Andere Märchenbücher wurden hingegen erst spät als Quellen hinzugezogen und waren oft ihrerseits bereits durch Grimms Märchen beeinflusst. Wilhelm Grimms Bearbeitung zielt auf klare, ausgewogene Textstruktur. Dies löste somit die bis dahin verbreiteten, oft langatmigen Feenmärchen ab. Durchgängig ist eine Tendenz zur Dreigliedrigkeit. Der Biograph Steffen Martus benennt eine schwer fassbare Gegenwärtigkeit des Entrückten, die sich auch in Wilhelm Grimms autobiographischen Kindheitserinnerungen wiederfindet. Dies lässt sich bis zur Häufung von Brüdermärchen mit sich wiederholenden Konflikten verfolgen (siehe z. B. "Die drei Federn"), während die Bearbeitungen von "Die Gänsehirtin am Brunnen" oder "Der Löwe und der Frosch" weniger Interesse zeigen. Gleichwohl fehlen für die konkrete Textgestaltung persönliche Vorbilder. Lediglich die Anmerkungen zu "Die Hochzeit der Frau Füchsin" und "Die Sterntaler" nennen übereinstimmend eigene, "dunkle" Erinnerung. Eine ähnliche Andeutung findet sich brieflich zu "Wie Kinder Schlachtens miteinander gespielt haben". Häufiges Strukturelement ist der Erwerb von Zaubergaben, wovon die erste oft Nahrung, die zweite Schnelligkeit und die dritte, oft ein Musikinstrument, die Gegenspieler verprügelt, z. B. "Das blaue Licht", "Der Jude im Dorn". Die Heldin in Not drückt im Monolog ihre moralische Haltung aus. Neben Märchen und Tiermärchen sind in fließendem Übergang viele Schwänke vertreten, teilweise auch mit Motiven der Sage. Nur etwa 50 Texte der Auflage letzter Hand würde man heute als reine Märchen bezeichnen.

Die 1810 an Brentano gesandte handschriftliche "Urfassung" blieb erhalten und ist heute eine wertvolle Vergleichsquelle, da sonstige frühe Märchenaufzeichnungen von den Brüdern Grimm nach dem Druck vernichtet wurden. Schon 1808 schickte Jacob Grimm außerdem sieben handschriftliche Texte an Savigny. Die Erforschung ihrer Märchen begann mit Wilhelms Sohn Herman, der auch die Notizen in ihren Handexemplaren auszuwerten versuchte. Besonders irreführend für die Märchenforschung wirkte lange Zeit seine irrtümliche Zuschreibung von Beiträgen der jungen Marie Hassenpflug an eine "Alte Marie". Die wissenschaftliche Rezeption beschränkte sich verständlicherweise lange Zeit auf die Ausgabe letzter Hand. Viele teils erhebliche Bearbeitungen erkennt man durch Vergleich verschiedener Auflagen. Heinz Rölleke veröffentlichte 1975 eine Ausgabe der 1810 von Jacob Grimm an Brentano geschickten, handschriftlichen Urfassung, womit ein weniger zuverlässiger Abdruck von Joseph Lefftz aus dem Jahr 1927 abgelöst wurde. Heinz Rölleke weist darauf hin, dass immer wieder versucht wurde, einen Gegensatz zwischen den Geschwistern Grimm zu sehen. Tatsächlich ist es nicht möglich, Unterschiede in der Bearbeitung zwischen Jacob und Wilhelm Grimm auszumachen. Das bürgerliche Umfeld in Kassel war vielfach hugenottisch geprägt. Dorothea Viehmann war keineswegs die alte Bäuerin, als die die Grimms sie darstellten, sondern eine gebildete Frau. Nach Ansicht vieler Forscher war die Pose der sorgfältigen Sammler alter Traditionen, die die Brüder einnahmen, weitgehend eine der Zeitstimmung der Romantik geschuldete Fiktion: Die Märchensammlung stellt vielmehr eine Mischung aus neuen Texten, Kunstmärchen und teils stark bearbeiteten und veränderten Volksmärchen dar.

Die Texte wurden von Auflage zu Auflage weiter überarbeitet, teilweise „verniedlicht“ und mit christlicher Moral unterfüttert. Die Grimms reagierten damit auch auf Kritik, die Märchen seien nicht „kindgerecht“. Um dem zeitgemäßen Geschmack des vorwiegend bürgerlichen Publikums entgegenzukommen, wurden auch wichtige Details geändert. In ihrer Vorrede zu der Ausgabe der KHM von 1815 erwähnen sie explizit, es handle sich bei ihrer Sammlung von Märchen um ein "Erziehungsbuch". Sie versichern in ihrer Vorrede immer wieder, dass es sich bei den gesammelten Märchen um „echt hessische Märchen“ handle, welche ihren Ursprung in altnordischen und urdeutschen Mythen hätten. Dass es sich bei ihrer Hauptquelle, der Viehmännin, nicht um eine hessische Bäuerin, sondern um eine gebildete Schneiderin mit französischen Wurzeln handelt, verschweigen sie hingegen. In den Handschriften der KHM, die 1927 in einer Abtei im Elsass gefunden wurden, finden sich jedoch Vermerke über die französische Herkunft und die Parallelen zu Perraults Märchensammlung. Durch Perrault und durch die hugenottische Herkunft Dorothea Viehmanns und der Kasseler Familien Hassenpflug und Wild (sie verkehrten im Hause Grimm; eine Tochter der Familie Wild wurde später die Frau Wilhelms) flossen auch viele ursprünglich französische Kunstmärchen und Märchenvarianten in die Sammlung ein. Um ein Märchenbuch mit „rein deutschen“ Märchen zu haben, wurden einige Märchen, die aus Frankreich in den deutschen Sprachraum gelangten, wie etwa "Der gestiefelte Kater" oder "Blaubart", nach der ersten Ausgabe wieder entfernt. Dies geschah allerdings nicht konsequent, denn den Grimms war durchaus bekannt, dass zum Beispiel für "Rotkäppchen" auch eine französische Version mit tragischem Ende existierte. Eine nationale Eingrenzung war auch deshalb fragwürdig, weil einige Märchen wie etwa Aschenputtel eine umfangreiche europäische und sogar internationale Herkunfts- und Verbreitungsgeschichte haben.

Die Handexemplare der Brüder Grimm ("Kasseler Handexemplare") mit ihren handschriftlichen Notizen wurden 2005 von der UNESCO zum Weltdokumentenerbe erklärt und befinden sich im Bestand der Universitätsbibliothek Kassel.

Nach dem Zweiten Weltkrieg war in Westdeutschland die Meinung tonangebend, die Märchen der Brüder Grimm seien mitverantwortlich für die Gräueltaten der Nazis gewesen. Der britische Major T. J. Leonard prüfte 1947 die Schulbücher der wilhelminischen Zeit und kam in seiner im selben Jahr erschienenen Schrift "First steps in cruelty" zu dem Schluss, die Grimmschen Märchen hätten bei den deutschen Kindern eine unbewusste Neigung zur Grausamkeit erzeugt. In der amerikanischen Besatzungszone wurden die "Kinder- und Hausmärchen" aus den Schulen und Bibliotheken aussortiert und nach Übersee verschifft und in der britischen Besatzungszone wurde eine Zeit lang keine Lizenz für den Nachdruck ausgegeben.
Obwohl auch entgegengesetzte Meinungen geäußert wurden, beherrschten bis in die 1970er Jahre märchenkritische Stimmen den Diskurs. Einen allmählichen Umschwung brachte Bruno Bettelheims Veröffentlichung "Kinder brauchen Märchen" (1976), in der er aus psychoanalytischer Sicht die für Kinder tröstliche und bestärkende Wirkung der Grimmschen Märchen herausarbeitete.

Die Brüder Grimm selbst sahen ihre Sammlung immer wieder auch als ein "Erziehungsbuch". Dies zielte jedoch nicht auf die Vermittlung von Normen, sondern eher auf ein gewisses Weltverständnis, das zu pädagogischen Vorstellungen von Aufklärung und Romantik passte. So verlieren auch schauerliche Inhalte im behaglichen Tonfall ihren Schrecken. Jacob Grimm präsentierte das Sammelprojekt sogar auf dem Wiener Kongress.

Obwohl Grimms Märchen zu den bekanntesten Werken der deutschen Literatur gehören, sind die Originaltexte der Märchen den meisten Lesern unbekannt, so dass viele äußere Details, die Allgemeinwissen sind, tatsächlich nicht in der Grimm’schen Märchensammlung belegt sind. Einige verbreitete Irrtümer: Grimms Märchen beginnen keinesfalls immer mit „Es war einmal“. Die bekannte Eröffnungsformel wird bei etwa 40 Prozent der Geschichten verwendet. Möglich sind auch entsprechende mundartliche Varianten. Viele allgemein bekannte Verse haben im Originaltext eine andere Fassung als gemeinhin angenommen. Das Hexenhaus in "Hänsel und Gretel" besteht nicht aus Lebkuchen, sondern aus Brot, Kuchen und Zucker. Das Märchen "Schneewittchen" heißt bei den Brüdern Grimm "Sneewittchen". "Dornröschen" sticht sich nicht an einer Rose, sondern an einer Spindel; "Aschenputtel" fährt mit verschiedenen Kleidern zum Festball, erhält diese jedoch nicht von einer Fee, sondern von einem Baum am Grab ihrer Mutter. Der "Froschkönig" verwandelt sich zum Menschen zurück, nicht nachdem er geküsst, sondern nachdem er voll Abscheu an die Wand geschleudert wird (und übrigens kommt auch im ganzen Märchen das Wort "Prinz" nicht vor). 

Anlässlich des 200-jährigen Jubiläums der Kinder- und Hausmärchen (2012) schrieb der Literaturkritiker Jens Bisky: „War es ein Unfall der Literaturgeschichte, dass nicht Clemens Brentano die Märchen bearbeitete? Für den wahren Märchenfreund heißt es: Los von den Grimms! Zu stiefmütterlich sind sie mit der Phantasie umgegangen.“ Brentano hatte selbst schon äußerst kritisch Stellung bezogen: „Ich finde die Erzählung aus Treue äußerst liederlich und versudelt und in manchem dadurch sehr langweilig.“ Auch August Wilhelm Schlegel und Heinrich Voß äußerten sich ablehnend, während Bettina von Arnim, Görres, Goethe, Savigny und Friedrich Schlegel des Lobes voll waren.

Die Texte werden in der Forschungsliteratur nach ihrer Nummer innerhalb der "Kinder- und Hausmärchen (KHM)" abgekürzt, z. B. KHM 15 für "Hänsel und Gretel". Der Zusatz „a“ kennzeichnet Texte, die bis zur Auflage letzter Hand durch andere ersetzt wurden. Die Reihenfolge deutet nur vereinzelt regionale oder motivliche Zusammenhänge an. Eine alphabetische Auflistung findet sich hier .









Sechs Textbruchstücke sind im Anmerkungsband gesondert wiedergegeben: "Der Mann vom Galgen"; "Die Laus" (entspricht KHM 85b "Prinzessin mit der Laus"); "Der starke Hans"; "Der gestiefelte Kater"; "Die böse Schwiegermutter" (entspricht KHM 84a "Die Schwiegermutter"); "Märchenhafte Bruchstücke in Volksliedern". Die Texte "Die alte Hexe", "Mährchen v. Fanfreluschens Haupte" und "Vom König von England" aus der handschriftlichen Urfassung von 1810 schieden noch vor der ersten Druckfassung aus. Die Erstauflage des Anmerkungsbandes enthielt außerdem die Märchen aus Basiles "Pentameron" in erstmals kompletter, wenn auch zusammengefasster deutscher Übersetzung. Unabhängig von den Kinder- und Hausmärchen veröffentlichten die Brüder Grimm auch "Deutsche Sagen" (1816, 1818), "Irische Elfenmärchen" (1826) und diverse Einzeltexte in Zeitschriften und Almanachen.

Aktuell auf dem Buchmarkt gibt es zahlreiche Ausgaben der Grimm-Märchen: Illustrierte Bücher für Kinder, fast immer in einer Auswahl und in mehr oder weniger treuen Textversionen. Die von Nikolaus Heidelbach herausgegebene und illustrierte Ausgabe (Weinheim/Basel 1995 u. ö.) vermerkt hinter jedem Text die Auflage, aus der das Märchen stammt; die von Günter Jürgensmeier herausgegebene Edition (Düsseldorf 2007) bietet den Text der Ausgabe letzter Hand von 1857 zusammen mit einem nützlichen Register.

Wissenschaftlichen Ansprüchen genügen zurzeit vor allem drei Texteditionen: Die von Heinz Rölleke (Frankfurt 1985), die den vollständigen Text der dritten Auflage von 1837 bietet, mit einer informativen Editionsgeschichte der Grimm-Märchen, sehr knappen Einzelkommentaren, einer Auswahl der originalen Grimm-Anmerkungen und den Märchentexten der anderen Auflagen.

Von Heinz Rölleke stammt auch eine Neuedition der Ausgabe letzter Hand von 1857 (Stuttgart 1980), die einen Neusatz der Textbände und den faksimilierten Anmerkungsband von 1856 umfasst, mit ausführlichen Kommentaren und einer umfangreichen Bibliografie.

Eine dritte wichtige Edition ist die von Hans-Jörg Uther (Hildesheim/Zürich/New York 2004): Sie bietet neben einer kurzen Editionsgeschichte einen kompletten Reprint der wichtigen zweiten Auflage der Grimm-Märchen von 1819 und umfasst auch den dritten 1822 erschienenen Band mit den forschungsgeschichtlich bedeutenden Anmerkungen der Grimms.

Anfang Juli 2010 konnte die deutschsprachige Wikisource die Transkription aller großen Ausgaben bis zur siebten Auflage 1857, der Ausgabe letzter Hand, abschließen. Scans und E-Texte sind dort parallel einsehbar. Die Handexemplare der Erstauflage von 1812/15 lassen sich inzwischen ebenfalls online einsehen.









</doc>
<doc id="14578" url="https://de.wikipedia.org/wiki?curid=14578" title="Mitteltönige Stimmung">
Mitteltönige Stimmung

Unter mitteltönigen Stimmungen versteht man eine Reihe von temperierten Stimmungen, die in der Renaissance, im Barock und vielfach auch in späterer Zeit (bis in das 19. Jahrhundert) hauptsächlich für Tasteninstrumente gebräuchlich waren.
Die mitteltönige Stimmung mit ihren vielen reinen Terzen verwirklicht fast vollkommen die reine Stimmung für Tasteninstrumente – allerdings nur für eine begrenzte Zahl von Tonarten.

Wie bei der reinen Stimmung ist die Verwendung der charakteristischen reinen großen Terzen (mit dem Frequenzverhältnis formula_1) grundlegend, für die die reinen Quinten der pythagoreischen Stimmung leicht verengt werden. Die reine Terz gleicht das syntonische Komma (Frequenzverhältnis: formula_2) dadurch aus, dass vier aufeinander folgende Quinten um je formula_3 des syntonischen Kommas erniedrigt werden. Bei der reinen Stimmung wird die große Terz (formula_1) aufgeteilt in einen "großen Ganzton" (formula_5) und einen "kleinen Ganzton" (formula_6) (mit rationalen Frequenzverhältnissen), bei der mitteltönigen Stimmung – daher der Name – jedoch in zwei gleich große Ganztöne mit dem irrationalen Frequenzverhältnis formula_7.

Um die Wolfsquinte zu vermindern oder zu vermeiden, wurde die strenge Mitteltönigkeit in vielen Versuchen modifiziert, wobei aber gleichzeitig die reinen Terzen erhöht "(geschärft)" werden.

Bei der mitteltönigen Stimmung werden elf Quinten des Quintenzirkels jeweils um so viel vermindert, dass die sich aus vier dieser Quinten ergebenden großen Terzen rein oder annähernd rein werden. Bei der gebräuchlichsten und am häufigsten beschriebenen Variante ist die große Terz rein. Die vier Quinten werden daher um je formula_3 des syntonischen Kommas verkleinert. Mit anderen Worten: Verkleinert man die 11 Quinten um formula_3 des syntonischen Komma, so werden die benutzbaren Terzen exakt rein. Die so entstandene Stimmung ist die "formula_3-(syntonisches)-Komma-mitteltönige Stimmung".
Ein Hörbeispiel der leicht verstimmten mitteltönigen Quinten findet sich hier.

Hinweis: Reine Intervalle sind durch ganzzahlige Frequenzverhältnisse charakterisiert, temperierte Intervalle haben dagegen meist ein irrationales Frequenzverhältnis. Deshalb erfolgt der Größenvergleich mit der Einheit Cent. 

Beispiel: In der reinen Stimmung ist die reine Terz aufgeteilt in einen großen und einen kleinen Ganzton, bei der mitteltönigen Stimmung hingegen in zwei gleich große Ganztöne.

In den angegeben Tonarten erhalten wir wegen der reinen Terzen in Tonika, Subdominante und Dominante eine auffallend gute Klangqualität, die jedoch dadurch getrübt wird, dass in anderen Tonleitern unbrauchbare Intervalle entstehen.
Die zwölfte „Quinte“, die den Quintenzirkel abschließt, ist in Wahrheit eine verminderte Sexte (in der Regel Gis-Es), die stark von der reinen Quinte abweicht, und in der Regel musikalisch unbrauchbar ist. Sie wird häufig „Wolfsquinte“ genannt. Vier vermeintlich große Terzen, deren Quintenkette die Wolfsquinte enthält, sind verminderte Quarten (Cis-F, Fis-B, Gis-C, H-Es), die ebenfalls in der Regel nicht als große Terzen gebraucht werden können. Es bleiben daher acht reine große Terzen.

Die mit (!) gekennzeichneten Intervalle sind nur in der enharmonischen Verwechslung Terzen und Quinten und man sieht, dass nur die folgenden Dreiklänge spielbar sind:

Es-Dur, B-Dur, F-Dur, C-Dur, G-Dur, D-Dur, A-Dur und E-Dur sowie c-Moll, g-Moll, d-Moll, a-Moll, e-Moll, h-Moll, fis-Moll und cis-Moll.

Diese Akkorde wurden in der Lasso-Palestrina-Lechner-Cavelieri-Zeit (um 1600) voll ausgeschöpft, aber ganz selten zum Beispiel der As-Dur- oder der f-Moll-Dreiklang.

Um den As-Dur-Dreiklang spielen zu können, bräuchte man neben der Taste für Gis eine Taste für As 41 Cent höher;
um den H-Dur-Dreiklang spielen zu können, bräuchte man neben der Taste für Es eine Taste für Dis 41 Cent tiefer u.s.w.

Siehe unter dem Stichwort Cent (Musik) die "Tabellen der Quinten und Terzen in der mitteltönigen, wohltemperierten, gleichstufigen und pythagoreischen Stimmung".

Um weitere Tonarten spielbar zu machen wurden - auf Kosten der reinen Terz - wohltemperierte Stimmungen entwickelt, die in letzter Konsequenz zur gleichstufigen Stimmung unserer Tasteninstrumente führte.

Andere bekannte, jedoch geschichtlich in der Stimmpraxis nur selten bis kaum nachzuweisende mitteltönige Stimmungen sind die formula_14-, formula_15-, formula_16-, und formula_17-Komma-mitteltönige Stimmung, bei denen die 11 Quinten um den entsprechenden Bruchteil des syntonischen Kommas verkleinert werden. Die Verminderung des Missklangs der Wolfsquinte führt jedoch gleichzeitig zu einer Verminderung der Reinheit der „guten“ großen Terzen.

Spricht man gemeinhin von mitteltöniger Stimmung, so ist meistens die formula_3-Komma-mitteltönige Stimmung gemeint. Nur bei ihr sind die großen Terzen exakt rein. Die formula_3-Komma-mitteltönige Stimmung lässt sich relativ leicht realisieren, wenn man lernt, die vier temperierten Quinten genau zu stimmen. Die anderen Töne ergeben sich dann über das Einstimmen reiner großer Terzen.

Die mitteltönige Stimmung mit ihren vielen reinen Terzen nähert die reine Stimmung mit lauter reinen Terzen in Kadenzen am besten an.

Wie bei der reinen Stimmung unterscheidet man bei der mitteltönigen Stimmung zwischen dem diatonischen, großen Halbton mit 117,108 Cent und dem chromatischen, kleinen Halbton mit 76,049 Cent.

Intervalle der formula_3-Komma-mitteltönigen Stimmung

Hier gilt auch die Regel des Weißenburger Kantors Maternus Beringer (1610):

Für die musikalische Praxis ist der Wechsel von großen und kleinen Halbtönen in der mitteltönigen Stimmung folgenreich. So hat der Einsatz von chromatischen Abschnitten mit unterschiedlichen Halbtonschritten eine expressive Wirkung. Leittöne nach oben (cis, dis, e, fis, gis und h) werden tief und Leittöne nach unten (des, es, f, as und b) hoch intoniert, und die am häufigsten auftretenden Mollterzen sind recht klein. Kadenzen bekommen bei mitteltöniger Stimmung daher einen besonderen, insbesondere von der gleichstufigen Stimmung abweichenden Charakter (Beispiel im Kasten rechts).

Vergleich gleichstufig, mitteltönig und reine Stimmung (in Cent)
Ausführliche Tabelle: Intervalle der formula_3-Komma-mitteltönigen Stimmung

formula_3-Komma-mitteltönigen Stimmung in chromatischer Ordnung:

Frequenzverhältnisse zum Grundton C mit der mitteltönigen Quinte formula_23

Während die große (pythagoreische) Terz im Mittelalter meist als Dissonanz wahrgenommen wurde, bildete sie (als reines Intervall) ab der Renaissance eine wichtige Konsonanz.

Auch wenn man vereinzelte Quellen des 15. und frühen 16. Jahrhunderts bereits als praktische Beschreibung der mitteltönigen Stimmung ansehen kann, wurde sie erstmals 1571 durch Gioseffo Zarlino korrekt und eindeutig beschrieben. Im deutschen Sprachraum war es Michael Praetorius, der sie 1619 in seiner „Organographia“ ("Syntagma musicum", Band 2) als gängige Praxis beschrieb und drei Arten angab, wie man sie praktisch legen konnte (neben einer nicht bedeutsamen Modifikation, die jedoch keine Tonart zusätzlich ermöglicht). Aufgrund Praetorius’ Beschreibung wurde die mitteltönige Stimmung bis ins 18. Jahrhundert gern als „Praetorianisch“ bezeichnet. Im Orgelbau wurde sie in weiten Teilen Deutschlands bis weit in das 18. Jahrhundert als Standardstimmung verwendet – in einzelnen Regionen noch darüber hinaus –, weshalb in Orgelbauverträgen und Prüfungsberichten (Abnahmeberichten) die Stimmung nicht bezeichnet zu werden brauchte.

In Norddeutschland ist die mitteltönige Stimmung zum Beispiel für sämtliche Orgeln Hamburgs 1729 in gedruckten Quellen belegt, und auch die von Arp Schnitger neu erbaute Orgel des Bremer Doms stand noch bis zur Umstimmung 1775 bis 1776 in der mitteltönigen Stimmung. Neuere Forschungen haben auch wieder plausibel gemacht, dass die Orgeln, die Dieterich Buxtehude in Lübeck zur Verfügung standen, in dieser Standardtemperierung standen. Es gibt im übrigen keinerlei Äußerungen Buxtehudes zu Stimmungsfragen – sein Widmungsgedicht für Andreas Werckmeisters "Harmonologia Musica" von 1702, einer Kontrapunkt- und Improvisationslehre, nimmt auch nicht auf Stimmungsfragen Bezug und kann nicht als Unterstützung Werckmeisterscher Stimmungsentwürfe gedeutet werden.

In Mitteldeutschland hingegen haben der Orgelbauer Christian Förner, seine Schüler (Zacharias Thayßner, Christoph Junge, Tobias Gottfried Trost) und Enkelschüler (darunter Tobias Heinrich Gottfried Trost und Johann Friedrich Wender) die mitteltönige Stimmung modifiziert oder durch eine wohltemperierte Stimmung ersetzt, um alle Tonarten gebrauchen zu können. Belegt ist diese Stimmung für die 1668–1673 erbaute Förner-Orgel der Schlosskirche in Weißenfels. Zacharias Thayßner hat 1677–1682 die Orgel der Stiftskirche St. Servatii in Quedlinburg gebaut, an der Andreas Werckmeister amtierte. Johann Friedrich Wender erbaute 1687–1691 die Orgel von Divi Blasii in Mühlhausen und 1699–1703 die Orgel der Bonifatiuskirche (heute Bach-Kirche) in Arnstadt. An diesen beiden Orgeln amtierte der junge Johann Sebastian Bach in den Jahren 1703 bis 1708. Sie ermöglichten es ihm, von Anfang an Orgelwerke zu schreiben, die weit über die Tonarten hinausgehen, welche die mitteltönige formula_3-Komma-Stimmung zulässt.

Die Wolfsquinte und die vier verminderten Quarten wurden im 17. und 18. Jahrhundert als völlig unbrauchbar angesehen. Vermutungen aus jüngerer Zeit, dass diese kompositorisch eingesetzt wurden (also etwa H-Es-Fis als vermeintliches H-Dur, F-Gis-C als vermeintliches f-Moll, etc.), werden jedoch durch Äußerungen der Quellen des 17. und 18. Jahrhunderts regelmäßig widerlegt.

Um den Tonartenvorrat der gewöhnlichen mitteltönigen Stimmung zu erweitern, wurden an Stätten professioneller Musikpflege in Westeuropa zwischen zirka 1450 und 1700 nicht selten Tasteninstrumente mit zusätzlichen Obertasten ausgestattet, denn
auf einer zwölfstufigen Skala sind in mitteltöniger Stimmung nur folgende Tonarten spielbar.
Damit sind die zwölf Tasten ausgeschöpft 



Der Unterschied zwischen Gis und As oder Dis und Es usw. beträgt 41 Cent (fast ein halber Halbton).

In der Regel wurden Instrumente mit ein bis zwei, seltener vier, beim Cembalo universale sogar sieben „Subsemitonien“, englisch "split keys", ausgestattet. Solche Instrumente sind verwandt mit den sogenannten enharmonischen Instrumenten, wie dem Orthotonophonium mit 72 Tasten pro Oktave.

Die Entwicklung begann offenbar in Italien und gewann schnell eine gewisse Verbreitung. Nördlich der Alpen war es erst Gottfried Fritzsche, der in Deutschland 1612 die erste Orgel mit Subsemitonien baute (in der Kurfürstlichen Schlosskapelle Dresden). Michael Praetorius beschreibt ein „Cembalo universale“ („Cimbalo cromatico“), das über 19 Töne pro Oktave verfügt: Neben den fünf geteilten Obertasten gibt es zusätzliche schmale Obertasten für das Eis und His.

Auf besaiteten Tasteninstrumenten setzten sich seit Ende des 17. Jahrhunderts langsam aber zunehmend wohltemperierte Stimmungen durch, auch praktische Annäherungen an die gleichstufige Stimmung, das heißt solche Stimmungen, die den Gebrauch aller Tonarten zuließen. Die wohltemperierten Stimmungen waren nicht die heute auf elektronischen Instrumenten und meist auf Klavieren zu hörende gleichstufige Stimmung, sondern solche, bei denen die einzelnen Tonarten mal mehr, mal weniger „gespannt“ klangen (Tonartencharakteristik, die auch im 18. Jahrhundert als subjektives Moment verstanden wurde).

Lange konnte man nur vermuten, dass Bach bei der Transposition älterer Werke (!) und teilweisen Neukomposition von Präludien und Fugen der beiden Bände des „Wohltemperirten Claviers“ an die damals noch ganz neuen ungleichstufigen wohltemperierten Stimmungen gedacht hat, auch wenn die gleichstufige Stimmung, praktisch gelegt in seiner späteren Lebensphase nicht auszuschließen ist. Zu beachten ist auch, dass Friedrich Suppig 1722 in einem Manuskript beschrieb, dass alle "Claviere" in Dresden mitteltönig gestimmt seien – im gleichen Jahr, als Bach den ersten Band des Wohltemperirten Claviers zusammenstellte und mit dem datierten Titelblatt versah. Nach einer neuen, aber noch umstrittenen Deutung um das Jahr 2000 kann die Girlande auf dem Titelblatt zum Wohltemperierten Clavier als Stimmungsanweisung interpretiert werden.

Die Geschichte der mitteltönigen Stimmung ist zwar in ihren theoretischen Verästelungen recht gut bekannt, jedoch ist die praktische Anwendung, Verbreitung und der offenbar vielfach erst viel später als bisher angenommen erfolgende Übergang zu neueren Stimmungen (oft direkt zu Annäherungen an die gleichstufige Stimmung) in vielen Regionen erst in Ansätzen erforscht, da man allzu oft annahm, dass sich theoretische Stimmungsentwürfe alsbald auch in der Praxis durchsetzten. Wie jedoch Werckmeister und andere, die neue Stimmungen entwarfen, beklagten, folgten die Orgelbauer ihren Entwürfen nicht und blieben noch lange bei der mitteltönigen Stimmpraxis, ausgenommen mitteldeutsche Orgelbauer in der Nachfolge von Christian Förner.

Die mitteltönige Stimmung stellte die günstigste Annäherung an das Netz reiner Quinten und reiner Terzen der reinen Stimmung dar. Für die Begleitung von vokaler, instrumentaler und gemischt vokal-instrumentaler Musik bot sie lange Zeit die beste Voraussetzung. Außerdem waren im Gottesdienst Choräle und deren Vorspiele in Kirchentonarten mit Leichtigkeit mitteltönig zu begleiten. Es gab aus der kirchenmusikalischen Praxis heraus lange Zeit keinen Bedarf für eine Welle von Umstimmungen. Gewisse Probleme in der Begleitung von Ensembles ergaben sich jedoch durch die Existenz verschiedener Stimmtonhöhenstandards: In Deutschland etwa standen Orgeln um 1700 gemeinhin im (gemeinen) Chorton (a′ = ungefähr 465 Hertz) oder gelegentlich im Hohen Chorton (a′ = ungefähr 495 Hertz), während die meisten Instrumente und Sänger im Kammerton (a′ = ungefähr 415 Hertz) musizierten. (Zum Vergleich: In gleichstufiger Stimmung mit a′ = 440 Hertz ist gis′ = 415,3 Hertz, b′ = 466,2 Hertz und h′ = 493,9 Hertz). Vom Organisten war hier gefordert zu transponieren, wobei sich leicht ergab, dass die Grenzen der Mitteltönigkeit erreicht oder auch überschritten wurden. Solange dies nicht ständig geschah, konnte der Begleiter „Wolfs“-Töne auslassen, vielleicht umspielen oder mit einer Verzierung versehen (wodurch jedoch der Ton auch hervorgehoben werden kann), auch durch geeignete Registerwahl den hässlichen Ton verdecken. Gegen Ende des 17. Jahrhunderts war die musikalische Entwicklung der Ensemblemusik so weit fortgeschritten, dass die mitteltönige Stimmung vielfach nicht mehr als geeignet erschien. Hier setzte nun die Entwicklung neuer Stimmungen ein. Sie entsprang also nicht aus Forderungen, in solistischen Werken für Tasteninstrumente „entfernte“ Tonarten zu verwenden.

Reine Quinten, Oktaven und Terzen konnte man ohne Weiteres einstimmen. Die Quinten in der mitteltönigen Stimmung mussten jedoch um formula_3 Komma enger gelegt werden. Dafür gab es Anweisungen für die Beobachtung von "Schwebungen". Dabei musste man jedoch beachten, dass die Anzahl der Schwebungen pro Zeiteinheit umso größer ist, je höher die Quinten liegen. Nach dem Temperieren von vier etwas engeren Quinten konnte man die Stimmung durch eine reine Terz überprüfen. Die weiteren Töne ließen sich durch reine Terzen leicht stimmen. Hatte man beispielsweise C-G, G-D, D-A und A-E temperiert, konnten die weiteren Töne durch reine Terzen erzielt werden: D-Fis, Es-G, E-Gis, F-A, G-H, A-Cis und B-D. Waren alle zwölf Töne innerhalb einer Oktave gestimmt, vervollständigte man das gesamte Tonspektrum des Instruments durch reine Oktaven. Die alten Orgelbauer haben ihre Instrumente ohne Stimmgerät gestimmt. Als physikalische Geräte standen ihnen nur das Monochord, die Stimmpfeife und das Pendel sowie ihr eigener Pulsschlag zur Verfügung.

Berechnungen der Schwebungen
Erläuterung: Ist die Grundfrequenz formula_26, dann hat die reine Quinte darüber die Frequenz formula_27.

Die mitteltönige Quinte mit der Frequenz formula_28 liegt / Komma darunter:

Bei reinen Quinten ist der 3. Teilton formula_30 (Oktave + Quinte) des Grundtones identisch mit dem 2. Teilton formula_31 (Oktave) der Quinte. Die Frequenz der Schwebung bei temperierter Quinte errechnet sich dann aus der Differenz dieser Obertöne:

In unserem Beispiel berechnet sich aus a′ = 440 Hz die Frequenzen von e′ vorwärts und von d', g und c′ rückwärts folgendermaßen:

Die reine Terz hat keine Schwebungen, die Pythagoreische Terz <nowiki>c′e′</nowiki> (c′ = 260,74 Hz; e′ = 330 Hz) jedoch formula_38 (Schwebungen in der Sekunde), also etwa zehn Mal so viel wie bei den mitteltönigen Quinten, und wurde deshalb als Missklang empfunden.

"Grundton: C, Beginn des Quintenzirkel bei Es".

Das Frequenzverhältnis des syntonischen Kommas ist formula_39, das der Quinte formula_40.

Jede der 11 mitteltönigen Quinten Q ist eine um formula_3-syntonisches Komma verkleinerte reine Quinte.

Ihr Frequenzverhältnis ist demnach formula_42.

Abkürzungen: Ok=Oktave, Q = die formula_3-Komma-mitteltönige Quinte

Der Quintenzirkel der formula_3-Komma-mitteltönigen Quinten geht nicht auf. Die zwölfte Quinte Dis unterscheidet sich vom Beginn des Quintenzirkels Es um ein Intervall – kleine Diesis genannt – mit dem Frequenzverhältnis formula_45 (ca. 1/5 gleichstufiger Ganzton).

Alle möglichen Intervalle der mitteltönigen Stimmung findet man im Abschnitt Tonstruktur.
Somit erhalten wir folgende Intervalle:

Bei der mitteltönigen Stimmung stehen nicht alle erhöhten bzw. erniedrigten Töne zur Verfügung. Im obigen Beispiel nur Es, B, Fis, Cis und Gis, nicht aber deren enharmonische Wechseltöne Dis, Ais, Ges, Des und As. Dasselbe gilt auch für die enharmonischen Wechseltöne der übrigen Töne; zum Beispiel stehen auch Fes und Eis nicht zur Verfügung.

Die enharmonischen Wechseltöne – zum Beispiel Es und Dis – unterscheiden sich um die kleine Diësis von 41 Cent. Um dasselbe Intervall sind auch drei übereinandergelegte reine große Terzen kleiner als die Oktave. Auch bei den Varianten der mitteltönigen Stimmung, bei der die benutzbaren großen Terzen nur annähernd rein sind, bleibt ein großer Unterschied zwischen den enharmonischen Wechseltönen bestehen.

Man kann daher nur in Tonarten annehmbar spielen, in denen die fehlenden Töne nicht benötigt werden.






</doc>
<doc id="14583" url="https://de.wikipedia.org/wiki?curid=14583" title="Akkord">
Akkord

Ein Akkord ist in der Musik das gleichzeitige Erklingen unterschiedlicher Töne, die sich harmonisch deuten lassen.

Der Begriff "Akkord" leitet sich vom französischen "accord" (beide erst „Übereinkunft, Übereinstimmung der Gefühle“, dann auch „musikalischer Zusammenklang“) ab, das seinerseits auf das vulgärlateinische "*accordō, accordāre" „in Übereinstimmung bringen, anpassen, harmonisieren“ (von lateinisch "cor" „Herz“). Daneben wirkte, wohl schon seit dem Spätlateinischen, das griechische χορδή ("chordḗ" „Saite“) auf das Wort ein und bestärkte vielleicht die Anwendung auf den musikalischen Bereich. Ursprünglich stand der Begriff für den „Gleichklang“, also für die Erzeugung desselben Tones auf verschiedenen Saiten. So wurde er schon 1619 von Michael Praetorius verwendet. Erst 1732 tauchte der Begriff im Sinne von „Zusammenklang verschiedener Töne“ auf, der sich auch auf das Erklingen der Obertonreihe eines einzelnen Tones beziehen lässt.

In der realen Musik bilden sich Akkorde aus dem Zusammenklang vieler Stimmen, die verschiedene Funktionen haben: Melodie, Bass, Begleit-, bzw. Füllstimmen. Am deutlichsten wird die Behandlung der Akkorde in einzelnen Stimmen im vierstimmigen Satz. Es können aber auch in einer einzelnen Stimme Töne erklingen, die vom Hörer als gemeinsame Bestandteile einer harmonischen Struktur erlebt werden. Damit sind auch Akkordbrechungen (Arpeggien) und der langsame Aufbau von Zusammenklängen (z. B. Rachmaninoff, Melodie Op. 3 Nr. 3, vorletzter Takt) Akkorde im beschriebenen Sinne. Wie Akkorde in der Musik konkret verwendet werden, hängt vom Genre ab. Als gegensätzliche Pole kann man hierbei die Polyphonie (waagerechte Struktur, mehrere unabhängige Stimmen, etwa in einer Fuge) und die Homophonie (senkrecht-akkordische Struktur, etwa in einem Lied mit Gitarrenbegleitung) ansehen.

Normalerweise bezeichnet man erst mindestens drei unterschiedliche Töne als Akkord. Allerdings gibt es auch Zusammenklänge nur zweier Töne (Zweiklänge), die trotzdem akkordische Funktionen erfüllen. Es fehlt dann häufig die Quinte zum vollständigen Dreiklang. Da aber im natürlichen Obertonspektrum die Quinte bereits vorhanden ist, kann ein derartiger Zweiklang einen Dreiklang vollwertig vertreten. Häufig werden Zweiklänge (engl. dyad) in der Rockmusik in Form eines Powerchord, also nur Grundton und Quinte ohne die Terz, angewandt. Weiter gibt es die Quartenharmonik mit Strukturen, bei denen eine reine Quarte deutlich bevorzugt wird. Die relativ junge Quartenharmonik ist als bewusster Gegensatz zur traditionellen Terzenharmonik zu sehen. 

Von Dreiklang spricht man, wenn die drei erklingenden Töne sich – ggf. nach Oktavversetzungen – im Terzabstand über dem Grundton des Akkords „schichten“ lassen, beispielsweise "c-e-g". 

Für Akkorde der "Terzenharmonik" gilt: 

Nicht in Terzen geschichtete Akkorde oder so geschichtete, deren Grundton gar nicht erklingt, können gedanklich dem jeweiligen musikalischen Zusammenhang entsprechend zu terzgeschichteten Akkorden ergänzt oder aber anderweitig (z. B. durch Vorhalte) erklärt werden. So kann etwa der Dreiklang "e-g-b" in entsprechendem Zusammenhang als Dominantseptakkord "c-e-g-b" fungieren. So etwas ist aber im Zusammenhang des Stücks zu prüfen, da es oft mehrere Deutungsmöglichkeiten gibt.

Beim Zählen der unterschiedlichen Töne in einem Akkord werden lediglich verschiedene Tonnamen unabhängig von ihrer Tonhöhe berücksichtigt. Nach dieser Transformation werden die enthaltenen Töne des Akkordes als Intervalle zum tiefsten Ton des Akkordes benannt, im genannten Beispiel als Terz und Quinte zum tiefsten Ton, obwohl sie tatsächlich als Quint und Dezime vorlagen.
Die konkrete Bezeichnung des Akkordes hängt vom Benennungssystem ab.

Die Bezeichnung „Lage“ hat im Zusammenhang mit Akkorden zwei Bedeutungen:


Zur Bestimmung der Umkehrung eines Akkords ist der tiefste Ton (der Basston) entscheidend, unabhängig davon, ob der Akkord in enger oder weiter Lage erklingt.

Die Umkehrungen erhalten ihre Namen nach charakteristischen Intervallen, die vom tiefsten Ton aus gemessen werden. Die einzelnen Bezeichnungen (siehe unten) entstammen der Generalbass-Technik.

 Dreiklänge können in Grundstellung und in zwei Umkehrungen auftreten:


 Septakkorde können in Grundstellung und in drei Umkehrungen auftreten: 


Andere Vierklänge, die keine Septakkorde sind, lassen sich selbstverständlich ebenso umkehren, nur gelten bei ihnen nicht die obigen Bezeichnungen für die einzelnen Umkehrungen. Als Beispiel hierfür sei der Dreiklang mit hinzugefügter Sexte (Sixte ajoutée) genannt, der in seiner Grundstellung prinzipiell ein Quintsextakkord ist, aber eine völlig andere Funktion als die identisch aufgebaute 1. Umkehrung eines Septakkords hat, so dass man diese Begriffsverwirrung vermeiden sollte.

Als Fünf- und Mehrklang bezeichnet man einen vierstimmigen Akkord, dem ein (Fünfklang) oder mehrere Töne im Terzabstand (None, Undezime, Tredezime) der Tonleiter hinzugefügt wurden. Diese Klänge sind meist sehr farbig durch Sekundreibungen und werden kompositorisch für besondere Stimmungen verwendet. Manchmal in der Klassik (hier wohl am geläufigsten als Dominantseptnonenakkord), weitaus häufiger aber im Jazz, da sich hier durch die den Drei- und Vierklängen hinzugefügten Töne (im englischen auch "Tension Notes") Grundlage für die spannungsgeladene Jazzharmonik bildet.

"Hauptbeitrag: Fünfklang"

"Hauptbeitrag: Dominantseptnonakkord"

"Hauptbeitrag: Cluster"

Für die Benennung von Akkorden sind mehrere, unabhängige Systeme gebräuchlich, die im Folgenden kurz angerissen werden:

Die Benennungssysteme sind in der Reihenfolge aufgeführt, in der sie historisch entstanden sind. Jedes neue System hat einen Großteil der Errungenschaften der alten Systeme (speziell im Hinblick auf die Syntax der Modifikationen gegenüber dem Grunddreiklang) übernommen und weiter entwickelt. Die Grundstrukturen von Akkorden werden deshalb im ältesten Beschreibungssystem, dem Generalbass, genannt, in den darauf folgenden nicht mehr, obwohl sie dort ebenfalls verwendet werden. 

Diese Art der Beschreibung wird vor allem im Generalbass verwendet. Der Akkord heißt nach den Intervallen, die die enthaltenen Töne zum tiefsten Ton einnehmen. 

Die Benennung der Stufen ist eine Weiterentwicklung der Benennung über den Basiston, die im Gegensatz zu dieser die tonale Einordnung des Akkordes in den harmonischen Kontext beschreibt. 

Es können alle oben genannten Akkordtypen in entsprechender Weise identifiziert werden, wobei der Bezugston nicht ein konkreter Basiston, sondern stattdessen die Nummer dieses Basistones in der Tonleiter der Grundtonart ist. 

Beispiele in C-Dur:


Dieses System der klanglichen Beschreibung wird in der Stufentheorie verwendet.

Akkorde bestehen aus dem Tonvorrat, der vom jeweiligen musikalischen Kontext zur Verfügung gestellt wird. Im Falle der traditionellen westlichen Musik sind das die zwölf Töne der chromatischen Tonleiter und ihre Wiederholungen in verschiedenen Lagen. 

Da die traditionelle westliche Musik zum größten Teil auf einer Grundtonart basiert, ergibt sich ein Kerntonvorrat aus den 7 sogenannten leitereigenen Tönen. 

Um im musikalischen Ablauf zeitweilig zu anderen Tonarten zu wechseln, kann dieser Tonvorrat um die Töne erweitert werden, die abweichend von der Grundtonart in den anderen Tonarten vorkommen. Tatsächlich geschieht die Modulation (der Wechsel) durch die Einführung von leiterfremden Tönen. Die typischsten Erweiterungen, die zu Alterationen von leitereigenen Tönen zu leiterfremden Tönen führen sind die kleine Septime und die übermäßige Quarte. 

Das Alterieren vom h zum b wird als harmonischer Schwenk in Richtung der im Quintenzirkel nächsten Tonart empfunden, die diesen Ton zu ihren leitereigenen zählt. 

Dieser Schwenk wird auch empfunden, wenn noch kein Klang auf der Basis F-Dur gebildet wird, sondern zum Beispiel lediglich ein Septakkord über c ("c - e - g - b") erklingt. Modulationen zu weiter entfernten Tonarten erweitern entsprechend den Tonvorrat, aus dem Akkorde gebildet werden können.

Während der Generalbass und die Stufentheorie den Tonvorrat der Grundtonart zum Ausgangspunkt ihrer Benennung machen, lassen sich die Akkorde auch direkt als Repräsentanten einer Tonart auffassen. Dafür werden die enthaltenen Töne ausgewertet.

Beispiele:

Innerhalb eines Stückes in C-Dur ist

Diese Bezeichnung ist anschaulicher als „IV. Stufe“ oder „I. Stufe Sextakkord“, verzichtet aber darauf, den benannten Klang in den harmonischen Kontext des Stückes einzugliedern.

Die Funktionstheorie beschreibt Akkorde anhand der Verwandtschaftszusammenhänge, die sich aus dem Quintenzirkel ergeben. Auch hier lassen sich die oben beschriebenen Akkordtypen verwenden und durch weitergehende Vier-, Fünf- und Mehrklänge erweitern. 

Basis des Klanges ist die Tonart, die durch ihren funktionalen Zusammenhang zur Grundtonart benannt wird. Da der Basiston des Akkordes damit nicht beschrieben ist, wird zusätzlich eine Angabe zur Umkehrung oder zum Intervall gemacht, das der Basiston im Verhältnis zum Grundton der beschriebenen Funktion hat. 

Die oben genannten Beispiele führen zu folgenden Bezeichnungen:

"Siehe Hauptartikel Akkordsymbol" 

Jazz-Noten werden üblicherweise als "Leadsheets" mit Gesangsstimme und Akkordsymbolen verbreitet. 

Das Jazz-Akkordsymbol als Notation entspricht im weitesten Sinne einer Generalbassnotation, bei der der Grundton explizit als Notenname in Buchstabenform angegeben wird. Ein vom Grundton abweichender Basston wird durch einen Schrägstrich abgetrennt zusätzlich bestimmt (siehe: Slash-Akkord). Akkordmodifikationen werden durch Angaben zum Klanggeschlecht, Ziffern und andere Kurzbezeichnungen angeben.

Die jeweiligen Benennungssysteme spiegeln das Harmonieverständnis der Zeit wider, in der sie entstanden sind. Während die Generalbassnotation eine pragmatische Abkürzung im Schriftbild darstellt, baut die Stufentheorie einen ersten harmonischen Zusammenhang auf, der von der Funktionstheorie extrem erweitert wird. Die Jazz-Notation übernimmt die bis dahin entstandene Syntax, gibt aber den im Jazz nicht immer notwendigen Funktionszusammenhang vollständig auf. 

Am Beispiel des verminderten Septakkordes über cis ("cis - e - g - b") in einem C-Dur-Zusammenhang sei gezeigt, wie sich diese Unterschiede äußern:


Der verminderte Septakkord liegt außerhalb der üblichen Kadenz, ist aber auch für Bach nicht ungewöhnlich. Man kann erkennen, dass der Generalbass durch den Verzicht auf jede Erklärung den für ihn nicht alltäglichen Klang problemlos notiert, während die Stufentheorie ihr Basiskonstrukt (die Stufe) modifizieren muss und die Funktionstheorie weitere Informationen zur korrekten Bezeichnung benötigt. Die Notationsgewohnheiten im Jazz ähneln in ihrer Pragmatik dem Generalbass. 

Im Gegenzug dazu kann die Funktionstheorie ihre Vorteile realisieren, wenn es um die Beschreibung von Klängen geht, die in der Zeit des Generalbasses oder der Stufentheorie nicht denkbar waren. Das ist zum Beispiel bei Klängen der Fall, in denen die Terz gleichzeitig als große und als kleine Terz vorkommt, was funktional als Terz und übermäßige Sekunde/None notiert würde, oder bei solchen, die sich nicht mehr eindeutig auf einen Basiston beziehen lassen wie der aus Quarten geschichtete „Mystische Akkord“ von Alexander Skrjabin (c-fis-b-e’-a’-d’’) sowie der „Tristanakkord“ von Richard Wagner, mit dem die Oper „Tristan und Isolde“ beginnt.

In Liedern dienen Akkorde üblicherweise zur instrumentalen Begleitung. Sie geben der Melodie abschnittsweise einen harmonischen Bezug. Die Akkordbegleitung wird meistens mit einem polyphonen Tasten- oder Saiteninstrument (z. B. Klavier oder Gitarre) gespielt.

Die Abfolge von Akkorden wird Progression genannt. Falls sich die Akkordfolge immer zyklisch wiederholt (z. B. |: G, Em, C, D7 :|) spricht man von einem harmonischen Ostinato.

Die Töne eines Durakkordes zeichnen sich gegenüber anderen Tönen dadurch aus, dass sie die ersten ganzzahligen Unterteilungen der Schwingung des Grundtons darstellen.
Die Halbierung der Wellenlänge ergibt hierbei die erste Oktave, in Drittelung die zweite Quint und Fünftelung die dritte große Terz des Grundtons. Die nächsthöhere bzw. -niedrigere Oktave dieser Töne ergibt sich durch Verdopplung bzw. Halbierung der Frequenz.

Diese Obertöne, die auch bei praktisch allen Klangerzeugungen bereits natürlicherweise zusammen mit dem Grundton erklingen, werden dabei als harmonisch zum Grundton passend wahrgenommen. Die realen Obertöne hängen auch vom klangerzeugenden Instrument ab und sind nur näherungsweise hier als Beispiel angeführt. 

Da sich in der reinen Stimmung eines Tasteninstrumentes diese Verhältnisse nicht für alle Grundtöne genau stimmen lassen, wird seit dem 19. Jahrhundert meistens die gleichtemperierte Stimmung verwendet, die nur für die Oktaven ein exaktes Frequenzverhältnis sicherstellt. 
Die zwölf Zwischentöne einer Oktave werden dabei so gewählt, dass das Frequenzverhältnis zum nächsten Halbton immer identisch ist. 
Hieraus ergeben sich zwar leichte Abweichungen des Frequenzverhältnisses zu Terzen und Quinten um wenige Cent, doch ist so jeder Ton als Grundton eines Akkordes nutzbar.




</doc>
<doc id="14584" url="https://de.wikipedia.org/wiki?curid=14584" title="Schlacht bei Murten">
Schlacht bei Murten

Die Schlacht bei Murten wurde am 22. Juni 1476 zwischen Truppen der Eidgenossenschaft und des burgundischen Herzogs Karl des Kühnen im Rahmen der Burgunderkriege ausgetragen.

Karl der Kühne war seit 1467 Herzog von Burgund und beherrschte ein ausgedehntes Reich zwischen Frankreich und dem Heiligen Römischen Reich. 1474 verwickelte sich Karl in eine Reihe von Kriegen mit Frankreich, dem Haus Habsburg und der alten Eidgenossenschaft, die als Burgunderkriege in die Geschichte eingegangen sind und die zu seinem Tod und zum Untergang seines Reiches führten.

Karl verhandelte 1473 in Trier mit dem Kaiser Friedrich III. über die Erhöhung seines erweiterten Herzogtums zu einem neuen Königreich Burgund. Zudem strebte er nach dem Titel des Rex Romanorum, der ihn zum Nachfolger als Kaiser empfohlen hätte. Friedrich verlangte im Gegenzug die Vermählung von Karls einziger Erbin, Tochter Maria, mit seinem Sohn Maximilian, was Karl jedoch ablehnte. In der Folge griff er Neuss am Rhein an, mit dem Ziel, die wichtige Bischofsstadt Köln zu erobern und von da aus die rheinischen Städte südwärts einzunehmen.

Unter Einfluss des französischen Königs Ludwig XI. löste Herzog Siegmund 1474 die an den burgundischen Hofe verpfändeten Gebiete im Elsass und im Breisgau mit einem Kredit der Städte um Basel aus. Die Eidgenossenschaft schloss mit Siegmund ein dauerhaftes Bündnis ab, die sog. Ewige Richtung. In der Folge erklärten die Eidgenossen Burgund den Krieg und erweiterten diesen auf das Herzogtum Savoyen, als sich dieses weigerte, auf Druck der Verbündeten in Bern und Freiburg seinerseits gegen Burgund den Krieg zu eröffnen. Um Verstärkungen der Burgunder aus der Lombardei zu verhindern, besetzten Bern und Freiburg in der Folge die burgundische Freigrafschaft und die savoyische Waadt, während der mit Bern verbündete Bischof von Sitten das Unterwallis besetzte.

1476 unternahm Karl von der Freigrafschaft Burgund aus einen Feldzug gegen das Territorium der Eidgenossen. Ziel war vor allem die Unterwerfung der Städte Bern und Freiburg. Zu Karls Aufgebot gehörten zahlreiche Bogenschützen, ausserdem verfügte sein Heer über hunderte Kanonen. Hinzu kamen Armbrustschützen, schwere Reiterei und einige Soldaten, die mit frühen Arkebusen ausgerüstet waren. Zuerst plante Karl gegen Bern vorzugehen, das er zu Recht als treibende Kraft hinter der anti-burgundischen Liga erkannte. Am 28. Februar 1476 nahm er nach kurzer Belagerung die Stadt Grandson am Neuenburger See ein und liess die eidgenössische Besatzung von 412 Mann, die sich auf die Zusicherung freien Geleits ergeben hatte, an den Bäumen aufknüpfen. Die kurze Zeit der Belagerung hatte Bern genutzt, um ein grösseres Aufgebot zusammenzustellen und Karl entgegenzuziehen. Am 2. März 1476 kam es in der Schlacht bei Grandson zum ersten grossen Treffen, bei dem Karls Truppen eine erste Niederlage im Kampf gegen die eidgenössische Infanterie hinnehmen mussten.

Den Eidgenossen gelang die Erbeutung von über 400 burgundischen Geschützen. Mangels Kavallerie konnten die Eidgenossen den fliehenden Burgundern jedoch nicht nachsetzen, was es Karl erlaubte, mit «nur» zirka 1000 Mann Verlusten aus dieser Schlacht herauszukommen (von einem Heer von zirka 20'000 Mann). Karl konnte seine zwar geschlagenen, aber nicht vernichteten Truppen wieder neu sammeln und ordnen, was eine weitere Schlacht notwendig machte, um den Krieg zu entscheiden.

Bern als treibende Kraft hinter dem anti-burgundischen Bündnis versuchte vergeblich, die Eidgenossenschaft dazu zu bewegen, nach der Schlacht bei Grandson den geschlagenen Burgundern nachzusetzen und die Waadt zu besetzen. Selbst als Karl bereits in Lausanne wieder ein neues Heer sammelte und sich offensichtlich für einen neuen Feldzug gegen Bern rüstete, versagten die Eidgenossen jeder Präventivaktion ihre Unterstützung. Als symbolische Massnahme wurden rund 1000 Mann unter dem Hauptmann und späteren Zürcher Bürgermeister Hans Waldmann in die Stadt Freiburg gelegt. Für den Fall, dass der Herzog von Burgund in ihr Gebiet einfallen sollte, erhielt die Stadt Bern aber die Zusicherung, dass weitere eidgenössische Truppen ihr zu Hilfe kommen würden. Am 14. Oktober 1475 willigte Freiburg i. Ue. auf das Drängen der Berner ein (gegen den Rat der Eidgenossen), zusammen mit Bern die Stadt Murten zu besetzen. Bereits am folgenden Tag standen Bern und Freiburg vor Murten. Bern forderte von Murten, sich freiwillig zu ergeben und «Berner» zu werden. Andernfalls müssten «sy darumb liden dass inen an Lib und Gut übel keme». Auf das Versprechen von Freiburg, dass Murten selbständig bleiben dürfe, wenn es sich ergebe, gaben die Murtener ihren Widerstand auf, forderten jedoch, dass nur eine Besatzung von Freiburgern unter dem Kommando von Wilhelm Perrotet stationiert werde. Nach der Schlacht bei Grandson stationierte Bern eine Garnison von 1500 Mann unter Adrian I. von Bubenberg in der gut befestigten Stadt, um die dort stationierte Freiburger Garnison zu unterstützen. Damit waren beide Wege nach Bern, über Freiburg und Murten, für Karl vorläufig gesperrt. Ohne eine Belagerung einer der beiden Städte war Bern nicht zu erreichen. Auf diese Weise war sichergestellt, dass bei einem Angriff auf Bern genügend Zeit für einen Zuzug der Eidgenossen zur Verteidigung der Stadt zur Verfügung stehen würde.

Karl der Kühne besass das modernste Heer im damaligen Europa. Es war ein Söldnerheer bestehend aus Infanterie, Kavallerie und Artillerie. Bei der Infanterie sind die englischen Bogenschützen zu erwähnen, die einen hervorragenden Ruf genossen. Karl verfügte ausserdem über die modernste Artillerie Europas. Im Mai 1476 erliess Herzog Karl in Lausanne eine Militärordonnanz zur Neuorganisation seines Heeres in vier aktive und ein Reservekorps. Kommandeure der vier aktiven Korps waren der Herzog von Afry, der Prinz von Tarent, der Graf von Marle und Jakob von Savoyen, Graf von Romont. Das Reservekorps wurde von Anton Bastard von Burgund geführt.

Die Eidgenossen und ihre Verbündeten hatten ihre eigene Kampftaktik, die sich seit den ersten Schlachten gegen die Habsburger im 14. Jahrhundert herausgebildet hatte. Im Kampf gegen schwere Reiterei und gepanzerte Kämpfer formierten sie sich zu Gewalthaufen. Als wichtige Neuerung konnten sie erstmals in Grandson mit etwa 5 m langen Spiessen aus Eschenholz eine Art «Igelwand» bilden, welche von der Kavallerie nicht durchbrochen werden konnte. Machiavelli verglich diese Kampftaktik der Eidgenossen mit der griechischen Phalanx, damals wurde zeitweise eine ähnliche Waffe, die Sarissa, verwendet. Eine wichtige Waffe der Eidgenossen war die seit Morgarten bewährte Halbarte, eine kürzere Stangenwaffe mit durchschlagender Wirkung. Handfeuerwaffen kamen auf beiden Seiten zum Einsatz, waren aber wegen der Unausgereiftheit des Zündungsmechanismus und der Zielungenauigkeit von untergeordneter Bedeutung.

Nach der Schlacht bei Grandson erliess die eidgenössische Tagsatzung am 18. März 1475 erneut eine Kriegsordnung. Ziel war die Stärkung des Zusammenhalts der Truppe im Feld, um eine vermehrte Zusammenfassung aller Kräfte auf das Hauptziel einer Schlacht zu erreichen, nämlich die völlige Vernichtung der gegnerischen Streitmacht. Insbesondere wurde es untersagt, Kriegsgefangene zu machen, um Lösegeld zu erhalten. Es hatte sich nämlich gezeigt, dass die Kämpfer dadurch stark abgelenkt wurden und die Masse der Feinde deshalb entfliehen konnte. Hauptschwäche der Eidgenossen blieb der fehlende Oberbefehl. Jede Truppe hatte ihre eigenen Führer, es gab lediglich eine Versammlung der Hauptleute, die über Strategie und Taktik entschied.

Nach der Niederlage bei Grandson flüchtete Karl nach Lausanne. Der Fürstbischof von Lausanne stand unter dem Einfluss der Herzöge von Savoyen, die mit Karl verbündet waren. In Lausanne sammelte Karl in kurzer Zeit auf der "Plaine du Loup" ein neues Heer. Insbesondere musste er seine ganze Artillerie neu aufbauen, die er bei Grandson verloren hatte.

Im Juni stiess er durch die Waadt in Richtung Bern vor. Der Weg über Grandson-Neuenburg kam nicht in Frage, weshalb er sein Heer südlich des Neuenburger Sees vorstossen liess. Grundsätzlich gab es zwei Wege, auf denen ein grosses Heer mit Tross und Artillerie nach Bern gelangen konnte: Entweder über Payerne-Murten oder über Romont-Freiburg. Beide Städte waren mit Garnisonen versehen. Weshalb sich Karl schliesslich für den Weg über Murten entschloss, ist ungeklärt. Wahrscheinlich war ein Zusammenstoss seiner Vorhut mit einer Abordnung der Besatzung von Murten bei Avenches dafür entscheidend.

Am 9. Juni traf Karl vor Murten ein. An den folgenden zwei Tagen liess er die Stadt durch sein langsam eintreffendes Heer umzingeln und eine Belagerung aufbauen. Um nach Bern zu gelangen, war diese Belagerung zwar nicht unbedingt nötig – die Strasse lag etwas südlich der Stadt –, es wäre aber zu riskant gewesen, bei einer Belagerung Berns die 2000 Berner aus Murten im Rücken zu wissen. Die Aufstellung der burgundischen Truppen ist genau überliefert. Drei Korps lagen um Murten, zwei Korps als Reserve im burgundischen Lager südlich und östlich des Bois Domingue, einer dominierenden Anhöhe in der Ebene vor Murten, auf der das Hauptquartier Karls des Kühnen lag. «Bodemünzi», die spätere Bezeichnung der Anhöhe, entstammt nicht etwa wie weithin angenommen dem schweizerdeutschen «"z‘ bode müend si / zu Boden müssen sie"», sie ist nur eine Abwandlung des französischen «Bois Domingue». In östlicher Richtung liess Karl ein mit Artillerie bestücktes Hindernis errichten, das in der Schweizer Geschichte den Namen «Grünhag» erhalten hat. Damit sollte das Belagerungsheer vor einem Überraschungsangriff aus Bern geschützt werden.

Der Herzog von Burgund liess am 12. Juni zudem Voraustrupps in Richtung Bern marschieren, um die Brückenköpfe Laupen über die Sense und Gümmenen über die Saane zu erobern. Diese Vorstösse wurden von Bern aber blutig zurückgewiesen. Mit dem Vorstoss auf bernisches Gebiet traten nun die Hilfsverträge mit der Eidgenossenschaft in Kraft.

In der Zwischenzeit hatte die Belagerung der Stadt begonnen und die Artillerie hatte bereits einige Türme zerstört. Die Belagerten leisteten unter der energischen Leitung Adrians I. von Bubenberg heftigen Widerstand. Bern hatte praktisch die gesamte erbeutete Artillerie aus Grandson nach Murten gebracht, weshalb die Belagerung sich für die Burgunder zu einem verlustreichen Unternehmen entwickelte. Der anhaltende Widerstandswille wurde auch durch das Beispiel des Schicksals der Besatzung von Grandson gestützt – es war von vornherein klar, dass die Besatzungstruppe eine Niederlage nicht überleben würde. Dennoch war die Lage angesichts der starken Belagerungsgeschütze auf längere Sicht aussichtslos. Der schwerste Angriff erfolgte am Abend des 18. Juni. Die Verbindung mit Bern konnte über den Seeweg jedoch gehalten werden, und Adrian von Bubenberg schrieb nach Bern, dass ein solcher Angriff nur schwer nochmals abgewendet werden könne.

Das Lager der Berner von 5'000–6'000 Mann lag bei Ulmiz, wo ab dem 19. Juni nach und nach Truppen aus dem bernischen Herrschaftsgebiet, aus der Eidgenossenschaft und von den Verbündeten z. B. aus Thun eintrafen. Die Stadt Thun erhielt als Dank von Bern anstatt des schwarzen einen goldenen Stern in ihr Wappen. Von der Schweizer Geschichtsschreibung werden diese Zuzügler oft vergessen. Neben dem Grafen von Greyerz und dem Herzog René II. von Lothringen waren Kontingente der Städte Strassburg, Colmar, Schlettstadt und Rottweil sowie das Kontingent der vier vorderösterreichischen Waldstädte und der Grafschaft Hauenstein zugezogen. Besonders wertvoll war für die Eidgenossen die lothringische Reiterei, da sie selbst über keine nennenswerte Kavallerie verfügten. Zuletzt trafen rund 2000 Zürcher nach einem legendären dreitägigen Gewaltmarsch in der Nacht auf den 22. Juni in Ulmiz ein.

Die Eidgenossen planten, die Schlacht am Zehntausend-Ritter-Tag, dem 22. Juni, zu schlagen, da ihre mangelhafte Lebensmittelversorgung eine längere Wartezeit nicht zuliess. Ihr Heer umfasste mit den Verbündeten rund 22–24'000 Mann, davon rund 1800 Berittene. Das burgundische Heer war mit rund 22'500 Mann etwas kleiner und inhomogener, auch weil darunter zahlreiche Nichtkämpfende waren. Dafür verfügten die Truppen Karls über weit bessere Bewaffnung.

In den Wäldern östlich von Murten versammelten sich die Eidgenossen mit lothringischen Reitern unter dem Kommando von Hans von Hallwyl und Hans Waldmann.

Die burgundischen Aufklärungstruppen hatten das Herankommen der Eidgenossen zwar wahrgenommen, doch Karl ignorierte alle Warnungen, nachdem er am 21. Juni persönlich das eidgenössische Lager in Augenschein genommen hatte und mehrmals vergeblich in die verregneten Stellungen einrücken liess. So hatten die Eidgenossen den Vorteil des Überraschungsmoments auf ihrer Seite.

Am Morgen des 22. Juni erkundete ein 1'300 Mann starker berittener Spähtrupp unter Wilhelm Herter, Friedrich von Fleckenstein und Veltin von Neuenstein in aller Frühe das Terrain und die burgundischen Stellungen. Der Spähtrupp wurde zwar bemerkt, kehrte aber von burgundischer Seite unbehelligt ins Holz zurück, einer der Fehler, die Galeatto, einer von Karls Offizieren, später auflistete.

Petermann Etterlin als Augenzeuge berichtete das weitere Vorgehen: „denn da man kam zu dem Holz, da begann man anfangen, die Ordnungen zu machen; da war ein strenger, notfester Ritter, genannt Herr Wilhelm Herter, der damals (1475–1476) beiden Herren von Österreich und Lothringen Dienstmann war, der ward zu einem obersten Hauptmann gesetzt, der fing an und machet und ordnet die Ordnung“.

Danach schlug Oswald von Thierstein eine Reihe von Führungsleuten zum Ritter. Die Zeitverzögerung erregte so nachhaltigen Unmut, dass mehrere der so Geritterten später ihren Titel nicht in Anspruch nahmen. Hans Waldmann dagegen wurde nach einer Angabe Gerold Edlibachs von Wilhelm Herter erst nach der Schlacht zum Ritter geschlagen.

Um 12:00 Uhr des 22. Juni 1476 versammelten sich die Eidgenossen in Schlachtordnung und begannen den Vormarsch durch den Birchenwald, der den Galmwald und das Murtenholz verbindet. Im Angesicht des Feindes wurde das Schlachtgebet verrichtet. Laut mehreren Berichten habe der die ganze Nacht anhaltende Regen um diese Zeit aufgehört und die Sonne wurde als gutes Zeichen aufgefasst. Zu diesem Zeitpunkt lagen in der burgundischen Stellung nur die Artillerie und drei Ordonnanzkompanien.

Die ganzseitige Abbildung der Schlacht im Zürcher Schilling zeigt das Schlachtgeschehen über mehrere Zeitpunkte und aus Sicht beider Parteien.

Die Kirchturmuhr von Murten zeigt den Schlachtbeginn an: 12:00 Uhr. Als „Houptsecher“ steht die Niedere Vereinigung, erkennbar an ihren Bannern, im Zentrum. Die auf Grund des Bündnisfalles involvierten Eidgenossen, ebenfalls an ihren Bannern erkennbar, stossen von links oben hinzu. Die Hauptleute der grossen Abteilungen sind an ihren roten Jacken zu erkennen. Die Eidgenossen (links oben) werden von Hans Waldmann, den Spiess nach sich ziehend, angeführt. Rechts von ihm, eine Halbarte schulternd, läuft Kaspar von Hertenstein. Darunter schreitet im Harnisch mit der Armbrust Hans von Hallwyl. Weiter links, am unteren Rand der Niederen Vereinigung, kämpft Wilhelm Herter mit dem Spiess, ganz in Rot gekleidet und einen federgeschmückten Hut tragend. Wilhelm Herter ist durch seinen ihm folgenden schwarz uniformierten Gardisten zweifellos identifizierbar. Oben rechts in der Gruppe führt Oswald von Thierstein, erkennbar an seinem Federbusch, die Reiterei an. Die fliehenden Burgunder und die Belagerungstruppen des Herzogs von Savoyen sind nicht differenziert dargestellt.

Auf der zeitnahesten Abbildung der Schlacht von Murten, dem Holzschnitt des Meisters B. in der steht Wilhelm Herter, erkennbar an der Hutfeder, rechts neben dem Berner Bannerträger.

Der Kampf begann mit einer Kanonade und einem Schützenfeuergefecht. Laut dem Chronisten Petermann Etterlin stiess die Reiterei durch den dichten Pulverrauch vor, musste allerdings auf die Vorhut warten, die entlang der Strasse vormarschierte. Da das Feuer aber zu hoch lag, wurden nur wenige Reiter getroffen. Zunächst gelang der Vorhut der Durchbruch durch den Grünhag – eine Palisade – nicht, da sich die rund 2'000 dort stationierten Burgunder vehement zur Wehr setzten. Mehreren Berichten zufolge scheiterte der erste Angriff.

Während die burgundische Reiterei trotz der Überzahl der Eidgenossen angriff, gelang den Schwyzern unter Führung ihres Landammanns Dietrich in der Halden eine Umgehung des Grünhages. Vermutlich waren sie aus der Vorhut der ersten Angriffswelle rechts ausgebrochen und durch den tiefen sogenannten Burggraben der Artillerie in den Rücken gefallen. Zur selben Zeit wurde im burgundischen Lager Alarm gegeben. Ungefähr 4000 herbeieilende Truppen konnte Troylo da Rossano noch sammeln, um sie in die Schlacht zu werfen. Doch der Gewalthaufen der Eidgenossen war bereits auf den Grünhag zu gelaufen und hatte ihn niedergedrückt. In der Panik feuerten die Burgunder ihre Büchsen viel zu früh ab. Nun konnte die Masse der Halpartiere (ungefähr 15'000) über das burgundische Lager herfallen. Die Vorhut ging südlich von Murten gegen das II. Korps der Lombarden los und drängte sie in den See. Die Garnison der Stadt Murten unternahm ebenfalls einen Ausfall gegen die Lombarden und versuchte mit zwei Schiffen, die schwimmend Flüchtenden zu töten. Die Reiterei griff, während der Gewalthaufen auf das Lager bei Meyriez losging, die Stellungen des Herzogs auf dem Bodemünzi an. Die englischen Bogenschützen und die Bogenschützen der Garde versuchten hier noch Widerstand zu leisten, doch wurden sie von der Reiterei überrumpelt und ihre Hauptleute, Grimberghe und Georges de Rosimbois fanden ebenfalls den Tod.

Inzwischen versammelte Karl der Kühne den Rest seiner Truppen, wahrscheinlich die Berittenen des I. und III. Korps, zur Flucht. Die Beute fiel in Murten weitaus geringer als in Grandson aus, scheint aber dennoch beachtlich gewesen zu sein. Die eindrucksvollen Reste der sogenannten Burgunderbeute können noch heute zerstreut vor allem in historischen Museen der Schweiz, Österreichs und Deutschlands besichtigt werden. Das bedeutendste Konvolut findet sich im Historischen Museum in Bern.

Für die sterblichen Überreste der ums Leben gekommenen Soldaten wurde 1485 das Beinhaus zu Murten errichtet, das bis 1798 bestand.

Mit dem Sieg in der Entscheidungsschlacht der Burgunderkriege bereiteten die Eidgenossen das Ende des burgundischen Staates vor. Sie ebneten Frankreich den Weg zur Vormacht in Westeuropa, und das Haus Österreich konnte als Folge seine Besitzungen in den Niederlanden erwerben.

Die Eidgenossen etablierten sich dank der Überlegenheit ihrer Fusstruppen als europäische Militärmacht und wurden von nun an häufig in fremde Dienste als Söldner angeworben. Die Geschichte der schweizerischen Söldner begann in dieser Zeit und dauerte bis zum Krimkrieg an.

Zum Andenken an diesen Sieg etablierte sich der 22. Juni in vielen eidgenössischen Orten als Schlachtfeiertag.

Eine Legende berichtet über einen eidgenössischen Läufer, der, ähnlich wie der ebenso sagenhafte Pheidippides, mit einem Lindenzweig in der Hand die Strecke von Murten nach Freiburg (zirka 17 km) ohne Unterbrechung lief, um den Ausgang der Schlacht zu verkünden. Nach seiner Ankunft konnte er noch die Worte „Sieg, Sieg!“ verkünden und brach anschliessend tot zusammen. An dieser Stelle wurde ein Baum gepflanzt, die sogenannte „Murtenlinde“, die nach ihrem Absterben 1985 durch ein Denkmal vor dem Rathaus ersetzt wurde. Aus der Murtenlinde konnte man siebzehn Abkömmlinge züchten, von denen man heute in Murten und in Freiburg je einen jeweils auf dem Rathausplatz besichtigen kann. Dem Läufer zu Ehren wird seit 1933 jedes Jahr der Murtenlauf durchgeführt.





</doc>
<doc id="14585" url="https://de.wikipedia.org/wiki?curid=14585" title="Pantheismus">
Pantheismus

Der Ausdruck Pantheismus oder Pantheïsmus (von altgriechisch "pān" „alles“ sowie "theós" „Gott“) bezeichnet die Auffassung, dass „Gott“ eins mit dem Kosmos und der Natur ist. Das Göttliche wird im Aufbau und in der Struktur des Universums gesehen, es existiert in allen Dingen und beseelt von daher auch alle Dinge der Welt bzw. ist mit der Welt identisch. Somit ist hier kein personifizierter Gott vorhanden. Deshalb wird häufig ein durch geistige Eigenschaften definierter Urgrund als einziges Grundprinzip (Monismus) angenommen. Der vonseiten der Theologie häufig vorgebrachte Einwand, dass der Pantheismus (deutsch auch „Allgottlehre“) identisch mit dem Atheismus sei, ist nur in dem Sinne gerechtfertigt, dass in der Tat kein von der Welt verschiedener Gott angenommen wird; keineswegs jedoch, dass überhaupt kein Gott bzw. göttliches Prinzip angenommen wird.

Schwierig zu unterscheiden vom Pantheismus ist der Kosmotheismus: Während sich das Göttliche für den Pantheisten in der Vielfalt der Welt einmalig und einzigartig ausdrückt, ist die Welt für den Kosmotheisten nur "eine" Erscheinungsform des göttlichen Seins, neben der es noch andere geben könnte.

Der Begriff entstand in der Zeit der Aufklärung und geht auf den britischen Philosophen John Toland zurück, der ihn 1709 als Ausdruck seiner religiösen Überzeugung schuf. Er postulierte, „es gebe kein von der Materie und diesem Weltgebäude unterschiedenes göttliches Wesen, und die Natur selbst, d. h. die Gesamtheit der Dinge, sei der einzige und höchste Gott.“ 1720 schrieb er sein Werk "Pantheisticon", in dem er Ideen aus der Orphik mit solchen des Hylozoismus kombinierte.

In der zweiten Hälfte des 18. Jh. wurden „Spinozismus“ und „Pantheismus“ oft synonym gebraucht, denn Baruch de Spinoza hatte eine Gleichsetzung von Gott und Natur („Deus sive Natura“, „Gott bzw. Natur“) vertreten. In den Pantheismusstreit, der von Friedrich Heinrich Jacobi 1785 mit seiner These der Übereinstimmung von Pantheismus und Atheismus ausging, waren als seine Kontrahenten berühmte Aufklärer, wie Moses Mendelssohn, Johann Gottfried Herder und Immanuel Kant, verwickelt.

Monotheistische Denker, die an einen persönlichen Gott glaubten, wandten die Zuschreibung Pantheist polemisch gegen Autoren, die den von ihnen postulierten Unterschied zwischen Gott und der Welt bzw. der Natur nicht hinreichend betonten. Sie bezeichneten alle Schriftsteller und Gelehrten, die von Spinoza beeinflusst waren, abwertend als „Pantheisten“, so etwa Johann Wolfgang Goethe und zahlreiche Vertreter der Romantik und des Biedermeier.

Jean Guitton (1901–1999) schrieb, jeder Atheismus sei eine Form von Pantheismus, da der Gottesbegriff "irgendwie" in die Welt hinein gelegt werde. Laut Geo Widengren entwickelt sich aus dem Pantheismus der Polytheismus.

Bereits in der Antike entwickelten die Vorsokratiker eine Naturphilosophie, die auch Seele und Göttliches miteinbezog. Auch Platons Kosmologie der Weltseele hat pantheistische Züge. Der Neuplatoniker Plotin betonte das " All-Eine" und war damit ein direkter Vorgänger der Pantheisten. Die Stoiker betrachteten den Logos als universelles Vernunftprinzip, das Göttliche, welches auch in jedem Menschen war. Im Mittelalter gab es, anknüpfend an Plotin, vereinzelt pantheistische Tendenzen, z.B. bei Nicolaus Cusanus. In der frühen Neuzeit betrachtete Giordano Bruno das Göttliche als Teil des ewigen Kosmos, wobei sich Göttlichkeit in allen Dingen offenbare.

Auch aus den ethnischen Religionen nicht-europäischer Kulturen sind pantheistische Vorstellungen bekannt, so etwa die als Kitchi Manitu bezeichnete, den gesamten Kosmos durchdringende "Große Kraft" der Algonkin-Indianer oder Wakan Tanka, ein sehr ähnliches Konzept der Sioux-Indianer Nordamerikas. Als Schöpfer eines sufischen Pantheismus gilt der im 9. Jahrhundert lebende persische Mystiker Bāyazīd Bistāmī.

Im 20. Jahrhundert gehörten Frank Lloyd Wright, Neale Donald Walsch und Arnold Toynbee zu den Vertretern des Pantheismus. Auch Albert Einstein („Gott würfelt nicht“) stand pantheistischem Denken nahe, hat er sich selbst doch nicht nur als konfessionslos, sondern explizit auch als Spinozist verstanden. Mit dem wachsenden Bewusstsein für Umweltproblematiken im späten 20. Jahrhundert erstarkte der Pantheismus, unter anderem auch als Alternative zu Christentum und reinem Atheismus.

Laut Eigendarstellung der Freireligiösen Bewegung gibt es unter den Freireligiösen auch Pantheisten und pantheistische Gottesvorstellungen. In Österreich gibt es seit 2017 eine Pantheistische Kirche, wo Pantheismus als Religion betrachtet wird bzw. aus dem Pantheismus eine Religion gemacht wird.

Jean-François Leriget de La Faye (1674–1731) verfasste 1709 eine Streitschrift gegen den Pantheismus Tolands. Auch Gottfried Wilhelm Leibniz kritisierte Toland und seinen „Pantheismus“, da er über die Welt rede wie über Gott.

Arthur Schopenhauer (1788–1860) kritisierte Pantheismus als „Euphemie für Atheismus“: „Ein unpersönlicher Gott ist gar kein Gott, sondern bloß ein missbrauchtes Wort.“ Ebenso beschrieb zu Beginn des 21. Jahrhunderts Richard Dawkins Pantheismus als aufgepeppten Atheismus („sexed-up Atheism“).

Während der traditionelle Gottesbegriff im Theismus von einer völligen Unterschiedenheit von Gott und Welt ausgeht, glaubt der Pantheismus, die Welt mit Gott identifizieren zu können. Dagegen halten christliche Theologen daran fest, dass weder die Welt mit Gott noch Gott mit der Welt identifiziert werden könne. Wenn Gott im „Endlichen“ gründe, werde die Transzendenz Gottes – ein nach christlicher Überzeugung wesentliches Kennzeichen – aufgehoben.

Für die katholische Kirche entschied das 1. Vatikanische Konzil 1870, dass man Gott „als wirklich und wesentlich von der Welt verschieden verkünden“ müsse („praedicandus est re et essentia a mundo distinctus“, DS 3001).

Im Januar 2010 kritisierte der Vatikan den Pantheismus aufgrund dessen Verneinung einer menschlichen Überlegenheit über die Natur und warf Pantheisten vor, die Erlösung in der Natur und nicht in Gott zu suchen.




</doc>
<doc id="14588" url="https://de.wikipedia.org/wiki?curid=14588" title="Schrammelharmonika">
Schrammelharmonika

Die Schrammelharmonika ist ein chromatisch<nowiki>es</nowiki> Akkordeon mit drei Knopfreihen in B-Lage. Sie ist meist zweichörig und mit zwölf diatonischen Bässen ausgestattet.

Klanglich hebt sich die Schrammelharmonika durch den weicheren Klang vom modernen chromatischen Knopfakkordeon ab. Der Grund dafür kann zum Teil in den relativ kleinen Abmessungen und dem geringen Gewicht des Instruments gesehen werden, aber auch die handgemachten Stimmplatten und die Art der Stimmung tragen zu dessen besonderen Klangcharakter bei.

Die erste schriftliche Erwähnung findet das Instrument im Jahr 1854, als der Wiener Harmonikamacher Matthäus Bauer auf der Industrieausstellung in München neben einem Instrument mit Klaviertasten (Patent erstmals 1851) auch eines „mit halben Tönen, versehen mit dreireihiger Maschine“ und eben nicht wechseltönig, präsentierte. Laut Walter Maurer soll die ersten Ideen dazu ein Musiker namens Franz Walther entwickelt haben. Das älteste zurzeit bekannte und erhaltene Instrument stammt von 1874, nach 1954 bis 1970 wurden in Wien Schrammelharmonikas nur noch vereinzelt gebaut.

Alfred Mirek erwähnt das Instrument mit dem „"Wiener System"“ als Vorbild für die Entwicklung des heutigen Bajan ab 1870. In der Sowjetära wurde das Griffmuster als „Moskauer“ bezeichnet, im Gegensatz zum „Petersburger“, das diatonisch war.

Die meisten erhaltenen Instrumente stammen aus den 1920er- und 1930er-Jahren.

Erwähnung im Amtlicher Bericht über die Allgemeine Deutsche Gewerbe-Ausstellung zu Berlin im Jahre 1844. "II. Harmonikas [...] August Schopp, in Wien, zeigte durch die Einsendung einer Harmonika zu 30 Nthlrn. 20 Sgr., wie sehr auch dieses Instrument in den kürzen Jahren seines Entstehens sich hat vervollkommnen lassen, indem das Vorliegende, auf welchem von geübter Hand recht artige Musikstücke hervorzubringen sind, einen Umfang von etwa 4 Oktaven hat. Das Aeußere des Instruments ist mit Perlmutter und sonstigen Verzierungen höchst elegant ausgestattet."

Zwei Generationen Reisinger, Edmund Hochholzer, Josef Trimmel, Pospischil, Bauer, Pick, Adolf Regelstein, Franz Kuritka, Rudolf Barton sind neben Karl Budowitz (1882–1925) die wesentlichen Hersteller, dazu eine unbekannte Zahl von Handwerkern, die vermutlich aus gewerberechtlichen Gründen ihre Namen geheim hielten.

Die folgenden Angaben sind unvollständig und stammen aus dem Buch: Eine Wiener Knopfharmonika entsteht von Lisl Waltner, und sind nur im Buch weil sich Karl Macourek bei seinen Erzählungen auf dies Firmen bezog. Diese Angaben wurden aber zusätzlich in den noch vorhandenen Gewerbebüchern recherchiert.

Matthäus Bauer: Firmeninhaber Karl Bauer, geb. 1883 gest. 1946, Gegründet 1836 (Angabe gemäß Inserat), Gewerbeschein 1911–1950 (Geschäfirtttsführer Josef Reisinger), Wien VI Mariahilfer Straße 19/21

Albert Trimmel, geb. 1880 gest. 1953; Gewerbeschein 1910;

Emanuel Napravnik, geb. 1874 gest. 1951, Gewerbeschein seit 1912, Wittwenfortbetrieb (Maria, geb. 1887) seit 10. April 1952, Geschäftsführer Franz Napravnik, geb. 1909, Gewerbeschein 1954–1967, Wien XVI Habichergasse 4; Sohn Franz Napravnik; Harmonikatischler

Reisinger Josef geb. 1885, Gewerbeschein 1919–1958 Wien XV Oelweingasse 3

Josef Sagat, geb. 1895; Gewerbeschein 1927 Meisterprüfung 1935, Gewerberücklegung 1959; gestorben 1968.

Regelstein Adolf geb. 1876, Gewerbeschein 1931–1938, Wien XVI Liebhergasse 44

Wenzel Rudolf, geb. 1895, Gewerbeschein 1933–1966; Nachfolger von Zwirsch; Wien Ottakrinerstraße 164, Erzeugte noch bis 1966 Stimmplattenrohmaterial.

Kuritka Franz, geb. 1908, Gewerbeschein 1938–1965, Wien Liebhergasse 44 Nachfolger von Regelstein &Raab

Rudolf Pospischil, Gewerbeschein 1952–1998; Wien Benedikt Schellingergasse 11

Karl Macourek geb. 22. Januar 1928; Meisterprüfung am 26. November 1951; Gewerbeschein 19. Mai 1952–1998; Josefine Macourek Gewerbeschein für Handel mit Musikinstrumenten von 1957 bis 1998

Derzeit sind nur zwei selbständige Harmonikabauer bekannt, die auch einzelne Schrammelharmonikas bauen. Der gelernte Harmonikabauer Herfried Zernig aus Sebersdorf in der Steiermark fertigt auf Wunsch Schrammelharmonikas, die sich weitgehend an historischen Instrumenten orientieren, im Detail sind diese aber keine Kopie von Original-Instrumenten. Zernig weicht zumindest in den bis jetzt gefertigten Instrumenten in wesentlichen Details vom Original ab. Er verwendet heute erhältliche Stimmplatten und wachst diese auf. Früher war es generell üblich, diese auf Leder zu legen. Auch die Holzverbindungen wurden früher mit Warmleim ausgeführt, was heute auch möglich wäre und auch nicht schwieriger zu handhaben ist als synthetischer Weissleim. Die Bassmechanik orientiert sich eher an der diatonischen Harmonika als am historischen Original, wobei die Anordnung der Basstasten und der Tastenbelegung davon nicht beeinflusst wird. Basstasten sind zumindest in den bekannten Erzeugnissen nicht am Bassboden, sondern an der Vorderseite des Bassteils, wie das heute beim Akkordeon üblich ist. Der Balg ist nicht traditionell, sondern nach der Art einer traditionellen steirischen Harmonika. Musiker, die sich eine Harmonika heute nach historischen Vorbild anfertigen lassen, könnten aber auch diese Details einfordern. Ein Unikat wird aber jedes heute kopierte Instrument immer bleiben, bei dem sich erst im Nachhinein zeigen wird, ob es den Erwartungen einigermaßen gerecht wird. Jedoch sind die bekannten von Zerwig gefertigten Instrumente schöne Beispiele dafür, dass auch heute traditionelle Handwerkskunst im Harmonikabau möglich ist. Bemerkenswert ist auch, dass diese Instrumente bis auf einige Spritzgussgelenke für die Bassmechanik den Stimmplatten, Perlmuttknöpfen, Balgecken und Balgkartonfalten komplett bis auf die Rohmaterialien wie Holz, Draht, Schrauben, Leder, Filz und Leinen vom Harmonikabauer gefertigt werden. Neben Herfried Zernig fertigt auch Gerhard Grübel aus St. Martin am Tennengebirge unter dem Markennamen EDLER Harmonikas auf Bestellung Schrammelharmonikas.

Seit 1870 spielten die Geiger Johann und Josef Schrammel in Georg Dänzers Quartett zusammen mit Anton Strohmayer an der Kontragitarre. Dänzer war in Wien berühmt als Virtuose der G-Klarinette, es wurden Ländler, Polkas und die legendären „alten Tanz“ gespielt. Nach 1873, dem Jahr des großen Börsenkrachs und einer darauf folgenden weitreichenden „Zurück zur Natur“-Bewegung, hieß die exakt selbe Besetzung Die Schrammeln – nachdem die beiden Geiger ihre Studien beendet und den Wunsch nach Karriere in der Hochkultur aufgegeben hatten. Sie bereisten ganz Europa und näherten durch ihre Virtuosität und den strengen mehrstimmigen Satz die Volksmusik der „Klassik“ an. Johann Strauß (Sohn) und Kronprinz Rudolf waren bekennende Fans. Die Aufführungen der Schrammeln waren ausverkauft; sie erfanden den Musiktourismus in seiner heutigen Form: was heute der Buschauffeur in Grinzing, waren damals die Fiaker.

1890 starb Georg Dänzer, aus Mangel an guten Klarinettisten ersetzte ihn Anton Ernst, ein Cousin von Johann Schrammels Frau. Das war der erste bekannte „Schrammel“-Harmonikaspieler. Von ihm sind mehrere sehr gute Quartettarrangements und eine Harmonikaschule erhalten.

In kürzester Zeit etablierte sich diese Besetzung (zwei Geigen, Harmonika, Kontragitarre) als Schrammel-Quartett und ist mit der dazugehörigen Schrammelmusik bis heute in Wien kammermusikalische Tradition.




</doc>
<doc id="14590" url="https://de.wikipedia.org/wiki?curid=14590" title="Diatonisches Akkordeon">
Diatonisches Akkordeon

Das diatonische Akkordeon (auch "diatonische Harmonika") ist ein Akkordeon mit diatonischem und wechseltönigem Diskant sowie wechseltönigem Bass. Als Knopfgriffakkordeon gehört es zur Familie der Handzuginstrumente. Die Anzahl der Tasten kann in weiten Grenzen variieren.

Der erste Entwicklungsschub fand in Wien statt. In vielen Teilen der Welt sind daher diatonische Akkordeons unter der Bezeichnung „Wiener“ oder „Wiener Modell“ bis heute bekannt. Die Entwicklung der verschiedenen Modelle war bis ca. 1860 bereits im Wesentlichen abgeschlossen.

Heute werden in Mitteleuropa kaum mehr Handzuginstrumente mit durchschlagenden Zungen in reiner Handarbeit gefertigt. Die handwerkliche Fertigung von chromatischen und diatonischen Zuginstrumenten war jedoch noch in der ersten Hälfte des letzten Jahrhunderts weit verbreitet. Österreich spielte eine wesentliche Rolle in der Entwicklung der diatonischen Wiener (später der Steirischen) Harmonika und der chromatischen Schrammel- oder Budowitzer Harmonika. Später erlangten Castelfidardo in Italien, der Musikwinkel im Vogtland von Deutschland und Louny in Tschechien Bedeutung. In Deutschland, Tschechien und Italien werden auch heute noch Stimmplatten und Handzuginstrumente in zum Teil handwerklicher und maschineller Erzeugung hergestellt.

Außerdem werden auch viele diatonische Zuginstrumente in Slowenien, der Schweiz (Schwyzerörgeli), in Österreich, hier vor allem in den Bundesländern Steiermark und Kärnten, erzeugt. Im Wesentlichen werden viele Instrumente mit Zulieferteilen aus Italien, Deutschland und Tschechien gefertigt. In Klingenthal und Umgebung (Deutschland) gibt es selbst heute noch eine Handvoll guter Handwerker. In Wien gibt es jedoch keine Zuginstrumentenbauer mehr, die eine Schrammelharmonika fachgerecht reparieren und bauen können.

In der vorindustriellen Epoche mit ihren Handwerksbetrieben, die zur teilweisen industriellen Fertigung überging, kam es zu einer starken Verbreitung dieses Instruments. Die Firma Hohner hatte im 20. Jahrhundert daran einen wesentlichen Anteil.

Bereits im 19. Jahrhundert wurde ein Großteil der Instrumente exportiert. Auswanderer nahmen Instrumente in die verschiedensten Teile der Erde mit. Es entwickelte sich ein reger Handel, besonders mit den USA. Unter anderem bezeugt ein Katalog von C. Bruno & Son aus dem Jahre 1881 umfangreiche Importe aus Europa. Dieser Katalog hat über hundert Abbildungen. Der Zweite Weltkrieg und die Verbreitung der elektronischen Orgel führten jedoch zu einem rapiden Rückgang der Produktion wie auch der Exportzahlen.

Betrachtet man aber die Steirische Harmonika, so lässt sich seit etwa 1975, als die erste Rosenzopfschule erschien, ein stetiger Aufschwung beobachten. Allein in Österreich dürften sich die derzeit im Jahr produzierten Instrumente schätzungsweise auf 8000 Stück beziffern. Der 1874 in Klagenfurt gegründete Betrieb Novak produziert selbst heute noch Instrumente handwerklich.

Lubas war eine der frühesten Firmen, sie hatte in Laibach (Slowenien) und im österreichischen Klagenfurt ihren Betrieb. Angeblich ist Lubas auch die Firma, welche die ersten Helikonbässe eingebaut hat. Josef Fleiß führte bis 1943 den Betrieb in Slowenien. Nach dem Zweiten Weltkrieg lernte Peter Müller und andere bei Fleiß das Handwerk. In Slowenien gibt es heute eine große Anzahl kleiner Betriebe, die Harmonikateile fertigen oder spezifische Arbeiten an Harmonikas ausführen. Ausgebildet wurden die meisten noch im Melodia Werk (Ljubljana) dem Nachfolgebetrieb von Lubas. Eine wichtige Rolle spielte auch die Firma Halaváček in Louny (Tschechien). In Louny besteht bis heute ein Betrieb, der Stimmplatten erzeugt.

Auch in den USA baute ein Auswanderer (Anton Mervar, Button Accordion Manufacturer) Steirische Harmonikas in Cleveland.

Weiterführende Geschichte bei den jeweils bekannten Harmonikabauern:
Harmonikabauerliste

Die Steirische Harmonika findet heute hauptsächlich in der Volksmusik in Österreich, Südtirol, Tschechien, Slowenien und Bayern, aber auch in anderen Ländern Verwendung.

Die "Club-Handorgel" ("Deutsches Club-Modell", auch "diatonische Handharmonika") ist eine diatonische, wechseltönige Harmonika mit zwei Tonarten und Halbtönen in einer dritten Reihe.

In Deutschland wird in der traditionellen Volksmusik häufig zur Liedbegleitung oder in Harmonikaorchestern das Club-Modell gespielt, das auf der mittleren Reihe in der Mitte eine einzelne Taste besitzt, die auf Ziehen und Drücken den gleichen Ton spielt (Gleichton). Häufigste Tonart dieses Modells ist die Kombination C/F. Am meisten verbreitet sind die Instrumente der Firma Hohner. Jedoch werden sowohl Schwyzerörgeli und Steirische Harmonikas mit einer Tastenbelegung am Diskant und am Bass angeboten, die den Hohner Club-Modellen entspricht, ein genereller Standard für die Tastenbelegung ist nicht vorhanden. Manche Club-Modelle wurden auf der Bassseite mit einer erweiterten Anzahl von diatonischen Bässen angeboten. Sehr oft werden diese Modelle auch mit Registerumschaltung am Diskant verwendet.

Das Club-Modell entspricht somit baulich einer dreireihigen diatonischen Harmonika mit angepasster Tastenbelegung und wird daher von den meisten Herstellern als Variante angeboten. Die unten angegebene externe Webseite zeigt die Tastenbelegung.

Im "Musikverlag Holzschuh" in Manching gibt es als Lehrbehelf die "Neue Holzschuh-Schule" (gründlicher und leicht fasslicher Lehrgang für Handharmonika von Alfons Holzschuh).

Das Wiener Modell, das mit zwei Reihen ohne Gleichtöner ausgestattet ist, erfreut sich zunehmender Beliebtheit als Instrument vor allem für Live-Musik zu Volkstanzveranstaltungen, die ursprünglich von Frankreich aus inspiriert wurden („Bal Folk“, „Tanzhaus“). Hochwertige Instrumente vor allem aus Italien und Frankreich und eine Weiterentwicklung der Spieltechnik haben zur Attraktivität dieses Instruments beigetragen. Die übliche Notenschrift wird häufig mit einer französischen Tabulatur unterlegt.




</doc>
<doc id="14592" url="https://de.wikipedia.org/wiki?curid=14592" title="Steirische Harmonika">
Steirische Harmonika

Die Steirische Harmonika ist ein diatonisches, wechseltöniges Handzuginstrument, das noch in der Volksmusik vor allem in Bayern, Österreich, Südtirol, Tschechien, Slowenien, auch in vielen anderen Ländern Verwendung findet. Der Unterschied zu anderen diatonischen Akkordeons besteht in der Verwendung der stark klingenden "Helikonbässe" und dem "Gleichton".

Das Wort „steirisch“ hat nur wenig mit dem Land Steiermark zu tun. Diese Bauart des Akkordeons wurde in Wien erfunden. Durch den diatonischen Aufbau ist sie besonders geeignet, alpenländische Volksmusik zu spielen, diese Musik wurde in Wien „steirisch“ genannt als Synonym für ländliche Musik, und daher wurde das neue Instrument "Steirische" genannt.

Andere gebräuchliche Bezeichnungen für die Steirische Harmonika sind: "Steirische", "Ziehharmonika", "Knöpferlharmonika", "Harmonika", "Harmonie", "Zugorgel", im Dialekt "Ziach", "Ziacha", "Ziachharmonie", "Ziachorgel", "Zugin" und "Quetschn", scherzhaft auch "Faltenradio", "Wanznpress", "Zerrwanst" oder "Heimatluftkompressor".

Gebaut werden Instrumente mit drei, oft vier, selten fünf, ausnahmsweise sechs Reihen, wobei jede Diskant-Reihe eine Tonart bedeutet. Im Zudruck erklingen die Dreiklangstöne der jeweiligen Tonart, (Dur-Dreiklang der Tonika,) im Aufzug erklingen die dazwischen liegenden Töne (zugehöriger Dominant-Sept-Akkord). Da der fünfte Ton der Tonleiter sowohl in der Tonika als auch in der Dominante vorkommt, ist wegen der leichteren Spielbarkeit dieser Ton in mittlerer Lage sowohl in Zug als auch in Druck eingebaut, wird also mit dem gleichen Knopf gespielt, mit Ausnahme der äußeren Reihe. Diese in den folgenden Reihen je einmal vorhandene Taste wird "„Gleichton“" genannt. In der Melodie vorkommende Töne, die nicht in dieses Schema passen, werden in den anderen Reihen gedrückt. Durch diese Bauweise ist bei dazu passenden (alpenländischen) Melodien eine äußerst flüssige Spielweise möglich.

Instrumente mit drei Reihen wären für Volksmusik ausreichend, sind auch leichter zu erlernen. Häufiger werden jedoch vierreihige Instrumente hergestellt, da die Spieltechnik für viele Griffe einheitlicher ist. Durch die zusätzliche vierte Reihe kommt ein hoher Ton pro Tonart und ein Halbton pro Oktave hinzu.

Für jede Reihe im Diskant gibt es auf der Bassseite, äußere Reihe einen Bassknopf und einen Akkordknopf, ebenfalls wechseltönig. Auf Druck erklingt die Tonika, auf Zug die Dominante.

Die Bass-Tasten der inneren Reihe werden auf Druck meist mit Übergangsbässen (Terzbässen) belegt, auf Zug erklingt in der inneren Bassreihe oft die vierte Stufe, die auf Druck ohnedies vorhanden ist. Nur auf Wunsch werden Mollbässe eingebaut, in der überlieferten Bauart auf Zug. Eine Wechselbasstaste für die erste Reihe fehlt. Bei neueren Modellen ist oft ein Wechselbass für die erste Reihe vorhanden. In diesem Fall ist häufig die letzte Basstaste in der zweiten Reihe als Wechselbass ausgeführt. Ein Übergangsbass (Terz) ist dann für die erste Reihe nicht vorhanden, an der Stelle, wo der Übergangsbass wäre, liegt die Wechselbasstaste. Ist ein zusätzlicher, X-Bass eingebaut, so gibt es eine weitere Taste in der ersten Basstastenreihe, die mit der Wechselbasstaste der zweiten Reihe gekoppelt ist. Es fehlt aber noch der Übergangsbass für die erste Reihe. Erst wenn ein sogenannter H-Bass eingebaut wird, ist auch der Übergangsbass für die erste Reihe vorhanden. Die Bezeichnung H-Bass hat "Florian Michlbauer" aus Weyregg am Attersee eingeführt.

Das Gewicht auf der Bassseite wird nur unwesentlich höher, wenn man drei- und vierreihige Instrumente vergleicht. Der größere Teil der Gewichtserhöhung entfällt auf die Diskantseite. Die Faustformel lautet: Je höher die Qualität der Stimmplatten, desto schwerer ist das Instrument. Die Gewichtsangaben der Hersteller sind oft nicht korrekt, doch sind Instrumente desselben Modells fast immer gleich schwer.

Aufgrund der diatonischen Bauweise ist es zumindest für den Anfänger schwierig, nach Noten zu spielen. Der Verlag Helbling ließ darum 1916 eine Tabulatur für die zweireihige diatonische Harmonika patentieren. "Max Rosenzopf", ein Musiklehrer aus Bärnbach in der Steiermark, hat dieses ältere System für die drei- und vierreihige Harmonika adaptiert, "Griffschrift" genannt, und 1975 im Verlag Preissler ein erstes Schulwerk nach diesem System herausgegeben, das bis 1996 18 Auflagen erreichte. Seither hat sich dieses Griffschriftsystem durchgesetzt, so dass sich kaum ein Harmonikaspieler mehr vorstellen kann, nach normalen Noten zu spielen. Durch das leichtere Erlernen hat seither die Verbreitung der Steirischen Harmonika wieder stark zugenommen.

Bedingt durch die zunehmende Beliebtheit der Steirischen Harmonika gibt es inzwischen Versionen der Griffschrift. Jede größere Musikschule hat eine eigene Lehrmethode, die sich von den anderen oft nur geringfügig unterscheidet.

Es gibt etliche Spieler und Schulen, die wie früher nach Noten oder nach Gehör spielen oder unterrichten.

Details zu diesen Spielweisen und Lehrmethoden finden sich auf Akkordeonschule.

Das typische Aussehen ist das Markanteste, sieht man vom typischen Helikonbassklang ab. Die Art der Basskonstruktion teilt sich die Steirische mit der tschechischen Heligonka.

Es werden in Österreich, Deutschland, Italien und Slowenien diverse Modelle hergestellt. Hersteller in anderen Ländern einschließlich Italien kopieren die traditionellen Instrumente. In Österreich und Bayern wird in den letzten Jahrzehnten vermehrt eine Vielfalt an Modellen angeboten, unterschiedlich sind häufig nur die äußere Aufmachung und die Art der Verzierung. Holzintarsien wurden schon immer benutzt. Auch sehr frühe Wiener Modelle waren mit kunstvollen Verzierungen versehen. Zu den bekanntesten Marken gehören Kärntnerland, Jamnik, Müller, Öllerer, Strasser, Novak (siehe weiter unten Harmonikabauer für Steirische Harmonikas (Auswahl)).

Traditionell wurden immer Metallverdecke und Metalldekorborten verwendet. Jeder Hersteller hat ein eigenes Muster, an dem er erkannt werden kann. In letzter Zeit werden bedingt durch die neuen technischen Möglichkeiten vermehrt Holzverdecke und Dekorbänder angeboten, die mit Laserschneidetechnik oder mit Wasserstrahlschneidetechnik maschinell vorgeschnitten sind. Gehäuseteile werden in vielen Holzarten und Lackierungen angeboten, auch gewachste Oberflächen sind bei manchen Erzeugern im Angebot. Hin und wieder werden auch farbige Zelluloidoberflächen gefertigt. Meist kommt aber eine Schleiflackoberfläche mit widerstandsfähigem Lack zum Einsatz. Die Fertigung der Schleiflackbeschichtung ist arbeitsintensiv, da mindestens sieben Lackschichten aufgetragen werden, wobei zwischendurch mehrfach die Oberfläche geschliffen und poliert werden muss. So entsteht die beständigste und strapazierbarste Beschichtung. Gewachste Oberflächen sind zwar wesentlich einfacher zu fertigen, aber weniger beständig. Instrumente mit gewachster Oberfläche weisen meist nach einigen Jahren irreparable Gebrauchsspuren am Gehäuse auf. Kleinere Beschädigungen können durch leichtes Anfeuchten und nachträgliches Überschleifen und anschließendes Wachsen behoben werden.

Das Korpus wird meist aus Mehrschichtholz gefertigt, das mit der gewünschten Holzoberfläche furniert wurde. Es gibt einige Erzeuger, die für die äußeren Rahmen auf Wunsch Fichtenmassivholz verwenden (Jamnik, Schmidt, Novak, Zernig). Jamnik und Haglmo fertigen alle Modelle ausschließlich aus Massivholz (nicht nur Fichte). Novak-Gehäuse bestanden früher immer aus Fichtenholz, seit der letzten Fertigung allerdings nicht mehr. Ein Blick in das geöffnete Gehäuse kann Klarheit schaffen. Schmidt fertigt alle Modelle mit Fichtenholzrahmen. Ein Verziehen der Rahmen ist nicht zu befürchten, da das Holz handverlesen und gut abgelagert ist. Die Holzpfosten werden ähnlich vorgespalten wie für Dachschindeln. Dadurch wird erreicht, dass die Maserung gleichmäßig und im rechten Winkel zur Oberfläche ausgerichtet ist. Für die Böden (Füllung) verwendet Schmidt Buchenschichtholz, Jamnik Erlenvollholz. Strasser hat ein Modell im Programm, das Kirschvollholz für die Gehäuserahmen verwendet. Zernig baut zwei Modellreihen, eine aus Mehrschichtholz, die zweite aus Massivholz in Erle, Fichte, Birne und Nuss. Das für die Gehäuserahmen verwendete Material hat einen gewissen Einfluss auf den Klang, dies sollte aber nicht überschätzt werden. Dem verwendeten Holz kommt nicht die gleiche Bedeutung zu wie bei Streichinstrumenten. Vorrangige Eigenschaft ist, dass das Gehäuse wenig Eigenschwingungen ausführt und den mechanischen Anforderungen gerecht wird. Mit weichen Holzarten erreicht man einen weicheren Klag, auch wenn diese in Form von Mehrschichtplatten zur Anwendung kommen. Furniertes fünf Millimeter starkes Mehrschichtholz ist von den mechanischen Eigenschaften gut geeignet. Für das Korpus sind daher praktisch alle Materialien verwendbar. Kritischer sind die Materialien für den Stimmstock und die eingebauten Kammern im Bassteil. Mehr Masse wirkt in Richtung kräftiger, brillanter Klang. Jedes Instrument hat seinen individuellen Klangcharakter. Die Abweichungen im Klang sind gering und wurden zumindest für einige Zeit, als erste Tests veröffentlicht wurden, als unbedeutend angesehen. Darüber besteht unter den Harmonikabauern eine geteilte Meinung.

Der Balg wird bei einigen Erzeugern zum Teil selbst gefertigt oder stammt von einem Zulieferunternehmen aus Italien oder Tschechien. Im Wesentlichen unterscheiden sich die Bälge kaum. Bei allen Erzeugnissen wird darauf Wert gelegt, dass die Bälge leichtgängig und extrem dicht sind. Die Unternehmen Müller, Strasser und Zernig fertigen die Bälge selbst. Zum Teil werden Arbeiten in Heimarbeit vergeben. Bälge mit abgerundeten Ecken kommen normalerweise aus Italien. Die metallischen Eckschoner sind bei Bälgen aus Österreich meist gezackt, eckiger und breiter, was bei der gleichen Anzahl von Falten zu einem etwas breiteren Balg führen kann, der dafür aber flexibler ist als ein vergleichbarer Balg aus italienischer oder tschechischer Produktion. Die Eckschoner werden mitunter unter den verschiedene Herstellen ausgetauscht beziehungsweise gehandelt und daher kann an der Art der Ecken nicht garantiert werden wo der Balg gefertigt wurde. Die Eckschoner werden in der Fertigung verpresst, daher kann die Gesamtbreite des Balges auch bei gleicher Art der Eckschoner unterschiedlich breit ausfallen. Karton, Leder, Leinen, Dekor und Eckschoner ergeben nach Verklebung und Vorpressung die Gesamtbreite.

Um gleiche Ergebnisse zu erzielen, muss ein Balg mit stark abgerundeten Ecken einige Falten mehr aufweisen. Gute Bälge sind beweglich, dicht und geräuschlos. Für das Leder in den beweglichen Ecken kommt hauptsächlich geschärftes Schafleder zum Einsatz. Beim Schärfen des Leders werden die Kanten, mit einem Messer oder einem speziellen Lederhobel, nach außen hin verjüngt.

Früher war eher Ziegenleder in Verwendung. Ziegenleder ist zwar meist dünner, aber dafür nicht so dicht. Einen absolut dichten Balg gibt es nicht. Selbst wenn die Innenseite der Eckleder mit einer Silikon- oder Teflonbeschichtung versehen wird, ist der Balg nicht völlig dicht. Auch die Klappenabdichtungen können nur bis zu einem gewissen Grad dicht sein. Ein Instrument verliert immer etwas an Luft.

Am Anfang wurden alle Harmonikas mit offenen Holzhebeln gebaut. Seit langem haben alle Hersteller die Mechanik verbessert, auch wenn diese im Aussehen noch an die traditionelle Bauweise erinnert.

Die Hebel werden nicht mehr ausschließlich aus Holz gefertigt. Es wird ein faserverstärkter, farbiger Polyamidkunststoff verwendet, in dem meist Aluminiumteile eingegossen sind. Der unter dem Verdeck liegende, nicht sichtbare Teil des Hebels ist aus Aluminium. Früher bestanden diese Teile aus runden Stahldrähten, die insbesondere bei den längeren Hebeln zu Vibrationen beim Spielen neigten. Die Präzision ist beim Griffbrett höher, da die erforderlichen Bearbeitungsschritte mit CNC-Maschinen ausgeführt werden. Eine „Holzmechanik“ verwendet meistens zwei Griffbrettteile, die übereinander geschraubt werden. Bei vierreihigen Instrumenten trägt jeder Teil zwei Tastenreihen, deshalb werden zwei Achsen als Lagerung für die Tastenhebel verwendet. Jamnik setzt in den massiven Holzgriffstock einen Kunststoffblock ein, in dem die Tasten einzeln montiert werden, teflon- oder kugelgelagert. Haglmo hat eine Diskantmechanik entwickelt, bei der die Hebel für die vier Reihen an vier Achsen aufgefädelt sind, somit also in jeder Reihe gleich lang, was einen gleich starken Andruck auf allen Tasten ermöglicht. Im Griffbrett befindet sich unter jedem Hebel eine Feder zum Andrücken der jeweiligen Klappe. Auch wenn sich diese Art der Tastatur äußerlich bei den verschiedenen Herstellern sehr ähnelt, sind geringe Unterschiede zu beobachten. Öllerer baut die Tastenreihen etwas enger aneinander. Der Abstand der Tasten beträgt meistens 19 mm. Nur das Unternehmen Beltuna in Italien baut die Tasten enger zusammen. Die Höhe der Abstufung von Reihe zu Reihe muss nicht bei allen gleich sein. Sogar bei gleichen Herstellern hängt es mitunter vom jeweiligen Mechaniker ab, ob die Tasten mehr oder weniger Hub aufweisen.

Manche Hersteller bieten zwei Varianten von Diskantmechaniken an, andere nur eine Variante. Öllerer (mit Kunststoffhebel oder Lagerung) und Edler (Holzhebel mit Messingbuchsenlagerung) bauen nur traditionelle Diskantmechaniken, Jamnik Holzhebel mit Kunststoff, wobei Jamnik eine Variante mit Kugellagern im Programm hat. Haglmo (Holzhebel mit Messingbuchsenlagerung) baut sowohl eine Holz- als auch eine klassische Aluminiummechanik, wobei die Besonderheit der Holzmechanik ein eigens patentiertes, vierachsiges System ist. Schmidt und andere Hersteller bauen mehrere Varianten (abgedeckte oder offene Mechanik). Die Diskantmechanik hat Einfluss auf den Klang des Instrumentes, da die bei einer abgedeckten Mechanik entstehenden Hohlräume eine gewisse Filterwirkung bringen. Dieser Einfluss auf den Klang kann nicht überbewertet werden.

Die zweite abgedeckte Variante der Diskantmechanik geht auf neuere chromatische Klaviaturen zurück, die in Italien und Deutschland gebaut wurden. In der Anordnung und Abstufung der Tasten unterscheidet sich diese abgedeckte Diskantmechanik nicht. Die Abdeckung ist meist aus Holz und abgestuft. Die Knöpfe haben wie bei der Holzmechanik Filzunterlagen, die auf die Abdeckung bei gedrückter Taste aufliegen. Die Ausnehmungen in der Abdeckung dienen auch als seitliche Führung der Tasten. Die Hebel sind komplett aus Aluminium und neigen im Knickbereich zu Einrissen, wenn bei der Verarbeitung nicht achtsam gebogen wurde. Unter den Tasten ist ein rundes, zylinderförmiges Kunststoffteil auf den Hebel aufgepresst, an das der jeweilige Knopf angeschraubt ist. Alle Hebel für vier Tastenreihen sind auf einer Achse montiert. Die Achse ist jedoch etwas dicker als bei einer traditionellen Tastatur, da sie lediglich einmal nach vier Hebeln abgestützt wird. Die Gelenke sind als Gleitlager ausgeführt. Ein auf den Aluhebel aufgenieteter Messingbügel stellt zwei Stützpunkte pro Hebel bereit. Dies bedingt geringe Gleitflächen, daher ist diese Mechanik reibungsarm. In der Werbung wird dies oft als „frei schwebend“ bezeichnet. Diese Aluminiumhebel können relativ einfach in die erforderliche Position gebogen werden, was in der Fertigung einen nicht unwesentlichen Arbeitsaufwand darstellt. Müller baut seit 2014 eine eigene Lagerung, die ein aufgespritztes Kunststoffteil als Lager verwendet. Rihard in Slowenien verwendet stabile in die gestanzen Alubügel eingeschraubte Messinglager. Rahmen und Abdeckung der Tastatur sind nicht unmittelbar miteinander verbunden. Der Tastaturrahmen mit Daumenleiste und die Abdeckung werden mittels CNC-Maschine gefräst und wie das restliche Gehäuse weiter bearbeitet. Die am stärksten abweichende Diskantmechanik, die mit kombinierten Holz und Aluminiumbügeln aufgebaut ist baut Haglmo in seine Patentmodelle ein. Vier Achsen, für jede Tastenreihe eine eigene, die Hebel sind mit Messingbuchsen gelagert. Der Diskantboden ist gestuft und hat zusätzliche Reflexionsschirme. Dadurch erreicht er, dass die Reaktionsfreudigkeit aller Reihen einheitlich ist, bedingt aber auch eine Änderung des Diskantklanges.

Müller fertigt nahezu alle Teile für die Tastatur selbst und beliefert kleinere Harmonikabauer mit Zulieferteilen. Diskanttastaturen dieser Art werden von einigen Herstellern in Italien gefertigt. Autark kann kein Harmonikabauer mehr existieren, da sich bestimmte Maschinen nur bei hohen Produktionszahlen lohnen.

Kunststoffspritzgussteile und zum Teil auch Stanzteile werden bei Zulieferunternehmen in Auftragsproduktion vergeben. Knöpfe und Stimmplatten kommen von Unternehmen, die sich darauf spezialisiert haben. Die Diskantklappen sind meist Kunststoffspritzteile. Novak, Müller und die meisten italienischen Harmonikabauer verwenden dieselbe Art Klappen. Strasser verwendet ebenfalls ein Spritzgussteil für die Klappen; dieses sieht aber anders aus, da Strasser die Hebel mit Heißkleber an den Klappen befestigt. Öllerer verwendet Aluminium-Pressteile für die Klappen, die mit einem beweglichen Gummiteil auf den Aluhebel aufgeschoben werden. Aber bei der ersten Tastenreihe werden Weißblechpressteile für die Klappen verwendet, da in diesem Bereich weniger Platz für die Klappenbefestigung vorhanden ist. Bei der ersten Reihe werden die Klappen an die Hebel gelötet. Strasser verwendet eine eigene Tastatur, die prinzipiell so aufgebaut ist wie die Alumechanik, nur sind die Tastenhebel Kunststoffspritzgussteile, in welche die Aluminiumverlängerungen zu den Klappen eingegossen sind. Für jede Reihe ist somit ein eigenes Spritzgussteil vorhanden. Strasser spricht von einem Konuslagerring. Jeder Hebel hat im Bereich der Lagerung seitlich je eine konusförmige Ausformung, die einem Gegenstück als Lagerbock entspricht. Die Hebel bewegen sich somit nicht auf der Achse, sondern in den seitlichen konischen Ausformungen. Damit dies möglich wird, ist zwischen je zwei Hebeln ein Kunststoffteil als Lagerbock vorhanden, darüber hinaus muss seitlich ein Druck auf die Lager wirken. Eine Feder, die an einem Ende der Tastatur auf die Achse aufgesetzt ist, liefert den nötigen seitlichen Druck. Die Fertigung der Tastatur ist wenig arbeitsintensiv, es bleiben relativ viele handwerkliche Tätigkeiten.

Präzision und Genauigkeit werden bei allen Varianten der Tastatur großgeschrieben. Im Einzelfall ist eine objektive Beurteilung kaum möglich, persönliche Vorlieben spielen eine große Rolle. Die Klappen sind generell mit Filz- und Lederabdichtungen versehen, die sich bei allen Herstellern gleichen.

Register werden von "Beltuna" in steirische Harmonikas eingebaut. Nur Öllere baut ein zweichöriges Modell mit Cassotto im Diskant. Die Füllung wird oft auch als Resonanzboden bezeichnet. Sie trägt wenig zum Klangbild des Instrumentes bei, geringe Einflüsse sind möglich. Manche italienischen Harmonikabauer sowie Öllerer und Haglmo verwenden Aluminiumfüllungen. Alpengold und Risch verwenden Karbonböden. Vorteil der Aluminiumfüllung und Karbonfüllung ist die geringe Dicke, somit ist es leichter, die problematischen höheren Töne und manche Töne in der vierten Reihe in den Griff zu bekommen. Meist wird aber Mehrschichtholz mit 5 mm Dicke verwendet. Strasse verwendet ebenfalls Mehrschichtplatten, aber mit 7 mm Dicke, Jamnik massives Erlenholz mit 4 mm Stärke. Damit die kritischen Töne trotzdem einwandfrei arbeiten, ist bei manchen Tönen der Öffnungsquerschnitt erhöht.

Für die Diskantstimmstöcke werden in der Herstellung unterschiedliche Verfahren verwendet. Italienische Hersteller und das Unternehmen Öllerer verwenden die Klebetechnik. Jamnik, Strasser und Müller verwenden CNC-Maschinen. Das verwendete Holz ist nicht bei allen Erzeugen gleich. Jamnik verwendet eine weiche (Abachi, Fichte, Rotzeder, Gelbzeder) für den Kanzellenteil des Stimmstockes. Die italienischen Hersteller benutzen zum Teil Mahagoni. Strasser und Müller verwenden Fichtenholz für den mittleren Teil. Für die Fußleiste wird bei Müller Ahornholz verwendet, Strasser verwendet eine Mehrschichtplatte. Die Kopfleiste ist immer aus härteren Holz. Auch die Ausformung der Kammern ist unterschiedlich. Manche und frühere italienischen Produkte sind im Bereich der Kopfleiste konisch, österreichische und deutsche Erzeuger hingegen fertigen die Kopfleiste (Rücken) gleich breit über die gesamte Länge. Somit ist die Trennwand bei den Letzteren in Richtung höherer Töne dicker (Keil) oder die Kammern der höheren Töne werden unterschiedlich tief gefräst beziehungsweise ausgefüllt. Müller hat etwas größere Tonöffnungen als Strasser. Die Stimmstöcke sind bei manchen Herstellern wie Müller je nach Einbaureihe schräger gestellt, um mehr Platz für die schwingenden Zungen zu schaffen.

Da der Stimmstock nach den Stimmplatten den größten Anteil an der klanglichen Qualität hat, sind wesentliche Unterschiede bei den diversen Herstellern zu erwarten. Grundlegende Unterschiede in der Gestaltung der Stimmstöcke sind nicht vorhanden. Abmessungen und Anordnung der Stimmstöcke sind beinahe bei allen Herstellen gleich. Die Außenabmessungen werden im Wesentlichen von der Anzahl der eingebauten Bassstimmplatten bestimmt.

Bei vierreihigen Instrumenten werden meist vier doppelte und vier einfache Helikonstimmplatten eingebaut (38 cm × 20,5 cm). Dies ist die am weitesten verbreitete Bauart. Es gibt Sondermodelle von verschieden Herstellen die acht und mehr Doppelte Helikonbässe besitzen.

Wird bei gleichen Abmessungen und Bauart ein H-Bass eingebaut, bedeutet dies aber, dass an einer weiteren Stelle im Gehäuse ein halber Helikonbass eingebaut sein muss. Oder dass ein weiterer doppelter Helikonbass durch zwei einfache Bassplatten ersetzt wird. Nachträglich ist ein derartiger Umbau nicht möglich. Auch das Ersetzen einer doppelten Bassplatte durch zwei einfache ist nicht ohne Kompromisse möglich, da zwei einfache Helikonstimmplatten mehr Platz brauchen als ein doppelter. Daher müssen die verbleibenden drei doppelten Helikonstimmplatten durch etwas schmälere doppelte Helikonbassplatten ersetzt werden.
Die strassersche Art des Bassteilaufbaus weicht von der traditionellen steirischen Bauweise ab und besitzt einen flachen Bassboden ohne Abstufung. Jedoch wurde in Klingenthal und Wien diese Ausführung schon immer verwendet. Ältere Strasser-Harmonikas waren im Bassteilaufbau traditionell steirisch. Die Helikonkammern bei dieser Bauweise sind etwas weniger tief, eine ebene Füllung erleichtert den Einbau der Bassmechanik. Erst durch diese ebene Füllung wird es leichter möglich, einen weiteren einfachen Helikonbass um 90° gedreht an der Vorderkante des Bassgehäuses einzubauen. Dieser Bass funktioniert schlechter als die restlichen einfachen Bässe und bleibt ein Kompromiss. Auch bei der traditionellen Bauweise gibt es die Möglichkeit vorn im Bereich der Abstufung bis zu zwei einfache Helikonbässe einzubauen nur ist der Einbau nicht einfach.

Auch die nachfolgenden Varianten, die von manchen Harmonikabauern angeboten werden, sind kein Kompromiss, aber führen zu nicht unwesentlich mehr Gewicht der Bassseite.

Es werden fast von allen Harmonikabauern Kunststoffgelenke verwendet. Die meisten sind ähnlich und funktionieren gut. Basstasten sind mit mehr Federdruck auf den Klappen ausgestattet. Dies ist notwendig, da auch die Tonöffnungen einen größeren Querschnitt aufweisen. Die Hebel werden aus Edelstahldraht gebogen, da Aluminiumdrähte zu weich wären und zu stark federn würden. Manche Teile werden in der Fertigung mit Pressen vorgebogen, die meisten Teile werden noch beim Einbau mit der Hand zurechtgebogen. Jamnik verwendet eine Konstruktion aus Stahldraht und verdrehfestem Messingrohr, teflon- oder kugelgelagert. Strasser hat auch für diesen Bereich weitgehend eine Vereinfachung in der Montage erzielt. Manche italienischen Produkte waren nicht so leichtgängig und geräuschlos.

Die Tasten sind mit Filzführungen ausgeführt. Sind die Klappen aus formgepresstem Weißblech, so sind sie direkt an die Hebel angelötet. Dies ist auch bei fast allen Herstellern gleich. Die Lederdichtungen und der Filz unter den Klappen sind auch so dick wie auf der Diskantseite. Öllerer verwendet bei den Luxusmodellen extra Leder auch auf der Gegenseite, wo die Klappen aufliegen. Dies führt zu einer Verminderung der Klappengeräusche. Vollständig können die Geräusche aber weder auf der Diskantseite noch auf der Bassseite vermieden werden. Allgemein sind kurze Hebel problemloser als lange. Sind übermäßige Geräusche wahrnehmbar, sind meist die Lager nicht in Ordnung. Bei neuen Instrumenten sind aber kaum Probleme zu befürchten. O.R.A und viele andere italienische oder slowenische Hersteller kopieren die traditionelle Bauweise fast komplett, oft sieht die Mechanik anders aus. Diese Unternehmen orientieren sich an einer gekoppelten Mechanik. Diese Mechaniken die gemeinsame Lagerböcke für mehrere Achsen verwenden sind bereits verbreitet. Die in diesen Mechaniken verwendeten Hebel und Achsen sind punktgeschweißt. Alle Längsachsen befinden sich „schwimmend“ in einem gemeinsamen Lagerbock und werden durch Überlager in Position gehalten. Dabei führen relativ viele Achsen über die komplette Länge des Bassteils. Die älteren Ausführungen hatten relativ hohe Reibung der Achsen gegeneinander. Oft bedingt diese Bauweise ein etwas höheres Gewicht als bei herkömmliche Konstruktion. Ein Vorteil dieser Konstruktion ist, wenn viele weit entfernte Tasten gekoppelt werden, oder bei größeren Bausystemen. Manche slowenische Fabrikate wie Ruthar und Rihard verwenden eine dritte Variante der Mechanik die mit stabileren Messinglagern und kräftigen Aluminumhebel aufgebaut ist, die aber vorzugsweise nur in Basssystemen mit maximal sechzehn Knöpfen Verwendung findet.

Die Menge und die Anordnung der Durchbrüche im Verdeck beeinflussen den Klang sehr stark. Die Schalltrichter wirken sich hingegen nur unwesentlich auf den Bassklang aus, sie stellen in erster Line ein dekoratives Element dar. Vergleiche sind nur unter gleichen Aufbauten möglich. Beim traditionellen Einbau der Helikonkammern wirkt das Verdeck im hinteren Bereich als Cassotto, die Öffnungen im Verdeck befinden sich bei der traditionellen Bauweise mit Helikonbässen, äußerst selten im hinteren Bereich der Abdeckung. Bei Aufbauten aus dem Hause Strasser, aber auch dem Einbau bei Öllerers Luxusmodellen oder Jamniks Patentmodellen oder anderen abweichenden Basseinbauten ist unbedingt erforderlich, dass das Verdeck auch im hinteren Bereich Durchbrüche aufweist. Klanglich sind daher bei diesen Modellen Unterschiede vorhanden.

Die Begleittasten (Bassakkorde) sind normalerweise mit vier Stimmplatten (acht Stimmen – vier auf Zug und vier Druck) pro Akkord ausgeführt. Einige Erzeuger wie Alpengold oder Kaiser verwenden nur drei pro Akkord. Die Zusammensetzung der Töne ist nicht bei allen Akkorden identisch. Auch bei Instrumenten gleicher Stimmung gibt es je nach Erzeuger Abweichungen. Der Stimmstock ist traditionell in einem Stück und an der Vorderseite senkrecht vor den Bassplatten montiert, jedoch beim Jamnik Patentmodell, Martin Schaider’s Eigenentwicklung und Kaiser zweigeteilt und waagrecht oben und unten in Basskasten montiert.

Für die erste Basstastenreihe ist die relative Belegung immer gleich. Für die zweite Basstastenreihe sind viele Varianten in Verwendung. (Siehe Link Tastenbelegung.)

Die Diskantbelegung ist immer identisch. Nur manche Tasten am unteren (selten auch am oberen) Ende der Tastatur sind bei manchen Instrumenten mit unterschiedlichen Halbtönen belegt. Oft werden nur eine oder zwei Tasten der ersten Reihe durch Halbtöne ersetzt. Bei Modellen mit 50 Tasten ist pro Reihe am unteren Ende eine Taste mehr vorhanden (mit „unten“ sind die tiefer klingenden Tasten oben Richtung Kinn gemeint). (Siehe Link Tastenbelegung.)

Unter Stimmung wird die Grundtonart pro Reihe verstanden. Übliche Stimmungen sind: A, D, G, C – G, C, F, B – F, B, Es, As – B, Es, As, Des … Aber Stimmung bezieht sich auf die verwendete Skala „temperiert“ oder „rein“.

Allgemein ist es üblich, die Drucktöne nicht exakt an der temperierten gleichstufigen Stimmung zu orientieren. Müller und Öllerer bringen gewisse Korrekturen an, Strasser stimmt eher temperiert. Auf das Rein-Stimmen von Instrumenten kann viel Zeit verwendet werden. Je höher die Qualität der Stimmplatten ist, desto genauer kann auch das Instrument gestimmt werden.

Meist wird angeboten, dass das Tremolo nach den Wünschen des Kunden eingestimmt wird. Es gibt aber Varianten, pro Ton sind meist drei (seltener zwei) Stimmplattensätze eingebaut, dies heißt dreichörige oder zweichörige Harmonikas. Ein Satz Stimmplatten wird exakt auf die jeweilige Skala gestimmt. Ein Satz wird nach oben hin in der Tonhöhe verstimmt. Somit ergibt sich ein Schwebungston, der von kaum merklich bis zu sehr aufdringlich gewählt werden kann. Das gilt für den dritten Stimmplattensatz, nur wird dieser in der Tonhöhe unter den ersten Stimmplattensatz gelegt. Für Stubenmusik ist meist schwaches Tremolo in Verwendung, für Tanzmusik wird ein kräftigeres Tremolo bevorzugt, da damit die Lautstärke und die Hörbarkeit bei Hintergrundgeräuschen erhöht wird. Zwischen sehr kräftig bis fast keines sind viele Zwischenstufen möglich. Einen Standard gibt es nicht. (Tremolo beim Akkordeon siehe Tremolo.) Das Tremolo wird in den meisten Fällen nach dem Gehör von Hand gestimmt. Ein Hilfsmittel ist der Rumberger Tremolo Generator. Dieser wurde 1995 von Peter Rumberger entwickelt. Er ermöglicht es, eine bestimmte gewünschte Schwebung zu erzeugen, sodass mehrere Instrumente mit dem exakt gleichen Tremolo gestimmt werden können oder ein einzelnes Instrument bei jedem Stimmen das gleiche Tremolo erhält.


Diese Bezeichnungen sind verwirrend, da es keine Normen gibt, betrachte die Gegenüberstellung der drei größten Hersteller von Stimmplatten. Es gibt Akkordeonerzeuger, die nicht die vom Erzeuger verwendeten Bezeichnungen angeben, sondern eigene. Die Stimmplattenqualität unterscheidet sich in Bezug auf Fertigungstoleranzen, Dimension und Konstruktion wie auch in der Art der Fertigung. Die beste Qualität ist handgenietet. (Weitere Angaben siehe Stimmplatte.)
Verwirrend sind die verschiedenen Tonhöhenbezeichnungen in Deutsch, Englisch oder Italienisch. Je nach Stimmplattengröße gibt es eine größere Anzahl von möglichen Tönen, das heißt, dass die meisten Töne auf verschieden Stimmplattengrößen möglich sind. Stimmplattensätze können unterschiedlich groß ausfallen und sind daher nicht ohne weiteres, ohne Eingriffe in die Stimmstöcke, auf denen die Stimmplatten montiert sind, austauschbar.
Die drei wichtigsten Erzeuger von Stimmplatten, die in der steirischen Harmonika verwendet werden, sind die Firmen:
VOCI ARMONICHE S.r.l.
Cagnoni s.p.a.
HARMONIKAS s.r.o
Fertigungsprozess bzw. Produktionsablauf ähneln sich bei den diversen Erzeugern sehr. Vergleicht man die beiden größeren Erzeuger in Österreich heute, so findet man gewisse Unterschiede. Alle restlichen Betriebe in Europa sind in einer Größenordnung, die unter dem der Unternehmen Müller, Strasser und Öllerer einzuordnen sind. Manche Unternehmen in Italien sind wohl größer, wie zum Beispiel Beltuna. Jedoch erzeugt Beltuna in erster Linie chromatische Instrumente. Die meisten restlichen Betriebe sind auch heute noch kleine Familienbetriebe. Müller und Strasser beschäftigen zurzeit zusammengerechnet etwa 58 Mitarbeiter und produzieren im Jahr zusammengerechnet circa 2200 Instrumente.


Manche Fertigungsmerkmale sind bei modernen Modellen nicht mehr anzutreffen. Abgesehen von äußeren Ausführungsmerkmalen, die stark modebedingt sind, war die Vielfalt bis zum Zweiten Weltkrieg größer. Es gab eine größere Anzahl kleiner Unternehmen, die sehr individuell auf die Wünsche der Kunden eingingen. Manche Diskant- und Bass-Tonbelegungen sind heute nicht mehr anzutreffen. In Aufbau und Konstruktion wurden praktisch alle Möglichkeiten ausprobiert. Spricht man mit erfahren Harmonikabauern, so bekommt man oft die Antwort „alles bereits mal dagewesen“, viele Neuerungen, die heute patentiert oder ohne Patent umgesetzt werden, wurden in ähnlicher Form bereits früher verwendet. Die größte Verbreitung fanden früher dreireihige Instrumente, jedoch wurden auch vier- und fünfreihige Instrumente gebaut. Vergleicht man dreireihige Instrumente, die in der Zeit von 1914 bis 1930 gebaut wurden, mit nach 1990 gebauten, so sind die älteren Instrumente meistens etwas kleiner in den Abmessungen. Der Griffstock mit den Diskanttasten war immer schlanker. Der Bassteil klang fast immer weicher. Der Einbau der Helikonstimmplatten variiert im Detail je nach Hersteller etwas, weist jedoch bei den meisten Herstellern bis heute große Übereinstimmungen auf. Die Mehrzahl der Hersteller, ausgenommen der Hersteller aus Sachsen, verwendeten für die meisten Modelle eine Bauweise, bei denen die Basskammern liegend im hinterm Teil eingebaut wurden und die Begleitstimmstöcke vorne in den Balg ragen. Diese Bauweise wird bis heute vorrangig von den meisten Herstellern verwendet. Sperrholz kam noch sehr selten zur Anwendung, fast immer wurden Gehäuseteile aus Fichtenholz mit Intarsien verziert. Die Helikonstimmplatten besaßen praktisch alle heutigen Merkmale. Noch frühere Instrumente unterscheiden sich auch in den Abmessungen der Helikonstimmplatten.

Der verwendete Stahl für die Stimmzungen war etwas weicher. Heutige Stimmplatten sind in den meisten Fällen, wenn Spitzenqualität verwendet wird, genauer und klingen daher obertonreicher. Die Rahmen der Stimmplatten war bereits fast durchgängig aus Aluminium, einzelne historische Instrumente wurden jedoch auch mit Messingrahmen und in vielen Fällen, besonders bei den Begleitern und im Diskant, mit Zinkrahmen gefertigt.
Man findet auch Helikonstimmplatten mit Hartholzrahmen. Diese Instrumente wurden während des Zweiten Weltkrieges gefertigt. Der Grund für die Verwendung von Hartholz für die Rahmen war, Metall war während des Zweiten Weltkrieges schwer verfügbar.

Die Firma Lubas baute größere Modelle bei denen der Bassteil innen eine zusätzliche Abdeckung über alle Stimmplatten besaß. Die Abdeckung hatte eine Öffnung, die Größe der Öffnung konnte variiert werden. Dies führte zu einem Filtereffekt ähnlich wie das heute mit Diskantverdecken die einen Schieber besitzen, der die Abdeckung mehr oder weniger stark schließt, der Fall ist. Ähnliches war auch bei kleineren deutschen diatonischen Harmonikas manchmal üblich, dabei wurde der Balg in zwei Hälften geteilt und mit einer Trennwand versehen die die beiden Hälften verbindet. Die Trennwand besaß eine Öffnung die mehr oder weniger geschlossen werden konnte.

"Hauptbeitrag" → Peter Stachl Patent

Das Unternehmen Peter Stachl fertigte bis 1926 nach eigenem Patent etliche drei- und vierreihige umschaltbare Harmonikas. Der Diskant ist bei diesen Modellen mit einem Knopf auf der Griffbrettrückseite umschaltbar von beispielsweise G-C-F auf B-Es-As, und zwar sind die Stimmstöcke verschiebbar. Die Bassknöpfe sind bei manchen Modellen in drei Reihen angeordnet. Die Instrumente sind nur unwesentlich schwerer als gleichwertige nicht umschaltbare Harmonikas, sie waren zu ihrer Zeit sehr gefragt, das Unternehmen kam mit der Produktion nicht nach.

Weitere Harmonikabauer, deren Firmen nicht mehr weiterbestehen und von denen keine weiteren Daten vorhanden sind.


Im Archiv des Mühlviertler Schlossmuseums in Freistadt gibt es einen Nachweis über den Instrumentenbauer Karl Christa, der von 1926 bis zu seinem Ableben 1937 in Freistadt gelebt und gearbeitet hat. Er ist von Enns zugezogen, wo er von etwa 1893 an ein Geschäft betrieben und vorwiegend Ziehharmonikas aber auch Zither gebaut hat.

Albin Flatscher ist ein österreichischer Musikinstrumentenbauer und Musiker. Er erlernte den Beruf bei Georg Öllerer in Freilassing, schloss die Lehre als Harmonikabauer im Jahr 1969 ab. Im Jahr 1974 gründete er seinen eigenen Betrieb.

Im Laufe der Jahre bildete Flatscher sechs Lehrlinge aus.
Somit war Albin Flatscher einer der wenigen Betriebe, die Lehrlinge in diesem Handwerk ausbildeten. Fachkräfte werden in diesem Gewerbe häufig nur für bestimmte Tätigkeiten geschult, aber selten für das komplette Handwerk.

Martin Flatscher (* 30. Juli 1979), erlernte das Handwerk bei Peter Müller in Bad St. Leonhard im Lavanttal. Er übernahm 1999 die Unternehmensführung von seinem Vater Albin.

Die allererste Altholzharmonika entstand im Hause Flatscher. Hiermit kam ein neuer Stil in die Volksmusikszene, welcher sich laufend weiterentwickelt.
Die Herstellung, Entwicklung und Reparatur von steirischen Harmonikas und Akkordeons aller Art wird im eigenen Haus durchgeführt. Es besteht oder bestand eine enge Zusammenarbeit mit Peter Müller und der Schaumanufaktur in Klingenthal bis zu deren Schließung.

Bekannte Musikanten auf der Flatscher-Harmonika:

Johann Herbst (* 11. Oktober 1960) ist ein österreichischer Musikinstrumentenbauer.

Er erlernte das Handwerk bei Albin Flatscher von 1976–1979 und absolvierte die Meisterprüfung im Jahr 1983. Seit 1985 führt er einen selbstständigen Meisterbetrieb namens „Musik Herbst“. Umfangreiche Kenntnisse eignete er sich mit der Reparatur von vielen verschiedenen Harmonikamarken an, dadurch zeichnen sich seine Harmonikas der Marke „Alpenklang“ durch ein sehr gutes Gesamtkonzept aus, die er in Zusammenarbeit mit dem Unternehmen Mengascini von 1990 bis 1999 ziemlich preisgünstig erzeugte. Ab 1999 konzentrierte sich Johann Herbst auf den Verkauf, Service und Reparatur aller gängigen Harmonikamarken. Als Besonderheit bietet der Betrieb auch den Umbau und Verkauf "selbstspielender Harmonikas" und anderer Instrumente.
Durch die voll funktionsfähige Miniatur-Harmonika mit den Ausmaßen 19 cm × 11 cm × 7,5 cm erfolgte 1993 der Eintrag als "kleinste Harmonika der Welt" ins Guinness-Buch der Rekorde. Der Rekord ist noch immer gültig.

Othmar Kühn ist ein österreichischer Musikinstrumentenbauer.

Er lernte den Beruf eines Harfenbauers in Tirol bei Petuschnig. Danach arbeitete Othmar Kühn lange Zeit bei Rudolf Novak und Peter Müller. 1996 gründete er sein Unternehmen in St. Gertraud Kärnten. Er beschäftigte bis 2007 einen Mitarbeiter und erzeugte etwa 50 Harmonikas im Jahr in traditioneller Bauweise.

Peter Müller (* 2. September 1952 in Loben bei Bad St. Leonhard † 20. August 2014 in Graz) war Hersteller von Steirischen Harmonikas in Bad St. Leonhard im Lavanttal, Kärnten.

Peter Müller übernahm im Jahr 1975 den Betrieb von Josef Fleiß. Nach seinem Tod 2014 übernahm seine Gattin Edith das Unternehmen mit ihren zwei Kindern Marcel und Janine. Harmonika Müller ist derzeit der größte Betrieb, der Steirische Harmonikas erzeugt. Die Jahresproduktion beträgt ca. 1300 Stück, er beschäftigt 45 Mitarbeiter (2016). Bevor Peter Müller sich selbständig machte, war er 2 1/2 Jahre bei der Fa. Hohner als Konstrukteur und anschließend drei Jahre bei einem Schwyzerörgeli-Erzeuger tätig.

Das Unternehmen Harmonika Müller GmbH wird derzeit von seiner Witwe geführt.

Ein technischer Vergleich von Müller-Harmonikas mit anderen Produkten siehe unter Technische Angaben.

1948 gründete "Georg Öllerer sen." (* 1907; † 1991) eine Reparaturwerkstatt für Harmonikas und Akkordeons in Freilassing.

Der Betrieb Öllerer befand sich ursprünglich in der Ludwig-Zeller-Straße 26, Freilassing. Heute ist er in der Jägerndorferstraße 1.

1950 begann sein Sohn "Georg Öllerer jun." eine Lehre als Handzuginstrumentenmacher beim Vater.
Er legte 1979 die Meisterprüfung im Fach Handzuginstrumentenmacher ab.

1989 wurde Georg Öllerer jun. von der Bayerischen Landesregierung eine Auszeichnung für hervorragende innovatorische Leistungen für die Entwicklung einer Tonkammer-Harmonika verliehen.

1978 erlernte "Hans Kirchhofer", der Neffe von Georg Öllerer jun., das Harmonikamacherhandwerk und legte 1990 seine Meisterprüfung mit Auszeichnung ab.

Zuerst wurden nur Reparaturen an Akkordeons durchgeführt, aber bereits in den 1960er Jahren begann Georg Öllerer jun., halbfertige Instrumente der Hersteller Rupert Novak und Strasser zuzukaufen und fertigzustellen. Die Nachfrage für seine Harmonikas entwickelte sich sehr positiv. Das Zukaufen von Halbfertigprodukten in den 1970er Jahren wurden aber immer schwieriger, so begann man 1972 mit der Fertigung von kompletten Harmonikas.

Viele der kleinen Veränderungen, die heutige Produkte auszeichnen, wurden bei Georg Öllerer zuerst angewendet, wie zum Beispiel:
Sehr viel Wert wird auf eine ausgezeichnete traditionelle Stimmung der Instrumente gelegt. Dafür kommt in den meisten Fällen, wenn vom Kunden nicht ausdrücklich anders erwünscht, eine Stimmpraxis zur Anwendung, bei der die Terzen abgesenkt werden. (Harmonische Stimmung)

Am 30. September 2004 hat die Firma Öllerer das Patent Nr. 19960833 erhalten, das ihre neue, konische Tastatur schützt.

Das Unternehmen zählt heute zu den renommiertesten der Branche.

Das Unternehmen Harmonika Schmidt ist ein österreichischer Musikinstrumentenbauer.

Unter anderem war Franz Schmidt Senior von 1975 bis 1984 Meister und Leiter der Harmonikaerzeugung bei Stachl. Nachdem das Unternehmen Stachl die Produktion einstellte, gründete Franz Schmidt Senior 1984 seinen eigenen Betrieb mit Teilen der damaligen Belegschaft in Weinitzen bei Graz. Seit 2007 leitet sein Sohn, Franz Schmidt Junior, nun den Betrieb und legt genauso wie schon sein Vater zuvor, großes Augenmerk auf qualitativ hochwertige Instrumente.

Franz Schmidt Senior führte die traditionelle Bauweise von Peter Stachl großteils in derselben Weise weiter, wie dieser sie gebaut hatte. Es werden aber bei vielen Modellen auch moderne Metalldiskantmechaniken eingesetzt.

Bei der Harmonika-Erzeugung verwendet Schmidt für die Rahmen der Gehäuse als weltweit einziger Harmonikabauer kein Sperrholz. Er ist somit der derzeit einzige Betrieb, der noch einen besonderen Aufwand bezüglich des Rohmaterials für die Gehäuse betreibt, wie er in den Anfangsjahren des Harmonikabaues allgemein üblich war. Das verwendete Fichtenholz stammt aus der näheren Umgebung und wird in ähnlicher Weise aufbereitet, wie dies für handgefertigte Dachschindeln üblich ist. Vor dem Spalten der Scheiter werden die Pfosten ein Jahrzehnt oder mehr luftgetrocknet. Vor der Aufbereitung zu Brettern werden die Pfosten handgespalten, dabei wird die Maserung des Holzes berücksichtigt. Anschließend werden die kurzen Bretter auf die erforderliche Dicke zugehobelt.

Der Betrieb fertigt die meisten Teile einschließlich der Bälge und der traditionellen Diskantmechanik selbst an. Gestimmt werden die Instrumente von Hand unter anderem noch von einem ehemaligen Mitarbeiter des Unternehmens Stachl.

Ein Mitarbeiter von Schmidt Harmonika, Franz Hirt, hat ein neues Instrument entwickelt und es unter dem Markennamen Hirtion schützen lassen. Das Instrument ist leicht und hat nur eine Knopfreihe auf der Diskantseite. Diese Tasten bringen jede für sich einen Akkord auf Zug und einen auf Druck hervor. Jeder Akkord setzt sich aus mehreren Tönen und einem zusätzlichen Tiefton zusammen. Dur Akkorde sind rein gestimmt. Der Klang ist ergreifend und durchdringend. Das Instrument ist in erster Linie zum Begleiten und in der Therapie in Verwendung. Das Instrument wird mit Spielanleitung geliefert, kann aber mit etwas Rhythmusgefühl auch ohne Anleitung praktisch von jedem intuitiv verwendet werden.

Ernst Spirk ist ein Musikinstrumentenbauer, Komponist, Musiker und Musiklehrer in Laxenburg, Niederösterreich.

Anton Strasser (* 1897; † 1959) kam nach dem Ersten Weltkrieg mit einer Kriegsverletzung nach Hause. Dadurch konnte er seinen Beruf als Forstarbeiter nicht weiter ausüben. Als 22-Jähriger begann er 1919 erneut eine Lehre als Harmonikabauer bei Robert Zechner in Graz.
1926 machte er sich dann selbstständig.

Sohn Ernst Strasser und Enkel Ernst Strasser waren beide ebenfalls Harmonikabauer.

1930 Umstellung auf elektrobetriebene Maschinen. In der Zeit bis zum Zweiten Weltkrieg wurden alle Arten von Akkordeons produziert: Steirische, Wiener, Schrammelharmonikas und Pianotasten-Akkordeons. Es wurde bis nach Holland und in die USA exportiert.

Während des Zweiten Weltkriegs war die Produktion auf einem Tiefpunkt, da die Produktionsstätte durch Bombardierungen beschädigt wurde und Metalle nur eingeschränkt erhältlich waren.

Ab 1949 produzierte Anton Strasser aber wieder mit zwei Mitarbeitern 150 Stück im Jahr.

Anton Strassers Sohn Ernst lernte beim Vater den Harmonikabau und war gerade mit der Lehre fertig, als Anton Strasser 1959 verstarb. Vorübergehend wurde der Betrieb von der Witwe weitergeführt, bis ihn dann Ernst Strasser sen. übernahm.
Die Produktion ist heute sehr stark modernisiert und findet in einem neuen Unternehmensgebäude statt. Es wurden Anstrengungen unternommen, eine neue Diskantmechanik herauszubringen. Die letzte Neuerung ist, dass diese Mechanik nun auch für fünfreihige Instrumente in einer etwas abgewandelten Form neu konstruiert wurde und auch gebaut wird.

Valentin Zupan (* 2. März 1936 in Selo bei Vodice (Slowenien)) kam als sechstes von sieben Kindern zur Welt. In der Musikinstrumentenfabrik Melodija in Mengeš machte er seine Ausbildung und baute 1951 sein erstes Akkordeon. 1956 wurde Zupan Chef der Entwicklungsabteilung. Parallel zu seiner Arbeit in der Musikinstrumentenfabrik gründete Zupan seine eigene Werkstatt, in der er begann, Akkordeons und Harmonikas zu konstruieren und zu fertigen.

LBL – GmbH, Österreich, Kirchenstraße 20, Uttendorf Im Pinzgau, 5723, Salzburg, Geschäftsführer ist Günther Lerch. (7. Juli 2009) Manfred Lerch fertigt Gehäuse für die diatonischen Luxusmodelle in Uttendorf. Der Gründer von Alpengold Hans Lapper ist noch als Produktmanager für Alpengold tätig. Kooperationen mit FISMEN sind seit beginn 2014 nicht mehr gegeben. 2014 sind 14 Mitarbeiter an insgesamt drei Standorten in Österreich, Italien und Slowenien beschäftigt, die jährlich ca. 300 Instrumente produzieren. Günther Lerch und seine Frau Claudia, die selbst Italienerin ist fassten im Herbst 2013 den Entschluss, in Castelfidardo eine Produktionsstätte aufzubauen. Mit seinen langjährigen Freunden und Partnern Samuele und Nello Menghini wurde die Firma Alpengold Sinova gegründet. Auch in Slowenien wurde eine Produktionsstätte eingerichtet. Der professionelle Partner ist Emil Kitek, welcher selbst eine Harmonikamarke hatte. Die Firma Alpengold produziert die Serie Stubach in Slowenien.

ALPEN HARMONIKA D.O.O. ist eine Firma aus Slowenien, die bereits seit über 10 Jahren Harmonikas herstellt. Zu den Kunden zählen unter anderem auch namhafte deutsche Unternehmen.

SteiRiesch Harmonikabau Alois und Florian Riesch, Am Weingarten 20, 83646 Bad Tölz (Seit 2013. An der Firmengründung wird zurzeit gearbeitet.)

Haglmo GmbH & Co. KG, Hackl 1, 84163 Marklkofen. Hersteller traditioneller Harmonikas mit patentierter Diskantmechanik. Einer der wenigen niederbayerischen Harmonikabauer. www.haglmo-harmonika.de

Bezüglich Jahrzahlen bei den jeweiligen Harmonikabauern wird in erster Linie auf die
Magisterarbeit von Helmut Gutleder, Universität Mozarteum, Salzburg verwiesen.
Sowie viele persönliche Gespräche mit noch lebenden Harmonikabauern.



Bilder

Videos


</doc>
<doc id="14597" url="https://de.wikipedia.org/wiki?curid=14597" title="Schwache Wechselwirkung">
Schwache Wechselwirkung

Die schwache Wechselwirkung (auch schwache Kernkraft genannt, vereinzelt auch β-Wechselwirkung) ist eine der vier Grundkräfte der Physik. Im Gegensatz zu den aus dem Alltag bekannten Wechselwirkungen der Gravitation und des Elektromagnetismus wirkt sie jedoch nur auf sehr kurze Distanzen. Dabei kann sie wie andere Kräfte für Energie- und Impuls-Austausch sorgen, wirkt aber vor allem bei Zerfällen oder Umwandlungen der beteiligten Teilchen, etwa dem Betazerfall bestimmter radioaktiver Atomkerne. Durch die schwache Wechselwirkung lassen sich keine gebundenen Zustände bilden, was sie von den anderen drei Wechselwirkungen unterscheidet.

Entscheidende Bedeutung hat die schwache Wechselwirkung durch ihre Rolle bei der Fusion von Wasserstoff zu Helium in der Sonne (Proton-Proton-Reaktion), da nur durch sie die Umwandlung von Protonen in Neutronen möglich ist. So entsteht aus vier Protonen (den Wasserstoffkernen) über mehrere Zwischenschritte ein stabiler Heliumkern mit zwei Protonen und zwei Neutronen. Durch diesen Prozess setzt die Sonne Energie frei. Aufgrund der geringen Stärke der schwachen Wechselwirkung läuft dieser Prozess so langsam ab, dass die Sonne schon seit 4,5 Milliarden Jahren stabil leuchtet und dies voraussichtlich noch fünf bis sechs Milliarden Jahre tun wird.

Die schwache Wechselwirkung lässt sich in geladene Ströme und ungeladene Ströme unterscheiden. Geladene Ströme wirken zwischen allen (linkshändigen) Quarks und (linkshändigen) Leptonen sowie den (rechtshändigen) Antiquarks und (rechtshändigen) Anti-Leptonen. Ungeladene Ströme wirken zwischen denselben Teilchen, die durch geladene Ströme wechselwirken, aber zusätzlich auch zwischen allen geladenen (Anti-)Quarks und (Anti-)Leptonen unabhängig von ihrer Chiralität.

Die elektromagnetische ist ca. 10 Mal, die starke Wechselwirkung ca. 10 Mal stärker als die schwache Wechselwirkung. Wie die starke und die elektromagnetische Wechselwirkung wird sie durch den Austausch von Eichbosonen beschrieben. Diese Austauschteilchen der schwachen Wechselwirkung sind das neutrale Z-Boson sowie die beiden positiv bzw. negativ geladenen W-Bosonen. Da diese massiv sind, hat die schwache Kraft nur eine extrem kurzer Reichweite unterhalb eines Atomkernradiuses.

Die schwache Wechselwirkung lässt sich am einfachsten bei Zerfällen von Quarks oder Leptonen beobachten. In Streuexperimenten hingegen ist diese eher schwer zugänglich, da sie bei geladenen Leptonen oder Hadronen von der starken bzw. elektromagnetischen Wechselwirkung überlagert wird. Teilchen, die weder der starken noch der elektromagnetischen Wechselwirkung unterliegen (keine Farbladung und keine elektrische Ladung tragen), sind die ungeladenen Leptonen, also die Neutrinos, die aber in Streuexperimenten äußerst kleine Wirkungsquerschnitte besitzen.

Die schwache Wechselwirkung verletzt die Paritätserhaltung, wie im Wu-Experiment nachgewiesen wurde. Außerdem verletzt sie die CP-Erhaltung etwa beim Zerfall des ungeladenen K-Mesons (Kaonen).

Eine Quantenfeldtheorie, die die schwache Wechselwirkung zusammen mit der elektromagnetischen Wechselwirkung beschreibt, ist das Glashow-Weinberg-Salam-Modell. Man spricht in dieser Formulierung auch von zwei Aspekten der elektroschwachen Wechselwirkung, die durch den Higgs-Mechanismus vereinheitlicht werden.

Die Austauschteilchen der schwachen Wechselwirkung sind massive Vektorbosonen. Sie haben den Spin 1. Ihr Verhalten kann durch die Proca-Gleichung beschrieben werden.

Folgende Tabelle gibt eine Übersicht der Eigenschaften der Austauschteilchen (Masse und Resonanzbreite nach Particle Data Group, Lebensdauer über die Energie-Zeit-Unschärferelation berechnet):

Die Reichweite formula_1 lässt sich grob abschätzen, indem man annimmt dass sich die Teilchen während ihrer Lebensdauer formula_2 (im Ruhesystems des Teilchens) mit 71 % der Lichtgeschwindigkeit formula_3 im Laborsystem bewegen (Lorentzfaktor formula_4): formula_5. Dies ergibt für eine Lebensdauer von 3·10 s eine Reichweite von etwa 0,09 Femtometer – der kleinste Atomkern, das Proton, hat einen Durchmesser von etwa 1,7 Femtometer.

In der elektroschwachen Theorie ist das Massenverhältnis von W- und Z-Bosonen mit dem Weinbergwinkel formula_6 (CODATA 2014) verknüpft

Als Konsequenz der Weinbergmischung ergibt sich, dass die Kopplungsstärke der Z-Bosonen nicht mit der der W-Bosonen identisch ist. Die Kopplungsstärke des W-Bosons an ein linkshändiges Fermion ist gegeben durch

die Kopplungsstärke des formula_9 an ein Fermion ist dagegen durch

wobei formula_11 die Ladung des Fermions in Einheiten der Elementarladung formula_12 ist. formula_13 bezeichnet die dritte Komponente des schwachen Isospins. Für linkshändige Neutrinos gilt beispielsweise formula_14.

Die Kopplungsstärken von schwacher und elektromagnetischer Wechselwirkung hängen zusammen über

Zur Beschreibung eines schwachen Prozesses verwendet man üblicherweise die Schreibweise einer Reaktionsgleichung, wie

Die Teilchen a und b werden also in einem Prozess zu den Teilchen c und d umgewandelt. Ist dieser Vorgang möglich, so sind auch alle anderen möglich, die nach der Vertauschungsregel des Kreuzens (engl. ) entstehen. Ein Teilchen kann also auf die andere Seite der Reaktionsgleichung geschrieben werden, indem dort sein entsprechendes Antiteilchen notiert wird:

Außerdem sind die Umkehrprozesse möglich.

Ob diese Prozesse tatsächlich in der Natur beobachtet werden (also ihre Wahrscheinlichkeit, die sich um viele "Größenordnungen" unterscheiden kann), hängt nicht nur von der Stärke der schwachen Wechselwirkung ab, sondern unter anderem auch von Energie, Ruhemasse und Impuls der beteiligten Teilchen.

Für jede Reaktion gelten die bekannten Sätze der Energieerhaltung, Impulserhaltung und Drehimpulserhaltung, die nach dem Theorem von Noether mit den Invarianzen gegenüber zeitlicher und räumlicher Translation sowie Drehungen im Raum verbunden sind.

Sind die Summen der Ruhemassen der beteiligten Teilchen auf der rechten Seite größer als auf der linken, so handelt es sich um eine endotherme Reaktion, die nur möglich ist, wenn die Teilchen auf der linken Seite ausreichend kinetische Energie tragen. Sollte auf der linken Seite nur ein Teilchen stehen, dann ist die Reaktion in diesem Fall verboten, denn es gibt immer ein Bezugssystem, in dem dieses Teilchen in Ruhe ist (d. h., dass Masse aus dem Nichts erzeugt werden müsste, was nicht möglich ist).

Sind die Ruhemassen der eingehenden Teilchen größer als die Ruhemassen der erzeugten Teilchen, so ist die Reaktion exotherm, und die Differenz der Ruhemassen findet sich als kinetische Energie der erzeugten Teilchen wieder.

Man unterscheidet schwache Prozesse sowohl danach, ob Leptonen und/oder Quarks an ihnen beteiligt sind, als auch danach, ob der Prozess durch ein elektrisch geladenes formula_20- oder formula_21-Boson (geladene Ströme bzw. charged currents: CC) oder das neutrale formula_9-Boson (neutrale Ströme bzw. neutral currents: NC) vermittelt wurde. Die Bezeichnungen schwacher Prozesse lauten wie folgt:

Alle Reaktionen, an denen Neutrinos beteiligt sind, verlaufen ausschließlich über die schwache Wechselwirkung (die Gravitation vernachlässigt). Umgekehrt gibt es aber auch schwache Reaktionen ohne Beteiligung von Neutrinos.

Ähnlich wie das Photon und im Gegensatz zu den W-Bosonen vermittelt das Z-Boson eine Wechselwirkung zwischen Teilchen, ohne die Teilchenart (genauer: Flavour) dabei zu verändern. Während das Photon aber nur Kräfte zwischen elektrisch geladenen Teilchen vermittelt, wechselwirkt das Z-Boson auch mit den ungeladenen Neutrinos. Bei neutralen Prozessen bleiben die beteiligten Fermionen unverändert (keine Änderung von Masse oder Ladung). Das Z-Boson wirkt auf alle linkshändigen Fermionen und durch die Weinberg-Mischung auch auf die rechtshändigen Anteile von geladenen Fermionen. Es ist nicht wie die W-Bosonen maximal paritätsverletzend, da es einen Anteil des B-Bosons enthält (siehe: Elektroschwache Wechselwirkung).

Beispiele für neutrale Prozesse sind: Die Streuung zweier Elektronen aneinander (wird für geringe Energien aber durch die stärkere elektromagnetische Wechselwirkung überlagert und erst bei hohen Energien werden die Wechselwirkungen in der Stärke vergleichbar). Die Streuung von Myon-Neutrinos an Elektronen (keine konkurrierenden Prozesse, erster experimenteller Nachweis der neutralen Ströme 1973 am CERN).

Ein elementarer geladener leptonischer Prozess ist ein Zerfallsprozess eines Leptons L in ein Lepton L' unter Beteiligung ihrer entsprechenden Neutrinos bzw. Antineutrinos (formula_23):

Ein Beispiel dazu ist der Zerfall von Myonen:

wie auch die damit verbundenen Streuprozesse

Bei einem elementaren geladenen semileptonischen Prozess sind neben Leptonen auch Quarks bzw. Antiquarks (formula_28) beteiligt:

Ein Beispiel für einen semileptonischen Prozess ist der bereits genannte β-Zerfall des Neutrons, bei welchem sich ein Down-Quark des Neutrons in ein Up-Quark umwandelt:

Dadurch wird ein Neutron "n = udd" zu einem Proton "p = uud":

Ein Down- und ein Up-Quark sind unbeteiligt. Sie werden „Zuschauerquarks“ (engl. spectator quarks) genannt.

Dieser Prozess wird durch ein formula_21-Boson vermittelt, wobei das negativ geladene Down-Quark in ein positiv geladenes Up-Quark umgewandelt wird — die negative Ladung wird durch ein formula_21-Boson „weggetragen“. formula_34 und formula_35 müssen also Quarks sein, deren Ladungsdifferenz gerade formula_36 ist.

Weitere Beispiele von semileptonischen Prozessen sind:

Bei einem elementaren geladenen hadronischen (bzw. nichtleptonischen) Prozess sind nur Quarks bzw. Antiquarks beteiligt:

Der Kaon-Zerfall ist ein gutes Beispiel für einen hadronischen Prozess

Quarkdarstellung: formula_40

Hadronendarstellung: formula_41

Wobei die beteiligten Teilchen folgendermaßen aufgebaut sind: formula_42 und formula_43 sowie formula_44. Bei diesem Prozess ist das Up-Quark des Kaons wieder ein unbeteiligter Zuschauer. Die positive Ladung des Strange-Antiquarks wird durch ein formula_20-Boson weggetragen. Durch diesen Austausch ändert das Quark seinen Flavor zu einem Anti-Up-Quark.

Weitere Beispiele von hadronischen Prozessen sind zwei Zerfallskanäle des Λ-Baryons:

Bei geladenen Strömen der schwachen Wechselwirkung können sich nur Teilchen aus demselben Dublett ineinander umwandeln:

Es handelt sich nur um linkshändige Fermionen. Diese besitzen einen schwachen Isospin formula_49, wobei die dritte Komponente des schwachen Isospins für die oberen Teilchen formula_50 und die unteren formula_51 ist. Die schwache Hyperladung formula_52, also die doppelte Differenz aus elektrischer Ladung und dritter schwacher Isospinkomponente, ist innerhalb eines Dubletts konstant. Sie beträgt für die Leptonendubletts formula_53 und für die Quarkdubletts formula_54.

Rechtshändige Fermionen koppeln nicht an W-Bosonen und tragen deshalb keinen schwachen Isospin. Weiterhin stellt man fest, dass Neutrinos in der Natur nur linkshändig vorkommen (Goldhaber-Experiment). Somit werden rechtshändige Fermionen als Singuletts formula_55 beschrieben. Da die geladenen Ströme ausschließlich an die linkshändigen Dubletts koppeln, tritt bei diesen Vorgängen eine maximale Verletzung der Parität auf. Experimentell wurde dies im Wu-Experiment untersucht und durch die V-A-Theorie erklärt.

Bei den Quarks sind die Dubletts (u,d'), (c,s'), (t,b') Eigenzustände der schwachen Wechselwirkung und nicht (u,d), (c,s), (t,b). Die Zustände der gestrichenen Teilchen sind jeweils eine Linearkombination von drei Zuständen. D.h. die gestrichenen Quarkzustände formula_56 sind gegenüber den Quarkzuständen formula_57 wie folgt rotiert:

Dabei ist formula_59 die sog. CKM-Matrix. Diese ist unitär und hat vier unabhängige Parameter. Die Quadrate der Elemente der angegebenen Matrix formula_60 sind proportional zu den Übergangswahrscheinlichkeiten zwischen den Quarks.

Die Übergänge innerhalb derselben Quarkfamilie (u,d), (c,s), (t,b) finden am häufigsten statt, da die Diagonalelemente die größten Übergangswahrscheinlichkeiten anzeigen. Es besteht mit geringerer Wahrscheinlichkeit auch die Möglichkeit, dass sich die Generation des Teilchens ändert. Dieses Verhalten wird dadurch verursacht, dass die Masseneigenzustände nicht mit den so genannten Wechselwirkungseigenzuständen übereinstimmen.

Der Zerfall von Quarks oder Leptonen durch neutrale Ströme, also z. B. die Übergänge c → u oder s → d oder μ → e wurden bisher nicht beobachtet.

Die Neutrino-Eigenzustände der schwachen Wechselwirkung formula_61, formula_62, formula_63 (Flavour-Zustände sind Eigenzustände des schwach wechselwirkenden Teils des Hamilton-Operators) sind nicht identisch mit den Eigenzuständen des Massenoperators formula_64, formula_65, formula_66 (Eigenzustände des kinematischen Teils des Hamilton-Operators). Analog zur CKM-Matrix lässt sich hier die sog. Pontecorvo-Maki-Nakagawa-Sakata (PMNS)-Matrix einführen

Aktuelle Werte liegen bei:

Die Matrix hat große Werte auch außerhalb der Diagonalen. Dies unterscheidet sie von der CKM-Matrix und führt zu einer starken Mischung der Neutrinofamilien mit der Zeit.

Wurde ein Neutrino ursprünglich mit einem bestimmten dieser drei Flavours erzeugt, so kann eine spätere Quantenmessung einen anderen Flavour ergeben (Erhaltung der Leptonenfamilienzahlen ist verletzt). Da die Wahrscheinlichkeiten für jeden Flavour sich periodisch mit der Ausbreitung des Neutrinos ändern, spricht man von Neutrinooszillationen.

Beim Zerfall eines (linkshändigen) Leptons durch die schwache Wechselwirkung ändert sich während der Wechselwirkung nicht der Flavour (Erhaltung der Leptonenfamilienzahl in jedem Wechselwirkungsvertex), jedoch können sich entstehende Neutrinos in der weiteren Zeitevolution ineinander umwandeln, wodurch sich der Flavour ändert und somit die Leptonenfamilienzahl-Erhaltung verletzt ist. Die Leptonenzahl ist jedoch bei dieser Oszillation stets erhalten.

Hätten die Neutrinos keine Masse, dann wäre jeder Flavorzustand auch ein Eigenzustand des Massenoperators. Folglich könnte man keine Flavor-Oszillationen beobachten.

Im Folgenden werden für die Lagrange-Dichte formula_71 der schwachen Wechselwirkung die Wechselwirkungsanteile zwischen Fermionen und Eichbosonen analysiert.

Um die Beschreibung der schwachen Wechselwirkung besser einordnen zu können, wird zunächst die elektromagnetische Wechselwirkung beschrieben. Alle im Folgenden mit griechischen Indizes versehenen Größen stellen Vierervektoren dar.

In der Quantenelektrodynamik ist die Wechselwirkungsenergie die Kopplung von (Vierer-)Strömen geladener Teilchen formula_72 an Photonen, dargestellt durch das elektromagnetische (Vierer-)Potential formula_73, gegeben durch: 
Die Kopplungskonstante ist die Elementarladung formula_12. Die Stromdichte ist gegeben durch

wobei formula_77 die Ladungsquantenzahl (die elektrische Ladung der Teilchen in Einheiten der Elementarladung) ist, formula_78 sind die Dirac-Matrizen. formula_79 ist das Feld des einlaufenden Fermions (bzw. auslaufenden Antifermions) mit (Vierer-)Impuls formula_80 und formula_81 das des auslaufenden Fermions (bzw. des einlaufenden Antifermions) mit Impuls formula_82. In einem Feynman-Diagramm beschreiben die Spinoren formula_79 und formula_81 die äußeren durchgezogenen Linien.

Die Streuung zweier geladener Teilchen wird in der Bornschen Näherung (niedrigste Ordnung Störungstheorie) durch das nebenstehende Feynman-Diagramm beschreiben. Die dazugehörige Streuamplitude ist

An jeden Vertex der Ladung formula_86 muss ein Faktor formula_87 multipliziert werden. Am Vertex gilt wegen der Energie-Impuls-Erhaltung für den Vierervektor des Photons formula_88.

Innere Linien des Feynman-Diagramms sind die sog. Propagatoren, hier der Photonenpropagator formula_89, wobei formula_90 der (Vierer-)Impulsübertrag und formula_91 der metrische Tensor der speziellen Relativitätstheorie ist.

Bei der schwachen Wechselwirkung beschreiben formula_92 (neutral current) und formula_93 (charged current) die Summanden der Lagrange-Dichte, die die Wechselwirkung zwischen Fermionen und den Eichbosonen enthalten.

Die schwachen geladenen Ströme werden durch folgenden Wechselwirkungsanteil beschrieben:
Die formula_95-Bosonen koppeln mit derselben Kopplungskonstante formula_96 an alle linkshändigen Leptonen und Quarks.

Bei der Beschreibung der einzelnen Strömen tritt jeweils der Chiralitätsoperator formula_97 auf (dieser transformiert einen polaren in einen axialen Vektor). Bei massiven Teilchen wandelt dieser Teilchenspinoren positiver Helizität in Antiteilchenspinoren negativer Helizität um und umgekehrt (formula_98). Daraus lässt sich der Linkshändigkeitsoperator konstruieren:

Dieser Operator auf einen Spinor formula_100 angewandt, projiziert auf den linkshändigen Anteil: 
Wegen des Auftretens dieses Operators ist die schwache Wechselwirkung eine chirale Theorie. Der linkshändige Strom

ist die (Halbe) Differenz aus Vektorstrom formula_103 und Axialvektorstrom formula_104, deswegen V minus A (siehe: V-A-Theorie).

Schwache geladene linkshändige Quarkströme mit formula_105,
formula_106, formula_107,
formula_108 ist die CKM-Mischungsmatrix:

Schwache geladene linkshändige Leptonenströme mit formula_110,
formula_111:

An einen formula_113 Vertex muss folgender Faktor multipliziert werden:

Der Propagator für massive (Masse formula_115) Spin-1-Teilchen, wie es die W- und Z-Bosonen sind, lautet:

Da für die meisten Fälle formula_117 gilt, kann der Propagator genähert werden. Im Gegensatz zum Photonenpropagator formula_118 ist der Propagator formula_119 für kleine Impulsüberträge konstant.

Bei kleinen formula_120-Werten ist die schwache Wechselwirkung viel schwächer als die elektromagnetische. Dies liegt nicht an der Kopplungskonstante der schwachen Wechselwirkung, denn die Kopplungsstärke formula_121 liegt in derselben Größenordnung wie die elektrische Ladung formula_12. Der Grund für die Schwäche der Wechselwirkung liegt in der Gestalt des Propagators der Austauschteilchen, da die riesige Bosonenmasse im Nenner steht und somit den Wechselwirkungsterm herabsetzt.

Die durch ein W-Boson vermittelte Streuung zweier Leptonen, hat eine Streuamplitude (in niedrigster Ordnung) von:

In der genäherten Form
wird die Streuamplitude durch die Kopplung zweier linkshändiger Ströme mittels einer Kopplungskonstanten beschrieben. Dies wurde von Enrico Fermi durch die Fermi-Wechselwirkung, und zwar als Wechselwirkung von vier beteiligten Teilchen an einem Raumzeitpunkt, beschrieben. Die Fermi-Konstante hat den Wert formula_125.

Die schwachen neutralen Ströme werden durch den folgenden Wechselwirkungsanteil beschrieben:

Die formula_127-Bosonen koppeln mit der Kopplungskonstante formula_128 an den neutralen Strom formula_129. Dieser setzt sich aus einem Isospin-Strom formula_130 und dem elektromagnetischen Strom formula_72 zusammen.

Der Isospin-Strom berechnet sich über

f steht dabei für die Spinor-Wellenfunktion des Fermions. formula_133 ist die dritte Komponente des schwachen Isospins. Sie wird wie folgt berechnet:


Wegen des Linkshändigkeits-Operators formula_140 koppelt das formula_127-Boson über den Isospin-Strom also nur an die linkshändigen Anteile von Fermionen.

Der elektromagnetische Strom berechnet sich gemäß

wobei formula_143 die elektrische Ladung des beteiligten Fermions bezeichnet.

Bei der Berechnung von Streuquerschnitten mit Hilfe von Feynman-Diagrammen muss für jeden formula_127 Vertex der Faktor formula_145 mit einem weiteren Faktor in Abhängigkeit der beteiligten Teilchenart multipliziert werden. Dieser lautet


Bei den letzten drei Faktoren treten Summanden ohne den Linkshändigkeits-Operator auf. Diese Z-Kopplungen wirken damit sowohl auf links- und rechtshändige Anteile der beteiligten Fermionen.

Bei den Neutrinos formula_136 koppeln also nur die linkshändigen Anteile an das Z-Boson. Bei den geladenen Fermionen formula_135, formula_138 und formula_139 koppeln dagegen rechts- als auch linkshändige Anteile an das Z-Boson. Bei der Streuung geladener Fermionen kann somit neben der Wechselwirkung über ein elektromagnetisches Feld auch eine Wechselwirkung über das Feld des ungeladenen Z-Bosons stattfinden. Sind die beteiligten Teilchenenergien klein im Vergleich zur Ruheenergie des Z-Bosons überwiegt bei Streuprozessen allerdings die elektromagnetische Wechselwirkung.

In der elektroschwachen Theorie lassen sich elektromagnetische und schwache neutrale Ströme kombinieren. Statt elektromagnetische Ströme an Photonen und schwache neutrale Ströme an Z-Bosonen

koppeln nun Isospin-Ströme an formula_163- und Hyperladungs-Ströme an formula_164-Bosonen:

Wobei ein Hyperladungsstrom basierend auf der Hyperladung eines Fermions formula_166 eingeführt wurde:

Der Zusammenhang der Eichbosonen ist über den Weinbergwinkel mit formula_168 (wobei formula_169 das Photon ist) und formula_170 sowie formula_171 gegeben und der Zusammenhang der beiden schwachen Kopplungskonstanten mit der Elementarladung über formula_172.

Die schwache Wechselwirkung wurde zuerst beim Betazerfall entdeckt (für dessen Geschichte siehe den Artikel Betazerfall). Die Entdeckung, dass der Betazerfall ein kontinuierliches Spektrum zeigte und scheinbar die Energieerhaltung verletzte, führte Wolfgang Pauli 1930 zur Postulierung des Neutrinos als drittem Zerfallspartner. Darauf aufbauend gab, nachdem 1932 auch noch das Neutron entdeckt worden war, Enrico Fermi 1934 eine erste Theorie des Betazerfalls. Sie hatte einen ähnlichen Aufbau wie die Quantenelektrodynamik (QED), aber die Form einer Stromkopplung mit verschwindender Reichweite und mit einer dimensionsbehafteten Kopplungskonstante. Sie war im Gegensatz zur QED nicht-renormalisierbar. Weitere Fortschritte in den 1930er Jahren waren die Auswahlregeln von George Gamow und Edward Teller (Gamow-Teller-Übergänge, 1936) und die Entdeckung der Rolle der schwachen Wechselwirkung bei der Nukleosynthese in Sternen durch Gamow und Hans Bethe (1938) und bei der Bildung von Neutronensternen in Supernovae (Robert Oppenheimer, Lew Landau). Außerdem wurden bis in die 1950er Jahre neue schwache Prozesse entdeckt wie die Zerfälle von Myonen, Pionen, Kaonen und Hyperonen. In den 1950er Jahren wurde die Paritätsverletzung der schwachen Wechselwirkung entdeckt (theoretisch vorgeschlagen von Tsung-Dao Lee, Chen Ning Yang 1956, experimentell entdeckt durch Chien-Shiung Wu 1957). Das wurde in der V-A-Theorie der schwachen Wechselwirkung von Richard Feynman und Murray Gell-Mann einerseits und Robert Marshak und George Sudarshan andererseits 1958 eingebaut, ein wichtiger Schritt zur modernen Theorie der schwachen Wechselwirkung im Standardmodell. Dazu trugen Sheldon Lee Glashow, Abdus Salam und Steven Weinberg mit der Vereinigung von elektromagnetischer und schwacher Wechselwirkung Ende der 1960er Jahre bei (mit Einführung massiver Vektorbosonen, deren Austausch die punktförmige Wechselwirkung in der Fermi-Theorie ersetzte), sowie Makoto Kobayashi and Toshihide Maskawa mit dem Einbau der 1964 von James Cronin und Val Fitch entdeckten CP-Verletzung in die Theorie über ihre KM-Matrix bzw. CKM-Matrix (zusätzlich nach Nicola Cabibbo, der zur Beschreibung schwacher Zerfälle seltsamer Teilchen 1963 den Cabibbo-Winkel einführte).



</doc>
<doc id="14599" url="https://de.wikipedia.org/wiki?curid=14599" title="Satz">
Satz

Ein Satz (früher „das Gesetzte“) steht für:


Musik:

Regelsatz:

Zusammengehöriger Verbund mehrerer Einzelteile/-komponenten, die eine vorgegebene, abgeschlossene Menge bilden:

Weiteres:
Satz ist der Familienname folgender Personen:

Siehe auch:


</doc>
<doc id="14600" url="https://de.wikipedia.org/wiki?curid=14600" title="Verhaltensbiologie">
Verhaltensbiologie

Die Verhaltensbiologie (auch: Verhaltensforschung) ist eine Teildisziplin der Biologie. Sie erforscht das Verhalten von Tieren und Menschen. Sie beschreibt das Verhalten, stellt Vergleiche zwischen Individuen und Arten an und versucht, das Entstehen bestimmter Verhaltensweisen im Verlauf der Stammesgeschichte zu erklären, also den „Nutzen“ für das Individuum bzw. die Arterhaltung. 

Wissenschaftshistorisch betrachtet ist die Verhaltensbiologie eine Nachbardisziplin der Verhaltensgenetik und der Psychologie. Ihre heutigen, äußerst vielgestaltigen Zweige wurzeln in der Tierpsychologie des späten 19. Jahrhunderts und der "„klassischen“ vergleichenden Verhaltensforschung" (Ethologie) des frühen 20. Jahrhunderts. Die Aussagen und Ergebnisse der Verhaltensforschung finden über die Biologie hinaus auch in mehr oder weniger weit entfernten wissenschaftlichen Disziplinen wie der Soziologie und der Pädagogik Beachtung.

Verhaltensbiologen analysieren insbesondere, durch welche inneren und äußeren Faktoren das Verhalten ausgelöst und gesteuert wird sowie die Wechselwirkungen zwischen Verhalten und Umwelt. Nikolaas Tinbergen beschrieb 1963 in seinem Artikel "On aims and methods of ethology" proximate und ultimate Ursachen von Verhalten.

Die klassische vergleichende Verhaltensforschung („Ethologie“) beschäftigt sich vorwiegend mit der Frage, "wie" etwas passiert, also mit den auslösenden Reizen und den körperlichen Mechanismen der Verhaltenssteuerung. Dies entspricht den proximaten (unmittelbaren) Ursachen des Verhaltens.

Die neueren Zweige der Verhaltensbiologie (insbesondere die Verhaltensökologie und die Soziobiologie) beschäftigen sich vorwiegend mit der Frage, "warum" etwas passiert, also mit der evolutionären Angepasstheit eines Verhaltensmerkmals. Dies entspricht den ultimaten (grundlegenden) Ursachen des Verhaltens.

Die Verhaltensbiologie ist eine synthetische Wissenschaft, deren Arbeitsmethoden und Fragestellungen in erheblichem Maße Überlappungen mit anderen Fachgebieten aufweisen.

Die anfangs Tierpsychologie und später Ethologie genannte, „klassische“ vergleichende Verhaltensforschung wurde in den 1930er Jahren von Oskar Heinroth, Konrad Lorenz und Nikolaas Tinbergen begründet. Diese Forscher gingen von dem damals grundlegend neuen Ansatz aus, dass die äußerst vielfältig und komplex erscheinenden Verhaltensabläufe der Tiere aus bestimmten Grundbausteinen des Verhaltens aufgebaut sind, den sogenannten Erbkoordinationen oder Instinktbewegungen. Daher bemühten sie sich vor allem um eine genaue Beschreibung der Verhaltensweisen einzelner Tierarten mit Hilfe von Ethogrammen, wozu auch experimentell gearbeitet wurde (u. a. zum Phänomen der Prägung). Ferner gingen sie, in krassem Gegensatz zum Behaviorismus, von inneren, spontanen Antrieben für das Verhalten aus.

Zentrale Konzepte der klassischen Ethologie wurden 1990 von Wolfgang Wickler, einem Schüler von Konrad Lorenz, und 1992 von Hanna-Maria Zippelius, einer Schülerin von Karl von Frisch, kritisiert (vgl. hierzu u. a. Übersprungbewegung und Leerlaufhandlung). Im Zuge dieser Diskussion wurden auch die experimentellen Befunde von Tinbergen und Lorenz, die ursprünglich zu den zentralen Begriffsbildungen geführt hatten, als nicht-reproduzierbar entlarvt.

Der Begriff „Ethologie“ wird gelegentlich auch als gleichbedeutende Bezeichnung (als Synonym) für die gesamte Verhaltensbiologie verwendet.

Die Humanethologie erforscht insbesondere jene Verhaltensweisen des Menschen, die als angeboren gelten und die daher als Anpassungen an die natürliche Umwelt verstanden werden. Solche Verhaltensweisen müssen sich im Verlauf der Stammesgeschichte entwickelt haben und sollten daher bei Menschen unterschiedlichster Kulturen in ähnlicher Ausprägung nachweisbar sein. Tatsächlich haben sich im Kulturenvergleich zum Beispiel zahlreiche Gesten und viele Aspekte der Mimik als erstaunlich ähnlich herausgestellt.

Der bekannteste deutschsprachige Forscher auf diesem Gebiet ist Irenäus Eibl-Eibesfeldt.

Die Neuroethologie ist in gewissem Sinne eine Fortsetzung der „klassischen“ vergleichenden Verhaltensforschung mit den Methoden der Neurologie. Beispielsweise untersucht sie die neurophysiologischen Entsprechungen für Phänomene wie spontanes Instinktverhalten und angeborene Auslösemechanismen (AAM), aber auch Rezeption (Aufnahme), Fortleitung und Verarbeitung von Lichtsinneseindrücken. Arbeitsmethoden sind hierfür u. a. die Ableitung von elektrischen Impulsen aus einzelnen Zellen, die Nutzung moderner bildgebender Verfahren, die elektrische Reizung bestimmter Hirnareale und das Untersuchen von Ausfallerscheinungen.

Die endokrinologische Verhaltensforschung untersucht die Wechselwirkungen von Hormonsystem und Verhalten, also beispielsweise den Einfluss von Adrenalin und Serotonin sowie der Endorphine auf das Verhalten und – umgekehrt – den Einfluss des Verhaltens auf die Ausschüttung von Hormonen. Bekannt (aber nicht verstanden) sind solche Wechselwirkungen beispielsweise schon lange aus dem Gebiet des Sexualverhaltens und der Erforschung von Stressoren.

Die Verhaltensökologie (Verhaltensökobiologie, Ethoökologie) beschreibt und analysiert das Verhalten der Lebewesen in einer spezifischen Umwelt und untersucht die evolutionäre Angepasstheit des Verhaltens an spezifische Umweltbedingungen. Grundlage für diese Forschungsrichtung ist die Evolutionstheorie, was besagt, dass die Angepasstheit eines Merkmals an die Umwelt des Merkmalsträgers (des Individuums) letztlich das Ergebnis einer Selektion ist, die zur Erhöhung der Überlebenswahrscheinlichkeit (zur Fitnessmaximierung) führt. Unter anderem versucht man in mathematischen Modellen zu beschreiben, wie sich optimal angepasste Individuen verhalten sollten.

Die Soziobiologie kann als Teilbereich der Verhaltensökologie aufgefasst werden; sie beschäftigt sich mit dem Sozialverhalten der Tiere und des Menschen: untersucht werden zum Beispiel die Bedingungen, unter denen soziale Gruppen (Sozialverbände, Insektenstaaten) und Hierarchien entstehen; das Phänomen der Territorialität und des Altruismus (siehe hierzu auch: Eintrageverhalten); Fortpflanzungsstrategien (Monogamie, Polygamie, Polygynie).

Die Evolutionäre Psychologie versteht sich als biologische Grundlage für viele Disziplinen innerhalb der Psychologie und versucht, menschliche Handlungsweisen aus der Perspektive der evolutionären Entwicklung zu verstehen. Ein originelles Teilgebiet ist beispielsweise das Erforschen der Fähigkeit zum Unterscheiden von Mengen bei Tieren, da das sprachliche Zählvermögen beim Menschen im Verlauf seiner Stammesgeschichte nicht plötzlich neu ("de novo") aufgetreten sein kann, sondern aus biologischen Vorläufern entstanden sein muss.

Weitere Zweige der Biologie, die der Verhaltensforschung nahestehen, sind die Verhaltens-Genetik, die Psychobiologie, die Chronobiologie, die Biologie des Orientierungsverhaltens und – sehr allgemein formuliert – die Biologie der Individualentwicklung (Ontogenese), des Lernens und der Kommunikation.

Am Beginn vieler verhaltensbiologischer Studien steht auch heute noch die Beobachtung der Tiere, und zwar am besten unter natürlichen Bedingungen und ohne Einflussnahme durch den Beobachter. Das beobachtbare Verhalten wird so genau wie irgend möglich beschrieben und quantifiziert, in der Regel mit Hilfe von Verhaltensprotokollen.

Schwierig ist häufig




Von Verhaltensbiologen werden sowohl Freiland- als auch Laborexperimente durchgeführt. Letztere dienen häufig dem Erforschen der physiologischen Grundlagen des Verhaltens, also zum Beispiel der Feststellung von Hormonkonzentrationen im Blut und von Aktivitätsmustern der Nervenzellen sowie zur Klärung von Verwandtschaftsbeziehungen. Verhaltensexperimente, wie zum Beispiel der Open-Field-Test, werden in der Regel an lebenden Tieren durchgeführt, gelegentlich – zur Klärung von Detailfragen – aber auch an isolierten Geweben oder an einzelnen Zellen.

Eine noch immer wichtige Vorgehensweise der Verhaltensbiologen kann als physiologische Variante der Black Box-Methode bezeichnet werden, da trotz der stetig wachsenden Erkenntnisse von Neurophysiologie und Gehirnforschung die spezifischen, das Verhalten steuernden inneren Strukturen noch immer unbekannt sind: Man erforscht den Zusammenhang bestimmter Reize mit bestimmten Reaktionen, blendet aber viele Details der inneren Regelungsprozesse aus der Analyse aus.

Die Verhaltensforschung nutzt Experimente mit Tieren. Beispiele:

Weitere Beispiele:

Häufig werden die Erkenntnisse solcher Experimente auf den Menschen übertragen. Dabei geht es zum Teil auch darum, zu zeigen, dass bestimmte, bisher angenommene Alleinstellungsmerkmale des Menschen nicht gegeben sind.

Tierexperimente der Verhaltensforschung können auch Tierversuche sein, bei denen Tieren Schmerzen oder Leid zugefügt wird (z. B. Experimente von Harry Harlow).

Wie in jeder experimentell arbeitenden naturwissenschaftlichen Disziplin werden auch die Einzelbefunde einer verhaltensbiologischen Studie zuletzt zu einem Modell des Verhaltens zusammengefasst, aus dem neue Schlussfolgerungen abgeleitet werden können. Diese Schlussfolgerungen sind häufig Ausgangsbasis für weitergehende Experimente. Da kein Experiment ohne gewisse Vorüberlegungen begonnen wird, basieren Experimente stets auf bestimmten (bewussten oder unbewussten) Grundannahmen, den Arbeitshypothesen.

Ein häufig wiederkehrender, grober Fehler derartiger Modellbildungen ist die allzu rasche Verallgemeinerung von Erkenntnissen: Da schon bei nah verwandten Arten unterschiedliches, artspezifisches Verhalten auftritt, ist es grundsätzlich (das heißt: bis zum Beweis des Gegenteils im Einzelfall) nicht möglich, Erkenntnisse der Verhaltensforschung von einer Art auf andere, nah oder entfernt verwandte Arten oder gar auf den Menschen zu übertragen. Die Vertreter der klassischen vergleichende Verhaltensforschung (der Ethologie im engeren Sinne) haben in der Vergangenheit einerseits wertvolle Erkenntnisse über Verhaltensunterschiede nah verwandter Arten geliefert, zugleich aber immer wieder einzelne Beobachtungen bei bestimmten Tierarten ohne jedes zusätzliche Experiment auf den Menschen übertragen.

Beispiele:

Die genaue Beobachtung und die Analyse des Verhaltens der Tiere durch den Menschen reicht vermutlich bis in die früheste Vorzeit zurück, war dies doch lebensnotwendig, wenn man Tiere jagen wollte. Sicher belegt ist die Erforschung des Tierverhaltens seit dem klassischen Altertum. Schon Aristoteles (384–322 v. Chr.) hielt in seiner "Historia animalium" beispielsweise fest, dass es zu untersuchen gelte, ob das Verhalten durch innere Antriebe gesteuert werde und wie man dessen Ursachen erklären könne.

Jahrhundertelang wurde das Verhalten von Mensch und Tier allerdings häufig kurzerhand nach folgenden Punkten interpretiert:

Eine im modernen Sinne rationale Verhaltensforschung kam erst als Folge des Darwin'schen Hauptwerkes "Über die Entstehung der Arten" auf: Charles Darwin hatte aufgrund eigener, jahrelanger Kreuzungsexperimente (vor allem an Tauben) auch den Weg dafür geebnet, Verhalten als in gleicher Weise vererbbar wie körperliche Merkmale zu betrachten. Dies führte allerdings noch nicht zu einer Verselbständigung der Verhaltensforschung; im 19. und auch noch im frühen 20. Jahrhundert blieb die Erforschung des Tierverhaltens ein (Rand-) Gebiet der Psychologie.

"Weitere Stufen der Entwicklung des Faches:"


Die moderne Verhaltensbiologie geht hingegen davon aus, dass Verhalten grundsätzlich kausal erforschbar ist.


Da psychisches Erleben nur subjektiv durch Introspektion erfahrbar ist, lehnt die moderne Verhaltenslehre psychische Vorgänge als letzte Ursache von Verhalten ab, ohne ihre Existenz aber grundsätzlich zu bestreiten. Psychische Vorgänge können Verhalten begleiten, sind aber nicht objektiv erforschbar.





</doc>
<doc id="14601" url="https://de.wikipedia.org/wiki?curid=14601" title="Augsburger Puppenkiste">
Augsburger Puppenkiste

Die Augsburger Puppenkiste ist ein Marionettentheater in Deutschland.

Sie ist untergebracht im historischen Heilig-Geist-Spital in der Augsburger Altstadt und führt seit 1948 Märchenaufführungen und ernste Schauspiele auf. Mit ihren zahlreichen Fernsehproduktionen (u. a. Stücke über Jim Knopf und Urmel) erlangte die Puppenkiste seit 1953 bundesweite Bekanntheit, siehe Liste der Produktionen der Augsburger Puppenkiste.

1943 gründeten Walter Oehmichen (1901–1977), seine Frau Rose Oehmichen (1901–1985) und ihre Töchter Hannelore (1931–2003) und Ulla ein eigenes kleines Marionettentheater: den „Puppenschrein“, eine kleine Bühne, die in einem Türrahmen aufgebaut werden konnte. In der Nacht zum 26. Februar 1944 wurde der Puppenschrein bei einem Bombenangriff auf Augsburg zerstört. Die Figuren blieben aber erhalten – Walter Oehmichen hatte sie glücklicherweise mit nach Hause genommen, nachdem er eine Vorstellung im Stadttheater Augsburg für die Kinder der Bühnenangehörigen gegeben hatte, wo der Puppenschrein ein Opfer der Flammen wurde. Heute ist lediglich noch eine Rosette des Puppenschreins erhalten.

Nach Kriegsende begann Walter Oehmichen mit den Planungen für ein neues Puppentheater. Mit dem ehemaligen Heilig-Geist-Spital fand er einen Raum, der als ständiger Aufführungsort dienen konnte. Zunächst musste sich Oehmichen die Spielstätte allerdings mit dem Statistischen Amt teilen.

Allen Widrigkeiten der Nachkriegszeit zum Trotz gelang es der Familie Oehmichen schließlich, unter dem Namen "Augsburger Puppenkiste" ihr Marionettentheater am 26. Februar 1948 – auf den Tag genau vier Jahre nach Zerstörung des Puppenschreins – mit dem Stück "Der gestiefelte Kater" zu eröffnen. Als Puppenspieler und Sprecher wurden junge Augsburger Schauspieler verpflichtet, unter ihnen Manfred Jenning. Er wurde schnell zum Hausautor der Puppenkiste und begründete 1951 mit dem alljährlich wechselnden Silvesterkabarett für Erwachsene eine Tradition, die seither beibehalten wird. Die erste „Kabarett“-Premiere wurde am 31. Dezember 1950 präsentiert.

Zunächst schnitzte Walter Oehmichen die Marionetten, übergab diese wichtige Aufgabe aber bald an seine Tochter Hannelore. Unter ihren talentierten Händen entstanden all die berühmten „Stars an Fäden“. Ihre erste Figur schnitzte Hannelore bereits im Alter von nur 13 Jahren, noch heimlich, weil sie das scharfe Schnitzmesser eigentlich nicht hätte benutzen dürfen. Ihre erste bekannte Figur war der kleine Prinz. In der Premierenvorstellung des Theaters führte Hannelore den gestiefelten Kater. Oehmichens Frau Rose kleidete die Puppen ein und übernahm als Sprecherin viele Mutter- und Großmutterrollen.

Zum 25. Jubiläum im Jahre 1973 übernahmen Hannelore und ihr Mann Hanns-Joachim Marschall, Schauspieler und ebenfalls lange Jahre in der Puppenkiste tätig, die Leitung des Marionettentheaters. 1977 starb Walter Oehmichen, der bis zu seinem Tode dem Theater helfend beistand. Rose Oehmichen starb 1985 und Tochter Hannelore wurde Inhaberin der Puppenkiste.

Seit Anfang der 1980er arbeitet Klaus Marschall, Sohn von Hannelore und Hanns-Joachim Marschall, im Theater mit. 1992 übernahm er die Leitung von seinen Eltern. Hanns-Joachim Marschall zog sich aus dem Theater zurück und starb 1999. Seine Frau Hannelore aber schnitzte weiter die Figuren und stand immer wieder helfend auf der Spielbrücke. Klaus' Bruder Jürgen stieg Anfang der 1990er in den Betrieb ein und unterstützte seine Mutter bei der Puppenherstellung. Nach ihrem Tod am 16. Mai 2003 trat er ihr Erbe an.

Die Spielstätte am Roten Tor wurde im Laufe der Jahre für das Theater räumlich sehr eng. Im Rahmen der Sanierung des Heilig-Geist-Spitals und Planungen für einen „Kulturpark Rotes Tor“ wurden von der Stadt Augsburg weitere Räumlichkeiten im Jahr 2000 zur Verfügung gestellt und ein neuer Theatersaal, dem alten gegenüber gelegen, eingerichtet. Am 21. Oktober 2000 wurde dieser Saal eröffnet.

Im Jahr 2004 wurde die Augsburger Puppenkiste mit der "Goldenen Kamera" ausgezeichnet.

Markenzeichen des Theaters sind Kistendeckel mit dem schräg gedruckten Namenszug "Augsburger Puppenkiste" und dem Zusatz "Oehmichens Marionettentheater." Der Name "Puppenkiste" geht auf Walter Oehmichen zurück: Nachdem der Puppenschrein zerstört worden war, sollte sein neues Marionettentheater komplett Platz in einer Kiste finden, um so immer leicht transportfähig zu sein.

Den Bühnenausschnitt (0,90 × 2 Meter) im Theater in Augsburg verschließen zwei überdimensionierte Kistendeckel. Fürs Fernsehen gab es gesonderte, kleinere Deckel, die an die Bildschirmgröße beziehungsweise das Bildschirmformat von 4:3 angepasst waren. Bei fast allen Fernsehproduktionen der Puppenkiste kamen diese Kistendeckel, die nahezu unverändert seit Ende der 1950er Jahre verwendet wurden, zum Einsatz. Speziell für die Produktion Der Raub der Mitternachtssonne (1994) wurde ein neuer so genannter Insertkasten gefertigt, der dem Sendeformat entsprechend länglicher ist.

Lediglich Fernsehsendungen, die unter alleiniger Beteiligung der Puppenkiste entstanden sind oder sich von den regulären Produktionen des Hessischen Rundfunks abheben sollten, kamen ohne die Kistendeckel aus. Zu nennen sind hier die diversen Folgen für das "Sandmännchen" (1962–1982), "Die Museumsratten" (1965–1972), "Ich wünsch’ mir was" (1968–1970), "Wir Schildbürger" (1972), "Natur und Technik" (1972–1976) und "Ralphi" (2004–2006).

Viele klassische Märchen, nicht nur der Brüder Grimm, sondern auch aus "Tausendundeine Nacht" oder nach Wilhelm Hauff, werden in der Puppenkiste gespielt. Stücke wie "Aladin und die Wunderlampe", "Der Zwerg Nase" oder "Frau Holle" stehen schon seit Jahrzehnten – neu inszeniert – auf dem Spielplan. "Der Räuber Hotzenplotz" (1966), das beliebteste Stück des Theaters, oder "Die kleine Hexe" (1971) – beide nach Vorlagen Otfried Preußlers – werden gar seit ihrer Erstinszenierung unverändert gespielt. 1983 wurde das zweiteilige Stück "Wolkenreiter und Sohn" ausgestrahlt. Neu in der Puppenkiste ist das Stück "Der Zauberer von Oz" (2013) und seit 2011 wird "Der kleine Wassermann" gespielt. Von Dezember 2014 bis Anfang Januar 2015 war "Die Weihnachtsgeschichte" auf der Bühne zu sehen und 2015 sind neben den genannten Stücken vorwiegend Märchen wie "Dornröschen", "Rumpelstilzchen" oder "Hänsel und Gretel" im Programm vorgesehen.

Anders, als man es bei einer Puppenbühne vielleicht erwartet, wurden und werden in der Puppenkiste auch viele Bearbeitungen von Stoffen für Erwachsene auf die Bühne gebracht:
Oehmichen inszenierte zunächst viele Stücke, die er am Stadttheater Augsburg, an dem er als Spielleiter tätig war, nicht realisieren konnte, unter anderem "Ein Traumspiel" von August Strindberg. Waren die ersten Spielzeiten noch schwierig, folgte am 26. Februar 1951 mit Antoine de Saint-Exupérys "Der kleine Prinz" der große Durchbruch für das kleine Marionettentheater. Walter Oehmichen trat hier selbst in der Rolle des Fliegers auf.

Die Erstaufführung von Bertolt Brechts "Die Dreigroschenoper" in Augsburg durch die Puppenkiste sorgte am 25. September 1960 für Aufsehen. Oehmichen trat hier als Bänkelsänger wiederum selbst auf. Der vier Jahre zuvor in der DDR gestorbene Dichter war in der Zeit des Kalten Krieges ein eher ungeliebter Sohn der Stadt. Immer wieder steht ferner "Der Prozess um des Esels Schatten" (1962) von Friedrich Dürrenmatt auf dem Spielplan.

Neben ernsten und komischen Stoffen sowie Klassikern wie "Dr. Johannes Fausti" inszenierte und inszeniert die Puppenkiste aber auch Opern und andere Musikwerke, vornehmlich nach Vorlagen Mozarts. Schon 1952 brachte Walter Oehmichen sowohl die komische Oper "Bastien und Bastienne" auf die Bühne als auch Prokofjews musikalisches Märchen "Peter und der Wolf". 1985 brachte das Theater "Eine kleine Zauberflöte" heraus. Das Stück wurde von Oehmichens Schwiegersohn Hanns-Joachim Marschall marionettengerecht umgesetzt. So erhielten die Figuren ihre Stimmen von singenden Schauspielern, große Arien wurden ganz gestrichen. "Die kleine Entführung aus dem Serail" (1991) wurde erfolgreich nach gleichem Muster inszeniert.

Zum Mozartjahr 2006 inszenierte Klaus Marschall im Jahre 2005 den klassischen Stoff des Don Giovanni als "Don Giovanni und der steinerne Gast". In der Rolle des Dieners von Don Giovanni ist der Kasperl der Puppenkiste zu sehen, der das Stück mit dem ihm eigenen Witz würzt.

Sehr erfolgreich wird jedes Jahr neu das "Kabarett" gezeigt. Das aktuelle Kabarett-Programm hat immer an Silvester Premiere. In den folgenden sechs Monaten des neuen Jahrs werden rund 100 Vorstellungen gespielt. Dabei passt sich der Inhalt des Puppenspiels oft dynamisch an aktuelle Ereignisse an.

Ab 1953 wurde die Puppenkiste auch bundesweit bekannt: Am 21. Januar, nur wenige Wochen nach Premiere der Tagesschau, fand die erste Fernsehsendung mit der Geschichte "Peter und der Wolf" statt.

Die Sendung wurde im "Bunker" des NWDR in Hamburg nachgespielt und – wie auch die folgenden, bis 1954 vom Hessischen Rundfunk im Frankfurter Fernsehstudio produzierten Sendungen – aufgrund fehlender Aufzeichnungstechnik live übertragen. Zwischen 1956 und 1959 war der Bayerische Rundfunk eine weitere Station der Puppenkiste, da der HR in diesem Zeitraum sein Kinderprogramm abgesetzt hatte. Nach der Wiedereinführung bot dieser dem Hausautor Manfred Jenning jedoch die Möglichkeit an, seine Idee eines Mehrteilers (Die Geschichte der Muminfamilie) zu realisieren und die Puppenkiste ging wieder zum HR. Waren diese ersten Fernsehproduktionen der Puppenkiste noch abgefilmtes Theater, wurde schon bald mit dem Aufwand eines Spielfilms gedreht. Pro Arbeitstag entstanden nun drei bis vier Sendeminuten. Den Puppenspielern, die wegen der Scheinwerfer bei etwa 60 Grad arbeiteten, stand dabei der Schweiß auf der Stirn. Produziert wurde fortan im zum Studio umgebauten Foyer des Augsburger Theaters. Unter Jennings Leitung entwickelten sich die reinen Theateraufzeichnungen zu echten Fernsehfilmen, die alle Möglichkeiten des Mediums ausschöpften. Die Fernsehstücke wurden früh von den Bühneninszenierungen abgekoppelt. So kam es, dass all die bekannten Fernsehstars nie auf der Bühne in Augsburg zu sehen waren.

Zu den ersten großen „Stars an Fäden“ zählten – noch in schwarz-weiß produziert – "Die Muminfamilie" (1959/60, zwei Staffeln), "Jim Knopf und Lukas der Lokomotivführer" (1961/62, zwei Staffeln, Neuverfilmung in Farbe 1976), "Der kleine dicke Ritter" (1963), "Klecksi, der Tintenfisch" (1963), von dem leichtsinnigerweise fast alles vernichtet wurde, und der "Kater Mikesch" (1964).
Produziert wurde seit 1954, abgesehen von einem kurzen Zwischenspiel beim Bayerischen Rundfunk 1956–1958, mit dem Hessischen Rundfunk.

Ab 1965 wurde im Hinblick auf das kommende Farbfernsehen die "Löwe-Trilogie" auf (16 mm)- Farbfilm produziert, (sie lief zunächst 1965–1967 noch in Schwarzweiß und wurde später in Farbe wiederholt). Nachfolgend wurden alle Puppenkiste-Produktionen farbig verfilmt: "Räuber Hotzenplotz" (1967), "Bill Bo" (1968) und "Urmel aus dem Eis" (1969).

Zu einem der meistverfilmten Autoren wurde Max Kruse. Er lieferte die Vorlagen zu "Der Löwe ist los", "Kommt ein Löwe geflogen" und "Gut gebrüllt, Löwe", "Urmel spielt im Schloss" (1974) sowie zu "Don Blech und der Goldene Junker" (1973) und dem Wildwest-Abenteuer "Lord Schmetterhemd" (1978).

Die Drehbücher zu diesen Produktionen stammten von Manfred Jenning, der ab 1970 mit "Kleiner König Kalle Wirsch" auch die Regie für die Fernsehstücke von Harald Schäfer übernahm. 1979 starb Jenning nach langer schwerer Krankheit im Alter von 50 Jahren.

Sein Nachfolger in den Fernsehproduktionen wurde Sepp Strubel. Bereits seit Anfang der 1960er war Strubel als Sprecher für die Puppenkiste tätig und hatte mit "Natur und Technik" beziehungsweise "Denk und Dachte" (1972–1976) eine eigene wissenschaftliche Magazinreihe für Kinder mit Marionetten der Puppenkiste erdacht und umgesetzt. Strubel zog anstelle von bekannten Autoren, mit denen die Puppenkiste bereits Erfolge feiern konnte, junge Autoren für die Fernseharbeit heran: 1980 entstand "Die Opodeldoks" nach einem für die Puppenkiste geschriebenen Buch von Paul Maar, 1982 die Verfilmung von "Katze mit Hut". Der Weltraum wurde ein neues Ziel Strubels: Zunächst ging es auf den Apfelstern ("Fünf auf dem Apfelstern", 1981), 1986/87 kam dann der kleine Roboter "Schlupp vom grünen Stern" auf die Erde (Buch: Ellis Kaut).

Am 16. April 1983 musste eine reguläre Vorstellung nach ihrem Beginn abgebrochen werden, weil im Rahmen einer Publikumswette in der gerade in Augsburg stattfindenden ZDF-Show Wetten, dass..? ein kurzfristiger Auftritt der Marionetten in der Livesendung nicht für möglich gehalten wurde. Einige Akteure aus dem Puppenspielerteam begaben sich mit bekannten Marionetten zur ZDF-Show und damit unverhofft an diesem Tag ins Fernsehen. 1994 wurde die zunächst letzte Fernsehproduktion mit dem Hessischen Rundfunk realisiert: Bei "Der Raub der Mitternachtssonne" führte Sepp Strubel nicht mehr selbst Regie. Von ihm stammte lediglich – wie auch bei "Das Burggespenst Lülü" (1992) und "Der Zauberer Schmollo" (1993) – das Drehbuch.

1997 schaffte die Augsburger Puppenkiste mit "Die Story von Monty Spinnerratz" des amerikanischen Kinderbuchautors Tor Seidler den Sprung auf die Kinoleinwand. Etwa 900.000 Kinobesucher erleben mit, wie (Ratten-)Marionetten und Menschen zusammen in der New Yorker Hafen- und Unterwelt agieren. Der Film, Regie Michael F. Huse, wurde 1997 mit dem Bayerischen Filmpreis als "Bester Kinderfilm" ausgezeichnet.

2000/01 gab es eine neue Serie fürs Fernsehen: In "Lilalu im Schepperland" (insgesamt 13 Folgen) werden die Abenteuer der Prinzessin des Märchen-Sing-Reiches Melodanien, des Hofküchenwichtels Pimpernell und der Krähe Lukulla im Kampf gegen die mächtige Hexe Synkopia, gegen den Roten Kobold, Hexen und Zauberer erzählt. Das Drehbuch dazu – entstanden nach Motiven der "Browny Tales" von Enid Blyton – stammte von Peter Scheerbaum, dem Hausautor der Puppenkiste, der bereits am Drehbuch zu "Die Story von Monty Spinnerratz" beteiligt war.

Neuestes Fernsehprojekt der Puppenkiste ist "Ralphi" (2005/06): Der Schlaubär geht für den Wissenskanal BR-alpha auf Erkundungstour. Kindgerecht werden verschiedenste Themen aufbereitet. Ralphi ist jedoch nicht in den Fantasiewelten der Puppenkiste, sondern in der Realität unterwegs: Er besucht Museen, ist auf Flüssen und Seen unterwegs oder ist auf Stippvisite bei Bürgermeistern.

Außerdem produziert die Puppenkiste Dokumentationen, die sich an Kinder wie auch Erwachsene richten: In "Augusta Kasperlicorum" (2004) stellt der Kasperl der Puppenkiste seine Stadt vor. Als Beitrag zum Mozartjahr begibt sich der Kasperl in "Augusta Mozarteum" (2006) auf die Spuren des berühmten Komponisten, dessen Vater wie auch der Kasperl Sohn der Stadt Augsburg ist. Beide Dokumentationen sind allerdings lediglich auf DVD erschienen.

Zu ihren Wurzeln kehrte die Puppenkiste mit der Verfilmung des Kabaretts zurück: An Silvester 2005 wurde mit dem "Kabarett der Puppen" ein Zusammenschnitt der aktuellen Augsburger Inszenierung gezeigt, abgefilmt von der Bühne des Augsburger Stammhauses. Des Weiteren werden seit dem 1. April 2006 verschiedene Musiknummern aus dem Kabarettprogramm als Pausenfüller auf BR-alpha unter dem Titel "Poesie des Staunens" präsentiert.

Nachdem Bayerns Staatsminister für Unterricht und Kultus, Ludwig Spaenle, Anfang 2012 sich darüber aussprach, dass er sich um das gemeinsame Kinderprogramm von ARD und ZDF sorge, kam das Thema „Puppenkiste im TV“ wieder auf. Nun steht seit Mitte März 2012 die Überlegung einer Kooperation von Bayerischem Rundfunk und Puppenkiste bezüglich Fernseh-Neuinszenierungen im Raum.

Im Jahre 2016 verewigt die Augsburger Puppenkiste ihre Inszenierung der biblischen Weihnachtsgeschichte auf Film. Dieser wird, während der Adventszeit, bundesweit im Kino zu sehen sein.

Seit Juli 2017 versuchen Fans der Puppenkiste die Produktionen mithilfe einer Online-Petition wieder ins Fernsehen zu bringen.

Zu ihrem 50. Jubiläum begab sich die Augsburger Puppenkiste 1998 auf eine zweijährige Tournee durch Deutschland, unterstützt durch den Bertelsmann Club.

Seit 2003 tourt das Marionettentheater mit dem Mutmachstück "Das kleine Känguru und der Angsthase" nach einem Buch von Paul Maar durch deutsche Kinderkliniken. Das Stück wurde auch zwischen dem 29. April und 7. Mai 2006 dreimal täglich bei einem Besuch der Puppenkiste in der japanischen Partnerstadt Augsburgs, Amagasaki, aufgeführt.

Im Jahr 2006 startete eine weitere Tournee, Ziel sind Kindergärten. Durch das Stück "Paula und die Kistenkobolde" von Peter Scheerbaum sollen Kinder lernen, mit ihren Gefühlen umzugehen. Das Stück wurde im Rahmen des Projekts "Papilio" des beta-Institutes unter Berücksichtigung wissenschaftlicher Erkenntnisse erarbeitet. Auftakt der Papilio-Tournee war am 7. März 2006 im Bayerischen Landtag in München.

Außerdem tourt die Augsburger Puppenkiste mit dem Stück „Urmels große Reise“ durch Deutschland.

Im Zuge der Renovierung des Heilig-Geist-Spitals zog die ganze Bühne im Hause um. Und zwar, vom Eingang aus gesehen, von rechts nach links. Zudem besitzt die Puppenkiste seitdem ihr eigenes Museum. Am 6. Oktober 2001 wurde "Die Kiste – Das Augsburger Puppentheatermuseum" im ersten Stock des Heilig-Geist-Spitals direkt über den Theaterräumlichkeiten eröffnet.

In einer Dauerausstellung werden die bekannten "Stars an Fäden" wie das Urmel, Jim Knopf und Kalle Wirsch gezeigt. In viermonatlich wechselnden Sonderausstellungen werden sowohl Figuren aus dem Fundus als auch von anderen Theatern und Sammlungen zu verschiedenen, oft aktuellen Puppentheaterthemen präsentiert.

Einige der bekanntesten Stücke der Augsburger Puppenkiste, wie etwa "Das Lummerlandlied" wurden von Hermann Amann komponiert.

Die Gruppe Dolls United erzielte mit einer Dance-Version von "Eine Insel mit zwei Bergen" aus der Puppenkisten-Verfilmung von "Jim Knopf und Lukas der Lokomotivführer" im September 1995 einen Hitparadenerfolg im deutschsprachigen Raum (Rang 2 der deutschen Hitparade). In Deutschland wurde die Single 1996 für mehr als 500.000 verkaufter Exemplare mit einer Platinschallplatte ausgezeichnet.

2017 traten Figuren der Augsburger Puppenkiste zusammen mit Ärzte-Sänger Bela B. in dessen Musikvideo "Einer bleibt liegen" auf.

Der FC Augsburg kooperiert seit 2009 mit dem Marionettentheater. Im Rahmen dieser Kooperation tippt das Kasperle vor jedem Heimspiel das Ergebnis. Zudem überreicht der Kapitän des FCA dem gegnerischen Kapitän anstelle eines Wimpels eine jährlich wechselnde Marionette. In der Saison 2015/16 war Kater Mikesch das Geschenk an die Gäste, in den Jahren davor wurden unter anderem Urmel, Jim Knopf und Lukas, der Lokomotivführer überreicht. Nach jedem Tor des FCA wird als Torhymne die Melodie „Eine Insel mit zwei Bergen“ aus der Verfilmung von Jim Knopf und Lukas der Lokomotivführer eingespielt.

Seit Juli 2017 gibt es nahe der Augsburger Puppenkiste eine Fußgängerampel mit einem grünen Kasperle. Die Idee für das berühmte Ampelmännchen, die nach dem Vorbild der Mainzelmännchen-Ampel in Mainz entstand, hatte der Augsburger Fernsehsender a.tv. Dessen Mitarbeiter waren der Meinung, dass das Kasperle mindestens genauso bekannt sei wie die berühmten Maskottchen des ZDF. Nach Prüfung und Genehmigung von den Behörden wurde die Kasperle-Ampel schließlich in Betrieb genommen.






</doc>
<doc id="14603" url="https://de.wikipedia.org/wiki?curid=14603" title="Etzel">
Etzel

Etzel bezeichnet:
Etzel ist der Name von:
Etzel ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="14607" url="https://de.wikipedia.org/wiki?curid=14607" title="IC">
IC

IC steht für:

Unternehmen und Institutionen:

Produkte und Marken:

IC als Unterscheidungszeichen auf Kfz-Kennzeichen:

Ic steht für:

Ic (römisch Eins c) steht für:

ic steht für:

Siehe auch:


</doc>
<doc id="14608" url="https://de.wikipedia.org/wiki?curid=14608" title="Chip">
Chip

Chip bezeichnet

im Sinne von "chip" (eigentlich „Splitter“ oder „Span“):

Sonstiges:
Chip als Name
CHIP steht für:
ChIP ist die Abkürzung für:
Siehe auch:



</doc>
<doc id="14609" url="https://de.wikipedia.org/wiki?curid=14609" title="Bildschirm">
Bildschirm

Ein Bildschirm ("Screen, Display" oder "Monitor") ist eine elektrisch angesteuerte Anzeige ohne bewegliche Teile zur optischen Signalisierung von veränderlichen Informationen wie Bildern oder Zeichen. Der Bildschirm kann dabei ein eigenständiges Gerät oder Teil eines Gerätes sein.

Die Begriffe "Bildschirm", "Screen", "Display" und "Monitor" werden im Deutschen häufig synonym verwendet. Gleichwohl gibt es sprachliche Unterschiede. Im Gegensatz zum Videoprojektor, dessen Projektionsfläche, etwa die „Leinwand“, englisch ebenfalls ', ‚Schirm‘, genannt wird, ist die Darstellung eines Bildschirms direkt auf dem Gerät sichtbar. "Displays" ( allg. ‚Anzeige‘) werden im Deutschen Anzeigegeräte im weiteren Sinne genannt, etwa auch Flip-dot displays. Als "Monitor" (engl. ' ‚Überwachung‘) werden hingegen nur Bildschirme benannt, die als Bauteile und Peripheriegeräte eingesetzt werden.

Als Ausgabegerät kommen Bildschirme auf Basis von Braunschen Röhren seit den 1930er Jahren zum Einsatz, zunächst mit vektorieller Ansteuerung (siehe Vektorbildschirm). Die später aufgekommenen Videoterminals verwenden vorwiegend Rastergrafiken zum Aufbau des Bildes. Bei den meisten Videoterminals lassen sich die einzelnen Bildpunkte nicht direkt ansprechen, vielmehr übernimmt ein Zeichengenerator "(character prozessor)" die Darstellung von einzelnen Zeichen; die Anzahl und das Aussehen darstellbarer Zeichen sind somit vorgegeben.

In den frühen 1980er Jahren wurde zunehmend die graphische Ausgabe wichtiger, treibend hierbei war unter anderem die Entwicklung von CAD und von grafischen Benutzeroberflächen (GUI, Graphical User Interface), die mit dem Macintosh populär wurden.

2003 wurden nach einer Erhebung der "Gesellschaft für Unterhaltungs- und Kommunikationselektronik" (gfu) in Deutschland erstmals mehr Flüssigkristallbildschirme abgesetzt als konventionelle Geräte mit Bildröhre. Die Bildschirmdiagonalen für typische Desktop-Anwendungen liegen heute meistens zwischen 50 und 75 cm, für Notebooks sind etwa 34 bis 43 cm üblich.

Frühe Personal Computer verfügten wie damalige Computerterminals häufig über integrierte Bildschirme. Heute ist das nur noch bei portablen Computern wie Notebooks und Handhelds üblich. Bildschirme für ortsfesten Einsatz werden in der Regel in separate Gehäuse eingebaut, die auch die Elektronik zu ihrer Ansteuerung beinhalten und meist über standardisierte Schnittstellen wie VGA oder DVI an eine in den Computer eingesteckte Grafikkarte angeschlossen werden.

Seit etwa 2001 gibt es auch Bildschirme, die dreidimensionale Bilder darstellen können, sogenannte autostereoskopische Displays oder auch 3D-Monitore. Die Entwicklung befindet sich noch im Anfangsstadium, eine perfekt ausgearbeitete virtuelle Realität ist mit ihnen noch nicht möglich.

Bei einem sogenannten Smart Display handelt es sich um einen Monitor, der per Funk mit dem Rechner verbunden und so in Grenzen portabel ist.

Die Bildschirmdiagonale ist die Größenangabe des Bildschirms und bezieht sich immer auf die Diagonale der Bildröhre, diese war bei Bildröhren etwas größer als die sichtbare Diagonale. Weitere Kennzeichen eines Monitors sind das Seitenverhältnis, die maximale Zeilenfrequenz, aus der sich für eine bestimmte Bildhöhe die Bildwiederholfrequenz ergibt, der maximale Pixeltakt (aus diesen drei Frequenzen sowie dem vertikalen und horizontalen blanking interval ergibt sich die Auflösung), die Helligkeit, der Kontrast, die Reaktionszeit sowie besonders bei Flüssigkristallbildschirmen die Anzahl der Pixel und der maximale Blickwinkel. Bei Röhrenmonitoren ist noch die Art (Loch-, Streifen- oder Schlitzmaske) und das Raster der Maske relevant.

Eine Mehrfachanzeige, auch Multifunktionsanzeige, dient der Darstellung mehrfacher Informationen. Dies kann ein einzelnes elektronisches Gerät oder ein Verbundsystem mit mehreren Bildschirmen sein. Sinn und Zweck dabei ist, eine große Anzahl von benötigten Werten oder auch Bildern und Signalen zur gleichen Zeit mit mehreren Anzeigegeräten oder nacheinander auf demselben "Display" anzeigen zu können. Dazu müssen vorgegebene oder aktuell aufbereitete Daten visualisiert und ausgegeben werden.

Heute wird dies bei einer Vielzahl von Anwendungen verwendet, beispielsweise beim elektronischen Fluginstrumenten-Anzeigesystem in Luftfahrzeugen, bei Mobiltelefonen (Handys), bei Tablet-PCs oder bei digitalem Fernsehen.

Bekannte und weitverbreitete Methoden zur simultanen Darstellung ist die Fenstertechnik – die dem Betriebssystem Windows zu seinem Namen verhalf – beim Personal-Computer. Hierbei können verschiedene Programme in sogenannten „Fenstern“ nebeneinander angeordnet werden, wobei auch hier ein oder mehrere Monitore eingesetzt werden können.

Verwendete Darstellungstechniken sind:


Anwendung finden diese in




</doc>
<doc id="14611" url="https://de.wikipedia.org/wiki?curid=14611" title="Personal Computer Memory Card International Association">
Personal Computer Memory Card International Association

Die 1990 gegründete Personal Computer Memory Card International Association (PCMCIA) war eine Vereinigung von IT-Herstellern; zu ihren Mitgliedern zählten Dell, Hewlett-Packard, IBM, Intel, Lexar Media, Microsoft, SCM Microsystems und Texas Instruments.

Sie war Namensgeber eines Standards für Erweiterungskarten mobiler Computer. Diese Karten sind unter den Namen PCMCIA-Karte oder PC Card bekannt.

Die PCMCIA hat sich 2009/2010 aufgelöst. Die Spezifikationen sind vom USB Implementers Forum (USB-IF) übernommen worden.

Der PCMCIA-Standard wird häufig als Beispiel für abschreckende, nicht selbsterklärende Bezeichnungen im Computerbereich genannt. Andrew Grove (ehemals CEO von Intel) sagte einmal: “People Can’t Memorize Computer Industry Acronyms” (etwa: "Niemand kann sich die Akronyme der Computer-Industrie merken"); dieses Zitat fand seinen festen Platz im Sprachschatz von Computerspezialisten.


</doc>
<doc id="14617" url="https://de.wikipedia.org/wiki?curid=14617" title="Flüssigkristallanzeige">
Flüssigkristallanzeige

Eine Flüssigkristallanzeige (, "LCD") oder ein Flüssigkristallbildschirm ist eine Anzeige (engl. ) oder ein Bildschirm, dessen Funktion darauf beruht, dass Flüssigkristalle die Polarisationsrichtung von Licht beeinflussen, wenn ein bestimmtes Maß an elektrischer Spannung angelegt wird.

LCDs bestehen aus Segmenten, die unabhängig voneinander ihre Transparenz ändern können. Dazu wird mit elektrischer Spannung in jedem Segment die Ausrichtung der Flüssigkristalle gesteuert. Damit ändert sich die Durchlässigkeit für polarisiertes Licht, das mit einer Hintergrundbeleuchtung und Polarisationsfiltern erzeugt wird.

Soll ein Display beliebige Inhalte darstellen können, sind die Segmente in einem gleichmäßigen Raster angeordnet (siehe Pixel). Bei Geräten, die nur bestimmte Zeichen darstellen sollen, haben die Segmente oft eine speziell darauf abgestimmte Form, so insbesondere bei der Sieben-Segment-Anzeige zur Darstellung von Zahlen (siehe auch Matrixanzeige).

Eine Weiterentwicklung ist das Aktiv-Matrix-Display, das zur Ansteuerung eine Matrix von Dünnschichttransistoren (engl. , TFT) enthält. Bei Flachbildschirmen dominiert diese Technik seit etwa 2005.

In der Werbung wird seit etwa 2009 häufig von LED-Fernsehern gesprochen. Dabei handelt es sich in den meisten Fällen ebenfalls um Flüssigkristallbildschirme (LCDs) zur Bilddarstellung, bei denen zur Hintergrundbeleuchtung LEDs eingesetzt werden (LED-Backlight). Bildschirme mit organischen Leuchtdioden (OLEDs) sind für großflächige Fernsehgeräte erst seit kurzem in Europa erhältlich, nachdem anfänglich nur der Testmarkt in Südkorea bedient wurde.

LCDs finden Verwendung an vielen elektronischen Geräten, etwa in der Unterhaltungselektronik, an Messgeräten, Mobiltelefonen, Digitaluhren und Taschenrechnern. Auch Head-up-Displays und Videoprojektoren arbeiten mit dieser Technik.

1904 veröffentlichte Otto Lehmann sein Hauptwerk "Flüssige Kristalle". Im Jahre 1911 beschrieb Charles Mauguin die Struktur und die Eigenschaften von Flüssigkristallen. 1936 erhielt die American Marconi Wireless Telegraph Company ein Patent auf die erste praktische Anwendung der Technologie, das Flüssigkristall-Lichtventil. 1962 erschien die erste wichtige englischsprachige Publikation über "Molekulare Struktur und Eigenschaften von Flüssigkristallen" (Original: "") von George William Gray.

Pionierarbeiten über Flüssigkristalle wurden in den späten 1960er Jahren vom britischen "Radar Research Establishment" in Malvern geleistet. Das dortige Team unterstützte die fortschreitenden Arbeiten von George William Gray, der mit seinem Team an der Universität Hull in Kingston upon Hull (England) schließlich flüssigkristalline Cyanobiphenyl-Verbindungen synthetisierte, die die Anforderungen bezüglich Stabilität und Temperaturverhalten für LCD erfüllte.

Das erste funktionierende LCD basierte auf dem "dynamischen Streumodus" (engl. , "DSM") und wurde 1968 in den USA von einer Gruppe bei der Radio Corporation of America (RCA) unter der Leitung von George H. Heilmeier eingeführt. Heilmeier gründete die Firma Optel, die einige LCDs nach diesem Prinzip entwickelte.

Am 4. Dezember 1970 meldeten Martin Schadt und Wolfgang Helfrich, damals im Central Research Laboratory der Firma Hoffmann-LaRoche tätig, das erste Patent über die „nematische Drehzelle“ (auch "TN-Zelle", "Schadt-Helfrich-Zelle", ) in der Schweiz an. Das Patent wurde in 21 Ländern erteilt, nicht jedoch in Deutschland.

Am 22. April 1971 reichte James Fergason von der Kent State University in den USA seine Patentanmeldung über den in Flüssigkristallen ein und stellte 1971 in seiner Firma "ILIXCO", die heute (2005) "LXD Incorporated" heißt, LCD mit dieser Technik her. Sie ersetzten schnell die schlechteren DSM-Typen.

Ideen, welche heute in "Aktiv-Matrix-Displays" verwendet werden, entstanden bei der Konzeption von Flüssigkristall-Balkenanzeigen (engl. ). So wurde 1971 ein LC-Matrixdisplay für Balkenanzeigen bei der Firma Brown, Boveri & Cie, Baden, Schweiz, entwickelt, wobei jedem LC-Segment eine Diode (nichtlineares Element) vorgeschaltet und ein zusätzlicher Kondensator als Speicherelement parallelgeschaltet wurde.

Am 28. Juni 1973 wurde in den USA eine Anordnung zur Balkenanzeige zum Patent angemeldet, bei welcher Interdigital-Elektroden auf nur einer Glasplatte angebracht werden, um ein elektrisches Feld parallel zu der Glasplatte zu erzeugen und die Flüssigkristalle in einer Ebene umzuorientieren (, IPS, siehe "Anzeigetypen").

Am 7. Juli 1983 mit einer Ergänzung vom 28. Oktober 1983 reichten H. Amstutz und seine Miterfinder des Forschungszentrums der Firma Brown, Boveri & Cie, Baden, Schweiz, eine Patentanmeldung ein, welche die Basis für STN-LCD bildete (siehe "Anzeigetypen").
Mit STN-LCD ließen sich erstmals monochrome, passive Matrixanzeigen mit ausreichender Auflösung für einfache Bilddarstellungen realisieren (siehe Darstellung einer Weltkarte unter "Elektronische Ansteuerung"). Dieses Patent wurde in vielen Ländern erteilt. Vor allem asiatische Hersteller wurden Lizenznehmer (weltweit über 60).

Am 9. Januar 1990 meldeten G. Baur und seine Miterfinder der Fraunhofer-Gesellschaft in Freiburg i. Br. ein Patent in Deutschland an, welches die konkrete Basis für optimiertes "In-Plane Switching" in Matrixanzeigen (IPS-LCDs) bildete. Dieses Patent wurde in vielen Ländern erteilt, von der Firma Merck KGaA Darmstadt, dem weltweit größten Hersteller von Flüssigkristallsubstanzen, übernommen und an viele Firmen lizenziert.

Am 18. September 1992 mit Nachtrag vom 20. Januar 1993 machten K. Kondo und seine Miterfinder bei Hitachi eine Patentanmeldung in Japan, welche als weiteres wesentliches Element des "In-Plane Switching" eine spezifisch geeignete Verbindungstechnik der Dünnfilmtransistoren in Matrixform darlegte. Später erfolgte eine weitere Hitachi-Patentanmeldung, welche die Blickwinkelabhängigkeit dieser Art von FK-Anzeigen verbesserte.

In Flüssigkristalldisplays verwendete Flüssigkristalle sind organische Verbindungen, die sowohl Eigenschaften von Flüssigkeiten als auch von Festkörpern aufweisen. Sie sind einerseits mehr oder weniger fluide als eine Flüssigkeit, andererseits zeigen sie Eigenschaften wie Doppelbrechung.

Ein einfaches Flüssigkristall-Anzeigeelement lässt sich mit der Schadt-Helfrich-Zelle (nematische Drehzelle, engl. , "TN"-Zelle) realisieren. In nebenstehender Darstellung sind nur die Bestandteile der unteren Hälfte einer solchen Drehzelle nummeriert. Die Bezugsnummern (x) sind in dieser Beschreibung eingefügt. Die Innenseiten zweier sehr dünner Glasplatten (Substrate) (4) sind mit einer transparenten Elektrodenschicht (Indium-Zinn-Oxidschicht, ITO) (3) überzogen, dazwischen befindet sich die Flüssigkristallschicht (1) von weniger als 10 Mikrometer Dicke. Die Flüssigkristallmoleküle ordnen sich in eine vorgegebene Richtung, nämlich parallel zu der beispielsweise mit einem Polyimid (2) beschichteten und in einer Vorzugsrichtung gebürsteten Oberfläche. Die Vorzugsrichtungen der beiden Substratplatten sind um 90° zueinander verdreht. Bei der Herstellung handgefertigter Prototypen kann man zum Bürsten Polystyrolschaum oder mit samtartigen Textilien beschichtete Walzen benutzen.

Zusätzlich sind die beiden Substratplatten (4) mit um 90° zueinander verdrehten Polarisationsfiltern (5) beschichtet. Auf der Rückseite dieser Anordnung kann sich ein Spiegel (6) befinden (Reflektor oder Transreflektor), der das einfallende Licht zurückwirft (reflexive Betriebsart). In der transmissiven Betriebsart befindet sich anstelle des Reflektors eine Beleuchtungseinheit hinter dem Anzeigeelement.

Aus den gegeneinander verdrehten Substratplatten ergibt sich eine schraubenförmige Struktur im Flüssigkristall; bei einer um 90° gedrehten Schraube spricht man von TN. Einfallendes Licht wird also vor dem Eintritt in die Flüssigkristallschicht linear polarisiert. Durch die Verdrillung der Moleküle folgt eine Drehung der Polarisationsrichtung des Lichtes, wodurch das Licht den zweiten Polarisator passieren kann und die Zelle lichtdurchlässig (transparent) ist. Da das Display im Ruhezustand durchsichtig ist, wird diese Betriebsart als "Normally-White-Mode" bezeichnet. Legt man eine elektrische Spannung an die Elektroden an, so tritt unter dem Einfluss des elektrischen Feldes eine Drehung der Flüssigkristallmoleküle ein, die sich parallel zum elektrischen Feld ausrichten. Die Verdrillung wird damit zunehmend aufgehoben, die Polarisationsrichtung des Lichts wird nicht mehr gedreht und damit kann es den zweiten Polarisationsfilter nicht mehr passieren.

Ordnet man die Polarisationsfilter parallel zueinander an, dann ist die Zelle ohne Spannung dunkel und wird erst mit zunehmender Spannung transparent. Man spricht dann vom "Normally-Black-Mode". Die Schadt-Helfrich-Zelle ist also (wie andere Flüssigkristallanzeigen auch) ein "spannungsgesteuertes Lichtventil". Ein Bildschirm kann aus beliebig vielen solcher Zellen (Bildelemente, Pixel) bestehen. Beim Taschenrechner stellt eine einfache 7-Segment-Anzeige jeweils eine Ziffer dar, bei einem farbfähigen Bildschirm werden pro Bildelement (Pixel) drei Teilbildelemente (Subpixel) für die Grundfarben Rot, Grün und Blau verwendet.

Bei (monochromen) "STN"-Displays (engl. ) wird der Verdrillwinkel der Moleküle auf 180° bis 270° erhöht. Dadurch kann eine steilere elektro-optische Kennlinie und so eine verbesserte Multiplexbarkeit als bei TN-Displays erreicht werden. Aufgrund verschiedener technischer Beschränkungen gelingt es nur mit einigem Aufwand (doppelte Zelle = DSTN-Zelle, Kompensation mit doppelbrechenden Verzögerungsfolien – ), die Darstellung farbneutral zu gestalten (d. h. nur Graustufen zwischen Schwarz und Weiß zu erzeugen). Stattdessen sind die Hellzustände gelblich und die Dunkelzustände fallen dunkelblau (mit Violett-Ton) aus. Eine Weiterentwicklung stellt "CSTN" (engl. color super-twist nematic) durch die Firma Sharp dar, bei dem Filter in den drei Grundfarben Rot, Grün und Blau vor den Pixeln für die Darstellung von Farben verwendet werden.

Es wurde mit verschiedenen Techniken versucht, eine Schwarz-Weiß-Darstellung auf dem Passiv-Matrix-Display zu erreichen: mit der "Guest-Host-Technik", dem "OMI-Verfahren" von Martin Schadt () und der "Double-Super-Twisted-Technik". Durchgesetzt hat sich letztere als DSTN-Technik.

Das Aufbauprinzip dieser "DSTN"-Zelle ist im Bild zu erkennen. Es liegen nun zwei STN-Schichten vor. In der aktiven Zelle – das ist diejenige, an die ein elektrisches Feld gelegt werden kann – ist der Flüssigkristall um 240° gegen den Uhrzeigersinn gedreht. Die passive Zelle enthält ebenfalls nematisches Material, das hier aber um 240° mit dem Uhrzeigersinn gedreht vorliegt. Beide Zellen sind so zueinander gedreht, dass die Orientierung der Stäbchen an der Eingangsseite senkrecht zu der an der Ausgangsseite ist. Die Polarisationsfolien sind ebenfalls um 90° gegeneinander gedreht.
In der konventionellen TN- oder STN-Zelle erhält man nach dem Durchgang linear polarisierten Lichtes genau betrachtet nicht einfach linear polarisiertes Licht mit verdrehter Schwingungsebene, sondern elliptisch (oder zirkular) polarisiertes Licht. Die Spitze des elektrischen Feldvektors beschreibt eine Ellipse oder einen Kreis. Solches Licht geht durch den Polarisator hindurch, wobei die durch den Dichroismus bewirkte Farbaufspaltung – abhängig von der Polarisation und der Folienorientierung am Strahlaustritt – zu farbigem Licht führt.
Im Bild ist die Arbeitsweise von DSTN-LCD verdeutlicht: Weißes Licht fällt auf den hinteren Polarisator (im Bild unten) und wird dort linear polarisiert. Dann gelangt es in die aktive STN-Zelle, die (ohne Feld) nun zirkular polarisiertes Licht daraus erzeugt. Dieses Licht ist – wie bei der herkömmlichen STN-Zelle – durch Dichroismus verändert. Der Weg durch die anschließende passive Zelle (die das gleiche Flüssigkristall-Material enthält wie die erste – aktive – Zelle, aber in entgegengesetzter Richtung verdreht) führt zur Kompensation der Farbaufspaltung (die Phasendifferenz wird gleich Null). Als Ergebnis liegt linear polarisiertes Licht vor, das die gleiche Schwingungsebene aufweist wie zuvor nach dem Passieren der hinteren Polarisationsfolie. Weil aber der vordere Polarisator um 90° verdreht ist, lässt er kein Licht durch: Der Bildschirm ist an dieser Stelle schwarz.

Liegt an der aktiven Zelle ein elektrisches Feld an, dann geht das linear polarisierte Licht aus dem hinteren Polarisator dort glatt hindurch, ohne verändert zu werden. Erst in der passiven Zelle erfolgt nun zirkulare Polarisation. Weil aber zirkular polarisiertes Licht von Polarisatoren nicht zurückgehalten wird, ist der Bildschirm an dieser Stelle hell. Durch genaues Justieren sowohl des verwendeten Materials als auch der Zellenabmessungen wird das durchgelassene Licht weiß.

Auf diese Weise wurden Displays realisiert, die ein sauberes Schwarz-Weiß mit einem Kontrastverhältnis von bis zu 15:1 bieten.

Der komplexe Aufbau einer DSTN-Flüssigkristallzelle bedingt einen relativ hohen Aufwand bei ihrer Herstellung. Es wurde deshalb ein neues Verfahren entwickelt, das zu flacheren Displays mit geringerem Gewicht führt. Diese neue Lösung trägt den Namen „Triple Super-Twisted Nematic“-LCD ("TSTN"). Das Bild zeigt das Aufbauprinzip eines solchen TSTN-Displays.

Hier findet sich nur eine STN-LC-Zelle. Die Farbstörungen der normalen STN-Technik werden durch zwei spezielle Folien ausgeglichen, die vor und hinter der Zelle – zwischen Polarisator und Glas – angebracht sind. Diese Folien sind verantwortlich für einen weiteren Namen dieser Technik: "FST", was bedeutet „Film-Super-Twisted“ (gelegentlich bezeichnet man Displays, in denen nur eine Kompensationsfolie verwendet wird, als FST-, solche mit zwei oder mehr Folien als TST-LCD; ebenfalls geläufig ist die Bezeichnung FSTN für Film-STN). Der verbesserte Kontrast (bis zu 18:1), das geringere Gewicht und die flachere und weniger aufwendige Bauweise haben TSTN-LC-Displays zum Durchbruch verholfen. In Notebook-Computern wurden solche Displays als „VGA-Bildschirm“ erstmals realisiert.

Bei der Patterned-Vertical-Alignment-Technik, auch kurz "PVA" genannt, handelt es sich um eine Weiterentwicklung der Multi-Domain-Vertical-Alignment-Technik ("MVA") eines Herstellers. Beide Techniken basieren auf einem ähnlichen Verfahren. Die Vorteile von MVA-/PVA-Bildschirmen liegen in einem höheren Kontrast (> 1000:1 ist üblich) als bei einem TN-Bildschirm (< 800:1). Zudem bieten MVA-/PVA-Bildschirme eine große Blickwinkel-Unabhängigkeit. Der Nachteil von MVA/PVA-Bildschirmen ist, dass sie langsamer als TN-Bildschirme und daher für Bewegtbilder – wie bei Spielen und Videos – weniger gut geeignet sind. Zudem liegt der Preis über dem von TN-Bildschirmen.

Bei der In-Plane-Switching-Technik ("IPS", englisch für "in der Ebene schaltend") befinden sich die Elektroden nebeneinander in einer Ebene parallel zur Display-Oberfläche. Bei angelegter Spannung drehen sich die Moleküle in der Bildschirmebene; die für TN-Displays typische Schraubenform entfällt. "IPS" verringert die Blickwinkelabhängigkeit des Kontrastes.

An Untertypen unterscheidet man zwischen "S-IPS" (), "AS-IPS" (), "A-TW-IPS" (), "H-IPS" (Horizontal IPS), seit 2009 "E-IPS" () sowie seit 2011 "AH-IPS" (). Bis auf den H-IPS-Typ lassen sich die anderen IPS-Typen daran erkennen, dass sie, seitlich betrachtet und im Gegensatz zu VA-Panels, einen leichten lila Farbton aufweisen. Die E-IPS-Technologie, die 2009 auf den Markt kam, bietet einen noch größeren Blickwinkel und geringere Schaltzeiten von 5 Millisekunden.

Die Weiterentwicklung PLS () wurde von Samsung entwickelt und bietet unter anderem eine höhere Transparenz (geringerer Stromverbrauch bei gleicher Helligkeit) und das Wegfallen des bei IPS-Panels typischen Glitzer-Effektes. Die ersten Bildschirme mit PLS-Panels kamen 2011 auf den Markt.

Bei ersten Prototyp-LCDs Anfang der 1970er Jahre wurde erkannt, dass der Gleichstrombetrieb irreversible elektrochemische Prozesse in der Flüssigkristallschicht und damit Lebensdauerbeschränkungen der Anzeige verursachen kann. Obschon es sich im Gegensatz zu LCDs nach dem (engl., DSM) bei TN-Zellen um einen elektrischen Feldeffekt handelt, also kein Stromfluss nötig ist, bestanden trotzdem die erwähnten Probleme beim Anlegen einer Gleichspannung. Deshalb wurden lange Zeit alle kommerziell eingesetzten LCDs mit Wechselspannung betrieben. Im Wesentlichen ist ein LCD-Element eine kleine elektrische Kapazität, welche periodisch durch eine impulsartige "elektrische Spannung" umgeladen wird. Für diesen Zweck eigneten sich die schon damals verfügbaren elektronischen Komplementär-MOS-Schaltkreise (CMOS-ICs) sowohl bezüglich einstellbarem Spannungshub wie auch Symmetrie der Eigenschaften ausgezeichnet. Einer der Vorteile von TN-Zellen ist die tiefe Betriebsspannung und damit die geringe Stromaufnahme. Schon früh wurden Flüssigkristallsubstanzen für TN-LCDs synthetisiert, welche bei 3-Volt-Betrieb einen guten Kontrast ergaben und einen brauchbaren Betriebstemperaturbereich hatten.
Der Betrieb von Passiv-Matrix-Displays hängt davon ab, wie ausgeprägt eine elektrooptische Schwellenspannung vorhanden ist. Weil die Ansteuerung der einzelnen Bildelemente (Pixel) periodisch wiederholt werden muss, um nicht nur eine Zeile, sondern alle Pixel einer Matrix anzusteuern, werden z. B. die Zeilen einer Matrix sequenziell impulsförmig angesteuert. Bei den Kolonnen wird die Bildinformation derart eingegeben, dass bei den aktivierten Bildelementen eine möglichst große Spannungsdifferenz zu den Zeilenimpulsen und an den nicht aktivierten Bildelementen eine Spannungsdifferenz unterhalb des Schwellenwerts entsteht. Entscheidend zur Optimierung war die Erkenntnis von Peter J. Wild, dass bei impulsartiger, periodisch wiederholter Ansteuerung der Effektivwert (englisch , RMS) der Spannungsdifferenzen maßgebend ist. Detaillierte Angaben über die bestmöglichen Ansteuerungstechniken für Passiv-Matrix-Displays finden sich in weiterführender Literatur.

Wie schon erwähnt, ist eine elektrooptische Kennlinie mit ausgeprägtem Schwellenwert und daraufhin steilem Anstieg bei höherer Spannung Voraussetzung zur Realisierung von Passiv-Matrix-Displays mit vielen Zeilen und Spalten. Nur so kann ein ausreichender Kontrast auch bei Matrizen mit vielen Pixeln erzielt werden. Die langwierige Optimierung der Spannungs-Kontrast-Kennlinie durch Flüssigkristallsubstanzwahl und Zellstruktur von TN-Zellen brachte nicht die gewünschten Resultate. Es dauerte über 10 Jahre, bis 1983 der Durchbruch mit der Entdeckung der superverdrillten nematischen LCDs (englisch , STN-LCDs) bei Brown, Boveri & Cie, Baden (heute Asea Brown Boveri, ABB) gelang. Die Kennlinie von STN-Zellen ist viel besser zur Ansteuerung von Passiv-Matrix-Displays geeignet als diejenige von TN-Zellen.

Schon relativ früh wurde versucht, jedem Pixel individuell Schalttransistoren vorzuschalten, um derart die Kennlinienbeschränkungen eines Passiv-Matrix-Display zu umgehen. Dazu mussten Dünnschichttransistoren (englisch , TFT) geeigneter Dimension und Verbindungstechnik in Matrixanordnung auf dem Glassubstrat der Flüssigkristallanzeige aufgebracht werden. Man nennt eine Anzeige dieser Technik Aktiv-Matrix-Display, weil der Betrieb der einzelnen Pixel durch die zugeordneten Transistoren aktiv gesteuert wird. Obschon die Ideen dazu bereits 1968–1973 bei der Radio Corporation of America (RCA) und bei Westinghouse Research Laboratories, USA, formuliert wurden, dauerte es noch lange, bis die technologischen Voraussetzungen für die Massenfertigung erarbeitet waren. Insbesondere wurde mit verschiedenen Halbleitermaterialien experimentiert, bis sich schließlich eine spezielle Art von "amorphem Silizium" (siehe Dünnschichttransistor) als am besten geeignet für Feldeffekttransistoren in Dünnschichttechnik durchsetzte. Dieser materialtechnische Durchbruch gelang in Europa. Bei der Realisierung von kommerziellen Produkten waren japanische Firmen federführend.
Ohne die erwähnten Fortschritte bei der elektronischen Ansteuerung wären großflächige Fernseh-Flüssigkristallbildschirme nicht möglich geworden. Allerdings sind Aktiv-Matrix-Displays wegen der zahlreichen zusätzlichen Prozessschritte für TFTs in der Herstellung teurer und können zudem deswegen Pixelfehler aufweisen, sodass für einfachere Anzeigen mit geringerem Informationsgehalt immer noch Passiv-Matrix-Displays zum Einsatz kommen.

Von Beginn an bestand die Aufgabe, die transparenten Leiterbahnen auf beiden LCD-Glassubstraten mit der Ansteuerungselektronik zu verbinden. Dazu wurden neuartige Verbindungstechniken entwickelt.

Für Anzeigen mit nicht zu engem Kontaktraster kommen sogenannte "Zebras" (siehe Leitgummi) zum Einsatz, welche abwechselnd aus isolierenden und leitenden Elastomer„kanälen“ bestehen. In nebenstehender Aufnahme mit einem Vergleichsmaßstab in cm ist das dunkle Raster des Zebragummis von 180 Mikrometer deshalb nur bei Anklicken des Bildes mit Vergrößerung sichtbar: im rosaroten isolierenden Elastomer-Band befinden sich die schwarzen Leitelemente, separiert durch isolierende Elemente. Durch den Aufbau (die Elemente sind wesentlich kleiner als die zu kontaktierenden Flächen) spielen Lagetoleranzen des Gummis keine Rolle. Der Gummi kann Maßtoleranzen abfedern. Typische Anwendungen sind Displays mit Siebensegmentanzeigen.

Früh wurden auch Lösungen mit erprobt. Dabei wurden auf die Kontakte der Ansteuerungsschaltkreise Lötpunkte aufgebracht, dann der Chip auf den korrespondierenden Kontakten der Anzeige positioniert und daraufhin bei erhöhter Temperatur angelötet.

Einen wichtigen Fortschritt bedeutete die Verwendung von flexiblen, dünnen Leiterplatten mit entsprechenden Verbindungsbahnen zur Anzeige, welche ein sehr enges Kontaktraster erlauben. Diese Leiterplatten tragen auch oft die ICs als Nacktchips (Flip-Chip-Montage), die die digitalen seriellen Datenströme wandeln.

Bei Passiv-Matrix-Displays werden die Bildelemente (ein Segment oder ein Symbol) im Zeitmultiplexbetrieb angesteuert. Das heißt, dass jedes Bildelement direkt und permanent mit einer Ansteuerschaltung verbunden ist, deren Ausgang einen geringen Widerstand hat. Deshalb baut sich die zum Zeitpunkt der Adressierung aufgebrachte Ladung relativ schnell wieder ab und muss in der folgenden Bildperiode (engl. ) wieder erneuert werden. Dieser Wechsel in der elektrischen Ansteuerung führt zu ausgeprägten Modulationen der optischen Antwort der Anzeige (sog. ).

Bei Adressierung und Ansteuerung über eine Matrix mit aktiven Bauelementen bei Aktiv-Matrix-Displays wird zum Zeitpunkt der Adressierung eine Ladung auf das Bildelement aufgebracht, dem meist noch ein zusätzlicher Kondensator parallelgeschaltet ist (Speicherkondensator). Nach dem Aufbringen der Ladung, deren Höhe der Dateninformation entspricht, wird das aktive Bauelement (meist ein Dünnschichttransistor, "TFT") wieder in den hochohmigen Zustand geschaltet, wodurch die Ladung und somit die Ansteuerung während einer Bildperiode im Wesentlichen erhalten bleibt.

Diese Art der Ansteuerung bewirkt bei Aktiv-Matrix-Displays eine höhere effektive Spannung über dem Bildelement, damit eine höhere Aussteuerung des Flüssigkristalls und damit einen verbesserten Kontrast und eine reduzierte Abhängigkeit des Kontrastes von der Betrachtungsrichtung.

Die LC-Bildschirme haben gegenüber den älteren Kathodenstrahlröhrenbildschirmen (CRT) einige Vorteile.

Darüber hinaus besitzen sie ein flimmerfreies, verzerrungsfreies, bei Idealauflösung scharfes Bild, ein geringeres Gewicht sowie eine geringe Einbautiefe.

Im Gegensatz zu Anzeigegeräten mit Kathodenstrahlröhre werden Flüssigkristallbildschirme in der Praxis nicht durch Magnetfelder wie das Erdmagnetfeld oder die Magnetfelder von Oberleitung, NMR-Geräten, Transformatoren oder Lautsprechern beeinträchtigt.

Während der Entwicklung der Geräte, mindestens bis zur Entwicklung von mit TFTs angesteuerten LCD, bestanden Nachteile durch den geringen Kontrast und die langen Schaltzeiten. Mittlerweile kann die Farbwiedergabe von LCD (der darstellbare Farbraum, engl. ) durch Anpassung der Hintergrundbeleuchtung sogar extremen Anforderungen gerecht werden (, ). Ein weiteres Problem war der eingeschränkte Bereich von Betrachtungsrichtungen mit konstantem Kontrast und gleichbleibendem Farbeindruck; neuere Techniken wie "In-Plane-Switching" (IPS), (MVA) und (PVA) sowie die Anwendung von doppelbrechenden Kompensationsfolien () schafften hier Abhilfe. Diese Nachteile existieren weiterhin, sind aber bei weitem nicht mehr so gravierend wie früher. Da jeder Pixel eine eigene kleine Einheit darstellt, kommt es produktionsbedingt zu vereinzelten Fehlern (Pixelfehler): Pixel, die durchgängig nur in einer Farbe leuchten oder die vorgegebene Farbe fehlerhaft wiedergeben. Je nach Anzahl der fehlerhaften Pixel werden die Displays in verschiedene Fehlerklassen eingestuft, die Einfluss auf den Preis haben können.

Bei der Herstellung wird die physikalische Bildauflösung festgelegt, die Ansteuerung mit einem Signal anderer Auflösung kann zu Qualitätsverlusten führen. Ein TFT-basierter LC-Bildschirm liefert im Vergleich zu einem CRT-Bildschirm ein viel schärferes Bild – allerdings nur in seiner konstruktionsbedingten physikalischen Auflösung. Signale geringerer Auflösung müssen interpoliert werden und erscheinen verschwommen. Alternativ lässt sich das Bild auch mit schwarzen Rändern zentriert in voller Schärfe darstellen (bei digitalem Anschluss lässt sich das üblicherweise im Grafikkartentreiber einstellen).

Die Hintergrundbeleuchtung durch sog. Kaltkathodenröhren wird gefiltert, um die Grundfarben der Pixel (zumeist rot, grün und blau) zu erhalten, allerdings muss der Kompromiss zwischen Helligkeit und Farbwiedergabequalität gefunden werden. Die Farben von LCD sind keineswegs weniger gesättigt als bei der CRT- oder Plasmabildschirmtechnologie. Vom erzeugten Licht dringen nur etwa 4 % durch das Panel (bei weißen Bildinhalten).

Ein Grund dafür, warum Röhrenmonitore (CRT) in Tests oft besser abschnitten als Flachbildschirme, ist keinesfalls der bessere Schwarzwert im Dunkelraum und der Kontrast zu den hellen Bildstellen, wenn kein Umgebungslicht auf den Bildschirm fällt, sondern die bessere Wiedergabe von bewegten Bildinhalten (siehe unten). Mittlerweile ist die LCD-Technik jedoch so weit fortgeschritten, dass teils sogar bessere Ergebnisse (je nach Art des Panels) als mit CRT-Monitoren erreicht werden können.

Die Leuchtstoffröhren der Hintergrundbeleuchtung haben eine begrenzte Lebensdauer (etwa 100.000 Stunden). Die durch die Hintergrundbeleuchtung beeinflusste Qualität der Darstellung von Weißflächen ändert sich schon deutlich nach nur wenigen Tausend Betriebsstunden meist stärker ins Gelbliche, da sich Leuchtstärke der Leuchtstoffröhren mit der Zeit verringert. Allerdings lässt auch die Helligkeit von Röhrenmonitoren im Laufe des Betriebs nach. Die Hintergrundbeleuchtung mittels LEDs ist zwar alterungsbeständiger, zeigt aber auch je nach Typ der verwendeten Leuchtdioden und Betriebsweise langsame Alterungserscheinungen. Zudem erlaubt Beleuchtung mittels LEDs eine kompaktere Bauweise, homogenere Ausleuchtung und Kontraststeigerung durch selektive, vom Bildinhalt abhängige Ansteuerung (LED-Backlight).

Die Reaktionszeit moderner LCDs liegt derzeit zwischen 1 ms und 5 ms. Hierbei ist die Reaktionszeit die Zeitspanne, die bei der Änderung der Leuchtdichte (Helligkeit) eines Bildbereiches von 10 % nach 90 % verstreicht; hierbei sind 0 % und 100 % die Leuchtdichten der stationären (eingeschwungenen) Zustände. Die Bildaufbauzeit nach ISO 13406-2 ist die "Summe der Schaltzeiten" von Hell nach Dunkel (oder umgekehrt) und wieder zurück. Aufgrund des asymptotischen Schaltverlaufs werden jedoch nach ISO 13406-2 Schaltzeiten von < 3 ms benötigt, um sichtbare Schlierenbildung zu vermeiden.

Die Einschaltzeit formula_1 (zunehmende Spannung) und die Ausschaltzeit formula_2 (abnehmende Spannung) ergibt sich nach den Formeln von Jakeman und Raynes:

Hierbei ist formula_5 die "Rotationsviskosität" des Flüssigkristalls, die die „Trägheit“ des Flüssigkristalls auf eine Änderung der Ausrichtung beschreibt; formula_6 der "Abstand" zwischen den Glasplatten (= Dicke der Flüssigkristallschicht); und formula_7 die "Elastizitätskonstante", welche die „Kraft“ (Drehmoment) der Rückstellung der Kristalle in die ursprüngliche Ausrichtungslage angibt.

Beispielsweise beschleunigt ein großes formula_7 die Rückstellung des Kristalls in den Ausgangszustand, wirkt jedoch auch der Ausrichtung des Kristalls bei Anlegen einer Spannung entgegen (durch die entsprechend erhöhte Schwellenspannung, formula_9). Auch lassen sich durch eine Verringerung der Schichtdicke, formula_6 die Schaltgeschwindigkeiten erhöhen. Wenn die Schichtdicke beispielsweise um 30 % verringert wird (formula_11) gehen die Schaltzeiten auf etwa die Hälfte zurück (denn formula_12).

Bei "Hold-Type-Displays" wie LCD und OLED-Bildschirmen bleibt der Zustand eines Pixels für die Dauer einer Bildperiode bestehen, bis die angelegte Spannung im Zuge des Bildaufbaus eines neuen Bildes geändert wird (Erhaltungsdarstellung). Da das Auge bei der Verfolgung eines bewegten Bildinhalts (englisch ) die „Helligkeit“ über eine Bildperiode integriert, während der Bildinhalt aber fixiert bleibt, kommt es zum Verwischen des Bildes auf der Netzhaut des Betrachters. Dies fällt besonders bei der Darstellung schnell bewegter Szenen auf und wird deshalb auch als Bewegungsunschärfe (auch engl. ) bezeichnet. Es ist zu beachten, dass selbst bei verschwindend geringen Schaltzeiten, das heißt bei nahezu unendlich schnellem Schalten, wegen der Erhaltungsdarstellung die Bewegungsunschärfe nicht beseitigt wäre, weshalb der Verwischeffekt auch bei schnellen OLED-Bildschirmen auftritt.

Neben dieser prinzipbedingter Unschärfe erzeugt die verzögerte Annahme des Soll-Werts bei einzelnen Pixel-Elemente ebenfalls unerwünschte Effekte ("Schlieren", "Schweif", "Schmieren"), die ähnlich störend wirken. Bei heutigen LCDs ist diese Art der Bewegungsunschärfe schon erheblich reduziert. Die Reaktionszeit von „grau nach grau“ (engl. ) liegt durchschnittlich bei 6 ms, dennoch können die Schaltzeiten in extremen Situationen (weiß-nach-schwarz, schwarz-nach-weiß, schwarz-nach-grau) erheblich davon abweichen.








In Digitaluhren und Taschenrechnern werden LCDs schon seit Anfang der 1970er Jahre verwendet. Diese einfarbigen Displays ohne Hintergrundbeleuchtung zeichnen sich durch geringsten Energieverbrauch und sehr hohe Lebensdauer aus und finden alternativlos überall dort Anwendung, wo ein langer wartungsfreier Betrieb erforderlich ist.

Später fanden LCDs als Aktiv-Matrix-Displays mit Hintergrundbeleuchtung Verbreitung über weitere tragbare oder batteriegespeiste Geräte wie etwa Mobiltelefone, Notebooks und ähnliches.

Typische Auflösungen bei Computer-Flachbildschirmen reichen von 1024×768 Pixel (38 cm/15″) über 2560×1600 Pixel (76 cm/30″), bis 3840x2160 Pixel, bei Notebooks reichen sie von 800×480 Pixel bis 3200×1800 Pixel. PDAs und portable DVD-Spieler weisen Auflösungen zwischen 320×240 und 800×480 Pixel, Displays von Standbild- und Videokameras zwischen 160×176 Pixel (84 Tausend Pixel) und 640×480 Pixel (900 Tausend Pixel) auf. Insbesondere bei Smartphones hat die Firma Apple mit dem „Retina-Display“ einen neuen Marketingbegriff für hohe Bildauflösung geschaffen.

Mittlerweile haben LCD- und Plasma-Displays die Kathodenstrahlröhre weitgehend verdrängt. Dies betrifft Computermonitore (seit 2006) und Fernsehgeräte mit größeren Bilddiagonalen (seit 2008). Auch andere Anwendungsgebiete wie Oszilloskope sind schon seit längerem in der Hand von computerangesteuerten LCD. 2003 wurden in Deutschland bereits mehr LCD als herkömmliche Röhrenmonitore für PCs und 2006 mehr Flachbildfernseher – also LCD und Plasmabildschirme – als Röhrengeräte verkauft.

Versuche, mit LCD-Matrixanzeigen Bildschirmprojektoren zu realisieren gab es ab den 1970er Jahren. Der Imagina 90 war weltweit der erste in Serie gefertigte Videogroßbildprojektor mit Flüssigkristallbildgenerator, der sich auch für den Dauerbetrieb eignete.

Die LCD-Technik hat in den letzten Jahren insbesondere durch die Entwicklung von Flachbildschirmen einen enormen Aufschwung erlebt. Große Produktionsstätten für Flachbildschirme wurden zunächst in Japan errichtet. Schon bald setzte jedoch die Abwanderung der Industrie in die neuen asiatischen Industrienationen ein, in denen billige Arbeitskräfte und üppige staatliche Förderung lockte. Derzeit befindet sich der Schwerpunkt der Flachbildschirmindustrie in Taiwan und insbesondere Südkorea. In Südkorea betreiben die dort ansässigen weltweit größten Flachbildschirmhersteller – Samsung, LG Display und Chi Mei Optoelectronics (CMO) – die zurzeit (2008) größten LC-Bildschirm-Produktionsstätten. Die Wanderung der Industrie geht jedoch weiter.

Auf der Suche nach noch kostengünstigeren Produktionsstandorten hat der Boom inzwischen China erreicht. Produktionsstätten zur Herstellung hochwertiger Flachbildschirme sind dort derzeit (2008) im Aufbau.

Aus der Sicht des Klimaschutzes wird die Flüssigkristallbildschirmfertigung als problematisch angesehen, da in der traditionellen Produktion sehr große Mengen klimagefährdender Substanzen eingesetzt würden. Im wichtigen „Arrayprozess“, in dem die TFT-Steuermatrix großflächig auf dünne Glasscheiben aufgebracht wird, werden potente Treibhausgase wie Schwefelhexafluorid (SF) – GWP 22800 COe – und Stickstofftrifluorid (NF) – GWP 17200 COe – in sehr großem Umfang verwendet und in die Atmosphäre freigesetzt, wie eine Studie aus dem Jahre 2008 aufzeigt.





</doc>
<doc id="14619" url="https://de.wikipedia.org/wiki?curid=14619" title="Kathodenstrahlröhrenbildschirm">
Kathodenstrahlröhrenbildschirm

Ein Kathodenstrahlröhrenbildschirm ist ein Bildschirm, der auf der Kathodenstrahlröhre von Ferdinand Braun (Braunsche Röhre) basiert. Häufig wird er auch als Kathodenstrahl-, Röhren- oder CRT- (Abk. für ) Bildschirm bezeichnet. Er kann in unterschiedlichen Geräten wie Oszilloskopen, Fernsehern und Bildschirmen (z. B. PC-Bildschirmen, Überwachungssysteme usw.) und vielen anderen Bereichen eingesetzt werden. Jedoch wird diese Technik seit den 2000er Jahren in weiten Bereichen durch Flachbildschirme verdrängt.

Bildschirme werden in verschiedenen Größen hergestellt. Dabei wird die Diagonale des Bildschirms als Maß benutzt. So haben Bildschirme für moderne Registrierkassen eine Diagonale von ca. 23 cm, während größere Computerbildschirme bis zu ca. 56 cm erreichen. Meist werden diese Diagonalen nicht in cm, sondern in Zoll angegeben. Auf größeren Bildschirmen können mehr Pixel und damit mehr Informationseinheiten dargestellt werden als auf kleineren Modellen, da die Bildauflösung nicht beliebig gesteigert werden kann. Im Unterhaltungsbereich (Fernsehgeräte) sind Bildschirmgrößen von bis zu 82 cm erhältlich. Entscheidend ist hier nicht das Auflösungsvermögen (die Anzahl der Pixel ist durch die jeweilige Fernsehnorm festgelegt), sondern der Betrachtungsabstand.

Mit Ausnahme des Oszilloskops und anderer wissenschaftlicher Geräte wird der Bildschirm zur Darstellung von Rastergrafiken benutzt. Dabei wird die Bildinformation in einer Abfolge von nacheinander übertragenen Informationen pro Pixel übertragen (Fernsehsignal). Diese wird dann von der Elektronik innerhalb des Gerätes aufbereitet und zur Darstellung des ursprünglichen Bildes auf der Leuchtschicht benutzt. Das Fernsehsignal ist dabei nur eine Möglichkeit, wie die Informationen zum Bildschirm gelangen können. In der Computertechnik werden die Informationen für die Primärfarben auf getrennten Signalwegen übertragen, ebenso die Informationen für die Synchronisation der Position des Elektronenstrahles auf der Leuchtschicht.

Hintergrund der Trennung und gemeinsamen Übertragung der Signale ist, dass von der Signalerzeugung auf der Grafikkarte zum Bildschirm nur kleine kabelgebundene Strecken überwunden werden müssen. Daher ist der stets mit Verlusten verbundene Aufwand der Mischung und Entmischung der komplexen analogen Signale hier nicht notwendig. Vom Fernsehstudio zum Fernsehzuschauer zuhause steht üblicherweise nur ein Übertragungskanal zu Verfügung, welcher die Übertragung über große Strecken sicherstellen muss. Hier lohnt sich der Aufwand dann.

In Farbmonitoren bzw. Farbfernsehgeräten befindet sich als wichtigstes Bauteil die Kathodenstrahlröhre. Durch Glühemission aus geheizten Glühkathoden mit anschließender elektrostatischer Fokussierung werden drei Elektronenstrahlen erzeugt, die auf der Leuchtschicht durch Fluoreszenz einen mehr oder minder hellen Leuchtfleck erzeugen.

Auf dem Weg vom Strahlerzeugungssystem zur Leuchtschicht werden diese Elektronenstrahlen gemeinsam durch Magnetfelder abgelenkt, so dass ein Raster entsteht.

Die Helligkeit eines Pixels abhängig von seiner Position auf dem Leuchtschirm ergibt den Bildinhalt.

Das eingangs erwähnte Fernsehsignal wird im Bildschirm zur Steuerung dieser Helligkeitsinformationen in Abhängigkeit zur Position des Elektronenstrahles benutzt.

Die jeweiligen Frequenzen, mit der die beiden Magnetfelder die Ablenkung des Strahles in waagerechter (horizontaler) und senkrechter (vertikaler) Richtung durchführen (=Zeilenfrequenz und Bildwiederholfrequenz), sowie der Pixeltakt (auch Videobandbreite genannt und bei PC-Monitoren als RAMDAC-Frequenz) bestimmen die Eigenschaften des Rasters: Anzahl der Zeilen bzw. Pixel, Seitenverhältnis der Pixel und wie oft pro Zeit ein Pixel von neuem zum Leuchten angeregt wird.

Die europäische Fernsehnorm sieht eine Horizontalfrequenz von 15 625 Hz vor sowie eine vertikale Frequenz von 50 Hz. Der Bildaufbau erfolgt im Zeilensprungverfahren. Die 50 Hz sind bekannt für das sogenannte "Flimmern" bei Röhrenfernsehern.

Die meisten Computerbildschirme (fast alle ab ca. 1990 gebauten) können diese beiden Frequenzen in gewissen Grenzen dem Eingangssignal anpassen. Diese liegen in horizontaler Richtung zwischen ca. 30 und 130 kHz, vertikal zwischen 60 und 200 Hz.

In der Computertechnik ist man bestrebt, die Vertikalfrequenz auf mehr als ca. 80 Hz einzustellen. Nur so kann eine augenschonende, flimmerfreie Darstellung gewährleistet werden. Die Grenze der Flimmerfreiheit hängt von mehreren Faktoren ab:

Die Steigerung der Zeilenfrequenz steigert mithin auch die Rate, mit der die Helligkeitsinformationen übertragen und verarbeitet werden müssen (Pixeltakt, s. o.). Im Computerbereich zeigt sich diese Wirkung z. B. recht deutlich, wenn zur Signalübertragung minderwertige Kabel verwendet werden. So wirkt ein entsprechendes Bild mit deutlichen Kontrasten immer unschärfer, je höher die Wiedergabefrequenzen bei gleichbleibender Auflösung eingestellt werden.

Man unterscheidet zwei Techniken des Bildaufbaus:



Computerbildschirme mit Kathodenstrahlröhre können bauartbedingt unterschiedliche Bildschirmauflösungen ohne nennenswerte Skalierungsverluste darstellen, wie sie z. B. von LC-Bildschirmen bekannt sind. Bei geringen Bildschirmauflösungen streift der Elektronenstrahl mehrere Bildschirmpixel gleichzeitig und erledigt so die Skalierung. Kathodenstrahlbildschirme eignen sich daher besonders für barrierefreie Computerarbeitsplätze, bei denen aufgrund der besseren Lesbarkeit eine geringe Bildschirmauflösung mit großen Schriften gefordert ist. Bei besonders kleinen Auflösungen macht sich allerdings der Zwischenraum zwischen den geschriebenen Zeilen als waagerechtes Muster aus schwarzen Linien bemerkbar, da die Schärfe des Strahles auch bei geringerer Auflösung konstant bleibt.

Die ersten Geräte waren mit Schwarz-Weiß-Bildröhren im 4:3-Format ausgestattet, deren Größe bis Mitte der 1970er Jahre auf 63 cm gesteigert werden konnte. Größere Bildschirme bedingen einen stabileren Aufbau der Röhre durch dickere Glaskonstruktionen, was sich im Gewicht niederschlägt.

Prinzipbedingt weist eine Kathodenstrahlröhre eine gewisse Einbautiefe auf. Diese wurde mit steigendem Ablenkwinkel immer wieder verringert, wird aber nie die geringe Einbautiefe von modernen Flachbildschirmtechnologien erreichen können.

In den späten 1960er Jahren war die Entwicklung des Farbfernsehens soweit abgeschlossen, dass kommerzielle Geräte erschwinglich wurden. Anfang der 1990er versuchte man, das Bildformat einzuführen, was jedoch scheiterte. Ab 2000 wurde es wieder versucht, mit dem Erfolg, dass ungefähr 20 % aller Bildröhrenfernseher im 16:9-Format verkauft wurden. Einige Bildröhren waren für das besonders in Japan und Nordamerika seit den 1990er Jahren beliebte HDTV ausgelegt.

Computermonitore gab es ab den 1960er Jahren mit den monochromen Leuchtfarben Weiß, Grün und Bernstein. Besonders Bernsteinmonitore wiesen durch eine hohe Nachleuchtdauer eine sehr ruhige Bilddarstellung auf.



</doc>
<doc id="14623" url="https://de.wikipedia.org/wiki?curid=14623" title="Die Weltbühne">
Die Weltbühne

Die Weltbühne war eine deutsche Wochenzeitschrift für Politik, Kunst und Wirtschaft. Sie wurde von Siegfried Jacobsohn in Berlin unter dem Namen ‚Die Schaubühne‘ als reine Theaterzeitschrift gegründet und erschien am 7. September 1905 zum ersten Mal. Am 4. April 1918 wurde die "Schaubühne", die sich seit 1913 für wirtschaftliche und politische Themen geöffnet hatte, in "Die Weltbühne" umbenannt. Nach dem Tode Jacobsohns im Dezember 1926 übernahm Kurt Tucholsky die Leitung des Blattes, die er im Mai 1927 an Carl von Ossietzky weitergab. Die Nationalsozialisten verboten nach dem Reichstagsbrand die "Weltbühne", die am 7. März 1933 zum letzten Mal erscheinen konnte. Im Exil wurde die Zeitschrift bis 1939 unter dem Titel "Die neue Weltbühne" fortgeführt. Nach dem Ende des Zweiten Weltkrieges erschien die "Weltbühne" unter ihrem ursprünglichen Namen wieder in Ost-Berlin, wo sie bis 1993 Bestand hatte. 1997 haben sich die Zeitschriften "Ossietzky" und "Das Blättchen" in die Tradition des berühmten Vorbilds gestellt.

Mit ihren kleinen roten Heften galt die "Weltbühne" in der Weimarer Republik als "das" Forum der radikaldemokratischen bürgerlichen Linken. Rund 2500 Autoren schrieben von 1905 bis 1933 für die Zeitschrift. Dazu gehörten neben Jacobsohn, Tucholsky und Ossietzky auch prominente Journalisten und Schriftsteller wie Lion Feuchtwanger, Moritz Heimann, Kurt Hiller, Erich Mühsam, Else Lasker-Schüler, Erich Kästner, Alfred Polgar, Robert Walser, Carl Zuckmayer und Arnold Zweig. Auch ein wenig in Vergessenheit geratene Publizisten wie Rudolf Arnheim, Julius Bab, Erich Dombrowski, Axel Eggebrecht, Hellmut von Gerlach, Richard Lewinsohn, Fritz Sternberg und Heinrich Ströbel gehörten zu den wichtigen Mitarbeitern des Blattes. Ferner die erste weibliche Journalistin der Volkswacht (Freiburg im Breisgau), Käthe Vordtriede.

Selbst in ihrer Hochphase hatte die "Weltbühne" nur eine geringe Auflage von rund 15.000 Exemplaren. Publizistisch drang sie dennoch durch. Beispiele dafür sind die Aufdeckung der Fememorde innerhalb der Schwarzen Reichswehr sowie Berichte über die heimliche Aufrüstung der Reichswehr, die später zum sogenannten "Weltbühne-Prozess" führten. Auch der von Tucholsky geprägte Satz „Soldaten sind Mörder“ führte zu einer Anklage gegen den damaligen Herausgeber Ossietzky.

Die Gründung der "Schaubühne" war das Resultat einer Plagiatsaffäre, in die der 23 Jahre alte Theaterkritiker Siegfried Jacobsohn verwickelt war. Am 12. November 1904 hatte das "Berliner Tageblatt" auf Parallelen zwischen Kritiken von Jacobsohn und Alfred Gold aufmerksam gemacht. Jacobsohn war zu diesem Zeitpunkt Theaterkritiker der "Welt am Montag", die ihren streitbaren, und in Presse- und Theaterkreisen daher zum Teil verhassten Mitarbeiter aufgrund der öffentlichen Empörung nicht mehr halten wollte. Der beruflich fürs Erste gescheiterte Jacobsohn trat eine mehrmonatige Reise durch Europa an und beschloss, eine eigene Theaterzeitschrift ins Leben zu rufen. Diese Lebensphase, von Beginn der Plagiatsaffäre bis zur Gründung der "Schaubühne", beschrieb er in der 1913 erschienenen Schrift "Der Fall Jacobsohn". Im Rückblick schilderte er seine Affäre als „Sensationsstück ersten Ranges, für das es sich lohnte, die berliner Litfaßsäulen mit Riesenplakaten – Jacobsohns Entlarvung; Plagiator Jacobsohn; Siegfrieds Tod – wochenlang vollzukleben“ (S. 50). Neueren Untersuchungen zufolge fand der Fall in der Hauptstadtpresse aber nur ein geringes Echo. Jacobsohns Broschüre enthält auch eine Briefpassage, die seine Vorstellungen von der zukünftigen Arbeit als Herausgeber und Redakteur wiedergibt (S. 47):

Die Zeitschrift hat während ihres Bestehens von 1905 bis 1933 mehrere Entwicklungsphasen durchlaufen. Bis 1913 konzentrierte sie sich auf „die gesamten Interessen des Theaters“, wie es bis dahin in ihrem Untertitel hieß. Jacobsohn war überzeugt, dass „der Geist eines Volkes und einer bestimmten Zeit eindringlicher als in der übrigen Literatur im Drama zum Ausdruck kommt“ – so heißt es in seinem Beitrag "Zum Geleit", mit dem er das erste Heft der "Schaubühne" eröffnete.

Den ersten vier Nummern war ein Zitat aus Friedrich Schillers Aufsatz "Die Schaubühne als moralische Anstalt betrachtet" als Motto vorangestellt: „So gewiß sichtbare Darstellung mächtiger wirkt als toter Buchstabe und kalte Erzählung, so gewiß wirkt die Schaubühne tiefer und dauernder als Moral und Gesetze“. Das war ein Hinweis darauf, wie Jacobsohn sein Unternehmen verstanden wissen wollte: als Aufklärung im Geist der Klassik. Die große Bedeutung, die künstlerischen Debatten in der damaligen Zeit zukam, lag allerdings auch darin begründet, dass die Kunst im Deutschen Reich unter Kaiser Wilhelm II. weniger Repressionen ausgesetzt war als Politik und Journalismus.

Zu den wichtigsten Mitarbeitern in der Anfangsphase der "Schaubühne" zählten die Theaterkritiker Julius Bab, Willi Handl und Alfred Polgar, in den Folgejahren traten auch Schriftsteller wie Lion Feuchtwanger, Robert Walser, und Harry Kahn sowie der Theaterkritiker Herbert Ihering hinzu. Im November 1908 wurde Feuchtwangers Zeitschrift "Der Spiegel" nach nur 15 Ausgaben mit der "Schaubühne" vereinigt.

Als Theaterkritiker war Jacobsohn ein Antipode Alfred Kerrs. Anders als dieser war er ein entschiedener Kritiker des Naturalismus und schätzte im Gegensatz zu Kerr auch die Leistungen von Max Reinhardt als Theaterleiter und -regisseur weit höher ein als die von Otto Brahm. Reinhardts 1910 beginnende Hinwendung zum Massentheater in Zirkusarenen, die in Berlin schließlich im Bau des Großen Schauspielhauses mündete, wurde von Jacobsohn jedoch missbilligt.

Am 9. Januar 1913 erschien erstmals ein Beitrag des an diesem Tage 23 Jahre alt gewordenen Jura-Studenten Kurt Tucholsky in der "Schaubühne". Schon im ersten Jahr seiner Zusammenarbeit mit Jacobsohn avancierte Tucholsky zu dessen wichtigstem Mitarbeiter.

Um das Blatt nicht allzu „Tucholsky-lastig“ erscheinen zu lassen, legte er sich bereits 1913 drei Pseudonyme zu, die er bis zum Ende seines publizistischen Wirkens beibehielt: Ignaz Wrobel, Theobald Tiger und Peter Panter. Unter dem Einfluss von Tucholskys Mitarbeit sollte sich auch der Charakter der "Schaubühne" rasch wandeln. Schon im März 1913 erschienen die ersten „Antworten“, eine Rubrik, in der die Zeitschrift in Zukunft auf echte oder fingierte Leserbriefe Stellung nehmen sollte. Wichtiger war jedoch die Entscheidung Jacobsohns, sein Blatt für Themen aus Politik und Wirtschaft zu öffnen. Am 25. September berichtete der Wirtschaftsjurist Martin Friedlaender unter dem Pseudonym „Vindex“ über Monopolstrukturen in der amerikanischen Tabakindustrie. Jacobsohn nahm in einer fingierten „Antwort“ dazu Stellung:

Während des Krieges gelang es Jacobsohn, dass seine Zeitschrift trotz schwieriger Bedingungen regelmäßig erscheinen konnte. Von August 1914 an eröffnete er jedes Heft mit einem politischen Leitartikel, in dem ein „patriotischer“ Standpunkt vertreten wurde. Im November 1915 startete der Journalist Robert Breuer unter dem Pseudonym „Cunctator“ eine Serie von Artikeln, die sich kritisch mit der Politik der Reichsregierung und dem politischen Zustand des Reiches auseinandersetzten. Die Reihe gipfelte am 23. Dezember in dem Beitrag "Die Krise des Kapitalismus", der mit der Feststellung endete: „Nur die Internationale des Proletariats kann die Krise des national verbrämten Kapitalismus überwinden.“

Aufgrund dieses Artikels wurde die "Schaubühne" zunächst verboten. Jacobsohn konnte jedoch ein weiteres Erscheinen des Blattes sicherstellen, indem er in eine Vorzensur einwilligte. Zum "Germanicus" gewandelt kehrte Breuer im Januar 1916 als Kommentator zurück in das Blatt und führte dort trotz seines Namens einen permanenten Kampf gegen die Annexionsforderungen des Alldeutschen Verbandes. Von 1916 druckte Jacobsohn, der 1915 nach dem Tod seines jüngsten Bruders an der Front ein leidenschaftliches pazifistisches Bekenntnis abgegeben hatte, regelmäßig Annoncen zur Zeichnung von Kriegsanleihen. Ungeklärt ist bislang, ob diese Anzeigen vergütet wurden und damit möglicherweise entscheidend zur Existenzsicherung der Zeitschrift beitrugen. Das insgesamt keineswegs pazifistische, politisch bestenfalls als lavierend zu bezeichnende Erscheinungsbild des Blattes trug Jacobsohn später nicht unberechtigte Kritik u. a. von Franz Pfemfert und Karl Kraus ein.

Dem Wandel vom reinen Theaterblatt zur „Zeitschrift für Politik, Kunst, Wirtschaft“ trug Jacobsohn schließlich am 4. April 1918 mit der Umbenennung der "Schaubühne" in "Weltbühne" Rechnung.

Nach den Anfangserfolgen der deutschen Frühjahrsoffensive 1918 rückte Jacobsohns Leitartikler Robert Breuer von seiner bis dahin anti-annexionistischen Position ab und verließ auch auf anderen Gebieten die bisherige Linie des Blattes. Die Differenzen zwischen dem MSPD-Anhänger Breuer und Jacobsohn, der sich mehr und mehr der Position der USPD näherte, führte schließlich zum Abschied von „Germanicus“. Während der Novemberrevolution ließ sich die "Weltbühne" nicht auf einen Parteikurs festlegen. Von März 1919 bis Oktober 1920 schrieb der Sozialdemokrat Heinrich Ströbel die politischen Leitartikel.

Am 21. November 1918 veröffentlichte Jacobsohn das Programm des „Rates geistiger Arbeiter“, dem er selbst kurzzeitig angehörte, den er aber verließ, weil er sich nicht für einen „Debattierklub“ die Zeit für die Redaktionsarbeit stehlen lassen wollte. Schon bald beschäftigte sich die "Weltbühne" kritisch mit der Zusammenarbeit von Sozialdemokratie und dem alten Heer sowie der unzureichenden Säuberung von Justiz und Verwaltung von monarchistisch und antirepublikanisch eingestellten Beamten.

Im März 1919 wehrte sich Tucholsky in dem programmatischen Text „Wir Negativen“ gegen den Vorwurf, die neue Republik nicht positiv genug zu sehen:

In den folgenden Jahren vertrat die "Weltbühne" einen strikt pazifistischen und antimilitaristischen Kurs, forderte eine harte Reaktion der Republik auf die zahlreichen politischen Morde und drängte auch während des Ruhrkampfes auf die Erfüllung der im Versailler Vertrag festgelegten Friedensbedingungen.

Daher trat das Blatt auch entschieden für die Aussöhnung mit den Kriegsgegnern ein. Ein besonderes Verdienst der "Weltbühne" bestand darin, auf die Fememorde innerhalb der Schwarzen Reichswehr aufmerksam gemacht zu haben. Obwohl Jacobsohn wusste, dass er sich damit einer großen persönlichen Gefahr aussetzte, veröffentlichte er vom 18. August 1925 an entsprechende Aufzeichnungen des ehemaligen Freikorpsangehörigen Carl Mertens.

Wegweisend für die weitere Entwicklung der Zeitschrift war auch die Verpflichtung des politischen Publizisten Carl von Ossietzky, der vom April 1926 an als Redakteur und politischer Leitartikler von Jacobsohn beschäftigt wurde. Mit dem plötzlichen Tod Jacobsohns am 3. Dezember 1926 war der Fortbestand der "Weltbühne", die damals eine Auflage von rund 12.500 Exemplaren hatte, jedoch in Frage gestellt.

Nach dem Tod seines Mentors Jacobsohn gab Tucholsky zunächst sein Korrespondentendasein in Paris auf, kehrte zurück nach Berlin und wurde – wie er es spöttisch nannte – „Oberschriftleitungsherausgeber“ der "Weltbühne". Jacobsohns Witwe Edith Jacobsohn übernahm 1927 die Leitung des Verlags. Es zeigte sich jedoch schon bald, dass Tucholsky die Position des Herausgebers nicht behagte. Daher übernahm Ossietzky im Mai 1927 die Redaktion und wurde von Oktober 1927 offiziell als Herausgeber genannt, „unter der Mitarbeit von Kurt Tucholsky“, wie es bis 1933 auf dem Titelblatt hieß. Obwohl von Ossietzky vom Typus her ein völlig anderer Redakteur als Jacobsohn war, blieb die Kontinuität der Zeitschrift gewahrt. Aus den Briefen Tucholskys an seine Frau Mary Gerold geht jedoch hervor, dass dieser in den Jahren 1927 und 1928 alles andere als zufrieden über die Arbeitsweise seines Nachfolgers „Oss“ war. Typische Briefpassagen lauteten: „Oss antwortet überhaupt nicht – geht auf nichts ein – und zwar sicherlich nicht aus Gemeinheit, sondern aus Faulheit“ (14. August 1927); „Oss ganz weit weg. Ich habe den lebhaften Eindruck, zu stören. Er mag mich nicht u. ich ihn nicht mehr. Behandelt mich um die entscheidende Nuance zu wenig respektvoll. Kriegt auf den Kopf“ (20. Januar 1928); „Oss ist ein aussichtsloser Fall – er weiß nicht einmal, wie langweilig er alles macht. Er ist faul und unfähig.“ (25. September 1929) Erst in den kommenden Jahren sollten sich die beiden Journalisten inhaltlich und persönlich näherkommen, sodass Tucholsky im Mai 1932 schließlich einräumte, Ossietzky habe dem Blatt einen „gewaltigen Auftrieb“ gegeben.

Dieser Auftrieb schlug sich auch in der Auflage nieder, die Anfang der 1930er-Jahre mit 15.000 Exemplaren ihr Maximum erreichte. Von der Bedeutung der "Weltbühne" zeugen u. a. die Leserzirkel, die sich in zahlreichen deutschen Städten und selbst in Südamerika bildeten. Für Aufmerksamkeit auch über den Kreis der Leser hinaus sorgten die juristischen Auseinandersetzungen, die die "Weltbühne" aufgrund ihrer antimilitaristischen Aufklärungsarbeit fast permanent mit dem Reichswehrministerium führte. Höhepunkt dieser Konflikte war der sogenannte "Weltbühne-Prozess", in dessen Folge von Ossietzky und der Journalist Walter Kreiser wegen Spionage zu 18 Monaten Haft verurteilt wurden.

Dem Kampf gegen die „Reise ins Dritte Reich“ (Tucholsky) galt gegen Ende der Weimarer Republik die volle Konzentration des Blattes, obgleich das kulturelle Leben nicht völlig ausgeblendet wurde. Allerdings hatte Tucholsky Anfang 1932 bereits resigniert und veröffentlichte nur noch sporadisch eigene Texte. Im Mai 1932 übernahm Hellmut von Gerlach vorübergehend die Leitung, da Ossietzky seine Haftstrafe absitzen musste. Während dieser Zeit fungierte der Journalist Walther Karsch als so genannter Sitzredakteur, war also verantwortlicher Redakteur im Sinne des Presserechts. Im Sommer wurde Ossietzky ebenfalls wegen des Tucholsky-Satzes „Soldaten sind Mörder“ angeklagt. Ein Gericht sprach den bereits Inhaftierten jedoch frei, der Weihnachten 1932 aufgrund einer Amnestie schließlich aus der Haft entlassen wurde.

Mit der Machtübernahme der Nationalsozialisten am 30. Januar 1933 war vorauszusehen, dass ein Verbot der "Weltbühne" erfolgen würde. In der Nacht des Reichstagsbrands vom 27. auf den 28. Februar 1933 wurden Ossietzky und weitere Mitarbeiter verhaftet. Nach der Flucht Hellmut von Gerlachs übernahm Walther Karsch, der spätere Mitbegründer des Berliner "Tagesspiegels", auch die Funktion des Chefredakteurs der "Weltbühne". Die für den 14. März geplante Ausgabe konnte zwar noch gedruckt, aber nicht mehr ausgeliefert werden. Die letzte Ausgabe der "Weltbühne" erschien somit am 7. März 1933 (Nr. 10) und endete mit der trotzigen Versicherung: „Denn der Geist setzt sich doch durch“.

Das Verbot der Zeitschrift traf den Verlag der Weltbühne nicht unvorbereitet. Schon am 29. September 1932 war in Wien ein Ableger des Blattes erschienen, die "Wiener Weltbühne". Für die Nummern 11–13 1933 (2. Jahrgang) schrieben bereits verschiedene Berliner Emigranten. Als Leiter der Wiener Dependance fungierte der Journalist Willi Siegmund Schlamm, ein Schüler von Karl Kraus und Leo Trotzki. Im Redaktionsvertrag zwischen Schlamm und Edith Jacobsohn war vorgesehen, dass Carl von Ossietzky im Falle einer Emigration auch die Redaktion des Exilblattes übernehmen würde. Doch dazu kam es nicht.

Edith Jacobsohn gelang gemeinsam mit ihrem Sohn Peter die Flucht in die Schweiz. Von dort aus versuchte sie, weiterhin Einfluss auf die Zeitschrift zu nehmen, die nach der Entmachtung des österreichischen Parlaments durch Kanzler Engelbert Dollfuß ihren Redaktionssitz nach Prag hatte verlegen müssen. Da das Berliner Original inzwischen auch verboten worden war, änderte die Zeitschrift ihren Namen in "Die Neue Weltbühne" um. Zwischen 6. April 1933 (Nr. 14) und 31. August 1939 (Nr. 35) erschienen knapp 4000 Artikel.

Schon bald gab es Differenzen zwischen Schlamm und Jacobsohn, denn die Auflage des Blattes war nicht hoch genug, um die finanziellen Ansprüche Edith Jacobsohns zu decken. Dafür wurde auch Schlamms politische Linie verantwortlich gemacht, da dieser die Politik der Sozialdemokratie und der Sowjetunion kritisierte. Unter dem Einfluss des Wirtschaftsjournalisten Hermann Budzislawski, der in Berlin sporadischer Mitarbeiter der "Weltbühne" gewesen war, ließ Jacobsohn es auf den Bruch mit Schlamm ankommen. Von März 1934 an übernahm Budzislawski die Redaktion in Prag. Zwar änderte er sogleich die politische Linie der Zeitschrift, doch die Auflage konnte er nicht wesentlich erhöhen. Dies lag auch daran, dass mit Österreich und bald auch dem Saargebiet wichtige Absatzgebiete der Exilzeitschriften verloren gingen. Daher sah Edith Jacobsohn sich im Juni 1934 gezwungen, Verlag und Titelrechte zu verkaufen.

Als Käufer traten der Physiker Albrecht Seidler-Stein (60 Prozent Anteile), der Rechtsanwalt Hans Nathan-Ludwig (31 Prozent) und der frühere "Weltbühne"-Mitarbeiter Heinz Pol (neun Prozent) auf. Im Juli 1935 verkaufte Nathan-Ludwig seine Anteile jedoch an die mit Budzislawski befreundete Helene Reichenbach, Tochter eines chinesischen Diplomaten und Geschäftsmannes. Pol gab seinen Anteil im November 1935 ebenfalls wieder ab, sodass Seidler-Stein schließlich zwei Drittel der Anteile, Reichenbach ein Drittel besaß. Da Seidler-Stein versuchte, Budzislawski durch einen anderen Redakteur zu ersetzen, wurde er von Budzislawski schließlich aus dem Verlag gedrängt. Obwohl Budzislawski über keine finanziellen Rücklagen verfügte, stimmte die in Moskau lebende Reichenbach im August 1936 einem Vertrag zu, der beiden zu gleichen Teilen das Eigentum am Verlag zusicherte. Unter diesen Bedingungen konnte die Zeitschrift noch rund drei Jahre existieren. Im Juni 1938 wechselte die Redaktion von Prag nach Paris, da "Die neue Weltbühne" in der Tschechoslowakei bereits mehrfach wegen Deutschland-kritischer Artikel konfisziert worden war. In Frankreich verboten die Behörden schließlich das Blatt ebenfalls, das am 31. August 1939 zum letzten Mal erscheinen konnte.

Budzislawski ist in der Vergangenheit häufig vorgeworfen worden, die "Weltbühne" lediglich als kommunistischer Agent übernommen zu haben, um sie im Sinne der KPD und der Kommunistischen Internationale weiterführen zu können. Neuere Forschungen unter Auswertung des Redaktionsarchivs gehen eher davon aus, dass Budzislawski aus Gründen der persönlichen Reputation und als entschiedener Hitler-Gegner die Leitung der "Neuen Weltbühne" übernehmen wollte. Dennoch bleibt festzuhalten, dass unter seiner Herausgeberschaft nach Moskau emigrierte deutsche Kommunisten wie Walter Ulbricht und Franz Dahlem ein Forum in dem Blatt fanden. Außerdem vermied es Budzislawski, über die so genannten Stalinschen Säuberungen zu berichten. Kurt Hiller, seit 1915 Mitarbeiter der "Weltbühne", appellierte 1937 aber vergeblich an Budzislawski, die charakteristische Ausgewogenheit und Freizügigkeit der Zeitschrift wiederherzustellen (vgl. seine kritische Schrift "Rote Ritter. Erlebnisse mit deutschen Kommunisten", Gelsenkirchen 1951).

1946 wurde die "Weltbühne" von Maud von Ossietzky und Hans Leonhard wieder gegründet und im "Verlag der Weltbühne", Ost-Berlin, herausgegeben. Von den USA aus erhoben sowohl Peter Jacobsohn als auch Budzislawski Einspruch gegen die Neugründung.

In den Jahren nach dem Kriege fand die Zeitschrift auch in den westlichen Besatzungszonen viele Abnehmer. In den 1950er und 1960er wurde die "Weltbühne" daher als Brücke zu den intellektuellen Kreisen im Westen gesehen sowie als Möglichkeit betrachtet, diese Kreise zu beeinflussen. In einem Antrag auf die Neuausstellung einer Lizenzurkunde im Jahre 1962 hieß es daher:

Im Zweifel entschied sich die Redaktion dabei für die aktuellen politischen Erfordernisse und gegen die Tradition der Zeitschrift, wie aus einer internen Charakteristik von Mitte der 1950er-Jahre hervorgeht:

„Kam die "Weltbühne" immer etwas intellektueller daher als andere DDR-Zeitschriften, so war sie doch im Grunde linientreu“, lautet das Resümee von Petra Kabus. Allerdings erreichte die Auflage mit 170.000 Exemplaren eine Größenordnung, die diejenige der Original-"Weltbühne" um mehr als das Zehnfache überstieg.

Von 1967 bis 1971 fungierte Budzislawski wieder als Herausgeber und Chefredakteur der "Weltbühne". Von Dezember 1989 bis zur Einstellung des Blattes im Juli 1993 übernahm Helmut Reinhardt diese beiden Aufgaben. Die Zeitschrift musste auch deswegen eingestellt werden, weil Peter Jacobsohn nach der Wiedervereinigung die Rechte an dem Zeitschriftentitel geltend machte. Einen ersten Prozess vor dem Landgericht Frankfurt am Main verlor Jacobsohn jedoch. Der zwischenzeitliche Eigentümer des Verlages, Bernd F. Lunkewitz, versuchte sich im anschließenden Berufungsverfahren vor dem Oberlandesgericht Frankfurt am Main außergerichtlich mit Jacobsohn zu einigen. Da diese Einigung misslang, stellte er die hochdefizitär gewordene Zeitschrift am 6. Juli 1993 ein. Seine Begründung:

Der Verlag der Weltbühne hatte als Vorleistung für den Vergleich die Ansprüche Jacobsohns voll anerkannt, was nicht mehr rückgängig gemacht wurde. Herausgeber Helmut Reinhardt war bis zuletzt davon ausgegangen, dass der Prozess vor dem Oberlandesgericht gewonnen werden würde. Die Redaktion des Blattes zeigte sich von dem eigenmächtigen Vorgehen Lunkewitz' daher völlig überrascht und fügte dessen Erklärung eine eigene Stellungnahme hinzu:

Durch die Anerkenntnis des Klagebegehrens wurde juristisch nie geklärt, ob die Titelrechte tatsächlich den Jacobsohn-Erben zugestanden hätten. Zwar sicherte sich Jacobsohn zwischenzeitlich die Titelrechte, jedoch wurden diese anschließend nie genutzt. Dies ist mit einer Sicherung von Markenrechten nicht dauerhaft vereinbar (siehe: Schutzdauer im Markenrecht)

Lunkewitz verkaufte im August 1993 schließlich den Verlag samt Abonnentenkartei an Peter Großhaus, der damals auch die frühere FDJ-Zeitung "Junge Welt" verlegte. Im Dezember 1993 wechselte der Verlag ein weiteres Mal den Besitzer und wurde in Webe Verlag und Beteiligungsgesellschaft umbenannt. Drei Jahre später, im November 1996, kaufte "Titanic"-Verleger Erik Weihönig den Verlag. Am 29. November 2001 wurde die Webe schließlich aus dem Handelsregister gelöscht.

1997 wurden sowohl in Berlin als auch in Hannover Wiederbelebungsversuche unternommen. Beide Autorengruppen scheuten eine juristische Auseinandersetzung um das Recht an dem Namen "Weltbühne". Nicht nur Peter Jacobsohn, sondern auch die neuen Besitzer des früheren Weltbühne-Verlages wollten die Verwendung des Namens unterbinden. Das Projekt aus Hannover wurde daher "Ossietzky" genannt und erscheint im gleichnamigen Verlag. Die Redaktion zog aber im Jahre 2000 von Hannover nach Berlin um. Herausgeber ist Eckart Spoo, früher Korrespondent bei der "Frankfurter Rundschau". Das Ost-Berliner Zwillingsblatt legte sich den redaktionsinternen Spitznamen der Original-Weltbühne "Das Blättchen" zu und wurde bis September 2009 als gedruckte Ausgabe von einem Zirkel um Jörn Schütrumpf herausgegeben. Seit 2010 erscheint "Das Blättchen" als reine Online-Zeitschrift.

Dass die "Weltbühne" trotz ihrer geringen Auflage eine so große Wirkung entfalten konnte, lässt sich wohl nur mit der Person Siegfried Jacobsohns begründen. Über einen Zeitraum von zwei Jahrzehnten war es ihm gelungen, wichtige Vertreter der intellektuellen Linken an sein Blatt zu binden und eine gleich bleibend hohe Qualität der Texte zu gewährleisten. „Der Mann war der idealste Redakteur, den unsre Generation gesehen hat“, schrieb Tucholsky nach dem überraschenden Tod Jacobsohns im Dezember 1926. Im Unterschied zu Karl Kraus' "Fackel" und Maximilian Hardens "Zukunft" dominierten in der "Weltbühne" jedoch von Anfang an nicht die Texte eines egomanischen Herausgebers. Jacobsohn sah sich stets als der „Regisseur einer gedruckten Bühne“, wie er im Mai 1905 in einem Brief geschrieben hatte.

Die geringe Auflage steht nicht im Widerspruch zur, sondern kann eher als Begründung für die besondere Stellung der "Weltbühne" herhalten. Denn im Gegensatz zu größeren Blättern musste Jacobsohn weder auf Verlags-, Partei- noch Anzeigeninteressen Rücksicht nehmen. Auch um die Ansprüche seiner Leser scherte sich Jacobsohn wenig. „Sie haben nur ein Recht: mein Blatt nicht zu lesen“, zitierte Tucholsky mehrfach das Credo seines Mentors. Charakteristisch dafür war eine "Antwort", die Jacobsohn einem Leser gegen Ende des Ersten Weltkrieges gab:

Diese Unabhängigkeit war auch ein Grund dafür, dass ein Autor wie Tucholsky trotz des nicht gerade üppigen Honorars immer wieder zur "Weltbühne" zurückkehrte und dort Texte veröffentlichte, die er in bürgerlichen Blättern wie der "Vossischen Zeitung" oder dem "Berliner Tageblatt" nicht unterbringen konnte. Ein Resultat der Radikalität waren Vorwürfe, die sich das Blatt schon Anfang 1919 gefallen lassen musste und die Tucholsky damals wie folgt zusammenfasste:

Der Hintergrund dieser Kritik lag wohl darin, dass sich die "Weltbühne" in der Weimarer Republik von Beginn an nicht auf eine bestimmte parteipolitische Position festlegen ließ und bei keiner Partei ihre Vorstellungen von einem demokratischen und sozialen Deutschland verwirklicht sah. Vor allem die SPD musste sich bis zum Ende der Weimarer Republik vorhalten lassen, die Ideale der Novemberrevolution verraten und nicht energisch genug mit den Traditionen des Kaiserreiches gebrochen zu haben.

Die Radikalität und Offenheit der "Weltbühne"-Positionen waren jedoch gleichzeitig ein Grund dafür, dass sie innerhalb von Journalismus und Politik sehr aufmerksam wahrgenommen wurden. Diese Leserschicht des Blattes erfüllte somit eine Multiplikatorenfunktion und sorgte dafür, dass die "Weltbühne"-Positionen in anderen Blättern Verbreitung fanden, wenn auch häufig verkürzt und verfälscht. „Die 'Weltbühne' hat immer zwei gewichtige Gegenpole gehabt: die Parteien und die große Presse“, heißt es bei Tucholsky in „Fünfundzwanzig Jahre“.

Charakteristisch für Rezeption und Wirkung der "Weltbühne" sowie Ton und Inhalt der damaligen Debatten ist folgende "Antwort", die die Kritik eines sozialdemokratischen Blattes an der "Weltbühne" wiedergibt:

Trotz dieser Dauerkritik an der SPD war der "Weltbühne" stets klar, dass die wahren Feinde der Republik auf der anderen Seite des politischen Spektrums zu suchen waren. In einem Gedicht Tucholskys hieß es Ende 1919:

Das Blatt scheute daher nicht davor zurück, aus Protest gegen die judenfeindliche Politik der Kahr-Regierung die Leser dazu aufzufordern, ihren Urlaub nicht mehr in Bayern zu verbringen. Die Kampagne „Reisende, meidet Bayern!“ schlug hohe Wellen, wie die folgende, von extremem Antisemitismus geprägte Reaktion zeigt:

Die "Weltbühne" wurde von Vertretern der radikalen politischen Rechten aber nicht nur aufmerksam verfolgt und angegriffen, sondern wegen ihrer Konzeption und ihres sprachlichen Niveaus auch bewundert. So schrieb der Nationalist Franz Schauwecker im Januar 1926 an Ernst Jünger:

Tatsächlich scheint die "Weltbühne" für einige nationalistische Blätter ein Vorbild abgegeben zu haben.

Bemerkenswert ist auch eine Stellungnahme des jungkonservativen Publizisten Heinrich von Gleichen-Rußwurm, der seine Kritik an der Haltung der "Weltbühne" mit einer scharfen Missbilligung antisemitischer Pöbeleien verband:

Die weiter oben zitierte Beurteilung durch den Reichstagsabgeordneten Anton Erkelenz findet sich in ähnlicher Form auch Texten wieder, die sich aus historischer Perspektive mit der "Weltbühne" befassen. So kritisierte Rudolf Augstein die überzogenen Ansprüche des Blattes an die Politiker:

Allerdings lässt sich der "Weltbühne" nicht vorwerfen, sie habe von einer rein idealistischen und ästhetischen Warte aus agiert, ohne sich um die Aufdeckung konkreter Missstände zu kümmern. So ging Jacobsohn ein hohes persönliches Risiko ein, als er 1925 die Berichte über Fememorde innerhalb der Vaterländischen Verbände veröffentlichte. Nach Angaben Ossietzkys soll Jacobsohn darin auch seine wichtigste journalistische Leistung gesehen haben: "„Und wenn ich nichts getan hätte als die Aufdeckung der Fememorde, so wäre mir das genug …“" Auch die Reaktion der Reichsregierung auf die Enthüllungen, die zum "Weltbühne"-Prozess führten, zeigten sehr deutlich, dass bereits 1929 nur noch wenig von dem Staat übrig war, den die "Weltbühne" hätte verteidigen wollen.

Und wie eine vorweggenommene Antwort auf die Kritiker der Nachkriegszeit liest sich eine Stelle aus einem Brief Tucholskys an Walter Hasenclever vom 17. Mai 1933:

Die "Schaubühne" erschien zunächst in der Schaubühne GmbH, die am 1. August 1905 eigens zu diesem Zweck ins Leben gerufen worden war. Im Januar 1906 übernahm der neu gegründete Verlag Oesterheld & Co. die Zeitschrift. Vom 1. Januar 1909 bis zum 1. Oktober 1912 kam die "Schaubühne" im Verlag Erich Reiß heraus. Danach erschien die Zeitschrift bis zu ihrem Verbot 1933 in Jacobsohns Verlag der Schaubühne (1918 in Verlag der Weltbühne umgewandelt). Die finanzielle Situation der Zeitschrift war bis Mitte der zwanziger Jahre eher prekär. Außerdem entstanden Jacobsohn durch erfolglose Buchausgaben von Texten seiner Autoren hohe Verluste, die er durch die Einnahmen aus seiner Zeitschrift decken musste.

Die "Schau"- und "Weltbühne" verzichteten fast völlig auf Fotografien und Illustrationen. Lediglich in einigen Ausgaben der "Schaubühne" finden sich Darstellungen von Bühnentechnik. Die Inserate in der "Weltbühne" beschränkten sich vorwiegend auf Anzeigen von Büchern. In einer Ausgabe von 1930, die 36 redaktionelle Seiten umfasst, finden sich zwölf Seiten Buchinserate und eine Seite mit Kleinanzeigen.







</doc>
<doc id="14624" url="https://de.wikipedia.org/wiki?curid=14624" title="Siegfried Jacobsohn">
Siegfried Jacobsohn

Siegfried Jacobsohn (* 28. Januar 1881 in Berlin; † 3. Dezember 1926 ebenda) war ein deutscher Journalist und Theaterkritiker.

Schon mit 15 Jahren beschloss der aus einer jüdischen Familie stammende Siegfried Jacobsohn Theaterkritiker zu werden. Im Oktober 1897 verließ er ohne Abschluss die Schule und begann – was damals auch ohne Abitur möglich war – ein Studium an der Friedrich-Wilhelms-Universität in Berlin. Seine akademischen Lehrer waren unter anderem Erich Schmidt, Ulrich von Wilamowitz-Moellendorff und Max Herrmann. Aber mehr noch als bei ihnen lernte er durch das Studium ihm vorbildlich erscheinender Kritiken, allen voran jenen Maximilian Hardens, Fritz Mauthners und Paul Schlenthers. Zusätzlichen Rat holte er sich von Schauspielern, unter ihnen Albert Bassermann, Jakob Tiedtke und Richard Leopold. 
Noch als Student wurde Jacobsohn von Hellmut von Gerlach als Theaterkritiker für die Berliner Wochenzeitung "Die Welt am Montag" engagiert. „Dieser Jüngling“, erinnerte sich Gerlach später, „hatte buchstäblich jeden Abend seines Schülerdaseins im Theater verbracht. Er kannte jeden Schauspieler in jeder Rolle, und er kannte die gesamte Bühnenliteratur. Dabei eine Treffsicherheit des Urteils, die bei solcher Jugend kaum vorstellbar schien. Es war ein Phänomen“ ("Frankfurter Zeitung", 8. November 1926, 1. Morgenblatt).

In der "Welt am Montag" erschien im März 1901 Jacobsohns erster Beitrag. Im Juni 1902 erhielt er einen Vertrag als Redakteur mit einer Laufzeit von drei Jahren. Im September 1902 übernahm er zusätzlich die Theaterkorrespondenz für die Wiener Tageszeitung "Die Zeit". 

Er profilierte sich schnell als scharfer Kritiker von Dilettantismus auf der Bühne und scheute sich 1902 in der Auseinandersetzung um Hermann Sudermanns Streitschrift "Die Verrohung in der Theaterkritik" auch nicht, das "Berliner Tageblatt" als den „Sitz der Kunstkorruption“ zu attackieren. Im November 1904 revanchierte sich die Feuilletonredaktion dieser im Mosse-Verlag erscheinenden Tageszeitung, indem sie Jacobsohn des Plagiats in zwei Fällen bezichtigte. Die Übereinstimmungen mit Texten des Theaterkritikers Alfred Gold erklärte er damit, dass nach der Arbeit an seinem Buch "Das Theater der Reichshauptstadt" in seinem Gedächtnis „von fremden Autoren Worte, Bilder, Sätze und ganze Satzfolgen [schlummerten], die durch die geringste Assoziation geweckt“ worden seien. Trotz der Fürsprache von Maximilian Harden und Arthur Schnitzler, die angesichts der Übereinstimmungen bei Allerweltsformulierungen an einen „geistigen Diebstahl“ nicht glauben mochten, entließ die "Welt am Montag" ihren Kritiker. 

Nach einer mehrmonatigen Europareise, die ihn unter anderem nach Wien, Rom und Paris führte, kehrte er Ende Mai 1905 mit Plänen zu einer Theaterzeitschrift zurück nach Berlin. Das erste Heft der zunächst ganz auf Theaterfragen spezialisierten Wochenschrift, die in Anspielung auf Friedrich Schillers Aufsatz "Die Schaubühne als moralische Anstalt betrachtet" den Titel "Die Schaubühne" erhielt, erschien am 7. September 1905. Zu den wichtigsten Mitarbeitern gehörten Julius Bab, Willi Handl, Alfred Polgar, Robert Walser (ab 1907), Lion Feuchtwanger (ab 1908), Herbert Ihering (ab 1909), Robert Breuer (ab 1911) und Kurt Tucholsky (ab 1913).
Von 1913 an öffnete Jacobsohn das „Blättchen“, wie er seine Zeitschrift gerne nannte, allmählich auch für politische Themen. Im April 1918 benannte er sie in "Die Weltbühne" um und entwickelte sie zu einem über Deutschland hinaus bekannten pazifistischen Forum der politischen Linken. Mit der Neuausrichtung änderte sich allmählich auch das Mitarbeiterprofil. Polgar und Tucholsky gehörten weiterhin zum engen Kreis der Autoren. Hinzu kamen u. a. der politische Publizist Kurt Hiller (ab 1915), der Nationalökonom Alfons Goldschmidt (ab 1917), der satirische Schriftsteller Hans Reimann (ab 1917), der Mitgründer der Deutschen Friedensgesellschaft Otto Lehmann-Rußbüldt (ab 1918), der sozialdemokratische Politiker Heinrich Ströbel (1919/20), der Kunstkritiker Adolf Behne (ab 1920), der Schriftsteller Walter Mehring (ab 1920), der Wirtschaftsjournalist Richard Lewinsohn (ab 1921), der Publizist Friedrich Sieburg (ab 1921) und der politische Redakteur Carl von Ossietzky (ab 1926).

Als Theaterkritiker war Jacobsohn Antipode von Alfred Kerr, anders als Kerr ein entschiedener Kritiker des Naturalismus und schätzte im Gegensatz zu ihm auch die Leistungen von Max Reinhardt als Theaterleiter und -regisseur weit höher ein als die von Otto Brahm. Reinhardts Hinwendung zu einem Arenatheater, die in Berlin in den Umbau des Zirkus Schumann zum 1919 eröffneten Großen Schauspielhaus mündete, hat er jedoch entschieden missbilligt. Nach dem Ersten Weltkrieg setzte er große Hoffnungen in die Arbeit von Leopold Jessner als Intendant des Staatlichen Schauspielhauses in Berlin, registrierte auch aufmerksam die Inszenierungen von Ludwig Berger, Jürgen Fehling, Heinz Hilpert, Berthold Viertel und schließlich Erwin Piscator.

Die von Jacobsohn neben den Klassikern, hier vor allem William Shakespeare, geschätzten und geförderten Autoren waren in den ersten Jahren unter anderen Hugo von Hofmannsthal und Arthur Schnitzler. Die Hoffnungen, die er auf die neoromantische Dramatik setzte, erfüllten sich für ihn jedoch nicht. Das war wahrscheinlich ein wesentlicher Grund für seine 1913 einsetzende allmähliche Politisierung. Die Bühnenwerke des Expressionismus beurteilte er – mit Ausnahme der Stücke von Georg Kaiser und Ernst Toller – außerordentlich kritisch: „In dieser ganzen Generation von Dichtern“, schrieb er in der "Schaubühne" vom 14. März 1918, „ist des eigentlichen Dichtertums zu wenig.“ 

In der Zeit der Weimarer Republik gehörte Jacobsohn zu den wenigen Theaterkritikern, die sofort das Potential von Autoren wie Bertolt Brecht, Arnolt Bronnen und Carl Zuckmayer erkannten, obwohl ihn – wie er Herbert Ihering 1920 gestand – längst Theatermüdigkeit plagte.

Durch „Heranziehung der Richtigen“, so Kurt Hiller 1950 in seinem Buch "Köpfe und Tröpfe", habe Jacobsohn nach dem Ersten Weltkrieg einen „gewaltigen Aufstieg“ seines Blatts zustandegebracht. Bemerkenswert ist vor allem, dass Jacobsohn es auch für Beiträge zu Themengebieten öffnete, die zwar kulturpolitisch wichtig waren, ihm selbst aber nicht im Geringsten am Herzen lagen. Hierzu zählen vor allem Filmkritiken, für die er Hans Siemsen, Frank Warschauer, Roland Schacht und Rudolf Arnheim als Mitarbeiter gewinnen konnte. Auch in der Musikkritik setze er neue Akzente, indem er – selbst entschiedener Gegner Richard Wagners – den sozialdemokratischen Wagnerianer Klaus Pringsheim mit der Musikkritik betraute.

Politisch näherte er sich schon gegen Ende des Ersten Weltkriegs mehr und mehr den Positionen der USPD. Im Jahr 1918 engagierte er sich für kurze Zeit in Hillers Rat geistiger Arbeiter, verließ ihn aber bald, weil ihm die Redaktionsarbeit für die "Weltbühne" wichtiger erschien, er auch von Parteien nicht vereinnahmt werden wollte und ihm doktrinäre Positionen grundsätzlich zuwider waren. Er zeigte keine Berührungsängste vor gesellschaftlichen Kontakten etwa mit Oskar von Hindenburg, worüber er Tucholsky ironisch nach Paris berichtete. Als dieser seine publizistische Polemik gegen Reichspräsident Ebert, den angeblichen „Verräter seiner Klasse“, ins Maßlose steigerte, ermahnte ihn Jacobsohn mit prophetischen Worten im November 1924:

Jacobsohn war ein ausgesprochen streitbarer Charakter. In der Rubrik „Antworten“ und in eigenen Artikeln und Artikelreihen attackierte er auch Gegner, von denen man meinen könnte, sie hätten seine Unterstützung verdient: So kam es 1913 zu einem heftigen Konflikt mit Theodor Lessing, nachdem Jacobsohn dessen Gutachten über Hermann Sudermanns Theaterstück "Der gute Ruf" abgedruckt hatte und Sudermann deswegen die Gerichte bemühte. Nachdem er sich schon um 1918 mit Stefan Großmann überworfen hatte, behauptete Jacobsohn 1920 aufgrund von Zwischenträgereien, Großmann lasse sich von Intendanten für positive Kritiken bezahlen (1922 musste er diesen Vorwurf zurücknehmen). Im Jahr 1925 bezichtigte er – auf einen Hinweis von Kurt Tucholsky hin – Heinrich Fischer unberechtigterweise des Plagiats, was zum endgültigen Zerwürfnis mit Karl Kraus führte, der - obgleich charakterlich ganz ähnlich gelagert - ihm in der "Fackel" einen „an Morphinismus grenzenden Drang zum 'Vorknöpfen“ bescheinigte. Tatsächlich brachten Jacobsohns manchmal vorschnelle, weil nicht ausreichend durch Recherchen abgesicherte Attacken ihm im Lauf der Jahre rund vierzig Prozesse ein, von denen er viele, aber nicht alle gewann. 

Jacobsohn war auch für zahlreiche andere Periodika tätig, darunter die "Deutsche Montagszeitung" (Berlin), die "Frankfurter Nachrichten", die "Weser-Zeitung" (Bremen), die "Prager Presse", das "Prager Tagblatt" und die "Zeit im Bild" (Berlin / München / Wien). 

Er starb am 3. Dezember 1926 und wurde auf dem Südwestkirchhof Stahnsdorf, südlich von Berlin, beigesetzt (Ehrengrab der Stadt Berlin). Er war seit 1915 mit der Verlegerin und Übersetzerin Edith Jacobsohn geb. Schiffer verheiratet, die mit ihrem Vermögen mehrmals den Verlag vor dem Konkurs rettete. Mit ihr hatte er einen Sohn, Peter Jacobsohn (1916–1998).

Seine Wochenzeitschrift "Die Weltbühne" wurde nach seinem Tod zunächst für einige Monate von Kurt Tucholsky (1926/1927), dann von Carl von Ossietzky weitergeführt (1927–1933). Seine Witwe Edith Jacobsohn übernahm 1927 die Verlagsleitung. 1933 wurde die Zeitschrift verboten und ihr Archiv konfisziert; es ist seitdem verschollen. Sie erschien nach dem Verbot 1933 als „Neue Weltbühne“ in Prag; Neugründung 1946 in Berlin (Ost), 1993 eingestellt.





</doc>
<doc id="14625" url="https://de.wikipedia.org/wiki?curid=14625" title="Wolfsquinte">
Wolfsquinte

Unter einer Wolfsquinte versteht man eine sehr stark „verstimmte“ Quinte, wie sie in verschiedenen historischen Stimmungen auftritt.

Bei der Benutzung der pythagoreischen Stimmung werden die Töne der Tonleiter (Skala) mit Hilfe von aufeinandergesetzten reinen Quinten definiert. Eigentlich sollte nach 12 Schritten der Grundton wieder erreicht sein. Tatsächlich ist dieser Ton jedoch um das pythagoreische Komma höher als die entsprechende Oktave des Grundtones. Um alle Tonarten „spielen“ zu können und die Quintreihe zum Quintenzirkel zu schließen, wird die zwölfte Quinte entsprechend angepasst. Die Tonreihe besteht dann aus 11 reinen und einer um ein pythagoreisches Komma verminderten Quinte, die der klassischen Vorstellung von Wohlklang stark widerspricht. Man sagte damals, "die Quinte heule wie ein Wolf."

Die pythagoreische Wolfsquinte ist um ein pythagoreisches Komma kleiner als die reine Quinte.

Zweite Rechnung: Da 11 Quinten um je 1,955 Cent größer sind als die gleichstufige Quinte mit 700 Cent, ist die 12. Quinte entsprechend kleiner: (700 − 11·1,955) Cent = 678,495 Cent.

Hinweis: Um Intervalle in ihrer Größe besser vergleichen zu können, gibt man häufig ihre Größe mit der logarithmischen Einheit Cent an. Dabei ist 1 Oktave = 1200 Cent.

Bei der mitteltönigen Stimmung entsteht eine noch weniger brauchbare Quinte, die ebenfalls "Wolfsquinte" genannt wird. Es handelt sich dabei jedoch nicht um eine Quinte, sondern um eine verminderte Sexte (in der Regel Gis–Es). Diese entsteht als Restintervall zwischen 11 aufeinandergeschichteten mitteltönigen Quinten und der siebten Oktave des Grundtons. 

Die mitteltönige Wolfsquinte errechnet sich deshalb als Differenz aus 7 Oktaven und 11 mitteltönigen Quinten. Sie ist größer als eine reine Quinte. 

Zweite Rechnung: Da 11 Quinten um je 3,422 Cent (genauer 3,42157 Cent) kleiner sind als die gleichstufige Quinte mit 700 Cent, ist die 12. Quinte entsprechend größer: (700 + 11·3,42157) Cent ≈ 737,637 Cent.

Die reine Quinte hat im Vergleich dazu das Frequenzverhältnis:

Die Quinte bei Gleichstufiger Stimmung (700 Cent) weicht davon nur wenig ab.



</doc>
<doc id="14635" url="https://de.wikipedia.org/wiki?curid=14635" title="Blue Note">
Blue Note

Als Blue Notes bezeichnet man Töne, die in besonderem Maß den Bluescharakter von Melodien prägen. Im engeren Sinne versteht man darunter die kleine Terz, die kleine Septime und die verminderte Quinte (Tritonus), deren Intonation sich jedoch von den gleichnamigen Tonstufen im westlichen Tonsystem unterscheidet. So wird die Terz gemeinhin zwischen kleiner und großer Terz intoniert. Die Septime als Blue Note wird zwischen kleiner und großer Septime oder zwischen großer Sexte und kleiner Septime nahe der für den Blues typischen Naturseptime intoniert. Die Quinte als Blue Note wird meist zwischen verminderter und reiner Quinte intoniert.

Die Blue Notes können auf eine pentatonische Tonleiter afrikanischer Herkunft zurückgeführt werden, die sich nicht in das klassische europäische Tonsystem einfügen lässt und sich stark an der Naturtonreihe orientiert. Deren Töne variieren in ihrer Höhe schon damit zwangsläufig gegenüber den Tönen unseres nicht mehr ausschließlich an der Naturtonreihe orientierten Tonsystems.

Die Blue Notes werden nur melodisch verwendet und haben keinen Einfluss auf die Harmonik eines Bluesstückes, da sich mit dem in der abendländischen Musik zur Verfügung stehenden Tonmaterial auf den Blue Notes keine Dreiklänge und somit auch keine Harmoniefolgen bilden lassen. Die Blue Notes werden daher, zum Beispiel während einer Improvisation, über die normalen, dem Dur- oder Moll-Tonsystem entnommenen Akkorde gespielt. 

Da die Blue Notes aus dem abendländischen Tonsystem herausfallen, wurden sie häufig als einfache Ergänzung zu einer Durtonleiter erklärt und notiert: Einer Durtonleiter wurde die kleine Terz, die verminderte Quinte und die kleine Septime hinzugefügt. Die so entstandene zehntönige Tonleiter ist aber wegen ihrer hohen Zahl chromatisch aufeinander folgender Töne in der Praxis kaum verwendbar. 

<score vorbis="1">
\new Staff \with { \remove "Time_signature_engraver" \remove "Bar_engraver" } 
\relative c' { 
</score>

Um die Blue Notes trotzdem in unserem Tonsystem notieren und praktikabel spielbar machen zu können, wurde die Bluestonleiter aus der pentatonischen Tonleiter in Moll durch Ergänzen der verminderten Quinte gebildet. Diese lässt sich nun zum Beispiel in der Improvisation über einen Blues gut anwenden.

Blasinstrumente sowie Saiteninstrumente (insbesondere die Gitarre) sind in der Lage, durch Ansatzveränderungen (Bläser) oder Saitenziehen (Gitarren) die Blue Notes zwischen kleiner und großer Terz, Tritonus und Quinte und kleiner und großer Septime zu treffen. Auf Tasteninstrumenten und anderen Instrumenten, auf denen eine freie Intonation nicht möglich ist, werden die Blue Notes wie folgt gespielt: b3 als kleine Terz, b5 als Tritonus und b7 als kleine Septime.

Eine exakte Definition der Tonhöhe der Blue Note ist nicht sinnvoll, da sie dem subjektiven Empfinden unterliegt und in ihrer Artikulation für den speziellen Klang vieler Bluesmusiker wie zum Beispiel der Gitarristen Albert King, Robert Johnson oder Eric Clapton, der Sängerinnen Bessie Smith, Ella Fitzgerald, Billie Holiday oder den Bluesharp-Spielern Little Walter oder Sonny Terry ausschlaggebend ist.



</doc>
<doc id="14636" url="https://de.wikipedia.org/wiki?curid=14636" title="Galapagosinseln">
Galapagosinseln

Die Galapagosinseln (; offiziell ' ‚Kolumbusarchipel‘) sind ein Archipel im östlichen Pazifischen Ozean. Sie liegen am Äquator ca. 1000 km westlich der ecuadorianischen Küste in Südamerika, gehören zu Ecuador und bilden die gleichnamige Provinz Galápagos mit der Hauptstadt Puerto Baquerizo Moreno. Das Wort ' (spanisch u. a. für ‚Wulstsattel‘) bezieht sich auf den Schildkrötenpanzer, der bei einigen Unterarten der Galápagos-Riesenschildkröte im Nackenbereich wie ein Sattel aufgewölbt ist.

Die außerordentliche und einmalige Flora und Fauna der Inseln gehören zum Weltnaturerbe der UNESCO. Sie werden durch den Nationalpark der Galapagosinseln geschützt. Etwa 97 % der Fläche der Galapagosinseln und 99 % der sie umgebenden Gewässer innerhalb der Ausschließlichen Wirtschaftszone stehen dadurch unter strengem Naturschutz. Die landwirtschaftliche und fischereiliche Nutzung sowie das Betreten der Inseln und das Befahren der Gewässer sind streng reglementiert und werden durch die Nationalparksverwaltung mit Sitz in Puerto Ayora kontrolliert.

Die Inselgruppe besteht aus 13 Inseln mit einer Fläche von mehr als 10 km² und über 100 kleineren bis winzigen Inseln, darunter die weit nordwestlich liegenden Darwin und Wolf. Fünf Inseln sind besiedelt: Santa Cruz, San Cristóbal, Isabela, Floreana und Baltra (keine Wohnbevölkerung, aber ein Militärstützpunkt mit Kasernen für ca. 400 Soldaten und Angehörige der Küstenwache).

Inseln mit einer Fläche von mehr als einem Quadratkilometer sind in der nachstehenden Tabelle aufgelistet:

Unter den zahlreichen kleineren Inseln sind die Daphne-Inseln (vor allem Daphne Major), Plaza Sur, Isla Sin Nombre sowie der isolierte Roca Redonda, eine Vogelbrutstätte, erwähnenswert.

Die Galapagosinseln wurden am 10. März 1535 zufällig von Spaniern entdeckt. Tomás de Berlanga, der damalige Bischof von Panama, kam mit seinen Leuten auf dem Weg nach Peru vom Kurs ab und strandete an einer der Vulkaninseln. Mehrere Tage verbrachten sie dort und suchten nach Trinkwasser. Zehn Pferde und zwei Spanier verdursteten. Die restlichen tranken den Saft der Kakteen und erbeuteten Seelöwen und Riesenschildkröten. In einer Schlucht fanden sie schließlich ausreichend Trinkwasser für die Heimfahrt.
Die Inselgruppe wurde zunächst als "Islas Encantadas" („Verzauberte Inseln“) bezeichnet, da niemand so weit draußen im Ozean noch Inseln vermutet hätte und starke Strömungen zwischen den Inseln und um sie herum bei den Seefahrern leicht den Eindruck erwecken konnten, die Inseln selbst änderten immer wieder ihre Lage. Im 17. Jahrhundert waren die Inseln Versteck und Fluchtort für Seeräuber – darunter John Cook oder William Cowley – die meist Goldschiffe der Spanier aus Mexiko überfielen.

Im 19. Jahrhundert wurden die Inseln nach den dort vorkommenden Riesenschildkröten in "Islas Galápagos" umbenannt. Am 12. Februar 1832 nahm General José María Villamil die Inseln für Ecuador in Besitz. Er nannte die Inseln "Archipiélago del Ecuador". Es begann die erste dauerhafte Besiedlung der Inseln. Zuvor waren die Inseln im Besitz der Spanier, die jedoch kein Interesse an ihnen zeigten. 1835 besuchte Charles Darwin die Inseln. 1892 wurden die Galapagosinseln zu Ehren von Christoph Kolumbus in "Archipiélago de Colón" umbenannt.

1953 entdeckte der norwegische Archäologe Thor Heyerdahl Reste früherer Niederlassungen auf Santa Cruz und Floreana (Santa Maria). Eine Interpretation der Funde lautet, dass sie von früheren europäischen Seefahrern bei Zwischenlandungen stammen könnten. Viele Inseln, wie Floreana oder Isabela, waren von 1934 bis 1959 Strafkolonien.

1959 erklärte die ecuadorianische Regierung die Galapagosinseln zum "Nationalpark Galápagos." 1968 waren 97 Prozent der Landfläche unter Schutz des Nationalparks gestellt; Siedlungen und bisherige landwirtschaftliche Flächen erhielten Bestandsschutz. Seit 1978 stehen die Inseln auf der UNESCO-Liste des Weltnaturerbes. 1996 kam es zur Ausrufung des "Marine-Reservats Galápagos" durch das "Instituto Ecuatoriano Forestal de Areas Naturales y Vida Silvestre." 1998 wurde der Schutz des Marine-Reservats Galapágos durch den Nationalpark Galápagos gesetzlich verankert. 2001 wurde das Weltnaturerbegebiet um das Marine-Reservat erweitert. Von 2007 bis 2010 war das Naturerbe von der UNESCO als gefährdet eingestuft, bis 2010 war es auch auf der Roten Liste der UNESCO geführt.

Die Galapagosinseln sind vulkanischen Ursprungs. Die tektonische Platte (Nasca-Platte), auf der die Inseln liegen, bewegt sich über einen Hot-Spot, der auch heute noch vulkanische Aktivität auf den Inseln Isabela und Fernandina bewirkt und bereits davor seine Spuren in der Karibik hinterließ. Die Inseln werden in Richtung Südosten älter, allerdings überlappen die Datierungen der Gesteine der einzelnen Inseln, da diese nicht durch singuläre vulkanische Ereignisse entstanden. Auf der Insel Fernandina brach der Vulkan Cumbre zuletzt im April 2009 aus, auf Isabela der Wolf im Mai 2015. Einige Gesteine der Galapagosinseln entstanden vor 89 Ma unter den heißesten Temperaturen des Phanerozoikums.

Aufgrund ihrer Entfernung von anderen Landmassen zeichnen sich die Galapagosinseln durch eine Vielzahl endemischer Tier- und Pflanzenarten aus. Die Einführung fremder Arten sowie die Jagd haben im 19. Jahrhundert viele einzigartige Tierarten fast zum Aussterben gebracht. Die Galápagos-Riesenschildkröte war lange durch Anfang des 18. Jahrhunderts eingeführte Ziegen bedroht, die den Schildkröten ihre Nahrung wegfraßen; dieses Problem wurde durch gezielte Tötung der Ziegen, u. a. aus Helikoptern, im Jahr 2007 gelöst. Die Gelege von Vögeln und darüber hinaus das Überleben aller Tiere, die über Jahrmillionen an die spezifischen Lebensräume der Inseln angepasst wurden, sind durch die mittlerweile (2009) etwa 30.000 Einwohner und ihre ursprünglich mitgebrachten, für das Ökosystem fremden Kleinraubtiere (Hunde, Katzen und Ratten) bedroht. Durch Lebensmittellieferungen gelangen oft Parasiten, Krankheitskeime, Tiere und Pflanzen auf die Inseln. Über 200 neue Arten kamen in den letzten zehn Jahren auf die Galapagosinseln, darunter Parasiten, die das Blut von Finkenjungen saugen, oder Malariaerreger, die Pinguine befallen. Die eingeschleppte Fruchtfliege "Ceratitis capitata" ist eine gefährliche Plage, denn sie kann sehr viele verschiedene Kulturarten befallen und ihre Früchte verfaulen lassen. Sie ist eine enorme Bedrohung für die Galapagosinseln.

Obwohl die Inseln in Äquatornähe liegen, ist das Klima aufgrund des mit 20 Grad Celsius relativ kühlen Meerwassers (vom Humboldtstrom und von aufsteigendem Tiefenwasser) eher gemäßigt. Das nährstoffreiche Tiefenwasser ist verantwortlich für den Artenreichtum rund um den Archipel.

Die Regenzeit dauert mit einem Gipfel im April von Januar bis Juni. Das übrige Jahr über fällt fast kein Niederschlag.

In sogenannten El-Niño-Jahren verändern sich die Meeresströmungen und die Niederschlagsmengen. Das ausbleibende Tiefenwasser dezimiert auf das Meer angewiesene Arten, während sich aus den hohen Regenmengen ein überdurchschnittliches Pflanzenwachstum und daraus ein saisonbedingter Tierreichtum ergibt.

1999 verursachte ein heftiger El Niño ein schwerwiegendes Korallensterben sowie die Vernichtung von ca. 60 % der Pinguin-Population.

Nach der Ausrufung des Nationalparks und der Gründung der Charles-Darwin-Forschungsstation wurden die Inseln zunehmend von Tourismus und Fischereiwirtschaft geprägt. Ursprünglich ein Traumziel für Biologen, wurden die Galapagosinseln zu einem boomenden Wirtschaftsraum, in dem ökonomische Interessen gegen ökologische abgewogen werden müssen.

Das Meeresgebiet rund um die Inseln bietet großen Fischreichtum. Besonders gefragt sind Haifischflossen und Seegurken. Der Fang von Seegurken ist unter Auflagen erlaubt, der Fang von Haifischen dagegen verboten. Allgemein ist der Fischfang im Marine-Reservat Galápagos nur unter Auflagen erlaubt, wobei für bestimmte Arten besondere Bestimmungen und Auflagen gelten.

Es kommt immer wieder zu Konflikten zwischen Naturschützern und Fischern, die sich durch die Auflagen behindert fühlen. Im März 2004 kam es wieder zu Unruhen; im Juni desselben Jahres wurden die Charles-Darwin-Forschungsstation und die Nationalparkverwaltung für zwei Wochen blockiert. Daraufhin wurde nicht, wie in Nachhaltigkeitsuntersuchungen gefordert, die Quote gesenkt. In der folgenden Fangsaison, Herbst 2004, wurde dann diese Quote bei weitem nicht ausgeschöpft. Das lässt vermuten, dass die Bestände stark abgenommen haben und eine nachhaltige Reproduktion zurzeit nicht stattfindet.

Im April 2007 erklärte die Regierung Ecuadors die Inseln mit ihrer einzigartigen Fauna und Flora zu einem ökologischen Risikogebiet. Tourismus, Luftfahrt und Ansiedlung sollen künftig eingeschränkt werden. Ecuador wollte damit möglichen Maßnahmen der UNESCO zuvorkommen, die wiederholt damit gedroht hatte, der Inselgruppe im Pazifischen Ozean den vor drei Jahrzehnten verliehenen Status als Welterbe der Natur abzuerkennen. Im Juni 2007 setzte die UNESCO die Inseln auf die Rote Liste des gefährdeten Welterbes. Regulierungen des Tourismus, Siedlungsbeschränkungen und Bestrebungen zur Selbstversorgung mit Energie und Lebensmitteln führten im Juli 2010 zur Streichung von der Roten Liste. Noch immer gelten jedoch mehr als die Hälfte aller endemischen Tierarten und jede fünfte Pflanzenart als bedroht.

Seit Jahren gibt es strenge Zuzugsregeln für die einheimische Bevölkerung. Diese wurden in der Vergangenheit zunächst kaum durchgesetzt. Seit dem Inkrafttreten der Gesetze von 2007 hat sich der Zustrom von illegalen Zuwanderern vom Festland zwar spürbar verringert, trotzdem nimmt die Wohnbevölkerung insgesamt weiter zu.

Auf den Galapagosinseln gibt es sieben unterschiedliche Vegetationszonen, in Abhängigkeit von der Höhenlage (Höhenangaben sind je nach Quelle sehr verschieden). Hierzu zählen folgende:


Auf der zweitgrößten Insel Santa Cruz sind alle auf den Galapagosinseln vorkommenden Vegetationszonen ausgeprägt. Auf den anderen Inseln des Archipels kommen die genannten Vegetationszonen nicht vollständig und sehr unterschiedlich ausgeprägt vor. Zum Beispiel ist auf der Insel Española nur die trockene Küstenzone und die Übergangszone vorhanden. Auf der Insel Fernandina ist die Vegetation auf die von Lava verschonten „Vegetationsinseln“ beschränkt und man findet eine hochreichende Trockenzone sowie die Zone des Scalesia-Waldes.

Auf den Galapagosinseln kommen 700 heimische Pflanzenarten vor, von denen 250 endemisch sind. Durch den Menschen wurden aber auch 500 nichtheimische Arten eingeführt, die zum Teil große Probleme verursachen.

Auf den Galapagosinseln und um die Inseln herum herrscht großer Artenreichtum. Viele Arten, die nur dort heimisch sind (rund 40 Prozent), wurden auch nach den Inseln benannt. Die meisten von ihnen kommen dort endemisch vor, das heißt nirgendwo anders auf der Erde. Einige der auf den Galapagosinseln lebenden Tiere sind:


Der Tourismus ist auf den Galapagosinseln inzwischen die größte Einnahmequelle, aber zugleich auch die größte Bedrohung für das sensible Ökosystem und die Tierwelt des Archipels.

Die Touristen kommen in der Regel per Flugzeug auf den Galapagosinseln an (Flughäfen Baltra (IATA-Code: GPS) und San Cristóbal (IATA-Code: SCY)) und treten dann eine meist organisierte Gruppenreise an. Bei den Gruppenreisen handelt es sich um Kreuzfahrten oder um landbasierte Rundreisen, wobei sich die landbasierten Rundreisen mit Hotelübernachtungen inzwischen größerer Beliebtheit erfreuen. Die wichtigsten Städte mit Übernachtungsmöglichkeiten und bedeutendsten Häfen für die Kreuzfahrttouristen sind: Baltra (nur Hafen - keine Übernachtungsmöglichkeiten), Puerto Ayora (Insel Santa Cruz), Puerto Baquerizo Moreno (Insel San Cristóbal) und Puerto Villamil (Insel Isabela). Aufgrund des Einschreitens der UNESCO werden die Touristenströme inzwischen sehr stark kontrolliert und gelenkt. Seit 2009 wurde hierfür ein neues Kontrollsystem eingeführt: die sogenannte "INGALA-Transit-Kontrollkarte" (Spanisch: Tarjeta de Control de Transito TCT). Hierbei handelt es sich um eine Art elektronisches Visum, das vor Abflug auf die Galapagosinseln erworben werden muss. Die Idee hinter diesem Visum ist, die illegale Einwanderung vor allem von Arbeitskräften, seien es Ausländer oder Einheimische, besser kontrollieren und verfolgen zu können.

Nationalparkregeln

Zur Erhaltung der Inselwelt sowie zum Schutz von Flora und Fauna wurden folgende Regeln festgelegt:

Die für den Tourismus bedeutendsten Inseln sind: die Insel Santa Cruz mit der Stadt Puerto Ayora (touristisches Zentrum des Archipels mit sehr guter touristischer Infrastruktur), die Insel San Cristóbal mit der Stadt Puerto Baquerizo Moreno (Verwaltungssitz und zweitwichtigster Flughafen), die Insel Isabela mit der Stadt Puerto Villamil (man findet hier ebenfalls einen kleinen Flughafen) sowie Floreana mit der Siedlung Puerto Velasco Ibarra. Die touristische Bedeutung der Insel Baltra bzw. South Seymour beschränkt sich auf den Besitz des wichtigsten Flug- und Kreuzfahrthafens.

Auf den Galapagosinseln gab es im Jahr 2008 insgesamt 77 Schiffe, die über ein gültiges Patent verfügten und Touristen befördern durften. Insgesamt gab es jedoch 86 Patente, mit denen pro Woche maximal 1.866 Touristen befördert werden durften.

Die Galapagosinseln wurden im Jahr 1979 noch von nur insgesamt 11.765 Touristen besucht. 2004 wurde die 100.000er-Grenze überschritten (mit 108.948 Touristen).

2016 waren es schon 218.365. Dabei sind 31 % der Besucher ecuadorianischer Nationalität und 69 % der Besucher anderer Nationalität. Die größten Gruppen ausländischer Touristen stammten dabei aus den USA (29 %), Großbritannien (5 %), Kanada (4 %) und Deutschland (4 %). Im Jahr 2007 bereiste ein Großteil der Touristen (51 %) die Galapagosinseln noch per Schiff während einer der zahlreich angebotenen Kreuzfahrten, während es 2015 nur noch 32 % waren. Der Großteil der Touristen reiste dabei über den Flughafen Baltra (78 %) und den Flughafen San Cristobal (22 %) an.

Auch in den Jahren 2014 (215.691), 2015 (224.755) und 2016 (218.365) blieben die Besucherzahlen in derselben Größenordnung.






</doc>
<doc id="14638" url="https://de.wikipedia.org/wiki?curid=14638" title="Gleichstrom">
Gleichstrom

Als Gleichstrom wird ein elektrischer Strom bezeichnet, dessen Stärke und Richtung sich zeitlich nicht ändert. In der Elektrotechnik wird auch Mischstrom mit überwiegendem Gleichanteil als Gleichstrom bezeichnet, wenn sich zwar zeitlich der Wert ändert, aber nicht die Richtung des Stromes und dabei die Schwankungen für die beabsichtigte Wirkung unwesentlich sind. In diesem Fall wird als Gleichstrom der Gleichwert des Stromes angesehen. Ein so erklärter Gleichstrom behält seine Bezeichnung bei, wenn er in der Stärke veränderlich ist, aber diese Veränderungen durch Belastungsschwankungen entstehen.

In der Umgangssprache findet oftmals eine Verwechslung bzw. Gleichsetzung zwischen Gleichspannung und den von Gleichspannungen verursachten Gleichströmen statt. An einer Gleichspannungsquelle tritt bei zeitlich konstanter Belastung ein Gleichstrom auf, der Zusammenhang dieser beiden Größen an einem ohmschen Widerstand wird über das ohmsche Gesetz beschrieben.

Die englische Bezeichnung ist mit dem Kürzel , welches synonym auch für Gleich"spannung" verwendet wird (vgl. für Wechselstrom). Die geschichtliche Entstehung erster Stromnetze im nordamerikanischen Raum ist unter Stromkrieg dargestellt.

Als Kennzeichen für Gleichstrom dienen nachstehende Schaltzeichen.

Gleichstrom wird in erheblichem Umfang durch Gleichrichter aus Wechselstrom erzeugt. Vielfach muss der bei der Gleichrichtung auftretende Wechselanteil durch Glättungskondensatoren oder Glättungsdrosseln reduziert werden. Daneben gibt es verschiedene elektrische Energiequellen, die an eine angeschlossene Last unmittelbar Gleichstrom liefern, wie Solarzellen, elektrochemische Akkumulatoren, Batterien und auch Gleichstromgeneratoren.– Umgekehrt kann aus Gleichstrom mit Hilfe eines Wechselrichters Wechselstrom erzeugt werden.

Im Haushaltsbereich wird Gleichstrom vielfach verwendet, z. B. in Rechnern, audiotechnischen oder videotechnischen Geräten. Gleichwohl gibt es keinen Gleichstrom aus der Steckdose. Vielmehr wird jedes Gerät individuell aus einem Netzteil versorgt, eventuell mit Zwischenspeicherung in einem Akkumulator. 

Im industriellen Bereich wird Gleichstrom z. B. für die Elektrolyse und in der Galvanotechnik eingesetzt. Diese Anwendungen setzen Gleichstrom voraus, da bei Wechselstrom nicht die gewünschten elektrochemischen Prozesse ablaufen würden. Ferner wird Gleichstrom teilweise als Bahnstrom verwendet.

Bei der elektrischen Energieübertragung hat Wechselstrom wesentliche Vorteile gegenüber Gleichstrom. Mit der Beherrschung von Stromrichterstationen bekommt die Hochspannungs-Gleichstrom-Übertragung zunehmend Bedeutung.


</doc>
<doc id="14642" url="https://de.wikipedia.org/wiki?curid=14642" title="Georg Philipp Telemann">
Georg Philipp Telemann

Georg Philipp Telemann (* in Magdeburg; † 25. Juni 1767 in Hamburg) war ein deutscher Komponist des Barock. Er prägte durch neue Impulse, sowohl in der Komposition als auch in der Musikanschauung, maßgeblich die Musikwelt der ersten Hälfte des 18. Jahrhunderts.

Georg Philipp Telemann verbrachte seine Jugendzeit ab 1697 in Hildesheim. Hier erhielt er eine maßgebliche Förderung, die seine musikalische Entwicklung entscheidend
prägte. In den vier Schuljahren am Gymnasium Andreanum erlernte er mehrere Instrumente, hier komponierte er die "Singende und Klingende Geographie". Danach erhielt er zahlreiche Aufträge für weitere Kompositionen.

Später erlernte er die Musik weitgehend im Selbststudium. Erste größere Kompositionserfolge hatte er während seines Jurastudiums in Leipzig, wo er ein Amateurorchester gründete, Opernaufführungen leitete und zum Musikdirektor der damaligen Universitätskirche aufstieg. Nach kurzzeitigen Anstellungen an den Höfen von Sorau und Eisenach wurde Telemann 1712 in Frankfurt am Main zum städtischen Musikdirektor und zum Kapellmeister zweier Kirchen ernannt, daneben begann er mit der Veröffentlichung von Werken im Selbstverlag. Ab 1721 besetzte er als "Cantor Johannei" und "Director Musices" der Stadt Hamburg eines der angesehensten musikalischen Ämter Deutschlands, wenig später übernahm er die Leitung der Oper. Auch hier stand er weiterhin mit auswärtigen Höfen in Verbindung und veranstaltete für die städtische Oberschicht regelmäßige öffentliche Konzerte. Mit einem achtmonatigen Aufenthalt in Paris 1737/38 erlangte Telemann endgültig internationalen Ruhm.

Telemanns musikalischer Nachlass ist außerordentlich umfangreich und umfasst alle zu seiner Zeit üblichen Musikgattungen. Typisch für Telemann sind gesangliche Melodien, einfallsreich eingesetzte Klangfarben, vor allem im Spätwerk auch ungewöhnliche harmonische Effekte. Die Instrumentalwerke sind oftmals stark von französischen und italienischen, gelegentlich auch folkloristischen polnischen Einflüssen geprägt. Im Zuge des gewandelten kulturgeschichtlichen Ideals wurde Telemanns Schaffen im 19. Jahrhundert kritisch betrachtet. Die systematische Erforschung des Gesamtwerks begann erst in der zweiten Hälfte des 20. Jahrhunderts und dauert aufgrund seines großen Umfangs an.

Telemann stammte aus einer gebildeten Familie; sein Vater und eine Reihe weiterer Vorfahren hatten Theologie studiert. Abgesehen von Telemanns Urgroßvater väterlicherseits, der zeitweilig Kantor war, hatte aber niemand aus seiner Familie direkten Bezug zur Musik. Sein Vater, Pastor Heinrich Telemann, starb am 17. Januar 1685, erst 39 Jahre alt. Die Mutter, Johanna Maria Haltmeier, war gleichfalls in einem Pastorenhaus geboren worden und vier Jahre älter als ihr Ehemann. Von den sechs Kindern erreichten nur der jüngste Sohn, Georg Philipp, sowie der 1672 geborene Heinrich Matthias Telemann das Erwachsenenalter. Dieser Bruder starb 1746 als evangelisch-lutherischer Pastor in Wormstedt bei Apolda.

Georg Philipp besuchte das Gymnasium der Altstadt und die Schule am Magdeburger Dom, wo er Unterricht in Latein, Rhetorik, Dialektik und deutscher Dichtung erhielt. Besonders in Latein und Griechisch wies der junge Schüler Telemann gute Leistungen auf. Von seiner umfassenden Allgemeinbildung zeugen beispielsweise seine selbst verfassten deutschen, französischen und lateinischen Verse, die er in seiner späteren Autobiografie wiedergab. Daneben beherrschte Telemann die italienische und die englische Sprache bis ins hohe Alter.

Da öffentliche Konzerte zur damaligen Zeit in Magdeburg noch unbekannt waren, ergänzte die in der Schule aufgeführte weltliche Musik die Kirchenmusik. Insbesondere die altstädtische Schule, die über konzertierende Musikinstrumente verfügte und regelmäßig Aufführungen veranstaltete, hatte für die Musikpflege der Stadt große Bedeutung. Auch in den kleineren Privatschulen, die Telemann besuchte, erlernte er im Selbststudium unterschiedliche Instrumente wie Geige, Blockflöte, Cyther und Clavier. Er zeigte beachtliches musikalisches Talent und begann mit zehn Jahren, seine ersten Stücke zu komponieren – oft heimlich und auf ausgeliehenen Instrumenten. Erste fundierte musikalische Erfahrungen verdankte er seinem Kantor Benedikt Christiani. Bereits nach wenigen Wochen Gesangsunterricht war der damals zehnjährige Telemann in der Lage, den lieber komponierenden als unterrichtenden Kantor in den Oberklassen zu vertreten. Abgesehen von einer zweiwöchigen Unterweisung im Klavierspiel erhielt er keinen weiteren Musikunterricht. Gedämpft wurde sein Eifer von seinen Eltern. Insbesondere die Mutter − Witwe seit 1685 − missbilligte seine Beschäftigung mit der Musik, zumal Bekannte sie vor dem oftmals als minderwertig geltenden Musikerstand warnten.

Mit nur zwölf Jahren komponierte Telemann seine erste Oper, "Sigismundus", auf ein Libretto von Christian Heinrich Postel. Um Georg Philipp von einer musikalischen Karriere abzubringen, beschlagnahmten seine Mutter und Verwandten alle seine Instrumente und schickten ihn Ende 1693 oder Anfang 1694 zur Schule nach Zellerfeld.

Wahrscheinlich wusste sie nicht, dass der dortige Superintendent Caspar Calvör sich in seinen Schriften intensiv mit Musik beschäftigte und Telemann förderte. Calvör hatte mit Telemanns Vater die Universität von Helmstedt besucht. Er ermutigte Telemann, die Musik wieder aufzunehmen, jedoch auch die Schule nicht zu vernachlässigen. Fast wöchentlich komponierte Telemann für den Kirchenchor Motetten. Daneben schrieb er auch Arien und Gelegenheitsmusiken, die er dem Stadtpfeifer vorlegte.

1697 wurde Telemann Schüler des Gymnasium Andreanum in Hildesheim. Unter der Leitung des Direktors Johann Christoph Losius vervollkommnete er seine musikalische Ausbildung und lernte – auch hier größtenteils als Autodidakt – Orgel, Violine, Gambe, Traversflöte, Oboe, Schalmei, Kontrabass und Bassposaune. Daneben komponierte er Vokalwerke für das Schultheater. Weitere Kompositionsaufträge für den Gottesdienst des St.-Godehardi-Klosters erhielt er vom jesuitischen kirchenmusikalischen Direktor der Stadt, Pater Crispus.

Telemann wurde auch durch das Musikleben in Hannover und Braunschweig-Wolfenbüttel beeinflusst, wo er mit französischer und italienischer Instrumentalmusik in Berührung kam. Die zu dieser Zeit gesammelten Erfahrungen sollten große Teile seines späteren Werks prägen. Außerdem lernte er bei heimlichem Musikunterricht die italienisch geprägten Stile von Rosenmüller, Corelli, Caldara und Steffani kennen.

1701 beendete Telemann seine Schulausbildung und schrieb sich an der Universität Leipzig ein. Unter dem Druck seiner Mutter nahm er sich vor, wie vorgesehen Jura zu studieren und sich nicht mehr mit der Musik zu beschäftigen. Zumindest versicherte er dies in seiner Autobiografie; dennoch scheint die Wahl der Stadt Leipzig, die als bürgerliche Metropole der modernen Musik galt, nicht zufällig gewesen zu sein. Schon auf dem Weg nach Leipzig hielt Telemann in Halle, um den damals sechzehnjährigen Georg Friedrich Händel zu treffen. Mit ihm begründete er eine Freundschaft, die sein ganzes Leben andauern sollte. Telemann schrieb, dass er seine musikalischen Ambitionen zunächst vor seinen Kommilitonen verheimlicht habe. Angeblich fand jedoch Telemanns musikbegeisterter Zimmerkamerad dank eines (wohl fingierten) Zufalls eine Komposition unter dessen Handgepäck, die er am folgenden Sonntag in der Thomaskirche aufführen ließ. Daraufhin wurde Telemann vom Bürgermeister beauftragt, zwei Kantaten pro Monat für die Kirche zu komponieren.

Nur ein Jahr nach dem Eintritt in die Universität gründete er für die musikalischen Studenten ein 40-köpfiges Amateurorchester (Collegium musicum), das auch öffentliche Konzerte gab. Im Gegensatz zu ähnlichen studentischen Einrichtungen dieser Art blieb das Collegium auch nach Telemanns Weggang bestehen und wurde unter dessen Namen weitergeführt. Unter Leitung von Johann Sebastian Bach, von 1729 bis 1739, trat das „Telemannische“ Collegium Musicum im Café Zimmermann mit Konzerten von Werken Bachs und anderer zeitgenössischer Komponisten auf, die großen Einfluss auf das Musikleben der Stadt ausübten.

Im selben Jahr leitete Telemann Aufführungen des Opernhauses, an denen auch viele Mitglieder des Collegium teilnahmen und dessen Hauptkomponist er bis zur Schließung blieb. Zu den Aufführungen spielte er den Generalbass und sang gelegentlich. Von Telemanns wachsendem Ansehen irritiert, warf der offizielle städtische Musikdirektor Johann Kuhnau ihm vor, mit seinen weltlichen Werken zu großen Einfluss auf die geistliche Musik genommen zu haben, und verweigerte die Teilnahme seiner Choristen an den Opernaufführungen. 1704 wurde Telemann nach erfolgreicher Bewerbung von der Neukirche, der damaligen Universitätskirche der Stadt, als Musikdirektor eingestellt. Die damit verbundene Organistenstelle gab er allerdings an Studenten ab.

Telemann unternahm von Leipzig aus zweimal Reisen nach Berlin. 1704 erhielt er von Graf Erdmann II. von Promnitz das Angebot, als Nachfolger von Wolfgang Caspar Printz Kapellmeister am Hof von Sorau in der Niederlausitz zu werden – weshalb er die gräfliche Aufmerksamkeit auf sich zog, ist unbekannt. Daraufhin bot die Stadt, die den neuen Kompositionsstil schätzte, Telemann das Thomaskantorat und die Nachfolge Kuhnaus an. Möglicherweise bewogen die zwischen Kuhnau und Telemann entstandenen Spannungen Letzteren dazu, Leipzig dennoch frühzeitig zu verlassen.

Im Juni 1705 begann Telemann seine Arbeit in Sorau. Der Graf war ein großer Bewunderer der französischen Musik und sah in Telemann einen würdigen Nachfolger der von Lully und Campra geprägten Versailler Musikschule, von deren Kompositionen er bei einer Frankreich-Reise einige Abschriften mitbrachte und die Telemann nun studierte. In Sorau traf Telemann auf Erdmann Neumeister, dessen Texte er später vertonte und den er auch in Hamburg wiedersehen sollte. Auf Reisen nach Krakau und Pleß lernte er die polnische und mährische Folklore, wie sie wohl in Wirtshäusern und auf öffentlichen Veranstaltungen aufgeführt wurde, schätzen.

1706 verließ Telemann das vom Einmarsch der schwedischen Armee bedrohte Sorau und ging nach Eisenach, vermutlich auf eine Empfehlung des mit den sächsischen Herzogsfamilien verwandten Grafen Promnitz. Dort wurde er im Dezember 1708 Konzertmeister und Kantor am Hof des Herzogs Johann Wilhelm und gründete ein Orchester. Oft musizierte er gemeinsam mit Pantaleon Hebenstreit. Außerdem traf Telemann auf den Musiktheoretiker und Organisten Wolfgang Caspar Printz sowie auf Johann Bernhard und Johann Sebastian Bach. Er komponierte in Eisenach Konzerte für verschiedene Besetzungen, etwa 60 bis 70 Kantaten sowie Serenaden, Kirchenmusiken und „Operetten“ für festliche Anlässe. Den Text dazu verfasste er meistens selbst. Hinzu kamen etwa vier oder fünf Jahrgänge an Kantaten für den Gottesdienst. Als Bariton war er bei der Aufführung seiner eigenen Kantaten beteiligt.

Im Oktober 1709 heiratete Telemann Amalie Luise Juliane Eberlin, eine Hofdame der Gräfin von Promnitz. Kurz zuvor noch wurde er vom Herzog zum Sekretär ernannt – eine zur damaligen Zeit hohe Auszeichnung. Telemanns Frau, eine Tochter des Komponisten Daniel Eberlin, verstarb bereits im Januar 1711 bei der Geburt der ersten Tochter am Kindbettfieber.

Vielleicht weil er auf der Suche nach neuen Herausforderungen war, vielleicht um vom Adel unabhängig zu sein, bewarb sich Telemann in Frankfurt am Main. Dort ernannte man ihn im Februar 1712 zum städtischen Musikdirektor und zum Kapellmeister der Barfüßer-, wenig später auch der Katharinenkirche. Er vollendete seine in Eisenach begonnenen Kantatenjahrgänge und komponierte fünf weitere. Außerdem war er für den Unterricht einiger Privatschüler zuständig. Wie auch in Leipzig begnügte sich Telemann in Frankfurt nicht mit diesen Verpflichtungen. 1713 übernahm er die Organisation der wöchentlich stattfindenden Konzerte sowie verschiedene Verwaltungsaufgaben der vornehmen Stubengesellschaft "Zum Frauenstein" im "Haus Braunfels" auf dem Liebfrauenberg, wo er selbst auch wohnte. Außerdem ernannte der Eisenacher Hof Telemann zum Kapellmeister „von Haus aus“, sodass er seinen Titel behielt, aber Kantaten und Gelegenheitsmusiken nur noch an den Hof und an die Kirchen lieferte. Dies geschah bis 1731.

Während seiner Zeit in Frankfurt komponierte Telemann neben den Kantaten Oratorien, Orchester- und Kammermusik, von der ein Großteil veröffentlicht wurde, sowie Musik für politische Festakte und Hochzeitsserenaden. Allerdings fand er keine Gelegenheit, Opern zu veröffentlichen, wenngleich er weiterhin für die Leipziger Oper schrieb.

1714 heiratete Telemann die 16-jährige Maria Catharina Textor (1697–1775), die Tochter eines Ratskornschreibers. Ab dem darauffolgenden Jahr gab er seine ersten gedruckten Werke im Selbstverlag heraus. Auf einer Reise nach Gotha im Jahr 1716 wurde Telemann vom Herzog Friedrich eine Stelle als Kapellmeister angeboten. Der Herzog versprach ihm nicht nur, seine Tätigkeit als Kapellmeister von Haus aus für den Eisenacher Hof zu bewahren, sondern veranlasste auch den Herzog von Sachsen-Weimar, Telemann eine weitere Kapellmeisterstelle zuzusagen. Damit wäre Telemann gewissermaßen Oberkapellmeister aller sächsisch-thüringischen Höfe geworden.

Ein an den Frankfurter Rat gerichteter Brief, in dem Telemann in höflichen Worten ein Ultimatum bezüglich seines Gehaltes stellte, beweist sein diplomatisches Geschick. Er blieb in Frankfurt und setzte eine Gehaltserhöhung von 100 Gulden durch. Zusammen mit seinen Einkünften aus der Gesellschaft Frauenstein und Honoraren für Gelegenheitskompositionen bezifferten sich Telemanns Jahreseinkünfte auf 1.600 Gulden, womit er zu den Bestbezahlten in Frankfurt gehörte.

Während eines Besuchs in Dresden im Jahr 1719 traf er wieder auf Händel und widmete dem Geigenvirtuosen Pisendel eine Sammlung von Violinkonzerten. Telemann schrieb auch weiterhin bis 1757 alle drei Jahre Werke für Frankfurt, nachdem er die Stadt verlassen hatte.

1721 nahm Telemann das Angebot an, als Nachfolger von Joachim Gerstenbüttel das Amt des "Cantor Johannei" und "Director Musices" der Stadt Hamburg zu übernehmen. Vermutlich schlugen Barthold Heinrich Brockes und Erdmann Neumeister seinen Namen vor. Telemann war allerdings schon früher mit der Hansestadt in Verbindung gekommen, da er bereits an ein oder zwei Opern für die Oper am Gänsemarkt beteiligt gewesen war. Als musikalischer Leiter der Stadt wirkte Telemann unter anderem an den fünf großen ev. luth. Stadtkirchen – mit Ausnahme des Domes, für den Johann Mattheson verantwortlich war. Telemanns feierlicher Amtsantritt fand am 16. Oktober statt. Erst hier begann mit der Möglichkeit, Werke aller Formen zu komponieren und aufzuführen, seine 46 Jahre lang andauernde Hauptschaffensphase. Die naheliegende Übersetzung von Telemanns Amtstitel als „Kantor“ ist insofern irreführend, als sich die eigentliche Kantoratsarbeit am Johanneum auf gelegentliche Festkantaten und die musikalische Ausstattung der sonstigen Schul-Actus beschränkte.

In seinem neuen Amt verpflichtete sich Telemann zur Komposition von zwei Kantaten wöchentlich und einer Passion pro Jahr, in späteren Jahren griff er allerdings bei seinen Kantaten auf frühere Werke zurück. Daneben komponierte er zahlreiche Musiken für private und öffentliche Anlässe, etwa für Gedenktage und Hochzeiten. Das Amt des Cantoris Johannei war auch mit einer Tätigkeit als Musiklehrer des Johanneum verbunden; seinen Verpflichtungen zu außermusikalischem Unterricht kam Telemann jedoch nicht selbst nach. Außerdem baute er das bereits 1660 von Matthias Weckmann gegründete, aber mittlerweile nicht mehr konzertierende Collegium musicum neu auf. Die Eintrittskarten verkaufte er persönlich.

Auch in seiner neuen Heimatstadt ließ Telemann die Verbindungen nach Thüringen zunächst nicht abreißen. Er diente dem Herzog von Sachsen-Eisenach ab 1725 als Agent und berichtete dem Eisenacher Hof über Neuigkeiten aus Hamburg. Erst 1730 gab er die Stelle an den Arzt Christian Ernst Endter ab.

In Hamburg nahm Telemann seine Tätigkeit als Verleger wieder auf. Um Kosten zu sparen, stach er entweder selbst die Kupferplatten, oder er verwendete ein 1699 von William Pearson entwickeltes und bis dahin nur in England gebräuchliches Verfahren, bei dem er mit Bleistift die Noten spiegelverkehrt auf eine Platte aus Hartzinn aufzeichnete. Die Druckplatte wurde dann von einem anderen ausgeschabt und abgezogen. Dabei schaffte Telemann neun bis zehn Platten pro Tag. Bis 1740 veröffentlichte er 46 Notenwerke im Selbstverlag, die er in mehreren deutschen Städten sowie in Amsterdam und London an Buchhändler verkaufte. Man konnte auch beim Komponisten selbst Partituren bestellen; bis 1739 informierten regelmäßig ergänzte Kataloge den Musikfreund. Unter den Werken sind zum Beispiel Zwölf Fantasien für Viola da Gamba solo, die er 1735 druckte.

Telemann hatte jedoch in der Hansestadt mehr Ärger, als er erwartet hatte. Der Ratsdrucker verweigerte eine Beteiligung Telemanns am Verkaufserlös der Kantaten- und Passions-Textheftchen. Aus dem darauffolgenden langwierigen Rechtsstreit sollte Telemann erst 1757 siegreich hervorgehen. Zudem beschwerte sich das Kollegium der Oberalten, als Telemann 1722 einige Kantaten in einem vornehmen Wirtshaus (gemeint war das Baumhaus im Hamburger Hafen) aufführen wollte. Zusammen mit der unzureichenden Bezahlung und seiner zu kleinen Wohnung bewogen ihn diese Vorfälle dazu, sich nach dem Tode Kuhnaus um die Stelle als Thomaskantor in Leipzig zu bewerben. Unter den sechs Bewerbern wurde er einstimmig gewählt, worauf er am 3. September 1722 ein Entlassungsgesuch einreichte, das im Gegensatz zu seinem Brief an den Frankfurter Rat durchaus ernsten Anschein hat. Da der Hamburger Rat nun sein Gehalt um 400 Mark lübisch erhöhte, lehnte Telemann die Stelle als Thomaskantor etwas später ab und blieb in Hamburg. Seine gesamten Jahreseinkünfte betrugen damit etwa 4.000 Mark lübisch.

Erst jetzt gedieh Telemanns Tätigkeit in Hamburg auf allen Gebieten. Noch im selben Jahr übernahm er für ein Jahresgehalt von 300 Talern die Leitung der Oper. Dieses Amt führte er bis zur Schließung des Hauses im Jahr 1738 weiter. Von den etwa 25 Opernwerken aus dieser Zeit sind die meisten verschollen. 1723 übernahm Telemann zusätzlich eine Stelle als Kapellmeister von Haus aus für den Hof des Markgrafen von Bayreuth. Dorthin lieferte er von Zeit zu Zeit Instrumentalmusik sowie eine Oper jährlich. Telemanns Konzertveranstaltungen fanden meist im „Drillhaus“, der Exerzierhalle der Hamburgischen Bürgerwehr, statt und waren aufgrund des hohen Eintrittspreises den höheren Klassen vorbehalten. Telemann lieferte für seine Aufführungen – abgesehen von denen im Opernhaus – fast ausschließlich eigene Kompositionen.

1728 gründete Telemann zusammen mit Johann Valentin Görner die erste deutsche Musikzeitschrift, die auch Kompositionsbeiträge unterschiedlicher Musiker enthielt. "Der getreue Musikmeister" sollte das Musizieren daheim fördern und erschien zweiwöchentlich. Neben Telemann und Görner trugen auch elf andere zeitgenössische Musiker, unter anderem Keiser, Bonporti und Zelenka, mit ihren Kompositionen zur Zeitschrift bei. Weitere Sammelwerke zu Lehrzwecken folgten.

In zwölf Jahren gebar Telemanns Frau Maria Catharina neun Kinder, von denen zwei starben. Sie musste bei fast permanenter Schwangerschaft einen wachsenden Hausstand mit bis zu zwölf Personen versorgen, darunter Georg Philipp Telemanns Tochter aus erster Ehe und drei weitere Personen (vermutlich eine Magd, einen Hauslehrer und einen Schüler Telemanns) sowie Telemann selbst. Zehn Jahre nach der Geburt des letzten Kindes trennte sich das Ehepaar, nachdem Telemann entdeckt hatte, dass seine Frau im Glücksspiel 5.000 Reichstaler (15.000 Mark lübisch) verloren hatte. Man geht davon aus, dass die Scheidung wegen Ehebruchs Maria Catharinas ausgesprochen wurde. Sie ging 1735 nach Frankfurt zurück, während in Hamburg das Gerücht gestreut wurde, sie sei verstorben. Ohne Telemanns Wissen ließen einige Hamburger Bürger eine Spendenaktion organisieren, um ihn vor dem Bankrott zu retten. Dass es Telemann dennoch gelang, seine dringlichsten Gläubiger hauptsächlich aus eigener Tasche zufriedenzustellen, und dass er sich mehrere – offensichtlich von der Stadt bewilligte – Kuraufenthalte in Bad Pyrmont leistete, beweist, dass er ein vermögender Mann war.

Einem langgehegten Wunsch folgend, besuchte Telemann im Herbst 1737 Paris, nachdem er von einer Gruppe dortiger Musiker (Forqueray, Guignon und Blavet) dazu eingeladen worden war. Während seiner Abwesenheit ließ sich Telemann von Johann Adolf Scheibe vertreten. Sieben Werke Telemanns lagen in Paris bereits im Nachdruck vor. Nach viermonatigem Aufenthalt verlieh der König ihm ein 20 Jahre dauerndes Exklusivrecht an seinen Veröffentlichungen, das vor Raubdrucken schützen sollte. Mit mehreren Aufführungen seiner Werke gelangte Telemann endgültig zu internationalem Ruhm. Als erster deutscher Komponist durfte er sich am "Concert Spirituel", das öffentliche Konzerte gab, vorstellen. Im Mai 1738 kehrte Telemann, dessen Ansehen auch in Deutschland durch die Reise erhöht worden war, nach Hamburg zurück. 1739 wurde er in die von Lorenz Mizler begründete "Correspondierende Societät der musicalischen Wissenschaften", die sich mit musiktheoretischen Fragen beschäftigte, aufgenommen.

In einer im Oktober 1740 erschienenen Zeitungsanzeige bot Telemann die Druckplatten von 44 selbstverlegten Werken zum Verkauf an, da er sich nunmehr auf die Veröffentlichung von Lehrschriften konzentrieren wolle. Aus den folgenden 15 Jahren sind vergleichsweise wenige Kompositionen erhalten. Zunehmend setzte Telemann ungewöhnliche Instrumentenkombinationen und neuartige harmonische Effekte ein. Außerhalb seiner Pflichten widmete er sich der Sammlung seltener Blumen.

Aus der Zeit ab 1755 sind noch drei große Oratorien und weitere geistliche und weltliche Werke erhalten. Telemanns Sehvermögen verschlechterte sich zusehends, außerdem litt er an Beinbeschwerden. Immer häufiger zog er seinen ebenfalls komponierenden Enkel Georg Michael zur Unterstützung beim Schreiben heran. Telemanns Humor und Innovationskraft litten nicht unter seiner Müdigkeit. Sein letztes Werk, eine "Markus-Passion", komponierte er 1767. Am 25. Juni, im Alter von 86 Jahren, starb Telemann an den Folgen einer Lungenentzündung. Er wurde auf dem Friedhof des St.-Johannis-Klosters beigesetzt, an dessen Stelle sich heute der Rathausmarkt befindet. Dort erinnert eine Gedenkplatte links neben dem Eingang zum Rathaus an ihn. Sein Nachfolger im Amt wurde sein Patensohn, Carl Philipp Emanuel Bach.

Über Telemanns Leben und Werk sind mehr Details überliefert als über viele seiner zeitgenössischen Kollegen. Neben etwa 100 Briefen sind auch Gedichte, Vorworte und diverse Artikel des Komponisten überliefert. Die wichtigsten Textquellen aber sind – ungeachtet ihrer Fehler – seine drei Autobiografien, die er auf Wunsch der Musikgelehrten Mattheson (1718 und 1740) sowie Johann Gottfried Walther (1729) schrieb. Die Lebensabschnitte in Sorau und Eisenach sowie nach dem Erscheinen der letzten Autobiografie sind in den von Telemann selbst stammenden Textquellen kaum beschrieben, lassen sich aber durch indirekte Hinweise in diversen anderen Dokumenten grob rekonstruieren.

Mehrere zeitgenössische Musiker – darunter auch Telemanns Schüler Johann Christoph Graupner, Johann Georg Pisendel und Johann David Heinichen – griffen Elemente von Telemanns Schaffen auf. Andere Komponisten wie Gottfried Heinrich Stölzel eiferten ihnen bald nach. Weitere Schüler aus der Hamburger Zeit, denen Telemann nicht das Instrumentalhandwerk, sondern „Stilkunde“ vermittelte, sind Jacob Wilhelm Lustig, Johann Hövet, Christoph Nichelmann, Jacob Schuback, Johann Christoph Schmügel, Caspar Daniel Krohn und Georg Michael Telemann. Telemanns polnische Einflüsse regten Carl Heinrich Graun zum Nachahmen an; Johann Friedrich Agricola lernte in jungen Jahren aus Telemanns Werken. Auch Johann Friedrich Fasch, Johann Joachim Quantz und Johann Bernhard Bach erwähnten Telemann ausdrücklich als Vorbild für einige ihrer Werke. Aus eigenhändigen Bemerkungen, mit denen er die Manuskripte von Telemann versah, geht hervor, dass Carl Philipp Emanuel Bach etliche seiner Kompositionen studiert und aufgeführt hat. Die rege Freundschaft Telemanns mit Händel drückte sich nicht nur darin aus, dass Telemann mehrere von Händels Bühnenwerken – teilweise mit eigenen Einlagen – in Hamburg aufführte, sondern auch darin, dass Händel in späteren Jahren oftmals Themen von Telemann in seinen eigenen Kompositionen verwendete. Johann Sebastian Bach fertigte Abschriften mehrerer Kantaten Telemanns an und führte seinen Sohn Wilhelm Friedemann in einem für ihn angelegten Klavierbüchlein an dessen Musik heran. Das von Leopold Mozart für Wolfgang Amadeus angelegte Notenbuch enthält elf Menuette sowie eine Klavier-Fantasie von Telemann. Sowohl der Klavierstil Carl Philipp Emanuel Bachs als auch Wolfgang Amadeus Mozarts erinnert mitunter an Telemanns Schreibweise.

Neben seinen Leistungen als Komponist hatte Telemann Einfluss auf die bürgerliche Haltung zur Musik. Telemann war der Begründer eines dynamischen Hamburger Konzertlebens, indem er regelmäßige öffentliche Aufführungen außerhalb jeglicher aristokratischer oder kirchlicher Rahmenbedingungen ermöglichte.

Mit über 3600 verzeichneten Werken ist Telemann einer der produktivsten Komponisten der Musikgeschichte. Dieser große Umfang ist teils auf seine flüssige Arbeitsweise, teils auf eine mit 75 Jahren sehr lange Schaffensphase zurückzuführen. Einen Eindruck von Telemanns Arbeitsweise gab Friedrich Wilhelm Marpurg, der berichtete, zu seiner Zeit als Kapellmeister am Eisenacher Hofe seien Telemann wegen der bevorstehenden Ankunft eines hohen Besuchs nur drei Stunden Zeit gegeben worden, eine Kantate anzufertigen. Der Hofpoet verfasste den Text, und dazu schrieb Telemann gleichzeitig die Partitur, wobei er meist noch vor dem Dichter mit der Zeile fertig war. Nach etwas über einer Stunde war das Stück fertig.

Telemanns Erbe umfasst alle zu seiner Zeit verbreiteten Gattungen. Allerdings sind viele Kompositionen verschollen. Aus Telemanns Anfangszeit sind nur wenige Werke erhalten; der Großteil der überlieferten Stücke fällt in die Zeit von Frankfurt und Hamburg. Das Werk wird im "Telemann-Werke-Verzeichnis" (TWV, 1984–1999) von Martin Ruhnke aufgelistet, in das das "Telemann-Vokalwerke-Verzeichnis" (TVWV, 1982–1983) von Werner Menke einbezogen ist.

Telemann bewies Flexibilität, indem er sowohl nach wechselnden Moden seiner Zeit als auch nach der Musik verschiedener Nationen komponierte. In seiner Hauptschaffensphase wandte er sich dem empfindsamen Stil zu, der kunstgeschichtlich eher dem Rokoko als dem Barock zuzuordnen ist und eine Brücke zur Wiener Klassik schlug; oft vereinigte er diesen „galanten“ Stil mit kontrapunktischen Elementen.

Im Zentrum von Telemanns Schaffensprinzip steht ein gesanglich fundiertes Melodieideal. Er selbst betonte mehrmals die grundlegende Wichtigkeit dieses Kompositionselements; auch Mattheson charakterisierte Telemann zu Lebzeiten als einen Komponisten schöner Melodien.

In der Harmonik drang Telemann in für damalige Zeiten ungewohnte Klangbereiche vor. Er machte absichtsvollen Gebrauch der Chromatik und Enharmonik und verwendete oft Rückungen, ungewöhnliche (übermäßige und verminderte) Intervalle sowie alterierte Akkorde. In seinem Spätwerk treten die ausdruckssteigernden Dissonanzen besonders deutlich hervor. Die funktionssichere Anwendung von Dur/Moll-Paralleltonarten und Leittonwechselklängen geht zum Teil auf Jean-Philippe Rameau zurück. Im empfindsamen Stil hatte die akkordisch begleitete Oberstimme einen hohen Stellenwert. Ausgeprägte Polyphonie betrachtete Telemann daher als nicht zeitgemäß und setzte sie nur dort ein, wo sie ihm zweckdienlich erschien.

Im Gegensatz zu vielen seiner Kollegen spielte Telemann kein Musikinstrument virtuos, war aber mit einer Vielzahl vertraut und beherrschte alle gebräuchlichen. Der so erlangte Einblick in die unterschiedlichen Wirkungen verschiedener Klangfarben erklärt seine Behandlung der Instrumentation als unerlässliches Kompositionselement. Am meisten schätzte Telemann wohl die Traversflöte und Oboe, insbesondere die Oboe d’amore. Selten verwendete Telemann hingegen das Violoncello außerhalb seiner Generalbassfunktion. Gelegentlich, wie etwa in einer Arie der "Lukas-Passion" von 1744, schrieb er Skordatur vor. An Kompositionen mit besonders schwierigem oder schnellem Instrumentalspiel zeigte Telemann kein Interesse; er schrieb auch Lehrwerke bewusst geringen technischen Schwierigkeitsgrades.

Neben der im Barock und vor allem im empfindsamen Stil verbreiteten musikalischen Umsetzung von Seelenstimmungen betrieb Telemann des Öfteren akribisch ausgearbeitete Tonmalerei. Bei Vokalwerken verwendete er zur Unterstreichung von Textstellen malende Figuren, Koloraturen und Wortwiederholungen. Sowohl in weltlichen als auch in geistlichen Vokalwerken legte Telemann auf Deklamation und musikalische Wortausdeutung, insbesondere in Rezitativen, großen Wert.

Da die literarischen Strömungen des Zeitalters der Aufklärung Telemanns geistige Orientierung beeinflussten, kommt der Dichtung eine besondere Bedeutung in seinem musikalischen Schaffen zu. Die Texte zu den Vokalwerken wurden teils von ihm selbst verfertigt, teils stammten sie von den bekanntesten deutschen Schriftstellern seiner Zeit, darunter Brockes, Hagedorn, König, Klopstock, Neumeister und anderen. Telemann gab seine Erwartungen an geeignete Texte sowie an deren innere Gliederung den Textdichtern vor. Gelegentlich nahm er an den Libretti nachträgliche Änderungen gemäß seinen Vorstellungen vor.

Um den Charakter eines Musikstückes präzise anzugeben, wohl aber auch wegen seiner Zugehörigkeit zum Dichterverband "Teutschübende Gesellschaft", setzte sich Telemann – bereits 100 Jahre vor Robert Schumann – für die Verwendung deutscher Vortrags- und Ausdrucksbezeichnungen (z. B. „liebreich“, „unschuldig“ oder „verwegen“) ein, allerdings ohne damit Nachahmer gefunden zu haben.

Zu Telemanns Instrumentalmusik gehören etwa 1.000 (davon erhalten sind 126) Orchestersuiten sowie Sinfonien, Konzerte, Violinsoli, Sonaten, Duette, Triosonaten, Quartette, Klavier- und Orgelmusik.

Die Instrumentalwerke weisen oftmals starke Einflüsse verschiedener Nationalstile auf; gelegentlich wird dieser Stil auch „vermischter Geschmack“ genannt. Einige Stücke sind vollständig nach italienischer oder französischer Art geschrieben. Letztere übte besonders großen Einfluss auf Telemann aus und findet sich in lebhaften, fugierten Sätzen, Tanzsuiten und französischen Ouvertüren wieder. Auch die Tonmalerei ist zum Teil französischen Ursprungs.

Als erster deutscher Komponist integrierte Telemann auch im großen Umfang Elemente der polnischen Volksmusik. Im Gegensatz zu anderen Komponisten wie Heinrich Albert beschränkte er sich dabei nicht auf bekannte Elemente und Tanzformen, sondern prägte sowohl Orchester- als auch Kammermusik mit slawischer Melodik und Rhythmik. Letztere drückt sich etwa in Synkopierungen und häufigen Tempowechseln aus. Zeitweise, wenn auch seltener, nahm Telemann folkloristische Elemente weiterer Völker wie etwa des Spanischen in seine Werke auf.

Telemann trug zur Emanzipation bestimmter Instrumente bei. So schrieb er das erste bedeutende Solokonzert für Bratsche und nutzte dieses Instrument erstmals im Rahmen der Kammermusik. Ungewöhnlich für die damalige Zeit war eine Komposition ("Concert à neuf parties"), in der zwei Kontrabässe verwendet wurden. Außerdem komponierte er – ohne es so zu benennen – das erste Streichquartett. Gleichzeitig mit und unabhängig von Johann Sebastian Bach entwickelte Telemann einen Sonatentyp, in dem das Cembalo nicht mehr als Continuo, sondern als Soloinstrument auftrat. In seinen "Nouveaux Quatuors" ließ Telemann zum ersten Mal in der Musikgeschichte das Violoncello gleichberechtigt neben anderen Instrumenten konzertieren. Oft weisen seine Instrumentalwerke eine ungewöhnliche Führung der Melodiestimmen auf; in einigen Stücken beispielsweise sah er als Alternative für die Blockflöte auch ein zwei Oktaven tiefer gespieltes Violoncello oder Fagott vor.

In einigen Instrumentalwerken spielt der sich in Tonmalerei ausdrückende Humor eine große Rolle. Der Schlusssatz « L’Espérance du Mississippi » der Ouvertüre "La Bourse" etwa mit seinem Auf und Ab spielte auf den Krach an der Pariser Börse im Jahre 1720 an. Ein anderes Beispiel bietet das Konzert "Die Relinge", das das Liebesspiel eines Froschpaares musikalisch umsetzt.

Zu den heute populärsten Instrumentalwerken Telemanns gehören diejenigen, die im "Getreuen Music-Meister" und in den "Essercizii Musici" (1739/40) veröffentlicht wurden, sowie die Wassermusiken "Hamburger Ebb’ und Fluth" (1723) und die "Alster-Ouvertüre", die "Tafelmusik" (1733) und die "Nouveaux Quatuors" („Pariser Quartette“, 1737). Zu Telemanns Zeit genossen ebenso die Musiksammlungen "Singe- Spiel- und Generalbassübung" (1733) und "Melodische Frühstunden" (1735) Bekanntheit. 1730 veröffentlichte er sein "Fast allgemeines Evangelisch-musikalisches Liederbuch", das über 2000 Kirchenliedmelodien in verschiedenen Varianten enthält und für Organisten bestimmt war.

Telemanns 1.750 Kirchenkantaten stellen fast die Hälfte seines gesamten Nachlasses dar. Daneben schrieb er 16 Messen, 23 Psalmvertonungen, über 40 Passionen, 6 Oratorien sowie Motetten und andere sakrale Werke.

Telemanns Kantaten lösen sich vom älteren Typ, der nur Choräle und unveränderte Bibelstellen vertonte. Früher als Johann Sebastian Bach und in ganz anderem Umfang hält sich Telemann an die von Erdmann Neumeister entwickelte Form, worin einem einleitenden Bibelvers "(Dictum)" oder Choral Rezitative, Arien und ggf. Ariosi folgen und meist in einen Schlusschoral oder die Wiederholung des Eingangschors münden. In der Regel schrieb Telemann Soloarien, Duette vergleichsweise selten; von Soloterzetten und -quartetten gibt es nur Einzelbeispiele.

Neben vierstimmigen Chören finden sich auch Beispiele von Drei- oder Fünfstimmigkeit, selten Doppelchöre. Wie auch in der Instrumentalmusik zieht Telemann hier fugierte Abschnitte vollständig gearbeiteten Fugen vor. Allerdings ist die Permutationsfuge recht zahlreich vertreten.

Dramatik und detaillierte Tonmalerei bestimmen Telemanns Oratorien. Dabei verwendet er mannigfaltige Ausdrucksformen wie wiederholte Rezitative, häufige Instrumenteneinsätze zur Unterstreichung von Seelenstimmungen und Situationen sowie kurze konzertante Phrasen. Die Chöre setzen vehement und selbstbewusst, gelegentlich unisono, ein. Die Harmonik ist meist einfacher, aber anschaulicher und weiter auf die jeweilige Situation zugeschnitten als im älteren barocken Stil.

Zu den populärsten geistlichen Werken von Telemann zählten seinerzeit – gemessen an den nachweisbaren Aufführungen und erhaltenen Quellenabschriften – die "Brockes-Passion" (1716), das "Selige Erwägen" (1722), der "Tod Jesu" (1755), die "Donner-Ode" (1756), "Das befreite Israel" (1759), "Der Tag des Gerichts (Geschrieben von Christian Wilhelm Alers)" (1762) sowie "Der Messias" (1759). Um den Anforderungen der sehr zahlreichen kleineren Kirchen sowie den Lehrzwecken für den Hausgebrauch gerecht zu werden, veröffentlichte Telemann auch Kantatensammlungen in kammermusikalischer Besetzung, wie "Der harmonische Gottesdienst" (1725/26; Fortsetzung 1731/32).

Telemann schrieb zudem zahlreiche Trauermusiken für hochgestellte Persönlichkeiten seiner Zeit – so für August den Starken ("Unsterblicher Nachruhm Friederich Augusts", fälschlich auch "Serenata eroica", 1733), Georg II. von Großbritannien (1760), die römisch-deutschen Kaiser Karl VI. (1740, verschollen), Karl VII. (1745) und Franz I. (1765, verschollen), neun weitere für verschiedene Hamburger Bürgermeister (darunter der so genannte "Schwanengesang" für Garlieb Sillem, 1733), zwei für das Pastoren-Ehepaar Elers sowie die nicht näher datierbare, aber vielleicht bekannteste Kantate "Du aber, Daniel, gehe hin" und noch sieben weitere, die jedoch teils nur noch fragmentarisch oder im Textbuch überliefert sind.

Telemanns weltliche Vokalwerke lassen sich in Opern, großangelegte Festmusiken für offizielle Angelegenheiten, Kantaten im privaten Auftrag und Kantaten, in denen er dramatische, lyrische oder humorige Texte vertonte („Oden“, „Kanons“, „Lieder“) unterteilen.

Die meisten der überlieferten Opern wenden sich dem komischen Genre zu. Romain Rolland bezeichnete Telemann als den Komponisten, der der Opéra comique in Deutschland zu größerer Verbreitung verhalf.

Im Gegensatz zu Händel, der sich fast ausschließlich auf Soloarien beschränkte, machte Telemann in seinen Opern Gebrauch von äußerst verschiedenartigen Stilmitteln. Dazu gehören unterschiedlich gearbeitete Rezitative, Da-capo-Arien, tänzerische Motive, singspielartige Arien, "arie di bravura" und Stimmlagen vom Bass bis zum Kastraten. Charaktere und Situationen stellte Telemann konsequent mit darauf abgestimmter Melodik, Motivik und Instrumentation dar; auch hier machte er einfallsreichen Gebrauch diverser pittoresker Figuren.

Zu den ehemals beliebtesten und heute zum Teil wiederentdeckten der rund 50 Opern gehören "Der geduldige Sokrates" (1720), "Sieg der Schönheit oder Gensericus" (1722), "Der neumodische Liebhaber Damon" (1724), "Pimpinone oder Die ungleiche Heirat" (1725) und "Emma und Eginhard" (1728). Die Oper "Germanicus" war lange bis auf wenige Arien verschollen; Arien einer Sammlung konnten ihr vor einigen Jahren (2005?) zugeordnet werden und wurden inzwischen aufgeführt und eingespielt.

Zu den Festmusiken gehören die Hamburger Admiralitätsmusik sowie die 12 Kapitänsmusiken, von denen 9 vollständig und 3 teilweise erhalten sind. Diese Werke zeichnen sich durch musikalische Prachtentfaltung und besonders sangliche Melodien aus.

Telemanns letzte weltliche Kompositionen weisen eine hohe Dramatik und ungewöhnliche Harmonik auf; die Kantaten "Ino" (1765) und "Der May – Eine musicalische Idylle" (um 1761), aber auch das geistliche Spätwerk "Der Tod Jesu" erinnern ob ihrer extremen Gefühlsregungen an die Musik Christoph Willibald Glucks. Die weltliche Kantate "Trauer-Music eines kunsterfahrenen Canarienvogels" („Kanarienvogel-Kantate“) gehört zu seinen bekanntesten Kompositionen. Die sogenannte „Schulmeisterkantate“ ("Der Schulmeister in der Singschule"), die lange Zeit als ein Werk Telemanns galt, stammt jedoch tatsächlich von Christoph Ludwig Fehre.

In seinen Liedern knüpfte Telemann an das Schaffen Adam Kriegers an und entwickelte es in textlicher und melodischer Hinsicht weiter. Die Melodien sind einfach gehalten und häufig in unregelmäßige Perioden gegliedert. Telemanns Lieder stellen das wichtigste Bindeglied zwischen dem Liedschaffen des 17. Jahrhunderts und der Berliner Liederschule dar.

In seiner späteren Schaffensphase plante Telemann mehrere musiktheoretische Abhandlungen, darunter eine über das Rezitativ (1733) und einen "Theoretisch-practischen Tractat vom Componiren" (1735). Keine dieser Schriften ist überliefert, sodass davon ausgegangen werden muss, dass sie entweder verlorengingen oder von Telemann wieder verworfen wurden.

1739 veröffentlichte Telemann die "Beschreibung der Augenorgel", eines vom Mathematiker und Jesuitenpater Louis-Bertrand Castel entworfenen Instruments, das Telemann während seiner Paris-Reise besichtigte. Überliefert ist auch ein Stimmungssystem, an dem Telemann noch einen Monat vor seinem Tod arbeitete und bei dem er sich offenbar an Arbeiten von Johann Adolph Scheibe orientierte. Über dieses in Mizlers "Musikalischer Bibliothek" vorgestellte Neue System gab es innerhalb der Correspondierenden Societät der musicalischen Wissenschaften etliche Auseinandersetzungen, vor allem weil diese Beschreibung in musiktheoretischer Hinsicht nicht nachvollziehbar war. Telemann hatte vorgeschlagen, die Oktave in 55 gleich große Mikrointervalle zu teilen. Diese Teilung ist mit der damit verbundenen mathematischen Aufgabenstellung relativ kompliziert. Erst Georg Andreas Sorge gelang es in seiner Schrift "Ausführliche und deutliche Anweisung zur Rational-Rechnung", Telemanns System auf der Basis von Logarithmen exakt zu beschreiben. An der Lösung derartiger Fragen hatte Telemann im Gegensatz zu anderen Zeitgenossen kein Interesse, denn die Beschäftigung mit der Musikalischen Mathematik wurde im Gegensatz zum älteren Musikdenken von den Vertretern des Galanten Stils abgelehnt.

In der gesamten Geschichte der europäischen Kunstmusik war das Ansehen kaum eines Tonkünstlers einem derart radikalen Wandel unterworfen wie das von Georg Philipp Telemann.

Während Telemann zu Lebzeiten ein großes Ansehen genoss, das auch über die Ländergrenzen hinausstrahlte, schwand die Wertschätzung bereits wenige Jahre nach seinem Tod. Einen Tiefpunkt erreichte seine Anerkennung während der Romantik, als die bloße Bemängelung des Werks einer unbegründeten, auch seine Person betreffenden Diffamierung wich. Musikwissenschaftler des 20. Jahrhunderts räumten, zunächst zögerlich, auf Werkanalyse gestützten Einschätzungen mehr Raum ein und leiteten schließlich eine Wiederentdeckung Telemanns ein, die von sporadischer Kritik begleitet wird.

Neben den prestigeträchtigen Posten und Angeboten aus höfischem und städtischem Umfeld zeugen auch Quellen aus künstlerischen und populären Kreisen von Telemanns hohem, stetig wachsendem Ansehen. Während Telemann schon in Frankfurt weit über die Stadtgrenzen hinaus bekannt war, erreichte sein Ruhm in Hamburg den Höhepunkt. Beigetragen zu seiner beispiellosen Karriere hatten neben der Tatsache, dass er neue, beliebte musikalische Entwicklungen vorantrieb, auch sein Geschäftssinn und die Unverfrorenheit, die er höher gestellten Personen entgegenbrachte.

Dass Telemann eine europäische Berühmtheit war, zeigt sich beispielsweise an den Bestelllisten seiner "Tafelmusik" und seiner "Nouveaux Quatuors", in denen Namen aus Frankreich, Italien, Dänemark, der Schweiz, Holland, Lettland, Spanien und Norwegen sowie Händel (aus England) aufgeführt sind. Ebenso beweisen Einladungen und Kompositionsaufträge aus Dänemark, England, dem Baltikum und Frankreich sein internationales Ansehen. Wie ein Angebot aus Sankt Petersburg zum Aufbau einer Hofkapelle aus dem Jahr 1729 zeigt, interessierte sich auch der Hof des russischen Zaren für Telemanns Talent. Zu Aufführungs- und Studienzwecken wurden von Telemanns beliebtesten Werken allerorts Abschriften wie auch Raubdrucke angefertigt.

Bereits kurz nach Telemanns Amtsantritt in Hamburg berichtete der als „Kunstrichter“ regelmäßig publizierende Johann Mattheson, dass jener „sich bisher, der ihm beywohnenden grossen Geschicklichkeit und Arbeitsamkeit zu Folge, äuserst, und mit sehr gutem Fortgange, angelegen seyn lassen, die geistliche Music so wohl, als auch Privat-Concerte, aufs neue zu beseelen […]; also hat man auch, seit kurzem, ein fast gleiches Glück an den hiesigen Opern zu erleben angefangen“.

Geschätzt wurde neben Telemanns Ausdruckskraft und melodischem Einfallsreichtum auch sein international geprägtes Schaffen. Johann Scheibe behauptete, Johann Sebastian Bachs Werke seien „keinesfalls von solchem Nachdruck, Überzeugung und vernünftigem Nachdenken […wie diejenigen von Telemann und Graun…] Das vernünftige Feuer eines Telemanns hat auch in Deutschland diese ausländische Musikgattungen bekannt und beliebt gemacht […] Dieser geschickte Mann hat sich auch sehr oft in seinen Kirchensachen derselben mit guter Wirkung bedienet, und durch ihn haben wir die Schönheit und die Anmut der französischen Musik mit nicht geringem Vergnügen empfunden“. Auch Mizler, Agricola und Quantz lobten Telemanns Verwendung fremder Einflüsse.

Während seines Hamburger Lebensabschnitts, nachdem Händel nach England ausgewandert war, galt Telemann als bekanntester Komponist der deutschsprachigen Welt. Besondere Wertschätzung erfuhr seine geistliche Musik, die nicht nur an seinen Wirkungsstätten, sondern in vielen weiteren nord-, mittel- und süddeutschen Gemeinden, teilweise auch im Ausland, Anklang fand. Der Musikkritiker Jakob Adlung schrieb 1758, es gebe kaum eine deutsche Kirche, in der Telemanns Kantaten nicht aufgeführt würden. Einige Kirchenkantaten, die Johann Sebastian Bach im Bachwerke-Verzeichnis zugeschrieben wurden, konnten von der Forschung seither als Werke Telemanns identifiziert werden, so die Kantate BWV 141 „Das ist je gewisslich wahr“ und BWV 160 „Ich weiß dass mein Erlöser lebt“. — Friedrich Wilhelm Zachariä bezeichnete Telemann in einem Vergleich mit Bach als "„Vater der heiligen Tonkunst“". Nach einigem erfolglosem anfänglichem Widerstand fand letztendlich auch der „theatralische“ Stil des Kirchenkomponisten allgemeinen Beifall.

Zu den kritisch betrachteten Aspekten von Telemanns Schaffen gehörte die von Mattheson missbilligte musikalische Umsetzung von Natureindrücken. Anders als bei der nach dem Tode Telemanns einsetzenden Kritik an der Tonmalerei ging es Mattheson vor allem darum, die Musik als menschliche Ausdrucksform vor der Beschreibung der „unmusikalischen“ Natur zu bewahren. Die ungewohnte Harmonik wurde unterschiedlich aufgenommen, aber als Mittel zur Unterstreichung des Ausdrucks generell akzeptiert. Teilweise getadelt wurde die Komik und der Mangel an „Schamhafftigkeit“ (Mattheson) von Telemanns Opern, ebenso die damals gebräuchliche Mischung von deutschen und italienischen Texten.

Die zu Telemanns Lebzeiten vorherrschende Wertschätzung überdauerte seinen Tod nicht lange. Schon wenige Jahre danach häufte sich die Kritik an seinem Werk. Der Grund für diesen Wechsel lag im Übergang vom Barock zu einer Zeit des Sturm und Drang und der beginnenden Wiener Klassik mit dem damit einhergehenden modischen Wandel. Die Aufgabe der Musik lag nicht mehr im „Erzählen“, sondern im Ausdruck subjektiver Empfindungen. Außerdem löste sich die Bindung der Musik an bestimmte Anlässe; die sogenannte Gelegenheitsmusik wurde von Kompositionen verdrängt, die „um ihrer selbst willen“ angefertigt wurden.

Kritisch betrachtet wurden zum einen die Textvorlagen der geistlichen Musik von Telemann und anderen Kirchenkomponisten, denn auch diese hatten sich nun den modernen Regeln der Dichtung unterzuordnen. Zum anderen wurde die von Telemann besonders konsequent betriebene Umsetzung textueller Ideen wie Herzklopfen, wütendem Schmerz und ähnlichem in die Musik heftig kritisiert. Außerdem betrachtete man die komische Oper als Zeichen eines angeblichen Verfalls der Musik.

Repräsentativ für die nun vorherrschenden, veränderten Auffassungen über die Komposition und Dichtung ist folgende Aussage Gotthold Ephraim Lessings:

Weitere Kritik aus der Musikersphäre kam von Sulzer, Kirnberger, Schulz und anderen. Telemanns Ansehen schwand rapide, und andere Komponisten wie Graun, denen man einen „zärtlicheren“ Geschmack nachsagte, kamen in Mode.

1770 äußerte der Hamburger Literaturprofessor Christoph Daniel Ebeling erstmals die später sehr häufig verwendete Folgerung, aus dem enormen Umfang von Telemanns Werk ließe sich auf eine mangelnde Qualität des Opus schließen, indem er Telemanns "„schädliche Fruchtbarkeit“" mit der Begründung "„Selten hat man von Polygraphen [Vielschreibern] viele Meisterstücke“" angriff.

Telemanns weltliches und instrumentales Werk konnte sich vor den Kritikern noch einige Zeit lang behaupten, doch bald übertrug sich die Kritik auf sein gesamtes Schaffen.

Der Komponist und Musikkritiker Johann Friedrich Reichardt bemängelte, Telemanns Tonmalerei gehe mit Gefälligkeit einher:

Eine Würdigung des Werkes im Bewusstsein eines veränderten Geschmacks fand nur vereinzelt statt. John Hawkins etwa bezeichnete Telemann in seinem Werk "A General History of the Science and Practice of Music…, Volume the Fifth" (1776) als „den größten Kirchenmusiker in Deutschland“; auch Christian Friedrich Daniel Schubart rühmte Telemann ausdrücklich.

Ernst Ludwig Gerber hat in seinem bekannten Musiklexikon (1792) wenig Gutes über Telemann zu sagen. Auch er beanstandet die zu textgebundene Deklamation des „Polygraphen“. Häufig zitiert wurde Gerber später in seiner Behauptung, die beste Schaffensperiode des Künstlers liege in der Zeit von 1730 bis 1750.

Nach seinem Tod waren Telemanns Partituren in den Besitz seines Enkels übergegangen, der später nach Riga berufen wurde und dort mehrere Werke aufführte. Dabei nahm er häufig als unerlässlich empfundene Bearbeitungen – teils bis zur Unkenntlichkeit – vor, um das Schaffen seines Großvaters zu „retten“. Dennoch war das Interesse an Telemann nunmehr fast historisch; seine Werke wurden nur noch zuweilen in den Kirchen Hamburgs und einigen Konzertsälen aufgeführt. In Paris sind letzte Aufführungen bis 1775 nachzuweisen. Ab etwa 1830 bestand, abgesehen von wenigen Aufführungen, keine auf eigener Hörerfahrung basierende Kenntnis von Telemanns Werk.

Dessen ungeachtet sind einige Beispiele von Persönlichkeiten überliefert, die Interesse an Telemanns Schaffen zeigten. So erwähnte der Schriftsteller Carl Weisflog in "Phantasiestücke und Historien", dass er von einer 1827 stattfindenden vereinzelten Aufführung der "Donner-Ode" beeindruckt war.

Charakteristisch für die musikhistorischen Erwähnungen Telemanns im 19. Jahrhundert ist der Mangel an fundierter, auf den Werken basierender Analyse und die verschärfte Weiterführung bereits früher erwähnter Kritikpunkte. Vor allem Telemanns geistlichen Kompositionen warf man mangelnde Ernsthaftigkeit vor, welche man offenbar von einem deutschen Komponisten erwartete. Carl von Winterfeld betrachtete den den Werken zugrundeliegenden Text als flach und pathetisch, als "„ermüdende[s] Einerlei“". Weiterhin bezeichnete er Telemanns Werk als "„leicht und schnell hingeworfen“", den Ausdruck der geistlichen Vokalwerke als fehlerhaft und der Kirche unwürdig:

Inzwischen waren Telemanns Partituren von Georg Michael in den Besitz des Musikaliensammlers Georg Poelchau übergegangen. 1841 gelangten sie mit Poelchaus Sammlung nach dessen Tod durch Kauf zum „Musikalischen Archiv“ an der „Königlichen Bibliothek zu Berlin“, der heutigen Staatsbibliothek, wo sie der Quellenforschung zur Verfügung standen.

Bis zum Ende des 19. Jahrhunderts verschärfte sich die Wortwahl der Bemängelung von Telemann stetig; laut Ernst Otto Lindner schuf er "„keine künstlerischen Schöpfungen sondern Fabrikwaare“". Die Kritik übertrug sich auch auf seine Person; Lindner etwa verurteilte Telemann ob seiner Autobiografien und der Wahl seines anagrammatischen Pseudonyms "Signor Melante" als eitel. Weitere kritische Ansichten äußerte Eduard Bernsdorf, der Telemanns Melodien als "„sehr häufig steif und trocken“" bezeichnete; auch hier übernahmen viele andere Musikkritiker diese Formulierung.

Im 19. Jahrhundert kam es zu einem Geniekult, wobei einsam und der Zeit weit vorauseilend geglaubte Meister verherrlicht wurden; Publikumslieblinge wurden mit Skepsis betrachtet. In der Musikwelt leiteten Carl Hermann Bitter, Philipp Spitta und andere im Zuge ihrer Forschungen eine historische Wiederentdeckung Johann Sebastian Bachs ein. Damit begann auch eine Zeit der abschätzigen Bewertung vieler anderer Komponisten, ungeachtet der Tatsache, dass man, wenn überhaupt, sich nur Kenntnis eines kleinen Bruchteils des Gesamtwerks aneignete und zudem nie ernsthafte Werkanalysen durchführte. Im Falle Telemanns orientierten sich Musikwissenschaftler vor allem an den Ausführungen Ebelings und Gerbers. Einige Bach- und Händelforscher intensivierten ihre Kriterien hinsichtlich Telemanns Schaffensprinzipien, um die qualitative Differenz zu diesen Komponisten zu verdeutlichen:

Der Bachbiograf Albert Schweitzer konnte es nicht fassen, dass Bach scheinbar unkritisch ganze Kantaten von Telemann abschrieb. Spitta kam im Zuge seiner Analyse der Kantate "Ich weiß, dass mein Erlöser lebt" (BWV 160) zu folgendem Urteil: "„Was Bach daraus gemacht, ist ein wahres Kleinod an ergreifender Deklamation und herrlichem melodischen Zuge.“" Später stellte sich heraus, dass diese Kantate von Telemann komponiert wurde. Ein ähnlicher Fehltritt unterlief Schweitzer, als er sich bei der Betrachtung der Kantate "Ich lebe, mein Herze, zu deinem Ergötzen" (BWV 145) besonders vom – von Telemann stammenden – Eingangschor „So du mit deinem Munde“ beeindruckt zeigte.

Weiterhin wurde Telemann ab den 1870er Jahren Konventionalität vorgeworfen. Lindner schrieb, dass Telemann, der "„altbewährten Schule“" entstammend, eigentliche Selbständigkeit nie erreicht hätte; Hugo Riemann bezeichnete ihn als "„das Urbild eines deutschen Komponisten von Amts wegen“", der auf eine Wiederbelebung wenig Anspruch habe.

Im ausgehenden 19. Jahrhundert erreichte das Ansehen Telemanns in musikhistorischen Kreisen einen absoluten Tiefpunkt.

Die ersten Versuche der gründlicheren Auseinandersetzung mit Telemanns Werk fanden zu Beginn des 20. Jahrhunderts statt. Vor allem die intensivere Beschäftigung mit dem Quellenmaterial führte zum erneuten, zunächst fast unmerklich vonstattengehenden Wandel in der Telemann-Rezeption.

Zu den ersten Musikwissenschaftlern, die eine unvoreingenommenere Beurteilung von Telemanns Werken formulierten, zählte Max Seiffert, der 1899 bei der Analyse einiger seiner Klavierkompositionen eine eher beschreibende als wertende Haltung einnahm. 1902 würdigte Max Friedlaender Telemann, in dessen Liedern voller „witziger und pikanter Melodien“ er sich als „eigenartigen, liebenswürdigen, interessanten Componisten, der sich von der Schablone des Zeitgeschmacks gern emancipirt“ zeige. Damit behauptete er das genaue Gegenteil der häufig geäußerten Kritik an den „trockenen“ Melodien und der „Schablonenhaftigkeit“. Andererseits stellte er auch eine große Ungleichheit in seinem Werk fest. Arnold Scherings Urteil von Telemanns Instrumentalkonzerten war folgendes:

Den Grundstein für die Wiederentdeckung Telemanns lieferten aber erst die Publikationen Max Schneiders und anderer. Schneider war der erste, der die Praxis der unbegründeten Kritik an Telemann angriff und versuchte, ihn in seiner eigenen Historizität zu begreifen. Er veröffentlichte 1907 in den "Denkmälern Deutscher Tonkunst" das Oratorium "Der Tag des Gerichts" und die Solokantate "Ino". In seiner Kommentierung von Telemanns Autobiografien wies er auf den beispiellosen Wandel des Telemann-Verständnisses in den vergangenen Jahrhunderten hin. Schneider kritisierte insbesondere den Vorwurf der „Oberflächlichkeit“ des Werks und darüber angestellte „Scheinuntersuchungen“. Er forderte „‚Bonmots‘ und vages Gerede über einen Meister geflissentlich [zu] vermeiden, der zwei Menschenalter hindurch von der ganzen gebildeten Welt zu den Ersten seiner Kunst gerechnet wurde und Anspruch darauf hat, in der Geschichte der Musik die rechte Würdigung zu finden.“

Im Anschluss daran veröffentlichten Romain Rolland und Max Seiffert detaillierte Werkanalysen und Editionen von Telemanns Werken.

Vom breiten Publikum wurden diese Äußerungen allerdings vorläufig nicht wahrgenommen.

Erst nach dem Zweiten Weltkrieg begannen Arbeiten zur methodischen Erforschung von Telemanns Gesamtwerk. Im Zuge der nun häufiger erscheinenden Arbeiten über den Komponisten wandelte sich auch die musikgeschichtliche Einschätzung. 1952 stellte Hans Joachim Moser fest:

1953 gab die Gesellschaft für Musikforschung den ersten Band der Auswahlausgabe von Telemanns Werken heraus. Seit 1955 wurde dieses Projekt von der Musikgeschichtlichen Kommission e.V. unterstützt.

1961 wurde in Magdeburg der "Arbeitskreis „Georg Philipp Telemann“ e.V." gegründet, der sich hauptsächlich der Forschung widmete. Er wurde 1979 unter dem Namen "Zentrum für Telemann-Pflege und -Forschung" eine Abteilung der "Georg-Philipp-Telemann-Musikschule", die ihrerseits im September 2000 in Konservatorium Georg Philipp Telemann umbenannt wurde. 1985 wurde das Telemann-Zentrum zu einer eigenständigen Institution.

Seit 1962 veranstaltet die Stadt Magdeburg zusammen mit dem Arbeitskreis „Georg Philipp Telemann“ zweijährlich die international beachteten Telemann-Festtage, die sich mit zahlreichen Veranstaltungen und Konferenzen gleichermaßen an Musikfreunde, Musiker und Forscher wenden. Daneben verleiht die Stadt jährlich den Georg-Philipp-Telemann-Preis. In mehreren Städten bildeten sich eingetragene Vereine, die sich sowohl mit der Forschung als auch mit der Praxis befassen. Dazu gehören die "Telemann-Gesellschaften" in Magdeburg, Frankfurt und Hamburg.

Neben Werkausgaben und weiteren Publikationen gelangten auch bald Tonträger-Veröffentlichungen und Rundfunk-Übertragungen an die Öffentlichkeit. Das erste auf Schallplatte eingespielte Werk von Telemann war ein Quartett aus der "Tafelmusik", das 1935 in der französischen Reihe "Anthologie sonore" veröffentlicht wurde. Dank des Erfolgs der Langspielplatte in den 1960er Jahren und im Zuge der Entdeckung des wirtschaftlichen Potentials der Barockmusik wurden bis 1970 etwa 200 Werke von Telemann auf Tonträgern veröffentlicht, was nur einem kleinen Teil des Gesamtwerks entspricht. Auch heute noch ist seine Instrumentalmusik am besten erschlossen.

Die historische Aufführungspraxis erwies sich angesichts des entscheidenden Anteils der Instrumentation an Telemanns Werken als unerlässlich. Moderne Instrumente verzerren das vom Komponisten vorgesehene Klangbild aufgrund unterschiedlicher Klangfarben völlig, sodass die anfangs häufig praktizierte „romantische“ Aufführungspraxis eine adäquate Wiederentdeckung von Telemanns Werk beim Musikfreund verzögerte. Dem vor allem im 19. Jahrhundert gefestigten Bild des Komponisten konnte die Telemann-Pflege des 20. und 21. Jahrhunderts indes mit teilweisem Erfolg entgegenwirken.

Im März 1990 wurde der Asteroid (4246) Telemann nach ihm benannt.

Am 8. Mai 2011 wurde in Hamburg durch die Hamburger Telemann-Gesellschaft e.V. ein Museum eröffnet, das dem Komponisten gewidmet ist. Das Hamburger Museum ist das erste Telemann-Museum weltweit. Es dient der Förderung von Kultur und Bildung in Hamburg, zudem gehört es zu seinen Aufgaben, umfassendes Wissen über den Hamburgischen Director Musices, den Kantor der fünf Hauptkirchen der Jahre 1721 bis 1767 und Leiter der Hamburgischen Oper von 1722 bis 1738 zu vermitteln. Das Telemann-Museum ist in der Peterstraße 39 im selben Haus wie die Johannes-Brahms-Gesellschaft und -Museum ansässig.

Im Jahr 2013 wurde die Telemann-Stiftung gegründet, um dauerhaft und ausschließlich das Hamburger Telemann Museum zu fördern.
Vorstand: Erich Braun-Egidius (Vorsitzender), Marcus Buschka, Esther Hey, Mathias v. Marcard, François Maher Presley.










</doc>
<doc id="14645" url="https://de.wikipedia.org/wiki?curid=14645" title="Friedrich III. (Deutsches Reich)">
Friedrich III. (Deutsches Reich)

Friedrich III., mit vollem Namen Friedrich Wilhelm Nikolaus Karl von Preußen (* 18. Oktober 1831 im Neuen Palais in Potsdam; † 15. Juni 1888 ebenda), aus dem Haus Hohenzollern war in seinem Todesjahr 99 Tage lang König von Preußen und damit Deutscher Kaiser. Im Deutschen und im Deutsch-Französischen Krieg war er ein preußischer Feldherr.

Friedrich kam am 18. Oktober 1831 als Sohn des Prinzen Wilhelm von Preußen und der Augusta von Sachsen-Weimar-Eisenach zur Welt. Preußischer Kronprinz war zu diesem Zeitpunkt Wilhelms älterer Bruder, Friedrich Wilhelm. Dessen 1823 geschlossene Ehe mit Elisabeth Ludovika von Bayern war bis zu seiner Thronbesteigung 1840 kinderlos geblieben. Als König bestimmte Friedrich Wilhelm IV. seinen Bruder Wilhelm zu seinem Thronfolger unter dem Namen "Prinz von Preußen". Infolgedessen stand Friedrich ab 1840 als erstgeborener Sohn Wilhelms präsumtiv an der zweiten Stelle in der preußischen Thronfolge.

Die Ehe zwischen Wilhelm und Augusta von Sachsen-Weimar-Eisenach war eine nicht glücklich verlaufende Vernunftehe. Wilhelm war ursprünglich in die polnische Prinzessin Elisa Radziwiłł verliebt. Da eine eheliche Verbindung mit der nicht als ebenbürtig geltenden Prinzessin aus Sicht des preußischen Königshofs eine dynastische und politische Mesalliance darstellte, untersagte Wilhelms Vater, Friedrich Wilhelm III. im Juni 1826 die Verbindung. Ohne seine emotionale Bindung an Elisa Radziwiłł aufzugeben, hielt Wilhelm unter Druck seines Vaters am 29. August 1828 schriftlich um die Hand der Prinzessin aus Weimar an.

Augusta von Sachsen-Weimar-Eisenach war die zweite Tochter des Großherzogs Carl Friedrich und der Großfürstin Maria Pawlowna Romanowa, einer Schwester Zar Alexanders I. von Russland. Während ihr Vater ein schüchterner Mensch war, dessen bevorzugte Lektüre bis zum Ende seines Lebens Märchen blieben, nannte Johann Wolfgang von Goethe ihre Mutter „eine der besten und bedeutendsten Frauen ihrer Zeit“. Augusta selbst erhielt am liberal geltenden Weimarer Hof eine umfassende Bildung, die darauf ausgerichtet war, später höfische Repräsentationspflichten wahrnehmen zu können. Wilhelm von Preußen dagegen fühlte sich vorwiegend dem Militär verpflichtet und stand liberalen und nationalen Neuerungen überwiegend skeptisch und ablehnend gegenüber.

Der Rufname des Kindes war "Friedrich". Nach seiner Thronbesteigung ersetzte ihn Friedrich Wilhelm IV. durch den Doppelnamen "Friedrich Wilhelm". Zu den ersten Erzieherinnen gehörte unter anderem Marie von Clausewitz, die Witwe des 1831 verstorbenen Militärtheoretikers Carl von Clausewitz. Förmlicheren Schulunterricht erhielt Friedrich Wilhelm ab seinem fünften Lebensjahr, die Erziehung erfolgte ab diesem Zeitpunkt ausschließlich durch Männer.

Von 1838 bis 1844 war der reformierte schweizerische Theologe Frédéric Gordet Friedrich Wilhelms Zivilerzieher, der ihn und seinen Freund und Mitschüler Rudolf von Zastrow durch den Tag begleitete. Ab Oktober 1844 übernahm der Althistoriker Ernst Curtius das Amt des Zivilerziehers. Curtius wird nachgesagt, in dem preußischen Prinzen Begeisterung für Kunst und für das antike Griechenland und Rom geweckt zu haben. Zur Ausbildung gehörte entsprechend der Tradition des Hauses Hohenzollern neben Fechten, Reiten, Tanzen und Turnen auch die Vermittlung praktisch-handwerklicher Kenntnisse. Friedrich Wilhelm wurde in Tischlerei, Buchdruckerei und Buchbinderkunst unterrichtet. Schwerpunkt der Erziehung war jedoch die militärische Ausbildung. Verantwortlich dafür war bis 1849 der Oberst Karl von Unruh. Bereits im Sommer 1844 nahm der 13-jährige Prinz an so genannten Kadettenmanövern teil, bei dem er und sein drei Jahre älterer Cousin Friedrich Karl jeweils an der Spitze eines „Heeres“ standen. Im Jahre 1846 folgte die erste Teilnahme an einem wirklichen Manöver, bei dem Friedrich Wilhelm Offiziere der 1. Garde-Division begleitete.

Zu den einschneidenden Erlebnissen der Jugendjahre Friedrich Wilhelms zählt die Märzrevolution 1848.

Die Thronbesteigung Friedrich Wilhelms IV., am 7. Juni 1840 war im liberalen und patriotischen Lager mit großen Erwartungen verknüpft gewesen, da er sich während seiner Kronprinzenzeit den Ruf eines modernen, aufgeschlossenen Menschen erworben hatte. Friedrich Wilhelm IV. weigerte sich jedoch, seinem Land eine Verfassung zu geben, und regierte weit konservativer, als seine Kronprinzenjahre hatten vermuten lassen. Den Vereinigten Landtag, den Friedrich Wilhelm angesichts der am 22. April 1847 ausgebrochenen Hungerrevolte einberief und dessen Mitbestimmung sich auf finanzielle Fragen beschränkte, löste er schon wenige Monate später wieder auf.

Für die blutigen Auseinandersetzungen der Märzrevolution 1848, bei denen das Militär Kartätschen und Granaten gegen die Aufständischen einsetzte, machte die Bevölkerung Friedrich Wilhelms Vater, den Prinzen von Preußen, verantwortlich. Auf Bitten seines königlichen Bruders floh der als „Kartätschenprinz“ beschimpfte Wilhelm, mittlerweile 51 Jahre alt, nach London. Augusta von Sachsen-Weimar-Eisenach zog sich mit Friedrich Wilhelm und seiner sieben Jahre jüngeren Schwester Luise nach Potsdam zurück. In liberalen Kreisen wurde ernsthaft die Idee diskutiert, ob das Königspaar nicht abdanken, der Kronprinz auf den Thron verzichten und stattdessen Augusta, die „edle und freisinnige Fürstin“, die Regentschaft bis zur Volljährigkeit ihres Sohnes Friedrich Wilhelm übernehmen solle. Da die Briefe und Tagebücher jener Zeit später durch Augusta vernichtet wurden, ist heute nicht mehr nachvollziehbar, ob Friedrich Wilhelms Mutter diesen Plan ernsthaft erwogen hat. Im Juni 1848 konnte Wilhelm bereits wieder nach Preußen zurückkehren. Er war daher zugegen, als im September 1848 sein Sohn in der Schlosskapelle eingesegnet wurde. Wenige Monate später, am 3. Mai 1849, begann Prinz Friedrich Wilhelm seinen aktiven Militärdienst beim 1. Garderegiment zu Fuß.

Friedrich Wilhelm wurde nach Hohenzollernschem Hausgesetz mit 18 Jahren, am 18. Oktober 1849 großjährig. Der Geburtstag wurde in Schloss Babelsberg begangen und der Prinz sprach erstmals öffentlich vor Mitgliedern der königlichen Familie, den Abordnungen des Hofes, des Staatsministeriums, der Generalität und der städtischen Vertreter. Parallel wurde seine militärische Laufbahn unterbrochen, weil Friedrich Wilhelm – vermutlich auf Anregung seiner Mutter – in Bonn an der Rheinischen Friedrich-Wilhelms-Universität Bonn ein Universitätsstudium der Rechtswissenschaften aufnahm. Daneben hörte er Geschichte, Politik und Vorlesungen über die englische Verfassung. Seine Eltern lebten in dieser Zeit in der Nähe. Der Prinz von Preußen wurde 1849 zum Generalgouverneur der Rheinprovinz ernannt, und bezog im Frühjahr 1850 gemeinsam mit seiner Frau eine Residenz in Koblenz.

Friedrich Wilhelm war nicht der einzige hochadelige Student der Bonner Universität. Die 1818 von Friedrich Wilhelm III. gestiftete Universität war Anziehungspunkt vieler junger Fürstensöhne, Friedrich Wilhelm umgab sich überwiegend mit Angehörigen des Hochadels. Sein seit 1849 amtierender Militärerzieher Friedrich Leopold Fischer sorgte jedoch dafür, dass Friedrich Wilhelm zahlreiche Personen liberaler und nationaler Gesinnung kennenlernte. Unter seinen Professoren waren bekannte Nationalliberale wie Ernst Moritz Arndt und Friedrich Christoph Dahlmann. Auch Kronprinzessin Augusta empfing in Koblenz zahlreiche Personen, die liberal oder konservativ-liberal dachten. Unter ihrem Einfluss wurde auch Kronprinz Wilhelm allmählich für den Gedanken einer konstitutionellen Monarchie empfänglich, die sich nach englischem Vorbild ausrichtete.

Am 21. Januar 1851 befand er sich auf der Rückfahrt aus Berlin zu seinem Studienort. In Berlin hatte am 18. Januar 1851 die 150-Jahr-Feier der Erlangung der Königswürde durch Preußen stattgefunden. Der Prinz reiste ab Minden in einem Zug der Köln-Mindener Eisenbahn-Gesellschaft in einem reservierten Abteil der ersten Wagenklasse, als die Lokomotive des Zuges entgleiste und mehrere Wagen mit sich riss. Drei Menschen starben. Der Prinz war unter den Verletzten. Es war einer der bis dahin schwersten Eisenbahnunfälle in Deutschland.

Als 1851 in London die erste Weltausstellung stattfand, zählten Wilhelm von Preußen und Prinzessin Augusta sowie ihre beiden Kinder Friedrich Wilhelm und Luise zu den von Königin Victoria und Prinz Albert eingeladenen Gästen. Bei diesem Besuch begegnete Friedrich Wilhelm gleichzeitig zum ersten Mal der ältesten Tochter der englischen Königin. Trotz des großen Altersunterschiedes – Prinzessin Victoria war zum Zeitpunkt des Besuches elf Jahre alt, Friedrich Wilhelm dagegen 19 – verstanden sich die beiden gut. Der jungen Prinzessin war die Aufgabe übertragen worden, den Prinzen durch die Ausstellung zu führen – auf sein zögerndes Englisch antwortete sie in fließendem Deutsch. Noch Jahre später betonte Friedrich Wilhelm, wie sehr ihn die Mischung aus Kindlichkeit, intellektueller Neugier und natürlicher Würde beeindruckt habe, die sie während der Führung gezeigt habe. In Prinz Albert fand der präsumtive Thronfolger einen Gesprächspartner, der seine liberalen politischen Ansichten teilte und stärkte. Friedrich Wilhelm, der insgesamt vier Wochen in England verbrachte, war zudem von der Umgangsweise innerhalb der britischen Königsfamilie angetan. Anders als seine Eltern waren Königin Victoria und Prinz Albert einander herzlich zugetan und führten ein Familienleben, das weit entfernt von der Strenge und Förmlichkeit des preußischen Hofes war.
Nach der Rückkehr des Prinzen nach Deutschland begannen Prinzessin Victoria und Prinz Friedrich Wilhelm, einander regelmäßig zu schreiben. In einem Brief an ihren Onkel, König Leopold I. von Belgien, gab Königin Victoria der Hoffnung Ausdruck, dass sich aus dieser Begegnung mittelfristig eine engere Bindung ergeben werde.

Friedrich Wilhelm schloss Ostern 1852 seine Studienzeit ab. Dem folgte eine längere Reise an den russischen Zarenhof. Daran schloss sich eine Zeit an, in der Friedrich Wilhelm zunehmend einzelne Aspekte der Verwaltung des preußischen Staates kennenlernte. Dazu gehörten Hospitanzen im Finanz- und Handelsministerium sowie im Kriegsministerium und in der Bezirksverwaltung von Potsdam und Breslau. Schwerpunkt war jedoch unverändert die militärische Ausbildung. Während des Herbstmanövers 1853 war er dem Generaladjutanten des Königs, dem Kommandierenden General des Gardekorps Karl von der Groeben als Adjutant zugeteilt und wurde wegen seiner Tüchtigkeit am 11. September 1853 noch auf dem Paradefeld zum Major ernannt.

Auf Bitte des preußischen Königs wurde Oberst Helmuth von Moltke persönlicher Adjutant des Prinzen und damit sein wichtigster militärischer Lehrer. Dieses Amt nahm Moltke bis zu seiner Berufung an die Spitze des Großen Generalstabs im Oktober 1857 wahr. Wilhelm von Preußen und Prinzessin Augusta hatten zunächst mit ihrer Einwilligung zu dieser Ernennung gezögert, weil sie dahinter ein Spiel der Hofkamarilla sahen, die den Thronfolger enger an den regierenden König binden würde. Erst nach einem persönlichen Kennenlernen erfolgte ihre Zustimmung: Moltke war weder ausgesprochen konservativ noch unkritisch liberal, Prinzessin Augusta war außerdem von den Umgangsformen des preußischen Generalstabsoffiziers und seiner Bildung angetan.

Vier Jahre nach der Londoner Weltausstellung reiste Friedrich Wilhelm nach Schottland, um die britische Königsfamilie in ihrem Schloss Balmoral zu besuchen und sich Klarheit darüber zu verschaffen, ob Prinzessin Victoria für ihn eine geeignete Ehepartnerin sei. Seine Reise nach Großbritannien fand in preußischen Hofkreisen nicht nur Unterstützung, im Gegenteil hielten viele am Hofe eine eheliche Verbindung mit dem russischen Zarenhaus für politisch wünschenswerter. König Friedrich Wilhelm IV. hatte seine Einwilligung zu einer möglichen Ehe zwischen seinem Neffen und der britischen Prinzessin nur widerwillig gegeben und seine Zustimmung zunächst sogar vor seiner eigenen Frau geheim gehalten, die England abgeneigt war.

Bereits am dritten Tag seines Aufenthalts bat Prinz Friedrich Wilhelm Königin Victoria und Prinz Albert um die Erlaubnis, um die Hand ihrer Tochter anhalten zu dürfen. Die Zustimmung von Königin Victoria und Prinz Albert war unter anderem an die Bedingung geknüpft, dass die Hochzeit nicht stattfinden solle, bevor Victoria 17 Jahre alt sei.

Die Verlobung zwischen Prinzessin Victoria und Prinz Friedrich Wilhelm, die erst am 17. Mai 1856 bekannt gegeben wurde, stieß in der britischen Öffentlichkeit auf viel Kritik: Diese lastete Preußen seine neutrale Haltung während des Krimkriegs nach wie vor an. In einem Artikel kritisierte die britische Zeitung "Times" das Haus Hohenzollern als eine armselige Dynastie, die eine unbeständige und unglaubwürdige Außenpolitik verfolge und deren Fortbestand von Russland abhängig sei. Der Artikel bemängelte auch, dass die preußische Königsfamilie die Zusicherungen, die sie während der Revolution 1848 dem Volk gegeben habe, nicht eingehalten habe. In Deutschland war die Reaktion auf die Verlobung geteilter. Liberale Kreise begrüßten die Verbindung mit dem britischen Königshaus, während die meisten Mitglieder des preußischen Königshauses und der politisch konservativen Kreise die geplante Verbindung ablehnten.

Prinz Albert von Sachsen-Coburg und Gotha zählte zu den Liberalen des Vormärzes und war ein Anhänger des sogenannten Coburger Plans. Bereits während des unfreiwilligen Aufenthalts von Friedrich Wilhelms Vater in London im Jahre 1848 hatte Prinz Albert versucht, diesen von seiner Vision eines unter der Vorherrschaft eines liberalen Preußens vereinigten Deutschlands zu überzeugen. Nach Prinz Alberts Auffassung war dieses Ziel nur zu erreichen, wenn sich Preußen ähnlich wie das Vereinigte Königreich zu einer konstitutionellen Monarchie entwickeln würde. Die knapp zwei Jahre zwischen Verlobung und Hochzeit nutzte Prinz Albert, um seine Tochter in diesem Sinne weiterzubilden. Er unterrichtete sie persönlich in Politik und neuzeitlicher europäischer Geschichte und ließ seine Tochter Aufsätze über Ereignisse in Preußen schreiben. Prinz Albert überschätzte bei seinen politischen Instruktionen allerdings die Stärke der liberalen Bewegung in Preußen, deren Unterstützer im Wesentlichen auf eine im Vergleich zu Großbritannien kleine Mittelschicht und wenige Intellektuelle begrenzt war. Allen Beteiligten wurde jedoch zunehmend klar, welche schwierige Rolle auf die junge Prinzessin Victoria an dem gegenüber Großbritannien überwiegend kritischen preußischen Hof zukommen würde. Feodora zu Leiningen, die deutsche Halbschwester von Königin Victoria, bezeichnete in einem Brief an das Königspaar den preußischen Hof als eine Brutstätte von Neid, Eifersucht, Intrige und bösartigen Gaunereien.

Friedrich und Victoria heirateten am 25. Januar 1858 in der Kapelle des St James’s Palace in London. Zuvor hatte es noch Meinungsverschiedenheiten um den Hochzeitsort gegeben. Königin Victoria setzte als regierende Monarchin ihren Anspruch durch, ihre älteste Tochter in England zu vermählen. Das preußische Königshaus hielt es für selbstverständlich, dass der Prinz, der damals als zweiter in der Thronfolge stand, in Berlin heiratete.

Mit der Thronbesteigung seines Vaters Wilhelm I. avancierte Friedrich Wilhelm 1861 zum preußischen Kronprinzen. Von eingeschränkt liberaler politischer Gesinnung, die seine Mutter und seine Gattin förderten und unterstützten, galt er in den Folgejahren als Gegner der Innenpolitik seines Vaters und des Ministerpräsidenten Otto von Bismarck, zeigte sich allerdings in dieser Oppositionsrolle aufgrund seiner Loyalität zum Vater und Monarchen sowie aufgrund der außenpolitisch-militärischen Erfolge Bismarcks immer wieder gespalten und schwankend. Dies zeigte sich insbesondere 1863, als er sich in einer Rede vor dem Magistrat und den Stadtverordneten Danzigs von der repressiven Pressepolitik Bismarcks distanzierte, gleichzeitig aber sein Vertrauen in seinen Vater, König Wilhelm I., der den Ministerpräsidenten berufen hatte, zum Ausdruck brachte.

Nachdem Preußen am 9. Juni 1866 in das von Österreich verwaltete Holstein einmarschiert war, beantragte Österreich in Frankfurt die Mobilisierung des nichtpreußischen Bundesheeres, dem am 14. Juni stattgegeben wurde. Preußen reagierte darauf mit dem Einmarsch in Sachsen, Hannover und Kurhessen – der Beginn des sogenannten Deutschen Krieges. Danach drangen preußische Verbände immer weiter nach Süden vor, bis sich die österreichische Armee am 3. Juli bei Königgrätz der preußischen stellte. Generalstabschef Helmuth von Moltke, ein alter Freund des Kronprinzen, hatte sich entschieden, das preußische Heer in drei getrennten Armeen marschieren zu lassen. Zunächst eröffneten die Elbarmee unter Leitung von Herwarth von Bittenfeld und die erste Armee unter Leitung von Prinz Friedrich Karl Nikolaus von Preußen die Kampfhandlungen gegen die österreichische Armee, die nördlich der Festung Königgrätz Stellung bezogen hatte. Die preußischen Angriffe konnten trotz hoher Verluste zunächst keine nennenswerten Erfolge erzielen, so dass der 2. preußischen Armee unter Leitung des Kronprinzen, die sich in Gewaltmärschen dem Schlachtfeld näherte, die schlachtentscheidende Rolle zufiel. Kronprinz Friedrich Wilhelm entschied sich für einen Flankenangriff auf die kaiserlichen Streitmächte, um die zwei anderen preußischen Armeen zu entlasten. Dabei gelang es ihm, die Höhe von Chlum zu besetzen, von der aus seine Artillerie ein verheerendes Flankenfeuer gegen die österreichische Armee eröffnen konnte. Die Niederlage von Königgrätz zwang Österreich letztlich zur Kapitulation. Im Friedensschluss vom 23. August in Prag schied Österreich aus dem Deutschen Bund aus. Schleswig-Holstein, Hannover, Kurhessen, Nassau und Frankfurt wurden von Preußen annektiert.

Im Deutsch-Französischen Krieg befehligte der Kronprinz die 3. Armee. In den Anfangstagen des Krieges gewannen die von ihm geführten Truppen die Schlacht bei Weißenburg und die Schlacht bei Wörth. In der Schlacht bei Sedan kam seiner Truppe erneut eine entscheidende Rolle zu. Bis zum Ende des Krieges befehligte er mit seiner Armee einen Teil der Belagerung von Paris. Seither galt er in Deutschland als Kriegsheld und wurde zum Generalfeldmarschall ernannt. 1871 unterstützte er Bismarck bei der Erhebung seines Vaters zum „Deutschen Kaiser“, nachdem dieser sich aufgrund innenpolitischer Erwägungen zunächst widersetzt hatte. An der Kaiserproklamation in Versailles am 18. Januar 1871 nahm er teil.

Seit 1871 in der Doppelrolle als „Deutscher Kronprinz und Kronprinz von Preußen“, wurde Friedrich Wilhelm durch die Langlebigkeit seines Vaters und die Dauer-Herrschaft Bismarcks politisch immer stärker zermürbt. Lediglich nach einem Attentat auf Wilhelm I. führte der Kronprinz 1878 vorübergehend die Regierungsgeschäfte, wurde von Bismarck aber so geschickt ausmanövriert, dass er auf dessen Politik keinerlei Einfluss nehmen konnte. Nach diesem Stellvertretungs-Semester wurde er schließlich wieder in einen machtlosen Wartestand zurückgestuft.

Den Maler Anton von Werner verband mit Friedrich seit dem Deutsch-Französischen Krieg ein persönliches Verhältnis. Werner erinnerte später an die Verbindung des Kronprinzen Friedrich mit den Köpfen der Opposition gegen Bismarck in dem nebenstehenden Gemälde "Kaiser Friedrich als Kronprinz auf dem Hofball 1878", dem Jahr der Regentschaft. Das Bild zeigt den Kronprinzen im Mittelpunkt einer abgesonderten Gruppe auf dem Hofball im Berliner Schloss. Ganz links lauscht dem Gespräch der nationalliberale Abgeordnete Robert von Benda, 1878 noch ein Gegner der bismarckschen Schutzzollpolitik, rechts daneben Ernst Curtius, der liberal-humanistische Lehrer und Freund des Kronprinzen. Im Vordergrund der Gruppe diskutiert Max von Forckenbeck, an der Amtskette erkennbar als frisch gewählter Oberbürgermeister von Berlin, ein Revolutionär von 1848 und Mitgründer der Deutschen Fortschrittspartei. Deren Programm hatte 1878 eine stärkere Parlamentarisierung der Reichsverfassung und eine dem Parlament verantwortliche Regierung gefordert. Forckenbeck galt bereits 1866 als Friedrichs Favorit in der Nachfolge Bismarcks. Zwischen Forckenbeck und dem Kronprinzen steht im roten Talar des Dekans der Medizinischen Fakultät Rudolf von Virchow, ein Fortschrittler und persönlicher Feind Bismarcks. „Forchow und Wirckenbeck“, wie Bismarck die beiden spöttisch nannte, galten ihm als liberale Einflüsterer des Kronprinzen. In den Folgejahren entfernte er sie aus dem Umfeld des Thronfolgers. Zwischen diesen und dem Kronprinzen steht der politisch liberale Physiker Hermann Helmholtz. Rechts am Fenster registriert der von Werner hoch verehrte Adolph Menzel, beobachtet vom Maler Ludwig Knaus, die Szene. Werner selbst hat sich im Hintergrund rechts neben Virchow porträtiert.

Seit Januar 1887 litt Kronprinz Friedrich Wilhelm, ein starker Raucher, zunehmend an Heiserkeit, führte diese jedoch zunächst auf ein anstrengendes Manöver zurück. Der vom Leibarzt Dr. Wegner im März 1887 hinzugezogene Berliner Kehlkopfspezialist Carl Gerhardt entdeckte schließlich Knötchen am linken Stimmband, die man in einer quälenden Prozedur zunächst zu entfernen suchte. Am Stimmband tauchte allerdings bald erneut eine Geschwulst auf. Wie Gerhardt bereits am 15. Mai, vermutete auch der auf Gerhardts Wunsch ebenfalls hinzugezogene Chirurg Ernst von Bergmann am 16. Mai 1887 ein Karzinom und empfahl eine Entfernung des befallenen Gewebes durch eine Spaltung des Kehlkopfes. Otto von Bismarck intervenierte – laut Sinclair – zu diesem Zeitpunkt und sorgte dafür, dass man den – von Wegner vorgeschlagenen – englischen Laryngologen Morell Mackenzie einlud. Der ebenfalls konsiliarisch befragte Berliner Laryngologe Tobold diagnostizierte am 18. Mai ein Krebsleiden des linken Stimmbandes. Die Gewebeprobe, die der englische Arzt Mackenzie am 21. Mai dem Kronprinzen entnahm und die von Rudolf Virchow untersucht wurde, wies jedoch nicht eindeutig auf eine Krebserkrankung hin. Das Kronprinzenpaar reiste nach England, wo mit Einverständnis des deutschen Ärztekollegiums Morell Mackenzie seine Behandlung fortsetzen sollte. Gerhardt kam Ende Mai zu dem Schluss, dass Mackenzie möglicherweise eine Gewebeprobe am rechten Stimmband entnommen habe. Nachdem Kronprinz Friedrich – vertrauend auf Mackenzie und entgegen dem Rat von Ernst von Bergmann und Carl Gerhardt – eine weitere feingewebliche Diagnostik abgelehnt hatte, reiste er über Toblach, Venedig und Baveno nach Sanremo, von dessen milderem Klima Mackenzie und er sich eine Linderung seiner Beschwerden erhofften.

In der Villa Zirio fand er Quartier und wurde hier von seinem Sohn Wilhelm besucht. Nachdem Ernst von Bergmann und Carl Gerhardt öffentlich durch Mackenzie, der wie Wegner das Weiterwachsen des Kehlkopftumors verschwieg, diffamiert wurden, bat Prinz Wilhelm am 10. November alle Ärzte zu sich ins Hotelzimmer. Von geringen Abweichungen abgesehen kamen sie zur selben Diagnose. Einer aus ihrer Mitte, der Wiener Professor Leopold Schrötter von Kristelli, informierte den Patienten über seinen Zustand, wobei er das Wort "Krebs" vermied und stellte ihn vor die Wahl Exstirpation oder Tracheotomie. Friedrich Wilhelm entschied sich für Letzteres, falls es nötig wäre.

Vorübergehend besserte sich sein Zustand wieder, bis er in der Nacht vom 8. auf den 9. Februar 1888 mit Erstickungsanfällen zu kämpfen hatte. Er verlangte nun nach der Tracheotomie, die am 9. Februar unter schwierigsten Bedingungen im Verlauf von zwanzig Minuten durch von Bergmanns Oberarzt Friedrich Gustav von Bramann durchgeführt wurde. Er konnte nun wieder atmen, war aber völlig stumm.

Nach dem Tod seines Vaters, Kaiser Wilhelms I., wurde er zurückgerufen und traf zwei Tage danach in Berlin ein.

Als Friedrich Wilhelm durch den Tod seines Vaters am 9. März 1888 König von Preußen und damit Deutscher Kaiser wurde, nahm er, wie schon angekündigt, seinen ursprünglichen Rufnamen "Friedrich" an. Er war bereits so schwer an Kehlkopfkrebs erkrankt, dass er nicht mehr sprechen konnte. Seine nur dreimonatige Regentschaft („99-Tage-Kaiser“) endete bereits im Jahr der Thronbesteigung und machte mit der Thronbesteigung seines Sohnes Wilhelm II. (1888–1918) das Jahr 1888 zum Dreikaiserjahr. Die kurze Zeit seiner Regentschaft verbrachte er bis auf die letzten zwei Wochen im Schloss Charlottenburg. Volkstümlich wurde ihm der Spruch „Lerne leiden, ohne zu klagen!“ zugeschrieben.

Mit der Zählung als Friedrich "III." hatte er als Kaiser die Zählung der preußischen Könige übernommen. Er selbst wollte sich ursprünglich in der Tradition des Heiligen Römischen Reiches Deutscher Nation in Anknüpfung an dessen Kaiser Friedrich III. "Friedrich IV." nennen, musste aber nach einer verfassungsrechtlichen Belehrung durch Bismarck darauf verzichten.

Aufgrund seiner schweren Erkrankung konnte Friedrichs Plan, die Macht des Monarchen und des Reichskanzlers stärker an die Verfassung zu binden, nicht in die Tat umgesetzt werden. Unterdessen war Friedrichs wichtigste Entscheidung als preußischer König die Entlassung des konservativen preußischen Innenministers Robert von Puttkamer infolge der Affäre um dessen unbotmäßige Einmischung bei der Preußischen Landtagswahl 1885.

Friedrich III. empfing noch am 13. Juni König Oskar von Schweden, bevor er zwei Tage später, am 15. Juni 1888, im Neuen Palais in Potsdam starb. Sein Leichnam wurde am 18. Juni zunächst in der Sakristei der Friedenskirche beigesetzt. Nach der Fertigstellung des Mausoleums, dessen feierliche Einweihung am 18. Oktober 1890 stattfand, wurde der Verstorbene in die Gruft des Mausoleums umgebettet. Der Berliner Bildhauer Reinhold Begas schuf das Sarkophagdenkmal mit der Liegefigur des Kaisers, das in der Rotunde des Mausoleums aufgestellt wurde.

Der englische Kehlkopfspezialist Morell Mackenzie soll – laut seinem Biographen R. Scott Stevenson – gewusst haben, dass Friedrich neben Kehlkopfkrebs auch Syphilis gehabt habe, mit der er sich 1869 bei einer Spanierin, die er bei der Eröffnung des Suezkanals kennen gelernt hatte, infiziert habe. Doch habe Mackenzie aus „Loyalität“ gegenüber der englischen Königin Victoria und deren Tochter Victoria, der Ehefrau Friedrichs, sein Wissen für sich behalten. Die Behauptung Mackenzies war schon 1888 in der französischen Presse erschienen und ist kürzlich anhand der Reiseroute des Prinzen widerlegt worden. Durch den Verlauf von Friedrichs Erkrankung kann die Syphilis auch als Todesursache weitgehend als widerlegt gelten.

Kronprinz Friedrich Wilhelm wurde 1853 durch seinen Vater in die Freimaurerei eingeführt und in die Große Landesloge der Freimaurer von Deutschland aufgenommen. Gleichzeitig wurde er Ehrenmitglied der beiden anderen altpreußischen Großlogen Große National-Mutterloge „Zu den 3 Weltkugeln“ und Große Loge von Preußen genannt „Royal York zur Freundschaft“. Am 18. Juni 1860 wurde er Ordensmeister der Großen Landesloge und übernahm ab 1861 von seinem Vater das Protektorat über die drei Großlogen in Berlin. Zugleich wurde er Vorsitzender des Großmeistervereins. Der Kronprinz wirkte intensiv auf eine Vereinigung aller freimaurerischen Körperschaften in Deutschland hin und strebte eine umfangreiche Reform der Großen Landesloge an, in der nicht haltbare Verbindungen zum Templerorden aus Symbolik und Ritual entfernt werden sollten, die zu diesem Zeitpunkt noch als historische Fakten angenommen wurden. Die Reform sollte zur Streichung aller rituellen Inhalte führen, die nicht historisch durch Dokumente belegbar waren. Außerdem sollte der Orden in seinen höheren Graden deutlich umstrukturiert werden, um ihn den anderen deutschen Großlogen anzupassen. Mit den konservativen Brüdern in der Ordensleitung kam es zum Streit, so dass er am 7. März 1874 sein Amt niederlegte. Die Templerlegende als historische Tatsache wurde gestrichen, die rituellen und symbolischen Inhalte aber größtenteils behalten. Er blieb aber Protektor der altpreußischen Großlogen.

Friedrich III. galt als die „liberale Hoffnung“ Preußens und des Deutschen Kaiserreiches nach 1871, die durch seine späte Thronbesteigung und seinen frühen Tod zunichtegemacht worden sei („Kaiser-Friedrich-Legende“, vergleiche Kaiserin Friedrich, Deutsche Freisinnige Partei und Franz August von Stauffenberg). Es ist jedoch unklar, wie liberal die Politik dieses zwischen preußischer Militärtradition und liberalen Ansichten schwankenden Monarchen tatsächlich gewesen wäre. Wegen des schlechten Gesundheitszustandes des Kaisers und der sich daraus ergebenden Rücksichtnahmen fanden Personalveränderungen, bis auf die Entlassung Puttkamers, kaum statt.

Vielmehr muss davon ausgegangen werden, dass es zu keiner weitergehenden Liberalisierung des Reiches gekommen wäre, auch wenn Friedrich eine längere Lebenszeit vergönnt gewesen wäre. Weder war er ein Anhänger des Parlamentarismus, noch vom liberalen Glauben an den politischen Fortschritt erfüllt. Bereits als Kronprinz hatte er sich als konservativer Konstitutionalist erwiesen, dem nicht an einer Weiterentwicklung der Reichsverfassung – etwa hin zu einem stärkeren Parlament – gelegen war. Liberale Hoffnungen knüpften sich vor allem an eine 1862 in Danzig gehaltene Ansprache Friedrichs, in der er sich von beschlossenen Restriktionen die (liberale) Presse betreffend distanzierte, jedoch tat er dies wohl weniger aus prinzipieller Sorge um die Pressefreiheit, sondern weil er das mit einer Brüskierung des Parlaments verbundene Zustandekommen dieser Beschlüsse verurteilte und eine Entfremdung zwischen Herrscherhaus und Bevölkerung befürchtete. (Sein Vater untersagte ihm daraufhin weitere derartige Äußerungen.) Als er 1878 Stellvertreter seines beim zweiten Attentat verwundeten Vaters war, war er von der Notwendigkeit des in diesem Jahr verabschiedeten Sozialistengesetzes überzeugt, achtete aber darauf, dass es dabei zu keinem Verfassungsbruch kam. Friedrich bezeichnete den im 19. Jahrhundert auch in Deutschland aufkommenden Antisemitismus als „Schmach für Deutschland“. Nach seinem Tod stilisierten die Regierung und die Liberalen den Kaiser zu einem Vertreter des Liberalismus, mit dem das Deutsche Reich ein liberaler Parlamentarismus nach britischem Vorbild hätte werden können – was heute als Mythos gilt.

Friedrich glaubte, dass ein Herrscher niemals gegen die Volksmeinung regieren sollte.

Friedrich Wilhelm wurde – auch entsprechend seiner langen Kronprinzenzeit – gemeinsam mit seiner Frau Victoria als Förderer von Wissenschaft, Kunst und Kultur rezipiert. Unter anderem war er mit dem Aufbau der Museen auf der Berliner Museumsinsel betraut. Zu Ehren Friedrichs III. wurde denn auch das 1904 eröffnete "Kaiser-Friedrich-Museum" nach ihm benannt. Am 1. März 1956 benannte der DDR-Kulturminister Johannes R. Becher das Museum dann allerdings nach seinem Gründer und ersten Direktor Wilhelm von Bode in Bode-Museum um.

Friedrich III. war verheiratet mit Victoria von Großbritannien und Irland, Prinzessin von Großbritannien und Irland.



Einer Ehrung Friedrichs III. als Kaiser durch ein Denkmal stand entgegen, dass er nur 99 Tage regiert hatte. So empfahl 1889 der Reichskanzler und preußische Ministerpräsident Bismarck eine derartige Ehrung nur dann, wenn ihr am Ort eine Kaiser Wilhelms I. vorangegangen war oder einen erkennbaren Bezug zu seiner Rolle als Feldherr in den Einigungskriegen aufwies.


Thomas Mann schuf ihm ein zweifelhaftes Denkmal im "Zauberberg", in dem stillen, mehr reagierenden als agierenden Joachim Ziemßen, dem Vetter des Protagonisten Hans Castorp (er benannte Joachim nach dem Kaiser-Biografen Ludwig Ziemssen). 1970 strahlte das ZDF das Biopic „Friedrich III. ‚gestorben als Kaiser‘“ von Rudolf Nussgruber aus.





</doc>
<doc id="14646" url="https://de.wikipedia.org/wiki?curid=14646" title="Skalarprodukt">
Skalarprodukt

Das Skalarprodukt (auch inneres Produkt, selten Punktprodukt) ist eine mathematische Verknüpfung, die zwei Vektoren eine Zahl (Skalar) zuordnet. Es ist Gegenstand der analytischen Geometrie und der linearen Algebra. Historisch wurde es zuerst im euklidischen Raum eingeführt. Geometrisch berechnet man das Skalarprodukt zweier Vektoren formula_1 und formula_2 nach der Formel
Dabei bezeichnen formula_4 und formula_5 jeweils die Längen (Beträge) der Vektoren. Mit formula_6 wird der Kosinus des von den beiden Vektoren eingeschlossenen Winkels formula_7 bezeichnet.
Das Skalarprodukt zweier Vektoren gegebener Länge ist damit null, wenn sie senkrecht zueinander stehen, und maximal, wenn sie die gleiche Richtung haben.

In einem kartesischen Koordinatensystem berechnet sich das Skalarprodukt zweier Vektoren formula_8 und formula_9 als
Kennt man die kartesischen Koordinaten der Vektoren, so kann man mit dieser Formel das Skalarprodukt und daraufhin mit der Formel aus dem vorhergehenden Absatz den Winkel formula_11 zwischen den beiden Vektoren ausrechnen, indem diese nach formula_7 aufgelöst wird: 

In der linearen Algebra wird dieses Konzept verallgemeinert. Ein Skalarprodukt ist dort eine Funktion, die zwei Elementen eines reellen oder komplexen Vektorraums einen Skalar zuordnet. Im Allgemeinen ist in einem Vektorraum von vornherein kein Skalarprodukt festgelegt. Ein Raum zusammen mit einem Skalarprodukt wird als Innenproduktraum oder Prähilbertraum bezeichnet. Diese Vektorräume verallgemeinern den euklidischen Raum und ermöglichen damit die Anwendung geometrischer Methoden auf abstrakte Strukturen.

Vektoren im dreidimensionalen euklidischen Raum oder in der zweidimensionalen euklidischen Ebene kann man als Pfeile darstellen. Dabei stellen Pfeile, die parallel, gleich lang und gleich orientiert sind, denselben Vektor dar. Das Skalarprodukt formula_14 zweier Vektoren formula_1 und formula_2 ist ein Skalar, das heißt eine reelle Zahl. Geometrisch lässt es sich wie folgt definieren:
Bezeichnen formula_17 und formula_18 die Längen der Vektoren formula_1 und formula_2 und bezeichnet formula_21 den von formula_1 und formula_2 eingeschlossenen Winkel, so ist
Wie bei der normalen Multiplikation, aber seltener als dort, wird das Multiplikationszeichen manchmal auch weggelassen, wenn klar ist, was gemeint ist:
Statt formula_26 schreibt man in diesem Fall gelegentlich auch formula_27

Andere übliche Notationen sind formula_28 und formula_29

Um sich die Definition zu veranschaulichen, betrachtet man die orthogonale Projektion formula_30 des Vektors formula_2 auf die durch formula_1 bestimmte Richtung und setzt
Es gilt dann formula_34 und für das Skalarprodukt von formula_1 und formula_2 gilt:
Diese Beziehung wird manchmal auch zur Definition des Skalarprodukts verwendet.

In allen drei Beispielen gilt formula_38 und formula_39. Die Skalarprodukte ergeben sich mithilfe der speziellen Kosinuswerte formula_40, formula_41 und formula_42:
Führt man in der euklidischen Ebene bzw. im euklidischen Raum kartesische Koordinaten ein, so besitzt jeder Vektor eine Koordinatendarstellung als 2- bzw. 3-Tupel, das meist als Spalte geschrieben wird.

In der euklidischen Ebene erhält man dann für das Skalarprodukt der Vektoren
die Darstellung

Für die kanonischen Einheitsvektoren formula_46 und formula_47 gilt nämlich:
Daraus folgt (unter Vorwegnahme der weiter unten erläuterten Eigenschaften des Skalarproduktes):
Im dreidimensionalen euklidischen Raum erhält man entsprechend für die Vektoren
die Darstellung
Zum Beispiel berechnet sich das Skalarprodukt der beiden Vektoren
wie folgt:

Aus der geometrischen Definition ergibt sich direkt:
Als Funktion, die jedem geordneten Paar formula_74 von Vektoren die reelle Zahl formula_14 zuordnet, hat das Skalarprodukt folgende Eigenschaften, die man von einer Multiplikation erwartet:
Die Eigenschaften 2 und 3 fasst man auch zusammen zu: Das Skalarprodukt ist bilinear.

Die Bezeichnung „gemischtes Assoziativgesetz“ für die 2. Eigenschaft verdeutlicht, dass dabei ein Skalar und zwei Vektoren so verknüpft werden, dass die Klammern wie beim Assoziativgesetz vertauscht werden können.
Da das Skalarprodukt keine "innere" Verknüpfung ist, ist ein Skalarprodukt von drei Vektoren nicht definiert, daher stellt sich die Frage nach einer echten Assoziativität nicht. Im Ausdruck formula_88 ist nur die erste Multiplikation ein Skalarprodukt von zwei Vektoren, die zweite ist das Produkt eines Skalars mit einem Vektor (S-Multiplikation). Der Ausdruck stellt einen Vektor dar, ein Vielfaches des Vektors formula_87 Hingegen stellt der Ausdruck formula_90 ein Vielfaches von formula_1 dar. Im Allgemeinen gilt also
Weder die geometrische Definition noch die Definition in kartesischen Koordinaten ist willkürlich. Beide folgen aus der geometrisch motivierten Forderung, dass das Skalarprodukt eines Vektors mit sich selbst das Quadrat seiner Länge ist, und der algebraisch motivierten Forderung, dass das Skalarprodukt die obigen Eigenschaften 1–3 erfüllt.

Mit Hilfe des Skalarproduktes ist es möglich, aus der Koordinatendarstellung die Länge (den Betrag) eines Vektors zu berechnen:

Für einen Vektor formula_43 des zweidimensionalen Raumes gilt
umgeformt werden. Daher lässt sich auch in allgemeinen reellen Vektorräumen mittels
der Winkel formula_7 zweier Vektoren definieren. Der so definierte Winkel liegt zwischen 0° und 180°, also zwischen 0 und formula_97 Für Winkel zwischen komplexen Vektoren gibt es eine Reihe unterschiedlicher Definitionen.

Auch im allgemeinen Fall nennt man Vektoren, deren Skalarprodukt gleich Null ist, orthogonal:

Ist formula_99 ein formula_100-dimensionaler Vektorraum und formula_101 eine Basis von formula_102 so kann jedes Skalarprodukt formula_103 auf formula_99 durch eine (formula_105)-Matrix formula_106 die "Gramsche Matrix" des Skalarprodukts, beschrieben werden. Ihre Einträge sind die Skalarprodukte der Basisvektoren:
Das Skalarprodukt lässt sich dann mit Hilfe der Basis darstellen:
Haben die Vektoren formula_110 bezüglich der Basis formula_111 die Darstellung
so gilt im reellen Fall
Bezeichnet man mit formula_115 die Koordinatenvektoren
so gilt also
wobei das Matrixprodukt eine formula_119-Matrix liefert, also eine reelle Zahl. Mit formula_120 wird der Zeilenvektor bezeichnet, der durch Transponieren aus dem Spaltenvektor formula_121 entsteht.

Im komplexen Fall gilt entsprechend
wobei der Überstrich komplexe Konjugation bezeichnet und formula_123 der zu formula_121 adjungierte Zeilenvektor ist.

Ist formula_111 eine Orthonormalbasis, das heißt, gilt formula_126 für alle formula_127 und formula_128 für alle formula_129 so ist formula_130 die Einheitsmatrix, und es gilt
im reellen Fall und
im komplexen Fall.
Bezüglich einer Orthonormalbasis entspricht das Skalarprodukt von formula_133 und formula_134 also dem Standardskalarprodukt der Koordinatenvektoren formula_121 und formula_136 bzw. formula_137





</doc>
<doc id="14647" url="https://de.wikipedia.org/wiki?curid=14647" title="Wilhelm II. (Deutsches Reich)">
Wilhelm II. (Deutsches Reich)

Wilhelm II., mit vollem Namen Friedrich Wilhelm Viktor Albert von Preußen (* 27. Januar 1859 in Berlin; † 4. Juni 1941 in Doorn, Niederlande) aus dem Haus Hohenzollern, war von 1888 bis 1918 letzter Deutscher Kaiser und König von Preußen. Wilhelm war ein Enkel Kaiser Wilhelms I. und ein Sohn Kaiser Friedrichs III. Dieser regierte nur 99 Tage, so dass im „Dreikaiserjahr“ 1888 auf einen 90-jährigen und einen 57-jährigen Herrscher der 29-jährige Wilhelm II. folgte. Durch seine Mutter Victoria von Großbritannien und Irland war Wilhelm Enkel der britischen Königin Victoria.

Mit seiner traditionellen Auffassung vom Kaisertum zeigte Wilhelm – aus der Sicht der jüngeren Geschichtswissenschaft – zu wenig Verständnis für die Anforderungen einer modernen konstitutionellen Monarchie. Erst im Oktober 1918, unter dem Druck des sich für Deutschland und seine mit ihm verbündeten anderen Mittelmächte als verloren abzeichnenden Ersten Weltkriegs, stimmte Wilhelm den Oktoberreformen zu, denen zufolge der Reichskanzler formell das Vertrauen des Reichstags benötigte.

Nach Beginn der Novemberrevolution verkündete Reichskanzler Max von Baden am 9. November 1918 die Abdankung Wilhelms und dessen Sohnes, Kronprinz Wilhelm von Preußen. Bereits seit dem 29. Oktober hielt sich der Kaiser im deutschen Hauptquartier im belgischen Spa auf. Er ging von dort ins Exil in die nahen Niederlande, wo ihm Königin Wilhelmina Asyl gewährte und 1919 die von den Entente-Mächten verlangte Auslieferung als „Kriegsverbrecher“ ablehnte. Vom niederländischen Doorn aus bemühte sich Wilhelm II. erfolglos um eine Restauration der Monarchie in Deutschland. Er starb dort 1941 im Alter von 82 Jahren und wurde in einem Mausoleum im Park des Hauses Doorn beigesetzt.

Die dreißigjährige Herrschaft Wilhelms II. im Deutschen Reich (von 1888 bis 1918) wird auch als die wilhelminische Epoche bezeichnet. Wesentliches Merkmal war das Streben des Kaisers, das Reich als politische Größe unter den bestehenden Weltmächten zu sichern. Eng verbunden mit diesem Anspruch war die militärische Aufrüstung des Kaiserreichs sowie die Forcierung der Kolonialpolitik in Afrika und der Südsee. Dies und die Verwicklung Deutschlands in internationale Krisen – zum Beispiel die Geschehnisse um die Krüger-Depesche 1896, der Doggerbank-Zwischenfall 1904, die Marokkokrisen 1904–1906 und 1911 sowie die Daily-Telegraph-Affäre 1908 – führten zu einer Destabilisierung der Außenpolitik.

Die Vorliebe Wilhelms für militärischen Prunk, die sich beispielsweise in zahlreichen Paraden zu den unterschiedlichsten Anlässen ausdrückte, führte auch gesellschaftlich zu einer Überbetonung des Militärs und der militärischen Hierarchie bis hinein ins zivile Leben der deutschen Gesellschaft, in der für eine berufliche Laufbahn – nicht nur im Verwaltungsapparat – die Ableistung des Militärdienstes und der militärische Rang eines Menschen von entscheidender Bedeutung war (Militarismus). Einen Rang als Reserveoffizier innezuhaben galt im wilhelminischen Bürgertum als Eintrittskarte in die „bessere Gesellschaft“; ebenso war das Fehlen eines militärischen Ranges ein Karrierehindernis.

Der wirtschaftliche Aufschwung Deutschlands während Wilhelms Herrschaft, verbunden mit technologischem, naturwissenschaftlichem und industriellem Fortschritt, begünstigte eine auch vom Kaiser mitgetragene, allgemein verbreitete Technik- und Fortschrittsgläubigkeit. Innenpolitisch setzte er die für ihre Zeit als modern und fortschrittlich geltende Sozialpolitik Bismarcks fort und erweiterte sie. Er setzte sich für die Abschaffung des Sozialistengesetzes ein und suchte, teilweise erfolglos, den Ausgleich zwischen ethnischen und politischen Minderheiten.

Wilhelm II. wollte sowohl die Innen- als auch Außenpolitik des Reiches wesentlich stärker beeinflussen als sein Großvater Wilhelm I. Das „persönliche Regiment“ des Kaisers war jedoch oft eine von häufig wechselnden Beratern gesteuerte Politik, die die Entscheidungen Wilhelms – auch im Urteil der meisten Historiker – oft widersprüchlich und letztlich unberechenbar erscheinen ließen. Die Marokkokrisen und die Erklärung des unbeschränkten U-Boot-Krieges sind nur zwei Beispiele für Entscheidungen, die den Ruf des Kaisers heute nachhaltig belasten.

Auch war seine Amtszeit von politischen Machtkämpfen zwischen den einzelnen Parteien geprägt, die es den amtierenden Kanzlern schwer machten, längerfristig im Amt zu bleiben. So wurden im Kampf zwischen dem sogenannten nationalliberal-konservativen Kartell (beziehungsweise dem Bülow-Block) und den Sozialdemokraten fünf von sieben Kanzlern unter kritischem Mitwirken des Reichstags vom Kaiser entlassen.

Während des Ersten Weltkriegs von 1914 bis 1918 wurde Wilhelms strategische und taktische Unfähigkeit offenbar. Ab 1916 enthielt er sich zunehmend relevanter politischer Entscheidungen und gab die Führung des Reiches faktisch in die Hände der Obersten Heeresleitung, namentlich in die der Generale von Hindenburg und Ludendorff, die die konstitutionelle Monarchie während der letzten Kriegsjahre mit starken Zügen einer Militärdiktatur versahen. Als sich Wilhelm II. infolge der Novemberrevolution, die zum Ende der Monarchie und zur Ausrufung der Weimarer Republik führte, zur Abdankung bewegen ließ und in die Niederlande ins Exil ging, hatte das Deutsche Kaiserreich den „Großen Krieg“ bereits verloren. Etwa 10 Millionen Menschen waren auf den Schlachtfeldern gefallen.

Als ältester Sohn des Prinzen Friedrich Wilhelm von Preußen und dessen Frau Victoria, die 1861 zum Kronprinzenpaar wurden, war Wilhelm Enkel der britischen Königin Victoria (1819–1901) sowie infolge der Verbindung seiner Großtante Charlotte mit Nikolaus I. von Russland auch ein Onkel dritten Grades des Zaren Nikolaus II.

Bei der Geburt des Prinzen im Berliner Kronprinzenpalais waren, wie bei Thronfolgergeburten üblich, hohe Beamte anwesend, um die Geburt zu bezeugen. Doch es gab Komplikationen: Er kam als Steißgeburt zur Welt und überlebte nur durch den als Ultima Ratio hinzugezogenen Direktor der Entbindungsanstalt im Charité-Krankenhaus Berlin, Prof. Eduard Arnold Martin, und durch das couragierte Eingreifen einer Hebamme, die das scheinbar leblose Baby ganz gegen das Protokoll mit einem nassen Handtuch schlug. Professor Martin musste die seit Stunden verschleppte Geburt voranbringen und wendete dafür das in dieser Anwendung neuartige Narkosemittel Chloroform an. Er drehte den Thronfolger intrauterin und schaffte es, die Beine voranzubringen, so dass das Gesäß und der Unterleib hervortraten. Der Nabelschnurpuls war fast nicht mehr fühlbar, daher musste der Geburtsvorgang beschleunigt werden. Es gelang Martin noch, den linken Arm zu wenden und parallel zum Torso zu legen, um dann mit kräftigem Zug den Kopf mit dem noch hochgeschlagenen rechten Arm zu entbinden. Infolge der stundenlangen fruchtlosen Wehen und der dann zügig zu bewerkstelligenden Notentbindung (ein Kaiserschnitt hatte damals häufig den Tod der Mutter zur Folge, was in diesem Falle völlig indiskutabel war) überlebte der Säugling zwar, aber es kam zu einer linksseitigen Armplexus-Lähmung. Einige Tage danach bemerkte man, dass das Kind diesen Arm nicht bewegen konnte. Der Arm blieb fortan in seiner Entwicklung deutlich zurück und war im Erwachsenenalter deutlich kürzer als der rechte und nur eingeschränkt beweglich. Es bleibt bis heute umstritten, ob Prof. Martin dem Kind das Leben rettete oder die Behinderung zu verantworten hatte.

Keinen gesunden Thronfolger geboren zu haben, empfand Victoria als persönliches Versagen, und sie war nur schwer bereit, die Behinderung des Sohnes zu akzeptieren. Kaum etwas blieb unversucht, seine Behinderung zu beheben. Legendär sind Kuren wie das Einnähen des kranken Armes in ein frisch geschlachtetes Kaninchen oder Metallgerüste, die Wilhelm umgeschnallt wurden, um seine Haltung zu verbessern. Auf Fotografien versuchte man, die körperliche Behinderung dadurch zu kaschieren, dass der linke Arm auf dem Säbelkorb ruhte oder im Ärmel versteckt wurde. Wilhelm, von Geburt derart behindert, verbrachte laut eigenen Aussagen „eine recht unglückliche Kindheit“.

Wie im Hochadel üblich, traten seine Eltern als unmittelbare Erzieher ganz hinter seinem calvinistischen Lehrer Georg Ernst Hinzpeter zurück. Als Siebenjähriger erlebte er den Sieg über Österreich 1866 mit der daraus resultierenden Vorherrschaft Preußens in Deutschland. Mit zehn Jahren, im damals üblichen Kadettenalter, trat er beim "1. Garde-Regiment zu Fuß" formell als Leutnant in die preußische Armee ein. Als Zwölfjähriger wurde er mit der Gründung des Deutschen Kaiserreiches nach dem Sieg über Frankreich 1871 auch zweiter Anwärter auf den deutschen Kaiserthron.

Nach dem Abitur am Friedrichsgymnasium in Kassel trat er am 9. Februar 1877 seinen wirklichen Militärdienst bei seinem Regiment, der 6. Kompanie unter Hauptmann von Petersdorff, an. 1880 wurde er am 22. März, dem Geburtstag seines Großvaters Kaiser Wilhelm I., zum Hauptmann befördert. Bereits in diesen Jahren bildete sich bei ihm ein Verständnis seiner monarchischen Rolle, das den liberal-konstitutionellen Vorstellungen seiner Eltern zuwiderlief.

Seine folgenden Lebensstationen sind unter dem Aspekt einer Erziehung zum Monarchen zu sehen: Er sollte möglichst vielerlei Erfahrungen sammeln, erhielt aber in keinem Feld, nicht einmal im militärischen, die Chance, sich beruflich solide einzuarbeiten. Zum Studium von vier Semestern von Oktober 1877 bis 1879 bezog er die von seinem Urgroßvater gegründete Rheinische Friedrich-Wilhelms-Universität Bonn. 1878 wurde er Corpsschleifenträger der Borussia Bonn. 1881 heiratete er Prinzessin Auguste Viktoria von Schleswig-Holstein-Sonderburg-Augustenburg (1858–1921), auch als ein dynastischer Versöhnungsakt gegenüber dem von Preußen um seine Herrschaft über Schleswig-Holstein gebrachten Herzogshaus.

Bis 1888 war er dann wechselnden Regimentern zugeordnet, dem "1. Garde-Regiment zu Fuß", dann dem "Garde-Husaren-Regiment" und dem "1. Garde-Feldartillerie-Regiment", wurde schnell bis zum untersten Generalsrang (Generalmajor) befördert und zuletzt Kommandeur der "2. Garde-Infanterie-Brigade". Der Militärdienst wurde immer wieder durch Beurlaubungen unterbrochen, damit er sich auch soweit möglich mit der zivilen Verwaltung vertraut machen konnte. Sehr gründlich konnte dies nicht geschehen, denn immer mehr Eile war geboten: Sein Großvater stand im höchsten Alter, und sein Vater war mittlerweile todkrank.

Für die Regierungsgeschäfte war dies weniger problematisch, da bereits seit 1862 Otto von Bismarck, zunächst als preußischer Ministerpräsident, ab 1871 als Reichskanzler, die politische Macht fest in seiner Hand konzentriert hatte. Bismarck war nach drei siegreichen Kriegen (1864, 1866, 1870/71) und als Vereiniger Deutschlands zur stärksten kontinentaleuropäischen Macht ein weltweit respektierter Staatsmann. Wilhelm I. und Friedrich III. hatten ihm gelegentlich widersprochen, aber am Ende stets vertraut. Von diesem Vertrauen hing nach der Reichsverfassung der Reichskanzler auch ab, nicht vom Vertrauen des Reichstags. Bismarck baute selbstbewusst darauf, auch den dritten Kaiser lenken zu können.

Das Jahr 1888 ging als Dreikaiserjahr in die Geschichte ein. Nach dem Tod Wilhelms I. am 9. März 1888 regierte Friedrich III. auf Grund seines bereits fortgeschrittenen Kehlkopfkrebses nur für 99 Tage (der „99-Tage-Kaiser“) und starb am 15. Juni in Potsdam.

Aussagen wie dieses Zitat und die Februarerlasse (1890) Wilhelms weckten in den ersten Jahren seiner Regentschaft in der Arbeiterschaft zeitweilig Hoffnungen auf einen sozialen Wandel im Reich. Die Sozialpolitik lag Wilhelm II. durchaus am Herzen. Allerdings folgten seinen sozialen Reformen keine strukturellen Veränderungen im Reich. Im Gegenteil, er baute seinen politischen Einfluss noch aus und lehnte eine Demokratisierung der Verfassung ab. Preußen behielt das seit Anfang der 1850er Jahre bestehende undemokratische Dreiklassenwahlrecht, das eine repräsentative Landtagsvertretung verhinderte. Nach wie vor wurde die Regierung nicht vom Reichstag gewählt, sondern vom Kaiser ohne Berücksichtigung der parlamentarischen Verhältnisse bestimmt oder entlassen. Es war dem Kanzler aber nicht möglich, ohne Mehrheit im Parlament Gesetze zu erlassen oder den Haushalt zu beschließen.

Noch während Bismarcks Kanzlerschaft, am 178. Geburtstag Friedrichs des Großen, verkündete Kaiser Wilhelm II. in einer Proklamation an sein Volk die Devise (dt. „Ich will ein König der Bettler sein“) und forderte das Verbot der Sonntagsarbeit, der Nachtarbeit für Frauen und Kinder, der Frauenarbeit während der letzten Schwangerschaftsmonate sowie die Einschränkung der Arbeit von Kindern unter vierzehn Jahren. Außerdem forderte er bei dem zur Erneuerung anstehenden („Sozialistengesetz“) die Streichung des Paragraphen, der es der Landespolizeibehörde erlaubte, „Verurteilten“ den „Aufenthalt in bestimmten Bezirken und Ortschaften“ zu versagen. Bismarck kommentierte dies als und verweigerte sich dem (in seinen Forderungen durch den Reichstag unterstützten) Kaiser. Seine Forderungen konnte der junge Kaiser erst mit Leo von Caprivi, dem Nachfolger Bismarcks, verwirklichen. Allerdings war Wilhelm II. bei allen sozialen Ambitionen so wenig ein Freund der Sozialdemokratie, wie Bismarck es gewesen war. Er hoffte, durch seine Reformen die Sympathien für die trotz der Sozialistengesetze erstarkte Sozialdemokratie zu schwächen und durch die Aufhebung des repressiven Sozialistengesetzes der 1890 von SAP in SPD umbenannten Partei ihren Märtyrerbonus zu nehmen.

Die Sozialdemokraten ihrerseits ließen sich nicht von den Reformen Wilhelms II. beeindrucken und setzten unter August Bebel aus ihrem antimonarchistischen Selbstverständnis heraus weiter auf Fundamentalopposition. Obwohl sie den Fortschritt der im Arbeitsschutzgesetz zusammengefassten Reformen sahen, stimmten sie im Reichstag dagegen. Sie forderten grundlegende strukturelle Veränderungen, wie zum Beispiel eine Verfassungsänderung, Demokratisierung, ein ausgeweitetes Wahlrecht, Vorrang des Parlaments bei politischen Entscheidungen, eine Umstrukturierung des Haushalts, deutliche Senkung der Rüstungsausgaben, Freiheit für die Kolonien und anderes mehr – für den Kaiser unerfüllbare Anliegen, die seine Abneigung gegen die Sozialdemokratie stärkten.

Der Wohlstand der deutschen Arbeiterschaft stieg von Jahr zu Jahr, doch gelang es Wilhelm II. nicht, den Arbeitern in den Städten das Gefühl zu geben, anerkannte Mitglieder der Gesellschaft zu sein. Bei vielen Reichstagswahlen und Landtagen wuchs der Stimmenanteil der SPD.

Diese Vorgänge ließen in Wilhelm II., der immer noch „ein König der Armen“ sein wollte, die These reifen, dass eine Versöhnung mit den Sozialdemokraten nicht möglich sei. Er rief schließlich in Königsberg auf. Schon 1887 hatte er, noch als Prinz, mit seiner Gemahlin den "Evangelischen Kirchlichen Hilfsverein für Berlin" gegründet, weil er glaubte, durch Förderung der Kirchen die „soziale Frage“ lösen zu können; dem folgte 1890 der Evangelische Kirchenbau-Verein, Berlin, mit dessen Hilfe er auch außerhalb Berlins auf Kirchneubauten im Reich Einfluss nahm (etwa auf die Erlöserkirche in Bad Homburg). Zugleich manifestierte er damit seine Vorstellung einer neuen Verbindung von „Thron und Altar“ in Fortführung einer Linie von Konstantin dem Großen über Otto den Großen zu ihm selbst.
In der letzten Periode der Regierungszeit Bismarcks hatte das Deutsche Reich einer „Kanzlerdiktatur“ geglichen, deren politische Ziele nicht die des jungen Kaisers waren. Bismarck wollte Russland als einen starken Verbündeten, Wilhelm II. vertraute hingegen nur auf Österreich-Ungarn. Bismarck wollte den „Kulturkampf“ gegen den politischen Katholizismus fortsetzen, der Kaiser war strikt dagegen. Bismarck wollte das Sozialistengesetz verschärfen, Wilhelm II. es abschaffen: Als der Reichskanzler hartnäckig blieb, schickte der Kaiser am Morgen des 17. März 1890 den Chef seines Militärkabinetts, General von Hahnke, in die Reichskanzlei: Der Kanzler solle am Nachmittag ins Schloss kommen und sein Abschiedsgesuch mitbringen. Dieses wurde dem Kaiser aber erst im Verlauf des nächsten Tages durch einen Boten überbracht. Bismarcks – immer auch als Rechtfertigung und Gegenangriff zu lesende – Darstellung betont das Entwürdigende der Maßnahme. Bismarck schreibt im erst postum erschienen dritten Band seiner Memoiren, dass er sich im Kabinett schon vor der Entlassung isoliert oder gar verraten gefühlt habe und dass sein Stellvertreter Karl Heinrich von Boetticher in seiner Abwesenheit und ohne seine Billigung mit dem Kaiser in dessen Sinne verhandelt habe, so dass er genötigt gewesen sei, eine 38 Jahre alte Kabinettsorder Wilhelms I. heranzuziehen, die es preußischen Ministern untersagte, ohne Billigung des Ministerpräsidenten mit dem Souverän zu sprechen. Mit Bismarcks Entlassung machte der Kaiser den Weg frei zu seinem persönlichen Regiment.

Am 20. März 1890 entließ Wilhelm II. den „eisernen Kanzler“. Bismarck akzeptierte dies innerlich nie und sorgte indirekt durch vielfach lancierte Kritik an den „Hintermännern“ der wilhelminischen Politik und durch sein Memoirenwerk "Gedanken und Erinnerungen" für nachhaltige Kritik an Wilhelm II. Deren dritter Band, in dem Bismarck seine Entlassung darstellte, wurde wegen seiner politischen Brisanz erst 1919 veröffentlicht. Der Rücktritt Bismarcks war damit zwar primär innenpolitisch begründet, aber langfristig gesehen vor allem außenpolitisch fatal. Aus Wien erinnerte Kaiser Franz Joseph I. eingedenk des 1866er Friedens von Wien in einem Brief sofort und explizit an Bismarcks Verdienste. Als Bismarcks Nachfolger ernannte Wilhelm II. General Leo von Caprivi, der vom Kaiser als gefeiert und ob seiner Leistungen in den Grafenstand erhoben wurde. Mit Caprivi glaubte Wilhelm II. eine anerkannte Persönlichkeit gefunden zu haben, mit der er seine geplante Politik der inneren Versöhnung sowie das Arbeitsschutzgesetz durchzusetzen hoffte.

Ein wichtiges außenpolitisches Ereignis fiel (gleichsam „genau passend“) in das Jahr des Kanzlerwechsels. Der Rückversicherungsvertrag mit Russland widersprach teilweise den Bedingungen des Dreibundpaktes mit Italien und Österreich-Ungarn. Der Kaiser war gegen ein Verletzen des letztgenannten Paktes, während Bismarck den Rückversicherungsvertrag seinerzeit für unbedingt notwendig gehalten hatte. Jetzt ging es um seine Verlängerung. Von der Öffentlichkeit unbemerkt (es handelte sich um einen Geheimvertrag) und von Caprivi hingenommen, wurde der 1890 auslaufende Rückversicherungsvertrag vom Deutschen Reich bewusst nicht erneuert. In Russland nahm man realistischerweise einen deutschen Kurswechsel an und begann sich Frankreich anzunähern.

Caprivis Kanzlerzeit war durch entschiedene Englandfreundlichkeit geprägt. Innenpolitisch war er einer der Hauptverantwortlichen für den Wandel des Deutschen Reiches von der Agrarwirtschaft zur industriellen Exportwirtschaft. Die Reformen in diesem Zeitraum trugen dazu bei, dass Deutschland wenig später Großbritannien überholen und zur Weltwirtschaftsmacht Nr. 1 aufsteigen konnte. Der Begriff „Made in Germany“ wurde zu dieser Zeit ein Synonym für höchste Qualität.

Die turbulente Ersetzung des alten Deutschen Bundes durch das neu geschaffene Deutsche Reich ohne die deutschen Österreicher – die kleindeutsche Lösung – brachte einige Probleme mit sich. Die rheinländische, süddeutsche und polnische Opposition gegen die preußische Vorherrschaft stützte sich auf das sich politisierende katholische Bürger-, Arbeiter- und Bauerntum. Als Partei des politischen Katholizismus hatte sich im Jahr 1870 die Deutsche Zentrumspartei formiert. Die Versuche Bismarcks, die katholischen Parteien in ihrer Arbeit zu behindern, führten zu Eingriffen in das Leben der Katholiken. Auch die Judenintegration, die es vorher außer in Preußen nur in wenigen anderen Staaten gab, war jung, und der merkliche soziale Aufstieg der jüdischen Bevölkerung nährte Neid und Antisemitismus in der Bevölkerung. In den östlichen Gebieten Preußens, vor allem in der Provinz Posen, gab es eine starke Unterdrückung der polnischen Minderheit, die zu Unruhen und Gefühlen der Ungerechtigkeit führte. Der Kaiser erkannte die Ernsthaftigkeit dieser Probleme und zähle sie zu seinen Hauptaufgaben.

Am besten gelang die Integrationspolitik gegenüber den Katholiken. Sie waren zuvor durch den Bismarckschen Kulturkampf benachteiligt und an der Teilnahme am politischen Leben sowie an der freien Ausübung ihrer Religion gehindert worden. Schon zu seiner Prinzenzeit war Wilhelm gegen diese Praktiken und befürwortete die Beendigung des Kulturkampfes. Um die Einigkeit zwischen Protestanten und Katholiken im Reich zu verbessern, zahlte das Reich die den Opfern vorenthaltenen Gelder zurück, hob allerdings nicht alle gefassten Beschlüsse und Gesetze aus dem vorangegangenen Kulturkampf wieder auf.

Die östlichen Provinzen Preußens (Ostpreußen, Westpreußen und Schlesien) waren damals mehrheitlich von Deutschen, minderheitlich von Polen sowie regional von Kaschuben und Masuren bewohnt. In der Provinz Posen stellten die Polen die Mehrheit. Seit der Bismarckzeit versuchte der Staat, die hier lebenden Polen zu germanisieren, was allerdings misslang und in offenen Protest mündete. Kaiser Wilhelm II. hob viele dieser Repressionen auf, die vor allem die Unterrichtssprache und später auch die des Gottesdienstes regelten, und erkannte die Polen als eigenes Volk und Minderheit im Deutschen Reich an.

Einer der umstrittensten Bereiche in der Einordnung der politischen Meinung des Kaisers ist seine Beziehung zum Judentum und zum Antisemitismus. Die Juden waren im 1871 gegründeten Deutschen Reich freie und gleiche Bürger geworden. Die Einschränkungen, die sie, von Land zu Land unterschiedlich, teils zu Schutzbefohlenen eines Herrschers machten und ihnen wirtschaftliche Beschränkungen auferlegten oder ihnen bestimmte Berufsverbote erteilten, waren mit der Reichsgründung für ganz Deutschland einheitlich aufgehoben worden. Auch der Dienst bei Militär, in Schulen oder der Justiz stand ihnen jetzt offen. Wilhelm II. pflegte engen und freundschaftlichen Kontakt zu vielen prominenten Juden. Zu den später von Chaim Weizmann teils anerkennend, teils verächtlich „Kaiserjuden“ genannten Politikern, Industriellen, Bankiers und Intellektuellen gehörten Albert Ballin, James Simon, Emil und Walther Rathenau, Eduard Arnhold sowie Carl Fürstenberg. Die Einschätzungen der Historiker gehen hier weit auseinander:

Im Anschluss an die „Gründerkrise“ und wohl auch als Reaktion auf die wirtschaftlichen Erfolge jüdischer Unternehmer verstärkte sich der Antisemitismus im Deutschen Reich und führte zur Bildung der „Berliner Bewegung“. Infolge der Reichstagswahl 1893 zogen zum ersten Mal Vertreter antisemitischer Parteien in den Reichstag ein. Mit 16 Abgeordneten bildeten sie eine antisemitische Fraktion.

Bei seiner Integrationspolitik kam Kaiser Wilhelm II. der Parlamentarismus im Reich entgegen. Die Wahl wurde in Einmannwahlkreisen mit absolutem Mehrheitswahlrecht durchgeführt. So besaßen die Dänen (ein bis zwei Abgeordnete), Elsass-Lothringer (acht bis 15 Abgeordnete) und Polen (13 bis 20 Abgeordnete) von 1871 bis zur letzten Wahl 1912 stets eigene Fraktionen im Reichstag. Juden organisierten sich nicht in einer eigenen Partei. Dies hätte ihrem damaligen Selbstverständnis als deutsche Staatsbürger widersprochen, das besonders in Preußen durch lange Tradition sehr stark ausgeprägt war. Das Wahlsystem grenzte aber auch politische Minderheiten nicht aus. Dies sorgte dafür, dass sich auch die preußenfeindlichen Welfen, aber vor allem die Antisemiten aus der Christlichsozialen Partei und der Deutschen Reformpartei organisieren konnten. Die Zahl ihrer Abgeordneten überschritt aber nie die Zahl der Abgeordneten aus den Parteien der ethnischen Minderheiten.

Trotz dieser Unterstützung gibt es von Wilhelm II. mehrere Zitate, die einen antisemitischen Ton haben, so die während der Novemberrevolution gefallene Äußerung: Ob er allerdings auf die Juden als Kollektiv schimpfte oder einzelne meinte, wie die ihn oft kritisch betrachtenden jüdisch geleiteten Zeitungskonzerne, ist unklar. In einem Brief an seinen amerikanischen Freund Poultney Bigelow schrieb Wilhelm II. am 15. August 1927: 

Caprivi setzte einen weiteren von Bismarck verwehrten Wunsch Wilhelms durch, die progressive Einkommensteuer, die höhere Einkommen stärker belastete: die Miquelsche Einkommensteuerreform von 1891. Durch die industriefreundliche und exportorientierte Eindämmung des Protektionismus zog sich Caprivi die Feindschaft der im Bund der Landwirte organisierten Grundbesitzer („Ostelbier“, „Junker“) zu, die eng mit der Konservativen Partei verbunden waren. Die nach Abschaffung der Schutzzölle wachsenden Agrarexporte der USA bewirkten für sie einen Preisverfall. Durch die Förderung des Einsatzes von Landmaschinen konnte man die Verluste zwar teils auffangen, erhöhte aber die agrarprotektionistischen Ansprüche der ohnehin unterkapitalisierten und zu Investitionen genötigten Großgrundbesitzer.

1893 löste Wilhelm II. den 1890er Reichstag auf, weil der die auch von ihm gewünschte Aufrüstung des Heeres abgelehnt hatte. Im darauf folgenden Wahlkampf siegten die Befürworter der wilhelminischen Politik aus der Konservativen und Nationalliberalen Partei. Auch die gegen Caprivis Widerstand von Alfred von Tirpitz propagierte Aufrüstung der Kaiserlichen Marine, im Volk durchaus populär, erkennbar etwa am allgegenwärtigen Matrosenanzug für Knaben, wurde in der Folgezeit von Wilhelm gefördert.

Im Januar 1894 kam es zu einem Aussöhnungstreffen mit Otto von Bismarck. Als Bismarck 1896 den geheimen Rückversicherungsvertrag mit Russland in der Presse veröffentlichte, wollte Wilhelm diesen aber in der ersten Erregung wegen Landesverrats verhaften und in die Zitadelle Spandau verbringen lassen.

Am 26. Oktober 1894 wurde Caprivi entlassen. Wilhelm berief erstmals einen Nichtpreußen, den bayerischen Fürsten (und seinen Onkel, wie er in seinen Memoiren "Ereignisse und Gestalten" schreibt) Chlodwig zu Hohenlohe-Schillingsfürst. Er sollte anders als seine beiden Vorgänger keinen Führungsehrgeiz entwickeln.

1895 wurden der Kaiser-Wilhelm-Kanal, der heutige Nord-Ostsee-Kanal, fertiggestellt und die Marinehäfen Kiel und Wilhelmshaven in großem Maßstab ausgebaut. In diesem Zusammenhang besetzte und pachtete das Deutsche Reich die chinesische Hafenstadt Tsingtao auf 99 Jahre. Wilhelm erkannte trotz seiner Englandfreundlichkeit nicht, dass damit die weltweite Hegemonialmacht Großbritannien aufs Äußerste beunruhigt wurde. Der anhaltende deutsche Kolonialismus – gegen den Bismarck und Caprivi sich noch gewehrt hatten – wurde von ihm nicht als riskant gegenüber den Großmächten England, Frankreich und Japan erkannt und gebilligt: 1899 erwarb das Reich die Karolinen, Marianen, Palau und 1900 Westsamoa. 1896 versäumte Hohenlohe-Schillingsfürst es, Wilhelm von der „Krüger-Depesche“ abzuhalten, einem Glückwunschtelegramm an die Buren zur Abwehr des britischen Jameson Raid, die in Großbritannien mit Empörung aufgenommen und nachhaltig als Abkehr von der englandfreundlichen Politik Caprivis gedeutet wurde. In seinen Memoiren betonte Wilhelm, dass er gegen die Depesche gewesen sei, aber vom Kanzler Hohenlohe zur Unterschrift genötigt worden sei.

1900 ersetzte Wilhelm Hohenlohe durch Graf Bernhard von Bülow, der als Reichskanzler weder die anstehenden innenpolitischen Reformen betrieb noch die sich neu gruppierenden außenpolitischen Konstellationen zu meistern vermochte, die in Deutschland zunehmend als „Einkreisungspolitik“ empfunden wurden. Das Verhältnis zu Frankreich wurde jedenfalls nicht verbessert, England nun auch durch die Flottenpolitik herausgefordert und Russland auf dem Balkan nicht gegen die Österreichisch-Ungarische Monarchie unterstützt. Wilhelm vertraute Bülow, der ihm nachhaltig zu schmeicheln wusste, lange, bis zur Daily-Telegraph-Affäre 1908 und den Eulenburg-Prozessen.

Mit Ausbruch des russisch-japanischen Kriegs im Februar 1904 und dem Abschluss der Entente Cordiale zwischen Frankreich und Großbritannien am 8. April 1904 veränderte sich das europäische Machtgefüge fundamental. Mit dem englisch-französischen Kolonialausgleich war die Freihandelspolitik offenbar gescheitert. In der Wilhelmstraße wurde überlegt, wie man auf die französisch-britische Annäherung reagieren sollte, ohne selbst an politischem Handlungsspielraum zu verlieren und außenpolitisch isoliert zu werden. Nach den schweren Niederlagen Russlands im Sommer 1904 und den scharfen Spannungen zwischen London und St. Petersburg nach dem Doggerbank-Zwischenfall (21./22. Oktober 1904) wurde Russland als ein möglicher Partner weiter interessant.

Im November 1904 unterbreitete Wilhelm dem Zaren Nikolaus II. ein Defensivbündnis. Frankreich sollte erst nach Abschluss des Vertrages von dem Bündnis in Kenntnis gesetzt werden. Die russische Regierung widersetzte sich aber einem solchen Bündnis. In der Ersten Marokkokrise (1904–1906) standen bald darauf wieder die Spannungen zwischen Frankreich und Deutschland im Fokus. Friedenspolitisch ergriff Wilhelm II. im Juli 1905 eine Initiative: Im Sinne einer Wiederannäherung an Russland, das gerade seinen Krieg gegen Japan zu verlieren drohte, schloss er mit Nikolaus II. den Freundschaftsvertrag von Björkö. Frankreich sollte einbezogen werden.

Der Vertrag von Björkö wurde allerdings schon 1907 von Russland für gegenstandslos erklärt, weil er mit der französisch-russischen Annäherung, die inzwischen stattgefunden hatte, nicht vereinbar war. Diese Annäherung hatte sich ergeben, nachdem Wilhelm II. im März 1905 in der Ersten Marokkokrise Tanger besucht hatte (Näheres hier). Resultat war überdies eine Verschlechterung der Beziehungen zu Japan, das Preußen–Deutschland bisher als wissenschaftlichen und militärischen Lehrmeister angesehen hatte.

1908 wurde Wilhelms Hilflosigkeit durch die "Daily-Telegraph"-Affäre deutlich: Er beschwerte sich in einem Interview mit der Zeitung über seine eigene Regierung – sie sei nicht englandfreundlich genug. Bismarck war ein Meister darin gewesen, seine Politik medial zu flankieren. Bei Wilhelm II. dagegen sollten das Interview und markige Reden die Politik ersetzen. Ein besonders eklatantes Beispiel hatte der Kaiser mit der bereits am 27. Juli 1900 in Bremerhaven gehaltenen Hunnenrede gegeben. Mit dem Interview im "Daily Telegraph" fiel er nunmehr der Reichspolitik in den Rücken, indem er darin erklärte, er sei ein guter „Beschützer Englands“, hielte er doch die anderen europäischen Mächte immer davor zurück, England zu provozieren. Dies wurde in England als Ärgernis empfunden: Es lasse sich von niemandem beschützen und empfand das Interview als Anmaßung. Wilhelm knickte angesichts des deutschen Pressesturms ein und versprach, sich künftig außen- wie auch innenpolitisch zurückzuhalten.

Inzwischen hatte die öffentliche Meinung begonnen, den Kaiser grundsätzlich kritisch zu sehen, und eine Kampagne schadete ihm konkret: Schon 1906 hatte der Journalist Maximilian Harden in seiner Zeitschrift "Die Zukunft" die Kamarilla um den Kaiser und damit das „persönliche Regiment“ des Kaisers angegriffen. Zu besonders harten Auseinandersetzungen führte seine Enthüllung, dass Philipp zu Eulenburg, ein enger Freund und Berater des Kaisers, homosexuell sei (was damals noch strafbar war) und einen Meineid geleistet habe, als er dies leugnete. Es folgten drei Sensationsprozesse gegen Eulenburg, die trotz Freisprüchen das Ansehen des Kaisers beschädigten.

1909 zerbrach der sogenannte Bülow-Block, in dem sich die regierungsunterstützenden linksliberalen Parteien sowie die Nationalliberale und die Konservative Partei zusammengeschlossen hatten. Auslöser war der Versuch Bülows, das preußische Wahlrecht zu reformieren, worauf ihm die im preußischen Landtag dominierenden Konservativen die Gefolgschaft verweigerten. Sozialdemokraten und Zentrumspartei, die diesen Versuch in seinen Grundsätzen unterstützen, verweigerten trotzdem die Zusammenarbeit mit Bülow. Sie warfen ihm Prinzipienlosigkeit vor, da er erst kurz zuvor in Zusammenarbeit mit den Konservativen neue Repressalien gegen die polnische Minderheit durchgesetzt hatte. Die Germanisierungspolitik wurde auf Betreiben Kaiser Wilhelms eingeschränkt. Dass Bülow nun aber, um sich die Loyalität der Konservativen Partei zu sichern, die Enteignung von polnischen Gütern erleichterte, ignorierte der Kaiser zunächst, um die stabile Parlamentsmehrheit nicht zu gefährden.

Doch entließ er Bülow und ernannte Theobald von Bethmann Hollweg zum Reichskanzler. Er überließ ihm die Außenpolitik, die aber ihre Ziele – Wiederannäherung an England und Distanzierung von der antirussischen Balkanpolitik Österreich-Ungarns – nicht erreichte. Die antifranzösische Politik wurde 1911 in der zweiten Marokkokrise durch deutschen Interventionismus im „Panthersprung nach Agadir“ verschärft. Heer und Flotte wurden weiter verstärkt. Markante Eingriffe Wilhelms unterblieben. Der Kaiser war zwar Militarist, aber kein Bellizist, er wollte trotz seiner kriegerischen Reden im Grunde keinen Angriffs- oder Präventivkrieg. Er tat aber auch wenig, um dies deutlich zu machen.

Insgesamt ist Wilhelms II. Anteil an der deutschen Außenpolitik umstritten. Während John C. G. Röhl in ihm eine wirkungsmächtige Instanz hervorhebt, die in die Politik des Reiches eigenständig eingriff, sieht die Mehrzahl der Historiker wie Wolfgang J. Mommsen die zivile Reichsleitung im Zentrum der Verantwortung. Unbestreitbar ist, dass der Kaiser nicht als Koordinator zwischen Außen-, Heeres- und Flottenpolitik wirkte. So kam es, dass Reichskanzler, Heeres- und Marineleitung jeweils unterschiedliche Ziele verfolgten, die miteinander nicht vereinbar waren. Vor allem der Aufbau der Flotte schuf ein außenpolitisches Problem.

In der Julikrise 1914 spielte Wilhelm II. eine ambivalente Rolle. Er versuchte einerseits, den Frieden durch einen fieberhaften Briefwechsel mit dem russischen Zaren („Lieber Nicky!“ – „Lieber Willy!“) zu retten, der bei der nunmehr objektiven Kriegsentschlossenheit sämtlicher Kontinental-Großmächte gar nichts bewirkte. Andererseits drängte er zum Losschlagen. Faktisch steigerte der Kaiser letztlich die Kriegsgefahr, denn er ermächtigte Bethmann Hollweg nach dem Attentat von Sarajewo am 28. Juni 1914, Österreich-Ungarn eine Blankovollmacht für dessen aggressive Politik gegen Serbien zu erteilen.

Obwohl die Stärke Deutschlands immer mehr zugenommen hatte, hielt Wilhelm, mit seinen Ängsten vor „Sozialismus“, „gelber Gefahr“, „slawischer Flut“ und seiner Idee vom „unvermeidlichen Gegensatz von Slawen und Germanen“, die Zeit für die letzte Abrechnung gekommen. Dabei unterschätzte er den serbienfreundlichen Panslawismus, mit dem seit 1905 die russische Politik die Unruhen im eigenen Reich zu bändigen fest entschlossen war. Der deutsche Botschafter in Wien Heinrich von Tschirschky drängte auf Wilhelms Anweisung zu einer Aktion gegen Serbien: Er solle „mit allem Nachdruck erklären, daß man in Berlin eine Aktion gegen Serbien erwarte und daß es in Deutschland nicht verstanden würde, wenn wir die gegebene Gelegenheit vorübergehen ließen, ohne einen Schlag zu führen“.

Faktisch wurde nach der österreichisch-ungarischen Kriegserklärung an Serbien die Außenpolitik von Kaiser und Kanzler dem deutschen Generalstab überlassen: Die Mobilmachung im Russischen Reich erlaubte es nach dem Urteil der Generalität dem Deutschen Reich nicht, mit der Kriegserklärung an Russland und Frankreich länger zu warten, da sonst der deutsche Schlieffen-Plan, bei einem Zweifrontenkrieg erst schnell Frankreich, dann Russland zu schlagen, undurchführbar zu werden drohte. Wilhelm mischte sich in der Folge nicht in militärische Zielsetzungen ein, überließ diese aber nicht verfassungsgemäß dem Reichskabinett, sondern der Obersten Heeresleitung (OHL).

Im Verlauf des Ersten Weltkrieges 1914–1918 wurde die Bedeutung des Kaisers immer geringer. Besonders mit der Dritten Obersten Heeresleitung unter Hindenburg und dem dominierenden Ludendorff wurde er 1916–1918 zunehmend von den politisch-militärischen Entscheidungen ausgeschlossen. Jedoch schob die Heeresleitung ihm 1917 die auch im Reich umstrittene Entscheidung über die Wiederaufnahme des nach dem „Lusitania-Zwischenfall“ 1915 eingestellten „uneingeschränkten“ U-Boot-Kriegs zu. Er schloss sich – gegen den Rat seines Reichskanzlers – der Meinung der Militärs an, was im April 1917 zur Kriegserklärung der USA führen sollte. Diese machten später die Abdankung des Kaisers zur Bedingung für die Eröffnung von Friedensverhandlungen. Ab 1917 hatte Ludendorff eine faktisch diktatorische Position. Auf weitere Reichskanzlerwechsel nahm Wilhelm II. keinen Einfluss, die 1918er Reform der Reichsverfassung in Richtung auf eine parlamentarische Monarchie wurde ohne ihn versucht. Die „stille Diktatur der OHL“ war auch durch die Schwäche Kaiser Wilhelms bedingt, der in den beiden letzten Kriegsjahren immer hilfloser agierte, was die Position der OHL stärkte.

Am 13. Mai 1917 präsentierte Wilhelm II. seinem Staatssekretär für Äußeres ein Kriegszielprogramm, das die Bestrafung aller Gegner, sogar der USA, in Form von Reparationen vorsah. Neben ausgedehnter kolonialer Expansion – Malta, Zypern, Ägypten, Mesopotamien sollten an das Osmanische Reich fallen, Madeira, die Kapverden, Azoren und der Kongo an Deutschland – erwartete er die Anbindung von Polen, Kurland, Litauen, Ukraine, Livland und Estland an sein Reich. Außerdem forderte er unrealistische Kriegsentschädigungen von allen Kriegsgegnern.

Allerdings stand Wilhelm II. gerade in dieser Zeit eher im Hintergrund, er hatte selten ein entscheidendes Wort mitzureden, so dass sein Programm in Kreuznach nicht sehr ernst genommen wurde und nur, was den kolonialen Bereich betraf, überhaupt in der politischen Planung berücksichtigt wurde. Im Rahmen einer Balkanreise begeisterte sich der Kaiser über die reichen Gebiete Rumäniens. Das eroberte Land hatte ihm „außerordentlich gefallen“, „bei guter Verwaltung würde das Land zu einer Quelle größten Reichtums werden“.

1918 autorisierte er den Plan, Russland nach Abtretung Polens, des Baltikums und des Kaukasus in vier unabhängige „Zarentümer“ zu teilen, nämlich "Zentralrussland", "Sibirien", die "Ukraine" sowie einen "Südostbund" als antibolschewistisches Gebiet zwischen der Ukraine und dem Kaspischen Meer. Diese Form der Beherrschung hätte eine „Brücke nach Zentralasien zur Bedrohung der britischen Stellung in Indien“ ergeben. Der Plan eines „Südostbundes“ stand dabei in Konkurrenz zu osmanischen Absichten. Kanzler Hertling, der Livland und Estland „in gewisser Ferne als freundschaftlich uns angeschlossene Staaten“ bezeichnete, wurde von Wilhelm zurückgewiesen: „Unsinn! Das Baltikum ist eins, und ich werde sein Herr und dulde keinen Widerspruch, Ich habe es erobert und kein Jurist kann es mir nehmen!“

Wilhelm sah sein protestantisches Kaisertum, vor allem im Gegensatz zum Haus „Habsburg-Parma“, zunehmend als seine Sendung an:

Der Zentrumspolitiker Matthias Erzberger, der diesen Interessen diene, sei „ein schurkenhafter Verräter, der unschädlich gemacht werden muß.“

Nach der gescheiterten Frühjahrsoffensive im Westen 1918, den Erfolgen der Westalliierten an der Westfront und dem drohenden Zusammenbruch des verbündeten Österreich-Ungarn verlangte die Oberste Heeresleitung am 28. September 1918, ein Waffenstillstandsgesuch an die Kriegsgegner zu richten und zugleich die Regierung des Deutschen Reichs auf eine breitere Grundlage zu stellen.

In mehreren diplomatischen Noten machte US-Präsident Woodrow Wilson die Gewährung des Waffenstillstands indirekt von einer Abdankung des Kaisers abhängig. Die USA weigerten sich, vorher Friedensverhandlungen aufzunehmen. Da sie infolge von Wilsons 14-Punkte-Programm als die gemäßigtste der kommenden Siegermächte galten, fand seine Forderung Widerhall in Deutschland.

Am 16. Oktober 1918 empfahl die Fortschrittliche Volkspartei Wilhelm II. die freiwillige Abdankung. Reichskanzler Prinz Max von Baden betrieb diese seit dem 28. Oktober; am Tag darauf reiste Wilhelm auf Anraten insbesondere Friedrich von Bergs von Berlin nach Spa (Belgien). Er residierte dort im La Fraineuse und versuchte eine Pendeldiplomatie zwischen sich und der OHL (deren Sitz im Hotel Britannique war). In Anbetracht der Stimmung im Volk und der Meinung des Kabinetts hielt Wilhelm die Armee noch am ehesten für loyal. Diese Hoffnungen zerschlugen sich im Laufe des Kieler Matrosenaufstands und der Novemberrevolution. Um radikaleren Forderungen der Revolutionäre die Spitze zu nehmen, verlangten auch die Mehrheitssozialdemokraten ab dem 7. November den Rücktritt von Kaiser und Kronprinz. Am Tag darauf sprach sich auch die Zentrumspartei für die Abdankung aus.

Der zu diesem Zeitpunkt politisch paralysierte Monarch sah sich nun mit drei Optionen konfrontiert. General Groener vertrat, auch gestützt auf das Ergebnis einer Befragung von 39 Generälen und Regimentskommandeuren, die Auffassung, das Heer sei nicht mehr in der Hand der Befehlshaber; ein militärisches Vorgehen gegen die Revolution sei zwar wünschenswert, aber vorerst unmöglich, insbesondere mit dem Kaiser an der Spitze. Groeners Analyse, die implizit nahelegte, dass der Kaiser verschwinden müsse, wurde – nach dem Krieg eine ständige Quelle der Verlegenheit – de facto von Hindenburg gedeckt und fand in Paul von Hintze und Werner Freiherr von Grünau zwei energische Fürsprecher, die auch die „Holland-Lösung“ ins Gespräch brachten. Eine andere Gruppe um General Friedrich Graf von der Schulenburg, Stabschef der Heeresgruppe Deutscher Kronprinz, hielt dagegen einen „Marsch auf Berlin“, also die militärische Zerschlagung der Revolution, für durchführbar. Dieser Position neigte zunächst auch Wilhelm zu. Die dritte Möglichkeit wurde von der militärischen Entourage des Kaisers nur in Andeutungen ausgesprochen: Der Monarch solle sich „nach vorn“, also zur Front begeben, um dort den Tod zu suchen. Eine solche Geste würde, so die Spekulation vor allem jüngerer Generalstabsoffiziere, einen völligen Meinungsumschwung zugunsten der Dynastie bzw. der Monarchie als Institution herbeiführen. Vorbereitungen für ein derartiges Unternehmen hatten Groener und Major Joachim von Stülpnagel, der Chef der Operationsabteilung der OHL, bereits getroffen.

Die letzte, von den Ereignissen bereits überholte Initiative Wilhelms war der am späten Vormittag des 9. November gefasste Entschluss, zwar als Kaiser, nicht aber als preußischer König abzudanken. Die Revolution hatte mittlerweile Berlin erfasst. Während in Spa an einer Abdankungsurkunde gearbeitet wurde, traf die Nachricht ein, dass Max von Baden die Abdankung Wilhelms als Kaiser und König bekanntgegeben und Friedrich Ebert das Amt des Reichskanzlers übertragen hatte. Durch dieses Manöver versuchte der badische Prinz in letzter Minute, den revolutionären Druck zu kanalisieren und die faktisch schon nicht mehr bestehende Monarchie als solche zu retten. Am selben Tag riefen Philipp Scheidemann (SPD) und Karl Liebknecht (Spartakusbund) die Republik aus.

Da Gerüchte umliefen, dass die Mannschaften in der Umgebung des Hauptquartiers nicht mehr zuverlässig seien, übersiedelte der Kaiser am Abend des 9. November in den Hofzug und fuhr am frühen Morgen des nächsten Tages ab, nachdem von „anmarschierenden Aufständischen“ berichtet worden war. In der Nähe des niederländischen Ortes Eijsden (südlich von Maastricht) bat er die Niederlande um Internierung. Durch Vermittlung der niederländischen Regierung (Kabinett Beerenbrouck I unter Ministerpräsident Charles Ruijs de Beerenbrouck) fanden Wilhelm II. und sein Gefolge Unterkunft bei Graf Godard von Bentinck im Schloss Amerongen.

Wilhelm II. dankte offiziell am 28. November 1918 ab, 19 Tage nach Ausrufung der Republik, nach eigener Aussage in der Hoffnung, die Situation im Reich zu stabilisieren. Er gab nie den Wunsch auf, wieder auf den Thron zurückzukehren.

Der Text der Abdankungsurkunde lautete:

Ein alliiertes Gesuch, den früheren Regenten an die Siegermächte auszuliefern, lehnte die niederländische Regierung am 22. Januar 1920 ab. Bis 1920 lebte Wilhelm II. auf Schloss Amerongen (Niederlande), danach im Haus Doorn bei Utrecht im Exil. Am 11. April 1921 starb seine Frau, Kaiserin Auguste Viktoria. Kurz vor ihrem Tod äußerte Auguste Victoria den Wunsch nach einer Wiedervermählung des Kaisers nach ihrem Ableben. Am 5. November 1922 heiratete er die verwitwete Prinzessin Hermine von Schönaich-Carolath, geborene Prinzessin Reuß älterer Linie (1887–1947), der fortan die Titulatur einer „Kaiserin“ zukam, während sie amtlich nur eine „Prinzessin von Preußen“ war. 

Der im konservativen Milieu weithin als „Fahnenflucht“ empfundene, kampflose Abgang des Kaisers war noch bis in die 1940er Jahre Gegenstand einer Debatte, in der phasenweise erbittert über die Deutung des Ereignisses und die Frage der Verantwortung gestritten wurde. Die neuere Forschung führt die auffällige strukturelle Schwäche der explizit monarchistisch-restaurativen Strömung der deutschen Rechten, die schon in der ersten Hälfte der 1920er Jahre unübersehbar zutage trat, zu einem erheblichen Teil auf den verheerenden Eindruck der „Kaiserflucht“ zurück. Hier liege die entscheidende Wegmarke einer „Ablösungsbewegung vom Kaiser, die sich selbst für den innersten Kern des preußischen Adels nachweisen lässt“ und als Grundlage der – verglichen etwa mit der Langlebigkeit des französischen Legitimismus – überraschend schnellen und dauerhaften „Auflösung des Monarchismus“ in Deutschland betrachtet werden müsse.

Wilhelm versammelte Gelehrte zu kulturhistorischen Studien um sich („Doorner Arbeitskreis“), verfasste seine Memoiren und weitere Bücher und hielt sich für die Wiederherstellung der Monarchie bereit. Unter anderem durch den Hitlerputsch 1923 sah er sich in der These bestätigt, nur ein Monarch könne Ruhe und Ordnung garantieren. Gleichwohl wurden Hoffnungen auf eine kurzfristige und übergangslose Restauration der Monarchie schon bald auch im engsten Kreis um Wilhelm als – so Magnus von Levetzow 1927 – Ausdruck „vollkommener Hirnverbranntheit“ betrachtet. Diese nachhaltige Ernüchterung wurde nicht zuletzt durch die Tatsache gefördert, dass maßgebliche Monarchisten in Deutschland nach 1925 offen aussprachen, dass weder Wilhelm noch einer seiner Söhne ernsthaft als Thronprätendent in Betracht komme. Der wegen der Flucht und der Gerüchte über seinen Lebenswandel seit 1919 geradezu als „unmöglich“ geltende Kronprinz vertrat im Einvernehmen mit seinem Vater bereits im Mai 1924 die Auffassung, dass zunächst „ein Diktator den Karren aus dem Dreck ziehen“ müsse.

Obwohl die Hohenzollern vom republikanischen Deutschland großzügig abgefunden wurden, machte Wilhelm aus seinem Hass auf die „Saurepublik“ keinen Hehl. Während der Endphase der Weimarer Republik machte sich Wilhelm (bestärkt durch seine Frau, die im Reich umherreiste, und zwei Besuche Görings 1931 und 1932) Hoffnungen auf eine Wiederherstellung der Monarchie durch die Nationalsozialisten. Dies erschien damals insofern nicht ganz unrealistisch, als die in vieler Hinsicht für die Nationalsozialisten vorbildhaften italienischen Faschisten den König von Italien auch während Mussolinis Diktatur im Amt beließen. Die Hoffnungen auf eine Wiedereinsetzung des Kaisers erwiesen sich nach der Machtergreifung der NSDAP Anfang 1933 als Illusion. Wilhelm entwickelte eine zunehmend distanzierte Haltung zur politischen Entwicklung in Deutschland.

„Alles wird von den Leuten ja beseitigt: die Fürsten, der Adel, die Offiziere, die Stände usw.; aber das wird sich rächen, man wird die einzige Fahne, die sie noch übrig gelassen haben, die mit dem Hakenkreuz, noch einmal verfluchen, und die Deutschen selber werden sie eines Tages verbrennen“, urteilte er am 7. September 1933. Als Wilhelm II. im November 1938 von dem antijüdischen Pogrom, der „Kristallnacht“, erfuhr, äußerte er sich entsetzt, bezeichnete es als Schande und forderte jeden Deutschen auf, dagegen zu protestieren.

Die niederländische Königin Wilhelmina, die während seines gesamten Exils jeden direkten Kontakt zu Wilhelm vermieden hatte, ließ ihm angesichts eines bevorstehenden deutschen Angriffs auf die Niederlande im April 1940 eröffnen, dass er sich nicht mehr als Internierter zu betrachten habe und darum ausreisen könne, wann und wohin er wolle. Die niederländische Regierung legte ihm mehrfach nahe, einen Ort aufzusuchen, der nicht unmittelbar in der Kampfzone lag. Selbst das britische Königshaus unter der Regentschaft von König Georg VI. bot Wilhelm Asyl an. Der Kaiser lehnte aber alle Angebote dankend mit der Erklärung ab, er wolle wegen seines hohen Alters in Doorn bleiben und seinem Schicksal dort entgegensehen. Bei der Besetzung der Niederlande im Mai 1940 ließ Hitler das Anwesen durch die Geheime Feldpolizei abriegeln. Der Kaiser durfte es nach wie vor nur zu kurzen Ausflügen und in Begleitung verlassen.

Wilhelm schickte Adolf Hitler am 17. Juni 1940 ein Glückwunschtelegramm, in dem er ihm zum deutschen Sieg über Frankreich kurz zuvor gratulierte: 

Hitler antwortete Wilhelm, angeredet als , am 24. Juni 1940.

Wilhelm II. starb am 4. Juni 1941 um 12:30 Uhr im Haus Doorn nach einer Lungenembolie. Trauerfeiern im Reich wurden verboten. Die NS-Machthaber erlaubten nur einer kleinen Zahl von Personen (dem engeren Familienkreis, einigen ehemaligen Offizieren, darunter Generalfeldmarschall August von Mackensen) die Fahrt in die besetzten Niederlande zur Teilnahme an der Beisetzung. Der Kaiser hatte seine Beisetzung im engsten Kreis verfügt und Trauerreden, Kränze, Fahnen (um Hakenkreuzfahnen zu vermeiden) untersagt. An der Trauerfeier nahmen Abordnungen der alten Armee und der neuen Wehrmacht teil, die Bestattung endete auf Wunsch des Kaisers mit dem von der Wehrmachtskapelle gespielten Choral und Gebetslied des Großen Zapfenstreichs „Ich bete an die Macht der Liebe“.

Wilhelm wurde zunächst in einer Kapelle nahe dem Doorner Torhaus beigesetzt, wobei drei Hände Potsdamer Erde aus der Gegend des Antikentempels – dem Bestattungsort der Kaiserin Auguste Viktoria, später auch von Hermine von Schönaich-Carolath – auf seinen Sarg gestreut wurden. Er selbst hatte verfügt, dass eine „Umbettung seiner Gebeine in deutsche Erde“ erst nach der Wiedererrichtung der Monarchie in Deutschland durchzuführen sei.
Später wurde sein Sarg in das nach seinen Zeichnungen postum erbaute Mausoleum im Park von Haus Doorn überführt. Sein von ihm selbst ausgewählter Grabspruch lautet:

Eine besondere Zuwendung von seinen Eltern erfuhr Wilhelm II. nicht, was zu einem bleibenden Ressentiment besonders gegen seine Mutter führte, die ihn ihrerseits, folgt man ihren familiären Briefen, auch politisch sehr kritisch sah. Schmerzvoll waren die Versuche der Familie, seiner Behinderung entgegenzuwirken. Sein verkümmerter linker Arm führte zu Gleichgewichtsstörungen und Haltungsschäden sowie häufigen Schmerzen im linken Ohr. Doch der zukünftige König von Preußen sollte ein „ganzer Mann“ und kein Krüppel sein. So wurden dem Kind verschiedene schmerzhafte Therapien zugemutet. Das oft erforderliche Reiten fiel ihm lebenslang schwer.

Die Behinderung verminderte vermutlich sein Selbstwertgefühl und steigerte seine Egozentrik, leichte Kränkbarkeit und Sprunghaftigkeit. Das Tragen von Uniformen und das Abstützen der linken Hand auf der Waffe waren da hilfreiche Angewohnheiten. Ob von einer ernsthaften seelischen Erkrankung oder von einer Anlage zu einer Geisteskrankheit gesprochen werden kann, ist strittig. Ein schwermütiger Zug wird ihm mitunter attestiert. Der noch heute berühmte Psychiater Emil Kraepelin sah Wilhelms Gemütsverfassung sogar – in einer auf öffentlich zugängliche Quellen gestützten Ferndiagnose – als einen , wobei die hier insinuierte manisch-depressive Disposition von anderer Seite bestritten wurde.

Der US-amerikanische Historiker Robert K. Massie beschreibt ihn zum Zeitpunkt des Regierungsantritts: 

Dem historischen Publizisten Volker Ullrich galt der Kaiser als „unsicher und arrogant, intelligent und impulsiv, vernarrt in die moderne Technik und zugleich verliebt in Pomp und Theatralik“. Anhaltende Schwierigkeiten waren Wilhelm II. verhasst. Das begünstigte wohl auch seine sprichwörtliche Reiselust. Vor allem aber ließ er deswegen auch bewährte Freunde und Parteigänger schnell im Stich, so dass zunehmend Höflinge mit eher diplomatischem Charakter seinen Umgang ausmachten und seine Personalauswahl bestimmten (so wohl auch die Wahl Bülows). Offiziere, unter denen er sich wohl fühlte, erweiterten sein Urteil wenig, denn sie hatten im Zweifel die politischen Vorurteile ihrer kastenartig abgeschlossenen Berufsgruppe, und auch ihr Stil des Schwadronierens färbte auf ihn ab.

Von seiner Persönlichkeit her gesehen behinderten narzisstische Züge seine Einfühlungsgabe und sein Urteil über Andere, wie etwa über Nikolaus II. von Russland. Er selbst sah sich als geradezu und offen, doch seine Taktlosigkeiten waren bekannt. Sie fielen seiner Mitwelt besonders bei seinem Regierungsantritt und bei Bismarcks Entlassung ins Auge und wurden von diesem in seinen "Gedanken und Erinnerungen" eifrig ausgebreitet. Eine diese Nachteile ausbalancierende Welt- und Menschenkenntnis zu erwerben, hatte sein Werdegang ihm nicht erlaubt.

Trotz der Wesensunterschiede zu seinem altpreußisch-schlichten und im Persönlichen loyalen Großvater Wilhelm I. versuchte Wilhelm II. immer, dessen Regierungsmuster zu folgen. Man kann sein anfängliches Verhältnis zu Caprivi dergestalt deuten, dass er hier „seinen eigenen Bismarck“ gefunden zu haben hoffte. Zum militärischen Oberbefehlshaber ernannte er den Neffen des berühmten Generalfeldmarschalls Helmuth von Moltke (), der dann aber aus dem Schatten Alfred von Schlieffens nicht herauszutreten vermochte. Allerdings wurde die Zurückhaltung seines Großvaters bei direkten politischen Eingriffen keineswegs bleibendes Merkmal des Enkels. Wiederholt griff Wilhelm II. durch Personalentscheidungen und Befehle für Gesetzesvorlagen direkt in die Politik ein.

Gar nicht folgte er der öffentlichen Zurückhaltung des alten Kaisers. Mit Selbstdarstellungseifer drängte Wilhelm II. oft ostentativ in die Öffentlichkeit, wobei seine nicht unbeachtliche Rednergabe ihm ein lebhaftes Echo einbrachte, ihn aber auch zu politisch bedenklichen Formulierungen hinriss. Auch begünstigte dieser Übereifer sein Verhältnis zu den Massenmedien. Man kann ihn als ersten Medienmonarchen des 20. Jahrhunderts ansehen.

Sein Faible für Uniformen und Orden trug zum Klischee-Bild des nach ihm benannten Wilhelminismus bei. Ein Höhepunkt dieses Stils war die pompöse und von der Berliner Bevölkerung als „Puppenallee“ in gewohnter Weise verspottete Siegesallee im Großen Tiergarten mit 32 Statuen der brandenburgischen Markgrafen und Kurfürsten, der preußischen Könige und weiteren 64 Nebenfiguren. Für das Standbild des Askaniers Albrecht der Bär fertigte Wilhelm eigenhändig Kostümskizzen an. In der sogenannten „Rinnsteinrede“ zur Eröffnung des Prachtboulevards am 18. Dezember 1901 verordnete Wilhelm den Stil der Bildenden Künste von oben ().

Eigene Interessen entwickelte er ferner für die Archäologie, seine Korfu-Aufenthalte waren davon bestimmt. Außerdem oblag er, wie in Adelskreisen nicht unüblich, begeistert der Jagd und später der Forstwirtschaft. Seine Trophäenzahl erfreute ihn (er erlegte rund 46.000 Tiere), im Exil fällte er gerne Bäume. Bei der Jagd hatte Wilhelm auch seinen später engen Freund Philipp Graf zu Eulenburg kennengelernt, der besonders in den Jahren 1890 bis 1898 zu seinen wichtigsten Beratern zählte.

Wilhelm liebte wie sein Bruder Heinrich das Segeln. Er segelte vor der Küste Südenglands mit seinen Yachten "Meteor I–V" in prestigeträchtigen Regatten und war Stammgast bei der Kieler Woche, die er 1894 zum ersten Mal besucht hatte. Auch Automobile machten ihm Freude. Er fuhr gerne mit den neuesten Wagen und war Protektor des Kaiserlichen Automobilclubs.

Desengagement, wenn die Dinge anders liefen, als er wollte, blieb sein Wesenszug. 1918, angesichts der Novemberrevolution, entwich er ins neutrale Ausland. Seine in Holland verfasste Autobiografie gibt sprechende Zeugnisse aus seiner Kindheit, bezeugt aber mit ihren Rechtfertigungen oder Themenvermeidungen seine Urteilsschwächen.

Wilhelm war zunächst recht populär. Die weniger geschätzten Züge einer Reichseinigung „von oben“ mit Bewahrung alter Machtstrukturen fand in der Kaiserverehrung einen willkommenen Ausgleich. Die weithin monarchistisch gesinnte Presse nahm dies auf, man fand für ihn die Bezeichnungen „Arbeiterkaiser“ und „Friedenskaiser“. Die letztere Bezeichnung geht u. a. auf den Vorschlag von Emanuel Nobel von 1912 zurück, Kaiser Wilhelm II. den von Alfred Nobel gestifteten Friedensnobelpreis zuzusprechen, damals hatte das Deutsche Reich unter seinem Kaisertum 24 Jahre Frieden gehalten. Doch wurde er andererseits auch als bedrohlich empfunden (vgl. Ludwig Quiddes als Kritik an Wilhelm II. aufgefasste und vielrezipierte 1894er Studie "Caligula" zum „Cäsarenwahnsinn“) oder aber verspottet: Auch in der Bezeichnung „Redekaiser“ steckte Kritik. Über seine vielen verschiedenen Uniformen – Graf Philipp zu Eulenburg sprach von „Alle Tage Maskenball!“ – wurden Witze gemacht:
Gefährlicher als die Kritik der Demokraten, Sozialisten, Katholiken, auch den im Reich vertretenen Minderheiten (die Polen, die Dänen seit 1864, die welfisch gesinnten Hannoveraner seit 1866, die Elsass-Lothringer seit 1871) traf ihn die Skepsis des die öffentliche Meinung beherrschende Bürgertum. Bei vielen Schriftstellern war er nicht angesehen, der ironische Thomas Mann war in seinem Roman "Königliche Hoheit" noch am mildesten mit einem behinderten und etwas einfältigen Dynasten umgegangen. Direkte Kritik verbot der Paragraph zur „Majestätsbeleidigung“ im Strafgesetzbuch, aber die Witze über ihn wurden immer beißender. Man vergleiche nur das viel positivere Kaiserbild des alten Kaisers Franz Joseph in Österreich-Ungarn.

Sein eigener Onkel, der britische König Eduard VII. beschrieb ihn einmal als den „brillantesten Versager der Geschichte“.

Nach seinem lange hinausgezögerten Entschluss, 1918 nicht an der Spitze seiner Truppe zu fallen, sondern ins Exil zu gehen, wurde ihm auch Feigheit vorgeworfen. Bei vielen verschob sich die Meinung hin zu Verachtung. Dennoch blieb durch die Jahre der Weimarer Republik hindurch der monarchistische Flügel stark. Doch Wilhelms Hoffnungen auf eine Rückkehr als Monarch zerschlugen sich nach der Präsidentenwahl Hindenburgs 1925 und noch einmal nach Hitlers Machtantritt 1933. Hindenburg nahm seinen Eid auf die Republik ernst, Hitler seine „Führer“-Diktatur. Volker Ullrich urteilte auf Grund der nunmehr vollständig vorliegenden Studie Röhls über Wilhelm II. 2008: 

Der Historiker Christopher Clark kommt in seinem Werk "Wilhelm II. Die Herrschaft des letzten deutschen Kaisers" jedoch zu einem anderen Urteil. Clark plädiert dafür, die in seinen Augen veraltete Theorie des deutschen Sonderweges zu überdenken und das Deutsche Kaiserreich und seinen letzten Kaiser nicht als Vorläufer der nationalsozialistischen Diktatur zu sehen.

Wilhelm heiratete 1881 Prinzessin Auguste Viktoria von Schleswig-Holstein-Sonderburg-Augustenburg (1858–1921). Sie hatten sieben Kinder. Nach dem Tod seiner ersten Frau heiratete er 1922 die verwitwete Prinzessin Hermine von Schönaich-Carolath, geborene Prinzessin Reuß ä. L. (1887–1947), die von ihm als „Kaiserin“ tituliert wurde, amtlich aber nur eine „Prinzessin von Preußen“ war.

Der britische König Georg V. war sein Cousin ersten Grades. Sein Bruder Prinz Albert Wilhelm Heinrich von Preußen war Großadmiral der Kaiserlichen Marine.


Nach Wilhelm II. wurden benannt:


Memoiren

Historische Werke

Kulturgeschichtliche Werke






</doc>
<doc id="14651" url="https://de.wikipedia.org/wiki?curid=14651" title="Manfred Stolpe">
Manfred Stolpe

Manfred Stolpe (* 16. Mai 1936 in Stettin) ist ein deutscher Politiker (SPD).

Er war von 1990 bis 2002 der erste Ministerpräsident des Landes Brandenburg nach der Wende und von 2002 bis 2005 Bundesminister für Verkehr, Bau- und Wohnungswesen.

Nach dem Abitur 1955 in Greifswald begann Stolpe ein Studium der Rechtswissenschaft an der Universität Jena, welches er 1959 mit dem Abschluss Diplom-Jurist beendete. Von 1959 bis zum Mauerbau 1961 war er Gaststudent an der Freien Universität Berlin.

Stolpe war von 1959 bis 1969 bei der Evangelischen Kirche Berlin-Brandenburg tätig, ab 1962 als Leiter der Geschäftsstelle der Evangelischen Kirchenleitungen in der DDR, von 1963 bis 1966 auch Referent des lausitz-neumärkischen General-Superintendenten Günter Jacob. Von 1969 bis 1981 war er dann Leiter des Sekretariats des Bundes der Evangelischen Kirchen in der DDR. Ab Januar 1982 war er Konsistorialpräsident der Ostregion der Evangelischen Kirche Berlin-Brandenburg. Von 1982 bis 1989 war er zusätzlich stellvertretender Vorsitzender des Bundes der Evangelischen Kirchen in der DDR.

Vom 1. November 1990 bis zum 26. Juni 2002 war er Ministerpräsident des Landes Brandenburg. Als Landtagsabgeordneter vertrat er gleichzeitig den Wahlkreis Cottbus mit einem Direktmandat. Im Sommer 2002 trat er zugunsten von Matthias Platzeck als Ministerpräsident zurück. Einer der Gründe für seinen Rücktritt war die Koalitionskrise der Großen Koalition in Brandenburg infolge einer Abstimmung im Bundesrat über das Zuwanderungsgesetz: Stolpe hatte mit „ja“ gestimmt, der stellvertretende Ministerpräsident und Innenminister Jörg Schönbohm (CDU) dagegen mit „nein“. Das uneinheitliche Votum wurde von Bundesratspräsident Klaus Wowereit (SPD) als Zustimmung gewertet. Diese Entscheidung wurde vom Bundesverfassungsgericht ein halbes Jahr nach Stolpes Rücktritt als Ministerpräsident als grundgesetzeswidrig annulliert. Stolpe hinterließ dem Land die mit erheblichen Landesmitteln geförderten und letztendlich gescheiterten Investitionsruinen Cargolifter und Chipfabrik Frankfurt (Oder) sowie den unwirtschaftlichen EuroSpeedway Lausitz.

Nach der Bundestagswahl 2002 wurde er am 22. Oktober 2002 als Bundesminister für Verkehr, Bau- und Wohnungswesen in die von Bundeskanzler Gerhard Schröder geführte Bundesregierung berufen. In seiner Position als Bundesverkehrsminister ist der Name von Manfred Stolpe eng mit dem misslungenen Einführungsversuch einer Lkw-Maut in Deutschland zum 31. August 2003 verbunden. Die umstrittenen Verträge waren von seinem Vorgänger Kurt Bodewig ausgehandelt und unterzeichnet worden. Stolpe wurde von mehreren Seiten ein nicht nachvollziehbar nachsichtiges Verhalten gegenüber dem Vertragspartner Toll Collect vorgeworfen. Die CDU forderte im Zusammenhang mit der gescheiterten Mauteinführung und dem unvorhergesehenen Milliardenverlust für die Staatsfinanzen ab Herbst 2003 seinen Rücktritt als Bundesminister.

Der in seiner Amtszeit verabschiedete Bundesverkehrswegeplan 2003 für den Zeitraum 2001 bis 2015 trägt seine Handschrift (er war von der EU-Ost-Integration überzeugt) und ist von der EU-Erweiterung 2004 geprägt: erstmals rückte die Entwicklung von Ost-West-Verkehrsachsen bzw. -Magistralen nach Tschechien und Polen in den Fokus der Planungen.

Nach der Bundestagswahl 2005 – sie führte zu einem Regierungswechsel und einer großen Koalition – schied Stolpe am 22. November 2005 aus dem Amt. Sein Nachfolger als Verkehrsminister wurde Wolfgang Tiefensee (SPD), den Gerhard Schröder schon 2002 in dieses Amt berufen wollte.

Stolpe gehört dem Domkapitel Brandenburg an. Außerdem ist er Mitglied im Lenkungsausschuss des deutsch-russischen Petersburger Dialogs.

Während seiner Tätigkeit in der Kirchenleitung in der DDR hatte Stolpe regelmäßige Kontakte zum Ministerium für Staatssicherheit. Nach seiner eigenen Aussage waren alle diese Kontakte im Interesse und Sinne der Kirchenmitglieder; Kritiker hingegen meinen, er habe Kircheninterna und Informationen aus der DDR-Opposition verraten. Oppositionskreise der DDR waren überzeugt, dass Stolpe mit der Stasi zusammenarbeitete. Die Realisierung von Ausreiseanträgen brachten sie mit seinem Namen in Verbindung. Seine Tätigkeiten waren und sind umstritten. Manche Vorwürfe werden als haltlos angesehen. Kritiker meinen, er habe sicherlich „nicht ohne Grund“ 1978 die Verdienstmedaille der DDR in einer konspirativen Wohnung der Staatssicherheit erhalten.
In seiner Amtszeit als Ministerpräsident von Brandenburg kam es zu einem Rechtsstreit mit der Gauck-Behörde. Das Verwaltungsgericht Berlin entschied am 3. Juni 1993, dass Joachim Gauck nicht länger behaupten darf, Stolpe sei ein wichtiger inoffizieller Mitarbeiter der DDR-Staatssicherheit gewesen. Dagegen lehnte das Gericht die Forderung Stolpes ab, Gauck alle bisher wertenden Äußerungen über Stolpe zu verbieten.

Ein parlamentarischer Untersuchungsausschuss des Landes Brandenburg kam 1994 zu dem Ergebnis, Stolpe sei kein Zuträger der Staatssicherheit gewesen, sondern von staatlicher Seite als gleichrangiger Verhandlungspartner angesehen worden. Er habe weder Menschen noch der Kirche geschadet.

Die Birthler-Behörde legte 2003 ein über 1200-Seiten-Dossier zu Manfred Stolpe alias IM „Sekretär“ vor. Damit thematisierte sie erstmals die Unterlagen über einen amtierenden Bundesminister.

Das Bundesverfassungsgericht (Az: 1 BvR 1696/98) entschied im Oktober 2005 im Rahmen einer Aufhebung eines Urteils des Bundesgerichtshofs aus dem Jahre 1998, dass eine Bezeichnung Stolpes als ehemaliger Stasi-Mitarbeiter oder „Inoffizieller Mitarbeiter“ nicht zulässig sei. Die Stasi-Unterlagenbehörde verwies allerdings umgehend darauf, dass sich das Urteil lediglich auf „eine Äußerung im politischen Meinungsstreit“ beziehe und bekräftigte nochmals, dass Stolpe – wie in einem Gutachten aus dem Jahre 1992 festgestellt wurde – unabhängig von der Tatsache, ob er je als Mitarbeiter rekrutiert worden sei, als IM „Sekretär“ und über 20 Jahre hinweg als „ein wichtiger IM im Bereich der evangelischen Kirche der DDR“ in den Akten der Staatssicherheit geführt worden sei. Spätere Aktenfunde, zuletzt 2003, hätten diese Bewertung weiter untermauert. Nach seiner eigenen Aussage hat Manfred Stolpe wissentlich niemandem durch seine Kontakte zur Stasi geschadet.

2011 kam ein für die Enquetekommission des Brandenburger Landtages erstelltes Gutachten zu dem Ergebnis, Stolpe sei "wichtiger IM" der Stasi gewesen und hätte zwischen 1990 und 1994 eine Aufforderung zur Niederlegung seines Landtagsmandat ob der Stasi-Kontakte erhalten müssen. Diese Sichtweise wurde allerdings kontrovers in der Sitzung der Enquetekommission diskutiert und teilweise abgelehnt.

Seit 1990 ist Stolpe Mitglied der SPD. Von 1991 bis 2002 war er Mitglied im SPD-Bundesvorstand.

Überwiegend positiv bewertete die deutsche Wirtschaftspresse das von ihm initiierte Stadtumbauprogramm. Es ermöglicht schrumpfenden Kommunen die Umgestaltung und den Abriss ganzer Stadtareale, um der Slumbildung vorzubeugen.


Manfred Stolpe ist seit 1961 mit der inzwischen pensionierten Ärztin Ingrid Stolpe verheiratet.

Im April 2009 wurde bekannt, dass Stolpe bereits 2004 während seiner Amtszeit als Bundesminister für Verkehr an Darmkrebs erkrankt war. 2008 musste er wegen Metastasen in der Leber erneut operiert werden. Ingrid Stolpe erkrankte 2008 an Brustkrebs. Die Stolpes haben in der Sendung "Menschen bei Maischberger" (ARD) offen über ihre Krebskrankheiten gesprochen und ein Buch darüber geschrieben.




Weiterführende Literatur



</doc>
<doc id="14654" url="https://de.wikipedia.org/wiki?curid=14654" title="Bernard von Brentano">
Bernard von Brentano

Bernard von Brentano (* 15. Oktober 1901 in Offenbach am Main; † 29. Dezember 1964 in Wiesbaden) war ein deutscher Schriftsteller, Lyriker, Dramatiker, Erzähler, Romancier, Essayist und Journalist.

Bernard von Brentano war ein Sohn des hessischen Innen- und Justizministers Otto von Brentano di Tremezzo und ein Bruder von Clemens und Heinrich von Brentano. Seine Mutter Lilla Beata, geborene Schwerdt stammte aus der Frankfurter Linie der Brentanos.

Anders als seine Brüder hat Bernard von Brentano den Langnamen seiner Familie, "Brentano di Tremezzo", kaum benutzt.

Nach dem Abitur in Offenbach studierte Brentano Philosophie in Freiburg, München, Frankfurt und Berlin. In Freiburg wurde er aktives Mitglied der katholischen Studentenverbindung Bavaria, in München des K.St.V. Rheno-Bavaria, beide im KV und seit 1920 war er Mitglied im P.E.N.

Von 1925 bis 1930 arbeitete Brentano beim Feuilleton im Berliner Büro der "Frankfurter Zeitung" und wurde dort Nachfolger von Joseph Roth, mit dem er eng befreundet war. Im Bund proletarisch-revolutionärer Schriftsteller engagierte er sich stark und war auch Mitarbeiter der Zeitschrift "Die Linkskurve". Zusammen mit Bertolt Brecht wollte er 1929/30 eine weitere literarische Zeitung herausgeben, dieses Projekt konnte jedoch nicht realisiert werden.

In seinem 1929 erschienenen Essay „Über den Ernst des Lebens“ wertete Brentano Kriegsbriefe gefallener Soldaten aus und kritisierte die Verherrlichung des Krieges. Mit dem 1932 erschienenen prophetischen Buch "Der Beginn der Barbarei in Deutschland" zog er sich endgültig den Hass der Nationalsozialisten zu. Seine Bücher wurden nach deren Machtergreifung auf dem Scheiterhaufen verbrannt. 

Ob Brentano Mitglied der KPD gewesen ist, konnte bislang nicht eindeutig geklärt werden, es gibt allerdings etliche Indizien dafür. In jedem Fall lehnte Brentano nach zwei Moskaureisen im Jahre 1930 und 1932 das Stalinregime ab und distanzierte sich später auch vom Marxismus.

Brentano verließ Anfang April 1933 Berlin und emigrierte in die Schweiz, seit 1934 lebte er mit seiner Familie in Küsnacht bei Zürich. Dort arbeitete er für die "Neue Zürcher Zeitung" und die "Weltwoche". In dem Zürcher Verlag Oprecht erschien 1936 Brentanos Hauptwerk "Theodor Chindler", das mehrfach neu aufgelegt und von Hans W. Geißendörfer 1979 verfilmt wurde. Dieser Roman, von Thomas Mann sehr gelobt, schildert den Zusammenbruch des Wilhelminischen Kaiserreiches am Beispiel einer katholischen Familie und greift teilweise auf die eigene Familiengeschichte zurück.

Ab 1940 bemühte sich Brentano um eine Repatriierung, kehrte aber erst 1949 aus der Emigration nach Deutschland, in das „Land der Liebe“, wie er es in seiner Autobiographie 1952 nannte, zurück und lebte mit seiner Familie bis zu seinem Tode in Wiesbaden.

posthum:



</doc>
<doc id="14655" url="https://de.wikipedia.org/wiki?curid=14655" title="August Wilhelm Schlegel">
August Wilhelm Schlegel

August Wilhelm Schlegel von Gottleben , ab 1812 von Schlegel (* 5. September oder 8. September 1767 in Hannover; † 12. Mai 1845 in Bonn), war ein deutscher Literaturhistoriker und -kritiker, Übersetzer, Alt-Philologe und Indologe. Er lehrte ab 1795 an der Universität Jena, von 1798 bis 1801 als außerordentlicher Professor. Zusammen mit seiner Frau Caroline Schlegel, seinem Bruder Friedrich und dessen Frau Dorothea Schlegel, Johann Gottlieb Fichte und später Ludwig Tieck sowie Novalis prägte er die neue „romantische Schule“. Als Übersetzer machte er sich um die italienische, spanische und portugiesische Literatur verdient; seine Hauptleistung ist aber die Übersetzung von 17 der Stücke Shakespeares.

August Wilhelm Schlegel war der vierte Sohn des evangelisch-lutherischen Pastors Johann Adolf Schlegel, der ursprünglich aus Sachsen stammte. Sein Vater war Pfarrer in Hannover an der Marktkirche. Die Mutter Johanna Christiane Erdmuthe Hübsch (1735–1811) war die Tochter eines Mathematiklehrers in Schulpforta und das Paar hatte insgesamt acht Söhne und zwei Töchter. In der Familie bestand ein künstlerisch und intellektuell aufgeschlossenes Umfeld. August absolvierte das Gymnasium in Hannover.

Schlegel studierte zunächst (1786) in Göttingen Theologie, entschied sich aber für Philologie, als er in Gottfried August Bürger einen Mentor fand, der ihm Einblicke in die Übersetzungspraxis aus klassischen sowie neueren Sprachen vermittelte. August Wilhelm Schlegel wurde ein fleißiger Schüler des Altphilologen Christian Gottlob Heyne. Schon im Juni des nächsten Jahres verdiente er sich mit einer lateinischen Abhandlung über die Homerische Geographie (1788 gedruckt) einen akademischen Preis. In diese Zeit lernte er Caroline Böhmer und Wilhelm von Humboldt kennen. 1789 starb sein Bruder, Carl August Schlegel, im Alter von 28 Jahren in Hannoverschen Regimentsdiensten in Madras. Um 1790 zog sein jüngster Bruder Friedrich zu ihm nach Göttingen. Die beiden Brüder wurden beeinflusst von Johann Gottfried Herder, Immanuel Kant, Tiberius Hemsterhuis, Johann Joachim Winckelmann und Karl Theodor von Dalberg. August Wilhelm Schlegel unternahm eine Teil-Übersetzung von Dantes La Divina Commedia und eine Übersetzung von Shakespeares Midsummer Night's Dream (1789). 1791 beendete er sein Studium.
Von 1791 bis 1795 war er Hauslehrer von Willem Ferdinand Mogge Muilman (1778–1849), des späteren Direktors der Nederlandsche Bank, am Gouden Bocht in Amsterdam. Als seine spätere Frau Caroline Böhmer vom preußischen Militär verhaftet worden war, beteiligte sich Schlegel an den Bemühungen zu ihrer Freilassung und brachte sie von Kronberg im Taunus nach Leipzig und schließlich im benachbarten Städtchen Lucka bei einer Bauernfamilie unter. Danach kehrte er nach Amsterdam zurück. Seitdem Schlegel 1794 in Briefwechsel mit Schiller getreten war, wurde er als Kritiker und Rezensent in den von Schiller herausgegebenen "Horen" tätig. Er ging zunächst zu seiner Mutter, dann nach Braunschweig. Hier traf er wieder mit Caroline Böhmer zusammen. Schlegel hoffte auf eine Anstellung am Collegium Carolinum, ging dann aber nach Jena und hielt dort Vorlesungen zur Ästhetik. In den nächsten vier Jahren verfasste er etwa dreihundert, mitunter höchst umfangreiche, Rezensionen, großenteils für die Jenaer Allgemeine Literatur-Zeitung.

Am 1. Juli 1796 feierte er seine Hochzeit mit Caroline Böhmer. Friedrich folgte seinem Bruder August und dessen Frau nach Jena. Zu Schiller war das Verhältnis zunächst gut. Das änderte sich, als August Wilhelm Schlegel Schillers Lied von der Glocke kritisierte, in dem nach Schlegel von allem und jedem die Rede sei und das sachliche Fehler enthalte (der Klöppel würde nicht erwähnt und die Mischung der Bestandteile – beim Glockenguss sind es Zinn und Kupfer – stimme ebenfalls nicht). Der daraufhin verstimmte Schiller griff seinerseits Schlegel in den "Xenien" (erschienen im "Musenalmanach auf das Jahr 1797") an. Friedrich Schlegels verletzende Rezension von Schillers Zeitschrift "Die Horen" führte im Mai 1797 zum endgültigen Bruch. Die Brüder entschieden sich im Oktober 1797, selbst eine Zeitschrift "Athenaeum" herauszugeben, die ab Mai 1798 zweimal pro Jahr bis 1800 erschien. Sie gilt als das Sprachorgan der Jenaer Frühromantik. In ihr wurden die Französische Revolution, das Werk Goethes und Fichtes Wissenschaftslehre besprochen. Der Inhalt bestand meist aus Fragmenten. Schon 1796 hatte Schlegel seine Übersetzung der Werke Shakespeares angekündigt, die 1797–1810 erschien und 17 Dramen in 14 Bänden umfasste. Diese Übertragung, später durch Dorothea Tieck und Wolf Heinrich Graf von Baudissin ergänzt, ist bis heute die deutsche Standardversion. 1798 lernte er während eines zweimonatlichen Aufenthaltes in Berlin Ludwig Tieck persönlich kennen.

1799 lebten die beiden Brüder, August Wilhelms Ehefrau Caroline sowie Dorothea Veit für ein halbes Jahr zu viert zusammen – im Hinterhaus, An der Leutra 5, in Jena. „Das kleine Jena war zu einer Geistesmetropole geworden.“ Diese „Romantiker-Wohngemeinschaft“ bildete das Kernstück der Jenaer Romantik und publizierte im Musen-Almanach. Die Autoren brachen mit vielen Konventionen: Beispielsweise mischten sie in ihre Romane Gedichte und Balladen, kleine Märchen etc.; dabei bezogen sie sich oft auf Goethes Werke („Die Leiden des jungen Werthers“, „Wilhelm Meisters Lehrjahre“). Goethe bat seinerseits August Wilhelm Schlegel in dessen Jenaer Zeit mehrfach um Rat in Fragen der Metrik. Er schätzte Schlegel als Literaturhistoriker und -kritiker, als Übersetzer und als Person; als Dichter hielt Goethe ihn für weniger bedeutend. Im Kampf gegen den Rationalismus standen die Brüder Schlegel auf Goethes Seite.

Eine erste Sammlung von August Wilhelm Schlegels Gedichten erschien 1800. Viel frischer und unmittelbarer erwies sich sein Talent in der Satire. Den Mittel- und Höhepunkt der Satire bildete das empfindsam-romantische Schauspiel in zwei Aufzügen „Kotzebue’s Rettung oder der tugendhafte Verbannte“, voll boshaft-witziger Anspielungen auf die meisten Werke des Angegriffenen und auf seine neuesten Schicksale in Russland und Sibirien.

Als Dichter erlebte er Misserfolge. Anfang 1802 fiel sein klassizistisches Schauspiel "Ion", basierend auf einem Original von Euripides, aber ohne griechische Formelemente wie Prolog und Chor, durch. Das Stück lag im Rahmen einer zeitgenössischen Strömung, sich die Antike durch Modernisierung bzw. „Romantisierung“ anzueignen. Auch Goethe nahm Stellung. Er hatte sich als Theaterdirektor um die Einstudierung und Aufführung bemüht, tadelte Schlegel jedoch dafür, weil dieser bei seiner Überarbeitung des "Ion" den Euripides gebührenden Respekt habe vermissen lassen.

Schlegel lebte mittlerweile in Berlin. Dort hielt er von 1801 bis 1804 die Vorlesungsreihe "Über schöne Literatur und Kunst", in der er die Literaturen des klassischen Altertums, des germanischen und provenzalischen Mittelalters und der romanischen (besonders spanischen und italienischen) Neuzeit als ebenbürtig darstellte. Neben den Jenaer Vorlesungen "Über philosophische Kunstlehre" (1798/1799) und den 1803/1804 ebenfalls in Berlin gehaltenen "Vorlesungen über Enzyklopädie" sind die Berliner Vorlesungen "Über schöne Literatur und Kunst" die Hauptquelle für August Wilhelm Schlegels sprachphilosophische Konzepte, die neben denjenigen Wilhelm von Humboldts als der bedeutendste Beitrag des frühen 19. Jahrhunderts zu diesem Themenkomplex gelten können. Im Sinne Herders und Winckelmanns forderte und versuchte Schlegel eine Verbindung von philosophischer Theorie und von Geschichte der Kunst; das vermittelnde Bindeglied zwischen beidem sah er in der Kritik. Die Berliner Jahre waren „die Glanzzeit Schlegels, auch im gesellschaftlichen Leben“.

Im Frühling 1802 beschlossen Caroline und August Wilhelm Schlegel, ihre Ehe zu lösen. Das gelang erst nach Überwindung mehrerer Hindernisse am 17. Mai 1803. Caroline heiratete bald darauf Friedrich Schelling. Nach der Auflösung der Ehe war Schlegel bis 1817 literarischer Berater und Sekretär von Madame de Staël, die seit kurzem getrennt vom Schriftsteller Benjamin Constant lebte. Schlegel traf sie im Frühjahr 1804 in Berlin. Gegen ein üppiges Gehalt wurde er zum Erzieher von Madame de Staëls Kindern ernannt. Einige Wochen später besuchten sie Caroline und Schelling in Würzburg.

Zu ihrem Freundenkreis auf ihrem Schloss Coppet am Genfersee gehörten Karl Viktor von Bonstetten (ein durch die Revolution gestürzter Berner Staatsmann), der Genfer Historiker Jean-Charles-Léonard Simonde de Sismondi, der Herzog Mathieu de Montmorency-Laval, Benjamin Constant und später auch Adelbert von Chamisso. Ende des Jahres reisten De Staël und Schlegel nach Italien. Schlegel veröffentlichte 1804 „Blumensträuße italienischer, spanischer und portugiesischer Poesie“ mit mustergültigen Übertragungen aus Dante, Petrarca, an dem er sich seit seinen Universitätsjahren wiederholt versucht hatte, Boccaccio, Tasso, Guarini, Montemayor, Cervantes und Camões. Schlegel veröffentlichte in der Zeitschrift „Europa“ seines Bruders den Aufsatz "Über das spanische Theater", eine überschwängliche Lobrede auf Calderón. Ende Juni 1805 war er mit Madame de Staël wieder nach Coppet zurückgekehrt. Mit ihr verbrachte er den folgenden Winter großenteils in Genf und reiste im Frühling 1806 nach Frankreich. 1806 war er in Auxerre und Rouen, 1807 in Aubergenville und hielt sich im Schloss d'Acosta auf. Im Mai 1807 fuhren sie zurück nach Coppet. Im Dezember 1807 besuchten sie Schelling und seine Frau in München. Anschließend fuhr er ohne Madame de Staël nach Wien.

Höhepunkt dieser Jahre waren, neben seiner Hamletübersetzung, die 1808 in Wien gehaltenen Vorlesungen "Über dramatische Kunst und Literatur" (1809–1811 veröffentlicht), die aus seinen Shakespeare- und Calderón-Übersetzungen (1803–1809) hervorgingen, und zur Verbreitung der romantischen Ideen beitrugen. In Frankreich und Amerika erschienen Übersetzungen; nach der italienischen Übersetzung folgten weitere Übersetzungen u. a. ins Spanische, Portugiesische, Polnische, Russische. Trotz seiner altphilologischen Schulung und trotz seiner Nähe zur Weimarer Klassik Goethes und zu Wilhelm von Humboldt vertrat Schlegel die Ansicht, dass die großen mittelalterlichen bzw. frühneuzeitlichen Dichter Dante, Cervantes, Calderón, Shakespeare die maßgeblichen Vorbilder der modernen Poesie seien.

Mit Madame de Staël, in die er offenbar unglücklich und eifersüchtig verliebt war, ging Schlegel im Mai 1808 nach Dresden und Weimar. Da ihm vorgeworfen wurde, Feind Napoleons, Frankreichs und der französischen Literatur zu sein, wurde er vom Präfekten aus dem ganzen französischen Reich, ja selbst aus Coppet ausgewiesen. 1811 schloss er mit der zweibändigen Sammlung seiner „Poetischen Werke“ seine dichterische Tätigkeit im großen und ganzen ab. Mit ihr entkam er über Wien, Kiew, Moskau und Sankt Petersburg nach Stockholm. Er trat als Regierungsrat und Sekretär in die Dienste Jean Baptiste Bernadottes, des künftigen schwedischen Königs. Aus sicherer Entfernung beteiligt sich Schlegel von Schweden aus an der politischen Publizistik gegen Napoleon. Er publizierte u. a.: "Über das Continentalsystem und den Einfluß desselben auf Schweden". Im Frühling 1813 folgte er Bernadotte ins Hauptquartier der Nordarmee nach Stralsund. 1813 beschäftigte er sich mit der Völkerschlacht bei Leipzig. Während der Herrschaft der Hundert Tage war er mit Madame de Staël in Paris, bis sie Napoleons Rückkehr von Elba im März 1815 wieder nach Coppet zurücktrieb. Im nächsten Jahr beschäftigte er sich in Florenz mit etymologischen, antiquarischen und kunstgeschichtliche Studien. Er blieb bis zu ihrem Tod bei Madame de Staël und blieb auch danach mit deren Tochter Albertine und ihrem Ehemann Herzog Victor de Broglie und deren Kindern eng verbunden (sie besuchten ihn 1834 in Bonn). Als Madame de Staël 1817 starb, heiratete er in Heidelberg Sophie Paulus, Tochter des Theologen Heinrich Eberhard Gottlob Paulus. Er zog nach Bonn, wo er Professor für Literatur- und Kunstgeschichte an der neu gegründeten Universität wurde. Da er Sophie nicht bewegen konnte, ihm dahin zu folgen, scheiterte die Ehe schon nach einigen Wochen.

1818 wurde er Inhaber des ersten Lehrstuhls für Indologie in Deutschland an der neu gegründeten Universität Bonn. Im selben Jahr unternahm er eine Rhein-Reise mit seinem Bruder Friedrich. In Bonn war er 1819/1820 der Literatur-Lehrer von Heinrich Heine. August Wilhelm Schlegel hatte sich in Paris Buchstaben für den Satz des indischen Devanagari-Alphabets herstellen lassen, um damit die ersten Sanskrit-Texte in Europa zu drucken. Das erste Buch war 1823 die Bhagavad Gita mit einer lateinischen Übersetzung von Schlegel selbst. Zwischen 1818 und 1825 arbeitete er an einer „Indischen Bibliothek“. Den Satz und Druck finanzierte Schlegel selbst. Von 1829 bis 1838 erschien die lateinische Übersetzung des Ramayana in drei Bänden, 1829 bzw. 1831 erschien die lateinische Übersetzung des Hitopadesha in zwei Bänden. Der Norweger Christian Lassen setzte als sein Schüler und Nachfolger diese Arbeit fort. Seine Berühmtheit wusste Schlegel mit seinem residenzartigen Haus in der Sandkaule 529 in Bonn zu unterstreichen. Sein Auftreten mit Kalesche, Diener und in modischem Pariser Anzug machte seine Eitelkeit in Bonn sprichwörtlich. 1824/25 amtierte er als Rektor der Universität.

Er gehörte wie Humboldt und Franz Bopp zu den Begründern der Komparatistik, d. h. der modernen komparativen Linguistik und Philologie. Aber nicht nur die reine Philologie interessierte ihn. Ausdrücklich schreibt er, dass er sich von den vergleichenden Sprachforschungen auch Aufschlüsse über die „Naturgeschichte des Menschen“ und die „Blutsreinheit“ der von ihm beschriebenen Menschengruppen erhoffe ("pureté du sang", vgl. "Oeuvres").

Mit zunehmendem Alter wurde der berühmt gewordene Schlegel häufig Ziel von Kritik (etwa seines Schülers Heinrich Heine), der seine Eitelkeit und Ehe mit Sophie Paulus verspottete. Seine 1827 in Berlin gehaltenen "Vorlesungen über die Theorie und Geschichte der bildenden Künste" waren ein Misserfolg. Der Zwiespalt, der sich zwischen den Brüdern auftat, wurde nicht mehr überbrückt und führte 1828 zur öffentlichen Distanzierung August Wilhelms von Friedrich. 1841 reist er erneut nach Berlin, wegen der Herausgabe der Gesammelten Werke Friedrichs des Großen, kehrte aber nach einem Semester nach Bonn zurück.

Er starb am 12. Mai 1845 in Bonn; sein Grab befindet sich dort auf dem Alten Friedhof.

August Wilhelm Schlegel gilt als der wichtigste Sprachphilosoph der deutschen Frühromantik sowie als Mitbegründer der altindischen Philologie. Er war Mitarbeiter an Schillers "Horen", dem "Musenalmanach" und der "Jenaer Allgemeinen Literatur-Zeitung". Mit seinem Bruder Friedrich teilte er sich die Herausgeberschaft der Zeitschrift "Athenäum". Später war er Herausgeber der "Indischen Bibliothek". An literarischen Werken verfasste er Sonette, Balladen und Dramen. Blieben seine eigenen literarischen Werke auch unbedeutend und ohne Erfolg, so sind seine Verdienste für die deutsche Literatur als Übersetzer, zum Teil gemeinsam mit Ludwig Tieck (und dessen Tochter Dorothea sowie Wolf von Baudissin), unbezweifelbar und maßgebend. August Wilhelm Schlegel gilt zusammen mit seinem Bruder Friedrich als wichtigster Initiator der literarischen Romantik in Deutschland. Beide versammelten einen Kreis hochrangiger Literaten, wie Novalis, Ludwig Tieck oder Friedrich Wilhelm Joseph Schelling um sich und legten das Fundament für eine literarische Strömung, die das erste Drittel des 19. Jahrhunderts beherrschte und auch danach noch zahlreiche Anhänger fand.


Straßen in Deutschland wurden nach ihm benannt u. a. in Lünen an der Lippe.


Große Teile des Nachlasses August Wilhelm Schlegels erwarb 1873 die Königliche Bibliothek zu Dresden. Weitere Nachlassteile wurden 1998 aus Schweizer Privatbesitz bei Christie’s in London für die Sächsische Landesbibliothek – Staats- und Universitätsbibliothek Dresden (SLUB) mit Sondermitteln des Freistaates Sachsen ersteigert und 2004 erstmals der Öffentlichkeit präsentiert. Damit besitzt die SLUB mit rund 650 von insgesamt rund 3100 Briefen an Schlegel sowie Manuskripten zu Gedichten, Übersetzungen, Vorlesungen, Kritiken und wissenschaftlichen Beiträgen den größten Teil seines schriftlichen Nachlasses.



Digitalisate und Volltexte
Korrespondenz


</doc>
<doc id="14663" url="https://de.wikipedia.org/wiki?curid=14663" title="Band">
Band

Das Band (Mehrzahl: "Bänder") steht für:



Der Band (Mehrzahl: "Bände") steht für:

Die Band (englisch; Mehrzahl: "Bands") steht für:


band steht für:

Band ist der Name folgender geographischer Objekte:

Band ist der Familienname folgender Personen:

BAND steht als Abkürzung für:

Siehe auch:


</doc>
<doc id="14664" url="https://de.wikipedia.org/wiki?curid=14664" title="Z">
Z

Z bzw. z [] ist der 23. und letzte Buchstabe des klassischen und der 26. und ebenfalls der letzte Buchstabe des modernen lateinischen Alphabets. Er ist ein Konsonant. Der Buchstabe Z hat in deutschen Texten eine durchschnittliche Häufigkeit von 1,13 %. Er ist damit der 20-häufigste Buchstabe in deutschen Texten.

Das Fingeralphabet für Gehörlose bzw. Schwerhörige stellt den Buchstaben "Z" dar, indem die geschlossene Hand vom Körper weg zeigt während der Zeigefinger nach oben weist und ein 'Z' in die Luft in Form einer Zick-Zack-Bewegung schreibt.

Der Ursprung des Buchstabens im proto-semitischen Alphabet ist das Symbol Ze, das eine Stichwaffe symbolisiert. Im phönizischen Alphabet wurde der Buchstabe leicht abgewandelt und bekam den Namen Zajin (Sajin), das Dolch oder Waffe bedeutet. Der Lautwert des Buchstabens im phönizischen Alphabet war das stimmhafte S [z]. Bei den Phöniziern war das Zajin an der siebten Stelle des Alphabets.

In das griechische Alphabet wurde der Buchstabe als Zeta übernommen. Bei den Griechen stand das Zeta für den Lautwert [z] und für die Affrikate [dz]. In verschiedenen Dialekten wurde das Zeta auch als stimmhafter oder stimmloser dentaler Frikativ [ð] oder [θ] ausgesprochen (wie das englische "th:" " thin" [θɪn], „dünn“; " this" [ðɪs], „dies“). Zu Beginn hatte das Zeta noch die dem I ähnliche Form des Zajin, bis zur klassischen Zeit kippte der Längsbalken allerdings nach rechts, wahrscheinlich weil es so schneller zu schreiben war, möglicherweise auch wegen der Ähnlichkeit zum Iota.

Die Etrusker übernahmen das Zeta in der dem I ähnlichen Form in ihr Alphabet. Da das Etruskische allerdings keine stimmhaften Verschlusslaute kannte, wurde aus [dz] ein [ts]. Dieser Buchstabe wurde auch von den Römern übernommen. Allerdings war die Affrikate [ts] im Lateinischen nicht vorhanden und so wurde im 5. Jahrhundert v. Chr. das I-förmige Z im Alphabet durch das (aus dem Buchstaben C) neu entstandene G mit dem Lautwert [g] ersetzt.

Das griechische Zeta wurde im ersten Jahrhundert von den Römern zusammen mit dem Ypsilon ins lateinische Alphabet aufgenommen, um die griechischen Wörter und Eigennamen korrekt wiedergeben zu können. Daher ihr Platz am Ende des Alphabets. Marcianus Capella berichtet, der revolutionäre Staatsmann Appius Claudius Caecus habe das Z abgelehnt: „Z idcirco Appius Claudius detestatur, quod dentes mortui, dum exprimitur, imitatur.“ Der weitgeöffnete Mund des Toten entspricht der Haltung der Zähne beim Aussprechen des Z. Um im Bild zu bleiben: Es beginnt das Leben mit dem staunenden „A“ und schließt mit dem „Z“. A bis Z.

Eine graphische Variante ist das „Z mit Unterschlinge“ oder auch „geschwänztes Z“, welches in Anlehnung an die gebrochenen Schriften (wie Fraktur) auch in lateinischen Schreibschriften auftaucht. In einigen lateinischen Antiqua- und Grotesk-Schriften ist diese Variante als Einzelbuchstabe ähnlich „ʒ“ und in Ligaturen (z. B. „ſʒ“ zu „ß“) anzutreffen.

Eine graphische Variante des "Z mit Unterschlinge" ist der lateinische Buchstabe Ezh (Ʒ), wie er im Internationalen Phonetischen Alphabet als Symbol für den stimmhaften postalveolaren Frikativ verwendet wird.

Eine weitere graphische Variante ist das ȥ "(Z mit Haken)," ein Graphem welches in der Standardtranskription des Mittelhochdeutschen für den mittelhochdeutschen koronalen Frikativ gedacht ist, im Gegensatz zu der mit Z "(Z ohne Haken)" bezeichneten Affrikate.

Bis in die mittelhochdeutsche Zeit schrieb man denjenigen Laut, der bei der 2. Lautverschiebung aus kurzem "t" entstanden war, mit "z" bzw. mit "zz:" mittelhochdeutsch "daz, ez, ezzen" im Gegensatz zu niederdeutsch "dat, et, eten." Dieser Laut wurde vermutlich als stimmloser alveolarer Frikativ [s] ausgesprochen so wie unser heutiges stimmloses "s" und blieb lange Zeit vom alten germanischen "s" verschieden, das als stimmloser alveolopalataler Frikativ ausgesprochen wurde. Schon früh begann man, anstelle von "zz" auch "sz" zu schreiben, um es besser vom "tz" zu unterscheiden. So entstand das deutsche "ß." Im zwölften Jahrhundert fiel der "z/zz"-Laut mit dem alten "s/ss"-Laut zusammen. Das hatte zur Folge, dass schon bald die beiden Schreibungen durcheinandergebracht wurden und schließlich nach vielen Jahrhunderten die heutige Verteilung der Buchstaben "ß, ss, s" entstand.

Langes "tt" und "t" am Wort- bzw. Silbenanfang wurden hingegen zu einem ts-Laut, dem in der Lautschrift, der bis heute so ausgesprochen wird, z. B. in "Zahl", "sitzen" im Gegensatz zu niederdeutsch "Tahl", "sitten".

In vielen Sprachen unterscheidet sich die Aussprache des „z“ vom deutschen Zett und entspricht "dem stimmhaften S", das in der "IPA-Lautschrift" als dargestellt wird. Einige Beispiele sind , "zero" oder , Émile Zola, und für das Griechische siehe die Beispiele bei Zeta.



</doc>
<doc id="14665" url="https://de.wikipedia.org/wiki?curid=14665" title="Horn (Begriffsklärung)">
Horn (Begriffsklärung)

Horn steht für:


Horn ist der Name folgender geographischer Objekte:


Verwaltungsgebiete:


Gemeinden:


Gemeindeteile, Orte:



Weiteres:


Baulichkeiten:
Horn, Weiteres:
Siehe auch:


</doc>
<doc id="14666" url="https://de.wikipedia.org/wiki?curid=14666" title="Das Lied der Deutschen">
Das Lied der Deutschen

Das Lied der Deutschen, auch Deutschlandlied genannt, wurde von August Heinrich Hoffmann von Fallersleben am 26. August 1841 auf Helgoland gedichtet.

Der konkrete Anlass für Hoffmann, das Lied zu verfassen, waren französische Gebietsansprüche auf das Rheinland in der Rheinkrise. Diese Ansprüche wies er mit dem Lied zurück, wie es auch mit anderen deutschen "Rheinliedern" dieser Zeit geschah. Er ergänzte dies mit weiteren Gedanken, vor allem mit dem der deutschen Einigkeit, die allein die Voraussetzung für Abwehr feindlicher Angriffe jeder Größenordnung bieten könne (erste Strophe). Der Dichter schuf sein Werk ausdrücklich zur Melodie des älteren Liedes "Gott erhalte Franz, den Kaiser" von Joseph Haydn (1797). Das Lied entstand auf einer Reise Hoffmanns auf die damals britische Insel Helgoland. Für lange Zeit war es jedoch nur eines der vielen Lieder der deutschen Nationalbewegung. 

Größere Bedeutung erlangte das Lied erst im Ersten Weltkrieg, als die Oberste Heeresleitung (OHL) verlautbaren ließ, es sei bei einem Gefecht in der Nähe des belgischen Ortes Langemarck nördlich von Ypern spontan von deutschen Soldaten angestimmt worden. Die OHL kommentierte die Ereignisse vom 10. November 1914 am folgenden Tag mit einem – offensichtlich propagandistisch formulierten – folgenreichen Bericht, der von fast allen deutschen Zeitungen auf der ersten Seite abgedruckt wurde: 

Dieser Bericht der OHL wurde von großen Teilen der deutschen Öffentlichkeit unkritisch aufgenommen und löste die Entstehung des sogenannten Mythos von Langemarck über den heldenhaften Opfergang junger Soldaten aus. Erst am 11. August 1922, in der Weimarer Republik, wurde das Deutschlandlied mit allen drei Strophen auf Veranlassung des sozialdemokratischen Reichspräsidenten Friedrich Ebert zur offiziellen Nationalhymne Deutschlands bestimmt. 

Kurz nach dem verlorenen Ersten Weltkrieg entstand zusätzlich noch eine „vierte Strophe“, die aber niemals Bestandteil der Nationalhymne war. Sie fand unter anderem Aufnahme in der "Weltkriegs-Liedersammlung" (1926), im "Liederbuch der Deutschen Kriegsmarine" (1927) und im "Schlesier-Liederbuch" (1936). Dort wird als Verfasser Albert Matthai genannt. Matthai schrieb diese Strophe unter dem Eindruck der Versailler Friedensverträge, die für Deutschland harte Sanktionen wie Gebietsabtretungen und hohe Reparationszahlungen mit sich brachten. Sie wurde bis in die 1930er Jahre in Frontkämpferverbänden wie dem „Stahlhelm“ und unter Deutschnationalen gesungen.

Zur Zeit des Nationalsozialismus (1933–1945) wurde nur die erste Strophe gesungen, auf die danach stets das Horst-Wessel-Lied folgte. 

Nach 1945 kam es zu Diskussionen über die weitere Verwendung des Liedes, bis 1952 ein offizieller Briefwechsel zwischen Bundespräsident Theodor Heuss (FDP) und Bundeskanzler Konrad Adenauer (CDU) dahingehend entschied, dass "Das Lied der Deutschen" insgesamt die Nationalhymne blieb, zu offiziellen Anlässen jedoch nur die dritte Strophe gesungen werden sollte. Nach der Wiedervereinigung wurde im Jahr 1991 nach einem weiteren Briefwechsel zwischen Bundespräsident Richard von Weizsäcker (CDU) und Bundeskanzler Helmut Kohl (CDU) die dritte Strophe zur Nationalhymne Deutschlands erklärt.

<poem style="font-style:italic; margin-left:2em;">
Das Lied der Deutschen

Deutschland, Deutschland über alles,
Über alles in der Welt,
Wenn es stets zu Schutz und Trutze
Brüderlich zusammenhält,
Von der Maas bis an die Memel,
Von der Etsch bis an den Belt –
Deutschland, Deutschland über alles,
Über alles in der Welt!

Deutsche Frauen, deutsche Treue,
Deutscher Wein und deutscher Sang
Sollen in der Welt behalten
Ihren alten schönen Klang,
Uns zu edler Tat begeistern
Unser ganzes Leben lang –
Deutsche Frauen, deutsche Treue,
Deutscher Wein und deutscher Sang!

Einigkeit und Recht und Freiheit
Für das deutsche Vaterland!
Danach lasst uns alle streben
Brüderlich mit Herz und Hand!
Einigkeit und Recht und Freiheit
Sind des Glückes Unterpfand –
Blüh’ im Glanze dieses Glückes,
Blühe, deutsches Vaterland!
</poem>

Das Heilige Römische Reich Deutscher Nation ging in den Napoleonischen Kriegen unter, nicht zuletzt aufgrund internen Streites. Hatte man sich am Anfang sowohl von der Französischen Revolution als auch von den Reformen Napoleons viel versprochen, so wich dies bald einer Ernüchterung und Ablehnung angesichts der als Erniedrigung empfundenen Herrschaft des Franzosen und seiner Günstlinge. Viele Untertanen in Preußen, Sachsen, Bayern usw. suchten nach deutschen Gemeinsamkeiten: Schon vor den Befreiungskriegen wurde die Walhalla konzipiert, "Die Hermannsschlacht" geschrieben und die Frage "Was ist des Deutschen Vaterland?" gestellt. Das Metternichsche System verhinderte jedoch jahrzehntelang innenpolitische Reformen und nationale Einheit.

Die Mitgliedstaaten des 1815 gegründeten Deutschen Bundes behielten zwar ihre Souveränität, doch schlossen sich die meisten nach und nach dem Deutschen Zollverein an. Politisch blieb die deutsche Frage jedoch unbeantwortet. Im Februar 1840 dichtete August Heinrich Hoffmann, der sich nach seinem Geburtsort "von Fallersleben" nannte, sein „unpolitisches Lied“ "Der Deutsche Zollverein", das ebenfalls zur Haydn-Melodie gesungen wird. Beginnend mit „Schwefelhölzer, Fenchel, Bricken“ stellt er darin fest, dass der freie Warenaustausch von „deutschen Sachen“ (er zählt hier 33 Waren auf) mehr zu einem Bewusstsein deutscher Einheit beigetragen habe als der Deutsche Bund dies getan hätte.

Im Sommer des Jahres 1840 erlitt Frankreich eine außenpolitische Niederlage in der Orientkrise gegen eine Koalition aus Großbritannien, Russland, Österreich und Preußen. Die französische Öffentlichkeit fühlte sich gedemütigt; es war von einem „diplomatischen Waterloo“ die Rede. Das Kabinett von Adolphe Thiers lenkte die wachsende nationale Empörung gegen die Verträge des Wiener Kongresses von 1815 und gegen die benachbarten, zersplitterten deutschen Staaten: Anstelle von Eroberungen im Orient war das neue Ziel das gesamte deutsche Westufer des Rheins stromabwärts von Elsass-Lothringen, das 150 Jahre zuvor zu Beginn der deutsch-französischen Erbfeindschaft von Ludwig XIV. erobert worden war. Der Wiener Kongress hatte Elsass-Lothringen der wiederhergestellten französischen Monarchie belassen, um diese nicht zu schwächen.

Nun forderte Thiers die gesamten linksrheinischen deutschen Gebiete, in denen die Franzosen schon früher vier Départements errichtet hatten (Linkes Rheinufer). Man drohte dem Deutschen Bund offiziell und in der Presse monatelang mit Krieg und rüstete militärisch und moralisch auf. Französische Geistesgrößen wie etwa Edgar Quinet und Victor Hugo schlossen sich der Forderung nach der Rheingrenze an. Diese Rheinkrise sorgte für ein Aufleben der deutschen nationalen Bewegung, die zur Verteidigung beider Rheinufer aufrief. In Anlehnung an das "Rheinlied" von Nikolaus Becker entstanden weitere sogenannte Rheinlieder, wie "Die Wacht am Rhein" von Max Schneckenburger oder Ernst Moritz Arndts "Kriegslied gegen die Wälschen".

In diesem Zusammenhang entstand auch das Lied der Deutschen, dessen Text Hoffmann von Fallersleben 1841 verfasste. Anders als in den Rheinliedern wird im "Lied der Deutschen" jedoch weder Frankreich noch der Rhein genannt; Hoffmann zählt aber vier andere Gewässer auf, die den damaligen deutschen Sprachraum umreißen.

In einer Persiflage aus nationalsozialistischer Sicht tauchte 1922 dann hingegen der Rhein auf:
„Alles, alles über Deutschland.
Feinde ringsum in der Welt,
Weil es nicht zum Schutz und Trutze
Brüderlich zusammenhält.
Welsch der Rhein, die Weichsel polnisch,
Nicht mehr deutsch das deutsche Meer,
Sklavenketten trägt Germania
Schmachvoll ohne Wehr und Ehr.

Der französische Philosoph Alfred Fouillée übersetzte die dritte Zeile der ersten Strophe mit "„pour se défendre et attaquer“" („um sich zu verteidigen und anzugreifen“), während Hoffmann von Fallersleben einen Pleonasmus für „sich verteidigen“ verwendet hatte. Die Fehlübersetzung ändert den Charakter der Strophe; die defensive Absicht („zu Schutz und Trutze“) wird in eine aggressive "(attaquer)" umgedeutet.

In der Zeit des Vormärz (ca. 1830 bis zur Märzrevolution 1848) waren die nationale Einigung und der Wunsch nach Überwindung der Fürstenherrschaft, nach Volkssouveränität, politischer Freiheit und Selbstbestimmung Ziele der liberalen Opposition. Daher werden in der dritten Strophe die Freiheit und Brüderlichkeit der Deutschen und das Recht im Sinne der Rechtsstaatlichkeit beschworen.

Das besungene „Deutschland“ wird durch den Vers „Von der Maas bis an die Memel, von der Etsch bis an den Belt“ geographisch umgrenzt. Mit der Auswahl ein- und zweisilbiger Gewässernamen, entsprechend dem Versmaß, sowie mit der zusätzlichen Alliteration „Maas – Memel“ ist Hoffmann von Fallersleben eine besonders einprägsame Formulierung geglückt.

Von den genannten vier Gewässern (drei Flüsse und eine Meerenge) markierten zwei auch die damaligen Grenzen des Deutschen Bundes:

Die beiden anderen Gewässer begrenzten Territorien, die damals (noch) nicht zum Deutschen Bund gehörten, aber von der deutschen Nationalbewegung aufgrund der dortigen deutschsprachigen Bevölkerung als Teil des zu schaffenden Deutschlands betrachtet wurden:

Die deutsche Sprachgrenze war nicht deutlich umrissen, am schärfsten noch in Südtirol aufgrund der klaren Ränder der Gebirgstäler sowie der Salurner Klause. Zum Niederländischen bestanden damals an der "Maas" (wie überall) nur fließende Übergänge, zumal nördlich der Benrather Linie bedeutende Bevölkerungsanteile im Lebensalltag ihre angestammte niederdeutsche Sprache verwendeten. Im Norden war das Dänische weiter verbreitet als heute, am Ufer des "Belt" war aber durchaus auch Deutsch üblich. Nördlich der "Memel" wurde in einigen ländlichen Gebieten auch mehrheitlich Litauisch gesprochen.

Der Dichter vermied es, an zwei wunde Punkte zu rühren, die damals kontrovers diskutiert wurden, nämlich an die Abgrenzung gegenüber Frankreich (Elsass und Lothringen) im Südwesten, und im Südosten an die Trennungslinie zwischen den deutschsprachigen Gebieten Österreichs und den slawischen, ungarischen und rumänischen Landesteilen der Habsburgermonarchie, zu der auch deutsche Siedelungsinseln wie Siebenbürgen oder das Banat gehörten. Die Etsch als „Südgrenze“ steht, obwohl in die Adria mündend, nur für Südtirol, während das österreichische Kaiserreich damals weiter nach Süden reichte.

Heute befindet sich die Sprachgrenze insbesondere im Osten weit von der Memel entfernt. Die Staatsgrenzen der Bundesrepublik stimmen seit den Vereinbarungen im 1990 geschlossenen deutsch-polnischen Grenzvertrag mit Wirkung zum 16. Januar 1992 auch völkerrechtlich und endgültig an keiner Stelle mit den geographischen Angaben des Liedes überein; die Maas fließt aber streckenweise nur wenige Kilometer westlich der deutsch-niederländischen Grenze. Nordschleswig musste 1920 abgetreten werden, die deutsche Minderheit macht dort heute noch etwa 6 % der Bevölkerung aus. Südtirol fiel nach dem Ersten Weltkrieg an Italien und wurde zur heutigen autonomen Provinz „Alto Adige“ („Hoch-Etsch“), Deutsch ist – neben Italienisch – inzwischen wieder Amtssprache. Die größte Verschiebung der deutschen Sprachraumgrenzen erfolgte im Osten durch die Vertreibungen nach dem Zweiten Weltkrieg, in deren Folge die Oder-Neiße-Grenze geschaffen wurde. Die Memel ist Grenze zwischen dem russischen Kaliningrader Gebiet und Litauen.

Mit dem Einfall, die Grenzen Deutschlands im Lied durch den Verlauf von Flüssen zu paraphrasieren, griff Hoffmann von Fallersleben auf eine Idee Walthers von der Vogelweide zurück, der – wahrscheinlich im Jahr 1198 oder kurz danach – in seinem "Ir sult sprechen willekomen" formuliert hatte:

In seiner Kinderhymne "(Anmut sparet nicht noch Mühe)", die Bertolt Brecht 1950 anlässlich der ins Auge gefassten Wiedereinführung des "Liedes der Deutschen" als Nationalhymne der Bundesrepublik als bewussten Gegenentwurf dichtete, aktualisierte er den geografischen Bezug mit den Versen "„Von der See bis zu den Alpen / Von der Oder bis zum Rhein“". Entsprechend dem damals auch in der DDR erhobenen gesamtdeutschen Vertretungsanspruch bezog sich diese Abgrenzung auf jene im Ergebnis des Zweiten Weltkriegs entstandenen Grenzen, die im Wesentlichen mit denen der Bundesrepublik seit 1990 identisch sind.

Auch für die zweite Strophe ließ sich Hoffmann von Fallersleben vom Preislied "Ir sult sprechen willekomen" des Walther von der Vogelweide inspirieren. In einem Brief vom 27. August 1841 an seine unerfüllte Jugendliebe Henriette von Schwachenberg aus Westfalen schrieb er:
Schon im Juni 1841 nahm Hoffmann von Fallersleben in seinem in den "Unpolitischen Liedern" veröffentlichten Gedicht "Eins und Alles" Bezug auf den Gedanken eines geeinten Deutschland jenseits der Einzelinteressen von Fürsten:

<poem style="font-style:italic; margin-left:2em;">
Deutschland erst in sich vereint!
Auf! wir wollen uns verbinden,
Und wir können jeden Feind
Treuverbunden überwinden.
</poem>

Die „Einigkeit“ in der dritten Strophe ist vermutlich von den Worten des sterbenden Attinghausen in Schillers "Wilhelm Tell („Seid einig – einig – einig“)" und von Seumes Gedicht "An das deutsche Volk" beeinflusst "(„[…] Hass und Spaltung herrscht in unsern Stämmen, Einheit nur kann das Verderben hemmen […]“)".

Das Lied der Deutschen ist unter anderem von seinem Autor auch als Trinklied verstanden worden, was den Lobgesang auf deutschen Wein, deutsche Frauen und deutschen Sang in der zweiten Strophe erklärt. Der Autor hat in seiner eigenen Niederschrift als Alternative zu
<poem style="font-style:italic; margin-left:2em;">
Blüh’ im Glanze dieses Glückes, blühe, deutsches Vaterland!
</poem>

auch den Trinkspruch

<poem style="font-style:italic; margin-left:2em;">
Stoßet an und ruft einstimmig: Hoch das deutsche Vaterland!
</poem>

vorgesehen.

Das Lied der Deutschen wurde seit seiner Entstehung meist nach der von Joseph Haydn zu Worten von Lorenz Leopold Haschka für Kaiser Franz II. komponierten Hymne "Gott erhalte Franz, den Kaiser" gesungen.
In den ersten Jahrzehnten nach 1841 entstanden noch 58 weitere Vertonungen des Textes. Haydn ließ sich möglicherweise von dem kroatischen Volkslied "Vjutro rano se ja stanem" inspirieren, mit dessen ersten drei Takten die Hymne beginnt. Erstmals aufgeführt wurde sie am 12. Februar 1797 im Wiener Burgtheater anlässlich des 29. Geburtstags Franz’ II. (des späteren "Doppelkaisers" Franz I.). Nachdem dieser als Kaiser von Österreich 1804 das Kaisertum Österreich gegründet hatte, war sie bis 1918 die Melodie der österreichischen Kaiserhymne, deren Text jeweils an den herrschenden Kaiser angepasst wurde.

<score> \relative c'
f4 g f8[ d] bes4 | as' g f8[ d] bes4 | bes' as g4. g8 | a4 a8[ bes] bes2 |
\repeat volta 2 { es4. d8 d[ c] bes4 | c4. bes8 bes[ as] g4 |
\addlyrics {
für das deut -- sche Va -- ter -- land!
\new Lyrics
Ei -- nig -- keit und Recht und Frei -- heit
sind des Glü -- ckes Un -- ter -- pfand.
Blüh’ im Glan -- ze die -- ses Glü -- ckes,
blü -- he, deut -- sches Va -- ter -- land!
}</score>

Hoffmann von Fallersleben wurde 1841 auf Helgoland von seinem Hamburger Verleger Julius Campe besucht, der ihm das Lied abkaufte. Er berichtet 1868 darüber:

Ab 3. Oktober 1841 hielt sich der liberale badische Politiker Karl Theodor Welcker in Streit’s Hotel am Jungfernstieg in Hamburg auf. Am 5. Oktober „abends 10 ½“ wurde ihm „ein Ständchen gebracht“. Im Beisein Hoffmanns sangen Mitglieder der Hamburger Liedertafel und der Hamburger Turnerschaft von 1816 „bei Fackelschein und mit Hornmusik“ des Hamburger Bürgermilitärs vor dem Hotel erstmals öffentlich „Deutschland, Deutschland über alles“. Der Schweizer François Wille (1811–1896) brachte ein Hoch auf Welcker aus. Zum Schluss sang man Hoffmanns "Deutsche Worte hör’ ich wieder (Rückkehr aus Frankreich)" und Karl Follens "Bundeslied" „Brause, du Freiheitssang“, begrüßte Welcker und überreichte ihm das „Lied der Deutschen“.

Im folgenden Jahr nahm Hoffmann den Liedtext in seinen Band "Deutsche Lieder aus der Schweiz" auf. Obwohl Campe im Erstdruck den Hinweis „Text Eigentum des Verlegers“ angebracht hatte, wurde das Lied mangels damaliger Rechtsverbindlichkeit bald nachgedruckt und fand seinen Weg in zahlreiche Kommers- und andere Liederbücher.

In seiner Entstehungszeit war das Lied wenig beachtet, und zwar weil erstens mit der Beilegung der Orientkrise im Sommer 1841 die Kriegsgefahr gebannt war und zweitens das Lied nicht wie das "Rheinlied" ein Kampflied war, sondern mehr besinnlich als kriegerisch wirkte. Auch nach der Reichsgründung von 1871 wurde das bis dahin bereits in Preußen übliche Lied "Heil dir im Siegerkranz" im Sinne einer Nationalhymne verwendet und die Hymne Hoffmanns von Fallersleben durch die Krone als republikanisch abgelehnt. Beim Volk war zu dieser Zeit dagegen auch "Die Wacht am Rhein" beliebt. Eine offizielle Hymne gab es nicht. Das Lied der Deutschen war damals ein beliebtes patriotisches Lied unter mehreren. Bei einer offiziellen Gelegenheit wurde es erstmals 1890, bei der Feier anlässlich der Übernahme von Helgoland (infolge des Helgoland-Sansibar-Vertrags), aufgeführt. In der Folge dieses Vertrages gründete sich 1891 der Alldeutsche Verband, der die imperialen Expansionsbestrebungen aufnahm und kanalisierte und das „über alles in der Welt“ in diese Richtung interpretierte. Seitdem wertete man zunehmend auch in Großbritannien das „über alles“ als Zeichen des Expansionsstrebens. Wie Victor Klemperer später in seinem "LTI – Notizbuch eines Philologen" schrieb, gab es aber noch eine andere Deutung; für ihn drückten die Worte im Ersten Weltkrieg „nur die Wertschätzung des Gemüts, die der Patriot seinem Vaterland entgegenbringt“, aus.

Erst in der Weimarer Republik, am 11. August 1922, wurde das "Lied der Deutschen" mit allen drei Strophen vom sozialdemokratischen Reichspräsidenten Friedrich Ebert zur Nationalhymne erklärt.

Nach der Machtübernahme der Nationalsozialisten 1933 wurden die zweite und die dritte Strophe des Liedes der Deutschen nicht mehr bei öffentlichen Anlässen gesungen. Wenn die Nationalhymne gespielt und gesungen wurde, folgte in der Regel das Horst-Wessel-Lied, die Parteihymne der Nationalsozialisten; 1940 wurde dies Vorschrift. Das Ziel war die Symbolisierung der Einheit zwischen NSDAP und Staat. Mit dem Beibehalten wenigstens eines Teiles des Liedes der Deutschen knüpften die Nationalsozialisten aber nicht etwa an die Tradition der Weimarer Republik an: Vielmehr ging ihre Begeisterung für das Lied auf den "Mythos von Langemarck" zurück, nach dem deutsche Soldaten im Ersten Weltkrieg "Deutschland, Deutschland über alles" singend in die Schlacht gezogen sind. Die von den Nationalsozialisten verwendeten Symbole versuchten häufig, eine Kontinuität mit der Zeit vor der Weimarer Republik herzustellen. Dies gilt nicht nur für die Hymne, sondern bereits für den Begriff „Drittes Reich“. Die Hakenkreuzflagge der Nationalsozialisten griff die Farben Schwarz-Weiß-Rot auf, die Farben des Deutschen Reiches von 1871.

Nach Kriegsende verbot der Alliierte Kontrollrat zwar den Gebrauch charakteristischer „nazistischer oder militärischer Grußformen“, nicht aber das Lied der Deutschen oder auch nur dessen öffentlichen Gesang. Nur in der amerikanischen Zone war „das Singen oder Spielen […] irgendwelcher Militär- oder Nazi-Lieder oder […] deutscher National- oder Nazi-Hymnen“ untersagt. Es ist zweifelhaft, ob davon auch die dritte Strophe des Liedes der Deutschen erfasst war, weil sie bekanntlich in der Zeit des Nationalsozialismus keine offizielle Verwendung mehr gefunden hatte. In der französischen Zone gab es überhaupt kein Verbot, auch nicht in der britischen Zone. Eine Verordnung der Militärregierung vom 15. September 1945 verbot lediglich „das öffentliche Singen oder Spielen militärischer oder Nazi-Lieder oder Melodien“. 1949 wurden diese Verbote von der Alliierten Hohen Kommission aufgehoben.

1921 schrieb Albert Matthai als Reaktion auf den verlorenen Weltkrieg und den Versailler Vertrag eine Ergänzung, die Aufnahme in das "Liederbuch der Deutschen Kriegsmarine" von 1927 fand und bis in die 1930er Jahre vor allem in Frontkämpferverbänden wie dem "Stahlhelm" und unter Deutschnationalen gesungen und von ihnen als „vierte Strophe“ bezeichnet wurde. Sie war jedoch nie Bestandteil der Nationalhymne.

<poem style="font-style:italic; margin-left:2em;">
Deutschland, Deutschland über alles
Und im Unglück nun erst recht.
Nur im Unglück kann die Liebe
Zeigen ob sie stark und echt.
Und so soll es weiterklingen
Von Geschlechte zu Geschlecht:
<nowiki>|:</nowiki> Deutschland, Deutschland über alles
Und im Unglück nun erst recht. <nowiki>:|</nowiki>
</poem>

Da die Bundesrepublik kurz nach dem Zweiten Weltkrieg noch keine Nationalhymne hatte, wurde bei einem Besuch Bundeskanzler Konrad Adenauers in Chicago aus Verlegenheit das Kölner Karnevalslied "Heidewitzka, Herr Kapitän" gespielt. Bei offiziellen Empfängen wurde auch ein anderes Lied von Karl Berbuer, der "Trizonesien-Song" – eine Anspielung auf die drei Besatzungszonen der Westmächte –, gespielt. Angeblich durch die Verwendung der „Gassenhauer“ verärgert, forderte Adenauer bereits am 18. April 1950 in einer Rede im Berliner Titania-Palast die Zuhörer auf, die dritte Strophe des Liedes der Deutschen zu singen.

Das "Lied der Deutschen" wurde 1952 durch einen im Bulletin der deutschen Bundesregierung veröffentlichten Briefwechsel zwischen Bundeskanzler Konrad Adenauer und Bundespräsident Theodor Heuss im Mai 1952 offiziell zur Nationalhymne der Bundesrepublik. Darin erklärte Heuss, dass er „der Bitte der Bundesregierung nachkomme“, die durch Adenauer „erneut“ darum gebeten hatte, „das Hoffmann-Haydn’sche Lied als Nationalhymne anzuerkennen. Bei staatlichen Veranstaltungen soll die dritte Strophe gesungen werden.“

Auf Bitte des damaligen baden-württembergischen Ministerpräsidenten Hans Filbinger nahm 1977 der Schlagersänger Heino das Lied der Deutschen mit allen drei Strophen für eine Single-Veröffentlichung auf, die nur für den Schulunterricht des Landes gedacht war, und erntete damit kritische Reaktionen.

Der Literaturwissenschaftler Jost Hermand hielt es 1979 für unmöglich, Lied und historische Rezeption zu trennen. Es genüge nicht, das "Lied der Deutschen" einfach durch einen Hinweis auf seine demokratische Vergangenheit zu rechtfertigen.

Nach der deutschen Wiedervereinigung am 3. Oktober 1990 wurde nur die dritte Strophe des "Liedes der Deutschen" Nationalhymne des vereinigten Deutschland. Bundespräsident Richard von Weizsäcker schrieb Bundeskanzler Helmut Kohl in einem Brief vom 19. August 1991: „Die 3. Strophe des Liedes der Deutschen von Hoffmann von Fallersleben mit der Melodie von Joseph Haydn ist die Nationalhymne für das deutsche Volk“, und dieser stimmte dem mit Schreiben vom 23. August 1991 „namens der Bundesregierung“ zu. Der Briefwechsel wurde zunächst im Bulletin der Bundesregierung vom 27. August 1991 veröffentlicht und als Bekanntmachung vom 19. November 1991 dann noch einmal im Bundesgesetzblatt Nr. 63 vom 29. November 1991.

Als staatliches Symbol und Verfassungswert ist die dritte Strophe des Liedes der Deutschen als Nationalhymne gemäß StGB gegen Verunglimpfung geschützt. Der strafrechtliche Schutz ist aber dadurch eingeschränkt, dass Autoren von Nachdichtungen sowie Parodien der Nationalhymne sich ihrerseits unter Umständen auf die Kunstfreiheit des Abs. 3 Grundgesetz berufen können.

Das Singen des Liedes wurde zwar nach dem Zweiten Weltkrieg von den amerikanischen Militärbehörden für kurze Zeit verboten, jedoch stellt heute das Abspielen oder Singen der ersten und zweiten Strophe – die offiziell kein Teil der Nationalhymne sind – keine strafbare oder verbotene Handlung dar; allerdings wird das Singen insbesondere der ersten Strophe in der öffentlichen Meinung zuweilen als Ausdruck einer nationalistischen Einstellung gewertet.
Zum 175. Jahrestag wurde am 6. Oktober 2016 eine 20-Euro-Gedenkmünze durch das Bundesfinanzministerium herausgegeben, auf deren Vorderseite ein Porträt von Hoffmann von Fallersleben mit der Unterschrift "EINIGKEIT UND RECHT UND FREIHEIT" zu sehen ist. Der Rand ist mit "…SIND DES GLUECKES UNTERPFAND" und "175 Jahre Deutschlandlied" beschriftet. Am selben Tag veröffentlichte das Bundesfinanzministerium über die Deutsche Post AG zudem eine Sonderbriefmarke zu 70 Cent, auf der die Nationalhymne zu lesen ist. Vorgestellt wurden beide am 26. August 2016 durch Werner Gatzer, Staatssekretär im Bundesministerium der Finanzen, auf Helgoland.

Der Nachlass des Dichters und Philologen Hoffmann von Fallersleben gelangte 1903 durch seinen Sohn in die "Königliche Bibliothek", die heutige Staatsbibliothek zu Berlin. Ein Teil dieses Nachlasses wurde im Zweiten Weltkrieg zum Schutz vor Kriegszerstörung in das schlesische Kloster Grüssau ausgelagert und gelangte nach Ende des Krieges, als das Gebiet östlich von Oder und Lausitzer Neiße unter polnische Verwaltung kam, in polnischen Besitz. Dieser Teilnachlass befindet sich heute in der Biblioteka Jagiellonska in Krakau und gehört dort zu den Berliner Beständen, den sogenannten "Berlinka". In dem kleineren Teil des Nachlasses, der weiterhin in der Handschriftenabteilung der Staatsbibliothek zu Berlin aufbewahrt wird, befindet sich eine eigenhändige Handschrift des Deutschlandliedes mit der Signatur „Nachl. Hoffmann v. Fallersleben 70“.

Ob diese Handschrift, die das eigenhändige Datum 26. August 1841 trägt, die Originalhandschrift (die Urschrift) des Liedes der Deutschen ist, lässt sich nicht feststellen. Eine weitere eigenhändige Handschrift des Liedes der Deutschen mit der gleichen Datierung findet sich in der Stadt- und Landesbibliothek Dortmund.




</doc>
<doc id="14667" url="https://de.wikipedia.org/wiki?curid=14667" title="Celsius (Begriffsklärung)">
Celsius (Begriffsklärung)

Celsius steht für:


Siehe auch:


</doc>
<doc id="14668" url="https://de.wikipedia.org/wiki?curid=14668" title="Short Message Service">
Short Message Service

Obgleich die Abkürzung SMS den Dienst "(Service)" zur Übertragung von Kurznachrichten bezeichnet, steht sie heute zumeist für die Kurzmitteilung "(Short Message)" selbst (siehe auch Metonymie). Der Duden führt die Abkürzung SMS als Femininum; in der Schweiz und teilweise auch in Österreich ist das Neutrum üblich.

Der Dienst, welcher ursprünglich als Teil des Signalisierungskanals zum Rufaufbau gedacht war, um Informationen über Störungen im GSM-Netz an die Teilnehmer zu senden, ist eigentlich ein Nebenprodukt und wurde daher oftmals kostenlos angeboten, entwickelte sich aber zum größten Ertragsbringer der Netzbetreiber. Verglichen mit einem normalen Telefongespräch haben Kurzmitteilungen einen niedrigen Bandbreitenbedarf. Die Marge pro versandter Kurznachricht ist daher für den Netzbetreiber entsprechend hoch.

Weiterentwicklungen des SMS existieren unter den Namen "Enhanced Message Service" (EMS), "Multimedia Messaging Service" (MMS) sowie "Joyn".

Erste Überlegungen zur Errichtung eines Textnachrichtendienstes gab es seit 1984 bei den europäischen Telekommunikationsgesellschaften. Die erste Version des endgültigen Standards wurde Anfang 1989 verabschiedet. Der ursprüngliche Konzeptvorschlag für einen Short Message Service wurde von Friedhelm Hillebrand von der damaligen Deutschen Bundespost mit Beiträgen von Bernard Ghillebaert von der PTT (Vorgänger der "France Télécom") 1984 erarbeitet und im Februar 1985 in die GSM-Standardisierung eingebracht. Er legte beispielsweise die Länge auf 160 Zeichen fest, weil er festgestellt hatte, dass die meisten Postkarten und Telexe weniger als 160 Zeichen enthielten. Finn Trosby von der norwegischen Telenor war von 1987 bis 1990 Leiter der Standardisierungsgruppe GSM4 DGMH (drafting group message handling), die das erste technische Design erarbeitete und standardisierte. Von 1990 bis 2009 wurde der SMS-Standard in dieser Gruppe unter Leitung von Kevin Holley von Cellnet und Ian Harris von Vodafone weiterentwickelt.

Am 3. Dezember 1992 schickte der Ingenieur Neil Papworth die erste Kurzmitteilung des Short Message Service (mit dem Text »Merry Christmas«) von einem PC an ein Orbitel TPU 901 Mobiltelefon im britischen Vodafone-Netz. Dies geschah etwa ein Jahr nach der Einführung des GSM-Standards für Mobiltelefone in Europa.

Durch die hohe Akzeptanz von SMS nehmen Kurznachrichten einen zunehmenden Einfluss auf soziale Interaktion (z. B. Terminvereinbarungen) und Sprache.

So ist "simsen" seit den 1990er Jahren im deutschsprachigen Raum auch die umgangssprachliche Bezeichnung für das Versenden von Kurzmitteilungen mit einem Mobiltelefon. Dieser Neologismus ist gerade bei der Jugend beliebt, da er deutlich kürzer ist als „eine Kurzmitteilung senden“. Entwickelt hat er sich aus dem Versuch heraus, "smsen" auszusprechen und eine angemessene, praktikable Schreibweise zu finden. Der Begriff „texten“ ist ebenfalls gebräuchlich. In Österreich, in der Schweiz und im süddeutschen Raum hat sich der Ausdruck "SMSen" (ausgesprochen als „es-em-es-en“) eingebürgert.

Um mehr Inhalt in die auf 160 Zeichen beschränkten Nachrichten zu bringen, hat sich eine weitverbreitete Abkürzungskultur entwickelt, die ursprünglich in Chats und E-Mails entstanden war und von dort übernommen wurde, siehe Netzjargon und die Liste der Abkürzungen des Netzjargons. Ebenfalls werden inzwischen Emoticons so oft in Kurznachrichten verwendet, dass viele davon in modernen Mobiltelefonen bereits vorgespeichert sind (und wahlweise auch grafisch angezeigt werden können).

Der SMS-Dienst benötigt nur wenig zusätzliche Infrastruktur, hauptsächlich sind dies die Kurzmitteilungszentralen. Die übertragene Datenmenge ist im Vergleich zur Sprachübertragung gering, eine Kurzmitteilung umfasst etwa 1/1000 der Datenmenge einer Gesprächsminute. Bei nutzungsabhängigen Tarifen ist der Preis bezogen auf die Datenmenge erheblich höher als bei der Sprachübertragung. Jedoch sind auch die Kosten für den Aufbau und Betrieb des Netzes nur zum Teil von der zur Verfügung gestellten Datenübertragungsrate abhängig.

Anfangs wurde SMS von den Netzanbietern kostenlos angeboten, was später aufgrund des wirtschaftlichen Potenzials eingestellt wurde. Innerhalb Europas schwanken die Preise für SMS sehr stark. Eine Nachricht kostet in Frankreich etwa 9 Cent bis 15 Cent, in Österreich netzintern ab 1 Cent, in Deutschland etwa 3 Cent bis 39 Cent und mehr, in der Schweiz zwischen 10 und 20 Rappen (ungefähr und  Cent) und in Italien etwa 15 Cent; in Dänemark jedoch meist nur 20 Øre, was etwa  Cent entspricht.

Im Jahr 2003 wurden in Europa über 115 Milliarden Kurzmitteilungen versendet. In Deutschland wurden davon die meisten, nämlich 25,5 Milliarden Nachrichten versendet. 2010 waren es in Deutschland schon 41,3 Milliarden. Der Erfolg des mit Hilfe von Mobiltelefonen genutzten SMS liegt damit deutlich über dem der Vorgänger, also der digitalen Funkmeldeempfänger (Pager).

Seit etwa 2013 / 2014 ist die Nutzung von Kurznachrichten stark rückläufig. Dies ist auf die zunehmende Verbreitung von internetfähigen Smartphones einerseits und auf kostenlose Instant-Messaging-Dienste (wie z. B. WhatsApp) andererseits zurückzuführen. Messaging-Dienste erleichtern es zudem, auch Sprachnachrichten, Fotos und Videos zu verschicken. Auf der anderen Seite wird die SMS beispielsweise im Online-Banking bevorzugt genutzt, um Transaktionen zu legitimieren ("mobile TAN"). Hierbei spielt eine Rolle, dass für die SMS weder eine Internet-Verbindung noch eine Registrierung bei einem Drittanbieter erforderlich sind.

Die SMS-Beliebtheit stieg in Deutschland fast jedes Jahr an. Wurden im Jahr 2000 etwa 11,4 Milliarden Kurzmitteilungen verschickt, waren es 2005 schon über 22 Milliarden.
2010 tippten die Deutschen nach Angaben des Branchenverbands Bitkom (auf Basis von Daten der Bundesnetzagentur) 41 Milliarden Kurzmitteilungen in ihre Handys und Smartphones. Nach dem weiteren starken Anstieg bis 2012 sank die Zahl der versendeten Kurzmitteilungen im Jahr 2013 erstmals um 37 Prozent auf etwa 38 Milliarden. Auch 2014 sank die Zahl der versendeten SMS erneut um 41 Prozent gegenüber dem Vorjahr auf 22,5 Milliarden. 

Seit dem Frühjahr 2003 sind in Deutschland auch "Premium-Dienste" möglich. Die Kosten für eine "Premium Rate SMS" (PR-SMS) beginnen bei 0,29 Euro und steigt dann in 10-Cent-Schritten an, bis zu 4,99 Euro. PR-SMS dient als Abrechnungsmöglichkeit im Micropayment-Bereich (zum Beispiel für gekaufte Klingeltöne, Logos, Televoting und andere, einzeln zu bezahlende Dienstleistungen), wird aber auch zur erotischen Kommunikation ("Flirtline") genutzt und steht hier im Wettbewerb zu den 0900-Telefonnummern (früher: 0190).

Der Anbieter eines kostenpflichtigen Service erhält etwa 50 bis 60 % der Einnahmen aus den Premium-Nachrichten, der Rest geht an den Mobilfunkbetreiber (vergleiche 0900-Nummer: etwa 80 bis 90 % für den Serviceanbieter). Trotz dieser erheblich schlechteren Konditionen wird in diversen Servicebereichen, die speziell von jungen Zielgruppen genutzt werden, vermehrt auf die leicht zu kommunizierenden Short Codes gesetzt.

Der Mobilfunkbetreiber ist beim Einzug des Verbindungsentgelts dem Kunden gegenüber zur Auskunft über die Anspruchsgrundlage verpflichtet.

Im Jahr 2009 wurden in Österreich erstmals über fünf Milliarden Kurzmitteilungen verschickt und eine neue Rekordmarke von 1,5 Milliarden Kurzmitteilungen im vierten Quartal aufgestellt. Insgesamt ergab sich eine Steigerung des SMS-Aufkommens von mehr als 21 % im Vergleich zum Jahr 2008. Die Gründe für dieses Wachstum lagen unter anderem darin, dass die Mobilfunkbetreiber immer häufiger Pauschalpakete anboten, die nicht nur Gesprächsminuten, sondern auch eine definierte Anzahl an Kurzmitteilungen beinhalteten (meist zwischen 100 und 1000). Ein zweiter Grund lag laut RTR-Telekom-Monitor-Jahresbericht 2011 in der zunehmenden Verbreitung von Maschine-zu-Maschine-Anwendungen.

In den Folgejahren nahm die Wachstumsrate wieder stetig ab und erreichte 2012 nur noch 6,5 %. Nach einem Maximum von 7,7 Milliarden Kurzmitteilungen im Jahr 2012 ist auch die absolute Zahl seit 2013 stark rückläufig. 

In den Vereinigten Staaten war SMS lange Zeit unbekannt, da sich dort das Pager-System etabliert hatte und Nachrichten nur innerhalb desselben Mobilfunknetzes versandt werden konnten . Dieses Hindernis wurde mittlerweile beseitigt, und so steigt die Anzahl versendeter Kurznachrichten pro Monat, während die versendeten Pager-Mitteilungen stagnieren.
Der Dienst SMS wird in den Vereinigten Staaten teilweise unter der Bezeichnung „text messaging“ vermarktet. Die Kosten variieren zwischen komplett kostenlos, 10 US-¢ (ca. Cent) pro versandter Nachricht/Empfang kostenlos und 5 US-¢ (ca. Cent) pro empfangener oder versandter Nachricht.

Kurzmitteilungen lassen sich auch über das Internet verschicken. In den ersten Jahren war der Versand von Kurzmitteilungen über das Internet, genau wie innerhalb der Mobilfunknetze selbst, kostenlos. Heute ist der Internet-Versand von Kurzmitteilungen meist zu einem Preis zwischen 4 und 10 Cent möglich, je nachdem, welche Zustellgeschwindigkeit und zusätzlichen Dienste angeboten werden. Es existieren aber auch Gratisangebote, die sich meist über Werbung finanzieren.

Letztere finanzieren sich oft über den Handel mit persönlichen Informationen, dies können unter anderem E-Mail-Adressen sein, die an externe Adress-Broker weitergegeben werden, um z. B. Werbemails zu versenden, oder komplette Datensätze des jeweiligen Users, aus denen sich Interessenprofile erstellen lassen, welche einen hohen Verkaufswert haben, da Adressbroker ihren Kunden somit Datensätze anbieten können, die bestimmte Zielgruppen maßgeschneidert enthalten. Zunehmend werden kostenpflichtige SMS-Abos abgeschlossen; der User wird dabei oft nicht ausreichend über die entstehenden Kosten aufgeklärt. Des Weiteren wird den meisten Gratis-Kurzmitteilungen ein Werbetext angehängt, wobei dem Nutzer auch weniger als die üblichen 160 Zeichen zur Verfügung stehen.
Der SMS nutzt einen Signalisierungs-Kanal des GSM-Standards wie etwa SDCCH (Stand-alone Dedicated Control Channel) oder FACCH (Fast Associated Control Channel). Diese Kanäle werden auch genutzt, um Gespräche aufzubauen und zu halten. Kurzmitteilungen kann man parallel zu einer Telefonverbindung versenden/empfangen. Hierzu wird ein Teil der Bandbreite des Verkehrsdatenkanals temporär zum Signalisierungskanal (SACCH) umkonfiguriert und zum Versand/Empfang einer Kurzmitteilung genutzt.

Der Versand einer solchen Nachricht erfolgt grundsätzlich vom Mobiltelefon an eine Kurzmitteilungszentrale (SMSC), gewöhnlich die des Netzbetreibers. Die Nummer der Kurzmitteilungszentrale hat den gleichen Aufbau wie eine „normale“ Mobilfunknummer (MSISDN = Mobile Subscriber Integrated Services Digital Network Number) und ist in den Einstellungen der SIM-Karte hinterlegt. Die Kurzmitteilungszentrale liest aus dem "Header" unter anderem die Zielnummer aus und sendet die Nachricht entweder im eigenen Netz an diese Zielnummer oder übergibt sie an den Netzbetreiber der Zielnummer. Die verschiedenen Netzbetreiber sind untereinander verbunden (interkonnektiert). Ist der Empfänger kein Mobilfunkgerät, sondern eine Anwendung (zum Beispiel im Fall einer Anmeldung bei einem Short-Message-Newsletter-Service), wird der Inhalt der Nachricht über Datenverbindungen an die Server des Service-Anbieters weitergeleitet.

Die Beschränkung auf 160 Zeichen bei einer Kurzmitteilung ergibt sich aus der maximalen Nutzdatenlänge des MAP (Mobile Application Part) des Signalisierungssystems Nummer 7. Dieses wird zur Übertragung der Kurzmitteilungen zwischen dem MSC und dem SMSC verwendet. Aufgrund einer maximalen MSU (Message Signal Unit)-Paketgröße von 272 Oktetten im Signalisierungssystem Nummer 7 ist die verfügbare Nutzdatenlänge auf 140 Oktette (140 Oktette = 140 zu je 8 bit = 1120 bit) beschränkt. Kurzmitteilungen können in verschiedenen Zeichenkodierungen übertragen werden, so beispielsweise in der in Mitteleuropa üblichen GSM-7-bit-Kodierung, mit der maximal 160 Zeichen (160·7 bit = 1120 bit) möglich sind; in 8-Bit-Kodierungen sind 140 Zeichen, in der 16-Bit-Kodierung nur 70 Zeichen möglich.

Die Verwendung auch nur eines 16-bit-Sonderzeichens ändert das Codierschema einer Kurzmitteilung und reduziert ihre (Teil-)Länge auf 70 Zeichen, was kostenrelevant sein kann.

Bei überlangen Kurzmitteilungen, sogenannten Multi-SMS (Concatenated SMS, Long SMS), werden längere Texte aufgeteilt und einzeln versendet und in der Regel jeder einzelne Teil als separate Kurzmitteilung abgerechnet. Der Empfänger (sofern dazu fähig) setzt die Teile dann wieder zu einem zusammenhängenden Text zusammen.

Eine Kurzmitteilung besteht aus zwei Teilen:

Es gibt insgesamt drei verschiedene Arten der SMS-Kodierung:

Der SMS kennt 3 Betriebsmodi:


Ist eine Nachricht nicht zustellbar, weil das Empfangsgerät ausgeschaltet ist oder sich in einem Funkloch befindet, speichert sie die Kurzmitteilungszentrale (SMSC) des Netzes, zu der die Empfängernummer gehört, für eine Vorhaltezeit zwischen mindestens 5 Minuten und maximal 63 Wochen. Die maximale Vorhaltezeit ist vom Anbieter abhängig. In regelmäßigen Abständen unternimmt die SMSC weitere Sendeversuche, bis die Nachricht nach Ablauf der Zeit schließlich gelöscht wird. Die Netzbetreiber berechnen in einem solchen Fall trotzdem die normalen Versandgebühren.

Ist eine Nachricht wegen unbekannter Empfängernummer unzustellbar, lehnt das SMSC sie bereits beim Absenden ab.

Ein Benutzer kann von einem Handy eine Kurzmitteilung verschicken und sich die Übermittlung bestätigen lassen. Dadurch erfährt der Absender, in welchem Moment der Empfänger sein Handy einschaltet, wenn es zuvor z. B. ausgeschaltet war. Da die Netzbetreiber die dafür geschaffenen Standards nicht immer einhalten, besitzen SMS-Empfangsbestätigungen nur bedingte Aussagekraft. Beispielsweise quittieren manche Netzbetreiber jede SMS aus einem Fremdnetz automatisch sofort, unabhängig davon ob das Handy eingeschaltet ist und der Empfänger die SMS erhalten hat oder auf Grund der Vorhaltezeit je erhalten wird, andere quittieren nie. Schweizer Provider unterstützen den Code *N# vor den Text gesetzt als Kommando für eine Empfangsbestätigung. Andere Codes wie *T# und #*# sind seltener anzutreffen.

Die Qualität der Nachrichtenübermittlung ist nicht in den Standards des ETSI oder in den Verträgen der einzelnen Provider spezifiziert.

Alternativ zu „normalen“ Telefonnummern kann man Kurzmitteilungen auch an sogenannte Kurzwahlnummern oder auch Shortcodes senden. Shortcodes werden direkt in der Kurzmitteilungszentrale des Netzbetreibers verwaltet, sind also nicht an eine SIM-Karte gebunden. Gerade im Marketing-Bereich nutzt man häufig Shortcodes, da sie leicht zu kommunizieren sind, einen höheren Nachrichtendurchsatz erlauben und sich bei Bedarf durch erhöhte Kosten als Abrechnungsmethode nutzen lassen (siehe wirtschaftliche Bedeutung/Anwendungen).

Es gibt ebenfalls sogenannte Langnummern (long codes, z. B. +447624556335) – auch als virtuelle Empfangsnummern bekannt. Hierbei handelt es sich auch um einen SMS-Empfangs-Mechanismus, der es einer Vielzahl von Unternehmen ermöglicht, ein hohes SMS-Aufkommen von Handy-Nutzern, aber zusätzlich auch Anrufe zu empfangen.
Neben einer internationalen Erreichbarkeit bieten Langnummern Unternehmen wie Werbeagenturen die Möglichkeit, eigene und somit personalisierte Nummern langfristig für ihre Marketingkampagnen zu verwenden. Diese können mobile Werbeaktionen, Fernsehabstimmungen, Gewinnspiel-Kampagnen etc. sein. Je nach Fokus der jeweiligen Unternehmung werden sowohl Short Codes als auch Langnummern verwendet.

Das 1998 erfundene Texteingabesystem Text on 9 keys, kurz T9 genannt, machte die Texteingabe deutlich einfacher und komfortabler, weil das Mehrfachtippen einer Taste für den richtigen Buchstaben im Regelfall entfällt. T9 basiert auf einer intelligenten Texterkennung anhand eines im Mobiltelefon gespeicherten Wörterbuches. T9 funktioniert nicht für Schweizerdeutsch, das vor allem bei der jungen Generation in der Schweiz beliebt ist.

In Motorola-Mobiltelefonen wird das ähnlich funktionierende iTap verwendet.

Abgesehen von der Funktion als Kommunikationsmittel können SMS ggf. auch zur Steuerung und Überwachung technischer Geräte verwendet werden.

So ist es beispielsweise möglich, das Öffnen eines Garagentors oder die Ausgabe eines Getränks aus einem Getränkeautomaten durch das Senden einer Kurzmitteilung an eine bestimmte Telefonnummer zu veranlassen. In Österreich ist es möglich, via SMS über den Service Paybox Zigaretten an Automaten zu erwerben. Die Abrechnung erfolgt über das Bankkonto.

Bei Alarmanlagen und bei Heizungen kann sich der Besitzer mittels vorgefertigter, in der Anlage gespeicherter Nachrichten per Kurzmitteilung alarmieren bzw. über Störungen verständigen lassen.

In Vorarlberg wird die SMS-Technik verwendet, um öffentlichen Linienbussen per Knopfdruck an der Haltestelle den Bedarf zur Anfahrt abgelegener sogenannter „Rufbushaltestellen“ zu melden.

In einigen Gemeinden wie dem nordrhein-westfälischen Dörentrup lässt sich die aus Kostengründen ausgeschaltete Straßenbeleuchtung kurzzeitig per SMS einschalten.

Beim deutschen Mautsystem verwenden die On-Board-Units SMS zum Übertragen von Abrechnungsdaten.

Das Verfassen und Lesen von Textnachrichten durch Fahrer eines Autos ist genauso bußgeldbewehrt wie das Telefonieren, wenn dafür das Telefon gemäß Abs. 1a StVO mit der Hand bedient wird. Die Gefahr, die von der Verwendung der SMS-Funktion und der damit einhergehenden Ablenkung ausgeht, ist laut einer repräsentativen Studie nicht zu vernachlässigen. Knapp 49 % aller Deutschen lesen SMS am Steuer, im internationalen Umfeld sind ähnliche Zahlen bekannt. Wegen der steigenden Unfallzahlen, die auf SMS zurückzuführen sind, startete der Mobilfunkanbieter AT&T im Juli 2012 eine Kampagne gegen das Lesen und Senden von Textnachrichten am Steuer.

Obwohl die Mobilfunknetze in Deutschland gut ausgebaut sind, kann es zum Jahreswechsel zu Netzüberlastungen und somit zu Verzögerungen von Kurzmitteilungen kommen oder gar dazu, dass es nicht mehr möglich ist, eine Kurzmitteilung abzusetzen. Weltweit wurden beim nächtlichen Übergang von 2007 zu 2008 43 Milliarden versendete Kurzmitteilungen gezählt, davon 300 Millionen in Deutschland. sagte Bitkom-Sprecher Christian Hallerberg dazu.





</doc>
<doc id="14669" url="https://de.wikipedia.org/wiki?curid=14669" title="Abitur">
Abitur

Das Abitur (von ‚davon gehen‘, aus "Abiturium", von neulat. ‚abgehen wollen‘) bezeichnet den höchsten Schulabschluss in der Bundesrepublik Deutschland und damit die allgemeine Hochschulreife.

Mit ihm wird die Befähigung zum Studium an Universitäten und gleichgestellten Hochschulen nachgewiesen.

Im Unterschied dazu ermöglicht die Fachgebundene Hochschulreife, auch als Fachgebundenes Abitur bzw. Fachabitur bezeichnet, das Studium nur fachbezogener Studiengänge an Hochschulen, während die Fachhochschulreife, welche ebenfalls umgangssprachlich als Fachabitur bezeichnet wird, ein Studium an Fachhochschulen (Hochschulen für angewandte Wissenschaft) und diesem gleichgestellter Studiengänge ermöglicht. 

In Belgien heißt das Abschlusszeugnis der Oberstufe des Sekundarunterrichts auch "Abitur". Es wird auch "Abiturdiplom" genannt und unterschieden in allgemeines, technisches oder berufliches Abitur.

In Österreich und der Deutschschweiz sowie in einer Vielzahl anderer Länder spricht man nicht von "Abitur", sondern von der "Matura" (von lat. ‚Reifeprüfungen‘; zu lat. ‚Reife‘). In Teilen der Deutschschweiz wird auch die Kurzform "Matur" verwendet.

Im 18. Jahrhundert bestimmten die Universitäten noch allein über die Aufnahme von Studenten. Als erster deutscher Staat regelte Preußen mit dem "Abiturreglement" von 1788 durch den Kultusminister Karl Abraham von Zedlitz die Abgangsprüfung. Dieses Reglement geht auf Carl Ludwig Bauer zurück, der am Lyceum Hirschfeld 1776 zum ersten Mal ein besonderes Examen einführte, mit dem Schulabgänger auf ihre Hochschulreife geprüft wurden. Auch Johann Heinrich Ludwig Meierotto wirkte zu dieser Zeit als Rektor des Joachimsthalschen Gymnasiums Berlin in dieselbe Richtung.

Die oft zitierten preußischen Regelungen vom 23. Dezember 1788 (Rescript) und vom 25. Juni 1812 (Instruction, – ausdrücklich bestätigt durch königliches Edict vom 12. October 1812), – hatten "nicht die Absicht" […], "das Abgehen eines zur Zeit noch unreifen Jünglings auf die Universität unbedingt zu verbieten, wenn dessen Eltern oder Vormünder durch irgend einen ihrem Gewissen zu überlassenden Grund bestimmt glaubten, so soll auch fernerhin eine freie Wahl unbeschränkt bleiben …" (§ 1 der Instruction vom 25. Juni 1812), sie schrieben also keine Schul-Abgänger-Prüfung, Abiturienten-Prüfung oder ein Maturitätszeugnis für die zur Universität abgehenden Schüler vor, obwohl der preußische Staat hierfür eine Gesetzgebungskompetenz beanspruchte (ALR 1794: „Die Universität ist eine Veranstaltung des Staates“); sie boten sie lediglich zur Information der Eltern oder Vormünder über den Leistungsstand und die mutmaßliche Studierfähigkeit an.

Derartige strengere Prüfungen wurden jedoch erst in Folge der Karlsbader Beschlüsse (31. August 1819 und dem darauf folgenden "Provisorischen Beschluß über die in Ansehung der Universitäten zu ergreifenden Maaßregeln" des Bundestags des Deutschen Bundes vom 20. September 1819) nach der Ermordung August von Kotzebues († 23. März 1819) und den zum Teil monatelang andauernden antijüdischen Hep-Hep-Unruhen (Beginn: 2. August 1819 in Würzburg) zur Verhinderung weiterer "Politisierung" von Studenten und Professoren in den 1820er und 1830er Jahren in fast allen Ländern des Deutschen Bundes eingeführt. Auch dieser Beschluss machte eine Maturitätsprüfung nicht zur Pflicht. Nach wie vor konnten auch für "unreif" befundene Jünglinge auf der Universität studieren, sie erhielten jedoch – nach der Vorschrift – keine Benefizien, Freitische u. A. (Stipendien) mehr. An den Universitäten wurden weiterhin Sekundaner und Tertianer akzeptiert und auch bei der Stipendienvergabe schien es in der Praxis eher locker vor sich zu gehen. Das Abgänger-Reglement erschien versierten Zeitgenossen als "Blendwerk".

Den Anfang strengerer Vorschriften machte das Kurfürstentum Hessen (Hessen-Kassel, Landes-Universität in Marburg) 1819 und das Großherzogtum Hessen (Hessen-Darmstadt, Landes-Universität in Gießen) 1825: Kurfürst Wilhelm I. (Kurfürstentum Hessen) "befahl" seiner Landesuniversität Marburg für die "staatsnahen" Berufe, "keinen Unterthan …," [der] "Theologie, Jurisprudenz, Medicin oder Cameral-Wissenschaften" studieren wollte, "ohne Vorzeigung eines … förmlichen Zeugnisses der Reife von irgend einem öffentlichen Gymnasium, zu immatrikuliren."

Es folgten fast alle Staaten des Deutschen Bundes, so Braunschweig 1826, Oldenburg 1827, Hannover und Sachsen 1829, Mecklenburg 1833 und Württemberg 1834.

Wilhelm von Humboldt und Johann Wilhelm Süvern versuchten die Vereinheitlichung der Maturitätsprüfung durch die Direktive von 1812 mit Prüfungen in beiden alten Sprachen, also Latein und Griechisch, ferner in Deutsch, Mathematik, den „historischen Fächern“ sowie Französisch und Naturlehre, die in Preußen aber noch bis 1834 durch Eingangsprüfungen der Universitäten umgangen werden konnte. Diese Möglichkeit nutzten vor allem Heranwachsende aus vermögenden Kreisen. 

Gegner der Pflicht zur Vorlage eines Schulabgangs-Zeugnisses war u. A. der preußische Justizminister Karl Albert von Kamptz, "weil es Jedem freistehen müsse, die Universität zu beziehen, ohne seine Befähigung vorher prüfen zu lassen, …" und Jacob Grimm erklärte: "Wie Kirche und Schauspiel dem Eintretenden offen gehalten sind, sollte jedem Jüngling das Thor der Universität aufgethan und ihm selbst überlassen sein, allen Nachtheil zu empfinden und zu tragen, wenn er unausgerüstet in diese Halle getreten ist." Dessen ungeachtet verschärften die deutschen Staaten nach 1835 den Universitätszugang schrittweise und machten die Vorlage eines Maturitätszeugnisses zur Voraussetzung der Immatrikulation. 

Am 25. Juni 1834 genehmigte dann der preußische König Friedrich Wilhelm III. mit allerhöchster Kabinettsorder ein Reglement für die Prüfung der zu den Universitäten übergehenden Schüler. Danach musste sich „jeder Schüler – vor seinem Abgange zur Universität, er mag eine inländische oder auswärtige Universität besuchen wollen, einer Maturitätsprüfung unterwerfen“. Zweck dieser Prüfung war, „auszumitteln, ob der Abiturient den Grad der Schulbildung erlangt hat, welcher erforderlich ist, um sich mit Nutzen und Erfolg dem Studium eines besonderen wissenschaftlichen Faches widmen zu können.“

Auch der zunehmende Finanzbedarf der Universitäten und die Anerkennung anderer „hohen“ Schulen als akademische Einrichtungen (z. B. Technische Hochschulen, Bergbau-, Forsthochschulen usw.) in der zweiten Hälfte des 19. Jahrhunderts förderten den Staatseinfluss.

In Preußen kam es aufgrund des in Art. 26 der Verfassungs-Urkunde von 1850 vorgesehenen Unterrichtsgesetzes zu einem Entwurf des Unterrichtsministers Adalbert von Ladenberg, der aber nicht weiter verfolgt wurde. In dem Entwurf war in den §§ 222–228 die Immatrikulation geregelt. In § 223 werden zwei Voraussetzungen für die Einschreibung vorgesehen, einmal ein von einem inländischen (= preußischen) Gymnasium ausgestelltes Zeugnis der Reife (Maturitätszeugnis) und zum zweiten die Erlaubnis durch den Vater oder Vormund zum Studium an der betreffenden Universität. Das galt aber nur für diejenigen, die sich "der Theologie, der Jurisprudenz und den Staatswissenschaften, der Medicin und Chirurgie, der Philologie oder einem sonstigen die Universitätsbildung gesetzlich erfordernden Berufe" widmen wollten.

Das 1871 gegründete Deutsche Reich änderte an den vorhergehenden Vorschriften der Bundesstaaten nichts, weil die Unterrichts- und Hochschulangelegenheiten in der Zuständigkeit der Bundesstaaten verblieben. Demgemäß taucht das Wort "Abitur" in den amtlichen Schriften vor 1945 fast gar nicht auf, wohl aber das Wort Abiturienten (= Abgänger: Damit sind nicht diejenigen gemeint, die das "Abitur" hatten, sondern die, die von der Schule abgingen.). Beim Abitur (oder genauer: der erfolgreichen Abiturprüfung, damals noch: Maturitätsprüfung, später Reifeprüfung oder in Bayern: Gymnasialabsolutorialprüfung) ging es um die Zulassung zu Staatsexamen (meist am Ende des Studiums), nicht um den Universitätszugang (vor Beginn des Studiums). Die Maturitätsprüfung/Reifeprüfung am Gymnasium berechtigte zum uneingeschränkten Studium in allen Fächern an der Universität, die des Realgymnasiums berechtigte in aller Regel nur zum Studium der Fächer der Staatswirtschaftlichen und Naturwissenschaftlichen Fakultät und Neuere Sprachen und Geschichte an der Philosophischen Fakultät (das war aber von Universität zu Universität verschieden). Häufig bot die Universität Lateinkurse an, die zum Erwerb des kleinen oder großen Latinums führen konnten, dann war ein Studium fast aller Fächer möglich (außer Theologie, später auch Altphilologie). Demgemäß gab es auch ein "Reifezeugnis der (zehnklassigen) Oberrealschule"; es berechtigte aber nur zum Studium in der Naturwissenschaftlichen Fakultät (ab 1899 nach einer Ergänzungsprüfung in Latein auch zum Studium an der Philosophischen Fakultät).

1896 konnten erstmals sechs Frauen in Preußen am Luisengymnasium Berlin ihre Reifeprüfung ablegen; studieren durften sie aber damit nicht, dazu bedurfte es einer ministeriellen Sondergenehmigung.
Ab 1904 wurde das Monopol des Gymnasiums auf ein Studium aller Fächer aufgehoben (Ausnahme: altsprachliche Kenntnisse für Studien der Theologie und der Altphilologie). Noch 1908/09 (1908 Frauenstudium, aber nur nach Genehmigung des Ministers) waren bis zu 10 % der männlichen Studierenden (an den zwölf preußischen Universitäten) ohne Reifeprüfung (z. B. Chemiker, Nationalökonomen, Pharmazeuten, Zahnmediziner).

Seit 2005 findet an deutschen Auslandsschulen die sogenannte Deutsche Internationale Abiturprüfung Anwendung.

Gymnasien und Gesamtschulen mit gymnasialer Oberstufe bereiten in Deutschland mit ihrer zwei- bis dreijährigen Oberstufe auf das Abitur vor. Die Regelschulzeit bis zum Abitur beträgt 12 oder 13 Jahre, beim Abitur nach zwölf Jahren (G8) wird die Prüfung am Ende der Jahrgangsstufe 12 abgelegt, bei G9 dagegen am Ende des 13. Jahrgangs. Dort, wo die Regelschulzeit auf zwölf Jahre verkürzt ist, gilt in vielen Bundesländern die 10. Klasse am Gymnasium zugleich als erstes Jahr der gymnasialen Oberstufe. Realschüler müssen bei diesem Modell die 10. Klasse zweimal besuchen, einmal auf der Realschule zur Erlangung des Erweiterten Sek I-Abschlusses und anschließend auf dem Gymnasium als erstes Jahr in der gymnasialen Oberstufe.

Das allgemeine Abitur wird auch an Beruflichen Gymnasien bzw. Berufskollegs mit gymnasialer Oberstufe (Höhere Berufsfachschule) erworben. Dies sind zum Beispiel das Wirtschaftsgymnasium (WG), Technische Gymnasium (TG), Ernährungswissenschaftliche Gymnasium (EG), Sozial- und Gesundheitswissenschaftliches Gymnasium (SGG), Agrarwissenschaftliche Gymnasium (AG) oder auch das Biotechnologische Gymnasium (BTG). In manchen Ländern werden sie auch Fachgymnasium genannt, weil ein berufliches Fach verbindlich ist. Die Jahrgangsstufe 13 der Fachoberschule oder die Berufsoberschule (BOS) führen ebenfalls zur allgemeinen Hochschulreife. Neben der allgemeinen Hochschulreife erwerben die Abiturienten zusätzlich eine berufliche Vertiefung.

Staatliche und private Schulen bieten Lehrgänge des sogenannten zweiten Bildungsweges an, die über sämtliche Schulabschlüsse bis zum Abitur führen.

In Baden-Württemberg sowie in den meisten anderen Bundesländern können Erwachsene mit einem mittleren Bildungsabschluss und einer abgeschlossenen Berufsausbildung an den Berufsoberschulen ihr Abitur machen.

Das Abitur kann für Erwachsene mit einer abgeschlossenen Berufsausbildung oder gleichgesetzten Tätigkeiten und Zeiten auch an Abendgymnasien und Kollegschulen, in einigen Regionen auch an Volkshochschulen erworben werden.

Einige dieser Schulen bieten das sogenannte „Abitur online“ an. Hierbei besucht der Schüler nur an zwei Abenden oder Vormittagen die Schule. In der übrigen Zeit hat der Schüler Zeit für Beruf oder Kindererziehung. Es findet kein Unterricht über das Internet statt, aber es müssen Hausaufgaben eingeschickt werden. Außerdem findet der Schüler über seine Lernplattform eine Bibliothek vor, mit der er sich von zu Hause aus eigenständig auf den Unterricht und das Abitur vorbereiten kann. Zusätzlich bieten die Lehrer für diesen Kurs die Möglichkeit an, bei Fragen über E-Mail zur Verfügung zu stehen.

Unter den Privatschulen sind die staatlich anerkannten "Ersatzschulen" von den staatlich nur zugelassenen "Ergänzungsschulen" zu unterscheiden. Die meisten Privatschulen, besonders die zahlreichen kirchlichen und die Waldorfschulen, sind staatlich anerkannt, sodass es möglich ist, die Hochschulreife regulär im Rahmen interner Prüfungen zu erlangen, die allerdings von der Schulaufsicht des jeweiligen Landes wie an staatlichen Schulen kontrolliert werden.

Andere nur staatlich zugelassene private Schulen mit einem Oberstufenzweig bereiten – tagsüber oder am Abend – auf das Abitur vor, das allerdings erst nach einer externen staatlichen Prüfung ("Nichtschülerabiturprüfung", "Schulfremdenreifeprüfung" oder auch "Schulfremdenprüfung") zuerkannt wird.

Weiterhin ist es möglich, sich nach selbst organisierter Vorbereitung für eine "Nichtschülerprüfung" anzumelden, um das Zeugnis der allgemeinen Hochschulreife zu erwerben (siehe unten).

Waldorfschulen führen in zwölf Schuljahren zu einem eigenen Abschluss, dem Waldorfschulabschluss, mit starkem Gewicht auf künstlerischen Fächern und sozialen Kompetenzen. Der Waldorfabschluss kann als gleichwertig mit einem staatlichen Hauptschul- oder Realschulabschluss anerkannt werden. Auf das Abitur bereiten die Waldorfschulen in einem zusätzlichen 13. Schuljahr vor, dieses wird dann dem staatlichen Abitur gleichgehend in acht Fächern unter Aufsicht einer Staatsschule abgelegt. Eine Ausnahme bildet Hessen, wo die Sekundarstufe II der Waldorfschulen meist als gymnasiale Oberstufe anerkannt ist. Das Prozedere unterscheidet sich damit nicht von staatlichen Schulen. Auf die Fachhochschulreife wird in der 12. Klasse vorbereitet, wobei dann der künstlerische Abschluss wegfällt. An einigen Waldorfschulen kann dies mit einem Abschluss in einem anerkannten Ausbildungsberuf verbunden werden. Dadurch kann man die Schule nach 14 Schuljahren mit Lehrberuf und Abitur verlassen.

Auch Fernschulen (zum Beispiel Institut für Lernsysteme (ILS), Studiengemeinschaft Darmstadt (SGD) oder Hamburger Akademie für Fernstudien (HAF)) bereiten in etwa 30 bis 42 Monaten – je nach Vorkenntnissen – auf die staatliche Externenprüfung zum Abitur vor. Dieser Weg wird vor allem von Erwachsenen genutzt, die parallel zu ihrem Berufsalltag ihr Abitur nachholen möchten.

Mit einem Hochschulabschluss (Bachelor, Diplom usw.) verbunden ist die allgemeine Hochschulreife, die Zugangsberechtigung für sämtliche Hochschulstudiengänge, wenn das Studium zuvor mit einem Zeugnis der fachgebundenen Hochschulreife oder der Fachhochschulreife aufgenommen wurde. Rechtsgrundlage hierfür sind die Hochschulgesetze der Länder.

Mit Bestehen der Vordiplomprüfung bzw. dem Erlangen von 90 ECTS an einer Hochschule wird in einigen Ländern Inhabern des Zeugnisses der Fachhochschulreife die fachgebundene Hochschulreife zuerkannt (vgl. Hessisches Hochschulgesetz § 63 Abs. 3 Satz 2). Je nach Land unterscheidet sich unter anderem die Zahl der mit ihr möglichen Studienfächer.

Beim Abitur für Nichtschüler (auch: "Begabtenabitur", "Nichtschülerabitur", "Fremdenprüfung" oder "außerschulisches Abitur") wird die allgemeine Hochschulreife durch eine Prüfung ohne vorangegangenen Besuch einer entsprechenden Schule erworben. Die Prüfungsvorbereitung erfolgt autodidaktisch oder mithilfe externer Anbieter, die Zulassung zur Prüfung durch das Kultusministerium des Landes, in dem die Prüfung abgelegt werden soll.

In den "Abitur- oder Reifeprüfungen" werden die in der Oberstufe erworbenen Kompetenzen in ausgewählten vier oder fünf Fächern geprüft. Die Prüfungsnoten fließen in die Durchschnittsnote des Reifezeugnisses mit ein.

Die Abiturprüfungen erfolgen schriftlich und mündlich. Das Verfahren unterscheidet sich in Deutschland von Land zu Land deutlich.

In 15 von 16 Bundesländern wird in den schriftlichen Prüfungen ein sogenanntes Zentralabitur abgelegt, lediglich in Rheinland-Pfalz werden sie dezentral durchgeführt. Alle Abiturklausuren werden von einem Erst- und einem Zweitgutachter bewertet. Das Verfahren zur Festlegung der Note unterscheidet sich wieder von Land zu Land.

In welchen und wie vielen Fächern eine mündliche Prüfung abgelegt wird, ist ebenso unterschiedlich geregelt. Mindestens eine mündliche Prüfung ist obligatorisch. Möglich ist bisweilen auch eine "Präsentation" (z. B. in Hessen). In manchen Ländern können auf Wunsch des Schülers in einem oder mehreren Fächern zusätzliche mündliche Prüfungen stattfinden, wenn er bei schlechter schriftlicher Leistung auf eine Verbesserung hofft. In einigen Ländern sind auch Gruppenprüfungen von bis zu drei Schülern möglich, die Prüfungsdauer verdreifacht sich in diesem Fall. Die mündliche Prüfung wird von einer Prüfungskommission, bestehend aus mindestens drei Lehrkräften, abgenommen. In Privatschulen kann ein staatlicher Beauftragter den Vorsitz der Prüfung übernehmen oder der Prüfung beiwohnen.

Oft ist es möglich, eine Besondere Lernleistung, zum Beispiel die Teilnahme an einem Bundeswettbewerb oder eine Facharbeit, in die Abiturnote mit einzubringen. In manchen Bundesländern kann sie das vierte oder fünfte Prüfungsfach ersetzen.

Im Jahr 2012 beendeten 498.408 Schulabsolventen in Deutschland die Schule mit der Fachhochschul- oder allgemeinen Hochschulreife.

2007 erwarben 432.500 (2006: 412.800) Schüler in Deutschland die Hochschul- oder Fachhochschulreife (4,2 Prozent plus seit 2006). Davon erreichten 302.200 Schüler die allgemeine Hochschulreife (= 69,9 Prozent), und zwar meist (258.900) an allgemeinbildenden Schulen. In Sachsen, Thüringen und Berlin war ein Rückgang der Abiturientenzahl zu verzeichnen.

2004/2005 erwarben erst 244.000 Abgänger allgemeinbildender Schulen die allgemeine Hochschulreife oder Fachhochschulreife (allgemeine Hochschulreife 24,1 Prozent; Fachabitur 1,3 Prozent; Realschulabschluss 41,6 Prozent; Hauptschulabschluss 24,8 Prozent; ohne Abschluss 8,2 Prozent). Dazu müssen die etwa 155.000 Absolventen der beruflichen Schulen mit allgemeiner Hochschulreife oder Fachhochschulreife (89,3 Prozent) gerechnet werden.

Der Anteil der Schüler, die in Deutschland eine allgemeine Hochschulzulassung erlangten („Abiturientenquote“), betrug 2005/2006 43,1 Prozent und lag im internationalen Vergleich damit unter dem Durchschnitt. Große Unterschiede gibt es zwischen den deutschen Ländern, ebenso zwischen Stadt und Land. Den geringsten Wert erreicht Mecklenburg-Vorpommern mit 32,2 Prozent, den höchsten Nordrhein-Westfalen mit 53,4 Prozent. Lehrerverbandsvertreter kritisieren den Druck zur Erhöhung der Quote, der wegen internationaler Vergleiche auf Eltern und damit auf Schülern lastet.

Im Jahr 2009 stand Thüringen mit einer durchschnittlichen Abiturnote von 2,3 auf Platz 1, gefolgt von Mecklenburg-Vorpommern, Baden-Württemberg, Brandenburg und Bayern. Thüringen stand 2009 auch im Einser-Abiturienten Ranking an erster Stelle. 1,99 Prozent erreichten dabei die Note 1,0. Es folgten Baden-Württemberg, Bremen, Mecklenburg-Vorpommern und das Saarland.

Im Hinblick auf die Durchfallquote, stand Sachsen-Anhalt 2009 mit 6 Prozent an der Spitze, dicht gefolgt von Berlin und Sachsen.

In Deutschland korreliert der Schulabschluss des Kindes sehr stark mit dem seiner Eltern. Im Jahre 2010 hatten demnach 60 % der Gymnasiasten Eltern mit Abitur, aber nur 8 % Eltern mit Hauptschulabschluss.

Im Jahr 2007 betrug der weibliche Anteil der Abiturienten 53,3 Prozent.

Die Ausländer in Deutschland waren im Jahr 2013 mit einem Anteil der Hochschulreife mit 29,4 Prozent vertreten, die Deutschen mit 27,8 Prozent knapp darunter.

Die Abiturientenquote in Deutschland wird gemessen als Anteil der Hochschulzugangsberechtigten an den 18- bis 20-Jährigen, also den Abgängern aus den allgemeinen und beruflichen Schulen mit und ohne Abschluss.

Historisch ist ein langsames Anwachsen der Abiturientenzahl seit Anfang des 19. Jahrhunderts zu verzeichnen. Für 1820 wird ihre Zahl in Preußen mit 590 angegeben, für 1829 mit 1409. Die Zahl der deutschen Studenten stieg von 1815 bis 1830 von etwa 5000 auf über 16.000. Danach ging, auch durch das Abiturreglement, die Zahl wieder zurück, bevor sie in den 1850er Jahren erneut anstieg. Die Abiturientenzahl lag damit dauerhaft vor dem Jahr 1900 unter ein Prozent des Altersjahrgangs. Um 1900 schwankte sie zwischen ein und zwei Prozent, auch weil einige Mädchen dazukamen. Ein Großteil der Gymnasiasten machte keinen Abiturabschluss.

Die deutsche Studentenzahl stieg von 1860 bis 1914 rasant von 11.901 auf 60.235. Noch in den 1950er Jahren lag die Abiturientenquote in Deutschland bei unter 5 % eines Altersjahrgangs. 1960 lag die Abiturientenquote in Deutschland bei 6,1 % des Altersjahrgangs, danach wurde im Zuge der Bildungsexpansion der Besuch von Gymnasien stark gefördert und die Abiturientenzahlen stiegen an. Noch Anfang der 1980er Jahre lag die Abiturientenquote in Deutschland aber bei unter 22 % des Altersjahrgangs. Im Jahre 2014 erreichten bereits 41 % der gleichaltrigen Bevölkerung in Deutschland das Abitur und unter Berücksichtigung der Fachhochschulreife erlangten sogar mehr als 50 % des Altersjahrgangs eine Studienberechtigung.

Da die Bildung in Deutschland im Kompetenzbereich der Länder liegt, gibt es hier von Land zu Land Unterschiede im Abitur. Für alle verbindlich sind aber die von der Kultusministerkonferenz (KMK) vereinbarten „Einheitlichen Prüfungsanforderungen in der Abiturprüfung“ (EPA), die bundesweit gelten.

Aus den unterschiedlichen Regelungen resultieren unterschiedliche Notendurchschnitte der Abiturprüfungen in den Ländern. So weisen die Länder Thüringen (2,30) und Baden-Württemberg (2,33) einen wesentlich besseren Notendurchschnitt auf als die Länder Berlin (2,71) oder Nordrhein-Westfalen (2,67). Der niedrigste Notendurchschnitt ergibt sich mit 2,72 in Niedersachsen (Stand 2005). Dabei zeigt sich in fast allen Ländern ein Trend zur Noteninflation: So verbesserte sich der Notendurchschnitt in zehn Jahren (Stand 2015) beispielsweise im ohnehin schon großzügigsten Land Thüringen noch einmal von 2.30 auf 2,16, in Berlin von 2.71 auf 2,39, in Nordrhein-Westfalen von 2.67 auf 2.47 und im strengeren Niedersachsen von 2.72 auf 2.59. Lediglich Baden-Württemberg senkte seinen Notendurchschnitt in dieser Zeit von 2.33 auf 2.44.

Relativiert werden diese Unterschiede dadurch, dass der Anteil der Schüler an den einzelnen Schularten zwischen den Ländern stark variiert. So erwerben in Niedersachsen und Nordrhein-Westfalen mehr als ein Viertel der Schüler die Hochschulreife, in Bayern dagegen nur 19 Prozent.

Zudem gewichten die Bundesländer die einzelnen Noten bei der Berechnung der Durchschnittsnote unterschiedlich. In einem von der tageszeitung veröffentlichten Fallbeispiel erhält ein Schüler - bei identischen Noten in allen Klausuren und Prüfungen - in Hamburg die Abitur-Durchschnittsnote 1,9, in Thüringen die Durchschnittsnote 2,3 und in Sachsen-Anhalt erhält er kein Abitur.

Das Europäische Abitur (auch Europäisches Baccalauréat) ist eine von allen EU-Ländern anerkannte allgemeine Hochschulreife, die an Absolventen der Europäischen Schulen vergeben wird. In Deutschland entspricht sie dem Abitur.

Mit der 68er-Bewegung wurden alte Bräuche obsolet. Nach einer Zeit der Ablehnung jeglicher Feier entwickelten sich ab den 80er Jahren neue Formen, die regional unterschiedlich stark ausgeprägt sind. Als Hintergrund wird der Bedeutungsverlust des Abiturs durch die hohe Zahl der Abiturienten angenommen. Oft erfolgt aber noch die Übergabe der Zeugnisse im Rahmen einer festlichen "Abiturfeier". Immer häufiger nehmen die Abiturienten an einer gemeinsamen "Abireise" teil, ähnlich dem amerikanischen Spring Break. Nach außen hin zeigen die Abiturienten vor allem anhand selbst gestalteter Kleidung oder Autoaufkleber, dass sie ihr Abitur bestanden haben.

Abiturienten veranstalten oft einen Abistreich, in Norddeutschland als Nulltagefeier bekannt, bei dem sie die Schule für einen Schultag „erobern“ dürfen und in Absprache mit dem Lehrerkollegium Lehrer-Schüler-Spiele oder sonstige Unterhaltung für die anderen Schüler durchführen. Seit den achtziger Jahren entwickeln viele Schüler der Abitur-Jahrgangsstufen sogenannte „Abi-Logos“, die oftmals auf selbst kreierten Slogans mit Abiturbezug basieren. Die Dokumentation dieser Bräuche kann zudem – neben einer Rückbetrachtung der gewöhnlichen Schulzeit – Teil der Abiturzeitung sein. An einigen Schulen „verewigen“ sich die Schüler mit der Gestaltung einer Wand.

Um Abiball und andere mit dem Abschluss verbundene Events oder die Abizeitung finanzieren zu können, werden teilweise Partys veranstaltet. Diese werden zum Teil Vorfinanzierungspartys oder Stufenpartys genannt. Aber auch der einfache Kuchenverkauf in den Pausen oder während schulischer Veranstaltungen macht sich bezahlt.

Aus Anlass des Abiturs werden auch Abibälle veranstaltet, oft finden diese am Sonnabend nach der Zeugnisvergabe statt. In der Regel organisieren die Abiturienten den Ball und laden ihre Eltern und Lehrer ein.

Nach den Abiturprüfungen fahren die Abiturienten teilweise gemeinsam in den Urlaub. Ziel ist oft eine günstige Partymetropole. Aufgrund des lukrativen Marktes gibt es spezialisierte Reiseagenturen, die mit zahlreichen Zusatzleistungen und niedrigen Preisen werben.

Die "Sonderreifeprüfung" war in der Deutschen Demokratischen Republik eine Zulassungsprüfung zum Hochschulstudium. Der zweitägigen Prüfung mussten sich vor allem Akademikerkinder unterziehen, die nach dem Abitur zunächst nicht hatten studieren dürfen und eine Berufsausbildung absolvierten. Besondere Bedeutung hatte die Sonderreifeprüfung für diejenigen, die Evangelische Theologie in Leipzig oder Rostock studieren wollten. Ein Beispiel ist Rainer Müller.

Als Notabitur, Notreifeprüfung oder Kriegsabitur wurde ein Abitur nach Ablegung einer erleichterten Reifeprüfung gegenüber den normal üblichen Bedingungen bzw. Voraussetzungen bezeichnet. Das Notabitur gab es während des Ersten und Zweiten Weltkriegs im Deutschen Reich.





</doc>
<doc id="14670" url="https://de.wikipedia.org/wiki?curid=14670" title="Dynamic Host Configuration Protocol">
Dynamic Host Configuration Protocol

Das Dynamic Host Configuration Protocol (DHCP) ist ein Kommunikationsprotokoll in der Computertechnik. Es ermöglicht die Zuweisung der Netzwerkkonfiguration an Clients durch einen Server.

DHCP wurde im RFC 2131 definiert und bekam von der Internet Assigned Numbers Authority die UDP-Ports 67 und 68 zugewiesen.

DHCP ermöglicht es, angeschlossene Clients ohne manuelle Konfiguration der Netzwerkschnittstelle in ein bestehendes Netzwerk einzubinden. Nötige Informationen wie IP-Adresse, Netzmaske, Gateway, Name Server (DNS) und ggf. weitere Einstellungen werden automatisch vergeben, sofern das Betriebssystem des jeweiligen Clients dies unterstützt.

DHCP ist eine Erweiterung des Bootstrap-Protokolls (BOOTP), das für Arbeitsplatz-Computer ohne eigene Festplatte (Diskless-Workstation) notwendig war, wo sich der Computer beim Startvorgang zunächst vom BOOTP-Server eine IP-Adresse zuweisen ließ, um danach das Betriebssystem aus dem Netzwerk zu laden. DHCP ist weitgehend kompatibel zu BOOTP und kann entsprechend mit BOOTP-Clients und -Servern (eingeschränkt) zusammenarbeiten.

DHCP wurde erstmals 1993 in RFC 1531 und RFC 1541 definiert, aufbauend auf dem 1985 entstandenen BOOTP (RFC 951).


Der DHCP-Server wird – wie alle Netzwerkdienste – als Hintergrundprozess (Dienst oder Daemon) gestartet und wartet auf UDP-Port 67 auf Client-Anfragen. In seiner Konfigurationsdatei befinden sich Informationen über den zu vergebenden Adresspool sowie zusätzliche Angaben über netzwerkrelevante Parameter wie die Subnetzmaske, die lokale DNS-Domain oder das zu verwendende Gateway. Außerdem lassen sich auch weitere BOOTP-Server oder der Ort des zu verwendenden Bootimages einstellen.

Es gibt drei verschiedene Betriebsmodi eines DHCP-Servers: manuelle, automatische und dynamische Zuordnung.

In diesem Modus ("statisches DHCP") werden am DHCP-Server die IP-Adressen bestimmten MAC-Adressen fest zugeordnet. Die Adressen werden der MAC-Adresse auf unbestimmte Zeit zugeteilt. Der Nachteil kann darin liegen, dass sich keine zusätzlichen Clients in das Netz einbinden können, da die Adressen fest vergeben sind. Das kann unter Sicherheitsaspekten erwünscht sein.

Manuelle Zuordnungen werden vor allem dann vorgenommen, wenn der DHCP-Client beispielsweise Server-Dienste zur Verfügung stellt und daher unter einer festen IP-Adresse erreichbar sein soll. Auch Port-Weiterleitungen von einem Router an einen Client benötigen in der Regel eine feste IP-Adresse.

Bei der automatischen Zuordnung wird am DHCP-Server ein Bereich von IP-Adressen ("range") definiert. IP-Adressen werden automatisch an die MAC-Adressen von neuen DHCP-Clients zugewiesen, was in einer Tabelle festgehalten wird. Im Unterschied zur dynamischen Zuordnung sind automatische Zuordnungen permanent und werden nicht entfernt. Der Vorteil ist, dass Hosts immer dieselbe IP-Adresse erhalten und eine zugewiesene IP-Adresse keinem anderen Host zugewiesen wird. Der Nachteil ist, dass neue Clients keine IP-Adresse erhalten, wenn der gesamte Adressbereich vergeben ist, auch wenn IP-Adressen nicht mehr aktiv genutzt werden. Gegenüber der manuellen und dynamischen Zuordnung spielt dieser Modus in der Praxis eine untergeordnete Rolle.

Dieses Verfahren gleicht der automatischen Zuordnung, allerdings hat der DHCP-Server hier in seiner Konfigurationsdatei eine Angabe, wie lange eine bestimmte IP-Adresse an einen Client „verliehen“ werden darf, bevor der Client sich erneut beim Server melden und eine „Verlängerung“ beantragen muss. Meldet er sich nicht, wird die Adresse frei und kann an einen anderen (oder auch denselben) Rechner neu vergeben werden. Diese vom Administrator bestimmte Zeit heißt Lease-Time (zu Deutsch also: „Leihdauer“).

Manche DHCP-Server vergeben auch von der MAC-Adresse abhängige IP-Adressen, d. h. ein Client bekommt hier selbst nach längerer Netzwerkabstinenz und Ablauf der Lease-Zeit die gleiche IP-Adresse wie zuvor (es sei denn natürlich, diese ist inzwischen schon anderweitig vergeben).


DHCP-Relay ist eine Funktion, um DHCP über Netzwerkgrenzen (Broadcastdomäne) hinaus nutzen zu können. Damit wird die Notwendigkeit der Bereitstellung eines DHCP-Servers in jedem Subnetz, in dem sich DHCP-Clients befinden, vermieden. Die DHCP-Relay-Funktion wird meist durch den Router selbst erbracht.
Dabei werden Client-seitig mittels Broadcast verschickte DHCP-Anfragen durch den DHCP-Relay empfangen und mittels Unicast einem oder mehreren DHCP-Servern zugestellt. Der DHCP-Relay-Agent wird funktional auf der Schnittstelle des Routers platziert.

Bei einem Active Directory ist der DHCP-Dienst in die AD-Datenbank integriert.
Dabei dürfen nur autorisierte Server Leases verteilen. Um einen Server zu autorisieren, muss dieser ein DC sein oder ein Domänenkonto besitzen.

Damit der Client einen DHCP-Server nutzen kann, muss sich dieser im selben Netzwerksegment befinden, da DHCP Broadcasts verwendet und Router keine Broadcasts weiterleiten (Router bilden Broadcast-Domänen). Befindet sich der DHCP-Server in einem anderen Netzwerksegment, so muss ein so genannter DHCP-Relay-Agent installiert werden, der die DHCP-Anfragen an den eigentlichen Server weitergibt.

Wenn ein Client erstmals eine IP-Adresse benötigt, schickt er eine "DHCPDISCOVER"-Nachricht (mit seiner MAC-Adresse) als Netzwerk-Broadcast an die verfügbaren DHCP-Server (es kann durchaus mehrere davon im selben Subnetz geben). Dieser Broadcast hat als Absender-IP-Adresse 0.0.0.0 und als Zieladresse 255.255.255.255, da der Absender noch keine IP-Adresse besitzt und seine Anfrage „an alle“ richtet. Dabei ist der UDP-Quellport 68 und der UDP-Zielport 67. Die DHCP-Server antworten mit "DHCPOFFER" und machen Vorschläge für eine IP-Adresse. Das geschieht entweder mit einem Broadcast an die Adresse 255.255.255.255 mit UDP-Quellport 67 und UDP-Zielport 68 oder mit einem Unicast an die vorgeschlagene IP-Adresse und die MAC-Adresse des Clients, je nachdem ob der Client in der "DHCPDISCOVER"-Nachricht das Broadcast-Bit gesetzt hat.

Der Client darf nun unter den eingetroffenen Angeboten ("DHCPOFFER") wählen. Wenn er sich für eines entschieden hat (z. B. wegen längster Lease-Zeit oder wegen Ablehnung eines speziellen, evtl. falsch konfigurierten DHCP-Servers, oder einfach für die erste Antwort), kontaktiert er per Broadcast und einem im Paket enthaltenen Serveridentifier den entsprechenden Server mit der Nachricht "DHCPREQUEST". Alle eventuellen weiteren DHCP-Server werten das als Absage für ihre Angebote. Der vom Client ausgewählte Server bestätigt in einer "DHCPACK"-Nachricht (DHCP-Acknowledged) die IP-Adresse mit den weiteren relevanten Daten, oder er zieht sein Angebot zurück ("DHCPNAK", siehe auch Sonstiges).

Bevor der Client sein Netzwerkinterface mit der zugewiesenen Adresse konfiguriert, sollte er noch prüfen, ob nicht versehentlich noch ein anderer Rechner die Adresse verwendet. Das geschieht üblicherweise durch einen ARP-Request mit der soeben zugeteilten IP-Adresse. Antwortet ein anderer Host im Netz auf diesen Request, so wird der Client die vorgeschlagene Adresse mit einer "DHCPDECLINE"-Nachricht zurückweisen.

Zusammen mit der IP-Adresse erhält der Client in der "DHCPACK"-Nachricht neben der ""lease time"", also der Gültigkeitsdauer der IP-Konfiguration, zwei Fristen: Die ""renewal time"" T1 und die ""rebinding time"" T2. Der Standard schlägt vor, dass T1 auf die Hälfte und T2 auf 7/8, also 87,5 % der Gültigkeitsdauer der "lease time" gesetzt wird. Für beide Werte kann der DHCP-Server optional andere Werte vorgeben.

Nach Ablauf der Zeit T1 versucht der Client, seine "lease time" zu verlängern. Dazu sendet der Client wieder "DHCPREQUEST"s per Unicast an den Server, der die IP-Konfiguration vergeben hat. Der Server sollte dann in der Regel ein "DHCPACK" mit identischen Daten wie vorher, aber einer neuen "lease time" senden. Damit gilt die Gültigkeitsdauer der IP-Konfiguration des Clients als verlängert.

Antwortet der Server nicht, weil er zum Beispiel abgeschaltet wurde und nun ein neuer Server für die Verwaltung der IP-Adressen zuständig ist, so kann der Client die IP-Konfiguration ohne Einschränkungen weiter verwenden, bis die "lease time" abgelaufen ist. Er wird jedoch nach Ablauf von T2 anfangen, "DHCPREQUEST"s nunmehr per Broadcast zu versenden, um eine neue IP-Konfiguration von irgendeinem anderen DHCP-Server zu erhalten.

Sollte der Client es versäumen, bis zum Ablauf der Lease-Zeit eine Verlängerung zu beantragen, muss er seine Netzwerkkarte dekonfigurieren und wieder bei "DHCPDISCOVER" mit einer initialen Adresszuweisung beginnen. Sollte der DHCP-Server keine Adressen mehr zur Verfügung haben oder während des Vorganges schon ein anderer Client seine letzte Adresse zugesagt bekommen haben, sendet der Server ein DHCPNAK (DHCP-Not Acknowledged), und der Vorgang der Adressanfrage beginnt erneut.

Eine negative Bestätigung "DHCPNAK" kann als Ursache haben, dass der Client versucht, seine ehemalige IP-Adresse zu leasen (engl. lease: mieten oder pachten), die jedoch inzwischen nicht mehr verfügbar ist, oder wenn der Client-Computer in ein anderes Subnetz verschoben wurde.

Um die Ausfallwahrscheinlichkeit zu verringern, ist es auch möglich, mehrere DHCP-Server in einem Netz zu platzieren. Dabei sollte allerdings beachtet werden, dass sich die Adressbereiche der einzelnen Server nicht überlappen, da es sonst zu Doppelvergaben von IP-Adressen kommen kann. Dazu gibt es die „authoritative“ (engl. für „maßgebliche“) Einstellung, mit der man einstellen kann, ob ein "DHCPNAK" auch verschickt werden soll, wenn der DHCP-Server für die vom Client vorgeschlagene Adresse nicht zuständig ist.

Wenn der Client eine negative Bestätigung erhält, wird der DHCP-Lease-Vorgang erneut gestartet.

Ein Client sendet "DHCPRELEASE", wenn er eine IP-Adresse vor Ablauf der Lease-Zeit zurückgeben will.

Sollte der Client feststellen, dass die zugewiesene Adresse bereits benutzt wird, so teilt er das dem Server durch "DHCPDECLINE" mit, der seinerseits den Administrator von dieser potentiellen Fehlkonfiguration unterrichten sollte.

Damit ihre Namensauflösung möglich ist, registrieren Computer ihren Namen und ihre IP-Adresse in der Regel bei einem DNS-Server. Einige DHCP-Server können das an Stelle der Clients übernehmen. Bei Betriebssystemen von Microsoft war das vor Windows 2000 erforderlich.

Standardmäßig kann DHCP dem Client unter anderem folgende Einstellungen zuweisen:


Es gibt jedoch weit mehr DHCP-Optionen, die vom DHCP-Client implementiert werden können. Ein gutes Beispiel ist die Angabe eines TFTP-Servers für den Betrieb eines Diskless-Workstation. In diesem Fall muss das BIOS/UEFI die TFTP-Zuweisung per DHCP unterstützen, um einen PXE-Boot zu initiieren.

Der DHCP-Server kann (Teil-)Netze bedienen, wenn er über Definitionen für das jeweilige Netz verfügt. Die Auswahl der Definition wird dann durch die Netzwerkkarte bestimmt, über welche die Anforderung hereinkommt. Beim Start des DHCP-Servers kann angegeben werden, auf welchen Interfaces der Server hört.

Andererseits kann ein DHCP-Server auch entfernte Netze bedienen, wenn diese durch einen DHCP-Relay-Agenten (vielfach als Funktion eines Routers verfügbar) verbunden sind. Der Relay-Agent empfängt im entfernten Netz die DHCP-Broadcast-Anforderungen und leitet diese als Unicast-Botschaften an den/die konfigurierten DHCP-Server weiter. Die IP-Adresse der Schnittstelle, über welche der Broadcast empfangen wurde, wird vom Relay-Agenten dem Unicast-Paket im DHCP-Header hinzugefügt, so dass der DHCP-Server anhand dieser Information bestimmen kann, aus welchem Netzwerksegment die Anfrage kommt. Der DHCP-Relay-Agent empfängt die Antwortpakete der DHCP-Server auf Port UDP 67 und leitet diese dann mit Zielport UDP 68 an den Client weiter.

DHCP kann leicht gestört und manipuliert werden, weil DHCP-Clients jeden DHCP-Server akzeptieren.

Die versehentliche Aktivierung eines DHCP-Servers, beispielsweise durch den Anschluss eines einfachen DSL-Routers oder WLAN-Routers im Auslieferungszustand, kann ein Netz weitgehend lahmlegen. Dieser antwortet möglicherweise schneller als der eigentlich vorgesehene DHCP-Server und verteilt dadurch ggf. ungültige Konfigurationen.

Ein Angreifer kann alle Adressen eines DHCP-Servers reservieren (DHCP Starvation Attack), dadurch dessen Antwort auf weitere Anfragen verhindern und anschließend als einziger DHCP-Server auftreten. Er hat nun die Möglichkeit ein rogue DHCP Spoofing zu betreiben, indem er auf andere DNS-Server umleitet, die auf Computer verweisen, die die Kommunikation kompromittieren.

Zum einen kann der Angreifer beispielsweise Denial-of-Service-Angriffe starten, indem er jedem Client ein eigenes Subnetz zuweist, kein Gateway übermittelt oder auf alle Anfragen mit der gleichen IP-Adresse antwortet. Zum anderen kann er versuchen, mithilfe falscher Gateway- und DNS-Angaben einen fremden Router einzuschleusen, der den Datenverkehr des Clients mitschneidet oder umleitet (Man-in-the-Middle-Angriff). 

Die vermeintliche Eindeutigkeit der MAC-Adresse darf nicht als Sicherheitskriterium angewandt werden. Es ist viel zu einfach, MAC-Adressen-Spoofing zu betreiben. Fast alle Betriebssysteme erlauben es gewöhnlichen Benutzern, die MAC-Adresse komfortabel in Konfigurationsmasken oder mit einfachen Tools wie "ifconfig (UNIX, Linux)" oder "ip link" (Linux) zu überschreiben. Gültige MAC-Adressen in einem Schicht-2-Netz können durch "Abhören" des Netzverkehrs ausfindig gemacht werden. Dazu ist lediglich der physische Zugang zum Netzwerk nötig. Die exklusive Vergabe von IP-Adressen nur an registrierte MAC-Adressen über RARP oder DHCP schließt also nicht aus, dass Unberechtigte Zugriff auf das Netzwerk erhalten; dafür ist der Einsatz eines sicheren Authentifizierungsmechanismus wie IEEE 802.1X notwendig.

Eine Persiflage der Bemühungen, diese Probleme zu umgehen, ist das Wäscheklammerprotokoll Peg DHCP.

IPv6 benötigt für die eigentliche Adressvergabe keinen DHCP-Dienst (siehe IPv6 Autokonfiguration). Allerdings benötigt ein Client neben einem Gateway üblicherweise noch die Zuweisung eines DNS-Servers. Ein standardisiertes Verfahren für die Mitteilung der DNS-Server wird in RFC 6106 (RDNSS, DNSSL) beschrieben und stellt eine Erweiterung zu Autokonfiguration dar. Werden darüber hinaus Konfigurationsinformationen benötigt, kann anstatt von Autokonfiguration DHCPv6 verwendet werden.

DHCPv6 ist seit Juli 2003 in RFC 3315 spezifiziert und ermöglicht für IPv6 die gleiche Funktionalität wie das gegenwärtig aktuelle DHCPv4 für IPv4. Darüber hinaus ist DHCPv6 darauf ausgelegt, über optionale Felder im DHCPv6-Protokoll Konfigurationsinformationen über NIS+-, SIP-, NTP- und weitere Dienste zu transportieren. Welche Optionen in DHCPv6 aufgenommen werden, wird von der DHCP-Arbeitsgruppe der IETF festgelegt. Weitere Features von DHCPv6 sind die integrierten Sicherheitsfunktionen, durch die es möglich ist, DHCPv6 nur autorisierten Clients zugänglich zu machen, sowie die Möglichkeit, die Adresskonfiguration weiterhin per statusloser Autokonfiguration erfolgen zu lassen, jedoch weitere Konfigurationsdetails per DHCPv6 auf die Clients zu bringen.

Abweichend von DHCPv4 läuft bei v6 die Kommunikation über die UDP-Ports 546 (Client) und 547 (Server).




</doc>
<doc id="14671" url="https://de.wikipedia.org/wiki?curid=14671" title="Embarcadero Delphi">
Embarcadero Delphi

Delphi ist eine vom Unternehmen Borland entwickelte Entwicklungsumgebung für die Programmiersprache Object Pascal. Im November 2006 wurden die Entwicklerteams der Entwicklungswerkzeuge, darunter auch Delphi, von Borland in eine neue Tochtergesellschaft namens CodeGear ausgegliedert. Am 7. Mai 2008 wurde die IDE-Sparte an Embarcadero Technologies verkauft.

Der Name "Delphi" hat seinen Ursprung in der starken Datenbankorientierung der Entwicklungsumgebung. Oracle ist sowohl der Name eines Datenbanksystems als auch das englische Wort für Orakel. Als bekanntestes Orakel gilt das Orakel von Delphi. Dieser Zusammenhang wird auch von Borland-Mitarbeitern als Ursprung des Namens genannt.

Für die Entwicklung werden außer der Entwicklungsumgebung noch folgende Programmierschnittstellen installiert, die der Nutzer anschließend in seinen Projekten verwenden kann:
Eine Besonderheit von Delphi ist, dass damit erstellte Programme in aller Regel keinerlei Anforderungen an installierte Software auf dem Zielbetriebssystem stellen. Ausnahmen gibt es beispielsweise bei Verwendung von Ribbons und natürlich .NET. Für all diese Schnittstellen gibt es eine unüberschaubare Zahl an Erweiterungen durch die Nutzer. Bei den kostenpflichtigen Versionen von Delphi liegt der komplette Quelltext dieser Schnittstellen bei.

Darüber hinaus können auch Programme ohne diese Schnittstellen direkt mit Windows-API-Aufrufen erstellt werden, was zu kleineren Programmdateien führt, aber deutlich weniger komfortabel in der Entwicklung ist.

Die erste Vorabversion von Delphi wurde im Herbst 1994 von Borland an einen begrenzten Entwicklerkreis verkauft. Diese mitsamt ihrem Quellcode ausgelieferte Version zeigte noch deutliche Unterschiede zur kommenden Delphi-Version 1.

Die erste Version von Delphi wurde am 14. Februar 1995 veröffentlicht und erzeugte nur Code für 16-Bit-Anwendungen für Windows 3.x.
Der Umfang der mitgelieferten Komponenten war relativ gering.

Delphi 2 wurde im März 1996 veröffentlicht und ist die erste Version, die 32-Bit-Anwendungen für Windows erzeugen konnte.
Die Vorgängerversion Delphi 1 und die englische Version von Delphi 2 waren zusätzlich auf der CD-ROM enthalten.

Delphi 2 war durch den Übergang von 16 auf 32 Bit erheblich weiterentwickelt worden. Neben moderneren Komponenten im Windows-95-Stil lag der Vorteil vor allem bei dem viel größeren lokalen Speicher. Vorher war die Größe aller Datentypen auf maximal 64 KByte (-16 Byte) beschränkt. Zeichenketten konnten nur 255 Zeichen lang sein. Nun konnten Zeichenketten und Datenstrukturen bis zu 2 GByte groß bzw. lang sein.

Delphi 3 wurde im Mai 1997 veröffentlicht.

Wesentliche Veränderungen zur Vorgängerversion waren das Eliminieren zahlreicher Bugs, eine Erweiterung der IDE durch Code Insight, weitere Common-Control-Komponenten (ToolBar, CoolBar) und die Aufnahme von Komponenten für Internet-Protokolle. Die wichtigste Änderung betrifft die Einführung der „Packages“. In Version 2 mussten zusätzliche Komponenten in die Komponentenbibliothek integriert werden. Mit der Einführung der Packages konnten eigene oder externe Komponenten dynamisch hinzugefügt oder entfernt werden.

Delphi 4 wurde im Juli 1998 veröffentlicht und brachte die Unterstützung dynamischer Arrays und ActionLists sowie einige Verbesserungen der IDE, besonders in Bezug auf Übersichtlichkeit und die Verwaltung großer Projekte und Projektgruppen.

Delphi 5 wurde im August 1999 veröffentlicht.
Mit Delphi 5 konnte erstmals über ADO auf Datenbanken zugegriffen werden.
Diese Erweiterung musste in der Professional-Version allerdings als "ADO Express" zugekauft werden.
Außerdem hielten Komponenten Einzug, mit denen via OLE auf Microsoft-Office-Anwendungen zugegriffen werden konnte. Dies war allerdings durch direkten Aufruf der OLE-Befehle schon vorher möglich.

Delphi 6 wurde im Mai 2001 veröffentlicht.
Mit Delphi 6 konnte erstmals mit dem zusätzlichen Modellierungswerkzeug Modelmaker aus UML-Modellen Delphi-Programmcode erzeugt werden. Modelmaker erlaubt das Erstellen von Diagrammen in der Unified Modeling Language UML, einer Standardsprache zur Modellierung objektorientierter Systeme. Die manuelle Programmierung wird somit reduziert.
Für Datenbankzugriffe hielt die neue Schnittstelle dbExpress Einzug. Aufzählungstypen kann ein bestimmter, ordinaler Wert zugewiesen werden.

Von Delphi 6 existierte eine kostenlose Personal Edition.

Delphi 7 wurde im August 2002 veröffentlicht.
Mit Delphi 7 wurde ein Preview Compiler für .NET-Entwicklung in Delphi ausgeliefert.
Das Modellierungswerkzeug Modelmaker wurde ab dieser Version in Delphi integriert.
Der Berichtsgenerator wurde mit Delphi 7 von QuickReport hin zu Nevrona Rave Reports 5 geändert.

Auch von Delphi 7 existierte eine kostenlose Personal Edition.

Delphi 8 wurde im Dezember 2003 veröffentlicht.
Es umfasste nur .NET-Entwicklung. Im Lieferumfang war daher auch Delphi 7 enthalten.

Delphi 2005 wurde seit November 2004 ausgeliefert. Neben nativen Windows-32-Bit-Anwendungen können mit Delphi 2005 auch .NET-Anwendungen in Delphi und C# erstellt werden. Im Gegensatz zu den Vorversionen Delphi 7 (Win32) und Delphi 8 (.NET) gibt es nun nur noch eine Delphi-IDE oder auch „Borland Developer Studio“.

Mit Delphi 2005 hielten umfassende Neuerungen wie zum Beispiel Refactoring in die IDE Einzug. Auch die Programmiersprache selbst wurde um einige Funktionen wie zum Beispiel das For-In-Konstrukt erweitert (entspricht For-Each in Visual Basic).
Der Compiler kann nun auch sprachspezifische Zeichen interpretieren. So können im deutschen Quelltext auch die Sonderzeichen ä, ö und ü verwendet werden.
Die IDE zeigt umfassende Hilfetexte in einer besseren Übersichtlichkeit an.
Allerdings traten auch viele Fehler innerhalb der IDE und den mitgelieferten Komponenten auf, was zu einer gewissen Unzufriedenheit bei den Nutzern geführt hat, vor allem auch in Bezug auf die lange Ladezeit der IDE. Dies wurde mit einigen Programmaktualisierungen behoben.

Auch von Delphi 2005 existierte eine kostenlose Personal Edition.

Diese Version hat neue Sprachunterstützung für den Win32-Compiler für Records mit Methoden, Operatorüberladung und Klassenvariablen erhalten; damit wird fast der Stand der bestehenden Features des Delphi für .NET-Compiler erreicht.

Delphi 2006 enthält außerdem volle C++-Sprachunterstützung, inklusive VCL-Formdesigner (wie im Borland C++Builder), die allerdings, da sie nicht rechtzeitig fertig wurde, erst mit Update 1 vervollständigt wurde. Dies wurde offiziell angekündigt, ein Novum in der Produktpolitik von Borland.

Die IDE enthält neue Funktionen wie z. B. die „Live Templates“, Blockvervollständigung und Einrückung während des Tippens und veränderte Zeilen-Indikatoren am linken Editor-Rand.

Außerdem ist ein Kommandozeilen-Compiler für .NET-Compact-Framework-Anwendungen enthalten. Damit können prinzipiell Anwendungen für Windows CE und Pocket PCs erstellt werden, auch wenn der Formulardesigner noch nicht dafür ausgelegt ist, es noch keinen Debugger gibt und man einen Emulator von einer anderen Webseite herunterladen muss (kostenlos).

Seit dem 5. September 2006 gibt es von Delphi 2006 so genannte „Turbo-Versionen“. Es handelt sich dabei um abgespeckte Versionen des Borland Developer Studios 2006 (Professional).

Parallel zum Spin-off der IDE-Sparte von Borland in das neue Unternehmen CodeGear wurde eine neue "Turbo"-Produktlinie eingeführt, die wohl auf die Erfolge von Turbo Pascal in den 1980ern und frühen 1990ern anspielen sollte. Als "Turbo"-Produkte wurden die Komponenten Delphi, Delphi für .NET, C++ und C# als einzelne Module vertrieben.

Hierbei gab es für alle Produkte eine kostenlose, im Funktionsumfang eingeschränkte "Explorer"-Version, die keinen Kommandozeilen-Compiler enthielt und in der keine Komponenten anderer Hersteller in die Tool-Palette installiert werden konnten – man musste mit den 200 mitgelieferten auskommen oder die Drittkomponenten zur Laufzeit erzeugen. Daneben gab es "Professional"-Versionen, die diese Einschränkung nicht besaßen. Allen Turbo-Versionen ist gemein, dass auf einem Rechner jeweils nur ein Produkt der Turbo-Reihe installiert werden kann. Dies hat allerdings keine lizenzrechtlichen, sondern ausschließlich technische Gründe, die mit einem Tool umgangen werden konnten.

Ursprünglich war geplant, die Turbo-Versionen als festen Bestandteil in die Produktpalette mit aufzunehmen, dieser Plan wurde jedoch wieder aufgegeben. Die Turbo-Produktlinie blieb auf dem Stand von Delphi 2006, ab September 2009 wurden die Download-Links von der Seite „turboexplorer.com“ entfernt und durch Links zu Testversionen ersetzt.

Diese im März 2007 erschienene und separat lieferbare Variante besitzt auf vielfachen Kundenwunsch nur die Delphi für Win32-Personality und wartet unter anderem mit folgenden Neuerungen auf: Anpassungen und Erweiterungen für Windows Vista, überarbeitetem Datenbankframework dbExpress, einer erweiterten Version von Intraweb mit Ajax-Unterstützung unter dem Namen „VCL for the Web“ und einer überarbeiteten Hilfe. Diese Version ist auch die erste Version, die als Download über das Internet bezogen werden kann sowie alle Sprachen (Englisch, Deutsch, Japanisch, Französisch) zusammen enthält. Ebenfalls erschienen ist ein Delphi für PHP, das aber außer dem Namen und einer gewissen Ähnlichkeit der IDE mit Delphi weiter nichts zu tun hat.

Die im September 2007 erschienene Version kombiniert Delphi, Delphi für .NET und C++ Builder unter dem Namen RAD Studio 2007 (RAD steht hierbei für Rapid Application Development). Die ursprünglich für diese Version angekündigte Unterstützung von mobilen Geräten auf Basis des Compact Frameworks wurde auf einen späteren Zeitpunkt verschoben. Zusätzlich wird das mit Delphi 2007 zur Version 4 umgebaute Datenbankframework dbExpress weiter ausgebaut und eine neue embedded Datenbank namens BlackFish SQL mitgeliefert. Diese basiert auf .NET und ist kompatibel zum existierenden JDataStore. Mit RAD Studio 2007 wird auf der .NET Seite das Framework Version 2.0 unterstützt, inklusive ASP.NET 2.0 und damit auch Generics. Klassen des .NET Frameworks 3.0 können mit etwas manueller Arbeit auch genutzt werden, da diese Version keine Spracherweiterungen (wie 2.0 Generics), sondern lediglich neue Funktionen beinhaltet. Die integrierte Hilfe wurde weiter überarbeitet und enthält inzwischen wieder einige Codebeispiele. Der WinForms-Formulardesigner für .NET wurde jedoch weggelassen, da Microsoft die APIs des im .NET-Framework enthaltenen Designers grundlegend geändert hatte.

Ursprünglich als Delphi 2008 angekündigt, ist Tiburón seit dem 25. August 2008 als Delphi 2009 erhältlich. Es gab umfangreiche Änderungen an IDE, RTL und VCL. Bei der IDE wurde u. a. der "ClassBrowser" überarbeitet, ein "ResourceBrowser" zum Verwalten der mit in die exe-Datei einzukompilierenden Ressourcen ergänzt und benutzerdefinierte Build-Configurations (debug, release) hinzugefügt. Mehrere neue Komponenten, darunter ein "Ribbon Control", mit dem Menüs wie bei Office 2007 möglich sind, erweitern die VCL. Zudem wurde die gesamte RTL und VCL auf Unicode umgestellt, es wird seitdem insgesamt eine breite Unicode-Unterstützung durch die Programmiersprache zur Verfügung gestellt. Als Spracherweiterung hielten Lambda-Funktionen und Generics Einzug in Delphi für Win32. Die .NET Unterstützung wurde entfernt.

Neben Delphi 2009 firmiert auch der C++Builder 2009 unter dem Namen "Tiburón" (Spanisch für „Hai“). Werden beide installiert, integrieren sie sich in eine einzige IDE.

Die Delphi-.NET-Framework-Version, Delphi Prism, wurde Ende Oktober 2008 von Embarcadero vorgestellt. Im Gegensatz zu früheren Delphi-für-.NET-Versionen wird für Prism nicht mehr die Delphi-IDE, sondern Visual Studio verwendet. Embarcadero arbeitet hierfür mit RemObjects zusammen, um das Produkt Oxygene als Delphi Prism weiterzuführen.

Mittels Prism wird es möglich, Mono als .NET-Implementation zu nutzen; damit können Programme für Linux und macOS entwickelt werden. Die Entwicklung für Windows Mobile wird weiterhin nicht offiziell unterstützt, da der WinForms-Oberflächengestalter hardcodiert für VB.NET und C# ist.

Die Programmiersprache ist nicht kompatibel zum bestehenden Delphi-.NET-Sprachumfang, für die fehlende RTL- und VCL.Net-Unterstützung gibt es ein Projekt namens ShineOn.

Ende August 2009 hat Embarcadero Delphi 2010 veröffentlicht. Der Fokus liegt bei dieser Version auf Verbesserungen der IDE, Windows-7-Unterstützung und Gesten-Steuerung für die erzeugten Programme. Die Gestensteuerung wird auch für Betriebssysteme (Windows 2000 und XP) unterstützt, für die Microsoft selbst keine entsprechende Programmierschnittstelle bietet. Die mit Delphi 2009 umgebaute Lösung DataSnap zur Erzeugung mehrschichtiger verteilter Anwendungen wurde ebenfalls stark erweitert. Neu ist in den Enterprise- und Architect-Varianten auch ein dbExpress-Datenbanktreiber für Firebird. Die Delphi-2010-IDE unterstützt offiziell kein Windows 2000 mehr, Programme für Windows 2000 lassen sich jedoch weiterhin problemlos damit entwickeln. Als Vorlage für die integrierte Hilfe dienen nun die Texte eines Wikis.

Am 31. August 2010 hat Embarcadero Delphi mit dem Namenszusatz XE veröffentlicht. Cross-Plattform-Features (OSX) wurden auf spätere Versionen verschoben. Die Linux-Unterstützung sowie die 64-Bit-Unterstützung ist ebenfalls noch nicht in Delphi XE erschienen. Neu enthalten ist u. a. eine Subversion-Integration sowie die Unterstützung von Cloud-Programmierung und weitere DataSnap-Verbesserung (u. a. das Erstellen von DataSnap-Servern aus C++-Quelltext heraus). Ebenfalls neu sind einige mitgelieferte Produkte von Drittanbietern wie AQTime Standard (ein Laufzeitprofiler), Final Builder (Management von Buildvorgängen) und eine professionelle Logging-Lösung. Ebenso fanden zahlreiche Fehlerkorrekturen (z. B. für Generics oder IDE-Funktionen) den Weg ins Produkt.

Am 27. Januar 2011 hat Embarcadero verkündet, dass nun eine Starter Edition verfügbar ist, deren Funktionsumfang unterhalb der bisherigen Pro Edition ist und die speziell für Hobby-Anwender und Startups geeignet ist. Die Lizenzbedingungen erlauben nur eine eingeschränkte kommerzielle Verwendung. Seit September 2016 ist Delphi Starter (zu dem Zeitpunkt also 10.1) kostenlos. Somit wurde erstmals seit 10 Jahren wieder eine kostenlose Delphi-Version veröffentlicht.

Am 1. September 2011 ist Delphi XE2 auf dem Markt erschienen. Es bringt eine ganze Reihe neuer Funktionen mit, unter anderem: 64-Bit-Windows-Compiler, 32-Bit-Mac OS-Compiler sowie Kompilierung für das Betriebssystem Apple iOS. Die Unterstützung für iOS (iPhone/iPad) erfolgt derzeit noch mittels Free-Pascal-Compiler und erfordert einen Apple-Computer mit XCode-Entwicklungsumgebung. Sämtliche nicht 32-Bit-Ziele werden von der Delphi-Entwicklungsumgebung per Crosscompilation bedient, es gibt dabei sowohl für 64 Bit als auch für macOS jeweils einen Remote Debugging Client, der einerseits die Verteilung des kompilierten Programms, als auch das Debuggen aus der Entwicklungsumgebung heraus unterstützt. Außer den neuen Compilern führt Delphi XE2 eine neue GUI-Komponentenbibliothek namens FireMonkey ein, die zur Entwicklung plattformunabhängiger Anwendungen benötigt wird. Firemonkey unterstützt 2D- und 3D-Grafikbeschleunigung und basiert in weiten Teilen auf der aufgekauften VGScene-Technik. Ebenfalls neu ist die Möglichkeit, VCL-Programme mittels Styles optisch zu verändern, auch mit selbsterstellten Styles. Weitere Neuerungen sind ein Databinding-Konzept, um Werteänderungen von Klasseneigenschaften mit dem GUI zu synchronisieren, ein ODBC-dbExpress-Datenbanktreiber für Windows und weitere Erweiterungen für die DataSnap-RFC-Technik.

Eine Starter Edition gibt es gleich von Beginn an, und eine weitere neue Edition namens Ultimate wurde eingeführt, die zwischen Enterprise und Architect liegen soll. Einziger nennenswerter Unterschied ist, welches Datenbankprogramm beiliegt. Als neuer Reportgenerator ist eine Light-Version von "FastReports" enthalten.

Am 3. September 2012 hat Embarcadero Delphi XE3 veröffentlicht. Zu den Neuerungen zählt die Unterstützung für Microsoft Windows 8 unter anderem mit neuen Controls und Styles für Modern-UI, jedoch ohne die Unterstützung Anwendungen für den Windows Store schreiben zu können. Das plattformübergreifende Framework Firemonkey liegt nun in der Version 2 vor. Die in XE2 eingeführte iOS-Unterstützung wurde entfernt, da Embarcadero an einem eigenen iOS-Compiler arbeitet. Außerdem wurde Firemonkey 2 an einigen Stellen geändert, was einen größeren Aufwand für die Anpassung der auf Free Pascal basierten Lösung bedeutet hätte. Auf der C++ Builder Seite kam mit dem Update 1 ein LLVM basierter 64 Bit Compiler hinzu.

Am 22. April 2013 ist Delphi XE4 auf dem Markt erschienen. Ein ARM-Compiler für iOS ist nun im FireMonkey-FM3-Framework (erfordert mindestens die Enterprise-Edition oder Mobile Add-On Pack für die Professional-Edition) enthalten. Nach der Übernahme der AnyDAC Datenbankkomponenten wurden diese erwartungsgemäß unter dem Namen FireDAC integriert; sie sind in der Enterprise-Version enthalten bzw. erfordern bei Professional den Kauf des FireDAC Client/Server Pack.

Am 11. September 2013 ist Delphi XE5 auf dem Markt erschienen. Ein ARM-Compiler für Android sowie die notwendige RTL-, Firemonkey- und IDE-Integration samt Debugger sind hinzugekommen. Die FireDAC-Datenbankkomponenten sind nun bereits in der Professional Version enthalten.

Am 15. April 2014 ist Delphi XE6 auf dem Markt erschienen. Wesentliche Neuerungen dieser Version sind z. B. "App-Tethering"-Komponenten, also Komponenten, damit mobile Delphi-Apps einfach mit anderen Anwendungen interagieren können, sowie Integration von Cloud-Services (BaaS), insbesondere für etablierte Cloud-Dienstleister incl. Benutzerauthentifizierung. Weiterhin wurden Tools/Komponenten zur einfachen Einbindung von Kauf- und Werbemöglichkeiten aus einer Anwendung heraus hinzugefügt, sowie erweiterte Sensor-Anbindungen in der VCL. Zusätzlich sind ein erweitertes Refactoring und weitere VCL-Styles sowie Verbesserungen für Mobil-Programmierung hinzugekommen.
Es können nun auch – neben normalen Android-Apps – Android-Anwendungen erstellt werden, die sich auf Google Glass ausführen lassen.
Zudem wurden in einer Qualitäts-Offensive über 2000 Fehler behoben.

Am 2. September 2014 ist Delphi XE7 auf dem Markt erschienen. Neuerungen sind unter anderem der "FireUI Multi-Device Designer" mit UI Komponenten, nativer Bluetooth Support, Enterprise Mobility Services, eine Parallel Computing Library und eine native OmniXML library.

Mit dieser neuen Delphi-Version sind auch Entwicklungen auf Einplatinencomputern wie dem Banana Pi (einem dem Raspberry PI ähnlichen Einplatinencomputer) oder
Beaglebone Black 
unter Android oder weiteren mobilen Devices, wie z. B. der Android Smart-Watch Moto 360 möglich.

Am 7. April 2015 ist Delphi XE8 auf dem Markt erschienen und bringt neben einer iOS-64-Bit-Unterstützung einen weiter entwickelten Multi-Plattform-Designer sowie eine Vorschau für das Aussehen einer Form auf unterschiedlichen Plattformen mit. Die zugekaufte IDE-Erweiterung Castalia wurde direkt in die IDE integriert, und das Hilfe-System wurde auf HTML-Help (CHM) umgestellt. Für Classic-Bluetooth wurde nun auch eine Komponente für die RAD-Anwendung hinzugefügt und mit Box2D wurde eine 2D-Physik-Engine integriert. Für Windows-Entwickler interessant könnte die neue Client-seitige Unterstützung des HTTP-API von Windows sein, da dadurch für verschlüsselte Verbindungen die Betriebssystem eigene Funktionalität, die über Windows-Update aktuell sein sollte und nicht das selbst aktuell zu haltende OpenSSL zum Einsatz kommt. Ein einfacher Paket-Manager namens GetIt hielt auch Einzug in die IDE. Über diesen können verschiedene Open-Source-Komponenten auf einfache Weise installiert werden.
Weitere Neuerungen dieser Version sind u. a. die Unterstützung von EMS-Services, Komponenten für das Management von iBeacons, die Möglichkeit der Einrichtung von Anwendungsverwendungs-Nachverfolgung bzw. Benutzerstatistiken z.B: für Werbezwecke.

Am 31. August 2015 wurde Delphi 10 Seattle veröffentlicht.
Zudem wurden die Kürzel DX (Delphi), CX (C++ Builder) und RX (RadStudio) für die jeweilige Version 10 eingeführt. Neu sind beispielsweise die Unterstützung von Services unter Android oder VCL-Erweiterungen für die bessere Unterstützung von Windows 10, NoSQL-Mongo-DB-Unterstützung via FireDAC. Außerdem unterstützt die IDE selbst die Verwendung von mehr Hauptspeicher, was dazu führt, dass bei größeren Projekten nicht so schnell "out of memory" Probleme auftauchen.

Am 19. April 2016 wurde Delphi 10.1 Berlin veröffentlicht. Das plattformübergreifende Framework Firemonkey und die VCL wurde in dieser Version stark verbessert. Auch wurde der von Delphi 7 bekannte "ungedockte" Designer für das VCL- und FMX-Formulardesign wieder eingeführt.

Am 22. März 2017 wurde Delphi 10.2 Tokyo mit einem zusätzlichen 64-Bit Linux Compiler veröffentlicht. Dieser beschränkt sich auf Konsolen-, Webserver (Apache 2.4 Module) und sonstige GUI-lose Anwendungen. Daneben gibt es Verbesserungen für die IDE, offizielle Unterstützung für Android 7.x (Nougat) , und einen Mehr-Mandaten RAD Server (Multi-Tenancy) über EMS.

Die gegenwärtige Roadmap listet mehrere Projekte zur Weiterentwicklung von Delphi auf, die unter anderem folgende Punkte umfassen: Unterstützung von 4K-Monitoren, Unterstützung für Internet of Things, 64-Bit-Unterstützung für OSX. Linux Serveranwendungen.

Unter dem Namen AppMethod gab es Delphi auch als preiswertere Editionen für einzelne Plattformen, wobei jeweils nur eine begrenzte Anzahl an Plattformen pro PC gleichzeitig installiert sein konnten. Die Veröffentlichungstermine der AppMethod-Versionen unterschieden sich allerdings in der Regel von denen der regulären Delphi- und RAD-Studio-Veröffentlichungsterminen. Anfang 2016 wurde dieses Angebot wieder eingestellt.

Für Linux/X11 auf Intel x86 war ein ähnliches Paket unter dem Namen Kylix verfügbar, das allerdings nicht mehr gepflegt wird. Da die Entwicklungsumgebung diverse Fehler enthielt, gibt es zahlreiche Unternehmungen, diese zu stabilisieren und die Kompilate unter neueren Linux-Distributionen lauffähig zu machen. Dazu zählen inoffizielle Fehlerkorrekturen, als auch eine Alternative namens CrossKylix, mit der sich der Kylix-Compiler unter Windows benutzen lässt.

Für die Sprache C++ bietet Embarcadero ein ähnliches Entwicklungssystem namens C++Builder an, das eng mit der Delphi-IDE verwandt ist. (Der oft hiermit verwechselte C++BuilderX ist eine auf der Primetime-IDE des JBuilder basierende, erweiterbare IDE für Cross-Plattform-Entwicklung sowie die Unterstützung mehrerer Compiler und hat mit C++Builder kaum Gemeinsamkeiten.)

Lazarus ist eine Entwicklungsumgebung für Object Pascal, zu dessen Sprachfamilie auch Delphi gehört. Der Quellcode ist zu großen Teilen Delphi-kompatibel, was es einfach macht, Code zwischen beiden Entwicklungsumgebungen zu migrieren. Die IDE steht unter der LGPL. Lazarus ähnelt älteren Delphi-Versionen, auch werden ähnliche Konzepte (LCL statt VCL) verwendet. Die Lazarus-IDE setzt auf den Free-Pascal-Compiler auf und ist unter Linux, macOS und Windows lauffähig, ebenso können für diese und einige andere Plattformen Programme erzeugt werden.

WDSibyl wurde ursprünglich als kommerzielles Produkt von dem deutschen Unternehmen Speedsoft entwickelt (Speed Pascal, Sibyl Pascal). Die Sourcen der IDE unterliegen der GPL, die Quelltexte der Laufzeit- und Klassenbibliotheken hat das Unternehmen schon vor Jahren auf seiner Homepage freigegeben. Die ursprünglichen Quellcodes werden von freien Entwicklern weitergeführt und unterliegen ebenfalls der GPL. Die Delphi-ähnliche Entwicklungsumgebung läuft unter OS/2 und Windows. Ein ähnliches Projekt war Virtual Pascal des britischen Unternehmens fPrint UK Ltd.

Die von Microsoft früher vertriebene Entwicklungsumgebung Visual Basic (bis Version 6) war ebenfalls mit Delphi konzeptionell und optisch bis zu einem gewissen Grad vergleichbar, wobei der Sprachumfang jedoch, insbesondere objektorientierte Aspekte betreffend, nicht an den von Delphi heranreichte. Auch war es in Visual Basic nicht möglich, Komponenten in der eigenen Umgebung mit der gleichen Sprache selbst zu entwickeln, was in Delphi von Anfang an der Fall war. In Visual Basic musste man dies in C/C++ durchführen. Weiterhin können in Delphi seit der ersten Version Codepassagen für optimierte Programmgeschwindigkeit direkt in Assemblersprache formuliert werden, was in Visual Basic auch nicht möglich war. Zusätzlich musste zu jedem Programm, das in Visual Basic geschrieben war, die spezielle Visual Basic Runtime (Laufzeitumgebung) mitgeliefert werden. Die letzte von Microsoft herausgegebene Version Visual Basic 6 stammt aus dem Jahre 1998, die Unterstützung wurde im April 2008 eingestellt. Das Produkt wurde von der überwiegend objektorientierten Neuentwicklung VB.NET auf Basis des .NET-Frameworks abgelöst. Quellcode aus VB 6 ist nicht mit VB.NET kompatibel. Die Visual Basic 6.0 Laufzeitumgebung wird weiterhin bis einschließlich Windows 8 über die Dauer des jeweiligen "support life cycle" unterstützt.




</doc>
<doc id="14672" url="https://de.wikipedia.org/wiki?curid=14672" title="Rapid Application Development">
Rapid Application Development

Rapid Application Development (RAD) bzw. schnelle Anwendungsentwicklung ist ein von Barry Boehm in den 1980er Jahren miterdachtes Konzept zur Softwareentwicklung mit einem prototypischen Vorgehensmodell. Es dient dazu, Softwareentwicklung im Vergleich zu klassischen Vorgehensmodellen wie dem Wasserfallmodell flexibler zu gestalten und sich an schnell ändernde Anforderungen anpassen zu können.

Grundlage von RAD ist das Spiralmodell. RAD sieht ein prototypisches Vorgehen vor, bei dem Anforderungen an eine Software gesammelt und möglichst schnell in ausführbaren Code umgesetzt werden. Dieser wird dem Auftraggeber in einer relativ frühen Phase vorgelegt, um Missverständnisse bei den Anforderungen sowie hinzugekommene Anforderungen zu identifizieren. Die Änderungen werden in einer weiteren Version implementiert und wiederum vorgelegt. Diese Zyklen werden so oft durchlaufen, bis der Auftraggeber mit der Software zufrieden ist und diese abnimmt.

RAD-Entwicklungsumgebungen unter Windows sind Microsoft Visual Basic und Embarcaderos "RAD Studio" (ein Softwarepaket bestehend aus Delphi und C++Builder). Als RAD-Tools gelten Visual Studio LightSwitch für .NET, Servoy sowie XDEV 3 für Java, Omnis Studio und 4th Dimension für Mac und Windows, oder der IBM Lotus Domino Designer. Speziell Bedienoberflächen lassen sich mit solchen Werkzeugen visuell entwerfen.

Der RAD Ansatz benötigt in der Regel weniger Vorarbeit und Vorplanung und sorgt dadurch schneller für produktive Ergebnisse. Dank der eingesparten Zeit und der parallelen Entwicklung der verschiedenen Software-Entwicklungsphasen, kann durch diese Form der Softwareentwicklung eine einsatzbereite Software im Regelfall in weniger als 120 Tagen ausgeliefert werden. Außerdem kann durch die konstante Interaktion zwischen Anwendern und dem entwickelnden Prototypen eine höhere Qualität sichergestellt werden. 



</doc>
<doc id="14674" url="https://de.wikipedia.org/wiki?curid=14674" title="Maas">
Maas

Die Maas (, , , und ) ist ein etwa 874 Kilometer langer Fluss, der Frankreich, Belgien und die Niederlande durchfließt. Die Maas mündet in den südlichen Hauptstrom des Rhein-Maas-Deltas und gehört damit zum Flusssystem des Rheins. Die Maas ist der weitaus längste und, nach der Aare, der zweitwasserreichste Nebenfluss des Rheins. Sie ist zudem die Namenspatin des drittgrößten Rhein-Nebenflusses, der Mosel (lateinisch "Mosella": „Kleine Maas“).

Die Maas entspringt auf in Pouilly-en-Bassigny im Gemeindegebiet von Le Châtelet-sur-Meuse (Département Haute-Marne) auf dem Plateau von Langres. Nach ihrem langen Oberlauf in Frankreich durchquert sie innerhalb Belgiens die Ardennen sowie die Städte Namur und Lüttich und passiert dann in den Niederlanden die Städte Maastricht, Roermond und Venlo. Von Borgharen bei Maastricht bis etwa Höhe Maasbracht bildet die Maas (hier "Grensmaas" genannt) die Grenze zwischen den Provinzen Belgisch Limburg und Niederländisch Limburg. Die Grenzmaas ist zwischen Borgharen und Maaseik nicht schiffbar. Der Schiffsverkehr läuft über den parallel geführten Julianakanal. Die Maas wendet sich dann westwärts und bildet die Grenze zwischen den niederländischen Provinzen Gelderland im Norden sowie Nordbrabant im Süden. Als "Amer" mündet sie in das Hollands Diep, eine ehemalige Meeresbucht, die heute Teil des südlichsten der Mündungsarme des Rheins ist.
Das heute sehr schmale Einzugsgebiet des Maas-Oberlaufes ist eine Folge von Gebietsverlusten durch Flussanzapfungen aus den etwas tiefer liegenden benachbarten Flussgebieten heraus, also dem der Seine im Westen und dem der zum Rhein fließenden Mosel im Osten. Die Maas verläuft im Oberlauf vorwiegend durch Serien meist widerstandsfähiger Kalksteine des Oberjura (in Deutschland bekannt als "Malm"), wogegen die dortigen Nachbarflussgebiete eher in leichter ausräumbaren Gesteinen liegen. 

Vor rund einer Million Jahren wurde die heutige Aisne zur Oise und damit zum Flusssystem der Seine hin abgelenkt. Durch das verlassene breite Tal fließt heute die Bar zur Maas. Während der Saale-Kaltzeit, vor rund 250.000 Jahren, verlor die Maas sogar ihren Oberlauf, in diesem Fall an die Mosel. Das 12 km lange Tal zwischen Toul und Pagny-sur-Meuse fiel als Folge der Flussanzapfung trocken, ein damaliger linksseitiger Maas-Nebenfluss wurde so zum heutigen Oberlauf der Maas.
Unterhalb des Austritts aus ihrem Engtal im sich aktuell hebenden Rheinischen Schiefergebirge (Ardennen) lagert die Maas ihre mitgeführte Sedimentfracht in einer breiten Aufschüttungsebene ab, die sich bis zur Nordsee erstreckt und vor allem von Ablagerungen des Rheins gebildet wird. Sandig-kiesige Ablagerungen der Maas finden sich vor allem in der belgisch-niederländischen Region Limburg. Dort entstanden durch den in großem Umfang betriebenen Abbau von Kies Mitte des 20. Jahrhunderts die Maasseen (niederl. Maasplassen; von niederl. "plas" für Tümpel). Nach der großen Flut Mitte des 20. Jahrhunderts (Hollandsturmflut) wurden durch gewaltige Bauvorhaben im Bereich der Küste für Deiche und Schutzwehre (Deltawerke) sowie auch für die Eindeichungen (Polder) in den Provinzen Zeeland und Noord-Holland große Mengen an Sand und Kies zur Betonherstellung und für Aufschüttungen benötigt.

Der Mündungsbereich der Maas bildet mit dem des Rheins das Rhein-Maas-Delta. Bei Cuijk zweigt der Maas-Waal-Kanal ab, der bei Nijmegen den Hauptarm des Rheins erreicht. Die Maas selbst mündet seit 1904 über einen entlang des ehemaligen Baches "Oude Maasje" künstlich geschaffenen Abflussweg, genannt "Bergse Maas" und "Amer", in das "Hollands Diep", eine heute vom wasserreichen Rheinarm "Nieuwe Merwede" durchflossene frühere Nordseebucht. Davor floss der Hauptteil des Maaswassers über den seit 1273 bestehenden und nun "Afgedamde Maas" genannten Wasserlauf in den Hauptarm des Rheindeltas, die Waal. Seit dem Bau des Haringvlietdammes im Jahr 1970 sind auch Hollands Diep und Haringvliet zu Teilstrecken des südlichen Rhein-Hauptarms über Waal und Nieuwe Merwede geworden. Das Wasser von Maas und Rhein erreicht seitdem wieder vereint das Meer, bei Niedrigwasser vorwiegend über den "Nieuwe Waterweg" bei Rotterdam, bei hohem Wasserstand überwiegend über die Schleusen des Haringvlietdammes. Insgesamt wird die Wasserverteilung im Rhein-Maas-Delta sorgfältig gesteuert, vor allem in Abhängigkeit von der Wasserführung des Rheins beim Pegel Lobith an der Deutsch-Niederländischen Grenze. So durchquert bei mittleren und niedrigen Wasserständen das meiste Wasser der Maas zusammen mit dem des Rheins noch die Stadtgebiete von Dordrecht und Rotterdam, bevor es das offene Meer erreicht.

Die Landschaft des Mündungsbereiches liegt großenteils unter dem Niveau des Meeresspiegels. In früheren Jahrhunderten veränderte sich nach fast jeder schwereren Sturmflut bzw. Hochwassersituation an Maas und Waal ihr Aussehen und damit auch der Verlauf der Flüsse und Bäche. Die Benennung der Gewässer im Rhein-Maas-Delta ist weitgehend unberührt geblieben von den stark geänderten Abflusswegen. Wegen des einst weiter nördlich verlaufenden Rheins waren viele Rheinarme einstige Unterläufe der von Süden kommenden Maas wie die "Nieuwe Maas" und "Oude Maas".

Das Einzugsgebiet des französischen Oberlaufs im Hügelland Lothringens (rund 30 Prozent des gesamten Einzugsgebietes) ist lang gestreckt und schmal, was extremen Hochwasserspitzen entgegenwirkt, ebenso wie das oft wasserspeichernde Gestein. Dagegen entwässert der belgische Teil (rund 40 Prozent) die niederschlagsreichen Ardennen mit vielen gefällereichen Nebenflüssen, was die Hochwassergefährlichkeit der Maas erhöht. An der Grenze zu den Niederlanden (am Pegel Borgharen) führt die Maas rund 260 m³/s Wasser. In den Niederlanden ist das Gelände nahezu eben. An ihrer Mündung entwässert die Maas mit einer Wasserführung von rund 357 m³/s ein Einzugsgebiet von rund 33.000 km². Im unteren Teil werden die Grenzen des Einzugsgebietes uneinheitlich gezogen. Mitunter sind auch die Randbereiche von Hollands Diep und Haringvliet einbezogen, die aber im Wesentlichen von Rheinwasser durchströmt werden ("siehe hierzu auch" Flusssystem des Rheins). Entsprechend gibt es auch Längenangaben, die über 874 Kilometer (gerundet 875 km) hinausgehen.
In der folgenden Tabelle sind ausnahmsweise auch ein paar Kanäle eingetragen, weil sie für den Wasserhaushalt von Bedeutung sind. Einige verwirrende Seitenbeziehungen ergeben sich daraus, dass die nordbrabantische Aa mehrfach mittels Dükern unter der Zuid-Willemsvaart hindurchgeleitet wird.

Bei der wallonischen Nummerierung handelt es sich nicht um Wasserlauf-, sondern um Wasserkörper­nummern. Ist ein Fluss in mehrere Wasserkörper unterteilt, so hat die Behörde deren Benennungen mit römischen Zahlen unterschieden, Reihenfolge flussabwärts. Wo größere Zuflüsse dieser Gewässer als eigene Wasserkörper definiert wurden, kann das Gesamteinzugsgebiet größer sein als die Summe der hier aufgelisteten Wasserkörper. Umgekehrt ist wegen eingerechneter kleinerer Zuflüsse die für einen Wasserkörper angegebene Gewässerlänge oft größer als die Abschnittslänge seines Hauptgewässers. Liste aller Wasserkörper in der Wallonie siehe Umweltportal.

Im französischen Abschnitt wurde die Maas kanalisiert und trägt als Schifffahrtsweg den Namen Canal de la Meuse ("früher: Canal de l’Est – branche Nord"). Der Kanal verläuft zwischen der französisch-belgischen Grenze bei Givet und dem Ort Troussey. Aufgrund der Abmessungen der Schleusen und Brücken ist er nur für Pénichen im Freycinet-Maß, sowie für Sport- und Hausboote befahrbar.
In Dom-le-Mesnil zwischen Charleville-Mézières und Sedan zweigt der Canal des Ardennes ab und folgt nach Überquerung der Wasserscheide bei Le Chesne der Aisne bis in die île de France um Paris.
Flussaufwärts ist die Maas bis Troussey schiffbar. Hier kreuzt der Canal de la Marne au Rhin den Fluss und verbindet ihn westwärts mit Paris sowie ostwärts bei Toul mit der Mosel und bei Straßburg mit dem Rhein.

Über die Mosel und weiter im Canal des Vosges (früher Canal de l’Est, branche Sud) erreicht man die Saône und die Rhone, auf der man bis zum Mittelmeer gelangen kann.

Freizeitskipper aus Nordeuropa nehmen auf ihrer Fahrt zum Mittelmeer lieber diese Binnenroute als den wesentlich längeren und anspruchsvolleren Weg um Spanien herum.

Im belgischen und niederländischen Abschnitt ist die Maas für die Großschifffahrt ausgebaut.

Das oberste belgische Anschlussgewässer ist die Sambre, von der in Frankreich der Canal de la Sambre a l'Oise an die Oise führt. In Belgien gelangt man von der Sambre in den Kanal Charleroi-Brüssel (an die Senne), von dem nach Westen der Canal du Centre abzweigt, als Verbindung zur Schelde. 

In Lüttich zweigt von der Maas der am 30. Juli 1939 eröffnete Albertkanal nach Antwerpen ab, den die Maasschifffahrt nehmen muss, da ein weiteres Befahren der Maas hier nicht möglich ist. Bei Lanaye verbindet der "Kanaal van Ternaaien" den Albertkanal wieder mit der Maas. In Maastricht schließt die Zuid-Willemsvaart an, die bei 's Hertogenbosch wieder die Maas erreicht.

Im niederländischen Abschnitt wird mit insgesamt sieben Staustufen eine Mindestfahrwassertiefe von drei Metern gewährleistet. Seit 1822 ist die "Grensmaas", der Flussabschnitt zwischen Maastricht (NL) und Ohe en Laak (nordöstlich von Maaseik), für die durchgehende Schifffahrt nicht mehr schiffbar. Dort nimmt die Schifffahrt von Maastricht bis Maasbracht den parallel zur Maas 1925–1935 angelegten Julianakanal. Ebenfalls kürzt in den Niederlanden bei Roermond der 8,9 km lange Seitenkanal "Lateraalkanal Linne-Buggenum" zwischen Heel und Buggenum die (dort allerdings schiffbare) Maas ab.

Der Maas-Waal Kanal verbindet die Maas bei Nijmegen mit der Waal (Rhein).

Im Dezember 2016 wurde für die Dauer von 4 Wochen ein Teil der Maas für den Schiffsverkehr komplett gesperrt, nachdem das deutsche Tankschiff "Maria Valentine" der Reederei Gefo GmbH mit 2,5 Millionen Litern Benzin im Nebel das Stauwehr bei Grave durchbrochen und zerstört hatte. Dadurch verringerte sich der Wasserstand auf der Maas zwischen den Staustufen Sambeek und Grave sowie im Maas-Waal-Kanal bis zur Schleuse Nijmegen um drei Meter. Auf diesem Streckenabschnitt verkehren jährlich etwa 9000 Schiffe. Durch die Sperrung wurden größere Schiffe (Klasse III) über Antwerpen umgeleitet, kleinere Schiffe (Klasse II) über die Zuid-Willemsvaart. Im Januar 2017 gab das zuständige Ministerium bekannt, dass man bis zur endgültigen Reparatur der Anlage, die auf ein halbes Jahr veranschlagt wird, unterhalb des Stauwehres ein Provisorium aus einem Steinschuttwall errichten würde. Damit sollte durch Anheben des Wasserstands der Schiffsverkehr wieder möglich gemacht und Vorsorge für ein Ansteigen des Wasserspiegels bei Hochwasser getroffen werden, indem man binnen 48 Stunden dann den Damm kurzfristig wieder absenken könnte. Durch den Notdamm konnte der Wasserspiegel wieder gehoben werden, sodass am 23. Januar 2017 das erste Frachtschiff wieder die gesperrte Strecke zwischen den Schleusen Grave und Sambeck befahren durfte. Seit dem 24. Januar 2017 gibt es auf der Maas keine Beschränkungen für die Schifffahrt mehr. Die Reparatur der Staustufe konnte im Juli 2017 abgeschlossen werden. Rijkswaterstaat beziffert den Schaden auf min. 20 Mio. Euro.
Bei der Aufteilung des Fränkischen Reichs unter den Söhnen Ludwigs des Frommen wurde die Maas zur Grenze zwischen dem Westfränkischen Reich unter Karl dem Kahlen und dem Mittelreich unter Lothar I. Aus dessen Namen leitet sich der geografische Name Lothringen her.

Am Oberlauf der Maas liegt Domrémy-la-Pucelle, der Geburtsort von Jeanne d’Arc.

Die Geldrischen Gebiete westlich der Maas trat Preußen beim Wiener Kongress 1815 an das Königreich der Vereinigten Niederlande ab. Sie gehören seitdem zur Provinz Limburg. Noch heute bildet die sogenannte "Kanonenschusslinie" die Grenze zwischen der Bundesrepublik Deutschland und den Niederlanden. Der östlich der Maas gelegene Teil der damals neu geschaffenen und 1830 zwischen Belgien und den Niederlanden geteilten Provinz Limburg gehörte trotz niederländischer Hoheit als Herzogtum Limburg zum Deutschen Bund. Darauf bezieht sich die Zeile „"… von der Maas bis an die Memel …"“ der ersten Strophe des Deutschlandliedes von 1841, dessen dritte Strophe heute Nationalhymne ist.

Seit den 1820er Jahren wurde das wallonische Maastal zwischen Namur und Lüttich industrialisiert.

Die Maas und die Höhenzüge links und rechts der Maas beeinflussten einige militärische Aktionen im Ersten und Zweiten Weltkrieg. Z.B. war sie bei der Schlacht um Verdun ab Februar 1916 eine wichtige natürliche Barriere, die Auswirkungen auf den Frontverlauf hatte bzw. an einigen Abschnitten die Frontlinie war. Am 12. Mai 1940, dem dritten Tag des Westfeldzuges, erreichten Truppen der Wehrmacht die Maas bei Sedan; in den Tagen darauf Schlacht bei Sedan besiegten sie die dort befindlichen französischen Truppen. 

Am 2. Januar 1926 wurde mit 29,91 Meter über NAP der bisher höchste Wasserstand der Maas gemessen, der rund neun Meter über dem durchschnittlichen Wasserstand lag.

Im Dezember 1930 starben im Maastal zwischen Huy und Seraing etwa 60 Menschen bei der sogenannten "Maastal-Katastrophe", als infolge einer Inversionswetterlage die Abgase der Fabriken nicht aufsteigen konnten und es zu toxischen Konzentrationen in der bodennahen Luftschicht kam.



</doc>
<doc id="14675" url="https://de.wikipedia.org/wiki?curid=14675" title="Epistolografie">
Epistolografie

Epistolografie (auch Epistolographie) ist die literaturwissenschaftliche Bezeichnung für die literarische Gattung Brief, für Briefsammlungen und die wissenschaftliche Beschäftigung mit ihnen (Briefschreiblehre).

Briefsammlungen machen den privaten oder beruflichen Schriftverkehr einer Person oder einer Organisation der Öffentlichkeit zugänglich. Sie können auch in Buchform publiziert werden. Als solches dienen sie zur Abrundung von Biografien oder geschichtlichen Darstellungen. 





</doc>
<doc id="14676" url="https://de.wikipedia.org/wiki?curid=14676" title="Lautsprecher">
Lautsprecher

Lautsprecher sind Wandler, die ein (meist elektrisches) Eingangssignal in mechanische Schwingungen, als Schall wahrnehmbar, umwandeln.

Sie dienen meist der Schallerzeugung zur Wiedergabe von Sprache und Musik für den Menschen mit einem typischen Arbeitsbereich bei Frequenzen von 20 … 50 Hz bis hinauf zu 20 kHz. Es gibt allerdings auch andere Einsatzbereiche, wie Sonar unter Wasser, Ultraschall (für Tiere oder für Ultraschallreinigungsgeräte) oder Untersuchungen der Vibrationsempfindlichkeit von Geräten oder Gebäuden. In den 1970er Jahren gab es Ultraschallfernbedienungen. Auch einfache Entfernungsmessgeräte benutzen gelegentlich Lautsprecher.

Die Größe variiert zwischen sehr kleinen Formen, die beispielsweise bei In-Ear-Kopfhörern zum Einsatz kommen, bis hin zu mehrere Meter hohen Säulen für die Beschallung von Großkonzerten.

Die Verwendung des Begriffs Lautsprecher ist nicht ganz einheitlich:<br>
Er wird sowohl für das einzelne „Lautsprecher-Chassis“ bzw. „Lautsprecher-Treiber“ als auch für die Gesamtkonstruktion mit Frequenzweiche, Gehäuse und ggf. Verstärker, der Lautsprecherbox genutzt.
Solche Begriffsanwendungen sind jedoch ebenso ungenau wie die verbreitete Bezeichnung von Lautsprecherboxen als „Lautsprecher“: Das "Chassis" umfasst nämlich – je nach Definition – nur die unbeweglichen Teile eines Lautsprechers, während unter einem "Treiber" genau genommen der schallwandelnde Teil eines Hornlautsprechers zu verstehen ist; hier kann es leicht zu Verwechslungen kommen.

Die Entwicklung des Lautsprechers ist direkt mit der Erfindung des Telefons verknüpft und begann 1860 mit der ersten öffentlichen Vorführung eines Fernsprechapparates durch Antonio Meucci. Ein Jahr später präsentierte Philipp Reis sein Telefon, das später von Alexander Graham Bell weiterentwickelt wurde.

Nach dem von Thomas Alva Edison 1877 zum Patent angemeldeten Phonographen stellte Emil Berliner 1887 sein Grammophon vor. Dessen markanter Schalltrichter war jedoch insofern noch kein Lautsprecher im Sinne eines „Wandlers“, als er lediglich die mechanischen Schwingungen der Nadel durch ein akustisches Horn abstrahlt.

Werner von Siemens erhielt bereits 1878 das Patent für einen elektrodynamischen Lautsprecher; sein Pech war aber das Fehlen geeigneter Verstärker. Als Begründer der modernen Lautsprecher gilt Sir Oliver Lodge. Bei der Berliner Funkausstellung wurde 1925 der erste elektrodynamisch angetriebene Lautsprecher öffentlich vorgestellt. Im gleichen Jahr hatten Edward Kellog und Chester Rice von der amerikanischen Firma Western Electric den dynamischen Tauchspulenlautsprecher entwickelt, wie er im Prinzip bis heute in den meisten schallabstrahlenden Systemen zum Einsatz kommt.

Neben kontinuierlicher Optimierung der verwendeten Materialien ermöglichte es später vor allem die Erfassung der einzelnen physikalischen Größen, den klassischen Lautsprecher gezielt zu verbessern – speziell unter Berücksichtigung seiner Wechselwirkungen mit dem jeweiligen Gehäuse. Zugleich wurden neue Formen entwickelt, elektrische Impulse in Schall umzuwandeln, bis hin zur Schwingungsanregung von Luftplasma.

Ein Lautsprecher besteht in den meisten Fällen aus drei Komponentengruppen: der Membran, der Antriebseinheit sowie deren verbindenden Elementen. Beim Standardmodell des sogenannten Tauchspulenlautsprechers wird die Membran von einer mittig angebrachten Schwingspule in Bewegung versetzt; zum Antrieb gehört neben der Spule ein Magnet, in dessen Feld sie schwingt. Ein Korb verbindet den Magneten mit einer Sicke sowie einer Zentrierspinne, welche ihrerseits die Membran führen.

Abhängig vom Einsatzzweck bestimmen unter anderem die benötigte Lautstärke, der Frequenzumfang, das Platzangebot und die erwünschte Signaltreue, wie der Wandler jeweils konstruiert wird: Lautsprecher für Durchsagen am Flughafen müssen ganz anderen Anforderungen entsprechen als Player-Ohrhörer oder etwa Ultraschallreiniger.

Bei HiFi-Lautsprecherboxen werden meist mehrere frequenzspezifisch optimierte und über Lautsprecherweichen selektiv angesteuerte Wandler eingesetzt, um das gesamte Hörspektrum abzudecken. Lautsprecher lassen sich in ihrem unteren Arbeitsbereich durch ihre Thiele-Small-Parameter beschreiben, wodurch das komplexe Zusammenspiel ihrer Bauelemente rechnerisch fassbar wird.

Je nach Bauart und Frequenz bewegt sich die Membran kolben- oder wellenförmig. Während zum Beispiel Biegewellenwandler den Verformungseffekt nutzen, stellen Partialschwingungen bei den meisten Lautsprechern unerwünschte Störkomponenten dar.

Zur Erhöhung ihrer Steifigkeit kann man zentral angetriebene Membranen trichterförmig bauen (was den üblichen Konuslautsprechern ihr Aussehen verleiht) und ein möglichst stabiles Material wählen. Harte Werkstoffe wie Aluminium oder Keramik weisen jedoch ausgeprägte Eigenresonanzen auf, während Kunststoffe wie Polypropylen zwar über eine gute innere Dämpfung verfügen, aber bei Präzision und Wirkungsgrad schwächeln. Heute werden die unterschiedlichsten Mischungen und Schichtungen eingesetzt; Papier – schon seit Jahrzehnten für Lautsprechermembranen verwendet – liefert dabei nach wie vor beste Ergebnisse.

Zur Umgehung von Partialschwingungen kann man die Membran auch möglichst weitflächig antreiben. Diesem Prinzip folgen beispielsweise Magnetostaten, bei denen sich die Schwingspule – folienmäßig aufgetragen – über die gesamte Membranfläche verteilt; deren Material darf dann entsprechend dünn (sprich: leicht und somit impulsschnell) sein. In ihrer Auslenkung sind solche Flächen jedoch durch die umgebenden Magnetstäbe limitiert.

Je tiefer der Ton, also je niedriger die Frequenz, desto mehr Luft muss eine Membran für die gleiche Lautstärke verschieben. Dieses Volumen ergibt sich aus Gesamtfläche und Auslenkungsvermögen. Größere Membranen sind entsprechend schwerer und träger; außerdem nimmt die Schallbündelung zu. Größere Auslenkung wiederum bewirkt eine höhere mechanische Belastung und erfordert einen aufwendigeren Antrieb.

Die Parameter des Antriebs hängen sowohl von der Größe der Spule ab (Durchmesser und Wicklungshöhe) als auch von der Stärke und Reichweite des Magnetfeldes. Maßgeblich sind zudem der Spulen-Innenwiderstand (abhängig von der Leitfähigkeit ihres Materials), die Distanz zwischen Spule und Magnet sowie eventuelle Verluste durch Wirbelströme (abhängig von der – unerwünschten – Leitfähigkeit des Spulenträgers).

Durch den Einsatz neuer Materialien wie Neodym lassen sich heute wesentlich stärkere Magneten herstellen, als noch zu Beginn der Lautsprecherentwicklung. Von den offensichtlichen Vorteilen abgesehen, bewirkt ein kräftigerer Antrieb allein jedoch nicht zwangsläufig eine „Verbesserung“ des Lautsprechers; im Falle eines eingebauten Tieftöners verringert sich zwar das benötigte Gehäusevolumen, gleichzeitig steigt aber die untere Grenzfrequenz.

Bei Piezolautsprechern entfallen Spule und Magnet: Das Signal wird an eine Scheibe aus piezoelektrischem Material angelegt, die direkt mit der Membran verbunden ist. Elektrostaten wiederum arbeiten mit einem hochtransformierten Signal, das über Statoren auf eine unter konstanter Spannung stehende Folienmembran wirkt; beim Plasmalautsprecher wird ein ionisiertes Gas mit einem Hochspannungsfeld in Schwingungen versetzt.

Beim klassischen Konuslautsprecher ist der Magnet am hinteren Ende eines Korbes angebracht. Die vordere Korböffnung hält innen die Sicke – die flexible Aufhängung des Membranrandes – und dient darüber hinaus zum Einbau des Lautsprechers (etwa in eine Box). Weiter hinten, in der Nähe des Antriebs, befindet sich eine Zentrierspinne: Sie führt die Schwingspule, damit diese sich berührungsfrei im Luftspalt des Magneten bewegt. Von ihren mechanischen Funktionen abgesehen tragen diese Elemente auch wesentlich zum klanglichen Verhalten des Lautsprechers bei.

Billige Korbmaterialien wie Kunststoff oder Stahlblech sind weniger verwindungssteif als Druckguss, was unter anderem zu Resonanzen führen kann. Zu breite Stege behindern, ebenso wie fehlende Öffnungen zwischen Zentrierung und Magnet, die Membranbewegung, weil die Luft dort nicht frei zirkulieren kann. Ähnliches gilt für die Zentrierspinne: Je undurchlässiger sie ist, desto mehr bremst sie die Schwingungen.

Die Sicke beeinflusst nicht nur über ihre relative Nachgiebigkeit die Parameter des Lautsprechers, sondern trägt teilweise selbst zur Schallabstrahlung bei. Im Falle von Ringradiatoren etwa (einem Spezialfall der Kalottenhochtöner) sind quasi nur zwei konzentrische Sicken für die gesamte Tonwiedergabe zuständig.

Im unteren Arbeitsbereich um die Resonanzfrequenz lassen sich Lautsprecher, die als getriebenes Masse-Feder-System arbeiten, durch die Thiele-Small-Parameter beschreiben.

Ähnlich wie elektrischer Strom, der durch die elektrische Stromstärke und die elektrische Spannung beschrieben werden kann, gibt es bei Schallwellen die Schallschnelle und den Schall(wechsel)druck.
Schallschnelle beschreibt die Geschwindigkeit, mit der die Luftmoleküle durch den Schall bewegt werden, Schalldruck den dabei entstehenden Druck.
Schallabstrahlung einer Membran an die sie umgebende Luft ist auf Grund der geringen Schallimpedanz von Luft eine sehr ineffiziente Sache, die bei tiefen Frequenzen weiter auf 0 abfällt, bei höheren Frequenzen steigt sie mit einigen Überschwingern bis auf einen von Form und Größe der Membran vorgegebenen Grenzwert an. Beim klassischen Problem des Kolbenstrahlers in unendlicher Schallwand im allseits unendlich großen Raum ist der Strahlungswiderstand bis zum Grenzwert proportional zur Frequenz. Dies sollte ein stark höhenbetontes Klangbild zur Folge haben. Es ist bei fast allen Strahlern aber so, dass die aufgebrachte mechanische Kraft für höhere Frequenzen konstant ist und damit nach Newton auch die Beschleunigung. Dies bedingt, dass die Schnelle umgekehrt proportional zur Frequenz ist. Dies kompensiert genau das Ansteigen des Strahlungswiderstandes. Bei den meisten Strahlern gibt es also ohne weiteres Zutun einen Bereich, in dem die abgestrahlte Leistung unabhängig von der Frequenz ist, und dieser wird folglich zum Hauptarbeitsbereich gewählt.

Das Beispiel mit der unendlichen Schallwand zeigt, dass eine analytisch mathematische Behandlung nur in einfachen Modellfällen möglich ist. Mehrere Membranen oder Resonatoren interagieren miteinander und mit den Strukturelementen von Räumen. Dies verändert unter anderem den Strahlungswiderstand. Man kann sich den Strahlungswiderstand als zusammengefasste Rückwirkung des Strahlungsfeldes auf den Strahler vorstellen.

Bei Direktstrahlern, die nur aus Membranen bestehen, sind die Kräfte, mit denen das Strahlungsfeld auf die Membran zurückwirkt, gegenüber der Antriebskraft, der Massenträgheit und den elastischen Federkräften vernachlässigbar. Die Bewegung der Membran ist also praktisch unabhängig vom barometrischen Gleich-Luftdruck, bis hin zum Vakuum. Elektrische und mechanische Messungen an Chassis sind somit im Freifeld vergleichbar mit solchen im Hallraum. Die Berechnung der Zusammenschaltung mehrerer Membranen zu einem Feld kann daher durch einfache, rückwirkungsfreie Überlagerung der Einzelcharakteristiken erfolgen. In diesem Fall spricht man von einer Fehlanpassung mit entsprechend geringem Wirkungsgrad.

Der Einsatz von akustischen Resonatoren oder Impedanztransformatoren, beispielsweise Hörnern, ändert schmal- oder breitbandig sehr drastisch die Ankopplung der Membran an das Strahlungsfeld. Die Kräfte des Strahlungsfeldes auf die Membran sind nicht länger vernachlässigbar. Die Zusammenschaltung ist in diesem Fall nicht rückwirkungsfrei und auch die anderen Vereinfachungen gelten nicht mehr. In diesem Fall spricht man von einer Leistungsanpassung mit gutem Wirkungsgrad.

Dieses Prinzip ist nach wie vor die am weitesten verbreitete Konstruktionsart. Die Bezeichnung rührt daher, dass dabei eine Spule im Feld eines umgebenden Magneten schwingt, also quasi in diesen „eintaucht“. Je nach Einsatzzweck können solche Lautsprecher aber vollkommen unterschiedlich aussehen. Im Home-HiFi-Bereich erstreckt sich die Bandbreite der Wandler von 12″-Tieftönern mit gut 30 Zentimetern Membrandurchmesser bis hin zu 3/4″-Hochtönern mit knapp 2 cm Membrandurchmesser.

Technisch gesehen zählen sie zu den dynamischen Lautsprechern, werden also elektrodynamisch angetrieben. Die Bewegung wird von einer mittig angebrachten Spule ausgelöst; sie ist auf einen zylindrischen Träger gewickelt, der wiederum an der Membran befestigt ist. Leitet man ein elektrisches Signal durch die Spule, wird durch die Lorentzkraft (Wechselwirkung mit dem Feld des umgebenden Magneten) eine Kraft auf die Membran ausgeübt, die diese zum Schwingen veranlasst. Spule und Membran bewegen sich im Magnetfeld senkrecht zum Feldverlauf hin und her. Eine Zentrierspinne und die Sicke sorgen für die Rückführung der Membran in ihre Ruhelage sowie für die Zentrierung der Schwingspule.

Membran, Schwingspule, Sicke und Zentrierspinne sind die beweglichen Teile, während Magnet und Lautsprecherkorb fest stehen. Letzterer verbindet konstruktionsmäßig den Magneten mit Spinne/Sicke und dient zudem mit seinem Außenrand zur Montage des Lautsprechers. Die Sicke ist – als auch teilweise schallabstrahlendes Element – luftdicht; Zentrierspinne und Korb sollen hingegen dem rückwärtigen Luftaustausch möglichst wenig Widerstand entgegensetzen.

Je tiefer der wiederzugebende Ton, desto mehr Luftvolumen muss für gleiche Lautstärke „verschoben“ werden. Tieftonlautsprecher verfügen daher meist über große Membranen und/oder weite Auslenkung; hier wird eine (aus Stabilitätsgründen konusförmige) Membran von einer mittigen Schwingspuleneinheit angetrieben. Hochtonlautsprecher wiederum müssen rascheren Impulsen folgen; kleine Membranen wirken außerdem der zunehmenden Schallbündelung bei höheren Frequenzen entgegen. Daher haben Hochtöner meist eine kalottenförmigen Membran, bei der die Schwingspule am Außenrand ansetzt.

Da die Membran grundsätzlich als akustischer Dipol arbeitet – den Schall also ebenso nach vorne wie nach hinten abstrahlt –, kann jedoch (abhängig von ihrem Durchmesser in Relation zur wiedergegebenen Frequenz) ein akustischer Kurzschluss entstehen. Um die wechselseitige Auslöschung der front- und rückseitig abgegebenen Schallanteile zu vermeiden, müssen solche Wandler daher meist in Gehäuse eingebaut werden: In der Praxis betrifft das mehr oder weniger alle Lautsprecher dieser Bauart, die nicht ausschließlich hohe Frequenzen wiedergeben sollen.

Zur möglichst unverfälschten Wiedergabe des Originalsignals ist unter anderem ein annähernd geradliniger Frequenzgang erforderlich. Nun bilden aber die vielen Komponenten (einschließlich der Luft) ein hochkomplexes Masse-Feder-System: Gewicht und Steifigkeit der Membran sind dafür ebenso ausschlaggebend wie die Nachgiebigkeit von Sicke/Spinne, und auch die Werte des Antriebs (Felder von Spule und Magnet) beeinflussen maßgeblich das Verhalten des Lautsprechers.

Erst seit Albert Thiele und Richard Small die nach ihnen benannten Thiele-Small-Parameter festlegten, ist es möglich, die Eigenschaften von Lautsprechern bereits in der Entwurfsphase vorauszuberechnen – nicht zuletzt, was den Einfluss des jeweils gewählten Gehäuses betrifft.

Magnetostatische Lautsprecher sind entgegen ihrem Namen elektrodynamische Lautsprecher. Sie funktionieren nach dem gleichen Grundprinzip der Tauchspulen-Lautsprecher. Sie sind nicht mit elektrostatischen Lautsprechern zu verwechseln, die ein anderes Antriebsprinzip nutzen und auch ganz anders angesteuert werden (müssen).

Hier wird die Schwingspule nicht auf einem separaten Träger montiert, sondern direkt auf die Membran aufgebracht (Folien-Magnetostaten) oder gleich ganz weggelassen: Beim klassischen „Bändchen“ wirkt das elektrische Signal auf die Membran selbst. Der großflächige Antrieb und das eingesparte Gewicht – die Membran braucht keinerlei Steifigkeit und kann daher hauchdünn ausfallen – sorgen für beste Impulstreue und Detailauflösung.

Allerdings muss sich der Schall seinen Weg zwischen den umgebenden Magneten (hier: Magnetstäben) bahnen. Deren Feldstärke limitiert wiederum die Auslenkung der Membran, und mit zunehmender Fläche – wenn sie auch tiefere Frequenzen wiedergeben soll – stellen sich, wie bei allen Membranen, Bündelungseffekte ein.

Magnetostatische Lautsprecher werden heute hauptsächlich im Hoch- und Mitteltonbereich eingesetzt.

Als Membranmaterial findet hier meist Aluminium Anwendung (etwa 10 µm, also etwa Alufolie). Die Folie wird vertikal vom Signal durchflossen und befindet sich im Statorfeld von Permanentmagneten, deren Feldlinien horizontal verlaufen; die resultierende Lorentzkraft bewegt die Membran vor und zurück und führt zur Schallabstrahlung – ähnlich wie bei allen dynamischen Wandlern.

Als technische Hürden erweisen sich dabei jedoch einerseits die extreme Empfindlichkeit des Materials (irreversible Überdehnung bei zu hoher Lautstärke) und andererseits die geringe Impedanz: Der minimale Innenwiderstand der Folie würde jeden normalen Verstärker durchbrennen oder abschalten lassen, weshalb diese Lautsprecher mit einem zusätzlichen Übertrager ausgestattet werden müssen.

Bändchen kommen de facto nur als Hochtöner zum Einsatz.

Als Membran fungiert hier eine Kunststofffolie, auf die eine Leiterbahn (meist aus Aluminium) aufgebracht wird, welche ihrerseits die Schwingspule darstellt.

Ein Vorteil gegenüber dem klassischen Bändchen besteht darin, dass sich die Impedanz in verstärkerfreundlichen Regionen bewegt (4–8 Ohm), weshalb solche Lautsprecher ohne Übertrager direkt angeschlossen werden können. Als Membranmaterial stehen diverse zähe – also belastbare – Kunststoffe zur Auswahl. Folienmagnetostaten kann man daher deutlich größer bauen, wodurch sich ihr Einsatzbereich Richtung tieferer Frequenzen erweitert.

Dem Bündelungseffekt großflächiger Membranen wird oft durch einen gebogenen Aufbau des ganzen Lautsprechers begegnet. Für eine ernsthafte Basswiedergabe reicht es jedoch trotz allem nicht. Die meisten Magnetostat-Boxen enthalten daher einen zusätzlichen Tauchspulenwandler für die Tieftonreproduktion.

Der von Oskar Heil entwickelte Lautsprecher löst mehrere Probleme: 

Auf diese Art kann mit vergleichsweise geringer Membranbewegung ein Vielfaches an Schalldruck erzeugt und eine Kennimpedanz von 4 bis 8 Ohm erreicht werden, was den sonst notwendigen Transformator überflüssig macht. Andererseits müssen die umgebenden Magnetstabgruppen weiter auseinander liegen als bei anderen Magnetostaten, weil die gefaltete Membran mehr Platz braucht – und durch die wesentlich größere Gesamtfläche wird sie auch schwerer. Air-Motion-Transformer sind daher bis heute nur als Hochtöner im Handel.

Elektrostaten nutzen die Coulomb-Kraft statt der Lorentzkraft als Antrieb. Es wird eine hohe Ansteuerspannung statt eines großen Ansteuerstroms benötigt. Weiterhin muss diese an sich hochnichtlineare Kraft (k = 100 %)
durch Nutzung einer Vorspannung und des Gegentaktprinzips linearisiert werden. Konstruktiv wird diese Form des Antriebs so gut wie immer mit Flächenlautsprechern kombiniert. Die Spannung wird nicht einer Elektrode zugeführt, sondern liegt immer zwischen Elektroden an. Verschiedene Formen der Ansteuerung sind möglich.

Hier wird das Prinzip des Magnetostaten quasi umgekehrt. Das Signal liegt nicht an der Membran, sondern den umgebenden Elementen an: zwei Elektrodengittern (auch Statoren genannt), die im Gegentakt arbeiten.

Elektrostaten (kurz: „ESL“) nutzen die elektrostatische Anziehungskraft. Die straff montierte Membranfolie wird unter eine hohe, konstante Spannung gesetzt (zwischen 1.000 und 5.000 Volt).

Was Probleme bezüglich der Auslenkung oder der Schallbündelung bei höheren Frequenzen betrifft, gleichen Elektrostaten ihren magnetostatischen Pendants hier ebenso wie in deren klanglichen Vorzügen. Allerdings muss ein deutlich höherer technischer Aufwand betrieben werden − und ohne separate Stromversorgung aus der Steckdose funktionieren sie nicht.

Diese Bauart wurde zu einer Zeit entwickelt, als man noch keine ausreichend starken Permanentmagneten herstellen konnte, wie sie für großflächige Magnetostaten notwendig sind. Lautsprecher wie der legendäre Quad-Elektrostat (1957) waren die ersten Wandler mit Folienmembran, welche annähernd das gesamte menschliche Hörspektrum abdeckten.

Piezo-Lautsprecher nutzen den piezoelektrischen Effekt. Ein Piezokristall ändert seine Dicke proportional zur angelegten Spannung. Piezoelemente arbeiten somit bereits als direkt schallabstrahlende Wandler.

Wegen der vergleichsweise geringen Lautstärke finden sie sich als alleinige Schallgeber jedoch nur in Kleingeräten, etwa als Summer. Wo mehr Pegel gefordert ist, wird das Element mit einer Konusmembran versehen, die ihrerseits in ein Horn strahlt.

Im HiFi-Bereich werden Piezolautsprecher so gut wie gar nicht eingesetzt. An ihrem Impedanzverlauf scheitern übliche Frequenzweichen, und die heftigen Resonanzen der Wandler (meist im Bereich von 1–5 kHz) ermöglichen dann keine verzerrungsfreie Wiedergabe. Tatsächlich sind diese Probleme jedoch leicht zu umgehen: Nach Parallelschaltung eines passenden Widerstandes lässt sich ihr Frequenzspektrum wie gewohnt filtern.

Neben dem Ultraschall-Einsatz zur Vertreibung von Insekten oder als Marderabwehr finden Piezolautsprecher daher auch als HiFi-Wandler Anwendung; oberhalb ihrer Resonanzfrequenz klingen sie ebenso sauber wie hochwertige dynamische Hochtöner.

Das Konstruktionsprinzip stammt aus der Frühzeit der Audiotechnik. Es wird entweder eine Eisenmembran bewegt, die den Schall direkt abstrahlt (siehe Bild), oder ein von einer Spule umschlossener Eisenstab schwingt vor dem Luftspalt eines Dauermagneten und ist mit einer Papiermembran verbunden.

Solche Wandler (auch funktionsgleiche, magnetische Mikrofone) wurden beispielsweise in der militärischen Kommunikation eingesetzt; in Morse-Kopfhörern und Telefonhörern fanden sie ebenfalls Verwendung.

Wegen ungenügender Wiedergabequalität (blechener Klang, hoher Klirrfaktor) werden elektromagnetische Lautsprecher seit den 1930er Jahren kaum mehr verwendet. Man fand sie in den 1980er Jahren noch in Kinderspielzeug.

Sie sind die einzigen Lautsprecher, die elektrische Signale ohne Umweg über Festkörper wie Schwingspulen oder Membranen direkt in Schallwellen übertragen. Sie erzeugen zwischen zwei Hochspannungselektroden ein Luftplasma, dessen Feld im Signaltakt amplitudenmoduliert schwingt, und nutzen so die Eigenschaft der Luft, sich bei Erwärmung auszudehnen und bei Abkühlung wieder zusammenzuziehen.

Das Prinzip ist theoretisch nahezu perfekt: Plasmalautsprecher arbeiten praktisch verzögerungsfrei, kennen keine mechanischen Beeinträchtigungen wie Trägheit, Materialresonanzen oder Vor-/Nachschwinger und liefern einen linearen Frequenzgang bis weit über den Hörbereich hinaus.

Einschränkungen der idealen kugelförmigen Abstrahlung ergeben sich zwangsläufig durch jene Bauteile, die das Plasma erzeugen und quasi „im Weg“ stehen.

In der Praxis gab es anfangs Probleme mit der Geruchsbelästigung (wegen der Bildung von Ozon), was jedoch gelöst werden konnte. Der Hauptgrund, warum Plasmalautsprecher heute kaum mehr eine Rolle spielen, sind DIN-Normen, welche unter anderem die elektromagnetische Verträglichkeit betreffen: Die Regeln schreiben eine Abschirmung vor – etwa durch umgebende Metallgitter –, wodurch viele akustische Vorteile dieser Wandler ad absurdum geführt werden.

Plasmalautsprecher sind bis heute nur als Hochtöner im Gebrauch.

Hornlautsprecher können auf einem beliebigen Wandlerprinzip beruhen. Hornlautsprecher strahlen Schall nicht direkt, sondern über ein vorgeschaltetes Horn ab. Dies erhöht den Wirkungsgrad, indem es den Übergang des Schalls aus einem dichten Medium (Membran) in das dünne Medium (Luft) kontinuierlicher gestaltet.

Horntreiber können zusätzlich mit einer Druckkammer kombiniert werden, diese stellt eine Verengung der Schallführung vor dem eigentlichen Horn dar. Druckkammern steigern den Wirkungsgrad weiter, erhöhen allerdings den Klirrfaktor. Die animierte Grafik stellt einen Horntreiber mit einer zusätzlichen Druckkammer dar.

Horntreiber unterscheiden sich insofern von anderen dynamischen Lautsprechern, als sie für den Betrieb mit einem frontseitig anzubringenden Horn optimiert werden. Sie haben daher unter anderem keinen Montagering zur Fixierung in einer Schallwand, sondern einen (genormten) Anschlussflansch. Ihr Korb besteht aus einer weitgehend geschlossenen Hülle, die sich vor der Membran verjüngt.

Der obligate Hornvorsatz sorgt für eine deutliche Erhöhung des Wirkungsgrades, beeinflusst jedoch auch den Frequenzgang sowie das Abstrahlverhalten. 

Im PA-Bereich werden Schallführungen und Hornvorsätze zum Erhöhen des Wirkungsgrades häufig eingesetzt. In der HiFi-Praxis werden Horntreiber nur für die Schallreproduktion vom Mitteltonbereich aufwärts angeboten.

Während die Membran bei Konuslautsprechern möglichst steif sein soll, um eine kolbenförmige Bewegung zu gewährleisten, nutzen Biegewellenwandler gerade die Verformbarkeit: Die Wellen breiten sich auf der Membran, konzentrisch vom Ansatz der Schwingspule ausgehend, wie auf einer Wasseroberfläche aus.

Dafür muss zum Beispiel die Sicke – genauer gesagt: die Aufhängung der Membran am Außenrand – anders gebaut werden; der Rand schließt mit einem Wellenwiderstand ab, damit Reflexionen vermieden werden. Die Unterdrückung unerwünschter Partialschwingungen ist neben der geringen Schallausbeute eines der größten Probleme solcher Lautsprecher. Andererseits glänzen sie mit homogener Wiedergabe und breitem Abstrahlverhalten.

Neben den bekannten Flachmembranen, die Josef Wilhelm Manger entwickelte, arbeiten heute auch andere Lautsprecherkonstruktionen nach diesem Prinzip (siehe unten, Abschnitt „Rundumstrahler“).

Von Grenzfällen wie Soundboards abgesehen (hier werden Teile einer Zimmerwand durch Exciter – siehe unten, Abschnitt „Sonstige Varianten“ – zum Schwingen gebracht), sind solche Biegewellenlautsprecher ziemlich teuer und werden daher hauptsächlich im hochpreisigen HiFi-Segment angeboten.

Um der Schallbündelung entgegenzuwirken, strahlen solche Konstruktionen zumindest horizontal (möglichst) omnidirektional ab. In den meisten Fällen wird das jedoch über Gehäuseelemente realisiert, zum Beispiel mit Hilfe von Dispersionskegeln, die man vor konventionelle Wandler montiert. Von sich aus omnidirektionale Lautsprecher sind sehr selten.

Bislang kommen nur Plasmalautsprecher (siehe unten) dem theoretischen Ideal einer kugelförmigen Abstrahlung nahe. Eine immerhin kreisförmige Abstrahlung bieten andere spezielle Lautsprecher, etwa von German Physiks oder MBL. Erstere nutzen die „Rückseite“ einer langgestreckten Konusmembran, die nach dem Biegewellenprinzip arbeitet; bei Letzteren wird ein Lamellenring im Takt des Signals gestaucht.

Ihr Vorteil bei der HiFi-Wiedergabe liegt im gleichmäßigen Abstrahlverhalten, das den Hörer nicht auf einen Punkt im Stereodreieck festlegt. Andererseits werden raumakustische Effekte verstärkt, was die Abbildungspräzision beeinträchtigt: Die von den Zimmerwänden reflektierten, laufzeitdifferenten Schallanteile überlagern sich mit den bereits in der Aufzeichnung enthaltenen Rauminformationen.

Rundumstrahlende Lautsprecher werden nur zur Wiedergabe des Mittel- und Hochtonbereiches eingesetzt, da in der Praxis auch konventionelle Wandler niedrigere Frequenzen bereits annähernd kugelförmig abstrahlen.

Subwoofer stellen für Bass und Tiefbass spezialisierte Lautsprecherboxen dar, die zusammen mit Satellitenboxen erst das vollständige Spektrum wiedergeben.

Exciter stellen als membranlose Schwingungsanreger eine Sonderform des Lautsprechers dar. Sie werden wie normale HiFi-Wandler von entsprechenden Verstärkern angetrieben, benötigen jedoch ein festes Medium als „Membran“ – das heißt, sie müssen erst an einem Objekt fixiert werden, das sie in Schwingung versetzen. In der Praxis können sie zum Beispiel hinter Wandpaneelen montiert werden, wodurch Teile der Zimmerwand dann als „unsichtbare Lautsprecher“ agieren. Solche Kombinationen arbeiten im Prinzip als Biegewellenwandler. Andere Typen werden – zur Ergänzung des Klangbildes, statt eines Subwoofers – an Sitzmöbel geschraubt, wo sie Körperschall erzeugen und so durch tieffrequente Vibrationen das subjektive Bass-Empfinden des Zuhörers verstärken.

Ultraschallwandler werden unter anderem zur Tierabwehr eingesetzt (siehe weiter oben, Abschnitt „Piezolautsprecher“) oder zu Messzwecken (nach dem Laufzeitprinzip, siehe Echolot und Sonar), ferner zur Reinigung, Materialbearbeitung und in der Medizin (siehe Sonografie). Unter Ausnutzung von subharmonischen Schwingungen kann Ultraschall auch zur Wiedergabe hörbarer Frequenzen Verwendung finden, dieses Prinzip ist jedoch kaum praktikabel.

Sogenannte "Parabollautsprecher" hingegen sind keine eigenständigen Wandler, sondern nutzen nur die Bündelung durch mechanische Reflektoren. Beispiele dafür sind „Soundduschen“ (im Ausstellungsbereich für lokal begrenzte Audio-Information eingesetzt) oder – im militärischen Kontext – Schallkanonen.

Die Bezeichnung "Flachlautsprecher" wiederum wird in so vielen unterschiedlichen Zusammenhängen gebraucht, dass sie praktisch nichts aussagt. Es können damit ebenso dynamische Wandler mit gerader statt konusförmiger Membran gemeint sein wie per Exciter betriebene Wandelemente (siehe oben) oder Lautsprecher, die statt in Boxen direkt in eine Zimmerwand montiert werden (wodurch ihre Wiedergabecharakteristik annähernd dem Einbau in eine unendliche Schallwand entspricht).

Dynamische Lautsprecher (das sind zum einen Tauchspulen-Lautsprecher wie auch sogenannte Magnetostaten) nutzen die Lorentzkraft als Kraft zwischen einem Stator-Magneten und einem stromdurchflossenen Leiter (als Spule oder als auf die Membran aufgebrachte Leiterbahnen ausgeführt) aus.

Die Lorentzkraft als Antriebskraft beträgt
Damit formula_2 ist, muss sich immer die gleiche Spulenlänge im Magnetfeld befinden. Erreicht werden kann dies mittels folgender drei Anordnungen:

Lautsprecher mit Überhangspule:<br>
Die Spule ist länger als die Polplattenhöhe. Bis zu einer gewissen Grenzauslenkung wird nur ein (gewisser) Teil der Antriebsspule genutzt.
Anwendung bei fast allen Lautsprechern, die erhebliche Auslenkungen durchführen müssen.
Der Fluss des Statormagneten wird komplett genutzt, die der Spule nur teilweise, da sich Teile außerhalb deren Magnetfeld befinden.

Lautsprecher mit Unterhangspule:<br>
Die Spule ist kürzer als die Polplattenhöhe. Bis zu einer gewissen Grenzauslenkung befindet sich die Antriebsspule immer komplett zwischen den Polplatten.
Anwendung bei fast allen Lautsprechern, die nur geringe Auslenkungen durchführen müssen.
Der Fluss des Spule wird komplett genutzt, die des Statormagneten nur teilweise, da Teile nicht von einer Spule ausgefüllt sind.

Hybride Lösung:<br>
Bis zur halben Auslenkung der Schwingspulenhöhe befindet sich immer die halbe Schwingspule zwischen den beiden Polplatten.
Der Antrieb ist aufwändig, weist aber eine große Symmetrie auf, was ungeradzahlige Harmonische reduziert. So weisen Antriebskraft und Induktivität eine von Antriebsstrom und Auslenkung unabhängigeren Wert auf.
Der Antrieb kann durch zwei Zentrierspinnen (nicht eingezeichnet) gehalten werden.
Der Fluss des Statormagneten wie der Spule wird je zur Hälfte genutzt. Der Fluss des Statormagneten wird allerdings zweimal genutzt, muss allerdings auch zwei Luftspalte überwinden.

Die geometrischen Flächen heutiger dynamischer Lautsprechermembranen sind zur Vermeidung von Knickschwingungen in sämtlichen Richtungen gekrümmt (sog. nicht abwickelbare Flächen):

Kalottenlautsprecher:<br>
Membran und Antriebsspule haben (meist) den gleichen Durchmesser. Eingesetzt wird dieses Prinzip im Wesentlichen bei Hochtönern, manchmal auch bei Mitteltönern. Übliche Größen sind 19 mm bis 28 mm für Hochtöner, 50 mm bis 76 mm für Mitteltöner. Die Kalotte ist meist konvex (erhabener Dome), manchmal aber auch konkav (Inverskalotte). Arbeitsbereiche beginnen bei 19-mm-Hochtönern bei ca. 3 kHz, bei 76-mm-Mitteltönern bei 450 Hz (Werte sind Richtwerte).

Konuslautsprecher:<br>
Die Membran hat einen wesentlich größeren Durchmesser als die Antriebsspule und ist konkav. Eingesetzt wird dieses Prinzip im Wesentlichen bei Tief- und Mitteltönern. Übliche Größen fangen bei 10 cm an und enden bei 45 cm. Tieftöner und Subwoofer haben eher größere Durchmesser, Mitteltöner, aber auch Tieftöner für kleinere Boxen, sind eher kleiner.

Flachmembranen:<br>
Eine weitere Möglichkeit sind Flachmembranen. Diese werden entweder vollflächig angetrieben (z. B. bei Flächenlautsprechern) oder man nutzt durch geschickte Konstruktion gedämpfte Biegeschwingungen zur Schallabstrahlung aus.

Lautsprechenboxen bestehen aus einem und mehreren Chassis mit Weiche, einem geeigneten Gehäuse und ggf. Verstärkern.
Folgende Grundprinzipien sind möglich:

Lautsprecherbox#Einteilung nach Anzahl der Wege

Lautsprecherbox#Speisung

Rein akustisch bedingte Wiedergabefehler sind entgegen landläufiger Meinung sehr wohl messbar, und deren Auswirkungen auf das Hörerlebnis sind, soweit es nicht die Aufnahme des Schallereignisses durch das menschliche Ohr betrifft, abschätzbar.

Eine Grundvoraussetzung für gute Audiowiedergabe ist, dass die Lautsprechersysteme elektrisch korrekt an einen geeigneten Audioverstärker mit möglichst geringer Ausgangsimpedanz angeschlossen sind. Die Quelle, etwa der CD-Spieler oder Schallplattenspieler, der Audioverstärker und der Lautsprecher sowie dessen akustische Anpassung an das Boxengehäuse und an die freie Schallausbreitung haben unterschiedliche Einflüsse auf die Wiedergabequalität. Diese Thematik wird kontrovers diskutiert.

Zu einem Hörerlebnis gehören neben der Aufnahme durch das Ohr auch sensorische Wahrnehmungen der Erschütterungen des Körpers über den Boden oder den tieffrequenten Schall. Sie können nur mit Vollkörpersimulationen erfasst werden. Zudem fließen in großem Maße individuelle Hörgewohnheiten, Vorlieben, die aktuelle Befindlichkeit des Hörenden und schließlich dessen Gehörzustand in die Beurteilung des Hörerlebnisses mit ein.

Lautsprecherboxen interagieren zudem vielfältig mit dem Abhörraum, daher spielt die Raumakustik in Kombination mit dem Lautsprechersystem eine wesentliche Rolle für das Abhörergebnis.

Lineare Wiedergabefehler sind pegelunabhängige Fehler. Sie treten bei allen Schallpegeln auf. Weiterhin entstehen keine im Original nicht vorhandenen Frequenzen. Dieser letzte Punkt ist entscheidend für die Unterscheidung von linearen und nichtlinearen Fehlern. Mathematisch lässt sich durch Additionstheoreme zeigen, dass nur im Falle nichtlinearer Fehler neue Frequenzen im Spektrum entstehen.

Lineare Verzerrungen sind etwa Nichtlinearitäten im Amplitudenfrequenzgang, d. h., unterschiedliche Frequenzen werden trotz identischen Eingangssignalpegels vom Lautsprecher unterschiedlich laut wiedergegeben. Je nach Art und Ausprägung dieser Nichtlinearitäten führen diese bei der Wiedergabe zu Klangverfärbungen (zu laute Bässe, zu wenig Mitten usw.). Im Idealfall sollte ein Lautsprecher alle Frequenzen im Hörbereich (20–20000 Hz) gleich laut wiedergeben. In der Praxis sind Abweichungen bis ± 0,5 dB für das menschliche Ohr nicht unterscheidbar, Abweichungen bis etwa ± 2 dB, sofern sie nur schmalbandig sind, gelten hörtechnisch als nicht störend. Je breitbandiger diese Verfärbungen sind, desto eher sind sie hörbar und störend. Anhebungen einzelner Frequenzbänder sind besser hörbar und störender als Absenkungen.

Linearer Frequenzgang wird mit Mehrwege-Lautsprecherboxen oder entsprechend breitbandigen Wandlern erreicht. Hörraum und Boxengeometrie sowie die Lautsprecherdämpfung durch den Verstärker und die Dämmung der Box haben neben dem Lautsprecher großen Einfluss auf den Frequenzgang. Abweichungen der Frequenzgänge "(Paarabweichungen)" der beteiligten Lautsprecher untereinander führen zu Lokalisationsunschärfen und zu Klangänderungen von bewegten Quellen. Letzteres ist besonders bei Videowiedergabe störend. Das ergibt vor allem bei sogenannten Center-Lautsprechern Probleme, weil diese meist anders konstruiert und aufgestellt sind als die zugehörigen Frontlautsprecher.

Die Empfindlichkeit ist unterschiedlich:

Abweichungen im Bereich 250 Hz bis 2 kHz sind ab 0,5 dB feststellbar, maximale Unterschiede von 0,25 dB sind daher anzustreben, jedoch kaum zu erreichen.

Neben den Verfärbungen auf der idealen Abstrahlachse des Lautsprechers (Hörachse) ist für den Höreindruck jedoch auch entscheidend, wie der Schall abseits dieser Achse abgegeben wird, weil sich nicht immer alle Hörer in der Hörachse befinden können. Idealerweise sollte ein Lautsprecher in jede Raumrichtung alle Frequenzen identisch laut wiedergeben, wobei nur der Gesamtpegel abweichen darf (gleichmäßige Schallbündelung). In der Praxis ist diese Bündelung aber insbesondere im Mittel- und Hochtonbereich oft stark abhängig von der Frequenz, was im Heimbereich durch Verstetigung des Abstrahlverhaltens („Constant Directivity“) vermieden werden sollte. Hier sind Kalottenhochtöner vorteilhaft, denn diese besitzen bei hohen Frequenzen eine wesentlich bessere Rundum-Abstrahlung als Membran- oder Trichter- bzw. Hornlautsprecher.

Im Außenbereich ist man dagegen oft daran interessiert, hohe Frequenzen gerichtet in einem schmalen Raumwinkel abzustrahlen, um deren größere Luftdämpfung bei größeren Entfernungen auszugleichen. Während nahestehende Hörer dann außerhalb des Hauptabstrahlkegels der Hochtonlautsprecher (z. B. Hornlautsprecher) sind, werden entfernt stehende Hörer vom Hauptkegel erreicht und nehmen hohe Frequenzen ausreichend laut wahr. Eine Alternative sind im hinteren Zuhörerraum aufgestellte, gerichtet auf die hinteren Zuhörer abstrahlende zusätzliche Hochton-Lautsprecher. Diese müssen jedoch zeitverzögert angesteuert werden.

Reflexionen bringen im Hallraum sehr große Pegelschwankungen mit sich, die durchaus im Bereich +10 dB … −40 dB liegen können. Besonders bei höheren Frequenzen ergeben sich durch die Überlagerung von Direktschall und mehrfachen Reflexionen äußerst komplizierte räumliche Schallfelder. Bei Wiedergabe eines Sinustons können diese Pegelunterschiede beim Umhergehen deutlich wahrgenommen werden.

Ein Problem sind Interferenzen zwischen den verschiedenen Schallwegen von Mehrweg-Lautsprecherboxen im Bereich der Trennfrequenzen oder mehreren Boxen, die gleiche Frequenzen wiedergeben. Dadurch kommt es zu ortsabhängigen Verstärkungen und Auslöschungen von Frequenzen durch konstruktive und destruktive Interferenz, was letztendlich zu ortsabhängigen Frequenzgangfehlern führt. Man sollte dabei aber beachten, dass es im Hallraum stets zu solchen Erscheinungen kommt, auch wenn nur ein Lautsprecher betrieben wird.

Das menschliche Gehör ist für Phasendrehungen, wie z. B. durch ein Allpassfilter hervorgerufen, recht unempfindlich. Es gibt jedoch Fälle, bei denen Phasenunterschiede wahrnehmbar sind, beispielsweise in Situationen, bei denen zwei Töne in die kritische Bandbreite fallen. In diesem Fall können die Sinneszellen des Innenohrs mit ihrer Einweggleichrichterwirkung Unterschiede feststellen. Weit bedeutender als die Phasendrehungen sind jedoch die daraus resultierenden unterschiedlichen Gruppenlaufzeiten. In extremen Fällen werden dadurch Impulse in einzelne Wellikel zerlegt, aus einem Konsonanten wie „t“ wird dann so etwas wie „huii“. Das zeitliche Auflösungsvermögen des Gehörs bezüglich des Eintreffens unterschiedlicher Reize bei verschiedenen Frequenzgruppen ist jedoch sehr beschränkt. Gruppenlaufzeitunterschiede bis zu einigen ms sind daher nicht wahrnehmbar. Das bedeutet, dass mehrere Lautsprecher in einer Box eher weniger, der Abhörraum oder mehrere unterschiedlich entfernt stehende Boxen dagegen entscheidend zu den Verfälschungen beitragen.

Als Impulstreue wird das Vermögen eines Lautsprechers bezeichnet, bei einem impulsförmigen Signal dessen Zeitverlauf mit möglichst wenigen Ein- und Ausschwingvorgängen zu folgen. Dabei handelt es sich im Wesentlichen um tiefe und mittlere Frequenzen, die entstehen, wenn resonante Komponenten (Partialschwingungen auf der Membran, hart aufgehängte Membran insgesamt, Hohlraumresonanzen in der Lautsprecherbox und im Hörraum) zu Schwingungen angeregt werden.
Plötzliche Einschwingvorgänge lösen Bewegungen der Lautsprechermembran aus, die wellenförmig nach außen laufen. Dadurch wird noch Schall abgestrahlt, obwohl der Impuls längst zu Ende ist. Im Regelfall ist der Rand nicht mit der korrekten Wellenimpedanz abgeschlossen, daher wird die Welle reflektiert und verlängert den Impuls weiter.

Die Impulstreue wird neben der Lautsprecherqualität (möglichst weiche Aufhängung einer möglichst steifen Membran, großer Koppelfaktor beziehungsweise Wirkungsgrad) und dessen Montage (Boxengeometrie und gute Dämpfung) wesentlich auch durch die möglichst niederohmige Speisung der Schwingspule bestimmt. Ist der Innenwiderstand des Verstärkerausganges und der Widerstand der Lautsprecher-Anschlussleitungen (und einer eventuellen Frequenzweiche) insgesamt zu hoch, führt der Lautsprecher umso ungedämpfter weitere Schwingungen mit seiner Eigenresonanz aus, die nicht Inhalt des Musiksignals sind. Das Ohr ist jedoch in der Lage, auch wenige einzelne Schwingungen einer gedämpften Schwingung bereits als kurzen Ton zu interpretieren und dessen Tonhöhe zu bestimmen.

Insbesondere Bassreflexboxen liefern schlechte Impulsantworten im Bereich ihrer unteren Grenzfrequenz, da sie auf der Grundlage von Resonanz des Feder-Masse-Systems "Luftvolumen in der Box bzw. Luftmasse im Bassreflexrohr" funktionieren.

In der realistischen Situation eines normalen Wohnzimmers oder gar eines Raumes mit noch mehr Hall (z. B. leerer Konzertsaal) können die Effekte durch Reflexionen bzw. Hohlraumresonanzen jedoch oft größere und andere Effekte auf die Impulstreue zur Folge haben, als sie durch die Konstruktion des Lautsprechers beziehungsweise der Box verursacht werden. Hier kommen auch Laufzeitunterschiede hinzu, die durch Reflexionen auf verschiedenen Wegen oder mehrere, weit entfernt aufgestellte Lautsprecher verursacht werden und auch die Impulsantwort bei hohen Frequenzen verfälschen und bis zur Unverständlichkeit von Sprache führen können. Effekte durch Mehrfachreflexionen sind nicht Gegenstand dieses Artikels. Hingegen können Laufzeiteffekte, die aus der Wiedergabe mit mehreren, unterschiedlich weit vom Hörer aufgestellten Lautsprechern herrühren, vermieden werden, wenn die Lautsprecher alle in eine Richtung abstrahlen und man sie zeitverzögert entsprechend ihrer Entfernung von der Bühne ansteuert.

Nichtlineare Wiedergabefehler sind im Wesentlichen pegelabhängige Fehler. Hauptursache ist die Nichtlinearität des elektromechanischen Motors aus Spule und Magnetsystem. Bei hohen Schallpegeln ist zudem die Schallausbreitung in der Luft nichtlinear, was sich typischerweise bei den Hornlautsprechern für Großbeschallung bemerkbar macht.

Die nichtlinearen Verzerrungen werden üblicherweise als Frequenzspektrum angegeben, weil das Gehör die Nichtlinearitäten weitgehend genauso wahrnimmt. Man sagt, die Nichtlinearität „erzeugt zusätzliche Frequenzen“ – je nach Art und Stärke der Störung unterschiedliche mit verschiedenen Pegeln.

Dagegen wird diese Art der Verzerrung bei geringeren Anteilen als beim "Klirr" schon als störend empfunden. Professionelle Anlagen erreichen beim üblichen Arbeitspegel unter 1 % Differenz- und Summentonverzerrungen. Stärker kompromissbehaftete Konsumlautsprecher erzeugen je nach Größe und Frequenzbereich bei Arbeitspegel auch mehr als 10 %.

Kleinere Hörräume wechselwirken mit Lautsprechern und erzeugen Klangverfärbungen. Es kommt zu Interferenzen zwischen direktem Schall und reflektiertem Schall, der den Klang verändert. Erst große (ab einigen tausend Kubikmetern) und gut ausgelegte Räume weisen geringe Klangverfärbungen auf.

Für einen bestimmten Punkt im Raum könnten die Verfälschungen durch inverse Filterung beseitigt werden. Allerdings werden die Probleme wenige Zentimeter daneben nicht besser, sondern eher schlimmer. Damit ist klar, dass das Schallfeld eines Aufnahmeraumes auf keinen Fall im normalen Hörraum reproduziert werden kann und dass das Optimieren des Frequenzganges im echoarmen Raum im Falle des normalen Hörraumes relativ uninteressant ist. Diese Effekte treten auch bei anderen Schallquellen auf, etwa bei einem Sprecher oder einem Musikinstrument an Stelle des Lautsprechers. Die Verfälschungen sind immer vorhanden und gehören zur Alltagserfahrung, es ist kein Zufall, dass das Gehör unempfindlich gegenüber solchen Störungen ist.

Jeder Schallwandler, also der/die Treiber einschließlich aller Elemente des Gehäuses bzw. der Schallführung (im Grunde auch des Hörraums) ist ein System mit verteilten Parametern. Die klassische Vorstellung eines elektromechanischen Systems mit konzentrierten Parametern (Massen, Federsteifigkeiten, Schwingkreisgüten) vermag nur erste Anhaltspunkte einer Simulation zu geben. Zur rechnerischen Betrachtung dienen auch die Thiele-Small-Parameter. Damit eine Optimierung mit den im System verteilten Parametern durchgeführt werden kann, wurden verschiedene Korrekturtechniken entwickelt. Diese lassen sich grob in Steuerungen und Regelungen differenzieren.

Die einfachste und wichtigste Maßnahme ist die exakte Steuerung über die dämpfende Wirkung des Verstärkerausgangs. Aufgrund der Gegenkopplung sind die meisten Leistungsverstärker eine Regelschleife. Sinkt oder steigt der Momentanwert der Ausgangsspannung infolge einer Rückwirkung vom Lautsprecher, führt die Gegenkopplung den Wert auf denjenigen des Steuersignals zurück. Der Verstärkerausgang stellt für den Lautsprecher idealerweise eine Quellimpedanz des Wertes null dar.

Jeder dynamische Lautsprecher ist vereinfacht ein gedämpftes Feder-Masse-System, das eine Grundresonanz und infolge unterschiedlicher Schwingungsmodi der Membran immer auch Partialschwingungen bei höheren Frequenzen aufweist. Infolge der sich in Betrag und Phase ändernden Impedanz belastet der schwingende Lautsprecher den Verstärker im Vergleich zu einem ohmschen Widerstand anders. So wirkt ein dynamischer Lautsprecher immer auch wie ein elektrischer Generator. Wichtig ist das zumal bei schwach mechanisch bedämpften Lautsprechern im Bereich ihrer Grundresonanz. Die erzeugte Spannung ist oft gegenüber der Speisespannung phasenverschoben. Die auf den Verstärker rückwirkende Spannung wird durch den zumeist sehr geringen Innenwiderstand des Verstärkerausgangs mehr oder weniger kurzgeschlossen, und die Dämpfung des Lautsprechers steigt. Hieraus folgt, dass Lautsprecher, Lautsprecherkabel und Verstärker nicht nur hinsichtlich ihrer elektrischen Leistung dimensioniert werden müssen, sondern dass die Quellimpedanz des Verstärkers "und" die Impedanz des Kabels (und die vom Lautsprecher her gesehene Impedanz einer eventuellen Frequenzweiche) klein gegenüber dem Lautsprecherwiderstand (unbewegt) sein sollten.

Bei aktiven Lautsprechersystemen gibt es Anordnungen, die die Bewegung messen, meist nahe dem Antrieb (Schwingspule). Dafür sind Lautsprecherchassis mit einem dynamischen, piezoelektrischen- oder kapazitiven Sensor entwickelt worden. Mit dem Signal des Sensors wird versucht, das Antriebssignal geeignet vorzuverzerren. Damit wird zumindest im Bereich des Sensors eine Membranbewegung erzeugt, die dem gewünschten Audiosignal (Schalldruck) besser entspricht. Die Partialbewegungen (an anderen Stellen der Membran) werden dadurch kaum beeinflusst.

Es gibt Versuche, mit einer oder mehreren Messspulen näher am Rand der Membran oder metallisierten Membranoberflächen hinter einem Metallgitter und Messung der Kapazitäts- oder Ladungsänderungen zwischen Membranoberfläche und isoliert befestigtem Metallgitter bessere und genauere Korrektursignale zu gewinnen. Diese einige Zentimeter vom Zentrum entfernten Sensoren liefern wegen der endlichen Geschwindigkeit der Wellenausbreitung in Richtung Rand zeitversetzte Signale, die eine "schnelle" Regelung unmöglich machen. Eine langsame Regelung im Bassbereich erscheint möglich. Technisch gesehen handelt es sich um eine Regelung mit Totzeit, die immer als problematisch und ungenau gilt.

Eine „Bewegung der Gesamtmembran“ gibt es wegen der Vielzahl an Partialschwingungen nicht und kann deshalb auch nicht „gemessen“ werden. Es bleibt unklar, "was genau" metallisierte Membranoberflächen hinter einem Metallgitter messen. Es ist physikalisch unmöglich, die Partialschwingungen in ihrer Gesamtheit durch einen geänderten Antrieb der Schwingspule zu unterbinden.

Ziel der Membranvorauskorrektur ist, manche Wiedergabefehler des Gesamtsystems zu korrigieren, indem aus dem Eingangssignal und gemessenen Parametern des Systems ein Korrektursignal erzeugt und an einer geeigneten Stelle "mit umgekehrtem Vorzeichen" zum eigentlichen Audiosignal addiert wird. Der Lautsprecher wird also mit einem vorverzerrten Signal gespeist.

Auch diese Methode kann nicht beliebig große Fehler kompensieren – also aus einem schlechten schmalbandigen Lautsprecher kein HiFi-System machen – und besitzt Limitationen mathematischer Art.

Eine Regelung des Schallfeldes kann das Signal so beeinflussen, dass die linearen Artefakte für "einen" Ort korrigiert werden. Das führt an benachbarten Orten zu verstärkten Abweichungen. Sensor ist ein Messmikrofon in unmittelbarer Nähe der Hörposition. Raumresonanzen sowie andere spezifische Eigenheiten des Hörraumes werden in Bezug auf die "Position des Messmikrofons" hinsichtlich Frequenzlinearität der Wiedergabe weitgehend ausgeglichen. Hierzu wird z. B. das Frequenzverhalten der gesamten Übertragungskette einschließlich des Hörraums mit einem über das hörbare Frequenzspektrum gleitenden Sinus, einem Rauschen oder mit einem oder mehreren steilflankigen Impulsen eingemessen und die Abweichungen werden mit einem elektrisch einstellbaren Equalizer ausgeglichen. Effekte von Resonanzen auf die Impulstreue und von Echos und Laufzeiten auf den Raumeindruck können jedoch nicht vermieden werden.

Es ist nicht möglich, mit Mikrofonen einen Regelkreis inklusive des Verstärkers zu bauen. Dies würde es ermöglichen, analog zur Verstärkertechnik auch die nichtlinearen Artefakte deutlich zu reduzieren. Durch die akustischen Laufzeiten und durch die Phasenverdrehungen im Lautsprecher und im Mikrofon und vor allem durch die Schalllaufzeit zum Mikrofon entsteht ein äußerst instabiler Regelkreis, ganz ähnlich wie man es von der Aufnahmetechnik beim Mikrofon-Rückkopplungspfeifen her kennt.

Diskussionen und Aktivitäten zur Verbesserung der Wiedergabe befassen sich häufig nur mit den linearen Artefakten. Oben wurde dargelegt, dass bei normalen Abhörsituationen gegenüber diesen Fehlern der Lautsprecher die Effekte durch Interferenzen und Reflexionen im Raum überwiegen, sodass außer in reflexionsarmen Räumen auch gute Boxen keine gute Wiedergabe liefern können – die kammfilterartigen Auslöschungen führen dazu, dass bestimmte Frequenzen, die auf dem Tonträger vorhanden sind, schlecht oder nicht gehört werden können.

Die nichtlinearen Artefakte sind demgegenüber weit irritierender, weil zusätzlich Frequenzen entstehen, die in der Aufnahme nicht enthalten sind. Sie werden maßgeblich durch die Lautsprecher verursacht und nicht wie oft vermutet durch den Verstärker oder andere Übertragungsglieder. Sie sind daher ein wesentliches Qualitätskriterium von Lautsprechern, erklären jedoch nur teilweise deren große Preisunterschiede.

Typische technische Daten sehen wie folgt aus:

Die Belastbarkeit eines Lautsprechers wird durch zwei Effekte limitiert. Zum einen wird wegen des geringen Wirkungsgrades die meiste Energie in Wärme umgewandelt, und zwar im Antrieb. Dadurch kann der Lautsprecher thermisch zerstört werden. Zum anderen kann der Antrieb oder die Membran durch zu große Auslenkungen mechanisch geschädigt werden. Dies tritt vor allem bei den tiefsten zulässigen Frequenzen auf.

Die Angabe einer Sinusleistung (Leistung bei einer festgelegten Frequenz), wie sie z. B. bei Verstärkern üblich ist, ist für die Ermittlung der thermischen Belastbarkeit bei Lautsprechern nicht angebracht, da unter Umständen auch bei geringer Temperatur durch zu große Auslenkungen die mechanische Zerstörung einsetzt. Außerdem sind übliche Musiksignale im zeitlichen Mittel eher einem um 3 dB/Oktave abfallenden Frequenzgemisch ähnlich; siehe 1/f-Rauschen (rosa Rauschen). Dabei muss man beachten: Die zulässige thermische Leistung wird mit einem rosa Rauschen, "begrenzt auf den angegebenen Frequenzbereich", gemessen und als Mittelwert P angegeben. Das bedeutet, ein Hochtöner für den Frequenzbereich 8 kHz bis 16 kHz bekommt von der Maximalrauschleistung durch die Filterung nur ein Hundertstel ab.

Für die mechanische Zerstörung ist dagegen sehr wohl ein Sinussignal relevant. Bei Hoch- und Mitteltönern kann man zu große Auslenkungen meistens am drastischen Ansteigen des Klirrens feststellen, für Tieftöner kann man das Erreichen der maximal zulässigen Auslenkung leicht messen. Leider werden diese Daten nie von den Herstellern angegeben, man kann sie jedoch meistens aus anderen Daten berechnen. Typisch geht bei Hoch- und Mitteltönern durch die Frequenzweichen die mechanische Überlastung mit der thermischen einher. Eine Ausnahme sind Horntreiber. Diese sind für kleine Auslenkungen und große akustische Belastung entworfen. Ein Betrieb ohne diese, also unterhalb der Horngrenzfrequenz oder gar ohne Horn, kann zum sofortigen Ausfall trotz noch unkritischer Temperatur führen.

Für einen wirksamen Schutz von Tieftönern ist sowohl der thermische als auch der Auslenkungsgesichtspunkt zu beachten. Hohe Pegel lassen sich nur sinnvoll darstellen, wenn die Schutzvorrichtung auch die Wärmekapazität in Rechnung stellt. So kann z. B. ein Tieftöner durchaus für einige zehn Sekunden mit einer Leistungsaufnahme betrieben werden, die deutlich über der Dauerbelastungsangabe liegt. Die Schwingspule braucht Zeit, um sich aufzuwärmen. Die kleineren Antriebe von Hochtönern haben erheblich geringere Zeitkonstanten und bedürfen umso mehr der Vorsicht.

Lautsprecher können nicht durch leistungsschwache Verstärker vor Überlastung geschützt werden: Bei Übersteuerung (Clipping) erzeugen diese vor allem ungeradzahlige Harmonische, die bei Mehr-Wege-Lautsprechern zur Überlastung von Mittel- und Hochtöner führen können. Es ist sinnvoll, die Verstärkernennleistung (RMS) höher als die Lautsprecherbelastbarkeit (RMS) zu wählen, da dann die Wahrscheinlichkeit einer Überlastung zumindest geringer ist.

Aus der Angabe einer zulässigen Spitzenleistung kann man – mit dem in den technischen Angaben aufgeführten Wirkungsgrad – einen maximal erzielbaren Schalldruck errechnen. In der Praxis wird der Schalldruck jedoch oft durch Kompression und Verzerrungen auf einen niedrigeren Wert begrenzt, da die Schwingspule den Bereich des homogenen Magnetfeldes verlässt und die Membraneinspannung mechanische Grenzen setzt. Die Angabe einer Spitzenleistung „PMPO“, wie sie bei Lautsprechern der untersten Preisklasse zu finden ist, folgt keiner geschützten Definition und besitzt keine Aussagekraft.

Der "Wirkungsgrad" ist das Verhältnis von abgegebener Schallleistung zu zugeführter elektrischer Leistung. Er kann theoretisch zwischen 0 und 100 Prozent liegen.

Der Wirkungsgrad von typischen Lautsprechern ist erheblich von Baugröße, Verstärkerprinzip und Einsatzzweck abhängig. Er kann weit unter 0,1 Prozent liegen (häufig bei elektrostatischen Lautsprechern), aber auch Werte von 30 bis 40 Prozent erreichen (Hornlautsprecherarrays bei Bühnenbeschallung). Typische Hifi-Lautsprecher liegen im Bassbereich zwischen 0,2 und 1 Prozent. Die restlichen 99,x Prozent werden in Wärme umgewandelt – ein Teil in (passiven) Frequenzweichen, ein Teil direkt in der Schwingspule. Da eine nicht unbeträchtliche Menge an Wärme in der verhältnismäßig kleinen Schwingspule entsteht, kann diese bei fehlenden Vorsichtsmaßnahmen bei einer Überlastung schnell zerstört werden.

Für Lautsprecher wird aber nie der Wirkungsgrad angegeben, sondern der Kennschalldruckpegel bei einer bestimmten zugeführten Spannung.
Bei Passivlautsprechern ist diese Spannung meist 2 Volt (vorzugsweise bei Lautsprechern mit einer Nennimpedanz von 4 Ohm) oder 2,83 Volt (bei 8 Ohm, manchmal auch bei 4 Ohm). Die Messung erfolgt nie bei Leistungsanpassung, eine häufig zu findende Angabe von dB/W/m ist neben falschen Assoziationen (1 W: 85 dB, 2 W: 170 dB) auch aus diesem Grund Unsinn. Der Kennschalldruckpegel wird als logarithmisches Größenverhältnis in dB auf einen Norm-Schalldruck von 20 µPa (für Luft-Lautsprecher) bezogen.

Den Kennschalldruck kann man nicht in einen Wirkungsgrad umrechnen, die Fehler übersteigen üblicherweise eine Größenordnung.
Es ist schon mal ein guter Richtwert, wenn man weiß, dass bei Normaldruck 1 Watt Schallleistung in alle Richtungen abgestrahlt in 1 Meter Entfernung einem Schalldruck von 109 dB entspricht. Erreicht man 89 dB Schalldruck, hat man es mit 1 Prozent Wirkungsgrad zu tun. So weit, so gut – kommen wir jetzt zu den "Abers":

Ein gut konstruierter Lautsprecher hat eine lineare Übertragungsfunktion zwischen Eingangsspannung und Schalldruck auf der Hörachse bei halbwegs monoton fallendem Bündlungsfaktor.

Der (technisch mögliche) Wirkungsgrad bzw. der Kennschalldruck eines Lautsprechers ist abhängig von verschiedenen Größen:
Die betrachteten Schallwandler zeichnen sich alle durch einen recht geringen energetischen Wirkungsgrad aus. Dieser liegt hauptsächlich in der fehlenden Anpassung zwischen der elektrischen Impedanz der Schallimpedanz. Zwar spielen insbesondere in der HiFi-Technik andere Kenngrößen (Frequenzverhalten, Verzerrungen) eine wesentlichere Rolle, jedoch kommt dem Wirkungsgrad aus mehreren Gründen eine Bedeutung zu: Ein wirkungsgradschwacher Wandler (z. B. ein Magnetostat oder ein dynamischer Lautsprecher mit einem schwachen Magneten) benötigt beträchtliche Verstärkerleistung, die als Wärmeleistung von der Schwingspule abgeführt werden muss, damit eine Beschädigung der Spule vermieden wird. Erforderliche höhere Verstärkerleistung ist u. a. bei batteriebetriebenen Anwendungen nachteilig, verursacht ihrerseits Wärme oder erfordert Verstärker mit hoher Effizienz, die nicht immer auch gute Übertragungseigenschaften besitzen.

Eine effektive Kopplung des Lautsprechers an die Luft (z. B. Bassreflexprinzip, große Schallwand, großes Volumen bei geschlossenen Boxen, Exponentialtrichter) erhöht die Effizienz.
Allerdings kann die Effizienzverbesserung durch bessere Luftankopplung unter Umständen auch zu einem verzerrten Frequenzgang führen: Ausgeprägte Eigenresonanzen kleiner Boxenvolumina oder des Bassreflexweges führen zu einer selektiven Erhöhung der Lautstärke, aber auch zu einer Verschlechterung der Impulstreue.

Große Auslenkungen verursachen u. a. bei dynamischen Lautsprechern auch hohe Intermodulationsverzerrungen. Großer Wirkungsgrad und gute Schallwiedergabe werden daher mit großen Lautsprechern (geringere Auslenkung bei gleichem Schallpegel) erreicht. Große Bauformen sind jedoch häufig nicht erwünscht, sie sind teurer oder weisen andere Nachteile auf (z. B. Partialschwingungen der Membran).

Bei der Beschallung z. B. von Bahnhöfen kommt es auf eine gute Sprachverständlichkeit bei großem Pegel an. Oft werden hier Hornlautsprecher oder Druckkammerlautsprecher eingesetzt, die nur den relativ geringen Frequenzumfang der Sprache mit hohem Wirkungsgrad wiedergeben. Deren gerichtete Abstrahlung, insbesondere der hohen Frequenzen (Zischlaute), kann zur Erhöhung der Effizienz, aber auch zur Vermeidung von Laufzeit-Verzerrungen (Reflexionen, mehrere Quellen) genutzt werden, die ansonsten die Sprachverständlichkeit beeinflussen.





</doc>
<doc id="14682" url="https://de.wikipedia.org/wiki?curid=14682" title="Dreifaltigkeit">
Dreifaltigkeit

Dreifaltigkeit, Dreieinigkeit oder Trinität (lateinisch ""; altgriechisch "Trias" ‚Dreizahl‘, ‚Dreiheit‘) bezeichnet in der christlichen Theologie die Wesenseinheit Gottes in drei Personen oder Hypostasen, nicht drei Substanzen. Diese werden „Vater“ ("Gott der Vater", "Gott Vater" oder "Gottvater"), „Sohn“ ("Jesus Christus", "Sohn Gottes" oder "Gott Sohn") und „Heiliger Geist“ ("Geist Gottes") genannt. Damit wird zugleich ihre Unterscheidung und ihre unauflösbare Einheit ausgedrückt.

Die christliche Trinitätslehre wurde zwischen 325 (Erstes Konzil von Nicäa) und 675 (Synode von Toledo) durch mehrere Konzile und Synoden entwickelt. Die beiden konträren Hauptrichtungen waren dabei die Antiochenische und die Alexandrinische Schule. Zu Beginn des arianischen Streits im Jahr 318 vertrat der Presbyter Arius als Antiochener die Auffassung der "Wesensähnlichkeit" (mit Unterordnung des Sohnes unter seinen Vater) im Unterschied zur "Wesensgleichheit" (mit Gleichrangigkeit von Vater und Sohn) zwischen Gott und seinem Sohn, wie sie von den Bischöfen Alexander und später Athanasius als Alexandrinern vertreten wurde. Später ging es auch um die Stellung des Heiligen Geistes. Nachdem in der Frühzeit des Christentums die Sicht des Arius zeitweise vorherrschend war, setzte sich die Sicht des Athanasius schließlich durch. Heute befinden sich Antitrinitarier und Unitarier in der Minderheit.

Im Kirchenjahr ist "Trinitatis", der erste Sonntag nach Pfingsten, dem Gedenken der Dreieinigkeit Gottes gewidmet.

Die Vorstellung von einer göttlichen Dreiheit (Trias) gibt es auch in anderen Religionen. Inwieweit vorchristliche antike Konzepte Analogien zur Trinitätslehre aufweisen oder sogar deren Entstehung beeinflusst haben, ist umstritten. Im Judentum und im Islam wird das Konzept der Trinität abgelehnt.

Das Alte und Neue Testament enthalten nach christlicher Interpretation Hinweise auf eine Trinitätslehre, ohne aber eine solche zu entfalten. Für die Rezeptionsgeschichte bedeutsam sind neben Formeln, die direkt auf die Trinität bezogen wurden, auch Aussagen zur Göttlichkeit von Sohn und Geist.

Die neutestamentliche Rede vom heiligen Geist hat Vorläufer in Formulierungen des Alten Testaments, beispielsweise ; ; oder und der zeitgenössischen Theologie, in der es auch gewisse Parallelen für Vorstellungen gibt, die sich im Neuen Testament mit Jesus Christus verbinden. Darüber hinausgehende Bezugnahmen sind spätere Reinterpretationen. So beziehen sich etwa frühe christliche Theologen allgemein auf Stellen, wo vom Engel, Wort (davar), Geist (ruah) oder der Weisheit (hokhmah) oder Gegenwart (shekhinah) Gottes die Rede ist, sowie auf Stellen, wo Gott von sich im Plural spricht (, ) sowie insbesondere das dreifache „Heilig!“ der Seraphim in , das in der Liturgie im Trisagion aufgenommen wurde. Immer wieder wurde auch der Auftritt dreier Männer in auf die Trinität bezogen. In der jüdischen Religion wird die Idee der Trinität aber abgelehnt.

Man hat die Spezifikation eines bereits im AT manifesten „Immanenzwillens“ Gottes sowie eine Rede in „unvertauschbaren“ Namen von Geist, Sohn und Vater diagnostiziert.

Die frühesten wirkungsgeschichtlich einschlägigen Formulierungen prägt jedenfalls Paulus. Er verwendet in vermutlich einen Segensgruß der frühen christlichen Liturgie: „Die Gnade Jesu Christi, des Herrn, die Liebe Gottes und die Gemeinschaft des Heiligen Geistes sei bei euch!“ In werden Gnadengaben „in gezielter Steigerung“ auf Geist, Herr und Gott zurückgeführt. Auch ordnet Vater, Sohn und Geist neben- und aufeinander hin.

Besonders wirkungsgeschichtlich einflussreich, wenn auch nicht zum „Prototyp der christlichen Taufe“, wird die Taufformel in . „Auf den Namen“ (εἰς τὸ ὄνομα, wörtl. „in den Namen“) bezeichnet dabei eine Übereignung. Als „Pendant“ dazu hat man die Erzählung der Taufe Christi im Jordan durch Johannes den Täufer gesehen, weil dort durch Herabschweben des Geistes und Himmelsstimme des Vaters ebenfalls Vater, Sohn und Geist vereinigt sind. Vermutlich ist diese Taufformel die Erweiterung einer Taufe „auf den Namen Christi“. Auch die nach 100 n. Chr. entstandene Didache (der frühe „Katechismus mit Anweisungen über die liturgischen Vollzüge“) kennt bereits eine solche erweiterte Taufformel: „Tauft auf den Namen des Vaters und des Sohnes und des Heiligen Geistes“.

Die Bezeichnung „Gott“ bezieht sich im Neuen Testament meistens auf den "Vater". Gott und der Sohn Gottes erscheinen als voneinander unterschieden, wenn es etwa heißt: „Gott sandte seinen Sohn“ (). Oder wenn Jesus „zur Rechten Gottes steht“ (). Gott, das ist (z. B. in ) der „Vater unseres Herrn Jesus Christus“. Diese Vorstellung betrifft auch die Zukunft; am Ende „wird sich auch der Sohn unterwerfen“ und „Gott alles in allem“ oder „in allen“ () sein.

Bereits die ältesten Texte des Neuen Testaments zeigen eine enge Verbindung von Gott und Jesus: Dieser wirkt mit göttlicher Vollmacht – so sehr, dass Gott selbst in Jesus und durch ihn sein Schaffen, Richten, Erlösen und Sich-Offenbaren vollzieht. Zu den christologisch besonders aussagekräftigen Texten zählt etwa der Hymnus in ff., der u. a., wie , eine Präexistenz und ein Geschaffensein des Kosmos in Christus aussagt. Die Relation zwischen Christus als Sohn Gottes und Gott-Vater ist mehreren Autoren des Neuen Testaments wichtig. Eine besondere Vertrautheit wird in der Abba-Anrede und dem „Erkennen“ des Vaters durch den Sohn betont; vor allem das Johannesevangelium () spricht von einer Relation der Einheit und wechselseitigen Immanenz zwischen Vater und Sohn in der Liebe.

Der Geist ist nach Matthäus und Lukas bereits bei der Empfängnis Jesu wirksam. Der irdische Jesus ist sodann nach den Evangelisten Träger („voll“) des Heiligen Geistes, insbesondere nach Paulus der Auferstandene dann dessen Mittler. Im Johannesevangelium offenbart der Geist die Einheit zwischen Vater und Sohn, mehr noch, Jesus bekennt sogar: „Gott ist Geist“ (), womit die Präsenz und das Wirken Gottes als Geist glaubbar wird (; ).

Der biblischen Rede von Vater, Sohn und Geist lassen sich nur Weichenstellungen für die späteren Rezeptionen bei der Ausarbeitung einer Trinitätslehre entnehmen. Prägend wird besonders die rituelle Praxis und Gebetspraxis der frühen Christen.

Die frühesten deutlich dreiheitlich strukturierten Formeln begegnen als Taufformeln und in Taufbekenntnissen, die mit drei Fragen und Antworten die Übereignung an Vater, Sohn und Geist vorbereiten und dann vollziehen.

Auch in der Eucharistiefeier finden sich trinitarische Formeln: Durch den Sohn wird dem Vater gedankt, dann um Herabsendung des Geistes gebeten. Die Schlussdoxologie verherrlicht den Vater durch den Sohn und mit dem Geist (oder: mit dem Sohn durch den Geist).

Auch die regula fidei bei Irenäus, die u. a. in der Taufkatechese Verwendung fand, ist trinitarisch strukturiert.

Die christliche Theologie war in den ersten Jahrhunderten nicht eindeutig definiert. Es gab jedoch schon früh Abgrenzungen zu extremen Varianten der Christologie, wie dem Adoptianismus (Jesus wurde bei der Taufe von Gott adoptiert) oder Doketismus (Jesus war rein göttlich und erschien nur als Mensch). Unter diversen Versuchen befanden sich einige – wie Adoptionismus und modalistischer Monarchianismus (der Vater und der Sohn sind „nur“ verschiedene Erscheinungsformen des einen Gottes) –, die von den führenden Kirchenvätern einmütig als Häresie verurteilt wurden.

Justin der Märtyrer verwendet zahlreiche trinitarische Formeln.

Irenäus von Lyon entwickelt – unter anderen auf dem Prolog des Johannesevangeliums () aufbauend – eine Logos-Theologie. Jesus Christus, der Sohn Gottes, wird mit dem präexistenten Logos als wesentlichem Akteur der Schöpfung und der Offenbarung Gottes gleichgesetzt. Auch eine eigenständige Pneumatologie arbeitet Irenäus aus. Der Heilige Geist ist Gottes Weisheit. Geist und Sohn gehen nicht durch eine Emanation hervor, welche sie auf eine andere ontologische Stufe zum Vater stellen würde, sondern durch „geistige Emanation“.

Tatian versucht einen eigenständigen Sonderweg, wobei der Geist auch als Diener Christi, des Logos, auftritt und einem weltjenseitig-unwandelbaren Gott nachgeordnet wird.

Das griechische Wort "trias" für Gott Vater, Sohn und Heiliger Geist, das in den Ostkirchen bis heute das übliche Wort für die christliche Dreifaltigkeit ist, wird erstmals erwähnt in der zweiten Hälfte des 2. Jahrhunderts bei dem Apologeten Athenagoras von Athen:

In die Westkirche wurde, wenige Jahrzehnte nachdem Athenagoras von Athen von „trias“ gesprochen hatte, das entsprechende lateinische Wort "trinitas" von Tertullian eingeführt. Es ist eine eigens hierfür geschaffene Neubildung aus "tres – drei" und "unitas – Einheit". Von Haus aus Jurist, erklärte er die Wesenheit Gottes in der Sprache des römischen Rechtswesens. Er führt den Begriff "personae" (Plural von "persona" – Partei im rechtlichen Sinn) für Vater, Sohn und Heiligen Geist ein. Für die Gesamtheit von Vater, Sohn und Heiligen Geist verwendete er den Begriff "substantia", das den rechtlichen Status in der Gemeinschaft bezeichnet. Nach seiner Darstellung ist Gott in der "substantia" einer, aber in der "monarchia" – der Herrschaft des einen Gottes – wirken drei "personae", Vater, Sohn und Heiliger Geist. Einer anderen Version zufolge entlehnte Tertullian die Metapher „persona“ dem Theater von Karthago, wo die Schauspieler Masken (personae) vor ihr Gesicht hielten, je nach Rolle, die ihnen zukam.

Die Gegensätze zum Trinitätsdogma in der frühen Kirche können unter den Strömungen des Monarchianismus, des Subordinatianismus und des Tritheismus zusammengefasst werden. Am einflussreichsten wurde der Arianismus, eine Spielart des Subordinatianismus, der drei verschiedene Personen in Gott annimmt, aber der zweiten und dritten Person die Wesensgleichheit mit dem Vater und damit die wahre Gottheit abspricht und folglich Jesus in eine Zwischenposition zwischen göttlich und menschlich setzt. Diese Lehre wurde vom ersten Konzil von Nicäa (325) als Irrlehre zurückgewiesen. Die erhoffte Einigung blieb aus. Nach dem Konzil von Nizäa folgte ein jahrzehntelanger theologisch und politisch motivierter Streit zwischen Nizänern und Arianern. Die arianische Partei gewann in den Jahren nach Nizäa besonders bei der höheren Geistlichkeit und den hellenistisch Gebildeten bei Hof und im Kaiserhaus viele Anhänger, so dass 360 die Mehrheit der Bischöfe freiwillig oder gezwungen arianisch stimmten (siehe unter Arianischer Streit). Es traten verschiedene arianische Synoden zusammen, die zwischen 340 und 360 vierzehn verschiedene nichttrinitarische Bekenntnisse beschlossen.

Neben der christologischen Frage, die beim Konzil von Nicäa im Vordergrund gestanden hatte, kam Mitte des Jahrhunderts die nach der Stellung des Heiligen Geistes hinzu. Ist der Geist Gottes eine Person der göttlichen Trinität, eine unpersönliche Kraft Gottes, eine andere Bezeichnung für Jesus Christus oder ein Geschöpf?

Die Makedonianer (nach einem ihrer Führer, dem arianischen Patriarchen Makedonios I. von Konstantinopel) oder Pneumatomachen (Geistbekämpfer) vertraten die Ansicht, der Heilige Geist sei ebenso ein Geschöpf wie der Sohn.

Ab 360 wurde die Frage von den Anhängern von Nizäa aufgenommen. Athanasius schrieb seine "Vier Briefe an Serapion". 362 formulierte das Konzil von Alexandria eine Lehre über den Heiligen Geist. Kurz darauf kam von Gregor von Nyssa eine "Predigt über den Heiligen Geist", wenige Jahre später von seinem Bruder Basilius die Abhandlung "Über den Heiligen Geist"; sein Freund Gregor von Nazianz hielt 380 die fünfte Theologische Rede über den Heiligen Geist als Gott. Fast gleichzeitig schrieb Didymus der Blinde eine Abhandlung über den Heiligen Geist. Die griechische Theologie des vierten Jahrhunderts verwendet dabei statt "Person" das griechische Wort "Hypostase" (Wirklichkeit, Wesen, Natur), das auch heute in der Theologie oft vorgezogen wird, da der moderne Begriff Person oft fälschlicherweise mit dem antiken Begriff "persona" gleichgesetzt wird.

Hilarius von Poitiers schrieb auf Lateinisch über die Trinität und Ambrosius von Mailand veröffentlichte 381 seine Abhandlung "De Spiritu Sancto".

381 wurde das erste Konzil von Konstantinopel einberufen, um den arianischen Streit beizulegen. Dort wurde das mit dem nicänischen Glaubensbekenntnis verwandte Nicäno-Konstantinopolitanum beschlossen, das insbesondere den Teil bezüglich Heiligem Geist erweiterte und damit die Trinität mehr betonte als alle früheren Bekenntnisse.

Das Nicäno-Konstantinopolitanum formulierte die trinitarische Lehre, die bis heute sowohl von den westlichen als auch von allen orthodoxen Kirchen anerkannt wird und in allen christologischen Auseinandersetzungen der nächsten Jahrhunderte übernommen wurde.

Im Konzil von Chalzedon wurden die mit der Trinitätslehre zusammenhängenden christologischen Fragen präzisiert.

Während sowohl die östliche als auch die westliche Tradition der Kirche die Trinität seit dem Konzil von Konstantinopel als festen Bestandteil ihrer Lehre sehen, gibt es doch Nuancen: In der östlichen Tradition, basierend auf der Theologie von Athanasius und den Drei Kappadokiern, wird etwas mehr Wert auf die drei Hypostasen gelegt, die westliche Tradition betont, basierend auf der von Augustinus von Hippo einige Jahrzehnte später in drei Bänden ausgeführten Interpretation der Trinität, eher die Einheit.

Augustinus von Hippo argumentiert, dass es nur durch die Trinität möglich sei, dass Liebe ein ewiger Wesenszug Gottes sein kann. Liebe braucht immer ein Gegenüber: ein nichttrinitarischer Gott könnte also erst lieben, nachdem er ein Gegenüber erschaffen hat, das er lieben kann. Der dreieinige Gott habe jedoch von Ewigkeit her das Gegenüber der Liebe in sich selbst, wie Jesus es in beschreibt.

Unterschiedliche Auffassungen über die Relationen zwischen Vater, Sohn und Geist führten schließlich zum Filioque-Streit, der eine der Ursachen für das Morgenländische Schisma war und bis heute nicht beigelegt ist.

Im 6. Jahrhundert entstand dann im Westen das nach Athanasius von Alexandria bezeichnete, aber nicht von ihm verfasste Athanasische Glaubensbekenntnis. Die Theologie dieses Glaubensbekenntnisses basiert stark auf der Theologie der westlichen Kirchenväter Ambrosius († 397) und Augustinus († 430) und wurde von Bonaventura von Bagnoregio († 1274) sowie Nikolaus Cusanus († 1464) weiterentwickelt.

Heute sehen die meisten Kirchenhistoriker das Nizänokonstantinopolitanum von 381 als das erste und wesentliche verbindliche Bekenntnis zur Trinität an. Das etwa zweihundert Jahre jüngere und nur im Westen verbreitete Athanasische Glaubensbekenntnis hat auch in der westlichen Kirche nie den theologischen oder liturgischen Stellenwert des Nizänokonstantinopolitanum gehabt.

Die katholische Kirche formulierte die Trinitätslehre in der 11. Synode von Toledo 675 als Dogma, bestätigt sie im 4. Laterankonzil 1215 und stellte sie auch danach nie in Frage.

Athanasius meint, dass der Erlöser Jesus Christus selbst Gott sein müsse, da nach Gott die Welt mit sich versöhnen wird.

Athanasius, Gregor von Nazianz und Ambrosius von Mailand verweisen im 4. Jahrhundert auf Stellen, in denen Jesus in ihrer Sicht als einziger dem Schöpfer gleichgesetzt wird, zum Beispiel oder und auf das Wort "kyrios" (Herr), das in der griechischen Septuaginta für das hebräische JHWH (wie auch für Adonai, „Herr“) verwendet wird, und im Neuen Testament sowohl für Gott und als auch für Jesus, wobei "kyrios" (Jesus) im Neuen Testament häufig im gleichen Kontext steht wie "kyrios" (JHWH) im Alten Testament (vgl. und , und , und ).

Weitere Bibelstellen sind: „Ehe Abraham wurde, bin ich.“ () mit einem von den Zuhörern verstandenen Bezug auf das „Ich bin“ von , und „Ich und der Vater sind eins“ (), was von den Zuhörern in so verstanden wurde, dass Jesus sich selbst zu Gott machte, worauf sie ihn wegen Gotteslästerung zu steinigen suchten. Thomas nennt ihn in „Mein Herr und mein Gott“, und in wird er der „wahrhaftige Gott“ genannt. schreibt Jesus die göttliche Eigenschaft Unwandelbarkeit zu: „Jesus Christus ist derselbe, gestern, heute und in Ewigkeit“; in Hebräer 1,8-10 heißt es von dem Sohn: „Dein Thron, o Gott, währt von Ewigkeit zu Ewigkeit.“ (Sohn wird als Gott bezeichnet).

Oft als Gegenbeleg verwendet wird „Mein Gott, mein Gott, warum hast du mich verlassen“, ein wörtliches Zitat des Anfangs von . Augustinus sieht die Unterordnung von Jesus unter den Vater als freiwillige Unterordnung , nicht als qualitativen Unterschied. Daraus folgend kann er Aufträge, die Jesus vom Vater erhält und ausführt, als einen Hinweis auf eine unterschiedliche Funktion, nicht einen unterschiedlichen Rang verstehen.

Arius deutete – wie auch viele Kirchenväter – die Weisheit als alttestamentlichen Ausdruck Jesu Christi. Als biblisches Argument gegen die Gottgleichheit Christi bezog er sich vor allem auf die Aussage der „Weisheit“ im Buch der Sprichwörter, die von sich sagt, sie sei von Gott noch „vor den Werken der Urzeit“ geschaffen worden .

Basilius von Caesarea, Gregor von Nazianz und Athanasius führen Bibelstellen an, wo der Geist als Person handelt und mit anderen Personen in Beziehung tritt. Besonders deutlich sehen sie das z. B. in , wo sich ein männliches Pronomen auf das im Griechischen neutrale Wort πνεῦμα "pneuma" (Geist) bezieht. In tritt der Geist für uns ein, in wirkt er Wunder, in deckt er Sünde, Gerechtigkeit und Gericht auf. Er kann belogen werden (), betrübt werden (), gelästert werden . Die Apostel verwenden ihn in zusammen mit sich selbst als Subjekt des Satzes („Der Heilige Geist und wir haben entschieden …“). Basilius führt Beispiele von Typologien und Personifizierungen von Abstrakta in der Bibel auf, unterscheidet sie aber klar von der Beschreibung des Heiligen Geists im Neuen Testament.

Nachdem die Entwicklung des Dogmas selbst abgeschlossen war, wurde es in der Scholastik spekulativ durchdacht und systematisch eingeordnet.

Thomas von Aquin sah in der zweiten und dritten Person Gottes die ewige Selbsterkenntnis und Selbstbejahung der ersten Person, d. h. Gott Vaters. Weil bei Gott Erkenntnis bzw. Wille und (sein) Wesen mit seinem Sein zusammenfallen, ist seine vollkommene Selbsterkenntnis und Selbstliebe von seiner Natur, also göttlich.

Johannes Duns Scotus stellte heraus, dass durch Vernunft nur die Existenz Gottes erkannt werden kann, als eindeutiger (univoker) Kern von Begriffen, der nichts Inhaltliches über sein Wesen aussagen kann. Glaubenswahrheiten wie die Trinität setzen Offenbarung voraus und gehören in den Bereich der Theologie. Sie können nur im Nachhinein durch Analogien verstanden werden.

Meister Eckhart entwickelte eine konsequent Negative Theologie. Die Gotteserkenntnis wird zu einem momenthaften Geschehen, zu einem bloßen „Fünklein“, in dem Erkennender und Erkanntes im Heiligen Geist immer wieder zu Eins verschmelzen. Die Trinität als fortlaufende Gottesgeburt ist ein dynamisches Geschehen von Erkennen oder Gebären und Vergehen an der Grenze der Welt. Die Erlösungslehre Eckharts stellt die Menschwerdung Gottes in den Mittelpunkt, die ein Werk der Trinität ist. Die menschliche Natur Christi ist keine andere als die jedes anderen Menschen: „Wir alle haben die menschliche Natur mit Christus gemeinsam und zwar in gleicher Weise und gleichem Sinne (univoce)“. Der einzelne Mensch als Teilhaber an der allgemeinen Menschennatur kann auf Grund der hypostatischen Union mit Gott eins sein wie Christus. „Der Mensch kann Gott werden, weil Gott Mensch geworden ist und dadurch die menschliche Natur vergöttlichte.“

In der barocken Deutung der Trinität finden sich Hinweise auf die pythagoreisch-platonische Ideenlehre, demzufolge der musikalische Dreiklang, der aus einer harmonischen und arithmetischen Teilung der Quinte hervorgeht, eine symbolhafte Darstellung der Trinität sei. Wiewohl der Dreiklang aus drei Klängen besteht, so vereinigt er sich doch zu einem Klang.

In der Theologie des 20. Jahrhunderts wurden solche trinitätstheologischen Ansätze besonders wichtig, welche von drei als gleichursprünglich gedachten göttlichen Personen ausgehen und die Beziehung, das Zu-, Für- und Ineinander der drei betonen, das die Einheit Gottes konstituiere. Sie berufen sich dazu auf altkirchliche Vorbilder wie die Trinitätslehre Tertullians, den ostkirchlichen Gedanken der Perichorese und auf das Diktum des Athanasius, auch der Vater sei nur Vater, weil er einen Sohn habe, mit dem er sich gegen die Subordination des Sohnes wendet.

Soziale Trinitätslehren werden sowohl von protestantischen Theologen wie Jürgen Moltmann und Wolfhart Pannenberg, aber auch von römisch-katholischen wie Gisbert Greshake und dem Befreiungstheologen Leonardo Boff vertreten. Zentral für diese Ansätze ist dabei, dass sie die innertrinitarische Gemeinschaft als der Gottheit ursprünglich und als Modell für Gesellschaft und Kirche verstehen. Besonders Leonardo Boff versteht die dreifaltige Gemeinschaft in Gott als Kritik und Inspiration der menschlichen Gesellschaft und begründet die befreiungstheologische Option für die Armen trinitätstheologisch.

Die Motivation für die Entstehung der Trinitätslehre sieht Joseph Ratzinger nicht in einer Spekulation über Gott – also in einem Versuch des philosophischen Denkens, sich zurechtzulegen, wie der Ursprung allen Seins beschaffen sei –, sondern sie habe sich aus dem Mühen um eine Verarbeitung geschichtlicher Erfahrung ergeben. Zentral ist somit die Interpretation biblischer Texte. Die von den Kirchenvätern begründete Auslegungstradition wird von allen drei großen christlichen Traditionen anerkannt. Die historische Differenz der biblischen Sprache zu einer daran zu messenden, von einem philosophischen Vorverständnis geprägten Interpretation war den Kirchenvätern bewusst und ist heute weitgehend anerkannt. So urteilte die "New Catholic Encyclopedia" 1967: „Exegeten und Bibeltheologen, darunter auch immer mehr Katholiken, erkennen an, dass man von einer Trinitätslehre im Neuen Testament nicht ohne wesentliche Einschränkungen sprechen sollte.“ Zwar wird in der 2. Auflage von 2003 wiederholt, von den Kirchenvätern als Vorahnungen interpretierte alttestamentliche Stellen könnten nicht als explizite Offenbarungen der Trinität verstanden werden. Jedoch würden , und den Glauben der apostolischen Kirche „an eine Lehre von drei Personen in einem Gott“ bezeugen, auch ohne die später eingeführte Terminologie zu verwenden.

Karl Rahner fasste Gott als selbstmitteilend auf. Er beruft sich dabei (indirekt) auf die Alte Kirche und vor allem Thomas von Aquin. Rahner begründet den Glauben an Gott als dreieinen mit der Gotteserfahrung, die die Menschen durch ihre Begegnung mit Jesus Christus machen – und leitet nicht die Christologie aus der Trinitätslehre ab; Christus kann nur von der Heilsgeschichte (Ökonomie) her verstanden werden: „Die ‚ökonomische‘ Trinität "ist" die ‚immanente‘ Trinität und umgekehrt.“ Dies bedeutete für Rahner weder einen Reduktionismus noch die Möglichkeit, Gottes inneres Wesen aus seinem Handeln abzuleiten. Er wollte klarstellen, dass im historischen Jesus Gott selbst so in der Welt gegenwärtig ist wie in seiner inneren göttlichen Realität; die immanente Trinität ist völlig, wenn auch für den menschlichen Verstand unerschöpflich "in" der Ökonomie gegenwärtig und nicht "hinter" ihr.

Rahner behandelt in seinen Schriften zur Trinitätslehre regelmäßig die Frage nach der Geltung und Bedeutung der so genannten psychologischen Trinitätslehre des Augustinus von Hippo, das Axiom der gegenläufigen Identität von ökonomischer und immanenter Trinität und die Problematik des Begriffes „Person“ (dieser habe nach Rahner in der Trinitätslehre nur die Bedeutung einer Daseinsweise eines geistigen Wesens, nicht aber die Bedeutung eines individuellen, seiner selbst bewussten Subjekts).


In seiner gesamten Theologie betonte Dietrich Bonhoeffer den Aspekt christlicher Diesseitigkeit, begründete sie durch die Menschwerdung Gottes und das Kreuz Christi und radikalisierte diesen Ansatz in der Frage nach einem religionslosen Christentum jenseits klassischer Metaphysik. Das „Vorletzte“ ist „Hülle des Letzten“, nur durch die Welt kommt der gläubige Mensch zu Gott. An Karl Barth kritisiert er einen „Offenbarungspositivismus“, der keine „Stufen der Erkenntnis und Stufen der Bedeutsamkeit“ kenne, „wo es dann heißt: ‚friß, Vogel, oder stirb‘; ob es nun Jungfrauengeburt, Trinität oder was immer ist, jedes ist ein gleichbedeutsames und -notwendiges Stück des Ganzen, das eben als Ganzes geschluckt werden muß oder gar nicht.“ Dagegen will Bonhoeffer eine Arkandisziplin wiederherstellen, die letzte Dinge nicht mit profanen Tatsachen gleichsetzt, sondern ihr Geheimnis wahrt, das sich nur in der Praxis des Glaubens an die Person Jesu erschließt. Dessen Wesen ist „Für-andere-da-sein“, und an diese zentrale Einsicht ist die Idee der innergöttlichen Liebe gebunden. Im „Teilnehmen an diesem Sein Jesu“ kann Transzendenz im Hier und Jetzt erfahren werden: „Nicht die unendlichen, unerreichbaren Aufgaben, sondern der jeweils gegebene erreichbare Nächste ist das Transzendente.“

Von Bonhoeffer übernahm Karl Barth in seiner Kirchlichen Dogmatik den Gedanken einer "analogia relationalis" zwischen der innertrinitarischen Bezogenheit Gottes, seiner Bezogenheit als der eine Gott zum Menschen und der geschlechterspezifischen Relation von Frau und Mann. Ähnlich wie Rahner fasste Barth Gott als Ereignis der Offenbarung auf, dessen Struktur trinitarisch ist: Gott ist Subjekt (Vater), Inhalt (Sohn) und Geschehen (Geist) von Offenbarung. Somit wird der immanente (unsichtbare) Aspekt auf den ökonomischen (sichtbaren) rückbezogen, was auch als Neo-Modalismus eingeordnet wird.

Der christlichen Tradition der göttlichen Trinität entspringe das Motto der Französischen Revolution „Freiheit (Sohn), Gleichheit (Vater) und Brüderlichkeit (Heiliger Geist)“, erklären Vertreter der Rechtsphilosophie. Dieses politische Credo bildet die Grundlage der westlichen Demokratien. Auch die Präambel der Europäischen Grundrechtecharta greift diese Dreifaltigkeit im Verein mit der monistischen Idee der Menschenwürde auf.

Schon von Kirchenvätern wurden Analogien zur Veranschaulichung der Trinität verwendet, meist mit dem ausdrücklichen Hinweis, dass sie nur ganz unvollkommene Bilder bzw. im Grundsatz falsch seien.

Das bekannteste Zeichen der Dreieinigkeit ist das Dreieck. Es war schon ein Symbol der Manichäer. Dennoch wurde es weiter benutzt, und seine neue christliche Bedeutung soll jetzt durch das Einfügen des Auges Gottes betont werden; das Auge ist bei vielen Völkern allerdings ein Symbol des Sonnengottes. Bekannt sind außerdem Kombinationen aus Christusmonogramm, Alpha und Omega und dem Kreuz mit dem Dreieck.

Eine Weiterentwicklung des Dreiecks ist der Schild der Dreifaltigkeit.

Eine geometrische Anordnung von drei sich überschneidenden Kreisen findet sich häufig als Maßwerk (schmückende ornamentale Formen) in der gotischen und neugotischen Baukunst (siehe Grafik rechts). Beide im Folgenden erklärten Formen des Maßwerkes findet man in vielfach ausgeschmückter und auch unterschiedlich gedrehter Ausrichtung.


Vereinfacht kann man sagen: Der Dreipass betont mehr die Einheit der drei Personen („Dreieinigkeit“), das Dreiblatt mehr ihre Unterscheidbarkeit („Dreifaltigkeit“). Im heutigen Sprachgebrauch wird zwischen Dreieinigkeit und Dreifaltigkeit jedoch meist nicht unterschieden.

Die Interpretation von Dreihasenbildern als Symbole der Trinität in der kirchlichen Kunst ist umstritten.

Die Flagge Äthiopiens hat ebenfalls eine dem religiösen Bereich zugeschriebene Deutung: die Farben beziehen sich auf die christliche Dreifaltigkeit. Danach steht Grün für den Heiligen Geist, Gelb für Gott Vater, Rot für den Sohn. Gleichzeitig versinnbildlichen die Farben die christlichen Tugenden Hoffnung (Grün), Nächstenliebe (Gelb) und Glaube (Rot).

Bei Hildegard von Bingen ("Scivias") finden sich mystische Farbzuordnungen: Die „feuchte Grünkraft“ ("viriditas") steht dort für Gott Vater, der Sohn ist durch eine „purpurfarbene (Grün-)Kraft“ ("purpureus viror") gekennzeichnet.

Die älteste bildliche Darstellung beruht auf dem typologisch gedeuteten "Besuch der drei Männer" bei Abraham in Mamre . Dabei werden drei gleich aussehende, junge Männer nebeneinander dargestellt. Das früheste erhaltene Beispiel findet sich in der Katakombe an der Via Latina und stammt aus dem 4. Jahrhundert. Spätere Darstellungen stellen die drei Männer an einem Tisch sitzend dar und fügen ihnen Merkmale der Engeldarstellung bei. Als Höhepunkt dieses Bildtyps kann die Ikone von Andrei Rubljow aus dem 15. Jahrhundert gesehen werden.

Eine andere figürliche Darstellung ist die Darstellung der Taufe Jesu. Dabei werden der Vater durch eine Hand und der Heilige Geist durch eine Taube dargestellt.

Im Mittelalter wird die Darstellung der gemeinsam thronenden Gestalten des greisen Vaters und des jugendlichen Sohnes üblich. Der Heilige Geist wird wieder als Taube dargestellt.

Mit der aufkommenden Passionsmystik entwickelt sich der Bildtyp des Gnadenstuhls. Der thronende Vater hält das Kreuz mit dem gekreuzigten Sohn, während der Heilige Geist wieder als Taube dargestellt wird. Die frühesten erhaltenen Beispiele sind Gebets-Illustrationen in Messbüchern, das älteste davon im Missale von Cambrai aus dem 12. Jahrhundert. In einer Weiterentwicklung des Bildtyps hält der Vater den vom Kreuz abgenommenen toten Sohn in den Armen.

Außer der Passion wird auch die Geburt Jesu für die Darstellung der Trinität genutzt. Der greise Vater und der Heilige Geist als Taube freuen sich über den als Säugling gezeigten Sohn. Beispiel hierfür ist die Darstellung der Geburt Christi in der Kirche von Laverna aus dem 15. Jahrhundert.

In der Volkskunst entwickeln sich auch Darstellungen der Trinität als eine Figur mit drei Köpfen oder mit einem dreigesichtigen Kopf "(tricephalus)". Diese Darstellung wird von der kirchlichen Autorität als mit dem Glauben unvereinbar abgelehnt, so z. B. durch das Verbot der Darstellung des "Tricephalus" durch Papst Urban VIII. im Jahr 1628.

Eine spezielle Variante ist die Darstellung des heiligen Geistes als femininer Jüngling, wie etwa an einem Deckenfresko der St. Jakobuskirche von Urschalling bei Prien am Chiemsee aus dem 14. Jh. oder in der Wallfahrtskirche Weihenlinden im 18. Jahrhundert, basierend auf Visionsberichten von Maria Crescentia Höss aus Kaufbeuren. Eine derartige Darstellung wurde allerdings dann von Benedikt XIV. mit dem Dekret "Sollicitudine Nostrae" von 1745 verboten.

Das Fest der Heiligen Dreifaltigkeit (Sanctissimæ Trinitatis) wird in der Westkirche zu Trinitatis, dem Sonntag nach Pfingsten, gefeiert. In der Ostkirche gilt das Pfingstfest selbst als Fest der Dreifaltigkeit. Die Sonntage vom Dreifaltigkeitsfest bis zum Ende des Kirchenjahrs – die längste Zeitspanne im Kirchenjahr – werden als in der evangelischen Kirche als Sonntage nach Trinitatis bezeichnet.

Der größte Teil der sich auf die christliche Bibel beziehenden Religionsgemeinschaften folgt dem trinitarischen Dogma. Sowohl die westlichen (römisch-katholische und evangelische) als auch die östlichen (östlich-orthodoxe und orientalisch-orthodoxe) Kirchen haben seit dem Ende des 4. Jahrhunderts durchgehend die Trinitätslehre vertreten.

In der Gegenwart ist die Trinität in der Verfassung des ökumenischen Rats der Kirchen aufgeführt und wird damit von allen ihm angeschlossenen (orthodoxen, anglikanischen, größeren evangelischen) Kirchen anerkannt und ebenso von der römisch-katholischen Kirche. Ebenso gehört das Bekenntnis der Trinität zu den Glaubensgrundlagen der evangelikalen Bewegung.

Für nichttrinitarische christliche Bewegungen siehe Nichttrinitarier.

Der jüdische Glaube kennt keine Trinität. Ihr widerspricht der jüdische Zentralgedanke des "Schma Jisrael", dem Toravers eines "einzigen" und in einer „Form“ vorhandenen Gottes elementar. Der Geist ("hebräisch" ruach) wird als Lebensatem Gottes verstanden. Auch der erwartete Meschiah ("deutsch" Messias) ist in diesem Glauben ein Mensch, möglicherweise mit besonderen Gaben oder Charismen. In der Person Jesu von Nazaret sieht der jüdische Glaube keine Person der Trinität, sondern lediglich einen jüdischen Wanderprediger (wie es zu Lebzeiten Jesu viele gab), der jüdisches Gedankengut verbreitete und wegen Aufrührertums von der römischen Besatzungsmacht hingerichtet wurde. Laut Talmud endet Jesus in der Hölle.

Die klassische islamische Theologie (Ilm al-Kalam) versteht die christliche Trinitätslehre als unvereinbar mit der Einheit Gottes (Tauhīd) und als Spezialfall dessen, dass dem einen Gott ein anderer „beigesellt“ wird (Schirk). Nur vereinzelt weichen heutige islamische Intellektuelle von dieser Einschätzung ab, um beispielsweise zu vertreten, dass der Koran nur ein Missverständnis christlicher Trinität, nämlich einen Dreigötterglauben (Tritheismus), ablehne.

Im Koran wird der christliche Trinitätsgedanke als Dreiheit von Gott, Jesus und Maria aufgefasst. Maria sei demnach Teil der Trinität und werde von den Christen als Gott verehrt.
Die Dreieinigkeit wird im Koran unter anderem an folgenden Stellen explizit abgelehnt:

Dem entgegnen christliche Theologen einerseits, dass diese „sehr körperliche“ Vorstellung der Trinität nicht der Trinität entspreche, wie sie das Christentum versteht, das die absolute Geistigkeit Gottes betont: Der Sohn wird vom Vater gezeugt nicht auf körperliche, sondern auf geistige Weise. Ebenso geht der Heilige Geist – nach westkirchlicher Ansicht aus der Liebe von Vater und Sohn, nach ostkirchlicher Ansicht aus dem Vater – auf geistige Weise hervor. Islamische Theologen machen hingegen darauf aufmerksam, dass diese Frage von untergeordneter Bedeutung sei, da nach islamischem Verständnis bereits die Anrufung etwa Jesu in die Kategorie des "shirk" (im Deutschen oft mit "Vielgötterei" übersetzt) falle. Außerdem wird die Unvereinbarkeit der Offenbarung Gottes in einem Menschen mit dem Bilderverbot in den mosaischen Zehn Geboten angeführt.

Göttliche Triaden (Dreiheiten, d. h. drei verschiedene, zusammengehörende Gottheiten), oft bestehend aus Vater, Mutter und Kind, sind aus den meisten Mythologien bekannt, beispielsweise in der römischen Mythologie Jupiter, Juno und Minerva oder Osiris, Isis, und Horus in der ägyptischen Mythologie.

Es wurden sogar vage „Anfänge von … Trinität“ bis auf ägyptische theologische Überlieferung zurückverfolgt.

Daneben gibt es auch Triaden mit dem Konzept des Modalismus: Eine Gottheit erscheint in verschiedenen (oft auch drei) Gestalten: So wurden vorchristliche Göttinnen im asiatischen, kleinasiatischen und europäischen Raum (wie z. B. die keltische Morrígan oder die Matronen) oft als drei verschiedene Personen abgebildet: als Jungfrau („Liebesgöttin“), als Mutter („Fruchtbarkeitsgöttin“) und als Altes Weib („Todesgöttin“) – jeweils zuständig für den Frühling, den Sommer und den Winter – alles Manifestationen derselben Göttin. Im Neopaganismus wurde daraus eine Dreifaltige Göttin.

Eine Trimurti („Dreigestalt“, „dreiteiliges Götterbild“) ist im Hinduismus die Einheit der drei Aspekte von Gott in seinen Formen als Schöpfer Brahma, als Erhalter Vishnu und Zerstörer Shiva. Diese Dreiheit in der Einheit "(trimurti)"repräsentiert das formlose Brahman und drückt die schöpfenden, erhaltenden und zerstörenden Aspekte des höchsten Seienden aus, die sich gegenseitig bedingen und ergänzen. Ob es sich dabei um „Personen“ im christlichen Sinn handelt, hängt von der Auffassung der jeweiligen theologischen Richtung von „Person“ ab: Bei pantheistischen Richtungen wie der Shankaras erübrigt sich die Frage; Richtungen, die Persönlichkeit betonen wie die Ramanujas oder Madhvas, neigen eher dazu, die drei Aspekte als eine Art „Erzengel“ einer transzendenten Gottheit wie Vishnu oder Shiva unterzuordnen. Speziell im tamilischen Shivaismus wird Shiva als transzendenter Gott gesehen und seine zerstörende Funktion als Rudra bezeichnet. Teilweise zählt man auch Verblendung und Erlösung zu den (jetzt fünf) Hauptaspekten Shivas, die dann aber im Bild des Tanzenden Shiva symbolisch dargestellt werden.

Jedoch ist die Trimurti kein zentrales Konzept des Hinduismus, denn es gibt auch „zweigestaltige“ Bilder, allen voran die verbreitete Darstellung von Shiva als zur Hälfte Mann und zur Hälfte Frau (Ardhanarishvara), das auch sehr häufige Harihara-Bild, das halb Vishnu und halb Shiva ist, und in dem der heute wenig verehrte Brahma fehlt. Eine weitere Gruppe von Göttern, die auch als höhere Einheit aufgefasst werden kann, ist Shiva und Parvati mit ihren Kindern Ganesh und/oder Skanda als Götterfamilie.

Shakti-Verehrer, die Anhänger der weiblich dargestellten Form Gottes, kennen auch eine weibliche Trimurti mit Sarasvati – der Schöpferin, Lakshmi – der Erhaltenden und Kali – der Zerstörerin.

Triadische bzw. trinitarische Formulierungen finden sich auch in Texten von Nag Hammadi.

Der Philosoph Jens Halfwassen schreibt in seinem Buch "Plotin und der Neuplatonismus": „Es gehört zu den merkwürdigsten Ironien der Geschichte, dass ausgerechnet der erklärte Christenfeind Porphyrios mit seinem trinitarischen Gottesbegriff, den er aus der Interpretation der "Chaldäischen Orakel" entwickelte, zum wichtigsten Anreger für die Ausbildung des kirchlichen Trinitätsdogmas im 4. Jahrhundert wurde ... Es war ausgerechnet Porphyrios, der die rechtgläubigen Kirchenväter gelehrt hatte, wie man die wechselseitige Implikation und damit die Gleichwesentlichkeit von drei unterschiedlichen Momenten in Gott mit der Einheit Gottes zusammendenken kann, wodurch die Gottheit Christi erst mit dem biblischen Monotheismus vereinbar wurde.“ Eine Menschwerdung einer der Personen der Trinität war für einen Neuplatoniker wie Porphyrios jedoch unannehmbar.









</doc>
<doc id="14683" url="https://de.wikipedia.org/wiki?curid=14683" title="Messgerät">
Messgerät

Messgeräte (auch "Messinstrumente" genannt) dienen zur Bestimmung geometrischer oder physikalischer Größen. Meistens führen sie im Rahmen einer Messung mittels einer Skalen- oder Ziffernanzeige auf eine quantitative Aussage über die zu messende Größe. Diese Aussage, der Messwert, wird als Produkt von Zahlenwert und Einheit angegeben. Die prinzipiell zugrunde liegenden Messmethoden werden unter Messtechnik angegeben. Allgemeine Merkmale der Messgeräte gemäß DIN 1319-1 werden unter Messmittel aufgeführt. Statt einer ablesbaren Anzeige kann ein Messgerät auch ein Signal, vorzugsweise ein elektrisches Signal, ausgeben; oder es kann Daten speichern, elektronisch oder auf Papier (z. B. als Messschreiber oder Registrierapparat).

Eine "Messeinrichtung" ist in den „Grundlagen der Messtechnik“ in DIN 1319 als „Gesamtheit aller Messgeräte und zusätzlicher Einrichtungen zur Erzielung eines Messergebnisses“ definiert und besteht im einfachsten Fall aus einem einzigen Messgerät.

Messgeräte zur Ausführung von Messungen zählen allgemein zur Gruppe der Messmittel. Werden diese zur Prüfung eingesetzt, werden sie gemäß DIN 1319-2 auch als Prüfmittel bezeichnet.

Das Messgerät kann fehlerhaft arbeiten, bzw. der Messwert kann Messabweichungen enthalten; diese sind herauszurechnen bzw. in ihrer Größe abzuschätzen. Besonders genaue Messgeräte können zur Kalibrierung, Justierung oder Eichung anderer Messgeräte dienen (siehe auch Messmittelüberwachung). Für ermittelte Werte kann eine Messunsicherheit angegeben werden.

"Siehe auch Liste von Messgeräten und Liste physikalischer Größen einschließlich deren Einheiten"

Ohne die grundlegenden Messgeräte zur Bestimmung der Zeit und zur Messung von Längen sowie dem simplen Zählen können keine anderen Messgeräte hergestellt bzw. benutzt werden. Andere Größen, auch Basisgrößen werden abgeleitet oder aber die Messgeräte werden durch Anwendung dieser Größen bestimmt.

Die Zeit wird mit verschiedenen Uhren gemessen:

Prinzipiell unterscheiden wir zwei einfache Formen der Messmittel zur Längenmessung: das "Strichmaß," welches das Maß durch den Abstand zwischen zwei Strichen verkörpert und das "Endmaß" (das Urmeter etwa), bei dem das durch den Abstand zweier Flächen geschieht.

Wegsensoren wie beispielsweise:

Zählen ist das elementarste Messprinzip: Auch bei der Zeit- oder Längenmessung wird oft schlicht gezählt. Mit der Durchdringung der Messtechnik durch digitale Methoden hat das Messprinzip enorm an Bedeutung gewonnen.

Zählen im messtechnischen Sinne ist das Bestimmen der Anzahl (siehe auch "Stückmenge"). Zählwerke messen die Anzahl von Objekten oder Ereignissen, bei befristeter Zählung bestimmen sie deren Häufigkeit:

Messgeräte der Flächeninhaltsmessung "(Planimetrierung)"


Die Bestimmung sowohl des "Hohlvolumens" als auch des "Volumens fester Körper, von Flüssigkeiten oder Gasen" wird historisch durch Hohlkörper oder skalierte Messgefäße realisiert, meist aber über Volumenberechnung.


Geodäsie: Alle Geräte für die Winkelmessung im Gelände sind auch (unterschiedlich gut) für die Standortbestimmung geeignet. Hierzu werden Landkarten oder Koordinaten benötigt. Durch Winkelmessung und Strahlensatz lassen sich Höhe oder Höhendifferenz von Objekten berechnen.


Die Gewichtsmessung ist ein Fachgebiet der "Massenmesstechnik". Während früher Waagen vor allem durch den geschickten Aufbau der mechanischen Elemente wie Hebel, Gewichtsstücke oder/und Federn bestimmt wurden, ist die Wägetechnik heute durch die Elektronik geprägt.


Die Temperaturmessung wird in einem extra Artikel ausführlicher behandelt, diese Aufzählung gilt nur als Überblick.
Thermometrie ist die Wissenschaft von der Temperaturmessung – Messung durch Thermometer in verschiedenen Ausführungen.

"Frühe Thermometer"

"Moderne Thermometer"




Abgeleitete Messgeräte sind aus den ursprünglichen Messgeräten entstanden.

Die Geschwindigkeit ist der Quotient aus Weg und Zeit.



Die Beschleunigung ist die Geschwindigkeitsänderung pro Zeitspanne.




Alle Messgeräte zur Längenbestimmung und der Dichte, des Gewichts und Härtemessgeräte sowie Röntgengeräte können ebenfalls bei Feststoffen eingesetzt werden.


Die folgenden Messgeräte werden in der Meteorologie und natürlich aber auch in anderen technischen Bereichen eingesetzt.



Schallpegelmessgeräte messen in den meisten Fällen den Schalldruckpegel. Zu diesem Zweck enthalten sie ein präzises Mikrofon, eine hochgenaue Verstärkerschaltung und eine logarithmische Anzeige. Der Schalldruckpegel wird aus allen Richtungen gleich gut empfangen, weshalb Position und Orientierung des Geräts keine Rolle spielen.
Die Messgeräte werden in den meisten Fällen zur Bestimmung von Lärmbelastungen am Arbeitsplatz und im Straßenverkehr verwendet. Ein weiterer Einsatzzweck ist die Bestimmung von Schwingungen und Laufgeräuschen an technischen Geräten und der Untersuchung von Gegenmaßnahmen auf ihre Wirksamkeit.

Schallpegelmessgeräte müssen vom Anwender regelmäßig kalibriert werden, um die Funktion zu überprüfen und um geänderte atmosphärische Bedingungen wie Temperatur, Luftfeuchte und Luftdruck zu kompensieren.



Diese Geräte sind nicht für einen speziellen Anwendungsfall entwickelt:


Messgeräte-Hersteller sollen Angaben zu den Fehlergrenzen (maximale Beträge der Messgeräteabweichung des Anzeigewertes vom richtigen Wert) machen.

Bei elektrischen Messgeräten mit "Skalenanzeige" (z. B. Analogmultimeter), werden diese Grenzen vorzugsweise in % v.E. (Prozent vom Endwert) angegeben, häufig mittels eines Klassenzeichens. Damit ist die maximale "absolute" Messabweichung gemeint; sie wird über den Messbereichsendwert berechnet. Ein Messgerät mit einem Messbereichsendwert von z. B. 100 V und einer Klasse 1,5 kann selbst im günstigsten Fall bis zu 1,5 % ∙ 100 V = 1,5 V in seiner Anzeige vom richtigen Wert abweichen. Diese Angabe gilt im gesamten Messbereich unabhängig vom Messwert.

Zu Messgeräten mit "Ziffernanzeige" siehe Messgeräteabweichung, auch Digitalmultimeter.

Die "relative" Fehlergrenze eines Messwertes ist definiert als "absolute" Fehlergrenze geteilt durch den richtigen Wert; sie wird umso größer, je kleiner der Messwert ist. Bei umschaltbaren Messgeräten soll deshalb immer "der" Messbereich gewählt werden, mit dem man den größtmöglichen Ausschlag erhält.

Weitere Messabweichungen, etwa verursacht durch Eigenverbrauch oder durch nicht sinusförmigen Verlauf bei Wechselgrößen, lassen sich mit den genannten Angaben nicht erfassen und müssen getrennt bestimmt werden.


Diese Geräte werden in den meisten Fällen zur Herstellung eines Produktes verwendet oder dienen beispielsweise in der Werkstoffprüfung der Qualitätssicherung der Produkte beziehungsweise der Abrechnung von Leistungen.



Bei der Härtemessung führt eine definierte Krafteinwirkung zu einer bleibenden Verformung des Testkörpers oder einem Eindringen einer Prüfgeometrie in den Testkörper. Die Messgeräte werden nach dem angewandten Verfahren bezeichnet. Beispiel: Brinell-Messgerät.


Die verschiedenen Verfahren sind je nach Art und Härte des zu prüfenden Werkstoffs unterschiedlich gut geeignet.

"Norm-Messgeräte" – sind Messgeräte die eine Reihe von "in einer Norm festgelegten" Messungen durchführen. Diese werden meist auch protokolliert um eine Nachweisführung bei Gutachten zu ermöglichen.

Die Bezeichnung der Messgeräte geschieht nach der Norm.

Beispiel: VDE113 (EN60204) mit 10 A – Schutzleiterprüfung, Hochspannungsprüfung, Widerstandsmessung und Grenzbereicherkennung, Isolationsprüfung

Wichtige Norm-Messgeräte:


Diese Norm-Messgeräte untersuchen die korrekte Ausführung der Kabelanschlüsse (Verbindung zwischen Stecker und Kabel) und/oder die Physik der Datentechnik, also Pegel des Signals und Störungen. Im industriellen Bereich werden diese Geräte vor allem für Feldbusse oder Ethernet verwendet. Neben den "Testern," also Geräten, die die Physik untersuchen, gibt es noch "Protokoll-Analyse-Geräte," die den Dateninhalt untersuchen. Die Aufzählung gibt nur exemplarisch einige typische Geräte wieder.



Ist aufgrund seiner Verbreitung das System für das es die größte Anzahl von Analyseprogrammen gibt. Hier eine kleine Auswahl ohne Wertung …


"Normale" sind Maßverkörperungen, Messgeräte, Referenzmaterialien oder Messeinrichtungen, die den Zweck haben, eine Einheit oder einen oder mehrere Größenwerte festzulegen, zu verkörpern, zu bewahren oder zu reproduzieren, um diese an andere Messgeräte durch Vergleich weiterzugeben. Routinemäßig eingesetzte Normale heißen "Gebrauchsnormale". "Bezugsnormale" werden dagegen nur zur gelegentlichen Kalibrierung der Gebrauchsnormale eingesetzt, ggf. auch über weitere dazwischenliegende Normale, die dann "Normale höherer (zweiter, dritter) Ordnung" heißen. Dadurch wird die Belastung der höherwertigen Normale minimiert. Auch die Bezugsnormale werden über eine weitere Kalibrierhierarchie auf ein "Primärnormal," das den höchsten metrologischen Anforderungen entspricht, zurückgeführt. Dabei handelt es sich in der Regel um ein von einem nationalen metrologischen Institut unterhaltenes nationales Normal oder um ein internationales Normal. Innerhalb der Kalibrierhierarchie nimmt die Genauigkeit der Normale nach oben hin stetig zu.

"Prüfstände" dienen zur Fehlerkontrolle zur Qualitätssicherung oder Eichung von Messgeräten (beispielsweise für Wasserzähler).

Messgeräte, deren Messergebnis zur Berechnung von gewerblichen Leistungen verwendet wird (beispielsweise Waagen im Handel, Wasserzähler), müssen eichgesetzliche Auflagen erfüllen. Das heißt, ihre Bauart muss von der Physikalisch-Technische Bundesanstalt (PTB) zugelassen und die Geräte müssen geeicht sein, wobei eine Eichung nach einer bestimmten Zeit durch staatlich anerkannte Prüfstellen mit einem von der Eichbehörde zugelassenen Normal aufgefrischt werden muss. Das Eichgesetz definiert Eichfehlergrenzen, die für verschiedene Lastbereiche nicht überschritten werden dürfen.

Beispiele: Waagen, Wasserzähler, Gaszähler, Stromzähler, Wärmezähler, Kraftstoffzähler, Durchflussmesser

Beispiel für ein ausnahmsweise erlaubtes nicht eichpflichtiges Messgerät: Heizkostenverteiler

Im April 2004 wurde die Europäische Messgeräterichtlinie (MID) veröffentlicht, deren Umsetzung in nationales Recht bis zum 20. Oktober 2006 stattfinden muss.

Das Messprinzip von eigensicheren oder explosionsgeschützten Messgeräten ist den o. g. gleich, jedoch müssen diese Geräte besonderen Ansprüchen für ihren Einsatzfall genügen, die sie z. B. im untertägigen Bergbau oder der (chemischen) Industrie finden.
Richtlinien wie die 94/9/EG bzw. ATEX bestimmen die Anforderungen, die hinsichtlich elektrischer, mechanischer und auch werkstofftechnischer Vorgaben geprüft werden.
Zugelassene Prüfstellen erteilen bei erfolgreicher Zulassung ein Zertifikat, welches Grundvoraussetzung für die Inbetriebnahme von Messgeräten in den besonderen explosionsgefährdeten Bereichen ist.

Für Messgeräte in der Medizin gelten besondere Regeln. Sie müssen die Vorschriften der MedGV, der Medizin-Geräte-Verordnung einhalten. Dies gilt aber nur für Messgeräte die a) als Medizinische Geräte eingestuft und b) in der anerkannten Medizin verwendet werden. Der Bereich der alternativen Medizin bleibt davon unberührt. So fallen das "Teslameter," ein "Biofeld-Messgerät" oder die "Körperfettwaage" nicht unter die Bestimmungen.


Technische Hilfsmittel für Messungen in der Medizin sind eigentlich keine Messgeräte, werden aber für Messungen verwendet:







</doc>
<doc id="14689" url="https://de.wikipedia.org/wiki?curid=14689" title="Klavier">
Klavier

Klavier (von lt. "clavis" „Schlüssel“, im übertragenen Sinne auch „Taste“) bezeichnet heute das moderne, weiterentwickelte Hammerklavier, also ein Musikinstrument, bei dem auf Tastendruck über eine spezielle Mechanik Hämmerchen gegen Saiten geschlagen werden. Die ebenfalls übliche Bezeichnung Pianoforte oder verkürzt Piano entstand, weil das Hammerklavier erstmals die Möglichkeit bot, die Lautstärke jederzeit stufenlos zwischen leise (piano) und laut (forte) zu verändern. Die heutigen Hauptformen des Klaviers sind der Flügel (englisch "grand piano") und das Pianino (englisch "upright piano"). Letzteres wird umgangssprachlich oft einfach mit „Klavier“ gleichgesetzt.

Historisch bezeichnete „Klavier“, bis ins 19. Jahrhundert in der Schreibung "Clavier" oder "Clavir", allgemein irgendein Tasteninstrument, gelegentlich auch nur eine Klaviatur, also einen Teil eines Instruments.
Das heutige Klavier ist hinsichtlich der Bedienung ein Tasteninstrument, hinsichtlich der Erregungsart ein Schlaginstrument und hinsichtlich des schwingenden Mediums ein Saiteninstrument.

Das Wort "clavis" (lateinisch für „Schlüssel“) stand in der mittelalterlichen Musiktheorie für eine mit einem Buchstaben bezeichnete Tonstufe. Weil Tonbuchstaben manchmal direkt auf die Tasten der Orgel geschrieben wurden, konnte die Bezeichnung "clavis" auf die Taste selbst übergehen. In notierter Musik wurden Tonbuchstaben vor die Liniensysteme geschrieben, wodurch die Bezeichnung auch auf den Notenschlüssel überging. Im englischen Wort "key" hat sich die mehrfache Bedeutung Schlüssel/Tonstufe/Taste/Notenschlüssel bis heute erhalten.

Für die Gesamtheit aller "claves" („Tasten“) wurde über französisch "clavier" [] „Tastatur; Klaviatur“ das deutsche Wort "Clavier" gebräuchlich. Bis Ende des 18. Jahrhunderts fasste man alle Tasteninstrumente unabhängig von der Art der Klangerzeugung, also auch die Orgeln ("Windclaviere"), unter diesem Namen zusammen (Sebastian Virdung, 1511; Jakob Adlung, 1758).

1619 nannte Michael Praetorius jedes über eine Tastatur zum Klingen gebrachte Saiteninstrument "clavicordium" – sowohl die Tangentenklaviere (vor allem die Clavichorde im engeren Sinn) als auch die Zupfklaviere (Cembali, Virginale und Spinette). In seinem Lehrwerk "Versuch über die wahre Art das Clavier zu spielen" (1753) bezeichnete Carl Philipp Emanuel Bach Spieler aller besaiteten Tasteninstrumente einschließlich des noch recht jungen Hammerklaviers als „Clavieristen“. Das Cembalo hieß bei ihm „Flügel“, das Clavichord „Clavicord“ und das Pianoforte „Forte piano“. Im 19. Jahrhundert setzte sich das Wort „Klavier“ als Bezeichnung für Tasteninstrumente mit Hammermechanik allgemein durch.

1960 empfahl der Musikhistoriker Friedrich Wilhelm Riedel die Rückübertragung des Begriffs „Clavier“ in dieser Schreibweise auf alle Tasteninstrumente, weil in Alter Musik die Wahl des Tasteninstruments häufig offen gelassen wurde.

Der ebenfalls übliche Name „Piano“ ist die Kurzform von „Pianoforte“ (von italienisch "piano" [] „leise“ und "forte" [] „laut“). Er bezieht sich darauf, dass auf Hammerklavieren – anders als auf älteren Tasteninstrumenten – durch unterschiedlich starkes Anschlagen der Tasten große Unterschiede der Lautstärke (siehe Dynamik (Musik)) erreichbar sind.

Oft wird mit dem Begriff "Klavier" einengend nur das Pianino (ital. „kleines Piano“, vertikale Besaitung) bezeichnet, im Gegensatz zum Flügel (horizontale Besaitung). Seit der Erfindung von Tasteninstrumenten mit elektrischer, elektronischer oder digitaler Klangerzeugung (Digitalpianos) wird er zudem meist für Instrumente akustisch-mechanischer Bauweise reserviert, während das Wort "Piano" auch die Digitalpianos, die Klang und Anschlaggefühl des akustisch-mechanischen Instrumentes wirklichkeitsnah zu simulieren versuchen, umfasst.

Besaitete Tasteninstrumente werden historisch auf das Monochord zurückgeführt. Mehrere Monochorde entwickelten sich zur beidhändig gespielten Floß- oder Röhren-Zither weiter. Daraus entstanden in der Antike einerseits mit Tasten gespielte Orgeln, andererseits verschiedene gezupfte, geschlagene oder gestrichene Saiteninstrumente, darunter das Psalterium.

Das Organistrum aus dem 12. Jahrhundert – eine Drehleier mit durch Tangententasten veränderbaren Saitenlängen – gilt als Zwischenglied der Entstehung besaiteter Tasteninstrumente. 1397 erwähnt ein Jurist in Padua erstmals ein mit Tasten bedientes Psalterium. 1404 erwähnten die Minneregeln des Eberhard von Cersne erstmals ein "clavicordium" und "clavicymbolum". 1425 erschien ein solches Instrument auf einem Altarbild in Minden, 1440 beschrieb Arnaut Henri de Zwolle diese neue Instrumentengattung in einem Traktat, darunter auch ein mit einer Hammermechanik bedientes, dem Hackbrett verwandtes "Dulce melos".

Durch Hinzufügen einer Tastatur entwickelten sich im Spätmittelalter aus dem Monochord und dem Psalterium das Clavichord (fest mit der Taste verbundene Tangenten schlagen die Saiten an) und in der Renaissance das Cembalo sowie deren Varianten Clavicytherium, Spinett und Virginal, bei denen der Ton durch Anreißen der Saiten mit einem Kiel erzeugt wird.

Die Flügelform des Cembalos wurde schließlich zum Vorbild für die ersten Klaviere.

Gegen Ende des 17. und Anfang des 18. Jahrhunderts wurde viel experimentiert, um ein Tasteninstrument zu konstruieren, das eine dynamische Spielweise (leise, laut und feine Abstufungen) durch unterschiedlich starken Anschlag der Tasten ermöglichte. Der erste, dem dies gelang, war Bartolomeo Cristofori, ein italienischer Instrumentenbauer aus Padua, der spätestens seit 1690 am Hofe Ferdinando de’ Medicis in Florenz als Hofcembalobauer und Kustos der Musikinstrumente-Sammlung angestellt war. Das Inventar der Musikinstrumente aus dem Jahre 1700 listet ein „"arpicembalo che fà il piano e il forte"“ (Cembalo, das laut und leise spielen kann) auf, das üblicherweise auf das Jahr 1698 datiert wird und als erstes Hammerklavier gelten kann. Vermutlich baute Cristofori in den Werkstätten im Erdgeschoss der Uffizien bereits 1694 einen Prototyp. Nach einem Treffen mit Cristofori veröffentlichte der römische Literat und Journalist Scipione Maffei im Jahre 1711 einen Artikel im "Giornale dei letterati d’Italia" über ein um 1709 von Cristofori gebautes Instrument, das „"gravicembalo col piano e forte"“ (Cembalo mit (Befähigung zu) Leise und Laut) genannt wurde. Dieser Artikel enthielt eine Skizze der besonderen Spielmechanik und eine detaillierte Beschreibung der Mechanik, mittels dessen Übersetzung ins Deutsche später der Orgelbauer Gottfried Silbermann 1726 seinen ersten Hammerflügel konstruierte.

Cristoforis Instrumente waren bereits erstaunlich ausgereift. Die Mechanik verfügt über einen Mechanismus, bei dem der Hammer mittels einer Stoßzunge und Übersetzungshebel gegen die Saite geschleudert wird (Stoßmechanik mit Treiber, d. h. übersetzendem Zwischenhebel); eine sogenannte Auslösung (Auskopplung des Hammers von der Tastenbewegung kurz vor dem Anschlag) verhindert ein Festdrücken des Hammers und ungewolltes Bedämpfen an den Saiten. Per Ton separierte Dämpfer verhindern das Weiterklingen der im Vergleich zum Cembalo kräftigeren Saiten nach dem Loslassen der Taste. Cristofori verwendete bereits Doppelsaiten (zwei Saiten pro Ton), um das Klangvolumen zu vergrößern, sowie seit 1722 den "una corda"-Mechanismus; die Instrumente umfassten vier Oktaven (heutige meistens 7/, s. o. unter "Klaviatur"). Das Instrumentengehäuse hatte er für die deutlich höheren Zugkräfte des Hammerklaviers gründlich verstärkt.

Trotz ihrer ausgezeichneten Qualität fanden die ersten Hammerklaviere in Italien keine große Resonanz, wohl wegen ihres zum Cembalo vergleichsweise hohen Fertigungsaufwandes und anfangs auch schwachen Tones, weshalb Cristofori 1726 aufhörte, Hammerflügel zu bauen. Er widmete sich bis zu seinem Lebensende wieder allein dem Cembalobau. Insgesamt fertigte er knapp 20 Hammerflügel an, von denen heute noch drei erhalten sind. Das älteste bekannte Exemplar von 1720 steht im Metropolitan Museum of Art in New York, eines aus dem Jahre 1722 im Musikinstrumentenmuseum in Rom und eines aus dem Jahre 1726 in der Instrumentensammlung der Universität Leipzig.

Zwei Schüler und Gesellen Cristoforis, Domenico del Mela (1683 bis ca. 1760) und Giovanni Ferrini (ca. 1699 bis 1758), bauten noch einige Instrumente mit Hammermechanik, die v. a. auf der iberischen Halbinsel Beliebtheit erlangten und an den Königshöfen Spaniens und Portugals eine eigene Tradition begründeten. Im Jahre 1732 hatte Lodovico Giustini in Florenz die erste speziell fürs Hammerklavier geschriebene Musik komponiert, die Anweisungen zum Lauterwerden (Crescendo) und Leiserwerden (Decrescendo) enthielt und anlässlich eines diplomatischen Besuches des portugiesischen Kronprinzen am florentinischen Hof der Medici gespielt wurde. Der Prinz machte den Christofori-Lehrlingen Angebote, in Portugal unter seiner Sponsorenschaft weiterzuarbeiten, die sie annahmen; sie begleiteten ihn auf dem Rückweg nach Portugal. Hieraus entstand die portugiesische und spanische Klavierbautradition.

In Italien hingegen endete nach Ferrinis Tod für viele Dekaden die Klavierbautradition.

Einige unabhängige Erfindungen in Frankreich, Cuisinés "Clavier" (1708) und Jean Marius' "Clavecin à maillets" (1716), beide vermutlich inspiriert durch Hebenstreits "Pantaleon", schienen auf Grund technischer Schwierigkeiten nicht über den Status von Kuriositäten hinauszugehen. Der Funke sprang hingegen auf Deutschland über, das für die folgenden Jahrzehnte, zusammen mit England, maßgeblich zur Entwicklung des modernen Klaviers beitragen sollte. Der deutsche Clavichord-Lehrer Gottlieb Schröter erfand etwa um 1717 zwei Hammermechaniken für Cembali, die er allerdings aus finanziellen Gründen nicht weiterentwickeln konnte. Trotzdem galt er lange Zeit als Erfinder des Klaviers. Einer der bedeutendsten Orgelbauer der Barockzeit, Gottfried Silbermann, lernte im Jahre 1717 einen Hammerflügel aus Cristoforis Werkstatt kennen. Das Instrument gelangte im Tross von Musikern nach Dresden. Diese waren einer Einladung gefolgt, am kurfürstlichen Hof drei neue Opern Antonio Lottis uraufzuführen. Zusammen mit Johann Ulrich von König konnte er das Instrument untersuchen und König übersetzte Maffeis Beschreibung der Mechanik ins Deutsche. Silbermann hatte das nötige Know-how sowie die finanziellen Mittel um ein eigenes Modell, basierend auf Cristoforis Mechanik, zu entwickeln, das er im Jahre 1726 präsentieren konnte. Er baute in der Folge ein weiteres Hammerklavier. „"Eins davon hatte der sel. Kapelm. Hr. Joh. Sebastian Bach gesehen und bespielet. Er hatte den Klang desselben gerühmet, ja bewundert: Aber dabey getadelt, daß es in der Höhe zu schwach lautete, und gar zu schwer zu spielen sey. Dieses hatte Hr. Silbermann, der gar keinen Tadel an seinen Ausarbeitungen leiden konnte, höchst übel aufgenommen. Er zürnte deswegen lange mit dem Hrn. Bach."“
Trotzdem arbeitete Silbermann fast zehn Jahre lang an der Verbesserung seiner Instrumente und erntete schließlich Bachs Anerkennung. Nach dem Regierungsantritt König Friedrichs II. von Preußen konnte der Freiberger Instrumentenbauer 15 Instrumente an den Hof nach Potsdam liefern. 1747 improvisierte dann Johann Sebastian Bach vor dem König auf einem dieser Hammerflügel sein dreistimmiges Ricercare. Dieses heute im Neuen Palais Potsdam aufbewahrte Instrument wird von der Firma Neupert nachgebaut.

Zu dieser Zeit verfügte das Hammerklavier offenbar bereits über einen guten Ruf. Es war das universellste Tasteninstrument und ein exzellentes Klangwerkzeug für einen professionellen Musiker.
Silbermanns "Piano Fort" genannte Hammerklaviere verfügten über eine Prellmechanik. Neu kam eine Dämpfungsaufhebung mit Handhebeln dazu, die seither (heute über die Bedienung durch das Forte-Pedal) zur Grundausstattung eines jeden Klaviers gehört.

Zahlreiche Schüler Silbermanns führten seine Arbeit fort und entwickelten sie weiter. Als besonders innovativ erwies sich Christian Ernst Friederici. Er baute als Erster ein Tafelklavier und experimentierte viel mit aufrecht stehenden Instrumenten; berühmt und eindrucksvoll sind seine Pyramidenflügel. Zwölf von Silbermanns Studenten (deshalb auch „die zwölf Apostel“ genannt) flohen in den Wirren des Siebenjährigen Krieges nach England, wo sie die englische Klavierbautradition begründeten.

Der Orgelbauer Johann Andreas Stein erlernte beim elsässischen Zweig der Silbermann-Familie in Straßburg sein Handwerk. Er gründete 1750 in Augsburg seine eigene Werkstatt und begann, eigene Hammerklaviere zu entwickeln. Er nahm entscheidende Veränderungen vor, die den Klavierbau der folgenden Dekaden nachhaltig prägten. Er verbesserte Silbermanns Prellmechanik, indem er eine Auslösung einbaute, wodurch sie leichter spielbar wurde. Diese Prellzungenmechanik entstand um 1781 und wurde als "Deutsche Mechanik" bekannt. Die Gehäuse seiner Instrumente waren viel robuster gebaut und vielfach verstrebt. Der Resonanzboden war kräftiger dimensioniert und unter Spannung durchgehend berippt. All diese Neuerungen verliehen Steins Hammerklavieren einen neuen Klangcharakter. Sie waren heller, durchdringender und präsenter. Die neue Ausdruckskraft stieß bei Komponisten und Musikern auf Begeisterung und schuf damit die Grundlage für das Klavier als Soloinstrument.

Steins Nachkommen führten das Geschäft weiter, seine Kinder Andreas und Nanette zogen 1794 nach Wien. Nach weiteren Verbesserungen wurde Steins Mechanik unter dem Namen "Wiener Mechanik" bekannt und von zahlreichen Klavierbauern adaptiert. Insbesondere bewirkte der "Fänger", ein mit Lederauflage versehener Klemmklotz an der Taste, eine große Verbesserung der Spielmechanik. Er verhindert, dass der von den Saiten herabfallende Hammer zurückprellen kann und einen ungewollt doppelten Ton erzeugt.

Wien war damals neben London eine Weltmetropole der Musik und ein idealer Nährboden für Künstler und Erfinder. Über 100 Instrumentenbauer waren zeitweise in Wien aktiv, höchst angesehen die Geschwister Stein sowie Joseph Brodmann, Conrad Graf und Anton Walter.

Im Gegensatz zu Johann Andreas Stein, der Silbermanns Prellmechanik weiterentwickelte, griffen die englischen Klavierbauer, darunter viele Silbermann-Schüler, die in den Wirren des Siebenjährigen Krieges nach England ausgewandert waren, direkt auf Cristoforis Stoßmechanik zurück. Aus praktischen und finanziellen Gründen fertigte Johann Christoph Zumpe etwa zwischen 1760 und 1762 sein erstes Tafelklavier an. Es war ein kostengünstig herstellbares Instrument mit einer einfachen Mechanik und wenig Ausschmückungen. Doch das Tafelklavier entwickelte sich zum echten Renner in London. Es wurde Mode, eines zu besitzen, so dass Zumpe „"sie nicht schnell genug produzieren konnte, um das Verlangen des Publikums zu befriedigen"“. Nun begannen zahlreiche andere Londoner Klavierbauer ebenso, Tafelklaviere zu bauen. Der im Vergleich zum Hammerklavier und zum Cembalo verhältnismäßig günstige Preis erlaubte es auch dem Bürgertum, ein Instrument zu erwerben. Der kommerzielle Erfolg des Tafelklaviers in England legte die Basis dafür, dass sich das Klavier schließlich zu einem der beliebtesten und weitest verbreiteten Instrumente des europäischen Bürgertums entwickelte.

Auch Americus Backers entwickelte um etwa 1772 eine neue Stoßzungenmechanik. Nach Verbesserungen durch Robert Stodart und John Broadwood wurde diese als "Englische Mechanik" bekannt. John Broadwood, schottischer Vorarbeiter, dann Schwiegersohn des nach London emigrierten Schweizers Burkhard Tschudi, war vermutlich einer der ersten, der wissenschaftliche Methoden anwandte, um Mechanik und Klang zu verbessern. Er ermittelte die optimale Position, an der der Hammer die Saite anschlagen sollte, damit diese voller tönt. Seither werden Klaviersaiten ca. bei einem Siebtel bis Neuntel ihrer klingenden Länge angeschlagen, eine ungerade Teilzahl, um Oberschwingungen und eine Klanganreicherung zu erzielen. Broadwood überbrückte erstmals den die Struktur des Flügels schwächenden Hammerschacht mit einer stählernen Klammer, Anbeginn der Entwicklung innerer Verstrebungen der Flügel. Die Hammerschachtbrückenklammer ermöglichte ihm, den Tonumfang der Klaviatur um eine Oktave zu erweitern. Die Mehrung und Qualitätsverbesserung innerer Abstützungen bewirkte dann binnen weniger Jahrzehnte die Verbreiterung des Tonumfangs auf die heute gebräuchlichen 88 Tasten. Broadwoods Erfindungen waren äußerst erfolgreich. Gegen Ende des 18. Jahrhunderts fertigte er rund 400 Pianos pro Jahr, deutlich mehr als jeder andere Hersteller. Broadwood wurde in den ersten Jahrzehnten des 19. Jahrhunderts mit seiner Manufaktur zum größten Klavierbauer der Welt.

Zu Beginn des 19. Jahrhunderts waren zwei Flügelmechaniken vorherrschend: Die auf Johann Andreas Stein zurückgehende Wiener Mechanik (Prellzungenmechanik) und die von Backers, Stodart und Broadwood entwickelte Englische Mechanik (Stoßzungen­mechanik). Die mit Wiener Mechanik ausgestatteten Instrumente waren graziler in der Bauart. Der Klang war dünner und süßer. Doch die Musiker und Komponisten der aufkommenden Romantik verlangten nach mehr Kraft, Lautstärke, größerem Tonumfang und mehr Ausdrucks­möglich­keiten, so dass sich die Englische Mechanik mehr und mehr durchsetzte. Um das Klangvolumen weiter zu verstärken waren etliche Anpassungen nötig. Mehr Klang erfordert größere und schwerere Hämmer. Dies war konstruktions­bedingt mit der englischen Stoßzungen­mechanik besser zu realisieren. Zwischen 1750 und 1850 wuchs die Klaviatur von rund fünf auf sieben­einhalb Oktaven an. Der Trend zu größerer Lautstärke und größerem Tonumfang verlangte mehr und dickere Saiten, deren enorme Zugkraft aufgefangen werden musste. Der Weg führte über zusätzliche Verstrebungen und Eisenspreizen (ab 1799) schließlich zum eisernen Gussrahmen. Erste Patente dazu stammen von Broadwood (1827), Chickering (1843) und die heute übliche Form von Steinway & Sons (1859). Ab 1824 wurden Klavier­saiten aus stärker belastbarem Gussstahl hergestellt. Der 1830 erfundene kreuzsaitige Bezug erlaubte die Anordnung der Saiten in zwei diagonal übereinander verlaufenden Gruppen. Dies brachte Vorteile für die Statik des Instruments und ermöglichte längere Saiten auch in kürzeren bzw. niedrigeren Instrumenten.

Eine Innovation von Johann Heinrich Pape (1789–1875) im Jahre 1826 sollte tiefgreifende Auswirkungen auf den Klavierklang haben und diesen grundlegend verändern. Er umwickelte die Hammerköpfe nicht wie bisher üblich mit Leder, sondern mit einem Filzbelag. Filz kann bei richtiger Behandlung widerstandsfähiger als Leder sein und lässt sich auch besser bearbeiten. In der Maximal­ausprägung des Hammerbaues nach den Entwicklungen von Henri Herz in Paris hatten die Flügel von Herz, Erard und Pleyel in Paris zur Zeit Chopins bis zu neun Lagen, innen am Holzkern begonnen mit zwei Lagen Hirschleder, mehrere unterschiedlich dichte Lagen Filz und Wolle bis hin zu Kaninchenfell außen als weichstem Werkstoff. Hämmer dieser extrem aufwendigen Art erlaubten Kundigen eine Reichhaltigkeit und Farbigkeit des Klavier­klanges zu erzeugen, die mit der Entwicklung zu noch größeren Konzertsälen und zu höherer Lautstärke, erzielt mit dichtem ein- oder zweilagigem Filz, teils wieder verlorenging. Die Aufbringung des Filzes auf den Hammer ist ein delikater Prozess. Bei vielen Hammerherstellern ist die genaue Vorgehensweise ein gut gehütetes Geheimnis. Die Intonierung eines Klavieres, die durch Auflockern und teils Härten des Filzes erzielte Detail­veränderung des Klanges eines Einzeltones zur Angleichung innerhalb des gesamten Tonumfanges, ist seither die höchste Kunst der Klavierbauer.

Eine bahnbrechende Erfindung im Klavierbau stammt vom Franzosen Sébastien Érard. Er entwickelte auf der Basis der Englischen Mechanik eine Repetitionsmechanik, die er 1821 patentieren ließ. Sie erlaubt mittels eines gefederten Repetierschenkels auf Höhe des auskoppelnden Stößels das Repetieren eines Tones, ohne die Taste ganz loslassen zu müssen. Der Repetierschenkel Érards ermöglicht seither im Flügel eine rasche Anschlagfolge für ein virtuoses, schnelles Spiel. Nach Verfeinerungen von Henri Herz, etwa in den Jahren 1840 bis 1850, entstand die Flügelmechanik der sogenannten "doppelten Auslösung", die bis heute praktisch unverändert blieb.

Die Dämpfungsaufhebung erfolgte bei einfachen Instrumenten über einen Handzug, den Pantaleonzug oder Fortezug, im „Mozartflügel“ über gut funktionierende Kniehebel, dann aber zunehmend über Pedale; neben der Dämpferaufhebung waren ein Moderator (Filztuchstreifen) und zunehmend die Verschiebung üblich, aber auch Fagottzug (gegen die Saiten gedrückte Pergamentrolle), Harfenzug (Bürsten- oder Tuchfransenleiste), Lautenzug (mit Leder bespannte Leiste), Janitscharen<nowiki>zug</nowiki> (Schlagwerk mit Pauke, Glocken bzw. Schellen) etc. Diese noch vom Cembalobau stammenden Modifikationen des Saitenklanges gingen jedoch nach 1830 drastisch zurück. Es verblieben am Klavier zunächst zwei Pedale, die Dämpfungs­aufhebung („forte“) und die seitliche Verschiebung des Hammeranschlags („una chorda“).

Das Hammerklavier erlebte in der ersten Hälfte des 19. Jahrhunderts eine Blütezeit und war nicht mehr aus der Gesellschaft wegzudenken. Das Klavier war den Fürstensalons entwachsen, es wurde in Form des großen Konzertflügels integraler Bestandteil des Konzertwesens großer Städte und in der Form von Tafelklavieren, beginnenden Hochklavieren und teils Flügeln auch der gutbürgerlichen Wohnung.

Schon von Anfang an wurden auch aufrecht stehende Flügel gebaut, so bereits vom Cristofori-Schüler Domenico del Mela und vom Silbermann-Schüler Christian Ernst Friederici (1745). Diese Instrumente hatten oft eindrückliche Formen, die mit Namen wie Giraffenklavier, Harfenklavier, Lyraflügel, Pyramidenklavier oder Schrankklavier belegt wurden; sie waren meist sehr hoch, sehr exklusiv und hatten nicht viel gemeinsam mit den heutigen Pianinos.

Die ersten kleinen Pianinos entstanden um 1800 unabhängig von Matthias Müller in Wien und John Isaac Hawkins in Philadelphia.
Technisch und kommerziell erfolgreich wurde Robert Wornum, der um 1811 ein "Cottage Piano" baute, das sich bis 1826 zum "Piccolo Piano" entwickelte und zum Vorbild für alle späteren Pianinos werden sollte. Seine Mechanik ist eine Stößelmechanik mit Auslösung; sie beruht auf den Prinzipien der englischen Mechanik von Flügeln und wandelt diese mittels des Hammer-Drehgelenks ab, der sogenannten Hammernuss. Er entwickelte sie in den 1830er Jahren weiter. Diese Mechanik wurde in Paris von Pleyel und Pape weiterentwickelt und kommerziell erfolgreich gemacht, weshalb sie auch als "Französische Mechanik" bekannt wurde. Sie entspricht im Wesentlichen schon der heutigen Klaviermechanik. Die Bauweise der Pianinos löste die material- und platzaufwendigeren und klanglich benachteiligten Tafelklaviere in Europa bereits um ca. 1850, in den USA bis ca. 1900 ab.

Um die Mitte des 19. Jahrhunderts waren die meisten Elemente des modernen Klaviers, sowohl beim Flügel als auch beim Pianino, entwickelt. Was folgen sollte, waren einige wenige Neuerungen, v. a. die Kreuzbesaitung beim Flügel, besonders aber kontinuierliche Verfeinerungen und Verbesserungen bei Mechanik, Konstruktion und Herstellungsverfahren. Charakteristisch für die zweite Hälfte des 19. Jahrhunderts ist eine nie zuvor da gewesene Intensivierung der Produktion. 1850 wurden in Europa rund 33.000 Klaviere gefertigt, 1910 waren es bereits 215.000 Stück. Die starke Zunahme dürfte zum einen mit der stetig steigenden Beliebtheit des Klaviers bei der bürgerlichen Mittelklasse, bei der der Besitz eines Pianinos zum Statussymbol avancierte, zum anderen aber auch mit der generellen Bevölkerungszunahme im 19. Jahrhundert zusammenhängen. Das einst so beliebte Tafelklavier wurde vom Pianino verdrängt, wobei es gewissermaßen Opfer seines eigenen Erfolgs wurde. Es entwickelte sich vom anfangs einfachen, kleinen Instrument zu einem großen und schweren Koloss in exklusiver Ausführung. Die Lücke füllte das neue, kleinere und preiswertere Pianino, das international zum mit Abstand beliebtesten Hausinstrument des Bürgertums wurde. Gegen Ende des 19. Jahrhunderts hatten die meisten Instrumentenbauer ihre Tafelklavierproduktion eingestellt.

An der Londoner Industrieausstellung (Great Exhibition) von 1851, eine der ersten großen internationalen Weltausstellungen, trafen sich erstmals Klavierhersteller aus ganz Europa und der neuen Welt. Die Ausstellung war ein riesiger Erfolg und sollte fortan regelmäßig stattfinden. Solche Anlässe ließen technologische Vergleiche zu, stachelten die Konkurrenz an und trugen wesentlich zu Innovationen bei. Eine zentrale Rolle bei den weiteren Entwicklungen des Klaviers spielte Heinrich Steinweg und sein Sohn Henry Steinway. Sie patentierten 1859 die vollständige Verbindung von Gussrahmen und Kreuzbesaitung bei Flügeln und 1866 den Einbau von Gussrahmen und Kreuzbesaitung bei Pianinos. 1878 ließ Steinway die Formbiegung des Flügelgehäuses aus laminierten Ahornschichten patentieren. Mit diesen Neuerungen war die Form und Grundkonstruktion des modernen Klaviers entstanden, die sich seither, seit über 140 Jahren, kaum mehr verändert hat. Die Neuerungen wurden bald von anderen Herstellern übernommen. 

Den Konzertflügel kann man mit den Entwicklungen des Steinway & Sons Modell Centennial D vom Dezember 1875 als weitenteils ausentwickelt betrachten. Er hat die Kreuzbesaitung von 1859, die einteilige Gussplatte, das Mechanikgestell von 1871, das Sostenutopedal und die Pilotenschrauben von 1875, erst auch noch die Bass-Spannschrauben am Resonanzboden, die 1878 entfielen. Die dann noch folgenden kleineren Modifikationen dienten weniger der Klangverbesserung als eher der Vereinfachung und Verbilligung der Produktion und der Verbesserung des Handlings - unter Beibehaltung des erzielten Klangergebnisses. Sein Nachfolger, der 1884 herausgebrachte und noch heute produzierte D-Flügel, ist beinahe 200 Kilogramm leichter. Der Centennial D zeigte über seine Produktionszeit noch einige experimentelle Entwicklungen, aber mit der Installation des "Rims", des aus Dickten verleimten Außengehäuses, beim Modell D ab 1880, war die endgültige Form gefunden. Was in jenen Jahren fortschreitender Technologie zunächst kaum auffiel, war die Verarmung des Klanges der Flügel mit Hämmern aus gebogenen Filzstreifen nach den Dolge-Patenten und Saiten aus dem 1856 erfundenen Bessemer-Stahl - Entwicklungen, die den Anforderungen an die Beschallung sehr großer Konzertsäle mit 2500 bis 7000 Zuhörern geschuldet waren, eine Leistung, die die Flügel 30 Jahre zuvor keinesfalls hätten erbringen können. Dieser bis heute als technisch aktuell anzusehende Flügeltyp wurde auf der Weltausstellung 1876 prämiert - und seither kaum noch entscheidend verbessert. 

Die französischen Flügel der 1830er und 1840er Jahre von Hertz, Boisselot, Erard und insbesondere Pleyel waren jedoch klangreicher, feuriger, allerdings leiser und nicht für Publikum von mehr als 1.000 Personen geeignet, und ihr Klangreichtum musste mit einem ungemein hohen, nach heutigen Maßstäben keinesfalls zu leistendem Wartungsaufwand an den schnell verschleißenden, aufwendig handgefertigten Hämmern bezahlt werden. 

Während in den Kriegen und politischen Umwälzungen des 18. und frühen 19. Jahrhunderts viele Klavierbauer aus Deutschland und Frankreich nach England und nach Amerika flohen, kehrten in der zweiten Hälfte des 19. Jahrhunderts viele wieder zurück nach Europa. Deutschland wurde, vor England, Frankreich und den USA, zum führenden Klavierherstellerland weltweit. Deutsche Klavierbauer lieferten in die ganze Welt.

Gegen Ende des 19. Jahrhunderts waren Berlin (über 200 Klavierbauer) und Leipzig die Zentren des Klavierbaus.
Eine so große Nachfrage konnte nur durch veränderte und standardisierte Produktionsmethoden und die aufkommende, fabrikmäßige Massenproduktion befriedigt werden.

Zu Beginn des 20. Jahrhunderts boomte das Klavier auch in den Vereinigten Staaten, die die europäische Produktion bald überholten. 1910 wurden in den USA 370.000 Klaviere produziert, im Gegensatz zu 215.000 in Europa.
Der Hochblüte der Klavierbaukunst in Deutschland wurde durch die beiden Weltkriege und die Weltwirtschaftskrise ein jähes Ende gesetzt. Zahlreiche Hersteller mussten ihre Fabriken schließen, verloren sie durch Zerstörung im Krieg oder mussten auf Kriegsmaterialproduktion umstellen. Nur zaghaft erholte sich die Branche nach dem Zweiten Weltkrieg und erst in den 1960er Jahren begann allmählich wieder ein Aufschwung. Auch die deutsche Wiedervereinigung wirkte sich positiv auf den Klavierbau aus, konnten sich doch bis 1990 traditionsreiche Firmen in Ostdeutschland (zum Beispiel Blüthner) nicht voll entfalten. Der Einbruch der europäischen Klavierproduktion wurde von der amerikanischen und der aufkommenden asiatischen kompensiert. Besonders die letzten Jahrzehnte sind durch den boomenden Klavierbau in Japan, Südkorea und China geprägt. Die japanische Yamaha Corporation fertigt mittlerweile Flügel auf höchstem Niveau, die man immer öfter in Konzertsälen (z. B. der Philharmonie in Berlin) antrifft. Die koreanische Young Chang und die chinesische Pearl River Gruppe gehören heute zu den zahlenmäßig größten Klavierherstellern der Welt.

Seit den 1980er Jahren werden vermehrt auch die Vorteile der Elektronik im Klavierbau eingesetzt. Das Resultat ist eine Kombination von akustisch-mechanischem Piano und Digitalpiano. Dazu wird in der Klaviermechanik eine Stoppleiste montiert, die die Hämmer kurz vor dem Anschlagen der Saite auffängt (Stummschaltung). Gleichzeitig wird unter den Tasten eine Sensorik montiert, die die Spielsignale auf eine Box überträgt an der Kopfhörer angeschlossen werden können. Somit lässt sich das Klavier auch „stumm“ spielen. Diese Technik wird von verschiedenen Klavierherstellern verwendet und mit verschiedenen, ähnlich klingenden Namen versehen. Yamaha nennt sie "Silent Piano (TM) " und seit der Neuvorstellung der nächsten Generation auch "TransAcoustic (TM)", Kawai "Anytime" und PianoDisc "QuietTime". Auch zum Nachrüsten werden solche Stummschalt-Systeme angeboten.

Die führenden Klavierhersteller sind heute Steinway & Sons, Yamaha (v. a. mit der CF-Serie), Fazioli, Kawai und Bösendorfer (Wien) (gehört seit 2007 zur Yamaha Gruppe) sowie auch die deutschen Unternehmen C. Bechstein, Julius Blüthner, Wilhelm Schimmel und Steingraeber & Söhne.

Eine charakteristische Entwicklung des 20. Jahrhunderts sind die elektronischen Tasteninstrumente. Bereits Ende des 19. Jahrhunderts wurde, kaum nach Entdecken der Elektrizität, mit deren neuen Möglichkeiten experimentiert. Aus ihnen entwickelten sich selbstständige und neue Instrumentengruppen, so beispielsweise das Rhodes Piano, welche meist für andere Musikstile als das klassische Piano Verwendung finden. So hat beispielsweise ein Keyboard nicht mehr viel mit einem Klavier zu tun.

Eine ganz andere Entwicklungslinie, welche in den 1980er Jahren begann, steht hinter den Digitalpianos. Im Gegensatz zu früheren Neuentwicklungen in der Geschichte des Klaviers, ist das Ziel nicht die Verbesserung des Bestehenden oder die Erschaffung von etwas Neuem, sondern im Gegenteil die Absicht, das „Original“ möglichst genau zu imitieren. Die entscheidenden Elemente sind dabei der Klang und das Spielgefühl (Klaviatur und Mechanik). Heute wird der Klang eines Tones nicht synthetisiert, sondern unter verschiedenen Bedingungen (Anschlagstärke, Pedalgebung, Resonanzen in Abhängigkeit von bereits zuvor niedergedrückten Tasten) mit hochwertigen Mikrophonen aufgenommen, digitalisiert und gespeichert (englisch: „Sampling“) und dann durch das digitale Instrument entsprechend der Betätigung der Tasten wiedergegeben.

Um das Spielgefühl möglichst genau zu imitieren wurden eigene Mechaniken für Digitalpianos entwickelt. Teilweise werden sogar Klaviermechaniken von mechanisch-akustischen Instrumenten eingebaut, deren Bewegung mit Sensoren erfasst wird. Man spricht in diesem Falle von "Hybridpianos".

Digitale Instrumente werden zunehmend auch von professionellen Pianisten zu Übungszwecken und zum Unterrichten eingesetzt. Sie bieten gegenüber akustischen Klavieren nicht nur Nachteile, sondern auch bestimmte Vorzüge, wobei die Bandbreite und Qualität auch bei derlei Instrumenten sehr stark variieren kann: der Bezugston lässt sich transponieren und die Tonhöhe kann frequenzgenau angepasst werden. Ferner können bei manchen Modellen die Klangfarbe, Klangeffekte und das Stimmungssystem gewählt werden. Viele Digitalpianos verfügen über digitale Schnittstellen und können sowohl zur Aufnahme der darauf gespielten Musik als auch zur Wiedergabe eingesetzt werden. Sie sind verhältnismäßig leicht und brauchen kaum Wartung. Die Lautstärke lässt sich regulieren und das Instrument kann mit Kopfhörern gespielt werden. Dafür reicht der Klang und das Anschlagsgefühl eines Digitalpianos in der Regel nicht an ein echtes Klavier heran.

Flügel und Pianos haben alle wesentlichen Bauteile gemeinsam:


Diese Bauteile waren ca. 1880 bis zur Perfektion entwickelt und werden ohne wesentliche Änderung bis heute (2018) zusammengefügt. Die einzigen Fortschritte ergaben sich in der Mechanisierung und Automation der Fertigung der Kleinteile.

Das Spielwerk, auch als "Klaviermechanik", "Hammermechanik" oder "Anschlagmechanik" bezeichnet, ist eine Hebel-Konstruktion, bei der auf Tastendruck Hämmer gegen die Saiten des Klaviers geschleudert werden, um diese zum Klingen zu bringen. Die Mechaniken wurden über die Jahrhunderte immer wieder verbessert, zu unterscheiden sind Mechaniken für die senkrecht besaiteten Pianinos und Mechaniken für waagerecht besaitete Flügel bzw. Tafelklaviere.

Die Klaviatur der meisten Flügel, Pianinos und Digitalpianos besteht aus 88 Tasten (bei älteren Instrumenten sind es oft nur 85, weil bei ihnen die Klaviatur in der Höhe beim a endet), davon 52 „weiße Tasten“ (auch „Vordertasten“ oder „Untertasten“) und 36 „schwarze Tasten“ (auch „Hintertasten“ oder „Obertasten“), die über die weißen Tasten hinausragen, verhältnismäßig schmal sind und zusätzlich abgeschrägte Seitenflächen haben. Aus der normierten Tastenbreite moderner Instrumente ergibt sich eine Gesamtbreite der Klaviatur von 123 cm; die Oberfläche der weißen Tasten befindet sich etwa 74 cm über dem Boden.

Im Klavierbau besteht eine Oktave aus sieben weißen und fünf schwarzen Tasten. Die weißen Tasten bilden eine diatonische Leiter (auf den Grundton c bezogen eine C-Dur-Tonleiter), die schwarzen Tasten eine pentatonische Leiter (auf den Grundton fis bezogen eine Fis-Dur-Pentatonik) – zusammengenommen ergibt das eine chromatische Leiter. Links liegen die tiefsten Töne, rechts die höchsten.

Die sieben weißen Tasten heißen c, d, e, f, g, a und h, die fünf schwarzen Tasten je nach musikalischem Zusammenhang cis, dis, fis, gis und ais (Erhöhungen der Stammtöne) oder des, es, ges, as und b (Erniedrigungen der Stammtöne). Die Klaviatur verkörpert also das europäische diatonische Tonsystem, das C-Dur und a-Moll als Ausgangstonarten nutzt und die übrigen Tonarten davon ableitet. Der Schritt von einer weißen auf eine schwarze Taste erleichtert den Fingerübersatz, der Schritt von einer schwarzen auf eine weiße Taste den Daumenuntersatz.

Der Klavierklang kann durch mehrere Pedale beeinflusst werden. Heute sind meist zwei bis drei Pedale Standard.

Das rechte Pedal heißt "Fortepedal" (von it. "forte": kräftig, laut), auch "Dämpferpedal" oder "Haltepedal" (nicht zu verwechseln mit dem weiter unten beschriebenen Tonhaltepedal); mit der Aufforderung „senza sordino“ (it. für „ohne Dämpfer“, oft in der italienischen Pluralform „senza sordini“, etwa im 1. Satz von Beethovens „Mondscheinsonate“) ist ebenfalls das rechte Pedal gemeint. Es sorgt dafür, dass alle Dämpfer von den Saiten abgehoben werden, damit die angeschlagenen Töne auch nach dem Loslassen der Tasten weiterklingen. Außerdem schwingen die nun ungedämpften Saiten anderer Töne mit, was dem Klavier einen volleren Klang gibt. Im künstlerischen Klavierspiel wird das rechte Pedal in hochdifferenzierter Weise eingesetzt; man unterscheidet z. B. das Harmoniepedal (Sammelpedal), das synkopierte Pedal (Legato- oder Bindepedal), das Halbpedal, das voraus getretene und das gleichzeitig getretene Pedal.
Das linke Pedal heißt „Pianopedal“ (von it. "piano": leise), auch "Leisepedal", "Verschiebung" oder "una corda" (it. für „eine Saite“). Beim "Flügel" wird die gesamte Mechanik einige Millimeter nach links oder rechts verschoben, sodass die Hämmer nicht mehr alle drei Saiten eines Saitenchors treffen, sondern nur noch zwei bzw. eine Saite. Dadurch verändert sich auch die Klangfarbe, weil nunmehr Saiten existieren, die nicht durch direkten Anschlag, sondern durch Resonanz erregt werden. Außerdem treffen durch die Verschiebung andere Stellen des Hammerfilzes auf die Saiten. Diese Stellen sind anders intoniert (d. h. vom Klavierstimmer mit der Intoniernadel aufgeweicht bzw. mit einer Feile gehärtet) als die Filzstellen, die in Normalstellung die Saiten anschlagen. Beim "Pianino" bewegt das linke Pedal die Hämmer der Klaviermechanik näher an die Saiten, sodass die Kraft, die jeder Hammer bei Betätigung aufbauen kann, geringer ist. Damit wird das Spielen besonders leiser Stellen vereinfacht. Der Hersteller Fazioli bietet ein Flügel-Modell mit zwei Piano-Pedalen an, die dem Pianisten die Wahl zwischen der „Verschiebung“ und dem Pianopedal der Pianino-Technik ermöglicht.

Das (nicht immer vorhandene) mittlere Pedal ist entweder ein Tonhaltepedal, ein Moderatorpedal oder ein Stummschaltepedal (bei Hybridpianos). Wenn ein "Flügel" ein mittleres Pedal besitzt, handelt es sich in der Regel um das so genannte Tonhalte-, Tonhaltungs-, Sostenuto- oder Steinway-Pedal. Diese Vorrichtung wurde von französischen Klavierbauern entwickelt (Jean Louis Boisselot 1844, Claude Montal 1862) und in den USA zum Erfolg geführt (Albert Steinways Patent von 1874). Sie hindert die gerade gehobenen Dämpfer daran, wieder zurückzufallen. Der Spieler kann damit also einzelne Töne oder Klänge festhalten, während alle anderen Dämpfer weiterhin auf das Spielen und Loslassen der Tasten (bzw. das rechte Pedal) reagieren. Das Tonhaltepedal – mittlerweile ist es auch bei größeren und teureren Pianinomodellen anzutreffen – findet vor allem in der Klaviermusik des 20. Jahrhunderts Verwendung. Wenn ein "Pianino" ein mittleres Pedal besitzt, handelt es sich meist um den so genannten Moderator. Bei Betätigung schiebt sich ein Filzstreifen zwischen Hämmer und Saiten und macht das Instrument deutlich leiser. Dieses Pedal kann oft in der unteren Position durch eine Seitwärtsbewegung verriegelt werden. Bei manchen Pianinos wird der Moderator nicht über ein Pedal, sondern über einen schiebbaren Knopf oder einen drehbaren Hebel aktiviert, der links der Klaviatur oder unter ihr sitzt. Vor allem in den 1960er Jahren versahen einige Hersteller den Filzstreifen mit Nieten, die dem Klavier einen klimpernden, cembaloähnlichen Klang verliehen. Da diese Metallplättchen allzu leicht Saiten und Hammerköpfe beschädigten, haben sie sich nicht durchgesetzt.

Ein Flügel steht, wie ein Cembalo, frei im Raum. Raste, Resonanzboden und Besaitung sind horizontal, parallel zum Boden, angeordnet. Der Klang strahlt daher vom Resonanzboden überwiegend nach unten und oben ab. Unten wird er vom Fußboden reflektiert und verteilt, oben entweder vom geschlossenen Deckel gedämpft oder vom geöffneten Deckel gebündelt zur Seite hin abgestrahlt.

Ein Tastendruck führt zu einer Aufwärtsbewegung des hinteren Teils der Tastenwippe. Beim Flügel wird dadurch der Hammer nach oben an die Saite geschleudert. Das Gewicht des Hammers ist direkt an der Taste spürbar und ermöglicht eine differenzierte Klanggestaltung. Durch ihre horizontale Lagerung wird das Zurückschnellen der Hämmer von der Saite durch die natürliche Schwerkraft unterstützt. Die Repetitionsfähigkeit eines Flügels, also die Geschwindigkeit, mit der ein und derselbe Ton mehrfach hintereinander angeschlagen werden kann, ist daher stärker ausgeprägt als bei einem Pianino.

Manche Konzertflügel, etwa der „Imperial“ von Bösendorfer, haben eine auf bis zu acht Oktaven Tonumfang (C bis c) erweiterte Klaviatur.

Beim Pianino stehen Raste, Resonanzboden, Gussrahmen, Besaitung und Hammermechanik (Ständermechanik) senkrecht zum Boden, so dass man es platzsparend an die Wand stellen kann und der Klang zunächst nach vorne und nach hinten abstrahlt. Bei der üblichen Aufstellung wird der hintere Anteil direkt von der Zimmerwand reflektiert und zurück auf den Resonanzboden gelenkt. Eine leicht von der Wand abgewandte Position oder ein kleiner Winkel zur Wand verändert oft den Klang von Pianinos enorm zum Vorteil. Der vordere Klang-Anteil wird im Gehäuse reflektiert.

Durch den Anschlag auf der Vorderseite der Harfe ist die Resonanzbodenfläche eines Hochklaviers oft vergleichsweise groß. Das macht höhere Pianinos (ab ca. 120 cm Höhe) oft erstaunlich klangstark – speziell im Vergleich zu kleineren Flügeln (unterhalb von 170 cm Länge).

Beim Pianino muss die Aufwärtsbewegung der Tastenwippe in eine Vorwärtsbewegung des Hammers umgesetzt werden. Dadurch ist der Fingerkontakt zum Hammer indirekter.

Die Dämpfung eines Pianinos oder Hochklaviers befindet sich normalerweise unterhalb der Hämmer auf derselben Seite der Saitenanlage, im Bereich der stärkeren Amplituden der Schwingungsbäuche.

Ältere Pianinos haben jedoch (bis ca. 1910) teils eine sogenannte "Oberdämpfer"-Mechanik; die Dämpfer-Puppen sitzen über den Hämmern. Im Englischen findet man hierfür auch den Begriff „birdcage action“, „Vogelkäfig“-Mechanik, wegen der vor die Hammermechanik gebauten Dämpfer-Betätigungsdrähte. Diese Art der Dämpfung ist zum einen weniger effektiv als bei einem Unterdämpfer-Klavier, da sie die Schwingungen nur im Randbereich der Schwingungsbäuche abdämpft, zum Weiteren kann die Dämpferpuppe bei kurzen Diskantsaiten einen optimalen Hammeranschlagspunkt vereiteln – mit entsprechenden Nachteilen für die Klangqualität. Das Stimmen und vor allem die Regulation der Mechanik können durch die vorn liegenden Dämpferdrähte erschwert sein. Dass Oberdämpfer-Klaviere aus diesen Gründen jedoch generell völlig untauglich seien, wie man oft behauptet findet, kann man nicht sagen. Ein gut reguliertes Oberdämpferklavier ist wegen seines deutlichen Nachklingens das prädestinierte Instrument für frühen Jazz und vor allem für den Ragtime.

1925 wurden allein in Deutschland, dem damals führenden Produktionsland, 137.000 Klaviere gebaut. In den USA ging mit dem Erfolg des Ragtime zu Beginn des 20. Jahrhunderts ein enormer Aufschwung des Klavierbaus einher, auch (bis etwa 1930) beim Bau pneumatisch und elektrisch angetriebener Reproduktionsklaviere. 2007 wurden weltweit zirka 450.000 Pianinos und Flügel produziert, etwa zwei Drittel davon im Fernen Osten; aus Deutschland kamen weniger als 10.000 Instrumente. Preisunterschiede zwischen ähnlich dimensionierten Klavieren (auch zwischen verschiedenen Produktlinien eines und desselben Herstellers) ergeben sich aus kürzeren oder längeren, mehr oder weniger automatisierten Produktionsprozessen, aus der Produktion in Niedrig- oder Hochlohnländern und aus unterschiedlichen Qualitäten etwa des Klangholzes oder des Filzes.

Im Jahr 1980 gab es in den westdeutschen Privathaushalten etwa 9.300.000 Flöten, 8.400.000 Mundharmonikas/Melodikas, 3.800.000 Gitarren, 2.200.000 Akkordeons und 1.600.000 Pianinos/Flügel.

Das Freizeitverhalten in Deutschland hat sich geändert: Gerade einmal zwei Prozent der Menschen musizieren täglich, 78 Prozent jedoch nie. Entsprechend hat sich der Absatz von Klavieren seit 1925 auf etwa ein Zehntel (12.000 im Jahr) verringert. Es gibt 1,5 Millionen Instrumente; 130.000 Schüler nehmen Unterricht. Gebrauchte Klaviere werden wegen der hohen Kosten bei Umzügen und für das Stimmen häufig verschenkt; pro Jahr werden rund 3.500 Instrumente verschrottet.

Zu den spezifischen Merkmalen des Klavierklangs gehören die festgelegten Tonhöhen, eine an die Anschlagsgeschwindigkeit und somit die Lautstärke gekoppelte Färbung des Klangs und das unwiderrufliche Verklingen des Tons, der nach erfolgtem Anschlag nur noch durch Gebrauch des rechten Pedals verlängert und durch allmähliches oder abruptes Aufsetzen der Dämpfung allmählich oder abrupt beendet werden kann.

Eine Besonderheit des Klaviers ist, dass die Töne (abgesehen von den tiefsten) nicht nur von einer, sondern von zwei bis drei gleich gestimmten Saiten erzeugt werden, einem so genannten Saitenchor. Ursprünglich sollte diese „Mehrchörigkeit“ die Lautstärke des Instrumentes erhöhen; vor allem aber führte sie zu einem komplexeren Verlauf des aus Sofort- und Nachklang zusammengesetzten Klanges.

Die Saiten eines Saitenchors werden gemeinsam angeschlagen und schwingen gleichphasig, da sie gleich gestimmt sind, allerdings mit leicht unterschiedlicher Amplitude, da die Form des Hammers nie vollkommen regelmäßig ist. Sobald die am schwächsten angeschlagene Saite zur Ruhe kommt, beginnt sie mit den anderen Saiten mitzuschwingen. Nun fungieren die Saiten des Saitenchors als gekoppelte Pendel und tauschen einen Großteil ihrer Energie miteinander aus.

Als "Sofortklang" wird der laute, aber schnell abklingende Teil des Klaviertones bezeichnet. Er entsteht hauptsächlich durch eine Transversalschwingung der Saiten in Richtung des Hammerschlags, also senkrecht zum Resonanzboden. Diese Schwingung wird primär vom Hammer angeregt, aber vergleichsweise rasch senkrecht auf den Resonanzboden übertragen, wodurch sie ihre Energie als Schall an die Luft abgibt.

Als "Nachklang" wird der leisere, dafür aber langsamer abklingende Teil des Klaviertones bezeichnet. Dieser entsteht vor allem durch eine leichte Transversalschwingung der Saiten quer zum Hammerschlag, also parallel zum Resonanzboden. Diese Schwingung gibt ihre Energie nur schwer an den Resonanzboden ab und verklingt daher langsam.

Die Verwendung des linken Pedals schwächt einerseits den Sofortklang, da nur zwei der drei Saiten eines Saitenchores angeschlagen werden, und unterstützt andererseits den Nachklang, da der Saitenchor als System gekoppelter Pendel seine Energie vergleichsweise langsam abgibt. Das linke Pedal führt also nicht nur zu einem anfangs leiseren, sondern auch zu einem relativ länger anhaltenden Ton.

Der Klang und die Lautstärke eines Tones auf dem Klavier ist allein abhängig von der Geschwindigkeit und somit von der Bewegungsenergie des Hammers, der die Saiten anschlägt, nicht jedoch von der Art und Weise, wie der Klavierspieler den Hammer auf diese Geschwindigkeit beschleunigt, also auch nicht von einer bestimmten Anschlagstechnik. Wenn man die Pedale unberücksichtigt lässt und von einigen Phänomenen absieht, die eine zusätzliche Rolle spielen, etwa den „oberen“ und „unteren Geräuschen“, die abhängig von der Spielweise beim Zusammenstoß zwischen Finger und Taste bzw. zwischen Tastenholz und Tastenboden entstehen, verlaufen Klangfarben- und Lautstärkenänderung auf dem Klavier also stets parallel zueinander.

Allerdings hängt der Zeitpunkt des Anschlags der Saiten nach dem Beginn des Niederdrückens einer Klaviertaste vom zeitlichen Kraftverlauf und somit der Beschleunigung des Hammers während des Niederdrückens ab, wodurch ein trainierter Pianist einen bestimmten Ton trotz gleicher Lautstärke in gewissen Grenzen gezielt etwas früher oder später erklingen lassen kann („Mikro-Agogik“) und unabhängig von der Lautstärke Akzente setzen kann. Insofern hat die Anschlagtechnik des Pianisten durch den tatsächlich erzielten Zeitpunkt des Einsetzens des Klaviertones einen entscheidenden Einfluss auf den Klaviervortrag.

Da sich Klaviere durch den Saitenzug, durch die Spielbelastung und durch klimatische Schwankungen verstimmen und in der Folge unschön klingen, sollten sie mindestens einmal jährlich gestimmt werden. Aufgrund von Inharmonizitäten der Obertöne ist auch die Stimmung subjektiv vom Klavierstimmer festgelegt. (In Konzerthäusern werden Flügel bis zu 3-mal täglich gestimmt.) Standard ist die gleichstufige Stimmung; für originale oder nachgebaute historische Instrumente werden oft ungleichstufige Stimmungen bevorzugt (historische Aufführungspraxis). Um den Flügel oder das Pianino klanglich auszuarbeiten, wird der Klavierbauer nicht nur stimmen, sondern auch intonieren. Zu den möglichen Vorarbeiten zählt das leichte Abziehen der aus Filz bestehenden Hammerköpfe mit Sandpapierfeilen – das macht den Klang gleichmäßiger und gegebenenfalls etwas „härter“. Dann folgt das eigentliche Intonieren durch gezieltes Stechen in bestimmte Hammerkopfbereiche mit Intoniernadeln – eine Arbeit, die den Klang in der Regel „weicher“ macht. Neben dem Stimmen und Intonieren wirkt sich auch das Regulieren der Mechanik (des Spielwerks, der Klaviatur und der Pedale) unmittelbar auf den Klang des Instrumentes aus.

Das Raumklima hat direkte Auswirkungen auf den Klang des Instruments, außerdem auf Regulierung, Stimmung und insgesamt auf die Wertbeständigkeit.

Vor allem die Luftfeuchtigkeit sollte möglichst konstant sein. Empfohlen wird eine relative Luftfeuchte zwischen 40 und 70 %, idealerweise zwischen 50 und 60 %. Werte unter 40 % führen zu starker Austrocknung des Holzes und sollten unbedingt vermieden werden, Werte über 70 % begünstigen Rostbildung an Metallteilen, zum Beispiel den Saiten. Nicht empfohlen wird die Aufstellung an schlecht isolierten Außenwänden, in der Nähe von Heizkörpern oder auf einem geheizten Fußboden; auch Zugluft und direkte Sonneneinstrahlung sind zu vermeiden.

Klaviere reisen oft um den halben Erdball, bevor sie ihren Bestimmungsort erreichen. Das kann zu schwerwiegenden Problemen führen, beispielsweise, wenn ein für das schwüle Klima Ostasiens konzipiertes Instrument in Mittel- oder Nordeuropa den ersten kalten und somit trockenen Winter durchstehen muss. Heute produzieren große und renommierte Klavierfirmen wie Yamaha ihre für den Export nach Europa oder Nordamerika bestimmten Instrumente in spezifisch klimatisierten Räumen.

Sinkt die Luftfeuchtigkeit über einen längeren Zeitraum stärker ab, so verlieren die Holzbauteile Feuchtigkeit und ziehen sich zusammen. Die Gefahr besteht, dass sich Stimmwirbel und Schrauben lockern, Klaviaturrahmenbalken und Mechanikbalken verziehen (was die Regulierung von Klaviatur und Mechanik beeinträchtigt), dass der Resonanzboden seine Wölbung verliert (wodurch die Stimmung sinkt und der Klang leidet) und vielleicht sogar reißt. Steigt hingegen die Luftfeuchtigkeit über einen längeren Zeitraum stärker an, so verstärkt sich die Wölbung des Resonanzbodens, steigt die Stimmung, können Achsen und Tasten klemmen und wird der Klang dumpfer (weil der Hammerfilz Feuchtigkeit aufnimmt). Diesen Problemen kann bis zu einem gewissen Maß durch hochwertige Materialien entgegengewirkt werden. Auch sind Klaviaturrahmen und Mechanikbalken aus Metall möglich, bringen allerdings wieder andere Nachteile mit sich. Schichtverleimte Resonanzböden arbeiten kaum, klingen aber deutlich schlechter.

Materialien wie Plexiglas oder Kohlenstofffaser-Verbundwerkstoffe (CFK) reagieren nur wenig auf Klimaschwankungen und werden inzwischen bei einzelnen Serienmodellen zur Fertigung des Klavierkörpers bzw. des Resonanzbodens eingesetzt.

Der erste Komponist, welcher speziell für das von Bartolomeo Cristofori erfundene Hammerklavier schrieb, war Lodovico Giustini aus Pistoia. Er komponierte zwölf Sonaten mit dem Titel "„Sonate Da Cimbalo di piano e forte detto volgarmente di martelletti“", die im Jahre 1732 in Florenz publiziert wurden. Damit die Interpreten die Möglichkeiten des neuen Instruments ausschöpfen würden, versah er seine Musik mit Anmerkungen wie „più forte“ (lauter) oder „più piano“ (leiser).

Komponisten wie die Bach-Söhne, Wolfgang Amadeus Mozart und Ludwig van Beethoven und andere komponierten Musik, die in der zweiten Hälfte des 18. Jahrhunderts zu Teilen bereits solistisch für das Klavier geschrieben war.

In der ersten Hälfte des 19. Jahrhunderts war es insbesondere Frédéric Chopin, welcher vornehmlich für das Klavier Musik schrieb. In der zweiten Hälfte waren es Komponisten wie Franz Liszt, Sergei Rachmaninow, Anton Rubinstein, Ignacy Jan Paderewski und andere Komponisten des romantischen Repertoires, die sich in der Pianomusik hervortaten, oft noch mit dem Hauptanliegen, auf den Bühnen als Pianist vorrangig ihre eigenen Musikkompositionen zur Aufführung zu bringen.

Mit Beginn des zwanzigsten Jahrhunderts trat der Komponist-Interpret in den Hintergrund; die Tätigkeiten der Komposition zum Einen und des Aufführens, Interpretierens zum Anderen trennten sich. Es waren zum einen Komponisten der Moderne wie Béla Bartók und Ferruccio Busoni im Segment der sogenannten „E-Musik“ (ernster Musik), als auch im Bereich der „U-Musik“, unterhaltender, populärer Musik vor Allem die Entwicklungen im US-amerikanischen Raum, wie der Blues, der Ragtime, der Boogie Woogie und der Jazz mit Komponisten wie Scott Joplin, Jelly Roll Morton, Albert Ammons und George Gershwin, die der Klaviermusik große Impulse gaben.





</doc>
<doc id="14693" url="https://de.wikipedia.org/wiki?curid=14693" title="Säuren">
Säuren

Säuren sind im engeren Sinne alle chemischen Verbindungen, die in der Lage sind, Protonen (H) an einen Reaktionspartner zu übertragen – sie können als Protonendonator fungieren. In wässriger Lösung ist der Reaktionspartner im Wesentlichen Wasser. Es bilden sich Oxonium-Ionen (HO) und der pH-Wert der Lösung wird damit gesenkt. Säuren reagieren mit sogenannten Basen unter Bildung von Wasser und Salzen. Eine Base ist somit das Gegenstück zu einer Säure und vermag diese zu neutralisieren.

Im weiteren Sinn beschreiben verschiedene Säure-Base-Konzepte wesentlich breitere Paletten von chemischen Reaktionen, die weit über die oben erwähnten Reaktionen hinausreichen können.

Wohl die älteste bekannte Säure (lat. "acidum") ist Essig (lat. "acetum"), eine etwa fünfprozentige wässrige Lösung der Verbindung Essigsäure. Säuren waren etwas "Essigartiges" ("oxos" od. "acidus"). In der Alchemie galt Säure als ein "Urstoff". Die sauren Eigenschaften basierten auf einer einzigen "Ursäure", die in verschiedenen Substanzen zur Wirkung kam. Säuren lassen Kalk bzw. Carbonate aufschäumen, haben eine ätzende Wirkung und einen sauren Geschmack. Bis Ende des 13. Jahrhunderts waren wohl neben Essig und anderen Pflanzensäften keine Säuren bekannt. Erweiterte Kenntnisse bezüglich Säuren wurden dem im 8. Jahrhundert lebenden Geber zugeschrieben, sie basieren jedoch auf Schriften der deutlich jüngeren, sogenannten "Pseudo-Geber". Es entwickelten sich Kenntnisse über verschiedene anorganische Säuren, die alle den "Varietäten des Elements Wasser" zugeschrieben wurden. Ab dem 18. Jahrhundert wurden sie als Mineralsäuren bezeichnet.


Der wichtige Begriff Base als phänomenologisches Gegenstück zur Säure wurde im 17. Jahrhundert von Alchimisten und Chemikern wie G. E. Stahl, R. Boyle und G. F. Rouelle verwendet, weil „basische“ Stoffe die "nichtflüchtige" Grundlage zur "Fixierung flüchtiger Säuren" bildeten und die (ätzende) Wirkung von Säuren aufheben konnten. Grundlegende Schritte in die Chemie gelangen A. L. Lavoisier im 18. Jahrhundert, der bestimmten chemischen Verbindungen bestimmte Eigenschaften zuwies. Er dachte, dass Säuren stets aus Nichtmetalloxiden und Wasser sowie Basen aus Metalloxiden und Wasser entstünden. Humphry Davy fand 1808 ein Gegenbeispiel (Chlorwasserstoff). J. von Liebig sah Säuren als Wasserstoff-Verbindungen, die sich durch Metalle in Salze überführen lassen.

1887 definierte S. Arrhenius Säuren als Stoffe, die beim Auflösen in Wasser unter Abgabe von Protonen (H) dissoziieren, und Basen als Stoffe, die beim Auflösen in Wasser unter Abgabe von Hydroxidionen (OH) dissoziieren. Gibt man Säuren und Basen zusammen, neutralisieren sie sich unter Bildung von Wasser. Die Theorie war jedoch noch unzureichend, da Verbindungen ohne Sauerstoff nicht einbezogen wurden: Auch Ammoniak neutralisiert eine Säure. J. N. Brønsted und T. Lowry beschrieben 1923 unabhängig voneinander die heute noch wichtigste Definition von Säuren und Basen. Sie bilden die Basis der unten erläuterten Erklärungen zur Säure.

Ohne näher auf verschiedene "Säure-Base-Konzepte" einzugehen, soll hier als Einstieg eine mögliche und übliche Betrachtungsweise beschrieben werden. Im engen Zusammenhang mit Säuren stehen "in der Regel" und häufig "ohne ausdrückliche Erwähnung" die Anwesenheit und bestimmte Eigenschaften des Wassers. Reines Wasser unterliegt einer sogenannten Autoprotolyse. Hierbei entstehen aus dem Wasser in sehr kleinen und gleichen Mengen Oxoniumionen (HO) und Hydroxidionen (OH):

In dieser Reaktionsgleichung des Wassers zeigt sich die Eigenschaft einer Säure, nämlich die Fähigkeit der Bildung von HO-Ionen in Wasser. Gleichzeitig bilden sich OH-Ionen in Wasser – eine der Eigenschaften, über die eine Base verfügen kann. Man bezeichnet jedoch Wasser weder als eine Base noch als eine Säure und nennt sein Verhalten "neutral". Dies bezieht sich auf den pH-Wert, der die Konzentration der HO-Ionen in Wasser angibt. Reines Wasser hat den pH-Wert von 7, also eine sehr kleine Konzentration. Diese Reaktion ist, wie alle in diesem Abschnitt beschriebenen Reaktionen, eine Gleichgewichtsreaktion: Die Bildung der Ionen sowie deren Vereinigung zu Wasser findet ständig und mit gleicher Häufigkeit statt. "Neutral" heißt also nicht, dass nichts passiert.

Als Säuren kann man chemische Verbindungen bezeichnen, die in einer bestimmten Wechselwirkung mit Wasser stehen können. Sie verfügen über Wasserstoffatome, die ionenähnlich (ionogen) gebunden sind. So reagiert reine Essigsäure (HC-COOH) mit Wasser und bildet dabei weitere HO-Ionen. Tritt eine solche Reaktion auf, kann man eine Verbindung als Säure bezeichnen.
Neben dem Oxoniumion entstehen auch das Acetat-Anion HC-COO:

Essigsäure ist eine Carbonsäure und im Vergleich zu anorganischen Säuren wie Chlorwasserstoff eine eher schwache Säure. In wässriger Lösung liegen ein guter Teil der Moleküle undissoziiert als HC-COOH vor. Auch hier stellt sich zügig ein Gleichgewicht ein. Die obige Reaktionsgleichung (2) lässt sich aus diesem Grund mit gleicher Berechtigung von rechts nach links lesen. Ein Acetat-Anion reagiert mit einem Oxoniumion zu Wasser und Essigsäure. In dieser Leserichtung findet eine "basische Reaktion" statt: die Umsetzung von Hydroxoniumionen zu Wassermolekülen. Setzt man einer Essigsäurelösung in geeigneter Menge Acetat-Anionen, beispielsweise in Form des gut löslichen Natriumacetats zu, kann die saure Eigenschaft der Essigsäure vollständig durch die basische Eigenschaft des Acetat-Ions kompensiert werden. Die wässrige Lösung wird neutralisiert. "Neutral" heißt auch hier keineswegs, dass nichts in der Lösung passiert. Nur die Konzentration der HO-Ionen ist so gering wie in reinem Wasser.

Ergänzend soll nun die "basische Reaktion" betrachtet werden, die auftritt, wenn Natriumacetat in "reinem" Wasser gelöst wird (Das Na-Kation ist in der Reaktionsgleichung weggelassen):

Hier bilden sich Hydroxidionen (OH). Fügt man dieser Acetatlösung eine geeignete Menge an wässriger Essigsäurelösung zu, wird die Lösung "neutral". Es stellt sich zwischen HO und OH das Gleichgewicht (1) ein, das zu Anfang als Grundeigenschaft von Wasser vorgestellt wurde und hier als Gleichung (1a) andersherum dargestellt ist.

Viele als Säure bezeichnete Substanzen sind von vornherein wässrige Lösungen und können nicht ohne weiteres als chemische Verbindungen verstanden werden, die über ionogen gebundene Wasserstoffatome verfügen. Salzsäure ist eine "wässrige Lösung" des Gases Chlorwasserstoff (HCl) und gilt als eine starke Säure. In dieser Lösung liegt – vor jeder praktischen Verwendung der Säure – bereits die Gleichgewichtsreaktion (4) vor, bei der das Gleichgewicht fast vollständig auf der rechten Seite liegt.

Der Chlorwasserstoff hat sein Potential, eine Säure zu sein, schon längst ausgespielt, und es haben sich HO-Ionen gebildet. Die chemischen Auswirkungen, die durch eine praktische Anwendung der Salzsäure auftreten, sind auf Reaktionen der HO-Ionen zurückzuführen. Die Säure "ist" das HO-Ion. Das Maß für den Säuregehalt (Konzentration) ist hier der pH-Wert, während bei schwächeren Säuren, wie Essigsäure, das Maß für die Säurestärke, der pKs-Wert, im Vordergrund steht. Starke und schwache Säuren unterscheiden sich durch ihre Tendenz, „gerne“ oder „weniger gerne“ HO-Ionen in Wasser zu bilden. Im Abschnitt Säure-Base-Gleichgewicht werden diese Tendenzen näher beschrieben.

Bei der Protolyse nimmt ein Reaktionspartner (in der Regel Wasser) das von der Säure abgegebene Proton auf. Dies ist abzugrenzen von den Redoxreaktionen, bei denen Elektronenübergänge stattfinden.

Die allgemeine Gleichgewichtsreaktion einer Säure HA "in wässriger Lösung" lautet:

Die Säuren unterscheiden sich in ihrer Tendenz, H-Ionen an Wasser zu übertragen. Diese wird als "Säurestärke" "K" bezeichnet und gibt die Gleichgewichtskonstante (Säurekonstante) der Säurereaktion an. Die Säurekonstante wird häufig in Form des p"K"-Wertes angegeben, der als negativer dekadischer Logarithmus der Säurekonstante definiert ist.

Säuren mit großem "K"-Wert (kleinem p"K"-Wert) sind starke Säuren. Liegt ein pH-Wert einer Lösung, die eine Säure enthält, zwei Einheiten unter dem p"K"-Wert, werden nur noch ein Hundertstel der HO-Ionen gebildet.

Säuren, die mehrere Protonen abspalten können, nennt man "mehrprotonige Säuren" oder auch "mehrbasige Säuren". Schwefelsäure (HSO) ist eine zweiprotonige (auch diprotonige), Phosphorsäure (HPO) eine dreiprotonige (auch triprotonige) Säure. Das Bestreben der Abgabe der einzelnen Protonen (Protolyse) ist unterschiedlich groß und lässt sich durch die Säurekonstante (K) beschreiben. Für die einzelnen Protolyseschritte gilt allgemein: K(I) > K(II) > K(III) (bzw. p"K"(I) < p"K"(II) < p"K"(III)).

Für Phosphorsäure gilt:

Siehe p"K"- und p"K"-Werte einiger Verbindungen.

Die Eigenschaften von Säuren, insbesondere die von ihnen ausgehenden Gefahren, sind sehr unterschiedlich. Als Beispiele seien hier Salpetersäure (Hauptgefahr: ätzend), Blausäure (starkes Gift) und Pikrinsäure (ein Sprengstoff) genannt.


Analog zu den Säure-Base-Reaktionen die in wässrigen Lösungen und unter Beteiligung des Wassers ablaufen, existieren Reaktionen in anderen Medien. In wasserfreiem Ethanol findet mit Chlorwasserstoff eine Reaktion statt, bei dem Ethanol die Rolle einer Base übernimmt:

In der Gasphase reagieren die Gase Ammoniak und Chlorwasserstoff unter Bildung des Salzes Ammoniumchlorid.

In Säure-Base-Reaktionen können neben Wasser auch andere hinreichend polare Lösungsmittel als Reaktionspartner wirken. Ein gutes Beispiel ist die Autoprotolyse des flüssigen Ammoniaks:

Wichtige Säuren sind:

Auch Salze mehrprotoniger Säuren können als Säuren wirken („saure Salze“), beispielsweise


Historische Entwicklung der Säuren:


</doc>
<doc id="14694" url="https://de.wikipedia.org/wiki?curid=14694" title="Gustaf Gründgens">
Gustaf Gründgens

Gustaf Gründgens (* 22. Dezember 1899 in Düsseldorf als "Gustav Heinrich Arnold Gründgens"; † 7. Oktober 1963 in Manila) war ein deutscher Schauspieler, Regisseur und Intendant. 

Die Chansonnière und Kabarettistin Marita Gründgens (1903–1985) war seine Schwester.

Gründgens kam als Sohn von Arnold Hubert und Emmi Gründgens zur Welt. Seine Schulzeit verbrachte er auf der Oberrealschule am Fürstenwall in Düsseldorf, dem Gymnasium Oberkassel (heute: Comenius-Gymnasium) und dem Gymnasium zu Mayen (heute: Megina-Gymnasium Mayen), das er zu Ostern 1917 mit der Versetzung in die Obersekunda verließ.

Nach einer dreimonatigen kaufmännischen Lehre in Düsseldorf wurde er im Herbst 1917 zum Militär einberufen. Er übte jedoch keinen Frontdienst mit der Waffe aus, sondern wurde unter vorgetäuschter Bühnenerfahrung an das Fronttheater Saarlouis versetzt, dessen Leiter er 1918 wurde und das auch nach dem Krieg in Thale weiter existierte. Eine Ausbildung erhielt er 1919/20 bei Louise Dumont und Gustav Lindemann auf der "Hochschule für Bühnenkunst" des Schauspielhauses Düsseldorf. Seine Lehrer waren Paul Henckels, Peter Esser und Elsa Dalands.

Am Beginn von Gründgens’ Karriere standen kurze Engagements an den Städtischen Bühnen Halberstadt (Spielzeit 1920/21), am Vereinigten Städtischen Theater Kiel (Spielzeit 1921/22), am Berliner Theater in der Kommandantenstraße (Spielzeit 1922/23) und am Kurtheater Eckernförde (Sommerspielzeit 1923) aufeinander, wo er 1923 als Regisseur debütierte. 1923 ging er zu Erich Ziegel an die Hamburger Kammerspiele. In dieser Zeit erweiterte er sein Repertoire an klassischen und zeitgenössischen Stücken und inszenierte u. a. "Anja und Esther" von Klaus Mann. Klaus Mann und dessen Schwester Erika spielten zusammen mit Gründgens und Pamela Wedekind die Hauptrollen. 1927 folgte Klaus Manns "Revue zu Vieren" mit denselben Hauptdarstellern. Gründgens und Erika Mann heirateten 1926. Die Ehe wurde 1929 geschieden.

1928 ging Gründgens zu Max Reinhardt ans Deutsche Theater in Berlin, wo er auch selbst Regie führte. Ab 1929 führte er äußerst erfolgreich Opernregie und wirkte auch in Kabarettrevuen mit. Zu diesem Zeitpunkt übernahm er auch erste Filmrollen. Einer der bekanntesten Auftritte aus dieser Zeit ist die Rolle des Schränkers in "M".

1930 hatte er größeren Erfolg als Regisseur mit "Menschen im Hotel" von Vicki Baum, einem Theaterstück nach ihrem gleichnamigen Roman, das ein Welterfolg wurde.

1932 begann Gründgens, am Preußischen Staatstheater in Berlin zu arbeiten. Seine erste Rolle war die des Mephistopheles in Goethes Faust.

Gründgens blieb 1933 in Deutschland und stieg im NS-Reich auf der Karriereleiter nach oben. 1934 wurde er Intendant des Staatlichen Schauspielhauses und zum Staatsschauspieler ernannt. Ein halbes Jahr nach der Ermordung Ernst Röhms, Ende Dezember 1934, wandte er sich an seinen obersten Dienstherrn Hermann Göring mit der Bitte um Entlassung als Leiter des Schauspielhauses und verwies dabei – ohne diese "expressis verbis" zu nennen – auf seine Homosexualität. Göring nahm das Rücktrittsgesuch aber nicht an. Am preußischen Staatstheater am Gendarmenmarkt spielte Gründgens im Januar 1936 Shakespeares "Hamlet" unter der Regie Lothar Müthels (mit Käthe Gold als Ophelia und im Bühnenbild von Rochus Gliese), eine Aufführung, die zum Ausgangspunkt für massive Angriffe aus nationalsozialistischen Kreisen um den Parteiideologen Alfred Rosenberg wurde, da Gründgens’ Darstellung des Hamlet die Tragödie eines vereinsamten Intellektuellen inmitten eines verbrecherischen Staates hervorzuheben schien und er Sätze wie „Die Zeit ist aus den Fugen“ und „Dänemark ist ein Gefängnis“ angeblich tendenziös vortrug. Nach Angriffen im "Völkischen Beobachter" wich Gründgens 1936 in die Schweiz aus, kehrte aber kurz darauf zurück und wurde von Göring zum preußischen Staatsrat ernannt, um – laut Gründgens – einer etwaigen Verhaftung vorzubeugen, welche nun nur noch mit Görings persönlicher Zustimmung als Ministerpräsident hätte erfolgen können. Gründgens amtierte bis 1945 als General-Intendant der Preußischen Staatstheater und zog seinen Vorteil daraus, dass diese dem Preußischen Ministerpräsidenten Göring unterstanden und nicht wie die übrigen Theater Reichspropagandaminister Joseph Goebbels.

Gründgens führte außerdem sporadisch Filmregie und übernahm 1938 bei der UFA-Tochter "Terra" die Leitung einer eigenen Herstellungsgruppe, wo unter anderem die Filme "Zwei Welten" (1939) und "Friedemann Bach" (1941) entstanden. "Tanz auf dem Vulkan" als Mischung aus Historienfilm und Revuefilm aus dem Jahr 1938 zeigte eine besondere schauspielerische Leistung Gründgens’. Er wirkte in Propagandafilmen wie "Ohm Krüger" (1940/41) mit.

Als Joseph Goebbels am 18. Februar 1943 den „Totalen Krieg“ ausrief, meldete Gründgens sich freiwillig an die Front, wurde von Göring jedoch im Frühjahr 1944 nach Berlin zurück befohlen. Sein Name stand fortan auf der Gottbegnadetenliste.

In zweiter Ehe war Gründgens von 1936 bis 1946 mit der Schauspielerin Marianne Hoppe verheiratet. Seine Homosexualität war damals in der Öffentlichkeit kein Geheimnis (wie Spottverse aus der damaligen Zeit zeigen: „Hoppe, Hoppe, Gründgens, die kriegen keine Kindgens, und das hat seine Gründgens“ oder „Hoppe, Hoppe, Gründgens, die kriegen keine Kindgens; und kriegt die Hoppe Kindgens, dann sind die nicht von Gründgens“).

Von 1945 bis 1946 war er im Speziallager Jamlitz inhaftiert. Im Rahmen der Entnazifizierung wurde er von vielen Kollegen entlastet und entlastete seinerseits unter vielen auch die Schauspielerin Emmy Göring (die Witwe Hermann Görings) und Veit Harlan, den Regisseur des berüchtigten antisemitischen Propagandafilmes "Jud Süß". Ausschlaggebend für seine Entlassung aus der Haft war die intensive Bemühung Ernst Buschs, den Gründgens während des Zweiten Weltkriegs durch seine Intervention bei Göring vor dem Galgen gerettet hatte, sowie auch etlicher anderer Schauspieler und Mitarbeiter, die sich persönlich oder schriftlich für ihn einsetzten. Bereits 1946 stand Gründgens wieder auf der Bühne, zunächst im sowjetischen Sektor von Berlin, und führte am Deutschen Theater und an den Kammerspielen Regie. Von 1947 bis 1955 war er Generalintendant in Düsseldorf, zunächst der "Städtischen Bühnen", dann des Düsseldorfer Schauspielhauses. Die Schallplattenfassung seiner Düsseldorfer Inszenierung des "Faust", die 1954 bei der Deutschen Grammophon auf drei Sprechplatten erschien, gilt vielen als Geburtsstunde des heutigen Hörbuches.

Ab 1955 war Gründgens Generalintendant des Deutschen Schauspielhauses in Hamburg, das er auf den Höhepunkt seines Ruhmes führte, und wo er vielbeachtete klassische und moderne Inszenierungen zeigte, eine hohe Sprechkultur pflegte und bedeutende Schauspieler um sich sammelte und prägte; genannt seien nur Elisabeth Flickenschildt, Joana Maria Gorvin und Will Quadflieg. 1960 adaptierte er seine Hamburger "Faust"-Inszenierung (mit der er auch in Moskau und New York gastierte) für den Film, mit Will Quadflieg als Faust und sich selbst in der Rolle des Mephisto – in der Maske, die sich seit den 1930er Jahren nicht verändert hatte. Dieser auch im Fernsehen gezeigte Film wurde ein großer Publikumserfolg.

Zum Sommer 1963 beendete er überraschend seine Intendanz am Deutschen Schauspielhaus und begab sich auf eine Weltreise. In der Nacht vom 6. zum 7. Oktober 1963 starb er dabei in Manila auf den Philippinen an einer Magenblutung, die von einer Überdosis Schlaftabletten ausgelöst worden war; ob es Suizid oder ein Unfall war, wurde nie eindeutig geklärt. Sein diesbezügliches Vermächtnis schrieb er auf einen Briefumschlag: 

Gründgens’ Grabstätte liegt auf dem Friedhof Ohlsdorf in Hamburg, ganz in der Nähe des Haupteinganges (am äußersten südöstlichen Rand des "Althamburgischen Gedächtnisfriedhofs"), neben dem Grab Ida Ehres, der Prinzipalin der Hamburger Kammerspiele, und in unmittelbarer Nähe zu Jürgen Fehling.


Gründgens ist als bedeutender Schauspieler und Theaterregisseur verhältnismäßig lange im öffentlichen Gedächtnis geblieben. Zahlreiche Theater-, Film- und Fernsehdarsteller hat er vor allem in seiner Hamburger Zeit stark geprägt.

Seit 1960 besaß Gründgens ein Ferienhaus auf der Insel Madeira.

Klaus Manns Exilroman "Mephisto" von 1936 beschreibt die Karriere seines ehemaligen Schwagers Gründgens, im Buch als „Hendrik Höfgen“ deutlich identifizierbar, als Musterbeispiel des Opportunismus eines Künstlers zu Anfang des Dritten Reichs. Der Roman weist, indem er jene moralisch fragwürdigen Aspekte der Karriere seines ehemaligen Freundes destilliert, eine scharfe persönliche Färbung auf.

Auf die Klage von Gründgens’ Lebensgefährten, Adoptivsohn und Alleinerben Peter Gorski hin wurde dem Nymphenburger Verlag mit der bis heute gültigen Mephisto-Entscheidung des Bundesverfassungsgerichts von 1971 die Veröffentlichung des Romans verboten. Ungeachtet der Mephisto-Entscheidung erschien im Jahre 1981 eine Neuausgabe im Rowohlt Verlag. In der DDR wurde der Roman bereits 1956 veröffentlicht.

Klaus Mann bestand darauf, keinen Schlüsselroman, sondern einen zeitgenössichen Kommentar über das Dritte Reich verfasst zu haben: „Alle Personen dieses Buches stellen Typen dar, nicht Porträts.“

Ferner entstanden über ihn:

Der auf dem Roman von Klaus Mann fußende Film "Mephisto" wurde 1981 von István Szabó mit Klaus Maria Brandauer in der Hauptrolle gedreht.

In Hamburg wurde 2011 ein mit 15.000 Euro dotierter „Gustaf-Gründgens-Preis“ ins Leben gerufen. Dieser Preis wurde durch das Ernst Deutsch Theater in Hamburg sowie Lions Clubs International initiiert und von der Mercedes-Benz Niederlassung Hamburg gestiftet. Mit dem Preis sollen Persönlichkeiten geehrt werden, die mit ihrem Lebenswerk die darstellende Kunst in Hamburg und darüber hinaus geprägt haben. Der Preis soll darüber hinaus an den bedeutenden Theatermann Gustaf Gründgens erinnern, der als Schauspieler, Regisseur und Intendant Theatergeschichte geschrieben hat. 

Bisherige Preisträger





Theaterfotos von Gustaf Gründgens liegen von 1935 bis 1963 ganz überwiegend von der bedeutenden Theaterfotografin Rosemarie Clausen, für 1947 bis 1951 auch von Liselotte Strelow vor.







</doc>
<doc id="14695" url="https://de.wikipedia.org/wiki?curid=14695" title="Dimension (Mathematik)">
Dimension (Mathematik)

In der Mathematik wird mit der Dimension ein Konzept bezeichnet, das im Wesentlichen die Anzahl der Freiheitsgrade einer Bewegung/Position in einem bestimmten Raum bezeichnet.

Der Begriff der Dimension tritt in einer Vielzahl von Zusammenhängen auf. Kein einzelnes mathematisches Konzept vermag es, die Dimension für alle Situationen zufriedenstellend zu definieren, darum existieren für verschiedene Räume auch unterschiedliche Dimensionsbegriffe.

Am bekanntesten ist die Dimension eines Vektorraums, auch "Hamel-Dimension" genannt. Sie ist gleich der Mächtigkeit einer Basis des Vektorraums. Folgende Aussagen sind hierzu äquivalent:

Beispielsweise besitzt der geometrisch anschauliche euklidische 3-Raum die Dimension 3 (Länge, Breite, Höhe). Die euklidische Ebene hat die Dimension 2, die Zahlengerade die Dimension 1, der Punkt die Dimension 0. Allgemein hat der Vektorraum formula_1 die Dimension formula_2.

Vektorräumen, die kein endliches Erzeugendensystem besitzen, kann man ebenfalls die Mächtigkeit eines minimalen Erzeugendensystems als Dimension zuordnen; es handelt sich dabei dann um eine unendliche Kardinalzahl.

Das Wort „Hamel-Basis“ wird vor allem für unendlichdimensionale Vektorräume verwendet, weil Georg Hamel als erster (mit Hilfe des Wohlordnungssatzes, also des Auswahlaxioms) die Existenz einer Basis auch in diesem Fall bewiesen hat.

Jeder Hilbertraum besitzt eine Orthonormalbasis. Nur wenn diese endlich viele Elemente hat, ist sie eine Hamel-Basis im oben definierten Sinne. Man kann zeigen, dass je zwei Orthonormalbasen gleich viele Elemente haben, und somit ist es möglich, die Dimension des Hilbertraums als die Kardinalität einer Orthonormalbasis zu definieren; es handelt sich auch hierbei um eine Kardinalzahl. Diese Kardinalzahl ist ausreichend, um Hilberträume komplett zu klassifizieren: Zu jeder Kardinalzahl gibt es bis auf Isomorphie genau einen Hilbertraum, der eine Orthonormalbasis der entsprechenden Kardinalität besitzt.

Beispiel: Der Hilbertraum formula_3 der quadratintegrierbaren Funktionen auf [0, 1] hat Hilbertraum-Dimension formula_4 – die Hamel-Dimension ist aber echt größer.

Daneben ist die Dimension einer Mannigfaltigkeit ebenfalls anschaulich einsichtig. Gemäß Definition hat jeder Punkt einer Mannigfaltigkeit eine Umgebung, die homöomorph zum formula_2-dimensionalen Euklidischen Raum ist; dieses formula_2 heißt Dimension der Mannigfaltigkeit. Um zu verhindern, dass die Dimension von der Wahl des Punktes abhängt, wird der Dimensionsbegriff üblicherweise nur für zusammenhängende Mannigfaltigkeiten verwendet oder Mannigfaltigkeiten werden von vorneherein so definiert, dass der Modellraum und damit die Dimension überall die gleichen sind.

Bekannte zweidimensionale Mannigfaltigkeiten sind die Oberfläche einer Kugel oder das Möbiusband.

Die Dimension eines abstrakten Simplex, das formula_7 Ecken enthält, ist definiert als formula_8. Die Dimension des Simplizialkomplexes formula_9 ist definiert als das Maximum der Dimension aller in formula_9 vorkommender Simplizes. Falls die Dimension der Simplizes nicht beschränkt ist, dann heißt formula_9 unendlichdimensional.

Die Dimension eines Vektorraums ist gleich der maximalen Länge (Anzahl von Inklusionen) einer Kette von ineinander enthaltenen Unterräumen. Die Sichtweise der Dimension als Kettenlänge lässt eine Verallgemeinerung auf andere Strukturen zu.

So ist etwa die Krulldimension eines kommutativen Rings als maximale Länge einer Kette von ineinander enthaltenen Primidealen minus 1 definiert.

Ebenso ist die Dimension einer Mannigfaltigkeit die maximale Länge einer Kette von ineinander enthaltenen Mannigfaltigkeiten, bei der jedes Glied der Kette Rand einer Teilmenge des vorigen ist. Zum Beispiel ist der Rand der Erdkugel die Erdoberfläche; Rand von deren Teilmenge Deutschland ist die Staatsgrenze; Rand eines bestimmten Grenzabschnitts sind die beiden Endpunkte – da es keine längere Kette gibt, hat die Erdkugel Dimension 3. Da Inklusion und Randbildung immer definiert sind, liefert dies einen Dimensionsbegriff für jeden topologischen Raum (sog. induktive Dimension). Ein gebräuchlicherer topologischer Dimensionsbegriff ist aber die Lebesguesche Überdeckungsdimension.

Ein topologischer Raum formula_12 hat die "Dimension formula_2", wenn formula_2 die kleinste natürliche Zahl ist, derart dass es zu jeder offenen Überdeckung formula_15 eine feinere offene Überdeckung formula_16 gibt, so dass jeder Punkt aus formula_12 in höchstens formula_18 der Mengen formula_19 liegt. Gibt es kein solches formula_2, so heißt formula_12 von unendlicher Dimension.

Daneben wird in der Topologie als Alternative zur Lebesgue’schen Überdeckungsdimension noch die sogenannte "Induktive Dimension" herangezogen: 

Neben den bislang angegebenen ganzzahligen Dimensionen kennt man auch verallgemeinerte, rational- oder reellzahlige Dimensionsbegriffe, mit deren Hilfe sogenannte Fraktale verglichen werden können.

Siehe Algebraische Varietät und Dimension (kommutative Algebra) (Krulldimension).

Der Begriff der Ordnungsdimension basiert auf dem Satz von Dushnik-Miller, wonach auf einer Menge formula_12 jede teilweise Ordnung als Durchschnitt von linearen Ordnungen darstellbar ist. Einer teilweise geordneten Menge formula_23 wird dann als Ordnungsdimension die kleinste Mächtigkeit eines derartigen darstellenden Systems linearer Ordnungsrelationen auf formula_12 zugeordnet.




</doc>
<doc id="14696" url="https://de.wikipedia.org/wiki?curid=14696" title="Walther Kiaulehn">
Walther Kiaulehn

Walther Kiaulehn (* 4. Juli 1900 in Berlin; † 7. Dezember 1968 in München) war ein deutscher Journalist und Schriftsteller.

Nach einer Lehre als Elektromonteur wurde Kiaulehn Journalist. 1924 arbeitete er beim "Berliner Tageblatt", von 1930 bis 1933 bei der "B.Z. am Mittag". Sein journalistisches Kürzel damals war "Lehnau", ein verkürztes Anagramm.

Ende der 1930er Jahre arbeitete er als gelegentlicher Sprecher der "Deutschen Wochenschau" und vieler NS-Kulturfilme. 1939/40 war er Soldat. Von 1940 bis 1943 diente er als Pk-Berichterstatter z. b. V. mit Sonderstatus für die propagandistische NS-Auslandsillustrierte "Signal" in Frankreich, Belgien, den besetzten Gebieten der UdSSR und auf dem Balkan. Aus Frankreich schrieb er Jubelartikel über die Zerstörung der Altstadt von Marseille, aus den besetzten Gebieten der UdSSR berichtete er über die vermeintlich vorbildliche Arbeit der deutschen Polizei- und Sicherheitsdienste. Ferner war er als Korrespondent in Belgien und auf dem Balkan eingesetzt. Den Rest des Krieges verbrachte er als Redakteur des "Signal" in Berlin. Ab 1944 war er hier in erster Linie für die Durchhaltepropaganda mit Berlinbezug in der Illustrierten verantwortlich. Einer seiner letzten Beiträge feierte im Februar 1945 Panzerfaust und Volkssturm: Der "Widerstand" dieser "revolutionären Truppe" werde, so Kiaulehn, "in der Erinnerung der Menschen unvergeßlich sein". 1943 wurde er überdies vom Propaganda-Ministerium für Drehbucharbeiten zu NS-Kulturfilmen unabkömmlich (uk) gestellt. Kiaulehn wurde 1947 vom französischen Autor und Résistancekämpfer Vercors seine Mitarbeit "im verbreitetsten Hitlerorgan während der deutschen Besetzung in Frankreich" vorgeworfen.

Nach 1945 zog Kiaulehn nach München. Dort arbeitete er zuerst als Schauspieler und als Kabarettist. Er spielte u. a. an der „Kleinen Komödie“ und im „Volkstheater“. 1946 spielte er in einem Programm des Kabaretts „Die Schaubude“, das er auch gleichzeitig conferierte. Er bestritt gemeinsam mit Werner Finck und Hellmuth Krüger „Das Schmunzelcolleg“. Nebenbei spielte er in verschiedenen Nachkriegsfilmen. Seine anfänglichen Versuche, bei der "Neuen Zeitung" journalistisch wieder Fuß zu fassen, scheiterten zunächst am Widerstand von Hans Habe, obwohl er von Erich Kästner protegiert wurde. Erst nach Habes Ablösung hatte Kiaulehn Erfolg. Anfang der 1950er Jahre wurde er Feuilletonchef beim "Münchner Merkur", wo er bis zu seinem Tod 1968 als leitender Redakteur und Theaterkritiker arbeitete.


Als Schauspieler:

Als Drehbuchautor




</doc>
<doc id="14697" url="https://de.wikipedia.org/wiki?curid=14697" title="Interpreter">
Interpreter

Ein Interpreter (im Sinne der Softwaretechnik) ist ein Computerprogramm, das einen Programm-Quellcode im Gegensatz zu Assemblern oder Compilern nicht in eine auf dem System direkt ausführbare Datei übersetzt, sondern den Quellcode einliest, analysiert und ausführt. Die Übersetzung des Quellcodes erfolgt also zur Laufzeit des Programmes. Der Übergang zu Kommandozeileninterpretern ist unscharf, da diese häufig auch eine entsprechende Skriptsprache interpretieren können.

Auch Emulatoren gehören zu den Interpretern, da diese den Maschinencode des Gastsystems befehlsweise auf einem virtuellen Prozessor abarbeiten. Nicht dazu gehören viele virtuelle Maschinen, da diese Teile des Maschinencodes des Gastsystems auf dem Hostsystem ausführen können; dazu ist Binärkompatibilität der Prozessoren erforderlich.

Interpreter liegen zumeist in Maschinensprache des Zielprozessors vor, können aber auch selbst wieder in einer Interpretersprache vorliegen (siehe Beispiel unten) oder von einem Emulator interpretiert werden. Dies ist beliebig schachtelbar und wirkt sich dementsprechend auf die Abarbeitungsgeschwindigkeit aus.

Ein Nachteil der Interpretersprachen ist die im Vergleich zu kompilierten Programmen deutlich langsamere Ausführungsgeschwindigkeit. Reine Interpreter lesen und analysieren den Quellcode eines Programmes und führen dann die entsprechenden Aktionen aus. Dies ist im Vergleich zu Compilersprachen, bei denen das Programm vor seiner Ausführung in Maschinencode übersetzt wird, der dann vom Prozessor direkt ausgeführt wird, vergleichsweise zeitaufwändig. Der Vorteil liegt darin, dass reine Interpreter auf jeder Rechnerarchitektur lauffähig sind, wenn der Quellcode des Interpreters dort übersetzt werden kann.

Eine Kompromisslösung ist ein Just-in-time-Compiler (JIT-Compiler), bei dem das Programm erst zur Laufzeit, jedoch direkt in Maschinencode übersetzt wird. Danach wird der übersetzte Code direkt vom Prozessor ausgeführt. Durch Zwischenspeicherung des Maschinencodes müssen mehrfach durchlaufene Programmteile nur einmal übersetzt werden. Auch ermöglicht der JIT-Compiler eine stärkere Optimierung des Binärcodes. Allerdings sind solche Interpreter natürlich nur auf einer bestimmten Rechnerarchitektur lauffähig, weil sie Maschinencode für diese Architektur erzeugen.

Eine weitere Zwischenstufe sind Bytecode-Interpreter. Dabei wird der Quelltext (vorab oder zur Laufzeit) in einen einfachen Zwischencode übersetzt, der dann von einem Interpreter – auch häufig als virtuelle Maschine bezeichnet – ausgeführt wird.

Besonders in den 1980er Jahren benutzte man die Zwischenstufe, Befehle zum Eingabezeitpunkt in leichter dekodierbare Tokens umzuwandeln, die bei der (List-)Ausgabe wieder in Klartext umgewandelt wurden. Neben der Geschwindigkeitssteigerung war die Kompression des Quelltextes ein gewichtiges Argument. Prinzipiell war es damit auch möglich, jeweils muttersprachliche Schlüsselwörter zu verwenden, wenn man den Datenaustausch auf Basis des tokenisierten Quellprogramms durchführte.

Als Interpretersprachen werden häufig Programmiersprachen bezeichnet, deren Haupt- oder Erstimplementierung ein Interpreter ist, als Gegenteil wird meist der Begriff Compilersprache verwendet. Der Begriff Interpretersprache ist jedoch nicht sehr differenziert, da keine Einigkeit darüber herrscht, wann eine Programmiersprache eine Interpretersprache ist.

Es gibt jedoch auch Programmiersprachen, die unter Gesichtspunkten der späteren Implementierung gestaltet wurden; dies ist bei manchen älteren Sprachen noch gut zu erkennen. So mussten Interpreter aufgrund der geringen Leistungsfähigkeit der frühen Computer möglichst einfach und klein gehalten werden, um nicht zu viel Rechenzeit und Arbeitsspeicher zu verbrauchen. Compiler hingegen konnten viel Rechenzeit und auch viel Arbeitsspeicher verbrauchen, denn wenn das Programm lief, waren sie nicht mehr aktiv.
Deshalb wurden Sprachen, die interpretiert werden sollten, so gestaltet, dass sie einfach analysiert und ausgeführt werden können. Wohingegen Sprachen, die compiliert werden sollten, auch aufwändig zu analysierende und bearbeitende Konstrukte enthalten konnten. Heute spielt dies beim Entwurf einer Programmiersprache nur noch in den allerseltensten Fällen eine Rolle.

Grundsätzlich ist eine Programmiersprache nicht an eine Art der Implementierung gebunden.

Für einige Sprachen existieren verschiedenartige Implementierungen. Hierbei sticht die Sprache Scheme hervor, für die eine unüberschaubare Vielzahl an Implementierungen existiert, die auf vielen verschiedenen Konzepten basieren. Hierzu noch ein Beispiel: Die Programmiersprache C, ist sehr stark darauf ausgelegt compiliert zu werden. Doch es existieren trotzdem Interpreter wie der CINT und der Ch für diese Sprache und das, obwohl C oft als ein Paradebeispiel für eine Sprache genannt wird, die keine „Interpretersprache“, sondern eine „Compilersprache“ ist.

Als Interpretersprachen bekannt sind APL, BASIC, Forth, Perl, Python, Ruby, PHP und viele andere. Als eine Unter- oder verwandte Kategorie der Interpretersprachen werden manchmal die Skriptsprachen genannt.

Bekannte Programmiersprachen, die üblicherweise in Bytecode übersetzt werden, sind Java, C#, Perl und Python.

Für manche Sprachen (etwa Smalltalk) gibt es je nach Anbieter Interpreter, Bytecode-Interpreter, JIT-Compiler oder Compiler in andere Sprachen (beispielsweise nach C oder .NET).

Der Übergang zwischen reinen Interpretern und reinen Compilern ist fließend.


</doc>
<doc id="14698" url="https://de.wikipedia.org/wiki?curid=14698" title="Ernst Rowohlt">
Ernst Rowohlt

Ernst Rowohlt (* 23. Juni 1887 in Bremen; † 1. Dezember 1960 in Hamburg), mit vollständigem Namen Ernst Hermann Heinrich Rowohlt, war ein deutscher Verleger. 1908 gründete er seinen Verlag, den Rowohlt Verlag, in Leipzig zum ersten Mal, zwei weitere Verlagsgründungen sollten folgen.

Ernst Hermann Heinrich Rowohlt wurde als Sohn des Maklers Heinrich Rowohlt und seiner Frau Anna Dorothea, geb. von Hunteln, 1887 in Bremen geboren. Er hatte zwei Schwestern, Maria und Margarethe. 1903 erreichte er die Obersekundarreife und absolvierte nach Beendigung seiner Schulzeit eine Lehre im Bankhaus Carl F. Plump & Co. in Bremen. Anschließend wurde er Volontär in der Druckerei Breitkopf & Härtel in Leipzig, der damaligen Metropole des Buchhandels, wo Rowohlt unter anderem die Bereiche Buchdruck und Buchbinderei kennenlernte. Die Anstellung bekam er durch die Vermittlungsbemühungen Anton Kippenbergs, des Leiters des Insel-Verlags (von 1905 bis 1950).

1908 gründete er den Rowohlt Verlag Paris-Leipzig, mit dem er nach Leipzig in das Vorderhaus der Offizin Drugulin in der Königstraße 10 zog und sein erstes Werk, Gustav C. Edzards "Lieder der Sommernächte" veröffentlichte. 1909 erschien das zweite Buch Rowohlts, Paul Scheerbarts "Kater-Poesie". Weiter erschienen im ersten Ernst Rowohlt Verlag beispielsweise Werke von Herbert Eulenberg, Hugo Ball, Max Dauthendey, Georg Heym, Carl Hauptmann, Max Brod, Franz Kafka, Mechtilde Lichnowsky, Hermann Harry Schmitz oder Arnold Zweig. Kurt Pinthus und Walter Hasenclever wurden Lektoren des Verlages. 1912 trennten sich Rowohlt und Kurt Wolff, der im Juli 1910 stiller Teilhaber geworden war und der nun den Verlag übernahm. Wolff erwarb für 15.000 Mark die Verlagsrechte unter anderem an Johannes R. Becher, Max Brod, Georg Heym, Franz Kafka und Stefan Zweig und benannte den Verlag im Februar 1913 in Kurt Wolff Verlag um. Im Jahr 1913 war Rowohlt Prokurist im S. Fischer Verlag und Geschäftsführer des Hyperion-Verlags, Berlin.

Zu Beginn des Ersten Weltkrieges meldete sich Rowohlt freiwillig zum Dienst in der Armee und blieb bis zum Kriegsschluss im aktiven Dienst.

Nach seiner Rückkehr nach Berlin gründete er den zweiten Rowohlt-Verlag, der von 1919 bis 1943 bestand und in dem Paul Mayer und Franz Hessel als Lektoren arbeiteten. Wichtige Autoren in den zwanziger Jahren waren Alfons Goldschmidt, Kurt Pinthus, Carl Ludwig Schleich, Franz Blei, Honoré de Balzac, Heinrich Eduard Jacob, Alfred Polgar und Kurt Tucholsky. Ab 1923 wurde Emil Ludwig der Erfolgsautor des Hauses (u. a. mit "Napoleon" und "Juli 14"). Seit 1928 nahm der Verleger auch zeitgenössische amerikanische Literatur in sein Programm auf, wie beispielsweise Sinclair Lewis’ "Elmer Gantry", Ernest Hemingways "Fiesta" oder Thomas Wolfes "Schau heimwärts, Engel! " und "Von Zeit und Strom".

Zu Beginn der 1930er Jahre wurde dem Geschäft der Auslands- und Feuilletondienst unter der Leitung Peter Zinglers angegliedert. Allerdings geriet der Betrieb in finanzielle Nöte, so dass zwei Drittel der Anteile an den Ullstein Verlag übergingen. Durch Hans Falladas Erfolgsbuch "Kleiner Mann – was nun?", das in Deutschland und den USA verfilmt wurde, konnte der Verlag wieder Auftrieb gewinnen.

Nach der Machtergreifung der Nationalsozialisten wurden 50 Prozent (46 Werke) der lieferbaren Verlagswerke verboten, beschlagnahmt und verbrannt. 1936 führte das Buch "Adalbert Stifter" von Urban Roedl (Pseudonym für Bruno Adler) zum Berufsverbot von Ernst Rowohlt, da dem Verleger vorgehalten wurde, jüdische Schriftsteller zu tarnen, was auch zutraf. 1938 wurde er aus der Reichsschrifttumskammer ausgeschlossen. Seine Lektoren Mayer und Hessel gingen in die Emigration, und auch der Verleger reiste mit seiner Familie aus Deutschland über Zürich, Paris, London und Rotterdam nach Rio Grande in Brasilien. Trotzdem blieb Rowohlt Mitglied der NSDAP, der er 1937 beigetreten war.

Unterdessen wurde sein Unternehmen der Stuttgarter Deutschen Verlags-Anstalt als Tochtergesellschaft angegliedert und von seinem Sohn Heinrich Maria Ledig weitergeführt. Kurt Kusenberg konnte 1940 mit dem Titel "La Botella und andere seltsame Geschichten" verlegt werden.

Ernst Rowohlt kehrte nach einer 57-tägigen Reise Ende Dezember 1940 auf dem Blockadebrecher "Rio Grande" aus Brasilien in sein Heimatland zurück und wurde am 10. Februar 1941 Hauptmann bei der Wehrmacht in einer Propagandakompanie. Dafür war er zuerst in Griechenland (Kap Sunion). 1942 wurde er an die Kaukasusfront verlegt, musste die Armee Ende Juni des folgenden Jahres jedoch wegen „politischer Unzuverlässigkeit“ verlassen, da eine Petition von 1927 für Max Hoelz auch seine Unterschrift trug. Der Reichsleiter Amann beantragte 1943 die Schließung der Firma in Stuttgart.

Der dritte Rowohlt-Verlag konnte 1946 in Stuttgart wieder ins Leben gerufen werden, als Heinrich Maria Ledig die Verlagslizenz von den Amerikanern für die Wiedereröffnung erhielt. Erste Autoren waren Erich Kästner, Joachim Ringelnatz und Kurt Tucholsky. Außerdem wurden die Zeitschriften "Pinguin" und "story" hier veröffentlicht.

Am 27. März erhielt Ernst Rowohlt von den Engländern die Lizenz für den Verlag in Hamburg. Vier Jahre später siedelte die Stuttgarter Firma nach Hamburg über. Im selben Jahr kam es zu den ersten vier Ausgaben der rororo-Taschenbücher ("Rowohlt-Rotations-Romane"), die nun monatlich erscheinen sollten. Nach einer finanziell bedrohlichen Phase durch die Währungsreform 1948 konnte sich der Verlag langsam wieder erholen.

In den folgenden Jahren wurden Werke von Wolfgang Borchert, Walter Jens, Dieter Meichsner, Gregor von Rezzori, Arno Schmidt, Ernest Hemingway und Ernst von Salomon verlegt.

1951 erlitt Rowohlt seinen ersten Herzinfarkt, blieb aber weiterhin im Betrieb tätig. 1954 wurde er Mitbegründer und Präsident der „Knut-Hamsun-Gesellschaft“ und erhielt drei Jahre später zu seinem 70. Geburtstag das Große Bundesverdienstkreuz. Im selben Jahr bekam er die Ehrendoktorwürde der Universität Leipzig verliehen.

Der Verleger engagierte sich politisch in der DFU.

Am 1. Dezember 1960 starb Ernst Rowohlt an den Folgen eines Herzinfarktes. Er wurde auf dem Friedhof Volksdorf beigesetzt. Auf der Fotografie sind auch die Grabsteine seiner Mutter sowie die seiner Frau Maria (geb. Pierenkämper) zu erkennen.


Ernst Rowohlt hatte drei Kinder: Heinrich Maria Ledig-Rowohlt (1908–1992) (der aus der Verbindung mit der Schauspielerin Maria Lee (Ledig) hervorging), Anna Elisabeth (1930–ca. 1975), verheiratet mit Günter Steffens, und Harry Rowohlt (1945–2015).

Sein ältester Sohn Heinrich Maria Ledig-Rowohlt übernahm die Mehrheit der Verlagsanteile und leitete das Unternehmen, das 1960 nach Reinbek bei Hamburg verlegt worden war, bis 1982 weiter. Ernst Rowohlts jüngerer Sohn Harry arbeitete vor allem als Schauspieler und freier Übersetzer.




</doc>
<doc id="14699" url="https://de.wikipedia.org/wiki?curid=14699" title="Rhode Island (Insel)">
Rhode Island (Insel)

Rhode Island (inoffiziell auch oft Aquidneck Island) ist die größte Insel der Narragansett Bay und gehört zum County Newport des US-Bundesstaates Rhode Island.

Die Insel ist 24 km lang und 8 km breit und besitzt eine Bevölkerung von etwa 60.000 Einwohnern. Der größte Ort auf der Insel ist die Stadt Newport, der Rest verteilt sich auf die Gemeinden Middletown und Portsmouth. Die Insel ist über mehrere Brücken aus vom Festland erreichbar. Im Norden verbindet die Mount Hope Bridge die Insel mit der Stadt Bristol, im Nordosten bildet Sakonnet River Bridge eine Verbindung zum Festland. Südlich der Sakonnet River Bridge verlief mit der Stone Bridge früher eine weitere Brücke, diese wurde jedoch 1954 durch den Hurrikan Caroll zerstört, seitdem wird sie durch eine Fährverbindung ersetzt. Auf der Westseite der Insel verbindet die Claiborne Pell Newport Bridge sie mit der Nachbarinsel Conanicut Island, welche durch eine weitere Brücke mit dem Festland verbunden ist. 

Auf der Insel befinden sich mit der Salve Regina University und dem Naval War College auch zwei Hochschulen.



</doc>
<doc id="14700" url="https://de.wikipedia.org/wiki?curid=14700" title="Kohlensäure">
Kohlensäure

Kohlensäure (HCO) ist eine anorganische Säure und das Reaktionsprodukt ihres Säureanhydrids Kohlendioxid (CO) mit Wasser.
Die Salze der zweiprotonigen Säure sind die Carbonate und Hydrogencarbonate.
Ihre Ester werden ebenfalls Carbonate oder Kohlensäureester genannt. Technische Bedeutung haben die Polyester, die als Polycarbonate bezeichnet werden.

Das Gas CO ist im Vergleich zu O und N relativ gut löslich in Wasser und reagiert zu einem geringen Anteil (etwa 0,2 %, je nach Temperatur) zu Kohlensäure:

Das Gas CO wird umgangssprachlich oft ungenau als Kohlensäure bezeichnet.
Tatsächlich wird in der Wasserchemie gelöstes CO mit der eigentlichen Säure HCO üblicherweise als "freie Kohlensäure" zusammengefasst.
Sie steht der Summe von Carbonat und Hydrogencarbonat als der "gebundenen Kohlensäure" gegenüber.

Kohlensäure spielt eine wichtige Rolle im Säure-Basen-Haushalt sowohl des Wassers als auch des Blutes und der Körperflüssigkeiten.

Die Natur der Kohlensäure als gelöstes Kohlendioxid erkannte 1741 William Brownrigg. Für die Erfindung des Sodawassers ist allerdings Joseph Priestley bekannt (in seiner Zeit als Priester in Leeds ab 1767, wo ihm genug Kohlendioxid aus einer nahen Brauerei zur Verfügung stand).

Gelöstes Kohlenstoffdioxid steht in wässriger Lösung im Gleichgewicht mit Kohlensäure:

Die Erlenmeyer-Regel beschreibt die Instabilität von Molekülen mit zwei Hydroxygruppen am selben Kohlenstoff-Atom. Daher liegt das Gleichgewicht sehr weit auf der Seite des Anhydrids; der Anteil des Säuremoleküls liegt in wässriger Lösung bei nur rund 0,2 %. Dieser Anteil ist mäßig von der Temperatur abhängig. In Organismen wird die Reaktion durch das Enzym Carboanhydrase beschleunigt. Die Kohlensäure ist eine zweiprotonige Säure. Sie gibt daher ihre Protonen in zwei Dissoziationsstufen an Wasser oder andere Basen ab:

Der pK-Wert der ersten Säurekonstante kann lediglich berechnet werden. Er liegt mit temperaturabhängigen Abweichungen eigentlich bei ca. 3,6, Kohlensäure ist damit eine mittelstarke Säure vergleichbar mit Essigsäure (pK 4,76) und Ameisensäure (pK 3,77). Da aber der Anteil der Kohlensäure gemäß Gleichung (1) schlecht zu bestimmen ist, werden die Reaktionen (1) und (2) zu (3) zusammengefasst:

und ergeben den (fast immer genannten und experimentell bestimmbaren) Wert von ca. 6,5 für den pK-Wert. Freie Kohlensäure ist damit eine schwache Säure. Reaktionsprodukt ist das Hydrogencarbonat-Ion HCO.

Der pK-Wert für die zweite Säurekonstante liegt um 10,5. Reaktionsprodukt ist das Carbonat-Ion CO.

Die Konzentrationen der drei (eigentlich vier) Kohlensäure-Spezies, also der freien Kohlensäure (HCO und gelöstes CO), des Hydrogencarbonats und des Carbonats sowie der Oxoniumionen stehen miteinander durch das Massenwirkungsgesetz in einem berechenbaren Zusammenhang.
Die Konzentration der Oxoniumionen wird durch den pH-Wert ausgedrückt. Bei einem gegebenen pH-Wert ist somit das Mengenverhältnis der Spezies festgelegt.

Diese Eckwerte spiegeln die Zusammenhänge im vielfach genutzten Bicarbonat-Puffer wider.

Kohlensäure wird für unzählige Produktionsprozesse weltweit eingesetzt, wobei sie dem Endverbraucher wohl am ehesten aus Erfrischungsgetränken bekannt sein dürfte.
Jacob Schweppe entwickelte im späten 18. Jahrhundert ein Verfahren, mittels dessen sich Wasser mit Kohlenstoffdioxid versetzen lässt.
Im 19. Jahrhundert begann man, Mineralwasser Kohlenstoffdioxid beizumischen, um dieses haltbar zu machen.

Im Labor ist es gelungen, Kohlensäure (im engeren Sinn) als Reinsubstanz zu erzeugen.
Das ist jedoch – praktisch betrachtet – eher eine wissenschaftliche Kuriosität.
Bei tiefen Temperaturen und unter absoluter Abwesenheit von Wasser oder Metallionen (beide katalysieren stark die Zersetzungsreaktion zu Kohlenstoffdioxid und Wasser) kann die Kohlensäure HCO als wasserklare, farblose Flüssigkeit dargestellt werden.

Darüber hinaus sind organische Derivate der Kohlensäure bekannt, so verschiedene Kohlensäureester. Sie sind leicht zugänglich durch die Reaktion von Phosgen mit Alkoholen. Von technischer Bedeutung sind vor allem die Amide der Kohlensäure. Ihre Stammverbindung ist der Harnstoff, ein Diamid der Kohlensäure; als Beispiel seien die Urethane (von "Urea", Harnstoff) genannt. Es sind substituierte Ester des Monoamids der Kohlensäure, der Carbaminsäure; diese sind die Stammverbindungen überaus wichtiger Kunststoffe, der Polyurethane.

Eine weitere Gruppe von Trivialnamen, die "nicht" chemisch unterschiedliche "Spezies", sondern "Mengenanteile" bezeichnen, stammt aus dem Bereich der Wasserchemie für kalkhaltige Wässer.
Es sei darauf hingewiesen, dass die folgenden Begriffe jeweils Mengenanteile der sogenannten "freien Kohlensäure" betreffen, bei denen zwischen Kohlenstoffdioxid und der Kohlensäure im engeren Sinn nicht unterschieden wird.

Entsprechend dem Kalk-Kohlensäure-Gleichgewicht sind die Konzentrationen von Calcium und Kohlensäure voneinander abhängig.
Man unterscheidet die Menge der "zugehörigen" Kohlensäure von der Menge der "überschüssigen" und der "(kalk-)aggressiven Kohlensäure".
"Zugehörige" Säure hält im Mengengleichgewicht der Kohlensäurespezies den pH-Wert gerade so niedrig, dass die hiervon abhängige Konzentration des Carbonates multipliziert mit der des Calciums gerade noch nicht das Löslichkeitsprodukt des Calciumcarbonats überschreitet.
Darüber hinaus vorhandene freie Kohlensäure gilt als überschüssig.
Davon wiederum ein Teil könnte weiteren Kalk in Lösung bringen, ist also "(kalk-)aggressiv";
der Rest des Überschusses würde danach als zusätzliche zugehörige Kohlensäure benötigt.

Mit steigendem Werten für die Carbonathärte steigt der Anteil der zugehörigen freien Kohlensäure überproportional an. Beispielsweise beträgt dieser Wert bei 5,1 °dH 1,83 mg/l CO und bei 10,2 °dH 11,67 mg/l CO. Dies führt bei der Mischung von Wässern zu einem "Mischwasserproblem". Die Mischung von Wässern mit unterschiedlicher Carbonathärte ergibt Mischwässer mit aggressiver Kohlensäure, selbst wenn die Ausgangswässer im Kalk-Kohlensäure-Gleichgewicht waren.

Die mathematischen Zusammenhänge sind in der "Tillmans"schen Gleichung zusammengefasst, mit der die "zugehörige" „freie Kohlensäure“ für jeden Gehalt an Calcium berechnet werden kann. Nachfolgend die Kurzfassung dieser Gleichung:

Die Elemente der Gleichung bedeuten:

Weitere Einzelheiten hierzu unter Tillmanssche Gleichung.

Für die Aufbereitung und Entkarbonisierung von Wässern ist die genaue Kenntnis dieses Gleichgewichtes und seiner Einstellgeschwindigkeit von großer Bedeutung. So wird bei der Trinkwasseraufbereitung das Rohwasser über halbgebranntes Dolomit (Calciummagnesiumcarbonat, CaMg(CO)) geleitet, damit es keine "überschüssige" „freie Kohlensäure“ enthält, da Eisen oder andere Metalle mit dieser reagieren und so beispielsweise Rohrleitungen aus Stahl korrodieren würden. Auch diese Reaktionen sind konzentrationsabhängig im Gleichgewicht mit entsprechenden Carbonaten. Deshalb spricht man dann z. B. von „Eisen-aggressiver Kohlensäure“. Dolomit wird verwendet, weil in Anwesenheit von Magnesiumionen die Einstellgeschwindigkeit des Tillmans’schen Gleichgewichtes erheblich beschleunigt wird, was mit reinem Calciumcarbonat viel zu lange dauern würde.

Bei manchen Anwendergruppen, z. B. in der Fischerei, werden die hier genannten Mengenbegriffe oft so "missverstanden", als ob z. B. die „aggressive Kohlensäure“ besonders schädlich wäre, etwa für die Fische. Da aber Fische nicht aus Kalk bestehen, richtet sich die aggressive Kohlensäure nicht in anderer Weise gegen sie als der Rest der Kohlensäure. Für die Atmung der Fische ist vielmehr die gesamte gelöste CO-Konzentration ausschlaggebend, für eine allfällige sauere Verätzung ausschließlich der pH-Wert des Wassers. Die „zugehörige Kohlensäure“ wird dort so missverstanden, als ob sie in besonderer Weise an das Hydrogencarbonat gebunden und deshalb nicht durch Wasserbelüftung auszutreiben oder durch Photosynthese von Algen zu verbrauchen wäre. Tatsächlich steht beiden Vorgängen die gesamte „freie Kohlensäure“ zur Verfügung, so dass es zu einer Steigerung des pH-Wertes, dadurch zu einer Verschiebung des Mengengleichgewichts hin zu mehr Carbonat und dadurch schließlich zu einer Überschreitung des Löslichkeitsproduktes des Kalkes, also zu einer Kalkfällung kommt.



</doc>
<doc id="14702" url="https://de.wikipedia.org/wiki?curid=14702" title="Middletown (Rhode Island)">
Middletown (Rhode Island)

Middletown ist eine kleine ländliche Stadt im Newport County, im Südosten des US-Bundesstaates Rhode Island auf der gleichnamigen Insel.

Der Name Middletown kommt von der geographischen Lage zwischen Newport und Portsmouth. Seit 1731 ist Middletown kein Teil mehr von Newport. Während des Amerikanischen Unabhängigkeitskriegs wurde das Städtchen 1776 von den Engländern geplündert.



</doc>
<doc id="14703" url="https://de.wikipedia.org/wiki?curid=14703" title="Aufmerksamkeit">
Aufmerksamkeit

Aufmerksamkeit ist die Zuweisung von (beschränkten) Bewusstseins­ressourcen auf Bewusstseinsinhalte. Das können z. B. Wahrnehmungen der Umwelt oder des eigenen Verhaltens und Handelns sein, aber auch Gedanken und Gefühle. 

Als Maß für die Intensität und Dauer der Aufmerksamkeit gilt die Konzentration. Aufmerksamkeit, die auf das Eintreffen bestimmter Ereignisse gerichtet ist, bezeichnet man als "Vigilanz".

Das Phänomen der Aufmerksamkeit rückte aufgrund des technischen Fortschritts im Zweiten Weltkrieg in den Forschungsfokus: Die Soldaten waren häufig nicht in der Lage, die neuen Geräte adäquat zu bedienen, obwohl sie daran geschult waren. Das Gehirn hat eine eingeschränkte Verarbeitungskapazität, es kann nicht sehr viele Reize gleichzeitig verarbeiten. Daher muss es selektieren, welche Informationen für den Organismus von Bedeutung sind und mit Aufmerksamkeit bedacht werden müssen und welche Informationen weniger relevant sind und daher ausgeblendet werden können. Einige Reize ziehen automatisch Aufmerksamkeit auf sich (zum Beispiel ein Knall), andererseits kann die Aufmerksamkeit absichtlich gesteuert werden. Wird einer Information nicht innerhalb von fünf Sekunden Aufmerksamkeit geschenkt, geht sie verloren (zum Ultrakurzzeitgedächtnis siehe sensorisches Gedächtnis).

Der Prozess der Aufmerksamkeitszuwendung ist dabei gekennzeichnet durch Zuwendung (Orientierung) und Auswahl (Selektivität) der Gegenstände und der damit verbundenen Unaufmerksamkeit gegenüber anderen Gegenständen. Die Zuwendung ist durch eine gesteigerte Wachheit und Aktivierung charakterisiert, während die Selektivität die Funktion eines Filters hat, um wichtige und unwichtige Informationen voneinander zu trennen.

Vom Gehirn als relevant eingestuft werden zuallererst Gefahrensignale, außerdem Unbekanntes. So werden einerseits neuartige Reize mit Aufmerksamkeit bedacht (Orientierungsreaktion, Neugier). Andererseits richtet sich die Aufmerksamkeit auf emotional belegte Informationen, die ein indirekter Marker für die Wichtigkeit für den Organismus sind. Je emotionsgeladener eine Wahrnehmung ist, desto leichter fällt es uns, unsere Aufmerksamkeit darauf zu richten. Bedürfnisse, Interessen, Einstellungen und Motive spielen daher bei der Entstehung und Verteilung der Aufmerksamkeit eine große Rolle.

Die Aufmerksamkeit ist eng mit unserem Bewusstsein verbunden: Die Zuwendung der Aufmerksamkeit zu einem Reiz oder einem Gedanken ist erst die notwendige Bedingung dafür, dass uns dieser bewusst wird. Dennoch verarbeitet das Gehirn auch Reize, auf die wir nicht unsere Aufmerksamkeit richten. Diese Verarbeitung findet jedoch unbewusst statt.

Regelmäßige bewusste Lenkung der Aufmerksamkeit auf einzelne Körperteile oder den gesamten Körper führt zu einer besseren Durchblutung, einer Stärkung des Immunsystems und allgemein zu einem verbesserten Gesundheitszustand. Dies wird u. a. im Taijiquan und Yoga zur Gesunderhaltung genutzt.

Bestimmte Ereignisse im phänomenalen Erlebnisraum verursachen eine Fokussierung der Aufmerksamkeit auf einzelne Objekte des Wahrnehmungsbereiches. Zumeist erfolgt diese Aufmerksamkeitsfokussierung, wenn kein eindeutiges Reaktionsmuster auf einen Reiz existiert und bewusste Verarbeitung notwendig wird. Indem die Wahrnehmung sich mit einem reduzierten Wahrnehmungsbereich beschäftigt, ergibt sich zugleich die Abgrenzung gegen andere Aufmerksamkeitsauslöser niedrigerer Priorität.

Die Zuwendung der Aufmerksamkeit hängt von bestimmten Eigenschaften der Objekte ab, vor allem vom Ausmaß der Abweichung von einer Mittellage:

Von der Werbeindustrie werden diese Zusammenhänge genutzt, um Werbung optimal zu gestalten, z. B. Plakate, Inserate oder Prospekte.

Der Umfang der visuellen Aufmerksamkeit wird durch die Anzahl gleichartiger Gegenstände bestimmt, die mit einem Blick, d. h. in etwa 200 Millisekunden wahrgenommen werden können. Beim Erwachsenen sind das 6 bis 12, im Mittel 8 Objekte, bei Kindern weniger. Der Aufmerksamkeitsumfang hängt auch ab von:

Es ist nahezu unmöglich, gleichzeitig einen optischen und einen taktilen Reiz zu beurteilen, wie Richard Pauli (1914) zeigte. Das stützt auch die als "Enge des Bewusstseins" bezeichnete Annahme, dass sich die Aufmerksamkeit jeweils nur einem Inhalt zuwenden kann (von Michael Posner als "spotlight"-(Scheinwerfer)-Modell bezeichnet.) Mehrfachleistungen beruhen offenbar auf einem schnellen Wechsel der Zuwendung von einer Aufgabe zu einer anderen. Das ist anstrengend und führt rasch zur Ermüdung. Diese Ermüdung der Aufmerksamkeit und der rasche Wechsel verschiedener Aufmerksamkeitstypen (von auditiv zu visuell usw.) machen sich auch die so genannten Pfänderspiele zunutze, die aber auch ein gutes Training derselben bedeuten.

Bei sehr schwachen Reizen, z. B. beim leisen Ticken einer entfernten Armbanduhr, sind periodische Schwankungen der Aufmerksamkeit nachweisbar. Viktor Urbantschitsch (1875) stellte eine Phasenlänge von 5 bis 8 Sekunden fest. Individuelle Besonderheiten des aufmerksamen Verhaltens führten zur Unterscheidung von Aufmerksamkeitstypen:


Es wird von "fluktuierender" Aufmerksamkeit gesprochen, wenn sich die Aufmerksamkeit einer Person nicht auf einen bestimmten Reiz oder ein Detail richtet, sondern rasch von einem Reiz zum nächsten gleitet. Auf fluktuierende Art aufmerksam zu sein, bedeutet, sich einen Überblick zu verschaffen. Es werden viele verschiedene Objekte oder Reize in kurzer Zeit wahrgenommen, sodass sie ein Gesamtbild ergeben. Während die "fixierende" Aufmerksamkeit analytisch ist, da sie die Wahrnehmung einzelner Objekte und ihre Zerlegung bis ins Detail begünstigt, ist die fluktuierende Aufmerksamkeit synthetisch. Das Wahrnehmungsspektrum ist weit und die einzelnen Eindrücke werden miteinander verbunden.

Beispiele für Befindlich- und Tätigkeiten, die tendenziell eine fluktuierende Aufmerksamkeit begünstigen, finden sich etwa beim Aufräumen, bei der Teilnahme am Straßenverkehr, bei der Interaktion mit größeren Gruppen oder in der Umgebung von Menschenmengen.

Seit Ernst Meumann (1913) unterscheidet man bei Bevorzugung bestimmter Sinnesgebiete visuelle, auditive und motorische Aufmerksamkeit.

Zur Erklärung der Aufmerksamkeit wurden zahlreiche Theorien aufgestellt. Die Erklärungsversuche durch Gottfried Wilhelm Leibniz (1704) und Wilhelm Wundt (1873) gehen von der Annahme aus, die Aufmerksamkeit sei ein "innerer Willensprozess" und diene der selektiven Ausgliederung von Bewusstseinsinhalten und der Apperzeption von Vorstellungen. Die Theorien von Georg Elias Müller (1924), H. Henning (1925) und H. Rohrbacher (1953) nehmen im Zentralnervensystem physiologische Mechanismen an, die eine spezifische Erregbarkeitssteigerung bestimmter Bereiche der Hirnrinde und Bahnungseffekte bewirken.

Die Gestaltpsychologen negieren die Aufmerksamkeit als eigenständigen Prozess. Pjotr Jakowlewitsch Galperin (1968) betrachtete die Aufmerksamkeit als eine besondere Form der psychischen Tätigkeit, nämlich als Kontrolltätigkeit, die den Vollzug geistiger Handlungen steuert.

Modernere Modelle gehen von verschiedenen Filtersystemen des Wahrnehmungssystems aus (z. B. Donald Broadbent 1958), die an unterschiedlichen Stellen des Wahrnehmungsprozesses eingreifen und die Information selektieren. So wird die Aufmerksamkeit bei starker persönlicher Relevanz automatisch fokussiert (Beispiel Cocktailparty-Effekt: Im Stimmengewirr kann man sich bewusst auf eine Stimme fokussieren; wird der eigene Name auf einer lauten Party genannt, zieht dies automatisch die Aufmerksamkeit auf sich). Ähnliches gilt für den so genannten Pop-out-Effekt: Auf einer Fläche mit gleichförmigen geometrischen Figuren (z. B. Strichen) fällt eine andersartige Figur (Kreis) sofort ins Auge. Dieser Effekt ist bis zu einer gewissen Komplexität und Ähnlichkeit der geometrischen Figuren trainierbar, und es gibt diesen Effekt nicht nur in ähnlicher Weise auf Farben (Textilfacharbeiter können bis zu 300 Rottöne unterscheiden), Töne usw., sondern auch auf semantischer Ebene (z. B. der Cocktailparty-Effekt). Ebenso sind die Fokussierung auf bestimmte charakteristische Details und die Aufmerksamkeitsfokussierung nur in einem bestimmten Wahrnehmungsbereich (hinter mir, rechte Ecke des Monitors) in Untersuchungen bestätigt. Nicht immer ist uns bewusst, was die Aufmerksamkeit steuert. Unbewusst aufgenommene Informationen können einen steuernden Effekt haben und die Aufmerksamkeit lenken. Man kann dabei in bewusstseinsfähige und -unfähige Informationen unterteilen. Erstere können häufig durch gezielte Analyse entdeckt und so manches „Expertenwissen“ zum Allgemeingut werden lassen. Ein Anwendungsbeispiel ist die Produktplatzierung in der Werbung. Bewusstseinsunfähige Informationsaufnahme, zum Beispiel ultrakurzzeitige Einblendung bestimmter Signale, sind im Allgemeinen gesetzlich verboten, da sie unbewusste manipulative Effekte haben können.

Eine relativ moderne und immer mehr Anklang findende Theorie der Aufmerksamkeit ist die "Theory of Visual Attention" (TVA) von Claus Bundesen. Sie konnte durch ihre Komplexität viele Befunde erklären und ermöglicht durch Formulierung per mathematischer Gleichungen eine einfache Modellberechnung von Aufmerksamkeitsprozessen.

Weil die Aufmerksamkeit im Umfang beschränkt ist, gleichzeitig aber einen gesellschaftlichen Wert darstellt, ist das Erreichen der Aufmerksamkeit einer oder mehrerer Personen für viele ein wichtiges Ziel. Möglich wird es auf sehr unterschiedliche Weise, zum Beispiel durch Auftreten in Presse, Rundfunk oder Fernsehen. Sehr schnell erreichen Skandale eine große öffentliche Aufmerksamkeit. Veränderung erweckt schneller Aufmerksamkeit als Bleibendes, bereits die Ankündigung kann Aufmerksamkeit erregen. Das wird zum Beispiel von Politikern im „Sommerloch“ genutzt, aber auch von Künstlern, die Skandale nutzen, um Aufmerksamkeit zu erwecken.

Das Erwecken von Aufmerksamkeit kann auch im Rahmen eines Ablenkungsmanövers eingesetzt werden. Diese Taktik machen sich unter anderem Redner, Sportler, Zauberkünstler und Taschendiebe zunutze.

Allgemein stellt Aufmerksamkeit die Konzentration der Wahrnehmung auf bestimmte Stimuli unserer Umwelt dar. Ein wesentlicher Bestandteil von Aufmerksamkeit ist die Auswahl von Informationen (Selektion), um sie dem Bewusstsein zugänglich zu machen und das Denken und Handeln zu steuern. Ursache dieses Mechanismus ist die Beschränkung der menschlichen Kapazität für die Verarbeitung von Reizen.

Diese Tatsache belegte Alan T. Welford 1952 mit dem Paradigma zur Untersuchung der Psychologischen Refraktärperiode ("psychological refractory period" PRP). In diesen Untersuchungen wurden Versuchspersonen zwei Reize hintereinander präsentiert, auf die sie jeweils so schnell wie möglich reagieren sollten. Es stellte sich heraus, dass sich die Reaktionszeit auf den zweiten Reiz veränderte, in Abhängigkeit vom Zeitintervall zwischen dem Einsetzen des ersten Reizes und dem Einsetzen des zweiten Reizes ("stimulus onset asynchrony" SOA). Kürzere SOAs (Zwischenintervalle) forderten längere Reaktionszeiten auf den zweiten Reiz. Als Erklärung dieser Befunde gilt der so genannte „Engpass“ ("bottleneck") im menschlichen Verarbeitungssystem. Da die Verarbeitung von Reizen seriell erfolgt, muss der erste Reiz bereits verarbeitet sein, bevor die Verarbeitung des zweiten Reizes beginnen kann (vgl. Aufmerksamkeitsblinzeln).

Colin Cherry folgte 1953 mit seinen Tests zum „Dichotischen Hören“. Den Versuchspersonen wurde jeweils eine Nachricht auf dem linken und dem rechten Ohr präsentiert (zwei Nachrichten gleichzeitig). Die Nachricht einer Seite sollte laut nachgesprochen werden ("shadowing"). Es zeigte sich, dass bei dieser Testbedingung die nicht beachtete Nachricht nicht erinnert werden konnte. Auffällig jedoch war, dass beispielsweise ein Wechsel des Geschlechts der Sprecher oder präsentierte Beep-Töne wahrgenommen werden konnten.

Ein weiteres Paradigma ist das „Split-Span-Paradigma“ von Donald Broadbent aus dem Jahr 1954. Den Versuchspersonen wurden Ziffernpaare simultan nach dem Prinzip des Dichotischen Hörens präsentiert. Dabei zeigte sich, dass die Wiedergabe bevorzugt nach Ohr und nicht nach Paaren erfolgte. Aus diesem Ergebnis und dem von Cherry schlussfolgerte Broadbent, dass ein Abblocken aufgabenirrelevanter Nachrichten erfolgt und dass physikalische Reizmerkmale (Reizort, Frequenz) als effektive Hinweisreize fungieren.

Weitere Untersuchungen zum Thema der selektiven Aufmerksamkeit wurden von Broadbent, Treisman und Deutsch & Deutsch vorgenommen, deren Theorien im Folgenden erläutert werden sollen.

Aus den Erkenntnissen der Paradigmen entwickelte Broadbent 1958 die "Filtertheorie der Aufmerksamkeit". Sie besagt, dass gleichzeitig dargebotene Inputs parallel bzw. simultan in einen sensorischen Speicher gelangen. Jedoch kann nur ein Input auf der Basis seiner physikalischen Merkmale den so genannten selektiven Filter passieren. Weitere Inputs werden abgeblockt, verbleiben jedoch für Sekundenbruchteile im Speicher für eventuelle spätere Zugriffe. Da es sich um ein strikt serielles Verarbeitungsmodell handelt, ist ein Filter nötig, um dieses vor Überlastungen zu schützen. Aber nur Informationen, die diesen Filter zur weiteren Verarbeitung passiert haben, werden dem Menschen bewusst und können Bestandteil des Langzeitgedächtnisses werden.

1960 entwickelte Anne Treisman die "Attenuations- (Dämpfungs-)theorie" der Aufmerksamkeit. Sie entwickelte diese Theorie unter anderem, weil einige Forschungsergebnisse durch Broadbents Filtertheorie nicht ausreichend erklärt werden konnten. Hiermit ist zum Beispiel gemeint, dass beim „Split-Span-Paradigma“ auf der nicht beachteten Seite einige Reize doch bemerkt und erinnert werden konnten (Beep-Töne, Sprachwechsel). Auch der sogenannte Cocktailparty-Effekt konnte noch nicht erklärt werden. Treismans Theorie zufolge funktioniert der Filtermechanismus nicht nach dem Alles-oder-Nichts-Prinzip, sondern vielmehr nach dem Prinzip eines Dämpfers, indem er die Reizstärke auf dem unbeachteten Kanal reduziert. Folglich können diese Informationen in abgeschwächter Form weitergeleitet und, je nach ihrer Bedeutung, bis zu einem gewissen Grad semantisch verarbeitet werden.

Entgegen Broadbents und Treismans Vorstellungen gingen "Deutsch & Deutsch" 1963 mit ihrer "Theorie der späten Selektion" davon aus, dass alle sensorischen Signale das gleiche (höchste) Verarbeitungsniveau erreichen, unabhängig davon, ob Aufmerksamkeit auf sie gerichtet ist oder nicht. Durch einen parallelen multiplen Vergleichsprozess wird daraufhin das Signal bestimmt, welches für die aktuelle Aufgabe die größte Relevanz besitzt. Folglich wird nur das wichtigste Signal bewusst und bewirkt eine Reaktion. Nach dieser Theorie erfolgt die Selektion somit erst nach der vollen Verarbeitung der Signale und auf Grundlage ihrer inhaltlichen Bedeutung.

Jüngere Forschung führte zu der Erkenntnis, dass "selektive visuelle Aufmerksamkeit" ortsbasiert, objektbasiert oder dimensionsbasiert sein kann. Diese Annahme konnte mit Hilfe von Funktionelle Magnetresonanztomographie–Studien zur Aufmerksamkeitsmodulation von Brefczynski und DeYoe (1999) bestätigt werden. Es wurden Hinweise dafür gefunden, dass visuelle Aufmerksamkeit die Aktivität der Großhirnrinde beeinflusst. Bei Verschiebung der Aufmerksamkeit verändert sich die Aktivität in der Großhirnrinde des Hinterkopfs retinotop, also dem Sehmuster auf der Netzhaut entsprechend.

Diese Beobachtung wurde schon früher mit dem Elektroenzephalogramm (EEG) gemacht. Werden die Augen geschlossen und somit Aufmerksamkeit vom Sehsinn abgezogen, zeigt sich dies in einem verstärkten Vorherrschen des Alpha-Rhythmus (siehe nebenstehende Tabelle) an den Elektroden des Hinterkopfs.

Es konnte auch ein Zusammenhang zwischen Aufmerksamkeit und Arbeitsgedächtnis beschrieben werden. Bildgebende Verfahren (fMRT) und EEG-Studien zeigen, dass beide Prozesse sehr ähnliche neuronale Aktivitäten hervorrufen und insbesondere im primären visuellen Cortex simultan Modulationen kontralateral zum präsentierten Reiz bewirkt werden. Daraus kann gefolgert werden, dass sich räumliches Arbeitsgedächtnis und räumliche Aufmerksamkeit ähnlicher Mechanismen bedienen bzw. dass es sich um überlappende Prozesse handelt. 

Die Forschung der Mehrfachaufgabenperformanz beschäftigt sich mit der Ausführung parallel durchgeführter Doppel- oder Mehrfachhandlungen. 

Die Aufgaben werden also nicht seriell abgearbeitet, sondern es wird beispielsweise während der Autofahrt telefoniert oder während einer Fernsehshow eine E-Mail geschrieben. Oft wird dies auch Multitasking genannt. Mehrfachaufgabenperformanz ist zuletzt auch deshalb nicht uninteressant, weil sie Rückschlüsse auf die Funktionsweise und Grenzen der menschlichen Informationsverarbeitungstheorien (siehe oben) zulässt.


Historisches
Neuere Arbeiten
Lehrbücher und Lexika



</doc>
<doc id="14704" url="https://de.wikipedia.org/wiki?curid=14704" title="Senat">
Senat

Der Senat (lateinisch "senatus" von "senex" „Greis, alter Mann“), wörtlich „Ältestenrat“, ist ein Kollegialorgan, das der Exekutive, Legislative oder auch Judikative zugehören und unterschiedliche Funktionen und Aufgabenfelder wahrnehmen kann. Die Mitglieder eines Senats werden meist Senatoren (Singular: "Senator"; weibliche Form: "Senatorin" oder selten auch "Senatrix") genannt. 

"Siehe auch: Ältester und Alter" (zur traditionellen Wertschätzung der „Alten“)

Im antiken Griechenland existierte unter anderem in Sparta ein Ältestenrat (Gerusia), ebenso in Athen (Areopag). Auch die meisten anderen griechischen Stadtstaaten kannten Ältesten- bzw. Adelsräte, die neben Volksversammlungen und Magistraturen zu den typischen Einrichtungen einer Polis gehörten. Auch in nichtgriechischen Städten wie Karthago gab es einen Adelsrat, der in den lateinischen Quellen „Senat“ genannt wird.

Der Römische Senat war der oberste Rat des römischen Reiches. Formal hatte er in der Republik als Versammlung der ehemaligen Amtsträger zwar nur beratende Funktion, faktisch aber war er das Machtzentrum des Staates. Alle höheren Beamten des römischen Staates erhielten im Regelfall im Anschluss an ihre Amtszeit einen Sitz im Senat. In der frühen und mittleren Republik erhielten nach Beendigung des Amtsjahres ehemalige kurulische Aedilen, seit dem 2. Jahrhundert v. Chr. auch ehemalige Volkstribune sowie plebejische Aedilen und nach den Reformen Sullas ehemalige Quaestoren (deren Anzahl von Sulla erhöht worden war) einen Sitz im Senat. Ihr Ansehen und ihr Einfluss bemaßen sich an ihrem zuletzt bekleideten Amt. Die höchsten Ämter des "cursus honorum", die Praetur und das Konsulat sowie die Censur, wurden hauptsächlich aufgrund der immensen Wahlkampfkosten nahezu ausschließlich von Männern einiger weniger sehr einflussreicher und vermögender Familien der Oberschichten erreicht. Doch konnten nun auch „gewöhnlichere“ Bürger über die Bekleidung der rangniederen Ämter einen Senatssitz erlangen.

Der Senat umfasste in der Zeit der frühen Republik etwa 100, später etwa 300, seit Sulla 600 und in der Zeit Caesars sogar über 900 Mitglieder. In der frühen Kaiserzeit wurde die Anzahl der Senatoren allerdings wieder auf etwa 600 reduziert. Das Gremium verlor zugleich den größten Teil seiner Macht, seine Mitglieder blieben aber sehr angesehen. Die Mitglieder des Senats zeichneten sich durch einige Privilegien aus, z. B. das Tragen der Tunica mit dem breiten Purpurstreifen (latus clavus) oder besondere Ehrensitze im Theater. Nur wenige sogenannte "homines novi" konnten durch die Ernennung des Kaisers "(adlectio)" in den Senat aufsteigen. Durch den Verlust der Ehre oder Kinderlosigkeit starben die meisten Familien nach kurzer Zeit wieder aus und die Zahl 600 blieb relativ konstant erhalten. Seit dem 4. Jahrhundert gab es noch einen zweiten Senat in Konstantinopel, der seit Constantius II. dieselben Rechte genoss wie der in Rom. Der weströmische Senat überdauerte das Ende des westlichen Kaisertums 476 und verschwand erst um das Jahr 600. In Ostrom/Byzanz blieb er, wenngleich ebenfalls ohne reale Macht und mit verändertem Charakter, noch bis in die Palaiologenzeit bestehen.

In der ausgehenden Spätantike und dem beginnenden Frühmittelalter bezeichneten sich im gallorömischen (vor allem südlichen) Gallien Mitglieder der romanischen Reichsaristokratie als Senatoren, wie die Werke des Gregor von Tours belegen (siehe Gallorömischer Senatsadel). Deren Vorfahren hatten in römischer Zeit staatliche Ämter bekleidet und bewahrten ihre hervorgehobene soziale Stellung nun durch die Wahrnehmung hoher lokaler und kirchlicher Ämter.

Auch während des Mittelalters treten verschiedentlich Personen auf, die sich als „Senatoren“ Roms bezeichneten, wie etwa Theophylakt I. von Tusculum, wenngleich es bis ins 12. Jahrhundert keinen Senat im eigentlichen Sinne mehr gab. 1143 konstituierte sich in Rom jedoch wieder ein „Senat“, der von breiten Teilen der stadtrömischen Bevölkerung getragen wurde. Er sollte vor allem die Interessen der römischen Kommune gegenüber dem Papst, hohen Geistlichen, aber auch den großen Adelsfamilien vertreten. Noch im Spätmittelalter sind Senatoren belegt.

In politischen Systemen, in denen das Parlament aus zwei Kammern besteht (Bikameralismus) wird die erste Kammer (die Vertretung von Ständen oder Regionen) häufig als Senat bezeichnet. So etwa

In Deutschland werden die Landesregierungen der Stadtstaaten als Senate bezeichnet, so der

Die Stadtregierung der österreichischen Bundeshauptstadt Wien ist der Wiener Stadtsenat.

Historische Senate gab es in der Freien und Hansestadt Lübeck, der Freien Stadt Frankfurt und der Freien Stadt Danzig.

Spruchkörper innerhalb eines oberen oder höchsten Gerichts werden als "Senat" oder "Kollegialgericht" bezeichnet. Damit bezeichnet man mehrköpfige Gremien von (Berufs-)Richtern, im Unterschied zum Einzelrichter, und zu Formen wie dem Geschworenengericht (mit einer Laienbank) oder dem Schöffengericht (mit ehrenamtlichen Richtern). Senate sind typisch für schwierigere Entscheidungen, insbesondere an Höchstgerichten. Je nach Rechtsgebiet sind Senate aufgeteilt in Zivilsenate oder Strafsenate. 

In Deutschland unterscheidet man Senat im eigentlichen Sinne und Kammer. Auch gerichtsähnliche Gremien innerhalb der Verwaltung, wie die österreichischen Verwaltungssenate tragen die Bezeichnung.

An Hochschulen ist der Akademische Senat ein Selbstverwaltungsorgan und das oberste Gremium. Als demokratisch gewähltes Kollegialorgan steht er neben den Einzelorganen (dem Rektor, dem Präsidenten oder dem Kanzler) und versieht je nach Gesetzeslage legislative (z. B. Satzungsbeschlüsse, Einrichtung von Studiengängen), beratende, strategische, kontrollierende und Leitungsaufgaben. Teilweise werden auch die Ausschreibungen für Professorenstellen und die vorgeschlagenen Berufungslisten durch den Senat behandelt. – In Bayern wurde durch das Hochschulgesetz von 2006 zu Ungunsten des Senats der Einfluss des Präsidenten deutlich gestärkt.

Die Würde eines „Ehrensenators“ kann von Hochschulen für außergewöhnliche Verdienste um die Hochschule verliehen werden, und gilt als die höchste Auszeichnung einer Hochschule. Die Verleihung erhalten u.a. Amtsträger, etwa der Oberbürgermeister der Stadt, der Präsident einer Industrie- und Handelskammer oder eine Stiftungspersönlichkeit, wie beispielsweise an der Universität Freiburg oder oftmals werden auch Persönlichkeiten geehrt, welche in Forschung und Lehre mit der Hochschule verbunden sind. Diese bringen sich dann über das normale Maß hinausgehend in den Hochschulalltag ein. Die formale Ansprache eines Ehrensenators ist entweder Herr oder Frau Senator(in) + Name oder Herr oder Frau Ehrensenator(in). 

Auch ein Gremium gewählter Mitglieder wissenschaftlicher Gesellschaften und akademischer Berufsverbände wird Senat genannt.




</doc>
<doc id="14705" url="https://de.wikipedia.org/wiki?curid=14705" title="Komplexitätstheorie">
Komplexitätstheorie

Die Komplexitätstheorie als Teilgebiet der Theoretischen Informatik befasst sich mit der Komplexität algorithmisch behandelbarer Probleme auf verschiedenen formalen Rechnermodellen. Die Komplexität von Algorithmen wird in deren Ressourcenverbrauch gemessen, meist Rechenzeit oder Speicherplatzbedarf, manchmal auch speziellere Maße wie die Größe eines Schaltkreises oder die Anzahl benötigter Prozessoren bei parallelen Algorithmen. Die Komplexität eines Problems ist wiederum die Komplexität desjenigen Algorithmus, der das Problem mit dem geringstmöglichen Ressourcenverbrauch löst.

Die Komplexitätstheorie unterscheidet sich von der Berechenbarkeitstheorie, die sich mit der Frage beschäftigt, welche Probleme prinzipiell algorithmisch gelöst werden können. Demgegenüber besteht das wichtigste Forschungsziel der Komplexitätstheorie darin, die Menge aller lösbaren Probleme zu klassifizieren. Insbesondere versucht man, die Menge der effizient lösbaren Probleme, deren Ressourcenverbrauch in der Praxis bewältigt werden kann, von der Menge der inhärent schwierigen Probleme abzugrenzen.

Die Komplexitätstheorie gilt, neben der Berechenbarkeitstheorie und der Theorie der Formalen Sprachen, als einer der drei Hauptbereiche der Theoretischen Informatik. Zu ihren wesentlichen Forschungszielen gehört die Klassifizierung von Problemen im Hinblick auf den zu ihrer Lösung notwendigen Aufwand. Eine besondere Rolle spielt dabei die Abgrenzung der praktisch effizient lösbaren Probleme. Die Komplexitätstheorie grenzt daher diejenigen Probleme ein, zu denen andere Disziplinen der Informatik überhaupt sinnvollerweise nach effizienten Lösungen suchen sollten, und motiviert so die Entwicklung praxistauglicher Approximationsalgorithmen.

Neben dem reinen Erkenntnisgewinn bereichert auch das Methodenarsenal der komplexitätstheoretischen Forschung zahlreiche angrenzende Gebiete. So führt etwa ihre enge Verzahnung mit der Automatentheorie zu neuen Maschinenmodellen und einem umfassenderen Verständnis der Arbeitsweise von Automaten. Die häufig konstruktive Beweisführung findet auch im Rahmen des Entwurfs und der Analyse von Algorithmen und Datenstrukturen Anwendung.
Den zentralen Gegenstand der Komplexitätstheorie bilden Probleme, und zwar in der Regel Entscheidungsprobleme, deren Instanzen eine Ja/Nein-Antwort erfordern. Ein Entscheidungsproblem wird dabei oft als formale Sprache dargestellt. Man drückt jede Probleminstanz als Wort über einem Alphabet aus, d. h. als Folge von Zeichen aus diesem Alphabet. Die fragliche Sprache besteht aus den Wörtern, denen eine Instanz mit der Antwort „Ja“ entspricht. Die Aufgabe besteht dann in der Lösung des Wortproblems, also darin, für ein gegebenes Wort zu entscheiden, ob es zu der Sprache gehört oder nicht, und damit hat man auch die Antwort auf die entsprechende Probleminstanz gefunden.

Wenn zum Beispiel das Problem darin besteht zu entscheiden, ob ein Graph zusammenhängend ist oder nicht, dann wäre ein Wort eine Darstellung eines beliebigen Graphen. Die zugehörige zu entscheidende Sprache wäre die Menge der Wörter, die einen zusammenhängenden Graphen darstellen.

Man könnte annehmen, dass die Einschränkung auf Entscheidungsprobleme viele wichtige Probleme ausschließt. Es gibt jedoch zu allen im Sinne der Komplexitätstheorie relevanten Problemen auch ein entsprechendes Entscheidungsproblem. Betrachtet man zum Beispiel das Problem der Multiplikation zweier Zahlen, so besteht die dazugehörige Sprache des Entscheidungsproblems aus allen Zahlen-Tripeln formula_1, für die der Zusammenhang formula_2 gilt. Die Entscheidung, ob ein gegebenes Tripel zu dieser Sprache gehört, entspricht dem Lösen des Problems der Multiplikation zweier Zahlen.

Darüber hinaus ist man in der Komplexitätstheorie manchmal daran interessiert, Problemlösungen auch wirklich zu generieren. Hier begnügt man sich nicht mit der einfachen Ja/Nein-Antwort eines Wortproblemes für formale Sprachen. Stattdessen versteht man das Problem als eine Abbildung aus einem Definitionsbereich in seinen Lösungsraum.
Die Lösung des Problemes der Multiplikation zweier Zahlen in etwa würde man als das Ergebnis der Abbildung formula_3 mit formula_4 verstehen.
Für die Definition der meisten Komplexitätsklassen wird jedoch die Formulierung durch Entscheidungsprobleme bevorzugt.

Eine wichtige Unterkategorie der Berechnungsprobleme stellen die Optimierungsprobleme dar. Bei Optimierungsproblemen besteht der funktionale Zusammenhang aus der Forderung, dass ein Maximum bzw. Minimum zu einer gegebenen Kostenfunktion zu der Eingabe ausgegeben werden soll.
Man könnte so zum Beispiel beim Problem des Handlungsreisenden nach der optimalen Route durch die Landeshauptstädte Deutschlands fragen, welche die geringste Gesamtlänge besitzt. Viele Optimierungsprobleme sind von großer praktischer Bedeutung.

Eine Probleminstanz ist nicht mit dem Problem selbst zu verwechseln.
Ein Problem stellt in der Komplexitätstheorie eine allgemeine Fragestellung, eine Schablone, dar. Eine Instanz des Problems ist dann eine vollständige Fragestellung, welche die richtige Antwort (ja bzw. nein im Fall eines Entscheidungsproblems) festlegt.

Eine Instanz des Problems des Handlungsreisenden könnte zum Beispiel die Frage sein, ob eine Route durch die Landeshauptstädte Deutschlands mit einer maximalen Länge von 2000 km existiert. Die Entscheidung über diese Route hat jedoch nur begrenzten Wert für andere Probleminstanzen, wie etwa eine Rundreise durch die Sehenswürdigkeiten Mailands. In der Komplexitätstheorie interessiert man sich daher für Aussagen, die unabhängig von konkreten Instanzen sind.

Ein Problem wird so allgemein formuliert, dass es eine unendliche Menge von Probleminstanzen definiert, denn es ist nicht sinnvoll, nach der Komplexität einer endlichen Menge von Instanzen zu fragen; ein Programm könnte eine Liste von vorgefertigten Antworten enthalten und nur durch Tabellenzugriff die richtige Lösung ausgeben, was den Aufwand für die Ermittlung der Antworten nicht widerspiegelt.
Interessant wird es erst, wenn eine unendliche Menge von Instanzen gegeben ist und man einen Algorithmus finden will, der für jede Instanz die richtige Antwort berechnet.

Als formale Sprachen werden Probleme und deren Instanzen über abstrakten Alphabeten definiert. Häufig wird das binäre Alphabet mit den Symbolen 0 und 1 gewählt, da dies der Verwendung von Bits bei modernen Rechnern am nächsten kommt. Eingaben werden dann durch Alphabetsymbole kodiert. An Stelle von mathematischen Objekten wie Graphen verwendet man möglicherweise eine Bitfolge, die der Adjazenzmatrix des Graphen entspricht, an Stelle von natürlichen Zahlen zum Beispiel deren Binärdarstellung.

Auch wenn sich Beweise komplexitätstheoretischer Aussagen in der Regel konkreter Repräsentationen der Eingabe bedienen, versucht man Aussagen und Betrachtung unabhängig von Repräsentationen zu halten. Dies kann etwa erreicht werden, indem man sicherstellt, dass die gewählte Repräsentation bei Bedarf ohne allzu großen Aufwand in eine andere Repräsentation transformiert werden kann, ohne dass sich hierdurch die Berechnungsaufwände insgesamt signifikant verändern. Um dies zu ermöglichen, ist unter anderem die Auswahl eines geeigneten universellen Maschinenmodells von Bedeutung.

Hat man ein Problem formal definiert (zum Beispiel das Problem des Handlungsreisenden in Form eines Graphen mit Kantengewichten), so möchte man Aussagen darüber treffen, wie sich ein Algorithmus bei der Berechnung der Problemlösung in Abhängigkeit von der Schwierigkeit des Problems verhält. Im Allgemeinen sind bei der Beurteilung der Schwierigkeit des Problems viele verschiedene Aspekte zu betrachten. Dennoch gelingt es häufig, wenige skalare Größen zu finden, die das Verhalten des Algorithmus im Hinblick auf den Ressourcenverbrauch maßgeblich beeinflussen. Diese Größen bezeichnet man als die Problemgröße. In aller Regel entspricht diese der Eingabelänge (bei einer konkret gewählten Repräsentation der Eingabe).

Man untersucht nun das Verhalten des Algorithmus in Abhängigkeit von der Problemgröße. Die Komplexitätstheorie interessiert sich für die Frage: "Wie viel" Mehrarbeit ist für wachsende Problemgrößen notwendig? Steigt der Aufwand (in Relation zur Problemgröße) zum Beispiel linear, polynomial, exponentiell oder gar überexponentiell?

So kann man beim Problem des Handlungsreisenden die Problemgröße als Anzahl der vorgegebenen Orte definieren (wobei man vernachlässigt, dass auch die vorgegebenen Streckenlängen verschieden große Eingabegrößen aufweisen können). Dann ist dieses Problem für die Problemgröße 2 trivial, da es hier überhaupt nur eine mögliche Lösung gibt und diese folglich auch optimal sein muss. Mit zunehmender Problemgröße wird ein Algorithmus jedoch mehr Arbeit leisten müssen.

Auch innerhalb einer Problemgröße lassen sich verschiedene Verhaltensweisen von Algorithmen beobachten. So hat das Problem des Handlungsreisenden für die 16 deutschen Landeshauptstädte dieselbe Problemgröße formula_5 wie das Finden einer Route durch 16 europäische Hauptstädte. Es ist keineswegs zu erwarten, dass ein Algorithmus unter den unterschiedlichen Bedingungen (selbst bei gleicher Problemgröße) jeweils gleich gut arbeitet. Da es jedoch in der Regel unendlich viele Instanzen gleicher Größe für ein Problem gibt, gruppiert man diese zumeist grob in drei Gruppen: bester, schlechtester und durchschnittlicher Fall. Diese stehen für die Fragen:


Die Betrachtung bester, schlechtester und durchschnittlicher Fälle bezieht sich stets auf eine feste Eingabelänge. Auch wenn die Betrachtung konkreter Eingabelängen in der Praxis von großem Interesse sein kann, ist diese Sichtweise für die Komplexitätstheorie meist nicht abstrakt genug. Welche Eingabelänge als groß oder praktisch relevant gilt, kann sich aufgrund technischer Entwicklungen sehr schnell ändern. Es ist daher gerechtfertigt, das Verhalten von Algorithmen in Bezug auf ein Problem gänzlich unabhängig von konkreten Eingabelängen zu untersuchen. Man betrachtet hierzu das Verhalten der Algorithmen für immer größer werdende, potentiell unendlich große Eingabelängen. Man spricht vom asymptotischen Verhalten des jeweiligen Algorithmus.

Bei dieser Untersuchung des asymptotischen Ressourcenverbrauchs spielen untere und obere Schranken eine zentrale Rolle. Man möchte also wissen, welche Ressourcen für die Entscheidung eines Problems mindestens und höchstens benötigt werden. Für die Komplexitätstheorie sind die unteren Schranken von besonderem Interesse: Man möchte zeigen, dass ein bestimmtes Problem "mindestens" einen bestimmten Ressourcenverbrauch beansprucht und es folglich keinen Algorithmus geben kann, der das Problem mit geringeren Ressourcen entscheidet. Solche Ergebnisse helfen, Probleme nachhaltig bezüglich ihrer Schwierigkeit zu separieren. Jedoch sind bisher nur vergleichsweise wenige aussagekräftige untere Schranken bekannt. Der Grund hierfür liegt in der Problematik, dass sich Untersuchungen unterer Schranken stets auf alle denkbaren Algorithmen für ein Problem beziehen müssen; also auch auf Algorithmen, die heute noch gar nicht bekannt sind.

Im Gegensatz dazu gelingt der Nachweis oberer Schranken in der Regel durch die Analyse konkreter Algorithmen. Durch den Beweis der Existenz auch nur eines Algorithmus, der die obere Schranke einhält, ist der Nachweis bereits erbracht.

Bei bestimmten Problemen, etwa der Komplexität von Verschlüsselungsverfahren, wird der Nachweis versucht, dass der zu erwartende Ressourcenverbrauch beim Versuch, den Code zu knacken, jedes Maß übersteigt. Für Probleme, die selbst mit einem Computer von der Größe der Erde nicht während der Lebensdauer der Erde zu lösen sind, wurde der Begriff transcomputationales Problem geprägt.

Zur Analyse des Ressourcenverbrauchs von Algorithmen sind geeignete Kostenfunktionen zu definieren, welche eine Zuordnung der Arbeitsschritte des Algorithmus zu den verbrauchten Ressourcen ermöglichen. Um dies tun zu können, muss zunächst festgelegt werden, welche Art von Arbeitsschritt einem Algorithmus überhaupt erlaubt ist. Diese Festlegung erfolgt in der Komplexitätstheorie über abstrakte Maschinenmodelle – würde man auf reale Rechnermodelle zurückgreifen, so wären die gewonnenen Erkenntnisse bereits in wenigen Jahren überholt. Der Arbeitsschritt eines Algorithmus erfolgt in Form einer Befehlsausführung auf einer bestimmten Maschine. Die Befehle, die eine Maschine ausführen kann, sind dabei durch das jeweilige Modell streng limitiert. Darüber hinaus unterscheiden sich verschiedene Modelle etwa in der Handhabung des Speichers und in ihren Fähigkeiten zur parallelen Verarbeitung, d. h. der gleichzeitigen Ausführung mehrerer Befehle. Die Definition der Kostenfunktion erfolgt nun durch eine Zuordnung von Kostenwerten zu den jeweils erlaubten Befehlen.

Häufig wird von unterschiedlichen Kosten für unterschiedliche Befehle abstrahiert und als Kostenwert für eine Befehlsausführung immer 1 gesetzt. Sind auf einer Maschine beispielsweise Addition und Multiplikation die erlaubten Operationen, so zählt man für jede Addition und jede Multiplikation, die im Laufe des Algorithmus berechnet werden müssen, den Kostenwert von 1 hinzu. Man spricht dann auch von einem "uniformen Kostenmaß". Ein solches Vorgehen ist dann gerechtfertigt, wenn sich die erlaubten Operationen nicht gravierend unterscheiden und wenn der Wertebereich, auf dem die Operationen arbeiten, nur eingeschränkt groß ist. Dies wird schon für eine einfache Operation wie die Multiplikation klar: Das Produkt zweier einstelliger Dezimalzahlen dürfte sich ungleich schneller errechnen lassen als das Produkt zweier hundertstelliger Dezimalzahlen. Bei einem uniformen Kostenmaß würden beide Operationen dennoch mit einem Kostenwert von 1 veranschlagt. Sollten sich die möglichen Operanden im Laufe eines Algorithmus tatsächlich so gravierend unterscheiden, so muss ein realistischeres Kostenmaß gewählt werden. Häufig wählt man dann das "logarithmische Kostenmaß". Der Bezug auf den Logarithmus ergibt sich daraus, dass sich eine Dezimalzahl n im Wesentlichen durch formula_6 Binärziffern darstellen lässt. Man wählt zur Repräsentation der Operanden Binärziffern aus und definiert die erlaubten booleschen Operationen. Sollte das jeweilige Maschinenmodell Adressen verwenden, so werden auch diese binär codiert. Auf diese Weise werden die Kosten über die Länge der Binärdarstellung logarithmisch gewichtet. Andere Kostenmaße sind möglich, werden jedoch nur selten eingesetzt.

Man unterscheidet verschiedene Berechnungsparadigmen: der pragmatischste Typ ist sicher der der deterministischen Maschinen; weiterhin gibt es den in der Theorie besonders relevanten Typ der nichtdeterministischen Maschinen; weiterhin gibt es noch probabilistische Maschinen, alternierende und andere. In der Regel kann man jedes Maschinenmodell mit jedem der obigen Paradigmen definieren. Einige Paradigmen, so zum Beispiel der Nichtdeterminismus, modellieren dabei einen Typ, der der Theorie vorbehalten bleiben muss, da man den Nichtdeterminismus in der dort definierten Form physikalisch nicht bauen kann, (sie „erraten“ einen richtigen Pfad in einem Berechnungsbaum), lassen sich jedoch häufig leicht zu einem gegebenen Problem konstruieren. Da eine Transformation von nichtdeterministischen in deterministische Maschinen immer relativ einfach möglich ist, konstruiert man daher zunächst eine nichtdeterministische Maschinenversion und transformiert diese später in eine deterministische.

Daraus geht eine wichtige Beweistechnik der Komplexitätstheorie hervor: Lässt sich zu einem gegebenen Problem ein bestimmter Maschinentyp konstruieren, auf dem das Problem mit bestimmten Kosten entschieden werden kann, so kann damit bereits die Komplexität des Problems eingeschätzt werden. Tatsächlich werden sogar die unterschiedlichen Maschinenmodelle bei der Definition von Komplexitätsklassen zugrundegelegt. Dies entspricht einer Abstraktion von einem konkreten Algorithmus: Wenn ein Problem auf Maschine formula_7 entscheidbar ist (wobei ein entsprechender Algorithmus evtl. noch gar nicht bekannt ist), so lässt es sich unmittelbar einer bestimmten Komplexitätsklasse zuordnen, nämlich derjenigen, die von formula_7 definiert wird. Dieses Verhältnis zwischen Problemen und Maschinenmodellen ermöglicht Beweisführungen ohne die umständliche Analyse von Algorithmen.

Besonders häufig eingesetzte Modelle sind:

Zur Untersuchung parallelisierbarer Probleme können darüber hinaus auch parallelisierte Versionen dieser Maschinen zum Einsatz kommen, insbesondere die parallele Registermaschine.

Für die Verwendung von Maschinenmodellen in der Komplexitätstheorie ist eine Erweiterung der Church-Turing-These von Bedeutung, die auch als "erweiterte Church-Turing-These" bezeichnet wird. Sie besagt, dass alle universellen Maschinenmodelle in Bezug auf die Rechenzeit bis auf polynomielle Faktoren gleich mächtig sind. Dies ermöglicht dem Komplexitätstheoretiker eine relativ freie Wahl des Maschinenmodells im Hinblick auf das jeweilige Untersuchungsziel. Auch diese These ist nicht beweisbar; im Gegensatz zur gewöhnlichen Church-Turing-These wäre es aber möglich, sie durch ein Gegenbeispiel zu widerlegen.

Zur Untersuchung des Mindestspeicherbedarfs, der für die Lösung von Problemen benötigt wird, nimmt man häufig die folgenden Modifikationen des verwendeten Maschinenmodells (in der Regel eine Turingmaschine) vor:


Für die Untersuchung des Speicherbedarfs dürfen dann Ein- und Ausgabe der Maschine unberücksichtigt bleiben. Die Motivation für diese Änderungen ist die folgende: Würde zum Beispiel der Eingabespeicher in die Speicherplatzanalyse einbezogen, so könnte kein Problem in weniger als formula_9 Platzbedarf gelöst werden, denn das Eingabewort hat ja immer genau die Länge und damit den Speicherbedarf n. Indem man die Eingabe nur lesbar macht, verhindert man, dass sie für Zwischenrechnungen verwendet werden kann. Man kann dann die Eingabe bei der Berechnung des Platzbedarfs vernachlässigen. Eine ähnliche Argumentation führt zu der Einschränkung der Ausgabe. Durch die zusätzliche Einschränkung einer möglichen Kopfbewegung wird verhindert, dass die Kopfposition verwendet wird, um sich Information zu „merken“. Insgesamt stellen all diese Einschränkungen sicher, dass Ein- und Ausgabe bei der Speicherplatzanalyse nicht berücksichtigt werden müssen.

Die vorgenommenen Modifikationen beeinflussen das Zeitverhalten der Maschine übrigens nur um einen konstanten Faktor und sind damit vernachlässigbar.

Bei der Untersuchung von Größenordnungen für Aufwände wird in der Komplexitätstheorie ausgiebig von der O-Notation Gebrauch gemacht. Dabei werden lineare Faktoren und Konstanten aus der Betrachtung ausgeblendet. Diese Vorgehensweise mag zunächst überraschen, wäre doch für den Praktiker häufig bereits eine Halbierung der Aufwände von hoher Bedeutung.

Der Standpunkt der Komplexitätstheorie lässt sich theoretisch mit einer Technik rechtfertigen, die man als "lineares Beschleunigen" oder auch Speedup-Theorem bezeichnet. (Wir beschränken uns hier auf das Zeitverhalten. Analoge Beweise kann man auch für den Speicherbedarf oder andere Ressourcen führen.) Das Speedup-Theorem besagt vereinfachend, dass sich zu jeder Turingmaschine, die ein Problem in Zeit formula_10 entscheidet, eine neue Turingmaschine konstruieren lässt, die das Problem in Zeit weniger als formula_11 entscheidet. Dabei kann formula_12 beliebig klein gewählt sein. Das bedeutet nichts anderes, als dass sich jede Turingmaschine, die ein bestimmtes Problem löst, um einen beliebigen konstanten Faktor beschleunigen lässt. Der Preis für diese Beschleunigung besteht in einer stark vergrößerten Arbeitsalphabetgröße und Zustandsmenge der verwendeten Turingmaschine (letztlich also „Hardware“).

Diese Beschleunigung wird unabhängig von der Problemgröße erreicht. Bei der Betrachtung des asymptotischen Verhaltens von Problemen ergibt es daher keinen Sinn, konstante Faktoren zu berücksichtigen – solche Faktoren ließen sich durch Anwendung der Beschleunigungstechnik wieder beseitigen. Die Vernachlässigung konstanter Faktoren, die sich in der O-Notation ausdrückt, hat daher nicht nur praktische Gründe, sie vermeidet auch Verfälschungen im Rahmen komplexitätstheoretischer Betrachtungen.

Eine wesentliche Aufgabe der Komplexitätstheorie besteht darin, sinnvolle Komplexitätsklassen festzulegen, in diese die vorliegenden Probleme einzusortieren und Aussagen über die wechselseitigen Beziehungen zwischen den Klassen zu finden.

Eine Reihe von Faktoren nehmen Einfluss auf die Bildung von Komplexitätsklassen. Die wichtigsten sind die folgenden:


Zur Angabe oder Definition von Komplexitätsklassen verwendet man Schrankenfunktionen. Schreibt man beispielsweise DTIME(f), so meint man damit die Klasse aller Probleme, die auf einer deterministischen Turingmaschine in der Zeit formula_13 entschieden werden können. formula_3 ist dabei eine Schrankenfunktion. Um als Schrankenfunktion für komplexitätstheoretische Analysen eingesetzt werden zu können, sollte die Funktion mindestens die folgenden Anforderungen erfüllen:


Eine Funktion, die diesen Anforderungen genügt, bezeichnet man auch als "echte Komplexitätsfunktion". Der Sinn der Anforderungen ist zum Teil technischer Natur. Die Schrankenfunktion kann selbst auf konstruktive Art (zum Beispiel als Turingmaschine) in Beweise einfließen und sollte sich daher für diese Zwecke „gutartig“ verhalten. An dieser Stelle soll nur darauf hingewiesen werden, dass bei der Wahl der Schrankenfunktion eine gewisse Vorsicht walten muss, weil sonst bestimmte algorithmische Techniken nicht angewandt werden können.

Die meisten in der Praxis auftretenden Funktionen entsprechen den oben genannten Einschränkungen, so etwa die konstante Funktion, die Logarithmusfunktion, die Wurzelfunktion, Polynome, die Exponentialfunktion sowie alle Kombinationen dieser Funktionen. Es folgt eine Übersicht der wichtigsten Schrankenfunktionen mit der jeweils üblichen Sprechweise. Die Angabe erfolgt wie üblich in O-Notation.

Die wichtigsten Schrankenfunktionen

Für die gebildeten Klassen möchte man möglichst beweisen, dass durch zusätzlich bereitgestellte Ressourcen tatsächlich "mehr" Probleme gelöst werden können. Diese Beweise bezeichnet man als "Hierarchiesätze" (auch "Separationssätze" genannt), da sie auf den Klassen der jeweiligen Ressource eine Hierarchie induzieren. Es gibt also Klassen, die in eine echte Teilmengenbeziehung gesetzt werden können. Wenn man solche echten Teilmengenbeziehungen ermittelt hat, lassen sich auch Aussagen darüber treffen, wie groß die Erhöhung einer Ressource sein muss, um damit eine größere Zahl von Problemen berechnen zu können. Von besonderem Interesse sind dabei wiederum die Ressourcen Zeit und Raum. Die induzierten Hierarchien bezeichnet man auch als "Zeithierarchie" und "Raumhierarchie".

Die Hierarchiesätze bilden letztlich das Fundament für die "Separierung" von Komplexitätsklassen. Sie bilden einige der frühesten Ergebnisse der Komplexitätstheorie. Es muss ergänzt werden, dass alle Hierarchiesätze auf diversen Voraussetzungen beruhen. Eine dieser Voraussetzungen ist etwa, dass die oben genannten Anforderungen an "echte Komplexitätsfunktionen" erfüllt werden. Ohne die Einhaltung dieser Anforderungen bricht tatsächlich die gesamte Klassenhierarchie in sich zusammen.

Der Zeithierarchiesatz besagt:

Es gibt also Probleme, deren asymptotischer Zeitbedarf auf einer deterministischen Turingmaschine innerhalb der Klasse formula_21 aber nicht in formula_22 liegt. Eine ähnliche Beziehung lässt sich für nichtdeterministische Turingmaschinen finden.

Der Raumhierarchiesatz besagt:

Die Aussage ist analog zum Zeithierarchiesatz. Man erkennt jedoch, dass im Vergleich zur Zeit bereits eine geringere Steigerung des Raumes ausreicht (Faktor formula_24 im Vergleich zu formula_25), um eine größere Klasse zu bilden. Dies entspricht auch einer intuitiven Erwartung, verhält sich doch der Raum insgesamt aufgrund seiner Wiederverwendbarkeit (alte Zwischenergebnisse können überschrieben werden) gutmütiger.

Die Hierarchiesätze beziehen sich ausschließlich auf den jeweils gleichen Berechnungsmodus und eine einzelne Ressource, also zum Beispiel auf die Ressource Zeit auf einem deterministischen Maschinenmodell. Es wird jedoch keine Aussage darüber getroffen, wie sich etwa Raum- und Zeitklassen zueinander verhalten oder in welchem Verhältnis deterministische oder nichtdeterministische Klassen zueinander stehen. Dennoch gibt es derartige Zusammenhänge. Sie werden in den Abschnitten "Beziehungen zwischen Raum- und Zeitklassen" und "Beziehungen zwischen deterministischen und nichtdeterministischen Klassen" behandelt.



"Siehe auch:" Liste von Komplexitätsklassen

Für jede Komplexitätsklasse K lässt sich ihre Komplementklasse CoK bilden: Die Komplementklasse enthält genau die Komplemente der Elemente der ursprünglichen Klasse. Fasst man K als Menge von Sprachen auf (formula_26, siehe Potenzmenge), so ist die Komplementklasse formula_27. Bezogen auf die entsprechenden Entscheidungsprobleme heißt das: CoK enthält die Probleme, auf deren Instanzen die Antwort immer gegensätzlich lautet wie bei einem Problem in K.

Im Gegensatz dazu kann man auch das Komplement einer Klasse K betrachten. Dieses enthält genau die Sprachen/Probleme aus einer gegebenen Grundmenge, die nicht in K sind; diese Probleme sind in der Regel viel schwerer als die in K. Die Komplementklasse CoK hingegen besitzt mit K in der Regel einen nichtleeren Durchschnitt.

Für deterministische Maschinen gilt in der Regel K = CoK, da in der Übergangsfunktion einfach nur die Übergänge zu akzeptierenden Zuständen durch Übergänge zu verwerfenden Zuständen ausgetauscht werden müssen und umgekehrt. Für andere Berechnungsmodi gilt dies jedoch nicht, da hier die Akzeptanz anders definiert ist. Beispielsweise ist bislang unbekannt, ob NP = CoNP gilt. P = CoP ist wahr, da das zugrunde liegende Modell deterministisch ist und hier die akzeptierenden und ablehnenden Zustände in den Berechnungen einfach ausgetauscht werden können, wie im vorherigen Absatz angesprochen. So sehen wir sofort, dass P im Durchschnitt von NP und CoNP enthalten ist. Ob dieser Durchschnitt genau P ist, ist nicht bekannt.

Eines der wichtigsten und nach wie vor ungelösten Probleme der Komplexitätstheorie ist das P-NP-Problem. "Ist die Klasse P gleich der Klasse NP?" Diese Frage kann als eine zentrale Forschungsmotivation der Komplexitätstheorie angesehen werden, und eine Vielzahl der komplexitätstheoretischen Ergebnisse wurde erzielt, um der Lösung des P-NP-Problems näher zu kommen.

Die Tragweite des P-NP-Problems resultiert aus der Erfahrung, dass die Probleme der Klasse P in der Regel praktisch lösbar sind: Es existieren Algorithmen, um Lösungen für diese Probleme effizient oder doch mit vertretbarem zeitlichem Aufwand zu berechnen. Der zeitliche Aufwand zur Problemlösung wächst für die Probleme der Klasse P maximal polynomial. Oft lassen sich sogar Algorithmen finden, deren Zeitfunktionen Polynome niedrigen Grades sind. Da das gewählte Maschinenmodell dieser Zeitklasse deterministisch (und realisierbar) ist, entsprechen die Probleme der Klasse P auch bei nicht-banaler Größe in etwa den praktisch lösbaren Problemen.

Die Algorithmen zur Lösung der Probleme in NP basieren auf einem nichtdeterministischen Maschinenmodell. Für solche Maschinen wird eine unbeschränkte Parallelisierbarkeit der sich verzweigenden Berechnungspfade angenommen, die technisch nicht realisiert werden kann. Zwar arbeiten auch die Algorithmen zur Lösung der Probleme in NP in polynomialer Zeit, aber eben auf der Basis eines unrealistischen Maschinenmodells. Auf einer deterministischen Maschine, wie es alle existierenden Computer sind, kann der Aufwand zu ihrer Berechnung mit wachsender Problemgröße mehr als polynomial ansteigen. Es gibt daher Probleme in NP, die für praktische Zwecke als nicht lösbar gelten. Dazu gehören die NP-vollständigen Probleme. In dieser Klasse finden sich Probleme aus fast allen Bereichen der Informatik, deren effiziente Lösung enorm wichtig wäre. Es sind aber nicht alle Probleme in NP schwer, weil NP auch die Klasse P enthält.

Würde das P-NP-Problem im Sinne von P = NP gelöst, so wüssten wir, dass es selbst für NP-vollständige Probleme Algorithmen geben muss, die mit polynomiellem Zeitaufwand arbeiten.

Da umgekehrt die Definition der NP-Vollständigkeit Algorithmen voraussetzt, mit denen es gelingt, beliebige Probleme aus NP in polynomieller Zeit auf NP-vollständige Probleme zu reduzieren, wären mit der polynomialen Lösbarkeit auch nur eines einzigen NP-vollständigen Problems sofort sämtliche Probleme der Klasse NP in polynomieller Zeit lösbar. Dies hätte eine Problemlösekraft in der gesamten Informatik zur Folge, wie sie auch durch noch so große Fortschritte in der Hardware-Entwicklung nicht erreicht werden kann.

Andererseits ist für bestimmte Anwendungsfälle eine Lösung des P-NP-Problems im Sinne von P = NP eher unerwünscht. Beispielsweise würden asymmetrische Verschlüsselungsverfahren erheblich an Sicherheit verlieren, da diese dann in Polynomialzeit gebrochen werden könnten.

Würde das P-NP-Problem im Sinne von P ≠ NP gelöst, so wäre klar, dass weitere Bemühungen, polynomielle Lösungen für NP-vollständige Probleme zu finden, sinnlos wären. Man kann sich leicht vorstellen, dass aufgrund der hohen Bedeutung der Probleme in NP die Bemühungen um eine effiziente Lösung erst dann eingestellt werden, wenn diese nachgewiesenermaßen unmöglich ist. Bis zu diesem Zeitpunkt wird noch viel private und öffentliche Forschungsenergie aufgewandt werden.

In vielen Theoremen wird heute jedoch angenommen, dass P ≠ NP gilt, denn nur so kann ohne einen Beweis der Gleichheit trotzdem effektive Forschungsarbeit geleistet werden. Man sucht nach Auswegen durch Approximationen und Heuristiken, nach Problemeinschränkungen, die die Praxis nicht vernachlässigen.

Zu den wichtigsten Forschungszielen der Komplexitätstheorie gehört die Abgrenzung des praktisch Machbaren und damit des Betätigungsfeldes der Informatik schlechthin. Die vorherigen Abschnitte haben die Wichtigkeit dieser Grenzziehung verdeutlicht. Im Zuge der Versuche, das P-NP-Problem zu lösen, hat die Komplexitätstheorie zahlreiche Ergebnisse zu Tage gefördert und ihre Analysemethoden Zug um Zug verfeinert. Diese Ergebnisse werden insbesondere beim Entwurf und der Analyse praktisch wichtiger Algorithmen angewandt und wirken so auch unmittelbar auf die Praktische Informatik.

Die seit über dreißig Jahren andauernden Bemühungen, das P-NP-Problem zu lösen, gewähren darüber hinaus dem praktischen Informatiker ein großes Maß an Sicherheit, dass isolierte Bemühungen zur effizienten Lösung von Problemen aus NP mehr oder weniger sinnlos sind. Die praktische Informatik konzentriert sich daher bei der Lösung für Probleme aus NP auf Näherungslösungen oder die Abwandlung der ursprünglichen Probleme. So kann sich beispielsweise die Problemkomplexität von Optimierungs-Algorithmen enorm verringern, wenn man keine optimale Lösung anstrebt, sondern mit einer fast optimalen Lösung zufrieden ist. Die Komplexitätstheorie liefert für diese Vorgehensweise die theoretische Rückendeckung.

Das folgende Inklusionsdiagramm gibt einen – recht groben – Überblick über die Zusammenhänge zwischen Klassen der Berechenbarkeitstheorie, der Chomsky-Hierarchie und den bedeutendsten Komplexitätsklassen.

Nachdem in den vorhergehenden Abschnitten zahlreiche Grundbegriffe und wichtige Ergebnisse der Komplexitätstheorie erläutert wurden, wird in den folgenden Abschnitten ein geschichtlicher Abriss gegeben, der die zeitliche Abfolge dieser Ergebnisse einordnen helfen soll.

Vor dem eigentlichen Beginn der explizit auf die Komplexität von Algorithmen bezogenen Forschung wurden zahlreiche Grundlagen erarbeitet. Als wichtigste kann dabei die Konstruktion der Turingmaschine durch Alan Turing im Jahr 1936 angesehen werden, die sich für spätere Algorithmen-Analysen als ausgesprochen flexibles Modell erwies.

Als erste informelle komplexitätstheoretische Untersuchungen werden Ergebnisse von John Myhill (1960), Raymond Smullyan (1961) und Hisao Yamada (1962) angesehen, die sich mit speziellen raum- und zeitbeschränkten Problemklassen beschäftigt haben, jedoch in ihren Arbeiten noch keinen Ansatz zu einer allgemeinen Theorie entwickelten.

Einen ersten großen Schritt in Richtung einer solchen Theorie unternehmen Juris Hartmanis und Richard E. Stearns in ihrer 1965 erschienenen Arbeit "On the computational complexity of algorithms." Sie geben bereits eine quantitative Definition von Zeit- und Platzkomplexität und wählen damit bereits die beiden Ressourcen aus, die bis heute als die wichtigsten angesehen werden. Dabei wählen sie die Mehrband-Turingmaschine als Grundlage und treffen damit eine sehr robuste Entscheidung, die in vielen komplexitätstheoretischen Feldern Bestand hat. Sie erarbeiten auch bereits erste Hierarchiesätze.

In den folgenden Jahren kommt es zu einer Reihe fundamentaler Ergebnisse. 1967 veröffentlichte Manuel Blum das Speedup-Theorem. 1969 folgt das Union-Theorem von Edward M. McCreight und Albert R. Meyer. Und 1972 veröffentlicht Allan Borodin das Gap-Theorem. Diese Ergebnisse lassen sich nicht nur als grundlegend für die Komplexitätstheorie ansehen, sie stellen auch ein Abtasten des noch neuen Forschungsgebietes dar, das sich zugleich noch durch möglichst „spektakuläre“ Ergebnisse rechtfertigen muss. So treffen diese Theoreme z. T. zwar überraschende Aussagen, sind aber mitunter auf Annahmen gebaut, die man heute einschränken würde. Beispielsweise werden keine "echten Komplexitätsfunktionen" (siehe oben) vorausgesetzt.

In derselben Zeit, die etwa die ersten zehn Jahre komplexitätstheoretischer Forschung umfasst, kommt es zur Formulierung der Klasse P als der Klasse der „praktisch lösbaren“ Probleme. Alan Cobham zeigt, dass die Polynomialzeit robust unter der Wahl des Maschinenmodells ist (was man heute unter der erweiterten Church-Turing These zusammenfasst). Darüber hinaus erweisen sich viele mathematische Funktionen als in Polynomialzeit berechenbar.

Die Klasse NP tritt zuerst bei Jack Edmonds auf den Plan, der jedoch zunächst nur eine informelle Definition gibt. Die Tatsache, dass zahlreiche wichtige Probleme in NP zu liegen scheinen, lässt diese Klasse jedoch bald als attraktives Forschungsfeld erscheinen. Der Begriff der Reduzierbarkeit und die darauf basierende NP-Vollständigkeit wird entwickelt und findet zuerst im Satz von Cook (1971) prägnanten Ausdruck: Das Erfüllbarkeitsproblem (SAT) ist NP-vollständig und damit ein "schwerstes" Problem in NP. Nebenbei bemerkt bezog sich die ursprüngliche Arbeit von Stephen Cook auf Tautologien (aussagenlogische Formeln, die durch "jede" Belegung erfüllt werden), während der Begriff der Erfüllbarkeit nicht erwähnt wird. Da die Ergebnisse bezüglich der Tautologien jedoch relativ einfach auf die Erfüllbarkeit übertragen werden können, rechnet man sie Stephen Cook zu. Einen Teil dieser Übertragung leistet Richard Karp (1972), indem er die Technik der Reduktion ausarbeitet. Völlig unabhängig von diesen Arbeiten entwickelte Leonid Levin (1973) in der damaligen Sowjetunion eine Theorie der NP-Vollständigkeit, die im Westen für lange Zeit unbeachtet blieb.

1979 veröffentlichen Michael R. Garey und David S. Johnson ein Buch, welches 300 NP-vollständige Probleme beschreibt ("Computers and intractability"). Dieses Buch wurde für künftige Forscher zu einer wichtigen Referenz.

1982 stellt Andrew Yao das Konzept der Falltürfunktionen (trapdoor functions) vor, die eine spezielle Art von Einwegfunktionen (one way functions) darstellen, und zeigt deren grundlegende Wichtigkeit in der Kryptographie auf. Jedoch genügt für die Schwierigkeit, einen Code zu knacken, die Worst-Case-Betrachtungsweise der Komplexitätsklassen wie NP nicht. Es dürfen vielmehr auch keine Algorithmen existieren, die diese Probleme in einem signifikanten Anteil aller Fälle effizient lösen. Dies korrespondiert zum Modell der probabilistischen Turingmaschine und motiviert die Einführung randomisierter Komplexitätsklassen wie ZPP, RP oder BPP (alle eingeführt von John T. Gill, 1977).

Mit dieser Übersicht wurden die wesentlichen Grundsteine der Geschichte der Komplexitätstheorie gelegt. Wie in anderen Forschungsgebieten auch, fächern sich die neueren Ergebnisse in viele, teils sehr spezielle Teilbereiche auf.




</doc>
<doc id="14707" url="https://de.wikipedia.org/wiki?curid=14707" title="Portsmouth (Rhode Island)">
Portsmouth (Rhode Island)

Portsmouth ist eine kleine Stadt im Newport County, im Südosten des US-Bundesstaates Rhode Island auf der gleichnamigen Insel in der Narragansett Bay. Zum Stadtgebiet zählt auch Dyer Island.

Portsmouth liegt an der Nordspitze der Insel Rhode Island. Das Stadtgebiet umfasst neben einem Teil von Rhode Island mehrere Inseln, darunter Prudence Island, Patience Island, Hope Island und Hog Island. Es hat eine Fläche von 153,6 km², mehr als die Hälfte davon besteht aus Wasserflächen. Die mittlere Höhe von Portsmouth beträgt 43 m.

Die Siedlungsfläche konzentriert sich vor allem auf die Ostküste der Insel. Der Westen des Stadtgebietes ist überwiegend ländlich geprägt und vom Weichbild der Stadt durch die Rhode Island State Route 24 getrennt. Die State Route ist über die "Sakonnet Bridge" die Hauptverbindung nach Fall River auf dem östlichen Festland in Massachusetts. Das nördliche Festland ist über die State Route 114 und die "Mount Hope Bridge" erreichbar. Die 114 stellt auch die Verbindung mit Newport im Süden der Insel her. Von der Westküste des Stadtgebietes führt eine Fährverbindung über Prudence Island nach Bristol.

Bei der Volkszählung 2000 wurden 17.149 Einwohner gezählt. Fast 96 % der Einwohner waren Weiße, der restliche Anteil verteilte sich auf verschiedene Volksgruppen. Das Pro-Kopf-Einkommen betrug 28.161 US-Dollar; 3,4 % der Bevölkerung lebte unter der Armutsgrenze.

Die Stadt wurde 1638 von Anne Hutchinson als zweite europäische Siedlung in Rhode Island gegründet. Eine große Rolle im Wirtschaftsleben der Stadt bildeten Fischfang und Schiffbau. Im 19. Jahrhundert war darüber hinaus der Steinkohlenbergbau wichtig.


</doc>
<doc id="14708" url="https://de.wikipedia.org/wiki?curid=14708" title="San Juan Islands">
San Juan Islands

Die San Juan Islands sind eine Inselgruppe im Nordwesten des US-Bundesstaats Washington. Je nach Definition von „Insel“ (in Abgrenzung zu aus dem Wasser ragenden Felsen) spricht man von 172 oder sogar mehreren hundert Inseln und Inselchen, die den San Juans zugerechnet werden. 172 ist dabei die Anzahl der Inseln, die groß genug sind, um mit eigenem Namen bezeichnet zu werden. Gelegentlich findet man auch die Angabe von 176 Inseln mit eigenem Namen. Auf den San Juan Islands leben etwa 16.000 Menschen.

Die Inseln liegen östlich der kanadischen Insel Vancouver Island beziehungsweise der Haro-Straße und des Boundary Pass und trennen die Juan-de-Fuca-Straße im Süden von der Straße von Georgia im Norden. Die Ostgrenze ist die Rosario-Straße. Die drei größten Inseln sind San Juan Island, Orcas Island und Lopez Island. 

Die San Juan Islands bilden das San Juan County.

Die Inseln gehörten vor Ankunft der ersten Europäer zum Siedlungsgebiet der Küsten-Salish, die Nooksack, die Northern Straits Salish (zu denen die Lummi gehören) und Klallam gerechnet werden. Die Küsten-Salish wanderten im Gefolge der Jahreszeiten innerhalb ihrer jeweiligen Gebiete, bewohnten ihre Dörfer also nur jeweils zeitweise. Die Lummi siedelten in diesem Sinn auf den San Juan Islands und legten dort Riffnetze auf Orcas Island, San Juan Island, aus, dazu am Point Roberts und am Sandy Point. Um 1780 schätzt man ihre Zahl auf etwa 1000 Menschen. Der erste Kontakt mit Europäern führte aber ab den späten 1770er Jahren zu Pocken-Epidemien unter ihnen und in den folgenden Jahrzehnten siedelten sie auf der Flucht vor den Pocken und plündernden Stämmen aus British Columbia auf das benachbarte Festland über. 

Der erste Europäer, der San Juan Island entdeckte, war 1791 ein Offizier unter dem Kommando des spanischen Entdeckers Francisco de Eliza namens Gonzalo López de Haro. Nach ihm ist die Haro-Straße benannt, die San Juan Island von Vancouver Island trennt. De Eliza gab der Insel und dem Archipel ihren Namen, "Isla y Archipelago de San Juan". Da die Expedition unter der Autorität des Vizekönigs von Mexiko, "Juan Vicente de Güemes Padilla Horcasitas y Aguayo", segelte, benannte De Eliza verschiedene Orte nach ihm, neben San Juan Island auch Orcas Island (als Kurzform von Horcasitas).

Ein Jahr zuvor hatten Spanier während einer Fahrt unter dem Kommando von Manuel Quimper die Inseln bereits gesichtet, ohne dass ihnen jedoch klar wurde, dass es sich um Inseln handelte. 

Den beiden spanischen Expeditionen folgten britische und US-amerikanische. George Vancouvers Expedition von 1792 und 1841 die Erkundungsfahrt von Charles Wilkes führten dazu, dass etliche spanische Benennungen der Inseln durch englische ersetzt wurden. Vancouver verfügte dabei noch über einige Informationen aus spanischen Quellen und übernahm meist die spanischen Bezeichnungen. Wilkes dagegen waren möglicherweise die spanischen Bezeichnungen nicht bekannt und er gab in etlichen Fällen Inseln und Wasserstraßen, die bereits von Spaniern oder von Vancouver benannt worden waren, neue Namen. Häufig ehrte er damit Mitglieder seiner Crew oder US-amerikanische Helden des Krieges von 1812 gegen die Briten. Die San Juan Islands etwa nannte er Navy Archipelago und etliche Inseln erhielten die Namen von amerikanischen Marine-Offizieren.

Um die daraus folgende Konfusion von Bezeichnungen auf unterschiedlichen Karten aufzulösen, startete die Britische Admiralität 1847 unter der Leitung von Henry Kellett ein Projekt zur Klärung der Namensfragen in der Region. Da die San Juans zu dieser Zeit von den Briten beansprucht waren, verschwanden hier bis auf wenige Ausnahmen die patriotischen amerikanischen Namen, die Wilkes vergeben hatte, wieder zugunsten der ursprünglichen spanischen bzw. von Vancouver vergebenen Namen.

Der Text des Oregon-Kompromisses von 1846, durch den der 49. Breitengrad als Grenze zwischen US-amerikanischem und britisch-kanadischem Territorium festgelegt wurde, sparte die Klärung der Zugehörigkeit der San Juan Islands aus. Diese Unklarheit führte 1859 zum sogenannten Schweinekonflikt um den Verlauf der Grenze zwischen den USA und Großbritannien im Bereich des Archipels. Ein von einem US-amerikanischen Landwirt erschossenes (britisches) Schwein war das einzige Opfer des Konfliktes. Endgültig beigelegt wurde der Grenzstreit erst 1872, nachdem der deutsche Kaiser Wilhelm I. um Vermittlung gebeten worden war.

Die Inseln sind Namensgeber für den Indianerstamm der "Mitchell Bay Band of San Juan Islands".

Der Roman "Insel der flüsternden Stimmen" von Laurie R. King spielt auf der fiktiven San Juan Insel "Folly".

Die Romane "Amber Beach" (dt. Bernsteinfeuer) und "Jade Island" (dt. Jadeherzen) von Elizabeth Lowell spielen in den San Juan Inseln. 

Der Roman "What I Didn't Say" von Keary Taylor spielt auf Orcas Island.




</doc>
<doc id="14711" url="https://de.wikipedia.org/wiki?curid=14711" title="Tragfläche">
Tragfläche

Die Tragfläche, auch Tragflügel oder Flügel, ist ein Bauteil eines Fahrzeugs, dessen Hauptaufgabe in der Erzeugung von dynamischem Auftrieb besteht. Die Funktion des Tragflügels besteht darin, durch Beeinflussung der Umströmung eine ausreichend große Kraft senkrecht zur Anströmrichtung zu erzeugen. Diese Kraft ist der Auftrieb, der ein Flugzeug in der Luft hält, oder ein Tragflächenboot aus dem Wasser hebt.

An Flugzeugen sind Tragflächen meist mit Klappen ausgestattet, mit denen die Fluglage, der Auftrieb oder der Luftwiderstand beeinflusst werden kann. Bei großen Flugzeugen hängen zumeist Triebwerke daran, zudem befinden sich die Treibstofftanks in den Tragflächen. Der Abstand zwischen linker und rechter Tragflächenspitze wird als Spannweite bezeichnet. 

Voraussetzung für die Erzeugung von Auftrieb durch Tragflächen ist die Bewegung in einem geeigneten Fluid (wie zum Beispiel Luft oder Wasser), das die Eigenschaften Masse, Viskosität und zumindest in gewissem Umfang Inkompressibilität aufweist. 

Tragflächen mit geeignetem Profil und Anstellwinkel lenken das anströmende Fluid um ("downwash"); dadurch wird eine senkrecht zur Anströmung wirkende Kraft erzeugt. Durch die Umlenkung wird dem Fluid ein Impuls übertragen. Nach dem ersten Newtonschen Gesetz erfordert diese Richtungsänderung der Strömung nach unten eine stetig wirkende Kraft. Nach dem dritten Newtonschen Gesetz (Actio und reactio) wirkt dabei eine gleiche und entgegengesetzte Kraft, der Auftrieb, auf die Tragfläche.

Die Masse der abgelenkten Luft formula_1 pro Zeiteinheit formula_2 ist abhängig von ihrer Dichte, von der Größe (Fläche) der Tragflächen und von der Fluggeschwindigkeit: je schneller das Flugzeug fliegt, umso mehr Luft wird in derselben Zeit abgelenkt. Die Beschleunigung der abgelenkten Luftmasse ist abhängig von der Fluggeschwindigkeit und vom Anstellwinkel der Tragfläche.

Bei konstanter Luftdichte, Tragflächengröße und gleichbleibendem Anstellwinkel ist die Auftriebskraft proportional zum Quadrat der Fluggeschwindigkeit: Sowohl die abgelenkte Luftmasse pro Zeiteinheit als auch deren vertikale Beschleunigung wachsen proportional mit der Fluggeschwindigkeit. Bei doppelter Fluggeschwindigkeit und ansonsten gleicher Anströmung der Luft verdoppelt sich sowohl die nach unten beschleunigte Luftmenge als auch ihre Geschwindigkeit. Das bedeutet, der Auftrieb vervierfacht sich.

Da aber die Ablenkungsgeschwindigkeit in die dafür benötigte Antriebsleistung "quadratisch" eingeht, ist die für die Auftriebserzeugung benötigte Leistung "umgekehrt proportional" zur Fluggeschwindigkeit sowie zur Größe der Tragflächen. Das bedeutet, je höher die Fluggeschwindigkeit oder je größer die Tragflächen, desto geringer die für den Auftrieb benötigte Antriebsleistung. (Diese ist jedoch kleiner als die gesamte für den Flug benötigte Antriebsleistung, siehe unten).

Der oben beschriebene Wirkmechanismus ist Teil des induzierten Luftwiderstandes: er entzieht dem Auftrieb liefernden Strömungssystem die dafür benötigte Energie in Form von Strömungswiderstand. Dieser Teil des induzierten Luftwiderstands lässt sich prinzipiell nicht beseitigen, da er physikalisch dem Energie- und Impulserhaltungssatz Rechnung trägt.

Eine weitere Form des induzierten Luftwiderstands wird durch Randwirbel an den Tragflächenenden verursacht: Hier entsteht ein Druckausgleich zwischen Überdruck unter dem Flügel und Unterdruck über dem Flügel. So entsteht an jeder Tragflächenspitze eine Randwirbel um die Längsachse des Flugzeugs, deren kinetische Energie dem Auftrieb erzeugenden Strömungssystem entzogen wird und so ungenutzt verloren geht. Die Randwirbel lassen sich durch eine hohe Streckung (= Verhältnis der Spannweite zur mittleren Flügeltiefe) verringern, aber bei endlichen Tragflügeln prinzipiell nicht völlig ausschalten. Auch die Winglets an den Tragflächenenden moderner Flugzeuge dienen der Verringerung dieser Widerstandsform, indem sie den Druckausgleich quer zur Flugrichtung (und damit die Wirbelbildung) teilweise unterbinden. Dabei ist zu beachten, dass die Gesamtwirbelstärke der Randwirbel auch durch Winglets wegen des Helmholzschen Wirbelsatzes nicht beeinflusst werden kann. Eine Verminderung der Wirbelstärke würde nach dem Satz von Kutta-Joukowski auch eine Verringerung des Gesamtauftriebes des Flugzeuges bedeuten. Winglets können jedoch durch geschickte Verlagerung der Wirbel einen positiven Einfluss auf die Auftriebsverteilung haben und so den induzierten Luftwiderstand senken. Weiterhin ist es möglich, durch Winglets die Flugeigenschaften im unteren Geschwindigkeitsbereich positiv zu beeinflussen.

Neben dem induzierten Luftwiderstand erhöhen weitere Formen von Strömungswiderstand den Leistungsbedarf eines Flugzeugs:

Der Reibungswiderstand an der Oberfläche der Tragfläche bremst das Flugzeug, indem er in der Grenzschicht Bewegungsenergie in Wärmeenergie wandelt. Der Reibungswiderstand (oder Schubspannungswiderstand) ist davon abhängig, ob die anliegende Strömung laminar oder turbulent ist. Er kann durch eine hohe Oberflächengüte (Glätte) gemindert werden, indem die Strömung laminarer gehalten wird, aber nicht völlig ausgeschaltet werden. Auch Riblets können den Reibungswiderstand verringern.

Der Form- oder Druckwiderstand kommt dadurch zustande, dass der Druck auf der Vorder- und Rückseite eines umströmten Körpers unterschiedlich ist. Wo die Strömung in Turbulenz umschlägt – generell an der Hinterkante der Tragfläche, aber z. B. auch an den Kanten von Landeklappen und Querrudern etc. – , entsteht ein bremsender Sog, der dem Querschnitt des Strömungsabrisses entspricht. Der Formwiderstand kann durch eine sinnvolle Wahl und sorgfältige Ausformung des Tragflächenprofils minimiert werden.

Der Wellenwiderstand schließlich kommt im Überschallflug zum Tragen: hier induziert der überschallschnelle Aufprall der Luftteilchen auf die Vorderseite des Flugzeugs eine sich kegelförmig ausbreitende Stoßwelle (Machscher Kegel), die am Boden als Überschallknall wahrnehmbar ist.

Der Strömungswiderstand (und damit der Leistungsbedarf zu dessen Überwindung) steigt mit dem Quadrat der Fluggeschwindigkeit. Zusammen mit dem zur Fluggeschwindigkeit umgekehrt proportionalen Leistungsbedarf für die Auftriebserzeugung ergibt sich konstruktionsabhängig für jedes Flugzeug eine bestimmte Geschwindigkeit, bei der – auf die Flugzeit bezogen – der Energiebedarf für den Horizontalflug am geringsten ist. Auf die "Flugstrecke" bezogen liegt das Minimum des Energieverbrauchs jedoch bei einer deutlich höheren Geschwindigkeit, da das Flugzeug dann für dieselbe Strecke weniger lange in der Luft gehalten werden muss. Die Geschwindigkeit mit dem geringsten Energieverbrauch pro Strecke nennt man "Reisegeschwindigkeit".

Der zur Auftriebserzeugung erforderliche Anstellwinkel steigt bei niedrigen Geschwindigkeiten: Da bei höherer Geschwindigkeit im selben Zeitraum mehr Luftmasse abgelenkt wird und der Betrag der vertikalen Beschleunigung ebenfalls steigt, genügt ein geringerer Ablenkungswinkel zur Erzeugung desselben Auftriebs. Umgekehrt muss der Anstellwinkel umso mehr erhöht werden, je langsamer das Flugzeug fliegt.

Der Coandă-Effekt kann an der Oberseite der Tragfläche nur bis zu einem bestimmten, von Profilform, Oberflächenqualität und Reynolds-Zahl abhängigen Anstellwinkel ein Anliegen der Strömung gewährleisten, welches in der Regel um 15-20° liegt. Jenseits dieses Anstellwinkels reißt die Strömung von der Oberfläche ab. Dies bewirkt eine drastische Erhöhung des Formwiderstands, gleichzeitig bricht der größere Teil des Auftriebs zusammen, da das Profil in diesem Strömungszustand den Luftstrom an der Oberseite der Tragfläche nicht mehr effektiv ablenken kann, sondern im Wesentlichen nur noch verwirbelt. Die Fluggeschwindigkeit, bei der aufgrund des gestiegenen Anstellwinkels die Strömung abreißt, nennt man "Überziehgeschwindigkeit" oder Stallspeed; der dabei entstehende Flugzustand, in dem das Flugzeug durchsackt und nur noch sehr eingeschränkt steuerbar ist, ist der (englisch) Stall. Die Überziehgeschwindigkeit ist somit die niedrigste Geschwindigkeit, bei der sich ein Flugzeug gerade noch in der Luft halten kann; sie ist konstruktionsabhängig und reicht in der Praxis von ca. 20 km/h (Gleitschirm) bis zu ca. 300 km/h (schnelle Strahlflugzeuge ohne aktivierte Landehilfen).

Die Überziehgeschwindigkeit ist bei einem Flugzeug vom Gewicht und vom Lastvielfachen abhängig, d. h. von der zusätzlichen Beschleunigung, die beispielsweise in Kurvenflug entsteht. Außerdem steigt die Überziehgeschwindigkeit ("true air speed") bei kleinerer Luftdichte. Die angezeigte Geschwindigkeit ("indicated air speed") ist jedoch dabei dieselbe, da die mechanischen Instrumente auch von der Luftdichte in gleichem Maße beeinflusst werden. 

Das Profil einer Tragfläche ist der Querschnitt durch dieselbe in Strömungsrichtung. Die Form des Profils dient einerseits dazu, möglichst viel Auftrieb bei möglichst wenig Strömungswiderstand zu erreichen, und andererseits dazu, einen möglichst großen Anstellwinkel-Bereich ohne Strömungsabriss zu ermöglichen. Je nach Konstruktion (Einsatzzweck, Geschwindigkeitsbereich, Flächenbelastung) werden dazu unterschiedliche Profile verwendet.

In der Frühzeit der Fliegerei waren die Tragflächengrundrisse in ihrer Form dem Vogelflügel nachempfunden, da zunächst das gewölbte Profil von Bedeutung war. Zum Tragflächenprofil haben vor allem Otto Lilienthal (Wölbung) und Hugo Junkers (Profildicke) entscheidende Beiträge geleistet. Heutige Tragflächen haben eine Vielzahl verschiedener Formen. In der Regel sind sie lang gestreckt und verjüngen sich im Außenbereich (Zuspitzung), um eine bessere Auftriebsverteilung und somit einen geringeren induzierten Luftwiderstand zu erreichen.

Bei moderneren Verkehrsflugzeugen gehen sie in so genannte Winglets über. Durch den geringeren Luftdruck auf der Oberseite der Tragflächen strömt die Luft an deren Spitzen von unten nach oben. So entstehen Luftwirbel, die sich unter anderem in den gefürchteten Wirbelschleppen fortsetzen. Die Winglets verbessern die Verteilung der Randwirbel, reduzieren so den Energieverlust, den die Wirbelschleppen mit sich bringen, und machen so das Flugzeug sparsamer im Verbrauch. Anders als oft angenommen, kann die Stärke der Wirbeln bei konstanter Geschwindigkeit nicht geändert werden, da sie direkt mit der Entstehung von Auftrieb verbunden ist. Die Winglets können nur die Auftriebsverteilung und damit die Geometrie der Wirbeln vergünstigen.

Überschallflugzeuge haben oft Deltaflügel, deren Vorderkanten in der Regel gerade verlaufen, im Extremfall aber auch mehrfach gekrümmt sein können, wie z. B. bei der „Ogival“-Tragfläche der Concorde. Deltaflügel sind den beim überschallschnellen Flug auftretenden Effekten besser angepasst als der sonst üblicherweise eingesetzte Trapezflügel.
Beim Flug mit Überschallgeschwindigkeit treten Verdichtungsstöße auf. Dies sind Bereiche, in denen der Druck des umgebenden Fluids, also der Luft, sprunghaft ansteigt. Einige dieser Stöße breiten sich in einer Form um das Flugzeug aus, der die Pfeilung des Flügels angepasst ist. (Je höher die gewünschte Fluggeschwindigkeit, umso stärker muss der Flügel gepfeilt sein.)
Beim Flug mit Überschallgeschwindigkeit tritt ein (schräger) Stoß an der Vorderkante auf. Beim Flug mit Transschallgeschwindigkeit tritt ein (senkrechter) Stoß auf der Flügeloberseite auf, hinter dem die Geschwindigkeit der Luftströmung plötzlich in den Unterschall fällt, was eine Umkehrung einiger strömungmechanischer Effekte zur Folge hat. Kombiniert man also durch eine falsche Flügelkonfiguration diese unterschiedlichen Effekte auf einem Flügel, können sich diese gegenseitig eliminieren.
Man erhält eine homogene Anströmungsgeschwindigkeit auf die Vorderkante des Flügels, wenn diese der Anströmung selbst angepasst ist.
Durch die Pfeilung verringert sich diese Geschwindigkeit mit dem Kosinus des Pfeilwinkels und führt zum Verlust von Auftrieb. Nachteilig ist außerdem, dass neben dieser Normalgeschwindigkeit auch eine Tangentialkomponente auftritt, die sich entsprechend vergrößert. Diese bewirkt ein Abschwimmen der Grenzschicht zum äußeren Flügelbereich hin. Dadurch wird die Grenzschicht aufgedickt, und es kann zu einem Ablösen der Strömung an den Flügelspitzen kommen. Dies verringert die Querruderwirksamkeit.

Daneben sind noch eine Reihe weiterer Formen, zum Beispiel ringförmige Tragflächen (Ringflügel) möglich, die aber bislang nur bei Modell- und Experimentalflugzeugen verwirklicht wurden.

Insbesondere bei Flugzeugen mit Strahlantrieb („Düsenflugzeuge“) sind die Tragflächen zum Ermöglichen des Überschallflugs oft pfeilförmig nach hinten abgewinkelt. Eine Reihe von Militärflugzeugen, die in den 1960er und 1970er Jahren konstruiert wurden, können durch eine variable Geometrie die Pfeilung ihrer Tragflächen im Flug verstellen (Schwenkflügel), um sie optimal an die jeweilige Geschwindigkeit anzupassen.

Ein Forscherteam (Fish/Howle/Murray) hat 2008 nach dem Vorbild der Vorderflossen des Buckelwals eine Flügelform im Windkanal erprobt, die an der Vorderkante gewellt ist. Dadurch konnte gegenüber einem sonst gleichen Flügel mit gerader Vorderkante der Auftrieb um bis zu 8 Prozent gesteigert und gleichzeitig der Luftwiderstand um bis zu 32 Prozent gesenkt werden. Der Anstellwinkel, bei dem es zum Strömungsabriss (Stall) kam, lag 40 Prozent höher. Der Grund für diese guten Leistungsdaten liegt in der Energieeinleitung in die Strömung durch die gewellte Vorderkante (ähnlich Vortexgeneratoren).

Je nach Höhe der Anbringung der Tragflächen teilt man Flugzeuge in Tiefdecker (die Tragflächen sitzen bündig mit der Rumpfunterkante), Mitteldecker (mittlere Höhe), Schulterdecker (bündig mit der Rumpfoberkante) und Hochdecker (Tragflächen über dem Rumpf) ein. Flugzeuge, bei denen das Höhenleitwerk vor dem Flügel angeordnet ist, heißen Enten- oder Canardflugzeuge, Flugzeuge, bei denen das Höhenleitwerk hinter dem Flügel angeordnet ist, heißen Drachenflugzeuge. Moderne Großraumflugzeuge sind als Tiefdecker konstruiert, wobei die beiden Flügel über einen Flügelmittelkasten mit dem Flugzeugrumpf verbunden werden.

Die meisten modernen Flugzeuge besitzen auf jeder Seite des Rumpfs eine Tragflächenhälfte. In den ersten Jahrzehnten der Fliegerei waren Doppeldecker mit jeweils zwei Tragflächen übereinander häufig, vereinzelt wurden sogar Dreidecker gebaut. Heute werden Doppeldecker nur noch für den Kunstflug gebaut. Es gibt auch Flugzeuge mit nur einer Tragfläche, ohne Leitwerk. Solche nennt man Nurflügel oder auch Schwanzlose. Flugzeuge mit zwei oder mehreren hintereinander angeordneten Tragflächen (Tandemanordnung) blieben eine Rarität. Als weitere Variante gibt es noch die Boxwing-Tragfläche, die in der Praxis bisher nur bei Modellflugzeugen und dem Ultraleichtflugzeug Sunny verwendet wird.

Die "Flügelstellung" ist grob gekennzeichnet durch die Form ihrer Stirnansicht. Sie kann "gerade" sein, eine mehr oder weniger stark ausgeprägte "V-Stellung" aufweisen oder sich als Knickflügel präsentieren.

Anders als bei den Flügeln der Tiere, die Vortrieb und Auftrieb erzeugen, liefern Tragflächen nur Auftrieb. Der Vortrieb muss von separaten Triebwerken erzeugt werden. Zu Beginn der Fliegerei wurde mit Tragflächen experimentiert, die den Flügelschlag der Vögel nachahmen und dadurch Vortrieb erzeugen sollten. Diese Konstruktionen (Schwingenflugzeuge oder auch Ornithopter) erwiesen sich jedoch für die manntragende Fliegerei als ungeeignet und wurden bisher nur im Modellflug erfolgreich verwirklicht.

Die einzige praktikable Lösung einer Kombination von Vor- und Auftrieb in der Tragfläche besteht darin, die Tragflächen um eine vertikale Achse rotieren zu lassen. In diesem Fall spricht man dann aber von einem Rotorblatt (siehe Hubschrauber).

Tragflächen moderner Flugzeuge erfüllen noch eine Reihe weiterer Funktionen:




 

</doc>
<doc id="14712" url="https://de.wikipedia.org/wiki?curid=14712" title="Statischer Auftrieb">
Statischer Auftrieb

Der statische Auftrieb ist eine der Schwerkraft entgegengesetzte Kraft auf einen Körper in Flüssigkeiten oder Gasen. Der statische Auftrieb wird durch die Verdrängung des umgebenden Mediums hervorgerufen. Er bewirkt, dass Schiffe schwimmen, Heißluftballone schweben, Luftblasen in Wasser, heiße Abgase und Gewitterwolken aufsteigen.

Die Stärke des statischen Auftriebs ergibt sich aus dem archimedischen Prinzip.

Die Ursache für die Auftriebskraft liegt darin, dass der hydrostatische Druck von der Höhe des betrachteten Orts abhängt. Auf die Unterseite des Körpers wirkt ein höherer Druck als auf die Oberseite. Wenn kein Fluid an die Unterseite des Körpers gelangen kann, dann gilt das archimedische Prinzip nicht. In diesem Fall ergibt sich keine Auftriebskraft.

Der statische Auftrieb entspricht der Gewichtskraft der entsprechenden Verformung des Fluids. Das Fluid kann eine Flüssigkeit wie Wasser oder auch ein Gas wie etwa Luft sein. Dieser Zusammenhang ist als archimedisches Prinzip bekannt. Auf einen Körper, der in ein Fluid mit der Dichte formula_1 getaucht ist, wirkt also eine Auftriebskraft formula_2 mit dem Betrag:
Dabei ist formula_4 das vom Körper verdrängte Volumen des Fluids. Das Produkt formula_5 ist die Masse des vom Körper verdrängten Fluids. Und formula_6 ist ihre Gewichtskraft.

Das hydrostatische Paradoxon sagt aus, dass der Druck nur von der Tiefe und nicht von der Form eines Fluids abhängt. Daher ist die Auftriebskraft unabhängig von der Menge des Fluids, in dem der Körper eingetaucht ist.

Das Prinzip gilt demnach auch, wenn etwa die vorhandene Flüssigkeit ein geringeres Volumen besitzt als der eingetauchte Teil des Schwimmkörpers. Andererseits entsteht nur dann eine Auftriebskraft, wenn sich "unterhalb des Körpers" Fluid befindet. (Dieses ist bspw. nicht der Fall bei einem senkrecht stehenden geraden Zylinder, der auf dem Grund steht.)


Vulkanismus, Geysire, Schlammtöpfe, Gas- und Erdölquellen, doch auch Artesische Brunnen, basieren auf Auftriebseffekten ebenso wie das Sich-Durchmischen einer von unten erhitzten Flüssigkeit, das sich beim Kochen durch das Aufsteigen von Dampfblasen vom Gefäßgrund noch verstärkt.

Gelagerte, unbefestigte Güter, die in Wasser aufschwimmen können, stellen bei Hochwasser ein Gefahrenpotential dar. Ein Heizöltank kann im gefluteten Tankraum aufschwimmen, kippen, Leitungen können abreißen und lecken. Geschlägertes Holz, Seecontainer oder Wechselaufbauten können bei erhöhten Wasserstand abtreiben und durch Anstoßen an Brückenpfeilern, Verklausen (Verschließen der Durchflüsse) von Brücken oder Rammen von Hochwasserschutzwänden zu schweren Schäden führen.



</doc>
<doc id="14713" url="https://de.wikipedia.org/wiki?curid=14713" title="Trockeneis">
Trockeneis

Trockeneis ist festes Kohlenstoffdioxid (CO), das unter Normaldruck bei −78,48 °C sublimiert, also direkt in die Gasphase übergeht, ohne vorher zu schmelzen.

Auf der Erde kommt Trockeneis nicht natürlich vor, in der CO-reichen Atmosphäre des Mars friert es jedoch im Winter in höheren Breiten aus und bildet die bekannten Polkappen. Auch einige Meteoroiden enthalten größere Mengen davon.

Trockeneis ist ein weißer, wassereisähnlicher und geruchloser Feststoff. Bei Normaldruck sublimiert Trockeneis bei −78,48 °C (194,67 K) und benötigt für diesen Phasenübergang 571,1 kJ/kg (bzw. 25,1 kJ/mol). Da der Tripelpunkt "P" von Kohlenstoffdioxid bei "T" = 216,58 K (−56,57 °C) und "p" = 5,185 bar liegt, kann Kohlenstoffdioxid erst oberhalb von 5,185 bar schmelzen. Die Dichte beträgt ungefähr 1,56 g·cm und liegt damit über der des Wassers. Der kritische Punkt "P" von Kohlenstoffdioxid liegt bei einer Temperatur von "T" = 304,13 K (30,98 °C) und einem Druck von "p" = 73,75 bar.

1 m³ Trockeneis wiegt je nach Pressdruck etwa 1500 kg, während 1 m³ gasförmiges Kohlenstoffdioxid bei 0 °C und 1013 hPa nur 1,98 kg wiegt. Das bedeutet bei der Sublimation eine Volumenänderung auf das 760-fache.

Die Mohs-Härte von Trockeneis beträgt je nach Pressdruck bei der Herstellung 1 bis 3.

Trockeneis wird hergestellt, indem unter Druck verflüssigtes Kohlenstoffdioxid entspannt wird. Ein Teil des Kohlenstoffdioxids verdampft und entzieht dabei dem Rest des Kohlenstoffdioxids die für die Verdampfung erforderliche Wärme, dieser kühlt damit ab. Es entsteht dabei sogenannter gefrorener Kohlensäureschnee. Dieser wird dann, je nach Anwendung, in die gewünschte Form gepresst.

Kleine Mengen Trockeneis können erzeugt werden, indem man eine CO-Flasche auf den Kopf stellt und aufdreht. Das auslaufende flüssige CO verdampft zum größten Teil sofort und kühlt sich dadurch so stark ab, dass ein Teil als CO-Schnee gefriert. Nach diesem Prinzip funktionieren auch CO-Löscher, die – neben der Verdrängung des Luftsauerstoffes – dem Feuer Wärme entziehen und die brennenden Stoffe unter den Flammpunkt abkühlen sollen.

Trockeneis wird überwiegend als Kühlmittel verwendet.




Beim Umgang mit Trockeneis sollten Handschuhe und Schutzbrille getragen werden. Bei versehentlichem Hautkontakt sublimiert es zwar an seiner Oberfläche und bildet eine dünne schützende Gasschicht, die den direkten Kontakt zur Haut verhindert (Leidenfrost-Effekt). Bekommt Trockeneis trotzdem über mehrere Sekunden einen festen Kontakt zur Haut, wird die Sublimierung unterbrochen und das Eis bleibt, ähnlich wie beim Anfassen tiefkalter Metallgegenstände, an der Haut kleben. Um langzeitige Schädigungen der Haut und der darunter liegenden Gewebeschichten zu verhindern, muss es augenblicklich entfernt (abgerissen) werden, da es sonst zu einer Kälteverbrennung kommt, bei der das Gewebe in wenigen Sekunden abstirbt. Die Schädigung breitet sich kegelförmig nach innen aus. Dieser Vorgang wird bei der Behandlung von Warzen medizinisch genutzt.

Bei der Aufbewahrung in geschlossenen Räumen verdrängt das entstehende Kohlenstoffdioxidgas aufgrund seiner höheren Dichte die Luft am Boden. Bei der Sublimation dehnt sich das Gas auf das 760-fache des ursprünglichen Volumens aus, weswegen auch kleine Mengen Trockeneis einen Raum völlig mit Gas füllen können. In Konzentrationen oberhalb von 5 % wirkt es auch bei noch ausreichendem Sauerstoffgehalt der Atemluft erstickend. In geschlossenen Behältern kann sich ein gefährlicher Gasdruck aufbauen.

Handelsübliches Trockeneis für den industriellen Einsatz kann unter Umständen Verunreinigungen enthalten und sollte daher nicht zum Kühlen von Nahrungsmitteln (Getränken) verwendet werden. Das Verschlucken von Trockeneis ist aufgrund von Erstickungsgefahr und Kälteverbrennungen lebensgefährlich.



</doc>
<doc id="14721" url="https://de.wikipedia.org/wiki?curid=14721" title="Antoninus Pius">
Antoninus Pius

Antoninus Pius (* 19. September 86 bei Lanuvium; † 7. März 161 ebenda) war vom 10. Juli 138 bis zu seinem Tod römischer Kaiser. Das Römische Reich erlebte unter ihm, dem vierten der sechs Adoptivkaiser und Gründer der Antoninischen Dynastie, seine letzte längere Friedensperiode. Sein Geburtsname war "Titus Aurelius Fulvus Boionius Arrius Antoninus". Als Kaiser nannte er sich "Titus Aelius Hadrianus Antoninus Augustus Pius".

Antoninus’ Familie stammte aus Nemausus (Nîmes) im südlichen Gallien (Provinz Gallia Narbonensis). Seine Eltern waren Titus Aurelius Fulvus, der 89 das Konsulat bekleidete, und Arria Fadilla, die Tochter des zweimaligen Suffektkonsuls Gnaeus Arrius Antoninus. Antoninus wuchs in Lorium bei Rom auf und heiratete um 110 Annia Galeria Faustina. Er durchlief eine normale senatorische Karriere (Quästor 111, Prätor 117, Konsul 120, Prokonsul der Provinz Asia 135/136 oder ein Jahr früher). Hadrian machte Antoninus zu einem der vier ehemaligen Konsuln, die in Italien für die Rechtsprechung zuständig waren, und zum Mitglied seines Thronrats "(consilium)".

Hadrian bestimmte ihn am 24. Januar 138 zu seinem Nachfolger, ließ ihn zum "Caesar" erheben und adoptierte ihn am 25. Februar, nachdem der vorgesehene Thronerbe und "Caesar" Lucius Aelius gestorben war. Antoninus sollte zugleich einen Neffen seiner Frau, Marcus Annius Verus (den späteren Kaiser Mark Aurel), und den Sohn des Aelius (später als Lucius Verus bekannt) adoptieren. Offensichtlich sollte der bereits 51-Jährige nur als Platzhalter dienen; und zwar laut der späteren Tradition für Annius Verus, seinen angeheirateten Neffen (und damit wohl nächsten männlichen Verwandten), der selbst noch zu jung für das Kaisertum war. Es spricht allerdings einiges dafür, dass der sterbenskranke Hadrian eigentlich den noch jüngeren Lucius Verus bevorzugen wollte, der im Februar 138 mit Antoninus' Tochter Faustina verlobt wurde. 

Nach dem Tod Hadrians im Sommer desselben Jahres wurde Antoninus sein Nachfolger als "Augustus" und "Imperator". Er griff sogleich entscheidend in die von Hadrian festgelegte Regelung ein und hob Mark Aurel stärker gegenüber Lucius Verus hervor: Die von Hadrian angeordnete Verlobung seiner Tochter mit Lucius Verus wurde gelöst, stattdessen verheiratete Antoninus sie mit Mark Aurel (Annius Verus) und machte diesen so zum einzigen kaiserlichen Schwiegersohn.

Die Regierungszeit des Antoninus Pius war nach Augustus die zweitlängste eines römischen Kaisers vor der Spätantike, obwohl er eigentlich nur als Übergangskaiser vorgesehen war.

In der Forschung wird erwogen, dass Hadrian den militärisch nicht besonders hervorgetretenen Antoninus als seinen Nachfolger auswählte, weil er sich von ihm eine Fortsetzung seiner auf Konsolidierung des Reiches, nicht auf Expansion angelegten Politik versprach. Allerdings ließ Antoninus schon am Beginn seiner Regierungszeit, vielleicht schon ab 139, spätestens in den Jahren 142 bis 144, durch Quintus Lollius Urbicus die Grenze in Britannien zu dem nach ihm benannten Antoninuswall vorverlegen, der etwa 160 km weiter nördlich als der von seinem Vorgänger errichtete Hadrianswall vom Firth of Forth zum Firth of Clyde verläuft. Der Kaiser ließ sich nochmals als Imperator akklamieren; vermutlich wollte er durch einen militärischen Erfolg Zweifel an seiner Eignung zum Herrscher zerstreuen. Umstritten ist aufgrund der schlechten Quellenlage, ob Antoninus, der anlässlich der Operationen Gedenkmünzen schlagen ließ, überdies auch einen Triumphzug abhielt.
Auch in Obergermanien wurde vermutlich im Jahre 159 der Limes um 25 bis 30 Kilometer nach Osten vorverlegt. Von größeren Krisen blieb das Reich verschont, aber Unruhen und kleinere Konflikte gab es an weiteren Grenzen des Reiches, so von 145 bis 152 in Mauretanien, anschließend von 152 bis 153 in Oberägypten und Dakien. Die Herrschaft des Antoninus war also keine reine Friedenszeit. An der mittleren Donau trug Antoninus durch die Einsetzung eines quadischen Königs zur Grenzsicherung bei (Münzprägungen tragen die Legende "rex Quadis datus", „den Quaden wurde ein König gegeben“). Im Osten bauten sich gegen Ende der Regierungszeit wegen der Besetzung des armenischen Throns Spannungen mit den Parthern auf; seit etwa 158 wurden Truppen und erfahrene Heerführer an die römische Ostgrenze verlegt (siehe auch Partherkrieg des Lucius Verus). Unmittelbar nach Antoninus’ Tod brach unter seinen Nachfolgern der Krieg aus, ebenso einige Jahre später an der Donaugrenze. Ob Antoninus an diesen Entwicklungen eine Mitschuld trug, ist in der Forschung umstritten.

Im Inneren pflegte Antoninus im Gegensatz zu seinem Vorgänger ein demonstrativ gutes Verhältnis zum Senat. Seitdem er im ersten Jahr seiner Regierung die Vergöttlichung Hadrians durchgesetzt hatte, trug er den Beinamen "Pius" („der Fromme“). 145 n. Chr. weihte er den zu Ehren Hadrians errichteten Tempel ein, das sog. Hadrianeum. Anders als die Kaiser vor und nach ihm verließ Antoninus Italien während seiner Regierungszeit nie, kümmerte sich aber durch seine Statthalter oder Briefe (von denen einige inschriftlich in Städten wie Ephesos erhalten sind) um die Verwaltung des Reiches. 

Der späten (unzuverlässigen) Überlieferung zufolge sah Antoninus seinen eigenen Tod kommen und ließ in der Nacht seines möglicherweise einer Lebensmittelvergiftung folgenden Sterbens die kleine Statue der Göttin Fortuna aus seinem Schlafgemach in das seines Nachfolgers Mark Aurel bringen. An die Prätorianer, die bei Kaiserwechseln und ungeklärten Machtfragen immer nervös waren, ließ er die Parole „Gleichmut“ ausgeben.

Der Leichnam des Antoninus Pius wurde im Hadriansmausoleum (der späteren Engelsburg) beigesetzt.

Für die lange Regierungszeit des Antoninus ist die Quellenlage ungünstig. Der ruhige Verlauf seiner Herrschaftszeit, der Mangel an spektakulären Ereignissen, das Ausbleiben größerer militärischer Auseinandersetzungen dürften zur relativen Kargheit der Berichte der antiken Geschichtsschreiber wesentlich beigetragen haben. Die Hauptquelle ist die spätantike Biographie des Kaisers in der "Historia Augusta". Dieses Geschichtswerk gilt generell als unzuverlässig, denn manche seiner Lebensbeschreibungen von Kaisern bieten frei erfundene Angaben. Die kurze Darstellung der Regierung des Antoninus gehört jedoch zu den wertvollen Teilen des Werks; sie ist im Allgemeinen glaubwürdig, denn sie enthält Material aus guter älterer Überlieferung. Cassius Dio behandelte die Zeit im siebzigsten Buch seines Geschichtswerks, das aber nur fragmentarisch erhalten ist. Weitere literarische Quellen sind die Briefe des Marcus Cornelius Fronto und die "Selbstbetrachtungen" Mark Aurels. Hinzu kommen numismatische und archäologische Zeugnisse. Die Münzen liefern wertvolle Informationen über die Selbstdarstellung des Kaisers. 

Der zurückhaltende Regierungsstil des Antoninus fand in der Führungsschicht des Reichs große Anerkennung. Bei der römischen Stadtbevölkerung hingegen scheint seine Milde seiner Autorität geschadet zu haben. Eine spätantike Quelle, die "Epitome de Caesaribus", berichtet, der Kaiser sei, als man eine bevorstehende Versorgungskrise befürchtet habe, mit Steinen beworfen worden und habe daraufhin, statt den Aufruhr niederzuschlagen, der Menge die Lage erklärt und sie damit beruhigt. 

Die Konsekration des Antoninus erfolgte schon anlässlich der Feierlichkeiten bei seiner Beisetzung. Der von ihm für die 141 verstorbene Faustina erbaute Tempel auf dem Forum Romanum wurde nach seinem Tod auch seinem Kult gewidmet; der Tempel des Antoninus Pius und der Faustina gehört heute zu den am besten erhaltenen römischen Tempeln. Außerdem wurde ihm zu Ehren eine Säule auf dem Marsfeld errichtet. Von ihr ist nur die Basis erhalten.

Von seinem Nachfolger Mark Aurel wurde Antoninus ausführlich gerühmt, und das Urteil der antiken Geschichtsschreiber über seinen Charakter und seine Regierung fiel einhellig sehr positiv aus. 

Hinsichtlich der Innen-, Rechts- und Finanzpolitik teilt die moderne Forschung im Wesentlichen die günstige Einschätzung der Leistungen des Kaisers in den Quellen und würdigt die Erfolge seiner konsensorientierten Vorgehensweise. Auch die Berechtigung der sehr vorteilhaften antiken Urteile über seinen Charakter wird nicht bezweifelt. Die rühmenden Worte Mark Aurels werden oft ausführlich zitiert. Willy Hüttl, der 1933–1936 eine umfangreiche wissenschaftliche Biographie des Kaisers veröffentlichte, ein zweibändiges, jahrzehntelang maßgebliches Standardwerk, nannte ihn eine der idealsten Herrschergestalten unter den römischen Kaisern. Ernst Kornemann (1939) bezeichnete ihn als „einen tüchtigen Juristen und Verwaltungsbeamten“. Alfred Heuß (1960) konstatierte, Antoninus sei „ein äußerst gewissenhafter Mensch von strenger Pflichtauffassung“ gewesen; er habe das monarchische Ideal des Zeitalters erfüllt. Anthony Birley (1966) meinte, Antoninus habe „das Beispiel eines großen Charakters auf dem Thron“ gegeben. Karl Christ (1988) stellte fest, die „zentralen Bereiche, nämlich Finanzpolitik und Staatsverwaltung“ hätten unter Antoninus „eine geradezu reibungslose Perfektion“ erlangt. Der Kaiser sei den Erwartungen der auf Frieden und Wohlstand hoffenden Bevölkerung „in geradezu vollkommener Weise“ entgegengekommen: „Denn sein großer Erfolg als Herrscher, seine allgemeine Beliebtheit, beruhen eben darin, dass Forderungen und Wünsche seiner Zeit in ungewöhnlicher Form mit seinen eigenen Intentionen übereinstimmten, ja dass er sie in höchstem Maße verkörperte.“ Ähnlich urteilte Hildegard Temporini-Gräfin Vitzthum (1997). Sie schrieb, Antoninus sei „ein perfekter Verwalter“ gewesen: „Die gefestigten Verwaltungsstrukturen funktionierten mit gleichbleibender Zuverlässigkeit. Trotz großer Freigebigkeit in Rom und den Provinzen hinterließ Antoninus gut gefüllte Staatskassen.“ 

Auf dem Gebiet der Kultur- und Religionspolitik wird die konservative Haltung des Kaisers und seine Betonung des Römertums – im Gegensatz zu Hadrians Begeisterung für die griechische Kultur – hervorgehoben. Seine Traditionspflege wird teils als sinnvoll, teils als unzeitgemäß beurteilt. Alfred von Domaszewski bemerkte 1909, Antoninus habe „die Festbräuche einer gänzlich erstarrten Religion der grauen Urzeit“ erneuert. Ernst Kornemann (1939) meinte, Antoninus habe „die Gefahr erkannt, die dem römisch-italischen Primat im Reiche vom Hellenismus und vom hellenistischen Orientalismus her“ gedroht habe. Er habe nicht an „Hadrians romantischem Treiben“ festgehalten, sondern „Wandlung zu schaffen versucht durch die Wiederbelebung des altrömischen Glaubens und der hohen Tugenden der Altvorderen“. Karl Christ (1988) schrieb, in der Religionspolitik habe sich ein „betonter, stark ausgeprägter Archaismus“ gespiegelt, der für Antoninus charakteristisch sei. Hildegard Temporini-Gräfin Vitzthum (1997) wies darauf hin, dass kein Kaiser seit Augustus „so stark wie er für die Rückbesinnung auf die kultischen und mythischen Wurzeln Roms gewirkt“ habe. Bernard Rémy (2005) wandte sich gegen die Vorstellung, Antoninus habe einen bornierten Konservatismus gepflegt und versucht, längst aufgegebene Riten zu erneuern. 

Unterschiedlich und meist ungünstig beurteilen die Historiker die Militär- und Außenpolitik. Verbreitet ist unter ihnen die Auffassung, Antoninus’ Mangel an militärischer Erfahrung habe zu einer Vernachlässigung der Sicherheitspolitik geführt. Er habe nicht bemerkt, dass eine kritische Situation entstanden sei, deren militärische Bewältigung schließlich seinem Nachfolger aufgebürdet worden sei. Schon Theodor Mommsen urteilte 1883 in einer Vorlesung, Antoninus sei „übertrieben friedliebend“ gewesen. Alfred von Domaszewski (1909) bemängelte eine Lockerung der militärischen Disziplin: „Die eiserne Zucht (…) wich einer allzubereiten Nachsicht.“ Besonders entschieden vertrat Ernst Kornemann diese Auffassung. Er schrieb 1939, die Gesinnung des Kaisers sei pazifistisch gewesen, er sei gänzlich unsoldatisch gewesen und daher sei seine Außenpolitik verfehlt gewesen: „Er lebte, außenpolitisch gesehen, völlig in den Wolken.“ Auch Anthony Birley (1966) äußerte die Ansicht, Antoninus habe militärischen Fragen zu wenig Interesse geschenkt. Hermann Bengtson (1973) meinte, „infolge der schwächlichen Außenpolitik“ seien „die Keime des künftigen Niedergangs“ gesät worden. Hildegard Temporini-Gräfin Vitzthum (1997) hingegen hielt diese Kritik für überzogen, denn sie finde in den Quellen keine Basis; die modernen kritischen Beurteiler hätten sich zu weit von den Quellen entfernt.




</doc>
<doc id="14724" url="https://de.wikipedia.org/wiki?curid=14724" title="GNUstep">
GNUstep

GNUstep [] ist eine freie, plattformübergreifende und objektorientierte Programmierschnittstelle, die die OpenStep-Spezifikation von NeXT (seit 1997 Apple) vollständig implementiert und zusätzlich viele der Erweiterungen von Cocoa (aus macOS) eingebaut hat. Sie ist in Objective-C geschrieben.

GNUstep läuft auf POSIX-Systemen (Unix und unixoiden Systeme wie Linux) sowie auf Windows mit MinGW und MSYS.

Wie Cocoa stellt GNUstep auch eine Java-Schnittstelle zur Verfügung sowie Brücken (Bindings) zu Ruby und Scheme.

Des Weiteren verfügt GNUstep über ein Scripting-Framework namens StepTalk.

Die Entwicklung von GNUstep begann, als Paul Kunz und andere von SLAC das Programm HippoDraw von NeXTStep auf andere Plattformen portieren wollten.

Anstatt HippoDraw komplett neu zu schreiben, entschied man sich, den Teil von NeXTSTEP nachzubilden, von dem die Applikation abhing. Diese erste Version nannte man "libobjcX". Damit waren sie in der Lage, HippoDraw auf alle möglichen Systeme zu portieren, auf denen X11 zur Verfügung stand, ohne eine einzige Zeile im Quelltext von HippoDraw zu ändern.

Nachdem im Jahr 1994 die Spezifikation von OpenStep veröffentlicht worden war, entschied man sich, ein neues "objcX" zu schreiben, welches sich an die API von OpenStep hielt. Dieses objcX wurde dann unter dem Namen „GNUstep“ bekannt.

GNUstep ist eine Implementierung von OpenStep und erbt daher die Design-Richtlinien, wie sie die Spezifikation von OpenStep vorschlägt und die Objective-C unterstützt.



Nützliche Bibliotheken außerhalb des GNUstep-Kernes sind zum Beispiel „WebServer“, um ein Programm mit grundlegenden Web-Server-Fähigkeiten auszustatten, zum Beispiel zur Fernwartung, und „sqlclient“ für den Zugriff auf Datenbanken.


Die Integrierte Entwicklungsumgebung ProjectCenter und der GUI-Designer Gorm – Nachkommen von Project Builder und Interface Builder aus NeXTStep – unterstützen eine schnelle Entwicklung.

Viele Anwendungen wurden geschrieben, um die Arbeit mit dem Computer einfacher zu gestalten. Darunter befinden sich zum Beispiel der Workspace Manager und GNUMail. GNUstep besitzt "keinen" eigenen Fenstermanager.

Die Desktop-Umgebung Étoilé mit ihrem Fenstermanager Azalea soll hier Abhilfe schaffen. Bis dessen Entwicklungsstadium seine Anwendung im Alltagsgebrauch zulässt (und natürlich auch später noch), kann man jeden beliebigen Fenstermanager benutzen, empfohlen wird jedoch Window Maker.

GNUstep ist kein Fenstermanager. Die Frameworks von GNUstep werden auch nicht von Window Maker benutzt; dieser verwendet stattdessen WINGs („WINGs is not GNUstep“) als Toolkit, das GNUstep optisch nachempfunden ist, aber im Gegensatz dazu in C geschrieben ist.

GNUstep-Base ist sowohl auf Unix-Plattformen als auch auf Windows stabil und verrichtet auch in kommerziellen Programmen eher im Hintergrund seinen Dienst.

GNUstep-GUI ist unter Linux und Co. schon gut benutzbar. Unter Windows gibt es noch einige Unzulänglichkeiten bei der Integration, aus mehreren Gründen:




Da GNUstep zum allergrößten Teil kompatibel zu Cocoa ist, lohnt es sich, sich auch mit den umfangreichen Ressourcen zu Cocoa zu befassen.


</doc>
<doc id="14725" url="https://de.wikipedia.org/wiki?curid=14725" title="Blindenschrift">
Blindenschrift

Als Blindenschrift werden übergreifend Systeme von Schriftzeichen bezeichnet, die von Blinden gelesen werden können. Das Lesen erfolgt dabei mit dem Tastsinn der Finger, mit denen die erhabenen Strukturen der Zeichen von ihrer planen Umgebung unterschieden und identifiziert werden können. Es existieren verschiedene solcher Systeme von Blindenschriften, von denen heute die im Jahr 1825 von Louis Braille entwickelte Brailleschrift am weitesten verbreitet ist.

Viele dieser Schriftsysteme für Blinde sind dabei gut durchdacht, haben aber oft den Nachteil, dass sie von Sehenden für Erblindende, wie z. B. das Moonalphabet, entworfen wurden. Nicht so bei der Brailleschrift, die von ihrem Namensgeber, der selbst blind war, für blinde Menschen als ein für sie praktikables Schriftsystem entwickelt wurde.

Zur Abgrenzung von Blindenschriften verwendet man häufig für die Schrift der Sehenden den Begriff Schwarzschrift. 

Es gibt zwei grundsätzliche Richtungen der Blindenschrift:

Punktschriften sind Schriften, die aus erhabenen, aus dem Material heraustretenden und damit fühlbaren Punkten bestehen. Die erste dieser aus ertastbaren "Punkten" und "Strichen" bestehende Schrift war von Francesco Lana Terzi bereits 1670 entworfen worden, sie wurde allerdings nie verwendet. Charles Barbier entwickelte 1815 seine militärischen Zwecken dienende, sogenannte Nachtschrift, die im Dunklen lesbar sein sollte. Da diese Schrift beim Militär nicht ankam, bot sie Barbier in der Pariser Blindenschule an.

Der Begriff Punktschrift wird heute synonym für die normale Brailleschrift, die sechs Punkte verwendet, benutzt. Diese ist zwar mit Abstand die am weitesten verbreitete und bekannteste Punktschrift, daneben gibt es aber noch andere Punktschriftsysteme. Bevor Brailleschrift sich allgemein durchsetzte, war in Nordamerika das von William Bell Wait entwickelte "New York Point" (auch kurz als "Wait" bezeichnet) verbreitet. Des Weiteren hat "Computer-Braille", das ein 8-Punkte-System verwendet, um leichter Großbuchstaben und Sonderzeichen darstellen zu können, oder die 7-Punkt- und 8-Punkt-Blindenstenographie Verbreitung erlangt. „Computer-Braille“ wird vor allem von Braillezeilen benutzt, mit denen Blinde den Inhalt eines Computer-Bildschirmes auslesen können.

Die Brailleschrift wird mit speziellen Brailledruckern oder Punktschriftmaschinen zu Papier gebracht. Der Platzbedarf der Punktschriftsysteme ist enorm, denn die Zeichen sind größer als in der Schwarzschrift. Auch das Papier muss viel dicker sein, um dauerhaft geprägt werden zu können. So umfasst der Duden in Punktschrift 18 Bände.
Es ist immerhin möglich, die Seiten bei maschineller Herstellung beidseitig zu prägen, denn die Finger spüren nur die Erhebungen, nicht aber die Vertiefungen. Dazu müssen die Zeichen von Vorder- und Rückseite um ein Geringes gegeneinander versetzt sein, um nicht bereits hervorgedrückte Punkte der einen Seite von der anderen Seite her wieder einzudrücken. Um die Nachteile durch das hohe Gewicht von Braille-Drucksachen im Briefverkehr auszugleichen, transportiert die Deutsche Post AG Sendungen in Brailleschrift portofrei (Kennzeichnung: "Blindensendung / Cécogramme").

Die Verbreitung der Punktschrift nimmt weiter zu. So müssen gemäß der 12. AMG-Novelle (Arzneimittelgesetz) seit 2004 in Deutschland alle Pharmaverpackungen auch mit Blindenschrift versehen sein.

Analog zur Schwarzschrift, die zum Beispiel permanent durch Handschrift oder Druck auf Papier oder temporär auf Displays und Bildschirmen dargestellt wird, kann auch Punktschrift temporär mit Braillezeilen oder dauerhaft in Papier/Karton geprägt dargestellt werden.

Die permanente Darstellung erfolgt in der Regel auf Karton bzw. etwas dickerem Papier. Weitere Möglichkeiten sind Metallplatten mit Punktschrifthinweisen in Fahrstühlen oder Kunststofffolien zur Etikettierung. Es gibt verschiedene Arten, Punktschrift (insbesondere Brailleschrift) permanent zu schreiben:





Werkzeuge:
Literaturquellen:


</doc>
<doc id="14726" url="https://de.wikipedia.org/wiki?curid=14726" title="Schwarzschrift">
Schwarzschrift

Der Begriff Schwarzschrift wird zur Bezeichnung einer Schrift für Sehende zur Unterscheidung von Blindenschrift verwendet.

Die Bezeichnung Schwarzschrift stammt daher, dass ursprünglich zum Schreiben und Drucken fast ausschließlich schwarze Tinte bzw. Druckerschwärze und als Träger mehr oder weniger weißes Papier verwendet wurden. Entsprechend sagt man auch heute, wenn etwas in Schriftform vorliegt: „Ich habe es schwarz auf weiß.“

Im Gegensatz dazu besteht Blindenschrift aus erhabenen Punkten oder Linien auf glattem Untergrund, die mit dem Tastsinn erfasst werden. Die am weitesten verbreitete Blindenschrift ist die Brailleschrift, die aus erhöhten Punkten auf glattem Untergrund besteht.

Im Gegensatz zur Punktschrift für Blinde wird die Schwarzschrift auch als "Normalschrift" bezeichnet.


</doc>
<doc id="14728" url="https://de.wikipedia.org/wiki?curid=14728" title="Hugo Ball">
Hugo Ball

Hugo Ball (* 22. Februar 1886 in Pirmasens; † 14. September 1927 in Sant’Abbondio-Gentilino, Schweiz) war ein deutscher Autor und Biograf. Außerdem war er einer der Mitgründer der Dada-Bewegung und ein Pionier des Lautgedichts.

Hugo Ball wuchs in einer gutbürgerlich-katholischen Familie auf. Sein Vater war Schuhfabrikant. Von 1895 bis 1901 besuchte er das zu diesem Zeitpunkt nur sechsklassige Königliche Progymnasium zu Pirmasens ohne Abiturrecht. Nach dem Abschluss fügte sich Ball widerstrebend dem Wunsch seiner Eltern, eine Lehre bei dem Lederhändler Ferdinand Schohl zu beginnen. Aus gesundheitlichen Gründen brach Ball die Lehre ab. Seine Eltern gaben jetzt seinem Wunsch nach, das Abitur in Zweibrücken am Herzog-Wolfgang-Gymnasium nachzuholen.

Nach dem Abitur begann er im Oktober 1906 das Studium der Germanistik, Geschichte und Philosophie in München und wurde ein „glühender Verehrer“ der Musik von Max Reger. 1907 wechselte er für zwei Semester nach Heidelberg, wo er eine Vorlesung über "Schopenhauer und Nietzsche" hörte und sein Dissertationsprojekt entwickelte. Ab 1908 war er in München immatrikuliert. 1909/1910 studierte er wieder zwei Semester in Heidelberg und arbeitete in Schnaitsee an seiner Dissertation "Nietzsche in Basel", die er jedoch nicht einreichte (und die erst 1978 veröffentlicht wurde). Er brach sein Studium im Frühjahr 1910 ab. Es entstand ein offener Konflikt mit seiner Familie.

Nach dem Abbruch des Studiums zog Ball nach Berlin. Dort machte er eine Ausbildung als Hilfskraft für „Regie, Dramaturgie und Verwaltungsfragen“ an Max Reinhardts Schauspielschule, wobei Paul Legband Balls Regielehrer war. Im selben Jahr gelang ihm beim Ernst Rowohlt Verlag die Veröffentlichung seiner Tragikomödie "Die Nase des Michelangelo". Am Theater Plauen arbeitete er 1911/12 als Dramaturg, zwischen 1912 und 1914 an den Münchner Kammerspielen.
Nach einer internen Krise des Theaters wurde Hugo Ball zum alleinigen Dramaturgen des Hauses. In München lernte er Hans Leybold, Leontine Sagan, später auch Richard Huelsenbeck und Emmy Hennings kennen. Durch die Uraufführung von Frank Wedekinds "Franziska" wurde Hugo Ball mit dem Autor bekannt. Ende 1912 nahm er die Arbeit an Der Henker von Brescia auf. Doch bereits 1913 kam es bei den Münchner Kammerspielen erneut zur Krise. Durch den Wechsel der Direktion verlor Ball an Einfluss auf den Spielplan.

Ball veröffentlichte 1913 in verschiedenen Zeitschriften ("Revolution", "Die Neue Kunst", "Die Aktion" und "Jugend"; ab 1914 auch in der Theaterzeitschrift "Phöbus"). Während "Die Aktion" 1914 häufiger Gedichte Balls veröffentlichte, scheiterte ein Projekt mit Wassily Kandinsky. Es war ein Almanach als Ergänzung zum Blauen Reiter geplant, jedoch beendete der Beginn des Ersten Weltkrieges das Projekt. Ball meldete sich freiwillig zum Kriegsdienst, aber wurde für untauglich erklärt. Mit der Absicht einen verwundeten Freund in Lunéville zu besuchen, bekam er dennoch einen Eindruck von der Kriegsfront. Seine Erlebnisse veröffentlichte er in der Pirmasenser Zeitung. Danach ging er zurück nach Berlin und schrieb weiter für Zeitschriften. Durch seine Fronteindrücke wurde sein Interesse für den Anarchismus geweckt. Er las Schriften von Kropotkin und Bakunin.

Im Mai 1915 emigrierte Ball gemeinsam mit Emmy Hennings in die Schweiz, wo er zunächst in Zürich wohnte. Er tingelte mit einem Varieté-Ensemble als Klavierspieler und Texter durch das Land. Schließlich kam er in Kontakt mit der Tanzschule von Rudolf von Laban, die als Treffpunkt der Dadaismusbewegung galt. Im Februar 1916 gründete er mit Hans Arp, Tristan Tzara und Marcel Janco in Zürich das Cabaret Voltaire, wo er im Juni zum ersten Mal eines seiner Lautgedichte (Gadji beri bimba) vortrug und welches als „Wiege des Dadaismus“ bezeichnet wird. In Zürich lernte er auch den Schriftsteller Friedrich Glauser kennen, der an zwei Dada-Soireen mitwirkte. Im Sommer 1917 verbrachte Glauser mit Ball und Emmy Hennings zwei Monate in .

Hugo Ball zog sich schon bald wieder aus dem Kreis der aktiven Dadaisten zurück und arbeitete von 1917 bis 1919 als Mitarbeiter, schließlich als Verlagsleiter der " Freien Zeitung", für die er politische Tageskommentare und kritische Beiträge verfasste, auch unter dem Einfluss von Bakunin. Nach dem Konkurs des Verlags verlor er das Interesse an der politischen Aktion. Er befreundete sich mit dem als Schriftsteller tätigen Studentenpfarrer Paul de Mathies an. Im Sommer 1920 kam es zu Balls erneuter Hinwendung (Reversion) zum Katholizismus. Hier schloss er sich dezidiert strenggläubigen Kreisen an und studierte u. a. die alten Mystiker. Vortragsreisen führten ihn durch Deutschland und die Schweiz.
Nach seiner Heirat am 21. Februar 1920 mit Emmy Hennings wohnte Ball, unterbrochen von einem Italien-Aufenthalt in Rom und bei Salerno von Herbst 1924 bis Frühjahr 1926, in dem kleinen Dorf Agnuzzo unterhalb von Montagnola im Kanton Tessin und ab 1926 in der "Casa Schori" in Sorengo. Ab 1924 beschäftigte er sich vermehrt mit katholischen Themen und schrieb für die katholische Zeitschrift "Hochland" über katholische Theologie. Seit seinem Umzug ins Tessin verband ihn eine enge Freundschaft mit Hermann Hesse, dessen Biografie er von Anfang Oktober 1926 bis Anfang März 1927 schrieb und die im Juni 1927 bei S. Fischer erschien.

Ball starb am 14. September 1927 an einem Magenkarzinom und wurde auf dem Friedhof Sant’Abbondio in Gentilino beigesetzt, auf dem auch seine Frau Emmy – sie starb 1948 – und Hermann Hesse begraben sind.
1957 wurde die von Harald Szeemann kuratierte Ausstellung "Dichtende Maler – Malende Dichter" im Kunstmuseum St. Gallen Hugo Ball gewidmet. 1976 rief seine Heimatstadt Pirmasens – als Forum der Hugo-Ball-Forschung – den "Hugo-Ball-Almanach" ins Leben. 1988 widmete ihm das Kunstmuseum Zürich eine Ausstellung mit Katalog. 1990 lobte die Stadt Pirmasens den Hugo-Ball-Preis aus, mit dem im Drei-Jahres-Rhythmus Persönlichkeiten geehrt werden, die geisteswissenschaftlich oder künstlerisch im Sinne Hugo Balls arbeiten. Für das Jahr 2014 erhielt der schweizerische Schriftsteller Thomas Hürlimann die mit 10.000 Euro dotierte Auszeichnung. Außerdem wurde in Balls Heimatstadt das Neusprachliche Gymnasium in "Hugo-Ball-Gymnasium" umbenannt. Im Mainzer Walk of Fame des Kabaretts ist Ball ein Stern gewidmet.
Die Schweizer Post würdigte Hugo Ball im Jahr 2016 mit einer Briefmarke, die das oben gezeigte Foto seines Auftritts im Cabaret Voltaire von 1916 reproduziert. Seit Ende 2016 würdigt das "Hugo-Ball-Kabinett" als Dauerausstellung sein Leben und Werk im Forum Alte Post in Pirmasens.


Postum erschienen:







</doc>
<doc id="14729" url="https://de.wikipedia.org/wiki?curid=14729" title="Duvergers Gesetz">
Duvergers Gesetz

Duvergers Gesetz ("Loi de Duverger"), benannt nach dem französischen Juristen und Politikwissenschaftler Maurice Duverger, ist die vermutete Gesetzmäßigkeit, nach der ein relatives Mehrheitswahlsystem zur Bildung eines Zweiparteiensystems führt. Im Umkehrschluss gelte, dass ein Verhältniswahlsystem zu einer Vielzahl von Parteien führt.

Die Idee hinter diesen Vermutungen ist, dass in einem relativen Mehrheitswahlsystem in Einerwahlkreisen nur wenige Kandidaten eine realistische Chance haben, gewählt zu werden. Ein rational denkender Wähler gibt seine Stimme dann nicht dem Kandidaten der Partei, die er persönlich bevorzugt, sondern einem anderen Kandidaten, der größere Erfolgsaussichten hat. So konzentrieren sich die Stimmen auf zwei Parteien, eine eher rechte und eine eher linke. Dieser Effekt wurde von Duverger in mehreren Aufsätzen in den 1950er- und 1960er-Jahren formuliert und daraufhin allgemein als "Duvergers Gesetz" in die Fachliteratur übernommen.

Das Gesetz ist inzwischen jedoch vielfach kritisiert worden. Der Effekt komme nur zustande, wenn es wenige Konfliktlinien in der Gesellschaft gibt, typischerweise die zwischen Arm und Reich (Arbeit und Kapital). Das nütze einer sozialdemokratischen Partei einerseits und einer konservativ-liberalen andererseits. In den meisten Gesellschaften kommen aber noch weitere Konfliktlinien hinzu, zum Beispiel konfessioneller, ideologischer oder kultureller Art. Wichtig ist auch, ob Minderheiten in einzelnen Regionen konzentriert wohnen (und folglich Wahlkreise erobern können) oder verstreut über das Land.

Als das klassische Beispiel von Duvergers Gesetz galt Großbritannien, 1974 und 2010 gelang es jedoch der Liberal Party bzw. den Liberal Democrats als dritte Partei eine mehrheitsentscheidende Anzahl von Parlamentsplätzen zu erreichen. Die Regierungskoalition zwischen der Conservative Party und den Liberal Democrats in den Jahren 2010 bis 2015 kann als Gegenbeispiel für die unterstellte Gesetzmäßigkeit dienen. Ebenso wählt Indien auch nach Einerwahlkreisen, dort kommt eine Vielzahl von regionalen Kleinparteien ins Parlament. In den USA dagegen bestätigt sich das Gesetz, unter anderem unterstützt durch intensives Gerrymandering.


</doc>
<doc id="14730" url="https://de.wikipedia.org/wiki?curid=14730" title="Verleger">
Verleger

Der Verleger ist Leiter eines Verlages für Bücher, Zeitschriften, Zeitungen, Musiknoten, Kalender oder andere Medienprodukte (CDs, DVDs, Online-Medien, PC-Software, Spiele etc.). Er ist (häufig) als Eigentümer oder Geschäftsführer für den Verlag in seiner Gesamtheit verantwortlich. Beim Verleger liegt letztlich die Entscheidung, in welche Veröffentlichungen ein Verlag investiert (verlegen = vorlegen), nicht nur für die Herstellung und Vervielfältigung des Mediums, sondern auch für dessen Verbreitung bzw. Vermarktung, wozu er sich im Verlagsvertrag mit einem Autor verpflichtet und wofür er zu sorgen hat. Der Verleger trägt die finanzielle Verantwortung und bestimmt die Richtlinien für Lektorat, Herstellung und Vertrieb. 

Nach Fritz J. Raddatz, von 1960 bis 1969 Cheflektor und stellvertretender Chef des Rowohlt Verlages muss „"der wahre Verleger ... Vater und Mutter sein, Amme und Zuchtmeister, Gläubiger und Fordernder, Duellant und Sekundant, Beichtvater und Ministrant, Heiliger und Hurenbold. Er muss das Gehirn eines Philosophen, den Blick eines Radiologen, die Sanftmut einer Krankenschwester haben. Eines darf er auf gar keinen Fall: ein eigenes Leben haben".“

Früher bezeichnete man mit Verleger den Auftraggeber im Verlagssystem.




Seit 1994 zeichnet das Branchenmagazin BuchMarkt eine/n „Verleger/in des Jahres“ aus.





</doc>
<doc id="14731" url="https://de.wikipedia.org/wiki?curid=14731" title="Göttinger Sieben">
Göttinger Sieben

Die Göttinger Sieben waren eine Gruppe von Göttinger Professoren, die 1837 gegen die Aufhebung der 1833 eingeführten liberalen Verfassung im Königreich Hannover protestierten. Die sieben Professoren wurden deshalb entlassen; drei von ihnen wurden darüber hinaus des Landes verwiesen.

Die Namen dieser sieben Professoren der Georg-August-Universität waren:

Nachdem die 123-jährige Personalunion zwischen Großbritannien und Hannover geendet war, bestieg Ernst August I. 1837 den Thron im Königreich Hannover. Unmittelbar nach seinem Regierungsantritt hob er die relativ freiheitliche Verfassung, das vier Jahre zuvor in Kraft getretene Staatsgrundgesetz, zum 1. November 1837 wieder auf. Am 18. November des Jahres reichten die Göttinger Sieben schriftlich Protest ein.

Ende November 1837 übergaben der Prorektor und die vier Dekane, ohne ein Mandat von der Universität zu haben, dem König im Jagdschloss Rothenkirchen eine Adresse, mit der sich die Universität „von aller Gemeinschaft mit den Sieben lossagt“ und deren Gesinnung schmäht.

Am 12. Dezember 1837 entließ Ernst August I. die Professoren und verwies drei von ihnen – Friedrich Dahlmann, Jacob Grimm und Georg Gottfried Gervinus – sogar des Landes (vgl. Verbannung). Diese wurden dann 1840 vom preußischen König Friedrich Wilhelm IV. empfangen, der politisch Verfolgte teilweise rehabilitierte. Gleichzeitig zeigte sich der große Solidarisierungseffekt in der Bevölkerung, die den drei Ausgewiesenen ihr Gehalt aus Spendengeldern zahlte. Spätestens hier wurde erkennbar, dass der Liberalismus als Massenbewegung nun nicht mehr durch Beschlüsse und Verordnungen unterdrückt werden konnte.

Die Protestation, der Protestbrief, fand in ganz Deutschland Verbreitung und förderte eine liberale Gesinnung. Jacob Grimm begründete später seine Entscheidung zu dem Protest in einer Rechtfertigungsschrift:

In der Frankfurter Nationalversammlung 1848 hatte Jacob Grimm einen Ehrenplatz inne, Albrecht, Dahlmann und Gervinus waren Mitverfasser der gesetzgebenden Initiativen. Der Ruf der Göttinger Universität litt noch lange Zeit an der Entlassung dieser als hervorragend geltenden Lehrer.

1987 wurde in der Aula der Georg-August-Universität Göttingen eine Gedenktafel für die „Göttinger Sieben“ angebracht.

1988 enthüllte der Niedersächsische Landtag in Hannover in der Wandelhalle des Leineschlosses eine Gedenktafel für die sieben Professoren.
1998 wurde auf einem Vorplatz zum Niedersächsischen Landtag () am "Platz der Göttinger Sieben" das bronzene Denkmal der Göttinger Sieben aufgestellt, das von dem italienischen Künstler Floriano Bodini (1933–2005) geschaffen wurde.

In Göttingen trägt der zentrale Campus der Georg-August-Universität Göttingen ebenfalls den Namen "Platz der Göttinger Sieben". Seit 2011 befindet sich hier eine von Günter Grass gestaltete und von ihm und seinem Verleger Gerhard Steidl gestiftete Skulptur zum Gedenken an die Göttinger Sieben.

Am 2. November 2012 erschien eine deutsche 55-Cent-Sonderbriefmarke zum 175. Jahrestag der Entlassung der sieben Männer.

Am 19. November 2015 wurde auf dem Vorplatz des Bahnhofs Göttingen ein zuvor stark kontrovers diskutiertes Denkmal zur Erinnerung an die Göttinger Sieben enthüllt. Es besteht aus einer Nachbildung des Sockels des Ernst-August-Denkmals am Hauptbahnhof Hannover. Die Reiterfigur des Königs fehlt. „Dem Landesvater seine Göttinger Sieben“ steht auf der einen Seite. Der Name der Künstlerin Christiane Möbus steht unter den Namen der sieben Professoren auf der anderen Seite des Sockels.




</doc>
<doc id="14732" url="https://de.wikipedia.org/wiki?curid=14732" title="Roland Koch">
Roland Koch

Roland Koch (* 24. März 1958 in Frankfurt am Main) ist ein deutscher Manager, Rechtsanwalt und ehemaliger Politiker (CDU). Er war von April 1999 bis August 2010 Ministerpräsident des Landes Hessen. Von 1998 bis 2010 war er zudem Landesvorsitzender der CDU Hessen. Vom 1. März 2011 an war er Vorstandsmitglied und vom 1. Juli 2011 bis zum 8. August 2014 Vorstandsvorsitzender des deutschen Baukonzerns Bilfinger. Seit November 2017 ist er 'Professor of Management Practice in Regulated Environments' an der Frankfurt School of Finance & Management.

Kochs Vater Karl-Heinz Koch (1924–2007) war CDU-Politiker und von 1970 bis 1987 Landtagsabgeordneter im Hessischen Landtag; 1987 bis 1991 war er hessischer Justizminister (Kabinett Wallmann).

Nach dem Abitur 1977 am Eichwaldgymnasium in Sulzbach (Taunus) leistete Koch zunächst von 1977 bis 1978 seinen Wehrdienst und studierte dann an der Johann Wolfgang Goethe-Universität Frankfurt am Main Rechtswissenschaft. 1982 machte er das erste und 1985 das zweite juristische Staatsexamen. Seit 1985 ist er als Rechtsanwalt zugelassen. Spezialisiert auf Wirtschaftsrecht und Wettbewerbsrecht übte er diesen Beruf bis 1999 in Eschborn aus.

Koch trat mit 14 Jahren der CDU-Jugendorganisation Junge Union bei. 1979 wurde Koch im Kreisverband Main-Taunus jüngster Vorsitzender eines CDU-Kreisverbandes. Er zählte zu Beginn seiner politischen Karriere zu den „jungen Wilden“ seiner Partei. Seit 1979 war er Mitglied des sogenannten „Andenpaktes“, einer nichtoffiziellen Seilschaft damals junger CDU-Politiker. Von 1983 bis 1987 war er stellvertretender Bundesvorsitzender der Jungen Union. Von 1998 bis Juni 2010 war er Landesvorsitzender der CDU Hessen. Koch war bis zum 15. November 2010 außerdem einer der fünf stellvertretenden Bundesvorsitzenden der CDU und damit Mitglied im CDU-Bundesvorstand.

Bis 1993 war Koch Mitglied in der Stadtverordnetenversammlung seiner Heimatstadt Eschborn und gehörte außerdem dem Kreistag des Main-Taunus-Kreises an, wo er von 1989 bis 1997 Vorsitzender der CDU-Fraktion war.

Von 1987 bis 2010 war Koch als Abgeordneter des Wahlkreises Main-Taunus I Mitglied des Hessischen Landtages. 1991 wurde er hier Stellvertretender Vorsitzender der CDU-Landtagsfraktion; 1993 wurde er ihr Vorsitzender.

Nachdem die rot-grüne Regierung (Kabinett Eichel II) unter Hans Eichel bei der Landtagswahl im Februar 1999 ihre Mehrheit verloren hatte, bildete die CDU unter Kochs Führung eine Koalition mit der FDP (Kabinett Koch I). Koch wurde am 7. April 1999 als Nachfolger Eichels zum Ministerpräsidenten des Landes Hessen gewählt. Bei der Landtagswahl 2003 erreichte die CDU mit 48,8 % eine absolute Mehrheit der Sitze im Landtag. Koch wurde als Ministerpräsident im Amt bestätigt. Bei der Landtagswahl 2008 verlor die CDU die absolute Mehrheit; auch CDU und FDP zusammen hatten keine absolute Mehrheit der Landtagssitze. Da in der konstituierenden Sitzung des neuen Landtages am 5. April 2008 und auch in den weiteren Sitzungen kein Ministerpräsident gewählt wurde, führte Koch die Regierungsgeschäfte geschäftsführend weiter. Nach der vorgezogenen Landtagswahl 2009, bei der die CDU 37,2 % der Stimmen erhielt, wurde Koch mit den Stimmen von Union und FDP erneut zum Ministerpräsidenten gewählt.

Im Wahlkampf um die Landtagswahl 1999 führte die CDU die Unterschriftenaktion gegen die Reform des deutschen Staatsbürgerschaftsrechts gegen die damalige rot-grüne Bundesregierung durch. Kritiker warfen Koch deswegen vor, Ausländerfeindlichkeit zu schüren und für den Wahlkampf zu instrumentalisieren.

Im Zusammenhang mit der Spendenaffäre der Bundes-CDU wurde auch eine Affäre der hessischen CDU bekannt. Unter anderem hatten der ehemalige Innenminister Manfred Kanther und der frühere CDU-Landesschatzmeister Casimir Prinz zu Sayn-Wittgenstein mehrere illegale Parteispenden als angebliches Vermächtnis von verstorbenen Juden verbucht.

Roland Koch erklärte, diese Vorgänge seien ihm nicht bekannt gewesen, und er versprach die „brutalstmögliche Aufklärung“. Auf einer Pressekonferenz am 10. Januar 2000 verschwieg er trotz mehrfacher Nachfrage die Rückdatierung eines Kreditvertrags über 2 Mio. D-Mark, der Geldflüsse in der Parteibuchhaltung rechtfertigen sollte.

Unterstützt durch die CDU und die FDP Hessen verblieb Koch trotz diverser Rücktrittsforderungen im Amt. Der damalige Chef der Staatskanzlei, Franz Josef Jung, trat hingegen zurück, was von Beobachtern als Bauernopfer interpretiert wurde. Die Opposition im hessischen Landtag kritisierte auch insbesondere, dass Kochs Wahlkampf 1998/1999 teilweise durch die schwarzen Kassen finanziert worden war, und klagte erfolglos für eine Annullierung der Wahl.

Bei der Landtagswahl im Februar 2003 gewann die CDU mit Roland Koch erneut, diesmal mit einer absoluten Mehrheit. Ende 2003 entwarfen Koch und der damalige nordrhein-westfälische Ministerpräsident Peer Steinbrück (SPD) einen Plan zum Abbau von Subventionen, das sogenannte Koch-Steinbrück-Papier.

2006 wurde Koch vorgeworfen, die Nichtteilnahme der Freien Wähler Hessen an der Landtagswahl mit finanziellen Zusagen „kaufen“ zu wollen. Dies war auch Gegenstand eines Untersuchungsausschusses.

Im Dezember 2007 verprügelten zwei ausländische Jugendliche einen Rentner in einer Münchener U-Bahn-Station, nachdem dieser die beiden aufgefordert hatte, ihre Zigaretten auszumachen. Der Rentner erlitt einen mehrfachen Schädelbruch. Roland Koch forderte daraufhin eine Verschärfung des Jugendstrafrechts, sprach sich indirekt für eine Abschiebung krimineller Ausländer aus und thematisierte beides auch in seinem Wahlkampf. „Wer sich als Ausländer nicht an unsere Regeln hält, ist hier fehl am Platz“ sagte er der Bild-Zeitung. Der Generalsekretär des Zentralrats der Juden in Deutschland, Stephan Kramer, warf Koch NPD-Nähe vor. CDU-Generalsekretär Ronald Pofalla wies die Kritik zurück. Der Vorwurf, Kochs Wahlkampf unterscheide sich kaum noch von dem der NPD, sei „an Absurdität gar nicht mehr zu überbieten“. Unterstützung für seinen Vorstoß erhielt Koch vom Weißen Ring, der Migrantenverbänden Verharmlosung vorhielt.

Koch polarisierte im Wahlkampf ebenfalls mit einer Plakataktion, auf der er mit dem Slogan „Ypsilanti, Al-Wazir und die Kommunisten stoppen!“ warb. Ihm wurde daraufhin vorgeworfen, die ausländisch klingenden Namen absichtsvoll gewählt zu haben, um auch die Stimmen von Wählern mit ausländerfeindlichen Ressentiments zu mobilisieren. Der damit ebenfalls angeschlagene persönlich verletzende Tonfall gegenüber Tarek Al-Wazir (dem Spitzenkandidaten der Grünen) und Andrea Ypsilanti (der Spitzenkandidatin der SPD) gilt als einer der Gründe, warum Koalitionsverhandlungen mit den Grünen und der SPD nach der Wahl erfolglos verliefen. Während der Koalitionsverhandlungen machten SPD und Grüne zur Bedingung, dass Roland Koch "nicht" Ministerpräsident werde; dies lehnte die CDU ab.

Später wurde Koch auch innerparteilich kritisiert, etwa indirekt in einem offenen Brief von 17 prominenten Unionspolitikern in der Zeit, die darin schrieben: „Integrationspolitik ist so fundamental für die Zukunft unseres Landes, dass sie nicht zum Wahlkampfthema degradiert werden darf.“ Eine Studie der CDU-nahen Konrad-Adenauer-Stiftung kritisierte Kochs Wahlkampf; die Debatte um Jugendkriminalität habe ihm geschadet.

Bei den Wahlen zum 17. Hessischen Landtag am 27. Januar 2008 verlor die Hessen-CDU unter seinem Vorsitz 12 Prozentpunkte, wurde aber mit 36,8 % der Wählerstimmen knapp vor der SPD (36,7 %) stärkste Kraft. SPD und CDU erhielten gleich viele Abgeordnetensitze im hessischen Landtag und beide Parteien hatten mit ihren traditionellen Koalitionspartnern keine absolute Mehrheit. Da keine Regierungsbildung erfolgte, blieb Koch laut Hessischer Verfassung als geschäftsführender Ministerpräsident im Amt.

Nachdem der Versuch einer Regierungsbildung von SPD und Grünen unter Tolerierung der Partei Die Linke unter anderem am Widerstand von vier SPD-Abgeordneten gescheitert war, erfolgte die Auflösung des Landtages und eine Neuwahl im Januar 2009. Roland Koch wurde von der CDU erneut als Spitzenkandidat nominiert. Ein Fernsehduell mit dem Spitzenkandidaten der hessischen SPD Thorsten Schäfer-Gümbel lehnte Koch ab. Die CDU gewann gegenüber der Wahl 2008 leicht hinzu, wohingegen die SPD 13 Prozentpunkte verlor. Die CDU stellte nun mit deutlichem Abstand zur SPD die stärkste Fraktion im Landtag und bildete zusammen mit der FDP die Landesregierung. Koch wurde am 5. Februar 2009 als Ministerpräsident wiedergewählt. Dabei stimmten vier Koalitionsabgeordnete nicht für ihn: eine Nein-Stimme, eine ungültige Stimme und zwei leer abgegebene Stimmzettel.

Am 25. Mai 2010 gab Koch auf einer Pressekonferenz bekannt, dass er am 31. August 2010 von seinem Amt als Ministerpräsident zurücktreten, sein Landtagsmandat niederlegen und alle parteipolitischen Funktionen aufzugeben gedenke. Beim Parteitag der hessischen CDU am 12. Juni 2010 trat er nicht mehr als deren Vorsitzender an und im November 2010 kandidierte er auch nicht mehr als einer der vier stellvertretenden Vorsitzenden seiner Partei auf Bundesebene. Als Grund für seinen Rückzug aus der Politik deutete Koch an, er wolle künftig in der Wirtschaft tätig werden. Beim Parteitag der hessischen CDU am 12. Juni 2010 wurde er mit 96 % der Stimmen zum Ehrenvorsitzenden der CDU Hessen gewählt.

Roland Koch wurde am 30. August 2010 in einer Feier mit militärischer Serenade verabschiedet und trat am Folgetag offiziell vom Amt des Ministerpräsidenten zurück. Als sein Nachfolger wurde der bisherige hessische Innenminister Volker Bouffier (CDU) gewählt. Mit Ablauf des 1. September 2010 legte Koch auch sein Landtagsmandat nieder. Nachrücker im Landtag wurde Christian Heinz.

Am 29. Oktober 2010 entschied der Aufsichtsrat des Baukonzerns Bilfinger Berger, Roland Koch ab 1. März 2011 als Vorstandsmitglied und zum 1. Juli 2011 als Vorstandsvorsitzenden des Unternehmens zu berufen. Kritiker wandten ein, es bestehe möglicherweise ein Zusammenhang mit der Auftragsvergabe für den Bau der neuen Landebahn Nordwest und anderer Bauprojekte des Frankfurter Flughafens, dessen Anteilseigner unter anderem das Land Hessen ist. Im Jahr 2012 hat Roland Koch 1,5 Millionen Euro bei Bilfinger verdient.

Am 9. November 2010 wurde mitgeteilt, dass Koch zum 1. Januar 2011 auch Aufsichtsratsvorsitzender der deutschen Tochter der Großbank UBS werden soll. Diese Funktion übt er bis heute (Stand November 2016) aus.

Am 4. August 2014 teilte Bilfinger mit, dass Koch am 8. August 2014 aus dem Unternehmen ausscheidet. Noch am selben Tag verkündete Koch seinen Rücktritt; er erhielt bis zum Ende seines Vertrags im Februar 2016 sein Gehalt von 2,35 Millionen Euro pro Jahr weiter. Bereits im Juli hatte der bisherige Bilfinger-Aufsichtsrat Holger Timmer Koch Managementfehler vorgeworfen. Koch selbst nannte das Unternehmen, trotz eines Gewinneinbruchs, bei seinem Rücktritt „unverändert erfolgreich“. Der Hauptgeschäftsführer der Deutschen Schutzvereinigung für Wertpapierbesitz (DSW), Marc Tüngler, bezeichnete dagegen den ehemaligen Vorstandsvorsitzenden auf der Hauptversammlung 2015 als den Hauptverantwortlichen der Unternehmenskrise.

Am 20. Februar 2018 beschloss der Aufsichtsrat von Bilfinger SE Schadenersatz von allen zwischen 2006 und 2015 amtierenden Vorstandsmitgliedern zu verlangen.

Seit März 2015 ist Koch Mitglied des Aufsichtsrats von Vodafone.

Seit dem 1. Januar 2003 ist Koch ehrenamtlich Aufsichtsratsvorsitzender der Hessischen Staatsweingüter Kloster Eberbach.

Seit November 2017 ist Koch Professor of Management Practice in Regulated Environments an der Frankfurt School of Finance & Management.

Gemeinsam mit seiner Frau Anke hat Roland Koch die Schirmherrschaft für den Tuberöse Sklerose Deutschland e.V übernommen und ist zudem Stiftungsvorstand der Deutsche Tuberöse Sklerose Stiftung.

Roland Koch ist römisch-katholisch, verheiratet und hat zwei Söhne.

Helmut Kohl galt als Kochs politischer Ziehvater, Unterstützer und Ratgeber. Koch arbeitete bundespolitisch an den Verhandlungen zur Neustrukturierung der Verhältnisse zwischen Bund und Ländern mit und gilt als Finanz- und Wirtschaftsexperte sowie Vertreter des konservativ-christlichen Flügels der CDU. Ihm wurde ein ambivalentes Verhältnis zu Angela Merkel nachgesagt. Einerseits wurde er als Angela Merkels „bester Mann“ bezeichnet, andererseits wurde ihm ein gespanntes Verhältnis zur Bundeskanzlerin nachgesagt, das er aufgrund seiner eigenen politischen Ambitionen als Rivale der Kanzlerin habe. Er polarisiere und positioniere sich daher vielfach als „konservativer Hardliner“. Sein Biograph Hajo Schumacher schrieb 2004, er sei ein „Risikopolitiker“ und „kein Moralpolitiker, sondern ein politischer Ökonom, ein Machtmathematiker“. Im November 2013 zitierte eine Zeitung Koch mit der Aussage, die „völlig unsinnige Dauer-Subventionierung“ von Photovoltaikanlagen auf deutschen Dächern lasse sich nur stoppen, wenn „man Steine drauf wirft“.

Unter Roland Koch brachte die hessische Landesregierung im Dezember 2003 gegen großen Protest ein Studienguthabengesetz in den Landtag ein. Zum Wintersemester 2007/08 führte die Hessische Landesregierung unter Roland Koch allgemeine Studiengebühren von 500 bis 1500 Euro pro Semester ein. Die Gebührenerhebung war umstritten und nach der Auffassung vieler Rechtsexperten nicht mit der Landesverfassung vereinbar. Über 78.000 Bürger in Hessen unterzeichneten hierzu eine Volksklage. Die Oppositionsparteien im Landtag reichten ebenfalls eine Klage ein. Am 11. Juni 2008 erklärte der hessische Staatsgerichtshof mit sechs zu fünf Stimmen das Gebührensystem als mit der Landesverfassung vereinbar.
Nach der Landtagswahl in Hessen am 27. Januar 2008 wurde das Gesetz am 17. Juni 2008 vom Landtag wieder abgeschafft.
Im Jahr 2005 wurde die Einführung des Abiturs nach zwölf Schuljahren beschlossen, 2007 die Einführung des Zentralabiturs. Die Anzahl der freiwilligen und verpflichtenden Ganztagsschulen sowie die Betreuungsangebote für Grundschüler wurden erhöht.

Koch unterstützte 2007 das in der Öffentlichkeit stark kritisierte Vorhaben von Karin Wolff, die biblische Schöpfungslehre künftig im Biologieunterricht zu behandeln. Zahlreiche Medien sahen in diesem Vorschlag einen Versuch der Etablierung des Kreationismus in hessischen Schulen, nachdem Wolff zuvor erklärt hatte, dass sie keinen Widerspruch zwischen der biologischen Evolutionstheorie und der Erklärung in der Bibel sehe.

Mitte 2004 beschloss die hessische Landesregierung, die mittelhessischen Universitätskliniken in Gießen und Marburg zu fusionieren und die neu entstandene Universitätsklinikum Gießen und Marburg GmbH an einen privaten Betreiber (Rhön-Klinikum) zu verkaufen. Im Jahr 2005 verabschiedete der hessische Landtag ein Autonomiegesetz (TUD-Gesetz), welches die TU Darmstadt als erste Universität in die Autonomie entließ. Seither kann die TUD u. a. den Haushalt und die Liegenschaften selbst verwalten und selbständig mit Professoren über ihr Gehalt und ihre Ausstattung verhandeln und sie ernennen.

Ende 2005 wurde die deutschlandweit erste teilprivatisierte Justizvollzugsanstalt (JVA Hünfeld, 502 Haftplätze) in Betrieb genommen.

Koch sprach sich 2010 zusammen mit Horst Seehofer für eine Änderung des Länderfinanzausgleichs aus. Das Land Hessen zahle jährlich ca. 1,9 Mrd. Euro. Koch schloss einen Gang vor das Bundesverfassungsgericht nicht aus. Er verwies darauf, dass Hessen einen weit überproportionalen Teil des Länderfinanzausgleichs in Deutschland leiste.

Roland Koch war von 1999 bis 2003 Aufsichtsratsvorsitzender der Fraport AG, an der das Land Hessen seinerzeit einen Anteil von etwa 45 Prozent hatte. Neben seinem Einsatz für den Ausbau des Frankfurter Flughafens geriet dabei auch seine Doppelfunktion als Ministerpräsident und Aufsichtsratsvorsitzender in die Kritik. Im November 2003 soll sich Koch dafür eingesetzt haben, dass die Gehälter zweier Vorstandsmitglieder um fast 50 Prozent angehoben werden. Vor dem Hintergrund, dass im selben Jahr das Weihnachtsgeld für die Betriebsrentner der Fraport ersatzlos gestrichen wurde, stieß auch dieses Vorgehen auf Kritik.

Koch setzte sich für einen Ausbau des Frankfurter Flughafens ein. Das von ihm angestrebte Nachtflugverbot war jedoch nach Meinung der Landesregierung rechtlich nicht durchsetzbar, so dass am 18. Dezember 2007 durch das Wirtschaftsministerium eine Ausbaugenehmigung erteilt wurde, die zwar eine massive Reduzierung der Nachtflüge, aber kein Verbot vorsah. Koch wurde daraufhin von Seiten der Opposition und der Ausbaugegner Wortbruch vorgeworfen.

2006 sprach sich Koch dafür aus, eine Option für den Bau neuer Kernkraftwerke offen zu halten. Er verwies dabei auf das Nachbarland Frankreich, in welchem Kernkraftwerke der „nächsten Generation“ gebaut würden. 2013 bezeichnete er den Ausstieg aus der Kernkraft als alternativlos.

Am 12. Dezember 2002 warf Koch ver.di-Chef Frank Bsirske vor, in der Vermögenssteuerdebatte Namen reicher Deutscher genannt zu haben. In Anspielung auf den in der Zeit des Nationalsozialismus eingeführten Judenstern äußerte er, dies sei „eine neue Form von Stern an der Brust“ und „eine schlimme Parallele zu anderen Zeiten“. Koch hat sich später für diesen Vergleich entschuldigt.

Das seit 2004 geltende Kopftuchverbot wurde von der hessischen Landesregierung beschlossen und vom hessischen Staatsgerichtshof mit sechs zu fünf Stimmen am 10. Dezember 2007 als verfassungskonform bestätigt. Hessen verbietet damit Beamten das Tragen von Kleidungsstücken, die den „politischen Frieden gefährden“ können.

Im Jahr 2009 wurde bekannt, dass mehrere CDU-Politiker unter Führung von Koch darauf hinwirkten, den Vertrag von ZDF-Chefredakteur Nikolaus Brender nicht zu verlängern. Kochs Vorgehen im Rahmen seines Amtes als stellvertretender Vorsitzender im ZDF-Verwaltungsrat wurde unter anderem durch einen offenen Brief von 35 Verfassungsrechtlern gerügt.






</doc>
<doc id="14733" url="https://de.wikipedia.org/wiki?curid=14733" title="Jacob Grimm">
Jacob Grimm

Jacob Ludwig Karl Grimm (auch: "Carl"; * 4. Januar 1785 in Hanau; † 20. September 1863 in Berlin) war ein deutscher Sprach- und Literaturwissenschaftler sowie Jurist und gilt als Begründer der deutschen Philologie und Altertumswissenschaft.

Sein Lebenslauf und Werk ist eng mit dem seines ein Jahr jüngeren Bruders Wilhelm Grimm verbunden, worauf die oft gebrauchte Bezeichnung Brüder Grimm hinweist.

Jacob Grimm verbrachte seine Jugend in Steinau an der Straße, wohin sein Vater Philipp Wilhelm Grimm 1791 als Amtmann versetzt worden war, und besuchte ab 1798 mit seinem Bruder Wilhelm das Friedrichsgymnasium in Kassel.

1802 schrieb er sich an der Universität Marburg ein, wo er bei Friedrich Carl von Savigny Jura studierte. Durch dessen rechtshistorische Forschungen sowie durch Ludwig Wachlers Vorlesungen wurde seine Aufmerksamkeit auf die geschichtliche Entwicklung der deutschen Sprache und Literatur gerichtet. Als Savigny 1804 wegen wissenschaftlicher Forschungen (betreffend das römische Recht im Mittelalter) nach Paris ging, ließ er Grimm bald nachkommen. Dieser wurde jedoch bald der juristischen Studien überdrüssig und gab in Briefen kund, dass er sich künftig der altdeutschen Literatur widmen wolle.
Als er im September 1805 nach Kassel, dem Wohnort seiner Mutter, zurückgekehrt war, erlangte er einen Posten beim Kriegskollegium, das aber noch vor Ablauf eines Jahres aufgelöst wurde.

Nach dem Tod der Mutter 1808 musste Jacob Grimm die Familie ernähren. Er wurde Bibliothekar Jérôme Bonapartes, des Königs von Westphalen, und wurde im Februar 1809 außerdem zum Beisitzer im Staatsrat ernannt. Die Muße, die ihm die amtlichen Geschäfte ließen, verwendete er auf das Studium der altdeutschen Poesie und Sprache. Seit 1806 hatte er gemeinsam mit seinem Bruder Wilhelm sowie dem Kreis um seinen Freund Werner von Haxthausen Märchen gesammelt, die nun bearbeitet und herausgegeben wurden.

Nach der Wiederherstellung des Kurfürstentums Hessen verlor Grimm seine Stelle als Privatbibliothekar des Königs Jérôme, wurde jedoch in den diplomatischen Dienst des zurückgekehrten Kurfürsten übernommen. 1814/15 war er kurhessischer Legationssekretär beim Wiener Kongress, wo er mit Gleichgesinnten im Gasthaus "Zum Strobelkopf" in der Straße "Wollzeile" die „Wollzeilergesellschaft“ gründete. Kurzzeitig verhandelte er in Paris über die Rückführung geraubter Kunstschätze nach Hessen und Preußen.

In dieser Zeit begann er mit dem Studium der slawischen Sprachen. 1815 nahm er seinen Abschied als Diplomat, um sich der Literaturgeschichte und Sprachforschung zu widmen. Ein Jahr später wurde er Zweiter Bibliothekar an der Bibliothek zu Kassel (sein Bruder Wilhelm war 1814 dort Sekretär geworden). Als die Brüder 1829 nach dem Tod des Oberbibliothekars nicht wie erwartet befördert wurden, sahen sie sich nach einer neuen Position um.

1830 erhielt Jacob Grimm eine Professur an der Universität Göttingen, wo er auch als Rechtsbibliothekar tätig war. In den Jahren 1834–1837 hielt er dreimal eine Vorlesung über deutsche Literaturgeschichte, die durch studentische Mitschriften überliefert ist. In Göttingen diente ihm Georg Schulze als sein Amanuensis. 1837 wurde Grimm als Mitverfasser des Protestes der „Göttinger Sieben“ durch den König von Hannover seines Amtes enthoben und des Landes verwiesen.

1841 folgte er dem Ruf des preußischen Königs Friedrich Wilhelm IV. nach Berlin und wurde Mitglied der Preußischen Akademie der Wissenschaften mit dem Recht, Vorlesungen an der Friedrich-Wilhelms Universität zu halten. Auf den beiden Germanistenversammlungen 1846 in Frankfurt am Main und 1847 in Lübeck war er tonangebend. 

1848 gehörte er dem Vorparlament an. An der Frankfurter Nationalversammlung in der Paulskirche nahm er im Frühjahr 1848 als parteiloser Abgeordneter des preußischen Wahlkreises Essen-Mülheim an der Ruhr teil, wobei er einen Ehrenplatz erhielt. Da die Versammlung sehr schleppend verlief und die Resultate eher enttäuschend für ihn waren, legte er im Oktober 1848 sein Mandat nieder und hielt sich fortan aus dem aktiven politischen Leben heraus. Im selben Jahr beendete er seine Vorlesungstätigkeit und veröffentlichte in Leipzig seine "Geschichte der deutschen Sprache".

Danach konzentrierte er sich auf die Arbeit am Deutschen Wörterbuch, das den gesamten neuhochdeutschen Sprachschatz von Luther bis Goethe darlegen sollte. Die Arbeiten an diesem gewaltigen Gemeinschaftsprojekt mit seinem Bruder hatten schon 1838 begonnen.

Über dem Artikel "Frucht" des Deutschen Wörterbuches starb Jacob Grimm am 20. September 1863. Er wurde auf dem alten St.-Matthäus-Kirchhof in Berlin-Schöneberg bestattet. Das Ehrengrab des Landes Berlin befindet sich im Feld F, F-S-001/004, G1.

Zusammen mit seinem Bruder Wilhelm gilt er als Begründer der germanistischen Altertumswissenschaften, der germanistischen Sprachwissenschaft und der deutschen Philologie. Beiträge wie "Über den deutschen Meistergesang" (1811) dürfen neben der Forschung Karl Lachmanns als die ersten soliden Bestandsaufnahmen älterer deutscher Literatur gelten. Zur Durchsetzung seiner Positionen pflegte Jacob Grimm einen konfrontativen Stil, der weder Polemik noch die Verunglimpfung von Konkurrenten scheute. Zu den „Gründungsmythen“ der Germanistik gehört etwa der sogenannte „Wissenschaftskrieg“ gegen Friedrich Heinrich von der Hagen und Johann G. G. Büsching.

Berühmt wurden die beiden Brüder Grimm durch ihre gemeinsame Sammlung "Kinder- und Hausmärchen der Brüder Grimm" (2 Bände, 1812–1815) und das "Deutsche Wörterbuch" (ab 1838, 1. Band 1854).

Jacob Grimm formulierte 1822 ein erstes Lautgesetz für die germanischen Sprachen, das bis heute als „Erste Lautverschiebung“ bezeichnet wird. In angelsächsischen Ländern spricht man von "Grimm's Law", im Französischen sinngemäß von "loi de Grimm".

Ein weiteres für die Germanistik wegweisendes Werk ist die 1835 in drei Bänden publizierte "Deutsche Mythologie", welche sich auf linguistischem Wege der Lebensweise und Göttersicht nicht nur der Germanen nähert. In der Folge entstehen ähnliche Werke im finno-ugrischen und slawischen Raum. Grimms historisch-vergleichendes Vorgehen schuf auch die Voraussetzungen für die Begründung der romanischen Sprachwissenschaft durch Friedrich Diez.

Jacob Grimm war seit dem 31. Mai 1842 Mitglied des preußischen Ordens Pour le Mérite für Wissenschaft und Künste. 1857 wurde er in die American Academy of Arts and Sciences gewählt.

Nach Jacob Grimm sind ein Gymnasium in Kassel (Jacob-Grimm-Schule), eine gleichnamige Förderschule in Soest und eine Gesamtschule (Jakob-Grimm-Schule) in Rotenburg an der Fulda benannt.



Briefwechsel der Brüder Jacob und Wilhelm Grimm, Kritische Ausgabe in Einzelbänden:

Zwei Bände der Kasseler Ausgabe (Werke und Briefwechsel der Brüder Grimm):

Teile des Nachlasses (wie beispielsweise Bücher seiner Bibliothek mit Randbemerkungen) liegen in der "Staatsbibliothek zu Berlin – Preußischer Kulturbesitz" (SBB-PK). Ein weiterer Teil, darunter Briefe von und an die Brüder Jacob und Wilhelm Grimm, verschiedene Manuskriptenkonvolute und vor allem Handexemplare mit handschriftlichen Zusätzen, wird im Nachlass seines Neffen Herman Grimm im Hessischen Staatsarchiv Marburg aufbewahrt. Weitere wichtige Quellen befinden sich im Brüder Grimm-Museum Kassel.







</doc>
<doc id="14734" url="https://de.wikipedia.org/wiki?curid=14734" title="Walter Momper">
Walter Momper

Walter Momper (* 21. Februar 1945 in Sulingen, Landkreis Grafschaft Diepholz, Provinz Hannover) ist ein deutscher Politiker (SPD) und Politikwissenschaftler. Von 1989 bis 1991 war er der 11. Regierende Bürgermeister von Berlin. Die Friedliche Revolution in der DDR mit dem anschließenden Fall der Berliner Mauer und die Deutsche Wiedervereinigung fielen in seine Amtszeit. Von 2001 bis 2011 war er Präsident des Abgeordnetenhauses von Berlin. 
Die Schulzeit verbrachte Momper in Bremen. Nach dem Abitur 1964 begann er ein Studium der Politischen Wissenschaften, Geschichte und der Volkswirtschaftslehre an der Westfälischen Wilhelms-Universität Münster, in München und an der Freien Universität Berlin, welches er 1969 als Diplom-Politologe beendete.

Er wurde wissenschaftlicher Assistent am Institut für Politische Wissenschaften an der FU Berlin. 1970 wechselte er als wissenschaftlicher Angestellter an das Geheime Preußische Staatsarchiv der Stiftung Preußischer Kulturbesitz. Von 1972 bis 1986 war er wissenschaftlicher Angestellter und Geschäftsführer der Historischen Kommission zu Berlin. Nachdem Momper anschließend ausschließlich politisch aktiv war, übernahm er von 1992 bis 1993 die Geschäftsführung der Dr. Ellinghaus GmbH. Seit August 1993 ist er geschäftsführender Gesellschafter der Momper Entwicklungsgesellschaft mbH in Berlin. Kritiker und Politiker anderer Parteien kreideten ihm mehrfach die Verknüpfung seiner politischen und seiner unternehmerischen Tätigkeit an, so als er den Einrichtungskonzern Ikea bei dessen Ansiedlungsvorhaben im Osten Berlins beriet.

Seit 1967 ist Momper Mitglied der SPD. Von 1986 bis 1992 war er Landesvorsitzender der SPD in Berlin. Von 1988 bis 1993 war er außerdem Mitglied im SPD-Bundesvorstand. 1975 wurde Momper in das Abgeordnetenhaus von Berlin gewählt. Dort war er ab 1985 Vorsitzender der SPD-Fraktion.

Aus der Wahl zum Abgeordnetenhaus von Berlin 1989 ging die SPD unter Mompers Spitzenkandidatur als Sieger hervor, während die CDU/FDP-Koalition unter dem Regierenden Bürgermeister Eberhard Diepgen (CDU) überraschend eine schwere Niederlage erlitt. Vorausgegangen war unter anderem der in den Medien breit diskutierte Skandal um den Charlottenburger Baustadtrat Wolfgang Antes (CDU).

Am 16. März 1989 wurde Momper zum Regierenden Bürgermeister gewählt. Er konnte sich auf eine Rot-Grüne Koalition zwischen SPD und Alternativer Liste (AL) (dem damaligen Berliner Äquivalent der Grünen) stützen. Dem Senat Momper gehörten als erster Landesregierung in Deutschland mehr Frauen als Männer an (acht zu sechs).

In der Nacht vom 9. zum 10. November 1989 wurde die Berliner Mauer geöffnet – ein Ereignis, über dessen Vorbereitung seitens der DDR-Regierung Momper nach eigenen Angaben seit dem 29. Oktober 1989 aus einem Gespräch mit Ost-Berlins SED-Chef Günter Schabowski und Ost-Berlins Oberbürgermeister Erhard Krack informiert war und seinerseits entsprechende Vorbereitungen traf. Mompers Satz „Berlin, nun freue dich“ ging um die Welt. Momper wurde in dieser Zeit über die Grenzen Deutschlands hinaus bekannt. Die Grundlage für das Zusammenwachsen der beiden Stadthälften und Berlins mit dem Umland wurde am 12. Dezember 1989 gelegt: Bei einem Treffen Mompers mit DDR-Ministerpräsident Hans Modrow wurde als erstes grenzüberschreitende Gremium der provisorische Regionalausschuss gegründet.

Als Regierender Bürgermeister von Berlin war Momper vom 1. November 1989 bis zum 31. Oktober 1990 Bundesratspräsident und damit Stellvertreter des Bundespräsidenten.

Bekannte Entscheidungen seines rot-grünen Senats waren eine Geschwindigkeitsbegrenzung (100 km/h) auf der bis dahin tempolimitfreien AVUS sowie die Einrichtung von weiteren Busspuren für die BVG, den Berliner öffentlichen Personennahverkehr. Nach der Beendigung einer Reihe von Hausbesetzungen mittels der Räumung der Mainzer Straße durch die Polizei am 14. November 1990 kündigte die AL die Koalition mit der SPD auf, da sowohl Momper als auch der zuständige Innensenator Erich Pätzold (SPD) diesen Einsatz als politisch richtig einstuften.

Daher ging Momper mit einem SPD-Minderheitssenat in die Wahl des Abgeordnetenhauses vom 2. Dezember 1990. Die SPD blieb dabei zehn Prozentpunkte hinter der CDU, deren Spitzenkandidat Diepgen, Mompers Vorgänger, daraufhin am 24. Januar 1991 erneut zum Regierenden Bürgermeister gewählt wurde. Momper blieb zunächst SPD-Landesvorsitzender, erklärte aber schließlich am 17. August 1992 im Zusammenhang mit seinem Einstieg in die Immobilienwirtschaft seinen Rücktritt. 1995 trat er bei der parteiinternen Urwahl der Berliner SPD für die Spitzenkandidatur zu den Abgeordnetenhauswahlen an, unterlag aber der Sozialsenatorin Ingrid Stahmer. Bei der Wahl 1995 schied er zunächst aus dem Abgeordnetenhaus aus. Nachdem er sich 1999 bei der Urwahl des SPD-Spitzenkandidaten gegen den Vorsitzenden der SPD-Fraktion im Berliner Abgeordnetenhaus, Klaus Böger, durchgesetzt hatte, wurde er zwar wieder Mitglied des Abgeordnetenhauses, unterlag aber deutlich dem Regierenden Bürgermeister Diepgen. Er wurde jedoch zum Vizepräsidenten des Abgeordnetenhauses gewählt. Nachdem die SPD bei den vorgezogenen Wahlen am 21. Oktober 2001 stärkste Partei geworden war, wurde er zum Präsidenten des Abgeordnetenhauses gewählt und nach der Wahl zum Abgeordnetenhaus von Berlin 2006 in diesem Amt bestätigt.

Bei der Wahl zum Berliner Abgeordnetenhaus am 18. September 2011 verzichtete Momper auf eine erneute Kandidatur. Am 1. September 2011 leitete er zum letzten Mal eine Sitzung des Abgeordnetenhauses.

Bei der Wahl des Regierenden Bürgermeisters am 23. November 2006 erhielt der vorgeschlagene SPD-Kandidat Klaus Wowereit nur 74 von 149 Stimmen. Obwohl die erforderliche Mehrheit von 75 Stimmen damit nicht erreicht war, fragte Momper Wowereit, ob er die Wahl annehme. Danach wollte Momper bereits zur Vereidigung schreiten, als er durch Zwischenrufe auf seinen Fehler hingewiesen wurde. Die Berliner Oppositionsparteien forderten daraufhin seinen Rücktritt als Parlamentspräsident – auch weil die Vereidigung Wowereits durch Momper im Anschluss an den erfolgreichen zweiten Wahlgang nicht pannenfrei verlief. Momper bat öffentlich um Entschuldigung, lehnte einen Rücktritt aber ab.

Momper trat in der Öffentlichkeit regelmäßig mit einem roten Schal auf, der als „Momper-Schal“ bezeichnet wurde. Momper ist mit Anne Momper verheiratet und hat zwei Kinder.




</doc>
<doc id="14736" url="https://de.wikipedia.org/wiki?curid=14736" title="Wilhelm Grimm">
Wilhelm Grimm

Wilhelm Carl Grimm (* 24. Februar 1786 in Hanau; † 16. Dezember 1859 in Berlin) war ein deutscher Sprach- und Literaturwissenschaftler sowie Märchen- und Sagensammler. Sein Lebenslauf und sein Werk ist eng mit dem seines ein Jahr älteren Bruders Jacob Grimm verbunden, worauf die oft gebrauchte Bezeichnung Brüder Grimm hinweist.

Wilhelm Grimm verbrachte seine Jugend in Steinau an der Straße, wohin der Vater Philipp Wilhelm Grimm 1791 als Amtmann versetzt worden war. Er besuchte dann wie sein Bruder Jacob das Friedrichsgymnasium in Kassel und schrieb sich gleichfalls an der Universität Marburg ein, wo er bei Friedrich Carl von Savigny Jura studierte. Nach Beendigung seines Studiums lebte er wieder bei der Mutter in Kassel. Asthmatische Beschwerden sowie eine Herzerkrankung hinderten ihn längere Zeit daran, sich um eine feste Anstellung zu bewerben. Seit 1806 sammelte er gemeinsam mit seinem Bruder Jacob Märchen, die sie später bearbeiteten und herausgaben. Dabei wurde er u. a. durch Werner und August von Haxthausen unterstützt. 1809 unterzog er sich bei dem berühmten Arzt Johann Christian Reil einer Kur in Halle/Saale. Bei dieser Gelegenheit wurde er von dem Komponisten Johann Friedrich Reichardt gastfreundlich aufgenommen. Mit Clemens Brentano reiste er daraufhin nach Berlin; dort lebte er mit diesem und mit Achim von Arnim in dessen Wohnung. Auf der Rückreise nach Kassel traf er Johann Wolfgang von Goethe, der sich lobend über seine „Bemühungen zu Gunsten einer lang vergessenen Kultur“ äußerte. 1813 lernte er bei der Familie Haxthausen die Schwestern Jenny und Annette von Droste-Hülshoff, die Dichterin, kennen. Beide halfen bei der Sammlung von Märchen und Volksliedern. Mit Jenny verband ihn danach eine lange Brieffreundschaft, auch gibt es Anzeichen dafür, dass eine unerfüllte Liebesbeziehung zwischen ihnen bestand.

Von 1814 bis 1829 war Grimm als Sekretär an der Bibliothek in Kassel angestellt. 1825 heiratete er Henrietta Dorothea Wild. 1828 kam sein Sohn Herman Grimm zur Welt, der später relativ bekannt wurde für seine kunstgeschichtlichen Vorlesungen unter Einsatz von Lichtbildprojektion an der Universität Berlin.

1831 wurde Wilhelm Grimm Bibliothekar an der Universität Göttingen, 1835 erhielt er dort eine außerordentliche Professur. Als Mitunterzeichner des Protestes der „Göttinger Sieben“ wurde er – wie auch sein Bruder – 1837 durch den König von Hannover seines Amtes enthoben und des Landes verwiesen. Der preußische König Friedrich Wilhelm IV. lud beide 1841 nach Berlin, wo sie sich niederließen. Im selben Jahr wurden sie Mitglied der Preußischen Akademie der Wissenschaften. 1848 gehörte er dem Vorparlament an. 1852 wurde er zum auswärtigen Mitglied der Bayerischen Akademie der Wissenschaften gewählt. Wilhelm Grimm lehrte bis zu seinem Tod 18 Jahre an der Universität Berlin und arbeitete dort zusammen mit seinem Bruder an ihrem Deutschen Wörterbuch, über das er auf dem Germanistentag 1846 in Frankfurt gesprochen hatte.

Neben der gemeinsamen Arbeit mit dem Bruder konzentrierte sich Wilhelm Grimm bei seinen Forschungen auf die Poesie des Mittelalters, die deutsche Heldensage sowie die Runenforschung. Zusammen mit seinem Bruder begründete er die germanistischen Altertumswissenschaften, die germanistische Sprachwissenschaft und die deutsche Philologie.

Berühmt wurden beide durch ihre Sammlung "Kinder- und Hausmärchen" (2 Bände, 1812–1815), an deren Bearbeitung Wilhelm besonderen Anteil hatte, und durch die Arbeit am "Deutschen Wörterbuch" (ab 1838, 1. Band 1854). 1839 gab er die Werke seines Freundes Achim von Arnim heraus. Wilhelm Grimm veröffentlichte auch Altdänische Heldenlieder, Balladen und Märchen. 

Die Berliner Akademie schrieb im Januar 1860: 
Er wurde auf dem Alten St.-Matthäus-Kirchhof in Berlin-Schöneberg bestattet (heute Ehrengrab der Stadt Berlin). Die Grabstätte befindet sich im Feld F, F-S-001/004, G1. Sie ist seit 1952 als Ehrengrab der Stadt Berlin gewidmet.

Ein Teil des Nachlasses von Jacob und Wilhelm Grimm, darunter Briefe von und an die Brüder, verschiedene Manuskriptenkonvolute und vor allem Handexemplare mit handschriftlichen Zusätzen, wird im Hessischen Staatsarchiv Marburg verwahrt. Der Bestand ist vollständig erschlossen und über HADIS online recherchierbar.





</doc>

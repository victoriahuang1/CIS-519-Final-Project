<doc id="14338" url="https://de.wikipedia.org/wiki?curid=14338" title="Chișinău">
Chișinău

Chișinău [], (, veraltet "Kischenau/Kischinew"; ), ist die Hauptstadt der Republik Moldau und mit mehr als 700.000 Einwohnern auch die bevölkerungsreichste Stadt des Landes. Sie ist ein wichtiger Wirtschaftsstandort sowie Universitätsstadt und Kulturzentrum. Chișinău hat eine Fläche von 120 km². Zusammen mit ihrem Umkreis bildet sie das Munizipium Chișinău, das 563,3 km² groß ist und über 800.000 Menschen beheimatet.

"Chișinău" lässt sich nach Ansicht von Historikern etymologisch aus der Kombination des altrumänischen Wortes "chișla" (Wasserquelle, heute "cișmea") und "nouă" (neu) ableiten und weist so auf eine Grundwasserquelle hin, die in den Ursprüngen der Stadt als wichtige Versorgungsgrundlage diente. Diese Quelle befindet sich heute an der Kreuzung der Straßen "A. Pușkin" und "Albișoara".

Unter anderen rumänischen Historikern, wie früher Ștefan Ciobanu, wird dieselbe Ableitung wie für Chișineu-Criș vertreten, also vom ungarischen Namen "Kis-Jenő" (auf Ungarisch: "kis" „klein“ + "Jenő" „Eugen“ und zugleich Name eines der sieben altmagyarischen Stämme, bei Konstantin VII. in De Administrando Imperio: "Genach"). Als das Kumanische Reich im 13. Jahrhundert besiegt worden war, geriet die Region unter ungarische Hegemonie. Die Szekler errichteten in dieser Region Befestigungen, um das Königreich Ungarn gegen weitere Mongoleneinfälle zu schützen. Dazu zählen Miclăușeni (ung. Miklóshely), Orhei (ung. Várhely) und Ciubărciu (Ciobruciu; ung. Csupor) unweit der heutigen Hauptstadt Moldaus. So erscheint ein ungarischer Ursprung plausibel.

Die offizielle Bezeichnung der Stadt lautet "Municipiul Chișinău" (Munizip Chișinău), wobei auch die umliegenden, zum Munizip gehörenden Gemeinden gemeint sind. Durch die häufigen Veränderungen der Gebietszugehörigkeit hatte die Stadt mehrmals verschiedene Schreibweisen ihres Namens. So nannten sie die Russen Кишинёв/"Kischinjow", Aussprache [], als sie Hauptstadt von Bessarabien war.Später, als Bessarabien Teil von Rumänien wurde, bekam die Stadt den offiziellen Namen "Chișinău" []. Während der kurzzeitigen deutschen Besetzung im Zweiten Weltkrieg wurde die Stadt vermutlich mit der damals gängigen russisch-deutschen Transkription als "Kischinew" bezeichnet.Mit der sowjetischen Annexion nach dem Zweiten Weltkrieg führte man in der Moldauischen Sowjetrepublik die kyrillische Schrift ein. "Chișinău" wurde entsprechend "Кишинэу" geschrieben, parallel dazu ein zweites Mal die russifizierte Variante "/Kischinjow".Bereits kurz vor Ende des Ostblocks beschloss die Regierung der Moldauischen SSR am 31. August 1989 die Rückkehr zur lateinischen Schreibweise des Rumänischen – aus Кишинэу wurde wieder "Chișinău".

Die Stadt Chișinău liegt am Bîc auf etwa bei 47° 00' 50" nördlicher Breite und 28° 51' 00" östlicher Länge. Das etwa 120 km² große Stadtgebiet ist in fünf, mit Ausnahme des Centru flächenmäßig etwa gleich große Stadtbezirke (rumänisch "Sector") unterteilt:

Zum Munizip Chișinău "(Municipiul Chișinău)" mit einer Fläche von 635 km² gehören neben elf kleineren Dörfern die folgenden sechs umliegenden Gemeinden:

Politisch liegt die Stadt mitten im Zentrum der Republik Moldau. Geographisch im Osteuropäischen Flachland gelegen, ist die Stadt umgeben von einer flachhügeligen Landschaft mit sehr fruchtbarem Erdboden aus Schwarzerde, der schon seit Urzeiten die Grundlage für landwirtschaftliche Nutzung bot für den Anbau sowohl von Wein als auch von Obst. Durchzogen von dem Fluss Bîc zeigt die Stadt, besonders im Frühling und im Sommer, ein sehr naturbezogenes Stadtbild mit vielen Bäumen und großen Parkanlagen.

Erste Wetterdaten reichen bis in das Jahr 1884 zurück. Damals beschäftigten sich die Forschungen allerdings eher mit dem idealen Klima für einen optimalen Weinbau. Dabei rechnet man im Verlauf eines Jahres mit etwa 2.215 Stunden Sonnenschein – davon 329 Stunden alleine im Rekordmonat Juli – im Dezember dagegen nur mit 54 Stunden. Regional herrscht ein kontinentales Klima mit einer Jahresdurchschnittstemperatur von 9,6 °C und einer Niederschlagsmenge von 547 mm. Der Sommer beginnt etwa Mitte Mai, er fällt kurz aus, dafür kräftig. Hohe Temperaturen um 25 °C erreicht das Thermometer vor allem in den Monaten Juni, Juli und August. Mit verstärktem Niederschlag ist im Juni und Juli zu rechnen. Wie der Sommer ist auch der Winter sehr kurz. Der Januar erreicht mit durchschnittlich −3,2 °C die tiefsten Temperaturen, der Oktober mit 27 mm die geringste Niederschlagsmenge. Ausgeprägt lang und warm ist der Herbst dank der Lage nahe dem Schwarzen Meer, welches das Klima der Region stark beeinflusst. Meist herrscht jedoch eine mittlere Temperatur um 10 °C mit wenig Niederschlag während des gesamten Jahres.

Chișinău ist eine ausgesprochen grüne Stadt. Viele Hauptstraßen sind von Bäumen gesäumt. Hinzu kommen großzügige Parkanlagen, die auf dem ganzen Stadtgebiet verteilt liegen und das Stadtbild prägen. Zu den wichtigsten Parks gehören:

In den Parkanlagen von Chișinău leben etwa 14 Vogel-, 14 Reptil- und Amphibienarten. Vertretene Säugetiere sind Igel, Maulwurf, Wiesel und Marder sowie Fledermäuse. Dazu kommen verschiedene Nagetiere, wie beispielsweise Eichhörnchen und Feldmaus. Bei den Vögeln findet man eine Reihe verschiedener Taubenarten, wie zum Beispiel die Waldtaube. Zudem gibt es Mauersegler, Stare und Spatzen.
In den Gewässern des Parks "Valea Morilor", im südwestlichen Teil von Chișinău gelegen, leben etwa 20 verschiedene Fischarten. Der See bietet Lebensraum für Barsche, Karauschen, Brachsen, Karpfen sowie weitere Karpfenfische.

Die erste schriftliche Erwähnung von Chișinău geht ins Jahr 1436 zurück, als die Ortschaft Teil des Fürstentums Moldau war. Dieses Fürstentum stand zuerst unter polnischer, später unter osmanischer Oberhoheit. Eine nennenswerte Entwicklung blieb aus, und die Ortschaft blieb bis ins 19. Jahrhundert als Bojaren- und Klostersiedlung praktisch unverändert. 1818 wurde die kleine Stadt als "Kischinjow" Verwaltungssitz des vom Osmanischen Reich an das Russische Kaiserreich im Frieden von Bukarest 1812 abgetretenen Gouvernements Bessarabien. Kischinjow genoss als Stadt am Rande des Russischen Reichs und als Strafversetzungslager für Unzufriedene und Aufmüpfige keinen guten Ruf. Der junge russische Nationaldichter Alexander Sergejewitsch Puschkin war vom 21. September 1820 bis 1823 als Übersetzer nach Kischinjow verbannt und schrieb über die Stadt:

"„O Kischinjow, o dunkle Stadt!“"; "„Verfluchte Stadt Kischinjow, die Zunge wird nicht müde, Dich zu beschimpfen.“"

Ab 1834 entstand durch einen großzügigen Stadtentwicklungsplan ein imperiales Stadtbild mit breiten und langen Straßen. Dieser teilte die Stadt grob in zwei Bereiche: die "Altstadt" mit ihren verwinkelt gebauten Straßen und unregelmäßigen Gebäudestrukturen sowie die "Innenstadt" mit dem im Voraus geplanten Konzept des Straßenverlaufs. Zur selben Zeit wurden auch das Stadtzentrum und der im Bezirk "Centru" liegende Bahnhofsplatz geplant. Zwischen dem 26. Mai 1830 und dem 13. Oktober 1836 errichtete der Architekt Avraam Melnikov die "Catedrala Nașterea Domnului" mit ihrem prächtigen Glockenturm. 1840 folgte der Bau des im folgenden Jahr fertiggestellten Triumphbogens durch den Architekten Luca Zaușkevici. In unmittelbarer Umgebung wurde mit dem Bau einer Vielzahl weiterer Gebäude und Plätze begonnen.

1858 entstand die von dem Architekten P. Piskariov erbaute "Catedrala Sfîntul Mare Mucenic Teodor Tiron", die sich mit ihrem blauen Erscheinungsbild vom Rest abhebt. Im weiteren Verlauf des Jahrhunderts wuchs die Stadt kontinuierlich. 1891 leitete der Schweizer Architekt Alexander Bernardazzi den Bau mehrerer Projekte, darunter den der "Biserica Sfîntul Pantelemon" ("Grecească" – griechische Kirche), sowie von 1900 bis 1903 des Frauengymnasiums "Dadiani" und der dortigen Kapelle (1895–1897). Zwischen 1898 und 1901 entstand am "Bulevardul Ștefan cel Mare și Sfînt" durch Mitrofan Elladi und Alexander Bernardazzi das Rathaus der Stadt, das "Fosta Dumă Orășenească".

Kischinjow war um 1900 ein Zentrum jüdischen Lebens im Russischen Kaiserreich. So bildeten Juden mit einem Anteil von 45,9 % laut einer Zählung aus dem Jahr 1897 die größte Bevölkerungsgruppe in Kischinjow, vor den Russen (27,0 %) und den Rumänen (17,6 %).
Am und , dem ersten Osterfeiertag, kam es in Chișinău zu einem großen antisemitischen Pogrom. Dabei starben 47–49 jüdische Einwohner; schätzungsweise 400 wurden verletzt. Hunderte Haushalte und hunderte Geschäfte wurden geplündert und zerstört. Der damalige Bürgermeister Karl Schmidt (1846–1928), der bessarabiendeutscher Herkunft war, trug wesentlich zur Aufklärung und Strafverfolgung der Täter bei. Die als das "„Pogrom von Kischinjow“" bezeichneten Ausschreitungen wurden offenbar vom Verleger der damals einzigen offiziellen Zeitung, "Bessarabez" , demagogisch geschürt und wiesen Anzeichen einer organisierten Aktion auf. Die Reaktionen in der Weltpresse waren heftig, selbst im Russischen Zarenreich. So erhielt im Juli 1905 Zar Nikolaus II. eine vom amerikanischen Volk an Präsident Theodore Roosevelt aufgetragene Petition, die sich allerdings auf seine Politik nicht auswirkte. Seit ihrer Ablehnung durch den Zaren ist sie (bis heute) im Besitz der US-Regierung. Der Hilfsverein der deutschen Juden unter dem Vorsitz von Paul Nathan rief die Vertreter von relevanten jüdischen Organisationen aus verschiedenen Ländern zu einer Erörterung der Situation zusammen.

Am 17. Juni 1903 überlebte der Zeitungsverleger Pawel Alexandrowitsch Kruschewan eine Messerattacke durch den Kiewer Studenten Pinchas Daschewski auf dem Newski-Prospekt in Sankt Petersburg, der ihn nur leicht verwundet hatte. Zeitungen wurden zu dieser Zeit durch die russische Geheimpolizei Ochrana in ihrem antisemitischen Tun bewusst unterstützt und gefördert. Dazu gehörte auch das Verbreiten von Publikationen, z. B. der „Protokolle der Weisen von Zion“.

Am 22. August 1905 kam es in der Stadt erneut zu einer blutigen Eskalation, als die Polizei das Feuer auf geschätzt 3.000 demonstrierende Landarbeiter eröffnete. Vergleichbar ist diese Tragödie mit dem Petersburger Blutsonntag, der sich am in Sankt Petersburg ereignete; dort wurden etwa 1.000 demonstrierende Arbeiter getötet.

Wenige Monate später, am und , geriet ein Demonstrationszug, der sich gegen die Erklärung des Oktobermanifestes von Zar Nikolaus II. richtete, außer Kontrolle, und Anhänger der Oktobristen und Schwarzhunderter führten in der Stadt bewaffnete Attacken gegen Juden, liberale Studenten und sozialdemokratische Arbeiter durch. Dabei starben 19 Juden, 56 wurden verletzt. Diese Judenfeindlichkeit führte schließlich zu einem stetigen Abwandern der jüdischen Bevölkerung in die Vereinigten Staaten und nach Palästina.

Im Zuge der russischen Oktoberrevolution übernahm im November 1917 eine nationale Vollversammlung namens "Sfatul Țării" (Landrat) mit Sitz in Chișinău die Regierung. Am erklärte sich das Land zu einem autonomen Gebiet innerhalb von Russland und die "Moldauische Demokratische Republik" wurde ausgerufen. Nachdem Bolschewiki am 5. Januar 1918 Chișinău besetzt hatten, bat der Landrat Rumänien um militärischen Beistand. Die am 16. Januar einmarschierten rumänischen Truppen stellten innerhalb von wenigen Tagen die Ordnung im Land wieder her. Am erklärte der "Sfatul Țării" die Unabhängigkeit und am , unter Beibehaltung einer Teilautonomie, den Anschluss an Rumänien. Der Anschluss wurde 1920 im Pariser Vertrag durch die westlichen Alliierten bestätigt. Chișinău verlor mit der Auflösung des Sfatul Țării seinen Status als Hauptstadt und damit an Bedeutung.

In der Zwischenkriegszeit unternahm die Stadt große Renovierungsarbeiten im Zentrum. Dabei wurde 1927 auch ein Denkmal des Fürsten der Moldau, Ștefan cel Mare și Sfînt, durch den Künstler Alexandru Plămădeală und den Architekten Eugen Bernardazzi errichtet.

Im Zweiten Weltkrieg wurde Chișinău fast vollständig zerstört. Am 28. Juni 1940 wurde die Stadt durch die Rote Armee besetzt. Dabei wurde das zu Rumänien gehörende Gebiet Bessarabien von der Sowjetunion annektiert. Am 10. November 1940 ereignete sich ein verheerendes Erdbeben. Das Beben mit Epizentrum im östlichen Rumänien hatte eine Stärke von 7,3 auf der Richterskala und führte in der Stadt zu massiven Zerstörungen.

Nach knapp einem Jahr Friedensverhandlungen (deutsch-sowjetischer Nichtangriffspakt) folgte am 22. Juni 1941 der Deutsch-Sowjetische Krieg, dem sich auch rumänische Truppen anschlossen. Zu Beginn des Großangriffs war auf dem Gebiet der Stadt das II. mechanisierte Korps (Panzer- und motorisierte Infanterie) stationiert. Das Gebiet um die Stadt wurde von der 9. Roten Armee von Jakow Tscherewitschenko und der von Andrei Smirnow befehligten 18. Roten Armee kontrolliert. Im Juli 1941 war die Stadt schwer umkämpft, bei zähem Widerstand der sowjetischen Truppen. Es gab Bombardierungen durch die deutsche Luftwaffe. Die vorrückende deutsche 11. Armee unter Generaloberst Eugen von Schobert, Teil der Heeresgruppe Süd unter Generalfeldmarschall Gerd von Rundstedt, wurde durch Truppen der rumänischen 3. und 4. Armee unterstützt. Der sowjetische Widerstand hielt bis zum 17. Juli 1941, als Chișinău schließlich erobert wurde. Deutsche und rumänische Truppen besetzten die Stadt von Norden über die Ortschaft Sculeni und von Süden via Hîncești.

Während der deutsch-rumänischen Besetzung kam es in der Stadt zu systematisch organisiertem Massenmord überwiegend an jüdischen Einwohnern. Die zusammengetriebenen Personen wurden auf Lastwagen verladen und aus der Stadt transportiert. Dort mussten sie teilweise selbst die Gruben ausheben, in denen sie erschossen wurden. Das Kommando über die Ausführung hatte Paul Zapp, Anführer des "Sonderkommandos 11a". Als Teil der Einsatzgruppe D unterstand dieses Kommando dem SS-Gruppenführer Otto Ohlendorf. Die Zahl der nach der Besetzung von Chișinău ermordeten Juden wird auf etwa 10.000 geschätzt.

Das am 24. Juli 1941 in der Altstadt von Chișinău eingerichtete Ghetto diente als Zwischenstation, dessen Bewohner man als Arbeitskräfte im Steinbruch von Ghidighichi arbeiten ließ. Das Ghetto beschränkte sich auf wenige Straßen und bestand aus wenigen Gebäuden, von denen die meisten bereits stark zerstört waren. Um das Ghetto verlief eine Mauer mit mehreren kontrollierten Ein- und Ausgängen. Laut Angaben der rumänischen Armee befanden sich im Ghetto von Chișinău 11.525 Personen.

Zehntausende Juden und andere unerwünschte Ethnien wurden direkt in sogenannten Todesmärschen in das östlich gelegene Transnistria (nicht zu verwechseln mit dem heute flächenmäßig kleineren Transnistrien) deportiert. Es gab Überquerungsorte bei Rezina nahe Rîbnița, bei Cosăuți in der Nähe von Soroca und in Otaci bei der ukrainischen Ortschaft Mohyliw-Podilskyj. Etwa ein Drittel von ihnen starb an Erschöpfung, andere wurden erschossen; nur wenige konnten sich in der Ukraine verstecken. Einige ausgesonderte Gruppen ließ man erst in Lagern sammeln, wie etwa 23.000 im Lager in Vertujeni (heute "Vertiujeni"), um sie zur Zwangsarbeit zu pressen. Andere Lager befanden sich in Secăreni, Edineț und Mărculești.

Verschiedene Berichte zeugen von grässlichen Geschehnissen in dieser Region. Dazu gehört der Todeszug von Iași. Am 1. August 1941 brachte man auf Befehl der Gestapo 450 Juden aus dem Ghetto von Chișinău, vor allem Frauen und Gelehrte, nach Visterniceni, etwa zwei Kilometer von der Stadt entfernt; 411 wurden erschossen, wie Überlebende nach ihrer Rückkehr berichteten. Am 6. August wurden etwa 200 Juden von Polizeibeamten aus Chișinău erschossen, ihre Leichen wurden in den einige Kilometer östlich von Chișinău fließenden Dnister geworfen. Am 7. und 8. August brachte man 525 Juden, darunter 25 Frauen, zur Arbeit am Bahnhof Ghidighichi; von ihnen kamen nach einer Woche noch ca. 200 zurück.

Auf Befehl des rumänischen Marschalls Ion Antonescu begann man schließlich, das Ghetto in Chișinău zwischen dem 4. Oktober 1941 und Mai 1942 ebenfalls zu räumen und die Gefangenen auf Todesmärschen nach Transnistria zu deportieren. Von den ehemals 65.000 Juden in Chișinău im Jahr 1939 fielen 53.000 dem NS-Regime zum Opfer. Der Holocaustforscher Matatias Carp befasste sich eingehend mit dem Holocaust in Rumänien.
Stark ins Kriegsgeschehen einbezogen wurde die ehemalige bessarabische Provinzhauptstadt auch gegen Kriegsende, beim Rückzug der deutschen und rumänischen Truppen. Am 28. März 1944 überschritten Teile der sowjetischen 2. Ukrainischen Front den Pruth nördlich von Jassy (Iași) und bezogen eine Linie am Karpatenkamm. Die deutsch-rumänische Front wurde immer weiter zurückgedrängt, bis Anfang April die 3. Ukrainische Front im Osten bei Tiraspol entlang dem Dnister zum Stehen kam.

Am 20. August 1944 folgte schließlich der sowjetische Großangriff „Operation Jassy-Kischinew“ unter der Führung von Fjodor Tolbuchin und Rodion Malinowski. Durch den Angriff in Form einer Zangenoperation geriet ein Teil der Heeresgruppe Süd von Generaloberst Johannes Frießner, darunter die neugruppierte deutsche 6. Armee unter Führung des Generals der Artillerie, Maximilian Fretter-Pico, am 24. August bei ihrem Rückzug südwestlich von Chișinău und Huși in einen Kessel und wurde vernichtet. Ebenfalls am Kampfgeschehen beteiligt war der spätere sowjetische Stadtkommandant von Berlin, Generaloberst Nikolai Bersarin, der mit seiner 5. Stoßarmee die deutschen Linien am Dnister durchbrach. Teile der deutschen 8. Armee konnten sich über die Karpaten nach Ungarn zurückziehen, während die 6. Armee zum Großteil vernichtet wurde. Die bis dahin mit den Deutschen verbündete rumänische Armee wechselte bereits am 23. August 1944 die Seite und stellte den Kampf ein. Am 24. August 1944 wurde Chișinău von der Roten Armee besetzt.

Die Stadt verlor, obwohl sie von direkten Kampfhandlungen verschont blieb, bis Kriegsende schätzungsweise 70 % ihrer Wohnfläche. Vor allem das Erdbeben von 1940 und die Luftangriffe beim Vorbeiziehen der Fronten trugen wesentlich dazu bei.

Nach der Wiedereroberung forderte die Sowjetunion unter Josef Stalin das bereits aufgrund des geheimen Zusatzprotokolls des deutsch-sowjetischen Nichtangriffspakts von 1939 annektierte Bessarabien zurück. Mit dem Friedensvertrag von Paris im Februar 1947 erkannte Rumänien Bessarabien als Teil der UdSSR an. Chișinău wurde Hauptstadt der Moldauischen Sozialistischen Sowjetrepublik.

In der Stadt befand sich das Kriegsgefangenenlager "198" für deutsche Kriegsgefangene des Zweiten Weltkriegs.

Nach den schweren Beschädigungen im Zweiten Weltkrieg setzte ab Ende der 1940er Jahre ein rasantes Bevölkerungswachstum in Chișinău ein.
Von 1947 bis 1949 entwickelte der Architekt Alexei Schtschussew unter Mithilfe eines mehrköpfigen Architektenteams einen Plan zum schrittweisen Wiederaufbau der Stadt. Während Stalin weiterhin auf riesige Paläste im Zuckerbäckerstil (Sozialistischer Klassizismus) setzte, litt die Bevölkerung zunehmend unter Wohnungsmangel.

Mit dem Beginn der Ära Chruschtschow im September 1953 wurde in der ganzen Sowjetunion zu Sparmaßnahmen aufgerufen. Chruschtschow versammelte im Dezember 1954 die leitenden Architekten und Baufunktionäre der Sowjetunion zur „Allunionskonferenz der Bauschaffenden“ und ließ öffentlich die Entstalinisierung der Baukultur und die Abschaffung des „Konservatismus in der Architektur“ bekanntgeben – unter dem Motto „Besser, billiger und schneller bauen“ folgten drastische Änderungen im Wohnkonzept. Mit dem neuen Baustil jener Zeit entstand das bis heute charakteristische Stadtbild von Chișinău mit vielen großen Wohnblocks, angeordnet im Stil von „Chruschtschowki“ (, Plattenbau-Siedlungen). Um das eigentliche Stadtzentrum herum entstanden dabei neue Wohnbezirke, sogenannte Schlafstädte mit Einzelhandelsgeschäften und Schulen, aber wenig sozialer Infrastruktur.

"Siehe auch: Sozialistischer Städtebau"

Am 4. März 1977 ereignete sich in der Stadt ein schweres Erdbeben, das Panik auslöste und mehrere Todesopfer forderte. 1989 kam es in Chișinău zu Spannungen mit der Zentralregierung in Moskau, was zur Wiedereinführung der rumänischen Sprache und 1991 zur Unabhängigkeit des Landes führte.


1817 gab es in Chișinău den ersten Bürgermeister. 1990 wurde das Amt, das 1941 abgeschafft worden war, wieder eingeführt und Nicolae Costin zum ersten Bürgermeister nach der Sowjet-Ära gewählt.

Bei der Wahl 2007 wurde der prowestliche, damals erst 28-jährige Dorin Chirtoacă per Stichwahl mit 61 % der Stimmen gewählt. Er setzte sich als Herausforderer klar gegen den kommunistischen Kandidaten Veaceslav Iordan (38 %) durch. Die Wahlbeteiligung lag bei 35 %.

Chișinău hat insgesamt 13 Partnerschaften mit anderen Städten geschlossen. Seit Dezember 1989 unterhält Chișinău eine Städtepartnerschaft mit Mannheim, die einzige deutschsprachige. Weitere Partnerschaften gibt es mit

Zudem gibt es eine Reihe von Kooperationsabkommen mit anderen Städten und Gebieten, darunter mit Moskau, Saratow und Tula (Russland), Jerusalem (Israel), Città di Castello (Italien), Hampshire (Vereinigtes Königreich), Astana (Kasachstan), Damaskus (Syrien), Cherson (Ukraine), Vilnius (Litauen), Pitești (Rumänien), Woiwodschaft Lebus (Polen) und Roms Munizip XVII (Italien).

In Chișinău arbeiten mehrere internationale Organisationen, und zwar politische, humanitäre und solche der Entwicklungshilfe.


Chișinău ist ein Zentrum der Lebensmittelindustrie. So finden sich neben der Tabak- und Textilindustrie etwa eine große Weinkellerei sowie Produktionsstätten für Obst- und Gemüsekonserven. Nach dem Ende des kommunistischen Systems in Moldau entwickelte sich die Stadt zunehmend zu einem attraktiven Standort für Banken. Aufgrund der schwierigen gesetzlichen Lage und der anhaltenden Korruption im Lande blieb jedoch der Zuzug großer ausländischer Investoren wie in anderen ehemals kommunistisch regierten Ländern bislang aus.

Die Bewohner von Chișinău genießen eine im Vergleich zu ihren ländlichen Mitbürgern höhere Lebensqualität. Im europäischen Vergleich ist der Lebensstandard aber weit unterdurchschnittlich. Nach dem großen wirtschaftlichen Tief um das Jahr 2000 ist jedoch wieder Besserung eingetreten.

Einer der größeren Industriebetriebe, das ehemals Kettentraktoren herstellende Kischinjowski Traktorny Sawod ist seit 2008 insolvent.

 An öffentlichen Transportmitteln stehen neben einem dichten Trolleybus-System (seit 12. Oktober 1949) Minibusse und Taxis zur Verfügung. Letztere können rund um die Uhr telefonisch gerufen werden.

Bereits kurz nach dem Zweiten Weltkrieg gab es in Chișinău eine Straßenbahn mit 1.000 mm Spurbreite. Die Bahn wurde zunächst mit Betriebswagen des Typs "MAN 1914" geführt. In den 1950er Jahren waren erste Gothawagen des Typs "T57" aus der deutschen Gothaer Waggonfabrik im Einsatz. Der Betrieb des Tramnetzes wurde jedoch 1961 eingestellt, die Wagen wurden nach Lemberg (Lwiw) in der Ukraine verlegt.

Das meistbenutzte Verkehrsmittel zur Personenbeförderung in Moldau ist der Bus. Beliebte Ziele sind beispielsweise Bukarest, Constanța (Rumänien) und Odessa (Ukraine). Für die Fahrt nach Odessa gibt es auch Busse, deren Route nicht durch Transnistrien, sondern über die Grenzorte Palanca oder Tudora führt.
Die Stadt Chișinău verfügt über drei Busbahnhöfe, die sowohl nationale als auch internationale Routen bedienen.

Wegen des anhaltenden Konflikts zwischen der Republik Moldau und Transnistrien kam der Schienenverkehr in Richtung Ukraine zeitweise komplett zum Erliegen. Nationale Bahngesellschaft ist die "Calea Ferată din Moldova". Beim Hauptbahnhof liegt auch der einzige Rangierbahnhof des Landes.

Der internationale Flughafen Chișinău (KIV) befindet sich ca. 15 km südlich vom Stadtzentrum und bietet internationale Flugverbindungen unter anderem nach Athen, Budapest, Bukarest, Frankfurt am Main, Istanbul, Lissabon, London, Madrid, Moskau, München, Paris, Prag, Rom, Sankt Petersburg, Tel Aviv, Timișoara, Verona und Wien.

Öffentlich-rechtlichen Rundfunk überträgt die Mediengruppe "Teleradio Moldova" (TRM), die sowohl Fernsehsender als auch einige Radiostationen betreibt. Den privaten Bereich dominieren die Mediengruppe Jurnal TV und Publika. Alle drei haben ihren Sitz in Chișinău.

Der nationale TV-Sender "Moldova 1" hat seinen Hauptsitz in Chișinău. Er ist Eigentum der staatlichen TRM.

Der lokale Fernsehsender "Pro TV Chișinău" sendet seit dem 3. September 1999 täglich ein Nachrichtenformat sowie zwei Programme auf Rumänisch bzw. Russisch. Der restliche Sendezeit wird von Bukarest (Rumänien) aus bestritten.

Daneben gibt es einige lokale Radiosender in Chișinău. Hinzu kommen Sender aus Rumänien, die in lokalen Sendefenstern in Chișinău übertragen werden; die wichtigsten sind Vocea Basarabiei, Radio Noroc (lokal), Kiss FM, Pro FM, Radio 21/Hit Radio und Național FM/Fresh FM (rumänisch) sowie HIT FM, Radio Chanson, Русское Радио (Russkoje Radio) (russisch).

Jeweils am 14. Oktober feiern die Einwohner Chișinăus den Geburtstag der Stadt mit einem großen Umzug und diversen kleinen Ständen und Attraktionen im autofreien Stadtzentrum.





In Chișinău gibt es mehrere Fußballklubs, die in der Divizia Națională spielen; darunter sind CSF Zimbru Chișinău, FC Dacia Chișinău, FC Unisport-Auto Chișinău und CS Steaua Chișinău. Zu den größeren Fußballstadien in Chișinău gehören das "Stadionul Dinamo" (Dinamo-Stadion) mit 2.692 Plätzen sowie das am 20. Mai 2006 eröffnete und nach dem gleichnamigen Fußballclub benannte "Stadionul Zimbru" (Zimbru-Stadion), das Platz für rd. 10.500 Zuschauer bietet (parallel dazu wurde das "Stadionul Republicii" [Stadion der Republik; 8.000 Sitzplätze] abgerissen).

National bedeutend ist der Verein HC Olimpus-85-USEFS, der auf internationaler Ebene beispielsweise am Europapokal der Pokalsieger teilnahm.

Zu den Söhnen und Töchtern der Stadt Chișinău gehören u. a. der sowjetische Astronom Wladimir Albizki (1891–1952), der russische Arzt und Zionist Jacob Bernstein-Kohan (1859–1929), der US-amerikanische Filmproduzent Samuel Bronston (1908–1994), die moldauische Opernsängerin Maria Cebotari (1910–1949), der russische Pianist und Komponist Julius Isserlis (1888–1968), die moldauisch-österreichische Geigerin Patricia Kopatchinskaja (* 1977), der israelische Außenminister Avigdor Lieberman (* 1958), der ukrainische Tennisspieler Denys Moltschanow (* 1987), der russische Politiker Wladimir Purischkewitsch (1870–1920), der deutsche Historiker Georg Sacke (1902–1945) und der US-amerikanische Bananenunternehmer Sam Zemurray (1877–1961).

Personen mit Bezug zu Chișinău





</doc>
<doc id="14339" url="https://de.wikipedia.org/wiki?curid=14339" title="Tuba">
Tuba

Die Tuba ( für „Rohr“, „Röhre“), Mehrzahl Tuben oder Tubas, ist das tiefste aller gängigen Blechblasinstrumente. Sie besitzt drei bis sechs Ventile und zählt infolge ihrer weiten Mensur und der entsprechend stark konisch verlaufenden Bohrung zur Familie der Bügelhörner.

Tuba war im Römischen Reich die Bezeichnung für ein Blasinstrument aus Messing oder Bronze. Es besaß die Form einer geraden, langgestreckten Röhre mit schmalem Schallbecher, ähnlich einer Fanfare, und entsprach wahrscheinlich der griechischen Salpinx.
Heute versteht man unter einer Tuba das Bassinstrument der Familie der Bügelhörner. Die ersten Tuben wurden in Berlin um 1835 kurz nach der Erfindung der Ventiltechnik entwickelt. Wilhelm Wieprecht und Carl Wilhelm Moritz erhielten in diesem Jahr ein Patent auf eine Basstuba in F mit fünf Ventilen.

Dank ihres vorteilhafteren Klangs und der präziseren Intonation ersetzte dieses Instrument in Orchestern schon bald ihre Vorläufer, den Serpent beziehungsweise das Basshorn und die Ophikleide.

Im Orchester werden Basstuba und Kontrabasstuba gleichermaßen verwendet. Die Tuba ist im Orchester meistens einfach, seltener auch zweifach besetzt. Ob eine Partie mit der Bass- oder Kontrabasstuba gespielt wird, entscheidet meist der Tubist selbst anhand von Gesichtspunkten wie etwa der geforderten Tonlage, Lautstärke oder Klangfarbe, dem Wunsch des Dirigenten, der Raumakustik und ähnlichem. Einige Komponisten wie etwa Richard Wagner schreiben die Bauform jedoch vor. Ein Orchestertubist muss im deutschsprachigen Raum beide Bauformen virtuos beherrschen. In den skandinavischen Ländern, in England, Frankreich und weiten Teilen Amerikas sowie in Australien wird hauptsächlich die Kontrabasstuba in C verwendet. Die Alternative zur Basstuba in F bildet in diesen Ländern eine Es-Tuba.

Im Blasorchester sowohl volkstümlicher als auch sinfonischer Prägung (Concert Band) werden meist mindestens zwei Tuben besetzt. Wenn diese nicht unisono spielen, befinden sie sich in der Regel in der Oktave zueinander.

Im Jazz nahm die Tuba bis ca. 1925 die Bassrolle wahr, bevor sie vom Kontrabass abgelöst wurde. Nach dem Zweiten Weltkrieg feierte sie im Amateur-Dixieland ein Comeback.

Das Hauptmerkmal der Tuba ist die starke Erweiterung der Bohrung (weite Mensur) in einem Verhältnis von bis zu 1:20 vom Mundstück bis zum Schallstück des Instrumentes.

Im Sitzen ruht die Tuba auf den Oberschenkeln des Bläsers, bei kleineren Tubisten mit besonders großen Instrumenten auch auf dem Stuhl selbst. Zum Spiel im Stand ist ein spezieller Ständer, ein Schultergurt oder ein Tubagürtel ("Tubabelt") erforderlich. Der Trichter weist in der Regel nach oben und meist leicht nach links bei Ausführung mit Drehventilen, bzw. nach rechts bei Ausführung mit Périnet-Ventilen (vom Spieler aus gesehen).

Die Finger der rechten Hand liegen auf den ersten drei bis fünf Ventilen. Mit der linken Hand wird das Instrument gestützt und je nach Konstruktion werden bis zu drei weitere Ventile, die der besseren Intonation dienen, gedrückt.

Die Tuba wird mit einem Kesselmundstück gespielt.

Die Naturtöne der B-Tuba sind:

Es-Tuba:

F-Tuba:

Die in Klammern genannten Töne sind nur bedingt verwendbar, da sie deutlich von den entsprechenden gleichstufig gestimmten Tönen abweichen.

Der Grundton oder tiefste Naturton wird auch als „Pedalton“ bezeichnet. Dieser ist nur deshalb auf der Tuba intonierbar, da sie eine weite Mensur aufweist. In der Praxis benötigt der Spieler sehr viel Luft und seine Atemstütze, um die stehende Welle des Grundtons in der Tuba anzuregen.

Die Tuba besitzt einen nutzbaren Tonumfang von mehr als vier Oktaven.

Die Notationsweise ist im internationalen Vergleich nicht einheitlich: In Deutschland, Italien und England wird klingend (also nicht transponierend) im Bassschlüssel notiert. In Frankreich, Belgien und den Niederlanden wird transponierend im Bassschlüssel notiert (Basstuba in F klingt eine Quinte tiefer als notiert). In der Schweiz wird transponierend im Violinschlüssel notiert (Basstuba in F klingt eine Oktave plus Quinte tiefer als notiert).


Kurioses:



Im Jahr 1979 rief der amerikanische Musiker Joel Day den "International Tuba Day" aus, der seit dem Jahr 1982 jährlich am ersten Freitag im Mai begangen wird. Am Welt-Tuba-Tag finden Konzerte, Vorträge und Ausstellungen rund um das Instrument statt.

Laut Joel Day findet der Tubist nicht das Maß an Respekt und Anerkennung, das ihm zustehen würde. Die Tuba würde oftmals als unwichtig abgetan und als „imposantes“ Anhängsel angesehen.





</doc>
<doc id="14344" url="https://de.wikipedia.org/wiki?curid=14344" title="Plastik (Kunst)">
Plastik (Kunst)

Eine Plastik ist ein dreidimensionales, körperhaftes Objekt der bildenden Kunst. Die Begriffe "Plastik" und "Skulptur" sind weitgehend deckungsgleich. Beide Bezeichnungen sind sowohl auf ein einzelnes Kunstwerk anwendbar als auch auf die Bildhauerkunst insgesamt als Kunstgattung.

Die ursprünglich differenzierte Bedeutung – eine Plastik entsteht durch Auftragen von Material und Modellieren, eine Skulptur dagegen durch Hauen und Schnitzen – ist heute nur noch selten im Sprachgebrauch anzutreffen.

Das Wort "Plastik" für ein Kunstwerk aus geformtem Material wurde im 18. Jahrhundert aus der französischen Sprache ins Deutsche entlehnt. Das französische Substantiv "plastique" ist eine Substantivierung des Adjektivs "plastique" „formbar“. Das Adjektiv geht seinerseits auf lateinisch "[ars] plastica" „formende/geformte [Kunst]“ zurück und dies wiederum auf gleichbedeutend πλαστική [τέχνη] "plastikē [téchnē]" im Griechischen.

Verwandte Wörter im Griechischen sind πλάστης "plástēs" („Bildhauer“, eigentlich „Former“) und das Verb πλάσσειν "plássein" („formen“). Etymologisch verwandte Wörter im Deutschen sind "Plasma" und "Pflaster".

Manche Autoren möchten im Sinn der ursprünglichen Wortbedeutung unterscheiden: Eine "Plastik" (oder eine Plastik im engeren Sinne) entstehe durch „Antragen“ von weichem Material und einen Aufbau von innen nach außen; dagegen entstehe eine "Skulptur" durch Abschlagen und Wegschneiden von Stein oder Holz. Zumindest bei einzelnen Kunstwerken ist es möglich, wenn auch nicht allgemein üblich, diese feine Unterscheidung im Sprachgebrauch nachzuvollziehen. Im Deutschen Wörterbuch der Brüder Grimm wird unter "Plastik" die engere Bedeutung erwähnt: „die bildende kunst, welche die organischen formen selbst körperlich (durch formen, schnitzen, meiszeln, gieszen) hinstellt, im engern sinne die form-, modellierkunst“.

Der differenzierte Wortgebrauch ist heute sowohl in der Fachsprache wie in der Umgangssprache die Ausnahme. Arbeitsschritte wie die Bearbeitung einer Gussform oder die Feinarbeit an einem Gipsmodell entziehen dieser Einteilung zudem die Allgemeingültigkeit. Die Materialmöglichkeiten und Gestaltungstechniken haben sich gerade in der Moderne so sehr erweitert, dass eine Unterscheidung der Begriffe anhand der Art der Formgebung nicht mehr für eine ganze Kunstgattung möglich ist.

In der Klassischen Archäologie wird üblicherweise für alle Bildwerke der Ausdruck "Plastik" bevorzugt.

Der Begriff "Plastik" wird heute auch außerhalb der bildenden Kunst verwendet, insbesondere in der plastischen Chirurgie, gelegentlich auch für Objekte im Bereich Technik.

"Kleinplastik" ist ein gebräuchlicher Oberbegriff für Bildwerke im „Vitrinenformat“, also zum Beispiel Arbeiten aus Elfenbein. Ein Beispiel für eine "Großplastik" ist Gerhart Schreiters "Memento maris" in Bremerhaven, eine moderne Plastik aus Beton und Stahl.

"Freiplastiken" sind rundum ansichtig, darin unterscheiden sie sich von einem Relief. Zu den häufigen Formen zählen die Statue und die Büste.

Nach dem Material lassen sich unter anderem unterscheiden: Kunstwerke aus Stein (siehe Steinbildhauer), aus Holz (siehe Schnitzen) und aus Metall (siehe Metallplastik und Bronzebildwerk sowie Kunstguss).

Es gibt einen fließenden Übergang zu körperhaften Objekten in neueren Kunstgattungen wie etwa Installationen oder Werken der Land Art.





</doc>
<doc id="14345" url="https://de.wikipedia.org/wiki?curid=14345" title="Kunststoff">
Kunststoff

Als Kunststoffe (auch Plaste, selten Technopolymere, umgangssprachlich Plastik) bezeichnet man Werkstoffe, die hauptsächlich aus Makromolekülen bestehen.

Wichtige Merkmale von Kunststoffen sind ihre technischen Eigenschaften, wie Formbarkeit, Härte, Elastizität, Bruchfestigkeit, Temperatur-, Wärmeformbeständigkeit und chemische Beständigkeit, die sich durch die Wahl der Makromoleküle, Herstellungsverfahren und in der Regel durch Beimischung von Additiven in weiten Grenzen variieren lassen. Kunststoffe werden bezüglich ihrer physikalischen Eigenschaften in drei großen Gruppen unterteilt: Thermoplaste, Duroplaste und Elastomere.

Kunststoffe werden zu Formteilen, Halbzeugen, Fasern oder Folien weiterverarbeitet. Sie dienen als Verpackungsmaterialien, Textilfasern, Wärmedämmung, Rohre, Bodenbeläge, Bestandteile von Lacken, Klebstoffen und Kosmetika, in der Elektrotechnik als Material für Isolierungen, Leiterplatten, Gehäuse, im Fahrzeugbau als Material für Reifen, Polsterungen, Armaturenbretter, Benzintanks und vieles mehr. In Wirtschaftsstatistiken werden Chemiefasern, sowie Kunstharze in Lack- und Klebstoffen oft von anderen Kunststoffen getrennt ausgewiesen.

Die jeweiligen Makromoleküle eines Kunststoffes sind Polymere und daher aus wiederholenden Grundeinheiten aufgebaut. Die Größe der Makromoleküle eines Polymers variiert zwischen einigen tausend bis über eine Million Grundeinheiten. Beispielsweise besteht das Polymer Polypropylen (Kurzzeichen PP) aus sich vielfach wiederholenden Propyleneinheiten. Die Polymere können unverzweigte, verzweigte oder vernetzte Moleküle sein.

Die Polymere können aus Naturstoffen gewonnen oder rein synthetisch sein. Synthetische Polymere werden durch Kettenpolymerisation, Polyaddition oder Polykondensation aus Monomeren oder Prepolymeren erzeugt. Halbsynthetische Kunststoffe entstehen durch die Modifikation natürlicher Polymere (vorwiegend Zellulose zu Zelluloid), während andere biobasierte Kunststoffe wie Polymilchsäure oder Polyhydroxybuttersäure durch die Fermentation von Zucker oder Stärke hergestellt werden.

Zwischen 1950 und 2015 wurden weltweit rund 8,3 Mrd. Tonnen Kunststoff hergestellt – macht gut 1 Tonne pro Kopf der Weltbevölkerung. Die Hälfte hiervon stammt aus den letzten 13 Jahren. Von dieser Menge wurden ca. 6,3 Mrd. Tonnen zu Abfall, der zu 9 % recycelt, zu 12 % verbrannt und zu 79 % auf Müllhalden deponiert bzw. sich in der Umwelt anreichert.

Biopolymere und natürlich vorkommende Polymere werden von Menschen schon seit Urzeiten verwendet. Alle Tiere und Pflanzen enthalten in ihren Zellen Polymere. Holz diente dem Menschen zunächst als Brennholz und Werkzeug, etwa als Wurfholz, Speer und als Baumaterial. Der Zellverband Tierhaut oder Fell wurde durch Gerben stabilisiert, damit vor dem raschen Verwesen geschützt und so zu haltbarem Leder. Aus Wolle, abgeschnittenen Tierhaaren, stellte man durch Verspinnen und Weben oder durch Filzen Bekleidung und Decken her.

Birken lieferten den ersten Kunststoff der Menschheitsgeschichte, das aus Birkenrinde durch Trockendestillation gewonnene Birkenpech, das sowohl Neandertalern als auch dem steinzeitlichen Homo sapiens als Klebstoff bei der Herstellung von Werkzeugen diente.

In Mesopotamien wurden Wasserbecken und Kanäle mit natürlichem Asphalt abgedichtet. Ebenso wurden dort bestimmte Baumharze als Gummi Arabicum eingesetzt und nach Europa exportiert. Aus Europa ist Bernstein als fossiles Harz für die Verwendung bei Pfeilspitzen und Schmuckgegenständen bekannt. Im Mittelalter wurde Tierhorn durch bestimmte Verfahrensschritte in einen plastisch verformbaren Stoff verwandelt.
Bereits um 1530 wurde im Hause der Fugger nach einem Rezept des bayerischen Benediktinermönches Wolfgang Seidel transparentes Kunsthorn aus Ziegenkäse gefertigt und vertrieben.

Im 17. und 18. Jahrhundert brachten Naturforscher aus milchigen Baumsäften gewonnene, elastische Massen (Kautschuk) aus Malaysia und Brasilien mit. Für diese wurde in Deutschland der Begriff "Gummi" eingeführt. Seit Mitte des 19. Jahrhunderts entwickelte sich eine rasch wachsende Gummi-Industrie.

Der Erfinder Charles Goodyear stellte 1839 fest, dass sich Kautschuk bei Hitzeeinwirkung durch Zusatz von Schwefel in Gummi umwandelt. Dieser Prozess wird Vulkanisation genannt. Charles Goodyear fertigte aus dem neuen Material zunächst Gummihandschuhe. Um 1850 entdeckte er außerdem Hartgummi, ein durch Erhitzen in Gegenwart von Schwefel erhärteter Naturkautschuk, der anfangs als Ebonit vermarktet wurde. Daraus wurden zum Beispiel Schmuckstücke, Füllfederhalter, Klaviertasten, Tabakpfeifen und Teile von Telefonen hergestellt. Dieser erste Duroplast startete die Entwicklung der Kunststoffe als Werkstoff im Umfeld des Menschen.
Die Entwicklung des Zelluloids ist mehreren Chemikern zu verdanken. Christian Friedrich Schönbein entwickelte 1846 die Schießbaumwolle, indem er Baumwolle mit Salpetersäure versetzte. Der Engländer Maynard löste Schießbaumwolle in einem Ethanol-Äther-Gemisch und erhielt nach Verdampfung elastische Häutchen (Kollodium). Der Engländer Cuttin verknetete das Kollodium mit alkoholischer Campherlösung zu Zelluloid.
Im Jahr 1869 nutzte John Wesley Hyatt das Zelluloid als Kunststoff und entwickelte drei Jahre später die erste Spritzgussmaschine. Später wurde in England das Zellulosenitrat zur Imprägnierung von Textilien und in den USA Schellack entwickelt.

Linoleum wurde 1844 von Frederic Walton erfunden. Es wurde aus Leinöl, Sikkativen und Harzen durch Lufteinblasung gewonnen. Anwendungsbereiche waren Fußbodenbeläge, Wandbekleidungen, Tischflächen.

Max Fremery und Johann Urban lösten mit einer ammoniakalischen Kupferhydroxidlösung Zellulose auf. Mit dieser Lösung (Cupro) konnten leicht Kupfer-Reyon-Fäden als erste Viskosefaser hergestellt werden.
Adolf von Baeyer beschrieb 1872 die Polykondensation von Phenol und Formaldehyd. Der belgische Chemiker Leo Hendrik Baekeland untersuchte die Wirkung von Säure und Alkali bei dieser Reaktion und entwickelte ein Verfahren (1907; seit 1909 technische Produktion) zur Herstellung und Weiterverarbeitung eines Phenolharzes. Dieser von ihm Bakelit getaufte Kunststoff war der erste in großen Mengen industriell hergestellte, synthetische Duroplast. Dank seiner Eignung als elektrischer Isolator wurde er unter anderem in der aufstrebenden Elektroindustrie eingesetzt.

Wilhelm Krische und Adolf Spittler entwickelten 1885 das Galalith (Kunsthorn). Der Kunststoff ähnelt stark dem tierischen Horn oder Elfenbein. Das Kunsthorn wird aus Kasein und Formaldehydlösung hergestellt. Man fertigte daraus zum Beispiel Knöpfe, Anstecknadeln, Gehäuse für Radios, Zigarettendosen, Spielzeuge, Griffe für Regenschirme und vieles mehr in den verschiedensten Farben.

Der deutsche Chemiker Fritz Hofmann meldete 1909 ein Patent auf den synthetischen Kautschuk Buna an. Die ersten vollsynthetischen Reifen aus Isoprenkautschuk wurden 1912 hergestellt.

Der Berliner Apotheker Eduard Simon beschrieb im Jahr 1839 das Polystyrol. Das Styrol verwandelte sich zunächst in eine gallertartige Masse. Im Jahr 1909 untersuchte H. Stobbe die Polymerisationsreaktion von Styrol detailliert. Erst zwanzig Jahre später wurde diese Entdeckung genutzt.

Im Jahr 1835 entdeckte Victor Regnault das Vinylchlorid, aus dem sich Polyvinylchlorid (PVC) herstellen ließ. Die erste Patentierung von PVC und von Polymeren aus Vinylacetat geht auf Fritz Klatte im Jahr 1912 zurück. Als weltweiter Pionier der Kunststoffverarbeitung gilt aber Coroplast, das sich als eines der ersten Unternehmen mit der Verarbeitung des PVC beschäftigte. Erst 1950 wurde dieses Verfahren durch Verbesserungen von Dow Chemical abgelöst.

Schon 1901 befasste sich Otto Röhm mit der Herstellung von Acrylsäure und Acrylsäureestern. Aber erst im Jahr 1928 fand er die für die Polymerisation besser geeigneten Methacrylsäuremethylester. Das Patent für Polymethylmethacrylat (PMMA, Handelsname „Plexiglas“) startete eine Ära.

Bis Ende des 19. Jahrhunderts war wenig über die genauen Strukturen polymerer Materialien bekannt. Man wusste lediglich aus Dampfdruck- und Osmosemessungen, dass es sich um sehr große Moleküle mit hoher Molmasse handeln müsste. Fälschlicherweise war man jedoch der Meinung, dass es sich um kolloidale Strukturen handelte.

Als Vater der Polymerchemie gilt der deutsche Chemiker Hermann Staudinger. Bereits 1917 äußerte er vor der Schweizerischen Chemischen Gesellschaft, dass „hochmolekulare Verbindungen“ aus kovalent gebundenen, langkettigen Molekülen bestehen. 1920 veröffentlichte er in den "Berichten der Deutschen Chemischen Gesellschaft" einen Artikel, der als Begründung der modernen Polymerwissenschaften gilt. Vor allem in den Jahren von 1924 bis 1928 folgten weitere wichtige Theorien über den Aufbau von Kunststoffen, die die Grundlage für das heutige Verständnis dieser Werkstoffklasse bilden. Für diese Arbeiten erhielt Staudinger 1953 den Nobelpreis.

Die Arbeiten Staudingers ermöglichten der chemischen Industrie nun, basierend auf gesicherten naturwissenschaftlichen Grundlagen, eine rasante Entwicklung auf dem Gebiet der Polymerchemie.

Der Münchner Chemiker Ernst Richard Escales gab 1910 der Werkstoffgruppe den Namen „Kunststoffe“. Die von ihm gegründete gleichnamige Zeitschrift erschien erstmals 1911.

Bei dem Unternehmen Imperial Chemical Industries (ICI) in Großbritannien wurde unter hohem Druck (200 bar) und bei hohen Temperaturen im Jahre 1933 erstmals Polyethylen hergestellt. Erst zwanzig Jahre später entwickelte Karl Ziegler ein Verfahren, das mit Katalysatoren aus Aluminiumalkylen und Titantetrachlorid die Polymerisation von Ethen zu Polyethylen schon bei Raumtemperatur erlaubt. Das Niederdruck-Polyethylen erwies sich als wärmestabiler und mechanisch belastbarer. Kurz darauf fanden Ziegler und Giulio Natta einen Katalysator zur Polymerisation von Propen zu Polypropylen. 1955–1957 liefen die großtechnischen Synthesen von Polyethylen und Polypropylen an. Heute sind die so hergestellten Polyethylene (PE) und Polypropylen (PP) neben Polystyrol (PS) die am häufigsten als Verpackungsmaterialien von Lebensmitteln, Kosmetika etc. verwendeten Kunststoffe. Ziegler und Natta erhielten im Jahre 1963 für ihre Arbeiten den Nobelpreis für Chemie.

Kunststoffe aus Polyestern wurden schon sehr früh angedacht (Berzelius, 1847). 1901 gab es Glyptalharze (aus Glycerin und Phthalsäure). Fritz Hofmann, Wallace Hume Carothers und Paul Schlack suchten erfolglos nach synthetischen Fasern auf Basis von Polyestern. Erst den Briten Whinfield und Dickson gelang bei Calico Printers im Jahre 1941 die Herstellung von brauchbaren Polyesterfasern (Polyethylenterephthalat, PET). Wichtige Polyesterfasern wurden Dacron (DuPont), Diolen (ENKA-Glanzstoff), Terylen (ICI), Trevira (Hoechst).

In Ludwigshafen begann 1934 die Herstellung von Epoxidharzen nach einem Verfahren von Paul Schlack. 1935 wurde gleichzeitig von Henkel (Mainkur) und Ciba (Schweiz) die Entwicklung von Melaminharz beschrieben.

Im Jahr 1931 meldete der US-Chemiker Wallace Hume Carothers bei DuPont ein Patent für ein Polyamid aus Hexamethylendiamin und Adipinsäure an. Erst sieben Jahre später war die neue Kunstfaser Nylon (1938) verkaufsfähig. Das von Paul Schlack 1937 hergestellte Polyamid 6 auf Basis von Caprolactam wurde Perlon getauft. Die großtechnische Herstellung begann 1939 bei den IG-Farben. Das Herstellungsverfahren von Perlon in Deutschland war preiswerter als die Nylonproduktion in den USA.

Etwa zeitgleich begannen die Buna-Werke der I.G. Farben mit der Fertigung von Buna S und Buna N als synthetischem Gummi-Ersatz.
1939 entwickelte Otto Bayer das Polyurethan (PU) in Leverkusen.

Bei DuPont wurde 1938 der Kunststoff Polytetrafluorethylen (Teflon) von R.J. Plunkett entwickelt. Das Produkt zeigte hohe Temperaturbeständigkeit und eine hohe chemische Beständigkeit. Die Verarbeitung stieß jedoch auf Probleme. Erst 1946 ging Teflon in die Großproduktion.

Silikon hatte im Jahr 1901 bereits Frederic Stanley Kipping aus Silanonen hergestellt. Erst durch die Synthese von Organosiliciumhalogeniden mit Alkylhalogeniden gelang es 1944 in den USA und Deutschland, Silikon günstig herzustellen (Eugene G. Rochow, Richard Müller).

Seit Anfang der 1930er Jahre war die Polymerisation von Acrylnitril bekannt. Es war als Kunststoff jedoch so nicht brauchbar. Der Chemiker Rein konnte Polyacrylnitril in Dimethylformamid lösen und so für die Kunststoffproduktion brauchbar machen. 1942 wurde bei den IG Farben ein Polymerisationsverfahren zu Polyacrylnitril entwickelt. 1942 entdeckte Harry Coover (USA) bei Eastman Kodak den „Sekundenkleber“ Methylcyanacrylat.

Vor allem nach 1950 nahm aufgrund der zahlreichen Erfolge auf dem Gebiet der Polymerchemie die Produktion von Kunststoffen enorm zu. Durch die Entwicklung der Thermoplaste und insbesondere von entsprechenden Verarbeitungsverfahren konnten Formteile jetzt auf unschlagbar billige Weise hergestellt werden. Kunststoff wurde von einem Ersatzstoff mit besonderer Bedeutung zu einem Werkstoff für die industrielle Massenfertigung. In der Folge ging der Anteil der Duroplaste stetig zurück und lag im Jahre 2000 nur noch bei 15 %. Der Pro-Kopf-Verbrauch an Kunststoffen lag im Jahr 2000 bei 92 kg in Westeuropa, 13 kg in Osteuropa, 130 kg in Nordamerika, 19 kg in Lateinamerika, 86 kg in Japan, 13 kg in Südostasien und 8 kg im Mittleren Osten / Afrika.

Die Kunststoffindustrie ist bis heute eine Wachstumsbranche, wobei die Herstellungskapazitäten in Asien zwischen 2006 und 2008 die führenden und etwa gleich starken Regionen Europa sowie Nord- und Südamerika überholten.

Je nach Blickwinkel des Betrachters und Anforderung können Kunststoffe verschiedenartig eingeteilt werden. Gängig sind Einteilungen nach mechanisch-thermischem Verhalten (häufigste Einteilung), Ursprung (natürlich oder synthetisch), Verwendung oder Entstehungsreaktion. Eine strenge Abgrenzung einzelner Kunststoffe ist oft nicht möglich, diese Einteilungen bieten allerdings eine gute Übersicht.

Die Einteilung nach mechanisch-thermischem Verhalten erfolgt in Thermoplaste, Duroplaste und Elastomere. Außerdem existieren mit deutlich untergeordneter Bedeutung thermoplastische Elastomere und reversible Duroplaste. Diese Einteilung ist anwendungstechnischer Herkunft. Die unterschiedlichen Polymerklassen unterscheiden sich in ihren mechanischen Eigenschaften aufgrund der unterschiedlichen Vernetzung und dem jeweiligen Verhältnis zwischen Gebrauchstemperatur (meist Raumtemperatur) und physikalischer Übergangstemperatur (Glasübergangstemperatur und Schmelzpunkt).

Thermoplaste sind Kunststoffe, die aus langen linearen Molekülen bestehen. Durch Energiezufuhr werden diese Materialien beliebig oft weich und formbar (plastisch) und schmelzen schließlich. Sie können durch verschiedene Ur- und Umformverfahren in die gewünschte Form gebracht werden. Nachdem das Werkstück abgekühlt ist, behält es seine Form bei. Dieser Prozess ist somit reversibel (lat. umkehrbar). Ursache für dieses Verhalten sind fadenförmige, lineare Makromoleküle.

Die meisten der heute verwendeten Kunststoffe fallen unter diese Gruppe (Polyethylen, Polypropylen, Polystyrol, Polyester). Für einfache Konsumwaren, Verpackungen etc. werden sie ebenso häufig eingesetzt wie für technische Teile in der Automobil- und Elektroindustrie oder in der Bauindustrie, insbesondere für Dachbahnen, Fensterprofile und Rohre.

Um neue, bisher noch nicht vorhandene Eigenschaften zu erzeugen, können auch zwei oder mehrere (miteinander verträgliche) Thermoplaste vermischt werden (Polyblend).

Teilkristalline Thermoplaste (Beispiele): POM – Polyoxymethylen, PE – Polyethylen, PP – Polypropylen, PA – Polyamid, PET – Polyethylenterephthalat, PBT – Polybutylenterephthalat.

Amorphe Thermoplaste (Beispiele): ABS – Acrylnitril-Butadien-Styrol, PMMA – Polymethylmethacrylat, PS – Polystyrol, PVC – Polyvinylchlorid, PC – Polycarbonat, SAN – Styrol-Acrylnitril-Copolymer, PPE – Polyphenylenether.

Duroplaste (Duromere) sind Polymere, die in einem Härtungsprozess aus einer Schmelze oder Lösung der Komponenten durch eine Vernetzungsreaktion hervorgehen. Diese irreversible Reaktion wird meist durch Erhitzen bewirkt (daher auch der englische Fachterminus "thermosets"), kann aber auch durch Oxidationsmittel, energiereiche Strahlung oder Einsatz von Katalysatoren initiiert und beschleunigt werden. Eine Erwärmung von Duroplasten führt nicht zu einer plastischen Verformbarkeit, sondern lediglich zu deren Zersetzung. Ausgehärtete Duroplaste sind meist hart und spröde sowie im weitergehenden Fertigungsprozess nur noch mechanisch bearbeitbar. Ursache für dieses Verhalten sind die raumvernetzten Makromoleküle.

Wegen ihrer mechanischen und chemischen Beständigkeit auch bei erhöhten Temperaturen werden sie häufig für Elektroinstallationen verwendet. Der verbreitetste und älteste Kunststofftyp dieser Klasse sind die Phenoplaste. In diese Gruppe fallen auch Polyesterharze, Polyurethanharze für Lacke und Oberflächenbeschichtungen und praktisch alle Kunstharze wie beispielsweise Epoxidharze.

Durch Druck oder Dehnung können Elastomere ihre Form kurzzeitig verändern, nach Beendigung von Druck oder Dehnung nimmt das Elastomer schnell wieder seine ursprüngliche Form an. Die Elastomere sind weitmaschig vernetzt und daher flexibel. Sie werden beim Erwärmen nicht weich und sind in den meisten Lösemitteln nicht löslich.

Zu den Elastomeren gehören alle Arten von vernetztem Kautschuk. Die Vernetzung erfolgt beispielsweise durch Vulkanisation mit Schwefel, mittels Peroxiden, Metalloxiden oder Bestrahlung. Elastomere werden zu 60 % für Reifen verwendet. Der Rest verteilt sich auf sonstige Gummiartikel, zum Beispiel Chemikalienhandschuhe und Hygieneartikel.

Elastomere sind Naturkautschuk (NR), Acrylnitril-Butadien-Kautschuk (NBR), Styrol-Butadien-Kautschuk (SBR), Chloropren-Kautschuk (CR), Butadien-Kautschuk (BR) und Ethylen-Propylen-Dien-Kautschuk (EPDM).

Unter chemischen Gesichtspunkten können Kunststoffe als makromolekulare Stoffe mit anderen makromolekularen Stoffen verglichen werden. Die verschiedenen makromolekularen Stoffe können dann nach Ursprung eingeteilt werden in:

Nur ein Teil der aufgeführten makromolekularen Stoffe sind Kunststoffe im engeren Sinn, da Kunststoffe als Stoffe definiert sind, die auf Polymeren basieren und außerdem als Werkstoffe bei der Verarbeitung „plastische“ Zustande durchlaufen. Trotzdem kann diese Einordnung zum Verständnis beitragen.

Je nach Preis, Produktionsvolumen und Verwendungsmöglichkeit können Thermoplaste in die vier Anwendungsklassen eingeteilt werden: Standardkunststoffe, technische Kunststoffe, Funktionskunststoffe und Hochleistungskunststoffe. Standardkunststoffe (auch: Massenkunststoffe) sind sehr vielseitig einsetzbar und werden in großen Mengen hergestellt. Standardkunststoffe werden häufig als Verpackungsmaterial verwendet, zu ihnen gehören beispielsweise Polyethen oder Polyvinylchlorid. Technische Kunststoffe verfügen über bessere mechanische Eigenschaften als Standardkunststoffe und behalten diese auch noch oberhalb von 100 °C und unterhalb von 0 °C. Technische Kunststoffe werden häufig für technische Konstruktionen verwendet, zu ihnen zählen beispielsweise Polyethylenterephthalat und einige aliphatische Polyamide. Funktionskunststoffe dienen nur einer einzigen Funktion, wie beispielsweise als Barriere für Aromen und Gase in Kunststoffverpackungen. Duroplaste können nicht nach diesem Schema eingeordnet werden, sondern bilden eine eigene Klasse.

Hochleistungskunststoffe zeichnen sich gegenüber Standard-, technischen und Spezialkunststoffen durch ihre Wärmeformbeständigkeit und z. T. auch gute mechanische Eigenschaften aus. Während die Wärmeformbeständigkeit von Standardkunststoffen meist nur etwa 100 °C beträgt und die von technischen Kunststoffen bis zu 150 °C erreicht, können Hochleistungsthermoplaste auch Temperaturen von bis zu 300 °C standhalten. Hochleistungskunststoffe sind mit etwa 20 € pro kg recht teuer; ihr Marktanteil beträgt nur etwa 1 %.

Der Vergleich von Standardkunststoffen, technischen Kunststoffen und Hochleistungskunststoffen wird durch die folgende Abbildung veranschaulicht:

Kunststoffe werden durch verschiedene Polyreaktionen erzeugt: Polymerisation, Polykondensation und Polyaddition. Entsprechend wird das Produkt entweder als Polymerisat, als Polykondensat oder als Polyaddukt bezeichnet.

Einzelne Kunststoffe werden nach einem weltweit standardisierten Kurzzeichen-System bezeichnet, das für Deutschland in der DIN EN ISO 1043 Teil 1:2016-09:"Basis-Polymere und ihre besonderen Eigenschaften", der DIN ISO 1629:2015-03: "Kautschuk und Latices - Nomenklatur (ISO 1629:2013)" sowie DIN EN ISO 18064:2015-03: "Thermoplastische Elastomere - Nomenklatur und Kurzzeichen (ISO 18064:2014; Deutsche Fassung EN ISO 18064:2014)" geregelt ist.

Kunststoffe zeichnen sich, verglichen mit keramischen oder metallischen Werkstoffen, durch eine Reihe von ungewöhnlichen Eigenschaften aus:
Die Dichte der meisten Kunststoffe liegt zwischen 0,8 und 2,2 g·cm. Sie sind damit erheblich leichter als metallische (um 8 g·cm) oder keramische Werkstoffe (etwa 6 g·cm).

In Bezug auf die mechanischen Eigenschaften sind Kunststoffe anderen Werkstoffklassen häufig unterlegen. Ihre Festigkeit und Steifigkeit erreicht meist nicht die von Metallen oder Keramiken. Wegen der geringen Dichte kann dies jedoch teilweise mit konstruktiven Mitteln (höhere Wandstärken) oder dem Einsatz von faserverstärkten Kunststoffen kompensiert werden.

Obwohl die Festigkeiten vergleichsweise niedrig sind, brechen Kunststoffteile weniger leicht als beispielsweise Keramik oder Glas durch ihre zumeist gute Zähigkeit. Deshalb werden Gebrauchsgegenstände für Kinder und Spielzeug vielfach aus Kunststoff gefertigt.

Viele Kunststoffe sind im Gegensatz zu Metallen aufgrund ihrer organischen Natur beständig gegenüber anorganischen Medien. Dies schließt Mineralsäuren, Laugen, sowie wässrige Salzlösungen ein. Daher bevorzugt man Werkstoffe aus Kunststoff zur Herstellung von pflegeleichten Haus- und Elektrogeräten, Fahrzeugausstattungen, Spielzeugen usw.

Im Gegensatz zu Metallen reagieren sie allerdings empfindlich auf organische Lösungsmittel, wie Alkohole, Aceton oder Benzin. Dennoch gelang es auch auf diesem Gebiet, beständige Kunststoffe zu entwickeln. Ein Beispiel ist der Kraftstofftank aus Polyethylen in modernen Personenkraftwagen. Er ist überaus beständig gegenüber Korrosion und trotzdem unempfindlich gegenüber dem Benzin.

Degradation bezeichnet bei Kunststoffen deren Abbau oder Zerfall, dabei spricht man häufiger von "„Alterung“" und daraus resultieren meist Quellung, Versprödung, Rissbildung und Festigkeits­verlust. Die Degradation ist üblicherweise ein unerwünschter Vorgang und erfolgt entweder chemisch, physikalisch oder durch eine Kombination beider Abbauarten.

Die gängigen Verarbeitungstemperaturen für Kunststoffe liegen im Bereich von 250 bis 300 °C. Während Metalle bei hohen Temperaturen aufwendig gegossen werden müssen und Einschränkungen bezüglich der Gussformen bestehen, lassen sich aus Thermoplasten auch kompliziertere Formteile mit vergleichsweise geringem Aufwand fertigen (siehe Extrusion und Spritzguss). Gleichzeitig können in einem Verarbeitungsschritt Additive, wie Farbpigmente oder Fasern, in das Material eingearbeitet werden, die sich bei den hohen Temperaturen des Metallgießens oder des Sinterns von Keramik zersetzen würden.

Die Wärmeleitfähigkeit von Kunststoffen ist nur einen Bruchteil so groß wie die von Metallen. Da aus diesem Grund bei einer Berührung vergleichsweise wenig Wärmeenergie von der Hand übertragen wird (Kunststoffe sich also bei niedrigen Temperaturen dennoch warm anfühlen), werden Griffe an Werkzeugen oder Geländern gerne aus Kunststoff hergestellt oder damit überzogen.

Werkstoffe wie Schäume, Vliese und Flocken isolieren vor allem durch den Gehalt an (räumlich fixierter) Luft. Kunststoffe als Matrixmaterial fördern die Isolierwirkung; wie etwa in Dämmstoffplatten, Textilien oder Matratzen. Die leichte Brennbarkeit ist hingegen ein klarer Nachteil gegenüber mineralischer Glas- oder Steinwolle, Schaf- und Baumwolle, Kork, aber auch Massivholz.

Die elektrische Leitfähigkeit von Kunststoffen ist um 15 Größenordnungen kleiner als die von Metallen. Daher werden Kunststoffe zur Isolation eingesetzt. Metallisiert werden Kunststofffolien als Dielektrikum eingesetzt und zu Kondensatoren zusammengerollt. Den hohen Oberflächenwiderstand, der mit Reibung über Kontaktelektrizität zu elektrostatischer Aufladung führt, bricht man mit Füllstoffen (so in Schuhsohlen) oder Antistatika etwa in Möbelpolitur oder Textilwaschmittel.

Kunststoffe werden generell durch schrittweises Aneinanderfügen von Monomeren zu langen Ketten – den Polymeren – hergestellt, wobei grundsätzlich zwischen "Kettenpolymerisation" und "Stufenpolymerisation" unterschieden wird.

Bei einer Kettenpolymerisation beginnt das Wachstum mit einem Molekül, an das sukzessive weitere Monomere addiert werden. Das die Polymerisation startende Molekül nennt man "Initiator", das auf diesen aufwachsende heißt "Monomer". Die Zahl der Monomere, aus denen das Polymer letztendlich besteht, ist der Polymerisationsgrad. Der Polymerisationsgrad kann durch das Verhältnis von Monomer zu Initiator eingestellt werden. Mathematisch wird er durch die Mayo-Gleichung abgeschätzt.

Bei der radikalischen Polymerisation werden die Wachstumsreaktionen durch Radikale initiiert und fortgepflanzt. Sie ist verglichen mit anderen Kettenreaktionen unempfindlich, leicht zu kontrollieren und liefert schon bei recht kleinen Umsätzen hohe Polymerisationsgrade. Sie wird daher vor allem bei der Herstellung von billigen Kunststoffen, wie LD-PE, PS oder PVC, eingesetzt.

Eine Gefahr bei diesem Verfahren stellt die freiwerdende "Polymerisationswärme" dar. Die radikalische Polymerisation ist exotherm, das heißt bei der Reaktion wird Wärme freigesetzt. Diese Wärme erzeugt, wenn sie nicht abgeführt wird, weitere Radikale, so dass sich die Reaktion selbst beschleunigen kann. Im Extremfall kann eine solche „Selbstbeschleunigung“ zur Überlastung des Reaktormaterials und damit zu einer "thermischen Explosion" führen.

Bei ionischen Polymerisationen werden die Wachstumsreaktionen durch ionische Spezies initiiert und fortgepflanzt. Die wachsenden Ketten sind langlebiger (mehrere Stunden bis Tage) als ihre radikalischen Analoga (Lebensdauer etwa 10 s), man spricht in diesem Zusammenhang auch von sogenannten "lebenden Polymeren". Daher kann man nach Abschluss einer Polymerisation auf die noch "lebenden", das heißt zur Polymerisation befähigten Ketten, ein weiteres Monomer aufgeben und so ein erneutes Wachstum fortführen.

Polymere, deren Ketten aus zwei oder mehr unterschiedlichen Monomertypen bestehen, nennt man "Copolymere". Findet man in einem Copolymeren lange Blöcke des einen Monomers, gefolgt von Blöcken des anderen, spricht man von "Blockcopolymeren". Für eben solche speziellen Anwendungen wird die ionische Polymerisation angewandt. Ein Beispiel sind die synthetischen Gummis Acrylnitril-Butadien-Kautschuk (NBR) und Styrol-Butadien-Kautschuk (SBR), die bei der Herstellung von Autoreifen Verwendung finden. Nachteil dieses Verfahrens ist seine hohe Empfindlichkeit gegenüber Verunreinigungen, Wasser und Sauerstoff. Ionische Polymerisationen sind daher aufwendiger und kostenintensiver als die radikalische Polymerisation.

Diese Polymerisationen finden in Gegenwart von Katalysatoren statt. Beim Katalysator handelt es sich um einen Metallkomplex (Verbindung aus Metallatomen, umgeben von weiteren Spezies), der in der Lage ist, die wachsende Kette zu binden. Die Addition weiterer Monomere geschieht durch Einschub "(Insertion)" des Monomers zwischen wachsende Kette und Katalysatorspezies. Resultat ist ein höherer Ordnungsgrad der entstehenden Polymere sowie ein geringerer Verzweigungsgrad. Aufgrund dieser reguläreren Struktur erfolgt auch die Packung der einzelnen Ketten im Festkörper effizienter, der Kunststoff wird dichter. Die zurzeit industriell wichtigste Katalysatorklasse ist die der Ziegler-Natta-Katalysatoren. Eine Rolle spielen sie zum Beispiel bei der Herstellung von Polyethylen.

Beim Low-Density-Polyethylen (LD-PE) handelt es sich um in der Gasphase polymerisiertes Ethen mit geringem Ordnungsgrad, vielen Seitenverzweigungen und geringer Dichte. Diesen Kunststoff findet man vor allem als transparente oder gefärbte Verpackungsfolie von Getränkeflaschen, Büchern, CDs etc.

High-Density-Polyethylen wird mit einem metallorganischen Katalysator im Ziegler-Natta-Verfahren hergestellt. Es resultiert ein Polymer mit hohem Ordnungsgrad, wenigen Verzweigungen und hoher Dichte. Dieser Kunststoff findet beispielsweise Verwendung als Material für Autotanks, Benzinkanister etc.
Im Gegensatz zur Kettenpolymerisationen erfolgt in Stufenpolymerisationen die Bildung der Polymere nicht durch Initiation einer wachsenden Kette, die weiter sukzessive Monomere addiert, sondern durch direkte Reaktion der Monomere untereinander. Diese Reaktion kann unter Freisetzung eines Nebenprodukts wie Wasser als Polykondensation oder durch einfache Addition der Monomere zu einer neuen Verbindung durch Polyaddition erfolgen.

Bei Polykondensationen erfolgt die Bildung der linearen Kette durch intermolekulare Reaktion bifunktioneller Polymere unter Abspaltung einer kleineren Spezies, wie beispielsweise Wasser oder Alkohole. Eine wesentliche Bedeutung besitzt die Polykondensation für die Polyamide.

Carbonsäuren reagieren mit Aminen zu Amiden. Setzt man Moleküle ein, die zwei Carbonsäuregruppen tragen, kann eines dieser Moleküle mit zwei Aminen reagieren. Es entsteht so ein Polymer aus drei Monomeren (eine Carbonsäureeinheit, zwei Amine). Tragen die eingesetzten Amine auch wieder zwei Amingruppen, kann die zuvor entstandene Spezies wiederum mit zwei Carbonsäuremolekülen reagieren usw. Die so entstehenden Polymere können sich dann auch noch weiter untereinander verbinden, so dass der Polymerisationsgrad entscheidend von der Reaktionsdauer abhängt. Der Vorgang wird durch die Carothers-Gleichung beschrieben.

Durch Reaktion von Dicarbonsäuren mit Diolen (Dialkohol) werden so Polyester hergestellt. Unter den wichtigsten durch Polykondensation hergestellten Kunststoffen sind Polyester, wie Polyethylenterephthalat (PET), Polyamide und Phenoplaste. Maleinsäure- und Phthalsäurepolyester werden industriell ausgehend von deren Anhydriden hergestellt.

Bei Polyadditionen erfolgt die Bildung des Polymers durch Addition der einzelnen Monomere untereinander, ohne die Bildung von Nebenprodukten. Eine große Gruppe von Polyaddukten bilden die Polyurethane.

Isocyanate reagieren mit Alkoholen in einer Additionsreaktion zu sogenannten Urethanen. Auch hier gilt: setzt man bifunktionelle Monomere ein, erfolgt die Bildung langer linearer Ketten. Auf diese Weise hergestelltes Polyurethan wird für Armaturenbretter, Lacke, Klebstoffe etc. verwendet.
Setzt man der Polymerisationsmischung Wasser zu, reagiert dieses mit den Isocyanaten zu Harnstoffen und Kohlenstoffdioxid. Das in der Mischung freiwerdende CO wird in Form von Bläschen in den Kunststoff eingeschlossen, so dass man einen Schaumstoff erhält. Polyurethanschaumstoff wird für Matratzen, Sitzmöbel, Schwämme etc. verwendet.

Kunststoffen werden im Verlauf des Herstellungsprozesses sogenannte Additive zugesetzt "(Compoundierung)". Sie dienen der genauen Einstellung der Materialeigenschaften auf die Bedürfnisse der jeweiligen Anwendung und der Verbesserung der chemischen, elektrischen und mechanischen Eigenschaften. Solche mit Zuschlagsstoffen versehene Formmassen werden nach DIN EN ISO 1043 (Thermoplaste) und nach DIN 7708 (Duroplaste) gekennzeichnet.

Etwa zwei Drittel der weltweit hergestellten Additive werden für die Produktion von Polyvinylchlorid aufgewendet, fast drei Fünftel der hergestellten Additive sind "Weichmacher". Sie verringern Sprödigkeit, Härte und Glastemperatur eines Kunststoffes und machen ihn so besser form- und verarbeitbar. Es handelt sich um Stoffe, die in der Lage sind, auf molekularer Ebene in den Kunststoff einzudringen und so die Beweglichkeit der Ketten gegeneinander zu erhöhen. Qualitativ kann man sie als „molekulares Schmiermittel“ verstehen. Bis vor wenigen Jahren war Diethylhexylphthalat (DEHP) (synonym: Dioctylphthalat DOP) der am häufigsten verwendete Weichmacher. Dieser stellte sich jedoch als umwelt- und gesundheitsschädlich heraus, weshalb die europäische Industrie inzwischen weitgehend auf seinen Einsatz verzichten will. Als Ersatz für DEHP kommt oftmals das im Jahre 2002 eingeführte 1,2-Cyclohexandicarbonsäurediisononylester (DINCH) zum Einsatz. Weitere neue Weichmacher sind die analogen Adipinsäureester wie Diethylhexyladipat.

"Extender" verbessern ebenfalls die Verarbeitbarkeit, man spricht deshalb auch von "sekundären Weichmachern". Wichtige Extender sind epoxidierte Öle, hochsiedende Mineralöle und Paraffine.

"Stabilisatoren" dienen der Verbesserung der chemischen Eigenschaften. Sie erhöhen die Lebensdauer des Kunststoffes und schützen ihn vor schädigenden Einflüssen (Oxidation, Strahlung und Wärme etwa durch Feuer) in seinem Einsatzgebiet.

Durch Reaktion mit Luftsauerstoff kann sich der Kunststoff verfärben, und die Polymerketten können sich zersetzen oder neu vernetzen. Dies verhindert man durch Zugabe von "Antioxidantien", die die bei der Reaktion entstehenden freien Radikale abfangen "(Radikalkettenabbrecher)", oder gleich die Bildung der Radikale verhindern "(Desaktivatoren)".
Als Abbrecher setzt man beispielsweise Phenole oder Amine zu, als Desaktivatoren dienen Phosphane und ebenfalls Amine.

"Lichtschutzmittel" schützen gegen eine Schädigung durch ultraviolettes Licht. Doppelbindungen zwischen Kohlenstoffatomen sind in der Lage, Licht dieser Wellenlänge zu absorbieren, daher sind vor allem Kunststoffe durch UV-Licht gefährdet, die dieses Strukturelement aufweisen (z. B. Polyisopren). Allerdings können aufgrund von Katalysatorrückständen, Strukturfehlern und Nebenreaktionen bei der Verarbeitung praktisch alle Polymere ein Absorptionsvermögen für UV-Strahlung zeigen. Diese induziert die Bildung von freien Radikalen im Material, die Nebenreaktionen, wie Zerfall der Kette und Vernetzungen einleiten. Es existieren grundsätzlich drei Wege eine Schädigung zu verhindern: Reflexion des Lichts, Zusatz von lichtabsorbierenden Stoffen und Zusatz von Radikalfängern. Wichtige Lichtschutzmittel sind Ruß, der die Strahlung absorbiert, σ-Hydroxybenzophenon, das die Energie in Infrarotstrahlung umwandelt und Dialkyldithiocarbamate, die UV-Licht absorbieren und als Radikalfänger fungieren.

Kunststoffe sind empfindlich gegenüber Wärmeeinwirkung. Oberhalb einer für das Material charakteristischen Temperatur "(Zersetzungstemperatur)" setzt der Zerfall der molekularen Struktur ein. "Wärmestabilisatoren" sollen dies verhindern.
Unerlässlich sind diese für Polyvinylchlorid, das sonst, unter Bildung von Chlorwasserstoff und u. U. gesundheitsschädlicher Zerfallprodukte, seine mechanische Stabilität einbüßen würde. Der Zerfallmechanismus verläuft über die Bildung von Doppelbindungen. Organische Barium-, Zink-, Zinn-, und Cadmiumverbindungen und anorganische Bleisalze komplexieren diese und unterbrechen so den Zerfallmechanismus. Vor allem die Bleiverbindungen stellen hinsichtlich der Entsorgung des Kunststoffs ein nicht unerhebliches Umweltproblem dar. Derzeit sind 80 % der Wärmestabilisatoren auf der Basis von Blei. Die chemische Industrie ist zurzeit allerdings bemüht, diese zu ersetzen. So wurde bei Cognis speziell für Fensterprofile ein Stabilisator auf der Basis von Calcium und Zink entwickelt.

Bei Bränden geht von Kunststoffen eine große Gefahr aus, da sie zum einen in der Lage sind die Brände zu unterhalten und zum anderen bei einer unkontrollierten Verbrennung giftige oder ätzende Gase, wie Blausäure, Kohlenstoffmonoxid, Chlorwasserstoff und Dioxine frei werden. "Flammschutzmittel" verhindern entweder den Sauerstoffzutritt zum Brand oder stören die chemischen Reaktionen (Radikalkettenmechanismen) der Verbrennung. Polycarbonate erfordern oft keine Flammschutzmittel, da als Löschmittel wirkendes Kohlendioxid ein Zerfallsprodukt des Polymers darstellt.

Wichtige Flammschutzmittel sind

Die meisten Polymere sind in reiner Form farblos, farbig werden sie erst durch Zusatz von "Farbmitteln". Man unterscheidet zwischen "Farbstoffen" (lösen sich auf molekularer Ebene im Polymer oder adsorbieren an der Oberfläche) und "Pigmenten" (unlösliche, meist organische / anorganische Aggregate). Textilien färbt man praktisch ausschließlich mit Farbstoffen ein. Der weit überwiegende Teil der Kunststoffe wird allerdings mit Pigmenten gefärbt, da diese lichtechter und meist auch billiger sind. Wichtige Pigmente in diesem Bereich sind Rutil (weiß), Ruß (schwarz), Cobalt- oder Ultramarinblau, sowie Chromoxidgrün. Inzwischen ist auch der Einsatz von Effektpigmenten möglich, so zeigen mit seltenen Erden dotierte Strontium-Aluminate ein intensives Nachtleuchten. Einsatzgebiete für derartig gefärbte Kunststoffe sind bei Dunkelheit leichter auffindbare Sicherheitsmarkierungen, Lichtschalter oder Taschenlampen. Um Metallglanz zu erreichen werden Aluminiumpigmente in Blättchenform eingesetzt, sphärische Pigmentkörner ergeben eine Graueinfärbung. In der Kunststoffverarbeitung werden zum Einfärben meist konzentrierte Pigmentpräparationen sogenannte Flüssigfarben oder Masterbatches eingesetzt.

"Füllstoffe" sind klassische Streckmittel, die so die Herstellung des Kunststoffs verbilligen. „Aktive Füllstoffe“ verbessern zusätzlich die mechanischen Eigenschaften des Materials. Wichtige Füllstoffe sind unter anderem: Kreide, Sand, Kieselgur, Glasfasern und -kugeln, Zinkoxid, Quarz, Holzmehl, Stärke, Graphit, Ruße und Talkum.
Wichtig sind Füllstoffe auch um das Brandverhalten der Kunststoffe zu minimieren.

Unter Verstärkungsstoffen (reinforcement) versteht man in Kunststoffen eingesetzte Zusatzstoffe, die die Kunststoffmatrix verstärken sollen. Folge ist die Verbesserung mechanischer und physikalischer Eigenschaften, wie Elastizität oder Biegefestigkeit. Beispiele sind Glasfasern, Kohlenstofffasern oder auch Flachs und Jute.

Die Beschichtung mit Metallen wird Kunststoffmetallisierung genannt. Einsatz findet es in Bereichen in denen Kunststoff zum Ersatz von Metallen verwendet wird, aber das hochwertigere Aussehen von Metallglanz beibehalten werden soll. In der Automobilindustrie werden galvanisierte Kunststoffelemente in der Außenverkleidung eingesetzt. In Elektrogeräten erlaubt der metallisierte Kunststoff eine Abschirmung. Im Sanitärbereich werden Elemente für Mischbatterien, Dusch-Köpfe und Wasserhahngriffe verwendet.

Die kunststofferzeugende Industrie ist ein wichtiger Zweig der chemischen Industrie. 2006 erzielten in diesem Bereich in Deutschland 3570 Unternehmen mit rund 372.900 Beschäftigten einen Gesamtumsatz von 79,4 Milliarden Euro. Tätig sind sie in den sich teilweise überschneidenden Teilgebieten
Die Kunststofferzeugung erfolgt zu großen Teilen bei global agierenden Chemiekonzernen wie beispielsweise Basell, BASF, Bayer, Celanese/Ticona, DowDuPont, DSM, und Solvay. Sie liefern ein begrenztes Sortiment an Kunststoffen in Mengen von teilweise mehreren 100 kt pro Jahr. Die Preise für Kunststoffe variieren sehr stark von einigen Eurocent pro Kilogramm für Massenkunststoffe bis hin zu einigen hundert Euro pro Kilogramm für Hochleistungspolymere.

Die Kunststoffverarbeitung ist Gegenstand eines eigenständigen Industriezweiges. Dabei kommen überwiegend Urformverfahren zum Einsatz, die im Gegensatz zu den metallischen Werkstoffen bei wesentlich geringeren Verarbeitungstemperaturen (bis 430 °C) ablaufen. Dadurch können die Fertigungseinrichtungen (sog. "Werkzeuge") mehrfach verwendet werden und erlauben so eine kostengünstige Fertigung.

Es kommt eine Vielzahl von Verfahren zum Einsatz, die teilweise ihren Ursprung in der wesentlich älteren Metallbearbeitung haben und auf die Eigenschaften der Kunststoffe abgestimmt und weiterentwickelt wurden. So ist beispielsweise das Spritzgießen für Kunststoffe dem Druckguss für Metalle sehr ähnlich. Das Extrudieren oder Blasformen ist aus der Glasproduktion hervorgegangen.

Die Schäumverfahren haben wiederum ihren Ursprung bei den Kunststoffen, werden aber, wie Metallschaum, inzwischen auch für andere Werkstoffklassen verwendet. Sie lassen sich weiter in chemische, physikalische oder mechanische Treibverfahren untergliedern.

Für alle diese Verfahren werden spezielle Maschinen und Werkzeuge benötigt, die der Kunststoffmaschinenbau zur Verfügung stellt.

Etwa 90 % der weltweiten Produktion (jährlich etwa 150 Mio. t.) entfallen in der Reihenfolge ihres Anteils auf die folgenden sechs Kunststoffe:

Polyethylen wird hauptsächlich in drei unterschiedlichen Qualitäten hergestellt: HD-PE "(High-Density-PE)", LLD-PE "(Linear-Low-Density-PE)", LD-PE "(Low-Density-PE)".
HD-PE wird mittels Ziegler-Natta-Katalysatoren synthetisiert, seine Ketten zeigen einen sehr hohen Ordnungs- und niedrigen Verzweigungsgrad. Diese können sich daher im Festkörper effizient anordnen, so dass ein teilkristallines Material entsteht, dessen Dichte höher ist als die von LD-PE (beide weisen aber eine Dichte auf, die geringer ist als die von Wasser). Es wird zur Fertigung von Flaschen, Getränkekästen, Fässern, Batteriegehäusen, Eimern, Schüsseln etc. verwendet.
LD-PE wird unter hohem Druck in der Gasphase polymerisiert, in LLD-PE werden 1-Buten, 1-Hexen und 1-Octen einpolymerisiert, um so einen kontrollierten Verzweigungsgrad zu erzeugen. Beide Varianten weisen so einen geringen kristallinen Anteil und einen hohen oder mittleren Verzweigungsgrad auf. Das Material besitzt hervorragende filmbildende Eigenschaften und wird vor allem zur Herstellung von Verpackungsfolien für Zigarettenpäckchen, CDs, Bücher, Papiertaschentücher etc. sowie Tragetaschen verwendet.

Polypropylen wird fast ausschließlich auf metallkatalytischem Wege hergestellt, da nur das so erhaltene kristalline Material kommerziell verwertbare Eigenschaften aufweist. Es handelt sich um einen sehr harten, festen und mechanisch belastbaren Kunststoff mit der geringsten Dichte aller Massenkunststoffe. Aufgrund dieser Eigenschaften hat es teilweise bereits Metallwerkstoffe verdrängt. Wie bei dem rechts abgebildeten Deckel zeigt es außerdem den sogenannten "Filmscharniereffekt", d. h., es kann durch einen dünnen Film Gehäuse und Deckel miteinander verbinden, ohne aufgrund der Biegebelastung zu brechen. Ein erheblicher Teil des weltweit hergestellten Polypropylens wird für Lebensmittelverpackungen aufgewendet, weitere Anwendungsgebiete sind:
Polyvinylchlorid galt aufgrund des ungewöhnlich hohen Chloranteils, und der damit bei der Verbrennung entstehenden Nebenprodukte wie Chlorgas und Chlorwasserstoff (Salzsäure), lange Zeit als umweltschädlichster Kunststoff. Zudem ist das zur Herstellung benötigte Vinylchlorid krebserregend. Inzwischen führt man jedoch den Chloranteil auch als positiven Aspekt an (Einsparung von Rohöl). Man unterscheidet generell zwischen Hart-Polyvinylchlorid und durch Zusatz von Weichmachern hergestelltes Weich-Polyvinylchlorid.
Hart-PVC ist ein amorpher Thermoplast und besitzt eine hohe Steifigkeit und Härte. Es ist extrem schwer entflammbar, kann in der Hitze eines bestehenden Brandes allerdings Chlorwasserstoff und Dioxine freisetzen. Es zeigt eine sehr gute Beständigkeit gegen Säuren, Basen, Fette, Alkohole und Öle. Aus diesem Grund wird es auch vor allem zur Herstellung von Abwasserrohren und Fensterprofilen eingesetzt. Gravierende Nachteile sind seine sehr geringe Wärmebeständigkeit, es kann dauerhaft nur bis 65 °C und kurzfristig bis 75 °C eingesetzt werden, und seine Neigung zum „Weißbruch“ beim Biegen.
Weich-PVC ist ein gummielastischer, lederähnlicher Thermoplast. Wichtige Anwendungen sind die Herstellung von Bodenbelägen, Dichtungen, Schläuchen, Kunstleder, Tapeten, Dachbahnen, Wood-Plastic-Composite-Produkte etc.

Polystyrol wird überwiegend als amorpher Thermoplast hergestellt, durch neuere Entwicklungen gibt es aber mittlerweile auch kristallines Polystyrol, dieses hat aber geringere Bedeutung. Beide Varianten zeichnen sich durch geringe Feuchtigkeitsaufnahme, gute Verarbeitbarkeit und sehr gute elektrische Eigenschaften aus. Sie unterscheiden sich in ihrer Schlagfestigkeit. Nachteile sind seine Neigung zur Spannungsrissbildung, die geringe Wärmebeständigkeit, Entflammbarkeit und seine Empfindlichkeit gegenüber organischen Lösungsmitteln. Mittels Kohlenstoffdioxid bei der Polymerisation aufgeschäumtes Polystyrol wird unter anderem als "Styropor" vertrieben.

Anwendungsgebiete:

Die Eigenschaften von Polyurethanen können durch Wahl der Isocyanat- oder Urethan-haltigen Monomerkomponenten sehr stark in ihrer Elastizität variiert werden. So werden sehr elastische PUR-Textil-Fasern "(Elastan)" aus Polyestern und Urethan-haltigen Polyestern hergestellt, ebenso dienen Urethan-haltige Polymere als Zusatz in Lacken und Materialien für Leiterplatten "(Bectron)".

Polyethylenterephthalat ist ein Polyester aus Terephthalsäure und Ethylenglycol, bei der Herstellung werden stöchiometrische Mengen eingesetzt und die Veresterung bis zu einem Umsatz von 99 % durchgeführt. Die erstarrte Schmelze kristallisiert sehr langsam, so dass man auch hier je nach Anwendungsbereich amorphes und teil-kristallines "(C-PET)" Material herstellen kann. C-PET besitzt hohe Steifigkeit, Härte, Abriebfestigkeit und ist beständig gegen verdünnte Säuren, Öle, Fette und Alkohole. PET-Flaschen sind jedoch empfindlich gegenüber heißem Wasser.

Anwendungsbeispiele:

Amorphes PET zeigt eine geringere Steifigkeit und Härte als C-PET, aber bessere Schlagzähigkeit. Da es transparent, aber leichter als Glas ist, wird es als Material für Getränkeflaschen und Verpackungen für Lebensmittel und Kosmetika verwendet. In der Elektrotechnik finden PET-Folien als Trägermaterial für Magnetbänder Verwendung.

Manche Kunststoffe werden in großen Mengen für Massenartikel hergestellt. Andere hingegen werden nur in geringen Mengen eingesetzt, da ihr Preis hoch ist oder sie nur in Spezialanwendungen nützlich sind. Solche Kunststoffe werden als Sonderkunststoffe bezeichnet (englisch: specialty polymers oder auch special purpose plastics). Manche Sonderkunststoffe werden mit der Zeit gebräuchlicher und nehmen eine Rolle als technische Kunststoffe ein, andere bleiben Spezialanwendungen vorbehalten.

Beispiele für Sonderkunststoffe sind Hochleistungsthermoplaste (auch Hochtemperaturkunststoffe genannt), Elektroaktive Polymere, Polymer Electrolytes, Flüssigkristallpolymere, Ionic Polymers, Polymer Nanokomposite und weitere. Im Folgenden werden einige Sonderkunststoffe sowie einige speziellere Anwendungen vorgestellt.

Thermoplastische Kunststoffe, die eine Dauergebrauchstemperatur von über 150 °C aufweisen, werden als Hochtemperaturkunststoffe bezeichnet. Da Kunststoffe dieser Art auch besondere mechanische Eigenschaften und eine besondere Resistenz gegenüber Chemikalien aufweisen, werden sie auch als Hochleistungskunststoffe bezeichnet. Hochleistungskunststoffe sind verhältnismäßig teuer und werden nur in geringen Mengen produziert.

Aufgrund ihrer guten mechanischen Eigenschaften und einer im Vergleich geringen Dichte werden Hochleistungskunststoffe häufig als Ersatz für Metalle verwendet. Auch ihre Chemikalienresistenz kann ein Einsatzgrund sein. Sie finden daher u. a. Anwendung in der Luft- und Raumfahrt (z. B. Turbinen), in der Automobilindustrie (z. B. an heißen Stellen im Motorraum) oder in der chemischen Industrie (in Kontakt mit aggressiven Chemikalien).

Als "flüssigkristalline Polymere" (engl. "liquid crystalline polymers" (LCP)) bezeichnet man Polymere, deren Ketten in der Schmelze flüssigkristalline Phasen bilden.
In Kristallen liegt generell eine feste Ordnung vor, während in Flüssigkeiten und Schmelzen die Verteilung der Moleküle oder Atome in der Regel weitgehend zufällig ist.
Insoweit ist der Ausdruck "flüssigkristallin" eigentlich ein Widerspruch.
In LCPs orientieren sich die Polymerketten jedoch aufgrund intramolekularer Wechselwirkungen parallel zu Bündeln an. So bilden beispielsweise aromatische Polyamide in Schwefelsäure in Verbindung mit Calcium- oder Lithiumchlorid derartige Phasen.
Presst man eine derartige Lösung aus einer Spinndüse durch einen Zwischenraum mit Luft in ein Fällbad "(Dry-Jet-Wet-Spinnverfahren)", erhält man Fasern, in denen die Ketten in Richtung der Längsachse orientiert sind.
Derartige Fasern sind in der Lage, eine für Kunststoffe ungewöhnlich hohe Zugbelastung auszuhalten, die vergleichbar mit Metallen oder Kohlenstofffasern ist. Aufgrund ihrer geringen Dichte setzt man sie, eingebettet in Kunstharze "(Composites)" im Flugzeug- und Fahrzeugbau ein. Weitere Anwendungen sind schusssichere Westen, Schutzhelme, Schutzanzüge, Surfbretter, Segelbootbau etc. Wichtige Marken sind: Kevlar, Nomex und Faser B.

Kunststoffe gelten im Allgemeinen als hervorragende Isolatoren. Das liegt daran, dass Polymeren die Grundvoraussetzung für elektrische Leitfähigkeit, quasi freie Elektronen, völlig fehlt. Durch Zugabe von Substanzen "(Dotierung)", die entweder der Kette Elektronen zuführen "(Reduktion)" oder durch Entfernung "(Oxidation)" freie Stellen für die Elektronenbewegung schaffen, ist es möglich elektrisch leitfähige Polymere zu erzeugen. So werden beispielsweise Polyacetylen und Poly("p"-phenylen) elektrisch leitend, wenn man sie mit Brom, Iod oder Perchlorsäure dotiert. Weitere wichtige elektrisch leitende Polymere sind Polyanilin, dotiert mit Salzsäure und Polypyrrol aus anodischer Oxidation. Anwendungen sind Materialien für Elektroden und Batterieelemente, sowie antistatische Beschichtungen. Durch geeignete Dotierung können den bisher genannten Polymeren auch halbleitende Eigenschaften verliehen werden. Aus solchen Materialien bestehen beispielsweise Polymer-Leuchtdioden. Für die Entwicklung leitfähiger Polymere wurde den Wissenschaftlern Alan J. Heeger, Alan G. MacDiarmid und Hideki Shirakawa im Jahr 2000 der Nobelpreis für Chemie verliehen.

Kunststoffe erfüllen in der Medizin vielfältige Aufgaben: Sie dienen als Behälter für Infusionslösungen, Bauteile von medizinischen Geräten, Wegwerfartikel (Spritzen, Pflaster, Katheter, Schläuche etc.) und Implantate (Herzklappen, Knochenersatz, Gelenkpfannen, resorbierbare Knochenschrauben etc.). Für Materialien, die auf direkte oder indirekte Weise im Kontakt mit lebendem Gewebe stehen, gelten naturgemäß besondere Auflagen: Zum einen darf der Kunststoff den Organismus nicht schädigen, zum anderen darf umgekehrt das biologische Milieu die Materialeigenschaften des Kunststoffs nicht beeinträchtigen. Sind diese Bedingungen erfüllt, spricht man von Biokompatibilität. Wichtigstes Argument für den Einsatz von Kunststoffen in der Medizin war und ist die Hygiene, so konnten medizinische Instrumente aus Glas oder Metall durch Wegwerfartikel aus Kunststoff ersetzt werden. Ein bemerkenswertes Beispiel ist Polymilchsäure (auch: "Polylactid"), ein Polyester der natürlich vorkommenden Milchsäure. Er wird zu Fasern gesponnen, die als resorbierbare chirurgische Nähfäden Verwendung finden. Nach dem Einsatz der Fäden werden diese enzymatisch abgebaut. Die Dauer der Degradation kann dabei über die Stereochemie (Wahl der Ketten aus rechts- oder linksdrehender Milchsäure) des Polymers eingestellt werden.

"Siehe auch Abschnitt „Umweltproblematik“ in „Plastiktüte“; außerdem Friendly Floatees, Mikroplastik sowie Plastikmüll in den Ozeanen"

Aus der Produktion von Kunststoffen ergibt sich zwangsläufig das Problem der Entsorgung der aus ihnen erzeugten Produkte („Plastikmüll“): Die polymeren Bestandteile der Kunststoffe sind zum einen nicht wasserlöslich und zum anderen nicht in der Lage, die Zellmembranen von Mikroorganismen zu passieren, das heißt, eine Wechselwirkung mit "lebenden" Organismen ist außer bei den biologisch abbaubaren Kunststoffen und bei der Entstehung von Mikroplastik nicht bekannt. Dies hat zwar den Vorteil, dass Polymere als gesundheitlich unbedenklich eingestuft werden können, aber eine Umwandlung in der belebten Natur eben auch nicht völlig ausgeschlossen werden kann.

Die von Kunststoffen verursachten Umweltprobleme werden u. a. in den Dokumentarfilmen "Plastic Planet" (2009) des österreichischen Regisseurs Werner Boote, "Plastik über alles" (OV: "Addicted to plastic") (2008) des kanadischen Regisseurs Ian Connacher sowie "Midway" (2009–2013) des US-Regisseurs Chris Jordan gezeigt.

In diversen Kunststoffen enthaltene Bestandteile werden als hormonell wirksam eingestuft, einzelne davon werden unter anderem über die menschliche Nahrung aufgenommen:

Bisphenole wie Bisphenol A (BPA), C (BPC) oder S (BPS) werden als Härtemittel z. B. in Beschichtungen von Konservendosen oder in Kunststoffvorrats- oder sonstigen Behältnissen eingesetzt: sie lösen sich vor allem in säurehaltigen Stoffen wie Tomaten- oder Fruchtflüssigkeiten, schneller noch unter Hitzeeinfluss. Statistiken zufolge haben 95 bis 98 % der Menschen BPA in ihrem Urin, wobei die Halbwertszeit des Abbaus im Körper bei ca. einem halben bis einem Tag liegt.

Phthalate werden als Weichmacher vor Allem z. B. in Kosmetika, aber auch in Lebensmittelfolien eingesetzt: sie sind ebenfalls hormonell wirksam.

Gelangen biologisch nicht abbaubare Kunststoffe in die Umwelt, werden sie zu einer Gefahr: Von den weltweit jährlich produzierten mehr als 200 Millionen Tonnen Kunststoffen gelangen nach unterschiedlichen Schätzungen sechs bis 26 Millionen Tonnen in die Meere, 70 Prozent davon sinken auf den Meeresboden. Aus Europa und Nordamerika stammen zusammen weniger als 5 % des Eintrags. Mehrere Millionen Tonnen Kunststoffmüll treiben in sogenannten Müllstrudeln im Nordpazifik und im Nordatlantik. Jedes Jahr tötet dieser Müll mehrere hunderttausend höhere Meerestiere. Kleine Plastikteile und Mikroplastik gelangt in die Nahrungskette von Meerestieren und führen dazu, dass Tiere mit vollem Magen verhungern oder innere Verletzungen erleiden. Oft verwechseln Tiere Plastikteile mit ihrer Nahrung und verschlucken sie. Größere Plastikteile wie Planen, defekte Fischernetze oder Taue verletzen Meerestiere. Plastikplanen bedecken Korallenstöcke, Schwämme oder Muschelbänke und verhindern so deren Besiedlung. Nach einer Studie der UNEP befinden sich in dem Strudel im Pazifik bis zu 18.000 Kunststoffteile auf jedem Quadratkilometer Meeresfläche. Auf ein Kilogramm Plankton kommen hier sechs Kilogramm Kunststoff. Die Größen der Strudel lassen sich kaum angeben, da sie nicht scharf begrenzt sind.
Kunststoffe galten lange als biologisch nicht abbaubar, erst in jüngerer Zeit wurden einige Organismen gefunden, die Kunststoffe abbauen können (siehe Abschnitt Biologischer Abbau herkömmlicher Kunststoffe). Chemische und physikalische Prozesse benötigen für Kunststoff-Abbau sehr lange, da auf solchen Wegen Zerfallszeiten von mehrere hundert Jahren errechnet wurden, werden Kunststoffe auch als persistent bezeichnet. Eine Möglichkeit für anorganischen Abbau ist die Einwirkung von UV-Strahlung (Sonnenlicht), dabei "zerbrechen" die Kunststoffketten stückweise, dies äußert sich makroskopisch in vergilben und/oder verspröden. 

Mikroorganismen können Kunststoffe im Grunde nur durch extrazelluläre Enzyme verarbeiten, die das Material in kleinere Bestandteile zerlegen, die dann von der Zelle aufgenommen werden können. Allerdings sind die Enzyme zu groß, um effektiv in das verrottende Material einzudringen, so dass dieser Prozess nur als Oberflächenerosion ablaufen kann. 

Wenn bei dem Abbau durch biochemischen Prozesse giftige Zwischenstufen entstehen, können diese sich in der Natur anreichern. Zusätzliche Gefahr geht von den Additiven der Kunststoffe wie Weichmachern, Farbstoffen oder Flammschutzmitteln aus. Dabei sind als Schadstoffquellen insbesondere flüchtige organische Verbindungen zu nennen.

Von den ca. 6,3 Mrd. Tonnen Kunststoff, die bis 2015 zu Abfall wurden, wurden ca. 9 % recycelt und 12 % verbrannt. Etwa 79 % der Kunststoffe wurden auf Müllhalden deponiert bzw. wurden in der Umwelt ausgebracht, wo sie sich nun anreichern. In der Schweiz wird etwa 90 % des Plastikmülls energetisch verwertet.

Im Jahr 2012 betrug die weltweite Recyclingquote für Kunststoffabfälle nur etwa 3 Prozent bei einer jährlichen globalen Jahresproduktion an Kunststoffen von rund 280 Millionen Tonnen. Ein Großteil der anfallenden Kunststoffabfälle wird auf Müllkippen deponiert oder verbrannt und geschätzte 20 Millionen Tonnen des nicht-recycelten Plastikmülls landen schließlich in den Ozeanen, wo er ein enormes Umweltproblem darstellt. Dagegen werden in Deutschland und der Schweiz keine Kunststoffe mehr deponiert. In der EU soll dieses Ziel bis zum Jahr 2020 erreicht werden. In der Bundesrepublik lag die Recyclingquote im Jahr 2010 bei 45 Prozent, womit Deutschland Vorreiter im europäischen Vergleich ist. Die restlichen 55 % werden thermisch verwertet (Müllverbrennung). Die Kunststoffindustrie hat zur Unterstützung dieses Vorhabens eine Kampagne "Zero Plastics to Landfill by 2020" gestartet. Inzwischen gibt es auch Industrieunternehmen, die sich auf das Recycling von Plastik spezialisiert haben.

Grundsätzlich lassen sich drei Möglichkeiten der Weiterverwertung erschließen:

Thermoplaste lassen sich, einmal zu einem Werkstück geformt, wieder einschmelzen und zu einem neuen Produkt formen. Die Abfolge von Wärmebehandlungen führt allerdings bei vielen Verfahren zu einem fortschreitenden Qualitätsverlust des Materials "(Downcycling)". Größtes Problem bei einer erneuten werkstofflichen Verwertung ist allerdings die Trennung der einzelnen Kunststoffe. Mischt man verschiedene Polymere in einem Material, führt dies in der Regel zu einem starken Qualitätsverlust und wesentlich schlechteren mechanischen Eigenschaften. Um die Trennung zu erleichtern, führte man 1988 den Recycling-Code ein. Die Wiederverwertung nicht sortenreiner Abfälle, wie beispielsweise Hausmüll gestaltet sich aber dennoch sehr schwierig. Die gängigen Trennverfahren sind sehr personalintensiv und erfordern einen hohen Einsatz an Wasser und Energie, so dass hier sowohl eine Kosten-Nutzen-Rechnung, als auch die Ökobilanz negativ ausfallen.

Die werkstoffliche Verwertung wird daher zurzeit fast ausschließlich dort eingesetzt, wo große Mengen eines sortenreinen Materials zur Verfügung stehen. Beispielsweise werden in Deutschland Schaumpolystyrolverpackungen gesammelt, die eine erneute Verwertung als Bodenverbesserer in der Landwirtschaft oder bei der Herstellung von Schaumpolystyrol-Beton oder Ziegelsteinen finden. Die Recyclingquote für Schaumpolystyrol betrug im Jahre 2000 etwa 70 Prozent. Für PVC existiert ebenfalls ein Rücknahmesystem, gesammelt werden vor allem Fußbodenbeläge, Dachbahnen, Fensterprofile und PVC-Rohre. Weitere Anwendungsbereiche für die werkstoffliche Wiederverwertung sind zum Beispiel in der Wiederverwertung von Fahrzeugen oder Getränkeflaschen, oder in Ländern der zweiten oder dritten Welt, wo das Sammeln sortenreiner Kunststoffabfälle zum Einkommen beiträgt. So entstehen aus den Sekundärrohstoffen erneut Verpackungen oder Produkte wie Fensterprofile, Rohre, Blumen- und Getränkekästen, neue Folien, Fensterrahmen oder Gießkannen.

Durch Pyrolyse lassen sich Kunststoffe wieder in die jeweiligen Monomere oder weitere petrochemisch verwertbare Stoffe, wie Methanol oder Synthesegas spalten. Für die Gewinnung der Monomere ist aber ebenfalls die Verfügbarkeit sortenreinen Materials Voraussetzung. Beispiele sind das Hamburger Verfahren, das zurzeit von der BP betrieben wird und sowohl zur Gewinnung von Monomeren, als auch petrochemischer Rohstoffe dient und das von Walter Michaeli und anderen entwickelte Verfahren der degradativen Extrusion, das in der Lage ist, vermischte Kunststoffabfälle in rohstofflich verwertbare Gase, Wachse und Öle umzuwandeln. Diese Verfahren werden naturgemäß vor allem für die Verwertung von Mischkunststoffen genutzt, die sich nur unter großem Aufwand trennen lassen würden.

Bei der energetischen Verwertung werden die Kunststoffe zur Energiegewinnung genutzt. Dies geschieht fast ausschließlich durch Verbrennung. Einsatzgebiete sind vor allem Hochöfen, Zementwerke, Kraftwerke etc. Die dort vorherrschenden hohen Temperaturen sorgen für eine vollständige und schadstoffarme Verbrennung. Der Heizwert von Kunststoffen entspricht ungefähr dem von Steinkohle.

Herkömmliche Kunststoffe galten bislang als biologisch nicht abbaubar. Ohne biologischen Abbau zersetzen sich Kunststoffe nur sehr langsam durch chemische und physikalische Prozesse (siehe Abschnitt Persistenz). In jüngerer Zeit wurden einige Organismen gefunden, die auch herkömmliche Kunststoffe abbauen können.

Bereits seit langem sind biologisch abbaubare Kunststoffe bekannt, meist Polyester.

Von zwei Insekten, der Dörrobstmotte "Plodia interpunctuella" und der Großen Wachsmotte "Galleria mellonella" ist bekannt, dass sie mit Hilfe von Darmbakterien Polyethylen abbauen können.

Die Mottenlarve Galleria mellonella kann Polyethylen-Folien innerhalb von wenigen Stunden durchlöchern. Der genaue Mechanismus ist noch unbekannt, vermutlich besteht ein Zusammenhang mit der Fähigkeit der Mottenlarven, Wachs aus Bienenwaben zu verdauen (Wachs und Polyethylen sind sich chemisch ähnlich, in beiden spielen CH-CH-Bindungen eine wichtige Rolle). 2016 wurde das Bakterium Ideonella sakaiensis entdeckt, das in der Lage ist, sich von PET-Abfällen zu ernähren. Es benötigte jedoch für den Abbau eines dünnen Kunststofffilms sechs Wochen. Da es PET in seine Ausgangsstoffe Terephthalsäure und Glykol zersetzt, wäre es prinzipiell zum Recycling einsetzbar.

Ebenso können Mehlkäfer "(Tenebrio molitor)" ausschließlich mit Polystyrol ernährt werden, wobei bei Mehlkäfern der Abbauprozess noch unbekannt ist. Da Insektenlarven mit ihren Kauwerkzeugen Kunststoffe zu feinen Partikeln verarbeiten, kann der Abbau durch bakterielle Enzyme schneller erfolgen.

2017 wurde in einer Publikation gezeigt, dass Tübinger Gießkannenschimmel ("Aspergillus tubingensis") Polyester-urethane abbauen kann. Innerhalb von zwei Monaten konnte ein Kunststofffilm vollständig abgebaut werden.

Seit etwa 1990 forscht man intensiv an durch Kompostierung entsorgbaren Kunststoffen. Definiert wird die "Prüfung der Kompostierbarkeit von Kunststoffen" seit 1998 unter der DIN-Norm V 54900. Damit ein Kunststoff biologisch abbaubar wird, muss er Angriffsstellen für die Enzyme der Mikroorganismen bieten, die ihn für ihren eigenen Stoffwechsel nutzen wollen. Diese Enzyme verwandeln die langen Polymerketten in handlichere wasserlösliche Bruchstücke. Dazu kann man bereits natürlich vorkommende Polymere "(Biopolymere)" nutzen, oder in synthetisch hergestellte Ketten Einheiten wie Zucker, Bernsteinsäure oder Milchsäure integrieren. Entscheidend ist die Anwesenheit von Heteroatomen wie Stickstoff oder Sauerstoff im Kunststoff. So sind die meisten der bisher etwa 30 bekannten, vermarktungsfähigen, biologisch abbaubaren Kunststoffe Polyester, Polyamide, Polyesterurethane und Polysaccharide. Bei synthetisch hergestellten Polyestern und -amiden besteht das Problem, dass gerade die Eigenschaften, die die Schlag- und Zugfestigkeit der Materialien ausmachen (intramolekulare H-Brücken in Amiden, aromatische Komponenten in Polyestern), einer Verwertung durch die Natur entgegenstehen. Eine Verbesserung der biologischen Abbaubarkeit bedeutet so auch fast immer eine Verschlechterung der Werkstoffeigenschaften.
Die Weltproduktion an biologisch abbaubaren Kunststoffen betrug im Jahr 2007 300.000 Tonnen (im Vergleich zu 240 Mio. Tonnen Standardkunststoff).

Polysaccharide (Stärke, Cellulose) dienen der Natur als Energiespeicher und Gerüstsubstanzen. Unzählige Einfachzucker (wie Glukose oder Fruktose) bilden lange Ketten und stellen somit natürlich vorkommende Polymere dar, die als solche auch von der Natur abgebaut werden können. Sie sind billig und in großen Mengen verfügbar, zeigen allerdings einen gravierenden Nachteil: Sie können nicht durch Aufschmelzen zu Folien, Formteilen oder Fasern verarbeitet werden, d. h., sie sind "nicht" thermoplastisch formbar. Die thermoplastische Formbarkeit ist jedoch gerade einer der großen Vorzüge von Kunststoffen. Eine Veresterung der freien OH-Gruppen der Zucker verbessert zwar die Materialeigenschaften, setzt aber auch ihre Fähigkeit zur biologischen Abbaubarkeit herab. Will man also Polysaccharide als Werkstoff einsetzen, muss man einen Kompromiss zwischen Werkstoffeigenschaften und biologischer Abbaubarkeit finden.

Polyhydroxybuttersäure ist ein ebenfalls natürlich vorkommendes Polymer, das von bestimmten Mikroorganismen zur Energiespeicherung gebildet wird. Durch Fermentation können diese dazu angeregt werden, das Polymer bis zu 90 % ihrer eigenen Masse anzureichern. Es ist als Biopolymer biologisch abbaubar und zeigt Materialeigenschaften, die denen von Polyestern ähneln. Gegenwärtig forscht man daran, PHB auch in gentechnisch veränderten Pflanzen produzieren zu können "(„Plastikkartoffeln“)".


Weltweit werden derzeit rund 380 Millionen Tonnen Kunststoff pro Jahr verbraucht (Stand: 2017). Im Durchschnitt wuchs die Produktion von Kunststoffen seit 1950 um ca. 8,4 % pro Jahr und damit 2,5 mal so schnell wie das durchschnittliche Bruttoinlandsprodukt. 

Die Gründe dafür sind vielfältig (siehe auch Kapitel Eigenschaften): Zunächst ist Erdöl als Rohstoffquelle leicht zugänglich; dabei beträgt der Anteil am weltweiten Erdölverbrauch von Kunststoffen nur 4 %. Das Gewicht von Kunststoff ist, verglichen mit Eisen- und Keramikwerkstoffen, sehr gering. Die Verarbeitung von Kunststoffen (und speziell Thermoplasten) ist bei niedrigen Temperaturen möglich und damit kostengünstig. Schließlich sind Kunststoffe auch noch durch ihre speziellen Eigenschaften als Funktionswerkstoffe (siehe Kapitel Sonderkunststoffe) für Anwendungen verwendbar, für die sich sonst kein anderes Material in dieser Weise eignen würde und die z. T. erst durch Kunststoffe ermöglicht werden.

Produktion von Kunststoffen in Deutschland 2007
Die Polymerstrukturen der Kunststoffe selbst gelten, da die Zellen lebender Organismen nicht in der Lage sind sie aufzunehmen, als biologisch inaktiv und somit vollkommen unbedenklich. Gefahr kann allerdings von den zugesetzten Additiven ausgehen. Diese können an der Oberfläche des Materials, z. B. Bodenbelägen, austreten "(Ausschwitzen)". Aus diesem Grunde gelten für Lebensmittelverpackungen, Kunststoffe in der Medizin und ähnliche Anwendungen besonders strenge Auflagen hinsichtlich der Verwendung von Additiven. Die in solchen Bereichen eingesetzten Kunststoffe bedürfen einer Zulassung, beispielsweise durch die FDA.

In diesem Zusammenhang ist in der Vergangenheit vor allem Weich-PVC in die Kritik geraten, da diesem Kunststoff besonders große Mengen an Weichmachern zugesetzt werden. Es ist daher schon seit langem nicht mehr als Verpackung für Lebensmittel zugelassen. Ebenso ist in der Europäischen Union Herstellung und Vertrieb von Spielzeug für Kinder bis zum Alter von drei Jahren aus Material untersagt, das Phthalat-Weichmacher (z. B. DEHP) enthält. Allerdings werden bis heute vor allem in Fernost produzierte Spielzeuge aus Weich-PVC verkauft.










</doc>
<doc id="14352" url="https://de.wikipedia.org/wiki?curid=14352" title="Tromba">
Tromba

Tromba steht für:

Tromba ist der Familienname von:
Siehe auch:


</doc>
<doc id="14364" url="https://de.wikipedia.org/wiki?curid=14364" title="Helikon">
Helikon

Helikon steht für:
Siehe auch:


</doc>
<doc id="14368" url="https://de.wikipedia.org/wiki?curid=14368" title="Euphonium">
Euphonium

Das Euphonium (griechisch für „wohlklingend“) ist ein tiefes Blechblasinstrument, das aufgrund seiner konischen Mensur zur Familie der Bügelhörner gehört wie das Flügelhorn, das Tenorhorn, das Bariton und die Tuba.

Das Euphonium als Tenor- und Baritonstimme entwickelte sich aus der Ophikleide, die wiederum aus dem Serpent hervorging.

Als etwa 1813 die Périnet-Ventile erfunden wurden, wurde neben der Trompete der Tenortrompetenbass erbaut, der sich zum Flügelhorn entwickelte. Zu diesem Instrument baute man eine weitere Form, das Tenorhorn.

1843 wurde das Euphonium erfunden, um einen tieferen und weicheren Klang zu erhalten. Es wurde allerdings damals noch mit dem italienischen "corno basso chromatico" bezeichnet. Als Erfinder gelten „Capellmeister Sommer“, der aus Weimar stammte, und Adolphe Sax. Das Euphonium erhielt eine weitere Mensur. Das Euphonium nennt man auch Infanteriecello, es ist das „Violoncello“ der Blasmusik.

Die Grundstimmung des Euphoniums ist in B; es klingt eine Oktave tiefer als eine Trompete und eine Oktave höher als eine Tuba in dieser Stimmung. Die Notation erfolgt entweder im Violinschlüssel als B-Stimme, d. h. eine Note tiefer klingend als notiert oder im Bass-Schlüssel untransponiert als C-Stimme. Seltener sieht man auch nach B transponierte Notationen im Bass-Schlüssel, also eine große Sekunde tiefer klingend als notiert; diese Notation ist üblich vor allem in Frankreich und den Benelux-Staaten.

Das Euphonium wird mit einem Trichter- oder einem Kesselmundstück gespielt.

Während manche frühen Modelle wegen der unvermeidlichen Intonationsprobleme von Ventilkombinationen mit bis zu sechs Ventilen ausgestattet waren, haben heutige Euphonien stattdessen mitunter ein Kompensationssystem, in jedem Fall aber nur noch drei oder vier Ventile. Die ersten drei werden mit der rechten Hand und je nach Bauart des Instruments entweder von oben oder von vorn gegriffen ("top action" oder "front action"). Das vierte Ventil, so vorhanden, ist ein Quartventil. An einfachen Instrumenten befindet es sich neben den ersten drei Ventilen ("in-line") und wird mit dem kleinen Finger der rechten Hand gegriffen.

An aufwändigeren Instrumenten ist das vierte Ventil seitlich angebracht und wird mit der linken Hand gespielt. Die Bohrung in der Ventilmaschine beträgt heute bis zu 16,2 mm, im vierten Ventil bis zu 17,2 mm. Der Schalltrichter weist entweder nach oben oder ist schräg nach vorn abgewinkelt („Bellfront“ oder „Frontbell“) und hat einen Durchmesser von bis zu 310 mm.

Der Tonumfang entspricht in etwa dem der Posaune, wobei selbst weit mensurierte Euphonien in der Höhe oft etwas besser ansprechen. Nur kompensierte Euphonien mit Quartventil sind jedoch bereits ab der Pedallage aufwärts voll chromatisch spielbar. Auf nicht kompensierten Instrumenten dagegen intonieren die Töne ab dem großen As abwärts bis zum Kontra-H zunehmend schlechter. Der auf allen vier Ventilen gegriffene Ton liegt hier deutlich näher am C als am Kontra-H.

In der Klangfarbe unterscheiden sich Euphonium und Posaune erheblich: Infolge seiner konischen Mensur ähnelt der weichere Klang des Euphoniums stark dem des Waldhorns, während die Posaune aufgrund ihrer zylindrischen Mensur härter klingt und eher der Trompete oder dem Tenorhorn ähnelt.

Das Euphonium wird in Blaskapellen als Soloinstrument eingesetzt, weil es einen weichen, aber auch spitzen, lauten Klang haben kann. Es ist aber auch in der Blasmusik nicht selten vertreten.

Im symphonischen Bereich wird das Euphonium – wenn überhaupt – solistisch eingesetzt, und zwar für bestimmte Partien in meist spätromantischen Werken, bei denen nicht völlig klar ist, ob der Komponist mit „Tuba“ eine Tenortuba, eine Wagnertuba, ein Saxhorn oder ein anderes, heute eventuell nicht mehr gebräuchliches Instrument im Sinn hatte, und die oberhalb des „normalen“ Tonumfangs einer modernen F-Tuba notiert sind. Das Euphonium wird im sinfonischen Bereich auch anstatt des Tenorhorns oder Baritons verwendet.

Beispiele für Werke, in denen statt eines Tenorhorns ein Euphonium verwendet wird
In Deutschland, Österreich und Tschechien ist das Euphonium wenig verbreitet, da man hier traditionell die ovalen Hörner mit Drehventilen wie das Baritonhorn bevorzugt. Das Euphonium verbreitet sich aber auch zusehends in diesen Ländern, besonders im Bereich der symphonischen und traditionellen Blasmusik, vor allem weil das Euphonium zur Standardinstrumentierung der Bläserklasse gehört.

Zur weiteren Literatur gehören Konzerte, die ursprünglich für Violoncello geschrieben wurden. Auch Ragtimes von unter anderem Scott Joplin werden gerne auf dem Euphonium gespielt. Das Euphonium spielt teilweise die Posaunenstimme einiger Musikstücke.




</doc>
<doc id="14369" url="https://de.wikipedia.org/wiki?curid=14369" title="Bassklarinette">
Bassklarinette

Die Bassklarinette ist ein Holzblasinstrument, der Bass der Klarinettenfamilie. Als transponierendes Musikinstrument klingt sie in "tief-B", also eine große None tiefer als notiert. Die Kontrabassklarinette liegt noch eine Oktave tiefer, die Subkontrabassklarinette ist das tiefste Instrument der Klarinettengattung. In manchen älteren Werken (z. B. bei Wagner oder Maurice Ravel) wird auch eine Bassklarinette in A verlangt. Da aber Instrumente in dieser Stimmung heute nicht mehr gebaut werden, muss der Klarinettist, wenn er ein B-Instrument verwendet, die Stimme entsprechend (primavista) transponieren. Abgesehen von der normalen Notation im Violinschlüssel wird, vielfach in französischen Werken, auch der Bassschlüssel verwendet, in dem eine Oktave tiefer, also einen Ganzton über dem klingenden Ton, notiert wird.

Die Bassklarinette wird in der Regel aus Grenadillholz (einfachere Ausführungen auch aus Kunststoff, Resonite) gefertigt. Sie besteht aus einem Mundstück, einem ein- oder zweiteiligen S-Bogen, einem Ober- und einem Unterstück sowie einem Schallbecher. Der Schallbecher wird in der Regel aus Metall gefertigt, jedoch gibt es neuerdings auch Schallbecher aus Holz, was sich positiv auf den Klang und die Resonanz des Instrumentes auswirkt. Die Bassklarinette wird in der Regel im Sitzen gespielt. Jedoch verfügt sie, ähnlich wie ein Saxophon, auch über eine an der Verbindung zwischen Ober- und Unterstück angebrachte Öse, in die ein Tragegurt eingehängt werden kann (etwa für die Verwendung in der Marschmusik oder für Solisten, welche ein Konzert im Stehen spielen wollen). Um die Bassklarinette der Körpergröße der im Sitzen spielenden Musiker anzupassen, ist sie mit einem dem Violoncello ähnlichen, höhenverstellbaren Stachel versehen. Selten sieht man Instrumente mit gewickelter Röhre, die Fagotten ähneln. Einige Hersteller verwenden diese Variante vor allem für Kontrabassklarinetten. Wie alle Klarinetteninstrumente hat es eine zylindrische Mensur und ein Mundstück mit einfachem Rohrblatt.

Anders als die B-Klarinette, die das "kleine E" (klingend D) als tiefsten Ton hat, können die meisten Bassklarinetten heute bis zum "großen Es" (dem klingenden "großen Des") spielen. "Profi-Instrumente" gehen im Bass bis zum "großen C" (dem klingenden "Kontra-B"). Die obere Grenze des Tonumfangs hängt, wie bei allen Blasinstrumenten, von der Kunstfertigkeit des Bläsers und dem verwendeten Mundstück oder Blatt ab.

Mehr noch als bei der B-Klarinette hat sich bei der Bassklarinette das Boehmsystem durchsetzen können, nicht zuletzt, da Bassklarinetten mit deutschem System (Müller-System) auf Grund der international sehr begrenzten Nachfrage sehr viel teurer sind. Zudem ist das Angebot an Rohrblättern mit deutschem Schnitt sehr gering.

Der genaue Ursprung der Bassklarinette ist unsicher. Erste Instrumente, die eine Oktave tiefer klingen als die Klarinette tauchen um 1750 auf. Ein 1770 gebautes Instrument ist im Stadtmuseum München erhalten. 1772 stellt Gilles Lott in Paris ein Instrument vor. Ab 1793 werden in der Werkstatt von Heinrich Grenser Instrumente entwickelt, die eher wie ein Fagott konstruiert sind, ähnlich dem Instrument, das Streitwolf in Göttingen etwas später baut. Andere Instrumente aus der Zeit erinnern an einen Serpent. Erst der belgische Musikinstrumentenbauer Adolphe Sax entwickelt um 1830 die heutige Form der Bassklarinette, die er 1838 zum Patent anmeldet: Die Bassklarinette hat nun eine viel größere Bohrung und keine offenen Tonlöcher mehr, sondern nur noch Klappen, außerdem einen gebogenen S-Bogen und einen geraden oder gebogenen Schalltrichter, wie man ihn heute kennt.

Bassklarinetten werden oft zur Erweiterung des Klangs in kleineren Ensembles eingesetzt, außerdem findet man sie ab 1850 häufig im Symphonieorchester, wo sie zumeist die Bassfunktion ausführen. In der Filmmusik markiert die Bassklarinette oft spannende Stellen. Auch im Jazz wird das Instrument gerne eingesetzt.

Im Orchester wird die Bassklarinette meistens vom zweiten oder dritten Klarinettisten als Nebeninstrument gespielt, große Orchester haben manchmal aber auch reine Bassklarinettisten.

Das bekannteste Bassklarinettensolo in der Klassik ist wohl der "Tanz der Zuckerfee" aus Tschaikowskis Ballett "Der Nussknacker", wo die tiefen Töne einen Kontrast zu den glitzernden Höhen der Celesta bilden. Weitere solistische Passagen kann man in einigen Wagner-Opern (vor allem im Tristan und der Walküre) und in der "Rhapsodie Espagnole" von Maurice Ravel hören. Eine wichtige Rolle spielt die Bassklarinette in den symphonischen Dichtungen von Richard Strauss (z. B. "Till Eulenspiegels lustige Streiche") und in praktisch allen Orchesterkompositionen von Gustav Mahler.

Weiterhin ist die Bassklarinette in beinahe allen sinfonischen Blasorchestern besetzt. Hier ist sie in der zeitgenössischen Literatur nicht mehr wegzudenken. Häufig wird ihr mysteriöser Klang in ruhigen Passagen eingesetzt, in Form von langen, ausgehaltenen Tönen. Anders als z. B. auf dem Fagott ist es auf der Bassklarinette sehr viel einfacher möglich, weich und leise auf einem tiefen Ton einzusetzen. Die oktav-freie Obertonreihe der Klarinette tut ihr Übriges, einen geheimnisvollen, entfernten Ton erklingen zu lassen.

Des Weiteren wird ihre hohe Beweglichkeit bis in die tiefe Lage von den Komponisten eingesetzt.

Vor allem in der Neuen Musik wird die Bassklarinette gelegentlich auch in kleinen Besetzungen eingesetzt. Die bekannteste Verwendung fand sie wohl in Arnold Schönbergs "Pierrot Lunaire". Des Weiteren ist die Bassklarinette als Bassinstrument im Klarinettenquartett stets besetzt. In manchen Arrangements kann sie jedoch auch wahlweise durch eine B-Klarinette ersetzt werden. Auch in größeren Klarinettenensembles oder Klarinettenchören ist die Bassklarinette und zum Teil auch die Kontrabassklarinette ein wichtiger Bestandteil des Ensembles. In für diese Ensembles angefertigten Bearbeitungen von Orchesterwerken dient sie als Ersatz für die tiefen Streich- bzw. Blasinstrumente.

Für die Bassklarinette wurde insbesondere im vergangenen Jahrhundert eine Vielzahl von Solostücken (teilweise mit Klavierbegleitung) komponiert. Diese sind häufig sehr virtuos und außerordentlich schwierig. Sie zeigen, dass die Bassklarinette nicht nur leise, tief und langsam spielen kann.
Von Josef Schelb stammt das erste dreisätzige Konzert, uraufgeführt 1931 von Hans Rosbaud, 1943 vom Komponisten nach dem kriegsbedingten Verlust des Originalmanuskripts rekonstruiert. Ein Stück für Bassklarinette solo mit dem Titel "Schattenklänge" komponierte Mauricio Kagel 1995. Weitere Kompositionen gibt es z. B. von Elliott Carter ("Steep Steps", 2001), Dietrich Erdmann ("Monolog", 1984), Othmar Schoeck ("Sonate für Bassklarinette und Klavier", op. 41, 1931), Olga Neuwirth ("Spleen", 1994), Shigeru Kan-no ("Otnacca", 1999), Iris ter Schiphorst ("Hi Bill", 2005), Léonid Karev ("Manteau noir", 2007). Uwe Lohrmann ("Solo für Harry Sparnaay", 1979). Anders Eliasson integriert in seinem einsätzigen, 1996 entstandenen und 2004 von Bo Pettersson uraufgeführten "Konzert für Bassklarinette und Orchester" das Soloinstrument ins symphonisch durchgearbeitete Geschehen. Timo Jouko Herrmann zitiert in seinem 2008 für Volker Hemken, den Solo-Bassklarinettisten des Gewandhausorchesters, geschriebenen Concertino "L’ombre de Dinorah" die sogenannte „Schattenarie“ aus Giacomo Meyerbeers Oper "Dinorah ou Le pardon de Ploërmel". Einen wesentlichen Beitrag zur Erweiterung der Sololiteratur für Bassklarinette hat Josef Horák geleistet. 1955 spielte er erstmals ein abendfüllendes Soloprogramm für Bassklarinette und Klavier. Seither wurden über 500 Kompositionen, die für ihn geschrieben oder bearbeitet wurden, von seinem "Due Boemi di Praga" uraufgeführt.

1926 spielte Omer Simeon das erste Jazz-Bassklarinettensolo der Jazzgeschichte in der Nummer "Someday Sweethart" von Jelly Roll Morton und seinen "Red Hot Peppers". Zu Beginn der 1930er Jahre spielte Harry Carney in Duke Ellingtons Arrangements gelegentlich Bassklarinette. Der erste wichtige Solist des Instruments aber war Eric Dolphy, der es als ernstzunehmendes Jazzinstrument etablierte. Seither wird die Bassklarinette oft verwendet, selten aber spezialisieren sich Musiker (wie zum Beispiel Michel Pilz, Rudi Mahall, Claudio Puntin oder Thomas Savy) ausschließlich auf das Instrument, oft wird es als Nebeninstrument von Saxophonisten gespielt. Bei der Jazz-Version von Layla, die Eric Clapton 1997 auf einigen Jazz-Festivals spielte, ist Marcus Miller an der Bassklarinette zu hören.


</doc>
<doc id="14371" url="https://de.wikipedia.org/wiki?curid=14371" title="Altklarinette">
Altklarinette

Die Altklarinette ist ein Holzblasinstrument aus der Familie der Klarinetten. Sie ist wie das Bassetthorn in Es oder F gestimmt und liegt damit zwischen der (Sopran-)Klarinette und der Bassklarinette.

Die heutige Klarinette entwickelte sich aus dem Chalumeau neben dem Bassetthorn, das ebenfalls in F gestimmt war. Eine frühe Bauform der Klarinette in Altlage war die Klarinette d’amore („Liebesklarinette“) in G mit einem birnenförmigen Schallbecher, dem so genannten Liebesfuß, die die Oboe d’amore zum Vorbild hatte. Sie entstand um 1730 und ist neben dem Bassetthorn eine Vorgängerin der heutigen Altklarinette.

Die ersten Altklarinetten traten im frühen 19. Jahrhundert mit dem Aufkommen weit mensurierter Klarinetten in F und später E für Militärkapellen in Frankreich (zum Beispiel aus den Werkstätten von Cuvillier und Simiot) und Deutschland (1808 von Heinrich Grenser in Dresden und Wiesner) auf. In England wird die Altklarinette in F heute noch mit einer engeren Mensur gespielt und als „Tenorklarinette“ bezeichnet. Eine andere Bezeichnung ist „Baritonklarinette“.

Experimentell blieben die Kontraalt- und die Subkontraaltklarinette, die eine beziehungsweise zwei Oktaven tiefer gestimmt sind als die Altklarinette.
Wie jede Klarinette besteht die Altklarinette aus einem Mundstück (Schnabel), einem Hals (Birne), einem Ober- und einem Unterstück aus Holz (meist Grenadill, heute auch aus Kunststoff oder Metall) mit den Grifflöchern und einem Schalltrichter. Die Griffweise ist wie bei der Sopranklarinette, wobei die Grifflöcher meist mit Klappen bedeckt sind, weil sie zum Greifen zu groß sind. Wie die Bassklarinette und das heutige Bassetthorn hat die Altklarinette einen aufwärts gebogenen Schalltrichter aus Metall, einen nach hinten gebogenen Hals aus Metall. Insgesamt ist das Instrument 75–90 cm lang. Häufig gibt es zwei Überblasklappen. Die Wandung ist stärker und die Mensur enger als beim Bassetthorn, außerdem fehlt die Erweiterung des Tonumfangs in der Tiefe bis zum C. Wegen des Gewichts wird die Altklarinette häufig mit einer Trageschnur am Hals getragen.

Der Tonumfang geht (klingend) vom F bis zum f’’’ beziehungsweise vom Es bis zum es’’’.

Die Altklarinette wird heute vor allem in der Blasmusik verwendet, zum Beispiel als Bestandteil von Blaskapellen und Militärkapellen sowie sinfonischen Blasorchestern und Klarinettenensembles. In britischen Militärkapellen ist die Altklarinette nicht mehr enthalten. Der Klang ist voll und mischt sich anders als beim Bassetthorn gut mit anderen modernen Instrumenten. Im Sinfonieorchester kommt die Altklarinette selten vor, was Hector Berlioz bedauerte.



</doc>
<doc id="14372" url="https://de.wikipedia.org/wiki?curid=14372" title="Chalumeau">
Chalumeau

Das Chalumeau (Aussprache: []) (pl. Chalumeaux – aus franz. "Chalumeau", „Schalmei/Rohrblattinstrument“, das auf , u. Ä.) ist ein Holzblasinstrument mit einfachem Rohrblatt. Es ist verwandt mit der Klarinette, die aus dem Chalumeau entwickelt wurde.

Das Instrument hat eine zylindrische Röhre, sieben vorderständige Grifflöcher und ein Daumenloch. Der Tonumfang des historischen Chalumeau beträgt eine große None. Seit Beginn des 18. Jahrhunderts wurden meist zwei Klappen zugefügt, um den Tonumfang nach oben auf eine Undezime zu erweitern. Es wurde nicht überblasen, sondern für verschiedene Stimmlagen in unterschiedlichen Größen in f/c-Stimmung gebaut. Das Chalumeau klingt eine Oktave tiefer als eine Blockflöte gleicher Länge. Der Klang ist weicher und offener als der einer Klarinette.

Das Chalumeau ist zu unterscheiden von dem Doppelrohrblattinstrument Schalmei, dessen Name etymologisch verwandt ist.

Der Name "Chalumeau/Schalmei" wurde seit dem Mittelalter für Rohrblattinstrumente mit doppeltem und einfachem Rohrblatt verwendet. Marin Mersenne verwendet in seiner "Harmonie Universelle" (1636) den Begriff für zwei einfache Instrumente aus Strohhalmen sowie für die Melodiepfeife der Sackpfeife (mit Doppelrohrblatt, bei ihm auch „cornemuse“ genannt). Als „chalumeau eunuque“ bezeichnet er ein Mirliton.

Die Instrumente aus Strohhalmen erhalten ihre tonerzeugende Zunge durch einen Aufwärtsschnitt in den Halm. Mit Hilfe von Fingerlöchern lässt sich die Tonhöhe variieren. Es handelt sich um ideoglotte Rohre, die einfachste Form von Einfachrohrblattinstrumenten, wie sie in vielen Musiktraditionen Europas, Asiens und Nordafrika bis heute bekannt sind. Deren Vorgeschichte reicht bis in die Antike zurück.

Als Instrument mit flötenartigem Korpus und einfachem Rohrblatt ist das Chalumeau erst seit Ende des 17. Jahrhunderts nachweisbar. Das Chalumeau ist damit nur wenig älter als die Klarinette, die zu Beginn des 18. Jahrhunderts entwickelt wurde. Johann Christoph Denner (1655–1707), dem üblicherweise die Erfindung der Klarinette zugeschrieben wird, hat nach einer biographischen Angabe von 1730 auch das Chalumeau verbessert.

Eine ursprünglich anmutende Form hatten Instrumente, die in der letzten Dekade des 17. Jahrhunderts in England unter dem Namen „Mock trumpet“ verbreitet waren. Bei ihnen war die Stimmzunge durch einen Abwärtsschnitt aus dem Rohr geschnitten (ideoglott). Diese Form erscheint auch noch auf einer Abbildung von Reynvaan (1795). Leider ist kein Instrument dieses Typs erhalten. Die acht erhaltenen Chalumeaux haben Mundstücke, an denen ein aufschlagendes Rohrblatt befestigt wird (heteroglott).

Die Instrumente wurden in verschiedenen Größen für die verschiedenen Stimmlagen sowie in verschiedenen Stimmungen gebaut. Bei größeren Instrumenten wurden Klappen hinzugefügt, um das Greifen zu erleichtern. Meist wurden unterhalb des Mundstücks zwei gegenüberliegende Klappen zur Erweiterung des Stimmumfangs nach oben angebracht. Für die Diskantlage wird der Umfang f bis a angegeben (selten überblasen bis c), für die Altlage eine Quart darunter (c-f). Tenor- und Basslage liegen je eine Oktave tiefer. Ob erhaltene größere Instrumente mit fagottartig geknicktem Rohr als Sub-Bassinstrumente des Chalumeau-Chors anzusehen sind, ist umstritten. Die G-Stimmung wird häufiger erwähnt, aber auch andere Grundtöne sind möglich.

An der Klappenanordnung eines erhaltenen Chalumeaus lässt sich ablesen, dass das Blatt auf der Oberseite des Instruments befestigt war, beim Spielen also von der Oberlippe berührt wurde.

Die Lexikographen bis Mitte des 18. Jahrhunderts beschreiben den Klang des Instruments wenig günstig: „als wenn ein Mensch durch die Zähne singet“ (Walther, 1708), „etwas heulende Symphonie“ (Mattheson, 1713). In der Encyclopédie von Diderot und le Rond D’Alembert wird der Ton als unangenehm und wild beschrieben, wenn es von einem gewöhnlichen Musiker gespielt wird. Erst spätere Urteile fallen positiver aus: „Der Ton desselben hat so viel Interessantes, Eigenthümliches, unendlich Angenehmes, dass die ganze Scale der Tonkunst eine merkliche Lücke hätte, wenn dieses Instrument verloren ginge“ (Schubart 1784/85).

Im 18. Jahrhundert bestanden Chalumeau und Klarinette nebeneinander und wurden als verschiedene Instrumente wahrgenommen. Chalumeaux hatten ein breiteres Blatt und dienten für das tiefere Register. Klarinetten hatten ein schmaleres Blatt, ein nach oben versetztes Daumenloch mit Metallhülse (Überblasloch) und wurden für das höhere Register verwendet. Die Klarinette unterschied sich ferner durch Birne und offenen Schalltrichter von der geraden Bauweise des Chalumeau. Mit der Weiterentwicklung des tiefen Registers der Klarinette wurde das Chalumeau aus dem Orchester verdrängt.

Im Hoch- und Spätbarock fand das Chalumeau (vorübergehend) Eingang in die bürgerliche und höfische Musik. Die frühesten erhaltenen Werke sind Duette für Mocktrumpet zu Beginn des 18. Jahrhunderts in England. Diese, für musikliebende Laien geschriebenen, Stücke imitieren vor allem die fanfarenartige Klänge der Trompete.

Im deutschsprachigen Raum wurde das Chalumeau im Rahmen der höfischen Orchester verwendet. Dabei sind zwei Schwerpunkte auszumachen: Im ersten Drittel des Jahrhunderts lag er bei der Wiener Oper (Fux, G. u. A. Bononcini, Caldara, später Gluck). Hier wurde der Klang des Chalumeau in pastoralen Szenen sowie zum Ausdruck zarter und intimer Gefühle verwendet. Das Sopranchalumeau überwiegt. Im zweiten Drittel des Jahrhunderts bildet die Verwendung bei Telemann und vor allem Graupner einen zweiten Schwerpunkt. Sie setzen das Instrument meist paarweise ein, die tieferen Lagen treten bevorzugt hervor. Virtuose Passagen sind selten. In Telemanns Passion (1728) "Seliges Erwägen" erscheint des Tenorchalumeau in der Arie "Es ist vollbracht" in hochemotionalem Zusammenhang.

Solistisch wird das Chalumeau unter anderem von folgenden Komponisten verwendet:


Vor einigen Jahren wurde das Chalumeau wiederentdeckt oder auch „wiedererfunden“. Verschiedene Ausführungen dienen unterschiedlichen Zwecken. Einerseits werden Instrumente zur historischen Aufführungspraxis für die Musik des 17. und 18. Jahrhunderts gebaut. Andererseits wird das Instrument von Folk- und Mittelalterformationen wegen seiner leichten Spielbarkeit und seines charakteristischen Klangs geschätzt.

Schließlich wurden Instrumente für die Instrumentalerziehung entwickelt, um den Übergang von Blockflöte auf Klarinette oder Saxophon vorzubereiten. Dabei ist meist ein modernes Klarinettenmundstück mit einem Blockflötenkorpus verbunden. Diese Instrumente werden unter sehr verschiedenen Bezeichnungen vertrieben (Sopranklarinette, Kinderklarinette, Clarineau, Pocket-Clarineau bzw. -Chalumeau, Saxonett: "sudden smile clarinet"). Es gibt Typen mit und ohne Klappen, z. T. mit Überblasklappe, in unterschiedlichen Stimmungen.

Auch das als „Taschensaxophon“ erfundene Xaphoon mit Bambuskorpus und entsprechende Instrumente aus Kunststoff oder Holz (Woodensax) sind als Chalumeaux anzusehen. Eine Kombination von Klarinettenmundstück und dem Metallkorpus der Tin Whistle wird als „Highland-Hornpipe“ angeboten.




</doc>
<doc id="14373" url="https://de.wikipedia.org/wiki?curid=14373" title="Bassett">
Bassett

Bassett ist der Familienname folgender Personen:




Bassett steht für folgende Orte in den Vereinigten Staaten:
Bassett steht für folgende Orte in Großbritannien:
Bassett steht für folgende im NRHP gelistete Objekte:
Bassett steht außerdem für:
Siehe auch:


</doc>
<doc id="14379" url="https://de.wikipedia.org/wiki?curid=14379" title="Guangzhou">
Guangzhou

Guangzhou (Exonym: Kanton bzw. Canton, , Abk.: ) ist eine Stadt im Süden der Volksrepublik China mit 11.114.200 Einwohnern im geographischen Stadtgebiet und 14.043.500 Einwohnern im administrativen Stadtgebiet (Stand Jahresende 2011 bzw. 2016). Sie ist Hauptstadt der Provinz Guangdong sowie ein bedeutender Industrie- und Handelsstandort. Die Region wird auch als „Fabrik der Welt“ bezeichnet.

Guangzhou ist die größte Stadt im Perlflussdelta, einer der größten zusammenhängenden Stadtlandschaften (Megalopolen) weltweit. Zum Perlfluss-Delta gehören neben Guangzhou unter anderem die Millionenstädte Hongkong, Shenzhen, Dongguan, Foshan, Jiangmen, Huizhou, Zhongshan und Zhuhai. Insgesamt wohnen hier auf einer Fläche, die in etwa der von Baden-Württemberg entspricht, über 100 Millionen Menschen.

Die im Deutschen (und ähnlich in anderen westlichen Sprachen) verwendete Bezeichnung "Kanton" geht auf den Namen der Provinz Guangdong zurück. In China wird Guangzhou auch oder genannt. Das Wahrzeichen der Stadt ist eine Statue mit fünf Ziegen. Die Nähe zu Hongkong hat – wie im gesamten Perlflussdelta – einen positiven Einfluss auf die wirtschaftliche Entwicklung gehabt. In Guangzhou findet zweimal jährlich – im Frühjahr und im Herbst – die Canton Fair, Chinas größte Import- und Exportmesse, statt. Im Oktober 2010 wurde hier der damals höchste Fernsehturm der Welt (Canton Tower, 600 m) eröffnet, 2012 ging dieser Rekord an den Tokyo Skytree.

Guangzhou liegt am Perlfluss. Durch die geographische Lage bedingt, herrscht in Guangzhou ein subtropisch-feuchtes Monsunklima mit einer Jahresdurchschnittstemperatur von 22 °C. Der meiste Niederschlag (Jahresdurchschnitt: 1.982 mm) fällt in der Regenzeit von April bis August.
Auf Kreisebene setzt sich die Unterprovinzstadt Guangzhou aus elf Stadtbezirken zusammen. Die Volksregierung Guangzhous hat ihren Sitz im Stadtbezirk Yuexiu. Am 28. April 2005 wurden die Stadtbezirke Dongshan und Fangcun aufgelöst und nach Yuexiu und Liwan eingegliedert. Außerdem wurde der Stadtbezirk Nansha aus Panyu ausgegliedert, und der Stadtbezirk Luogang aus Teilen von Baiyun, Tianhe und Zengcheng errichtet, sowie aus einem Teil von Huangpu, der eine Exklave des neuen Stadtbezirks bildet. Im Februar 2014 wurde Luogang wieder aufgelöst und seine Fläche in den Stadtbezirk Huangpu integriert. Gleichzeitig wurden die beiden kreisfreien Städte Conghua und Zengcheng in Stadtbezirke umgewandelt.

Es wird angenommen, dass das Gebiet um Guangzhou bereits im 9. Jh. v. Chr. besiedelt wurde. Während der Qin-Dynastie war die Stadt die Hauptstadt der Präfektur Nanhai (南海).

Andere Historiker gehen davon aus, dass sich im Jahr 214 vor Christus Menschen niedergelassen haben. Das älteste Siedlungsgebiet im heutigen Guangzhou lag nach dieser Theorie in Panyu (番禺 Pinyin Pānyú). Seitdem war das Gebiet durchgehend bewohnt. Panyu wuchs vor allem, als es im Jahre 206 vor Christus die Hauptstadt des Nanyue-Königreiches wurde. Die westliche Han-Dynastie annektierte das Königreich im Jahr 111 vor Christus. Seitdem ist Panyu eine Provinzhauptstadt. Im Jahre 226 wurde die Stadt zum Sitz des Bezirkes "Guang" (广州 Pinyin Guǎngzhōu). Die Menschen gewöhnten sich daran, die Stadt Guangzhou statt Panyu zu nennen.

Lange Zeit war Guangzhou Ausgangspunkt der sogenannten „Seidenstraße auf dem Meer“ (海上絲路 / 海上丝路 Pinyin Hǎi shàng Sīlù). Über den Seeweg unterhielt die Stadt Handelsbeziehungen mit südasiatischen Ländern wie Indien und Arabien.

Von 1757 bis 1842 war Guangzhou der einzige Handelshafen, in dem Ausländern vertraglich das Recht zugesichert worden war, Handel treiben zu dürfen.

1711 errichtete die Britische Ostindien-Kompanie einen Handelsposten in Guangzhou. Der Kaiser Qianlong beschränkte das Recht von Ausländern, Niederlassungen in Guangzhou zu gründen, auf ein kleines Gebiet in Guangzhou.

Guangzhou war einer der fünf Vertragshäfen, deren Öffnung durch den Vertrag von Nanking nach dem Ersten Opiumkrieg von Großbritannien erzwungen wurde. Die anderen Häfen waren Fuzhou, Xiamen, Ningbo und Shanghai.

1918 wurde Guangzhou der offizielle Name der Stadt. Panyu wird seitdem ein Stadtbezirk im Süden Guangzhous genannt.

Sun Yat-sen bereitete in Guangzhou die Revolution gegen den Kaiser vor. Die von ihm errichtete Militärschule, die Huangpu Junxiao, auch Whampoa-Militärakademie, (黄埔军校) genannt wird, ist bis heute erhalten und ein beliebtes Ausflugsziel.

Japanische Truppen besetzten Guangzhou vom 12. Oktober 1938 bis zum 16. September 1945.

Nach dem Ende des Bürgerkriegs führten Erneuerungen zu einem höheren Lebensstandard vieler Menschen in Guangzhou. Ab dem Ende der 1970er Jahre profitierte die Stadt von den von Deng Xiaoping eingeleiteten wirtschaftlichen Reformen. Das wirtschaftliche Wachstum wurde vor allem durch die Nähe zu Hongkong und die Lage am Perlfluss stark gefördert.

Im Stadtteil Tianhe wurde mit dem 530 Meter hohen Chow Tai Fook Center 2016 das aktuell siebenthöchste Gebäude der Welt fertig gestellt. Zusammen mit dem 438 Meter hohen gegenüberliegenden Guangzhou International Finance Center, dem nördlich gelegenen CITIC Plaza und dem südlich gelegenen 600 Meter hohen Canton Tower, dem zweithöchsten Fernsehturm der Welt, bildet es städtebaulich eine architektonische Raute.

In China heißt es: "„Die Kantonesen essen alles, was schwimmt, fliegt oder vier Beine hat, außer U-Booten, Flugzeugen und Tischen.“". Die kantonesische Küche ist sehr vielfältig mit zum Teil sehr wohlschmeckenden, für Europäer teilweise gewöhnungsbedürftigen Gerichten; unter anderem werden Katzen, Hunde und Schlangen verzehrt, sie sind allerdings auf dem Speiseplan der meisten Kantonesen nicht oder nur selten zu finden.

Kantonesisches Essen ist im Allgemeinen nicht scharf. Typische Gerichte sind traditionale kantonesische Suppe (广式靓汤 Pinyin guǎngshì liàng tāng, Jyutping gwong2sik1 leng3 tong1), Brei (粥 Pinyin zhōu Jyutping zuk1) und Dim Sum (点心 Pinyin diǎnxīn Jyutping dim2sam1).

Die chinesische Küche in Europa und Nordamerika ist meist stark kantonesisch geprägt (v. a. durch Auslandschinesen aus Hongkong), allerdings meist in abgewandelter Form.

Von Guangzhou aus gibt es Zugverbindungen nach Hongkong und viele weitere chinesische Städte wie Peking, Shanghai und Nanning. Seit dem 26. Dezember 2012 ist die mit 2.298 Kilometer längste Hochgeschwindigkeitsstrecke der Welt zwischen Peking und Guangzhou in Betrieb. Die Hochgeschwindigkeitszüge fahren von dem 2010 eröffneten Bahnhof im südlichen Bezirk Panyu ab. Für eine kurze Zeit nach der Fertigstellung war der neue Bahnhof der größte in Asien und ist mit Stand 2015 der viertgrößte Bahnhof in China. Eine gute Erreichbarkeit des Fernbahnhofes ist durch den Anschluss an das U-Bahn-Netz gegeben, Buslinien und Taxis sind ebenfalls Bestandteil der öffentlichen Verkehrsinfrastruktur. Ein mittelfristiges Projekt ist die Schnellstreckenverbindung zwischen Hongkong und Guangzhou für die die Inbetriebnahme für das Jahr 2017 erwartet wird. Diese Verbindung wird das nördliche Perlflussdelta zwischen den Städten Hongkong und Guangzhou mit Stationen in Shenzhen und Dongguan erschließen.

Das wichtigste öffentliche Verkehrsmittel in Guangzhou sind Omnibusse. Eine Fahrt mit dem Bus kostet einen oder zwei Yuan (Renminbi). Der Preis hängt nicht davon ab, wie weit man fährt, sondern nur davon, ob der Bus klimatisiert ist oder nicht. Die meisten Omnibusse sind klimatisiert.

Die Guangzhou Metro wurde am 28. Juni 1997 eröffnet. Die Linien 1 bis 9, 13 und 14 sowie die S-Bahn-ähnliche Guangfo-Linie in die Nachbarstadt Foshan sind bereits in Betrieb, sieben weitere Linien sind in Bau. Zwecks Feinerschließung des Geschäftsviertels Zhujiang Xincheng (珠江新城) im Osten der Stadt wurde eine unterirdische Kabinenbahn, sogenannte APM ("Automated Person Mover") errichtet. Die Metro befördert täglich 4,392 Mio. Fahrgäste (Stand: Mai 2011) auf 236 km Streckenlänge. Mit 1,19 Milliarden Passagieren pro Jahr (Stand: Ende 2010) steht sie auf der Liste der weltweit meistfrequentierte U-Bahn-Systeme auf Platz 10.

Fast überall findet man Taxis. Diese sind jedoch teurer als in anderen Städten wie zum Beispiel Peking oder Qingdao. Der Basispreis liegt bei 10 RMB (etwa 1,20 EUR) für die ersten zwei Kilometer. Deutlich billiger sind dagegen die Motorradtaxis. Motorräder sind aber seit 2007 in großen Teilen der Stadt generell verboten.

Wichtige Straßen sind die Dongfenglu (东风路) und die Zhongshanlu (中山路). Die Dongfenglu verläuft durch den Norden der Stadt, entlang der Sun Yat-sen-Gedenkhalle und der Provinz- und Stadtregierung und in Richtung des Flughafens. Die Zhongshanlu ist fast so lang wie die ganze Stadt von Westen nach Osten. Sie verbindet unter anderem das alte Stadtzentrum bei Gongyuanqian (公园前) mit dem neuen Stadtzentrum Tianhe (天河).

2004 wurde der neue Flughafen Guangzhou Baiyun International Airport eröffnet. Er ist etwa 28 km vom Stadtzentrum entfernt. Die Kapazität beträgt im Jahr 2010 über 40 Millionen Passagiere.

2014 stand der Baiyun International Airport mit fast 55 Millionen Passagieren an 15. Stelle auf der vom "Airports International Council" erstellen Liste, die Flughäfen nach deren jährlichem Passagieraufkommen untersucht.

Nach der voraussichtlichen Fertigstellung des zweiten Terminals im Jahr 2018 und der bereits 2014 erfolgten Inbetriebnahme der dritten Start- und Landebahn wird die Passagierkapazität des Flughafens auf 80 Millionen, das Logistikaufkommen auf 2,5 Millionen Tonnen steigen.

Bootsverbindungen von der West- zur Ostgrenze der Innenstadt oder vom Nord- an das Südufer sind regelmäßig im 5-, 20- bzw. 30-Minuten-Takt. Weiterführende Strecken können mit Fähren an den jeweiligen Knotenpunkten der Stadtgrenzen genutzt werden. Diverse Ausflugsziele werden mit Tourschiffen angefahren. Touristisch attraktiv ist eine Fahrt bei Nacht dank der farbenfrohen und üppigen Beleuchtung der Brücken und an die Ufer angrenzenden Häuser und Straßen bzw. Geh- und Fahrradwege. Weitere Fährverbindungen erschließen Guangzhou mit Hongkong oder Foshan.

Eine Gondelbahn mit Gondeln für acht Personen befördert Passagiere auf den höchsten Punkt der Stadt, welcher sich auf dem Baiyun-Berg befindet. Wörtlich bedeutet Bai weiß und Yun Wolke.

Zwischen 1949 und 1979 war Guangzhou bei weitem die wirtschaftlich führende Stadt des Gebiets. Dies lag vor allem an der "China Import and Export Fair" (auch "Canton Fair"), die in Guangzhou im April und Oktober jeden Jahres veranstaltet wird. Dass Guangzhou als Austragungsort dieser Messe ausgesucht wurde, lag an seiner Nähe zu Hongkong und seiner traditionellen Rolle als Chinas Tor zur Welt. Zwischen 1949 und 1979 war die Canton Fair die einzige nennenswerte Verbindung der chinesischen Wirtschaft mit der Weltwirtschaft.

Ab den achtziger Jahren investierten viele ausländische Unternehmen in Guangzhou. Zunächst war ihr Ziel vor allem die Erschließung des südchinesischen Marktes. Guangzhou war im Jahr 2002 nach Shanghai und Peking die Stadt mit der größten Wirtschaft in Festlandchina. Im Vergleich zu anderen Städten des Perlflussdeltas ist die Wirtschaft Guangzhous nicht nur die größte, sondern auch die vielseitigste.
Im Jahr 2002 waren die wichtigsten Industriezweige Guangzhous die Transportausrüstung (5,09 Mrd. US$), Chemie (4,21 Mrd. US$), elektronische Telekommunikationsgeräte (3,67 Mrd. US$) und elektrische Geräte und Maschinen (2,72 Mrd. US$). In der Leichtindustrie ist die Produktion von Textilien, Lederprodukten und Plastikprodukten weit entwickelt. Andere wichtige Industrien sind der Schiffbau, Nahrungsmittelverarbeitung, Zuckerraffinerie, Eisen- und Stahlproduktion und Kautschukprodukte.

Guangzhou ist innerhalb Guangdongs das führende Zentrum von einer Reihe von Dienstleistungen. Dazu gehören Software, Logistik und Distribution. Weitere führende Dienstleistungssektoren sind Transport, Lagerung, Post und Telekommunikation, Handel, Banken, Versicherungen und Immobilien. Guangzhou ist auch ein wichtiger Austragungsort von Messen, darunter die LED CHINA. Das neue Messegelände "Pazhou International Convention and Exhibition Centre" hat eine Fläche von 10,5 Quadratkilometern.

Im Jahr 2002 hatten mehr als 8700 multinationale Unternehmen Niederlassungen in Guangzhou. Bedeutende Investoren sind unter anderem japanische Autohersteller wie Honda, Nissan und Isuzu. 2004 kam auch Toyota hinzu. Ausländische Dienstleister wie zum Beispiel Carrefour, 7-Eleven und Allianz SE werden von der wohlhabenden Bevölkerung der Stadt angezogen. FedEx betreibt seit Februar 2009 sein regionales Drehkreuz am Flughafen Guangzhou (Näheres hier).

Seit Ende 2013 befindet sich mit „Tianhe-2“ einer der leistungsstärksten Superrechner der Welt in Guangzhou, bis 2016 der leistungsstärkste der Welt. Sein Standort ist die Sun Yat-sen University. Mit einer Leistung von 30,65 Petaflops erreicht der Rechner fast die doppelte Leistung des bisherigen Spitzenreiters, "Titan" vom Oak Ridge National Laboratory in Tennessee, USA. Nach einer Testphase mit weiteren Optimierungen soll der Rechner eine Leistung von 54,90 Petaflops erreichen.

Im Juni 2016 wurde Tianhe-2 von der fast dreimal schnelleren chinesischen Eigenentwicklung Sunway TaihuLight mit 93 PFLOPS abgelöst.

Die Stadt besitzt über 30 öffentliche Museen mit zum Teil internationaler Bedeutung, wie zum Beispiel das Museum des Mausoleums des Königs von Nanyue, welches zugleich eines der offiziellen Denkmäler der Volksrepublik China ist, oder das Guangzhou Museum of Art.

2010 wurde das von Zaha Hadid entworfene Opernhaus Guangzhou am Perlfluss eröffnet. Im selben Jahr eröffnete auch der gegenüberliegende Neubau des Guangdong Museums.

Seit 2002 veranstaltet das Guangdong Museum of Art die Guangzhou-Triennale für moderne und zeitgenössische Kunst. Auf der 55. Biennale von Venedig organisierte das Museum mit 'Voice of the unseen' die bisher größte außerhalb Chinas gezeigte Schau unabhängiger chinesischer Kunst.

Seit 1957 hat Guangzhou ein Sinfonieorchester, das Symphonieorchester Guangzhou (Guangzhou Symphony Orchestra, Abk.: GSO, chin. 广州交响乐团 Pinyin Guǎngzhōu Jiāoxiǎng Yuètuán).

Guangzhou ist Austragungsort des erstmals 1998 und dann seit 2002 jährlich im Januar ausgetragenen Vier-Nationen-Turniers für Frauen-Fußballnationalmannschaften. 1991 fanden hier Spiele der Fußball-Weltmeisterschaft der Frauen statt, so ein Viertel- und Halbfinale und das Spiel um Platz 3.

Guangzhou war 2010 Gastgeber der XVI. Asienspiele, sowie Austragungsort der Gedächtnisweltmeisterschaft.

Guangzhou Evergrande, (广州恒大足球俱乐部; Pinyin: "Guǎngzhōu Héngdà Zúqiú Jùlèbù") wurde 1954 gegründet. Der Verein wurde 2011 erstmals Chinesischer Meister und gewann 2012 das Double aus Meisterschaft und Pokal. 2013 wurde der Verein abermals Landesmeister und zugleich AFC-Champions-League-Sieger. Am 17. Dezember 2013 trat er im Halbfinale der Club-Weltmeisterschaft in Marokko gegen den FC Bayern München an, der das Spiel 3:0 gewann.

Mitte der 1990er Jahre gründete sich der Rugby Club "Guangzhou Rams". Sie spielen gegen Teams aus Macau und Hongkong.


"Ebenso:"

Guangzhou unterhält Städtepartnerschaften mit folgenden Städten:

Der am 8. Oktober 1964 entdeckte Asteroid (3048) Guangzhou trägt seit 1989 den Namen der Stadt.



</doc>
<doc id="14380" url="https://de.wikipedia.org/wiki?curid=14380" title="Mähren">
Mähren

Mähren (selten auch "Morawien"; von , ' [] bzw. lat. ') ist (neben Böhmen und Österreichisch-Schlesien bzw. Tschechisch-Schlesien) eines der drei historischen Länder Tschechiens, gelegen in dessen Osten und Südosten. Im 9. Jahrhundert bestand auf dem Gebiet Mährens (sowie den angrenzenden westlichen Teilen der Slowakei) das Kerngebiet des Mährerreichs. Anfang des 11. Jahrhunderts wurde Mähren ein Land der Böhmischen Krone. In der Tschechischen Republik, zu der es heute gehört, ist Mähren eine historische Landschaft, stellt seit dem Jahr 1949 somit keine eigene Verwaltungseinheit mehr dar.
Der einheimische Name Mährens, ', stammt von dem des Hauptflusses des Gebiets, dem Donau-Nebenfluss March (tsch./slowk. ').
An seiner Westflanke wird Mähren von Böhmen, dem größten historischen Land Tschechiens, begrenzt und an seiner Nordflanke von Tschechisch-Schlesien, dem kleinsten historischen tschechischen Land. Im Osten grenzt Mähren an die Slowakei und im Süden an Österreich.

Mähren bildet das östliche Drittel Tschechiens. Nicht zum eigentlichen Mähren zählen die Quellgebiete der Oder von Krnov und Opava gegen Ostrava, die historisch zum tschechischen Teil Schlesiens gehören.

Das "Statistische Jahrbuch der Österreichischen Monarchie für das Jahr 1864" gibt für das "Land" Mähren eine Fläche von 403,77 Geographischen Quadratmeilen an, was 22.233 km² entspricht.

Mähren grenzt im Norden an Polen und den tschechischen Teil Schlesiens, im Osten an die Slowakei, im Süden an Niederösterreich und im Westen an Böhmen. Die Nordgrenze bilden die Sudeten, die nach Osten und Südosten in die Karpaten übergehen. Das historische Dreiländereck mit Böhmen und Österreich befindet sich an der Spitze der Böhmischen Saß am Hohen Stein bei Staré Město pod Landštejnem (Dreiländerstein). An der Grenze zu Österreich fließt die stark mäandrierende Thaya; im Umkreis von Hardegg befindet sich der bilaterale Nationalpark Thayatal.

Den Kern des Landes (Höhenlage 180–250 m) bildet das Sedimentbecken der March und teilweise der Thaya. Im Westen (Böhmisch-Mährische Höhe) steigt das Land bis über 800 m, der höchste Berg ist jedoch der im Nordwesten liegende Altvater (1490 m) in den Sudeten. Südlich davon liegt das Hochland Niederes Gesenke (600–400 m), das bis zum Oberlauf der Oder (Mährische Pforte bei Hranice na Moravě) auf 310 m absinkt und weiter zu den Beskiden auf 1322 m (Kahlberg) ansteigt. Diese drei Gebirgsketten, mit der Pforte zwischen den letzten beiden, sind ein Teil der europäischen Wasserscheide. Die Ostgrenze bilden die Weißen Karpaten mit maximal (Velká Javořina).

Ein Teil der Mährer betrachtet sich als eigenständige Volksgruppe mit tschechischer Staatsangehörigkeit. Nach der letzten Erhebung im Jahre 2011 bekennen sich 630.897 Personen zum mährischen Volk (davon 108.423 Personen in einer sprachlichen Kombination, mehrheitlich als "mährisch-tschechisch"). Hinzu kommen Roma, Slowaken und die alteingesessenen Polen. Fast alle Angehörigen dieser ethnischen Minderheiten besitzen die tschechische Staatsbürgerschaft.

Bis 1945 bestand die Bevölkerung Mährens zu etwas mehr als einem Viertel aus Deutschmährern. Nach den Ergebnissen der österreichisch-ungarischen Volkszählung 1910 betrug der tschechische Bevölkerungsanteil an der damaligen Gesamtbevölkerung Mährens (2.622.000 Einwohner) 71,8 % und der deutsche Bevölkerungsanteil 27,6 %. Die Deutschmährer wurden 1945/46 infolge der sogenannten Beneš-Dekrete größtenteils enteignet und vertrieben.

In der Umgangssprache Mährens unterscheidet man verschiedene Dialekte, die sich charakteristisch von den böhmischen Dialekten bzw. von der tschechischen Schriftsprache unterscheiden.


Einen wesentlichen Aspekt, der die Wechsel in den Lebensverhältnissen Mitteleuropas weitgehend dominierte, stellten die großen Expansionsphasen der Gletscher dar, die als Eis- oder Kaltzeiten bezeichnet werden. In den langen Kältephasen lag Mähren am Rande eines von Menschen noch mit Mühe zu bewohnenden Korridors zwischen Asien und Westeuropa, eines nach heutigen Maßstäben tundrenhaften Gebietes, in dem die Jagd auf Großwild dominierte. Zugleich stellte Mähren eine Verbindung zwischen dem heutigen Polen, insbesondere Schlesien, und Niederösterreich in den weniger kalten Phasen dar. Archäologische Forschungen setzten dabei 1867 ein. 

Der älteste altpaläolithische Fundort in Mähren ist Stránská skála bei Brünn, der der Cromer-Warmzeit zugeordnet wird, die auf 850.000 bis 475.000 Jahre datiert wird. Die Steinwerkzeuge sind jedoch in ihrem Status als Artefakte teils umstritten, so dass die Anwesenheit von Menschen in dieser Zeit lediglich vermutet werden kann.

Als gesichert gilt hingegen die Besiedlung im Mittelpaläolithikum, nämlich in der Saale-Kaltzeit, genauer im Intra-Saale-Interglazial (vor etwa 200 000 Jahren). Die Lagerplätze befanden sich zunächst überwiegend im Freien, erst zu Beginn der Würm-Kaltzeit, also vor etwa 115.000 Jahren, zogen sich die Menschen in Höhlen zurück. Über lange Zeiträume bewohnt waren die Kúlna-Höhle und die Höhle von Moravský Krumlov. Die dort entdeckten Werkzeuge gehören den Industrien des Taubachien, des Moustérien und des Micoquien an. Aus dem Mittelpaläolithikum gibt es erste Hinweise auf Rohstoffe, die aus größerer Entfernung kamen und die auf nicht-utilitäre Handlungen hinweisen (Farbreste, symmetrische Gravuren oder ein Faustkeilblatt aus Bergkristall). Am Übergang vom Mittel- zum Jungpaläolithikum stehen zwei Industrien, die sich besonders in Mähren fanden: das Szeletien, das in Vedrovice auf 40.000–35.000 BP datiert wurde, und das Bohunicien aus Stránská skála (43.000–35.000 v. Chr.).

Schicht 3 und 4 in der Šipka-Höhle werden gleichfalls dem Mittelpaläolithikum zugeordnet. Der in dieser Höhle 1880 entdeckte Šipka-Unterkiefer, Überrest eines etwa zehnjährigen Kindes, wird dem späten Neandertaler zugeordnet. Er wurde auf älter als 40.000 BP datiert. In der Kúlna-Höhle wurden ebenfalls Spuren des Neandertalers entdeckt. Auch der 1905 entdeckte Unterkiefer von Ochoz wurde in diese Epoche eingeordnet, wenn auch Šipka und Ochoz Anzeichen aufweisen, die möglicherweise auf den anatomisch modernen Menschen ("Homo sapiens") hinweisen.

Die erste archäologische Kultur der aus Afrika zugewanderten Jäger und Sammler, die als "Homo sapiens" bezeichnet werden, lässt gleichfalls eine weiträumige Einbindung in Tauschbeziehungen erkennen. Zugleich bildeten sich regionale Eigenheiten, die den Geräten der Zeit die Bezeichnung "Morava River type" (Klima, 1978) bzw. "Miškovice type" (Oliva, 1990) eintrug. Die Konzentration von Fundstätten in Mähren ist dabei ungewöhnlich hoch, allerdings erfolgte sie vergleichsweise spät und wahrscheinlich anfangs in Interaktion mit der Neandertalerkultur des Szeletien. Realitätsnahe Darstellungen von Tieren entstanden erstmals im Aurignacien.

Aus dem Gravettien, das in Mähren "Padovien" genannt wird, sind zahlreiche Kunstwerke erhalten, die zugleich Werke der symbolischen Sphäre darstellen. So wurden dem Kind von Dolní Věstonice zwei gekreuzte Fuchszähne auf den Kopf gelegt. Zudem fand man Garnituren verzierter Zylinder und Plättchen in Dolní Věstonice I, doppelte Perlen in Předmostí und Pavlov, ebenso wie fein geschnitzte Ringe, vielleicht Fingerringe, in Pavlov I. Darüber hinaus wurde einigen Artefakten der Rang von Kunstwerken zugesprochen, wie etwa den zoomorphen Scheiben mit Öffnungen, mondförmigen Anhängern, Fibeln und so genannten „Stirnbändern“ aus Pavlov, oder verzierten Anhängern aus Předmostí. Insgesamt fanden sich derartige Kunstwerke nur in Předmostí, Dolní Věstonice I, Pavlov I und Petřkovice I. Dabei ließen sich weiträumige Tauschbeziehungen nachweisen. In Dolní Věstonice ließen sich Cherts nachweisen, die aus einer Abbaustätte in Südpolen stammten, die 180 km entfernt liegt, auch fand man in Mähren Obsidian aus einer 500 km entfernten, ungarischen Stätte.

Mähren weist dabei Eigenheiten des östlichen Gravettien auf, aber auch Besonderheiten, wie die besagten Stirnbänder oder Figurinen wie die Venus von Věstonice, deren Stil nur dort erscheint. Zahlreiche Figurinen aus gebranntem Ton finden sich nur in Dolní Věstonice I und Pavlov I. Eine überaus komplexe Gravierung auf dem Mammutstoßzahn aus Pavlov wurde von Bohuslav Klìma als eine Art „Karte“ der Landschaft unter den Pollauer Bergen gedeutet, was jedoch in Zweifel gezogen wurde. Bei den Tierdarstellungen werden zwar Mammute oder Pferde dargestellt, niemals jedoch Hasen, Wildschweine oder Boviden, die die wichtigsten Jagdtiere darstellten. In den Überresten einer Hütte fand man Fehlbrände von Figurinen; ähnliche Befunde kamen im niederösterreichischen Alberndorf zu Tage. Offenbar kamen gebrannte Tier- und Frauenfigurinen nur nordöstlich der Alpen vor. Es handelt sich um die ältesten keramischen Stücke der Menschheit. Gebrannt bei 500 bis 800 °C wurden sie anscheinend rituell zerbrochen. 

Mähren liegt am Ostrand des Magdalénien-Komplexes, seine Fundstellen ballen sich in den Höhlen des Mährischen Karstes. Dabei sind in Mähren 25 Fundstellen bekannt (Stand: 2016), allen voran die Pekárna-Höhle (Schichten G und H, 12.940 ± 250 BP und 12.670 ± 80 BP). Die Zahl der als Kunstwerke betrachteten Artefakte ist gering und beschränkt sich auf fünf Fundstätten. Schmuck aus Muscheln, durchbohrten Tierzähnen, Knochen, Stein und Lignit kommt in Mähren ebenso vor, wie in anderen Gebieten der Jäger- und Sammlerkulturen. Auf Knochen, Geweih, Schiefer und in Ausnahmefällen Mammutelfenbein fanden sich Gravierungen, meist handelt es sich um Darstellungen von Pferden, Wisenten, Bären, Rentieren und Saiga-Antilopen. Selten sind Darstellungen sehr häufiger Kleintier-Jagdbeute, aber auch Pflanzen- und Frauendarstellungen, letztere in sehr seltenen Fällen als Figurinen (Pekárna-Höhle), die dann eher geometrisch angelegt sind, ein Element, das auch bei Schiefergravierungen erscheint. Dabei bestehen Ähnlichkeiten mit den Frauendarstellungen des Fundplatzes Gönnersdorf (Typus Lalinde-Gönnersdorf), aber auch mit Funden aus dem Donauraum. Sie sind kopf- und fußlos, oft in Form von Anhängern gearbeitet.

In Mähren traten etwa ab 5700 v. Chr. erstmals bäuerliche Kulturen auf, die der Linearbandkeramik zugeordnet werden (bis 4900 v. Chr.). Auf diese folgte die Stichbandkeramik (4900–4700 v. Chr.), dann die sogenannte "Mährisch Bemaltkeramische Kultur" (4700–4000 v. Chr.), eine Gruppe der übergreifenden Lengyel-Kultur. Im Jahr 2008 waren mehr als 300 neolithische Siedlungen in Mähren bekannt.

Dabei bietet das früheste Neolithikum die größte Funddichte mit recht großen Friedhöfen (Vedrovice–"Široká u lesa" und "Za Dvorem" oder Kralice na Hané–"Kralický háj"), Begräbnisgruppen innerhalb von Siedlungen, aber auch isolierten Einzelgräbern (Brno–"Starý Lískovec", "Nový Lískovec", "Bohunice"), Begräbnisformen, die mit der bäuerlichen Kultur assoziiert sind. In "Široká u lesa" lag der Anteil der Frauen unter den 81 Toten bei 45 %, der der Männer bei 30 %. Bei Friedhöfen der Stichbandkeramik lag der Anteil von Männern und Frauen bei jeweils 25 %. Dabei waren linearbandkeramische Männer etwa 1,65 m groß, die Frauen 1,55 m. Die Stichbandkeramiker waren etwa zwei Zentimeter kleiner. Die Männer der Lengyel-Kultur waren im Schnitt 162,1 cm groß, die Frauen lagen bei 153,3 cm. Die meisten der neolithischen Individuen hatten ein Alter von 20 bis 35 Jahren erreicht; Anzeichen von längeren Phasen der Unterernährung ließen sich nachweisen. Auch ließen sich am Skelett Folgen schwerer oder einseitiger körperlicher Arbeit erkennen.

Mähren entwickelte sich schon in der vorgeschichtlichen Zeit beiderseits der Bernsteinstraße. Um 60 v. Chr. zogen die keltischen Boier aus dem Gebiet ab und wurden durch germanische Markomannen und Quaden ersetzt, welche um 550 n. Chr. zusammen mit den Rugiern in das Alpenvorland weiterzogen.

Im 6. Jahrhundert besiedelten die slawischen Mährer die Region. Im 7. Jahrhundert gehörte Mähren zum Reich des Samo. Anfang des 8. Jahrhunderts stand der südliche Teil im Einflussbereich der Awaren. Nachdem Karl der Große die Awaren vertrieben hatte, entstand gegen Ende des 8. Jahrhunderts im heutigen südöstlichen Mähren, Teilen der südwestlichen Slowakei (Záhorie) und später auch in Teilen Niederösterreichs das "Mährische Fürstentum". Aus ihm wurde im Jahre 833 durch die Eroberung des Fürstentums Nitra (die heutige Slowakei und Teile des nördlichen Ungarns) das Reich Großmähren, das später zeitweise auch verschiedene große Nachbargebiete (Teile Böhmens, Ungarns, des Weichsel-Gebiets u. a.) beherrschte. 863 berief der mährische Herrscher Rastislav die beiden byzantinischen Mönche Kyrill und Method, die das Christentum einführten.

Das Großmährische Reich unterlag um 907 im Kampf gegen die vordringenden Ungarn. Die heutige Slowakei wurde in das von der Dynastie der Arpaden beherrschte ungarische Fürstentum (später Königreich) eingegliedert und blieb bis 1918 unter dem Namen Oberungarn ein Land der Stephanskrone.

Das heutige Mähren war nach der verheerenden Niederlage gegen die Ungarn für kurze Zeit noch unabhängig und kam etwa 955 unter böhmische Oberhoheit. Nachdem es von 999 bis 1019 kurzzeitig von Polens Herrscher Boleslaw Chrobry regiert wurde, wurde Mähren 1031 endgültig böhmisch. Das Fürstentum und spätere Königreich Böhmen wurde von der Dynastie der Přemysliden regiert, und zwar bis zu ihrem Aussterben in männlicher Linie im Jahr 1305 (Ermordung des Königs Václav III.) Über längere Zeit bestanden in Mähren drei regionale Fürstentümer, deren Herrscher allesamt aus der Dynastie der Přemysliden stammten. Die Zentren dieser Fürstentümer waren Brünn ("Brno"), Olmütz ("Olomouc") und Znaim ("Znojmo").

Seit dem Jahr 1031 verläuft die mährische Geschichte fast ununterbrochen parallel zur Geschichte Böhmens. 1182 wurde Mähren zur Markgrafschaft erhoben und damit reichsunmittelbar, jedoch 1197 wieder der böhmischen Lehnshoheit unterstellt. Nach dem Aussterben der Přemysliden wurde das Königreich bis 1437 vom Haus Luxemburg regiert. Die Dynastien der Přemysliden und der Luxemburger stellten auch die mährischen Markgrafen, so u. a. den späteren König Ottokar II. Přemysl, den späteren König und Kaiser Karl IV. und dessen Neffen, den größtenteils selbständig herrschenden Markgrafen Jobst von Mähren. Während der bewegten Hussitenzeit blieben die meisten mährischen Adligen dem katholischen Glauben und dem böhmischen sowie ungarischen König und späteren Kaiser Sigismund von Luxemburg treu.

Auch während der Herrschaft des minderjährigen böhmischen Königs Ladislaus Postumus (eines Habsburgers, 1440–1457) und der Regentschaft und nachfolgenden Regierungszeit des utraquistischen Königs Georg von Podiebrad (1420–1471) blieb Mähren eher auf der katholischen Seite.

Im Jahre 1469 rückte der ungarische König Matthias Corvinus mit seiner Streitmacht nach Mähren ein, um seinen Schwiegervater Georg von Podiebrad, dessen Tochter Katharina er 1461 geheiratet hatte, als böhmischen König zu stürzen. Auf Wunsch der Grünberger Allianz ließ er sich in Olmütz 1469 zum böhmischen Gegenkönig wählen. Papst Paul II. unterstützte seinen Kampf gegen die Türken und die böhmischen "Häretiker". Der plötzliche Tod Podiebrads 1471 kam Matthias Corvinus bei der Durchsetzung seiner Ziele zu Hilfe. Matthias konnte aber das eigentliche Böhmen nie erobern, seine Herrschaft erstreckte sich nur über die böhmischen Nebenländer Mähren, Schlesien (mit Breslau), Ober- und Niederlausitz. Trotzdem nannte er sich seit 1469 böhmischer König und ließ sich 1471 krönen. Der Kampf um den böhmischen Thron wurde erst 1479 durch den Frieden von Olmütz beendet, in dem das Königreich Böhmen zeitweise unter Vladislav II. und Matthias Corvinus aufgeteilt wurde. In Böhmen selbst behauptete sich der von den dortigen Ständen erwählte Vladislav II. Jagellonský, der später auch die Nachfolge des Matthias Corvinus in Ungarn antreten sollte.

Es war noch der weitsichtige Georg von Podiebrad, der den Weg auf den böhmischen Thron für die beiden Jagiellonen, die Könige Vladislav II. und seinen Sohn Ludwig Jagellonský ebnete. Nach den langen Jahren der Herrschaft von Vladislav II. kam jedoch der plötzliche Tod (Ertrinken in einem Fluss) von Ludwig II. nach der Niederlage des ungarischen Heeres in der Schlacht bei Mohács (1526) gegen die Osmanen. Aufgrund der vorher geschlossenen Verträge trat nunmehr das Haus Habsburg die Herrschaft sowohl im Königreich Böhmen mit allen seinen Nebenländern als auch in Ungarn an. Das Königreich Böhmen und mit ihm die Markgrafschaft Mähren wurden von diesem Haus in der Folgezeit fast ununterbrochen bis 1918 regiert. Zunächst war noch Prag effektiver Regierungssitz, insbesondere in der Zeit des Königs und Kaisers Rudolf II., der selbst in Prag residierte. Nach 1621 wurden die böhmischen wie auch die mährischen Regierungsgeschäfte größtenteils nach Wien verlagert.

Bereits 1526 bildete sich im Raume Nikolsburg um Balthasar Hubmaier eine der ersten Gütergemeinschaften der radikal-reformatorischen Täuferbewegung. Die nach der Hinrichtung Hubmaiers 1528 drohende Auflösung der Wiedertäufergemeinde konnte der aus Tirol stammende Jakob Hutter verhindern. Nach ihm wurden die Wiedertäufer auch Hutterische Brüder genannt. Es lebten bis zu 60.000 Täufer in Mähren, davon 12.000 in Nikolsburg. Kurz nach den Wiedertäufern und gefördert durch den ansässigen Adel hielt auch die reformatorische Lehre Martin Luthers Einzug in Südmähren. Dadurch kam es zur Kirchenspaltung und zur Bildung der evangelisch-lutherischen Kirche und zu Konfessionen des Protestantismus. 

Während der Gegenreformation und der durch die von Jesuiten erfolgreich durchgeführten Rekatholisierung konnte eine ganze Anzahl von Kirchen wieder katholisch eingeweiht werden. Nach der Verfolgung in Mähren 1535 bis 1767 durch Katholiken, Evangelische und Türken floh ein Überrest von Täufern nach Russland.

Hauptstadt Mährens und Sitz der Markgrafen war seit der Herrschaft der Luxemburger bis 1641 das zentral gelegene Olmütz. Danach wurde das größere Brünn die Hauptstadt des Landes.

Als Markgrafschaft Mähren bildete das Land im Kaisertum Österreich bzw. seit 1867 in der westlichen Reichshälfte Österreich-Ungarns ein eigenes Kronland. Nach dem Ausscheiden Ungarns aus dem Kaisertum und der Schaffung der Realunion Österreich-Ungarn 1867 wurden die verbliebenen Kronländer amtlich als Cisleithanien bzw. "die im Reichsrat vertretenen Königreiche und Länder" bezeichnet.
Mähren wählte Abgeordnete in den Wiener Reichsrat und besaß einen eigenen Landtag und eine Landesausschuss genannte Landesregierung. Im Jahre 1905 wurde ein Kompromiss zwischen den beiden stärksten Ethnien in Mähren geschlossen, der als der Mährische Ausgleich in die Geschichte eingegangen ist, wonach die Landtagsabgeordneten der Deutschen und der Tschechen in ethnisch getrennten Wahlkreisen gewählt wurden. Im Sinne eines angestrebten österreichisch-tschechischen Ausgleichs zielte dieser Kompromiss auf ein konfliktfreies Zusammenleben der beiden Völker in Mähren. Tomáš Garrigue Masaryk meinte:
Mit der Gründung der Tschechoslowakei am 28. Oktober 1918 wurde Mähren Teil des neuen Staates, der sich einerseits als Nachfolgestaat des Königreiches Böhmen mit seinen Nebenländern verstand, andererseits auch die Idee des Tschechoslowakismus in die Wiege gelegt bekommen hatte. In seinen Grenzgebieten gab es jedoch seitens der deutschsprachigen Bevölkerung zunächst Bestrebungen, diese Landstriche von der Tschechoslowakei abzutrennen und den benachbarten Staaten, d. h. dem Deutschen Reich und der Republik Österreich (Deutschösterreich) anzugliedern. Alle diese Gebiete, so auch die von Deutschmährern besiedelten südmährischen Gebiete, wurden jedoch schnell von tschechoslowakischen Truppen besetzt. Die Teilnahme an der Wahl der Konstituierenden Nationalversammlung Deutschösterreichs Anfang 1919 wurde in Südmähren von der Tschechoslowakischen Republik verhindert. Letztlich trog auch die Hoffnung, das von US-Präsident Woodrow Wilson beworbene Selbstbestimmungsrecht der Völker würde sich bei den 1919 geführten Friedensverhandlungen (siehe: Friedensvertrag von Saint-Germain) zu Gunsten der deutschen Südmährer durchsetzen. Den Bestrebungen, das Gebiet der Republik Österreich de facto um historische Gebiete Mährens zu vergrößern, standen die beiden europäischen Siegermächte Vereinigtes Königreich und insbesondere Frankreich ablehnend gegenüber.

In der Tschechoslowakischen Republik behielt Mähren seine Stellung als Land. Den westlichen Teil der Tschechoslowakei bildeten die mehrheitlich tschechisch besiedelten Länder Böhmen und Mähren, deren Grenzgebiete jedoch einen hohen Anteil von Deutschböhmen und Deutschmährern aufwiesen, sowie das mehrheitlich von deutschsprachiger Bevölkerung besiedelte Land Mährisch-Schlesien. Das Gebiet des letztgenannten, relativ kleinen Landes entsprach dem Großteil des ehemaligen Österreichisch-Schlesiens. Mährisch-Schlesien, in dem auch viele ethnische Tschechen und Polen lebten, wurde nach einigen Jahren in die mährische Landesverwaltung eingegliedert. Die deutschsprachige Bevölkerung Böhmens, Mährens und Mährisch-Schlesiens sowie der Slowakei erhielt, wie auch die meisten anderen nicht zum offiziell proklamierten "tschechoslowakischen Volk" gehörenden Einwohner, die tschechoslowakische Staatsangehörigkeit. Die deutschsprachige Bevölkerung blieb bis zu ihrer Vertreibung (sog. "odsun") in den Jahren 1945 und 1946 zahlenmäßig weitgehend intakt. Über ihre staatsbürgerlichen, kulturellen und sonstigen Rechte und deren Wahrung in der sog. Ersten Tschechoslowakischen Republik wird noch immer kontrovers geurteilt.

In der Zeit des Nationalsozialismus gingen am 1. Oktober 1938 auf Grund des zu Lasten der Tschechoslowakei geschlossenen Münchner Abkommens überwiegend deutsch besiedelte Gebiete in Nord- und Südmähren an das Deutsche Reich über und wurden bis zum 10. Oktober militärisch besetzt. Diese Gebiete wurden damals (und werden in der Literatur außerhalb Tschechiens vielfach auch heute) zusammen mit den deutsch besiedelten Randgebieten Böhmens unter dem Begriff Sudetenland subsumiert. Die restlichen, ganz überwiegend von Tschechen besiedelten Gebiete Böhmens und Mährens wurden am 15. März 1939 von der deutschen Wehrmacht besetzt und von Nazi-Deutschland zum Protektorat Böhmen und Mähren erklärt.

Am 14. April 1939 wurde das nordmährische Annexionsgebiet dem neu gebildeten Reichsgau Sudetenland zugeteilt. Das südmährische Gebiet wurde dem Reichsgau Niederdonau, dem vormaligen Niederösterreich, zugeschlagen. Im Nordosten des tschechoslowakischen Landes Mähren-Schlesien wurde 1938 ein kleiner, historisch zu Österreichisch-Schlesien gehörender Gebietsteil (Olsagebiet) Polen angeschlossen, nach dessen Besetzung im Herbst 1939 dann dem deutschen Oberschlesien.
Die so genannten Sudetendeutschen bzw. Deutschmährer waren seit 1938 deutsche Staatsbürger durch Sammeleinbürgerung. und hatten in der Wehrmacht zu dienen; 1945 diente unter anderem dies als Argument für ihre Vertreibung.

Die mährischen Ressourcen und Industriebetriebe wurden für die deutsche Kriegswirtschaft genutzt. Die tschechische Bevölkerung sollte nach dem Krieg zum Teil germanisiert, zum Teil ausgesiedelt werden. Die tschechische Protektoratsregierung in Prag war vom „Reichsprotektor“, wie der oberste deutsche Funktionär im Gebiet genannt wurde, völlig abhängig. Bereits vor dem Attentat auf den stellvertretenden Reichsprotektor Reinhard Heydrich wurde gegen die tschechische Bevölkerung des Protektorats mit sehr repressiven Maßnahmen vorgegangen. Nach diesem gelungenen Attentat wurden im ganzen Protektorat mehrere Hundert Personen, die mit dem Attentat selbst in keiner Verbindung standen, widerrechtlich zum Tode verurteilt und hingerichtet.

Die tschechischen Universitäten und Hochschulen im gesamten Protektorat Böhmen und Mähren wurden bereits im Jahr 1939 von der Besatzungsmacht verboten und aufgelöst. Die tschechischen Gymnasien und sonstigen Bildungseinrichtungen unterhalb des Hochschulniveaus durften bis 1945 bestehen bleiben. Hingegen funktionierte z. B. die Deutsche Technische Hochschule Brünn während der gesamten Protektoratszeit bis 1945.

Nach dem Ende des Zweiten Weltkrieges am 8. Mai 1945 kamen die durch das Münchener Abkommen von 1938 an das Deutsche Reich (bzw. 1938 an Polen und dann 1939 an das Deutsche Reich) gelangten Territorien wieder zur Tschechoslowakei zurück.

In den Jahren 1945 und 1946 entlud sich die während der Protektoratszeit ständig gewachsene Spannung zwischen den tschechischen und deutschen Bevölkerungsteilen. Bis auf eine relativ kleine Anzahl von Personen, etwa aus ethnisch gemischten Ehen oder wichtigen Berufen, wurden die deutschmährischen Bürger, beginnend bereits Mitte Mai 1945, sowohl spontan als auch vorsätzlich "wild" über die Grenze nach Österreich und Deutschland vertrieben. Andere wiederum flüchteten vor den Misshandlungen und Gewalttaten. Die Alliierten nahmen am 2. August 1945 im Potsdamer Protokoll, Artikel XIII, zu den wilden und kollektiv verlaufenden Vertreibungen der deutschen Bevölkerung konkret nicht Stellung. Explizit forderten sie jedoch einen „geordneten und humanen Transfer“ der „deutschen Bevölkerungsteile“, die „in der Tschechoslowakei zurückgeblieben sind“. Zwischen dem Februar und Oktober 1946 erfolgte die offizielle "ordnungsgemäße und humane" Zwangsaussiedlung der deutschen Bürger aus Mähren. Im Bericht von Francis E. Walter an das Repräsentantenhaus wurde vermerkt, dass die Transporte keineswegs dieser Bestimmung entsprachen. Alles private und öffentliche Vermögen der deutschen Mährer wurde durch das Beneš-Dekret Nr. 108 konfisziert, das Vermögen der deutschen evangelischen Kirche durch das Beneš-Dekret Nr. 131 liquidiert.

Die Zahl der Vertreibungstoten, deren Namen und Schicksal bekannt sind, wird für Südmähren mit 637 Personen angegeben. Weiters forderte der „Brünner Todesmarsch“ (30. Mai 1945) nach heutigen Erkenntnissen zwischen 1700 und 5200 Menschenleben, davon sollen mindestens 890 im Lager Pohořelice ums Leben gekommen sein. Eine juristische Aufarbeitung des Geschehens hat nicht stattgefunden. Das Beneš-Dekret Nr. 115/1946 erklärte bis 28. Oktober 1945 begangene Handlungen "im Kampfe zur Wiedergewinnung der Freiheit ..., oder die eine gerechte Vergeltung für Taten der Okkupanten oder ihrer Helfershelfer zum Ziel hatte, ..." für nicht widerrechtlich.

Die noch heute in Mähren lebenden Angehörigen der deutschen Minderheit in Tschechien, für die oft der Überbegriff Sudetendeutsche verwendet wird, identifizieren sich selbst häufig nicht mit dieser Bezeichnung.

Die nach 1945 ausschließlich vom tschechischen Klerus geführte römisch-katholische Kirche wurde während der kommunistischen Ära in allen Landesteilen der Tschechoslowakei weitgehend enteignet. Eine Entschädigung ihres Vermögens ist seitens der Tschechischen Republik bisher nicht erfolgt. Sie ist jedoch nach langen parlamentarischen Beratungen im Jahr 2013 für alle anerkannten kirchlichen Gemeinschaften gesetzlich verankert worden, so dass vielfach mit der Rückgabe des konfiszierten Eigentums der Kirchen gerechnet wird.

Seit dem 1. Januar 1993 ist Mähren integraler Bestandteil der Tschechischen Republik, eines der zwei Nachfolgestaaten der Tschechoslowakischen Föderativen Republik, die zum Jahresende 1992 einvernehmlich und völkerrechtsverbindlich aufgelöst wurde.

Der böhmische König, gleichzeitig Markgraf von Mähren und Kaiser des Heiligen Römischen Reiches Karl IV. begann in der Mitte des 14. Jahrhunderts, sein Königreich in große Verwaltungseinheiten einzuteilen. Eine solche Verwaltungseinheit hieß in den Urkunden auf deutsch "Kreis", auf tschechisch "kraj" und auf lateinisch "circulus". In Mähren bestanden zwischen zwei und sechs solcher Kreise.

Die Anzahl der Kreise und somit auch deren Größe änderten sich mehrmals. Diese Kreiseinteilung galt bis 1862, spielte aber schon kurz nach der Revolution von 1848 praktisch keine Rolle mehr für die Verwaltung.

Ab 1850 wurden in allen Gebieten der Monarchie außer Ungarn die alten großen Kreise durch politische Bezirke (der Exekutive) ersetzt, von denen jeder aus einem oder mehreren Gerichtsbezirken (der Judikative) bestand. In den österreichischen Bundesländern besteht diese Einteilung bis heute. Normalerweise war ein politischer Bezirk (tschechisch: "politický okres") kleiner als ein ehemaliger alter Kreis, und ein Gerichtsbezirk (tschechisch: "soudní okres") ist kleiner als ein Politischer Bezirk. Mähren hatte 32 politische Bezirke.

Die nachfolgende Bezirkeinteilung galt, abgesehen von kleineren Änderungen und der Neuschaffung dreier Bezirke (Bärn, Mährisch Ostrau und Wesetin), auch in der Ersten Tschechoslowakischen Republik weiter:

Für die gleichzeitige Entwicklung in Böhmen und der Slowakei, siehe "Okres".

Aufgrund des Münchner Abkommens vom 29. September 1938 wurde der vorwiegend deutschsprachige Teil Nordmährens dem Reichsgau Sudetenland des Deutschen Reichs zugeschlagen, südmährische Gebiete mit deutscher Bevölkerungsmehrheit wurden dem Reichsgau Niederdonau angegliedert. Das annektierte Gebiet wurde in Stadt- und Landkreise eingeteilt; übergeordnet waren Regierungsbezirke. Der restliche Teil Mährens im "Protektorat Böhmen und Mähren" blieb weiterhin in politische Bezirke und Gerichtsbezirke eingeteilt, wobei allerdings über je einer Gruppe von politischen Bezirken noch ein Oberlandratsbezirk eingeführt wurde.

Im gesamten Reichsgau Sudetenland gab es fünf Stadtkreise und 52 Landkreise. Im Protektorat Böhmen und Mähren gab es 67 böhmische und 30 mährische politische Bezirke. Diese Verwaltungsgliederung galt bis zum Ende des Zweiten Weltkrieges.

Die Gebiete der heutigen tschechischen "Okresy" (diese wurden jedoch formell aufgelöst und spielen nurmehr in der NUTS-Gliederung eine Rolle) bzw. "Kraje" spiegeln nur teilweise die Gebiete der historischen Länder wider. Einige Bezirke umfassen aus historischer Sicht sowohl mährische als auch schlesische Gebiete oder vielfach mährische und böhmische Gebiete. Das historische Mähren ist heute auf folgende Bezirke verteilt (vom Westen nach Osten und vom Norden nach Süden): Ostteil des Bezirkes Pardubice, Südostteil des Südböhmischen Bezirkes, die Osthälfte des Bezirkes Vysočina, der gesamte Südmährische Bezirk, Mehrheit des Olmützer Bezirkes, Teile des Mährisch-Schlesischen Bezirkes sowie der gesamte Bezirk Zlín.

Im Süden bei Hodonín und Břeclav hat Mähren Anteil am Wiener Becken, in dessen tieferen Sedimenten nach Erdöl, Erdgas und Lignit gebohrt wird. Es gab dem Moravikum seinen geologischen Namen. Bei Ostrava (Nordosten) wurde bis etwa 1995 intensiv Steinkohle abgebaut.

Als wichtige Industriezweige sind in Mähren Eisen- und Stahlindustrie, Maschinenbau, Chemische Industrie wie auch die Herstellung von Bekleidung, Leder und Baustoffen hervorzuheben. Wichtige Wirtschaftszentren sind Brno (früher "mährisches Manchester" genannt), Olomouc, Ostrava und Zlín. Die moderne Stadt Zlín wurde in der ersten Hälfte des 20. Jahrhunderts von den Tomáš Baťa-Werken geprägt. Das nordmährischen Schwerindustrie- und Bergbaugebiet gehörte bereits zu der Zeit der österreichisch-ungarischen Monarchie zu den wichtigsten Industrieregionen Europas. In einer der modernsten Autofabriken weltweit, gelegen in der Industriezone Nošovice (Kreis Frýdek-Místek), werden seit September 2009 Personenwagen des koreanischen Konzerns Hyundai Motor Company überwiegend für den Export auf westeuropäische Märkte hergestellt.

Die Wirtschaft Tschechiens und somit auch Mährens ist seit den umfangreichen Privatisierungen und Restitutionen der frühen 90er Jahre des 20. Jahrhunderts fast ausschließlich privatwirtschaftlich organisiert. Sie ist vor allem auf die Märkte in der Europäischen Union ausgerichtet, davon zum erheblichen Teil auf die Märkte Deutschlands und Österreichs. Dies ist auch dadurch bedingt, dass zahlreiche Betriebe sich im Besitz von Unternehmen aus EU-Ländern befinden.

Neben der intensiven, zu Teil großflächigen Landwirtschaft (Getreide, Raps, Zuckerrüben usw.) im Gebiet der Hanna (Mähren), in der Mährischen Slowakei und in anderen Gebieten ist Mähren für seinen Weinbau und den Obst- und Gemüsebau bekannt. Das südmährische Weinanbaugebiet bringt rund 90 % des in Tschechien produzierten Weines hervor. In den letzten 23 Jahren, d. h. seit der Privatisierung der gesamten Wirtschaft, wurden im Weinanbau große Fortschritte im Hinblick auf die Qualität der hauptsächlich erzeugten Weiß- wie auch der Rotweine erreicht.


Urgeschichte




</doc>
<doc id="14381" url="https://de.wikipedia.org/wiki?curid=14381" title="Kanton (Schweiz)">
Kanton (Schweiz)

Die 26 Kantone ( in der Deutschschweiz traditionell auch Stand, im Plural Stände genannt) sind die Gliedstaaten der Schweizerischen Eidgenossenschaft. Der Ausdruck "Kanton" wurde 1475 zum ersten Mal in einer Freiburger Akte verwendet.

Jeder Kanton hat eine eigene Verfassung und eigene gesetzgebende, vollziehende und rechtsprechende Behörden. Alle Kantone besitzen ein Einkammer-Parlament (Grosser Rat, Kantonsrat, Landrat, Parlament; "siehe auch:" Kantonsparlament). Dieses hat je nach Kanton 49 bis 180 Parlamentssitze. Die Kantonsregierung (Regierungsrat, Regierung, Staatsrat, Standeskommission) besteht je nach Kanton aus fünf oder sieben Mitgliedern. In jedem Kanton existiert ein zweistufiges Gerichtssystem (erste Instanz: Bezirksgericht, Amtsgericht, Kantonsgericht, Kreisgericht, Landgericht, Regionalgericht, Strafgericht, Zivilgericht; zweite Instanz: Obergericht, Kantonsgericht, Appellationsgericht), dem eine Schlichtungsbehörde (Friedensrichteramt, Vermittleramt) vorangestellt ist.

Alle staatlichen Bereiche, die nicht von der schweizerischen Bundesverfassung dem Bund zugewiesen bzw. von einem Bundesgesetz geregelt werden, gehören in die Kompetenz der Kantone, beispielsweise kantonales Staats- und Verwaltungsorganisationsrecht, Schulwesen, Sozialhilfe, Baurecht, Polizeiwesen, Notariatswesen, kantonales und kommunales Steuerrecht, zu grossen Teilen auch Gesundheitswesen, Planungsrecht, Gerichtsverfassung und anderes. In vielen Bereichen verfügen sowohl der Bund als auch die Kantone über Kompetenzen. Kantone sind wie die deutschen Länder derivative Völkerrechtssubjekte und können innerhalb ihrer Kompetenzen Staatsverträge untereinander (sogenannte Konkordate) oder mit fremden Staaten schliessen.

Die Kantone ihrerseits gewähren ihren Gemeinden eine gewisse Autonomie. Diese ist in der östlichen Schweiz tendenziell grösser als in der westlichen.

In zwei Kantonen – Glarus und Appenzell Innerrhoden – erlässt das Volk die kantonalen Gesetze an einer Versammlung aller Bürger, der Landsgemeinde. Im Kanton Appenzell Innerrhoden werden an der Landsgemeinde überdies die Mitglieder der kantonalen Regierung und der kantonalen Gerichte gewählt. In allen anderen Kantonen finden Wahlen und Abstimmungen an der Urne statt.

Die sogenannten Urkantone, welche 1291 die Eidgenossenschaft begründet haben sollen, sind Uri, Schwyz und Unterwalden. In der Alten Eidgenossenschaft wurden die Kantone noch "Orte" genannt. Deshalb spricht man in Bezug auf die Ausweitungsphasen der Schweiz von den Acht Alten Orten und den Dreizehn Alten Orten (bzw. der achtörtigen und der dreizehnörtigen Eidgenossenschaft). Verbündete, welche nicht Vollmitglied der Eidgenossenschaft waren, wurden als zugewandte Orte bezeichnet. Die Vollmitglieder und erst recht die zugewandten Orte der Eidgenossenschaft waren noch eigenständige Staatengebilde.

Mit der Helvetischen Republik (1798–1803) bekam die Bezeichnung "Kanton" eine gewichtigere Verwendung, auch wenn der Ausdruck in der alten Eidgenossenschaft schon seit 1475 als Synonym für "Ort" und "Stand" verwendet werden konnte. Im neu geschaffenen Einheitsstaat waren die Kantone jedoch blosse Verwaltungsbezirke ohne Autonomierechte. Die Grenzziehung wurde geändert, um annähernd gleich grosse Kantone zu schaffen und die alte Ordnung zu zerschlagen. Dabei entstanden auch die kurzlebigen Kantone Säntis, Linth, Waldstätte, Oberland, Baden, Lugano und Bellinzona, ab 1802 für ein Jahr auch noch der Kanton Fricktal.

Mit der Mediationsverfassung 1803 erhöhte sich die Zahl der Kantone auf 19 und mit dem Wiener Kongress 1815 auf 22. Zugewandte Orte wie zum Beispiel die Republik Gersau, das Gebiet der Abtei Engelberg und Weitere wurden teilweise gegen ihren Willen einzelnen Kantonen zugeschlagen. 1833 spaltete sich der Kanton Basel-Landschaft in einem bewaffneten Konflikt vom Kanton Basel-Stadt ab, in der gleichen Zeit auch der Kanton Ausserschwyz von Schwyzer Zentrum (was aber nicht Bestand hatte). Die bislang letzten Spuren der Gebietszuteilungen des Wiener Kongresses wurden 1979 mit der Gründung des Kantons Jura und dem Übertritt des bernischen Amtsbezirks Laufen zum Kanton Basel-Landschaft, beides Abspaltungen vom Kanton Bern, auf demokratischem Weg bereinigt; die Zukunft des Berner Juras könnte nochmal zu Veränderungen führen.

Als 1848 ein Bundesstaat gegründet wurde, wurde die Souveränität der Kantone eingeschränkt, und Bereiche wie Aussenpolitik, Zölle, Währung und Postwesen gingen an die Bundesgewalt über. Mit Industrialisierung und Wirtschaftswachstum wurde das staatliche Leben zunehmend komplexer, was weitere Zentralisierungen erforderlich machte und in Gebieten wie Zivilrecht, Strafrecht, Handels- und Wirtschaftsrecht zu einer Vereinheitlichung des materiellen Rechts führte. Heute sind die Bereiche, in denen die Kantone wirklich noch autonom legiferieren können, ziemlich begrenzt. Es wird zunehmend von «Vollzugsföderalismus» gesprochen.

Heute wird die Zahl der Kantone mit 26, manchmal noch mit 23 angegeben.
Der Grund ist, dass sechs Kantone (Obwalden, Nidwalden, Appenzell Innerrhoden, Appenzell Ausserrhoden, Basel-Stadt und Basel-Landschaft) aus historischen Gründen gelegentlich noch als Halbkantone bezeichnet werden. Seit der Totalrevision der Bundesverfassung von 1999 gelten sie als Kantone mit halber Standesstimme. Diese Unterscheidung ist lediglich bei der Besetzung des Ständerates und beim Ständemehr relevant und hat keinen Einfluss auf die innere Autonomie.

Die offizielle Reihenfolge der Kantone (siehe untenstehende Liste), wie sie auch in Artikel 1 der Bundesverfassung vorkommt, geht auf die Zeit vor der Gründung des Bundesstaates zurück. Genannt werden zunächst die drei Vororte der Zeit zwischen 1815 und 1848, gefolgt von den weiteren Kantonen in der Reihenfolge ihres Beitritts.

Die zweibuchstabigen Kantonsabkürzungen (Siglen) sind verbreitet, sie sind unter anderem für die Autokennzeichen vorgeschrieben und werden in der verwendet (mit dem Präfix «CH-», zum Beispiel "CH-SZ" für den Kanton Schwyz).

Die Kantonsnamen werden in der Schreibweise und der offiziellen Reihenfolge wiedergegeben, wie sie in Artikel 1 der Bundesverfassung zu finden sind. Hervorgehoben sind die Bezeichnungen in den jeweiligen Amtssprachen in der amtlichen Vollnamensform. Die Bezeichnungen im Schweizerdeutschen sind nicht bindend, da es für sie keine offizielle Rechtschreibung gibt. Im Folgenden wird für ganz oder teilweise deutschsprachige Kantone diejenige Dialektlautung angegeben, die in der jeweiligen Regionalmundart gilt.

Als Binnenkantone werden die Kantone bezeichnet, die nicht ans Ausland grenzen. Die Schweiz hat elf Binnenkantone: Appenzell Ausserrhoden, Appenzell Innerrhoden, Bern (seit dem Wechsel des bernischen Amtsbezirks Laufen zum Kanton Basel-Landschaft im Jahre 1994), Freiburg, Glarus, Luzern, Nidwalden, Obwalden, Schwyz, Uri und Zug. Nid- und Obwalden werden ausschliesslich von Binnenkantonen umgeben.


Die Vergrösserung der Schweiz durch die Aufnahme weiterer Gebiete als Kantone endete 1815.

Nach dem Ende der Habsburgermonarchie gab es Bestrebungen, das österreichische Bundesland Vorarlberg als Kanton in die Schweiz aufzunehmen. Die Volksabstimmung 1919 in Vorarlberg ergab, dass eine Mehrheit von gut 80 Prozent der Vorarlberger den Beitritt zur Schweiz befürwortete. Das Vorhaben scheiterte jedoch an der zögerlichen Politik der (provisorischen) Vorarlberger Landesversammlung und am Schweizer Bundesrat, der das sorgsam austarierte Verhältnis zwischen den Sprachen der Schweiz und den Konfessionen in der Schweiz nicht durch einen zusätzlichen Kanton mit deutschsprachigen Katholiken ins Ungleichgewicht bringen wollte, sowie an den Friedensverträgen mit der Entente.

2010 forderte Dominique Baettig, ein Abgeordneter der rechtsbürgerlichen Schweizerischen Volkspartei (SVP) im Nationalrat, einen gesetzlichen Rahmen, um an die Schweiz angrenzende Regionen als weitere Kantone in die Schweiz aufnehmen zu können. Der Bundesrat lehnte dies ab, da ein solcher Erlass einen unfreundlichen politischen Akt darstellen würde, den die Nachbarstaaten als Provokation auffassen könnten.





</doc>
<doc id="14382" url="https://de.wikipedia.org/wiki?curid=14382" title="Geschichte des Fernsehens">
Geschichte des Fernsehens

"Vorbemerkung": Obgleich im Artikel Geschichte des Fernsehens eine chronologische Erzählweise überwiegt, ist er in erster Linie thematisch gegliedert. Eine chronologische Auflistung der Ereignisse findet sich unter Chronologie des Fernsehens. Außerdem stellt der folgende Artikel die technischen Innovationen nicht für jedes Land gesondert dar. Der Artikel beschäftigt sich überwiegend mit den Entwicklungen im deutschsprachigen Raum.

Auf die Möglichkeit, Bilder punkt- und zeilenweise abzutasten und die Helligkeitswerte elektrisch zu übertragen, sowie den Nutzen einer derartigen Technik wies Alexander Bain schon 1843 hin. Die erste brauchbare Realisierung erfand 1883 Paul Nipkow. Sein elektrisches Teleskop, welches mit Hilfe einer rotierenden Scheibe (Nipkow-Scheibe), die mit spiralförmig angeordneten Löchern versehen war, Bilder in Hell-Dunkel-Signale zerlegte beziehungsweise wieder zusammensetzte meldete er am 6. Januar 1884 zum Patent an. Nach den Ideen von Paul Nipkow gelangen Anfang des 20. Jahrhunderts die ersten Fernsehbildübertragungen. Paul Nipkow wird deshalb als der Erfinder der ersten praktischen Realisierung des Fernsehens bezeichnet.

Nipkow selbst hat seine Idee jedoch nie verwirklicht, es gab zur damaligen Zeit noch keine geeignete Verstärkungsmöglichkeit, auch war die damals einzige bekannte lichtempfindliche Zelle, die Selenzelle, zu träge für Fernsehübertragungen.

1897 entwickelten Ferdinand Braun und Jonathan Zenneck die Kathodenstrahlröhre, auch „Braunsche Röhre“ genannt. Mittels eines Elektronenstrahls und seiner Steuerung durch elektrostatische Ablenkplatten oder elektromagnetische Spulen ließen sich aufeinanderfolgende Bildpunkte auf eine mit Leuchtstoff beschichtete Glasscheibe projizieren. Die Kathodenstrahlröhre fand ihre ersten Anwendungen in Messapparaturen (z. B. Oszilloskope). Bis Anfang der 2000er Jahre ständig weiterentwickelt, bildete sie lange Zeit die Grundlage zur Darstellung von Fernsehbildern. 1906 benutzte Max Dieckmann eine "Braunsche Röhre" zur Wiedergabe von 20-zeiligen schemenhaften Schattenbildern im Format 3 × 3 cm. 1907 gelang dem Russen Boris Rosing die erste Übertragung und der Empfang eines schemenhaften Fernsehbildes, wofür er in vielen Ländern, darunter auch in Deutschland, ein Patent erhielt. Auch Campbell Swinton verwendete 1911 eine Kathodenstrahlröhre zur Bildwiedergabe.

Wladimir Kosmitsch Sworykins Experimente führten zur Entwicklung des Ikonoskop, einer ersten brauchbaren Bildaufnahmeröhre. Damit stand erstmals für das senderseitige Verfahren der Bildzerlegung eine elektronische Lösung zur Verfügung. Sworykin, der ein Schüler von Boris Rosing war, beantragte 1923 für sein Ikonoskop ein Patent.

In der Literatur wird mehrfach berichtet, dass Dénes von Mihály 1919 einfache Bilder über mehrere Kilometer hinweg übertrug. Ob er dazu bei der Bildzerlegung ein Verfahren der Optomechanik oder ein elektronisches anwandte, ist allerdings bisher nicht belegbar. Bekannt ist nur, dass er Bairds Verfahren der Bildzerlegung als Provisorium betrachtete. 1925 gelang Dieckmann erneut eine Bildwiedergabe mit einer Braunschen Röhre in München. August Karolus entwickelte den nach ihm benannten Telefunken-Karolus-Bildtelegraphen. Seine Bildvorführungen die er mit Hilfe der von ihm verbesserten Kerr-Zelle erzielte, beruhten auf der Ausnutzung des elektrooptischen Kerr-Effekts. Karolus erreichte 1925 damit eine Bildübertragung von Berlin nach Leipzig.

Alle diese Versuche hatten mit den in den 1920er-Jahren von John Logie Baird in Großbritannien sowie Herbert E. Ives und Charles Francis Jenkins in den USA unternommenen Vorführungen gemeinsam, dass mechanische Bildzerleger eingesetzt wurden. Baird hatte hierzu Nipkows Erfindung zu einer wirkungsvolleren Scheibe weiterentwickelt. Am 26. Januar 1926 fand in London durch J. L. Bairds die weltweit erste Fernsehvorführung statt. 1927 übertrug Baird ein Fernsehsignal zwischen Glasgow und London und am 8. Februar 1928 überbrückte seine Fernsehtechnik mit mechanischer Bildzerlegung, bereits den Atlantik. Seine Versuchsendungen setzte 1931 die BBC fort.

1926 experimentierte Kenjiro Takayanagi mit Hilfe Baird´s Art der Bildzerlegung, benutzte aber zur Wiedergabe der Bilder eine Elektronenstrahlröhre, ähnlich der in Kálmán Tihanyi´s (weiter unten erwähnten) "Radioskop". Er bildete das zuvor aufgenommene Katakana-Schriftzeichen イ , auf einer Braunschen Röhre ab. Die erste vollelektronische Übertragung von Bildern mit Elektronenstrahlröhren auf Sender- und Empfangsseite gelang Philo Farnsworth am 7. September 1927.

Der ungarische Erfinder Kálmán Tihanyi verbesserte die Empfindlichkeit der Kathodenstrahlröhre und erfand 1928 das "Radioskop", ein vollständiges elektronisches Fernseh-System bestehend aus einer Kamera, einer Bildaufnahmeröhre (aufnahmeseitig ähnlich Sworykins "Ikonoskop") und einem Fernsehgerät.
Am 11. Mai 1928 präsentierte der Ungar Dénes von Mihály in Berlin einem kleinen Kreis mit seinem Telehor die erste Fernsehübertragung in Deutschland. Im selben Jahr stellte auch August Karolus auf der 5. Großen Deutschen Funk-Ausstellung Berlin seine Fernsehanlage vor; das Empfangsbild des Telefunken-Prototyps hatte eine Größe von 8×10 cm und eine Auflösung von etwa 10.000 Bildpunkten. Dénes von Mihálys Telehor lieferte mit einer Bildgröße von 4×4 cm und nur 900 Bildpunkten eine wesentlich schlechtere Bildqualität, erzielte aber eine größere öffentliche Resonanz: Im Gegensatz zum unverkäuflichen Telefunken-Prototyp versuchte von Mihály, sein Gerät zu verkaufen. Angesichts von nur stundenweisen Versuchssendungen über einige Sender der Reichspost (DRP), der eher schlechten Bildqualität, den hohen Gerätepreisen und vor allem der 1929 beginnenden Weltwirtschaftskrise ein eher aussichtsloses Unterfangen. Dennoch gilt der 31. August 1928 als "Startdatum des Fernsehens" in Deutschland.

Anfang der 1930er-Jahre gab es praktisch nur mechanisches Fernsehen. Die Kathodenstrahlröhre galt zunächst als zu kompliziert und zu teuer. Man versprach sich jedoch durch ein vollelektronisches Fernsehsystem eine wesentlich höhere Bildauflösung. In Deutschland präsentierte Manfred von Ardenne auf der Deutschen Funkausstellung 1931 erstmals öffentlich ein vollelektronisches Fernsehen mit Kathodenstrahlröhre („Weltpremiere des elektronischen Fernsehens“).

Dennoch konkurrierten noch über 1937 hinaus mechanische Fernsehsysteme mit dem elektronischen Fernsehen. Insbesondere die Fernseher mit Spiegelschraube von der Firma TeKaDe konnten durch große Helligkeit und Bildschärfe überzeugen. Erst nach Erhöhung der Zeilenzahl auf 441 Zeilen wurde der mechanische Fernseher in der Herstellung unwirtschaftlich. Einzig die britische Firma Scophony baute noch bis zum Beginn des Zweiten Weltkrieges mechanische Fernseher für 405 Zeilen, bzw. 441 Zeilen nach einer damaligen Norm in den USA.

1929 begann der "Rundfunksender Witzleben" (Berliner Funkturm) mit ersten deutschen regelmäßigen Testsendungen. Die ersten Fernsehbilder wurden von ihm am 8. März zu Testzwecken in das Fernsehlaboratorium der Post übertragen. Kurz darauf setzte die DRP die erste deutsche Fernseh-Norm fest: Zerlegung des Bildes in 30 Zeilen (= 1200 Bildpunkte) bei 12,5 Bildwechseln pro Sekunde. Die Norm wurde der technischen Entwicklung angepasst:

Kurz vor Aufnahme der ersten Versuchsendungen in Deutschland begann in Großbritannien auch John Logie Baird in den Nachtstunden mit einem regelmäßigen Versuchsprogramm auf Sendern der BBC. Die dortige Fernsehnorm betrug bis 1935 30 Zeilen, vertikal in einem Seitenverhältnis 3 zu 7, 12,5 Bilder pro Sekunde. 1936 begann auch in Großbritannien das Zeitalter des hochauflösenden Fernsehens, es wurde zunächst im Versuchsbetrieb im wöchentlichen Wechsel in dem System Bairds mit 240 Zeilen und nach dem System der Marconi Company mit 405 Zeilen gesendet. Bereits im Februar 1937 wurde das System mit 405 Zeilen, 25 Bilder pro Sekunde, Bildseitenverhältnis zunächst 5 zu 4 festgelegt (1950 wurde auf ein Seitenverhältnis von 4 zu 3 gewechselt). Das später als Fernsehnorm A festgelegte System blieb bis 1965 der alleinige Standard in Großbritannien, ab 1965 wurde es zunächst durch die europäische CCIR-Norm mit 625 Zeilen ergänzt, ab 1985 komplett abgelöst.

Auch in anderen europäischen Staaten gab es Fernsehversuchssendungen. In den Niederlanden gab es auf Privatinitiative ab 1934 einen regelmäßigen Fernsehversuchsdienst in der englischen Norm mit 30 Zeilen, welcher bis September 1939 (Beginn des Zweiten Weltkrieges) in Betrieb blieb. 1949 wurde das Fernsehen nach der CCIR-Norm mit 625 Zeilen eingeführt.

In Frankreich wurde ebenfalls Fernsehversuchsendungen durchgeführt. Um 1937 hatte man auch ein serienreifes hochauflösendes Fernsehsystem entwickelt, welches zunächst mit 437 Zeilen sendete. Nach der Besetzung von Paris durch die Wehrmacht wurde der Sender auf dem Eiffelturm beschlagnahmt und auf die deutsche Norm mit 441 Zeilen umgestellt. (Das Programm war mit französischen Empfängern ohne Probleme empfangbar). Ausgestrahlt wurde ein Programm hauptsächlich für verwundete Soldaten in Lazaretten in Paris und Umgebung. Frankreich war das einzige Land Europas, in dem auch während des Krieges ununterbrochen Fernsehen empfangen werden konnte, in Deutschland mussten nach einem Bombenangriff 1944 die Sendungen eingestellt werden. In Großbritannien wurde das Fernsehprogramm nach Kriegsausbruch 1939 eingestellt und erst 1946 wieder aufgenommen.

In der Sowjetunion begannen öffentliche Versuchsendungen in den Städten Leningrad und Moskau ebenfalls bereits in den 1930er Jahren, überwiegend mit in den USA eingekaufter Technik. Die Entwicklung wurde während des Krieges nur verlangsamt, bereits nach Kriegsende wurden wieder Ausstrahlungen vorgenommen. Bekannt ist der Fernseher "Leningrad", welcher nach Vorbild des in Deutschland entwickelten "Volksfernsehers" E1 in der SBZ, bzw. später DDR für den sowjetischen Markt gefertigt wurde.

In den USA gab es bereits in den 1920er Jahren zahlreiche Entwicklungen und Firmen, welche sich mit der Entwicklung des Fernsehens beschäftigten. Um 1929 gab es von jeder Firma eigene Normen, in denen Versuchssendungen durchgeführt wurden. Die Bildauflösungen lagen zwischen 24 Zeilen (Jenkins) über 30 und 45 Zeilen bis hin zu 60 Zeilen. Nicht zuletzt aufgrund der ständigen Veränderungen und Verbesserungen konnte ein Standard über lange Zeit nicht etabliert werden, die öffentlichen Versuchssendungen waren um 1934 nahezu beendet. Erst mit der Weiterentwicklung der Technik ab 1938 begannen wieder Versuchssendungen in Auflösungen zwischen 441 Zeilen und über 700 Zeilen. Erst 1942 einigte sich das "National Television System Committee" (NTSC) auf die noch heute übliche Norm mit 525 Zeilen und 30 Vollbildern pro Sekunde (NTSC bezieht sich hier natürlich nicht auf das (damals nicht vorhandene) Farbsystem, sondern es ist der Name der Normungskommission, welche später auch die Farbnorm beschlossen hat, welche heute den Namen dieser Kommission trägt).

Nach dem Krieg wurde in Deutschland und den meisten Nachbarländern mit Ausnahme von Frankreich (zunächst 819 Zeilen, erst ab 1980 komplette Umstellung auf 625 Zeilen) und Großbritannien (405 Zeilen, 625 Zeilen erst ab 1965) bei der Wiederaufnahme des Sendebetriebs dann auf die bis heute verwendete 625-Zeilen-Norm mit 25 Bildwechseln pro Sekunde (Gerber-Norm) umgestellt. Die technischen Eckdaten für die Auflösung und das Seitenverhältnis sowie die Bildwechselfrequenz des Fernsehbildes blieben dann über mehr als ein halbes Jahrhundert unverändert.

Ende 1929 veröffentlichten Elektronik-Bastler erste Bauanleitungen für Fernsehempfänger, die teilweise sogar Bild und Ton empfangen konnten; einen praktischen Nutzen hatten diese Basteleien nur bedingt, da der Versuchssender Witzleben erst ab 1934 Fernsehprogramme mit Ton ausstrahlte. Die BBC strahlte schon seit 1931 Sendungen mit Ton aus. Die seit 1930 erscheinende britische Zeitschrift "Practical Television" geht in ihrer Ausgabe von März 1934 von rund 3000 Besitzern selbstgebauter Fernseher, sowie rund 1000 Besitzer käuflich erworbener Fernseher alleine in Großbritannien aus.

Zu Beginn des Jahres 1951 gab es in den USA bereits 10.000.000 Fernsehzuschauer, in Großbritannien verfügten immerhin 600.000 und in Frankreich noch 4.000 Zuschauer über Fernsehempfänger. Das als „Flimmerkasten“ bespöttelte Medium war noch keine Konkurrenz für den Hörfunk, zumal das Programm auf zwei Stunden pro Tag begrenzt war.

Die Anzahl der Fernsehteilnehmer nahm in den folgenden Jahren weltweit rapide zu: 1952 gab es in den USA bereits 15 Millionen Teilnehmer, in Großbritannien 1,45 Millionen, in Frankreich knapp 11.000, in der Bundesrepublik Deutschland rund 300. Für diese 300 Teilnehmer wurde das damalige „NWDR-Fernsehen“ am 25. Dezember 1952 eröffnet, das erste regelmäßige deutsche Fernsehprogramm der Nachkriegszeit überhaupt wurde im September und Oktober 1951 vom Grundig-Werkssender in Fürth gesendet. Die DDR begann den regelmäßigen Fernsehbetrieb einige Tage vor der Bundesrepublik Deutschland: am 21. Dezember 1952 - dies war auch der 73. Geburtstag Stalins. Empfangsbereit waren etwa 60 Geräte, alle in Berlin.

Das erste im Fernsehen direkt übertragene Großereignis war die Krönung von Elizabeth II. am 2. Juni 1953, erstmals übertraf dabei die Zahl der Fernsehzuschauer in Großbritannien (27 Millionen bei einer Bevölkerung von damals 36 Millionen) die der Rundfunkhörer (11 Millionen). Die Anzahl der Fernsehlizenzen stieg von knapp 1,5 Millionen im Jahr 1952 auf über 3 Millionen 1954. Durch die Übernahme nach Deutschland und Frankreich war dies auch die erste länderübergreifende europäische Liveübertragung.

1955 gab es 100.000 Geräte im Bundesgebiet und 1957 war die erste Fernsehteilnehmer-Million erreicht; in der Folgezeit entwickelte sich der Fernseher zum Prestigeobjekt. Der Durchbruch zum Massenmedium gelang dem Fernsehen in der Bundesrepublik Deutschland 1959: Täglich wurden 5.000 Geräte verkauft, Ende des Jahres gab es zwei Millionen, 1960 knapp 3,5 Millionen Teilnehmer. 1961 gab es schließlich in 26 Ländern der Welt weit über 100 Millionen Fernsehteilnehmer.

Den Fernsehherstellern war, wie den Kunden, die Bautiefe der Röhrengeräte ein Dorn im Auge, weil sie nicht mit „der modernen Inneneinrichtung“ von Haushalten der 1950er Jahre harmonierte. Auf der "National Radio Show" in London 1959 stellte eine Firma einen Fernseher vor, den man wie einen „Bilderrahmen [...] an die Wand hängen“ konnte. Tatsächlich hatte das Gerät die Anmutung, sehr flach zu sein, weil es nicht von vorn nach hinten konisch zulief. Jedoch war die Tiefe erheblich, und der Hersteller riet daher, das Gerät nur in den Ecken von Räumen aufzuhängen. Diese Anwendung war bis zur Einführung der Flachbildschirme um 2000 vor allem in Restaurants häufig anzutreffen.

1960 gab es mit knapp 16 Millionen Radioteilnehmern etwa ebenso viele Hörer wie 1945. Gesteigert wurde der Absatz durch die Einführung des Transistors, der den Bau kleiner und leichter Koffer-, Reise- und Autoradios ermöglichte; der Hörfunk erlebte damit Anfang der 1960er-Jahre seine stärkste Verbreitung. Das Fernsehen nahm jedoch auch an Beliebtheit zu; 1964 gab es bereits 7 Millionen Fernsehzuschauer. Bis Anfang der 1970er-Jahre kehrten sich die Zuwachsraten um: die Zahl der Hörfunkteilnehmer nahm zwar weiterhin zu, jedoch nur um etwa 2 Prozent pro Jahr; die Zahl der Fernsehteilnehmer stieg pro Jahr um fast 20 Prozent. Ab Mitte der 1960er-Jahre war der Markt für Radiogeräte weitgehend gesättigt; die Industrie reagierte mit einer Diversifizierung der Produktpalette, die vom Kunden akzeptiert wurde: Es wurden Zweit- und Drittgeräte gekauft.
Nicht nur das Publikum, auch die Politik interessierte sich vermehrt für das Fernsehen: Bundeskanzler Konrad Adenauer versuchte, ein dem Bund unterstelltes privatwirtschaftlich organisiertes Fernsehen, die Deutschland-Fernsehen-GmbH, einzurichten. Adenauers Vorstellungen, Rundfunk als „politisches Führungsmittel der jeweiligen Bundesregierung“ zu etablieren, konnten sich jedoch nicht durchsetzen. Mit dem „Fernsehurteil“ des Bundesverfassungsgerichts vom 28. Februar 1961 wurde jedoch die Autonomie der Länder in Rundfunkfragen bestätigt. Ersatzweise wurde eine weitere gemeinnützige Anstalt des öffentlichen Rechts eingerichtet: das "Zweite Deutsche Fernsehen" (ZDF) mit Sitz in Mainz, welches den Sendebetrieb am 1. April 1963 aufnahm. Zusätzlich richtete die ARD zwischen 1964 und 1969 fünf regionale dritte Fernsehprogramme ein.

Der Hörfunk stellte dem immer attraktiver werdenden Programmangebot des Fernsehens am 30. August 1963 eine technische Innovation entgegen: Hörfunk im Zweikanalton (Stereofonie). Eine weitere Besonderheit des UKW-Hörfunkprogramms ist der Verkehrsfunk (Verkehrsrundfunk, Verkehrswarnfunk).

Das Fernsehen setzte den technischen Verbesserungen des Hörfunks ebenfalls eine technische Neuerung entgegen: Die Einführung des Farbfernsehens in der Bundesrepublik Deutschland am 25. August 1967. Bereits 1963 erfolgte die Ausstrahlung des ersten Farbfernseh-Testbilds.

Die ersten Experimente mit farbigen Fernsehbildern basieren auf der Aufteilung des Farbspektrums in Grundfarben; John Logie Baird verwendete bei seinen Experimenten in den späten 1920er Jahren eine Nipkowscheibe mit „Spiralarmen“ für die Farben Rot (R), Grün (G) und Blau (B). Das Verfahren wurde 1930 von E. Andersen verbessert und 1935 von der Forschungsanstalt der Deutschen Reichspost (RPF) aufgegriffen, als sie mit der Entwicklung eines Farbfernsehverfahrens begann. Man arbeitete nach einem bisequentiellen Verfahren, das auf dem Kinemacolor-Zweifarbenfilm beruhte und ein Zweifarbenbild mit 2 × 90 Zeilen und 25 Rastern pro Sekunde ermöglichte. Der Zweite Weltkrieg unterbrach die deutsche Farbfernseh-Entwicklung.

Ab Juni 1951 wurde in New York von der CBS (Columbia Broadcasting System) das erste Farbfernsehprogramm der Welt ausgestrahlt, das ebenfalls auf dem bisequentiellen Verfahren beruhte; es wurde nach wenigen Monaten eingestellt. Das CBS-Verfahren wies verschiedene gravierende Nachteile auf, unter anderem war das System inkompatibel zum Schwarz-Weiß-Fernsehen, die Bildwechselfrequenz musste von 60 Hz auf 140 Hz erhöht werden, um Flickererscheinungen zu vermeiden; dies wiederum erforderte aufgrund der begrenzten Frequenzbandbreite eine Reduktion der Auflösung.

Um weitere kostspielige Fehlschläge zu vermeiden, wurde mit einem erheblichen Aufwand von der eigens gegründeten National Television Systems Committee (NTSC), das Gremium setzte sich aus Wissenschaftlern aller namhaften Elektronik-Firmen zusammen, anschließend eine technisch leistungsfähigere Lösung entwickelt. Das Ergebnis, die NTSC-Norm, wurde am 23. Dezember 1953 für verbindlich erklärt. Sie ist gekennzeichnet durch drei Eigenschaften:


Zu den entscheidenden Nachteilen von NTSC gehört das instabile Farbsignal, das auch während einer Übertragung zu „drastischen Farbverschiebungen [...], zum Beispiel von Blau nach Grün“ führen kann. Ursache ist die Verknüpfung der Phase des Farbhilfsträgers mit dem Farbton. Spötter interpretieren daher die Abkürzung NTSC als „Never The Same Color“. Als Abhilfe verfügt jeder NTSC-Fernsehempfänger über einen sogenannten „Tint“-Regler (Tint für „Farbton“), mit dem die Farbwiedergabe angepasst werden kann.

Um 1955 tauchte der Gedanke auf, in ganz Europa ein einheitliches Farbfernsehsystem einzuführen. In einer vom "Comité Consultatif International des Radiocommunications" (CCIR) einberufenen Konferenz wurde festgestellt, dass die unterschiedlichen Zeilennormen erhebliche Probleme bei der Standardisierung aufwerfen würden: In den USA wurde eine 525-Zeilen-Norm verwendet, in England 405 Zeilen, in Frankreich 819 und in den übrigen europäischen Ländern 625 Zeilen.

Beim Schwarz-Weiß-Fernsehen wurde nur ein Signal gesendet, ein Helligkeitssignal von weiß bis schwarz. Der Entwicklung des Farbfernsehens lag der Gedanke zugrunde, auch weiterhin nur ein einziges Signal zu senden. Im Studiobereich wird jedoch mit RGB-Signalen gearbeitet, die theoretisch auch gesendet werden könnten; dazu müsste jede der drei Farben auf eine eigene Welle aufmoduliert werden, was eine enorme Bandbreite erforderte und dazu noch unwirtschaftlich wäre. Die Farbfernsehsysteme NTSC, SECAM und PAL dienen also dazu, die drei RGB-Signale auf ein einziges zu übertragendes Signal zu reduzieren.

In Frankreich wurde mit massiver Unterstützung durch die Regierung das SECAM-Farbfernsehsystem entwickelt (SECAM = "Séquentiel (couleur) à mémoire"). Aufgrund technischer Unzulänglichkeiten musste es mehrfach überarbeitet werden; es entstanden die Varianten SECAM 2, SECAM 3, SECAM 3a und schließlich SECAM 3b. Während der PAL-Erfinder Walter Bruch die Notwendigkeit zu ständigen Modifikationen als konzeptionelle Schwäche von SECAM ansah, äußerte sich der damalige WDR-Fernsehingenieur Franz Josef In der Smitten folgendermaßen: „Ich habe die Glanzleistungen der französischen Ingenieure bewundert, denen es immer wieder gelungen ist, das SECAM-System weiter zu verbessern [...]“.

In Deutschland studierte Walter Bruch die Farbfernsehsysteme NTSC und SECAM, um aus den Fehlern der Konkurrenzsysteme zu lernen. Basierend auf NTSC entwarf er bei Telefunken in Hannover das Farbfernsehsystem PAL (= "Phase Alternating Line", „Phasenwechsel je Zeile“). Es unterschied sich vor allem durch eine integrierte Farbkompensation, die das Auftreten des entscheidenden Problems der NTSC-Norm, der Farbverzerrungen, verhinderte; das „PAL-Verfahren [ist] im Vergleich mit NTSC und SECAM das stabilste Verfahren“. In der Erprobungsphase von PAL konnten PAL-Signale noch nicht magnetisch aufgezeichnet werden; alle Fernsehsendungen aus dieser Zeit waren also Live-Übertragungen. Spielfilme wurden mittels optischer Systeme (16-mm-/35-mm-Filmgeber und Diageber) projiziert und dann ebenfalls live von dieser Projektion übertragen.

Die ersten industriell gefertigten Aufzeichnungssysteme für PAL-Signale stammten von der US-Firma Radio Corporation of America (RCA). Die Systeme wurden erstmals Ende 1966 geliefert, etwa ein dreiviertel Jahr nach dem erfolgreichen Test des Prototyps beim Kölner Karneval.

Auf der vom 24. März bis 7. April 1965 in Wien abgehaltenen Konferenz der Studiengruppe XI des CCIR zur Vereinheitlichung des Farbfernsehsystems, an der Vertreter aus 39 Staaten teilnahmen, sprachen sich 21 für das französische SECAM, 11 für das westdeutsche PAL und 7 Staaten für das US-amerikanische NTSC-Farbfernsehsystem aus. Die USA und die Bundesrepublik Deutschland planten unterdessen ihre Systeme PAL und NTSC unter der Bezeichnung QAM (Quadraturamplitudenmodulation, Quadrature Amplitude Modulation) zu kombinieren. Frankreich und die Sowjetunion konnten sich vorläufig auf die gemeinsame Verwendung des SECAM-Verfahrens einigen.

Die ersten Live-Farbfernsehsendungen wurden per Kabel ins Sendehaus übermittelt. Den ersten Farbübertragungswagen erhielt der WDR im Frühjahr 1967, weitere mobile Sendestudios besaßen damals nur der NDR und das ZDF. Bis in die 1970er Jahre besaßen nicht alle Landesrundfunkanstalten Farbübertragungswagen; stattdessen existierte ein Pool, der von einigen Fernsehanstalten gemeinsam genutzt wurde. Zwischen 1967 und 1970 wurde in der Bundesrepublik Deutschland das Fernsehen auf Farbe umgestellt.

Das Fernsehbild wurde von einer Farbbildröhre wiedergegeben, die auf einem 1931 angemeldeten Patent von Manfred von Ardenne basierte: Drei mikroskopisch schmale Streifen eng nebeneinander liegender Leuchtstoffe in den drei Primärfarben waren so angeordnet, dass sie sich, mit einem Elektronenstrahl abgetastet, zu weißem Licht ergänzten; ein Verfahren zur getrennten Ansteuerung der drei Farben enthielt das Patent nicht.

Das Heimfarbfernsehen basiert auf der Idee, die farbigen Lichter aus der photographischen Projektion (Lochricht-Rasterverfahren von Raphael Eduard Liesegang, 1896) durch Elektronenstrahlen zu ersetzen. Diese frühe Lochmaskenröhre wurde von Werner Flechsig zur Schattenmasken-Farbbildröhre weiterentwickelt (patentiert 1938). Weitere Verbesserungen des Verfahrens brachte A. N. Goldsmith und Harold B. Law von der amerikanischen RCA ein. Der Durchbruch gelang der Konkurrenz; CBS-Hydron konstruierte erstmals eine Farbröhre, wie sie schließlich auch beim deutschen Nachkriegsfernsehen ab den 1960er Jahren eingesetzt wurde.
Deutliche Verbesserungen der Bildschärfe und Farbwiedergabe konnten durch eine Schattenmaskenröhre mit Langlochschlitzen erzielt werden. Bei diesem Verfahren sind alle drei Elektronenkanonen nebeneinander (Inline), statt wie bisher in einem Dreieck (Delta-Röhren) in einem System vereinigt. Solche Röhren wurden in Deutschland ab 1972 angeboten. Einen eigenen Weg ging die Firma Sony bereits Ende der 60er Jahre mit der Trinitron-Röhre, bei der statt einer Schattenmaske an einem Spannrahmen befestigte Drähte die Projektion auf die Farbstreifen voneinander trennte. Durch diesen starken Spannrahmen ist eine Trinitron-Röhre immer deutlich schwerer als eine Inline Röhre mit Schlitzmaske.

Von besonderer Bedeutung für die Entwicklung des Fernsehens als Massenmedium waren internationale Großereignisse des Sports; dies hatte sich bereits 1936 bei den Spielen der XI. Olympiade in Berlin gezeigt, wo erstmals eine direkte Fernsehübertragung stattfand. Mit den Fernsehübertragungen von der XVIII. Olympiade 1964 in Tokio über den Syncom 3-Satelliten wurde erstmals eine aktuelle weltweite Reportagetätigkeit ermöglicht, die seitdem einen festen Platz in der Berichterstattung der Olympischen Spiele hat. Als erste Olympiade wurde in den USA von den Sommerspielen 1968 Mexiko in Farbe berichtet. Von der Olympiade 1972 München gingen dann Farbbilder in alle Welt.

1971 wurden erste Ultraschall-Fernbedienungen vorgeführt. Nordmende präsentierte das drahtlose „Fernhören“ über Infrarot-Kopfhörer. 1981 wurde der Stereoton beim Fernsehen eingeführt.

Die Anfänge des Satellitenfernsehens liegen im Jahr 1962, als zum ersten Mal Fernsehsendungen zwischen den USA (Bodenstation Andover) und Frankreich mittels des Satelliten Telstar übertragen wurden. Am 6. April 1965 wurde der erste kommerziell genutzte Nachrichtensatellit in Betrieb genommen; der Intelsat I F1 („Early Bird“) ermöglichte die Übertragung von Ferngesprächen, Fernschreiben und Fernsehsendungen.

In den Bestrebungen, eine von Herman Potočnik entdeckte und bereits im Jahre 1928 publizierte geostationäre Position für den Direkt-Fernsehempfang in Europa zu nutzen, wurden in der Funkverwaltungskonferenz World Administrative Radio Conference (WARC) in Genf im Jahre 1977 ein weltweiter Rundfunk-Satellitenplan beschlossen. Ab 1. Januar 1979 galt eine Vereinbarung mit einer Laufzeit von 15 Jahren, die vorsah, das jedes Land fünf TV-Programme oder mehrere Hörfunk-Programme direkt vom Satelliten zum Teilnehmer abstrahlen konnten. Die Position musste sich jedes Land mit bis zu acht anderen Ländern (und damit Satelliten) teilen. Je geostationärer Position waren so 40 Transponder bei einem Transponderabstand durch Frequenzüberlappung von 19,18 MHz zu 27 MHz angedacht. Die Direct Broadcasting Satellites (DBS) sollten in 36.000 km Höhe mit einem Abstand von 6° (ca. 4000 km) über dem Äquator positioniert werden. Eine gemeinsame Orbitposition (19° West) wurde Belgien, der Bundesrepublik Deutschland, Frankreich, den Niederlanden, Italien, Luxemburg, Österreich und der Schweiz zugewiesen. Zu den Eigenschaften der Satellitentechnik gehört das so genannte Overspill; damit bezeichnet man das Überlappen der Gebiete, in denen die Beams (Richtstrahlen) empfangbar sind. Dieses Phänomen ist für den Endverbraucher, der mit seiner Satellitenempfangsanlage die Programme der Nachbarländer empfangen kann, zwar von Vorteil, warf aber in den staatlichen Plänen für Satellitenfernsehen urheber- und hoheitsrechtliche Probleme auf.

In Luxemburg wurde die SES ASTRA S.A. (Société Européenne des Satellites) zu der Zeit gegründet, als gerade die rasant fortschreitende technische Entwicklung in der LNB-Technik es Privathaushalten erlaubte, mit relativ handlichen Satellitenschüsseln von noch lediglich 1,2 Meter Durchmesser Direktempfang von leistungsschwachen (20 Watt je Transponder) Post-Fernmeldesatelliten zu praktizieren. Es war eine logische Schlussfolgerung der Privaten SES, dass sich durch den Einsatz von Modernen Satelliten mit einer EIRP von 51 dBW die notwendige Schüsselgröße auf ein erstmals wirklich massentaugliches Format von 75 cm und weniger reduzieren ließe. Durch staatliche Fehlplanung war die Sendeleistung des Astra-Mitbewerbers TV-SAT durch jahrelange Verzögerung auf 230 Watt je TV-Kanal festgelegt worden, was nur vier TV-Kanäle zuließ, das schlanke Konzept von Astra sah hingegen 16 TV Kanäle mit vollem Ekliptikschutz vor.

Durch den Misserfolg von TV-Sat 1 am 21. November 1987 war auch der Zeitbonus des staatlichen Direktsatelliten verspielt und der Weg für den Markterfolg des am 11. Dezember 1988 gestarteten Astra 1A frei. TV-Sat 2 startete zwar noch am 8. August 1989, jedoch zu spät, das private Projekt Astra hatte das milliardenschwere staatliche TV-Sat-Projekt geschlagen.

Unter der Administration des Bundespostministers Christian Schwarz-Schilling wurde nicht die Satellitentechnik, sondern die flächendeckende Verkabelung aller Haushalte mit breitbandigen Koaxialkabelnetzen angestrebt. Das ab 1983 von der Deutschen Bundespost verlegte Breitbandkommunikationskabelnetz ermöglichte unter Ausnutzung des Frequenzbereichs bis 300 MHz die gleichzeitige Übertragung von maximal 29 Fernsehprogrammen und 24 Stereo-Hörfunkprogrammen. Der „verkabelte Rundfunk“, das Kabelfernsehen, wurde zunächst in vier Pilotprojekten getestet, die als Modellversuche in Ludwigshafen am Rhein, München, Dortmund und West-Berlin ausgeschrieben waren. Das Ludwigshafener Kabelpilotprojekt war auch gleichzeitig die Geburtsstunde des Privatfernsehens, das am 1. Januar 1984 mit PKS (heute Sat.1) seinen Sendebetrieb aufnahm.

1954 brachte die RCA ein Gerät auf den Markt, das Fernsehbilder aufzeichnen und wiedergeben konnte; dieses „welterste Video-Gerät“ verschlang 21.600 Meter Magnetband pro Stunde und arbeitete noch nicht nach dem heute verwendeten Schrägspur-Aufzeichnungsverfahren, sondern basierte auf Patenten der deutschen Firmen Telefunken und Loewe. In Verbindung mit weiteren Lizenzen von einem amerikanischen Hersteller von Profi-Equipment, der Firma Ampex, gelang es den japanischen Firmen Sony und JVC (Japan Victor Company), die klobigen und teuren professionellen Magnetbandaufzeichnungs- und Wiedergabegeräte zu einem handhabbaren und preiswerten Massenprodukt der Unterhaltungselektronik zu machen. Das Aufzeichnen von Fernsehübertragungen bedeutete eine Loslösung von zeitlichen Abhängigkeiten fester Sendetermine. Auf der Funkausstellung 1971 wurden von Philips und Grundig die ersten Video-Cassetten-Rekorder nach dem VCR-System vorgestellt.

Ab den 1980er Jahren kam es zu massiven Veränderungen der bundesdeutschen Medienlandschaft. Vorboten der Entwicklung waren die so genannten neuen Medien, zum Beispiel das auf Glasfasertechnik basierende BIGFON, das „Breitbandige, Integrierte Glasfaser-Fernmelde-Orts-Netz“, das Satelliten-Pilot-Projekt TV-SAT, Bildschirmtext (BTX) und Videotext (VTX) und das Kabelfernsehen, wobei es sich jedoch lediglich um „neue Verteiltechniken, andere Organisationsformen und größere Programmquantitäten“ handelte.

Mit der Einführung des dualen Rundfunksystems infolge auf das 4. Rundfunk-Urteil des Bundesverfassungsgerichts vom 4. November 1986 kam es zu einem Paradigmenwechsel, der die bundesdeutsche Medienlandschaft bis heute entscheidend prägt. Ab Mitte der 1980er Jahre wurde begonnen, auch terrestrische Frequenzen an private Anbieter zu vergeben. Hierbei wurden UKW-Frequenzen im Bereich von 100 bis 104 MHz aus dem Bereich des Flugnavigationsfunkdienstes verwendet, die zuvor durch das Genfer Abkommen von 1984 freigegeben worden waren.

Neben der Deregulierung der deutschen Medienlandschaft deuteten sich auch technische Veränderungen an. Nachdem in den 1990er Jahren mit der Digitalisierung des Rundfunks mittels DVB-S (Satellit), DVB-C (Kabel) sowie den bereits 1999 wieder eingestellten Standards DSR, ADR und DVB begonnen worden war, wurde die flächendeckende Einführung des terrestrischen digitalen Rundfunks DVB-T sowie des digitalen Radio-standards DAB in Angriff genommen, um die zukünftige Einstellung analoger Übertragung vorzubereiten.

Eine auf IP-Technologie aufbauende Digitalisierung herkömmlicher analoger Telekommunikationsnetze im sogenannten Next Generation Network (NGN) ermöglichte eine weitere Technologische Revolution in der Rundfunkübertragung. Ein digitalisiertes Telekommunikationsnetz ermöglicht, neben einer Übertragung von Telefonie und Internet-Daten, auch eine Übertragung von digitalen Fernsehsignalen (Triple Play). Die Übertragung per Internetprotokoll beinhaltet auch einen Rückkanal, weswegen Internet-TV oder IPTV sowie P2PTV als interaktives Fernsehen erstmals ohne getrennten Rückkanal möglich werden. Es entfällt außerdem die Beschränkung auf eine bestimmte Anzahl verfügbarer Kanäle, so sind durch den Transportkanal Internet darüber hinaus neue parallele Fernsehprodukte, wie zum Beispiel Mediatheken oder Abruffernsehen möglich.

Teletext (auch Fernsehtext oder Videotext) ist ein Verfahren, bei dem Text- und Blockgrafik-Zeichen übertragen werden. Wegen der Namensgleichheit kann es zu Verwechslungen mit dem Bildschirmtext-System kommen. Der erste Teletext wurde im japanischen Fernsehen gesendet. Anfangs konnte nur 1/6 der Bevölkerung Japans den Teletext empfangen und anzeigen.


Aufgrund der komplexen, aus vielen Teilen bestehenden Fernsehtechnik enthält seine Geschichte Elemente vieler anderer Techniken, insbesondere der Funk-, Film-, Hörfunk- und Raumfahrttechnik. Die Geschichte des Fernsehens geht nicht nur mit technischen, sondern auch mit sozialen und politischen Entwicklung einher. Sie ist Teil der Mediengeschichte.

Zur historischen Chronologie der Rundfunk- und Fernsehtechnik hat Oberpostdirektor Gerhart Goebel zwei umfangreiche Bücher geschrieben.





</doc>
<doc id="14383" url="https://de.wikipedia.org/wiki?curid=14383" title="Chinua Achebe">
Chinua Achebe

Albert Chinụalụmọgụ „Chinua“ Achebe (* 16. November 1930 in Ogidi, Nigeria; † 21. März 2013 in Boston, USA) war ein nigerianischer Schriftsteller, der in englischer Sprache schrieb. Der Name Chinụalụmọgụ stammt aus der Igbo-Sprache und bedeutet „Gott kämpft zu meinen Gunsten“. Er gilt als einer der Väter der modernen afrikanischen Literatur.

Achebe wurde als fünftes von sechs Kindern in Ogidi, Nigeria, geboren. Seine Familie gehörte den Igbo an und sein Vater war ein evangelikaler Katechist. Achebe wurde in den 1930er Jahren in einer Missionsschule in Ogidi unterrichtet und besuchte später ein College in Umuahia. Ab 1948 studierte er Anglistik, Geschichte und Theologie am University College von Ibadan und schloss 1953 sein Studium ab. Von 1954 an arbeitete er zwölf Jahre lang für die "Nigerian Broadcasting Corporation" in Lagos.

Während des nigerianischen Bürgerkrieges engagierte er sich auf Seiten Biafras und war zwischen 1967 und 1970 Sonderbotschafter in den USA und in Europa. Von 1976 bis 1990 war er Professor für Literatur an der Universität von Nsukka. 1979 eröffnete er in West-Berlin das erste Horizonte Festival der Weltkulturen. Er war an einer Reihe britischer und US-amerikanischer Universitäten Gastprofessor. Zuletzt lehrte er am Bard College in Annandale-on-Hudson, New York. Seit 1990 war er infolge eines Autounfalls von der Hüfte abwärts gelähmt und bewegte sich im Rollstuhl.

Achebe war skeptisch gegenüber Autoritäten und Vaterfiguren und äußerte sich kritisch zu Politik, Wirtschaft und der Beachtung der Menschenrechte in seinem Heimatstaat. Aus Protest gegen die anhaltende Korruption in Nigeria lehnte er 2011 ein weiteres Mal den Titel des "Commander of the Federal Republic", der von der nigerianischen Regierung verliehen wird, ab.

Am 21. März 2013 starb er nach kurzer Krankheit im Alter von 82 Jahren in Boston, USA.

Achebe gilt als der Begründer der modernen nigerianischen Literatur und weltweit als einer der herausragenden englischsprachigen Schriftsteller. Seine Werke wurden in rund 50 Sprachen übersetzt. Dabei entwickelte er einen eigenen Stil, der auf der Erzähltradition seiner Heimat aufbaut. Er verzichtete bewusst auf europäische Literaturkonventionen, verarbeitete jedoch nigerianische Erzählungen in seinen Romanen. Nach seinen eigenen Worten „sollte jede gute Geschichte, jeder gute Roman, eine Botschaft enthalten, einen Zweck haben“.

Sein erster Roman "Things Fall Apart" gilt heute als Meilenstein der afrikanischen Literatur. Das rund 200 Seiten umfassende Werk erschien 1958 auf Englisch in London. Darin erzählt Achebe die Geschichte der nigerianischen Igbo in den 1890er Jahren. Der Bildungsroman schildert in realistischer Erzählweise im ersten Teil Wirtschaft, Kultur, Traditionen, Religion und Geschlechterverhältnisse einer Dorfgemeinschaft. In einem zweiten und dritten Teil werden die Auswirkungen der neuen christlichen und kolonialistischen Einflüsse auf das Dorfleben dargestellt.










</doc>
<doc id="14384" url="https://de.wikipedia.org/wiki?curid=14384" title="Oszillatorschaltung">
Oszillatorschaltung

Eine Oszillatorschaltung, auch kurz "Oszillator" genannt, ist eine elektronische Schaltung zur Erzeugung einer sinusförmigen Wechselspannung. Es gibt zahlreiche Möglichkeiten, eine solche Schaltung aufzubauen, zum Beispiel:

Verstärker können mit einer geeigneten Rückkopplung zu einem Oszillator werden. Die Schwingungsbedingungen, die zunächst im Stabilitätskriterium von Barkhausen formuliert wurden, führen zu einer dauerhaften Schwingung an einem linearen rückgekoppelten Verstärker mit einer bestimmten Frequenz. Sie lauten anschaulich:

Im theoretischen Modell würde eine Schleifenverstärkung, die den Wert 1 auch nur geringfügig übersteigt, zu einem unendlichen Anwachsen der Schwingung führen. Damit der Oszillator eigenständig („von allein“) anschwingt, muss aber zunächst der Wert 1 überschritten werden (siehe Bild). Dieser Widerspruch führt zu Untersuchungen über das Anschwingverhalten und über das Abreißen der Schwingungen und wird durch entsprechende Kennlinien dargestellt. Man spricht auch von einem harten bzw. weichen Schwingungseinsatz. Den physikalischen Hintergrund bilden im Wesentlichen zwei Erscheinungen:

In der Realität ist die Amplitude beschränkt (die dem Verstärker von der Stromversorgung zugeführte Leistung ist endlich).

Rückgekoppelte Oszillatoren bestehen aus einem Verstärker und einem passiven, frequenzabhängigen Netzwerk. Der Verstärkerausgang speist den Eingang des Netzwerks. Der Ausgang des Netzwerks ist mit dem Verstärkereingang verbunden (Rückkopplung). Man kann in Gedanken die Rückkopplungsleitung auftrennen und erhält so an Stelle des geschlossenen Kreises eine Übertragungskette mit Eingang (E) und Ausgang (A). Für gleiche Verhältnisse wie beim geschlossenen Kreis muss die Phasenlage der Ausgangsschwingung (φ3) mit der Phasenlage der Eingangsschwingung (φ1) übereinstimmen (Phasenbedingung).

Wenn der Verstärker selbst eine Phasendrehung von 180° bewirkt und die Signallaufzeit Null ist, muss das Netzwerk zumindest für "eine" Frequenz eine weitere Phasendrehung um 180° bewirken, um eine Gesamtphasendrehung von 360° = 0° zu erzielen. Die frequenzabhängige Phasendrehung wird in einem Phasengangdiagramm dargestellt.

Beim Phasenschieberoszillator besteht das Netzwerk aus (mindestens) drei hintereinander geschalteten RC-Gliedern (Tief- oder Hochpässe). Bewirkt jedes dieser Glieder eine Phasendrehung von 60°, genügen drei RC-Glieder für eine Gesamtphasendrehung von 180°. Wenn der Verstärker nicht übersteuert ist, ist die erzeugte Wechselspannung sinusförmig.

Ein verlustbehafteter Schwingkreis kann durch ein Bauelement mit negativem differentiellen Widerstand, beispielsweise einer Tunneldiode oder Lambda-Diode, entdämpft werden und erzeugt dann Wechselspannung. Bedingung ist, dass der Gesamtwiderstand Null ist. Die für den Betrieb nötige Energie wird von einem externen Netzteil oder einer Batterie geliefert.

Bei anderen Oszillatortopologien, welche beispielsweise auf der negativen Kennlinie wie der Relaxationsoszillator (s. u.) basieren, hat das Stabilitätskriterium keinen unmittelbaren Bezug.

Die Qualität eines Oszillators wird generell nach der Stabilität von Amplitude, Frequenz und Phase beurteilt. Sind die Schwankungen nur statistisch beschreibbar, werden sie als Rauschen bezeichnet. Als eigenständiger Begriff ist hier nur das Phasenrauschen (Jitter) üblich, der die Empfindlichkeit eines Überlagerungsempfängers in unmittelbarer Nachbarschaft eines starken Signals kennzeichnet. Wichtig ist auch die Stabilität gegenüber Schwankungen der Temperatur und der Versorgungsspannung, wobei es markante Unterschiede gibt: Die Frequenz von Relaxations- und Ringoszillatoren reagiert sehr empfindlich auf Änderungen der Betriebsspannung. Bei Oszillatoren mit Resonanzkreis ist diese Abhängigkeit sehr gering und bei Quarzoszillatoren vernachlässigbar.

Ein weiteres wichtiges Kriterium beispielsweise bei Messgeräten ist die Genauigkeit, mit der die gewünschte Kurvenform erzeugt wird. Bei Sinusoszillatoren kann dies recht einfach durch den Klirrfaktor beschrieben werden. Obwohl primär für Sinusschwingungen gebraucht, gilt dieses Kriterium entsprechend auch für andere Signalformen.

Unmodulierte Oszillatoren werden eingesetzt, um die Taktfrequenz von Computern oder elektrischen Uhren zu erzeugen.

Bei modulierten Oszillatoren werden Amplitude, Frequenz oder Phase durch zusätzliche Bauelemente in gewissen Grenzen beeinflusst. Damit kann man durch Modulation Nachrichten übertragen. Diese wird verwendet, um

Bei einem "Resonanzoszillator" wird die erzeugte Frequenz durch einen Schwingkreis, einen Schwingquarz oder einen Keramikresonator bestimmt. Der Resonanzoszillator liefert üblicherweise eine frequenzstabile Sinusschwingung.
Dazu gehört auch das Magnetron, obwohl es zugleich auch Laufzeitoszillator ist.

Hauptsächliches Anwendungsgebiet der Sinusoszillatoren ist die Funktechnik.

Stets wird darauf geachtet, dass der Resonator einen ausreichend hohen Gütefaktor besitzt, damit die Bandbreite des erzeugten Signals auf die enge Umgebung der Resonanzfrequenz beschränkt ist. Das verringert den Anteil der Oberwellen im Ausgangssignal, auch wenn das verstärkende Element, beispielsweise ein Transistor, übersteuert ist und eigentlich starke Oberwellen erzeugt. Resonanzoszillatoren liefern – im Gegensatz zu beispielsweise einem Wien-Robinson-Oszillator – auch ohne Amplitudenstabilisierung ein gut sinusförmiges Signal.

Bei "Laufzeitoszillatoren" bestimmt die Laufzeit von Impulsen in bestimmten Schaltungsteilen die Schwingungsdauer und damit die Frequenz. Als Beispiel dient hier der Ringoszillator mit seiner Inverter-Kette. Aber auch Oszillatoren wie das Reflexklystron und ein Gunndiodenoszillator zählen zu dieser Kategorie, obwohl beide Schwingkreise besitzen. In Phasenschieberoszillatoren wird die Signallaufzeit durch RC-Glieder erzeugt. Es gibt einen Überlappungsbereich zu den Relaxationsoszillatoren, weil die dort zeitbestimmenden RC-Glieder auch als Laufzeitglieder angesehen werden können. Die Frequenzstabilität ist generell eher mittelmäßig.

Ein "Relaxationsoszillator" ist ein Kippgenerator. Er ist kein Oszillator im engeren Sinne, da er normalerweise keine Sinusschwingung erzeugt. Die Frequenz wird typischerweise durch Entladungsvorgänge eines Kondensators in einem RC-Glied bestimmt. Bei Erreichen eines bestimmten Werts der Kondensatorspannung wird die Ausgangsspannung umgeschaltet (sie „kippt“) und der Kondensator wird wieder aufgeladen. Die bekanntesten Schaltungen sind Multivibrator und Kippschwinger. An geeigneten Punkten der Schaltung können Rechteck- oder Dreieckschwingungen abgegriffen werden. Da neben einem RC-Glied auch noch die Schwellenspannung der beteiligten Kippstufe die Stabilität beeinflusst, sind Relaxationsoszillatoren wesentlich unstabiler als Resonanzoszillatoren und können deshalb in der Funktechnik nicht eingesetzt werden. Diese leichte Beeinflussbarkeit wird bei elektronischen Sirenen oder in der digitalen Messtechnik beispielsweise bei Spannungs-Frequenz-Wandlern ausgenutzt.

Moderne Oszillatoren vermeiden die Nachteile der vor etwa 100 Jahren erfundenen klassischen Oszillatorschaltungen (wie Meißner-Schaltung, Hartley-Schaltung, Colpitts-Schaltung), die bei ungünstiger Dimensionierung der Bauelemente unerwünschte "parasitäre" Schwingungen auf einigen Giga-Hertz erzeugen können, zu tieffrequenten Kippschwingungen neigen, oder eine merklich von der Sinusform abweichende Schwingungsform besitzen.

Eine mögliche Schaltung verwendet einen Differenzverstärker mit zwei Transistoren und zeichnet sich durch sehr gutmütiges Verhalten aus (siehe Differenzverstärker-Oszillator). In den untenstehenden Bildern ist eine Variante mit NPN-Transistoren dargestellt, mit der sich – abhängig von den Daten des Schwingkreises – ohne Änderung anderer Bauelemente Frequenzen im Bereich 0,05 MHz bis 40 MHz erzeugen lassen. Bei der anderen Schaltung wurden PNP-Transistoren verwendet und die Werte der Bauelemente für Frequenzen im Bereich 1 Hz bis 500 kHz dimensioniert. Bei dieser Schaltung ist der Schwingkreis auf Null-Potential, was für manche Anwendungen vorteilhaft ist (im Regelfall ist der Minuspol Bezugspunkt für alle Messungen).

Die spektrale Reinheit der erzeugten Schwingung wird besser, wenn die Rückkopplung so schwach ist, dass sie für ein sicheres Anschwingen gerade ausreicht. Bei Differenzverstärkern setzt die Amplitudenbegrenzung auch sanfter ein als bei anderen Oszillatorschaltungen. Das verringert den Oberwellengehalt.




</doc>
<doc id="14385" url="https://de.wikipedia.org/wiki?curid=14385" title="Elster-Kaltzeit">
Elster-Kaltzeit

Die Elster-Kaltzeit, auch Elster-Glazial oder Elster-Zeit, in der älteren und der populärwissenschaftlichen Literatur auch Elster-Eiszeit genannt, ist die älteste Kaltzeit, bei der es nachgewiesenermaßen zu einer großräumigen Vergletscherung Norddeutschlands gekommen ist. Sie wird zeitlich mit der süddeutschen Mindel-Kaltzeit korreliert. Die Elster-Kaltzeit wird derzeit auf etwa 400.000 bis 320.000 Jahre vor heute datiert. Sie löste den langen Zeitabschnitt des im Durchschnitt etwas wärmeren Cromer-Komplexes ab. Zwei Eisvorstöße sind weiträumig verbreitet. Auf die Elster-Kaltzeit folgt die Holstein-Warmzeit.

Die Elster-Kaltzeit ist nach der Weißen Elster benannt, einem rechten Nebenfluss der Saale. Der Name wurde erstmals von Konrad Keilhack im Jahre 1910 als „Elster-Eiszeit“ verwendet. Eine Typuslokalität benannte er nicht. Auf Beschluss der Subkommission für Europäische Quartärstratigraphie wurde ein Profil bei Voigtstedt (Kyffhäuserkreis, Thüringen) zur Lecto-Stratotyplokalität bestimmt. Der Begriff „Elster-Eiszeit“ löste den älteren Begriff „Erste Eiszeit“ ab. Heute sind in der wissenschaftlichen Literatur die Begriffe Elster-Kaltzeit oder Elster-Glazial am gebräuchlichsten. Da die Elster-Kaltzeit jedoch auch wärmere Zeitabschnitte beinhaltet, sprechen manche Forscher auch vom Elster-Komplex. Allerdings halten Litt et al. (2007) diesen Begriff für unzutreffend und lehnen den Begriff ab, da die Elster-Kaltzeit durch keine Warmzeit unterteilt ist. In der Stratigraphischen Tabelle von Deutschland 2002 ist der Name zu Elsterium abgewandelt worden, um eine Angleichung an die chronostratigraphischen Einheiten zu erreichen.

Das genaue Alter der Elster-Kaltzeit ist bis heute umstritten. Der Grund liegt in den fehlenden absoluten Datierungsmöglichkeiten der eiszeitlichen Sedimente, so dass die Altersangaben auf Korrelation mit den etwas jüngeren Ablagerungen der Holstein-Warmzeit beruhen. Aber die Altersstellung gerade dieser Warmzeit wird bis heute kontrovers diskutiert. Von einem Teil der Quartärgeologen wird nach wie vor die Sauerstoff-Isotopenstufe (MIS oder OIS) 11 für das Holstein favorisiert, während MIS-Stufe 7 aktuell kaum noch vertreten wird. Die in den letzten 10 Jahren verbesserten Methoden der Altersbestimmung, insbesondere die Uran-Thorium-Datierung sowie die Radiofluoreszenz als neue Methode der Thermolumineszenzdatierung machen die Sauerstoff-Isotopenstufe 9.3 heute am wahrscheinlichsten. Damit wird die Elster-Kaltzeit in Mitteleuropa meist mit der globalen marinen Sauerstoffisotopen-Zone MIS 10 korreliert und eine Zeitspanne 400.000 bis 320.000 Jahren vor heute angesetzt. Bei einer Einstufung des Holsteins in die MIS-Stufe 11 ergäbe sich dann für die Elster-Kaltzeit eine Einstufung in die MIS-Stufe 12 (Höhepunkt um 430.000 Jahre vor heute).

Ablagerungen der Elster-Kaltzeit sind in Nord- und Mitteldeutschland weit verbreitet zu finden. Die Ablagerungen reichen bis zur maximalen Ausdehnung des Fennoskandischen Eisschildes, die in Sachsen, Sachsen-Anhalt und Thüringen durch die Feuersteinlinie gekennzeichnet ist. Der aus kreidezeitlichen Ablagerungen stammende Feuerstein wurde mit dem Eis von Nordeuropa nach Süd- oder Mitteldeutschland verfrachtet und an den Eisrandlagen, den Endmoränen, abgelegt. Das Eis der Elster-Kaltzeit erreichte in Deutschland den nördlichen Harzrand, verlief von dort nach Südosten und überwand östlich des Bodetales den Unterharz. Südlich des Harzes wandte sich das Eis nach Westen und drang auf eine Linie Bad Langensalza-Erfurt-Weimar vor. Von dort lässt sich der Eisrand über Jena, Weida bis nach Zwickau verfolgen. Von Zwickau verlief er weiter am Erzgebirge entlang über Chemnitz, Roßwein nach Freital, das Elbsandsteingebirge und das Lausitzer Bergland. Westlich des Harzes ist der Verlauf des Eisrandes noch bis etwa Seesen, Alfeld und Rinteln recht genau bekannt. Weiter im Westen ist er unsicher, da er hier von den Gletschern der jüngeren Saale-Kaltzeit überfahren wurde und die Endmoränen eingeebnet wurden. Anhand der Feuersteinlinie lässt sich die Lage des Eisrandes jedoch ungefähr weiter festlegen: entlang des Teutoburger Waldes, nach einem Knick von wenigen Zehnerkilometern nach Süden weiter nach Nordwesten nördlich der Ems bis in die nördlichen Niederlande und die Nordsee.

Die Grenze zum älteren Cromer-Komplex wird mit einer deutlichen Abkühlung nach dem letzten Interglazial des Cromer-Komplexes gezogen. Im Lecto-Stratotypprofil wird die Elster-Kaltzeit vom Voigtstedt-Interglazial unterlagert. Allerdings ist die Grenze äußerst problematisch. Nach Litt et al. (2007) spricht einiges dafür, dass das Voigtstedt-Interglazial nicht mit dem jüngsten Interglazial des Cromer-Komplexes zu korrelieren ist, sondern mit einem älteren Interglazial innerhalb des Cromer-Komplexes. Damit ist aber die in Voigtstedt definierte Untergrenze der Elster-Kaltzeit sehr fraglich geworden; es ist mit einer größeren Schichtlücke zwischen Voigtstedt-Interglazial und der Elster-Kaltzeit zu rechnen.

Im "Elsterhochglazial", einem Zeitabschnitt von 65.000 Jahren, gab es mindestens zwei große Eisvorstöße, deren maximale Ausdehnung bis an den Fuß der deutschen Mittelgebirge reichte. Die Vorstöße werden durch einen Eisrückzug voneinander getrennt.


Im "Elsterspätglazial" erfolgte im Verlauf von rund 15.000 Jahren wieder eine allmähliche Erwärmung. Drei Interstadiale sind bisher ausgeschieden worden:

Die drei Interstadiale sind jeweils durch kurze Stadiale voneinander getrennt, das Esbeck-Interstadial ist durch ein kurzes Stadial von der Holstein-Warmzeit getrennt. Diese Stadiale sind bisher nicht mit eigenen Namen benannt worden.

Im Gegensatz zu den späteren Vereisungen ist die maximale Ausdehnung nicht mehr anhand von Endmoränen erkennbar. Eine noch weiter südlich reichende Vereisung während der späteren Saale-Kaltzeit im Westen Deutschlands löschte hier alle oberirdisch erkennbaren Spuren. Die später nicht mehr durch Eis überformten östlicheren Endmoränen wurden vor allem durch langandauernde periglaziale Prozesse eingeebnet. Rekonstruierbar ist die Ausdehnung anhand der Feuersteinlinie. In Schleswig-Holstein schufen subglaziäre Ausräumungen Rinnen bis −360 m unter NN, die später mit jüngeren Ablagerungen verfüllt wurden.



</doc>
<doc id="14386" url="https://de.wikipedia.org/wiki?curid=14386" title="Saale-Komplex">
Saale-Komplex

Der Saale-Komplex, auch Saale-Kaltzeit oder Saale-Glazial (umgangssprachlich auch Saale-Eiszeit oder Saale-Zeit) beinhaltet die mittlere von drei größeren in Nordeuropa und dem nördlichen Ost-, Mittel- und Westeuropa aufgetretenen Vergletscherungen durch den skandinavischen Inlandeisschild zwischen der älteren Elster-Kaltzeit und der jüngeren Weichsel-Kaltzeit. Sie löste die Holstein-Warmzeit ab, und ihr folgte die Eem-Warmzeit. Der Saale-Komplex wird derzeit, je nach Literatur, um 300.000 bis 130.000 bzw. 347.000 bis 128.000 Jahre vor heute angesetzt (Dauer: rund 219.000 Jahre), etwa zeitgleich mit den Vergletscherungen der Riß-Kaltzeit im Alpenraum. Die eigentliche „Eiszeit“ nimmt nur einen Teil der Saale-Kaltzeit bzw. des Saale-Komplexes ein. Die erste Kältephase (Fuhne-Kaltzeit) zu Beginn des Saale-Komplexes ist durch eine Warmzeit (Dömnitz-Warmzeit) von der eigentlichen Saale-„Eiszeit“ getrennt. Der Begriff Saale-Eiszeit oder Saale-Glazial ist in der Literatur daher zweideutig; er bezeichnet einerseits nur die Phase, in der die Gletscher bis nach Norddeutschland vorgedrungen sind, andererseits auch den gesamten Saale-Komplex. Die Begriffe gehen auch in der wissenschaftlichen Literatur häufig durcheinander.

Der Name leitet sich vom Elbnebenfluss Saale ab. Die Geologen Jakob Stoller und Konrad Keilhack prägten im Jahre 1910 den Begriff „Saale-Eiszeit“. Der Begriff sollte die ältere Bezeichnung „Vorletzte Eiszeit“ ersetzen. Allerdings legten Stoller und Keilhack weder eine Typuslokalität noch ein Typusprofil fest. Typusregion ist auf jeden Fall das Saale-Gebiet. 1986 und 1992 beschloss die Subkommission für Europäische Quartärstratigraphie, den Zeitraum vom Übergang zwischen Holstein-Warmzeit und Fuhne-Kaltzeit bis zum Beginn der Eem-Warmzeit als Saale-Komplex zu definieren. In Anlehnung an die im Deutschen übliche Endung "-ium" für chronostratigraphische Einheiten bezeichnet die Stratigraphische Karte von Deutschland 2002 den Saale-Komplex auch als "Saalium-Komplex". Die Benennung der einzelnen Stadien des Eisvorstoßes ist ebenfalls nicht einheitlich.

Die Untergrenze des Saale-Komplexes (und damit die Obergrenze der Holstein-Warmzeit) wird etwa auf das Ende der marinen Sauerstoffisotopen-Zone MIS 9 datiert. Die Obergrenze des Saale-Komplexes (und damit die Untergrenze der Eem-Warmzeit) wird mit der marinen Sauerstoffisotopen-Zone MIS 5e korreliert. Dies entspricht dem Zeitraum von 300.000 Jahren bis etwa 130.000 Jahren vor unserer Zeit. Im Alpenvorland korreliert der Saale-Komplex mit der Riß-Kaltzeit.

Der maximale Vorstoß des Fennoskandischen Eisschildes während des Drenthe-Stadiums lässt sich in Norddeutschland mit der Linie Düsseldorf – Paderborn – Hameln – Goslar – Eisleben – Zeitz – Meißen – Görlitz beschreiben. Vom östlichen Harzrand nach Osten (Polen, Brandenburg, Sachsen und Sachsen-Anhalt) blieb der Eisvorstoß ungefähr 10 bis 50 km hinter dem maximalen Vorstoß der Elster-Kaltzeit zurück. Am Nordrand des Harzes sind beide Verbreitungsgrenzen gleichverlaufend, westlich des Harzes griff das Eis des Saale-Komplexes um über 100 km weiter nach Süden als das Eis der Elster-Kaltzeit. Vor dieser Linie, d. h. vor den ehemaligen Gletschern, sind fluviatile und periglaziäre Sedimente weit verbreitet. Im Drenthe-Stadium waren auch das heutige Nordseebecken, Großbritannien und Irland betroffen.

Der Saale-Komplex lässt sich in einen unteren (auch Saale-Frühglazial) und einen oberen Abschnitt (auch Mittleres und Oberes Saaleglazial, bzw. Jüngere Saaleeiszeit), mit Vorstoß der Gletscher bis nach Norddeutschland, gliedern.

Das "Saale-Frühglazial" umfasst die:

Der "obere Teil des Saale-Komplexes" ist in Norddeutschland durch drei große Gletschervorstöße (möglicherweise in Schleswig-Holstein auch vier Vorstöße) gekennzeichnet. Sie werden meist bezeichnet als:
Unstrittige Spuren für deutliche Thermomere (Interstadiale, Intervalle) zwischen diesen Vorstößen liegen aus dem nördlichen Deutschland nicht vor.
In der Arbeit von Litt et al. (2007) mit Schwerpunkt auf dem südlichen Rand der norddeutschen Vereisungen wird der obere Teil des Saale-Komplexes wie folgt gegliedert:

Das Drenthe-Stadium entspricht der maximalen Vereisung während des Saale-Komplexes. Das letzte Stadium, das Warthe-Stadium, überzog nur Nordostniedersachsen (Teile der Lüneburger Heide), die Altmark, das Elbtal flussabwärts von Magdeburg und den Bereich östlich davon nochmals mit Gletschern (vergleiche: Südlicher Landrücken), so dass diese Landstriche geomorphologisch jünger sind als das nordwestdeutsche Tiefland, aber älter und oberflächlich verwitterter als die viel später von der Weichsel-Kaltzeit erfassten Jungmoränengebiete Nordostdeutschlands. Die zuletzt von der Saale-Kaltzeit vergletscherten Bereiche, etwa die Westfälische Bucht, ein großer Teil Niedersachsens und Sachsen-Anhalts, das südliche Brandenburg, oder die Leipziger Tieflandsbucht und die Lausitz in Sachsen, werden als "Altmoränenlandschaften" bezeichnet. Sie wurden während der späteren Weichsel-Kaltzeit durch periglaziale Prozesse wie beispielsweise Verwehungen von Flugsand und Löss weiter geformt und verändert. Als Urstromtal ist dem Saaleglazial insbesondere das Breslau-Magdeburg-Bremer Urstromtal zuzuordnen. Es wurde danach nicht wieder vom Eis überfahren.




</doc>
<doc id="14388" url="https://de.wikipedia.org/wiki?curid=14388" title="Grundmoräne">
Grundmoräne

Eine Grundmoräne ist eine glaziale Aufschüttungslandschaft, die unter Gletschern oder unter Inlandeis entsteht. Sie ist Bestandteil der Glazialen Serie. Das typische Sediment der Grundmoräne ist der Geschiebemergel, den der Gletscher ablagerte.

Gletscher führen große Mengen an Moränen­material in Form von Kies, Sand, Schluff und Ton mit. Dieses wird am Grund unsortiert abgelagert und kommt beim Abschmelzen des Eises zum Vorschein. Je nach seinem Gehalt an Karbonat wird das Ergebnis der Sedimentation als Geschiebelehm oder Geschiebemergel bezeichnet. Auch sehr große Gesteinsbrocken (Geschiebe/Blöcke) konnten durch die vordringenden Gletscher während der verschiedenen Eisvorstöße über sehr weite Strecken transportiert werden. Besonders große Steine, die vereinzelt in der sonst flachwelligen Landschaft liegen, werden Findlinge genannt.

Die typische Grundmoräne besitzt eine flachwellige Oberfläche mit relativ geringen Höhenunterschieden. Im Allgemeinen sind die Grundmoränen des Altmoränenlandes noch reliefärmer als die des Jungmoränenlandes. Es gibt aber auch dort extrem flache (z. B. das Kulmerland in Polen) oder sehr hügelige (z. B. nördlich von Chorin/Brandenburg) Grundmoränengebiete. Typisch für die Grundmoränen der Jungmoränenlandschaft sind weiterhin zahlreiche geschlossene Hohlformen, die Sölle.

Wegen der meist fruchtbaren Böden, die aus Geschiebemergel entstehen, werden Grundmoränenflächen in Mitteleuropa heute vor allem als Acker genutzt.

Weitere Formen, die sich oft in Grundmoränenlandschaften finden, sind Drumlins, glaziale Rinnen und Oser.

Grundmoränenlandschaften sind in Norddeutschland und im Alpenvorland sehr weit verbreitet. Sie entstanden während der wiederholten Vergletscherungen im Eiszeitalter. Die Grundmoränen liegen im Alpenvorland in den weiten Gletscherzungenbecken und werden oft von den Endmoränen umrahmt. In Norddeutschland nehmen sie nördlich der Endmoränenzüge sehr große Flächen ein.

In der Fachliteratur ist es in den letzten Jahren üblich geworden, den Begriff Grundmoräne nur noch auf die Landschaftsform, die unter dem Gletschereis entsteht, zu beziehen. Das Material (Sediment) selbst hingegen, welches unter dem Gletscher abgelagert wird, wird als Geschiebemergel oder "Till" bezeichnet.




</doc>
<doc id="14389" url="https://de.wikipedia.org/wiki?curid=14389" title="Endmoräne">
Endmoräne

Eine Endmoräne oder "Stirnmoräne" ist eine wallartige Aufschüttung (Moräne) von Gesteinsmaterial am Ende von glazialem Inlandeis oder eines Gletschers. Eine Endmoräne kennzeichnet die Linie des maximalen Gletschervorstoßes oder eines Gletscherstillstandes. Sie ist Bestandteil der Glazialen Serie.

Endmoränen entstehen, wenn sich am Ende eines Gletschers Abschmelzen und Eisnachschub die Waage halten. Der Eisrand bleibt dann über längere Zeit stabil; das Eis selbst bewegt sich aber nach wie vor.

Da der Eisrand nicht schnurgerade verläuft, sondern in einzelne Loben (Gletscherzungen) zerfällt, haben auch Endmoränen einen solchen lobenartigen Verlauf. (Zum Begriff "Gletscherlobus" siehe Vorlandgletscher.) Die Berührungsstelle zwischen zwei Loben nennt man auch Endmoränengabel. Dort befinden sich meist besonders kräftig ausgeprägte Endmoränen und große Gletschertore, von denen aus die Sander geschüttet wurden.

Aus dem Gletscher ausschmelzendes Material lagert sich am Eisrand ab und baut nach und nach die Endmoräne als Satzendmoräne auf. Durch das austretende Schmelzwasser wird Feinmaterial meist weggespült, so dass die Ablagerungen von Satzendmoränen für gewöhnlich grob sind (Kies, Steine, Findlinge). Zwischengeschaltet findet sich aber auch feineres Material (Sand) oder Geschiebemergel. Sehr grobes Endmoränenmaterial bezeichnet man als Blockpackung. Im Stirnbereich einer Gletscherzunge abgelagertes Material bildet normalerweise Satzendmoränen.

Stauchendmoränen, die sich oft durch sehr hohe Reliefenergie auszeichnen, entstehen, wenn durch den Druck eines vorstoßenden Gletschers älteres Material, das vor der Gletscherfront abgelagert wurde, unter horizontalen und vertikalen Druck gerät und dabei gestaucht und aufgeworfen wird. Je nach der Beschaffenheit des älteren Materials bestehen Stauchendmoränen aus verschiedensten Sedimenten. Meist handelt es sich um ältere glaziale Ablagerungen wie Sand, Eisstausee­sedimente oder älteren Geschiebemergel.

Die Stauchung des Untergrundes ist allerdings nicht an den Eisrand und damit an eine Endmoräne gebunden. Auch unterhalb eines aktiven Gletschers, innerhalb der späteren Grundmoränen­landschaft, kann durch den ausgeübten Druck Material intensiv gestört werden.

In der Fachsprache hat sich daher der neutralere Begriff "Stauchmoräne" durchgesetzt.

Da vor allem im nördlichen Mitteleuropa die Endmoränenzüge regelmäßig Lücken aufweisen, hat sich dort in der Fachsprache der Begriff Eisrandlage eingebürgert, der sowohl Endmoränen als auch andere Landformen umfasst, aus denen sich der ehemalige Eisrand rekonstruieren lässt.

Ein Beispiel für eine Endmoränenlandschaft der Weichsel-Kaltzeit befindet sich bei Chorin in Brandenburg. Die "Osthannoversche Endmoräne" der Saale-Kaltzeit erstreckt sich zwischen Drawehn in der Lüneburger Heide und Göhrde im Wendland. Ein weiteres Relikt der Saale-Kaltzeit ist die Ankumer Höhe im Landkreis Osnabrück. Jene Eisrandlage nannte man hier "Rehburger Phase". Viele Endmoränen prägen das Norddeutsche Tiefland. In Schleswig-Holstein und Mecklenburg-Vorpommern geben Ahrensbök, Kiel und Strasburg (Uckermark) markante Beispiele.

Der gesamte Übergangsbereich von den Alpen in die Alpenvorländer ist von den Randmoränen der großen Gletschervorstöße geprägt. Bei Otterfing südlich von München findet man gut ausgebildete Endmoränen. Der Hausruck in Oberösterreich, ein markanter Hügelzug, ist die bis knapp 400 m hohe Endmoräne des Hallstätter Gletschers in seiner größten Ausdehnung.

Ein eindrucksvolles Beispiel hochalpiner Endmoränen findet sich am Talende des Fornogletschers in Graubünden.



</doc>
<doc id="14390" url="https://de.wikipedia.org/wiki?curid=14390" title="Amplitude">
Amplitude

Amplitude ist ein Begriff aus der Mathematik sowie aus der Physik und Technik zur Beschreibung von Schwingungen. Er ist anwendbar bei Größen wie beispielsweise einer Wechselspannung und deren Verlauf über der Zeit. Dabei wird er definiert als die maximale Auslenkung einer sinusförmigen Wechselgröße aus der Lage des arithmetischen Mittelwertes.
Der Begriff ist auch anwendbar auf Wellen, wenn sich die Schwingung mit einer konstanten Geschwindigkeit örtlich ausbreitet (Sinuswelle).
In DIN 40110-1 wird unterschieden zwischen
Für weitere Benennungen, die nicht auf Wechselgrößen beschränkt sind, aber allgemein für periodische Vorgänge verwendet werden, z. B. bei Mischspannung, siehe unter Scheitelwert.

Der Abstand zwischen Maximum und Minimum wird bei Schwingungen als Schwingungsbreite oder auch als "Spitze-Tal-Wert" bezeichnet (früher als Spitze-Spitze-Wert).

Eine ungedämpfte sinusförmige oder harmonische Schwingung wird durch

mit der "Amplitude" formula_4, Kreisfrequenz formula_5 und Nullphasenwinkel formula_6 beschrieben. Die Amplitude ist zeitunabhängig und damit konstant.

Eine andere Möglichkeit der Beschreibung ist die komplexe Darstellung mittels der Eulerschen Formel (mit dem in der Elektrotechnik üblichen Formelzeichen formula_7 für die imaginäre Einheit:)

Diese Form erleichtert viele Berechnungen, siehe Komplexe Wechselstromrechnung. Der Ausdruck
ist die komplexe Amplitude, deren Betrag gleich der Amplitude formula_4 und deren Argument gleich dem Nullphasenwinkel formula_6 ist.

In bestimmten Zusammenhängen kann sich die Amplitude auch langsam gegenüber der zugehörigen Schwingung ändern, z. B. bei Dämpfung oder Modulation.

Eine schwach gedämpfte, nicht periodische Schwingung wird mit dem Abklingkoeffizienten formula_12 durch
beschrieben. Der Ausdruck
ist die zeitveränderliche "Amplitudenfunktion".

Zur gezielten Beeinflussung der Amplitude siehe Amplitudenmodulation.

Gerne wird die Amplitude an mechanischen Beispielen veranschaulicht, insbesondere am Pendel.

Ein Federpendel führt im Idealfall (ungedämpft) eine Sinusschwingung aus. Die Distanz zwischen
ist die Amplitude.

Ein ebenes Physikalisches Pendel schwingt auch bei ungedämpfter Bewegung weder im Winkel noch in der horizontalen Auslenkung sinusförmig. Die horizontale Distanz zwischen Umkehrpunkt und Ruhepunkt ist ein "Scheitelwert". Nur bei geringer Auslenkung, wenn der Scheitelwert sehr viel kleiner ist als die Pendellänge, also wenn die Kleinwinkelnäherung angewendet werden kann, wird die Schwingung sinusförmig, und der Scheitelwert wird zur Amplitude.

Als Amplitude im weiteren Sinne werden auch die Grenzwerte der Abweichungen vom jeweiligen Mittelwert bei anderen Kurven in grafischen Darstellungen bezeichnet. Teilweise wird der Amplitude auch eine andere Bedeutung wie "Differenz zwischen dem Minimum und dem Maximum" zugeordnet. Hier hat eine Übernahme des Fachbegriffes in die Fachsprache anderer Fachwissenschaften stattgefunden, die ihn "nicht" der oben definierten Norm entsprechend verwenden, so dass die spezielle Bedeutung fallweise ungewiss ist, zum Beispiel in der Pneumologie bei der Spirometrie, in der Seismologie beim Seismogramm oder auch in der Meteorologie und Klimageographie beim Klimadiagramm.




</doc>
<doc id="14391" url="https://de.wikipedia.org/wiki?curid=14391" title="Mission">
Mission

Mission (lat. "mittere" ‚entsenden‘, ‚schicken‘; "missio" ‚Sendung‘, ‚Auftrag‘) steht für:



Mission heißen folgende geographische Objekte:

Orte und Ortsteile:
historisch:
Mission, The Mission, weitere Eigennamen:


Siehe auch:


</doc>
<doc id="14392" url="https://de.wikipedia.org/wiki?curid=14392" title="Gitarre">
Gitarre

Die Gitarre, auch "Guitarre" (vermutlich über französisch "guitare" von , "Kithara", eine antike Leier) ist ein Musikinstrument aus der Familie der Kastenhalslauten, bei der Tonerzeugung ein Saiteninstrument, spieltechnisch ein Zupfinstrument.

Bei der Tonerzeugung wird zwischen akustischen und elektrischen Gitarren (E-Gitarren) unterschieden. Dieser Artikel legt den Schwerpunkt auf die akustische Gitarre und die Gemeinsamkeiten mit ihrer elektrischen Verwandten.

Eine Gitarre ist in drei verschiedene Teile untergliedert:

Eine Gitarre besitzt einen Hals, über dem zwischen Sattel (am Kopf) und Steg (auf dem Korpus) Saiten der Dicke nach geordnet aufgespannt sind.

Bei heutigen Gitarren besteht der Hals meist nicht aus einem Stück, sondern hat ein aufgeleimtes Griffbrett, über das die Saiten laufen. Diese Konstruktion hat zum einen Vorteile für die Stabilität des Halses, zum anderen hat die Wahl der Hölzer für Hals und Griffbrett einen erheblichen Einfluss auf den Klang und die Bespielbarkeit der Gitarre.

Bei klassischen Gitarren mit Darm- oder Kunststoffsaiten besitzt ein einfacher massiver Holzhals ausreichend Stabilität, um dem Zug der Saiten ohne störende Verformung standzuhalten. Viele Instrumente mit Stahlsaiten, vor allem Western-, beziehungsweise Steelgitarren und E-Gitarren, sowie ganz besonders E-Bässe, besitzen jedoch noch einen in den Hals eingelassenen einstellbaren Halsspannstab (auch "truss rod" oder "Trussrod"). Dieser liegt etwa in der Mitte des Halses in einem gebogenen Kanal und bewirkt eine Vorspannung des Halses entgegen der Saitenzugspannung.

Typische Gitarren haben auf dem Griffbrett Bünde. Diese helfen, die Saite beim Greifen punktgenau zu verkürzen, um einen bestimmten Ton beim Anschlagen zu erzeugen. Jedes Bundstäbchen entspricht dabei im Allgemeinen einem Halbtonschritt. Ursprünglich bestanden die Bünde aus Darm, später wurden sie auch aus Elfenbein oder Silber gefertigt. Moderne Gitarrenbünde werden meist aus Neusilber gefertigt. Bünde aus festen Materialien sind unverrückbar in das Griffbrett eingelassen. Diese Bauweise erlaubt es eigentlich nicht, Zwischentöne zu erzeugen. Mit geeigneten Spieltechniken wie zum Beispiel Ziehen "(Bending)", "Bottleneck" (beziehungsweise "Slide") ist aber auch das möglich.

Der Hals variiert je nach Art der Gitarre. Klassische Gitarren haben eher einen breiten und flachgewölbten Hals, Stahlsaitengitarren eher schmale und fast halbrunde Hälse sowie gewölbte Griffbretter.

Am Ende des Griffbrettes befindet sich der Sattel. Am verbreitetsten sind Sättel aus Kunststoff und aus Knochen. Sie werden entweder in eine in das Griffbrett gefräste Nut eingelassen oder an das Ende des Griffbretts geleimt. Kunststoffsättel werden industriell hergestellt und sind daher preiswerter. Bei Knochensätteln wird zwischen zwei verschiedenen Materialien unterschieden: zwischen ausgekochten und nahezu weißen, gebleichten Knochensätteln und sogenannten Fettsätteln, die aus nichtausgekochtem, ungebleichtem Rinderknochen bestehen. Letztere sorgen aufgrund des im Knochen verbliebenen Fettanteils für eine Schmierung in den Sattelkerben, was ein Festklemmen der Saiten erschwert. Fettsättel haben aufgrund ihrer Naturbelassenheit eine leicht gelbliche Färbung. Aufgrund guter Verarbeitbarkeit und Schmiereigenschaften werden auch verschiedene Kunststoff-Graphit-Mischungen für die Herstellung von Gitarrensätteln verwendet.

Am Ende des Halses befindet sich der Kopf / die Kopfplatte, an der das eine Ende der Saiten an den Wirbeln befestigt ist. Mittels der Stimmmechanik (übersetzte Wirbel) werden die Saiten gespannt und durch Regulierung der Spannung gestimmt. Der notwendige Druck der Saiten auf den Sattel entsteht dabei durch die Abwinkelung der Kopfplatte gegenüber dem Hals oder durch geeignete Hilfsmaßnahmen wie zum Beispiel Saitenniederhalter oder „gestaggerte“ Mechaniken (zum Ende der Kopfplatte niedriger werdende Wirbel).

Spezielle Bauformen von Kopfplatten gibt es vor allem bei manchen neueren E-Gitarren. So gibt es beispielsweise Klemmsättel, bei denen die Saiten am Sattel arretiert werden, um besonderes in Verbindung mit Vibratosystemen eine bessere Stimmstabilität zu erzielen. Noch weiter geht der komplette Verzicht auf eine Kopfplatte ("headless"-Design, populär gemacht in den frühen 1980er Jahren durch Ned Steinberger). In beiden Fällen werden die Wirbel durch Stimmmechaniken am Steg ergänzt oder ersetzt. Das heißt, die eigentliche Stimmfunktion wandert an das andere Saitenende auf dem Korpus.

Der Korpus ist je nach Bauform der Gitarre stark unterschiedlich ausgeprägt. Bei akustischen Instrumenten (im Gegensatz zu elektrischen) besteht er meistens aus einem leichten hölzernen Resonanzkörper, bestehend aus Boden, Zargen und Decke. Die Decke besitzt dabei ein meistens kreisrundes Schallloch. Es gibt jedoch, vor allem im Bereich der E-Gitarren, noch zahlreiche andere Bauformen wie zum Beispiel Halbresonanz-Gitarren und "Solidbody"-Gitarren (ohne Hohlkörper).

Auf dem Korpus befindet sich der Steg. An diesem ist das andere Ende der Saiten befestigt, oder – zumeist bei elektrischen Gitarren – unterhalb davon an einem Saitenhalter. Auch für den Steg gibt es zahlreiche unterschiedliche Bauformen mit unterschiedlichen Einstellmöglichkeiten für Saitenlage, exakte Mensur einzelner Saiten oder auch mit Sonderfunktionen (zum Beispiel Tremolo-Hebel – eigentlich Vibrato).

Die sechs verschieden dicken Saiten der heutigen Gitarre sind meistens auf E – A – d – g – h – e’ gestimmt (Standardstimmung), in der Regel wird dabei vom Kammerton a = 440 Hz ausgegangen. Die offenen Saiten liegen also auf E2 = 82,4 Hz bis E4 = 329,6 Hz. Am 19. Bund (klassische Gitarre) liegt der höchste Grundton bei B5 (deutsch h) mit 987,8 Hz, bei der E-Gitarre auf typisch 22. Bund mit D6 = 1174,7 Hz. Die Gitarre wird im Violinschlüssel notiert, eine kleine Acht unter dem Schlüssel weist darauf hin, dass sie eine Oktave tiefer als im reinen Violinschlüssel notiert erklingt.

Jede Saite klingt somit eine Quarte, das heißt fünf Halbtonschritte, höher als die darüber liegende Saite. Eine Ausnahme ist die h-Saite, die eine große Terz und damit vier Halbtonschritte höher als die darüber liegende g-Saite klingt. Es gibt verschiedene Merksprüche für die Standardstimmung, wobei die bekanntesten lauten:

Diese Stimmung ist erst seit der zweiten Hälfte des 18. Jahrhunderts gebräuchlich. Gelegentlich werden auch eine oder mehrere Saiten der Gitarre auf andere Töne gestimmt. Eine solche veränderte Stimmung nennt man Skordatur (= „Umstimmung“). Die häufigste Skordatur in der klassischen Gitarrenmusik ist D – A – d – g – h – e’. Seltener anzutreffen ist: D – G – d – g – h – e’. Um Renaissancelautenmusik auf der modernen Gitarre zu spielen, wird oft die Skordatur E – A – d – fis – h – e’ verwendet, da so die Intervalle zwischen den Saiten die gleichen sind wie zwischen den ersten sechs Chören der Renaissancelaute. So kann der originale Lauten-Fingersatz verwendet werden.

Daneben werden in modernen Folk- und Fingerstyle-Richtungen Skordaturen verwendet, bei denen die leeren Saiten einen einfachen Akkord ergeben. Solche Skordaturen werden "offene Stimmungen" (open tunings) genannt. Ein bekanntes Beispiel hierfür ist das Stück "Das Loch in der Banane" von Klaus Weiland. Durch das Mitschwingen der leeren Saiten erhält die Gitarre einen volleren Klang. Wichtige offene Stimmungen sind:


Die Stimmung D – A – d – g – h – e’ wird als Dropped-D-Stimmung manchmal auch zu den offenen Stimmungen gezählt, obwohl die leeren Saiten keinen einfachen Akkord ergeben.

In irischer Musik wird gerne die so genannte "modale" Stimmung D – A – d – g – a – d’ verwendet, und man spielt Harmonien, deren Klanggeschlecht (Dur/Moll) nicht bestimmt ist, da die Terz fehlt.

Weit seltener als sechssaitige sind Gitarren mit sieben, acht oder zehn Saiten. Die recht häufige zwölfsaitige Gitarre besitzt zum herkömmlichen EADGHE-Saitensatz sechs Saitenpaare. Die vier tiefen Saiten (E, A, d und g) werden um höher gestimmte Oktavsaiten und die zwei hohen Saiten (h und e’) um gleich gestimmte Saiten ergänzt. Die so entstehenden, jeweils eng nebeneinander liegenden Saitenpaare werden zusammen gegriffen bzw. angeschlagen. So wird ein volleres Klangbild als bei der sechssaitigen Gitarre erzielt, durch minimale Verstimmungen der Doppelsaiten gegeneinander und der daraus resultierenden Phasenschwingungen ergibt sich ein sphärisch klingender Chorus-Effekt.

Grifftabelle für einfache Akkorde: Siehe Gitarrengriff

Gitarren gibt es in unterschiedlicher Größe und Mensur. So gibt es unter anderem Kindergitarren und auch speziell für kleinere Menschen angefertigte Instrumente, die unter anderem von Künstlern wie Prince gespielt wurden, sowie Gitarren unterschiedlicher Stimmlagen innerhalb der Gitarrenfamilie wie die Terzgitarre, die Quintbassgitarre und die Kontrabass-Gitarre.

Beim Bau der Gitarre werden für den Korpus und den Hals traditionell Hölzer verwendet. Jedoch kommen auch hier vereinzelt andere Materialien, wie zum Beispiel Metall, Verbundwerkstoffe oder Carbon, zum Einsatz. Kleinteile wie die Stegeinlage bestehen je nach Preisklasse ebenfalls aus verschiedenen Materialien, z. B. Kunststoff, Horn oder Knochen. Die Mechanik kann je nach Fabrikat (teilweise) aus Holz, Kunststoff oder veredelten Metallteilen bestehen.

Beim Gitarrenbau werden in der Regel spezielle Klanghölzer verwendet – je nach Art und Eigenschaften in unterschiedlichen Kombinationen. Bei einfachen Instrumenten bestehen Decke und Boden aus Sperrholz. Diese Bauweise ist kostengünstig und darüber hinaus weniger anfällig für Risse, allerdings ist die Klangqualität in der Regel geringer als bei Gitarren aus Massivhölzern. Die nächste Stufe hat eine Decke aus massivem Holz, und Spitzeninstrumente sind meist komplett aus massiven Hölzern gefertigt.

Eine Kindergitarre ist eine normale akustische Gitarre, die für unterschiedliche Körpergrößen verkleinert gefertigt wird. Kindergrößen sind, ungefähr bezogen auf Gitarren mit einer Mensur von 65 cm, ⅛, ¼, ½, ¾ und ⅞. Der Hals mit dem Griffbrett ist schmaler und dünner, damit ihn eine Kinderhand umfassen und die Saiten ohne Mühe greifen kann. Die Saiten sind in Standard-Stimmung. Eine tiefe Saitenlage und geringe Spannung der Saiten ist von Vorteil für kleine Finger. Stahlsaiten schneiden stark ein und sind daher für Kinderhände eher ungeeignet.

Allgemein haben Flamenco-Gitarren dünnere Decken, Böden und Zargen, sind insgesamt leichter und oft flacher gebaut. Böden und Zargen werden meist aus sehr leichtem Holz hergestellt. Eine Mittelstellung zwischen der traditionellen Flamencogitarre mit Böden und Zargen aus Zypresse ("flamenca blanca") und der klassischen Gitarre nimmt wegen der verwendeten Hölzer die "flamenca negra" ein, deren Böden und Zargen aus Palisander sind. Ursprünglich aus Kostengründen, heute aber eher aus Traditions- und Gewichtsgründen verzichtet mancher Flamenco-Gitarrenbauer auf eine Mechanik mit Gewinde und verwendet stattdessen hölzerne Wirbel, wie sie bei Geigen üblich sind.

Die Flamenco-Gitarre klingt in den oberen Lagen stärker, spricht schnell an und klingt schnell aus. Dies unterstützt den harten und brillanten Charakter des Flamencospiels, der sich gegen die anderen perkussiven Elemente dieser Musik und die Tänzer durchsetzen können muss. Die Saitenlage ist traditionell eher niedrig, wodurch durchaus erwünschte perkussive Nebengeräusche entstehen. Da Flamenco-Gitarristen jedoch heute oft einen konzertanten Stil pflegen, wird mitunter eine höhere Saitenlage verlangt. Don Antonio de Torres (1817–1892) gilt als erster Erbauer spezieller Flamenco-Gitarren (um 1867).

Speziell Flamenco-Gitarren sind oft mit einem Golpeador bestückt, einer dünnen, heute meist transparenten, früher oft weißen oder schwarzen aufgeklebten Kunststoffschicht. Sie umgibt das Schallloch von drei Seiten bis hin zum Steg und soll die Gitarrendecke vor Beschädigungen schützen; insbesondere bei Verwendung der perkussiven Technik Golpe. Ein Golpeador kann auch nachträglich an einer Gitarre angebracht werden.


Es reicht meist nicht, bei einer normalen Gitarre die Saiten „verkehrt“ aufzuziehen. Ein nachträglicher Umbau ist oft unbefriedigend. Einige wenige Hersteller bauen spiegelbildlich gestaltete Modelle, bei denen ggf. selbst die Schlagbretter und Cutaways (die das Spielen in den höchsten Lagen erleichtern sollen) stimmen.

Gute Gitarren werden heute nicht symmetrisch gebaut. Die Stegeinlage ist schräg angeordnet, um den Ton auf den hohen Bünden oktavrein zu halten. So haben die tiefen Saiten – bedingt durch ihre größere Amplitude und die höhere Stegeinlage – eine größere Schwingungslänge als die hohen, dünneren Saiten. Würde man auf einer Gitarre die Saitenlage lediglich umdrehen, würde durch den schrägen Steg die Oktavunreinheit verstärkt. Die Einkerbungen im Sattel werden entsprechend der Saitendicke unterschiedlich ausgeführt. Die Deckenverleistung im Inneren ist gewöhnlich den statischen und akustischen Anforderungen entsprechend asymmetrisch konstruiert.

Heutzutage bieten die meisten großen Hersteller von elektrischen Gitarren und Stahlsaitengitarren auch spezielle Linkshänder-Gitarren an. Aufgrund der geringeren Nachfrage und des gesteigerten Produktionsaufwandes sind sie allerdings 10 bis 30 Prozent teurer als Rechtshänder-Gitarren des gleichen Modells. Es werden zudem nur einige wenige Modelle aus der Modellpalette auch als Linkshand-Version angeboten. Die Tatsache, dass es überhaupt Linkshänder-Gitarren gibt, stellt eine Eigenheit dieser Instrumentengattung dar. Selten findet man Streicher, die ihren Bogen mit der linken Hand halten und entsprechende Instrumente spielen.

Einige Linkshänder (z. B. Mark Knopfler, Gary Moore oder Noel Gallagher) spielen jedoch ganz normale Rechtshänder-Gitarren wie Rechtshänder (die Anschlaghand ist die rechte, die Greifhand die linke). Schließlich gibt es einige wenige Linkshänder (etwa Doyle Bramhall II), die eine normal rechtshändig bespannte Gitarre linkshändig umgekehrt halten und spielen.

Die zwölfsaitige Gitarre wird ähnlich gestimmt wie die sechssaitige Gitarre; zu den Saiten "E", "A", "D" und "G" kommt hier jedoch jeweils eine Oktavsaite. Die "H"-Saite und die hohe "E"-Saite werden durch gleichgestimmte Saiten gedoppelt (Schema: "Ee Aa Dd Gg hh ee"). Die sechs so entstandenen Saitenpaare werden (Saiten-)Chöre genannt, durch die sich im Vergleich zur sechssaitigen Gitarre ein volleres Klangbild ergibt (Chorus-Effekt).

Zwölfsaitige Gitarren bespannt man, abgesehen von sechschörigen Vihuelas und Barockgitarren, ausschließlich mit Stahlsaiten, da Nylon- und Darmsaiten für die enge Positionierung zu weit ausschwingen würden. Die Oktavsaiten sind dünner als die zugehörigen „normalen“ Saiten.

Bekannte Interpreten, die hauptsächlich zwölfsaitige Gitarren verwenden, sind z. B. Leo Kottke, Melissa Etheridge, Roger McGuinn und John Denver.

Zur Erweiterung des Tonumfanges (mit Erzielung eines obertonreicheren Klangs) werden Gitarren mit sieben, acht, zehn oder mehr Saiten gebaut.

Auch historische Zupfinstrumente (z. B. Pandora oder Orpheréon) verfügten zuweilen über mehr als sechs Saiten, in der Regel doppelchörig besaitet. Seit dem 19. Jahrhundert werden gezielt einchörige Instrumente mit mehr als sechs Saiten verwendet. Bekannte Beispiele sind der siebensaitige "Heptachorde" und der zehnsaitige "Décachorde" des französischen Gitarrenbauers René François Lacôte, für die die zeitgenössischen Gitarristen Ferdinando Carulli und Napoléon Coste eigene Lehrwerke verfassten.

In Russland wurde die siebensaitige Gitarre in der ersten Hälfte des 19. Jahrhunderts durch die Gitarristen Ignatz Held und Andreas Sichra (1773–1850) populär gemacht und im 20. Jahrhundert durch Musiker wie Vladimir Maškewič (1888–1971), Wasil Juriew (1881–1962) und Michail Iwanow (1889–1953) repräsentiert.

Bekannte Interpreten auf Gitarren mit erweitertem Tonumfang sind:


Eine Sonderform ist mit einem zweiten Griffbrett und dem dazugehörigen Schallloch in der Korpusdecke ausgestattet. Damit können entweder unterschiedliche Bespannungen (z. B. Darm- und Stahlsaiten) oder verschiedene offene Stimmungen bespielt werden, ohne das Instrument wechseln zu müssen. Auch ist es möglich, einen der Hälse für eine zwölfsaitige Bespannung auszulegen. Instrumente mit einem dritten Hals sind selten, aber ebenfalls schon gebaut worden.

Auch die Kontragitarre hat zwei Hälse, wobei jedoch nur einer mit einem Griffbrett ausgestattet ist, während auf dem zweiten freischwingende Basssaiten angebracht sind.

Der Künstler Günther Beckers entwickelte eine Doppelhalsgitarre mit normaler und Quintbassbespannung, jeweils erweitert um eine siebente Saite. Der Gitarrenbauer Konstantin Hirsch konstruierte und baute das Instrument. Die Gitarre ist neben den normalen Stimmungen speziell für "Das Buch der Stimmungen" gebaut und ein Unikat. Er gab ihr den Namen "g#b". Sie ist im Künstlermuseum Beckers ° Böll in Aachen zu sehen.

Weitere bekannte Musiker, die eine Doppelhalsgitarre spielen bzw. spielten sind Jimmy Page von Led Zeppelin und Don Felder von den Eagles (1974–2001).

Auch Gitarlele, Guitalele oder Guitarlele genannt; siehe Gitalele.

Lauteninstrumente wie die Gitarre waren bereits vor 5000 Jahren in Gebrauch. Ein der europäischen Laute ähnliches Instrument ist bereits auf einem Relief aus dem Tempel des Hammurapi von Babylon (1792–1750 v. Chr.) zu finden. Ägyptische Zeichnungen zeigen Frauen, die Instrumente wie eine Gitarre aus der Zeit der Pharaonen spielen. Die spanische Vihuela aus der Renaissance ist die Vorform der heutigen Gitarre. Sie hat einen schmalen Korpus und eine Wirbelplatte. Die im 16. Jahrhundert, vor allem in Frankreich benutzte Gitarre hatte meist vier Saiten bzw. Chöre.

Der Name "Gitarre" wurde aus dem Spanischen entlehnt, wobei spanisch "guitarra" über arabisch "qīṯāra" letztlich auf das altgriechische Wort κιθάρα (Kithara) zurückgeht. Jedoch gehört dieses Instrument wie die Lyra zu den Leiern der griechischen Antike und ist eher ein Vorläufer der Zither oder des Psalters.

Es wird vermutet, dass die Ursprünge der Gitarre auf eine Weiterentwicklung von Instrumenten zurückgehen, die ähnlich wie ein Monochord (Bild unten links) funktionieren. Solche Instrumente sind möglicherweise aus einem einfachen Pfeil und Bogen entstanden. Es gibt Höhlenzeichnungen in der Drei-Brüder-Höhle in Südfrankreich (ca. 14.000 v. Chr.), welche vermutlich einen Musiker zeigen, der seinen Mundraum für einen Mundbogen als Resonanzkörper verwendet (ähnlich einer Maultrommel). Diese Vermutung stützt sich jedoch lediglich darauf, dass ähnliche Instrumente wie der Berimbau (Bild unten Mitte) noch heute im Einsatz sind und es zwischen solchen rudimentären Saiteninstrumenten und gitarrenähnlichen Lauteninstrumenten einen fast nahtlosen Übergang gibt.

Solche Instrumente sind z. B. die türkische Saz (Bild unten rechts) oder die indische Sitar. Der Vorläufer der Sitar war die persische Setar mit ursprünglich drei Saiten ("se" „drei“, "tar" „Saite“). Wann und wo erstmals auf einer echten Vorgängerin der Laute gespielt wurde, ist jedoch ungewiss. Abbildungen aus Mesopotamien und Ägypten von Saiteninstrumenten mit einem Hals sowie einem Resonanzkörper weisen auf einen Ursprung in den frühen Hochkulturen hin.

Die alten Griechen spielten auf Leiern (Jochlauten). Erst in hellenistischer Zeit verwendeten sie auch Lauten, deren Saiten sich im Unterschied zu den Leiern mit den Fingern an einem Griffbrett verkürzen ließen. Die Leiern bestanden aus einem Schallkörper, der nach oben in zwei seitliche Arme auslief, welche mit einem Querholz verbunden waren. In den so gebildeten Rahmen wurden die Saiten gespannt.

Das Leierinstrument erfreute sich, nach der Eroberung Griechenlands von dort importiert, im Römischen Reich großer Beliebtheit. Aber auch Saiteninstrumente mit Resonanzkasten und Hals waren in Gebrauch und machten sogar einen wichtigen Schritt in ihrer Entwicklung. Der ursprünglich längs über den gesamten Resonanzkörper hinweg gehende Hals wurde nämlich stattdessen an den Körper angesetzt, wie es bei heutigen Gitarren auch noch der Fall ist. Diese Instrumente wurden hauptsächlich von der Unterschicht gespielt, also auch den Soldaten, die das Instrument während der Punischen Kriege (264–146 v. Chr.) nach Spanien brachten. Hier grenzt sich der Begriff Kithara jedoch von seiner griechischen Bedeutung ab und bezieht sich von nun an nicht mehr auf die ursprüngliche Jochlaute.

Durch den Einfluss des Christentums änderten sich auch die Anforderungen an die Instrumente. Besonders die Entstehung der Mehrstimmigkeit forderte eine Weiterentwicklung der Bauform. Der Resonanzkörper wurde nun vorwiegend aus Brettchen zusammengeleimt und die Seitenteile nach außen gebogen, um dem Druck, der durch den angesetzten Hals ausgeübt wurde, standhalten zu können. Manche Instrumente hatten keinen ausgeprägt bauchigen Körper, sondern einen zunehmend flachen, wie wir es von den heutigen Gitarren her kennen.

Zwar waren diese Instrumente auch im übrigen Europa bekannt, doch wurden sie hauptsächlich in Spanien verwendet. Seit dem Jahr 711 herrschten dort die Mauren, welche aus ihrer Heimat ein bereits voll ausgereiftes Instrument mitbrachten, die Oud (arab. "al-oud" „das Holz“), eine arabische Laute, die heute ohne Bünde gespielt wird (Bild links). Aus der Oud entwickelte sich die Renaissancelaute in ähnlicher Bauweise (Bild Mitte) mit Bünden: Saiten aus Darm wurden im richtigen Abstand um den Hals „gebunden“. Die Spanier entwickelten aus ihr die Vihuela, welche die gleiche Besaitung, aber einen flachen Körper hat (Bild rechts). In Spanien bestanden im 16. Jahrhundert die Vihuela und die kleinere und für ein anderes Repertoire genutzte, möglicherweise aus der "guitarra latina" hervorgegangene, vierchörige Gitarre ("guitarra de quarto órdenes") nebeneinander.

Die Renaissancegitarre besaß – gemäß einem 1555 erschienenen Buch von Bermudo – meist vier Chöre, seltener fünf oder gar sechs. Die Musik des 16. und 17. Jahrhunderts ist zum großen Teil in Form von Tabulaturen überliefert.

Als jedoch in der Barockzeit die Gitarrenmusik unter Verwendung verschiedener rhythmischer Anschlagsarten ("batteries") akkordbetonter wurde, gelangen nur bei der "Guitarra" die nötigen baulichen Anpassungen; die Vihuela starb aus. Auch diese Entwicklung vollzog sich auf spanischem Boden, mitgeprägt durch Gaspar Sanz und seine Gitarrenschule ("Instrucción de música sobre la guitarra española"), und so wurde die Gitarre mit der Zeit als „Spanische Gitarre“ (spanisch "guitarra española", italienisch "chitarra [alla] spagnola") – nun (laut Sanz durch Vicente Espinel in Madrid saitenmäßig ergänzt) fünfchörig (in Frankreich vierchörig) und im Gegensatz zur Vihuela mit nur einer Saite im ersten Chor – bezeichnet.

Die fünfchörige (gelegentlich auch sechschörige) Barockgitarre gelangte im 17. Jahrhundert über Italien durch Francesco Corbetta nach Frankreich, wo sie am Hof von Ludwig XIV. ein beliebtes Musikinstrument wurde. Auch der deutsche Instrumentenbauer Joachim Tielke fertigte um 1684 solche Gitarren an. In Italien unterschied man im 17. Jahrhundert die "chitarra" von einer kleineren "chitarriglia".

Mit dem Fortschreiten des Barock tendierte die Spielweise wieder von den "batteries" und dem (barocken) "rasgueado" (italienisch "battuto" oder "battente", englisch „strumming“), dem Schlagen von Akkorden, zum kontrapunktischen und Melodiespiel, dem "punteado", bis ein endgültiger Bruch schließlich in die Frühklassik mündete. Während dieser Zeit änderte sich die Besaitung der Gitarre ständig, da nun die Melodie, als tragendes Element, in den Vordergrund trat und viel experimentiert wurde.

Kurz vor 1800 fand eine Art Ringtausch zwischen Mandora und Gitarre statt. Die Gitarre, die als Barockgitarre meist nicht linear, sondern rückläufig ("reentrant tuning") gestimmt worden war (zum Beispiel e' – h – g – d' – a) und somit (ähnlich wie bei der Ukulele) zum Melodiespiel auf den Basssaiten (mit Daumen) Gelegenheit gab, übernahm die sechste Saite und die Stimmung der Mandora (e' – h – g – d – A – G, später auch e' – h – g – d – A – E). Die Mandora dagegen übernahm von der Gitarre die inzwischen eingeführte Besaitung mit einzelnen Saiten statt Chören. Ein später Erbe dieser Entwicklung auf Seiten der Mandora war die sogenannte Gitarrenlaute, die durch die fehlende Doppelchörigkeit aber nicht die Möglichkeit des selektiven Spielens der in Oktaven gestimmten Doppelsaiten einer Barockgitarre hat.

Im 17. und 18. Jahrhundert fand die Gitarre wie die Laute auch Verwendung als Generalbassinstrument. So gelehrt etwa 1674 von Gaspar Sanz, 1680 von Nicola Matteis und 1714 Santiago de Murcia.

Weitere Komponisten, welche die Gitarre im Barockzeitalter populär machten, waren Giovanni Paolo Foscarini, Girolamo Montesardo ("Nuova inventione d'intavolatura", 1606) und Robert de Visée. 

In Spanien veröffentlichte der Musikprofessor Fernando Ferandiere (etwa 1740–1816) noch 1799 ein Lehrwerk für eine sechschörige Barockgitarre, für die er auch zahlreiche Werke komponiert hatte.

Auf diese Weise wandelte sich Ende des 18. Jahrhunderts die (vier- bis) fünfchörige Barockgitarre bzw. Spanische Gitarre, wie sie etwa von Antonio Stradivari gebaut wurde, zur sechssaitigen Gitarre des 19. Jahrhunderts, mit einer robusteren und im Vergleich zu den Verzierungen der Barockgitarre funktionaleren Bauweise und, ablesbar auch in der Gitarrenliteratur ab etwa 1750, Möglichkeiten zu einer differenzierten Tonbildung und gleichzeitig einem die tiefen Töne stärker als zuvor hervorhebenden sowie auch durch eine lineare Stimmung Akkordumkehrungen (vgl. auch Voicings) beim "Strumming" erst richtig hörbar machenden und sonoreren, der Musik der Romantik und des Impressionismus entsprechenden Klang. Für den Klang bedeutsam war auch der Einbau von Resonanzleisten, welche die Schwingungen auf den gesamten Körper übertrugen, wodurch die Töne lauter wurden und sogar den Einsatz der Gitarre in kleineren Orchestern ermöglichte. Eines der ersten Lehrwerke für die klassische Gitarre wurde 1825 von Dionisio Aguado veröffentlicht.

Ihre klassische Epoche durchlebte die Gitarre hauptsächlich in Wien und Paris. In Wien prägte Johann Georg Stauffer das Wiener Gitarrenmodell. Später als in diesen beiden Städten bildete sich in London ein weiteres Zentrum der Gitarre europäischen Ranges aus. Zu den international wirkenden Komponisten der Gitarre zählte auch der Geigenvirtuose Niccolò Paganini.

Die Hauptkomponisten für die Gitarre in ihrer Blütezeit waren neben anderen in Paris Fernando Sor, Ferdinando Carulli, Dionisio Aguado, Pierre-Jean Porro und Napoléon Coste (1805–1883) sowie in Wien Mauro Giuliani, Johann Kaspar Mertz und Johann Dubez. In London waren zahlreiche Gitarristen, auch aus Deutschland stammend, wohnhaft. Die bekanntesten unter ihnen waren Leonhard Schulz, Wilhelm Neuland, Luigi Sagrini (* 1809), Felix Horetzky (1796–1870), Ferdinand Pelzer (1801–1861) und dessen Tochter Catharina Josepha Pratten (1821–1895). Zu den bedeutendsten Gitarrenvirtuosen nach Giulianis Lebenszeit zählte Giulio Regondi (1822–1872); er lebte ebenfalls die längste Zeit seines Lebens in London. Schon in der Romantik führten jedoch einige Entwicklungen wieder nach Spanien. Der Gitarrist Francisco Tárrega (1852–1909) beschritt dort mit seinen bis heute üblichen Griff- und Anschlagtechniken neue Wege. Zur gleichen Zeit vervollkommnete der Gitarrenbauer Antonio de Torres (1817–1892) die Gitarre in Form und Abmessungen, Anordnung der (fächerförmigen) Decken-Verleistung und mechanischen Details.

Zwar gab es im 20. Jahrhundert – auch bedingt durch Elektronik – viele Neuerungen wie beispielsweise die "Bottoni-Greci-Gitarre" von 1987, doch deren Auswirkungen werden sich erst zu einem späteren Zeitpunkt abschließend beurteilen lassen. Die Torres-Gitarre ist bis heute die Grundlage einer jeden klassischen Konzertgitarre geblieben.

Die Bezeichnung "Klassische Gitarre" wurde, abgesehen von russischen Veröffentlichungen zur Gitarre zwischen 1904 und 1915, erst nach 1946 durch die Zeitschrift "Guitar Review" eingeführt.

Grundsätzlich unterscheidet man zwischen Spieltechniken, die mit der Greifhand oder der Anschlagshand ausgeführt werden. Einige Techniken werden in der Praxis auch mit beiden Händen angewendet, z. B. Tapping. Siehe auch: Technik der klassischen Gitarre.

Die Gitarre wird bei der "klassischen Haltung" auf dem Oberschenkel auf Schlaghandseite abgestützt. Die untere Einbuchtung im Corpus kommt auf dem Oberschenkel der Greifhandseite zu liegen. Der Hals zeigt dann zur Greifhandseite hin. Es ist möglich, mit einer Fußbank das Bein der Greifhandseite um einige Zentimeter zu erhöhen, damit eine bessere Sitzhaltung erreicht werden kann. Dabei zeigt der Hals etwa im Winkel von 45° nach oben. Alternativ kann der Fuß der Greifhandseite auf dem Boden bleiben, wenn zwischen dem Auflagepunkt des Gitarrenkorpus und dem Bein der Greifhandseite eine Gitarrenstütze oder ein Kisse angebracht wird, womit ebenfalls die Position des Halses erhöht und eine optimale Haltung erreicht werden kann.

Der Ellenbogen der Greifhandseite sollte entspannt und um etwa 90 Grad abgewinkelt sein. Der Unterarm der Anschlagshand sollte in der Nähe des Ellbogen auf dem Zargenrand liegen. Die Greifhand sollte so positioniert werden, dass noch etwas Platz zwischen dem Hals und dem Handgelenk ist. Der Daumen sollte auf der Rückseite des Griffbretts etwa in der Mitte aufgesetzt werden. Beim Greifen der Saiten ist zu vermeiden, dass die Fingergelenke der Greifhand durchgedrückt, also entgegen ihrer natürlichen Abknickrichtung gedehnt werden; diese für den Anfänger möglicherweise anstrengende Handhaltung kann durch etwas Übung leicht aufrechterhalten werden, sie ist für ein präzises Spiel und viele Techniken der Greifhand unverzichtbar. Beim Greifen eines „Barrégriffes“, also beim Greifen mehrerer Saiten mit nur einem Finger, sollte der durchgestreckte Finger nahe am Bundstäbchen angesetzt werden.

Die Anschlagshand, bei Rechtshändern ist es die rechte, ist die „führende“ Hand. Sie gibt oftmals Rhythmus und Geschwindigkeit vor und produziert die Töne durch Anschlagen der Saiten.

Die Finger der "Anschlagshand" werden mit p-pulgar (Daumen), i-index (Zeigefinger), m-medio (Mittelfinger), a-anular (Ringfinger) und M-meñique (auch q, ch, l, k und e, bei Dionisio Aguado c) (Kleiner Finger) bezeichnet.

Generell lassen sich für die Anschlagshand die Spieltechniken


unterscheiden, die sich jeweils wieder in verschiedene Techniken aufteilen lassen:

Beim Zupfen werden einzelne Saiten mit den Fingern (Fingerkuppen und/oder Fingernägeln) oder einem Plektrum angeschlagen. Auf diese Weise können nicht nur einstimmige Tonfolgen, sondern auch mehrstimmige Sätze gespielt werden. Um höhere Geschwindigkeiten zu erreichen und das Spiel flüssiger klingen zu lassen, wird dabei meistens eine Form des "Wechselschlags" eingesetzt: Zwei oder mehr Finger schlagen die Saiten abwechselnd an. Eine besondere Form des Wechselschlags ist das "Tremolo", bei dem drei oder mehr Finger in schneller Folge hintereinander dieselbe Saite anschlagen. Diese Technik ist besonders von der Mandoline her bekannt und ist häufig in spanischer und lateinamerikanischer Gitarrenmusik sowie in härteren Formen von Heavy Metal zu hören. Man unterscheidet darüber hinaus die Anschlagsarten ' (span. „schießend, ziehend“) und ' (span. „aufstützend“), die die Klangeigenschaften des produzierten Tones verändern. Beim "tirando" wird nur die Saite berührt, die gerade angeschlagen wird, beim "apoyando" kommt der Finger nach dem Anschlag auf der nächstunteren Saite zu liegen. Eine weitere Form der Klangerzeugung ist der einhändige "Flageolett"-Anschlag, bei dem nach Zupfen der Saite diese sofort wieder mit einem anderen Finger (normalerweise p) abgedämpft wird. Diese kann man auch bei gezogener Saite spielen, so dass ein pfeifender Ton entsteht – die genaue Funktionsweise des Flageoletts und das Ziehen der Saite wird weiter unten ausführlicher erklärt.


"Vibrato": der greifende Finger wird in einer mehr oder weniger schnellen „Zitterbewegung“ entlang der Halsachse leicht hin und her bewegt. Dadurch ändert sich die Tonhöhe nach oben hin in einer leichten Schwingung. Man unterscheidet dabei das klassische Vibrato (die Vibratobewegung wird parallel zur Saite ausgeführt, es entsteht ein eher dezenter Effekt) und das meistens von E-Gitarristen benutzte Vibrato, bei dem wie beim Bending (ziehen) die Saite entlang des Bundstäbchens periodisch gedehnt und entspannt wird.

"Flageolett": eine Technik, um Obertöne einer Saite oder eines gegriffenen Tones zu erzeugen. Durch leichtes Berühren der Saite an bestimmten Punkten erklingt ein höherer Ton anstatt des eigentlich angeschlagenen Tones. Bei dieser Technik berührt ein Finger nur leicht bestimmte Punkte der Saite und verlässt ganz kurz nach dem Anschlag wieder die Saite. Diese Technik ist nur an bestimmten Punkten der Saite für das Flageolett sinnvoll einsetzbar. Bei ungegriffenen Saiten sind diese Punkte:


Flageoletts sind auch an anderen Stellen möglich, sind jedoch je nach Bauart der Gitarre mehr oder weniger leicht darstellbar. Sie bilden dann nicht mehr so klare einzelne Töne, sondern es erklingen Mehrklänge.

Man unterscheidet:


„Aufschlagbindung“ (engl. "Hammer-On"): ein Finger der Greifhand schlägt kräftig auf die Saite. Die Tonerzeugung erfolgt also „klopfend“ durch die Greifhand.

„Abzugsbindung“ (engl.: "Pull-Off"): Ein Finger, der vorher einen Ton gegriffen hat, lässt die Saite schnell los bzw. zupft sie leicht an. Dadurch erklingt der Ton, der an einem tieferen Bund auf dieser Saite gegriffen ist, oder aber der Ton der leeren Saite (= Zupfen mit der linken Hand).

"Ziehen" (auch „bending“): Man greift eine Saite und zieht oder schiebt diese mit dem greifenden Finger entlang der Bundachse, wodurch der momentan erklingende Ton sich stufenlos dem angepeilten Zielton annähert, bis dieser schließlich erklingt.

"Gleiten" (auch „sliden“ oder „Glissando“): der Finger gleitet von einem Bund zu einem anderen, wobei die Saite heruntergedrückt bleibt. Diese Technik wird häufig im Blues mit einem Röhrchen, dem Bottleneck gespielt. Dieser steckt auf einem Finger der Greifhand.

"Rake": die ersten paar Saiten werden vor dem eigentlichen Ton abgedämpft, aber trotzdem mit angeschlagen. Dadurch entsteht ein perkussiver Effekt.

"Dead Note" auch "Ghost-Notes" genannt: die/der Finger wird nur leicht auf die Saite(n) gelegt, so dass beim Anschlag der durch die Fingerberührung gedämpften Saiten nur ein perkussives Geräusch erzeugt wird. Ein Beispiel dafür ist in Nirvanas "Smells Like Teen Spirit" zu hören, oder beim Intro von AC/DCs "Back in Black".

Stücke für Gitarre werden sowohl in Tabulaturen als auch (seit etwa 1750) in Noten schriftlich festgehalten. Ein, die Stimmführung erleichternder spezieller Notensatz für Gitarre (wie er sich später auch bei Mauro Giuliani findet) wurde erstmals zwischen 1779 und 1802 von dem französischen Gitarristen Jean-Baptiste Phillis (* um 1751; † 1823) verwendet. Die Noten für Gitarre werden im oktavierten Violinschlüssel notiert, erklingen also eine Oktave tiefer. Die Tabulaturschreibweise, welche die Saiten der Gitarre nachbildet, geht auf die Lautenmusik der Renaissance zurück. Während klassische Gitarrenstücke bevorzugt in Noten angeboten werden, ist die Tabulatur für Musik aus den Bereichen Rock, Pop und Folk populär. Dem Gitarrenspieler werden oft beide Varianten (wie im Bild dargestellt) angeboten.

Wie in der übrigen Musikliteratur wurden für neuartige Klangeffekte der Neuen Musik im 20. Jahrhundert (z. B. in Kompositionen von Xavier Benguerel, Alberto Ginastera, Roman Haubenstock-Ramati, Leo Brouwer, Hans Werner Henze und Hans-Martin Linde) auch für die Gitarre neue Formen der Notation entwickelt.

Im Gegensatz zur akustischen Gitarre werden bei einer elektrischen Gitarre (E-Gitarre) die Saitenschwingungen über elektrische ferromagnetische Tonabnehmer (Pick-up) oder über Piezokristalle abgenommen und elektronisch verstärkt, üblicherweise mit Gitarrenverstärkern. Der Korpus ist zumeist massiv. Außerdem gibt es elektroakustische Gitarren. Dabei handelt es sich um akustische Gitarren mit eingebautem Tonabnehmer. Dadurch kann der Ton wie bei der elektrischen Gitarre über einen Verstärker ausgegeben werden.

Als Urform der Jazzgitarre (auch "Plektrum-" oder "Schlag-Gitarre" genannt) wird das 1923 hergestellte Modell "L-5" der Gibson Mandolin-Guitar Manufacturing Company in Kalamazoo/USA angesehen. Für damalige Verhältnisse wartete die Gitarre mit Besonderheiten auf, die den Standard für alle danach gefertigten Instrumente dieses Genres bestimmen sollten. Dies war ein nach Vorbild des Geigenbaus hergestellter Korpus mit gewölbtem Boden und gewölbter Decke (Archtop). Anstelle der sonst runden oder manchmal ovalen Schalllöcher waren zwei F-Löcher in die Decke eingearbeitet. Die Stahlsaiten waren in einem trapezförmigen Saitenhalter aus Metall am unteren Korpusende verankert, welche über einen zweiteiligen und damit höhenverstellbaren Steg führten. Der Hals – bis dahin in Höhe des 12. Bundes mit dem Korpus verbunden – gab bei der L-5 volle 14 Bünde frei. Um dem Saitenzug des nun längeren Halses entgegenzuwirken, zog Gibson in einer Nut längs des Halses einen Stahlstab ein, der an seinem Austritt, unter dem Sattel auf der Kopfplatte, über eine Gewindemutter noch zusätzlich verstellbar war. Die Firma hatte lange Zeit ein Patent auf diese Konstruktion.

In der musikalischen Entwicklung verdrängte die Jazzgitarre das bis dahin verbreitete Banjo. Es wurde zwar im traditionellen Jazz weiterhin eingesetzt, musste aber mit Anbruch der Swing-Ära das Feld der „edler“ klingenden Gitarre überlassen, die von da an in keiner Big Band und keinem Tanzorchester fehlen durfte. Problematisch für den Gitarristen jener Tage war jedoch die Situation, sein Instrument gegen die vorherrschenden Lautstärken in mittleren und großen Orchestern hörbar in Szene zu setzen. Der Instrumentenbau reagierte, indem die Resonanzkörper der Jazzgitarren zunehmend vergrößert wurden. Von den damals 16" (untere Korpusbreite) der ersten L-5 betrugen die Maße gegen Ende der 1930er Jahre 18" bei Gibsons "Super 400" und bei einigen Modellen von Epiphone und Stromberg sogar 19". Wirkliche Abhilfe schafften hier die ebenfalls in den 1930er Jahren begonnenen Versuche, die Schwingungen der Stahlsaiten durch elektromagnetische Tonabnehmer zu erfassen und diese von Verstärkern aus der frühen Radiotechnik übertragen zu lassen. Diese ersten Tonabnehmer wurden entweder freischwebend mittels entsprechender Halterungen zwischen Decke und Saiten platziert oder direkt auf die Decke montiert. Damit war es Jazzgitarristen möglich, neben den Aufgaben in der Rhythmus-Sektion nun auch als Solist aufzutreten. Die erste industriell in Serie gefertigte Jazzgitarre mit fest montiertem Tonabnehmer war die 1936 eingeführte Gibson ES-150. Der US-Jazzgitarrist Charlie Christian wurde mit diesem Modell zum Pionier des „bläserartigen“ Spiels (Läufe, Melodielinien und Soli) auf der elektrisch verstärkten Gitarre. Er ist besonders in Aufnahmen von 1939 bis 1941 mit Benny Goodmans Combo-Besetzungen zu hören.

Nach Ende des Zweiten Weltkriegs ergaben sich weitere Veränderungen im Bau der Jazzgitarre. Zum Spiel in den oberen Lagen, also aufwärts des 14. Bundes, musste die angrenzende Korpusflanke stets überwunden werden. Als Neuerung wurden Instrumente mit einem „Cutaway“ ausgestattet, einer Ausformung an der beschriebenen Stelle in den Korpus, womit die linke Schulter der Gitarre niedriger liegt als auf der rechten Seite. Der so gewonnene Raum gestattet der Greifhand auch oberhalb des 14. Bundes noch bequemes Spiel. Die dem Geigenbau entlehnten Wölbungen des Bodens und der Decke mussten aus entsprechend massiven Holzplanken herausgearbeitet werden, was hohe Handwerkskunst verlangte und deshalb auch sehr zeitaufwendig war. So ging man dazu über, Böden, Decken und Zargen aus Sperrholz zu fertigen, die dann in speziellen Pressmaschinen geformt wurden. Die sonst dazu verwendeten Hölzer (meistens Ahorn und Fichte) bildeten nur noch die äußere Furnierschicht, so dass der optische Eindruck nach dem Finish keinen Unterschied zur anderen Bauweise erkennen lässt. So konnten Gitarren schneller und kostengünstiger hergestellt werden. Für Spitzenmodelle kam diese Produktionsweise nicht zum Tragen, wenngleich massiv hergestellte Decken auch mit Zargen und Böden aus Sperrholz kombiniert wurden. Die Sperrholzgitarren klingen in der rein akustischen Anwendung mit den aus Massivhölzern hergestellten Instrumenten nicht gleichwertig. Doch dieser Vergleich trat zunehmend in den Hintergrund, da die Jazzgitarren immer häufiger nur noch elektrisch verstärkt gespielt wurden. Dazu hatten die namhaften Hersteller eigene Tonabnehmer (Pickups) im Programm, wie Gibson seinen „P 90“ oder die „New Yorker“ Pickups bei Epiphone. Andere ließen sich von Firmen wie DeArmond (z. B. Gretsch) beliefern, um die Elektrik ihrer Gitarren mit diesen Produkten auszustatten.

Allerdings beginnt 1950 die Dekade, in der Gibson mit der Les Paul und der ES 335 Furore machte und die radikal neukonzipierten Gitarren von Leo Fender aus Kalifornien den Markt gewaltig belebten. Diese Instrumente revolutionierten den Gitarrenbau und setzten Maßstäbe in einer Nachhaltigkeit, die bis in die heutige Zeit reichen. Die damit einsetzende Jagd nach Sustain, Effekten und Overdrive war nie das Terrain der Jazzgitarre. Dass sie trotz dieser Entwicklung von den führenden Herstellern weiterhin gefertigt wurde, hatte nicht nur traditionelle Gründe. Kein anderer Gitarrentyp bringt in der akustischen Spielweise perkussivere Anschläge und überträgt sauberer filigrane Rhythmusarbeit. Elektrisch verstärkt, mit guten Pickups, liefert sie aufgrund ihrer Resonanzstruktur klare, runde Töne mit Substanz. Mit diesen Vorzügen konnte die Jazzgitarre seit ihrer Entstehung immer neue Generationen von Musikern für sich begeistern.

Die Halbresonanzgitarre (auch Semiakustik-Gitarre bzw. Halbakustikgitarre genannt) ist eine Variante der elektrisch verstärkten Vollresonanz-Gitarre und unterscheidet sich von dieser durch die regelmäßig geringere Korpustiefe. Gelegentlich sind auch die übrigen Korpusmaße kleiner ausgelegt als bei der Vollresonanz-Gitarre.

Die reine Halbakustik-Bauweise wird als "Hollow Body" bezeichnet. Daneben ist die Verarbeitung eines massiven Mittelbalkens ("Center-Block" / "Sustain-Block") anzutreffen, welcher in der Verlängerung des Halses bis an das untere Korpusende reicht und diesen in zwei Kammern teilt. Diese Instrumente werden häufig unter der Bezeichnung "Semi-Solids" geführt, da das Klangverhalten der massiv gebauten E-Gitarre ("Solid-Body") näher kommt als der rein akustischen Version. Die Bezeichnung "Semi-Solids" wird auch für massiv gebaute E-Gitarren verwendet, die im Korpusinneren mit größeren Resonanzkammern ausgestattet sind.

Die typische Halbresonanzgitarre ist eine F-Loch-Gitarre mit Single-Cutaway (siehe Bild) oder Double-Cutaway. Ebenso sind auch Modelle ohne F-Löcher erhältlich, um den unerwünschten Rückkopplungseffekt im Verstärkerbetrieb zu minimieren. Die elektrische Regelausstattung umfasst zwei Tonabnehmer, die samt Volumen- und Klangregelung auf der Decke angebracht sind.

Eine Baritongitarre ist größer und eine Quinte tiefer gestimmt als eine Gitarre in Standardstimmung.

Der E-Bass entstand aus dem Bemühen, den Kontrabass durch ein elektrisch verstärkbares Instrument mit gleicher Stimmung und gleichem Tonumfang, aber der Größe einer Gitarre zu ersetzen. Er hat in der Regel vier Saiten (es gibt aber auch Modelle mit fünf und mehr Saiten), die durchgehend in Quarten gestimmt werden. Deshalb sind die E-, A-, D- und G-Saite eine Oktave tiefer gestimmt als die korrespondierenden Saiten einer Gitarre. Wie die Gitarre ist der E-Bass ein oktavierendes Instrument, sein Ton erklingt also eine Oktave tiefer als notiert.

Silent Guitar und Traveller Guitar sind die Markennamen von korpuslosen Gitarren, die sich wie eine Konzert- oder eine Folk- oder Westerngitarre spielen. Durch den fehlenden Resonanzkörper sind sie wesentlich leiser, aber auch leichter als andere Gitarren. Der Ton kann darüber hinaus auch elektrisch abgenommen und verstärkt werden.

Eine andere Art von Traveller Guitar, oder Reisegitarre, ist die 1975 in Deutschland von Roger Field erfundene Foldaxe (1977 für kurze Zeit von Hoyer hergestellt, dann von Field weiterentwickelt), eine zusammenfaltbare E-Gitarre, die für Chet Atkins konzipiert war (in Atkins’ Buch "Me and My Guitars").




</doc>
<doc id="14393" url="https://de.wikipedia.org/wiki?curid=14393" title="Heiliger Geist">
Heiliger Geist

Der Heilige Geist (griechisch Ἅγιον Πνεῦμα oder Πνεῦμα τὸ Ἅγιον, lateinisch "Spiritus Sanctus") ist im Christentum die dritte Person der göttlichen Trinität, wie dies im Nicäno-Konstantinopolitanum, einem wichtigen altkirchlichen Bekenntnis, formuliert wurde. Der Heilige Geist, oft Geist Gottes genannt, ist Gott, keine eigenständige Gottheit oder Substanz. An Pfingsten feiert die Christenheit das Kommen des Heiligen Geistes. Bereits im Alten Testament ist vom Geist Gottes die Rede.

Im Tanach bedeutet das Wort ruach (רוּחַ), mit dem später der „Geist“ bezeichnet wird, zunächst „Wind“ (z. B. ), dann auch „Hauch“, „Atem“ (; ). In anderen Zusammenhängen bedeutet es den geistigen Zustand, die Stimmung, die Haltung, Einstellung (; ; ). Die Haltung eines Menschen, seine ruach, gilt in gewisser Weise als selbstständiges Wesen. Sie kann sich ausbreiten, auf einen anderen Menschen überspringen und in ihn eindringen.

Der zunächst auf Menschen angewendete Begriff des Geistes wurde entsprechend der menschenähnlichen Gottesauffassung auf Gott übertragen . Nur sehr selten hebt der Tanach die Zugehörigkeit des Geistes zu Gott durch das Attribut „heilig“ hervor (; ; ); meist spricht er vom „Geist Gottes“: "ruach JHWH" – „Atem des Herrn“; "ruach ha-Elohim" – „Gottesatem“; "ruchaká" – „dein Atem“. Diese Ausdrücke bezeichnen die wirkmächtige Gegenwart Gottes im Leben der Menschen. Der Begriff „Geist Gottes“ ist insbesondere auf Israel und die Propheten bezogen, findet aber auch auf die ganze Schöpfung Anwendung.

Die Begriffskombination "ruach ha-qodesh" erscheint in den hebräischen Schriften einmal als "ruach qodesho" („sein heiliger Geist“, ) und einmal als "ruach qodeshcha" („dein heiliger Geist“, ). Die Kombination von "ruach" mit dem Gottesnamen oder der Gottesanrede ist dagegen häufig. Zu den bedeutsamsten Aussagen zählen:
In erscheint „der Geist des Herrn“ unmittelbar neben „sein Heiliger Geist“ "(ruach qodesho)".

Weitere Aussagen finden sich in den Apokryphen, vor allem in der Weisheitsliteratur.

Im griechischen Neuen Testament erscheint der Begriff „Heiliger Geist“ "(pneuma hagion"; "πνεῦμα ἅγιον)" rund einhundertmal. Im Johannesevangelium wird er auch Paraklet ("παράκλητος", „Tröster“, „Beistand“) genannt.
Insbesondere folgende Stellen sind in der Theologie des Heiligen Geistes von größerer Bedeutung:

Das Christentum hat auch verschiedene alttestamentliche Stellen auf den Heiligen Geist bezogen, insbesondere die Weissagung, dass Gott seinen Geist über alle Menschen ausgießen wird in ; und die Erwähnung in der Schöpfungsgeschichte: „Gottes Geist schwebte über dem Wasser“ . Im weiteren Verlauf der biblischen Geschichte waren es einzelne Personen, die nach christlicher Auslegung besonders mit dem Geist Gottes erfüllt waren, z. B. Josua .

"Ruach HaQodesh", der "Heilige Geist", wörtlich „heiliger Atem“, „heiliger Wind“, auch als "Ruach JHWH" („Atem Adonais“) bezeichnet, wird allegorisch gebraucht. Der "Heilige Geist" wird im Judentum als die Kraft Gottes, nicht als eine göttliche Person betrachtet und entsprechend auch nicht als Person angerufen. Diese Kraft kann von Menschen Besitz ergreifen und sie bevollmächtigen.

Die Alte Kirche ging einen langen Weg der Lehrentwicklung auch im Blick auf den Heiligen Geist; der Diskurs fand der in den altkirchlichen Bekenntnissen seinen Niederschlag.

Der Heilige Geist ist im Christentum „eins“ mit Gott-Vater und Gott-Sohn und wird zugleich als eine der drei Personen oder Hypostasen Gottes resp. als dritte Person des dreieinen Gottes verstanden (siehe Dreifaltigkeit). Vor der Etablierung des trinitarischen Dogmas war dies umstritten, insbesondere bei der Gruppe der Pneumatomachen im 4. Jahrhundert. Heilig wird im Sinne von göttlich gebraucht, um den Geist Gottes von anderen Geistwesen zu unterscheiden.

Die Christen fast aller Kirchen feiern 50 Tage nach Ostern Pfingsten als den Tag, an dem die Jünger Jesu „mit dem Heiligen Geist erfüllt“ () und zur Verkündigung des Evangeliums bevollmächtigt wurden (, ).

Der sogenannte Filioque-Streit, das heißt die Auseinandersetzung, ob der Heilige Geist von Gott Vater oder von Gott Vater und Gott Sohn ausgeht, war eine entscheidende Unstimmigkeit zwischen östlicher und westlicher Kirche und stellt noch heute einen wichtigen dogmatischen Unterschied zwischen der römisch-katholischen Kirche und den aus ihr erwachsenen reformatorischen Kirchen einerseits und den orthodoxen Kirchen andererseits dar. Die Westkirche hält im Allgemeinen an dem "filioque" fest, betont also Ursprung und Herkunft des Heiligen Geistes mittels einer Hauchung aus Gott Vater und Gott Sohn. Die Ostkirche bewahrt die ursprüngliche Textfassung des Nicäno-Constantinopolitanums und betont die Gott-Vater und Gott-Sohn gleichgestellte Göttlichkeit des Geistes.

Nach der Lehre der römisch-katholischen Kirche geht der Heilige Geist aus dem Vater und dem Sohn als einem einzigen Prinzip durch „eine einzige Hauchung“ hervor. Im Unterschied zum Sohn, der durch „Zeugung“ aus dem Vater hervorgeht, geht der Geist den Weg der Hauchung aus dem Vater und dem Sohn.

Die römisch-katholische Kirche kennt wie die orthodoxen Kirchen das Sakrament der Firmung. Darin empfängt der Firmling die sieben Gaben des Heiligen Geistes, die dessen Früchte hervorrufen sollen. Die Firmung wird meist Jugendlichen von einem Bischof gespendet, im Gegensatz zu den orthodoxen Kirchen, wo sie sofort nach der Taufe durch den Priester gespendet wird und den erwachsenen Katechumenen, denen meist alle drei Initiationssakramente in einer einzigen Feier gespendet werden.

Die anglikanische Kirche ist aus der westlichen Tradition hervorgegangen; ihr Credo enthält darum das Filioque. Mit Blick auf die Orthodoxie und nach vorbereitenden Beschlüssen der Lambeth-Konferenz 1978 und 1988 beschlossen die Primasse der verschiedenen Kirchenprovinzen 1993, dass in künftigen Ausgaben liturgischer Bücher das Filioque nicht mehr verwendet werden solle. Die amerikanische "Episcopal Church" fasste für sich 1994 denselben Beschluss. Beide Beschlüsse blieben folgenlos.

Auch in der anglikanischen Kirche wird die Firmung vom Bischof durch Auflegen der Hände gespendet, damit der Firmling im Heiligen Geist gestärkt wird.

Von ihrer gemeinsamen Wurzel her sind die evangelischen Kirchen vom Filioque ähnlich geprägt wie die katholische Kirche.

In der kirchlichen Praxis spielt das Handeln des Heiligen Geistes eine zentrale Rolle bei der Konfirmation, die seit der Ziegenhainer Zuchtordnung von 1539, beeinflusst von Martin Bucer, von Hessen aus ihren Siegeszug durch die evangelischen Landeskirchen angetreten hat.

Aus dem reformatorischen Christentum hervorgegangene Bewegungen, die die Erfahrung mit dem Heiligen Geist und die Lehre über ihn stark betonen, sind ab 1906 die Pfingstbewegung (s. u.) und seit den 1960er Jahren die Charismatischen Erneuerungsbewegungen.

Im 20. Jahrhundert kam es im evangelischen Bereich zur Bildung neuer Gemeinschaften, die sich meistens verselbständigten; diese Vorgänge werden zusammenfassend als Pfingstbewegung bezeichnet, besonders in den USA. Mittlerweile verzeichnen diese in Afrika, Asien und Südamerika den größten Zulauf an Mitgliedern.
Das Wirken des "Heiligen Geistes" wird in ihnen antihierarchisch verstanden. Daher sind die einzelnen Gemeinden am Ort unabhängig und selbständig. Betont werden:

Einige Konfessionen sprechen statt von Salbung oder Firmung von der Versiegelung mit dem Heiligen Geist (Katholisch-apostolische Gemeinden, Neuapostolische Kirche).

Christliche Glaubensgemeinschaften, die nicht an das Dogma der Dreifaltigkeit glauben (Nichttrinitarier), sehen den Heiligen Geist nicht als Person, sondern als Gottes wirksame Kraft.

Nach sunnitischer Auffassung des Islam wird der Erzengel Djibril auch als bezeichnet. Der Trinitätsbegriff des Christentums wird von islamischer Seite vielfach als eine Gott-Dreiheit, bestehend aus dem Schöpfer, Jesus und Maria reinterpretiert. Daher stelle sie eine "Schirk" (Beigesellung eines anderen Wesens neben Gott) dar, was einem Vielgötterglauben entspreche.

Vergleiche hierzu die islamische Sichtweise der Dreifaltigkeit und Gabriel (Erzengel).

Zum Begriff des Heiligen Geistes bestehen Parallelen in anderen Religionen. Das Konzept „Hagion pneuma“ findet sich in sehr ähnlicher Form bereits in vorchristlicher Zeit in griechischen und altindischen religiösen und philosophischen Abhandlungen.

Das schamanische Pantheon kennt bei manchen Völkern ebenfalls höchste Geistwesen, denen alle anderen geistigen Entitäten untergeordnet sind. Dies ist aber bei den verschiedenen nord- und zentralasiatischen Völkern nicht durchgängig der Fall.

Der einflussreichste Text über den Heiligen Geist überhaupt ist die 79 Kapitel umfassende Spätschrift des Basilius von Caesarea "Peri tou hagiou pneumatos" („über den Heiligen Geist“). Weitere wichtige Texte der Alten Kirche sind die fünfte theologische Rede "Über den Heiligen Geist" des hl. Gregor von Nazianz und "De Trinitate" des Kirchenvaters Augustinus.

Es gibt zahlreiche Hymnen, in denen der Heilige Geist direkt angeredet wird, beispielsweise "Nunc sancte nobis spiritus" des hl. Ambrosius, die Pfingstsequenz "Veni Sancte Spiritus" "(Komm herab, o Heilger Geist)", "Komm, Schöpfergeist" oder das "Veni Creator Spiritus" von Rabanus Maurus, "Komm, Heiliger Geist, Herre Gott" nach der Pfingstantiphon "Veni Sancte Spiritus, imple tuorum corda fidelium", fortgesetzt von Martin Luther, "Zieh ein zu deinen Toren" von Paul Gerhardt, "Weihe an den Heiligen Geist" (nach Pius X.), oder "Heiliger Geist, der Sieg ist dein" (von Pius XI.).

Thomas C. Oden führt die folgenden neutestamentlichen Symbole und Metaphern für den Heiligen Geist auf, die in den Schriften der Kirchenväter und -lehrer aufgegriffen wurden:

Viele Ordensgemeinschaften unterstellten sich dem Patrozinium des Heiligen Geistes, etwa die Spiritaner, die Dienerinnen des Heiligen Geistes oder der Orden vom Heiligen Geist. Insbesondere auf die Brüder vom Orden des Heiligen Geistes, die sich vor allem der Krankenpflege widmeten, gehen Gründungen von Heilig-Geist-Kirche, Heilig-Geist-Klöstern und -Hospitälern zurück, die dem Heiligen Geist geweiht wurden. (Zu den so benannten Einrichtungen siehe Heilig-Geist.)




Bibliographie

Aktuelle einführende Darstellungen

Katechetische Darstellungen

Ältere einführende Darstellungen


</doc>
<doc id="14396" url="https://de.wikipedia.org/wiki?curid=14396" title="Besetzungsinversion">
Besetzungsinversion

Besetzungsinversion ( ‚Umkehr‘) ist ein Begriff aus der Physik von Systemen (beispielsweise Atomen), die nur bestimmte Zustände mit diskreten Energien annehmen können, wie sie durch die Quantenmechanik beschrieben werden. Besetzungsinversion liegt vor, wenn sich mehr Teilchen in einem energetisch höheren Zustand "E" befinden als im energetisch niedrigeren Zustand "E". Sie kann "nicht" im thermischen Gleichgewicht auftreten.

Im thermischen Gleichgewicht nach der Boltzmann-Verteilung gilt, wenn eine einheitliche Temperatur formula_1 vorausgesetzt wird:

mit
Da die Energielücke zwischen zwei Niveaus stets größer 0 ist:

kann die Exponentialfunktion niemals größer 1 werden:

Somit befinden sich im natürlichen Gleichgewicht "weniger" Teilchen in einem energetisch höheren Zustand als im energetisch niedrigeren Zustand:

Daraus folgt, dass eine Besetzungsinversion

nur vorliegen kann, wenn sich das System "nicht" im thermischen Gleichgewicht befindet.

Jedes System strebt danach, seine Entropie zu maximieren, also seine freie Energie zu minimieren. Die Besetzungsinversion stellt eine Abweichung vom lokalen thermodynamischen Gleichgewicht dar und ist somit nicht stabil. Sie kann daher nur unter steter Energiezufuhr, dem sogenannten "Pumpen", in Nichtgleichgewichtssystemen künstlich herbeigeführt und aufrechterhalten werden. Das Pumpen muss selektiv erfolgen, d. h. es darf nur bestimmten Teilchen Energie zugeführt werden. Damit kann erreicht werden, dass ausgewählte Niveaus stärker besetzt werden als dies im natürlichen Gleichgewicht der Fall wäre.

Wird die Anregungsquelle (z. B. optisches Pumpen, Gasentladung) abgeschaltet, dann wird die thermische Überbesetzung des invertierten elektronischen Zustands durch Emission und Stöße mit anderen Atomen oder Molekülen abgebaut. Das lokale thermische Gleichgewicht wird erreicht, wenn angeregte elektronische Zustände, Ionisationsgrad und die Bewegungsenergie der Atome/Moleküle wieder entsprechend der Boltzmann-Statistik verteilt sind. Je nach Lebensdauer der Zustände und der Teilchendichte im System kann der Vorgang einige Zeit (in der Größenordnung von Millisekunden) in Anspruch nehmen.

Eine häufige Art des Pumpens ist das optische Pumpen, wobei Blitzlampen oder die Strahlung anderer Laser genutzt werden. Die Strahlung der Pumpquelle muss dabei energiereicher sein als das Licht, das später vom damit gepumpten Laser emittiert wird. Besetzungsinversion wird erreicht, wenn die Energiedifferenz zwischen dem Grund- und einem höher angeregten elektronischen Zustand des Teilchens sowie die Photonenenergie der Pumpquelle übereinstimmen. Die Energie eines Photons ist proportional zu seiner Frequenz formula_12 und der Planck-Konstante formula_13:

Eine andere Form der selektiven Anregung ist der Stoß mit einem anderen angeregten Teilchen (B), das durch Abregung die Energiedifferenz austauschen kann, um stattdessen das erste Teilchen (A) in den höher angeregten Zustand zu bringen. Um die Teilchen der Sorte B nach der Stoßabregung wieder in den angeregten Zustand zu bringen, wird ihnen Energie, z. B. durch Elektronenstöße, zugeführt (siehe He-Ne-Laser). Die Energie kann in Form einer elektrischen Entladung (z. B. Glimmentladung, Hohlkathode, Mikrowellen) in das Medium eingebracht werden.

Ein Laser stellt eine Anordnung dar, um einen Lichtstrahl zu erzeugen, dessen Photonen sich durch gleiche Frequenz, Phase (zusammen: Kohärenz) und Polarisation auszeichnen. Die nutzbare Strahlung wird aus dem Strahlungsfeld des Resonators ausgekoppelt, z. B. durch teildurchlässige Spiegel.

Eine notwendige, aber nicht alleine ausreichende Voraussetzung für den Betrieb eines Lasers ist die Verstärkung eines Strahls durch stimulierte Emission. Dazu muss im einfachsten Fall (3-Niveau-Laser) Besetzungsinversion zwischen dem Grundzustand formula_15 und dem Laserniveau formula_16 herrschen. Das nebenstehende Bild zeigt einen 4-Niveau-Laser, der prinzipiell genauso funktioniert, aber ein zusätzliches Niveau oberhalb formula_15 (nämlich formula_18) besitzt, das sich wiederum schnell in den Grundzustand formula_15 entleert. Im 4-Niveau-Laser ist daher Besetzungsinversion einfacher herzustellen, da formula_18 praktisch leer ist.

Die Besetzungsinversion kann stationär nur erreicht werden, wenn sowohl der Zustand formula_21 schnell relaxiert (sich entleert, geschieht im µs-Bereich), als auch, falls vorhanden, formula_18 eine kurze Lebensdauer besitzt, bzw. die Anregung aus formula_15 schnell genug erfolgt. Das laseraktive Niveau formula_16 muss dagegen eine große Lebensdauer (ms) besitzen, da es ansonsten durch spontane Emission schnell selbst entvölkert wird und sich ein thermisches Gleichgewicht nach der Boltzmann-Verteilung einstellt.

Die detaillierte Aufstellung der Gleichgewichte einzelner Strahlungsprozesse ist wie folgt:

formula_26 Einsteinkoeffizient für spontane Emission

formula_27 Einsteinkoeffizient für Absorption

formula_28 Einsteinkoeffizient für stimulierte Emission

formula_29 Energiedichte des Strahlungsfeldes

Die Einsteinkoeffizienten stellen Übergangswahrscheinlichkeiten zwischen Niveaus dar. Der Koeffizient für stimulierte Emission steht mit dem für Absorption in Zusammenhang: formula_30. 

Das detaillierte Gleichgewicht gilt im Nichtgleichgewichtszustand nur mikroskopisch; die Strahlungsdichte nimmt über die Weglänge innerhalb des Resonators exponentiell zu. In einem Laser wird Strahlung der Laserwellenlänge optisch verstärkt, während andere Wellenlängen aus mehreren Gründen unterdrückt werden. Dazu gehört einerseits die Verstärkungscharakteristik des aktiven Lasermediums (nur Verstärkung gewisser Wellenlängenbereiche), als auch die Laserbedingung (Ausbildung scharfer Wellenlängen aufgrund der Resonatorabmessungen).



</doc>
<doc id="14398" url="https://de.wikipedia.org/wiki?curid=14398" title="Tieftemperaturphysik">
Tieftemperaturphysik

Die Tieftemperaturphysik befasst sich mit Vorgängen in kalter Materie. Der betrachtete Temperaturbereich liegt in der Nähe des absoluten Nullpunkts der Temperaturskala, d. h. nahe an 0 Kelvin (ca. −273,15 Grad Celsius). Es gibt keine genau definierte Temperatur, ab der man von Tieftemperaturphysik spricht. Experimente sind jedoch meist mit der Verwendung von Kryoflüssigkeiten wie flüssigem Stickstoff (Siedepunkt: 77,4 K) oder flüssigem Helium (Siedepunkt: 4,21 K) verknüpft. Unter 1 Kelvin spricht man oft von ultratiefen Temperaturen.

Mit dem Erreichen immer tieferer Temperaturen waren oft grundlegende Entdeckungen verknüpft (z. B. die der Supraleitung). Als Begründer der modernen Tieftemperaturphysik gilt Heike Kamerlingh Onnes, dem 1908 in Leiden erstmals die Verflüssigung von Helium gelang. 2017 gelang es Schweizer Physikern, einen Chip auf weniger als drei Millikelvin über eine Zeit von sieben Stunden abzukühlen.



Es gibt keine Methode, mit der die thermodynamische Größe Temperatur direkt gemessen werden kann. Zur Bestimmung von tiefen Temperaturen werden deshalb vor allem der Dampfdruck von verflüssigten Gasen (He, H, N etc.) herangezogen. Dies geschieht bei der derzeit gültigen Temperaturskala ITS-90, die den Bereich 0,65 bis 1350 K abdeckt. Für noch tiefere Temperaturen gibt es die "provisorische" PLTS-2000-Skala, welche bis zu 0,9 mK hinab, der Néel-Temperatur in festem He, reicht.
Zur Messung der Temperatur können, je nach Messbereich und experimentellen Möglichkeiten, unterschiedliche Primärthermometer oder Sekundärthermometer eingesetzt werden.




</doc>
<doc id="14399" url="https://de.wikipedia.org/wiki?curid=14399" title="Kleintrombe">
Kleintrombe

Eine Kleintrombe ist ein kleinräumiger Luftwirbel mit vertikaler Achse und meist geringer Höhenerstreckung, der auf die atmosphärische Grenzschicht beschränkt ist. Im Unterschied zu Großtromben (Tornados) besteht kein direkter Zusammenhang mit konvektiver Bewölkung.

Je nach ihrem Erscheinungsbild und dem aufgewirbelten Material sind verschiedene Bezeichnungen gebräuchlich: "Heuteufel, Al Hul" (bei Beduinen), "Nebelteufel, Staubteufel, Sandteufel, Staubtrombe, Sandtrombe, Staubhose und Sandhose". Die letzten beiden Begriffe sind aber irreführend, da sie mit Wind- und Wasserhosen verwechselt werden können, bei denen es sich um Großtromben handelt. Selten treten auch Schneeteufel auf.
Gustnados (Böenfrontwirbel) hingegen dürfen mit Staubteufeln nicht verwechselt werden.
Bedingung für die Entstehung von Kleintromben ist eine bodennahe Überhitzung der Atmosphäre. Bei dieser trockenlabilen Schichtung lösen sich Thermikblasen vom Boden ab, welche beim raschen Aufsteigen eine vorhandene schwache Rotation der Luft durch Streckung des Wirbels konzentrieren können. Aufgrund der Drehimpulserhaltung nimmt dabei die Windgeschwindigkeit durch den Pirouetteneffekt rasch zu und kann in Extremfällen bis Orkanstärke erreichen. 

Ein Beispiel eines heftigen Staubteufels liegt aus den USA vor. Am 14. September 2000 fegte über die Coconino County Fairgrounds in Arizona eine Sandtrombe mit geschätzten Windspitzen von 120 km/h. Dabei wurden mehrere Personen leicht verletzt, an den in der Zugbahn des Wirbels befindlichen Gebäuden traten leichte Schäden wie abgedeckte Planen, losgerissene Dachziegel sowie verbogene Metallgestänge auf.

Im Zuge von Sandtromben wurden Windspitzen bis knapp über 150 km/h gemessen, maximal 200 km/h erscheinen nach dem heutigen Stand der Wissenschaft möglich. In Summe sind die meisten Kleintromben aber recht schwach und richten nur selten Schäden an. In einem durchschnittlichen Staubteufel werden nur Windspitzen um 50 km/h erreicht.

Die Ausdehnung einer Sandtrombe ist sehr unterschiedlich – von 0,5 m Durchmesser bis hin zu rund 200 Metern Breite und mehreren hundert Metern Höhe sind sämtliche Größenordnungen möglich. Gelegentlich treten Staubteufel auch in Verbänden oder Gruppen auf, etwa an einer Böenlinie.

Die Lebensdauer einer Kleintrombe liegt zwischen wenigen Sekunden bis rund einer halben Stunde. Sie können nahezu stationär verharren oder sich im Schritttempo, oder in Extremfällen mit bis zu 100 km/h, vorwärts bewegen.

Die Drehrichtung von Staubteufeln wird aufgrund der geringen räumlichen Ausdehnung des Windes nicht von der Corioliskraft bestimmt, Windrichtung und Orografie spielen hier eine Rolle. Kleintromben treten dort am häufigsten auf, wo durch starke Sonneneinstrahlung trockenlabile Bedingungen erreicht werden, so vor allem in Wüstengebieten. In den mittleren Breiten sind sie am ehesten in der warmen Jahreszeit über offenen Landflächen (unbewachsene Äcker, abgemähte Wiesen, Sportplätze, Ödland, Hangflächen oberhalb der Baumgrenze) anzutreffen. Die Sonderform des Gustnados wird dynamisch an Böenfronten vor Schauern oder Gewittern ausgelöst. Eine seltene Variante bei niedrigen Lufttemperaturen ist über (relativ) warmen Wasseroberflächen als "Nebelteufel" zu beobachten.

Staubteufel kommen nicht nur auf der Erde vor, sondern wurden auch auf dem Mars beobachtet.



</doc>
<doc id="14401" url="https://de.wikipedia.org/wiki?curid=14401" title="Imaginäre Zahl">
Imaginäre Zahl

Eine (rein) imaginäre Zahl (auch "Imaginärzahl," lat. "numerus imaginarius") ist eine komplexe Zahl, deren Quadrat eine nichtpositive reelle Zahl ist. Äquivalent dazu kann man die imaginären Zahlen als diejenigen komplexen Zahlen definieren, deren Realteil null ist. Die Bezeichnung „imaginär“ wurde zuerst 1637 von René Descartes benutzt, allerdings für nichtreelle Lösungen von algebraischen Gleichungen. Die imaginäre Einheit formula_1 erlaubt die Erweiterung des Körpers der reellen Zahlen zum Körper der komplexen Zahlen.

In der Elektrotechnik wird als Symbol statt formula_1 ein formula_3 benutzt, diese Bezeichnung geht auf Charles P. Steinmetz zurück. Die Bezeichnung formula_3 ist gemäß DIN 1302, DIN 5483-3 und ISO 80000-2 als Symbol erlaubt, um in Anwendungen wie der komplexen Wechselstromrechnung eine Verwechslung mit dem Momentanwert formula_5 der Stromstärke zu vermeiden.

Mit imaginären Zahlen lassen sich Gleichungen lösen, deren Lösungen keine reellen Zahlen sein können. Die Gleichung

hat zur Lösung zwei reelle Zahlen, −1 und +1.

Die Gleichung

hingegen kann keine reelle Lösung haben, da Quadrate reeller Zahlen niemals kleiner als 0 sind, also nie den Wert formula_8 annehmen können. Die Lösungen der Gleichung in den komplexen Zahlen sind formula_9 und formula_10, zwei imaginäre Zahlen.

Eine Beschäftigung mit Quadratwurzeln aus negativen Zahlen wurde bei der Lösung von kubischen Gleichungen im Fall des Casus irreducibilis nötig.

Heute versteht man imaginäre Zahlen als spezielle komplexe Zahlen. Jede komplexe Zahl kann dargestellt werden als Summe einer reellen Zahl und eines reellen Vielfachen der imaginären Einheit formula_11, einer Zahl mit der Eigenschaft

Algebraisch wird formula_11 definiert als eine Nullstelle des Polynoms formula_14 und die komplexen Zahlen als die dadurch erzeugte Körpererweiterung. Die zweite Nullstelle ist dann formula_10. Man kann die beiden Nullstellen aber erst unterscheiden, wenn man eine der beiden mit formula_11 bezeichnet hat. Da man sie aber ohnehin nicht unterscheiden kann, spielt es keine Rolle, „welche“ Nullstelle man nun mit i bezeichnet. (Wird jedoch, wie üblich, der komplexe Zahlenbereich auf der Struktur des formula_17 definiert statt nur mit seiner Hilfe dargestellt, so kann man die möglichen Nullstellen sehr wohl unterscheiden und wählt naheliegenderweise formula_18 statt des ebenso möglichen formula_19.)

Alle komplexen Zahlen lassen sich in der Gaußebene darstellen, einer Erweiterung der reellen Zahlengeraden (siehe Abbildung rechts). Die komplexe Zahl formula_20 hat den Realteil formula_21 und den Imaginärteil formula_22. Aufgrund der Rechenregeln komplexer Zahlen ist das Quadrat einer Zahl, deren Realteil gleich 0 ist, eine nichtpositive reelle Zahl:

Die imaginären Zahlen bilden eine Gerade, die durch die Zahl 0 geht und senkrecht auf der reellen Zahlengeraden steht. Sie sind reelle Vielfache der imaginären Einheit formula_11.

Eine weitergehende Beschreibung findet sich im Artikel über komplexe Zahlen.

Für die Potenzen von formula_11 mit ganzzahligen Exponenten gilt

für alle formula_27, z. B.:

und formula_29 für alle formula_27.

Erweiterungen stellen die hyperkomplexen Zahlen dar, die über die komplexen Zahlen hinausgehend mehrere imaginäre Einheiten aufweisen. Beispielsweise treten bei den vierdimensionalen Quaternionen drei imaginäre Einheiten auf, bei den achtdimensionalen Oktonionen gibt es sieben imaginäre Einheiten.

In der eulerschen Identität wird ein prägnanter, einfacher Zusammenhang der imaginären Einheit formula_1 mit drei anderen grundlegenden mathematischen Konstanten hergestellt, nämlich mit der eulerschen Zahl formula_32, der Kreiszahl formula_33 sowie der reellen Einheit 1:


</doc>
<doc id="14403" url="https://de.wikipedia.org/wiki?curid=14403" title="Nahrung">
Nahrung

Nahrung setzt sich aus verschiedenen Stoffen zusammen und ist Grundlage für Stoffwechsel und damit für das Leben. Bei Heterotrophen (meist Tiere) enthält Nahrung energiereiche organische Verbindungen. Bei Autotrophen (meist Pflanzen) spricht man auch von Nährstoffen - diese enthalten dann keine energiereichen organischen Substanzen. Nahrung für den Menschen wird als Nahrungsmittel bezeichnet. 

Bei Tieren mit Verdauungstrakt wird Nahrung nach der Aufnahme in den Körper meist mechanisch (z. B. durch Kauen) und chemisch (z. B. durch Magensäure) in ihre Bestandteile zerlegt. Die in bestimmten Nahrungsbestandteilen gespeicherte Energie wird im Energiestoffwechsel verwendet, um z. B. bei Warmblütern die Körpertemperatur konstant zu halten. Des Weiteren wird die Energie aus der Nahrung im anabolen Stoffwechsel (Anabolismus bzw. Baustoffwechsel) für Erhalt und Aufbau des Körpers (z. B. Wachstum bei Kindern oder Muskelaufbau bei Erwachsenen) eingesetzt.

Wasser und Salze gehören auch zur Nahrung, auch wenn sie nicht direkt im Energiestoffwechsel genutzt werden können. Zur Nahrung gehören Vitamine, das heißt essentielle Verbindungen, die von einigen Arten nicht selbst synthetisiert werden können. Der Mangel an Nahrung wird als Hunger bezeichnet und kann zum Tod eines Lebewesens führen (Hungertod).

Das Nahrungsangebot beschreibt das Vorhandensein von entsprechender Nahrung, welche der jeweilige Organismus benötigt. Nahrung ist oft ein Limitierender Faktor und hat bei einem Überangebot für Lebewesen mit hoher Reproduktionsrate meist eine explosionsartige Vermehrung zur Folge. 

Bestes Beispiel ist Bierhefe, welche für das Bierbrauen verwendet wird. Die Bierwürze stellt das Nährmedium dar, in dem sich die Pilze exponentiell vermehren. Solange bis sie an ihren eigenen Stoffwechselprodukten, dem Ethanol, zugrunde gehen und schlagartig absterben. Das ist bei Bakterien­kulturen zu beobachten.

Im Tierreich, wie z. B. bei Säugetieren, wirkt das Angebot regulierend auf die Bestandsentwicklung einzelner Tierarten. Das bedeutet, dass bei zunehmenden Nahrungsangebot eines Fleischfressers, dessen Bestand nach und nach wächst. Dadurch wächst der Druck auf die Population der Beute, deren Bestand wieder schrumpft und somit zeitversetzt, auch die des Jägers. Um die durch die Räuber-Beute-Beziehung bedingten Schwankungen der Populationszahlen nachzuvollziehen, wurden die wichtigsten Zusammenhänge in den Lotka-Volterra-Regeln zusammengefasst.

Da der Mensch an oberster Stelle der Nahrungskette steht, greift er am massivsten in die Gleichgewichte der Natur ein. Durch die synthetische Herstellung von Dünger und Pflanzenschutzmitteln (Haber-Bosch-Verfahren) wird eine Weltbevölkerung von ca. 7 Milliarden Menschen erst ermöglicht. Einen weiteren wichtigen Faktor der Nahrungssicherung bietet die Konservierung. Da der Mensch am Ende der Nahrungskette steht, ist er durch Schadstoffbelastungen am meisten gefährdet. Deshalb wird oft davor gewarnt, Seefische zu verzehren, da sie die Schadstoffe des Meeres in ihrem Körper besonders anreichern.

Mikroorganismen sorgen für eine Schließung der Nahrungskette, indem sie abgestorbene Pflanzen und Tiere zersetzen und somit wieder Nahrung für Pflanzen bereitstellen. Sie können aber auch das Nahrungsangebot anderer Lebewesen erweitern, wie z. B. Joghurt oder Käsekulturen. Oder sie schaden anderen Organismen, wie z. B. Krankheitserreger, welche sich von Gewebe und Blutzellen ernähren. Sie sorgen damit für eine natürliche Auslese.

Mit dem Begriff „geistige Nahrung“ werden oft Herausforderungen für das Denken umschrieben, beispielsweise Denksport­aufgaben oder „Lesestoff“, als „Nahrung für die Seele“ hingegen die Weitergabe von Emotionen und die Psyche schmeichelnde menschliche Reaktionen wie Anerkennung, Akzeptanz, Aufmerksamkeit, Bestätigung, Fürsorge, Lob, Komplimente, Respekt, Verständnis und Wertschätzung.




</doc>
<doc id="14406" url="https://de.wikipedia.org/wiki?curid=14406" title="After">
After

After steht für:

Siehe auch:


</doc>
<doc id="14407" url="https://de.wikipedia.org/wiki?curid=14407" title="Anus">
Anus

Der Anus (, ), deutsch After (mhd. "after", ahd. "aftero", eigentlich „Hinterer“; substantiviert von „hinter, nachfolgend“), ist die Austrittsöffnung des Darmkanals vielzelliger Tiere. Durch den After verlässt der Kot den Darm.

Der Anus entstand evolutiv gemeinsam mit dem Darm als dessen Austrittsöffnung und trat erstmals bei den Bilateria auf, die aus drei Keimblättern bestehen. Im Gegensatz dazu besitzen phylogenetisch ursprünglichere Organismen wie etwa die Nesseltiere (Cnidaria) und die Rippenquallen (Ctenophora) zwar einen Gastralraum mit einer Mundöffnung, jedoch keinen von dieser getrennten After. Durch den Anus wird dabei ein gerichteter Nahrungstransport ermöglicht, bei dem unverdauliche Nahrungsreste getrennt von der Mundöffnung ausgeschieden werden können. Innerhalb der Bilateria besitzen nur sehr wenige Gruppen keinen After, etwa die Plattwürmer (Plathelminthes) und die Kiefermündchen (Gnathostomulida).

Gebildet wird der Anus anders als der Darm nicht vom Entoderm, das mit der Einstülpung eines Urmundes (Blastoporus) in die Blastula eine so genannte Gastrula bildet, sondern durch das Ektoderm. Die bei der Gastrulation entstehende Höhle (Blastocoel) entwickelt sich zum Darm, der am anderen Körperende eine weitere Öffnung bildet, die ektodermal ausgekleidet wird. Auf diese Weise entsteht ein Darmkanal mit einer Eintritts- und einer Austrittsöffnung, die zum späteren Mund und After werden. Dabei wird bei den Urmündern (Protostomia) der Urmund zum endgültigen Mund und der Durchbruch zum After. Die konkrete Bildung des Darms kann dabei sehr unterschiedlich verlaufen, bei den Ringelwürmern (Annelida) entsteht der Blastoporus als schlitzartige Einstülpung, die bauchseitig geschlossen wird und aus dem damit ohne neuen Durchbruch die Mund- und die Afteröffnung hervorgehen.

Bei den Neumündern (Deuterostomia), zu denen auch die Wirbeltiere und damit auch der Mensch gehört, wird dagegen der Urmund zum späteren After und der sekundäre Durchbruch an der Vorderseite des Embryo zur Mundöffnung.

Der Anus stellt in seiner einfachsten Form eine einfache Öffnung des Darmes nach außen dar, durch den der unverdaubare Nahrungsrest diesen verlassen kann. In der Regel ist er von Muskulatur umgeben, die die Darmperistaltik fortsetzt und so den Kot hinausbefördert. Bei Wirbeltieren befinden sich am Darmausgang zudem Schließmuskel, durch die die Darmentleerung kontrolliert stattfinden kann.

Bei zahlreichen wirbellosen Tieren und deren Larven erfüllt der Anus neben der Darmentleerung weitere Aufgaben. So befindet sich etwa im After der Großlibellenlarven ein stark durchblutetes Gewebe, dass der Darmatmung dient. Bei Insekten ist der Analbereich von einem chitingen Endring, dem Periprokt, sowie den Analklappen bestehend aus einem unpaaren Epiprokt und den paarigen Paraprokten umgeben.

In der Analgegend vieler Säugetierarten sind Analdrüsen ausgebildet. Bei den Amphibien, den zu den Reptilien zusammengefassten und den Vögeln existiert mit der Kloake ein gemeinsamer Körperausgang für die Verdauungs-, Geschlechts- und Exkretionsorgane. Es handelt sich um einen Abschnitt des Enddarms, in den die Ausführgänge der Geschlechtsorgane (Gonodukte) und die Harnleiter münden, deren Produkte (Spermien und Eizellen) und Exkrete wie die Exkremente über den Anus abgegeben werden.

Der Analkanal "(Canalis analis)" des Menschen kann in drei Abschnitte untergliedert werden, die durch einen allmählichen Übergang von der Schleimhaut des Darmes zur äußeren Haut gekennzeichnet sind:
Die "Linea dentata" markiert die Grenze zwischen dem Plattenepithel des Analkanals und dem Epithel des Rektums.

Um die Öffnung des Anus sind unter der Haut bzw. Schleimhaut zwei Schließmuskeln angeordnet, die gemeinsam mit weiteren Strukturen des Enddarms das Kontinenzorgan bilden:

Die Peristaltik des Analkanals wird über parasympathische Nervenfasern aus dem Kreuzabschnitt des Rückenmarks "(Nervi pelvini)" angeregt. Diese bewirken auch eine Erschlaffung des inneren Afterschließmuskels. Im Zusammenspiel mit den Bauchmuskeln (Bauchpresse) führt dies über den Defäkationsreflex zu einer Entleerung des Mastdarms (Kotabsatz, Defäkation). Dabei schiebt sich die Kotsäule aus dem Darm. Wenn die Bauchmuskulatur zur Ausscheidung nicht verwendet wird, dauert die Defäkation länger.

Die sympathischen Nervenfasern des Nervus hypogastricus reduzieren die Peristaltik und erhöhen den Tonus des inneren Afterschließmuskels. Dadurch wird die Stuhlverhaltung "(Continentia alvi)" ermöglicht. Durch willkürliche Beeinflussung des äußeren Afterschließmuskels
kann der Kotabsatz unterdrückt werden. Er wird durch den Nervus pudendus (bzw. dessen "Nervus rectalis caudalis") innerviert.

Die sensible Innervation des Afters erfolgt über die Nervi anococcygei und den Nervus perinealis superficialis („oberflächlicher Dammnerv“) des Nervus pudendus. Da am After eine Vielzahl von Nervenendigungen liegen, ist er sehr empfindsam und wird auch als erogene Zone betrachtet, insbesondere der "Musculus sphincter ani externus" und die sich davon absetzende Dammmuskulatur ("siehe auch" Analreflex, Analverkehr).

Der Anus und Enddarm werden vom Proktologen untersucht und behandelt:

Dabei kann der Anus verschiedene Fehlbildungen und Erkrankungen aufweisen, etwa

Auch Fremdkörper in Anus und Rektum können auftreten.

Ein operativ künstlich angelegter Anus wird als "Anus praeter" (verkürzend für "Anus praeternaturalis", lat. für „Darmausgang vor dem natürlichen Ende“) bezeichnet.



</doc>
<doc id="14410" url="https://de.wikipedia.org/wiki?curid=14410" title="Matching (Graphentheorie)">
Matching (Graphentheorie)

Die Theorie um das Finden von Matchings in Graphen ist in der diskreten Mathematik ein umfangreiches Teilgebiet, das in die Graphentheorie eingeordnet wird.

Folgende Situation wird dabei betrachtet: Gegeben eine Menge von Dingen und zu diesen Dingen Informationen darüber, welche davon einander zugeordnet werden könnten. Ein Matching (in der Literatur manchmal auch Paarung) ist dann als eine solche Auswahl aus den möglichen Zuordnungen definiert, die kein Ding mehr als einmal zuordnet.

Die am häufigsten gestellten Fragen in dieser Situation sind dann die folgenden:

Die Theorie um die Matchings untersucht möglichst effiziente Lösungsverfahren dieser Probleme, klassifiziert diese nach ihrer „Schwierigkeit“ mit den Methoden der Komplexitätstheorie und stellt Beziehungen dieser Probleme zueinander und zu anderen Problemen in der Mathematik her.

Das oben beschriebene Problem lässt sich wie folgt formalisieren. Gegeben sei ein endlicher, ungerichteter Graph formula_3. Eine Menge formula_4 heißt (gültiges) Matching wenn keine zwei Kanten aus formula_5 einen Knoten gemeinsam haben. Ein Matching heißt

Für das gewichtete Matchingproblem spielt eine Kostenfunktion formula_12 eine Rolle. Ein gültiges Matching heißt dann …

Als eine der frühesten systematischen Untersuchungen von Matchings wird ein Artikel von Julius Petersen angeführt, der 1891 über „Die Theorie der regulären graphs“ schrieb. Er untersuchte ein Zerlegungsproblem aus der Algebra, das David Hilbert 1889 gestellt hatte, indem er es als Graphenproblem formulierte. Letztlich bewies er darin folgendes:
Die Tatsache (2), bekannt als Satz von Petersen, lässt sich auch als eine leichte Verallgemeinerung des Eulerkreisproblems formulieren.

Rückblickend erscheinen Petersens Argumente, mit denen er das Obige bewies, kompliziert und umständlich. Bei der weiteren Untersuchung etwa durch Brahana 1917, Errera 1922 und Frink 1926 sowie zusammenfassend durch Kőnig 1936 wurden aber viele Methoden der modernen Graphentheorie entwickelt oder zuerst systematisch formuliert. Petersens Denkansatz wurde dann von Bäbler 1938 1952 und 1954 sowie von Gallai 1950, Belck 1950 und schließlich Tutte auf andere reguläre Graphen übertragen.

In modernen Lehrbüchern und Vorlesungen tauchen Petersens ursprüngliche Resultate, wenn überhaupt, meist nur noch als Folgerungen aus den Resultaten von Tutte oder Hall auf. Im Buch von Diestel folgt die erste Aussage aus dem Heiratssatz von Hall. Die zweite Aussage wird auf den Satz von Tutte zurückgeführt.

Eines dieser frühen Resultate betrifft bipartite Graphen, die sich in der Folge als ein sehr natürlicher und aus heutiger Sicht für die Praxis zentraler Spezialfall herausgestellt haben. Kőnig und Egerváry untersuchten beide unabhängig voneinander das bipartite Matchingproblem und das Knotenüberdeckungsproblem und fanden dabei heraus, dass beide Probleme in dem folgenden Sinn äquivalent sind:
Dieser Satz wird meistens Kőnig zugeschrieben oder "Min-Max-Theorem" bzw. "Dualitätssatz" genannt. Beide bewiesen die Aussage für endliche Graphen. Aharoni bewies 1984 die Aussage für überabzählbar unendliche Graphen. Ein elementarer Beweis von (3) findet sich in Lovász & Plummer "43," der von den meisten Lehrbüchern übernommen wurde. Bondy & Murty "200" führt den Satz auf ein Resultat der linearen Programmierung zurück: Ist formula_19 die Inzidenzmatrix des Graphen formula_1, dann lassen sich maximum Matchings als Lösungen von folgendem ganzzahligen linearen Programm auffassen:
Dabei ist formula_22 der Einsvektor bestehend aus lauter Einsen. Das Programm des Knotenüberdeckungsproblems hat folgende Gestalt:
Diese Programme haben eine sogenannte "primal-dual"-Gestalt. Für Programme von dieser Gestalt wird in der Theorie der linearen Programme gezeigt, dass sie in ihren Optima übereinstimmen. Für bipartite Graphen lässt sich außerdem leicht zeigen, dass formula_19 total unimodular ist, was in der Theorie der ganzzahligen linearen Programme ein Kriterium für die Existenz einer optimalen Lösung der Programme mit Einträgen nur aus formula_25 (und damit in diesem speziellen Fall sogar aus formula_26) ist, also genau solchen Vektoren, die auch für ein Matching bzw. für eine Knotenüberdeckung stehen können. Dieser Primal-Dual-Ansatz der linearen Programme scheint zunächst wenig mit der Matching-Theorie zu tun zu haben, stellt sich aber als einer der fruchtbarsten Ansätze zur effizienten Berechnung von Matchings, insbesondere im gewichteten Fall, heraus.

Es gibt eine ganze Vielzahl von Sätzen, die zum Satz von Kőnig äquivalent sind. Darunter der Satz von Birkhoff und von Neumann, der Satz von Dilworth und das Max-Flow-Min-Cut-Theorem für bipartite Graphen. Für die Matchingtheorie am interessantesten ist folgende Bedingung, die Hall 1935 angab, um bipartite Graphen mit perfektem Matching zu charakterisieren. Dieser Charakterisierungssatz ist ebenfalls äquivalent zum Satz von Kőnig.
Aus (4) folgt schnell, dass sich unter den bipartiten Graphen genau alle regulären Graphen formula_33-faktorisieren lassen und die Aussage (1) von Petersen lässt elegant auf diese Folgerung zurückführen. Eine Verallgemeinerung dieses Resultats liefert eine Formel für die Größe eines maximum Matchings, die sogenannte "Kőnig-Ore" Formel:
Viele der folgenden Konzepte spielen in fast allen Lösungsverfahren von Matchingproblemen eine Rolle. Ist ein Graph formula_1 mit einem Matching formula_5 gegeben, dann heißt ein Knoten von formula_1 "frei" (in der Literatur auch "ungepaart, exponiert, verfügbar" …) falls er zu keiner Kante in formula_5 inzident ist. Andernfalls heißt der Knoten "gesättigt." Ein Pfad formula_38 in formula_1 heißt "alternierend," falls dieser abwechselnd Kanten aus formula_5 und aus formula_41 enthält. Falls dieser Pfad in einem freien Knoten beginnt und endet, heißt der Pfad "verbessernd" oder auch "augmentierend." Die letzte Bezeichnung kommt von der Tatsache, dass formula_38 durch formula_43 ein größeres Matching als formula_5 liefert. Folgendes grundlegendes Resultat von Berge 1957 motiviert das Studium von augmentierenden Pfaden.
Diese Bezeichnungen entsprechen genau der Sprache, die auch bei der Behandlung von Flüssen in Netzwerken gebraucht wird. Das ist kein Zufall, denn Matchingprobleme lassen sich in der Sprache der Netzwerktheorie formulieren und mit den dort entwickelten Verfahren lösen. Im bipartiten Fall ist diese Zurückführung, wie das folgende Beispiel zeigt, sogar fast trivial.
Gegeben ein Graph formula_1 mit Knotenmenge formula_48. Konstruiere ein Netzwerk formula_49. Dabei ist formula_50 und formula_51. Außerdem ist formula_52 die Fortsetzung von der Kostenfunktion formula_53, die alle neuen Kanten mit codice_1 belegt.

Mit dem Satz von Berge lässt sich auch gleich ein Algorithmus (I) zum Finden von maximum Matchings angeben. Weil jeder verbessernde Pfad zu einem gegebenen Matching einen weiteren Knoten matcht und maximal formula_54 Knoten zu matchen sind, beschränkt sich die Zahl der Schleifendurchläufe asymptotisch durch formula_55. Eine sehr naive Methode zum Finden verbessernder Pfade stellen sogenannte Graph Scans dar, etwa eine Breitensuche (BFS) mit einer Laufzeit von formula_56. Ferner ist formula_57, weil der Graph bipartit ist und damit ist die angegebene Methode in formula_58.

Einer der frühesten Beiträge zum Berechnen von Maximum-Matchings, der über die oben angeführte naive Methode hinausgeht, war der Algorithmus von Hopcroft und Karp 1973. Die Grundidee folgt dem Algorithmus von Dinic (mit dem das Problem mit derselben asymptotischen Laufzeit gelöst werden kann ), der in jeder Phase, wo der Algorithmus nach einem verbessernden Pfad sucht (Zeile 2), möglichst kurze Pfade und nach Möglichkeit „mehrere gleichzeitig“ sucht.

Alt, Blum, Mehlhorn & Paul 1991 schlagen eine Verbesserung von Hopcroft & Karp vor, indem sie ein Scanningverfahren für Adjazenzmatrizen nach Cheriyan, Hagerup, and Mehlhorn 1990 anwenden. Eine einfache Beschreibung der Methode findet sich auch in Burkard, Dell’Amico & Martello "47 ff." Feder und Motwani 1991 haben eine Methode vorgeschlagen, die auf der Zerlegung von formula_1 in bipartite Cliquen beruht und erreichen damit eine asymptotische Laufzeit von formula_60. Eine Methode, die "nicht" auf der Idee augmentierender Pfade beruht, sondern sogenannte „starke Spannbäume“ benutzt, haben Balinski & Gonzalez 1991 vorgeschlagen und erreichen damit eine Laufzeit von formula_58.

Während Charakterisierungen von Matchings und effiziente Algorithmen zum Bestimmen relativ schnell nach der Formulierung von Matchings als Problem gefunden wurden, dauerte es bis 1947 bis Tutte eine Charakterisierung für perfekte Matchings in allgemeinen Graphen formulieren und beweisen konnte. Aus diesem tiefliegenden Resultat lassen sich alle bisher besprochenen vergleichsweise leicht herleiten. Tutte benutzt die einfache Tatsache, dass eine Komponente mit ungerader Knotenzahl in einem Graphen kein perfektes Matching haben kann. Wenn also eine Knotenmenge formula_62 so gefunden werden kann, dass formula_63 mehr ungerade Komponenten als formula_62 Knoten hat, dann müsste für ein perfektes Matching aus jeder solcher Komponente wenigstens ein Knoten mit einem Knoten aus formula_62 gematcht werden und das kann nicht sein. Es stellt sich heraus, dass die Existenz einer solchen Menge formula_62 Graphen ohne perfektes Matching nicht nur beschreibt, sondern charakterisiert:

Eine solche Menge formula_71 heißt "Tutte-Menge" und die Bedingung in (5) heißt "Tutte-Bedingung." Dass sie notwendig für die Existenz perfekter Matching ist, wurde schon skizziert und es gibt mittlerweile viele Beweise dafür, dass die Bedingung hinreichend ist: Tuttes ursprünglicher Beweis formulierte das Problem als ein Matrix-Problem und benutzte die Idee der pfaffschen Determinante. Elementare Abzählargumente wurden relativ rasch danach veröffentlicht, wie in Maunsell 1952, Tutte 1952, Gallai 1963, Halton 1966 oder Balinski 1970. Andere Beweise, wie Gallai 1963, Anderson 1971 oder Marder 1973 verallgemeinern den Satz (4) von Hall systematisch. Ferner gibt es Beweise aus der Perspektive der Graphentheorie, die die Struktur von Graphen betrachten, die selbst kein Perfektes Matching besitzen, doch falls eine Kante ergänzt wird hat der resultierende Graph ein solches. Diesen Ansatz verfolgen etwa Hetyei 1972 oder Lovász 1975.
Der erste Polynomialzeitalgorithmus für das klassische Matchingproblem stammt von Jack Edmonds (1965). Die Grundstruktur der Methode entspricht Algorithmus (I): Sie sucht verbessernde Pfade und gibt ein maximum Matching zurück, falls kein solcher gefunden werden kann. Einen verbessernden Pfad zu finden, stellt sich hier aber als schwieriger heraus als im bipartiten Fall, weil einige neue Fälle auftreten können. Edmonds Suchmethode konstruiert nach und nach einen "alternierenden Wald." Das ist ein kreisfreier Graph formula_72 mit so vielen Zusammenhangskomponenten wie es freie Knoten gibt. Jeder freie Knoten ist Wurzel formula_73 eines Baumes formula_74 und formula_72 ist so konstruiert, dass für alle anderen Knoten formula_76 der eindeutig bestimmte formula_73-formula_78-Pfad ein alternierender Pfad ist. Ein Knoten in formula_76 heißt dann "innen" oder "ungerade," falls formula_80 und andernfalls "außen" oder "gerade." formula_81 sei hier die Distanzfunktion in formula_71, gebe also die Länge des eindeutig bestimmten formula_73-formula_78-Pfades an.

Es genügt die Betrachtung auf die Konstruktion eines alternierenden Baumes zu reduzieren. Falls diese Konstruktion keinen augmentierenden Pfad findet, wird sie mit einem neuen freien Knoten reinitialisiert und alle bereits betrachteten Kanten werden ignoriert. Existiert kein freier Knoten mehr, dann existiert auch kein augmentierender Pfad. Diesen alternierenden Baum konstruiert Edmonds, indem er ausgehend von einem freien Knoten nach und nach alle Kanten hinzufügt oder ignoriert. Dabei können für eine neue Kante formula_85 (formula_52 gehöre bereits zum Baum) folgende Fälle auftreten:


Blüten können, anders als bei Fall formula_116, nicht ignoriert werden. Der Knoten, der die Blüte mit dem Baum verbindet, lässt sich in das Schema der inneren und äußeren Knoten nicht einordnen. Die naheliegende Idee, ihn als „sowohl innen als auch außen“ zu behandeln führt zu einem falschen Algorithmus. Die Behandlung von Blüten mit Kontraktion ist neben dem Ansatz von Berge "die" zentrale Idee von Edmonds’ Algorithmus und Grundlage vieler späterer Verfahren. Bipartite Graphen enthalten keine ungeraden Kreise und damit auch keine Blüten. Edmonds’ Algorithmus reduziert sich daher im bipartiten Fall auf die Methode von Munkres.

Man kann ablesen, dass die skizzierte Methode von Edmonds einen Aufwand von formula_117 hat. In Fall formula_118 reinitialisiert Edmonds die Suche und verwirft damit den bereits geleisteten Suchaufwand. Gabow 1976 und Lawler haben eine naive Implementierung vorgeschlagen, die den Suchaufwand nicht verwirft und eine Laufzeit von formula_58 erreicht. Das Beispiel folgt bereits dieser Methode.



</doc>
<doc id="14411" url="https://de.wikipedia.org/wiki?curid=14411" title="Knotenüberdeckung">
Knotenüberdeckung

Eine Knotenüberdeckung bezeichnet in der Graphentheorie eine Teilmenge der Knotenmenge eines Graphen, die von jeder Kante mindestens einen Endknoten enthält. Das Finden von kleinsten Knotenüberdeckungen gilt als algorithmisch schwierig, denn das damit eng verwandte Knotenüberdeckungsproblem ist NP-vollständig.

Sei formula_1 ein ungerichteter Graph und formula_2 eine Teilmenge von formula_3, wobei formula_3 die Menge der Knoten und formula_5 die Menge der Kanten ist.
Dann ist formula_2 eine "Knotenüberdeckung" (Vertex Cover) von formula_7, wenn jede Kante von formula_7 wenigstens einen Knoten aus formula_2 enthält.
Entsprechend dazu ist eine "Kantenüberdeckung" des Graphen eine Teilmenge formula_10 seiner Kantenmenge, so dass jeder Knoten in mindestens einer Kante aus formula_10 enthalten ist. 

Eine Knotenüberdeckung formula_2 von formula_7 nennt man "minimal", wenn es keinen Knoten formula_14 gibt, so dass formula_2 ohne formula_16 immer noch eine Knotenüberdeckung ist.
Gibt es in formula_7 keine Knotenüberdeckung, die weniger Elemente als formula_2 enthält, so nennt man formula_2 eine "kleinste Knotenüberdeckung". Die Anzahl der Knoten einer kleinsten Knotenüberdeckung von formula_7 nennt man "Knotenüberdeckungszahl" von formula_7.

Gerichtete Graphen oder solche mit Mehrfachkanten sind nicht Gegenstand derartiger Betrachtungen, da es nicht auf die Richtung oder Vielfachheit der Kanten ankommt.


Das Entscheidungsproblem zu einem Graphen formula_7 und einer natürlichen Zahl formula_23 zu entscheiden, ob formula_7 eine Knotenüberdeckung der Größe höchstens formula_23 enthält, wird Knotenüberdeckungsproblem genannt. Das zugehörige Optimierungsproblem fragt nach der Knotenüberdeckungszahl eines Graphen. Das zugehörige Suchproblem fragt nach einer kleinsten Knotenüberdeckung. 

Das Knotenüberdeckungsproblem ist NP-vollständig, das zugehörige Optimierungs- und Suchproblem ist NP-äquivalent. Die NP-Schwere des Knotenüberdeckungsproblems folgt aus dem Satz, dass die Stabilitätszahl eines Graphen immer der Anzahl Knoten eines Graphen abzüglich seiner Knotenüberdeckungszahl entspricht, denn das Komplement einer kleinsten Knotenüberdeckung ist immer eine größte stabile Menge und umgekehrt.

König konnte jedoch schon 1931 zeigen, dass in bipartiten Graphen die Knotenüberdeckungszahl der Paarungszahl entspricht (Satz von König). Für das Problem, eine größte Paarung zu finden, gibt es aber einen polynomiellen Algorithmus. In bipartiten Graphen lässt sich daher auch eine kleinste Knotenüberdeckung und eine größte stabile Menge in polynomieller Zeit berechnen.
Tatsächlich gilt sogar etwas stärker, dass die Knotenüberdeckungszahl in perfekten Graphen in polynomieller Zeit berechnet werden kann.

Es existiert ein Approximationsalgorithmus, der eine Knotenüberdeckung mit relativer Güte 2 berechnet. Es ist kein besserer Algorithmus mit fester Güte bekannt.

Der Algorithmus berechnet eine nicht-erweiterbare Paarung formula_10 in formula_7. Da eine derartige Paarung immer eine Knotenüberdeckung darstellt und höchstens doppelt so groß ist wie eine minimale Knotenüberdeckung, berechnet der Algorithmus eine Knotenüberdeckung mit relativer Güte 2.

codice_1

Der Algorithmus hat bei einer geeigneten Datenstruktur eine Laufzeit von formula_38.



</doc>
<doc id="14425" url="https://de.wikipedia.org/wiki?curid=14425" title="Matching">
Matching

Matching (engl. "dazu passend" oder: der Vorgang des "Passend-Machens") steht für:


im weiteren Sinn für:
Siehe auch:


</doc>
<doc id="14429" url="https://de.wikipedia.org/wiki?curid=14429" title="Hindenburgdamm">
Hindenburgdamm

Der Hindenburgdamm verbindet die nordfriesische Insel Sylt mit dem Festland von Schleswig-Holstein. Er wurde am 1. Juni 1927 nach einer Bauzeit von vier Jahren eröffnet und dient ausschließlich dem Eisenbahnverkehr. Er ist Teil der Marschbahn von Hamburg nach Westerland. Ursprünglich eingleisig erbaut, später mit einer Ausweiche versehen, ist er seit 1972 durchgehend zweigleisig. Der Damm ist 11,3 km lang; aufgrund der später erfolgten Landgewinnung im Zuge der Eindeichungsmaßnahmen, durch die der Friedrich-Wilhelm-Lübke-Koog und der Rickelsbüller Koog gebildet wurden, verlaufen davon lediglich 8,10 km durch das Wattenmeer.

Nach Ende des Deutsch-Dänischen Krieges 1864 gehörten Sylt und Westerland zum neuen Kreis Tondern. Das Seebad Westerland gewann zunehmend an Bedeutung. Die Marschbahn führte 1887 bereits von Altona über Husum und Niebüll nach Tondern. Von dort aus erhielt sie eine Zweigstrecke bis zum Umschlaghafen Hoyerschleuse, von dem Raddampfer bis zum Sylter Hafen Munkmarsch fuhren, um den wachsenden Verkehr nach Sylt zu bedienen.

Die Verbindung war tidenabhängig, und im Winter schob sich gelegentlich das Eis im Wattenmeer zu einer unüberwindlichen Barriere zusammen. So dauerte die Überfahrt rund sechs Stunden, bei widrigen Witterungs- und Strömungsbedingungen auch länger. In der Zeit von 1875 bis 1876 führte Ludwig Meyn Untersuchungen und Bohrungen im Wattenmeer vor Sylt zum Bau eines Dammes vom Festland zur Sylter Ostspitze "Nösse" durch. Die zunehmende Bedeutung Westerlands als Seebad führte schließlich 1910 zur Aufnahme der amtlichen Planungen, 1914 begannen Bauvorbereitungen, die aber durch den Ersten Weltkrieg unterbrochen wurden. Als Folge des Kriegs kamen Tondern und Hoyerschleuse 1920 zu Dänemark; Sylt verblieb nach einer Volksabstimmung bei Deutschland. Nun mussten deutsche Reisende, um den Festlandsfährhafen nach Sylt zu erreichen, die neue Grenze überqueren und benötigten dafür ein Visum, dies erhöhte die Dringlichkeit zum Bau eines Dammes auf deutschem Staatsgebiet. Zwar wurde die Visumpflicht 1922 durch eine Transitregelung mit plombierten Zügen über dänisches Gebiet und Überwachung des Umsteigens in Hoyerschleuse durch den dänischen Zoll abgelöst, doch hatte sich Dänemark dazu nur für eine begrenzte Zeit und nur unter der Bedingung bereitgefunden, dass das Deutsche Reich diese Zeit nutzt, um einen neuen Zugang nach Sylt vom deutschen Festland aus zu schaffen.

Auf Grund der geringen Kapazität der Straßen nordwestlich von Niebüll wurde 1922 ein Gleis nach Klanxbüll verlegt, auf dem der Materialtransport erfolgte. 1923 wurde schließlich mit dem Bau des Eisenbahndammes begonnen. Vier Monate nach Baubeginn spülte eine Sturmflut das bis dahin Geschaffene fort. Nach dieser Erfahrung wurde die Trasse weiter nach Norden gelegt. Der Damm wurde von den Firmen Philipp Holzmann in Frankfurt am Main (vom Festland her) und Peter Fix Söhne in Duisburg (von Sylt her) unter der Regie des "Preußischen Wasserneubauamtes Dammbau Sylt" in Husum erbaut. Zwischen Buschlahnungen und Spundwänden wurde ein Spülfeld geschaffen. 1.000 bis 1.500 Arbeiter waren als Dammbauer tätig. In dem vier Jahre dauernden Bauprozess wurden über drei Millionen Kubikmeter Sand und Klei sowie 120.000 Tonnen Steine vom Festland angefahren. Der Damm erhielt den Querschnitt eines zweiseitigen Seedeiches mit 50 Meter Fußbreite und 11 Meter Kronenbreite.

Die Baukosten für den Damm beliefen sich auf 18,5 Millionen Reichsmark (das ergibt etwa 1700 Mark pro Meter – so viel kostete auch ein zweigleisiger Tunnel).
Um die Baukosten für den Dammbau in Höhe von 25 Millionen Mark (samt Zufahrstrecken) aufzufangen, wurde für die Fahrt über den Hindenburgdamm ein Zuschlag zum Preis einer Fahrt von 40 Kilometer Länge eingeführt. Der Zuschlag wurde ab 1933 schrittweise gesenkt und fiel 1940 weg.

Im Zusammenhang mit den befürchteten Strömungsänderungen durch den Dammbau wurden im Festlandsbereich nördlich und südlich des Damms je etwa 600 Meter Anwachs mit neuen, höheren Deichen eingedeicht, so dass der Wiedingharder Neue Koog und der Dreieckskoog entstanden.

Der Damm wurde nach dem damaligen Reichspräsidenten Paul von Hindenburg benannt, der die Eisenbahnverbindung am 1. Juni 1927 eröffnete und als einer der ersten Passagiere im Eröffnungszug vom Festlandbahnhof Klanxbüll nach Westerland auf Sylt fuhr. Beim anschließenden Frühstück im Kurhaus von Westerland taufte Julius Dorpmüller, der Generaldirektor der Deutschen Reichsbahn, den Damm auf den Namen "Hindenburgdamm".

Nach dem Zweiten Weltkrieg stand der Name lange Zeit in der Kritik, da Hindenburg wegen seiner zögerlichen Haltung als Wegbereiter Adolf Hitlers gesehen wurde. Es gab zahlreiche Initiativen, den Damm umzubenennen. Vorschläge wie „Sylt-Damm“, „Friedens-Damm“ und „Nordfriesland-Damm“ konnten sich jedoch nicht durchsetzen.

Der Damm unterbrach den Gezeitenstrom, der bis dahin zwischen dem Festland und Sylt floss. Es wird heute vermutet, dass die dadurch verursachte Änderung der Strömungsverhältnisse mitverantwortlich für den erheblichen Landverlust an der Hörnum-Odde am Südende von Sylt ist.

Der Damm liegt in der besonders geschützten Zone I des Nationalparks Schleswig-Holsteinisches Wattenmeer, die nicht betreten oder befahren werden darf. Wattwanderungen sind in diesem Teil des Wattenmeeres darüber hinaus auch deswegen nicht erlaubt, da die Tidenströme dort sehr stark sind.

Ab 1932 wurden auch Kraftfahrzeuge mit dem Zug nach Sylt befördert. Bis zum Zweiten Weltkrieg wurden sie nach Sylt nur als Wagenladungen transportiert. Täglich wurde ein Güterzug gefahren, der während der Sommersaison um einen reinen Kfz-Güterzug täglich ergänzt wurde.

Die Fahrzeuginsassen durften ab 1950 in den Fahrzeugen verbleiben, die nun nicht mehr mit Sicherungsseilen verzurrt wurden.
Ab 1951 gab es spezielle Autotransportzüge, die als Naheilzüge eingesetzt wurden. Anfangs fuhren die Autozüge viermal täglich. Im Folgejahr fuhren Autozüge sechsmal pro Tag. Bald drohte die Kapazität für den Transport von Kraftfahrzeugen an ihre Grenzen zu stoßen und der Wunsch nach dem Bau einer Straßenverbindung nach Sylt wurde immer lauter.
1955 wurden zur Kapazitätssteigerung Kreuzungsmöglichkeiten auf dem Damm und auf dem Festland bei Lehnshallig geschaffen. Ostern 1957 wurden 450 Fahrzeuge übergesetzt. Nach Abschluss der Beschleunigungsarbeiten wurde die Strecke bis Morsum auf Sylt 1957 zur Hauptbahn heraufgestuft. Ab 1960 wurden Kraftfahrzeuge nur noch in reinen Autozügen befördert. 1961 wurden neue doppelstöckige Autotransportwagen in Betrieb genommen. 1964 kamen neue doppelstöckige Gliedertransportwagen zum Einsatz. Seit 1972 ist die Strecke auf dem Damm zweigleisig. Seit einigen Jahren verkehren die Züge als "Sylt-Shuttle", seit 2016 auch als "RDC Autozug Sylt". Von 1997 bis 2013 war für den Autoverladeverkehr die DB AutoZug zuständig. Diese wurde Ende September 2013 aufgelöst und auf DB Fernverkehr verschmolzen.

Am 20. Januar 2011 entschied das Oberverwaltungsgericht in Münster, dass "DB AutoZug" Bedingungen festlegen muss, wie die Verladeterminals von anderen Anbietern mitgenutzt werden können.


Im Personenfernverkehr nutzen vor allem Urlauber den Hindenburgdamm zur Fahrt nach Sylt. Mehrmals täglich verkehren Intercity-Züge über den Damm. Regionalzüge der Deutschen Bahn fahren etwa im Stundentakt; sie dienen unter anderem dem Transport von Pendlern aus dem Bereich Niebüll. Der Güterverkehr zur Versorgung der Insel wird über den Hindenburgdamm per Lkw gefahren und nutzt überwiegend den Sylt-Shuttle.

Auf dem Damm befand sich eine Blockstelle, zu der das Personal mit planmäßigen Halten von Autozügen gebracht und abgeholt wurde. Seit 1996 wird sie als selbsttätiger Streckenblock ohne Personal betrieben.

"Hindenburgdamm" heißen auch Straßen in Berlin-Lichterfelde, Pinneberg und Rahden. Die in den 1920er Jahren neu angelegte Allee "Hindenburgdamm" in Dortmund wurde nach 1945 in "Rheinlanddamm" umbenannt.




</doc>
<doc id="14430" url="https://de.wikipedia.org/wiki?curid=14430" title="Alphorn">
Alphorn

Das Alphorn ist ein Blechblasinstrument. Diese Zuordnung ergibt sich (unabhängig vom verwendeten Material – meistens Holz, seltener Plexi) aus der Technik der Tonerzeugung und dem Mundstücktypus (Kessel- oder Trichtermundstück bzw. Kombinationen hieraus). Da es keine Möglichkeit hat, seine Rohrlänge flexibel zu verändern, ist das Alphorn an die Töne der Naturtonreihe gebunden (Zur Physik der Tonerzeugung siehe den Artikel Polsterpfeife) und ist somit ein Naturhorn. Es gilt als ein Nationalsymbol der Schweiz. Auch in Österreich und den bayerischen Alpen sind Alphörner verbreitet.

Lange Holztrompeten gibt oder gab es in vielen Kulturen und Ländern, z. B. in den Karpaten ("trembita"), in der Ukraine ("truba"), in Rumänien ("bucium"), in Skandinavien ("lur"), in Peru ("pampa corneta") und bei den Maori in Neuseeland ("pūkaea"). 

Das Alphorn gehört aufgrund seiner Anblastechnik instrumentenkundlich zu den Blechblasinstrumenten, obwohl es traditionell überwiegend aus Holz gefertigt wird. Es kann, je nach Landschaft, 5 bis 10 km weit gehört werden. In der Schweiz erfreut sich das Alphorn allgemeiner Beliebtheit. Es besitzt weder Klappen, Züge noch Ventile und ist daher bezüglich der zu spielenden Töne auf die Naturtonreihe beschränkt. Der in der Schweiz nach demselben Prinzip funktionierende Büchel und die Tiba sind weniger verbreitet.

Beispielsweise hat in der "Sinfonia pastorella" für Alphorn und Streicher von Leopold Mozart das Soloinstrument einen Tonumfang von nur vier Tönen, dem Dreiklang aus 4., 5. und 6. Naturton und der darunterliegenden Quarte (3. Ton). Geübte Spieler erreichen allerdings die ersten 16 Töne der Naturtonreihe. Drei der Naturtöne (7./14., 11. - das Alphorn-Fa, 13.) liegen relativ mittig zwischen zwei aufeinanderfolgenden Halbtönen der gleichstufigen Tonleiter. Von einigen Musikern werden diese für mit westlicher Musik vertraute Ohren ungewohnt bzw. dissonant klingenden Töne deshalb nicht verwendet. Andere sehen sie als Instrumententypisch und setzen sie als klangliche Eigenheit eines Naturinstruments bewusst ein.

Die erste bekannte schriftliche Erwähnung eines Alphorns in der Schweiz datiert auf 1527. Von damals stammt ein Eintrag in einem Rechnungsbuch des Klosters von St. Urban über „zwei Batzen an einen Walliser mit Alphorn“.

Im 18. Jahrhundert geriet das Alphorn fast in Vergessenheit, da die verarmten musizierenden Hirten in den Städten es im 17. Jahrhundert in Verruf brachten und es als Bettelhorn verspottet wurde. Doch die Romantik und die Touristen in den Schweizer Alpen (zuerst waren es vor allem die Engländer) brachten im 19. Jahrhundert die Folklore und auch das Alphorn zum Blühen. Heute gilt in der Schweiz das Alphorn und das Schweizer Taschenmesser neben Käse und Schokolade als das Nationalsymbol. Die ersten Hirtenfeste (Unspunnenfeste) mit Alphorn-Musik fanden 1805 und 1808 statt.

Der Eidgenössische Jodlerverband zählt an die 1800 organisierte Alphornbläser in der Schweiz und in der ganzen Welt zu seinen Mitgliedern. In Deutschland gibt es zahlreiche Alphornbläser, die sich auch zu internationalen Treffen zusammenfinden und dort als Solisten, Ensembles oder auch in Massenchören auftreten, wie beispielsweise beim jährlichen Allgäuer Alphornbläsertreffen oder beim Landestreffen der baden-württembergischen Alphornbläser. In Österreich gibt es unter anderem das Alphornfestival in Baad im Kleinwalsertal.

Im jetzt französischen Munster (Elsass), dessen Geschichte von den Zuwanderern aus den Alpenländern nach dem Dreißigjährigen Krieg geprägt wurde, hat sich das Alphorn-Spielen unter den Sennern auf den vogesischen Gipfeln seit zwei Jahrhunderten eingebürgert. Obwohl diese Tradition erst am Anfang des 19. Jahrhunderts schriftlich belegt ist, besteht kein Zweifel daran, dass die Herstellung und Anwendung des Alphorns durch Münsterer Sennhirten auf den vogesischen Almen auf die Schweizer oder Tiroler Einwanderung in das verwüstete Tal zurückzuführen sind. Es bedurfte einiger Jahrhunderte, bis dieses eingeführte Wissen sich dauerhaft im Fecht-Tal etablierte und so weit entwickelte, dass eine lokale Produktion auch mit anderen Materialien wie Glas oder Weißblech anfing.
Es gibt wenige klassische Kompositionen für Alphorn, die bekanntesten davon sind die "Sinfonia pastorella" für Alphorn und Streicher in G-Dur von Leopold Mozart sowie die "Parthia auf Bauerninstrumenten" von Jiří Družecký (Georg Druschetzky). Neuere Werke sind das "Concertino Rustico" des ungarischen Komponisten Ferenc Farkas sowie das "Konzert für Alphorn und Orchester" und "Dialog mit der Natur" für Alphorn, Piccolo und Orchester des Schweizer Dirigenten und Komponisten Jean Daetwyler. 1996 entstand das "Concertino für Alphorn in F und Streicher" von Franz Kanefzky. Im Jahre 2004 entstand im Auftrag des Menuhin Festivals in Gstaad das "Concerto for Alphorn and Orchestra" des Schweizer Komponisten Daniel Schnyder, das von Arkady Shilkloper uraufgeführt wurde. Es verwendet ein konventionelles, klassisches Sinfonieorchester, zieht jedoch einen dreifach zu besetzenden Schlagzeugpart und Synthesizer hinzu. Stilistisch kann man es als Crossover zwischen Jazz und Klassik bezeichnen. 2014 wurde als Kompositionsauftrag der musica viva das "concerto grosso Nr. 1" für 4 Alphörner und Orchester von Georg Friedrich Haas uraufgeführt, das mit Obertonreihen und Mikrotonalität arbeitet.

Vereinzelt wird das Alphorn auch rein im Jazz verwendet. Die Gruppe "Kerberbrothers Alpenfusion" setzt das Instrument in den Stücken "Alphornblues" und "Geierwalli", beide 1998 auf CD veröffentlicht, ein. Auch der Jazztrompeter und Komponist Matthias Schriefl benutzt teilweise mehrere Alphörner in verschiedenen Tonarten bei seiner Band "6, Alps and Jazz".

Die Behauptung, dass Hirten früher ihre Hörner vorwiegend als Signalhörner benutzten, ist falsch. Die Ortung des Horns wäre in einem mit Bergen umgebenen Gebiet beinahe unmöglich, da der Schall von den Wänden reflektiert werden würde. Der Schall wäre somit mehrfach und von verschiedenen Seiten hörbar (Echo).

Die Technik der Rohrherstellung aus Holz ist uralt, bis in die jüngste Zeit wurden sogar Wasserleitungen so oder ähnlich hergestellt.

Heute gibt es einige spezialisierte Instrumentenbauer, die aus geeigneten Holzstämmen ein Alphorn herstellen. Seine unten abgebogene Form stammte ursprünglich von der am Hang und somit krumm gewachsenen Fichte, die geschält und der Länge nach halbiert wird. Schon seit längerem wird dieser gebogene Teil nicht mehr verwendet und die Form des Bechers hängt nicht mehr von der natürlichen Krümmung ab, sondern ist standardisiert: In der Schweiz unterscheidet man die "Berner" Form – größerer Bogen – von der "Luzerner" oder "Innerschweizer" Form, mit etwas engerer Schallbecherkrümmung. Das anschließende Aushöhlen der beiden Hälften auf eine Wanddicke von 6 bis 8 Millimeter ist eine über siebzig Stunden dauernde Handarbeit. Eine anschließende Umwicklung aus Peddigrohr (früher Rindenblätter, Holzstreifen oder Wurzeln) dient als Wetterschutz und ein (zumeist) hölzernes Mundstück als Mittel zur Tonerzeugung, wie bei Blechblasinstrumenten üblich. Der Preis für ein solches Instrument liegt bei etwa 1200 bis 3300 Euro (Stand: 2013).

Folgende Stimmungen werden heute gebaut:

In der Schweiz ist das Fis/Ges-Alphorn am weitesten verbreitet, in Deutschland das F-Alphorn. In der üblichen Ausführung kann man Alphörner heute in drei Teile zerlegen.

Das längste Alphorn der Welt hat eine Länge von 47 Metern. Diesen Weltrekord hält der Alphornbauer Josef Stocker aus Kriens zusammen mit dem US-Amerikaner Peter Wutherich, wobei zum endgültigen Entscheid auch um den Durchmesser des Bechers (engl.: bell) gerungen wurde. Nach Angaben von Josef Stocker ist dieses Alphorn nicht bespielbar. Wenn jedoch beim Zusammenbau nicht alle Teile verwendet werden, dann entsteht mit einer Länge von 14 Metern das längste bespielbare Alphorn. Dieses hat 64 Töne gegenüber den 16 Tönen eines „normalen“ Alphorns. 

Das längste an einem Stück gefertigte Alphorn mit 20,67 Metern, eingetragen ins Guinness-Buch der Rekorde, stammt aus der Werkstatt von Alois Biermaier in Bischofswiesen (Oberbayern) und kann dort besichtigt werden.

Heute werden vereinzelt Alphörner aus mit Glas- oder Kohlenstofffasern verstärktem Kunststoff, aber auch aus Acrylglas gefertigt. Sie sind nicht mehr als ein knappes Kilogramm schwer und kosten ca. 2.500 Euro. Klanglich ist solch ein modernes Alphorn den Holzhörnern deutlich unterlegen. Versuchsweise wurden auch Instrumente mit Klappen oder einer Ventilmaschine (Wirkung der Ventile wie bei einer Trompete) gebaut, um den Tonumfang auf eine diatonische Tonleiter (Klappen) oder eine chromatische Tonleiter (Ventilmaschine) zu erweitern.

In Orgeln findet man gelegentlich das Register "Alphorn". Der spezielle Klang des Alphorns wird bei diesen Orgeln durch eine Orgelpfeife (meist aus Holz) imitiert, zu finden ist zum Beispiel in der Schwalbennestorgel von 1977 im Ulmer Münster als 16′-Register im Hauptwerk.

Ein Dokumentarfilm über das Alphorn stammt von Stefan Schwietert und heißt "Musik der Alpen – Das Alphorn". Er hat eine Spielzeit von 76 Minuten. Er behandelt die Ursprünge des Instruments und leitet über zu moderner Auffassung über das Alphornmusizieren – sozusagen vom Jodlerverband bis zum Jazz.

Schweizer Protagonisten im Bereich der "alpinen Weltmusik" sind Hans Kennel, Eliana Burki, MYTHA mit Betty Legler und Hans Kennel, Balthasar Streiff als Soloperformer, mit hornroh, modern alphornquartet und Stimmhorn. Hartmut Schmidt schrieb zwei Konzerte für Alphorn und Orchester.

Armin Rosin brachte klassische Musik in Zusammenhang mit dem Alphorn: "Alphorn Goes Classic" (CD-Einspielung 2001).

Auch in der moderneren Popmusik fand das Alphorn schon Verwendung. Der Schweizer Musiker Pepe Lienhard und seine Band verwendeten das Instrument 1977 in ihrem Song "Swiss Lady", mit dem sie am Eurovision Song Contest teilnahmen und damit den sechsten Platz erreichten. In der Schweiz war der Titel ein Nummer-eins-Hit und hielt sich 18 Wochen lang in den nationalen Charts.

Bekannte Alphornspielerinnen sind unter anderen Lisa Stoll und Eliana Burki.



</doc>
<doc id="14431" url="https://de.wikipedia.org/wiki?curid=14431" title="Rind">
Rind

Rind steht für:

Rind ist der Familienname folgender Personen:

RIND als Abkürzung steht für:

Siehe auch:



</doc>
<doc id="14432" url="https://de.wikipedia.org/wiki?curid=14432" title="Rinder">
Rinder

Die Rinder (Bovini) sind eine Gattungsgruppe der Hornträger (Bovidae). Es sind große und stämmige Tiere, von denen einige Arten als Nutztiere eine wichtige Rolle spielen, allen voran das Hausrind. Einige Rinderarten werden auch „Büffel“ genannt, dies ist eine willkürliche Bezeichnung, die keine systematische Relevanz hat.

Rinder erreichen eine Kopfrumpflänge von 1,60 m bis 3,50 m, wozu noch ein bis zu 1,00 m langer Schwanz kommt. Die Schulterhöhe variiert von 0,70 m bis 2,00 m, das Gewicht von 150 kg bis über 1000 kg (spanische "Kampfstiere" um 500 kg). Diese Tiere weisen einen stämmigen Rumpf mit kräftigen Gliedmaßen auf. Das Fell ist meist in Grau-, Braun- oder Schwarztönen gefärbt, die Länge und Beschaffenheit variiert je nach Lebensraum. Beide Geschlechter tragen Hörner, die der Weibchen sind jedoch kleiner und dünner. Die Hörner sind im Gegensatz zu denen vieler anderen Hornträger glatt. Wie alle Wiederkäuer haben sie einen mehrkammerigen Magen, der ihnen die Verwertung von schwer verdaulicher Pflanzennahrung ermöglicht.

Das ursprüngliche Verbreitungsgebiet der Rinder umfasste Nordamerika, Eurasien und Afrika. Sie bewohnen eine Reihe von Lebensräumen, bevorzugen jedoch vorwiegend offene Waldgebiete und Grasländer. Sie leben meist in Herden unterschiedlicher Sozialstruktur zusammen und sind Pflanzenfresser.

Mindestens fünf Rinderarten, Auerochse, Banteng, Gaur, Yak und Wasserbüffel wurden domestiziert; insbesondere das Hausrind und der Wasserbüffel haben dadurch eine weltweite Verbreitung erlangt und kommen in verwilderten Populationen auch in Regionen vor, in denen ursprünglich keine Rinder beheimatet waren. Im Gegensatz dazu sind die meisten wildlebenden Arten in ihrem Bestand bedroht. Der Auerochse ist im 17. Jahrhundert ausgestorben, der Kouprey wohl in den 1980er Jahren. Der Tamarau wird von der IUCN als vom Aussterben bedroht gelistet, viele andere Arten als gefährdet.

Stammesgeschichtlich sind die Rinder eine recht junge Gruppe. Erst im Pliozän sind die frühesten Rinder fossil belegt. Sie verbreiteten sich mutmaßlich von Asien aus über Europa, Nordamerika und Afrika. Vor allem im Pleistozän waren sie artenreich vertreten.
Im hier verwendeten engeren Sinn umfassen die Rinder drei Gattungen mit insgesamt 17 Arten, von denen zwei rezent ausgestorben sind:

Die Abgrenzung ist dabei umstritten. So wird die Vierhornantilope manchmal ebenfalls zu den Rindern gestellt, teilweise gilt "Bison" als identisch mit "Bos". In die nähere Verwandtschaft der Bovini gehören die Boselaphini und die Tragelaphini. Alle drei Triben bilden zusammen die Unterfamilie der Bovinae innerhalb der Hornträger.
Ursprünglich wurde die Gattung "Bison" mit dem Amerikanischen Bison ("Bos bison") und dem Wisent ("Bos bonasus") von den Eigentlichen Rindern ("Bos") abgetrennt. Eine im Jahr 2004 veröffentlichte molekulargenetische Studie basierend auf mitochondrialer DNA, vorgestellt von Alexandre Hassanin und Anne Ropiquet, kam zu einem anderen Schluss ("Bubalus mindorensis" und "Bubalus quarlesi" sind in dieser Systematik nicht erfasst). Demnach ist der Amerikanische Bison näher mit dem Yak, der Wisent dagegen mit dem Hausrind (beziehungsweise dem Auerochsen) verwandt. Zu beachten ist dabei jedoch, dass eine Untersuchung mitochondrialer DNA nur matrileneare Verwandtschaftsverhältnisse beschreibt und damit nur einen kleinen Ausschnitt der möglichen Verwandtschaftsverhältnisse. Untersuchungen der (ebenfalls nur einen kleinen Ausschnitt der möglichen Verwandtschaftsverhältnisse beschreibenden) paternal vererbten Y-Chromosomen stützen dagegen die klassische Systematik mit der Aufteilung in die Gattungen "Bos" und "Bison". Ein Erklärungsansatz für diese widersprechenden Ergebnisse matrilinearer und patrilinear Phylogenetik ist, dass der neuzeitliche Wisent durch eine ingressive Verdrängungseinkreuzung männlicher "Bison"-Vertreter (z. B. Steppenwisent "Bos priscus") in eine Population von "Bos"-Kühen entstanden ist, Dies würde bedeuten, dass (a) prähistorische "Bison"-Bullen sich mit prähistorischen "Bos"-Kühen gepaart haben, so dass nur "Bison"-Y-Chromosomen und "Bos"-Mitochondrien weitergegeben wurden, und (b) dass die daraus hervorgegangenen Hybridpopulationen über genügend viele Generationen von Nachkommen hinweg ausschließlich von "Bison"-Bullen gedeckt wurden, um den Phänotyp dieser Hybridpopulationen wieder ununterscheidbar vom "Bison"-Phänotyp zu machen.

Das Verwandtschaftsverhältnis kann durch folgenden Stammbaum verdeutlicht werden:




</doc>
<doc id="14435" url="https://de.wikipedia.org/wiki?curid=14435" title="Hausrind">
Hausrind

Das Hausrind oder schlicht Rind ("Bos primigenius taurus") ist die domestizierte Form des eurasischen Auerochsen. Es wurde zunächst wegen seines Fleisches, später auch wegen seiner Milch und Leistung als Zugtier domestiziert. Seitdem hat der Mensch eine Anzahl unterschiedlicher Rinderrassen gezüchtet, in die teilweise auch Wildrinder (etwa der Amerikanische Bison beim Beefalo) eingekreuzt wurden. Rinder sind Spitzengänger und Paarhufer.

Die Zebus ("Bos primigenius indicus") stammen von der indischen Unterart des Auerochsen ab. In Abgrenzung von Rassen zebuinen Ursprungs bezeichnet man die in Europa üblichen Hausrinder als taurine Rinder. Zebus wurden von manchen Autoren auch als eigene Art ("Bos namadicus") geführt, was aufgrund der Ähnlichkeit mit den restlichen Auerochsentypen und der uneingeschränkten Kreuzbarkeit des Zebus mit taurinen Hausrindern nicht berechtigt ist.

Vor allem in Asien sind weitere Tiere domestiziert worden, die von anderen Arten abstammen, so das Balirind ("Bos javanicus" f. domestica) aus dem Banteng ("Bos javanicus"), der Gayal ("Bos gaurus" f. frontalis) aus dem Gaur ("Bos gaurus") und der Hausyak ("Bos mutus" f. grunniens) aus dem Wildyak ("Bos mutus").

Im Gegensatz zu den bisher genannten Arten, die der Gattung "Bos" (Eigentliche Rinder) angehören, zählt der Wasserbüffel ("Bubalus arnee") zur Gattung "Bubalus" (Asiatische Büffel). Aus ihm wurde der Hausbüffel gezüchtet.

Heute geht man davon aus, dass die taurinen Hausrinder, welche in Europa und Nordamerika üblicherweise gehalten werden, ursprünglich aus Anatolien und dem Nahen Osten stammen, wo die eurasische Unterart des Auerochsen, "B. p. primigenius", ebenfalls vorkam. DNA-Untersuchungen ergaben, dass sich bereits die Ahnen der taurinen Rinder und der Zebus genetisch unterschieden und somit unabhängig voneinander domestiziert wurden.

Die Domestizierung zum taurinen Hausrind fand bereits vor rund 10.000 Jahren statt. Als Bestätigung gilt, dass um diese Zeit Ackerbauern zusammen mit Rindern und weiteren Nutztieren, die sich damals äußerlich noch nicht von den Wildtieren unterschieden, auf das bis dahin rinderlose Zypern gelangten. Die Zebus wurden aus der indischen Unterart des Auerochsen ("Bos primigenius namadicus") gezüchtet.

2012 wurde von einer internationalen Forschergruppe rund um Wissenschaftler der Universität Mainz festgestellt, dass die heutigen taurinen Rinder letztendlich von 80 weiblichen Tieren aus dem „Fruchtbaren Halbmond“ abstammen. Introgression männlicher und mitunter sogar einzelner weiblicher Auerochsen in den Genpool europäischer Hausrinder wird durch einige Studien nicht ausgeschlossen bzw. sogar suggeriert.

Mit der Domestizierung wurde die Anatomie der Auerochsen deutlich verändert. Nicht nur wurde stets nach den umgänglichsten Exemplaren selektiert, sondern auch nach den ertragreichsten. Dies führte dazu, dass der Rumpf der Rinder länger und massiger wurde, die Beine kürzer und das Euter größer und oft haarlos. Der einst geschwungene Rücken mit der kräftigen Nacken- und Schulterpartie des Wildrinds wurde gerade und niedrig. Auch haben viele Rinder ein pädomorphes „Kälbchengesicht“, d. h. eine verkürzte Schnauze und Stirn. Viele der sogenannten Hochleistungsrinder haben zusätzlich verkümmerte Hörner. Auch traten beim Hausrind neue Farbschläge auf, etwa durch das Fehlen von Pigmenten oder die für Haustiere typische gescheckte Zeichnung. Typisch für Hausrinder ist auch eine oft dramatische Reduktion des Geschlechtsdimorphismus bezüglich Größe und Fellfarbe, welcher bei einigen ursprünglichen Rassen allerdings noch vorhanden sein kann.

Der Grad der züchterischen Modifikation des Hausrindes hängt von der Form der Landwirtschaft und dem Verwendungszweck ab. Einige Rinder in Südeuropa, vor allem Iberien, sind aufgrund der stellenweise noch sehr extensiven Haltung in ihrer Anatomie teilweise sehr ursprünglich. Sie sind robust genug, das ganze Jahr über frei auf der Weide zu leben, und bekommen kaum Zufütterung. Sie haben kleine Euter und eine hochbeinige Statur. Oft ist auch noch eine ursprüngliche Hornform vorhanden. Kräftige Zugrassen wie Sayaguesa, Pajuna oder Maronesa haben zusätzlich noch die geschwungene Rückenlinie. Das Spanische Kampfrind wurde, da es primär für Kampflust gezüchtet wurde, ebenfalls wenig modifiziert und weist noch deutliche Ähnlichkeit mit dem Auerochsen auf.

Da einige Rinderrassen ihrer Stammform näher sind als andere, gibt es seit langem die Idee, ein dem Auerochsen entsprechendes Rind rückzuzüchten. Das Heckrind war das erste Resultat dieser als Abbildzüchtung bekannten Zuchtmethode, doch wird dessen Authentizität oft für unzureichend befunden. TaurOs Project ist ein neueres, multidisziplinäres Projekt, welches mit den ursprünglichen Rassen aus Südeuropa und anderen Robustrassen arbeitet.

 Kalb und Jungrind

Noch nicht zuchtreife (juvenile) Jungtiere werden bis zum siebenten Monat als "Kalb" bezeichnet und vom achten bis zum zwölften Monat dann als "Jungrind". Erste Brunstanzeichen treten im Alter zwischen sechs und zwölf Monaten auf und zeigen die Geschlechtsreife an. Da die Jungrinder in diesem Alter körperlich noch nicht für eine Belegung gebaut sind, werden sie erst in einem Alter von 15 bis 20 Monaten zugelassen. Dann haben sie etwa ein Gewicht von 350 bis 400 Kilogramm Lebendmasse.

Im Alter von etwa vier bis zwölf Monaten heißt das Jungtier auch "Fresser" (je nach Geschlecht "Bullen-" oder "Färsefresser", bei der Abstammung von zwei verschiedenen Rassen "Kreuzungsfresser"), sofern es der Milchviehhaltung entstammt. Das Mutterrind eines Fressers ist demzufolge eine Milchkuh. Wie die Bezeichnung Fresser vermuten lässt, ist das Tier von diesem Alter an nicht mehr auf die Milch der Mutter oder sogenannte Milchaustauscher angewiesen. Es ernährt sich ausschließlich von Raufutter und Kraftfutter. Je intensiver die Haltung, desto früher erfolgt die Entwöhnung von der Mutter und der Milch:
Dementsprechend wird das Tier früher oder später als "Fresser" bezeichnet.

Demgegenüber entstammen "Absetzer" – ebenfalls Kälber oder Jungrinder – dem Produktionsverfahren der Fleischrinderhaltung. Sie werden bis zum Zeitpunkt des Absetzens (im Alter von sechs bis elf Monaten) im Regelfall bei dem Mutterrind aufgezogen und anschließend als Absetzer vermarktet oder aber bis zur Verwendung in der Zucht weiterversorgt oder bis zur Schlachtung gemästet.

 Färse

Ein zuchtreifes (adultes) weibliches Rind wird mit circa 18 Monaten besamt (seltener bedeckt) und hat somit ein Erstkalbealter von etwa 27 Monaten. Bis dahin wird es als "Färse" oder (im Süddeutschen/Österreichischen) "Kalbin" bezeichnet. Weitere regionale Bezeichnungen sind zum Beispiel "Quie"/"Quiene", "Starke"/"Sterke" sowie "Queen/Queene/Beijst/Beijste" in Norddeutschland sowie "Gusti"/"Guschti" um Bern in der Schweiz und "Mäse(n)"/"Mese(n)" in Bündnerdeutsch im Kanton Graubünden. Im Allgäu bezeichnet man weibliche Jungtiere als "Schump(e)".

Erst nach dem ersten Kalben wird das geschlechtsreife weibliche Hausrind als "Kuh" bezeichnet (ein sehr altes Wort: althochdeutsch "kuo", indogermanisch *"gou-"). Dient die Kuh zur Milch- oder Fleischgewinnung, wird sie auch als "Milchkuh" bezeichnet. Eine Kuh, die ausschließlich ihr Kalb aufzieht, nennt man "Mutterkuh". Eine Kuh, die (auch) fremde Kälber mit aufzieht, nennt man "Ammenkuh". In alemannisch- und romanischsprachigen Alpendialekten wird die Kuh auch als "Lobe" bezeichnet.

Ein sterilisiertes weibliches Rind (jedweden Alters) nennt man "Schnitzkalbin". Hierzu gehören auch weibliche Tiere aus Zwillingsgeburten, bei denen eines der Zwillingskälber ein Bulle ist (Zwicke, Freemartin). Der sich entwickelnde Hormonhaushalt des Bullenkalbes verhindert über Verbindungen der Blutgefäße (Anastomosen) beider Mutterkuchen (Plazentae) bei seiner Zwillingsschwester die vollständige Ausbildung der Eierstöcke, so dass weibliche Kälber aus zweigeschlechtlichen Zwillingsgeburten zu 95 % unfruchtbar sind.

 Bulle/Stier

Das geschlechtsreife männliche Hausrind heißt "Stier", in Deutschland auch "Bulle", und wird auch als "Samenochse", "Samenrind", "Farre", "Farren", "Fasel" oder "Faselochse" (älter auch: "Fasselochse") bezeichnet, im Südbadischen und Allgäuerischen als "Hägel", "Häge", "Haigel" oder "Hage", im Schweizerdeutschen und anderen alemannischen Dialekten oft als "Muni" (Stier) und im Schwäbischen als "(der) Hummel", was das Schimpfwort "hummeldumm" erklärt („dumm wie ein Stier“).

Man unterscheidet zwischen "Mastbullen" und "Zuchtbullen".

Ein geschlechtsreifes, aber noch junges männliches Rind bis zu einem Höchstalter von 24 Monaten wird gemäß EU-Verordnung als "Jungbulle" oder "Jungstier" bezeichnet. Der dreijährige Stier findet sich als "Terz" genannt.

Ein kastriertes („verschnittenes“) männliches Rind jeglichen Alters heißt "Ochse". Ein durch Verlagerung der Hoden an die Bauchdecke sterilisierter Bulle wird "Muchse" genannt.


Hausrinder sind in mehrerer Hinsicht für Menschen nützlich, wobei einige Rassen im Hinblick auf eine oder mehrere bestimmte Nutzungsarten besonders gezüchtet wurden. Man unterscheidet dabei die Zweinutzungsrassen von den milch- und fleischbetonten Rassen. Neben Milch, Fleisch, Leder oder Fellen liefern Rinder Gülle oder Jauche und Mist, die in der Landwirtschaft als natürliche Düngemittel oder auch als Brenn- und Baumaterial eine wichtige Rolle spielen, außerdem erfüllen besonders Ochsen in vielen Teilen der Welt noch heute als Zugtiere für Karren oder zum Pflügen eine wichtige Funktion. Des Weiteren sind Robustrassen wie das Schottische Hochlandrind, Ungarisches Steppenrind, Heckrind, Galloway-Rind oder südeuropäische Primitivrassen wie Sayaguesa ein wichtiger Faktor in der Landschaftspflege und im Naturschutz (Almwirtschaft).

Bei den Rindern selbst lassen sich die Nutzungsrichtungen Milchproduktion und Fleischproduktion unterscheiden. Es gibt Rassen, die überwiegend auf eine der beiden Nutzungsrichtungen hin gezüchtet wurden, aber auch solche, bei denen beide Nutzungsrichtungen züchterisch bearbeitet werden (= Doppelnutzung, DN). Die Unterschiede zwischen beiden Richtungen sind genetisch bedingt. Die Spezialisierung auf einzelne Leistungsmerkmale setzte im 18. Jahrhundert ein als Züchter wie Robert Bakewell lokale Rassen, die vorwiegend in der Subsistenzwirtschaft eine Rolle spielten, durch eine selektive Auswahl von qualitativ hervorstechenden Elterntieren gezielt auf einzelne Leistungsmerkmale verbesserte.

Rassen mit hoher Milchleistung zeigen typischerweise hohe Spiegel endogen synthetisierter Wachstumshormone (Somatotropin, BST). Typische Milchvieh-Rassen sind beispielsweise Holstein-Friesian (= Rot- und Schwarzbunte, HF), Braunvieh (= Brown Swiss, BS) oder Fleckvieh (= Simmentaler, FV) als Doppelnutzungsrind.

"Siehe auch: Milchviehhaltung"

Fleischrinder haben eine günstigere Struktur des Fleisches (Faserigkeit, Marmorierung). Früher wurden männliche Tiere zur Verbesserung des Fleisches kastriert und somit zu Ochsen gemacht. In Deutschland ist dies heutzutage nur noch in extensiven Haltungsformen üblich. Es werden sowohl männliche als auch weibliche Tiere geschlachtet. Verbreitete Fleischrassen sind beispielsweise Hereford, Charolais und Limousin, daneben andere, mehr regional verbreitete Rassen wie Angus und Galloway. Bei der Nutzungsrichtung Fleischproduktion wird zwischen Rassen unterschieden, die ein schnelles Wachstum aufweisen, aber nicht zwangsläufig großrahmig sind (zum Beispiel Limousin) und solchen Rassen, die auf ein hohes Endgewicht kommen (beispielsweise Charolais).

In vielen weidewirtschaftlich oder nomadisch geprägten Kulturen gelten Hausrinder als Statussymbol und Gradmesser des Vermögens. Dort kommt eine Schlachtung deswegen in der Regel nicht infrage. Insbesondere in Indien werden Hausrinder bis heute religiös verehrt.

Vor Entwicklung humaner Antiseren galt für die ausschließlich verfügbaren tierischen Seren die Reihenfolge Pferd, Rind, Hammel. Dadurch sollte eine Sensibilisierung durch artfremdes Eiweiß umgangen werden. Diese Empfehlung galt bis zum letzten Drittel des 20. Jahrhunderts.

Einige Rinder erlangten aufgrund ihrer Leistungen oder Eigenschaften größere Bekanntheit:

"Siehe auch: "

Je nach nationaler Gesetzgebung tragen Kühe eine Identifikationsnummer, wie in der EU in Form einer Ohrmarke. Eine namentliche Benennung ist damit nicht erforderlich. Dennoch tragen viele Kühe einen individuellen Namen. Dies geschieht vorwiegend als Merkhilfe, etwa um Verwandtschaftsverhältnisse durch gleiche Anfangsbuchstaben zu verdeutlichen.
Das "Landeskuratorium der Erzeugerringe für tierische Veredelung" (LKV) erstellt hierzu die Grundprinzipien zur Kuhnamensvergabe und das „Verzeichnis der Kuh-Namen“ in Buchform. In der Herdbuchzucht haben die meisten Zuchttiere Namen, denen oft eine Buchstabenkombination als fälschlich sogenannter Betriebssuffix vorangestellt ist. Amerikanische Kuh- und Bullennamen enthalten oft die Namen der Eltern wie Jenny-Lou Mrshl Toystory-ET den seiner Mutter Jenny-Lou und seines Vaters Marshall. Osborndale Ivanhoe hatte den Hofnamen der Osborndale-Farm vorangestellt.

Männliche Hausrinder wurden und werden insbesondere in Spanien, Portugal, Südfrankreich sowie in ehemaligen spanischen Kolonien und spanisch beeinflussten Regionen in Lateinamerika bei Schauveranstaltungen vorgeführt, bei denen sie gegen einen Menschen kämpfen, sogenannte Stierkämpfe. Diese enden teilweise für das Tier tödlich, manchmal aber auch für den Menschen. Ähnliches gilt – abgeschwächt – auch für die Stierhatz.

Weibliche Rinder sind dagegen weitaus friedfertiger als männliche. Eine wesentliche Ausnahme sind dabei Mutterkühe, die ihr Kalb allenfalls auch ungerechtfertigt in Gefahr sehen und selten Kontakt zu Menschen haben. Besonders gefährlich wird es, wenn die Person auch einen Hund an der Leine mit sich führt und eine ganze Herde Rinder beisammen ist. So sind schon mehrere Fälle dokumentiert, in denen Kühe Menschen töteten. So wurde beispielsweise Ende Juli 2014 eine deutsche Urlauberin (mit angeleintem Hund) auf einer Alm im Tiroler Stubaital (Pinnisbach) von 20 Kühen angegriffen und zu Tode getrampelt. Strafrechtliche Ermittlungen wurden eingestellt, Angehörige des Opfers klagen 2017 auf 360.000 € Schadenersatz. Der Zivilprozess begann am 9. Mai 2017 und wird aufmerksam beobachtet, da bei einer Verurteilung des Bauern zu befürchten ist, dass Bauern ihre Weiden absperren und diese dadurch als Wandergebiet verloren gehen. Den Fall nahmen Vertreter der Landwirtschaftskammer, des Tierschutzvereins, der Veterinärmedizin und der Tirol Werbung zum Anlass, „Verhaltensregeln im Umgang mit Rindern auf Almen“ zu äußern.

Am 8. Mai 2017 wurde in Kirchberg an der Raab (Steiermark) ein 80-jähriger ehemaliger Landwirt von einer trächtigen Kuh attackiert, woran er starb; die Kuh kalbte noch am selben Tag. Am 7. Juni 2017 wurden auf der Kranzhornalm bei Erl (Tirol) zwei 70-jährige Frauen – mit Hunden – auf einem Wanderweg über eine eingezäunte Weide von Kühen – mit Kälbern – angegriffen, eine Frau starb.

Das Hausrind ist weltweit verbreitet, wobei die Zebu-Rassen wesentlich besser an die Tropen angepasst sind als Rassen eurasischen Ursprungs. Seit dem Ende des 15. Jahrhunderts brachten Europäer das Hausrind nach Amerika, auf viele Inseln und nach Australien und Neuseeland, wo sich bald große verwilderte Bestände entwickelten, die jedoch ab dem 18. Jahrhundert zusammenbrachen. Es gibt jedoch auch heute noch eine Reihe von wildlebenden Hausrinderpopulationen. Lange Tradition haben etwa die Chillingham-Rinder oder die Betizuaks (siehe wildlebende Hausrinder).

Indien ist das Land mit dem größten Hausrind-Vorkommen: Dort leben ca. 226 Millionen Rinder. In Brasilien gibt es etwa 200 Millionen Schlachtrinder. In China sind es 108 Millionen Rinder, in den USA 96 Millionen und in Deutschland knapp 14 Millionen. Insgesamt leben etwa 1,5 Milliarden Rinder auf der Erde, deren Gesamtmasse ist fast doppelt so hoch wie die der Menschen.

Kühe wiegen etwa 500 bis 800 kg, Bullen 1000 bis 1200 kg. Die natürliche Lebenserwartung eines Rinds beträgt maximal 20 Jahre. Im Regelfall haben Rinder Hörner, hornlose Rinderrassen sind die Ausnahme. Bei einem Kalb kann das Hornwachstum durch einen heißen Metallstab, der auf die Hornansätze gepresst wird, verhindert werden. Dadurch können die Kosten für die Aufzucht weiter gesenkt werden. Infolgedessen liegt die Enthornungs-Quote in der Schweiz bei rund 90 Prozent.

Rinder sind, wie Pferde auch, Pflanzenfresser, nutzen aber als Wiederkäuer wie auch Schafe und Kamele die Nahrung weit besser aus. Sie können das Gras aber nicht so kurz abfressen wie Pferde.

Das Gebiss des Rindes enthält beim erwachsenen Tier 32 Zähne. In jeder Hälfte des Unterkiefers befinden sich drei Schneidezähne und ein Eckzahn, der die gleiche Größe hat. Außerdem befinden sich auf jeder Seite sechs Backenzähne. Im Oberkiefer fehlen Eck- und Schneidezähne. Stattdessen ist dort eine Knorpelleiste vorhanden. Wie der Unterkiefer besitzt er auf jeder Seite ebenfalls sechs Backenzähne. Zwischen den Eckzähnen des Unterkiefers und der Knorpelleiste des Oberkiefers und den Backenzähnen ist jeweils eine große Lücke vorhanden. Kurzes Gras wird zwischen den Schneidezähnen und der Knorpelleiste eingeklemmt und mit einem Kopfruck abgerupft.

Die Nahrung durchläuft vier Mägen (Pansen, Netzmagen, Blättermagen, Labmagen). Der Rinderkot, landläufig als "Kuhfladen" bezeichnet, hat einen nennenswerten Brennwert. Getrocknete Kuhfladen werden deshalb in Entwicklungsländern als raucharmer Brennstoff benutzt und geschätzt.

Ein Rind macht beim Fressen und Wiederkäuen pro Tag 30.000 Kaubewegungen und produziert bis zu 150 Liter Speichel. So verwundert es nicht, dass es an heißen Tagen bis zu 180 Liter Wasser zu sich nimmt und dabei bis zu 25 Liter pro Minute schluckt. Hochleistungskühe produzieren unter günstigen Ernährungs- und Haltungsbedingungen innerhalb eines Jahres weit über 10.000 Liter Milch.

Bei der Verdauung der Nahrung entstehen im Pansen wie bei allen Wiederkäuern Fermentationsgase, die vom Tier „herausgerülpst“ werden, und die beim Hausrind neben Kohlenstoffdioxid einen besonders hohen Anteil von Methan enthalten, insbesondere bei Raufutter.

Grundsätzlich unterscheidet man zwei Grundtypen von Hausrindern. Dies sind zum einen die taurinen oder buckellosen Rinder und zum anderen die Zebus oder Buckelrinder. Zebus stammen von einer anderen Unterart des Auerochsen ab als taurine Rinder. Nach Auffassung mancher Experten könnte die Urform des Zebus eine eigene Art ("Bos indicus") neben dem Auerochsen darstellen. Genetische Untersuchungen belegen, dass die heutigen Hausrinder nicht, wie lange geglaubt, einem Stamm angehören, sondern von zwei verschiedenen Linien abstammen. Beide Formen scheinen sich schon im wilden Zustand vor rund 600.000 Jahren getrennt zu haben.

Afrikanische Rinderrassen ähneln äußerlich entweder Indischen Zeburindern oder buckellosen Rindern. Ursprünglich wurden auf diesem Kontinent offenbar buckellose Rinder gezüchtet, während Tiere des Zebutyps erstmals vor rund 4000 Jahren eingeführt wurden und erst seit dem frühen Mittelalter (um 700 n. Chr.) im Zuge der Arabischen Invasion vermehrt auftraten. Eigenartigerweise zeigen sowohl die afrikanischen Buckelrinder als auch die afrikanischen buckellosen Rinder weit stärkere mitochondriale Übereinstimmung mit europäischen buckellosen Rassen als mit indischen Zeburindern. Das mitochondriale Genom wird ausschließlich maternal vererbt. Man nimmt daher an, dass männliche Zebus in die ursprünglichen afrikanischen Rassen eingekreuzt worden sind.
Die buckellosen ursprünglichen Rassen Afrikas werden als er bezeichnet. Archäologische Funde deuten darauf hin, dass Sanga-Rinder auf ein Domestikationsereignis zurückgehen, das in Afrika stattfand. Demnach wäre der Afrikanische Auerochse unabhängig domestiziert worden.

Es gibt eine große Zahl von Rinderrassen, die für verschiedene Ansprüche gezüchtet werden.
Die für die Nutztierhaltung in Europa wichtigsten Rinderrassen sind:

Allerdings gehen gerade in der heutigen Zeit wegen der durch den wirtschaftlichen Druck verstärkten Massentierhaltung und Technisierung der Landwirtschaft viele Rassen verloren. Aus diesem Grund wird jedes Jahr in Deutschland durch die GEH eine gefährdete Haustierrasse des Jahres gewählt, um auf diese Situation aufmerksam zu machen. Insbesondere von diesem Rückgang betroffen sind Rassen, die für spezielle Lebensräume oder als Zugtiere optimiert wurden (wie die Arouquesa). Zur Katalogisierung der Rinderrassen und Kennzeichnung im Rinderpass gibt es einen verbindlichen Rasseschlüssel.

Der Żubroń ist eine Kreuzung aus Hausrind und Wisent. Der Beefalo ist eine Kreuzung aus Hausrind und Amerikanischem Bison. Beide sind weniger anspruchsvoll und krankheitsresistenter als Hausrinder. Ein "Dzo" (männlich) oder "Zhom" (weiblich) ist die Kreuzung zwischen Yak und Hausrind. Das Tier wird vor allem in der Landwirtschaft in Nepal eingesetzt.

Einige für die Nutztierhaltung wichtige Krankheiten des Rindes sind Infektionskrankheiten. Die wichtigsten bakteriell verursachten Krankheiten sind: Brucellose, Milzbrand, Paratuberkulose, Panaritium, Rauschbrand, Salmonellose und Tuberkulose. Die wichtigsten durch Viren hervorgerufenen Krankheiten sind: Bovines Herpesvirus IBR/IPV, Mucosal Disease/Virusdiarrhoe BVD, Maul- und Klauenseuche MKS.
Die häufigsten Stoffwechselerkrankungen sind Ketose, Hypokalzämie und Tetanie.
Die wichtigsten Parasiten sind: Lungenwürmer, Spulwürmer, Leberegel und Kokzidien.
Weitere Erkrankungen von Bedeutung sind: BSE, Fremdkörpererkrankung des Netzmagens, Pansentympanie, Labmagenverlagerung und Trichophytie der Haut.





</doc>
<doc id="14436" url="https://de.wikipedia.org/wiki?curid=14436" title="Koralle">
Koralle

Als Korallen (altgr. "korállion") werden sessile, koloniebildende Nesseltiere (Cnidaria) bezeichnet. Die verschiedenen Gruppen von Korallen sind nicht näher miteinander verwandt, sondern gehören verschiedenen Taxa der Nesseltiere an. Am bekanntesten sind die Steinkorallen (Scleractinia), die den Hauptanteil an der Entstehung der Korallenriffe haben. Eine weitere bedeutende, artenreiche Gruppe sind die Oktokorallen (Octocorallia), zu denen die Weich-, Leder- und Röhrenkorallen, sowie die Gorgonien gehören. Die Schwarzen Korallen (Antipatharia) sind mit etwa 235 Arten sehr viel artenärmer. Während die bisher genannten Gruppen Blumentiere (Anthozoa) sind, gehören die Feuer- ("Millepora") und die Filigrankorallen (Stylasteridae) zur Klasse der Hydrozoa.

Neben den rezenten (heute lebenden) sind Rugosa und Tabulata ausgestorbene Korallengruppen. ("Siehe dazu: Korallen der Schwäbischen Alb".)

Korallen kommen ausschließlich im Meer vor, insbesondere im Tropengürtel. Im Hinblick auf die Wuchsform unterscheidet man zwischen Weichkorallen und Steinkorallen, wobei letztere durch Einlagerungen von Kalk Skelette bilden, durch die Korallenbänke oder ein Korallenriff entstehen, da totes Skelettmaterial fortwährend von lebendigem Gewebe überwuchert wird. Korallenskelette bestehen zum größten Teil aus Aragonit, den die Korallentiere aus ihrer Fußscheibe oder ihrem Ektoderm absondern, um der Kolonie Stütze zu verleihen. Die Einzelskelette sind in der Regel pflanzenartig verzweigt und an den Zweigenden, den Wachstumsspitzen, befinden sich oft farbenprächtige Polypen, die darüber hinaus den Eindruck vermitteln, Korallen seien unterseeische Blütenpflanzen.

Wie bei den meisten sessilen (= festsitzenden) Meerestieren handelt es sich auch bei Korallen um Filtrierer, d. h., sie ernähren sich auch durch das Herausfiltern von Mikroplankton, Nährstoffen und Spurenelementen aus dem strömungsreichen Meerwasser. Viele der Korallen, die in Nähe der Wasseroberfläche leben, ernähren sich jedoch nicht alleine durch Filtrieren von Plankton, sondern auch (oder sogar zum größeren Teil) durch Endosymbionten, d. h. in die Polypenzellen eingelagerte Symbiosealgen, sogenannte Zooxanthellen, welche auch die intensiven Farben im lebendigen Gewebe der Koralle verursachen. Diese einzelligen Algen sind mit ihrem Photosynthese-Stoffwechsel nahtlos in den Nährstoffhaushalt der Koralle eingebunden. Je nach vorhandenem Plankton kann die Größe der Korallenpolypen sehr unterschiedlich sein, deshalb unterscheidet man zwischen großpolypigen (LPS – Large Polyp Sclerantinia) und kleinpolypigen (Small Polyp Sclerantinia), wobei die Polypengröße von Millimeter-Bruchteilen bis zu mehreren Zentimetern variiert. Korallen gibt es seit über 400 Millionen Jahren; sie helfen dem Geologen bei Paläoklimarekonstruktionen (s. auch: Schuppen-Altersbestimmung (Biologie)).

Neben den Steinkorallen der Tropen findet man auch Kaltwasserkorallen (oder Tiefseekorallen), welche keine Zooxanthellen besitzen und sich ausschließlich durch die Filtration von Plankton ernähren. Sie sind erst seit Ende des 20. Jahrhunderts bekannt und wurden in allen Weltmeeren (einschließlich Mittelmeer, aber nicht im Schwarzen Meer und nicht in der Ostsee) in Meerestiefen von 40 (New England Seamount Chain) bis zu 3383 m (Nordatlantik) bei Temperaturen von 4 bis 12 °C nachgewiesen; hauptsächlich kommen sie aber in Tiefen zwischen 100 und 200 m unter dem Welleneinflussbereich vor. Unterhalb des Wellenbereiches, also unterhalb 100–300 m sind auch Kaltwasserkorallen riffbildend und bieten dann ebenso wie ihre oberflächennahen Verwandten einer vielfältigen Tierwelt Lebensraum. Etwa 4000 Tierarten wurden in den Kaltwasserkorallenriffen nachgewiesen. Ungefähr 600 Arten Kaltwasserkorallen sind bekannt, 17 davon sind zur Bildung größerer Riffe befähigt.

Siehe:

Die oben erwähnten Algen sind sehr temperaturempfindlich. Erwärmt sich das Wasser zu stark, beginnen sie, Giftstoffe zu produzieren, und werden daraufhin von den Korallen abgestoßen, woraufhin diese sofort absterben. Der weiße Kalkmantel bleibt bestehen, daher der Begriff Korallenbleiche. Durch die globale Erwärmung kommt es häufiger und länger andauernd zum „Überhitzen“ des Meerwassers. Dadurch verläuft eine ansonsten leicht verlaufende Korallenbleiche, von der sich eine Kolonie erholen kann, schwerer und führt schließlich zum Absterben. 

Eine weitere Gefahr droht durch die Versauerung der Meere, die einen Teil der anthropogenen Emissionen von Kohlenstoffdioxid aufnehmen, was die Bildung neuer Kalkschalen hemmt. Außerdem scheint das Einleiten von Fäkalien ebenfalls ein Faktor für die Korallenbleiche darzustellen, da sie an entsprechenden Stellen vermehrt beobachtet werden konnte. Als Auslöser werden coliforme Bakterien in den Fäkalien vermutet. Die Korallenbleiche hat in den letzten Jahren viele beliebte Tauchgebiete zerstört. Auch wenn noch nicht sicher ist, ab welcher Temperatur die Riffe aufgrund der oben genannten menschlichen Aktivitäten absterben, zeigen Korallenriffe in warmen Gebieten bereits erste Symptome einer irreversiblen Änderung des Ökosystems (Stand 2015). 2016 wurden bei der schwersten jemals gemessenen Korallenbleiche im Great Barrier Reef 55 % der Riffe schwer geschädigt, während es bei den beiden vorangegangenen schweren Bleichen 1998 und 2002 nur 18 % gewesen waren. Insgesamt waren 2016 93 % aller dortigen Riffe von der Bleiche betroffen.

Durch Tiefseefischerei (Schlepp- und Grundnetzfischerei) sind Tiefseekorallen bedroht. Ein negativer Einfluss von Bohrplattformen zur Erdöl- oder Erdgasförderung in der Umgebung von Kaltwasserkorallenriffen wird nicht ausgeschlossen.
Transkontinentale Unterwasserkabel zur Telekommunikation stellen ebenfalls eine Bedrohung dar.

Seit mehreren Jahrzehnten versuchen Enthusiasten wie Wolf Hilbertz, Tom Goreau oder die Global Coral Reef Alliance der Zerstörung der Korallenriffe entgegenzusteuern. Sie schaffen Nationalparks in den Meeren und versuchen künstliche Korallenriffe zu schaffen und absterbende Riffe zu erhalten. Dabei wurden die verschiedensten Methoden angewandt, wie der Riffball, die Biorock-Technologie, das Versenken von Schiffen, Flugzeugen und Fahrzeugen. Ein künstlich angelegtes Riff aus versenkten Autoreifen (Osborne-Riff) hat sich in den USA zur ökologischen Katastrophe entwickelt.

Die kalkigen Achsenskelette einiger Oktokorallen und Schwarzen Korallen werden für die Schmuckherstellung verwendet. Diese Nutzung lässt sich bis in die vorgeschichtliche Zeit nachweisen. Als Schmuckstein ist die rote Edelkoralle am begehrtesten. Wichtiger sind jedoch die lebenden Korallen als Brutstätte und Kinderstube für viele Meeresbewohner. Hier finden sie Schutz vor ihren Feinden und genügend Nahrung. Im Lebensraum der Korallen existieren etwa ein Viertel aller bekannten Meeresfische.

Korallen und Korallenäste wurden schon in der Antike für Amulette verwendet. Sie galten als Schutz gegen Krankheiten, Blitzschlag und Misswuchs.
Sie waren im alten Ägypten der Isis und in Rom der Venus heilig.
Rosenkränze aus Korallen („Paternosterkrallen“) waren im Nachmittelalter sehr beliebt.
Im italienischen Volksglauben schützen Korallen Kinder gegen Unheil. Daher findet man auch viele Darstellungen des Jesuskinds mit Korallenkette und Halsband mit Korallenast.
In der Profanikonographie ist die Korallenkette ein Attribut der Kindheit geworden. In der Erzählung „Der Leviathan“ von Joseph Roth spielt die Koralle eine zentrale symbolische Rolle.

Für das Mittelalter ist auch die heilkundliche Anwendung der Kalkskelette von Korallen belegt. Verwendet wurden sie zu Pulver gemahlen in Arzneitränken gegen Milz- und Harnsteinleiden.



</doc>
<doc id="14437" url="https://de.wikipedia.org/wiki?curid=14437" title="Bhopal">
Bhopal

Bhopal (), ‚Stadt der Seen‘ genannt, ist die Hauptstadt des Bundesstaates Madhya Pradesh in Indien mit 1,8 Millionen Einwohnern (Volkszählung 2011). Sie liegt am östlichen Ufer eines 361 Quadratkilometer großen künstlichen Sees namens Upper Lake auf dem Vindhya-Plateau. Sie ist Industriestadt (chemische Industrie, Baumwoll- und Edelsteinverarbeitung), Kulturzentrum mit Universität, Musikakademie, Theater, Kinos und Museen, außerdem Verkehrsknoten (Straße, Eisenbahn, Flughafen).

Die Katastrophe von Bhopal 1984 gilt als die schwerste Chemiekatastrophe der Geschichte.

Bhopal liegt am "National Higway No. 12", der Jaipur im Westen mit Jabalpur im Osten verbindet. Nach Indore führt der "State Highway 17". Der "Raja Bhoj Airport" befindet sich 15 Kilometer nordwestlich der Stadt und ist nach dem Flughafen von Indore der zweitgrößte Flughafen des Bundesstaates.

Bhopals Name geht auf das 11. Jahrhundert zurück, als Raja Bhoj (1010–53), Parmar König von Dhar, von seinen Hoflehrmeistern die Weisung erhielt, den Mord an seiner Mutter durch die Verbindung der neun Flüsse seines Königreiches zu sühnen. Nachdem einer dieser Flüsse durch einen Damm gebändigt worden war, gründete der Herrscher an den beiden so entstandenen Seen seine neue Hauptstadt "Bhojapal". Ende des 17. Jahrhunderts eroberte Dost Muhammad Khan (1672–1740), ein opportunistischer Glücksritter, Ex-Soldat und ehemaliger General des Großmoguls Aurangzeb (1618–1707) das Gebiet, um auf den Trümmern des Mogulreiches seinen eigenen Staat Bhopal zu errichten. Im Jahre 1723 wurde der Ort Bhopal zur Hauptstadt des Fürstenstaats erklärt und blieb es bis 1956. 

Die von Dost Muhammad Khan gegründete islamische Dynastie sollte zu einer der bedeutendsten Herrscherfamilien Zentralindiens werden, deren Angehörige unter Großbritanniens Vizekönigen zu den wenigen Auserwählten gehörten, die durch einen Salut mit 19 Schüssen geehrt wurden – in Anerkennung ihrer Unterstützung von General Thomas Goddard (1740–83) bei dessen Marsch auf den hinduistischen Staat Maratha im Jahre 1778. Von 1820 bis 1926 regierten in Bhopal ausschließlich Frauen. Die Begums (Fürstinnen) führten das Zepter hinter dem Parda (Vorhang; von Männern abgetrennter Wohnbereich, Schleier) und ließen noble Bauwerke errichten, zu denen die drei Moscheen aus Sandstein gehören, die noch heute das Stadtbild prägen.
Am 3. Dezember 1984 ereignete sich in Bhopal eine der größten Katastrophen in der Geschichte der industriellen Chemie. Im Werk der "Union Carbide of India Limited", einer Tochtergesellschaft der "Union Carbide Corporation", wurden rund 40 Tonnen Methylisocyanat (MIC) in die Atmosphäre freigesetzt und trieben in einer Giftgaswolke dicht über dem Boden durch ein angrenzendes Elendsviertel und betraf etwa eine halbe Million Menschen. Dadurch starben nach offiziellen Angaben 1.600 Menschen sofort und rund 6.000 weitere an den unmittelbaren Nachwirkungen, bis heute summiert sich die Zahl der Opfer auf mindestens 20.000 Personen. Rund 100.000 Menschen leiden heute unter chronischen und unheilbaren Krankheiten, die sich offensichtlich zum Teil weitervererben können; Noch heute ist jede vierte Geburt in Bhopal eine Totgeburt. Die verantwortliche Firma Union Carbide zahlte in den folgenden Jahren insgesamt 690 Millionen Dollar an den indischen Staat. Doch nur ein kleiner Teil des Geldes kam den Opfern zugute.

Im Dezember 1992 kam es in Bhopal, ausgelöst durch die Zerstörung der Babri-Moschee in Ayodhya, zu blutigen Straßengefechten zwischen Hindus und Muslimen, worauf eine elftägige Ausgangssperre verhängt wurde. Doch trotz dieser religiös bedingten gewalttätigen Ausschreitungen bezeugen viele Geschichten von Hindus und Muslimen, die ihre andersgläubigen Freunde damals vor Übergriffen durch die aufgebrachte Menge schützten, die lange Tradition der religiösen Toleranz dieser Stadt.

Sehenswert ist die im Altstadtviertel aufragende Freitagsmoschee. Die Moschee mit ihren roten Sandsteinmauern und gedrungenen Minaretten wurde im Jahre 1837 auf Veranlassung von Qudsia Begum (1801–1881), des ersten weiblichen Herrschers von Bhopal, erbaut.

Im Osten der Stadt steht die "Moti Masjid", eine 1860 von Qudsias Tochter Sikander Begum (1818–1858) errichtete Perlen-Moschee. Bemerkenswert sind ihre schlanken Minarette mit goldenen Spitzen und Sandsteinkuppeln. 

Das eindrucksvollste Bauwerk der Stadt ist die "Darul Uloom Tajul Masjid". Dieses zu den größten Moscheen Indiens zählende Bauwerk mit seinen kolossalen pinkfarbenen Minaretten, die hoch über das Stadtbild hinausragen, trägt nicht zu Unrecht den Beinamen "Mutter aller Moscheen".

Unbedingt sehenswert ist das an der Ostküste des Sees auf einem Hügel gelegene Adivasi-Zentrum, das in natürlicher Umgebung das Leben der indischen Stämme zeigt, ohne dabei wie ein menschlicher Zoo zu wirken. Angeschlossen ist ein Museum.

Die Universität Bhopal wurde im Jahre 1970 eröffnet.





</doc>
<doc id="14442" url="https://de.wikipedia.org/wiki?curid=14442" title="Symmetrischer Graph">
Symmetrischer Graph

symmetrischer Graph wird in der Graphentheorie mit zwei unterschiedlichen Bedeutungen verwendet:




</doc>
<doc id="14446" url="https://de.wikipedia.org/wiki?curid=14446" title="Adjazenzliste">
Adjazenzliste

In der Graphentheorie sind Adjazenzlisten (oder auch Nachbarschaftslisten) eine Möglichkeit Graphen zu repräsentieren. Dabei wird für jeden Knoten eine Liste, die Adjazenzliste, aller seiner Nachbarn (in ungerichteten Graphen) bzw. Nachfolger (in gerichteten Graphen) angegeben. Oft basieren Datenstrukturen für Graphen auf Adjazenzlisten. Im einfachsten Fall wird in einem Array für jeden Knoten eine einfach verkettete Liste aller Nachbarn gespeichert.

Bei einem ungerichteten Graphen formula_1 versteht man unter einer Adjazenzliste für einen Knoten formula_2 
eine Liste aller Nachbarn von formula_3, d. h. eine Liste der Knoten formula_4.

Bei einem gerichteten Graphen formula_1 versteht man unter einer Adjazenzliste für einen Knoten formula_2 
eine Liste aller Nachfolger von formula_3, d. h. eine Liste der Knoten formula_8.

In beiden Fällen ist die Reihenfolge der Knoten in der Adjazenzliste beliebig.
Eine Adjazenzlisten-Repräsentation eines Graphen erhält man indem man für jeden Knoten eine Adjazenzliste angibt.
Ein ungerichteter Graph mit Knoten formula_9 und Kanten formula_10, und seine Repräsentation mit Hilfe von Adjazenzlisten.

Ein gerichteter Graph mit Knoten formula_9 und Kanten formula_12, und seine Repräsentation mit Hilfe von Adjazenzlisten.

Die Adjazenzlisten-Repräsentation von Graphen dient oft als Basis von Datenstrukturen für Graphen. 
Es gibt unterschiedliche Varianten diese Adjazenzlisten-Repräsentation in einer Datenstruktur umzusetzen, 
die auch unterschiedliche Verhalten der Datenstrukturen verursachen.

Einige Varianten: 




</doc>
<doc id="14450" url="https://de.wikipedia.org/wiki?curid=14450" title="Bezirk Mitte">
Bezirk Mitte

Mitte ist der erste Verwaltungsbezirk von Berlin und hatte Einwohner per .

Der Bezirk entstand 2001 mit der Berliner Verwaltungsreform durch Zusammenlegung der bis dahin eigenständigen Bezirke Wedding, Tiergarten und Mitte.

Im Bezirk Mitte befinden sich sowohl der Regierungssitz als auch die meisten Verfassungsorgane der Bundesrepublik Deutschland. Gleiches gilt für den Senat des Landes Berlin.

In Mitte haben sich eine Vielzahl von Zweigstellen international tätiger Unternehmen angesiedelt. Globale Bekanntheit besteht außerdem durch zahlreiche kulturelle Institutionen und aufgrund seiner Stellung als Gründerzentrum.

Der Bezirk Mitte grenzt Im Norden an den Bezirk Reinickendorf, im Osten an den Bezirk Pankow und im Südosten an den Bezirk Friedrichshain-Kreuzberg. Im Süden teilt er sich die Bezirksgrenze mit dem Bezirk Tempelhof-Schöneberg und im Westen grenzt er an den Bezirk Charlottenburg-Wilmersdorf.

Der Großteil des Bezirks ist verhältnismäßig dicht besiedelt mit einer durchschnittlichen Einwohnerdichte, die doppelt so hoch ist wie allgemein in Berlin. Eine Ausnahme bildet der Ortsteil Tiergarten, dessen Einwohnerdichte nur halb so hoch ist wie die von Berlin. Das Hansaviertel ist der flächenmäßig kleinste unter den 96 Ortsteilen Berlins.

→ "Siehe Liste der Naturdenkmale im Bezirk Mitte"

Am 1. Januar 2001 fusionierten die drei ehemaligen Bezirke Mitte, Tiergarten und Wedding zum neuen Bezirk "Mitte". Er ist einer von zwei Bezirken (neben Friedrichshain-Kreuzberg), die aus Teilen des ehemaligen Ost- und West-Berlin bestehen. Im allgemeinen Sprachgebrauch bezeichnet "Mitte" weiterhin den heutigen Ortsteil Berlin-Mitte und nicht den neuen Fusionsbezirk.

Der Bezirk Mitte unterteilt sich in sechs Ortsteile:


→ "Hauptartikel: Geschichte Berlins"

Das alte Berlin entstand an einer Gabelung der Spree, sodass sich eine seichte Furt mit vielen Sandbänken herausbildete. Auf einer erhöhten Sandbank in der Mitte der Spree siedelten die ersten Fischer (genannt: "Fischerinsel" auf der Spreeinsel von Berlin) und es bildete sich ein Marktflecken heraus – am Molkenmarkt stand auch der Roland von Berlin (heute gegenüber auf der anderen Spreeseite am Märkischen Museum).

Die beiden Spreearme sind mittlerweile stark umbaut, auf der Insel steht im Norden die Museumsinsel. In der Mitte befanden sich früher das Berliner Stadtschloss (1950 gesprengt und abgetragen) und am selben Ort seit 1976 der Palast der Republik, dessen Abriss im Jahr 2009 beendet wurde und auf dessen Gelände derzeit das Humboldtforum entsteht. Im Süden liegen Gebäude des Wohngebietes Fischerinsel. Am Ostufer der Spree breitet sich heute ein großer Platz mit dem Fernsehturm hin zum Alexanderplatz. Nur einen Steinwurf von der Fischerinsel und dem Molkenmarkt steht das Rote Rathaus und das Nikolaiviertel mit der ältesten Kirche Berlins.

Am zählte der Bezirk Mitte Einwohner auf einer Fläche von 39,5 Quadratkilometern. Somit lag am Stichtag die Bevölkerungsdichte bei Einwohnern pro Quadratkilometer. Am 31. Dezember 2016 lag der Ausländeranteil bei 32,6 Prozent, während der Anteil der Bevölkerung mit Migrationshintergrund am Stichtag bei 50,8 Prozent lag. Beide Werte waren jeweils die höchsten in allen Berliner Bezirken. Die Arbeitslosenquote bezifferte sich am 30. April 2013 auf 14,1 Prozent. Am 31. Dezember 2016 betrug das Durchschnittsalter der Bevölkerung 38,9 Jahre.

Im Bezirk befinden sich drei größere Industrie- und Gewerbegebiete:

Moabit-West: Auf dem rund 43 Hektar großen Areal zwischen Sickingenstraße und Huttenstraße haben sich eine Vielzahl von kleinen und mittelgroßen Betrieben angesiedelt. Der Standort ist jedoch in erster Linie geprägt durch großflächige Betriebe wie Siemens, das dort Gasturbinen für den Weltmarkt produziert. Nördlich des S-Bahnringes liegt der Berliner Großmarkt mit dem Fruchthof Berlin und der Westhafen.

Fennstraße: Der Standort ist rund 14 ha groß und liegt an der Fennstraße und Sennstraße im Ortsteil Wedding. Das prägende Unternehmen ist hier Bayer.

Humboldthain: An diesem 21 ha großem Forschung, Entwicklungs- und Produktionsstandort haben sich vorwiegend kleinere Unternehmen angesiedelt. Städtebaulich markant für den Standort sind die ehemaligen AEG-Fabriken, die an die Zeit der Industriemetropole Berlin Ende des 19. Jahrhunderts erinnern. Heute beherbergen sie u.a. das in den 80er Jahren gegründete erste deutsche Gründerzentrum (BIG) mit dem Technologie- und Innovationspark (TIB).

Aufgrund der zentralen Lage und dem Regierungsviertel haben sich im Bezirk viele Medienunternehmen und Hotels angesiedelt. Zu den bekanntesten Hotels im Bezirk gehört das Hotel Adlon.

Mit dem Alexanderplatz, der Friedrichstraße und dem Potsdamer Platz hat der Bezirk drei überörtliche Einzelhandelsstandorte. Als weitere Einkaufsstraßen sind noch die Müllerstraße, Turmstraße, Badstraße und die Potsdamer Straße zu nennen.

Durch den Bezirk Mitte führen die Bundesstraßen B 1, B 2, B 5 und B 96.

Durch die S-Bahn-Linien S1, S2, S3, S5, S7, S9, S25, S26, S41, S42 und S75 sowie die U-Bahn-Linien U1, U2, U5, U6, U8, U9 und U55 ist der Bezirk an den ÖPNV angebunden.

Ein neuer Teilabschnitt der U-Bahn-Linie U5 befindet sich gegenwärtig im Bau (geplante Fertigstellung: 2020). Ebenfalls in der Bauphase befinden sich die Tunnelarbeiten des S-Bahn-Projektes S21.


Die Bezirksverordnetenversammlung (BVV) des Bezirks Mitte wurde am 18. September 2016 gewählt.

Bezirksbürgermeister von Mitte war Christian Hanke (SPD), der 2006 im Rahmen einer Zählgemeinschaft der SPD mit den Stimmen der Verordneten der Linken und der FDP gegen die CDU und die Grünen zum Bezirksbürgermeister gewählt wurde. In den Berliner Bezirken ist eine Zählgemeinschaft (anders als in den Landesparlamenten der Bundesrepublik) nicht mit einer Koalition verbunden, sodass sich die Fraktionen bei Sachfragen jeweils Mehrheiten suchen. Von 2011 bis 2016 bestand eine Zählgemeinschaft von SPD und CDU.

Hier werden die Bürgermeister des Bezirks Mitte seit 2001 aufgeführt. Die Bürgermeister des ehemaligen Bezirks Mitte und heutigen Ortsteils Mitte finden sich dort.


Das Wappen des Bezirks Mitte wurde nach der Bezirksfusion der drei Bezirke Mitte (alter Bezirk), Tiergarten und Wedding durch den Heraldiker Theodor Lorenz (* 6. April 1929, † 25. März 2005) neu entworfen und gestaltet. Das heutige Wappen wurde am 9. Oktober 2001 durch den Senat von Berlin verliehen.

Blasonierung: Der sechsfach in Rot und Silber geständerte Schild ist mit einem goldenen Mittelschild belegt, darin ein wachsender schwarzer, rotbewehrter und -gezungter Bär, der in den Pranken einen blauen Schild mit aufrechtem goldenem Lilienzepter hält. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit dem Berliner Wappenschild belegt ist.

Wappenbegründung: Die Farben von Rot und Silber entsprechen den historischen Stadtfarben Berlins. Auch das Herzschild mit dem Berliner Bären verweist auf den ehemaligen Bezirk Mitte als der historischen Mitte Berlins ebenso wie die Lage in der gefächerten Schildteilung auf die zentrale Lage des Bezirks deutet. Der kleine Schild mit dem Zepter war bereits das Herzschild im Wappen der Dorotheenstadt – es ist ein Kennzeichen der Herrschergewalt und verweist auf die historische Rolle Berlins als Residenzstadt und Hauptstadt mehrerer deutscher Staaten. Die Mauerkrone ist das verbindende Element aller Berliner Bezirke.

International
, Japan (seit 1959, damals mit dem alten Bezirk Wedding und dem japanischen Fuse begründet)
, Israel (seit 1970 freundschaftliche Beziehungen, seit 1980 Partnerstadt, damals mit dem alten Bezirk Wedding begründet)
, Frankreich (seit 1995, damals mit dem alten Bezirk Wedding begründet)

National


Im Bezirk Mitte befindet sich das Regierungsviertel mit den wichtigsten Institutionen der Bundesregierung mit ihren Ministerien und des Deutschen Bundestages, dem Parlament der Bundesrepublik Deutschland, sowie zahlreiche Botschaften und Landesvertretungen;

Es gibt im Bezirk insgesamt 32 Grundschulen und zwei Grundstufen an Gemeinschaftsschulen. Eine Grundschule befindet sich im Bau. 14.573 Schüler und Schülerinnen besuchten im Schuljahr 2013/2014 eine Grundschule im Bezirk. Neben den zwei Gemeinschaftsschulen gibt es zehn integrierte Sekundarschulen (ISS), sieben Gymnasien und vier Sonderschulen. Mit Beginn des Schuljahres 2013/2014 betrug die Zahl der Schülerinnen und Schüler in den integrierten Sekundarschulen 4689 und in den Gymnasien 3204. 

Unter den nachgefragtesten Oberschulen in Berlin belegte 2017 die Herbert-Hoover-Schule (ISS) im Gesundbrunnen den siebten Platz und die Heinrich-von-Stephan-Gemeinschaftsschule in Moabit den achten Platz. Auf Platz zwei unter den Gymnasien wurde das Lessing Gymnasium in Wedding aufgeführt.

Des Weiteren gibt es noch sieben private Grundschulen, zwei private integrierte Sekundarschulen, vier private Gymnasien, zwei private Gemeinschaftsschulen und eine Waldorfschule.



Der Bezirk hat 40 allgemeine Sportanlagen inklusive vier Schwimmhallen der Berliner Bäder-Betriebe und 43 Schulsportanlagen. Zu den größten Sportanlagen gehört das Poststadion, das Stadion Rehberge und das Erika-Hess-Eisstadion.

Neben 259 öffentlichen Spielplätzen, gibt es 222 öffentliche Grünflächen. Die größten Grünanlagen sind: Großer Tiergarten, Fritz-Schloß-Park, Volkspark Rehberge und Volkspark Humboldthain.





→ "Siehe auch Liste der Kinos in Berlin-Mitte"

Auf dem Pariser Platz, dem Alexanderplatz, dem Gendarmenmarkt, dem Boulevard Unter den Linden und der Straße des 17. Juni und anderen städtebaulich herausragenden Orten finden zahlreiche überregional bedeutende Veranstaltungen statt. 



</doc>
<doc id="14451" url="https://de.wikipedia.org/wiki?curid=14451" title="Bezirk Friedrichshain-Kreuzberg">
Bezirk Friedrichshain-Kreuzberg

Friedrichshain-Kreuzberg ist der zweite Verwaltungsbezirk von Berlin und hatte am Einwohner.

Der flächenkleinste der zwölf Berliner Bezirke hat zugleich die höchste Bevölkerungsdichte und das geringste Durchschnittsalter. Er entstand 2001 mit der Berliner Verwaltungsreform durch Fusion der bis dahin eigenständigen Bezirke Friedrichshain und Kreuzberg.

Der zentral gelegene Bezirk gilt als alternativ und kreativ und ist bekannt für ein vielfältiges Nacht- und Kulturleben.

In der Nähe der Alexandrinenstraße 12 im Ortsteil Kreuzberg liegt der geometrische Mittelpunkt Berlins. ()

Friedrichshain-Kreuzberg ist einer von zwei Bezirken (neben dem Bezirk Mitte), die aus Teilen des ehemaligen Ost- und West-Berlin bestehen. Die beiden Ortsteile des heutigen Bezirks, Friedrichshain und Kreuzberg, sind durch die Spree voneinander getrennt. Die Oberbaumbrücke verbindet beide Altbezirke und ist damit zum Wahrzeichen des neuen Verwaltungsbezirks geworden, was im Bezirkswappen zum Ausdruck gebracht wird.

Beide Ortsteile gehören zu den Szenevierteln Berlins und erleben durch die hohe Nachfrage nach Wohnraum einen Strukturwandel in Form von Gentrifizierung. Da Kreuzberg und Friedrichshain seit der Fusion im Sprachgebrauch mitunter als Einheit betrachtet werden, haben sich Verkürzungen wie „Kreuzhain“, „Xhain“ oder "FR-KR" etabliert.

Eine Kuriosität ist, dass die Flussinsel "Großer Wall" im Ortsteil Hakenfelde des Bezirks Spandau der Verwaltung des Bezirksamtes Friedrichshain-Kreuzberg unterstellt ist.

Eine Reihe von historischen Stadtteilbezeichnungen auf dem Gebiet des Bezirks, wie zum Beispiel Luisenstadt und Tempelhofer Vorstadt, leben in den Bezeichnungen der Grundbücher weiter.



Im Zuge der Gründung Groß-Berlins wurde 1920 der Bezirk Friedrichshain gebildet. Er umfasste den größten Teil der Stralauer Vorstadt, einen kleinen Teil der Königsstadt und Stralau. Den weitaus größten Anteil an der 1920 geschaffenen Bezirksfläche umfasste die vormalige Stralauer Vorstadt. Der Name Friedrichshain war als Stadtgebietsname neu und leitete sich vom Volkspark am Rande des damaligen Bezirks her. 

Der Bezirk Kreuzberg wurde 1920 aus der Tempelhofer Vorstadt, der Oberen Friedrichsvorstadt, der südlichen Friedrichstadt und einem großen Teil der Luisenstadt gebildet. Benannt wurde der ehemalige Bezirk und heutige Ortsteil nach dem im Südwesten gelegenen Kreuzberg im heutigen Viktoriapark. 

Am 1. Januar 2001 wurden die zuvor eigenständigen Berliner Bezirke Kreuzberg und Friedrichshain zum neuen Bezirk Friedrichshain-Kreuzberg vereinigt. Im Jahr 2014 wurde ein Tausch von Flächen mit dem benachbarten Bezirk Tempelhof-Schöneberg beschlossen.

Per zählte der Bezirk Friedrichshain-Kreuzberg Einwohner auf einer Fläche von 20,2 Quadratkilometern. Am gleichen Stichtag lag die Bevölkerungsdichte bei Einwohnern pro Quadratkilometer. Friedrichshain-Kreuzberg zählte damit zu den am stärksten besiedelten Orten in Deutschland.

Strukturell unterscheiden sich beide Ortsteile insbesondere hinsichtlich ihrer Bevölkerungszusammensetzung. Im Jahr 2002 lag der Ausländeranteil gemäß Statistischem Landesamt Berlin bei 32,8 % in Kreuzberg und bei 8,7 % in Friedrichshain. Am 31. Dezember 2016 bezifferte sich der Ausländeranteil im gesamten Bezirk auf 25,6 %. Am Stichtag betrug der Anteil der Bevölkerung mit Migrationshintergrund 41,0 %. 

Die Alterszusammensetzung der Bevölkerung unterscheidet sich deutlich. In Kreuzberg ist der Anteil der 35- bis 60-Jährigen höher, der Anteil der Jüngeren ist dementsprechend niedriger als in Friedrichshain. Im Bezirk wohnen vornehmlich junge Erwachsene. Das Durchschnittsalter der Bevölkerung war am 31. Dezember 2016 mit 37,8 Jahren das niedrigste aller Berliner Bezirke, womit Friedrichshain-Kreuzberg der Bezirk mit den jüngsten Einwohnern ist (Vergleich: Der Bezirk Steglitz-Zehlendorf hat mit 46,2 Jahren die älteste Bevölkerung). 

Innerhalb Deutschlands zählt der Bezirk zu den Gegenden mit den im Durchschnitt jüngsten Bewohnern.

Im Jahr 2014 wurden in Friedrichshain-Kreuzberg 4276 Gewerbeanmeldungen und 2736 Gewerbeabmeldungen verzeichnet. Die Arbeitslosenquote im Bezirk bezifferte sich am 30. April 2013 auf 13,3 %.

Die Handwerkskammer Berlin, ein Unternehmensverband für Handwerksbetriebe, hat ihren Sitz in Friedrichshain-Kreuzberg. Stephan Schwarz ist aktuell der Präsident des Verbandes (Stand: 2017). Im Jahr 2015 waren von den 30.015 in Berlin vertretenen Handwerksfirmen insgesamt 1.894 im Bezirk Friedrichshain-Kreuzberg gemeldet.



Die Bundesstraßen B 1, B 5, B 96 und B 96a führen durch Friedrichshain-Kreuzberg.

Die Oberbaumbrücke, die die beiden Ortsteile Kreuzberg und Friedrichshain verbindet, zählt zu den am meisten von Radfahrern frequentierten Passagen in Berlin. 

Die S-Bahn-Linien S1, S2, S3, S5, S7, S8, S9, S25, S41, S42, S75 und S85 sowie die U-Bahn-Linien U1, U2, U5, U6, U7 und U8 führen durch den Bezirk.

Der in Friedrichshain gelegene Bahnhof Ostkreuz ist mit täglich etwa 100.000 S-Bahn-Nutzern einer der größten Umsteigebahnhöfe Berlins. Der Bahnhof wird gegenwärtig umgebaut (Stand: 2017) und soll an das Regionalbahnnetz angeschlossen werden. Im Zuge der Bauarbeiten wird der Platz vor dem Bahnhof umgestaltet und ein neues Bahnhofsgebäude gebaut.

Der Ostbahnhof ist ein Fern- und Nahverkehrsbahnhof im Bezirk und gilt nach dem Hauptbahnhof und dem Bahnhof Südkreuz als drittgrößter Bahnhof Berlins.

Das Wappen des Bezirks Friedrichshain-Kreuzberg wurde am 7. Oktober 2003 durch den Senat von Berlin verliehen.

Blasonierung: In silbernem Schild mit blauem Wellenschildfuß, belegt mit vier silbernen Wellenfäden, eine rote gezinnte Brücke, die durch zwei spitzbedachte und beknaufte Türme, die mit Zinnen, Zwischendächern, Simsen und schwarzen Schießscharten und Spitzfenstern bedeckt sind, in drei gleichmäßige Bögen unterteilt ist. Die beiden äußeren Bögen sind als Halbbögen dargestellt. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit dem Berliner Wappenschild belegt ist.

Wappenbegründung: Das Wappen ist vom erst 1991 entworfenen Wappen des ehemaligen Bezirks Friedrichshain abgeleitet. Es zeigt mit der Oberbaumbrücke das verbindende Wahrzeichen des neugebildeten Bezirks auf der ehemaligen Grenze der vorherigen Bezirke. Das blaue Wellenband verweist auf die Spree, über die die Oberbaumbrücke führt, und wurde auch aus dem vorherigen Wappen übernommen, das es wiederum aus der blauen Tingierung des Wappens Stralaus entlehnte. Die Mauerkrone ist das verbindende Element aller Berliner Bezirke.

Der Bezirk Friedrichshain-Kreuzberg unterhält aktuell zehn Städtepartnerschaften.

Die Direktion 5 der Berliner Polizei ist für die Bezirke Friedrichshain-Kreuzberg und Neukölln zuständig. 




Der "SC Kreuzberg" stellte mehrmals den Deutschen Meister im Schach. Mehrere Großmeister sind Mitglied des Clubs.

Die Berlin Bombshells wurden 2013 Deutscher Meister im Roller Derby.

Die Mercedes-Benz Arena ist die Heimspielstätte des achtmaligen deutschen Basketball-Meisters ALBA Berlin und des DEL-Eishockey-Rekordmeisters Eisbären Berlin. Die Arena ist auch hin und wieder Schauplatz von internationalen Sportveranstaltungen.

Die Skate­halle Berlin hat eine überdachte Fläche von über 6000 m² und gilt als eine der größten Hallen für Skateboard-Sport in Europa.

Eine Besonderheit in der Sportlandschaft des Bezirks ist der Fußballplatz von Blau Weiß Friedrichshain. Es ist die einzige FIFA-konforme Spielfläche der Stadt, die vollständig auf dem Dach eines Großmarktes eingerichtet wurde.




Im Jahr 2008 erhielt der Bezirk den von der Bundesregierung verliehenen Titel "Ort der Vielfalt".

In der Revaler Straße befindet sich das Wagendorf "Hänger & Laster," als Modell für alternative Lebensformen.

Bis 2014 fand auf der Oberbaumbrücke jedes Jahr eine als Demonstration angemeldete Gemüseschlacht zwischen Bewohnern der vormaligen Bezirke Kreuzberg und Friedrichshain statt. Sie galt als eine parodistische Auseinandersetzung der Bewohner der beiden Ortsteile.

Im Film "Unknown Identity" (2010) wurde die Oberbaumbrücke zu einem spektakulären Schauplatz. Der Sturz eines Taxis in die Spree wurde gefilmt.




</doc>
<doc id="14453" url="https://de.wikipedia.org/wiki?curid=14453" title="Bezirk Pankow">
Bezirk Pankow

Pankow [] ist der dritte Verwaltungsbezirk von Berlin und hatte Einwohner per .

Die bis 2001 eigenständigen Bezirke Pankow, Prenzlauer Berg und Weißensee wurden mit der Verwaltungsreform zum heutigen bevölkerungsreichsten Berliner Bezirk Pankow vereint. Flächenmäßig ist er der zweitgrößte Bezirk der Metropole.

Pankow ist der geburtenreichste Bezirk und weist eine eher vorteilhafte Sozialstruktur auf.

Der Bezirk ist geprägt von vielen Neuberlinern, die nach 1995 zugezogen sind. Eine verhältnismäßig hohe Anzahl von selbstständig Tätigen und Firmengründern lebt in Pankow.

Pankow liegt im Nordosten Berlins und grenzt an die Bezirke Lichtenberg (im Osten), Friedrichshain-Kreuzberg (im Süden), Mitte (südwestlich) und Reinickendorf (westlich) sowie im Norden an die Landkreise Oberhavel und Barnim im Land Brandenburg.

Der nördlichste Punkt des Bezirks befindet sich im Ortsteil Buch, der zugleich der nördlichste Ortsteil Berlins ist.

Der Bezirk liegt nahezu vollständig auf der eiszeitlich gebildeten Hochfläche des Barnim, die hier eben bis flachwellig ausgebildet ist. Zum größten Teil besteht die Landschaft aus Grundmoränenflächen. Entlang der Panke zieht sich auch ein Sander von Norden nach Süden zum Berliner Urstromtal.

"Siehe: Liste der Naturdenkmale im Bezirk Pankow"

Im Ortsteil Weißensee befindet sich der Park am Weißen See mit dem fast kreisrunden Weißen See, dem größten natürlichen Gewässer im Bezirk.

Die Karower Teiche sind ein Naturschutzgebiet in Pankow. Als Habitat für Amphibien, Libellen und Wasservögel ist es von überregionaler Bedeutung.

Der Bezirk ist relativ unterschiedlich besiedelt. Zwei Drittel der Bevölkerung des Bezirks leben in den drei größten Ortsteilen: Prenzlauer Berg, Pankow und Weißensee. Prenzlauer Berg ist der am fünftdichtesten besiedelte Ortsteil Berlins, während Blankenfelde und Stadtrandsiedlung Malchow die zwei am dünnsten besiedelten Ortsteile der Stadt sind. Diese zwei Ortsteile stellen gemeinsam fast ein Fünftel der Fläche des Bezirks dar, haben aber weniger als ein Prozent der Bevölkerung des Bezirks. Nach Neukölln ist Prenzlauer Berg der zweitbevölkerungsreichste Ortsteil Berlins.

Pankow gilt als grüner Bezirk, dabei sind der Bürgerpark, der Schlosspark und der Volkspark Schönholzer Heide besonders zu erwähnen.

Im Bezirk Pankow bestehen die folgenden 13 Ortsteile:

Die kleinräumige Gliederung für Berlin sind die Lebensweltlich orientierten Räume (LOR) mit Abgrenzung nach fachlichen Kriterien. Diese werden für sozialräumliche Planungszwecke genutzt und haben das Raumbezugssystem der „Statistischen Gebiete / Verkehrzellen“ ersetzt. Daten zu den Bevölkerungsstrukturen in diesen Planungsgebieten sind im Kiezatlas der Sozialraumdaten online zugänglich. Die Schlüsselnummern der LOR setzen sich jeweils aus den zwei Ziffern des Bezirks (Pankow=03), dem Prognoseraum, der Bezirksregion (ungefähr an den Ortsteilen ausgerichtet) und den eigentlichen Planungsräumen zusammen.

Bei der Bildung von Groß-Berlin im Jahr 1920 wurde aus den folgenden bis dahin zum Landkreis Niederbarnim gehörenden Gebieten der 19. Verwaltungsbezirk von Berlin gebildet:

Der Westteil der Landgemeinde Rosenthal einschließlich Wilhelmsruh kam zum Bezirk Reinickendorf. Nach seinem bevölkerungsreichsten Ortsteil erhielt der Bezirk den Namen "Pankow." Das Pankower Rathaus wurde auch Sitz des Bezirksbürgermeisters und der Bezirksverordnetenversammlung. 1930 wurde der Bezirk Pankow mit der Eröffnung der Streckenabschnitts Schönhauser Allee – Vinetastraße von der Berliner U-Bahn erreicht.

Die Berliner Gebietsreform mit Wirkung zum 1. April 1938 hatte zahlreiche Begradigungen der Bezirksgrenzen sowie einige größere Gebietsänderungen zur Folge. Das Gebiet westlich der Berliner Nordbahn an der Wollankstraße wurde vom Bezirk Pankow in den Bezirk Wedding umgegliedert. Der Bezirk Reinickendorf gab sein östlich der Nordbahn gelegenes Gebiet, bestehend aus Wilhelmsruh sowie der südlich anschließenden Waldsteg-Siedlung, an den Bezirk Pankow ab. Außerdem kam es zu kleineren Änderungen der Grenze zum Bezirk Prenzlauer Berg an der Bornholmer Straße und der Wisbyer Straße. Die Einwohnerzahl des Bezirks wuchs durch die Grenzänderungen um 1.629 Bewohner und die Fläche des Bezirks vergrößerte sich um 81 Hektar. Die ehemalige Grenze zwischen den Bezirken Pankow und Reinickendorf wird noch heute durch den Übergang der Hauptstraße in die Kopenhagener Straße mit dem Abspannwerk Wilhelmsruh als erstem Gebäude an der Kopenhagener Straße markiert.

Gegen Ende des Zweiten Weltkriegs in Europa wurde im Zuge der Schlacht um Berlin der Bezirk Pankow am 22. April 1945 von sowjetischen Streitkräften eingenommen und gemäß der Erklärung von Jalta dem Sowjetischen Sektor von Berlin zugeordnet. Ab Oktober 1949 war Pankow als Stadtbezirk von Ost-Berlin de facto Bestandteil der DDR, unabhängig vom strittigen Status der Stadt bis 1990.

Ab 1949 war das Schloss Schönhausen Sitz des Präsidenten der DDR, Wilhelm Pieck. Bis 1964 war das Gebäude Amtssitz des Staatsoberhaupts, des Vorsitzenden des Staatsrates. Ab 1964 bis zur politischen Wende 1989 diente das Schloss als das offizielle Gästehaus des SED-Regimes für hohe Staatsgäste. Zusammen mit dem nahegelegenen Majakowskiring bildete es eines der politischen Zentren der DDR.

Nach dem Bau der Berliner Mauer im August 1961 wurde der genaue Verlauf der Bezirksgrenze entlang der Nordbahn bedeutsam. Die S-Bahnhöfe Wilhelmsruh und Schönholz lagen trotz ihres Namens auf West-Berliner Gebiet und waren vom Bezirk Pankow aus nicht mehr zugänglich. Der S-Bahnhof Wollankstraße lag zwar auf dem Boden des Bezirks Pankow, sein östlicher Zugang wurde aber ebenfalls gesperrt. Der Bahnhof blieb von West-Berlin aus zugänglich.

Im Jahr 1985 wurden die Ortsteile Karow, Heinersdorf und Blankenburg in den damaligen Stadtbezirk Weißensee umgegliedert, der seinerseits einen Großteil seines Gebiets an den neuen Bezirk Hohenschönhausen abgab.

Mit der deutschen Wiedervereinigung am 3. Oktober 1990 wurde der Bezirk Pankow Teil des Bundeslandes Berlin. Am 16. September 2000 wurde die U-Bahn-Linie U2 von der Vinetastraße bis zum Bahnhof Berlin-Pankow verlängert.

Zum 1. Januar 2001 wurde der Bezirk Pankow im Rahmen der Berliner Bezirksreform mit den Bezirken Weißensee und Prenzlauer Berg zusammengeschlossen. Der neue Großbezirk erhielt nach längeren Auseinandersetzungen um die Bezeichnung auf Beschluss seiner Bezirksverordnetenversammlung wieder den Namen "Pankow."

Per zählte der Bezirk Pankow Einwohner auf einer Fläche von 103,07 Quadratkilometern. Somit lag am Stichtag die Bevölkerungsdichte bei Einwohnern pro Quadratkilometer. Siehe hierzu auch: Liste der Bezirke und Ortsteile Berlins.

Am 31. Dezember 2016 lag der Ausländeranteil bei 12,0 %, wobei dieser Anteil je nach Ortsteil stark schwankt. Der Anteil der Bevölkerung mit Migrationshintergrund lag am Stichtag bei 18,5 %. Die Arbeitslosenquote bezifferte sich am 30. April 2013 auf 10,0 % und war somit der niedrigste Wert aller Berliner Bezirke (zum Vergleich: Der Bezirk Neukölln hatte mit 17,1 % die höchste Arbeitslosenquote). Das Durchschnittsalter der Bevölkerung belief sich am 31. Dezember 2016 auf 40,7 Jahre. Der Frauenanteil lag im Bezirk bei 51,2 % gegenüber 48,8 % Männeranteil.

Mit rund 4500 Neugeborenen im Jahr 2013 war Pankow der geburtenreichste Bezirk in Berlin.

In Pankow konzentrieren sich IT-, Kreativ- und Medienwirtschaft, Schienenfahrzeugbau, Biotechnologie, Gesundheitswirtschaft, und Maschinenbau.

Im Jahr 2014 wirtschafteten mehr als 42.000 Unternehmen in Pankow. Eine Vielzahl darunter sind Kleinunternehmen aus den Bereichen Dienstleistungen, Baugewerbe und Handel. 74 % der Unternehmen haben bis zu fünf Mitarbeiter. Im Bezirk arbeitet ein hoher Anteil an Selbstständigen und Existenzgründern.

Am Standort Buch befindet sich einer der größten Klinikstandorte in Deutschland. Eine Vielzahl von Unternehmen im Bereich Biotechnologie sind dort tätig.

Im Unternehmen "Stadler Rail" waren 2015 rund 1200 Mitarbeiter beschäftigt.


Der Straßenverkehr im Bezirk wird geprägt durch die drei radial vom Stadtzentrum nach Norden bzw. Nordosten verlaufenden Bundesstraßen B 96a (Schönhauser Allee/Berliner Straße), B 109 (Prenzlauer Allee/Prenzlauer Promenade), die am S-Bahnhof Pankow-Heinersdorf in die Bundesautobahn 114 mündet, und die B 2 (Greifswalder Straße/Berliner Allee). Pankow ist der einzige Bezirk der vom Berliner Ring, der Autobahn A 10, erschlossen ist. Von großer Bedeutung für den Straßenverkehr sind auch die beiden Ringstraßen: Danziger Straße (innerhalb des S-Bahn-Rings) und Ostseestraße – Wisbyer Straße – Bornholmer Straße (außerhalb des S-Bahn-Rings).

Pankow ist in das Netz der Radfernwege eingebunden. So verläuft der Radfernweg Berlin–Usedom von der Museumsinsel entlang der Schönhauser Allee, biegt dann in die Schwedter Straße ab und führt durch den Mauerpark und den Schlosspark Pankow und weiter nach Karow und Buch. Entlang der Bezirksgrenze zwischen Pankow und dem Bezirk Reinickendorf und dem Ortsteil Wedding vom Bezirk Mitte, die dem früheren Grenzverlauf der DDR entspricht, verläuft der Berliner Mauerweg.

Pankow wird von den S-Bahn-Linien S1, S2, S25, S41, S42, S8, S85 und S9 angebunden. Im Bezirk liegt fast der gesamte nördliche Teil des Ostringes der Ringbahn mit den S-Bahnhöfen Storkower Straße, Landsberger Allee, Greifswalder Straße, Prenzlauer Allee und Schönhauser Allee. An dem auf der Grenze zum Ortsteil Gesundbrunnen des Bezirks Mitte gelegenen Bahnhof Bornholmer Straße teilen sich die von Süden kommenden Linien in die Strecken Richtung Oranienburg/Hennigsdorf (Berliner Nordbahn) und Bernau (Stettiner Bahn) auf. Auf der Grenze zum Bezirk Reinickendorf liegen die Bahnhöfe Wollankstraße, Schönholz und Wilhelmsruh, wobei sich die beiden zuletzt genannten Stationen auf Reinickendorfer Gebiet befinden. An der nach Nordosten führenden Stettiner Bahn liegen die Bahnhöfe Pankow, Pankow-Heinersdorf, Blankenburg, Karow und Buch. Am Karower Kreuz soll im Jahr 2021 der Bahnhof Karower Kreuz in Betrieb gehen.

Am Bahnhof "Berlin-Karow" beginnt die Regionalbahn-Linie NB27 (Heidekrautbahn) der Niederbarnimer Eisenbahn nach Groß Schönebeck beziehungsweise Wensickendorf.

Im Bezirk Pankow gibt es fünf U-Bahnhöfe der Linie U2: Im Ortsteil Prenzlauer Berg die Bahnhöfe Senefelderplatz, Eberswalder Straße und Schönhauser Allee sowie im Ortsteil Pankow die Bahnhöfe Vinetastraße und Pankow. Dabei sind die Bahnhöfe Schönhauser Allee und Pankow Umsteigebahnhöfe zur S-Bahn.

Das Straßenbahnnetz im Bezirk folgt im Wesentlichen den im Abschnitt Individualverkehr beschriebenen Hauptstraßen. Daneben gibt es noch eine von Südwesten nach Nordosten verlaufende Ergänzungslinie. Die aus der Schönhauser Allee nach Norden verlaufende Straßenbahnlinie verästelt sich im Ortsteil Pankow in drei Zweige. Die Linienführung in der Berliner Allee verlässt den Bezirk Pankow ostwärts in Richtung des Bezirks Lichtenberg. Die Strecke in der Berliner Allee wurde im Herbst 2005 saniert und mit lärmschluckendem Rasengleis sowie neuen Haltestellen ausgerüstet.
Auf einem Feld im Norden Pankows, Ortsteil Buch, (mit den Geo-Koordinaten: ) sowie bei der Stadtrandsiedlung Malchow befinden sich die einzigen beiden Windkraftanlagen Berlins (Stand: 2017). Bei beiden Anlagen, die 2008 bzw. 2014 in Betrieb genommen wurden, handelt es sich um Anlagen des Typs Enercon E-82 mit einem Rotordurchmesser von 82 m, einer Nabenhöhe von 138 m und einer Gesamthöhe von 179 m.

Die Nennleistung der ersten Anlage beträgt zwei Megawatt (MW), die zweite Anlage verfügt über eine Leistung von 2,3 MW. Das Regelarbeitsvermögen der Windkraftanlagen liegt bei 4 bzw. 5 Millionen kWh pro Jahr. Die Investitionssumme lag bei jeweils ca. 3,4 Millionen Euro.

Das Heizkraftwerk Berlin-Buch hat eine elektrische Leistung von fünf Megawatt sowie eine thermische Leistung von 130 MW.

Volksvertretung des Bezirks ist die Bezirksverordnetenversammlung (BVV) Pankow von Berlin. Die Wahl fand am 18. September 2016 statt.

Bezirksbürgermeister nach der Verwaltungsreform 2001, bestehend aus den früheren Bezirken Pankow, Prenzlauer Berg und Weißensee:

Blasonierung: In Silber oben eine durchgehende und oben anstoßende rote Mauer ohne Fugen, mit drei offenen Toren ohne Torflügel, der mittlere der Torbögen breiter und höher als die äußeren; unten ein achtspeichiges rotes Rad, das oberhalb seiner Mitte beidseits von einer grünen Hopfendolde mit Blatt begleitet ist. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit dem Berliner Wappenschild belegt ist.

Die Suche für ein Wappen des Bezirks Pankow fand am 28. Juli 2009 ihr Ende. Im Bezirk erfolgte die Entscheidung durch die Bezirksverwaltung nach Empfehlung durch die Wappenkommission. Nach der Verwaltungsreform 2001 ging der ehemalige kleinere Bezirk Pankow in den Großbezirk Pankow auf. Ende 2006 wurde die Auseinandersetzung um den Bezirksnamen beendet und am 1. Oktober 2007 folglich zur "Wappenfindung" aufgerufen. Am 30. April 2008 beschloss die Bezirksverordnetenversammlung, den Heraldiker Jörg Mantzsch mit der Gestaltung zu beauftragen, der mehrere Entwürfe nach vorgegebenem Inhalt zur Entscheidung vorlegte. Nach einigen Diskussionen zwischen dem Bezirksamt und dem ausführenden Heraldiker gab das Pankower Bezirksamt am 24. Februar 2009 den beschlossenen Entwurf bekannt. Die offizielle Verleihung des Wappens durch den Senat des Landes Berlin erfolgte am 28. Juli 2009.

Das Wappen des alten Bezirks Pankow wurde im Rahmen der 750-Jahr-Feier Berlins im Jahr 1987 verliehen. Es greift Elemente der ehemaligen Landgemeinde Pankow auf und zeigte . Das achtspeichige Rad wurde in vereinfachter Form aus dem Wappen von Weißensee übernommen. Im Rahmen der Bezirksreform wurde das Wappen zunächst um ein dreitürmige Mauerkrone ergänzt, deren mittlerer Turm mit dem Berliner Wappenschild belegt ist. Die Mauerkrone ist das verbindende Element aller Berliner Bezirke.

Die Direktion 1 der Berliner Polizei ist für die Bezirke Pankow und Reinickendorf zuständig.




Die Max-Schmeling-Halle wurde im Zuge der Olympia-Bewerbung Berlins für das Jahr 2000 im Stadtteil Prenzlauer Berg errichtet und 1996 von Max Schmeling eingeweiht. Die Mehrzweckhalle ist gegenwärtig die Heimspielstätte des Handballklubs Füchse Berlin. Außerdem wird sie für weitere sportliche Ereignisse, Veranstaltungen und Konzerte genutzt.

Der Rugby Klub 03 Berlin ist ein Rugby-Union-Verein aus Weißensee mit 250 Mitgliedern. Gegenwärtig spielt der Klub in der ersten Bundesliga (Stand: Saison 2016/17).

Gegründet im Jahr 1893 als "BFCC Rapide Niederschönhausen 1893", kann die SG Empor Pankow auf eine lange Tradition zurückblicken.

Das Velodrom steht am S-Bahnhof Landsberger Allee. Die Radrennsporthalle ist mit 12.000 Zuschauerplätzen nach der Mercedes-Benz Arena (17.000 Zuschauer) die zweitgrößte Veranstaltungshalle Berlins.

Direkt neben dem Velodrom befindet sich die Schwimm- und Sprunghalle im Europasportpark. In der Schwimmhalle fanden mehrere Deutsche Schwimmmeisterschaften und Schwimmeuropameisterschaften statt. Beide Gebäude wurden ebenfalls im Zuge der Berliner Olympia-Bewerbung erbaut.

Insgesamt gab es im Jahr 2017 mindestens 19 Yoga­studios in Pankow; die meisten davon im Ortsteil Prenzlauer Berg.


In Prenzlauer Berg sind im älteren Bereich zwischen Prenzlauer und Schönhauser Allee breite Bürgersteige mit Cafés und Kneipen trotz der dichten Bebauung sehenswert. Bemerkenswert sind die Großsiedlungsbauten aus den 1920er Jahren, die im Gegensatz zu den Mietskasernen das neue Bauen ihrer Zeit dokumentieren. Besonders gilt dies für die Wohnstadt Carl Legien, die von Bruno Taut projektiert wurde und zusammen mit fünf anderen Berliner Siedlungen Weltkulturerbe der UNESCO ist.







</doc>
<doc id="14454" url="https://de.wikipedia.org/wiki?curid=14454" title="Bezirk Charlottenburg-Wilmersdorf">
Bezirk Charlottenburg-Wilmersdorf

Charlottenburg-Wilmersdorf ist der vierte Verwaltungsbezirk von Berlin und hatte Einwohner per .

Er entstand 2001 mit der Berliner Verwaltungsreform durch die Fusion der damaligen Bezirke Charlottenburg und Wilmersdorf.

Der Bezirk gilt heute als eher bürgerlicher Wohnbezirk und weist eine vorteilhafte Sozialstruktur auf.

Charlottenburg-Wilmersdorf ist eines der umsatzstärksten Geschäfts- und Handelszentren in Berlin. Die im Bezirk ansässige Technische Universität zählt zu den größten ihrer Art in Deutschland.

Charlottenburg ist hervorgegangen aus der Gemeinde "Lietzenburg" (ursprünglich: "Lietzow"), auf deren Territorium zu Ehren von Königin Sophie Charlotte das Schloss Charlottenburg errichtet und 1705 die Stadt Charlottenburg gegründet wurde. Bis zur Eingemeindung zu Groß-Berlin im Jahr 1920 entwickelte sich Charlottenburg zur reichsten Stadt Preußens.
Der ehemalige Bezirk Wilmersdorf und heutige Ortsteil wurde nach 1220 gegründet. Mitte des 18. Jahrhunderts erwarben die ersten Berliner Bürger Land und Bauernhäuser im damaligen "Deutsch-Wilmersdorf" und richteten Sommersitze in der ein. Mit dem 1. April 1907 schied Wilmersdorf aus dem Kreis Teltow aus und wurde ein selbstständiger Stadtkreis. Ab 1912 führte die Stadt die Bezeichnung "Berlin-Wilmersdorf." Zum 1. Oktober 1920 wurde die Großstadt nach Groß-Berlin eingemeindet.

Der Bezirk entstand zum 1. Januar 2001 durch die Fusion der beiden ehemaligen West-Berliner Bezirke Charlottenburg und Wilmersdorf. Im Jahr 2004 wurde der Bezirk in die heutigen Ortsteile gegliedert.

Der Bereich um den Kurfürstendamm nimmt als "City West" neben der historischen Mitte eine der beiden Zentrumsfunktionen für ganz Berlin wahr. Nennenswerte im Bezirk gelegene Institutionen sind unter anderem die Technische Universität, die Universität der Künste, die Deutsche Oper, das Olympiastadion sowie das Messegelände unter dem Funkturm mit dem CityCube, der das nahegelegene Internationale Congress Centrum ICC ersetzt.

Der Bezirk ist sehr unterschiedlich besiedelt. Neben den dicht besiedelten Ortsteilen Wilmersdorf (Rang 7 in Berlin), Charlottenburg und Halensee (Rang 11 und 12) liegt Grunewald, einer der am dünnsten besiedelten Ortsteile Berlins. Im Ortsteil Grunewald leben drei Prozent der Bevölkerung des Bezirks auf mehr als einem Drittel der Gesamtfläche des Bezirks, allerdings entfallen rund 85 Prozent der Fläche des Ortsteils auf den unbewohnten Forst Grunewald und Wasserflächen. Zwei Drittel der Bevölkerung des Bezirks lebt in Charlottenburg und Wilmersdorf. Halensee ist nach dem Hansaviertel der zweitkleinste Ortsteil Berlins, während der Ortsteil Grunewald zu den größten zählt.

Der Bezirk unterteilt sich in sieben Ortsteile:

Der Ortsteil Charlottenburg umfasst unter anderem die historische Altstadt Charlottenburg mit dem gleichnamigen Schloss und das ehemalige "Zooviertel", heute besser bekannt als "City West" oder zwischen den Weltkriegen auch "Neuer Westen" genannt. Bis heute nimmt die Gegend um den Bahnhof Zoo mit dem Boulevard Kurfürstendamm eine Zentrumsfunktion für große Teile des Berliner Westens ein.

Während des Zweiten Weltkriegs wurde vor der Ostteil der Ortslage stark zerstört, woran bis heute die in Teilen als Ruine erhaltene Kaiser-Wilhelm-Gedächtniskirche erinnert. Wesentlich weniger Zerstörungen erlitten der Norden und Westen Charlottenburg, sodass in gewissen Gegenden, wie etwa in der Altstadt oder dem Danckelmannkiez, ein fast homogenes Vorkriegsstadtbild erhalten blieb.

Nördlich von Charlottenburg liegt der 2004 neu definierte Ortsteil Charlottenburg-Nord, der mit seinen Großwohnsiedlungen im Kontrast zur historischen und zum Teil sehr großzügig angelegten Bausubstanz im Süden liegt. Auch ist hier bereits die industrielle Atmosphäre des Berliner Nordens spürbar. Als Siedlungsgebiet entstand die Gegend erst nach dem Zweiten Weltkrieg als Reaktion auf den allgegenwärtigen Wohnungsmangel.

Ursprünglich ab den 1860er Jahren als reine Villenkolonie für die wachsende Großstadt Charlottenburg geplant, haben heute einige Gegenden in Westend auch das typische (West-)Berliner Ambiente mit einer Mischung aus Mietshäusern und Gewerbe. Anlässlich der Olympischen Sommerspiele 1936 ließ das Nazi-Regime hier groß angelegte Sportstätten errichten, die bis heute für Großveranstaltungen genutzt werden. Eine Besonderheit ist das Corbusierhaus, ein Hochhaus, das vom Architekten Le Corbusier 1957 im Rahmen der Internationalen Bauausstellung "Interbau 1957" errichtet wurde.

Der heutige Ortsteil Wilmersdorf besteht im Wesentlichen aus dem ehemaligen Ortsteilzentrum des ehemaligen Bezirks Wilmersdorf um die sogenannte „Carstenn-Figur“, einer historischen Straßenstruktur mit der Bundesallee (bis 1950: "Kaiserallee") im Zentrum und den vier Eckpunkten Fasanenplatz, Nürnberger Platz, Prager Platz und Nikolsburger Platz. Die Gegend östlich der Bundesallee wurde im Zweiten Weltkrieg stark zerstört, sodass die historische Stadtstruktur dort kaum noch erkennbar ist. Auch die autogerechte Stadtplanung der 1960er und 1970er Jahre trug in diesem Teil der Ortslage stark dazu bei, das geschlossene stadtplanerische Bild der Wilmersdorfer Carstenn-Figur zu zerstören. Westlich der Bundesallee hingegen sind Ensembles wie die Schmuckplätze Fasanenplatz und Nikolsburger Platz gut erhalten.

Die eigentliche Keimzelle der Ortslage, ist die nördlich des Volksparks Wilmersdorf gelegene Wilhelmsaue mit der neogotischen Auenkirche und dem Schoeler-Schlösschen, dem ältesten Gebäude Wilmersdorfs. Der Volkspark entstand 1915 an Stelle des ehemaligen Wilmersdorfer Sees, der ab 1915 trockengelegt wurde.

Südlich angrenzend und ebenfalls zum Ortsteil Wilmersdorf gehörend, befindet sich das Rheingauviertel und die Künstlerkolonie. Das Rheingauviertel wurde kurz vor dem Ersten Weltkrieg als Landhauskolonie im englischen Stil für gehobenes Wohnen fertiggestellt und ist in dieser Gestalt weitestgehend erhalten. Auch die Künstlerkolonie aus den 1920er Jahren, die von der "Interessenvertretung für Künstler und Schriftsteller" für die Kulturschaffenden Berlins errichtet wurde, ist in ihrer Gestalt größtenteils unverändert geblieben.

Benannt nach dem im angrenzenden Grunewald gelegenen Halensee wurde die Gegend des heutigen Ortsteils Halensee als Villen- und Mietshaussiedlung für gehobenes Wohnen geplant. In den 1920er Jahren siedelten sich hier viele russische Emigranten an, die ihre Heimat in Folge der Oktoberrevolution hatten verlassen müssen. Die Zerstörungen im Zweiten Weltkrieg trafen Halensee besonders stark und der anschließende Wiederaufbau veränderten den Charakter des Ortsteils nachhaltig. Es entstanden zunächst vor allem Bauten des Sozialen Wohnungsbaus und später zahlreiche Bürohochhäuser und die Stadtautobahn. Diese Trasse begrenzt Halensee zu den Ortsteilen Grunewald und Schmargendorf. Der Kurfürstendamm teilt den kleinen Ortsteil in zwei Hälften.

Zwischen AVUS und dem Grunewald liegt Schmargendorf, das seinen kleinstädtischen Charakter mit seinem eigenen historischen Ortsteilzentrum bis heute erhalten hat. Besonderer Beliebtheit bei Hochzeitspaaren erfreut sich das Standesamt im historischen Rathaus Schmargendorf. In der Friedrichshaller Straße 23 wohnte Lilly Wust, besser bekannt als "Aimée" aus dem Film und Tatsachenroman "Aimée & Jaguar", der von ihrer Liebe zu der jüdischen Journalistin Felice Schragenheim erzählt. Ein Großteil der Handlung von Film und Buch spielt in ebendieser Wohnung.

Der Ortsteil Grunewald ist benannt nach dem gleichnamigen Forst, der einen Großteil seiner Fläche einnimmt. Seit seiner Entstehung in den 1880er Jahren, zählt er zu wohlhabendsten Gebieten Berlins. Grunewald ist geprägt von herrschaftlicher Villenbebauung, wie beispielsweise dem Palais Mendelssohn, das in den 1960er Jahren mit zeitgenössischen Bauelementen wiederhergestellt wurde und seither von der Johannischen Kirche als St.-Michaels-Heim genutzt wird. Damals wie heute befinden sich viele Botschaften und zahlreiche Botschafter-Residenzen im Ortsteil Grunewald. Eine dunkle Geschichte hat der Bahnhof Grunewald. Von dort erfolgte ab 1941 die Deportation der Berliner Juden, vornehmlich in Konzentrations- und Vernichtungslager in Osteuropa. An diese Verbrechen erinnert seit 1988 das Mahnmal Gleis 17.

Charlottenburg-Wilmersdorf gilt als eher bürgerlicher Bezirk. Per zählte der Bezirk Charlottenburg-Wilmersdorf Einwohner auf einer Fläche von 64,7 Quadratkilometern. Somit lag am Stichtag die Bevölkerungsdichte bei Einwohnern pro Quadratkilometer.

Am 31. Dezember 2016 lag der Ausländeranteil bei 29,2 %, während insgesamt der Anteil der Bevölkerung mit Migrationshintergrund bei 39,6 % lag. Die Arbeitslosenquote bezifferte sich am 30. April 2013 auf 10,7 %. Am 31. Dezember 2016 betrug das Durchschnittsalter der Bevölkerung 45,5 Jahre.

Im Vergleich mit den anderen Berliner Bezirken sind mit etwa 28.000 Unternehmen in Charlottenburg-Wilmersdorf die meisten Unternehmen ansässig.

Im Jahr 2012 waren von den 30.862 in Berlin ansässigen Handwerksbetrieben insgesamt 2761 in Charlottenburg-Wilmersdorf gemeldet.

Die Autobahnen A 100, A 111 und A 115 sowie die Bundesstraßen B 2 und B 5 führen durch den Bezirk Charlottenburg-Wilmersdorf. Bemerkenswert für den Individualverkehr sind in diesem Zusammenhang die beiden folgenden historischen Garagen:


Einige regionale und internationale Radwanderwege verlaufen durch den Bezirk, u. a. der Spreeradweg und der Europaradweg R1. 

Die S-Bahn-Linien S5, S7, S41, S42, S45, S46 und S75 sowie die U-Bahn-Linien U1, U2, U3, U7 und U9 führen durch den Bezirk.

Die Bezirksverordnetenversammlung von Charlottenburg-Wilmersdorf wurde im September 2016 gewählt.

Das Wappen des Bezirks Charlottenburg-Wilmersdorf wurde am 4. September 2001 durch den Senat von Berlin verliehen.

Wappenbeschreibung: Unter silbern-blau gespaltenem Schildhaupt, darin drei Lilien in verwechselten Farben, in Gold ein gezinntes blaues Burgtor mit offenem schwarzem Fallgatter, die Seitentürme mit Kuppeldächern, der Mittelbau mit einem Walmdach, das oben mit zwei abgewendeten Flaggen besteckt ist, die rechte schwarz-silbern, die linke silbern-rot geteilt. Die Seitentürme sind belegt mit je einem Schild: rechts in Silber ein goldengekrönter und -bewehrter schwarzer Adler mit goldenen Kleeblattstängeln auf den Flügeln und auf der Brust belegt mit dem goldenen Monogramm "FR", überhöht von einer goldenen Königskrone; links in Rot ein silbernes springendes Ross. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit dem Berliner Wappenschild belegt ist.

Wappenbegründung: Das Wappen wurde aus den bisherigen eigenen Wappen der Bezirke Charlottenburg und Wilmersdorf erstellt. Das untere Feld des geteilten Schildes zeigt ein schwebendes blaues gezinntes Burgtor mit aufgezogenem schwarzen Fallgatter im Durchgang des Mittelbaus. Dieses Element Charlottenburgs hat einige geschichtliche Details – der vordere Turm symbolisiert das Königreich Preußen mit dem auflegten Schild mit Preußischem Adler und der schwarz-weißen Flagge, während der hintere Turm für das Haus Hannover steht, mit dem springenden Ross und der weiß-roten Flagge. Das Element des Schlosses Charlottenburg wurde um die drei blau-silberfarbenen Lilien angereichert, die aus dem Wappen des Bezirks Wilmersdorf entlehnt wurden – diese Elemente verweisen dabei auf die Gründer von Wilmersdorf aus dem 1802 ausgestorbenen Geschlecht derer von Wilmersdorf. Die Mauerkrone ist das verbindende Element aller Berliner Bezirke.

Der Bezirk Charlottenburg-Wilmersdorf von Berlin hat folgende Partnerschaften:

Die Direktion 2 der Berliner Landespolizei ist für die Bezirke Charlottenburg-Wilmersdorf und Spandau zuständig.


Im Bezirk liegen u. a.:















</doc>
<doc id="14455" url="https://de.wikipedia.org/wiki?curid=14455" title="Bezirk Steglitz-Zehlendorf">
Bezirk Steglitz-Zehlendorf

Steglitz-Zehlendorf ist der sechste Verwaltungsbezirk von Berlin und hatte Einwohner per .

Der im Südwesten Berlins gelegene Bezirk entstand 2001 im Zuge der Berliner Verwaltungsreform durch die Zusammenlegung der früheren Bezirke Steglitz und Zehlendorf. Steglitz-Zehlendorf ist der flächenmäßig drittgrößte Bezirk und gilt als verhältnismäßig wohlhabende Wohngegend mit einer günstigen Sozialstruktur. 

International bekannt ist die Gegend durch ihre renommierte Hochschul- und Forschungslandschaft.

Steglitz-Zehlendorf grenzt im Westen an die brandenburgische Landeshauptstadt Potsdam, ferner an die Berliner Bezirke Spandau (nordwestlich), Charlottenburg-Wilmersdorf (nördlich), sowie Tempelhof-Schöneberg im Osten. Hinter der Landesgrenze im Süden liegen Kleinmachnow und Teltow im Landkreis Potsdam-Mittelmark. In ost-westlicher Richtung erstreckt sich der Bezirk über 19 Kilometer und in nord-südlicher Richtung über 9 Kilometer.

Der westlichste Punkt Berlins befindet sich im Ortsteil Wannsee in der Havel, auf der die Grenze zu Brandenburg verläuft.

Der Bezirk Steglitz-Zehlendorf umfasst ausgedehnte Erholungsgebiete, Villensiedlungen sowie unterschiedliche Wohn- und Gewerbelagen. Im Westen finden sich unter anderem Erholungsgebiete wie der Wannsee, der südliche Grunewald mit seinen Seen Krumme Lanke und Schlachtensee, die besonders im Sommer von vielen Berlinern besucht werden. Der Ortsteil Dahlem ist Sitz zahlreicher wissenschaftlicher Einrichtungen und der Freien Universität. Das Ortsgebiet Zehlendorf hat unterschiedliche Wohnlagen, im Zentrum ein kleines Einzelhandelsgebiet. Das südlich von Dahlem gelegene Lichterfelde ist im nördlichen Teil geprägt von der gründerzeitlichen Villenkolonie Lichterfelde und kleineren Einzelhandelszentren; in Lichterfelde-Süd finden sich teilweise Wohngebiete aus der Zeit nach dem Zweiten Weltkrieg.

Der weiter östlich gelegene Ortsteil Steglitz ist hingegen stärker städtisch strukturiert, wie die Schloßstraße im Ortskern mit der höchsten Ladendichte Berlins zeigt. Sie stellt das Hauptzentrum für die südwestlichen Bezirke der Stadt dar. Botanischer Garten und Botanisches Museum Berlin-Dahlem liegen, entgegen der eigenen Bezeichnung, im Ortsteil Lichterfelde.

Der Bezirk ist sehr unterschiedlich besiedelt. In Steglitz und Lichterfelde, die beide von der Einwohnerzahl her in das oberste Viertel aller 96 Berliner Ortsteile gehören, lebt die Hälfte der Bevölkerung. Wannsee, der größte Ortsteil von Steglitz-Zehlendorf mit fast einem Viertel der Gesamtfläche, ist mit seinen drei Prozent Bevölkerungsanteil sehr dünn besiedelt.

Die vormaligen Bezirke Steglitz und Zehlendorf wurden 1920 bei der Bildung von Groß-Berlin aus zuvor eigenständigen Landgemeinden sowie Gutsbezirken des Landkreises Teltow gebildet. Am 1. Januar 2001 wurden die beiden ehemals eigenständigen Bezirke im Rahmen der Berliner Verwaltungsreform zum Bezirk Steglitz-Zehlendorf zusammengelegt.

Das gesamte Gebiet des heutigen Bezirks gehörte nach dem Zweiten Weltkrieg von 1945 bis 1990 zusammen mit den Bezirken Tempelhof, Schöneberg, Neukölln und Kreuzberg zum Amerikanischen Sektor von Berlin, vgl. auch → Geteilte Stadt (dort hellblauer Bereich).

Per zählte der Bezirk Steglitz-Zehlendorf Einwohner. Bedingt durch den Anteil von Wasser- und Waldflächen an der Gesamtfläche von knapp 103 Quadratkilometern sowie der in der Mehrzahl aufgelockerten Bebauung lag somit am Stichtag die durchschnittliche Bevölkerungsdichte bei Einwohnern pro Quadratkilometer. Siehe hierzu auch: Liste der Bezirke und Ortsteile Berlins. 

Steglitz-Zehlendorf ist gekennzeichnet durch ein relativ hohes Haushaltseinkommen der Bewohner sowie eine vergleichsweise niedrige Arbeitslosenquote mit 10,1 % (Stand: 30. April 2013). Das Durchschnittsalter der Bevölkerung wies mit 46,2 Jahren (Stand: 31. Dezember 2016) den höchsten Wert aller Berliner Bezirke auf, womit Steglitz-Zehlendorf der Bezirk mit den im Durchschnitt ältesten Einwohnern ist (zum Vergleich: Der Bezirk Friedrichshain-Kreuzberg hat mit 37,8 Jahren die jüngste Bevölkerung).

Bevölkerungsgruppen

Der Ausländeranteil lag am 31. Dezember 2016 bei 13,5 %, während der Anteil der Einwohner mit Migrationshintergrund bei 25,4 % lag. Allerdings variiert der Anteil zwischen den einzelnen Ortsteilen. An sieben der 33 öffentlichen Grundschulen des Bezirkes liegt der Anteil der Kinder mit Migrationshintergrund bei über 40 %. Im städtisch geprägten Ortsteil Steglitz haben fast 30 % der Einwohner einen Migrationshintergrund, im dünn besiedelten Wannsee dagegen rund 17 %. Die größten Migrantengruppen von Steglitz-Zehlendorf setzen sich aus Polen, Türken, Jugoslawen, US-Amerikanern, Arabern und diversen afrikanischen und asiatischen Ethnien zusammen. Bei letzteren sind vor allem Menschen aus Ghana und Nigeria, bzw. aus China, Südkorea und Thailand vertreten.

Ein wichtiges Geschäfts- und Einkaufszentrum für den südwestlichen Bezirk Steglitz-Zehlendorf ist die Steglitzer Schloßstraße. Mit über 200.000 m² Verkaufsfläche ist sie einer der größten Einzelhandelsstandorte Berlins. Neben den vier großen Einkaufszentren "Forum Steglitz", "Das Schloss", "Schloss-Straßen-Center" und "Boulevard Berlin" gibt es dort zahlreiche, zum Teil mehrgeschossige Ladengeschäfte. In den Seitenstraßen befinden sich eine Vielzahl von gastronomischen Einrichtungen.

Das größte Gewerbegebiet des Bezirks umfasst etwa 77 ha und befindet sich rund um Goerzallee, Beeskowdamm und Am Stichkanal. Über 280 Unternehmen haben dort ihren Sitz und beschäftigen etwa 3500 Menschen.

Im Jahr 2015 waren von den 30.015 in Berlin vertretenen Handwerksfirmen insgesamt 2.276 im Bezirk Steglitz-Zehlendorf gemeldet.

Zu den größten Arbeitgebern im Bezirk zählen u. a. das in Berlin gegründete Unternehmen Robert Lindner. Im gesamten Stadtgebiet sind Feinkostläden des Lebensmitteleinzelhändlers präsent. Weitere große Arbeitgeber im Bezirk sind die Immanuel Diakonie, das Diakoniewerk Bethel und 3B Dienstleistung Deutschland.

Die Autobahnen A 100, A 103, A 115 und die Bundesstraße 1 führen durch den Bezirk Steglitz-Zehlendorf. Die Glienicker Brücke ist Teil der B 1 und verbindet Berlin mit Potsdam.

Durch den Bezirk führen die S-Bahn-Linien S1, S2, S25 und S7 sowie die U-Bahn-Linien U3 und U9. Ferner verkehren zahlreiche Buslinien einschließlich Nachtlinien im Bezirk.


Das Heizkraftwerk Lichterfelde wurde 1972 erstmals in Betrieb genommen und arbeitet nach dem Prinzip der Kraft-Wärme-Kopplung. Das heutige Gaskraftwerk mit seinen 158 Meter hohen Kaminen verfügt über drei Blöcke und hat eine elektrische Gesamtleistung von 450 MW sowie eine thermische Leistung von 720 MW.

Seit 2006 bilden CDU und Grüne eine Zählgemeinschaft in der Bezirksverordnetenversammlung (BVV).

Hier werden die Bürgermeister des Bezirks Steglitz-Zehlendorf seit 2001 aufgeführt. Die Bürgermeister der ehemaligen Bezirke Steglitz und Zehlendorf finden sich dort.


Das Wappen des Bezirks Steglitz-Zehlendorf orientiert sich an den Wappen der namensgebenden Ortsteile Steglitz und Zehlendorf. Die Wappen der übrigen Ortsteile bleiben unberücksichtigt. Dies entspricht dem Muster der Wappenbildung in den meisten Berliner Bezirken. Auch schon die von 1920 bis 2000 bestehenden ehemaligen Verwaltungsbezirke Steglitz und Zehlendorf hatten jeweils nur die Wappen der beiden alten preußischen Dörfer verwandt, ergänzt um die allen Berliner Wappen gemeinsame Mauerkrone als Sinnbild des Stadtverbands. Das heutige Bezirkswappen wurde am 25. März 2003 durch den Senat von Berlin gestiftet.

Blasonierung: In Gold eine grüne Kiefer mit schwarzem Stamm auf grünem Boden, darunter ein silberner Wellenschildfuß, darüber im Schildhaupt ein schwebender rotbewehrter und -gezungter schwarzer Adlerkopf. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit dem Berliner Wappenschild belegt ist.

Wappenbegründung: Die Kiefer und der Wellenschildfuß (Wassersymbolik) sind dem Zehlendorfer Wappen (seit Wappenentwurf von 1907) entnommen. Sie versinnbildlichen den Wald- und Wasserreichtum des Bezirks Zehlendorfs. Das Adlermotiv ist dem Steglitzer Wappen (Bestandteil im Wappen um 1887 als dem Ort Steglitz unüberlicherweise ein eigenen Wappen zuerkannt wurde) entnommen. Das Adlermotiv ist auf den Wappenverleiher Kaiser Wilhelm I. zurückzuführen. Die Mauerkrone ist das verbindende Element aller Berliner Bezirke.

Der Bezirk Steglitz-Zehlendorf pflegt folgende Städtepartnerschaften:
Darüber hinaus ist der Bezirk Patenschaften mit der "2. Kompanie des Wachbataillons beim Bundesministerium der Verteidigung" und der "Reservistenkameradschaft Berlin-Südwest „Flakregiment 12“" eingegangen.

Die Direktion 4 der Berliner Landespolizei mit Dienstsitz im Ortsteil Lankwitz ist für die Bezirke Steglitz-Zehlendorf und Tempelhof-Schöneberg zuständig.

In Berlin haben sich nach dem Jahr 2000 eine Vielzahl von diplomatischen Vertretungen niedergelassen. Im Bezirk Steglitz-Zehlendorf haben die Botschaft des Königreichs Thailand in der Lepsiusstraße 64, die Botschaft der Republik Tschad in der Lepsiusstraße 114 und die Botschaft der Republik Sierra Leone in der Herwarthstraße 4 ihren Sitz.


Der Berliner HC war mehrfacher Deutscher Meister im Damen- und Herrenfeldhockey. Der Bundesligaclub trägt seine Heimspiele in erster Linie im Ernst-Reuter-Stadion in Zehlendorf aus. Unter den erfolgreichen Spielern der Hockeyabteilung sind einige Mitglieder der Familie Keller. Erwin Keller gewann eine Silbermedaille bei den Olympischen Spielen. Carsten, Andreas und Natascha Keller konnten bei Olympischen Spielen je eine Goldmedaille erringen.

Verschiedene Jugendmannschaften der Basketballabteilung des TuS Lichterfelde Berlin waren Berliner und Deutscher Meister.

Der Fußballverein FC Viktoria 1889 Berlin ist mit rund 1600 spielenden Mitgliedern in 65 verschiedenen Teams einer der größten aktiven Fußballvereine in Berlin.

Eine beliebte Strecke für Radsport und Inlineskaten ist der Kronprinzessinnenweg im Grunewald. Der im Wald gelegene asphaltierte Weg ist insgesamt vier Kilometer lang.

Als ältester Golfclub in Deutschland gilt der 1895 gegründete Golf- und Land-Club Berlin-Wannsee. Aktuell sind rund 1800 eingetragene Mitglieder Teil des Clubs (Stand: 2017). Auf einem 1913/1914 errichteten 18-Loch-Golfplatz am Schäferberg sowie auf einer 1926 angelegten 9-Loch-Erweiterung werden die Golfpartien gespielt.





</doc>
<doc id="14456" url="https://de.wikipedia.org/wiki?curid=14456" title="Bezirk Treptow-Köpenick">
Bezirk Treptow-Köpenick

Treptow-Köpenick ist der neunte Verwaltungsbezirk von Berlin und hatte Einwohner per .

Der Bezirk entstand nach der Verwaltungsreform 2001 durch die Zusammenlegung der zuvor eigenständigen Bezirke Treptow und Köpenick.

Das im Südosten gelegene Treptow-Köpenick ist der flächengrößte der zwölf Berliner Bezirke und weist eine verhältnismäßig günstige Sozialstruktur auf. Berlins größter See, der Müggelsee, liegt in der Gegend.

Internationale Bedeutung hat der Standort durch sein Innovations- und Technologiezentrum in Adlershof.

Der Bezirk Treptow-Köpenick liegt im Südosten von Berlin. Es ist der östlichste und zugleich südlichste Bezirk der Stadt. Der östlichste Ortsteil ist Berlin-Rahnsdorf, der südlichste Berlin-Schmöckwitz.
Treptow-Köpenick grenzt im Norden an die Bezirke Friedrichshain-Kreuzberg, Lichtenberg und Marzahn-Hellersdorf. Im Westen grenzt es an Neukölln. Im Süden und Osten teilt sich der Bezirk eine Grenze mit dem Bundesland Brandenburg.

Etwa 70 % des Areals bestehen aus Wasser wie dem Müggelsee und dem Zeuthener See, Wald, Parks, Naturschutz- und Landschaftsschutzgebieten. Der Bezirk verfügt mit 12,9 % über die größte Wasserfläche und mit 41,5 % über die größte Waldfläche aller Berliner Bezirke. 36,4 % der gesamten Wasserfläche und 42,9 % der totalen Waldfläche Berlins entfallen auf diesen Bezirk. Des Weiteren liegt in diesem Bezirk Köpenick, der größte Ortsteil Berlins, mit einer Fläche von 34,92 km².

Von der Bezirksverwaltung wird auch eine Einteilung nach Bezirksregionen gewählt und hat jeweils Kurzprofile bereitgestellt.

Bereits zur Zeit der Slawen, die der Stadt mit "Copnic" (Inselort) den Namen gaben, entstanden am Zusammenfluss von Dahme und Spree Burgen und Siedlungen. Im Jahr 1209 findet sich die erste Erwähnung in Dokumenten unter dem Namen "Copenic." Als Hauptburg und Hauptansiedlung des slawischen Stammes der Sprewanen unter ihrem Fürsten Jaxa von Köpenick (* vor 1125; † Februar 1176) wurde es zum Besiedlungszentrum des Köpenicker Raums.

Mit der Bildung Groß-Berlins entstand im Grunde nördlich der Spree der Verwaltungsbezirk Cöpenick aus den Orten: Bohnsdorf, Carolinenhof, Cöpenick, Friedrichshagen, Grünau, Hessenwinkel, Hirschgarten, Müggelheim, Rahnsdorf, Rauchfangswerder, Schmöckwitz, Wilhelmshagen. Vorrangig südlich der Spree wurde der Verwaltungsbezirk Treptow aus Adlershof, Alt-Glienicke, Johannisthal, Niederschöneweide, Oberschöneweide, Treptow, Wuhlheide gebildet. Die Ortslagen und Ortsteile von 1920 wurden, insbesondere 1938, umgeordnet.

In der Nachkriegszeit gehörte das Gebiet der Bezirke Treptow und Köpenick bis zur deutschen Wiedervereinigung zu Ost-Berlin.

Die bis zur Verwaltungsreform 2001 eigenständigen Bezirke Treptow und Köpenick wurden vereint. Sie nehmen seitdem fast 19 % der gesamten Stadtfläche Berlins von rund 892 km² ein. Damit ist Treptow-Köpenick der flächenmäßig größte Berliner Bezirk.

Nach dem Bezirk Spandau hat Treptow-Köpenick die geringste Einwohnerzahl aller Berliner Bezirke. Mit Stand betrug diese Personen.

Bedingt durch den hohen Anteil von Wasser- und Waldflächen an der Gesamtfläche von 168,4 Quadratkilometern, lag somit am Stichtag die durchschnittliche Bevölkerungsdichte bei Einwohnern pro Quadratkilometer und ist damit der niedrigste Wert aller Berliner Bezirke. Siehe hierzu auch: Liste der Bezirke und Ortsteile Berlins.

Am 31. Dezember 2016 lag der Ausländeranteil bei 7,7 %, während der Anteil der Bevölkerung mit Migrationshintergrund bei 12,3 % lag. Die Arbeitslosenquote bezifferte sich am 30. April 2013 auf 10,1 %. Am 31. Dezember 2016 betrug das Durchschnittsalter der Bevölkerung 45,0 Jahre.
In Treptow-Köpenick gab es 2014 rund 134.000 Wohnungen (Berlin: 1.892.000). Das mittlere monatliche Haushaltsnettoeinkommen lag im selben Jahr bei etwa 1825 Euro (Berlin: 1750 Euro).

Im Jahr 2016 wurden in Berlin die meisten Wohnungen im Bezirk Treptow-Köpenick gebaut. Insgesamt wurden rund 2.000 Wohneinheiten (Berlin: 13.842) neu errichtet.

Im Ortsteil Alt-Treptow steht im Treptowers-Gebäudekomplex das höchste Berliner Bürohaus mit 31 Etagen.

Der Wissenschafts- und Wirtschaftsstandort Adlershof, auch WISTA genannt, liegt im Ortsteil Adlershof und gilt mit rund 1000 Unternehmen (Stand: Ende 2016) und etwa 15.000 Mitarbeitern als einer der größten Technologieparks Deutschlands. In den Studios des WISTA Geländes wird u. a. die Fernsehsendung "Anne Will" der ARD produziert. Bis 2017 wurden hier Sendungen der Unterhaltungsshow "Circus HalliGalli" aufgezeichnet.


Die Autobahnen A 113 und A 117 sowie die B 96a führen durch den Bezirk Treptow-Köpenick. Das Adlergestell (11,9 Kilometer) ist Berlins längste Straße.

Die S-Bahn-Linien S3, S41, S42, S45, S46, S47, S8, S85 und S9 erschließen den Bezirk für den ÖPNV.

Das Blockheizkraftwerk Berlin-Köpenick ist ein Blockheizkraftwerk nach dem Prinzip der Kraft-Wärme-Kopplung im Ortsteil Köpenick. Das Kraftwerk erbringt eine elektrische Leistung von 10 MW sowie eine thermische Leistung von 50 MW.

In der Bezirksverordnetenversammlung (BVV) von Treptow-Köpenick saßen nach der Wahl 2011 sechs Parteien. Stärkste Kraft wurde die SPD (29,7 %), gefolgt von der Linken (23,8 %), der CDU (15,3 %), Grünen (10,9 %) und der Piraten (9,2 %).

Bezirksbürgermeister ist gegenwärtig (Stand: 2016) der SPD-Politiker Oliver Igel, sein Stellvertreter Gernot Klemm (Die Linke). Neben dem Bezirksbürgermeister stellt die SPD im Bezirksamt einen weiteren Stadtrat, AfD und CDU stellen je einen Stadtrat.

Der Bezirk pflegt Beziehungen zu Städten und Kommunen in Deutschland, Europa und auf dem amerikanischen Kontinent. Einige der Partnerschaften pflegten bereits die früheren Einzelbezirke Treptow und Köpenick und werden vom fusionierten Bezirk Treptow-Köpenick fortgesetzt. Neben den offiziellen Partnerstädten und -kommunen kooperiert der Bezirk im Rahmen ganz konkreter Projekte mit weiteren europäischen Städten.

Das heutige Wappen wurde am 21. September 2004 durch den Senat von Berlin verliehen und ist aus den vorherigen Wappen der Ortsteile Treptow und Köpenick hervorgegangen und führte die verschiedenen stilistischen Elemente zusammen.

Blasonierung: In Blau ein grünes Taukreuz mit getatzten Enden, versehen mit einem goldenen Stabbord, begleitet von zwei nach innen gewendeten aufrecht stehenden silbernen Fischen. Das Taukreuz belegt mit einem goldenen Schlüssel mit linksgewendetem Bart und 7 vierstrahligen Sternen (4:2:1). Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit dem Berliner Wappenschild belegt ist.

Wappenbegründung: Das Wappen des Bezirks Treptow-Köpenick wurde aus den Elementen der vorher bestehenden Wappen der Bezirke Treptow und Köpenick erstellt. Das Wappen Köpenicks übte hierbei einen größeren Einfluss aus, da es deutlich älter ist als das Wappen Treptows. Das Wappen der Stadt Köpenick ist bereits im 14. Jahrhundert belegt und zeigte schon in jener Zeit einen Schlüssel und zwei silberne Fische auf blauem Schild als Verweis auf den hl. Petrus den Schutzpatron der Fischer, sowie sieben Sterne. Auf den Bezirk Treptow verweist eine T-förmige Schildteilung mit grüner Tingierung, der die vorherrschende Farbe des Treptower Wappens und deren Darstellung von Bäumen aufgreift und dabei auf den Waldreichtum hinweist. Bei der Gründung des Bezirks im Rahmen der Bezirksreform 2001 wurden zuerst beide alten Wappen der verheirateten Bezirke gemeinsam hoheitlich geführt – im Rahmen der Wappenfindung wurde drei Jahre später der Entwurf des Heraldikers Frank Diemar als einheitliches Wappen angenommen. Die Mauerkrone ist das verbindende Element aller Berliner Bezirke.

Der Dienststab der Bundespolizeidirektion Berlin hat seinen Sitz in Niederschöneweide. Er ist für die Bundesländer Berlin und Brandenburg zuständig und mit rund 3700 Mitarbeitern besetzt (Stand: 2017). Der Stab gliedert sich in die drei Bereiche "Einsatz", "Polizeitechnik / Materialmanagement" und "Verwaltung" sowie in vier Stabsstellen "(Innenrevision", "Controlling / Qualitätsmanagement", "Projektgruppe BER" und "Öffentlichkeitsarbeit / Beschwerdemanagement)". Außerdem sind dem Stab zwölf Inspektionen nachgeordnet.

Für den Bezirk sind die Abschnitte 65 (Treptow) und 66 (Köpenick) örtlich zuständig und nehmen sämtliche landespolizeilichen Aufgaben wahr.




Der Fußballverein 1. FC Union Berlin wurde 1966 gegründet und hat heute über 16.000 eingetragene Mitglieder (Stand: 2017). Die Heimspiele des Klubs werden im Stadion An der Alten Försterei in Köpenick ausgetragen.

Die Regattastrecke Berlin-Grünau ist eine der traditionsreichen Wassersportgebiete in Berlin. Wettkämpfe im Rudern, Kanurennsport, Kanupolo und Drachenbootfahren werden hier ausgetragen. Die ADAC-Motorboot Masters, eine Rennserie für Motorboote, werden ebenfalls in Grünau abgehalten.

Der Mellowpark zählt zu den größten Skateboard- und BMX-Parks in Europa. Beachvolleyballfelder, ein Bolzplatz und Basketballplätze gehören auch zu dem Freizeitgelände des Parks.

Der Berlin-Triathlon, auch Hauptstadttriathlon genannt, findet jedes Jahr Anfang Juni in Treptow statt.

Jochen Schümann, geboren in Köpenick, wurde der erfolgreichste deutsche Olympionike im Segelsport. Seine Laufbahn begann er mit einem selbstgebauten Optimisten auf dem Müggelsee und als Mitglied des Yachtclubs Berlin-Grünau. Er gewann drei Gold- und eine Silbermedaille bei vier Olympischen Spielen.








</doc>
<doc id="14457" url="https://de.wikipedia.org/wiki?curid=14457" title="Bezirk Marzahn-Hellersdorf">
Bezirk Marzahn-Hellersdorf

Marzahn-Hellersdorf ist der zehnte Verwaltungsbezirk von Berlin. Am hatte er Einwohner. Er entstand 2001 durch die Fusion der Bezirke Marzahn und Hellersdorf.

Der Bezirk liegt im Nordosten Berlins und grenzt im Westen an den Bezirk Lichtenberg, im Süden an den Bezirk Treptow-Köpenick sowie im Norden an den Landkreis Barnim und im Osten an den Landkreis Märkisch-Oderland des Landes Brandenburg.
Die Anhöhen des Bezirks entstanden in der Eiszeit und wurden durch Ablagerung von Schutt und durch den Aushub beim Bau von Wohnhäusern erhöht und verdichtet. Die Ahrensfelder Berge sind mit 112 und 101 Meter die höchsten Erhebungen im Bezirk. Der Kienberg (102 Meter) und die Biesdorfer Höhe (82 Meter) sind weitere Anhöhen in Marzahn-Hellersdorf.


Der Bezirk besteht aus fünf Ortsteilen. Die Ortsteile Hellersdorf, Kaulsdorf und Mahlsdorf bildeten von 1986 bis 2001 den eigenständigen Berliner Bezirk Hellersdorf. Die Ortsteile Biesdorf und Marzahn bildeten den Bezirk Marzahn. Der Ortsteil Marzahn setzt sich aus der Flur des Dorfes Marzahn und dem nördlichen Teil der Flur Friedrichsfelde zusammen.

Alle fünf Ortsteile, aus denen der Bezirk besteht, stammen ursprünglich aus dem Landkreis Niederbarnim und wurden 1920 durch das Groß-Berlin-Gesetz nach Berlin eingemeindet. Zusammen mit den Ortsteilen Lichtenberg und Friedrichsfelde bildeten sie bis 1979 den Bezirk Lichtenberg. Durch den Aufbau des Neubaugebietes Marzahn wuchs Ende der 1970er Jahre vor allem der Ortsteil Marzahn, sodass 1979 aus den fünf aktuell den Bezirk bildenden Ortsteilen und dem nördlichen Teil von Friedrichsfelde (Gebiet westlich des Eisenbahnringes, umgangssprachlich nach der S-Bahn-Station Friedrichsfelde-Ost genannt) der Bezirk Marzahn gebildet wurde.

Nachdem die Einwohnerzahl – bedingt durch die Entstehung der Neubaugebiete in Hellersdorf und Kaulsdorf – weiter gestiegen war, wurde am 1. Juni 1986 aus den Ortsteilen Hellersdorf, Kaulsdorf und Mahlsdorf der Bezirk Hellersdorf gegründet, der bis zur Bezirksreform 2001 eigenständig blieb.

Im Jahr 1997 wurde das neugebaute Stadtzentrum "Helle Mitte" fertiggestellt, das seitdem das urbane Zentrum von Hellersdorf bildet.

In den 2000er Jahren kam es zu einem starken Zuzug in die Einfamilienhausgebiete Biesdorf, Kaulsdorf und Mahlsdorf, während im nördlichen Teil des Bezirks die Abwanderung geringer wurde.

Zum 1. Januar 2001 wurde der Bezirk Marzahn-Hellersdorf im Rahmen der Berliner Bezirksreform gegründet.

Am 25. Mai 2009 erhielt der Bezirk den von der Bundesregierung verliehenen Titel "Ort der Vielfalt".

Am 30. Juni 2016 zählte der Bezirk Marzahn-Hellersdorf Einwohner auf einer Fläche von 61,8 Quadratkilometern. Somit lag am Stichtag die Bevölkerungsdichte bei  Einwohnern pro Quadratkilometer.

Am 31. Dezember 2016 lag der Ausländeranteil bei 8,3 %, während der Anteil der Bevölkerung mit Migrationshintergrund bei 16,2 % lag. Schätzungsweise leben rund 30.000 Russlanddeutsche in dem Bezirk. Die Arbeitslosenquote bezifferte sich am 30. April 2013 auf 11,5 %.

In Marzahn-Hellersdorf hat es hinsichtlich der Demografie etliche Veränderungen gegeben. 1991 betrug das Durchschnittsalter der beiden Bezirke 30,5 Jahre. Es erhöhte sich bis 2009 um knapp zwölf Jahre auf 42,4 Jahre. Am 31. Dezember 2016 betrug das Durchschnittsalter 43,6 Jahre.

Die Zahlen (Stand jeweils 31. Dezember) basieren, abweichend von der Bevölkerungsfortschreibung des Amtes für Statistik Berlin-Brandenburg, auf Daten des Einwohnermelderegisters des Berliner Landesamtes für Bürger- und Ordnungsangelegenheiten.

Der Marzahn-Hellersdorfer Wirtschaftskreis e. V. (MHWK) wurde 1991 von Marzahner Unternehmen unter dem Motto „Gemeinsam zum Erfolg“ gegründet. Dieser Verband hat es sich zur Aufgabe gemacht, die Interessen Marzahn-Hellersdorfer Unternehmen und Angehöriger freier Berufe gegenüber Entscheidungsträgern in Politik und Wirtschaft wahrzunehmen. Durch Informationen und Beratung bietet der MHWK außerdem seinen Mitgliedern Unterstützung bei ihren Aufgaben als Arbeitgeber, Unternehmer oder Freiberufler.

Im Jahr 2012 waren von den 30.862 in Berlin ansässigen Handwerksbetrieben insgesamt 2.547 in Marzahn-Hellersdorf gemeldet. Das waren 47 Betriebe mehr als im Jahr zuvor.

Marzahn-Hellersdorf verfügt mit dem Berlin eastside über das größte zusammenhängende Gewerbegebiet der Stadt. Die wichtigsten (größeren) Unternehmen sind:

Durch Marzahn-Hellersdorf führen die S-Bahn-Linien S5, S7, S75 sowie die U-Bahn-Linie U5. Des Weiteren führen mehrere Straßenbahn- und Buslinien durch den Bezirk.

Über die Bundesstraßen B 1 / B 5 und B 158 sowie über die Landsberger Allee/Landsberger Chaussee hat der Bezirk eine Anbindung an die Bundesautobahn 10 (Berliner Ring) mit den Anschlussstellen "Berlin-Hohenschönhausen", "Berlin-Marzahn" und "Berlin-Hellersdorf".

Ein Luftrettungs­zentrum ist am Unfallkrankenhaus Berlin in Marzahn eingerichtet und wird von der DRF Luftrettung betrieben. Auf dem Klinikgelände ist der Intensivtransporthubschrauber "Christoph Berlin" stationiert, der im Rettungsdienst der Berliner Feuerwehr für Primäreinsätze (Notfallrettung) als auch für Sekundäreinsätze (Intensiv- und Verlegungsflüge) eingesetzt wird. Ein Hubschrauberlandeplatz ist Teil des Klinikareals.

Bei der Wahl zur Bezirksverordnetenversammlung (BVV) 2001 kam die PDS (heute: Linkspartei) auf 51,1 % der gültigen Stimmen. Bei der Wahl 2016 erreichte sie nur noch 26,0 %, blieb aber stärkste Partei.

Die BVV tritt in der Regel einmal im Monat zusammen. Die Tagung ist frei zugänglich, und Bürgerinnen und Bürger können ihre Beiträge mit in die BVV einbringen.

Erster Bürgermeister des neuen Bezirks war von 2001 bis 2006 Uwe Klett (PDS). Seine Nachfolgerin Dagmar Pohle (Die Linke) amtierte von 2006 bis 2011. Nach der Wahl zur BVV 2011 wurde Stefan Komoß (SPD) Bürgermeister von Marzahn-Hellersdorf. Bezirksstadträte waren Dagmar Pohle (Die Linke), Christian Gräff (CDU), Stephan Richter (SPD) und Juliane Witt (Die Linke).

Im Jahr 2016 wurde Dagmar Pohle erneut Bezirksbürgermeisterin. Bezirksstadträte sind Gordon Lemm (SPD), Johannes Martin (CDU), Juliane Witt (Die Linke) und Thomas Braun (AfD).

Das Wappen des Bezirks Marzahn-Hellersdorf greift auf die Gestaltungselemente des Wappens des alten Bezirks Marzahn zurück. Der Bezirk Hellersdorf mit seinen Ortsteilen war vor seiner Gründung Bestandteil des Bezirks Marzahn. Dennoch wollte die Bezirksverordnetenversammlung nicht das alte Wappen von Marzahn für den neuen Bezirk übernehmen. Das Bezirksamt bildete eine Wappenkommission, die den Heraldiker Lutz Döring mit dem Entwurf des neuen Wappens beauftragte. Das heutige Wappen wurde am 7. Oktober 2003 durch den Senat von Berlin verliehen.

Wappenbeschreibung: In grünem Schild ein schräglinker silberner Wellenbalken, darüber eine goldene Korngarbe mit fünf Ähren, darunter ein silbernes Zahnrad. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit dem Berliner Wappenschild belegt ist.

Wappenbegründung: Die Korngarbe mit Ähren symbolisiert die jahrhundertelange landwirtschaftliche Prägung aller Ortsteile des Bezirkes, und die fünf Ähren verweisen auf die fünf Ortsteile Biesdorf, Hellersdorf, Kaulsdorf, Mahlsdorf und Marzahn, aus denen der Bezirk besteht. Der Wellenbalken steht für die Wuhle, die den Bezirk von Nord nach Süd durchfließt, und die vielen Gewässer des Bezirks. Die grüne Tingierung des Schildes steht für die großzügigen Grünflächen und Parks. Das Zahnrad symbolisiert wie im alten Wappen von Marzahn die starke Industrialisierung des Bezirks im 20. Jahrhundert. Die Mauerkrone ist das verbindende Element aller Berliner Bezirke.

Der Bezirk unterhält folgende Städtepartnerschaften:

, IV. Bezirk (Újpest) und XV. Bezirk (Rákospalota), Ungarn, seit 9. November 1991 
, Stadtbezirk Hoàng Mai, Vietnam, seit 2. Dezember 2013 
, Deutschland, seit 10. Juni 1999 
, Stadtbezirke Partizan und Oktjabr, Weißrussland, seit 26. Mai 1993 

Die Direktion 6 der Berliner Landespolizei ist für die Bezirke Marzahn-Hellersdorf, Treptow-Köpenick und Lichtenberg zuständig. Der Direktionsleiter der Direktion 5 ist Michael Lengwenings (Stand: 2017).

Im Jahr 2012 gab es im Bezirk Marzahn-Hellersdorf 30 Grundschulen, zwölf Integrierte Sekundarschulen, fünf Gymnasien und zwei Oberstufenzentren. Seit 2008 existiert im Bezirk die private Freie Schule am Elsengrund in Mahlsdorf. Am Victor-Klemperer-Kolleg können Erwachsene in Vollzeit ihr Abitur ablegen.



Die Alice-Salomon-Hochschule Berlin (ASH) ist eine Fachhochschule in Berlin-Hellersdorf mit den Schwerpunkten Soziale Arbeit sowie Gesundheits- und Pflegemanagement.

In Marzahn-Hellersdorf existiert eine große Zahl von Sportvereinen und Sportanlagen. Überregionale Bedeutung besitzt dabei der Athletik-Club Berlin (verschiedene deutsche und Berliner Meister im Bereich Leichtathletik).

Der East Side Beach hat insgesamt fünf Plätze, die für Beachvolleyball und andere Strandsportarten zur Verfügung stehen. Hier fand 2011 die Deutsche Meisterschaft der U20-Mannschaften im Beachvolleyball statt.

Eine weitere Anlage ist der „Hellersdorfer Walkout“ am Cecilienplatz. Hier können Hobbysportler zwischen zwei Routen wählen und sportliche Aufgaben erfüllen, die überall in der näheren Umgebung auf kleinen blau-weißen Schildern verteilt sind. Die „lässige Route“ hat eine Länge von rund 300 Metern und hat acht Stopps, während die „flotte Route“ etwa 1100 Meter lang ist und vierzehn Stopps hat.

Wanderrouten wie der Wuhletal-Wanderweg, der Wuhle-Hönow-Weg, die Route Am Barnimhang, und Neue Urbanität sind für vielseitige sportliche Freizeitaktivitäten eingerichtet.


Das ORWOhaus in der Frank-Zappa-Straße ist eine Anlaufstelle für Musiker und Künstler in Berlin. Es bietet auf rund 4000 m² Proberäume (rund um die Uhr bespielbar), Treffpunkte, Tonstudios, musiknahe Dienstleistungen und gemeinsame Aktivitäten wie Konzerte und Veranstaltungen. Die Hallen im Erdgeschoss wurden ab dem Jahr 2011 zu Veranstaltungsräumen für Konzerte und Tour-Vorbereitungen ausgebaut. Jedes Jahr im Juli findet dort das ORWOHAUS-Festival statt.

Anfang 2005 wurde im Bezirk das Jugendsinfonieorchester Marzahn-Hellersdorf gegründet, in dem Schüler aus der Musikschule Marzahn-Hellersdorf und Schüler aus anderen Musikschulen spielen.

Im Park des Schlosses Biesdorf befindet sich die Parkbühne mit rund 5000 Plätzen. Sie wird häufig für Konzertveranstaltungen, Parkfeste und Schultreffen genutzt.

Ilka Bessin, eine Komikerin aus dem brandenburgischen Luckenwalde, ist mit ihrem Künstlernamen "Cindy aus Marzahn" deutschlandweit bekannt geworden.



</doc>
<doc id="14458" url="https://de.wikipedia.org/wiki?curid=14458" title="Bezirk Lichtenberg">
Bezirk Lichtenberg

Lichtenberg ist der elfte Verwaltungsbezirk von Berlin. Am hatte er Einwohner. Er entstand 2001 durch die Fusion der bis dahin eigenständigen Bezirke Lichtenberg und Hohenschönhausen.

Alle Ortsteile des heutigen Bezirks gehören seit der Gründung von Groß-Berlin im Jahr 1920 zum Berliner Stadtgebiet, darunter der namensgebende Ortsteil Lichtenberg.

– Ausgewählte Beispiele – 

Der Landschaftspark Herzberge ist ein Landschaftsschutzgebiet im Bezirk Lichtenberg. Die Parkanlage wird landwirtschaftlich genutzt durch die Beweidung der Flächen mit Rauwolligen Pommerschen Landschafen. 

Der Stadtpark Lichtenberg, im 18. Jahrhundert von General Möllendorff mit seltenen und exotischen Pflanzen gestaltet, gehört seit 1908 der Gemeinde Lichtenberg. Für seine Besucher gibt es einen Rodelhang, Sportplätze, ein Planschbecken, Liegewiesen. 

Der Rummelsburger See ist eine Spreebucht mit einer Länge von 1,6 km. Historisch waren am Ufer zahlreiche Industrieunternehmen angesiedelt, wodurch der See noch immer (Stand in den 2010er Jahren) stark kontaminiert ist und nicht zum Baden genutzt wird. 

Der Fennpfuhlpark, etwa 12,5 ha groß, wurde Anfang der 1970er Jahre zusammen mit dem Bau der umliegenden Großwohnsiedlung geplant und angelegt. Die zwei etwa 20.000 Jahre alten Pfühle sind Relikte aus der Eiszeit und wurden bei einer Sanierung zwischen 1978 und 1981 durch einen Kanal verbunden. 

Auf der Grenze zwischen den Berliner Ortsteilen Weißensee und Alt-Hohenschönhausen befinden sich Fauler See, Orankesee und Obersee mit sie umgebenden Parks. 1933 wurde der Faule See als Naturschutzgebiet gesichert. Durch die Bildung einer großen Faulschlammschicht verlandet der Faule See allmählich. Er ist Brut- und Raststätte für seltene Wasservogelarten und hier finden sich Röhrichte und typische Erlen- und Weidenbestände.

Die bedeutendste und flächenmäßig größte Naturanlage im Bezirk ist der Tierpark Berlin, der 1955 eröffnet wurde. Er entstand aus dem früheren Friedrichsfelder Schlosspark. 
Der Bezirk besteht aus zehn Ortsteilen. Die heutigen Ortsteile Lichtenberg, Friedrichsfelde, Karlshorst, Rummelsburg und Fennpfuhl gehörten bis zur Bezirksfusion 2001 zum damals eigenständigen Stadtbezirk Lichtenberg. Die heutigen Ortsteile Alt-Hohenschönhausen, Neu-Hohenschönhausen, Wartenberg, Falkenberg und Malchow bildeten von 1985 bis 2001 den eigenständigen Berliner Bezirk Hohenschönhausen.

Innerhalb der amtlichen Ortsteile werden durch Tradition auch einzelne Gebiete unterschieden. Diese sind unter dem Ortsteilnamen gesondert angegeben.

Die kleinräumige Gliederung für Berlin sind die Lebensweltlich orientierten Räume (LOR) mit Abgrenzung nach fachlichen Kriterien. Diese werden für sozialräumliche Planungszwecke genutzt und haben das Raumbezugssystem der „Statistischen Gebiete / Verkehrzellen“ ersetzt. Daten zu den Bevölkerungsstrukturen in diesen Planungsgebieten sind im Kiezatlas der Sozialraumdaten online zugänglich. Die Schlüsselnummern der LOR setzen sich jeweils aus den zwei Ziffern des Bezirks (Lichtenberg=11), dem Prognoseraum, der Bezirksregion (ungefähr an den Ortsteilen ausgerichtet) und den eigentlichen Planungsräumen zusammen.

Die frühesten archäologischen Funde auf dem Gebiet des heutigen Bezirks datieren aus der mittleren Steinzeit, aber auch Funde aus der Bronzezeit, Eisenzeit und dem Frühmittelalter zeugen von einer wiederkehrenden Besiedelung vor der mittelalterlichen deutschen Landnahme.

Der Verwaltungsbezirk Lichtenberg entstand durch Zusammenlegung von mehreren historisch lange Zeit voneinander unabhängigen Dörfern und Stadtteilen. Das eigentliche Dorf Lichtenberg, seit dem 21. Jahrhundert auch als "Alt-Lichtenberg" bezeichnet, ist nur eine dieser Siedlungen. Zu den weiteren ehemaligen Dörfern im Bezirk Lichtenberg zählen Friedrichsfelde, Hohenschönhausen, Falkenberg und Malchow. Viele Ortsteile des Verwaltungsbezirks Lichtenberg tragen die Namen dieser Dörfer.

Falkenberg<br>
Falkenberg wird in der Schreibung "Valkenberg" 1370 erstmals in einer Urkunde des Markgrafen Otto V. genannt. Marie-Elisabeth von Humboldt, Mutter von Alexander und Wilhelm von Humboldt, erhielt 1791 das Rittergut Falkenberg. Nach ihrem Tod wurde sie in der Falkenberger Kirche beigesetzt.

Friedrichsfelde<br>
Der erste historische Nachweis für das Dorf Rosenfelde (1699 in "Friedrichsfelde" umbenannt) stammt aus dem Jahr 1265.

Hohenschönhausen<br>
Die Nennung von "Henricus bilrebeke" (Heinrich Billerbeck) als Pfarrer der Kirche von Hohenschönhausen im Jahr 1352 war der erste direkte Nachweis dieses späteren Ortsteils.

Karlshorst<br>
Im Kaiserreich entstand auf dem Vorwerk "Carlshorst" (seit 1901: Karlshorst) die gleichnamige Villenkolonie. Im Offizierskasino der Pionierschule I wurde in der Nacht vom 8. auf den 9. Mai 1945 die bedingungslose Kapitulation der Wehrmacht unterzeichnet. In Andenken an dieses Ereignis befindet sich dort das Deutsch-Russische Museum. 

Lichtenberg<br>
Das Dorf Lichtenberg entstand im Zuge der deutschen Kolonisation des Barnim um 1230. Es wurde allerdings erst am 24. Mai 1288 urkundlich in einem Grenzvertrag erwähnt.

Malchow<br>
1344 wurde Malchow in einer Schenkungsurkunde von Ludwig dem Älteren an (Markgraf von Brandenburg) erstmals genannt. 1684 ließ der Gutsbesitzer Paul von Fuchs in Malchow ein zweigeschossiges Herrenhaus, das Schloss Malchow sowie ein Brauhaus und weitere Gebäude errichten.

Wartenberg<br>
Das Dorf Wartenberg wurde 1270 erstmals in einer brandenburgischen Urkunde genannt. Im Jahr 1783 erwarb der spätere preußische Staatsminister Otto von Voß Dorf und Gut Wartenberg. Er errichtete ein Herrenhaus mit Wirtschaftsgebäuden, die seit den 1980er Jahren unter Denkmalschutz stehen.

Bei der Gründung von Groß-Berlin im Jahr 1920 wurde die Stadt Lichtenberg zusammen mit der Landgemeinde sowie dem Gutsbezirk Biesdorf, den Landgemeinden Friedrichsfelde, Kaulsdorf, Mahlsdorf, Marzahn und Hellersdorf einschließlich des Gutsbezirks Wuhlgarten zum neu gegründeten 17. Berliner Verwaltungsbezirk, der den Namen Lichtenberg erhielt.

Teile der ehemaligen Gemeinde Boxhagen-Rummelsburg, die innerhalb des S-Bahn-Rings liegen, wurden 1938 dem Verwaltungsbezirk Friedrichshain zugeschlagen.

Im Jahr 1965 wies Lichtenberg 168.897 Einwohner auf, deren Zahl sich bis 1979 verringerte, weil die Ortsteile Marzahn, Mahlsdorf, Kaulsdorf, Biesdorf und Hellersdorf ausgegliedert worden waren. Der Magistrat schuf daraus den eigenständigen Stadtbezirk Marzahn, der 1986 noch einmal in die Stadtbezirke Marzahn und Hellersdorf geteilt wurde. 

Zwischen der Normannenstraße und der Frankfurter Allee siedelte sich in den späten 1960er Jahren die Zentrale des Ministeriums für Staatssicherheit an, wozu vorhandene Gebäude genutzt aber auch neue errichtet wurden. In einigen Räumen befindet sich seit 1990 die Forschungs- und Gedenkstätte Normannenstraße.

Die im Jahr 2001 vom Senat beschlossene Verwaltungsreform führte zur Fusion von Lichtenberg mit dem zuvor eigenständigen Stadtbezirk Hohenschönhausen. Der so entstandene elfte Berliner Bezirk trägt weiterhin den Namen Lichtenberg.

Im Jahr 2008 erhielt der Bezirk den von der Bundesregierung verliehenen Titel "Ort der Vielfalt".

Am 30. Juni 2016 zählte der Bezirk Lichtenberg Einwohner auf einer Fläche von 52,3 Quadratkilometern. Somit lag am Stichtag die Bevölkerungsdichte bei Einwohnern pro Quadratkilometer. Die Arbeitslosenquote bezifferte sich am 30. Juni 2016 auf 8,2 Prozent. Am 31. Dezember 2016 betrug das Durchschnittsalter der Bevölkerung 42,7 Jahre.

Aufgelistet sind die Einwohnerzahlen des Berliner Verwaltungsbezirks Lichtenberg seit seiner Gründung im Jahr 1920 sowie in einigen Jahren davor in den entsprechenden Gebietsgrenzen. Historisch weiter zurückreichende Einwohnerzahlen des Dorfes und der Stadt Lichtenberg vor ihrer Eingemeindung nach Berlin finden sich im Artikel über den Ortsteil Lichtenberg.

Die Zahlen ab 2001 (Stand jeweils 31. Dezember) basieren auf Daten des Einwohnermelderegisters des Berliner Landesamtes für Bürger- und Ordnungsangelegenheiten.

Am 31. Dezember 2016 lag der Ausländeranteil bei 14,3 %, während der Anteil der Bevölkerung mit Migrationshintergrund bei 22,2 % lag. Die Anzahl der Migranten variiert jedoch zwischen den verschiedenen Ortsteilen: So hatten 2010 im nördlich gelegenen Hohenschönhausen 3,5 % einen Migrationshintergrund, während im südlichen Ortsteil Lichtenberg rund 25 % der Einwohner einen Migrationshintergrund aufwiesen. Die größten ausländischen Herkunftsländer stellen die Staaten der ehemaligen Sowjetunion dar, von denen die meisten Aussiedler bzw. Russlanddeutsche sind. Die zweitgrößte Gruppe bilden Südostasiaten, vornehmlich Vietnamesen. Rund 15 % der Schüler an Lichtenberger Gymnasien haben einen vietnamesischen Migrationshintergrund. Des Weiteren gibt es einen vermehrten Zuzug von Menschen mit muslimischen Wurzeln. So entstammten am 30. Juni 2011 rund 8.800 Einwohner (3,3 % der Bevölkerung des Bezirks) einem muslimischen bzw. nahöstlichen Land.

Die Wirtschaft Lichtenbergs ist geprägt von einer Vielzahl kleiner und mittlerer Unternehmen im Handel, Handwerk und Dienstleistungsbereich.

Ebenso prägend für die Wirtschaftsstruktur des Bezirks sind moderne und exportintensive Industrie- und Technologieunternehmen der Metallverarbeitung, des Werkzeug- und Fahrzeugbaus, des wissenschaftlichen Gerätebaus und der Elektrotechnik. Darüber hinaus finden sich Druckereien und Lebensmittelhersteller sowie industrienahe Dienstleister.

Der Bezirk hat zehn Gewerbegebiete mit insgesamt über 460 Hektar Fläche. 2015 konnten etwa 20.000. Betriebe registriert werden. Im Gewerbegebiet um die Herzbergstraße nahm der Bestand im Jahr 2015 um 97 Betriebe zu.

In Würdigung des gesellschaftlichen und ökonomischen Engagements der Lichtenberger Unternehmerschaft für den Standort wird in jedem Jahr ein „Lichtenberger Unternehmen des Jahres“ ausgezeichnet. Als einziges Berliner Bezirksamt bietet Lichtenberg als Serviceleistung ein Unternehmensportal (UPL) als Informations- und Kommunikationsplattform für die Unternehmen der Region an und gestaltet jährlich mehrere Netzwerktreffen der Portalmitglieder.


Im Bezirk Lichtenberg liegen der Fern- und Regionalbahnhof Lichtenberg sowie die Regionalbahnhöfe Hohenschönhausen und Karlshorst. Der Bahnhof Ostkreuz liegt zu einem kleinen Teil auf Lichtenberger Gebiet.

Durch den Bezirk führt eine der längsten Straßen Berlins, die Landsberger Allee. Sie war bis zur Bezirksfusion 2001 die Grenze zwischen dem Bezirk Lichtenberg (südlich der Straße) und Hohenschönhausen (nördlich der Straße). Ein längeres Teilstück der – auf gemeinsamer Trasse geführten – Bundesstraßen B 1/B 5, die den Bezirk in Ost-West-Richtung durchquert, sind ein Abschnitt der Frankfurter Allee und die Straße Alt-Friedrichsfelde. Die Bundesstraße B2 führt ebenfalls durch Lichtenberg. Die Fertigstellung der Tangentiale Verbindung Ost (TVO), deren dritter Bauabschnitt durch Lichtenberg verläuft, wurde 2011 beschlossen und befindet sich gegenwärtig (Stand:2017) in der Planungsphase.

Im Nahverkehr ist der Bezirk durch die S-Bahn-Linien S3, S41, S42, S5, S7, S75, S8, S85 und S9 sowie die U-Bahn-Linie U5 erschlossen. Mehrere Straßenbahn- und Omnibuslinien führen durch den Bezirk. Der größte Teil des Berliner Straßenbahnnetzes befindet sich in Lichtenberg. In der Siegfriedstraße gibt es darüber hinaus einen Betriebshof für Straßenbahn und Omnibus, der bis 1973 den einzigen Ost-Berliner Obus-Fuhrpark beherbergte.

Die Politik im Bezirk Lichtenberg war nach 1990 durch eine absolute Mehrheit der PDS in der Bezirksverordnetenversammlung (BVV) bestimmt. Lichtenberg war auch einer der drei Bundestagswahlkreise, die die PDS bei der Bundestagswahl 2005 direkt gewinnen konnte. Kritiker argumentieren, die starke Position der heutigen Linkspartei in Lichtenberg begründe sich in der noch aus DDR-Zeiten stammenden Sozialstruktur. 

Bei der Wahl zur BVV 2006 verlor die PDS (nun: Linkspartei.PDS) neun Sitze und damit ihre absolute Mehrheit, blieb jedoch stärkste Fraktion. Zweitstärkste Fraktion wurde die SPD mit 17 Sitzen. 

Seit 2005 gibt es einen Bürgerhaushalt, der erstmals für das Haushaltsjahr 2007 aufgestellt wurde.

Am 17. September 2006 fand mit der Wahl auch der erste Bürgerentscheid Berlins auf Bezirksebene statt. Konkret konnten die Wahlberechtigten Lichtenbergs über die Fusionspläne der drei südlichen Gymnasien des Bezirks abstimmen. Eingebracht wurde der Bürgerentscheid von Eltern, Schülern und Lehrern des Hans-und-Hilde-Coppi-Gymnasiums, die eine geplante Fusion mit dem Immanuel-Kant-Gymnasium ablehnten und alternativ eine Fusion zwischen Immanuel-Kant-Gymnasium und Georg-Forster-Gymnasium bewirken wollten. Der letztere Vorschlag wurde dann umgesetzt. 

Das Wappen des Bezirks Lichtenberg wurde nach der Fusion der ehemaligen Bezirke Lichtenberg und Hohenschönhausen neugestaltet. Während der Name für den Bezirk schnell feststand, zog sich der Prozess der Wappenfindung knapp über fünf Jahre hin. Es gab bereits 2004 einen Entwurf, der den heraldischen Anforderungen durch den Senatsbeauftragten nicht genügte und deshalb überarbeitet werden musste. Das seitdem gültige Wappen wurde am 28. Februar 2006 durch den Senat von Berlin verliehen.

Blasonierung: Das obere grüne Feld des geteilten Schildes zeigt einen silbernen Berg, der zu beiden Seiten jeweils mit einem kleinen silbernen Hügel verbunden ist. Hinter dem Berg zeigt sich eine wachsende goldene strahlende Sonne. Die Sonne wird von zwei silbernen Laubbäumen mit schwarzen Ästen und schwarz-silbernem Stamm begleitet, die auf den kleinen Hügeln stehen. Im unteren blauen Feld befinden sich nebeneinander drei schwebende goldene Ähren, die Mittlere etwas tiefer. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit dem Berliner Wappenschild belegt ist.

Wappenbegründung: Das quergeteilte Schild zeigt im oberen Feld Elemente aus dem ehemaligen Bezirkswappen Lichtenbergs und im unteren Feld Elemente aus dem Bezirkswappen Hohenschönhausens. Das obere Feld mit der Sonne hinter dem Berg verweist auf die erhöht gelegene helle Lichtung, die dem Bezirk den Namen gab. Das untere Feld mit den drei Ähren symbolisiert die mit dem Bezirk Hohenschönhausen eingebrachten drei historischen Dörfer Malchow, Wartenberg und Falkenberg. Das Blau des unteren Feldes greift das blaue Wellenband des alten Lichtenberger Wappens auf und verweist damit auf die Lage am Rummelsburger See und dem Spreeufer. Das Grün und die Bäume im oberen Feld sind Ausdruck des Waldreichtums. Die Mauerkrone ist das verbindende Element aller Berliner Bezirke.

Der Bezirk Lichtenberg verfolgt mit seinen Städtepartnerschaften das Ziel, einen Beitrag zur Völkerverständigung und friedlichen Miteinander zu leisten. Lichtenberg unterhält sieben Partnerschaften mit Städten in sechs verschiedenen Ländern.

Besonders die Kooperationen mit den folgenden Institutionen und Vereinen haben dazu beigetragen, die Kontakte zu Lichtenbergers Partnerstädten zu intensivieren und lebendig zu halten:

Verein Solidaritätsdienst International (SODI), Verein für Solidarität und Entwicklung von Selbsthilfe in Mosambik (Associação de Solidariedade e Desenvolvimento de Auto-Ajuda, ASDA), Evangelisches Jugend- und Fürsorgewerk (EJF), Verein Osteuropa Zentrum Berlin (OEZB), Alexander-Puschkin-Schule, Barnim-Gymnasium, Verein der Vereinigung der Vietnamesen in Berlin & Brandenburg (VdVBB e. V.), Verein für ambulante Versorgung Hohenschönhausen, Bürgerverein Fennpfuhl und Berliner Gesellschaft für internationale Zusammenarbeit (BGZ).

Der Bezirk Lichtenberg unterhält Partnerschaftsbeziehungen mit

, Stadtbezirk von Maputo in Mosambik, seit 22. August 1995

, Stadtbezirk von Warschau in Polen, seit 8. März 2000

, Stadt in Polen, seit 21. September 2001

, Stadt in Litauen, seit 29. Oktober 2003

, Stadtbezirk von Wien in Österreich, seit 20. April 2015

, Stadtbezirk von Hanoi in Vietnam

Zuständig für die innere Sicherheit im Bezirk Lichtenberg ist die Direktion 6 der Berliner Landespolizei. Sie hat ihren Sitz im Nachbarbezirk Marzahn-Hellersdorf in der Poelchaustraße 1.

In der Josef-Orlopp-Straße befindet sich der Standort der Lichtenberger Berufsfeuerwehr. In einigen Ortsteilen gibt es darüber hinaus ebenfalls Berufsfeuerwachen wie in Karlshorst in der Dönhoffstraße und Freiwillige Feuerwehren wie in der Ferdinand-Schultze-Straße 128 in Neu-Hohenschönhausen.



Im Bezirk Lichtenberg sind 93 Sportvereine mit 112 Standorten ansässig, das Angebot umfasst 66 Sportarten (Stand: 2004). Insgesamt sind etwa 23.000 Vereinsmitglieder in Lichtenberg registriert.

Zu den bekanntesten Sportvereinen im Bezirk gehören der mehrmalige deutsche Eishockey-Meister EHC Eisbären Berlin, der frühere DDR-Serienmeister im Fußball BFC Dynamo sowie der SC Berlin (Leichtathletik und Schwimmsport), die allesamt auf dem Gelände des Sportforums Hohenschönhausen beheimatet sind.

Weitere bekannte Vereine des Bezirks sind die Fußballvereine SV Sparta Lichtenberg und SV Lichtenberg 47. Letzterer trägt seine Heimspiele in der HOWOGE-Arena „Hans Zoschke“ aus.

Das Sportforum ist Teil des Olympiastützpunktes Berlin und war bzw. ist Trainingsstätte für zahlreiche bekannte deutsche Sportler (wie die Eisschnelllauf-Weltmeisterin Claudia Pechstein oder die ehemalige Schwimm-Welt- und -Europameisterin Franziska van Almsick).


Seit 2007 findet jährlich die "Lange Nacht der Bilder" in den Bezirken Lichtenberg und Friedrichshain-Kreuzberg statt. An verschiedenen Standorten in den Bezirken werden bei dieser Veranstaltung Ausstellungen präsentiert, offene Ateliers gezeigt sowie Konzerte, Lesungen und Gesprächsrunden veranstaltet.

siehe auch: Liste der Kinos im Berliner Bezirk Lichtenberg

Das Tierheim Berlin in Falkenberg diente mehrmals als Film- und Fotokulisse, wie z. B. im Science-Fiction-Film "Aeon Flux" (2005).

siehe auch: Liste der Brunnen im Berliner Bezirk Lichtenberg




</doc>
<doc id="14459" url="https://de.wikipedia.org/wiki?curid=14459" title="Bezirk Spandau">
Bezirk Spandau

Spandau ist der fünfte Verwaltungsbezirk von Berlin und hatte Einwohner per .

Der heute flächenmäßig viertgrößte Berliner Bezirk wurde 1920 im Zuge der Bildung von Groß-Berlin aus dem Gebiet der bereits 1232 urkundlich erwähnten Stadt Spandau und mehreren umliegenden Gemeinden und Gutsbezirken gebildet.

Spandau ist bekannt für seine ausgedehnten Wald- und Wasserflächen und für seine Lage entlang der Havel.

Spandau liegt zum größten Teil am westlichen Ufer der Havel (siehe auch Zehdenick-Spandauer Havelniederung). Die Stadt wurde gegründet am Zusammenfluss von Spree und Havel. Spandau grenzt an das Land Brandenburg, den Landkreis Oberhavel, den Landkreis Havelland und die Landeshauptstadt Potsdam (kreisfreie Stadt).


Der Bezirk Spandau unterteilt sich in neun Ortsteile:

Die kleinräumige Gliederung für Berlin sind die Lebensweltlich orientierten Räume (LOR) mit Abgrenzung nach fachlichen Kriterien. Diese werden für sozialräumliche Planungszwecke genutzt und haben das Raumbezugssystem der „Statistischen Gebiete / Verkehrzellen“ ersetzt. Daten zu den Bevölkerungsstrukturen in diesen Planungsgebieten sind im Kiezatlas der Sozialraumdaten online zugänglich.

Der Bezirk Spandau ging aus der Besiedlung Spandaus hervor. Siehe Details zu dessen Geschichte unter "Berlin-Spandau."

Die Besiedlung des Gebietes lässt sich bis ins 6. Jahrhundert zurückverfolgen, als das Havelland von den Sprewanen und Hevellern (slawische Stämme) besiedelt wurde. Albrecht der Bär soll hier eine Burg errichtet haben, aus der die namensgebende Burganlage "Spandow" entstand, die im Jahr 1197 zum ersten Mal urkundlich erwähnt wurde. Um diese Burg entwickelte sich die Stadt Spandow, die zum Zentrum des Gebietes wurde. Entgegen allgemein verbreiteter Ansicht wurde in der am 7. März 1232 von den Markgrafen Johann I. und Otto III. ausgestellten Urkunde Spandau nicht das Stadtrecht erteilt. Der Text der Urkunde – sofern die erhaltene deutsche Übersetzung authentisch ist, was teilweise angezweifelt wird – macht vielmehr deutlich, dass Spandau bereits Stadtrechte besaß und hier noch zusätzliche Rechte – vor allem der Bau einer Flutrinne, der Vorgängerin der Schleuse Spandau – gewährt wurden. Wann die Verleihung der Stadtrechte erfolgte, geht daraus nicht hervor. Da jedoch für das Alter von Städten die förmliche Verbriefung (das "Urkundsprinzip") gilt, und in der genannten Urkunde Spandau zum ersten Male als Stadt erwähnt wird, ist es erst ab 1232 als Stadt anzusehen. Am 1. November 1539 trat Kurfürst Joachim II. in der Spandauer St.-Nikolai-Kirche zum protestantischen Glauben über, wodurch die Mark Brandenburg ebenfalls protestantisch wurde.

Der Zustand der Stadt und Umgebung um 1728 wird in beeindruckender Weise in einem Katasterwerk für die befestigte Stadt Spandau dokumentiert, womit erstmals ein komplettes Liegenschaftskataster mit den beiden großmaßstäblichen Karten "Spandau Intra moenia" (1:1000) und "Spandau extra moenia" sowie entsprechenden Eigentümerverzeichnissen geschaffen wurde. Dieses liegt heute noch vor (Stadtarchiv Spandau, Vermessungsamt Spandau). Erarbeitet wurde dieses Werk von dem Landmesser Gustav Haestskau.

Die Schreibweise des Namens wurde im Jahr 1878 von "Spandow" in "Spandau" geändert. Am 1. April 1887 schied die Stadt aus dem Landkreis Osthavelland aus und wurde ein Stadtkreis.

Mit Bau der Berlin-Hamburger Bahn erhielt Spandau am Standort des heutigen S-Bahnhofs Stresow im Jahr 1846 einen Eisenbahnanschluss, 1871 folgte die Berlin-Lehrter Eisenbahn. Der mit der Industrialisierung Spandaus stetig steigende Pendlerverkehr führte zwischen 1909 und 1911 zum Bau der Spandauer Vorortbahn, die 1928 als S-Bahn elektrifiziert wurde. Während des Ersten Weltkrieges wuchs vor allem die Rüstungsindustrie, sodass Spandau zum Ende des Krieges ein bedeutendes Rüstungszentrum des Deutschen Reiches geworden war. Das hier gefertigte Maschinengewehr "MG 08/15" wurde zum Synonym für die gleichnamige Redewendung. Ab 1897 siedelte das Elektrounternehmen Siemens & Halske Produktion und Verwaltung im Osten Spandaus auf den Nonnenwiesen an. Ein Novum war der mit dem Aufbau dieses Industriegebiets einhergehende Bau einer Wohnsiedlung durch Siemens. Dazu gehörte auch deren Infrastruktur: der Bahnhof Fürstenbrunn, die Straßenbahn an der Nonnendammallee und 1929 die von der S-Bahn befahrene Siemensbahn zum neuen Ortsteil Siemensstadt.

Im Rahmen der Bildung von Groß-Berlin am 1. Oktober 1920 wurde aus den folgenden Gebietseinheiten der Bezirk Spandau, damals der achte Bezirk Berlins, gebildet: Stadtkreis Spandau, Gemeinde Staaken, Gemeinde Tiefwerder, Gemeinde Pichelsdorf, Gemeinde Gatow, Gemeinde Kladow, Gutsbezirk Spandau-Zitadelle, Gutsbezirk Pichelswerder, Gutsbezirk Heerstraße (nördlicher Teil).

Nach dem Zweiten Weltkrieg gehörte der Bezirk Spandau in der „Vier-Sektoren-Stadt“ Berlin zum Britischen Sektor und verlor West-Staaken an die Sowjetische Besatzungszone. Bis 1987 befand sich an der Wilhelmstraße das Kriegsverbrechergefängnis Spandau, in dem die Alliierten zuletzt nur noch den zu lebenslanger Haft verurteilten Rudolf Heß bewachten. Sofort nach dessen Tod wurde das Gefängnis abgerissen und ein Einkaufszentrum für die in Spandau stationierten britischen Truppen errichtet. 1990 erhielt Spandau das seinerzeit abgetrennte West-Staaken zurück.

Der gesamte S-Bahn-Verkehr im Bezirk kam 1980 auf den drei Strecken von Jungfernheide nach Gartenfeld "(Siemensbahn)" sowie über Fürstenbrunn nach Spandau und von Friedrichstraße über Westkreuz – Spandau nach Staaken völlig zum Erliegen. 1980 erhielt der Bezirk mit der Verlängerung der damaligen U-Bahn-Linie 7 (heute: U7) zum U-Bahnhof Rohrdamm erstmals Anschluss an das Berliner U-Bahn-Netz. 1984 wurde die U7 bis zum Endbahnhof Rathaus Spandau fertiggestellt. Seit 1998 fährt die S-Bahn wieder über die Gleise der Spandauer Vorortbahn bis zum damals neu errichteten Bahnhof Berlin-Spandau mit Anschluss an den Regional- und Fernverkehr.

Das kulturelle Zentrum des Bezirks ist die Spandauer Altstadt, von deren ursprünglicher Bausubstanz allerdings aufgrund der Ereignisse des Zweiten Weltkriegs wenig erhalten geblieben ist. Dennoch gelang mit der Sanierung in den 1970er Jahren und der Beseitigung der nach dem Krieg in Baulücken entstandenen eingeschossigen Behelfsbauten für Geschäfte ein überzeugendes Bild der neuen Altstadt. Sie wurde gleichzeitig vom Durchgangsverkehr weitgehend befreit und in eine Fußgängerzone umgewandelt. Sie bietet daher ausreichend Platz für Wochenmärkte und den alljährlich in der Adventszeit stattfindenden "Spandauer Weihnachtsmarkt". Trotz der Zerstörungen im Zweiten Weltkrieg hat Spandau immer noch eines der ältesten Häuser in ganz Berlin, das als Museum genutzt wird.

Im Süden des Bezirks befindet sich seit 1995 auf dem ehemaligen Flugplatz Gatow, der während der Teilung von der Royal Air Force genutzt wurde, das Luftwaffenmuseum.

Ein ehrgeiziges Projekt ist seit Anfang der 1990er Jahre die städtebauliche Entwicklungsmaßnahme „Wasserstadt Berlin-Oberhavel“, die im Jahr 2008 weitgehend abgeschlossen wurde. Das zu gestaltende Areal erstreckt sich beidseits der Insel Eiswerder sowie nördlich davon auf ehemaligen, für die Öffentlichkeit nicht zugänglichen Industrie- und Brachflächen an der Havel. Der hier vollzogene Stadtumbau ist ökologisch orientiert. Besonders attraktiv sind die Wohnquartiere unmittelbar am Ufer der Havel. Geboten werden neben städtebaulichen Standards, wie Spielplätzen und Parks, neue Wassersportmöglichkeiten (wie seit 2007 der Maselakepark). Die Entwicklungsmaßnahme hat dem Land Berlin hohe Investitionen abverlangt, die sich – bei sinkenden Grundstückspreisen in Berlin seit etwa 1995 – nicht aus Wertsteigerungen der entwickelten Grundstücke refinanzieren ließen. Auch sind die neuen Wohnbaugebiete in Teilen nur sehr zögerlich vom Grundstücksmarkt angenommen worden, sodass im Jahr 2009 noch große Flächen (in der Anflugschneise des Flughafens Tegel) brachlagen.

Per zählte der Bezirk Spandau Einwohner. Spandau hat damit die geringste Einwohnerzahl aller Berliner Bezirke, gefolgt vom Bezirk Treptow-Köpenick. Bedingt durch den relativ hohen Anteil von Wasser- und Waldflächen an der Gesamtfläche von knapp 92 Quadratkilometern lag am Stichtag die durchschnittliche Bevölkerungsdichte bei Einwohnern pro Quadratkilometer.

Die Bevölkerungsprognose 2016 nimmt an, dass die Bevölkerung in Spandau bis 2030 auf 248.000 wachsen wird. Gerechnet ab 2015 entspricht dies einer Zunahme von 7,7 % und liegt damit leicht über dem Berliner Durchschnitt von 7,5 %.

Am 31. Dezember 2016 betrug der Ausländeranteil 18,0 %, der Anteil der Bevölkerung mit Migrationshintergrund 33,8 %. Die Arbeitslosenquote bezifferte sich am 30. April 2013 auf 13,9 %. Am 31. Dezember 2016 betrug das Durchschnittsalter der Bevölkerung 43,7 Jahre.

Kirchen und Kapellen
Die evangelischen Kirchen Spandaus gehören zum Kirchenkreis Spandau in der Evangelischen Kirche Berlin-Brandenburg-schlesische Oberlausitz, die römisch-katholischen Pfarreien zum Dekanat Spandau im Erzbistum Berlin.



Moscheen
Die Moscheen im Bezirk Spandau sind sunnitisch. Sie gehören unterschiedlichen Dachverbänden an:


Buddhistische Tempel

Der Bezirk Spandau ist mit seinen Ver- und Entsorgungseinrichtungen wie dem Kraftwerk Reuter, dem Müllverbrennungs- und Klärwerk Ruhleben sowie zahlreichen Produktionsstätten (Siemens, Osram, BMW-Motorräder) ein bedeutender industriell geprägter Wirtschaftsstandort für Berlin.

Mehr als 30 Unternehmen haben ihren Sitz im Thelen Technopark, u. a. "Boschen & Oetting Automatisierungs-Bau GmbH" und BSH Technologie­zentrum Wäschepflege.

Der Bezirk besitzt auch große Wald- und Wasserflächen, die als touristisch beliebtes Ausflugsgebiet genutzt werden.

Im Jahr 2007 zählte das verarbeitende Gewerbe (Betriebe mit mehr als 20 Beschäftigten) im Bezirk 64 Betriebe mit 16.691 Beschäftigten. 2012 gehörten 12.461 Gewerbetreibende aus Spandau der IHK Berlin an.

Die Stadt Spandau besaß ab Juni 1892 eine erste Pferdebahn-Linie (Hauptbahnhof bis Fehrbelliner Tor). 1894 wurde der Betriebsbahnhof angelegt und es kam die zweite Linie (Hauptbahnhof bis Pichelsdorf) und 1896 die dritte Linie (Hauptbahnhof bis Schützenhaus) dazu. 1896 wurden alle Strecken elektrifiziert, 1909 kam die Spandauer Straßenbahn in Stadtbesitz. In den 1930er Jahren gab es einen O-Bus-Verkehr in Spandau. 1967 fuhr die letzte West-Berliner Straßenbahn (Linie 55) von Hakenfelde zum Bahnhof Zoo. Eine Eisenbahnstrecke vom Bahnhof Spandau West über Johannesstift nach Bötzow (Bötzowbahn) ist seit dem Ende des Zweiten Weltkriegs außer Betrieb beziehungsweise dient nur noch in Teilen als Güterverkehrsstrecke.

Durch Spandau führen die beiden Bundesstraßen B 2 und B 5.

Spandau ist an zahlreiche nationale, internationale und regionale Radwanderwege angeschlossen: Unter anderem an die europäische EuroVelo-Route EV 7 (verläuft von Norwegen bis Malta), an den Radweg Berlin–Kopenhagen (u. a. über Oranienburg, Zehdenick, Fürstenberg, Rostock) und an den Havelradweg (u. a. über Potsdam und Brandenburg an der Havel).

In Spandau gibt es Anbindungen mit folgenden öffentlichen Verkehrsmitteln:

Folgende Linien des Schienenpersonenfernverkehrs haben einen Verkehrshalt im Bahnhof Berlin-Spandau:

Durch Spandau führen die Bundeswasserstraßen Untere Havel-Wasserstraße, Obere Havel-Wasserstraße und Spree-Oder-Wasserstraße. Der Spandauer Südhafen an der Havel ist der zweitgrößte Frachthafen Berlins.

Folgende Fähren verkehren im Bezirk Spandau:

Die Wahl zur Bezirksverordnetenversammlung (BVV) Spandau erfolgte am 18. September 2016.

In der heutigen Form wurde das Wappen am 4. Februar 1957 durch den Senat von Berlin verliehen.
Blasonierung: In silbernem Schild über blauen Wellen eine nach außen ansteigende rote Stadtmauer; vor deren offenem, breitem, oben mit einem Zinnenkranz abgeschlossenem Mitteltor steht ein silberner Dreieckschild mit dem brandenburgischen goldenbewehrten roten Adler, dessen Flügel mit goldenen Kleestengeln belegt sind. Das Tor ist beseitet von zwei hinter der Mauer hervorkommenden goldenbeknopften roten spitzbedachten befensterten Türmen, zwischen denen auf dem Zinnenkranz des Tores ein naturfarbener Topfhelm – der brandenburgische Wappenhelm – sitzt. Er trägt als Helmzier einen mit goldenen Lindenblättern besäten schwarzen Flug. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit einem kleinen Berliner Wappenschild belegt ist.

Wappenbeschreibung:Das Wappen des Bezirks Spandau leitet sich von dem Wappen der Stadt Spandau ab, dessen heraldischen Elemente bereits im 13. Jahrhundert belegt sind – der Helm und die Mauern verweisen auf die Festungsstadt, das blaue Wellenband auf den Fluss Havel, und der rote märkische Adler auf die Gründungsherren. Die Mauerkrone wurde dem Wappen dabei am 1. Januar 2001 im Rahmen der Bezirksreform als verbindendes Element aller Berliner Bezirke hinzugefügt.

 und Kreis Siegen-Wittgenstein in NRW seit 1952

Die Direktion 2 der Berliner Polizei ist für die Bezirke Spandau und Charlottenburg-Wilmersdorf zuständig.


Der LSV Spandau gewann 1939 den Titel des ersten deutschen Basketballmeisters, der durch einen deutlichen 47:16 Sieg über Bad Kreuznach errungen wurde. Nach 1945 wurde der Verein aufgelöst.

Die Wasserfreunde Spandau 04 sind einer der erfolgreichsten deutschen Sportvereine. Die Erfolgsbilanz weist bis heute rund 100 Deutsche Meisterschaften, zehn Europameisterschaften und eine Weltmeisterschaft sowie Titel im Schwimmen, Springen und Wasserball auf. Hagen Stamm, langjähriger Kapitän des Wasserball-Teams, wurde mit dem Verein von 1979 bis 1992 vierzehnmal in Folge Deutscher Meister, zwölfmal Deutscher Pokalsieger und gewann 1982, 1985, 1986 und 1989 den Europapokal der Landesmeister. Gegenwärtig ist er der Präsident des Vereins (Stand: 2017).

Die SG ASC/VfV Spandau spielte in der 2. Handball-Bundesliga.

Das "Blaue Band der Spree" ist eine der größten deutschen Tanzsport-Veranstaltungen und findet jedes Jahr über Ostern im Sport Centrum Siemensstadt statt.

Der SC Siemensstadt und der TSV Spandau 1860 haben jeweils über 5000 eingeschriebene Mitglieder (Stand: 2014) und zählen damit zu den mitgliederstarken Sportvereinen in der Stadt.
In Spandau existiert der Arbeitskreis Spandauer Künstler Berlin e. V. das Kulturhaus Spandau, das Theater Zitadelle, die Freilichtbühne Zitadelle, der Kunstlandschaft Spandau, die Bastion Jugendkunstschule Berlin-Spandau sowie andere private und öffentliche Initiativen und Einrichtungen.





Die britische Musikgruppe Spandau Ballet wählte ihren Namen nach einer Berlinreise, bei der sie u. a. den – nahe der damals noch existierenden Berliner Mauer gelegenen – Flugplatz Gatow besuchte. Der Name sollte nach Auffassung der Bandmitglieder keine direkten politischen Anliegen verkörpern, sondern auf das Romantische und Düstere der Grenzsituation anspielen.





</doc>
<doc id="14460" url="https://de.wikipedia.org/wiki?curid=14460" title="Bezirk Neukölln">
Bezirk Neukölln

Neukölln ist der achte Verwaltungsbezirk von Berlin und hat Einwohner (Stand: ). Nach den Bezirken Friedrichshain-Kreuzberg und Mitte ist er der am dichtesten besiedelte Bezirk in Berlin.

Neukölln wurde nach dem gleichnamigen Ortsteil benannt, der den nördlichen Teil des Bezirks ausmacht. Zwischen 1945 und 1990 war es Teil des amerikanischen Sektors von West-Berlin.

Neukölln liegt im südlichen Bereich der Bundeshauptstadt Berlin zwischen den Bezirken Tempelhof-Schöneberg im Westen und Treptow-Köpenick im Osten sowie Friedrichshain-Kreuzberg im Norden. Im Süden grenzt Neukölln an das Land Brandenburg.

Der Bezirk zeigt unterschiedliche bauliche Strukturen, im Norden innerstädtisch hochverdichtet, im Süden eher vorstädtisch aufgelockert, teils sogar ländlich wirkend: Der Ortsteil Neukölln (auch "Neukölln-Nord" oder "Neukölln 44" – nach der ehemaligen Postleitzahl 1000 Berlin 44 – genannt) ist im Norden des Bezirks (zwischen Ringbahn und Hermannplatz) überwiegend vom Altbaubestand der Gründerzeit geprägt, der aus typischen Berliner Mietskasernen mit begrünten Hinterhöfen besteht. Südlich der Ringbahn dominieren Einfamilienhausgebiete, vorstädtischer Siedlungsbau und Großsiedlungen mit vielen Hochhäusern das Gesicht des Bezirks.

In diesen uneinheitlichen Strukturen eingebettet haben sich die alten Dorfkerne von Rixdorf mit dem Böhmischen Dorf, Britz mit dem Gutshof und Schloss und Buckow mit der ältesten Feldsteinkirche Berlins erhalten.

Das bekannteste Beispiel für einen sozial motivierten Siedlungsbau/Reformwohnungsbau im Neukölln der 1920er und 1930er Jahre findet sich in Britz. Die Großsiedlung Fritz-Reuter-Stadt wurde geprägt von den Architekten Bruno Taut und Martin Wagner einerseits und von Paul Engelmann und Emil Fangmeyer im anderen Teil der Siedlung. Die beiden Teile bestehen jeweils aus etwa 1000 Wohnungen. In der Randbebauung und im Hufeisen sind dies Mehrfamilienhäuser, ansonsten Einfamilienhäuser mit Nutzgärten. Ein Teil der Großsiedlung, die Hufeisensiedlung im engeren Sinn, ist seit 2008 UNESCO-Welterbe.

Darüber hinaus befindet sich im Süden ein bekanntes Beispiel einer Großsiedlung der Nachkriegszeit: die Gropiusstadt. Sie ist – neben dem Märkischen Viertel – eine der beiden größten Großsiedlungen des ehemaligen West-Berlins. Zunächst "Großsiedlung Berlin-Buckow-Rudow" genannt, wurde ihr 1972 der Name des Bauhausgründers Walter Gropius verliehen. 1976 wurde die Gropiusstadt fertiggestellt und 2002 als eigener Ortsteil abgetrennt. Abweichend vom Hochhaus-Konzept der „Urbanität durch Dichte“ und „autogerechten Stadt“ wie in der Gropiusstadt wurde in den 1970er und 1980er Jahren die High-Deck-Siedlung für rund 6000 Bewohner errichtet. Das ursprünglich als innovativ beurteilte städtebauliche Konzept einer funktionalen Trennung von Fußgängern und Autoverkehr mit hochgelagerten, begrünten Wegen (den namensgebenden „High-Decks“) erwies sich allerdings schnell als gescheitert.

→ "Siehe Liste der Naturdenkmale im Bezirk Neukölln"

Auf 632 Hektar Verkehrsfläche befinden sich 715 Straßen und Plätze.

Wie der gesamte Norden Neuköllns ist auch die Sonnenallee und ihre unmittelbaren Umgebung von Zuwanderung geprägt, sowohl von Menschen aus dem Ausland, als auch aus den übrigen Teilen Deutschlands, die wegen der vergleichsweise günstigen Mieten in diese Gegend ziehen. 

Innerhalb der letzten Jahrzehnte hat sich dort eine arabische Infrastruktur gebildet, bestehend aus Restaurants, Cafés und Einzelhandel für den täglichen Bedarf. Vor allem die Nebenstraßen sind durch Zuzug von Studenten, Kreativen und jungen Familien von Gentrifizierung betroffen.

Die Schillerpromenade bildet den Mittelpunkt des sogenannten Schillerkiezes. Das Viertel wurde um 1900 als „Wohnquartier für Besserverdienende“ angelegt. Nachdem Neukölln – wie die meisten West-Berliner Ortsteile nahe dem innerstädtischen Mauerstreifen – stark an Beliebtheit eingebüßt hatte, kam es auch im Schillerkiez zum Wegzug der besser situierten Bevölkerung. Dieser Prozess wurde durch den zunehmenden Fluglärm des angrenzenden Flughafens Tempelhof zusätzlich verstärkt. 

Seit Schließung des Flughafens erfreut sich auch der Schillerkiez wieder zunehmender Beliebtheit. Dies führte zu einem Anstieg der Mieten und in Folge dessen auch hier zu einer Gentrifizierung.


Eine Besonderheit im Norden Neuköllns ist der Richardplatz mit dem umliegenden Viertel Alt Rixdorf zwischen Sonnenallee und Karl-Marx-Straße. Der Platz ist die Keimzelle des ehemaligen Dorfes, aus dem Neukölln einst entstanden ist. Die umliegenden Straßen stellen somit die Altstadt des Ortsteils dar. Im Zweiten Weltkrieg blieb das Viertel unversehrt und wurde anschließend von den Stadtplanern weitestgehend ignoriert, sodass der dörfliche Charakter erhalten blieb. Besonderer Beliebtheit erfreut sich der alljährliche Weihnachtsmarkt auf dem Platz. Weitere nennenswerte Plätze sind:


Neukölln hieß bei seiner ersten urkundlichen Erwähnung im Jahr 1360 "Richardsdorp", später "Ricksdorf (Rieksdorf)" und schließlich "Rixdorf". Der Ortskern befand sich am Richardplatz. Das Dorf gehörte anfangs dem Johanniterorden, die den Ort von den Tempelrittern übernahmen, die in Tempelhof ansässig waren. Aus diesem Grund trägt das Wappen des Bezirks das Johanniterkreuz.

Im Jahr 1737 gestattete Friedrich Wilhelm I. die Ansiedlung böhmischer Exilanten in "Rieksdorf," die wegen ihres evangelischen Glaubens vertrieben wurden. Diese Anhänger der Herrnhuter Brüdergemeine bauten ihre eigene Kirche und siedelten in einem eigenen Bereich abseits des Dorfangers, entlang der heutigen Richardstraße, der 1797 als "Böhmisch-Rixdorf" eine eigene Verwaltung bekam.

Bei der Wiedervereinigung der beiden selbstständigen Gemeinden am 1. Januar 1874 hatte Rixdorf 8000 Einwohner. Ab dem 1. Mai 1899 bildete Rixdorf, bis dahin als größtes Dorf Preußens zum Kreis Teltow gehörig, einen eigenen Stadtkreis, dessen Bevölkerung von zunächst 80.000 Einwohnern bis 1910 auf über 237.000 Einwohner wuchs.

Die Umbenennung des Ortes von "Rixdorf" zu "Neukölln" erfolgte 1912. Der Grund der Umbenennung durch die Behörden war der seinerzeit negative Gesamteindruck des Ortes: Rixdorf galt als Hochburg von Kriminalität und „schlechten Sitten“.

Das Dorf Britz wurde erstmals 1305 urkundlich erwähnt. Zum Ende des 19. Jahrhunderts griff die Verstädterung vom nördlichen Nachbarort Rixdorf auf Britz über, wodurch sich die Einwohnerzahl bis 1920 auf mehr als 13.000 erhöhte.

Das südlich von Britz gelegene Buckow wurde 1230 als Angerdorf gegründet und besaß seit 1913 eine Straßenbahnverbindung über Britz und Neukölln nach Berlin.

Das Straßendorf Rudow wurde erstmals 1373 erwähnt und behielt bis 1920 weitgehend seinen dörflichen Charakter.

Mit dem 1. Oktober 1920 wurde die Stadt Neukölln nach Groß-Berlin eingemeindet. Zusammen mit den Gemeinden Britz, Buckow und Rudow bildete Neukölln als Namensgeber den 14. Verwaltungsbezirk. Am Ende des 20. Jahrhunderts sind die Geschehnisse um die Umbenennung Rixdorfs vergessen. Stattdessen gibt es vereinzelte Bestrebungen zur Rückbenennung des Ortsteils Neukölln in "Rixdorf," zum einen als Abgrenzung zum größeren Bezirk Neukölln, zum anderen wegen der größeren Prägnanz des alten historischen Namens.

Von 1945 bis 1990 gehörte der Bezirk Neukölln zum Amerikanischen Sektor von Berlin. 1987 wurde der Bezirk mit dem Europapreis für seine hervorragenden Bemühungen um den europäischen Integrationsgedanken ausgezeichnet. Am 23. September 2008 erhielt der Bezirk den von der Bundesregierung verliehenen Titel "Ort der Vielfalt."

Am zählte der Bezirk Neukölln Einwohner auf einer Fläche von 44,9 Quadratkilometern. Somit lag am Stichtag die Bevölkerungsdichte bei Einwohnern pro Quadratkilometer. Am 31. Dezember 2016 lag der Ausländeranteil bei 24,4 %, während der Anteil der Bevölkerung mit Migrationshintergrund bei 43,9 % lag (lediglich der Bezirk Mitte hatte mit 50,8 % einen höheren Anteil von Einwohnern mit Migrationshintergrund). 

Die Arbeitslosenquote bezifferte sich am 30. April 2013 auf 17,1 % und war somit der höchste Wert aller Berliner Bezirke (zum Vergleich: Der Bezirk Pankow hatte mit 10,0 Prozent die niedrigste Arbeitslosenquote). Am 31. Dezember 2016 betrug das Durchschnittsalter der Bevölkerung 41,3 Jahre.

Teile von Neukölln sind vom Berliner Senat als "Gebiete mit besonderem Entwicklungsbedarf" ausgewiesen worden. Von den 17 Gebieten dieser Art in ganz Berlin liegen neun in Neukölln: Reuterplatz, Rollbergsiedlung, High-Deck-Siedlung, Schillerpromenade, Richardplatz Süd, Gropiusstadt/Lipschitzallee, Flughafenstraße, Dammwegsiedlung/Weiße Siedlung und Körnerpark. Hier wurde jeweils ein Quartiersmanagement zur integrativen Entwicklung des Wohnumfeldes eingerichtet.

Berlins damaliger Innensenator Ehrhart Körting (SPD) äußerte im Januar 2004 vor dem Innenausschuss des Berliner Abgeordnetenhauses die Auffassung, dass sich gerade um das Rollbergviertel (Neukölln-Nord) herum teilweise Ghettos entwickelten. Auffällig sei die deutlich erhöhte Kriminalitätsrate in diesen Bereichen. Hinzu komme eine mangelnde Integrationsbereitschaft der ausländischen Bevölkerung sowie die Tendenz, Polizeigewalt nicht anzuerkennen. Die Richterin Kirsten Heisig machte in Ihrem Buch "Das Ende der Geduld: Konsequent gegen jugendliche Gewalttäter" auf die Probleme in Neukölln aufmerksam. Der Integrationsbeauftragte des Abgeordnetenhauses ergänzte, die Kieze seien jedoch nicht nur wegen eines hohen Migrantenanteils, sondern wegen großer sozialer Probleme Brennpunkte. Diese ließen sich genau benennen: hohe Arbeitslosigkeit, schlechte Bildung, eine überdurchschnittliche Zahl jugendlicher Schulabbrecher.

Seit 2004 gibt es das erfolgreiche Integrationsinstrument "Stadtteilmütter in Neukölln", das jetzt auch im Brunnenviertel im Ortsteil Wedding (Bezirk Mitte) Schule macht. Getragen wird das Projekt durch die Kooperationsvereinbarung mit dem Bezirksamt Neukölln, dem JobCenter Neukölln und dem Diakonischen Werk Neukölln-Oberspree e. V. Berlin wurde dafür mit dem "Metropolis Award 2008" ausgezeichnet.

Die im Bezirk durchgeführten Schuleingangsuntersuchungen im Jahr 2010 haben zu folgenden Ergebnissen geführt: Jedes sechste Kind in Neukölln ist übergewichtig, jedes fünfte Kind hat kariöse Zähne und wächst in einem Raucher-Haushalt auf. Zwei Drittel von ihnen sind in ihrer Entwicklung auffällig. Ein Viertel der Mädchen und Jungen haben bei Schuleintritt die fortlaufenden Untersuchungen zur Vorsorge nur unvollständig besucht. Neukölln war 2010 der Berliner Bezirk mit den meisten sprachlichen Entwicklungsauffälligkeiten und den geringsten Deutschkenntnissen.

Im Jahr 2010 hat der Bezirk eine kommunale integrierte Strategie der Gesundheitsförderung, die Neuköllner Präventionskette ins Leben gerufen. Durch Vernetzung der bestehenden Institutionen und Angebote von Jugendhilfe, Bildung und Gesundheit sollen Lücken in der gesundheitlichen Entwicklung von Kindern geschlossen und die Kooperation aller beteiligten Partner verbessert werden. Zwar zeichnet sich mit dem Gesundheitsbericht 2016 eine Verbesserung der Lage ab, die Werte liegen aber weiterhin unterhalb des Berliner Durchschnitts.

Andererseits setzte im Norden Neuköllns seit etwa 2007 ein Prozess der Gentrifizierung (Stadtteil-Aufwertung) ein. Das auch "Kreuzkölln" genannte Viertel, gilt seit 2012 als attraktive Wohngegend, das eine Vielzahl von Ateliers und gastronomischen Angeboten bietet. Somit hat sich aus einem sozialen Brennpunkt zwischenzeitlich eine Gegend entwickelt, das eine höhere Durchmischung von Bewohnern unterschiedlicher sozialer Schichten aufweist. Der hinzuziehende Bevölkerungsteil besteht vornehmlich aus Künstlern, Studenten und jungem Bürgertum.


Das Estrel Hotel an der Sonnenallee ist mit 1125 Zimmern und einem Umsatz von 70,6 Millionen Euro im Jahr 2016 Deutschlands größtes und umsatzstärkstes Hotel. Es ist in der DEHOGA-Kategorie 4+ klassifiziert und beschäftigt rund 550 festangestellte Mitarbeiter.

Die Biotronik SE & Co. KG hat ihren Unternehmenssitz in Berlin Neukölln und ist ein Hersteller von medizintechnischen Produkten. Das Unternehmen unterhält Forschungsstätten in Europa, Nordamerika und Singapur und erwirtschaftet einen Jahresumsatz von über 500 Million Euro (Stand: 2013).

Die Autobahnen A 100 und A 113 führen durch den Bezirk Neukölln. Gegenwärtig wird die A 100 in Richtung Treptow-Köpenick verlängert (Stand: 2017). Der Weiterbau auf Neuköllner Gebiet ist der sogenannte Abschnitt (BA 16) und führt entlang der Ringbahn.

Durch den Bezirk Neukölln führen die S-Bahn-Linien S41, S42, S45, S46 und S47 sowie die U-Bahn-Linien U7 und U8.


Stimmenanteile der Parteien in Prozent:
1921–1933

1946–2011

Das heutige Wappen geht auf den königlichen Erlass der Stadtgemeinde vom 29. Mai 1903 zurück. Es wurde am 12. April 1956 vom Senat von Berlin verliehen.

Blasonierung: Ein halbgespaltener und geteilter Schild, darin im ersten, schwarzen Felde ein silberner Abendmahlskelch, im zweiten, silbernen Felde ein auf den Flügeln mit goldenen Kleestengeln belegter goldenbewehrter roter Adler und im dritten, roten Felde ein silbernes achtspitziges Kreuz. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit einem kleinen Berliner Wappenschild belegt ist.

Wappenbegründung: Das Wappen des Bezirks Neukölln wurde von der für den Bezirk namensgebenden Stadt Neukölln übernommen. Das Johanniterkreuz verweist dabei auf die Gründung durch den Johanniterorden, der brandenburgische rote Adler verweist auf die Gründungsherren der Mark Brandenburg, und der Hussitenkelch steht symbolisch für die böhmischen Kolonisten. Die Mauerkrone wurde dem Wappen dabei am 1. Januar 2001 im Rahmen der Bezirksreform als verbindendes Element aller Berliner Bezirke hinzugefügt.

Der Bezirk Neukölln pflegt folgende Städtepartnerschaften:
Sonstiges

Die Direktion 5 der Berliner Landespolizei ist für die Bezirke Neukölln und Friedrichshain-Kreuzberg zuständig. Der Direktionsleiter der Direktion 5 ist Michael Krömer (Stand: 2017).



Das Stadtbad Neukölln ist durch seine neoklassizistische Bauweise überregional bekannt. Der Saunabereich des Bades umfasst eine finnische Sauna, eine Kräutersauna, ein Marmordampfbad, ein Caldarium und ein Sanarium.

Der TuS Neukölln 1865 wurde 1865 gegründet und zählt zu ältesten Sportvereinen Berlins.

Die Tanzformation "Dance Deluxe" (TSV Rudow 1888 Berlin e. V.) ist mehrfacher Deutscher Meister, Europa- und Weltmeister im Cheer Dance.

Die SG Neukölln Berlin ist einer der größten und erfolgreichsten deutschen Vereine im Schwimmsport. Olympiasiegerin Britta Steffen wurde dort trainiert.

Die Boxabteilung der Neuköllner Sportfreunde zählt zu den erfolgreichsten in Deutschland. Ehrenmitglied der Sportfreunde ist der Weltergewicht-Europameister von 1996 Oktay Urkal.



→ "Siehe auch: Liste der Kinos im Berliner Bezirk Neukölln"

Das Festival "48 Stunden Neukölln" ist ein spartenübergreifendes Kunstfestival für die freie Kunstszene Berlins. Sämtliche künstlerischen Genres von Performance, Malerei, Fotografie, Skulptur bis Installationen, Intervention, Tanz, Theater und Musik sind auf dem Festival vertreten.






</doc>
<doc id="14461" url="https://de.wikipedia.org/wiki?curid=14461" title="Bezirk Reinickendorf">
Bezirk Reinickendorf

Reinickendorf ist der zwölfte Verwaltungsbezirk von Berlin und hatte am insgesamt Einwohner.

Der flächenmäßig fünftgrößte Bezirk von Berlin ist nach dem Ortsteil Reinickendorf benannt. 

Der Bezirk befindet sich im Nordwesten Berlins. Die Nachbarbezirke sind im Südwesten Spandau, im Süden Charlottenburg-Wilmersdorf, im Südosten Mitte und im Osten Pankow. Der gesamte Norden grenzt an den brandenburgischen Landkreis Oberhavel.

Sein Charakter ist von Wäldern und Gewässern geprägt, seine südlichen Teile haben aber in der Bebauung viele Ähnlichkeiten mit dem angrenzenden Ortsteil Wedding des Bezirks Mitte. Darüber hinaus liegt in Reinickendorf die zwischen 1929 und 1931 erbaute Siedlung "Weiße Stadt" und das von 1963 bis 1974 als Großbausiedlung geschaffene "Märkische Viertel" in dem über 30.000 Menschen wohnen. In den restlichen Ortsteilen herrscht Einzelhausbebauung vor, die in einigen Bereichen wie Hermsdorf und insbesondere Frohnau auch villenartigen Charakter annimmt.

Die Dicke Marie, eine Stieleiche im Tegeler Forst, gilt als ältester Baum in Berlin. Ihr Alter wird auf mehr als 900 Jahre geschätzt.

Der Bezirk unterteilt sich in elf Ortsteile:

Mit dem Groß-Berlin-Gesetz wurden 1920 die sechs Landgemeinden Reinickendorf, Wittenau, Tegel, Heiligensee, Hermsdorf bei Berlin und Lübars, der westliche Teil der Landgemeinde Rosenthal sowie die Gutsbezirke (bzw. Teile davon) Frohnau, Tegel-Schloss, Jungfernheide-Nord und Tegel-Forst-Nord zum Bezirk Reinickendorf zusammengeschlossen.

Der Bezirk Reinickendorf gehörte nach 1945 in der „Vier-Mächte-Stadt“ Berlin zum französischen Sektor und damit zu West-Berlin. 1946 wurde erstmals wieder seit der „Machtergreifung“ Hitlers im Jahr 1933 ein kommunales Parlament in Reinickendorf gewählt.

Die jüngste Entwicklung ist die Ausgestaltung der (Werksarbeiter-)Siedlung Borsigwalde als eigener Ortsteil.

Per zählte der Bezirk Reinickendorf Einwohner und hat daher nach den Bezirken Spandau und Treptow-Köpenick die drittniedrigste Einwohnerzahl aller Berliner Bezirke. Bedingt durch den Anteil von Wasser- und Waldflächen (Tegeler See und -Forst) an der Gesamtfläche von 89,5 Quadratkilometern in Verbindung mit der lockeren Bebauung vieler Ortsteile lag damit am Stichtag die durchschnittliche Bevölkerungsdichte bei Einwohnern pro Quadratkilometer, was ebenfalls den drittniedrigsten Wert aller Berliner Bezirke darstellt. 

→ "Siehe hierzu auch: Liste der Bezirke und Ortsteile Berlins." 

Am 31. Dezember 2016 lag der Ausländeranteil bei 16,2 %, während der Anteil der Bevölkerung mit Migrationshintergrund bei 30,6 % lag. Die Arbeitslosenquote bezifferte sich am 30. Juni 2013 auf 12,8 %. Am 31. Dezember 2016 betrug das Durchschnittsalter der Bevölkerung 44,7 Jahre.

Im Jahr 2013 waren im Bezirk Reinickendorf 9168 Unternehmen registriert. Die Branche des verarbeitenden Gewerbes ist besonders stark vertreten. 
Zu den größten Arbeitgebern (Stand: 2016) im Bezirk gehören unter anderem 


Im Jahr 2012 waren von den 30.862 in Berlin ansässigen Handwerksbetrieben insgesamt 2.279 in Reinickendorf gemeldet.

Die Autobahn A 111 und die Bundesstraße 96 führen durch Reinickendorf.

Die U-Bahn-Linien U6 und U8 sowie die S-Bahn-Linien S1, S25 und S85 erschließen den Bezirk für den ÖPNV.

Der Flughafen Tegel befindet sich im gleichnamigen Ortsteil.

Aus der ersten Wahl zur Bezirksverordnetenversammlung (BVV) ging die SPD als Wahlsieger mit der absoluten Mehrheit hervor. Ihren Höhepunkt erreichte sie bei der Wahl 1948 mit einem Wahlergebnis von 67,9 % der Stimmen – das höchste Wahlergebnis einer Partei in Reinickendorf seither. Die Mehrheit als stärkste Fraktion konnte sie sich durchgehend bis zur Wahl 1981 in der BVV erhalten.

Die Reinickendorfer CDU schaffte es dann 1981, sich die Mehrheit im Parlament zu sichern, 1985 und 1999 sogar die absolute Mehrheit.

Im Jahr 1989 gelang es dem Sozialdemokraten Detlef Dzembritzki für sechs Jahre Bürgermeister zu werden. Bei der Wahl 1995 scheiterte er jedoch an der Christdemokratin Marlies Wanjura, die als erste Frau zur Bezirksbürgermeisterin Reinickendorfs in dieses Amt gewählt wurde. Mit ihr erzielte die CDU 1999 auch ihr höchstes Ergebnis von 56,5 % der abgegebenen Stimmen. 2009 schied Wanjura mit der Wahl ihres Nachfolgers, dem bisherigen Bezirksstadtrat Frank Balzer, aus dem Amt aus.

Seit 2011 bilden CDU und Grüne eine Zählgemeinschaft in der BVV.

Bei den Bundestagswahlen 2013 erwies sich der Bezirk Berlin-Reinickendorf als der „durchschnittlichste Wahlkreis Deutschlands“. Die Ergebnisse von CDU, SPD, Linkspartei, Grünen, FDP und AfD wichen dort nur unwesentlich vom Bundesergebnis ab – im Schnitt um 0,8 Prozentpunkte und damit so gering wie in keinem anderen Wahlkreis.

Das Bezirksamt setzt sich aktuell aus folgenden Stadträten zusammen:

Für das allgemeine Zivilrecht des Bezirks Reinickendorf ist das Amtsgericht Wedding zuständig.

Das heutige Wappen des Bezirks Reinickendorf wurde vom Senat von Berlin am 28. November 1955 verliehen.

Wappenbeschreibung: In schwarzem Schild ein goldener Schrägbalken, belegt mit einem laufenden roten Fuchs und begleitet von sechs (3 : 3) goldenen Ähren. Auf dem Schild ruht eine rote dreitürmige Mauerkrone, deren mittlerer Turm mit einem kleinen Berliner Wappenschild belegt ist.

Wappenbegründung: Das Motiv des „Reinecke Fuchs“ als redendes Wappen, wurde aus dem Wappen Reinickendorfs übernommen. Die goldenen Ähren symbolisieren zum einen die landwirtschaftliche Prägung der ehemaligen selbstständigen Gemeinden des Bezirkes. Zum Anderen symbolisieren sie in ihrer Anzahl die sechs eingemeindeten Gemeinden: Heiligensee, Hermsdorf, Lübars, Reinickendorf, Tegel, Wittenau.

Städtepartnerschaften

Freundschaftliche Kontakte

Über die Städtepartnerschaften hinaus pflegt Reinickendorf freundschaftliche Kontakte mit Breslau, Burkina Faso, Dénia, Katalonien, Kiew, Meseritz, Minsk, Orkney, Sankt Petersburg, Zielenzig, Templewo, Washington, D.C., Wolgograd, Woltschja Gora, Lichtenfels (Bayern), dem Landkreis Oberhavel (Brandenburg), dem Landkreis Schmalkalden-Meiningen (Thüringen) und Zeltingen-Rachtig (Rheinland-Pfalz).

Die Direktion 1 der Berliner Polizei ist für die Bezirke Reinickendorf und Pankow zuständig. Am Flughafen Tegel übernimmt die Bundespolizei Aufgaben des Grenzschutzes und der Luftsicherheit.




Der Sportverein Füchse Berlin Reinickendorf wurde 1891 gegründet und bietet gegenwärtig mehrere Abteilungen an. Die Handball- und Fußballabteilungen gehören zu den bekanntesten. Der aus dem Verein hervorgegangene Profi-Klub Füchse Berlin spielt in der 1. Handball-Bundesliga (Stand: 2017).

Im nördlich gelegenen Ortsteil Lübars befindet sich eine Vielzahl von Reiterhöfen. Reiten als Freizeitaktivität, Springreiten und Dressurreiten haben hier eine lange Tradition. Auch Turnierveranstaltungen im Reitsport werden in Lübars abgehalten.

Reinickendorf zählt mit seinen vielen Wassergebieten zu den beliebten Zielen für Angler. Zahlreiche Anglervereine sind im Bezirk vertreten.

Im Bezirk Reinickendorf gab es im Jahr 2016 mindestens 14 Fitnessstudios. Muskelaufbauprogramme, Ausdauertrainings aber auch Aerobic- und Pilates-Kurse werden dort angeboten.







</doc>
<doc id="14462" url="https://de.wikipedia.org/wiki?curid=14462" title="Wodka">
Wodka

Wodka (aus dem Slawischen, poln. "wódka oder" russ. "wodka"/""; Diminutiv von "woda" für "Wasser", also „Wässerchen“) ist eine meist farblose Spirituose mit einem Alkoholgehalt von idealerweise 40 Volumenprozent. Er zeichnet sich besonders durch seinen fast neutralen Geschmack und das Fehlen jeglicher Fuselöle, künstlicher Aromen oder anderer fermentierter Stoffe aus. Er wird entweder pur getrunken oder in Cocktails vermischt.

Das Wort Wodka kommt aus den slawischen Sprachen. Es ist ein Diminutiv des polnischen Wortes "woda" bzw. des russischen Wortes "" für "Wasser". Früher wurde die Spirituose auf Deutsch "Wutka" genannt, was der polnischen Aussprache ("wódka") am nächsten kommt. Die heutige deutsche Schreib- und Aussprechweise ähnelt am ehesten der niedersorbischen Sprache ("wodka") oder beispielsweise der tschechischen Aussprache ("vodka").

Der erste Wodka wurde laut schriftlicher Erwähnung 1405 im ehemaligem Königreich Polen in Sandomierz gebrannt, dieser hat ein traditionelles noch bis heute in Polen verwendetes Destillationsverfahren. Mit hoher Wahrscheinlichkeit entstand die heutige Art der Wodkaherstellung evolutionär und dank des in agrarisch geprägten Ländern wie Polen oder Russland hohen Überflusses an Roggen. Der frühe Wodka war nur etwa halb so stark wie der heutige.

Vom 16. bis zum 18. Jahrhundert durfte der Wodka in Russland nur in Tavernen verkauft werden, die vom Zaren eine Genehmigung hatten. Da sich dieses Prinzip jedoch nicht bewährte, gab Zar Peter der Große schließlich die Wodkaproduktion frei, ließ diese jedoch besteuern. Katharina II. schränkte das Produktionsrecht wieder ein und nur noch Adelige und Staatsunternehmen durften die Spirituose offiziell herstellen. Im 19. Jahrhundert kam die Kartoffel als Rohstoff auf, und Billigbrände überschwemmten den osteuropäischen Markt. Zum Ende des 19. Jahrhunderts wurde in Russland deshalb wieder das Staatsmonopol für die Produktion eingeführt. Unklar ist, ob der Chemiker und Entwickler des Periodensystems Dmitri Mendelejew auch der „Erfinder“ des „modernen“ Wodkas ist. Bekannt ist aber, dass er die Maßeinheit Gramm für Wodka eingeführt hat, denn 100 Gramm fasst die sogenannte „Stopka“, das traditionelle Wodkaglas. Seine Doktorarbeit hat er zwar zum Thema der Verbindung von Wasser und Ethanol verfasst, diskutierte aber nur Mischungsverhältnisse ab 70% Ethanol.

Als ältester Markenwodka gilt der seit 1823 im polnischen Posen hergestellte "Wyborowa", der ab 1873 auch ins westeuropäische Ausland exportiert wurde. Ab 1874 entwickelte die Versuchs- und Lehranstalt für Spiritusfabrikation des "Vereins der Spiritusfabrikanten in Deutschland" mit dem Adler-Wodka eine erste deutsche Wodka-Marke.

Wodkaproduzenten mit einer langen Tradition sind neben Polen, Russen und Ukrainern auch die Schweden und Finnen. Daneben waren in Norddeutschland dem Wodka sehr ähnliche Kornbrände beliebt. In anderen Nationen war diese Spirituose in der breiten Bevölkerung zum Ende des 19. Jahrhunderts völlig unbekannt. Wodka wurde in Westeuropa lediglich in Adelskreisen als rare osteuropäische Spezialität geschätzt.

Bei Ausbruch des Ersten Weltkrieges 1914 verbot der russische Zar Nikolaus II. den Ausschank und Verkauf von Wodka. Dies führte zu einem Einbruch der Staatseinnahmen um ein Drittel. Als Folge blühte überall die Schwarzbrennerei auf. Beim Sturm auf den Winterpalast im Zuge der Oktoberrevolution wurden auch die Alkoholvorräte des Zaren geplündert.

Die Bolschewiki verboten die Produktion und den Verkauf aller Arten von Alkohol vollständig, mussten dieses Verbot jedoch 1925 wieder aufgeben. Im Zweiten Weltkrieg wurden Wodkarationen für die Soldaten der Roten Armee eingeführt.

Durch die zeitweise Prohibition in Russland wanderten zahlreiche Hersteller von Wodka nach der Oktoberrevolution aus und brachten die Produktion des Wodkas nach Westeuropa, Nordamerika und sogar nach Neuseeland. Zunächst nur in exilrussischen Gemeinden konsumiert, wurde Wodka durch den Cocktailboom ab den 1950er Jahren zu einem Weltgetränk. Vormalige exilrussische Gründungen wie Smirnoff und Gorbatschow wandelten sich ebenso zu Weltmarken wie neu hinzu kommende Westmarken wie Puschkin, eine Tochter von Berentzen.

In den 1980ern gab es in der Sowjetunion Jahre, in denen Juri Andropow und Michail Gorbatschow die Wodkaherstellung stark einschränken ließen. Unter der Regierung von Boris Jelzin wurde zu Beginn der 1990er Jahre die Wodkaproduktion in Russland wieder freigegeben und die Anzahl der Wodkamarken wuchs beträchtlich, teilweise in Kooperation mit westlichen Spirituosenkonzernen.

Wodka kann aus unterschiedlichen, kohlenhydrathaltigen Ausgangsstoffen hergestellt werden. Meist wird Getreide verwendet, aber auch Kartoffeln und Melasse sind üblich. In den meisten Ländern gibt es keinerlei spezielle Beschränkung der möglichen Rohstoffe für Wodka, sofern diese für die Herstellung von Spirituosen im Allgemeinen zugelassen sind. So wird beispielsweise in Australien, Italien, Frankreich oder den Vereinigten Staaten Wodka mitunter aus Weintrauben produziert.

Polen und mehrere nordeuropäische Länder fordern, dass echter Wodka lediglich aus Kartoffeln, Getreide und allenfalls noch Melasse hergestellt werden dürfe. Ein solches Reinheitsgebot gibt es schon in Russland und der Ukraine. Diese beiden Länder sind weltweit (noch vor der EU) die größten Wodkaproduzenten. Der Ausschuss für Umweltfragen, Volksgesundheit und Lebensmittelsicherheit des Europaparlaments einigte sich jedoch am 30. Januar 2007 auf den Vorschlag des deutschen CDU-Abgeordneten Horst Schnellhardt, dass beim Brennen von Wodka auch andere Substanzen verwendet werden dürfen. Allerdings müsse der Ursprungsstoff im Namen getragen werden.

Das traditionelle Getreide zur Wodkaherstellung in Osteuropa ist bis heute Roggen. Daraus hergestellter Wodka schmeckt lieblich, weich, mild, leicht süßlich; daher gilt er als der beste Ausgangsstoff. In westlichen Ländern wird oft Weizen sowie in Skandinavien zum Teil auch Gerste verwendet. Andere Getreidesorten wie Mais oder Reis gehören nicht zu den traditionellen Rohstoffen und werden selten verwendet, wobei Reis allerdings in jüngerer Zeit bei einigen in Europa eher unbekannten asiatischen Wodkaherstellern Verbreitung gefunden hat.

Kartoffeln werden seit dem 19. Jahrhundert verwendet. Der Geschmack des daraus gewonnenen Wodkas ist gewöhnlich schwerer und süßlicher als der des aus Weizen destillierten Wodkas. Vor allem in Polen und der Ukraine werden Wodkas aus Kartoffeln hergestellt.

Melasse – ein Nebenprodukt der Zuckerproduktion – gilt als der billigste und qualitativ schlechteste Rohstoff für Wodka. Der Geschmack des daraus gebrannten Wodkas ist meistens etwas süßer als der von Getreidewodka. Die Verwendung dieses Ausgangsstoffs ist sowohl in den Zuckerrohranbauländern als auch in einigen europäischen Ländern wie z. B. Deutschland oder Tschechien verbreitet. Hieraus hergestellte Wodkas zählen nahezu immer zum unteren Preissegment.

Der erste Schritt zur Wodkagewinnung ist dem des Bierbrauens sehr ähnlich. Man beginnt mit der Herstellung der sogenannten Maische, dem Vermischen des jeweiligen Ausgangsstoffs, also geschrotetes und gemälztes Getreide oder zerkleinerte Kartoffeln, mit Wasser, wobei bei Kartoffeln zusätzlich noch Enzyme z. B. aus Malz zugesetzt werden müssen. Beim anschließenden Erhitzen werden die im Malz enthaltenen Enzyme (v. a. Amylase) aktiv und spalten die Stärkemoleküle auf. Der nun süßen Maische (der Würze) wird Hefe hinzugefügt, um die Gärung in Gang zu setzen. Beim Gären wird der Zucker in der Maische in Alkohol umgewandelt, bis zu einem Gehalt von 6 bis 7 Volumenprozent Alkohol. Danach findet das eigentliche Brennen statt, wobei der sogenannte "Rohalkohol" gewonnen wird. Der Brennvorgang wird stufenweise wiederholt, um die Produktqualität zu verbessern. Der Brennprozess findet dabei kontinuierlich statt, das Maischen hingegen geschieht in Chargen.

Um den Wodka möglichst geschmacksneutral herzustellen, wird das Destillat anschließend filtriert. Dabei werden Begleitaromen entfernt, vor allem die sogenannten Fuselöle. Dazu wird die Flüssigkeit durch Säulen mit Aktivkohle gepumpt, die die unerwünschten Stoffe an sich bindet. Auch biologisch durch Milcheiweiß, durch Einfrieren und Ausfällen von Verunreinigungen und elektrisch kann das Destillat gereinigt werden. Abschließend werden die verbliebenen Schwebeteilchen mit Hilfe von sehr feinporigen Filtrationsanlagen abgetrennt. Der gereinigte Wodka besteht also fast nur noch aus Wasser und Ethanol. Diese Neutralisierung des Geschmacks durch einen Filterprozess unterscheidet den Wodka vom Kornbrand. Die Qualität des Filterprozesses ist entscheidend für den verbleibenden Geschmack des Endproduktes und damit auch dessen Preis. Billigere Produkte können noch Reste an Fuselölen enthalten, wobei umstritten ist, ob diese Stoffe an der Ausbildung eines Katers beteiligt sind. Im Tiermodell Moschusspitzmaus vermögen die Fuselöle einen „Kater“ zu lindern.

Eine Reifung nach dem Brennen ist nicht erforderlich. Die Lagerung bis zur Abfüllung geschieht in Glas-, Stein- oder Edelstahltanks. In einem letzten Arbeitsgang wird der Wodka mit Wasser auf Trinkstärke verschnitten; sie beträgt in Deutschland meist 37,5 oder 40 % Vol.. Traditionell hat Wodka seit Mendelejew 40 % Vol. Alkohol. Moderne Marken schwanken jedoch zwischen 37,5 % (z. B. Smirnoff) und 56 % (Krepkaya), wobei unverdünnt getrunkene Sorten für gewöhnlich 45 % Vol. nicht überschreiten. Das Wasser wird vor der Zugabe meistens ebenfalls gefiltert, bei Premiummarken noch weiter veredelt. Dann wird der Wodka abgefüllt.

Neben den reinen oder puren Wodkas werden inzwischen auch aromatisierte Wodkas hergestellt, indem das Destillat oder der pure Wodka mit Früchten, Gewürzen, Extrakten oder Essenzen angesetzt oder Aromaöle hinzugegeben werden. Die häufigsten Aromatisierungen sind die Typen Zitrone bzw. Lemon und Schwarze Johannisbeere (oft englisch "Black Currant"), die es mittlerweile von den vielen größeren Markenherstellern gibt.

Wodka wird üblicherweise in Flaschen verkauft. In Polen gibt es Wodka in 0,20-l-Flaschen (an EU-Norm angepasst, vorher 0,25 l) in 0,5, 0,7 und seltener in 1-l-Flaschen. In Russland wird er neben den üblichen 0,5-l-Flaschen ebenfalls in kleinen Flaschen mit 0,25 l Inhalt verkauft. Ebenfalls gibt es kleine Gläser mit Einwegverschluss. Eine Zeitlang gab es in Russland Plastikbecher mit 0,1 l Inhalt, diese verschwanden jedoch wieder vom Markt, da der Alkohol Schadstoffe aus dem Plastik löste. In Deutschland werden meist 0,7-l- und 1-l-Flaschen verkauft, letztere gewöhnlich in Großhandlungen und Duty-free-Shops.

Wodka wird in Polen wie auch in Russland meist im Rahmen einer langen Mahlzeit konsumiert. Meist werden viele kleine kalte oder warme Speisen, etwa eingelegte Pilze, Salzgurken, Fleischbällchen, Kartoffelpüree, Roggenbrot und Butter, säuerliches (nicht zu süßes) Obst etc. über einen Tisch verteilt, und jeder isst etwas, während zwischendurch immer wieder ein Wodka getrunken wird. Ebenfalls gehört sehr oft eine halbe Zitronenscheibe dazu, ähnlich wie beim Tequila. Traditionell fassen die Gläser etwa hundert Gramm Wodka (etwa 0,1 Liter), fünfmal so viel wie deutsche Schnapsgläser. Diese traditionellen Gläser wurden jedoch seit dem Ende der Sowjetunion in Osteuropa allmählich von Schnapsgläsern des westlichen Typus verdrängt. Beim Trinken hält man die Luft an und trinkt das Glas auf einen Zug aus, anschließend atmet man tief aus und isst etwas. Wodka ohne Gesellschaft zu trinken, ist in Russland und in Polen verpönt und gilt als Zeichen von Alkoholismus.

In den traditionellen Herstellungsländern wird Wodka gern pur und bei Zimmertemperatur getrunken. In westlichen Ländern wird Wodka daneben mitunter gern zum Mixen von Cocktails und Longdrinks verwendet. So wird Wodka zum Beispiel mit Orangensaft („Screwdriver“), sonstigen Fruchtsäften oder mit Limonaden gemixt. Aufgrund seines neutralen Geschmacks ist er für die Zubereitung einer Bloody Mary unerlässlich. Der in den USA entwickelte Cocktail Moscow Mule trug zu einem großen Teil zum Erfolg des Wodkas in den USA ab den 1950er Jahren bei.




</doc>
<doc id="14470" url="https://de.wikipedia.org/wiki?curid=14470" title="Mediennutzung">
Mediennutzung

Mit Mediennutzung bezeichnet man den Konsum von Medienangeboten insbesondere der Massenmedien. Sie ist Thema der "Publikums-" bzw. "Rezipienten­forschung".

Medien werden in Erwartung einer Belohnung und eines Nutzens konsumiert. Man unterscheidet zwei Arten von Belohnungen, die unmittelbare und die aufgeschobene (Freud; Schramm 1949).

Unmittelbare Belohnung versprechen

Aufgeschobene Belohnung verheißen

Der Medienkonsum in den alten Bundesländern 1977 und 1992 sowie in Deutschland 2005 und 2015. Durchschnittliche Nutzungsdauer pro Tag in Minuten

Nach der Studie "Massenkommunikation" im Auftrag von ARD und ZDF erstellt, stieg der Medienkonsum zwischen 2000 und 2005 um fast 90 Minuten auf zehn Stunden täglich. Nach Daten des Statistischen Bundesamtes betrug die Mediennutzung im Bundesdurchschnitt 2012 ca. 183 Minuten. Das Internet und die Tageszeitung dienen vor allem als Informationsmedien, das Fernsehen als Informations- und Unterhaltungsmedium und das Radio als Tagesbegleiter und „Stimmungsmodulator“.

Nach "TimeBudget 12", einer Langzeitstudie zur Mediennutzung von SevenOne Media (ProSiebenSat1) ist das Internet 2005 mit einem täglichen Nutzungsdurchschnitt von 59 Minuten zum drittwichtigsten Medium aufgestiegen (1999: 9 Minuten). DSL-Nutzer (116 Min.) führen demnach vor ISDN- (45 Min.) und Modemnutzern (41 Min.). Sie führen auch bei der Nutzung von E-Commerce, Banking, Auktionen, Webradio und Onlinespielen. Das Internet ist der Studie zufolge erste Wahl bei der Suche nach Reiseinformationen. Bei Gesundheitsinformationen stieg der Wert von 3 (1999) auf 16 Prozent (2005).

Aktuelle Zahlen liefert eine Untersuchung des Verbandes der deutschen Internetwirtschaft (eco): So geht aus der Studie „Dauer der Mediennutzung in Deutschland von 2006 bis 2012“ hervor, dass die Deutschen im Durchschnitt 70 Minuten täglich online sind. Für 2012 prognostiziert der Verband 80 Minuten.

Mitte 2012 lag der durchschnittliche tägliche TV-Konsum bei 242 Minuten, die Hörfunk-Nutzungsdauer lag bei 191 Minuten und das Internet wurde im Schnitt 83 Minuten täglich genutzt. Die Daten der Langzeitstudie Massenkommunikation führen vor Augen, dass mittlerweile der meiste Radiokonsum außerhalb der Freizeit erfolgt (2010: 151 Min.), während das Fernsehen seit 2005 mit knapp 190 Minuten relativ stabil das mit Abstand beliebteste Freizeitmedium bleibt (während seine Nutzung außerhalb der Freizeit 2010 bei 34 Min. lag). Das Internet wurde mit 55 Minuten 2010 ebenfalls vornehmlich innerhalb der Freizeit genutzt (außerhalb: 33 Min.). Über alle tagesaktuellen Medien hinweg lag die Parallelnutzung für die Gesamtbevölkerung 2010 bei 50 Minuten und nahm damit rund 9 Prozent Anteil an der Bruttomediennutzung ein. Bei den 14- bis 29-Jährigen war dieser Anteil mit 11 Prozent etwas höher.

Der "Tagesspiegel" referierte im September 2015 die seinerzeit neuesten Befunde: Das Fernsehen bleibt mit einer Tagesreichweite von 80 Prozent vorne, gefolgt vom Radio mit 74 Prozent. Das Internet bleibt trotz grassierenden Zuwachses und 46 Prozent Tagesreichweite auf Abstand, die Tageszeitung hält bei 33 Prozent. Im Tagesablauf habe das Radio unverändert die Funktion eines Tagesbegleiters. Fernsehen sei - trotz einer hohen Reichweite schon am Nachmittag - das Abendmedium. Die Tageszeitung sei das Medium für den (frühen) Morgen, während sich die Internetnutzung gleichmäßig über den Tag verteile. Die um den Jahrtausendwechsel beliebte These, dass Fernsehen ein „Auslaufmodell“ sei (und das Radio und die Printmedien es ohnehin seien), habe sich eindrucksvoll nicht bewahrheitet. Der "Tagesspiegel" bewertet den Umfang des Medienkonsums in Deutschland mit den Worten: „Die Deutschen sind Medienjunkies: Neuneinhalb Stunden am Tag sehen sie fern, hören Radio, nutzen das Netz, lesen Zeitung.“

In der ARD/ZDF-Onlinestudie 2015 ist von 44,5 Millionen Menschen die Rede, die täglich das Internet benutzen. Das sind deutlich mehr als die 46 Prozent, die der "Tagesspiegel" angibt.

Was die Nutzung von sozialen Medien angeht, gab es 2018 eine Umfrage des Branchenverbands Bitkom. Neun von zehn Internetnutzern sind in sozialen Netzwerken wie Facebook, Youtube & Co unterwegs, bei den 14- bis 29-Jährigen sind es sogar fast 100 Prozent. Im Schnitt sind Social-Media-Nutzer auf drei Plattformen angemeldet, die jüngeren Befragten sogar bei fünf. Aber auch die Älteren mischen mit: 80 Prozent der 50 bis 64-Jährigen sind bei mindestens einem Netzwerk dabei.

72 % der Österreicher ab 14 Jahren sind aktive Internet-User, womit Österreich im europäischen Vergleich im europäischen Spitzenfeld liegt.

88 Prozent der Schweizer Bevölkerung im Alter zwischen 16 und 74 Jahren nutzen das Internet mindestens einmal wöchentlich (Stand: 2016).

Eine Umfrage des Pew Research Center kam zum Ergebnis, dass in den USA das Internet 2008 erstmals die Druckmedien als Informationsquelle für Nachrichten überrundet hatte. Nur das Fernsehen lag zu der Zeit im allgemeinen Durchschnitt noch vor dem Internet an erster Stelle. Bei Jüngeren jedoch hatte das Internet sogar mit dem Fernsehen gleichgezogen.

Ein wichtiges Problem im Zusammenhang mit der Entwicklung der Medien stellt das Phänomen der „digitalen Kluft“ dar, die sich vor allem zwischen Jüngeren und Älteren öffnet und darin besteht, dass viele Senioren nicht mit IT-Technik (hinreichend gut) umgehen können und/oder wollen.

Je älter ein Befragter in Studien ist, desto geringer ist die Wahrscheinlichkeit, dass er PC und Internet nutzt. Dies liegt zum Teil daran, dass ältere Menschen in ihrem Berufsleben wenig oder erst spät mit diesen Medien in Berührung kamen. Bereits in der ersten Hälfte der 2000er Jahre zeichnete sich aber ab, dass die Altersgruppe der über 55 Jahre Alten den PC zwar nicht so oft wie Jüngere, aber doch häufiger als früher nutzt.

Während seit 2010 praktisch alle 14 bis 19 Jahre Alten in Deutschland zumindest gelegentlich ins Internet gehen (100 Prozent), tut dies erst seit 2015 eine Mehrheit der über 60 Jahre Alten (50,4 Prozent). In allen Altersgruppen unter 60 Jahren benutzt eine Mehrheit täglich das Internet; unter den 50-59-Jährigen beträgt ihr Anteil 56,1 Prozent.

Zum Vergleich: 2003 gingen bereits 92,1 Prozent der 14-19-Jährigen zumindest gelegentlich ins Internet, aber erst 13,3 Prozent der über 60 Jahre Alten.

Offensichtlich schließt sich also allmählich in Deutschland die digitale Kluft.




</doc>
<doc id="14472" url="https://de.wikipedia.org/wiki?curid=14472" title="Gotthardmassiv">
Gotthardmassiv

Das Gotthardmassiv oder "Sankt-Gotthard-Massiv" () ist ein Gebirgsmassiv der Schweizer Alpen in der Region Zentralschweiz. Es ist nach dem Gotthardpass benannt, welcher wiederum den Namen des hl. Godehard von Hildesheim trägt.

Das Gotthardmassiv liegt an der Grenze der Kantone Graubünden, Tessin, Wallis und Uri.

"Gotthardmassiv" ist ein ursprünglich petrologisch-tektonischer Begriff ("Massiv" im Sinne einer kompakten Gesteinsmasse), der schon in der frühen – geologisch orientierten – Alpenforschung des beginnenden 19. Jahrhunderts eingeführt wurde.

Weil auch die Geomorphologie den Begriff des Massivs für sich verwendet (im Sinne einer scharf umgrenzten, in sich wenig gegliederten Bergformation), hat die Bezeichnung auch Eingang in andere Fachgebiete gefunden. Die Gegend bildet das "Herz" der Zentralalpen; von hier streben vier der grossen Alpenflüsse (Rhein, Reuss–Aare, Rhone, Ticino) in alle Himmelsrichtungen und bilden orographisch fundamentale Gliederungen des Alpenbogens.

Auch nach der traditionellen schweizerischen Usanz, Gebirgsgruppen der Alpen nach Kantonen zu benennen, nimmt die Berggruppe im Vierkantonseck eine Sonderstellung ein, sodass die Bezeichnung in der Landesgeographie üblich geworden ist.

Je nach geographischem Konzept werden die Grenzen und die Einordnung des Gotthardmassivs unterschiedlich gesehen.
Relativ übereinstimmend ist bei allen räumlichen Modellen das Kerngebiet, das sich auf gut 20 Kilometer Länge zwischen dem Furkapass () im Westen über den Gotthardpass (Passo del San Gottardo ) bis zum Oberalppass (Alpsu, ) im Nordosten erstreckt, und einerseits in den Muttenhörnern (Grosses ) und andererseits im nach seiner Lage benannten Pizzo Centrale () kulminiert.
Im Westen lässt sich orographisch gesehen die Grenze bis an den Nufenenpass (Passo della Novena, ) ausdehnen, wodurch hier der Pizzo Rotondo () der höchste Punkt ist. Das Massiv im Sinne der Geologie erstreckt sich aber bis Brig-Glis im Oberwallis, daher findet auch die Monte Leone-Gruppe (im weiteren Sinne, Simplongruppe) bis an den Simplonpass  mit dem Monte Leone () bei dieser Gruppe genannt, die dann aber schon weit ins Italienische bis vor Domodossola reicht. Moderne Konzepte der Alpengliederung sehen einen erweiterten Monte-Leone–Sankt-Gotthard-Zug vor (SOIUSA).
Im Osten bietet sich der Lukmanierpass (Passo del Lucomagno, Pass dil Lucmagn, ) als Begrenzung an, dann mit dem Piz Gannaretsch () als höchstem Punkt des Ostteils. Geologisch läuft das Gotthardmassiv aber ebenfalls noch weiter, bis Übersaxen schon tief im Tal des Vorderrheins. Daher findet sich auch der Greinapass (Passo della Greina, Pass Crap ) als Grenze, und der Piz Medel () als höchste Erhebung (Diener).

Geologisch hört das Massiv nördlich in der Urseren (Reussquelltal) am Nordfuss des Gotthardpasses auf. Die meisten Beschreibungen verwenden die orographisch charakteristische Linie Goms (Rhone/Rotten) – Urseren – Surselva (Vorderrhein), doch rechnen manche an Talungen orientierte Gebirgsgliederungen die Berge nördlich (Dammagruppe) noch dazu, womit die Gruppe im Dammastock (), an dem die Rhone ihren Ursprung hat, ihren höchsten Punkt findet.
Als eindeutige Südgrenze gilt das Val Bedretto (Ticinoquelltal) bei Airolo. Geologisch läuft die Südgrenze dann aber über das vom Ritomsee ostwärts streichende Pioratal (dem folgt in etwa der SAC). Orographisch bliebe hier der südstreichende, schon relativ wenig hohe Kamm zwischen Valle Leventina und Val di Blenio (Pizzo del Sole ) übrig, der dann oft zum Gotthardmassiv dazugerechnet wird (Diener, SOIUSA).

Auch innerhalb des Gesamtbaues der Alpen hat die Gruppe eine Sonderstellung, denn hier treffen sich der Alpenhauptkamm – der in den Westalpen wie auch den Ostalpen einen relativ eindeutigen Verlauf hat – und der in der Schweiz durch die Rhone-Rhein-Furche zwischen Martigny und Chur abgesetzte Nebenkamm, in dem die 4000er der Zentralschweiz zu finden sind, und der als solcher als Zentralkamm gesehen wird. Die Europäische Hauptwasserscheide läuft über den Gotthardpass. Viele Gliederungen der Alpen rechnen diese Gruppe deshalb schon zur Alpensüdseite oder zumindest dem inneren (italienseitigen) Alpenbogen, und stellen das Gotthardmassiv zusammen mit den Tessiner Alpen zu den Lepontinischen Alpen (Diener, SOIUSA), andere aber zu den Zentralschweizer Alpen (SAC), wieder andere führen sie aus demselben Grund gänzlich eigenständig (SAC Modern).

Insgesamt finden sich also folgende Umgrenzungen verbreiteterer Systeme (Talungen kursiv, für die streng orographischen Systeme die Hoch- und Tiefpunkte der Umgrenzung mit Höhenangabe):




Die Systematik der biogeographischen Regionen der Schweiz des BUWAL teilt entsprechend den Kantons- beziehungsweise Talungsgrenzen die Zentralalpen (4) aber am Furka-Pass und von der Alpensüdflanke (6) am Hauptkamm, sodass die nordwestlichen Teile des Gotthardmassivs (mit der Talung des Rhonegletschers) zu den Westlichen Zentralalpen (41/WA1), die Nordostteile zu den Östlichen Zentralalpen (42/WA2) und die Südabdachung zu den Südalpen (61/SA1) gerechnet werden.

Das Gotthard-Massiv zerfällt in zwei Regionen, von denen eine westlich und eine östlich des Gotthardpasses liegt.

Die SOIUSA gliedert die Gruppe in:

Die Abgrenzung der drei Untergruppen ist die Süd–Nord-Linie Tremolatal – Gotthard – Gotthardreuss respektive das vom Lago Ritóm ostwärts streichende Pioratal.

Die dritte, südöstliche Nebengruppe, die geologisch nicht mehr Teil des Massivs ist, gehört nach der SAC–Systematik zu den Tessiner Alpen. Die Berge um den Piz Medel zählt Diener zur Gotthardgruppe, der SAC zu den Bünder Alpen, die SOIUSA zu den Adula-Alpen.

Der SAC gliedert in Muttenhörner und Saashörner, Blashorn – Pizzo Gallina, Pizzo Nero – Poncione di Cassina Baggio, Chüebodenhorn – Witenwasserenstock, Pizzo Lucendro – Winterhorn, Pizzo Centrale – Gemsstock, Piz Alv – Badus, Piz Borel – Piz Cavradi, Piz Blas – Piz Paradis, Piz Rondadura – Piz Gannaretsch.

Das Gotthardmassiv gehört neben dem Aarmassiv, dem "Aiguilles Rouges-/Arpille-Massiv" und dem Mont Blanc-Massiv zu den vier "Zentralmassiven" der Schweizer Alpen. Es wird geologisch als Zentralmassiv bezeichnet, weil es zwar als kristallines Grundgebirge gestaucht, aber nicht in den Bau der Helvetischen Decke einbezogen worden ist und deshalb als autochthon gilt. Die speziell intensive Gebirgsbildung beim Gotthardmassiv (hoher Metamorphosegrad, starke innere Verschieferung) hat zu fast senkrechten Kontakten (Gottharddecke mit senkrechten Strukturen) zu den helvetischen Sedimenten an den Massivrändern geführt.

An seinem Nordrand grenzt das Massiv an die "Urserenzone" des Mesozoikum. Diese trennt Aar- und Gotthardmassiv als steile, schmale Furche ab Brig-Glis längs des Rhone-Urseren-Vorderrheintales (Furkapass und Oberalp). Im Vorderrheintal schiebt sich noch das "Tavetscher Zwischenmassiv" (Bugnei-Hügel bei Sedrun) zwischen das Aarmassiv und die Urserenzone. Die Südgrenze des Gotthardmassivs verläuft über Brig, Oberwallis, Nufenenpass, Val Bedretto, Airolo, Val Canada, Val Piora, Piz Scopí, Greina, Piz Tgietschen, Piz da Vrin bis Obersaxen.

Das Massiv kann auf der Ost-West-Achse in die nördliche Paragneiszone, die zentrale Orthogneiszone und die südliche Paragneiszone gegliedert werden. Dazwischen und mehrheitlich in der Orthogneiszone liegen die Granitkörper von Rotondo, Fibbia, Gamsboden und Medelser/Cristallina. Auf dem ganzen Südrand liegen entweder triadische Gesteine oder Bündnerschiefer auf dem Kristallin des Gotthardmassivs.

Die vorherrschende Gesteinsart im Gotthardmassiv ist Gneis.

Johann Wolfgang von Goethe irrte sich bezüglich der Bodenstruktur in seiner Schrift "Über den Granit": Durch eine geologische Überschiebung liegt der Gneis im Gotthardmassiv über jüngeren Schichten, ähnlich wie beim Tauernfenster.

Albert Heim führte 1859 die geologische Bearbeitung des Blattes 14 "Altdorf, Chur", Massstab 1:100'000, der Dufourkarte aus.

Eine umfassende geologische Untersuchung des Massivs nahm Karl von Fritsch um 1870 vor, als er noch Dozent am Zürcher Polytechnikum, der späteren ETH Zürich war. Seine geologische Karte des Gotthardmassivs von 1873 und sein Werk "Das Gotthardgebiet" bildeten eine wesentliche Grundlage für das Projekt des Eisenbahn-Gotthardtunnels, der von 1872 bis 1882 gebaut wurde.

Im Festungsgebiet Gotthard wurden bereits in den 1890er Jahren die ersten Festungen zur Sicherung der Nord-Süd-Verbindung angelegt (Festung Motto Bartola, Forte Airolo, Fort Hospiz). In diesem Raum befanden sich einige der wichtigsten Anlagen des Schweizer Reduits, grosse Festungsbauwerke wie die Festung San Carlo, die Festung Foppa Grande und die Festung Sasso da Pigna, die im Zweiten Weltkrieg nochmals stark ausgebaut oder neu erstellt wurden, um die Schweizer Alpen als Rückzugsraum der Armee gegen einen möglichen Einmarsch der deutschen und der italienischen Truppen zu verteidigen.

Über das Gotthardmassiv führen in nord-südlicher Richtung der Gotthardpass (2108 m) und der Lukmanierpass (1984 m), in ost-westlicher Richtung die Route über den Oberalppass (2044 m), durch das Urserental und über den Furkapass (2431 m).

Durch das Gotthardmassiv sind der Eisenbahn-Scheiteltunnel (1882), der Gotthard-Strassentunnel (1980) und der Gotthard-Basistunnel (2016), der Eisenbahntunnel der NEAT, gebaut worden.

Im Gotthardmassiv liegt die Europäische Hauptwasserscheide zwischen dem Mittelmeer und der Nordsee. Hier liegt auch der Wasserscheidepunkt der Nordsee, des westlichen Mittelmeers und der Adria. Bei den Bergen des Gotthardmassivs entspringen im Osten die beiden Quellflüsse des Rheins (zur Nordsee und zum Atlantik), der Vorderrhein und der Hinterrhein, gegen Norden die Reuss, ein Nebenfluss der Aare, die bei Waldshut und Koblenz in den Rhein mündet, im Westen die Rhone, die beim Rhonegletscher beginnt und ins Mittelmeer mündet, sowie auf der Südseite der Tessin, ein Nebenfluss des Po, der in die Adria, einen Teil des Mittelmeers, fliesst.

Der im August 2012 eröffnete "Vier-Quellen-Weg" ist ein 85 km langer Wanderweg im Gotthardmassiv, der in fünf Etappen zu den Quellen der vier Flüsse Rhein, Reuss, Tessin/Ticino und Rhône/Rotten führt.

Etwa seit der Mitte des 19. Jahrhunderts werden Reliefs der Schweizer Berge für Schulen, Militärs und andere Zwecke erstellt. 2015 kam ein Gotthardmodell bei der Weltausstellung in Mailand (Expo) mit der CNC-Technik aus Granit gefräst hinzu. Das Relief besteht aus fünf nebeneinander liegenden Granitblöcken und entspricht in seiner Grösse von 5 Metern auf 3.20 Metern dem Massstab (1:25‘000) der Landeskarten der Landestopografie. Um diese Miniversion des Gotthards zu erstellen, war … die Hilfe von swisstopo, dem Geoinformationszentrum des Bundes, gefragt.

Geologie, nach Datum:



</doc>
<doc id="14473" url="https://de.wikipedia.org/wiki?curid=14473" title="Bernsteinzimmer">
Bernsteinzimmer

Das Bernsteinzimmer, ein im Auftrag des ersten Preußenkönigs Friedrich I. von Andreas Schlüter gefertigter Raum mit Wandverkleidungen und Möbeln aus Bernsteinelementen, wurde ursprünglich im Berliner Stadtschloss eingebaut. 1716 wurde es vom preußischen König Friedrich Wilhelm I. an den russischen Zaren Peter den Großen verschenkt. Aber erst 1743 wurde das Bernsteinzimmer unter der Regentschaft von Zarin Elisabeth im Winterpalast dauerhaft aufgebaut. Dieselbe Regentin ließ im Jahre 1755 einen Raum für die Bernstein-Paneele im Katharinenpalast in Zarskoje Selo bei Sankt Petersburg einrichten, in dem das Bernsteinzimmer dann fast zwei Jahrhunderte verblieb. Nachdem die Wandverkleidungen und das Interieur 1941 als Kriegsbeute durch die deutschen Besatzer geraubt und ab 1942 im Königsberger Schloss ausgestellt wurden, sind sie seit der ebenfalls kriegsbedingten Evakuierung des Schlosses von 1945 verschollen.

Im Katharinenpalast befindet sich seit 2003 eine originalgetreue Nachbildung des Bernsteinzimmers.

Das Bernsteinzimmer war ursprünglich für das Charlottenburger Schloss bestimmt. Entworfen wurde es von dem Architekten und Bildhauer Andreas Schlüter. Es handelte sich um eine komplette Wandvertäfelung aus Bernstein, die später auch als das „achte Weltwunder“ bezeichnet wurde. Der dänische Bernsteindreher Gottfried Wolffram befand sich aufgrund einer Empfehlung Friedrichs IV. von Dänemark wohl seit 1701 in Diensten Friedrichs I. in Königsberg. Im Jahr 1706 wurde die Ausführung den Danziger Bernsteinmeistern Ernst Schacht und Gottfried Turau übertragen, da Wolfframs Preise als zu hoch empfunden wurden. 1712 wird die Arbeit noch erwähnt, ist dann aber erst nach dem Tode Friedrichs I. teilweise in ein Kabinett am Weißen Saal des Berliner Stadtschlosses eingebaut worden.

Der russische Zar Peter der Große bewunderte das Zimmer bei seinem Besuch in der preußischen Residenz des „Soldatenkönigs“, der im Gegensatz zu seinem Vorgänger für derlei Kunst am Bau wenig übrig hatte, dafür aber „Lange Kerls“ für seine Leibgarde suchte. So kam es mit Zar Peter zum Austausch von Geschenken zur Besiegelung einer Allianz gegen Schweden, und das Zimmer wurde gegen Soldaten mit Gardemaß getauscht. Das wertvolle Geschenk an den russischen Monarchen verursachte bereits damals Schlagzeilen in deutschen Zeitschriften, so z. B. im Journal "Remarquable Curiosa". Die auf Peter I. folgenden Regenten (Katharina I., Peter II., Anna und der Kindkaiser Iwan VI.) nahmen sich des Bernsteinzimmers nicht an. Erst die Tochter Peter I. und Katharina I., Zarin Elisabeth, ließ das Zimmer unter maßgeblicher Beteiligung des am Zarenhof tätigen italienischen Restaurators und Stuckateurmeisters Alexander Martelli umgestalten und in Sankt Petersburg zunächst im Winterpalast installieren, später im Katharinenpalast in Zarskoje Selo. Der im Dienste des russischen Hofes stehende italienische Architekt Bartolomeo Francesco Rastrelli brachte das Zimmer durch Einfügung von Spiegelpilastern und vergoldeten Schnitzereien zu seiner endgültigen Größe.

Im September 1941 wurde der Katharinenpalast von der Wehrmacht als Wohnunterkunft beschlagnahmt. Der sowjetischen Verwaltung war es nicht gelungen, die Wandtafeln abzutransportieren, sie wurden durch Pappe notdürftig gegen Splitter gesichert. Ab dem 14. Oktober 1941 wurde das Bernsteinzimmer im Auftrag des Einsatzstabs Reichsleiter Rosenberg unter Aufsicht des Rittmeisters Ernstotto zu Solms-Laubach und des Hauptmanns Georg Poensgen innerhalb von 36 Stunden demontiert, in 28 Kisten verpackt und nach Königsberg abtransportiert, wo sich die Prussia-Sammlung befand. Am 13. November 1941 berichtete die Königsberger Allgemeine Zeitung ausführlich über eine Ausstellung von Teilen des Bernsteinzimmers im Königsberger Schloss. Ebenso erschien ein Artikel in der Zeitschrift "Pantheon", dessen Fotomaterial offenbarte, dass ein florentinisches Mosaik fehlte. Nach einem Brand in einem Ausstellungsraum des Königsberger Schlosses 1944 wurde die Wandverkleidung demontiert und wahrscheinlich im Keller des Königsschlosses in Kisten eingelagert. Durch zwei britische Luftangriffe auf Königsberg Ende August 1944 wurden wahrscheinlich nur die sechs Sockelverkleidungen beschädigt.

Seit 1945 ist das Bernsteinzimmer verschollen. Über seinen Verbleib gibt es eine kaum noch überschaubare Fülle an Behauptungen, Vermutungen und Spekulationen. In der einschlägigen Literatur werden allein mehrere hundert Orte benannt, wo es verborgen sein soll. Zahlreiche in- und ausländische Forscher haben bisher vergeblich nach dem Bernsteinzimmer gesucht. Fest steht lediglich, dass das Bernsteinzimmer letztmals in Königsberg gesehen worden ist. Unklar ist allerdings, wann. Nach Erkenntnissen der beiden britischen Forscher Adrian Levy und Catherine Scott-Clark soll das Bernsteinzimmer 1945 dort verbrannt sein, und zwar nachdem die Sowjetarmee die Stadt und das Schloss erobert hatte (also nicht schon vorher bei der Bombardierung der Stadt durch die Engländer am 30. August 1944). Das gehe aus bislang unbeachteten Archivdokumenten aus dem Nachlass des sowjetischen Bernsteinzimmer-Beauftragten Anatoli Kutschumow hervor, wird aber von Sachverständigen bezweifelt oder gilt nach heutigem Wissensstand als widerlegt.

Das Schloss von Königsberg, in dem sich das Bernsteinzimmer befand, wurde 1945 stark beschädigt und die Ruine 1968 auf Befehl von Leonid Breschnew abgerissen, um dort das Haus der Sowjets zu errichten. Aufgrund von Statikproblemen wurde dieses Hochhaus nicht fertiggestellt; bisher konnte man sich weder zu einer Fertigstellung noch zu einem Abriss entschließen.

Der Schlossunterbau mit den Kellergewölben, in welchen das Bernsteinzimmer nachweislich eingelagert war, soll zum Teil noch existieren. Der riesige Gebäudekomplex verfügte nach erhaltenen Plänen über tief gelegene Kellerräume, die bis heute noch nicht freigelegt worden sind. Der Verbleib des Bernsteinzimmers in Königsberg wird daher durchaus für denkbar gehalten. Andererseits wird spekuliert, dass russische Stellen Hinweise verbergen wollen, wonach die Sowjetmacht den Verlust des Bernsteinzimmers nicht verhindern konnte.

Das Königsberger Schloss war, so wird von russischen Suchern vermutet, mit dem Dom durch einen unterirdischen Gang verbunden. In seinen Nischen könnten wertvolle Gegenstände gelagert sein. Vermutet wird auch, dass die Zugänge gegen Ende des Zweiten Weltkriegs gesprengt wurden. Auf der Suche nach dem Bernsteinzimmer wollte der russische Katastrophenschutz deshalb 2009 erstmals im neuen Pregel graben ("Königsberger Express", März 2009). Gefunden wurde bisher nichts.

Im Zusammenhang mit dem spektakulären Schwabinger Kunstfund behauptete ein Vetter des Sohnes von Hildebrand Gurlitt im November 2013, dieser wisse, wo sich das Bernsteinzimmer befinde.

Eine neuere Spur führt auf Schloss Friedland in Böhmen. Im Februar 1945 sollen zahlreiche Kisten dorthin verbracht und im Keller des Schlosses eingemauert worden sein.

Seit 2012 führt eine neue Spur in die bergische Großstadt Wuppertal. Karl-Heinz Kleines Theorie besagt, dass der ehemalige Gauleiter in Ostpreußen Erich Koch, in den letzten Kriegsmonaten das Bernsteinzimmer per Zug in seine Heimatstadt liefern ließ. Mittlerweile hat Wilfried Fischer die Leitung der Schatzsuche in Wuppertal übernommen, da Karl-Heinz Klein wieder zurück nach Leipzig gezogen ist.

In der Zeit des Zweiten Weltkrieges ist es unter ungeklärten Umständen zu Diebstählen von einzelnen Ausstattungsstücken des Bernsteinzimmers gekommen. Darauf lässt die Tatsache schließen, dass eine Kommode und ein Steinmosaik, das angeblich bereits 1941 vor der Ankunft in Königsberg gestohlen wurde, Ende der 1990er Jahre in Deutschland aufgefunden wurden. Das Mosaik tauchte 1996 in Norddeutschland auf und wurde auf dem „grauen Kunstmarkt“ für 2,5 Millionen US-Dollar angeboten. Bevor es jedoch zu einem Verkauf kam, wurde das Objekt von der Polizei in Bremen beschlagnahmt. Einige Zeit nach diesem spektakulären Fund meldete sich aufgrund von Presseberichten die Besitzerin der Kommode in Berlin. Diese vermutlich letzten beiden noch erhaltenen Originalteile des Bernsteinzimmers wurden von der Bundesregierung an Russland zurückgegeben.

Im Katharinenpalast wurde ab 1976 an der Rekonstruktion des Bernsteinzimmers gearbeitet, die sich hauptsächlich auf Schwarz-Weiß-Fotos des Originals sowie auf das einzige vorhandene Farbfoto stützte. Nach einer Unterbrechung aufgrund von Finanzierungsproblemen konnten die Arbeiten durch eine Spende der deutschen Ruhrgas AG von 3,5 Millionen Dollar abgeschlossen werden. Im Rahmen des 300-jährigen Stadtjubiläums von Sankt Petersburg wurde das rekonstruierte Bernsteinzimmer am 31. Mai 2003 in einem feierlichen Akt durch den damaligen Bundeskanzler Gerhard Schröder und den russischen Präsidenten Wladimir Putin der Öffentlichkeit übergeben. Heute kann das Bernsteinzimmer im Katharinenpalast besichtigt werden.




</doc>
<doc id="14475" url="https://de.wikipedia.org/wiki?curid=14475" title="Piasten">
Piasten

Die Piasten (nach ihrem legendären Stammvater Piast benannt) waren eine Herrscherdynastie in Polen samt seinen (zeitweiligen) Abspaltungen Masowien und Schlesien, die zwischen dem 10. und 17. Jahrhundert zahlreiche Herzöge und Könige stellte. Sie sollen dem Stamm der Polanen entstammen, der erstmals im Jahr 1000 genannt wurde. 1370 starben die Piasten in der königlichen Linie aus, dreihundert Jahre später die letzte Nebenlinie in Oberschlesien.

Erstes Herrschaftszentrum der Piasten war 940 eine Befestigung in Giecz. Kurz darauf übernahm das nahegelegene Gnesen diese Funktion. Die geschriebene Geschichte Polens begann mit Herzog Misaca, später Mieszko I. genannt, seiner Taufe 966 und der Entwicklung eines ersten Staatsgebildes auf dem Gebiet des polnischen Landschaftsteils Großpolen (Posen-Gnesen-Kalisch) unter den Piasten. Der Name „Polani“ ist erst ab 1015 belegt.

Die zeitweise eigenständigen masowischen Piasten starben 1526 aus und Masowien wurde wieder mit Polen vereint. Die seit dem 14. Jahrhundert der Böhmischen Krone und gleichzeitig dem Römisch-Deutschen Kaiserreich unterstehenden Schlesischen Piasten starben 1675 in männlicher Linie aus.

Durch Eroberungen waren zeitweilig auch Pommern, Böhmen und die beiden Lausitzen Teil des Herrschaftsgebiets der Piasten, später Ruthenien, sowie durch Pfändung die Zips in Oberungarn, der heutigen Slowakei.

Die Nachbarschaft dieses Polens zum Heiligen Römischen Reich bedingte eine teilweise gespannte Koexistenz, in der vereinzelte Piasten (zum Beispiel Mieszko I., Kasimir I. der Erneuerer, Władysław I. (Herman)) durch Treueide oder Tributpflicht gegenüber den römisch-deutschen Kaisern, Ehen mit Vertretern des deutschen Hochadels (Salier, Ottonen) und andere Verträge ihr Staatswesen vor äußeren Eingriffen schützen wollten. In einem etwa 1080 erwähnten päpstlichen Auszug wird ein Dagome Iudex vom Jahre 991/2 genannt, wo, wie man annimmt, Mieszko I. das Reich der Polen in päpstliche Obhut ("Peterspfennig") übergab. Die Titel der später sogenannten Piasten schwankten, je nach Machtposition, zwischen Herzog und König. Weitere einflussreiche Nachbarn des piastischen Polen waren damals das Königreich Böhmen unter den Přemysliden, das Königreich Ungarn unter den Árpáden und den Anjou, das Reich der Kiewer Rus sowie ab dem späten 13. Jahrhundert der Deutsche Orden und das Großfürstentum Litauen.

Mit dem Tod von Bolesław III. Schiefmund 1138 brach in Polen der Partikularismus aus, der für fast 150 Jahre die Geschicke Polens bestimmen sollte. Polen zerbrach in eine Vielzahl zeitweilig einander bekriegender piastischer Herzogtümer, wodurch die politische Stellung und Autorität Polens im Europa des 13. Jahrhunderts stark geschwächt wurde. Mit Bolesławs Tod wurde das Senioratsprinzip rechtlich wirksam, verhinderte aber nicht zahlreiche kriegerische Auseinandersetzungen zwischen den Piastenherrschern der polnischen Teilherzogtümer. Die jeweiligen Beherrscher Krakaus, der Hauptstadt der Senioratsprovinz Kleinpolen, waren "Seniorherzöge", die ihnen unterstellten Familienmitglieder „Juniorherzöge“. Einige dieser Territorialfürsten – besonders Mieszko III., Władysław III. Dünnbein, Leszek I. der Weiße und die schlesischen Sprosse der Dynastie (siehe: Schlesische Piasten) – bestiegen den Krakauer Thron mehrere Male und wurden auch mehrmals abgesetzt.

Das „Goldene Zeitalter der Piasten“ endete, als 1370 die königliche und gleichzeitig jüngste Linie der Piasten, die vom jüngsten Sohn des Bolesław III. abstammte, mit König Kasimir III. dem Großen erlosch. Die masowische Linie erlosch 1526, die schlesische und zugleich älteste Linie blühte und regierte jedoch weiter, wenn auch ab 1348 außerhalb Polens. Davon starb der Teschener Zweig im Jahre 1625 aus. Der letzte legitime männliche Nachkomme des Geschlechts, August Freiherr von Liegnitz, starb 1679. Er entstammte einer nicht standesgemäßen Ehe, weshalb er den Herzogstitel nicht führen durfte und auch nicht erbberechtigt war. Letzter regierender Herzog von Liegnitz, Brieg und Wohlau und zugleich letzter Schlesischer Piast war Herzog Georg Wilhelm. Dessen Schwester Charlotte starb 1707. Sie ruht in der Klosterkirche in Trebnitz neben dem Sarkophag ihrer Ahnin, der Hl. Hedwig von Schlesien.

Zählt man die in Oberschlesien vertretene Linie der Freiherren von Hohenstein zu den Piasten, die allerdings aus einer unehelichen Verbindung hervorgegangen ist, dann war der Urenkel des Teschener Fürsten Adam Wenzel († 1617), Ferdinand II. Freiherr von und zu Hohenstein († 3. April 1706), auch ein männlicher Piast. Dies ist jedoch umstritten, da er nicht als legitimer Spross der Piasten angesehen wird.
Georg-Wilhelm I. (Liegnitz-Brieg-Wohlau) hatte ein uneheliches Kind Martin mit Dorthea Thugendreich von Streit im Feld. Martin von Streit im Feld oder auch Streitenfeld, geb. 21. Januar 1676, führte mit seinen Nachkommen, den Grafen und Freiherrn von Streit die älteste Linie fort und ist als Linie noch existent.

Ab 1370, nach dem Tod Kasimirs des Großen, des letzten Königs aus der Piastendynastie wurde Polen von seinem Neffen, dem ungarischen König Ludwig I. von Anjou (Nebenlinie der Kapetinger) in Personalunion regiert. Den Herrscher sollten nach dessen Tod (1382) seine Töchter beerben. Der polnische Thron ging an die jüngere Hedwig. Aufgrund der gemeinsamen Bedrohung durch den Ordensstaat verbündete sich Polen mit Litauen und die Königin vermählte sich mit dem litauischen Großfürst Jogaila. Die Ehe war kinderlos, nach dem Tod Hedwigs blieb Jogaila auf dem polnischen Thron als Alleinherrscher und wurde Stammvater aller folgenden Jagiellonenkönige. Ihm folgten seine Söhne aus einer späteren Ehe.



"<nowiki>*</nowiki> Aus der böhmischen Dynastie der Přemysliden, mit den Piasten auf der Spindelseite verwandt. Wenzel II. heiratete eine Tochter von König Przemysł II."

GL – großpolnische Linie (Großpolen in Posen, Kalisz und Gnesen), Nachkommen Mieszkos III. des Alten.
JL – jüngste Linie (Masowien, Kujawien, Kleinpolen, Sieradz und Łęczyca), Nachkommen Kasimirs II. des Gerechten.
SL – älteste Linie (Schlesien), Nachkommen Władysławs II. des Vertriebenen.


Andere häufig vorkommende Formen der slawischen Namen der Piastenfürsten in der deutschen Historiographie





</doc>
<doc id="14478" url="https://de.wikipedia.org/wiki?curid=14478" title="Gerechtigkeit">
Gerechtigkeit

Der Begriff der Gerechtigkeit (griechisch: διϰαιοσύνη "dikaiosýne", lateinisch: "iustitia", englisch und französisch: "justice)" bezeichnet seit der antiken Philosophie in ihrem Kern eine menschliche Tugend, siehe Gerechtigkeitstheorien. Gerechtigkeit ist nach dieser klassischen Auffassung ein Maßstab für ein individuelles menschliches Verhalten.

Die Grundbedingung dafür, dass ein menschliches Verhalten als gerecht gilt, ist, dass Gleiches gleich und Ungleiches ungleich behandelt wird. Wobei in dieser Grunddefinition offen bleibt, nach welchen Wertmaßstäben zwei Einzelfälle als zueinander gleich oder ungleich zu gelten haben.

In der Erkenntnis, dass kein Mensch für sich beanspruchen kann, stets und unter allen Gesichtspunkten gerecht zu handeln, setzte sich im Mittelalter die Auffassung durch, wonach Gerechtigkeit keine menschliche, sondern eine göttliche Größe sei. Gerechtigkeit konnte es nach dieser Auffassung nur im Himmel und nicht auf Erden geben. In der Renaissance wurde die Göttlichkeit der Gerechtigkeit durch die Idee eines Naturrechts ersetzt. Die Gerechtigkeit sei im Prinzip in der Natur schon angelegt und der Mensch müsse danach streben, diese Gerechtigkeit zu erkennen.

Hiergegen hat der Philosoph Immanuel Kant aus der Position der Aufklärung heraus seine Vernunftethik formuliert. Eine göttliche oder naturgegebene Gerechtigkeit seien keine vernünftigen Kategorien, weil beide für den Menschen grundsätzlich nicht oder jedenfalls nicht vollständig erkennbar seien. Gerecht handelt nach dem kategorischen Imperativ derjenige Mensch, der sich über die Maximen seines Handelns unter Anspannung seiner Geisteskräfte Rechenschaft ablegt und entsprechend handelt, sofern diese Maximen seines Handelns auch zum allgemeinen Gesetz erhoben werden können.

Zum modernen Gerechtigkeitsbegriff gehört auch, dass dieser nicht nur auf einzelne Handlungen von Menschen angewandt wird, sondern gerade auch auf die Summe und das Zusammenwirken einer Vielzahl menschlicher Handlungen in einer Gesellschaftsordnung. Abstrakt ist eine Gesellschaftsordnung dann gerecht, wenn sie so ausgestaltet ist, dass die einzelnen Individuen frei sind, sich gerecht zu verhalten.

Eine weitere Erweiterung erfährt der Gerechtigkeitsbegriff unter sozialen Gesichtspunkten hin zur sozialen Gerechtigkeit. Dieser Begriff bezeichnet keine menschliche Tugend mehr, sondern einen Zustand einer Gesellschaft, wobei es nicht darum geht, dass die Individuen in dieser Gesellschaft frei sind, sich selbst gerecht - im Sinne von tugendhaft - zu verhalten, sondern darum, dass jedem Mitglied der Gesellschaft die Teilhabe an der Gesellschaft durch die Gewährung von Rechten und möglicherweise auch materiellen Mitteln ermöglicht wird.

Gerechtigkeit wird weltweit als Grundnorm menschlichen Zusammenlebens betrachtet; daher berufen sich in quasi allen Staaten Gesetzgebung und Rechtsprechung auf sie. Sie ist in der Ethik, in der Rechts- und Sozialphilosophie sowie in der Moraltheologie ein zentrales Thema bei der Suche nach moralischen und rechtlichen Maßstäben und für die Bewertung sozialer Verhältnisse.

Nach Platons Verständnis ist Gerechtigkeit eine innere Einstellung. Sie ist für ihn die herausragende Tugend (Kardinaltugend), der entsprechend jeder das tut, was seine Aufgabe ist, und die drei Seelenteile des Menschen (das Begehrende, das Muthafte und das Vernünftige) im richtigen Verhältnis zueinander stehen. Aristoteles und Thomas von Aquin betonten hingegen, dass Gerechtigkeit nicht nur eine (Charakter-)Tugend, sondern stets in Bezug auf andere zu denken sei (Intersubjektivität). Handlungen wie Wohltätigkeit, Barmherzigkeit, Dankbarkeit oder Karitas gehen über den Bereich der Gerechtigkeit hinaus (Supererogation).

In den neueren Gerechtigkeitstheorien stehen sich Egalitarismus, Libertarismus und Kommunitarismus als Grundpositionen gegenüber.

Globalisierung, weltwirtschaftliche Probleme, Klimawandel und demographische Entwicklungen haben dazu beigetragen, dass neben Fragen innerstaatlicher sozialer Gerechtigkeit auch die nach Generationengerechtigkeit und nach einer gerechten Weltordnung in den Vordergrund rücken.

Die Grunddefinition von gerechtem Handeln, Gleiches gleich und Ungleiches ungleich zu behandeln, ist lediglich formaler Natur. Ob zwei Situationen als zueinander gleich oder ungleich bewertet werden, hängt von den zugrunde gelegten Wertmaßstäben ab. Der Gerechtigkeitsbegriff ist also stets ausfüllungsbedürftig.

Beispiel 1: Frauenwahlrecht. Frauen wurde in der Frühzeit der Demokratie kein Wahlrecht zugestanden. Dies wurde nicht als ungerecht empfunden, weil Frauen keine Männer sind. Es wird also Ungleiches ungleich behandelt, was grundsätzlich als gerecht erscheint. Nach heutigen Wertmaßstäben gibt es im Hinblick auf das Wahlrecht keinen Unterschied zwischen Frauen und Männern. Frauen unterliegen genau wie Männer den demokratischen Entscheidungen, sie tragen die Gesellschaft ebenso wie die Männer und sie sind ebenso wie Männer vernunftbegabt und zu demokratischen Entscheidungen fähig. Daher ist es ungerecht, wenn ihnen, obwohl sie im Hinblick auf die Demokratiefähigkeit Männern tatsächlich gleich stehen, nicht auch das Recht zur Teilhabe zugestanden wird.

Beispiel 2: gleicher Lohn. Die Bibel kennt das Gleichnis von den Arbeitern im Weinberg als klassische Erzählung zur Gerechtigkeit. Der Herr des Weinbergs erteilt jedem Arbeiter den aus Sicht des Herrn gerechten Lohn, nämlich das, was dieser zum Leben braucht. Er unterscheidet dabei nicht, dass einige Arbeiter zwölf Stunden gearbeitet haben, andere aber nur eine. Letztere erhalten also einen zwölffach höheren Stundenlohn. Je nachdem, ob die freie Entscheidung des Herrn, die Bedürfnisse der Arbeiter oder deren Leistung als Maßstab herangezogen werden, erscheint die Entlohnung als gerecht oder auch als ungerecht.

In den allgemeinen Gerechtigkeitsbegriff gehen also unterschiedliche Wertvorstellungen ein. Dies birgt nach John Rawls die Gefahr, dass in einer politischen Auseinandersetzung über Gerechtigkeit diejenigen Wertvorstellungen besonders in den Vordergrund gerückt werden, die den eigenen (Standes-)Interessen besonders förderlich sind. Für eine offene Diskussion über Gerechtigkeit und die hierzu zugrunde zu legenden Wertvorstellungen fordert er daher den Schleier des Nichtwissens. Nur wer unabhängig von seinen Interessen argumentiert, hat eine Chance, einen vernünftigen Ausgleich der verschiedenen Wertvorstellungen, die in einen modernen Gerechtigkeitsbegriff einfließen sollen, zu erdenken. Insbesondere das Verhältnis von Leistungsgerechtigkeit, wonach derjenige, der mehr leistet als andere, auch besser leben solle, zu einer egalitären Gerechtigkeit, wonach alle Menschen, mit ähnlichen Bedürfnissen versehen, auch ähnliche materielle Möglichkeiten haben sollten, soll nach Rawls in Unkenntnis über die eigene Leistungsfähigkeit bzw. unter Außerachtlassung derselben diskutiert werden.

Im Althochdeutschen ist das Adjektiv „gireht“ erstmals im 8. Jahrhundert nachzuweisen. Es bedeutete „gerade“, „richtig“ „passend“ (stärkere Form von „reht“), beim mittelhochdeutschen „gereht“ kommt die abstraktere Bedeutung „dem Rechtsgefühl entsprechend“ hinzu, wie bereits zuvor im Gotischen „garaihts“. Später steht „gerecht“ auch für „gradlinig“, „angemessen“ und „gemäß“.

Gerechtigkeit ist ein normativer, mit einem Sollen verbundener Begriff. Mit ihm ist die Aufforderung verbunden, ungerechte Zustände in gerechte umzuwandeln. Wer gerecht sein will, hat die Pflicht gegenüber sich selbst, aber auch in der Erwartung der Anderen, entsprechend zu handeln. Wenn man Gerechtigkeit als Gebot der Sittlichkeit anerkennt, trägt man einen Teil der Verantwortung dafür, dass gerechte Verhältnisse hergestellt werden.

"Ungerechtigkeit" ist eine Verletzung der Gerechtigkeit. Zur Ungerechtigkeit gehört auch die Unterlassung einer pflichtgemäßen Handlung.
"Willkür" ist einer der Hauptgründe für Ungerechtigkeit, weil durch sie das Prinzip der Unparteilichkeit durchbrochen wird.

Der Begriff der Gerechtigkeit wird in unterschiedlichen Zusammenhängen benutzt, etwa bezogen auf
Gelegentlich werden Institutionen oder sogar Emotionen (gerechter Zorn) als gerecht bezeichnet.

Gerechtigkeit als Prinzip einer ausgleichenden Ordnung in einer Gesellschaft findet sich in allen Kulturen und ist historisch sehr weit zurückzuverfolgen. Ursprünglich wurde Gerechtigkeit als das Einhalten von sozialen Normen und Gesetzen aufgefasst. Dabei betrachtete man die gesellschaftliche Ordnung als Naturprinzip (Naturrecht) oder als Setzung transzendenter Mächte, wie beispielsweise einer Gottheit, die als personifizierte Gerechtigkeit angesehen wurde oder der diese Eigenschaft zumindest als wesentlich zugesprochen wurde. Gerecht zu sein hieß somit, die Gebote Gottes beziehungsweise der Götter zu erfüllen.

In frühen Kulturen wurden Begriffe verwendet, die heute nur ungenau und zu eng mit „gerecht“ übersetzt werden. Sie waren religiös geprägt und beinhalteten auch Bedeutungen wie rechtschaffen oder weise, so in der ägyptischen Ma’at-Lehre oder dem alt-israelischen Begriff der Sädäq (Gemeinschaftstreue). Ähnlich weit gefasst ist auch das „Yi“ (义,Rechtschaffenheit), eine der vier Säulen des Lunyu im Konfuzianismus, das eine Haltung fordert, die man vor sich selbst rechtfertigen kann. Gerechtigkeit wurde in diesen traditionellen Lehren vor allem als personale Gerechtigkeit, als Eigenschaft und Tugend eines Menschen innerhalb des Herrschaftsgefüges verstanden, die zur Aufrechterhaltung der vorgegebenen Ordnung beitragen sollte.

In der Philosophie der Antike finden sich die ersten systematischen Betrachtungen über die Gerechtigkeit bei Platon und Aristoteles. Vor allem Aristoteles traf die Unterscheidung zwischen personaler und gesellschaftlicher Gerechtigkeit als Bürgertugend. Diese Auffassung der personalen Gerechtigkeit war bis ins Mittelalter vorherrschend. Erst mit Beginn der Neuzeit entstanden ausgearbeitete Konzepte, Gerechtigkeit als Vertragsbeziehung zwischen Menschen zur Lösung von Konflikten zu bestimmen, so in den Vorstellungen vom Gesellschaftsvertrag Mitte des 17. Jahrhunderts bei Thomas Hobbes oder ungefähr ein Jahrhundert später im Zeitalter der Aufklärung bei Jean-Jacques Rousseau. Recht wurde nun nicht mehr nur als Ausdruck einer göttlichen Ordnung aufgefasst. Gerechtigkeit erhielt die Bedeutung einer Institution zum Ausgleich unterschiedlicher Interessen. Entsprechend wurde die Bestimmung als personale Gerechtigkeit von der Sicht einer institutionellen Gerechtigkeit, der "iustitia legalis", verdrängt.

Eine inhaltliche Erweiterung und Verschiebung erfuhr der Begriff der Gerechtigkeit mit der industriellen Revolution und der damit einhergehenden Verarmung („Pauperisierung“) großer Bevölkerungsteile, durch die die Soziale Frage aufgeworfen wurde.
Hegel, der die „Erzeugung des Pöbels“ durch die wirtschaftlichen Verhältnisse konstatierte, reflektierte die Problematik philosophisch und forderte eine Abschaffung der „Notdurft“ durch die Öffentlichkeit. In der entstehenden Arbeiterbewegung konkretisierte sich dies in der Forderung nach sozialer Gerechtigkeit, die so bis in die Gegenwart zum Inhalt politischer Auseinandersetzungen geworden ist.

Es gibt historisch gesehen einen starken "Wandel wertbezogener Postulate". Zwar wird im rechtsphilosophischen Diskurs der Begriff der Gerechtigkeit oftmals im Singular verwendet. Hiergegen wird jedoch eingewandt, dass dies nach den Erfahrungen zahlreicher System- und Verfassungswechsel eine Illusion sei. Danach ist der Inhalt des Begriffs der Gerechtigkeit in Geschichte und Gegenwart von religiösen oder weltanschaulichen Vorverständnissen bestimmt und wechselt in hohem Maße mit dem Wandel der Kulturen und der politisch etablierten Wertvorstellungen. Wer seine Gerechtigkeit als „die“ Gerechtigkeit einfordert, verkenne die Subjektivität und Relativität wertbezogener Postulate. In einer freiheitlichen Staats- und Gesellschaftsordnung existiere Gerechtigkeit nur im Plural, nämlich als Abbild der unterschiedlichen Gerechtigkeitsideale in der Gesellschaft und als Wettbewerb um mehrheitsfähige Lösungen von Gestaltungs- und Regelungsproblemen.

Auf die Gefahr, dass ob der Allgegenwart der Kategorie Gerechtigkeit in allen Religionen, Philosophien und Weltanschauungen schließlich nur mehr eine "Wortfassade" übrig bleibt, hat Ernst Topitsch hingewiesen. Er postulierte „die Tatsache, dass bestimmte sprachliche Formeln durch die Jahrhunderte als belangvolle Einsichten oder sogar als fundamentale Prinzipien des Seins, Erkennens und Wertens anerkannt wurden und es heute noch werden – … gerade weil und insofern sie keinen, oder keinen näher angebbaren Sach- oder Normengehalt besitzen.“ Zu einer ähnlichen Einschätzung aus der Sicht des Rechtspositivismus kam auch Hans Kelsen: "Die Bestimmung der absoluten Werte im Allgemeinen und die Definition der Gerechtigkeit im Besonderen, die auf diesem Wege erzielt werden, erweisen sich als völlig leere Formeln, durch die jede beliebige gesellschaftliche Ordnung als gerecht gerechtfertigt werden kann." Ebenso war für Max Weber das Postulat der Gerechtigkeit „aus ‚ethischen‘ Prämissen unaustragbar“ Auch für den Systemtheoretiker und Konstruktivisten Niklas Luhmann bleibt die Frage der Gerechtigkeit auf das Rechtssystem beschränkt. Für ihn kann „die Idee der Gerechtigkeit als Kontingenzformel des Rechtssystems aufgefasst werden“, weil „die Voraussetzungen eines naturrechtlichen Gerechtigkeitsbegriffs entfallen sind.“

Damit Gerechtigkeit als angemessener Ausgleich der in jeder historischen Gesellschaft existierenden vielfältigen Unterschiede weitgehend wirksam werden kann, ist es eine notwendige Voraussetzung, dass die vorhandenen Interessen und moralischen Bewertungen offen und uneingeschränkt kommuniziert werden können. Sie werden in einer offenen Diskussion erörtert und anschließend in einem politischen Prozess, der unterschiedlich gestaltet sein kann, in gültige, aber veränderbare Rechtsnormen bzw. Vereinbarungen umgesetzt.

Werden die gesellschaftlichen Normen heteronom (von außen, fremdbestimmt) zum Beispiel durch einen (diktatorischen) Herrscher oder eine Herrschafts-Elite (zum Beispiel Aristokratie) vorgegeben, sind die Menschen von den Interessen und der Macht Weniger oder Einzelner abhängig und können keinen gleichberechtigten Diskurs führen.

Zum Zweiten muss als formales Grundprinzip die "Gleichheit (Gleichberechtigung)" der Menschen sichergestellt sein.

Auf dem Gleichheitsprinzip beruht auch der sogenannte Minderheitenschutz. Durch ihn soll gewährleistet werden, dass nicht eine Mehrheit Einzelne oder Minderheiten in Hinblick auf sexuelle Orientierung, Geschlecht, Religion, Rasse oder andere Bedingungen per Mehrheitsbeschluss dauerhaft dominiert.

In unterschiedlichen Bereichen des menschlichen Zusammenlebens spielen verschiedene Gerechtigkeitskonzepte eine Rolle. Sie sind abhängig von den Adressaten sowie von den jeweiligen gesellschaftlichen Voraussetzungen:

Wird der Gerechtigkeitsbegriff nicht auf ein individuelles Verhalten angewandt, sondern auf eine Gesellschaftsordnung, dann gibt es gedanklich zwei Möglichkeiten: Entweder die Gesellschaftsordnung wird als Summe der menschlichen Handlungen verstanden, wobei dann auch Handlungen von Menschen aus der Vergangenheit mitwirken (z. B. durch Gesetze und Institutione, die in der Vergangenheit geschaffen wurden) oder aber die Gesellschaftsordnung wird als Ganzes unter dem Gesichtspunkt eines zu definierenden und hierauf anzuwendenden Gerechtigkeitsbegriffs beurteilt. Letzteres ist dann ein systemischer Gerechtigkeitsbegriff der sich schon aus der Herangehensweise grundsätzlich vom klassischen Tugendbegriff unterscheidet und mit diesem nicht gleichgesetzt werden darf.

Bei systemischer Gerechtigkeit bleibt allerdings die Kerndefinition von Gerechtigkeit erhalten, dass Gleiches gleich und Ungleiches ungleich behandelt werden müsse. Auch hier stellt sich die Frage, nach welchen Wertmaßstäben in einer Gesellschaftsordnung zwei Fälle als gleich oder als ungleich bewertet werden. Hinzu kommt, dass der im Gerechtigkeitsbegriff enthaltene Handlungsappell nun einen Adressaten benötigt. Wird eine Situation systemisch als ungerecht beurteilt, steht noch nicht fest, wer sich dann wie verhalten soll. 
So kann eine akute Notsituation aufgrund einer Naturkatastrophe zunächst einmal nicht als gerecht oder ungerecht bewertet werden, weil eine Naturkatastrophe kein menschliches Verhalten ist. Wenn diese Not aber durch Handlungen von Menschen bewältigt oder gelindert werden kann, dann kann ein Gerechtigkeitsbegriff, der Empathie und Barmherzigkeit beinhaltet, wie von Thomas von Aquin gefordert, das Gebot, Hilfe zu leisten, als gerecht erklären. Systemisch stellt sich dann aber die nächste Frage, wer sich gerecht verhalten solle: Ist dies der Staat, vielleicht eine Gliederung oder eine Institution desselben, ist es die internationale Staatengemeinschaft, ist es der Nächste, der dem Leidenden räumlich oder aufgrund persönlicher Beziehungen am nächsten steht, ist es eine wie auch immer zu definierende Volksgemeinschaft, ist es Jedermann, der von dem Leid weiß, in gleicher Weise, oder ist Hilfe gerechter Weise am ehesten durch eine Versichertengemeinschaft zu organisieren?

Vor der Frage des Adressaten steht aber bei systemischer Gerechtigkeit in besonderem Maße die Frage, welche Werte den Gerechtigkeitsbegriff ausfüllen. Zwar scheiden offenkundig egoistische Zielsetzungen, die sich z. B. in Korruption und Willkür äußern, schon aus der Grunddefinition von Gerechtigkeit heraus, darüber hinaus lassen sich aus der Grunddefinition aber noch keine allgemeinen Ziele ableiten. Quais jede wohlmeinende Zielsetzung für eine Gesellschaftsordnung lässt sich auch als gerecht definieren. Wenn ein als gut erkannten Ziel eine Ungleichbehandlung rechtfertigt, dann passt die Bevorzugung unter den Aspekt, dass Ungleiches ungleich behandelt werden müsse.

Vor diesem Hintergrund lassen sich verschiedene als gerecht postulierte Zielsetzungen zu unterschiedlichen Gerechtigkeitsbegriffen zusammenfassen, die dann ihrerseits wieder in unterschiedlicher Weise definiert werden können. Dies sind z.B.:

- egalitäre Gerechtigkeit

- soziale Gerechtigkeit

- Leistungsgerechtigkeit

- ökologische Gerechtigkeit

- Generationengerechtigkeit

- Vertragsgerechtigkeit

- Verfahrensgerechtigkeit

Dabei geraten diese unterschiedlichen Gerechtigkeitsbegriffe in Konflikt, wodurch offengelegt wird, dass unterschiedliche Werte in einer Gesellschaft zu Konflikten, insbesondere auch Interessenskonflikten führen.

Beispielsweise kann die Subventionierung von Gütern, die nur von wohlhabenden Menschen erworben werden, unter dem Gesichtspunkt des Egalitarismus als ungerecht, unter anderen, z. B. ökologischen Gesichtspunkten aber als Gerecht erscheinen. Dies gilt z. B. für Photovoltaikanlagen, die nur Hausbesitzer erwerben oder teure Elektroautos, die ebenfalls nicht von Geringverdienern erworben werden. Auch kann die in Deutschland eingeführte als Lebensleistungsrente bezeichnete Rente mit 63, die fast ausschließlich Menschen zugute kommt, die ohnehin eine weit überdurchschnittliche Rente zu erwarten haben, als leistungsgerecht angesehen werden, auch wenn sie egalitären Überlegungen und Überlegungen der Generationengerechtigkeit diametral zuwider läuft.

Die soziale Funktion von Debatten und Konzeptionen zur Gerechtigkeit besteht darin, innerhalb menschlicher Beziehungen Werturteile über Verteilungen bzw. Zuteilungen zu ermöglichen. Maßstab dafür kann sein, was jemand nach eigener Auffassung oder der anderer "benötigt", worauf er ein "Recht" hat, oder was er "verdient".

Es gibt vielfältige Kriterien, nach denen das Maß der Gerechtigkeit beurteilt werden kann. Die Festlegung solcher Kriterien erfolgt dabei nach unterschiedlichen Distributionsprinzipien, die oftmals in Abhängigkeit von der konkreten Entscheidungssituation gewählt werden:

Gegenüber dem Prinzip der Distribution, das auf den Empfänger ausgerichtet ist, bezieht sich das Prinzip der Subsidiarität auf den Geber von Gütern und Leistungen. Es besagt, dass sich zunächst jeder so weit wie möglich selbst helfen soll. Dies umfasst die Pflicht des Einzelnen, seinen Beitrag zur Gemeinschaft nach den ihm gegebenen Möglichkeiten zu leisten. Erst wenn auf diesem Wege (Grund-)Bedürfnisse nicht erfüllt werden, ist die Gemeinschaft verpflichtet, einen Ausgleich zu schaffen. Der Gedanke der Subsidiarität liegt zum Beispiel der deutschen Sozialhilfe zugrunde. Überdies besagt das Subsidiaritätsprinzip, dass die höhere Ebene der Gemeinschaft dort nicht zuständig ist, wo die untergeordnete Ebene Aufgaben selbstverantwortlich lösen kann. In diesem Sinn ist das Prinzip grundlegend für die Beziehungen von Bund, Ländern und Gemeinden in Deutschland, aber auch für die Institutionen der Europäischen Union gegenüber den Mitgliedsstaaten.

Die Vielzahl der Vorstellungen zur Gerechtigkeit zeigt die Breite der Thematik und zugleich deren Problematik. Keines der einzelnen Prinzipien ist geeignet, alle konträren Interessenlagen zur Zufriedenheit aller zu lösen. Vertreter einzelner Ansätze neigen dazu, vor allem die Nachteile alternativer Entwürfe aufzuzeigen. Je nach Begründung der unterschiedlichen Postulate, die im Zusammenhang mit der Lebenswelt ihrer Urheber stehen, kommt es zu unterschiedlichen Werturteilen. Die Frage der Gewichtung ist für viele praktische Lebensbereiche bedeutsam, wenn es darum geht, als ungerecht erachtete Verhältnisse zu korrigieren. Dies betrifft Bildungschancen ebenso wie die Mitbestimmung in Unternehmen, die Steuergerechtigkeit, einen gerechten Lohn oder die Bemessung gerechter Strafen. Der schon in der Antike formulierte Maßstab „Jedem das Seine“ (Suum cuique) gibt einen Anhaltspunkt, löst aber weder das Verteilungsproblem (Quantifizierung) noch Interessenkonflikte. Auf die Gefahr der missbräuchlichen Verwendung des Begriffs verweist Ludwig Erhard: „Ich habe es mir angewöhnt, das Wort Gerechtigkeit fast immer nur in Anführungszeichen auszusprechen, weil ich erfahren habe, daß mit keinem Wort mehr Mißbrauch getrieben wird als gerade mit diesem höchsten Wert.“

Gerechtigkeit war schon immer ein zentrales Thema der politischen Philosophie. So verweist B. Sitter auf den antiken Vorsokratiker Anaximander, der Gerechtigkeit im umfassenden Sinne verstand, als kosmisches Ordnungsprinzip und als Ideal des menschlichen Verhaltens gegenüber allem Seienden. Die Frage nach der Gerechtigkeit bestimmt auch in der Gegenwart wesentlich das politische Denken und die Themen der praktischen Politik. Dabei sind in den westlichen Industrieländern neben die klassischen Auseinandersetzungen um innerstaatliche soziale Gerechtigkeit, die vom Ringen um die Lösung der Sozialen Frage und um die Schaffung und Entwicklung eines sozialen Sicherungssystems mit seinen verschiedenen Zweigen geprägt waren und sind, Fragen der Gleichberechtigung der Geschlechter, der kulturellen und individuellen Selbstbestimmung sowie der Gerechtigkeit gegenüber Tieren und der Natur getreten.

Vor allem aber hat die immer engere Verflechtung durch die Globalisierung das Problembewusstsein hinsichtlich der internationalen Verteilungsgerechtigkeit, der Verwirklichung von Gerechtigkeit durch Menschenrechte und durch eine gerechte politische Ordnung weltweit geschärft. Standen in der internationalen Diplomatie und Verständigungspolitik traditionell Kriegsverhütung, Friedensschlüsse und nationale Handelsinteressen im Mittelpunkt, so ist die Agenda (Tagesordnung) der Vereinten Nationen sowie die der internationalen Gipfeltreffen und -foren (Weltwirtschaftsforum, Weltsozialforum) heute zunehmend mit Problemen der Armut, des Klimaschutzes, der Migration sowie der weltweiten Verlagerung von Kapitalströmen, Unternehmensinvestitionen und von branchenbezogenen Arbeitsplätzen befasst.

Auch über die Frage, ob es „gerechte Kriege“ geben kann, wurde und wird im Rahmen einer internationalen Sicherheitspolitik und darüber hinaus immer wieder neu diskutiert.

Jürgen Habermas u. a. stellen in diesem Zusammenhang die Frage nach einer Weltinnenpolitik. Otfried Höffe schwebt sogar in Anlehnung an Kant eine „Weltrepublik“ vor. Konkurrierende Positionen betonen den Vorrang ökonomischer, sozialer und kultureller Gerechtigkeit. Eine gerechtere Weltordnung ist, darüber besteht ein weitgehender Konsens, nur im globalen Zusammenwirken erreichbar.

Eine vertiefte Darstellung einzelner Forschungsthemen findet sich im "Hauptartikel Gerechtigkeitsforschung".

Eine Rolle spielen für die Untersuchungen zur Gerechtigkeit auch die unterschiedliche Einstellungen zum Gegenstand (Psychologie), und inwieweit diese in den gegebenen gesellschaftlichen Verhältnissen widergespiegelt werden (Sozialwissenschaften). Dabei werden folgende Fragestellungen bearbeitet:

Aus psychologischer Sicht interessiert insbesondere, welche Faktoren die Haltung eines Menschen in Hinblick auf seine Gerechtigkeitsvorstellung beeinflussen und welche Auswirkungen als ungerecht beurteilte Sachverhalte haben. Welchen Einfluss hat die Moralerziehung? Wie wirken sich welche Verfahrensprinzipien und Verteilungsnormen auf das Gerechtigkeitsempfinden aus? Dabei werden zumeist Erhebungen mit den Methoden der empirischen Sozialforschung durchgeführt.

Wegbereiter der Gerechtigkeitsforschung in der Sozialpsychologie waren in den 1950er Jahren die Theorie der kognitiven Dissonanz von Leon Festinger und seine soziale Vergleichstheorie. George C. Homans führte erstmals ein Konzept der Verteilungsgerechtigkeit in seiner Austauschtheorie des sozialen Verhaltens ein. Bedeutende Theorien aus diesen Forschungsbereichen sind die Equity-Theorie von J. Stacy Adams. und die Gerechtigkeitsmotivtheorie von Melvin Lerner, die in Deutschland von Leo Montada vertreten wird. Die sozialpsychologische Forschung beschäftigt sich mit dem Entstehen, Erleben und Beurteilen von Ungerechtigkeiten und den Reaktionen darauf; denn tatsächliche oder vermeintliche Ungerechtigkeit(en) werden stark wahrgenommen und führen zu teilweise heftigen Reaktionen.

In den Sozialwissenschaften, vor allem in der Soziologie, wird die Frage erweitert, wie sich gesellschaftliche Institutionen, beispielsweise das Steuersystem, die Chancen auf Erwerbstätigkeit und Bildung, Zugang zum Gesundheitswesen, betriebliche Entlohnungssysteme oder das Strafrecht auf Gerechtigkeitsvorstellungen auswirken. Dabei wird zugleich der soziale Kontext der jeweiligen Werthaltungen untersucht.

Der französische Soziologe Pierre Bourdieu hat insbesondere mit seinen Werken "Die feinen Unterschiede" und "Das Elend der Welt" empirische Studien zur Erforschung sozialer Tatsachen, die auf Ungerechtigkeit verweisen, vorgelegt.

Nach Jean Piaget durchläuft der Mensch eine kognitive Lernentwicklung, die er in prinzipielle Entwicklungsstadien untergliederte. Dazu zählte er auch die Erweiterung moralischer Urteilsfähigkeit. Nach einem ursprünglichen, amoralischen Stadium unterschied Piaget drei Stufen:

Das Modell Piagets wurde in der Folgezeit von Lawrence Kohlberg an der Universität Chicago weiterentwickelt und differenziert. Kohlberg setzte dabei die moralische Entwicklung mit der des Gerechtigkeitsempfindens gleich. Dieses stellt eine Balance zwischen Ansprüchen und Bedürfnissen her. Die Annahme von Entwicklungsstufen ist eine idealtypische Konstruktion. Jede Stufe baut auf der vorhergehenden auf und ist zugleich eine Erweiterung des kognitiven Rahmens.

Zu Beginn ihrer Entwicklung befinden sich Kinder auf einem präkonventionellen Niveau. Vorrangig ist die egoistische Perspektive. In einer ersten Entwicklungsstufe folgen sie Regeln einer Autorität, der Eltern, eines Lehrers. Orientierungsmaßstab sind Strafen und Belohnung. Die zweite Stufe zeigt die Verfolgung eigener Bedürfnisse im Zusammenspiel mit anderen. Sie ist noch individualistisch. Vorherrschend ist das Denken in den Kategorien Kosten und Nutzen nach dem Prinzip: wenn du mir hilfst, helfe ich dir auch. Ab etwa zehn Jahren setzt das konventionelle Niveau ein. Der Mensch beginnt, sich an übergeordneten Regeln zu orientieren. In der dritten Stufe strebt er nach Anerkennung, verhält sich in seinem sozialen Umfeld (Familie, Schule, Freizeit) konform und will vor allem anderen gefallen. In der vierten erfolgt die Orientierung an der gesellschaftlichen Ordnung. Man erfüllt seine Pflichten gegenüber Institutionen (Staat, Religion, im Verein) und versucht, zum Wohl der Gesellschaft beizutragen. Das dritte, postkonventionelle Niveau stellt einen Übergang zur Werte-Orientierung dar. Es kann etwa ab einem Alter von 20 Jahren erreicht werden. Allerdings gelingt es nicht allen Erwachsenen, sich dieser oder noch höheren Denkformen anzunähern. In der fünften Stufe wird die Gesellschaft als Sozialvertrag begriffen. Individuelle Rechte, vor allem Grundrechte, und allgemeine Rechtsprinzipien bestimmen das Denken über Fragen der Gerechtigkeit. Die sechste und höchste Stufe bringt die Orientierung an universalen ethischen Prinzipien (Kategorischer Imperativ), die auch Kritik an gesellschaftlichen Strukturen enthalten (Prinzip Verantwortung).

Kohlberg untersuchte sein Entwicklungsschema in Interview-Reihen mit Hilfe hypothetischer Dilemmata. Den Probanden wurden Situationen geschildert, in denen moralische Werte im Konflikt zueinander stehen und als vorrangig oder nachrangig beurteilt werden mussten. Dabei konnte seine sechste Stufe der moralischen Entwicklung empirisch nicht bestätigt werden. Die Forschungen Kohlbergs haben zu einer Vielzahl weiterer empirischer Untersuchungen geführt und Eingang in die Pädagogik gefunden. In Deutschland hat sich zu diesem Themenbereich ein Forschungszentrum an der Universität Konstanz gebildet, in dem Georg Lind, ein Schüler Kohlbergs, die Konstanzer "Methode der Dilemmadiskussion" entwickelt hat.

Carol Gilligan, eine ehemalige Mitarbeiterin Kohlbergs, stellte während der gemeinsamen Untersuchungen fest, dass Frauen innerhalb dieses Schemas regelmäßig im Durchschnitt niedrigere Stufen erreichten als Männer. In einem eigenen Befragungskonzept erweiterte sie den Bereich der untersuchten Werte und kam zu dem Schluss, dass das Konzept der Gerechtigkeit und die dabei unterstellte Autonomie typisch beim männlichen Geschlecht dominant, also androzentrisch seien. Für Frauen hingegen hat nach Gilligan in einem erheblich größeren Maß die Fürsorge als ethischer Wert Bedeutung. Hieraus entwickelte Gilligan die "Fürsorgemoral", die sie als von der Gerechtigkeit, aber auch von der Barmherzigkeit unabhängige ethische Kategorie ansieht. Gerechtigkeit ist für sie daher nicht die oberste, sondern eine von mehreren gleichwertigen Tugenden.

Hanah Chapman am „Affect and Cognition Lab“ der Universität Toronto hat, angeregt durch Charles Darwin und dessen These, dass zu jeder Emotion ein bestimmter Gesichtsausdruck gehört (siehe "Der Ausdruck der Gemütsbewegungen bei dem Menschen und den Tieren"), die Reaktionen bei Ekel untersucht. Dabei haben die 27 Versuchspersonen bei bitteren Getränken und Bildern von Kot oder Insekten durch Schließen der Augen, Rümpfen der Nase und Hochziehen der Oberlippe mit dem „Ekelmuskel“ (Musculus levator labii superioris) reagiert. Die gleiche Reaktion konnte sie auch feststellen, wenn die Probanden sich in einem Experiment unfair behandelt fühlten. Chapman führt die analoge Abwehrreaktion auf einen Prozess der Evolution zurück. „Wir glauben, dieses alte Ekelsystem konnte dann für die soziale Welt auf neue Weise genutzt werden. Als die Menschen komplexere Gesellschaften entwickelten, mussten sie Verhaltensweisen ausgrenzen, die die Normen verletzen. Die Evolution hat sich dafür nichts Neues einfallen lassen, sie hat einfach den Geltungsbereich des Ekelgefühls ausgeweitet.“

Im Bereich der Primatenforschung haben Frans de Waal und Sarah Brosnan an der Emory University in Atlanta Experimente mit Kapuzineraffen (Cebus apella) durchgeführt, wobei es für gleiche Leistung als unterschiedliche Belohnung Weintrauben oder Gurke gab. Darauf verweigerten die benachteiligten Affen die Gurke als geringwertige Belohnung. De Waal und Brosnan schließen aus diesem Verhalten, dass Primaten über ein ursprüngliches Gerechtigkeitsgefühl verfügen, das sie im Laufe der Evolution zum Zweck der Kooperation entwickelt haben. Zu ähnlichen Ergebnissen kam auch Susan Perry vom Max-Planck-Institut für evolutionäre Anthropologie in Leipzig.

Gerechtigkeit ist in allen Religionen ein herausragender, positiv belegter Wert. Der Begriff hat vielfältige Bedeutungen, selbst innerhalb einer Religion.

Für das Judentum meint Gerechtigkeit ("sädäq") sowohl die Bundestreue Gottes als auch den Gehorsam des Menschen, den er durch seine innere Einstellung wie auch durch sein äußeres Handeln zum Ausdruck bringt. Mose wird in der Tora als Vermittler der Gesetze Gottes dargestellt: „Seht, ich lehre euch Gesetze und Rechtsvorschriften, wie mir der Herr, mein Gott, geboten hat.“ (Deut. 4, 5) Ein glaubender Jude ist aufgefordert, sein Handeln an der Gerechtigkeit auszurichten. „Der Gerechtigkeit, der Gerechtigkeit jage nach.“ (Deut, 16, 20) Dabei ist nicht nur das Erfüllen der Gebote im Tanach, sondern eine grundsätzliche ethische Haltung gefordert: „Es ist dir gesagt, Mensch, was gut ist und was Gott bei dir sucht: nichts anderes als Gerechtigkeit tun, Freundlichkeit lieben und aufmerksam mitgehen mit deinem Gott.“ (Micha 6, 8) Der hebräische Begriff "sädäq" ist eigentlich unübersetzbar und verbindet Gerechtigkeit, Güte und Liebe zu einer Einheit. „Sie bezeichnet all unser Wohltun, vom Almosengeben bis zur Selbsthingabe für den Nächsten als etwas, was diesem Nächsten gebührt und mit dessen Erfüllung wir nur das getan haben, was unsere Pflicht vor Gott ist. […] Eine lieblose, blinde mit verbundenen Augen agierende Gerechtigkeit wäre auf hebräisch ein Selbstwiderspruch, während "Zedaka", juridisch gesehen, eine Ungerechtigkeit zugunsten der Armen ist.“

Im christlichen Sinne entsteht menschliche Gerechtigkeit indirekt durch die dem Menschen innewohnende selbstlose Gottesliebe als Motivation für eigene Handlungen und direkt durch die automatisch daraus folgende Einhaltung der Gebote Gottes (vgl. Jesus: „Wenn ihr mich liebt, werdet ihr meine Gebote halten“, Evangelium nach Johannes, Vers 14,15). Gerechtigkeit schließt die Barmherzigkeit aus (Gottes-)Liebe mit ein. Dieser Gerechtigkeit übergeordnet ist die "Gerechtigkeit Gottes", durch dessen Liebe und Handeln dem auf Erden lebenden Menschen Gottes Gerechtigkeit als Gnade geschenkt ist, die aber, nach dem biologischen Leibes-Tod, dem sündigen aber doch „gerechten“ Menschen im Gericht Gottes als Vergebung und Erlösung gegenübertritt.

Nach dem Alten Testament offenbart sich Gottes gerechtes Handeln auch in den sogenannten Heilserweisungen. Die Evangelien des Neuen Testamentes zeugen von der Verkündigung Jesu aus der gleichen Tradition. Die zahlreichen Wunder bzw. Heilungen lassen die rettende Liebe von Jesu „Vater im Himmel“ erkennen. Die Gleichnisse Jesu machen deutlich, dass jedem Einzelnen, durch eigenes irdisches Leben als „Gerechter“, Vergebung und Erlösung Gottes sowie ein ewiges Leben im Paradies zugesichert sind. Das Gleichnis von den Arbeitern im Weinberg (Mt 20, 1-16) zeigt, dass es nach dem christlichen Gerechtigkeitsbegriff vor allem darauf ankommt, dass ein Sünder überhaupt auf Gott vertraut und ein gerechtes Leben auf Erden beginnt und dafür Gottes „Lohn“ erhält, gleich ob er schon am Anfang oder erst am Ende seinen Erwachsenenlebens Jesus nachgefolgt ist. Der Apostel Paulus erlebt in der Urgemeinde das rettende Handeln Gottes durch seine eigene Spontan-Bekehrung. Das persönliche Vertrauen auf dieses Geschehen, d. h. der daraus gewonnene Glaube, stellt ihn unter das gerechtmachende Werk Gottes. Nach Paulus ist Gottes Gerechtigkeit „offenbart aus Glauben zum Glauben“ (Röm. 1, 17) „ohne Zutun des Gesetzes“ (Röm. 2, 12). Die Gerechtigkeit Gottes ist das ewige Geschenk Gottes an die Welt, die ewige Quelle der Liebe Gottes im Menschen ist die Motivation für gerechtes Handeln der Menschen untereinander; Die Unterscheidung von „vergänglicher“ Ungerechtigkeit in der Welt und „ewiger“ Gerechtigkeit als Werk Gottes im praktischen diesseitigen Handeln: „Stellt eure Glieder nicht der Sünde als Waffen der Ungerechtigkeit zur Verfügung, sondern stellt euch ganz Gott zur Verfügung als Menschen, die von den Toten auferweckt leben, und stellt eure Glieder als Waffen der Gerechtigkeit in den Dienst Gottes.“ (Röm 6, 13). Weil Gerechtigkeit ein Leitbegriff in der Bibel ist, kam es in der Neuzeit mit Beschluss der VI. Vollversammlung des Weltkirchenrates in Vancouver 1983 zur Einigung auf den Konziliaren Prozess für Gerechtigkeit, Frieden und Bewahrung der Schöpfung. Der Vorrang des Zieles „Frieden in Gerechtigkeit“ wurde von der ersten europäischen Ökumenischen Versammlung 1989 in Basel von einer repräsentativen gesamtchristlichen Versammlung bestätigt und hat seitdem in Verfassungstexten Eingang gefunden. Ein Jahr später formulierte die Ökumenische Weltversammlung zu Gerechtigkeit, Frieden und Bewahrung der Schöpfung in Seoul unter Beteiligung aller christlichen Konfessionsfamilien als Grundüberzeugungen unter anderem: „Die einzig mögliche Grundlage für einen dauerhaften Frieden ist Gerechtigkeit (Jesaja 32,17)“, „Die Quelle der Menschenrechte ist die Gerechtigkeit Gottes, der sein versklavtes und verelendetes Volk aus der Unterdrückung befreit (2. Mose 3,7f)“, „Gottes Gerechtigkeit schützt die ‚Geringsten‘ (Matthäus 25,31-46), die, die am verletzlichsten sind (5. Mose 24). Gott ist der Anwalt der Armen (Amos 5)“.

Im Islam ist Gerechtigkeit („’adl“) ein Gebot Allahs im Rahmen der von ihm gegebenen Weltordnung. Er selbst ist Verkörperung der Gerechtigkeit: „Allah bezeugt, in Wahrung der Gerechtigkeit, dass es keinen Gott gibt, außer ihm.“ (Kor. 3, 18) Entsprechend ist Gerechtigkeit ein grundlegender Anspruch an den Muslim. „Mein Herr hat die Gerechtigkeit geboten.“ (Kor. 7, 29) Dabei wird im Koran in einer Vielzahl von Stellen auf das konkrete menschliche Handeln abgestellt.
Dabei führt auch im Islam der Glaube zu einer gerechten Haltung. „Ihr, die ihr glaubt, steht für Gott als Zeuge der Gerechtigkeit.“ (Kor. 5, 8) Ebenso ist Gott der Richter, der am Tage des Gerichts über Unrecht und Recht urteilt. (Kor. 10, 54)

In den asiatischen Weisheitslehren des Konfuzianismus, Daoismus und Buddhismus ist die Kategorie der Gerechtigkeit (als richtiges Handeln) Bestandteil umfassenderer Tugend- und Pflichtenlehren, die vor allem auf das Individuum ausgerichtet sind, sich im Konfuzianismus aber auch auf Staat und Gesellschaft beziehen.

Ein grundlegendes Problem für alle Religionen mit der Vorstellung eines allmächtigen, allgütigen, gerechten und in das Weltgeschehen eingreifenden Gottes ist angesichts des in der Welt vorhandenen Bösen, die sogenannte Theodizee: die Frage, wie die Existenz und jenseitige Wirklichkeit Gottes mit dem Bösen in der diesseitigen, irdischen Naturwirklichkeit vereinbar ist.

Eine vertiefte Darstellung einzelner Theorien zur Gerechtigkeit findet sich im "Hauptartikel Gerechtigkeitstheorien".

Die Frage nach der Natur der Gerechtigkeit ist seit der griechischen Antike Gegenstand philosophischer Erörterungen. Frühe Erklärungen griffen dabei auf metaphysische Begründungen zurück. So wurde Gerechtigkeit als eine in der Natur vorhandene Ordnung oder als göttlichen Ursprungs verstanden. Dabei wurde Gerechtigkeit zunächst nicht vorrangig an kodifiziertem Recht gemessen, sondern als Ausdruck einer persönlichen Lebenshaltung betrachtet. Sowohl Platon als auch Aristoteles sahen die Eudaimonie (gutes, gelingendes Leben; oft mit „Glück“ übersetzt) als den höchsten anzustrebenden Wert an. Gerechtigkeit als Tugend und grundlegende Charaktereigenschaft galt ihnen als Voraussetzung für das Erlangen der Eudaimonie.

Für Platon ist Gerechtigkeit eine ewige, überweltliche, unveränderliche Idee, an der die Seele Anteil hat. Gerechtigkeit herrscht, „wenn man das Seine tut und nicht vielerlei Dinge treibt“ (Politeia IV, 433a), wenn jeder Mensch und jeder Seelenteil nur das ihm Gemäße verrichtet. Daher hat der Staat dafür zu sorgen, dass jeder seine Aufgabe nach seinen Fähigkeiten wahrnimmt und sich nicht in fremde Zuständigkeiten einmischt. Die Forderung, jedem das ihm Gebührende zukommen zu lassen (Suum cuique), bejaht Platon zwar, doch lehnt er sie als Definitionsmerkmal der Gerechtigkeit nachdrücklich ab.

Die analytische Aufteilung des Gerechtigkeitsbegriffs durch Aristoteles wird bis in die Gegenwart verwendet. Er unterscheidet zwischen der legalen (allgemeinen) Gerechtigkeit und der für die zwischenmenschlichen Beziehungen maßgeblichen besonderen Gerechtigkeit (iustitia particularis/specialis). Letztere differenzierte er in „Verteilungsgerechtigkeit“ (iustitia distributiva) und „ausgleichende Gerechtigkeit“ (iustitia commutativa). Epikur löste sich von der Vorstellung des Naturrechts und betrachtete Gerechtigkeit als eine Übereinkunft zum wechselseitigen Nutzen in der menschlichen Gemeinschaft.

In der römischen Gesellschaft bildeten sich allmählich die kodifizierten Rechtsvorschriften stärker aus. Gerechtigkeit wurde zwar immer noch mit einer persönlichen Haltung verbunden, war aber zum Beispiel bei Cicero schon stärker an der gesellschaftlichen Ordnung orientiert. So beginnt die Rechtssammlung des Kaisers Justinian I. (527–565), das Corpus Juris Civilis, mit der Definition des Rechts aus allgemeinen Prinzipien:
Beginnend in der Spätantike und bis ins späte Mittelalter reichend, dominierten in der Folge christliche Vorstellungen die Debatte. Die Gerechtigkeit Gottes hatte Vorrang und daraus folgend konnte der Mensch Gerechtigkeit nur durch die Gnade Gottes erlangen.

Mit der Neuzeit kam es schrittweise zu der Lösung von der Vorstellung einer gottgegebenen Gerechtigkeitsordnung. Gerechtigkeit wurde bei Thomas Hobbes als notwendiges Prinzip aus der Natur der Menschen begründet. In der Folge der neuen Weltsicht entstanden von Hobbes über John Locke bis zu Jean-Jacques Rousseau verschiedene Konzepte des Gesellschaftsvertrages, die auch politischen Einfluss auf neue absolutistische bis hin zu freiheitlichen Gesellschaftsordnungen hatten. Die jeweils als Gedankenexperiment konzipierten Modelle eines Sozialvertrages folgen unterschiedlichen, zum Teil gegensätzlichen Begründungskonzepten.

Hobbes’ Vertrag ist das Modell einer rein rationalen Zweckgemeinschaft, mit der die aggressive Natur des Menschen gebändigt werden soll. In der Widmung zu seinem Werk "De Cive" (Über den Bürger) heißt es zur ursprünglichen Natur des Menschen: „Homo homini lupus est“, das heißt der Mensch ist des Menschen Wolf. Zur Vermeidung eines Krieges aller gegen alle („Bellum omnia contra omnes“), schreibt er im "Leviathan", übereignen die Menschen ihre natürlichen Rechte, wie das auf Selbstverteidigung, an einen Souverän, der autonom Recht setzt und dieses auch mit Gewalt durchsetzt. Diese Legitimation eines absoluten Herrschers ist zugleich die Begründung eines uneingeschränkten Rechtspositivismus, der Unrecht nur als Verstoß gegen geltendes Recht kennt. „Wo keine allgemeine Gewalt ist, ist kein Gesetz, und wo kein Gesetz, keine Ungerechtigkeit.“

Für Locke hingegen gibt es ein vorpositives, gottgegebenes Naturrecht, in dem der Mensch, ähnlich wie es vor ihm schon Seneca sah, frei ist und das Recht hat, Eigentum zu bilden. Der Gesellschaftsvertrag ist ein Kooperationsvertrag, so dass die Regierung nur den Willen des Bürgers des Staates repräsentiert. Sie ist an die Verfassung als Grundlagenvertrag gebunden und wird durch Gewaltenteilung kontrolliert.

Bei Rousseau hat der Staat eine ähnliche Schutzfunktion wie bei Hobbes. Rousseau sah allerdings den „Kampf aller gegen alle“ verursacht durch Eigentum und den Schritt des Menschen in die Zivilgesellschaft. Der Staat dient der Verhinderung von Ungerechtigkeit und darf zu diesem Zweck auch Gewalt anwenden. Der Gesellschaftsvertrag ("contrat social") wird aber bei Rousseau nicht mit einem unabhängigen Herrscher geschlossen, sondern mit einem Staat, der den Gemeinwillen ("volonté générale") verkörpert. Hierdurch ist die einzig mögliche Regierungsform die Republik ohne indirekte Repräsentation (Parteien, Parlament). Zur Natur des Sozialvertrages gehört die soziale Gerechtigkeit, also dass „kein Staatsbürger so reich sein darf, um sich einen andern kaufen zu können, noch so arm, um sich verkaufen zu müssen.“

Während die Ideen Lockes Einfluss auf die Verfassung der Vereinigten Staaten von 1787 mit ihrem wenig später aufgenommenen Grundrechtekatalog, der Bill of Rights hatten, prägten Teile von Rousseaus Konzepten eher die Französische Revolution.

Einen weiteren Schritt vollzogen der Skeptiker David Hume und Immanuel Kant, die auf die Unmöglichkeit einer Verknüpfung des Seins mit dem Sollen (Humes Gesetz) verwiesen. Kant wies das Naturrecht als metaphysisch zurück und entwickelte die Idee des Vernunftrechts.

David Hume vertrat einen radikalen Empirismus und lehnte damit ebenfalls Vorstellungen eines Naturzustandes und eines Naturrechts ab. Er betrachtete Gerechtigkeit als Produkt der Vernunft, als sekundäre Tugend, die keine moralischen Gebote oder Verbote begründet, sondern den Zweck verfolgt, die Ordnung des menschlichen Zusammenlebens sicherzustellen. Vor der Errichtung des Staates gab es bereits Regeln des Zusammenlebens in der Familie, die zur Basis des Sozialvertrags wurden. Dieser schützt vor allem das individuelle Eigentum in einer Gesellschaft, die üblicherweise von Knappheit an Gütern gekennzeichnet ist. Lebte der Mensch im Überfluss, könnte jeder nach seinen Bedürfnissen existieren; das Leistungsprinzip wäre nicht erforderlich. Herrscht andererseits jedoch ein extremer Mangel an Gütern, wird sich der Mensch egoistisch verhalten, es entsteht Ungerechtigkeit.

Das kantische Vernunftrecht basiert auf der Überlegung, dass es für den Menschen eine unabweisbare Tatsache der Erfahrung ist, dass er seine Handlungen durch Vernunft bestimmen kann. Er ist frei und autonom. Diese Autonomie ergibt sich auch im äußeren Verhältnis der Menschen untereinander. Der Mensch ist aus seiner Vernunft heraus verpflichtet, die Persönlichkeit und in ihr die Würde des anderen zu achten. Daraus ergibt sich nach Kant der kategorische Rechtsimperativ:
Die Freiheit des Einzelnen wird durch ein selbst gesetztes Gesetz sichergestellt. Hierdurch wird die Autonomie gewährleistet, aber durch die gemeinsame Bestimmung des Rechts zugleich auch die Freiheit beschränkt. Diese Bestimmung der (iuridischen) Gerechtigkeit ist rein formal. Für die materiale Gerechtigkeit bedarf es nach Kant der empirischen Erfahrung. Ausgehend von Locke und Hume hat Kant das Modell des Gesellschaftsvertrages übernommen, die Idee des Naturrechts aber ebenso wenig akzeptiert wie die Relativität der Gerechtigkeit, die sich aus Humes Skepsis ergab. Seine Begründung der Gerechtigkeit liegt in der Sittlichkeit als Gebot der reinen praktischen Vernunft.

Anknüpfend an Hume entstand im englischsprachigen Raum der Utilitarismus als dominierendes ethisches Prinzip, das die allgemeine Wohlfahrt (den gesamtgesellschaftlichen Nutzen) in den Mittelpunkt der Werte stellte und die Gerechtigkeit auf die Ebene einer Rahmenbedingung verwies. Jeremy Bentham formulierte zunächst „das größte Glück der größten Zahl“ als ursprüngliches utilitaristisches Ziel. Bereits John Stuart Mill relativierte diesen reinen Hedonismus durch die qualitative Bewertung von Präferenzen und die Berücksichtigung von Werten und Tugenden. In der Gerechtigkeit sah er eine vollkommene Pflicht, weil sie eingefordert werden kann. Gerade deshalb ist sie auch mit Sanktionen durchsetzbar. Der Kritik, dass in individuellen Handlungssituationen der aus der Handlung resultierende Gesamtnutzen nicht bestimmt werden könne, begegnete Henry Sidgwick mit dem Regelutilitarismus, wonach Werte und Tugenden als sekundäre Prinzipien den gesamtgesellschaftlichen Nutzen sicherstellen (siehe auch Richard Mervyn Hare). Moderne Vertreter des Utilitarismus sind J.J.C. Smart, Peter Singer oder John Harsanyi. Durch das zugrunde liegende Nutzenkonzept ergibt sich eine starke Nähe des Utilitarismus zur Wohlfahrtsökonomie und der damit eng verbundenen Entscheidungstheorie.

Im Gefolge der Aufklärung steht die "Skepsis" gegen eine verbindlich vorgegebene (heteronome) Gerechtigkeit. Friedrich Nietzsche bestritt, dass die Lebenspraxis im Wesentlichen überhaupt durch praktische Vernunft bestimmt sei. - Karl Marx stellte der Auffassung, dass Gerechtigkeit aus vorgegebenen Prinzipien abzuleiten sei, die Ansicht gegenüber, Recht und Gerechtigkeit gehörten zum sogenannten Überbau: Gerechtigkeit beruhe auf den jeweiligen materiellen Verhältnissen und sei zum Beispiel im Kapitalismus Ausdruck der Herrschaft einer bürgerlichen Klasse. - Walter Benjamin und Jacques Derrida verwiesen darauf, dass Gerechtigkeit eine metaphysische, dem Recht zwar immanente, aber als Kategorien nicht fassbare Größe darstellt. - Nach Ansicht des Kritischen Rationalismus vollzieht sich Gerechtigkeitserkenntnis experimentierend durch "trial and error". Dieses schrittweise Weiterschreiten gründe sich (sehr deutlich im Fallrecht) auf Gewissen und Konsens der Juristen.

Auch die Diskurstheorie, insbesondere die Diskurstheorie des Rechts von Jürgen Habermas, liefert Ansatzpunkte, Gerechtigkeitsfragen rational zu lösen und stellt einen Versuch dar, zu ausgewogenen und damit annähernd gerechten Ergebnissen zu gelangen: innerhalb einer Gesellschaft und darüber hinaus.

Ein neuer Ansatz in der Diskussion entstand mit der Theorie der Gerechtigkeit als Fairness von John Rawls, der eine Reihe von Grundrechten als Voraussetzung für Gerechtigkeit annahm und damit allgemeine Prinzipien für die gerechte Gestaltung der Gesellschaft in der Fortentwicklung kantischer Vorstellungen bietet. An Rawls anknüpfend entspann sich in den letzten 30 Jahren des 20. Jahrhunderts eine intensive Debatte um die Frage der Gerechtigkeit. Kritik an dem von Rawls oder Ronald Dworkin vertretenen liberalen Egalitarismus kam dabei sowohl von radikal liberalen Positionen von Robert Nozick, Friedrich Hayek oder James Buchanan, als auch vom auf die Perspektive der Gemeinschaft ausgerichteten Kommunitarismus, die insbesondere von Charles Taylor, Michael Sandel, Alasdair McIntyre und Michael Walzer vertreten wurde.

Einen Weg weg von der alleinigen Dominanz ökonomischer Kriterien weisen Martha Nussbaum und Amartya Sen, mit ihrem Ansatz der Entwicklungs- und Verwirklichungschancen ("Capabilities"), dem ein Bündel von Werthaltungen zur Beurteilung von Gerechtigkeit zugrunde liegt. Damit tragen sie den sehr unterschiedlichen Bedürfnissen der Menschen Rechnung und berücksichtigen vor allem die Probleme internationaler Gerechtigkeit.

"Gerechtigkeit" zählt neben Bezeichnungen wie "Recht", "Gesetz" und "Strafe" zu den Grundlagenbegriffen der Rechtsphilosophie und der Rechtswissenschaft.

Legale Gerechtigkeit unterscheidet sich von der ethischen Gerechtigkeit dadurch, dass die in der Gemeinschaft gültigen Normen für verbindlich erklärt worden sind. Dabei entsteht die Grundfrage, wer für diese Festlegung zuständig ist, wer die Rechtsetzungsmacht hat. Als Rechtsquellen sind kodifiziertes (geschriebenes) Recht, Gewohnheitsrecht und Vertragsrecht zu unterscheiden. In einer rechtlichen Ordnung wird sowohl die Beziehung der Einzelnen zueinander als auch die des Einzelnen zur Gemeinschaft geregelt. Die Verbindlichkeit des Rechts entsteht, wenn Abweichungen davon als Rechtsbruch festgestellt und (mit Rechtsfolgen) geahndet werden können.

Schon das Bestehen einer Rechtsordnung überhaupt – wie auch immer diese ausgestaltet sein mag –, das heißt das Abweichen von reiner Willkür, bildet einen Grundtatbestand der Gerechtigkeit, weil hierdurch eine Rechtssicherheit für den Einzelnen entsteht und er seinen Handlungsspielraum hieraus ableiten kann. Dies ist eine formale Bedingung von Gerechtigkeit, die durch positives Recht als solches gewährleistet wird. Materielle Gerechtigkeit ist auf drei Ebenen herzustellen: in der Gesetzgebung, im Gerichtsverfahren und im Strafrecht. Rechtssicherheit als notwendige Bedingung formaler Gerechtigkeit ist dabei ein Nahziel des Rechts, die Herstellung materieller Gerechtigkeit das Fernziel. Die Rechtsprechung soll zwischen diesen beiden Rechtswerten vermitteln. Größte Ungerechtigkeit kann sich ergeben, wenn Rechtssicherheit zum Alleinziel erhoben wird. Außerdem muss der auf legaler Gerechtigkeit beruhende Staat seine Rechtsordnung für die sozialen Akteure glaubwürdig praktisch umsetzen. Dabei wird das Zusammenwirken von Gerechtigkeit(svorstellungen), Recht(ssystem) und (Durchsetzungs-)Macht deutlich.

Eine der grundlegenden Fragen der Rechtsphilosophie ist, ob es ein allgemeingültiges Naturrecht gibt oder ob Gesetze allein aufgrund menschengemachter Regeln zustande kommen sollen. Letztere Auffassung bezeichnet man als Rechtspositivismus. Je nach Auffassung wird das Verhältnis von Gerechtigkeit und Recht unterschiedlich bestimmt. Soweit auf die Gesetzesordnung angewendet, fordern Vertreter des Naturrechts wenigstens, dass Gesetze aus ethischen Prinzipien abgeleitet werden und mit diesen im Einklang stehen müssen.

Für Rechtspositivisten ergibt sich das Recht aus der sozialen Praxis und ist unabhängig von der Moral (Trennungsthese), zu der auch die Idee der Gerechtigkeit gezählt wird. Recht wird allein nach seiner Zweckmäßigkeit beurteilt. Darüber hinaus gibt es Ansätze, die in unterschiedlicher Weise versuchen, Elemente beider Richtungen miteinander zu verknüpfen.

Der Widerstreit der beiden Positionen beruht im Grundsatz auf der Differenz darüber, welchen Charakter die Grundwerte einer Gesellschaft haben. Für den Rechtspositivisten sind sie vom Menschen gewählt und je nach kulturellem Kontext verschieden. Für manche Naturrechtler sind moralische Festlegungen wie die Würde des Menschen, Freiheit, Gleichheit oder die Unverletzlichkeit des Lebens unveräußerliche Werte, die kulturunabhängig und unabhängig vom geltenden Recht bestehen; andere beziehen sich auf die Vernunft. In der praktischen Konsequenz können positive Gesetze für den Naturrechtler danach Unrecht darstellen.

Einer verbreiteten, aber zu pauschalen Auffassung zufolge ist für den Rechtspositivismus das gesetzte Recht ohne Einschränkung gültig, während nach Naturrechtsverständnis unter bestimmten (extremen) Bedingungen bürgerlicher Widerstand gegen bestehende Gesetze begründet und sogar ethisch geboten sein kann. Damit (nicht identisch, aber) verwandt ist die Unterscheidung zwischen verpflichtender Rechtsgeltung und bloßer Durchsetzbarkeit von Vorschriften, die, wie z. B. die Lagerordnung eines Konzentrationslagers, nur ein bedingtes Müssen begründen, aber Ungehorsam legitimieren. Eine nichtpositivistische Begründung für ein Recht auf Widerstand liefert die Radbruchsche Formel, die Gustav Radbruch vor dem Hintergrund des nationalsozialistischen Unrechtsstaates entwickelt hat. Dass diese Dichotomie die Haltung des Rechtspositivismus und der Naturrechtslehre zum Problem der Gerechtigkeit jedoch zu stark vereinfacht, machen zwei Gegenbeispiele deutlich: Auf der einen Seite haben positivistisch orientierte Rechtsphilosophen wie der Brite H.L.A. Hart, neben Hans Kelsen der bedeutendste Rechtspositivist des 20. Jahrhunderts, sowie im deutschsprachigen Raum der Philosoph Norbert Hoerster wiederholt darauf hingewiesen, dass gerade die positivistische Trennungsthese eine kritische Beurteilung der geltenden Gerechtigkeitsmaßstäbe ermögliche. Rechtskritik könne einer rechtspositivistischen Theorie sogar besser gelingen als einer naturrechtlich fundierten, da nur jene in der Lage sei, zwischen dem Recht, wie es ist, und dem Recht, wie es sein sollte, zu unterscheiden. Hans Kelsen betonte aber in einer Diskussion mit Naturrechtsvertretern ausdrücklich, das Naturrecht habe gegenüber seiner Reinen Rechtslehre „den Nachteil, daß man dann annehmen muß, daß das Sowjetrecht …(und) daß das Nazirecht kein Recht ist …“ Andererseits hat beispielsweise Immanuel Kant zwar eine vernunftrechtlich begründete, auf materiellen Grundsätzen aufbauende Rechtslehre entwickelt, ein Recht des Volkes auf Widerstand gegen ungerechte, selbst gegen die Menschenrechte verstoßende Gesetze, jedoch strikt abgelehnt.

Der Gleichheitssatz ist eine wichtige Grundlage des juristischen Bemühens um Gerechtigkeit. Er ist ein Leitfaden der Rechtsentwicklung und insgesamt ein Fundament des Rechtsstaates. Daher ist er ein Bestandteil der meisten Verfassungen. So bestimmt Abs. 1 des deutschen Grundgesetzes (GG): „Alle Menschen sind vor dem Gesetz gleich“ und bezieht sich auch hierzu auf die Gerechtigkeit: „Das Deutsche Volk bekennt sich ... zu unverletzlichen und unveräußerlichen Menschenrechten als Grundlage jeder menschlichen Gemeinschaft, des Friedens und der Gerechtigkeit in der Welt.“ ( Abs. 2 GG)

In der Allgemeinen Erklärung der Menschenrechte der Vereinten Nationen von 1948 heißt es: „Alle Menschen sind frei und gleich an Würde und Rechten geboren. […] Jeder Mensch hat Anspruch auf die in dieser Erklärung verkündeten Rechte und Freiheiten ohne irgendeine Unterscheidung, wie etwa nach Rasse, Farbe, Geschlecht, Sprache, Religion, politischer oder sonstiger Überzeugung, nationaler oder sozialer Herkunft, nach Eigentum, Geburt oder sonstigen Umständen.“

Neben dem kodifizierten Recht ist die Einrichtung von Gerichten ein wesentlicher Schritt zur Schaffung von Rechtssicherheit und damit von Gerechtigkeit, insbesondere wenn sie mit dem Verbot von Privatjustiz bzw. Selbstjustiz einhergeht. Die Institution der Justiz soll gewährleisten, dass Rechtsstreitigkeiten im Zivilrecht ebenso wie Rechtsverstöße im öffentlichen und im Strafrecht von sachkundigen, unabhängigen Personen in gleicher Weise und angemessen beurteilt und entschieden bzw. geahndet werden. Dies ist zur Gewährleistung von Gerechtigkeit notwendig, da Gesetze vom Charakter her allgemein formuliert sind und auf konkrete Tatbestände nur durch eine Bewertung angewendet werden können.

Zur Gerechtigkeit in einem Gerichtsverfahren tragen eine Reihe von Faktoren bei:

Wünschenswert, aber nicht erzwingbar sind die Urteilsfähigkeit sowie damit verbunden eine persönliche Gerechtigkeit des Richters. Da Gesetze auslegungsbedürftig sind und die Möglichkeit des Irrtums besteht, hat sich die Möglichkeit entwickelt, Rechtsmittel bei einem höheren Gericht einzulegen. Zur Rechtssicherheit trägt auch die Veröffentlichung von Urteilen bei, die auf die Gleichmäßigkeit der Rechtsprechung hinwirkt. Bei Straftaten trägt zur gleichmäßigen Rechtsanwendung ohne Ansehen der Person bei, dass bei Bekanntwerden eines Verdachts die Staatsanwaltschaft von Amts wegen ermitteln muss. Weitgehend können diese idealen Bedingungen in demokratischen Rechtsstaaten verwirklicht werden.

Da Gesetze generell-abstrakte Regelungen sind, kommt es immer wieder vor, dass in einzelnen Lebenssachverhalten Rechtsprobleme auftauchen, die gesetzlich nicht geregelt sind. Dies kann zu als ungerecht empfundenen Situationen führen. Abhilfe kann hier eine Rechtsfortbildung schaffen, die zu so genanntem Richterrecht führt. So waren in der ursprünglichen Fassung des Bürgerlichen Gesetzbuches im Schuldrecht nur zwei Fälle vorgesehen, in denen bei Leistungsstörungen der Schuldner dem Gläubiger Schadensersatz leisten muss: zu vertretende Unmöglichkeit und Verzug. Bald stellte sich heraus, dass eine weitere Fallgruppe vom Gesetzgeber übersehen worden war, nämlich die Schlechtleistung: Der Schuldner erbringt zwar die Leistung, fügt hierbei aber dem Gläubiger einen Schaden zu. Um hier zu befriedigenden Lösungen zu gelangen, ergänzte die Rechtsprechung das Gesetz mit der Rechtsfigur der Positiven Forderungsverletzung. Diese war als lückenfüllendes Richterrecht anerkannt, bis sie im Jahr 2001 durch eine Neufassung des BGB vom Gesetzgeber ausdrücklich geregelt wurde.

Häufiger in der Rechtsprechung sind jedoch Analogien. Analog bedeutet hier, ein Gesetz auf einen anderen Sachverhalt entsprechend anzuwenden. Die Ergänzung von Gesetzeslücken ist im Schweizer Recht ausdrücklich vorgesehen:
In Deutschland gibt es eine solche gesetzliche Regel nicht, jedoch ist es anerkannt, dass die Rechtsprechung unter Umständen ein auf einen Sachverhalt nach seinem Wortlaut nicht passendes Gesetz analog anwenden darf, wenn unter Gerechtigkeitsgesichtspunkten eine derartige Anwendung erforderlich ist und eine planwidrige Regelungslücke besteht (also der Gesetzgeber nicht bewusst keine Regelung getroffen hat). Hiermit kann insbesondere eine verfassungskonforme Auslegung einfachen Gesetzesrechts erreicht werden.

So hat es das Bundesverfassungsgericht im "Soraya-Urteil" aus dem Jahr 1973 ausdrücklich gebilligt, dass der BGH entgegen dem Wortlaut des damaligen § 847 Abs. 1 BGB (jetzt Abs. 2 BGB) ein Schmerzensgeld für die Verletzung des Allgemeinen Persönlichkeitsrechts zugebilligt hat. Zur Begründung hat es ausgeführt:

Mit Urteil vom 25. Januar 2011 hat das Bundesverfassungsgericht aber die Grenzen derartiger Rechtsfortbildung aufgezeigt:

Das Problem derartigen Richterrechts analoger Anwendung von Gesetzen liegt darin, dass wegen des Grundsatzes der Gewaltenteilung die Gesetzgebung dem parlamentarischen Gesetzgeber vorbehalten ist, während der Richter an das geltende Recht gebunden ist, aber kein neues Recht schaffen soll. Auch verweisen Kritiker darauf, dass durch das Richterrecht ein Element der Willkür in der Rechtsprechung enthalten sei, das zur Rechtsunsicherheit beitrage. Insbesondere sei es ein systematischer Mangel des Richterrechts, dass die im Urteil festgelegten Rechtsprinzipien bis dahin für entsprechende Fälle keine Rechtsgrundlage waren. Richterrecht verstoße damit gegen den Grundsatz des Rückwirkungsverbotes von Gesetzen. Problematisch unter Aspekten der Gewaltenteilung ist, dass der Gesetzgeber oftmals ein Problem regeln könnte, das aber nicht will, weil eine Regelung parlamentarisch schwierig zu erreichen ist und er die Problematik den Gerichten überlässt.

Grundsätzlich unzulässig sind Rechtsfortbildungen und analoge Anwendung von Gesetzen zum Nachteil eines Angeklagten im materiellen Strafrecht, da dort der strikte Grundsatz Nulla poena sine lege gilt. Analogien im Strafverfahrensrecht sind aber erlaubt, sofern der Grundsatz des fairen Verfahrens nicht verletzt wird.

Das anglo-amerikanische Rechtssystem kommt mit weniger vom Gesetzgeber geschaffenen Rechtsnormen aus, es entwickelt sich – mehr am Einzelfall orientiert – im Wesentlichen durch Fortbildung von Richterrecht weiter. Hier ist eine Angleichung der Rechtssysteme zu bemerken. Andererseits greift der anglo-amerikanische Gesetzgeber immer mehr durch Setzung von Rechtsnormen ordnungspolitisch in die Gesellschaft ein, zum anderen werden im französisch-deutschen Rechtssystem immer mehr Rechtsfragen im konkreten Einzelfall durch die Gerichte
geregelt als vom Gesetzgeber.

Strafen sind ein wesentlicher Eingriff in das Selbstbestimmungsrecht des Betroffenen. Sie bedürfen daher einer grundlegenden Rechtfertigung. Gerichtlich verhängte Strafen aufgrund von Gesetzesverstößen haben mehrere Funktionen, die im Strafrecht verschiedener Staaten unterschiedlich gewichtet werden:
Durch seine Tat hat sich ein Straftäter in gewisser Weise einen Vorteil verschafft und das sittliche Gleichgewicht der Gesellschaft gestört. In diesem Sinne sind Strafen eine Form der ausgleichenden Gerechtigkeit und vergangenheitsbezogen. Ihnen liegt die grundsätzliche Annahme zugrunde, dass Täter aus freiem Willen handeln und sich möglicher Konsequenzen ihrer Taten bewusst sind oder zumindest bewusst sein könnten. Am konsequentesten haben diesen rein vergeltenden, ausgleichenden und vergangenheitsbezogenen Charakter der Strafe die Philosophen Immanuel Kant und Georg Wilhelm Friedrich Hegel vertreten. Kant zufolge „müßte [falls ein Inselvolk sich aufzulösen beschlösse] vorher der letzte im Gefängnis befindliche Mörder hingerichtet werden, damit jedermann das widerfahre, was seine Taten wert sind und die Blutschuld nicht auf dem Volke hafte, das auf diese Bestrafung nicht gedrungen hat“. Hegel brachte den Gedanken der ausgleichenden Gerechtigkeit dadurch zum Ausdruck, dass er das Verbrechen als Negation des Rechts, die Strafe hingegen wiederum als Negation dieser Negation ansah.

Das Konzept des Strafrechts als eine Form der ausgleichenden (vergeltenden) Gerechtigkeit wird von Vertretern eines reinen Präventionsstrafrechts in Frage gestellt. Schon Seneca führte im 1. Jahrhundert n. Chr. unter Berufung auf Platon aus, der sich hierbei selbst auf den antiken griechischen Skeptiker Protagoras bezogen hatte, dass ein kluger Mensch nicht strafe, weil gesündigt worden ist, sondern, damit nicht gesündigt werde („Nam, ut Plato ait: ‚Nemo prudens punit, quia peccatum est, sed ne peccetur …‘“). Eine Blütezeit erlebte der Präventionsgedanke sodann ab dem Zeitalter der Aufklärung. Bedeutende Vertreter eines überwiegend präventiv orientierten Strafrechts waren beispielsweise der italienische Strafrechtsreformer Cesare Beccaria Ende des 18. Jahrhunderts und der deutsche Strafrechtler Franz von Liszt, der dem Präventionsgedanken sein berühmtes Marburger Programm von 1882 widmete. Aus philosophischer Perspektive wurde der Vergeltungsgedanke beispielsweise von Friedrich Nietzsche abgelehnt. Er hielt Vergeltung als Begründung von Strafen für einen irrationalen Racheinstinkt, der durch katholische Lehren („Höllenfeuer“) gestützt wird.

Der Vorbeugung durch Abschreckung liegt die Annahme zugrunde, dass es der Staatsgewalt bei hinreichender Höhe der angedrohten Strafe gelingen wird, Umfang und Häufigkeit von Straftaten spürbar zu verringern. Hierdurch soll Gerechtigkeit von vornherein sichergestellt werden.

Erfolgreiche Maßnahmen zur Resozialisierung, die zur Wiedereingliederung von Straftätern in die Gesellschaft führen, wirken im Sinne von Gerechtigkeit. Denn sie erhöhen die allgemeine Rechtssicherheit durch sinkende Rückfallquoten.

Das psychologische und sozialwissenschaftliche Argument, dass Menschen Straftaten oftmals aufgrund persönlicher Disposition und gesellschaftlicher Bedingtheit begehen, also weitgehend determiniert (festgelegt) und dadurch schuldunfähig sind, wird neuerdings von Neurowissenschaftlern wie Gerhard Roth gestützt, ist aber stark umstritten.

Strafnormen dienen oft lediglich der Durchsetzung ordnungspolitischer Zwecke, ohne dass damit das sittliche Gleichgewicht der Gesellschaft gestört werden muss. So dienen Aufenthaltsbeschränkungen von Asylbewerbern nach deutschem Recht allein dazu, eine spätere Abschiebung zu erleichtern sowie eine gleichmäßige Verteilung der Asylanten auf alle Bundesländer zu gewährleisten. Wer dagegen verstößt, hat sich keinen sittlichen Vorteil verschafft, sondern unterläuft allein die gegebenen ordnungspolitischen Zwecke.

Es existieren zahlreiche künstlerische Darstellungen der Gerechtigkeit, zum Beispiel in Malereien und Skulpturen. Häufig wird die Gerechtigkeit allegorisch als Frau mit Schwert, Waage und Augenbinde gestaltet. In einer Reihe von Städten gibt es Gerechtigkeitsbrunnen, oft mit einer Statue der Justitia.

Die Gerechtigkeit ist auch oftmals zentrales Thema in der Literatur und im Sprechtheater. Bereits im antiken griechischen Theater widmet Aischylos in der "Orestie" einem sehr schwer ‚gerecht‘ zu entscheidenden Strafprozess seine Aufmerksamkeit, in dem Götter und Menschen über einen Muttermörder, der durch seine Tat seinen von der Mutter ermordeten Vater rächen wollte, beraten und urteilen. Im deutschen Sprachraum thematisierte Friedrich Schiller die Gerechtigkeit in vielen seiner Werke von "Die Räuber" bis zum "Wilhelm Tell", aber auch in der Ballade "Die Kraniche des Ibykus". Berühmte Musterbeispiele hat Heinrich von Kleist mit seiner Erzählung "Michael Kohlhaas" und seinem Lustspiel "Der zerbrochne Krug" gegeben. Gottfried Keller hat mit der Groteske "Die drei gerechten Kammmacher" zum Thema beigetragen. Auf die Unmöglichkeit, ein gerechtes Gesetz zu verfassen, verweist Franz Kafkas Parabel "Vor dem Gesetz". Berühmt ist auch "Der kaukasische Kreidekreis" von Bertolt Brecht. In dem Drama "Die Gerechten" thematisierte Albert Camus die Frage der Anwendung terroristischer Gewalt. Das Verhältnis von Recht und Gerechtigkeit problematisierte Friedrich Dürrenmatt in dem Roman "Justiz", der 1993 von Hans W. Geißendörfer verfilmt wurde.

In der Oper ringen zum Beispiel Gerechtigkeit und Großmut in Mozarts "La clemenza di Tito" miteinander. Giuseppe Verdi verarbeitete Schillers Räuber in der Oper "I masnadieri." Ludwig van Beethoven thematisiert Freiheit, Gerechtigkeit und Brüderlichkeit in seiner einzigen Oper "Fidelio." Im Hörspiel "Wer sich umdreht oder lacht …" von John von Düffel, Regie: Christiane Ohaus, der 42. Episode des Radio-Tatorts, Radio Bremen 2011, dreht sich die Psychose der eigentlichen Täterin um das manische Einfordern der Gerechtigkeit.

Bekannte Verfilmungen zum Thema Gerechtigkeit sind "Zeugin der Anklage", "Die zwölf Geschworenen", "Tabu der Gerechten", "Der Fall Winslow" oder "… und Gerechtigkeit für alle". In dem u. a. von Amnesty International prämierten Film "Jagd nach Gerechtigkeit" (Hunt for Justice) des Regisseurs Charles Binamé stehen die Gründung des Kriegsverbrechertribunals in Den Haag und die Widerstände bis zum Prozess gegen Slobodan Milosevic im Mittelpunkt der Handlung.







</doc>
<doc id="14481" url="https://de.wikipedia.org/wiki?curid=14481" title="Massenmedien">
Massenmedien

Massenmedien sind Kommunikationsmittel zur Verbreitung von Inhalten in der Öffentlichkeit, Medien für die Kommunikation mit einer großen Zahl von Menschen. Zu den Massenmedien zählen sowohl die klassischen gedruckten Medien (heute speziell Printmedien genannt, z. B. Zeitungen, Zeitschriften, Plakate, Flugblätter) als auch elektronische Medien (z. B. Rundfunk und Online-Dienste).

Die Medienwissenschaft ist das wissenschaftliche Fach, das sich mit der Geschichte und Wirkung der Massenmedien beschäftigt. Mit den Akteuren, Strukturen und Leistungen des Journalismus befasst sich die Journalistik als Teilgebiet der Kommunikationswissenschaft. Insgesamt ist die wissenschaftliche Betrachtung und Beschäftigung mit Massenmedien also interdisziplinär verteilt im Spannungsfeld sowohl der Geistes- als auch der Sozial- und der Kulturwissenschaften, wobei der jeweilige theoretische Hintergrund der unterschiedlichen Anwendungsgebiete ebenfalls noch zu berücksichtigen ist (z. B. Wirtschaftswissenschaften für den Wirtschaftsteil und Sportwissenschaft für den Sportteil).

Haushalte in Deutschland gaben 2016 durchschnittlich 39 Euro für Massenmedien (ohne Bücher) aus. Davon entfielen 42 Prozent auf den Rundfunkbeitrag, 33% auf Zeitungen und Zeitschriften, 20% auf Pay-TV und 6% für Digitale Medien.

Eine bekannte Definition lautet: „Die Massenmedien sind Kommunikationsmittel, die durch technische Vervielfältigung und Verbreitung mittels Schrift, Bild oder Ton Inhalte an eine unbestimmte Zahl von Menschen vermitteln und somit öffentlich an ein anonymes, räumlich verstreutes Publikum weitergeben.“

Durch Massenmedien wird Massenkommunikation ermöglicht. Die Massenkommunikation geschieht öffentlich, wodurch im Prinzip jeder Zugang zu den Angeboten von Massenmedien hat. In diesem Sinne umfasst die sozialwissenschaftliche Definition von Massenmedien:

Gerhard Maletzke definiert fünf entscheidende Faktoren für Massenkommunikation: „Unter Massenkommunikation verstehen wir jene Form der Kommunikation, bei der Aussagen "öffentlich" (also ohne begrenzte und personell definierte Empfängerschaft) durch "technische Verbreitungsmittel" (Medien) "indirekt" (also bei räumlicher oder zeitlicher oder raumzeitlicher Distanz zwischen den Kommunikationspartnern) und "einseitig" (also ohne Rollenwechsel zw. Aussagenden und Aufnehmenden) "an ein disperses Publikum" vermittelt werden.“

Diese Definition schließt z. B. Theaterveranstaltungen als Massenkommunikation aus, da das Publikum nicht ausreichend verteilt ("dispers") ist. Auch ist zu bedenken, dass Massenmedien zu komplexen sozialen Institutionen geworden sind, die durch Politik, Recht und Ökonomie in ihrer Ausgestaltung beeinflusst werden. Ohne diese Dimension ist ein sinnvoller internationaler Vergleich von Medien und Mediensystemen kaum möglich.

Während jedoch dieser Kritikpunkt als strittig behauptet werden kann, da auch die klassischen Massenmedien „durch Politik, Recht und Ökonomie“ in ihrer Ausgestaltung beeinflusst waren und weiterhin sind, wodurch dieser Charakter nicht zwangsläufig zur Definition Maletzkes im Widerspruch gesehen werden muss, sehen Kunczik und Zipfel die Schwachstelle in der fehlenden Vereinbarkeit mit der technischen Weiterentwicklung, die in den vergangenen Jahrzehnten stattgefunden hat:
Hierauf aufbauend entwickelte Ulrich Saxer 1998 eine Definition, die Medien nicht nur als technische Artefakte, sondern in ihrer gesellschaftlichen Dimension zu erfassen versucht. „Medien sind komplexe institutionalisierte Systeme um organisierte Kommunikationskanäle von spezifischem Leistungsvermögen“ und sind durch fünf mehr oder weniger stark ausgeprägte Merkmale gekennzeichnet:

Harry Pross teilt Medien abhängig von deren Produktions- und Rezeptionsbedingungen in Gruppen ein:

Ergänzt werden können "quartäre Medien", die auf beiden Seiten Geräte voraussetzen, nicht aber ausschließlich massenmedialer Kommunikation oder Mitteilungsverbreitung dienen. Das Internet ist z. B. ein Medium, das vom Nutzer in anderem Ausmaß aktive Entscheidungen über den Konsum verlangt und zum Teil direkte Rückkopplung des Nutzers zum Anbieter erlaubt. Daraus ergeben sich schnelle und spontane Wechsel der Zuordnung aufgrund der wechselnden Benutzungsmodi: Wechsel zwischen tertiären Eigenschaften und quartären sind etwas Neues, das in diese Struktur einzufügen ist. Digitalisierung ermöglicht die Integration und Mischung der ersten drei Medienstufen in der vierten. Quartäre Medien bieten eine enge Verbindung massenmedialer Eigenschaften (tertiäre Medien), erlauben aber den jederzeitigen, schnellen Wechsel zwischen individualer und Gruppenansprache bzw. Kommunikation, aber immer unter Bedingungen, die auf beiden Seiten der Kommunikation auf Geräte angewiesen ist.

Allein die Technizität eines Mediums definiert dieses aber noch nicht als Massenmedium, vielmehr muss dieses Medium in den sozialen Prozess der Massenkommunikation integriert sein. So ist beispielsweise ein nicht für den Markt produziertes, sondern für einen privaten Empfängerkreis bestimmtes Buch zwar als Printmedium technisch hergestellt, es fungiert aber nicht als Massenmedium. Dasselbe gilt für Hörfunktechnik, wenn sie im Küstenfunk eingesetzt wird, oder für Fernsehtechnik im Rahmen der Videoüberwachung.

Mediengeschichte im allgemeinen Sinn bezeichnet die historische Entwicklung der Kommunikationsmittel. Sie fokussiert vor allem auf Massenmedien wie Presse, Hörfunk und Fernsehen. Der Begriff „Medien“ etablierte sich erst in den 1960er-Jahren. Das Wort wurde vom englischen Begriff „mass media“ übertragen, der bereits in den 1920er Jahren aufkam.

Es gibt zahlreiche verschiedene Ansätze zur Mediengeschichte. Neben Einflussfaktoren und charakteristischen Merkmalen gibt es auch eine Reihe von Grundproblematiken, die das Schreiben einer Mediengeschichte erschweren. Medien sind vielfältig und in sich komplex. Zudem sind ihre Ausprägungen nationalspezifisch und die mediale Entwicklung hoch different. Mit Mediengeschichte als Wissenschaft haben sich in Deutschland vor allem Klaus Merten, Knut Hickethier und Werner Faulstich auseinandergesetzt.

Die mit Massenmedien einhergehende Massenkommunikation ist gegenüber der Individualkommunikation durch eine fehlende Auswahl der Empfänger gekennzeichnet, d. h., die Rezipienten sind nicht im Vorhinein festgelegt, sie sind räumlich verstreut (im Gegensatz zum „Präsenzpublikum“ z. B. bei einem Theaterstück, einem Vortrag oder einem Konzert) und ihre Anzahl ist prinzipiell unbegrenzt. Dieses disperse Publikum ist kein überdauerndes soziales Gebilde, die Rezipienten oder Rezipientengruppen sind untereinander anonym, unstrukturiert, unorganisiert und inhomogen (Menschen aus unterschiedlichen sozialen Schichten, mit unterschiedlichen Einstellungen, Lebensweisen und Interessen). Die Bezeichnung „Masse“ deutet in diesem Zusammenhang auf die unbestimmt große Anzahl von Menschen, an die Aussagen übermittelt werden, ohne sie persönlich individuell zu adressieren – in Abgrenzung zum soziologischen Begriff der Masse in seiner massenpsychologischen oder kulturkritischen Dimension.

Entsprechend fungiert das Internet nicht zwangsweise als Massenmedium, da auch hier Individualkommunikation möglich ist (z. B. bei der Nutzung von E-Mail-Systemen).

Meist kommt dazu eine räumliche Distanz, wie z. B. bei Live-Sendungen in Hörfunk und Fernsehen, oder eine räumliche und zeitliche Trennung zwischen Kommunikator und Rezipienten, etwa beim Lesen einer Zeitung oder beim Sehen einer bereits aufgezeichneten Fernsehsendung.





</doc>
<doc id="14482" url="https://de.wikipedia.org/wiki?curid=14482" title="Paraquat">
Paraquat

Paraquat ist eine quartäre Ammoniumverbindung aus der Familie der Bipyridin-Herbizide, die als Kontaktherbizid eingesetzt wird. Paraquat wurde von der englischen Firma Imperial Chemical Industries (deren Agrarsparte heute Teil der Schweizer Syngenta ist) 1955 entwickelt und kam 1962 erstmals unter dem Handelsnamen Gramoxone auf den Markt. Syngenta dominiert das rund eine Milliarde Dollar schwere globale Geschäft mit Paraquat, mit geschätzten 40 Prozent Marktanteil, also etwa 400 Millionen Dollar.

Paraquat kann durch Reaktion von 4,4′-Bipyridin und Methylchlorid gewonnen werden. Ersteres entsteht durch Kupplung von Pyridin mit Natrium in flüssigem Ammoniak und Dehydrierung mit Sauerstoff.
In wässrigen Lösungen nimmt das Paraquat-Kation in einer reversiblen Reaktion Elektronen auf. Das dabei entstehende Radikal färbt die Lösung kräftig violett, daher wird die Substanz gelegentlich als "Methylviologen" bezeichnet.

Paraquat wird, insbesondere bei feucht-warmem Klima, sehr schnell durch Pflanzenoberflächen absorbiert. In den Chloroplasten werden Elektronen vom Photosystem I auf das Paraquat-Kation übertragen, das dadurch zum Paraquat-Radikal wird. Das Radikal gibt sein überschüssiges Elektron an ein Sauerstoffmolekül ab, es entsteht Hyperoxid. Hyperoxide sind chemisch sehr reaktiv und zerstören ungesättigte Fettsäuren in den Chloroplasten- und Zellmembranen. Da das Kation durch Elektronen immer wieder zum Radikal reduziert wird, setzt sich dieser Vorgang fort, bis das Photosystem zerstört ist.
Die Zellmembran wird porös und es kommt zu Wasserverlust. Die Pflanzen vertrocknen bei sonnigem Wetter innerhalb weniger Stunden.

Paraquat wird gegen breitblättrige Pflanzen und Gräser eingesetzt. Da es die Rinde von Bäumen nicht durchdringt, kann es zur Unkrautbekämpfung in Obst- und Weingärten sowie beispielsweise in Kaffee-, Tee-, Ölpalmen- oder Bananenplantagen verwendet werden. Es ist ein Hilfsmittel der für Trockengebiete propagierten pfluglosen Bodenbearbeitung. Trotz seines häufigen Einsatzes gibt es nur wenige unbedeutende Fälle von Paraquat-Resistenz bei Unkräutern.

Seiner hohen Toxizität wegen ist Paraquat unter anderem in der EU und in der Schweiz seit langem verboten. In den USA wird Paraquat noch verwendet, in China wird es sukzessive aus dem Verkehr gezogen.

In Südafrika werden mithilfe von Paraquat Brandschneisen freigehalten, um Wald- und Buschbränden vorzubeugen.
Paraquat wird auch verwendet, um Pflanzenmaterial schneller trocknen zu können. Beispielsweise führt es dazu, dass Ernterückstände nach dem Anbau von Ananas schneller trocknen und eher verbrannt werden können.

Paraquat wird wie MPP auch zur Induktion von Morbus Parkinson in experimentellen Modellsystemen eingesetzt.

Die LD bei Ratten liegt bei 57-87 mg/kg Körpergewicht, für den Menschen wird die LD auf 35 mg/kg Körpergewicht geschätzt.
Paraquat führt zu vielen tödlichen Vergiftungsfällen. Früher kam es als rötlich-braune, geruchlose Lösung in den Handel. Wenn diese in leere Getränkeflaschen umgefüllt wurde, konnte sie mit Cola-Getränken oder Rotwein verwechselt werden. Zudem wird Paraquat häufig von Selbstmördern verwendet.
Zur Vorbeugung wird Paraquat-Formulierungen seit Mitte der 1970er Jahre meistens, jedoch nicht immer, ein auffälliger blauer Farbstoff, eine Substanz mit stechendem Geruch sowie ein schnell wirkendes Brechmittel zugesetzt.
Erste Symptome einer Vergiftung mit Paraquat sind häufig ein starkes Brennen im Mund und Hals, Schmerzen im Unterleib, Appetitlosigkeit, Schwindel, Erbrechen und Durchfall. Daneben können Kurzatmigkeit, Herzrasen, Nierenversagen, Schmerzen in der Lunge und Schädigungen der Leber auftreten.
Bei einer Paraquatvergiftung muss eine Sauerstoffgabe gut abgewogen werden, da sie zu einer Fibrosierung der Lunge führt. Ursächlich hierfür ist die Anreicherung von Paraquat im Lungengewebe. Der Sauerstoff regeneriert das Paraquat durch Oxidation und führt gleichzeitig zur Bildung von Wasserstoffperoxid-Radikalen, die vor allem die Lunge, aber auch Leber und Niere schädigen.
Die Aufnahme einer tödlichen Dosis führt zu Krämpfen, Koordinationsstörungen und schließlich zu einer irreversiblen Lungenfibrose. Der Tod tritt nach einigen Tagen, manchmal erst nach mehreren Wochen ein.

Laut WHO beträgt die Erlaubte Tagesdosis 0,004 mg/kg Körpergewicht. Bei Vergiftungen durch Paraquat wird die sofortige Gabe von Aktivkohle empfohlen, später gesteigerte Diurese, im Frühstadium auch Hämoperfusion. Neben primärer und sekundärer Giftelimination kommt der symptomatischen Therapie besondere Bedeutung zu. Es existiert kein Antidot.

Die US-Umweltbehörde EPA stufte Paraquat als "möglicherweise krebserregend" und als schwach mutagen ein. Bei hohen Dosen kann eine fruchtschädigende Wirkung auftreten.

Problematisch ist der Einsatz von Paraquatpräparaten durch Kleinbauern in Entwicklungsländern, die Pflanzenschutzmittel oft ohne die notwendigen Schutzmaßnahmen anwenden. So werden die Mittel häufig unsachgemäß gelagert, es wird keine Schutzbekleidung getragen bzw. sogar barfuß und in nicht körperbedeckender Kleidung gearbeitet, Kinder sind bei der Feldarbeit anwesend und Mindestabstände zu Gewässern werden mangelhaft oder gar nicht eingehalten. Die Folgen sind teils schwere und chronische Erkrankungen. Ein Grund für die unsachgemäße Verwendung ist, dass die Benutzungshinweise für viele der Anwender unverständlich sind, da sie nicht lesen können oder eine andere Sprache sprechen. Entwicklungspolitische Organisationen werfen den Herstellern vor, sich nicht für eine Verbesserung der Situation einzusetzen, da dies den Umsatz beeinträchtigen könne.

Die Giftigkeit für Fische ist von der Fischart und der Wasserhärte abhängig; eine Bioakkumulation wird bei Wassertieren nicht beobachtet. Paraquat adsorbiert leicht an Oberflächen (Sediment, Schwebstoffe) und wird nur langsam abgebaut.

Im Boden wird Paraquat von Tonmineralen und Humus gut absorbiert und daher kaum ausgewaschen. Durch die starke Adsorption wird die schädigende Wirkung von Paraquat abgepuffert, andererseits kann es so viele Jahre im Boden erhalten bleiben. Die Halbwertszeit (DT-Wert) wird von der FAO mit 1000 Tagen angegeben. Auf der Oberfläche von Pflanzen und unter Lichteinwirkung werden Paraquatrückstände hingegen rasch zersetzt.

Für Vögel scheint Paraquat nur mäßig giftig zu sein; Paraquat ist nicht bienengefährlich.

Paraquat ist weltweit in rund 100 Ländern zugelassen (darunter USA, Kanada, Australien, Japan, Neuseeland) und ist mittlerweile in über fünfzig Nationen verboten, in der Schweiz seit 1989, in der EU seit 2007. In der Schweiz hatte Syngenta jahrelang behauptet, Paraquat wegen fehlender Nachfrage vom Markt genommen zu haben. Der Bundesrat widersprach dieser Darstellung im Jahr 2002. Die Nichtregierungsorganisation Erklärung von Bern veröffentlichte 2011 ein Interview mit Urs Niggli, der bei der Forschungsanstalt Wädenswil für die Bewilligung zuständig war. Demnach hatte die Maag AG (heute Syngenta) in den 1980er Jahren versucht, Paraquat in der Schweiz wieder auf den Markt zu bringen. Die Forschungsanstalt Wädenswil lehnte das Gesuch aus toxikologischen und ökotoxikologischen Gründen ab. Nach dem Großbrand von Schweizerhalle zog die Maag AG den Einspruch gegen diese Entscheidung zurück.

Der Europäische Gerichtshof hat am 11. Juli 2007 in erster Instanz die Zulassungsrichtlinie für Paraquat aufgehoben. Im Oktober 2007 wurde von Syngenta ein Wiederzulassungsantrag vorbereitet. Im Februar 2009 entschied sich dann Syngenta aus wirtschaftlichen Gründen, keinen neuen Antrag zu stellen. Am 1. April 2011 hat das „Chemical Review Committee“ der UNEP-/FAO-Rotterdam-Konvention die Aufnahme von Gramoxone Super (Aktive Substanz: 200 g Paraquat/L) in die „Prior Informed Consent (PIC) procedure“ der Rotterdam-Konvention empfohlen. Jedoch wurde die Aufnahme in die PIC-Liste an der Konferenz von März/April 2013 von Guatemala und Indien blockiert. Die Diskussion wurde auf die nächste Konferenz im Jahr 2015 verschoben.

Ein juristisches Gutachten aus dem Jahr 2011, welches im Auftrag des European Center for Constitutional and Human Rights (ECCHR) und der Erklärung von Bern (EvB) erstellt wurde, kommt zum Schluss, dass Syngenta mit dem Verkauf seines Herbizids Paraquat in Entwicklungsländern elementare Menschenrechte missachtet.

Brasilien hat 2017 beschlossen, Paraquat aufgrund seiner gesundheitsschädigenden Wirkung mit einer dreijährigen Übergangsfrist zu verbieten.

1985 wurden in Fukuyama und Hiroshima mindestens zwölf Menschen mit Getränken vergiftet, die durch einen unbekannten Täter mit Paraquat vergiftet worden waren. Der Modus Operandi war stets derselbe: Auf der Oberseite von Getränkeautomaten oder im Automaten wurden scheinbar nicht zu beanstandende Getränkedosen zurückgelassen, die durch das darin enthaltene geschmackslose Paraquat ihre Finder töteten.

Nachdem auf diese Art und Weise zwölf Menschen getötet worden waren, wurden durch viele Automatenbetreiber Warnungen an den Automaten befestigt, auf denen dringend davon abgeraten wurde, aus herrenlosen Dosen zu trinken. Nachdem diese Warnungen veröffentlicht worden waren, kam es zu keinen weiteren Vergiftungen. Der oder die Täter wurden nie gefasst.



</doc>
<doc id="14485" url="https://de.wikipedia.org/wiki?curid=14485" title="John Rawls">
John Rawls

John Rawls (* 21. Februar 1921 in Baltimore, Maryland; † 24. November 2002 in Lexington, Massachusetts) war ein US-amerikanischer Philosoph, der als Professor an der Harvard University lehrte. Sein Hauptwerk "A Theory of Justice" (1971) gilt als eines der einflussreichsten Werke der politischen Philosophie des 20. Jahrhunderts.

Rawls war das zweite von fünf Kindern des Rechtsanwaltes William Lee Rawls und seiner Ehefrau Anna Abell Stump. Der Tod zweier Brüder infolge von Diphtherieerkrankungen überschattete seine Jugend. Rawls studierte ab 1939 am College der Princeton University, wo er sich für Philosophie zu interessieren begann. 1943 schloss er das Studium mit einem "Bachelor of Arts" ab und ging zur Armee. Im Zweiten Weltkrieg diente Rawls als Infanterist im Pazifik, wo er auf Neuguinea, den Philippinen und in Japan eingesetzt wurde. Er besuchte Hiroshima nach dem Abwurf der Atombombe. Diese Erfahrung brachte ihn dazu, eine Offizierskarriere, die ihm angeboten wurde, abzulehnen und die Armee im untersten Dienstgrad eines Private 1946 zu verlassen.

Nach seinem Abschied von der Armee kehrte Rawls nach Princeton zurück und promovierte dort 1950 in Philosophie mit einer Arbeit zur moralischen Beurteilung menschlicher Charakterzüge. Nach kurzer Lehrtätigkeit in Princeton erhielt Rawls 1952 ein Fulbright-Stipendium für einen einjährigen Forschungsaufenthalt an der englischen Oxford University, wo er von Isaiah Berlin, Stuart Hampshire und vor allem H.L.A. Hart beeinflusst wurde. Nach seiner Rückkehr in die Vereinigten Staaten hatte Rawls Professuren an der Cornell University und dem Massachusetts Institute of Technology inne. 1962 wechselte er an die Harvard University, wo er mehr als dreißig Jahre lehrte. 1966 wurde Rawls in die American Academy of Arts and Sciences gewählt. Ihm wurde für sein Buch "A Theory of Justice" 1972 der Ralph-Waldo-Emerson-Preis der Phi Beta Kappa Society verliehen. 1995 erlitt er den ersten von mehreren Schlaganfällen, die ihn bei seiner Arbeit stark behinderten. Trotzdem gelang es ihm, sein letztes Werk "The Law of Peoples" abzuschließen, in dem er eine liberale Theorie des Völkerrechts entwickelt. 1999 wurde ihm die National Humanities Medal verliehen.

Rawls, der als ausgesprochen uneitler und bescheidener Mensch beschrieben wird, starb am 24. November 2002 in seinem Haus in Lexington an Herzversagen. Er hinterließ seine Frau Margaret Warfield Fox Rawls, mit der er seit 1949 verheiratet war, und vier Kinder: Anne Warfield, Robert Lee, Alexander Emory und Elizabeth Fox. Die Hamburger Wochenzeitung Die Zeit veröffentlichte in einer Ausgabe gleich drei Nachrufe auf Rawls. Den Nachruf in der Süddeutschen Zeitung schrieb der Tübinger Philosoph Otfried Höffe, den Nachruf in der Frankfurter Rundschau der Frankfurter Philosoph Rainer Forst. Clemens Sedmak, ein österreichischer Theologe, schrieb den Nachruf für die Wochenzeitung Die Furche. Der Aachener Philosoph Wilfried Hinsch, der sich über die Gerechtigkeitstheorie von Rawls habilitierte, verfasste den Nachruf für die NZZ.

Rawls gilt als wesentlicher Vertreter des egalitären Liberalismus. Als Prämisse seines Werkes setzt er die Gerechtigkeit als maßgebliche Tugend sozialer Institutionen, die aber die Freiheit des Einzelnen nicht verletzen darf:

Die Aufgabe von Gerechtigkeitsgrundsätzen besteht ihm zufolge darin, die Grundstruktur der Gesellschaft festzulegen, d. h. die institutionelle Zuweisung von Rechten und Pflichten und die Verteilung der Güter. Wie aus der Bezeichnung seiner Theorie („Gerechtigkeit als Fairness“) und seinen Überlegungen zur Rechtfertigung ersichtlich wird, ist seine Gerechtigkeitstheorie eine Theorie der Verfahrensgerechtigkeit.

Rawls stellt sich dazu die Frage: Für welche Gerechtigkeitsgrundsätze würden sich freie und vernünftige Menschen in einer fairen und gleichen Ausgangssituation in ihrem eigenen Interesse entscheiden? Er argumentiert, dass zwei Grundsätze gewählt würden, deren Inhalt er in letzter Hand – nach einigen Veränderungen und Umarbeitungen gegenüber der ursprünglichen Fassungen – folgendermaßen formuliert:

Der erste Grundsatz hat Vorrang vor dem zweiten. Dasselbe gilt für die beiden Unterpunkte im zweiten Grundsatz: Es ist nicht erlaubt, die Chancengleichheit zu beschneiden, um dem Differenzprinzip mehr Geltung zu verschaffen. In Abgrenzung zum von ihm kritisierten Utilitarismus will er mit diesen Vorrangregeln verhindern, dass zugunsten der Güterverteilung auf Freiheiten verzichtet werden darf.

Hieran macht sich auch ein großer Teil der Kritik an Rawls Thesen fest: In der Praxis ist es nicht außergewöhnlich, dass Menschen zugunsten materieller Güter auf Freiheiten verzichten. Zunächst muss ein Mensch die Grundbedingungen dafür erfüllen, überhaupt "seine Freiheit" als oberstes Prinzip verteidigen zu wollen: Er muss seine Grundbedürfnisse befriedigt sehen. Der Verhungernde wird eher in die Sklaverei einwilligen als seinen sicheren Tod in Kauf nehmen. Auch demokratische Teilhaberechte und damit Freiheiten im Rawlschen Sinne genießen nicht in jeder Kultur denselben Stellenwert. Zudem sind Menschen beispielsweise wegen körperlicher Einschränkungen auch nicht immer in der Lage, die formal gewährten Freiheiten vollständig auszunutzen.

Rawls stellt die umfassende Doktrin eines säkularen „aufgeklärten Liberalismus“ selbst in die Nähe einer religiöser Doktrin: Es gäbe viele Liberalismen mit verschiedenen auch kulturspezifischen Interpretationen von Freiheit, Gleichheit und Gerechtigkeit, die auch religiös begründet werden könnten. Auch auf der Basis der Scharia könne eine konstitutionelle Demokratie gegründet werden.

Deutung:

Tatsächlich wird es in unserer Gesellschaft als ungerecht angesehen, wenn jemand wegen eines Mangels an Talenten durch sämtliche soziale Ränge fällt, weil das System entgegen dem Differenzprinzip Ungleichheiten schafft, denen sich die Person machtlos ausgeliefert sieht. Extreme Beispiele könnten körperlich und geistig behinderte Menschen betreffen.

Rawls konstruiert einen hypothetischen Urzustand in Form einer fairen und gleichen Verhandlungssituation, die die Gerechtigkeitsprinzipien legitimieren soll. In dieser rein theoretischen Situation wird der Gesellschaftsvertrag geschlossen,
der anders als in früheren Vertragstheorien nicht den Eintritt in eine bestimmte Gesellschaft regelt, sondern nur bestimmte Prinzipien festlegt, nach denen Gerechtigkeit realisiert werden kann. 
Annahmen:
Die Personen besitzen nur allgemeines Wissen (um gesellschaftliche Grundgüter, derer jedermann zur Verwirklichung seiner verschiedenen Interessen bedarf, Wissen um gesellschaftliche, politische, wirtschaftliche und psychologische Zusammenhänge, die Fähigkeit, Folgen abzuschätzen usw.), aber kein Wissen über sich selbst, ihre eigene soziale Stellung, ihre Interessen, Kenntnisse, Talente usw. sowie über die künftige konkrete politische Ausgestaltung ihrer Rechte und Pflichten.

Verfahren:

Warum würden sich die Menschen im Urzustand für die beiden Gerechtigkeitsprinzipien entscheiden?

Bedingung der Stabilität einer Gerechtigkeitsvorstellung:

Besonders Utilitaristen, Vertreter des politischen Liberalismus, Libertäre und Kommunitaristen stehen dem Werk Rawls’ kritisch gegenüber.

Utilitaristen sind von der scharfen Gegenüberstellung vertragstheoretischer und utilitaristischer Begründungen der Gerechtigkeit nicht überzeugt. John Harsanyi beschrieb bereits vor Rawls das Gedankenspiel einer Wahl von Grundsätzen hinter einem Schleier des Nichtwissens. Verstanden als rationale Entscheidung unter Risikobedingungen führe diese zur Maximierung des Durchschnittsnutzens und damit zum Bayes’schen Kriterium. Ließe sich der Unterschied zwischen Rawls’ Vertragstheorie und dem Utilitarismus Harsanyis tatsächlich auf die Frage zurückführen, ob unter den Bedingungen des Schleiers des Nichtwissens als Prinzip das Bayes’sche oder das des Maximin zu wählen sei, dann würde es sich um eine eher marginale entscheidungstheoretische Kontroverse handeln (→ Risikoethik#Mögliche Entscheidungskriterien, Gleichwahrscheinlichkeitsmodell).

Libertäre sehen besonders in Rawls’ Differenzprinzip eine Beschneidung der individuellen Freiheit. Jede Aneignung und jede Übertragung von Gütern sei legitim, solange sie nur ohne Zwang und Verletzung von Grundrechten zustande gekommen ist. Staatliche Korrektureingriffe zur Korrektur von Ungleichverteilungen dagegen seien unzulässig. Im Gegensatz zu Rawls zeichnet der Libertarismus eines auf einem Markt und nicht auf Verteilungsgerechtigkeit gründendes gesellschaftliches Interaktionsmodell. Nur drei Jahre nach dem Erscheinen von "A Theory of Justice" formulierte Robert Nozick mit "Anarchy, State, and Utopia" ein die individuellen Rechte ins Zentrum stellendes libertäres Gegenmodell. Für ihn ist lediglich ein Minimalsystem an Regeln des Zusammenlebens legitimierbar, das sich aus dem möglichen Gewinn und der Wahrung der Individualrechte aller ergibt.

Michael Sandel übt Kritik an Rawls im Rahmen des Kommunitarismus. Er kritisiert die Charakterisierung der Personen im Urzustand als zu individualistisch. Sandel versucht über eine solche Kritik die Rawls’sche Theorie als ganze in Frage zu stellen. Überdies stütze sich das ganze liberale Theoriengebäude auf jenes in Frage gestellte Menschenbild. Sandels Kritik setzt an der Konzeption des Urzustandes an. Das von Rawls konzipierte Selbst sei unwirklich, weil es nicht durch gemeinschaftliche Bindung geprägt ist, vielmehr gesellschaftlich isoliert entscheidet. Diese in der Theorie verwendete Konzeption der Person impliziere eine Anthropologie, die im Widerspruch zu den beobachtbaren moralischen Werten realer Personen stehe.

Jürgen Habermas kritisiert, dass Rawls’ vertragstheoretisch gewonnene Gerechtigkeitskonzeption keiner öffentlichen Überprüfung ausgesetzt wird. Wolfgang Kersting schließt sich dieser Kritik ebenso wie der von Nozick an und postuliert, dass demokratisch verfasste Gesellschaften keiner moralischen Geschäftsgrundlage bedürften, die über die Verfassung und eine geteilte politische Kultur hinausreichen. So könne von Menschen, die ihr ganzes Leben durch eine religiöse Ethik bestimmen lassen, nicht erwartet werden, dass sie bei der Behandlung zentraler gesellschaftlicher Fragen ihre religiösen Überzeugungen ausklammern und nur Gründe vorbringen, die auf allgemeine Akzeptanz in einer säkularen und pluralistischen Gesellschaft stoßen. Hier reiche die Konzeption Thomas Hobbes aus, Grundrechte auf der Basis allgemeiner Gesetzestreue zu gewähren.

Bücher

Aufsätze




</doc>
<doc id="14487" url="https://de.wikipedia.org/wiki?curid=14487" title="Ticino (Fluss)">
Ticino (Fluss)

Der Tessin (italienisch Ticino, in der Antike Ticinus) ist ein linker Nebenfluss des Po, der von der Schweiz nach Norditalien fliesst. Er hat dem Kanton Tessin den Namen gegeben.

Der Tessin entspringt beim Nufenenpass südwestlich des Gotthardmassivs und durchfliesst der Reihe nach das Bedrettotal, die Leventina, die Riviera und die Magadinoebene, bevor er in den Lago Maggiore (Langensee) mündet.

Auf der italienischen Seite verlässt er den See bei Sesto Calende und fliesst durch die Poebene, wo er südöstlich von Pavia in den Po mündet. 

Seine Länge beträgt 248 km, das Einzugsgebiet 7'228 Quadratkilometer. Die wichtigsten Nebenflüsse sind der Brenno, die Moësa, die Maggia und der Toce.

An den Ufern des Flusses, vermutlich in der Nähe von Vigevano, rund 25 km nordwestlich von Pavia, fand 218 v. Chr. das Gefecht am Ticinus zwischen den Römern und dem karthagischen Feldherrn Hannibal statt.


</doc>
<doc id="14488" url="https://de.wikipedia.org/wiki?curid=14488" title="Halle">
Halle

Halle steht für:

Geographie:

Personen:
Hallé ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="14490" url="https://de.wikipedia.org/wiki?curid=14490" title="Divisor">
Divisor

Der Begriff des Divisors spielt in der Algebraischen Geometrie und der Komplexen Analysis eine wichtige Rolle bei der Untersuchung Algebraischer Varietäten bzw. Komplexer Mannigfaltigkeiten und der darauf definierten Funktionen. Unterschieden werden müssen dabei der "Weil-Divisor" und der "Cartier-Divisor", welche in bestimmten Fällen übereinstimmen.

Ursprünglich kommt dem Divisor im eindimensionalen Fall die Bedeutung zu, die Null- und Polstellenmenge einer rationalen bzw. meromorphen Funktion vorzuschreiben, und es stellt sich die Frage, für welche Divisoren eine solche Realisierung möglich ist, was eng mit der Geometrie der Varietät bzw. Mannigfaltigkeit verknüpft ist.

Sei formula_1 ein Gebiet oder eine Riemannsche Fläche. Eine Abbildung formula_2 heißt Divisor in formula_3, falls ihr Träger formula_4 in formula_3 abgeschlossen und diskret ist. Die Menge aller Divisoren auf formula_3 bildet bezüglich der Addition eine abelsche Gruppe, die mit formula_7 bezeichnet wird. Auf dieser Gruppe führt man eine partielle Ordnung ein. Seien formula_8, dann setzt man formula_9, falls formula_10 für alle formula_11 gilt.

Zu jeder von Null verschiedenen meromorphen Funktion formula_12 kann ein Divisor formula_13 definiert werden, indem der Divisor jedem Punkt aus formula_3 die Null- beziehungsweise die Polstellenordnung zuordnet:

formula_15

Ein Divisor, der gleich dem Divisor einer meromorphen Funktion ist, heißt "Hauptdivisor".

Der Weierstraßsche Produktsatz besagt, dass in formula_16 jeder Divisor ein Hauptdivisor ist. In einer kompakten Riemannschen Fläche gilt dies jedoch nicht mehr und ist vom Geschlecht der Fläche abhängig. Dies wird im Artikel Satz von Riemann-Roch näher erläutert.

Sei formula_17 eine ebene algebraische Kurve. Eine formale Summe formula_18 heißt Divisor in formula_17, falls formula_20 außer für endlich viele formula_21. Durch punktweise Addition wird die Menge aller Divisoren in formula_17 zu einer freien Abelschen Gruppe.

Analog zur o.g. Definition definiert man für eine rationale Funktion den Divisor der Funktion. Ein Divisor, der gleich dem Divisor einer rationalen Funktion ist, heißt "Hauptdivisor".

Im Falle formula_23 ist für einen Divisor die Abbildung formula_24 ein Divisor im Sinne der Funktionentheorie. Allerdings gibt es Divisoren im Sinne der Funktionentheorie, die nicht auf diese Weise entstehen, da dort formula_25 für unendlich viele formula_21 (die allerdings keinen Häufungspunkt haben dürfen) zugelassen ist.

Sei formula_27 ein noethersches integres separiertes Schema, regulär in Kodimension 1. Ein "Primdivisor" formula_28 in formula_27 ist ein abgeschlossenes ganzes Unter-Schema der Kodimension Eins. Ein "Weil-Divisor" (nach André Weil) ist dann ein Element der frei erzeugten abelschen Gruppe formula_30 der Primdivisoren und wird meistens als formale Summe formula_31 geschrieben, wobei nur endlich viele formula_32 von Null verschieden sind.




Sei formula_27 eine komplexe Mannigfaltigkeit bzw. eine algebraische Varietät und formula_48 bezeichne die Garbe der holomorphen bzw. algebraischen Funktionen auf formula_27 und formula_50 bezeichne die Garbe der meromorphen bzw. rationalen Funktionen auf formula_27. Die Quotienten-Garbe formula_52 heißt "Garbe der Divisoren", und ein Schnitt in formula_53 heißt Cartier-Divisor (nach Pierre Cartier), meist nur als "Divisor" bezeichnet. Die Menge aller Schnitte formula_54 bildet eine Abelsche Gruppe.



Sei formula_27 ein noethersches integres separiertes Schema, dessen lokale Ringe alle faktoriell sind. Dann ist die Gruppe formula_30 der Weil-Divisoren auf formula_27 isomorph zur Gruppe der Cartier-Divisoren formula_54. Dieser Isomorphismus erhält die Eigenschaft, Hauptdivisor zu sein und führt die Quotientengruppen formula_46 und formula_56 ineinander über.



</doc>
<doc id="14491" url="https://de.wikipedia.org/wiki?curid=14491" title="Bruchrechnung">
Bruchrechnung

Im engeren Sinn bezeichnet Bruchrechnung das Rechnen mit "gemeinen Brüchen" (manchmal auch "gewöhnlichen Brüchen") in der „Zähler-Bruchstrich-Nenner-Schreibweise“ (siehe unten). Bruchrechnung gehört damit zur Arithmetik, einem Teilgebiet der Mathematik.

In einem weiteren Sinn wird das Wort auch für das Rechnen mit rationalen Zahlen gebraucht, gleichgültig, in welcher Schreibweise sie vorliegen.

Eine wichtigere Erweiterung besteht in der Zulassung von "Bruchtermen", das sind Ausdrücke, die formal wie gemeine Brüche gebildet werden, bei denen aber Zähler und Nenner Terme sein können, die Variablen enthalten. Für diese Bruchterme gelten die Bruchrechenregeln sinngemäß. Das Rechnen mit Bruchtermen gehört aber zur Algebra.

Die Regeln der Bruchrechnung beziehen sich auf die Grundrechenarten, also auf Addition, Subtraktion, Multiplikation, Division, sowie auf die Kehrwertbildung. Insbesondere bei Bruchtermen kommen auch Regeln für Potenzen und Wurzeln hinzu.

Außerdem gibt es eine Kürzungs- und Erweiterungsregel, die eine Besonderheit der Bruchrechnung ist. Sie beruht auf dem Unterschied zwischen "Bruch und Bruchzahl", der im folgenden Abschnitt genauer dargestellt wird.

Die Bruchschreibweise, also die Schreibweise mit Bruchstrich, wird ganz allgemein in verschiedenen Bereichen der Mathematik, besonders in der Algebra, immer dann verwendet, wenn in der untersuchten Struktur die elementaren Bruchrechenregeln, insbesondere die Kürzungs- und Erweiterungsregel, gelten. Auch hier spricht man immer dann von „Bruchrechnung“, wenn diese Regeln angewendet werden.

 Die Bruchrechnung beruht darauf, dass sich das "Ganze" (die "Eins" aus dem Rechnen mit natürlichen Zahlen) noch unterteilen lässt. "Einen" Kuchen kann man zum Beispiel in vier Teile teilen. Wenn diese Teile gleich groß sind, so ist jedes Teil ein Viertel des Kuchens. Wenn, wie im Bild, eines der Viertel schon fehlt, so sind drei Viertel Kuchen dargestellt.
Geschrieben wird dies gewöhnlich in der „Zähler-Bruchstrich-Nenner-Schreibweise“: Die Zahl "unter" dem Bruchstrich, der "Nenner" gibt an, in wie viele Teile das Ganze geteilt worden ist; die Zahl "über" dem Bruchstrich, der "Zähler" gibt an, wie viele Teile davon in diesem Falle gemeint sind. So erhält man einen "Bruch". Man kann diesen auch so deuten: Der Zähler gibt an, wie viele Ganze gemeinsam in soviele gleich große Teile zu teilen sind, wie der Nenner angibt. (Man legt drei Kuchen übereinander und teilt den Stapel in vier gleiche Teilstapel.)

Wird das Ganze (die Torte) stattdessen in "acht" Teile geteilt und werden davon "sechs" genommen, so ist das ein anderer Bruch: formula_1 statt formula_2. Aber diese beiden Brüche stehen offenbar für die gleiche Menge Kuchen: Sie stehen für dieselbe "Bruchzahl".

Für jede "Bruchzahl" gibt es viele (unendlich viele) verschiedene "Darstellungen", verschiedene "Brüche", die alle denselben Wert (dieselbe Größe) verkörpern, aber auf unterschiedliche Weise. Von einem Bruch zum anderen gelangt man durch Erweitern und Kürzen. Dadurch ändert sich der "Wert" einer Bruchzahl nicht, man erhält aber für diese Zahl verschiedene Darstellungsweisen: verschiedene Brüche.

Brüche lassen sich zunächst in gemeine Brüche (auch "gewöhnliche Brüche" genannt) und Dezimalbrüche (= Dezimalzahl, umgangssprachlich: „Kommazahl“) einteilen, daneben gibt es noch die Darstellung als gemischter Bruch. Wenn man von einem Bruch spricht, meint man in der Regel einen gemeinen Bruch, das Rechnen mit Dezimalbrüchen wird meistens nicht als Bruchrechnung bezeichnet.

In der nachfolgenden Tabelle sind gebräuchliche Bezeichnungen für Brüche zusammengefasst, die in diesem Abschnitt erklärt werden. Die in der Tabelle weiter unten stehenden Begriffe fallen jeweils unter die darüberstehenden Oberbegriffe, zum Beispiel ist jeder Scheinbruch ein gemeiner Bruch, nebeneinanderstehende Begriffe müssen sich nicht ausschließen. Dabei ist zu beachten, dass es sich um Bezeichnungen für "Zahlschreibweisen" und nicht für die dargestellten Zahlen handelt. Eine bestimmte Zahl kann verschiedene Darstellungen haben, die jeweils mit unterschiedlichen Begriffen aus der Tabelle bezeichnet werden. So kann man zum Beispiel jeden unechten Bruch auch als gemischten Bruch schreiben.

Weitere Formen, in denen Bruchzahlen dargestellt werden können (Kettenbruch, Prozent- und Promilleschreibweise, Binärbrüche usw.) werden in je eigenen Artikeln behandelt und in dieser Tabelle nicht aufgeführt.

Gemeine Brüche werden im Allgemeinen durch eine Übereinanderstellung von "Zähler" und "Nenner", getrennt durch einen waagerechten Strich, dargestellt:
Zähler und Nenner eines Bruches sind ganze Zahlen. Dabei darf der Nenner formula_4 nicht null sein, da eine Division durch Null nicht definiert ist.

Jeder Bruch kann nämlich auch als Divisionsaufgabe verstanden werden. Dabei ist der Zähler formula_5 der Dividend, der Nenner formula_6 der Divisor:
Das Entscheidende bei der Bruchrechnung ist, dass hier "jede" Division (außer durch null) möglich ist und ein einfach darstellbares Ergebnis hat, während ja im Bereich der ganzen Zahlen die Teilbarkeitsregeln gelten.

Üblicherweise werden für Zähler und Nenner natürliche Zahlen verwendet und ein eventuell vorhandenes negatives Vorzeichen wird vor den Bruch gesetzt, also beispielsweise formula_8 statt formula_9 oder formula_10. Sind Zähler "und" Nenner negativ, so bezeichnet das nach den Regeln der Division von ganzen Zahlen den "positiven" Bruch: formula_11

Bei einer Variante dieser Schreibweise, die oft verwendet wird, wenn gemeine Brüche in Texten vorkommen, werden Zähler, Bruchstrich und Nenner hintereinandergeschrieben und als Bruchstrich ein Schrägstrich verwendet, zum Beispiel 1/2, 3/8. Bei der Schreibweise mit Schrägstrich an Stelle des waagrechten Bruchstrichs werden (vor allem) einstellige Zähler und Nenner manchmal verkleinert über bzw. unter den Schrägstrich geschrieben: /. Zu diesem Zweck existieren in vielen Druckzeichensätzen Sonderzeichen, wie zum Beispiel ¾ oder ½.

Wenn bei einem Bruch der Betrag des Zählers kleiner als der des Nenners ist, dann spricht man von einem "echten" oder "eigentlichen" Bruch (z. B. formula_12 oder formula_13), andernfalls von einem "unechten" oder "uneigentlichen" Bruch (z. B. formula_14 oder formula_15).

Echte Brüche sind also die, deren Betrag kleiner ist als ein Ganzes.

Ist der Zähler in einem gemeinen Bruch gleich 1 (z. B. formula_16 oder formula_17), spricht man von einem "Stammbruch", ansonsten von einem "abgeleiteten Bruch" oder "Zweigbruch".

Unechte Brüche, bei denen der Zähler ein ganzzahliges Vielfaches des Nenners ist (z. B. formula_18), bezeichnet man als "Scheinbrüche", da sie sich durch Kürzen in ganze Zahlen umwandeln lassen (im Beispiel in die Zahl 4). Insbesondere lässt sich jede ganze Zahl formula_19 als Scheinbruch formula_20 schreiben.

Unechte Brüche, die keine Scheinbrüche sind, lassen sich immer als gemischte Brüche (auch: als gemischte Zahlen, in gemischter Schreibweise) darstellen.

Dabei wird zunächst der ganzzahlige Anteil, d. h. die zur Null hin gerundete Zahl, geschrieben und anschließend direkt danach der verbleibende Anteil als echter Bruch. Zum Beispiel formula_21 statt formula_22 oder formula_23 statt formula_24 .

Ein Problem der gemischten Schreibweise ist, dass sie als Produkt missverstanden werden kann:

So steht formula_25 meist für formula_26 und nicht für formula_27.

Schreibt man dagegen formula_28, so handelt es sich nicht um einen Bruch in gemischter Schreibweise, sondern (wegen der Variablen) um einen Term. Hier muss das weggelassene Rechenzeichen ein Malpunkt sein (andere Rechenzeichen dürfen in Termen nicht weggelassen werden). formula_28 muss also als formula_30 verstanden werden und niemals als formula_31.

Beim Rechnen mit Brüchen in den vier Grundrechenarten Addition, Subtraktion, Multiplikation und Division werden jeweils zwei Brüche verknüpft, sodass eine dritte Zahl entsteht. Dies darf nicht verwechselt werden mit dem Umformen von Brüchen, wobei ein einziger Bruch eine neue Form erhält, ohne dass sein Wert sich ändert.

Das Umformen (die Formänderung) ist oft die Voraussetzung dafür, dass mit Brüchen gerechnet werden kann. Deshalb wird es hier zuerst behandelt.

Um einen Bruch in eine Dezimalzahl umzuwandeln, dividiert man einfach den Zähler durch den Nenner. formula_32 ergibt 0,75 beziehungsweise 75 % vom Ganzen.

Der Wert der durch einen Bruch dargestellten Bruchzahl ändert sich nicht, wenn man Zähler und Nenner des Bruches mit derselben Zahl (ungleich 0) multipliziert (den Bruch erweitert) oder durch einen gemeinsamen Teiler von Zähler und Nenner teilt (den Bruch kürzt).

Beispiel: formula_33. Von links nach rechts gelesen wurde der Bruch erweitert, von rechts nach links gekürzt.

Der Wert einer in gemischter Schreibweise dargestellten Bruchzahl ändert sich nicht, wenn man den ganzzahligen Anteil als Scheinbruch mit dem Nenner des Bruchteils schreibt und die verbliebenen Bruchanteile hinzuzählt. Umgekehrt kann man bei einem unechten Bruch die Bruchteile, die Ganze ergeben, abspalten und die verbleibenden als Bruch anfügen.

Beispiel: formula_34. Von links nach rechts gelesen wurden Ganze abgespalten, von rechts nach links wurde die gemischte Zahl eingerichtet.

Gemeine Brüche, die in ihrem Nenner übereinstimmen, heißen "gleichnamig". Werden Brüche so erweitert, dass sie danach die gleichen Nenner haben, so nennt man das "gleichnamig machen". Beim praktischen Rechnen sollte dazu der Hauptnenner der Brüche bestimmt werden, das ist das kleinste gemeinsame Vielfache (kgV) der Nenner.

Beispiel: Die Brüche formula_35 sollen gleichnamig gemacht werden. Das kgV der Nenner ist formula_36, also werden alle drei Brüche so erweitert, dass ihr Nenner 42 lautet:
Die gleichnamigen Darstellungen lassen sich nun beispielsweise verwenden, um die dargestellten Bruchzahlen der Größe nach zu ordnen, indem man ihre Zähler vergleicht:

Die Brüche, die addiert oder subtrahiert werden sollen, werden zunächst gleichnamig gemacht, anschließend werden ihre Zähler addiert bzw.  subtrahiert.

Beispiel: formula_40.

Brüche werden multipliziert, indem man ihre Zähler und Nenner miteinander multipliziert. Das Produkt der Zähler ist dann der Zähler des Ergebnisses, das Produkt der Nenner ist dann der Nenner des Ergebnisses.

Beispiel: formula_41.

Durch einen Bruch wird dividiert, indem man mit seinem Kehrwert multipliziert.

Beispiel: formula_42.

Dabei dürfen, wie im Beispiel dargestellt, Zwischenergebnisse gekürzt werden (hier beispielsweise die "3" und die "2" im vorletzten Schritt).

Beim Multiplizieren oder Dividieren von gemischten Brüchen ist es meist nötig, diese zunächst in gewöhnliche Brüche umzuwandeln. (Außer bei ganz einfachen Aufgaben, wie etwa formula_43.)

Beim Addieren und Subtrahieren dagegen ist es viel günstiger, die Ganzen für sich zu betrachten und Bruchrechnung nur bei den verbleibenden echten Brüchen anzuwenden. Beim Addieren kann hier ein zusätzliches Ganzes auftreten, beim Subtrahieren mögen die Bruchteile nicht ausreichen, sodass eines der Ganzen zu einem Scheinbruch aufgeteilt werden muss:

Die folgenden Regeln gelten sowohl beim Bruchrechnen im engeren Sinn als auch beim Rechnen mit Bruchtermen.
Beim Rechnen mit Brüchen stehen die Variablen formula_46 in den Regeln für bestimmte ganze Zahlen. Setzt man stattdessen für diese Variablen andere Ausdrücke, z. B. selbst wieder echte Brüche, Dezimalbrüche oder Terme ein, dann erhält man Regeln für das Rechnen mit Bruchtermen, das Bruchrechnen im weiteren Sinn.

Beim Rechnen mit Brüchen liefern die abstrakten Rechenregeln stets korrekte Ergebnisse, häufig ist die Rechnung mit den „praktischen Rechenregeln“ weniger aufwändig.

Hilfreiche Eselsbrücken hierzu sind:

Aus der Äquivalenz formula_47 für beliebige natürliche Zahlen formula_48 folgt, dass jede rationale Zahl durch unendlich viele verschiedene Brüche dargestellt werden kann, denn es gilt formula_49.

Man dividiert also durch einen Bruch, indem man mit dem Kehrwert des Bruches, der als Divisor fungiert, multipliziert. Die Division wird also auf die Multiplikation zurückgeführt.

Bruchterme, also Rechenausdrücke in der Form von gemeinen Brüchen, spielen in der elementaren Algebra eine wichtige Rolle. Im Allgemeinen enthalten Bruchterme neben Zahlen auch Variablen.
Die Rechenregeln für Brüche können auch auf Bruchterme angewendet werden.

Bei der Bestimmung des Definitionsbereiches eines Bruchterms ist zu beachten, dass der Nenner nicht den Wert 0 haben darf. Beispielsweise wäre der von formula_56 abhängige Bruchterm formula_57 beim Einsetzen von formula_58 nicht definiert.
Der Definitionsbereich ist also formula_59, wenn als Grundmenge die Menge der reellen Zahlen vorausgesetzt wird.
In komplizierteren Fällen sollte der Nenner in Faktoren zerlegt werden, damit der Definitionsbereich erkennbar wird.

Beispiel: formula_60 hat den Definitionsbereich formula_61.

Kürzen bedeutet, dass man Zähler und Nenner durch denselben Rechenausdruck dividiert.
Wichtig dabei ist, dass nur Faktoren von Produkten herausgekürzt werden können. Summen und Differenzen im Zähler und im Nenner müssen gegebenenfalls zuerst
in Produkte zerlegt werden (Faktorisierung).

Beispiele:

Beim Kürzen eines Bruchterms kann sich der Definitionsbereich ändern! So ist im ersten Beispiel der ungekürzte, links stehende Term nur definiert, wenn formula_64 gilt, der rechtsstehende bereits, wenn nur formula_65 gilt. Im zweiten Beispiel ist der ungekürzte Term nur definiert, wenn formula_66 gilt, der gekürzte ist ohne Einschränkungen definiert.

Die Änderung des Definitionsbereiches eines Bruchterms beim Kürzen ist eine der Techniken, mit denen Funktionsterme stetig fortgesetzt werden können.

Wie bei Zahlen ist es nötig, die gegebenen Bruchterme gleichnamig zu machen, d. h. auf den gleichen Nenner zu bringen. Man bestimmt einen möglichst einfachen gemeinsamen Nenner (Hauptnenner), der durch alle gegebenen Nenner teilbar ist.

Beispiel:

Als Hauptnenner ergibt sich formula_68. Die Erweiterungsfaktoren der drei gegebenen Bruchterme erhält man dadurch, dass man jeweils den gefundenen Hauptnenner durch den bisherigen Nenner dividiert. Die Erweiterungsfaktoren sind also formula_69,
formula_70 und formula_68.

Häufig lässt sich der Hauptnenner nur erkennen, wenn man die Nenner in Faktoren zerlegt (Faktorisierung). Dabei greift man oft auf die Methode des Ausklammerns zurück oder verwendet binomische Formeln.

Beispiel:

Beim Multiplizieren von Bruchtermen müssen sowohl die Zähler als auch die Nenner multipliziert werden. Gemeinsame Faktoren von Zähler und Nenner sollten herausgekürzt werden.

Beispiel: formula_79

In komplizierteren Aufgaben sollte man Zähler und Nenner in Faktoren zerlegen, um sie bereits vor der eigentlichen Multiplikation herauskürzen zu können.

Beispiel: formula_80

Die Division von Bruchtermen lässt sich auf die Multiplikation zurückführen. Man dividiert durch einen Bruchterm, indem man mit seinem Kehrwert multipliziert.

Beispiel: formula_81

Brüche kann man oft in sogenannte "Partialbrüche" zerlegen, deren Nenner ganze Potenzen von Primzahlen sind; z. B.:

Es gibt auch Zerlegungen als sogenannte "ägyptische Brüche" (Stammbrüche), z. B.

die alten Ägypter kannten nur solche Summen und haben mit diesen gerechnet.

Das Zahlentripel formula_88 ist ein Beispiel eines "pythagoreischen Bruchs" (siehe auch pythagoreisches Tripel), denn

"Siehe Rationalisierung (Bruchrechnung)."

Die Konstruktion des Körpers der rationalen Zahlen als Brüche aus dem Ring der ganzen Zahlen wird in der abstrakten Algebra durch das Konzept des Quotientenkörpers auf beliebige Integritätsringe verallgemeinert.





</doc>
<doc id="14492" url="https://de.wikipedia.org/wiki?curid=14492" title="Division (Mathematik)">
Division (Mathematik)

Die Division ist eine der vier Grundrechenarten der Arithmetik. Sie ist die Umkehroperation der Multiplikation. Die Division wird umgangssprachlich auch als Teilen bezeichnet. Es wird ein Dividend durch einen Divisor geteilt, das Resultat nennt sich Quotient. Die schriftliche Division ist die Methode des Teilens mit Stift und Papier. Sie wird im Schulunterricht der Grundschule gelehrt und noch einmal in Schulbüchern für die 5. Klasse dargestellt, wird aber nach Einführung elektronischer Hilfsmittel von Lernenden kaum noch angewandt. Rechenzeichen für die Division sind codice_1 als Geteiltzeichen.

Teilen oder Dividieren bedeutet: Zu einer gegebenen Zahl formula_1 (dem ersten Faktor) eine passende Zahl formula_2 (den zweiten Faktor) zu finden, sodass die Multiplikation ein gewünschtes Produkt formula_3 ergibt: Finde zu gegebenem formula_3 und formula_1 ein formula_2 so, dass formula_7.

Beschränkt man sich auf natürliche oder auf ganze Zahlen, so ist dies nicht immer möglich (siehe Teilbarkeit).

In Körpern, zum Beispiel im Körper der rationalen Zahlen oder in den Körpern der reellen sowie der komplexen Zahlen, gilt dagegen:

Für jede Zahl formula_3 und für jede von null verschiedene Zahl formula_1 gibt es genau eine Zahl formula_2, die die Gleichung formula_11 erfüllt.

Die Division ist also die Umkehrung der Multiplikation zur Bestimmung dieses formula_2. Man schreibt
Dabei heißen:

Merkhilfen:

Für die Division gilt weder das Kommutativgesetz noch das Assoziativgesetz. Allerdings lässt sie sich auf die Multiplikation zurückführen, denn es gilt formula_16.

Es kann also von Vorteil sein, die Division als Multiplikation mit dem Kehrwert zu schreiben, da die Multiplikation sowohl assoziativ als auch kommutativ ist und somit ein leichteres und weniger fehleranfälliges Umformen erlaubt. Für die Division gilt allerdings mit der Addition und der Subtraktion das zweite Distributivgesetz, das heißt formula_17 und formula_18.

Man spricht hier auch von der Rechtsdistributivität der Division. Das erste Distributivgesetz (Linksdistributivität) ist jedoch mit der Addition und der Subtraktion im Allgemeinen nicht erfüllt.

Bei mehreren aufeinanderfolgenden Divisionen in einer Zeile wird die Reihenfolge von links nach rechts abgearbeitet; die Division ist daher 

Beispiel aus einer Konditorei: Wenn man einen Kuchen zwischen null Personen aufteilen möchte, wie viel vom Kuchen bekommt dann jede Person?

Es ist nicht möglich, die Frage zu beantworten, da niemand da ist, der die Kuchen bekommen könnte. Übersetzt man diese Frage in die Sprache der Mathematik und abstrahiert von allen möglichen außermathematischen Bedeutungen, wird aus der anschaulichen Frage „Wie verteile ich etwas auf 0 Plätze?“ das rein mathematische Problem „Wie dividiere ich durch 0?“.

 Einige Menschen meinen, dass die Lösung der Division durch null unendlich sein müsse, da erfahrungsgemäß der einzelne immer mehr bekommt, je weniger da sind, mit denen er sich etwas teilen muss. Wenden wir diese Vorstellung des Verteilens auch auf positive Größen an, die kleiner als 1 werden können. Zum Beispiel auf das Verteilen von 1 Liter Wasser in quader- oder zylinderförmige Gefäße mit immer kleinerer Grundfläche, dann wird die Wassersäule desto höher, je kleiner die Grundfläche wird. Tatsächlich gibt es in der Mathematik die Methode des Grenzwertes, mit der (manchmal) ein sinnvolles Ergebnis für eine nicht direkt berechenbare Aufgabe ermittelt werden kann. Wendet man diese Methode auf zum Beispiel formula_20 an, so strebt das Ergebnis tatsächlich gegen unendlich. Allerdings nur, wenn man sich der Null von der positiven Seite aus nähert. Nähert man sich der Null aus Richtung der negativen Zahlen an, passiert das genaue Gegenteil und der Wert der Funktion strebt gegen formula_21. Somit strebt die Funktion an der Stelle formula_22 sowohl gegen formula_23 als auch gegen formula_21, hat also keinen Grenzwert, auch keinen zweideutigen. Auch dies zeigt, dass es nicht sinnvoll möglich ist, formula_25 zu definieren.

Sei formula_26 ein Ring, der nicht der Nullring ist, also mindestens 2 verschiedene Elemente hat. Gesucht sind Lösungen der Gleichung formula_27.


Wie im Abschnitt Ring (Algebra)#Folgerungen gezeigt, folgt aus den Ringaxiomen, maßgeblich dem Distributivgesetz:

Bei elektronischen Rechensystemen, welche eine Gleitkommazahlimplementation haben, die der IEEE 754-Norm folgt oder teilweise folgt (was fast überall der Fall ist), erzeugt eine Gleitkommadivision durch null das Ergebnis formula_46 (bzw. NaN im Falle von 0/0) .

Eine Division durch null mit Festkommazahlen löst auf praktisch allen Rechnern einen Laufzeitfehler (eine Ausnahme) vom Typ Division durch null (engl. "zero-divide-exception") aus. Eine zugehörige Behandlung dieser Ausnahme wird für gewöhnlich von der Laufzeitumgebung der verwendeten Programmiersprache vorgegeben und geleistet, kann aber auch durch den Benutzer zusätzlich, bspw. durch eine codice_2-Anweisung, näher spezifiziert werden. In einigen Laufzeitumgebungen löst eine Division durch null undefiniertes Verhalten aus.

Da der Kernel (in Zusammenarbeit mit der Laufzeitumgebung der Programmiersprache) die fehlerbehandelnde Laufzeitumgebung zur Verfügung stellt, kann eine Division durch null im Kernel selbst ggf. den gesamten Rechner zum Absturz bringen.

Im Bereich der ganzen Zahlen gilt: Eine Division ist nur dann gänzlich durchführbar, wenn der Dividend ein ganzzahliges Vielfaches des Divisors ist. Im Allgemeinen ist die Division hingegen nicht vollständig durchführbar, das heißt, es bleibt ein Rest übrig.

Es gibt mehrere Schreibweisen für die Division: formula_47 oder formula_48 oder formula_49 oder formula_50 

Der Doppelpunkt als Zeichen für die Division ist erst seit Leibniz (1646–1716) allgemein üblich, wenngleich er auch in älteren Schriften bekannt ist. William Oughtred führte die Notation in seinem Werk "Clavis Mathematicae" von 1631 ein.

Die Schreibweise formula_50 heißt auch "Bruchdarstellung" oder kurz "Bruch." Die Bruchschreibweise ist nur bei kommutativer Multiplikation eindeutig; das spielt in allgemeineren mathematischen Strukturen eine Rolle, wie sie unten unter „Verallgemeinerung“ erwähnt werden.
In der abstrakten Algebra definiert man algebraische Strukturen, die Körper genannt werden. Körper zeichnen sich dadurch aus, dass in ihnen die Division (außer durch 0) stets möglich ist. Die Division erfolgt hier durch Multiplikation mit dem inversen Element des Divisors.

In allgemeineren Strukturen (mit nichtkommutativer Multiplikation) muss man zwischen Linksdivision und Rechtsdivision unterscheiden. Auch hat die (Nicht-)Gültigkeit des Assoziativgesetzes Einfluss auf die Eigenschaften von Quotienten.

In Österreich wird gelegentlich zwischen "Messen" (wie oft geht es in …?) und "Teilen" (wie viel ergibt es geteilt durch …?) unterschieden.



</doc>
<doc id="14493" url="https://de.wikipedia.org/wiki?curid=14493" title="Subtraktion">
Subtraktion

Die Subtraktion (von lat. "subtrahere" „wegziehen“, „entfernen“), umgangssprachlich auch Minus-Rechnen genannt, ist eine der vier Grundrechenarten der Arithmetik. Unter der Subtraktion versteht man das Abziehen einer Zahl von einer anderen. Mathematisch handelt es sich bei der Subtraktion um eine zweistellige Verknüpfung. Die Subtraktion ist die Umkehroperation der Addition. Das Rechenzeichen für die Subtraktion ist das Minuszeichen „−“.

Für die Elemente einer Subtraktion gibt es folgende Symbole und Sprechweisen:


Merkhilfen (mit Berücksichtigung des Vorzeichens!):

Beispiele (mit Berücksichtigung des Vorzeichens!):

Die Menge der natürlichen Zahlen ist bezüglich der Subtraktion nicht abgeschlossen, das heißt mit der Subtraktion erzielt man eventuell ein Ergebnis, das den Bereich der natürlichen Zahlen überschreitet.

Eine Notation für formula_4 ist formula_5, was vor allem im Sinne von formula_6 platzsparende Anwendung findet sofern der Ausdruck für formula_7 ein langer ist.

Bei mehreren hintereinander auftretenden Subtraktionen wird der Ausdruck von links nach rechts abgearbeitet; die Subtraktion ist daher linksassoziativ:

Die Subtraktion ist die Umkehroperation der Addition.
In Gruppen lässt sich zu jedem gegebenen formula_9 und formula_10 genau ein formula_11 finden, so dass gilt:

Die Bestimmung von formula_11 heißt "Subtraktion". formula_11 lässt sich bestimmen, indem man formula_10 von formula_9 "subtrahiert" („abzieht“):

formula_9 heißt der "Minuend", formula_10 der "Subtrahend". Das Ergebnis einer Subtraktion, hier formula_11, heißt "Wert der Differenz". Eine Subtraktion wird mit dem Minuszeichen notiert:

Die Subtraktion formula_21 kann auch als Addition der Gegenzahl formula_23 des Subtrahenden zum Minuenden formula_9 definiert werden:

Bei der graphischen Methode werden die Zahlenwerte als Balken, Linien, Punkte oder andere abstrakte Objekte dargestellt. Eine weitere Möglichkeit ist die Darstellung mit Vektoren, wobei die Richtung des Subtrahend-Vektors umgekehrt und die Vektoren anschließend aufaddiert werden.


Bei der "Subtraktion-Subtraktion-Methode" wird so lange ein Teilbetrag des Subtrahends von Subtrahend und Minuend abgezogen, bis der Subtrahend 0 ist. Dabei wird meist eine Zehnerstelle als Zwischenschritt gewählt. 


Bei der "Subtraktion-Addition-Methode" werden Subtrahend und Minuend in Teilkomponenten zerlegt, von diesen subtrahiert, und anschließend die Teilbeträge wieder addiert. 


Bei der "Komplement-Methode" wird von dem Subtrahend das zugehörige Komplement berechnet. Anschließend werden der Minuend und das Komplement des Subtrahenden addiert. Das Verfahren wird insbesondere in der technischen Informatik, etwa beim mechanischen Feld-Tarrant-Comptometer, dem mechanischen Hoffritz-Addierer, sowie elektronischen Addierwerken in modernen Computersystemen, angewendet.


Ausgangsformel: 

Dies entspricht: 

Berechnung des Komplements: 
Addition:

Die schriftliche Subtraktion ist neben der schriftlichen Addition eine der grundlegenden Kulturtechniken, die bereits in den ersten Schuljahren der Grundschule erlernt wird. Die Beherrschung der schriftlichen Subtraktion ist Voraussetzung für das Erlernen der schriftlichen Division.

In den Grundschulen werden heute meist Verfahren gelehrt, bei denen die einander entsprechenden Stellen der Minuenden und Subtrahenden "übereinander" stehen. Die Stellen werden nacheinander abgearbeitet, meist von rechts nach links. 

Für das schriftliche Subtrahieren muss der Minuend (Zahl oben) größer oder gleich dem Subtrahenden (Zahl(en) unten) sein. Negative Ergebnisse sind somit direkt nicht möglich.

Wenn der Minuend doch kleiner ist als der Subtrahend, dann können die Vorzeichen zum Rechnen vertauscht werden. 
Der Subtrahend wird so zum Minuend (oben geschrieben) und der Minuend zum Subtrahend (unten geschrieben). Es kann dann mit den unten beschriebenen Verfahren gerechnet werden. Das Ergebnis muss aber zum Schluss mit einem Minus versehen werden, denn es ist immer negativ (keine natürliche Zahl). Damit wird der zuvor zum Berechnen durchgeführte Vorzeichenwechsel wieder rückgängig gemacht.

Wenn die einzelnen Stellen der Subtrahenden größer sind als die gleichen Stellen der Minuenden, müssen Überträge gehandhabt werden. Das heißt, der Minuend wird, um die Subtraktion zu ermöglichen, um 10 erhöht; um dies auszugleichen, muss in der links benachbarten Spalte entweder der Minuend erniedrigt (Entbündelungsverfahren; Vorabberechnung der Überträge) oder der Subtrahend erhöht werden (Ergänzungsverfahren; Subtraktion von rechts nach links). Im deutschsprachigen Raum hat sich mit dem Ergänzungsverfahren die letztgenannte Vorgehensweise durchgesetzt. Im Jahr 2000 trat in einigen Bundesländern ein neuer Lehrplan in Kraft, der nun statt des Ergänzens das Entbündeln als Standard vorschreibt.

Beim Ergänzungsverfahren, das auch Auffülltechnik oder (in den USA) "Austrian method" („Österreichische Methode“) genannt wird, wird keine Subtraktion vorgenommen, sondern der Subtrahend umgekehrt bis zum Minuenden "erhöht". Falls dies nicht möglich ist, wird der Minuend um 10 erhöht. Die 10 wird nicht „geborgt“, sondern als 1 zum Subtrahenden der nächsten Teilberechnung addiert. Im deutschsprachigen Raum wird dieses Verfahren an den Grundschulen als Standardmethode gelehrt. Einer der Vorteile des Verfahrens besteht darin, dass es den Umgang mit Aufgaben vorbereitet, bei denen von einem Minuenden mehrere Subtrahenden abgezogen werden sollen.

Die Subtraktion kann auch von links nach rechts durchgeführt werden. Bei diesem ungewöhnlichen Verfahren, das eine Variante des Ergänzungsverfahrens ist, werden die Überträge abgearbeitet, bevor die Differenz genau ausgerechnet wird. Da die Überträge weder notiert noch gemerkt werden müssen, ist die Methode nicht nur vergleichsweise resistent gegen Flüchtigkeitsfehler, sondern auch sehr schnell und sogar fürs Kopfrechnen geeignet.

Findet sich eine Spalte oder eine Sequenz von mehreren Spalten, in denen zwei gleiche Ziffern stehen, und rechts daneben eine Spalte mit einem Minuend, der kleiner als der Subtrahend ist, so muss die bei diesem Verfahren routinemäßige „Vorausschau“ nicht nur die zwei gleichen Ziffern, sondern auch die darauf folgenden Spalten umfassen. Jede Spalte mit den gleichen Ziffern erhält dann eine Neun statt einer Null als Ergebnis. 

Die Vorausschau über mehreren Spalten in den oben geschilderten Fällen ist eine Schwachstelle dieser Methode.

Abziehen mit „Entbündeln“ bedeutet, dass der zu kleine Minuend bei seinem linken Nachbarn eine „Anleihe“ macht. Der Minuend wird um 10 erhöht und der linke Nachbar um 1 erniedrigt. Das Verfahren wird an den Grundschulen z. B. der Vereinigten Staaten als Standardmethode gelehrt. Der reine Rechenaufwand ist ähnlich wie beim Ergänzungsverfahren; wenn von einer Null „geliehen“ werden muss, muss diese jedoch bei ihrem eigenen linken Nachbarn eine „Anleihe“ machen – eine Technik, die zusätzlich erlernt werden muss (beim Ergänzungsverfahren wird sie nicht gebraucht). Außerdem muss beim Entbündeln mehr geschrieben werden.

Eine Variante des Entbündelungsverfahrens besteht darin, dass alle Stellen in einem ersten Arbeitsgang vollständig entbündelt werden, sodass für den zweiten Arbeitsgang, bei dem nur noch subtrahiert wird, hinreichend große Minuenden zur Verfügung stehen.

Die "Partial Differences"-Methode unterscheidet sich von anderen vertikalen Subtraktionsmethoden dadurch, dass keine Überträge verwendet werden. An deren Stelle treten Teildifferenzen, die – je nachdem, ob in einer Spalte der Minuend oder der Subtrahend größer ist – ein Plus- oder ein Minuszeichen erhalten. Die Summe der Teildifferenzen ergibt die Gesamtdifferenz.

Die Berechnung einer Differenz muss nicht Stelle für Stelle erfolgen. Meist umständlich, aber möglich ist es auch, den zwischen einem Subtrahenden und einem Minuenden liegenden Zahlenraum auszuschreiten.


1234 − 567 = kann über folgende Schritte errechnet werden:
Um die Differenz zu ermitteln, werden die Werte der Einzelschritte addiert: 3 + 30 + 400 + 234 = 667.

Eine weitere Vorgehensweise, die sich gleichermaßen für die schriftliche Subtraktion wie für das Kopfrechnen eignet, ist die Zergliederung des Subtrahenden, der in Einzelschritten vom Minuenden abgezogen wird.


„1234 − 567 =“ kann über folgende Schritte errechnet werden:

Grundlage der "Same change"-Subtraktion ist die Beobachtung, dass eine Subtraktion einfach durchzuführen ist, wenn am Ende des Subtrahenden eine oder mehrere Nullen stehen. Der Subtrahend wird bei diesem Verfahren darum auf den nächstliegenden Zehner erhöht oder erniedrigt; da der Minuend um dieselbe Differenz erhöht oder erniedrigt wird, nimmt die Manipulation auf die Differenz keinen Einfluss. Wenn die Aufgabe danach immer noch zu schwer ist, kann die Operation wiederholt werden.


„1234 − 567 =“ kann über folgende Schritte errechnet werden:



</doc>
<doc id="14496" url="https://de.wikipedia.org/wiki?curid=14496" title="Gebundene Rotation">
Gebundene Rotation

Die gebundene Rotation (Drehung) ist ein Begriff aus der Astronomie und beschreibt ein Phänomen zwischen zwei einander eng umkreisenden Himmelskörpern: Die Eigendrehung des einen (i. d. R. masseärmeren) Himmelskörpers ist hier nicht "unabhängig" von der Umlaufperiode um den anderen Himmelskörper, sondern mit ihr "gekoppelt."

Die gebundene Rotation findet sich zwischen Monden, Planeten und Sternen. Beim Erdmond war Gezeitenreibung die Ursache, in manchen Doppelsternsystemen bremsen starke Magnetfelder die Rotation, siehe AM-Herculis-Stern.

Bei der gebundenen Rotation ist die Rotationsperiode des Planeten bzw. Mondes gleich seiner Umlaufzeit um den Zentralkörper, die Rotationsachse steht etwa senkrecht auf der Bahnebene und der Drehsinn ist gleich. Das heißt, während eines Umlaufs wendet er dem Zentralkörper stets dieselbe Seite zu (vgl. erste Abb.).

In den meisten Fällen ist der Zentralkörper deutlich schwerer als sein Begleiter. Dann wirken seine hohen Gezeitenbeschleunigungen auf ein vergleichsweise geringes Massenträgheitsmoment dämpfend ein. Sind die Massen nicht sehr verschieden, so ist der Zustand der gebundenen Rotation meist noch nicht erreicht. Jedoch kann sich in diesem Fall der gebundene Zustand am Ende bei beiden Körpern einstellen, sodass beide Körper sich vom jeweils anderen aus gesehen nicht drehen. Hier hat man eine "doppelt gebundene Rotation." Der wohl bisher einzige bekannte Fall einer doppelt gebundenen Rotation durch Gezeitenreibung ist das sehr enge System Pluto-Charon, die sich beide jeweils dieselbe Seite zeigen. In diesem Fall ist auch die Exzentrizität der Bahnen gering.

Im Allgemeinen ist die Bahn kein genauer Kreis, also die Winkelgeschwindigkeit der Bahnbewegung nicht konstant und nur im Mittel gleich der Rotationsgeschwindigkeit. Dadurch sind z. B. im Laufe eines Monats auch schmale Randgebiete der Mondrückseite zu sehen, siehe Libration.

Wenn die Umlaufbahn deutlicher exzentrisch ist, kann auch eine gebrochen-ganzzahlige Spin-Orbit-Resonanz stabil sein, also eine weitere Abbremsung der Rotation unterbleiben. Das Verhältnis der Umlaufperiode zur Rotationsperiode kann dann durch zwei kleine natürliche Zahlen ausgedrückt werden. Ein Beispiel hierfür ist die 3:2-Resonanz des Merkurs:

Das heißt, pro Umlauf (in 88 irdischen Tagen) vollführt Merkur drei halbe Umdrehungen. Merkur ist etwas länglich und seine Längsachse ist im Perihel jeweils radial ausgerichtet. Diese energetisch günstige Orientierung (siehe Stabilisierung) behält er in einem gewissen Bereich um das Perihel bei, da dort die Winkelgeschwindigkeit der Bahnbewegung sogar etwas größer ist als die der Rotation. Zur Stabilität der Resonanz trägt bei, dass die Gezeitenbeschleunigung (reziprok proportional zur dritten Potenz des Sonnenabstands) im Perihel etwa fünfmal so groß ist wie im Aphel.

Bisher ging man davon aus, dass bei Sternen, die kleiner sind als die Sonne, Planeten in einem potentiell lebensfreundlichen Abstand eine zu der Sonne gebundene Rotation aufweisen. Da hierbei auf der sonnenabgewandten Seite vorhandenes Wasser und eventuell die komplette Atmosphäre ausfrieren würde, sinkt wiederum die Wahrscheinlichkeit einer lebensfreundlichen Umgebung. Dem kann jedoch der Effekt der thermischen Gezeiten entgegenwirken, bei dem sich aufgrund der Trägheit bei der Aufheizung ein hinterherhinkender, thermischer Gezeitenberg der Atmosphäre ausbildet. Dieser liegt nicht in direkter Linie zum Zentralgestirn. Dadurch wirkt die hinterlaufende atmosphärische Masse durch die Gravitationskraft des Sterns als Impulsgeber. Bei der Venus verhindert dieser Effekt die gebundene Rotation zur Sonne. Neuere Annahmen gehen aber davon aus, dass selbst dünne, erdähnliche Atmosphären eine gebundene Rotation verhindern können, was die Wahrscheinlichkeit der Existenz von extraterrestrischem Leben erhöhen würde.

Bindung an die Erde

Bindung an den Mars

Bindung an Jupiter

Bindung an den Saturn

Bindung an den Uranus

Bindung an den Neptun

Bindung an Pluto




</doc>
<doc id="14497" url="https://de.wikipedia.org/wiki?curid=14497" title="Libration">
Libration

In der Astronomie bezeichnet Libration eine echte oder scheinbare Taumelbewegung eines Mondes, gesehen von seinem Zentralkörper.

Fast alle größeren Monde des Sonnensystems befinden sich in einer gebundenen Rotation um ihren Zentralplaneten, das heißt, sie drehen sich während eines Umlaufs um den Planeten auch einmal um die eigene Achse. Deshalb wenden diese Monde ihrem Planeten im Prinzip immer dieselbe Seite zu. Da die Monde allerdings nicht auf exakten Kreisbahnen mit konstanter Winkelgeschwindigkeit ihre Planeten umkreisen, während die Eigenrotation eine konstante Winkelgeschwindigkeit aufweist, und da sich ein Beobachter auf dem Planeten nicht exakt auf der Verbindungslinie der Massenzentren befinden muss, sieht der Beobachter im Laufe eines „Monats“ nicht immer "exakt" dieselbe Seite des Mondes. Durch die verschiedenen Effekte, die zu dieser Taumelbewegung führen, sind von der Erdoberfläche aus im Laufe der Zeit insgesamt 59 Prozent der Mondoberfläche zu sehen.

Man unterscheidet folgende Arten der Libration, hier am Beispiel des Erdmondes:


Die optische Libration lässt sich in guter Näherung aus den himmelsmechanischen Eigenschaften des Erde-Sonne-Mond-Systems berechnen. Vernachlässigt man in erster Näherung den Einfluss der Sonne, so erhält man aus der Lösung des Zweikörperproblems Erde-Mond folgende Werte:
Die großen Störungen des Erde-Mond-Systems vornehmlich durch die Sonne bewirken zusätzliche Abweichungen, deren wichtigste die folgenden sind:



</doc>
<doc id="14498" url="https://de.wikipedia.org/wiki?curid=14498" title="Wertvorstellung">
Wertvorstellung

Wertvorstellungen oder kurz Werte bezeichnen im allgemeinen Sprachgebrauch als erstrebenswert oder moralisch gut betrachtete Eigenschaften bzw. Qualitäten, die Objekten, Ideen, praktischen bzw. sittlichen Idealen, Sachverhalten, Handlungsmustern, Charaktereigenschaften beigemessen werden. Mit "Wertentscheidung" ist eine auf Werten gegründete Entscheidung gemeint. Das aus den Wertvorstellungen bzw. Werten einer Gesellschaft geformte Gesamtgebilde wird als "Wertesystem" oder "Wertordnung" bezeichnet. Das Geflecht miteinander verknüpfter, aber unterschiedlich gewichteter Werte nennt man "Werte-Hierarchie". Enthält eine Werteordnung einen alleinigen Anspruch auf Wahrheit, ist sie das Kennzeichen einer Ideologie. "Wertschöpfung" kann im materiellen und ideellen Sinne verstanden werden.

Der Begriff erfährt in der Volkswirtschaftslehre, Betriebswirtschaftslehre und Finanzwirtschaft weithin eine andere inhaltliche Bedeutungszuweisung als in den Geisteswissenschaften, speziell der Ethik, der Theologie, Soziologie oder Pädagogik.

Ist es das Ziel ökonomischen Handelns, eine höchstmögliche materielle betriebliche Wertschöpfung (Gewinn) zu erzielen, so geht es beim ethischen Handeln um das Schaffen ideeller Werte. Beide Zielsetzungen treten in der Praxis häufig in Widerspruch und erschweren eine Orientierung und Prioritätensetzung.

Die Bedeutung des Wertbegriffs verändert sich, je nachdem ob die Wertzuschreibung von Einzelnen, von sozialen Akteuren oder von einer Gesellschaft erfolgt und ob sie als objektive Erkenntnis oder subjektive Haltung verstanden werden. Mitunter gelten Wertentscheidungen als konstitutive Elemente der Kultur, insofern sie Sinnzuschreibungen innerhalb eines Sozialsystems (Gruppe, Gesellschaft usw.) festlegen. Umgekehrt ist die Kultur ein Medium, in dem Wertvorstellungen weitergegeben und verändert werden können, entweder durch direkte Vermittlung von Wertentscheidungen oder durch diese vermittelnde Gewohnheiten, Bräuche etc.

Grundlegende Werte eines Menschen oder einer Gesellschaft bezeichnet man auch als "Grundwerte".

Beim Versuch, einen gemeinsamen Wertekatalog zu definieren, stellen sich Fragen wie die, ob ein gemeinsamer Wertekatalog über Vorstellungen vom „Guten“ (etwa Solidarität) hinaus auch Verfahrensregeln (etwa die Rechtsstaatlichkeit) einbeziehen solle, und inwieweit auch Postulate dazugehören können, welche in der Realität bisher nicht umgesetzt werden.

Individuelle Werte und Einstellungen untersucht die Differentielle Psychologie. Das Teilen, Weitergeben oder Diskutieren von Werten in Gruppen behandeln die Sozialwissenschaften und die Sozialpsychologie. Andere Wissenschaften, wie etwa die Moraltheologie und die Pädagogik, müssen sich mit Fragen des Wertbestands und der Weitergabe von Werten direkt befassen. Diese sind darüber hinaus Gegenstand gesellschaftlicher und politischer Diskussion.

Im fachsprachlichen Gebrauch der deutschsprachigen Philosophie können „Werte“ zum Beispiel Teilaspekte des Guten ausmachen. Darüber hinaus existiert ein breites Spektrum philosophischer Wertbegriffe sowie moralphilosophischer und metaethischer Rahmentheorien – ein Themengebiet, das auch als Axiologie bezeichnet wird.

In der Wertphilosophie, speziell ihrem Teilbereich Ethik, beinhalten die Begriffe „Wertvorstellung“, „Werthaltung“ oder „Wertschöpfung“ nach ihren bedeutenden Vertretern Oskar Kraus, Hermann Lotze oder Max Scheler die Fundierung und Ausrichtung des Denkens und Handelns nach ideellen Werten. Unter ideellen Werten versteht man nach Siegbert A. Warwitz Werte, die nicht primär der materiellen Gewinnvermehrung dienen, sondern sich nach sozialen Maßstäben ausrichten bzw. eine Steigerung der geistigen Lebensqualität, eine innere Bereicherung, eine Reifung der Persönlichkeit bedeuten. Dies setzt ein Verständnis für immaterielle Werte und die Unterscheidungsfähigkeit von Nutzdenken und Sinnstreben voraus. Als bedeutendste Motivationsquellen sieht er „eine metaphysische, auch religiöse Orientierung, ein humanistisches Denken oder eine soziale Ausrichtung“.

Erich Fromm differenziert in seiner Gesellschaftskritik grundsätzlich zwischen „idealistischen“ und „materialistischen“ Wertanschauungen. Dabei geht es ihm um die Alternative einer Bereicherung durch äußere Güter oder menschliche Qualitäten. Von 
Hermann Lotze wird der Terminus „Wert“ im Sinne eines „von den Menschen gefühlsmäßig als übergeordnet Anerkannte[n], zu dem man sich anschauend, anerkennend, verehrend, strebend, verhalten kann“ gebraucht.

Vertreter der Wertphilosophie sind der Ansicht, dass die Wertfrage bereits seit den Anfängen des philosophischen Denkens der Frage nach dem Charakter und der Seinsweise der Werte gestellt worden sei, so vor allem in der Güterethik des Aristoteles. Platon beschrieb in seinem Werk die Idee des Guten. Die antike Güterethik aristotelischen Ursprungs wurde auch in der Theologie aufgegriffen und im Rahmen der Moraltheologie weitergeführt.

Windelband, Rickert und andere entwickelten eine Wertethik mit der Intention, die philosophische Ethik stärker anthropologisch als ontologisch zu fundieren. Maßgebliche Bedeutung erhält der Begriff im Ansatz der materialen Wertethik von Max Scheler in den Jahren 1913 bis 1916. Scheler hat seine Wertethik ausdrücklich von der traditionellen Güterethik abgegrenzt.

Bochenski (1902–1995) unterschied 1959 drei Gruppen immaterieller Werte, die man durch sein Verhalten verwirklichen kann: die moralischen, die ästhetischen und die religiösen. 

In der jüngeren Diskussion sind die Versuche, Werte ontologisch oder anthropologisch zu begründen, stark in die Kritik geraten. So argumentiert der Freiburger Philosoph Andreas Urs Sommer 2016 in einem stark beachteten Buch, Werte seien "regulative Fiktionen", die je nach den individuellen und sozialen Bedürfnissen immer wieder umgestaltet würden. Die Vorstellungen ewiger, für sich bestehender Werte weist Sommer zurück, ohne jedoch einen Werteverfall zu diagnostizieren. Werte seien notwendig plural und relativ - und dass sie es seien, sei begrüßenswert.

Der Wertbegriff wurde in der Psychologie „großzügig“ gehandhabt und „vielfach nur im Sinne der Umgangssprache“ verwendet. Es war auch üblich, den in philosophischer Sichtweise eingesetzten Begriff aufgrund der Ergebnisse psychologischer Forschung zu erklären und zu variieren. 1924 wurde der Begriff in dem jahrzehntelang neu aufgelegten, jugendpsychologischen Werk Eduard Sprangers in Formulierungen wie „Wertganzes“, „Wertverwirklichung“ und „Wertgehalt der Welt“ verwendet.

Der Begriff erhielt allerdings seit den 1960er Jahren aufgrund vielfacher Untersuchungen (zum Beispiel Kurt Lewin, Clark L. Hull, Edward C. Tolman, Desmond Morris) eine definitorische Zweideutigkeit, „nach zwei Richtungen hin“ (Rolf Oerter): 1. Werte als den Dingen oder Lebewesen eigene Bezugspunkte wirken anziehend oder abstoßend. 2. Ein mit der Kultur vermittelter Wert dient als „Richtlinie“ dem Menschen zum Verständnis bzw. zur Erkenntnis der Welt und wird infolgedessen bei der Planung des Verhaltens zur Prämisse.

Als hypothetisches Konstrukt einer Individuum-Welt-Beziehung wird der Wert entweder als Komplex von Wirkungsfaktoren der Welt auf das Lebewesen wahrgenommen oder im motivationalen Konzept des Individuums als Zielentwurf oder Korrektiv zur Gestaltung der Welt verwendet. Überwiegend war jedoch der Wertbegriff als dynamisches Konzept in der Literatur zu finden. In diesem auf eine breitere Basis psychologischer Untersuchungen gestellten „Wertkonzept“ wurden die handlungsorientierten Bedeutungen der im deutschsprachigen Raum beschriebenen Begriffe „Werterleben“ und „Wertverwirklichung“ wiedergefunden. Als ein Ergebnis seiner Forschung über die Kognitionsentwicklung erklärte Jean Piaget 1966, dass das im Kindheitsstadium erworbene formale Denken eine später auch affektiv begleitende Voraussetzung sei, um zur Planung von Lebensentwürfen im Erwachsenenalter die „mit Zukunftsprojekten verbundenen Werte“ passend strukturieren zu können. Aus der Sicht der Existenzanalyse gab Frankl 1974 den Werten die Geltung als „umfassende Sinnmöglichkeiten“

Innerhalb der Motivationstheorie beschrieb Haseloff 1974 die Werteinstellungen als langfristig effiziente Wirkungskomplexe aus der Motivklasse der Strebungen, „die sozio-kulturell thematisierte und normierte Dauerquellen“ darstellen, direkten Bezug auf die „Wertsysteme und die Präferenzordnung der Persönlichkeit“ nehmen und sich „meist […] gemäß dem Gesetz von der funktionellen Autonomie der Motive“ (G. Allport) verfestigen. Aus einer Synopse von psychologischer mit soziologischer Literatur resultierte bei Hans Joas 2004 die Beschreibung einer inner-individuellen Dynamik in dem Begriff „Wertbindungen“, die der Mensch in einem aktiven Vorgang, „in den Prozessen der Selbstbildung und […] in Erfahrungen der Selbsttranszendenz“ entwickelt.

Aus Werten (z. B. dem Wert der Achtung des Eigentums) lassen sich soziale Normen (konkrete Vorschriften für das soziale Handeln) ableiten – z. B. „Wer eine fremde bewegliche Sache, in der Absicht, sie sich anzueignen, wegnimmt …“. Allerdings gehen historisch konkrete Gebote wie „Du sollst nicht stehlen!“ oft ihren Wert-Abstraktionen voraus. Werte sind ein zentraler Bestandteil vieler Verhaltensvorschriften, jedoch sind sie nicht selber Verhaltensvorschriften. Werte sind attraktiv, während Normen restriktiven Charakter haben.

„Die Norm sagt, was in einer Situation notwendig und allgemeingültig geschehen soll.“ Eine bestimmte Art der Verknüpfung von Handlungsbedingungen in einer Situation mündet in den Anspruch einer Forderung zum Tun. Wie verhält sich die soziale Norm bezogen auf die geistigen Dispositionen des Wollens? Zu den Normen gehört die Idealität. Ihnen liegen Entwürfe zugrunde, die als ideale Möglichkeiten im Geist beim Aufbau eines Lebenskonzeptes vorbereitet werden. Bezugspunkt dieser Normen ist „eindeutig der Wert als Kategorie der Selektion“. Die Befolgung der Normen „wird durch die negativen Konsequenzen ihrer Nichtbefolgung“ lanciert. „Die Normen des sozialen Umgangs verleihen den Verhaltensweisen Ordnung. Sie fungieren als Gruppenstabilisatoren.“ Mit gesellschaftspolitischem Blick bezieht sich Habermas 2004 wie selbstverständlich auf die Orientierung des Bürgers am Normativen; er verwendet für diese ethische Disposition den Begriff „Normbewusstsein“.

Werte werden in der Regel über die Sozialisation an nachfolgende Generationen weitergegeben. Dies geschieht nicht vollständig. So lässt sich beispielsweise in den westlichen Industriegesellschaften ein stetiger Wertewandel beobachten. Die Ursachen für den Wertewandel sind vielfältig (veränderte Umweltbedingungen, Konflikthaltung gegenüber anderen Generationen usw.). Werte unterscheiden sich von Einstellungen darin, dass sie stabiler sind.

Das System aller Werte ist anscheinend nicht widerspruchsfrei bzw. einzelne Werte scheinen mit bestimmten anderen Werten in einem Konkurrenzverhältnis zu stehen. So wird gelegentlich postuliert, dass der Wert des Wohlstands im Konflikt mit dem Wert der Nachhaltigkeit oder der Wert der individuellen Freiheit mit anderen Werten (etwa der Gleichheit) steht.

Eine differenziertere Betrachtung ergibt allerdings auch hier ein differenzierteres Bild. So werden bei solchen Debatten oft verschiedene Zeit- und Abstraktionsebenen vermischt. Im obigen Beispiel etwa steht der Wert des Wohlstands nur kurzfristig im Konflikt mit dem Wert der Nachhaltigkeit; langfristig kann ohne Nachhaltigkeit kein Wohlstand generiert werden. Auch die Freiheit steht im Grunde nicht im Gegensatz zu anderen Werten, sondern mit anderen Freiheiten (bzw. der Freiheit anderer).

Andererseits können Werte, die abstrakt gesehen durchaus vereinbar scheinen, in konkreten Situationen miteinander in Konflikt treten. Es ist dann nicht möglich, sich so zu verhalten, dass man allen Werten gleichzeitig gerecht wird. In diesem Zusammenhang wird auch von einer "Werte-Hierarchie" gesprochen. Nicht alle Werte werden als gleichrangig angesehen, sodass auch in solchen Fällen meist eine mehr oder weniger klare Orientierung gegeben ist. Die jeweilige Gewichtung eines Wertes ist im Einzelfall situations- und/oder kulturabhängig. Auch hier ist zu prüfen, ob es sich tatsächlich um eine Kollision von (abstrakt-generellen) Werten an sich handelt – oder nicht doch um einen (konkret-individuellen) normativen Zielkonflikt („Pflichtenkollision“). Dieser Konflikt wurde einschlägig von Max Weber durch die Unterscheidung zwischen Verantwortungs- und Gesinnungsethik zum Ausdruck gebracht.

Politische, geschäftliche, zwischenmenschliche oder auch innerpersonale Konflikte lassen sich häufig auf eine Kollision zwischen unterschiedlichen Werten bzw. Glaubenssätzen zurückführen. Im Gordon-Modell, einem Kommunikations-Modell zur Lösung von Konflikten, wird zwischen Wertekonflikten und Bedürfniskonflikten unterschieden.

Problematisch ist auch, wie man die allgemein anerkannten Werte durchsetzt. Aus egoistischer Sicht ist es manchmal vorteilhafter, sich nicht an soziale Normen zu halten, insbesondere dann, wenn man eine gute Chance hat, nicht erwischt zu werden. Deswegen braucht eine Gesellschaft ein (möglichst gut funktionierendes) Sanktionssystem, damit aus Werten abgeleitete Normen möglichst gut von allen eingehalten werden. Ist dieser Druck zu groß, beschneidet man allerdings wieder die individuelle Freiheit des Einzelnen.
In den 1980er Jahren hatte der Psychologe Shalom H. Schwartz zusammen mit Wolfgang Bilsky die Frage aufgeworfen, ob es universelle Werte gibt. Er entwarf ein "Wertemodell" und postulierte eine Anzahl von Werten, die alle Menschen in unterschiedlichen Ausprägungen gemeinsam haben müssten. Sein Forschungsschwerpunkt lag dabei allerdings auf der Wertestruktur und deren motivationale Beziehung zueinander.

Das InterAction Council, eine Expertengruppe aus Politikern, Sozialwissenschaftlern und Vertretern weltweiter Religionsgemeinschaften erarbeitete eine möglichst umfangreiche Minimalsynthese, ausgehend von politischen Prämissen und einer Bestandsaufnahme weltanschaulicher und religiöser Ideale. 1997 wurden ethische Optionen für den Alltag als „Allgemeine Erklärung der Menschenpflichten“ vorgelegt.

Weitere Ansätze sind das Projekt Weltethos von Hans Küng, die internationale Erd-Charta, die Diskursethik oder das Projekt "Ethify Yourself".

Allerdings werden global-ethische Perspektiven nicht ohne Kritik akzeptiert. 2004 formulierte J.-C. Kapumba Akenda als Dilemma des ethischen Universalismus: Einerseits ist der weltweite Anspruch der Vernunft und der Gerechtigkeit und andererseits die Souveränität lokaler Gemeinschaften zu achten "(siehe hierzu auch die unterschiedlichen Überzeugungen der „kalten und heißen Kulturen“.)" Als „Bausteine des ethischen Universalismus“ schlug Akenda diesbezüglich die „Solidarität ohne Paternalismus“ und die „Kommunikation ohne Konsenszwang“ vor.

Im Wirtschaftsleben findet der Wertebegriff vorrangig in materieller Bedeutung Verwendung: So versteht etwa die Geldwirtschaft „Wertschöpfung“ als das wesentliche Ziel produktiver Tätigkeit. Dabei geht es um die Umwandlung vorhandener Güter in Güter mit höherem Geldwert. Produzierende Unternehmen rechnen mit einem Produktionskonto, mit dem die durch die Produktionstätigkeit entstandenen Einnahmen und Ausgaben dargestellt werden. Die „Bruttowertschöpfung“ gilt als Messgröße für die wirtschaftliche Leistung eines Betriebes.

Das Thema Werte hat jedoch im Zusammenhang mit der Banken- und Managerkrise in den letzten Jahren auch in der ökonomischen Diskussion eine zunehmende (und neue) Beachtung gefunden. Es ist im Sinne von Erich Fromm eine neuerliche Ethikdiskussion über das Verhältnis von materiellen und immateriellen Werten in einer wissensbasierten Ökonomie und deren Bewertung aufgebrochen. Relevante Stichworte dazu sind Nachhaltigkeit, soziale Verantwortung ("Corporate Social Responsibility"), Wertemanagement, werteorientierte Personalführung, wertebalancierte Unternehmensführung und ethische Entwicklung. Angesichts der Skandale ist zunehmend in den Blickpunkt der Öffentlichkeit gerückt, dass die materielle Wertorientierung von der ethischen nicht abgekoppelt werden darf, wenn die Gesellschaft eine humane Ausrichtung erhalten soll.










</doc>
<doc id="14507" url="https://de.wikipedia.org/wiki?curid=14507" title="Tourist (Begriffsklärung)">
Tourist (Begriffsklärung)

Tourist steht für

Mehrere Filme und Romane tragen Titel Tourist oder The Tourist:
Tourist bezeichnet außerdem:
Siehe auch:


</doc>
<doc id="14508" url="https://de.wikipedia.org/wiki?curid=14508" title="Riesling">
Riesling

Riesling ist eine weiße Reb- und Weinsorte, die zu den hochwertigsten und kulturprägenden Gewächsen gezählt wird. Die besten Rieslinge werden in klimatisch kühleren Weinbaugebieten erzeugt. Vor allem wird die Sorte in Deutschland, aber auch in zahlreichen anderen Weinbauländern angebaut. Riesling-Weine genießen hohes Ansehen auf internationalen Märkten. Viele Spitzenlagen sowohl in Deutschland als auch in anderen Weinbauländern sind mit Riesling bestockt. Vor allem in nördlichen Anbauländern wie Deutschland wird die Sorte fast ausschließlich in steilen Hängen angebaut.

Riesling wird in Deutschland nachweislich seit mehr als 600 Jahren kultiviert.

Riesling ist eine natürliche Kreuzung aus Heunisch × "Vitis vinifera subsp. sylvestris" und Traminer-Klon und wurde vermutlich aus Wildrebenbeständen am Oberrhein ausgelesen.

Die Abstammung des Rieslings stellt Ferdinand Regner von der Höheren Bundeslehranstalt für Wein- und Obstbau in Klosterneuburg in seinen Untersuchungen folgendermaßen dar: „Unsere genetischen Analysen lassen beim Riesling drei genetische Phänomene erkennen, die zur heutigen Rebsorte geführt haben. Der vermutlich letzte Schritt war die Einkreuzung der Rebsorte Heunisch und hat dem Riesling Einiges an Beständigkeit, Vitalität und möglicherweise sein Säurepotential gebracht. Zuvor war es eine Rebe, die eine Kombination aus Traminer mit einer autochthonen Rebe vom Rhein darstellte. Die Traminereinkreuzung könnte spontan erfolgt sein und stellte wahrscheinlich eine qualitative Verbesserung dar. Den Traminer haben vermutlich die Römer an den Rhein gebracht. Die ursprüngliche Rebe bringt vor allem die Frosthärte und Kleinbeerigkeit mit. Diese Rebe war vermutlich schon von den Germanen in Besitz genommen und könnte auf Grund der Nähe des Rieslings zu den Wildreben aus diesen ausgelesen worden sein. Die örtliche Herkunft vom Rhein dürfte unbestritten sein.“

Neben dem Weißen Riesling gibt es auch einen Roten Riesling. Dieser hat gegenüber dem Weißen Riesling nur rote Beeren, die einen Weißwein ergeben. Schwarzriesling und der Blaue Riesling sind nicht verwandt mit dem Weißen Riesling. Letztere Sorte findet sich nur in Sortimenten von Rebzuchtanstalten und hat keine wirtschaftliche Bedeutung.

Vom Rheintal aus verbreitete sich der Riesling am Ende des Mittelalters in die meisten deutschen Anbaugebiete. Die erste urkundliche Erwähnung von Riesling-Reben stammt aus einer Rüsselsheimer Rechnung des Kellers Klaus Kleinfisch an seinen Herrn Graf Johann IV. von Katzenelnbogen vom 13. März 1435: Für einen neuen Weinberg wurden für 22 Schilling Setzreben einer neuen Weißweinsorte, eben des Rieslings, gekauft. Die vorletzte Zeile lautet: "Item 22 ß umb seczreben rüßlingen in die wingarten" (Urkunde Marburg Staatsarchiv). Weinanbau hatte in Rüsselsheim am Main bereits eine lange Tradition. Die früheste Erwähnung von Weingärten in der Rüsselsheimer Gemarkung ist in einer Rechnung aus dem Jahr 1401 von Zwingenberg und Auerbach enthalten – wie lange vor diesem Datum in Rüsselsheim schon Wein angebaut wurde, ist nicht bekannt.

Weitere Erwähnungen des Namens Riesling folgen in Bingen (1463), an der Mosel (1464/1465) und 1552 ein Eintrag im lateinischen "Kreuterbuch" des pfälzischen Botanikers Hieronymus Bock. Wegen seiner geringen Erträge und der späten Reife konnte er sich aber nur langsam durchsetzen.

Nach langer Blüte des Weinbaus ging im 20. Jahrhundert die Anzahl der Weinberge in Rüsselsheim mehr und mehr zurück. Die letzte Erwähnung stammt aus dem Jahr 1915. Im Verlauf des Ersten Weltkrieges scheint der Weinanbau in Rüsselsheim eingestellt worden zu sein. Im Frühjahr 1980 wurde auf einem stadteigenen Grundstück ein 800 m² großer historischer Weinberg angelegt. Der Wingert ist lebendiges Denkmal und er erinnert neben der Ersterwähnung des Rieslings in Rüsselsheim auch an die rege Weinbautätigkeit der Gemeinde in vergangener Zeit. 1985 fand im Museumskeller der nahe gelegenen historischen Festung Rüsselsheim eine 550-Jahr-Feier zur Ersterwähnung der Riesling-Rebe mit festlicher Weinprobe statt, bei der wieder Rüsselsheimer Riesling ausgeschenkt wurde.

Bei der Neubestockung der Rheingauer Weinberge von Schloss Johannisberg im 18. Jahrhundert unter der Ägide der Fuldaer Erzbischöfe wurde fast ausschließlich auf den Riesling gesetzt. Im Jahre 1787 verfügte der Trierer Kurfürst Clemens Wenzeslaus von Sachsen, dass in seinem Herrschaftsbereich nur noch Riesling zu kultivieren sei. Das hatte schließlich auch zur Folge, dass die Mosel das größte zusammenhängende Rieslinganbaugebiet der Welt wurde.

Staatliche Anbauempfehlungen im 17. Jahrhundert, das Streben nach höherer Qualität und hohe Weinpreise führten ab dem Ende des 19. Jahrhunderts zu einer starken Zunahme. Um die Wende vom 19. zum 20. Jahrhundert waren deutsche Rieslingweine ebenso hoch (bzw. teils höher) eingeschätzt wie die großen Rotweine Frankreichs und erzielten ähnliche Preise.

Im Orgelbau wird ein bestimmter nichtakustischer Registerzug auch als "Riesling-Register" bezeichnet.


mittelhoch, ca. 60–110 hl/ha

Lage
Der Riesling stellt in nördlich gelegenen Anbaugebieten hohe Anforderungen an die Lage, da er sehr spät reift. Optimale Bedingungen bieten die wärmespeichernden steinigen Steillagen in südwestlicher bis südöstlicher Ausrichtung entlang der Flusstäler, wie sie etwa am Rhein, Ahr, Mosel, Saar, Ruwer, Nahe, Lahn, Main, Elbe und in Österreich in der Wachau sowie in den Höhentälern Südtirols in Italien gegeben sind.

Boden
Skelettreiche, leichte bis mittelschwere Böden sind am besten geeignet wie zum Beispiel Rankerböden (als Urgesteinsböden bezeichnet). Schwere, nasskalte Böden sind nicht geeignet. Tiefgründige, fruchtbare Böden in Blachlagen bringen zwar vollmundige Weine, die aber nicht typische und mineralische Rieslingweine hervorbringen.



Der Wein ist rassig, lebendig, frisch-elegant, stahlig und mineralisch. Kennzeichnend ist die typische pikante, fruchtige Säure. Fruchtaromen nach Steinobst (Marille) und exotischen Früchten bestimmen den Charakter des Weines. Die Farbe des Weins kann von Blassgelb mit Grünstich bis Goldgelb reichen. Erst nach längerem Weinausbau wird die volle Reife des Weines erreicht. Jungweine sind meist noch säurebetont und unharmonisch im Geschmack. Der Riesling besitzt die Fähigkeit, den Charakter der jeweiligen Lage besonders gut zum Ausdruck zu bringen. Aufgrund des relativ hohen Säuregehalts besitzen Rieslingweine eine gute Lagerfähigkeit (5–10 Jahre). Altersgereifte Weine weisen zudem häufig eine „Petrolnote“ auf. 
Mit Riesling können auch hochwertige süße Weine erzeugt werden. Entweder als Eiswein geerntet, oder die Konzentration erfolgt über die Edelfäule, die durch die Grauschimmelfäule ("Botrytis cinerea") hervorgerufen wird.
Rieslingwein eignet sich außerdem zur Herstellung von Schaumwein.

2010 betrug die globale Anbaufläche von Riesling 49.833 ha. Deutschland hat den größten Flächenanteil.

In Deutschland wurden 2013 insgesamt etwa 102.000 ha Rebanbaufläche kultiviert. Riesling ist mit 23.293 ha Anbaufläche (2014) die am weitesten verbreitete Rebsorte. Sie nimmt über 22,4 % der Rebfläche ein und wird in allen deutschen Weinbaugebieten kultiviert. Die größten Riesling-Anteile hat der Rheingau.
Die meisten deutschen Spitzenlagen sind mit Riesling bestockt. Bekannte deutsche Riesling-Lagen sind
In Österreich wird der Riesling in mehreren Anbaugebieten auf einer Fläche von 2.015 ha (6,6 % von den Weißweinsorten, Stand 2015), kultiviert. In Abgrenzung zum populären Welschriesling wird der Riesling in Österreich auch als Rheinriesling bezeichnet. Unter den Anbaugebieten ist die Wachau das bekannteste. Die steilen Steinterrassen am Nordufer der Donau und im Spitzer Graben tragen durch ihre Fähigkeit, Wärme zu speichern, dazu bei, Spitzenweine mit ausgewogener Säure und fruchtigem Geschmack zu erzeugen.

Die bekanntesten Riesling-Lagen von Österreich sind:

In Frankreich ist der Riesling ausschließlich im Elsass zur Erzeugung von Qualitätsweinen zugelassen. Die Anbaufläche beträgt dort 3480 ha (Quelle ONIVINS). Für das Elsass charakteristisch sind trockene Weine mit verlockendem Duft und kräftigem Alkoholgehalt (häufig 12 % oder mehr). Im trockenen Elsässer Klima besteht nur sehr wenig Fäulnisgefahr, und es sind ausgedehnte Reifeperioden möglich, die dann zu Vendanges Tardives oder den noch süßeren Sélections de Grains Nobles führen können. Der Riesling ist ebenfalls Bestandteil der zugelassenen Rebsorten der Appellation Alsace Grand Cru.

In Luxemburg sind mit 159,1 ha (Stand 2008) etwa zwölf Prozent der 1300 Hektar Gesamtfläche mit Riesling bestockt. Er erbringt trockene, dank Chaptalisation recht körperreiche Weine, die dem Elsässer Stil näher sind als dem der benachbarten Mosel.

Die bestockte Rebfläche wird auf ca. 4432 Hektar geschätzt (Stand 2007).

Der erste Riesling Australiens wurde vermutlich von William Macarthur 1838 in der Nähe von Penrith in New South Wales angepflanzt. Bis 1992 blieb er die meistangebaute weiße Rebsorte des Landes. In Australien wird der Wein auch als "Rhine Riesling" bezeichnet. Bis vor wenigen Jahren wurden dort jedoch nahezu alle fruchtigen Weißweine Riesling genannt. So bezeichnete man mit "Hunter River Riesling" die Sémillon-Traube. Aufgrund des warmen Klimas leidet der Riesling in Australien unter Identitätsproblemen; bisher kann man noch nicht flächendeckend gute Qualitätsweine erhalten. Schwerpunkte des Anbaus sind das Clare Valley bei Watervale, Eden Valley und die Adelaide Hills, die allesamt in Südaustralien in der Nähe von Adelaide liegen.

Neuseeland ist ein Weinerzeugerland mit kühlem Klima. Dies trifft insbesondere auf die Südinsel zu. Insbesondere das Anbaugebiet Marlborough sowie das Gebiet um die Stadt Nelson bringen Rieslinge mit exzellenter Säure und von großer Delikatesse hervor. Da die Geschichte qualitativ hochwertiger Rieslingweine in Neuseeland noch sehr kurz ist (sie begann Ende der 1980er Jahre), verspricht die Zukunft noch vieles. Im Jahr 2008 lag die bestockte Rebfläche bei noch steigender Tendenz bei 917 Hektar. Im Jahr 2007 lag die Rebfläche noch bei 868 Hektar.

Die bestockte Fläche an Riesling beträgt 1343 ha (2000). Riesling wird sortenrein oder zu Schaumwein weiterverarbeitet.

Riesling wird mehr und mehr in den USA angebaut. Winzer im Staat New York produzieren Rieslingweine in der Fingerlake-Region (Niagarafälle und Buffalo), und an der Westküste gibt es Anbaugebiete in Kalifornien und Oregon.

Der Riesling wurde häufig für Neuzüchtungen verwendet. Bei folgenden Sorten hat er als Vater- oder Muttersorte Pate gestanden:

Als Muttersorte diente Riesling bei der Kreuzung der Rebsorten Alb de Yaloven, Arnsburger, Augustriesling, Beutelriesling, Bouquetriesling, Dalkauer, Donauriesling, Edelmuskat, Ehrenfelser, Feinriesling, Floricica, Frühriesling, Johanniter, Kocsis Zsuzsa, Manzoni Bianco, Marienriesling, Müller-Thurgau, Multaner, Muscat de la Republique, Oraniensteiner, Osiris, Osteiner, Quanyu B, Rabaner, Rieslaner, Rieslina, Riesling Magaracha, Romeo.

Als Vatersorte diente Riesling bei der Kreuzung der Rebsorten Aris, Arnsburger, Aurelius, Dona Emilia, Dr. Deckerrebe, Elbriesling, Kamchia, Kerner, Lafayette, Misket Varnenski, Negritienok, President Carnot, Rabaner, Rieslaner, Riesling Bulgarski, Ruling, Scheurebe, Thurling, Witberger.

In komplexeren Züchtungen diente sie als Kreuzungspartner von Albalonga, Bacchus, Breidecker, Goldriesling, Grando, Königsast, Merzling, Perlriesling, Primera, Quanyu B, Rotberger, Ruling.

Einige Züchtungen wie Müller-Thurgau, Kerner und Scheurebe haben größere Bedeutung erlangt.

Beregi Riesling, Beyaz Riesling, Biela Disuca Grasiva, Biela Grasevina, Bukettriesling, Dinca Grasiva Biela, Edelriesling, Edle Gewuerztraube, Feher Rajnai, Gentil Aromatique, Gentile Aromatique, Gentile Aromatique Petracine, German Riesling, Gewuerzriesling, Gewuerztraube, Graefenberger, Graschevina, Grasevina Rajnska, Grauer Riesling, Grobriesling, Hochheimer, Johanisberger, Johannisberg, Johannisberg Riesling, Johannisberger, Johannisberger Gontil Aromatique, Johannisberger Weisser, Johannisberger White, Jonanisberger Riesling, Karbacher, Karbacher Riesling, Kastellberger, Kis Rizling, Kleigelberger, Kleiner Riesling, Kleinriesler, Kleinriesling, Klingelberger, Klingenberger, Klingerberger, Krauses, Krausses Roessling, Lipka, Moselriesling, Nieberlander, Niederlaender, Oberkircher, Oberlaender, Petit Rhin, Petit Riesling, Petracine, Pfaelzer, Pfefferl, Piros Rajnai Rizling, Pussilla, Raisin Du Rhin, Rajinski Rizling, Rajnai Rizlin, Rajnai Rizling, Rajnai Rizling Gm 239-20, Rajnski Rizling, Rajnski Ruzling, Rano, Reichsriesling, Reissler, Remo, Rendu, Reno, Renski Rizling, Rey Rislinqi, Reyn Rislinqi, Reyn’s Risling, Reynai, Rezlik, Rezlin, Rezling, Rezlink, Rhein Riesling, Rheingauer, Rheingauer Rieling, Rheinriesling, Rheinriesling GM 239-20, Rheinrisling, Rhiesling, Rhine Riesling, Riesler, Riesling, Riesling Bianco, Riesling Blanc, Riesling De Rhin, Riesling Echter Weisser, Riesling Edler, Riesling Gelb Mosel E 43, Riesling Giallo, Riesling Grau, Riesling Grosso, Riesling Gruener Mosel, Riesling Mosel, Riesling Reinskii, Riesling Renano, Riesling Renano Bianco, Riesling Rhenan, Riesling Rhine, Riesling Rothstieliger, Riesling Weisser, Riesling White, Rieslinger, Rieslingtraube, Rislinenok, Risling, Risling Reinskii, Risling Rejnski, Rislinock, Rislinok, Rislinq, Rizling Linner, Rizling Rajinski, Rizling Rajnai, Rizling Rajnski, Rizling Rajnski Bijeli, Rizling Rejnskij, Rizling Rynsky, Roesling, Roessiling, Roessling, Rohac, Rossling, Rosslinger, Ruessel, Ruessiling, Ruessling, Russel, Ryn-Riesling, Rynsky Ryzlink, Ryzlink Rynsky, Starosvetske, Starovetski, Szuerke Rizling, Uva Pussila, Weiser Riesler, Weisser Kleiner Riesling, Weisser Riesling, White Riesling.




</doc>
<doc id="14511" url="https://de.wikipedia.org/wiki?curid=14511" title="Freie Software">
Freie Software

Freie Software ("freiheitsgewährende Software", oder auch "libre software") bezeichnet Software, welche die Freiheit von Computernutzern in den Mittelpunkt stellt. Freie Software wird dadurch definiert, dass ein Nutzer mit dem Empfang der Software die Nutzungsrechte mitempfängt und diese ihm nicht vorenthalten oder beschränkt werden.

Insbesondere bedeutet es,
Man darf hinsichtlich der Software wahlweise "kommerzielle" Tätigkeiten anbieten (Softwareanpassungen, Wartungsverträge, Support, Service- und Garantieleistungen usw.). Gewünschte Analyse und Änderungen (siehe Freiheit der Kontrolle) darf wegen gewährter Kollaboration von jedem – auch unabhängigen Dritten seiner eigenen Wahl – durchgeführt werden.

Durch diese Freiheitsrechte wird es dem Nutzer erlaubt, "Eigenkontrolle" und "Privatsphäre" über die Software und die eigene Datenverarbeitung zu haben oder Teil einer (öffentlichen oder eigenen) Gruppe von Nutzern "(Gemeinschaft)" zu werden, welche die Software kontrollieren (Kollaboration ist möglich) und für sich nutzen.

Dies steht im Gegensatz zu proprietärer Software (freiheitsentziehender Software), bei der die Entwickler und Distributoren der Software den Endnutzern die genannten Freiheitsrechte explizit entziehen, beispielsweise durch absichtliche Nicht-Auslieferung von Quelltext oder Verbote und Einschränkungen per Vertragsregelungen oder Geheimhaltungsvertrag.

Die 1985 von Richard Stallman gegründete Free Software Foundation (FSF) definiert Software als "Freie Software", wenn dem Empfänger per Lizenz folgende Freiheiten einräumt werden:

Für weitere Informationen dazu siehe den Abschnitt „Definition“.

Die Freie-Software-Bewegung ist aus der Hacker-Gemeinschaft hervorgegangen. Deren Freiheitsvision manifestiert sich in dem seit September 1983 existierenden GNU-Projekt, der Freien-Software und der 15 Jahre später entstandenen Open-Source-Bewegung. Ein Merkmal eines Hackers ist nicht die Aktivität selbst, sondern die Art wie sie durchgeführt wird. Jemand der beispielsweise für ein Freie-Software-Projekt entwickelt, ist nicht automatisch ein Hacker, aber die Hackergemeinschaft ist eng verknüpft mit diesen Bewegungen. Innerhalb der frühen Hackergemeinschaft der 1960er- und 1970er-Jahre war es an akademischen US-Einrichtungen wie MIT, Stanford, Berkeley und Carnegie Mellon, selbstverständlich, Quellcodes offenzulegen und eigene Softwareverbesserungen mit anderen Programmierern zu teilen. Software wurde damals als Beigabe zu der (teuren) Hardware betrachtet. Ende der 1970er und Anfang der 1980er Jahre fingen IT-Unternehmen an, Software zu kommerzialisieren und den Quelltext geheimzuhalten. Richard Stallman ist ein prominenter Hacker, der wesentliche Beiträge zum Selbstverständnis der "akademischen Hackerkultur" geleistet hat, unter anderem indem er dieser Entwicklung etwas entgegenstellte.

Bis in die 1980er Jahre gab es Freie Software als Public domain Software. Daneben wurde Software frei als gedruckter Quelltext in Computermagazinen und Büchern verbreitet. Die Hackergemeinschaft und das intellektuelle Klima rund um den „AI“-Rechner des MIT inspirierten Richard Stallman maßgeblich zur Schaffung des GNU-Projekts. Das hatte zunächst die Erstellung eines freien Betriebssystems zum Ziel. 1985 folgte die Gründung der FSF, einer Stiftung zur Förderung derartiger Projekte, die im Februar 1986 die erste Definition der Freien Software veröffentlichte. Wobei „frei“ die Freiheiten für die Gesellschaft meint, die ein derart lizenziertes Produkt bietet.

Für weitere Informationen dazu siehe die Abschnitte „Entwicklungen im Vorfeld“ und „Die Entstehung Freier Software“.

Das englische Wort "free" hat zwei unterschiedliche Bedeutungen und steht in dem seit 1982 gebräuchlichen Begriff "Freeware" für „kostenfrei“ (genauer für „kostenlose Software“); in "Freie Software" (englisch "Free Software") steht es für „Freiheit“ (genauer für „freiheitsgewährende Software“). Englischsprachige Aktivisten machen die Unterscheidung mit "free as in free beer" („frei wie Freibier“) und "Free as in Freedom" („frei wie in Freiheit“) deutlich.

Freeware räumt dem Benutzer nicht die von der Free Software Foundation aufgelisteten Freiheiten ein, sondern die der individuellen Lizenzvereinbarung mit dem Urheber. Daher gilt sie als „unfreie“ Software.

Freie Software enthält hingegen die genannten Freiheiten und kann, muss aber nicht kostenlos sein.

Der Begriff "Open Source" (zu deutsch „quelloffen“) wurde 1998 von den Gründern der Open Source Initiative (OSI) eingeführt: Eric S. Raymond, Bruce Perens und Tim O’Reilly. Sie wollten den pragmatischeren Ansatz derartiger Software in den Mittelpunkt stellen, statt auf eine (aus ihrer Sicht) möglicherweise abschreckend wirkende, moralisch aufgeladene und polarisierende Freie-Software-Idee zu setzen. Quelloffene Software wird von ihnen als vorteilhaftes Entwicklungsmodell beschrieben, wobei die Frage, ob Software quelloffen sein sollte, dort eine rein praktische und keine ethische Frage ist.

Mit der Betonung der Überlegenheit des Entwicklungsprozesses gibt die OSI eher die Sichtweise der Entwickler wieder, während die FSF auf die Sicht der Anwender fokussiert. Die FSF begreift unfreie Software als gesellschaftliches Problem. In ihren Augen ist die Entscheidung für oder gegen Freie Software deshalb primär eine ethische und soziale Entscheidung; der praktische Nutzen ist sekundär. Da in der Darstellung der OSI die Freiheit, die Freie Software den Benutzern gibt, nicht erwähnt wird, wirft die FSF der OSI eine Ablenkung von den wesentlichen Punkten vor.

Diese zwei unterschiedlichen Bewegungen mit unterschiedlichen Sichtweisen verbindet die gemeinsame Wertschätzung für quelloffenen Code und das Ziel des Aufbaus eines freien Softwareökosystems, was in zahlreichen Projekten mündet, in denen sie zusammenarbeiten. Alternative Kompromissbezeichnungen wie "Free and open source software" (FOSS) oder „Free/Libre Open Source Software“ (FLOSS), die von Anhängern beider Positionen akzeptiert werden, sollen die Gemeinsamkeiten betonen.

Für weitere Informationen dazu siehe den Abschnitt „Vergleich mit der Open-Source-Definition“.

Wegen Bedenken bezüglich kommerzieller Ausnutzung oder amoralischem Gebrauch der eigenen Software gab und gibt es Bestrebungen, nicht alle Freiheiten aus der Definition freier Software in ihrer Lizenz uneingeschränkt zu gewähren. Werden die von der FSF aufgelisteten Freiheiten um die kommerzielle Weiterverbreitung vermindert (aber die sonstigen unverändert beibehalten), wurde dies von der FSF bis 2011 ablehnend als "halbfreie Software" (englisch "semi-free software") bezeichnet.

Seit 2012 nimmt die FSF diese Unterscheidung nicht mehr vor und zählt Software mit derart angepassten Lizenzen zur „unfreien“ Software.

Sind eine oder mehrere Bedingungen der von der FSF aufgelisteten Freiheiten nicht erfüllt, wird die Software als proprietär oder „unfrei“ (im Sinne fehlender Freiheiten) bezeichnet.

"Freie Hardware" (, auch bezeichnet als "open hardware" oder "open source hardware") steht der Freie-Software- und Open-Source-Bewegung nahe bzw. geht auf diese zurück. Dabei handelt es sich um Hardware, die nach freien Bauplänen hergestellt wird.

Die auf Februar 1986 datierte früheste bekannte Veröffentlichung der Definition stammt von der (jetzt nicht mehr weitergeführten) "„GNU's Bulletin Publication“" der FSF. Die Quelle für dieses Dokument ist im Philosophieabschnitt der Webseite des GNU-Projekts zu finden. Die Definition bezog sich zunächst auf zwei Punkte:

Im Jahr 1996 wurde Freie Software auf der Webseite gnu.org definiert, indem man sich auf die „drei Ebenen von Freiheit“ bezog und explizit noch hinzufügte, dass man auch die Freiheit haben müsse, die Software studieren zu können. Das kann auch in der älteren Zweipunktedefinition als Teil der Freiheit, das Programm verändern zu können, herausgelesen werden, ist dort aber nicht so deutlich hervorgehoben. Später mied Stallman das Wort „Ebene“, weil man alle Freiheiten brauche und das Wort dafür etwas irreführend sei.

Schlussendlich wurde noch eine Freiheit hinzugefügt, die explizit sagt, dass Nutzer die Möglichkeit haben sollten, das Programm auszuführen, wie er möchte, für jeden Zweck. Die bestehenden Freiheiten wurden bereits von eins bis drei nummeriert, aber diese Freiheit sollte vor den anderen kommen; darum wurde sie ergänzt als „Freiheit 0“.

Die moderne Definition definiert Freie Software durch die bereits weiter oben aufgeführten vier Freiheiten. Zusammengefasst definiert sie Freie Software als Software, die Endnutzern die Freiheiten der Nutzung, des Überprüfens/Studierens, des Teilens und des Modifizierens der Software gewährleistet.

Seit April 2008 wird die Definition in 39 Sprachen auf der FSF Webseite veröffentlicht. Ebenfalls veröffentlicht die FSF auf ihrer Webseite eine Liste von Lizenzen, die den Anforderungen an diese Definition gerecht werden.

Im Juli 1997 publizierte Bruce Perens die Debian Free Software Guidelines.

Die Unterschiede zur Freien Software wurden bereits weiter oben im Abschnitt zur Abgrenzung zur Open-Source-Software verdeutlicht. Demgegenüber gibt es auch viele Gemeinsamkeiten. So wurde beispielsweise die „Debian Free Software Guidelines“ von der Open Source Initiative (OSI) unter dem Namen „Open Source Definition“ verwendet; die einzige Änderung ist die Ersetzung des Begriffs „Freie Software“ durch „Open Source Software“. Die FSF kommentierte dazu folgendes:

Die genannten Ziele von Freier Software, die Freiheit in der Kontrolle der eigenen Datenverarbeitung und die Kooperation, werden durch die Gewährung folgender Rechte erreicht: die Nutzer dürfen Freie Software ausführen, kopieren, verbreiten, untersuchen, ändern und verbessern; diese Freiheiten werden gewährt und nicht (wie bei proprietärer Software) entzogen. Entscheidend ist daher nicht die Vermeidung von Kosten, sondern die Freiheiten der Endnutzer. So wird das Kontrollrecht über die Software dadurch garantiert, dass ein Nutzer von Freier Software immer den dazugehörigen Quellcode zur Verfügung hat oder diesen zumindest nachträglich beziehen kann (was untersuchen und modifizieren ermöglicht),- und dadurch, dass ein Nutzer durch gewährte Kooperation dies wahlweise von Anderen durchführen lassen kann.

Freie Software ist an ihrer Lizenz erkennbar. Dazu gehört die GNU General Public License und andere freie Softwarelizenzen. Ein soziales und ethisches Grundprinzip hinter den verfochtenen Rechten an und mit Freier Software ist, dass ihre Entwickler die Freiheit sowie die Gemeinschaft der Endnutzer schätzen und respektieren, weil die Nutzungsbedingungen von Freier Software es Anwendern wie Entwicklern gleichermaßen ermöglichen, ein Umfeld der Unabhängigkeit, Gemeinschaft, Zusammenarbeit, Ethik, Solidarität und des Austauschs zu schaffen und zu gestalten.

Der Begriff „Freie Software“ und dessen genaue Definition sowie die Unterscheidung zu proprietärer Software so wie der spezifische Freiheitsgedanke gehen entscheidend zurück auf den Beginn des GNU-Projekts um Programmierer-Aktivisten wie Richard Stallman und die damit verbundene Gründung der Free Software Foundation (FSF) im Jahr 1985.

Die in Bezug auf Anspruch und Zielsetzungen zwar eng verwandte, der Wahl ihrer Mittel und Wege dorthin nach nicht identische und eigenständige Open-Source-Bewegung entstand erst später (1998) und in einem anderen personellen Umfeld. Der Unterschied zwischen den Ansätzen Freie Software und Open Source liegt vor allem in der jeweiligen Gewichtung prinzipiell gemeinsam vertretener Werte: Im Sinne von Open Source liegt das Hauptaugenmerk beim praktischen Nutzen und den Entwicklungsmethoden, während der Fokus der Freien-Software-Gemeinschaft auf den ethischen, sozialen und politischen Implikationen liegt.

Zu den Nutzern Freier Software gehören Privatnutzer, Unternehmen und öffentliche Einrichtungen, wie Regierungen (vor allem auf Grund der Unabhängigkeit, Freiheit und Kontrolle der eigenen Datenverarbeitung), Forschungszentren (CERN), Universitäten, der New York Stock Exchange, Wikipedia, das Verteidigungsministerium der Vereinigten Staaten.

Der Fokus von "Freier Software" ist also die Freiheit des Anwenders zu teilen, zu kopieren und zu modifizieren und steht den üblichen Beschränkungen "proprietärer Software" („unfreie Software“ - im Sinne fehlender Freiheiten) entgegen: Bei proprietärer Software versuchen Entwickler, die Endnutzer und den Endverbraucher-Markt zu kontrollieren und zu monopolisieren (etwa durch restriktive Endbenutzer-Lizenzverträge, Geheimhaltungsverträge, Produktaktivierungen, Dongles, Kopiersperren, proprietäre Formate oder den Vertrieb von binären ausführbaren Programmen ohne Quelltexte) und zwingen die Nutzer somit in die Abhängigkeit vom jeweiligen Entwickler-Unternehmen. Freie Software hingegen unterscheidet sich durch die gewährte Freiheit: Verwendung, Teilen, Modifizieren.

Das Selbstverständnis der gewährten Freiheiten ist es, dass diese als notwendig für die Förderung des sozialen und ethischen Anliegens gesehen werden, welches die Freiheit und Gemeinschaft der Nutzer respektiert und wertschätzt (im Bereich von Computer-Nutzung und Datenverarbeitung), indem Freie Software aktiv Zusammenarbeit und Kooperation ermöglicht: Nutzer haben so die Möglichkeit, im Gebrauch ihrer Computer und Datenverarbeitung eine Gemeinschaft von Wohlwollen und ethischer Aufrichtigkeit gründen zu können. Freie Software kann für jeden Zweck verwendet werden (ohne Notwendigkeit einer Bindung an bestimmte soziale oder ethische Werte), aber die FSF und das GNU-Projekt fördern aktiv die Werte der Freiheit, Gemeinschaft, Zusammenarbeit und ethische Solidarität welche Freie Software ermöglicht.

Freie Software hat durch ihre Grundsätze einen starken Fokus auf Zusammenarbeit und Zusammenhelfen in einer Gemeinschaft: An öffentlich geführten Freie-Software-Projekten kann sich jeder beteiligen. Dies schließt den kommerziellen und gewerblichen Nutzen nicht aus, da diese Freiheit nicht eingeschränkt wird. Das Wort "frei" im Begriff "Freie Software" bezieht sich auf Freiheit und nicht auf "kostenlos"; Freie Software hat nichts mit monetären Kosten oder Geld zu tun. Freie Software ist in der Regel kostenlos, aber unterliegt keiner solchen Einschränkung. Eine freie Software darf kommerziell verwendet, zu jedem Preis verkauft oder weitervertrieben werden und ist dennoch Freie Software, solange die Freiheitsrechte sichergestellt bleiben. Dafür erlaubt die am häufigsten verwendete GPL jedoch nicht, eine GPL-Software kompiliert zu verkaufen und für die Quelltext-Bereitstellung einen gesonderten hohen Preis zu verlangen (über einer geringen Erstellungsgebühren, z. B. Medium), was Kommerzialisierungsoptionen von GPL-Software in der Praxis einschränken kann.

Die Freiheiten im Umgang mit Freier Software sind durch Freie-Software-Lizenzen rechtlich verankert und können somit garantiert werden. Zu solchen Lizenzen gehören die GNU General Public License (GPL), welche durch das Copyleft-Prinzip besagt, dass Nutzern von bearbeiteten und wiederveröffentlichten Versionen der Software die gleichen Freiheiten gegeben werden müssen. Aber auch freizügige Lizenzen wie die BSD-Lizenz (die kein Copyleft erfordern) werden als Freie-Software-Lizenzen akzeptiert.

Im Gegensatz zu proprietärer Software, bei der sich ein Code zum unbemerkten Ausspionieren/Überwachen, zur eingeschränkten Nutzung von Medien durch Digitale Beschränkungsverwaltung sowie Hintertüren für unbemerktes ferngesteuertes Einschleusen von Änderungen (ungewollte, unbemerkte „Updates“) befinden kann, bietet Freie Software durch ihre Untersuchbarkeit eine Software, aus der jegliche unerwünschten und schädlichen Eigenschaften entfernt werden können und daher meist von vornherein nicht vorhanden sind.

Bei Freier Software geht es darum, dass Nutzer Freiheiten für die empfangene Software erhalten: in erster Linie den Quelltext (um Änderungen machen zu können).

Freiheitsgewährende Software (Freie Software) sagt nichts über die Verfügbarkeit der Software aus: Manche wird als öffentliches Projekt entwickelt (und ist somit jedem verfügbar), andere wird in kundenspezifischem Entwicklungs-Auftrag speziell für Unternehmen, Organisationen, Regierungen oder sogar einzelnen Nutzer entwickelt (und ist somit nur denen verfügbar, außer sie nutzen ihr Recht auf Weitergabe). Jedoch haben alle Empfänger mit dem Empfang der Software die Freiheitsrechte von Freier Software empfangen.

Software, die von einer Vielzahl von Nutzern brauchbar ist (Programme für Textverarbeitung, Webbrowsing), wird bei freiheitsgewährender Software meist in öffentlichen Projekten entwickelt. Diese Programme sind Freie Software, die öffentlich verfügbar ist. Durch die Rechte Freier Software dürfen Nutzer Änderungen an dieser Software vornehmen, ohne diese Änderungen verbreiten, oder die ursprünglichen Entwickler davon benachrichtigen zu müssen. Allerdings werden die Änderungen/Verbesserungen meist dem ursprünglichen Projekt zur Verfügung gestellt. Dadurch können die Verbesserungen ein fixer Teil der öffentliche Software werden (müssen also in dem Fall nicht immer selber dazugefügt werden, da etwaige Beiträge dann von der Community weitergewartet werden. Dies bedingt aber zuerst, dass sich die Koordinatoren des öffentlichen Projekts dafür entschieden haben, die spezifischen Verbesserungen bzw. Änderungen aufzunehmen und einzupflegen); außerdem kommen Verbesserungen der Allgemeinheit zugute, in dem sich die Software durch unterschiedlich Beiträge verbessern kann. Da es bei öffentlichen Projekten Meinungsverschiedenheiten geben kann (oft auf rein technischer Ebene, wo unterschiedliche technische Ziele verfolgt werden können), kommt es dazu, dass ein öffentliches Projekt abgezweigt (ge-"forked") wird und es fortan in zwei oder mehr unterschiedlichen separaten Varianten öffentlich zur Verfügung steht. Dies basiert auf dem Recht, das Freie Software änderbar ist (und die geänderte Version wiederveröffentlichbar ist).

Freiheitsgewährende Software, die einem Empfänger erlauben, die Freiheiten bei darauffolgender Weitergabe wieder zu entziehen, wird freizügige Software genannt. Freiheitsgewährende Software, die sicherstellt dass empfangene Freiheiten bei Weitergabe der Software weitergegeben werden müssen, wird Copyleft Software genannt. Beides gilt als Freie Software (Copyleft sichert Freiheiten für jeden, in dem die Freiheiten bei Weitergabe nicht wieder entzogen werden dürfen, wie es bei freizügiger Software der Fall ist.)

Programmierer, die substantielle Software der Öffentlichkeit als Freie Software zur Verfügung stellen, wählen (als Urheber) oft eine Freiheitsgewährende Software-Lizenz mit Copyleft, da sie somit verhindern können, dass die Software von Unternehmen aufgegriffen wird und Teile daraus zu proprietärer Software verarbeitet wird, welche Nutzern wieder Freiheitsrechte entziehen würde. Andere wählen ein Dual-License Konzept aus, wo die Software der Öffentlichkeit z. B. als Copyleft zur Verfügung steht, aber die Software gegen Bezahlung zu anderen Bedingungen erhältlich ist. Manche veröffentlichen Programme unter freizügigen Lizenzen (also ohne Copyleft); vor allem dann, wenn das Ziel verfolgt wird, ein Programm so breitflächig wie möglich (selbst für proprietäre Softwareentwickler) nutzbar zu machen (wenn es darum geht ein neues Protokoll oder eine neue Bibliothek zu fördern).

Freie Software darf kommerzielle Software sein (kommerziell und proprietär sind nicht das gleiche). Freie Software darf kommerziell verkauft und weitergegeben und kommerzielle Tätigkeiten (z. B. Support) dürfen angeboten werden. Natürlich darf dies auch gratis geschehen, dann meist mit Verzicht auf Gewährleistung. Freie Software ist aber nie proprietär, indem sie Nutzern die Freiheiten auf Modifizierung und Weitervertrieb verbietet oder unmöglich macht.

Private oder kundenspezifische Software, welche für einen bestimmten Benutzer (normalerweise eine Organisation) entwickelt wurde (normalerweise gegen Bezahlung), und nicht öffentlich verfügbar ist, kann Freie Software sein. Dies ist der Fall, wenn der alleinige Benutzer der Software (der einzige Empfänger der Software), dennoch die vier Freiheiten erhalten hat.

Die Freie-Software-Bewegung lehnt die These ab, dass Programmierer berechtigt seien, Nutzern Freiheiten zu entziehen, um Gewinn zu erzielen. Stattdessen wertet die Freie-Software-Bewegung das Recht auf Freiheit der Nutzer höher als einen finanziellen Gewinn von Programmierern oder Software-Unternehmen, wenn dieser auf Kosten der Freiheit anderer (Eigenkontrolle und/oder Gemeinschaft, oder Privatsphäre) basiert. Deswegen erlaubt die am häufigsten verwendete GPL zwar die Kommerzialisierung, erlaubt jedoch nicht für den Quelltext mehr zu verlangen als für das kompilierte Programm (ausgenommen zusätzlicher Bereitstellungsgebühren z. B. für das Medium), was die Kommerzialisierung von GPL Software erschweren kann.

Die Freie-Software-Bewegung ist aber durchaus für eine Bezahlung von Programmierern, wenn diese die Freiheit der Nutzer achten, sowie für alle Geschäftsmöglichkeiten rund um Freie Software (Support, Wartung, Betreuung). Richard Stallman schildert mögliche Szenarien einer Softwaresteuer, mit der eine Regierung Geld für die Entwicklung von allgemeinnütziger Software (die zusätzlich Freiheiten gewährt) zur Verfügung stellen könnte.

Einige Menschen sehen in der Freie-Software-Bewegung Ansätze, die Möglichkeiten zur Überwindung des Kapitalismus zu zeigen. In Deutschland beschäftigt sich unter anderem das Projekt Oekonux mit dieser Thematik. Andere sehen in freier Software lediglich einen weiteren Wettbewerber innerhalb der marktwirtschaftlichen Ordnung. Die Freiheit, die Software in andere Sprachen zu übersetzen, kommt besonders denjenigen Sprachgruppen zugute, für die eine Übersetzung bisher kommerziell nicht interessant war.

Die Freiheit der Software wird vom UNO-Weltgipfel zur Informationsgesellschaft (WSIS) als schützenswert anerkannt. Sie gehört zu den elementaren Forderungen der Zivilgesellschaft, mit der die „Digitale Kluft“ überwunden werden soll. Unter „digitaler Kluft“ wird die Spaltung in Regionen der Erde bezeichnet, die sich die Schaffung von IT-Infrastruktur und damit vor allem die Teilnahme am Internet leisten können, und solchen, die dies nicht können. Anders als bei proprietärer Software fließt bei der Verwendung freier Software kein Geld in fremde Länder ab, wo die Anbieter proprietärer Software ihren Unternehmenshauptsitz haben. Alle Mittel, die vor Ort für die IT bereitstehen, können daher unmittelbar in die IT-Wirtschaft vor Ort einfließen.

1931 gründete Thomas J. Watson Sr. von IBM ein "Methods Research Department" um das Wissen zum Betrieb seiner Datenverarbeitungsverfahren zu sammeln und effektiv mit seinen Kunden zu teilen, was IBM mit den Nutzergruppen SHARE (Society to Help Avoid Redundant Effort) und GUIDE für seine Mainframe-Programmierung fortführte. Bis 1970 wurde Software von IBM kostenlos und inklusive Quellcode zur Verfügung gestellt. Zwischen 1960 und 1970 etablierte sich unter anderem an akademischen US-Einrichtungen (Stanford, Berkeley, Carnegie Mellon und MIT) eine „Hacker-Kultur“, für die es selbstverständlich war, eigene Software-Verbesserungen mit anderen Programmierern zu teilen. Programmierer tauschten die Software frei untereinander aus und gaben häufig den entsprechenden Quelltext weiter. Insbesondere in großen Benutzergruppen wie der DEC User Group (DECUS) war dies üblich. Es war gängige Praxis, den Quelltext der mit Computersystemen ausgelieferten Software mitzuliefern. Dadurch kamen viele Vorschläge für Verbesserungen und Fehlerkorrekturen zu den Computerherstellern zurück. Software wurde als Zugabe zu Computern gesehen, um diese nutzbar zu machen.

Am 23. Juni 1969 kündigte IBM neue Regeln für die Nutzung und Wartung seiner Software, getrennt von den Hardware-Nutzungsbedingungen an. Für Software wurde urheberrechtlicher Schutz in Verbindung mit Lizenzverträgen eingeführt. Die bislange freie Dienstleistung zur Wartung und Weiterentwicklung von Software wurde gesondert berechnet, was einen eigenen Wirtschaftsmarkt für diesen Dienstleistungssektor begründete. In den späten 1970er-Jahren begannen gleichfalls andere Unternehmen, „Softwarelizenzen“ einzuführen, welche den Nutzen, die Weitergabe und die Möglichkeit der Veränderung der Programme einschränkte. Außerdem wurden viele Programme nicht mehr im Quelltext geliefert, sondern nur noch in maschinenlesbarer Form, zum Schutz der Software als Geschäftsgeheimnis, was eine Veränderung nahezu unmöglich machte. Zusätzlich wurde es mit dem Aufkommen von finanzierbaren Mikrocomputern von IBM, Apple, Atari oder Commodore üblich, Software getrennt von Computer-Hardware zu verkaufen und den Quelltext vor der Konkurrenz zu verbergen, die Software wurde somit proprietär. Immer mehr Hacker wurden von den Softwareunternehmen angestellt, und die bisher wahrgenommenen Freiheiten wurden stark eingeschränkt, Software wurde zu einem künstlich verknappten Gut.

In diese Zeit fiel die Arbeit von Richard Stallman am „AI Lab“ (Abteilung für Künstliche Intelligenz) des Massachusetts Institute of Technology. Als dort ebenfalls proprietäre Software in den Laboren eingeführt wurde, bemühte Stallman sich darum, durch das Programmieren alternativer Software eine Monopolstellung proprietärer Anbieter zu verhindern. Er folgte damit seinen Prinzipien einer wissenschaftlichen Zusammenarbeit, die einen freien und ungehinderten Austausch von Software vorsahen.

Das Unternehmen AT&T entschied sich 1983, eine proprietäre Version seines Unix auf den Markt zu bringen: UNIX System V. Im September 1983 gründete Richard Stallman das GNU Projekt mit dem Ziel, ein freies, UNIX-ähnliches Betriebssystem mit Namen „GNU“ zu entwickeln.

Damit die Idee der Freiheit auch rechtlich abgesichert sein würde, mussten freie Lizenzen erdacht werden. Stallman entwarf das Copyleft-Prinzip, das bedeutet, "dass alle, die die Software (mit oder ohne Änderungen) weiter verteilen, die Freiheit zum Weitergeben und Verändern mitgeben müssen. Das Copyleft garantiert, dass alle Benutzer Freiheit haben." Auf diesem Prinzip beruhen die Lizenzen der GNU-Software.

Eine organisatorische Basis für GNU und Freie Software überhaupt ist die 1985 gegründete, gemeinnützige Stiftung Free Software Foundation (FSF). Für Softwareprojekte bestanden in den frühen und mittleren achtziger Jahren noch jeweils individuelle Lizenzen. Stallman verband die Rahmenpunkte zu einer einzelnen Lizenz und veröffentlichte 1989 die GNU General Public License (GNU GPL). Diese ist die heute am stärksten verbreitete Lizenz für Freie Software.

1991 war das GNU-Betriebssystem bis auf den Kernel vollständig. Einige Leute erkannten, dass Linux, ein damals proprietärer Kernel für Minix, sich für GNU eignen könnte. Nachdem die Entwickler den Linux-Kernel unter die „GNU GPL“ stellten, konnte zum ersten Mal ein vollständig freies Betriebssystem ausgebaut werden. Mit der darauffolgenden rasanten Entwicklung und Verbreitung von GNU und Linux wurde freie Software von zunehmend mehr Menschen genutzt.


Im Juli 2007 waren über 5000 Software-Pakete im "„FSF/UNESCO Free Software Directory“" eingetragen, welches 1999 ebenfalls als ein Projekt der FSF startete.


Eine Studie aus dem Jahr 2015 zeigt, dass die meisten Freie-Software-Projekte beim Webdienst GitHub von wenigen oder nur einem einzigen Entwickler abhängen.

Freie Software kann, gemäß den vier Freiheiten, meist nahezu beliebig kopiert und weitergegeben werden. Freie Software darf zwar zu einem beliebig hohen Preis verkauft werden, doch ist sie fast immer kostenlos im Internet erhältlich, und so ist ihr Verkaufswert auf Datenträgern meist nicht viel höher als die Selbstkosten. Eine bemerkenswerte Ausnahme ist das GNU-Projekt, das Freie Software mit deutlicher Profitspanne anbietet und in den 1980er-Jahren einen erheblichen Teil seiner Einnahmen aus dem Verkauf von Software bestritt. Das GNU-Projekt hatte damals jedoch wegen seiner zentralen Rolle bei der Entwicklung freier Software und der damals geringen Verbreitung und Leistungsfähigkeit des Internets eine besondere Rolle, die es heute nicht mehr gibt.

Einige Geschäftsmodelle, die mit freier Software zu tun haben, konzentrieren sich deswegen auf den Dienstleistungsaspekt der Softwareentwicklung, -weiterentwicklung und -anpassung. Wartung und individuelle Anpassung der Software sowie Schulung und technische Unterstützung sind für die Kunden vorrangig. Unternehmen, die allein diese Dienstleistungen als Geschäftsstrategie gewählt haben, sind zum Beispiel MySQL AB, Red Hat und Qt Development Frameworks. Freie Software unterliegt keiner Rivalität und nicht der Ausschließbarkeit, ist somit ein reines öffentliches Gut und kann folglich nicht einem üblichen Marktgeschehen unterliegen. Dennoch betrachten die Herausgeber proprietärer Software sie als eine ernste Bedrohung für ihr Geschäftsmodell der Lizenzierung und versuchen deshalb, potenzielle Kunden von der Benutzung freier Software abzuhalten. Trotzdem sind Hersteller proprietärer Software aktive Nutzer von freier Software und unterstützen die Verfügbarkeit von proprietärer Software auf Plattformen, die auf freier Software basieren.

Als Argumente für ihre Produkte führen Hersteller proprietärer Software unter anderem mehr Garantien, bessere Qualität – besonders im Hinblick auf Benutzerfreundlichkeit und bessere Dienstleistungen – an. Solche Argumente, die auf Versprechungen von Open Source, nicht von freier Software kontern, hat Richard M. Stallman wiederholt als tendenziös und thematisch verfehlt angegriffen; in seinen Augen ist die Entscheidung für oder gegen Freie Software primär eine ethische und soziale Entscheidung, von der nicht durch Qualitätsdiskussionen abgelenkt werden darf.

Es gibt verschiedene Typen von Lizenzen, die die Kriterien freier Software erfüllen:

Hardware-Hersteller gehen immer mehr dazu über, die Schnittstellenspezifikationen geheim zu halten, um der Konkurrenz die Nachahmung technischer Lösungsansätze zu verwehren. Der Grund hierfür liegt im zunehmenden Wettbewerbsdruck und darin, dass es billiger ist und schneller geht, einen solchen Schutz technisch einzubauen, als das errungene geistige Gut durch ein Patent für sich zu reservieren. Wenn nicht öffentlich dokumentiert ist, wie die Geräte anzusteuern sind, erleidet die Hardwareunterstützung freier Betriebssysteme mittels freier Treiber einen schweren Rückschlag, da sie allenfalls noch durch Reverse Engineering in Gang gebracht werden kann.

Andererseits haben die Hersteller die Benutzer der wichtigeren freien Betriebssysteme (vor allem GNU/Linux – wobei insbesondere Linux und der X.Org-Server relevant sind) als Kundengruppe erkannt. Viele von ihnen stellen jedoch proprietäre Treiber zur Verfügung. Diese Treiber stoßen unter den Anhängern freier Software auf höchst geteilte Meinungen: einige sind glücklich darüber, dass sie die „Unterstützung“ der Hardware-Hersteller gewonnen haben und deren Hardware durch das von ihnen bevorzugte Betriebssystem nun mehr oder weniger unterstützt wird, andere lehnen proprietäre Treiber grundsätzlich ab.

Von einer generellen Schnittstellenfreigabe würden die Benutzer freier Betriebssysteme sicherlich profitieren. Neben den ideologischen kommen hier Fragen der Systemstabilität zum Tragen. Sollte beispielsweise ein proprietärer Linux-Netzwerkkartentreiber regelmäßig zu Abstürzen des Systems führen, wären die Linux-Entwickler dagegen machtlos und es hinge vom Gutdünken des Herstellers ab, ob der Fehler behoben wird.

Die regelmäßig in den Schlagzeilen auftauchenden Softwarepatente haben auf Freie Software einen besonders schwerwiegenden Einfluss, denn es ist zum Teil rechtlich noch nicht einmal möglich, mit freier Software die Patentauflagen zu erfüllen. Diese bestehen nämlich in einigen Fällen auf einer Gebühr pro in Umlauf gebrachter Kopie, aber Freie Software verlangt gerade, dass der Herausgeber darauf keinen Einfluss haben darf. Selbst wenn er die Lizenzgebühren zum Beispiel durch Spenden zahlen würde, müsste er eine genaue Zahl der Kopien, die im Umlauf sind, vorlegen können, womit es keine Freie Software mehr wäre.

Trusted Computing kann Veränderungen an einer Computer-Plattform eindeutig erkennen und damit sowohl externe Software-Angriffe als auch Veränderungen durch den Benutzer, Konfigurationen, Fehlfunktionen, Sicherheitslücken oder von Anwendungsprogrammen eindeutig identifizieren. Die Reaktion auf eine solche Veränderung kann (aber muss nicht) durch ein entsprechendes, sicheres Betriebssystem erfolgen. Trusted Computing kann daher auch zur Absicherung von Digital Rights Management (DRM) und zum Kopierschutz verwendet werden.

"Politisch gesehen" muss Freie Software immer vom Benutzer ersetzbar und veränderbar sein. Software, die in binärer Form zertifiziert sein muss, ist dies nicht. "Technisch gesehen" kann in freier Software vor dem Benutzer nichts im Binärcode verheimlicht werden, weil der Quellcode für jeden zugänglich sein muss. Somit kann die Verschlüsselung, mit der die Daten vor dem Benutzer „bewahrt“ werden, einfacher hintergangen werden.

Eine weitere Inkompatibilität tut sich mit dem Kopierschutz von DVDs auf: Die Umgehung wirksamer Kopierschutzmaßnahmen ist seit den um die Jahrtausendwende weltweit nach und nach etablierten Reformen zum Copyright (in den USA der Digital Millennium Copyright Act (DMCA)) gesetzlich nur noch mit Zustimmung des Rechteinhabers erlaubt. Dieses Verbot erstreckt sich auf die Herstellung oder Verbreitung von Programmen, die diese Maßnahmen umgehen können, so dass freie Abspielsoftware für kopiergeschützte DVDs nicht legal geschrieben werden kann – aus ihren natürlichen Interessen heraus würden die Rechteinhaber ihre Zustimmung dazu niemals erteilen, weil dadurch der Sinn der Maßnahmen ad absurdum geführt würde.

Falls Hardwarehersteller wie Intel oder AMD funktionseinschränkende Verfahren in Chipsätze oder Prozessoren implementieren sollten, könnte Freie Software den vollen Funktionsumfang möglicherweise nur noch auf freier Hardware entfalten.





</doc>
<doc id="14518" url="https://de.wikipedia.org/wiki?curid=14518" title="Moderator">
Moderator

Moderator bzw. Moderation (lateinisch; von "", „mäßigen“, „steuern“, „lenken“) steht für:
Siehe auch:


</doc>
<doc id="14519" url="https://de.wikipedia.org/wiki?curid=14519" title="Kernreaktor">
Kernreaktor

Ein Kernreaktor, auch Atomreaktor oder Atommeiler ist eine Anlage, in der eine Kernspaltungsreaktion kontinuierlich als Kettenreaktion im makroskopischen, technischen Maßstab abläuft.

Weltweit verbreitet sind Leistungsreaktoren, das heißt Kernreaktoranlagen, die durch die Spaltung () von Uran oder Plutonium zunächst Wärme und daraus meist elektrische Energie (siehe Kernkraftwerk) gewinnen. Dagegen dienen "Forschungsreaktoren" zur Erzeugung von freien Neutronen, etwa für Zwecke der Materialforschung oder zur Herstellung von bestimmten radioaktiven Nukliden zu medizinischen oder anderen Zwecken. Im Erdaltertum kam es wiederholt zur Bildung natürlicher Kernreaktoren.

Ein Kernkraftwerk hat oft mehrere Reaktoren. Hier werden die beiden Begriffe oft ungenau verwendet. Zum Beispiel ist mit der Aussage „in Deutschland liefen bis zum Atomausstieg 17 Kernkraftwerke“ gemeint, dass 17 Kernreaktoren an deutlich weniger Standorten liefen. So etwa bestand das Kernkraftwerk Gundremmingen ursprünglich aus drei Reaktorblöcken; jeder Block besteht aus einem Reaktor mit Dampferzeuger und einem Turbosatz.

Die meisten Kernreaktoren sind ortsfeste Anlagen. In der Atom-Euphorie der späten 1950er- und frühen 1960er-Jahre kam der Gedanke an atomgetriebene Straßenfahrzeuge, Flugzeuge oder Raumschiffe auf. Inzwischen gibt es einige Kernreaktoren in U-Booten, Überwasserschiffen und Raumflugkörpern. Beispielsweise 

Zwischen den Protonen und den Neutronen eines Atomkerns wirken sehr starke anziehende Kräfte, die jedoch eine nur sehr begrenzte Reichweite haben. Daher wirkt diese Kernkraft im Wesentlichen auf die nächsten Nachbarn – weiter entfernte Nukleonen tragen zu der anziehenden Kraft nur in geringem Maße bei. Solange die Kernkraft größer ist als die abstoßende Coulombkraft zwischen den positiv geladenen Protonen, hält der Kern zusammen. Kleine Atomkerne sind stabil, wenn sie je Proton ein Neutron enthalten: Ca ist das schwerste stabile Nuklid mit gleicher Protonen- und Neutronenzahl. Mit zunehmender Protonenzahl wird ein immer höherer Neutronenüberschuss zur Stabilität erforderlich; die abstoßende Coulombkraft der Protonen untereinander wird durch die anziehende Kernkraft der zusätzlichen Neutronen kompensiert.

Fängt ein sehr schwerer Kern, etwa des Uranisotops U oder des Plutoniumisotops Pu, ein Neutron ein, so wird er durch die gewonnene Bindungsenergie zu einem hochangeregten, instabilen U- beziehungsweise Pu-Kern. Solche hochangeregten schweren Kerne regen sich mit extrem kurzen Halbwertszeiten durch Kernspaltung ab. Anschaulich gesagt gerät der Kern durch die Neutronenabsorption wie ein angestoßener Wassertropfen in Schwingungen und zerreißt in (meist) zwei Bruchstücke (mit einem Massenverhältnis von etwa 2 zu 3), die mit hoher Bewegungsenergie auseinanderfliegen; außerdem werden etwa zwei bis drei "schnelle" Neutronen frei. Diese Neutronen stehen für weitere Kernspaltungen zur Verfügung; das ist die Grundlage der nuklearen Kettenreaktion.

Wenn Neutronen auf Kernbrennstoff treffen, finden neben der Kernspaltung unvermeidlich auch andere Kernreaktionen statt. Von besonderem Interesse sind Reaktionen, in denen Bestandteile des Kernbrennstoffs, die selbst nicht spaltbar sind, in spaltbare umgewandelt werden. Solche Reaktionen heißen Brutreaktionen, der Vorgang "Brüten" oder auch "Konversion". Von einem Brutreaktor spricht man allerdings erst dann, wenn mehr neues spaltbares Material erzeugt wird, als der Reaktor selbst in der gleichen Zeit verbraucht, die Konversionsrate also über 1,0 beträgt.

Der Brennstoff fast aller Kernreaktoren enthält hauptsächlich Uran. Daher ist die Brutreaktion an dem nicht spaltbaren Uranisotop U besonders wichtig. Das U wandelt sich durch Neutroneneinfang in U um; dieses geht von selbst durch zwei aufeinander folgende Betazerfälle in das spaltbare Plutoniumisotop Pu über: 

Das Pu wird teilweise noch im Reaktor wieder gespalten, teilweise kann es aber durch Aufarbeitung des gebrauchten Brennstoffes abgetrennt und zu anderen Zwecken verwendet werden.

Falls das abgetrennte Plutonium zu Kernwaffenzwecken dienen soll ("Waffenplutonium"), muss es isotopisch möglichst rein sein, d. h., es darf nicht zu viel Pu enthalten. Dieses nächstschwerere Plutoniumisotop entsteht, wenn der Pu-Atomkern ein weiteres Neutron einfängt. Daher erhält man waffenfähiges Plutonium nur aus solchen Brennelementen, die schon nach relativ kurzer Betriebszeit dem Reaktor entnommen werden.

In entsprechender Weise wie Pu-239 aus U-238 kann auch das spaltbare U-233 aus Thorium Th-232 erbrütet werden.

Die neu entstandenen Kerne mittlerer Masse, die so genannten Spaltprodukte, haben eine größere Bindungsenergie pro Nukleon als der ursprüngliche schwere Kern. Die Differenz der Bindungsenergien tritt größtenteils als kinetische Energie der Spaltfragmente auf (Berechnung). Diese geben die Energie durch Stöße an das umgebende Material als Wärme ab. Die Wärme wird durch ein Kühlmittel abgeführt und kann beispielsweise zur Stromerzeugung, Heizung oder als Prozesswärme etwa zur Meerwasserentsalzung genutzt werden.

Etwa 6 % der gesamten in einem Kernreaktor frei werdenden Energie wird in Form von Elektron-Antineutrinos frei, die praktisch ungehindert aus der Spaltzone des Reaktors entweichen und das gesamte Material der Umgebung durchdringen. Diese Teilchen üben keine merklichen Wirkungen aus, da sie mit Materie kaum reagieren. Ihre Energie kann daher auch nicht technisch genutzt werden. Die verbleibende, nutzbare Energie aus der Spaltung von 1 Gramm U-235 beträgt etwa 0,91 MWd (Megawatt-Tage) oder 21500 Kilowattstunden. Dies entspricht etwa 9,5 Tonnen Braunkohle oder 1,8 Tonnen Heizöl. 

Zusammengenommen erzeugen die rund 440 Kernreaktoren der derzeit 210 Kernkraftwerke, die es weltweit in 30 Ländern gibt, eine elektrische Leistung von etwa 370 Gigawatt. Dies ist ein Anteil von 15 % der gesamten elektrischen Energie weltweit (Stand: 2009).

Die Kettenreaktion besteht darin, dass Neutronen Atomkerne des Kernbrennstoffs spalten, wobei außer den energiereichen Spaltfragmenten auch jeweils einige neue Neutronen frei werden; diese können weitere Kerne spalten usw. Der Wirkungsquerschnitt der Kerne für Spaltung nimmt bei den meistgenutzten Brennstoffen mit abnehmender Energie, also abnehmender Geschwindigkeit des Neutrons zu: Je langsamer das Neutron ist, desto wahrscheinlicher ist es, dass es von einem spaltbaren Kern absorbiert wird und dieser sich anschließend spaltet. Daher bremst man in den meisten Reaktoren die schnellen Neutronen aus der Kernspaltung mittels eines Moderators ab. Dies ist ein Material wie etwa Graphit, schweres oder normales Wasser, das leichte Atomkerne (kleinere Massenzahl) enthält und einen sehr niedrigen Absorptionsquerschnitt für Neutronen hat. In diesem Material werden die Neutronen durch Stöße mit dessen Atomkernen stark abgebremst, aber nur selten absorbiert. Sie stehen also der Kettenreaktion weiter zur Verfügung. Die Neutronen können bis herunter auf die Geschwindigkeiten der Kerne des Moderators abgebremst werden; deren durchschnittliche Geschwindigkeit ist nach der Theorie der Brownschen Bewegung durch die Temperatur des Moderators gegeben. Es findet also eine "Thermalisierung" statt. Man spricht daher statt von abgebremsten meist von thermischen Neutronen, denn die Neutronen besitzen anschließend eine ähnliche thermische Energieverteilung wie die Moleküle des Moderators. Ein Reaktor, der zur Kernspaltung thermische Neutronen verwendet, wird als "Thermischer Reaktor" bezeichnet. Im Gegensatz dazu nutzt ein "schneller" Reaktor die nicht abgebremsten, schnellen Neutronen zur Spaltung (daher die Bezeichnung "Schneller Brüter").

Im abgeschalteten Zustand, d. h. bei eingefahrenen Steuerstäben, ist der Reaktor unterkritisch. Einige freie Neutronen sind zwar stets im Reaktor vorhanden – beispielsweise freigesetzt durch Spontanspaltung von Atomkernen des Kernbrennstoffs – und lösen zum Teil Spaltungen aus, aber das Anwachsen einer Kettenreaktion wird dadurch unterbunden, dass die meisten Neutronen von dem in den Steuerstäben enthaltenen Material (z. B. Bor) absorbiert werden, so dass der Multiplikationsfaktor "k" unter 1 liegt.

Zum Anfahren des Reaktors werden die Steuerstäbe unter ständiger Messung des Neutronenflusses mehr oder weniger weit aus dem Reaktorkern herausgezogen, bis leichte Überkritikalität durch verzögerte Neutronen, also eine selbsterhaltende Kettenreaktion mit allmählich zunehmender Kernreaktionsrate erreicht ist. Neutronenfluss und Wärmeleistung des Reaktors sind proportional zur Reaktionsrate und steigen daher mit ihr an. Mittels der Steuerstäbe – bei Druckwasserreaktoren auch über die Konzentration von Borsäure im Wasser – wird der Neutronenfluss auf das jeweils gewünschte Fluss- und damit Leistungsniveau im gerade kritischen Zustand eingeregelt und konstant gehalten; "k" ist dann gleich 1,0. Etwaige Änderungen von "k" durch Temperaturanstieg oder andere Einflüsse werden durch Verstellen der Steuerstäbe ausgeglichen. Dies geschieht bei praktisch allen Reaktoren durch eine automatische Steuerung, die auf den gemessenen Neutronenfluss reagiert.

Der Multiplikationsfaktor 1,0 bedeutet, dass durchschnittlich gerade eines der pro Kernspaltung freiwerdenden Neutronen eine weitere Kernspaltung auslöst. Alle übrigen Neutronen werden entweder absorbiert – teils unvermeidlich im Strukturmaterial (Stahl usw.) und in nicht spaltbaren Brennstoffbestandteilen, teils im Absorbermaterial der Steuerstäbe, meist Bor oder Cadmium – oder entweichen aus dem Reaktor nach außen (Leckage).

Zum Verringern der Leistung und zum Abschalten des Reaktors werden die Steuerstäbe eingefahren, wodurch er wieder unterkritisch wird. Der Multiplikationsfaktor sinkt auf Werte unter 1, die Reaktionsrate nimmt ab, und die Kettenreaktion endet.

Ein verzögert überkritischer Reaktor steigert seine Leistung langsam genug, dass die Regeleinrichtungen dem Vorgang folgen können. Falls die aktive Regelung bei wassermoderierten Reaktoren versagt, also die Kritikalität nicht auf 1 zurückgeregelt wird, steigert sich die Leistung über den Nennwert hinaus. Dabei erwärmt sich der Moderator und dehnt sich in der Folge aus oder verdampft. Da moderierendes Wasser jedoch notwendig ist, um die Kettenreaktion aufrechtzuerhalten, kehrt der Reaktor – sofern "nur" das Wasser verdampft, aber die räumliche Anordnung des Brennstoffs noch erhalten geblieben ist – in den unterkritischen Bereich zurück. Dieses Verhalten heißt eigenstabil. 

Dieses Verhalten gilt beispielsweise nicht für graphitmoderierte Reaktortypen, da Graphit auch bei zunehmender Temperatur seine moderierenden Eigenschaften behält. Gerät ein solcher Reaktor durch Versagen der Regelungssysteme in den verzögert überkritischen Bereich, so kommt die Kettenreaktion nicht zum Erliegen, und dies kann zur Überhitzung und ggf. Zerstörung des Reaktors führen. Ein solcher Reaktor ist also nicht eigenstabil. Die Reaktoren aus Tschernobyl gehörten zu dieser Bauweise, die heute nur noch in Russland vorhanden ist.

Im Gegensatz zum "verzögert" überkritischen Reaktor ist ein "prompt" überkritischer Reaktor nicht mehr regelbar, und es kann zu schweren Unfällen kommen. Der Neutronenfluss und damit die Wärmeleistung des Reaktors steigt exponentiell mit einer Verdoppelungszeit im Bereich von 10 Sekunden an. Die erreichte Leistung kann die Nennleistung während einiger Millisekunden um mehr als das Tausendfache übersteigen, bis sie durch die Dopplerverbreiterung im so erhitzten Brennstoff wieder gesenkt wird. Die Brennstäbe können durch diese Leistungsexkursion schnell auf Temperaturen über 1000 °C erhitzt werden. Je nach Bauart und den genauen Umständen des Unfalls kann dies zu schweren Schäden am Reaktor führen, vor allem durch schlagartig verdampfendes (Kühl-)Wasser. Beispiele für prompt überkritische Leichtwasserreaktoren und die Folgen zeigen die BORAX-Experimente oder der Unfall im US-Forschungsreaktor SL-1. Der bisher größte Unfall durch einen zumindest in Teilbereichen prompt überkritischen Reaktor war die Nuklearkatastrophe von Tschernobyl, bei der unmittelbar nach der Leistungsexkursion schlagartig verdampfende Flüssigkeiten, Metalle und der anschließende Graphitbrand zu einer weiträumigen Verteilung des radioaktiven Inventars geführt haben.

Die automatische Unterbrechung der Kettenreaktion bei einer Leistungsexkursion eines wassermoderierten Reaktors ist, anders als gelegentlich behauptet, kein Garant dafür, dass es nicht zu einer Kernschmelze kommt, denn bei zusätzlichem Versagen aller aktiven Kühleinrichtungen reicht die Nachzerfallswärme aus, um diese herbeizuführen. Aus diesem Grunde sind die Kühlsysteme redundant und diversitär ausgelegt. Eine Kernschmelze wird als Auslegungsstörfall seit dem Unfall in Three Mile Island bei der Planung von Kernkraftwerken berücksichtigt und ist prinzipiell beherrschbar. Wegen der durch die Leistungsexkursion eventuell veränderten geometrischen Anordnung des Reaktorkerns ist erneute Kritikalität allerdings nicht grundsätzlich auszuschließen.

Eine Kettenreaktion mit gleichbleibender Reaktionsrate kann auch in einem unterkritischen Reaktor erreicht werden, indem man freie Neutronen aus einer unabhängigen Neutronenquelle einspeist. Ein solches System wird manchmal als "getriebener" Reaktor bezeichnet. Wenn die Neutronenquelle auf einem Teilchenbeschleuniger beruht, also jederzeit abschaltbar ist, bietet das Prinzip verbesserte Sicherheit gegen Reaktivitätsstörfälle. Die Nachzerfallswärme (siehe unten) tritt hier jedoch ebenso wie beim kritisch arbeitenden Reaktor auf; Vorkehrungen zur Beherrschung von Kühlungsverlust-Störfällen sind hier also ebenso nötig wie bei den üblichen Reaktoren. 

Getriebene Reaktoren sind gelegentlich zu Versuchszwecken gebaut und betrieben worden. Sie werden auch als Großanlagen zur Energiegewinnung und gleichzeitigen Transmutation von Reaktorabfall (siehe Accelerator Driven System) entworfen und in diesem Fall manchmal als "Hybridreaktoren" bezeichnet. In ihnen könnten auch die in Reaktoren entstehenden schwereren Actinoide, deren Generationenfaktor für eine kritische Kettenreaktion zu klein ist, als Kernbrennstoffe genutzt werden.

Durch einen Fortluft-Kamin und das Abwasser werden auch im Normalbetrieb ständig entstehende, radioaktive Verunreinigungen (Tritium, radioaktives Jod, etc. pp.) in die Umgebung geleitet.
Diesbezüglich wird vermutet, dass angebliche Häufungen von Krebs-Fallzahlen ursächlich mit diesen Emissionen zusammenhängen.

Wird ein Reaktor abgeschaltet, so wird durch den radioaktiven Zerfall der Spaltprodukte weiterhin Wärme produziert. Die Leistung dieser so genannten "Nachzerfallswärme" entspricht anfänglich etwa 5–10 % der thermischen Leistung des Reaktors im Normalbetrieb und klingt in einem Zeitraum von einigen Tagen größtenteils ab. Häufig wird dafür der Begriff "Restwärme" verwendet, welcher aber irreführend ist, denn es handelt sich nicht um die verbleibende aktuelle Hitze des Reaktorkerns, sondern um zusätzliche Wärmeproduktion, die durch die weiterlaufenden Zerfallsreaktionen hervorgerufen wird.

Um die Nachzerfallswärme in Notfällen (bei ausgefallenem Hauptkühlsystem) sicher abführen zu können, besitzen alle Kernkraftwerke ein aufwändiges "Not- und Nachkühlsystem". Sollten jedoch auch diese Systeme versagen, kann es durch die steigenden Temperaturen zu einer Kernschmelze kommen, bei der Strukturteile des Reaktorkerns und unter Umständen Teile des Kernbrennstoffs schmelzen. Dies war der Fall bei den Kernschmelzen in Fukushima, da dort bedingt durch einen kompletten Ausfall der Stromversorgung sämtliche aktiven Kühlsysteme zum Erliegen kamen.

Wenn Brennstäbe niederschmelzen und dadurch eine Zusammenballung von Brennstoff entsteht, nimmt der Multiplikationsfaktor zu, und es kann zu einer schnellen unkontrollierten Aufheizung kommen. Um diesen Prozess zu verhindern oder wenigstens zu verzögern, werden in einigen Reaktoren die im Reaktorkern verarbeiteten Materialien so gewählt, dass ihr Neutronen-Absorptionsvermögen mit steigender Temperatur anwächst, die Reaktivität also abnimmt. Bei Leichtwasserreaktoren, die heute fast 90 % der installierten Leistung darstellen, ist eine Kernschmelze im Betrieb nicht möglich, da die Kernspaltungskettenreaktion nur in Anwesenheit von Wasser stattfindet. Eine Kernschmelze ist jedoch bei mangelnder Kühlung im ausgeschalteten Reaktor aufgrund der Nachzerfallswärme möglich, wenn auch über längere Zeitskalen. Der Fall der Kernschmelze wird als "größter anzunehmender Unfall" (GAU) betrachtet, also als der schwerste Unfall, der bei der Auslegung der Anlage in Betracht zu ziehen ist und dem sie ohne Schäden für die Umgebung standhalten muss. Solch ein Unfall ereignete sich beispielsweise im Kernkraftwerk Three Mile Island.

Den schlimmsten Fall, dass zum Beispiel das Reaktorgebäude nicht standhält und eine größere, die zulässigen Grenzwerte weit überschreitende Menge radioaktiver Stoffe austritt, bezeichnet man als Super-GAU. Dies geschah zum Beispiel 1986 bei der Katastrophe von Tschernobyl und 2011 bei der Katastrophe von Fukushima.

Als inhärent sicher gegen Kernschmelzen gelten beim derzeitigen Stand der Technik nur bestimmte Hochtemperaturreaktoren geringerer Leistung und Leistungsdichte; ganz allgemein inhärent sicher ist aber auch dieser Reaktortyp nicht, da Unfälle wie Graphitbrand oder Wassereinbruch katastrophale Folgen haben könnten. 

Die Leistungsdichte in MW/m³ (Megawatt thermischer Leistung pro Kubikmeter Reaktorkern) bestimmt, welche technischen Vorsorgen getroffen werden müssen, um nach einer Schnellabschaltung die anfallende Nachzerfallswärme abzuführen. Typische Leistungsdichten sind für gasgekühlte Hochtemperaturreaktoren 6 MW/m³, für Siedewasserreaktoren 50 MW/m³ und für Druckwasserreaktoren 100 MW/m³.

Der Europäische Druckwasserreaktor (EPR) hat unterhalb des Druckbehälters zur Sicherheit für den Fall einer Kernschmelze ein besonders geformtes Keramikbecken, den "Core-Catcher". In diesem soll das geschmolzene Material des Reaktorkerns aufgefangen, aber an einer Zusammenballung gehindert und durch eine spezielle Kühlung abgekühlt werden.

Die ersten Versuchsreaktoren waren simple Aufschichtungen von spaltbarem Material. Ein Beispiel dafür ist der Reaktor Chicago Pile, in dem die erste kontrollierte Kernspaltung stattfand. Moderne Reaktoren werden nach der Art der Kühlung, der Moderation, des verwendeten Brennstoffs und der Bauweise unterteilt.

Mit normalem leichtem Wasser moderierte Reaktionen finden im Leichtwasserreaktor (LWR) statt, der als "Siedewasserreaktor" (SWR) oder "Druckwasserreaktor" (DWR) ausgelegt sein kann. Leichtwasserreaktoren erzeugen fast 90 % der Kernenergie weltweit (68 % DWR, 20 % SWR) und 100 % in Deutschland. Eine Weiterentwicklung des Vor-Konvoi, Konvoi (die deutschen DWR) und des N4 ist der Europäische Druckwasserreaktor (EPR). Ein russischer Druckwasserreaktor ist der "WWER". Leichtwasserreaktoren benötigen angereichertes Uran, Plutonium oder Mischoxide (MOX) als Brennstoff. Ein Leichtwasserreaktor war auch der Naturreaktor Oklo.

Wesentliches Merkmal des Leichtwasserreaktors ist der negative Dampfblasenkoeffizient: Da Wasser gleichzeitig Kühlmittel und Moderator ist, ist ohne Wasser keine Kettenreaktion möglich, also auch keine Kernschmelze beim Reaktorbetrieb. 

Die Brennelemente des LWR sind empfindlich gegenüber thermodynamischen und mechanischen Belastungen. Um diese zu vermeiden, sind ausgeklügelte, technische und betriebliche Schutzmaßnahmen erforderlich, welche die Auslegung des Kernkraftwerkes in Gänze prägen. Gleiches gilt für den Reaktordruckbehälter mit seinem Risiko des Berstens. Die verbleibenden Restrisiken der Kernschmelze der Brennelemente aufgrund der Nachzerfallswärme und des Berstens des Reaktordruckbehälters wurden in der Kernenergiewirtschaft wegen der Unwahrscheinlichkeit ihres Eintretens lange Zeit als irrelevant erklärt, zum Beispiel von Heinrich Mandel.

Mit schwerem Wasser moderierte "Schwerwasserreaktoren" erfordern eine große Menge des teuren schweren Wassers, können aber mit natürlichem, nicht angereichertem Uran betrieben werden. Der bekannteste Vertreter dieses Typs ist der in Kanada entwickelte CANDU-Reaktor.

Gasgekühlte graphitmoderierte Reaktoren wurden bereits in den 1950er-Jahren entwickelt, zunächst primär für militärische Zwecke (Plutoniumproduktion). Sie sind die ältesten kommerziell genutzten Kernreaktoren; das Kühlmittel ist in diesem Fall Kohlenstoffdioxid. In Großbritannien ist (2011) noch eine Reihe dieser Anlagen in Betrieb.
Wegen der aus einer Magnesiumlegierung hergestellten Brennstabhülle heißt dieser Reaktortyp "Magnox-Reaktor". Ähnliche Anlagen wurden auch in Frankreich betrieben, sind aber inzwischen alle abgeschaltet. 

Am 17. Oktober 1969 schmolzen kurz nach Inbetriebnahme des Reaktors 50 kg Brennstoff im gasgekühlten Graphitreaktor des französischen Kernkraftwerks Saint-Laurent A1 (450 MW). Der Reaktor wurde daraufhin 1969 stillgelegt (die heutigen Reaktoren des Kernkraftwerks sind Druckwasserreaktoren).

Ein Nachfolger der Magnox-Reaktoren ist der in Großbritannien entwickelte "Advanced Gas-cooled Reactor" (AGR). Im Unterschied zu den Magnox-Reaktoren verwendet er leicht angereichertes Urandioxid statt Uranmetall als Brennstoff. Dies ermöglicht höhere Leistungsdichten und Kühlmittelaustrittstemperaturen und damit einen besseren thermischen Wirkungsgrad. AGR haben mit 42 % den höchsten Wirkungsgrad aller bisherigen Kernkraftwerke erzielt.

"Hochtemperaturreaktoren" (HTR) nutzen ebenfalls Graphit als Moderator; als Kühlmittel wird Helium-Gas verwendet. Eine mögliche Bauform des Hochtemperaturreaktors ist der "Kugelhaufenreaktor" nach Farrington Daniels und Rudolf Schulten, bei dem der Brennstoff vollständig in Graphit eingeschlossen ist. Dieser Reaktortyp galt lange als einer der sichersten, da hier bei einem Versagen der Not- und Nachkühlsysteme eine Kernschmelze aufgrund des hohen Schmelzpunktes des Graphits unmöglich ist. Allerdings gibt es eine Reihe anderer schwerwiegender Unfalltypen wie Wassereinbruch oder Lufteinbruch mit Graphitbrand, welche die behaupteten Sicherheitsvorteile in Frage stellen, wie Rainer Moormann herausstellte, der dafür den Whistleblowerpreis 2011 erhielt. Auch eine Reihe ungelöster praktischer Probleme hat die kommerzielle Umsetzung des Konzepts verhindert. Hinzu kommt, dass die Anlagekosten des HTR höher als die des Leichtwasserreaktors sind. In Deutschland forschte man am Versuchskernkraftwerk AVR (Jülich) und baute das Prototypkraftwerk THTR-300 in Schmehausen, letzteres mit einem Reaktordruckbehälter aus Spannbeton. Beide wurden 1989 stillgelegt.

Die sowjetischen Reaktoren vom Typ "RBMK" nutzen ebenfalls Graphit als Moderator, jedoch leichtes Wasser als Kühlmittel. Hier liegt der Graphit in Blöcken vor, durch die zahlreiche Kanäle gebohrt sind, in denen sich Druckröhren mit den Brennelementen und der Wasserkühlung befinden. Dieser Reaktortyp ist träge (man braucht viel Zeit zum Regeln) und unsicherer als andere Typen, da der Dampfblasenkoeffizient positiv ist: Anders als bei Leichtwasserreaktoren bedeutet ein Kühlmittelverlust hier nicht Moderatorverlust, verringert aber die Neutronenabsorption durch das Kühlmittel; er erhöht also die Reaktivität, statt sie zu verringern. Die dadurch erhöhte Wärmeleistung ohne genügende Kühlung kann schnell zur Kernschmelze führen. Der havarierte Reaktor in Tschernobyl war von diesem Typ. Reaktoren dieser Art sind heutzutage nur noch in Russland zu finden.

Weiterhin gibt es Brutreaktoren ("Schnelle Brüter"), in denen zusätzlich zur Energiefreisetzung U so in Pu umgewandelt wird, so dass mehr neues Spaltmaterial entsteht als zugleich verbraucht wird. Diese Technologie ist auch sicherheitstechnisch anspruchsvoller als die der anderen Typen. Ihr Vorteil ist, dass mit ihr die Uranvorräte der Erde bis zu 50-100 mal besser ausgenutzt werden können als wenn nur das U „verbrannt“ wird. Brutreaktoren arbeiten mit schnellen Neutronen und verwenden flüssiges Metall wie beispielsweise Natrium als Kühlmittel.

Kleinere nicht brütende Reaktoren mit Flüssigmetallkühlung (Blei-Bismut-Legierung) wurden in sowjetischen U-Booten eingesetzt.

In einem Flüssigsalzreaktor () wird eine Salzschmelze, die den Kernbrennstoff (beispielsweise Thorium und Uran) enthält, in einem Kreislauf umgewälzt. Die Schmelze ist gleichzeitig Brennstoff und Kühlmittel. Dieser Reaktortyp ist jedoch nicht über das Experimentierstadium hinausgekommen.

Zugunsten von Flüssigsalzreaktoren sind verschiedene Sicherheits- und Nachhaltigkeitsargumente vorgebracht worden: Die verwendeten Fluoridsalze sind nicht wasserlöslich, was eine Kontamination der Umgebung bei Unfällen erschwert. Als Brutreaktoren können die Flüssigsalzreaktoren den Brennstoff sehr effizient verwenden, sowie mit einem breiten Spektrum an Brennstoffen betrieben werden. Diese Reaktoren wurden in den 60er Jahren in den USA für den Antrieb für Flugzeuge erforscht. Die Entwicklung wurde etwa 1975 aufgegeben, vor allem wegen Korrosionsproblemen. Erst in den 2000er Jahren wurde das Konzept wieder aufgegriffen, u. a. auch in den "Generation-IV"-Konzepten.

Es gibt weiterhin einige Sondertypen für spezielle Anwendungen. So wurden kleine Reaktoren mit hochangereichertem Brennstoff für die Stromversorgung von Raumflugkörpern konstruiert, die ohne flüssiges Kühlmittel auskommen. Diese Reaktoren sind nicht mit den Isotopenbatterien zu verwechseln. Auch luftgekühlte Reaktoren, die stets hochangereicherten Brennstoff erfordern, wurden gebaut, zum Beispiel für physikalische Versuche im BREN-Tower in Nevada.
Es wurden Reaktoren für den Antrieb von Raumfahrzeugen konstruiert, bei denen flüssiger Wasserstoff zur Kühlung des Brennstoffes dient. Allerdings kamen diese Arbeiten über Bodentests nicht hinaus (Projekt NERVA, Projekt Timberwind). Ebenfalls nicht über das Versuchsstadium hinaus kamen Reaktoren, bei denen der Brennstoff in gasförmiger Form vorliegt (Gaskernreaktor).

Derzeit wird weltweit aktiv an neuen Reaktorkonzepten gearbeitet, den "Generation-IV"-Konzepten, insbesondere mit Blick auf den erwarteten wachsenden Energiebedarf. Diese sollen besondere Kriterien von Nachhaltigkeit, Sicherheit und Wirtschaftlichkeit erfüllen. Insbesondere wird durch Brutreaktoren eine deutlich höhere Effizienz in der Ausnutzung vom Brennstoff erzielt und eine geringere Menge an radioaktivem Abfall. Das Risiko der Kernschmelze durch die Nachzerfallswärme wird mit einer ausreichend starken passiven Kühlung auf Null reduziert. Die ersten Gen-IV-Reaktoren sollen ab 2030 zum Einsatz kommen.

Ein weiterer, zurzeit noch im Experimentalstadium befindlicher Reaktortyp ist der Laufwellen-Reaktor. Dieses Konzept verspricht, sofern die Umsetzung gelingen sollte, eine vielfach effizientere Nutzung des Kernbrennstoffs sowie die massive Reduzierung der Problematik des radioaktiven Abfalls, da ein Laufwellen-Reaktor mit radioaktivem Abfall betrieben werden könnte und diesen dabei systematisch aufbrauchen würde.

Eine Kernspaltungs-Kettenreaktion kann nicht nur durch komplexe technische Systeme erreicht werden, sondern kam unter bestimmten – wenn auch seltenen – Umständen auch in der Natur vor. 1972 entdeckten französische Forscher in der Region Oklo des westafrikanischen Landes Gabun die Überreste des natürlichen Kernreaktors Oklo, der vor etwa zwei Milliarden Jahren, im Proterozoikum, durch Naturvorgänge entstanden war. Insgesamt wurden bisher in Oklo und einer benachbarten Uranlagerstätte Beweise für frühere Spaltungsreaktionen an 17 Stellen gefunden.

Eine Voraussetzung für das Zustandekommen der natürlich abgelaufenen Spaltungs-Kettenreaktionen war der im Erdaltertum sehr viel höhere natürliche Anteil an spaltbarem U im Uran, er betrug damals ca. 3 %. Auf Grund der kürzeren Halbwertszeit von U gegenüber U beträgt der natürliche Gehalt von U im Uran derzeit nur noch etwa 0,7 %. Bei diesem geringen Gehalt an spaltbarem Material können neue kritische Spaltungs-Kettenreaktionen auf der Erde nicht mehr natürlich vorkommen.

Ausgangspunkt für die Entdeckung des Oklo-Reaktors war die Beobachtung, dass das Uranerz aus der Oklo-Mine einen geringfügig kleineren Gehalt des Isotops Uran-235 als erwartet aufwies. Die Wissenschaftler bestimmten daraufhin die Mengen verschiedener Edelgasisotope, die in einer Materialprobe der Oklo-Mine eingeschlossenen waren, mit einem Massenspektrometer. Aus der Verteilung der verschiedenen bei der Uranspaltung entstehenden Xenonisotope in der Probe ergab sich, dass die Reaktion in Pulsen abgelaufen ist. Der ursprüngliche Urangehalt des Gesteins führte mit der Moderatorwirkung des in den Spalten des Urangesteins vorhandenen Wassers zur Kritikalität. Die dadurch freigesetzte Wärme im Urangestein erhitzte das Wasser in den Spalten, bis es schließlich verdampfte und nach Art eines Geysirs entwich. Infolgedessen konnte das Wasser nicht mehr als Moderator wirken, so dass die Kernreaktion zum Erliegen kam (Ruhephase). Daraufhin sank die Temperatur wieder ab, so dass frisches Wasser einsickern und die Spalten wieder auffüllen konnte. Dies schuf die Voraussetzung für erneute Kritikalität, und der Zyklus konnte von vorne beginnen. Berechnungen zeigen, dass auf die etwa 30 Minuten dauernde aktive Phase (Leistungserzeugung) eine Ruhephase folgte, die mehr als zwei Stunden anhielt. Auf diese Weise wurde die natürliche Kernspaltung für etwa 500.000 Jahre in Gang gehalten, wobei über 5 Tonnen Uran-235 verbraucht wurden. Die Leistung des Reaktors lag bei im Vergleich zu den heutigen Megawatt-Reaktoren geringen 100 Kilowatt.

Bedeutsam ist der Naturreaktor von Oklo auch für die Beurteilung der Sicherheit von Endlagerungen für Radionuklide (Atommüll). Die dort beobachtete sehr geringe Migration einiger Spaltprodukte und des erbrüteten Plutoniums über Milliarden Jahre hinweg wurden von Kernenergiebefürwortern so interpretiert, dass atomare Endlager existieren können, die über lange Zeiträume hinreichend sicher sind.

Die meisten Kernreaktoren dienen der Erzeugung von elektrischer (selten: nur thermischer) Energie in Kernkraftwerken. Daneben werden Kernreaktoren auch zur Erzeugung von Radionukliden zum Beispiel für die Nutzung in Radioisotopengeneratoren oder in der Nuklearmedizin verwendet. Dabei werden die gesuchten Nuklide 
Theoretisch könnte man in einem Reaktor auch Gold herstellen (Goldsynthese), was allerdings sehr unwirtschaftlich wäre.

Die wichtigste im Reaktor stattfindende Stoffumwandlungs-Reaktion (neben der Erzeugung von Spaltprodukten) ist die Erbrütung (siehe oben) von Plutonium-239 aus Uran-238, dem häufigsten Uranisotop. Sie erfolgt unvermeidlich in jedem mit Uran betriebenen Reaktor. Es gibt aber speziell dafür optimierte militärische Reaktoren, die insbesondere auf die Entnahme des Brennstoffs nach nur kurzem Betrieb eingerichtet sind, so dass Pu mit nur geringem Gehalt an Pu verfügbar wird.

Kernreaktoren dienen auch als intensive regulierbare Neutronenquellen für physikalische Untersuchungen aller Art. Weitere Anwendungen sind der Antrieb von Fahrzeugen (Kernenergieantrieb) und die Energieversorgung mancher Raumflugkörpern.

Das von Kernreaktoren ausgehende Gefahrenpotenzial sowie die bislang ungelöste Frage der Lagerung der anfallenden radioaktiven Abfälle haben nach Jahren der Euphorie seit den 1970er-Jahren in vielen Ländern zu Protesten von Atomkraftgegnern und zu einer Neubewertung der Kernenergie geführt. Während in den 1990er-Jahren vor allem in Deutschland der Ausstieg aus der Kernenergie propagiert wurde, fand etwa 2000 bis 2010 vor dem Hintergrund der verblassenden Erinnerungen an die Risiken (die Katastrophe von Tschernobyl lag 20 Jahre zurück) ein Versuch statt, die Atomkraft wieder gesellschaftsfähig zu machen. Anlass ist die durch internationale Verträge geforderte Reduktion des CO-Ausstoßes bei der Verbrennung fossiler Energieträger. Dem steht ein wachsender Energiebedarf aufstrebender Volkswirtschaften wie etwa China gegenüber.

Aus diesen Gründen entschlossen sich einige europäische Staaten, in neue Kernkraftwerke zu investieren. So bauen der deutsche Konzern Siemens und die französische Gruppe Areva einen Druckwasserreaktor vom Typ EPR im finnischen Olkiluoto, der 2018 ans Netz gehen soll. Russland will seine alten und teilweise maroden Kernkraftwerke erneuern und mindestens zehn Jahre lang pro Jahr einen neuen Reaktorbau beginnen. In Frankreich wird ebenfalls über den Neubau eines Reaktors verhandelt. Schweden stoppte seine Pläne zum Atomausstieg. Daneben gibt es kleinere und größere Neubauprojekte im Iran, der Volksrepublik China, Indien, Nordkorea, Türkei und anderen Staaten. (Hauptartikel: Kernenergie nach Ländern). Außerdem sind viele Länder im Forschungsverbund Generation IV International Forum bei der Entwicklung von sechs neue Reaktortypen, die höhere Nachhaltigkeit, Sicherheit und Wirtschaftlichkeit garantieren sollen.

Die atomaren Unfälle in dem japanischen Kraftwerk Fukushima-Daiichi in der Folge des Magnitude-9-Erdbebens und darauffolgenden Tsunami vom 11. März 2011 brachten hierzu fast überall neue Überlegungen in Gang. Anders als beim Unfall in Tschernobyl, bei dem eine völlig veraltete und bekanntlich gefährliche Reaktorart verwendet wurde, zeigten die Unfälle in Fukushima eine Schwäche von Leichtwasserreaktoren, die heute gängigste Bauart.

Die Lebensdauer von Kernreaktoren ist nicht unbegrenzt. Besonders der Reaktordruckbehälter ist ständiger Neutronenstrahlung ausgesetzt, die zur Versprödung des Materials führt. Wie schnell das geschieht, hängt unter anderem davon ab, wie die Brennelemente im Reaktor angeordnet sind und welchen Abstand sie zum Reaktordruckbehälter haben. Die Kernkraftwerke Stade und Obrigheim wurden auch deshalb als erste vom Netz genommen, weil hier dieser Abstand geringer war als bei anderen, neueren Kernreaktoren. Zurzeit versuchen die Betreiber von Kernkraftwerken, durch eine geschickte Beladung mit Brennelementen und zusätzliche Moderatorstäbe die Neutronenbelastung des Reaktordruckbehälters zu reduzieren. Unter anderem das Helmholtz-Zentrum Dresden-Rossendorf erforscht diese Problematik.





</doc>
<doc id="14523" url="https://de.wikipedia.org/wiki?curid=14523" title="Summe">
Summe

Eine Summe ist in der Mathematik das Ergebnis einer Addition. Im einfachsten Fall ist eine Summe also eine Zahl, die durch Addition zweier oder mehrerer Zahlen entsteht. Dieser Begriff besitzt viele Verallgemeinerungen. So sprach man früher beispielsweise von summierbaren Funktionen und meinte damit integrierbare Funktionen.

Das Wort "Summe" wurde im Mittelhochdeutschen von lateinisch "summa" entlehnt. "Summa" war bis in das 19. Jahrhundert neben "Summe" gebräuchlich und geht auf "summus" zurück, einen der lat. Superlative zu "superus" „oberhalb befindlich, der/die/das Höhere/Obere“, die folglich „der/die/das Höchste/Oberste“ bedeuten. „Das Oberste“ deshalb, weil die Römer die Summe in der obersten Zeile, also über den Summanden, zu notieren pflegten und nicht, wie heute üblich, „unterm Strich“.

In der Alltagssprache bezeichnet "Summe" einen Geldbetrag, unabhängig davon, ob er durch Addition zustande gekommen ist oder nicht.

In dem mathematischen Term
heißen die Zahlen 2 und 3 Summanden. Der gesamte Term formula_1 wird ebenso wie das Ergebnis 5 als die „Summe von 2 und 3“ bezeichnet.

Man kann eine Summe mit mehr als zwei Summanden bilden, so zum Beispiel formula_3. Eine häufige Konvention ist dabei, bei Linksklammerung die Klammern einfach wegzulassen, also formula_3 einfach mit formula_5 abzukürzen. Aufgrund der Assoziativität der Addition von natürlichen Zahlen spielt es hier übrigens für das Ergebnis keine Rolle, in welcher Reihenfolge die Additionen auszuführen sind. So gilt:
Mit dem Gleichheitszeichen wird dabei die Gleichheit der Ergebnisse der beiden unterschiedlichen Terme ausgedrückt.

Aufgrund des Kommutativgesetzes der Addition von natürlichen Zahlen ist auch die Reihenfolge der Summanden irrelevant, zum Beispiel gilt:

Wird formula_8-mal die gleiche Zahl formula_9 addiert, dann kann die Summe auch als Produkt formula_10 geschrieben werden.

In einigen Fällen werden die einzelnen Summanden nicht einfach addiert, sondern zuvor noch mit einem Gewicht multipliziert:

Zum Beispiel:

In diesem Fall spricht man von einer gewichteten Summe. Teilt man die gewichtete Summe durch die Summe der Gewichte, erhält man das gewichtete arithmetische Mittel.

Wenn eine Summe sehr viele Summanden hat, ist es zweckmäßig, eine abgekürzte Schreibweise zu vereinbaren. Die Summe der ersten 100 natürlichen Zahlen kann zum Beispiel als
angegeben werden, denn es ist leicht zu erraten, welche Summanden durch die Auslassungspunkte ersetzt wurden.

So wie man in der elementaren Arithmetik von Zahlenrechnungen wie formula_14 zu Buchstabenrechnungen wie formula_15 übergeht, kann man z. B. auch die Summe von hundert ganz bestimmten Zahlen zur Summe einer beliebigen Anzahl beliebiger Zahlen verallgemeinern. Dazu wird zunächst eine Variable gewählt, zum Beispiel formula_8, die die Anzahl der Summanden bezeichnet. Im obigen Fall, der Summe der ersten einhundert natürlichen Zahlen, wäre formula_17. Da beliebig große formula_8 zugelassen sein sollen, ist es nicht möglich, alle formula_8 Summanden mit formula_8 verschiedenen Buchstaben zu bezeichnen. Stattdessen wird ein einzelner Buchstabe, z. B. formula_9, gewählt und um einen Index ergänzt. Dieser Index nimmt nacheinander die Werte formula_22 an. Die Summanden heißen dementsprechend formula_23. Sie bilden somit eine Zahlenfolge.

Wir können nun für beliebige natürliche Zahlen formula_8 die Summe der ersten formula_8 Glieder der Zahlenfolge als
schreiben. Wenn man für formula_8 verschiedene Werte formula_22 einsetzt, bilden die formula_29 ihrerseits ebenfalls eine Folge. Eine solche Folge von Partialsummen über die Anfangsglieder einer Folge wird als Reihe bezeichnet.

"Beispiel:" Für die Folge der Quadratzahlen ist formula_30, formula_31, formula_32. Ganz allgemein gilt:

Die Reihe der Partialsummen dieser Folge beginnt mit formula_34, formula_35, formula_36. Eine Summationsformel besagt nun für beliebige formula_8:

Weitere Summationsformeln wie zum Beispiel "Der kleine Gauß"

finden sich in der Formelsammlung Arithmetik. Der Beweis solcher Formeln kann oft mittels vollständiger Induktion erfolgen.

Summen über endliche oder unendliche Folgen können statt mit Auslassungspunkten auch mit dem Summenzeichen notiert werden:

Das Summenzeichen besteht aus dem großen griechischen Buchstaben Σ (Sigma), gefolgt von "einem" Folgenglied, das durch einen zuvor nicht benutzten Index (hier formula_41) bezeichnet wird. Dieser Index wird oft als Laufindex oder Summationsvariable bzw. "Lauf- oder Zählvariable" bezeichnet. Hierfür wird oft einer der Buchstaben formula_42 verwendet. Wenn nicht eindeutig hervorgeht, welche Variable die Zählvariable ist, muss dies im Text angemerkt werden.

Welche Werte die Laufvariable annehmen kann, wird an der Unterseite, gegebenenfalls auch der Oberseite des Zeichens Σ angezeigt. Es gibt dafür zwei Möglichkeiten:

Diese Angaben können reduziert oder weggelassen werden, wenn angenommen werden kann, dass der Leser sie aus dem Kontext heraus zu ergänzen vermag. Hiervon wird in bestimmten Zusammenhängen ausführlich Gebrauch gemacht: In der Tensorrechnung vereinbart man häufig die einsteinsche Summenkonvention, der zufolge sogar das Summationszeichen weggelassen werden kann, da aus dem Kontext klar ist, dass über alle doppelt vorkommenden Indizes zu summieren ist. Hier eine Animation zur Sigma-Schreibweise:

Sei formula_48 eine (Index-) Menge, formula_49 ein kommutatives Monoid.
Für jedes formula_50 sei ein formula_51 gegeben.
Dann kann formula_52 zumindest für endliche Indexmengen durch Rekursion definiert werden:
Man setzt
und ansonsten
nach Wahl eines beliebigen Elementes formula_55. Kommutativität und Assoziativität der Addition in formula_49 garantieren, dass dies wohldefiniert ist.

Die Schreibweise formula_57 mit formula_58 ist in diesem Sinne nur eine Abkürzung für formula_59 mit formula_60.

Falls formula_48 unendlich ist, ist formula_62 allgemein nur definiert, falls formula_63 für fast alle formula_64 gilt. In diesem Fall setzt man

Rechts steht nach Voraussetzung eine endliche Indexmenge, also eine wie oben definierte Summe.
Sind unendlich viele formula_66 ungleich 0, dann handelt es sich trotz gleichartiger Schreibweise nicht mehr um eine Summe, sondern eine Reihe (siehe unten).

Wird das Folgeglied als Summe (oder Differenz) mitgeteilt, so muss es in Klammern geschrieben werden:

Wird das Folgeglied als Produkt (oder Quotient) mitgeteilt, so ist die Klammer überflüssig:

Vorsicht: Allgemein gilt:
formula_69

Für formula_70 besteht die Summe aus einem einzigen Summanden formula_71:

Für formula_73 hat man eine sog. leere Summe, die gleich 0 ist, da die Indexmenge formula_74 leer ist:

Ist das Folgeglied konstant (genauer: unabhängig von der Laufvariablen), kann die Summe zu einem einfachen Produkt umgeschrieben werden (sofern formula_76):

Auch über Summen kann wieder summiert werden. Das ist insbesondere dann sinnvoll, wenn die erste, die „innere“ Summe, einen Index enthält, der als "Laufindex" für die „äußere“ Summe verwendet werden kann. Man schreibt zum Beispiel:

Dabei gilt die Regel: formula_79

In der mathematischen Physik gilt für Doppelsummen zudem folgende Konvention:

Ein Apostroph am Summenzeichen besagt, dass bei der Summation Summanden auszulassen sind, für die die beiden Laufvariablen übereinstimmen:

Wenn unendlich viele Ausdrücke summiert werden, also zum Beispiel
mit (abzählbar) unendlich vielen Summanden ungleich null, müssen Methoden der Analysis angewendet werden, um den entsprechenden Grenzwert
zu finden, falls er existiert. Eine solche Summe wird "unendliche Reihe" genannt. Als Obergrenze schreibt man das Symbol formula_83 für Unendlichkeit.

Wichtige Unterschiede zwischen Reihen und echten Summen sind beispielsweise:

Es ist aber anzumerken, dass nicht jede Summe, die formula_83 als Obergrenze besitzt, eine unendliche Summe sein muss. Zum Beispiel hat die Summe

für Primzahlen formula_89 und mit der Ganzzahl-Funktion formula_90 zwar unendlich viele Summanden, aber nur endlich viele sind ungleich null. (Diese Summe gibt an, wie oft der Faktor formula_89 in der Primfaktorzerlegung von formula_92 vorkommt.)



</doc>
<doc id="14525" url="https://de.wikipedia.org/wiki?curid=14525" title="Präzession">
Präzession

Die Präzession bezeichnet die Richtungsänderung, die die Rotationsachse eines rotierenden Körpers (Kreisel) ausführt, wenn eine äußere Kraft ein Drehmoment senkrecht zu dieser Achse ausübt. Dabei beschreibt die Rotationsachse einen Umlauf auf dem Mantel eines gedachten Kegels mit fester Kegelachse. Anschaulich zeigt sich die Präzession beim Tischkreisel, der trotz Schiefstellung nicht umkippt, solange er rotiert.

Speziell in der Astronomie ist mit Präzession die Richtungsänderung der Erdachse gemeint, die eine Folge der Massenanziehung des Mondes und der Sonne in Verbindung mit der Abweichung der Erdfigur von der Kugelform ist. Sie äußert sich durch das Fortschreiten des Frühlingspunkts entlang der Ekliptik, woraus sich auch die Bezeichnung "Präzession" herleitet.

Wenn beim rotierenden Kreisel versucht wird, seine Rotationsachse zu kippen, dann zeigt sich eine Kraftwirkung senkrecht zur Kipprichtung der Rotationsachse. Je schneller der Kreisel rotiert, desto größer sind die auftretenden Kräfte. Erklären lässt sich das mit dem hohen Drehimpuls des Kreisels, der in seiner Richtung geändert werden muss. Dessen Änderung erfolgt in der Richtung, in der die Rotationsachse gekippt wird, und erfordert ein Drehmoment, das in der Kippebene liegt. Das aufzubringende Drehmoment bedingt die Kraftwirkung senkrecht zur Kipprichtung.

Es sei ein rotierender Tischkreisel angenommen, der schräg steht. Aufgrund seiner Masse wirkt auf den Schwerpunkt des Kreisels seine Gewichtskraft und eine gleich große entgegengerichtete Kraft am Auflagepunkt. Das daraus resultierende Drehmoment
ließe einen nicht rotierenden Kreisel umkippen.
Dabei gibt "α" den Winkel zwischen Rotationsachse und Schwerkraft an, "r" ist der Abstand zwischen Auflagepunkt und Schwerpunkt des Kreisel sowie "m" die Masse und "g" die Erdbeschleunigung.

Es ist bekannt, dass schief stehende Kreisel den charakteristischen Präzessionskegel mit der Kegelachse längs der Schwerkraft überstreichen. Daher sei eine Winkelgeschwindigkeit formula_2 angenommen, mit der die Rotationsachse des Kreisels geschwenkt wird und wodurch das Kreiselmoment formula_3 auftritt. Diese Winkelgeschwindigkeit sei nun längs der Schwerkraft ausgerichtet und soll einen Betrag aufweisen, dass sie das Drehmoment, wegen dessen der Kreisel kippt, aufhebt. formula_4 gibt den Drehimpuls des Kreisels an.
Das Kreiselmoment liegt in der Ebene senkrecht zur Schwerkraft und zeigt in entgegengesetzte Richtung wie das Drehmoment, das den Kreisel kippt. Durch Überführung des Kreuzprodukts in die Betragsschreibweise ergibt sich das Kreiselmoment im Betrag und lässt sich mit dem Drehmoment aus der Gewichtskraft gleichsetzen. Durch Umstellen folgt aus den Kreiseldaten die Winkelgeschwindigkeit der Präzessionsbewegung.

Dabei stellt "I" das Trägheitsmoment dar und "ω" die Winkelgeschwindigkeit des Kreisels. Das Kreiselmoment ist eine Näherungsformel für formula_7 und so auch die resultierende Formel.

Die resultierende Winkeländerung pro Zeit wird bei der Rotation der Erde als Präzessionskonstante bezeichnet.

Die Erde hat keine exakte Kugelform, sondern infolge ihrer Rotation annähernd die Form eines abgeplatteten Ellipsoids. Der äquatoriale Halbmesser ist um rund ein Dreihundertstel oder 21,4 km größer als die Entfernung der Pole vom Erdmittelpunkt. Der „Äquatorwulst“ (englisch "equatorial bulge"), durch den das Erdellipsoid sich von einer Kugel unterscheidet, bewirkt, dass die Gezeitenkräfte von Mond und Sonne ein Drehmoment erzeugen, das die Erdachse aufzurichten versucht und zur Präzession der Erdachse führt ("lunisolare Präzession", in der Zeichnung mit P markiert). Die Erdachse vollführt dadurch einen Kegelumlauf um eine Achse, die rechtwinklig auf der Ekliptikebene steht. Der (nahezu) konstante Winkel zwischen der Erdachse und der Achse des Kegels ist die "Schiefe der Ekliptik"; er beträgt derzeit etwa 23,44°. Ein voller Umlauf dieser Präzessionsbewegung der Erdachse dauert etwa 25.700 bis 25.850 Jahre. Dieser Zeitraum wird "Zyklus der Präzession" (auch "Platonisches Jahr") genannt und durch die Präzessionskonstante beschrieben.

Auch die Ebene der Mondbahn, die gegenüber der Ekliptik um rund 5° geneigt ist, weist eine Präzessionsbewegung mit einer Periodenlänge von 18,6 Jahren auf; d. h. ihr Normalenvektor beschreibt einen Kegelumlauf mit dieser Umlaufdauer um den Normalenvektor der Ekliptik. Die dadurch verursachte Änderung des Drehmoments hat ebenfalls eine Auswirkung auf die Richtungsänderung der Erdachse: Dem kegelförmigen Präzessionsumlauf überlagert sich eine periodische Abweichung mit einer Amplitude von 9,2″ und einer Periode von 18,6 Jahren. Diese nickende Bewegung der Erdachse heißt Nutation. In der Zeichnung ist sie mit N bezeichnet. Daneben gibt es noch weitere Nutationsanteile mit kürzeren Perioden und Amplituden unter 1″. (Der hier verwendete astronomische Begriff der Nutation ist nicht identisch mit dem in der Mechanik verwendeten Begriff der Nutation in der Kreiseltheorie.)

Zusammen mit dem Kegelumlauf der Erdachse dreht sich auch die zur Erdachse rechtwinklig liegende Ebene, der "Äquator". Dabei dreht sich die zum Frühlingspunkt gerichtete Gerade, in der sich der Äquator mit der Ekliptik unter dem Winkel von derzeit etwa 23,43° schneidet, auf der Ekliptik mit einer Umlaufdauer von ebenfalls rund 25.800 Jahren im Uhrzeigersinn (bei Betrachtung aus der Richtung des nördlichen Poles). Seine Winkelgeschwindigkeit von 360° in 25.800 Jahren oder rund 50″ pro Jahr ist die Präzessionskonstante. Der Frühlingspunkt ist eine Bezugsachse sowohl des äquatorialen als auch des ekliptikalen Koordinatensystems, deren räumliche Orientierungen sich somit infolge der Präzession allmählich ändern. Damit ändern sich auch die auf das äquatoriale System bezogenen Koordinaten der Fixsterne. Dieser Effekt ist schon seit über zweitausend Jahren bekannt. Der griechische Astronom Hipparchos verglich etwa um 150 v. Chr. die Sternörter seines neu gemessenen Kataloges mit den Daten aus mehrere hundert Jahre alten Aufzeichnungen und stellte Unterschiede fest. Die Babylonier dürften das Phänomen der Präzession aber schon etwa 170 Jahre früher entdeckt haben. Jedoch erst im 16. Jahrhundert hat Nikolaus Kopernikus die Neigung der Erdachse und ihre Bewegung als Ursache für die Verschiebung des Frühlingspunkts erkannt.

Die Präzession der Erdachse wirkt sich auch auf die Definition eines Jahres aus. Allgemein versteht man unter einem Jahr den Zeitraum, in dem die in der Ekliptik umlaufende Gerade von der Sonne zur Erde (oder von der Erde zur Sonne) ihre Richtung um 360° (gegen den Uhrzeigersinn, bei Betrachtung aus der Richtung des nördlichen Poles) ändert. Beim siderischen Jahr wird diese Richtungsänderung auf eine Bezugsachse bezogen, die sich nicht entlang der Ekliptik bewegt. Beim tropischen Jahr ist die Bezugsachse dagegen der Frühlingspunkt. Da der Frühlingspunkt selbst sich aufgrund der Präzession der Erdachse mit einer Winkelgeschwindigkeit von 50″ pro Jahr im Uhrzeigersinn auf der Ekliptik verlagert, ist die resultierende Winkelgeschwindigkeit der Geraden von der Erde zur Sonne relativ zum Frühlingspunkt etwas größer und damit ein tropisches Jahr etwas kürzer als ein siderisches Jahr. Weil der Frühlingspunkt innerhalb von 25.800 Jahren einen Umlauf von 360° ausführt, ist in diesem Zeitraum die Anzahl der Umläufe der Geraden von der Erde zur Sonne relativ zum Frühlingspunkt um 1 größer als relativ zu einer festen Bezugsachse. Die Differenz zwischen einem tropischen und einem siderischen Jahr summiert sich also in 25.800 Jahren zu einem ganzen Jahr; folglich ist ein tropisches Jahr um ein 25.800stel Jahr ≈ 20 Minuten kürzer als ein siderisches Jahr. Für die Jahreszeiten auf der Erde ist nicht die Richtung der Sonne in Bezug auf ein absolut festliegendes Koordinatensystem, sondern in Bezug auf das äquatoriale Koordinatensystem maßgeblich, dessen 3. Achse die präzedierende Erdachse ist; so ist etwa der Frühlingsanfang immer dann, wenn die Sonne in der Richtung des Frühlingspunktes steht, ungeachtet dessen, dass dieser sich langsam bewegt. Deshalb ist das Kalenderjahr durch die geltende Schaltjahrsregelung so festgelegt, dass es sich im langfristigen Mittel gut an das tropische Jahr anpasst.

Gegenwärtig zeigt die Erdachse recht genau in Richtung des Polarsterns, so dass alle Fixsterne scheinbar eine Kreisbahn um ihn beschreiben. Als Folge der Präzession liegt der Himmelspol aber nicht fest beim Polarstern, sondern er wandert auf einem Kreis mit einem Radius von etwa 23,5° (konstante Schiefe der Ekliptik angenommen) um den Ekliptikpol. In 12.000 Jahren wird er sich bei der Wega im Sternbild Leier befinden, dem zweithellsten nördlichen Stern, und das Sternbild „Großer Hund“ beispielsweise wird von Mitteleuropa aus nicht mehr sichtbar sein, vom Sternbild Orion nur noch die sogenannten Schultersterne.

Im Rahmen der Milanković-Zyklen gibt es einen Einfluss der Präzession auf die Eiszeiten, über dessen Ausmaß aber noch Unklarheit herrscht.




</doc>
<doc id="14526" url="https://de.wikipedia.org/wiki?curid=14526" title="Otto von Guericke">
Otto von Guericke

Otto von Guericke (Aussprache und ursprüngliche Schreibung: "Gericke" []) (*  in Magdeburg; †  in Hamburg) war ein deutscher Politiker, Jurist, Physiker und Erfinder. Bekannt ist er vor allem für seine Experimente zum Luftdruck mit den Magdeburger Halbkugeln.

Guericke entstammt als einziger Sohn der am 15. Januar 1602 geschlossenen Ehe des Hans Gericke (1555–1620), Ratskämmerer und späterer Schultheiß zu Magdeburg, und der vermögenden Patriziertochter Anna von Zweydorff aus Braunschweig (1580–1666).

Zu seinen vielfältigen Braunschweig-Bezügen gehört, dass Otto von Guericke von dem Großvater mütterlicherseits der Anna von Zweydorff, Conradus von Plauen (Plaue), „der Stadt Braunschweig 50jähriger Obersecretarius“, ein Studien-Stipendium über die Dauer von sechs Jahren erhielt, das ihm in seiner vielseitigen wissenschaftlichen Ausbildung zweifellos von Vorteil gewesen sein dürfte.

Er studierte von 1617 bis 1619 an der Universität in Leipzig an der Artistenfakultät, einige Wochen an der Universität Helmstedt und im Fachstudium Jura an der Universität Jena von 1621 bis 1623 sowie 1623 bis 1624 in Leiden Jura und Festungsbau.

Es folgte eine Bildungsreise (oder Kavalierstour) durch England und Frankreich. Im November 1625 war er zurück in Magdeburg. Am 18. September 1626 heiratete er Margaretha Alemann, Tochter des Ratsherren Jacob Alemann. Mit ihr hatte er drei Kinder. Anna Catharina (geb. August 1627, gest. 13. Oktober 1627), Otto Junior (geb. 23. Oktober 1628, gest. 1704) und Jakob Christoph (geb. 1630, gest. 1632). Im selben Jahr wurde er in den Rat seiner Heimatstadt gewählt. Als Ratsherr nahm er die Funktion eines Bauherren und im Verteidigungsfall 1629 und 1630/1631 die eines Schutzherren wahr. Nach der Zerstörung der Stadt im Jahr 1631 trat er als Festungsbauingenieur in Erfurt und ab 1632 in Magdeburg in schwedische Dienste. Von 1636 bis 1646 arbeitete er in Magdeburg außerdem im kursächsischen Dienst. 1646 wurde er nach ersten diplomatischen Erfolgen zu einem der vier Bürgermeister der Alten Stadt Magdeburg gewählt. 1652 (sieben Jahre nach dem Tod seiner ersten Frau) heiratete er Dorothea Lentke, die Tochter seines Amtskollegen Steffan Lentke. Diese Ehe blieb kinderlos.

Von 1642 bis 1663 war er im Auftrag der Stadt in diplomatischen Missionen unterwegs; darunter fällt auch die Teilnahme an den Verhandlungen zum Westfälischen Frieden, zum Exekutionstag in Nürnberg 1649/1650 und zum Reichstag in Regensburg 1653/1654.
Etwa von 1645 an stellte er, angeregt durch die Diskussion zur Astronomie, Untersuchungen zur Pneumatik an, für die er berühmt werden sollte. Bereits 1656 begann ein reger Briefwechsel mit dem Professor für Philosophie und Mathematik in Würzburg. Caspar Schott veröffentlichte 1657 sein Werk "Mechanica hydraulico-pneumatica", das den Anhang "Neuer Magdeburger Versuch" beinhaltet. Dies ist die erste Veröffentlichung von Otto von Guericke. 1664 veröffentlichte Caspar Schott ein weiteres Werk "Techica curiosa" in Würzburg. Es enthält die zweite Veröffentlichung von Otto von Guericke. 1663 stellte er das Manuskript der "Experimenta nova…" fertig, brachte es aber erst 1672 in Amsterdam in Druck.

Am 4. Januar 1666 wurde er durch Kaiser Leopold I geadelt, wobei er seinem Nachnamen ein "u" (Otto von Guericke) hinzufügte, damit er auch von ausländischen Diplomaten richtig ausgesprochen werden konnte, da Französisch zu dieser Zeit als aufkommende Sprache der Diplomaten genutzt wurde und dadurch sein Name im Französischen wie Deutschen gleich ausgesprochen wurde.

1676 lehnte er aus gesundheitlichen Gründen die turnusmäßige Übernahme des Bürgermeisteramtes ab. 1678 wurde er vom Rat „pro emerito“ erklärt. 1681 siedelte er, als in Magdeburg die Pest ausbrach, zu seinem Sohn nach Hamburg über, nachdem ihm die ehrenhalber gewährte Steuerfreiheit in Magdeburg wieder gestrichen worden war. In Hamburg starb Guericke 1686. Nach der Überführung seiner Gebeine nach Magdeburg wurde er am 2. Juli 1686 in der Johanniskirche in die Alemann/Guericke-Gruft beigesetzt. In der Napoleonischen Zeit, in der die Kirche Hals über Kopf in ein Lazarett umgewandelt wurde, ist die Gruft beseitigt worden. Die Gebeine wurden vor den Stadttoren in Großgräbern beerdigt. Die Gruft kann trotzdem noch heute besucht werden.
Der Grabstein seiner ersten Frau wurde vor einigen Jahren in der Johanniskirche gefunden.

Seine wissenschaftliche Hauptleistung ist die Begründung der Vakuumtechnik.

Er erfand 1649 die Kolbenvakuumluftpumpe, untersuchte die Eigenschaften des (Teil-)Vakuums in einer Vielzahl von Versuchen und schuf Anwendungen wie den Hebeversuch und die Windbüchse. Dabei konnte er zeigen, dass wohl Licht den luftleeren Raum durchdringt, nicht aber der Schall.

In der Öffentlichkeit demonstrierte er die Kraft des Luftdrucks mit spektakulären Experimenten, besonders 1654 auf dem Reichstag zu Regensburg in Anwesenheit von Kaiser Ferdinand III. Guericke hatte im Sommer 1657 zwei große Halbkugeln (¾ Magdeburger Elle) aus Kupfer (Magdeburger Halbkugeln) mittels einer Dichtung zusammengelegt und pumpte die Luft aus dem Inneren heraus. Anschließend wurden vor jede Halbkugel nacheinander acht Pferde gespannt, die sie auseinanderreißen sollten, was aber nicht gelang. Als die Kugeln wieder mit Luft gefüllt wurden, fielen sie von allein auseinander. Bei einem anderen Versuch hatte Guericke einen Zylinder mit beweglichem Kolben aufstellen lassen. An dem Kolben wurde ein Seil befestigt, das über eine Umlenkrolle lief und von 50 Männern festgehalten wurde. Als Guericke die Luft aus dem Zylinder absaugte, konnten die Männer den Kolben nicht am Absinken hindern, da der atmosphärische Luftdruck gegen ein Vakuum stärker war. Das war die folgenreiche Erfindung seiner Hebemaschine.

Mit seinen Versuchen bestätigte Guericke Schlüsse, die zehn Jahre zuvor Blaise Pascal aus dem Experiment Leere in der Leere gezogen hatte: Anders als zuvor mit Berufung auf Aristoteles angenommen wurde, saugt nicht das Vakuum die Materie an. Vielmehr ist es die Materie, die ins Vakuum drängt. Die als Erklärung für das Saugen angenommene „Abscheu der Natur vor der Leere“ (Horror Vacui) war damit hinfällig.

1632 fertigte er einen maßstäblichen Stadtplan seiner Heimatstadt an. Guericke setzte ein Barometer zur Wettervorhersage ein und war damit Wegbereiter der Meteorologie. Er baute auch ein 10 m hohes Wasserbarometer am Magdeburger Rathaus. Für 1660 ist die Vorhersage eines Unwetters bekannt, das dann auch eintraf.

Guericke werden auch neue Erkenntnisse auf dem Gebiet der Elektrizität zugeschrieben. Dies beruht auf von ihm zuerst 1672 publizierten Experimenten mit einer Schwefelkugel, in denen er versuchte, die von ihm postulierten kosmischen Wirkkräfte nachzuweisen. Hier beschreibt Guericke unter anderem Anziehungs- und Abstoßungserscheinungen, die wir heute als elektrische Phänomene verstehen. Sein Versuchsaufbau wird deshalb auch gelegentlich als erste Elektrisiermaschine beschrieben. Solche Zuschreibungen sind allerdings problematisch, da Guericke hier andere Kräfte wirken sah, Kräfte, die wir heute nicht mehr kennen. Es ist daher anzuzweifeln, ob Guericke aus den beobachteten Phänomenen Wissen über elektrische Zusammenhänge erlangt hat.

Er beschäftigte sich auch mit Astronomie und stellte als erster die Behauptung auf, dass sich der Zeitpunkt der Wiederkehr eines Kometen bestimmen lassen müsse.

Im Jahr 1678 konstruierte Guericke aus Knochen, die 1663 in der Nähe von Quedlinburg gefunden worden waren, ein Einhorn.






</doc>
<doc id="14527" url="https://de.wikipedia.org/wiki?curid=14527" title="Joseph Roth">
Joseph Roth

Moses Joseph Roth (* 2. September 1894 in Brody, Ostgalizien, Österreich-Ungarn; † 27. Mai 1939 in Paris) war ein österreichischer Schriftsteller und Journalist.

Roth wurde in dem galizischen Schtetl Brody geboren, das damals zur österreichisch-ungarischen Monarchie gehörte. Brody war Grenzstadt zum russischen Wolhynien. Seine Mutter Maria Grübel stammte aus einer in Brody ansässigen Familie jüdischer Kaufleute, sein Großvater handelte mit Tuch, seine fünf Onkel mit Hopfen. Roths Vater Nachum Roth stammte aus orthodox-chassidischem Umfeld. Bei der Heirat 1892 war er Getreidehändler im Auftrag einer Hamburger Firma. Als von ihm in Kattowitz eingelagerte Ware veruntreut wurde, musste er zur Regelung der Angelegenheit nach Hamburg reisen. Auf der Rückreise wurde er durch sein Verhalten im Zug auffällig. Er wurde deswegen zunächst in eine Anstalt für Geisteskranke eingewiesen, dann seinen westgalizischen Verwandten übergeben, die ihn der Obhut eines russisch-polnischen Wunderrabbis überließen, an dessen Hof ihn Jahre später einer der Onkel Joseph Roths ausfindig machte. Dieser beschrieb den Vater als sehr schön, unaufhörlich lachend und völlig unzurechnungsfähig.

Joseph Roth hat seine Herkunft zum Gegenstand von Verschleierung und Mystifikation gemacht. Vor allem die Person seines Vaters erschien in mehrfachen schillernden Umgestaltungen: Er sei der außereheliche Sohn eines österreichischen Offiziers, eines polnischen Grafen, eines Wiener Munitionsfabrikanten. Roth behauptete auch, in Szwaby (Schwaby), einem kleinen Dorf in der Nähe von Brody, geboren worden zu sein, dessen Einwohner mehrheitlich deutschstämmig waren, im Gegensatz zur jüdischen Bevölkerungsmehrheit in Brody. Tatsächlich lag Roths Geburtshaus in einem Viertel um den Bahnhof von Brody, das damals bei den Einwohnern den Beinamen „Schwabendorf“ oder „Szwaby“ hatte, weil hier die Familien ehemaliger deutscher Einwanderer wohnten. Roths Geburtshaus wurde im sowjetisch-ukrainischen Krieg 1919/1920 zerstört. Der frühe Vaterverlust und in übertragener Form der Verlust des Vaterlandes, nämlich der österreichischen Monarchie, zieht sich als roter Faden durch Roths Werk.

Roth berichtete von einer von Armut und Dürftigkeit geprägten Kindheit und Jugend. Demgegenüber weisen
Fotografien aus der Zeit und die Berichte seiner Verwandten zwar nicht auf Wohlhabenheit, aber auf durchaus bürgerliche Lebensumstände hin: Seine Mutter hatte ein Dienstmädchen, Joseph erhielt Violinunterricht und besuchte das Gymnasium.

In anderer als materieller Hinsicht war die Lage seiner Mutter allerdings tatsächlich prekär: Sie war nicht Witwe, da ihr Mann noch lebte bzw. als vermisst galt. Scheiden lassen konnte sie sich nicht, da dies einen Scheidebrief ("Get") ihres Mannes erfordert hätte, dazu jedoch hätte dieser bei Sinnen sein müssen. Außerdem galt im orthodoxen Judentum Galiziens Wahnsinn als Fluch Gottes, der auf der ganzen Familie lag und die Heiratsaussichten der Kinder deutlich verschlechterte. Deshalb wurde in der Familie über das Schicksal des Vaters geschwiegen, und man nahm lieber das Gerücht hin, Nachum Roth habe sich erhängt.

Die Mutter lebte zurückgezogen und versorgte den Haushalt des Großvaters bis zu dessen Tod im Jahre 1907. Sie konzentrierte sich auf die Erziehung des Sohnes, der abgeschlossen und behütet aufwuchs.

Ab 1901 besuchte Joseph Roth die Baron-Hirsch-Schule in Brody, eine vom jüdischen Eisenbahnmagnaten und Philanthropen Maurice de Hirsch gegründete Handelsschule, die sich, anders als die Cheder genannten orthodoxen Traditionsschulen, nicht auf den religiösen Unterricht beschränkte, sondern wo über Hebräisch und Thorastudium hinaus auch Deutsch, Polnisch und praktische Fächer unterrichtet wurden. Unterrichtssprache war Deutsch.

Von 1905 bis 1913 besuchte Roth das Kronprinz-Rudolf-Gymnasium in Brody. Es ist nicht ganz klar, ob das Schulgeld von 15 Gulden pro Semester (eine erhebliche Summe; in dieser Zeit war allerdings bereits die Kronenwährung eingeführt) von seinem Vormund und Onkel Siegmund Grübel bezahlt wurde, ob er ein Stipendium hatte oder ihm das Schulgeld erlassen wurde. Er war ein guter Schüler. Als einziger Jude seines Jahrgangs legte er 1913 die Matura "sub auspiciis Imperatoris" ab. Auf seine Mitschüler wirkte er teils zurückhaltend, teils arrogant, ein Eindruck, den er auch später bei seinen Kommilitonen an der Wiener Universität hinterließ. In diese Zeit fallen seine ersten schriftstellerischen Arbeiten (Gedichte). Zusammen mit anderen bekannten ehemaligen Schülern wird Roth in einem schuleigenen Museumsraum geehrt.

Nach seiner Matura (Abitur) im Mai 1913 übersiedelte Roth nach Lemberg, in die Hauptstadt Galiziens, wo er sich an der Universität Lemberg immatrikulierte. Unterkunft fand er bei seinem Onkel Siegmund Grübel, doch scheint es zwischen dem nüchternen Kaufmann und dem angehenden Dichter bald zu Spannungen gekommen zu sein. Eine mütterliche Freundin für viele Jahre fand er in der damals 59-jährigen Helene von Szajnoda-Schenk, einer gebrechlichen, aber geistig sehr lebhaften und hochgebildeten Dame, die im Haus des Onkels eine Wohnung gemietet hatte. Auch mit seinen Cousinen Resia und Paula verband ihn bald Freundschaft.

Die Atmosphäre Lembergs war damals geprägt von sich verschärfenden Spannungen, nicht nur zwischen den Nationalitäten (an der Universität kam es zu Kämpfen zwischen polnischen und ruthenischen Studenten), auch innerhalb des Judentums gärte die Auseinandersetzung zwischen Chassidismus, Haskala (Aufklärung) und der immer stärker werdenden zionistischen Bewegung. Inwieweit Roth tatsächlich in Lemberg studiert hat, ist nicht klar. Er hielt sich schon im Herbst 1913 zeitweise in Wien auf, wo er vom 2. bis 9. September 1913 am XI. Zionisten-Kongress teilnahm.

In Brody war Roths Jahrgang der letzte mit Deutsch als Unterrichtssprache gewesen, an der Universität Lemberg war seit 1871 Polnisch die Unterrichtssprache. Dass Roth seine literarische Heimat in der deutschen Literatur sah, war möglicherweise einer der Gründe, Lemberg zu verlassen und sich für das Sommersemester 1914 an der Wiener Universität zu immatrikulieren.

In Wien nahm sich Roth zunächst ein kleines Zimmer im 2. Gemeindebezirk, der Leopoldstadt, wo viele Juden lebten. Im folgenden Semester bezog er mit seiner Mutter, die vor den Wirren des ausbrechenden Ersten Weltkrieges nach Wien geflohen war, eine kleine Wohnung im benachbarten 20. Bezirk, Brigittenau (Wallensteinstraße 14/16). Roth und seine Mutter, später auch die Tante Rebekka (Riebke), lebten in dieser ersten Zeit in recht dürftigen Umständen. Roth war ohne Einkünfte, seine Mutter bezog eine geringe Flüchtlingshilfe. Nach dem Beginn des Ersten Weltkrieges erfolgten die Zuwendungen von Onkel Siegmund wegen der russischen Okkupation nur sporadisch.

Roth begann das Studium der Germanistik. Er legte Wert darauf, in den Prüfungen erfolgreich abzuschneiden und von den Professoren zur Kenntnis genommen zu werden. Im Nachhinein urteilte er negativ über Studenten und Lehrer. Eine Ausnahme bildete Walther Brecht, der Ordinarius für Neuere deutsche Literatur. Heinz Kindermann, Brechts Assistent, wurde zu einer Art Rivale. In der 1916 erschienenen ersten Erzählung Roths, "Der Vorzugsschüler", war Kindermann Vorbild für die Hauptfigur "Anton Wanzl", einen mit einigem Hass und einiger Kenntnis geschilderten Charakter.

Bald besserte sich die materielle Situation. Stipendien und Hauslehrerstellen (unter anderem bei der Gräfin Trautmannsdorff) erlaubten Roth die Anschaffung guter Anzüge. Mit Bügelfalte, Stock und Monokel beschrieben ihn Zeugen der Zeit als Abbild des Wiener „Gigerls“ (Dandys).

Zum wegweisenden Erlebnis wurde für Roth der Erste Weltkrieg und der darauf folgende Zerfall Österreich-Ungarns. Im Gegensatz zu vielen anderen, die bei Kriegsausbruch von nationaler Begeisterung erfasst wurden, vertrat er zunächst eine pazifistische Position und reagierte mit einer Art erschreckten Bedauerns. Doch im Verlauf der Zeit erschien ihm, der als kriegsuntauglich eingestuft worden war, die eigene Haltung als beschämend und peinlich: „Als der Krieg ausbrach, verlor ich meine Lektionen, allmählich, der Reihe nach. Die Rechtsanwälte rückten ein, die Frauen wurden übelgelaunt, patriotisch, zeigten eine deutliche Vorliebe für Verwundete. Ich meldete mich endlich freiwillig zum 21. Jägerbataillon.“

Am 31. Mai 1916 meldete Roth sich zum Militärdienst und begann am 28. August 1916 seine Ausbildung als Einjährig-Freiwilliger. Er und sein Freund Józef Wittlin optierten für das 21. Feldjäger-Bataillon, dessen Einjährigen-Schule sich im 3. Wiener Bezirk befand. Ursprünglich war geplant, das Studium in der Freizeit fortzusetzen.

In die Zeit der Ausbildung fiel der Tod von Kaiser Franz Joseph I. am 21. November 1916. Roth stand im Spalier der Soldaten entlang des Beerdigungszuges: „Der Erschütterung, die aus der Erkenntnis kam, daß ein historischer Tag eben verging, begegnete die zwiespältige Trauer um den Untergang eines Vaterlandes, das selbst zur Opposition seine Söhne erzogen hatte.“ Der Tod des 86-jährigen Kaisers wird zu einem zentralen Symbol für den Untergang des Habsburgerreiches und den Verlust von Heimat und Vaterland mehrfach in Roths Werken, unter anderem in den Romanen "Radetzkymarsch" und "Die Kapuzinergruft".

Roth wurde nach Galizien zur 32. Infanterietruppendivision versetzt. Von 1917 bis wahrscheinlich zum Kriegsende war er dem militärischen Pressedienst im Raum Lemberg zugeteilt. Roths angebliche russische Kriegsgefangenschaft ist nicht nachweisbar, mögliche Akten oder persönliche Briefe dazu sind nicht erhalten.

Nach Kriegsende musste Joseph Roth sein Studium abbrechen und sich auf den Erwerb des Lebensunterhalts konzentrieren. Bei der Rückkehr nach Wien fand er zunächst Unterkunft bei Leopold Weiß, dem Schwager seines Onkels Norbert Grübel. Nach einem Aufenthalt in Brody geriet er auf dem Rückweg in die Auseinandersetzungen zwischen polnischen, tschechoslowakischen und ukrainischen Einheiten, aus denen er nur mit Mühe zurück nach Wien entkam.

Noch während seiner Militärzeit begann Roth, Berichte und Feuilletons für die Zeitschriften "Der Abend" und "Der Friede" zu schreiben. In "Österreichs Illustrierter Zeitung" erschienen Gedichte und Prosa. Im April 1919 wurde er Redakteur bei der Wiener Tageszeitung "Der Neue Tag", die auch Alfred Polgar, Anton Kuh und Egon Erwin Kisch zu ihren Mitarbeitern zählte. In diesem beruflichen Umfeld gehörte es dazu, Stammgast im Café Herrenhof zu sein, wo Roth im Herbst 1919 seine spätere Frau Friederike (Friedl) Reichler kennenlernte.

Ende April 1920 stellte der "Neue Tag" sein Erscheinen ein. Roth zog nach Berlin. Dort hatte er zunächst Schwierigkeiten mit seiner Aufenthaltsgenehmigung wegen der Unklarheiten und Fiktionen in seinen Dokumenten. So hatte beispielsweise ein befreundeter Pfarrer ihm einen Taufschein ausgestellt, in dem als Geburtsort Schwaben in Ungarn eingetragen war. Bald erschienen Beiträge von ihm in verschiedenen Zeitungen, darunter die "Neue Berliner Zeitung". Ab Januar 1921 arbeitete er hauptsächlich für den "Berliner Börsen-Courier".

Im Herbst 1922 kündigte er die Mitarbeit beim "Börsen-Courier" auf. Er schrieb: „Ich kann wahrhaftig nicht mehr die Rücksichten auf ein bürgerliches Publikum teilen und dessen Sonntagsplauderer bleiben, wenn ich nicht täglich meinen Sozialismus verleugnen will. Vielleicht wäre ich trotzdem schwach genug gewesen, für ein reicheres Gehalt meine Überzeugung zurückzudrängen, oder für eine häufigere Anerkennung meiner Arbeit.“ Im gleichen Jahr erkrankte Roths Mutter an Gebärmutterhalskrebs und wurde in Lemberg operiert, wo sie der Sohn kurz vor ihrem Tod zum letzten Mal sah.

Ab Januar 1923 arbeitete er als Feuilletonkorrespondent für die renommierte "Frankfurter Zeitung", in der in den folgenden Jahren ein großer Teil seiner journalistischen Arbeiten erschien. Wegen der Inflation in Deutschland und Österreich und der deshalb abwechselnd relativ schlechteren wirtschaftlichen Lage pendelte Roth in dieser Zeit mehrfach zwischen Wien und Berlin und schrieb außer für die "FZ" auch Artikel für die "Wiener Sonn- und Montagszeitung", das "Neue 8-Uhr-Blatt" (Wien), "Der Tag" (Wien) und das "Prager Tagblatt" sowie für den deutschsprachigen "Pester Lloyd" in Budapest. Während dieser Zeit arbeitete er auch an seinem ersten Roman, "Das Spinnennetz", der im Herbst 1923 als Fortsetzungsroman in der Wiener "Arbeiter-Zeitung" abgedruckt wurde, aber unvollendet blieb.

Sein Verhältnis zur "Frankfurter Zeitung" und dem damals für die Feuilletonredaktion zuständigen Benno Reifenberg blieb nicht frei von Reibungen. Roth fühlte sich nicht hinreichend geschätzt und versuchte dies durch Honorarforderungen zu kompensieren. Als er sich von der Zeitung trennen wollte, bot man ihm an, als Korrespondent in Paris weiterzuarbeiten. Roth nahm an, siedelte im Mai 1925 nach Paris über und äußerte sich in seinen ersten Briefen enthusiastisch über die Stadt. Als er ein Jahr später als Korrespondent von Friedrich Sieburg abgelöst wurde, war er schwer enttäuscht. "Sie ahnen nicht, wieviel privat und die litterarische Carrière betreffend mir zerstört wird, wenn ich Paris verlassse," schrieb er am 9. April 1926 an Reifenberg.

Zum Ausgleich verlangte er, von der "FZ" mit großen Reisereportage-Serien beauftragt zu werden. Von August bis Dezember 1926 bereiste er daher die Sowjetunion, von Mai bis Juni 1927 Albanien und Jugoslawien, im Herbst 1927 das Saargebiet, von Mai bis Juli 1928 Polen und im Oktober/November 1928 Italien. Im Juni 1929 kündigte er seine Mitarbeit an der "FZ" auf.

Roth zeichnete parallel zu seiner "FZ"-Mitarbeit als „Der rote Joseph“ Beiträge für die sozialistische Zeitung "Vorwärts". Er pflegte in seinen Berichten und Feuilletons einen beobachtenden Stil und zog aus den wahrgenommenen Lebensfragmenten und unmittelbaren Äußerungen menschlichen Unglücks Folgerungen soziale Missstände und die politischen Verhältnisse betreffend. Freunde und Kollegen kritisierten ihn heftig, als er 1929 gegen gute Bezahlung für die nationalistischen "Münchner Neuesten Nachrichten" schrieb. In der Zeit vom 18. August 1929 bis zum 1. Mai 1930 verfasste er ca. 30 Beiträge für die "Münchner Neuesten Nachrichten." Sein Vertrag dort sah 2000 Mark monatlich für mindestens zwei zu liefernde Beiträge vor.

Am 5. März 1922 heiratete Roth in Wien die am 12. Mai 1900 geborene Friederike (Friedl) Reichler. Der attraktiven und intelligenten Frau entsprach nicht das ruhelose Leben an der Seite eines reisenden Starjournalisten. Roth dagegen zeigte Symptome einer fast pathologischen Eifersucht. 1926 traten erste Symptome einer geistigen Erkrankung Friedls zutage, 1928 wurde ihre Krankheit manifest. Sie wurde zunächst in der Berliner Nervenheilanstalt Westend behandelt, dann wohnte sie, von einer Krankenschwester betreut, eine Zeit lang bei einem Freund ihres Mannes.

Die Krankheit seiner Frau stürzte Roth in eine tiefe Krise. Er war nicht bereit, die Unheilbarkeit der Krankheit zu akzeptieren, hoffte auf ein Wunder, gab sich die Schuld an der Erkrankung: Wahnsinn galt und gilt unter frommen Juden als Strafe Gottes. Eine mögliche Besessenheit durch einen Dibbuk veranlasste ihn zu der (erfolglosen) Konsultation eines chassidischen Wunderrabbis. Während dieser Zeit begann er heftig zu trinken. Auch seine finanzielle Situation verschlechterte sich.

Als auch die Unterbringung bei Friedls Eltern keine Besserung brachte und die Kranke zunehmend in Apathie verfiel, brachte man sie im November 1930 in das Sanatorium in Rekawinkel bei Wien, im Dezember 1933 kam sie in die Landes-Heil- und Pflegeanstalt „Am Steinhof“ am Rand Wiens, schließlich im Sommer 1935 in die Heil- und Pflegeanstalt Mauer-Öhling in Niederösterreich. Friedls Eltern wanderten 1935 nach Palästina aus. Roth beantragte die Scheidung von seiner Frau. Im Jahr 1940 wurde Friedl Roth in die Anstalt Niedernhart (heute Landes-Nervenklinik Wagner-Jauregg) bei Linz verlegt, eine sogenannte Zwischenanstalt im Rahmen der Aktion T4, von wo sie weiter in die NS-Tötungsanstalt Hartheim verbracht wurde. Friederike Reichler wurde dort in der Gaskammer getötet. Als ihr Todesdatum gilt der 15. Juli 1940.

Die Krankheit seiner Frau blieb für Roth – auch während folgender Beziehungen – eine Quelle von Selbstvorwürfen und Bedrückung. 1929 lernte er Sybil Rares kennen, eine jüdische Schauspielerin aus der Bukowina, die am Frankfurter Schauspielhaus engagiert war, und nahm mit ihr ein kurz andauerndes Verhältnis auf.

Im August 1929 begegnete er Andrea Manga Bell, Tochter einer Hamburger Hugenottin und eines Kubaners. Sie war verheiratet mit Alexandre Manga Bell, "Prince de Douala et Bonanyo" aus der ehemaligen deutschen Kolonie Kamerun, Sohn des 1914 von den Deutschen exekutierten Douala-Königs Rudolf Manga Bell, der sie jedoch verlassen hatte und nach Kamerun zurückgekehrt war. Als Roth sie kennenlernte, war sie Redakteurin bei der Ullstein-Zeitschrift "Gebrauchsgraphik" und ernährte so ihre zwei Kinder. Roth war von der selbstbewussten und selbstständigen Frau sofort fasziniert. Bald bezog man zusammen mit den Kindern eine gemeinsame Wohnung. Möglicherweise war Andrea Manga Bell das Vorbild für die Figur der Juliette Martens in Klaus Manns Schlüsselroman "Mephisto".

Als Roth emigrieren musste, folgte ihm Andrea Manga Bell mit ihren Kindern. Im Laufe der Zeit kam es zwischen den beiden zu Spannungen, für die Roth die durch die Versorgung der Familie Manga Bells entstehenden finanziellen Probleme verantwortlich machte („Ich muß einen Negerstamm von neun Personen ernähren!“). Wahrscheinlichere Ursache für die Streitigkeiten und das endgültige Zerwürfnis Ende 1938 war Roths extreme Eifersucht.

Anfang Juli 1936 war Roth auf Einladung Stefan Zweigs nach Ostende gereist, wo er der dort seit kurzem in der Emigration lebenden Schriftstellerin Irmgard Keun begegnete. Beide interessierten sich sofort füreinander. Irmgard Keun:

Von 1936 bis 1938 lebten die beiden in Paris zusammen. Egon Erwin Kisch bescheinigte dem Paar einen Hang zum Alkoholexzess: „Die beiden saufen wie die Löcher“. Keun begleitete Roth auf seinen Reisen, unter anderem bei seinem Besuch in Lemberg zu Weihnachten 1936, wo er sie seiner alten Freundin Helene von Szajnoda-Schenk vorstellte. Auch diese Beziehung zerbrach schließlich. Nach Aussage Irmgard Keuns war wiederum Roths Eifersucht die Ursache:

Am 30. Januar 1933, dem Tag von Hitlers Ernennung zum Reichskanzler, verließ Roth Deutschland. In einem Brief an Stefan Zweig urteilte er:

Roths Bücher wurden Opfer der Bücherverbrennungen durch die Nationalsozialisten. Roth wählte als Ort seines Exils zunächst Paris, unternahm aber diverse, teils mehrmonatige Reisen, unter anderem in die Niederlande, nach Österreich und nach Polen. Von Juni 1934 bis Juni 1935 hielt sich Roth, wie viele andere Emigranten, an der französischen Riviera auf. Zusammen mit Hermann Kesten und Heinrich Mann mieteten Roth und Manga Bell ein Haus in Nizza.

Die Reise nach Polen erfolgte im Februar/März 1937; er hielt auf Einladung des polnischen PEN-Klubs eine Reihe von Vorträgen. Er unternahm bei dieser Gelegenheit einen Abstecher ins damals polnische Lemberg, um seine Verwandten zu besuchen, die alle Opfer der Shoa wurden.

Anders als vielen emigrierten Schriftstellern gelang es Roth, nicht nur produktiv zu bleiben, sondern auch Publikationsmöglichkeiten zu finden. Seine Werke erschienen in den niederländischen Exilverlagen "Querido" und "de Lange" sowie in dem christlichen Verlag "De Gemeenschap". Unter anderem deshalb hielt er sich während seines Exils mehrfach in den Niederlanden und Belgien auf (Mai 1935 in Amsterdam und 1936 längere Aufenthalte in Amsterdam und Ostende). Darüber hinaus verfasste er Beiträge für die von Leopold Schwarzschild herausgegebene Exilzeitschrift "Das neue Tage-Buch".

In den letzten Jahren verschlechterte sich Roths finanzielle und gesundheitliche Situation rapide. Im November 1937 wurde sein Aufenthaltsort für zehn Jahre, das "Hotel Foyot" in der Pariser "Rue de Tournon", wegen Baufälligkeit abgerissen. Er zog vis-a-vis in ein kleines Zimmer über seinem Stammcafe, dem "Café Tournon". Am 23. Mai 1939 wurde Roth in das Armenspital "Hôpital Necker" eingeliefert, nachdem er (angeblich nach Erhalt der Nachricht vom Selbstmord Ernst Tollers) im Café Tournon zusammengebrochen war. Am 27. Mai starb er an einer doppelseitigen Lungenentzündung. Der letale Verlauf der Krankheit wurde durch den abrupten Alkoholentzug (Alkoholdelirium) begünstigt.

Am 30. Mai 1939 wurde Roth auf dem zu Paris gehörenden Cimetière parisien de Thiais in Thiais, südlich der Hauptstadt, beerdigt. Die Beisetzung erfolgte nach „gedämpft-katholischem“ Ritus, da kein Beleg für die Taufe Roths erbracht werden konnte. Bei der Beerdigung kam es beinahe zu Zusammenstößen zwischen den sehr heterogenen Beteiligten der Trauergesellschaft: österreichische Legitimisten, Kommunisten und Juden reklamierten den Toten jeweils als einen der ihren. Das Grab liegt in der katholischen Sektion des Friedhofs („Division 7“). Die Inschrift auf dem Grabstein lautet: "écrivain autrichien – mort à Paris en exil" („österreichischer Schriftsteller – gestorben in Paris im Exil“).

Das schriftstellerische Werk Roths kann nicht ohne Weiteres einer bestimmten Richtung oder Gruppierung der zeitgenössischen Literatur zugeordnet werden, am ehesten noch der Neuen Sachlichkeit, vor allem hinsichtlich seiner frühen Romane. So trägt "Die Flucht ohne Ende" den Untertitel "Ein Bericht", und im Vorwort versichert der Autor: „Ich habe nichts erfunden, nichts komponiert. Es handelt sich nicht mehr darum, zu ‚dichten‘. Das wichtigste ist das Beobachtete.“

Roth war seinen Zeitgenossen in erster Linie als Journalist bekannt und journalistische Arbeiten machen gut die Hälfte seines Werkes aus. Am Sprachexperiment des die Literatur der Weimarer Zeit prägenden Expressionismus, deren Gegenbewegung die Neue Sachlichkeit war, nahm Roth nicht teil. Er vertrat die Position des journalistischen „Handwerkers“ und blieb in seinen sprachlichen Mitteln konservativ.

Allerdings erteilte Roth in seiner Schrift "Schluß mit der „Neuen Sachlichkeit“" dieser Richtung 1930 eine Absage. Er kritisierte von einem journalistischen Standpunkt aus die Ungeformtheit einer Literatur, die sich auf „nackte Tatsachen“ beschränken wolle, indem er der "Zeugenaussage" den (geformten) "Bericht" gegenüberstellte: „Das Faktum und das Detail sind der "Inhalt" der Zeugenaussage. Sie sind das "Rohmaterial" des Berichts. Das Ereignis ‚wiederzugeben‘, vermag erst der geformte, also künstlerische Ausdruck, in dem das Rohmaterial enthalten ist wie Erz im Stahl, wie Quecksilber im Spiegel.“ Er wirft der Neuen Sachlichkeit in diesem Text vor, sich die Haltung des naiven Lesers zu eigen zu machen: „Der primitive Leser will entweder ganz in der Wirklichkeit bleiben oder ganz aus ihr fliehen.“ Roth bevorzugt dagegen das angeblich Authentische des ungeformten Augenzeugenberichts. Als Journalist kannte er die Arbeit, die aus Einzelaussagen einen Bericht formt – und konstatiert als Dichter: „… erst das ‚Kunstwerk‘ ist ‚echt wie das Leben‘.“ Programmatisch für sein Werk ist der Satz: „Der Erzähler ist ein Beobachter und ein Sachverständiger. Sein Werk ist niemals von der Realität gelöst, sondern in Wahrheit (durch das Mittel der Sprache) umgewandelte Realität.“

Wahrheit und Gerechtigkeit sind – als göttliche Attribute – zentrale Begriffe der jüdischen Kultur. Roth fühlte sich diesen Werten verbunden. Allerdings arbeitete Roth auch als „Mythomane“ und „Mystifikator“. Beispielsweise erzählte er von den in Kriegsgefangenschaft erlittenen Härten – bis Egon Erwin Kisch ihm hinterherrecherchierte und nachwies, dass Roth nie in Kriegsgefangenschaft war. Doch Franz Tunda in "Flucht ohne Ende" war in Kriegsgefangenschaft und Roth verschmolz hier mit seiner Romanfigur. Roth konstatierte: „Es kommt nicht auf die Wirklichkeit an, sondern auf die innere Wahrheit.“

Weitere veränderte Erzählungen:

Roths dichterische Umgestaltung seiner Biographie verursachte bei seinen damaligen Freunden und Bekannten wie auch bei seinen Biographen Irritationen. Es ist allerdings kein Fall belegt, in dem Roth aus seinen Mystifikationen persönlichen Vorteil gezogen hätte. Vielmehr war er bekannt als über die eigenen Mittel hinaus großzügiger und selbstloser Helfer von in Not Geratenen.

Allgemein wird bei Roth um die Jahre 1925/26 eine Wandlung von früheren sozialistischen Positionen zu monarchistischen gesehen. Ein Teil seiner Artikel aus den früheren Jahren sind sozialkritisch geschrieben. Roth beschrieb das Konkrete und bemühte sich um eine sehr genaue Beobachtung. Dabei begab er sich nicht in den Bereich politischer Theorien. Einige Artikel Roths im sozialdemokratischen Vorwärts erschienen unter dem Pseudonym „Der rote Joseph“. Uwe Schweikert (1982) ordnet Roth im Nachhinein als Sozialromantiker ein und beschrieb seine spätere Abkehr von linker Position als typisch für einen nicht genügend durch sozialistische Theorie gefestigten bürgerlichen Intellektuellen. Roth gehörte der Gruppe 1925, einer Vereinigung linker Schriftsteller, an. Er unterzeichnete deren Resolutionen und verfolgte ihre Aktivitäten, nahm aber an den Treffen nicht teil.

Während sich Roth in frühen journalistischen Arbeiten sehr monarchiekritisch zeigte, wandelte sich diese Position später zu einer Idealisierung der Habsburger Monarchie. Er sah zwar die Fehler und Versäumnisse des nicht mehr existierenden österreichischen Kaiserreichs, malte aber gleichzeitig in romantischer Verklärung die Utopie eines Österreich, wie es hätte sein können oder sein sollen. Den Versuch der Transferierung Österreichs ins Mythisch-Utopische unternahm er wie weitere Vertreter dieser spezifischen k. u. k.-Nostalgie, etwa Fritz von Herzmanovsky-Orlando (Tarockanien) und Robert Musil („Kakanien“).

Als sich die Diktatur des Nationalsozialismus abzeichnete und Wirklichkeit wurde, sah Roth in Monarchie und katholischer Kirche die einzigen Kräfte, denen er zutraute, der „braunen Pest“ hinreichenden Widerstand entgegensetzen zu können − wenn sie sich dazu entschließen könnten. Er verstärkte dabei seine Selbststilisierung als katholischer österreichischer Offizier und unterstützte die Sache der Monarchisten durch Artikel und politische Arbeit. In seinen letzten Jahren suchte er den Kontakt zu legitimistischen Kreisen um den Thronprätendenten Otto von Habsburg und reiste in dessen Auftrag am 24. Februar 1938 (wenige Tage vor dem Anschluss Österreichs) nach Wien mit dem Ziel, den österreichischen Bundeskanzler Kurt Schuschnigg zu einer Abdankung zugunsten Otto von Habsburgs zu überreden. Roth gelang es nicht, mit Schuschnigg zu sprechen, und der Wiener Polizeipräsident Michael Skubl riet ihm, unverzüglich wieder nach Paris zurückzukehren.

Im Zentrum wichtiger Werke Roths der 1930er Jahre steht der Untergang Österreichs als Metapher für den Verlust von Heimat schlechthin, so in "Radetzkymarsch" (1932) und (an diesen erzählerisch anschließend) "Die Kapuzinergruft" (1938), sowie in der Erzählung "Die Büste des Kaisers" (1934). In seinem Vorwort zum "Radetzkymarsch"-Vorabdruck in der "Frankfurter Zeitung" schreibt er:

Dieses Gefühl von Verlorensein und Entwurzelung wiederholt das Erlebnis des frühen Verlustes des Vaters. Roth gestaltet es darüber hinaus als das Lebensgefühl der galizischen Juden und der Juden überhaupt, etwa in dem Essay "Juden auf Wanderschaft". Als explizit jüdische bzw. sich primär mit jüdischer Thematik befassende Werke gelten die Erzählung "Der Leviathan" und der Roman "Hiob".

Roth transformierte in seinen letzten Lebensjahren die Sehnsucht nach einer Heimkehr in die (auch religiöse) Geborgenheit der jüdischen Kultur des „Schtetl“ ins Katholische, etwa in der "Legende vom Heiligen Trinker", wo der von Wundern und Gottesgnade geradezu verfolgte obdachlose Trinker Andreas Kartak im Tod Erlösung und Heimkehr findet.

In seinem Vortrag auf einem internationalen Symposium in Stuttgart würdigte Marcel Reich-Ranicki 1989 das Romanwerk Joseph Roths. Insbesondere hob der Redner Roths Abneigung gegen das Monumentale sowie den kindlich-naiv anmutenden Duktus der ruhigen, abgeklärten, formvollendeten Sprache des Romanciers hervor.

In seiner Heimatstadt Brody erinnert eine kleine in Ukrainisch und Deutsch beschriftete Gedenktafel an den Sohn der Stadt. Im Jahr 2001 wurde in Wien Leopoldstadt (2. Bezirk) die "Joseph-Roth-Gasse" nach ihm benannt.

Unweit ehemaliger Berliner Wirkungsstätten Joseph Roths befindet sich in der Potsdamer Straße die "Joseph-Roth-Diele", eine mit Briefen, Bildern und Büchern des Schriftstellers dekorierte Gaststube.





Roths Teilnachlass liegt im Deutschen Literaturarchiv Marbach. Einzelne Stücke davon sind im Literaturmuseum der Moderne in Marbach in der Dauerausstellung zu sehen, insbesondere das Manuskript zu "Hiob" sowie sein "Radetzkymarsch" im 70 Folgen umfassenden Zeitungsvorabdruck aus der Frankfurter Zeitung.











</doc>
<doc id="14528" url="https://de.wikipedia.org/wiki?curid=14528" title="Johann Daniel Titius">
Johann Daniel Titius

Johann Daniel Titius (auch: "Tietz, Tietze"; * 2. Januar 1729 in Konitz; † 11. Dezember 1796 in Wittenberg) war ein deutscher Gelehrter, der sich unter anderem mit Astronomie, Physik und Biologie befasste. Sein offizielles botanisches Autorenkürzel lautet „Titius“.

Anfänglich besuchte er die Schule seiner Heimatstadt. Nach dem frühen Tod seines Vaters wurde Titius von seinem Onkel Michael Christoph Hanow, der Professor in Danzig war, erzogen und bezog das dortige Gymnasium. Nach dem Besuch des Gymnasiums in Danzig nahm er ein Studium an der Universität Leipzig auf, wo er 1752 den akademischen Grad eines Magisters erlangte (mit einer Magisterarbeit über das Mondlicht) und als Privatdozent an der philosophischen Fakultät aufgenommen wurde. 1756 übernahm er an der Universität Wittenberg die Professur der niederen Mathematik und übernahm 1761 die öffentlichen Vorlesungen über Physik für den aus Wittenberg abwesenden Georg Matthias Bose auf. Da Bose nicht mehr wiederkehrte, erhielt Titius die Physikprofessur vom polnischen König und sächsischen Kurfürsten aufgetragen. Er unterrichtete auch Philosophie, natürliche Theologie und Naturrecht. 1768 wurde er Rektor der Universität. Angebote auf eine Professur in Göttingen, Helmstedt, Kiel und Danzig schlug er aus.

Als Gelehrter war er sehr vielseitig. Heute ist er vor allem für die Aufstellung einer Formel von 1766 bekannt, die eine gewisse Regelmäßigkeit in den mittleren Abständen der Planeten von der Sonne mathematisch beschreibt und als Titius-Bode-Reihe bekannt ist. Als Physiker war er ein Anhänger von René Descartes und betonte die Rolle des Experiments, war aber auch an wirtschaftlichen Anwendungen interessiert. Insbesondere befasste er sich mit Thermometern. In der Biologie folgte er Carl von Linné. Er gab wissenschaftliche Zeitschriften heraus (und förderte Nachdrucke der Philosophical Transactions der Royal Society) 

Er schrieb auch in nur einem Jahr (1753–1754) eine deutsche Gesamtübersetzung der "Essais" von Michel de Montaigne, die lange Zeit als die beste anerkannt war, zumindest bis 1998, als die Neuübersetzung von Hans Stilett erschien. Seine Übersetzungen von Montaigne und der "Discours sur les sciences et les arts" von Jean-Jacques Rousseau unternahm er auf Anregung von Johann Christoph Gottsched. Er schrieb theologische und philosophische Aufsätze im Sinn der Natürlichen Religion der Aufklärungszeit und historische Werke (Geschichte Westpreußens, Geschichte der Elbübergänge bei Wittenberg).

Er war Mitglied zahlreicher Akademien wie den naturforschenden Gesellschaften in Berlin und Danzig, der Leipziger Ökonomischen Societät. (1767) und der Akademie Gemeinnütziger Wissenschaften in Erfurt und auf literarischem Gebiet der Deutschen Gesellschaften in Jena, Erlangen, Leipzig und Bernburg und Mitgründer der Wittenberger Deutschen Gesellschaft.

Zu Ehren von Johann Daniel Titius wurde der Asteroid (1998) Titius nach ihm benannt.

als Autor

als Herausgeber

als Respondent

als Übersetzer




</doc>
<doc id="14529" url="https://de.wikipedia.org/wiki?curid=14529" title="Konitz">
Konitz

Konitz steht für:
Konitz ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="14530" url="https://de.wikipedia.org/wiki?curid=14530" title="Christianisierung">
Christianisierung

Christianisierung (von kirchenlateinisch: ) bezeichnet die Ausbreitung des Christentums als vorherrschende Religion in zuvor mehrheitlich nicht christlich geprägten Regionen oder Ländern. Zwar gab es im Römischen Reich nach der konstantinischen Wende hin zum Christentum im frühen 4. Jahrhundert kaum repressive Maßnahmen gegen Pagane, doch verbesserten sich für Christen die Aufstiegschancen ganz erheblich. Gesetze gegen pagane Kultpraktiken wurden erst ab Ende des 4. Jahrhunderts zunehmend erlassen, als das Christentum zur Staatsreligion im Imperium erhoben wurde. Etwa zeitgleich wurde das äthiopische Königreich von Aksum christlich. Im europäischen Frühmittelalter wurde die Zwangsbekehrung von kirchlicher Seite offiziell abgelehnt, doch waren die mit militärischen Mitteln vorangetriebenen Missionierungen teils mit erheblichem Zwang und Gewalt verbunden (wie die Sachsenkriege). Dagegen erfolgte die Missionierung in Nordeuropa und weiten Teilen Osteuropas weitgehend friedlich.

Im Unterschied zur individuellen Bekehrung eines Einzelnen beschreibt die Christianisierung den in historischer Dimension verlaufenden Prozess, bei dem ganze Völker oder Kulturkreise mehrheitlich den christlichen Glauben annehmen. Dies kann aus freiem Willen geschehen, jedoch auch durch Gewalt erzwungen werden.

Mission und Christianisierung sind verwandte Begriffe, wobei Mission sich auf den theologischen Aspekt und Christianisierung sich auf den langfristigen kulturellen und historischen Aspekt bezieht. Christianisierung ist Mission ganzer Völker, im historischen Horizont betrachtet.

Viele der ersten Kirchen wurden auf oder in ehemaligen paganen (heidnischen) Kultstätten wie Hainen errichtet, da diese Örtlichkeiten bereits als heilig galten und die zu Missionierenden sich dort versammelten. Andere heidnische Orte, wie viele der Hünengräber in Norddeutschland, wurden zu Zwecken der Abschreckung mit unheimlichen Namen versehen ("Teufelssteine", "Teufelsbackofen" etc.), die sie teilweise bis heute tragen. Diese Entwicklung ist in Skandinavien nur stark abgemildert anzutreffen.

Manchmal wurden Götterbilder, heilige Steine oder ganze Monumente in Kirchen eingebaut, zu Kirchen umgebaut oder zu christlichen Symbolen umgestaltet. Beispiele sind die Christianisierten Megalithmonumente, der in der Kirche von Altenkirchen verbaute Svantevitstein und das aus einem Menhir herausgearbeitete Fraubillenkreuz auf dem Ferschweiler Plateau in der Eifel. Heidnische religiöse Bräuche erhielten sich nach der Christianisierung oft noch lange als Brauchtum.

Die Christianisierung war nach den ersten Jahrhunderten der Missionierung durch Mönche und Prediger später häufig auch eine Machtfrage, in Schlachten unterlegene Gruppen und Stämme des Frühmittelalters etwa ließen sich als Zeichen der Unterwerfung taufen oder wurden (kirchenrechtlich allerdings abgelehnt) zwangsgetauft. Weltliche und göttliche Macht gingen, wie bei Karl dem Großen, Hand in Hand. In vielen Fällen wurden bei der Christianisierung auch Elemente oder Teile der heidnischen religiösen Kultur übernommen.

Es gibt jedoch auch Beispiele, wie das Christentum in den lokalen kulturellen Kontext übertragen und mit den Mitteln dieser Kultur ausgedrückt wurde. Dazu gehören der sächsische Heliand ebenso wie Navidad Nuestra in Südamerika, das Masai-Bekenntnis aus Ostafrika und insbesondere die Afrikanischen Kirchen.

In einigen Fällen entwickelten sich nicht nur neue Formen des Christentums im Kontext der lokalen Kultur, sondern diese Völker wurden durch die Christianisierung zu einer kulturellen Identität befähigt (z. B. die Slawische Christianisierung durch Kyrill und Method).

Andererseits wurden später auch intakte Kulturen zerstört (Malaiischer Archipel, Ozeanien). Oftmals gingen (ab der frühen Neuzeit) christliche Mission und Kolonialisierung Hand in Hand, während anderenorts scharfe Gegensätze zwischen Missionaren und Kolonisatoren aufbrachen.

Christianisierung in moderner Zeit geht oft einher mit Entwicklungshilfe. Es werden christliche Werte vermittelt und Infrastrukturen, wie etwa Schulen, Pflegeheime oder auch Krankenhäuser, errichtet. Zugleich werden Kirchen, Klöster oder Missionsstationen gebaut. Es erwachsen neue kulturelle Macht-Zentren, die in der Folge ganze Regionen dominieren. So stellte man in Lateinamerika die Religion der indigenen Völker als Aberglauben dar, ihren Lebensstil als primitiv, und glich sie europäischen Normen wie Sesshaftigkeit, Ackerbau, Kleidung an, was bereits Alexander von Humboldt in seinen Reiseberichten 1800 ff zu kritischen Bemerkungen veranlasste.

Christianisierung ist auf viele Arten geschehen, und in den meisten Fällen spielten mehrere Faktoren zusammen.

Faktoren auf religiösem Gebiet:

Faktoren auf politischem Gebiet:

Die Gewinnung des jeweiligen Herrschers für das Christentum war in manchen Fällen ein wesentlicher Faktor für die Christianisierung, in anderen kam sie erst nach ziemlich vollendeten Tatsachen oder spielte überhaupt keine Rolle (z. B. Nordamerika). In Kulturen mit ausgeprägtem Gemeinschaftsbewusstsein konnte die Bekehrung des lokalen Oberhaupts auch fast selbstverständlich zur Bekehrung seiner Gefolgsleute führen, da die Gemeinschaft für alle wesentlicher war als die individuelle Entscheidung über die Religionszugehörigkeit.

In manchen Fällen führte die Christianisierung zur Eingliederung in die katholische Kirche, in anderen Fällen bildeten sich eigenständige lokale Kirchen (keltisches Christentum in Irland, bzw. in Russland, Nordamerika, afrikanische Kirchen). An manchen Orten wurde das Christentum zur Staatsreligion erklärt, andere weitestgehend christianisierte Länder lehnen staatlichen Einfluss auf die Religion prinzipiell ab.

Das erste christianisierte Reich war das Königreich Armenien. Um 301 kam es der Legende durch die Bekehrung des Königs Trdat III. zur Proklamation des Christentums als Staatsreligion, tatsächlich dürfte sich dieses Ereignis aber wohl erst 314/315 abgespielt haben, da sich um 301 die diokletianische Christenverfolgung zutrug.

Die Christianisierung des "Imperium Romanum" zog sich seit der konstantinischen Wende über Jahrhunderte hin und war erst am Ende der Spätantike abgeschlossen, wenngleich alle nichtchristlichen Religionen bereits 380 verboten worden waren. Im 5. Jahrhundert waren die Anhänger der paganen Kulte bereits in der Minderheit und auch Mitglieder der Oberschicht wandten sich verstärkt dem Christentum zu. Zwar wurden ab Ende des 4. Jahrhunderts zunehmend Gesetze gegen pagane Kulthandlungen erlassen, doch wurden sie zunächst anscheinend kaum ernsthaft umgesetzt. Pagane Amtsträger waren noch im 5. Jahrhundert im römischen Staatsdienst geduldet. Eine Zwangschristianisierung fand nicht statt, vielmehr verloren die paganen Kulte an Anziehungskraft, während sich für Christen neue Aufstiegschancen ergaben. Am Ende der Spätantike wurde jedoch auch stärker gegen die Überreste der paganen Kulte vorgegangen, so zur Zeit Justinians.

Eine herausragende Rolle in der frühmittelalterlichen Missionierung von Mitteleuropa um das 6. Jahrhundert spielten iro-schottische Mönche sowie die Einflüsse Roms. Irland wurde seit dem 5. Jahrhundert von Patrick von Irland christianisiert. Dort bildete sich eine eigenständige Irische Kirche und ein ganz unabhängiges keltisches Christentum heraus, das nicht durch Bischöfe, sondern durch Klöster geleitet wurde. In diesen Abteien gestaltete sich das Leben nach anderen Regeln, und es entwickelte sich eine hohe Kultur der Buchkunst mit reich verzierten Bibeln und anderen Büchern. Da Irland abseits der Ströme der Völkerwanderung lag, blieb hier ein großer Teil des Wissens der Antike erhalten und wurde auch durch Klöster bewahrt. Dabei hatten es die irischen Mönche, da es an einer zentralstaatlichen Einheit fehlte, immer wieder mit lokalen Herrschern zu tun, die kirchenfeindlich eingestellt waren und ebenso wie die Wikinger Klöster ausraubten. Dies hielt im Wesentlichen bis ins Hochmittelalter an. Erst im 12. Jahrhundert wurde die Irische Kirche auf Beschluss der Synode von Cashel nach römischem Vorbild umgestaltet, wobei Rom unter anderem wegen der anglo-normannischen Besetzung schnell wieder an Einfluss verlor. Mönche der irischen Kirche zogen sich immer wieder in Eremitagen und auf einsame Inseln zurück oder verließen die Insel und waren missionarisch aktiv.

Das Wandermönchtum hatte hier eine wichtige Bedeutung. Im 6. Jahrhundert wurde nicht nur die Missionierung Schottlands und Nordenglands unter Columban von Iona begonnen, sondern irische Mönche reisten auch nach Gallien, Süddeutschland und der Schweiz (Columban von Luxeuil), wo sie Klöster gründeten. Im Frankenreich wurde 499 mit der Taufe Chlodwigs auch die bis dahin heidnische fränkische Oberschicht römisch-katholisch. In der Folge von Columbans Missionsreisen auf dem Festland, war die iroschottische Mission so erfolgreich, dass dort im 7. Jahrhundert rund 300 Klöster gegründet wurden. Zuvor war fast ausschließlich die Stadtbevölkerung christlich geworden, doch jetzt gelang auch eine wirksame Christianisierung in ländlichen Gebieten.

Im 7. Jahrhundert wurde England gleichzeitig von Iro-schottischen und römisch-katholischen Missionaren missioniert, was wegen des unterschiedlichen Kirchenverständnisses zu Konflikten führte. Auf der Synode von Whitby wurde 664 zugunsten des römischen Ritus entschieden. Auch von England aus reisten zahlreiche Missionare auf den Kontinent, die sich insbesondere den mit den Angelsachsen verwandten germanischen Völkern widmeten. Die herausragende Figur dabei war Bonifatius, der insbesondere in Franken und Hessen zahlreiche Klöster gründete, aber auch in Thüringen und Friesland predigte.

Bonifatius betrachtete das keltische Christentum als ungenügend und verlangte ihre Unterwerfung unter Rom. Keltische Geistliche, die nicht dem Papst unterstellt waren, bezeichnete er als falsche Propheten, Götzendiener und Ehebrecher (da sie als Geistliche verheiratet waren). Die auf dem gallischen Konzil von Autun als verbindlich verabschiedete Ordensregel Benedikts wurde von ihm verbreitet und sollte die iroschottische Regel Columbans verdrängen. Insbesondere in Bayern traf er dabei auf energischen Widerstand der iroschottisch geprägten Christen.

Die Sachsen wurden im 8. und 9. Jahrhundert durch Karl den Großen teilweise gewaltsam zum Christentum gebracht. Karl besiegte um 800 die Sachsen in Norddeutschland, und erließ in der Capitulatio de partibus Saxoniae Vorschriften wie z. B.:
Die Ottonen wurden anschließend im 10. Jahrhundert eine starke Stütze des westeuropäischen Christentums. Der Nordosten Deutschlands kam erst im 10. Jahrhundert zum Christentum. Die etwa 820 angelaufene Christianisierung Skandinaviens war etwa um 1030 abgeschlossen. Eine wichtige Rolle hierbei spielte Missionsbischof Ansgar.

Böhmen wurde in erster Linie von Deutschland her missioniert. Im 10. Jahrhundert war Wenzel von Böhmen ein christlicher Herrscher, der von seinem heidnischen Bruder Boleslav I. ermordet wurde. Dessen Sohn, Boleslav II. förderte allerdings wieder aktiv das Christentum, gründete Klöster und baute Kirchen, und vervollständigte die nominelle Christianisierung Böhmens.

Um einer möglichen Zwangsbekehrung der Länder Polens durch das Heilige Römische Reich zu entgehen, entschloss sich der Polanenfürst Mieszko I. im Jahr 966 durch seine Heirat mit Dubrawka, einer Tochter des Přemysliden Boleslav I., das Christentum von den Böhmen (Tschechen) anzunehmen.

Die Christianisierung Ungarns erfolgte im späten 10. und frühen 11. Jahrhundert und wurde hauptsächlich durch das Königshaus erreicht, insbesondere durch Stephan I.

Die Völker des Baltikums, die Prußen, Wenden, Letten und andere baltischen Stämme, sowie die Esten wurden erst im 10. bis 13. Jahrhundert im Zuge der deutschen Ostsiedlung zwangschristianisiert, wobei das Großfürstentum Litauen nicht erobert werden konnte und sich erst Ende des 14. Jahrhunderts zum Christentum bekehrte.

Wann die Christianisierung einer Region oder einer Gruppe abgeschlossen war und ab wann die vorchristlichen Kulte nur noch in Brauchtum und Aberglaube fortbestanden, lässt sich in der Regel kaum exakt bestimmen.

Die Christianisierung Osteuropas geschah im Wesentlichen von Konstantinopel aus.

Vom siebten bis neunten Jahrhundert wurden die Serben missioniert. Im neunten Jahrhundert übersetzten die aus Saloniki stammenden Brüder Kyrill und Method von Saloniki Teile des Neuen Testaments und der Liturgie ins Slawische und schrieben sie in der von Kyrill entwickelten glagolitischen Schrift nieder. Sie missionierten im Auftrag von Photius I. in Böhmen und Mähren, wo sie in Streitigkeiten zwischen der Westkirche und der Ostkirche verwickelt wurden. Mähren bekannte sich zum Christentum, wurde aber nach dem Einfall der Ungarn wieder mehrheitlich heidnisch.

Die Übersetzungen von Kyrill und Method spielten eine wesentliche Rolle bei der Verbreitung des Christentums in Bulgarien und nach 950 auch in Russland. 864 wurde Boris, der Khagan der Bulgaren getauft, was bald zu einer Massenbekehrung führte. Bulgarien war das erste Land, das offiziell eine slawische Liturgie einführte. Und dem Boris' Sohn Simeon wurde das Land vollständig christianisiert. 917 erklärte sich die Bulgarische Kirche als autokephal unabhängig und wurde ein eigenes Patriarchat. Die Kirche war in der Lehre orthodox, in der Verwaltung aber unabhängig – die erste von mehreren slawischen Kirchen, die nach diesem Muster selbständig wurden.

Photius I. sandte im neunten Jahrhundert auch die ersten Missionare nach Russland. In der Mitte des zehnten Jahrhunderts gab es in der Hauptstadt Kiew eine christliche Kirche und die Großfürstin Olga von Kiew ließ sich taufen. Erst unter ihrem Enkel Wladimir I. (960 – 1015) kam es zu einer Massenbekehrung von Kiew und der Umgebung (siehe Christianisierung der Rus). 991 wurde die Bevölkerung von Nowgorod getauft. Beim Tod Wladimirs 1015 gab es drei Bistümer in Russland. Im zwölften Jahrhundert breitete sich das Christentum entlang der oberen Wolga aus. Die Mission geschah in erster Linie durch Mönche und es wurden zahlreiche Klöster gegründet.

Die Kolonialregime europäischer Mächte seit dem 15. Jahrhundert bedurften der Rechtfertigung und der Vereinbarkeit vor allem mit der christlichen Religion, welche die kolonisierenden Eroberer mit ihren europäischen Entsendemetropolen verband. Mit der päpstlichen Bulle Inter caetera waren den Spaniern 1493 beispielsweise die Rechte an neuen Ländern in Amerika zugebilligt worden, denen sie den katholischen Glauben bringen sollten. Der europäische Kolonialismus im Zeitalter des Imperialismus begünstigte ebenfalls weder eine Kultur- noch eine Religionssynthese (Synkretismus). Bereits die frühesten spanischen und englischen Kolonialtheoretiker stilisierten die Eroberungen zu einer Heiden-Missionierung im Rahmen eines göttlichen Heilsplans oder der „Zivilisierung“ der „Barbaren“. Auch der spätere US-amerikanische und japanische Kolonialismus bedienten sich solcher sendungsideologischen Rhetorik.

Im Mittelalter wurden Zwangschristianisierungen von kirchlicher Seite oft abgelehnt, da die Ansicht vorherrschend war, dass der mit Gewalt aufgezwungene Glaube nicht dauerhaft sein könne. So äußerte zur Zeit Karls des Großen etwa Alkuin Kritik am königlichen Vorgehen: Zur Taufe könne ein Mensch getrieben werden, nicht aber zum Glauben. Kirchenrechtlich war die erzwungene Taufe ebenfalls untersagt; niemand sollte mit Gewalt zum Glauben gezwungen werden. Die Christianisierung Nord- und weiter Teile Osteuropas erfolgte weitgehend friedlich und auch nicht im Rahmen militärischer Expansion (wie bei den Karolingern und einigen folgenden Königen Ostfrankens), sondern durch Missionierungen. Dennoch kam es infolge der Kreuzzüge immer wieder auch zu gewaltsamen Übergriffen gegen Nichtchristen.

Heute wird die großflächige Zwangschristianisierung des Mittelalters und der Kolonialzeit differenziert betrachtet und mehrheitlich kritisch gesehen.





</doc>
<doc id="14532" url="https://de.wikipedia.org/wiki?curid=14532" title="Memel (Begriffsklärung)">
Memel (Begriffsklärung)

Memel steht für:
Memel ist der Familienname folgender Personen:
Siehe auch:



</doc>
<doc id="14533" url="https://de.wikipedia.org/wiki?curid=14533" title="Gefäß">
Gefäß

Gefäß steht für



Siehe auch:



</doc>
<doc id="14535" url="https://de.wikipedia.org/wiki?curid=14535" title="Nikomachische Ethik">
Nikomachische Ethik

Die Nikomachische Ethik (altgriechisch , "ēthiká Nikomácheia") ist die bedeutendste der drei unter dem Namen des Aristoteles überlieferten ethischen Schriften. Da sie mit der "Eudemischen Ethik" einige Bücher teilt, ist sie möglicherweise nicht von Aristoteles selbst in der erhaltenen Form zusammengestellt worden. Weshalb die Schrift diesen Titel trägt, ist unklar. Vielleicht bezieht er sich auf Nikomachos (Sohn des Aristoteles) oder den Vater, der ebenfalls Nikomachos hieß.

Das Werk will einen Leitfaden geben, wie man ein guter Mensch werden und ein Leben im Sinne der Eudaimonia führen kann. Da hierfür der Begriff des Handelns zentral ist, ist bereits im ersten Satz davon die Rede: „Jedes praktische Können und jede wissenschaftliche Untersuchung, ebenso alles Handeln und Wählen, strebt nach einem Gut, wie allgemein angenommen wird.“
Ein Gut kann dabei entweder nur dazu da sein, ein weiteres Gut zu befördern (es wird dann zu den "poietischen" Handlungen gezählt), oder es kann ein anderes Gut befördern und gleichzeitig „um seiner selbst willen erstrebt werden“ (es hat dann "praktischen" Charakter), oder aber es kann als höchstes Gut das Endziel allen Handelns darstellen (= absolute "praxis"). Dadurch wird das Werk durch die Frage bestimmt, wie das höchste Gut, oder auch das höchste Ziel, beschaffen und wie es zu erreichen ist.

Die erste Antwort des Aristoteles auf die Frage nach dem Wesen des höchsten Gutes ist, dass die Glückseligkeit "(eudaimonía)" das höchste Gut ist. Sie ist ein seelisches Glück.
Das folgt für Aristoteles daraus, dass die Glückseligkeit für sich selbst steht – sie ist nicht, wie andere Güter, lediglich Mittel zum Zweck. Im Gegensatz zu anderen Gütern erstreben wir Glückseligkeit um ihrer selbst willen. Sie ist, wie Aristoteles sagt, „das vollkommene und selbstgenügsame Gut und das Endziel des Handelns.“ (1097b20)

Um den eigentlichen Inhalt der Glückseligkeit zu bestimmen, führt Aristoteles das Ergon-Argument ein. Hierbei geht er von einem Essentialismus aus, welcher besagt, dass jedes Wesen durch Eigenschaften gekennzeichnet ist, die es ermöglichen, dieses Wesen von anderen Wesen abzugrenzen. Des Weiteren verfolgt er einen eigenen Perfektionismus, welcher die Erfüllung der Bestimmung des Wesens von der Ausbildung seiner Wesenszüge abhängig macht.

Das Wesen des Menschen findet man in der Betrachtung seiner spezifisch eigentümlichen Leistung, welche ihn von anderen Lebewesen unterscheidet. Diese ist das Tätigsein der Seele gemäß dem rationalen Element (dem Nachdenken, der Vernunft) oder jedenfalls nicht ohne dieses. Daneben ist es entscheidend, dass der Mensch seine Vernunft sowohl auf vollendete Weise einsetzt als auch in seinem ganzen Leben und mehr zur Geltung bringt. „Und mehr“ bedeutet in diesem Fall, dass sogar die Hinterlassenschaften des Menschen (etwa Kinder) von der intensiven Nutzung seiner Vernunft zeugen.

Diese drei Argumente – Tätigsein der Seele gemäß der Vernunft, Tätigsein auf eine vollendete Weise und in einem vollen Leben – werden allgemeinhin als erste Glücksdefinition des Aristoteles betrachtet.

Zur Erlangung von Glückseligkeit ist, so gesteht Aristoteles zu, nicht nur vernunftgemäße Betätigung der Seele nötig, sondern auch erstens äußere und zweitens körperliche Güter. Äußere Güter sind etwa Reichtum, Freundschaft, Herkunft, Nachkommen, Ehre und ein günstig gestimmtes persönliches Schicksal. Gesundheit, Schönheit, physische Stärke, Sportlichkeit entsprechen körperlichen bzw. inneren Gütern des Körpers. Aus der vernunftgemäßen Betätigung der Seele ergeben sich die seelischen Güter, die Tugenden.

Die äußeren Güter ordnet Aristoteles dem zufälligen Glück zu, der "eutychia". Körperliche Güter sind teils ebenfalls von Zufall abhängend (z. B. unter Umständen Schönheit), teils aber auch auf eigenes Handeln (z. B. durch Sport oder Ernährung) zurückzuführen. Seelische Güter dagegen können nur von wirklich guten Menschen erlangt werden. Alles zusammen ergibt eine Glückseligkeit, die Aristoteles in seinem Werk nur kurz erwähnt: Die des „vollkommen glücklichen Menschen vor und nach seinem Leben“. Dieser Mensch ist dann wahrhaft glücklich, oder anders: er ist "makarios".

Aristoteles definiert die Glückseligkeit als eine Tätigkeit der Seele gemäß der vollkommenen Tugend "(arete)" in einem vollen Menschenleben. Allerdings können bestimmte dianoetische (verstandesmäßige) Tugenden nicht von jedem in vollkommener Form erreicht werden. Daher gibt es laut Aristoteles zwei grundlegende Weisen, wie ein glückliches Leben möglich ist.

Die am stärksten vollkommene Glückseligkeit besteht im "bios theoretikos", im kontemplativen Leben. Dieses schließt wissenschaftliche Betätigung, Gebrauch der Vernunft (Nous) in die für den Erkenntnisgewinn grundlegenden Wahrheiten, und Erlangung von Weisheit ein. Auch die übrigen Tugenden sind bei dieser Lebensweise vollkommen ausgebildet, stehen aber nicht im Mittelpunkt des Handelns.

Da einige Menschen sich von Natur aus nicht zu dieser Lebensweise eignen, weil sie laut Aristoteles insbesondere nicht in vollendeter Form über die Vernunft verfügen und dieses auch als einzige Tugend überhaupt nicht angebildet werden kann, gibt es eine zweite Lebensweise. Der "bios praktikos", das praktische Leben, beschränkt sich auf den vollkommenen Gebrauch der Vernunft in Bezug auf kontingente Tatsachen, d. h. auf den Gebrauch von Klugheit und Kunstfertigkeit in Verbindung mit den ethischen Tugenden.

Die Tugenden sind seelische Güter. Aristoteles teilt diese entsprechend der Seele in dianoetische Tugenden, welche aus Belehrung entstehen, und ethische Tugenden, die sich aus der Gewohnheit ergeben. In Analogie zum Beherrschen eines Musikinstruments erwirbt man die Tugenden, indem man sie ausübt.

Aristoteles unterteilt die Seele in einen spezifisch menschlichen, vernunftbegabten Teil (λόγον ἔχον) und einen vernunftlosen Teil (ἂλογον). Der vernunftlose Seelenteil (ἂλογον) besteht aus einem vegetativen Teil, der sich aus Wachstum (αὔξησις) und Ernährung (θρεπτικόν) zusammensetzt, aber auch das Strebevermögen der Seele beherbergt (ὂρεξις). Bis hierher ist der Mensch mit den Pflanzen bzw. Tieren auf einer Stufe, es sind nur Herzschlag und Stoffwechsel sowie Wachstum und Fortpflanzung sichergestellt. Der Mensch ist jedoch auch ein vernunft- und sprachbegabtes Wesen (ζῷον λόγον ἔχον), sein Strebevermögen (ὂρεξις) ist mit dem vernünftigen Teil der Seele verbunden. Deswegen kann er die Gemütsbewegungen wie Furcht, Zorn, Mitleid und andere zwar nicht in ihrem Aufkommen kontrollieren, wohl aber in der weiteren „Verarbeitung“.

Der vernunftbegabte Teil hat ebenso wie der vernunftlose Anteil am Strebevermögen und ist "Ort" der menschlichen Vernunft (λόγος). Diese besteht aus zwei Teilen: Der sich selbst genügenden Vernunft (ἐπιστημονικόν) und der angewandten Vernunft, also Überlegung (λογιστικόν) und Beratung (βουλευτικόν). In der selbstgenügsamen Vernunft (ἐπιστημονικόν) geht es um Wissenschaft (ἐπιστήμη), philosophische Weisheit (σοφία) und das reine Denken, das – losgelöst von allem – völlige geistige Selbstreferentialität bedeutet (νοῦς). Sie bezieht sich auf die unveränderlich seienden Dinge, auf die Mathematik, den Kosmos und die Metaphysik.

In der angewandten Vernunft geht es um praktische Kunstfertigkeit (τέχνη), herstellende Kunstfertigkeit (ποίησις) und Klugheit (φρόνησις). Sie bezieht sich auf das praktische Leben. Die Klugheit (φρόνησις) spielt eine große Rolle, denn sie wirkt sich wieder auf das Strebevermögen aus. Während alle bisherigen Tugenden dianoetisch waren, sich also direkt aus der Vernunft ergeben haben, entstehen im Zusammenspiel von Klugheit (φρόνησις) und Strebevermögen (ὂρεξις) die ethischen Tugenden, die durch Entschluss (προαίρεσις) und Gewöhnung (ἐθίζειν) zur Haltung (ἕξις) werden können. Die Klugheit (φρόνησις) bringt die affektiven Extreme, die im Strebevermögen (ὂρεξις) aufkommen, in eine tugendhafte Mitte (μεσότης). So bewirkt sie beispielsweise, dass der Mensch weder zu feige noch zu tollkühn in seiner Haltung ist, weder zu gefallsüchtig noch zu streitsüchtig. Tugendhaft sein bedeutet bei Aristoteles nicht, frei von Affekten zu sein, sondern seine eigenen Affekte zu beherrschen.

Die ethischen Tugenden beziehen sich auf die Leidenschaften und die Handlungen, die aus diesen Leidenschaften hervorgehen. Sie bestehen in der Zähmung und Steuerung des irrationalen, triebhaften Teils der Seele. Dabei postuliert Aristoteles eine Ethik des Maßhaltens. Bei den ethischen Tugenden gilt es, die richtige Mitte "(mesotes)" zwischen Übermaß und Mangel zu treffen. Am besten lässt sich dies am Beispiel der Tapferkeit verdeutlichen. Die Tapferkeit bewegt sich zwischen den Extremen der Feigheit und der Tollkühnheit – weder die Feigheit ist wünschenswert, noch eine übersteigerte, vernunftlose Tapferkeit, die Aristoteles als Tollkühnheit bezeichnet. Der Tapfere hält hingegen das richtige Maß. Ähnlich verhält es sich für andere ethische Tugenden wie Großgesinntheit, Besonnenheit, richtige Ernährungsweise usw.

Um die Mitte "(mesotes)" zu verstehen und nachzuvollziehen, sollte man das irrationale und triebhafte Treiben der menschlichen Seele erlebt haben. So erhalte man ein Verständnis für die "mesotes" und könne verstehen, dass die Maßlosigkeit des Treibens zu nichts führt. Die Besonnenheit wird sich einstellen, wenn man verstanden hat, dass ein maßloses Treiben sowie ein vollkommener Rückzug des „Ich“ zu nichts führt, und erkennt, dass nur die Mitte zwischen beiden Extremen "(mesotes)" als richtiges Maß zählt.

Die ethischen Tugenden werden von den Menschen bewertet. Sie sind daher sittlich werthaftig. Von Wert kann aber nur etwas sein, das keine spontane Bewegung ist, sondern ein Dauerzustand. Aufgrund dessen definiert Aristoteles die ethische Tugend zu einer festen Grundhaltung "(hexis)" (siehe auch Habitus).

Die dianoetischen Tugenden lassen sich gemäß Aristoteles in zwei Teile gliedern: Diejenigen Tugenden, die sich auf kontingente Tatsachen beziehen, und diejenigen, die sich auf notwendige Tatsachen beziehen. Erstere sind die Kunstfertigkeit "(techne)", also ein spezifisches Herstellungswissen (z. B. die Fähigkeit des Tischlerns) und die weit wichtigere Klugheit "(phronesis)", die sämtliche ethischen Tugenden steuert und die richtige Anwendung dieser erkennen lässt.

Auf notwendige Tatsachen beziehen sich die Wissenschaft "(episteme)", welche die Fähigkeit des richtigen Schließens bedeutet, die Vernunft ("Nous") und die Weisheit "(sophia)". Die Vernunft ist eine Art „intuitiver Verstand“ bzw. ein Wissen von den „obersten Sätzen“ (1141a15 ff.), aus denen die Wissenschaft dann Schlüsse ziehen kann. Weil die Vernunft als einzige Tugend überhaupt nicht erworben werden kann, sondern jedem in unterschiedlich ausgeprägter Weise gegeben ist, bleibt ihre Ausübung den zufällig Begünstigten vorbehalten. Auch die Wissenschaft ist auf eine hervorragend ausgeprägte Vernunft angewiesen und die Weisheit ist das Vorhandensein von Vernunft und Wissenschaft. Daher kann das Leben in der reinen Schau der Wahrheit "(theoria)", der "bios theoretikos", nicht von jedem erreicht werden (siehe Abschnitt zur theoretischen und praktischen Lebensweise).

Die ethischen Tugenden stehen in engem Zusammenhang mit Lust und Schmerz. Die Hinwendung der Menschen zum Schlechten erklärt Aristoteles damit, dass die Menschen die Lust suchen und den Schmerz fürchten. Diese natürliche Verhaltensweise gilt es, durch Erziehung zum Guten zu beeinflussen und zu steuern. Aus diesem Grund rechtfertigt er auch Züchtigungen: „Sie sind eine Art Heilung, und die Heilungen werden naturgemäß durch das Entgegengesetzte vollzogen.“

Doch auch die Ausübung der Tugend ist mit dem Angenehmen und der Lust verbunden. Aristoteles differenziert aufgrund seiner Theorie der Seele zwischen körperlichen und geistigen Lüsten. Die körperlichen Lüste weisen auf Grundbedürfnisse des Menschen hin, sollten jedoch nicht, wie von den Hedonisten praktiziert, über das in diesem Rahmen sinnvolle Maß bedient werden. Geistige Lüste lassen sich mit Tugenden verbinden, wie etwa die Lust der intellektuellen Betätigung im Sinne der Wissenschaft oder ganz allgemein die Lust, Gutes zu tun. In diesem Sinne wird also ein tugendhafter Mensch ein lustvolles Leben führen.


In der Nikomachischen Ethik entwickelt Aristoteles auch eine Rangordnung ihm bekannter Staatsformen ihrer Tugendhaftigkeit nach. Für die beste Staatsform hält Aristoteles die Monarchie, die er mit der fürsorglichen Obhut des Vaters über seine Söhne vergleicht. Es folgt die Aristokratie als Herrschaft der Tugendhaftesten und Tüchtigsten, die ihre Entsprechung in der – Aristoteles zufolge auf natürlichem Vorzug beruhenden – Herrschaft des Mannes über die Frau finde. Schließlich lobt Aristoteles eine auf Zensus beruhende Verfassung, die er Timokratie nennt, und vergleicht sie mit der Freundschaft zwischen älterem und jüngerem Bruder.

Den drei tugendhaften Staatsformen steht in Aristoteles' Systematik jeweils eine „Entartung“ gegenüber. Die Monarchie verkomme zur Tyrannis, wenn der Alleinherrscher um seines eigenen Vorteils willen eine Gewaltherrschaft errichte - gleich einem Vater, der seine Söhne wie Sklaven behandelt. Die Aristokratie verwandele sich in eine Oligarchie, wenn eine geringe Zahl nicht tugendhafter, sondern habgieriger Machthaber die Herrschaft monopolisiere und die Staatsgüter unter sich aufteile. Aristoteles bemüht als Analogie hier das Bild der reichen Erbtochter, die trotz vermeintlichen Mangels des natürlichen Vorzugs der Männlichkeit Gewalt ausübe aufgrund von Reichtum und Macht.
Die Demokratie hält Aristoteles schließlich für die „am wenigsten schlechte“ ausgeartete Staatsform, da sie sich nur graduell (durch niedrigere Zulassungsbeschränkungen zur Bürgerschaft) von der Timokratie unterscheide. In Aristoteles' Parallelisierung von Staatsformen und Familienbeziehungen erscheint die Demokratie als ein Haus, „wo der Herr fehlt“, alle folglich gleichberechtigt sind „und jeder tut, was ihm gefällt.“




Textausgaben und Übersetzungen



</doc>
<doc id="14537" url="https://de.wikipedia.org/wiki?curid=14537" title="Partition (Mengenlehre)">
Partition (Mengenlehre)

In der Mengenlehre ist eine Partition (auch Zerlegung oder Klasseneinteilung) einer Menge "M" eine Menge "P", deren Elemente nichtleere Teilmengen von "M" sind, sodass jedes Element von "M" in genau einem Element von "P" enthalten ist.
Anders gesagt: Eine Partition einer Menge ist eine Zerlegung dieser Menge in nichtleere paarweise disjunkte Teilmengen.

Das Mengensystem (= die Mengenfamilie) formula_1 ist eine Partition der Menge formula_2. Die Elemente von formula_3 sind dabei paarweise disjunkte Teilmengen von formula_4. formula_3 ist jedoch keine Partition der Menge formula_6, weil 1 zwar in formula_7, aber in keinem Element von formula_3 enthalten ist.

Die Mengenfamilie formula_9 ist keine Partition irgendeiner Menge, weil formula_10 und formula_11 mit 2 ein gemeinsames Element enthalten, also "nicht" disjunkt sind.

Die Menge formula_12 hat genau 5 Partitionen:

Die einzige Partition der leeren Menge ist die leere Menge.

Jede einelementige Menge formula_18 hat genau eine Partition, nämlich formula_19.

Jede nichtleere Menge formula_4 hat genau eine einelementige Partition formula_21, man nennt sie die „triviale Partition“.

Die Anzahl formula_22 der Partitionen einer formula_23-elementigen Menge nennt man Bellsche Zahl (nach Eric Temple Bell). Die ersten Bellzahlen sind:

Ist eine Äquivalenzrelation ~ auf der Menge formula_4 gegeben, dann bildet die Menge der Äquivalenzklassen eine Partition von formula_26 die auch „Faktormenge“ formula_27 von ~ auf formula_4 genannt wird.

Ist umgekehrt eine Partition formula_3 von formula_4 gegeben, dann wird durch
eine Äquivalenzrelation definiert, etwas formaler:
In der Gleichheit formula_37 der Partitionen und der Gleichheit formula_38 der Relationen manifestiert sich eine Gleichwertigkeit von Äquivalenzrelationen und Partitionen.

Für eine feste natürliche Zahl formula_39 heißen ganze Zahlen formula_40 kongruent modulo formula_41 wenn ihre Differenz formula_42 durch formula_39 teilbar ist. Kongruenz ist eine Äquivalenzrelation und wird mit formula_44 bezeichnet. Die zugehörige Partition der Menge der ganzen Zahlen ist die Zerlegung in die Restklassen modulo formula_39. Sie lässt sich darstellen als
wobei
die Restklasse bezeichnet, die formula_48 enthält. (Man beachte, dass diese Notation für Restklassen nicht allgemein üblich ist. Sie wurde nur gewählt, um die obige allgemeine Konstruktion zu illustrieren.)

Sind "P" und "Q" zwei Partitionen einer Menge "M", dann nennen wir "P" „feiner als“ "Q", falls jedes Element von "P" Teilmenge eines Elements von "Q" ist. Anschaulich heißt das, dass jedes Element von "Q" selbst durch Elemente von "P" partitioniert wird.

Die Relation „feiner als“ ist eine Halbordnung auf dem System aller Partitionen von "M", und dieses System wird dadurch sogar zu einem vollständigen Verband. Gemäß der oben erwähnten Gleichwertigkeit von Äquivalenzrelationen und Partitionen ist er isomorph zum Äquivalenzrelationenverband auf "M".



</doc>
<doc id="14538" url="https://de.wikipedia.org/wiki?curid=14538" title="Vereinigung">
Vereinigung

Vereinigung steht für:


Vereinigung heißen:
Siehe auch:


</doc>
<doc id="14543" url="https://de.wikipedia.org/wiki?curid=14543" title="Chromatische Zahl">
Chromatische Zahl

Die chromatische Zahl formula_1 (auch Knotenfärbungszahl oder kurz Färbungszahl, selten auch Farbzahl genannt) eines Graphen ist die kleinste Zahl formula_2, für die der Graph eine zulässige Knotenfärbung mit formula_2 Farben besitzt. (Eine Färbung heißt zulässig oder gültig, wenn es keine benachbarten Knoten gibt, die mit der gleichen Farbe gefärbt sind.) Die chromatische Zahl ist zugleich die kleinste natürliche Zahl λ, für die das chromatische Polynom formula_4 ist.

Die achromatische Zahl formula_5 eines Graphen formula_6 ist die größte Zahl formula_2, für die formula_6 eine gültige und vollständige Knotenfärbung mit formula_2 Farben hat. (Eine Färbung heißt vollständig, wenn es zu jedem Paar von verschiedenen Farben eine Kante gibt, deren Endknoten mit diesen beiden Farben gefärbt sind.) 

Die pseudo-achromatische Zahl formula_10 eines Graphen formula_6 ist die größte Zahl formula_2, für die formula_6 eine vollständige Knotenfärbung hat. Im Gegensatz zur achromatischen Zahl ist hier nicht die Gültigkeit der Färbung verlangt.



</doc>
<doc id="14550" url="https://de.wikipedia.org/wiki?curid=14550" title="Kantenfärbung">
Kantenfärbung

In der Graphentheorie ist eine Kantenfärbung eine Abbildung, die jeder Kante eines Graphen eine (abstrakte) Farbe zuordnet. Der Begriff ist eng verwandt mit der Knotenfärbung.

Für einen ungerichteten Multigraph formula_1 nennt man eine Abbildung formula_2 der Kantenmenge in die Menge der natürlichen Zahlen eine Kantenfärbung von formula_3. Die Elemente aus formula_4 werden in diesem Zusammenhang Farben genannt.
Man nennt formula_5 "gültig" oder "zulässig", falls für je zwei beliebige benachbarte Kanten formula_6 und formula_7 gilt, dass formula_8. Besitzt formula_3 eine Kantenfärbung formula_5, so dass höchstens "k" Farben im Bildbereich von formula_5 auftreten, nennt man "G" k-kantenfärbbar.

Das kleinste formula_12, für das ein Graph formula_12-kantenfärbbar ist, heißt chromatischer Index, Kantenfärbungszahl oder auch Kantenchromatische Zahl des Graphen formula_3 und wird meist mit formula_15 bezeichnet.

Nach dem Satz von Vizing ist der chromatische Index eines einfachen Graphen formula_3 mindestens so groß wie sein Maximalgrad, aber höchstens eins größer als dieser, also formal:

Graphen mit formula_18 nennt man Klasse‑1-Graphen, Graphen mit formula_19 nennt man Klasse‑2-Graphen (da die Abschätzung des Satzes nicht für Multigraphen gilt, werden Multigraphen Klasse‑2-Graphen genannt, wenn formula_20 gilt). Zu entscheiden, ob ein Graph Klasse 1 oder Klasse 2 ist (Klassifizierungsproblem), ist NP-vollständig. Das heißt, obwohl der Maximalgrad leicht zu bestimmen ist und der chromatische Index nur einen von zwei möglichen Werten annehmen kann, ist das Problem, für einen gegebenen Graphen genau diesen einen Wert zu bestimmen, NP-schwer.

Für bipartite Graphen ist formula_21. Damit sind alle bipartiten Graphen Klasse‑1-Graphen.

Die Bestimmung einer Kantenfärbung ist zur Bestimmung einer Eckenfärbung in der Weise dual, dass eine Kantenfärbung eines Graphen formula_22 genau eine Knotenfärbung des Kantengraphen formula_23 ist. Daraus folgt, dass formula_24 gilt. Die kantenchromatische Zahl eines Graphen ist also genau die chromatische Zahl des Kantengraphen. Trotz dieses engen Zusammenhangs sind die Probleme unterschiedlich schwer zu lösen und die verfügbaren Abschätzungen unterscheiden sich deutlich.

Eine wesentliche Verallgemeinerung der Kantenfärbung ist der Begriff der Listenfärbung. Hier wird für jede Kante (oder jeden Knoten) eine Liste mit verfügbaren Farben vorgegeben und der Graph soll nun eine gültige Kantenfärbung aus diesen Listen erhalten. Des Weiteren gibt es die Totalfärbung, bei der sowohl Knoten als auch Kanten gefärbt werden sollen.



</doc>
<doc id="14554" url="https://de.wikipedia.org/wiki?curid=14554" title="K-partiter Graph">
K-partiter Graph

Ein formula_1-partiter Graph ist in der Graphentheorie ein einfacher Graph, dessen Knotenmenge in "k" disjunkte Teilmengen zerfällt, sodass die Knoten jeder dieser Teilmengen untereinander nicht benachbart sind. Für formula_2 heißen diese Graphen bipartite Graphen.

Eine k-Partition eines Graphen formula_3 ist eine Zerlegung der Knotenmenge formula_4 in formula_1 disjunkte Teilmengen formula_6, sodass keine adjazenten Knoten in der gleichen Menge formula_7 liegen, das heißt

Man beachte, dass eine solche "k"-Partition nicht eindeutig ist. Es ist durchaus möglich, dass es mehrere "k"-Partitionen gibt, die diese Eigenschaft erfüllen. Ein Graph heißt nun k-partit, falls er eine "k"-Partition besitzt. Man nennt den Graphen vollständig k-partit, falls außerdem jeder Knoten mit allen Knoten aller anderen "k"-Partitionen verbunden ist, wenn also gilt:

Mit formula_10 notiert man einen vollständig "k"-partiten Graphen, mit formula_11.

Die Turán-Graphen formula_12 (formula_13) sind vollständige formula_14-partite Graphen. Das nebenstehende Beispiel formula_15 ist 3-partit. Bezeichnet formula_16 die Floor-Funktion, so ist

Für das nebenstehende Beispiel gilt damit




</doc>
<doc id="14558" url="https://de.wikipedia.org/wiki?curid=14558" title="Peter der Große">
Peter der Große

Peter I., der Große (, transkribiert "Pjotr I Weliki"), geboren als Pjotr Alexejewitsch Romanow (russ. ; *  in Moskau; †  in Sankt Petersburg), war von 1682 bis 1721 Zar und Großfürst von Russland und von 1721 bis 1725 der erste Kaiser des Russischen Reichs. Er gilt als einer der bedeutendsten Herrscher Russlands. Der Beiname "Der Große" bezieht sich dabei auf seine Leistungen, allerdings war auch seine Körpergröße entsprechend: Nach zeitgenössischen Angaben war Zar Peter tatsächlich ein Riese an Gestalt. Unterschiedliche Quellen nennen Maße zwischen 2,01 und 2,15 Meter.

Am 9. Juni 1672 wurde Peter im Moskauer Kreml geboren. Der Vater des künftigen Kaisers Russlands, Alexei Michailowitsch, hatte zahlreiche Nachkommen, darunter Peter als vierzehntes Kind. Peters Mutter war die zweite Frau von Alexei Michailowitsch, Natalja Kirillowna Naryschkina. Als Peters Vater 1676 starb, bestieg Peters Halbbruder Fjodor III. den Zarenthron.

Nach dem Tod Fjodors 1682 fand sich der zehnjährige Peter mitten in einem Kampf um den Thron seines Landes wieder, der im Ersten Strelizenaufstand gipfelte. Vor seinen Augen ermordeten die Strelizen zwei Brüder seiner Mutter sowie deren Ziehvater Matwejew. Dieses Ereignis gilt allgemein als das Schlüsselerlebnis für Peter, da sich bei ihm daraufhin der Hass auf die Strelitzen einprägte. 

1682 wurde Peter zusammen mit seinem älteren Halbbruder Iwan V. zum Zaren ernannt. Regentin wurde jedoch aufgrund der Minderjährigkeit der beiden Brüder zunächst Iwans Schwester und Peters Halbschwester Sophia, die ihre Macht wesentlich auf die Strelitzen stützte. Formal blieb Iwan bis zu seinem Tode im Jahre 1696 noch Zar neben Peter. Da Iwan aber Epileptiker, augenleidend und geistesschwach war, sollte er bis zu seinem Tod keinen Einfluss auf die Regierungsgeschäfte haben.

Seit 1682 hielt sich Peter zusammen mit seiner Mutter Natalja im Dorf Preobraschenskoje unweit von Moskau auf. Natalja zog es vor, ihren Sohn vom russischen Hof fernzuhalten, und schloss ihn damit auch von den dortigen Bildungsmöglichkeiten aus. So genoss Peter eine traditionelle altmoskowitische Erziehung, alles in allem eine dürftige Bildung.
Seit den 1680er Jahren war Peter in noch recht spielerischer Manier mit militär- und schifffahrtstechnischen Übungen und Manövern auf dem Exerzierplatz von Preobraschenskoje unweit der Moskauer Ausländervorstadt und am See von Perejaslavl befasst. In Preobraschenskoje beschäftigte er sich vor allem mit dem Kriegsspiel. Er bildete mit Gleichaltrigen eine Kriegerschar von 50 Mann, mit denen er Kriege simulierte. Aus dieser Spielzeugarmee entwickelte sich das Preobraschensker Leib-Garderegiment, das 1698 den Zweiten Strelitzenaufstand in Abwesenheit Peters I. niederschlug und damit Peters Herrschaft rettete.

Sophias Sturz im Jahre 1689 durch die Hofpartei von Peter und seiner Mutter bedeutete Peters Regierungsantritt im Russischen Zarentum. Ein Mordkomplott an Peter durch etwa 600 involvierte Strelitzen, angezettelt von Peters Halbschwester Sophia, war dem vorausgegangen. Das Komplott wurde aber verraten. Die Schuldigen wurden bestraft, seine Halbschwester Sofia ins Neujungfrauenkloster gesteckt und ihr Berater und Geliebter, Fürst Golizyn, entmachtet und nach Sibirien verbannt.

Zu diesem Zeitpunkt musste sich Peter noch dem Machtanspruch der konservativen Mutter beugen. Bereits im Januar 1689 hatte der siebzehnjährige Peter auf Drängen seiner Mutter die drei Jahre ältere Jewdokija Lopuchina (1669–1731) geheiratet. Sie gebar dem Herrscher im Februar 1690 einen Sohn mit Namen Aleksei. Ein zweiter Sohn, der im Oktober 1691 zur Welt kam, verstarb bereits nach einem halben Jahr. Die Ehe mit Jewdokija währte zwar formell zehn Jahre, war aber schon nach einigen Jahren völlig zerrüttet.

An den Regierungsgeschäften zeigte Peter in den frühen 1690er Jahren noch wenig Interesse. Inspiriert von technischen Neuerungen und Künsten ausländischer Handwerker in der Moskauer Ausländervorstadt Nemezkaja sloboda, suchte Peter durch Besuche das dortige Leben und Treiben näher kennenzulernen. So lud sich Zar Peter häufig bei Patrick Gordon ein, einem adligen Gutsbesitzer aus Schottland, der unter dem neuen Zaren zu dessen militärischem Hauptberater avancierte. Dort lernte er auch den Schweizer François Le Fort kennen, den späteren Admiral der Kriegsflotte. Durch seine Beziehungen zur Ausländervorstadt erhielt er erste, wenn auch nur rudimentäre Eindrücke von der damaligen westeuropäischen Lebensweise. Le Fort, der von fremden Ländern, ihren Merkwürdigkeiten und der hohen Entwicklungsstufe ihrer Kultur erzählte, erregte in dem jungen Zaren eine große Wissbegier, die mit dem Wunsch, Russland auf ein ähnliches Niveau zu bringen, vereinigt wurde. Peter erlernte von dem jungen Mann die deutsche und niederländische Sprache. Dass ein russischer Zar in der Ausländersiedlung ein und aus ging, bedeutete unter den damaligen Gegebenheiten jedoch einen eklatanten Bruch mit der Tradition.

In den ersten Jahren als russischer Monarch beschäftigte sich der junge Zar vorwiegend mit dem Aufbau einer schlagkräftigen Armee. Zahlreiche Kriegsspiele bestimmten seinen Alltag. Durch einen Zufall wurde auch Zar Peters Drang zum Meer geweckt. Im Juni 1688 entdeckte er auf dem Gut seines Großvaters Nikita Iwanowitsch Romanow ein altes englisches Boot. 1691 reparierte Schiffbauer Karsten Brant den „Großvater der russischen Flotte“. Mit diesem Schiff unternahm Peter I. die erste Seereise nach Kolomensk. Damit war das Interesse für Boote und Schiffe bei Peter geweckt, der nun von einem Hafen für Russland träumte. Durch jenen Karsten Brant ließ er zwei Fregatten und drei Yachten erbauen, die die Grundlage und den Anfang der künftigen russischen Seemacht bildeten. 

Um die maritime Position Russlands zu verbessern, trachtete Peter danach, neue Küstenplätze für sein Land zu gewinnen. Der einzige maritime Zugang dieser Zeit war am Weißen Meer bei Archangelsk. Die Ostsee wurde in dieser Zeit von Schweden kontrolliert, während das Schwarze Meer vom Osmanischen Reich beherrscht wurde. Zwei Jahre später, unmittelbar vor dem Ableben seiner Mutter, segelte er von Wologda nach Archangelsk. Dort traf sich, wenn das Eis gebrochen war, eine Flotte aus ganz Europa. Engländer, Holländer und Dänen kamen hierher, um mit Pelzen, Häuten, Hanf, Talg, Getreide und Pottasche zu handeln. Hier machte sich der Monarch in mehrmonatigen Aufenthalten am Weißen Meer in Begleitung von Gordon und Le Fort auch mit der Hochseefischerei, dem Murmansker Überseehafen und dem dortigen Handelsleben bekannt. Den dortigen Fahrten ins offene Meer entsprachen die Landmanöver, die Peter im Herbst 1694 im Moskauer Gebiet durchführen ließ, als Vorübung für den kommenden Ernstfall. Er verlegte die Werft auf die Insel Solombala und gründete dort eine Admiralität als Grundlage für eine russische Kriegsmarine und Handelsflotte. Das erste Schiff, das Handelsschiff „Sankt Paul“, lief im Juni 1694 vom Stapel. Nach Moskau zurückgekehrt, widmete sich der Zar den Staatsangelegenheiten.

Mit dem frühen Tod der Mutter, die im Februar 1694 im Alter von erst einundvierzig Jahren starb, erlosch für den jungen Herrscher auch der Grund zur Rücksichtnahme auf sie, die er bislang geübt hatte. Ihm selbst waren damit jedoch auch Bürde und Verantwortung der Regierungsgeschäfte aufgetragen.
Im darauf folgenden Jahre unternahm Peter eine Reise in mehrere Teile seines Reichs, um sich selbst von allem genau zu überzeugen und das Land mit dessen Bewohnern, die Mängel und Gebrechen aller Art kennenzulernen. Nach dem Tod seines Halbbruders Iwan V. übte Zar Peter I. die Alleinherrschaft in Russland aus.

Peter versuchte, einen Zugang zum Schwarzen Meer zu erhalten. Dafür musste er aber die Krimtataren der Umgebung besiegen. In einer Vereinbarung mit Polen-Litauen begann er einen Krieg gegen das Khanat der Krim und gegen den Oberherrn der Krimtataren, den osmanischen Sultan. Zar Peters Hauptziel war die Eroberung der osmanischen Festung von Asow, nahe dem Don. Im April 1695 zog das russische Heer unter ihm gegen Asow, aber die Eroberungsversuche scheiterten. Peter kehrte im November dieses Jahres nach Moskau zurück. Sofort begann er mit dem Aufbau einer großen Marine. Er ließ 1696 über dreißig Schiffe gegen die Osmanen zu Wasser. Im Juli 1696 wurde Asow in einem zweiten Feldzug erobert. Am 12. September 1698 gründete Zar Peter offiziell die erste russische Marinebasis in Taganrog.

Von 1697 bis 1698 war Peter I. zum Teil inkognito als Teil der Großen Gesandtschaft in Europa unterwegs. Der Weg führte ihn über Livland, Kurland, Preußen nach Holland und weiter nach England. Begleiter auf dieser Reise war Alexander Menschikow. Sein Freund François Le Fort fungierte als nomineller Erster Gesandter der Großen Gesandtschaft in den Niederlanden.

Im August 1697 wollte Peter im niederländischen Zaanstad Erfahrungen im Schiffbau sammeln. Hier studierte er die Konstruktion seegängiger Segler, die er als Modellschiffe kopieren und in Russland später nachbauen ließ. Nachdem entdeckt wurde, wer da wirklich in Zaandam weilte, war Peter gezwungen, in einer von der Öffentlichkeit abgeschirmten Werft in Amsterdam weiterzuarbeiten. Dort begann er am 20. August 1697 beim Werftmeister Gerrit Claesz Pool eine Zimmermannslehre in der Werft der Ostindischen Kompanie in Krummendijk. Hier arbeitete Peter am Bau der Fregatte "Peter und Paul" und erhielt von Pool ein glänzendes Zeugnis.

Peter wurde an allen großen Höfen empfangen, doch sein politisches Anliegen, die Unterstützung Russlands im Kampf mit dem Osmanischen Reich, wollte niemand erfüllen. Damit zerschlug sich auch die Hoffnung auf die Gewinnung eines Schwarzmeerhafens, worauf sich Peters Anstrengungen von nun an zur Ostsee hin verlagerten.

Noch während seines Aufenthaltes in Wien während der Großen Gesandtschaft bekam Peter im Sommer 1698 die Nachricht von einem erneuten Aufstand der Strelitzen in Moskau und kehrte eilig in die russische Hauptstadt zurück. Die Strelitzen galten als größtes Machtrisiko für Peters Reformkurs. Peter I. setzte eine Vernichtungsaktion in Gang, die mehrere Monate andauerte. Viele Tausende verloren auf diese Art ihr Leben, andere wurden nach Sibirien, Astrachan, Asow und weiteren Orten verbannt, das Korps der Strelitzen für immer aufgehoben, der Name abgeschafft und für ehrlos erklärt. Während der Niederschlagung des Strelitzenaufstandes verdächtigte Peter I. auch seine Frau Jewdokija Fjodorowna Lopuchina der Teilnahme an der Verschwörung und verbannte sie 1698 ins Kloster Susdal.

Nach der Vernichtung der Strelitzen stand Peters I. Reformplänen nichts mehr im Weg. Ziel war die Modernisierung Russlands nach westeuropäischen Maßstäben. Diese sehr sprunghaften Reformen wurden erst durch den Tod Peters I. gestoppt. Zu seinen Umgestaltungs- und Modernisierungsmaßnahmen gehörten die Förderung der Wirtschaft durch den Bau von Großbetrieben und die Unterstützung der Gründung von Privatunternehmen, Reformen im Schulwesen, das Verbot des Tragens von Bärten und altrussischer Kleidung, die stärkere Zentralisierung und Bürokratisierung der Verwaltung und die Erstellung einer Adelsrangstabelle. Diese Reformen gingen als Petrinische Reformen in die Geschichte ein und trugen zum Aufstieg Russlands als einer der führenden Mächte in Europa einen großen Anteil bei (siehe Abschnitt Reformen).

In der Türkeipolitik festgefahren, hatte Zar Peter I. erkannt, dass das Fehlen eines Zugangs zur Ostsee den russischen Handel beeinträchtigte. Seine Anstrengungen richteten sich vor allem deshalb gegen Schweden, mit dem Ziel die schwedische Vormachtstellung in der Ostsee zu brechen.

Im Zweiten Nordischen Krieg (1700–1721) konnte Peter trotz zahlreicher Niederlagen und erheblicher Verluste mit dem Sieg in der Schlacht bei Poltawa (1709) die Kriegswende herbeiführen. Zar Peters großer Sieg bei Poltawa und seine nachfolgenden Eroberungen im Baltikum wurden insbesondere am Hof des Sultans auf Drängen des Krim-Chans, Karl XII. und Mazepas mit Argwohn verfolgt. Peter schickte seinen Botschafter nach Istanbul und forderte die Auslieferung Karls. Ahmed III. ließ den Botschafter ins Gefängnis werfen. Am 10. November 1710 erreichte den russischen Monarchen die Kriegserklärung. Damit ergab sich für Zar Peter eine gefährliche Situation, die den Erfolg bei Poltawa in Frage stellen konnte, da von den Verbündeten keine Hilfeleistungen zu erwarten waren. So fiel Peter widerwillig und von Krankheiten geschwächt mit seiner Armee in das Osmanische Reich ein. Die osmanischen Truppen kesselten ihn bei Huși, einem kleinen Ort am Pruth, ein. Sie nutzten jedoch ihre überlegene Position nicht aus und ließen ihn ehrenvoll abziehen. Im Frieden vom Pruth verpflichtete Peter sich, die zuvor eroberte Festung Asow abzutreten und sich aus den Gebieten der Kosaken zurückzuziehen. Zudem musste die dortige russische Schwarzmeerflotte aufgegeben werden.
1703 gründete Peter die Stadt Sankt Petersburg, die er ab 1710 ohne entsprechenden Erlass als neue Hauptstadt des Russischen Reiches bezeichnete.

Peter war seit 1712 in zweiter Ehe mit Martha Skawronskaja verheiratet, die ihm zwölf Kinder gebar und nach seinem Tod als Katharina I. die Herrschaft übernahm.

Auf seiner zweiten großen Reise nach Westeuropa 1716/1717, diesmal nicht inkognito, versuchte Peter I., seinen militärischen Erfolg über Schweden zunächst durch die Einbindung Russlands in das europäische Staatensystem zu vollenden. Nach einer Kur in Pyrmont besuchte er in Kopenhagen seinen dänischen Verbündeten, Friedrich IV., traf Preußens König Friedrich Wilhelm I. und reiste nach Holland weiter, wo er den Winter verbrachte. Aufgrund von Fieberanfällen musste sich der Zar schonen, reiste aber nach Paris weiter, wo er freundlich vom siebenjährigen König Ludwig XV. empfangen wurde. Nach einem sechswöchigen Aufenthalt in Paris begab sich Peter I. wieder zu einer Kur nach Spa, um über Holland und Berlin zurück nach St. Petersburg zu reisen.

Der Konflikt mit seinem Sohn Alexei spitzte sich 1718 zu. Er ließ Alexei nach dessen Flucht über Österreich nach Italien 1717 nach Russland zurückholen, enterbte ihn und machte ihm wegen Hochverrats den Prozess. Alexei wurde zum Thronverzicht gezwungen und zum Tode verurteilt, starb jedoch vor der Vollstreckung am 26. Juni an den Folgen der Folter.

Der Große Nordische Krieg wurde 1721 durch den Frieden von Nystad beendet. Russland konnte sich als Führungsmacht in Nordosteuropa etablieren.

Direkt nach dem Friedensschluss mit Schweden änderte Peter seinen offiziellen Titel gemäß der gestiegenen außenpolitischen Bedeutung Russlands von "Zar" in "Kaiser" (russisch Император "Imperator"). Diesen Titel trugen die russischen Herrscher fortan bis 1917. 1722 änderte Peter per Erlass die traditionell praktizierte, von der Reihenfolge der Geburt abhängige Thronfolge ab. Nun konnte der herrschende Regent seinen Nachfolger frei bestimmen, auch von außerhalb der Familie, und einen unwürdigen Nachfolger wieder absetzen.

Schon kränkelnd befahl Peter in seinem Bemühen, Russland zu modernisieren, am 8. Februar 1724 die Errichtung einer Russischen Akademie der Wissenschaften in Sankt Petersburg. Seine Sommerresidenz war der Peterhof (siehe auch Neptunbrunnen).

Unmittelbar nach der Genesung von einer schweren Erkrankung (er litt an Harnverhaltung) brach Peter im Herbst 1724 zu einer längeren Seereise auf. Sie führte ihn unter anderem nach Schlüsselburg, wo er die Arbeiten am neuen Ladogakanal überprüfen wollte. Am 5. November kehrte er wieder nach Sankt Petersburg zurück, ging aber nicht an Land, sondern segelte stattdessen weiter am Finnischen Meerbusen entlang. Sein Ziel war die Gewehrfabrik bei Lachta. Bei anbrechender Dunkelheit zog ein Sturm auf. Unweit vom Ufer des Lachta-Sees entdeckte Peter der Große ein gekentertes Boot, die Besatzung drohte zu ertrinken. Um den Matrosen und Soldaten aus Kronstadt zu helfen, watete er bis zur Hüfte in das eiskalte Wasser des Sees. Hier wurden sein persönliches Engagement und seine Rücksichtslosigkeit gegen sich selbst deutlich. Am 8. Februar 1725 starb der zweiundfünfzigjährige Zar an den Folgen seiner Rettungstat (Blasenleiden in Verbindung mit einer Leberatrophie) in Sankt Petersburg.

Peter hinterließ kein Testament. Die seit der Zeit Napoleons aufkommende Behauptung, Peter habe in einem Testament alle künftigen Nachfolger verpflichtet, die Herrschaft Russlands über Europa anzustreben, ist eine propagandistische Fälschung. Bereits 1828 wurde sie als solche entlarvt, doch bis heute hat diese berüchtigte Fälschung immer wieder Anlass zu Fehldeutungen und Unterstellungen gegeben.

Peter der Große leitete zahlreiche Reformen in Russland ein, die zum Ziel hatten, Russland in einen modernen Staat zu verwandeln. Damit verbunden war die Gründung und Förderung der neuen Hauptstadt Sankt Petersburg.

Zur kulturellen Modernisierung gehörte die Einführung westmitteleuropäischer Kleidung. Die traditionell langen Bärte wurden mit einer Bartsteuer belegt. Der julianische Kalender wurde in Russland eingeführt, obgleich im restlichen Europa in dieser Zeit bereits langsam der gregorianische Kalender übernommen wurde.

Auch im Hinblick auf Technik und Wissenschaft orientierte sich Peter I. an den damals modernen Vorbildern. Er initiierte die Akademie der Wissenschaften und führte eine Schriftreform durch.

Umfangreiche Veränderungen nahm der russische Monarch in der Verwaltung seines Reiches vor. Grundlage dieses Reformwerkes bildete das schwedische Reglement, das auf die spezifischen Verhältnisse Russlands zugeschnitten wurde. So schuf Peter I. die Bürgermeisterei, richtete einen Senat, der neue Gesetze vorbereitete und die örtlichen und zentralen Organe anleitete, als oberste Verwaltungsinstanz ein. Außerdem entstanden in seiner Regierung die Kollegien, etwa mit den Fachministerien in Westeuropa vergleichbar. Bahnbrechend war die Einführung der Rangtabelle 1722, die die Verwaltungs- und Militärlaufbahnen in 14 Rangklassen einteilte. 

Das russische Reich wurde verwaltungsmäßig in acht Gouvernements und etwa 50 Provinzen aufgegliedert.

Durch die Umgestaltung der Armee und die Gründung der russischen Flotte konnte Peter der Große im Großen Nordischen Krieg nach anfänglichen Misserfolgen die Schweden zurückdrängen.

Peter baute eine merkantilistische Wirtschaft auf. Dazu zählt besonders seine starke Förderung der Manufakturen. Beim Amtsantritt Peters existierten in Russland nur zehn Manufakturen. Die Förderung der Industrie stand in engem Zusammenhang mit den Bedürfnissen der Armee während der langen Kriegsjahre. Aber darüber hinaus entstanden auch viele Manufakturen und Fabriken, die Gebrauchsgüter herstellten. Einige Fabriken, unter ihnen die Spiegelfabrik Menschikows, arbeiteten schon für den Export. 1716 wurde das Spinnrad in Russland eingeführt. Noch ein Jahr vor seinem Tod ordnete Peter I. an, dass alle Findelkinder zu Handwerkern und Fabrikanten erzogen werden sollten. In seinem letzten Regierungsjahr gab es etwa 100 Fabriken, darunter einige mit mehr als 3000 Beschäftigten – herausragend die Waffenfabrik von Tula. Wesentlichen Anteil an der Entwicklung der Hüttenindustrie hatte der deutsche Bergbauspezialist Baron von Hennin, der Vorsitzender des Bergkollegiums war. Am Ende der Regierung registriert die Statistik einen ausgeglichenen Staatshaushalt von etwa 10 Millionen Rubel.

Zar Peter I. hatte die russische Kirche stets als oppositionellen (starken) Gegner seiner Reformen betrachtet. Daher ließ er nach dem Tod des Patriarchen Adrian (1700) die Stelle des höchsten Geistlichen unbesetzt. Besonders verhasst waren ihm die Altgläubigen (Raskolniki), die er durch zahlreiche Gesetze bekämpfte. 1719 wurden die Jesuiten aus Russland vertrieben. Ab dem 25. Januar 1721 stellte der Zar die russisch-orthodoxe Kirche endgültig unter Staatsverwaltung. Das Geistliche Kollegium, später „Heiliger Synod“, ersetzte das seit 1593 bestehende Moskauer Patriarchat. Im vorletzten Jahr seiner Regierung holte Zar Peter I. noch zu einem entscheidenden Schlag gegen den Müßiggang in den Klöstern aus.

Aus der Ehe mit Jewdokija Lopuchina hatte Peter der Große drei Söhne:

Aus der Ehe mit Martha Skawronskaja (später Katharina I.) gingen zwölf Kinder hervor (nach anderen Zählungen elf), von denen nur zwei Töchter das Erwachsenenalter erreichten:

Zu seinen Ehren wurden Denkmale errichtet, siehe Denkmale für Peter I.

Die Peter-der-Große-Bucht wurde nach ihm benannt; ebenso mehrere russische Schiffe, siehe Pjotr Weliki.





</doc>
<doc id="14561" url="https://de.wikipedia.org/wiki?curid=14561" title="Sly Stone">
Sly Stone

Sylvester Stewart, genannt Sly Stone, (* 15. März 1943 in Dallas, Texas) ist ein US-amerikanischer Sänger, Songwriter und Musikproduzent aus den Genres Funk, Soul, Rock und Psychedelic Rock. Im Jahr 1967 gründete er Band Sly & the Family Stone.

Sly Stone zählt neben James Brown zu den Mitbegründern der Funk-Musik und der Psychedelischen Musik in den 1960er Jahren. Er kombinierte die Musikgenres Rock, Soul, Pop und Jazz miteinander. Seine Band Sly & the Family Stone wurde durch einen Auftritt beim Woodstock-Festival im Jahr 1969 bekannt und übte Einfluss auf die Hippie-Generation aus. Mitte der 1970er löste sich die Band auf. Aufgrund anhaltender Drogenprobleme, die Sly Stone häufig mit dem Gesetz in Konflikt brachten, verschwand er Ende der 1970er Jahre zunehmend von der Bildfläche. Er konnte nicht mehr an den Erfolg von Sly & the Family Stone anknüpfen. Im September 2011 erschien das Sly-Stone-Album "I’m Back"; es enthält außer Neuaufnahmen drei bisher unveröffentlichte Titel aus den Jahren 1988 und 1989.

Seinen bisher letzten Live-Auftritt hatte Sly Stone am 23. August 2015 in Red Bank (New Jersey), als er die Band The Family Stone – ehemalige Mitglieder von Sly & The Family Stone – als Keyboarder unterstützte.

Der "Rolling Stone" listete Sly Stone auf Rang 78 der 100 besten Sänger sowie auf Rang 42 der 100 besten Songwriter aller Zeiten.




</doc>
<doc id="14563" url="https://de.wikipedia.org/wiki?curid=14563" title="Baum des Jahres">
Baum des Jahres

Um den Wert des Baumes in der Gesellschaft herauszuheben, wird in vielen Ländern ein Baum des Jahres vorgestellt. Wenn auch alle Staaten dasselbe Ziel haben, ist doch die Organisation in diesen meist verschieden. Auch die Baumauswahl richtet sich jeweils nach dem lokalen Vorkommen dieser Arten. In manchen Ländern wird statt einer Baumart ein spezieller Baum als "Baum des Jahres" gewählt, wie beispielsweise in der Slowakei oder Tschechien. Auch die Auswahl wird in manchen Ländern von öffentlichen Stellen oder mit den Bäumen befassten Organisationen getroffen. In anderen werden sie durch Wahl der Bevölkerung ausgewählt.

In enger Verbindung mit dieser Aktion steht auch der von der FAO ausgerufene Internationaler Tag des Waldes, der alljährlich am 21. März stattfindet.

Jedes Jahr im Oktober wird der Baum des Jahres von der „BAUM DES JAHRES – Dr. Silvius Wodarz Stiftung“ (vormals Menschen für Bäume) und durch deren Fachbeirat, das „Kuratorium Baum des Jahres“ (KBJ), für das darauffolgende Jahr bestimmt.

Dieses Kuratorium wurde 1991 vom Gründer und Vorsitzenden des seit 1972 bestehenden Umweltschutzvereins Wahlstedt (heute Baum des Jahres e. V./Stiftung Baum des Jahres) in Schleswig-Holstein, Silvius Wodarz, ins Leben gerufen. Ab 1989 hat der Verein einen Baum des Jahres ausgerufen, genauer eine Baumart, seit 1991 mit dem Kuratorium.
Das "Kuratorium Baum des Jahres" ist ein Fachbeirat der „Baum des Jahres-Dr. Silvius Wodarz Stiftung “, Präsident der Stiftung ist Silvius Wodarz. Vorsitzender des KBJ ist das Vorstandsmitglied der Stiftung, Andreas Roloff.
„Baum des Jahres“ ist eine geschützte Marke.

Die Aktivitäten stehen unter dem Hauptmotto „Menschen für Bäume und Kinder brauchen Natur“. Die Stiftung legt zu jedem Baum des Jahres ein grünes Faltblatt für alle und ein gelbes für Kinder auf.
Zum jeweiligen Baum des Jahres veranstaltet die Stiftung jährlich eine Fachtagung im Wechsel in je einem Bundesland.

Bei der Ausrufung des 20. Jahresbaumes (Walnuss) am 19. Oktober 2007 in Berlin gab Wodarz die Gründung der Stiftung „Menschen für Bäume“ bekannt. Diese wurde im Juni 2010 in „Baum des Jahres – Dr. Silvius Wodarz Stiftung“ umbenannt. Schirmherr ist der jeweilige Bundesumweltminister. Außerdem wurde ein Grußwort des damaligen Bundespräsidenten Horst Köhler verlesen.

Seit 2010 wird auch jährlich eine Deutsche Baumkönigin zur Repräsentation in der Öffentlichkeit gewählt.

Am Schäfersee in Berlin-Reinickendorf und im Berliner Zoo wird jedes Jahr ein Exemplar des Baum des Jahres gepflanzt. Dort kann man etliche Bäume des Jahres seit 2001 nebst Namensstein bzw. Namenstafel besichtigen.

Im Zuge einer Neubaumaßnahme eines Teilstücks der BAB 4 bei Kerpen-Buir wurde in beiden Fahrtrichtungen die „Allee Baum des Jahres“ angelegt. Dort wurden die Bäume des Jahres von 1989 bis 2014 mit entsprechenden Hinweistafeln angepflanzt.

Es folgen die authentischen Namen der proklamierten Baumarten:

Zum Jahrtausendwechsel erklärte das "Kuratorium Baum des Jahres" den "Ginkgo biloba" zum Mahnmal für Umweltschutz und Frieden und zum Baum des Jahrtausends.

Grundsätzlich kann jede heimische Baumart „Baum des Jahres“ werden.

Die Kriterien orientieren sich zwar auch an der ökologischen Bedeutung und der Seltenheit oder Bedrohtheit der Baumart, im Vordergrund steht aber die Aufklärung der Bevölkerung über die Eigenarten der jeweils ausgewählten Bäume.

Folgende Organisationen und Personen sind gleichberechtigte Mitglieder des KBJ:

In Österreich wird seit 1994 ein Baum des Jahres gekürt. Ausgesucht wird der Baum gemeinschaftlich durch das Lebensministerium und das Kuratorium Wald. Berücksichtigt werden dabei Baumarten, die für die österreichische Ökologie und Ökonomie wichtig sind, aber stark gefährdet sind. Im Gegensatz zur deutschen Auswahl können in Österreich ganze Gattungen ebenso wie einzelne Arten ausgewählt werden. Sogar Gruppen, die biologisch nichts miteinander zu tun haben, wie die "Wildobstbäume" wurden ausgewählt.

In Ungarn wird der "Baum des Jahres" seit 1996 gekürt
In manchen Ländern wird nicht eine Baumart, sondern ein ganz spezieller Baum als "Baum des Jahres" gewählt. Diese Länder in Europa sind beispielsweise Tschechien, Slowakei, Ungarn, Bulgarien, Irland, Frankreich und Polen.

In den USA gibt es keine landesweite Wahl des „Baum des Jahres“. Vereinzelt wird ein „Baum des Jahres“ jedoch auf lokaler Ebene gekürt, zum Beispiel in Austin (Texas).



</doc>
<doc id="14567" url="https://de.wikipedia.org/wiki?curid=14567" title="Vogelbeere">
Vogelbeere

Die Vogelbeere, gemeinsprachlich häufiger die Eberesche oder der Vogelbeerbaum ("Sorbus aucuparia"), ist eine Pflanzenart aus der Gattung Mehlbeeren ("Sorbus") innerhalb der Familie der Rosengewächse (Rosaceae). Die Zugehörigkeit zu den Kernobstgewächsen (Pyrinae) kann man bei genauer Betrachtung der Früchte gut erkennen; sie sehen wie kleine Äpfel aus.

Andere deutschsprachige Trivialnamen sind Drosselbeere, Quitsche oder Krametsbeere. Die Bezeichnung als Speierling ist irreführend, da dies der gebräuchliche Name einer anderen "Sorbus"-Art ist. Die Vogelbeere ist in weiten Teilen Europas verbreitet und besitzt als Pionierart ein breites Bodenspektrum. Für Insekten, Vögel und Säugetiere ist sie eine wertvolle Futterpflanze. Die vielfältige Nutzung durch den Menschen spiegelt sich in zahlreichen regionalen Namensgebungen wider. Im Aberglauben und Brauchtum hat sie eine bedeutende Rolle inne. In Deutschland wurde die Vogelbeere im Jahr 1997 zum Baum des Jahres gekürt. Entgegen einer weitverbreiteten Annahme sind ihre Früchte ungiftig.

Der Name "Eberesche" leitet sich vom spätmhd. "eberboum" und von "Esche" ab und rührt daher, dass die Blätter jenen der Eschen ähneln, obwohl keine nähere Verwandtschaft zwischen diesen Baumarten besteht. Der erste Bestandteil wird aus gall. "eburos" ‚Eibe‘ entlehnt, der auf idg. *"erebʰ"- ‚dunkelrötlich, bräunlich‘ zurückgeht, welches die rötlich-braune Beerenfarbe bezeichnet. Volksetymologische Umdeutung mit "Aber" im Sinne von ‚falsche, minderwertige Esche‘ (wie in „Aberglaube“ und „Aberwitz“) ist sprachlich und von der Sache her nicht zu vertreten. Es ist eher anzunehmen, dass sich der Name auf den Eber bezieht, da früher die Früchte zur Schweinemast verwendet wurden.
Der wissenschaftliche Name "aucuparia" wird aus ‚au‘ (avis = der Vogel) und ‚cuparia‘ ("capere" = fangen) gebildet und stammt daher, dass die roten Beeren früher häufig als Köder beim Vogelfang eingesetzt wurden. Auch die Bezeichnung "Vogelbeere" (die gemeinsprachlich nur für die Früchte benutzt wird, nicht für den Baum) stammt daher, dass die „Beeren“ (Früchte) als Köder für Vögel verwendet wurden.

Die Eberesche – als verbreitete Baumart – hat in allen Zeiten dem Menschen ein beliebtes, wohlschmeckendes Nahrungsmittel und Heilmittel geboten. Aus diesem Grund sind viele regional sehr unterschiedliche Wortschöpfungen für diese Baumart entstanden.
Das wären: Vogelbär, Blumenesche, Ebschbeere, Zwergesche, Eibschen, Quetsche(n), Queckbeere, Quitsbeere, Kronawetterbeere, Drosselbeere, Vogelbeere, Quitschbeere, Queckenboom.

Die sommergrüne Vogelbeere erlangt ein gewöhnliches Alter von 80, in seltenen Fällen, vor allem als Gebirgsbaum auch bis 120 Jahren. Mit einer durchschnittlichen Wuchshöhe von 15 Metern ist die Eberesche ein eher kleinwüchsiger Baum. Einzelstehend, ohne Beschattung konkurrierender Pflanzenarten kann sie auch Wuchshöhen von bis 25 Metern erreichen. Stockausschläge der Eberesche wachsen gewöhnlich mehrstämmig als wesentlich kleinerer Strauch. In den ersten 20 Jahren wächst sie relativ schnell, danach stockt das Wachstum. Die Eberesche besitzt ein weitreichendes und tiefgehendes Senkerwurzelsystem und die Fähigkeit, sich über Stockausschläge und Wurzelbrut vegetativ zu vermehren. Auf Pseudogleyböden wurzelt sie hingegen relativ flach.

Kennzeichnend für die Eberesche ist ihre zierliche Gestalt sowie die oval bis rundliche, unregelmäßig aufgebaute und locker gehaltene Krone. Der Stamm der Eberesche zeichnet sich durch eine schlanke, walzenförmige Wuchsform aus. Die Äste stehen vom Stamm ab oder sind schräg nach oben gerichtet. Die glatte, glänzende Rinde jüngerer Bäume ist gelblich bis grünlich grau gefärbt und zeigt längliche, quer zur Wuchsrichtung gestellte Lentizellen, die den Gasaustausch mit der Umgebung sicherstellen. Mit zunehmendem Alter des Baumes nimmt die Rinde eine mattgraue Färbung und feinrissige Struktur an. Nur wenige Exemplare entwickeln im hohen Alter im unteren Stammbereich eine schwärzliche, längsrissige Borke. Jungtriebe bilden gewöhnlich eine weiche, filzige Behaarung aus und sind aschgrau gefärbt. Eine Besonderheit stellt das Chlorophyll dar, das sich unter der glatten Rinde der Zweige befindet. Dies befähigt den Baum bereits vor dem Laubaustrieb zur Photosynthese. Sein Vorkommen in höheren Lagen wird dadurch unterstützt.

Die Winterknospen der Vogelbeere sind meist dunkelviolett gefärbt und weißfilzig behaart. Dies stellt ein wichtiges Unterscheidungsmerkmal zum Speierling dar, dessen grüne und klebrige Knospen allenfalls an den Schuppenrändern eine feine Behaarung entwickeln. Die Endknospe an den Zweigspitzen ist gewöhnlich gekrümmt.

Die wechselständig an den Zweigen angeordneten Laubblätter sind in Blattstiel und Blattspreite gegliedert und sind dabei etwa 20 cm lang sowie 8 bis 11 cm breit. Die unpaarig gefiederte Blattspreite setzt sich gewöhnlich aus 9 bis 19 länglich-elliptischen Blattfiedern zusammen. Die 4 bis 6 cm langen und etwa 2 cm breiten Blättchen sitzen mit einem kurzen Stiel der Blattspindel an. Sie sind nach vorne zugespitzt und zum Grund hin asymmetrisch abgerundet. Am Blattrand bilden sie eine scharfe, ungleiche Zähnung aus, die zur Blattspitze hin ausgerichtet ist. Die unbehaarte Blattoberseite zeigt eine sommergrüne Färbung, wohingegen die Blattunterseite eher graugrün gefärbt ist und eine leichte Behaarung entwickeln kann. Die drehrunde Blattspindel weist zwischen den einzelnen Fiedern leichte Rinnen auf. Die Fiederblättchen der Eberesche besitzen keine Blattzahndrüsen an der Spitze.

Die Vogelbeere erlangt ihre Blühfähigkeit bereits im Alter von fünf bis sechs Jahren. Auf der Nordhalbkugel blüht sie von Mai bis Juli. Der Blütenstand entspricht einer ausgebreiteten Schirmrispe, in der 200 bis 300 Blüten vereinigt sind. Je nach Unterart sind die Infloreszensachsen flächig behaart ("ssp. aucuparia") oder fast bis ganz kahl ("ssp. glabrata") 

Die zwittrige Blüte ist bei einem Durchmesser von etwa 10 Millimetern radiärsymmetrisch und fünfzählig mit doppelter Blütenhülle. Die fünf Kelchblätter sind spitz, kurz dreieckig. Ihre Länge beträgt etwa 1,5 bis 1,8 Millimeter. Sie sind drüsig bewimpert, mehr oder weniger behaart oder auch kahl. Auch während der Fruchtreife behalten sie eine fleischige Konsistenz.
Die fünf weißen Kronblätter entwickeln eine Länge von (drei) vier bis fünf Millimeter. Ihre Form ist entweder kreisrundlich oder breiteiförmig ausgeprägt. Sie sind kurz genagelt und besitzen Richtung Grund etwas oberhalb des Nagels eine wollige Behaarung. Die Länge der 20 Staubblätter entspricht in etwa derjenigen der Kronblätter. Die Blüte besitzt zwei bis fünf freie Griffel, die in der unteren Hälfte behaart sind. Die unterständig stehende unverwachsenen Fruchtblätter sind in den Blütenboden eingesenkt und mit diesem verwachsen. Die fleischige Blütenachse verbindet sie miteinander.

Die Früchte reifen von August bis September. Die bei Reife leuchtend roten und kugeligen, im botanischen Sinne dreifächerigen Apfelfrüchte, werden manchmal als „Beeren“ bezeichnet. Sie enthalten gewöhnlich drei Samen und bilden einen Durchmesser von etwa 1 cm aus. Häufig hängen die Früchte bis in den Winter hinein in dichten „Büscheln“ am Baum.

Die Chromosomengrundzahl beträgt x = 17, es liegt Diploidie vor mit einer Chromosomenzahl von 2n = 34.

Bei der Vogelbeere handelt es sich um einen mesomorphen Makrophanerophyten oder Nanophanerophyten.

Ihre Wurzeln sind – typisch für "Sorbus"-Arten – von einer ektotrophen Mykorrhiza umgeben, wodurch die Versorgung mit Nährstoffen unterstützt wird. Der Pilz "Glomus intraradices" konnte als arbuskulärer Mykorrhizapartner der Eberesche festgestellt werden. 

Bei den Blüten der Eberesche reifen die Narben vor den Staubbeuteln, was botanisch als Proterogynie bezeichnet wird und Fremdbestäubung fördert. Nektar wird verdeckt angeboten. Der verhältnismäßig unangenehme Geruch der Blüten erinnert an Heringslake und beruht auf dem Wirkstoff Methylamin. Er lockt insbesondere Käfer und Fliegen zur Bestäubung an. Aber auch Bienen schätzen den Nektar.

Die Samenproduktion erfolgt bei der Eberesche amphimiktisch, also sexuell.

Die Früchte, dreifächrige Apfelfrüchte, werden vor allem nach dem Frost von verschiedenen Vögeln und Säugetieren (wie von Eichhörnchen) gesammelt. Die noch frischen, durch Karotinoide grellroten Früchte werden weniger häufig angenommen. Die Früchte werden über Speicher- und Versteckausbreitung, Verdauungsausbreitung und Bearbeitungsausbreitung ausgebreitet. Eine besondere Rolle spielen Vögel (Vogelausbreitung = Ornithochorie). Da die Samen unverdaut wieder ausgeschieden werden, wird die Ausbreitung der Eberesche effektiv sichergestellt (Endochorie). Menschenausbreitung geschieht durch die Nutzung als Ziergehölz. Die Samen sind nur nach einer längeren Lagerung in einer feuchten Umgebung keimfähig.

Die Eberesche ist eine wichtige Futterpflanze für Tiere. Nachgewiesen wurde dies bislang für 31 Säugetier- und 72 Insektenarten, darunter 41 Kleinschmetterlinge und zwölf Rüsselkäfer. Insgesamt wurden 63 Vogel- und 20 Säugetierarten als Nutzer der Früchte festgestellt. Insbesondere Singdrossel, Misteldrossel, Rotkehlchen, Mönchsgrasmücke, Kleiber und Gimpel schätzen die Früchte der Eberesche und nutzen den Baum, ebenso wie der Grünspecht, als Nistgehölz. Eine wichtige Rolle spielen die Früchte in der Ernährung von Rotdrossel und Seidenschwanz, die, aus Nordeuropa kommend, den Winter in unseren Breiten verbringen. Aber auch Rotfuchs und Dachs verschmähen die Früchte nicht.

Eichelhäher und verschiedene Nagetiere, wie Siebenschläfer, Haselmaus, Gelbhals- und Feldmaus legen sich – im Boden versteckt – Wintervorräte der Früchte an. Da diese oftmals vergessen werden, leisten sie ebenfalls einen wichtigen Beitrag zur Ausbreitung der Eberesche.
Paarhufer wie Reh und Rothirsch ernähren sich von den Blättern, Trieben und Knospen der Bäume, der Weißdornkäfer und der Mittlere Schwarze Rüsselkäfer ("Otiorhynchus niger") bevorzugen Triebe und Blätter.

Insbesondere für die Raupen des seltenen Spanners "Venusia cambrica" und des vom Aussterben bedrohten Gelben Hermelins ("Trichosea ludifica") stellt die Eberesche eine wichtige Nahrungspflanze dar. Die Raupen des Baum-Weißlings ("Aporia crataegi") tun sich ebenfalls an der Eberesche gütlich.

Seit 1960 wurden bei der Eberesche im mitteleuropäischen Raum starke Krankheitssymptome beobachtet, darunter chlorotische Ringe und Scheckungen. Reduziertes Wachstum und langsamer Verfall wurden ebenfalls beobachtet. Untersuchungen (Lit.: Benthack u. a. 2005) deuten darauf hin, dass es sich vermutlich um ein Virus handelt, das mit der Familie der Bunyaviridae verwandt ist.

Die Blätter der Vogelbeere werden von Rostpilzen der Gattung "Gymnosporangium" (Arten "Gymnosporangium cornutum" oder "Gymnosporangium tremelloides") sowie "Ochropsora ariae" und dem Echten Mehltau der Art "Podosphaera aucupariae" befallen.

Schädlinge, die an der Vogelbeere auftreten sind Ebereschenfruchtmotte ("Argyresthia conjugella"), Ebereschensamenwespe und Ebereschenpockenmilbe.

Die Vogelbeere hat eine europaweite Verbreitung. In der typischen Unterart besiedelt sie fast ganz Europa. Im Osten erstrecken sich die Vorkommen bis Westsibirien, südlich erreichen sie Nordspanien, Korsika, Sizilien, das nördliche Griechenland und Bulgarien. In Südeuropa sind Bestände nur in den Gebirgen und dort vergleichsweise selten belegt. Keine Vorkommen besitzt die Vogelbeere auf den Azoren, Balearen und Färöern, auf Kreta, Sardinien und Spitzbergen sowie im europäischen Teil der Türkei. Südwestasiatische Vorkommen werden in der Fachwelt teils als eigene Art ("Sorbus boissieri" Schneider), teils zu "Sorbus aucuparia" gehörig verstanden. Angegebene Vorkommen in Nordafrika gelten als nicht sicher belegt.
In Mitteleuropa ist die Vogelbeere weit verbreitet. Ihr Verbreitungsschwerpunkt liegt hier in den Alpen, im Alpenvorland, in den süd- und mitteldeutschen Mittelgebirgen und in der Norddeutschen Tiefebene. Auf Marschen, in Trockengebieten und wohl auch auf Alluvialböden kommt die Eberesche selten vor, beziehungsweise kann sie auch ganz fehlen. So besitzt sie beispielsweise im Mitteldeutschen Trockengebiet nur zerstreute Vorkommen. Bestände an der Nordseeküste und auf den friesischen Inseln gelten als eingeschleppt. In Österreich kommt die Vogelbeere zerstreut bis häufig in allen Bundesländern vor, fehlt aber im östlichsten Teil Österreichs. In der Schweiz ist sie verbreitet, gilt jedoch in der Südschweiz in weiten Teilen des Wallis sowie in Teilen Graubündens als unbelegt.

Die anspruchslose Vogelbeere ist ein schneller Besiedler von Brachflächen und kommt auf Lichtungen, in Hecken oder an Waldrändern, in Norddeutschland vorwiegend in Knicks als Überhälter vor. Ihr Bodenspektrum reicht von mager bis nährstoffreich, von trocken bis feucht und von sauer bis basenreich. Sie gedeiht sowohl in Laub- als auch in Nadelwäldern, auf Moorböden ebenso wie auf trockenen Steinhängen. Im Gebirge findet man den Baum bis an die Baumgrenze, in Norwegen bis an die Eismeerküste. Er löst in den Gebirgsvorwäldern häufig die Birke als vorherrschenden Baum ab. Sie steigt in Tirol bis 2400 Meter an. Im Bayrischen Wald sind Bestände bis 1400 Meter Höhe und im Erzgebirge bis 1100 Meter belegt. In den Allgäuer Alpen steigt sie im Tiroler Teil zwischen Gumpenegg und Vorderer Mutte oberhalb Holzgau bis zu einer Höhenlage von 2000 Metern auf.

Sie gedeiht optimal im Piceo-Sorbetum aus dem Verband Sambuco-Salicion, kommt aber auch in Pflanzengesellschaften der Verbände Quercion roboris oder im Epilobio-Salicetum der Ordnung Fagetalia vor.

Die Erstveröffentlichung von "Sorbus aucuparia" erfolgte 1753 durch Carl von Linné in "Species Plantarum", 1, Seite 477. Meyer gibt für "Sorbus aucuparia" mit Bezug auf Kutzelnigg drei Synonyme an: "Aucuparia sylvestris" 1789, "Pyrus aucuparia" 1791 und "Pyrenia aucuparia" 1811. Als weitere Synonyme werden angeführt: "Crataegus aucuparia" , "Mespilus aucuparia" , "Pyrus pohuashanensis" , "Sorbus adscharica" , "Sorbus amurensis" , "Sorbus bachmarensis" , "Sorbus boissieri" , "Sorbus gorodkovii" , "Sorbus pohuashanensis" , "Sorbus aucuparia" subsp. "pohuashanensis" , "Sorbus aucuparia" subsp. "gorodkovii" .

"Sorbus aucuparia" gehört zur Untergattung "Sorbus" aus der Gattung "Sorbus". In Europa ist "Sorbus aucuparia" der einzige Vertreter der Untergattung. 

Es gibt einige Unterarten von "Sorbus aucuparia": In Mitteleuropa sind die typische Unterart "ssp. aucuparia" und "ssp. glabrata" vertreten.

Neben der typischen Unterart "ssp. aucuparia" gibt es in Europa weitere Unterarten, die von einigen Autoren auch als eigene Art aufgefasst werden:

Über ihre natürlichen Vorkommen hinaus wird die Eberesche gerne im Garten- und Landschaftsbau eingesetzt. Aufgrund der dekorativen Frucht- und Blütenstände, sowie ihrer relativ großen Resistenz gegen Immissionen, ist sie in Städten häufig an Straßen als Allee- oder Einzelbaum und in Gärten sowie Parks als Zier- und Vogelschutzgehölz zu finden. In den höheren Lagen der Mittelgebirge und Alpen ist sie oft die einzige Zierholzpflanze. Die Eberesche gilt als Licht- bis Halbschattenbaumart.

Die Eberesche zeichnet sich besonders durch Frosthärte und Windfestigkeit aus. Auch gegenüber Spätfrösten zeigt sie sich resistent. Ihre weitreichenden Wurzeln dringen in tiefe Bodenschichten vor. Da sie sich durch Wurzelbrut auch vegetativ vermehren kann und eine hohe Ausschlagfähigkeit besitzt, wird sie gerne zur Bodenbefestigung im Kontext biologischer Wildbachverbauung und im Lawinenschutz eingesetzt. Das abgeworfene Laub der Eberesche zersetzt sich relativ rasch und setzt dabei verhältnismäßig viel Magnesium frei. Dies hat einerseits einen positiven Effekt auf die Humusbildung, andererseits verbessert der Baum hierdurch seine eigene Nährstoffversorgung und ist in der Lage, Umweltbelastungen besser stand zuhalten. Diese humusverbessernden Eigenschaften führten zur bewussten Anpflanzung des Baums in Fichtenwäldern.

Eberesche wächst zerstreutporig. Das Kernholz ist schön gemasert und eignet sich im Kunsthandwerk zu Drechselarbeiten. Das Kernholz älterer Vogelbeeren ist sehr hart und dauerhaft, vergleichbar mit Eichenkernholz; es wurde früher in der Wagnerei verwendet. Das Splintholz ist elastisch-feinfasrig und eignet sich daher sehr gut zu Schnitzarbeiten.

Auch wenn sich im Volksglauben hartnäckig das Gerücht hält, die Früchte seien giftig, ist dies nicht richtig. Allerdings enthalten die Beeren Parasorbinsäure, die zu Magenproblemen führen kann. Durch Kochen wird die Parasorbinsäure zu Sorbinsäure abgebaut, die gut verträglich ist. Gekochte Beeren können daher auch in größeren Mengen gegessen werden. Tatsächlich waren Vogelbeeren aufgrund ihres hohen Vitamin-C-Gehalts (bis zu 100 mg pro 100 g Beeren, das beim Kochen um etwa ein Drittel abgebaut wird) früher ein wichtiges Mittel gegen Skorbut. Sie enthalten außerdem Provitamin A und Sorbit, einen Zuckeraustauschstoff. Aus der Sorbose der Vogelbeeren wurde das Sorbit, ein Zuckerersatz für Diabetiker, gewonnen. Sorbit wird heute industriell durch Reduktion von Traubenzucker (Glukose) mit Wasserstoff hergestellt.

Die Naturheilkunde schreibt Blättern und Blüten eine besondere Heilwirkung zu. Getrocknet finden diese u. a. in Tees gegen Husten, Bronchitis und Magenverstimmungen Verwendung. Auch werden sie bei Verdauungsbeschwerden, Hämorrhoiden, Rheuma und Gicht eingesetzt. Die Wirkung ist allerdings nicht wissenschaftlich erwiesen. Sänger und Redner nutzen die Vogelbeeren z. B. auch, um ihre Stimmbänder geschmeidig zu halten. Laut „Kräuterpfarrer“ Johann Künzle sollen Vogelbeeren zähen Schleim von den Stimmbändern lösen und so bei Heiserkeit wertvolle Dienste leisten.
In der evidenzbasierten Medizin wird ein Auszug aus "Sorbus aucuparia", das Sorbit, intravenös zur Senkung des Augeninnendrucks bei Glaukom gespritzt.

Nach den ersten Frösten verlieren die Früchte ihren durch die Parasorbinsäure hervorgerufenen bitteren Geschmack und werden leicht süßlich. Die Parasorbinsäure wird hierbei zur Sorbinsäure umgebildet. Regional, zum Beispiel im Bayerischen Wald und in Böhmen, wird aus den Früchten Konfitüre gekocht, die wie Preiselbeeren als leicht säuerliche Konfitüre zu Wildgerichten gereicht wird. Hierfür eignet sich besonders die "Essbare" oder "Mährische Vogelbeere" – Sorbus aucuparia var. "moravica", auch var. "edulis" oder var. "dulcis" genannt, die einen höheren Zuckergehalt hat und frei von Parasorbinsäure ist und daher auch roh verzehrt werden kann.

Zwei verbreitete Kulturformen der mährischen Vogelbeere sind "Konzentra" und "Rosina," deren Auswahl 1946 im Institut für Gartenbau Dresden-Pillnitz begann und die 1954 in den Verkauf gebracht wurden. Dabei ist "Konzentra" für die Entsaftung geeignet und "Rosina" für Kompott oder zum Kandieren. Andere Sorten, die bitterstoffarm sind ähnlich der mährischen Vogelbeere, sind die aus Südrussland stammenden Kulturformen "Rossica" und "Rossica Major". Weitere essbare Sorten stammen aus Klosterneuburg in Niederösterreich. Es existieren Hybride mit anderen Fruchtbaumarten, die zum Verzehr geeignet sind. Dazu zählen "Burka," "Likjornaja," "Dessertnaja," "Granatnaja," "Rubinovaja" und "Titan".

Der Likör Sechsämtertropfen, der seit dem Ende des 19. Jahrhunderts im Fichtelgebirge gebrannt wird, und der tschechische Jeřabinka haben als Grundstoff auch Vogelbeerenfrüchte.

Vogelbeerschnaps hat in Tirol, Salzburg und in der Steiermark eine lange Tradition. Aufgrund der aufwändigen Gewinnung und Verarbeitung der Beeren und der geringen Ausbeute beim Brennen der Maische (ca. 2 Liter Edelbrand pro 100 Liter Maische) ist der fertige Edelbrand teuer.

Vor dem Maischvorgang werden die Beeren von den Dolden, die störende Gerbstoffe beinhalten, getrennt. Um gärhemmende Substanzen abzubauen, wird die Gärung bei höherer Temperatur durchgeführt. Parasorbinsäure wird durch Erhitzen beim Destillieren vollständig abgebaut.

In Hessen wird die Vogelbeere (Eberesche) von einigen kleinen Kelterern bei der Apfelweinherstellung verwendet, ähnlich wie der Speierling. Seltener wird zudem Vogelbeerwein angeboten.

Die Borke kann zum Braun- und Rotfärben von Wolle verwendet werden.

Der Vogelbeerbaum war den Germanen als Thor geweihter Baum heilig. In der Snorra-Edda (Skáldskaparmál 18) wird beschrieben, wie sich Thor an einem Ast der Eberesche aus dem Fluss Wimur zog.
In ärmlichen Waldgegenden war das Holz so begehrt, dass die Förster früher Not hatten, die Bäume vor den armen Drehern von Spielwaren, die ihr Holz nicht gern teuer kauften, zu schützen. Im Erzgebirge hat der Vogelbeerbaum den Status eines Nationalbaums und wird im von Max Schreyer gedichteten Volkslied vom "Vuglbärbaam" besungen. In Schottland gehört der "Rowan tree" vor jedes „gute Haus“, wenn Hexen keinen Einlass finden sollen.

In Dalsland in Schweden schmückt der Hirte an einem dem Himmelfahrtstag vorangehenden oder nachfolgenden Tag sein Vieh an den Hörnern mit Blumen und treibt es daraufhin bereits um die Mittagszeit nach Hause. Er selbst führt, mit einem geschmückten Vogelbeerbaum in beiden Händen, die Herde an. Im Stall wird der Baum an den Giebel gepflanzt und soll während der Weidezeit die Tiere vor bösen Geistern und Krankheit bewahren. Das Jungvieh wird benannt, indem es bei Verkündung seines Namens mit einer Rute des Vogelbeerbaums dreimal auf den Rücken geschlagen wird.

Nach dem keltischen Baumkreis – einer Erfindung des keltischen Neopaganismus – zählt die Eberesche – neben Apfelbaum, Walnuss und Tanne – zu den Lebensbäumen. Menschen, die in ihrem Zeichen geboren sind, wird vor allem Lebensfreude, aber auch Anpassungsfähigkeit an schwierige Lebensumstände nachgesagt. Die Kelten bepflanzten ihre heiligen Stätten, besonders Orakel- und Richtplätze, oftmals mit der Pflanze. Man sagt, dass sie die Eberesche zum Symbol des Wiedererwachens nach der dunklen Winterzeit gemacht haben. Einem irischen Sprichwort zufolge gilt die Vogelbeere als Schutzbaum gegen Blitzschlag und Hexenzauber. Äußerlich angewandt sollen die Beeren Wunden heilen, verzehrt man sie, so verlängert sich das Leben um ein weiteres Jahr.

Die Vogelbeere wurde in Deutschland zum Baum des Jahres 1997 erklärt.




</doc>
<doc id="14568" url="https://de.wikipedia.org/wiki?curid=14568" title="Wärmerauschen">
Wärmerauschen

Wärmerauschen, "thermisches Rauschen", Widerstandsrauschen, "Nyquist-Rauschen", "Johnson-Rauschen" oder "Johnson-Nyquist-Rauschen" genannt, ist ein weitgehend weißes Rauschen, das aus der thermischen Bewegung der Ladungsträger in elektrischen Schaltkreisen hervorgeht. Das Frequenzspektrum des Widerstandsrauschens wurde von John Bertrand Johnson experimentell erforscht und gleichzeitig von Harry Theodor Nyquist theoretisch begründet.

Wärmerauschen äußert sich bei unbelasteten ohmschen Widerständen als "thermisches Widerstandsrauschen", oft einfach "Widerstandsrauschen" genannt. Die thermische Bewegung der Leitungselektronen erzeugt an den Klemmen des Zweipols den Rauschstrom und die Rauschspannung. Die bei Kurzschluss oder Leerlauf vorliegenden Werte können als spektrale Rauschleistungsdichte allgemein angegeben werden. Sie sind proportional zur absoluten Temperatur. Beim unbelasteten Bauelement ist die Rauschleistung unabhängig vom elektrisch leitenden Medium, dagegen kann beim von Gleichstrom durchflossenen Bauelement Stromrauschen hinzu kommen, das beim Kohleschichtwiderstand weit über dem thermischen Rauschen liegen kann. Beim stromdurchflossenen Halbleiter entsteht Zusatzrauschen durch Modulation des Laststroms – bei Spannungseinprägung – wegen thermisch bedingter Schwankung der Trägerzahl im Leitungsband und Valenzband und damit der Leitfähigkeit.

Johnson experimentierte in den Jahren 1927/28 bei Temperaturen zwischen der Siedetemperatur des Stickstoffs und der des Wassers mit Widerständen sehr unterschiedlichen Materials. Verwendet wurden unter anderen Kohleschicht-, Kupfer- und Platinwiderstände sowie mit verschiedensten Elektrolyten gefüllte Kapillaren.

Johnson teilte mit, Schottky habe im Jahre 1918 aus theoretischen Erwägungen erkannt, dass Wärmerauschen von Leitungselektronen mit Röhrenverstärkern zu entdecken sein müsse, aber mit einem Resonanzkreis am Verstärkereingang werde der gesuchte Effekt durch das Schrotrauschen maskiert. Nyquist zitierte Schottkys Arbeit wegen der daraus gewonnenen Anregung, die elektrodynamische Rauschleistung aus Thermodynamik und statistischer Mechanik abzuleiten.

Die Leitungselektronen elektrisch leitender Materialien (Metalle, Halbleiter) nehmen an der weitgehend ungeordneten, thermisch angeregten Bewegung der Komponenten der atomaren Ebene teil und bewegen sich zufällig und ungerichtet. Sie tragen bei Raumtemperatur in geringem Maße zur spezifischen Wärme bei, und ihre ungeordnete Bewegung stellt an den Klemmen eines Zweipols die hier in Rede stehende endliche elektrische Rauschleistung zur Verfügung. Die Leitungselektronen erzeugen mit großer Rate statistisch unabhängige Spannungs- und Stromimpulse von endlicher, kurzer Dauer, deren Überlagerung zu der breiten Frequenzverteilung führt, die in der Elektrotechnik meistens als Rauschquelle mit weißem Spektrum wahrgenommen wird. Das Rauschleistungsspektrum reicht von der Frequenz null bis zu einer Grenzfrequenz, deren Wert durch die thermisch noch merklich anregbaren Quanten der elektromagnetischen harmonischen Komponenten bestimmt ist. Die erste Berechnung des Rauschspektrums von Nyquist macht vom Gleichverteilungssatz der Thermodynamik Gebrauch. – Eine endliche Gleichspannungskomponente wird nicht beobachtet; sie könnte nicht als zufällige Komponente betrachtet werden, vgl. Thermoelektrizität. Dazu wäre eine Symmetriebrechung notwendig, für die keine Veranlassung ersichtlich ist, weil beim Widerstandsrauschen thermodynamisches Gleichgewicht vorausgesetzt wird.

Das Widerstandsrauschen wird hier durch das in weiten Frequenzgrenzen weiße Leistungsspektrum charakterisiert. Eine andere Fragestellung ist die Beschreibung durch die Amplitudenverteilung der Momentanwerte von Spannung oder Strom. Erfahrungsgemäß liegt eine Normalverteilung (Gaußverteilung) mit Mittelwert null vor, deren Streuparameter durch die Rauschleistung gegeben ist. Insbesondere kann demnach eine beliebig große Amplitude erwartet werden bei exponentiell abnehmender Wahrscheinlichkeit.

Die stochastische Amplitudenstatistik bedingt, dass Rauschspannungen unter echter quadratischer Gleichrichtung gemessen werden müssen. Johnson verwendete dazu (nach elektronischer Verstärkung) einen Thermoumformer, in dem die Wärmeentwicklung durch die zugeführte Rauschleistung eine Temperaturerhöhung bewirkt. Diese wird mit einem Thermoelement gemessen, dessen zeitlich linear gemittelte Thermospannung dem Mittelwert des Rauschspannungsquadrats proportional ist. Diese Messvorschrift ist etwas verallgemeinert durch die Definition der Autokorrelationsfunktion mathematisch formuliert. Der Konvertierungsfaktor des Thermoumformers wird mit einer durch eine Gleichspannung gut definierbaren Leistung gemessen.

Analog den Zufallsschwankungen bei der brownschen Bewegung werden an einem ohmschen Widerstand im Verlaufe der Zeit formula_1 Schwankungen der Leerlaufspannung formula_2 beobachtet. Der Mittelwert dieser Spannungen ergibt null. Als Rauschgröße wird nach elektronischer Verstärkung der quadratische Mittelwert formula_3 der Spannung gemessen, der in den Effektivwert umgerechnet werden kann. Das mittlere Spannungsquadrat ist proportional der absoluten Temperatur formula_4, der Größe formula_5 des elektrischen Widerstandes und der Bandbreite formula_6 der Messanordnung.

Der Einfluss der Bandbreite ist mit einem breitbandigen Aufbau nicht leicht erkennbar, die Amplitudenstatistik lässt sich dabei recht gut beurteilen. Deren Varianz ist durch formula_3 gegeben. Die Amplitudenstatistik kann schmalbandig gut ermittelt werden. Schmalbandig ist der Einfluss einer bei formula_8 zentrierten Bandbreite deutlich an den Ein- und Ausschwingzeiten proportional zu formula_9 zu erkennen, durch die die Komponenten des Rauschspektrums um formula_8 moduliert sind.


Die Nyquist-Formel stellt folgenden Zusammenhang für die Rauschspannung im Leerlauf her:

mit der effektiven Leerlaufrauschspannung
folglich

Dabei sind "formula_14" die Boltzmann-Konstante, formula_4 die absolute Temperatur und formula_5 der ohmsche Widerstand des rauschenden Zweipols. formula_6 ist die zugelassene Bandbreite.

Dual dazu berechnet sich das zeitlich gemittelte Rauschstromquadrat formula_18 im Kurzschlussfall zu

mit dem effektiven Kurzschlussrauschstrom

Zur Allgemeingültigkeit der Formel von Nyquist und zu ihrer Bedeutung für tief reichende Fragen der Physik gibt Ginsburg umfassend Auskunft.

Das Ersatzschaltbild eines rauschenden Widerstands als konzentriertem Bauelement ist die Reihenschaltung des rauschfrei gedachten Widerstands "R" als Quellwiderstand mit der sein Rauschen darstellenden Spannungsquelle, die das Leerlaufspannungsquadrat formula_21 abgibt. Zur Darstellung mit einer Rauschstromquelle wird ein Quellstromgenerator vom Kurzschlussstromquadrat formula_22 dem idealen Innenwiderstand formula_5 parallel geschaltet.

Bei Kurzschluss dissipiert der rauschende ohmsche Widerstand selbst die generierte Leistung

weil die volle Quellenspannung über ihm abfällt.

Bei Leistungsanpassung dissipiert jeder der beiden rauschenden ohmschen Widerstände im jeweils anderen "und" bei sich selbst die Leistung

weil die halbe Quellspannung über ihnen abfällt. Dieses ist maximal von einer Quelle abgebbare Leistung und wird "verfügbare Leistung" genannt. Dieser Begriff macht von Zufälligkeiten einer Schaltung und von formula_5 unabhängig und eignet sich für eine allgemeine Diskussion, indem der thermisch aktivierte, aber elektrodynamisch vermittelte Energieaustausch der beiden rauschenden, an ein Wärmebad der Temperatur formula_4 gekoppelten Widerstände symmetrisch erfolgt.

Diese vier dissipierten Rauschleistungen ergeben zusammen wieder die Kurzschlussleistung, die folglich in dieser Anordnung ebenfalls insgesamt generiert wird. Die beiden zur Leistungsanpassung zusammengeschalteten Widerstände arbeiten, als eine Einheit vom Widerstand formula_28 aufgefasst, im Kurzschluss und ihre dissipierte Leistung ist von der Größe formula_29 und damit ebenfalls formula_30 wie für jeden Widerstand einzeln.


Die Formulierung als Leistungsbilanz erübrigt die Verwendung der Größe "elektrischer Widerstand" und verdeutlicht wegen dieser Allgemeingültigkeit die vorgeschlagene Benutzung des Lemmas Wärmerauschen. Leistung ist wegen der notwendig quadratischen Gleichrichtung ohnehin die eigentliche Messgröße.

Die Integration obiger Gleichungen über den gesamten Frequenzbereich führt zur Ultraviolett-Katastrophe. Ein streng weißes Spektrum verlangt außerdem die unrealistische Beteiligung beliebig kurz dauernder Impulse zur Anregung der harmonischen Komponenten. Deshalb ist für hohe Frequenzen die quantentheoretische Erweiterung notwendig. Nyquist leistete dies bereits. Die später erkannte quantenmechanische Nullpunktenergie wird als mögliche nicht thermische Rauschquelle gelegentlich angeführt.

Für hinreichend hohe Frequenzen oder entsprechend niedrige Temperaturen muss die, von Nyquist ebenfalls schon angegebene Formel

verwendet werden.
Dabei wurde zuletzt bereits die "quantentheoretische Grenzfrequenz" benutzt, definiert durch

Bei Raumtemperatur (300 K) beträgt sie formula_37.

Ein Beitrag der Nullpunktenergie zum Wärmerauschen wird gelegentlich zur Diskussion gestellt. Die Nullpunktenergie ist durch die heisenbergsche Unbestimmtheit gefordert und beträgt beim harmonischen Oszillator formula_40. Als vollständig korrigierte quantenmechanische Formel wird
häufig vorgeschlagen. Mit dieser Formel würde die Ultraviolett-Katastrophe "verstärkt" wieder eingeführt.

Die Nullpunktenergie steht für "thermische" Prozesse wie Wärmerauschen zum Austausch von Energie mit einem Lastwiderstand formula_5 nicht zur Verfügung. Die letztere, den quantenmechanischen Ansatz ganz "unmittelbar" ausdrückende Formulierung verlangt offensichtlich, dass die bei hinreichend hohen Frequenzen oder hinreichend tiefen Temperaturen allein der Nullpunktschwingung zuzuschreibende und bei Leistungsanpassung zwischen Quell- und Lastwiderstand auszutauschende "verfügbare" spektrale Leistungsdichte formula_43 sei.
Für den Maser wurde gezeigt, dass die Nullpunktenergie nicht verstärkt wird.

Das Leistungsspektrum betont die Tatsache, jeder elektromagnetischen Frequenzkomponente einzeln, unabhängig von den Schwingungen anderer Frequenz, einen eigenen thermischen Freiheitsgrad zubilligen zu müssen, Äquipartitionstheorem. Nyquist zeigt dieses für den elektromagnetischen Fall gedanklich durch Schaltung eines (nichtdissipativen) Reaktanzfilters zwischen die in Leistungsanpassung befindlichen Widerstände. Wären die harmonischen Schwingungen unterschiedlicher Frequenz nicht gleich stark an das Wärmebad gekoppelt, so könnte im Widerspruch zum 2. Hauptsatz der Wärmelehre der kältere Widerstand die Temperatur des wärmeren im Mittel erhöhen.


Das Leistungsspektrum für die "verfügbare Leistung" eines beliebigen ohmschen Widerstands wird definiert durch

mit dem Niederfrequenzwert

"Bemerkung:" Die spektrale "Leistung"sdichte ist von der Dimension "Energie".

Für Leistungsanpassung gilt

Die "verfügbare Gesamtleistung" ist

Die durch die Quantentheorie begrenzte "effektive Bandbreite" ist unter der Annahme einer durchgehend konstant weiß angenommenen spektralen Leistung formula_51

Die verfügbare Gesamtleistung bei Raumtemperatur (300 K) ist formula_53.

Zwei ohmsche Zweipole vom gleichen frequenzunabhängigen Widerstand formula_5 im Wärmebad der absoluten Temperatur formula_4 seien durch eine verlustlose Leitung vom Wellenwiderstand formula_56 verbunden, s. reelle Wellenimpedanz. Wegen dieser "Anpassung nach dem Wellenwiderstand" befinden sich auf der Leitung nur fortschreitende Wellen beider Ausbreitungsrichtungen. Einflüsse durch stehende Wellen infolge Reflexion sind nicht vorhanden, infolgedessen liegt Frequenzselektivität nicht vor. Bei dieser Beschaltung besteht ohnehin Leistungsanpassung.
Die elektromagnetischen Wellen auf der Leitung werden durch die rauschenden Widerstände emittiert und im jeweils anderen vollständig absorbiert.
Die zum anderen Widerstand übertragene Leistung stört das thermodynamische Gleichgewicht nicht, im Mittel findet kein gerichteter Energietransport statt.

Im Niederfrequenzgebiet ist die Anregung der elektromagnetischen Wellen nicht quantentheoretisch gemindert. Das weiße Spektrum besagt: mittels der Leitung wird durch jede Spektralkomponente der Frequenz formula_8 die verfügbare Schwankungsenergie formula_62 vom einen zum anderen Widerstand übertragen. Sie entspricht zwei Freiheitsgraden, was im Einklang mit der elektromagnetischen Natur des Übertragungsmechanismus ist. Elektrisches und magnetisches Feld steuern je einen Freiheitsgrad bei und daher nach dem Gleichverteilungssatz je die mittlere Schwankungsenergie formula_63.

Die Niederfrequenznäherung formula_64 in der Gestalt formula_65 gibt mit dem Faktor formula_66 die Anzahl der erregten Photonen formula_46 an. Fast 10 Quanten sind bei Raumtemperatur in der elektromagnetischen Welle der Frequenz formula_68 kondensiert, der potenziell quantenhafte Charakter der Welle kommt nicht augenfällig zum Tragen. – Die Spektralkomponente formula_8 einer elektromagnetische Welle kann beliebig viele Quanten formula_46 aufnehmen, vgl. Photonen und Bosonen.

Die Hochfrequenznäherung formula_71 mit formula_72 führt auf den Boltzmann-Faktor entsprechend der geringeren Verfügbarkeit entsprechend großer Energiebeträge im Wärmebad. Die Quanten formula_46 lassen sich thermodynamisch mit großer Ausbeute nur bis zur Größenordnung formula_62 effizient anregen, größere Quanten formula_46 sind bei vergleichsweise kleinen thermisch zur Verfügung stehenden Energien formula_62 "eingefroren" im Sinne des Einfrierens beispielsweise der Rotationsfreiheitsgrade der spezifischen Wärme bei niedrigen Temperaturen.

Bei formula_77 ist formula_78 und mit formula_79 wäre die quantentheoretische Frequenzgrenze gerade deutlich merkbar, nur in rund der Hälfte der Zeit wäre die elektromagnetische Mode mit einem Photon besetzt. Für Frequenzen bis zu 1 GHz kann der ideale "Schwarze Wellenleiter" mit gängigen elektrotechnischen Mitteln jedoch kaum hinreichend genau realisiert werden.

"Ein Vergleich: "Oben wurde die Gesamtleistung "P" = 4,26 · 10 Watt für Raumtemperatur berechnet. Bei ebenfalls "T" = 300 K wird vom "Schwarzen Strahler" nach dem Stefan-Boltzmannschen Gesetz bereits von einer Fläche 10 m² ungefähr dieselbe Leistung 4,6 · 10 Watt in den Halbraum abgestrahlt.

Der rauschende Widerstand formula_5 arbeite auf den idealen Kondensator der Kapazität formula_81.

Das Leerlauf-Spannungsspektrum formula_82 des Wärmerauschens ist an der kapazitiven Last um das Betragsquadrat formula_83 des Spannungsteilerfaktors reduziert.

Jedem ohmschen Widerstand als Bauelement liegt eine kleine Streukapazität parallel, das Spektrum seiner Klemmenspannung ist in der Praxis

Im thermischen Gleichgewicht wird gemäß der Formel formula_87 für die Energie auf einem Kondensator bei einer Kondensatorspannung formula_88 die mittlere Energie

gespeichert, wobei zuletzt formula_90 durch den Niederfrequenzwert formula_62 ersetzt ist. Dem Kondensator wird ständig in rund der Dauer formula_92, der Korrelationszeit, etwa die Energie formula_93 zugeführt und entzogen.

Die "effektive Bandbreite" des "RC"-Gliedes ist definiert durch


Die zur gespeicherten Energie formula_97 komplementäre Energie formula_98 der von formula_5 im effektiven Frequenzintervall formula_100 thermisch generierten Gesamtenergie formula_101 wird in formula_5 selbst dissipiert.

Diese Bilanz ist von der Aufladung eines Kondensators mit einer Konstantspannung bekannt und kann aus dem Prinzip der minimalen Entropieproduktion hergeleitet werden.
Natürlich wird die außerhalb der effektiven Bandbreite erzeugte Leistung in formula_5 selbst dissipiert; denn mit wachsendem formula_104 arbeitet der Widerstand zunehmend im Kurzschluss.

Die Zeitkonstante formula_105 und damit das effektive Frequenzband formula_106 fallen gerade so aus, dass dem "einen" thermischen Freiheitsgrad des Kondensators genügt wird.

"Folgerung 1:" Jeder reale Kondensator besteht im Ersatzschaltbild aus einem idealen Kondensator mit parallel geschaltetem, endlichen Isolationswiderstand, wodurch er die Ankopplung an ein Wärmebad erfährt. Der reale Kondensator speichert daher die zugeführte, nur von der Temperatur abhängige mittlere Energie formula_107
Gemäß formula_108 liegt am Kondensator die effektive Rauschspannung formula_109 wozu "dem Betrage nach" im Mittel formula_110 Elektronenladungen formula_111 gespeichert werden. An einem Kondensator von 1 pF beträgt bei Raumtemperatur die effektive Rauschspannung 64 µV, die 402 Elementarladungen benötigt, die im Mittel für die zufälligen Spannungsschwankungen transportiert werden. Erinnert wird an die Tatsache formula_112 und formula_113.

"Folgerung 2:" Die grundlegende Proportionalität der Rauschleistung zur absoluten Temperatur formula_4 wird "unmittelbar" erkennbar, wenn das Rauschspannungsquadrat formula_115 über einem Kondensator hochohmig gemessen wird. Ein Drahtwiderstand dient zweckmäßigerweise als rauschender Widerstand formula_5, weil er sehr große Temperaturänderungen erlaubt; gemäß der Formel beeinflusst seine unvermeidliche Temperaturabhängigkeit das Messergebnis bei dieser Schaltung "nicht".

Diese Anordnung eignet sich für ein eindrucksvolles "Demonstrationsexperiment". formula_117 muss stets so groß sein, dass das Eigenrauschen des Verstärkers nicht stört.

Tatsächlich müsste das Spannungsspektrum formula_90 als quantentheoretische Formel integriert werden, doch das bis zur elektrotechnischen Grenzfrequenz formula_119 reichende Frequenzband eines realen Kondensators begrenzt das wirksame Spektrum bei 300 K weit unterhalb der quantentheoretischen Grenzfrequenz formula_38

Diese Tatsache wird im Folgenden ausgenutzt zur Berechnung der im rauschenden Widerstand selbst unter kapazitiver Last dissipierten Leistung. Im Unterschied zum Vorstehenden ist hier das Spannungsquadrat über dem Widerstand selbst zu betrachten, das mit dem Betragsquadrat formula_121 des komplexen Spannungsteilerfaktors zu bewerten ist. Die in formula_5 dissipierte Leistung ist

Indem zum elektrotechnischen Teilerfaktor im Intergranden 1 addiert und subtrahiert wird und −1 in diesen Teilerfaktor eingerechnet wird, ergibt sich mit der quantentheoretischen Grenzfrequenz zunächst

Das Integral über den ersten Summanden, die Kurzschlussleistung in "R" selbst, wurde oben bereits ausgewertet, das Integral über den zweiten wird – meistens in ausgezeichneter – Näherung berechnet, indem vereinfachend der Faktor formula_125 gleich 1 gesetzt wird, weil das Frequenzband bis formula_38 im Allgemeinen wesentlich weiter ausgreift als das elektrotechnisch bedingte bis formula_119. Das unmittelbar erhaltene Ergebnis ist mit den Bandbreiten beziehungsweise den effektiven Bandbreiten ausgedrückt

Der zweite Term ist klein gegen den ersten, der die mittlere in "R" dissipierte Gesamtleistung bei Kurzschluss darstellt. Diese wird durch die kapazitive Last um die Leistung   formula_129 geschmälert, indem die Kondensatorspannung den Spannungsabfall über "R" und den Strom im Kreis mindert. Kondensatorspannung und Strom sind außer Phase, kennzeichnend für die "Speicherung" der Energie und den Transport von Blindleistung formula_62 in der Zeit formula_131.

Die Stoßvorgänge und die Emissions- und Absorptionsprozesse im Widerstandsmaterial verlaufen im Mittel zeitlich gleichverteilt, solange der Widerstand nicht altert. Insoweit ist das Widerstandsrauschen stationär. Die Auszeichnung einer Zeitmarke wie "t" = 0 hat für die allgemeine Charakterisierung des Rauschens keine Bedeutung. Damit erübrigt sich die Unterscheidung eines ungeraden und geraden Anteils der Urspannung formula_2, so dass der Tangens eines Phasenwinkels als dem üblichen Maß für deren Verhältnis kein wichtiges Kennzeichen ist für das stationäre Rauschen selbst. Folglich sollten zur mathematisch invarianten Beschreibung statt der Fouriertransformierten von formula_2, dem Amplitudenspektrum, quadratische Größen gewählt werden, wie vorstehend das Leistungsspektrum. Sie enthalten bereits hinreichende Informationen über die "zeitliche Struktur".

Als Information über Amplituden erleichtert formula_134 den gewohnten Vergleich mit einer Gleichspannung gleicher Wärmeerzeugung. Außer der zeitlichen Struktur kann die oben erwähnte "Amplitudenverteilung" ausgewertet werden. Die beiden Verteilungen sind voneinander unabhängig, allerdings beeinflusst eine Beschränkung des Frequenzbandes die Streuung formula_21 der Amplitudenstatistik. Zum weißen Spektrum gehört nicht zwingend eine Normalverteilung der Momentanwerte, wie sie beim Widerstandsrauschen vorliegt.

Zur Charakterisierung des stationären Rauschens im Zeitverlauf verbleibt nicht nur das mittlere Spannungsquadrat formula_21.

Die Abkürzung AKF ist für die Autokorrelationsfunktion eingeführt. Die AKF ist unabhängig von der Zeitrichtung: formula_139 und formula_140 haben dieselbe Autokorrelationsfunktion. Die Definitionsformel lässt unmittelbar erkennen, dass die Auszeichnung einer beliebigen Zeit formula_141 als neue Bezugszeit durch formula_142 keinen Einfluss hat.

Die AKF hat bei formula_143 ihr Maximum
formula_145 ist die im Widerstand formula_5 durch die Klemmenspannung formula_2 dissipierte Leistung.

Die AKF ist stets eine gerade Funktion von formula_148. Das bedeutet, dass keine kausale Abfolge durch die Zeit formula_1 indiziert ist. Dennoch sind formula_2 und formula_151 nicht unabhängig, formula_2 kann sich nicht beliebig schnell ändern. Das Leistungsspektrum legt beispielsweise durch seine obere Grenzfrequenz die wirksame schnellst mögliche Änderung fest.

Mit der AKF ist für die zeitpunktorientierte (oder "lokale") Beschreibungsebene (Zeitbereich) die Entsprechung zum Frequenzspektrum gewonnen. Letzteres beschreibt den inneren Zusammenhang für die Beschreibungsebene mit harmonischen Schwingungen (Frequenzbereich).


Tatsächlich begründet eine mathematische Transformation die äquivalente Darstellung des stationären Prozesses durch die AKF oder durch das Frequenzspektrum. Den Beweis erbrachten Wiener und Chintchin mit der Feststellung, dass die Fouriertransformation das gewünschte Ergebnis liefert:

formula_154 ist aus Gründen der Symmetrie der Transformationsformeln für negative Frequenzen definiert. Daher ist formula_155 zu beachten, formula_90 wurde oben in Anlehnung an den Messprozess nur für formula_157 definiert.

Widerstandsrauschspektren sind als Autospektren reelle, gerade Funktionen der Frequenz. Die Stellung der Vorzeichen im Exponenten ist insoweit Konvention, sie wird wie angegeben gewählt im Hinblick auf Kreuzkorrelationsfunktionen, bei denen die kausale Verkettung ein Ziel der Analyse ist.

Bei dem Transformationspaar rechts sind im Integranden die komplexe Exponentialfunktion durch formula_158 ersetzt und die Integrationsgrenzen 0 und formula_159 weil gerade Funktionen transformiert werden. Dies ist die "klassische Wiener-Chintchin-Formulierung," wobei häufig noch formula_160 durch das der Messtechnik näher stehende formula_90 ersetzt ist.

Die AKF zum Spektrum formula_162 der Klemmen"spannung" des Widerstands mit parallel liegender Streukapazität ist

Die Leistung, die bei parallel liegendem Kondensator der Kapazität formula_81 im rauschenden Widerstand selbst dissipiert wird, ist

Die normierte AKF wird allein durch den statistischen Zusammenhang bestimmt
Die mittlere Korrelationsdauer wird definiert durch


Exkurs zur messtechnischen Bedeutung der Korrelationszeit. Den verrauschten Ausschlag eines Messinstrumentes zu messen, erfordert viele "unabhängige" Ablesungen für eine ausreichende Statistik zur Berechnung von Mittelwert und seinem Fehler mit der gewünschten Genauigkeit. (Gaußsches Rauschen ist dazu von Vorteil.)

Die AKF zum quantentheoretisch begrenzten Spektrum formula_90 der verfügbaren "Leistung" ist hierunter berechnet.

Hinweis 1: formula_170 definiert auf formula_171 geht in diese Formel ein.

Hinweis 2: Vorstehend ist die Korrelationsfunktion der Klemmen"spannung" behandelt worden, jetzt ist formula_172 von der Dimension "Leistung".
Daraus folgt zunächst die oben bereits berechnete "verfügbare" Gesamtleistung
Die normierte AKF des quantenmechanisch begrenzten Rauschspektrums beschreibt wieder die innere zeitliche Struktur allein
zeigt, dass

Damit wird beispielhaft deutlich, dass ein schwacher Abfall des Spektrums einen steilen der Korrelationsfunktion zur Folge hat und umgekehrt. Das kapazitiv proportional zu formula_179 begrenzte Spektrum ist mit einem exponentiellen Abfall des statistischen Gewichts steigender Korrelationszeiten verknüpft. – Das quantentheoretisch begrenzte Spektrum fällt mit wachsender Frequenz praktisch exponentiell ab, seine Korrelationsfunktion schließlich näherungsweise nur entsprechend formula_180

Zur Frage des breiten Spektrums bei innerem Zusammenhang kurzer Dauer und umgekehrt wird der Extremfall angeführt. Dem weißen Spektrum entsprechen beliebig kurzdauernde Vorgänge. Ein Impuls, der im Entstehen schon wieder vergeht, kann dazu dienen und ist mit der Dirac-Distribution formula_181 mathematisch wohl definiert. Von diesem beliebig kurzzeitigen Objekt können nur die Werte formula_182 für formula_183 finit angegeben werden. Dennoch eignet es sich diese Delta-Distribution wegen der Mittelwerteigenschaft
zur Darstellung physikalischer Sachverhalte.

formula_181 führt zwingend auf Korrelationsfunktionen: Weil kein Quadrat der Distribution gebildet werden kann, muss zur Berechnung der Leistung auf die AKF, vgl. Faltungsintegral, zurückgegriffen werden:

Der Spannungspuls zur Zeit formula_187
erzeugt den Spannungsstoß
der Einheit 1 Vs und hat die AKF "beliebig kurzer" Korrelationszeit
sowie das "weiße" Frequenzspektrum

Umgekehrt führt das beliebig schmale Frequenzband bei formula_192
auf die AKF beliebig weit reichender "periodischer" Korrelation

Mit formula_195 wird die Korrelationsdauer beliebig groß. Bei der Gleichspannung formula_196 gilt

Hier kann einfach von unendlich großer Korrelationsdauer gesprochen werden bei ebenfalls streng lokalisiertem Spektrum.
Vorstehend definierte Spannungspulse sollen voneinander unabhängig zu beliebigen Zeiten gleich wahrscheinlich mit der mittleren Anzahldichte je Zeitintervall formula_198 erzeugt werden, sie bilden eine stationäre Folge. Die Spannungsstöße "p" seien mit positivem oder negativem Vorzeichen gleich häufig versehen, damit der lineare Mittelwert, die Gleichkomponente, verschwindet. Die Pulse seien statistisch unabhängig. Eine solche Konstruktion könnte als erster Ansatz für eine Beschreibung des Wärmerauschens gelten. Allerdings genügen die Momentanwerte offensichtlich nicht einer Normalverteilung (Glockenkurve).

Die statistische Unabhängigkeit erlaubt die einfache Angabe der AKF dieser Folge mit Hilfe des "Theorems von Campbell:"
Die AKF (Dimension "Leistung" der SI-Einheit 1 W nach Division durch einen Widerstand "R") ändert ihren Verlauf nicht, die Korrelationszeit bleibt verschwindend klein. Das Frequenzspektrum (Dimension "Energie" der Einheit 1 Ws nach Division durch den Widerstand "R", als Leistung pro Frequenzbandbreite) ändert sich ebenfalls nicht bis auf den Faktor formula_198


Zur Veranschaulichung werden in der vorstehend beschriebenen Impulsfolge – unter entsprechenden Bedingungen – die Stoßfunktionen durch Exponentialimpulse

ersetzt. Die AKF und das Frequenzspektrum, ein Lorentzprofil, der modifizierten Spannung sind:

Zu den Termen in eckigen Klammern s. Bemerkung.

formula_204 ist die am Widerstand formula_5 dissipierte Leistung. Durch das Produkt formula_206 kann der Grad der Überlappung eingestellt werden.

AKF und Spektrum haben dieselbe Abhängigkeit von formula_148 beziehungsweise formula_8 wie beim Rauschen des Widerstands mit parallelem Kondensator, s. oben, obgleich die Einzelimpulse sicher wesentlich verschieden sind. Entsprechend formula_209 entlädt sich mit der Zeitkonstanten formula_168 ein Kondensator über einen Widerstand.





</doc>
<doc id="14569" url="https://de.wikipedia.org/wiki?curid=14569" title="Funktionentheorie">
Funktionentheorie

Die Funktionentheorie ist ein Teilgebiet der Mathematik. Sie befasst sich mit der Theorie differenzierbarer komplexwertiger Funktionen mit komplexen Variablen. Da insbesondere die Funktionentheorie einer komplexen Variablen reichlich Gebrauch von Methoden aus der reellen Analysis macht, nennt man das Teilgebiet auch komplexe Analysis.

Zu den Hauptbegründern der Funktionentheorie gehören Augustin-Louis Cauchy, Bernhard Riemann und Karl Weierstraß.

Eine "komplexe Funktion" ordnet einer komplexen Zahl eine weitere komplexe Zahl zu. Da jede komplexe Zahl durch zwei reelle Zahlen in der Form formula_1 geschrieben werden kann, lässt sich eine allgemeine Form einer komplexen Funktion durch

darstellen. Dabei sind formula_3 und formula_4 reelle Funktionen, die von zwei reellen Variablen formula_5 und formula_6 abhängen. formula_3 heißt der "Realteil" und formula_4 der "Imaginärteil" der Funktion. Insofern ist eine komplexe Funktion nichts anderes als eine Abbildung von formula_9 nach formula_9 (also eine Abbildung, die zwei reellen Zahlen wieder zwei reelle Zahlen zuordnet). Tatsächlich könnte man die Funktionentheorie auch mit Methoden
der reellen Analysis aufbauen. Der Unterschied zur reellen Analysis wird erst deutlicher, wenn man komplex-differenzierbare Funktionen betrachtet und dabei die multiplikative Struktur des Körpers der komplexen Zahlen ins Spiel bringt, die dem Vektorraum formula_9 fehlt. Die grafische Darstellung komplexer Funktionen ist etwas umständlicher als gewohnt, da nun vier Dimensionen wiedergegeben werden müssen. Aus diesem Grund behilft man sich mit Farbtönen oder -sättigungen.

Der Differenzierbarkeitsbegriff der eindimensionalen reellen Analysis wird in der Funktionentheorie zur komplexen Differenzierbarkeit erweitert. Analog zum reellen Fall definiert man: Eine Funktion einer komplexen Variablen heißt "komplex-differenzierbar" (im Punkt formula_12), falls der Grenzwert

existiert. Dabei muss formula_14 in einer Umgebung von formula_12 definiert sein. Für die Definition des Grenzwerts muss dabei der komplexe Abstandsbegriff verwendet werden.

Damit sind für komplexwertige Funktionen einer komplexen Variablen zwei verschiedene Differenzierbarkeitsbegriffe definiert: die "komplexe Differenzierbarkeit" und die Differenzierbarkeit der zweidimensionalen reellen Analysis ("reelle Differenzierbarkeit"). Komplex-differenzierbare Funktionen sind auch reell-differenzierbar, die Umkehrung gilt nicht ohne zusätzliche Voraussetzungen.

Funktionen, die in einer Umgebung eines Punktes komplex-differenzierbar sind, nennt man "holomorph" oder "analytisch". Diese haben eine Reihe hervorragender Eigenschaften, die es rechtfertigen, dass sich eine eigene Theorie hauptsächlich damit beschäftigt – eben die Funktionentheorie.
Zum Beispiel ist eine Funktion, die einmal komplex-differenzierbar ist, automatisch beliebig oft komplex-differenzierbar, was im reellen Fall natürlich nicht gilt.

Einen anderen Zugang zur Funktionentheorie bietet das System der Cauchy-Riemannschen Differentialgleichungen

Eine Funktion ist nämlich genau dann komplex differenzierbar in einem Punkt, wenn sie dort reell differenzierbar ist und das System der Cauchy-Riemannschen Differentialgleichungen erfüllt. Daher könnte man die Funktionentheorie auch als Teilgebiet der Theorie der partiellen Differentialgleichungen verstehen. Jedoch ist die Theorie mittlerweile zu umfangreich und zu vielseitig mit anderen Teilgebieten der Analysis vernetzt, als dass man sie in den Kontext der partiellen Differentialgleichungen einbetten würde.

Geometrisch interpretieren lässt sich die komplexe Differenzierbarkeit als (lokale) Approximierbarkeit durch orientierungstreue affine Abbildungen, genauer durch Verkettungen von Drehungen, Streckungen und Translationen. Entsprechend ist die Gültigkeit der Cauchy-Riemannschen Differentialgleichungen äquivalent damit, dass die zugehörige Jacobi-Matrix die Darstellungsmatrix einer Drehstreckung ist. Holomorphe Abbildungen erweisen sich demzufolge (abseits der Ableitungsnullstellen) als lokal konform, d. h. winkel- und orientierungstreu.

Mit einem Integrationsweg, der keinerlei Singularitäten von formula_14 umläuft und für dessen Umlaufzahl um formula_18 gilt, dass

gilt die cauchysche Integralformel:

Diese besagt, dass der Wert einer komplex-differenzierbaren Funktion auf einem Gebiet nur von den Funktionswerten auf dem Rand des Gebiets abhängt.

Da die Menge der holomorphen Funktionen recht klein ist, betrachtet man in der Funktionentheorie auch Funktionen, die außer in isolierten Punkten überall holomorph sind. Diese isolierten Punkte werden isolierte Singularitäten genannt. Ist eine Funktion in einer Umgebung um eine Singularität beschränkt, so kann man die Funktion in der Singularität holomorph fortsetzen. Diese Aussage heißt riemannscher Hebbarkeitssatz. Ist eine Singularität formula_21 einer Funktion formula_14 nicht hebbar, hat jedoch die Funktion formula_23 in formula_21 eine hebbare Singularität, so spricht man von einer Polstelle k-ter Ordnung, wobei k minimal gewählt ist. Hat eine Funktion isolierte Polstellen und ist sonst holomorph, so nennt man die Funktion meromorph. Ist die Singularität weder hebbar noch ein Pol, so spricht man von einer wesentlichen Singularität. Nach dem Satz von Picard sind Funktionen mit einer wesentlichen Singularität dadurch charakterisiert, dass es höchstens einen Ausnahmewert a gibt, so dass
sie in jeder beliebig kleinen Umgebung der Singularität jeden beliebigen komplexen Zahlenwert mit höchstens der Ausnahme a annehmen.

Da man jede holomorphe Funktion in eine Potenzreihe entwickeln kann, kann man auch Funktionen mit hebbaren Singularitäten in Potenzreihen entwickeln. Meromorphe Funktionen können in eine Laurent-Reihe entwickelt werden, die nur endlich viele Glieder mit negativer Potenz haben, und die Laurent-Reihen von Funktionen mit wesentlicher Singularität haben eine nicht abbrechende Entwicklung der Potenzen mit negativen Exponenten. Der Koeffizient formula_25 von formula_26 der Laurent-Entwicklung heißt Residuum. Nach dem Residuensatz kann man nur mit Hilfe dieses Wertes Integrale über meromorphe Funktionen und über Funktionen mit wesentlichen Singularitäten bestimmen. Dieser Satz ist nicht nur in der Funktionentheorie von Bedeutung, denn man kann mit Hilfe dieser Aussage auch Integrale aus der reellen Analysis bestimmen, die wie das gaußsche Fehlerintegral keine geschlossene Darstellung der Stammfunktion besitzen.

Wichtige Ergebnisse sind außerdem noch der riemannsche Abbildungssatz und der Fundamentalsatz der Algebra. Letzterer besagt, dass sich ein Polynom im Bereich der komplexen Zahlen vollständig in Linearfaktoren zerlegen lässt. Für Polynome im Bereich der reellen Zahlen ist dies im Allgemeinen (mit reellen Linearfaktoren) nicht möglich.

Weitere wichtige Forschungsschwerpunkte sind die analytische Fortsetzbarkeit von holomorphen und meromorphen Funktionen auf die Grenzen ihres Definitionsbereiches und darüber hinaus.

Es gibt auch komplexwertige Funktionen mehrerer komplexer Variablen. Im Vergleich zur reellen Analysis gibt es in der komplexen Analysis fundamentale Unterschiede zwischen Funktionen einer und mehrerer Variablen. In der Theorie holomorpher Funktionen mehrerer Variablen gibt es kein Analogon zum cauchyschen Integralsatz. Auch der Identitätssatz gilt nur in einer abgeschwächten Form für holomorphe Funktionen mehrerer Veränderlicher. Die cauchysche Integralformel jedoch lässt sich ganz analog auf mehrere Variablen verallgemeinern. In dieser allgemeineren Form nennt man sie auch Bochner-Martinelli-Formel. Außerdem besitzen meromorphe Funktionen mehrerer Variablen keine isolierten Singularitäten, was aus dem sogenannten Kugelsatz von Hartogs folgt, und als Konsequenz auch keine isolierten Nullstellen. Auch der riemannsche Abbildungssatz – ein Höhepunkt der Funktionentheorie in einer Variablen – hat kein Äquivalent in höheren Dimensionen. Nicht einmal die beiden natürlichen Verallgemeinerungen der eindimensionalen Kreisscheibe, die Einheitskugel und der Polyzylinder, sind in mehreren Dimensionen biholomorph äquivalent. Ein großer Teil der Funktionentheorie mehrerer Variablen beschäftigt sich mit Fortsetzungsphänomenen (riemannsche Hebbarkeitssätze, Kugelsatz von Hartogs, Satz von Bochner über Röhrengebiete, Cartan-Thullen-Theorie). Die Funktionentheorie mehrerer komplexer Variablen wird zum Beispiel in der Quantenfeldtheorie benutzt.

Die komplexe Geometrie ist ein Teilgebiet der Differentialgeometrie, das auf Methoden der Funktionentheorie zurückgreift. In anderen Teilgebieten der Differentialgeometrie wie der Differentialtopologie oder der riemannschen Geometrie werden glatte Mannigfaltigkeiten mit Techniken aus der reellen Analysis untersucht. In der komplexen Geometrie dagegen werden Mannigfaltigkeiten mit komplexen Strukturen untersucht. Im Gegensatz zu den glatten Mannigfaltigkeiten ist es auf komplexen Mannigfaltigkeiten möglich, mit Hilfe des Dolbeault-Operators holomorphe Abbildungen zu definieren. Diese Mannigfaltigkeiten werden dann mit Methoden der Funktionentheorie und der algebraischen Geometrie untersucht. Im vorigen Abschnitt wurde erklärt, dass es große Unterschiede zwischen der Funktionentheorie einer Veränderlichen und der Funktionentheorie mehrerer Veränderlicher gibt. Diese Unterschiede spiegeln sich auch in der komplexen Geometrie wider. Die Theorie der riemannschen Flächen ist ein Teilgebiet der komplexen Geometrie und beschäftigt sich ausschließlich mit Flächen mit komplexer Struktur, also mit eindimensionalen komplexen Mannigfaltigkeiten. Diese Theorie ist reichhaltiger als die Theorie der n-dimensionalen komplexen Mannigfaltigkeiten.

Eine klassische Anwendung der Funktionentheorie liegt in der Zahlentheorie. Benutzt man dort funktionentheoretische Methoden, nennt man dieses Gebiet dann analytische Zahlentheorie. Ein wichtiges Ergebnis ist beispielsweise der Primzahlsatz.

Reelle Funktionen, die sich in eine Potenzreihe entwickeln lassen, sind
auch Realteile von holomorphen Funktionen. Damit lassen sich diese
Funktionen auf die komplexe Ebene erweitern. Durch diese Erweiterung kann
man oft Zusammenhänge und Eigenschaften von Funktionen finden, die
im Reellen verborgen bleiben, zum Beispiel die eulersche Identität.
Hierüber erschließen sich vielfältige Anwendungsbereiche in der Physik (beispielsweise in der Quantenmechanik die Darstellung von Wellenfunktionen, sowie in der Elektrotechnik zweidimensionale Strom-Spannungs-Diagramme).
Diese Identität ist auch die Basis für die komplexe Form der Fourier-Reihe und für die Fourier-Transformation. In vielen Fällen lassen sich diese mit Methoden der Funktionentheorie berechnen.

Für holomorphe Funktionen gilt, dass Real- und Imaginärteil harmonische Funktionen sind, also die Laplace-Gleichung erfüllen. Dies verknüpft die Funktionentheorie mit den partiellen Differentialgleichungen, beide Gebiete haben sich regelmäßig gegenseitig beeinflusst.

Das Wegintegral einer holomorphen Funktion ist vom Weg unabhängig. Dies war historisch das erste Beispiel einer Homotopieinvarianz. Aus diesem Aspekt der Funktionentheorie entstanden viele Ideen der algebraischen Topologie, beginnend mit Bernhard Riemann.

In der Theorie der komplexen Banachalgebren spielen funktionentheoretische Mittel eine wichtige Rolle, ein typisches Beispiel ist der Satz von Gelfand-Mazur. Der holomorphe Funktionalkalkül erlaubt die Anwendung holomorpher Funktionen auf Elemente einer Banachalgebra, auch ein holomorpher Funktionalkalkül mehrerer Veränderlicher ist möglich.







</doc>
<doc id="14570" url="https://de.wikipedia.org/wiki?curid=14570" title="Pendel">
Pendel

Ein Pendel, auch Schwerependel (früher auch Perpendikel, von lat. "pendere" „hängen“) ist ein Körper, der, an einer Achse oder einem Punkt außerhalb seines Massenmittelpunktes drehbar gelagert, um seine eigene Ruheposition schwingen kann. Seine einfachste Ausführung ist das Fadenpendel, das aus einem an einem Faden aufgehängten Gewicht besteht und baulich einem Schnurlot gleicht. In diesem Sinne keine Pendel sind das Federpendel und das Torsionspendel.

Alternativ auch als "Pendel" wird in der Technik – insbesondere im Automobilbau – die sich im Allgemeinen nicht infolge ihrer Schwere bewegende Schwinge bezeichnet (zum Beispiel Teile der Pendelachse).

Eine Eigenschaft des Schwerependels ist, dass seine Schwingungsdauer nur von der Länge, nicht aber von der Art, Gestalt oder Masse des Pendelkörpers abhängt; auch fast nicht von der Größe der maximalen Auslenkung, vorausgesetzt, diese bleibt auf wenige Grad beschränkt. Dies wurde erstmals von Galileo Galilei festgestellt und nach den vertiefenden Untersuchungen durch Christiaan Huygens zur Regulierung der ersten genauen Uhren verwendet. Ein Sekundenpendel hat, je nach geografischer Breite des Standorts, eine Länge zwischen 99,1 und 99,6 cm.

Das Pendel besteht meist aus einem Band oder einem Stab, der am freien Ende mit einer Masse beschwert ist. Bringt man ein solches Pendel aus seiner vertikalen Ruhelage, schwingt es unter dem Einfluss der Schwerkraft zurück und wird, solange keine Dämpfung erfolgt, symmetrisch zwischen den Scheitelpunkten als Umkehrpunkt der Bewegung um die tiefstmögliche Position des Massenmittelpunktes – die Ruheposition – weiterschwingen. Beim Schwingen wird die potentielle Energie der Masse in kinetische Energie und wieder zurückverwandelt. In der Ruheposition liegt die gesamte Energie der Schwingung als kinetische Energie vor, am Scheitelpunkt als potentielle Energie. Im zeitlichen Mittel ist die Energie gemäß dem Virialsatz zu gleichen Teilen in kinetische und potentielle Energie aufgeteilt.

Die Regelmäßigkeit der Schwingungsperiode eines Pendels wird bei mechanischen Pendeluhren genutzt. Ihre Pendel müssen, sollen sie genau gehen, möglichst kleine und konstante Amplituden zurücklegen.

Man unterscheidet mathematische Pendel von physikalischen Pendeln:
Das "ebene mathematische Pendel" und das sphärische Pendel sind idealisierende Modelle zur allgemeinen Beschreibung von Pendelschwingungen. Dabei wird angenommen, dass die gesamte Masse des Pendels in einem Punkt vereinigt vorliegt, der einen festen Abstand vom Aufhängepunkt hat. Ein solches Pendel wird näherungsweise durch ein Fadenpendel realisiert. Das "physikalische Pendel" unterscheidet sich vom mathematischen Pendel, indem bei ihm die Form und Größe des Pendelkörpers berücksichtigt wird, weshalb das Verhalten physikalischer Pendel eher dem von realen Pendeln entspricht. So ist beispielsweise die Periodendauer eines Stangenpendels, bei dem ein Pendelkörper an einer Stange mit endlicher Masse hängt, stets kürzer als die Periodendauer eines gleich langen mathematischen Pendels, bei dem die Masse der Aufhängung vernachlässigt werden kann. Für kleine Auslenkungen vereinfacht sich die Betrachtung der Bewegung des Pendels: Da hier die rückstellende Kraft näherungsweise proportional zur Auslenkung ist, handelt es sich um einen harmonischen Oszillator.

Mit dem "Foucaultschen Pendel" konnte die Erdrotation nachgewiesen werden: Die Corioliskraft wirkt von außen auf das Pendel, indem sie seine Schwingungsebene verändert und es von Schwingung zu Schwingung in einem wiederkehrenden Muster ablenkt.

Das Pendel ist eine mechanische Realisierung eines harmonischen Oszillators. Der harmonische Oszillator ist ein bedeutendes Modellsystem in der Physik, da es ein geschlossen lösbares System darstellt. Es ist dadurch charakterisiert, dass eine Kraft proportional zur Auslenkung entgegen der Auslenkungsrichtung wirkt. Mit der Auslenkung formula_1, der zweiten Ableitung nach der Zeit formula_2 und einer Proportionalitätskonstanten formula_3 gilt also:
Mit dieser Proportionalitätskonstanten besitzt der harmonische Oszillator einen Freiheitsgrad, der seine Kreisfrequenz formula_5 genannt wird. Die Lösung dieser Gleichung ist periodischer Natur, die abhängig von den physikalischen Anfangsbedingungen als Summe einer Sinus- und Kosinusfunktion geschrieben werden kann:
Die Bewegung, die ein harmonischer Oszillator beschreibt, heißt harmonische Schwingung. Nicht der strengen Definition des harmonischen Oszillators folgend, werden teilweise auch gedämpfte harmonische Osziallatoren als solche bezeichnet. Diese sind derart modelliert, dass die Amplitude, die maximale Auslenkung, der Schwingung mit der Zeit kleiner wird.

Das mathematische Pendel ist das einfachste Modell eines Pendels: Eine Massepunkt ist an einem masselosen, starren Faden aufgehängt und kann sich entsprechend nur in zwei Dimensionen auf einer Kreisbahn um die Aufhängung bewegen. Sein einziger Freiheitsgrad ist die Auslenkung um eine Gleichgewichtslage oder Ruheposition und die Gewichtskraft wirkt als rückstellende Kraft auf den Massepunkt. Bei hinreichend kleiner Auslenkung kann das mathematische Pendel als harmonischer Oszillator beschrieben werden. Die Kreisfrequenz hängt dabei nur von der Fadenlänge formula_7 und der Erdbeschleunigung formula_8 ab:
Die Verallgemeinerung des mathematischen Pendels in drei Dimensionen heißt sphärisches Pendel. Dessen gekoppeltes Gleichungssystem besitzt keine einfache Lösung mehr.

Das physikalische Pendel berücksichtigt im Gegensatz zum mathematischen Pendel die Ausdehnung des Pendelkörpers und die Masse des Fadens. Die Kreisfrequenz des physikalischen Pendels hängt entsprechend auch von seiner Masse formula_10 und seinem Trägheitsmoment formula_11 ab, während formula_7 als Abstand des Schwerpunkts von der Aufhängung präzisiert werden muss:

Bei zwei gekoppelten Pendeln üben zwei Pendel eine von beiden Auslenkungen abhängige Kraft aufeinander aus. Zum Beispiel verbindet man zwei gleiche Fadenpendel durch eine Feder miteinander, um im Demonstrationsexperiment die Eigenschwingungen und das Phänomen der Schwebung zu beobachten. Gebundene Atome (z. B. in einem Molekül oder in einem Festkörper) können oft näherungsweise durch ein Modell von vielen gekoppelten Pendeln beschrieben werden. Mehr als zwei gekoppelte Pendel können komplexe Schwingungsmuster zeigen, wenn die Grundschwingung von anders geformten Eigenschwingungen (oder Schwingungsmoden) mit höheren Eigenfrequenzen überlagert wird.

Beim Doppelpendel wird an der Masse eines Pendels ein zweites Pendel angebracht. Es dient unter anderem der Demonstration von chaotischen Prozessen, da die Bewegung chaotisch sein kann.

Federpendel sind keine Pendel im eigentlichen Sinne, denn sie verfügen im Unterschied zum Schwerkraftpendel über eigene Rückstellkräfte, die von der Schwerkraft unabhängig sind.

Es gibt u. a. folgende Varianten:










</doc>
<doc id="14572" url="https://de.wikipedia.org/wiki?curid=14572" title="Elektrisches Feld">
Elektrisches Feld

Das elektrische Feld ist ein physikalisches Feld, das durch die Coulombkraft auf elektrische Ladungen wirkt. Das elektrische Feld ist ein allgegenwärtiges Phänomen. Es erklärt beispielsweise die Übertragung elektrischer Energie und die Funktion elektronischer Schaltungen. Es bewirkt die Bindung von Elektronen an den Atomkern und beeinflusst so die Gestalt der Materie. Seine Kombination mit dem Magnetismus, das elektromagnetische Feld, erklärt die Ausbreitung von Licht- und Funkwellen.

Elektrische Felder werden hervorgerufen von elektrischen Ladungen und durch zeitliche Änderungen magnetischer Felder.

Mathematisch ist das elektrische Feld das Vektorfeld der elektrischen Feldstärke; es ordnet jedem Punkt im Raum einen Vektor für Richtung und Betrag der elektrischen Feldstärke zu. Die Eigenschaften des elektrischen Feldes werden zusammen mit denen des magnetischen Feldes in den Maxwell-Gleichungen beschrieben.

Eine anschauliche Vorstellung von elektrischen Feldern erhält man durch Feldlinienbilder. Diese bestehen aus orientierten (mit Pfeilen versehenen) "Feldlinien". Dabei gilt:
Die Richtung der Tangente in einem Punkt einer Feldlinie gibt die Richtung des Feldstärkevektors formula_1 an. Die Dichte (der Querabstand) der Feldlinien ist proportional dem Betrag der Feldstärke an dieser Stelle.

Besonders einfach zu ermitteln ist das elektrische Feld einer Punktladung. Gemäß dem coulombschen Gesetz ergibt sich für die Feldstärke in einem gegebenen Punkt:

Dabei steht "Q" für die felderzeugende Ladung im Ursprung des Koordinatensystems, formula_3 für den Ortsvektor des gegebenen Punktes, formula_4 für den zugehörigen Einheitsvektor, formula_5 für die elektrische Feldkonstante und formula_6 für die relative Permittivität.

Wird das elektrische Feld durch mehrere Punktladungen formula_7 an den Positionen formula_8 erzeugt, so erhält man den Feldstärkevektor des Gesamtfeldes an der Position formula_9 gemäß dem Superpositionsprinzip durch Addition der einzelnen Feldstärkevektoren:

Liegt eine kontinuierliche, durch die räumliche Ladungsdichte formula_11 gegebene Ladungsverteilung vor, so gilt entsprechend:

Das elektrische Feld einer Linienladung (eines unendlich langen, geladenen Drahtes) mit der linearen Ladungsdichte formula_13 ist gegeben durch

Dabei ist der Basisvektor formula_15 radial von der Linienladung zum Bezugspunkt gerichtet.

Eine Flächenladung (eine gleichmäßig geladene, unendlich ausgedehnte, dünne Platte) erzeugt auf beiden Seiten jeweils ein homogenes elektrisches Feld. Der Feldstärkevektor ist für einen beliebigen Punkt senkrecht zur Platte und bei positiver Ladung von der Platte weg gerichtet, bei negativer Ladung zur Platte hin. Setzt man die Flächenladungsdichte formula_16 voraus, so hat die elektrische Feldstärke den Betrag

Das elektrische Feld zwischen zwei großen planparallelen Kondensatorplatten, die Ladungen von gleichem Betrag, aber verschiedenem Vorzeichen enthalten, ist annähernd homogen (streng homogen, wenn die Platten unendlich groß sind). Für den Betrag der Feldstärke gilt:
Dabei ist formula_19 der Abstand zwischen den Platten, formula_20 die Fläche einer Kondensatorplatte, formula_21 die Spannung zwischen den beiden Platten und formula_22 der Betrag der Ladung auf einer Platte. Das Potential ändert sich linear von einer Platte zur anderen um den Betrag formula_21. Werden die Platten auseinander bewegt, so bleibt die Feldstärke konstant, die Spannung steigt. Die gegen die elektrostatische Anziehung geleistete Arbeit steckt in der Energie des Feldes. Außerhalb des Kondensators ist die Feldstärke (im Idealfall) gleich 0.

Die Ladungen auf den Kondensatorplatten verteilen sich dabei gleichmäßig auf den einander zugewandten Plattenflächen. Die absoluten Beträge der Flächenladungsdichte
und der elektrischen Flussdichte formula_25 stimmen überein. Allerdings ist formula_26 eine skalare Größe, formula_25 dagegen ein Vektor.

Ist der Kondensator nicht mit einer äußeren Ladungsquelle verbunden, ändert sich der Wert der Flächenladungsdichte formula_26 nicht, wenn ein Dielektrikum zwischen den Kondensatorplatten eingefügt oder weggenommen wird. Die elektrische Feldstärke formula_29 aber ändert sich beim Hinzufügen um den Faktor formula_30, beim Entfernen um formula_6.

Ein elektrischer Dipol, also eine Anordnung aus zwei Punktladungen formula_32 und formula_33 im Abstand formula_19, erzeugt ein rotationssymmetrisches Feld. Für die Feldstärkekomponenten parallel und senkrecht zur Dipolachse gilt in großem Abstand formula_35 in Richtung "ϑ":
Dabei zeigt "ϑ = 0" von der Mitte aus in Richtung der positiven Ladung.

Exakt gilt die Formel im Grenzübergang für verschwindendes formula_19 bei konstantem Betrag des Dipolmoments formula_38.

Bringt man einen Leiter langsam in ein zeitlich konstantes äußeres Feld, so bewirkt es im Leiter eine Ladungsverschiebung (Influenz). Das Innere bleibt dabei frei von Raumladungen, während sich an der Oberfläche eine Ladungsverteilung einstellt, die das Innere des Leiters in der Summe gerade feldfrei hält. Außen stehen die Feldlinien stets und überall senkrecht auf der Leiteroberfläche, sonst würde die Querkomponente eine weitere Ladungsverschiebung bewirken. An Spitzen entstehen hohe Feldstärken.

Das elektrische Feld lässt sich durch das Vektorfeld der elektrischen Feldstärke formula_1 beschreiben.

Die Energiedichte des elektrischen Feldes ergibt sich aus der elektrischen Feldstärke und der elektrischen Flussdichte zu

Der Zusammenhang zwischen der elektrischen Feldstärke und der elektrischen Flussdichte hängt vom Medium ab und ist aufgrund der elektrischen Polarisation im Allgemeinen nichtlinear. Die elektrische Polarisation in einem Material ist mit einer Ladungsverschiebung und daher mit einem Energietransport verbunden. Sie erfolgt daher nicht augenblicklich und ist dadurch auch frequenzabhängig. Für viele Medien kann man trotzdem näherungsweise einen linearen Zusammenhang in der Form
mit der elektrischen Feldkonstanten formula_5 und der Permittivitätszahl formula_6 annehmen.

Im Vakuum mit formula_54 ist der Zusammenhang zwischen beiden Feldern streng linear, und es gilt: formula_55.

Das elektrische Feld in allgemeiner Form ist sowohl orts- als auch zeitabhängig, formula_56. Es ist über die maxwellschen Gleichungen und die spezielle Relativitätstheorie eng mit dem magnetischen Feld verknüpft. In der speziellen Relativitätstheorie werden seine Vektorkomponenten daher untrennbar mit denen des magnetischen Feldes zu einem Tensor zusammengefasst. Je nachdem, in welchem Bezugssystem man sich als Beobachter befindet, d. h. in welcher relativen Bewegung zu eventuell vorhandenen Raumladungen, wird so über die Lorentz-Transformation das elektrische Feld in ein magnetisches Feld transformiert und umgekehrt.

In der Elektrostatik werden ausschließlich ruhende Ladungen betrachtet. Ohne Ströme existiert kein Magnetfeld. Das elektrostatische Feld ist deshalb nicht nur "stationär", also zeitlich unveränderlich, sondern auch rotationsfrei (wirbelfrei). Ein solches Feld kann durch ein Potential beschrieben werden.

In der Elektrodynamik muss man dagegen auch elektrische Felder berücksichtigen, die durch zeitlich veränderliche Magnetfelder hervorgerufen werden (elektromagnetische Induktion). Besonders wichtig sind die elektromagnetischen Wellen wie Licht, die aus miteinander verketteten elektrischen und magnetischen Feldern bestehen. Aufgrund der engen Beziehung zwischen elektrischem und magnetischem Feld fasst man beide in der Elektrodynamik zum elektromagnetischen Feld zusammen.

Bis zum Nachweis elektromagnetischer Wellen durch Heinrich Hertz bestand die Frage, ob die zwischen elektrischen Ladungen wirkenden Kräfte unmittelbar im Sinne einer Fernwirkung oder unter Vermittlung durch den Raum (Nahwirkung) zustandekommen.



Solange nur langsame Veränderungen der elektrischen und magnetischen Größen betrachtet werden, ist es nicht entscheidend, ob man mit den physikalischen Erscheinungen die eine oder die andere Vorstellung verknüpft. Berücksichtigt man jedoch, dass sich mit elektromagnetischen Wellen Impuls und Energie im Raum ausbreiten können, so lässt sich die Vorstellung einer Fernwirkung nur schwer mit den Beobachtungen in Übereinstimmung bringen.

Zusammenfassend geht man aus heutiger Sicht davon aus, dass die Wechselwirkung zwischen den Ladungen erst vom elektrischen Feld "vermittelt" wird. Da die Kraft vom elektrischen Feld an der betreffenden Stelle abhängt, aber nicht direkt vom elektrischen Feld an anderen Punkten, handelt es sich um eine Nahwirkung. Ändert sich die Position einer der Ladungen, so breitet sich die Änderung des Feldes mit Lichtgeschwindigkeit im Raum aus. Eine relativistische Betrachtung des elektrischen Feldes führt zum elektromagnetischen Feld. Dieses kann Impuls und Energie aufnehmen und transportieren und ist daher als ebenso real anzusehen wie ein Teilchen.

Im Rahmen der Quantenmechanik werden gewöhnlich die Felder weiterhin als klassisch angesehen, auch wenn die Zustände der wechselwirkenden Teilchen quantisiert sind. Quantenfeldtheorien kombinieren Prinzipien klassischer Feldtheorien (zum Beispiel der Elektrodynamik) und beschreiben Teilchen und Felder einheitlich. Es werden nicht nur Observable (also beobachtbare Größen) wie Energie oder Impuls quantisiert, sondern auch die wechselwirkenden (Teilchen-)Felder selbst; die Felder werden also ähnlich wie Observablen behandelt. Die Quantisierung der Felder bezeichnet man auch als Zweite Quantisierung.




</doc>

<doc id="12015" url="https://de.wikipedia.org/wiki?curid=12015" title="Wein">
Wein

Wein (über ahd. "wîn" aus ) ist ein alkoholisches Getränk aus dem vergorenen Saft der Beeren der Edlen Weinrebe. Wein ist ein Genussmittel. Durch spezifische önologische Ausbaumethoden kommt es bei der Lagerung zu zahlreichen biochemischen Reifeprozessen, die sehr vielfältig sein können und auch dazu führen, dass manche Weine jahrzehntelang reifen und haltbar sind.

Die häufigsten Weine sind Rot- und Weißweine sowie Roséweine. Schaumwein (Sekt, Cava, Champagner etc.) entsteht aus Wein während einer zweiten Gärung. Gering schäumende Weine werden als Perlweine bezeichnet (Prosecco frizzante, Secco etc.). Dabei wird in der Regel dem Wein die Kohlensäure technisch zugesetzt.

Die für die Weinherstellung benötigten Beerenfrüchte wachsen in traubenartigen, länglichen Rispen an der Weinrebe ("Vitis vinifera"). Sie stammen überwiegend von ihrer Unterart ab, der europäischen Edlen Weinrebe "Vitis vinifera" subsp. "vinifera". Da diese zu den nicht reblausresistenten Rebenarten gehört, wird sie zum Schutz vor der Reblaus auf teilresistente Unterlagen (Wurzeln) der wilden Rebarten "Vitis riparia", "Vitis rupestris", "Vitis berlandieri" bzw. deren interspezifischen Kreuzungen (Hybridreben) gepfropft.

Fachbegriffe zum Thema Wein werden im Artikel Weinsprache erläutert.


„Wein“ ist ein klassisches Wanderwort, das im ganzen mediterranen Raum verbreitet war. Das arabische "wayn", das lateinische "vinum", das griechische [oínos] bzw. * [woínos] – von myk. "wo-no" – sind miteinander verwandt, ohne dass man folgern könnte, aus welcher Sprache es ursprünglich stammt. Vermutlich kann eine ähnliche Verbindung zum georgischen Wort "ღვინო" [ghwino] gezogen werden. Zu beachten ist, dass nur das georgische Wort "Ghwino" eine andere Bedeutung hat und „siedende“ bedeutet.

Das hochdeutsche Wort Wein, das althochdeutsche "wîn" oder "winam", der französische Begriff "vin" und der englische Begriff "wine" sind alle dem lateinischen Wort "vinum" entlehnt. Auch das walisische Wort "gwin" sowie das irische "fíon" sind gleichen Ursprungs. Erklärt wird dies durch die Tatsache, dass sowohl Germanen als auch Kelten erstmals über die Römer in größerem Umfang mit Wein in Berührung kamen und somit den lateinischen Begriff übernahmen.

Über spätere Handelsbeziehungen gelangte der Begriff des Weins von den Germanen bis zu den Slawen (siehe das russische Wort vinó) und bis zu den Balten, wobei beispielsweise in Litauen der Begriff "vynas" und in Lettland das Wort "vins" bekannt ist.

Im Altertum erfuhr der Weinanbau eine erhebliche Beachtung und Ausbreitung. Weinbau wurde schon seit dem 6. Jahrtausend v. Chr. in Vorderasien betrieben. Georgien sowie das heutige Armenien gelten jedoch als die Ursprungsländer des Weines.

Der Wein spielte seit dem Altertum als landwirtschaftliches Erzeugnis eine bedeutende Rolle, sowohl in der Wirtschaft als auch in der Medizin sowie im sozialen und rituellen Leben. Insbesondere aber war und ist er ein Symbol zahlreicher Mythologien und Religionen.

Wein war und ist ein wesentlicher Bestandteil ritueller Praktiken in verschiedenen Kulturen. Die im Weingenuss gesuchte Ekstase wurde als etwas betrachtet, das Nähe zu einer Gottheit schaffen kann.

In der antiken Mythologie waren es Osiris (Ägypten), Dionysos (Griechenland), Bacchus (römische Mythologie) oder Gilgamesch (Babylonien), die den Wein und den Weingenuss repräsentierten.

In der griechischen Antike war der Wein ein Gegenstand religiöser Verehrung und Sinnbild der Kultur. Er stand im Mittelpunkt der Kulte und Mysterien des griechischen Gottes Dionysos. Die Bedeutung des Weines im antiken Kulturraum spiegelt sich auch in den Festen, die zu seinen Ehren abgehalten wurden: Im Dezember feierte man die Lenäen, das Fest der Weinpresse. Dabei wurde Dionysos der neue Wein geopfert. Im Februar folgten die Anthesterien, wo der Wein der letzten Ernte gekostet wurde. Wein war zudem wichtiger Teil des griechischen und römischen Libationsopfers. Dabei wurde Wein direkt auf die darzubringenden Opfer, auf die Erde oder ins Feuer verspritzt. Die Römer verehrten Bacchus als Gott des Weines. Die Herstellung des Weines war von religiösen Normen bestimmt: Priester setzen die Tage des Erntebeginns fest. Selbst das Stutzen der Rebstöcke war eine religiöse Pflicht. Der Wein war auch ein wichtiger Bestandteil religiöser Feste im Alten Rom, so zum Beispiel beim Frauenfest der Bona Dea, Göttin der weiblichen Fruchtbarkeit.

Eine messianische Bedeutung kommt dem Wein in der jüdischen und christlichen Religion zu.

Die Bibel – wo Noach als der erste Winzer gilt – macht vom Wein reichen symbolischen Gebrauch. Im Buch der Psalmen dient der Wein der Lebensfreude, bei Salomo ist er Arznei für Leidende, aber auch mit Vorsicht zu genießendes Rauschmittel. Das Volk Israel wird mit einem Weinberg verglichen; Jesus beschreibt die Verbindung zu seinen Nachfolgern wie die zwischen Rebstock und Reben. Das Wirken des Heiligen Geistes wird mit gärendem neuen Wein verglichen. Wein kann verführen und auch – als Taumelbecher – den göttlichen Zorn ausdrücken.

Der Wein steht für das Fest. Er lässt den Menschen die Herrlichkeit der Schöpfung spüren.

Im Christentum bildet der Wein im Sakrament der Eucharistie das Element für das "Blut Christi". Neben dem Gebrauch von Messwein wurde der Wein in der katholischen Kirche im Mittelalter auch als "geweihter Wein" vielfältig als Sakramentale verwendet.

In der jüdischen Religion gehört koscherer Wein zu den Ritualen des Kiddusch am Schabbat, des Pessach und der Hochzeit.

In der europäischen Kunst- und Kulturgeschichte stellt der Wein einen zentralen Motiv- und Themenkomplex mit verschiedenen Bedeutungsebenen dar. So verbindet die europäische Kultur der festlichen Tafel den Wein als Teil eines gesellschaftlichen repräsentativen Rituals mit dem festlichen Ereignis.

Seit 1981 besteht in Vicenza, Italien, die Internationale Bibliothek La Vigna für Weinkultur.

Der Wein wurde zu allen Zeiten, von der Antike bis zur Gegenwart, in einem eigenen Literaturgenre, den Trinkliedern, besungen. Dem griechischen Mythos nach spendete der Gott Dionysos den Menschen den Wein. Er brachte einen Schlauch Wein in das Haus des Pflanzenzüchters Ikarios mit, den er in den Rebbau einweihte. Die Anakreontik ist eine literarische Strömung, in der der Wein, Dionysos und das Feiern in lyrischer Form verehrt werden. Wein taucht außerdem in zahlreichen Erzählungen auf, wie zum Beispiel im griechischen Heldenepos der Odyssee. Odysseus gerät auf seiner hindernisreichen Heimfahrt von Troja in die Höhle des einäugigen Zyklopen Polyphem. Die Lage scheint aussichtslos, doch Odysseus bietet dem Riesen Wein an, den er bei den Kikonen erbeutet hatte. Polyphem versinkt im Weinrausch, wird geblendet und Odysseus und seine Gefährten können sich retten.

Im alten Testament finden sich zahlreiche Belege für Rebbau und Weinkonsum. Gott selbst stiftete den Menschen nach der Sintflut den Weinstock und Noach betätigte sich als Winzer.

Selbst in den sorgsam bereinigten Märchen der Gebrüder Grimm bringt beispielsweise Rotkäppchen seiner Großmutter „Kuchen und Wein“, wenn auch nicht als Arznei, so zumindest als Stärkung. In der Neuzeit preisen Dichter wie Friedrich Hölderlin in seiner Elegie „Brot und Wein“ den Wein als Gabe der Himmlischen.

Wein soll laut einiger Studien wie andere alkoholische Getränke mit geringem Alkoholgehalt, in geringer Menge genossen, das Herz-Kreislauf-System positiv beeinflussen; dies ist jedoch umstritten. Die dem Wein zugesprochenen positiven Wirkungen treffen einigen Studien zufolge auch auf Traubensaft zu.

Wein zählt zu den ältesten Kulturgütern der Menschheit. Sowohl die Kunst der Weinbereitung als auch die Kultur des Weingenusses ist über Jahrtausende hinweg bis heute immer fortentwickelt worden. Die Weinkultur wird sowohl auf öffentlichen Festveranstaltungen als auch in privaten Weinproben gepflegt und ist auch der Zweck von Zusammenschlüssen unter Weingenießern. In Deutschland wird die kreative Beschäftigung von Künstlern mit dem Kulturgut Wein in der Vergabe des Deutschen Weinkulturpreises gewürdigt.

Weinfeste besitzen oft Volksfestcharakter. Sie werden in allen Weinbaugegenden Europas (und zunehmend auch außerhalb derselben) gefeiert und dauern oft mehrere Tage. Nicht selten sind sie aus lokalen oder regionalen Festen, zum Beispiel Kirchweihfesten, entstanden. Hauptsächlich werden sie im Spätsommer oder Herbst gefeiert. In der Schweiz haben sie oft überregionale Bedeutung.

Das größte Weinfest der Welt ist mit über 600.000 Besuchern der Dürkheimer Wurstmarkt in Bad Dürkheim.

In vielen Ländern haben sich Weinliebhaber und -kenner zu Vereinen zusammengeschlossen, um den Weingenuss gemeinsam zu kultivieren. Im deutschen Sprachgebiet nennen sich diese Klubs meist Weinbruderschaften oder Weinkonvente. Sie blicken teilweise auf eine jahrhundertelange Tradition zurück.

Die Ursprünge liegen in heidnischen Fruchtbarkeitsriten, die später von den christlichen Bruderschaften assimiliert und verändert wurden. Meist war dies auch mit der Verehrung von Schutzpatronen verbunden. Waren früher ausschließlich Männer zugelassen, stehen die Zusammenschlüsse heute zunehmend auch Frauen offen. Während sich in früherer Zeit die Mitglieder bei einem Glas Wein der Geselligkeit in der Tradition der griechischen Symposien hingaben, organisieren sie heute auch öffentliche, kulturelle und fachliche Weinveranstaltungen. Weinbruderschaften pflegen und bewahren heute sowohl die Kultur und Geschichte des Weins als auch das Wissen um den Wein. Dazu gehört oftmals auch das Erinnern an historische Kleinodien der Weinbewertung wie dem Tastevin.

Struktur und Textur des Bodens bestimmen maßgeblich den Stil des Weins. Kalkhaltige Böden ergeben Weine mit Finesse und einem guten Alterungspotenzial. Lehmhaltige Böden stehen für wuchtige Weine und sandige sowie kieshaltige Böden begünstigen eine frühere Reife der Beeren. Ausschlaggebend ist dabei die Mächtigkeit der jeweiligen Bodenschicht und ein ausgeglichener Feuchtigkeitshaushalt – in niederschlagsarmen Gegenden entscheidend ist die Fähigkeit zum Speichern vorhandener Feuchtigkeit, bei hohen Niederschlagsmengen kommt einer guten Drainagefähigkeit große Bedeutung zu.

Im Laufe der Weinbauvergangenheit haben sich innerhalb der einzelnen Weinbaugegenden ideale Paarungen zwischen Bodentyp und Rebsorte herauskristallisiert. Der Riesling gedeiht vorzüglich auf den Schieferböden der Mosel, der rote Merlot zeigt seine Größe auf den lehmigen und kalkreichen Böden von Saint-Émilion und der Cabernet Sauvignon benötigt zur vollen Reife die kieshaltigen Böden des Médoc.

Die Rebe erbringt nur dann gute Qualität, wenn die Böden karg beziehungsweise nicht zu fruchtbar sind. Es ist die Aufgabe des Winzers, dem Boden nur so viel Dünger zuzuführen, wie von der Pflanze entnommen wird. Andernfalls steigen die Erträge auf Kosten der Qualität an.

Zu den Standortfaktoren zählt neben dem Bodentyp auch die vorhandene Mikroflora, die durch Temperatur, Luftfeuchtigkeit und Licht (→ Mikroklima) beeinflusst wird.

In Weinbaugegenden mit kühlem Weinbauklima (Weinbauzone A und B) kommt der Ausrichtung der Weinlage zur Sonne sowie die Nähe zu wärmespeicherndem Wasser (Flussläufe oder Seen) eine überragende Rolle zu. Dies kann insbesondere in den deutschen Weinbaugebieten von Ahr, Mosel, Nahe und Rheingau beobachtet werden und erklärt die große Rolle der Einzellage im deutschen Weingesetz.

Über die Jahrtausende haben sich tausende Rebsorten durch natürliche Kreuzung und folgender Auslese, oder aber durch gezielte Kreuzung einzelner oder mehrerer Sorten durch den Menschen, entwickelt. Die verschiedenen Sorten ermöglichen, besonders wenn sie für einen Standort gut geeignet sind, die Erzeugung differenzierter Weinqualitäten.

Von insgesamt weltweit über 20.000 bekannten Rebsorten sind nur etwa 1000 Sorten im Rahmen der offiziellen Listen für den Weinbau zugelassen.

In jedem Weinbaugebiet der EU gibt eine Liste die für den Weinbau gesetzlich zugelassenen Rebsorten vor. Die Liste autorisierter Sorten zur Erzeugung von Landweinen oder Tafelweinen (in der EU gesetzlich nurmehr als „Wein“ bezeichnet) ist umfangreich und enthält auch Massenträger. Die Liste der Sorten für die Erzeugung von Qualitätswein ist kleiner. Bei der Definition geschützter Herkunftsbezeichnungen wurde die Auswahl der Rebsorten innerhalb der EU stark eingeschränkt.

In zahlreichen Weinbaugebieten werden Weine sortenrein ausgebaut. In einzelnen Weingebieten wie Bordeaux, Châteauneuf-du-Pape oder Chianti hat sich hingegen eine Tradition des Verschnitts verschiedener Sorten historisch entwickelt.

Ein Weingarten bedarf einer Reihe von Pflegemaßnahmen wie Rebschnitt, Erziehung, Laubarbeiten, Traubenausdünnung, Bodenpflege und einer der Rebe (und Boden) angepassten organischen Humusdüngung und mineralischen Düngung (mineralische Handelsdünger). Zum Schutz gegen Krankheits- und Schädlingsbefall sind gezielte Pflanzenschutzmaßnahmen notwendig.

Bei der Traubenlese kann sich der Winzer meist zwischen der manuellen und der maschinellen Durchführung entscheiden. Keine Wahl haben Winzer mit sehr kleinen Parzellen oder Steillagen. Hier bleibt dem Winzer nur die manuelle Lese.

Die manuelle Lese ist erste Wahl, wenn man die Trauben möglichst unbeschädigt einbringen will: Bei gesundem, unbeschädigtem Lesegut kann die erforderliche Schwefelung des Weins stark reduziert werden. Die manuelle Lese ist im Weiteren erforderlich, wenn während des Lesevorgangs bereits eine Auslese stattfinden soll. Bei edelfaulen Beeren können bereits befallene Beeren einzeln und in mehreren Durchgängen in ausreichender Menge und in bester Güte eingeholt werden. Ein anderer Beweggrund für die Handarbeit ist, Beeren mit Stielen und Stielgerüst zu ernten. Bei einem gewissen Anteil von Stielen wird schonender gepresst, da die Stiele den entstehenden Beerenbrei auflockern, zudem kann der in den Stielen vorhandene Gerbstoff dem Wein förderlich sein. Der Vorteil der schonenden Handlese wird indes aufgehoben, wenn die gelesenen Trauben in den Erntebehältern mechanischem Druck ausgesetzt sind. In diesem Fall werden Beeren zerdrückt, und der austretende Saft kann gären.

Der Einsatz des Obstvollernters ist meist eine wirtschaftliche Entscheidung. In den europäischen Hochlohnländern kann der Kostenanteil der Lese halbiert bis gedrittelt werden oder sich aufdrängen, wenn nicht genügend Erntehelfer zur Verfügung stehen. Ein qualitativer Vorteil der mechanischen Ernte ist, dass das Lesegut innerhalb kürzester Zeit und zeitnah zum optimalen Reifezeitpunkt eingebracht werden kann. Nicht zu unterschätzen ist der Vorteil des Vollernters, die Trauben nachts oder in den frühen Morgenstunden bei sehr kühlen Temperaturen zu ernten: Dadurch wird dem Verlust von Aromastoffen vorgebeugt und es erfolgt ein langsamerer Start der Gärung durch eine kühlere Mosttemperatur.

Nachteilig ist, dass nicht jede Rebsorte gleich gut zur Lese mit der Maschine geeignet ist. Während Sorten wie Chardonnay und Cabernet Sauvignon sehr gut geeignet sind, kann der Spätburgunder nur unter Qualitätseinbußen mit dem Vollernter eingebracht werden. Eine maschinelle Lese erfordert auch besondere Vorkehrungen im Weinkeller. Durch die hohe Ernteleistung der Maschinen werden in sehr kurzen Zeitabschnitten große Mengen an Lesegut angeliefert. Zur Erzeugung von Qualitätswein ist es jedoch wichtig, dass zwischen Lese und Kelterung (im Fall von Weißwein) oder Maischung (im Fall von Rotwein) nur wenig Zeit vergehen sollte. Die Infrastruktur im Keller muss demnach die hohe Ernteleistung abbilden.

In einigen Weinbaugebieten wie dem Beaujolais, der Champagne und bei den Mitgliedern von Vinea Wachau Nobilis Districtus in der Wachau ist die maschinelle Ernte verboten. In Deutschland ist für die Mitgliedsbetriebe des Verbands Deutscher Prädikats- und Qualitätsweingüter ab dem Prädikat Auslese die Handlese obligatorisch.

Nach der Ernte werden die Trauben auf Sortiertischen gesichtet. Dabei können Blätter sowie unreife oder faule Beeren entfernt werden. Insbesondere bei Beeren zur Erzeugung von Rotweinen müssen faule Beeren rigoros ausgemustert werden, da der Schimmelpilz Botrytis cinerea negative Auswirkungen auf Geschmack und Farbe hat. Bei einer maschinellen Lese ist der Aufwand am Sortiertisch geringer, da beim Vollernter ein Großteil der Blätter durch Ventilatoren entfernt wird. Faule Beeren fallen durch das Rütteln der Rebstöcke meist schon zu früh ab; noch unreife beziehungsweise getrocknete Beeren fallen bei einer gut eingestellten Maschine nicht vom Stock.
Jeder Wein verfügt über folgende Grundbestandteile:



Der größte Anteil von Weiß- und Rotweintrauben wird zu Wein (und weiteren Produkten daraus) verarbeitet. Bei der Weinherstellung werden physiologisch reife Trauben verwendet.

Allgemeiner Überblick zur Weinherstellung:
Bei der Weißweinherstellung werden die Trauben gepresst, anschließend wird der Saft (Traubenmost) zu Weißwein vergoren:
Bei der Rotweinherstellung wird die Maische zu Rotwein vergoren:
Die Netto-Reaktionsgleichung lautet wie folgt:

formula_1

In Worten: Glucose + 2 Adenosindiphosphat + 2 Phosphat ergibt 2 Ethanol + 2 Kohlenstoffdioxid + 2 Adenosintriphosphat
Auf der Schale der reifen Beeren befindet sich eine Vielzahl von Hefen (sogenannte „Wilde Hefen“): welche Hefe sich im Wein-Ansatz durchsetzen würde, wäre ungewiss. Die Qualität und das Endergebnis wären somit ebenfalls zu einem gewissen Grad ungewiss. Beeinflusst wird der Anteil erwünschter Hefen durch die vorhandenen Kulturen in einem gut unterhaltenen Weinkeller des Winzers. Über die Weinpresse (im Fall von Weißwein) oder über das Umpumpen des noch gärenden Weins über den Tresterhut (im Fall des Rotweins) kann die Kellerflora in den Verlauf der Gärung eingreifen.

Um dem Zufall keine Chance zu geben, entwickelte man die Reinzuchthefe. Schon in alten Weinfachbüchern ist von Hefeansätzen der Rassen „Zeltingen“, „Scharlachberg“, „Geisenheim“ oder „Burgund“ zu lesen.

Die verschiedenen heute erhältlichen Hefestämme wurden zum einen auf ein Einsatzgebiet hin gezüchtet und sind zum anderen frei von Verschmutzungen wie Bakterien oder Schimmelpilzen. Seit der Mitte der 1980er-Jahre gibt es Reinzuchthefen in Granulatform, die wirbelschichtgetrocknet sind. Die Qualitätswende im deutschen und internationalen Weinbau ist zum Teil auch der Verwendung dieser Hefen geschuldet. Durch die Bildung sekundärer Nebenprodukte wie Ester beeinflusst die Hefe die Aromatik des Weins in seinem ganz jungen Stadium. Diese Aromen sind jedoch nur bedingt lagerfähig und zerfallen schnell. Die eigentlichen Bukett- und Aromastoffe des Weins bilden sich später. Dabei wird das Gärbukett zum Jungweinbukett, das schon weitgehend dem jeweiligen Sortenbukett entspricht.

In Weingütern, die sich auf komplex strukturierte Weine spezialisiert haben, kann eine sogenannte Spontangärung durch wilde Hefestämme erwünscht sein. Spontangärung bedeutet in der Praxis, dass die Gärung ohne Zugabe von Reinzuchthefen verläuft. Ziel ist, die Vielfalt eines nicht standardisierten Weingeschmacks zu erreichen, die durch Reinzuchthefen eingeschränkt werden kann. Mit nicht beziehungsweise weniger gerichteten Hefen kann eine größere geschmackliche Bandbreite und Eigenart erreicht werden, da mehr Hefestämme an der Gärung beteiligt sind. Allerdings ist hier das Risiko oft für den Hersteller höher, da die Gärung in eine vom Menschen unerwünschte Richtung verlaufen kann (zum Beispiel hoher Gehalt an Restzucker oder flüchtiger Säure).

Bei der alkoholischen Gärung entsteht Wärme. Hefen arbeiten in einem schmalen Temperaturband zwischen 12 und 37 °C. Früher verließ man sich darauf, dass die Klimabedingungen des Herbstes noch warm genug waren, um den Gärprozess in Gang zu bringen, gleichzeitig aber so kühl, dass die Temperatur im Gärbottich nicht über die Werte des Temperaturbandes stieg. Durch starke Belüftung des Kellers oder durch Bespritzen der Außenwand des Bottichs mit Wasser versuchte man eine Temperaturregulierung.

Die Temperaturkontrolle wurde erst durch den Einsatz von Edelstahlbehältern oder durch den Einsatz von Wärmetauschern möglich. Das Pumpen durch einen Wärmetauscher ermöglicht den Einsatz von Gärbehältern aus Holz oder Beton. Der Edelstahlbehälter kann durch eine Kühlschlange innerhalb der Außenwandung temperaturgeregelt werden, so dass der vergärende Most durch geringere Pumpaktivität schonender behandelt wird.

Die Menge des erzeugten Alkohols im Wein hängt vom Zuckergehalt des Mosts (siehe Mostgewicht) und somit vom Reifezustand der Beeren ab. In Gegenden mit kühlem Weinbauklima kann es in schlechten Jahrgängen zur Lese von nicht ausgereiften Trauben kommen. Um dennoch einen Wein mit ausreichend hohem Alkoholgehalt zu erzeugen, kann dem Most Zucker zugefügt werden. Diese weitverbreitete Praxis wird nach einem ihrer wichtigsten Fürsprecher Chaptalisation genannt. Man gibt dem Most entweder Trockenzucker, unvergorenen Traubensaft mit hohem Zuckeranteil, die sogenannte Süßreserve, oder neuerdings rektifiziertes Traubenmost-Konzentrat bei. Der Einsatz dieser Praxis sowie die maximal zulässige Alkoholerhöhung wird durch die jeweilige nationale Weingesetzgebung geregelt.

Die Anreicherung mit Zucker war ursprünglich eine Methode zur Rettung schwacher Jahrgänge, die sich jedoch zu einer gängigen Methode entwickelte. Angereicherte Weine schmecken angenehmer und reichhaltiger, da Alkohol ein ausgezeichneter Aromaträger ist. Der zu beobachtende Trend hin zu alkoholreichen Weinen lässt sich nicht nur mit der globalen Erderwärmung erklären. Eindrucksvoll lässt sich dieser Trend am Beispiel der großen Weine von Bordeaux belegen. Die Weine, die in der Klassifikation des Jahres 1855 in die Riege der führenden Güter eingestuft wurden, wären aus heutiger Sicht leichte Weine mit einem Alkoholgehalt von 11 bis 11,5 Prozent. Heute liegen die Werte um mindestens zwei Prozentpunkte höher. Die frühere Faustregel, dass ein Wein mit 12 Prozent Alkohol schon zu den mittelschweren Weinen gehört, gilt in dieser Form nicht mehr.

Beeren zur Erzeugung von Weißwein sollen von der Lese bis zum Entrappen möglichst unbeschädigt bleiben. Bei einer Beschädigung der Beerenhaut beginnt praktisch in kleinem Umfang eine ungewollte Maischegärung. Der Most nimmt Farbe und Aroma der Beerenschale an und auch der Wein neigt zur Oxidation, weshalb Weißweine in der Regel möglichst in reduktivem Zustand verarbeitet, vergoren und ausgebaut werden.

Die Beeren sollen nach der Lese möglichst schnell verarbeitet werden. Zur Vermeidung von Beschädigungen werden die Trauben in möglichst kleinen Kisten transportiert. Durch zu große Mengen an Trauben würden ansonsten die unten liegenden Beeren frühzeitig zerquetscht. In warmen Gebieten ist auch eine Lese während der Nacht oder in den frühen Morgenstunden qualitätsfördernd. Bei Rebsorten, die schnell zur Oxidation neigen, kann der Transport der Kisten vom Weinberg zum Weinkeller sowie das Pressen in einer inerten Gasatmosphäre bewerkstelligt werden.

Zuweilen werden die Trauben komplett mit den Stielen gepresst, meist werden die Trauben jedoch von den Stielen befreit (Abbeeren), da ein Großteil der im Weißwein wenig erwünschten Gerbstoffe dort enthalten ist. Bei der Pressung sollte das Fruchtfleisch kühl bleiben, damit die Gärung nicht zu früh einsetzt. Moderne Kellereien verfügen daher im Kelterbereich über Kühlkammern. Die Pressen sollen beim Weißwein einerseits eine möglichst hohe Ausbeute erlauben, aber die bitteren Kerne nicht zerquetschen.

In manchen Jahren kann ein kurzer Schalenkontakt bei der Gärung hilfreich sein, um dem Wein etwas mehr Extrakt zu geben. In diesem Fall limitiert sich der Kontakt jedoch auf einige wenige Stunden.

Da sich der rote Farbstoff nur in der Beerenhaut befindet, werden für den Rotwein die Trauben nicht gepresst, sondern lediglich ganz oder nur partiell entrappt und zerdrückt. Während der Gärung verbleiben die Schalen, Kerne sowie die beibehaltenen Stiele im Most. Dabei lösen sich die Phenole und Tannine aus den Schalen und färben den Most zunehmend ein. Damit die Farb- und Tanninausbeute ausreichend hoch ist, muss der Tresterhut regelmäßig mit dem Most vermengt werden. Der Tresterhut entsteht dadurch, dass die festen Bestandteile der Maische durch die während der Gärung entstehende Kohlensäure an die Oberfläche gedrückt werden. Das Vermengen kann durch Umpumpen von am Boden des Gärbehälters befindlichem Most über den Hut geschehen. Wahlweise kann der Tresterhut auch über Stangen oder lange Löffel untergetaucht werden. Das manuelle Untertauchen wird auf Französisch "pigeage" genannt.

Einfache Rotweine entstehen durch eine kurze Maischestandzeit von 2 bis 3 Tagen. Diese Zeit kann bei erstklassigen Weinen bis zu 4 Wochen betragen. Begrenzt wird die Standzeit durch die Dauer der Gärung. Eine Maischestandzeit über die Dauer der Gärung hinaus wirkt sich meist negativ aus.

Über eine Temperaturregelung kann die Gärdauer und damit die Maischedauer beeinflusst werden. Bei einer kühlen Gärung können das Fruchtaroma und die Feinheit eines Weines besser herausgearbeitet werden. Eine Gärung bei höherer Temperatur begünstigt die Tiefe der Farbe und die Geschmacksintensität.

Zur Konzentration von Geschmack und Aroma wurden diverse Methoden entwickelt. Bei der Saignée–Methode wird nach einigen Stunden oder wenigen Tagen ein geringer Anteil von 10 bis 20 Prozent des Mosts abgezogen und weiter zu Roséwein verarbeitet. Der verbleibende Mostanteil profitiert von einem vergleichsweise hohen Anteil von Schalen. Mit einer anderen Methode, der Umkehrosmose kann dem Most Wasser entzogen werden.

Nach erfolgter Gärung wird der Most abgelassen, geschwefelt und zum weiteren Ausbau in diversen Behältern, Holzfässern oder auch Barriques ausgebaut. Der Trester wird durch Pressen entsaftet. In gewissen Weinbaugebieten ist die durch Pressung erzielte Menge, die sogenannte Schüttung, limitiert.

Grundlage für die Herstellung von Schaumwein ist ein Grundwein mit einem gewissen Restzuckergehalt, der einer zweiten alkoholischen Gärung unterzogen wird. Dafür werden dem Grundwein höhervergärende Hefestämme ("Saccharomyces bayanus") zugesetzt, die auch als Nachgär-, Sekt- oder Champagnerhefe bezeichnet werden. Während der zweiten Gärung wird der Alkoholgehalt des Grundweins erhöht. Zudem bildet sich Kohlenstoffdioxid, das in der Flüssigkeit bleibt.

Schaumwein kann mittels verschiedener Methoden hergestellt werden. Aus historischer und qualitativer Sicht stellt die Flaschengärung das klassische Verfahren dar. Dieses Verfahren ist für die Herstellung von Champagner, Crémant, Prosecco und Cava zwingend vorgeschrieben. Auch in Deutschland werden hochwertige Schaumweine wie der Winzersekt zunehmend nach der sogenannten "Champagnermethode" erzeugt. Bei der Flaschengärung wird bereits vergorenem Grundwein etwa 24 g/l Zucker und Weinhefe (Fülldosage) zugefügt. Die in der Flasche stattfindende zweite alkoholische Gärung erzeugt Alkohol und Kohlenstoffdioxid. Durch die verschlossene Flasche bleibt das Kohlenstoffdioxid (etwa 12 g/l) in Lösung, woraus bei vollendeter Gärung 6 bis 8 bar CO-Druck bei 20 °C im Wein resultieren. Durch die zweite Gärung gewinnt der Wein auch etwa 1,3 Volumenprozent Alkohol, weshalb leichte Grundweine bevorzugt werden.

Neben Kohlenstoffdioxid entsteht während der Flaschengärung ein Depot aus abgestorbenen Hefen. Im Kontakt mit diesem Hefelager gewinnt der Schaumwein an Qualität und Finesse. Die Verweildauer auf der Hefe während der Reifung ist ein Qualitätsfaktor. Um das Depot zu entfernen, werden die Flaschen einem mechanischen Klärprozess unterzogen, der Remuage (deutsch: Rütteln). Für den Vorgang des Rüttelns werden die Flaschen in Rüttelgestelle beziehungsweise Rüttelpulte (französisch: "pupitres") umgelagert. In diesen Gestellen werden die Flaschen täglich gerüttelt und leicht gedreht. Außerdem verändert man langsam die Neigung der Flasche, bis sie im Laufe mehrerer Wochen nahezu senkrecht auf dem Flaschenkopf stehen. Bei dieser manuell durchgeführten Tätigkeit sinkt das Hefedepot in den Flaschenhals. Das maschinelle Abrütteln geschieht mittels Gyropalette. Hierbei wird die komplette Palette programmgesteuert gerüttelt, geneigt und gedreht. Das Rütteln dient allein der optischen Klarheit des Schaumweins, seine Haltbarkeit oder geschmackliche Qualität werden davon nicht beeinflusst.

Beim Entfernen des Depots, dem Degorgieren (französisch: "dégorgement"), wird der Flaschenhals in eine Kühlflüssigkeit getaucht. Dadurch gefriert das Depot zu einem Pfropfen, der beim nachfolgenden Öffnen der Flasche durch die Kohlensäure aus der Flasche gedrückt wird. Der bei diesem Arbeitsgang verloren gegangene Schaumwein wird durch eine Versanddosage aufgefüllt. Die Dosage besteht aus einer Mischung aus Wein und Zucker. Die Zusammenstellung und Menge der Dosage bestimmt das spätere Geschmacksbild des Schaumweins zwischen herb (französisch: "brut") bis süß (französisch: "doux"). Nach der Dosage und der Einstellung der gesetzlich vorgesehenen Füllmenge werden die Flaschen verkorkt, agraffiert, verkapselt, etikettiert und in Versandkartons verpackt. Ab diesem Zeitpunkt gewinnt der Schaumwein nicht mehr an Qualität.

Die Technik des "Transvasierverfahrens" ähnelt in einer ersten Phase der klassischen Flaschengärung. Nach einer kurzen zweiten Gärung in der Flasche wird der vergorene Schaumwein in einen Druckbehälter überführt. Die Einstellung des Geschmacksbilds erfolgt über die direkte Dosage in den Tank. Über eine Filteranlage gelangt der Schaumwein aus dem Drucktank in die Flasche. Dadurch entfällt das aufwändige Rütteln sowie die manuelle Entfernung des Depots. Bereits im 19. Jahrhundert experimentierte man mit dem Umfüllen (französisch: "transvaser") des entheften (degorgierten) Sekts in kleinere Gefäße. Das Problem des dabei auftretenden Druckverlustes konnte erst mit Drucktanks, druckstabilen Filteranlagen und Gegendruckfüllern gelöst werden. Die technischen Voraussetzungen hierfür standen allerdings erst Mitte des 20. Jahrhunderts zur Verfügung.

Bei der Großraumgärung (auch "Charmat-Verfahren" oder "cuve close" genannt) erfolgt bereits die zweite Gärung in einem Drucktank. Obwohl der Gärprozess dem einer Flaschengärung ähnelt, kommen die fertigen Schaumweine mit dem Charmat-Verfahren qualitativ nicht ganz an die der Weine mit klassischer Flaschengärung heran.

Die Familie der Likörweine ist groß. Portwein, Sherry, Marsala, Madeira, Commandaria, Mavrodaphne, Málaga, Moscatel de Setúbal oder Vin Doux Naturel ist eines gemeinsam: durch Beimengung von hochprozentigem Alkohol wird die meist noch nicht beendete alkoholische Gärung gestoppt.

Früher wurden die Weine in hauptsächlich warmen Weinbauregionen mit Alkohol stabilisiert, da die Weine häufig während des Transports durch ein erneutes Einsetzen einer unerwünschten Gärung verdarben. Aus technischer Sicht ist ein Aufspriten von Weinen nicht mehr nötig, sondern gehört vielmehr zum Stil des Likörweins.

Während die Zugabe von Alkohol meist noch während der Gärung erfolgt, dient das Aufspriten beim Sherry der Stabilisierung eines Zustands nach Alterung und Verschnitt.
Weine sind – wie auch Lebensmittel – (thermodynamisch) instabil. Je nach betrachteter Komponente des Weines äußert sich die Instabilität auf ihre Weise. Beispielsweise kann der im Wein befindliche Alkohol zu Essigsäure abgebaut (fermentiert) werden. Dieser Prozess benötigt Essigsäurebakterien, die sich in der Luft befinden. Man bezeichnet solche Weine auch als "tote Weine". Sie schmecken dann säuerlich, dumpf, wie alte Rosinen. Man kann diesen Vorgang unterdrücken, indem man den Wein vor Luft schützt. Deshalb wird beim reduktiven Ausbau des Weins darauf geachtet, dass jedes Weinfass und jeder Weintank ganz gefüllt und sicher verschlossen ist, sodass möglichst wenig Luft auf den Wein einwirken kann. Nach der Abfüllung des Fassweins übernimmt der Korken diese Schutzfunktion. Einmal mit Luft in Kontakt gekommen, sollte der Wein bald verbraucht werden.

Neben dem Verderb des Weins gibt es noch zahlreiche andere Mikroorganismen, die die Haltbarkeit des Weines negativ beeinflussen können. Die Stabilität eines Weines hängt davon ab, ob seine Inhaltsstoffe eine fördernde oder hemmende Wirkung auf Mikroorganismen haben: Je höher der (natürliche) Gehalt an Alkohol, Gerbsäure (Tanninen) und anderen Säuren (Weinsäure, Zitronensäure, Äpfelsäure und so weiter, nicht aber Essigsäure) ist, desto schlechter für die Mikroorganismen und besser für den Wein. Aber dieser natürliche Schutz reicht zumindest bei Weinen mit einer Alkoholkonzentrationen von weniger als etwa 18 Prozent nicht aus, so dass sie zusätzlich konserviert werden müssen.

Schon seit dem Altertum wird Wein zur Konservierung „geschwefelt“, indem Schwefeldioxid hinzugegeben wird. Diese Schwefelgabe wirkt stark antimikrobiell. Zum Schwefeln wurde elementarer Schwefel (aus Schwefelblüte) oberhalb der Flüssigkeit eines Weinfasses verbrannt. Es bildete sich dabei Schwefeldioxid.
Dieses Schwefeldioxid löst sich teilweise im Wein als Schweflige Säure, steht aber immer in einem Gleichgewicht mit freiem Schwefeldioxid (sogenannter Freier Schwefel).
Die Schwefelung von Maische, Most oder Wein soll
Je höher der Gehalt an freiem Schwefel ist, desto stabiler ist der Wein. Das obige Gleichgewicht wird dabei durch einen höheren Säuregehalt nach links verschoben. Das bedeutet, dass ein säurereicher Wein mit insgesamt weniger Schwefeldioxid auskommt als ein säurearmer Wein.

Wie stark ein Wein geschwefelt sein muss, hängt auch davon ab, ob er gelagert werden soll und auch von der mikrobiologischen Belastung selbst. Ein Wein, der aus Trauben gekeltert wird, die zum Zeitpunkt der Ernte schon stark von Fäulnis befallen waren, ist deutlich stärker belastet als ein Wein, der aus gesunden Trauben gekeltert wird. Werden diese Weine dann noch über weite Strecken transportiert und sind dabei auch noch größeren Temperaturschwankungen ausgesetzt (beispielsweise bei Container-Transporten), so muss der Schwefelgehalt auch dieses widerspiegeln.

Heute werden Weine normalerweise durch die Zugabe von gasförmigem Schwefeldioxid aus Gasflaschen oder durch Zugabe von Salzen der Schwefligen Säure geschwefelt, da sich diese Zugabe wesentlich genauer dosieren lässt.

Der Ausbau von Wein im Eichenfass (französisch: „Barrique“) trägt zur Verbesserung der Haltbarkeit bei.

Eine weitere Methode zur Steigerung der Haltbarkeit ist die Filtrierung vor dem Abfüllen. Hefen und Bakterien werden weitgehend ausgefiltert, ohne die weiteren Inhaltsstoffe des Weines zu beeinflussen. Dabei wird nicht nur der Gärvorgang unterbrochen, sondern auch die Haltbarkeit verbessert.

Auch chemische Verfahren werden zur Haltbarmachung von Weinen eingesetzt. Insbesondere Überseeweine werden vor der Flaschenfüllung mit Kaltentkeimungsmittel (zum Beispiel Dimethyldicarbonat) versetzt. Diese töten in der verschlossenen Flasche alle Mikroorganismen ab und das Dimethyldicarbonat zerfällt dann zu natürlichen Bestandteilen des Weines.
Die weinrechtlich definierte Qualität eines Weins bemisst sich danach, wie er bei der Amtlichen Qualitätsweinprüfung eingeschätzt wird.

Bei der sensorischen Prüfung von Wein werden auch die lateinischen Begriffe "Color" (Farbe), "Odor" (Geruch) und "Sapor" (Geschmack) benutzt. Einfluss auf die sensorischen Eigenschaften haben die Rebsorte, die Rebfläche und das Mengenverhältnis der Inhaltsstoffe, namentlich das Mostgewicht. Weitere qualitätsbestimmende Faktoren sind die Erntemenge, Behandlung des Leseguts bei der Ernte, die Gewinnung des Mosts beim Keltern, die Gärung und der Ausbau des Weins.

Niederschlag findet die weinrechtlich definierte Qualität in den nach nationalem Recht festgelegten Qualitätsstufen. Qualitätsweine und Prädikatsweine müssen in Deutschland die sensorische und analytische Prüfung der Amtlichen Qualitätsweinprüfung erfolgreich bestehen, um als solche bezeichnet werden zu können. Das erfolgreiche Passieren der Amtlichen Qualitätsweinprüfung dokumentiert die Amtliche Prüfungsnummer (AP-Nummer), die jedem Qualitäts- und Prädikatswein nach dem Deutschen Weingesetz zugeteilt wird. Sie muss als obligatorische Angabe auf dem Etikett deklariert werden. Gehobene Qualitätsweine können als Prädikatswein, abhängig hauptsächlich vom Mostgewicht, eines der folgenden Prädikate erhalten:
Die Qualitätsstufe bestimmt einerseits den Kostenaufwand des Erzeugers und andererseits den im Markt durchsetzbaren Preis des Weins.

Trotz der Tendenz von Weinherstellern, möglichst frühzeitig trinkreife Weine zu erzeugen, erhalten zahlreiche traditionell hergestellte Qualitätsweine durch eine Nachreife während der Flaschenlagerung eine bessere Geschmacksnote. Massenware und kleinpreisige Markenweine verbessern sich durch Lagerung nicht, da sie trinkfertig abgefüllt werden. Auch viele Bordeauxweine der Klasse eines Cru Bourgeois gewinnen höchstens während einer Flaschenlagerung von fünf bis acht Jahren Charakter hinzu. Nur ausgesprochene Spitzengewächse erreichen erst nach 15 bis 20 Jahren ihren optimalen Entwicklungszeitpunkt.

Der ideale Aufbewahrungsort für Wein ist ein lichtgeschützter, kühler Raum ohne größere Temperaturschwankungen und frei von Erschütterungen. Flaschen mit natürlichen Korken sollten liegend gelagert werden, damit der Korken feucht gehalten wird. Einzige Ausnahme hiervon ist der Madeirawein, der stehend gelagert werden soll.

Die optimale Lagerungstemperatur von 10 bis 13 °C wird in den seltensten Fällen eingehalten und meist überschritten. Der in Bezug zur optimalen Temperatur höhere Wert (typischerweise 13 bis 15 °C) bewirkt eine etwas schnellere Reifung der Weine und kann durchaus gewollt sein, wenn man sehr junge Weine mit großem Lagerpotenzial wie zum Beispiel Grand-Cru-Weine aus dem Bordeaux oder Jahrgangsportweine binnen 12 bis 15 Jahren mit Genuss trinken möchte. Die Gastronomie nutzt diesen Effekt, um die Weine nicht zu lange lagern zu müssen.

Die optimalen Bedingungen sind lediglich bei alten und sehr alten Gewächsen unabdingbar.

Problematischer als die absolute Lagertemperatur sind sich auf die Qualität des Weines negativ auswirkende Temperaturschwankungen: Der Lagerraum sollte eine möglichst konstante Temperatur aufweisen. Durch Temperaturschwankungen entstehen Volumenänderungen des Weins in der Flasche, so dass es über den Korken zu gesteigertem Gasaustausch kommt. Je häufiger Flaschen solchen Schwankungen ausgesetzt sind, umso mehr Sauerstoff steht zur Oxidation des Weins zur Verfügung und führt zu einer beschleunigten Alterung.

Die langjährigen Erfahrungen der Weinerzeuger belegen hingegen, dass jahreszeitliche Schwankungen von 5 Grad durchaus akzeptabel sind und kaum negative Auswirkungen auf den Wein zeigen.

Werden Weinflaschen mit Naturkorken verschlossen, sollte die Luftfeuchtigkeit am Aufbewahrungsort bei mindestens 60 Prozent liegen, damit der Korken nicht austrocknet. Zu hohe Luftfeuchtigkeit konnte in der Vergangenheit dazu führen, dass das Etikett schimmelte oder sich ablöste. Dies ist der Grund, warum Jahrgangsportweine und entsprechende Madeiraweine nicht mit einem Etikett versehen werden. Die relevanten Informationen werden direkt auf das Glas der Flasche aufgedruckt. Darüber hinaus versehen Spitzenweingüter den Korken mit dem Jahrgang und dem Namen des Weinguts.

Wein wird aus Weingläsern genossen, von denen es je nach Art des Weines spezielle Formen und Größen gibt. Die unterschiedlichen Glasformen dienen zwei Zwecken: Zum Ersten soll die Gesamtform eines Glases die Entfaltung der mit der Nase aufgenommenen Aromen unterstützen. Zum Zweiten soll durch Art und Anordnung der Mündung die Kopfhaltung beim Trinken beeinflusst und die Wahrnehmung durch die Zunge gesteuert werden (Gläser für Süßweine bedingen zum Beispiel durch ihre Form eine Haltung, die beim Trinken dafür sorgt, dass die Geschmackswahrnehmung „süß“ in den Hintergrund tritt, um dem Trinkenden die Erfassung der übrigen, vorhandenen Aromen zu ermöglichen).
Während Weißweine gekühlt (8 bis 12 °C) serviert werden, werden Rotweine bei 14 bis 18 °C getrunken. Zur Kühlung bei Tisch werden meist Weinkühler verwendet, vasenähnliche Behälter, die entweder durch Isolation, Eiseinlage oder durch Verdunstungskälte (poröse, genässte Terrakotta-Behälter oder ein um die Flasche gelegtes feuchtes Tuch) wirken.

In der gehobenen Gastronomie ist es üblich, Weinflaschen erst am Tisch zu öffnen und den Gast zunächst degustieren zu lassen. Einen besonders alten Rotwein, bei dem Inhaltsstoffe wie Weinstein auskristallisiert sind, wird ein Kenner zunächst aus der Flasche in eine Karaffe umfüllen, wobei er ihn dekantiert. Beim langsamen Abgießen über die Kante des Flaschenhalses bleibt eventueller Satz, das Depot, in der Flasche zurück. Anschließend lässt man den Rotwein längere Zeit „atmen“, das heißt, man gibt den Inhaltsbestandteilen Gelegenheit, mit dem Luftsauerstoff Verbindungen einzugehen. Bei sehr alten Weinen ist Vorsicht angeraten, da eine zu lange Oxidation durch Luftsauerstoff zum Verderb führen kann. Im Jahre 2006 veröffentlichte das "Institut national de la recherche agronomique" in Paris eine Studie, nach der eine übermäßige Oxidation durch Zugabe einer Prise gewöhnlichen Speisesalzes verhindert werden kann.

Weltweit wurden 2016 nach ersten Schätzungen der „Internationalen Organisation für Rebe und Wein (OIV)“ auf 7.528.000 Hektar (2012) Anbaufläche 259,5 Millionen Hektoliter Wein produziert. Die drei größten Produzenten waren Italien (48,8 Millionen Hektoliter), Frankreich (41,9 Millionen Hektoliter) und Spanien (37,8 Millionen Hektoliter).

Weinanbaufläche und -produktionsmenge der größten Weinerzeugerländer und deren weltweiter Anteil in % von der Gesamtfläche im Jahr 2012

Deutschland ist Netto-Importeur von Wein. In Deutschland wird mehr als doppelt so viel getrunken wie die Winzer hierzulande ernten. Etwas mehr als die Hälfte der weltweiten Weinerzeugung entfällt auf Europa. Deutschland rangiert etwa auf Platz 20 weit hinter China, Russland oder den Vereinigten Staaten.

In den frühen 1980er-Jahren wurde rund ein Viertel mehr als heute produziert. Während der Finanzkrise 2008 beziehungsweise der Wirtschaftskrise 2009 gab es einen konjunkturbedingten Einbruch. Etwa ein Fünftel des deutschen Weins wird exportiert.

Die Ausbildung in den Berufsfeldern Weinbau und Kellerwirtschaft kann in den angeführten deutschsprachigen Ländern in Landwirtschaftlichen Fachschulen (Weinbauschulen), Fachmittelschulen sowie Fachhochschulen und an Universitäten erfolgen.













Übermäßiger Konsum von Wein kann zu körperlicher und psychischer Abhängigkeit führen sowie Erkrankungen wie Leberzirrhose, Entzündungen der Bauchspeicheldrüse, Magenkrebs, Speiseröhrenkrebs und Nervenerkrankungen hervorrufen; regelmäßiger Konsum auch kleiner Mengen von Alkohol kann das Brustkrebsrisiko erhöhen.

Alkoholfreier Wein ist eine Alternative zu herkömmlichen Wein mit Alkohol. Er ist trotz der Bezeichnung "alkoholfrei" ungeeignet für Menschen, die aufgrund gesundheitlicher Aspekte auf Alkohol verzichten möchten oder müssen, da er immer noch bis zu 0,5 Vol.-% Alkohol enthalten kann.

Alkoholfreier Wein wird hauptsächlich im so genannten Vakuumverfahren hergestellt. Bei Temperaturen von ca. 28 Grad Celsius wird dem Wein im luftleeren Raum der Alkohol entzogen.

Die Herstellung von alkoholfreiem Wein ist in Deutschland gesetzlich geregelt. § 47 der Weinverordnung regelt explizit die Bestimmungen für alkoholfreien Wein. Danach müssen alkoholfreie Weine durch den Entzug von Alkohol gewonnen werden. Voraussetzung für die Bezeichnung "alkoholfrei" ist ein Alkoholgehalt von weniger als 0,5 Volumenprozent.


Allgemein

Geschichte

Kellerwirtschaft / Önologie

Weinbau

→ (u. a. über Robert Parker, Robert Mondavi, Michel Rolland)



</doc>
<doc id="12018" url="https://de.wikipedia.org/wiki?curid=12018" title="Hölle">
Hölle

Die Hölle ist nach traditionellen Vorstellungen des Christentums ein Ort der Qual, an welchen Übeltäter nach dem Tod gelangen, bevölkert von Dämonen und dem Teufel. In modernen christlichen Glaubenslehren ist diese Vorstellung allerdings in verschiedener Weise modifiziert oder auch ganz fallen gelassen worden. Andere Religionen und Kulturen hatten bzw. haben teilweise ähnliche Vorstellungen eines jenseitigen unwirtlichen Ortes der Verdammnis.

Das Wort "Hölle" – althochdeutsch "hell(i)a", mittelhochdeutsch "helle", altsächsisch "hellja", altfriesisch "helle, hille", angelsächsisch "hell", altnordisch "hel", gotisch "halja" – geht auf die germanische Sprachwurzel "*hel, *hal" „verbergen“) zurück. Entsprechend bedeutet in der altnordischen Mythologie der Name der Todesgöttin Hel und der gleichnamige Ort in der Unterwelt ursprünglich „[die] Bergende“, „[die] [die Toten] Aufnehmende“.

Die in den romanischen Sprachen verbreiteten Varianten wie italienisch "inferno", spanisch "infierno" oder französisch "enfer" gehen auf das lateinische "infernus" zurück, das sich von "inferus", „unten“, „unterirdisch“, ableitet.

Im Christentum wird die Existenz einer Hölle gelehrt. Dabei gibt es viele unterschiedliche Vorstellungen, was damit gemeint sei. Traditionell ist sie ein Ort ewiger Verdammnis, an den die Seelen der Missetäter nach dem Jüngsten Gericht gelangen. Sie steht im Gegensatz zu einem Ort absoluter Glückseligkeit (Paradies, ewiges Leben, Himmel). Das Purgatorium (Fegefeuer) nimmt als ein Ort der Läuterung eine Zwischenstellung ein.

In den Texten des Neuen Testament spricht Jesus Christus von einem Ort der Verdammnis, wenn er etwa vor Feuer warnt (; , ), vor der Finsternis, in der Heulen und Zähneklappern herrschen () und vor dem Tag des Gerichtes (). Das Christentum sieht sich andererseits als Erlösungsreligion, nach der die der Sünde und dem Tod verfallenen Menschen durch den Sühnetod und die Auferstehung Jesu Christi gerettet werden. Im Lehren und Wirken Jesu und der Apostel (vgl. ) wird verkündet, dass Christus gekommen sei, um alle Menschen zu erlösen (, , , ).

Die Offenbarung des Johannes erwähnt das Gericht über alle Toten. Dem „Feuersee“ werden, nachdem alle nach ihren Werken gerichtet wurden, letztlich „der Tod und die Unterwelt“ übergeben. .

Die Hölle wird in der christlichen Ikonographie häufig als Höllenrachen, als lodernder Flammenort und Höllenberg dargestellt. Darstellungen der orthodoxen Kirchen kennen auch den Feuerfluss.

Ebenso wie die wörtliche Lektüre der Offenbarung des Johannes prägte die apokryphe "Offenbarung des Petrus" des 2. Jahrhunderts stark die spätere mittelalterliche Vorstellung der Hölle als Ort der ewigen Strafen. Sie beschreibt diese detailliert, und dass die menschlichen Opfer teilweise sogar an der Bestrafung mitwirken können. Zwar wurde die Schrift nicht in den biblischen Kanon aufgenommen, einige Apologeten wie zum Beispiel Clemens Alexandrinus (150–215) sahen sie allerdings als ein Zeugnis des Apostels Simon Petrus an, so dass ihr Einfluss bedeutend war.

Viele Kirchenväter des ersten bis dritten Jahrhunderts (zum Beispiel Klemens von Rom, Ignatius, Justin der Märtyrer, Irenäus von Lyon, Tertullian und später Augustinus von Hippo) beschreiben eine ewige Hölle in unterschiedlicher Form. Auch in den nachbiblischen "Apophthegmata Patrum," den volkstümlichen "Aussprüchen der Wüstenväter", die großenteils aus dem christlichen Ägypten des 4. Jahrhunderts stammen, finden sich sehr drastisch-bildliche Schilderungen.

Manche frühen Theologen wie etwa Origenes (185–254) lehrten die Allaussöhnung, das heißt die Rückkehr "aller" Geschöpfe zu Gott, was auch von einigen Kirchenvätern des vierten und fünften Jahrhunderts aufgenommen wurde, etwa Gregor von Nyssa, Didymus dem Blinden, Diodor von Tarsus und Theodor von Mopsuestia. Durch die Liturgie des Letzteren wurde die Apokatastasis (Allaussöhnung) in die Assyrische Kirche übernommen. Von der katholisch-orthodoxen Reichskirche wurde diese Sichtweise abgelehnt. In einem lokalen Konzil wurde die Allversöhnungslehre 543 verurteilt, beeinflusst durch das von Kaiser Justinian I. verfasste "Liber adversus Origenem". Das Zweite Ökumenische Konzil von Konstantinopel im Jahre 553 verurteilte Origenes (Edikt contra Origenem) und verabschiedete den Kanon.
Im XVII. Artikel des Augsburgischen Bekenntnisses von 1530 formulierte die evangelisch-lutherische Kirche: 

Die in der Bibel vorkommenden Begriffe Scheol, Gehenna und Hades wurden bzw. werden mit Hölle übersetzt, haben aber zumindest teilweise einen verschiedenen Bedeutungszusammenhang und Aussage.

Im Alten Testament (Ps. 16,10) kommt der Begriff "Scheol" vor. Dort passiert nach Aussagen des Buches Kohelet jedoch nichts: "„Kein Tun ist, noch Berechnung, noch Erkenntnis, noch Weisheit im Sheol, wohin du gehen musst“" (Pred. 9,10; nach Buber), und "„die Toten aber, sie erkennen nichts, und kein Lohn ist ihnen noch weiterhin, denn vergessen ist ihr Gedenken“" (Pred. 9,5). "„Der Herr tötet und macht lebendig; er führt in den Scheol hinab und führt herauf“" (1. Samuel 2,6).

Der "Hades" des Neuen Testaments ist die griechische Übersetzung des hebräischen "Scheol". "Hades" wurde (manchmal bis in die Gegenwart) mit dem Ausdruck "Hölle" übersetzt. Martin Luther übersetzte es fünfmal mit ‚Hölle‘ (u. a. Mat. 16,18), zweimal mit ‚Toten‘, zweimal mit ‚Totenwelt‘, einmal mit ‚sein Reich‘. Neuere Bibelausgaben übersetzen meist nicht mit 'Hölle', sondern ‚Totenwelt‘, ‚Unterwelt‘, ‚Grab‘, ‚Gruftreich‘ oder ähnlich.

"Geenna" (oder Gehenna) ist eine Ortsbezeichnung in hebräischer Sprache und bedeutet ‚Schlucht von Hinnom‘ ("Ge-Hinnom"). Diese Schlucht kann südlich der Jerusalemer Altstadt bis heute besichtigt werden. Zu alttestamentlicher Zeit wurden hier laut Bibel bei kultischen Handlungen dem Ammoniter-Gott "Moloch" Kinder geopfert (2. Könige 23,10). Diese Praxis wurde von den Israeliten unter der Regentschaft Salomos im 10. Jh. v. Chr. und des Königs Manasse im 7. Jh. v. Chr. in Krisenzeiten weitergeführt bis in die Zeit des babylonischen Exils (6. Jh. v. Chr.). Der Prophet Jeremia, der diesen Brauch scharf verurteilte, nannte das Tal „Schlucht der Umbringung“ (Jer. 7,31-32; 19,5-9). Gehenna wurde später zu einer zentralen Müllhalde, unter anderem um eine Wiedereinführung solcher Bräuche zu verhindern. Nach Ansicht mancher Forscher wurden zu Zeiten Jesu an diesem Ort auch die Leichen von Gesetzesübertretern nach ihrer Hinrichtung verbrannt. Die Vorstellung von brennenden Menschenleichen inspirierte demnach jüdische wie danach auch christliche Theologen, hier ein Bild für die „Hölle“ zu sehen. Luther übersetzte "Geenna" achtmal mit ‚Hölle‘ (u. a. Mat. 5,22,29,30; 18,9; Mk 9,43,45) und viermal mit ‚höllisch‘. Auch neuere Bibelübersetzungen behalten ‚Hölle‘ als Übersetzung von "Geenna" bei.

Seit dem Zeitalter der Aufklärung bis in die Gegenwart wird die Hölle als angstauslösende Vorstellungswelt kritisiert bzw. verworfen, die für weltliche Zwecke oder zur Unterwerfung der Gläubigen eingesetzt worden sei – mit Hilfe ihrer Furcht vor dem Tod und dem, was danach kommt. Bezeichnend ist der Satz, „die erfunden werden müsste, wenn es sie nicht gäbe“ (Nicolas Sylvestre Bergier in der "Encyclopédie Française" von Denis Diderot, im Jahr 1772).

Die Weltanschauungen der Theosophie und der Anthroposophie suchen einen Sonderweg. Die Menschen des 20. und 21. Jahrhunderts mit seiner rasanten Weiterentwicklung wissenschaftlicher Forschung und deren Ergebnissen lösen sich von hergebrachten religiösen Vorstellungen und beginnen, Lösungsansätze für die „Hölle auf Erden“ zu suchen.

Die Lehre der katholischen Kirche besagt, dass es eine Hölle gibt und diese ewig dauert. Die katholische Kirche versteht den Begriff Hölle als den selbstverschuldeten endgültigen Ausschluss eines Menschen aus der Gemeinschaft mit Gott, also die Erfahrung letzter Sinnlosigkeit. („Ferne von Gott“). So etwa Papst Johannes Paul II. Nichtchristen guten Willens kommen nach katholischer Lehre nicht in die Hölle (KKK 847). Der Katechismus der Katholischen Kirche behandelt die Hölle im Artikel 12 ("„Ich glaube das ewige Leben“") unter IV: "Die Hölle". 

Einige katholische Theologen wie (undeutlich) Hans Urs von Balthasar oder (schärfer) Gisbert Greshake versuchten eine theologische Vermittlung zwischen Allerlösungstheorie und definitivem Höllendogma: Demnach gibt es zwar die Hölle als „reale Möglichkeit“ (Karl Rahner), aber sie könnte „am Ende leer“ sein, denn niemals wurde die ewige Verdammnis eines bestimmten Menschen verbindlich gelehrt.

Hans Küng schließt sich der Position von Gisbert Greshake an: Die Hölle sei kein bestimmter Ort und keine bestimmte Zeit, sondern gemeint sei der Moment der Begegnung eines sterbenden Menschen mit Gott. In diesem Moment begegne der unfertige und unvollkommene Mensch dem heiligen, unendlichen, liebevollen Gott. Diese Begegnung sei zutiefst beschämend, schmerzhaft und deswegen reinigend. Das Wort Fegefeuer sei eine falsche Übersetzung des lateinischen Wortes "purgatorium" (Reinigung). Moderne evangelische Theologen vertreten oftmals ebenfalls diese Position. Ähnlich sieht das – im Anschluss an Rudolf Bultmanns Darstellung, wonach Jesus die Höllen-Drohworte erst nach seinem Tod in den Mund gelegt worden seien – die katholische Theologin Uta Ranke-Heinemann.

In der derzeitigen theologischen Hauptrichtung wird auch gegen die Angstdrohung einer Strafe oder der Verdammnis Position bezogen, weil sie nicht mit Aussagen der Bibel oder mit den Eigenschaften Gottes wie Liebe, Barmherzigkeit und Gerechtigkeit vereinbar sei. Nach dieser Anschauung verkünde das Neue Testament statt wie auch immer gearteter Höllenqualen die frohe Botschaft der Versöhnung aller oder zumindest der meisten Menschen mit Gott.

Andere Theologen wiederum meinen, es sei nicht vertretbar, die Existenz einer Hölle zu leugnen. Sie müsse ebenso gelehrt werden wie die Möglichkeit des Menschen, durch Hinwendung zu Jesus Christus gerettet zu werden. In dieser Tradition steht auch die Aussage des emeritierten Papstes Benedikt XVI., der 2007 in seinem stark beachteten Jesusbuch sagte, dass Jesus Christus gekommen sei, um uns zu sagen, dass er uns alle im Paradies haben wolle. Die Hölle, von der man in unserer Zeit so wenig spräche, existiere und sei ewig für jene, die ihre Augen vor Jesu Liebe verschlössen. Bereits in seinem Buch "Einführung in das Christentum" aus dem Jahr 1968 befasste sich Ratzinger mit der christlichen Definition des Begriffes "Hölle" als Ort der Einsamkeit, an den keine Liebe mehr dringen kann.

In der ökumenischen Fassung des Apostolischen Glaubensbekenntnisses von 1971 wurden die Worte der lateinischen Fassung „descendit ad inferos“, die Luther mit ‚niedergefahren zur Hölle‘ übertragen hatte, ersetzt durch ‚hinabgestiegen in das Reich des Todes‘.

Neben der Höllenlehre werden seit Beginn des Christentums auch zwei andere theologische Denkschulen vertreten. Dies sind die Auslegungen der Allaussöhnung (Gott führt alle Menschen zu sich) und des Annihilationismus (Ungläubige werden vernichtet). 

Die orthodoxen Kirchen sehen sowohl Himmel als auch Hölle als intime Nähe zu Gott, diese werde aber von den Gerechten als freudig und segensreich, von den Bösen dagegen als qualvoll und voller Gewissensbisse erlebt.

Strikte Richtungen des Calvinismus in der Tradition von Augustinus von Hippo lehren, dass Gott in völlig freier und unerforschlicher Entscheidung nur einige Menschen zum Himmel und die anderen zur Hölle vorherbestimme (Prädestinationslehre). Die schicksalhafte Belastung der Menschen mit der Erbsünde schließe den freien Willen aus. Nur noch der von Gott eingegebene Glaube an das Selbstopfer und die Herrschaft Jesu Christi als dem Lamm Gottes und an dessen Auferstehung sei der Weg, um gerettet zu werden.

Andere Konfessionen, z. B. die anglikanische Kirche, methodistische und wesleyanische Kirchen und viele moderne reformierte Kirchen lehren, dass der Mensch auf Gottes Gnade frei antworten müsse, um gerettet zu werden, und daher das Heil der Seele letztlich doch mit vom Menschen abhinge (ebenso die katholische Kirche).

Der Katechismus der Episkopalkirche der Vereinigten Staaten von Amerika lehrt, dass die Hölle den Zustand des ewigen Todes in der Ablehnung Gottes bezeichne.

Die Kirche Jesu Christi der Heiligen der Letzten Tage (Mormonen) bezeichnet als Hölle zwei unterschiedliche Orte: zum einen den Ort der Ungehorsamen Geister in der Geisterwelt (wenn diese dort umkehren, kommen sie in eines der drei Reiche der Herrlichkeit), zweitens den Ort, wo Menschen, die den heiligen Geist trotz besseren Wissens leugnen, sein würden, manchmal auch als „äußere Finsternis“ bezeichnet.

Einige Gruppen der Adventbewegung, sowie die Bibelforscher, die Zeugen Jehovas und die Christadelphians lehren den Annihilationismus, wonach die Bösen beim Endgericht mit Leib und Seele vollständig vernichtet würden. In neuerer Zeit zeigten einige evangelikale Theologen, darunter der anglikanische Autor John Stott, für diese Lehre ein gewisses Maß an Sympathie.

Im altägyptischen Glauben war am Ende des Lebens die Reise nach Sechet-iaru, dem Lichtland im Totenreich, das Ziel. Das Totenreich ist in mehrere Bereiche aufgeteilt, beispielsweise in die Duat und die Vernichtungsstätte. In der dunklen Region der Vernichtungsstätte mangelt es an allem, an Wasser, Brot und Licht. Dämonische Wesen schlagen Köpfe ab, trennen Hälse vom Rumpf, reißen Herzen aus der Brust, richten Blutbäder an. Nur das Bestehen des "negativen Sündenbekenntnisses" in der "Halle der Vollständigen Wahrheit", dem Sitzungsort des Totengerichtes, konnte die Verbannung in die Vernichtungsstätte verhindern.

In der Vorstellungswelt des Mithras-Kultes gibt es am Ende der Welt eine Schlacht zwischen den Kräften des Lichts und den Kräften der Finsternis. Für Gläubige, die sich den Dogmen der Priester des Gottes Mithras angeschlossen hatten, gab es in diesem „Jüngsten Gericht“ die Möglichkeit, sich den „Geistern des Lichts“ anzuschließen und damit gerettet zu werden, oder als Ablehnende dieser Weltsicht zusammen mit dem bösen Geist Ahriman und den gefallenen Engeln, weiblichen Schutzgeistern, die versagt hatten, in einem Feuerschlund in Pech und Schwefel zu versinken.

Menschen, die sich nicht geheimes Wissen über die Götter angeeignet hatten, die Uneingeweihten also, kamen nach dem Tod in das Reich des Gottes Hades. Der "Hades" wurde als ein kaltes, dunkles Reich vorgestellt, „Reich der Schatten“. Als Schatten vegetieren die Toten ohne Bewusstsein vor sich hin. Daneben existierte der Tartaros, nach Auffassung einiger antiker Autoren der tiefste Teil des Hades, der von einer Mauer umschlossen war. Von diesem Ort gab es keine Wiederkehr. Zeus hatte in den Tartaros die Titanen verbannt, auch Tantalos und Sisyphos.

In den nordgermanischen Mythen gab es in der Unterwelt einen kalten, eintönigen Ort, beherrscht von der Todesgöttin Hel. Der Ort wurde ebenfalls "Hel" genannt. Er lag im Gebiet Niflheim und wurde durch den Fluss Gjọll begrenzt.

Im Hinduismus spielt die Vorstellung von Hölle ("naraka") eine untergeordnete Rolle. Trotzdem kennt die indische Mythologie verschiedene schreckliche Höllen, die nach dem Glauben mancher Hindus einen Teil des unendlichen Kreislaufs der Reinkarnation darstellen. Demnach erfährt der Verstorbene hier so lange großes Leid, bis sein schlechtes Karma, die negativen Folgen seiner Taten, verbraucht ist. Nach einiger Zeit kehrt das Individuum auf die Erde zurück, um wieder und wieder geboren zu werden – bis zur endgültigen Erlösung ("moksha").

Beschreiben einige indische Schriften die Höllen als Ort der Qual und den Himmel als freudvollen Ort, sprechen andere von geistigen Eigenschaften und Bewusstseinszuständen, den Gunas. So erklärt Krishna in der Uddhavagita, einem Teil des Bhagavatapurana (Kap. 19.42–43): "„Hölle ist das Ausdehnen von Tamas (Trägheit, geistige Dunkelheit). Himmel ist das Ausdehnen von Sattva (innere Harmonie, Einheit mit dem Selbst).“"

Der Buddhismus übernahm in modifizierter Form die hinduistischen Vorstellungen von Wiedergeburt und Hölle. Ähnlich wie im Hinduismus dienen auch hier die Qualen, die ein Sünder in den jeweiligen „Bestimmungen“ erleidet, dazu, diese Daseinsfaktoren zu reinigen und zu befreien, indem er dort den allgemeinen Satz „Alles Leben ist Dukkha“ sehr viel leichter einsehen kann als in dieser Welt. Dadurch kann er dann auf einer höheren Ebene wiedergeboren werden. Einer der sechs Daseinsbereiche des buddhistischen Lebensrades ist der „Bereich der Hölle“. Wie auch vieles andere im Buddhismus werden solche Lehren von vielen Buddhisten eher symbolisch verstanden.

Für die Vorstellungswelt im thailändischen Theravada-Buddhismus siehe Traibhumikatha.

Im Judentum wird die Vorstellung von der Hölle erst greifbar in den apokryphen Schriften, die später nicht in den Tanach aufgenommen wurden, wie beispielsweise im Buch Henoch (entstanden zwischen 130 und 68 v. Chr.). Dort wird der Aufenthaltsort der Verstorbenen mit vier tiefen Hohlräumen beschrieben, von denen drei dunkel sind und einer hell. In den dunklen Räumen wären die Sünder, die helle Abteilung sei für die Gerechten. Die Ungerechten würden von Engeln zu einem Platz gebracht, um für das Gericht vorbereitet zu werden. So heißt es: „Entsprechend der Taten der Bösen werden sie in lodernden Flammen brennen, schlimmer als Feuer“ (100.9) sowie „niemand wird ihnen helfen“ (100.4). „Und sei dir bewusst, dass sie [die Engel] eure Seelen in den Sheol [hebr. für „Hades“] bringen werden und sie [die Seelen] werden Böses erleiden und eine schwere Prüfung durchzustehen haben, in Dunkelheit, Fesseln und brennenden Flammen“ (103.7).

So wandelten sich viele ursprünglich ganz anders belegte Begriffe der hebräischen Bibel wie "Gehenna" (21.10) und "Sheol" zu Begriffen für verschiedene Orte, in denen Menschen mit Feuer gequält wurden, sofern sie sich im Leben etwas zu Schulden kommen ließen. Es wurden drei verschiedene Gruppen unterschieden (22.13): die Gerechten, die Sünder, die noch nicht im Leben bestraft wurden, und die „perfekten Kriminellen“ (die vollständig Bösen). Der Geschichtsschreiber Flavius Josephus (37–100 n. Chr.) schreibt in seiner Schilderung des Totenreichs vom Schoß Abrahams und der großen Kluft zwischen den verschiedenen Aufenthaltsräumen. In dieser ebenfalls apokryphen "Abrahamslegende" wird beschrieben, dass der Erzvater in den Sheol hinabsteigen und die Seelen der Ungerechten zu sich heraufholen dürfe, wenn sie genügend gebüßt und ihre Sünden gesühnt hätten. 

Im Islam wird die Hölle als Feuergrube gedacht, über die die schmale Brücke as-Sirāt in den Himmel führt. Alle Seelen der Toten müssen über diese Brücke gehen, und die Verdammten fielen in das Feuer hinunter, wenn sie nicht durch die Gnade Allahs erlöst würden. Für die Hölle gibt es im Koran etwa zehn verschiedene Bezeichnungen, die häufigsten unter ihnen sind „Feuer“ () und Dschahannam.

Im Koran ist wiederholt von Paradies und Hölle die Rede, so heißt es beispielsweise in Sure 23,103: „Diejenigen aber, die leichte Waagschalen haben, sind dann ihrer selbst verlustig gegangen. Sie werden ewig in der Hölle weilen“, und in Sure 11,106–107: „Die Unseligen werden dann im Höllenfeuer sein, wo sie laut aufheulen und hinausschreien, und wo sie weilen, solange Himmel und Erde währen, – soweit es dein Herr nicht anders will. Dein Herr tut, was er will.“ Eine sehr konkrete Vorstellung der Höllenstrafe findet sich in Sure 4,56: „Diejenigen, die nicht an unsere Zeichen glauben, die werden wir im Feuer brennen lassen: So oft ihre Haut verbrannt ist, geben wir ihnen eine andere Haut, damit sie die Strafe kosten. Wahrlich, Allah ist allmächtig, allweise.“

Im Islam dauert die Hölle nicht wie im Christentum unabänderlich ewig, sondern nur solange, wie Allah es will (Sure 6,128 und Sure 11,107). Auch hier werden verschiedene Grade der Pein unterschieden, abhängig von den Taten auf der Erde, wobei das diesseitige Leben als Prüfung gesehen wird und Himmel und Hölle als deren Konsequenzen.

Ein bekanntes Prophetenwort besagt, es seien mehr Frauen als Männer in der Hölle. Nach Ulrike Mitter war diese Annahme schon in der zweiten Generation des Islam weit verbreitet.

Richard Dawkins bezeichnet die Vorstellung einer Hölle als Kindesmisshandlung. Nach Schmidt-Salomon sind Atheisten der Auffassung, ethisch korrektes Verhalten lasse sich durch Anwendung der Vernunft erzielen. 

Laut der European Values Study glaubte im Jahr 1999 ein knappes Drittel der rund 40.000 befragten Europäer an die Existenz einer Hölle; in Deutschland rund 15 %. Am stärksten ist der Glaube an eine Hölle in der Türkei (90 %), Nordirland (60 %), Rumänien und Polen (je 55 %) verbreitet, am wenigsten in Dänemark, Schweden, Tschechien und den Niederlanden (etwa 10 %). 

Die Hölle war über die Jahrhunderte Gegenstand der Malerei. Bekannte Darstellungen stammen von Hieronymus Bosch (1450–1516), Hans Memling (vermutlich 1433/1440–1494), Luca Signorelli (vermutlich 1445/50–1523), Peter Paul Rubens (1577–1640) und Sandro Botticelli (1445–1510).

Ebenso wurde die Hölle zum Thema literarischer Werke. Zu den bekanntesten zählt die "Göttliche Komödie" von Dante Alighieri aus dem 14. Jahrhundert.

Dantes Hauptwerk Die Göttliche Komödie ist eine Art literarische Jenseitswanderung durch Hölle, Fegefeuer und Paradies. Die Hölle ist dort jener „Einschlagkrater“, den Satan bei seinem Sturz aus dem Paradies (Höllensturz) hinterlassen hat. In diesen Höllentrichter kommt man durch das Höllentor. Danach folgt zunächst eine Art Zwischenreich, wo diejenigen geplagt werden, die im Leben zu feige waren, sich zwischen Gut und Böse zu entscheiden. Nach der Passage des Höllenflusses Acheron folgt der Limbus, wo die tugendhaften Heiden in gramvoller Sehnsucht, aber ohne körperliche Leiden, ihr Schattendasein fristen. Im folgenden zweiten Kreis der Hölle werden die Wollüstigen gepeinigt, im dritten die Schlemmer. Dann folgen die Kreise der Geizigen und Verschwender sowie der Jähzornigen und Trägen. Kreis 5 ist auch der Ort des Höllenflusses Styx und der Stadt Dis. Im sechsten Kreis hausen die Ketzer und Gottlosen, im siebten Mörder, Selbstmörder, Gotteslästerer, Sodomiten, Wucherer. Der achte Kreis ist Kupplern vorbehalten, Verführern, Schmeichlern, Huren. Außerdem sind hier versammelt: Korrupte in kirchlichen oder öffentlichen Ämtern, Simonisten, Zauberer, Wahrsager, Heuchler, Diebe, Räuber, falsche Ratgeber, Häretiker und Zwietrachtstifter. Im neunten Kreis, im Mittelpunkt der Erde, steckt schließlich der ärgste Teufel, Luzifer, und peinigt die schlimmsten Sünder der Menschheitsgeschichte: Judas, Cassius und Brutus, die Mörder und Verräter des himmlischen und irdischen Kaisers. Von dort gelangen Dante und sein Führer Vergil zur südlichen Hemisphäre und zum Purgatorium, nach dessen Durchwanderung schließlich in das Paradies.







</doc>
<doc id="12019" url="https://de.wikipedia.org/wiki?curid=12019" title="Kulmbach">
Kulmbach

Kulmbach ist eine Große Kreisstadt im oberfränkischen Landkreis Kulmbach und der Sitz des Landratsamtes. Sie liegt am Main, etwa 20 km nördlich von Bayreuth.

Die Stadt ist bekannt wegen der dort ansässigen Brauerei, der Plassenburg, die unter anderem das Deutsche Zinnfigurenmuseum beherbergt, die größte Zinnfigurensammlung der Welt, und wegen der in Kulmbach produzierten Bratwurst.

Kulmbach liegt in der Mitte des bayerischen Regierungsbezirkes Oberfranken, etwa 25 km nordwestlich von Bayreuth. Am westlichen Stadtrand entsteht aus seinen beiden Quellflüssen, dem Roten und dem Weißen Main, der Main.

Die Stadt Kulmbach ist in 76 Stadtteile eingeteilt:
Beim Zensus am 9. Mai 2011 betrug die Einwohnerzahl der Großen Kreisstadt Kulmbach 26.678.

Aus der Zeit um 900 n. Chr. ist eine Kleinsiedlung im heutigen Stadtteil Spiegel bekannt, die aus einem Forsthof und einem bewehrten Fronhof zum Schutz des Mainübergangs am Grünwehr bestand. Das Gebiet ging später an die Grafen von Schweinfurt über, die sich in der Herrschaftsausübung meist von den Walpoten vertreten ließen.

Zum ersten Mal wurde der Name Kulmbach als "kulma" in einer Schenkungsurkunde in der Alkuinbibel zwischen 1028 und 1040 erwähnt. Der Name stammt von einem Bach, der vom Berg herunterkommt (Culmin-aha, Culmna). Dieser Bach wurde später aus einem Missverständnis heraus in Kohlenbach umbenannt.

Als die Schweinfurter Grafen im Mannesstamme ausstarben, kam Kulmbach durch die Heirat der Erbtochter Gisela mit Graf Arnold von Dießen an das Geschlecht der Dießen-Andechser.

Die Grafen erwarben weitere Gebiete rund um Kulmbach, erbauten im ersten Drittel des zwölften Jahrhunderts die erste Veste Plassenburg und gründeten in der heutigen Oberen Stadt eine Marktsiedlung und eine Kirche. Die Andechser erhielten im Jahre 1180 von Kaiser Barbarossa die Reichsfürstenwürde und den Herzogstitel von Meranien nach dem Ort Marano zwischen Venedig und Triest (heute Marano Lagunare).
Um 1231 erhielt Kulmbach Stadtrechte. Anstelle der alten Plassenburg, über deren Standort man sich nicht sicher ist, erbauten die Herzöge auf einem Bergsporn oberhalb Kulmbachs eine neue Burg. Mit dem letzten Herzog von Meranien, Otto II., der 1248 kinderlos starb, endete die Herrschaft der Meranier.

Nach jahrelangen Erbstreitigkeiten fiel die Herrschaft Plassenburg und damit Kulmbach 1260 an das thüringische Grafengeschlecht Orlamünde. Die Orlamünder vollendeten die neue Plassenburg und gründeten das Kloster Himmelkron. Schließlich verpfändeten sie Burg und Stadt. 1340 übernahmen nach dem Tode des letzten Orlamünde auf Grund eines Vertrages die Burggrafen von Nürnberg aus dem Geschlecht der Hohenzollern Kulmbach und die Plassenburg.

Bis ins 17. Jahrhundert wurde die Plassenburg Residenz des Burggrafenamtes, später des hohenzollernschen Fürstentums Kulmbach (auch: "Brandenburg-Kulmbach", bzw. nach 1604 "Brandenburg-Bayreuth"). Durch die günstige Lage an den Alt- und Geleitstraßen nach Bamberg, Nürnberg, Eger, Hof und Leipzig blühte der Handel in der Stadt – es bildeten sich Zünfte der Tuchmacher, Barchentweber, Färber und Seidensticker. Im Jahre 1398 lebten in Kulmbach 1500 bis 2000 Einwohner, dies lässt sich aus einem burggräflichen Bestandsverzeichnis über Grundstücke, Häuser und Steuern entnehmen. Die Stadt wurde regiert von einem herrschaftlichen Vogt, dem Bürgermeister und einem Rat angesehener Bürger.

Durch die Übertragung des Fürstenprivilegs 1363, der Kurwürde und des Besitzes der Mark Brandenburg 1415 nannten sich danach alle fränkischen Hohenzollern Markgrafen von Brandenburg-Kulmbach. Von 1411 bis 1529 standen Kulmbacher als Kanzler an der Spitze der Verwaltung der Mark Brandenburg (Friedrich Sesselmann, Sigismund Zehrer und Sebastian Stublinger).

Am 31. Januar 1430 brandschatzten die Hussiten die Stadt.

Im Zuge des Wiederaufbaus der weitgehend zerstörten Stadt wurde auch die Petrikirche in spätgotischem Stil wiedererrichtet, in der unter Markgraf Georg dem Frommen 1528 der erste evangelische Gottesdienst gefeiert wurde.

In Kulmbach bestand vor 1573 ein mittelalterliches Leprosorium; später existierte davon noch die Nikolaus-Kapelle, die 1666 vergrößert wurde.

Markgraf Albrecht Alcibiades wollte ganz Franken unter seine Herrschaft bringen und ein fränkisches Herzogtum gründen. Damit zog er sich die Gegnerschaft der Bistümer Bamberg und Würzburg zu, die sich mit der Freien Reichsstadt Nürnberg verbündeten. Im Bundesständischen Krieg, auch (2.) Markgräflerkrieg genannt, nahm diese Allianz am Konraditag (benannt nach dem heiligen Konrad von Konstanz), dem 26. November 1553 Kulmbach ein und plünderte die Stadt (→ Belagerung von Kulmbach und der Plassenburg). Am 21. Oktober 1554 wurde auch die Plassenburg, deren Besatzung den Feinden bis dahin standgehalten hatte, in Brand gesteckt und zerstört. Die Schlacht ist im Deutschen Zinnfigurenmuseum in Kulmbach nachgestellt.

Ab 1557 wurde die Stadt unter dem neuen Markgrafen Georg Friedrich wieder aufgebaut, 1559 begann auch der Wiederaufbau der Plassenburg zu einem der bedeutendsten Renaissancebauwerke in Deutschland. Allerdings verlegte der Nachfolger Georg Friedrichs, Markgraf Christian, den Sitz der Residenz 1604 in das benachbarte Bayreuth, da die Plassenburg nicht mehr den Vorstellungen des höfischen Absolutismus genügte, führte den Aufbau der Festung jedoch auch zu Ende. Die Residenz wurde später allerdings mehrmals nach Kulmbach zurückverlegt, einmal 1605 wegen eines Stadtbrandes in Bayreuth und dann im Dreißigjährigen Krieg aus Sicherheitsgründen. Auch Kulmbach wurde als protestantische Stadt von den kaiserlichen Truppen gebrandschatzt, die Plassenburg erwies sich jedoch als uneinnehmbar.

Nachdem die Residenz 1642 endgültig nach Bayreuth verlegt worden war, verlor Kulmbach seine politische Bedeutung. Somit lassen sich über die folgenden Jahre keine bedeutenden Fakten finden. Als Markgraf Carl Friedrich Alexander die Markgrafschaft Brandenburg-Bayreuth wegen seiner Geliebten Lady Elizabeth Craven an seinen Vetter, den König von Preußen verkaufte, wurde am 16. Januar 1791 auch Kulmbach preußisch. Die Plassenburg nutzte man in den folgenden Jahren als Lager für französische Kriegsgefangene.

Da die Plassenburg ein strategisches Hindernis für die im Jahre 1806 in den Krieg gegen Preußen ziehenden französischen und bayerischen Truppen darstellte, wurde die Stadt Kulmbach im Oktober und November 1806 besetzt. Nach der Kapitulation ihrer preußischen Besatzung wurden die Wehrmauern der Plassenburg geschleift, um sie als Festung militärisch wertlos zu machen. Schließlich wurde Kulmbach im Jahr 1810 Bayern zugesprochen.
Im Jahre 1846 bekam die Stadt einen Anschluss an die König-Ludwig-Süd-Nord-Bahn. Das hatte ein enormes Wirtschaftswachstum zur Folge, weil damit der Export von Gütern entschieden erleichtert wurde. Die Anzahl von Brauereien wuchs auf 26 bis zum Jahr 1882.

1890 wurde Kulmbach kreisfreie Stadt.

Auch in Kulmbach übernahm die NSDAP 1933 die Macht, auf der Plassenburg wurde die Reichsschule der deutschen Technik eingerichtet. Dafür bekam Kulmbach einen Autobahnanschluss, der Beginn der heutigen Bundesautobahn 70.
Anfang 1945 wurde Kulmbach kampflos von den amerikanischen Truppen eingenommen.

Bei der Kreisgebietsreform wurden am 1. Juli 1972 die Landkreise Kulmbach und Stadtsteinach zusammengelegt. Kulmbach wurde zum Sitz des Landkreises bestimmt.

Am 1. Januar 1902 wurde die bis dahin selbständige Gemeinde Blaich eingegliedert. Am 1. April 1946 kamen Kauernburg, Mangersreuth und Metzdorf hinzu. Am 1. Juli 1972 folgte Burghaig. Höferänger, das bis 1955 Unterdornlach hieß, kam am 1. Januar 1974 hinzu. Kirchleus, Lehenthal, Lösau und Oberdornlach folgten am 1. Januar 1976. Katschenreuth, Leuchau (ohne das 1. Januar 1972 eingegliederte Langenstadt) und Melkendorf schlossen am 1. Juli 1976 die Reihe der Eingemeindungen ab.


Die Kommunalwahlen 2002, 2008 und 2014 führten zu folgenden Sitzverteilungen im Stadtrat:

 Mit Lüneburg (Niedersachsen) besteht seit 1967 eine Städtefreundschaft.


Seit der Industrialisierung hatten Stadtumbauten mehr oder weniger große Verluste historischer Bausubstanz zur Folge. Galten diese Baumaßnahmen im späten 19./frühen 20. Jahrhundert noch der Errichtung von Industrieanlagen und Prachtstraßen, so war es besonders in den 1960er Jahren das Ziel, Kulmbachs Stadtmitte mit modernen Geschäftsflächen und zeitgemäßer Verkehrserschließung auszustatten. Heute ist es meist der marode Zustand der betreffenden Häuser, der eine denkmalgerechte Sanierung technisch und finanziell erschwert und so zum Abriss der Objekte führt (z.B. Unteres Stadtgässchen 4). Dennoch hat sich der historische Stadtkern Kulmbachs in großen Teilen erhalten. Das Ensemble der Altstadt wird geprägt vom Wiederaufbau ab der zweiten Hälfte des 16. Jahrhunderts (also nach der Zerstörung 1553) und weist daher stellenweise eine bemerkenswerte stilistische Geschlossenheit auf. Der im 16. Jahrhundert vorherrschenden Mode nach wurden die Häuser in der direkten Wiederaufbauzeit im Renaissance-Stil errichtet, sofern die Bauzeit schon in das 17. Jahrhundert fällt, finden sich auch bereits frühbarocke Formen. Da die Gebäude auf bestehende Fundamente und Parzellen gebaut wurden, ist der Stadtgrundriss allerdings wesentlich älter als die Bebauung selbst.

In die nachfolgende Liste wurden Gebäude entweder aufgrund ihrer Bedeutung für das herrschaftliche und städtische Leben Kulmbachs oder wegen ihrer architektonischen Qualität aufgenommen:








Neben den üblichen oberfränkischen Spezialitäten ist die Kulmbacher Bratwurst hervorzuheben; sie besteht aus einem sehr feinen Mett und wird in einem Anisbrötchen, dem sogenannten "Bratwurststollen", serviert. Die Bratwürste isst man vorzugsweise an einem der zahlreichen Bratwurststände in der Stadt. Weit über die Stadtgrenzen bekannt ist auch das von der Kulmbacher Brauerei AG gebraute Starkbier EKU 28, welches jahrelang als stärkstes Bier der Welt galt. Ihren Ruf als Lebensmittelstandort verdankt die Stadt Kulmbach auch dem Max Rubner-Institut (Bundesanstalt für Lebensmittelsicherheit, ehemals Bundesanstalt für Fleischforschung), das seit dem BSE-Skandal stark aufgewertet wurde.

Kulmbach ist traditionell ein Produktionsstandort der Getränke- und Lebensmittelindustrie. Eines der ältesten Kulmbacher Unternehmen ist die 1856 gegründete und heute international tätige IREKS GmbH mit den Geschäftsfeldern Backzutaten für Bäcker und Konditoren, Braumalze, Aromen, Speiseeis-Produkte und Agrarhandel. Die RAPS GmbH & Co. KG produziert in mehreren über Kulmbach verteilten Werken Gewürze für gewerbliche Anwendungen. Relativ neu am Ort sind die biotechnische Pharmazie in Gestalt der "Axolabs GmbH (seit März 2017 Teil des britischen LGC-Konzerns)", sowie der Bau und Vertrieb von Warmwasser- und Klimageräten, Wärmepumpen, Speicher- und Direktheizgeräten, wie sie die Glen Dimplex Deutschland GmbH herstellt. Die "AGO AG" ist auf Biomasse- und Blockheizkraftwerke und den wärme- und kältetechnischen Anlagenbau mit Kraft-Wärme-Kältekopplung spezialisiert. Bis 1996 war noch die Textilindustrie mit der Kulmbacher Spinnerei ein relevanter Wirtschaftszweig in Kulmbach. 

Die Max-Hundt-Volksschule Kulmbach ist eine der ersten zehn Ganztagsschulen in Bayern. Es gibt das Markgraf-Georg-Friedrich-Gymnasium und das Caspar-Vischer-Gymnasium sowie die Carl-von-Linde-Realschule und die zum Beruflichen Schulzentrum (BSZ) gehörenden Schulen Hans-Wilsdorf-Schule (Berufsschule und Wirtschaftsschule) und Adalbert-Raps-Schule (Berufs- und Fachoberschule).
Weitere Bildungseinrichtungen sind die Fachschule für Bautechnik, Fachrichtung Hochbau, Heizungs-, Lüftungs- und Klimatechnik (ebenfalls am BSZ), die Akademie für Neue Medien, die Schule für Pharmazeutisch-Technische Assistenten und die Fachschule für Lebensmitteltechnik.

Am 20. Juni 2017 beschloss das Bayerische Kabinett im Kulmbacher Rathaus die Errichtung einer siebten Fakultät der Universität Bayreuth. Die Fakultät "Life Sciences – Food & Health" soll der Erforschung von Nachhaltigkeit und Gesundheit von Ernährung und Lebensmitteln dienen. Der Fakultäts-Campus soll in Kulmbach auf dem ehemaligen Güterbahnhofs-Gelände 1000 Studienplätze bei 20 Professuren bieten. Neben einer enormen Aufwertung des Bildungsstandorts erhofft sich die Stadt eine Erholung vom Bevölkerungsschwund der letzten Jahre. Die Fertigstellung ist 2019 geplant, die ersten Immatrikulationen sollen 2020 erfolgen.


→ "Hauptartikel: Liste der Ehrenbürger von Kulmbach"




Namenspatenschaften:

"Kulmbach" ist auch der Name eines Airbus A321 der Deutschen Lufthansa mit dem Kennzeichen D-AIRL. Die Namensübertragung erfolgte am 3. November 1995.

Es besteht eine Patenschaft für das Minenjagdboot "Kulmbach" der Deutschen Marine (Minenjagdboot Klasse 333 – Minenjagdboote Kulmbach-Klasse); das Boot wurde am 31. März 2012 außer Dienst gestellt.




</doc>
<doc id="12022" url="https://de.wikipedia.org/wiki?curid=12022" title="Schnabelschuh">
Schnabelschuh

Ein Schnabelschuh ist ein nach wendegenähter Machart gefertigter Schuh, der mit einer auffällig langen Schuhspitze versehen ist. Ein weiterer Name der Schnabelschuhe ist "Poulaines". Die Schnäbel wurden "Kogeln", "Gogeln" oder "Gugeln" genannt. 

Schnabelschuhe sind eine Modeerscheinung, die sich primär in England und Frankreich ausbreitete. Interessanterweise ist deren Vorkommen in Mitteleuropa limitiert, wie die bislang gefundenen Beispiele zeigen.
Weiterhin war diese Mode offensichtlich eher den oberen Schichten der Gesellschaft vorbehalten, wie die Ausgrabungen bei Baynard’s Castle in London gezeigt haben. Hier war die königliche Garderobe im 14. Jahrhundert nicht weit entfernt.
Dieser Umstand wird auch von unzähligen Darstellungen im 14. und 15. Jahrhundert bestätigt, in denen nur die reiche Oberschicht solche Schuhe trug.

Allerdings ist bei bildlichen Quellen Vorsicht geboten. Einerseits gab es eine Form der Beinlinge, die eine dünne Ledersohle aufwiesen und fast ausschließlich im Haus oder eventuell außen mit Trippen getragen wurden. Diese Form findet man besonders oft in Gemälden des 15. Jahrhunderts, sie erweckt den Eindruck einer etwas längeren Spitze. Andererseits wurde die Oberschicht mit solchen Schnabelschuhen dargestellt, weil sie ein Statussymbol waren. Die Häufigkeit der Darstellung ist insbesondere in Mitteleuropa nicht mit den Funden in Einklang zu bringen. Der Schluss liegt nahe, dass hier die gesellschaftliche Stellung hervorgehoben werden sollte und die Bilder einen verzerrten Eindruck erwecken.

Ob Schnabelschuhe ihre Entstehung (um 1090) dem Grafen Fulko von Anjou oder Angers zu verdanken haben, der wegen seiner deformierten Füße lang zugespitzte Schuhe trug, ist fraglich. Spitz zulaufende Schuhe mit moderater Spitze sind zwar seit jener Zeit getragen worden, aber üblich war fast immer die runde Form.
Es mag sein, dass sie in Europa zuerst bei den Polen aufkamen, worauf der früheste englische Name "Cracowes" (von Krakau) vielleicht hinweist; doch schon zuvor wurden sie im Orient getragen.
Der Autor des "Eulogium Historiarum" datiert ihre erste Erscheinung auf die Jahre 1361–1362:

Schnabelschuhe wurden zuerst im späten 14. Jahrhundert populär – in den 1380er Jahren, vielleicht schon in den 1370er Jahren – und waren um 1400 schon wieder aus der Mode. Allerdings wurden sie in der Mitte des 15. Jahrhunderts erneut derart beliebt, dass Kleiderordnungen erlassen wurden, um ihre Verwendung und damit verbundene Exzesse zu regulieren. 1463 (zur Zeit Edwards IV.) wurde angeordnet, dass kein Ritter, Knappe, Adliger oder sonst eine Person Schuhe mit Spitzen länger als 2 Zoll tragen durfte. 1465 wurde der Erlass dahingehend verschärft, dass kein Schuster oder Schuhmacher Schuhe mit längeren Spitzen als 2 Zoll herstellen durfte. Selbst aus dieser Zeit sind aber keine Schuhe mit den oft zitierten Überlängen bekannt.

Im Laufe der Zeit trugen nicht nur die Adligen, sondern alle Schichten Schnabelschuhe, weswegen die Länge der Schuhspitze in Kleiderordnungen genau geregelt wurde und sich am sozialen Stand des Trägers orientierte. Daher stammt auch die Redensart „auf großem Fuß leben“. Trotz aller Reglementierungen hielten sich die Schnabelschuhe bis gegen das Ende des 15. Jahrhunderts, wo an ihre Stelle die "Entenschnäbel" und später die ganz stumpfen "Bärenklauen" oder "Ochsenmäuler" traten.

Gelegentlich wird behauptet, dass die Spitzen der Schnabelschuhe so lang gewesen seien, dass sie an das Knie oder den Gürtel angebunden werden mussten. Es gibt für solche Behauptungen keine Grundlage.
Sie beziehen sich auf zwei Fragmente einer Beschreibung aus dem 18. Jahrhundert eines Gemäldes von James I. von Schottland, die seitdem nie bestätigt wurde, und auf Behauptungen von zwei Antiquaren des späten 16. und frühen 17. Jahrhunderts, Stow und Camden. Stow gibt keine Quelle an. Camden bezieht sich auf das oben aufgeführte Zitat aus "Eulogium Historiarum", seine Übersetzung ist aber nicht zuverlässig.

Wie zum Beispiel die Ausgrabungen in London und Dordrecht zeigen, betrug die Spitzenlänge in der Regel ca. ein Fünftel der Fußlänge (93 von 210 gefundenen Schuhen in Baynard’s Castle), die längsten Spitzen waren etwa einen halben Fuß lang (7 von 210 Schuhen).

Schnabelschuhe waren wendegenäht. Das heißt, sie wurden zunächst mit der Innenseite nach außen genäht und dann gewendet. Dabei war die Spitze eine besondere Herausforderung, da sie nicht gewendet werden konnte. Daher wurde die Spitze erst nach dem Wenden des Schuhs mit versteckten Stichen genäht.

Dass die Schnabelschuhe für den rechten und den linken Fuß verschieden geschnitten wurden, war nicht neu. Die Mode, ein Paar gleiche Schuhe, also ohne Links-Rechts-Unterscheidung herzustellen, war erst im 17. Jahrhundert für einige Zeit populär, wurde aber dann wieder aufgegeben.

Zu den Schnabelschuhen kamen in der ersten Hälfte des 15. Jahrhunderts bei beiden Geschlechtern Trippen hinzu (Holzsohlen mit Riemenbefestigung). Diese Unterschuhe waren genau wie die Schnabelschuhe langspitzig gestaltet. Ziel der hölzernen Sohlen war es, die feinen Stoffe sowie das Leder möglichst zu schonen.

Die zivile Mode spiegelte sich oft im Design von Rüstungen wider. Auch Schnabelschuhe finden sich in Prunkharnischen wieder, z.B. in dem des Erzherzogs Siegmund (gefertigt 1485 von Meister Helmschmied aus Augsburg).




</doc>
<doc id="12023" url="https://de.wikipedia.org/wiki?curid=12023" title="Universität">
Universität

Universitäten (vom lateinischen "universitas magistrorum et scolarium", „Gemeinschaft der Lehrenden und Lernenden“, später im Sinne Humboldts für "universitas litterarum", „Gesamtheit der Wissenschaften“) sind Hochschulen mit Promotionsrecht, die früher als wissenschaftliche Hochschulen bezeichnet wurden, die der Pflege und Entwicklung der Wissenschaften durch Forschung, Lehre und Studium dienen, aber ihren Studenten auch praxisorientiert Berufsqualifikationen vermitteln sollen. Neben den Volluniversitäten, die ein breites Fächerspektrum (Universalität) anbieten und mehrere zehntausend Studierende haben können (Massenuniversität), gibt es auch kleinere staatliche und Privatuniversitäten, die meist auf wenige Fächer spezialisiert sind und deren Anzahl an Immatrikulierten eher im vierstelligen Bereich liegt.

Die Bezeichnung "Universität" (von ‚Gesamtheit‘) charakterisiert begrifflich im Wissenschaftsbereich ganz allgemein eine umfassende Bildungseinrichtung. An den damals neu gegründeten Institutionen von Bologna (gegründet 1088), Paris (gegründet um 1150) oder Oxford (gegründet im 12. Jahrhundert) studierte man im heutigen Sinne eines Studium generale. Es handelte sich um eine noch überschaubare Anzahl wissenschaftlicher Disziplinen ("septem artes liberales", ‚Sieben Freie Künste‘, ergänzt durch Theologie, Jurisprudenz und Medizin). Die Gesamtheit dieser Wissenschaften fasste man später unter der Bezeichnung "universitas litterarum" (‚Gesamtheit der Wissenschaften‘). Vor allem durch Wilhelm von Humboldt, der die Einheit von Lehre und Forschung zum Grundprinzip universitärer Arbeit erhob, wurde dieser Begriff für die moderne Universität prägend. Daneben trat das ursprüngliche Verständnis von "universitas", das aus den korporativen Organisationsformen mittelalterlicher Lehr- und Lerngemeinschaften ("universitas magistrorum et scholarium" ‚Gemeinschaft der Lehrenden und Lernenden‘) im Bereich bedeutender kirchlicher Bildungszentren erwachsen war, etwas in den Hintergrund. Es lebt aber im Begriff der Autonomie der Hochschulen weiter.

Mit der zunehmenden Ausdifferenzierung und Vermehrung der Wissenschaftsgebiete hat sich die an das Studium generale anknüpfende Begriffsfüllung überlebt, da heute keine einzelne Institution mehr die Gesamtheit der Wissenschaften vertreten kann. Insofern ist der Begriff Universität nur noch für die Gesamtheit sämtlicher, großenteils spezialisierter, Hochschulen sinnvoll verwendbar. Die auf die akademische Gemeinschaft ausgerichtete Begriffsfüllung hat ebenfalls ihren ursprünglichen Anwendungsort verloren und ihren Sinn erweitert, da diese Bedeutung für alle Hochschulen, also beispielsweise auch für die Fachhochschulen, zutrifft.

Prägend für den Begriff der Universität sind seit dem europäischen Mittelalter
Wesentlich ist auch, dass die Studierenden ihren eigenen Stundenplan zusammenstellen und er nicht wie bei einer Schule oder Fachhochschule vorgegeben wird.

Die Universitäten im deutschen Sprachraum bieten Ausbildungsgänge nach der International Standard Classification of Education (ISCED), dem UNESCO-System zur Klassifizierung von Ausbildungssystemen, in den Leveln 5 und 6. Sie gehören zum tertiären Bildungsbereich.

Die ersten Universitäten in Europa entstanden im hohen Mittelalter. Mit dem Aufkommen der Universitäten wurde das Lehr- und Wissensmonopol der Klöster durchbrochen. Dennoch wurde die universitäre Lehre vor allem in Mittel- und Nordeuropa noch bis über den Beginn der Neuzeit hinaus von den geistlichen Orden und dem Klerus beeinflusst.

Seit der Einrichtung der Berliner Universität im Jahre 1810 (nennt sich seit 1949 Humboldt-Universität) setzte sich auch international das Humboldtsche Modell der "Einheit von Forschung und Lehre" durch, das besagt, dass die Lehrkräfte zusätzlich zu ihrer Lehrtätigkeit auch Forschung betreiben sollen, damit das hohe Niveau der Lehre erhalten bleibt und den Studenten wissenschaftliche Qualifikationen besser vermittelt werden können.

Universitäten kennzeichnen sich grundsätzlich durch einen breiteren Fächerkanon. Dies Merkmal betrifft vor allem die sogenannten „Massenuniversitäten“. Ziel ist, unter dem Dach einer institutionellen Einheit (unitas) die Vielfalt (diversitas) anzubieten. Typisch sind die klassischen, schon im Mittelalter eingeführten Fakultäten für Philosophie (Geisteswissenschaften, heute auch die philologischen und historischen Fächer), Medizin, Theologie und Rechtswissenschaften. Dazu kommen die Naturwissenschaften – die bis in die Renaissance als ein Teilgebiet der Philosophie gelehrt wurden, ebenso wie die Mathematik – sowie die Wirtschafts- und Sozialwissenschaften und weitere Arbeitsgebiete.

Einige Universitäten haben thematische Schwerpunkte wie Technik und stellen dies auch im Namen dar (Beispiel: RWTH Aachen). Einige Hochschulen wie die statusmäßig gleichgestellten Pädagogischen Hochschulen führen die Bezeichnung „university“ zur besseren Identifizierung im Untertitel, vor allem bei der internationalen Korrespondenz. Die früheren Hochschulen für Agrarwissenschaft und Forstwissenschaft wurden in der Regel mit klassischen Universitäten zusammengelegt, so dass diese Ingenieurstudiengänge heute an Universitäten angeboten werden.

Kunsthochschulen sind künstlerische und künstlerisch-wissenschaftliche Hochschulen, die den Universitäten gleichgestellt sind. Darunter fallen neben den Kunsthochschulen im engeren Sinn, deren Fachbereiche die bildende Kunst, die visuelle Kommunikation und die Architektur umfassen, auch die Musikhochschulen, Hochschulen für Schauspielkunst und Filmhochschulen.

Teilweise werden auch Sporthochschulen wie die Deutsche Sporthochschule Köln als „Sportuniversitäten“ bezeichnet.

Das Konzept der Gesamthochschule, das an mehreren Studienorten in Nordrhein-Westfalen und in Kassel (Hessen) umgesetzt wurde, sah eine Integration der Fachhochschul- und Universitäts-Studiengänge vor. Letzte Immatrikulationen waren zum Wintersemester 2005/2006 in Nordrhein-Westfalen möglich. Im Hochschulgesetz von Hessen ist als Zugangsvoraussetzung für die Bachelor- und Masterstudiengänge an Universitäten eine Immatrikulation mit Abitur oder Fachhochschulreife vorgesehen.

Eine Besonderheit ist das Prinzip der Fernuniversität, die ein Studium mit Hilfe von schriftlich an den Wohnort der Studenten zugestelltem Unterrichtsmaterial anbietet (im Gegensatz zur Präsenzuniversität). Dieses Angebot wird meist von Studenten genutzt, die bereits ein Studium absolviert haben, die Familie oder Kinder haben oder schon im Berufsleben stehen. Auch Strafgefangene können an der Fernuniversität studieren.

Es gibt in Deutschland zwei Universitäten der Bundeswehr (UniBw), eine in München und eine in Hamburg. Der Großteil aller Offizieranwärter der Bundeswehr studiert an einer dieser Universitäten, die nahezu die gesamte Bandbreite der bei der Bundeswehr benötigten Studienrichtungen abdecken können. Das sind vor allem technische, aber auch wirtschafts- und organisationswissenschaftliche Fächer sowie Pädagogik. An der Universität der Bundeswehr in München können auch Fachhochschulabschlüsse erworben werden. Die Offizieranwärter des Sanitätsdiensts, die Medizin, Zahnmedizin, Tiermedizin oder Pharmazie studieren, besuchen reguläre zivile Universitäten. (siehe auch Geschichte der ehemaligen Offiziershochschulen der DDR)

Für Deutschland neu ist auch das Konzept der Stiftungsuniversität, das bis 2005 an drei Universitäten in Niedersachsen (Göttingen, Lüneburg, Hildesheim) zumindest teilweise umgesetzt wurde. Grundgedanke ist dabei, der Universität ein Stiftungskapital zur Verfügung zu stellen, aus dessen Erträgen sich die Universität finanziert. Dies soll die Universitäten von staatlichen Zwängen befreien und flexibler in ihren Entscheidungen machen. Traditionell existiert dieses Modell bereits in den Vereinigten Staaten von Amerika. Die bekanntesten Universitäten verfügen dort über ein sehr großes Stiftungskapital, das vor allem aus eigenen Wirtschaftserträgen und Erbschaften sowie privaten Schenkungen resultiert.

Zunehmend werden auch in Deutschland Privatuniversitäten gegründet. Kleinere Stiftungs- und Privatuniversitäten, wie sie traditionell im angloamerikanischen Raum existieren, haben gelegentlich mit dem Problem zu kämpfen, finanziell in zu starke Abhängigkeit von einem bestimmten Sponsor zu geraten. Zudem bilden die Studiengebühren eine weitere Finanzierungsquelle in erheblicher Höhe, was zu einer finanziellen Auslese unter den Studieninteressierten führen kann.

Bürgeruniversitäten und Kinderuniversitäten sind zeitlich begrenzte Veranstaltungen, die der Öffentlichkeitsarbeit einer Universität zuzurechnen sind. Sie sollen den Universitätsbetrieb für Kinder beziehungsweise Nicht-Akademiker transparent machen und für die Anliegen der Universitäten werben.

Die German University in Cairo (GUC) in Kairo/Ägypten ist das zurzeit weltweit größte von Deutschland unterstützte Projekt im Bildungsbereich. Zu Auslandsaktivitäten Deutschlands auf diesem Gebiet siehe auch Chinesisch-Deutsches Hochschulkolleg.

Historisch werden auch mittelalterliche Bildungseinrichtungen in außereuropäischen Ländern (in Afrika und Asien, dabei vor allem im islamischen Raum) als Universitäten bezeichnet, die nicht alle Merkmale einer europäischen Universität erfüllen (siehe auch Madrasa). Dabei ist vor allem die Verleihung akademischer Grade als speziell europäische Erfindung zu betrachten.

Bildungseinrichtungen der Antike, so zum Beispiel im antiken Ägypten und Griechenland oder im Römischen Reich, werden in der Regel nicht als Universitäten bezeichnet, obwohl entsprechende Begriffe auch damals üblich waren.

Die aus dem christlichen Bildungswesen und -gedanken des mittelalterlichen Westeuropas entstandene Universität gilt als eine klassisch europäische Schöpfung. Die Ursprünge liegen in den Kloster- und Domschulen, welche bis ins 6. Jahrhundert zurückreichen. Im Laufe der Zeit haben sich sowohl die Struktur, als auch die Fachbereiche der Universitäten erweitert und verändert. Der Grundgedanke der Bildung blieb jedoch erhalten. Infolge des anhaltenden wirtschaftlichen Nachkriegsaufschwungs und der Bildungsreformen wurden ab den 1960er und 1970er Jahren in Deutschland zahlreiche neue Universitäten gegründet, größtenteils durch Ausbau der vorhandenen Pädagogischen Hochschulen.

Nach dem Grundgesetz ist die Hochschulgesetzgebung grundsätzlich Sache der Länder. Dies entspricht, wenn man von der zentralistischen Zeit des Dritten Reichs oder der DDR absieht, auch der historischen Entwicklung in Deutschland. Fast alle alten Universitäten wurden von den Landesfürsten errichtet, die dazu allerdings ein Kaiserliches Privileg benötigten. Aus Gründen der Hochschulfinanzierung kam es jedoch auch zu rahmengesetzlichen Regelungen durch den Bund mit dem Hochschulrahmengesetz. Aufgrund der Föderalismusreform wird die Aufhebung des Hochschulrahmengesetzes angestrebt. Ansonsten müssen sich die Länder untereinander staatsvertraglich über gemeinsam gewollte oder nicht gewollte Sachverhalte verständigen, was in der Regel im Rahmen der Kultusministerkonferenz stattfindet. Auch dies hat historische Dimension: bereits 1654 trafen die evangelischen Reichsstände auf dem Reichstag zu Regensburg ein erstes Abkommen zur Eindämmung des damals ausufernden Pennalismus an den Universitäten. Das Grundgesetz wurde dahingehend geändert, dass Bund und Länder bei bestimmten Aufgaben zusammenarbeiten können.

In Deutschland sind die meisten Universitäten heute als rechtsfähige öffentlich-rechtliche Körperschaften organisiert und unterstehen der Aufsicht der Bundesländer. Zuständig ist das entsprechende Ministerium (beziehungsweise – in Stadtstaaten – der Senator) für Wissenschaft. Gesetzliche Grundlage für die Universitäten und die anderen Hochschulen eines Bundeslandes ist das Landeshochschulgesetz.

In der Schweiz sind die Kantone Träger der Universitäten und Hochschulen. Einzige Ausnahmen sind die Eidgenössische Technische Hochschule Zürich und die École polytechnique fédérale de Lausanne, die von der Schweizer Bundesregierung getragen werden.

Die Lernenden an einer Universität bezeichnet man als Studenten oder (in Hinblick auf die Gleichstellung der Geschlechter) als Studierende. Die verschiedenen Arten von Lehrenden werden unter dem Oberbegriff Dozenten (oder Dozierende) zusammengefasst. Lehre und Forschung werden an einer Universität von den Professoren des entsprechenden Faches eigenverantwortlich geleitet.

An der Spitze einer Universität steht ein Rektor oder Präsident, der in der Regel selbst ein Universitätsprofessor ist. Er wird üblicherweise unterstützt von mehreren Prorektoren beziehungsweise Vizepräsidenten, mit besonderen Zuständigkeiten wie für Lehre oder Forschung. Die traditionellen Anreden Magnifizenz für den Rektor bzw. Spektabilitäten für die Prorektoren und Dekane sind heute nicht mehr üblich. Der Leiter der Verwaltung wird in der Regel Kanzler genannt. Ein Kanzler einer Universität ist in der Regel ein Jurist oder ein Verwaltungsfachmann. Als wichtigstes Entscheidungsgremium fungiert der Senat, in dem Professoren, wissenschaftliche und nichtwissenschaftliche Mitarbeiter sowie teilweise auch Studenten ihren Sitz haben.

Für die Vertretung von Hochschulen gegenüber Politik und Öffentlichkeit gibt es auf Bundesebene die Hochschulrektorenkonferenz (HRK), für die Zusammenarbeit der Hochschulen auf Landesebene die Landesrektorenkonferenz (LRK). Dort wird die Universität vom Rektor oder Präsidenten vertreten.

Zum Aufgabenbereich der Universitätsverwaltung gehören Angelegenheiten von Forschung, Lehre und Studium, von Haushalt, Personal und Recht, aber auch das Gebäudemanagement sowie der Arbeits- und Umweltschutz.

Ein Beispiel ist das Studentensekretariat, das in einer Universität für die Verwaltung der Studenten zuständig ist. Hier immatrikulieren und exmatrikulieren sich die Studenten. Aufgrund der hier geführten Unterlagen ist das Sekretariat auch in der Lage, Studienbescheinigungen für die unterschiedlichsten Zwecke auszustellen.

Das Akademische Auslandsamt (AAA) ist der Ansprechpartner in allen Fragen bezüglich eines Studienaufenthalts im Ausland, diesbezüglicher Stipendien und der Anerkennung von Leistungsnachweisen. Akademische Auslandsämter prüfen auch für die jeweilige Hochschule die Hochschulzugangsberechtigung von internationalen Studenten und beraten sie bei ihrem Studium in Deutschland.

Universitäten gliedern sich in einzelne Fakultäten oder Fachbereiche, die von einem Dekan (traditionelle Anrede: Spektabilität) oder Fachbereichssprecher geleitet werden (siehe z. B. auch Medizinische Fakultät, Theologische Fakultät). Die Position des Dekans bzw. Sprechers wechselt meist zwischen den Professoren der Fakultät (siehe auch Fakultätsentwicklung). Fakultäten haben ein eigenes Siegelrecht und das Recht, akademische Prüfungen abzunehmen sowie daraufhin die entsprechenden akademischen Grade zu verleihen. Die Eigenständigkeit der Fakultäten geht bis auf das Mittelalter zurück, als die Universitäten aus eigenständigen Einheiten zusammenwuchsen.

Die Fakultäten können sich wiederum in Institute oder Seminare gliedern, die einzelne Fachgebiete in Lehre und Forschung vertreten. Sie werden von einem der dort lehrenden Professoren (zum Beispiel mit dem Titel Institutsdirektor) geleitet.

Die Forschung wird unterteilt in Grundlagenforschung und angewandte Forschung. Forschung wird gefördert und finanziert durch entsprechende Forschungsprogramme und -aufträge seitens des Bundeslandes, der DFG, sowie anderer Vereine und Stiftungen. Forschung findet aber auch im Auftrag von Unternehmen und anderen öffentlichen Einrichtungen statt. Vor allem Institute können durch angewandte Forschung (Drittmittelforschung) zur Finanzierung des Universitätsbetriebes beitragen und zusätzliche Möglichkeiten für die Studenten bieten. Auf der anderen Seite können Unternehmen durch die projektbezogene Vergabe von Forschungsaufträgen bei der praktischen Umsetzung unterstützt werden und somit davon profitieren. Aufgrund der Finanzierungsmöglichkeiten haben die Institute manchmal einen eigenen rechtlichen Status (siehe An-Institut).

Zu jeder Universität gehören auch zentrale, fakultätsübergreifende Einrichtungen.

Wichtig für die wissenschaftliche Arbeit sind die Universitätsbibliotheken, die für die Sammlung und Bereithaltung der erforderlichen wissenschaftlichen Literatur zuständig sind. Dabei werden nicht nur Bücher (Monografien) beschafft, sondern auch wissenschaftliche Zeitschriften und Buchreihen abonniert (siehe auch Fachzeitschrift).

Das Universitätsrechenzentrum ist eine zentrale Einrichtung, die informationstechnische (IT) Infrastruktur (Hochschulnetz, Server etc.) bereitstellt und betreibt sowie IT-Dienstleistungen (E-Mail, Web-Services etc.) und Beratung erbringt. Auch Rechenzentren versorgen manchmal mehrere Hochschulen mit IT-Infrastrukturen.

Aufgrund der zunehmenden Nutzung von Online-Medien in Lehre und Forschung gewinnen diese beiden zentralen Einrichtungen weiter an Bedeutung. Sie kooperieren in überlappenden Aufgabenbereichen.

Das Sportzentrum einer Universität ist in der Regel nicht nur für die Forschung und Lehre im Bereich der Sportwissenschaften zuständig, sondern bietet darüber hinaus für Studenten aller Fakultäten Trainingsmöglichkeiten in den verschiedensten Disziplinen im Rahmen des Universitätssports an. An einigen Universitäten gibt es Universitäts-Sportclubs (USC).

Alle Universitäten mit medizinischer Fakultät haben ein Universitätsklinikum, was einen größeren Posten im Etat der jeweiligen Universität darstellt. Die Chefärzte der einzelnen Fachkliniken sind in der Regel Universitätsprofessoren.

Weitere Einrichtungen können zum Beispiel wissenschaftliche Zentren, Sonderforschungsbereiche, An-Institute, Laboratorien, Observatorien, Museen, Sammlungen oder botanische Gärten sein, die von einzelnen Fakultäten oder fakultätsübergreifend unterhalten werden.

Das Studentenwerk kümmert sich um die sozialen Belange der Studenten. So sorgen Studentenwerke für einen regelmäßigen preiswerten Mittagstisch, die so genannte Mensa (lat. für „Tisch“), betreiben Studentenwohnheime oder bieten Beratungen für Studierende an. In der Regel gibt es an einem Hochschulstandort "ein" Studentenwerk, das sich um die Studenten aller Universitäten und Hochschulen der Stadt (oder Region) kümmert.

An zahlreichen Orten gibt es neben der Universität auch eigenständige Forschungseinrichtungen, wie beispielsweise Max-Planck-Institute.

Das Studium beginnt für den Studenten mit der Immatrikulation und endet mit der Exmatrikulation. Das Studienjahr ist in Deutschland in aller Regel in zwei Semester (Winter- und Sommersemester) unterteilt. Dazwischen liegt die Vorlesungsfreie Zeit, in welcher dennoch Arbeiten angefertigt und Prüfungen geschrieben werden, oder Semesterferien, die die Studierenden temporär von allen Verpflichtungen des Studiums befreien. An manchen Universitäten ist die Einteilung des akademischen Jahres in drei Trimester üblich (z. B. Universitäten der Bundeswehr, Bucerius Law School).
Grundsätzliche Voraussetzung für die Immatrikulation ist meistens die allgemeine oder fachgebundene Hochschulreife. Bei einigen Fächern (Medizin, Pharmazie, Tiermedizin und Zahnmedizin) bestehen bundesweite Zulassungsbeschränkungen (Numerus clausus) durch die Stiftung für Hochschulzulassung (SfH), andere Fächer können je nach Universität zulassungsbeschränkt sein. In diesem Fall muss der Studienbewerber eine Bewerbung bei der SfH oder der Hochschule einreichen.

Als wichtigste Lehrveranstaltungen der Universitäten gelten (zumindest theoretisch) die Vorlesungen, in denen ein Dozent mit akademischer Lehrbefugnis (Venia legendi) Lehrstoff aus seinem Fachgebiet, wenn möglich aus seinem Forschungsgebiet, vorträgt. Das können Professoren, aber auch Privatdozenten sein – Voraussetzung ist, dass der Dozent die "venia legendi" besitzt. Die Lehrinhalte werden in so genannten Seminaren oder Übungen praxisnah weiter vertieft. Diese Lehrveranstaltungen werden oft von Assistenten oder anderen Lehrbeauftragten geleitet. Hier ist auch die Mitarbeit der Studenten gefordert. In naturwissenschaftlichen Studiengängen werden beispielsweise Laborarbeiten durchgeführt, in den geisteswissenschaftlichen Fächern beteiligen sich die Studenten mit Referaten.

Nach der Hälfte des Studiums bzw. einem bestimmten Zeitabschnitt wird in der Regel eine Zwischenprüfung abgelegt, die oft eine fakultätsspezifische Bezeichnung trägt. So legen Mediziner nach vier Semestern ihres regulären Studiums ihr Physikum ab, bevor sie mit dem Klinikum (acht weitere Semester) beginnen.

Nach dem Hauptstudium, der zweiten Hälfte der regulären Studienzeit, legt der Student sein Examen ab, das auch wieder fakultäts- und studiengangspezifisch nach dem zu erlangenden akademischen Grad bezeichnet wird („Magisterprüfung“, „Diplomprüfung“, „Staatsexamen“ etc.).

Für die Zulassung zum Examen werden Leistungsnachweise, die so genannten "Scheine", verlangt. Diese werden zumeist nicht in den Vorlesungen, sondern in Übungen und Seminaren erworben. Zum Examen müssen in der Regel schriftliche und mündliche Prüfungen abgelegt sowie oft eine schriftliche Arbeit eingereicht werden, die nachweisen soll, dass der Student in der Lage ist, den Forschungsstand eines Teilbereiches der von ihm studierten Wissenschaft bzw. ein Spezialthema wiederzugeben und sich mit ihm auseinanderzusetzen, idealerweise eine aufgeworfene Fragestellung zu beantworten. Anders als bei der Dissertation wird nicht erwartet, dass der Kandidat einen wissenschaftlichen Fortschritt erzielt.

Bei Prüfungen, die auf den Staatsdienst vorbereiten (Rechtswissenschaften, Lehramt etc.) oder einer besonderen staatlichen Aufsicht unterliegen (Medizin, Pharmazie, Lebensmittelchemie etc.), wird ein Staatsexamen abgelegt.

Theologen werden für die kirchliche Laufbahn durch das kirchliche Examen, dem Äquivalent zum Staatsexamen, qualifiziert.

Nach dem erfolgreichen Examen bekommt der Student einen fakultätsspezifischen akademischen Grad (Diplom, Magister etc.) verliehen, der berufsqualifizierend ist. Das Staatsexamen berechtigt nicht zum Führen eines bestimmten Grades, wird allerdings in aller Regel als Ausgangspunkt für eine Promotion akzeptiert.

Im Rahmen des im Jahre 1999 begonnen Bologna-Prozesses hat sich diese Struktur des akademischen Studiums grundlegend geändert. Ein Großteil der Studiengänge in Deutschland ist bereits sukzessive auf die Erreichung der neuen Master- und Bachelor-Abschlüsse umgestellt worden, um eine europaweite Harmonisierung und Vergleichbarkeit der Abschlüsse zu gewährleisten. Europaübergreifend haben sich 45 Länder diesem Prozess angeschlossen, der in der Praxis vielfach mit enormen Problemen verbunden ist und intern scharfer Kritik ausgesetzt ist, die allerdings von der Öffentlichkeit kaum wahrgenommen wird. Eine Konsequenz des Bologna-Prozesses ist, dass die Hochschulabsolventen immer jünger werden und sich ihre Ausbildungszeiten deutlich verkürzen. Speziell in Deutschland benötigen Studenten für ihr Studium heute im Durchschnitt noch 10,6 Semester, nachdem es im Jahr 2000 noch 12,8 Semester waren. Das Durchschnittsalter der Hochschulabsolventen in Deutschland beträgt nur noch 27,1, nachdem es im Jahr 2000 noch bei 28,2 Jahren lag. Als Vorteil des Bologna-Prozesses gilt, dass die Studenten früher in den Arbeitsmarkt integriert werden. Kritiker bemängeln hingegen, dass die Qualität der Ausbildung unter dem neuen System leide und die akademische Ausbildung zudem allein Wirtschaftsinteressen untergeordnet würden.

Nach dem Examen kann ein Promotionsstudium begonnen werden, nach dessen Abschluss der Doktorand den Doktorgrad erwirbt, was in einigen Fakultäten für die Berufsqualifikation erwartet wird und in jedem Fall als der Nachweis „wissenschaftlicher Befähigung“ gilt. Dies sind vor allem die Geistes- und Naturwissenschaften sowie die Medizin. Der „Doktor“ ist der höchste akademische Grad. Die Promotion wird durch die Vorlage einer Dissertation, einer eigenständigen Forschungsarbeit, erlangt sowie durch das Bestehen eines Rigorosums und/oder einer wissenschaftlichen Disputation, in deren Verlauf der Doktorand meist seine Arbeit wissenschaftlich argumentativ verteidigen muss. Art und Ablauf dieses „mündlichen Verfahrens“ sind von Fach zu Fach und von Hochschule zu Hochschule zum Teil sehr unterschiedlich. Nach erfolgreichem Abschluss der letzten Prüfung gilt der Kandidat als promoviert und erhält sein Zeugnis mit der Note. Die Bezeichnung „Dr.“ darf man in Deutschland allerdings erst nach der Publikation der Doktorarbeit führen. Ein im Ausland erworbener Doktorgrad musste bis vor der Bologna-Reform „nostrifiziert“ werden, bevor er auch in Deutschland geführt werden durfte. Dies setzte eine gründliche Überprüfung der Gleichwertigkeit der Anforderungen durch das zuständige Kultusministerium voraus.

Nach der Promotion kann sich der Doktor auf die Habilitation vorbereiten. In der Regel bedeutet dies, dass vor allem eine weitere Qualifikationsschrift, die sogenannte Habilitationsschrift, angefertigt werden muss. Hierbei kann es sich um eine Monographie handeln. Sie kann aber auch aus mehreren Publikationen bestehen (kumulative Habilitation). Während der Erstellung dieser Schrift(en) ist der Habilitand in der Regel in der Position eines „wissenschaftlichen Mitarbeiters“ (nach TV-L 13 oder TVöD 13) beschäftigt. Häufig ist auch eine Anstellung bzw. Beamtung als „akademischer Rat auf Zeit“ (nach A13). Diese Position hat in einigen Bundesländern den „Hochschulassistenten“ (C1) ersetzt, der bundesweit mit der Reform der Dozentenbesoldung abgeschafft wurde.

Mit dem Abschluss der Habilitation wird der Titel eines Privatdozenten vergeben und die "Venia Legendi" verliehen. Dies ist die Erlaubnis, an einer Hochschule Vorlesungen zu halten und eigenständig Prüfungen abzunehmen. Angestrebt wird aber die Position als ordentlicher Professor, die nach einem bestimmten, recht aufwändigen Berufungsverfahren erfolgt. Eine Professorenstelle ist in Deutschland traditionell eine Beamtenposition und mit einer Einstellung in den Staatsdienst auf Lebenszeit verbunden. Mittlerweile ist es insbesondere bei Erstberufungen üblich, die Stelle zunächst nur befristet zu vergeben. Eine Entfristung nach Ablauf des vereinbarten Zeitraums erfolgt durch die zuständige Fakultät nach Feststellung der Bewährung.

Neuerdings gibt es auch die Einrichtung des Juniorprofessors, eine Position, die anstelle der Habilitation für eine Lebenszeitprofessur qualifizieren soll. Dies soll der Harmonisierung der akademischen Laufbahnen in der Welt dienen, da die meisten Länder außerhalb des deutschsprachigen Raums keine Habilitation kennen. Die Juniorprofessur wird aber kritisiert, da die Reform das entscheidende Problem – die mit dem Einschlagen einer akademischen Laufbahn verbundene berufliche Unsicherheit – nicht behebt: Auch der Juniorprofessor ist nur befristet beschäftigt und muss versuchen, nach spätestens sechs Jahren eine feste Anstellung zu erlangen. Daher streben inzwischen viele Juniorprofessoren auch die Habilitation an, um ihre Chancen auf eine Dauerstelle zu erhöhen.

In manchen künstlerisch orientierten Fachbereichen (zum Beispiel Kunst, Design, Architektur) wird eine Habilitation traditionell nicht als zwingende Voraussetzung für eine Professorenstelle betrachtet. Teilweise ist nicht einmal eine Promotion notwendig. Hier kann auch derjenige Lehrstuhlinhaber werden, der anstatt einer Promotion so genannte promotionsgleiche Leistungen nachweist. Hierzu zählt auch eine qualitativ hochwertige umfangreiche Publikationsliste. In den Ingenieurswissenschaften ist nach der Promotion Industrieerfahrung anstelle der Habilitation üblich.

Das Einschlagen der akademischen Laufbahn ist in Deutschland mit sehr hohen Risiken verbunden. Nach der Promotion – je nach Fach meist zwischen dem 26. und 33. Lebensjahr – muss man in der Regel weitere fünf oder sechs Jahre bis zur Habilitation einplanen. Da man nach der Reform des Hochschulrahmengesetzes faktisch nur noch zwölf Jahre lang befristet an einer Hochschule beschäftigt sein kann, bedeutet dies, dass man mit Anfang vierzig entweder eine feste Anstellung (also in der Regel eine Professur) hat – oder sich nun eine andere Anstellung – in der Regel in der Privatwirtschaft – suchen muss. Während es früher durchaus üblich war, dass ein weder lehrender noch forschender „Kustos“ eine Assistentenstelle über Jahrzehnte belegte, leidet heute fast der gesamte „akademische Mittelbau“ in Deutschland unter einem enormen Konkurrenzdruck und einer erheblichen Existenzangst – ein Umstand, der kaum einem Studenten bewusst ist: Nur wenigen ist bekannt, dass eine Vielzahl der Dozenten (und sogar manch ein Professor) nur mit einer befristeten Stelle ausgestattet ist.

Dieser Konkurrenzdruck resultiert zumeist daraus, dass der wissenschaftliche Arbeitsmarkt spezifischen Arbeitsmarktkonjunkturen unterliegt und somit in enger Wechselwirkung mit gesellschaftlichen Rahmenbedingungen steht. Aktuell von Bedeutung sind dabei vor allem Kürzungen staatlicher Gelder, die Konstruktion eines Bedeutungsverlusts in bestimmten Fächern (etwa den Sozialwissenschaften), die in den letzten Jahrzehnten steigenden Zahlen von Habilitationen und die mit den hochschulpolitischen Entwicklungen der letzten Jahre einhergehenden thematischen Fokussierungen in der Lehre und Forschung.

Universitäten sind Körperschaften, jedoch wegen ihrer anerkannt gemeinnützigen Funktion grundsätzlich von der Körperschaftsteuer befreit. Soweit Universitäten allerdings von Dritten Gelder erhalten um Forschungstätigkeiten in deren Auftrag zu erledigen, ist der Charakter der Gemeinnützigkeit partiell durchbrochen, sofern die Forschungsergebnisse nur dem Auftraggeber zugänglich gemacht werden. Die Ergebnisse dienen damit nicht mehr unmittelbar dem Allgemeinwohl. Eventuelle Gewinne, die hierdurch erwirtschaftet werden, sind körperschaftsteuerpflichtig. Gewerbesteuerpflicht besteht gemäß § 3 Nr. 30 GewStG zwar nicht; die Leistung ist, gemäß dem Umsatzsteuergesetz, jedoch mit dem vollen Umsatzsteuersatz zu versteuern.

Das Hochschulrahmengesetz (HRG) des Bundes schloss seit 2002 allgemeine Studiengebühren in Deutschland aus. Das Bundesverfassungsgericht gab der Klage einiger unionsgeführten Bundesländer, die darin einen unzulässigen Eingriff des Bundes in die Gesetzgebungskompetenz der Länder im Kultusbereich sahen, am 26. Januar 2005 recht.
Im Zuge dessen begannen 2006 auch die staatlichen Universitäten in manchen Bundesländern mit der Einführung von Studiengebühren. Die Höhe belief sich dabei meist auf etwa 500 Euro pro Semester. Das Thema Studiengebühren ist heftig umstritten und war Gegenstand von Studentenprotesten, sodass allgemeine Studiengebühren bundesweit von 2008 (Hessen) bis 2014 (Niedersachsen) wieder abgeschafft wurden.

Der Begriff "Universität" ist in den meisten deutschen Ländern geschützt und ausschließlich Hochschulen mit einem umfassenden Fächerkanon vorbehalten. Nur Universitäten ist es außerdem erlaubt, die Doktor- oder Professoren-Würde zu verleihen („Promotionsrecht“). Der Titel "Universität" wird dementsprechend vom zuständigen Staatsministerium verliehen und orientiert sich an strengen Akkreditierungsrichtlinien, die zumeist nur staatliche Hochschulen erfüllen. Jedoch gibt es in Deutschland auch einige private Hochschulen, die den Titel "Universität" tragen. Dazu gehören die EBS Universität für Wirtschaft und Recht, die Universität Witten/Herdecke, die WHU – Otto Beisheim School of Management, die Deutsche Universität für Weiterbildung in Berlin, die Jacobs University Bremen sowie die Zeppelin Universität in Friedrichshafen. Die meisten anderen privaten Bildungsinstitutionen tragen jedoch lediglich den Titel "Fachhochschule", "Kunst"- und "Musikhochschule" oder "Private Hochschule".

"Siehe auch: Liste der ältesten Universitäten"






</doc>
<doc id="12027" url="https://de.wikipedia.org/wiki?curid=12027" title="Leiden">
Leiden

Leiden steht für:

Personen:
"Siehe auch:"



</doc>
<doc id="12030" url="https://de.wikipedia.org/wiki?curid=12030" title="Atropin">
Atropin

Atropin (abgeleitet von "Atropos", griechische Schicksalsgöttin) ist ein sehr giftiges Tropan-Alkaloid; es ist ein Racemat (1:1-Mischung) aus den Isomeren ("R")- und ("S")-Hyoscyamin, das sich bei der Isolierung durch Racemisierung aus dem Naturstoff ("S")-Hyoscyamin bildet.

("S")-Hyoscyamin kommt in Nachtschattengewächsen wie Alraunen ("Mandragora"), Engelstrompete ("Brugmansia"), Stechapfel ("Datura stramonium") und Tollkirschen ("Atropa") vor. Seinen Namen verdankt das Alkaloid der Schwarzen Tollkirsche ("Atropa belladonna").

Als Entdecker des Atropins gilt der Heidelberger Pharmazeut Philipp Lorenz Geiger (1785–1836). Die Wirkung von Atropin wurden unter anderem von Friedlieb Ferdinand Runge (1795–1867) studiert. Im Jahr 1831 stellte der deutsche Pharmazeut Heinrich F. G. Mein (1799–1864) Atropin in kristalliner Form dar. Ab 1833 produzierte dann die Darmstädter Firma Merck das Atropin aus der Wurzel der Schwarzen Tollkirsche. Die erste Synthese gelang dem Chemiker Richard Willstätter im Jahr 1901.

Atropin ist die racemisierte Form des natürlich vorkommenden ("S")-Hyoscyamins. Die Racemisierung findet bereits bei der Isolierung statt, wenn Laugen zum Einsatz kommen; hierbei bildet sich intermediär ein Enolat. Durch eine Aufarbeitung unter neutralen Bedingungen (pH-Wert 7) kann die Racemisierung von ("S")-Hyoscyamin unterdrückt werden. Der Bedeutung der Enantiomerenreinheit von Arzneistoffen wird zunehmend Beachtung geschenkt, denn die beiden Enantiomere eines chiralen Arzneistoffes zeigen fast immer eine unterschiedliche Pharmakodynamik und Pharmakokinetik. Dies wurde früher aus Unkenntnis über stereochemische Zusammenhänge oft ignoriert. Arzneimittel enthalten den Arzneistoff Atropin als Racemat (1:1-Gemisch der Enantiomere), wobei aus grundsätzlichen Überlegungen die Verwendung des besser bzw. nebenwirkungsärmer wirksamen Enantiomers zu bevorzugen wäre.

Das ("S")-Hyoscyamin ist ein Ester der Tropasäure mit α-Tropin und zählt somit zu den Tropan-Alkaloiden. Allein das 1:1-Gemisch von ("R")- und ("S")-Hyoscyamin wird Atropin genannt (vgl. Cahn-Ingold-Prelog-Konvention zur Benennung). Ein dem Hyoscyamin strukturell nah verwandtes Alkaloid ist das Scopolamin ("Hyoscin").

Für die Verwendung als Edukt in chemischen Synthesen ist es vorteilhaft, von reinem natürlichem ("S")-Hyoscyamin oder ("R")-Hyoscyamin auszugehen.

Atropin gehört zu den Parasympatholytika (auch Anticholinergika genannt). Atropin wirkt demnach antagonistisch und konkurriert somit an den muskarinischen Rezeptoren des Parasympathikus mit dem Neurotransmitter Acetylcholin. Atropin blockiert teilweise die Rezeptoren und hemmt somit den Parasympathikus. Die Wirkung des Acetylcholins sinkt. Der Einfluss des Parasympathikus sinkt, wodurch der Einfluss des Sympathikus überwiegt.

Atropin hat folgende körperliche Wirkungen:

Atropin wurde in der kardio-pulmonalen Reanimation bei Asystolie und pulsloser elektrischer Aktivität (PEA) eingesetzt, Dosen von 0,5 bis maximal 3 mg wurden intravenös verabreicht. Wegen mangelnder Evidenz ist die Gabe von Atropin bei einer Reanimation nach den Richtlinien des European Resuscitation Council nicht mehr empfohlen. Eine zu niedrige Dosierung kann paradoxerweise zu einer schweren Bradykardie führen und sollte entsprechend vermieden werden (mindestens 0,02 mg/kg Körpergewicht).

Atropin wird in der Anästhesie, Intensiv- und Notfallmedizin bei der symptomatischen Behandlung einer zu niedrigen Herzfrequenz (Bradykardie) verwendet. Bei fehlender Effektivität, etwa bei höhergradigen AV-Blöcken, ist die Anwendung von Katecholaminen (Adrenalin) und einer Schrittmachertherapie notwendig.

Atropin wird in der Augenheilkunde zur diagnostischen und therapeutischen Akkommodationslähmung eingesetzt. Als Mydriatikum wird Atropin aufgrund seiner langen Wirkdauer zur therapeutischen, jedoch nicht zur kurzzeitigen diagnostischen Erweiterung der Pupillen verwendet.

Atropin hemmt die muscarinartigen Wirkungen des Acetylcholins durch kompetitive Inhibition der Acetylcholinrezeptoren an der postsynaptischen Membran und unterbricht die Signalübertragung in der Nervenleitung. In sehr hohen Dosen hemmt Atropin vermutlich auch einige Subtypen des nikotinischen Acetylcholinrezeptors.

Auf Grund dieses Wirkungsmechanismus wird Atropin als Gegengift (Antidot) bei Vergiftungen mit bestimmten Pflanzenschutzmitteln (Insektiziden) und Nervenkampfstoffen eingesetzt, deren Giftwirkung auf einer irreversiblen Hemmung der Acetylcholinesterase beruht (z. B. organische Phosphorsäureester und Phosphonsäureester wie Parathion, Tabun oder Paraoxon). Patienten (z. B. mit Sarin kontaminierte Soldaten) werden per Autoinjektor Gaben von 2 mg Atropinsulfat bzw. 2 mg Atropinsulfat plus 220 mg Obidoximchlorid verabreicht.
Atropin hemmt vor allem die M-, M- und M-Rezeptoren und verursacht so eine Steigerung der Herzfrequenz (M), eine Reduktion der Magensäureproduktion (M) sowie eine Speichelreduktion (M). Zusammen mit einer dezenten Bronchodilatation (M) sind diese Wirkungen auch von Vorteil für eine Narkoseeinleitung. Ein genereller Einsatz in der Prämedikation (medikamentöse Vorbereitung) von Narkosepatienten wird jedoch heute nicht mehr empfohlen, da das Nutzen-Nebenwirkungs-Verhältnis von Atropin-Sulfat schlecht ist.

Atropin vermindert die Speichel- und Schleimsekretion, was bei Operationen im Mund und Rachenbereich sowie bei fiberoptischen Intubationen und Bronchoskopien genutzt werden kann.

Seltener findet Atropin Anwendung bei Krämpfen der glatten Muskulatur im Bereich des Magen-Darm-Trakts. Auch kann Atropin bei erschwerter Blasenentleerung, bei Harninkontinenz und zur Behandlung einer Reizblase gegeben werden. Selten wurde Atropin in der Frauenheilkunde bei Dysmenorrhoe (schmerzhafte Regelblutung) eingesetzt. Den gleichen Effekt erzielt man heute mit Butylscopolamin, einem chemisch weiterentwickelten Derivat des Scopolamins, das entspannend auf die verkrampfte glatte Muskulatur wirkt und aufgrund der geringeren Nebenwirkungen rezeptfrei erhältlich ist. Als Asthmamittel wird Atropin nicht mehr verwendet, stattdessen werden besser verträgliche Arzneistoffe eingesetzt. Der Atropintest kann zur kardiologischen Diagnostik und als Hilfestellung bei der Feststellung des Hirntodes verwendet werden.

Außerdem wird Atropin auch gegen übermäßiges Schwitzen (Hyperhidrose) eingesetzt (Off-Label-Use).

Die Wirkungen auf Herz und Kreislauf stehen schon bei geringen Dosen im Vordergrund (z. B. zur Narkoseeinleitung). Psychische („berauschende“) Wirkungen sind erst bei hohen Dosen zu erwarten, bei denen unangenehme und gefährliche körperliche Nebenwirkungen auftreten.

Als Vergiftungssymptome wird bei hohen Dosen (siehe anticholinerges Syndrom) von Rötungen der Haut, Mydriasis, Herzrasen und Verwirrtheit wie Halluzinationen berichtet. Bei noch höheren Dosen tritt Bewusstlosigkeit ein, die von Atemlähmung gefolgt sein kann; bei einer Atemlähmung sind die Vergiftungen in der Regel tödlich. Die LD50 (oral) beträgt für den Menschen 453 mg. Ab 10 mg treten Delirien und Halluzinationen auf. Ab 100 mg kann eine tödliche Atemlähmung einsetzen. Insbesondere Kinder sind schon bei geringeren Dosen ab 10 mg in Gefahr.

Neben Vergiftungen durch freiwilligen oder unfreiwilligen Verzehr von Pflanzenteilen (zum Beispiel Tollkirsche) kommen medizinale Vergiftungen infolge Überdosierung, Verwechslung oder falscher Anwendung vor. Z. B. hat die Food and Drug Administration (FDA) im Jahre 2016 Globuli im Zusammenhang mit zehn Todesfällen in den USA von Kleinkindern untersucht, die nach der Verabreichung dieses Mittels, das Atropin enthielt, verstorben waren. Das Atropin war offenbar in zu hoher Konzentration in den Globuli enthalten. Die Behörde wies die Hersteller an, die Tabletten zurückzurufen.

Die Erste Hilfe bei Atropinvergiftung besteht in sofortiger Entleerung des Magen-Darm-Traktes (Erbrechen, Magenspülung) sowie erforderlichenfalls künstlicher Beatmung bzw. Atemspende. Die erweiterten Maßnahmen zielen auf die medikamentöse Hemmung der Acetylcholinesterase, durch Physostigmin als Antidot, wodurch der Abbau des Acetylcholins verzögert wird. Folglich erhöht sich die Konzentration im synaptischen Spalt. Am Rezeptor selbst wird somit indirekt eine parasympathische Wirkung erzielt. Das Atropin wird aus dem Bereich der Rezeptoren verdrängt und die Reizleitung ist wiederhergestellt.

Große Pupillen galten während der Renaissance unter europäischen Frauen als schön („bella donna“). Das Einträufeln der ("S")-Hyoscyamin enthaltenden Tollkirschen-Extrakte in die Augen bewirkte eine bis zu mehreren Tagen anhaltende Pupillenerweiterung („feuriger Blick“).

Bellafit (CH), Dysurgal (D), Generika (D, A, CH)




</doc>
<doc id="12033" url="https://de.wikipedia.org/wiki?curid=12033" title="Friederike Brion">
Friederike Brion

Friederike Elisabeth Brion (* vermutlich 19. April 1752 in Niederrödern im Elsass; † 3. April 1813 in Meißenheim bei Lahr) war eine elsässische Pfarrerstochter und hatte eine kurze, aber heftige Liebschaft mit dem jungen Goethe.

Friederikes Geburtsdatum ist nicht gesichert, da die Kirchenbücher in den Wirren der Französischen Revolution vernichtet wurden. Sie war das dritte von fünf überlebenden Kindern von Johann Jakob und Maria Magdalena Brion geb. Schöll. Zu Martini 1760 nahm ihr Vater eine Stelle als Dorfpfarrer in Sessenheim, von Goethe „Sesenheim“ geschrieben, an. Dort wuchs das hübsche, lebensfrohe, aber etwas kränkliche Mädchen heran.

Unter den jungen Leuten, die das gastfreundliche Pfarrhaus gelegentlich besuchten, war auch der Straßburger Rechtsstudent Johann Wolfgang Goethe aus Frankfurt. Im Herbst 1770 kam dieser zusammen mit seinem elsässischen Freund Friedrich Leopold Weyland beim Durchstreifen der Umgebung von Straßburg zum ersten Mal in das kleine, 40 Kilometer nordöstlich von Straßburg gelegene Dörfchen Sessenheim. Dieser Ausflug sollte eine der bekanntesten Liebesepisoden der Literaturgeschichte zur Folge haben. Goethe hatte sich zum Studium nach Straßburg begeben, um dort – in Fortsetzung seiner Studien in Leipzig – den juristischen Doktorgrad zu erwerben.

Goethe berichtete später von seiner ersten Begegnung mit Friederike: „In diesem Augenblick trat sie wirklich in die Türe; und da ging fürwahr an diesem ländlichen Himmel ein allerliebster Stern auf. […] Schlank und leicht, als wenn sie nichts an sich zu tragen hätte, schritt sie, und beinahe schien für die gewaltigen blonden Zöpfe des niedlichen Köpfchens der Hals zu zart. Aus heiteren blauen Augen blickte sie sehr deutlich umher, und das artige Stumpfnäschen forschte so frei in die Luft, als wenn es in der Welt keine Sorge geben könnte; der Strohhut hing am Arm, und so hatte ich das Vergnügen, sie beim ersten Blick auf einmal in ihrer ganzen Anmut und Lieblichkeit zu sehn und zu erkennen.“

In den nächsten Monaten machte Goethe noch viele „folles chevauchées“ (tolle Ausritte) nach Sessenheim, denen auch ausgedehnte Aufenthalte im Hause Brion folgten. Unbeobachtet durchstreiften er und Friederike die Umgebung, unternahmen Kahnfahrten in den damals noch weitläufigeren Rheinauen und besuchten Bekannte Friederikes. Für das nächste Jahr wurde der kleine Ort für Goethe der „Mittelpunkt der Erde“.

Durch dieses grenzenlose Glück „trat unversehens die Lust zu dichten“, die Goethe „lange nicht gefühlt hatte, wieder hervor“. Im Frühjahr 1771 entstand eine Reihe von Gedichten und Liedern, die manchmal mit „bemalten Bändern“ an die Geliebte gesandt wurden; diese „Sesenheimer Lieder“ gehören maßgeblich zum „Sturm und Drang“ und begründeten Goethes Ruf als Lyriker. Unter ihnen sind zum Beispiel das „Mailied“, „Willkommen und Abschied“ und „Das Heidenröslein“. Auch übersetzte er 1771 für sie die „Die Gesänge von Selma“ aus James Macphersons „Ossian“.

Die Liebesbeziehung war jedoch nicht von langer Dauer. Schon im Frühsommer 1771 dachte Goethe, der seine unruhige Seele mit dem „Wetterhähnchen drüben auf dem Kirchturm“ verglich, daran, die Beziehung zu beenden. Am 7. August 1771 sah er Friederike vor seiner Heimkehr nach Frankfurt zum letzten Mal: „Als ich ihr die Hand noch vom Pferde reichte, standen ihr die Tränen in den Augen, und mir war sehr übel zumute.“ Das „herrliche Elsass“ verließ er schon eine Woche später. Erst aus Frankfurt schrieb er Friederike einen Brief, der das Verhältnis endgültig löste. Die Antwort Friederikes „zerriss mir das Herz […] stets empfand ich, dass sie mir fehlte, und was das Schlimmste war, ich konnte mir mein eignes Unglück nicht verzeihen. […] Hier war ich zum erstenmal schuldig; – doch der Abschied war endgültig.“ Er empfand also selbst, dass er sich bei der Lösung nicht gerade wie ein „Gentilhomme“ verhalten habe. Goethe kehrte danach jedoch mindestens noch einmal – 1779 auf einer Reise in die Schweiz – auf den Pfarrhof von Sessenheim zurück. Einige unsichere Quellen erwähnen einen weiteren Besuch 1782 zur Hochzeit von Friederikes älterer Schwester Maria Salomea mit dem aus Straßburg stammenden Magister Gottfried Marx, der gerade Pfarrer in Diersburg geworden war.

Im Sommer 1772 warb der Dichter Jakob Michael Reinhold Lenz, ein Bewunderer Goethes, der mit diesem in Straßburg Kontakt hielt, um die noch an großem Liebeskummer leidende Friederike: „Wo bist du itzt, mein unvergeßlich Mädchen, | Wo singst du itzt? | Wo lacht die Flur? Wo triumfirt das Städtchen | Das dich besitzt?“

Friederike Brion blieb jedoch bis an ihr Lebensende unverheiratet und wohnte noch bis zum Tod ihres Vaters im Jahre 1787 in ihrem Elternhaus; die Mutter war bereits ein Jahr zuvor gestorben. Danach zog Friederike mit ihrer jüngeren Schwester Sofie zu ihrem Bruder Christian auf die Pfarrei Rothau im Steintal. Dort blieben die beiden auch nach dessen Versetzung. Zu ihrem Lebensunterhalt betrieben die Schwestern den Verkauf von Web-, Steingut- und Töpfereiwaren und Handarbeiten und unterhielten einige Zeit eine Pension für Mädchen aus Sessenheim und Umgebung, die in Rothau auf einer dafür errichteten Schule Französisch lernen sollten.
1801 siedelte Friederike zur Unterstützung der kränklichen Schwester ins Pfarrhaus nach Diersburg über und blieb danach mit einigen Unterbrechungen dort. Sie folgte der Familie 1805 auch ins badische Meißenheim. 1807 starb die Schwester. Friederike blieb bei ihrem Schwager. Auch sie war nicht von bester Gesundheit. Zu Beginn des Jahres 1813 musste sie ihre Schwester Sofie darum bitten, sie zu versorgen. Sie starb am 3. April 1813 und wurde am 5. April auf dem Meißenheimer Friedhof bestattet. Der dort heute noch zu sehende Grabstein, angefertigt vom Bildhauer Wilhelm Hornberger, wurde erst 1866 auf der völlig verwahrlosten Grabstätte errichtet. Am 19. August jenes Jahres hielt Friedrich Geßler dort die Weihrede. Die Inschrift lautet, nach einem Vers von Ludwig Eckardt: „Ein Stral der Dichtersone fiel auf sie, so reich, daß er Unsterblichkeit ihr lieh!“

Das schwärmerische Interesse, das nach der Veröffentlichung der Liebesgeschichte im zweiten und dritten Teil von Goethes "Dichtung und Wahrheit" (1812/14) einsetzte, hat Friederike Brion nicht mehr erlebt. Anders dagegen Goethe: In seinem Aufsatz „Wiederholte Spiegelungen“ (1823) ging er auf den ihm zugespielten Bericht von einer Reise ein, die der Bonner Professor August Ferdinand Naeke auf der Suche nach Spuren der Sessenheimer Liebesgeschichte im Jahr zuvor unternommen hatte. Bei einer ähnlichen Reise im Jahr 1835 fand Heinrich Kruse, ein Student Naekes, bei der letzten noch lebenden Schwester Friederikes die vollständige Sammlung von Goethes und Lenzens „Sesenheimer Liedern“.

Im weiteren Verlauf des 19. Jahrhunderts setzte ein regelrechter Friederiken-Kult ein, der diverse künstlerische und wissenschaftliche Werke hervorbrachte, in denen das "Sesenheimer Idyll" verklärt wurde. Empfindlich gestört wurde die Verklärung durch Gerüchte, wonach Friederike nicht die unschuldige Pfarrerstochter geblieben sei, als die Goethe sie geschildert hat, sondern ein oder mehrere uneheliche Kinder zur Welt gebracht habe. Sogar über ein Kind aus der Beziehung mit Goethe wurde spekuliert. Heftiger Widerspruch schlug dem Straßburger Goethe-Forscher Johann Froitzheim entgegen, als er diese Gerüchte in einer Reihe von Veröffentlichungen durch geschichtliche Quellen wissenschaftlich zu untermauern versuchte.

Franz Lehárs Operette "Friederike" von 1928 basiert auf der Liebesbeziehung zwischen ihr und Goethe.




</doc>
<doc id="12035" url="https://de.wikipedia.org/wiki?curid=12035" title="Cargolifter (Unternehmen)">
Cargolifter (Unternehmen)

Die im Jahr 1996 gegründete Cargolifter AG (Eigenschreibweise: "CargoLifter") ist ein seit 2002 insolventes Unternehmen der Luftfahrtindustrie. Hauptziel des Unternehmens war, ein Lastenluftschiff für bis zu 160 Tonnen schwere Fracht zu entwickeln, zu konstruieren und operativ zu betreiben. Nach einer erfolgreichen Phase mehrerer Privatplatzierungen folgten im Mai 2000 ein Börsengang und bereits im Dezember 2000 eine Aufnahme in den MDAX. Durch wiederholt auftauchende Kostensteigerungen innerhalb des technischen Entwicklungsprojekts und mangelnde Investitionsbereitschaft privater Investoren und der Öffentlichen Hand musste Cargolifter im Juni 2002 Insolvenz anmelden und das Luftschiff-Entwicklungsvorhaben beenden.

Die Cargolifter AG war während ihrer Existenz in der Öffentlichkeit sehr bekannt und erhielt während der ersten Jahre ihres Bestehens einen hohen emotionalen Zuspruch aus der Bevölkerung.

Höchste internationale Bekanntheit erreichte das Unternehmen vor allem durch seine im November 2000 fertiggestellte Luftschiffhalle. Diese 360 Meter lange, 210 Meter breite und 107 Meter hohe Halle gilt als größtes freitragendes Gebäude der Welt. Nach dem Verkauf der Werfthalle aus der Insolvenzmasse an einen malaysischen Konzern befindet sich innerhalb des Gebäudes seit dem Jahr 2004 ein tropischer Freizeitpark mit dem Namen Tropical Islands.

Die Ursprünge des Unternehmens CargoLifter reichen bis in das Jahr 1994 zurück. Unter Federführung des VDMA wurde 1994 eine erste Marktanalyse erstellt. Die zehn größten Anlagenbauunternehmen in diesem Verband vergaben damals jährliche Transportaufträge im Wert von über 60 Millionen DM. Bei diesen Schwerlasttransporten wurden zumeist Anlagen und Bauteile mit mehr als 100 t Gewicht und rund 25 Metern Länge transportiert. Da die Unternehmen zudem berichteten, erhebliche Probleme mit Schwerlasttransporten in Länder mit schlecht entwickelter Verkehrsinfrastruktur zu haben, entstand die Geschäftsidee des CargoLifters: Mit einem zu entwickelnden und zu bauenden Lastenluftschiff sollten zukünftig schwere Güter kostengünstiger und schneller transportiert werden.

Nach der ersten Studie wurde 1995 eine zweite Studie durchgeführt, welche die Möglichkeit von Schwerlasttransporten mittels Luftschiff in einen Vergleich zu herkömmlichen Transporttechnologien stellte und deren Nutzen herausarbeitete. Im Februar 1996 begannen kleinere Expertenteams damit, das Konzept des CargoLifters aus technischer, wirtschaftlicher sowie wissenschaftlicher Perspektive näher zu beschreiben. Im Sommer des gleichen Jahres begannen die Vorbereitungen zur Gründung eines Unternehmens, das ein Lastenluftschiff entwickeln und konstruieren sollte.

Das Unternehmen wurde am 1. September 1996 in Wiesbaden gegründet und CargoLifter als Aktiengesellschaft ohne Börsennotierung etabliert. 90 individuelle und institutionelle Aktionäre gehörten zum Gründungskreis. Carl-Heinrich von Gablenz, der in der Vorgründungsphase eine zentrale und treibende Kraft hinter dem Projekt des Lastenluftschiffs darstellte, wurde in der Gründungsversammlung zum alleinigen Vorstand der Gesellschaft berufen. Der Stuttgarter Luft- und Raumfahrtkonstrukteur Bernd Kröplin, der Logistiker und Spediteur Rolf Riedl sowie der damalige Vorstand der ABB Kraftwerke, Heinz Herrmann, wurden als Aufsichtsräte während der Gründungsversammlung gewählt. Der Aufsichtsrat bestimmte Herrmann zudem später als Aufsichtsratsvorsitzenden.

Das Jahr 1997 war zunächst von steigenden Aktienverkäufen gekennzeichnet. Neben einzelnen Individuen zeichneten auch vermehrt Unternehmen Aktien an CargoLifter. Zur ersten ordentlichen Hauptversammlung im November 1997 wurde bekannt, dass zu den mittlerweile rund 600 Aktionären beispielsweise ABB, Siemens, Schenker und Danzas gehörten. Später gesellte sich zum institutionellen Investorenkreis auch Thyssen.

Anfang des Jahres 1998 war der Aktionärskreis auf 1.350 Aktionäre angewachsen, die über 15 Mio. DM Eigenkapital in das Unternehmen gegeben hatten. 60 % der Unternehmensanteile hielten hierbei private Investoren, die großen Anlagenbauer kamen mit jeweils 5 % auf 15 %. Darüber hinaus hielten 20 Großspeditionen zusammen 25 %. Der Preis der nicht-börsennotierten Namensaktien wurde zudem von dem Unternehmen von 27,50 auf 32,50 DM angehoben. In der Folge wurden 2 Millionen Aktien an alte und neue Aktionäre emittiert.

Im Frühjahr des Jahres wurde durch den Aufsichtsrat der Vorstand um eine Person erweitert. Neben Carl von Gablenz wurde nun der Diplomingenieur und Absolvent eines MBA-Programmes, Karl Bangert, zum stellvertretenden Mitglied des Vorstandes berufen. Karl Bangert hatte schon als Mitglied des Expertenteams an der ersten Ausarbeitung der Idee des CargoLifters mitgearbeitet. Seit August 1997 war er auch schon Geschäftsführer der Cargolifter Network GmbH. Diese Geschäftsführertätigkeit nahm er auch weiterhin parallel zu der stellvertretenden Vorstandstätigkeit in Personalunion wahr.<ref name="Geschäftsbericht 1997/1998">"Geschäftsbericht 1997/1998 und Konzernabschluss" (Bericht des Aufsichtsrates, Januar 1998). CargoLifter AG, S. 2.</ref>

Anfang Mai 1998 wurde auf dem Standort Brand (siehe Abschnitt "Luftschiffhangar und dessen Standort") mit einem Fest der erste Spatenstich für die geplante Luftschiffwerft gefeiert. Mit 500 Spaten wurden dabei mit 1000 Spatenstichen die Konturen des geplanten Gebäudes von Besuchern und Aktionären ausgehoben.

Nur wenige Tage später präsentierte sich das Unternehmen dem Fachpublikum im Rahmen der Logistikmesse in Leipzig. Erstmals wurde der Öffentlichkeit hier auch das Luftschiff Joey vorgestellt, das zu dieser Zeit jedoch noch nicht flugtauglich war. Dennoch wurde das Luftschiff zusammengebaut präsentiert, indem es an der Decke der Messehalle befestigt worden war. Gezeigt wurde außer dem Luftschiff auch ein verkleinerter Laderahmen, an dem das geplante Lastaustauschverfahren demonstriert werden sollte. Das einzig fliegende Luftschiff, das CargoLifter auf der Messe Leipzig zeigen konnte, war das Solarluftschiff Lotte 3, das ferngesteuert über dem Außengelände der Messe schwebte.

Mitte des Jahres 1998 war die Zahl der Aktionäre des Unternehmens bereits auf rund 2.800 angewachsen, die dem Unternehmen 50 Mio. DM an Eigenkapital zugeführt hatten.
Weitere Bekanntheit in der Öffentlichkeit erhielt das Unternehmen bei einer Veranstaltung im Juli 1998. Es wurde vom Bundespräsidialamt eingeladen, sich auf dem „Tag der Innovation“ im Garten des Schloss Bellevue zu präsentieren. Neben CargoLifter präsentierten sich an diesem Tag noch weitere innovative deutsche Unternehmen. Initiiert wurde diese Veranstaltung von dem damaligen Bundespräsidenten Roman Herzog, der sich schon 1997 durch seine "Berliner Ruck-Rede" für einen Wandel in Deutschland und mehr Zukunftsdenken ausgesprochen hatte.

Im August 1998 verkündete das Unternehmen, dass der Standort für die geplante Werft von der brandenburgischen Regierung soeben gekauft worden sei. Parallel wurde bekannt gegeben, die Aktionärszahl sei auf über 4000 angewachsen.

Zum Ende des Jahres 1998 hielten 4200 Aktionäre Anteile an dem Unternehmen. Das Eigenkapital war damit auf rund 80 Mio. DM angestiegen.
Im Verlauf des Jahres hatte zudem auch die Linde AG Aktien im Wert von 120.000 DM erworben. Im Gegenzug sicherte CargoLifter zu, zukünftig mindestens 25 % jeder Heliumluftschifffüllung von Linde zu beziehen – bei damaligen Preisen hätte diese Kooperation einen Umsatz von 5 Mio. DM pro CL-160-Luftschiff für Linde bedeutet.

Zum Jahreswechsel Anfang des Jahres 1999 erhielt CargoLifter eine 80-prozentige Ausfallbürgschaft durch den Deutschen Bundestag für die Kreditaufnahme zum Bau seiner Werfthalle.

Zur Hauptversammlung des Unternehmens, die am 10. März in der Jahrhunderthalle in Frankfurt-Höchst abgehalten wurde, beschlossen die Aktionäre einen Wechsel des Unternehmenssitzes von Wiesbaden in die Bundeshauptstadt Berlin. Zudem wurde während der Veranstaltung erfolgreich über die Verdopplung des bisherigen Grundkapitals abgestimmt. Auch genehmigten die Kapitaleigner eine Kapitalerhöhung auf dann 120 Mio. DM.

Mitte März 1999 wurde der Öffentlichkeit im Rahmen einer "Tag der offenen Tür"-Veranstaltung erstmals das "Airship Design Center" präsentiert, innerhalb dessen ab dann Techniker und Ingenieure an der Entwicklung des Lasten-Luftschiffes arbeiten sollten. Dieses neu errichtete Zentrum bestand aus zwei Gebäuden und einer kleinen Halle, innerhalb derer zunächst der größen- und volumenmäßig verkleinerte Demonstrator Joey eines Kielluftschiffs fertig konstruiert werden sollte. Rund 10.000 Besucher folgten dabei der Einladung CargoLifters und besuchten das Areal der zukünftigen Produktionsstätte.

Zur Jahresmitte 1999 wurde erstmals breitere Kritik an dem Projekt CargoLifter geäußert. Ein Luftfahrtingenieur analysierte in einem Artikel den geplanten und veröffentlichten Business Case des CargoLifters und zeigte auf, dass die von dem Unternehmen herausgegebenen Kalkulationen und Zahlen vermutlich wesentlich zu gering seien. Diese Kritik äußerte der Journalist auch in einem Interview, das er mit dem Vorstandsvorsitzenden der Gesellschaft, Carl von Gablenz, führte.

Ebenfalls Mitte des Jahres 1999 gaben das Unternehmen und der brandenburgische Wirtschaftsminister Burkhard Dreher bekannt, dass CargoLifter sich als Projekt auf der Expo 2000 präsentieren werde.

Von finanzieller Seite her lief zu dieser Zeit die dritte Privatplatzierung von Aktien. Der Preis der Vinkulierten Namensaktie wurde dafür auf der vorhergegangenen Hauptversammlung auf nunmehr 25 Euro (~ 48,90 DM) festgelegt. Der Preis bei der zweiten Platzierung hatte noch bei 40 DM gelegen. Zu diesem Zeitpunkt hielten rund 5300 Aktionäre Anteile an dem Unternehmen. In einem Gespräch mit der Süddeutschen Zeitung erwähnte ein für die Finanzplanung zuständiger Mitarbeiter, dass die CargoLifter AG zu einer der größten nicht an der Börse notierten Gesellschaften in Deutschland zähle.

Im August 1999 setzte sich erstmals ein längerer Artikel im Magazin "Der Spiegel" mit der Cargolifter AG auseinander. Neben der Darstellung der Werfthalle und des generellen Geschäftszwecks des Unternehmens wurde auch Kritik an dem Unternehmen geäußert. Dargestellt wurde einerseits, dass der von CargoLifter genannte Zeitplan für die Entwicklung des Luftschiffs kaum realisierbar sei. Andererseits wurde aufgezeigt, dass die erste Version des Lastenluftschiffs noch nicht langstreckentauglich sein werde, da CargoLifter für dieses erste Exemplar noch keine Ballastwassergewinnungsanlage zur Verfügung haben werde. Darüber hinaus wurde in diesem Artikel dargelegt, dass seitens der Zeppelin Luftschifftechnik die Erfolgschancen des Projektes als gering eingeschätzt würden. Einerseits wurden hier enorme technische Hürden genannt, andererseits kam eine Studie der Zeppeliner zu dem Ergebnis, dass die Transportkosten eines CargoLifters um den Faktor 10 über denen herkömmlicher und etablierter Transportsysteme wie Lkw, Schiff oder Flugzeug lägen.

Mitte September 1999 wurde Carl von Gablenz als Vorstandsvorsitzender stellvertretend für die Cargolifter AG als Entrepreneur des Jahres ausgezeichnet.

Im Oktober feierte das Unternehmen ein sogenanntes "Bogenfest". Die ersten zwei von insgesamt fünf 107 m hohen Stahlbögen der im Bau befindlichen Luftschiffhalle waren bis dahin errichtet. Rund 25.000 Besucher strömten zu diesem öffentlichen Ereignis CargoLifters. Anwesend war bei dem Ereignis unter anderem der stellvertretende brandenburgische Regierungschef Jörg Schönbohm, der viele lobende Worte für das Unternehmen und dessen Vorhaben äußerte.

Nur wenige Tage später konnte das Unternehmen einen weiteren Meilenstein feiern: Das Experimentalluftschiff Joey hob am 18. Oktober 1999 gegen 17:45 Uhr Ortszeit zu seinem Jungfernflug ab, nachdem es vom Luftfahrt-Bundesamt Anfang Oktober eine vorläufige Verkehrszulassung erhalten hatte. Während des 16-minütigen problemlosen Testflugs flog der Testpilot als einzige Person an Bord einige Runden über das Werftareal CargoLifters.

Ebenfalls im Oktober verkündete das Unternehmen einen Personalstand von 154 Mitarbeitern; „Experten aus der ganzen Welt“ seien für das Unternehmen abgeworben worden.

Gegen Ende des Jahres hatte das Unternehmen rund 10.000 Aktionäre. Als institutioneller Investor hatte mittlerweile auch der Luft- und Raumfahrtkonzern DASA Anteile an CargoLifter erworben.

Zu Beginn des Jahres 2000 gab das Unternehmen bekannt, zur im März anstehenden Hauptversammlung detaillierte Pläne über den geplanten Börsengang und das Bankenkonsortium bekanntzugeben. Erstmals öffentlich bekanntgegeben wurde auch, dass ein britischer Pensionsfonds, Henderson, rund 5 % der Anteile an CargoLifter halte und damit der größte Einzelaktionär sei.

Anfang Februar gab CargoLifter bekannt, im Bereich der Informationstechnologie und Telekommunikation eng mit IBM, Cisco und E-Plus zusammenzuarbeiten. Vertreter der Unternehmen verbreiteten, dass es sich bei dieser Kooperation jedoch um keine finanzielle Beteiligung handele. Auf der nur wenige Monate später in Hannover stattfindenden CeBIT-Computermesse war über dem Messestand von IBM ein Miniatur-Luftschiff mit IBM-Logo aufgehängt.

Im März des Jahres 2000 standen mehrere größere Ereignisse an. Anfang des Monats verfügte CargoLifter über 13.000 Aktionäre, die insgesamt rund 300 Mio. DM Eigenkapital in das Unternehmen eingezahlt hatten. Nach Unternehmensangaben war CargoLifter damit das größte nicht-börsennotierte Unternehmen Deutschlands.
Auf der am 11. März stattgefundenen Hauptversammlung kündigte die Unternehmensleitung erstmals detailliertere Pläne für den anstehenden Börsengang an. Neben dem anvisierten zweiten Quartal wurden auch das betreuende Bankenkonsortium aus Commerzbank, Bayerischer Landesbank, Schroders sowie Hauck & Aufhäuser vorgestellt.

Der Börsengang von CargoLifter stand im Mai 2000 an und wurde von einer hohen medialen Aufmerksamkeit begleitet. Die Börsenneueinführung stand dabei unter ungünstigen wirtschaftlichen Rahmenbedingungen. So war beispielsweise der Neue Markt seit seinem historischen Höchststand stark gefallen. Auch die sogenannte Dotcom-Blase war bereits im März 2000 „geplatzt“.
Mitte Mai gaben CargoLifter und die betreuenden Banken die Bookbuilding-Spanne für die zu emittierenden Aktien bekannt. Die aus einer Kapitalerhöhung stammenden maximal 935.000 Aktien konnten zwischen dem 16. und 24. Mai von Investoren gezeichnet werden. Das Unternehmen peilte hierbei eine Spanne von 14 bis 18 Euro an. Bis zum Abschluss der Privatplatzierung und Bekanntgabe der offiziellen Börseneinführung hatte CargoLifter rund 16.000 Anteilseigner.
Während der Bookbuildingspanne wurde der Öffentlichkeit auch die Gesamtinvestitionssumme des Projektes mitgeteilt: Bis zum Erreichen der Serienproduktion würden zwei Prototypen und zwei Vorserienmodelle des Lastenluftschiffs nötig, die mit den Investitionen für das Werftareal insgesamt mit rund 1 Milliarde DM veranschlagt wurden.

Am Dienstag, den 30. Mai 2000, wurde die CargoLifter-Aktie erstmals im amtlichen Handel unter der Wertpapierkennnummer "540261" notiert. CargoLifter hatte sich bei der Erstnotiz bewusst gegen eine Platzierung im Marktsegment des Nemax entschieden. Der Emissionspreis betrug 15 Euro und die Aktie entwickelte sich in einem mäßigen Marktumfeld relativ stabil und schloss bei 14,50 Euro. Ursprünglich hatte das Unternehmen geplant, mit dem Börsengang rund 250 Mio. DM zu erlösen. Tatsächlich wurden beim Börsengang jedoch nur 89,2 Mio. US-Dollar (~188,2 Mio. DM) erlöst.

Begleitet war der Börsenstart von dem einen Tag vor der Neuemission im Magazin "Der Spiegel" erschienenen Bericht über das Unternehmen und den anstehenden Börsengang. Zwar wurde das Unternehmen durchaus positiv dargestellt und gar von einem „der spektakulärsten Börsengänge der deutschen Wirtschaftsgeschichte“ gesprochen. Daneben diskutierte der Magazinbericht jedoch auch die im Börsenprospekt genannten Projektrisiken, weitere mögliche technische Probleme sowie die Aktienoptionen, die manche Personen aus dem obersten Führungsteam im Zuge des Börsengangs zugestanden worden waren.

Anfang Juni 2000 wurde das Werftgelände CargoLifters in Brandenburg als Außenstandort offiziell Teil der Weltausstellung Expo 2000. Parallel dazu wurde ein Besucherzentrum auf der Baustelle der Werft offiziell eröffnet, wodurch der Öffentlichkeit Führungen über die Baustelle und das Areal angeboten wurden. Gegen Ende des Monats gab Noël Forgeard, damaliger Geschäftsführer von Airbus, auf einer Pressekonferenz in Paris bekannt, dass geprüft würde, für das in Planung befindliche Flugzeug A3XX (später A380 genannt) ein CL-160-Luftschiff einzusetzen, um damit den innereuropäischen Transport von Flugzeugbauteilen für das Großflugzeug zu realisieren.

Zu Beginn des Monats September 2000 stattete der damalige Bundeskanzler Gerhard Schröder im Rahmen einer Sommerreise durch Ostdeutschland CargoLifter einen Besuch ab. Während einer einstündigen Führung über das Werftgelände war der Bundeskanzler, der von vielen Journalisten und dem brandenburgischen Ministerpräsidenten Manfred Stolpe begleitet wurde, nach Medienberichten sichtlich von der Baustelle und den Ausmaßen des in Entstehung befindlichen Luftschiffhangars fasziniert.
Mitte September feierte CargoLifter zudem ein sogenanntes "Torfest." Bei diesem Tag der offenen Tür wurde mit 6000 Besuchern die Komplementierung der beiden Hangartore der im Bau befindlichen Werfthalle gefeiert. Während der Feierlichkeiten wurde zudem das wenige Monate zuvor erworbene Skyship 600 durch Bundesverkehrsminister Reinhard Klimmt getauft.

Im Oktober 2000 wurde die Energiezentrale auf dem Unternehmensstandort Brand erstmals in Betrieb genommen, die von der CargoLifter Tochtergesellschaft Energieversorgung Brand betrieben wurde.

Mitte November 2000 gab die Deutsche Börse bekannt, dass die Aktie der Cargolifter AG mit Wirkung zum 18. Dezember in den MDAX aufgenommen werde. In diesem Aktienindex der wichtigsten 70 deutschen Nebenwerte wurde durch die Fusion von SKW Trostberg mit der Degussa ein Platz frei. Die Aufnahme von CargoLifter basierte damit lediglich auf dem Handelsvolumen der Aktie.

Ende November 2000 stand das für das Unternehmen bedeutendste Ereignis seiner Geschichte an. Während zwei Veranstaltungen wurde die Fertigstellung der Werfthalle offiziell gefeiert. Am Mittwoch, den 22. November, wurde während einer Veranstaltung mit rund 500 Vertretern aus Politik und Wirtschaft die Übergabe der Halle an das Unternehmen zelebriert. Zu den geladenen Gästen gehörten unter anderem Manfred Stolpe und Wolfgang Fürniß, aber auch 50 Diplomaten und Vertreter von in Berlin ansässigen Botschaften erschienen zu dem Ereignis.
Die zweite Veranstaltung zur Feier der Halleneröffnung fand am Samstag, den 27. November, statt. CargoLifter hatte hierbei seine Aktionäre und die breite Öffentlichkeit zu einer zehnstündigen Feier eingeladen. Obgleich die Gäste 150 DM Eintritt bezahlen mussten, erschienen 10.000 Personen, die dem vom Moderator Volker Hirth präsentierten Programm folgten.

Die Feierlichkeiten zur Fertigstellung der Luftschiffhalle wurden nicht nur von einer hohen Anzahl und für das Unternehmen vorteilhafter medialer Berichterstattung begleitet. Am 21. und am 23. November setzte sich die Financial Times Deutschland in großen Artikeln mit dem Luftschiffprojekt und dessen Risiken auseinander. Insbesondere der zweite Artikel erhielt hierbei eine hohe Perzeption – so war der ganzseitige Artikel mit einer Zeichnung eines abstürzenden Luftschiffs versehen.

Parallel zur Halleneröffnung wurde auf einer Pressekonferenz am 22. November das zu dem Zeitpunkt noch als "Towtech" bezeichnete Projekt des CL-75-Lastenballons erstmals der Öffentlichkeit vorgestellt.

Gegen Ende des Jahres 2000 endete zum 1. Dezember eine Sperrfrist für Altaktionäre. Jene Aktionäre, die bereits während der Privatplatzierungsphase Aktien an CargoLifter gehalten hatten, hatten sich vor dem Börsengang zu einer freiwilligen sechsmonatigen Haltefrist verpflichtet. Nachdem der Aktienkurs im August seinen absoluten Höchstwert i. H. v. 27,60 Euro erreicht hatte, war er in der Folgezeit wieder gesunken. Am Tag des Auslaufens der Haltefrist fiel der Kurs um 10 % auf den Wert von 14,40 Euro und erreichte erstmals einen Betrag unter dem Erstausgabewert.

Mitte Dezember 2000 begann eine über mehrere Monate andauernde und öffentlich ausgetragene Auseinandersetzung der CargoLifter AG mit dem Luftfahrtjournalisten Heiko Teegen. Der Herausgeber des Luftfahrtmagazins "Pilot und Flugzeug" äußerte zu Beginn der Auseinandersetzung öffentlich Kritik an der generellen Realisierbarkeit des Luftschiffprojektes. Insbesondere seien die vom Unternehmen im Börsenprospekt angegebenen technischen Leistungsdaten des geplanten CL-160-Luftschiffes fehlerhaft, was den Redakteur dazu veranlasste, einen Brief an Brandenburgs Wirtschaftsminister Wolfgang Fürniß zu verfassen, um diesen darin aufzufordern, die Fördermittel an CargoLifter nicht weiter auszuzahlen. Die Geschäftsführung der CargoLifter AG reagierte auf diese Kritik mit einer öffentlichen Gegendarstellung, in der die technische Kritik als unbegründet zurückgewiesen wurde. Parallel dazu wurde gegen den Redakteur Strafanzeige wegen möglicher Verleumdung und eines Verstoßes gegen das Börsengesetz erstattet.

Kurz vor dem Jahreswechsel berichtete die "Financial Times Deutschland," welche sich auf interne Unternehmensdokumente berief, dass der erste Prototyp des CL-160-Luftschiffs leistungsmäßig eine geringere Auslegung haben würde, als bis dato öffentlich durch CargoLifter angekündigt. Statt auf ursprünglich 160 Tonnen Nutzlast und 10.000 km Reichweite würde der erste Prototyp nur auf 129 Tonnen Nutzlast und maximal 4.680 Kilometer Reichweite ausgelegt werden. Durch das Unternehmen wurde zudem verkündet, dass insbesondere ein Einsatz innerhalb eines 3000-km-Radius wirtschaftlich am profitabelsten sei, womit diese Leistungsverringerung begründet wurde. CargoLifter hatte diese Unternehmensdokumente und geänderten Daten insbesondere als Reaktion auf den Kritiker des Luftfahrtmagazins "Pilot und Flugzeug" auf seine Unternehmenshomepage gestellt, da dieser Herausgeber seine Kritik an CargoLifter noch einmal verstärkt wiederholt hatte. Der Kritiker hatte einen zweiten Brief an den Wirtschaftsminister des Landes Brandenburg gesendet und dem Unternehmen erneut die Kommunikation von falschen Zahlen und technischen Daten vorgeworfen.

Der Streit des Unternehmens mit Heiko Teegen hielt auch im Januar des Jahres 2001 an und wurde in den Medien höchst öffentlich dargestellt. CargoLifter reagierte zum Jahreswechsel auf die Kritik, indem unternehmensexterne Branchenexperten sowie Betriebsingenieure öffentlich technische Daten und Planungen des Unternehmens präsentierten. In den Medien und in der Öffentlichkeit kamen zu dem Zeitpunkt jedoch Zweifel auf, ob das Unternehmen an seinem ambitionierten Zeitplan für die Entwicklung des Luftschiffs CL-160 festhalten könne.
Mitte des Monats gab die Staatsanwaltschaft Berlin bekannt, dass sie die Anzeige des Luftfahrtjournalisten mangels eines hinreichenden Anfangsverdachts nicht weiter verfolgen würde, sodass dessen Strafanzeige wegen Subventionsmissbrauchs und Verstoßes gegen das Börsengesetz erfolglos blieb.

Ebenfalls Mitte Januar wurde in der Luftschiffhalle der in Amerika entworfene und konstruierte Towtech-Transportballon nach Medieninformationen erstmals mit Luft „aufgeblasen“. Die tatsächliche Bedruckung mit Helium folgte jedoch erst im August.

Wenige Tage vor der für den 23. Januar angesetzten Bilanzpressekonferenz kamen weitere schlechte Nachrichten für das Unternehmen auf. Airbus verkündete über eine Pressesprecherin, dass sich das Unternehmen gegen die Verwendung eines CL-160-Luftschiffes für den Transport von Einzelteilen des Flugzeuges A 380 entschieden habe. Zudem vermeldeten börsennahe Zeitungen, dass die Unternehmensleitung Eingeständnisse gegenüber ihrer ambitionierten Zeit- und Kostenplanung für das technische Entwicklungsprojekt machen werde. Von technischer Seite her wurde zudem bekannt, dass der CL-160 in seiner aktuellen Entwurfsplanung weniger, dafür aber schwenkbare Triebwerke erhalten würde. Statt 16 wurden in dem aktuellen Planungsentwurf nur noch 8 oder 10 Turbinen pro Luftschiff eingeplant. Statt der vorgesehenen Dieselaggregate würden die nun vorgesehenen Motoren Gasturbinen sein.

Während der Bilanzpressekonferenz verkündete die Unternehmensführung, dass CargoLifters Liquiditätssituation noch für ein volles Unternehmensjahr reichen würde. Aus diesem Grunde wurde für die kommende Hauptversammlung angekündigt, dort die Ausgabe von Wandel- oder Optionsschuldverschreibungen billigen zu lassen. Darüber hinaus wurden während dieser Pressekonferenz genauere Details zu dem geplanten Geschäftsmodell verkündet, indem geplant wurde, CL-160-Luftschiffe an Leasinggesellschaften zu verkaufen, um so ein Sale-and-lease-back-Verfahren einzusetzen. Der bis dato als "Towtech" bezeichnete Lasten-Transportballon wurde während dieser Veranstaltung erstmals als "CL-75 Towtech" benannt.

Anfang März 2001 verkündete das Unternehmen Liebherr, das seit 1997 zusammen mit CargoLifter an der Konzeption des Lastaufnahmerahmens für das CL-160-Luftschiff gearbeitet hatte, seinen Rückzug aus der Kooperation.
Zur gleichen Zeit erklärte die CargoLifter-Unternehmensführung dem Vorschlag des damaligen FDP-Landesvorsitzenden des Bundeslandes NRW Jürgen Möllemann, ein CL-160 für den Atommülltransport von Castor-Transportbehältern zu benutzen, eine deutliche Absage.

Einen Tag vor der Hauptversammlung des Jahres 2001 gab das Unternehmen bekannt, die Triebwerke für das CL-160-Luftschiff bei General Electric beziehen zu wollen. CargoLifter unterzeichnete hierzu mit dem GE-CEO Jack Welch einen Vorvertrag über die Lieferung und Wartung von bis zu 50 CT7-8-Turboshaft-Triebwerken. Die Absichtserklärung enthielt zudem Informationen über eine mögliche Zusammenarbeit bei der Entwicklung des Luftschiffes. Eine finanzielle Beteiligung durch GE erfolgte jedoch nicht. Der amerikanische Technologiekonzern sicherte lediglich die Unterstützung mit Ressourcen und Know-how zu. Die Ankündigung des Vertrages mit GE wurde in der Branche mit großer Überraschung aufgenommen, da lange Zeit Rolls-Royce mit dem Turbomeca-RTM322-Triebwerk als Favorit gegolten hatte.
Auf der Hauptversammlung, die am 17. März 2001 stattfand, musste CargoLifter öffentlich erklären, dass die Luftschiffentwicklung erneut einer Kostensteigerung von zusätzlichen 80 Mio. Euro unterliegen würde. Nach damaligem Stand der Kommunikation bliebe davon jedoch die Zeitplanung bis zur Indienststellung des Luftschiffes unberührt. Für die gesamte Profitabilität des Konzerns wurde angekündigt, dass zukünftig der CL-75-Lastenballon verkauft werden solle, so dass liquide Mittel in das Unternehmen gebracht werden könnten.
Heinz Herrmann, Aufsichtsratsvorsitzender von CargoLifter, gab während der Hauptversammlung zudem für die Öffentlichkeit überraschend bekannt, dass der CargoLifter-Vorstand zukünftig auf drei Personen erweitert werden solle. Insbesondere würde ein neues Mitglied gesucht, das als Technikvorstand neu in die Gesellschaft berufen werden könnte.

Der April des Jahres 2001 war erneut von öffentlicher Kritik an dem Unternehmen geprägt. Einerseits veröffentlichte das Magazin "Der Stern" einen kritischen Beitrag über die Zukunftsperspektiven von CargoLifter. Breite öffentliche Bekanntheit erreichte der Artikel jedoch erst, nachdem der Vorstandsvorsitzende von CargoLifter auf der Unternehmenshomepage in Reaktion auf diesen Zeitschriftenartikel einen offenen Brief veröffentlichte und sich dort mit dem Artikel auseinandersetzte und zudem die Zunft der Journalisten als „Negativjournalisten“ kritisierte.
Andererseits trat erneut der Redakteur der Flugzeitschrift "Pilot und Flugzeug", Heiko Teegen, in Aktion und erstattete eine Strafanzeige gegen den Vorstandsvorsitzenden von CargoLifter. Teegen warf Carl von Gablenz vor, im Dezember 2000 vor Gericht eine falsche Eidesstattliche Versicherung abgegeben zu haben. Zum Jahresende 2000 hatte die CargoLifter AG eine Einstweilige Verfügung gegen den Kritiker erwirkt, zog diese jedoch im Februar 2001 auf dringende Empfehlung des Landgerichts Berlin zurück. Bestandteil der Verfügung war eine Eidesstattliche Versicherung des Vorstandsvorsitzenden, nach der die Erprobungszeit des CL-160-Luftschiffes rund acht Jahre dauern würde. Diese genannte Erprobungszeit hätte jedoch auf den Beginn der Serienproduktion im Jahr 2011 hingewiesen, was im deutlichen Widerspruch zu dem sonst vom Unternehmen kommunizierten Plan stand, Ende des Jahres 2004 mit der Serienproduktion des Luftschiffes starten zu wollen.

Im Juli 2001 verkündete CargoLifter eine Vertragsunterzeichnung mit BAE Systems, die für das CL-160-Luftschiff Systeme der Flugsteuerung, Instrumentierung, Navigation und Kommunikation zuliefern würden. Auch verkündete die CargoLifter Presseabteilung, dass das hauseigene Skyship 600 im August die deutsche Flugzulassung durch das Luftfahrtbundesamt erhalten solle und dann mit bis zu neun zahlenden Personen Rundflüge anbieten könne.
Von technischer Seite konnte das Unternehmen eine Erfolgsmeldung verkünden. In Kooperation mit dem Hochspannungslabor der Technischen Universität Cottbus hatte die CargoLifter Development an einem heliumgefüllten Luftschiff-Modell im Maßstab 1:25 die optimale Anordnung von Blitzableitern an der Luftschiffhülle überprüft und dabei die Blitzschutz-Tests erfolgreich abgeschlossen. Auch der Bundeskanzler Gerhard Schröder kam im Rahmen dieser Universitätskooperation erneut mit CargoLifter in Kontakt. Bei einem Besuch des Regierungschefs Anfang Juli nahm dieser als Beobachter auch an den Blitzschutzversuchen teil.

Zwei wichtige Meilensteine in der Unternehmensgeschichte erreichte das Unternehmen im August 2001. Das Luftfahrt-Bundesamt erteilte der Tochtergesellschaft CL Development die Zulassung als Instandhaltungsbetrieb nach europäischem Luftrecht. Diese Zulassung erlaubte es dem Unternehmen, eigenverantwortlich plan- und außerplanmäßige Wartungen an musterzugelassenen Luftschiffen vorzunehmen.
Darüber hinaus gelang es dem Unternehmen, den CL-75-Ballon erstmals mit Helium zu befüllen. Während eines 38-stündigen Vorgangs wurde die Hülle dieses größten jemals gebauten Aerostaten zunächst mit Umgebungsluft prall gefüllt und im Anschluss über eine Schichtungsmethode (durch niedrigere Dichte von Helium gegenüber Luft) die 110.000 m³ Luft durch Helium ersetzt.

Ende September 2001 feierte CargoLifter in seiner Werfthalle unter Anwesenheit von Politikern wie Bundespräsident Johannes Rau und dem brandenburgischen Wirtschaftsminister Wolfgang Fürniß den Produktionsbeginn des CL-160-Luftschiffs. Während dieser Feierlichkeit wurde ein 262 m langer und 4 m breiter Luftschiffhüllen-Schneidetisch offiziell eingeweiht.
Tatsächlich ließ das Unternehmen die Öffentlichkeit jedoch im unklaren und verstand den Produktionsbeginn lediglich als „Vorbereitung und Test der Anlagen und Produktionseinrichtungen“. Der Produktionsbeginn ließ sich retrospektiv daher lediglich als große Öffentlichkeitsarbeitsveranstaltung verstehen.

Mitte Oktober wurde der CL-75-Ballon erstmals mit Hilfe von Sattelschleppern aus der Luftschiffhalle gezogen und im Freien getestet. Bei diesem ersten Testdurchgang wurde ein 24 Tonnen schwerer Kran mit dem Ballon auf sieben Meter Höhe gezogen.

Der bereits im März angekündigte Vertrag mit General Electric wurde im November des Jahres unterschrieben. CargoLifter verpflichtete sich darin, bis zu 400 Turboshaft-Turbinen GE- CT7-8L zu beziehen. Darüber hinaus sah der für den Zeitraum von zehn Jahren geschlossene Vertrag im Wert von bis zu 500 Mio. US-Dollar Reparaturleistungen und Ersatzteillieferungen vor.

Ebenfalls im November erhielt CargoLifter den Zuschlag für die Lieferung und Bereitstellung von Leichter-als-Luft-Logistikdienstleistungen nach Amerika. Ein amerikanisches Konsortium hatte zu dem Zeitpunkt vor, eine Magnetschwebebahn in Pennsylvania zu errichten. Das unter dem Namen "Maglev" in Pittsburgh ansässige Konsortium plante, eine 75 Kilometer lange Trasse auf einem äußerst hügeligen Terrain zu errichten, wofür sich der CL-75-Ballon sowie das CL-160-Lastenluftschiff als ideale Transportmethoden für die Konstruktionsarbeiten angeboten hätten. Bestandteil des Vertrages war ein Aktientausch zwischen der CargoLifter und dem bis dato aus sieben Partnern bestehenden Maglev-Konsortium. CargoLifter erhielt 12,5 % der Stimmen an Maglev und zudem einen Sitz im Board of Directors dieser amerikanischen Aktiengesellschaft. Maglev erhielt im Gegenzug rund 0,3 % an der CargoLifter, was einem Gegenwert von 500.000 US-Dollar entsprach.

Mitte November gab CargoLifter bekannt, dass dessen finanzielle Mittel nur noch bis Ende des Geschäftsjahres 2001 ausreichen würden.
Eine Anfang November angekündigte Kapitalerhöhung unter Ausgabe von 6,75 Mio. Aktien wurde im Verlauf des Monats zu 92 % gezeichnet, sodass das Unternehmen zwar neues Kapital i. H. v. 34,1 Mio. Euro erhielt.
Dennoch bescherte diese neuerliche Kapitaleinwerbung dem Unternehmen nur einen kurzen Aufschub, da gleichzeitig verkündet wurde, dass das frische Kapital den Geschäftsbetrieb nur bis Ende April 2002 sichern würde, wenn nicht eine neue Kapitalmaßnahme durchgeführt oder ein Investor in das Unternehmen einsteigen würde.

Um die für das Unternehmen bestandsbedrohende finanzielle Situation in den Griff zu bekommen und die Entwicklung des Luftschiffvorhabens voranzubringen, wandte sich die CargoLifter-Unternehmensführung Anfang Dezember an die Öffentlichkeit. Verkündet wurde, dass insbesondere verstärkt Ausschau nach strategischen Partnern in der Industrie gehalten würde. Daneben wurde jedoch auch an die Öffentliche Hand appelliert, die Bürgschaften für Kredite geben solle.

Rainer Hertrich, Co-Vorstand der EADS, wies Mitte Dezember jedoch darauf hin, dass Airbus und EADS auch nach einer zweiten intensiven technischen und finanziellen Prüfung keine Möglichkeiten der Finanzinvestition in CargoLifter sähen. Sein Unternehmen würde jedoch weiterhin Gespräche über strategische Abkommen oder Partnerschaften mit CargoLifter führen.

Das Jahr endete mit einem kritischen Bericht über das Unternehmen. Neben den besorgniserregenden Nachrichten über die finanzielle Zukunftsfähigkeit des Unternehmens wurde in der Presse nach wie vor kritisch über das technische Vorgehen CargoLifters und die technische Leistungsfähigkeit der Luftschifftechnologie berichtet. Das Magazin "Der Spiegel" publizierte kurz vor dem Jahreswechsel erneut einen größeren Artikel über technische Rückstände und Problembereiche des Unternehmens.

Januar:
Das Jahr 2002 startete für CargoLifter mit negativen Nachrichten. Ein Redakteur der "Financial Times Deutschland" nahm Mitte Januar an einer Aktionärs- und Öffentlichkeitsveranstaltung CargoLifters im Europäischen Patentamt in München teil. Im Anschluss an die Veranstaltung berichtete der Redakteur auf der Internetausgabe der Zeitung und schrieb dabei, dass der Vorstandsvorsitzende von CargoLifter während der Veranstaltung gesagt habe, dass das Projekt CargoLifter „im veränderten Umfeld ohne Staatshilfe nicht mehr zu machen“ sei – das Unternehmen würde demnach staatliche Hilfen benötigen, um eine Insolvenz zu verhindern. Nachdem die Deutsche Presseagentur diesen Bericht aufgriff und auch andere Online-Medien darüber berichteten, sackte der Börsenkurs von CargoLifter von 5 auf 2 Euro ab.
Die Pressestelle von CargoLifter bestätigte diesen Bericht zunächst gegenüber einigen Medien und verwies auf intensive Gespräche, die mit der Landes- und Bundesregierung geführt würden. Gegenüber anderen Medien wurde der Bericht hingegen noch am selben Tag dementiert und als Falschmeldung bezeichnet. Aus Perspektive des Unternehmens sei es zwar richtig, dass Gespräche mit der öffentlichen Hand geführt würden und CargoLifter sich wie andere öffentlich geförderte Luft- und Raumfahrtunternehmen Fördermittel erhoffe. Dennoch stünde das Projekt ohne Fördermittel nicht unmittelbar vor dem Ende.

Der Vorstandsvorsitzende von Gablenz verwies bei der öffentlichen Verteidigung des Unternehmens insbesondere auf eine Studie der Unternehmensberatung Roland Berger Strategy Consultants, die einen ersten Zwischenbericht präsentiert hätten und dabei noch größere zukünftige Marktchancen für das Unternehmen gefunden habe, als das Unternehmen bis dahin in seinen eigenen "Business-Case"-Berechnungen zugrunde gelegt hatte. Auch die technische Machbarkeit sei nach den Studienergebnissen der Berater gegeben, sodass von Gablenz darauf verwies, dass CargoLifter zukünftig profitabel arbeiten könne.

Ende Januar verkündete CargoLifter, dass der bisherige stellvertretende Aufsichtsratsvorsitzende Bernd Kröplin vom Aufsichtsrat in den Vorstand wechseln würde. Kröplin solle sich nach der Benennung vornehmlich um das Technik-Ressort kümmern, das bis dato der Vorstandsvorsitzende Carl-Heinrich von Gablenz übernommen hatte.
Verkündet wurde darüber hinaus auch, dass mit den Unternehmen Hamilton Sundstrand, Denel und mt-Propeller weitere Entwicklungspartner gefunden wurden. Hamilton sollte dem Vertrag nach sich um das System zur elektrischen Stromerzeugung kümmern. Denel würde die Struktur des Mittelkiels entwickeln und mt-Propeller würde als erster deutscher technischer Kooperationspartner acht Propeller für das CL-160-Luftschiff liefern.

Februar:
Zum Beginn des Monats Februar erklärte CargoLifter, die zu dem Zeitpunkt rund 70.000 Aktionäre umfasste, sich auf der im März anstehenden Hauptversammlung eine Verdoppelung des Grundkapitals genehmigen zu lassen. Zudem plante das Unternehmen, weitere Wandel- oder Optionsschuldverschreibungen in Höhe von rund 50 Mio. Euro durch die Aktionäre genehmigen zu lassen.
Auch der Aufsichtsrat sollte von drei auf sechs ständige Mitglieder erweitert werden. Durch den anstehenden Wechsel von Bernd Kröplin in den Vorstand gab CargoLifter zu diesem Zeitpunkt viele neue Kandidaten bekannt, die auf der Hauptversammlung neu zur Wahl stehen sollten.

Positiver Zuspruch für das Projekt und Unternehmen kam derweil vom Vorsitzenden des Bundesverbandes der Deutschen Luft- und Raumfahrtindustrie, Rainer Hertrich. Dieser sprach nicht nur von einem technisch hoch interessantem Projekt, sondern verkündete auch, dass der Verband alles Mögliche unternehmen würde, um CargoLifter zu unterstützen. Als ersten Schritt würde der Verband ein Lieferantenforum organisieren, um neue Kooperationspartner für CargoLifter vermitteln zu können.

März:
Anfang März 2002 stand für das CL-160-Entwicklungsprojekt ein zentraler technischer Meilenstein an. Während eines mehrtägigen "Preliminary Design Reviews" kamen unternehmensintern alle hochrangigen, an der Entwicklung des Luftschiffs beteiligten Projektleiter zusammen und führten dabei den Status der bisherigen Entwicklung zusammen.

Begleitet war dieses Milestonemeeting von einer kritischen Publikation des Nachrichtenmagazins "Der Spiegel". Der Artikel, der kurz vor dem Abschluss der Entwicklerkonferenz veröffentlicht wurde, und dessen Inhalt bereits vorab via Nachrichtenagenturen an die Öffentlichkeit lanciert wurde, berief sich auf unternehmensinterne Unterlagen, die verdeckt an das Nachrichtenmagazin gegeben worden waren. Laut "Spiegel" wies das CargoLifter-interne Dokument darauf hin, dass das Luftschiff in seinem derzeitigen Planungsstand zu wetterempfindlich und der Kerosinverbrauch zu hoch sei. Probleme könnten darüber hinaus mit dem Instrumentenflug auftauchen, da bei der geplanten Flughöhe von maximal 2000 Metern dieser vielfach gar nicht von der Flugsicherung angeboten würde und das Luftschiff auf den Sichtflug angewiesen wäre. Insgesamt würde das CargoLifter CL-160-Luftschiff den Aussagen dieses Dokumentes zufolge ein „Schönwetterfluggerät“ darstellen, wodurch sich erhebliche Probleme in der kommerziellen Nutzung ergeben würden.

CargoLifters Unternehmensführung reagierte unmittelbar auf diesen kritischen Bericht und ließ in einer Pressemitteilung verkünden, dass das PDR-Milestonemeeting höchst erfolgreich verlaufen sei, und vor allem keine fundamentalen technischen Projektrisiken vorliegen würden.
Neben dieser Pressemitteilung, in der sämtliche Aussagen als falsche Berichterstattung zurückgewiesen wurden, kündigte CargoLifter zudem an, den Fall beim Bundesaufsichtsamt für den Wertpapierhandel anzuzeigen, und straf- und zivilrechtliche Schritte gegen den Informanten und das Nachrichtenmagazin einzuleiten.

Erst Jahre später konnte in einer empirischen Untersuchung gezeigt werden, dass "Der Spiegel" mit seiner Berichterstattung richtig lag. Tatsächlich wurde nämlich in der internen Zusammenfassung des technischen Milestonemeetings davon gesprochen, dass wesentliche technische Problembereiche noch nicht ausreichend genug adressiert seien. Die Unternehmensführung von CargoLifter stellte demnach in der Außenkommunikation seine technische Leistungsfähigkeit und die bisherigen technologischen Errungenschaften wesentlich positiver dar, als sie intern in der Projektzusammenfassung diskutiert wurden.

Anfang März wurde zudem bekannt, dass die CargoLifter-Unternehmensführung ein Bundesdarlehen in Höhe von 300 Millionen Euro beim Bundeswirtschaftsministerium beantragt hatte. Bestätigt wurde dieser Antrag sowohl von Unternehmensseite, als auch von einer Ministeriumssprecherin, woraufhin der Aktienkurs des Unternehmens um 20 % anstieg. Später wurde hingegen bekannt, dass die beim Ministerium eingereichten Unterlagen noch unvollständig sein, so dass sich die Beantragung verzögerte.

Die Jahreshauptversammlung, in der schwerpunktmäßig auf die weiterhin vorherrschenden Probleme in der Zeit- und Kostenplanung im Hinblick auf das technologische Entwicklungsprojekt eingegangen wurde, stand Mitte März an. Die Berechnungen und Ergebnisse einer Studie der Unternehmensberatung Roland Berger wurden detailliert präsentiert. Finanziell fehlten dem Unternehmen zu dieser Zeit zwischen 420 und 580 Mio. Euro bis zum Beginn der Serienproduktion und im zeitlichen Ablauf hatten sich ebenfalls erneut Verzögerungen ergeben, sodass verkündet wurde, den ersten Prototyp des Luftschiffes erneut ein Jahr später auszuhallen.

Auf der Hauptversammlung beschlossen die Aktionäre, den Geschäftszweck von CargoLifter breiter zu definieren. Statt „Entwicklung, Bau und Betrieb des Cargolifter insbesondere zum Zweck Groß- und Schwerlasttransporte“ lautete die Definition nun: „Nutzung der ‚Leichter-als-Luft‘-Technologie“. Mit diesem Schritt wollte die Unternehmensführung das Zeichen setzen, sich auch für andere Einsatzmöglichkeiten der Leichter-als-Luft-Technologie jenseits des Groß- und Schwerlasttransportes einzusetzen.

Unmittelbar vor Beginn der Hauptversammlung konnte CargoLifter auf einer Pressekonferenz zudem einen ersten Verkaufserfolg verkünden. Mit dem kanadischen Unternehmen "Heavy Lift Canada" wurde ein Vertrag über den Kauf eines CL-75-Ballons und eine Option über den möglichen Kauf 25 weiterer CL-75-Systeme geschlossen. Als Kaufpreis wurden 10 Mio. $ pro Ballonsystem genannt. Die neu geschaffene kanadische Gesellschaft gab an, die Ballone im nördlichen Kanada auf Eisstraßen einsetzen zu wollen. Für den Transport von Ölförder-Equipment würden sich die Ballone insbesondere dann anbieten, wenn der Frühling einsetze und die Eisstraßen unbefahrbar mache. Mithilfe eines Ballons sei es jedoch möglich, auch auf weniger stabilen Straßen schwere Lasten zu transportieren.

CargoLifter gab darüber hinaus an, sich zu 20 % an dem Unternehmen Heavy Lift Canada Inc. zu beteiligen, um dauerhaft von dem operativen Betrieb von Leichter-als-Luft-Systemen zu profitieren.

Schon unmittelbar nach Ende der Hauptversammlung wurde durch Medien kritisch hinterfragt, ob hinter der Heavy Lift Canada Inc. überhaupt solvente Investoren stünden.
Der in Berlin erscheinende "Der Tagesspiegel" recherchierte hierzu intensiver und stellte rund zehn Tage nach Ende der Hauptversammlung fest, dass die Gesellschaft, die im kanadischen Calgary ihren Sitz hätte, noch nicht in das örtliche Handelsregister der Stadt eingetragen worden war. Dementsprechend musste es sich nach Interpretation des Tagesspiegels um eine sehr junge Gesellschaft handeln.
Sehr kritische Stimmen wurden auch von Analysten und Marktbeobachtern laut, was zeigte, dass die Finanzwelt den Verkauf höchst kritisch interpretierte. Da CargoLifter angegeben hatte, mit 1,5 Mio. $ 20 % der Anteile an der Heavy Lift zu kaufen, wäre der Wert des Unternehmens nur mit rund 7,5 Mio. $ zu bewerten. Ein Börsenanalyst kommentierte die von Heavy Lift getätigte Kaufoption auf 25 Ballonsysteme dann auch als reine „Propagandameldung“, da bei der Ausübung dieser Optionen rund 250 Mio. $ fällig würden.

Gegen Ende des Monats verkündete das Unternehmen in einer Ad-hoc-Mitteilung, dass das Unternehmen "GTS Global Trans Systems Concept GmbH" Anteile an der CargoLifter AG an einen strategischen Finanzinvestor verkauft habe und nun statt vorher mehr als 6 % weniger als 5 % an dem Luftschiffhersteller halte.
Die Presse beurteilte diesen Schritt recht kritisch, da die Haupt-Anteilseigner der GTS zugleich die beiden Vorstände der Cargolifter AG waren, sodass vermutet wurde, dass beide Vorstände kein Vertrauen mehr in ihr eigenes Unternehmen hätten.

Der Aufsichtsratsvorsitzende von CargoLifter wandte sich daraufhin unmittelbar an die Öffentlichkeit und erklärte, dieser Schritt sei nicht als Vertrauensverlust der beiden Vorstände in ihr Unternehmen zu interpretieren. Vielmehr habe die GTS im Jahr 1996 einen Kredit zur Finanzierung der Anteile an der CargoLifter AG aufnehmen müssen, der durch den nun erfolgten Verkauf von Aktien getilgt werden sollte. Der Aufsichtsratsvorsitzende gab weiterhin bekannt, die beiden Vorstände hätten zudem finanziell nicht von dieser Transaktion profitiert und bisher keine ihrer privat gehaltenen Aktien verkauft.

April:
Da sich das finanzielle Gleichgewicht des Unternehmens zunehmend verschlechterte, hatte die Geschäftsführung Ende März ihre Mitarbeiter und Aktionäre aufgerufen, eine Wandelanleihe zu zeichnen, um dadurch Liquidität in das Unternehmen zu bringen.
Bereits Mitte April konnte verkündet werden, dass zumindest genügend Anteile gezeichnet wurden, um den Betrieb des Unternehmens bis in den Mai hin aufrechtzuerhalten.

Mai:
Zum Beginn des Monats Mai bestätigten sich die schon seit Monaten in Medien kursierenden Gerüchte, dass CargoLifter mit dem Luft- und Raumfahrtkonzern Boeing zusammenarbeiten wolle. Durch beide Unternehmen wurde bekannt gegeben, dass nach anderthalb Jahren Verhandlung eine Absichtserklärung (Letter of intent) unterschrieben worden war, die förmlich festhielt, Möglichkeiten für die Entwicklung und Nutzung von Luftschiffen gemeinsam zu überprüfen. Die Möglichkeit eines finanziellen Engagements wollte Boeing zu diesem Zeitpunkt darüber hinaus nicht ausschließen. CargoLifter wies darüber hinaus darauf hin, dass Boeing ein finanzielles Engagement im Detail prüfen und im Verlauf des Jahres 2002 bekannt geben würde.

Von der Politik kamen derweil weniger positive Nachrichten. Die Landesregierung, die gebeten worden war, einen Überbrückungskredit zur Verfügung zu stellen, ließ mitteilen, dass dies nur erfolgen könne, wenn die Gesamtfinanzierung des Projektes gesichert wäre.

CargoLifter erklärte daraufhin, dass seine Zahlungsfähigkeit nur noch für wenige Tage ausreiche.
Bedingt war diese drastische Verschlechterung der finanziellen Lage vor allem durch eine äußerst schlechte Resonanz auf die angebotene Wandelanleihe. Die Anleihe, die den zu dem Zeitpunkt 71.000 Aktionären angeboten worden war, war ursprünglich mit einem Volumen von rund 50 Mio. Euro geplant. Mitte Mai waren dann jedoch lediglich 3,8 Mio. Euro überwiesen.

Nur wenig später wurde bekannt, dass das Bundeswirtschaftsministerium eine Förderung von CargoLifter mit Bundesmitteln ausschließen würde. Das Unternehmen teilte jedoch mit, dass weiterhin Gespräche mit dem Bundeskanzleramt und der Staatskanzlei in Potsdam geführt würden.

Wolfgang Fürniß, der brandenburgische Wirtschaftsminister, informierte einen Tag darauf das Kabinett während einer abendlichen Sitzung über die Situation des Unternehmens. Die Landespolitiker kamen dabei überein, CargoLifter nicht weiter fördern zu wollen. Weder der Bund, noch das Land zeigten daher zu dem Zeitpunkt Bereitschaft, das Unternehmen vor der drohenden Zahlungsunfähigkeit mit Krediten oder Zusagen zu unterstützen.

Weitere negative Nachrichten zogen wiederum nur einen Tag später für das Unternehmen auf. Durch die Wirtschaftswoche wurde berichtet, dass das Bundeswirtschaftsministerium in einer internen Studie die Zukunftschancen CargoLifters als vernichtend betrachtet. Hauptsächlich kritisiert wurde, dass das Unternehmen von Anfang an einer „falschen Entwicklungsstrategie“ gefolgt sei. Für Erstaunen sorgte dieses Gutachten und die Einschätzung jedoch beim Bundesverkehrsministerium, welche die Entscheidung, keine Förderungen zu geben, als „absurd“ bezeichnete.

Am 20. Mai gab die Unternehmensführung bekannt, ihr ambitioniertes CL-160 Entwicklungsvorhaben mangels vorhandener liquider Mittel vorläufig nicht weiter zu verfolgen. Stattdessen plante das Unternehmen, sich ausschließlich auf die weitere Entwicklung und Produktion des CL-75 Transportballons zu konzentrieren – insbesondere, um den existierenden Kaufvertrag mit der kanadischen Gesellschaft zu erfüllen und so Liquidität in das Unternehmen zu holen. Der parallel in Berlin verweilende Technologievorstand von Boeing David Swain erklärte im Zuge dieser Nachrichten, dass Boeing kein Interesse an einer Übernahme der CargoLifter habe und auch keine Übernahmepläne vorlägen.

Nachdem durch die Unternehmensführung in den Folgetagen keine neuerliche Finanzierung sichergestellt werden konnte, beantragte die mitarbeiterstärkste Tochtergesellschaft CL Development am Freitag, den 31. Mai 2002 ein vorläufiges Insolvenzverfahren beim Amtsgericht in Cottbus. Die CL-Geschäftsführung plante zu diesem Zeitpunkt noch, ein eigenes Konzept zur Restrukturierung und Sanierung des Unternehmens vorzulegen, und gemeinsam mit dem vorläufigen Insolvenzverwalter beim Gericht einzureichen.

Nachdem mit der CL-Development schon Ende Mai die wichtigste Konzerntochter Insolvenz angemeldet hatte, stellte auch die Konzernmutter CargoLifter AG wegen Zahlungsunfähigkeit am 7. Juni 2002 einen Insolvenzantrag. Zuvor war bereits das Berliner Büro an den Werftstandort nach Brand verlegt worden. Auch die Schließung der amerikanischen Tochtergesellschaft, CL Inc., war zuvor eingeleitet worden.

Im Verlauf ihrer Unternehmensgeschichte schaffte es die CargoLifter AG nicht, ihre ambitionierten Zeit- und Budgetplanungen einzuhalten. Innerhalb des Entwicklungsprojektes traten wiederholt zeitliche Verschiebungen und finanzielle Steigerungen auf.

Bei Unternehmensgründung war ursprünglich geplant, rund 135 Mio. DM für die Entwicklung und Konstruktion des ersten Lastenluftschiffprototypen aufzuwenden. Zusätzlich wurde mit 160 Mio. DM für den Unternehmensaufbau kalkuliert, sodass insgesamt von 295 Mio. DM Gesamtprojektkosten ausgegangen wurde.

Im Verlauf des Entwicklungsprojektes kam es nicht nur zeitlichen Verzögerungen, sondern auch mehrfach zu Kostensteigerungen. Im November 1997 beispielsweise berichtete ein Aufsichtsrat in einem Gastbeitrag für die Logistik-Fachzeitschrift "Transportmarkt" schon von Gesamtprojektkosten von 340 Mio. DM.

Zur Hauptversammlung im März 1999 wurden die Kosten für die Produktion bis zum ersten Luftschiff mit 437 Mio. und die Gesamtprojektkosten mit 850 Mio. DM angegeben.

Kurz vor dem geplanten Börsengang wurde auf der 3. ordentlichen Hauptversammlung im März 2000 eine weitere Kostensteigerung verkündet, sodass von 1,5 Mrd. DM oder 766,94 Mio. Euro Gesamtprojektkosten gesprochen wurde.

Auch in den Folgejahren wurden immer wieder Kostensteigerungen verkündet. Wenige Monate vor der Insolvenz wurden im März 2002 Kostenkalkulation aus einer Studie der Unternehmensberatung Roland Berger Strategy Consultants durch CargoLifter publik gemacht. Im günstigsten Fall berechnete die Unternehmensberatung die Kosten bis zum Beginn der Serienproduktion der Luftschiffe auf 720 Mio. Euro.

Obgleich CargoLifter bis zum Eintritt der Insolvenz rund 350 Mio. Euro an Eigenkapital und öffentlichen Fördermitteln eingesammelt hatte, fehlten dem Unternehmen noch mindestens 370 Mio. Euro zur Finanzierung der Entwicklung bis hin zur Serienreife der Lastenluftschiffe.

Im Frühjahr des Jahres 2002 waren weder private Investoren noch die öffentliche Hand bereit, dem Unternehmen weiteres Kapital zuzuführen, sodass CargoLifter wegen mangelnder liquider Mittel den Insolvenzantrag stellen musste.

Wenige Monate vor der Insolvenz, Anfang März 2002, stand für das CL-160-Entwicklungsprojekt ein zentraler technischer Meilenstein an. Während eines mehrtägigen Preliminary Design Reviews kamen unternehmensintern alle hochrangigen, an der Entwicklung des Luftschiffs beteiligten Projektleiter zusammen und führten dabei den Status der bisherigen Entwicklung zusammen.

Zum Zeitpunkt der Insolvenzanmeldung war die Entwicklung des Luftschiffs demnach noch nicht abgeschlossen. Das geplante Großluftschiff CL160 wurde schlussendlich nie gebaut.

Bei einem Unwetter, welches in Berlin und Brandenburg Sturmböen mit Geschwindigkeiten von bis zu 152 km/h mit sich brachte, wurde am 10. Juli 2002 die Hülle des Prototyps des Kran- und Transportballons CL75 AirCrane zerstört.

2002

2003

2004

2005

2007


2008

2011

2012


Hauptziel der CargoLifter AG war es, ein Lastenluftschiff für bis zu 160 Tonnen schwere Fracht zu entwickeln, zu konstruieren und operativ zu betreiben. Um diesem Geschäftszweck gerecht zu werden, wurden im Verlauf der Unternehmenshistorie insgesamt 13 Tochtergesellschaften gegründet.

Die Cargolifter AG hatte vor, ein Lastenluftschiff zu entwickeln, später zu konstruieren und auch operativ einzusetzen. Das Unternehmen definierte damit ein dreistufiges Produktionsverfahren: In der ersten Produktionsstufe sollte zunächst das Luftschiff entwickelt und gebaut werden.

Die zweite Produktionsstufe sah daran anschließend den operativen Betrieb des Luftschiffs vor. Auf dieser sollte also das Luftschiff Bestandteil einer Logistikdienstleistung sein.

Die dritte Produktionsstufe sah den operativen Betrieb eines Logistiknetzwerks vor. Neben Logistikdienstleistungen hätten auch die Routen für das Luftschiff in Hinblick auf eine ökonomische Flugplanung und die Wetterplanung entwickelt werden sollen. Zudem war daran gedacht, das Luftschiff auf dieser dritten Stufe als Teil einer Prozesskette in Kombination mit anderen Verkehrsträgern einzusetzen.

Über diese drei Produktionsstufen hinweg plante das Unternehmen, verschiedene Tochtergesellschaften zu gründen und die entsprechenden Aufgaben zu betrauen. Tatsächlich wurden die wesentlichen Gesellschaften auch in der Unternehmenshistorie gegründet und betrieben. Für die Entwicklung und den Bau des Luftschiffs, wurde die "CargoLifter Development GmbH" gegründet. Das Luftschiff operativ betreiben sollte die "CargoLifter Airship Operations GmbH." Für den Netzwerkbetrieb auf Produktionsstufe 3 wurde die "CargoLifter Network GmbH" ins Leben gerufen.

Die CargoLifter AG gründete im Verlauf ihrer Geschichte 14 Tochtergesellschaften. Vier Gesellschaften wurden dabei im Konzern als so genannte Kernkompetenzbereiche bezeichnet. Diese Gesellschaften, zu denen die CL Development, CL Network, CL Airship Operations sowie die CL World zählten, sollten für die Erbringung von unmittelbar mit dem Unternehmenszweck zusammenhängenden Aktivitäten leiten.

Daneben standen acht von CargoLifter als Servicebereiche bezeichnete Gesellschaften, die vornehmlich Dienstleistungen für den Konzern und alle weiteren Konzerngesellschaften erbrachten.

Eine 15. Gesellschaft, die "CargoLifter Alert" war als GGmbH konzipiert und befand sich in Gründung. Diese Gesellschaft sollte in Kooperation mit dem THW geführt werden und Katastrophen- und Hilfseinsätze unter Anwendung der CL-Luftschiffe koordinieren und planen. Durch die Insolvenz wurde diese Gesellschaft jedoch nie final im Handelsregister eingetragen.

Im Juli 1997 wurde die CargoLifter Development in Wiesbaden als Tochtergesellschaft der CargoLifter AG gegründet.

Unternehmensgegenstand war laut Satzung "die Entwicklung des CargoLifters inklusive des Baus von Versuchsträgern und Prototypen sowie Projektierung, Entwicklung, Bau und Betrieb von Luftfahrzeugen, insbesondere nach dem Prinzip „Leichter-als-Luft“ sowie alle mit diesem Zweck im Zusammenhang stehenden Aktivitäten und Dienstleistungen."

Zum Geschäftsführer wurde zunächst Dr. Ingolf Schäfer berufen, der das Projekt CargoLifter schon seit den Anfangstagen von technischer Seite her mit begleitet hatte und zudem stark in das Projekt des solarbetriebenen Luftschiffs LOTTE Anfang der 1990er-Jahre involviert war. Auf einer Gesellschafterversammlung im Januar 1998 wurde die Satzung des Unternehmens verändert und ein zusätzlicher weiter Geschäftsführer, Herr Norbert Meinl, berufen. Beide Geschäftsführer waren mit dieser Satzungsänderung stets einzeln vertretungsberechtigt. Im Juli 1998 schied Schäfer aus der Geschäftsführung aus, als neuer Geschäftsführer wurde stattdessen Carl von Gablenz berufen.

Im März 1999 wurde der Unternehmenssitz von Wiesbaden nach Krausnick verlegt und zudem erstmals ein Prokurist berufen. Mit einer Satzungsänderung Mitte des Jahres 2000 waren die beiden Geschäftsführer nicht mehr einzeln vertretungsberechtigt. Darüber hinaus wurden drei weitere Prokuristen benannt.

Im März 2000 erhielt die CargoLifter Development die Anerkennung als Entwicklungsbetrieb der Luft- und Raumfahrt durch das Luftfahrtbundesamt. Ein Novum war hierbei, dass das Unternehmen das erste nach "JAR-21, Subpart JA" genehmigte Unternehmen war. Die Anerkennungsurkunde trug daher die Nummer 001. Das Unternehmen hatte es zudem auch geschafft, mit nationalen wie internationalen Behörden sowie anderen Herstellern der Luftschifffahrt einen neuen Sicherheitskatalog als Standard zu schaffen, der als "Transport Airship Requirements (TAR)" festgeschrieben wurde.

Im Jahr 2001 schied von Gablenz als Geschäftsführer der Development GmbH aus. Als neuer Geschäftsführer ernannt wurden stattdessen Christoph von Kessel und Ralph Maurer.

Die CargoLifter Development GmbH war die erste Gesellschaft aus dem Verbund der Cargolifter AG, die Insolvenz anmeldete. Am 31. Mai 2002 wurde wegen drohender Zahlungsunfähigkeit ein Insolvenzantrag beim zuständigen Amtsgericht Cottbus gestellt. Zum 1. August 2002 wurde das Insolvenzverfahren offiziell eröffnet. Zum Zeitpunkt der Verfahrensanmeldung war das Unternehmen mit 283 Mitarbeitern die wichtigste und größte Tochter.

Im August 1997 wurde die CargoLifter Network GmbH in Frankfurt am Main als Tochtergesellschaft der CargoLifter AG gegründet. Zum Geschäftsführer wurde zunächst Karl Bangert berufen, der ab dem Frühjahr 1998 auch stellvertretender Vorstand der CargoLifter AG wurde und beide Tätigkeiten über die Unternehmenshistorie hinweg in Personalunion ausübte. Später wurde Dirk Steffes als zweiter Geschäftsführer berufen.

Der Geschäftszweck des Unternehmens bestand in der Planung und Realisierung des Aufbaus und späteren Betreibens eines weltweiten Netzwerkes von Infrastrukturen für die Luftschiffentwicklung, -konstruktion und den -betrieb. Daneben sollte die Gesellschaft Konzepte im Bereich Standortentwicklung und Logistik entwickeln sowie die Transportkapazitäten des CargoLifter-Luftschiffsystems vermarkten und Transportkapazitäten optimal verteilen.

Die CargoLifter AG hielt bis ins Jahr 2000 zunächst 51 % der Anteile an der Network GmbH. Weitere 49 % hielt das Unternehmen "GTS", an der zu 50 % Carl-Heinrich von Gablenz, zu 40 % Karl Bangert und zu 10 % Andreas Moder, dem Geschäftsführer der CargoLifter Communications, beteiligt waren. Vor dem Börsengang der Cargolifter AG wurde das Grundkapital der CL Network von 100.000 DM auf 5 Mio. DM angehoben und die Anteile der GTS gegen den Tausch von Aktien in die CargoLifter AG überführt.

Die CL World wurde laut ihres Geschäftszwecks zur Konzeption, Errichtung und den Betrieb von Besucherzentren und Themenparks an allen CargoLifter Standorten gegründet. Im Verlauf der Unternehmensgeschichte war das Unternehmen vor allem für die Vermarktung von Merchandising-Artikeln, die Planung von Veranstaltungen sowie die Erstellung von PR-Kampagnen und Unternehmensfilmen zuständig.

Der Filmemacher und Journalist Dirk Pohlmann war einer von zwei Geschäftsführern dieser in Berlin ansässigen Gesellschaft.

Das auf dem Werftstandort in Brand ansässige Besucherzentrum wurde von der CL World mit wirtschaftlichem Erfolg betrieben. Von der Eröffnung im Juni 2000 bis zum August 2002 besuchten über 400.000 zahlende Besucher den Werftstandort und nahmen an Werks- und Standortführungen teil.

Im April des Jahres 1998 wurde eine Tochtergesellschaft innerhalb der USA gegründet. Als Rechtsform wurde eine Kapitalgesellschaft auf Aktien, die Corporation, gewählt. Standort des Unternehmens war Raleigh in North Carolina.

Unternehmenszweck war vor allem, in Abstimmung mit der Muttergesellschaft alle Unternehmensaktivitäten in Amerika zu planen und durchzuführen.

Gegen Ende des Jahres 1999 gab die Tochtergesellschaft bekannt, dass sie bereits einen Standort für den Bau eines zweiten Werftareals und einer zweiten Luftschiffhalle für CargoLifter identifiziert habe: Im Pasquotank County unweit von Elizabeth City in North Carolina sei ein ideales Areal gefunden worden, auf dem zukünftige Nordamerikaflüge der Lastenluftschiffe hätten durchgeführt werden sollen. Das Management gab daher bekannt, Kaufverhandlungen für den Standort starten zu wollen. Nach einer neun-monatigen Due-Diligence wurde durch das Unternehmen offiziell bekanntgegeben, in der Nähe von New Bern, North Carolina einen Standort gefunden zu haben, der offiziell als Gelände für den Bau einer zweiten Werfthalle benannt wurde.

Darüber hinaus trat das Unternehmen auch als Käufer für die Konzernmutter in Aktion. Beispielsweise erwarb die Inc. das SkyShip 600 für einen Systempreis von 6,4 Mio. $ bei dem in Orlando (Florida) ansässigen Luftschiffhersteller "Airship Operations".

Die CL Airship Operations GmbH wurde Anfang Juli 2000 als 100-prozentige Tochter der CargoLifter AG gegründet. Hauptunternehmenszweck war insbesondere die Vorbereitung, Entwicklung und spätere Ausbildung von Luftschiffpiloten. Bei Unternehmensgründung wurde ursprünglich angekündigt, ab dem Jahr 2001 offiziell mit der Ausbildung von Piloten zu beginnen. Für den Geschäftszweck von CargoLifter wurde nach Unternehmensangaben geschätzt, dass mindestens 2000 Piloten bis zum Jahr 2015 nötig würden. Hierzu hätten pro Jahr 50 Luftschiffpiloten ausgebildet werden sollen, deren Ausbildungskosten bei rund 150.000 Euro gelegen hätten.

Anfang des Jahres 2000 wurde für das Unternehmen zudem ein SkyShip 600 als Schulungs-Luftschiffe erworben, um auf diesem die praktische Ausbildung von Piloten und das Training durchzuführen.

Die CL Finance GmbH und CL Finance B.V. wurden zur Durchführung von Finanzierungsmaßnahmen für die CargoLifter AG und alle weiteren Tochtergesellschaften gegründet.

Das Insolvenzverfahren über das Tochterunternehmen CL Finance wurde im Juli 2007 mangels Masse nicht eröffnet, sodass die Gesellschaft aufgelöst wurde.

An der Energieversorgung Brand hielt die CargoLifter AG 49 % der GmbH-Anteile. Die restlichen 51 % hielt die EWE AG, die auch für die Erdgasbelieferung der zwei auf dem Standort untergebrachten Blockheizkraftanlagen zuständig war. Anfang Oktober 2000 wurde die Energieversorgungszentrale in Betrieb genommen. Maximal ausgelegt war diese Anlage für die Produktion von 808 Kilowatt elektrische Energie und 1144 Kilowatt thermische Energie.
Kernaufgabe dieses auf dem Unternehmensstandort in Brand stationierten Unternehmens war es, den Standort mit Strom, Erd- und Flüssiggas sowie Wärme und Druckluft zu versorgen.<ref name="Geschäftsbericht 2000/2001, S. 13">"Geschäftsbericht 2000/2001." CargoLifter AG, S. 13, getthereport.com (PDF; 1,8 MB).</ref>
Die Energiezentrale wurde innerhalb von 5 Monaten für rund 7 Millionen DM errichtet.

Die CargoLifter AG erwarb im am 31. August 2001 beendeten Geschäftsjahr 50 % der Anteile dieser in Wiesbaden ansässigen Gesellschaft, die vornehmlich als Finanzinvestor und Vermittler von Aktien der CargoLifter AG aufgetreten war.<ref name="Geschäftsbericht 2000/2001, S. 79">"Geschäftsbericht 2000/2001." CargoLifter AG, S. 79, getthereport.com (PDF; 1,8 MB).</ref>

Die CL MAP steuerte Planungs- und Bauaktivitäten Konzerns und wurde Anfang Oktober 2000 in Frankfurt gegründet und durch zwei Geschäftsführer und zwei Prokuristen vertreten. Neben dem Hauptsitz in Frankfurt war das Unternehmen zudem mit einem Büro in München präsent. Der Gegenstand des Unternehmens war die Planung und Erbringung von Planungs- und Überwachungsleistungen, deren Vermittlung sowie die erforderlichen Beratungsleistungen zum Aufbau von Luftschiffstandorten in baulicher Hinsicht. Insbesondere war das Unternehmen maßgeblich bei der Planung und der Errichtung des Werftstandorts beteiligt und brachte sich mit Architekturwissen ein.

Im August 2008 wurde die Gesellschaft innerhalb des Insolvenzverfahrens der Konzernmutter aufgelöst.


Im Verlauf der Unternehmenshistorie hatte das Unternehmen CargoLifter nicht nur vor, ein Lastenluftschiff zu konstruieren. Parallel wurde ein Kielluftschiff kleinerer Größe selbst konzipiert und produziert. Im Jahr 2000 wurde zudem ein Lastenballon vorgestellt. Darüber hinaus erwarb das Unternehmen ein Prallluftschiff.

Carl-Heinrich von Gablenz, der Vorstand von CargoLifter, schrieb im November 1996 in einem Gastbeitrag für die Deutsche Verkehrszeitung davon, dass das Unternehmen unmittelbar nach der Unternehmensgründung damit begonnen habe, das für das CL-160-Lastenluftschiff angedachte Lastaustauschverfahren mit einem solarbetriebenen Luftschiff der Universität Stuttgart zu erproben. Bei diesem Luftschiff handelte es sich um ein unter dem Namen „Lotte 3“ konzipiertes Versuchsobjekt.

Bereits seit dem Jahr 1991 erforschten Studenten und Mitarbeiter der Universität Stuttgart Möglichkeiten und Grenzen von solarbetriebenen Luftschiffen. Tatsächlich gelang es dieser Projektgruppe in den folgenden Jahren, das weltweit erste solarbetriebene Luftschiff der Welt zu entwickeln, zu konstruieren und im operativen Einsatz zu betreiben. Das erste Luftschiffe dieser Bauart LOTTE, 10 m lang, wurde jedoch am 16. Juli 1993 durch Sturmböen während der Internationalen Gartenbauausstellung 1993 in Stuttgart durch einen gewitterbedingten Einsturz seiner Halle zerstört. „Lotte 2“, das zweite Luftschiff dieser Bauart, wurde im Jahr 1993 durch die Kollision mit einem Dornbusch während der in Australien stattfindenden World Solar Challenge zerstört.

Die dritte Version dieses nun auf 16 m Länge gewachsenen Luftschiffs wurde im Mai 1994 erstmals eingesetzt und verrichtete ab dem Winter 1996 für CargoLifter verschiedene Einsätze. Zu sehen war dieses Luftschiff noch während der im Jahr 1998 in Leipzig stattgefundenen Logistikmesse. Im Jahr 1999 wurde das Luftschiff von Unbekannten in Leipzig mutwillig beschädigt. Im Verlauf der Unternehmenshistorie wurde über den Einsatz und Verbleib des Luftschiffs kein weiterer öffentlicher Bericht abgegeben.

Im März des Jahres 2000 erwarb CargoLifter ein SkyShip-600-Luftschiff, um dieses für die Pilotenausbildung sowie Trainings- und Forschungszwecke einzusetzen. Bekannt gegeben wurde der Verkauf durch das in Orlando (Florida) ansässige US-Unternehmen "Airship Operations, Inc." Der Gesamtpreis i.H.v. 6,4 Mio. $ beinhaltete das Luftschiff sowie ein initiales Trainingspaket, mit dem CargoLifter bei der Zulassung nach deutschen Luftfahrtbestimmungen unterstützt werden sollte. Als Käufer des Luftschiffs trat die CargoLifter Inc. auf. Das Luftschiff wurde in einem Luftschiffhangar von einem Schwesterunternehmen der Airship Operations, der "Global Skyship Industries", in Cardington (Großbritannien) zusammengebaut und anschließend bei einem Überführungsflug über den englischen Kanal nach Deutschland geflogen.

Seinen ersten öffentlichen Einsatz hatte dieses Luftschiff für CargoLifter im Sommer des Jahres 2000. Vom 30. Juni bis zum 8. Juli 2000 fand in Friedrichshafen eine lange vorbereitete und von einer breiten Öffentlichkeit begleitete „Zeppelin-Jubiläumswoche“ statt, welche das 100-jährige Jubiläum des ersten Aufstiegs von Graf Ferdinand von Zeppelin mit seinem Luftschiff Zeppelin 1 feierte.
Am späteren Nachmittag des 2. Juli 2000 fand über dem Bodensee eine Luftschiffparade statt, an der das SkyShip 600 neben dem am gleichen Tag getauften Zeppelin NT sowie noch zwei weiteren Blimps des Unternehmens The Lightship Group teilnahmen.

Das Luftschiff wurde während des "Torfests" am 16. September 2000 durch Bundesverkehrsminister Reinhard Klimmt auf den Namen „Charly“ getauft.

Nachdem die Unternehmensführung CargoLifters 13 mögliche Standorte für den Bau des notwendigen Luftschiffhangars evaluiert hatte, begannen im September 1997 erste Verhandlungen für den Kauf eines Grundstücks mit der landeseigenen brandenburgischen Bodengesellschaft. Im September 1998 wurde sodann dieses in Brandenburg befindliche Areal von der Landesregierung erworben. Dieses 580 Hektar große Gelände befindet sich bei Briesen/Brand, einem südlichen Ortsteil der Gemeinde Halbe, im Landkreis Dahme-Spreewald, etwa 60 Kilometer südlich des Zentrums und etwa 35 Kilometer südlich der Stadtgrenze von Berlin.

Das Areal, bis dato als Flugplatz Brand bekannt, wurde schon zu Zeiten des Zweiten Weltkriegs für die Luftwaffe der Wehrmacht erschlossen. Nach dem Krieg erfolgte eine Weiternutzung und ein Ausbau durch die Luftstreitkräfte der Roten Armee. Verwendet wurde der Flughafen mit seinen drei Landebahnen bis zum Abzug der sowjetischen Streitkräfte im Jahr 1990. Die Übergabe an die deutsche Verwaltung erfolgte im Jahr 1992.

Im April 1998 verkündete der CargoLifter-Vorstandsvorsitzende Carl von Gablenz der Presse, dass von den Hausbanken die Zusagen für Finanzierung des Areals und die Bebauung vorlägen. Obgleich der Kaufvertrag für das Gelände zu dem Zeitpunkt noch gar nicht unterschrieben war, wurde am ersten Maiwochenende 1998 der erste Spatenstich für den Bau der Werfthalle während eines vom Unternehmen veranstalteten Volksfests getätigt, sowie der bevorstehende Erhalt der Baugenehmigung und der Baubeginn der Halle für Mitte Mai angekündigt.

Tatsächlich wurde mit den Bauarbeiten jedoch erst ein Jahr später im März 1999 begonnen.

Nach Unterzeichnung des Kaufvertrages im August 1998 begann CargoLifter zunächst das Gelände zu erschließen. Insbesondere mussten sowjetische Altlasten entsorgt, Teile der ehemaligen Kasernen abgerissen und weitere Elemente der verbliebenen Militärinfrastruktur rückgebaut werden. Zunächst erhalten blieben 39 alte Flugzeug-Shelter, die mit neuen Torsystemen ausgestattet wurden und teils als Lager verwendet wurden. Zwei dieser Shelter wurden später zum Besucherzentrum und IT-Serverraum ausgebaut und dementsprechend verwendet.

Ebenfalls rückgebaut wurde die nördliche Start- und Landebahn des Areals. Die frei gewordene Fläche diente hierbei nicht nur als Ausgleichsfläche für den geplanten Hallenbau, vielmehr wurde der zerkleinerte Beton dieser Bahn als Unterbau für den Hangarbau verwendet.

Im Sommer des Jahres 1999 wurde nach 24 Entwurfsrunden der Masterplan für den Luftschiffhallenbau festgeschrieben, sodass mit den Bauarbeiten tatsächlich begonnen wurde. Mitte Oktober 1999 war der erste von fünf Bögen der Halle aufgebaut, was das Unternehmen mit einem als “Bogenfest” bezeichneten Richtfest und 25.000 Besuchern feierte.

Im Mai 2000 startete die Eindeckung der 40.000 m² großen Hallen-Membrane, wozu 120 Industriekletterer eingesetzt wurden. Erstmals regendicht abgeschlossen war die Halle im September 2000, nachdem sowohl die Membrane als auch die Hallentore installiert worden waren. Die Fertigstellung des letzten Hallentores wurde dabei durch das Unternehmen mit einem als „Torfest“ bezeichneten Festakt gefeiert.

Die Bauarbeiten an der Luftschiffwerft wurden im November 2000 abgeschlossen, was das Unternehmen mit zwei groß angelegten Veranstaltungen feierte. Neben einer Veranstaltung für geladene Gäste aus Industrie und Politik wurde in einer Samstagsabendveranstaltung Ende November auch die Öffentlichkeit in die Halle gelassen. Trotz Eintrittspreisen von 150 DM für Unternehmensfremde und 120 DM für Aktionäre, nahmen an dieser Veranstaltung rund 10.000 Besucher teil".

Die Werfthalle wurde ursprünglich konzipiert für die gleichzeitige Produktion von zwei CL-160-Luftschiffen oder die Produktion eines bei der parallelen Wartung eines zweiten Schiffes. Für den Ein- und Aushallvorgang von fertigen oder zur Wartung anstehenden Luftschiffen wurde an den Stirnseiten der Halle Schalentore in viertelkreisförmiger Dimension angebracht. Möglichen Verformungen wird durch Tordichtungen in Federblatt-Konstruktion vorgebeugt, die jeweils 150 m Länge aufweisen. Ein Öffnungsvorgang der Tore bei auf der Halle aufliegender Schneelast bleibt untersagt, da hierbei zu starke Verformungen der Tore auftreten könnten.

Der Bau des Luftschiffhangars und die Bebauung des Areals wurden von der "SIAT GmbH & Co. KG" geplant, einem ehemaligen Architekturbüro und Tochterunternehmen der Siemens AG. Technisch realisiert wurde die Konstruktion des Hallenbaus durch ein Baukonsortium unter der Federführung des Berliner Büros der Hochtief AG. Weitere an dem Konsortium beteiligte Unternehmen waren beispielsweise die DSD Dillinger Stahlbau sowie die Max Bögl Bauunternehmung.

An dem Gebäude und seiner technischen Spezifikation wurde – trotz seiner imposanten Erscheinungsweise – Kritik geäußert:


Neben der Werfthalle wurden auf dem Standort noch weitere Gebäude errichtet. Zentral dienten diese dem geplanten Geschäftszweck CargoLifters, der Entwicklung und Konstruktion und dem späteren Betrieb von Lastenluftschiffen.



Über diese tatsächlich gebauten Gebäude hinweg sah der Master-Bebauungsplan des Areals vor, zwei Ankermasten für Luftschiffe zu konstruieren. Durch die Insolvenz des Unternehmens und die niemals aufgenommene Produktion von Luftschiffen wurde die Konstruktion dieser jedoch nie in Angriff genommen.

Am 11. Juni 2003 wurde die Werfthalle für 17,5 Millionen Euro an den Tanjong-Konzern aus Malaysia verkauft, der vom Land Brandenburg 10 Millionen als Subvention erhielt und das Gebäude daraufhin zu einem tropischen Freizeitpark umwidmete.

Die "Initiative Zukunft in Brand e. V." (IZiB)
ist eine im Jahr 2003 von CargoLifter-Aktionären gegründete Interessengemeinschaft, die sich für den Erhalt des Unternehmens und die Interessen der Aktionäre einsetzt. Ein wesentliches Ziel des Vereins ist es, Rechte und Interessen von Cargolifter-Aktionären zu verteidigen. Darüber hinaus soll der Fortbestand der Leichter-als-Luft-Technologie aktiv unterstützt werden.


1997

1999

2002

2004

2005

2007

2009

2010

2012






</doc>
<doc id="12038" url="https://de.wikipedia.org/wiki?curid=12038" title="Umgangssprache">
Umgangssprache

Die Umgangssprache, auch Alltagssprache, ist – im Gegensatz zur Standardsprache und auch zur Fachsprache – die Sprache, die im täglichen Umgang benutzt wird, aber keinem spezifischen Soziolekt entspricht. Ein Dialekt kann als Umgangssprache betrachtet werden, oder diese nimmt eine Zwischenstellung zwischen Dialekt und Standardsprache ein. Der Begriff ist nur unscharf zu den Begriffen Gemeinsprache und Gebrauchssprache abgrenzbar. In Bezug auf bestimmte Situationen stellen Verkehrssprachen die Umgangssprache dar.

Der Begriff "Umgangssprache" hat auch die Bedeutung „nachlässige, saloppe bis derbe Ausdrucksweise“. Dabei wird vor allem nach Sprachstil unterschieden und die Umgangssprache in Gegensatz zu einer gepflegten Ausdrucksweise gesetzt. Es wird hingegen nicht berücksichtigt, ob die Ausdrucksweise einem spezifischen Soziolekt entspricht oder nicht. Kennzeichen der Umgangssprache in dieser Bedeutung sind Kolloquialismen.

Die Umgangssprache im ersten Sinn wird geprägt durch regionale und soziale Gegebenheiten wie dem Bildungs­stand und dem sozialen Milieu der Sprechenden oder der Situation. Mitunter werden umgangssprachliche Ausdrucksformen auch synonym als „volksmundlich“ (in der Bedeutung von „Volksmund“) bezeichnet.

Der Begriff „Umgangssprache“ wurde zu Beginn des 19. Jahrhunderts von Joachim Heinrich Campe in die deutsche Philologie eingeführt.

Im deutschen Sprachraum gibt es keine standardisierte Hochsprache, die als Umgangssprache dient. Die lang andauernde historische Vielfalt regionaler Herrschaftsverhältnisse hat ihre Spuren in einem stark heterogenen (nicht standardisierten) umgangssprachlichen Sprechverhalten hinterlassen.

Weder ist die Hochsprache verbindlich festgelegt noch sind umgangssprachliche Abweichungen hiervon verbindlich abgegrenzt. Es gibt keine staatlichen Institutionen der deutschsprachigen Länder, die dafür zuständig sein könnten. Der Normierung der hochdeutschen Standardsprache (Standarddeutsch) hat sich hier aber der Verlag Brockhaus verschrieben, der in Zusammenarbeit mit staatlichen und nichtstaatlichen Stellen unter dem Markennamen Duden Wörterbücher herausgibt. Sie erscheinen seit dem späten 19. Jahrhundert. Die Orientierung an Schreibformen des Dudens, beispielsweise für den Schulunterricht oder in den Druckmedien, ist eine freiwillige Entscheidung der Kultusminister der Länder, der sonstigen staatlichen Behörden und der Verlagshäuser (vgl. Rechtschreibreform). Darum kann nicht von einer verbindlichen Norm in der Hochsprache gegenüber einer fehlenden Norm in der Umgangssprache gesprochen werden.

Auch die nicht standardisierte Umgangssprache unterliegt einer gewissen Einheitlichkeit, die dadurch entsteht, dass sich ihre Sprecher an anderen Sprechern orientieren und sich anpassen. Im Unterschied zur hochdeutschen Standardsprache, bei der die schriftliche Orientierung meist an Wörterbüchern erfolgt, ist die vereinheitlichende Orientierung der verschriftlichten Umgangssprache diffus, wechselhaft und oft nicht eindeutig zu ermitteln. Diese Unschärfe ist jedoch gleichzeitig die Quelle für ihren lebendigen Wortreichtum, der besonders für die Fortentwicklung der Standardsprache wichtig ist.

In der öffentlichen Wahrnehmung nimmt man öfter eine für die Sprachentwicklung als charismatisch geltende Sprachform als Ausgangsmaterial für die später sogenannten Hoch- und Umgangssprachen an. In Deutschland wird dies der Bibel­übersetzung Martin Luthers nachgesagt, in Großbritannien dem Englisch des Königshauses, in Frankreich der Umgangssprache der Region von Paris, in Russland dem Werk des Nationaldichters Alexander Sergejewitsch Puschkin.

In der Philosophie gilt nach einem Ausspruch von Karl-Otto Apel die Umgangssprache als „letzte Metasprache“ und als solche notwendig für die Metakommunikation, weil man ihr den geringsten Abstand zum individuellen Bewusstsein (im Sinne einer lingua mentis) unterstellt.

Die Umgangssprache unterscheidet sich von der gehobenen Sprache, von öffentlicher Rede, Drama, Gedicht, aber auch dem Lexikon­artikel sowie der Zwischenschicht von populärer gehobener Umgangssprache (Essay, Zeitungs­artikel, Bildungssprache Rundfunk- oder Fernsehsprache beziehungsweise „Fernsehdeutsch“). Dabei gilt das Primat der gesprochenen Sprache, d. h., Neubildungen und Feststellen der Korrektheit findet zunächst in konkreten Sprechsituationen Akzeptanz, die Verschriftlichung erfolgt im Allgemeinen mit einem gewissen Abstand.

Diskrepanzen zwischen der Umgangssprache und Fachsprachen sind nicht einheitlich. Sie sind vielmehr situations- und kontextabhängig. Es gibt unzweideutige, klar definierte Unterschiede, wegen unterschiedlicher Werte zwischen bestimmten Berufsgruppenangehörigen und Laien: Das Auseinanderklaffen heißt abwertend auch "déformation professionnelle" (etwa: „Fachidiotie“). Beispielsweise ist ein medizinischer Befund für die Fachperson „negativ“, wenn er eine bestimmte Diagnose ausschließt, der Laie hört aber die umgangssprachliche Bedeutung von „negativ“(= schlecht, unerwünscht) und vermutet, dass eine Erkrankung festgestellt wurde.

Der Prozess der Bildung, Fortentwicklung und Pflege einer Hochsprache beruht heutzutage in vielen Ländern auf einer ständigen Beobachtung der lebendigen Umgangssprache durch kulturelle Institutionen. Diese haben sich der Aufgabe selbst verschrieben, z. B. der Dudenverlag, oder sind staatlicherseits beauftragt, z. B. Kulturinstitute wie die Académie française oder die Accademia della Crusca. Für das Englische fehlt eine vergleichbare Einrichtung, abgesehen von einer gewissen Autorität der Ausdrucksweisen des britischen Königshauses oder von Absolventen der namhaften Universitäten.

Je nach nationaler Geschichte entwickelten sich Schrift- und Hochsprachen in den modernen Staaten höchst verschieden. Dementsprechend unterscheiden sich auch die Bewertung des Stellenwerts der Umgangssprache und der Einfluss der für die Gestaltung der Hochsprache zuständigen Institutionen.

Die gesteigerte Mobilität und die Massenmedien schmälern die Zahl der Sprecher von Mundarten und Dialekte kontinuierlich. Zugleich nimmt der Regionalcharakter umgangssprachlicher Elemente ab, d. h., die Umgangssprache wird standardisiert.

Höhere Mobilität, Fremdenverkehr, Massenmedien, EDV, U-Musik und anderes beschleunigen heute die alltägliche Sprachentwicklung. Andererseits "verlangsamen" normierende Wirkungen des Fernsehens und aufgelockerte Dialektgrenzen den Wandel auch etwas.

Ohnehin lehnt sich die formelle Beschreibung einer Sprache an die Umgangssprache an. Die Hochsprache nimmt Elemente aus der Umgangssprache auf und verändert ihren Sprachgebrauch gegebenenfalls mit ihr, meist mit einer gewissen Verzögerung und nur zu einem geringen Teil. Anhand der lexischen Unterschiede zwischen beiden Sprachformen lassen sich oft Regeln der Entstehung von Wörtern gut beobachten, zum Beispiel wenn aus dem deutschen Wort „Lokomotive“ auch in der Schriftsprache allmählich die „Lok“ geworden ist. Dies ist zugleich ein Beispiel dafür, dass diese Art des Sprachwandels die Sprache unsystematischer machen kann, denn aussprachegerecht wäre die Schreibung „Lock“ naheliegender.

Stets prägen insbesondere Jugendsprache und andere Szenesprachen die Umgangssprache der folgenden Generation – wesentlich mehr als die auf speziellere Gruppen beschränkte etwa Soldatensprache, Gefängnissprache, Studentensprache, Bergmannssprache, Jägersprache, Fachsprachen usw. Die Umgangssprache erneuert sich also immer wieder u. a. aus den Soziolekten und spiegelt dabei die kulturelle Bedeutung wider, die die Sprechergemeinschaft insgesamt den entsprechenden Gruppen zubilligt.

Die Entwicklung einer Hoch- oder Standardsprache verlief in den Niederlanden anders als in Deutschland und in der Schweiz.

In den Niederlanden wurde eine umgangssprachliche Varietät des Niederfränkischen, das Teil des niederdeutschen Dialektkontinuums ist, seit dem 13. Jahrhundert in einem mehrere Jahrhunderte dauernden Prozess allmählich zur Hochsprache ausgebaut.

In Deutschland ging mit dem Ende der Hanse das durch diese weitgehend standardisierte Niederdeutsche zurück; heute ist jede Varietät des „Plattdeutschen“ Umgangssprache und Dialekt. Das hochsprachliche Niederländisch, das eine größere sprachliche Nähe zum Plattdeutschen hat als Hochdeutsch, ist deshalb für Zuhörer mit Kenntnissen des Plattdeutschen umgangssprachlich vertraut.

Einen mit den Niederlanden vergleichbaren Schritt hat die Schweiz unterlassen: Sie hat weder eine umgangssprachliche noch eine dialektale Varietät standardisiert und zur Standardsprache ausgebildet. Man spricht von einem Fehlen eines Dialekt-Standard-Kontinuums. Offizielle Sprache in der Deutschschweiz ist das Schweizer Hochdeutsch, eine Varietät des Standarddeutschen mit einigen lokalen Eigenheiten, den Helvetismen.




</doc>
<doc id="12039" url="https://de.wikipedia.org/wiki?curid=12039" title="Synapse">
Synapse

Synapse (von griech. "syn" ’zusammen‘; "haptein" ’greifen, fassen, tasten‘) bezeichnet die Stelle einer neuronalen Verknüpfung, über die eine Nervenzelle in Kontakt zu einer anderen Zelle steht – einer Sinneszelle, Muskelzelle, Drüsenzelle oder einer anderen Nervenzelle. Synapsen dienen der Übertragung von Erregung, erlauben aber auch die Modulation der Signalübertragung, und sie vermögen darüber hinaus durch anpassende Veränderungen Information zu speichern. Die Anzahl der Synapsen beträgt im Gehirn eines Erwachsenen etwa 100 Billionen (10) – bezogen auf ein einzelnes Neuron schwankt sie zwischen 1 und 200.000.

Der Ausdruck "Synapse" wurde 1897 von Charles S. Sherrington geprägt für die Verknüpfung zwischen Neuronen, beispielsweise zwischen dem aufgezweigten Ende des Axons einer Nervenzelle und dem verästelten Dendriten einer anderen Nervenzelle.

In den meisten Fällen sind es chemische Synapsen. Bei ihnen wird das Signal, das als elektrisches Aktionspotential ankommt, in ein chemisches Signal umgewandelt, in dieser Form über den zwischen den Zellen bestehenden synaptischen Spalt getragen, und dann wieder in ein elektrisches Signal umgebildet. Dabei schüttet die sendende Zelle ("präsynaptisch") Botenstoffe aus, Neurotransmitter, die sich auf der anderen Seite des Spaltes ("postsynaptisch") an Membranrezeptoren der empfangenden Zelle binden. Hierdurch ist die Richtung der Signalübertragung (nur vorwärts) anatomisch festgelegt, was für die Verarbeitung von Information in neuronalen Netzen grundlegend ist. Der erregungsübertragende Transmitter wird entweder in der Endigung des Axons des sendenden Neurons gebildet oder in dessen Zellkörper synthetisiert und axonal zu den präsynaptischen Membranregionen transportiert.

Dagegen sind elektrische Synapsen als gap junctions Kontaktstellen, bei denen Ionenkanäle zweier Zellen unmittelbar aneinander koppeln und so einen Übergang von Ionen und kleinen Molekülen von einer Zelle zur anderen erlauben. Zuerst wurden solche Synapsen zwischen Neuronen entdeckt, doch kommen ähnliche Kontaktstellen noch in anderen Geweben vor, auch in Pflanzen.

In übertragenem Sinn werden als immunologische Synapsen die Stellen vorübergehender zellulärer Kontakte von Zellen des Immunsystems bezeichnet, sowohl untereinander als auch mit Zellen des umgebenden Gewebes. Dabei binden Moleküle auf der Oberfläche der einen Zelle an Rezeptormoleküle und Adhäsionsmoleküle in der Zellmembran der anderen und tauschen darüber Informationen aus.

In einem Synapsenendknöpfchen führt das eintreffende Aktionspotential schon während der Depolarisationsphase – neben der kurzzeitigen Öffnung von Natrium- und etwas verzögert auch von Kalium-Ionenkanälen – zur vorübergehenden Öffnung spannungsaktivierter Calcium-Ionenkanäle und damit zu einem kurzdauernden Calciumioneneinstrom. Das intrazellulär erhöhte Calcium bewirkt innerhalb weniger Millisekunden die Ausschüttung eines Botenstoffs in den synaptischen Spalt. Im Endknöpfchen wird dieser Neurotransmitter in besonderen synaptischen Bläschen vorrätig gehalten und nahe der Zellmembran in synaptischen Vesikeln bereitgestellt, die unter Einwirkung von Calcium mit der "präsynaptischen" Membran verschmelzen können und sich dann nach außen hin entleerend so die Transmittermoleküle freisetzen.

Dieser Vorgang, der auch Exozytose genannt wird, wird erst durch die Konformationsänderung von Calcium-bindenden Proteinen möglich, insbesondere von "Synaptotagminen". Sie stoßen die Bildung eines Proteinkomplexes aus SNARE-Proteinen an – aus einem "Synaptobrevin" in der Vesikelmembran einerseits sowie andererseits in der Zellmembran einem "Syntaxin" und zwei SNAP-Proteinen – der die Fusion beider Membranen erlaubt. Weitere Proteine sind dann daran beteiligt, die Öffnung des fusionierten Vesikels nach extrazellulär zu veranlassen und, wie beispielsweise "Complexin" I und II, die Ausschüttung der Neurotransmitter zu beschleunigen. Anschließend wird über "Synapsin" erneut eine bestimmte Anzahl synaptischer Vesikel am Axolemm bereitgestellt.

Auf der anderen Seite des synaptischen Spalts finden sich in der "postsynaptischen", "subsynaptischen", Membran der Zielzelle spezifische Rezeptormoleküle für den Neurotransmitter. Diese Rezeptoren sind zumeist mit ligandengesteuerten Ionenkanälen assoziiert (ionotrop), so dass sich unmittelbar ein Ionenkanal öffnen kann, wenn das Transmittermolekül an den passenden Rezeptor bindet. Je nach der Ionensorte, für welche dieser Kanal durchlässig ist, wird das Membranpotential in der postsynaptischen Region durch den Ionenstrom dann entweder angehoben (EPSP) oder aber abgesenkt (IPSP). Abhängig vom Rezeptortyp kann daneben mittelbar auch eine sogenannte Second-Messenger-Kaskade ausgelöst werden (metabotrop), die ebenfalls zu einer Änderung des Membranpotentials führen kann und darüber hinaus unter Umständen noch weitere Vorgänge in der postsynaptischen Zelle veranlasst. So kann – vermittelt durch den jeweiligen intrazellulären Botenstoff – auch eine Signalverstärkung hervorgerufen werden, allerdings erst mit verzögerter Wirkung.

Die Transmittermoleküle binden nicht irreversibel, sondern lösen sich nach einer gewissen Zeit wieder von ihrem Rezeptor. Im synaptischen Spalt beziehungsweise im Extrazellularraum werden sie oft durch besondere Enzyme (wie z. B. Acetylcholinesterase) abgebaut und damit in ihrer Wirkung begrenzt. Bei einigen Transmittern erfolgt kein Abbau, sondern sie werden wieder in die präsynaptische Endigung aufgenommen (beispielsweise Serotonin) oder von Gliazellen abgeräumt.

Die über chemische Synapsen übertragenen Signale haben eine biochemisch festgelegte Wirkung. Je nach Ausstattung der postsynaptischen Membran, auf die das sendende Neuron Einfluss nimmt, wird entweder eine erregende ("exzitatorische") oder aber eine hemmende ("inhibitorische") Wirkung erzielt. Nicht nur einzelne Synapsen, ganze Neuronen werden daher in exzitatorische und inhibitorische unterschieden, je nachdem ob sie nur erregende oder nur hemmende Synapsen an Zielzellen ausbilden. Für eine Zielzelle innerhalb des zentralen Nervensystems ist es gewöhnlich so, dass sie von verschiedenen Neuronen Signale erhält, auch gegensätzliche, und dass sich die von ihnen ausgelösten elektrischen Spannungsänderungen addieren. Überschreitet die Summe der einlaufenden exzitatorischen und inhibitorischen (postsynaptischen) Spannungsänderungen am Axonhügel dieser Nervenzelle einen bestimmten Schwellenwert bei der Potentialänderung, so wird diese Zelle ihrerseits aktiv, bildet ein Aktionspotential und leitet es über ihr Axon weiter.

Bei einer Vielzahl von psychiatrischen und neurologischen Erkrankungen wird davon ausgegangen, dass synaptische Übertragungswege gestört sind. So gibt es Anzeichen für einen Zusammenhang zwischen verschiedenen Formen von Depression und Störungen von Signalübertragungen durch den Neurotransmitter Serotonin.

Zahlreiche Medikamente oder Giftstoffe entfalten ihre Wirkung durch eine Interaktion mit Schritten der Transmission an Synapsen (Betablocker, Nicotin, Atropin, Hyoscyamin, Parathion, Kokain und viele mehr).

Die Mehrzahl der Synapsen arbeitet mit einer chemischen Informationsübertragung, doch in einigen Fällen gibt es auch eine unmittelbare elektrische Weiterleitung. In diesen elektrischen Synapsen wird das Aktionspotential direkt und ohne vermittelnde Neurotransmitter an die nachfolgende Zelle weitergegeben.

Bei vielen elektrischen Synapsen findet man Verbindungskanäle durch die Zellmembran, „gap junctions“ genannt, über welche die Intrazellulärräume unmittelbar aneinander grenzender Zellen miteinander gekoppelt sind. Diese gap junctions sind Poren in der Zellmembran, die durch bestimmte Proteine, sogenannte Connexine, gebildet werden. Sechs Connexin-Moleküle kleiden dabei die Pore einer Zelle aus, zusammen bilden sie ein Connexon. Durch den Kontakt zwischen zwei Connexonen von benachbarten Zellen entsteht dann ein Kanal, der die Membranen durchquert und beide verbindet. Die offene Verbindung erlaubt eine Diffusion selbst mittelgroßer Moleküle, z. B. sekundärer Botenstoffe, und ermöglicht über Ionenpassagen eine sehr rasche Übertragung von Änderungen des Membranpotentials bei relativ geringem elektrischen Widerstand. Solche elektrischen Synapsen kommen beispielsweise zwischen Neuronen der Retina vor; sie finden sich auch zwischen Gliazellen und insbesondere zwischen Zellen des Herzmuskels, die so elektrisch zu einer gemeinsamen Einheit gekoppelt synchronisiert agieren können, ähnlich auch bei glatter Muskulatur wie dem Uterus.

Eine weitere Form der elektrischen Erregungsübertragung ist die der kapazitiven Kopplung über einen großflächigen engen Membrankontakt, wie sie beispielsweise im menschlichen Ziliarganglion zu finden ist.

Synapsen können des Weiteren nach verschiedenen Gesichtspunkten unterschieden werden, beispielsweise






Chemische Synapsen arbeiten mit unterschiedlichen Transmittern und können durch Medikamente oder Drogen in verschiedenen Schritten der Signalübermittlung verändert werden, womit je nach Angriffsort und Vorbedingungen unterschiedliche Wirkungen zu erreichen sind. Differenziertere Funktionen des Nervensystems lassen sich damit jedoch nicht gezielt beeinflussen, da diese nicht vom Überträgerstoff, sondern vom Verknüpfungsmuster der Synapsen abhängen.

Chemische "Synapsengifte" stören oder unterbinden die Funktion von Synapsen. Sie können die Abgabe der Neurotransmitter in den synaptischen Spalt blockieren oder den Neurotransmittern so ähnlich sein, dass sie an deren Stelle an die Rezeptormoleküle in der postsynaptischen Membran binden und damit die Erregungsübertragung stören. Je nach Bindungsweise an den Rezeptor kann damit allein ein Platz besetzt werden oder aber darüber hinaus auch eine ähnliche Wirkung erreicht werden wie durch den eigentlichen Transmitter. Nach dem erzielten Effekt werden daher Substanzen mit ähnlicher Wirkungsaktivität als Agonisten bezeichnet und unterschieden von Antagonisten mit allein der Aktivität, Agonisten in der Wirkung zu hemmen – beispielsweise indem sie deren Platz einnehmen.

Zu den bekanntesten Substanzen mit störendem Einfluss auf die synaptische Transmission gehören zahlreiche giftige Alkaloide von Pflanzen wie Atropin, Nicotin, Mescalin, Curare oder von Pilzen, etwa die des Mutterkorns oder Muskarin. Doch auch der Trinkalkohol beeinflusst die Übertragung an Synapsen, verändert z. B. GABA-Rezeptoren und blockiert (NMDA)-Glutamat-Rezeptoren.
Ein schon in sehr geringer Dosis wirksames Gift ist das von einer Bakterienart der Clostridien gebildete Botulinumtoxin (Botulin) – dessen lähmende Wirkung kosmetisch zum Faltenglätten benutzt wird – und das ihm ähnliche Tetanustoxin. Zu den von Tieren gebildeten Nervengiften gehören beispielsweise die Conotoxine maritimer Kegelschnecken und die Gifte verschiedener Spinnenarten, so die Latrotoxine der dreizehnfleckigen Schwarzen Witwe.
Synthetische Synapsengifte sind die chemischen Kampfstoffe Tabun, Sarin und VX und ebenso zahlreiche Insektizide, etwa E 605 oder Neonicotinoide, sowie verschiedene Halluzinogene wie LSD und andere – und selbstverständlich Psychopharmaka.

Die Übertragungsfähigkeit von Synapsen unterliegt anatomischen Veränderungsprozessen. Diese sind die Grundlagen von Lernen und werden unter Synaptische Plastizität beschrieben.




</doc>
<doc id="12049" url="https://de.wikipedia.org/wiki?curid=12049" title="Regensburg">
Regensburg

Regensburg (von "Castra Regina;" auch lat. "Ratisbona" und "Ratispona") ist die Hauptstadt des Regierungsbezirks Oberpfalz mit Sitz der Regierung der Oberpfalz wie auch des Landrats des Landkreises Regensburg und eine kreisfreie Stadt in Ostbayern. Seit dem 13. Juli 2006 gehört die weitgehend erhaltene Regensburger Altstadt samt Stadtamhof mit ihren historischen Ensembles und Baudenkmälern zum UNESCO-Welterbe.

Die Stadt hat Einwohner und steht damit nach München, Nürnberg und Augsburg an vierter Stelle unter den Großstädten des Freistaates Bayern.

Sie ist Bischofssitz der Diözese Regensburg, hat drei Hochschulen und ist eines der 23 bayerischen Oberzentren.

Wirtschaftlich ist Regensburg stark vom verarbeitenden Gewerbe (Automobilbau, Maschinenbau, Elektrotechnik, Mikroelektronik) geprägt. Die Arbeitslosigkeit liegt unter dem bayerischen Landesdurchschnitt (Januar 2018: 2,7 %; Landesdurchschnitt im November 2017: 2,9 %). Mit 760 sozialversicherten Beschäftigten je 1000 Einwohner hat Regensburg eine hohe Arbeitsplatzdichte.

Regensburg liegt am nördlichsten Punkt der Donau und an den Mündungen der linken Nebenflüsse Naab und Regen. Im Stadtgebiet liegen zwei Donauinseln, der Obere Wöhrd (mit dem östlichen Zipfel "Jahninsel" unterhalb der Steinernen Brücke) und der Untere Wöhrd. Der Stadtteil Stadtamhof gehörte ursprünglich zum nördlichen Uferbereich der Donau, er wurde durch den Bau des Europakanals, einer Schifffahrtsumgehung der Altstadt, ebenfalls zu einer Insel. Im Stadtgebiet stoßen vier sehr unterschiedliche Natur-Großräume aneinander:


Diese Naturräume prägen den Stadtkörper bereits in seiner Form und Ausdehnung, die Stadt befindet sich in einer klassischen „Pfortenlage“ an einer Übergangsstelle zwischen topographischer Enge und Weite. Die Donau verlässt dort das Hügel- und Bergland und fließt in die Gäubodenebene.

Daraus ergibt sich für einige Stadtteile im Norden und im Westen kein oder wenig Außenerweiterungspotenzial. Alle heutigen und künftigen Stadterweiterungsgebiete liegen im Osten und Süden, also in der großen Donauebene und den relativ flachen Ausläufern des niederbayerischen Tertiär-Hügellandes.

In Regensburg endet die historische Bayerische Eisenstraße, auf der die Metallprodukte der Oberpfalz zum Umschlagplatz an der Donau gebracht wurden. Sie verläuft über 120 km von Pegnitz her, doch bei Amberg wurde das Halbzeug auf Kähne verladen. Die Besichtigungs- und Ferienstraße verbindet zahlreiche Industrie- und Kulturdenkmäler.

Folgende Städte und Gemeinden, die zum Landkreis Regensburg gehören, grenzen an die Stadt Regensburg. Sie werden nach dem Uhrzeigersinn beginnend im Norden genannt: Lappersdorf, Zeitlarn, Wenzenbach, Tegernheim, Barbing, Neutraubling, Obertraubling, Pentling, Sinzing und Pettendorf.

Regensburg hat im Gegensatz zu vielen anderen Städten einen relativ kompakten Siedlungskörper. Dies führt zwar zum einen zu einer „Stadt der kurzen Wege“, zum anderen wirkt die Gesamtstadt – mit Ausnahme der klar ablesbaren Altstadt, des Universitäts-Grünzuges und der Wöhrde (Donauinseln) – jedoch als eine einzige, fast schon zu homogene Siedlungsfläche. Zu den wesentlichen inneren Gliederungselementen gehören die relativ großen Grünanlagen der Stadt, der Altstadtkern und die großen Infrastrukturelemente (Wasserflächen, Bahnlinien, Autobahnen und Bundesstraßen).

Zu Anfang des 19. Jahrhunderts beschränkte sich Regensburg weitgehend auf das Gebiet des alten Römerlagers und einiger weiterer umfriedeter Gebiete im Westen. Mit der Eingemeindung von "Kumpfmühl" erfolgte 1810/18 ein Sprung über die heutige Bahnlinie in Richtung Süden. Von Beginn des 20. Jahrhunderts vergrößerte sich Regensburg in einer Serie von Eingemeindungen, besonders in den Jahren 1924, 1938 und 1977. Für die genaue Auflistung der Stadtbezirke, ihrer Fläche und der Folge der Eingemeindungen einschließlich der hinzugewonnenen Fläche siehe Einwohnerzahl und Fläche von Regensburg.

Die Stadt befindet sich in der gemäßigten Klimazone mit kontinentalem Einschlag. Das Regensburger Klima zeichnet sich insbesondere durch stabile und trockene Sommer aus und unterscheidet sich damit vom im Sommer regenreichen Klima des Voralpenlandes. Für den im Voralpenland üblichen Föhn ist Regensburg die nördliche Grenze. Er gibt vom Keilberg aus den Blick auf die Alpen frei, sein Auftreten ist außerordentlich selten. Im Gegensatz dazu halten sich im Herbst und Winter sehr lange beständige Nebel und Hochnebel, wobei längere Perioden mit einer geschlossenen Schneedecke selten sind. Bedingt durch seine Talkessellage litt Regensburg in früheren Zeiten häufig unter Smog. Die durchschnittliche Jahrestemperatur beträgt 8,0 Grad Celsius, die mittlere jährliche Niederschlagsmenge 646 Millimeter. Damit liegt Regensburg am unteren Rand der Städte Bayerns.

Die wärmsten Monate sind Juni bis August mit durchschnittlich 16,2 bis 18,0 Grad Celsius und die kältesten Dezember bis Februar mit −0,9 bis −2,7 Grad Celsius im Mittel.

Die größte Niederschlagsmenge fällt von Juni bis August mit durchschnittlich 74 bis 93 Millimeter, die geringste im März und November mit durchschnittlich 33 bis 39 Millimeter.

Erste Siedlungsspuren reichen bis um 5000 vor Christus zurück. Regensburg kann mit der Einrichtung eines römischen Lagers im Jahr 179 eine frühe Ersterwähnung durch den Kaiser Mark Aurel nachweisen. Im Laufe der Jahrhunderte ist Regensburg mit einer Vielzahl von Namen bedacht worden. Das weist auf die reichhaltige Geschichte hin. Der Name "Radaspona" ist erstmals um 770 bei Arbeo von Freising in der Literatur zu finden, geht aber vermutlich auf ältere keltische Bezeichnungen zurück. Daraus entstand die französische Benennung Regensburgs „Ratisbonne“ und die italienische „Ratisbona“. Der Namensursprung beruht auf zwei keltischen Wörtern: "rate" oder "ratis" „Wall“, „Stadtmauer“ und "bona" „Gründung“ oder „Stadt“.

Daneben wurde die Stadt auch mit humanistisch geprägten Neubildungen wie "Quadrata," "Germanisheim," "Hydatospolis," "Ymbripolis," "Reginopolis" und "Tyberina" bedacht.

Der Regensburger Donaubogen ist bereits seit der Steinzeit besiedelt. Anfang 2006 wurden etwa 100 m östlich der Mauern des späteren Legionslagers keltische Gräber mit teilweise hochwertigen Grabbeigaben gefunden. Sie wurden auf etwa 400 v. Chr. datiert.

Die römische Geschichte Regensburgs beginnt etwa um 79 n. Chr. mit der Einrichtung des Kohortenkastells Kumpfmühl auf dem Gebiet des heutigen Stadtteils Kumpfmühl-Ziegetsdorf-Neuprüll. Das Lager diente als Beobachtungsposten für die Naab- und Regenmündung und war durch Graben und Pfahlpalisaden gesichert, später auch durch eine Steinmauer. Im Lager waren Hilfstruppen stationiert, und zwar entweder eine rund 500 Mann starke berittene Kohorte oder eine rund 1000 Mann starke Doppelkohorte Fußsoldaten. Bald bildete sich um das Kastell eine Zivilsiedlung "(vicus)". Daneben gab es auch eine Ansiedlung in Form eines langgezogenen Dorfes ("vicus"), das im Bereich der heutigen westlichen Altstadt am Bismarkplatz begann und entlang einer zur Donau führenden Straße verlief, wo am Ufer ein Schiffsanlegeplatz nachgewiesen wurde. Wie Ausgrabungen in den Jahren 1967 / 77 gezeigt haben, hat diese Donausiedling schnell eine größere Ausdehnung erreicht, die sich nach Osten bis hin zum später entstandenen Legionslager Castra Regina erstreckte. Reste eines römischen Beobachtungsturmes wurden nahe der Naabmündung gefunden. Für diese Zeit (2. Jahrhundert) wird auch die älteste römische Brauerei nördlich der Alpen vermutet (heute Römer-Pavillon am Kornweg). Das Kastell und die Zivilsiedlungen wurden im Rahmen des Markomannensturms in der zweiten Hälfte der 160er-Jahre zerstört.

Nach dem Zurückdrängen der Markomannen bis etwa 170 n. Chr. wurde auf Anordnung von Kaiser Mark Aurel ab ca. 175 das Legionslager "Castra Regina" (Lager am Regen) errichtet. Dieser Steinbau mit seiner etwa 10 Meter hohen Mauer, den vier Toranlagen und zahlreichen Türmen ist heute noch gut im Grundriss der Regensburger Altstadt erkennbar. Von seiner Einweihung im Jahre 179 n. Chr. ist heute noch die steinerne Inschrift erhalten, die sich einst über dem Osttor befand und als die Gründungsurkunde Regensburgs gilt. Im Lager war die III. Italische Legion mit rund 6000 Soldaten stationiert. Es war militärischer Hauptstützpunkt der Provinz Raetia und bildete somit eine Ausnahme im römischen Verwaltungssystem, da die Legion nicht in der Provinzhauptstadt Augsburg stationiert war. Während der Wirren der Völkerwanderung kam es im Verlauf des 5. Jahrhunderts zur militärischen Aufgabe des Kastells, das fortan eine mauerbewehrte Zivilsiedlung war.

Von etwa 500 bis 788 war Regensburg der Hauptsitz der Herzöge der Bajuwaren aus dem Geschlecht der Agilolfinger. Regensburg wurde zu einem bedeutenden Zentrum des frühen bairischen Stammesherzogtums. Herzog Odilo verwirklichte im Jahr 739 die bairische Diözesaneinteilung. Die Bistümer Regensburg, Freising, Passau und Salzburg wurden kirchenrechtlich gegründet und ihre Grenzen festgelegt. Nach seinem Sieg über den bairischen Herzog Tassilo III. verbrachte Karl der Große zwei aufeinanderfolgende Winter (791–793) in der alten bairischen Herzogsstadt Regensburg, um die Einverleibung Baierns in das Fränkische Reich persönlich abzusichern. Unter Ludwig II.(dem Deutschen) wurde Regensburg wieder Residenz und Verwaltungszentrum.

Regensburg ist eines der ältesten Bistümer Deutschlands, das bereits einige Jahrzehnte bestand, als es 739 von Bonifatius dem Canonischen Recht und somit dem Bischof von Rom unterstellt wurde. Überreste diverser aufeinander folgender Epochen finden sich unter anderem in den Ausgrabungen unter der Niedermünster-Kirche, zu einer der ältesten Klosteranlagen der Stadt gehörig, der auch die sogenannte Erhardi-Krypta zuzuordnen ist. Ähnlich alt ist die romanische Kapelle "St. Georg und Afra". Auch wenn Regensburg als Reichsstadt ab 1542 protestantisch war, blieb die Stadt immer katholische Bischofsstadt, obwohl sie zeitweise von anderen Bistümern mitverwaltet wurde.

Im 9. Jahrhundert war Regensburg eine der wichtigsten Städte des ostfränkischen Karolingerreiches. Hemma († 876), die Gemahlin des ostfränkischen Königs Ludwig der Deutsche, sowie die beiden letzten ostfränkischen Karolingerherrscher, Kaiser Arnulf von Kärnten († 899) und sein Sohn König Ludwig das Kind († 911) wurden in der Benediktinerabtei St. Emmeram beigesetzt. St. Emmeram war ein Vorstadtkloster; der Bischof residierte – wie in allen mittelalterlichen Städten – im Episcopium, in nächster Nähe des Domes, seiner Bischofskirche, in der ummauerten Stadt.

Im Jahre 954 zog sich Liudolf, der älteste Sohn Ottos des Großen, nach dem Scheitern seines Aufstandes gegen seinen Vater nach Regensburg zurück. Nach einer mehrmonatigen Belagerung der Stadt durch Ottos Bruder Heinrich wurde Regensburg erobert und in Brand gesteckt; Liudolf gelang jedoch die Flucht.

Eine Regensburger Stadtsage aus dieser Zeit ist die Dollingersage.

Durch Fernhandel bis Paris, Venedig und Kiew erlebte die Stadt ihre wirtschaftliche Blütezeit. Sie war damals eine der wohlhabendsten und einwohnerstärksten Städte Deutschlands. Die romanische und gotische Architektur des Mittelalters bestimmt noch heute das Gesicht der Altstadt. Ein Zeichen für den damaligen Wohlstand der Stadt ist der Bau der Steinernen Brücke von 1135 bis 1146. Das mittelalterliche Bauwunder trug zur weiteren Steigerung des Wohlstandes der Stadt im 13. Jhdt. bei und wurde zum Vorbild für viele andere Brückenbauten, zum Beispiel für die Judithbrücke (Vorläuferin der Karlsbrücke) in Prag. Die Brücke ist zugleich Symbol für den Aufstieg der bürgerlichen städtischen Selbstverwaltung: Im Brückenprivileg Kaiser Barbarossas vom 26. September 1182 wird mit dem Brückenmeister (magister pontis) Herbord erstmals ein städtischer Funktionsträger namentlich genannt.

Im Mai 1147 brach Konrad III. in Regensburg zum zweiten Kreuzzug auf, der strategisch günstige Donauübergang dürfte dafür mit den Ausschlag gegeben haben. Kaiser Friedrich I. Barbarossa brach hier im Mai 1189 mit einer großen Streitmacht zum dritten Kreuzzug auf.

In den Jahren 1207 und 1230 verliehen König Philipp von Schwaben und Kaiser Friedrich II. der Stadt umfangreiche Privilegien (in der Forschung als "Philippinum" bzw. "Fridericianum" bekannt), die in der Folge den Aufstieg zur Freien Stadt ermöglichten. Schon am 10. November 1245 erreichten die Regensburger Bürger, dass Kaiser Friedrich II. der Stadt das Recht der Selbstverwaltung mit dem Privileg „einen Bürgermeister und Rat zu setzen“ bestätigte. Der nach dem Bau der Steinernen Brücke verstärkt einsetzende lukrative Fernhandel machte die Stadt zu einer Drehscheibe des Ost-West- und Nord-Süd-Handels. In der Stadt, die ca. 20.000 Einwohner hatte, entstand ein reiches Bürgertum von etwa 2000 Personen, die politisch eine Rolle spielten. Die Oberhäupter von 50 – 60 dieser Familien bildeten das Patriziat, aus denen sich die Stadtregierung zusammensetzte. Die Patrizierfamilien begannen eine rege Bautätigkeit und es entstanden mächtige Patrizier-Hausburgen aus Stein mit Geschlechtertürmen als Statussymbol, von denen der Goldene Turm in ursprünglicher Höhe erhalten ist. Nach dem Muster der Hausburgen wurde auch der älteste Teil des heutigen Alten Rathauses mit seinem Turm erbaut. In dieser Zeit der reichen Patrizier entstanden auch die Bettelordenskirchen und -klöster, wie die Minoritenkirche und die Dominikanerkirche St. Blasius.

Die in der Stadt residierenden bayerischen Herzöge der Wittelsbacher konnten wegen interner Konflikte nach der bayerischen Landesteilung von 1255 die Entwicklung der Stadt zur Unabhängigkeit nicht aufhalten. Sie gaben ihre Residenz in Regensburg am Kornmarkt auf, verließen Regensburg und zogen 1259 nach Landshut. Sie behielten aber weiterhin ihre seit 1185 bestehenden Rechte in der Stadt, wie Münzregal, Geleitrecht und Gerichtsbefugnisse in Vogteien. Die Rechte wurden an reiche Bürger oder an die Stadt verpfändet, was für die Stadt eine finanzielle Last bedeutete. Damit begann ein über vier Jahrhunderte andauernder Konflikt der Stadt mit den Herzögen des Herzogtums Bayern und mit den Regensburger Fürst-Bischöfen des Hochstifts Regensburg, deren Territorien das relativ kleine Stadtgebiet von Regensburg umschlossen. Dabei blieb es immer das Ziel der bayerischen Herzöge, die Lebensfähigkeit der Stadt Regensburg zu untergraben, um ihre verlorene Hauptstadt zurückzugewinnen.

Vermutlich um 1273 wurde mit dem Bau des Regensburger Doms St. Peter begonnen. Zusammen mit der Steinernen Brücke ist der Dom das Wahrzeichen der Stadt.

Am Beginn des 14. Jahrhunderts deutete sich in Regensburg ein wirtschaftlicher Abschwung an, verursacht durch Verlagerungen der Handelswege im Ost- und Orienthandel. Davon profitierten andere Städte wie Augsburg, Wien und Nürnberg, die wirtschaftliche Zuwächse und - anders als Regensburg - zunehmende Einwohnerzahlen zu verzeichnen hatten.

Ab 1330 kam es im Süden des Reiches in vielen Städten zu Unruhen und Aufständen der Zünfte und Handwerker, die von den Patriziern die Beteiligung an der Stadtregierung forderten. In Regensburg nahmen die Aufstände ganz besondere Ausmaße an, weil sich der von Kaiser Ludwig IV. unterstützte Patrizier Friedrich Auer zunächst mit den Zünften verbündete und ins Bürgermeisteramt aufstieg. Dort entfaltete er aber ein diktatorisches Regime und wurde 1334 wieder gestürzt. Friedrich Auer zog sich auf die Burg Brennberg in der Nähe von Regensburg zurück und betätigte sich von dort aus als Raubritter auf den Handelswegen der Regensburger Patrizier. Die durch den Auer-Aufstand verunsicherte und geschwächte Stadt Regensburg wurde 1337 überraschend von einem Heer des Kaisers Ludwig IV. bedroht, der - getreu seiner Herkunft aus dem Hause Wittelsbach - die Situation nutzen wollte und einen neuen, letztlich aber erfolglosen Versuch unternehmen wollte, die Stadt Regensburg in das Herzogtum Bayern zurückzuholen.

Sicher hatte auch die erste große Pestpandemie, die von 1347 bis 1353 ganz Europa und Vorderasien überzog und einen drastischen Rückgang der Bevölkerung zur Folge hatte, einen schädigenden Einfluss auf die wirtschaftliche Entwicklung der Fernhandelsstadt Regensburg. Wohl noch gravierendere Auswirkungen auf die wirtschaftliche Lage der Stadt hatten aber die immer stärker werdenden Blockadeaktionen der bayerischen Herzöge und ihre zunehmenden Schikanen gegen die städtischen Kaufleute und Händler. Deshalb trat die Stadt Regensburg 1381 dem Schwäbischen Städtebund bei. Der Bund hatte sich dem Schutz seiner ca. 50 Mitglieder vor den jeweiligen Landesfürsten verschrieben und war auch bereit, militärischen Druck auf die Fürsten auszuüben. So kam es im Jahr 1388 im Städtekrieg auch zu militärischen Aktionen im Umland von Regensburg, bei denen das Heer des bayerischen Herzogs Albrecht I. nicht nur die städtischen Weinberge zerstörte. Eine Belagerung der Stadt Regensburg blieb aber ohne Erfolg. Der Städtekrieg wurde mit dem Landfrieden von Eger beendet, der die vor dem Krieg bestehende Situation nicht veränderte. Die Städte wurden zu hohen Kriegsentschädigungen verpflichtet und hatten ihre eigenen hohen Kriegskosten zu tragen. Außerdem sah sich der Rat der Stadt Regensburg gezwungen, die Ausgaben zur Verbesserung der Stadtbefestigung zu erhöhen, um die Unabhängigkeit zu erhalten. Dadurch verschlechterte sich die bereits schwierige Finanzlage der Stadt zum Ende des Jahrhunderts noch weiter.

Im 15. Jahrhundert setzte sich der wirtschaftliche Niedergang von Regensburg fort und führte zum Bankrott der Stadt. Eingeleitet wurde der Absturz als 1419 die Hussitenkriege begannen. Die Kämpfe weiteten sich auch in die Oberpfalz aus und endeten 1434 mit der Niederlage der Hussiten und dem Verlust von Wirtschaftskraft und Absatzgebieten in der Region Böhmen und den weiter im Nordosten gelegenen Absatzgebieten, die nun für Regensburger Fernhandels-Kaufleute nicht mehr erreichbar waren. Die desolate Finanzlage der Stadt hatte sich verschlimmert, da in Erwartung der Hussiten der nördliche Brückenkopf der Steinernen Brücke verstärkt worden war und dafür die östlichen Bauten des Katharinenspitals abgebrochen werden mussten.

Es kam zum Abfluss von Kapital und zum Wegzug reicher Familien aus Regensburg, denn die nur vom Fern- und Transithandel lebende Stadt hatte es im 14. Jahrhundert versäumt, das Handwerk und die Produktion von Konsumgütern zu fördern, wie es in Nürnberg geschehen war. Auch vom Handel mit Venedig und Italien profitierten jetzt Nürnberg und Augsburg durch neue Möglichkeiten bei der Nutzung des Brennerpasses, während der von den Regensburger Händlern genutzte Tauernpass ins Abseits geriet.

Auch als Fernhandelsstadt mit dem Vorderen Orient war Regensburg durch das Vordringen der Türken in Südosteuropa in eine Randlage geraten. Die Ausbreitung der Türken konnte nach der Niederlage der Serben 1389 in der Schlacht auf dem Amselfeld und 1396 in der Schlacht bei Nikopolis und in den dann folgenden Türkenkriegen nicht gestoppt werden. Endgültig blockiert wurde der Handelsweg nach Osten, der schon früher durch das seit 1221 gültige Stapelrecht der Stadt Wien behindert wurde, durch die Eroberung von Konstantinopel durch die Türken.

Kaiser Friedrich III. versuchte auf allen Reichstagen im 15. Jahrhundert von den Reichsständen Geld für den Krieg gegen die Türken zu erhalten. Im Fall der Freien Reichstadt Regensburg waren die Bemühungen nicht erfolgreich, so dass der Kaiser 1483 der Stadt mit der Reichsacht drohen musste, um 6000 Gulden zu erhalten. Die Stadt, deren Einwohnerzahl inzwischen auf ca. 12.000 zurückgegangen war, konnte das Geld nicht aufbringen, zumal sie bereits 1476 von Kaiser Friedrich III. als dem Schutzherren der Juden wegen jahrelanger, unberechtigter Einkerkerung von siebzehn prominenten Juden zu einer Geldbuße von 8000 Gulden verurteilt worden war.<ref name="Briel244/45">Peter Brielmaier, Uwe Moosburger (Hrsg. Peter Morsbach): "Regensburg. Metropole im Mittelalter." Friedrich Pustet Regensburg 2007, ISBN 978-3-7917-2055-5, S. 244 f.</ref> Um die Schuld beim Kaiser zu begleichen, führte der Rat der Stadt neue Steuern ein. Das führte im August 1585 zu einem Aufruhr der Zünfte, wobei sich die Verärgerung der Bevölkerung gegen den Kaiser richtete.

In dieser Situation setzte der bayerische Herzog Albrecht IV. seine alten Rechte als Burggraf als Lockmittel ein. Er hatte die Rechte 1479 für 19.000 Gulden an die Stadt Regensburg verpfändet und bot sie nun dem Rat zum Rückkauf an. Mit dieser Summe konnte die Stadt die Schulden beim Kaiser begleichen und bekam zusätzlich eine finanzielle Verfügungsmasse. In der Bevölkerung gewann eine probayerische, antikaiserliche Stimmung die Oberhand, ausgedrückt in der Losung: „Besser ein Herzog als ein Kaiser! Der Herzog macht reich, das Reich macht arm.“ Im Oktober 1485 setzte eine probayerische Gruppe im Rat der Stadt die Annahme der Vorschläge des Bayernherzogs durch. Im Juli 1486 wurde der völlige Anschluss der Stadt an das Herzogtum Bayern mit einem Übergabevertrag durchgesetzt. Den Ausschlag gab das Argument, dass analog zu anderen prosperierenden bayerischen Landstädten nur mit bayerischer Förderung ein wirtschaftlicher Aufschwang für Regensburg erreichbar sei. Im August 1486 zog Albrecht IV. prunkvoll in Regensburg ein. Als Wittelsbacher hatte er seit Jahren eine Ausbreitungs- und Konfrontationspolitik gegenüber dem kaiserlichen Haus Habsburg betrieben und nun hatte er einen seiner größten Erfolge erzielt. In den Folgejahren bis 1492 plante Herzog Albrecht IV. viele nützliche Verwaltungs- und Baumaßnahmen in Regensburg, z. B. die Gründung einer Universität und den Bau einer Herzogsresidenz vor dem Prebrunntor. Von den Baumaßnahmen wurden nur einige verwirklicht, z. B. der Amberger Stadel (Regensburg) und die Verlegung der Straße nach Nürnberg durch das Prebrunntor im Herzogspark auf die nördliche Seite der Donau.

Der Habsburger Kaiser Friedrich III. und sein seit 1486 als König gekrönter und mitregierender Sohn, der spätere Kaiser Maximilian I. reagierten scharf auf die Unterwerfung Regensburgs unter bayerische Herrschaft und nutzten die Gelegenheit von Reichs wegen mit Rechtsmitteln gegen die Wittelsbacher Konkurrenten vorzugehen. Im Oktober 1491 und im Januar 1492 wurde über die Stadt Regensburg und über den Bayernherzog vom Reichskammergericht die Reichsacht verhängt. Die nötige militärische Unterstützung fand der Kaiser im schwäbischen Bund, einem Zusammenschluss schwäbischer Reichsstände, die den Ausbreitungsbestrebungen der Wittelsbacher Widerstand boten. Der Bayernherzog Albrecht IV. musste dem militärischen Druck nachgeben und die städtische Reichsunmittelbarkeit der Stadt Regensburg wurde 1492 wiederhergestellt. Dafür waren mehrere Vertragswerke nötig, in denen die territorialen Grenzen zwischen Regensburg und Bayern neu festgelegt wurden. In diesen Verträgen von 1496 verlor die Stadt Regensburg ihren Status als Freie Stadt und wurde zu einer Reichsstadt unter Aufsicht von kaiserlichen Kommissaren, deren Befugnisse in Regimentsordnungen und Schutzverträgen festgelegt wurden. Der Bayernherzog verlor seine alten Rechte als Burggraf in der Stadt samt den Einkünften daraus. Als Ausgleich wurde die Siedlung "Am Hof" zur bayerischen Landstadt Stadtamhof erhoben.
Die Stimmung in der Bevölkerung blieb jedoch angespannt, weil sich die wirtschaftliche Lage nicht besserte und weil es weiterhin Anhänger des Bayernherzogs gab, mit denen die kaiserlichen Kommissare, die ab 1499 Reichshauptmänner genannt wurden, hart abrechneten. In der Stadt begann eine 30-jährige Phase mit sozialen Unruhen, die 1519 zur Vertreibung der Regensburger Juden führten.

Die inneren Unruhen in der Stadt eskalierten im Jahr 1511 als Kaiser Maximilian I. den fränkischen Adeligen Thomas Fuchs von Wallburg zum neuen kaiserlichen Reichshauptmann für Regensburg berief. Die Mehrheit im Rat der Stadt widersetzte sich 2 Jahre lang der Berufung. Es begann ein Machtkampf zwischen dem Kaiser und dem Rat Stadt, in dessen Verlauf der kaisertreue Ratsälteste Konrad Liskircher 1513 vom Pöbel entführt, inhaftiert, gefoltert und gehängt wurde. Nach Entsendung einiger kaiserlicher Kommissionen wurde am Ende die Berufung von Thomas Fuchs von Wallburg zum neuen Reichshauptmann durchgesetzt. Danach wurde mit den Parteigängern des bayerischen Herzogs abgerechnet und als Rädelsführer der Dombaumeister Wolfgang Roritzer zusammen mit mehr als 100 Gefolgsleuten hingerichtet. Der Kaiser Maximilian I. oktroyierte der Stadt im Jahr 1514 eine neue Stadtverfassung, die sogenannte „Regimentsordnung“, die formal bis 1803 in Kraft blieb.
Nach seiner Berufung spielte der neue Reichshauptmann Thomas Fuchs von Wallburg in den Finanzangelegenheiten der Stadt und in Verhandlungen mit dem Bischof Johann von der Pfalz eine für die Stadt wichtige und günstige Rolle, so dass seine Berufung nicht mehr in Frage gestellt wurde.
Nach dem Tod von Kaiser Maximilian im Januar 1519 und der Wahl des neuen Königs Karls V. im Juni 1519 nutzte der Rat der Stadt Regensburg die kurze Zeit des Machtvakuums ohne Kaiser und organisierte ein Pogrom zur Vertreibung der Regensburger Juden, der damals größten jüdischen Gemeinde Deutschlands. Vorausgegangen war eine Anordnung des Stadtrats am 21. Februar, mit der man einer Forderung christlicher Handwerker nachkam. Das alte Judenviertel auf dem heutigen Neupfarrplatz und der jüdische Friedhof vor dem Peterstor wurden total zerstört. Ein glücklich verlaufender Unfall bei den Abbrucharbeiten wurde als Wunder mystifiziert und führte zur Wallfahrt „Zur Schönen Maria“. Die Wallfahrt fand sehr großen Zulauf und brachte der Stadt und dem Bischof einige Jahre hohe Einnahmen. Mit dem Geld wurde der Bau einer Wallfahrtskirche unter Verwendung jüdischer Grabsteine begonnen. Nach Fertigstellung des Chores ging die Wallfahrt zurück und der Bau musste wegen Geldmangels abgebrochen werden. Der Rumpfbau wurde provisorisch geschlossen und nach Einführung der Reformation 1542 als protestantische Stadtkirche genutzt. Erst im 19. Jahrhundert wurde die Kirche im Westen geschlossen; damit entstand die heutige Neupfarrkirche auf dem gleichbenannten Platz.

Im Jahr 1524 wurde mit dem Regensburger Konvent das erste Bündnis altkirchlicher Reichsstände in der Stadt abgeschlossen. 1541 fand in der Neuen Waag am Haidplatz das Regensburger Religionsgespräch zwischen Philipp Melanchthon und Johannes Eck statt. Das Gespräch war ein Versuch, die nach dem Thesenanschlag von Luther 1517 in Wittenberg aufgeworfenen tiefen Gräben zwischen Katholiken und Protestanten zu überbrücken, was aber nicht gelang.

In den Jahren nach 1517, als sich immer mehr Städte der Reformation anschlossen, war in Regensburg der religionspolitische Handlungsspielraum des Rates der Stadt mehrfach eingeengt. Neben dem Gebiet der Reichsstadt schloss das Stadtgebiet auch die Territorien des Bischofs mit dem Dom, des Klosters St. Emmeran, des Stiftes Obermünster und des Stiftes Niedermünster. Auch der mit seinem Territorium die Stadt umschließende Herzog von Bayern zögerte nicht, die Stadt in der Religionspolitik mit der Androhung von Wirtschaftsblockaden unter Druck zu setzen. Der Rat der Stadt musste in den Jahren nach 1517 einen politischen Drahtseilakt vollziehen und wurde dabei geführt und beraten von dem beim Kaiser sehr einflussreichen Reichshauptmann Thomas Fuchs von Wallburg. Er hielt den Rat der Stadt zurück, so dass sich die Stadt nie an die Spitze der Reformationsbewegung setzte. Gleichzeitig wurden aber die vielen reformatorischen Ansätze nicht behindert, die es in der Stadt von Seiten der Bürger gab und die von auswärtigen Adeligen, die sich in der Stadt aufhielten, unterstützt wurden. Seit 1526 wurden evangelische Abendmahlsfeiern in Bürgerhäusern und Häusern von Adeligen geduldet. Damit wuchs aber auch die Gefahr des religiösen Sektierertums. Die Täufer hatten sich seit 1525 in Regensburg niedergelassen und 1528 wurde der Täufer Wützelburger hingerichtet.

Als es der Kaiser im Reichsabschied von 1541 den Städten freistellte, sich der Augsburger Konfession anzuschließen, ergriff der Rat der Stadt die Gelegenheit und fasste nach einer Petition der Bürger vom 28. Sept. 1542 den Beschluss, am 15 Oktober 1542 mit einem Abendmahl-Gottesdienst in der Neupfarrkirche die Reformation in Regensburg offiziell einzuführen. Der Ratskonsulent Dr. Johann Hiltner lieferte die benötigte Rechtfertigungsschrift.
Nach der Einführung der Reformation kam es weiterhin zu vielfältigen Konflikten mit dem Fürstbischof. Die Lage beruhigte sich erst, nachdem der Kammerer Stephan Fugger vom Reh († 1602) für den Rat der Stadt Regensburg die lutherische Konkordienformel von 1577 unterzeichnet hatte.

Seit dem ausgehenden 16. Jahrhundert, insbesondere aber während des Dreißigjährigen Kriegs und noch Jahre nach dem Westfälischen Frieden war die Stadt einer der wichtigsten, ersten Zufluchtsorte evangelischer Glaubensvertriebener aus Österreich. Von den insgesamt ca. 100.000 Exulanten ließen sich einige in Regensburg dauerhaft nieder, viele aber zogen weiter nach Nürnberg, Franken, Schwaben, Preußen und in die Niederlande.
Eine zweite Welle von Salzburger Exulanten folgte dann im Spätherbst und Winter 1731/32.

In den ersten vierzehn Jahren des 30-jährigen Krieges war Regensburg von Kriegshandlungen nicht betroffen. Auf dem Regensburger Kurfürstentag von 1630 wurde der Oberbefehlshaber des kaiserlichen Heeres Wallenstein zunächst abgesetzt, dann aber am Ende des Jahres 1631 wieder zurückgerufen. Die militärische Lage hatte sich für den Kaiser drastisch verschlechtert und ein Angriff auf Österreich entlang der Donaulinie schien möglich. Damit begannen die Kämpfe um Regensburg. Regensburg wurde als potentielle Sperrfestung wichtig, im April 1632 von bayerischen Truppen besetzt und als Festung ausgebaut. Der Festungsbau blieb wirkungslos, weil Wallenstein es versäumte, Nachschub und Truppen für die Verteidigung zu entsenden. Im November 1633 wurde die Stadt von schwedischen Truppen unter Bernhard von Sachsen-Weimar erstürmt und besetzt. Alle katholischen Geistlichen wurden vertrieben und im Dom wurde protestantischer Gottesdienst gehalten. Nur kurz nach der Ermordung von Wallenstein wurde Regensburg im Juli 1634 gemeinsam von kaiserlichen und bayerischen Truppen wieder zurückerobert. Das war der erste große militärische Erfolg des Erzherzogs Ferdinand, Sohn von Kaiser Ferdinand II. (HRR). Als neuer Oberbefehlshaber des kaiserlichen Heeres nach Wallenstein agierte er bei dieser Schlacht gemeinsam mit dem bayerischen Kurfürsten Maximilian I. (Bayern). Auf dem Regensburger Kurfürstentag (1636/37) erfolgte die Wahl Ferdinands III. zum Römisch-deutschen König.

Regensburg war bereits ein wichtiges Zentrum des Ostfränkischen Reichs gewesen, in dem auch Reichstage abgehalten wurden. Ab 1594 wurden die Reichstage nur noch im Reichssaal des Regensburger Rathauses abgehalten. 1663 wurde der Reichstag nicht mehr aufgelöst, womit er zum "Immerwährenden Reichstag" erklärt wurde. In ihm tagten nicht nur die Fürsten des "Heiligen Römischen Reiches", sondern in der Regel Gesandte aus ganz Europa. Der Kaiser selbst wurde dabei meist durch kaiserliche Prinzipalkommissare vertreten.

Im Verlaufe des Spanischen Erbfolgekriegs wurden die Schlüssel der Stadt am 8. April 1703 auf der Steinernen Brücke kampflos dem bayerischen General Alessandro Maffei übergeben. Gleichzeitig wurde der Stadt jedoch ein Schreiben überbracht, in dem sich Kurfürst Max Emanuel verpflichtete, seine Truppen wieder zurückzuziehen, sobald ihm die Neutralität Regensburgs zugesichert sei und er die Gewähr habe, dass keine der beiden Kriegsparteien die Brücke nutzen könne.

1748 wurde der kaiserliche Generaloberpostmeister Fürst Alexander Ferdinand von Thurn und Taxis zum Prinzipalkommissar ernannt, der aus diesem Grund die Residenz seiner Familie von Frankfurt nach Regensburg verlegte. Architektonisches Zeugnis der Zeit des "Immerwährenden Reichstags" sind die zahlreichen Gesandtschaften in der gleichnamigen Straße in Regensburg. Der wirtschaftliche Nutzen für die Stadt war aber gering, da die Gesandten weder zoll- noch steuerpflichtig waren. Die politische Lage in der Stadt selbst war zu dieser Zeit recht kompliziert. Neben der Reichsstadt gab es weitere reichsunmittelbare "Herrschaften" in Regensburg, nämlich das Hochstift Regensburg des Regensburger Bischofs sowie die Reichsklöster St. Emmeram, Niedermünster und Obermünster. Dazu kamen Sonderrechte des Kaisers und der auf dem Reichstag tagenden Fürsten.

Ende des 18. Jhs. wurde die Stadt von schweren innenpolitischen Auseinandersetzungen erschüttert, als vor dem Hintergrund eines drohenden Finanzkollapses der Stadt Vertreter der Bürgerschaft und des Magistrats den "Geheimen Rat" der Stadt (das eigentliche Regierungsorgan) wegen Misswirtschaft und Verfassungsbruchs mit Erfolg vor dem Reichshofrat in Wien verklagten. Der Kaiser verordnete eine punktuelle Revision der Stadtverfassung und gewährte Regensburg – zum Schaden der städtischen Gläubiger – ein Moratorium, das den Kollaps des Stadtstaates abwendete.

Im Jahr 1800 nahm die siegreiche französische Armee in Regensburg Quartier und legte der Stadt hohe Kontributionsforderungen auf, die die Stadtfinanzen vollends ruinierten.

1803 fiel hier eine der letzten Entscheidungen des Reichstags: Der Reichsdeputationshauptschluss leitete die Auflösung des Heiligen Römischen Reiches ein und führte unter anderem zur Säkularisation des Großteils der Klöster. Mit dem Reichsdeputationshauptschluss entstand unter anderem das eigenständige Fürstentum Regensburg unter Karl Theodor von Dalberg, der sein Amt als Erzbischof aufgrund bayerischer Einwände erst am 1. Februar 1805 antreten konnte. Die Rheinbundstaaten erklärten auf der letzten Sitzung des Regensburger Reichstags am 1. August 1806 den Austritt aus dem Verband des "Heiligen Römischen Reiches deutscher Nation." Im Fünften Koalitionskrieg (Frankreich gegen Großbritannien und Österreich) besetzte ein österreichisches Armeekorps am 20. April 1809 Regensburg. Drei Tage später eroberten die Franzosen die Stadt zurück (→Schlacht von Regensburg). Hierbei erlitt Napoleon die einzige Verletzung auf allen seinen Feldzügen. Dalberg behielt zwar sein Amt als Regensburger Erzbischof (bis zu seinem Tod 1817), musste aber auf Druck Napoleons Regensburg am 22. Mai 1810 an das Königreich Bayern abtreten.

Die Inbesitznahme durch Bayern bedeutete den Verlust der politischen Bedeutung und der Sonderstellung der Stadt innerhalb des alten Bayern; die wirtschaftlichen Verhältnisse waren schon zu reichsstädtischer Zeit zuletzt so desolat geworden, dass eine weitere Selbstständigkeit schon deshalb ausgeschlossen schien. Regensburg wurde Hauptstadt des Regenkreises, ab 1838 des Kreises „Regensburg und Oberpfalz“, des späteren Regierungsbezirks Oberpfalz. Es war „kreisunmittelbare Stadt“ und zugleich Sitz des gleichnamigen Bezirksamts; Regensburg begann langsam wieder an Bedeutung zu gewinnen. 1859 erfolgte der Anschluss ans Eisenbahnnetz mit Verbindungen nach Nürnberg und München. Jedoch siedelte sich auch in der Folgezeit kaum Industrie an. Noch für lange Zeit beschränkte sich Regensburgs Rolle auf die eines Wirtschafts- und Handelszentrums für ein relativ begrenztes agrarisches Umland, neben der traditionellen Bedeutung, welche die alte, in sich ruhende Stadt als Kirchen- und Schulstadt sowie Behördensitz hatte. Bis heute bedeutend ist die Eröffnung des Luitpoldhafens 1910 (mittlerweile als Westhafen bekannt). Bis zum Ersten Weltkrieg (und noch während des Krieges) erlebte die Donauschifffahrt einen Aufschwung, namentlich aufgrund des Erdölimports aus Rumänien. 1913 wurde der Bayerische Lloyd gegründet. Der Regensburger Petroleumhafen erwies sich bald als zu klein.

Eine bedeutende Vergrößerung des Stadtgebiets (mehr als 26 Quadratkilometer) und einen Zuwachs von rund 20.000 Einwohnern erbrachte 1924 die Eingemeindung von sieben Gemeinden des Bezirksamtes Stadtamhof (Reinhausen, Sallern, Schwabelweis, Stadtamhof, Steinweg, Weichs und Winzer).

Nach der „Machtübernahme“ des NSDAP-Regimes am 30. Januar 1933 wurde Oberbürgermeister Otto Hipp (Bayerische Volkspartei) am 20. März 1933 abgesetzt. Dieser war ein entschiedener Gegner der Nationalsozialisten und hatte der NSDAP noch zu Beginn der dreißiger Jahre die Nutzung städtischer Gebäude rechtskräftig untersagt. Am 12. Mai 1933 fand auch in Regensburg auf dem Neupfarrplatz eine offizielle Bücherverbrennung statt. Im selben Jahr ließ Otto Schottenheim (Oberbürgermeister von 1933 bis 1945; NSDAP) im Norden der Stadt den Bau einer – seinerzeit nach ihm benannten – „nationalsozialistischen Mustersiedlung“ beginnen (heute: Konradsiedlung-Wutzlhofen).

1933 wurde Regensburg dem Gau "Bayerische Ostmark" der NSDAP (Sitz: Bayreuth) – ab 1943: Gau Bayreuth – zugeschlagen; es blieb Sitz der Regierung des 1932 gebildeten Kreises (ab 1939: Regierungsbezirks) Niederbayern/Oberpfalz. Im Herbst 1932 übernahm der Volksschullehrer Wolfgang Weigert das Amt des NSDAP-Kreisleiters von Wilhelm Brodmerkel. Am 9. November 1938 wurde im Zuge der Reichspogromnacht die Synagoge am Brixner Hof niedergebrannt, die verbliebenen jüdischen Geschäfte geplündert und die jüdische Bevölkerung Regensburgs terrorisiert.

Am 2. April 1942 wurden 106 Regensburger Juden vom Platz der zerstörten Synagoge aus nach Piaski transportiert und in den Vernichtungslagern Belzec und Sobibor später ermordet. Weitere Transporte führten ins KZ Auschwitz und ins KZ Theresienstadt. Insgesamt wurden etwa 250 der von Regensburg aus deportierten Juden während der Shoa ermordet. Ungefähr 230 Regensburger Juden und Jüdinnen konnten der Vernichtung durch Emigration oder Flucht entkommen.

Im Stadtteil Stadtamhof befand sich Anfang 1945 das KZ-Außenlager Colosseum, ein Außenlager des KZ Flossenbürg. In der Nachbargemeinde Obertraubling existierte auf dem Firmengelände der damaligen Messerschmitt AG das KZ-Außenlager Obertraubling. Heute gehört ein Teil dieses Areals (so das ehemalige sogenannte Russenlager II mit seinerzeit über Tausend vorwiegend russischen Zwangsarbeitern) zum Stadtgebiet von Regensburg.

Im Herbst 1942 verhaftete die Gestapo über 30 Personen und warf ihnen staatsfeindliches Verhalten vor. Da sich die Verfolgten, die von KPD über BVP bis NSDAP allen politischen Lagern angehörten, in loser Folge auf dem Regensburger Neupfarrplatz trafen, gab ihnen die Gestapo den Namen „Neupfarrplatz-Gruppe“. Die Polizei warf in ihrem Abschlussbericht den Festgenommenen zersetzende Mundpropaganda vor; diese habe „viele deutsche Volksgenossen in ihrer Siegeszuversicht ganz erheblich geschwächt“. Zwei der Angeklagten, Josef Bollwein und Johann Kellner, wurden vom 6. Senat des Volksgerichtshofs wegen „Vorbereitung zum Hochverrat“ zum Tode verurteilt und am 12. August 1943 in München-Stadelheim hingerichtet.
Andere wurden mit Zuchthausstrafen und Ehrverlust bestraft bzw. ins Konzentrationslager Flossenbürg verbracht. Dort starben weitere sechs Menschen.

Im Jahr 1940 beginnend wurden aus dem "Bezirksnervenkrankenhaus" an der "Ludwig-Thoma-Straße" insgesamt 638 Frauen, Männer und Jugendliche im Rahmen der „Euthanasie“-Krankenmordaktion T 4 in die Tötungsanstalt Hartheim deportiert. Mehr als 500 weitere Menschen wurden zwangsweise sterilisiert.

Mit Beginn des Zweiten Weltkrieges wurden in und um Regensburg mehrere Arbeitslager für Kriegsgefangene vieler Nationen errichtet. Etwa 700 von ihnen wurden Opfer der NS-Zwangsarbeit bzw. starben an Seuchen und elenden Lebensbedingungen. Insgesamt mussten fast 14.000 sogenannte Fremdarbeiter während der Kriegszeit in Regensburg Zwangsarbeit leisten.

Im Zweiten Weltkrieg hatte Regensburg im Vergleich zu anderen größeren Städten verhältnismäßig wenig unter Luftangriffen zu leiden. Allerdings waren im Westen der Stadt die Messerschmitt-Flugzeugwerke angesiedelt, die ein wichtiges strategisches Ziel für Luftangriffe waren. Zwei weitere strategische Ziele waren die Hafenanlagen und die Brücken über die Donau im Osten der Stadt und die Bahnanlagen am südlichen Rand der Altstadt mit Regensburg als Eisenbahnknoten zwischen München und Berlin. Das Messerschmitt-Flugzeugwerk, das damals zu den größten seiner Art gehörte, wurde am 17. August 1943 zerstört (Operation Double Strike) und bis zum Kriegsende wurden auch die beiden anderen Ziele in mehreren Angriffen völlig zerstört.
Die Altstadt aber wurde im Vergleich zum Zerstörungsgrad anderer deutscher Innenstädte kaum in Mitleidenschaft gezogen, wenngleich mit der Stiftskirche Obermünster eines der bedeutendsten Baudenkmale der Stadt völlig verloren ging und auch andere historische Bauten in der Altstadt, wie zum Beispiel die Alte Kapelle oder die Neue Waag am Haidplatz, schwer beschädigt wurden. Bei insgesamt 20 Bombenangriffen der Royal Air Force und der 8th US Air Force 1943–1945 starben etwa 3000 Menschen, unter ihnen viele Kriegsgefangene. Die Wohnraumsubstanz der Stadt wurde nur relativ wenig geschädigt: 82 % der Wohnungen galten als unbeschädigt, 9 % als mittel- bis schwerbeschädigt und 9 % als total zerstört.

1944 erklärte ein Führerbefehl Regensburg und zahlreiche andere Städte zur Festung. Dies hatte vor allem propagandistische Gründe.

Am 22. April 1945 forderte Gauleiter (Gau Bayreuth) und Reichsverteidigungskommissar Ludwig Ruckdeschel in einer fanatischen Rede bzw. Rundfunkansprache im Velodrom die Verteidigung von Regensburg bis zum letzten Stein. In dieser Nacht ließ er außer der Steinernen Brücke alle Donaubrücken sprengen. Kurze Zeit später flohen Ruckdeschel und Regierungspräsident Gerhard Bommel in das Schloss Haus bei Neueglofsheim.

Am 23. April 1945 bat Domprediger Johann Maier (1906–1945) auf einer Demonstration auf dem Moltkeplatz, die hauptsächlich von Regensburger Frauen mit Kindern und alten Leuten, aber auch Soldaten und Geistlichen getragen wurde, um die kampflose Übergabe, damit die Stadt nicht noch mehr beschädigt werde bzw. um weitere Opfer zu vermeiden. Am folgenden Tag wurde er wegen „Sabotage“ zusammen mit dem Regensburger Bürger Josef Zirkl und dem pensionierten Gendarmeriebeamten Michael Lottner öffentlich auf dem Moltkeplatz, dem heutigen Dachauplatz hingerichtet (siehe auch Endphaseverbrechen).

Ebenfalls am 23. April erfolgte gegen Abend eine Teilsprengung der Donaubrücken. Gesprengt wurden insbesondere vier Pfeiler eines der bedeutendsten Kulturdenkmale der Stadt, der Steinernen Brücke aus dem 12. Jahrhundert. Nachmittags am 25. April besetzten Einheiten der 71. Infanteriedivision den Stadtteil Stadtamhof. Am gleichen Tag erreichten sie Donaustauf und abends Bad Abbach.

Am Tag des 26. April verließen die Wehrmacht-Einheiten und der Kampfkommandant Hans Hüsson die Stadt Regensburg in Richtung Südosten. Major Othmar Matzke, der ranghöchste und entgegen der Befehlslage in der Stadt verbliebene Offizier, schickte daraufhin in den Morgenstunden des 27. April in Absprache mit Oberbürgermeister Otto Schottenheim einen Generalmajor a. D. als Parlamentär zu den US-amerikanischen Truppen. Dieser bot eine bedingungslose Kapitulation an und daraufhin wurde Regensburg kampflos an die 3. US-Armee übergeben.

Zum Gedenken an Regensburger Opfer des Nationalsozialismus verlegte Gunter Demnig am 12. Juni 2007 die ersten Stolpersteine in Regensburg.

Bereits 1945 überschritt die Einwohnerzahl die Marke von 100.000, bis zur Jahrtausendwende erreichte sie 150.000. Wesentlich beigetragen hierzu haben zunächst Flüchtlingsströme aus dem Osten (insbesondere aus dem Sudetenland), zahlreiche Eingemeindungen im Zuge der Gemeindegebietsreform zwischen 1971 und 1983 sowie diverse Infrastrukturmaßnahmen und Industrieansiedlungen:

1965 wurde der Grundstein der Universität gelegt, 1992 das dazugehörige Klinikum eröffnet; Anfang der 1970er-Jahre kam die Fachhochschule hinzu. 1960 nahm der Osthafen seinen Betrieb auf, 1978 der Main-Donau-Kanal. Der Siemens-Konzern hat seinen Standort Regensburg permanent ausgebaut, unter anderem durch Errichtung einer Fabrik zur Chipherstellung (heute Infineon AG). 1986 nahm das BMW-Werk bei Harting die Produktion auf. Ab 1989 produzierte Toshiba in Regensburg Laptops und Notebooks, hat aber 2009 seinen Regensburger Standort wieder geschlossen. Dafür hat sich, u. a. auf dem ehemaligen Toshiba-Gelände, das Unternehmen Osram neu angesiedelt, welches hier klassische und neuartige Lichtquellen produziert und erforscht.

Im Zuge der bereits genannten Eingemeindungen hat Regensburg einen Gebietszuwachs von knapp 3 km² erfahren. 1997 wurde Regensburg mit dem Europapreis für seine hervorragenden Bemühungen um den europäischen Integrationsgedanken ausgezeichnet.

Der historische Stadtkern Regensburgs mit engen Gassen, zahlreichen Patrizierhäusern und Kapellen aus allen Kunstepochen des Mittelalters ist weitestgehend erhalten und damit die größte mittelalterliche Altstadt Deutschlands. Außerdem besitzt sie die größte Anzahl an Geschlechtertürmen nördlich der Alpen, was ihr den Beinamen „Nördlichste Stadt Italiens“ eingetragen hat. Umsichtige und von der Bevölkerung mitgetragene Sanierungsmaßnahmen haben den Bestand von über 1000 geschützten Denkmälern bis heute gesichert. Am 13. Juli 2006 wurde die Regensburger Altstadt mitsamt Stadtamthof von der UNESCO als Weltkulturerbe anerkannt. Die Stadt richtete 2007 ein Weltkulturerbezentrum ein, das im historischen Salzstadl beim Eingangsturm der Steinernen Brücke untergebracht ist. Dort werden an zentraler Stelle detaillierte Informationen zur Stadtgeschichte gegeben (~ 2000 Jahre) und aktuelle Ausstellungen durchgeführt.

2015 wurde Regensburg der Ehrentitel „Reformationsstadt Europas“ durch die Gemeinschaft Evangelischer Kirchen in Europa verliehen.

Ende 2017 waren 51,4 % der Bevölkerung Regensburgs katholisch, 13,1 % evangelisch und 35,5 % gehörten anderen oder keiner Religionsgemeinschaft an. 1970 waren noch 82,4 % der Bevölkerung katholisch und 14,5 % evangelisch gewesen. Die Angabe „evangelisch“ schließt das lutherische, reformierte und unierte Bekenntnis, nicht aber die Freikirchen ein. Seit 1950 nimmt der Anteil der Katholiken an der Gesamtbevölkerung kontinuierlich ab.

Im Jahr 739 stiftete der Heilige Bonifatius das Bistum Regensburg. In der Folgezeit wurden zahlreiche Klöster gegründet. Regensburg stand mit Cashel, Irland, in engem kulturellen Austausch. Dort wurden Geistliche ausgebildet, die später nach Regensburg kamen. Das Bistum Regensburg war zunächst der Erzdiözese Mainz, später dem Erzbistum Salzburg unterstellt. Auch wenn durch Pfründehäufung Regensburg oft der persönlichen Anwesenheit seines Oberhirten entbehrte, so gibt es seit der Canonischen Errichtung des Bistums eine nur durch die Ernennungsmodalitäten geringfügig unterbrochene Folge der Regensburger Bischöfe bis heute.

Nach Einführung der Reformation im Oktober 1542 und der ersten öffentlichen Abendmahlsfeier am 15. Oktober 1542 in der Neupfarrkirche waren zwar der Rat der Stadt und die Bürger zum Protestantismus konvertiert, jedoch verblieben die katholischen Reichsstände im Stadtgebiet von Regensburg ebenso wie der katholische Bischofssitz und die reichsfreien Stifte Obermünster und Niedermünster und das reichsfreie Kloster St. Emmeran, die nicht zum Territorium der Reichsstadt selbst gehörten. Das katholische Bekenntnis blieb in der Stadt weiterhin vertreten und wurde nach 1810 durch zahlreiche Zuwanderungen zum vorherrschenden Bekenntnis. Die Konfessionsmischung hatte Regensburg schon früh eine Sonderstellung im Reich verliehen und war neben der Nähe zum kaiserlichen Wien auch ein Grund dafür, dass der Immerwährende Reichstag seinen Sitz in Regensburg nahm. Die Stadt bot ein Territorium im Reich, in dem beide Konfessionen friedlich zusammentreffen konnten.

Bis zum Reichsdeputationshauptschluss 1803 war an den erzbischöflichen Stuhl von Mainz das Amt des Reichserzkanzlers gebunden. Dies war 1803 Carl Theodor Anton Maria Reichsfreiherr von Dalberg. Mit dem Reichsdeputationshauptschluss wurden die Mainzer Rechte nach Regensburg übertragen, Dalberg wurde Erzbischof von Regensburg, was er bis zu seinem Tod 1817 blieb. 1817/1821 wurde das Bistum Regensburg neu umschrieben und der Kirchenprovinz München und Freising unterstellt. Das Bistum Regensburg ist das flächenmäßig größte bayerische Bistum mit 14.665 Quadratkilometern und setzt sich aus 33 Dekanaten zusammen. Die 24 Pfarrgemeinden und 4 weiteren Seelsorgestellen der Stadt Regensburg gehören innerhalb des Bistums zum Dekanat Regensburg, das mit den Dekanaten Laaber, Alteglofsheim, Donaustauf und Regenstauf die Region Regensburg bildet.

Die protestantischen Gemeinden wurden nach Einführung der Reformation von einem Superintendenten geleitet. Als Kirchenverwaltungsbehörde bestand ein Konsistorium. Nach dem Übergang an Bayern 1810 wurden die Gemeinden Teil der Evangelisch-Lutherischen Kirche in Bayern. Die sieben Regensburger Gemeinden gehören innerhalb dieser Landeskirche zum Dekanat Regensburg im gleichnamigen Kirchenkreis.

Im Bereich der Freikirchen bestehen in Regensburg heute Gemeinden der Adventisten, der Baptisten, der Mennoniten (seit 1820) und der Methodisten sowie eine pfingstlerische Freie Christengemeinde und eine Freie Evangelische Gemeinde.

Daneben besteht in Regensburg eine alt-katholische Pfarrgemeinde. Die russisch-orthodoxe Gemeinde nutzt die Maria-Schutz-Kirche im Stadtpark. Die Rumänisch Orthodoxe Kirchengemeinde "Hl. Dreifaltigkeit" nutzt die Klosterkirche St. Matthias in der Ostengasse. In deren Mönchschor hinter dem Hochaltar wurde 1974 durch das Ostkirchliche Institut Regensburg eine Kapelle eingerichtet, die heute von der serbisch-orthodoxen Gemeinde genutzt wird.

Die Kirche Jesu Christi der Heiligen der Letzten Tage ist in Regensburg mit einer Gemeinde vertreten.

Während seiner sechstägigen Pastoralreise durch Bayern im Jahr 2006 verbrachte Papst Benedikt XVI. drei Tage in Regensburg. Dabei feierte er auf dem am südlichen Stadtrand gelegenen Islinger Feld zusammen mit etwa 230.000 Menschen die Heilige Messe. Außerdem hielt er an der Universität eine in der Folge von islamischer Seite kritisierte Vorlesung und feierte im Dom eine ökumenische Vesper mit hohen Vertretern der evangelischen und orthodoxen Kirche.

Vom 28. Mai bis 1. Juni 2014 fand in Regensburg der 99. Deutsche Katholikentag statt. Es handelte sich um den dritten Katholikentag in Regensburg, der erste fand 1849 und der zweite 1904 statt.

Regensburg war die erste jüdische Gemeinde in Bayern und im Mittelalter eine der bedeutendsten in Europa. Aus dem Jahr 981 stammt die früheste urkundliche Erwähnung eines Juden in Regensburg. In den folgenden Jahrhunderten florierte die Gemeinde und brachte einige der bekanntesten zeitgenössischen Schriftgelehrten und Lyriker wie Isak ben Mordechai, Efraim ben Isaak (Efraim der Große aus Regensburg) und Jehuda ben Samuel he-Chasid (Jehuda der Fromme) hervor. Das Judenviertel befand sich am heutigen Neupfarrplatz. Im Jahre 1519 wurde die Synagoge zerstört, die Juden wurden vertrieben. Ab 1669 lebten wieder Juden in der Stadt. Zwischen 1861 und 1871 wuchs die Gemeinde von 150 auf 430 Mitglieder an. Ein Synagogenneubau entstand 1912.

Die wechselvolle Geschichte der jüdischen Gemeinde zu Regensburg endete vorläufig mit der Zerstörung der Synagoge in der Pogromnacht 1938 und der Deportation und Ermordung der Regensburger Juden während des Zweiten Weltkriegs. Die rund 400 Juden in Regensburg wurden enteignet, beraubt und verschleppt. An die 250 wurden ermordet. Nach 1945 nahm Regensburg etwa 3500 "Displaced Persons" auf: Juden, die entweder aus dem KZ Flossenbürg befreit worden waren oder aus den osteuropäischen Lagern in die bayerische US-Besatzungszone geflüchtet waren. Die meisten von ihnen wanderten in die USA oder nach Israel aus, so dass Regensburg 1953 nur noch rund 400 Juden zählte. Anfang der 1990er Jahre waren es gerade noch 60. Erst seit der Einwanderung aus Russland ist die Zahl wieder auf etwa 400 gestiegen.

Bei Grabungen am Neupfarrplatz wurden 1995 Reste der Synagoge wiederentdeckt. Dort richtete die Stadt ein Informationszentrum ein, das "document Neupfarrplatz". Es informiert unterirdisch über die wechselhafte Geschichte des Neupfarrplatzes: Jüdisches Ghetto und Religionszentrum von internationaler Bedeutung, Juden-Vertreibung, Katholische Wallfahrtskirche Zur Schönen Maria, Evangelische Neupfarrkirche als Mutterkirche des österreichischen und süd-osteuropäischen Protestantismus. Der Grundriss der ehemaligen Synagoge wird durch ein vom israelischen Künstler Dani Karavan gestaltetes begehbares Bodenrelief aus weißem Beton nachgezeichnet, das am 13. Juli 2005 eingeweiht wurde. Die heutige Synagoge steht auf dem Gelände des in der Pogromnacht zerstörten Vorgängerbaus wenige 100 Meter östlich des Neupfarrplatzes in der Straße Am Brixener Hof. Jüdische Friedhöfe befinden sich in der Schillerstraße westlich des Stadtparks sowie in einem Abschnitt des städtischen Friedhofs auf dem Dreifaltigkeitsberg.

Die Anzahl der in Regensburg wohnhaft gemeldeten Muslime dürfte zwischen 4000 und 5000 liegen, davon wohl gut die Hälfte türkischer Abstammung. Aus den restlichen Herkunftsländern muslimischer Einwanderer, die wie in vielen anderen deutschen Großstädten von Marokko bis Pakistan und Indonesien reichen, sind eine größere Gruppe von Tunesiern, die seit 1969 in der Stadt lebt, und die besonders in den Jahren nach 1995 ansässig gewordenen Iraker erwähnenswert.
Neben zwei türkischen Moscheevereinen der DITIB (Merkez-Moschee, Lindnergasse, 1978 gegründet, später DITIB-Verein) und des VIKZ (Adolf-Schmetzer-Straße) existieren ein „Albanisch-Islamisches Kulturzentrum“ (Alte Straubinger Straße), eine Niederlassung der Ahmadiyya (Von-Donle-Straße) und zwei arabische Vereine, der „Islamisch-Arabische Kulturverein“ (As-Siddiq, Walderdorffstraße) und das „Islamisch-Arabische Kulturzentrum“ (Al-Rahman-Moschee/Masjid Arrahman).

Nach seinem Auszug aus den früheren Räumlichkeiten in der Hemauerstraße errichtete letzterer im Jahr 2009 ein großes Gemeindehaus an der Alten Straubinger Straße.

Muslimische Friedhöfe befinden sich in einem Abschnitt des städtischen Friedhofs auf dem Dreifaltigkeitsberg sowie in der nördlich von Regensburg liegenden Ortschaft Kareth.

1978 etablierte sich in Regensburg eine buddhistisch praktizierende Meditationsgruppe unter Anleitung Lama Ole Nydahls. In der Folge gründete sich das buddhistische Zentrum Regensburg, seit 1999 in dem historischen Gebäude Brixener Hof, derzeit die größte buddhistische Gruppierung der Stadt. Das buddhistische Zentrum ist im gemeinnützigen Verein Buddhistische Zentren Bayern der Karma-Kagyü-Linie e. V. unter dem Buddhistischen Dachverband Diamantweg (BDD) e. V. organisiert.
Neben dem Diamantwegszentrum sind in Regensburg mehrere weitere buddhistische Gruppen aktiv, wie die buddhistische Meditationsgruppe Regensburg, Zen-Buddhisten und Won-Buddhisten.

Am 1. Januar 1904 wurde die bis dahin selbständige Gemeinde Karthaus-Prüll eingegliedert. Am 1. April 1924 kamen Reinhausen, Sallern, Schwabelweis, Stadtamhof, Steinweg, Weichs und Winzer hinzu. Am 1. April 1938 folgten Dechbetten, Großprüfening und Ziegetsdorf. Anlässlich der Gemeindegebietsreform in Bayern wurden Burgweinting, Harting und Oberisling am 1. Januar 1977 eingegliedert. Ein Teil der Nachbargemeinde Barbing mit mehr als 400 Einwohnern folgte am 1. Januar 1978.

Mit Beginn der Industrialisierung im 19. Jahrhundert setzte ein starkes Bevölkerungswachstum ein. Lebten 1830 in der Stadt 16.000 Einwohner, so waren es 1900 bereits 45.000. Bedingt durch zahlreiche Eingemeindungen in den Jahren 1924 und 1938 stieg die Einwohnerzahl bis 1939 auf 96.000. Im Jahre 1940 überschritt die Bevölkerungszahl der Stadt die Grenze von 100.000, wodurch sie zur Großstadt wurde. Am 31. März 2007 betrug die „Amtliche Einwohnerzahl“ nach Fortschreibung des Bayerischen Landesamtes für Statistik und Datenverarbeitung 131.489. Dabei zählen nur Hauptwohnsitze nach Abgleich mit den anderen Landesämtern. 2011 wurde erstmals die Grenze von 150.000 Einwohnern überschritten (152.089 Einwohner). Im Mai 2017 betrug der Bevölkerungsstand (Gesamtbevölkerung aus Haupt- und Nebenwohnsitzen) 164.896. Dabei waren 80.879 männlich und 84.017 weiblich.

Die "politische Führung" Regensburgs basierte jahrhundertelang auf der Reichsunmittelbarkeit. Regensburg erhielt 1245 durch Kaiser Friedrich II. das Recht der Selbstverwaltung und das Privileg, „einen Bürgermeister und Rat zu setzen“. Damit war sie Freie Reichsstadt und blieb es bis 1803. Der Rat hatte 16 Mitglieder. Diese Zahl wurde bis 1803 beibehalten. Zwischen 1803 und 1810 war Regensburg Kurfürstentum unter dem Reichserzkanzler Carl Theodor von Dalberg. 1809 stand die Stadt unter französischer Besatzung. Mit dem Übergang an Bayern 1810 wurde Regensburg Hauptstadt des Regenkreises und wurde ab 1811 durch einen königlichen "Polizeidirektor" geleitet.

Ab 1818 stand an der Spitze der Stadt ein "Erster Bürgermeister", der ab 1907 den Titel Oberbürgermeister (OB) erhielt. Heute gibt es neben dem OB noch zwei weitere hauptamtliche Bürgermeister. Der OB und der Stadtrat werden für eine Legislaturperiode von sechs Jahren gewählt. Beide Wahlen finden zum gleichen Termin statt. Der Stadtrat setzt sich aus 50 gewählten Mitgliedern und dem Oberbürgermeister zusammen. Darüber hinaus gibt es vier berufsmäßige Stadträte ohne Stimmrecht: Wirtschafts-, Wissenschafts- und Finanzreferent, Rechts- und Umweltreferent, Planungs- und Baureferent sowie ein Kulturreferent.

Die Stadtratswahl vom 16. März 2014 ergab folgendes Ergebnis (Stand: Mai 2014):

Dem Rat gehören sieben Fraktionen an. Die Piraten und die CSB haben keinen Fraktionsstatus. Der neue Stadtrat konstituierte sich am 8. Mai 2014.

Bei der Wahl zum Oberbürgermeister verfehlte Joachim Wolbergs (SPD) im ersten Wahlgang bei einer Wahlbeteiligung von 49,7 % die absolute Mehrheit um 18 Stimmen. Daher kam es am 30. März 2014 zu einer Stichwahl zwischen ihm und Christian Schlegl (CSU), der im ersten Wahlgang 32,3 % der Stimmen erreichte. Joachim Wolbergs ging aus der Stichwahl als Gewinner hervor, bei einer Wahlbeteiligung von 46,2 %.

Wolbergs ist Nachfolger von Hans Schaidinger (CSU), der seit 1996 das Amt des Oberbürgermeisters innehatte.

Am 18. Januar 2017 wurde der amtierende Oberbürgermeister Joachim Wolbergs (SPD) im Zuge der Regensburger Parteispendenaffäre wegen des Verdachts der Bestechlichkeit verhaftet und in Untersuchungshaft genommen. Als Haftgrund wurde von der zuständigen Staatsanwaltschaft Regensburg Verdunklungsgefahr angegeben. Am 27. Januar 2017 wurde er vorläufig des Dienstes enthoben, seitdem führt Bürgermeisterin Gertrud Maltz-Schwarzfischer kommissarisch die Amtsgeschäfte. Am 28. Februar 2017 wurde der Haftbefehl gegen Wolbergs mit Auflagen außer Vollzug gesetzt.

Regensburg unterhält folgende Städtepartnerschaften:


Am 10. November 1951 hat die Stadt Regensburg die Patenschaft über die Sudetendeutsche Volksgruppe übernommen.

Seit 1980 feiert die Stadt alljährlich am 10. November den "Stadtfreiheitstag." An diesem Tag im Jahre 1245 erhielt sie die Urkunde für ihre Selbstständigkeit, für die "Stadtfreiheit." Heute werden an diesem Tag verdiente Bürger der Stadt geehrt und ausgezeichnet.

Anlässlich des 750. Jahrestags der Reichsfreiheit stiftete die Stadt den Brückenpreis der Stadt Regensburg.

Regensburg verfügt über 1500 denkmalgeschützte Gebäude.
Davon bilden 984 im historischen Kern das Ensemble „Altstadt mit Stadtamhof“, welches 2006 von der UNESCO als Weltkulturerbe ausgezeichnet wurde. Sie ist die größte mittelalterliche Stadtanlage nördlich der Alpen.

Regensburg verfügt mit den Städtischen Bühnen über ein vollständig ausgestattetes Dreispartentheater und bietet so ein Programm von Oper, Operette, Musical, Schauspiel und Ballett. Das Ensemble tritt an mehreren Spielstätten in der Stadt auf:


Darüber hinaus gibt es noch eine ganze Reihe kleinerer Häuser. Darunter befindet sich das Figurentheater im Stadtpark, das Regensburger Bauerntheater in der Gaststätte Hubertushöhe, das STATT-Theater (Kleinkunstbühne), das Theater an der Universität und das Offene Theater Regensburg. Hinzu kommen die Regensburger Tage des Schülertheaters, dabei führen in drei Wochen im Juni etwa 20 Schulbühnen ihre Stücke auf.

Das offizielle Museumsportal der Stadt Regensburg listet 22 Museen und Dauerausstellungen auf. Dazu gehören neben städtischen Museen solche der Religionsgemeinschaften, sonstiger öffentlicher Träger und Privatsammlungen.

Von der Stadt Regensburg betrieben ist das am Dachauplatz im ehemaligen Minoritenkloster gelegene Historische Museum. Das Alte Rathaus beherbergt als "document Reichstag" das Reichstagsmuseum. Seine Hauptanziehungspunkte sind die aus dem Mittelalter unverändert erhaltene Folterkammer und der Reichssaal als Veranstaltungsort des immerwährenden Reichstages. Es folgt das Keplergedächtnishaus und die Städtische Galerie „Leerer Beutel“. Dort residiert auch der Jazz-Club Regensburg. Neu hinzugekommen ist die document Neupfarrplatz über die Synagoge und das frühere Judenviertel. Gemeinsam mit dem Naturwissenschaftlichen Verein Regensburg betreibt die Stadt das Naturkundemuseum Ostbayern.

Das im Bau befindliche Museum der Bayerischen Geschichte soll 2019 eröffnet werden. Das Kunstforum Ostdeutsche Galerie wird von einer gleichnamigen Stiftung getragen, welche wiederum der Freistaat gemeinsam mit Bund und Stadt trägt.

Das Bistum Regensburg unterhält die Bistumsmuseen Regensburg, die in Domschatzmuseum, Diözesanmuseum Obermünster und das Museum St. Ulrich aufgeteilt sind. Darüber hinaus ist das Domkapitel Träger des „document niedermünster“, das die Ausgrabungen unter der gleichnamigen Kirche erschließt.

Der Freistaat besitzt in Regensburg als Zweigmuseum des Bayerischen Nationalmuseums die Fürstliche Schatzkammer Thurn und Taxis Regensburg, die im ehemaligen Marstall des Schlosses St. Emmeram untergebracht ist. Das in der benachbarten Reithalle untergebrachte Marstallmuseum mit der Kutschensammlung ist nach wie vor im Eigentum des Fürstenhauses Thurn und Taxis.

Sonstige Museen sind das Besucherzentrum Welterbe Regensburg im Salzstadel, der Reptilienzoo in Burgweinting-Harting, das Donau-Schiffahrts-Museum, das Museum in der Dreieinigkeitskirche, das Psychiatriemuseum des Bezirksklinikums, die Volkssternwarte Regensburg sowie an Museen in privater Trägerschaft das Brückturmmuseum, das Uhrenmuseum, das Golfmuseum, das Postmuseum und das Dinoraeum. Das Feldbahnmuseum Friedrichzeche wird zusammen mit einem geologischen Lehrpfad auf einem Gelände im Süden von Dechbetten betrieben, auf dem nach wie vor Ton und Braunkohle abgebaut werden.

Am "Gewerkschaftshaus" in der "Richard-Wagner-Straße" wird seit 1986 mit einer Gedenktafel an zwei NS-Opfer aus der Arbeiterbewegung erinnert: an die SPD-Reichstagsabgeordnete Antonie Pfülf, die aus Verzweiflung über die Handlungsunfähigkeit ihrer Partei und der Gewerkschaften 1933 in den Freitod gegangen war. Nach ihr wurden auch die örtliche Parteizentrale benannt sowie ein Preis mit ihrem Namen gestiftet. Der andere war der SPD-Landtagsabgeordnete Alfons Bayerer, der nach langjähriger Zuchthaushaft 1939 an den Haftfolgen starb.

Seit 1986 erinnert eine Gedenktafel am Jüdischen Gemeindehaus an die Zerstörung der Synagoge und an die Verfolgung und Ermordung hunderter jüdischer Bürger, die Opfer des Holocaust wurden. Eine Erinnerungstafel an ermordete jüdische Schülerinnen haben Schüler des Von-Müller-Gymnasiums 1987 im Foyer ihrer Schule angebracht. Auf dem Jüdischen Friedhof an der Schillerstraße geben Inschriften auf Grabsteinen Auskunft über den gewaltsamen Tod dieser Verstorbenen.

An der sogenannten Alten Pforte des Bezirksklinikums erinnert seit 1990 eine Gedenktafel an die 638 ermordeten Psychiatriepatienten und an weitere Opfer des nationalsozialistischen Rassenwahns.

In einer Grünanlage an der "Kreuzung Siemens-/Straubingerstraße" erinnert seit 1988 das „Mahnmal am Hohen Kreuz“ (Bildhauer Heinrich Glas) an 700 sowjetische Kriegsgefangene, die im Zweiten Weltkrieg Opfer brutaler Zwangsarbeit und unmenschlicher Lebensbedingungen wurden. Mit einem Mahnmal wird der Opfer des KZ-Außenlagers Colosseum gedacht. Der Gedenkstein steht in Stadtamhof. Die Inschrift nennt als Standort des Außenlagers allgemein Stadtamhof, nicht jedoch konkret das Gebäude des ehemaligen Gasthofs „Colosseum“.

An die Morde an Domprediger Johann Maier, dem Lagerarbeiter Josef Zirkl und dem Polizeiinspektor Michael Lottner am 23. April 1945 wird an mehreren Stellen der Stadt erinnert. Sie hatten versucht, die Forderung der „Regensburger Frauendemonstration“ nach kampfloser Übergabe der Stadt an die US-Truppen zu vermitteln und wurden daraufhin von Beamten der Gestapo erschossen bzw. öffentlich erhängt. Daran erinnern seit 1946 eine Gedenktafel im Dom, zwei weitere Tafeln seit 1950 am Ort ihrer Hinrichtung am Dachauplatz sowie seit 1975 ein Denkmal in einer Grünanlage am Dachauplatz.

Das Mahnmal steht an exponierter Stelle an der Furtmayrstraße/Ecke Galgenbergstraße in Form der Telefonnummer des NOTRUF e. V. Regensburg (Notruf für vergewaltigte und belästigte Frauen und Mädchen) seit dem internationalen Frauentag am 8. März 2000.

Regensburg hat ein reiches Musikleben. International bekannt sind die Regensburger Domspatzen. Daneben etablierten sich erfolgreiche Ensembles wie Singer Pur oder Cantabile Regensburg. Die Alte Musik wird alljährlich im Festival „Tage alter Musik“ gepflegt. Sie verbinden historische Aufführungspraxis mit Konzerten in den historischen Räumen Regensburgs. Die klassische Musik wird in Konzertreihen auf hohem internationalen Niveau in Regensburg präsentiert. Insbesondere die "Odeon Concerte" im Audimax der Universität bringen internationale Orchester in die Domstadt. Seit 2003 gibt es im Juli die Regensburger Schlossfestspiele im Innenhof des Schlosses Thurn und Taxis. Auch sie finden im süddeutschen Raum immer mehr Beachtung.

Moderne Musikrichtungen, besonders der Jazz, werden alljährlich im Sommer während des Bayerischen Jazzweekends gepflegt. An einem verlängerten Wochenende im Sommer treten an mehreren Spielstätten in der Altstadt über einhundert verschiedene Bands, Combos und Solisten auf. Die Jury des Bayerischen Jazzinstituts sorgt für ein hohes musikalisches Niveau.

Seit 1996 gibt es das "Regensburger Music College", eine private Berufsfachschule für Pop, Rock und Jazz.

In Regensburg gibt es sechs Kinos mit insgesamt 15 Leinwänden. Außerdem gibt es zwei Open-Air-Kinos. Das erste wird von einem Lichtspielhaus und einem Restaurant am Donauufer in der Altstadt organisiert, in welchen über mehrere Wochen aktuelle Filme, aber auch Klassiker und Publikumslieblinge vergangener Jahre gezeigt werden. Das zweite Open-Air-Kino hat seinen Standort im Sommer in der Armin-Wolf-Arena. Dort werden ebenfalls aktuelle Filmhighlights gezeigt.

Im März findet die Internationale Kurzfilmwoche Regensburg statt, gefolgt von cinEScultura und der Stummfilmwoche im Innenhof des Historischen Museums Regensburg.

Im Stadtgebiet gibt es 1346 Baudenkmäler. 

Die Steinerne Brücke mit dem Brückturm wurde 1135 bis 1146 gebaut. Sie gehört zu den bedeutendsten Brückenbauwerken des Mittelalters und war unter anderem Vorbild für die Prager Karlsbrücke. Das Alte Rathaus mit dem Reichssaal war Sitz des Immerwährenden Reichstages. Der "Herzogshof" mit "Römerturm" am heutigen "Alten Kornmarkt" bildete die frühere Herzogspfalz der agilolfingischen Herzöge.

Aus der Römerzeit sind beim Bischofshof die Porta praetoria, ein Stadttor, und an der "Adolph-Kolping-Straße," im Parkhaus an der "D.-Martin-Luther-Straße" und am "Ernst-Reuter-Platz" Reste der römischen Kastellmauern erhalten.

Das um 1300 errichtete gotische "Ostentor" war das Eingangstor zur Stadt von Osten. Die "Historische Wurstkuchl" an der Donau gilt als älteste Wurstbraterei der Welt. An der Stelle des früheren Hafens findet man unterhalb der Steinernen Brücke den Salzstadel und oberhalb den Amberger Stadel. Geprägt ist das Stadtbild auch durch die so genannten Patrizierburgen bzw. Geschlechtertürme wie das "Haus an der Heuport" oder das "Goldene Kreuz" am "Haidplatz," das als Kaiserherberge für Karl V. diente. Weitere große Patrizierburgen sind das Goliathhaus, das Runtingerhaus und das Zandthaus. Der 1260 entstandene Goldene Turm in der Wahlenstraße ist wohl der bekannteste der Regensburger Geschlechtertürme, mit denen die Patrizierfamilien ihren Reichtum und Einfluss zur Schau stellten. Ebenfalls sehenswert ist der "Baumburger Turm."

Regensburg war im Mittelalter und bis in die Neuzeit eine Handelsstadt von europäischer Bedeutung. Die Patrizierburgen sind nur die überkommene machtvolle Zurschaustellung dieser herausragenden Position. Von größerer praktischer Bedeutung waren die zahlreichen öffentlichen und privaten Speicherbauten, in denen das Handelsgut gelagert wurde. Nur ganz wenige der Stadel sind in einem äußerlich unberührten Zustand erhalten geblieben, der die ursprüngliche Funktion dieser wichtigen Nutzbauten einer Handelsstadt dokumentiert: Neben den öffentlichen Speicherbauten „Salzstadel“, „Amberger Stadel“ und „Leerer Beutel“ ist hier der um 1580 erbaute und ehemals als Gewürzspeicher dienende große private Stadel in der "Westnerwacht, Weintingergasse 4" zu nennen.

Die königliche Villa am Ostrand der Altstadt entstand von 1854 bis 56 im Auftrag von König Maximilian II. im Stil der englischen Neugotik. Das fürstliche Schloss St. Emmeram ist mit 500 Zimmern das größte bewohnte Schloss in Deutschland. In der Nähe des Hauptbahnhofs befindet sich einer der acht letzten noch erhaltenen Pilzkioske, der hier „Milchschwammerl“ genannt wird.

In Regensburg gibt es eine Vielzahl historischer Kirchen und mehrere teils ehemalige Klöster. Der Dom St. Peter ist das Hauptwerk der Gotik in Bayern.
Nach mehreren Vorgängerbauten dürfte der gotische Dom bald nach 1260 begonnen worden sein. Ein vorläufiger Abschluss ist mit dem Jahr 1520 anzusetzen. Von 1859 bis 1872 erfolgte erst der Ausbau der Turmhelme und der Querhausgiebel. Die letzte große Innenrenovierung fand von 1985 bis 1988 statt.

Die Stiftskirche und Basilica minor zu Unserer lieben Frau zur Alten Kapelle (Stift Unserer Lieben Frau zur Alten Kapelle) wurde um 875 erbaut. Um die Mitte des 18. Jahrhunderts wurde sie neu ausgestattet und zählt seither zu den prachtvollsten Kirchen des Rokoko in ganz Bayern.

Die ursprünglich romanische, später stark barockisierte Kirche und heutige Pfarrkirche St. Emmeram war früher Teil der gleichnamigen, 1803 säkularisierten Fürstabtei Sankt Emmeram, deren Klostergebäude ab 1812 in das neu erbaute Schloss St. Emmeram integriert wurden. Sie hat den Status einer päpstlichen Basilica minor. Beachtenswert ist ebenfalls die Nebenkirche und vormalige Pfarrkirche St. Rupert.

Die Kirche St. Jakob auch bekannt unter dem Namen "Schottenkirche," eine romanische Basilika aus dem 12. Jahrhundert, leitet ihren Namen vom Kloster der irischen Benediktiner "(Skoten)" ab, zu dem sie gehörte. Der Haupteingang, das "Schottenportal", ist wegen seiner einzigartigen Steinmetzarbeiten weltberühmt.

Die frühgotische Kirche St. Ulrich beherbergt das Bistumsmuseum des Bistums Regensburg.

Die Klosterkirche St. Matthias gehörte zur Klosteranlage St. Klara in der Ostengasse.

Am Neupfarrplatz befindet sich die evangelische Neupfarrkirche. Eine der ersten evangelisch-lutherischen Kirchenneubauten in Bayern ist die im Auftrag der Stadt geplante und 1631 fertiggestellte Dreieinigkeitskirche, eine säulenlose Hallenkirche mit Emporen, die 1000 Sitzplätze bietet. Die Innenausstattung mit dem Holzgestühl ist noch im Originalzustand erhalten. Ihr Nordturm ist begehbar und bietet einen Panoramablick über die Dachlandschaft der Stadt Regensburg bis zur Walhalla.

Regensburg verfügt über einen die Altstadt fast vollständig umschließenden Grüngürtel, in den die Straßenbaumaßnahmen des 19. und 20. Jahrhunderts einige Lücken gerissen haben. Der Grüngürtel stößt im Osten am Herzogspark und im Westen am Villapark an die Donau an und entstand 1779–1782 nach Anregung und auf Kosten von Karl Anselm von Thurn und Taxis. Er trägt deshalb den Namen Fürst-Anselm-Allee und entstand zunächst als zweireihige Baumallee auf dem Gelände vor den maroden Befestigungsanlagen der Stadt (Stadtmauer, Stadtgraben, Zwinger mit Zwingermauer), die erst Laufe des 19. Jahrhunderts abgebrochen wurden. Ein nach Abbruchmaßnahmen vor dem Peterstor entstandenes Areal im Süden, das bis 1809 vom Vizepräsidenten des Landesdirektoriums Kaspar Maria von Sternberg als Botanischer Garten mit Gartenpalais genutzt worden war, wurde 1813 von Karl Alexander von Thurn und Taxis aufgekauft. Auf diesem Gelände entstand im Laufe der Folgejahre unter dem Einfluss der Fürstin Therese von Thurn und Taxis der Schlosspark der Fürsten von Thurn und Taxis.

Bemerkenswerte Parks im Einzugsbereich der Altstadt sind der Herzogspark, der Villapark, der Dörnbergpark und der Stadtpark. Am Oberen Wöhrd befindet sich der "Inselpark". Die größte Grünanlage, der "Donaupark", liegt im Westen an der Donau. Dort befinden sich auch das "Westbad" und der "Westbadweiher". Weitere Parks südlich der Donau sind der "Königswiesener Park", der "Georg-Hegenauer-Park", der "Karl-Freitag-Park", die "Grünanlagen der Universität" und der "Ostpark" an der Landshuter Straße, ein ehemaliger Exerzierplatz; nördlich der Donau liegen der "Hans-Herrmann-Park", der "Aberdeen-Park" und der "Tempe-Park".

An der Frankenstraße befindet sich der eigentümliche Max-Buchhauser-Garten mit seinen grotesken Skulpturen. Beliebtes Spaziergebiet sind die Winzerer Höhen mit gutem Ausblick auf die Stadt und der Möglichkeit der Weiterwanderung zum Biergarten in Adlersberg. Weitere beliebte Ausflugsgebiete im Stadtgebiet sind die Wanderungen von Keilberg aus, der Burgweintinger Wald sowie der Max-Schultze-Steig am westlichen Donauufer.

Im Stadtgebiet gibt es vier Naturschutzgebiete, ein Landschaftsschutzgebiet, drei FFH-Gebiete und mindestens zehn vom Bayerischen Landesamt für Umwelt ausgewiesene Geotope (Stand August 2016).


Die Sektion Regensburg des Deutschen Alpenvereins unterhält das DAV-Kletterzentrum Regensburg sowie einige Alpenhütten. Sie ist größter Sportverein der Stadt Regensburg, die achtgrößte DAV-Sektion und einer der größten Sportvereine Deutschlands auf Rang 29.

Die Regensburg Phoenix spielen American Football in der viertklassigen Bayernliga. Das Team ging aus dem ehemaligen Bundesligisten Regensburg Royals hervor.

Der DJK SB Regensburg spielt in der Saison 2016/17 in der Bayernliga. Fortuna Regensburg, bis zur Saison 2008/09 in der Bundesliga und 1990 Deutscher Meister, spielt heute in der Bezirksliga.

Die Buchbinder Legionäre spielen seit 1993 in der Baseball-Bundesliga. In den Jahren 2008 und 2010 bis 2013 wurden die Legionäre Deutscher Meister. Ihre Spielstätte ist die Armin-Wolf-Arena, das größte Baseballstadion Deutschlands. In Regensburg wurden Spiele der Baseball-Weltmeisterschaft (2009), des World Baseball Classic (2013) und der Baseball Europameisterschaft (2014) ausgetragen.

Die Eisbären Regensburg spielen in der drittklassigen Oberliga Süd. Bis zur Insolvenz im Jahr 2009 spielte die Mannschaft in der 2. Eishockey-Bundesliga.
Der EHC Regensburg spielt – mit einer Unterbrechung der Teilnahme am Ligenspielbetriebs zwischen 2012 und 2015 – in der Bezirksliga Ost. Beide Mannschaften spielen in der 1998 im Stadtosten neu errichteten Donau-Arena, die das alte Eisstadion an der Nibelungenbrücke ersetzte.

In der Saison 2017/18 tritt der SSV Jahn Regensburg in der 2. Bundesliga an, dessen zweite Mannschaft in der sechstklassigen Landesliga. Die Frauenmannschaft des SC Regensburg spielte in den Spielzeiten 2006/07 und 2007/08 in der 2. Bundesliga. Aktuell spielen sie in der drittklassigen Regionalliga Süd.

Die Damen des ESV 1927 Regensburg spielen seit dem Aufstieg in die Regionalliga 2008 ununterbrochen in der dritthöchsten deutschen Spielklasse, derzeit in der 3. Liga, Staffel Süd. Die Herren der SG Regensburg spielen in der Landesliga.

Die LG Telis Finanz Regensburg hat im Sportjahr 2007 zwei Welt- und elf Europameisterschaften, sowie 10 deutsche Meisterschaften errungen. Dazu kommen noch 55 Bayerische Meistertitel. Damit zählt sie zu den erfolgreichsten Leichtathletikvereinen Deutschlands.

Neben dem Veloclub Ratisbona, der als einer der größten Radsportvereine in Bayern Ausrichter des bekannten Arberradmarathons ist, kümmert sich unter anderem der RSC88 Regensburg seit 1988 um ambitionierte Mountainbiker und Radrennfahrer in Regensburg. 2007 war Regensburg zum zweiten Mal nach 1950 Etappenzielort der "Deutschland Tour," des wichtigsten internationalen Radrennens auf deutschem Boden.

Der Rugby Club Regensburg 2000 stieg 2015 in die 2. Rugby Bundesliga auf. Der Verein veranstaltet jährlich Turniere mit internationaler Beteiligung. Die Frauenmannschaft des RCR 2000 tritt in der Disziplin 7er-Rugby an. 2008/09 wurde der Verein erstmals Bayerischer Meister.

Die Damenmannschaft des TC Rot-Blau Regensburg spielt seit 2015 als "Eckert-Tennis-Team" in der 1. Bundesliga. 2016 wurde sie erstmals Deutscher Mannschaftsmeister. Derzeit gehören unter anderem die Topspielerinnen Julia Görges, Karolína Plíšková oder Angelique Kerber (seit September 2016 Weltranglistenerste) zum Kader.


Das Regensburger Kulturleben bietet einige herausragende, regelmäßige Veranstaltungen: zweimal im Jahr – Anfang Mai und Ende August – treffen sich die Regensburger zu ihrem Volksfest, der Regensburger Dult. Das Bürgerfest in der gesamten Altstadt findet alle zwei Jahre an einem langen Wochenende im Sommer statt und zieht weit über 100.000 Besucher an. Ebenfalls im Zweijahresintervall ist Ende März und Anfang April die neuntägige Donauausstellung (DONA) mit vielen Sonderausstellungen zu sehen. Jeweils am zweiten Juliwochenende treffen sich Ritter, Gaukler und Spielleute beim Regensburger Spectaculum, einem Mittelaltermarkt, unter den Bögen der Steinernen Brücke auf der Jahninsel. Jeden Juni findet in Stadtamhof das Oberpfälzer MundArt-Festival statt. Im Dezember folgen die Weihnachtsmärkte, besonders hervorzuheben ist hier der Christkindlmarkt. Seit 1987 findet auf der Jahninsel jedes Jahr im Sommer das Jahninselfest statt – ein mittlerweile oberpfalzweit bekanntes Jugendkulturfest mit Live-Musik, Kleinkunst und Kinderprogramm.

Die größte Regensburger Sportveranstaltung ist der Regensburg-Marathon am Sonntag nach Christi Himmelfahrt. Mit über 1000 Marathon- und über 3000 Halbmarathonfinishern gehört er zu den 20 größten Stadtmarathons in Deutschland. Am zweiten Sonntag im August folgt der Regensburg Triathlon über Kurz- und Jedermann-Distanz. Ebenfalls im August findet seit 2010 der Triathlon „Ironman Regensburg“ über die Langdistanz statt. Der Arberradmarathon führt am letzten Sonntag im Juli mehr als 6000 Teilnehmer auf unterschiedlich anspruchsvollen Strecken von bis zu 250 Kilometer Länge von Regensburg in den Bayerischen Wald und zurück.

Einmal im Jahr findet das Regensburger Uni Salsa Camp statt. Dort treffen jedes Jahr internationale Größen aus der Salsaszene sowie Teilnehmer aus dem kompletten Bundesgebiet ein.

Die Traumfabrik ist ein Showtheater, das seit 1983 jährlich Veranstaltungen im Bereich Sport, Theater und Körpererfahrung in Regensburg durchführt.

Der wirtschaftliche Aufschwung Regensburgs nach dem Zweiten Weltkrieg begann relativ spät. Die Fachhochschule und die Gründung der Universität 1967 bildeten die Grundlage für eine sehr dynamische wirtschaftliche Entwicklung, gestärkt durch die Ansiedlung einer Reihe von Großunternehmen. Die Arbeitslosenquote lag im Januar 2008 mit 5,0 % unter dem bayerischen Landesdurchschnitt. So erreicht Regensburg heute die zweithöchste Arbeitsplatzdichte unter allen Großstädten in Deutschland nach Frankfurt am Main mit 760 je 1000 Einwohnern. Die im Verhältnis zu den Hauptwohnsitzen hohe Zahl von 105.142 (Stand: 2012) sozialversicherten Beschäftigten folgt aus den starken Pendlerströmen aus dem Umland und führt zu einem hohen Bruttoinlandsprodukt der Stadt. Das Bruttoinlandsprodukt je Einwohner belief sich im Jahr 2013 auf 71.576 Euro, was deutschlandweit Platz 5 bedeutet. Im "Prognos Zukunftsatlas 2007," der 439 Kreise und kreisfreie Städte in Deutschland nach ihrer Stärke und Dynamik miteinander vergleicht, zählt Regensburg auf Platz 5 zu den acht „Top-Regionen mit Zukunftschancen“. Im Zukunftsatlas des Jahres 2013 erreichte die Stadt Rang 7 und wurde als „dynamischste Stadt“ Deutschlands bewertet, in der die beste Entwicklung der vergangenen zehn Jahre stattfand. Beim Zukunftsatlas 2016 belegte die Stadt Platz 11.

Seit 1903 wird in kleinen Mengen Braunkohle im Tagebau abgebaut.

2005 arbeiteten 75 der abhängig Beschäftigten in der Land- und Forstwirtschaft, 30.387 im verarbeitenden Gewerbe, 2.458 im Baugewerbe, 11.365 im Handel, 4.607 für Verkehr und Nachrichtenübermittlung, 3.220 im Kredit- und Versicherungsgewerbe, 32.844 im Sektor Dienstleistungen und 6.147 in den Verwaltungen der ansässigen Gebietskörperschaften. 1.192 Personen arbeiten in sonstigen Bereichen. Das verarbeitende Gewerbe hat seine Schwerpunkte in Automobilbau und -zulieferung, in der Elektrotechnik, in der Mikroelektronik und im Maschinenbau.

Außerdem ist Regensburg seit 2006 Sitz der Geschäftsstelle des bayerischen "Cluster Sensorik". Die vom Amt für Wirtschaftsförderung der Stadt Regensburg initiierte Strategische Partnerschaft Sensorik e. V. ist ein Technologie-Netzwerk mit über 30 Mitgliedern aus dem Bereich Sensorik im Raum Ostbayern.

Zu den bedeutendsten Firmen der Stadt gehören heute Bayernwerk, BMW, Continental Automotive, Siemens, Infineon, Osram Opto Semiconductors, Maschinenfabrik Reinhausen, BSH Hausgeräte GmbH, Schneider Electric, AREVA, die SGB-SMIT Holding, die Deutsche Telekom, General Electric und Andritz.

Außerdem war Regensburg der Standort der ältesten Zuckerfabrik Bayerns; das 1899 gegründete Werk gehörte zur Südzucker AG und wurde Ende 2007 stillgelegt.

Die Ansiedlung von Zukunftsbranchen wird durch die Stadt aktiv gefördert. Im Fokus stehend dabei die Bereiche Energietechnik, Biotechnologie, IT und Sensorik. Einrichtungen wie der Gewerbepark Regensburg, der Energiepark Regensburg, das Technologiezentrum TechBase oder der BioPark mit inzwischen über 40 Biotech-Firmen sind Bestandteile einer wirtschaftlichen Neustrukturierung der Stadt.

Zu Zeiten des New-Economy-Booms wurden hier eine Reihe von Unternehmen gegründet wie "ABC Telebuch", dem Vorläufer von Amazon.de; "Adori AG," "Feedback AG," "Offerto.de" oder die SPiN AG, womit Regensburg zeitweise eines der Zentren der deutschen Internetwirtschaft war. Einige Unternehmen sind zwischenzeitlich in geografisch günstiger gelegene Regionen in Deutschland übergesiedelt (von Amazon ist zum Beispiel nur ein Teil des Kundensupports verblieben), während andere Unternehmen, wie z. B. Adori oder Feedback, mit dem Absturz des Neuen Marktes untergingen.

Große Flächen für den Einzelhandel entstanden im 1967 errichteten und mehrmals erweiterten Donau-Einkaufszentrum im Nordosten der Stadt. In den letzten Jahren entstanden am Bahnhof innerstädtisch zusätzlich die "Regensburg Arcaden".

Von der Vielzahl Regensburger Brauereien sind nur die drei Stiftungsbrauereien Kneitinger, Bischofshof und die Spitalbrauerei geblieben. Die am Ort befindliche Fürstliche Brauerei Thurn und Taxis wurde 1996 durch Paulaner übernommen. Neben den drei Stiftsbrauereien wird in der Gastwirtschaft „Regensburger Weißbräuhaus“ und dem „Fürstlichen Brauhaus“ noch für den Eigenbedarf gebraut.

Im Jahr 2012 hatte die Stadt Regensburg 531.943 Ankünfte zu verzeichnen. Seit Mitte der 1990er Jahre ist bei den Touristenzahlen eine starke Zunahme festzustellen, so dass sich der Tourismus für Regensburg mittlerweile zu einem beachtlichen Wirtschaftsfaktor entwickelt hat.

Aufgrund der starken Bevölkerungszunahme in den letzten Jahren ist die Immobiliensituation in Regensburg heute durch eine hohe Dynamik gekennzeichnet. So stiegen die Immobilienpreise in jüngster Vergangenheit mit am stärksten innerhalb Deutschlands.

Mit dem Autobahnkreuz Regensburg und dem Regensburger Hauptbahnhof ist die Stadt ein Autobahn- und Eisenbahnknotenpunkt in Ostbayern.

Ein Projekt, das seit den 1980er Jahren im Plan der Stadt zum Ausbau der Verkehrsanbindung aufgeführt ist, ist der Beschluss zum Bau der Ostumgehung, die den Stadtnorden mit dem Stadtosten verbinden soll. Der ursprünglich für Oktober 2009 geplante Bau der Ostumgehung konnte erst nach einem Bürgerentscheid im März 2010 begonnen werden. Die Baumaßnahme wurde im November 2014 abgeschlossen und für den Verkehr freigegeben.

Mit der Reduzierung der Bedeutung der Eisenbahn ging eine Aufwertung der Straßenanbindung einher. Bis in die 1980er Jahre mündeten alle Bundesautobahnen um Regensburg in Bundesstraßen. In der Folgezeit wurde Regensburg stufenweise vollständig an das deutsche Bundesautobahnnetz angeschlossen.



Neben der Steinernen Brücke, die inzwischen für den motorisierten Verkehr gesperrt ist, existieren im Stadtgebiet als überregional bedeutsame Brücken die Nibelungenbrücke, die Donaubrücke Pfaffenstein sowie die Donaubrücke Schwabelweis.

Regensburg liegt an drei touristischen Straßen, der "Deutschen Limesstraße", der "Straße der Kaiser und Könige" sowie der "Europäischen Goethe-Straße".

Von Regensburg aus gehen Bahnstrecken nach Passau, Nürnberg, München, Hof und Ingolstadt. Im Fernverkehr wird die Stadt von ICEs der DB auf der Linie (Dortmund –) Frankfurt am Main – Wien sowie EuroNight-Nachtzügen der ÖBB angefahren. Im Nahverkehr fahren Regionalzüge der DB Regio, der Vogtlandbahn (als VBG und Alex) und von agilis die Stadt an.

Zentraler Bahnknoten ist der Hauptbahnhof. Daneben existieren aktuell in Burgweinting und Prüfening weitere Haltestellen in der Stadt. Die Haltestelle Walhallastraße soll bis 2017 im Nordosten der Stadt in der Nähe der Donau-Arena neu errichtet werden.

Bis in die 1970er Jahre war Regensburg der Sitz einer Eisenbahndirektion und Schnittpunkt von Fernverkehrszügen. Bis zur Eröffnung des Main-Donau-Kanals 1992 war die Bahnstrecke Nürnberg–Passau diejenige mit dem höchsten Güteraufkommen in Deutschland. Regensburg erfuhr durch den Wegfall der Interzonenzüge und der Einstellung der Interregio-Züge eine Bedeutungsreduzierung. Auf den Strecken Regensburg – Landshut – München, Regensburg – Weiden – Hof und Regensburg – Ingolstadt – Ulm verkehren seitdem nur noch Regionalzüge. Damit ist Regensburg kein Fernverkehrskreuz mehr.

Regionale Bahnstrecken nach Alling, zur Walhalla bzw. nach Wörth an der Donau und nach Falkenstein wurden bereits in der Nachkriegszeit stillgelegt.
Den öffentlichen Personennahverkehr (ÖPNV) versorgen 70 Buslinien der Regensburger Verkehrsbetriebe GmbH (RVB), die dem Regensburger Verkehrsverbund (RVV) angehören.

Zwischen 1903 und 1964 betrieb die Stadt ein kleines Straßenbahnnetz, ergänzend dazu verkehrte von 1953 bis 1963 der Oberleitungsbus Regensburg. Anfang der 1980er Jahre scheiterte ein Vorhaben zur Untertunnelung des Altstadtabschnittes für die Stadtbusse. Es laufen Planungen für den Bau eines Stadtbahnnetzes. Dafür werden bereits Trassen freigehalten beziehungsweise Bauwerke wie die Nibelungenbrücke entsprechend ausgelegt.

Der Hafen Regensburg ist mit einem Güterumschlag 2013 von insgesamt 8.002.000 Tonnen (Schiff: 1.645.000 Tonnen) der größte Hafen Bayerns. Der Hafen liegt an der Donau und ist ein wichtiger Umschlagplatz zwischen den Nordseehäfen und Osteuropa. Zur nördlichen Umfahrung der Steinernen Brücke wurde der Regensburger Europakanal errichtet.

Regensburg ist Kreuzungspunkt mehrerer Fernradwege: Der europäische EuroVelo-Fernradweg Flüsseroute EV 6 verbindet Regensburg mit dem Atlantik und dem Schwarzen Meer, der Donauradweg führt von der Donauquelle nach Budapest, der "Waldnaabtal-"/"Naabtal-Radweg" von Bärnau nach Regensburg, der "Regental-Radweg" von Regensburg nach Bayerisch Eisenstein, der Deutsche Limes-Radweg von Bad Hönningen nach Regensburg, der "Falkenstein-"/"Festspiel-"/"Chambtal-Radweg" von Regensburg nach Falkenstein/Cham/Furth im Wald, sowie der 5-Flüsse-Radweg (Donau-Naab-Vils-Pegnitz-Altmühl), ein Rundweg von Regensburg über Amberg, Sulzbach-Rosenberg, Nürnberg, Neumarkt und Kelheim wieder nach Regensburg.

15 Kilometer nördlich von Regensburg und drei Kilometer nordwestlich von Regenstauf befindet sich der Flugplatz Regensburg-Oberhub. Eingestuft als Verkehrslandeplatz wird er überwiegend zur Ausübung des Luftsports mit Motorflugzeugen, Segelflugzeugen, Ultraleicht-Flugzeugen und Hubschraubern genutzt.

1992 wurde das Universitätsklinikum in Betrieb genommen. Es bildet den Abschluss des Ausbaus der Universität Regensburg zur "Volluniversität," in der alle wesentlichen wissenschaftlichen Studiengänge angeboten werden. Die erste Ausbaustufe umfasste seit den frühen 1980er-Jahren die Zahn-, Mund- und Kieferklinik. Das Klinikum zählt zu den modernsten in Europa. Es hat 3.800 Beschäftigte, etwa 1.500 Studierende der Human- und Zahnmedizin, die höchste Versorgungsstufe "vier" sowie einen Einzugsbereich von 2,2 Millionen Einwohnern aus den Regierungsbezirken Oberpfalz und Niederbayern. Aktuell verfügt es über 833 Betten und 12 Dialyseplätze. 2013 wurden 125.500 ambulante Behandlungen und 31.500 stationäre Behandlungen bei einer durchschnittlichen Verweildauer von acht Tagen durchgeführt.

Die weiteren, teilweise mit dem Uniklinikum zusammen arbeitenden Krankenhäuser sind das Krankenhaus St. Josef, das Krankenhaus der Barmherzigen Brüder, die Frauen- und Kinderklinik St. Hedwig sowie die Kinder-Universitätsklinik Ostbayern KUNO. Des Weiteren befindet sich in Innenstadtnähe am Emmeramsplatz das Evangelische Krankenhaus. Seit 1852 gibt es mit dem Bezirksklinikum Regensburg eine Nervenheilanstalt in der Stadt.

Aus Regensburg berichten mehrere Rundfunkanstalten. Der Bayerische Rundfunk unterhält das Regionalstudio Ostbayern und der regionale Fernsehsender TVA sendet Informationen über die Region Regensburg/Kelheim/Straubing/Cham. Die Radiosender Gong FM, Absolut Radio, Absolut relax, Radio Galaxy und Radio Charivari Regensburg haben ebenfalls ihren Sitz in Regensburg. Die Mittelbayerische Zeitung, die größte Tageszeitung der Region, erscheint in Regensburg. Eine weitere Regensburger Tageszeitung ist die „Donau-Post“, Ableger des Straubinger Tagblatts. Erwähnenswert ist auch die seit 1998 erscheinende „Regensburger Soziale Straßenzeitung und Monatsmagazin Donaustrudl“ mit einer Auflage von etwa 7000 Exemplaren. Seit dem Frühjahr 2008 existiert mit regensburg-digital eine unabhängige Online-Zeitung in Regensburg.

Regensburg besitzt drei Hochschulen und damit alle Bildungseinrichtungen des ersten und zweiten Bildungsweges. Die 1962 gegründete "Universität Regensburg" nahm als vierte Universität Bayerns 1967 mit allen wesentlichen Fakultäten den Lehrbetrieb auf. Als ihr akademischer Nukleus fungierte die "Theologische Hochschule" am Ägidienplatz. Die "Ostbayerische Technische Hochschule Regensburg" wurde 1971 als Nachfolgerin des Polytechnikums, der Höheren Fachschulen und weiterer Einrichtungen gegründet. Die "Hochschule für Katholische Kirchenmusik und Musikpädagogik Regensburg" erhielt 2001 den Hochschulstatus. Sie ging aus der 1874 gegründeten weltweit ersten katholischen Kirchenmusikschule hervor. Mit insgesamt etwa 32.000 Studierenden (Stand Oktober 2014) besitzt Regensburg eine der höchsten Studierendendichten in Deutschland (ca. 23 %).

Zu den Hochschuleinrichtungen im weiteren Sinne gehören die Forschungs- und Gründerzentren. Im neue erbauten Technologiezentrum TechBase finden Neugründungen der Informationstechnik eine erste Bleibe. Zusammen mit der Fakultät für Informatik der OTH Regensburg fokussieren sich diese Einrichtungen u. a. auf den Schwerpunkt IT-Sicherheit. Auf dem Campus der Universität befindet sich der Biopark. Diese Einrichtung steht Neugründungen im Bereich der Biotechnologie und Life-Sciences zur Verfügung. Hier hat eine Arbeitsgruppe der Fraunhofer-Gesellschaft (FhG), mit Arbeitsschwerpunkt Tumor- und Alterskrankheiten, ihren Sitz. Unter dem Dach des Wissenschaftszentrums Ost- und Südosteuropa Regensburg kooperieren das Leibniz-Institut für Ost- und Südosteuropaforschung, das Institut für Ostrecht, das Hungaricum – Ungarisches Institut und das Forschungszentrum Deutsch in Mittel-, Ost- und Südosteuropa.

Regensburg hat ferner zahlreiche allgemeinbildende Schulen. Insgesamt gibt es 18 Grundschulen, sechs Hauptschulen, sechs Förder- und Sonderschulen, fünf Realschulen und acht Gymnasien. Zudem verfügt die Stadt über mehrere berufliche Schulen, vier Berufsschulen, zwei Wirtschaftsschulen, eine Fachoberschule und zwei Berufsoberschulen sowie 14 Berufsfachschulen. Hinzu kommen 13 freie Bildungseinrichtungen privater und öffentlicher Träger. Größte Einrichtung dieser Art sind die Eckert-Schulen für Berufsfortbildung. Außerdem gibt es seit 2010 eine Swiss International School in Regensburg.

Im Bereich Erwachsenenbildung ist die Volkshochschule der Stadt Regensburg tätig. Im Regensburger Bibliotheksverbund haben sich über 20 Einrichtungen des Bibliotheks-, Archiv- und Dokumentationswesens aus der Stadt und der Region Regensburg zusammengeschlossen.

In Regensburg gibt es ein Amtsgericht, ein Landgericht, ein Arbeitsgericht, ein Verwaltungsgericht, ein Sozialgericht und eine Staatsanwaltschaft. Außerdem befindet sich dort eine Justizvollzugsanstalt. Darüber hinaus sind in der Stadt das Amt für Ernährung, Landwirtschaft und Forsten Regensburg, das Wasserwirtschaftsamt und das staatliche Bauamt für die Stadt und den Landkreis Regensburg sowie die Landkreise Cham und Neumarkt in der Oberpfalz und die Berufsgenossenschaft der Bauwirtschaft mit ihrem arbeitsmedizinisch-sicherheitstechnischen Dienst vertreten.

Zu den bekanntesten Ehrenbürgern der Stadt gehören der emeritierte Papst Benedikt XVI., früherer Professor der Universität Regensburg, der frühere bayerische Ministerpräsident Franz Josef Strauß und der in Regensburg geborene frühere Ministerpräsident Alfons Goppel.

Regensburg ist ferner die Geburtsstadt einiger berühmter Personen ihrer Zeit. Der in Regensburg geborene Don Juan de Austria, ein unehelicher Sohn von Kaiser Karl V. und der Regensburger Bürgertochter Barbara Blomberg, war Führer der Flotte der heiligen Allianz in der siegreichen Seeschlacht über die Osmanen bei Lepanto (1571).

Der bedeutende Gelehrte und später heiliggesprochene Albertus Magnus war von 1260 bis 1262 Bischof von Regensburg. Vor Ort wirkte der am 9. Januar 1534 in Regensburg verstorbene berühmte Chronist Johannes Aventinus. Am 15. November 1630 starb in Regensburg Johannes Kepler, Mathematiker und Astronom. Zwischen 1945 und 1950 lebte in Regensburg der sudetendeutsche Industrielle Oskar Schindler, der während des Zweiten Weltkrieges über 1.200 Juden vor der drohenden Vernichtung in den nationalsozialistischen Konzentrationslagern rettete. Joseph Ratzinger nahm 1969 den Ruf an die Universität Regensburg an. Dort lehrte er Dogmatik und Dogmengeschichte und gründete zusammen mit Alma von Stockhausen die Gustav-Siewerth-Akademie. Im Jahr 1976 wurde er Vizepräsident der Universität, ehe er 1977 zum Erzbischof ernannt wurde. Auch nach seiner Wahl zum Papst blieb er weiterhin Honorarprofessor in Regensburg.

Der Hauptgürtelasteroid (927) Ratisbona wurde nach dem lateinischen Namen der Stadt benannt.





</doc>
<doc id="12051" url="https://de.wikipedia.org/wiki?curid=12051" title="Donauradweg">
Donauradweg

Der Donauradweg ist ein Radfernweg, der auf einer Strecke von etwa 2850 km von der Quelle der Donau bis zu deren Mündung ins Schwarze Meer führt. Er führt dabei durch die Länder Deutschland, Österreich, Slowakei, Ungarn, Kroatien, Serbien, Bulgarien und Rumänien. 

Der Donauradweg verläuft größtenteils beiderseits der Donau, teilweise jedoch nur auf einer Seite, so dass während der Reise die Donau mehrfach überquert werden muss. Er folgt dabei einer antiken Römerstraße entlang des Donaulimes, der die Stationen, Kastelle und Festungen bis zum Donaudelta verband. Diese war als Donauweg bzw. Via Istrum bekannt.

Laut der ADFC-Radreiseanalyse 2013 zählt der Donauradweg zu den zehn beliebtesten und meistbefahrenen deutschen Radfernwegen. Zusätzlich wird dieser als der beliebteste ausländische Radfernweg bewertet.

Die Donau durchfließt Landschaften von großer Vielfalt und mehrere Gebirge. Zusätzlichen Reiz erhält der Radweg durch viele Burgen, Klöster, kunstvolle Bauwerke, geologische Besonderheiten und bekannte Städte. Letztere unterscheiden sich charakterlich sehr voneinander: die einstigen Kaiserstädte Wien und Budapest sowie die Bischofsstadt Passau beispielsweise, die hinsichtlich ihrer historischen Bedeutung im Gegensatz zur modernen Kulturstadt Linz stehen. Auch einige Naturschutzgebiete sind Bestandteil der Route. Wo der Strom die großen Ebenen durchquert, verläuft der Radweg oft auf Hochwasserdämmen mit guter Aussicht – beginnend in Niederbayern, in Oberösterreich, im Marchfeld und schließlich in Ungarn.

Der Weg ist Teil der EuroVelo-Route "EV6" ("Flussroute", vom Atlantik bis zum Schwarzen Meer). Der deutsche Teil des Donauradwegs entspricht der D-Route "D6" "Donauroute", der oberösterreichische Teil ist mit "R1" "Donauweg", der niederösterreichische mit "R6" "Donauradweg" ausgeschildert. Während der Abschnitt im Oberlauf von der Quelle bis Budapest sowie im serbischen Abschnitt durchgängig beschildert ist, fehlen entsprechende Markierungen besonders im bulgarischen und rumänischen Abschnitt. Hier verläuft der Radweg zumeist auf mehr oder weniger ruhigen Landstraßen und der Radwanderer ist auf entsprechendes Kartenmaterial angewiesen.

Die "Donauroute D6" beginnt schon vor der Donauquelle und zwar im Dreiländereck bei Basel, ab Donaueschingen ist es der eigentliche "Donauradweg", ab Tuttlingen flussabwärts auch die "Flussroute EV6".


Von der Donauquelle in Donaueschingen führt der ca. 600 km lange deutsche Donauradweg durch die Baar nach Tuttlingen und weiter durch den Naturpark Obere Donau. Der Radweg folgt dabei über weite Strecken der Donautalbahn (ausgenommen im Bereich der fünf Bahntunnel), die zwischen Donaueschingen und Ulm verläuft. Dies bietet die Möglichkeiten zu kombinierten Touren aus Bahn und Fahrrad.

Der Abschnitt im "Oberen Donautal" (zwischen den Donaustädtchen Mühlheim an der Donau und Scheer), dem eigentlichen Kern des "Naturparks Obere Donau", wird gesäumt von einer großen Zahl von mächtigen Kalksteinfelsen sowie zahlreichen Burgen, Höhlen, Schlössern, Ruinen, Klöstern und Barockkirchen, wie zum Beispiel:


Hinter Scheer verlässt die Donau die Schwäbische Alb und der Donauradweg verläuft durch die breiten Ebenen des oberschwäbischen Donautals. Über Riedlingen, Obermarchtal und Ehingen erreicht der Radwanderer Ulm (km 194,5) und verlässt dann das Bundesland Baden-Württemberg in Richtung Bayern. 

Eine beliebte und auch ausgeschilderte Variante ab Ehingen ist die Wegstrecke durch das Blautal zum Blautopf mit den Orten Schelklingen, Blaubeuren und Blaustein, die in Ulm wieder auf den normalen Donauradweg zurückführt. Das Tal der Blau wurde von der Urdonau geformt. Als sich die Schwäbische Alb im Zuge der Auffaltung der Alpen hob, grub sich die Urdonau zunächst immer tiefer ins Gestein, bis sie sich schließlich doch ein Bett weiter südlich suchte.

Die landschaftlich ansprechendere Variante durch das Blautal berührt das Biosphärengebiet Schwäbische Alb, das 2009 von der UNESCO eingerichtet wurde. Die Wegführung folgt einer Schleife nach Norden, was 43,5 Streckenkilometer zwischen Ehingen und Ulm ergibt; der direktere Normalweg über Erbach ist dagegen zehn Kilometer kürzer.

Der Donauradweg zieht im Bundesland Bayern erst einmal durch die Landschaft, die landläufig Bayerisch-Schwaben genannt wird. Das große Naturschutzgebiet Schwäbisches Donaumoos flankiert über eine weite Strecke den Weg. Über Elchingen, Günzburg, Dillingen, Höchstädt führt der Weg nach Donauwörth. Die Region Oberbayern ist in Ingolstadt erreicht (km 352). Mit dem Kloster Weltenburg, Kelheim und Regensburg (km 443) kommt der Radfahrer in Niederbayern und der Oberpfalz an. Nach Wörth an der Donau, Straubing, Plattling und Vilshofen an der Donau geht es nach Passau (km 595,5). Unterhalb von Passau überquert die Donau – und damit auch der Donauradweg – die Grenze zu Österreich. Unterhalb von Obernzell bietet eine begehbare Staustufe die Möglichkeit, den Grenzübertritt mit der Donauquerung zu verbinden.

Es bestehen beim Donauradweg einige Anschluss- und Abzweigmöglichkeiten zum weiteren süddeutschen Radwegenetz:

Bereits in Donaueschingen führt der Heidelberg-Schwarzwald-Bodensee-Radweg sowohl vom Bodensee, als auch vom Schwarzwald zum Donauradweg heran. In Tuttlingen erreicht der Hohenzollern-Radweg den Donauradweg und führt nördlich an den Neckar, südlich an den Überlinger See weiter. In Sigmaringen kreuzt der Schwäbische-Alb-Radweg und führt entweder südlich ebenfalls zum Bodensee oder nordwestlich über die Schwäbische Alb nach Nördlingen.

Fast sternförmig zweigen gleich vier lange Radwege auf der Höhe von Ulm und Neu-Ulm ab:


Bei Gundremmingen startet der Mindeltal-Radweg als Flussroute nach Süden entlang der Mindel.

Der Abschnitt des Radfernweges zwischen Bad Gögging und Passau (216 km) in Bayern erinnert an die römische Zeit, stößt vor Regensburg auf den Obergermanisch-Raetischen Limes und nennt sich lateinisch "Via Danubia". Er ist als solcher ausgeschildert.

Die Entstehung des Radtourismus entlang der Donau kann in der ersten Hälfte der 1980er Jahre festgemacht werden. Ende der 1970er Jahre gingen klassische Erholungsurlaube zurück, Touristen reisten vermehrt motorisiert an und das oberösterreichische Donautal kann beispielhaft für diese touristische Wendezeit herangezogen werden.

In den 1980er Jahren wurde der Urlaub mit dem Fahrrad immer beliebter. Wege, sogenannte Treppel- und Treidelpfade, die einst für Fracht-Schiffszüge genutzt wurden, wurden ausgebaut, Beschilderungsmaßnahmen durchgeführt und Streckenabschnitte erweitert. Somit begann die Ära des Radtourismus an der Donau. Vater des Donauradweges ist der Journalist Paul Pollack. Er publizierte in den frühen 80er Jahren in der Tageszeitung Kurier eine Artikelserie, in der er den Radfernwanderweg Passau–Wien bereits unter dem Begriff „Donauradweg“ propagierte. Gegen den Widerstand der Behörde (damals Bundesstrombauamt) setzte er durch, dass das bis dahin geltende Fahrverbot auf den Treppelwegen durch eine Novelle zum Wasserstraßengesetz aufgehoben wurde. Pollack hat in der Folge mehrere Führer über den Donauradweg verfasst.

Im Jahre 1984 publizierte der damalige Geschäftsführer der Tourismusregion Mühlviertel, Manfred Traunmüller, den ersten Radwege-Führer zum Donauradweg. Zu dieser Zeit wurden auch die ersten buchbaren Radtouren organisiert.

1991 wurde die Werbegemeinschaft Donau Oberösterreich gegründet, um die regionale Entwicklung voranzutreiben und gemeinsame Marketingmaßnahmen zu entwickeln. 42 Gemeinden sind heute Vereinsmitglieder. Unter diesen befinden sich unter anderem auch Linz, Passau sowie St. Nikola an der Donau.

2010 wurden an 17 Stellen des Donauradweges Radzählungen in Kombination mit einer weiteren Gästebefragung durchgeführt. Die Ergebnisse zeigen auf, dass 437.000 Radfahrer den österreichischen Donauradweg nutzen, davon 33 Prozent Urlauber, 33 Prozent Tagesausflügler und 34 Prozent Alltagsradler. Österreichs jährliche Wirtschaftserfolge, die lt. Studie durch Radreisende initiiert werden, liegen bei 71,8 Millionen Euro.

Der Donauradweg wird ständig weiterentwickelt und optimiert. Im Jahr 2014 wurde der Abschnitt zwischen Wesenufer und Schlögen fertig ausgebaut, um ein noch dichteres Wegenetz zu gewährleisten.

Der Abschnitt von Passau nach Wien ist nach dem Bodensee-Radweg die zweit-meistbefahrene Radroute Europas. Jährlich sind etwa 630.000 Radler auf dem österreichischen Donauradweg unterwegs, von denen etwa 55.000 die gesamten 326 Kilometer zurücklegen. Die Radreiseanalysen des ADFC, der seit 2010 auch auf dem österreichischen Abschnitt Zählungen durchführt, zeigen eine deutliche Steigerung der Anzahl an Radfahrern im Vergleich zu den Vorjahren. Ansprechend ist die Route wegen ihrer vielfältigen Landschaft und Kultur und weil die (je nach Kondition) drei bis sieben Tagesetappen eine gute Infrastruktur aufweisen. Entlang der Wegstrecke finden sich die 2011 errichteten, kostenlosen E-Bike Ladestationen sowie zahlreiche fahrradfreundliche Betriebe. Der weitere Verlauf des Radwegs von Wien Richtung Ungarn (etwa 70 km) führt am Nationalpark Donauauen vorbei und ist etwas weniger stark befahren.

Von Passau () stromabwärts kann die Route rechts (südlich) oder links (nördlich) der Donau gewählt werden. Auf der Südroute empfiehlt sich ein Abstecher zum Schardenberg (600 m), dessen Aussichtswarte einen weiten Rundblick über das Alpenvorland bietet. Vor der Grenzstation bei Obernzell befindet sich die 800 Jahre alte Burg Krempelstein, der frühere Sitz der Rosenkreuzer. Es folgt die Burg Vichtenstein bei Kasten und man passiert das Donau-Kraftwerk Jochenstein, das 1952–56 gemeinsam von deutschen und österreichischen Kraftwerksbetreibern errichtet wurde. Nach der Raubritterburg Rannariedl und der Ortschaft Niederranna verläuft der Radweg vorbei am Ameisberg mit seinem Ferienpark, am Mühlviertler Donauland, durch die Schlögener Schlinge zum Eferdinger Becken. Das Stift Wilhering liegt vor der Flussenge am Kürnberger Wald.

Danach gelangt man zur oberösterreichischen Landeshauptstadt Linz (Kulturhauptstadt Europas 2009) mit ihren drei Donaubrücken. Entlang des neu gestalteten Uferbereichs reihen sich das Lentos Kunstmuseum, das vis-á-vis liegende Ars Electronica Center, das Brucknerhaus, die Tabakfabrik und die nahe Altstadt mit Flaniermeile und Dom aneinander. Dieser Linzer Stadtraum an der Donau zeichnet sich durch ein "Open-Air-Ensemble moderner Kunst" aus, das u. a. Kunstwerke von David Rabinowitch, Mathias Goeritz und Herbert Bayer umfasst. Achtung: Die Linzer Eisenbahnbrücke wird 2016 laufend abgerissen. Auf der Weiterfahrt bietet die Route am linksufrigen Hochwasserdamm einen guten Blick über das Linzer Donauknie, das Industriegelände der VÖEST und die Wallfahrtskirche auf dem Pöstlingberg. 

Der folgende Abschnitt lädt zu Besichtigungen von Mauthausen (KZ Mauthausen, Granitsteinbrüche), der Römerstadt Lauriacum und der Stadt Enns – hoch über der Mündung des gleichnamigen Flusses – ein. Nach dem fruchtbaren Machland mit dem Keltendorf Mitterkirchen folgt das Stift Ardagger. Die Stadt Grein besitzt gleich zwei touristisch sehr bedeutende Attraktionen: Das Stadttheater Grein (ältestes noch heute bespielbare Stadttheater Österreichs) und das herrschaftliche Anwesen von Schloss Greinburg mit seiner prachtvollen Innenausstattung sowie dem Oberösterreichischen Schifffahrtsmuseum. Dieser Teil der Strecke ist beidseitig entlang der Donau mit dem Rad befahrbar. Das südliche Donauufer ist auch als Mostviertler Donauradweg bekannt.

Dort beginnt auch der früher sehr gefährliche Strudengau mit dem Hauptort St. Nikola an der Donau, wo man früher eigene Lotsen auf die Schiffe holte. Heute gehört der Bereich zum Staubereich des nachfolgenden Kraftwerks und ist für die Schifffahrt problemlos befahrbar. Die Burgen Werfenstein und Freyenstein befinden sich am Beginn einer geraden Flussstrecke, wo von Norden das Waldviertler Yspertal mündet. Dieses ist auch als "Tal der Sonnenuhren bekannt". In der Flussschlinge bei Persenbeug und Ybbs, die man sich auch von oben (südliche Flussterrasse) ansehen sollte, wurde in den 1950er Jahren das größte Laufkraftwerk Österreichs, das Kraftwerk Ybbs-Persenbeug, errichtet.

Nach Pöchlarn (Bechelaren in der Nibelungensage) unweit der Ostarrichi-Gedenkstätte von 996 gelangt man zu dem von weitem sichtbaren Benediktiner-Stift Melk mit seinen beiden Türmen, der barocken Bibliothek und dem Stiftsgarten. Von Melk führen Radwege beiderseits der Donau in das Durchbruchstal der Wachau (Wein- und Obstbau, mittelalterliche Kirchen, Ruine Aggstein, Schiffermuseum, Dürnstein). Die Wachau endet bei der niederösterreichischen Stadt Krems, die mit einer Reihe von Ausstellungen, Museen und Befestigungswerken ausgestattet ist. Wein- und Naturfreunde sollten im Nachbarort Langenlois das Loisium besuchen und die imposante, aus Löss geformte Terrassen-Landschaft besteigen.

Nach dem Klosterberg von Stift Göttweig und den Burghügeln bei Hollenburg erreicht man vorbei an weiteren Kraftwerken das Flachland des Tullner Feldes, an dessen Beginn sich die sehr alte Siedlung Traismauer (das römische "Augustianis") befindet. Südlich der Donau liegen bäuerliche Straßendörfer, nördlich beginnen die Donauauen mit zahlreichen Altarmen. Der Radweg führt über das Gelände des nie in Betrieb gegangenen Kernkraftwerks Zwentendorf. Bei Tulln (zwei Donaubrücken, Altstadt) kommt der Wienerwald in Sicht, an dessen Fuß die Burg Greifenstein und das Babenberger-Stift Klosterneuburg liegen. Durch die Wiener Pforte am Steilhang des Leopoldsberges kommt man nach Wien und befährt die Donauinsel, eine als Hochwasserschutz errichtete langgestreckte Insel, die als Event- und Erholungsgebiet sowie Naturreservat dient.

Der Donauradweg durchquert das Wiener Becken und man kann bei Orth eines der vielen Marchfeldschlösser und das Zentrum des Nationalparks Donau-Auen besuchen. Auf der Flussterrasse am Südufer liegt die Römerstadt Carnuntum bei Petronell mit mehreren Ausgrabungsfeldern und dem Heidentor. Von weitem sieht man im Osten die Hainburger Berge näherkommen, wo die Karpaten beginnen, und gelangt in die Stadt Hainburg mit dem Wiener- und dem Ungarntor. Am Stadtrand führt die Route am Schloss- und Braunsberg (Kelten-Ausgrabungen) und der Thebner Kogel vorbei zur Ungarischen Pforte – dem Übergang in die Ungarische Tiefebene. Gleich nach der österreichisch-slowakischen Grenze, wo die March in die Donau mündet, liegt Bratislava, die Hauptstadt der Slowakei.

Von Bratislava geht es auf slowakischer Seite durch die Kleine Ungarische Tiefebene über die Dämme der aufgestauten Donau bis zum Wasserkraftwerk Gabčíkovo und von dort zum Teil auf Schotterwegen bis Komárno. Der alternative Weg, auf ungarischer Seite bis Komárom, bewegt sich meist einige Kilometer abseits der Donau, bietet dafür jedoch die Möglichkeit, die Städte Mosonmagyaróvár und Győr zu besuchen. Von der Doppelstadt Komárom/Komárno geht es wahlweise auf slowakischer oder ungarischer Seite bis Esztergom, wobei der Weg auf ungarischer Seite über das ungarische Mittelgebirge führt. Am Donauknie bei Esztergom knickt der Weg ab Richtung Süden und führt über Szentendre bis Budapest.

Auf der Donauinsel Csepel führt der Weg aus Budapest. Bis nach Baja geht es meist abseits vom Straßenverkehr über Hochwasserdämme auf zum Teil unbefestigten Wegen. Von Baja aus gibt es zwei Alternativrouten. Die eine führt direkt zur serbischen Grenze, dann über Sombor nach Novi Sad. Die andere führt zunächst zu der kleinen Stadt Mohács, die durch die Schlachten von 1526 und von 1687 gegen das Osmanische Reich bekannt ist. Mit einer Fähre geht es dort auf die rechte Seite der Donau in Richtung kroatischer Grenze.

Die Route ist in Kroatien unvollständig als "Ruta Dunav" beschildert. Durch kleinere Dörfer führt der Weg zunächst in das überwiegend von Ungarn bewohnte Zmajevac. Von hier kann man entweder weiter der Landstraße nach Osijek folgen, oder kurz vor dem Ortseingang auf einen Schotterweg abbiegen, der durch den Naturpark Kopački rit führt und einige Kilometer vor Osijek wieder auf der Landstraße endet.
Von Osijek geht es über Landstraßen nach Vukovar. Hier erinnern noch viele Gebäude an die Zerstörung der Stadt während der Schlacht um Vukovar im Kroatienkrieg. Die letzte Stadt in Kroatien ist Ilok. Von dort gibt es zwei Grenzübergänge nach Serbien, von wo man entweder auf der rechten oder der linken Flussseite nach Novi Sad gelangt.

Im kroatischen Abschnitt begegnet man vielen Schildern am Wegrand, die vor Landminen warnen. Dort sollte man es unbedingt vermeiden, die Straßen oder Wege zu verlassen. Karten und weitere Hinweise bietet das kroatische Minenräumzentrum.

Unter Federführung der Deutschen Gesellschaft für Technische Zusammenarbeit (GTZ) wurde hier zwischen 2006 und 2009 ein integriertes Konzept zur Förderung des Fahrradtourismus in Serbien entwickelt, das durch umfangreiche Online- und Printdokumentationen ergänzt wird. Dadurch ist die Route in Serbien vollständig und zum Teil mit Alternativrouten ausgeschildert. Meist verläuft die Strecke auf ruhigen Nebenstraßen, die nicht immer asphaltiert sind. Lediglich in den Großstädten Novi Sad und Belgrad gibt es gesonderte Radwegführungen.

Die Route von Ungarn direkt nach Serbien führt über Sombor und Bogojevo nach Bačka Palanka. Von dort geht es auf der linken Seite der Donau nach Novi Sad, der Hauptstadt der Provinz Vojvodina. Entlang der Uferpromenade führt der Donauradweg am Stadtzentrum vorbei und dann über eine Brücke zur Festung Petrovaradin. Alternativ gibt es bei Ilok einen zweiten Grenzübergang nach Serbien, von wo aus man weiter auf der rechten Flussseite fahren kann. Zwischen der Donau und dem kleinen Mittelgebirge Fruška Gora, das bekannt ist als Naturschutzgebiet und über zahlreiche orthodoxe Klöster verfügt, geht es bei dieser Variante mit mehr Steigungen aber weniger Verkehr nach Novi Sad.

Von Novi Sad führt die Route zunächst auf der Hauptstraße in Richtung Süden, dann auf weniger befahrenen Landstraßen durch kleinere Dörfer. Ab Batajnica geht es wieder zurück auf die Hauptstraße. Diese führt in den Belgrader Stadtteil Zemun, wo man auf Radwegen entlang des Ufers weiterfahren kann. An der Mündung der Save angekommen, zeigt sich auf der anderen Seite Kalemegdan mit den Resten der Festung von Belgrad. Nicht weit flussaufwärts von der Mündung führt die Brücke Brankov most über die Save in die Altstadt von Belgrad. Auf dieser Seite gibt es einen Aufzug, der auch Fahrräder von der Brücke zu einem Radweg entlang des Ufers befördert.

Belgrad wird über die Pančevo-Brücke verlassen, von wo aus sich die Route bis Ram an der serbisch-rumänischen Grenze auf der linken Donauseite fortsetzt. Diese führt hier entweder über nicht asphaltierte Wege oder (ab Pančevo) auf wenig befahrenen Straßen. Bei Kovin ist ein Abstecher nach Smederevo ausgeschildert. Bei Ram wird mit einer Fähre wieder das rechte Flussufer erreicht. Ab hier bildet die Donau die Grenze zwischen Serbien und Rumänien. Ab Golubac geht es durch das Eiserne Tor, das imposante Durchbruchstal der Donau durch die südlichen Karpaten. Ab Veliko Gradište folgen nur noch vereinzelt kleinere Ortschaften. Hier gilt es einige Höhenunterschiede bis zu 200 Meter zu überwinden und einige Tunnel zu durchqueren. Wichtige Sehenswürdigkeiten in diesem Abschnitt sind die Statue des Decebalus auf rumänischer Seite, die Tabula Traiana, die allerdings nur vom Schiff aus zu sehen ist, sowie die archäologische Fundstätte Lepenski Vir. Auf der Ostseite des Gebirges kann man über den Damm des Kraftwerks Eisernes Tor I bei Drobeta Turnu Severin die Donau und damit die Grenze nach Rumänien überqueren. Auf serbischer Seite führt der Donauradweg weiter über Negotin nach Bulgarien.

Auf einer Länge von etwa 500 Kilometern bildet die Donau die gemeinsame Grenze zwischen Bulgarien und Rumänien. Hier ist die Führung des Donauradwegs auf beiden Seiten, das heißt in beiden Ländern, geplant bzw. im Zuge der Umsetzung. Im Sommer 2010 begann an der serbisch-bulgarischen Grenze bei Widin die Beschilderung des bulgarischen Teils der Route (). Von Widin aus soll sie weiter über Lom, Orjachowo, Swischtow, Russe bis Silistra verlaufen, von wo aus die Donau komplett auf rumänischem Territorium fließt. Die Markierungsarbeiten wurden 2012 abgeschlossen.

Rumänien hat mit einer Länge von über 1000 Kilometern den längsten Abschnitt eines einzelnen Landes entlang des Donauradwegs. Die Route führt entlang der Grenze zu Bulgarien durch die Gebiete der Kleinen und Großen Walachei nach Osten und dann durch die Dobrudscha zunächst in nördlicher Richtung und dann zum Donaudelta.

Vom Damm des Kraftwerks Eisernes Tor I kommend erreicht man nach einigen Kilometern die Stadt Drobeta Turnu Severin. Von hier aus führt die Route immer in der Nähe der Donau durch zahllose Dörfer und kleinere Städte wie Calafat, Corabia, Turnu Măgurele, Zimnicea und Giurgiu. Giurgiu ist durch die Brücke der Freundschaft mit der bulgarischen Stadt Russe verbunden. Dies war bis zur Eröffnung der zweiten Donau-Brücke "Neues Europa" im Spätsommer 2013 die einzige Brücke zwischen Bulgarien und Rumänien über die Donau. Weiter führt die Route über Oltenița nach Călărași. Von dort gelangt man mit einer Fähre auf die andere Seite der Donau und fährt an der bulgarischen Stadt Silistra vorbei. Die Route verläuft nun auf der rechten Seite der Donau nach Cernavodă, wo der Donau-Schwarzmeer-Kanal von der Donau abzweigt, und schließlich über Hârșova und Măcin nach Tulcea. Ab hier bildet die Donau ein ausgedehntes Delta, welches als bedeutendes Biosphärenreservat zum UNESCO-Welterbe gehört. Die offizielle Kilometerzählung der Donau beginnt bei Sulina. Diese Stadt ist nicht an das Straßennetz angeschlossen, kann aber von Tulcea aus per Fähre erreicht werden.





</doc>
<doc id="12053" url="https://de.wikipedia.org/wiki?curid=12053" title="Windgeschwindigkeit">
Windgeschwindigkeit

Die Windgeschwindigkeit ist die Geschwindigkeit der Luft gegenüber dem Boden. Sie ist eine gerichtete Größe, definiert als Vektor mit einer waagerechten und einer senkrechten Komponente. Dieser wird jedoch in der Praxis meist auf die horizontale Komponente beschränkt und durch bestimmte Geschwindigkeitsintervalle in eine Windstärke kategorisiert. Es wird dadurch möglich, den Geschwindigkeitsvektor anwendungsorientiert aufzubereiten, um die jeweils relevanten Elemente hiernach beispielsweise in Wetterkarten eintragen zu können. Für bestimmte Anwendungen wie die Luftfahrt ist jedoch auch die vertikale Komponente der Windgeschwindigkeit wichtig, beispielsweise um Aufwinde für Segelflugzeuge oder Gleitschirme abzuschätzen.

Die Windgeschwindigkeit kann mit einem Windsack oder phänomenologisch beispielsweise über die Beaufortskala abgeschätzt werden. Aufwinde lassen sich über Wolkenformationen abschätzen und auch für zahlreiche andere Spezialfälle sind solche mit wenig Aufwand verknüpfte Hinweise hilfreich.
Mit einem Windgewehr wird bei der Artillerie die ungefähre Windgeschwindigkeit in verschiedenen Höhen bestimmt.

Genauer gemessen wird die Windgeschwindigkeit üblicherweise mit einem kleinen rotierenden Windmessgerät, dem Schalenkreuzanemometer. Es existieren inzwischen jedoch auch genauere Ultraschallanemometer und SODAR-Systeme, die die Ausbreitung von Schallwellen zur Erfassung der Windgeschwindigkeit nutzen und auf diese Weise oft auch in der Lage sind, vertikale Profile zu messen. 

Die SODAR-Systeme messen dabei die Windgeschwindigkeiten vom Boden aus bis in Höhen von 200 m. Das Höhenprofil kann mit dieser Methode gut bestimmt werden. Die absolute Genauigkeit reicht in der Regel jedoch nicht aus, um damit etwa Energieertragsberechnungen für Windkraftanlagen durchzuführen. 

Sehr kleine Windgeschwindigkeiten können mit dem Hitzdrahtanemometer gemessen werden.

Durch die World Meteorological Organization wurde festgelegt, dass für die in Wetterkarten und Stationsmeldungen angegebene Windgeschwindigkeit der Mittelwert der jeweils letzten 10 Minuten angegeben wird. Die Spitzenböen eines solchen Intervalls können durchaus doppelt so stark und stärker sein, gleichwohl kann es auch Momente mit Windstille geben. Diese Messvorschrift gilt nur für offizielle beziehungsweise veröffentlichte Messwerte, aber nicht für Windvorhersagen, für die der jeweilige Publizist eigene politische und wirtschaftliche Regeln festlegen kann.

Die Windgeschwindigkeit wird in Kilometer pro Stunde oder Meter pro Sekunde (m/s), Knoten (kn) = Seemeilen/Stunde (sm/h) und in den USA auch oft in Meilen pro Stunde (mph) ausgedrückt. Die verschiedenen Einheiten lassen sich wie folgt umrechnen:

Bei den Windpfeilen auf Wetterkarten ist die Bedeutung der angegebenen "Fiedern" unterschiedlich. Ebenso uneinheitlich ist, ob der betreffende Ort an der Spitze, in der Mitte oder an den "Fiedern" liegt. Ursprünglich wurde für jede Windstärke nach Beaufort eine "Fieder" abwechselnd ans Ende des Pfeils gesetzt. Diese Darstellung ist in der Seefahrt hilfreich, da so auch niedrige Windgeschwindigkeiten (Flautengebiete) aufgelöst werden können. Diese Form wird von den weltweit verbreiteten "Pilot Charts" der US-amerikanischen NGA benutzt. Ein neueres System setzt die "Fieder" auf die Seite des Pfeils mit dem niedrigeren Luftdruck und den Bezugsort an die Spitze. Dabei steht jedes Dreieck für eine Windgeschwindigkeit von 50 kn, jeder ganze Strich für 10 kn und ein halber Strich für 5 kn, die Schrittweite der Auflösung ist also 5 kn. Die Summe aller Einzelgeschwindigkeiten beschreibt die Windgeschwindigkeit. Das System eignet sich auch für Geschwindigkeiten weit jenseits der Beaufortskale. Die Bedeutung der 5 kn-Schritte ist aber uneinheitlich. Sie stellen entweder das auf 5 kn gerundete Intervall dar, zeigen also die Intervallmitte an. Diese Interpretation wird bei den GRIB-Daten, wie sie von der NOAA kostenlos bezogen werden können, für die kostenlose digitale Seekarte OpenCPN oder den GRIB-Viewer ZyGrib verwendet. Andererseits können sie auch die Obergrenze des Intervalls darstellen (siehe Tabelle mit Umrechnung von Knoten in km/h).

Wenn Windgeschwindigkeiten nicht mit ihrem Messwert angegeben werden, werden sie in der Regel nach der Beaufortskala klassifiziert, was durch die Verwendung der Begriffe "Windstärke (z. B. 5)" oder "Beaufort (z. B. 5)" gekennzeichnet wird. Sie wurde ab 1806 von Sir Francis Beaufort entwickelt und von ihm als "Erster Hydrograph der Admiralität" ab 1830 in die Royal Navy eingeführt. Die Benennung der Skala zu seinen Ehren erfolgte jedoch erst Anfang des 20. Jahrhunderts. Durch die Abkürzungen "bft.", "bft", "Bft" oder "BFT", z. B. als "Bft 5" oder "5 Bft" erweckt die Klassifizierung den Eindruck einer Einheit. Es handelt sich ursprünglich um eine phänomenologische Skala, die entwickelt wurde, indem die Auswirkungen des Windes auf die Segelführung eines Kriegsschiffes und später auch auf den Seegang beschrieben wurden. Einen Vorläufer bildete die "Smeaton-Rouse-Skala" von John Smeaton und Rouse, die 1759 das Verhalten von Windmühlenflügeln beschrieben hatten. 1898 entwickelte die Deutsche Seewarte Hamburg eine auf Messwerten in Kilometer pro Stunde beruhende Skala und 1906 veröffentlichte George Simpson vom britischen Wetterdienst eine auf Meilen pro Stunde beruhende Skala, die dann nach dem Gründer des "britischen Wetterdienst"es Beaufort benannt wurde. Die Skala von 1906 wird zur Abgrenzung von der Beaufortskala seit 1926 auch Simpsonskala genannt, die von 1898 nach Wladimir Peter Köppen oder als Seewarteskale. Beide Skalen versuchten unabhängig voneinander der in der Seefahrt bereits eingeführten empirischen Klassifizierung nahezukommen.

Andere Klassifikationssysteme bilden die Fujita-Tornado-Skala für Tornados und Downbursts sowie die Saffir-Simpson-Skala für tropische Wirbelstürme.

Der Tagesgang der Windgeschwindigkeit, der im Sommer wesentlich ausgeprägter ist als im Winter, zeigt ein Minimum in den Nachtstunden und eine Auffrischung am Tag. Im Jahresgang, basierend auf entweder Tages- oder Monatsmitteln als langjährige Durchschnittswerte, zeigt ein Minimum im Sommer und zwei Maxima im Frühjahr und Winter.

Die folgende Aufzählung von Geschwindigkeitsrekorden muss nach Windarten unterschieden werden: 


Die höchste Windgeschwindigkeit einer Bö des Gradientwindes, die in Deutschland bislang gemessen wurde, lag bei 335 km/h. Sie wurde am 12. Juni 1985 auf der Zugspitze registriert.

Die höchste Windgeschwindigkeit in der Schweiz wurde mit 285 km/h auf dem Jungfraujoch in der Nacht vom 26. auf den 27. Februar 1990 während des Orkans Wiebke gemessen.

Die höchste je gemessene Geschwindigkeit einer Bö des Gradientwindes ist 408 km/h und wurde am 10. April 1996 während des tropischen Zyklons Olivia auf der westaustralischen Insel Barrow Island gemessen. Sie löste den zuvor gültigen Spitzenwert von 372 km/h (231 mph) ab, der am 12. April 1934 auf dem Mount Washington (New Hampshire) registriert worden war.

Höhere Windgeschwindigkeiten können nur noch bei Sonderfällen registriert werden. So wurde bei Bridge Creek, Oklahoma (USA) mittels eines Doppler-Radars am 3. Mai 1999 innerhalb eines Tornados des Oklahoma Tornado Outbreak eine Windgeschwindigkeit von 496 ± 33 km/h gemessen. Über Japan wurden 1970 zudem Jetstreams mit einer Geschwindigkeit von 650 km/h gemessen, dies jedoch nicht in Bodennähe, sondern in der freien Atmosphäre.

Noch höhere Windgeschwindigkeiten wurden auf dem Mars, Jupiter und insbesondere auf Neptun beobachtet.

Die Geschwindigkeitsverteilung des Winds lässt sich recht gut mit einer Weibullstatistik beschreiben.



</doc>
<doc id="12054" url="https://de.wikipedia.org/wiki?curid=12054" title="Vostell">
Vostell

Vostell ist der Familienname folgender Personen:



</doc>
<doc id="12057" url="https://de.wikipedia.org/wiki?curid=12057" title="Siedlung">
Siedlung

Eine Siedlung, auch "Ansiedlung," Ort oder "Ortschaft," ist ein geographischer Ort, an dem sich Menschen niedergelassen haben bzw. gesiedelt haben und in Gebäuden, Behelfsbauten oder temporären Bauwerken zum Zwecke des Wohnens und Arbeitens zusammen leben. Der Begriff Siedlung bezieht sich in der Regel auf sesshafte Lebensformen, d. h. auf dauerhaftes bzw. langfristiges Sich-Niederlassen und Wohnen an einem Ort bzw. in einer Region.

In diesem Fall spricht man auch davon, dass Menschen an dem Ort oder in der Region siedeln oder sich dort ansiedeln. Bei temporären oder behelfsmäßigen Unterkünften bzw. Schlafplätzen spricht man eher davon, dass ein Lager aufgeschlagen oder aufgebaut wird (z. B. Zeltlager, Feldlager, Ferienlager, Flüchtlingslager, Basislager bei Expeditionen, Protest-Camp).

Siedlungen können sehr unterschiedliches Ausmaß haben, vom Einsiedlerhof bis zum Ballungsraum mit mehreren Millionen Einwohnern. Zu einer funktionsfähigen Siedlung gehören, abhängig von der Größe, – in heutiger Zeit – in der Regel auch Baulichkeiten der Wirtschaft, der Kultur, des Sozial- und des Verkehrswesens. Siedlungen bzw. Orte haben meist einen eigenen Siedlungsnamen (Ortsname, Oikonym).

Abweichend davon werden auch Ortsteile oder Trabantenstädte bestehender Ortschaften als Siedlung bezeichnet, die als geplante Stadtteile angelegt wurden; häufig sind es Wohngebiete in offener Bauweise. Das betrifft insbesondere Großwohnsiedlungen, größere genossenschaftliche Wohnanlagen, Trabantenstädte, aber auch Siedlungen aus sogenannten Siedlungshäusern (Kleinsiedlungen). Bei letzteren wird der Begriff Siedlung teilweise einfach zum Namen des Ortsteils, insbesondere wenn der Kernort eher dörfliche Dimensionen hat (z. B. Trogen, Ortsteil Siedlung).

Informelle Siedlungen sind ohne Genehmigung und ohne Eigentum an Grund und Boden errichtet.

In der Archäologie dient der Begriff Siedlung als neutrale Bezeichnung für jeden Fundplatz mit Häusern, Hütten oder sonstigen Strukturen, die zu Wohnzwecken dienten. Die Aufenthaltsdauer kann dabei zwischen einigen Tagen und mehreren Jahrhunderten, wenn nicht Jahrtausenden liegen (Tell). Auch Einzelhäuser werden als Siedlung bezeichnet. Der Begriff Dorf setzt die Existenz gemeinsam genutzter Strukturen oder Einrichtungen voraus.

Siedlungen werden in verschiedene "Siedlungsformen" (bzw. synonym "Siedlungstypen") eingeteilt. Diese können in reiner Form auftreten, meistens jedoch vermischen sich mehrere Siedlungsformen, vor allem wenn Siedlungen wachsen.

Häufig bestehen in größeren Kommunen verschiedene Siedlungsform nebeneinander. Als in der unmittelbaren Nachkriegszeit in Deutschland 11 Millionen Heimatvertriebene aufgenommen werden mussten, wurden neue Baugebiete mit gleichförmigen Haustypen („Siedlungshaus“) errichtet, häufig entlang einer Straße in der Peripherie der Ortschaften. Die Einwohnerzahl vieler Ortschaften in ländlichen Gebieten verdoppelte sich im Verlauf des Siedlungsbaus der 1950er Jahre.

Auch für die in den 1990er und 2000er Jahren ankommenden Aussiedler und Spätaussiedler musste neuer Wohnraum geschaffen werden. Zudem gab es in den neuen Bundesländern vergleichsweise wenige Einfamilienhäuser. So entstanden nach der Wende wieder neue "Siedlungsgebiete", die jedoch individueller und großzügiger angelegt wurden, als nach dem Krieg.

Kleine Siedlungen werden ab einer gewissen Größe „Dörfer“ genannt, mittlere und große Siedlungen „Stadt“, riesige Siedlungen „Metropolen“. Diese grobe Unterscheidung stimmt jedoch in der Praxis nicht immer mit den Gegebenheiten überein. Dorf ist ein Begriff der ländlichen Siedlungsstruktur, und das Recht, die Bezeichnung "Stadt" zu führen, ist in Europa nicht unmittelbar von der Einwohnerzahl abhängig, sondern vom Stadtrecht (die kroatische Stadt Hum hat etwa 17 Einwohner). Städte mit mehr als 1 Million Einwohner werden als "Millionenstadt" bezeichnet, darüber hinausgehende Agglomerationen als "Megacity".

Um eine internationale amtlich-statistische Vergleichbarkeit zu erzielen, hat die Statistical Commission der Vereinten Nationen eine kleinste siedlungsgeographische Einheit definiert, die "Siedlungseinheit" (SE, ). Die Untergrenze passt sich der Siedlungsstruktur des Staates an.

In Deutschland unterscheidet man – sofern die Siedlung den Titel „Stadt“ trägt – nach Einwohnerzahl:

In der "topographischen Siedlungskennzeichnung" der Statistik Austria unterscheidet man:

Neu erstellte Siedlungen haben oft ganz spezielle Formen. Mit der Zeit verschwindet die charakteristische Form jedoch oft. Aber vor allem in dörflichen Siedlungsformen bleibt die Form oft lange Zeit erhalten.

Folgende Formen werden unterschieden:


Unabhängig von Größe und Anordnung wird die Siedlungsform auch durch die Bauart der Häuser selbst bestimmt.

Hier sind vor allem folgende Unterscheidungen zu nennen:


Auch die Bauherren­schaft kann zur Unterscheidung herangezogen werden:


Seltener wird bei den Siedlungsformen nach gesellschaftlichen Gesichtspunkten unterschieden. Es gibt Siedlungen, in denen besondere Regeln gelten oder in denen nur Menschen leben, auf die bestimmte Kriterien zutreffen.

Folgende Unterscheidungen wären hier zu nennen:


In Österreich gibt es insgesamt etwa 55.000 – in der Österreichischen Karte (ÖK)/Geonam geführte – Orte (Siedlungsnamen), von Städten bis hin zu Einzellagen. Dazu kommen noch zahlreiche Gehöfte mit Vulgonamen, die in diesem Kartenwerk nicht verzeichnet sind. Diese Orte sind zu insgesamt 17.368 Ortschaften (Postleitzahlgebieten) und 2.357 Gemeinden (politischen Einheiten) zusammengefasst.

Die Untergrenze der UNO-Siedlungeinheit (SE) wurde in Österreich bei 501 Einwohnern gewählt. Nach dieser internationalen Definition gibt es 1.629 Siedlungeinheiten. Sie umfassen gesamt 1.653.456 Gebäude (71,1 % aller Gebäude in Österreich), ihre Gesamteinwohnerzahl beträgt 6.682.076 (79,8 % der Gesamtbevölkerung), das heißt, ein Fünftel aller Österreicher lebt in Orten unter 500 Einwohnern. Die durchschnittliche Größe einer Siedlungseinheit beträgt 1.015 Gebäude mit 4.102 Einwohnern, liegt also im Bereich einer kleinen Kleinstadt. Andererseits sind ein signifikanter Teil der etwa 2.300 Gemeinde- und ein Gutteil der weiteren 15.000 Ortschaftshauptorte kleiner als 500 Einwohner.

Das Verhältnis der Siedlungsnamen, Ortschaften und Siedlungseinheiten gibt Aufschluss über die lokale Siedlungsstruktur: So hat Salzburg zwar 6.300 Orte, aber nur 114 Siedlungseinheiten, was für viele kleine Ansiedlungen spricht, Burgenland aber 850 Orte bei 129 Einheiten, also primär größere Orte. Die Steiermark hat die meisten Orte Österreichs, aber nur halb so viele Siedlungseinheiten wie Niederösterreich, ist also kleinörtlicher strukturiert. Niederösterreich hat dreimal so viele Einwohner wie Salzburg, aber nur doppelt so viele Ortsnamen, was an der in der ÖK unbenannten Streubesiedlung liegt, während Salzburg mehr geschlossene Orte aufweist. Die 150 Ortsnamen Wiens sind die eingewachsenen Vor- und Umlandorte, die heute die Bezirke, Bezirksteile und Grätzl bilden, trotzdem hat das Stadtgebiet so viel ländlichen Raum, dass sich im Osten Wiens drei weitere, heute noch unabhängige Siedlungseinheiten ergeben (Aspern, Neuessling, Süßenbrunn).

Die Bezeichnungen für Arten von Siedlungen in Russland, der Ukraine und Weißrussland hat einige bedeutende Unterschiede zu Klassifikationssystemen in anderen Staaten. Diese Länder haben grundlegend übereinstimmende Bezeichnungen für die verschiedenen Siedlungstypen.

Urbane Siedlungstypen sind laut einem Beschluss des Allrussischen Zentralem Exekutivkomitees und des Rates der Volkskommissare vom 15. September 1924 alle Siedlungen mit mindestens 1.000 Einwohnern unter der Bedingung, dass nur 25 % der Bevölkerung von der Landwirtschaft leben.

Zu den ruralen Siedlungstypen zählen:

Alle Siedlungen, die vor dem 15. September 1924 als Datschensiedlung (russisch , datschni posjolok), Arbeitersiedlung (russisch , rabotschi posjolok) oder Kursiedlung (russisch , kurortni posjolok) galten mussten zu einem der urbanen oder ruralen Typen eingeordnet werden. Sie können jedoch auch als Unterpunkte der Siedlung städtischen Typs gelten.

In manchen von Turkvölkern besiedelten Teilen Russlands und islamisch geprägten Nachfolgestaaten der Sowjetunion werden Dörfer als Aul (russisch ) bezeichnet.

In verschiedenen Epochen der Russischen Geschichte gehörten Teile Polens zu Russland. Aus dieser Zeit stammt im Russischen der Begriff vom polnischen für Siedlungen städtischen Typs und entsprechen damit ebenfalls einer Minderstadt. In seltenen Fällen werden mit diesem Begriff auch Siedlungen mit einer signifikanten jüdischen Bevölkerung bezeichnet.








</doc>
<doc id="12061" url="https://de.wikipedia.org/wiki?curid=12061" title="Depression">
Depression

Die Depression (von „niederdrücken“) ist eine psychische Störung. Typisch für sie sind gedrückte Stimmung, negative Gedankenschleifen und ein gehemmter Antrieb. Häufig gehen Freude und Lustempfinden, Selbstwertgefühl, Leistungsfähigkeit, Einfühlungsvermögen und das Interesse am Leben verloren. Diese Symptome treten auch bei gesunden Menschen zeitweise auf. Bei einer Depression sind sie jedoch länger vorhanden, schwerwiegender ausgeprägt und senken deutlich die Lebensqualität.

In der Psychiatrie wird die Depression den affektiven Störungen zugeordnet. Die Diagnose wird nach Symptomen und Verlauf (z. B. einmalige oder wiederholte depressive Störung) gestellt. Zur Behandlung werden nach Abklärung möglicher Ursachen Antidepressiva eingesetzt oder eine Psychotherapie durchgeführt, je nach Schweregrad einzeln oder auch kombiniert.

Im alltäglichen Sprachgebrauch wird der Begriff "depressiv" häufig für eine normale Verstimmung verwendet. Im medizinischen Sinne ist die Depression jedoch eine ernste, behandlungsbedürftige und oft folgenreiche Erkrankung, die sich der Beeinflussung durch Willenskraft oder Selbstdisziplin des Betroffenen entzieht. Sie stellt eine wesentliche Ursache für Arbeitsunfähigkeit oder Frühverrentung dar und ist an rund der Hälfte der jährlichen Selbsttötungen in Deutschland beteiligt.

In einer internationalen Vergleichsstudie von 2011 wurde die Häufigkeit in Ländern mit hohem Einkommen verglichen mit der in Ländern mit mittlerem und niedrigem Einkommen. Die Lebenszeitprävalenz betrug in der ersten Gruppe (zehn Länder) 14,9 % und in der zweiten Gruppe (acht Länder) 11,1 %.

Das Verhältnis von Frauen zu Männern war ungefähr 2:1.
Eine Metaanalyse von 26 Studien mit Daten von 60.000 Kindern der Jahrgänge 1965–1996 ergab für die Altersgruppe unter 13 eine Prävalenz von 2.8 % und für die Altersgruppe 13-18 eine von 5,6 % (Mädchen 5,9 %, Jungen 4,6 %).

Die Krankheitslast durch Depressionen, etwa in Form von Arbeitsunfähigkeiten, stationären Behandlungen und Frühverrentungen, ist in Deutschland in den letzten Jahren stark angestiegen.Es wird angenommen, dass sich die tatsächliche Krankheitshäufigkeit deutlich weniger gravierend verändert hat und das vermehrte Auftreten durch eine bessere Erkennung und weniger Stigmatisierung von Menschen mit psychischen Störungen herrührt. Auch die mit der Zeit niedrigschwelliger gewordenen Diagnose-Kriterien für eine psychische Störung werden als Teilursache kritisch diskutiert. Ergebnisse von Langzeitstudien auf der anderen Seite sprechen jedoch eher für einen echten Anstieg, der mit verschiedenen gesellschaftlichen Einflussfaktoren in Zusammenhang gebracht wird. Auch in Deutschland scheinen nach Krankenkassendaten jüngere Generationen gefährdeter zu sein, im Laufe ihres Lebens eine psychische Störung zu erleiden.

Im Jahre 2011 wurde von mehreren Fachgesellschaften wie der Deutschen Gesellschaft für Psychiatrie und Psychotherapie, Psychosomatik und Nervenheilkunde (DGPPN) eine Versorgungsleitlinie zum Thema Depression erarbeitet. Sie empfiehlt, zur Diagnose nach ICD-10 zwischen drei Haupt- und sieben Zusatzsymptomen zu unterscheiden.

Die Hauptsymptome sind:

Die Zusatzsymptome sind:
Ferner kann zusätzlich noch ein somatisches Syndrom vorliegen:
Depressive Erkrankungen gehen gelegentlich mit körperlichen Symptomen einher, sogenannten "Vitalstörungen", Schmerzen in ganz unterschiedlichen Körperregionen, am typischsten mit einem quälenden Druckgefühl auf der Brust. Während einer depressiven Episode ist die Infektionsanfälligkeit erhöht. Beobachtet wird auch sozialer Rückzug, das Denken ist verlangsamt (Denkhemmung), sinnloses Gedankenkreisen (Grübelzwang), Störungen des Zeitempfindens. Häufig bestehen Reizbarkeit und Ängstlichkeit. Hinzukommen kann eine Überempfindlichkeit gegenüber Geräuschen.

Der Schweregrad wird nach ICD-10 gemäß der Anzahl Symptome eingeteilt:

"Leichte Depression:" zwei Hauptsymptome und zwei Zusatzsymptome
"Mittelschwere Depression:" zwei Hauptsymptome und drei bis vier Zusatzsymptome
"Schwere Depression:" drei Hauptsymptome und fünf oder mehr Zusatzsymptome

Die Symptomatik einer Depression kann sich bei Frauen und Männern auf unterschiedliche Weise ausprägen. Bei den Kernsymptomen sind die Unterschiede gering. Während bei Frauen eher Phänomene wie Mutlosigkeit und Grübeln verstärkt zu beobachten sind, gibt es bei Männern deutliche Hinweise darauf, dass eine Depression sich auch in einer Tendenz zu aggressivem Verhalten niederschlagen kann. In einer Studie von 2014 wurden die unterschiedlichen Ausprägungen bei Frauen und Männern mit Unterschieden bei den biologischen Systemen der Stressreaktion in Verbindung gebracht.

Das Erkennen von Depressionssymptomen bei Vorschulkindern ist inzwischen relativ gut erforscht, erfordert jedoch die Beachtung einiger Besonderheiten. Entsprechendes gilt für Schulkinder und Jugendliche. Bei Kindern liegt die Prävalenz von Depression etwa bei drei Prozent, bei Jugendlichen bei etwa achtzehn Prozent. Die Symptome sind bei Kindern und Jugendlichen oft nur schwer zu erkennen, da sie von alterstypischen Verhaltensweisen überlagert werden. Dies erschwert die Diagnostik.

Für Kinder und Jugendliche gelten die gleichen Diagnoseschlüssel wie für Erwachsene. Allerdings können bei Kindern eine ausgesprochene Verleugnungstendenz und große Schamgefühle vorliegen. In einem solchen Fall kann Verhaltensbeobachtung und die Befragung der Eltern hilfreich sein. Auch die familiäre Belastung in Hinblick auf depressive Störungen sowie anderen Störungen sollte in den Blick genommen werden. Im Zusammenhang mit Depression wird oft eine Anamnese des Familiensystems nach Beziehungs- und Bindungsstörungen sowie frühkindlichen Deprivationen oder auch seelischen, körperlichen und sexuellen Misshandlungen erstellt.

Zu den weiteren diagnostischen Schritten kann auch eine Befragung der Schule oder des Kindergartens hinsichtlich der Befindlichkeit des Kindes oder Jugendlichen zählen. Häufig wird auch eine orientierende Intelligenzdiagnostik durchgeführt, welche eine eventuelle Über- oder Unterforderung aufdecken soll. Spezifische Testverfahren für Depression im Kindes- und Jugendalter sind das Depressions-Inventar für Kinder und Jugendliche (DIKJ) und der Depressions-Test für Kinder (DTK).

Bei Jugendlichen sind Aufsässigkeit, Trotzverhalten, Aggressivität, negatives Körperbild und Schulunlust oft alterstypisches pubertäres Verhalten, daher können depressive Symptome leicht übersehen werden. Jüngere Kinder hingegen können oft noch gar nicht artikulieren, was sie bedrückt. Häufig finden Übertragungen in den somatischen Bereich statt. Sie klagen vermehrt über Bauch- oder Kopfweh. Außerdem ist bei Kindern oft eine übersteigerte Angst zu beobachten: Beispielsweise Angst vor Trennung der Eltern, Angst vor dem Tod eines Familienmitglieds, Angst allein gelassen zu werden, vergessen zu werden, nicht abgeholt zu werden; aber auch Angst vor Dunkelheit; Tieren, Monstern, Angst vor Strafe, Angst etwas falsch zu machen etc.

Da die Depression eine sehr häufige Störung ist, sollte sie bereits vom Hausarzt erkannt werden, was aber nur in etwa der Hälfte aller Fälle gelingt. Manchmal wird die Diagnose erst von einem Psychiater, von einem Arzt für Psychosomatische Medizin und Psychotherapie oder von einem psychologischen Psychotherapeuten gestellt. Wegen der besonderen Schwierigkeiten der Diagnostik und Behandlung von Depressionen im Kindesalter sollten Kinder und Jugendliche mit einem Verdacht auf eine Depression grundsätzlich einem Kinder- und Jugendlichenpsychiater oder Kinder- und Jugendlichenpsychotherapeuten vorgestellt werden.

Verbreitete Verfahren zur Einschätzung des Schweregrades einer depressiven Episode sind die Hamilton-Depressionsskala (HAMD), ein Fremdbeurteilungsverfahren, das Beck-Depressions-Inventar (BDI), ein Selbstbeurteilungsverfahren, und das Inventar depressiver Symptome (IDS), welches in einer Fremd- und einer Selbstbeurteilungsversion vorliegt.

Mitunter wird eine Depression von einer anderen Erkrankung überdeckt und nicht erkannt. 

In der ICD-10 fallen Depressionen unter den Schlüssel "F32.–-" und werden als „depressive Episode“ bezeichnet. Im Falle sich wiederholender Depressionen werden diese unter "F33.–" klassifiziert, bei Wechsel zwischen manischen und depressiven Phasen unter "F31.–". Die ICD-10 benennt drei typische Symptome der Depression: depressive Stimmung, Verlust von Interesse und Freude sowie eine erhöhte Ermüdbarkeit. Entsprechend dem Verlauf unterscheidet man im gegenwärtig verwendeten Klassifikationssystem ICD-10 die "depressive Episode" und die "wiederholte (rezidivierende) depressive Störung".

Laut S3-Leitlinie für unipolare Depression werden als Screening folgende Fragebögen empfohlen:
Folgende Fragebögen werden in der Leitlinie zur Verlaufsdiagnostik empfohlen, also um zu ermitteln, inwiefern die Therapie anspricht und die Symptomatik sich verbessert:

Fragebögen zur Selbstbeurteilung:
Fragebögen zur Fremdbeurteilung:

Durch eine Differentialdiagnose wird versucht, eine mögliche Verwechselung mit einer der folgenden Krankheiten oder Störungen auszuschließen:

Gegenwärtig ist das Diagnose-Schema nach ICD-10 in der medizinischen Praxis verbindlich. Die Schwere der Depression wird dort durch die Begriffe leichte, mittelgradige und schwere depressive Episode unterschieden, bei letzterer noch mit dem Zusatz mit oder ohne psychotische Symptome (siehe auch: Diagnose).

Nach dem ICD-10 Diagnose-Schema wird die chronische Depression nach Schwere und Dauer eingestuft in Dysthymie oder rezidivierende (wiederholte) Depression. Hier ist die DSM-5 genauer, da zu bestehenden chronischen depressiven Verstimmungen noch phasenweise zusätzliche Depressionen hinzukommen können. Innerhalb der DSM-5 wird dies dann „double depression“ genannt. Dort wurde jedoch auch der Ausschluss von Trauerreaktionen als Diagnosekriterium aufgehoben.

Organische Depression (ICD-10 F06.3 – „Organische affektive Störungen“) nennt man ein depressives Syndrom, das durch eine körperliche Erkrankung hervorgerufen wird, beispielsweise durch Schilddrüsenfunktionsstörungen, Hypophysen- oder Nebennierenerkrankungen, Schlaganfall oder Frontalhirnsyndrom. Nicht zur organischen Depression zählten hingegen Depressionen im Gefolge von hormonellen Umstellungen, z. B. nach der Schwangerschaft oder in der Pubertät. „Eine depressive Episode muss … von einer "organischen depressiven Störung" unterschieden werden. Diese Diagnose ist (vorrangig) zu stellen, wenn die Störung des Affekts sehr wahrscheinlich als direkte körperliche Folge eines spezifischen Krankheitsfaktors (z. B. Multiple Sklerose, Schlaganfall, Hypothyreose) angesehen wird.“ Dies gibt dem weiterbehandelnden Arzt Hinweise, dass eine somatische Erkrankung als Ursache der Depression zugrunde liegt und bei der Diagnostik und Behandlung zu berücksichtigen ist (und nicht die Depression die Ursache funktioneller oder psychosomatischer Beschwerden ist).

Die reaktive Depression wird als Reaktion auf ein aktuell belastendes Ereignis verstanden und heute als mögliches Symptom einer Anpassungsstörung (ICD-10: F43.2) diagnostiziert.

Der Begriff endogene Depression umfasst ein depressives Syndrom ohne erkennbare äußere Ursache, das meist auf veränderte Stoffwechselvorgänge im Gehirn und genetische Veranlagungen zurückgeführt wurde (endogen bedeutet innen entstanden). Heute wird sie im klinischen Alltag als eine Form der affektiven Psychose bezeichnet.

Die neurotische Depression oder Erschöpfungsdepression soll durch länger andauernde belastende Erfahrungen in der Lebensgeschichte verursacht sein.

Als Sonderform der Depression wurde die anaklitische Depression (Anaklise = Abhängigkeit von einer anderen Person) bei Babys und Kindern angesehen, wenn diese allein gelassen oder vernachlässigt wurden. Die anaklitische Depression äußere sich durch Weinen, Jammern, anhaltendes Schreien und Anklammern und könne in psychischen Hospitalismus übergehen.

Die somatisierte (≠ somatische) Depression (auch maskierte bzw. larvierte Depression genannt) ist eine Depression, bei der körperliche Beschwerden das Krankheitsbild prägten. Die depressive Symptomatik bleibt unterschwellig. Beschwerdeschilderungen in Form von Rückenschmerzen, Kopfschmerzen, Beklemmungen in der Brustregion, Schwindelempfindungen und vieles mehr sind beschrieben. Die unterschiedlichsten körperlichen Empfindungen fungierten als „Präsentiersymptome“ einer Depression. Die Häufigkeit der gestellten Diagnose „maskierte Depression“ betrug in der Hausarztpraxis bis zu 14 % (jeder siebte Patient). Das Konzept, das in den 1970er bis 1990er Jahren große Verbreitung fand, wurde inzwischen aufgegeben, wird aber von einigen Ärzten, entgegen der Empfehlung, noch heute verwendet.

Die zur depressiven Symptomatik gehörende "innere Unruhe" führte zu Erscheinungsformen, die unter agitierter Depression subsumiert wurde. Dabei werde der Patient von einem rastlosen Bewegungsdrang, der ins Leere lief, getrieben, wobei zielgerichtete Tätigkeiten nicht möglich seien. Der Kranke gehe umher, könne nicht still sitzen und auch Arme und Hände nicht still halten, was häufig mit Händeringen und Nesteln einher gehe. Auch das Mitteilungsbedürfnis sei gesteigert und führe zu ständigem, einförmigen Jammern und Klagen. Die "agitierte Depression" wurde bei älteren Menschen vergleichsweise häufiger beobachtet auf als in jüngerem und mittlerem Alter.

Etwa 15–40 % aller depressiven Störungen wurden als „atypische Depressionen“ bezeichnet. „Atypisch“ bezog sich auf die Abgrenzung zur endogenen Depression und nicht auf die Häufigkeit dieses Erscheinungsbildes einer Depression. In einer deutschen Studie aus dem Jahr 2009 betrug der Anteil atypischer Depressionen 15,3 %. Bei Patienten mit atypischer Depression wurde im Vergleich zu den anderen depressiven Patienten ein höheres Risiko ausgemacht, auch an somatischen Angstsymptomen, somatischen Symptomen, Schuldgedanken, Libidostörungen, Depersonalisation und Misstrauen zu leiden.

Als Spät-/Involutionsdepression galt eine Depression, die erstmals nach dem 45. Lebensjahr auftrat und deren Prodromalphase deutlich länger war als bei den Depressionen mit früherem Beginn. Frauen seien von der Spätdepression häufiger betroffen (gewesen) als Männer. Sie grenze sich u. a. von früher auftretenden Depressionen durch ihre längere Phasendauer, mehr paranoide und hypochondrische Denkinhalte, eine relative Therapieresistenz sowie eine erhöhte Suizidgefahr ab.

Hiervon zu unterscheiden sei die Altersdepression, die nach dem 60. Lebensjahr erstmals auftrete. Die Bezeichnung Altersdepression allerdings sei irreführend, da sich eine depressive Episode im Alter nicht von der in jungen Jahren unterscheide, jedoch bei Älteren häufiger Depressionen als bei Jüngeren auftreten.

Die Ursachen depressiver Störungen sind komplex und nur teilweise verstanden. Es existieren sowohl anlagebedingte als auch erworbene Anfälligkeiten (Prädispositionen) zur Ausbildung einer Depression. Erworbene Anfälligkeiten können durch biologische Faktoren und durch lebensgeschichtliche soziale Belastungen ausgelöst werden.

Eine Studie von 2015 auf der Grundlage von Familiendaten von 20.198 Personen in Schottland ergab eine Erblichkeit von 28 bis 44 %, wobei die gemeinsamen Umwelteinflüsse einer Familie nur einen kleinen Einflussfaktor von sieben Prozent bildeten. Zwillingsstudien zeigten, dass die genetische Komponente nur ein Faktor ist. Selbst bei identischer genetischer Ausstattung (eineiige Zwillinge) erkrankt der Zwillingspartner des depressiven Patienten in weniger als der Hälfte der Fälle. Inzwischen konnten auch bei der nachträglichen (epigenetischen) Veränderung der genetischen Information Unterschiede zwischen betroffenen und nicht betroffenen Zwillingspartnern festgestellt werden, also Einflüsse der Lebensgeschichte auf die Steuerung der Erbinformation. Ferner besteht zwischen genetischen Faktoren und Umweltfaktoren eine Gen-Umwelt-Interaktion, engl. gene–environment interaction (GxE). So können genetische Faktoren z. B. bedingen, dass ein bestimmter Mensch durch eine große Risikobereitschaft sich häufig in schwierige Lebenssituationen manövriert. Umgekehrt kann es von genetischen Faktoren abhängen, ob ein Mensch eine psychosoziale Belastung bewältigt oder depressiv erkrankt.

Ein wesentlicher genetischer Vulnerabilitätsfaktor für das Auftreten einer Depression wird in einer Variation in der Promotorregion des Serotonin-Transportergens 5-HTTLPR vermutet. 5-HTTLPR steht dabei für "Serotonin (5-HT) Transporter (T) Length (L) Polymorphic (P) Region (R)". Das Gen befindet sich auf dem Chromosom 17q11.1–q12. Es kommt in der Bevölkerung in unterschiedlichen Formen vor (sogenannter „unterschiedlicher Längenpolymorphismus“ mit einem sogenannten „kurzen“ und einem „langen Allel“). Träger des kurzen Allels reagieren empfindsamer auf psychosoziale Stressbelastungen und sollen damit ein bis zu doppelt so großes Risiko (Disposition) an einer Depression zu erkranken haben wie die Träger des langen Allels. In einer Meta-Analyse von 2011 wurden 54 Einzelstudien zu dieser speziellen Frage untersucht und die Hypothese eines Zusammenhangs zwischen kurzem Allel und Entwicklung von Depression nach Stress bestätigt. In einer Meta-Analyse von 2014 wurden im Zusammenhang mit Depression für insgesamt sieben Kandidatengene signifikante Daten festgestellt: 5HTTP/SLC6A4, APOE, DRD4, GNB3, HTR1A, MTHFR, und SLC6A3. Bestimmte Abweichungen, die für die Entstehung von Depression ausschlaggebend sind, konnten jedoch bislang (Stand Dezember 2015) trotz extrem umfangreicher Suche noch nicht gefunden werden.

Als gesichert gilt, dass gestörte Prozesse der Signalübertragung – insbesondere derjenigen mit Beteiligung der monoaminergen Neurotransmitter Serotonin, Dopamin und Noradrenalin – bedeutende Faktoren darstellen. Allerdings sind noch weitere Regelungsprozesse beteiligt, und ihre gegenseitige Beeinflussung ist hochkomplex. Die Folge davon ist, dass etwa ein Drittel der Patienten nicht oder nur unzureichend auf Medikamente anspricht, die monoaminerge Systeme beeinflussen.

Die sogenannte Winterdepression wird als eine unzureichende Anpassung an Jahresrhythmen und an die jahreszeitlichen Veränderungen des Tagesrhythmus aufgefasst. Daran beteiligt sind mehrere Faktoren, unter anderem die jahreszeitlichen Schwankungen bei der Bildung von Vitamin D durch Sonnenlicht.

Auch chronische Infektionen mit Krankheitserregern wie Streptokokken (früher auch das Virus der Bornaschen Krankheit) stehen in Verdacht, Depressionen auslösen zu können. Die depressiven Syndrome bei schweren Infektionen oder anderen schweren Erkrankungen können nach heutigem Kenntnisstand durch Entzündungsprozesse und die dabei wirksamen Zytokine vermittelt und als „""“ bezeichnet werden.

Depressive Syndrome können durch die Einnahme oder das Absetzen von Medikamenten oder psychotropen Substanzen verursacht werden. Die Unterscheidung zwischen einer substanzinduzierten Depression und einer von Medikamenteneinnahme unabhängigen Depression kann schwierig sein. Grundlage der Unterscheidung ist eine durch einen Psychiater erhobene, ausführliche Krankengeschichte.

"Medikamente", die am häufigsten depressive Symptome verursachen können, sind Antikonvulsiva, Benzodiazepine (vor allem nach Entzug), Zytostatika, Glucocorticoide, Interferone, Antibiotika, Statine, Neuroleptika, Retinoide, Sexualhormone und Betablocker. Als Medikamente mit potentiell depressionsauslösender Wirkung wurden z. B. Diazepam, Cimetidin, Amphotericin B und Barbiturate identifiziert.

Seit den 1980er Jahren hat die Verwendung von Anabolika im Kraftsport deutlich zugenommen. Da dies als Doping gilt, ist die Bereitschaft von Sportlern gering, sich beim Absetzen einem Arzt anzuvertrauen. Das Absetzen der Anabolika führt jedoch zu Entzugserscheinungen, die ähnlich sind wie bei anderem Drogenentzug. Bei geringerer Dosierung ist der Abfall des körpereigenen Steroidniveaus vergleichbar mit dem Rückgang, wie er bei älteren Menschen häufig anzutreffen ist. Nach einer Querschnittstudie von 2012 traten Depressionen bei Kraftsportlern mit einer Anabolika-Abhängigkeit doppelt so häufig auf wie bei Kraftsportlern mit Anabolika-Gebrauch aber ohne Abhängigkeit. Der Unterschied war hoch signifikant.

Das Auftreten eines depressiven Syndroms als Entzugserscheinungen nach "Drogenkonsum", ermöglicht es Forschern, diesen Effekt gezielt zu nutzen, um bei Tieren Depression und Möglichkeiten ihrer Behandlung zu untersuchen.

Ein Stimmungstief der Mutter nach einer Geburt („Baby-Blues“) wird in der Fachliteratur überwiegend auf hormonelle Ursachen zurückgeführt. Mit einer oft zitierten Häufigkeit von ungefähr 10 bis 15 Prozent ist diese sogenannte postnatale Depression weit verbreitet. Allerdings zeigte ein Vergleich von 143 Studien mit Daten aus 40 Staaten, dass die tatsächliche Häufigkeit im Bereich von 0 bis 60 % lag, was mit großen sozioökonomischen Unterschieden in Verbindung gebracht wurde. So war die Häufigkeit in Singapur, Malta, Malaysia, Österreich und Dänemark sehr gering, dagegen in Brasilien, Guyana, Costa Rica, Italien, Chile, Südafrika, Taiwan und Südkorea sehr hoch. Die Symptome können Niedergeschlagenheit, häufiges Weinen, Angstsymptome, Grübeln über die Zukunft, Antriebsminderung, Schlafstörungen, körperliche Symptome und lebensmüde Gedanken bis hin zur Suizidalität umfassen. Es werden verschiedene mögliche neuroendokrinologische Ursachen diskutiert.

Nach einer groß angelegten britischen Studie sind etwa zehn Prozent aller Frauen von Depressionen während der Schwangerschaft betroffen. Nach einer anderen Studie sind es in der 32. Schwangerschaftswoche 13,5 Prozent. Die Symptome können extrem unterschiedlich sein. Hauptsymptom ist eine herabgesetzte Stimmung, wobei dies nicht Trauer im engeren Sinn sein muss, sondern von den betroffenen Patienten auch oft mit Begriffen wie „innere Leere“, „Verzweiflung“ und „Gleichgültigkeit“ beschrieben wird. Psychosomatische körperliche Beschwerden sind häufig. Es dominieren negative Zukunftsaussichten und das Gefühl der Hoffnungslosigkeit. Das Selbstwertgefühl ist niedrig. Die depressive Symptomatik in der Schwangerschaft wird oft von schwangerschaftstypischen „Themen“ beeinflusst. Dies können etwa Befürchtungen in Bezug auf die Mutterrolle oder die Gesundheit des Kindes sein.

Nach Seligmans Depressionsmodell werden Depressionen durch Gefühle der Hilflosigkeit bedingt, die auf unkontrollierbare, aversive Ereignisse folgen. Entscheidend für die erlebte Kontrollierbarkeit von Ereignissen sind die Ursachen, auf die die Person ein Ereignis zurückführt.
Nach Seligman führt die Ursachenzuschreibung unangenehmer Ereignisse auf interne, globale und stabile Faktoren zu Gefühlen der Hilflosigkeit, die wiederum zu Depressionen führen. Mittels Seligmans Modell lässt sich die hohe Komorbidität zu Angststörungen erklären: Allen Angststörungen ist gemein, dass die Personen ihre Angst nicht oder sehr schlecht kontrollieren können, was zu Hilflosigkeits- und im Verlauf der Störung auch zu Hoffnungslosigkeitserfahrungen führt. Diese wiederum sind, laut Seligman, ursächlich für die Entstehung von Depressionen.

Im Zentrum von Aaron T. Becks kognitiver Theorie der Depression stehen kognitive Verzerrungen der Realität durch den Depressiven. Ursächlich dafür sind, laut Beck, negative kognitive Schemata oder Überzeugungen, die durch negative Lebenserfahrungen ausgelöst werden. Kognitive Schemata sind Muster, die sowohl Informationen beinhalten als auch zur Verarbeitung von Informationen benutzt werden und somit einen Einfluss auf Aufmerksamkeit, Enkodierung und Bewertung von Informationen haben. Durch Benutzung dysfunktionaler Schemata kommt es zu kognitiven Verzerrungen der Realität, die im Falle der depressiven Person zu pessimistischen Sichtweisen von sich selbst, der Welt und der Zukunft führen (negative Triade). Als typische kognitive Verzerrungen werden u. a. willkürliche Schlüsse, selektive Abstraktion, Übergeneralisierungen und Über- oder Untertreibungen angesehen. Die kognitiven Verzerrungen verstärken rückwirkend die Schemata, was zu einer Verfestigung der Schemata führt.
Unklar ist jedoch, ob kognitive Fehlinterpretationen, bedingt durch die Schemata, die Ursache der Depression darstellen oder ob durch die Depression kognitive Fehlinterpretationen erst entstehen.

Die Vertreter des Konzepts der emotionalen Intelligenz stehen Aaron T. Beck nahe, gehen aber darüber hinaus. Daniel Goleman sieht bei depressiven Teenagern zwei folgenreiche emotionale Defizite: Erstens zeigen diese, wie auch Beck beschreibt, eine Tendenz, Wahrnehmungen negativ, also depressionsverstärkend, zu interpretieren. Zweitens fehlt ihnen aber auch ein solides Können in der Handhabung zwischenmenschlicher Beziehungen (Eltern, Peergroup, Sexualpartner). Kinder, die depressive Neigungen haben, ziehen sich bereits in sehr jungem Alter zurück, weichen Sozialkontakten aus und verpassen dadurch soziales Lernen, das sie später nur noch schwer nachholen können. Goleman beruft sich u. a. auf eine Studie, die Psychologen der University of Oregon in den 1990er Jahren an einer High School in Oregon durchgeführt haben.

Nach dem Depressionsmodell von Lewinsohn, das auf der operanten Konditionierung der behavioristischen Lerntheorie beruht, entstehen Depressionen aufgrund einer zu geringen Rate an unmittelbar mit dem Verhalten verbundener Verstärkung. Nach Lewinsohn hängt die Menge positiver Verstärkung von der "Anzahl" verstärkender Ereignisse, von der "Menge" verfügbarer Verstärker und von den Verhaltensmöglichkeiten einer Person ab, sich so zu verhalten, dass Verstärkung möglich ist. Im weiteren Verlauf kann es zu einer "Depressionsspirale" kommen, wenn Betroffene sich aufgrund der Interessenlosigkeit sozial zurückziehen und der Verlust an Verstärkern wiederum zu einer weiteren Verschlechterung der Stimmung beiträgt. Dieser Entwicklung müsse dann durch Verhaltensänderungen im Sinne einer „Anti-Depressionsspirale“ entgegen gewirkt werden. Das entsprechende Konzept ist die Grundlage für die Verhaltensaktivierung in der Behandlung.

Anhaltende Stressbelastungen wie etwa Armut können Depressionen auslösen. Auch frühe Traumata können spätere Depression bedingen: Da bei Kindern vor dem zehnten Lebensjahr das Gehirn noch nicht reif genug ist, bilden sie bei einem Trauma nicht eine Posttraumatische Belastungsstörung aus, sondern zeigen in ihrem späteren Leben unter psychischen Belastungen eine schwere Depression.

Brown und Harris (1978) berichteten in ihrer als Klassiker geltenden Studie an Frauen aus sozialen Brennpunkten in London, dass Frauen ohne soziale Unterstützung ein besonders hohes Risiko für Depressionen aufweisen. Viele weitere Studien haben seitdem dieses Ergebnis gestützt. Menschen mit einem kleinen und wenig unterstützenden sozialen Netzwerk werden besonders häufig depressiv. Gleichzeitig haben Menschen, die erst einmal depressiv geworden sind, Schwierigkeiten, ihr soziales Netzwerk aufrechtzuerhalten. Sie sprechen langsamer und monotoner und halten weniger Augenkontakt, zudem sind sie weniger kompetent beim Lösen interpersonaler Probleme.

Der Medizinsoziologe Johannes Siegrist hat auf der Grundlage umfangreicher empirischer Studien das Modell der Gratifikationskrise (verletzte soziale Reziprozität) zur Erklärung des Auftretens zahlreicher Stresserkrankungen (wie Herz-/Kreislauf-Erkrankungen, Depression) vorgeschlagen.

Gratifikationskrisen gelten als großer psychosozialer Stressfaktor. Sie können vor allem in der Berufs- und Arbeitswelt, aber auch im privaten Alltag (z. B. in Partnerbeziehungen) als Folge eines erlebten Ungleichgewichtes von wechselseitigem Geben und Nehmen auftreten. Sie äußern sich in dem belastenden Gefühl, sich für etwas engagiert eingesetzt oder verausgabt zu haben, ohne dass dies gebührend gesehen oder gewürdigt wurde. Oft sind solche Krisen mit dem Gefühl des Ausgenutztseins verbunden. In diesem Zusammenhang kann es zu heftigen negativen Emotionen kommen. Dies wiederum kann bei einem Andauern auch zu einer Depression führen.

Eine Depression bei einem Familienmitglied wirkt sich auf Kinder aller Altersgruppen aus. Elterliche Depression ist ein Risikofaktor für zahlreiche Probleme bei den Kindern, jedoch insbesondere für Depressionen. Viele Studien haben die negativen Folgen der Interaktionsmuster zwischen depressiven Müttern und ihren Kindern belegt. Bei den Müttern wurde mehr Anspannung und weniger verspielte, wechselseitig belohnende Interaktion mit den Kindern beobachtet. Sie zeigten sich weniger empfänglich für die Emotionen ihres Kindes und weniger bestätigend im Umgang mit dessen Erlebnissen. Außerdem boten sich den Kindern Gelegenheiten zum Beobachten depressiven Verhaltens und depressiven Affektes.

Das Risiko einer Depression ist weltweit so beträchtlich, dass für manche eine evolutionäre Anpassung (adaptive Funktion) wahrscheinlicher erscheint als ein isoliertes Krankheitsgeschehen. Eine früher vorteilhafte Reaktionsweise kann unter heutigen Lebensbedingungen unbedeutend sein, d. h. die jeweilige Veranlagung nur noch als Krankheit oder Störung zu Tage treten. In der Diskussion ist ferner, ob Depressionen nicht auch heute noch eine Funktion haben, die evtl. zu wenig wahrgenommen wird.

Stevens und Price sehen aufgrund von Häufigkeit, Symptomatik und sozialem Kontext verschiedene psychische Störungen als einstmals adaptive soziale Reaktionsweisen. Depressionen werden in diesem Zusammenhang als Unterordnungsreaktion auf eine Niederlage betrachtet. Der zu beobachtende Anstieg der Krankheitslast durch Depressionen wird daher mit unseren Lebensbedingungen, speziell gesellschaftlichen Faktoren und Konkurrenz in Verbindung gebracht. Andere Autoren sehen den wesentlichen adaptiven Aspekt in der Handlungshemmung, die mit Depressionen verbunden ist, da diese unter verschiedensten Umweltbedingungen funktional sein kann. Diese weitere Interpretation beruft sich darauf, dass Depressionen ganz unterschiedliche Auslöser haben, d. h. als psychische Reaktion, als Reaktion auf körperliche Erkrankung sowie als Lichtmangelreaktion auftreten können.

Die evolutionsbiologischen Theorien zur Depressionsentstehung werden wissenschaftlich diskutiert, sind aber bisher nicht in Konzepten für die Prävention und/oder Therapie von Depressionen berücksichtigt worden.

Depressionen können bei der Mehrheit der Patienten erfolgreich behandelt werden. In Frage kommen eine medikamentöse Behandlung mit Antidepressiva, Psychotherapie (insbesondere verhaltenstherapeutische Verfahren) oder eine Kombination aus medikamentöser und psychotherapeutischer Behandlung, die zunehmend auch durch Online-Therapieprogramme ergänzt und unterstützt wird, sowie Stimulationsverfahren wie die Elektrokonvulsionstherapie.

Die aktuelle nationale Behandlungsleitlinie wertet bei mittelschweren bis schweren depressiven Perioden Antidepressiva als gleichwertig mit einer Psychotherapie. Bei schweren Depressionen wird eine Kombination von Psychotherapie und antidepressiver Medikation empfohlen.

Es gibt verschiedene psychotherapeutische Verfahren. Durchgeführt wird die Psychotherapie von Ärzten mit entsprechender Zusatzqualifikation (ärztlichen Psychotherapeuten), von psychologischen Psychotherapeuten, von Kinder- und Jugendlichenpsychotherapeuten, von Heilpraktikern oder von Heilpraktikern für Psychotherapie gemäß § 1 Heilpraktikergesetz (HeilprG). Häufig erfolgt parallel dazu die Gabe von Antidepressiva durch den Hausarzt oder Psychiater.

Eine Kombination von Psychotherapie und medikamentöser Behandlung kann von Ärzten (i. d. R. von Fachärzten für Psychiatrie oder Psychosomatik, teilweise auch von Allgemeinmedizinern und anderen Fachrichtungen) mit psychotherapeutischer Weiterbildung, oder durch eine Kooperation von Ärzten und Psychotherapeuten ambulant oder in psychiatrischen Kliniken bzw. Fachkrankenhäusern durchgeführt werden.

Bei hohem Leidensdruck und einem nicht zufriedenstellenden Ansprechen auf ambulante Therapie und Psychopharmaka – "insbesondere jedoch bei drohendem Suizid" – ist eine Behandlung in einer psychiatrischen Klinik in Erwägung zu ziehen. Eine solche Behandlung bietet dem Patienten eine Tagesstruktur und die Möglichkeit intensiverer psychotherapeutischer und medizinischer Maßnahmen, auch solche, die ambulant nicht abrechenbar und somit insbesondere in der kassenärztlichen Versorgung nicht möglich sind. Häufig ist auch die medikamentöse Einstellung, z.&nbspB. bei Lithiumtherapie, ein Grund für einen Krankenhausaufenthalt. Dabei ist es auch möglich, sich in einer Tagesklinik tagsüber intensiv behandeln zu lassen, die Nacht aber zu Hause zu verbringen.
Psychiatrische Kliniken haben in der Regel offene und geschlossene Stationen, wobei Patienten auch auf geschlossenen Stationen in der Regel Ausgang haben.

Stationäre Depressionsbehandlungen sind in den letzten Jahren sehr viel häufiger geworden, als extremes Beispiel ist etwa die Häufigkeit von Krankenhausbehandlungen aufgrund wiederholter (rezidivierender) Depressionen zwischen 2001 und 2010 auf mehr als das 2,8-fache angestiegen. Der Anstieg der Zahl an Krankenhausbehandlungen spiegelt jedoch nicht den der Behandlungstage wider, da sich die durchschnittliche Verweildauer im Krankenhaus gleichzeitig verkürzte. Depressionen verursachten nach Daten der Barmer GEK im Jahre 2010 über sechs Prozent aller Krankenhaustage und liegen damit mit großem Abstand an der Spitze aller Diagnosen. Die Erfolgsraten sind jedoch ernüchternd, so sind mehr als die Hälfte der Entlassenen auch ein Jahr nach Entlassung noch depressiv.

Zur Behandlung der Depression kann ein breites Spektrum psychotherapeutischer Verfahren wirksam eingesetzt werden (aktuelle Übersicht über evaluierte Therapieverfahren bei Hautzinger, 2008). Hierzu gehören die kognitive Verhaltenstherapie und die tiefenpsychologisch fundierte Psychotherapie. Auch die Gesprächspsychotherapie sowie die Gestalttherapie können zur Behandlung eingesetzt werden. Neuere integrative Ansätze zur Behandlung chronischer bzw. rezidivierender Depressionen sind das Cognitive Behavioral Analysis System of Psychotherapy (CBASP) sowie die Achtsamkeitsbasierte Kognitive Therapie (engl. "Mindfulness Based Cognitive Therapy", MBCT). Seit einigen Jahren kommen auch zunehmend Online-Therapieprogramme (Onlineberatung) zum Einsatz (z. B. deprexis24 oder iFightDepression.).

Die verhaltenstherapeutische Behandlung der Depression wird heutzutage auf der Grundlage der Kognitiven Verhaltenstherapie durchgeführt. In der Therapie sollen die depressionsauslösenden Denkmuster und Verhaltensmuster herausgearbeitet werden, um sie anschließend Schritt für Schritt zu verändern. Zusätzlich wird der Patient zu größerer Aktivität motiviert, um seine persönlichen Verstärkermechanismen wieder zu aktivieren und um die erwiesen positiven Wirkungen größerer körperlicher Aktivität auf die Stimmung zu nutzen.

In der tiefenpsychologischen Behandlung sollen durch die Aufdeckung und Bearbeitung unbewusster psychischer Konflikte und verdrängter Erfahrungen die zugrundeliegenden Ursachen für die Erkrankung bewusst gemacht werden. Die im Laufe der Therapie für den Patienten wahrnehmbar werdenden zu Grunde liegenden Motive, Gefühle und Bedürfnisse sollen dadurch in das aktuelle Leben integrierbar werden.

Hinsichtlich der Wirksamkeit verschiedener Psychotherapien lassen sich keine pauschalen Empfehlungen geben, sodass hier die Präferenzen, Hauptbeschwerden und auslösende oder aktuell belastende Faktoren des Patienten bei der Auswahl des Therapeutischen Verfahrens berücksichtigt werden sollten. Auch die aktuelle nationale Behandlungsleitlinie beinhaltet keine Empfehlung zu spezifischen Psychotherapieverfahren, sondern verweist auf Evidenztabellen mit unterschiedlichen Forschungsergebnissen.

Die Wirksamkeit von Antidepressiva ist stark abhängig vom Schweregrad der Erkrankung. Während bei mildem und mäßigem Schweregrad die Wirksamkeit fehlend oder gering ist, ist sie bei schwerer Depression deutlich. Metastudien weisen darauf hin, dass antidepressive Medikamente in ihrer Wirksamkeit von Patient zu Patient große Unterschiede zeigen und in manchen Fällen eine Kombination verschiedener Medikamente Vorteile haben kann.

Unerwünschte Nebenwirkungen sind seit Einführung der Serotonin-Wiederaufnahmehemmer (SSRI, siehe unten) in den 80er Jahren deutlich zurückgegangen, jedoch weiterhin zu beachten.

Die Therapietreue (Compliance) der Patienten bei der Anwendung der Medikamente ist wie bei anderen psychiatrischen Medikamenten relativ gering. Nur etwa die Hälfte bleibt in der akuten Phase dabei, und hiervon wiederum nur etwa die Hälfte auch in der Nachfolgephase. Verschiedene Strategien zur Verbesserung dieser Situation wurden wissenschaftlich verglichen. Aufklärende Gespräche alleine waren nicht effektiv. Umfangreiche begleitende Maßnahmen, z. B. auch über Telefon, waren hier erforderlich.

Die bekanntesten Antidepressiva lassen sich in drei Gruppen einteilen (siehe unten). Weitere Antidepressiva einschließlich Phytopharmaka wie Johanniskraut finden sich im Artikel Antidepressiva. Im Falle schwerer Depressionen ohne Ansprechen auf einzelne Antidepressiva werden teilweise Augmentationen mit weiteren Antidepressiva, Neuroleptika, Stimulanzien oder Phasenprophylaktika verordnet. Neuere Studien weisen auf eine geeignete Einsatzmöglichkeit von Ketamin, aufgrund seiner schnellen therapeutischen Wirkung, für die Akutbehandlung von therapieresistenten und vor allem suizidgefährdeten depressiven Patienten hin (siehe unten).

In neuerer Zeit haben sich starke Hinweise darauf ergeben, dass Antidepressiva ihre Wirksamkeit nicht nur über ihren Einfluss auf die Verschaltung von Nerven (via Neurotransmitter), sondern möglicherweise zu einem Teil auch durch die Neubildung des Wachstumsfaktors BDNF im Gehirn entfalten, wodurch Regenerationsprozesse insbesondere im präfrontalen Cortex und im Hippocampus ausgelöst werden.

Diese Wirkstoffe hemmen die Wiederaufnahme der Neurotransmitter Serotonin, Noradrenalin oder Dopamin in die Präsynapse. Direkte Wirkungen auf andere Neurotransmitter sind bei diesen "selektiven" Wirkstoffen deutlich schwächer ausgeprägt als bei trizyklischen Antidepressiva.

Die selektiven Serotonin-Wiederaufnahmehemmer (SSRI) werden bei Depressionen heute am häufigsten eingesetzt. Sie wirken ab einer Einnahmedauer von zwei bis drei Wochen. Sie hemmen (weitgehend) selektiv die Wiederaufnahme von Serotonin an der präsynaptischen Membran, wodurch eine „relative“ Vermehrung des Botenstoffs Serotonin bei der Signalübertragung erzielt wird. Ähnlich wirken Serotonin-Noradrenalin-Wiederaufnahmehemmer (SNRI), welche zusätzlich die Wiederaufnahme von Noradrenalin in die Präsynapse vermindern. Von vergleichbarem Wirkmechanismus sind Noradrenalin-Dopamin-Wiederaufnahmehemmer und selektive Noradrenalin-Wiederaufnahmehemmer, welche die Wiederaufnahme von Noradrenalin, bzw. Noradrenalin und Dopamin hemmen. SSRI und SNRI (z. B. Reboxetin) unterscheiden sich in ihrem Nebenwirkungsprofil.

Die Pathogenese von Depressionen, aber auch von Manien und Obsessionen (Zwangshandlungen), wird von Forschern u. a. mit serotonerg vernetzten Neuronen in Verbindung gebracht. Daher werden SSRI und SNRI auch gegen (komorbide) Zwangs- und Angstzustände eingesetzt, oft mit Erfolg. Da Serotonin auch bei anderen neural vermittelten Prozessen im ganzen Körper eine Rolle spielt, wie zum Beispiel bei Verdauung und Gerinnung des Blutes, resultieren daraus auch die typischen Nebenwirkungen, durch Interaktion mit anderen neural gesteuerten Prozessen.

SSRIs werden seit ca. 1986 eingesetzt; seit 1990 sind sie die am häufigsten verschriebene Klasse von Antidepressiva. Wegen des nebenwirkungsärmeren Profils, vor allem in Bezug auf Kreislauf und Herz, sind sie sehr beliebt. Relativ häufige Nebenwirkungen sind jedoch sexuelle Dysfunktion und/oder Anorgasmie. Diese bilden sich zwar einige Wochen nach Absetzen oder Wechsel des Medikaments fast immer vollständig zurück, können jedoch zu zusätzlichen (Beziehungs-)Problemen führen.

Die trizyklischen Antidepressiva (z. B. Opipramol, Amitriptylin) wurden bis zum Aufkommen der Serotonin-Wiederaufnahmehemmer am häufigsten verschrieben. Es handelt sich um eine relativ große Gruppe von Substanzen, die sich in ihren Wirkungen und vor allem in ihren Kombinationsmöglichkeiten mit anderen Klassen von Antidepressiva markant unterscheiden und daher fundiertes Wissen erfordern. Hauptnachteil sind deren ausgeprägter auftretende Nebenwirkungen (z. B. Mundtrockenheit, Verstopfung, Müdigkeit, Muskelzittern und Blutdruckabfall), wobei es besser verträgliche Ausnahmen gibt (z. B. Amoxapin, Maprotilin). Bei älteren und bei durch Vorerkrankungen geschwächten Menschen ist daher Vorsicht geboten. Zudem wirken einige Trizyklika häufig zunächst antriebssteigernd und erst danach stimmungsaufhellend, wodurch es zu einem höheren Suizidrisiko in den ersten Wochen der Einnahme kommen kann. In den USA müssen aber auch SSRI einen diesbezüglichen Warnhinweis tragen.

MAO-Hemmer wirken durch Blockieren der Monoaminoxidase-Enzyme. Diese Enzyme spalten Monoamine wie Serotonin, Noradrenalin und Dopamin – also Neurotransmitter (Botenstoffe zwischen den Nervenzellen im Gehirn) – und verringern dadurch deren Verfügbarkeit zur Signalübertragung im Gehirn. Die MAO-Hemmer hemmen diese Enzyme, wodurch sich die Konzentration der Monoamine und damit der Neurotransmitter erhöht und die Signalübertragung zwischen den Nervenzellen verstärkt wird.

MAO-Hemmer werden in selektiv oder nichtselektiv sowie reversibel oder irreversibel unterteilt. Selektive Inhibitoren der MAO-A (z. B. Moclobemid, reversibel) hemmen nur den Typ A der Monoaminoxidase und zeigen eine antidepressive Wirkung. Sie sind im Allgemeinen gut verträglich, unter anderem mit deutlich weniger Störungen bei Verdauungs- und Sexualfunktionen als bei SSRI. Selektiv MAO-B-hemmende Wirkstoffe (z. B. Selegilin, irreversibel) werden in erster Linie in der Parkinson-Behandlung eingesetzt. Nichtselektive MAO-Hemmer (z. B. Tranylcypromin, irreversibel) hemmen MAO-A und MAO-B und sind von hoher Wirksamkeit in der Behandlung von (therapieresistenten) Depressionen und Angststörungen. Irreversible MAO-Hemmer binden die MAO-A bzw. MAO-B dauerhaft. Um die Wirkung aufzuheben, muss das betroffene Enzym vom Körper erst neu gebildet werden, was Wochen dauern kann. Reversibilität besagt, dass das Medikament nur schwach an die MAO bindet und MAO-A bzw. MAO-B spätestens mit dem Abbau des Medikaments wieder intakt freigibt.

Monoaminoxidasehemmer gelten als gut wirksam. Allerdings müssen Patienten, die nichtselektive, irreversible MAO-Hemmer einnehmen, eine strenge, tyraminarme Diät halten. In Verbindung mit dem Verzehr bestimmter Lebensmittel, wie z. B. Käse und Nüssen, kann die Einnahme von nichtselektiven irreversiblen MAO-Hemmern zu einem gefährlichen Blutdruckanstieg führen.

Für die Behandlung von depressiven Notfällen (Suizidgefährdung) hat sich in der klinischen Forschung seit 2010 die schnelle Wirkung von Ketamin, einem Antagonisten am Glutamat-NMDA-Rezeptorkomplex, bewährt.

Untersuchungen von 2014 an der Charité in Berlin bestätigten diese Einsatzmöglichkeit für die Akutbehandlung von therapieresistenten und vor allem suizidgefährdeten Patienten.

Eine Metaanalyse von 2015 von acht randomisierten kontrollierten Studien bestätigte die Wirkung von Ketamin nach einmaliger Gabe zur sofortigen Behandlung uni- und bipolarer Depression. Nach einer weiteren Metaanalyse von 2015 führte eine einmalige Gabe zu einer signifikanten Besserung über einen Zeitraum von mindestens sieben Tagen. Eine Übersicht von 2015 über neun Einzelstudien zur Behandlung von insgesamt 137 Patienten mit "Suizidgefährdung" (Suizidalität) berichtete über eine schnelle Besserung (ab 40 Minuten) in jeder der 9 Einzelstudien.

Nach vorläufigen Ergebnissen von 2013 lässt sich mit einer sehr niedrigen Dosis von "Ketamin" unter der Zunge (sublingual) bei Depression sowohl eine schnelle als auch – bei regelmäßiger Einnahme – eine andauernde positive Wirkung bezüglich Stimmung, Stabilität, Wahrnehmung und Schlaf erreichen.

Mitte 2015 berichteten die Pharmakologie-Professoren Lutz Hein (Universität Freiburg) und Roland Seifert (Medizinische Hochschule Hannover), dass Ketamin aus guten Gründen bereits off-label, d. h. obwohl es von den Arzneimittelbehörden bis dato nur für andere Anwendungsgebiete zugelassen war, auch für depressive und suizidale Patienten verschrieben wird.

Nach einer von der Cochrane Collaboration veröffentlichten systematischen Übersichtsarbeit von 2015 waren die bis dato vorliegenden Studien zur Wirksamkeit der Lichttherapie bei saisonal auftretender Depression aus methodischen Gründen derart mangelhaft, sodass die Autoren keine Schlussfolgerungen ziehen konnten. Nach früheren Ergebnissen war Lichttherapie ebenfalls bei nicht jahreszeitlich bedingten Depressionen wirksam. Dabei sollten die Patienten – um einen Effekt zu gewährleisten – täglich für mindestens 30 Minuten in eine spezielle Lichtquelle schauen, die weißes Vollspektrumlicht von mindestens 10.000 lux abgibt. Es werden 10.000 Lux für 30–40 Minuten als anfängliche Dosis empfohlen, wenigstens zwei bis vier Wochen jeden Morgen und zwar so rasch wie möglich nach dem Erwachen.

Insbesondere bei schweren und über lange Zeit gegen medikamentöse und psychotherapeutische Behandlung resistenten Depressionen kommen gerade in jüngerer Zeit wieder stärker Stimulationsverfahren zum Einsatz, deren Wirkmechanismen jedoch noch weitgehend unklar sind.

Elektrokonvulsionstherapie (EKT)

Das häufigste diesbezüglich eingesetzte Verfahren ist die Elektrokonvulsionstherapie. In der Epilepsie-Behandlung war aufgefallen, dass bei Patienten, die gleichzeitig an einer Depression litten, nach einem epileptischen Anfall auch eine Verbesserung der Stimmung auftrat. Die Elektrokonvulsionstherapie wird in Narkose durchgeführt und stellt dann, wenn Medikamente bei schweren Depressionen nicht wirken, eine ernsthafte Alternative dar. Sie hat sich als hocheffektiv und erfolgreich erwiesen. Durch sie werden neuroendokrinologische Störungen normalisiert, und die gestörte Signalübertragung insbesondere der monoaminergen Neurotransmitter Serotonin, Dopamin und Noradrenalin wiederhergestellt.

Magnetkrampftherapie

Diese Therapieform befindet sich noch im Versuchsstadium. Sie funktioniert vom Grundprinzip her genauso wie die Elektrokonvulsionstherapie. Es wird wie bei der EKT ein antidepressiv wirkender Heilkrampf unter Narkose ausgelöst. Allerdings nicht durch direkte elektrische Stimulation, sondern mittels starker Magnetfelder. Diese Methode soll weniger Nebenwirkungen haben als die EKT, jedoch liegen Nachweise der Wirksamkeit bislang (Stand Dezember 2015) noch nicht vor.

Vagusnerv-Stimulation

Derzeit untersucht wird die Vagusnerv-Stimulation, bei der durch eine Art Herzschrittmacher im Abstand von einigen Minuten jeweils kleine elektrische Impulse an den Vagusnerv geschickt werden. Diese Therapie, die insbesondere bei Epilepsie-Patienten Anwendung findet, wird für ansonsten therapieresistente Patienten erforscht und ist in den USA bereits zugelassen. Allerdings besteht noch Unklarheit bezüglich der Wirksamkeit.

Transkranielle Magnetstimulation

Ebenfalls getestet wird derzeit die transkranielle Magnetstimulation (TMS), bei der das Gehirn der Patienten durch ein Magnetfeld außerhalb des Kopfes angeregt wird. Die TMS erfolgt bei vollem Bewusstsein. Die bisherigen Ergebnisse waren positiv.

Transkranielle Gleichstromapplikation (tDCS)

Hier wirkt ein schwacher elektrischer Strom durch den Schädelknochen hindurch auf das Gehirn ein. Die tDCS erfolgt bei vollem Bewusstsein. Bislang (Stand Dezember 2015) besteht noch Unklarheit bezüglich der Wirksamkeit.

Eine Form der unterstützenden therapeutischen Maßnahmen ist die Sporttherapie. Wenn Sport im gesellschaftlichen Zusammenhang stattfindet, erleichtert er eine Wiederaufnahme zwischenmenschlicher Kontakte. Ein weiterer Effekt der körperlichen Betätigung ist das gesteigerte Selbstwertgefühl und die Ausschüttung von Endorphinen. Positive Effekte des Joggings bei Depressionen sind empirisch durch Studien nachgewiesen; 1976 wurde das erste Buch unter dem Titel "The joy of Running" zu diesem Thema veröffentlicht.

Krafttraining beispielsweise konnte in einer Studie für alte Patienten (70+ Jahre) als wirksam erwiesen werden. Nach 10 Wochen angeleiteten Trainings war ein Rückgang der depressiven Symptome im Vergleich zu einer Kontrollgruppe (die nicht trainiert, sondern angeleitet gelesen hatte) feststellbar. Der Effekt war für einen Teil der Testpersonen auch zwei Jahre nach Ende des geführten Trainings noch nachweisbar.

Es existiert eine Vielzahl von methodisch robusten Studien über den Nutzen von Sport und Bewegung bei Depression. Diese zeigen beispielsweise, dass Sport (unter Anleitung, zuhause) gegen Depression gleich wirksam ist wie eine medikamentöse Therapie (Sertralin) oder Placebo-Medikation. Eine Metastudie von 2013 bewertet den Effekt zurückhaltender, unterstreicht aber den präventiven Effekt, da „moderate Bewegung im aeroben Bereich von mindestens 150 Minuten pro Woche […] mit einem merklich geringeren Risiko für die Entwicklung einer Depression in Zusammenhang“ steht.

Wissenschaftliche Studien lassen auf die besondere Bedeutung von Eicosapentaensäure (EPA) zur Stimmungsaufhellung und günstigen Einflussnahme auf Minderung von Depressionen schließen EPA ist eine mehrfach ungesättigte Fettsäure aus der Klasse der Omega-3-Fettsäuren.

Der Wirkungsmechanismus der Omega-3-Fettsäure ist noch nicht aufgeklärt, jedoch wird eine Interaktion der Fettsäure mit dem Neurotransmitter Serotonin vermutet: Ein Mangel an Serotonin wird häufig von einem Mangel an Omega-3-Fettsäure begleitet, umgekehrt scheint die Gabe der Fettsäure zur Erhöhung des Serotoninspiegels zu führen.
Die orthomolekulare Medizin versucht außerdem über die Aminosäuren Tyrosin, Phenylalanin und Tryptophan bzw. 5-HTP (Oxitriptan) Depressionen günstig zu beeinflussen.
Diese Aminosäuren werden im Körper in Noradrenalin, Dopamin und Serotonin umgewandelt. Die Erhöhung des Spiegels dieser Neurotransmitter im Gehirn kann stimmungsaufhellend sein.

Es ist sicher nicht falsch, auch nach Abklingen der depressiven Beschwerden auf eine ausgewogene und gesunde Ernährung zu achten. Dabei spielt vor allem ein gleichmäßiger Blutzuckerspiegel durch regelmäßige Mahlzeiten eine Rolle, ebenso wie ein maßvoller Umgang mit Genussmitteln wie Kaffee, Nikotin und Alkohol dazu beitragen kann, psychisch stabil zu bleiben.

Depression wirkt sich auf die Qualität des Schlafes aus (s. o.). Umgekehrt gilt, dass eine Verbesserung des Schlafes (Schlafhygiene) sich bessernd auf eine Depression auswirken kann. Dazu gehören regelmäßige Zu-Bett-geh-Zeiten, der Verzicht auf Monitor-Licht am Abend, angepasste Beleuchtung, Abdunklung der Schlafräume und weitere Maßnahmen.

Partieller (teilweiser) Schlafentzug in der zweiten Nachthälfte oder gar vollständiger Schlafentzug in einer Nacht ist die einzige antidepressive Therapie mit positiven Wirkungen bei ca. 60 % der Patienten noch am gleichen Tag. Der antidepressive Effekt ist jedoch selten anhaltend. Meistens kehren die depressiven Symptome bereits nach der nächsten Erholungsnacht wieder. Bis zu 15 % der Patienten in klinischen Studien zeigten jedoch eine anhaltende Verbesserung nach völligem Schlafentzug. Die nationale Behandlungsleitlinie empfiehlt, dass die Wachtherapie auf Grund ihrer relativ leichten Umsetzbarkeit, Nichtinvasivität, Kosteneffizienz und raschen Wirkung in bestimmten Fällen als ergänzendes Therapieelement erwogen werden sollte.

Eine systematische Übersichtsarbeit aus dem Jahre 2015 zeigte positive Effekte von Meditationstechniken. Eine ähnliche Arbeit von 2014 beschrieb kleine bis mäßige Wirkungen, wobei außerdem festgestellt wurde, dass es keine Belege dafür gab, dass Meditation effektiver war als andere gezielte Aktivitäten, wie zum Beispiel Sport oder progressive Muskelentspannung.

Von Seiten einiger Schönheitschirurgen wurde nach 2012 behauptet, die Verminderung der vertikalen Hautfalten zwischen den Augenbrauen (Glabella-Region) durch eine Injektion des Nervengifts Botulinumtoxin (Botox) sei wirksam gegen Depression. Die Tiefe der Falten nehme ab, und dadurch verbessere sich die Stimmung (Facial-Feedback-Hypothese). Die Autoren mussten jedoch einräumen, dass in den Fällen, bei denen eine Besserung der Stimmung beobachtet werden konnte, ein Placebo-Effekt die Wirkung erzielt haben konnte. Es war nämlich für die Teilnehmer der Studien schnell im Spiegel sichtbar, ob sie zur Gruppe der echten Behandlung oder zur Gruppe der Scheinbehandlung gehörten. Eine nachträgliche Untersuchung von 2015 zeigte dann, dass es nicht einmal einen Zusammenhang zwischen der Tiefe der Falten vor der Behandlung und den Anzeichen für einen Rückgang von Depression nach der Behandlung gab. Nur das Maß der Erregung der Patienten vor der Behandlung erlaubte eine Abschätzung der Wahrscheinlichkeit eines möglichen "Therapieerfolgs".

Begleitende Gesundheitsrisiken

Durch häufig ungesünderen Lebensstil leiden Patienten mit Depressionen vermehrt an Folgen von Rauchen, Bewegungsmangel, Ernährungsfehlern und Übergewicht. Zudem gibt es Hinweise darauf, dass unregelmäßige Medikamenteneinnahmen auch ein kardiovaskuläres Risiko darstellen, wodurch eine höhere Anfälligkeit für Schlaganfälle besteht. Dies trifft vor allem für Frauen im mittleren Alter zu.

Die Depression selbst ist ein Risikofaktor für die Entwicklung einer koronaren Herzkrankheit. Als Ursachen hierfür kommen Einflüsse der Depression auf die Steuerung der Hormonregulation in der Nebenniere, Einflüsse auf Immunsystem und Hämostase, aber auch ein ungesünderer Lebensstil oder Nebenwirkungen von Antidepressiva in Frage. Bei einem Patienten mit koronarer Herzkrankheit erhöht die Depression wiederum das Risiko auf einen Myokardinfarkt auf das drei- bis vierfache. Weiterhin ist das Risiko eines tödlichen Herzinfarkts erhöht. Studien zeigen, dass trotzdem bei Patienten mit Myokardinfarkt die Depression vielfach unbehandelt bleibt. Eine Behandlung der Depression würde günstige Effekte auf die Heilungsaussichten der Patienten haben.

Suizidgefahr

Man geht davon aus, dass rund die Hälfte der Menschen, die einen Suizid begehen, an einer Depression gelitten haben. Im Jahre 2010 verübten in Deutschland rund 7000 Menschen mit Depression Suizid. Bei der Depression handelt es sich daher um eine sehr ernste Störung, die umfassender Therapie bedarf.

Dem Staat und der Wirtschaft entstehen jährlich Kosten von bis zu 21,9 Milliarden Euro für die Behandlung; hinzu kommen Fehlzeiten und verminderte Produktivität betroffener Mitarbeiter. Knapp 25 % aller Fehltage im Beruf gehen auf das Konto von Depressionen.

Zur Verbesserung der Rahmenbedingungen hat das Gesundheitssystem seit den 1990er Jahren verschiedene Modellprojekte initiiert. Das Bundesministerium für Arbeit und Soziales hat den „Schutz der Gesundheit bei arbeitsbedingter psychischer Belastung“ zu einem Hauptziel der Gemeinsamen Deutschen Arbeitsschutzstrategie ab 2013 erhoben. Das Bundesministerium für Gesundheit (BMG) hat 2012 das Forschungsprojekt "Deprexis" zu den Möglichkeiten der Online-Therapie in Auftrag gegeben, was möglicherweise einen Weg darstellen könnte, um Versorgungslücken und lange Wartezeiten auf einen Therapieplatz zu überbrücken.

Die Behandlung depressiver Erkrankungen wurde 2006 als Gesundheitsziel verankert. Zu den Teilzielen gehören Aufklärung, Prävention und Rehabilitation.

Gesetzliche Krankenkassen sind verpflichtet, gemeinnützige Organisationen im Bereich Selbsthilfe zu fördern, im Jahr 2011 betrug diese Förderung insgesamt rund eine halbe Million Euro.

Vereine, gemeinnützige GmbH (gGmbH) und Stiftungen befassen sich mit dem Thema Depression. Die Angebote setzen an folgenden Punkten an:



Einführungen
Ratgeberliteratur
S3-Leitlinien



</doc>
<doc id="12062" url="https://de.wikipedia.org/wiki?curid=12062" title="Protisten">
Protisten

Die Protisten ( Protista, „Urwesen“, „Erstlinge“) sind eine Gruppe nicht näher verwandter mikroskopischer Lebewesen, die jedoch lange als Taxon (systematische Einheit) betrachtet wurde. Dazu gehören alle ein- bis wenigzelligen Eukaryoten, also Algen, Protozoen und einige Pilze.

Die ersten Protisten wurden 1675 von Antoni van Leeuwenhoek beobachtet. 1866 dann wurden die Protista von Ernst Haeckel als Taxon eingeführt. Sie wurden als eigenes Reich innerhalb der Eukaryoten gefasst und den Pflanzen (Reich Plantae), Tieren (Reich Animalia) und Pilzen (Reich Fungi) gegenübergestellt. Man wollte damals Mikroorganismen, also ein- und wenigzellige Organismen, von den übrigen Lebewesen trennen und in zwei Reichen (Monera und Protista) zusammenfassen.

Diese Einteilung hielt sich zwar lange, entspricht jedoch nicht den natürlichen Verwandtschaftsverhältnissen. Da Pflanzen, Tiere und Pilze sich aus Protisten entwickelten und viele vielzellige Lebewesen mit ein- und wenigzelligen Protisten verwandt sind, ergab sich mit den Protista ein unnatürliches System. In modernen, auf Verwandtschaftsverhältnissen beruhenden Systematiken gibt es die Protisten deshalb nicht mehr. Die zu den Protisten zählenden Gruppen wurden entsprechend ihren Verwandtschaftsverhältnissen verschiedenen Evolutionslinien zugeteilt, darunter auch zwei Linien, die Pilze und vielzellige Tiere beziehungsweise höhere Pflanzen enthalten. Die zu den Protisten gehörenden Choanoflagellaten ergeben zusammen mit den Pilzen und Tieren die Opisthokonta. Die Rotalgen (Rhodoplantae), Grünalgen und höheren Pflanzen (Viridiplantae) bilden die systematische Gruppe der Pflanzen (Plantae).

Trotzdem findet der Begriff "Protisten" noch bis in die Gegenwart Gebrauch als nicht-systematische Bezeichnung. Eine häufig zu findende Definition lautet „alle einzelligen Eukaryoten“. Dazu zählen sowohl einige Algen, einige Pilze und die Protozoen. Diese Definition bereitet allerdings bei Nesseltieren wie den Myxozoa ebenso Schwierigkeiten wie bei den Myxogastria. Heute werden unter der Bezeichnung "Protisten" meist „alle ein- bis wenigzelligen Eukaryoten“ verstanden.

Protisten bewegen sich oft schwimmend mit Hilfe von Geißeln oder Wimpern oder kriechend, gleitend, fließend oder schreitend durch Ausbildung von Scheinfüßchen (Pseudopodien). Etliche Arten schweben auch einfach nur im Wasser, das Schweben wird oft unterstützt durch lange Zellfortsätze.

Die meisten Protisten leben im Meer, teils nahe der Oberfläche, teils schwebend im Wasser, teils auf dem Grund kriechend, teils an Steinen, Pflanzen und dergleichen festsitzend; andere findet man im Süßwasser, wenige auf dem Land. Es gibt heterotrophe, autotrophe, mixotrophe, aerobe und anaerobe Formen. Etliche Protisten leben auch parasitisch in Tieren.

Die Protisten pflanzen sich gewöhnlich durch Zweiteilung ungeschlechtlich fort. Bei einigen Arten gibt es aber auch Vielfachteilungen, bei einigen kommen geschlechtliche Vorgänge vor.

Viele Protisten treten in erstaunlicher Individuenzahl auf. Die unverweslichen Überreste abgestorbener Vertreter – wie die Kieselskelette der Radiolarien und Kieselalgen (Bacillariophyta) oder die Kalkschalen der Foraminiferen – sind gesteinsbildend: Sie sedimentieren auf den Grund des Gewässers und werden durch Diagenese zu Gesteinen, zum Beispiel Kieselschiefer und Kreide, aus denen sich oft ganze Gebirgsschichten zusammensetzen.

Aus urtümlichen Protisten sind im Laufe der Evolution alle höheren vielzelligen Organismen hervorgegangen, wahrscheinlich überwiegend über den Weg der Zellkoloniebildung, wie sie heute noch bei etlichen Algen zu beobachten ist.

Weil sich an ihnen die Zellbestandteile sowie die wichtigsten Lebensäußerungen, wie Bewegung, Reizbarkeit, Fortpflanzung, gut studieren lassen, sind Protisten ein beliebtes Objekt biologischer Forschung. Nur etwa 40 Arten rufen jedoch Protozoeninfektionen beim Menschen hervor und sind von medizinischem Interesse.



</doc>
<doc id="12063" url="https://de.wikipedia.org/wiki?curid=12063" title="Fernsehnorm">
Fernsehnorm

Eine Fernsehnorm legt fest, auf welche Weise unterschiedliche Informationen, wie Bild- und Tondaten beim Fernsehen während der Übertragung vom Sender zum Empfänger übertragen werden. Dabei ist es sinnvoll, zwischen drei verschiedenen Stufen der Übertragung zu unterscheiden:


Ähnlich wie schon bei der weltweiten Ausbreitung des Hörfunk bestand auch für das Fernsehen der Bedarf nach internationaler Regulierung. Allerdings war beim Fernsehen wesentlich mehr festzulegen als nur Bandbreite, Senderabstand und die zu vergebenen Sendefrequenzen, denn Hörfunksender übertrugen ja lediglich Töne. Beim Fernsehfunk kommt zum Ton- das Bildsignal hinzu.

Fast gleichzeitig mit der Entwicklung des Schwarz-Weiß-Fernsehens wurde auch schon mit dem Farbfernsehen experimentiert. Bei den ersten Versuchen wurden mehrere Varianten entwickelt. Beispielsweise probierte der Fernsehpionier John Logie Baird für sein mechanisches Fernsehen ein System mit zwei Farben. Andere experimentierten entweder mit drei Kanälen (in jedem Kanal wurde jeweils ein Teilbild in einer der drei Grundfarben übertragen) oder der Übertragung über einen Kanal und synchronisiert rotierender Farbfilter vor Kamera und Empfänger. Hierbei musste allerdings eine wesentlich höhere Anzahl von Einzelbildern übertragen werden, damit der Eindruck eines flimmerfreien Bildes entsteht. Der amerikanische Fernsehsender CBS stellte 1943 ein Farbfernsehsystem mit schnell rotierenden Farbfiltern vor.

Übertragen wurden Bilder bei diesen frühen Versuchen des CBS-Farbfernsehen mit der Übertragungsnorm von 405 Zeilen und 144 Bildern pro Sekunde. Das Bild war zwar sehr gut, jedoch waren beispielsweise die Farbempfänger mit den Farbfilterscheiben durch diese sehr viel größeren Scheiben vor dem kleinen Bildschirm sehr klobig, laut und nicht kompatibel mit dem inzwischen eingeführten System mit 525 Zeilen und 60 Halbbildern pro Sekunde.
1941 führte die US-amerikanische Behörde "Federal Communications Commission (FCC)" ihre erste Schwarz-Weiß-Norm mit einem 525 Zeilenbild und dem Ton-Bild-Abstand von 4,5 MHz ein. (siehe auch → National Television Systems Committee)

In Deutschland wurden 1935, zur Einführung des Fernsehens, 180 Zeilen bei 25 Vollbildern pro Sekunde als Norm verwendet, jedoch schon ein Jahr später durch 441 Zeilen mit 50 Halbbildern ersetzt. In Großbritannien wurden bei der Einführung des Fernsehens zunächst mit 240 Zeilen bei 25 Vollbildern und 405 Zeilen mit 50 Halbbildern (ausgestrahlt im wöchentlichen Wechsel) experimentiert. Auch dort entschied man sich nach nur einem Jahr für die Norm mit 405 Zeilen, später "Norm A" genannt. In Frankreich wurde das Fernsehen mit 453 Zeilen offiziell eingeführt, während der Deutschen Besatzung Frankreichs wurde die Norm auf 441 Zeilen geändert und auch nach Kriegsende vorläufig beibehalten, zeitweise im Parallelbetrieb mit der neuen 819 Zeilen Norm.

Frankreich führte 1948 einen neuen Sendestandard mit 819 Zeilen pro Bild ein.

Im September 1948 beschloss in Westdeutschland eine 34-köpfige Expertengruppe zur Festlegung einer zukünftigen Sendenorm unter der Leitung des Technischen Direktors des NWDR Werner Nestel eine Zeilennorm von 625 Zeilen mit einem 2:1-Zeilensprungverfahren und einer Bildaufbaufrequenz von 50 Hz vor, das bedeutet:
25 mal in der Sekunde wird je ein aufeinanderfolgendes Filmbild elektronisch in 625 Zeilen zerlegt, wobei in den ungeraden 50stel Sekunden jeweils nur die Zeilen mit den ungeraden Zeilennummern 1, 3, 5 usw. des jeweiligen Bildes und in den geraden 50stel Sekunden anschließend die Zeilen mit den geraden Zeilennummern 2, 4, 6 usw., also 2•25=50 Halbbilder pro Sekunde, übertragen werden. Die zwei Halbbilder eines Bildes werden auf dem Bildschirm exakt zu einem Vollbild zusammengesetzt bzw. verzahnt (siehe Animation unten). Diese Parameter ergaben sich einerseits aus der traditionellen Frequenz des Netzwechselstromes in Europa von 50 Hz, andererseits entsprach die Festlegung auf 625 Zeilen einer annähernden Umrechnung des US-amerikanischen Standards "NTSC" auf diese gewachsenen europäischen technischen Gegebenheiten: 30 Bilder • 525 Zeilen ergeben insgesamt 15750 zu bildende Zeilen in der Sekunde. 25 Bilder • 625 Zeilen entsprechen 15625 Zeilen pro Sekunde. Die Zeilenfrequenz beträgt also 15625 Hz. Dieser Normenvorschlag wurde dann von dem zuständigen Gremium "CCIR" (Comité Consultatif International des Radiocommunications) der Genfer "Internationalen Fernmeldeunion" (ITU) als Grundlage für eine einheitliche west- und südeuropäische Fernsehsendenorm behandelt.

1961 wurde diese dann in Stockholm mit gewissen Abänderungen (insbesondere die Festlegung der Kanalbandbreite auf 7 MHz und einem Abstand der Ton- und Bild-Trägerfrequenz von 5,5 MHz) zum offiziellen Normenvorschlag, der sogenannten Gerber-Norm, des "CCIR" erklärt (benannt nach dem Vorsitzenden der "CCIR"-Arbeitsgruppe Walter Gerber). Dagegen verwendeten einige mitteleuropäische Länder (wie anfangs auch die DDR) und die Mehrzahl der osteuropäischen Länder die auf 8 MHz erweiterten Frequenzbänder und einen Ton- / Bildträger-Abstand von 6,5 MHz, allerdings wie die "CCIR"-Norm eine Zeilenzahl von 625. Sie wird als "OIRT-Norm" bezeichnet. Einige der "OIRT"-Länder wechselten im 21. Jahrhundert zur "CCIR"-Norm.

In der Anfangszeit des Fernsehens (1930er bis 1950er Jahre) erfolgte die Übertragung lediglich schwarz-weiß. Bereits in dieser Zeit entwickelten sich die technischen Normen in verschiedenen Ländern auseinander. Während des Zweiten Weltkrieges unterbrachen viele Länder ihre Fernsehaktivitäten und stiegen bei der Wiederaufnahme zum Teil auf eine andere Norm um.

Die nach dieser Umbruchphase noch gebräuchlichen Normen wurden von der ITU mit Römischen Zahlen und aktuell mit Großbuchstaben bezeichnet und wie folgt klassifiziert:

Beispiele einer kombinierten Bezeichnung aus den ITU-Festlegungen mit den Großbuchstaben-Codes für Schwarz-Weiß-Normen und für eine verwendete und dem angewendeten Farbfernsehsystem lauten folgendermaßen :


Die wichtigsten Parameter sind die Zeilenzahl, Bildwechsel pro Sekunde, Bild/Ton-Abstand, Ton-Modulation (FM oder AM) und Polarität der Bild-Modulation (positiv oder negativ). Die übrigen Spalten beziehen sich auf die spezielle Bandbreite, die ein TV-Kanal insgesamt zur Übertragung des Ton- und Bildsignales im Spektrum benötigt.

All diesen TV-Normen gemeinsam ist das Zeilensprungverfahren ("Interlacing"), d. h. jedes Vollbild wird in zwei aufeinanderfolgenden Halbbildern übertragen, so dass sich die doppelte Halbbildfrequenz ergibt.


Das Farbfernsehen kam später hinzu, für die USA und Japan in den 1950er Jahren und für die übrige Welt in den 1960er Jahren oder später.
Da es abwärtskompatibel zum Schwarz-Weiß-Fernsehen bleiben sollte, wurden die bestehenden Normen beibehalten und lediglich ein Farbsignal in Form eines Hilfsträgers zur Colorierung hinzugefügt.

Schwarz-Weiß-Empfänger können diesen Zusatzträger nicht dekodieren und empfangen Farbsendungen daher wie gewohnt in Schwarz-weiß; so wird die Kompatibilität hergestellt. Farbempfänger dekodieren Schwarz-weiß- und Farbsignal und generieren aus beiden zusammen das Farbbild. Dieses Verfahren ist allen terrestrischen Analog-Normen gemeinsam.

Das hinzugefügte Farbsignal kann auf drei verschiedene Arten moduliert werden: PAL, SECAM oder NTSC. Grundsätzlich kann jede der drei Farbnormen mit jeder der Schwarz-Weiß-Normen A–N kombiniert werden. Tatsächlich werden PAL und SECAM jedoch meistens mit einer der 625/25-Normen und NTSC ausschließlich
mit Norm M verwendet. Es existieren allerdings auch „Hybride“ wie z. B. PAL auf Norm M in Brasilien.

Unberücksichtigt bleiben bei dieser Betrachtung antennenspezifische Parameter, die zum Einfangen des Signals notwendig sind, wie Antennenrichtung, Polarisation, Antennenstandort.

Bei klassischer terrestrischer Übertragung sind die Parameter:
Kabel nutzt genau die gleichen Parameter wie die klassische terrestrische Übertragung. Es sind aber weitere Frequenzen zulässig, die bei der klassischen terrestrischen Übertragung für nicht-Fernseh-Zwecke reserviert sind; diese liegen

Bei der klassischen, analogen Satelliten-Übertragung sind die Parameter:

DVB, ATSC, ISDB



Bei analoger Übertragung im Basisband kommt jetzt noch als Bildparameter dazu:

Für die digitale Übertragung des Bildes wird MPEG-2 verwendet (selten MPEG-1), für Ton kommt sowohl MPEG-1 und MPEG-2 Audio wie auch Digital Dolby zum Einsatz, wobei MPEG Audio obligatorisch ist. Üblicherweise gibt es aber Einschränkungen:

Folgende Parameter sind dann die eigentlichen Bildparameter:

Oft wird auch im digitalen Bereich „PAL“ als Abkürzung für „625 Zeilen/25 Bildwechsel pro Sekunde mit PAL-Farbträger“ und „NTSC“ als Abkürzung für „525 Zeilen/30 Bildwechsel pro Sekunde mit NTSC-Farbträger“ verwendet. Dies ist jedoch falsch, weil im Digitalen fast ausschließlich Komponentensignale zum Einsatz kommen, und die genaue Angabe der TV-Norm eines Landes Schwarz-weiß-Norm + Farbnorm ist. So verwenden z. B. die USA die Norm M mit NTSC, der größte Teil Westeuropas Norm B/G mit PAL, die frühere DDR Norm B/G mit SECAM, der größte Teil Osteuropas Norm D/K mit SECAM oder PAL, Frankreich Norm L mit SECAM. Viele ehemalige SECAM-Länder sind inzwischen zu PAL migriert, behalten jedoch in der Regel die unterliegende Schwarz-weiß-Norm bei.

Weiterhin gibt es beim analogen Fernsehen unterschiedliche Normen für



Die digitalen Fernsehnormen orientieren sich an einigen Kenndaten der analogen Fernsehnormen, wie Zeilenzahl und Bildfrequenz. Zur Komprimierung der Daten wird in der Regel der MPEG-2-Standard verwendet.

Im Unterschied zu analogen Fernsehnormen existiert bei digitalen Fernsehnormen als weiteres Merkmal die Anzahl Spalten eines Bildes. Zusammen mit der Anzahl Bildzeilen erhält man so die so genannte Bildauflösung in Bildpunkten (Pixel). Die Spaltenzahl wird dabei idealerweise so gewählt, dass sich bei der Bilddarstellung die Höhe der einzelnen Pixel nicht wesentlich von ihrer Breite unterscheidet. Statt des traditionellen Bildverhältnisses von 4:3 wird wegen der veränderten Anzeigegeräte (Flachbildfernseher und Videoprojektoren) verstärkt eingesetzt.





</doc>
<doc id="12064" url="https://de.wikipedia.org/wiki?curid=12064" title="Mittelerde">
Mittelerde

Mittelerde (engl. Orig.: "Middle-earth") steht für:



</doc>
<doc id="12066" url="https://de.wikipedia.org/wiki?curid=12066" title="FreeBSD">
FreeBSD

FreeBSD ist ein freies und vollständiges unixoides Betriebssystem, das direkt von der Berkeley Software Distribution abstammt. Mit einer Community aus fast 390 dauerhaft aktiven, offiziellen und Tausenden mitwirkenden Entwicklern gehört FreeBSD zu den größten Open-Source-Projekten. Obwohl der Fokus der Entwickler auf der Erstellung einer stabilen Software-Plattform für Server und Appliances liegt, wird es auch auf Desktop-Computern verwendet. FreeBSD kommt hauptsächlich bei Internetdienstanbietern wie zum Beispiel Yahoo und Strato, für Hochlastanwendungen wie Netflix und WhatsApp, in Internet-Backbone-Systemen wie Hochleistungsroutern und Namensdiensten sowie als Webhosting-Plattform zum Einsatz. Dort belegt es regelmäßig die vordersten Plätze in der Liste der zuverlässigsten Systeme.

Mehrere Standards im Bereich der Rechnernetze, wie beispielsweise IPv6, wurden von FreeBSD zuerst umgesetzt und verbreiteten sich von dort aufgrund der freizügigen BSD-Lizenz auch auf andere Systeme, unter anderem nach OpenBSD und Linux. Außerdem ist FreeBSD teilweise die Grundlage für Darwin, die Open-Source-Plattform von Apple, auf der macOS basiert.

Ursprünglich als inoffizielle Sammlung von Patches zur Fehlerbereinigung von 386BSD gedacht, gründeten Nate Williams, Rod Grimes und Jordan K. Hubbard 1993 ein eigenständiges Projekt, nachdem die Pflege des Patchkit-Mechanismus zu aufwändig wurde. Das ursprünglich als "386BSD-Interim" bezeichnete Projekt musste eingestellt werden, als der Autor von 386BSD, Bill Jolitz, seine Mitarbeit an dem Projekt und auch 386BSD selbst aufgab. Als man einen neuen Namen suchte, wurde FreeBSD von Bill Greenman vorgeschlagen, einem Mitarbeiter des Unternehmens "Walnut Creek CDROM", bei der die Software des Projekts auf Datenträgern und mittels FTP vertrieben wurde.

Genau wie bei dem kurz zuvor von anderen Autoren des Patchkits gegründeten NetBSD entschloss man sich gleich zu Beginn des Projekts im Juli 1993 dazu, die Software zentral mithilfe eines CVS-Archivs zu entwickeln. Man griff dafür bei den ersten Versionen von FreeBSD auf die Quellen von 4.3BSD-Lite (Net/2) zurück und auf 386BSD, welches die Berkeley Software Distribution auf die Intel-80386-Prozessorplattform portierte. Als jedoch die Unix System Laboratories begannen, die Anbieter von BSD-Software aufgrund von Lizenzverletzungen zu verklagen, änderte sich die Basis von FreeBSD auf 4.4BSD-Lite, welches die UC Berkeley 1994 veröffentlichte, nachdem der Rechtsstreit eingestellt worden war. Diese Version enthielt keinerlei Quellcode aus den Unix Laboratories mehr. Da hierdurch auch Teile des Betriebssystems fehlten, die zum Booten notwendig waren, dauerte es für die Herstellung einer funktionsfähigen Softwaredistribution, die als FreeBSD 2.0 veröffentlicht werden konnte, bis zum November 1994. Ältere Versionen dürfen aus rechtlichen Gründen nicht mehr vertrieben werden.

Seither wurde das Betriebssystem auf diverse andere Plattformen portiert und hat sowohl an Funktionen als auch an nativ unterstützter Software von Drittanbietern zugenommen. Aufgrund seiner Projektgröße und Verbreitung, ohne dabei öffentlich in Erscheinung getreten zu sein, wird FreeBSD auch inoffiziell als "unbekannter Riese unter den freien Betriebssystemen" bezeichnet. Neben diversen Derivaten haben sich aus dem Projekt heraus mehrere Organisationen gegründet, die sich die Förderung von FreeBSD und der BSD-Familie zum Ziel gesetzt haben. Hierzu gehören z. B. die FreeBSD Foundation und die "BSD Certification Group".

Das vollständige System umfasst folgende Komponenten:
FreeBSD ist als unixartiges System weitgehend POSIX-konform. Es unterstützt alle grundlegenden Funktionen des POSIX.1-Standards, jedoch nicht alle Erweiterungen des "X/Open System Interfaces". Aus diesem Grund darf es auch nicht den rechtlich geschützten Namen UNIX tragen. An einer Herstellung der vollständigen Unterstützung der Schnittstellen von C99, POSIX und des XSI wird aber gearbeitet.
Neben der x86-, AMD64- und ehemals in Japan verbreiteten PC-98-Architektur werden eine Reihe weiterer Prozessortypen unterstützt. Dazu gehören IA-64, SPARC und PowerPC sowie die ARM- und experimentell auch die MIPS-Architekturen für eingebettete Systeme. Da FreeBSD eine eigene Binärschnittstelle (ABI) anbietet, ist die Installation proprietärer Software problemlos möglich. Ebenso können Windows-Netzwerkgerätetreiber, deren Hersteller die Hardware-Spezifikationen nicht freigegeben haben, über die NDIS-Schnittstelle verwendet werden, z. B. Intels Centrino.

FreeBSD hat einige besondere Funktionen in Bezug auf die Speicherung von Daten.

Das UFS-Dateisystem, das häufig von den BSD-Betriebssystemen genutzt wird, verfügt über Metadaten-Journaling und Softupdates, die die Konsistenz des Dateisystems im Falle eines Systemabsturzes sicherstellen. Schnappschüsse von Dateisystemen können in kürzester Zeit effizient erzeugt werden. Sie ermöglichen unter anderem zuverlässige Backups von laufenden Dateisystemen. Des Weiteren verfügt FreeBSD über GEOM, ein modulares Framework, welches RAID, Verschlüsselung ganzer Festplatten, Journaling, Caching und Zugriff auf netzwerk-basierten Speicherplatz zur Verfügung stellt. Mit Hilfe von GEOM können komplexe Speicherlösungen aufgebaut werden, die mehrere dieser Mechanismen gleichzeitig nutzen.

Ein weiteres häufig eingesetztes Dateisystem unter FreeBSD ist das von Sun entwickelte ZFS. Dieses wurde zunächst aus freien Quellen, die mit OpenSolaris veröffentlicht wurden, auf FreeBSD portiert und wird seit FreeBSD 8.0 als stabil eingestuft. Inzwischen werden im Rahmen von OpenZFS Verbesserungen des Dateisystems entwickelt, die dann zum Teil für FreeBSD übernommen werden. ZFS wird vor allem wegen seiner einfachen Bedienung, Flexibilität und Stabilität geschätzt. Seit FreeBSD 10.0 ist es möglich, ausschließlich über den System-Installer ein komplett auf ZFS basierendes System zu installieren. Zuvor musste dies manuell erfolgen.

Neben den Programmen aus dem Basissystem stehen über 26.000 Softwarepakete von Drittanbietern zur Verfügung. Über sie kann teilweise bereits während der Installation je nach Einsatzzweck eine Vorauswahl getroffen werden. Die meisten Pakete stehen auch als Binärdateien ("package") zur Verfügung und können somit direkt vom Installationsmedium oder einem regionalen Projektrepository installiert werden.

Häufiger kommen jedoch die sogenannten "Ports" als Paketverwaltung zum Einsatz. Neben einer größeren Auswahl liegen die Vorteile der FreeBSD-Ports in der komfortablen Abhängigkeitsauflösung, einer möglichen prozessorspezifischen Optimierung und der Nutzung von Compilezeit-Optionen. Die meisten Ports werden als Quellcode geladen, meist aus dem Netz, und dann auf dem eigenen System kompiliert. Hier finden sich die meisten Open-Source-Programme für Server und Desktop sowie einige proprietäre Programme. Die Flexibilität und einfache Bedienung der FreeBSD-Ports haben zu einer Verbreitung dieser Paketverwaltung auch auf anderen Systemen geführt. Neben NetBSD, DragonFly BSD (als "DPorts") und OpenBSD werden die Ports auch auf Darwin und Mac OS X/​OS X/​macOS unter dem Namen "MacPorts" eingesetzt. Aufgrund der weitgehenden Quellcode-Kompatibilität dieser Systeme sind dafür höchstens kleine Änderungen an den Metadaten der Ports notwendig.

Aufgrund der Ausrichtung des Projekts liegen die Stärken von FreeBSD im Netzwerkbereich. So waren wegen des KAME-Projekts die BSDs unter den ersten Betriebssystemen mit IPv6- und IPsec-Unterstützung. Es stehen mehrere redundante Paketfilter-Implementierungen zur Verfügung: das auch für proprietäre Unices verfügbare IPFilter, die Eigenentwicklung ipfw und pf aus OpenBSD. Des Weiteren existiert mit "dummynet" ein leistungsfähiger Trafficshaper.

FreeBSD beherrscht Netzwerkprotokolle auf verschiedenen Ebenen, zum Beispiel 802.1q VLANs, PPP, L2TP. Mit netgraph gibt es außerdem ein graphenbasiertes Kernelsubsystem, welches die modulare Implementierung neuer Protokolle und flexible Kombination vorhandener unterstützt. Ebenso arbeitet FreeBSD mit verschiedener Netzwerkhardware zusammen, u. a. 10-Gigabit-Ethernet, WLAN, ATM, ISDN, FDDI und UMTS.

FreeBSD stellt ab Version 4.x die Virtualisierungsumgebung "jails" (dt. "Gefängnisse") zur Verfügung. Aus dem Inneren eines Jail ist kein Zugriff auf die System- und Konfigurationsdateien des Hauptsystems möglich – es verwendet seine eigenen Dateien, Prozesse und auch User-Accounts, so dass die Umgebung sich fast nicht von der eines vollwertigen Systems unterscheidet. Gegenüber etwa Chroot bietet dies eine Reihe von Vorteilen, insbesondere im Hinblick auf Sicherheit, Administration und Performance. Im Vergleich zu Paketen wie Xen oder VMware wiederum ist die Umsetzung sehr viel einfacher und transparenter, bietet aber auch nicht die Möglichkeit, ein fremdes Betriebssystem zu betreiben.
In anderen Systemen sind Abwandlungen von FreeBSD-Jails als "sysjails" oder "container" bekannt.

Die Open-Source-Edition der Virtualisierungssoftware VirtualBox ist in den Ports enthalten und ermöglicht es, FreeBSD sowohl als Gast-Betriebssystem als auch als Virtualisierungshost, der virtuelle Maschinen beherbergt, zu betreiben.

Mit FreeBSD 10.0 hielt der extra für FreeBSD entwickelte Typ‑2-Hypervisor "bhyve" Einzug in das Betriebssystem. Er unterstützt zurzeit diverse FreeBSD-Versionen, Open-/NetBSD, Linux und Windows als Gast-Betriebssysteme.

FreeBSD ist als Xen-Gastsystem inklusive des paravirtualisierten PVH-Modus einsetzbar, ebenso hat FreeBSD mittlerweile Unterstützung als Hostsystem.

Die FreeBSD-Entwickler sind sehr darauf bedacht, neue Versionen des Betriebssystems abwärtskompatibel zu halten. Das bedeutet, dass Programme, die unter einer älteren Version des Betriebssystems ausgeführt werden konnten, auch unter der neueren Version noch ausgeführt werden können. Dieser Anspruch wird für alle Teile des Betriebssystems erhoben (insbesondere auch Kernel-Module). Der Benutzer wird auf Ausnahmen von dieser Regel bei Bedarf hingewiesen.

FreeBSD bietet Linux-Binärkompatibilität. Das bedeutet, dass Programme, die für GNU/Linux geschrieben und kompiliert wurden, unter FreeBSD ausgeführt werden können. Dies bietet vor allem die Möglichkeit, Programme, die nur in kompilierter Form für Linux erhältlich sind (wie z. B. Adobe Reader, Adobe Flash Player, Skype) unter FreeBSD zu nutzen. Die Linux-Binärkompatibilität wird häufig "Linux Emulator" oder "linuxulator" genannt. Technisch gesehen nutzt dieses Feature allerdings keine Emulation, sondern es beruht auf der Implementierung einer ABI.

Mithilfe der Laufzeitumgebung Wine, die in der Ports-Collection sowie als Binärpaket zur Verfügung steht, können unter FreeBSD eine Reihe von Windows-Anwendungen ausgeführt werden, z. B. Microsoft Office.

Die Entwicklungszweige des Quelltextes werden durch zentrale Subversion/CVS-Archive zur Verfügung gestellt. Das System ist sehr homogen, da alle Systembestandteile von der gleichen Entwicklergruppe gepflegt werden. Diese Archive werden regional gespiegelt, um das Netzwerk des Hauptarchivs zu entlasten. Aus Gründen der Performance und um Inkonsistenzen zu vermeiden sind die Archive mit direktem Schreibzugriff getrennt von denen, die gelesen werden können.

Bei FreeBSD spielt die Verfügbarkeit des Quelltexts auch praktisch eine große Rolle: Viele Anwender halten einen lokalen Quelltextbaum vor und synchronisieren ihn regelmäßig über das Netz mit einem zentralen Quelltextarchiv (Repository). Dadurch ist es möglich, ein lokales System genau für seinen Einsatzzweck anzupassen, indem z. B. der Kernel mit systemspezifischen Treibern neu gebaut oder Software des Basissystems bzw. aus den Ports angepasst werden kann. Im Repository kann man in den alten Versionen der Quelltexte und den Anmerkungen der Entwickler recherchieren und somit sehr gut Einsicht in den Aufbau und die Funktionsweise des Systems gewinnen.

Die Pflege des Quelltextes wird von drei Gruppen der FreeBSD-Entwicklergemeinde übernommen:
Zusätzlich zu diesen drei Gruppen gibt es noch Teams, die für das Releasemanagement, die Administration der Projektserver, das Einpflegen von Sicherheitsupdates, etc. zuständig sind. Dabei gibt es personelle Überlappungen zwischen all diesen Gruppen.

Koordiniert wird die Entwicklung vom "Core Team", das alle zwei Jahre von den aktiven Entwicklern mit CVS-Zugriff gewählt wird.

Die Kommunikation zwischen Entwicklern untereinander und mit Benutzern findet wie bei vielen Opensource-Projekten hauptsächlich über das Internet statt (Mailinglisten, Newsgroups, IRC, Foren).

Jedes Jahr werden weltweit mehrere Konferenzen abgehalten, sogenannte "BSDcons". Entwickler und interessierte Nutzer stellen BSD-bezogene Projekte vor, diskutieren sie und erlernen in Workshops den Umgang mit ihnen. Im Rahmen der Konferenzen finden gesonderte Entwicklertreffen statt (sog. "DevSummit"s), auf denen zukünftige Entwicklungen des Projektes diskutiert und koordiniert werden. Häufig finden die Konferenzen in universitärem Umfeld statt. Meist werden sie von der FreeBSD Foundation finanziell und logistisch unterstützt. So verwendet die FreeBSD Foundation einen Teil ihres Budgets, um Entwicklern die Teilnahme an Konferenzen zu finanzieren. Namhafte Firmen (darunter Google, Netflix, iXsystems), die in die Entwicklung von FreeBSD involviert sind oder FreeBSD in größerem Umfang nutzen, treten auch als Sponsoren von Konferenzen auf. Die Konferenzen werden geschätzt, da sie die Möglichkeit des Austausches zwischen Programmierern, System-Administratoren, Studenten, Professoren und IT-Firmen bieten. Immer häufiger werden Teile der Konferenzen live gestreamt oder es sind Mitschnitte von Vorträgen oder andere Konferenzmaterialien (Präsentationen, Handouts) online über die jeweilige Webseite zur Konferenz abrufbar.

Folgende Konferenzen finden regelmäßig statt:

Zusätzlich zu den Konferenzen finden unregelmäßige Zusammentreffen statt, sogenannte "BSDDays". Sie dienen demselben Zweck wie die Konferenzen, sind aus Mangel an Zeit oder Finanzen aber nicht so umfangreich.

Des Weiteren gibt es noch in vielen Ländern sogenannte "User Groups". Dies sind Gruppen, die aus Benutzern von FreeBSD oder solchen, die es werden wollen, bestehen. Sie dienen primär dem persönlichen Erfahrungsaustausch. Wann, wo und wie sich Mitglieder solcher Gruppen treffen, ist gruppenspezifisch und wird häufig auf den Webseiten dieser Gruppen öffentlich bekannt gegeben.

Die Entwicklung des FreeBSD-Kernels und -Userlands läuft in mehreren Zweigen parallel ab:
Um zum Beispiel sicherheitsrelevante Änderungen zwischen den einzelnen Zweigen austauschen zu können oder Verbesserungen aus dem Stable-Zweig auch in Current einfließen lassen zu können, wird im Hintergrund auf eine Versionsverwaltung mit Perforce zurückgegriffen, da sich mit CVS keine zweigübergreifenden Änderungen durchführen lassen.
Der Wartungszeitraum beträgt je nach Release und Priorität zwischen 6 und 24 Monaten.

Folgende Zeitleiste stellt den Lebenszyklus unterschiedlicher FreeBSD-Versionen dar. Bis zum End of life einer Version werden Sicherheitslücken und Softwarefehler beseitigt und ein Teil essentieller neuer Features, die in künftigen Versionen vorhanden sind, zurückportiert.

Der "BSD-Daemon" ist das „Maskottchen“ von BSD-Unix und wurde von Beginn an als Logo verwendet. Er wird oft "Beastie" genannt, obwohl ihn der Träger des Copyrights für namenlos erklärt.

Das Zeichen erwies sich jedoch teils schwer reproduzier- und skalierbar, daher riefen im Februar 2005 die Entwickler des FreeBSD-Projektes auf, ein Logo für FreeBSD zu entwerfen. Ende Juni 2005 wurden die Vorschläge verschiedener Grafiker eingereicht und Ende September 2005 wurde das neue Logo ausgewählt. Zum Schluss standen sieben Einsendungen in der engeren Wahl – gewonnen hat das Design von "Anton K. Gural", wie oben rechts in diesem Artikel verwendet.

Es gab darauf – wie bei NetBSD – verschiedene Erwägungen, das traditionelle Maskottchen nicht weiter als Logo zu verwenden. Im Ergebnis bleibt der Daemon "Beastie" nun doch das Maskottchen des Projektes. Das neue Logo ist eine Anspielung auf den Kopf des Daemons mit seinen Hörnern.

Man unterscheidet grob zwischen Distributionen und Derivaten: Bei Distributionen handelt es sich um andere Zusammenstellungen von Software, während es sich bei Derivaten um Veränderungen und Abspaltungen (auch "Forks") des Systems handelt. Da die Erstellung von BSD-Distributionen im Vergleich zu anderen Systemen, wie GNU/Linux, noch eine relativ neue Entwicklung ist und Änderungen meist tiefgreifender als bei Linuxdistributionen sind, werden in der BSD-Welt häufig auch Distributionen als "Derivate" bezeichnet. In der FreeBSD-Community werden Forks häufig negativ bewertet.




Die Hardwareanforderungen hängen vom Einsatzgebiet ab. Ein Embedded System beispielsweise hat andere Anforderungen als ein Webserver oder ein Desktop.

Für FreeBSD 11.x gelten 96 MB RAM und 1,5 GB Festplattenspeicher als Minimalanforderung. Bei Desktop-Systemen beginnen die Anforderungen bei 2-4 GB RAM und mindestens 8 GB freiem Festplattenspeicherplatz.





</doc>
<doc id="12067" url="https://de.wikipedia.org/wiki?curid=12067" title="Erneuerbare-Energien-Gesetz">
Erneuerbare-Energien-Gesetz

Das deutsche Gesetz für den Ausbau erneuerbarer Energien (Kurztitel Erneuerbare-Energien-Gesetz, EEG 2017) regelt die bevorzugte Einspeisung von Strom aus erneuerbaren Quellen ins Stromnetz und garantiert deren Erzeugern feste Einspeisevergütungen. Während sich das EEG in Bezug auf den Ausbau der erneuerbaren Energien als „ausgesprochen erfolgreich“ erwies, werden dessen ökonomische und ökologische Effizienz sowie Teilaspekte wie Ausnahmeregelungen für die Industrie kontrovers diskutiert.

Es soll gemäß Legaldefinition ( Abs. 1 EEG) "im Interesse des Klima- und Umweltschutzes"

Der Anteil erneuerbarer Energien an der Stromversorgung soll bis zum Jahr 2025 auf 40 bis 45 % und bis 2035 auf 55 bis 60 % erhöht werden ( Abs. 2 EEG 2014). Während damit ein fester Korridor für den Ausbau der erneuerbaren Energien mit festen Grenzen nach oben und unten festgelegt wurde, gab es in allen vorherigen Versionen des EEG Mindestziele, so dass es keine Obergrenze gab ( Abs. 2 EEG). Nach der Einführung des EEG im Jahr 2000 ist das Gesetz 2004 und 2009 durch Neufassungen angepasst worden. 2011 wurden umfassende Novellierungen beschlossen, die überwiegend 2012 in Kraft getreten sind, so dass die aktuelle Gesetzesfassung kurz als „EEG 2012“ bezeichnet wird.

Zwei Grundzüge sind zum Erreichen der Ziele gesetzlich verankert:

Der zu einer EEG-Anlage nächstgelegene Stromnetzbetreiber ist zu deren Anschluss und zur vorrangigen Einleitung des erzeugten Stromes verpflichtet ( Abs. 1 EEG). Die Zahlung der festgelegten gleitenden Marktprämie ist im gleichen Paragrafen grundsätzlich als gesetzliches Schuldverhältnis verankert und darf nicht vom Abschluss eines separaten Vertrages zwischen dem Anlagenbetreiber und dem Netzbetreiber abhängig gemacht werden ("Koppelungsverbot" nach Abs. 1 EEG). Von den Bestimmungen des EEG darf nicht abgewichen werden, soweit nicht ausdrücklich dort vorgesehen (ebenfalls § 4 EEG).

Die Vergütungssätze sind mit Laufzeiten von 20 Jahren nach Technologien und Standorten differenziert und sollen einen wirtschaftlichen Betrieb der Anlagen ermöglichen (Grundsatz , Vergütungssätze für die jeweiligen Technologien §§ 26 bis 31 EEG 2014). Der festgelegte Satz sinkt jährlich (im Falle der Photovoltaik monatlich, im Falle der Windenergie an Land vierteljährlich) um einen bestimmten Prozentsatz, so dass durch diese stetige Degression die zu erwartende Kostensenkung bereits im Gesetz berücksichtigt wird und für später errichtete Anlagen ein Kostendruck als Verbesserungsanreiz geweckt wird: Anlagen sollen effizienter und kostengünstiger hergestellt werden, um langfristig ohne Hilfen am Markt bestehen zu können (Höhe und Entwicklung s. Vergütungssätze).

Nach EEG wird die Erzeugung von Strom aus folgenden erneuerbaren Energien gefördert ( Nr. 3 EEG):

Außerdem wird die Stromerzeugung aus Grubengas gefördert, das eine fossile Energiequelle darstellt.

Die Regelungen des EEG ziehen Ausgleichsbedarf auf zwei Ebenen nach sich:

Die Stromerzeugung aus erneuerbaren Energien ist regional und saisonal unterschiedlich, so dass zwischen den Netzbetreibern eine bundesweite Ausgleichsregelung erforderlich ist (§§ 34 bis 36 EEG). Diese sogenannte „bundesweite Wälzung“ wurde bis 2010 als physikalische Durchleitung des EEG-Stroms über die Verteilnetzbetreiber zu den vorgelagerten Übertragungsnetzbetreibern und von dort weiter zu den Endverbrauchern betrieben, so dass sich ein fünfstufiges Wälzungssystem ergab.

Mit der Ausgleichsmechanismusverordnung (AusglMechV) vom Mai 2009 ist das Verfahren grundlegend geändert worden: EEG-Strom wird zu den Übertragungsnetzbetreibern durchgeleitet, von ihnen gemäß Vorgaben vergütet und an der Strombörse im Spotmarkt vermarktet. Die Möglichkeiten zur Direktvermarktung des EEG-Stroms sind im EEG 2012 mit dem Marktprämienmodell erweitert worden.

Der weitere Ausgleichsbedarf ergibt sich für die Kosten, die als Differenz zwischen den Erlösen für den EEG-Strom und den festgelegten Vergütungssätzen entstehen ( EEG). Dieser Betrag wird als EEG-Umlage bezeichnet und von den Endverbrauchern gezahlt. Für die Ermittlung der EEG-Umlage sind seit 2010 nach Einführung der Ausgleichsmechanismusverordnung (AusglMechV) die vier bundesweiten Übertragungsnetzbetreiber zuständig.
Die Höhe der einzelnen Beträge wird jährlich zum 15. Oktober in einer Prognose für das folgende Jahr ermittelt und im laufenden Geschäftsbetrieb durch Kontoabgleich Ende September ausgeglichen.

Das EEG sieht für Unternehmen des produzierenden Gewerbes mit hohem Stromverbrauch sowie für Schienenbahnen Ausnahmeregelungen von der EEG-Umlage vor, um die Stromkosten dieser Unternehmen zu senken und so ihre internationale und intermodale Wettbewerbsfähigkeit zu erhalten, „soweit hierdurch die Ziele des Gesetzes nicht gefährdet werden und die Begrenzung mit den Interessen der Gesamtheit der Stromverbraucherinnen und Stromverbraucher vereinbar ist“ ( EEG mit Einzelregelungen §§ 41 bis 44). Mit dem EEG 2012 ist die Grenze für Ausnahmeerteilungen auf 1 GWh/a Jahresstromverbrauch (vorher 10 GWh/a) deutlich gesenkt worden (Einzelheiten siehe Sonderregelungen für stromintensive Unternehmen).

Im EEG sind ferner Mitteilungs- und Veröffentlichungspflichten geregelt ( EEG mit Einzelregelungen §§ 45 bis 52) sowie Herkunftsnachweis, Doppelvermarktungsverbot, Rechtsschutz und behördliche Verfahren (§§ 55 bis 63). Das Gesetz endet mit umfangreichen Verordnungsermächtigungen im § 64 mit 8 zugehörigen (Buchstaben)-Paragrafen, der Vorgabe zum Erfahrungsbericht (§ 65) sowie umfangreichen Übergangsbestimmungen (§ 66 EEG).

Bei der Erarbeitung des Gesetzes in der ersten Fassung von 2000 waren die Abgeordneten Michaele Hustedt, Hans-Josef Fell (beide Bündnis 90/Die Grünen), Hermann Scheer und Dietmar Schütz (beide SPD) maßgeblich beteiligt. Im "Renewables 2013 Global Status Report" kommt die international besetzte Forschungsgruppe REN21 zu dem Ergebnis, dass es in 127 Ländern der Welt Instrumente zur Förderung der Stromerzeugung aus erneuerbaren Energie gibt, darunter als gängigstes Instrument die Einspeisevergütung, die in 71 Ländern und 28 Bundesstaaten praktiziert wird.

Seit Einführung des EEG im Jahr 2000 ist die Strommenge, die nach EEG vergütet wird, um das Zehnfache auf 103.000 GWh/a im Jahr 2011 gestiegen – dem aktuellen bilanzierten Jahr (Stand Dez. 2012). Damit wurden circa 5/6 der in diesem Jahr aus regenerativen Quellen erzeugten elektrischen Energie (123.186 GWh) nach EEG vergütet. Der größte Anteil mit 46 % stammt aus Windkraft (onshore), gefolgt von Biomasse mit 31 % und Photovoltaik mit rd. 15 %. Für Vergütungszahlungen sind 2011 für Strom aus Windkraft (onshore) 3,3 Mrd. Euro, für Strom aus Biomasse 4,3 Mrd. Euro und für Photovoltaik-Strom 5,1 Mrd. Euro an die Anlagenbetreiber geflossen. Die durchschnittliche Vergütung für EEG-Strom lag 2011 bei 16,3 Cent/kWh (siehe Abschnitt Zahlen zum EEG). Von diesen Aufwendungen sind die Einnahmen aus der Vermarktung des EEG-Stroms in Höhe von 4,4 Mrd. Euro für 2011 abzuziehen, entsprechend 4,3 Cent/kWh.

Vorläufer des Erneuerbare-Energien-Gesetzes war das seit 1991 geltende "Gesetz über die Einspeisung von Strom aus erneuerbaren Energien in das öffentliche Netz" – kurz Stromeinspeisungsgesetz – vom 7. Dezember 1990, das als weltweit erstes Ökostrom-Einspeisegesetz gilt. Die "Einspeisung" in das öffentliche Netz wurde hiermit verbindlich geregelt, weil Strom aus erneuerbaren Energien – mit Ausnahme von Strom aus Wasserkraft – nur von kleinen Unternehmen erzeugt wurde, denen große Stromerzeuger den Zugang zu ihrem Verteilernetz oftmals verweigerten oder stark erschwerten. Das Gesetz verpflichtete die Netzbetreiber zur Abnahme des Stroms und sicherte den Erzeugern Mindestvergütungen zu, die als Anteil vom Durchschnittserlös für Strom berechnet wurden, wie er zwei Jahre zuvor erzielt wurde. Nicht nach diesem Gesetz wurde Strom vergütet, der aus Wasserkraft, Deponie- oder Klärgas mit einem Generator von mehr als fünf Megawatt gewonnen wurde.

Nach Einführung des Gesetzes 1991 betrug diese Vergütung für Wasserkraft sowie Klär-, Deponie- und Biogas 75 % (ab 1994 80 %) und für Strom aus Sonnenenergie und Windkraft 90 % des zwei Jahre zuvor erzielten Durchschnittserlöses. Diese Regelungen und Vergütungen führten zu einer Verbesserung gegenüber dem vorherigen Zustand, waren für bestehende Anlagen, meist Wasserkraftanlagen, vorteilhaft und brachten Windkraftanlagen an besonders günstigen, küstennahen Standorten in die Nähe der Wirtschaftlichkeit, was zu einem ersten kleinen „Windkraft-Boom“ in diesen Regionen führte. Für Solarstromanlagen waren die Vergütungen noch weit von einer Kostendeckung entfernt. Ein Schritt in Richtung "kostendeckende Vergütung" stellte das Aachener Modell dar.

Für das Jahr 2000 – also kurz vor Einführung des EEG – lag die Vergütung für Strom aus Wind und Photovoltaik bei umgerechnet 8,23 ct/kWh, für Strom aus Wasserkraft, Klär-, Deponie- und Biogas bei umgerechnet 7,23 ct/kWh und für Strom aus sonstigen erneuerbaren Energien bei umgerechnet 5,95 ct/kWh. Bezugswert war die durchschnittliche Vergütung für Strom im Jahr 1998 – umgerechnet 9,15 ct/kWh.

Das Stromeinspeisungsgesetz wurde durch das Erneuerbare-Energien-Gesetz vom 29. März 2000 ersetzt. Zentrale Neuerungen gegenüber dem Stromeinspeisungsgesetz waren die Einführung des Vorrangprinzips und eines bundesweiten Wälzungsmechanismus sowie die Anhebung der Grenze für Wasserkraft, Deponie- und Klärgas von 5 MW auf 20 MW. Die Vergütungssätze wurden entsprechend dem Grundsatz der kostendeckenden Vergütung stärker differenziert, die Vergütungssätze für Photovoltaik stark angehoben und Technologien wie geothermisch erzeugte Energie einbezogen, um eine Anschubförderung zu geben.

Im EEG 2000 war eine Degression von jährlich 5 % für die Vergütungssätze für Strom aus solarer Strahlungsenergie vorgegeben. Zusätzlich wurde eine Leistungsgrenze von 350 MW eingeführt, nach deren Überschreiten im Folgejahr die Vergütung für neue Photovoltaikanlagen entfallen sollte. Dieser Wert ergab sich aus dem Anfangsbestand von 50 MW und aus den 300 MW, die durch das 100.000-Dächer-Programm für Solarstrom gefördert wurden. Die 350-MW-Grenze wurde 2003 überschritten, so dass ab 2004 keine Vergütung für Neuanlagen gezahlt worden wäre und ein massiver Einbruch im Photovoltaik-Markt drohte. Daher wurde eine EEG-Novelle bereits am 22. Dezember 2003 verabschiedet.

Die Vergütungssätze des EEG 2000 im Überblick:

Degressionssätze:
Seit dem 1. Januar 2002 wurden die Vergütungssätze für neu in Betrieb gehende Anlagen gesenkt:

Eine weitere novellierte Fassung des EEG vom 21. Juli 2004 trat am 1. August 2004 in Kraft. Vorausgegangen war eine Einigung im Vermittlungsausschuss des Deutschen Bundestages, bei der die CDU/CSU eine Reduzierung der Förderung von Windkraftanlagen erreichte. Neben der erforderlich gewordenen Anpassung an die von der EU erlassenen Richtlinie 2001/77/EG zur Förderung der Stromerzeugung aus erneuerbaren Energiequellen im Elektrizitätsbinnenmarkt bestanden die wesentlichen Änderungen in der Höhe der Fördersätze sowie der besseren juristischen Stellung der Betreiber von Anlagen zur Erzeugung erneuerbarer Energien gegenüber den örtlichen Netzbetreibern (etwa keine Pflicht zum Abschluss von gesonderten Einspeiseverträgen).

Die Novellierung 2008 hatte das Ziel, den Anteil erneuerbarer Energien an der Stromversorgung bis 2020 auf einen Anteil von mindestens 30 % zu erhöhen ( Abs. 2 EEG). In Ergänzung zum EEG, das sich nur auf die Stromerzeugung bezieht, wurde erstmals bundesweit in einem weiteren Gesetz zur Förderung Erneuerbarer Energien im Wärmebereich (EEWärmeG 2008) die Verwendung von erneuerbaren Energien im Bereich der Wärme- und Kälteerzeugung geregelt, mit dem die Erhöhung des Anteils erneuerbarer Energien für die Wärmeerzeugung auf 14 % bis 2020 bezweckt wird.

Das EEG 2009 behielt die Grundstrukturen des EEG 2004 zwar bei, führte aber zu einer vollkommenen Neunummerierung der Paragrafen, deren Anzahl von 22 auf nunmehr 66 anwuchs. Die Neufassung des Gesetzes gilt für Neu- und für zum Zeitpunkt seines Inkrafttretens vorhandene Altanlagen, für die umfassende Übergangsbestimmungen im Bestandsschutz gewähren und die bisherigen Bedingungen für die Abnahme und Vergütung aufrechterhalten. Die Neufassung enthält eine Vielzahl von Detailregelungen. So wurden zum Zwecke der Verbesserung der Transparenz die Meldepflichten erweitert. Betreiber von Solaranlagen müssen Standort und Leistung der Anlage an die Bundesnetzagentur melden ( Abs. 2 Satz 2 EEG). Der Anlagenbegriff wurde neu und eindeutig – auch für Altanlagen – definiert, um die Aufteilung in mehrere Kleinanlagen zu unterbinden, mit der höhere Vergütungssätze erzielt werden sollten. Mit der Neufassung der und EEG werden Anlagen, die in enger zeitlicher Folge von zwölf aufeinander folgenden Monaten und in lokaler Nähe auf demselben Grundstück oder in unmittelbarer Nähe in Betrieb genommen wurden, bei der Vergütung wie eine einzige Anlage gewertet.

Eine neuartige Ausgleichsregelung wurde für Engpässe bei der Stromeinleitung in die übergeordneten Stromnetze eingeführt. Diese Maßnahmen zum Netzmanagement sollten den weiteren Ausbau der erneuerbaren Energien ermöglichen, ohne auf den Netzausbau an Engpassstellen warten zu müssen. Herzstück ist die direkte Zugriffsmöglichkeit der Netzbetreiber auf die Steuerung der einspeisenden Erzeugungsanlagen, mit dem sie die Leistung gezielt absenken können.(, EEG) Die Netzbetreiber sind zur Information der Betreiber und zum Nachweis des Umfangs und der Dauer der Maßnahme verpflichtet. Derartige Eingriffe setzen einen tatsächlichen Engpass im Stromnetz nach Absenkung der Leistung konventioneller Kraftwerke voraus. Der Anlagenbetreiber hat das Recht auf Entschädigung, muss diese Forderung aber gegenüber dem Netzbetreiber nachweisen und durchsetzen. Alle bestehenden Anlagen mit einer Leistung ab 100 kW waren auf Kosten der Betreiber bis Ende 2010 mit technischen Einrichtungen zur Ferneinwirkung (Rundfunksteuerempfänger) und zur laufenden Erfassung der eingeleiteten Strommenge nachzurüsten.

Außerdem wurde für die Photovoltaik-Vergütung eine gleitende Degression eingeführt. Bei großem Zubau und damit höheren Vergütungskosten wird die garantierte Vergütung pro kWh im Folgejahr schneller abgesenkt, um die Gesamtkosten für alle Stromkunden in Grenzen zu halten. Wird das vorgegebene Zubau-Ziel nicht erreicht, wird die Absenkung verlangsamt (beispielsweise ab 1,5 GW Zubau im Jahr 2009 folgt ein Prozent zusätzliche Absenkung der Vergütung für das Jahr 2010). Der gültige Degressionssatz für die Einspeisevergütung ab 1. Januar des Folgejahres wird jeweils zum 31. Oktober des laufenden Jahres durch die Bundesnetzagentur veröffentlicht.

Die vom Deutschen Bundestag am 6. Juni 2008 beschlossene neue und erweiterte Fassung ist am 1. Januar 2009 in Kraft getreten.

Am 30. Juni 2011 beschloss der Deutsche Bundestag eine umfassende Novelle des EEG, darunter eine Neuregelung der Boni-Systeme für die Bioenergie sowie Veränderungen bei den Einspeisetarifen. Eine außerordentlich starke Kürzung wurde für die Photovoltaik beschlossen. Die Änderungen traten zum 1. Januar 2012 in Kraft:

Mit der „Photovoltaik-Novelle (PV-Novelle)“ sind Ende Juni 2012 umfangreiche Änderungen bei der Vergütung von Photovoltaik-Strom nach Einigung im Vermittlungsausschuss von Bundestag und Bundesrat beschlossen worden, die rückwirkend zum 1. April 2012 in Kraft getreten sind. Vorangegangen waren monatelange Diskussionen über die zukünftige Ausgestaltung der Förderung von Photovoltaik-Anlagen im Rahmen des Erneuerbare-Energien-Gesetzes (EEG). Das Ergebnis wurde als „Gesetz zur Änderung des Rechtsrahmens für Strom aus solarer Strahlungsenergie und weiteren Änderungen im Recht der erneuerbaren Energien“ (sog. PV-Novelle) am 23. August 2012 verkündet. Es beinhaltet im Wesentlichen:


Bundesumweltminister Peter Altmaier stellte im Frühjahr 2013 seine Reformvorschläge, als „Strompreisbremse“ zur Deckelung der EEG-Umlage, vor. In der Analyse stellt das Papier fest: 

Altmaier machte daraufhin folgende Reformvorschläge:

In einem gemeinsamen Papier des Bundesumweltministeriums und des Bundeswirtschaftsministeriums vom Februar 2013 wurden kurzfristige Maßnahmen zur Dämpfung der Strompreise vorgeschlagen („Strompreisbremse“), die auf dem Konzept Altmaiers aufbauen.

Der Bundesverband Erneuerbare Energie (BEE) lehnte die Vorschläge in ihrer Mehrzahl ab. Er warnte vor einem „Abwürgen der Energiewende“ und machte mehrere alternative Vorschläge zur Kostendämpfung. Vor allem das Einfrieren der EEG-Umlage auf den für 2013 geltenden Wert von 5,28 Cent pro kWh Strom führe zu einer Bremsung des Ausbautempos. Die EEG-Umlage sei kein Maßstab mehr für die Kosten der erneuerbaren Energien, sondern hänge wesentlich von der Entwicklung der Börsenstrompreise und der CO-Zertifikatepreise ab. Ebenfalls kritisierte der Branchenverband den Plan, künftigen Investoren keine Einspeisevergütung zu zahlen. Damit würde Investoren jegliche Planungssicherheit genommen, wovon vor allem Bürgerenergieprojekte sowie kleine und mittlere Unternehmen betroffen seien. Greenpeace forderte statt einer „Strompreisbremse“ eine „Abzockbremse“.

Grünen-Politiker Jürgen Trittin äußerte sich kritisch zu den geplanten Reformen. Er betonte dabei, die Preiserhöhungen seien nicht einmal zur Hälfte der EEG-Umlage anzulasten. Die Grünen legten zeitgleich mit Rösler und Altmaier ein alternatives Konzept vor, was weiterhin ein vorrangiges Einspeisen von Ökostrom in deutsche Netze vorsieht.

Die meisten vorgeschlagenen Maßnahmen setzten sich beim Energiegipfel am 21. März 2013 bei Bund und Ländern nicht durch, wobei viele Fragen ungeklärt blieben. Bereits errichtete Erneuerbare-Energien-Anlagen sollen allerdings nicht nachträglich schlechter gestellt werden können. Die Vertagung wichtiger Entscheidungen wurde von Branchenvertretern kritisiert, da Planungs- und Investitionssicherheit fehle.

Ein im Juni 2013 veröffentlichtes Gutachten der dena im Auftrag des Bundesverbands der Deutschen Industrie (BDI) stellte fest, dass die Ausbauziele für erneuerbare Energien nicht mehr infrage gestellt würden und die meisten Prognosen von einer Übererfüllung der Ausbauziele des Energiekonzepts ausgingen. Die Studie empfahl eine grundlegende Novellierung des EEG. Ein erster Schritt sei die Anpassung der Einspeisevergütungen an die Marktentwicklung, wobei die Systemkosten zu berücksichtigen seien.

Im Koalitionsvertrag der 18. Wahlperiode des Bundestages vom 27. November 2013 fanden sich eine Reihe von Reformansätzen für das EEG. Prämisse des künftigen Ausbaus erneuerbarer Energien war es, „der Kosteneffizienz und Wirtschaftlichkeit des Gesamtsystems einschließlich des Netzausbaus und der notwendigen Reservekapazitäten eine höhere Bedeutung zuzumessen“. Der weitere Ausbau der erneuerbaren Energien sollte künftig innerhalb eines gesetzlich festgelegten Ausbaukorridors erfolgen, um 40 bis 45 Prozent erneuerbare Energien an der Stromerzeugung im Jahr 2025 und 55 % bis 60 Prozent im Jahr 2035 zu erreichen.

Im Januar 2014 wurden die geplanten Maßnahmen in einem Eckpunktepapier des Bundeswirtschaftsministeriums konkretisiert. Die wesentlichen Ziele waren nunmehr, den Zubau von Erneuerbare-Energie-Anlagen besser zu steuern, die EEG-Umlage stabil zu halten und damit die Bezahlbarkeit von Strom sowie die Versorgungssicherheit sicherzustellen.

Am 27. Juni 2014 beschloss der Bundestag das EEG 2014, basierend auf der Bundeskabinettsvorlage von Wirtschafts- und Energieminister Sigmar Gabriel und den Vereinbarungen im Koalitionsvertrag. Mit den Maßnahmen sollen die Einspeisevergütungen von neuen Anlagen auf durchschnittlich 12 ct/kWh sinken. Im Einzelnen:

Für die einzelnen Energieträger wurden Ausbaupfade festgelegt. Bei der Windenergie liegt der Zielkorridor bei einem jährlichen Nettozubau von 2400 MW bis 2600 MW. Werden mehr Windräder errichtet, sinken die Vergütungen entsprechend stärker („atmender Deckel“). Repowering ist hiervon ausgenommen. Bei Offshore-Windkraft soll es eine Anfangsvergütung von rund 18 Cent je kWh geben, aber die Ausbauziele werden von 10.000 MW auf 6500 MW bis 2020 und 15.000 MW bis 2030 gekürzt. Zudem wurde eine Länderöffnungsklausel eingeführt, mit der länderspezifische Abstände von Windkraftanlagen von Wohnbebauung möglich gemacht wurden; damit könnten einzelne Bundesländer den Ausbau der Windkraft behindern.

Biogasanlagen sollen auf 100 MW im Jahr stark begrenzt werden, wobei v. a. Reststoffe verwertet werden sollen. Die Einspeisevergütungen wurden stark gekürzt. Kritiker verweisen darauf, dass ohne den Ausbau der Bioenergie es an Back-Up-Kraftwerken zum Ausgleich der wachsenden Stromproduktion aus Wind- und Solarenergie fehle.

Hinsichtlich Photovoltaik soll der selbst produzierte und selbst verwendete Eigenstrom künftig mit einer Abgabe belastet werden („Sonnensteuer“), obgleich regierungsinterne Gutachten davon abrieten. Ausgenommen sind kleine Solaranlagen bis 10 kW. Ende Januar 2015 beschloss die Bundesregierung die Verordnung zur Einführung von Ausschreibungen für die finanzielle Förderung von Photovoltaik-Freiflächenanlagen. Die Ausschreibung startete im Februar 2015, unter starker Kritik durch die Branchenverbände.

Die Besondere Ausgleichsregelung, d. h. Ausnahmeregelungen für energieintensive Industrien, wurde reformiert. Bislang wurden Firmen, deren Stromkosten 14 Prozent der Bruttowertschöpfung betrugen, teilweise von der EEG-Umlage befreit. Dieser Schwellenwert soll jetzt für 68 Kernbranchen auf 15 Prozent steigen.

Bei der zukünftig standardmäßig vorgesehenen Direktvermarktung wird der Strom nur noch als „Graustrom“ an der Börse gehandelt. Allerdings ist eine Verordnungsermächtigung in § 95 Nr. 6 EEG aufgenommen worden, die dem Gesetzgeber eine gesonderte Regelung zur Grünstromvermarktung ermöglicht.

Der Bundesverband Erneuerbare Energie kritisierte das Vorhaben und sah die nationalen Klimaziele gefährdet. Die Ausbaukorridore seien zu gering, um den wegfallenden Atomstrom zu ersetzen. Ein Anstieg der klimapolitisch unerwünschten Kohleverstromung sei die Folge. Zudem sei die verpflichtende Direktvermarktung und die Einführung von Ausschreibungen bis 2017 abzulehnen, da sie zu höheren Finanzierungskosten führten und Unsicherheit unter Investoren hervorriefen. So sei die „Energiewende aus Bürgerhand“ gefährdet. Der Deutsche Genossenschafts- und Raiffeisenverband kritisierte, dass die Novelle die Aktivität von Energiegenossenschaften deutlich einschränke, obwohl im Koalitionsvertrag mehr Bürgerbeteiligung versprochen wurde. Der Bundesverband Solarwirtschaft warnte vor einem Marktrückgang. Der Ausbau der Solarenergie werde gedrosselt, ohne die Kosten der Energiewende spürbar zu senken. Insbesondere die „Sonnensteuer“ stieß auf Kritik aus Wissenschaft und Solarbranche. Die für 2015 erwartete Senkung der EEG-Umlage sei nicht auf die Reform, sondern auf günstige Rahmenfaktoren zurückführbar. Der Bundesverband Windenergie kritisierte vor allem die Länderklausel, da diese den Ausbau der Windenergie im Binnenland verhindere. Zudem sei die angekündigte Einführung von Ausschreibung eine große Verunsicherung für den Markt. Der Fachverband Biogas sah die Existenz hunderter Biogasanlagen gefährdet und befürchtet über eine Milliarde Fehlinvestitionen, was dem ländlichen Raum schade. Die Verbraucherzentrale kritisierte, dass der Eigenverbrauch von Solarstrom belastet werden solle. Außerdem forderte sie eine weitergehende Begrenzung der Begünstigungen für die Industrie. Gutachter sehen das Klimaziel der Bundesregierung in Gefahr. Der ehemalige grüne Energiepolitiker Hans-Josef Fell, einer der Autoren des EEG-Entwurfs von 2000, sah die „Grundpfeiler“ des EEG durch die Reform gefährdet.

Am 24. Juli 2014 ist die Neufassung als EEG 2014 im Bundesgesetzblatt verkündet worden, so dass sie am 1. August 2014 in Kraft trat. Auch die EU-Kommission hatte bereits festgestellt, dass das Gesetz mit dem EU-Beihilferecht in Einklang steht, und hat es genehmigt.

Im April 2016 wurde der Referentenentwurf der nächsten EEG-Novelle 2016 vorgelegt, der in jeweils geänderter Fassung am 8. Juni 2016 das Kabinett und am 8. Juli 2016 – kurzfristig in EEG 2017 umbenannt – den Bundestag passierte. Das EEG 2016/2017 bezweckt einen grundsätzlichen Systemwechsel vom Modell der Einspeisevergütungen hin zum Ausschreibungsverfahren, der bereits mit dem im EEG 2014 im Bereich der Photovoltaik-Freiflächenanlagen als Pilotprojekt getestet worden war. Zudem werde erstmals der Ausbau der erneuerbaren Energien nach oben gedeckelt.

Branchenverbände wie der Bundesverband Erneuerbare Energie übten Kritik: Die geplanten Maßnahmen würden den Ausbau der Erneuerbaren Energien stark behindern, zehntausende Arbeitsplätze gefährden, die Klimaschutzziele verfehlen und Bürgerenergie-Projekte benachteiligen. Auch Umweltschutzorganisationen wie der WWF Deutschland sowie das Umweltbundesamt kritisierten die Novelle und forderten, der dynamische Ausbau erneuerbarer Energien müsse auch über die Novellierung hinaus gewährleistet bleiben. Eine Studie kommt zu dem Schluss, dass mit dem Ausbaupfad des EEG 2016/2017 die Pariser Klimaschutzziele nicht erreicht werden könnten. Darüber hinaus wurde die Vergütungskürzungen und die rückwirkende Ausschreibungspflicht teils als verfassungswidrig betrachtet. Ein Gutachten der SPD-nahen Friedrich-Ebert-Stiftung stützte die Umwelt- und Branchenverbände.

Befürworter der Energiewende begrüßten, dass Bürgerenergiegesellschaften Erleichterungen bei Ausschreibungen eingeräumt wurden und dass Mieterstrom-Modelle bei der EEG-Umlage nicht weiter benachteiligt sind. Die Novelle sieht ferner im neu eingeführten §79a "Regionalnachweise für direkt vermarkteten Strom aus erneuerbaren Energien" vor und schafft damit die Möglichkeit einer regionalen Grünstromkennzeichnung.

2016 profitierten insgesamt 2.137 Unternehmen mit 2.835 Abnahmestellen von Ausnahmeregelungen im EEG. Die privilegierte Strommenge liegt nach Angaben des Bundesamtes für Wirtschaft und Ausfuhrkontrolle bei 107 Mrd. kWh, womit den Unternehmen Entlastung in Höhe von ca. 4,7 Mrd. Euro entstanden. Auf Anfrage der Grünen veröffentlichte der Deutsche Bundestag am 7. Juli 2016 Informationen zu den Ausnahmeregelungen des EEG für stromkostenintensive Unternehmen. Danach mussten im Jahr 2016 insgesamt 717 solcher Unternehmen nur einen geringen Anteil oder gar keine EEG-Umlage bezahlen. Nur 58 Anträge von energieintensiven Unternehmen sind demnach abgelehnt worden. Die privilegierte Strommenge wurde mit 70,12 Millionen Megawattstunden Strom für das Antragsjahr 2015 angegeben, wodurch die Unternehmen eine finanzielle Entlastung von 3,4 Milliarden Euro erhielten.

Weitere Änderungen des EEG zum 1. Januar 2017 sowie unter anderem Änderungen des Kraft-Wärme-Kopplungsgesetzes, des Energiewirtschaftsgesetzes und des Windenergie-auf-See-Gesetzes enthält das "Gesetz zur Änderung der Bestimmungen zur Stromerzeugung aus Kraft-Wärme-Kopplung und zur Eigenversorgung".

Mit dem EEG 2017 wurden das Fördersystem von Einspeisevergütungen auf ein Ausschreibungssystem umgestellt, bei der die Regierung eine feste Menge an Leistung ausschreibt und anschließend die günstigsten Gebote den Zuschlag bekommen. Mit Stand Mai 2017 sind die Ergebnisse sowohl einer Offshore-Windenergie als auch einer Onshore-Windenergie-Ausschreibung bekannt:

Bei der ersten Ausschreibung für Offshore-Windkraft in Höhe von knapp 1,5 Gigawatt sind drei Zuschläge für 0 Cent/kWh und ein Zuschlag für 6 Cent/kWh bekanntgegeben worden. Die drei Windparks erhalten lediglich eine Förderung über der Bereitstellung des Netzanschlusses. Errichtet werden diese Windparks ab dem Jahr 2021.

Bei der ersten Onshore-Ausschreibung konkurrierten 256 Bieter mit einer Gesamtleistung von 2,1 GW um eine ausgeschriebene Leistung von 800 MW. Letztendlich erhielten 70 Projekte Fördergelder zwischen 5,25 und 5,78 ct/kWh, der Großteil davon Bürgergesellschaften. Geographisch wurden vor allem Projekte in Norddeutschland ausgewählt, während in Süddeutschland nur 7 Projekte einen Zuschlag erhielten.

Ungeachtet ihres Bedarfs müssen die Betreiber öffentlicher Netze allen Strom, der von in Deutschland einschließlich der deutschen ausschließlichen Wirtschaftszone betriebenen Anlagen nach dem EEG gewonnen wird ( Nr. 1 EEG), mit Vorrang vor solchem Strom abnehmen, der aus anderen Energiequellen erzeugt wird, vor allem aus fossilen Brennstoffen und Kernkraft. Gleichrangig mit dem Strom aus erneuerbaren Energien ist jedoch der mit Kraft-Wärme-Kopplungsanlagen erzeugte Strom ( Abs. 1 KWKG) einzuspeisen. Die Netzbetreiber sind verpflichtet, ihre Netze jeweils ausreichend auszubauen, so dass sie den bevorrechtigten Strom aufnehmen können, es sei denn, die Maßnahmen wären wirtschaftlich unzumutbar ( EEG). Eine Verletzung dieser Pflicht macht schadensersatzpflichtig ( Abs. 1 EEG). Umgekehrt ist der Anlagenbetreiber, soweit er eine Vergütung nach dem EEG geltend macht, verpflichtet, dem Netzbetreiber seinen Strom anzudienen, es sei denn, er oder unmittelbar angeschlossene Dritte nutzen den Strom selber ( Abs. 4 EEG) oder der Anlagenbetreiber vermarktet ihn in Übereinstimmung mit EEG selbst (was jedoch eine fristgebundene vorherige Ankündigung voraussetzt).

Für den eingespeisten Strom hat der Netzbetreiber dem Anlagenbetreiber die im Gesetz festgesetzten Vergütungssätze zu zahlen. Die Vergütungssätze unterscheiden sich je nach der bei der Stromerzeugung eingesetzten Energieart erheblich; mutmaßlich teurere Stromerzeugungsformen werden höher vergütet als günstigere. Die Vergütungen sind in dieser Höhe auf die Dauer von 20 Kalenderjahren zuzüglich des Inbetriebnahmejahres zu zahlen; bei großer Wasserkraft (ab 5 MW) verkürzt sich die Laufzeit auf 15 Jahre. Die gesetzlichen Vergütungssätze werden aufgrund einer bereits im Gesetz festgelegten Degression in Höhe eines dort vorgesehenen Prozentsatzes kalenderjährlich für dann in Betrieb gehende Neuanlagen gemindert ( EEG).

Während die Abnahme des EEG-Stroms durch die Netzbetreiber und die Weiterleitung dieses Stroms einschließlich der Weitergabe der Mehrkosten an die höherrangigen Netzbetreiber und Elektrizitätsversorgungsunternehmen gesetzlich geregelt ist, sind die die Endverbraucher beliefernden Elektrizitätsversorgungsunternehmen selber in der Verwertung des EEG-Stroms frei: er ist Teil ihres allgemeinen Stromportfolios. Nach ihren Allgemeinen Geschäftsbedingungen dürfen die Versorgungsunternehmer die durch das EEG verursachten Mehrkosten in ihre Kosten einstellen. Sie haben zudem das Recht, die EEG-Mehrkosten anteilig dem Endverbraucher gegenüber auszuweisen (Differenzkosten gem. EEG). Das Bundesamt für Wirtschaft und Ausfuhrkontrolle (BAFA) kann aber auf Antrag Endverbrauchern, welche stromintensive Unternehmen des produzierenden Gewerbes mit hohem Stromverbrauch oder Schienenbahnen sind ( Abs. 1 EEG), eine Ermäßigung der EEG-Umlage gewähren.

Durch die Vergütungspflicht entstehen den Netzbetreibern Kosten. Durch den Verkauf des EEG-finanzierten Stroms an der Börse erzielen sie Einnahmen. Die Differenz zwischen Vergütungen und Einnahmen bildet die Grundlage für die Ermittlung der EEG-Umlage, die von den Stromverbrauchern zu bezahlen ist. Vielfach wird sie als die Kosten bezeichnet, die aus der Förderung der Stromerzeugung aus erneuerbaren Energiequellen entstehen. Dies ist allerdings umstritten. Denn mit den Vergütungen werden die Vollkosten der Stromerzeugung aus erneuerbaren Energien finanziert, während die Strompreise an der Börse auf Basis der Grenzkosten ermittelt werden. Diese sind derzeit deutlich niedriger als die Vollkosten neuer Kohle-, Gas- oder Atomkraftwerke. Da aufgrund des Atomausstieges bis 2022, des insgesamt veralteten konventionellen Kraftwerksparks und der Anforderungen des Klimaschutzes eine Modernisierung des deutschen Kraftwerkparks ohnehin ansteht, müssten zur Ermittlung der tatsächlichen Zusatzkosten durch den Ausbau der erneuerbaren Energien durch das EEG nicht die Börsenpreise für Strom, sondern die Stromgestehungskosten konventioneller Kraftwerke verwendet werden. Diese liegen laut Bundeswirtschaftsministerium zwischen 7 und 11 Cent pro Kilowattstunde, während die Börsenpreise derzeit bei rund 4 Cent pro Kilowattstunde liegen.

Mit der Vermarktung des EEG-Stroms sind seit 2010 die vier deutschen Übertragungsnetzbetreiber (ÜNB) auf Grundlage der Ausgleichsmechanismus-Verordnung betraut ( AusglMechV). Sie ermitteln zum 15. Oktober die Höhe der EEG-Umlage für das Folgejahr in einer Prognose, die zusammen mit Forschungsinstituten anhand der erwarteten Ausgaben für EEG-Vergütungen und der voraussichtlichen Einnahmen aus dem Verkauf des EEG-Stroms an der Strombörse EPEX Spotmarkt aufgestellt wird.

Ausgaben fallen im Wesentlichen für die festgelegten Vergütungen für eingespeisten Strom und für die mit dem EEG 2012 eingeführte Marktprämie an. (Summe aus beiden Teilbeträgen 2012: 17,97 Mrd. Euro, 2013: 18,5 Mrd. Euro). Außerdem fallen Kosten für zugehörige Aufwendungen an, darunter ein Liquiditätsausgleich von 3 % der Ausgaben (2013: 1,61 Mrd. Euro), der größte Einzelposten unter den „Sonstigen Kosten“. Ende September eines laufenden Jahres findet ein Kontoausgleich statt, der den Ausgleich zwischen den prognostizierten und tatsächlichen Ergebnissen schafft. Er ist für 2013 mit 2,59 Mrd. Euro verbucht und macht rund 11 % der Ausgaben aus.

Die Ausgaben bei der Förderung der Stromerzeugung nach EEG lagen für 2013 bei 19,4 Mrd. Euro (zum Vergleich 2012: 19,43 Mrd. Euro; siehe Tabelle). Im Februar 2017 ist der Überschuss des EEG-Kontos auf einen Überschuss um einen Rekordwert von etwa einer Milliarde Euro auf 4,57 Mrd. Euro gewachsen. Bereits im Januar 2017 lag der Überschuss mit 674 Mio. Euro sehr hoch. Grund dafür sind gestiegene Einnahmen durch eine erhöhte EEG-Umlage an der Strombörse. Gleichzeitig sanken die Ausgaben aufgrund höherer Börsenstrompreise. So lagen die Einnahmen für Februar 2017 bei 2,42 Mrd. Euro und die Ausgaben bei 1,4 Mrd. Euro. 

Die wesentlichen Einnahmen werden aus dem Verkauf des eingespeisten Stroms gewonnen, dessen Vermarktung seit 2010 durch die Ausgleichsmechanismus-Verordnung (AusglMechV) geregelt ist. Die vier bundesweiten Übertragungsnetzbetreiber vermarkten den EEG-Strom an der Strombörse EPEX Spotmarkt, die von der Firma EPEX SPOT SE (Sitz in Paris) betrieben wird. EEG-Strom wird im Spotmarkt mit Tagesvorlauf (Day ahead) oder im Tagesverlauf (Intraday) als „Graustrom“ vermarktet. Die Vermarktung als gekennzeichneter „Grünstrom“ (Strom aus erneuerbaren Quellen) mit möglicherweise höheren Erlösen wird diskutiert, jedoch nicht praktiziert. Die Börsenpreise werden von EPEX SPOT SE im Internet veröffentlicht. Diese Einnahmen aus der Vermarktung am Spotmarkt fließen direkt in die EEG-Umlage ein. Der Börsenpreis wird auch bei der Bewertung des Stroms angesetzt, der im Marktprämienmodell direkt vermarktet wird. Die verbleibende Differenz zur EEG-Vergütung wird als Marktprämie im Zuge der EEG-Umlage ausgeglichen (siehe Ausgaben), ebenso die organisatorischen Aufwendungen (Profilservicekosten, Managementprämie).

Auf die Höhe der EEG-Umlage wirken sich Sonderregelungen aus, wie die Sonderregelungen für stromintensive Unternehmen, da diese teilweise von der EEG-Umlage befreit sind.

Der eingetretene Unterschied zwischen dem Prognosewert und dem tatsächlichen Ergebnis wird im Folgejahr mit der „Nachholung“ durch Kontoausgleich mit Stichtag Ende September ausgeglichen. Die Zahlung des Umlagebetrags erfolgt von den Verbrauchern über die Stromversorger (Verteilnetzbetreiber) an die vier deutschen Übertragungsnetzbetreiber (ÜNB).

Die verbleibende Differenz, die „EEG-Differenzkosten“ (für 2013 prognostiziert 19,9 Mrd. Euro), wird auf den erwarteten Stromverbrauch verteilt, der für die sog. EEG-pflichtigen Endverbraucher erwartet wird. Die von der EEG-Umlage befreiten Abnehmer werden als sog. „privilegierte Verbraucher“ entsprechend an der Umlage nicht beteiligt (Mitte 2012 waren es 18 % des bundesweiten Stromabsatzes, 88,7 TWh/a von insgesamt 477,5 TWh/a bzw. die Hälfte des Stromabsatzes an Industrie und Gewerbe). Die teilweise befreiten Abnehmer werden anteilig in der Umlageberechnung berücksichtigt (s. „Endverbraucher mit reduzierter EEG-Umlage“ in der Tabelle).

Da die Einspeisevergütungen für 20 Jahre zzgl. dem Jahr der Inbetriebnahme garantiert sind, entstehen durch das EEG langfristige Zahlungsverpflichtungen. Die Summe dieser Zahlungsverpflichtungen – auch implizite Verschuldung des EEG genannt – wird zum Ende 2014 auf nominal 290 Mrd. Euro geschätzt, was einem Barwert von 228 Mrd. Euro entspricht.

Das Aktionsbündnis „Freiburger Appell“ fordert ein neues Strommarktdesign, mit dem die EEG-Umlage umgehend um ein Drittel gesenkt werden könnte. Die derzeitige gesetzliche Zwangsvermarktung von EEG-Strom zu Niedrigstpreisen am Spotmarkt der Strombörse solle abgeschafft werden. Stattdessen soll der EEG-Strom zeitgleich den Stromhändlern zugewiesen werden. Die Bewertung des Stroms soll zum höheren Terminmarkt-Preis erfolgen. Damit würden die Erlöse für EEG-Strom stark steigen und die EEG-Umlage würde im Gegenzug um etwa ein Drittel sinken.

In ähnlicher Weise argumentiert der Solarenergie-Förderverein Deutschland e.V. (SFV) und macht darauf aufmerksam, dass es in bestimmten Situationen möglich ist, dass Kohlekraftwerksbetreiber Brennstoffkosten sparen können, indem sie ihre langfristigen Lieferverträge durch den Weiterverkauf von preiswertem Strom aus Erneuerbaren Energien vom Spotmarkt bedienen. Deshalb fordert der SFV Strom grundsätzlich nur noch am Spotmarkt zu handeln.

Der Strompreis an der Strombörse war bis zum Jahr 2008 kontinuierlich gestiegen und erreichte im Jahr 2008 das Maximum von 8,279 Cent/kWh. Durch das vermehrte Auftreten der erneuerbaren Energien ist der Strompreis unter Druck geraten. Im ersten Halbjahr 2013 betrug der mittlere Strompreis an der Strombörse nur noch 3,75 Cent/kWh und für den Terminmarkt 2014 lag dieser im Juli 2013 bei 3,661 Cent/kWh. Somit ist der Strompreis an der Strombörse bereits mehr als 4 Cent/kWh gefallen. Infolgedessen haben die erneuerbaren Energien nicht nur einen verteuernden Effekt, sondern auch einen verbilligenden Effekt, welche man beide gegeneinander aufwiegen müsste.

Greenpeace Energy argumentiert, dass konventionelle Stromerzeuger sog. externe Kosten hätten, wie etwa Umweltschäden, die nicht über den Strompreis abgewickelt werden, sondern vom Steuerzahler aufgebracht werden müssten. Dies sei eine Subvention der konventionellen Stromerzeuger und stelle eine Marktverzerrung dar. Die EEG-Umlage sei nur ein Ausgleich für diese Marktverzerrungen. Durch einen funktionierenden Emissionshandel könnten diese Marktverzerrungen behoben werden und die EEG-Umlage würde automatisch gegen Null sinken.

Stromintensive Unternehmen des produzierenden Gewerbes sowie Schienenbahnen sind durch die besondere Ausgleichsregelung im EEG zum Schutz ihrer internationalen und intermodalen Wettbewerbsfähigkeit von der EEG-Umlage teilweise befreit ( EEG sowie zugehörige Regelungen §§ 41–44 EEG). Antragsberechtigt sind Unternehmen des produzierenden Gewerbes mit einem Stromverbrauch über 1 GWh/a (bis 31. Dezember 2011: 10 GWh/a) und einem Verhältnis der Stromkosten zur Bruttowertschöpfung des Unternehmens von mindestens 14 %. Sind diese Voraussetzungen erfüllt, wird die EEG-Umlage für das Unternehmen wie folgt begrenzt: Für den Stromanteil bis 1 GWh/a wird die EEG-Umlage nicht begrenzt. Für den Stromanteil zwischen 1 und 10 GWh/a sind 10 % der EEG-Umlage zu bezahlen. Für den Stromanteil zwischen 10 und 100 GWh/a sind 1 % der EEG-Umlage zu bezahlen. Für den Stromanteil über 100 GWh/a beträgt die EEG-Umlage 0,05 Cent/kWh. Für Betriebe mit einem Strombezug über 100 GWh/a und einem Verhältnis der Stromkosten zur Bruttowertschöpfung von mindestens 20 % ist die EEG-Umlage auf 0,05 Cent/kWh begrenzt. Für Schienenbahnen gilt ein Stromverbrauch von 10 GWh/a als Voraussetzung. Ist diese Voraussetzung erfüllt, wird die EEG-Umlage wie folgt begrenzt: Für 10 % des Stromverbrauchs wird die EEG-Umlage nicht begrenzt. Für den übrigen Verbrauch beträgt die EEG-Umlage 0,05 Cent/kWh.

Nach Angaben des Bundesumweltministeriums vom März 2012 war durch diese Regelungen zu diesem Zeitpunkt etwa die Hälfte des industriellen Stromverbrauchs ganz oder teilweise von der EEG-Umlage befreit, wodurch sich die EEG-Umlage für die Endverbraucher 2011 um etwa 0,9 ct/kWh erhöht hat. Die Zahl der befreiten Unternehmen hat nach der EEG-Novelle 2012 erheblich zugenommen. Die Bundesregierung erwartete, dass 2013 voraussichtlich etwa zweieinhalb Mal so viele Unternehmen wie bisher von der Sonderregelung profitieren konnten. Im Jahr 2013 profitierten 1691 stromintensive Unternehmen von der Sonderregelung. Dies entsprach einem Anteil von 1,29 Cent/kWh oder ca. 25 % der Gesamthöhe (5,28 Cent/kWh)an der EEG-Umlage. Bei voller Veranlagung hätten sie vier Milliarden Euro entrichten müssen. Im Jahr 2014 profitierten 2098 Unternehmen bzw. Unternehmensteile von der besonderen Ausgleichsregelung des EEG. Die privilegierte Strommenge dieser Unternehmen lag bei 107.101 GWh. Der Liste der betroffenen Unternehmen ist zu entnehmen, dass viele dieser Unternehmen keinen Handel mit dem Ausland betreiben. Für 2015 wurden Anträge auf Entlastung in Höhe von 4,8 Mrd. Euro gestellt.

Diese Entlastung stieß mit Blick auf Wettbewerbsverzerrungen, Mehrbelastungen für Privatverbraucher und ökologisch fragwürdige Anreizeffekte auf breite Kritik. Die EU-Kommission leitete ein Beihilfeverfahren gegen Deutschland ein, um die Subventionierung von Unternehmen durch die Teilbefreiung von der EEG-Umlage wettbewerbsrechtlich zu prüfen.

Im Mai 2012 veröffentlichte die Bundesnetzagentur einen Evaluierungsbericht, in dem die Ausnahmeregelungen für die Großindustrie kritisiert werden. So betrachte die Netzagentur die „Reduktion des EEG-umlagepflichtigen Letztverbrauchs aufgrund der Ausweitung der Privilegierungsregelungen mit Sorge“, da mit den derzeit geltenden Regelungen die privilegierten Unternehmen zwar ca. 18 % des Stroms verbrauchten, allerdings nur 0,3 % der EEG-Umlage trügen. Zukünftig gelte es, eine bessere Balance zwischen Großverbrauchern sowie kleineren und mittleren Unternehmen sowie Haushaltskunden zu finden. Durch die Privilegierung der stromintensiven Industrie würde diese um ca. 2,5 Mrd. Euro jährlich entlastet, die Kosten würden auf nichtprivilegierte Unternehmen sowie Privathaushalte abgewälzt. Für die Produktion einer einzelnen Aluminiumhütte wurden so nach journalistischen Recherchen im Jahr 2014 rund 450 Mio. Euro Strompreisvergünstigungen gewährt. Da das betreffende Unternehmen jedoch nur eintausend Beschäftigte habe, ergab das eine Subvention von 440.000 Euro pro Jahr und Arbeitsplatz.

Auf Kritik stößt auch, dass viele Unternehmen durch Auslagern von Arbeiten über Werkverträge in den Genuss der Ausnahmeregelung kommen. Dadurch werden einerseits die eigenen Personalkosten verringert. Zudem werden Werkverträge laut Gesetz nicht zur betrieblichen Wertschöpfung hinzugezählt. Das Kriterium für die Ausnahmeregelung wird unter Umständen dadurch erreicht, dass Arbeiten an ausländische (Lohndumping-)Unternehmen vergeben werden, was der Zielsetzung des Gesetzes völlig widerspricht.

Das Bundesumweltministerium und das Bundeswirtschaftsministerium planten, die Ausnahmeregelungen zurückzufahren, um so den Kostenanstieg zu dämpfen. Unternehmen, die nicht im „intensiven internationalen Wettbewerb“ stehen, sollten sich wieder stärker am Ausbau der erneuerbaren Energien beteiligen. Nach Einschätzung der KfW hat die deutsche Industrie von der Energiewende profitiert; eine wettbewerbliche Benachteiligung sei aufgrund der unterdurchschnittlichen Entwicklung nicht ohne weiteres erkennbar. Diese Einschätzung wurde von weiteren Gutachten gestützt.
Aufgrund der verschiedenen steuer- und abgabenrechtlicher Privilegierungen sowie infolge des Merit-Order-Effekts erneuerbarer Energien sinkender Großhandelspreise bezieht die energieintensive Industrie in Deutschland im Vergleich zu den Vorjahren sowie im Vergleich zu anderen Industrieländern relativ günstig Strom. Im Jahr 2013 kostete eine Kilowattstunde am Spotmarkt im Schnitt 3,78 Cent. In Frankreich kostete das gleiche Produkt im selben Jahr dagegen 4,3 Cent, in Italien und Großbritannien 6,2 Cent. In den USA stiegen die Börsenstrompreise ab dem Jahr 2012 wieder an und liegen in manchen Regionen ebenfalls über dem deutschen Niveau. Auch die Strompreise im außerbörslichen Handel werden in Deutschland günstiger. Für die Jahre 2015 bis 2017 kostet der Strom bei direkten Lieferverträgen laut dem Verband der Industriellen Energie- und Kraftwirtschaft (VIK) zwischen 2,68 und 4,28 Cent pro Kilowattstunde. Weitere Gutachten aus dem Jahr 2013 stützen diesen Befund.
Im Januar 2014 kündigte eine niederländische Aluminiumhütte ihre Schließung an, da sie aufgrund des billigen Industriestroms in Deutschland nicht mehr wettbewerbsfähig sei.

Die Energiekosten wie auch die EEG-Umlage haben im verarbeitenden Gewerbe in Deutschland nur einen geringen Anteil am Bruttoproduktionswert, verglichen etwa mit Faktoren wie Material- und Personalkosten. Dennoch ist eine Belastung der Betriebe durch die gesetzliche EEG-Umlage durchaus messbar: Im Maschinenbau hatte diese im Jahr 2007 (aktuellste verfügbare Datengrundlage des Statistischen Bundesamts) einen Anteil von höchstens 0,05 % am Bruttoproduktionswert. In den energieintensivsten Branchen, etwa der Glas-, Keramik- oder Papierherstellung, betrug der Anteil der EEG-Umlage höchstens 0,3 %. Hochgerechnet auf die EEG-Umlage für nicht-privilegierte Endverbraucher im Jahr 2013 (5,23 Cent/kWh) ergäbe sich in diesen Branchen ein Höchstanteil von 1,5 %.

Nach einem vom Deutschen Institut für Wirtschaftsforschung unterbreiteten Reformvorschlag sollen Unternehmen künftig nur noch privilegiert werden, wenn sie aufgrund von europarechtlichen Kriterien auch unter die Strompreiskompensationsregel des EU-Emissionshandels fallen. Davon profitieren 15 zentrale Sektoren, darunter die Eisen-, Stahl-, Aluminium-, Kupfer-, Chemie- und Papierindustrie. Die bisherige Kopplung der Privilegierung an die Höhe des jährlichen Stromverbrauchs soll hingegen ersatzlos entfallen.

Die Deutsche Bahn zahlte nach eigenen Angaben im Jahr 2013 55 Millionen Euro EEG-Umlage. Aufgrund veränderter Bemessungsgrundlagen soll dieser Betrag im Jahr 2014 auf 108 Millionen Euro steigen. Laut eigenen Angaben werde sich ihr EEG-Anteil im Jahr 2015 um weitere 52 Millionen Euro, auf 160 Millionen Euro, erhöhen.

Der Eigenstromverbrauch von Kraftwerken, beispielsweise Kohle-, Gas- oder Atomkraftwerke, ist von der EEG-Umlage befreit. Diese Sonderregelung führt zu Mehrbelastungen von ca. 2,6 Mrd. Euro bzw. 12 % der EEG-Umlage (2013).

Durch die im Jahr 2009 erlassene Verordnung zur Weiterentwicklung des bundesweiten Ausgleichsmechanismus (AusglMechV), die für ab 2010 erzeugten EEG-Strom gilt, wird der gesetzliche Ausgleichsmechanismus des EEG grundsätzlich umgestaltet. Die AusglMechV erging auf der Grundlage des Abs. 3 EEG, der die Bundesregierung zu weitgehenden Änderungen des bundesweiten Ausgleichsmechanismus im Hinblick auf die für EEG-Strom anfallenden Kosten ermächtigt. Die AusglMechV entbindet die Übertragungsnetzbetreiber davon, den EEG-Strom an die Energieversorgungsunternehmer durchzuleiten, und diese werden wiederum aus ihrer Abnahmepflicht entlassen ( Nr. 1 und 2 AusglMechV). Die Übertragungsnetzbetreiber werden stattdessen verpflichtet, den EEG-Strom am Spotmarkt einer Strombörse transparent und diskriminierungsfrei zu verwerten ( Nr. 3 und AusglMechV). Die Übertragungsnetzbetreiber können zusätzlich von den Energieversorgungsunternehmen, die Strom an Endverbraucher liefern, anteilig Ersatz der erforderlichen Aufwendungen im Zusammenhang mit der EEG-Umlage verlangen ( Abs. 1 AuslgMechV). Die Umlage berechnet sich gemäß der AusglMechV nach der Differenz der Einnahmen aus der Vermarktung des EEG-Stroms nach EEG (zzgl. damit zusammenhängender Einnahmen) und der Aufwendungen im Zusammenhang mit der Abnahme des EEG-Stroms, hier vor allem der nach dem EEG zu leistenden Vergütungen ( AusglMechV).

Die Vergünstigung von stromintensiven Unternehmen und von Schienenbahnen nach EEG, wonach deren Pflicht zur Abnahme von EEG-Strom begrenzt werden kann, wird dahingehend geändert, dass jene nur einen Ausgleich von 0,05 Cent/kWh als EEG-Umlage zu zahlen haben. Darüber hinaus enthält die AusglMechV Grundsätze zu Ermittlung der EEG-Umlage und verpflichtet die Übertragungsnetzbetreiber, die für die Ermittlung der Umlage festgestellten Einnahmen und Ausgaben monatlich und jährlich auf ihren Internetseiten zu veröffentlichen, desgleichen auch eine Prognose für die erwartete Umlage des nächsten Jahres. Die Bundesnetzagentur wird wiederum ermächtigt, weitergehende Verordnungen zu erlassen. Mit der AusglMechV wird ein wesentlicher Teil des EEG auf dem Verordnungswege geändert, weswegen die verfassungsrechtliche Zulässigkeit einer gesetzesvertretenden Verordnung bezweifelt wird – ein Einwand, den der Bundesrat im Gesetzgebungsverfahren bereits erhoben hatte.

Kann Strom produziert aber nicht eingespeist werden, etwa weil der Netzbetreiber den Anlagenbetreiber aufgrund mangelnder Netzkapazitäten oder Netzüberlastung ferngesteuert herunterfährt, steht dem Anlagenbetreiber seit dem EEG 2009 eine Entschädigung zu, die sich gewöhnlich an der sonst zu zahlenden EEG-Vergütung orientiert.

Die abgeregelte Windenergie betrug in den Jahren 2011 bis 2013 jährlich knapp 1 % der erzeugten Windenergie und stieg 2014 auf 2,4 % und 2015 auf 5,2 % an. Bei Photovoltaik betrug die abgeregelte Energie 2015 rund 0,6 %, bei Biomasse 0,9 %. Im Jahr 2015 belief sich die gesamte abgeregelte erneuerbare Energie 4722 GWh. Davon entfielen 87 % auf Windenergie, 5 % auf Solarenergie und 8 % auf Biomasse. Die Engpässe lagen 2015 mit 89 % im überregionalen Übertragungsnetz, geographisch zu 97 % in Norddeutschland. Die hierfür entstandenen Entschädigungsansprüche beliefen sich 2015 auf 394 Mio. Euro (vorläufiger Wert). Im Jahr 2016 fielen die abgeregelte Strommenge auf 3.743 GWh, die dafür gezahlten Entschädigungszahlungen nahmen auf 373 Mio. Euro ab.

Mit dem EEG 2014 wurden erstmals Ausschreibungen eingeführt, die zunächst in Pilotmodellen mit Photovoltaik-Freiflächenanlagen erprobt und evaluiert wurden. Damit einher ging die Einführung von Wachstumspfaden für die einzelnen Energieträger, wie sie bis dahin nur für die Photovoltaik bestanden. Für Photovoltaikanlagen auf Gebäuden bis 1 MW besteht keine Ausschreibungspflicht. Das Eckpunktepapier aus dem Jahr 2015 sah Ausschreibungen für einen Ausbau der Windkraft von 2,4 GW vor, sofern die Inbetriebnahme zwischen 2021 und 2023 erfolgt. Dies sollte aber vom Erreichen des Ausbauziels abhängig gemacht werden.

Branchenverbände sahen die Ausschreibung skeptisch. Sie erwarten, dass die höheren Marktrisiken, die durch die Abkehr vom Vergütungssystem entstehen, eingepreist werden und damit zu höheren Kosten führen. Gerade Bürgerenergieprojekte seien gefährdet, da sie die Finanzierungsrisiken nicht tragen könnten.

Für die Jahre 2017 bis 2019 wurde das Ausschreibungsvolumen bei der Windenergie auf jährlich 2,8 GW festgelegt und damit gegenüber dem Eckpunktepapier erhöht. Ab 2020 sollen es jährlich 2,9 GW sein. Allerdings soll sich das Ausschreibungsvolumen ab dem Jahr 2018 jeweils um die Summe der installierten Leistung verringern, die im Rahmen von Ausschreibungen vergeben wurde.

Kleinere stromerzeugende Anlagen können als Vorhaben zum Zwecke der Energieversorgung die im Energiewirtschaftsgesetz (EnWG) vorgesehene Möglichkeit zur Enteignung gem. Abs. 1 Nr. 3 EnWG in Anspruch nehmen, was vor allem dann Bedeutung hat, wenn die privaten Anlagen Grund und Boden Dritter zur Durchleitung von Kabeln zum nächsten aufnahmebereiten öffentlichen Netz beanspruchen müssen. Strittig ist unter den Gerichten, ob dies das Recht zur vorzeitigen Besitzeinweisung ( EnWG) mitumfasst.

Die Befugnisse der Energieversorgungs- und Netzunternehmen, zur Wahrung der Sicherheit und Zuverlässigkeit des Elektrizitätsversorgungssystems nach Abs. 1 und Abs. 1 EnWG Maßnahmen (wie Produktionsbeschränkungen) zu ergreifen, bestehen den EEG-Strom erzeugenden Anlagenbetreibern gegenüber – und zwar insoweit ohne Entschädigungspflicht ( Abs. 2 EEG).

Mit dem EEG wird gewährleistet, dass Anlagenbetreiber den im Jahr der Inbetriebnahme der Stromerzeugungsanlage geltenden Vergütungssatz für dieses Jahr und zwanzig weitere Jahre erhalten. Mit dieser Festlegung soll den Anlagenbetreibern eine ausreichende Investitionssicherheit gegeben werden. Die im Gesetz vorgesehene jährliche Degression der Vergütungssätze gilt jeweils nur für im jeweiligen Jahr ans Netz gegangene Anlagen zum Zeitpunkt der offiziellen Inbetriebnahme ( EEG). Nicht geklärt ist, ob und unter welchen Voraussetzungen der Gesetzgeber rückwirkend für betriebene Anlagen die Vergütungsbedingungen einschließlich der Sätze kürzen kann. Das BVerfG, das sich in einer Entscheidung vom 18. Februar 2009 mit der rückwirkenden Anwendung des neuen Anlagenbegriffs im EEG von 2009 befasste, der seit Inkrafttreten des Gesetzes zu einem Vergütungseinbruch von knapp 50 % bei einigen Betreibern von Biomassenanlagen geführt hatte, ließ diese Frage unbehandelt, da es eine rückwirkende Änderung wegen unsicherer Rechtslage bereits für zulässig erachtet hatte. Der vom BMU beauftragte Gutachter Stefan Klinski war der Auffassung, dass es sich bei einer nachträglichen Änderung der geltenden Vergütungssätze für die Zukunft um eine „unechte Rückwirkung“ handele, die grundsätzlich erlaubt sei, bei der aber das vom Gesetzgeber hervorgerufene Vertrauen berücksichtigt werden müsse. Sein vom BMU veröffentlichtes Gutachten kam zu dem Ergebnis, dass in die laufende Vergütung und deren Bedingungen eingegriffen werden könne, falls EU-Recht dies verlange oder aber nachträglich festgestellt werde, dass die gezahlten Vergütungen wirtschaftlich zu hoch seien.

Das Stromeinspeisemodell des EEG greift auf verschiedenen Ebenen in die Vertrags- und Verwertungsfreiheit ein, so dass die verfassungsrechtliche Zulässigkeit im Hinblick auf die Berufsfreiheit ( GG) und auf das Recht auf Eigentum ( GG) fraglich erschien. Zudem führt das Gesetz in Form des EEG-Zuschlags zu einer Art Abgabe, deren Zulässigkeit als steuerrechtliche Sonderabgabe in Frage gestellt wurde. Indessen wird die Abgabe überwiegend als privatrechtlicher Preisbestandteil eingestuft, da die vom EEG verursachten Mehrkosten abgabenrechtlich öffentliche Haushalte nicht berühren. Im Übrigen werden die Vorschriften des EEG als zulässige Regelung der Berufsausübung bzw. der Inhaltsbeschränkung des Eigentums eingestuft.

Europarechtlich stand das Modell unter dem Gesichtspunkt einer Verletzung der Warenverkehrsfreiheit und des Verbots der Gewährung von Beihilfen auf dem Prüfstand. Die Europäische Kommission hatte jahrelang ein anderes Modell als marktwirtschaftlicher gestützt, wonach zur Verwendung Erneuerbarer Energien bei der Stromerzeugung Quoten zugeteilt werden, die durch den Kauf von grünen Zertifikaten (über EE-Strom) erfüllt werden können. Der Europäische Gerichtshof hatte aber bereits zum Stromeinspeisungsgesetz in seiner PreussenElektra-Entscheidung vom 13. März 2001 bestätigt, dass es sich bei der EEG-Umlage um keine Leistung der öffentlichen Hand handele, so dass ein Verstoß gegen das Beihilfeverbot ausschied; den vorliegenden Eingriff in die Warenverkehrsfreiheit sah das Gericht für den damaligen Zeitpunkt wegen der zwingenden Belange des Klima- und Umweltschutzes als noch hinnehmbar an. Indem die EG-Richtlinie 2009/28/EG vom 23. April 2009 das Modell des EEG (neben dem Quotenmodell) ausdrücklich bestätigte, wurden letzte europarechtliche Zweifel ausgeräumt.

Nach einem Gutachten der Rechtsfakultät der Universität Regensburg aus dem Jahr 2012 sind Teile des EEG seit der Novelle von 2009 möglicherweise verfassungswidrig. Problematisch sei dass mit Inkrafttreten der 2010 veränderte Ausgleichsmechanismusverordnung ein Teil der Abgaben eine „Sonderabgabe“ darstelle, die am Bundeshaushalt vorbeifließt bzw. im Juristendeutsch „haushaltsflüchtig“ ist. Die Situation sei vergleichbar mit dem früheren „Kohlepfennig“ zur Subventionierung des deutschen Steinkohlebaus, der 1994 verboten worden ist. Die auf dieses Gutachten gestützten Klagen verschiedener Unternehmen der Textil- und Modebranche wurden in letzter Instanz vom Bundesgerichtshof abgewiesen, eine anschließend erhobene Verfassungsbeschwerde wurde im Oktober 2014 vom Bundesverfassungsgericht nicht zur Entscheidung angenommen.

Im Februar 2013 wurde die Eröffnung eines Beihilfeprüfverfahrens durch EU-Wettbewerbskommissar Joaquín Almunia angekündigt. Den Unternehmen, die von der Regelung profitiert haben, droht unter Umständen die Rückzahlung bereits gewährter Befreiungen. Im Juli 2013 wurde bekannt, dass der Wettbewerbskommissar ein Beihilfeprüfverfahren bezüglich der Befreiung energieintensiver Unternehmen noch im selben Monat einleiten wollte. Nach einer Intervention durch die Bundesregierung wurde es auf Ende September, nach der Bundestagswahl 2013, verschoben. Am 18. Dezember 2013 wurde die eingehende Prüfung (Untersuchungsverfahren) eingeleitet, um festzustellen, ob die den stromintensiven Unternehmen gewährte Teilbefreiung von der EEG-Umlage in Deutschland mit EU-Beihilfevorschriften im Einklang steht. Zumindest eine Rückzahlung bereits gewährter Rabatte konnte jedoch in den Verhandlungen mit der EU-Kommission abgewendet werden. Restriktivere Regelungen für die Industrierabatte müssen jedoch bis 2018 in Kraft treten.

Am 9. April 2014 stellte die EU-Kommission neue Leitlinien für Beihilfen im Umwelt- und Energiesektor vor. Sie enthalten auch Kriterien dafür, wie energieintensive und besonders dem internationalen Wettbewerb ausgesetzte Unternehmen von Abgaben zur Förderung erneuerbarer Energien entlastet werden können. Während der Bundesverband der Energie- und Wasserwirtschaft die neuen Leitlinien begrüßte, da sie zu Ausschreibungen im Energiesektor verpflichten sollen und damit die Einspeisetarife des EEG untersagen, empfahl der Bundesverband Erneuerbare Energie den Mitgliedsstaaten, gegen die Leitlinien der EU-Kommission vor dem Europäischen Gerichtshof (EuGH) zu klagen, da diese einen Eingriff in die Kompetenz der Mitgliedsstaaten darstellen, im offenen Widerspruch zur EU-Richtlinie für Erneuerbare Energien stehen und die negativen Erfahrungen und kostentreibenden Effekte der dort favorisierten Ausschreibungsmodelle ignorierten. Ein rechtswissenschaftliches Gutachten der Stiftung Umweltenergierecht aus Würzburg beurteilt das EEG unter bestimmten Voraussetzungen für europarechtskonform. Ein Übergang zu Ausschreibungen sei nicht zwingend notwendig.

Laut jüngster Rechtsprechung des Europäischen Gerichtshofs besitzen die Mitgliedsstaaten großen Freiraum, erneuerbare Energien mit nationalen Instrumentarien zu fördern, und verstoßen damit nicht gegen das Diskriminierungsverbot gegen ausländische Investoren. Es liege im Allgemeininteresse, erneuerbare Energien zu fördern sowie Umwelt und Klima zu schützen.

Die Grundzüge der Vergütungen für Anlagen zur Stromerzeugung aus erneuerbaren Energien und Grubengas sind auch im EEG 2012 beibehalten worden:
Das EEG 2012 regelt die Vergütungen für Anlagen, die ab dem 1. Januar 2012 in Betrieb genommen werden (§§ 23–33 EEG 2012). Für bereits in Betrieb befindliche Anlagen gilt grundsätzlich die bisherige Rechtslage fort – vorbehaltlich einiger Übergangsbestimmungen ( EEG). Für Strom aus solarer Strahlungsenergie ist eine Ausnahme mit der Photovoltaik-Novelle eingeführt worden, wonach neben dem regulären Absinken zu Jahresbeginn bei hohem Zubau im Vergleich zum Vorjahr auch zum 1. Juli eine weitere Absenkung erfolgen kann („atmender Deckel“) ( EEG 2012).

Für die einzelnen Energiearten sieht das EEG angepasste Vergütungen je nach Ausbauleistung vor. Die sprungartigen Unterschiede an den Übergängen dieser Vergütungsgruppen werden ausgeglichen, indem die Ausbauleistung einer Anlage in die einzelnen Leistungsgruppen aufgeteilt wird und die Vergütung anteilig errechnet wird (s. nachfolgendes Beispiel "Wasserkraft").

Mit dem EEG 2012 ist die Vergütungsdauer auch für Wasserkraftanlagen einheitlich auf 20 Jahre zzgl. Inbetriebnahmejahr angehoben worden (vorher 15 Jahre für Anlagen über 5 MW) ( EEG). Auch die Unterscheidung von kleiner und großer Wasserkraft bei 5 MW – mit unterschiedlichen Vergütungsregelungen (s. Tab.) – wurde aufgehoben. Die Degression liegt bei einem Prozent pro späterem Jahr der Inbetriebnahme. Für die Erweiterung oder den Neubau sind eine Reihe umwelttechnischer und wasserrechtlicher Voraussetzungen – besonders zum Fischaufstieg – zu erfüllen, die entsprechend behördlich oder gutachterlich nachgewiesen werden müssen (§§ 33 bis 35 und 6 Absatz 1 Satz 1 Nummer 1 und 2 des Wasserhaushaltsgesetzes).

Die im Gesetz genannten Vergütungen sind in Leistungsgruppen unterteilt, die bei der Vergütungsberechnung für den Einzelfall entsprechend der Bemessungsleistung angesetzt werden. Als "Bemessungsleistung" einer Anlage ist im EEG (, Abs. 2a) der "Quotient aus der Summe der in dem jeweiligen Kalenderjahr erzeugten Kilowattstunden und der Summe der vollen Zeitstunden des jeweiligen Kalenderjahres" definiert. Beispiel: Eine Wasserkraftanlage mit 2 MW Anlagenleistung erzeugt bei 4800 Volllaststunden im Jahr 9.600.000 kWh. Die Bemessungsleistung beträgt 1.096 kW (Jahresvolllaststunden* Nennleistung / 8760 h). Hiervon werden 500 kW, also 45,6 %, der Vergütungsgruppe bis 500 kW und die weitere Leistung der Vergütungsgruppe bis 2 MW zugeordnet. Die Durchschnittsvergütung errechnet sich zu 0,456 × 12,7 ct/kWh + 0,544 × 8,3 ct/kWh = 10,31 ct/kWh. Bei einer Inbetriebnahme nach 2012 ist die Degression von 1 % pro Jahr zu berücksichtigen. Eine Anlage, die 2013 in Betrieb genommen wurde, bekommt statt 12,7 ct/kWh noch 12,57 ct/kWh in der Gruppe bis 500 kW.

Die Vergütungen sind den nachfolgenden Tabellen zu entnehmen. Anlagen bis 5 MW erhalten nach Anlage 1 zum EEG für innovative Anlagentechnik einen Bonus von 2,0 Cent/kWh (2004: 2,0). Weitere Boni gibt es bei Deponie- und Klärgas für die Gasaufbereitung.

Die Degression für Grundvergütung und Boni beträgt 1,5 % pro späterem Jahr der Inbetriebnahme ( EEG 2012).
Grubengas stellt keine erneuerbare, sondern eine fossile Energiequelle dar. Die Verbrennung von Grubengas (beispielsweise in einer Stromerzeugungsanlage) stellt aber dennoch ein Klimaschutzmaßnahme dar, da unverbrannt in die Atmosphäre entweichendes Grubengas einen höheres Treibhauspotential als das bei der Verbrennung entstehende Kohlenstoffdioxid. Die Förderung der Grubengasverstromung wurde eingeführt, da sich das Auffangen und die energetische Nutzung von Grubengas ohne finanzielle Förderung nicht lohnt.

Die Grundvergütung ist der nachfolgenden Tabelle zu entnehmen.

Um von den umfangreichen Boni von bis zu 18 Ct/kWh für die Stromerzeugung aus Biomasse zu profitieren, müssen folgende Bedingungen erfüllt sein:

Entsprechen die zur Gewinnung von flüssiger Biomasse verwandten Rohstoffe nicht den Anforderungen des der Biomassestrom-Nachhaltigkeitsverordnung oder stammen die Rohstoffe aus nicht nachhaltigem Anbau, wie dem Regenwald oder Feuchtgebieten, dann wird die Vergütung nach dem EEG hinfällig.

Die jährliche Degression auf der Grundvergütung und der Boni wurde im EEG 2009 um einen halben Prozentpunkt auf 1 % gesenkt (EEG 2004: 1,5 %).

In der EEG-2012-Novelle, die am 30. Juni 2011 beschlossen wurde, wurden folgende Prämien für Biogasanlagen eingeführt:
Ab dem 1. Januar 2012 wurden neu auch kleine Biogasanlagen bis 75 kW mit 25 ct/kWh vergütet.

Im EEG 2009 wurde die Grundvergütung für den Leistungsanteil bis 10 MW auf 16 ct/kWh und für den Leistungsanteil über 10 MW auf 10,5 ct/kWh angehoben. Zudem wurde ein Wärmenutzungsbonus in Höhe von 3 ct/kWh und ein Technologiebonus in Höhe von 4 ct/kWh für nicht-hydrothermale Systeme (beispielsweise Hot-Dry-Rock) eingeführt. Die bis 2011 geltenden Boni für Kraftwärmekopplung und Frühzeitigkeit entfielen durch die Anhebung der Vergütungssätze im EEG 2012. Der Technologie-Bonus für petrothermale Projekte wurde beibehalten und von 4 auf 5 ct/kWh erhöht.

Die jährliche Absenkung der Grundvergütung und des Technologie-Bonus wird im EEG 2012 auf das Jahr 2018 verschoben und beträgt 5 % pro Jahr (EEG 2004/2009: 1 % ab 2010).

Die Vergütungsstruktur für Strom aus Windkraft auf dem Festland ist abhängig vom Ertrag des Standorts gestaltet ( EEG 2014). Nach einer zunächst fünfjährigen Anfangsvergütung wird der anfangs erzielte Ertrag in das Verhältnis zu einer (theoretischen) Referenzanlage gesetzt und daraus für den einzelnen Standort errechnet, wie lange die Anfangsvergütung zusätzlich gewährt wird. Je wind- und damit ertragreicher ein Standort ist, umso kürzer wird diese Vergütungsphase. Die zugehörige Regelung lautete im EEG 2012: „Diese Frist verlängert sich um zwei Monate je 0,75 Prozent des Referenzertrags, um den der Ertrag der Anlage 150 Prozent des Referenzertrags unterschreitet“ ( Abs. 2 EEG 2012). Mit dem EEG 2014 wurde die Regelung zur Anfangsvergütung wie folgt angepasst: „Diese Frist verlängert sich um einen Monat pro 0,36 Prozent des Referenzertrags, um den der Ertrag der Anlage 130 Prozent des Referenzertrags unterschreitet. Zusätzlich verlängert sich die Frist um einen Monat pro 0,48 Prozent des Referenzertrags, um den der Ertrag der Anlage 100 Prozent des Referenzertrags unterschreitet“ ( Abs. 2 EEG 2014).

Sie führt für küstennahe Standorte, die beispielsweise 120 % des Referenzertrages erreichen, zu einer Verlängerung um 27 Monate, so dass die Anfangsvergütung von 8,79 Cent/kWh (Inbetriebnahme Januar 2016) insgesamt 7,25 Jahre gewährt wird. Für Standorte im Binnenland kann eine Unterschreitung des Referenzertrags eintreten, so dass ein Standort mit beispielsweise 90 % des Referenzertrags eine Verlängerung um fast 11 Jahre auf dann insgesamt 16 Jahre erhält. Nach Ablauf dieses Zeitraums wird nur noch die sog. Grundvergütung von 4,89 Cent/kWh gewährt ( Abs. 1 EEG 2014 mit Degression bei einer Inbetriebnahme im Januar 2016). Diese standortabhängige Vergütungsstruktur für Windkraft auf dem Festland führt im Verlauf des 20-jährigen EEG-Vergütungszeitraums zu deutlich unterschiedlichen mittleren Vergütungen.

Die Anfangsvergütung unterliegt nach EEG 2012 einer Degression von 1,5 % pro Jahr, je später die Inbetriebnahme erfolgt ( Abs. 2 EEG 2012). Nach EEG 2014 findet ab dem 1. Januar 2016 eine quartalsweise Anpassung der Vergütungssätze statt ( EEG 2014). Die Höhe der Anpassung hängt vom erfolgten Zubau in einem vorangegangenen 12-Monats-Zeitraum ab. Wurde ein besonders starker Zubau festgestellt, so erhöht sich die Degression auf bis zu 1,2 % pro Quartal, während bei besonders schwachem Zubau auch eine Erhöhung der Vergütung um bis zu 0,4 % möglich ist. Für den Zubau wird ein Zielkorridor von 2,4 GW bis 2,6 GW pro Jahr angestrebt.

Die Ertragsberechnung für die maßgebende Referenzanlage ist in der Anlage 3 zum EEG 2012 geregelt und bezieht im Wesentlichen die vermessene Leistungskennlinie des Anlagentyps und einen Referenzstandort ein, dessen Ertragsbedingungen anhand der mittleren Jahresgeschwindigkeit, der Messpunkthöhe, des Höhenprofils und der Rauhigkeitslänge festgelegt sind.

Im EEG 2012 sind für Windkraftanlagen auf dem Festland zwei Bonuszahlungen geregelt: der "Systemdienstleistungsbonus (SDL-Bonus)" und der "Repoweringbonus". Der "Systemdienstleistungsbonus" von 0,48 Cent/kWh wird für Anlagen gewährt, die die technischen Anforderungen der Systemdienstleistungsverordnung (SDLWindV) erfüllen ( EEG 2012) und vor dem 1. Januar 2015 in Betrieb genommen werden. Auch Bestandsanlagen, die die Voraussetzungen der genannten Verordnung erfüllen und zwischen dem 1. Januar 2002 und dem 31. Dezember 2008 in Betrieb genommen wurden, können diesen Bonus erhalten ( Abs. 1 Nr. 6 EEG 2012) (dann 0,7 Cent/kWh). Die technischen Anforderungen der Verordnung tragen zur Netzstabilität und einem verbesserten Last- und Erzeugungsmanagement bei. Die Systemdienstleistungsverordnung (SDLWindV) wurde am 3. Juli 2009 erlassen.

Der "Repoweringbonus" wird für Neuanlagen gewährt, die mindestens zehn Jahre alte Anlagen ersetzen ( EEG 2012). Um von der Erhöhung profitieren zu können, muss die neue Anlage mindestens doppelt so viel Leistung erbringen und im selben oder in einem angrenzenden Landkreis liegen. Eine weitere Bedingung ist, dass die Anzahl der Anlagen sich nicht erhöht. Unter diesen Voraussetzungen wird ein Repoweringbonus von 0,5 Cent/kWh gezahlt.

Offshore-Anlagen sind Windenergieanlagen, die in einer Entfernung von mindestens drei Seemeilen – gemessen von der Küstenlinie aus seewärts – errichtet werden ( Nr. 9 EEG). Im Herbst 2009 wurde das Testfeld „alpha ventus“ mit zwölf Anlagen der 5-MW-Klasse in Betrieb genommen. Im März 2010 startete der Bau von zwei weiteren Offshore-Windparks. Für 32 weitere Anlagen in der Nord- bzw. Ostsee wurden Genehmigungen erteilt. Weil bis 2009 trotz vorliegenden Genehmigungen kein einziges Vorhaben realisiert wurde, hat man die Vergütungssätze im EEG 2009 deutlich erhöht. In den ersten zwölf Jahren beträgt die Anfangsvergütung für Strom aus Offshore-Windkraftanlagen (Windenergie Offshore: EEG) 15,4 Cent/kWh. Seit dem EEG 2012 ist nach dem Stauchungsmodell auch eine höhere Anfangsvergütung von 19,4 Cent/kWh möglich, wobei sich aber der Vergütungszeitraum von 12 auf 8 Jahre verkürzt. Nach dem Zeitraum der Anfangsvergütung (12 respektive 8 Jahre), werden pro kWh 3,9 Cent/kWh (EEG 2004: 5,95 Cent/kWh) vergütet. Der Zeitraum der Anfangsvergütung verlängert sich in Abhängigkeit von der Entfernung der Anlage zum Festland (ab einer Entfernung von 12 Seemeilen eine Verlängerung um 0,5 Monate je zusätzlicher abgeschlossener Seemeile) und der Wassertiefe (ab einer Wassertiefe von 20 Metern eine Verlängerung um 1,7 Monate je abgeschlossenem zusätzlichen Meter). Beschränkungen bei der Genehmigung von Offshore-Anlagen bestehen primär zugunsten des Naturschutzes und der Sicherheit der Schifffahrt. Eine Degression, die den jährlichen Vergütungssatz bei Offshore-Anlagen ab 2015 um 5 % hätte senken sollen, wurde auf den 1. Januar 2018 verschoben, wobei dann ein erhöhter Prozentsatz von 7 % gilt.
Am 27. März 2010 wurde der erste deutsche Offshore-Windpark alpha ventus mit einer Leistung von 60 MW 45 Kilometer vor der Küste Borkums eröffnet. Bis 2030 sollen in deutschen Gewässern (Nord- und Ostsee) rund 30 Gigawatt (2014 nur noch 15 GW) installiert werden, auch mithilfe von Kooperationen.

Die Regierung Merkel beschloss 2012 die umstrittene Offshore-Haftungsregel. Entschädigungen für fehlende Anschlüsse von Windparks oder bei Störungen von mehr als zehn Tagen werden demnach zu einem Teil auf den Strompreis umgelegt. Die Netzbetreiber haften je nach Grad ihres Verschuldens bei Verzögerungen mit einem Höchstbetrag von 110 Millionen Euro, der Rest wird auf die Stromverbraucher umgelegt. Diese Umlage soll dabei höchstens 0,25 Cent/kWh betragen. Dass dort eine Milliardenlast droht, bestätigt auch die Bundesnetzagentur, die von Kosten für bereits bestehende bzw. schon jetzt abzusehende Verzögerungen von circa 1,6 Milliarden Euro ausgeht. Großverbraucher von über 100.000 kWh sind von der Zahlung der Umlage weitestgehend befreit. Oppositionsparteien hatten die Regel heftig kritisiert. Sie forderten stattdessen, dass der Bund über die KfW-Bankengruppe die Haftung für Schadensersatzansprüche von Seiten der Betreiber übernimmt. Dafür kann dieser entweder Anteile an den Übertragungsnetzbetreibern (ÜNBs) oder an einer Offshore-Netzgesellschaft übernehmen. Der Bund würde damit nicht nur die Risiken übernehmen, sondern könne mittel- und langfristig durch die Anteile der ÜNBs von den Einnahmen über Netzentgelte profitieren. Die Offshore-Haftungsumlage ist im EnWG (Energiewirtschaftsgesetz) gesetzlich geregelt und damit außerhalb des EEG.

Strom aus Photovoltaik machte 2011 21 % der EEG-Gesamtstrommenge und 46 % der gesamten EEG-Förderung aus. 2011 betrug die EEG-Vergütung für Photovoltaik 7,77 Milliarden Euro bzw. durchschnittlich 40,16 Cent/kWh.

Die jeweils neu installierte Leistung stieg von 2001 bis 2010 aufgrund sehr hoher Förderbeträge von Jahr zu Jahr sehr stark an; von 2010 bis 2012 blieb sie auf einem Rekordniveau von ca. 7,5 GWp. Infolge starker Kürzungen der Einspeisevergütungen sank die neu installierte Leistung im Jahr 2013 im Vergleich zu 2012 um 57 % und 2014 im Vergleich zum Vorjahr ebenfalls um 43 %. Das politische Ausbauziel wurde damit 2013 (erstmals seit 2008) eingehalten und nicht übertroffen, jedoch wurde das Ausbauziel 2014 mit 1,95 GW Kapazität an neu errichteten Anlagen nicht erreicht.

Speziell für Photovoltaik-Anlagen werden KfW-Kredite angeboten. Mit der Anlage selbst und der Einspeisevergütung als Sicherheit muss gegebenenfalls kein oder nur wenig Eigenkapital für die Anschaffung der Anlage eingesetzt werden. Die KfW-Kredite für Photovoltaik-Anlagen haben etwas ungünstigere Konditionen als die entsprechenden KfW-Kredite für Investitionen in andere erneuerbare Energien. 

Die bis Januar 2012 gültigen Einspeisevergütungen können der unten stehenden Tabelle entnommen werden (Angaben in Netto-Preisen). Für Photovoltaikanlagen beträgt die Vergütungsdauer 20 Jahre und bleibt vom Jahr der Inbetriebnahme bis zum Ende der Unterstützungsdauer unverändert. Je nach Leistungsklasse (Anlagen auf Gebäuden, im Freien …) werden die Vergütungssätze gestaffelt angewandt und die Vergütung erfolgt prozentual: Bei einer im August 2012 errichteten Dachanlage mit einer Spitzenleistung von 30 kW wird für 10 kW eine Vergütung von 18,73 Cent/kWh gezahlt, für die restlichen 20 kW werden 17,77 Cent/kWh gezahlt, bis Ende 2032.


Bei Fassadenanlagen (genauer: Anlagen, die nicht auf dem Dach oder als Dach eines Gebäudes angebracht sind und einen wesentlichen Bestandteil eines Gebäudes bilden) gab es bis Ende 2008 einen Zuschlag von 5 Cent/kWh, da mit einem geringeren Ertrag zu rechnen ist als bei Dachanlagen; das EEG 2009 hat diesen Zuschlag aber nicht übernommen.


Bei der Novellierung des Gesetzes 2011 wurde die bestehende Degressionsregelung beibehalten, und Maßnahmen zur Netzintegration wurden vorgesehen. Zugleich wurde die „Abregelung“ flexibilisiert: Bei hoher Netzbelastung kann der Netzbetreiber das vorübergehende Herunterregeln der Anlagenleistung vom Betreiber verlangen (dies geschieht automatisiert über einen eingebauten oder nachgerüsteten Abschalter) – dafür erhält der Anlagenbetreiber eine Entschädigung in Höhe von 95 % des entgangenen Ertrags. Die Abregelung ist beschränkt auf maximal 1 % der Anlagen-Jahresleistung.


Ende Juni 2012 wurde daraufhin eine etwas abgeschwächte EEG-Novelle von Bundestag und Bundesrat angenommen (siehe auch nachfolgende Tabelle). Je nach Anlagentyp verringerte sich danach die Vergütung für Anlagen, die zum oder nach dem 1. April 2012 in Betrieb genommen wurden (Definition: siehe § 3 Nr. 5 EEG 2012) um etwa 30 %. Für Dachanlagen, für die vor dem 24. Februar 2012 beim Netzbetreiber ein Netzanschlussbegehren gestellt wurde, gelten Förderungen nach den alten Regeln, sofern die Anlage vor dem 30. Juni in Betrieb ging. Laut Übergangsregelung stehen die alten Vergütungen Freiflächenanlagen zu, bei denen das Planungsverfahren vor dem 1. März begonnen wurde und die spätestens zum 30. Juni ans Netz gehen. Für Freiflächenanlagen auf Konversionsflächen, die mit aufwändigen Planungsverfahren verbunden sind, galt die Förderung nach den alten Regeln noch bis zum 30. September.

Zum 1. April 2012 wurde die Eigenverbrauchsvergütung abgeschafft, da der Vergütungssatz für eingespeisten Solarstrom mittlerweile unter den dem privaten Endverbraucher von den Energieversorgungsunternehmen in Rechnung gestellten Bezugspreis gefallen war und damit der Eigenverbrauch von solar erzeugtem Strom auch ohne Förderung günstiger ist als Einspeisung mit anschließendem Netzbezug.


Das EEG 2014 sieht zudem ein Pilotmodell zur Erprobung von Ausschreibungen für Photovoltaik-Freiflächenanlagen vor. Die erste Ausschreibungsrunde endete am 15. April 2015. Es gingen insgesamt 170 Gebote ein, so dass das Ausschreibungsvolumen von 150 MW mehrfach überzeichnet ist. Der Bundesverband Erneuerbare Energie befürchtet, dass Bürgergenossenschaften und -anlagen vom Markt verdrängt werden, da sie aufgrund geringerer Kapitalstärke weniger Vorleistungen auf sich nehmen und weniger Risiken tragen können.

Die EEG-Umlage 2016 steigt um 3 Prozent gegenüber dem Vorjahr an und beträgt damit 6,354 Cent je Kilowattstunde. Die EEG-Umlage 2015 sank noch gegenüber dem Vorjahr auf 6,17 Cent je Kilowattstunde (2014: 6,24 Cent; 2013: 5,28 Cent; 2012: 3,59 Cent; 2011: 3,53 Cent). Die Mehrkosten speisen sich größtenteils aus der steigenden Differenz zwischen den sinkenden Börsenstrompreisen und der fixen Einspeisevergütung sowie den zunehmenden Ausnahmeregelungen für die Industrie. Der Ausbau der erneuerbaren Energien trug währenddessen 15 % zur letzten Erhöhung 2014 bei. Die Einspeisevergütungen insbesondere für Solarstrom wurden in den letzten Jahren fortlaufend abgesenkt. Studien des Bundesumweltministeriums sowie der Netzbetreiber erwarten, dass die Umlage sich trotz fortschreitendem Ausbau in den kommenden Jahren stabilisieren und anschließend wegen sinkender Vergütungssätze fallen wird. Analysen zeigen, dass auch eine Verlangsamung der Energiewende die Kosten der Erneuerung des Kraftwerksparks nicht mindern würden, sofern an den Klimazielen festgehalten wird.
Ausnahmeregelungen bestehen für energieintensive Unternehmen, mit der Folge einer Umverteilung der Förderkosten zulasten kleiner und mittlerer Unternehmen sowie der Privathaushalte. Diese Regelung erregt verstärkt Kritik. Die Bundesnetzagentur erklärte 2012, sie betrachte diese Entwicklung „mit Sorge“. Andere Kritiker weisen auf eine Privilegierung auch für solche Unternehmen hin, die sich nicht unter Wettbewerbsdruck befinden, zulasten aller nicht-privilegierten Verbraucher. Nach Angaben des Deutschen Instituts für Wirtschaftsforschung machten die gesamten Stromkosten im Schnitt ca. 3 % des Umsatzes eines Unternehmens aus, wovon die EEG-Umlage nur gering beitrage. Bei stromintensiven Unternehmen liegen die Kosten höher, allerdings seien diese von EEG-Umlage und Ökosteuer größtenteils ausgenommen, um Wettbewerbsnachteile zu vermeiden. Energieintensive Branchen profitierten zudem derzeit von historisch niedrigen Preisen an der Strombörse.
Der jüngste Anstieg der EEG-Umlage ist nicht auf die steigenden Förderkosten, sondern auf verzerrende Effekte zurückzuführen. So sanken im Zuge der Wirtschaftskrise und des Merit-Order-Effekts die Strompreise an der Börse, sodass die Differenz zu den fixen Einspeisetarifen größer wurde und damit einhergehend die Förderkosten scheinbar stiegen. Zudem ist der Anstieg der EEG-Umlage auf eine Änderung bei der Berechnungsgrundlage zurückzuführen (Einführung der Liquiditätsreserve, Einführung der Marktprämie, Ausweitung der Ausnahmetatbestände für Industrie, Einberechnung bislang gesondert ausgewiesener Netzentgelte). Ohne diese Modifikation wäre die EEG-Umlage beispielsweise im Jahr 2011 gesunken statt gestiegen. Dieser Befund wurde von mehreren Studien, welche die Zusammensetzung der EEG-Umlage aufschlüsseln, übereinstimmend bestätigt. Im Jahr 2014 verursachte der gefallene Börsenpreis Zusatzkosten von 1,45 Cent pro Kilowattstunde und die Industrieprivilegien noch einmal 1,25 Cent. Die vorläufigen Werte für die Fremdkosten im Jahr 2015 (Börsenpreis: 1,83 Cent, Industrieprivilegien: 1,34 Cent) bringen die eigentliche Förderung der erneuerbaren Energien und den Zuwachs der EEG-Umlage in ein immer größeres Missverhältnis.

Aufgrund der zahlreichen verzerrenden Effekte gilt die EEG-Umlage nicht als valider Indikator für die Kosten der erneuerbaren Energien. Das Öko-Institut gab daher den "Energiewende-Kosten-Index" (EKX) heraus, der die EEG-Umlage um die verzerrenden Effekte bereinigen soll und im Gegenzug weitere Kostenfaktoren (wie beispielsweise die Förderung der Kraft-Wärme-Kopplung) miteinbeziehe. Demnach beruhe der Zuwachs der Stromkosten zwischen 2003 und 2012 zu über 50 % auf höheren Brennstoffpreisen und industriepolitischen Umverteilungseffekten.

Die Fotovoltaik macht den größten Kostenanteil der EEG-Umlage aus, da sie anfangs mit hohen Vergütungssätzen gefördert wurde und der Zubau schneller erfolgte als geplant. Da die Vergütung aber für 20 Jahre gesetzlich garantiert wurde, zieht dies beträchtliche Zahlungsverpflichtungen nach sich. Inzwischen wurden die Vergütungen in mehreren Schritten stark nach unten angepasst, sodass heute neu zugebaute Anlagen ungleich niedrigere Kostenbelastungen hervorrufen. Zudem sieht ein „atmender Deckel“ vor, dass die Vergütungen automatisch umso stärker gekürzt werden, je mehr weitere Kapazitäten zugebaut werden. Prognos rechnete bereits gemäß den Anfang 2012 gültigen Preisregelungen mit einem Anstieg der Strompreise um nur knapp zwei Prozent als Folge eines Solarwachstums von 70 % in den vier Jahren bis 2016. Über die schrittweise Absenkung der Solarstromtarife kam es zu kontroversen Debatten quer durch die politischen Lager, Wirtschaft und Wissenschaft.

Bei einer repräsentativen Umfrage von TNS Infratest im Auftrag der Agentur für Erneuerbare Energien im Oktober 2012 hielten 51 % der Bürger eine EEG-Umlage von 5 ct/kWh für „zu hoch“, während 46 % sie für „angemessen“ oder „zu niedrig“ erachteten. Noch 2011 hatten mehr als 75 % die damalige EE-Umlage von 3,5 ct/kWh für „angemessen“ oder sogar „zu niedrig“ betrachtet. In einer weiteren Umfrage sprachen sich 20 % der Befragten für eine Beibehaltung des EEG in seiner derzeitigen Form und 60 % für eine Reform des EEG aus.

Die Internationale Energieagentur (IEA) notiert mit Blick auf die Kosten des EEG in ihrem Länderbericht 2013 zu Deutschland: „Die Kostenauswirkungen des EEG müssen im Kontext der allgemeinen Entwicklungen im Energiesektor bewertet werden. Der jüngste Strompreisanstieg bereitet vor allem Haushalten mit geringem Einkommen Schwierigkeiten, wohingegen Großverbraucher von der Umlage weniger betroffen sind und zugleich in den Genuss der durch die erneuerbaren Energien herbeigeführten Senkung der Großhandelstarife kommen. Zudem erhöht sich die Energiearmut auch durch den starken Anstieg der Kosten fossiler Brennstoffe. Kosten und Nutzen der erneuerbaren Energien müssen fair und transparent verteilt werden.“

In der Vergangenheit hatte es mehrfach Prognosen zur Entwicklung der EEG-Umlage gegeben, die sich im Nachhinein häufig als nicht zutreffend erwiesen. Hierbei ist allerdings zu berücksichtigen, dass die EEG-Umlage alleine keinen validen Maßstab für die Kostenentwicklung der erneuerbaren Energien darstellt, da die Umlage auch verzerrende Effekte wie die tendenziell gewachsene Differenz zwischen Einspeisevergütung und Börsenstrompreis sowie die Verteilungswirkungen von Ausnahmetatbeständen wie beispielsweise die Entlastung von Unternehmen beinhaltet. Zudem wurden über die Jahre auch weitere Kostenfaktoren eingeführt, wie beispielsweise die kontrovers diskutierte Marktprämie. Die Kosten für Netzausbau, Reservekraftwerke und Energiespeicher sind in der EEG-Umlage nicht enthalten. Ein direkter Vergleich ist daher problematisch. Eine wichtige Ursache war überdies, dass die meisten Prognosen das Ausmaß des Zubaus von EE-Anlagen unterschätzten, insbesondere der zunächst relativ hoch geförderten Photovoltaik.

Nach den Regelungen des EEG genießen erneuerbare Energien Vorrang bei der Einspeisung ins Stromnetz. Dadurch verschiebt sich die Nachfragekurve: Erneuerbare Energien reduzieren die Nachfrage nach konventionellem Strom mit höheren Grenzkosten. Die Börsenstrompreise sinken oder bleiben trotz gestiegener Rohstoffpreise konstant. Dieser komplexe Preisbildungsmechanismus an der Strombörse, der sog. Merit-Order-Effekt, sparte im Jahr 2010 etwa 2,8 Mrd. Euro ein. Davon profitierte insbesondere die energieintensive Industrie. Nach einem Gutachten vom April 2012 müsste der Strompreis 2 Cent/kWh niedriger liegen, wenn die Versorger die gesunkenen Einkaufskosten weitergegeben hätten. Da sich der Kraftwerkspark langfristig dem Ausbau der erneuerbaren Energien anpasse und somit die Überkapazitäten zurückgingen, ist anderen Wissenschaftlern zufolge dieser Effekt strittig. Gleichzeitig ändern sich die Anforderungen an den Kraftwerkspark. Auf Grund der volatilen Einspeisung der EEs werden bei neuen Investitionen mehr Spitzen- und Mittellastkraftwerke realisiert um die Schwankungen der EEs auszugleichen. Im Vergleich zu Grundlastkraftwerken haben diese Kraftwerksarten höhere variable Kosten, welche den Börsenstrompreis erhöhen. Auf Grund der Unsicherheitsfaktoren lässt sich daher nicht bestimmen ob es langfristig zu einem positiven oder negativen Effekt kommt.

Aufgrund dieses Effekts profitiert die energieintensive Industrie im Vergleich zu europäischen Konkurrenten von günstigen Börsenstrompreisen. So kündigte ein niederländischer Aluminiumproduzent die Schließung einer Hütte an, da diese mit dem billigen Industriestrom in Deutschland nicht mehr konkurrieren könne. Dagegen entschied sich die Hydro Aluminium, ihre Produktion am Standort Grevenbroich deutlich auszuweiten. Der französische Industrieverband Uniden forderte im März 2014 von der französischen Regierung eine Preisbegrenzung für Atomstrom, da die Stromkosten für große industrielle Abnehmer in Deutschland bald um 35 % niedriger lägen als in Frankreich. Laut dem deutschen Verband der Industriellen Energie- und Kraftwirtschaft (VIK) lagen die Stromkosten für industrielle Abnehmer Anfang 2014 auf dem Niveau von zehn Jahren zuvor.

In einer Studie im Auftrag von Siemens stellten Wissenschaftler der Universität Erlangen fest, dass die Stromkosten in Deutschland ohne erneuerbare Energien deutlich höher lägen. So sparten nach der Studie die deutschen Stromverbraucher im Jahr 2013 insgesamt 11,2 Milliarden Euro. Zwar erhöhe die EEG-Umlage den Strompreis. Die erneuerbaren Energien würden aber auch den Strompreis an der Strombörse durch größere Konkurrenz deutlich senken, so dass die deutschen Stromverbraucher unter dem Strich günstiger wegkämen als ohne erneuerbare Energien.

Im Jahr 2010 vermied die erneuerbare Stromerzeugung fossile Energieimporte im Wert von rd. 2,5 Mrd. Euro; hiervon sind etwa 80 % dem EEG zuzuschreiben. Demgegenüber standen im gleichen Jahr Netto-Importe im Wert von 6,1 Milliarden Euro für Solarzellen- und Module. Die Windenergiebranche wies 2011 wie auch im Vorjahr einen Exportüberschuss von ca. 66 % auf, was Nettoexporten von 4 Mrd. Euro entspricht. Allerdings ist hierbei zu berücksichtigen, dass die Lebensdauer von Photovoltaikanlagen 25 Jahre und die von Herstellern unterstellte Lebensdauer von Windkraftanlagen 20 Jahre beträgt.

Als Effekt des EEG sind vielerorts Bürgersolaranlagen errichtet worden, bei denen Privatleute Kapital für bis zu 20 Jahre in diese Anlagen investieren. Bürgersolaranlagen (oder Bürgerkraftwerke) werden dabei häufig auf kommunalen Gebäuden errichtet, wodurch die Gemeinden Mieteinnahmen generieren für Flächen (Gebäudedächer), die ansonsten nicht wirtschaftlich genutzt werden konnten.

Die Energiewende spielt sich vor allem im ländlichen Raum ab, worauf die Agentur für Erneuerbare Energien hinweist. Rund 16 Milliarden Euro haben die Landwirte laut Schätzungen des Deutschen Bauernverbandes von 2009 bis 2012 in erneuerbare Energien investiert. Der ländliche Raum nimmt demnach für die dezentrale Versorgung Deutschlands mit umweltfreundlicher Energie schon heute eine Schlüsselrolle ein, wie die Landwirtschaftliche Rentenbank feststellt. Die Erzeugung von Strom, Kraftstoffen und Wärme werde für immer mehr Landwirte zu einem zweiten Standbein, das angesichts volatiler Agrarmärkte für eine zweite Einkommensquelle sorgt. Gemeinschaftsprojekte zum Ausbau erneuerbarer Energien schafften darüber hinaus Partnerschaften zwischen Landwirten und ihren Nachbarn im ländlichen Raum.

Die klimapolitische Wirksamkeit des EEGs wird von vielen bestritten, da ein Interaktionsmechanismus mit dem EU-Emissionshandel die Klimaschutzwirkung des EEG zunichtemache. Innerhalb der EU sind die CO-Emissionen durch den EU-Emissionshandel gedeckelt. Deswegen, so die Kritik, würden die durch das EEG gesenkten CO-Emissionen nur an andere Stellen verlagert; denn was die einen weniger ausstoßen, dürfen andere mehr ausstoßen, weil die absolute Obergrenze der Emissionen gleich bleibt. Europaweit würden also keine Emissionen vermieden. Diese Kritik wird insbesondere vom Sachverständigenrat zur Begutachtung der gesamtwirtschaftlichen Entwicklung, dem wissenschaftlichen Beirat beim Bundesministerium für Wirtschaft und Technologie, der Monopolkommission, dem Ifo Institut für Wirtschaftsforschung, der Deutschen Akademie der Technikwissenschaften und von André Schmidt von der Universität Witten/Herdecke in einer vom Bundesministerium für Bildung und Forschung in Auftrag gegebenen Studie vertreten, wonach das EEG nicht für mehr Klimaschutz sorge, sondern diesen nur deutlich teurer mache, da das EEG mit dem EU-Emissionshandel kollidiere. Das Weltklimarat der Vereinten Nationen (IPCC) bestätigt im Fünften Sachstandsbericht, dass in einem Emissionshandelssystem mit einer hinreichend strengen Deckelung andere Maßnahmen wie die Subventionierung erneuerbarer Energien keinen weiteren Einfluss auf den gesamten CO-Ausstoß hätten. Andererseits sei aber der Emissionshandel nicht wirksam, wenn der Deckel zu schwach sei. Auch Vertreter des Bundesverbands Erneuerbare Energie bestätigen, dass es zwischen EEG und Emissionshandel „eine gewisse Widersprüchlichkeit“ gebe.

Nach Auffassung des Deutschen Instituts für Wirtschaftsforschung (DIW) trifft die Kritik an den Interaktionseffekten zwischen EEG und Emissionshandel nur bei einer „rein statischen Betrachtung“ zu. Das EEG sei ein äußerst wirksames Instrument zur Förderung der Stromerzeugung aus erneuerbaren Energien und fördere den Klimaschutz, wenn Emissionshandel und EEG „gut aufeinander abgestimmt werden“. Auch das Bundesministerium für Umwelt, Naturschutz und Reaktorsicherheit weist die Kritik am EEG ebenfalls zurück. Wissenschaftler schlagen vor, die CO-Emissionshandelsvolumina entsprechend den Zielvorgaben des EEG anzupassen, um ungewünschte Interaktionseffekte zu vermeiden.

Durch die dezentrale Einspeisung sinken für die Netzbetreiber die variablen Kosten. Die Verbraucherzentrale hat im Jahr 2007 bemängelt, dass diese Kostenersparnis nicht dem Endkunden gutgeschrieben werde.
Andererseits ist Aufkommen von Windenergie und Photovoltaik regional unterschiedlich verteilt, was zu zusätzlichem Investitionsbedarf beim Ausbau des Stromnetzes führt.
Der Bundesverband der Energie- und Wasserwirtschaft (BDEW) schätzt die jährlichen Kosten für den Ausbau des Stromnetzes zur Aufnahme erneuerbarer Energien auf 3,4 – 4,5 Mrd. Euro. Noch nicht enthalten sind hierin die Netzanschlusskosten von Offshore-Windparks.

Laut DIW haben erneuerbare Energien auch unter Berücksichtigung der Förderkosten für die erneuerbaren Energien und der Verdrängungseffekte im konventionellen Energiesektor einen positiven Netto-Effekt auf das Wachstum in Deutschland. Denn der Ausbau löst Investitionen aus, die sich positiv auf die Einkommen auswirken. Hinzu kommen Einsparungen durch verringerte Importe fossiler Energien und erhöhte Einnahmen durch den Export von Erneuerbare-Energien-Anlagen und Komponenten. Diese Effekte überkompensieren deutlich die Auswirkungen durch die sinkenden Investitionen in konventionelle Energien. Auch die Nettobeschäftigungseffekte sind unter dem Strich positiv.

Da die erneuerbaren Energien mit Vorrang ins Stromnetz eingespeist werden, während die konventionellen Kraftwerke mit gleichen Fixkosten weiter betrieben werden müssen, sehen sich die Betreiber der bereits existierenden Kraftwerke in ihren Eigentumsrechten eingeschränkt. So hat beispielsweise der Vorsitzende des Energieversorgers EWE diesen Umstand als „nichts anderes als eine schleichende Enteignung konventioneller Kraftwerke“ bezeichnet.

Neben seinen ökologischen Zielsetzungen wird das EEG explizit auch als strategische Industriepolitik verstanden, um Arbeitsplätze zu schaffen und neue Märkte und Exportbereiche zu erschließen. Die Zahl der Beschäftigten in der Erneuerbare-Energien-Industrie hat sich von 2006 bis 2008 nahezu verdoppelt. Die Bruttobeschäftigung durch erneuerbare Energien betrug 278.000 Arbeitsplätze im Jahr 2008 und im Jahr 2009 bereits 340.000 Arbeitsplätze. Im Jahr 2020 sollen nach Prognosen des BMU über 400.000 Menschen in Deutschland im Bereich erneuerbare Energien beschäftigt sein. Erneuerbare Energien sind dezentral verteilt und daher arbeitsintensiver und damit teurer als zentrale Großkraftwerke, weswegen sie bei gleicher Produktionsmenge weitaus mehr Arbeitsplätze schaffen und höhere Preise verursachen als die konventionelle Energieproduktion. Nach Ansicht der Erneuerbare-Energien-Branche ist das EEG ein wirksames Instrument der Mittelstandsförderung.

Dem gegenüberzustellen ist jedoch der mögliche Wegfall von Arbeitsplätzen im Bereich der konventionellen Energien aufgrund der Konkurrenz von vorrangig eingespeistem Ökostrom, sowie in Teilen der übrigen Wirtschaft aufgrund der erhöhten Stromkosten. Allerdings können Unternehmen des verarbeitenden Gewerbes unter bestimmten Bedingungen Ausnahmen von der EEG-Umlage beantragen, weswegen für sie dann nur relativ geringe Mehrkosten entstehen. 2013 taten dies etwa 2000 Unternehmen mit zusammen 300.000 bis 400.000 Beschäftigten. Diese Unternehmen stellen zusammen 0,1 % aller Unternehmen in Deutschland dar und beschäftigen etwa 1 % aller Beschäftigten. Größere Unternehmen, die ihren Strom direkt an der Börse kaufen, profitieren zudem von deutlich niedrigeren Strompreisen an der Börse.<ref name="BMU vs RWI 10/09"> aufgerufen am 16. November 2009. F. Staiß, M. Kratzat (ZSW), J. Nitsch, U. Lehr (DLR), D. Edler (DIW), C. Lutz (GWS): (PDF; 5,1 MB) Forschungsvorhaben im Auftrag des Bundesministeriums für Umwelt, Naturschutz und Reaktorsicherheit (BMU), Juni 2006, S. I-8 bzw. S. 14 der PDF-Zählung</ref>

Die Entwicklung des EEG lässt sich von Beginn an mit den Jahresabrechnungen nachvollziehen, die von den vier deutschen Übertragungsnetzbetreibern (ÜNB) vorgelegt werden, die mit der Vermarktung des EEG-Stroms betraut sind.
Darin sind die jährlich erzeugten EEG-Strommengen, die dafür gezahlten Vergütungen an die Betreiber und die jeweilige EEG-Umlage dokumentiert (s. nachfolgende Tabellen). Aus der EEG-Strommenge und aus den dafür gezahlten Vergütungen ergibt sich die durchschnittliche Vergütung für EEG-Strom aus den unterschiedlichen erneuerbaren Energiequellen.

Die Strommenge aus erneuerbaren Energiequellen ist seit Einführung des EEG im Jahr 2000 auf mehr als das 13-fache von rd. 10.000 GWh/a auf 136.000 GWh/a im Jahr 2014 gestiegen (Stand Juli 2015). Bis 2010 wurde die sogenannte EEG-Quote angegeben, die den Anteil der EEG-geförderter Strommenge an dem nicht-privilegierten Letztverbrauch an Strom kennzeichnet. Sie lag 2010 bei 20 %.

Die durchschnittliche Vergütung für EEG-Strom ist seit dem Jahr 2000 von 8,5 ct/kWh auf 17,9 ct/kWh im Jahr 2011 gestiegen. Sie stieg dabei in den Anfangsjahren bis 2004 im Mittel nur um etwa 2,5 % pro Jahr, in der vier Folgejahren bis 2008 um rund 7 % pro Jahr, von 2009 bis 2011 jedoch jeweils um gut 13 %, so dass sich über den gesamten Zeitraum ein Mittelwert von rund 7 % im Jahr ergibt. Die durchschnittlichen Vergütungen bei den einzelnen Energiequellen reichten 2011 von 2 ct/kWh für „Gas“, 4,8 ct/kWh für „Wasser“ und 8,6 ct/kWh für „Wind onshore“ über 15 ct/kWh für „Wind offshore“, 16 ct/kWh für „Biomasse“ und 20,7 ct/kWh für „Geothermie“ bis 40,2 ct/kWh für „Solar“. Die mittlere Vergütung für Solar-EEG-Strom ist in den letzten Jahren deutlich rückläufig nach dem Höchstwert im Jahr 2005 mit 53 ct/kWh.

Mit dem EEG sind den vier deutschen Übertragungsnetzbetreibern (ÜNB) auch Pflichten zur Erstellung von Prognosen übertragen worden. Neben der jährlich vorausgreifenden Prognose für die EEG-Umlage werden mittelfristige Prognosen über einen Zeitraum von fünf Jahren erstellt.

Die durchschnittliche Einspeisevergütung im Jahr 2013 lag bei 12,5 Ct (gegenüber 25 Ct im Jahr 2010). Damit war das von Bundeswirtschaftsminister Sigmar Gabriel verfolgte Ziel, die durchschnittliche Vergütung bis 2015 auf 12 Ct zu senken, bereits vorzeitig beinahe erreicht.

Das EEG gilt insbesondere bei Umweltverbänden wie dem BUND und Greenpeace, Branchenvertretern wie dem Bundesverband Erneuerbare Energie und dem Bundesumweltministerium als weltweit erfolgreichstes Instrument zur Förderung erneuerbarer Energien. Auch eine Studie der Deutschen Energieagentur (dena) im Auftrag des Bundesverbands der Deutschen Industrie (BDI), die 2013 veröffentlicht wurde, empfiehlt eine Beibehaltung des EEG, schlägt jedoch eine systematische Reform vor.

Das Deutsche Institut für Wirtschaftsforschung (DIW), die EU-Kommission, die Internationale Energieagentur (IEA) sowie das Energieunternehmen EnBW lobten das EEG als hoch wirksam und wirtschaftlich effizient.

Die durch das EEG gesetzten wirtschaftlichen Anreize führten zur Gründung einer Vielzahl von Bürgerenergiegenossenschaften (Ende 2013 waren es 888).

Die Internationale Energieagentur (IEA) urteilte in ihrem Länderbericht 2013 über die deutsche Energiepolitik, das EEG habe sich „als sehr wirksames Instrument zur Verbreitung der erneuerbaren Energien und insbesondere der Stromerzeugung durch Biomasse, Windenergie und Photovoltaik erwiesen. Zudem habe es sich als erfolgreich bei der Drosselung der Kosten erwiesen, wie sich im besonderen Maße an der Senkung der Einspeisetarife für Solarstrom zeigt, zu der es in Antwort auf die rasche Expansion dieser Technologie in den letzten vier Jahren gekommen ist.“

Die Prinzipien des EEG wurden bis Anfang 2012 von mindestens 65 Länder weltweit übernommen. So basiert beispielsweise der Erfolg der erneuerbaren Energien in Spanien auf einem ähnlichen Mindestpreissystem wie in Deutschland. Nach dem KKW-Unfall in Fukushima 2011 haben auch Japan und China feste Einspeisetarife und Vorrangregelungen eingeführt, die sich stark am deutschen EEG orientieren. Auch in der deutschen Bevölkerung genießt das EEG großen Rückhalt. Im Jahr 2012 sprachen sich bei einer Umfrage im Auftrag der Agentur für Erneuerbare Energien vier von fünf Bürgern für die Beibehaltung des EEG aus; 20 % wollen es so behalten, wie es derzeit ist, 60 % halten eine Überarbeitung des EEGs für notwendig. Nach dem Vorbild Deutschlands wurde im Juli 2015 auch in Frankreich ein entsprechendes Gesetz beschlossen.

Viele Kritiker bestreiten, dass das EEG einen Nutzen für den Klimaschutz bringt. Vielmehr verteure es den Klimaschutz nur unnötig (siehe hierzu die Ausführungen im Abschnitt Klimaschutz).

Ferner wurde kritisiert, dass durch das Prinzip der Grundvergütung unabhängig vom Bedarf und dem daraus folgenden Marktpreis Strom erzeugt worden ist. Erst seit 2012 wurde durch die Flexibilitätsprämie der Grundvergütung eine bedarfsorientierte Komponente zugefügt.

Der wissenschaftliche Beirat beim Bundesministerium für Wirtschaft und Energie beurteilte in zwei Gutachten von 2004 und 2012 die industriepolitische Motivation des EEG kritisch, da die Privilegierung bestimmter Technologien die Wahlfreiheit von Marktpartnern einschränke und dadurch zu ineffizienten und unwirtschaftlichen Allokationseffekten führe. So verursache die Photovoltaik zwar 55 % der Förderkosten, liefere aber nur 20 % der Stromerzeugung der erneuerbaren Energien. Das Ziel des Klimaschutzes überzeuge nicht, da dieses über eine Modernisierung des konventionellen Kraftwerkparks und über den sog. Mechanismus für umweltverträgliche Entwicklung effizienter erreicht werden könne.

Die vom deutschen Bundestag eingesetzte Expertenkommission Forschung und Innovation riet in ihrem Jahresgutachten 2014 das EEG komplett abzuschaffen, da das EEG weder ein kosteneffizientes Instrument für den Klimaschutz sei, noch eine messbare Innovationswirkung entfalte. Diese Beurteilung wurde anschließend vom Fraunhofer-Institut für System- und Innovationsforschung teilweise als falsch zurückgewiesen, wobei ein Veränderungsbedarf am EEG durchaus anerkannt wurde.

Die Initiative Neue Soziale Marktwirtschaft, eine von der Metall- und Elektroindustrie getragene Public-Affairs-Agentur, warnte vor einer Kostenexplosion vor allem für Privathaushalte sowie kleine und mittlere Unternehmen. Als Gegenmodell schlug sie ein Quotenmodell vor, das die Kosten der Energiewende in den kommenden acht Jahren um 52 Mrd. Euro senken solle. Auch der Vorsitzende des Sachverständigenrats zur Begutachtung der gesamtwirtschaftlichen Entwicklung, Christoph M. Schmidt, plädierte für ein Quotenmodell. Sein Ratskollege Peter Bofinger kam in einer Studie 2013 jedoch zu einer anderen Bewertung.

Nach Ansicht von Rechtsexperten „ist das Regelwerk so ausdifferenziert“ und komplex geworden, dass die bisher übliche Kommentierung nicht mehr ausreicht. Eine Vielzahl von Verordnungen zum EEG wird daher inzwischen unter dem Stichwort „EEG II“ zusammengefasst.

Zum Mindestpreissystem des EEG standen als konkurrierende Systeme das "Quotenmodell" sowie das "Ausschreibungsmodell" zur Diskussion. Beim Quotenmodell setzt der Staat eine Quote an erneuerbaren Energien fest, die von den Energieversorgern produziert werden muss. Um die Einhaltung der Verpflichtung zu überprüfen, werden dann für erneuerbar erzeugten Strom Zertifikate vergeben, die von den Energieversorgern untereinander gehandelt werden können. Beim Ausschreibungsmodell wird hingegen eine bestimmte Menge an Regenerativstrom ausgeschrieben, wobei der Gewinner der Ausschreibung eine befristete Abnahmegarantie erhält. Beide Ansätze gelten als Mechanismen der Mengensteuerung, da die Förderung auf eine bestimmte Zubaumenge ausgerichtet ist und die Menge nicht wie bei Mindestpreissystemen indirekt durch den Preis gesteuert wird. Die Umstellung des Erneuerbare-Energien-Gesetz auf eine Mengensteuerung wird durch diverse Fachleute und Wissenschaftler befürwortet. Dazu gehörte zuvorderst die Monopolkommission und die Wirtschaftsweisen.<ref name="Jahresgutachten 2011/2012">"Kapitel vier aus dem Jahresgutachten des Sachverständigenrates zur Begutachtung der gesamtwirtschaftlichen Entwicklung 2011/2012" (PDF).</ref> Befürworter der alternativen Modelle verweisen gewöhnlich auf die höhere Kosteneffizienz durch die zielgenauere und meist technologieneutrale Steuerung der Zubaumenge. Auf diese Weise würden Mitnahmeeffekte vermieden, wenn etwa die Kosten für eine Technologie sinken und durch die steigende Gewinnspanne (Unterschied zwischen Stückkosten und fixer EEG-Einspeisevergütung) für Produzenten von erneuerbarem Strom ein sehr teurer Zubau auf Kosten der Energieverbraucher erfolge.

Tatsächlich sind die Vorteile von Quoten- und Ausschreibungsmodellen im Vergleich zum EEG-Mindestpreissystem (Einspeisevergütung) jedoch politisch umstritten. Andere Fachleute und Hersteller von geförderter Produktionsanlagen bzw. deren Verbände kritisieren sowohl Quoten- als auch Ausschreibungsmodelle und heben regelmäßig die Vorteile der EEG-Förderung durch Mindestpreise hervor. Sie führen an, dass sich nach einer Studie des Massachusetts Institute of Technology (MIT) beide Ansätze als weniger wirksam und weniger wettbewerbsfreundlich als Mindestpreissysteme erwiesen hätten. In Ländern mit solchen Systemen gäbe es aufgrund mangelnder Investitionssicherheit meist keine eigene Herstellerindustrie, und die Kosten für den Ausbau der erneuerbaren Energien sind hoch, weil das erhöhte Investitionsrisiko in die Preise einkalkuliert wird (beispielsweise Großbritannien, Italien). Einzelne Länder, wie beispielsweise Irland oder Großbritannien, haben inzwischen auf Mindestpreissysteme umgestellt oder nutzen diese ergänzend. Im Bezug auf das britische Beispiel zeigt die Monopolkommission in ihrem Energiesondergutachten allerdings auf, dass sich das neue britische Fördersystem vom deutschen Mindestpreissystem erheblich unterscheidet und zudem Elemente einer Mengensteuerung beinhaltet.

Der Bundesverband Erneuerbare Energie, der die Interessen der Branche vertritt, bewertete Quotenmodelle derweil als „rückwärtsgewandt“, „mittelstandsfeindlich“, „teuer“ und „unbrauchbar“. Auch ein Gutachten der Universität Würzburg unter der Federführung des Wirtschaftsweisen Peter Bofinger vom September 2013 kam zu dem Schluss, dass Quotenmodelle in Deutschland für Investoren zu große Risiken bergen. Er plädiert jedoch für ein um einen Auktionsmechanismus erweitertes EEG (Ausschreibungsmodell). Derweil werden auch Ausschreibungen kritisiert, deren theoretisch höhere Kosteneffizienz möglicherweise durch deutlich höhere Transaktions- und Finanzierungskosten konterkariert werden könnte. Im Ergebnis könne dies sogar höhere Kosten für die Erreichung der Ausbauziele zur Folge haben.

Kritiker der Alternativsysteme argumentieren, dass auch ein empirischer Vergleich der Fördersysteme in der EU darauf hinweise, dass in Ländern mit Einspeisevergütungen die regenerative Stromerzeugung günstiger und wirksamer ist als bei Einsatz von Quoten oder Ausschreibungen. In Ländern mit Einspeisevergütung wie Deutschland, Spanien, Frankreich oder Portugal seien etwa die Kosten für Onshore-Windstrom wesentlich geringer als in Staaten mit Quotenregelung wie Großbritannien, Polen, Belgien oder Italien. Allerdings wird hierbei stets mit der Technologie verglichen die im EEG die niedrigste Einspeisevergütung erhält. Kritiker von Ausschreibungsmodellen führen weiter an, dass ausländische Erfahrungen zeigten, dass diese keinesfalls zu verlässlichen Zubaumengen führen müssen. Die Probleme mit real existierenden Ausschreibungssystemen hätten in einigen Ländern bereits dazu geführt, dass Ausschreibungssysteme wieder abgeschafft wurden. Laut einer Umfrage von EuPD Research unter den Begünstigten den EEG-Förderung sähen fast drei Viertel der befragten Erneuerbare-Energien-Unternehmen in Einspeisevergütungen das geeignete Instrument für Klimaschutz, Markteinführung und Technologieentwicklung der Stromerzeugung aus erneuerbaren Energien. Quotensysteme hielten dagegen nur zwei Prozent der Unternehmen für sinnvoll. Übereinstimmend haben Berechnungen der Beratungsgesellschaft Ernst & Young sowie der University of Cambridge ermittelt, dass feste Einspeisetarife hinsichtlich Kosteneffizienz, Anwendbarkeit und Akteursvielfalt sowohl Quotenmodellen als auch Prämien/Bonus-Systemen überlegen sind.

Dänemark hat allerdings nach der Abkehr von der reinen Einspeisevergütung im Jahr 2004 erfolgreich ein Prämienmodell für die Onshore-Windkraft etabliert. Der Anlagenbetreiber erhält eine nicht kostendeckende Prämie von 3,66 ct und muss den Strom selbst vermarkten. Trotz dieser im Vergleich zu Deutschland wesentlich schlechteren Konditionen hat Dänemark auf diese Weise eine EE-Strom-Anteil von 30 % im Jahr 2012 erreicht, die Kosten für den Endverbraucher belaufen sich dabei auf 2,07 ct (PSO-Public Service Obligation, in der auch der hohe KWK-Anteil in Dänemark abgegolten ist).

Das Bundeswirtschaftsministerium unter Sigmar Gabriel kündigte kurz nach der EEG-Reform 2014 weitere Schritte an. Bereits 2016 soll es die nächste EEG-Novelle geben, um technologiespezifische Ausschreibungen für EE-Anlagen zu ermöglichen. Noch im Jahr 2014 soll eine Verordnung für ein Politikprojekt für die Ausschreibung bei PV-Freiflächenanlagen verabschiedet werden. Nach Plänen des Bundeswirtschaftsministeriums sollen Kapazitätsmärkte, d. h. eine staatliche Förderung von Kraftwerkskapazitäten, wie sie von der Energiewirtschaft gefordert werden, in absehbarer Zeit noch nicht etabliert werden. Diese seien aufgrund großer Überkapazitäten nicht erforderlich für die Versorgungssicherheit. Stattdessen würden sie zu erheblichen Mehrkosten führen. Auch der Bundesverband Erneuerbare Energie lehnt die Einführung von Kapazitätsmärkten ab. Gutachter befürchten, dass Ausschreibungen nicht geeignet sind, Kosteneffizienz, Ausbauziele und Akteursvielfalt zu erreichen, und empfehlen, dass Ausschreibungsverfahren kleine und mittlere Marktakteure besonders berücksichtigen und die Wirkungen evaluiert werden, bevor es auf andere erneuerbare Energien ausgeweitet wird.

Das Bundeswirtschaftsministerium hat mehrere Gutachten zur Versorgungssicherheit und Funktionsfähigkeit des Strommarkts veröffentlicht. Sie kommen zu dem Ergebnis, dass es im derzeitigen Marktdesign noch einige Optimierungspotenziale gebe, um vorhandene Flexibilitätspotenziale zu erschließen und Fehlanreize hinsichtlich der Integration erneuerbarer Energien und Klimaschutz abzubauen. Erst danach solle über die von Teilen der Energiewirtschaft geforderten Kapazitätsmärkte entschieden werden. Für eine sichere Stromversorgung seien Kapazitätsmechanismen aus aktueller Sicht nicht erforderlich. Sie führten vielmehr zu erheblichen Mehrkosten für die Stromverbraucher und seien mit hohen regulatorischen Risiken verbunden.

Eine Reihe von Landesregierungen, Unternehmen, Verbänden und Think Tanks haben Vorschläge zur Reform des EEG bzw. des kompletten Strommarktdesigns vorgestellt. Eine Metaanalyse hat das „Forschungsradar Erneuerbare Energien“ veröffentlicht, in der die zahlreichen Reformvorschläge miteinander verglichen werden. So hat etwa die grün-rote baden-württembergische Landesregierung in einem Gutachten acht mögliche Fördersysteme für erneuerbare Energien untersuchen und vergleichen lassen. Auch das rot-grüne Niedersachsen hat konkrete Weiterentwicklungsmöglichkeiten des EEG vorgelegt. Der Think Tank Agora Energiewende hat einen eigenen Vorschlag in die Debatte eingebracht. Ebenfalls der Sachverständigenrat für Umweltfragen (SRU) hat in seiner Empfehlung an die neue Bundesregierung ‚Den Strommarkt der Zukunft gestalten‘ Impulse zur Weiterentwicklung des EEG gesetzt.
Außerdem hat der Bundesverband der Energie- und Wasserwirtschaft sich mit seinem Vorschlag ‚Der Weg zu neuen marktlichen Strukturen für das Gelingen der Energiewende‘ für ein neues Strommarktdesign Gedanken für das verbesserte Zusammenspiel zwischen erneuerbaren und konventionellen Kraftwerken gemacht. Der Nachhaltigkeitsrat der Bundesregierung fordert eine Umwandlung des EEG zu einem Energiefonds.





</doc>
<doc id="12069" url="https://de.wikipedia.org/wiki?curid=12069" title="ARD">
ARD

Die ARD („Arbeitsgemeinschaft der öffentlich-rechtlichen Rundfunkanstalten der Bundesrepublik Deutschland“) ist ein 1950 gegründeter Verbund öffentlich-rechtlicher Rundfunkanstalten in Deutschland, der über die Rundfunkgebühren finanziert wird.

Derzeit besteht der Verbund aus neun Landesrundfunkanstalten, die die Gemeinschaftsfernsehprogramme Das Erste, One und tagesschau24 sowie jeweils eigene regionale Fernseh- und Hörfunkprogramme bereitstellen und verbreiten. Außerdem ist der deutsche Auslandsnachrichtensender Deutsche Welle Mitglied der ARD. Die ARD ist Träger des Deutschen Rundfunkarchivs. Zusammen mit dem ZDF und dem Deutschlandradio bildet die ARD den öffentlich-rechtlichen Rundfunk in Deutschland. Die ARD und das ZDF betreiben zusammen die Spartensender Phoenix und KiKA sowie mit weiteren nationalen öffentlich-rechtlichen Rundfunkanstalten die Fernsehsender 3sat (Österreich, Schweiz) und Arte (Frankreich).

Die Bezeichnung „ARD“ wird häufig als Synonym für deren Hauptfernsehprogramm verwendet, was aber eine Ungenauigkeit darstellt; dieses hieß zunächst "Deutsches Fernsehen", dann "Erstes Deutsches Fernsehen" und heute "Das Erste".

Die neun Landesrundfunkanstalten der ARD haben insgesamt rund 23.000 fest angestellte Mitarbeiter, sie veranstalten elf Fernsehprogramme, 55 Hörfunkprogramme und verfügen über 16 Orchester und acht Chöre. Das Gesamtbudget der neun Anstalten beträgt pro Jahr rund 6,3 Milliarden Euro. Davon entfallen 366 Millionen Euro auf Sportrechte. Die Mitglieder der ARD sind mit etwa 100 eigenen Hörfunk- und Fernsehkorrespondenten an 30 Orten der Welt ständig präsent.

Der ARD liegt die föderale Struktur der Bundesrepublik Deutschland zugrunde: Jedem Bundesland steht ein Sender zu. Historisch geht die föderale Rundfunkstruktur auf die Frühzeit des Rundfunks in den 1920er Jahren in Deutschland zurück, als sich in Preußen, Sachsen, Bayern und Württemberg einzelne Rundfunkanstalten etablierten und lose in der Reichs-Rundfunk-Gesellschaft organisiert waren. Die Nationalsozialisten zentralisierten den Rundfunk ab 1933 und zerbrachen die föderale Struktur weitgehend. Die ARD wurde am 9. Juni 1950 von den damaligen sechs Landesrundfunkanstalten Bayerischer Rundfunk (BR), Hessischer Rundfunk (HR), Radio Bremen, Süddeutscher Rundfunk (SDR), Südwestfunk (SWF) und Nordwestdeutscher Rundfunk (NWDR) sowie – mit beratender Stimme – RIAS Berlin gegründet. Sie diente der Zusammenarbeit auf verschiedenen Gebieten und sollte die Vorbereitung eines Zusammenschlusses ausschließen. Die Form der Arbeitsgemeinschaft beließ den Mitgliedern ihre volle Unabhängigkeit.

Auch in der Deutschen Demokratischen Republik konnte das Programm der ARD empfangen werden, was von der ARD auch beabsichtigt war. Lediglich in bestimmten Regionen, wie im Elbtal, in der Sächsischen Schweiz und in der Oberlausitz, war der Empfang nicht möglich, deswegen wurde die Abkürzung „ARD“ scherzhaft als „Außer Raum Dresden“ oder auch „Außer Reichweite Dresdens“ interpretiert und die Region auch „Tal der Ahnungslosen“ genannt. In diesen Gebieten konnten die über Lang-, Mittel- und Kurzwelle verbreiteten Hörfunkprogramme der ARD empfangen werden. Das über den Langwellensender Donebach ausgestrahlte Programm des Deutschlandfunks war im gesamten Gebiet der DDR sehr leicht zu empfangen.

Im Laufe der 1950er Jahre wurde mit der Vorbereitung und Ausstrahlung eines ersten gemeinschaftlichen Fernsehprogramms in der Bundesrepublik Deutschland begonnen. Nach Gründung des Sender Freies Berlin (SFB) und Teilung des NWDR in Westdeutscher Rundfunk Köln (WDR) und Norddeutscher Rundfunk (NDR) sowie Gründung des Saarländischen Rundfunks (SR) gehörten der ARD 1959 neun Mitglieder an. Sie betrieben das unter dem Namen „Deutsches Fernsehen“ bundesweit ausgestrahlte Fernsehgemeinschaftsprogramm. Es folgten weitere Änderungen in der Mitgliederstruktur der ARD: 1962 kamen die neugegründeten Anstalten des Bundesrechts Deutschlandwelle (DW) und Deutschlandfunk hinzu, 1992 – nach der deutschen Wiedervereinigung – die neuen Landesrundfunkanstalten Mitteldeutscher Rundfunk (MDR) und Ostdeutscher Rundfunk Brandenburg (ORB). 1994 gingen der Deutschlandfunk und RIAS im Deutschlandradio auf, das gemeinsam von der ARD und dem ZDF getragen wird, und schieden aus dem ARD-Verbund aus. Zuletzt fusionierten 1998 SDR und SWF zum Südwestrundfunk (SWR) sowie 2003 ORB und SFB zu Rundfunk Berlin-Brandenburg (RBB).

Die neun gegenwärtig zur ARD zusammengeschlossenen Rundfunkanstalten betreiben seither als Fernsehgemeinschaftsprogramm "Das Erste" sowie anstaltseigene Fernseh- und Hörfunkprogramme. Zehntes Mitglied der ARD ist die Deutsche Welle, die deutsche Auslandsrundfunkanstalt.

Neben dem Auslandsrundfunk der Deutschen Welle die neun Landesrundfunkanstalten:

Die ARD ist ein freiwilliger Verbund von zehn deutschen Rundfunkanstalten. Die Arbeitsgemeinschaft gab sich 1950 eine Satzung, die durch den Rundfunkstaatsvertrag ergänzt wird. In dieser Satzung ist die Arbeit und Organisation der ARD geregelt. Danach hat die ARD als Hauptorgan eine Mitgliederversammlung (keinen Fernsehrat wie die Sendeanstalt ZDF). Die Versammlung findet in Form von Arbeitssitzungen der Intendanten der Mitglieder statt. Ferner gibt es Hauptversammlungen, an denen auch Rundfunk- und Verwaltungsratsvorsitzenden der Landesrundfunkanstalten (GVK) teilnehmen.

Die Mitgliederversammlung betraut jeweils für ein Jahr eine ihrer Rundfunkanstalten mit der Geschäftsführung der ARD. Der Intendant dieser geschäftsführenden Rundfunkanstalt ist in diesem Zeitraum Vorsitzender der ARD. Die Wiederwahl für ein weiteres Jahr ist möglich und inzwischen auch üblich.

Das Generalsekretariat wurde 2006 gegründet. ARD-Generalsekretärin ist seit 1. Januar 2015 die Juristin Susanne Pfab.

Der Generalsekretär ist dem jeweiligen, alle zwei Jahre wechselnden ARD-Vorsitzenden unterstellt und mitverantwortlich für

Der Dienstort des Generalsekretärs ist Berlin, verwaltungstechnisch zuständig ist der Rundfunk Berlin-Brandenburg (RBB). Die Amtszeit beträgt fünf Jahre.

Der Generalsekretär ist zugleich der stellvertretende Vorsitzende der ARD-Strategiegruppe. Er hat ein Zutritts- und Mitwirkungsrecht in allen Kommissionen und Arbeitsgruppen, auch bei den Tochterunternehmen, in der Fernsehprogrammkonferenz und den ARD-politisch relevanten Gremien. Generalsekretärin war vom 1. Juli 2006 bis zum 30. Juni 2011 Verena Wiedemann, die zuvor beim Westdeutschen Rundfunk angestellt war und dort das ARD-Verbindungsbüro in Brüssel leitete. 2014 sollte die Juristin Susanne Pfab neue ARD-Generalsekretärin werden. Bis dahin bleibt der Posten unbesetzt und Michael Kühn nahm als "Bevollmächtigter des ARD-Vorsitzes" die Aufgaben wahr.

Die Aufgaben des Programmdirektors der ARD sind im ARD-Staatsvertrag festgelegt ( ARD-StV). Er wird für mindestens zwei Jahre mit Zweidrittelmehrheit der in der ARD zusammengeschlossenen Landesrundfunkanstalten gewählt ( ARD-StV). Die Aufgabe des Programmdirektors besteht darin, gemeinsam mit den Intendanten der Landesrundfunkanstalten das Programmangebot der ARD zu erarbeiten und die Programmzulieferung der einzelnen Landesrundfunkanstalten zu koordinieren.

Dem Programmdirektor ist der ARD-Programmbeirat als beratende Instanz zur Seite gestellt. Paul Siebertz ist seit April 2013 Vorsitzender des Programmbeirats.

Die Programmdirektion der ARD befindet sich im Hochhaus des Bayerischen Rundfunks in München.


Inhaltliche Gemeinschaftseinrichtungen der ARD sind beispielsweise ARD-Aktuell beim NDR in Hamburg, das ARD-Hauptstadtstudio in Berlin und das ARD Play-Out-Center beim RBB in Potsdam, das sämtliche Fernsehprogramme und Hörfunkkanäle der ARD Digital verbreitet. Zudem besitzt die ARD ein eigenes weltweites Korrespondentennetz (ARD-Auslandsstudios).

Seit dem 31. Oktober 1954 sendet die ARD das später als "Erstes Deutsches Fernsehen" und heute offiziell als "Das Erste" bezeichnete Fernsehprogramm als Gemeinschaftsproduktion der einzelnen Landesrundfunkanstalten. Im ARD-Sendezentrum (ARD-Stern) in Frankfurt am Main wird das Programm zentral organisatorisch (Zentrale Sendeleitung, ZSL) und technisch (vollständig digital) produziert und wieder in die Bundesländer über digitale Leitungswege und Rundfunksatelliten verteilt.

Die Landesrundfunkanstalten der ARD betreiben gemeinsam die Fernsehkanäle 3sat (mit ZDF, SRF und ORF), KiKA, Phoenix (beide in Zusammenarbeit mit dem ZDF), Arte (Kooperation mit dem ZDF und ARTE France) und das ARD-Digital-Bouquet mit den Programmen tagesschau24 und One.

Das deutsche Fernsehprogramm für das Ausland wird von DW-TV der Deutschen Welle gesendet.
Bis Ende 2005 betrieb die ARD gemeinsam mit dem ZDF und der Deutschen Welle für Nordamerika zusätzlich den Bezahlsender German TV.

Im Hörfunkbereich bestehen vier gemeinsame ARD-Nachtprogramme: die ARD-Hitnacht, das ARD-Nachtkonzert, die ARD-Popnacht sowie die ARD-Infonacht. Daneben kooperieren einige Landesrundfunkanstalten für die Nachtversorgung im Hörfunk auch davon abweichend. So wird zwischen 0 und 5 Uhr auf Radio Bremen 1 das Live-Programm von SWR1 ausgestrahlt und sieben Jugendradios senden von 23 Uhr bis 1 Uhr die Call-in-Sendung Lateline.

Mitglied in der ARD ist das Auslandsradio Deutsche Welle.

Die in der ARD zusammengeschlossenen Landesrundfunkanstalten und das ZDF kooperieren mit der öffentlich-rechtlichen Körperschaft Deutschlandradio (DLR) mit den drei nationalen Hörfunkprogrammen Deutschlandfunk, Deutschlandfunk Kultur, und Deutschlandfunk Nova.

Im Internet betreibt die ARD unter ARD.de ein eigenes Onlineangebot unter anderem mit den Schwerpunkten Nachrichten, Sport, Börse, Ratgeber, Wissen und Kultur. Während für die Rubriken „Nachrichten“, „Sport“ und „Börse“ die Inhalte der Redaktionen von tagesschau.de in Hamburg, sportschau.de in Köln und der ARD-Börsenredaktion in Frankfurt am Main übernommen werden, werden die Inhalte für die Rubriken „Ratgeber“, „Wissen“ und „Kultur“ von der ARD.de-Redaktion in Mainz zusammengestellt.

In ihrem Audio- und Videoportal ARD Mediathek bietet die ARD in begrenztem Umfang einen Zugriff auf Inhalte der ARD-Landesrundfunkanstalten und Gemeinschaftseinrichtungen. Hier werden die Sendungen und Beiträge des Ersten, der dritten Fernsehprogramme und der Radiowellen thematisch gebündelt. Außerdem betreibt die ARD einen eigenen Kanal bei YouTube.

Als Gemeinschaftseinrichtung der ARD wurde 1952 das „Deutsche Lautarchiv“, heute Deutsches Rundfunkarchiv (DRA), eingerichtet. Das Archiv hat die beiden Standorte Frankfurt am Main und Potsdam-Babelsberg (früher Berlin-Adlershof). Von 1994 bis 2002 vergab es im Auftrag der ARD das „DRA-Stipendium“, einen Förderpreis zur Erforschung der Rundfunk- und Mediengeschichte der DDR.

Das Programm von "Das Erste" und gemeinsame Programmteile des Hörfunks (etwa beim ARD-Nachtexpress) werden technisch im Sendezentrum der ARD im ARD-Stern in Frankfurt am Main (auf dem Gelände des Hessischen Rundfunks) zusammengeführt. Über das eigene Glasfasernetz HYBNET werden diese Signale an die einzelnen Rundfunkanstalten verteilt.

Seit Februar 2005 wird "Das Erste" von der Zentralen Sendeabwicklung (ZSAW) im ARD-Sendezentrum ausgespielt. Zuvor schaltete jeweils die ARD-Anstalt, die eine Sendung ins Gemeinschaftsprogramm (egal ob „live“ oder aufgezeichnet), eine Leitung zum ARD-Stern, von wo aus das Signal an alle anderen ARD-Sender zur terrestrischen Verteilung weitergegeben wurde. Mit der Inbetriebnahme der ZSAW wurden erstmals alle Sendungen, die nicht „live“ sind (Filme, Soaps, Dokumentationen etc.) direkt von den Servern in Frankfurt ausgespielt. Die ZSAW ist eine Gemeinschaftseinrichtung der Landesrundfunkanstalten und soll im Sendebetrieb deutliche Einsparungen bringen. Die Sendeabwicklung ist gleichwohl in der Lage, gleichzeitig bis zu zehn Programme auszuspielen. Dies ist notwendig für eine regionalisierte Werbungsausstrahlung im Vorabend.

Die ARD ruft im Zusammenhang mit Naturkatastrophen regelmäßig in den unterschiedlichen Programmformaten der angeschlossenen Sender zu Spenden auf. Im März 2011 wurde bekannt, dass sie dabei vertraglich mit dem "Bündnis Entwicklung Hilft" verbunden ist.

Vor dem Hintergrund der Nuklearkatastrophe von Fukushima und des vom Tōhoku-Erdbeben 2011 ausgelösten Tsunami weigerte sich die ARD auf Empfehlung ihres Kooperationspartners "Bündnis Entwicklung Hilft" gegenüber der "Aktion Deutschland Hilft", Spendenaufrufe für die Opfer dieser Katastrophen zu verbreiten. Auch Spendenaufrufe der Bundeskanzlerin und des Bundespräsidenten wurden nicht mit Hinweisen in den Nachrichtensendungen unterstützt. Die Begründung: Japan sei ein reiches Land und benötige daher keine Spenden. Die "Aktion Deutschland Hilft", unter der Schirmherrschaft von Richard von Weizsäcker und Mitgliedern wie den Hilfsverbänden Malteser, Johanniter, AWO und World Vision, beschwerte sich über diese Entscheidung bei der ARD-Leitung.

Cordt Schnibben kritisierte 1989, die ARD sei „Dilettanten und Dunkelmännern“ ausgeliefert. Eine Befragung der Rundfunkräte der ARD habe ergeben, dass diese schlechter über die zu kontrollierenden Programme informiert seien als die Fernsehzuschauer und dass die Räte den Grund für ihre mangelhafte Arbeit zu 76 Prozent in „anderweitigen beruflichen Verpflichtungen“ sehen, zu 39 Prozent in „mangelnder Sachkenntnis“ und zu 16 Prozent in „mangelndem Interesse“. In den Aufsichtsgremien der ARD würde dieser Dilettantismus gezüchtet, „um die öffentlich-rechtlichen Anstalten zu Reichssendern der Parteien zu machen“. Die ursprünglich liberalen Rundfunkgesetze seien so lange novelliert worden, bis der Zugriff der Parteien zum Gesetz wurde. Im ganzen Gremium sitze höchstens einer ohne Parteibuch, von jedem könne man das Abstimmungsverhalten voraussagen. Die Nazis hätten den Rundfunk missbraucht, dann hätten die Alliierten demokratische Strukturen verordnet, „und dann ist alles wieder degeneriert“, klagte der frühere ARD-Vorsitzende Hartwig Kelm. In einem Interview stellte er dar, wie er personalpolitisch und haushaltspolitisch unter Druck gesetzt wurde. Diesem Druck könnte nicht jeder widerstehen, da es schon bei Hauptabteilungsleitern schlichtweg um die Existenz gehe. Er forderte, die Parteienvertreter müssten aus den Aufsichtsgremien entfernt werden, die ARD müsse grundlegend neu organisiert werden. Auch nach dem Bundesverfassungsgerichtsurteil von 2014 sah Richter Andreas Paulus keine entscheidende Verbesserung: Faktisch dominierten auch weiterhin parteipolitische Maßstäbe, das Versprechen eines „staatsfernen Rundfunks und Fernsehens“ bleibe auch nach der nunmehr 14. Rundfunkentscheidung des Bundesverfassungsgerichts unerfüllt.

Ulrich Wickert kritisierte 2009 in der "Frankfurter Allgemeinen Zeitung" (FAZ), in Nachrichtensendungen der ARD wie des ZDF werde „bruchstückhaft informiert und schlampig formuliert, die Unterhaltung scheint das Wichtigste zu sein“.

Besonders an Tagesschau und Tagesthemen stellte er eine sprachliche Verlotterung fest, die Autoren beherrschten „zum großen Teil nicht einmal mehr den korrekten Satzbau“. Die „Floskelsprache der Politik“ und das „Kurzsprech der Nachrichtenagenturen“ würden übernommen.

Wegen der inhaltlichen Mängel kommt er zu dem Schluss: „Den Machern scheint das Bewusstsein für ihren öffentlich-rechtlichen Auftrag, für eine Grundversorgung politischer Information zu sorgen, abhandengekommen zu sein.“ Über die Wahlberichterstattung urteilte er: „Es fehlt offenbar an einem Verständnis für die politische Grundversorgung.“ Es fehle aber nicht nur an einem Sinn für die Verbreitung wichtiger aktueller politischer Inhalte, sondern erst recht an der Einordnung.

Kai Gniffke wies die Kritik zurück: Die Nachrichtensendungen seien sprachlich „außerordentlich akkurat“. Der übliche Vorwurf sei eher, sie seien zu staatstragend. „Besonders unterhaltsame Elemente hat man der 20-Uhr-‚Tagesschau‘ noch nie nachgesagt.“

Das Medienmagazin Zapp des Norddeutschen Rundfunks kommt nach Analysen von ARD-Sendungen von November 2013 bis Februar 2014 zum Euromaidan zu dem Schluss, dass „fast 80 Prozent der Interviewpartner Regierungsgegner (waren)“. „Ein beliebter Gesprächspartner: Ex-Boxweltmeister Vitali Klitschko, der zu einer Art Galionsfigur stilisiert wird. Dabei ist er einer von mehreren Oppositionsführern.“

Im Juni 2014 kritisierte der achtköpfige Programmbeirat der ARD einstimmig die Berichterstattung der ARD über den Ukraine-Konflikt im Zeitraum Dezember 2013 bis Juni 2014 als einseitig, undifferenziert und lückenhaft. Die Sendungen der ARD erweckten teilweise den „Eindruck der Voreingenommenheit“ und seien „tendenziell gegen Russland und die russischen Positionen“ gerichtet. Sahra Wagenknecht (Die Linke) beurteilte den Bericht des Programmbeirats als „vernichtend“. Er bestätige, dass die ARD „unausgewogen und tendenziös“ berichte und sich „einseitig gegen Russland“ positioniere. Auch der AfD-Vizevorsitzende Alexander Gauland schloss sich der Kritik an.

Der Chefredakteur der ARD, Thomas Baumann, wies die Kritik des Programmbeirats energisch zurück. Ulrich Clauß von der Zeitung Die Welt bezeichnete den Beirat als „einfältig“, da Putins Propagandalohnkolonnen die negativen Zuschauerkritiken lancierten und vervielfältigten und daher kein wirkliches Motiv zur Untersuchung der Sendequalität existiere. Er verglich das Vorgehen des Programmbeirats mit „stalinistischen Geheimprozessen“, da Protokolle nicht öffentlich seien und die Programmrichter schwiegen. Putins „langer Arm“ würde bis in Gremien der ARD reichen.

Der Spiegel urteilte, sowohl Form und Schärfe der Kritik und die Art und Weise, wie sie öffentlich wurde, seien ohne Beispiel in der Geschichte der ARD.

Die groß angelegte Studie der Otto Brenner Stiftung von Hans-Jürgen Arlt und Wolfgang Storz von März 2010 zum Thema „Wirtschaftsjournalismus in der Krise – Zum massenmedialen Umgang mit Finanzmarktpolitik“ betrachtete unter anderem eingehend die Arbeitsweise der ARD von Frühjahr 1999 bis Herbst 2009. Untersucht wurden besonders die Formate die ARD-Formate „Tagesschau“ und „Tagesthemen“.

Die Studie kommt zu dem Schluss, dass die ARD-Nachrichtensendungen „Tagesschau“ und „Tagesthemen“ – im Gegensatz zu den Print-Leitmedien – nicht nur handwerklich wie in den Jahren zuvor, sondern auch vor den inhaltlichen Herausforderungen der Berichterstattung über die Krise selbst versagt hätten. Die Redaktion arbeite „perspektiven-arm“, im Mittelpunkt stünden die jeweils offiziell wichtigsten Akteure: Vertreter der deutschen Regierung zuallererst, Bankenvertreter, wenige Wissenschaftler und deren Sichtweisen. „Hier handelt es sich um eine Perspektivenverengung mit enormen Wirklichkeitsverlusten, die als schwere journalistische Verfehlung einzustufen ist.“

Im Oktober 2015 räumte „ARD aktuell“-Chefredakteur Kai Gniffke eine verzerrende visuelle Darstellung der Flüchtlingsströme nach Europa durch die Tagesschau und die Tagesthemen ein: „Wenn Kameraleute Flüchtlinge filmen, suchen sie sich Familien mit kleinen Kindern und großen Kulleraugen aus“, obgleich „80 Prozent der Flüchtlinge junge, kräftig gebaute alleinstehende Männer sind“.

Der ARD wurde vorgeworfen, die Initiative Neue Soziale Marktwirtschaft (INSM) übe zu großen und verdeckten Einfluss auf Sendungen der ARD aus. Im September 2005 wurde bekannt, dass die INSM 2002 für 58.670 Euro Dialoge in der ARD-Serie Marienhof gekauft hatte. Die eingeflochtene Schleichwerbung wurde von Lobbycontrol im Detail analysiert. Die TAZ monierte 2009 die häufige Anwesenheit und überproportionale Repräsentanz von Beratern und Botschaftern der INSM in Talkshows von Sabine Christiansen und Anne Will wie zum Beispiel des INSM-Botschafters Arnulf Baring. Anne Will moderierte den „Kongress 2002“ der INSM und interviewte dabei den damaligen BDI-Präsidenten Michael Rogowski. Die Frage nach dem Honorar wurde von der Will Media GmbH grundsätzlich zurückgewiesen.

Sowohl der ARD als auch dem ZDF legt eine wissenschaftliche Studie der Otto-Brenner-Stiftung zur Last
im Rahmen ihrer Programme zur griechischen Staatsschuldenkrise einseitig und unausgewogen berichtet zu haben.
Persönliche Meinungen von Journalisten und objektive Tatsachen waren gegenüber den Zuschauern nicht eindeutig voneinander zu trennen, allgemeine Themen und die Reformbemühungen der griechischen Regierung wurden oberflächlich widergespiegelt, die griechische Regierung konnte ferner seltener zu Wort kommen als die deutsche, die Titel waren oft plakativ. Die Studie kam in einer Dauer von mehreren Monaten zu ihren Schlüssen. Wenige Stunden nach ihrer Veröffentlichung kritisierte die ARD die Studie vollständig.






</doc>
<doc id="12073" url="https://de.wikipedia.org/wiki?curid=12073" title="CDC">
CDC

CDC steht für:

ČDC steht für:

C.DC. ist das Autorenkürzel von:

cDc steht für:

cdc steht für:


</doc>
<doc id="12076" url="https://de.wikipedia.org/wiki?curid=12076" title="Technische Universität Berlin">
Technische Universität Berlin

Die Technische Universität Berlin (kurz: "TU Berlin"; vor 1946: "Technische Hochschule Berlin") in Berlin-Charlottenburg ist mit fast 34.000 Studierenden in 90 Studiengängen die drittgrößte der vier Berliner Universitäten und gehört zu den 20 größten Hochschulen in Deutschland. Sie steht in der Tradition der 1879 gegründeten "Königlich Technischen Hochschule zu Berlin" und ist damit eine der ältesten Technischen Hochschulen in Deutschland.

Das 1876/1877 von Richard Lucae entworfene Hauptgebäude an der heutigen Straße des 17. Juni wurde 1884 eröffnet. Die in Nähe des Großen Tiergartens gelegene TU Berlin gehört zum Zusammenschluss von neun führenden deutschen Technischen Hochschulen (TU9) und ist Gründungspartner des Europäischen Instituts für Innovation und Technologie. Amtierender Präsident ist seit 1. April 2014 der Physiker Christian Thomsen.

In der selbstständigen Stadt Charlottenburg entstand am 1. April 1879 durch die Zusammenlegung der Berliner Bauakademie und der Königlichen Gewerbeakademie die "Königlich Technische Hochschule zu Berlin" (KTH). Zum 2. November 1884 wurden die an der damaligen Berliner Straße (heute Straße des 17. Juni) von Charlottenburg errichteten Neubauten der Hochschule feierlich eingeweiht.

Auf Betreiben von Adolf Slaby erhielt die KTH zu Berlin anlässlich der Hundertjahrfeier der Berliner Bauakademie am 19. Oktober 1899 auf „Allerhöchstem Erlaß“ (Kabinettsorder) von Wilhelm II., des Königs von Preußen, das Promotionsrecht zugesprochen. Neben dem zeitgleich eingeführten Diplomgrad für Ingenieure an den Technischen Hochschulen Preußens durfte sie damit als erste Technische Hochschule Deutschlands den Grad eines Doktors der Ingenieurwissenschaften (Dr.-Ing.) verleihen.

Die Bergakademie Berlin wurde am 1. Oktober 1916 als „Abteilung für Bergbau“ der TH angegliedert. Nach der Eingemeindung der Stadt Charlottenburg durch das „Gesetz über die Bildung einer neuen Stadtgemeinde Berlin“ (Groß-Berlin-Gesetz) erhielt die Hochschule 1920 den Namen "Technische Hochschule zu Berlin". Im Herbst 1927 wurde die Geodätische Abteilung der Landwirtschaftlichen Hochschule Berlin an die TH verlegt.

Im Rahmen der Planungen zur „Reichshauptstadt Germania“ war während der NS-Zeit in der TH Berlin der Ausbau einer neuen sogenannten Wehrtechnischen Fakultät V zu einer der größten im Deutschen Reich geplant. Dieser kam aber nach dem Suizid des designierten Leiters Becker nur ansatzweise zustande, obwohl die Grundstücke hinter dem heutigen Telefunken-Hochhaus an der Bismarckstraße und unter dem heutigen Teufelsberg, einer Ruinenaufschüttung, bereits beschafft worden waren.

Während der Schlacht um Berlin wurde die Technische Hochschule am 20. April 1945 geschlossen. In einem provisorischen, 15-köpfigen Arbeitsausschuss, der sich schon im Mai 1945 mit Gustav Hertz, Max Volmer, Werner Hahmann, Walter Kucharski und anderen gebildet hatte, wurde am 2. Juni zuerst Gustav Hertz und Max Volmer zum Rektor bzw. Prorektor gewählt. Beide verfügten über Kontakte zur sowjetischen Besatzungsmacht. Nachdem beide Wissenschaftler eine Woche später aber nicht erschienen, wurden Georg Schnadel zum kommissarischen Rektor und Walter Kucharski zum kommissarischen Prorektor gewählt. Am 9. April 1946 wurde die zerstörte, nun im britischen Sektor des geteilten Berlins liegende Bildungseinrichtung als "Technische Universität Berlin" mit humanistischer Neuausrichtung (vorgeschriebener humanistischer Studienanteil, Studium generale) neu gegründet. Die Humanistische Fakultät wurde am 7. März 1950 gegründet.
1969 führte das Berliner Universitätsgesetz zur Ablösung der Ordinarien- durch die Gruppenuniversität. Im Zuge dessen wurden 1970 die bisherigen neun Fakultäten durch 21 Fachbereiche ersetzt, die – mit Ausnahme des Fachbereichs Mathematik – in Institute gegliedert waren. Der Fachbereich Mathematik organisierte sich in Arbeitsgruppen selbst. Am 1. April 2001 wurden die Fachbereiche wieder zugunsten von Fakultäten abgeschafft, die seitdem teilweise ihr Budget selbst verwalten. Begründet wurde dies mit der Absicht, die TU schlanker und effizienter zu organisieren.

Neben Veranstaltungsorten an der Freien Universität Berlin war das Auditorium maximum ("Audimax", im oberen Foto links von Kastanienbäumen eingerahmt) der TU ein Hauptveranstaltungsort während der Auseinandersetzungen der Studentenbewegung: nur wenige hundert Meter vom ehemaligen Telefunken-Hochhaus entfernt liegt die Deutsche Oper, der Ort, an dem am 2. Juni 1967 der Student Benno Ohnesorg erschossen wurde.

Im Berufsverbotestreik der Studenten der Berliner Universitäten, Hoch- und Fachhochschulen 1976/77 spielten die Studenten der TU eine entscheidende Rolle, als sie sich in unerwartet großer Zahl dem von der FU ausgehenden Ausstand anschlossen.

Am 16. Dezember 1976 meldete der "Der Tagesspiegel":
„TU-Präsident Wittkowsky hat zu dem Boykott der Lehrveranstaltungen aus Protest gegen Berufsverbote und verschlechterte Studienbedingungen, der jetzt auf 15 Fachbereiche [von 21] der TU übergegriffen hat, erklärt, er unterstütze die wesentlichen Forderungen der Studenten und halte die studentischen Protestmaßnahmen für zulässig.“

1980 wurden Teile der Pädagogischen Hochschule Berlin in die TU integriert.

Die Gegenwart der TU ist von massiven staatlichen Mittelkürzungen und Stellenreduzierung gekennzeichnet. Zugleich ist die TU Berlin die zweitgrößte technische Universität Deutschlands. Durch regelmäßig hohe Einwerbung von Drittmitteln versucht die TU extreme Auswirkungen der Mittelkürzungen abzumildern.

Mit dem Erwerb des Telefunken-Hochhauses 1975 und den Neubauten des Mathematikgebäudes 1983 (Straße des 17. Juni 136), des neuen Instituts der Physik 1984 (Hardenbergstr. 36) und des Produktionstechnischen Zentrums 1986 (Pascalstr. 8–9) wurde die TUB zur größten Technischen Universität der damaligen Bundesrepublik.

Im Rahmen der Exzellenzinitiative des Bundes und der Länder zur Förderung von Wissenschaft und Forschung an deutschen Hochschulen wurden der Technischen Universität im Jahr 2006 eine Graduiertenschule (Berlin Mathematical School) sowie 2007 ein Exzellenzcluster (Unifying Concepts in Catalysis) bewilligt. Die TU Berlin ist mit ca. 6000 ausländischen Studierenden international aufgestellt; die meisten Studenten kommen aus China, der Türkei, Russland, Vietnam und Kamerun. Das Europäische Institut für Innovation und Technologie wählte zwei Wissens- und Innovationsgemeinschaften mit Beteiligung der TU Berlin aus, die über einen Zeitraum von fünf Jahren jeweils 100 Millionen Euro Fördergelder erhalten. Als Ausgleich für schwindende öffentliche Mittel forcierte die Universität in jüngster Vergangenheit ihre Zusammenarbeit mit Unternehmen wie der Deutschen Telekom AG, den Telekom Innovation Laboratories oder der Siemens AG.

Das Hauptgebäude nach dem Entwurf von Richard Lucae – nach dessen Tod 1877 fertiggeplant von Friedrich Hitzig und Julius Carl Raschdorff – entstand von 1878 bis 1884 als Monumentalbau im Stil der italienischen Hochrenaissance. Nach schweren Schäden am Ende des Zweiten Weltkriegs wurden drei der ursprünglich fünf Innenhöfe, die Seitenflügel und der rückwärtige Teil wiederaufgebaut, die Front mit dem Portikus dagegen Anfang der 1950er Jahre abgerissen. 1965 wurde das heute von der Straße des 17. Juni aus sichtbare Hauptgebäude nach Entwürfen von Kurt Dübbers vor den Altbau gesetzt. Dem zehngeschossigen aluminiumverkleideten Bau ist das Auditorium maximum vorgelagert.

Am Ernst-Reuter-Platz entstanden das Institut für Bergbau und Hüttenwesen (1955–59 von Willy Kreuer) und die Gebäude der Architekturfakultät (Bernhard Hermkes, Hans Scharoun).

Nach 1960 dehnte sich der Campus mit zahlreichen weiteren Institutsgebäuden nördlich der Straße des 17. Juni bis über die Spree aus (Beispiel: Institutsgebäude für Werkstoffwissenschaften).

Am 1. April 2001 erfolgte die kontroverse (Rück-)gliederung in acht Fakultäten, zu der die 14 (ursprünglich 22) Fachbereiche verschmolzen wurden (siehe Gruppenuniversität). Die Fakultäten gliedern sich in weitere Institute:








Seit dem 1. April 2005 sind die Fakultäten VI (Bauingenieurwesen und Angewandte Geowissenschaften) sowie VII (Architektur Umwelt Gesellschaft) zu einer Fakultät fusioniert.

Die Universitätsbibliothek der TU gliedert sich in die Zentralbibliothek und die beiden Bereichsbibliotheken "Bereichsbibliothek Architektur und Kunstwissenschaft" und "Bereichsbibliothek Physik".
Als zentrale Universitätsbibliothek dient die gemeinsame Zentralbibliothek der TU und UdK Berlin im Gebäude "VOLKSWAGEN-Haus" in der Fasanenstraße am westlichen Tiergartenrand als östlichste Begrenzung des Campus Charlottenburg. Ein kleiner Teil des 2004 eröffneten Gebäudes wurde von der Volkswagen AG gesponsert (5 Mio. €), die auch Namensgeber des Gebäudes wurde. Neben der Universitätsbibliothek existieren an verschiedenen Fakultäten eigene Bibliotheken: Die beiden größten eigenständigen Bibliotheken sind hierbei "Die Bibliothek Wirtschaft & Management" der Fakultät "Wirtschaft und Management" im Hauptgebäude und die "Mathematische Fachbibliothek" des Instituts für Mathematik im Mathematikgebäude.

1958 erhielt das Recheninstitut von Wolfgang Haack den ersten Zuse-Rechner Z22. Das Institut ging 1974 in der Zentraleinrichtung Rechenzentrum ("ZRZ") der TU auf.
Informations- und Telekommunikations-Services, darunter das internationale Eduroam sowie Web-2.0- und Social-Media-Dienste, werden von der Nachfolgereinrichtung des "ZRZ" unter dem heutigen Namen "tubIT" erbracht.

An der TU Berlin existierten und existieren zahlreiche Sonderforschungsbereiche (SFB), große Forschungsverbünde, die von der Deutschen Forschungsgemeinschaft eingerichtet werden. Darunter waren:


Angebotene Studiengänge mit Vertiefungsrichtungen:


Eine Auswahl der durch den Nationalsozialismus vertriebenen Wissenschaftler:

Forschungs-Kernreaktor SUR BERLIN vom Typ Siemens-Unterrichtsreaktor.
Der Reaktor wird vom Institut für Energietechnik seit 1963 an der Straße des 17. Juni betrieben (erste Kritikalität: 26. Juli 1963) und soll stillgelegt werden.
Seit 2002 ist der Kernreaktor außer Betrieb, der Kernbrennstoff (angereichertes Uran) wurde im Oktober 2008 aus der Anlage entfernt.

Der Hochgeschwindigkeits-Parallelrechner Cray T3E wird von der Fakultät Elektrotechnik und Informatik betrieben.

Ein Großteil der Fachgebiete ist auf dem Hauptcampus (auch Campus Charlottenburg) an der Straße des 17. Juni angesiedelt. Daneben gibt es weitere Standorte in der näheren Umgebung, zum Beispiel das Severingelände (SG) am Salzufer 17–19.
Zudem gibt es eine Ansammlung von Fachgebieten am "Campus Wedding" (in den ehemaligen AEG-Fabrikgebäuden in Gesundbrunnen). Weitere Standorte sind am Institut für Gärungsgewerbe und Biotechnologie in der Seestraße 13 und in Dahlem.

Der Hauptcampus erstreckt sich über die Ortsteile Charlottenburg und Tiergarten. Begrenzt wird durch Marchstraße, Einstein-Ufer, Fasanenstraße und Hardenbergstraße. Einzelne Gebäude liegen außerhalb dieser Grenzen, werden jedoch mit zum Hauptcampus gezählt. Die Straße des 17. Juni trennt den Campus in den größeren Südteil und den kleineren Nordteil.

Hier befinden sich das Hauptgebäude mit der Universitätsverwaltung sowie die größten Hörsäle. Dort fanden im Großen Hörsaal des Physikalischen Instituts in der Zeit der deutschen Teilung in den Jahren 1955, 1956 und 1958 insgesamt sieben Sitzungen des 2. und 3. Deutschen Bundestags statt.

1991 schloss die TU Berlin mit Gustav Severin einen Erbrechtsvertrag. Der Vertrag sieht vor, dass die Universität das 19.000 m² große Gelände am Salzufer nach dem Tod des Sohnes erhält, spätestens jedoch zum 1. Januar 2051. Auf einem Schild am Zugang zum Gelände ist von Schenkung die Rede.

2012 wurde im ägyptischen el-Guna der erste Auslandcampus eröffnet. Anlässlich der Campuseröffnung wurde ein Buddy Bär als Symbol der besonderen Verbindung zwischen den beiden Studienstandorten enthüllt. In El Gouna werden drei Masterstudiengänge für maximal 90 Studenten pro Jahrgang angeboten: Energy Engineering, Urban Development und Water Engineering.

Die Professoren Vockel und Gobrecht gründeten 1950 den Verein "Studentische Darlehnskasse Berlin-Charlottenburg", der 1951 in Studentische Darlehnskasse e. V. umbenannt worden ist.





</doc>
<doc id="12078" url="https://de.wikipedia.org/wiki?curid=12078" title="Völkermord">
Völkermord

Ein Völkermord oder Genozid ist seit der Konvention über die Verhütung und Bestrafung des Völkermordes von 1948 ein Straftatbestand im Völkerstrafrecht, der nicht verjährt. Der Begriff "Genozid" setzt sich zusammen aus dem griechischen Wort "γένος (génos" „Herkunft, Abstammung, Geschlecht, Rasse“; im weiteren Sinne auch „das Volk“) sowie dem lateinischen "caedere" „morden, metzeln“.

Gekennzeichnet ist er durch die spezielle Absicht, auf direkte oder indirekte Weise . Daher wird er auch als einzigartiges Verbrechen, als "Verbrechen der Verbrechen" (englisch "crime of crimes") oder als "das schlimmste Verbrechen im Völkerstrafrecht" bezeichnet.
Die auf Raphael Lemkin zurückgehende rechtliche Definition dient auch in der Wissenschaft als Definition des Begriffs Völkermord. Seit dem Beschluss durch die Generalversammlung der Vereinten Nationen 1948 wurde die Bestrafung von Völkermord auch in verschiedenen nationalen Rechtsordnungen ausdrücklich verankert.

Am 9. Dezember 1948 beschloss die Generalversammlung der Vereinten Nationen in der Resolution 260 die „Konvention über die Verhütung und Bestrafung des Völkermordes“ "(Convention pour la prévention et la répression du crime de génocide, Convention on the Prevention and Punishment of the Crime of Genocide)," die am 12. Januar 1951 in Kraft trat. Die Bundesrepublik Deutschland ratifizierte die Konvention im Februar 1955, Österreich hinterlegte die Beitrittsurkunde am 19. März 1958 und die Schweiz am 7. September 2000. Nach der Konvention ist Völkermord ein Verbrechen gemäß internationalem Recht, .

Grundlage war die Resolution 180 der UN-Vollversammlung vom 21. November 1947, in der festgestellt wurde, dass „Völkermord ein internationales Verbrechen [ist], das nationale und internationale Verantwortung von Menschen und Staaten erfordert“, um der völkerrechtlichen Verbrechen im Zweiten Weltkrieg zu gedenken.

Die Konvention definiert Völkermord in Artikel II als 
Im deutschen Völkerstrafgesetzbuch wie auch im schweizerischen Strafgesetzbuch ist die Tat entsprechend der Konvention definiert.

Die praktische Bedeutung der Konvention war bis zu den Jugoslawienkriegen sehr gering. Bis dahin gab es nur sehr wenige Anklagen wegen Völkermords. Die erste Verurteilung durch ein internationales Gericht auf der Basis der Konvention erfolgte im September 1998 durch das Akayesu-Urteil des Internationalen Strafgerichtshofs für Ruanda.

Zu beachten ist, dass nur die "Absicht" zur Vernichtung der Gruppe erforderlich ist, nicht aber auch die vollständige Ausführung der Absicht. Es muss eine über den Tatvorsatz hinausgehende Absicht vorliegen, eine nationale, ethnische, rassische, religiöse oder auch soziale Gruppe als solche ganz oder teilweise zu zerstören.

Die Handlungen nach Artikel II Buchstaben a) bis e) der Konvention (in Deutschland umgesetzt durch Abs. 1 Nr. 1 bis 5 VStGB) hingegen müssen tatsächlich (und willentlich) begangen werden. Dies bedeutet insbesondere, dass es nicht vieler Opfer bedarf, damit die Täter sich des Völkermordes schuldig machen. Bloß ihre Vernichtungs"absicht" muss sich auf die ganze Gruppe oder einen maßgeblichen Teil von ihr richten. Die Täter erfüllen den Straftatbestand beispielsweise, wenn sie – in dieser besonderen Absicht – einzelnen Gruppenmitgliedern ernsthafte körperliche oder geistige Schäden zufügen oder den Fortbestand der Gruppe verhindern wollen, etwa durch Zwangskastration. Eine Anklage wegen Völkermordes bedarf daher nicht der Ermordung auch nur eines Menschen.

Umgekehrt gilt auch: Handlungen nach Artikel II Buchstaben a) bis e) der Konvention sind kein Völkermord, wenn ihr Ziel nicht darin besteht, eine Gruppe ganz oder teilweise zu vernichten, egal wie viele Mitglieder getötet oder sonst wie beeinträchtigt werden. Solche Maßnahmen sind ebenfalls kein Völkermord, wenn ihr Ziel darin besteht, eine Gruppe auszurotten, die nicht durch nationale, ethnische, rassische oder religiöse Eigenschaften definiert ist.

Ob auch die tatsächliche Gefahr der Zerstörung einer geschützten (Teil-)Gruppe bestehen muss, ist rechtlich umstritten. Von der Beantwortung dieser Frage hängt ab, ob auf einen isoliert handelnder Einzeltäter, der in der Hoffnung auf eine teilweise oder vollständige Zerstörung der Gruppe handelt, Völkerstrafrecht anzuwenden ist.

Artikel 6 der Konvention geht grundsätzlich vom Territorialitätsprinzip aus, wonach Völkermord vor den Gerichten in den Ländern verfolgt wird, in denen die Tat begangen worden ist. Darüber ist die Zuständigkeit von internationalen Gerichtshöfen vorgesehen, soweit die Vertragsstaaten sich dieser Gerichtsbarkeit unterworfen haben.

In Deutschland ist der Straftatbestand des Völkermordes in des Völkerstrafgesetzbuches niedergelegt. Gemäß VStGB gilt für Völkermord das Weltrechtsprinzip, d. h. Taten können auch dann in Deutschland verfolgt werden, wenn sie weder in Deutschland begangen sind noch ein Deutscher beteiligt ist.

Auch nach Schweizer Strafgesetzbuch gilt das Weltrechtsprinzip (Art 264"m" StGB). Eine parlamentarische Immunität oder ähnliche Schutzklauseln sind nicht anwendbar und schützen vor einer Verurteilung nicht (Art 264"n"). Selbst die normalerweise angewendete Regel, dass in der Schweiz nicht mehr verfolgt wird, wessen Tat im Ausland verjährt ist oder der dort freigesprochen wurde, ist nur insofern anwendbar, als nicht offensichtlich die ausländischen Gerichte die Tat bewusst verharmlosen. Einen „Freispruch“ durch ein Regime, das Völkermord und ähnliche Verbrechen offensichtlich billigt oder selber begeht, soll damit nicht als abschließendes Urteil anerkannt werden (Art 265"m" Abschnitt 3).

2011 wurde Pauline Nyiramasuhuko, ehemalige Familien- und Frauenministerin Ruandas, als erste Frau wegen Völkermord und Vergewaltigung als Verbrechen gegen die Menschlichkeit verurteilt.

Im Mai 2013 wurde Efraín Ríos Montt, Präsident Guatemalas von 1982 bis 1983, wegen Völkermord und Verbrechen gegen die Menschlichkeit von einem Gericht in Guatemala zu 80 Jahren Gefängnis verurteilt. Zwar würde er damit als erstes Staatsoberhaupt gelten, das wegen eines Völkermords im eigenen Land von einem einheimischen Gericht verurteilt worden wäre, jedoch wurde das Urteil wenige Tage später vom obersten Gerichtshof Guatemalas aufgrund von Formfehlern aufgehoben. Der neuerliche Prozess ist hingegen noch nicht abgeschlossen.

Der Ausdruck "Völkermord" taucht zum ersten Mal bei dem deutschen Lyriker August Graf von Platen (1796–1835) in seinen „Polenliedern“ auf, und zwar in der 1831 entstandenen Ode "Der künftige Held". Er wendet sich gegen die Auflösung des polnischen Staates, den Österreich, Preußen und Russland sich untereinander aufgeteilt haben, und wirbt mit anderen westdeutschen Demokraten, die beim „Hambacher Fest“ 1832 die polnische Nationalfahne neben der deutschen aufgezogen haben, für das Wiedererstehen des polnischen Staates. Im Besonderen geißelt er die Unterdrückungspolitik Russlands, indem er nach der Bestrafung der "Dschingiskhane" ruft, Für den liberalen ostpreußischen Abgeordneten Carl Friedrich Wilhelm Jordan ist der Ausdruck in Bezug auf die Polen so geläufig, dass er ihn in der Frankfurter Paulskirche am 24. Juli 1848 bei der Diskussion der verwendet, und zwar steigert er ihn noch:

Der Historiker Heinrich von Treitschke äußert sich in „Politik. Vorlesungen, 1897–1898“ zum Untergang der Prußen als Urbevölkerung Preußens und sagt:

Als der polnisch-jüdische Anwalt Raphael Lemkin (1900–1959) die Bezeichnung "Genozid" 1943 für einen Gesetzesentwurf für die polnische Exilregierung zur Bestrafung der deutschen Verbrechen in Polen verwendete, hatte dieser bereits eine durch die imperialistische Diskussion des 19. Jahrhunderts geprägte Geschichte. 1944 übertrug er den Ausdruck ins Englische als "genocide". Die griechisch-lateinische Begriffsprägung "Genozid" stellt eine Übertragung des polnischen "ludobójstwo" dar (von "lud" = Volk und "zabójstwo" = Mord).

Lemkin suchte spätestens seit 1941 nach einem Wort, das Untaten wie die des Osmanischen Reiches gegen die Armenier und des NS-Regimes treffend umschreibt. Dass er 1933 mit seinem Entwurf das Völkerbund-Gremium auf der Madrider Tagung nicht hatte überzeugen können, führte er auch darauf zurück, dass Worte wie Barbarei und Vandalismus, die er damals gebraucht hatte, solche Taten letztlich beschönigten. Es sollte ein Wort sein, dass alle Aspekte gezielter Angriffe auf eine Bevölkerungsgruppe greifbar machen sollte, darunter auch Maßnahmen wie Massendeportationen, die erzwungene Senkung der Geburtenrate, wirtschaftliche Ausbeutung und die gezielte Unterdrückung der Intelligentsia. Ein Begriff wie beispielsweise Massenmord umfasste all diese Aspekte nicht. Es sollte auch kein Begriff sein, der wie Barbarei und Vandalismus bereits in anderen Zusammenhängen benutzt wurde. Lemkin entwickelte den Begriff Genozid, wobei für ihn auch eine Rolle spielte, dass er sich (anders als Völkermord) in zahlreichen Sprachen in entsprechend abgewandelter Form verwenden ließ. In seinem Buch "Axis Rule in Occupied Europe" gab er auch eine erste Definition des Begriffes. Genozid sei
Der Begriff wurde im angelsächsischen Raum sehr schnell gebräuchlich, nachdem eine Reihe US-amerikanischer Zeitungen ihn verwendet hatte, als diese gegen Ende des Jahres 1944 begannen, ausführlich über die nationalsozialistischen Massenverbrechen in Europa zu berichten. Das ist zum Teil auf das direkte Einwirken von Lemkin zurückzuführen. So überzeugte er Eugene Meyer, den Herausgeber der "Washington Post", dass allein dieser Begriff der passende für diese Untaten sei. Tatsächlich erschien am 3. Dezember 1944 in der "Washington Post" ein Leitartikel, in dem „Genozid“ als das einzige passende Wort bezeichnet wurde, mit dem man beschreiben könne, dass zwischen April 1942 und April 1944 1.765.000 Juden in Auschwitz-Birkenau durch Gas getötet und verbrannt worden waren. Es wäre falsch, führte der Artikel weiter aus, dafür den Begriff „atrocity“ (dt. Gräueltat) zu verwenden, denn darin schwinge auch immer ein Unterton von Ungerichtetheit und Zufälligkeit mit. Der entscheidende Punkt aber sei hier, dass diese Taten systematisch und gezielt gewesen seien. Gaskammer und Krematorien seien keine Improvisationen, sondern gezielt entwickelte Instrumente für die Auslöschung einer ethnischen Gruppe.

Webster’s New International Dictionary nahm vergleichsweise schnell den Begriff auf. Die französische Encyclopédie Larousse verwendete den Begriff in ihrer Ausgabe von 1953, und im Oxford English Dictionary wurde er als 1955er-Update zur dritten Edition gelistet.

Die rechtliche Definition des Genozids ist häufig als unzureichend kritisiert worden. Der amerikanische Politikwissenschaftler Rudolph Joseph Rummel entwickelte daher das weitergespannte Konzept des Demozids, das in seiner Definition alle tödlichen Genozide einschließt. Nicht tödliche Handlungen einer Regierung, die auf die Vernichtung einer Kultur abzielen, werden hingegen häufig als Ethnozid bezeichnet.

Es ist nicht bekannt, wann die ersten Völkermorde stattfanden. Die Genozidforschung geht davon aus, dass Genozide in allen Epochen in nahezu allen von Menschen besiedelten Regionen vorkamen. Überliefert sind Völkermorde aus der Antike. Diejenigen der Neuzeit fanden vor allem in Kolonien statt: zunächst bei der Kolonisierung durch europäische Mächte (z. B. an Indianern während der Indianerkriege); dann teilweise erneut bei der Entkolonisation. Dabei prallten nach Abzug einer Kolonialmacht gelegentlich verschiedene ethnische Gruppen aufeinander, welche durch die Grenzziehungen ihrer Kolonialmacht nun in einem Staat lebten (wie etwa in Biafra und Bangladesch).


Die Kongogräuel in den Jahren 1888 bis 1908 waren Taten unter Verantwortung des belgischen Königs Leopold II., die zur Dezimierung der Bevölkerung des Kongo-Freistaats durch Sklaverei, Zwangsarbeit und massenhafte Geiselnahmen und Tötungen führten und schätzungsweise acht bis zehn Millionen Tote (etwa die Hälfte der damaligen Bevölkerung) forderten. Ob der Massenmord im Kongo, trotz seiner genozidalen Ausmaße, ein Völkermord war, ist umstritten. Denn es wurde nicht planmäßig versucht, eine bestimmte ethnische Gruppe zu vernichten, sondern der Massenmord war die Folge extremer Ausbeutung.

Ähnlich zu betrachten sind die bisweilen als "Völkermord an den amerikanischen Ureinwohnern" bezeichneten Indianerkriege.

Der Holodomor bezeichnet eine schwere, partiell anthropogene Hungersnot in der Ukraine in den Jahren 1932 und 1933, dem mehrere Millionen Menschen zum Opfer fielen. Ursachen waren die Zwangskollektivierung Stalins, um den Widerstand der Ukrainer zu brechen, die Entkulakisierung und auch wetterbedingte Missernten. Die Schätzungen der Opferzahlen in der Ukraine gehen weit auseinander, sie reichen von 2,4 Millionen bis 14,5 Millionen Hungertoten. Die Ukraine bemüht sich seit der Unabhängigkeit im Jahr 1991 um eine internationale Anerkennung des Holodomors als Völkermord.

Auch der Massenmord an den Kommunisten Indonesiens 1965 und 1966 stellt einen Sonderfall dar, bei dem je nach Schätzung zwischen 500.000 und 3 Mio. Menschen ermordet wurden. Zwar wurde hier keine religiöse, ethnische oder nationale Gruppe gezielt ermordet, aber es war dennoch das Ziel, eine klar definierte (nämlich politische) Bevölkerungsgruppe gesamthaft zu ermorden. Deswegen und weil die chinesische Bevölkerungsminderheit Opfer dieser Massenmorde wurde, sprechen sich einigen Autoren, darunter Yves Ternon, dafür aus, ihn als Völkermord zu betrachten. Der Begriff eines Autogenozids ließe sich in diesem Fall auch anwenden.

Auch die Ereignisse während der Herrschaft der Roten Khmer in Kambodscha von 1975 bis 1979 stellen einen Sonderfall dar. Da sich der Genozid in Kambodscha gegen die Bevölkerung des eigenen Landes richtete, ist hier auch der Begriff „Autogenozid“ (wörtlich „Völkerselbstmord“) angewandt worden. Beim Vorgehen der Roten Khmer gegenüber abgrenzbaren Gruppen wie z. B. den muslimischen Cham jedoch greift die Definition des Völkermordes.




</doc>
<doc id="12080" url="https://de.wikipedia.org/wiki?curid=12080" title="Musikwissenschaft">
Musikwissenschaft

Musikwissenschaft ist eine wissenschaftliche Disziplin, deren Inhalt die praktische und theoretische Beschäftigung mit Musik ist, d. h. die Erforschung und Reflexion aller Aspekte der Musik und des Musizierens. Das Phänomen Musik wird aus der Sicht aller relevanten Disziplinen (und ihrer Erkenntniswege) betrachtet; dazu gehören kultur-, natur-, sozial- und strukturwissenschaftliche Ansätze.

Die Musikwissenschaft erfuhr nach 1945 eine Ausdifferenzierung, die eine Gliederung in Teildisziplinen notwendig machte. Glen Haydon (1896–1966) und Friedrich Blume (1893–1975) befürworteten eine Dreigliederung: 

Diese Dreigliederung löste die von Guido Adler (1855–1941) geprägte Zweiteilung in Historische und Systematische Musikwissenschaft ab. Musikethnologie zählte Adler zum Bereich der Systematischen Musikwissenschaft. Weitere Einteilungen waren die fünfgliedrige Einteilung (Historische, Systematische, Musikethnologische, Musiksoziologische und Angewandte Musikwissenschaft) von Hans-Heinz Dräger (1909–1968) sowie die viergliedrige Einteilung (Systematische Musikwissenschaft, Musikgeschichte, Musikalische Volks- und Völkerkunde sowie Landes- und Gesellschaftskundliche Musikforschung) von Walter Wiora (1907–1997).

Der Gegenstandsbereich der historischen Musikwissenschaft ist Musik und Geschichte. Man verwendet die Quellenforschung, Notenkunde und Notentextanalyse, um Sachverhalte der Vergangenheit aufzudecken. Die Historische Musikwissenschaft will Quellen verfügbar machen und sie interpretieren. Erst seit den 1960er Jahren wendet man sich der Musik des 20. Jahrhunderts zu. 
Vor allem die Geschichte der Europäischen Musik (Kunstmusik) hat sie zum Inhalt. Sie hat verschiedene Teilgebiete:

Wichtige Hilfsdisziplinen sind:

Aus dem ursprünglichen Konzept von Adler (1885) (Stichwort: „Gesetzesmäßigkeiten“) sowie der heutigen internationalen Wissenschaftspraxis geht hervor, dass Gegenstand der systematischen Musikwissenschaft nicht in erster Linie spezifische Erscheinungsformen der Musik wie Stücke, Werke, Aufführungen, Traditionen, Gattungen, Komponisten, Stile, Perioden usw., sondern eher die Musik an sich und musikalische Phänomene im Allgemeinen sind. Um abstrakte, allgemeine Aussagen über Musik zu ermöglichen ist eine „systematische“ Vorgehensweise nötig (Beispiele: Erkenntnistheorie, Logik, Klassifikation, Messung, Empirik, statistische Analyse, Modellierung, Vorhersage). Die Systematische Musikwissenschaft ist in folgende Einzeldisziplinen gegliedert:

Seit Jaap Kunst 1950 das englische Wort "ethnomusicology" einführte, haben sich im Deutschen die Begriffe "Musikethnologie" oder "Ethnomusikologie" gegenüber der früheren "Vergleichenden Musikwissenschaft" durchgesetzt. Das Fachgebiet beschäftigt sich mit Musik außerhalb der westlichen Kunstmusik. Übrig bleibt die europäische Volksmusik, die Musik der außereuropäischen Naturvölker und die Musikkulturen, die nicht vom europäischen Abendland abhängig sind, wie zum Beispiel die asiatische Kultur.

Historische und systematische Aspekte sind hier streng mit eingegliedert. Die Abkapslung des dritten Zweiges ist jedoch wichtig, da hier die verschiedenen Kulturen verschiedene Anforderungen stellen. So gibt es nicht in jeder Kultur den Begriff der „Musik“. Und wo fängt für die Wissenschaft „Musik“ überhaupt an? Oft bringt hier nur die Feldforschung die Wissenschaft voran, wo das tägliche Leben der Menschen in möglichst unveränderter Form beobachtet wird. Forschungsbereiche:

Der in der hellenistischen Antike entwickelte Lehrplan basierte auf den beiden Säulen Musiké und Gymnastik, die die Bildung des Geistes und des Körpers vermitteln sollten. Deshalb nahm das Unterrichtsfach Musiké, das sich spätestens seit Platon auch mit mathematisch-musikwissenschaftlichen Fragen befasste, in seiner ursprünglichen und weit gefassten Bedeutung einen bevorzugen Platz im antiken Bildungswesen ein. Auch die aus dieser Tradition hervorgegangene ars musica wandte sich Themenfeldern zu, die heute den Fächern Musikwissenschaft bzw. einem speziellen Forschungszweig der Musiktheorie zugeordnet werden. In der mittelalterlich-universitären Ausbildung wurde der ars musica – als einer der vier mathematischen Künste (Quadrivium) der septem artes liberales – jedoch lediglich der Platz eines propädeutisches Faches der Philosophie zugewiesen. Letztere galt wiederum seit dem Mittelalter als ancilla theologiae und nahm als untere Fakultät gegenüber der höchsten Theologischen Fakultät nur eine dienende Funktion im christlich-abendländischen Bildungswesen ein. Das blieb nicht ohne Folgen für die ars musica, die sich gegenüber der Theologie mit einschlägigen Bibelzitaten hinsichtlich ihrer geistesgeschichtlichen Relevanz zu rechtfertigen versuchte. Eine endgültige Emanzipation der Philosophischen gegenüber den Theologischen Fakultäten fand erst in der Epoche der Aufklärung ab etwa 1740 statt, wobei die Musik inzwischen aus dem Kanon der universitären Ausbildung herausgefallen war.

Lorenz Christoph Mizlers Correspondierende Societät der musicalischen Wissenschaften (1738–1761) gilt als erste musikwissenschaftliche Gesellschaft. Mizler war mit seinen von 1736 bis 1742 gehaltenen Vorlesungen an der Leipziger Universität der erste Hochschullehrer, der nach dem Verfall der ars-musica-Tradition das Fach Musikwissenschaft gemäß einem von ihm vorgelegten Lehrplan an einer Universität unterrichtete. Es war seine erklärte Absicht, „die musikalischen Wissenschaften, so wohl was die Historie anbelanget, als auch was aus der Weltweisheit, Mathematik, Redekunst und Poesie dazu gehöret, so viel als möglich ist, in vollkommenen Stand zu setzen.“ In der Folgezeit knüpfte die jetzt wieder an den Philosophischen Fakultäten betriebene Musikwissenschaft nicht mehr an der "edlen Music-Kunst Würde" (Werckmeister 1691) des antiken Musikbegriffs an, sondern wandte sich vornehmlich anderen Aufgabenfeldern zu.

Als universitäre Disziplin sah sich die Musikwissenschaft in der Zeit der Weimarer Republik zunehmend in die Ecke gedrängt. Sie wurde konfrontiert mit Vorwürfen des „Elitarismus“ und eines „Elfenbeinturm-Daseins“. 
Darüber hinaus musste sie ihre Nützlichkeit für die Gesellschaft unter Beweis stellen und eine „aktivere Rolle in der Gesellschaft spielen.“ Besonders die Doppelerfahrung zahlreicher Musikwissenschaftler als Akademiker im Hochschulbetrieb als auch Praktiker im Bereich der Musik bzw. der Musikerziehung, war für diese Legitimation von besonderer Bedeutung und ermöglichte auf lange Sicht in einigen Fällen einen nahtlosen Übergang in das nationalsozialistische System.
Die Musikwissenschaft war 1918 zu Beginn der Weimarer Republik eine relativ junge akademische Disziplin, jedoch waren es die Vorarbeiten von namhaften Musikwissenschaftlern der ersten Generation wie z. B. Hermann Kretschmar, Guido Adler, Erich von Hornbostel, Curt Sachs und vielen anderen, die Pionierarbeit auf den Gebieten der Historischen, Systematischen und Vergleichenden Musikwissenschaft leisteten. So hatten „deutsche und österreichische Wissenschaftler in Forschung und Methodologie bahnbrechend gewirkt.“ Die sogenannten „Denkmäler deutscher Tonkunst“, galten in der Spätphase der wilhelminischen Epoche als Prestigeprojekt um einerseits den Wert der deutschen Musik zu belegen und gleichzeitig die Musikwissenschaft als akademisches Fach zu legitimieren und deren Nützlichkeit für die Bevölkerung unter Beweis zu stellen. Als „‚Erzieher‘ hatten sie die Aufgabe, das deutsche Volk über sein musikalisches Erbe und seine musikalische Stärke aufzuklären.“ Trotz all dieser Bemühungen konnte die Musikwissenschaft während der Weimarer Republik nie eine bedeutsame kulturpolitische Position einnehmen. 

Noch immer nicht vollständig erforscht ist die Rolle der Musikwissenschaft im Dritten Reich. Nach der Zwangsentlassung jüdischer Wissenschaftler übernahmen vielerorts überzeugte NSDAP-Mitglieder oder Gesinnungsgenossen die Institute und führten sie als willige Kunstvollstrecker im Sinne des Regimes. So fälschte etwa der Musikwissenschaftler Wolfgang Boetticher als Mitarbeiter im Sonderstab Musik im Einsatzstab Reichsleiter Rosenberg angebliche Briefe von Schumann an Mendelssohn im Sinne der nationalsozialistischen Ideologie. Die Musikwissenschaftler Theophil Stengel und Herbert Gerigk veröffentlichten das rassistische Lexikon der Juden in der Musik.
Der Musikwissenschaftler Joseph Müller-Blattau übernahm eine Professur für Musikwissenschaft in Frankfurt am Main. Seit 1933 SA-Mitglied, forschte er für die Forschungsgemeinschaft Deutsches Ahnenerbe der SS über das Germanische Erbe in der deutschen Tonkunst. 1936 spielte er eine unrühmliche Rolle bei der Entfernung von Wilibald Gurlitt durch den nationalsozialistischen Rektor der Universität Freiburg/Breisgau. 1937 wurde er zum Nachfolger Gurlitts berufen. Friedrich Blume hielt 1938 bei den ersten Reichsmusiktagen das Grundsatzreferat "Musik und Rasse- Grundlagen einer musikalischen Rasseforschung". Heinrich Besseler, Mitglied der SA und der NSDAP forderte bei den Musiktagen der Hitlerjugend in Erfurt, „daß die Musikpflege der Universität vom Geist des neuen HJ-Liedes durchdrungen werden müsse“.

Auch nach dem Zweiten Weltkrieg bestimmten viele belastete Musikwissenschaftler den Musikdiskurs der Bundesrepublik; sie waren zum Teil bis in die Gegenwart publizistisch tätig. Otto Emil Schumann etwa veröffentlichte 1940 „Die Geschichte der deutschen Musik“; in den 1980er Jahren folgten Standardwerke wie „Der große Konzertführer“ und das „Handbuch der Klaviermusik“. Andere Musikwissenschaftler – z. B. Hans Schnoor – verbreiteten ihre antisemitischen Ausführungen zu Mendelssohn weiter (Zitat: „Musikwunder ohne seelische Substanz“). Wolfgang Boetticher, beteiligt an der Konfiszierung jüdischen Eigentums, lehrte bis 1998 an der Universität Göttingen. Friedrich Blume war ab 1949 Herausgeber der ersten Auflage des Musiklexikons Die Musik in Geschichte und Gegenwart, Heinrich Besseler folgte einem Ruf an das neueingerichtete Ordinariat für Musikwissenschaft an der Friedrich-Schiller-Universität Jena, er erhielt den Nationalpreis der DDR.

Ein aktueller Fall ist der des Musikwissenschaftlers Hans Heinrich Eggebrecht. Die Behauptung des Historikers Boris von Haken in einem Referat auf der Jahrestagung der Gesellschaft für Musikforschung in Tübingen am 17. September 2009, Eggebrecht sei an Judenmassakern auf der Halbinsel Krim beteiligt gewesen, hat eine noch im Gang befindliche Debatte ausgelöst. Eine dazu angekündigte Buchpublikation von Hakens steht noch aus. Auch in Veröffentlichungen wie Albrecht Riethmüllers "Deutsche Leitkultur Musik – zur Musikgeschichte nach dem Holocaust" werden einige musikwissenschaftliche Karrieren aufgeführt und deren geistiges Erbe kritisch beleuchtet, ebenso in der Sammlung der Referate der Tagung „Musikforschung – Faschismus – Nationalsozialismus“, die im Jahr 2000 in Schloss Engers stattfand.

Die Populäre Musik wird von der Popularmusikforschung behandelt. Wo sie früher eher der Musikethnologie zugerechnet wurde, ist sie heute entweder eigenständig oder Teil der Systematischen Musikwissenschaft.




</doc>
<doc id="12081" url="https://de.wikipedia.org/wiki?curid=12081" title="Signifikant (Begriffsklärung)">
Signifikant (Begriffsklärung)

signifikant (v. lat.: "significans", „bezeichnend, anschaulich“; von "signum", „Zeichen“) ist:


der Signifikant ist 
Siehe auch:


</doc>
<doc id="12084" url="https://de.wikipedia.org/wiki?curid=12084" title="Bewegte Bilder">
Bewegte Bilder

Bewegte Bilder nennt man eine Folge von Bildern, die durch Anzeigen in kurzen Zeitabständen mit geeigneter Technik für den Betrachter die Illusion der Bewegung erzeugen. Meist wird der Begriff synonym zu „Filmbildern“ verwendet. Für die menschliche Wahrnehmung genügen bereits etwa 16 bis 18 Bilder pro Sekunde, um die Illusion von fließender Bewegung zu erzeugen, sofern sich die Einzelbilder nur geringfügig voneinander unterscheiden.

Die Erfindung der bewegten Bilder (Laufbilder) beruht darauf, Serien von Bildern oder Momentfotografien in natürlicher Folge (1, 2, 3, 4, 5) darzubieten. Die ersten „bewegten Bilder“ waren technische Weiterentwicklungen des Daumenkinos, wie das Phenakistiskop, das Zoetrop oder das Praxinoskop. Mit dem Elektrotachyscop, der Möglichkeit zur vergrößerten Wandprojektion, führten sie schließlich zum Film.

Die Fotografien selbst oder ein Bildträger werden stillstehend gesehen. Man spricht vom unterbrochenen oder intermittierenden Transport in Verbindung mit einem Verschluss bei der Aufnahme bzw. einer Blende bei der Wiedergabe. Zwischen den Moment- oder Phasenbildern gibt es eine Dunkelpause. Bei der gewöhnlichen Filmkamera zerfällt ein sogenannter Zyklus in zwei Teile, die durch die komplementären Winkel im kreisförmigen Umlaufverschluss ausgedrückt sind, zum Beispiel 190 Grad Transport-Dunkelphase und 170 Grad Belichtungs- oder Beleuchtungsphase.

Die „Trägheit des Auges“, der sogenannte Nachbild- oder Phi-Effekt, hat hingegen nichts mit der eigentlichen Bewegungswahrnehmung im Film zu tun, was vielerorts jedoch fälschlicherweise angenommen wird. Dazu Zglinicki: „Erscheinungen, die auf Nachbildwirkungen beruhen, haben mit der Kinematographie im Grunde nichts zu tun. Nur stroboskopische Erscheinungen – allerdings in Verbindung mit der Nachbildwirkung – dürfen als unmittelbare Vorläufer des Films angesprochen werden.“

Der Nachbildeffekt sorgt lediglich dafür, dass die eigentlich verschiedenen Einzelbilder als zusammenhängend gesehen werden, wobei die Dunkelphasen als Flimmern empfunden werden. Die eigentliche Bewegungswahrnehmung entsteht durch die Unterschiede im Inhalt aufeinanderfolgender Bilder, die im Gehirn als Positionsänderung und davon abgeleiteter Geschwindigkeit interpretiert werden. Dies wird Stroboskopeffekt genannt.

Flimmerfreie Darstellung ist ab ungefähr 45 Hell-Dunkel-Wechseln pro Sekunde möglich, bei denen die meisten Menschen das Flimmern nur noch unbewusst wahrnehmen. Allerdings können sehr helle und kontrastreiche Bilder auch hier noch zum Flimmern führen. Dies kann sich nach längerer Zeit der Betrachtung bewegter Bilder durch Ermüdung der Augen oder Kopfschmerzen bemerkbar machen. Ab etwa 60 Hell-Dunkel-Wechseln pro Sekunde ist die Gefahr solcher Wirkungen weitgehend ausgeschaltet. Ein Verfahren mit 60 Bildern in der Sekunde ist Showscan.

Um nicht tatsächlich 50 oder 100 Einzelbilder pro Sekunde aufnehmen zu müssen, wurde beim Film schon früh der Trick angewandt, jedes der aufgenommenen Bilder mehrmals wiederzugeben. 16 (Phasen-) Bilder pro Sekunde kommen bei Verwendung einer Dreiflügelblende oder einer dreifach übersetzt rotierenden Blende mit 48 Hell-Dunkel-Wechseln zur Darstellung. Aus 24 pro Sekunde aufgenommenen Filmbildern, die Bildfrequenz des Tonfilms, werden mit Hilfe einer zweiflügeligen Blende 48 projizierte Bilder, was ein Kompromiss ist zwischen ruckelfreier Darstellung, minimalem Materialverbrauch und als flimmerfrei empfundener Vorführung.

Bei Video und Fernsehen werden in den europäischen Verfahren PAL und SECAM 25 Bilder bzw. 50 Halbbilder pro Sekunde gezeigt, da dies der in europäischen Stromnetzen üblichen Wechselstromfrequenz von 50 Hertz entspricht.

Neben dem bis jetzt beschriebenen Simplex-Verfahren gibt es das Duplex-Verfahren.

Zur Wiedergabe bewegter Bilder gibt es verschiedene Techniken. Die wichtigsten sind:

Plasma-, Flüssigkristall-Bildschirme und OLEDs sind aufgrund ihrer Funktionsweise nicht vom Flimmern betroffen.

Falls gleichzeitig mit den bewegten Bildern auch noch Klänge wiedergegeben werden, so handelt es sich dann um eine audiovisuelle Sequenz.



</doc>
<doc id="12085" url="https://de.wikipedia.org/wiki?curid=12085" title="Mikrofon">
Mikrofon

Ein Mikrofon oder Mikrophon ist ein Schallwandler, der Luftschall als Schallwechseldruckschwingungen in entsprechende elektrische Spannungsänderungen als Mikrofonsignal umwandelt. Dieses unterscheidet Mikrofone von Tonabnehmern, die Festkörperschwingungen umsetzen. Unterwasser-Mikrofone werden als Hydrofone bezeichnet.

In der gängigen Bauform folgt eine dünne, elastisch gelagerte Membran den Druckschwankungen des Schalls. Sie bildet durch ihre Bewegung die zeitliche Verteilung des Wechseldrucks nach. Ein Wandler, der mechanisch oder elektrisch mit der Membran gekoppelt ist, generiert daraus eine der Membranbewegung entsprechende Tonfrequenz-Wechselspannung oder eine entsprechende pulsierende Gleichspannung.

Die Entwicklung des Mikrofons ging Hand in Hand mit der Entwicklung des Telefons. In der Geschichtsschreibung werden die Entwicklungen von grundlegenden Wandlerprinzipien angeführt, die Entwicklung verschiedener akustischer Bauformen ergab sich im Zuge der Verbesserung einzelner Modelle.

Der in die USA ausgewanderte italienische Ingenieur Antonio Meucci entwickelte bereits 1860 ein Telefon auf Basis eines ebenfalls von ihm erfundenen elektromagnetischen Wandlers. Er war jedoch kein erfolgreicher Geschäftsmann und konnte das Geld für eine Patentanmeldung nicht aufbringen. Der heute meistens als Erfinder des Mikrofons angeführte schottische Taubstummenlehrer Alexander Graham Bell, der in dem Labor tätig war, in dem Meuccis Erfindung aufbewahrt wurde, meldete ein technisch gleichartiges Patent am 14. Februar 1876 an. 1887 strengte die Regierung der USA ein Verfahren zur Annullierung des Patents an. Dieses wurde jedoch nach dem Tod Meuccis und dem Auslaufen des Patents eingestellt.

Im Zuge der Entwicklung des von ihm so genannten „Telephons“ war Philipp Reis der erste, der ein Kontaktmikrophon baute, das von ihm als Teil seines Fernsprechprototypen 1861 erstmals öffentlich vorgestellt wurde. Vom Modell einer Ohrmuschel ausgehend erkannte Reis, dass statt eines Trommelfells auch ein mit einer Membran bespannter Schalltrichter verwendet werden konnte. Dieser Schalltrichter mündete bei Reis in einem Gehäusekasten. Er versah die Membran mit einem Platinkontakt, der im ruhenden Zustand einen anderen Kontakt, der im Gehäuse befestigt war, gerade noch berührte. Über diesen Kontakt und einen äußeren Widerstand wurde Gleichstrom geleitet. Fand nun an der Membran ein Schallwechseldruck statt, kam diese in Schwingung, was dazu führte, dass die Kontakte je nach dem Lauf der Schallwellen mehr oder weniger zusammengedrückt wurden. Reis hatte mit dieser Versuchsanordnung das Kontaktmikrophon erfunden, aus dessen Prinzip später das Kohlemikrophon entwickelt wurde, das in der Frühzeit des Rundfunks für Aufnahmen Verwendung fand.

Die Erkenntnis, dass Kohle die Schwingung einer Membran sehr einfach in elektrische Impulse umsetzen kann, führte Ende des 19. Jahrhunderts zur Entwicklung des Kohlemikrofons. 1877 entwickelte Emil Berliner in den Bell Labs, USA, einen Schallwandler, der den druckabhängigen Übergangswiderstand zwischen Membran und einem Stück Kohle zur Signalgewinnung nutzte. Als Erfinder des Kohlemikrofons wird jedoch David Edward Hughes angesehen, der eine ähnliche Entwicklung auf der Basis von Kohlestäben erstmals am 9. Mai 1878 in der Königlichen Akademie in London öffentlich vorstellte. Hughes kannte zudem das Kontaktmikrofon von Philipp Reis, da er 1865 mit einem importierten Telefon des deutschen Erfinders experimentiert und gute Resultate erzielt hatte.

Noch im selben Jahr verbesserte der Engländer Henry Hunnings das Mikrofon, indem er anstatt von Kohlestäben Kohlekörner verwendete. Das Kohlemikrofon in der Form, in der es im Grundprinzip die nächsten 100 Jahre nicht mehr verändert wurde, konstruierte Anthony C. White im Jahre 1890. Dieses „Kohlekörner-Mikrofon“ war als Studiomikrofon bis in die 1940er Jahre in Gebrauch; es gilt heute als erstes „richtiges“ Mikrofon und wurde erst vom Kondensatormikrofon verdrängt.

Georg Neumann entwickelte im Jahr 1923 das Kohlemikrofon weiter, wodurch die Klangqualität besonders bei tiefen Frequenzen stark verbessert wurde. Der Durchbruch gelang ihm jedoch mit der Entwicklung des Niederfrequenz (NF)-Kondensatormikrofons. Membran und Gegenelektrode bilden hier einen Kondensator, der auf eine Gleichspannung aufgeladen wird; durch die Membranbewegung ändert sich die Kondensatorkapazität, aus dieser wird das Signal gewonnen. Dieses Wandlerprinzip war der Schallaufzeichnungstechnik seiner Zeit qualitativ weit voraus und ist bei Mikrofonen höchster Qualität noch heute Standard.

1928 gründete Georg Neumann zur Vermarktung seines Kondensatormikrofons eine Firma, die "Georg Neumann & Co KG" in Berlin, die noch heute zu den führenden Mikrofonherstellern gehört. Das erste funktionstüchtige Serienmodell, das Neumann CMV3, auch „Neumann-Flasche“ genannt, ist auf vielen zeitgenössischen Filmaufnahmen zu bewundern. Legendär ist auch das erste Mikrofon mit elektrisch umschaltbarer Richtcharakteristik, das Neumann U47 von 1949. Es zählt auch heute noch zu den begehrtesten und teuersten Mikrofonen: Ein funktionsfähiges, gut erhaltenes U47 wird für rund 5000 Euro gehandelt.

1923 startete der Rundfunk. Bühnenschauspieler und Kabarettisten, die nun im Radio auftraten, fanden das Mikrofon irritierend. Zu einem Mikrofon, statt zu einem Publikum zu sprechen, war gewöhnungsbedürftig; außerdem waren Mikrofone in den späten 1920er Jahren bereits so empfindlich, dass man nicht mehr in sie hineinschreien musste. Das BBC Hand Book von 1929 widmet dem Mikrofon ein eigenes Kapitel mit der Überschrift „My Friend Mike“ ("Mike" als englische Kurzform für Mikrofon):

1962 erfanden Gerhard M. Sessler und James Edward Maceo West das Elektret-Mikrofon, eine Variante des Kondensatormikrofons, die heute mit 90 Prozent Marktanteil den häufigsten Mikrofontyp darstellt. Gerhard M. Sessler und Dietmar Hohm erfanden außerdem in den 1980er Jahren an der TH Darmstadt das Silizium-Mikrofon.

Weitere Namen, die in der Entwicklung des Mikrofons auftauchen, sind: David Edward Hughes, Sidney Shure, Fritz Sennheiser, Eugen Beyer.

"Wichtige Hersteller von dynamischen Mikrofonen:" Sidney Shure, Electro-Voice, Sennheiser, Beyerdynamic (Spezialität: Bändchenmikrofone), AKG Acoustics.

"Wichtige Hersteller von Kondensatormikrofonen:" Sidney Shure "Georg Neumann GmbH" Berlin (gehört seit 1991 zur Fa. Sennheiser), Sennheiser (Spezialität: HF-Kondensatormikrofone), "Microtech Gefell GmbH" (in Gefell, ehemals Fa. "Neumann & Co. KG", später "VEB Mikrofontechnik Gefell"), Schoeps, Danish Pro Audio (ehemals "Brüel & Kjaer"), AKG Acoustics, Brauner Microphones.

"Wichtige Hersteller von Messmikrofonen:" Brüel & Kjaer, GRAS, Microtech Gefell GmbH, Norsonic, PCB Piezotronics.

Abhängig von der akustischen Bauform des Mikrofons folgt die Membran dem Schalldruck (Druckmikrofon, ungerichtetes Mikrofon) oder dem Schalldruckgradienten (Druckgradientenmikrofon, gerichtetes Mikrofon).
Das Wandlerprinzip ist maßgeblich für die technische Qualität des Mikrofonsignals, die durch Rauschabstand, Impulstreue, Klirrfaktor und Frequenzgang charakterisiert wird.

Mikrofonwandler können wie folgt kategorisiert werden:

Das dynamische Mikrofon arbeitet nach dem Prinzip der elektromagnetischen Induktion. Technisch betrachtet führt beim Dynamischen Mikrofon die Geschwindigkeit der Membranbewegung zum Signal, nicht die momentane Auslenkung, daher bezeichnet man es auch als Geschwindigkeitsempfänger. Der Haupteinsatzbereich von dynamischen Mikrofonen ist der Live-Bereich. Das Dynamische Mikrofon wird neben dem Live-Einsatz auch zur Mikrofonierung von Schlagzeugen (Snare Drum, Becken, Tom Toms usw.) genutzt, vereinzelt auch für Vokal- oder Instrument-Aufnahmen.

Das Tauchspulenmikrofon ist eine Bauform des dynamischen Mikrofons.
Der Begriff bezieht sich auf den Aufbau des Wandlers: Bei Tauchspulmikrofonen ist die Membran wie bei einem elektrodynamischen Lautsprecher fest mit einer Spule (Tauchspule) verbunden, die durch die Membranbewegung in einem dauermagnetischen Feld (Luftspalt eines Topfmagneten) bewegt wird. Die relative Bewegung von Spule und Magnetfeld erzeugt durch Induktion die Signalspannung. Die Vorteile dieses Mikrofontyps sind:

Tauchspulenmikrofone haben aufgrund der Spulenmasse ein nach oben begrenztes Wiedergabespektrum sowie ein schlechtes Impulsverhalten. Sie sind gut für Nahaufnahmen geeignet, da ihre nichtlinearen Verzerrungen auch bei hohen Schallpegeln gering sind.

Ein Bändchenmikrofon (engl. "ribbon microphone") ist eine Bauform des Dynamischen Mikrofons. Bei diesem Mikrofontyp sind Wandlerprinzip und akustische Funktionsweise eng verknüpft.

Die Membran des Bändchenmikrofons ist ein zickzack-gefalteter Aluminiumstreifen von zwei bis vier Millimetern Breite und einigen Zentimetern Länge. Er ist nur wenige Mikrometer dick.
Bei Anregung durch eintreffenden Schall induziert die Bewegung im Magnetfeld eine der Bewegungsgeschwindigkeit entsprechende Spannung, die an den Enden der Aluminiumstreifen abgegriffen werden kann.

Bändchenmikrofone besitzen einen im Arbeitsbereich nahezu linearen Frequenzgang; ihre äußerst leichte Membran verleiht ihnen ein gutes Impulsverhalten. Prinzipbedingt kann die Membran von beiden Seiten vom Schall erreicht werden. Die akustische Bauweise ist daher die eines Druckgradientenmikrofons. Daraus folgt die Richtcharakteristik einer Acht. Bändchenmikrofone sind nicht für die Aufnahme tiefster Frequenzen geeignet.

Das Kondensatormikrofon (engl. ) arbeitet nach dem physikalischen Prinzip des Kondensators. Da die Membranauslenkung und nicht die Membrangeschwindigkeit zum Signal führt, ist das Kondensatormikrofon technisch betrachtet ein Elongationsempfänger.

Kondensatormikrofone kommen in den verschiedensten Erscheinungsformen vor, da mit diesem Begriff nur das Wandlerprinzip bezeichnet wird. Der Begriff hat sich aber umgangssprachlich als Mikrofon-Klasse etabliert, da klangliche Eigenschaften mit dem Prinzip der Wandlung eng verknüpft sind.

Beim Kondensatormikrofon ist eine wenige tausendstel Millimeter dicke, elektrisch leitfähige Membran dicht vor einer Metallplatte elektrisch isoliert angebracht.
Technisch betrachtet entspricht diese Anordnung einem Plattenkondensator, der eine elektrische Kapazität besitzt.
Eintreffender Schall bringt die Membran zum Schwingen, wodurch sich der Abstand der beiden Kondensatorfolien und damit die Kapazität des Kondensators verändert. Solche Geräte können auch als Mikrosysteme realisiert werden.

Sobald eine elektrische Spannung angelegt wird, entsteht zwischen der Membran und der Platte ein Potentialgefälle. Die Kapazitätsschwankungen führen bei hochohmiger Versorgung (typischerweise Gigaohm-Bereich) zu Spannungsschwankungen bei im Wesentlichen konstanter Ladung des Kondensators – einem elektrischen Signal. Die Kapazität der Kapsel und der Versorgungswiderstand bilden als RC-Glied einen Hochpass, durch den die tiefste übertragbare Frequenz begrenzt wird. Um das Potentialgefälle zwischen den Kondensatorplatten zu erreichen sowie zur Versorgung des Mikrofonverstärkers (Impedanzwandler) ist eine Spannungsquelle notwendig. Üblicherweise nutzt man die 48-Volt-Phantomspeisung des Mikrofonvorverstärkers oder des Mischpults; siehe auch: Symmetrische Signalübertragung.

Alternativ kann die Kapazität des Kondensators auch mit HF-Technik gemessen werden. Dazu kann die Impedanz gemessen werden, insbesondere in einer Messbrücke mit phasenempfindlicher Auslesung, oder die Kapsel wird als frequenzbestimmendes Bauteil in einem Oszillator eingesetzt. Das erübrigt die Beschränkung auf hochohmige Folgeverstärkung. Außerdem ist es möglich, bis zu beliebig tiefen Frequenzen ein Signal zu generieren (eigentlich ist das Mikrofon dann ein schnelles Barometer). Die Kapsel wird auf andere Parameter optimiert als bei der NF-Technik, sie muss z. B. weniger spannungsfest sein. Der Schaltungsaufwand ist in der Regel höher als bei der NF-Technik. Bei unsauberer Versorgung (mit Resten der Taktung eines Switch-Generators für die Phantomspannung) können durch Intermodulation Störungen entstehen. Auch hier wird die Schaltung meist über Phantomspeisung versorgt.

Kondensatorkapseln sind sowohl als Druckmikrofon wie auch als Druckgradientenmikrofon gebräuchlich. Manche Kondensatormikrofone haben eine umschaltbare Richtcharakteristik. Ermöglicht wird dieses durch die Kombination zweier Druckgradientenmikrofone (Doppelgradientenmikrofon).

Der Kondensatorschallwandler ist heute wegen der hohen Signalqualität Aufnahmestandard in Tonstudios. Er ist allerdings recht empfindlich (insbesondere gegen Feuchtigkeit in jeder Art) und kann sogar durch sehr hohen Schalldruck beschädigt werden. Im Beschallungs- und Livebereich dominieren daher dynamische Schallwandler.

Das Elektretmikrofon ist eine mit dem Kondensatormikrofon eng verwandte Bauform. Auf die der Membran gegenüberliegende Kondensatorplatte ist eine Elektretfolie aufgebracht, in der die Membranvorspannung sozusagen „eingefroren“ ist. Üblicherweise befindet sich in der Mikrofonkapsel noch ein Mikrofonverstärker zur Verstärkung der schwachen Signalströme. Er benötigt mit 1,5 Volt eine viel geringere Spannung als ein reines Kondensatormikrofon (48 Volt). Der Strombedarf von 1 mA begünstigt den Einsatz in mobilen Geräten.

Elektretmikrofone sind mit 90 % Marktanteil die weltweit am häufigsten hergestellten und eingesetzten Mikrofone. Dank ihrer extrem kompakten Bauweise, des geringen Preises und der guten Signalqualität finden sie sich in praktisch jedem modernen Sprachkommunikationsmittel (Headsets, Handys, Hörgeräte usw.). Die Größe der Mikrofonkapsel liegt meistens zwischen einem Millimeter und einem Zentimeter. Der Frequenzgang kann bei guten Elektretmikrofonen als Druckempfänger (Mikrofon mit Kugelcharakteristik) von 20 Hz bis 20 kHz gehen.

Elektretmikrofone eignen sich nicht für große Schallamplituden – sie erzeugen dann nichtlineare Verzerrungen.

Als Kohlemikrofon wird ein elektroakustisches Wandlerprinzip bezeichnet, bei dem die Druckschwankungen des Schalls Änderungen eines elektrischen Widerstandes bewirken. Zur Wandlung dient dabei der druckabhängige Übergangswiderstand im hinter der Membran gelagerten Kohlegranulat.

Kohlemikrofone besitzen schlechte Wiedergabeeigenschaften; die Masse der Metallmembran begrenzt und verzerrt den Frequenzgang, die Kohlekörner verursachen insbesondere bei Bewegung Rauschen. Durch die nichtlinearen Zusammenhänge zwischen Druck und Übergangswiderstand der Kohlekörner entstehen nicht reproduzierbare, nichtlineare Verzerrungen.

Der Hauptvorteil des Kohlemikrofones ist dessen hohes Ausgangssignal – es liefert in einem Gleichspannungskreis einen für die Fernübertragung und Wiedergabe mit einer elektromagnetischen Hörkapsel ausreichendes Signal. Verstärkung ist nicht notwendig.

Kohlemikrofone wurden daher früher in großer Stückzahl in Telefonen eingesetzt. Man geht davon aus, dass durch die Erfindung des Kohlemikrofons die Entwicklung des Fernsprechwesens außerordentlich beschleunigt wurde.
Nach einer gewissen Zeit verdichtete sich das Kohlegranulat in den Mikrofonen der Telefone, was zu einer deutlichen Minderung der Sprachqualität führte. Aus diesem Grund wurden seit den 1970er Jahren dynamische Kapseln oder Elektret-Kapseln mit einer Zusatzschaltung zur Verstärkung und Signalanpassung eingesetzt. Diese Module konnten die Kohlemikrofone in Telefonen ohne Schaltungsänderung ersetzen.

In der professionellen Tontechnik wurde das Kohlemikrofon bereits in den 1920er und 1930er Jahren vom Kondensatormikrofon verdrängt. In der Kommunikationstechnik dominiert heute das Elektretmikrofon den Markt.

Ein Piezomikrofon ist eine Mikrofonbauform, deren Wandlerprinzip auf den Eigenschaften piezoelektrischer Elemente beruht.
Eine Membran folgt den Druckschwankungen des Schalls. Sie ist mechanisch mit einem piezoelektrischen Element gekoppelt. Es wird durch die Druckschwankungen minimal verformt und gibt diese als elektrische Spannungsschwankungen aus. Als piezoelektrisches Material wird üblicherweise die Piezokeramik Blei-Zirkonat-Titanat (PZT) verwendet.

Solche Mikrofone waren in den 1930er bis 1950er Jahren populär. Sie sind mechanisch robust und haben Vorteile durch ihre simple Bauweise. Ein großer Nachteil dieser Wandlertechnik ist der hohe Klirrfaktor. Sie eignen sich prinzipiell nicht für hochqualitative Aufnahmen und konnten sich auch in der Telekommunikationstechnik nicht gegen das Kohlemikrofon durchsetzen.
Die Schwingungswandlung durch piezoelektrische Elemente ist hingegen bei Kontaktschallwandlern (Tonabnehmer in Plattenspielern und für Instrumente, Körperschallaufnehmer, Schwingungsaufnehmer) weit verbreitet. Die hier zur Verfügung stehenden Kräfte sind in der Regel wesentlich größer und führen zu besseren Übertragungseigenschaften als es bei Luftschall der Fall ist.

Die akustische Bauform ist entscheidend für die Richtcharakteristik und den Frequenzgang.
Im Gegensatz zu Lautsprechern spielt die Membrangröße bei Mikrofonen bezüglich deren Tiefenwiedergabe keine Rolle, da Mikrofone wie die menschlichen Ohren lediglich als Sensoren wirken und nicht wie Lautsprecher Luft im tieffrequenten Bereich mit möglichst geringem Hub zu verdichten haben. Eine Ausnahme sind Infraschall-Sensoren.

In der Mikrofontechnik beschreibt die Richtcharakteristik in Form eines Polardiagramms die Empfindlichkeit eines Mikrofons, also die Ausgangsspannung im Verhältnis zum Schalldruck, in Abhängigkeit vom Schalleinfallswinkel. Man kann dabei zwischen den Verhältnissen im Direktfeld und im Diffusfeld differenzieren.

Die Richtcharakteristik hängt von der akustischen Bauform der Mikrofonkapsel und von äußeren Formelementen (z. B. Grenzflächenmikrofon, Richtrohrmikrofon) ab. Die Stärke der Richtwirkung beschreibt man mit dem Bündelungsgrad bzw. dem Bündelungsfaktor.
Die Richtcharakteristik von Mikrofonen wird in reflexionsarmen Räumen im Direktfeld D gemessen. Dabei wird das Mikrofon in 1 m Abstand von einer 1-kHz-Schallquelle gedreht und dabei der Ausgangspegel des Mikrofonsignals in Abhängigkeit vom Einfallswinkel gemessen.

Die Richtwirkung ist durch charakteristische Muster gekennzeichnet:

Ein reines Druckmikrofon besitzt keine Richtwirkung, also eine kugelförmige Richtcharakteristik (omnidirektional). Ein Druckgradientenmikrofon in seiner reinen Form (z. B. Bändchenmikrofon) liefert als Richtcharakteristik eine Acht. Die Richtcharakteristik „Keule“ wird durch das Prinzip des Interferenzrohres gewonnen (Richtrohrmikrofon).

Als standardisierte Formen zwischen Kugel- und Achtercharakteristik gibt es „breite Niere“, „Niere“, „Superniere“ und „Hyperniere“.

Aufgrund der komplexen Verhältnisse des Schallfelds weicht der reale Richtcharakter in der Praxis von diesen theoretischen Mustern individuell ab. Starke Abweichungen der Muster sind dann zu beobachten, wenn die Wellenlänge der Signalfrequenz sich im Bereich des Kapseldurchmessers bewegt. Daher sind diese Verzerrungen umso geringer, je kleiner der Membrandurchmesser ist.
Bei Druckgradientenmikrofonen, deren Richtcharakter durch akustische Laufzeitelemente oder Doppelmembranbauweise von der reinen Acht etwa zur Niere modifiziert wurde, sind die größten Abweichungen zu erwarten.
Bei Druckmikrofonen führen etwa der Druckstaueffekt wie auch Schallabschattung durch den Mikrofonkörper zu einer Richtwirkung bei hohen Frequenzen.

Sollen die Abweichungen von der theoretischen Richtcharakteristik auch bei hohen Frequenzen vermieden werden, darf das Mikrophon nur einen Bruchteil (weniger als die Hälfte) der Wellenlänge bei der höchsten benötigten Frequenz als Abmessungen des Schallwandlers haben. Realisiert wird dies bei Messmikrophonen mit typischerweise 6 mm Durchmesser der Kapsel. Da die aufnehmende Fläche und die aufgenommene Schallenergie quadratisch zum Durchmesser sind, führt das zu unempfindlichen Mikrophonen mit verhältnismäßig schlechtem Rauschverhalten. Mikrophone für Aufnahmeanwendungen sind daher meist größer.

Druckmikrofone (Mikrofon mit Druckcharakteristik, "Druckempfänger") arbeiten vorwiegend ungerichtet (Kugelcharakteristik). Diese Bauweise ist weit verbreitet in Form von Elektretmikrofonen, z. B. in Mobiltelefonen oder Headsets.

Bei einem Druckmikrofon ist die Mikrofonkapsel im Gegensatz zu der eines Druckgradientenmikrofons rückseitig geschlossen: Die schallaufnehmende Membran ist vor einem nach hinten geschlossenen Hohlraum angebracht. Dieser verhindert, dass der Schall die Membran umwandert und sich auch an deren Rückseite auswirkt. Einfallender Schall wird unabhängig von der Einfallsrichtung immer in gleicher Polarität wiedergegeben. Das Druckmikrofon reagiert ähnlich wie ein Barometer auf Luftdruckschwankungen. Daher kann ein solches Mikrofon auch bei sehr tiefen Frequenzen bis in den Infraschallbereich eingesetzt werden. In der Messtechnik werden daher üblicherweise Druckmikrofone verwendet.

Für Druckmikrofone wird immer die Richtcharakteristik einer Kugel angegeben. Sämtliche Mikrofone mit anderen Richtcharakteristiken als die der Kugel, speziell solche mit umschaltbarer Charakteristik, werden mit der Bauform des Druckgradientenmikrofons realisiert.

Bei einem Druckgradientenmikrofon (Mikrofon mit Druckgradientencharakteristik) ist die Mikrofonkapsel im Gegensatz zu einem Druckmikrofon rückseitig offen – die Membran ist für den Schall von allen Seiten zugänglich. Diese Mikrofonbauform wird wissenschaftlich auch als "Druckgradientenempfänger" oder "Schnelle-Empfänger" bezeichnet.

Da der Schall auch die Rückseite der Membran erreicht, folgt diese nicht dem absoluten Schalldruck, wie es beim Druckempfänger der Fall ist, sondern dem Druckgradienten bzw. der Schallschnelle. Ein typisches Beispiel ist das Bändchenmikrofon.

Die Druck-Differenz ergibt sich, da der Schall die Membran umwandern muss, um sich auch auf der Rückseite auszuwirken. Die dazu benötigte Zeit "Δt" resultiert in einer „Druckdifferenz“ (einem Druckgradienten).

Bei gegebenem "Δt" ist der Druckgradient umso höher, je schneller der Schalldruckwechsel erfolgt. Zu tiefen Frequenzen hin sinkt der resultierende Druckgradient "Δp" entsprechend ab. Siehe: akustischer Kurzschluss.

Trifft ein Signal genau von der Seite (90°) auf die Membran, so ergibt sich keine Druckdifferenz und somit auch keine Membranbewegung. Bei Beschallung der Membranrückseite ist die Polarität des Mikrofonsignals gedreht (spannungsinvertiert).

Die Richtcharakteristik ist in der beschriebenen symmetrischen Grundbauweise die einer Acht. Durch die Gestaltung des Mikrofons lassen sich auch andere Richtcharakteristiken realisieren, die zwischen Kugel und Acht liegen, wie die breite Niere, die Niere, die Superniere und die Hyperniere.

Sämtliche Richtcharakteristiken außer der Kugel (Druckmikrofon) können auch nur mit Druckgradientenmikrofonen realisiert werden.

Der Begriff Grenzflächenmikrofon, engl.: „“ oder „“, bezeichnet eine Mikrofonbauform hinsichtlich ihrer akustischen Funktionsweise. Es stellt einen Sonderfall dar, weil hier der Mikrofonkörper konzeptioneller Teil der akustischen Bauform ist.

Der Mikrofonkörper ist eine Platte, auf der meistens eine Druckmikrofonkapsel membranflächenbündig eingelassen ist. Seine Richtcharakteristik ergibt somit eine Halbkugel. Die Wandler sind üblicherweise in Kondensator- oder Elektretbauweise ausgeführt.
Diese Bauart wurde entwickelt, um die vorteilhaften akustischen Eigenschaften auszunutzen, die an schallreflektierenden Flächen auftreten, ohne das Schallfeld selbst zu beeinträchtigen. Das Mikrofon wird auf eine große schallreflektierende Fläche, z. B. auf den Fußboden oder einen Tisch, gelegt. Es erhält so den maximalen Schalldruck ohne Überlagerungen von Raumschallanteilen, was zu einem ausgewogenen Frequenzgang und einem akustisch guten Raumeindruck führt:

Bei einem Richtrohrmikrofon, auch "Interferenzmikrofon" (engl. "shotgun microphone") ist der Mikrofonkörper durch ein vorgebautes "Interferenzrohr" ergänzt.

Ein Richtrohrmikrofon besitzt eine ausgeprägte Keulencharakteristik, die durch ein vor ein Druckgradientenmikrofon vorgebautes, mit seitlichen Schlitzen oder Bohrungen versehenes, nach vorn offenes Interferenzrohr zustande kommt. Dieses bewirkt, abhängig von der Rohrlänge, eine deutliche Verstärkung der Richtwirkung ab etwa 1 bis 2 kHz. Bei tieferen Frequenzen entspricht die Richtwirkung derjenigen der Mikrofonkapsel (Nieren- oder Supernierencharakteristik).

Als Wandler sind Kondensator- oder Elektretmikrofone üblich.

Hohlspiegelmikrofone werden (besonders in Aeroakustik-Windkanälen mit offener Messstrecke) häufig zur Ortung von Geräuschen eingesetzt. Meistens werden Straßenfahrzeuge oder Flugzeuge untersucht.

Unter anderem zur Vogelbeobachtung werden Mikrofone im Fokus eines Parabolspiegels als Richtmikrofon verwendet. Die Richtwirkung tritt – abhängig von der Spiegelgröße – nur bei hohen Frequenzen (ab etwa 1 kHz) ein.

Die aus der Schallwandlung resultierende Wechselspannung, das Mikrofonsignal, ist durch folgende Kenngrößen gekennzeichnet:

Der Frequenzgang eines Mikrofons resultiert aus seiner akustischen Bauform, der Mikrofonabstimmung und dem Wandlerprinzip.
Je kleiner und je leichter die Membran (und gegebenenfalls die Tauchspule) ist, desto weniger Eigenresonanzen besitzt sie im hörbaren Frequenzband (20 Hz bis 20 kHz). Je weniger sie selbst in Resonanz gerät, desto unverzerrter gibt sie den Klang wieder.
Die akustische Bauform setzt etwa beim Druckgradientenmikrofon Grenzen zu tiefen Frequenzen hin; zudem ist der Frequenzgang aller Mikrofone abhängig vom Beschallungswinkel (Richtcharakteristik, Druckstaueffekt) und beim Druckgradientenmikrofon von dem Abstand zur Schallquelle (Nahbesprechungseffekt).

Mikrofone wandeln Schalldruck in Wechselspannung um. Man misst den Feldübertragungsfaktor in Millivolt pro Pascal (mV/Pa), der etwa proportional mit der Membrangröße ansteigt. So haben zum Beispiel bei Elektretmikrofonen kleine 1/4-Zoll-Kapseln 5 bis 10 mV/Pa, 1/2-Zoll-Kapseln 30 bis 50 mV/Pa, Ein-Zoll-Kapseln kommen bis auf 100 mV/Pa.

Je kleiner eine Kapsel ist, desto stärker ist sie aufgrund des geringen Übertragungsfaktors für Rauschen anfällig. Ursache des Rauschens ist jedoch nicht die Mikrofonmembran, sondern der elektrische Innenwiderstand der Kapsel. Das ist zum Beispiel bei dynamischen Mikrofonen der Widerstand der Tauchspule, beim Elektretmikrofon der Lastwiderstand. Je höher der Innenwiderstand ist, desto mehr rauscht das Mikrofon, umso höher ist in der Regel jedoch auch die Ausgangsspannung. Verglichen mit Tauchspulmikrofonen besitzen Elektretkapseln einen mindestens zehn Mal höheren Abschlusswiderstand und damit mindestens √10-mal (√10 ≈ 3) höheres Rauschen – sie liefern jedoch auch wesentlich höhere Signalspannungen.

Als Impedanz bezeichnet man den elektrischen Ausgangswiderstand des Mikrofons bei Wechselspannung im Tonsignalbereich.
Während dynamische Mikrofone häufig Impedanzen um 600 Ω besitzen, haben Kondensator-Kapseln eine sehr hohe Impedanz, da sie aber einen Arbeitswiderstand benötigen, erscheint nur dieser als Impedanz nach außen (bei Elektretmikrofonen im Bereich zwischen 1 und 5 kΩ). Je hochohmiger der Ausgang des Mikrofons ist, desto stärker macht sich die Kabelkapazität der Anschlussleitung bemerkbar: Hohe Frequenzen werden durch lange Kabel gedämpft.

Der Klirrfaktor gibt den Anteil nichtlinearer Signalverzerrungen am Nutzsignal in Prozent an.
Bei dynamischen Mikrofonen ist der Klirrfaktor gering, nichtlineare Verzerrungen kommen in der Regel nur bei sehr großen, nicht relevanten Schallpegeln vor.
Der nichtlineare Zusammenhang der Membranauslenkung zur abgegebenen Spannung verzerrt bei Elektret- und Kondensatormikrofonen prinzipbedingt das Signal ab bestimmten Pegeln nichtlinear.

Die häufigsten Brummstörungen entstehen durch Erdschleifen (auch Brummschleifen genannt). Somit ist meistens nicht das Mikrofon selbst, sondern das Kabel und die Art des Anschlusses für solche Störungen verantwortlich. Diese können durch differenzielle (symmetrische) Leitungsführung bzw. getrennt zur Abschirmung geführte Masseleitungen beseitigt werden. Die Störempfindlichkeit nimmt mit der Kabellänge zu. Eine gute Abschirmung des Kabels kann den elektrischen Störeinfluss beseitigen, gegen magnetische Störungen sind symmetrische Kabel ohnehin unempfindlich.

Mikrofonkabel haben teilweise einen Mikrofonieeffekt, sie sind empfindlich gegen Trittschall und Bewegung, wenn ihre Umflechtung bzw. Abschirmung bei Bewegung wechselnde Kontaktwiderstände erzeugt. Mikrofonie-Armut ist ein Qualitätskriterium für Mikrofonkabel.

Der AES42-Standard definiert eine digitale Schnittstelle für Mikrofone, die direkt einen digitalen Audiostrom erzeugen. Die Verarbeitungskette Impedanzwandler – Mikrofonvorverstärker – A/D-Wandler ist im Mikrofongehäuse integriert. Der Anschluss erfolgt durch einen XLR-Stecker, die Energieversorgung der Elektronik über Phantomspeisung (Digital Phantom Power (DPP), 10 V, max. 250 mA). Durch Modulation der Phantomspannung können solche Mikrofone fernbedient werden, etwa um Dämpfung/ Richtcharakteristik einzustellen.

Die symmetrische Signalübertragung ist insbesondere bei großen Leitungslängen weniger anfällig gegen Störsignale.
Diese Anschlussnormen sind heute am gängigsten. Manche ältere Mikrofone haben einen DIN- oder Tuchelstecker. Vereinzelt gibt es auch den „Klein-Tuchel“ – speziell bei kompakten Ansteckmikrofonen mit separatem Funksender.

Bei allen Mikrofonen gilt: Das „Männchen“ am Mikrofonstecker gibt das Signal ab und das „Weibchen“ an der Kabelkupplung nimmt das Signal an.

Einfache Mikrofone sind unsymmetrisch und besitzen als Anschlussleitung nur ein Koaxialkabel (2 Leitungen). Bei Elektretmikrofonen mit Tonaderspeisung ist dies ebenso – sie arbeiten am PC / an der Soundkarte, indem sie die an diesen Mikrofoneingängen bereitgestellte Speisespannung (in der Regel 5 V) nutzen und auf den Quellwiderstand (einige Kiloohm) dieser Spannung arbeiten.

Kabellose Mikrofone werden überall dort eingesetzt, wo eine Kabelverbindung aus technischen, praktischen oder optischen Gründen von Nachteil ist.
So sind etwa auf Bühnen dynamische Gesangsmikrofone mit integriertem Sender ("Bild") anzutreffen. Elektret-Ansteckmikrofone oder Kopfbügelmikrofone (auch Nackenbügelmikrofon bzw. Headset genannt) mit separatem batteriebetriebenem Funksender ("Bodypack") finden häufig bei Fernsehproduktionen oder auch bei Aufführungen von Musicals Verwendung.

Nachteile der Funkübertragung sind vor allem ein hoher Anschaffungspreis und höhere Betriebskosten (Batteriebetrieb).

Funkmikrofone übertragen in Europa das Nutzsignal meistens frequenzmoduliert (FM) auf dem anmeldefreien Frequenzband um 433 oder oberhalb 862 MHz, die Reichweite beträgt zwischen 100 und 250 m. Welche Frequenzbänder genutzt werden dürfen, hängt von den Vorschriften des entsprechenden Landes ab. In Deutschland existieren auch Allgemeinzuteilungen im Bereich von 790 bis 862 MHz, die allerdings aufgrund der Digitalen Dividende im Jahre 2015 auslaufen.
Im Februar 2011 erfolgte für die drahtlose Mikrofonnutzung die Zuweisung des als "Duplexmittenlücke" (auch "Duplexlücke" oder "Mittenlücke") bezeichneten Frequenzbereiches von 823 bis 832 MHz. Diese Zuteilung ist befristet bis 31. Dezember 2021.
Störungen werden bei professionellen Übertragungssystemen durch redundante Übertragung auf zwei Ausbreitungspfaden vermieden (Antenna Diversity). Zur Erhöhung der Dynamik wird bei den professionelleren Systemen ein Kompandersystem, bestehend aus einem Kompressor auf der Senderseite und einem Expander auf der Empfängerseite, eingesetzt; dadurch werden bis zu 110 dB Dynamik des Audiosignales erreicht. Einige Modelle übertragen die Signale digital (dann meistens im 2,4-GHz-ISM-Band).

Den Einsatz von Mikrofonen bezeichnet man als Mikrofonierung. Dabei wird je nach Anwendung nach technischen, klanglichen und wirtschaftlichen Gesichtspunkten optimiert. Zur Positionierung werden diverse Mikrofonstative eingesetzt.

Anwendungsbezogen können Mikrofone außerdem kategorisiert werden:

Sonderbauformen:

Reine Festkörperschwingungswandler und damit technisch gesehen Tonabnehmer und keine Mikrofone sind das

Zwei Mikrofone zusammen bilden ein Mikrofonsystem für Stereoaufnahmen, die damit einen ganz bestimmten Aufnahmebereich für die Hörereignisrichtung auf der vollen Stereo-Lautsprecherbasis einfangen. Es gibt eine Reihe von Stereo-Mikrofonierungsverfahren, die auf psychoakustischen Effekten beruhen:

Eine Besonderheit stellt die Raumklang-Mikrofonierung zur Aufzeichnung von besonders räumlichen 5.1-Raumklangsignalen dar. Eingesetzt werden solche Systeme im Kino- und Orchesterbereich.

Die akustische Messtechnik verwendet Mikrofone mit Kugelcharakteristik und möglichst linearem Frequenzgang. Eine Spezialanwendung ist die Lokalisierung von Schallquellen mittels Mikrofonarrays.

Ein Wind- oder Popschutz schützt Mikrofone vor Luftströmungen, die beim Sprechen oder im Freien auftreten und polternde, rumpelnde, „poppende“ (insbesondere bei Verschlusslauten wie „B“ oder „P“) Nebengeräusche verursachen. Mitunter sind die Geräusche so laut, dass sie den nachfolgenden Verstärker übersteuern und ein regelrechtes Knallen entsteht. Druckempfänger sind weniger anfällig als Druckgradientenempfänger. Verwendet werden Schaumstoff- oder Fellüberzüge (Jargon: "Windjammer", "Fell", "Zwelch", "Tote Katze", "Hund", "Pudel" oder "Puschel") sowie in Tonstudios auch Popschirme. Viele Mikrofone haben zum Schutz der Membran einen fest installierten Korb aus Metall- und Gazegeflecht, der auch Wind in Grenzen abhält. Bei Studiomikrofonen dient der Popschutz außerdem dazu, die beim Sprechen und Singen entstehende Feuchtigkeit und das Kondensat von der empfindlichen Kondensatormembran fernzuhalten.

Um Rumpeln oder Poltern im Tonsignal, hervorgerufen durch Erschütterungen (Körperschall), vom Mikrofon zu entkoppeln, werden Studio-Mikrofone am Stativ in eine elastische Aufhängung, die "Spinne", eingehängt. Spinnen bestehen aus einer Halterung, in der das Mikrofon durch ein im Zickzack gespanntes Gummiband frei schwingen kann. Kohlemikrofone sind besonders erschütterungsempfindlich, weshalb man auf alten Fotos oft auch Redner-Mikrofone außerhalb des Studios in elastischen Halterungen sieht. Gesangsmikrofone haben zur Entkopplung von Griffgeräuschen meistens eine Lagerung der Mikrofonkapsel mit Elastomerschäumen.





</doc>
<doc id="12086" url="https://de.wikipedia.org/wiki?curid=12086" title="Terrestrische Frequenzen">
Terrestrische Frequenzen

Als terrestrische Frequenzen bezeichnet man Frequenzen elektromagnetischer Wellen, die zur terrestrischen Übertragung von Fernseh- und Rundfunkprogrammen genutzt werden.

Diese Frequenzen sind für höhere Frequenzen, die nur begrenzte Ausbreitungsbereiche haben, landesabhängig

Im deutschsprachigen Europa werden dafür genutzt:
Für weitere Informationen siehe auch Frequenzband und Antennen-Fernsehen.


</doc>
<doc id="12087" url="https://de.wikipedia.org/wiki?curid=12087" title="Satellitenfrequenzen">
Satellitenfrequenzen

Als Satellitenfrequenzen bezeichnet man Frequenzen elektromagnetischer Wellen, die zur Übertragung von Fernseh- und Rundfunkprogrammen über Satelliten genutzt werden (siehe auch Satellitenrundfunk).

Die Empfangskanäle der Satellitenfrequenzen liegen bei europäischen Fernsehsatelliten im sogenannten Ku-Band 10,7–12,75 GHz, das sich aus folgenden Frequenzspektren zusammensetzt:

Die Sendefrequenzen (von der Erde zum Satelliten) sind ebenfalls auf den Seiten der einzelnen Unterbereiche des Ku-Bandes angegeben.

In Europa wird die Sendeleistung überwiegend von den Fernsehsatelliten Astra und Eutelsat übernommen. Einzig für den Empfang türkischer Fernsehprogramme wird der Fernsehsatellit Türksat verwendet, welcher mittels eines besonderen Transponders sowohl in Europa als auch Asien eingesetzt werden kann.



</doc>
<doc id="12088" url="https://de.wikipedia.org/wiki?curid=12088" title="Volksgruppe">
Volksgruppe

Das Wort Volksgruppe bezeichnet allgemein eine ethnische Gruppe, im engeren Sinne eine Minderheit innerhalb eines Staates, speziell eine einzelne ethnische Gruppe innerhalb eines polyethnischen Staates (Vielvölkerstaat). Eine "autochthone Volksgruppe" (griechisch: „eingesessene“, ursprüngliche, siehe Wortherkunft) ist eine ethnische, in Europa rechtlich geschützte "nationale Minderheit". Umgangssprachlich wird „Volksgruppe“ oft bedeutungsgleich zu "ethnische Minderheit" verwendet.

Wenn es um den Schutz von Volksgruppen innerhalb eines Staates geht, spricht man von "Minderheitenschutz", in Österreich auch von "Volksgruppenförderung".

Die Bezeichnung darf nicht mit "Völkergruppe" (abstrakte Zusammenfassung verschiedener Völker oder Ethnien wie etwa "Indianer" oder "Papua") verwechselt werden.

Anders als bei den Begriffen "Minderheit" und "indigene Völker" (lat. „eingeborene“, siehe Wortherkunft) existiert – abgesehen von der rechtlichen Definition des Gesetzgebers in Österreich – keine allgemein anerkannte spezifische Definition einer "Volksgruppe", was sich auch in der wechselnden Anwendung der Bezeichnung auf Minder- und Mehrheiten niederschlägt.

So werden etwa die afrikanischen Hutu und Tutsi oftmals gleichermaßen als „Volksgruppen“ bezeichnet, obwohl die erstere in Ruanda und Burundi die zahlenmäßige Mehrheit bildet, wohingegen die letztere die parlamentarische Mehrheit in Ruanda stellt. Außer der Zeit seit der Unabhängigkeit 1962 bis zum Genozid, beherrscht das Hirtenvolk der Tutsi traditionell die bäuerliche Hutu-Mehrheit.

In Fällen, wo mehrere Gruppen in etwa gleich stark vertreten sind, werden diese zu Volksgruppen, beispielsweise Flamen und Wallonen in Belgien. Gleichzeitig werden oft ethnische Minderheiten mit diesem Begriff benannt, so etwa deutschsprachige Minderheiten in Ostmitteleuropa (vgl. Nationalität).

Die Bezeichnung "Volksgruppe" kennt keine genaue Entsprechung in anderen Sprachen. Im Ungarischen existiert dasselbe Wort "népcsoport". Im Englischen, Spanischen und Französischen wird der Begriff zumeist als ethnische Gruppe verstanden.

Der Begriff "Volksgruppe" wurde durch die deutsche Kulturpropaganda und die politische „Grenzlandarbeit“ im Ersten Weltkrieg bekannt. Max Hildebert Boehm von der „Arbeitsstelle für Nationalitäten- und Stammesprobleme“ prägte diesen Begriff in der völkischen Bewegung und versuchte ihn zu verwissenschaftlichen.

Der Begriff der Volksgruppe ist aufgrund der explodierenden Bevölkerungszahlen und der damit verbunden, bis dahin unbekannten Bevölkerungsdichte insbesondere im 19. und 20. Jahrhundert zu einem besonderen Thema geworden. Radikalisiert wurde das ohnehin problematische Volksgruppenkonzept im Rahmen der verbrecherischen Volksgruppenpolitik der Nationalsozialisten. Diese benutzten die deutsche Minderheiten in Nachbarländern nicht allein, um gebietsmäßige Ansprüche geltend zu machen, sondern förderten insbesondere auch in Südosteuropa eine Politik des "divide et impera" („Teile und herrsche!“), deren Kern es war, verschiedene Volksgruppen gegeneinander auszuspielen.

Nach dem Volksgruppenkonzept organisieren sich die Menschen vielfach unter der Leitung von „Volksgruppenführern“ nicht nur nach ihren sozialen Bedürfnissen, sondern ordnen diese auch abstammungsorientierten (Volk) und daher nationalen Idealen unter. Das Konzept wurde noch in den 1970er Jahren im Kontext der Südtirolfrage und der Sudetendeutschen Landsmannschaft reaktiviert.

Der deutsche Sozialwissenschaftler Samuel Salzborn kritisiert, dass sich das Volksgruppenkonzept auf einen romantischen Volksbegriff beziehe und diesen insofern politisiere, als damit eine raumordnerische Konsequenz aus der kulturellen Teilung der Menschheit in Völker und Volksgruppen gezogen wird. Soziale und politische Konflikte werden „damit naturalisiert und in einen ethnischen Entstehungszusammenhang gerückt“. Indem Ethnizität als grundlegende Kategorie gedacht wird und zum höchsten Gut des „menschlichen Wesens avanciert, besteht das politische Ziel in einer kompletten sozialen und politischen Segregation von Menschen entlang ethnischer Kriterien sowie in der Schaffung separierter Ethnoregionen für die einzelnen Volksgruppen.“

Das Konzept betont „die ethnisch-kulturelle Homogenität der Bevölkerung, oder zumindest ihre kulturell-mentalitätsmäßige Ähnlichkeit bis hin zur gemeinsamen Betroffenheit durch negative Einwirkungen von außen. Davon ausgehend wird die Gleichartigkeit der Interessen der Betroffenen gegenüber anderen Regionen oder dem übergeordneten System behauptet.“





</doc>
<doc id="12089" url="https://de.wikipedia.org/wiki?curid=12089" title="Sender">
Sender

Sender steht für:

Personennamen:
Siehe auch:


</doc>
<doc id="12091" url="https://de.wikipedia.org/wiki?curid=12091" title="Empfänger">
Empfänger

Empfänger, teils auch "Adressat", steht für:
Siehe auch:



</doc>
<doc id="12092" url="https://de.wikipedia.org/wiki?curid=12092" title="Kanal (Informationstheorie)">
Kanal (Informationstheorie)

Ein Kanal (auch Informationskanal, Übertragungskanal, Übertragungsweg oder Shannon’sches Kanalmodell) ist in der Informationstheorie ein Konzept, um den Informationsverlust durch Störungen bei der Übertragung zu modellieren. Der Kanal beschränkt sich dabei nicht nur auf das Medium, über das die Übertragung erfolgt, sondern beschreibt den gesamten Übertragungsweg vom Sender zum Empfänger. Auch die Senderausgangsstufe und die Empfängereingangsstufe sowie gegebenenfalls zwischengeschaltete Geräte können Störungen verursachen. Das Kanalkonzept ist zentraler Bestandteil des Sender-Empfänger-Modells.

In der Praxis ist ein Kanal typischerweise ein Kupfer- oder Glasfaserkabel oder beim Mobilfunk das relativ komplexe Zusammenspiel von Sende- und Empfangseinheit sowie allen weiteren Störungen während der Funkübertragung, z. B. Mehrwegeausbreitung, Dispersion oder Dopplereffekt.

Auch ein Speichermedium (z. B. eine Festplatte oder DVD) ist in diesem Sinne ein Kanal, da Daten an es gesendet werden (hier: geschrieben) und später wieder davon empfangen (hier: gelesen) werden können.

Allgemein werden Daten über einen Kanal übertragen, damit sie eine räumliche oder zeitliche Distanz überbrücken. Eine räumliche Übertragung wäre z. B. eine Netzwerkverbindung (das Herunterladen einer Website), eine zeitliche Übertragung z. B. eine Videoaufzeichnung auf einer DVD.

Die Theorie der Kanalkodierung beschäftigt sich damit, wie Informationen, trotz Störungen durch den Kanal, korrekt übertragen werden können.

Das Verhältnis von Störsignal zu Nutzsignal bezeichnet man als Signal-Rausch-Verhältnis. Ein Kanal hat eine bestimmte Kapazität, die die maximale Datenrate bestimmt.

Man unterscheidet zwischen verschiedenen Typen von Kanälen. Ein Kanal heißt


Störungsfreie Kanäle existieren praktisch nicht. Durch geeignete Kanalkodierungsverfahren kann die Störung jedoch minimiert werden, bei entsprechendem Aufwand bis hin zur praktischen Störungsfreiheit.





</doc>
<doc id="12094" url="https://de.wikipedia.org/wiki?curid=12094" title="Sif">
Sif

Sif, altnordisch für „Verwandte, Gesippin“, ist in der nordischen Mythologie die Gattin des Donnergottes Thor. Mit ihm zeugt sie eine Tochter namens Thrud („Kraft“). Außerdem hat sie noch einen Sohn, den schnellen Bogenschützen Ullr, den sie mit in die Ehe brachte. Als Loki ihr hinterlistig das schöne, goldglänzende Haar abschor, zwang ihn Thor, ihr einen neuen Haarschmuck aus Gold von kunstreichen Zwergen machen zu lassen, der wie Haar wachsen konnte.

In der älteren Forschung wurden die goldenen Haare als Symbol des reifen Ährenfeldes gedeutet, dessen goldener Schmuck in der Glut des Spätsommers abgeschnitten, dann aber von unsichtbar wirkenden Erdkräften neu gewoben wird. Doch scheint Sif in der nordischen Mythologie keine andere Funktion gehabt zu haben, als die Gattin Thors zu sein, wie auch ihr Name nahelegt.



</doc>
<doc id="12095" url="https://de.wikipedia.org/wiki?curid=12095" title="Uller">
Uller

Uller (altnord. "Ullr:" „der Ehrenhafte“; altengl. "wuldor:" „Glanz, Ruhm“), auch Ullar, "Ull", "Holler", "Oller" oder "Vulder" genannt, ist in der nordischen Mythologie der Gott des Winters, der Jagd, des Zweikampfes, der Weide und des Ackers. Er gehört den Asen, dem jüngeren Göttergeschlecht, an und wohnt der Sage nach in seiner selbst gebauten Halle Ydalir (Eibental) in Asgard.

Die Forschung vermutet, dass Uller ein sehr alter Gott gewesen ist, was seine Beziehungen zur Magie unterstreicht. In manchen Gebieten wurde Uller als Hauptgott verehrt (Philippson).
In späterer Zeit taucht Uller in der "Edda" als der Sohn der Sif und Stiefsohn des Thor auf. 

Eine Darstellung des Ullers, teils auf Skiern, war als Talisman bei Wintersportlern weit verbreitet. Im übertragenen Sinne wurden kreisförmige Anhänger meist mit Wintersportmotiven im Erzgebirge auch als "Uller" bezeichnet.



</doc>
<doc id="12099" url="https://de.wikipedia.org/wiki?curid=12099" title="Hauptplatine">
Hauptplatine

Die Hauptplatine (, auch ) ist die zentrale Platine eines Computers. Auf ihr sind die einzelnen Bauteile wie Prozessorsockel, RAM-Steckplätze, der BIOS-Chip mit der integrierten Firmware, Schnittstellen-Bausteine und Steckplätze für Erweiterungskarten montiert; die dafür notwendigen Leiterbahnen sind auf mehrere Lagen (Layer) aufgeteilt.

Die Hauptplatine beinhaltet den Sockel für die CPU, Steckplätze für Speicherbausteine und Erweiterungskarten wie Grafik-, Sound- und Netzwerkkarten sowie Bausteine, die die Komponenten miteinander verbinden. Die ehemalige Aufteilung in eine Northbridge für die hochperformante Anbindung von Arbeitsspeicher und Grafikkarte und eine Southbridge für Festplatte, PCI-Steckplätze und Peripheriegeräte verschwindet derzeit (2010). Funktionen wie etwa der Speichercontroller sind mittlerweile in der CPU selbst integriert, weswegen ein Zwei-Chip-Design überflüssig wurde. Manche sprechen dennoch weiterhin von einer Southbridge und einem Chipsatz, obwohl es faktisch nur noch ein zentraler Baustein auf der Hauptplatine ist.

Viele ehemalige Einzelkomponenten sind heute bereits fest auf dem Mainboard integriert („onboard“), besonders Sound- und Netzwerkkarten sind praktisch ausnahmslos Standard und genügen den Anforderungen vieler Benutzer. Anders als bei Notebooks sind Onboard-Grafikkarten im Desktopbereich eher selten zu finden, obwohl sie für Bürorechner durchaus ausreichende Leistung bieten. Schnellere, spieletaugliche Grafikkarten sind weiterhin nur als Steckkarte zu haben – seitens der Prozessorhersteller gibt es allerdings die Bemühung, CPU und GPU wieder zu vereinen.
Auch bei den Schnittstellen fand im Laufe der Zeit eine Integration auf die Hauptplatine statt. Während früher selbst Standard-Anschlüsse wie die serielle („RS-232“) und parallele („LPT“) Schnittstelle nur über Steckkarten realisiert wurden, sind heute alle üblichen Ports schon auf der Hauptplatine vorhanden. Mehrere USB-Buchsen sind fester Bestandteil eines jeden I/O-Shields, ebenso PS/2-Schnittstellen für Maus und Tastatur (teils nur noch als Comboanschluss), die bereits erwähnten Audio- und Netzwerkanschlüsse, und, je nach Ausrichtung des Boards, Video-, eSATA, FireWire und andere Ports. Mitunter müssen selten genutzte Anschlüsse aus Platzgründen auf Slotblenden ausgelagert werden, sodass etwa noch ein Gameport zur Verfügung gestellt werden kann. Die ehemalige Vielfalt von verschiedenen Anschlüssen für externe Komponenten ist heute völlig verschwunden und durch USB ersetzt worden.

Für interne Komponenten werden auf dem Mainboard außerdem diverse Serial-ATA-Ports („SATA“) bereitgestellt, was inzwischen ein Standard für Festplatten und optische Laufwerke ist. ATA/ATAPI („PATA“)-Schnittstellen sind, wenn überhaupt, nur noch einmal vorhanden, während früher zwei (Primary und Secondary) zum Anschluss von bis zu vier Laufwerken üblich waren. Anschlüsse für Floppy-Laufwerke sind ebenfalls nur noch auf der Hälfte der aktuellen Hauptplatinen überhaupt noch vorhanden. Auch hier übernimmt ein Schnittstellentyp den Anschluss sämtlicher Komponenten, wobei auch die Substitution der Diskette durch USB-Sticks eine Rolle spielt.

Bei den Steckplätzen (engl. "slots") für Erweiterungskarten verlief die Entwicklung vom XT-Bus über den ISA-Bus, den EISA-Bus, den PCI-Bus zum PCI-Express-Bus (PCIe), der gegenwärtig (2010) im nichtprofessionellen ("consumer") Bereich aktuell ist. Im Serverbereich ist PCI-X noch aktuell, wobei auch hier der Übergang zu PCIe vollzogen wird. Ferner gab es Parallelentwicklungen wie den MCA-Bus und spezielle Steckplätze für Grafikkarten wie den VLB- und den AGP-Slot. Auch diese sind inzwischen vollständig durch PCIe abgelöst.

Steckplätze werden heute fast nur noch belegt, wenn in einem Aufgabenbereich besonders hohe Leistung gefragt ist, welche von Onboard-Komponenten nicht geliefert werden kann, oder mit Steckkarten spezielle Funktionen nachgerüstet werden, die von den Onboard-Komponenten nicht abgedeckt werden. All diese Karten besitzen eigene Prozessoren, die den Hauptprozessor des Systems entlasten und auf den jeweiligen Einsatzzweck optimiert sind. Onboard-Komponenten lassen Berechnungen dagegen von der CPU durchführen, weswegen ihre Leistung unter großer Belastung des Rechners einbrechen kann. Neben Grafikkarten sind insbesondere Festplattencontroller gängige Komponenten, die in professionellen Systemen RAID-Funktionalität für bis zu 28 SAS- oder SATA-Festplatten bieten können. Sie sind zudem bei Defekt ohne Datenverlust austauschbar, was bei Onboard-RAID nicht möglich ist, und besitzen eigene XOR-Einheiten für die Paritätsberechnung. Daneben gibt es noch Ein- oder Mehrport-Netzwerkkarten für Kupfer- und Glasfasernetzwerke sowie Soundkarten mit umfangreichen digitalen und analogen Ein- und Ausgängen. Für Controllerkarten für antiquierte Anschlüsse wie etwa RS-232 oder Parallelport genügt die PCI-Schnittstelle vollkommen, während aktuelle USB-3.0-Erweiterungskarten auf die hohe Datenrate von PCI-Express x1 oder höher angewiesen sind.
Das Format der Hauptplatinen wird nach dem Formfaktor unterschieden. Seit 1995 ist das ATX-Format aktuell. Es löste das AT-Format ab und brachte diverse Umstellungen an Gehäusen und Netzteilen mit sich. Es existieren diverse Variationen von ATX und AT, um auch kompaktere Geräte ohne proprietäre Formate bestücken zu können, etwa Baby AT oder µATX. Mini-ITX, Flex-ATX und Micro-ATX passen in ATX-Gehäuse. Steckplätze, Schrauben und das Fenster für I/O-Shield befinden sich an einheitlichen oder derselben Position.

Das ATX-Format sollte ab 2003 durch das damit inkompatible BTX-Format abgelöst werden, BTX konnte sich aber nicht durchsetzen und verschwand 2007 vom Markt. Bei BTX sind zu ATX die Steckkarten an der anderen Seite der Hauptplatine angeordnet. Dadurch befindet sich die Hauptplatine an der anderen Seitenwand des Gehäuses. Bei senkrecht eingebauten Hauptplatinen wirkte sich dies auf die Lage der Kühlkörper von Steckkarten aus.

AMD hatte Anfang September 2007 das DTX-Format angekündigt, das für sparsame Benutzer gedacht ist, da es kleiner und außerdem zum größten Teil ATX-kompatibel ist.



</doc>
<doc id="12102" url="https://de.wikipedia.org/wiki?curid=12102" title="Mönch (Begriffsklärung)">
Mönch (Begriffsklärung)

Mönch bezeichnet:

in der Geographie:
Weiteres:

Mönch ist eine Kurzform folgender Vogelnamen:
Siehe auch:


</doc>
<doc id="12104" url="https://de.wikipedia.org/wiki?curid=12104" title="Videokunst">
Videokunst

Die Videokunst ist eine Form der Medienkunst, die sich der Projektion als Medium der künstlerischen Aussage bedient. Die Videokunst entstand in den frühen 1960er-Jahren in Deutschland und Amerika.

Der Begriff bezieht sich darauf, dass die Künstler mit Videotechnik arbeiten, also Videos im Rahmen einer Videoinstallation oder in Form einer Videoskulptur präsentieren. Dabei wird entweder die Technik selbst thematisiert und die Möglichkeiten des Mediums ausgelotet, oder der Bildschirm wird als neue "Leinwand" betrachtet, die neue Möglichkeiten und Formen einer Malerei mit bewegten Bildern eröffnet. Verwandtschaften bestehen aber auch zum Experimentalfilm. Videokunst kann in Form einer raumgebundenen Videoinstallation auftreten, das Video kann Teil einer Rauminstallation sein, oder nicht raumgebunden, auf Unterhaltungsgeräten konsumierbar wie andere Medien.

1963 veränderte Nam June Paik in der Wuppertaler Galerie Parnass echte Fernsehbilder mit Hilfe starker Magneten so sehr, dass die Fernsehbilder zu gegenstandslosen Formen mutierten. Im selben Jahr entsteht Wolf Vostells "Sun in your head" und die Smolin Gallery in New York zeigt Wolf Vostells Installation "6 TV-Dé-coll/agen" das heute zur Kunstsammlung des Museo Reina Sofía in Madrid gehört, und weitere TV-Dé-coll/agen von Wolf Vostell, bei denen der Empfang gestört, die Geräte zerstört oder mit Stacheldraht umwickelt und vergraben wurden. Ursprünglich wurde "Sun in your head" auf 16-mm-Film gedreht und 1967 auf Videofilm überspielt. 

Die eigentliche Videokunst begann etwas später, nachdem es tragbare Videoausrüstungen gab. 1969 fand in der New Yorker Howard Wise Gallery die erste zusammenfassende Ausstellung unter dem Titel „“ statt. In der Frühphase der Videokunst wurde meist ein mit der Videokamera aufgenommenes Bild dem Zuschauer direkt auf einem angeschlossenen Monitor präsentiert. Später fertigten die Künstler längere Videoproduktionen unter künstlerischen Aspekten an, um sie in Form von Installationen vorzuführen, bei denen die bewegten Bilder auf einer Vielzahl von Monitoren gezeigt wurden. 1977 war Nam June Paik mit einer Videoinstallation auf der documenta 6 ebenso wie Wolf Vostell vertreten. Deutsche Videokünstler sind etwa Marcel Odenbach, Mike Steiner, Klaus vom Bruch, Ulrike Rosenbach, oder Julian Rosefeldt. Weitere Videokünstler sind die Amerikaner Bill Viola, Gary Hillmund und die Niederländerin Nan Hoover.

Performance- und Land-Art-Künstler bedienen sich auch oft des Mediums Video, um ihre (eigentlichen) Arbeiten zu dokumentieren. Diese gehören strenggenommen nicht zur Videokunst; allerdings haben sich die Entwicklungen der verschiedenen Richtungen stark untereinander beeinflusst. Gerry Schum entwickelte ab 1968 verschiedene Formate, die unter den Namen "Fernsehgalerie" oder "Videogalerie" Filme von Künstlern versammelten und diese außerhalb der herkömmlichen Ausstellungssituationen präsentieren sollten, die für die neuen Kunstrichtungen unzureichend schienen. Schum zeigte vor allem Künstler der Land Art wie Robert Smithson oder Richard Long, aber auch Joseph Beuys und erste Filme von Gilbert and George.

Während Künstler wie Vito Acconci, Chris Burden oder Joan Jonas Video vorerst zur Dokumentation benutzten, wurden Themen und Techniken der Performancekunst – wie die Akzentuierung einzelner Handlungsabläufe oder des menschlichen Körpers selbst – durch Bruce Nauman, Gary Hill oder Nan Hoover explizit zur Grundlage von Videoarbeiten.

Die Übergänge zwischen Videokunst und experimentellem oder traditionellem Film sind fließend. Viele Videokünstler beschäftigen sich mit dem Medium und seinen Strukturen selbst; die erzählerischen Möglichkeiten oder Konventionen des Kinos werden dabei zum eigentlichen Thema. Exemplarisch dafür sind Arbeiten von Douglas Gordon – wie "24 Hour Psycho" (1993), in der Alfred Hitchcocks Filmklassiker Psycho auf eine Laufzeit von 24 Stunden verlangsamt wird – oder von Künstlern wie Rodney Graham oder Isaac Julien, die typische Erzählmuster und Mythen des Hollywood-Kinos analysieren und mitunter persiflieren.

Im Frankreich der 60er Jahre kreiert Jean-Christophe Averty neben zahlreichen Musikvideos (u. a. für Gilbert Bécaud, Serge Gainsbourg und France Gall) auch Fernsehfilme, die als Experimente für Aufsehen sorgen (u. a. 1969 sein Sommernachtstraum). Für sein Videoschaffen erhält Videopionier Averty 1965 einen Emmy Award.

Auch die dokumentarischen Möglichkeiten des Films werden in der Videokunst weiterverfolgt, beispielsweise durch Steve McQueen, Tacita Dean oder Zarina Bhimji. Im Gegensatz zum traditionellen Dokumentarfilm wird in der Videokunst die ästhetische Wirkung oft hervorgehoben und durch Wiederholungen oder andere Manipulationen des Materials verstärkt.

Zwischen Videokunst und Musikvideo kann es ebenfalls zu Überschneidungen kommen; manche Künstler wie Pipilotti Rist inszenieren Videoarbeiten zu Musikstücken oder beziehen sich wie Candice Breitz auf die aus Musikvideos bekannten Bilder und Rollenmodelle. Der Regisseur Chris Cunningham hingegen, der ursprünglich Videos für Musiker wie Aphex Twin oder Squarepusher produziert hatte, stellt seine Arbeiten wie "Flex" (2000) mittlerweile im Kunstkontext aus.

Oft werden die Zeitläufe oder auch das Bildformat in der Videokunst so weit verändert, dass das Kunstwerk in die Nähe des Tafelbilds gerückt wird. Bill Viola ordnete beispielsweise seine Projektionen "Nantes Triptych" (1992) und "The City of Man" (1997) als Triptychon an und stellt sie auch inhaltlich in den Kontext des religiösen Bilds. In "The Greeting" (1995) stellt er ein Gemälde des florentinischen Malers Jacopo Pontormo als in Zeitlupe verlangsamte Filmszene nach. Der belgische Künstler David Claerbout benutzt ähnliche Mittel, um das photographische Bild ins bewegte Bild zu übertragen.

In der zeitgenössischen Kunst, die ihre Wurzeln in der Videokunst hat aber korrekterweise der Computerkunst zuzuordnen ist, entstehen sowohl konzeptuelle Arbeiten, als auch Arbeiten mit Referenzen an die Popkultur (Musikvideo, Spielfilm), Video-Performances und Experimente mit visueller Wahrnehmung. Wenige Kultursender wie 3sat oder ARTE senden vereinzelt Videokunst-Nächte, ansonsten findet diese Kunstform – obwohl für das Medium Fernsehen prädestiniert – weiterhin eher im White Cube statt.

Weiterhin gibt es seit 2001 den ersten Videokunst TV-Sender, „Souvenirs from earth“ (SFE) gegründet von dem französischen TV-Journalisten Laurent Krivine und Videokünstler Marcus Kreiss.

Im Jahre 2004 fand erstmals durch Willi Bucher und Ralf Kopp der Glasbaustein Einzug in die Videokunst (beboxx). Die Kombination aus Glasbaustein und moderner Bildprojektionstechnik generiert hier eine völlig neuartige visuelle Wahrnehmung der gezeigten Inhalte.






</doc>
<doc id="12105" url="https://de.wikipedia.org/wiki?curid=12105" title="Happening">
Happening

Das Happening (von ‚geschehen‘) ist neben Fluxus eine der wichtigsten Formen der Aktionskunst der 1960er Jahre. Ein Happening ist ein improvisiertes Ereignis direkt mit dem Publikum. Eine der frühen Formen des Happenings ist das Dé-coll/age-Happening. Dazu gehören das Werfen von Gegenständen ins Publikum, Exhibitionismus, Blut- und Farborgien, Zerstören, Zerreißen, Verdrecken von Gegenständen. Ziel ist die durch unterschiedlichste Handlungen verursachte Schockwirkung auf ein Publikum, das in das Ereignis einbezogen wird. Dieses ist dabei Teil der vom Künstler erdachten Aktion. Es wird mit in die künstlerischen Handlungen einbezogen, wobei der Geschehensablauf nicht von vornherein festgelegt ist. Je nach Reaktion der Zuschauer kann unterschiedlich improvisiert werden (wobei Happenings selten vollständig improvisiert sind, sondern durchaus zuvor durchgeprobt werden). Hieraus ergibt sich auch, dass Happenings für gewöhnlich keinen festen zeitlichen Rahmen haben, oftmals weiß das Publikum nicht einmal, wann das Happening beendet ist.
Ein weiteres Merkmal des Happenings ist die Verwendung verschiedener Gegenstände und ihr zufälliges oder gewolltes Nebeneinanderstellen, was auch ein Grundprinzip des Surrealismus ist.

Der Begriff wurde erstmals 1959 von Allan Kaprow für eine Aktion in der New Yorker Reuben Gallery benutzt, die aus 18 Happenings in sechs Teilen bestand. Es gab dabei drei Räume, welche durch Plastikfolie voneinander abgetrennt waren und in denen zur gleichen Zeit die Geschehnisse abliefen. Allerdings hatte Kaprow den Begriff bereits in seinem 1958 veröffentlichten Aufsatz „The Legacy of Jackson Pollock“ verwendet, wo er eine Prognose für die Kunst der Post-Pollock-Generation entwirft.

Ziel der Happening-Künstler war es, den traditionellen Kunstbegriff zu erweitern und die Kunst mit dem alltäglichen Leben zu verbinden. Dabei sollten den Menschen alltägliche Handlungen verdeutlicht und dadurch abstrahiert werden. Die Grenze zwischen Fluxus und Happening ist nicht genau zu definieren, da sich einige Künstler an beiden Aktionsformen orientieren und beteiligen. 

In den 1960er Jahren entwickelte sich in der Bundesrepublik eine übergreifende und umwälzende kulturelle und geistige Bewegung, die, selbstbewusst und engagiert durch Persönlichkeiten wie Joseph Beuys, Wolf Vostell und den Koreaner Nam June Paik, vorwiegend in Köln zu erleben war und die die Kunst revolutionierte. Die Happenings, die durch ihre provokante Motivation kritische Zeitbekenntnisse schufen, fanden schnell eine Schar von Anhängern. Die Extravaganz, die politischen und soziologischen Bezüge sowie visionäre Prophezeiungen waren Ingredienzien der Happenings von Wolf Vostell. Diese Ereignisse, die alle Sinne in das Happening und in die Fluxus Konzerte einbezogen, das Engagement von Beuys und Vostell, die Koryphäen dieser Kunst-Evolution, waren für viele Kunstliebhaber zu dieser Zeit noch inakzeptabel. 

Die Happening- und Fluxus-Aktivisten waren von der Fähigkeit der Menschen überzeugt, mit der Teilnahme an den Happenings und Fluxus-Aktionen, selbst aus ihren eigenen Ressourcen zu schöpfen und sich durch Kreativität zu „vervollkommnen“. Die Teilnahme des Publikums ist ein Bestandteil der Happenings. Frei und autonom stilisierten sich Beuys, Vostell und Paik im Rheinland auch durch die von ihnen entwickelte Videokunst zu bekannten Happening-Künstlern. Betrachtet man die internationalen Kunstmärkte seit den 1970er Jahren bis heute, so ist der Einfluss der Happening- und Fluxus-Bewegung unübersehbar mannigfaltig. 

Allan Kaprow, John Cage, George Brecht, Joseph Beuys, Bazon Brock, Wolf Vostell, Yoko Ono, Nam June Paik, Claes Oldenburg, Robin Page, Robert Whitman, Robert Rauschenberg, Red Grooms, Jim Dine, Al Hansen, Robert Jasper Grootveld, Zentrum für politische Schönheit.







</doc>
<doc id="12106" url="https://de.wikipedia.org/wiki?curid=12106" title="Adenosintriphosphat">
Adenosintriphosphat

Adenosintriphosphat, kurz ATP, ist ein Nukleotid, nämlich das Triphosphat des Nucleosids Adenosin.

Adenosintriphosphat ist der universelle und unmittelbar verfügbare Energieträger in Zellen und wichtiger Regulator energieliefernder Prozesse. Das Molekül des Adenosintriphosphats besteht aus einem Adeninrest, dem Zucker Ribose und drei Phosphaten (α bis γ) in Ester- (α) bzw. Anhydridbindung (β und γ).

Adenosintriphosphat wurde 1929 von dem deutschen Biochemiker Karl Lohmann entdeckt. Eine chemische Synthese von ATP wurde erstmals 1949 von James Baddiley und Alexander Robertus Todd veröffentlicht. Die Rolle als Hauptenergiequelle in Zellen wurde 1939 bis 1941 von Fritz Lipmann aufgeklärt, nachdem schon Wladimir Alexandrowitsch Engelhardt 1935 gezeigt hatte, dass ATP für Muskelkontraktionen notwendig ist, und Herman Kalckar 1937 die Verbindung von ATP-Synthase mit der Atmungskette festgestellt hatte. Die ersten an der Synthese von ATP beteiligten Enzyme wurden von Efraim Racker ab 1961 bestimmt.

Für Prozesse in Zellen wird Energie benötigt, um chemische, osmotische oder mechanische Arbeit zu leisten. Die Energie wird über das Molekül ATP bereitgestellt. Die Phosphate sind über Phosphoranhydrid-Bindungen (Säureanhydrid-Bindungen) miteinander verbunden. Werden diese Bindungen durch Enzyme hydrolytisch gespalten, entsteht das Adenosindiphosphat (ADP) und Monophosphat bzw. das Adenosinmonophosphat (AMP) und Pyrophosphat. Die Spaltung der Bindung verbraucht Energie; insgesamt werden jedoch durch die anschließende Hydrolyse des abgespalteten Phosphats unter Standardbedingungen jeweils 32,3 kJ/mol (Spaltung einer Bindung) oder 64,6 kJ/mol (Spaltung beider Bindungen) Energie für Arbeitsleistungen in den Zellen frei.

Als Energiequelle wird ATP für die grundlegenden energieverbrauchenden Prozesse aller Lebewesen genutzt: chemische Arbeit, wie Synthese organischer Moleküle, osmotische Arbeit, wie aktiver Stofftransport durch Biomembranen, sowie mechanische Arbeit, wie zum Beispiel Bewegungen bei der Muskelkontraktion.

ATP ist ein Cosubstrat der Kinasen, einer Gruppe von Phosphat-übertragenden Enzymen, die im Metabolismus und bei der Stoffwechselregulation eine Schlüsselrolle spielen. Bedeutende Mitglieder der letzteren Gruppe sind die Proteinkinasen, die je nach ihrem Aktivierungsmechanismus als Proteinkinase A (PKA, cAMP-abhängig), Proteinkinase C (PKC, Calcium-abhängig), Calmodulin-abhängige Kinase, oder Insulin-stimulierte Proteinkinase (ISPK) bezeichnet werden, um nur einige Beispiele zu nennen. Unter Blutzucker werden einige Grundprinzipien angesprochen, nach denen eine Serie von Kinasen zu einer Enzymkaskade zusammengeschaltet sein kann.

ATP (wie auch ADP und Adenosin) ist Agonist purinerger Rezeptoren, die sowohl im zentralen als auch im peripheren Nervensystem eine Rolle spielen. Somit ist es beteiligt an Prozessen wie der Durchblutungsregulation oder der Vermittlung von Entzündungsreaktionen. Es wird nach neuronalen Verletzungen ausgeschüttet und kann die Proliferation von Astrozyten und Neuronen stimulieren.

Aus dem bei der Energieabgabe aus ATP entstandenen AMP bzw. ADP regeneriert die Zelle das ATP. Dafür gibt es zwei verschiedene Prinzipien, die als Substratkettenphosphorylierung und Elektronentransportphosphorylierung (Atmungskette) bezeichnet werden.

Bei der "Substratkettenphosphorylierung" wird ein Phosphatrest an ein Zwischenprodukt des Abbaus von stofflichen Energiequellen gebunden und nach weiterem Umbau des Zwischenprodukts auf ADP übertragen. 

Bei der "Elektronentransportphosphorylierung" werden durch einen Transport von Elektronen entlang eines Redoxgradienten über verschiedene Elektronen- und Wasserstoff-Überträger in einer Membran Protonen von einem durch die Membran umschlossenen Raum der Zelle in einen anderen transportiert. In Bakterien werden so Protonen nach außen gepumpt. In Eukaryoten finden diese Prozesse in den Mitochondrien statt. Dort werden aus der Matrix des Mitochondriums Protonen in den Intermembranraum exportiert. In beiden Fällen wird ein Protonengradient erzeugt und als chemiosmotisches Potenzial ΔP genutzt, das sich aus einem Protonenkonzentrationsunterschied ΔpH und einer elektrischen Potentialdifferenz ΔΨ zusammensetzt. Der Rückfluss der Protonen durch das ebenfalls in der Membran lokalisierte Enzym ATP-Synthase treibt die von diesem Enzym katalysierte energieverbrauchende Bindung anorganischer Phosphatreste an das ADP an. In manchen Organismen werden anstatt Protonen Natriumionen verwendet, sie verfügen analog über eine Na-abhängige ATP-Synthase.

Bei chemotrophen Organismen werden die Elektronen in Form der Reduktionsmittel NADH, NADPH, FADH oder reduziertes Ferredoxin in die Atmungskette eingespeist. Diese stammen aus dem oxidativen Abbau energiereicher Verbindungen, wie beispielsweise Kohlenhydraten oder Fettsäuren. Die Elektronen werden bei aeroben Organismen auf Sauerstoff übertragen, dabei entsteht Wasser. In der anaeroben Atmung werden andere Elektronenakzeptoren verwendet, beispielsweise Schwefel oder Eisen(II). In beiden Fällen entsteht eine elektrochemische Differenz, die zur ATP-Bildung genutzt wird. Bei Eukaryoten findet der Vorgang in den Mitochondrien, bei Prokaryoten im Cytoplasma statt.

Bei phototrophen Organismen werden nach Absorption von Licht durch Chlorophylle von diesen Elektronen auf einem hohen Energieniveau abgegeben. Die Lichtenergie wird damit genutzt, um eine elektrochemische Differenz zu erzeugen. Bei grünen Pflanzen findet dies in den Chloroplasten, bei Bakterien im Cytoplasma statt. Wegen der Nutzung des Lichts spricht man in diesem Fall von Photophosphorylierung.
Da die oxidative Phosphorylierung in der Atmungskette ein relativ langsamer Prozess ist, muss der ATP-Vorrat in stark beanspruchten Zellen (Muskelzellen) auch kurzfristig wieder aufgefüllt werden. Der ATP-Vorrat (in der Muskelzelle ca. 6 mmol/kg Muskel) reicht bei maximaler Kontraktion nur etwa 2–3 Sekunden. Eine Reserve stellen hier Moleküle mit höherem Gruppenübertragungspotenzial als ATP dar. Säugetiermuskelzellen halten einen Vorrat an Kreatinphosphat (21 mmol/kg Muskel; 0,08 % pro Körpergewicht) bereit. Die Creatin-Kinase katalysiert die Übertragung der Phosphorylgruppe vom Kreatinphosphat an das ADP. Ist dieser Vorrat nach 6–10 Sekunden verbraucht, müssen die oben genannten Mechanismen die ATP-Regeneration allein tragen.

Während starker Muskelbeanspruchung bauen Muskelzellen Glucose zu Lactat in der Milchsäuregärung ab, um schnell ATP zu erzeugen. Lactat selbst wird in der Leber wieder zu Pyruvat und dann zu Glucose unter ATP-Verbrauch aufgebaut (Gluconeogenese). Diese Glucose wird dann wieder dem Muskel als Energiequelle zur Verfügung gestellt. Dieser Kreislauf wird auch als Cori-Zyklus bezeichnet.

Im Notfall werden zur Energieerzeugung auch körpereigene Proteine abgebaut. Proteine werden in Aminosäuren zerlegt, und diese meistens zu Pyruvat abgebaut. In einem dem Cori-Zyklus ähnlichen Weg wird Pyruvat zunächst zu Alanin transaminiert und zur Leber transportiert. Dort kehren sich diese Schritte um und die Leber erzeugt aus Pyruvat wieder Glucose, die dem Muskel bereitgestellt wird. Dieser Zyklus wird auch als Glucose-Alanin-Zyklus bezeichnet.

Der Herzmuskel nutzt Fettsäuren als Brennstoff, diese werden in der β-Oxidation in den zahlreichen Mitochondrien abgebaut. Des Weiteren können auch Glucose, Lactat (über Reoxidation zu Pyruvat), Ketonkörper und Glykogen abgebaut werden. Bei hoher Belastung können bis zu 60 % der Energie aus der Oxidation von Lactat gewonnen werden.

In der Zelle ist die ATP-Konzentration eine Regelgröße: Das Absinken unter 4–5 mmol/l aktiviert energieliefernde Reaktionen (siehe Phosphofructokinase); das Übersteigen des Schwellenwertes bewirkt Energiespeicherung, z. B. durch Bildung von Kreatinphosphat als schnell verfügbaren (ATP-liefernden) Speicher im Muskel oder Aufbau von Glykogen als „Energiepolster“ in der Leber. Kohlenhydrat- und Proteinspeicher sind allerdings limitiert. Weiterer Energieüberschuss führt (über Acetyl-CoA) zur Speicherung von Fett.

Bei einem durchschnittlichen Erwachsenen entspricht die Menge ATP, die täglich in seinem Körper auf- und abgebaut wird, etwa seiner halben Körpermasse. So verbraucht ein 80 kg schwerer Mann etwa 40 kg ATP am Tag (entspricht etwa 78,8 mol oder 10 Molekülen), die durch neu gebildete weitere 40 kg ersetzt werden. Der ATP-Durchsatz kann bei intensiver Arbeit auch auf 0,5 kg pro Minute ansteigen.




</doc>
<doc id="12107" url="https://de.wikipedia.org/wiki?curid=12107" title="Pater">
Pater

Pater (lat. „Vater“, Pl. "Patres", Abk. "P.", Pl. "PP." oder "P.P.") ist eine Anrede für einen Ordenspriester der römisch-katholischen Kirche.

Im deutschsprachigen Raum wird die Anrede im Allgemeinen nur bei Priestern verwendet. Ordensbrüder, die das Weihesakrament nicht empfangen haben, werden in der Regel als Bruder angesprochen (auch hier wird manchmal die lateinische Bezeichnung "Frater" verwendet). In Österreich ist es zum Teil üblich, auch Ordensmänner mit ewiger Profess, die nicht zum Klerus gehören, als "Pater" anzusprechen. Andererseits gibt es Ordensgemeinschaften, die auch für die Priester des Ordens die Anrede "Bruder" oder "Frater" bevorzugen. Angehörige eines Chorherren-Ordens (z. B. Prämonstratenser, Augustiner-Chorherren) werden als „Herr“ und nicht als „Pater“ angesprochen.

In den romanischen Sprachen wird die Anrede "Pater" ganz allgemein für alle Priester und Diakone, also auch für den Diözesanklerus, verwendet, meist in landessprachlicher Abwandlung wie z. B. "Padre" im Italienischen und Spanischen, "Père" oder "Abbé" im Französischen (was beides "Vater" bzw. "Pater" bedeutet, wobei "Abbé" nur für den niederen Weltklerus Verwendung findet).



</doc>
<doc id="12110" url="https://de.wikipedia.org/wiki?curid=12110" title="Huldrych Zwingli">
Huldrych Zwingli

Huldrych Zwingli (auch Huldreych, Huldreich und Ulrich Zwingli; * 1. Januar 1484 in Wildhaus; † 11. Oktober 1531 in Kappel am Albis) war ein Schweizer Theologe und der erste Zürcher Reformator. Aus der Zürcher und der Genfer Reformation ging die reformierte Kirche hervor.

Seine Theologie wurde in der zweiten Generation von Heinrich Bullinger und Johannes Calvin weitergetragen.

Im Gegensatz zu manch volkstümlichen Annahmen lautet Zwinglis Taufname im Gedenken an den Heiligen Ulrich von Augsburg «Ulrich». Erst mit der Zeit begann Zwingli selbst, seinen Vornamen zu Huldrych (auch Huldreich oder Huldrich) zu verändern; dies wohl als humanistisch-volksetymologische Spielerei und entgegen der sprachwissenschaftlichen Etymologie, wonach "Ulrich" von althochdeutsch "uodal" «Erbbesitz» und "rīch" «mächtig» abgeleitet ist.

Der Familienname «Zwingli» ist laut Heinrich Bruppacher ein Wohnstättenname zu dem nicht seltenen Örtlichkeitsnamen «Zwing, Twing», der auch im Toggenburg vorkommt und ursprünglich ein «eingefriedetes Stück Land» bezeichnete. Diese Erklärung wurde auch von Ulrich Gäbler wieder aufgenommen. Ulrich selbst dachte zuweilen an «Zwilling» oder an «Zwinge» und nannte sich daher in einigen Texten humanistisch-latinisiert «Geminius» beziehungsweise «Cogentius». Martin Luther und andere Widersacher dagegen sprachen bisweilen vom «Zwingel», da er die Heilige Schrift in seinem Sinne zwinge.

Ulrich Zwingli wurde als Sohn des Bauern und Ammanns Johann Ulrich Zwingli und der Maria Bruggmann, die in zweiter Ehe mit Zwingli verheiratet war, am 1. Januar 1484 in Wildhaus im Toggenburg als drittes Kind seiner Eltern geboren. Sein Geburtshaus ist heute als Museum eingerichtet.

Zwingli hatte mindestens neun Geschwister. Bereits im Alter von sechs Jahren verliess Zwingli sein Heimatdorf und lebte während der nächsten vier Jahre als Schüler bei seinem Onkel, dem Dekan Bartholomäus Zwingli, in Weesen. 1494 wechselte er an die Lateinschule in Basel und später an die Lateinschule in Bern. Wegen seiner grossen Musikalität hätten ihn dort die Dominikaner gern in ihr Kloster aufgenommen, doch sein Vater war dagegen. So verliess Zwingli 1498 Bern und begann als Fünfzehnjähriger sein Studium an der Universität Wien; dort immatrikulierte er sich als «Udalricus Zwinglij de Glaris». Von 1502 bis 1506 studierte er an der Universität Basel und schloss mit dem Titel "Magister artium" ab. Nach dem Magisterexamen studierte er noch sechs Monate Theologie und wechselte danach wie viele seiner Zeitgenossen ohne abgeschlossenes Theologiestudium in die kirchliche Praxis. Im September 1506 wurde Zwingli zum Priester geweiht.

Im Spätsommer 1506 wurde Zwingli als "Kirchherr" zum leitenden Pfarrer in Glarus gewählt. Am 21. September 1507 erfolgte mit einem feierlichen Essen die Einführung in sein Amt. Es gab wohl verschiedene Gründe, weshalb gerade der 22-jährige Magister berufen wurde. Zum einen dürfte Zwingli ihnen empfohlen worden sein. Zum anderen wollten die Glarner ihren Priester selber wählen und nicht den Vorschlag des Bischofs von Konstanz übernehmen. Eigentlich sollte nämlich der einflussreiche Zürcher Heinrich Göldi die einträgliche Pfründe, d. h. die Einkünfte der Pfarrei, vom Bischof erhalten. Göldi hatte auch schon eine beträchtliche Summe nach Konstanz überwiesen. Göldi wäre damit Inhaber der Pfründe und formell Pfarrer von Glarus geworden, doch er wollte nicht nach Glarus umziehen, da er die Stelle und ihre Einkünfte lediglich als eine Geldanlage betrachtete. Die Glarner waren aber nicht an einem Pfründenjäger interessiert, weshalb sie dringend einen eigenen Kandidaten brauchten, den sie in Zwingli fanden. Nach der Wahl Zwinglis wurde es für Göldi schwierig, das Pfarramt gegen den Willen der Glarner zu übernehmen. Um nicht leer auszugehen, verlangte Göldi eine hohe Abfindung. Zwingli musste dazu bei den Glarnern Geld aufnehmen, und die Abzahlung des Kredits machte ihm noch lange zu schaffen.

Bei der Kreditvergabe zeigten sich die Glarner durchaus grosszügig. Etwas weniger entgegenkommend scheinen sie beim Pfarrhaus gewesen zu sein; dessen Unzulänglichkeiten waren den Glarnern offenkundig bewusst. Als Zwingli 1516 um die Entlassung bat, versprachen sie ihm, wenn er bleiben würde, ein besseres Pfarrhaus zu bauen.

Die Glarner Pfarrei umfasste mehrere Dörfer, neben Glarus Riedern, Netstal, Ennenda und Mitlödi. Der Hauptort umfasste mit Riedern zusammen rund 1300 Einwohner. Für die geistliche Versorgung war Zwingli zusammen mit drei oder vier Kaplänen zuständig. Über die Tätigkeit Zwinglis in Glarus ist wenig bekannt. Die wenigen Zeugnisse lassen keine Kritik an der Kirche erkennen. Er las die Messe und erteilte die Absolution. 1512 schrieb er an Papst Julius II. und bat um Ablass für die Glarner. Zwingli war auch Feldprediger und nahm von 1512 bis 1515 an den Feldzügen der Italienischen Kriege, insbesondere an der Schlacht bei Marignano, der Glarner für den Papst gegen die Franzosen in der Lombardei teil.

Der Bauernsohn Zwingli scheint sehr volksverbunden gewesen zu sein. Im Laufe der Zeit lernte er wohl alle seine Kirchgenossen kennen. Zu einzelnen Familien hatte Zwingli mehr als nur offiziellen Zugang gefunden. So übernahm der Geistliche die Patenschaft für verschiedene Kinder. Zwinglis ungebrochene Kirchlichkeit zeigt sich auch im Bestreben, einen angeblichen Splitter des Kreuzes Christi nach Glarus zu holen, das ihm gelang. Um den Splitter würdig aufzubewahren, musste die alte Glarner Pfarrkirche erweitert werden. Auch dafür setzte sich Zwingli mit Erfolg ein. 1510 wurde die Kreuzkapelle angebaut, die ihren Namen von diesem Kreuzsplitter erhielt. Die Glarner sprachen aber noch lange von der Zwinglikapelle und nicht von der Kreuzkapelle.

In den Glarner Jahren bildete sich Zwingli intensiv fort. Mit grossem Eifer studierte er viele Werke der antiken Klassiker und die Kirchenväter. Ausserdem lernte er Griechisch und konnte so den Urtext des Neuen Testaments lesen, den Erasmus von Rotterdam 1516 in einer kritischen Edition veröffentlicht hatte. Durch den Humanisten Erasmus lernte Zwingli, einen anderen Sinn in den biblischen Texten zu suchen und zu erkennen. Dadurch fand er einen neuen, für ihn befreienden Zugang zur Heiligen Schrift. Trotz der Abgeschiedenheit des Bergtales Glarus stand Zwingli in regem Kontakt mit den Gelehrten seiner Zeit und war dadurch stets unterrichtet über das Erscheinen neuer Bücher. Zwingli besass am Ende seiner Glarner Zeit die damals bedeutende Zahl von über 100 Büchern.

Zwingli wollte sein Wissen weitergeben. Auf seine Veranlassung stimmte die Landsgemeinde 1510 der Gründung einer Lateinschule zu. Auf dieser höheren Schule konnten die Knaben Grundkenntnisse in Latein erwerben und mussten nicht eine auswärtige Schule besuchen. Zwingli wurde zum Lehrer gewählt. Zu Zwinglis Schülern gehörten eine Reihe bedeutender Glarner: Valentin Tschudi, Zwinglis Nachfolger in Glarus, Aegidius Tschudi, Chronist und Politiker, und vermutlich auch Fridolin Brunner, der spätere Reformator des Landes Glarus.

In der glarnerischen und eidgenössischen Politik Anfang des 16. Jahrhunderts wurde heftig gestritten, ob mit dem Papst, dem Kaiser oder mit den Franzosen zusammengearbeitet werden sollte. In Glarus ging es konkret vor allem darum, in wessen Dienste die jungen Glarner als Söldner treten sollten. Zwingli stellte sich stets auf die Seite des Papstes, worauf sich dieser mit einer stattlichen päpstlichen Pension von 50 Gulden erkenntlich zeigte. Zwingli, der als Feldgeistlicher der etwa 500 Schweizer Soldaten dabei war, mahnte in einer Predigt am 7. September 1515 in Monza zur Einigkeit. Im Oktober 1515, nach der für die Schweizer vernichtenden Niederlage gegen die Franzosen in der Schlacht bei Marignano, endete die eidgenössische Grossmachtpolitik. Danach offerierten die Franzosen einen schnellen Friedensschluss, allerdings nicht zu vorteilhaften Bedingungen. Zwingli votierte dagegen und unterstützte weiterhin den Gegenspieler der Franzosen, den Papst. In Glarus wie auch in der Eidgenossenschaft schlug die Stimmung zugunsten der Franzosenpartei um. Die Stellung des päpstlichen Parteimanns und Propagandisten Zwingli wurde deshalb unhaltbar.

Zwingli musste 1516 trotz grossen Rückhalts in der Bevölkerung weichen und wurde für drei Jahre beurlaubt.

1516 berief Diebold von Geroldseck Zwingli als Leutpriester und Prediger in das als Wallfahrtsort berühmte Kloster Maria-Einsiedeln, wo er am 14. April 1516 antrat. Angesichts der dortigen Missbräuche der Volksfrömmigkeit begann er, "wider Wallfahrten und andre Missbräuche" und wider den seit 1518 in der Schweiz wirkenden päpstlichen Ablassprediger Bernhardin Samson zu predigen. Er forderte sogar die Bischöfe zu Sitten und Konstanz auf, die Kirche nach Anleitung des göttlichen Wortes zu verbessern. Zu gleicher Zeit trat er aber auch aufgrund seiner Erfahrungen beim Italienfeldzug gegen die Demoralisation des Volkes durch das so genannte Reislaufen an, wie die Kriegsdienste der Schweizer in fremdem Sold damals genannt wurden. Als Konsequenz seiner Beteiligung am Krieg in der Lombardei übernahm er Erasmus’ Überzeugung: «Der Krieg erscheint den Unkundigen als süss» – «Dulce bellum inexpertis», ein Satz, den Zwingli sich in seiner Sprichwörterausgabe des Erasmus von Rotterdam anstrich.

Nach Glättung der Wogen, derentwegen Zwingli Glarus hatte verlassen müssen, hätte er das dortige Pfarramt wieder übernehmen sollen; doch er entschloss sich 1519, stattdessen eine Berufung an das Zürcher Grossmünster anzunehmen. Die intensiven Studien und seine Erfahrungen in Glarus wie auch in Einsiedeln hatten den bis dahin sehr kirchentreuen Priester verändert. Die Entwicklung, die in Glarus begonnen hatte, führte Zwingli in neue Bahnen, und er wurde zu einem scharfen Kritiker der damaligen kirchlichen Zustände.

Da die Zürcher Regierung wie Zwingli gegen das Söldnerwesen war, verschaffte ihm diese Haltung das einflussreiche Amt als Leutpriester am Grossmünsterstift in Zürich, das er am 1. Januar 1519 antrat. Das Grossmünsterstift war damals nach der Kathedrale das angesehenste geistliche Stift im Bistum Konstanz. In seinen kunstlosen, aber klaren, allgemein verständlichen Predigten legte er fortlaufend die Evangelien aus. Das Volk und der Rat von Zürich liessen sich davon überzeugen. Sämtliche Prediger in Stadt und Land wurden 1520 von der Obrigkeit angewiesen, das Evangelium gemäss Zwinglis Auslegung zu predigen.

Im Jahr 1519 brach eine Pestepidemie in Zürich aus, die auch Zwingli im September des Jahres befiel. Er überlebte die Krankheit, war aber noch ein Jahr lang geschwächt. Die Krankheitserfahrung regte ihn zum Schreiben von Gedichten und Liedern an und soll auch sein Gottesverständnis geprägt haben, da er seine Genesung auf Gottes Wirken zurückführte.

1522 veröffentlichte Zwingli seine erste reformatorische Schrift gegen das Fasten der römischen Kirche: "Von Erkiesen und Freiheit der Speisen". Dieses Werk schrieb er aus Anlass des Fastenbrechens bei Christoph Froschauer. Zwingli selbst war beim „Wurstessen“ anwesend, aber nicht beteiligt. Mit der Schrift rechtfertigte er das Handeln, da das Fastenhalten gegen den christlichen Glauben verstosse. An den Bischof von Konstanz sandte er ein ebenso bescheidenes wie nachdrückliches Bittschreiben, in welchem er und zehn seiner Genossen erklärten, dass sie «mit Gott fest entschlossen seien, das Evangelium ohne Unterlass zu predigen», und in dem sie um Aufhebung des Zölibats nachsuchten. Damals bemühte sich Papst Hadrian VI. noch, Zwingli durch einen die Frömmigkeit des Reformators anerkennenden Brief von weiteren Schritten gegen die katholische Kirche abzuhalten.

Mit dem Land Glarus blieb Zwingli weiterhin intensiv verbunden. Mit verschiedenen Personen korrespondierte er auch weiterhin als Zürcher Pfarrer. Die Hauptschrift "Auslegen und Gründe der Schlussreden" von 1523 widmete er dem Landsgemeindekanton. Am 12. Oktober 1522 predigte Zwingli sogar noch einmal in der Pfarrkirche Glarus anlässlich der Primiz seines ehemaligen Schülers Valentin Tschudi. In dieser Predigt wurde die Veränderung Zwinglis deutlich. Was er früher den Glarnern gepredigt habe, so sagte er, sei nicht die Wahrheit gewesen. Die Glarner sollen davon Abstand nehmen. Zwingli distanzierte sich von seiner Verkündigung in den Glarner Jahren 1506 bis 1516.

Als die Dominikaner in Zürich Zwingli Ketzerei vorwarfen, lud der Grosse Rat alle Theologen, die Zwingli der Ketzerei überführen könnten, auf den 29. Januar 1523 zu einer Disputation, der ersten Zürcher Disputation, über die von Zwingli aufgestellten Thesen nach Zürich ein. Etwa 600 geistliche und weltliche Personen fanden sich dazu in Zürich ein. Da die Abgeordneten des Bischofs von Konstanz, namentlich Johann Faber, gegen Zwinglis Thesen nur die Autorität der Tradition und der Konzilien geltend zu machen wussten, erkannte der Rat von Zürich Zwingli den Sieg zu.

Auf einem zweiten, vom 26. bis 28. Oktober 1523 gehaltenen Religionsgespräch in Zürich wurde in Gegenwart von fast 900 Zeugen aus eidgenössischen Orten über «Bilderdienst und Messe» gestritten. Grund für die zweite Zürcher Disputation waren die Predigt gegen Bilderverehrung und der daraus resultierende Bildersturm. Es wurde beschlossen, dass die Bilder innerhalb eines halben Jahres entfernt werden sollten, damit das Volk durch weitere Predigten auf diesen Einschnitt vorbereitet werden könne. Der «Bildersturm», der also nicht an einem Tag und plötzlich erfolgte, führte auch zum «Ittingersturm».

Ein drittes Gespräch am 13. und 14. Januar 1524, die dritte Zürcher Disputation, beseitigte auch die Messe. Noch im selben Jahr, am 19. April 1524, verheiratete sich Zwingli mit der 33-jährigen Witwe Anna Reinhart, mit der er schon vorher unehelich zusammengelebt hatte. Mit ihr zusammen hatte er vier Kinder: Regula (* 31. Juli 1524), Wilhelm (* 29. Januar 1526), Huldrich (* 6. Januar 1528) und Anna (* 4. Mai 1530). 

Die Reformation in Zürich betraf nicht nur die Religion. Der Rat, unter Beratung Zwinglis, ordnete Schul-, Kirchen- und Ehewesen neu und gab Sittengesetze heraus. Zwingli hatte kein politisches Amt, aber grossen Einfluss – der Rat wusste, dass das Volk auf Zwinglis Predigten hörte.

1525 gab Zwingli sein Glaubensbekenntnis «Von der wahren und falschen Religion» heraus, das er dem französischen König Franz I. schickte. Mit Luther und den anderen deutschen Reformatoren in vielen Punkten einig, verfuhr Zwingli doch in liturgischer Beziehung radikaler und verwarf die «leibliche Gegenwart» Christi im Abendmahl. Ab 1525 waren die Reformation und die Reform des Gottesdienstes in Zürich abgeschlossen. Es wurde das Abendmahl in beiderlei Gestalt in Gedächtnis gefeiert. Bilder, Messen und Zölibat waren abgeschafft, und es gab eine geregelte Armenfürsorge. Diese finanzierte sich aus Geldern, die durch die Säkularisation von Klöstern und geistlichen Stiftungen im Herrschaftsbereich der Stadt Zürich frei wurden. Ebenfalls 1525 wurde das bisherige Chorherrenstift Grossmünster in die Propstei am Grossmünster umgewandelt, um die Ausbildung weiterer reformierter Theologen sicherzustellen. Sie mussten Bibelexegese lernen und die gewonnenen Ergebnisse in deutschen Predigten dem Volk vortragen. Dadurch wurden die Theologen geschult, und das Volk sollte in der Bibel verwurzelt werden. Zwingli war als Antistes der Leiter der Zürcher Kirche.

In enger Zusammenarbeit mit Leo Jud übersetzte Zwingli zwischen 1524 und 1529 die Bibel neu in die eidgenössische Kanzleisprache. Diese Übersetzung ist heute als die «Zürcher Bibel» bekannt. Demnach schlossen die Zürcher Theologen die komplette Neuübersetzung aus dem Griechischen und Hebräischen fünf Jahre vor Luthers Bibelübersetzung ab. Die Zürcher Bibel ist somit die älteste protestantische Übersetzung der gesamten Bibel. Das Werk wurde zwischen 1524 und 1529 von Christoph Froschauer gedruckt. 1531 druckte er eine reich illustrierte und aufwendig gestaltete Gesamtausgabe. Diese Version war für lange Zeit die textlich und gestalterisch bedeutendste Ausgabe der Zürcher Bibel.

Zwingli lehnte Luthers Zwei-Reiche-Lehre ab, wonach der Staat für das «Äussere» und die Kirche für das «Innere» zuständig sei. Viel mehr sah er Kirche und Staat in enger Zusammenarbeit und darin für die Obrigkeiten eine ernste Verpflichtung. Er erklärte, dass «die Obrigkeit, welche ausser der Schnur Christi fahren», das heisst, die Vorschriften Christi sich nicht zum Massstab nehmen wolle, «mit Gott entsetzt werden möge». Der Landgraf von Hessen, Philipp der Grossmütige, welcher Zwinglis weittragende politische Ansichten teilte, organisierte im Oktober 1529 ein Streitgespräch zwischen Zwingli und Martin Luther in seinem Schloss in Marburg, den «Abendmahlsstreit zu Marburg». Luther wies Zwingli allerdings schroff zurück, womit der Plan eines gemeinsamen protestantischen Vorgehens gegen Kaiser und Papst an theologischen Differenzen scheiterte.

Philipp der Grossmütige und Zwingli hatten ehrgeizige Pläne. 1530 wollten sie «durch einen Bund von der Adria bis zum Belt und zum Ozean die Welt aus der Umklammerung des Habsburgers retten». Damals hatte Zwingli schon im Januar 1528 bei einem Religionsgespräch zu Bern auch diesen Kanton für die Reformation gewonnen. Ausserdem schien durch den Ersten Kappeler Landfrieden 1529 die drohende Gefahr eines Glaubenskriegs zwischen Zürich und den fünf katholischen Urkantonen vorläufig beseitigt.

1531 kam es zu einem Religionskrieg in der Eidgenossenschaft, dem Zweiten Kappelerkrieg zwischen Zürich und den katholischen Kantonen Luzern, Uri, Schwyz, Unterwalden und Zug. Bereits vorher waren Altgläubige wie beispielsweise die Mönche vor allem der Bettelorden aus den Klöstern vertrieben worden. Zwingli war es auch, der den Rat von Zürich zum Zweiten Kappelerkrieg gegen die Waldstätte drängte, um die Reformation, wenn nicht mit Überzeugung möglich, dann mit Feuer und Schwert auch in der Innerschweiz zu verbreiten. Am 11. Oktober 1531 unterlagen die Zürcher, und Zwingli selbst geriet während der Schlacht bei Kappel, an der er als Soldat teilgenommen hatte, am Albis in die Hände der katholischen Innerschweizer. Er wurde verhöhnt, indem man ihm anbot, noch einmal die Beichte abzulegen, und anschliessend getötet. Sein Leichnam wurde gevierteilt, anschliessend verbrannt und die Asche in den Wind gestreut. Erst 1838 wurde ihm in Kappel und 1885 in Zürich ein Denkmal errichtet. Heinrich Bullinger wurde Zwinglis Nachfolger in Zürich. Er konsolidierte den reformierten Glauben und gilt als eigentlicher Begründer der reformierten Kirche.

Zwinglis Reformation ging von anderen Voraussetzungen aus als Luthers und hatte bei vielen Gemeinsamkeiten auch deutliche Unterschiede zu dieser. Während Luther den Ablasshandel und andere Missstände in der Kirche, die seinem Verständnis der Bibel widersprachen, entfernen wollte, akzeptierte Zwingli in der Kirche nur das, was ausdrücklich in der Bibel stand. Von daher sind die reformierten Kirchen, noch ausgeprägter als die lutherischen, Kirchen des Wortes: kein Kirchenschmuck ausser Bibelsprüchen, sogar auf Musik im Gottesdienst wurde eine Zeit lang verzichtet – obwohl Zwingli selbst sehr musikalisch war.

Auswirkungen der Theologie Ulrich Zwinglis sind vor allem in der deutschsprachigen Schweiz sowie im Waadtland festzustellen. Der Erfolg der Reformation ist dabei nicht ohne weitere Persönlichkeiten wie Johannes Oekolampad und Oswald Myconius in Basel, Berchtold Haller in Bern, Sebastian Hofmeister und Erasmus Ritter in Schaffhausen, Joachim Vadian und Johannes Kessler in St. Gallen und Johann Comander in Graubünden denkbar.

In Deutschland gehen nur die reformierten Kirchen in Bad Grönenbach, Herbishofen und Theinselberg direkt auf Zwinglis Wirken zurück. Die übrigen reformierten Kirchen sind – wie sich am Heidelberger Katechismus ablesen lässt – stärker von Calvins Denken beeinflusst.

Als Schattenseite seines Wirkens wird oftmals Zwinglis Verhältnis zur Täuferbewegung angesehen. Auf Zwinglis Drängen liess der Rat von Zürich alle Täufer der Stadt entweder vertreiben oder nach Gefangennahme und Folterung in der Limmat ertränken. Eines der ersten Opfer unter den Schweizer Täufern war Felix Manz. Auch mit Balthasar Hubmaier, der im nahen vorderösterreichischen Waldshut wohnte, stand er auf schlechtem Fuss und wollte ihm kein Asyl geben, als dieser vor den Habsburgern flüchtete. Die Verfolgungen der Täufer hielten noch über Generationen an. Erst 2004 fand eine versöhnende Versammlung zwischen Zürcher Reformierten und Täufern statt.

Das bekannteste Denkmal Zwinglis wurde vom österreichischen Bildhauer Heinrich Natter gestaltet und am 15. August 1885 vor der Wasserkirche in Zürich eingeweiht, nachdem zuerst ein Entwurf des Baslers Ferdinand Schlöth zur Ausführung vorgesehen war. Die Weiherede hielt Antistes Diethelm Georg Finsler, die offizielle Ansprache der Stadtpräsident Melchior Römer.
Etliche Kirchengebäude aus dem 20. Jahrhundert tragen den Namen Zwinglikirche und erinnern damit an den Reformator.

"Zwinglistrassen" kommen verbreitet vor. So wurde beispielsweise im Jahre 1903 in Dresden eine Strasse nach Zwingli benannt, was heute auch durch eine Erklärungstafel unter dem Strassenschild mit weiteren Informationen zur Person ausgewiesen wird. In Berlin erinnert die Zwinglistraße im Moabiter „Reformatorenviertel“ an Zwingli.

Nach dem Tod Zwinglis wurden zahlreiche Porträts angefertigt, die sich fast alle nach denjenigen des Zürcher Malers Hans Asper richten. Es gibt kein zu Lebzeiten Zwinglis gemaltes Porträt. Zwingli wird üblicherweise in schwarzer Tracht mit schwarzer «Reformatorenmütze» dargestellt.



Zwinglis Gedenktag ist der 11. Oktober im Evangelischen Namenkalender.





</doc>
<doc id="12111" url="https://de.wikipedia.org/wiki?curid=12111" title="Ordensschwester">
Ordensschwester

Eine Ordensschwester ist ein weibliches Mitglied einer Ordensgemeinschaft. Die Ordensschwester weiht ihr Leben Gott und dem Dienst an den Menschen. Sie ist durch Gelübde oder Versprechen an Gott, die Kirche und ihre Gemeinschaft gebunden und dabei ihrer Oberin unterstellt.

Ordensschwestern werden oft fälschlich generell als „Nonnen“ bezeichnet. Nonnen sind jedoch nur die in Klausur lebenden, weiblichen Angehörigen monastischer Orden. Die männliche Entsprechung der "Nonne" ist der Mönch. 

Ordensschwestern oder Nonnen gibt es in praktisch allen vorreformatorischen Kirchen. Bestimmt wird die Lebensform wie bei allen Ordensleuten durch die evangelischen Räte (das heißt Ratschläge des Evangeliums), deren Einhaltung sie mit ihrer Profess öffentlich versprechen:
Ein wichtiges Wesenselement des Ordenslebens ist darüber hinaus das Leben in Gemeinschaft, etwa in Klöstern, Konventen, Fraternitäten oder anderen Kommunitäten.

Ordensfrauen gehören in den Kirchen, die das Weihesakrament kennen und keine Frauenordination zulassen, nicht zum Klerus. Traditionell bilden sie zusammen mit nicht-geweihten männlichen Religiosen, Eremiten, geweihten Jungfrauen und Witwen einen eigenen geistlichen Stand, der weder klerikalen noch laikalen Charakter besitzt und in der lateinischen Kirche heute zusammenfassend als Stand des geweihten Lebens (lat. "Vita consecrata") bezeichnet wird. Kirchenrechtlich sind sie in der lateinischen Kirche allerdings den Laien zuzurechnen.

Nicht zu den Ordensschwestern gezählt werden die ohne Ordensgelübde auf Beginenhöfen lebenden Beginen und vergleichbare Lebensformen sowie die evangelischen Diakonissen. Auch die Mitglieder der Säkularinstitute in der römisch-katholischen Kirche werden nicht als Ordensfrauen oder -männer bezeichnet, obwohl auch sie als Angehörige des geweihten Lebens meist Gelübde oder Versprechen ablegen.



</doc>
<doc id="12113" url="https://de.wikipedia.org/wiki?curid=12113" title="Laie">
Laie

Laie (von griechisch "λαός (laós)" „Volk“ über "λαϊκός (laikós)" „zum Volk gehörig“; kirchenlateinisch "laicus" „der (kirchliche) Laie“) bezeichnet:


Lāʻie bezeichnet außerdem:

Siehe auch:



</doc>
<doc id="12114" url="https://de.wikipedia.org/wiki?curid=12114" title="Abt">
Abt

Ein Abt (von spätlat. "abbas", aus aram. "abba" „Vater“, aus hebr. "ab") war ursprünglich ein allgemeiner Ehrenname und ist seit dem 5./6. Jahrhundert den Vorstehern eines Klosters vorbehalten; die weibliche Entsprechung ist die Äbtissin. Das Amt, die Amtszeit oder die Würde eines Abtes werden als Abbatiat bezeichnet.

Vor allem monastische Orden der katholischen Kirche wie die Benediktiner und Zisterzienser haben Äbte beziehungsweise Äbtissinnen. Diese sind Souveräne über die Abtei und direkt dem Papst unterstellt. Im Mittelalter hatten manche Äbte als Fürstäbte auch weltliche Gewalt und Gerichtsbarkeit in den Besitzungen der Abtei. Dies gilt auch für einen Teil der Augustiner-Chorherren und die Prämonstratenserchorherren, die sowohl Äbte als auch Pröpste kennen. Äbte sind Prälaten. Die Entsprechung in den orthodoxen Kirchen oder im byzantinischen Ritus ist Hegumen bzw. Archimandrit.

Äbte werden in der Regel auf unbestimmte Zeit gewählt; heutzutage ist allerdings ein Trend zu einer begrenzten Amtszeit auf sechs oder zwölf Jahre erkennbar. Die Konstitutionen der Orden sehen oft einen Amtsverzicht des Abtes (zum 70. oder 75. Lebensjahr) vor. Eine Verlängerung der Amtszeit ist jedoch unter bestimmten Voraussetzungen möglich. Ungeachtet dessen hat der Abt jederzeit auch die Möglichkeit der vorzeitigen Resignation.

Der Abt wird von allen stimmberechtigten Professen des Klosters gewählt. Wählbar sind in den Mönchsorden, die als klerikale Verbände gelten, nur Priester. Das Ergebnis der Wahl wird dem Diözesanbischof und dem Apostolischen Stuhl sowie der Ordensleitung mitgeteilt. Eine Bestätigung der Wahl durch den Papst ist nur nötig, wenn der neugewählte Abt zugleich Präses seiner Kongregation ist, in den anderen Fällen bestätigt der Generalabt oder Abtpräses der Kongregation. Anschließend empfängt der gewählte Abt die Abtsbenediktion und die Pontifikalien (Krummstab, Ring und Pektorale) und der Abt (naturgemäß nicht die Äbtissin) oft auch die Mitra.

Die Abtsbenediktion, umgangssprachlich als „Abtsweihe“ bezeichnet, ist die Amtseinsetzungsfeier eines Abtes. Die Abtsbenediktion lehnt sich zwar liturgisch stark an eine Bischofsweihe an, ist jedoch ein Sakramentale. Abt einer Gemeinschaft wird man durch die Wahl, nicht durch Weihe. Die Abtsbenediktion wird in der Regel vom Ortsbischof gespendet. Sie stellt keine Beauftragung durch den Ortsbischof dar, wohl aber den kirchlichen Segen für den Dienst des Abtes in seiner Gemeinschaft und mittelbar für das ausgeübte Apostolat der Gemeinschaft in der jeweiligen Ortskirche und in der Weltkirche. In der Benediktionsfeier werden dem Erwählten die Ordensregel und die Amtszeichen (Stab, Ring) und dem Abt oft auch die Mitra überreicht.

Abteien sind grundsätzlich exemt und unterstehen damit direkt dem Heiligen Stuhl. Die Äbte üben teils väterliche Gewalt ("potestas domestica"), teils Jurisdiktionsgewalt aus. Diese umfasst die Verwaltung des Klostervermögens, die Leitung des Klosters und die Disziplin der Angehörigen. Bei der Veräußerung von Klostergütern müssen sie laut Kirchenrecht die Zustimmung des Rates einholen. Ebenso ist in anderen wichtigen Fragen, je nach Bestimmung des Kirchenrechtes und der Ausgestaltung in der eigenen Ordensregel, der Abtsrat anzuhören oder es muss seine Zustimmung eingeholt werden. Äbte (nicht die Äbtissinnen) gehören zu den Prälaten. Die Abts- bzw. Äbtissinnenbenediktion ist keine Ordination wie die Weihe zum Diakon, Priester oder Bischof. Sie kann nur von Bischöfen oder Äbten erteilt werden.

Von den wirklichen (Regular)Äbten sind zu unterscheiden die Säkular-, Kommendatar- und Laienäbte – diese waren Personen, die die Pfründe, also die wirtschaftlichen Einkünfte eines Klosters innehatten, ohne jedoch im Kloster zu wohnen und die Amtsgeschäfte zu führen. Der Kommendatarabt war oft ein Weltgeistlicher oder Laie, der vom jeweiligen Landesherrn ernannt wurde. Die geistliche Leitung des Klosters lag meist hauptsächlich bei einem Mönch des Klosters, der oft als Prior betitelt wurde. Schon seit der Merowingerzeit wurden im fränkischen Reich Laien mit Abteien belehnt. Der zuerst unter Karl Martell aufgetretene Brauch wurde zwar von der Kirche meist bekämpft, doch je nach politischer Macht der jeweiligen Landesherrn blieb der Kirche zeitweise nichts anderes übrig, als diese Praxis zu akzeptieren. So hatte auf Grund eines zwischen Papst Leo X. und König Franz I. von Frankreich zwischen 1515 und 1521 abgeschlossenen Kontrakts der König von Frankreich das Recht, 225 "Abbés commendataires", also Kommendataräbte, für fast alle französischen Abteien zu ernennen. Diese bezogen Einkünfte aus einem Kloster, ohne dafür Dienst leisten zu müssen. Mit der Französischen Revolution bzw. nach der Säkularisation in Deutschland ist in der Praxis die Vergabe dieses Titels zu Beginn des 19. Jahrhunderts erloschen.

In seltenen Fällen wird der Titel und die Würde des Abts vom Papst auch als Ehrentitel an nicht „regierende“ Ordensleute verliehen; man bezeichnet sie als Titularäbte. Diese empfangen zwar in der Regel die Abtsbenediktion, besitzen aber keine Leitungsgewalt über einen Konvent, sondern sind lediglich mit den - vor allem liturgischen - Vorrechten der Äbte ausgestattet.

Der Stellvertreter eines Abtes wird Prior genannt, dessen Vertreter ist der Subprior. Prior und Subprior werden wie die anderen Offizialen vom Abt ernannt und nicht vom Konvent gewählt.








Die Vorsteher von Niederlassungen nicht monastischer Orden tragen andere Titel, wie Propst, Prior, Guardian, Superior, Rektor oder Direktor. Diese werden in der Regel vom Provinz- oder Generalkapitel und meist für eine zeitlich beschränkte Amtszeit gewählt. Bei dem 1095 gegründeten Hospitalorden der Antoniter wurde der Klostervorsteher als Präzeptor bezeichnet.

Im Zuge der Reformation behielten die evangelischen Äbte der reformierten Klöster zunächst die Amtsbezeichnung Abt bei. Im Laufe der Zeit setzten sich dann andere Bezeichnungen durch, so hießen die württembergischen Klostervorsteher bald nur noch "Prälaten". Doch gibt es auch heute noch evangelische Konvente, deren Obere den Titel "Abt" bzw. "Äbtissin" tragen. Ein streng reguliertes Leben wie katholische Klöster führen diese Gemeinschaften jedoch nicht.




</doc>
<doc id="12115" url="https://de.wikipedia.org/wiki?curid=12115" title="Bischof">
Bischof

Ein Bischof (von "epískopos" ‚Aufseher‘, ‚Hüter‘, ‚Schützer‘) ist in vielen Kirchen der Inhaber eines Amtes, der die geistliche und administrative Leitung eines bestimmten Gebietes hat, das üblicherweise zahlreiche lokale Gemeinden umfasst. Das Bischofsamt und auch die Gesamtheit der Bischöfe werden als "Episkopat" bezeichnet.

Im Neuen Testament bezeichnen ἐπίσκοπος ("epískopos" „Bischof“) ebenso wie πρεσβύτερος (Presbyter „Ältester“, die Wurzel des Wortes Priester) und (Diakon „Diener“) rein informelle Positionen in der lokalen Gemeinde.

Die frühen Christengemeinden wurden nicht von Einzelnen, sondern – wie schon in anderen Gemeinden im Altertum üblich – von einer Gruppe Ältester geleitet. Diese setzten je nach Bedarf und meist zeitlich befristet einen "επίσκοπος" ein oder wählten ihn auch wieder ab. Erst im Verlauf der ersten Jahrhunderte und abhängig vom Organisationsgrad der jeweiligen Gemeinde entwickelte sich neben dem Ältestenrat auch das Amt des Bischofs und des Diakons als Dauereinrichtungen mit definierten Zuständigkeiten. In dieser Zeit entstand also schrittweise und in regional sehr unterschiedlicher Geschwindigkeit das sogenannte „monarchische Episkopat“, in dem schließlich dem Bischof allein () die Führung () übertragen wurde, nachdem ihn der Ältestenrat vorgeschlagen und die Gemeinde bestätigt hatte. 

Erstmals belegt ist dieses Monepiskopat in den Schriften des Ignatius von Antiochien, doch erst in der Spätantike wurden die Presbyter systematisch von der Leitung der Gemeinde ausgeschlossen und eine eindeutige Hierarchie geschaffen. Im späten 2. und 3. Jahrhundert war der Bischof hingegen zumeist lediglich der Leiter einer lokalen Gemeinde, die teils weniger als 20 Personen umfasste, predigte und leitete die Feier der Eucharistie. Unterstützt wurde er von einem Gremium von Ältesten und von Diakonen. Diese Amtsfunktionen sind, mit unterschiedlichen Bezeichnungen, bis heute in den meisten Kirchen vorhanden.

Nach dem apostolischen Zeitalter etablierten sich ab dem ausgehenden zweiten Jahrhundert neben den weiterhin vorhandenen örtlichen Bischöfen mehr und mehr auch Bischöfe, die über mehrere Gemeinden die Aufsicht führten. In solchen Fällen leiteten dann Presbyter als Vertreter des Bischofs die Eucharistiefeier in den lokalen Gemeinden, die Diakone, waren die Mitarbeiter des Bischofs auf gemeindeübergreifender Ebene. Der Bereich eines solchen Bischofs wurde seit dem 4. Jahrhundert Diözese (von griech. "dioíkēsis" ‚Verwaltung[sbezirk]‘) genannt und umfasste meist eine Stadt und die umliegenden Dörfer; die Stadt war der Bischofssitz. Die Kirche übernahm damit die administrative Struktur des spätantiken Römischen Reiches, in dem es ebenfalls Diözesen gab: Die kirchliche Hierarchie (Bistum, Diözese und Patriarchat) entsprach teils sogar in der Grenzziehung der Sprengel genau der weltlichen von Provinz bzw. Civitas, Diözese und Prätorianerpräfektur. Sie bewahrte sie über das Ende der römischen Herrschaft hinaus. Als Nord- und Mitteldeutschland sowie andere nord- und osteuropäischen Gebiete jenseits der römischen Grenzen christianisiert wurden, gab es dort noch keine Städte, daher wurden die neuen Diözesen dort ziemlich große ländliche Bezirke. Noch heute sind die Diözesen hier viel größer als im einstigen Gebiet des "Imperium Romanum", wo es schon in der Antike Städte gab.

In der Auseinandersetzung mit häretischen Strömungen entwickelten sich drei Normen, um die christliche Glaubenslehre von abweichenden Lehren zu unterscheiden:

In der Folge kam es bei den Bischöfen zu unterschiedlichen Verantwortungsbereichen, wobei manche Bischöfe, gewöhnlich diejenigen einer Provinzhauptstadt, eine Aufsichtsfunktion über die übrigen Bischöfe der Gegend bekamen, woraus sich dann eine Rangordnung von Patriarch, Metropolit oder Erzbischof und Bischof entwickelte (Kirchenprovinz).

Die Ostkirche schließt sich in ihrem Verständnis des Bischofsamts eng an das der alten Kirche an. Die orthodoxen Bischöfe stehen ebenso wie die katholischen (römisch-katholisch, alt-katholisch, anglikanisch), in der apostolischen Sukzession.

Zur liturgischen Kleidung orthodoxer Bischöfe gehört der Sakkos, das dem römisch-katholischen Pallium entsprechende Omophorion, die mit einem Kreuz versehene Mitra oder Stephanos und das auf der rechten Seite getragene Epigonation.

Es gibt das dreifache Amtsverständnis, und beim Bischofsamt verschiedene Rangstufen vom Bischof bis zum Patriarchen. Die orthodoxe Kirche kennt jedoch keine geistliche Hierarchie der Bischöfe: Patriarch und Metropolit sind nur Primus inter pares im Bischofskollegium, nicht hierarchische Vorgesetzte, und ein Bischof ist innerhalb seiner eigenen Diözese nicht an Weisungen eines übergeordneten Bischofs gebunden. Andererseits kann eine lokale Synode Entscheidungen treffen, an die der lokale Bischof gebunden ist, und die Entscheidungen ökumenischer oder panorthodoxer Konzile sind auch für Patriarchen bindend.

Da Bischöfe in der orthodoxen Kirche im Zölibat leben, Priester und Diakone aber gewöhnlich verheiratet sind, kommen die meisten orthodoxen Bischöfe aus dem Mönchtum – ein verwitweter Priester kann aber ebenfalls Bischof werden.

Die Wahl der Bischöfe ist in den einzelnen orthodoxen Kirchen verschieden geregelt, jedoch wird die kollektive Zustimmung der Bevölkerung durch den Ruf "Axios!" (griechisch für „er ist würdig“) als wichtiger Teil der Weihe gesehen. Die Abdankung von Bischöfen aufgrund von Druck aus der Bevölkerung ist ebenfalls häufiger als in der katholischen Kirche. Die Größen der Diözesen unterscheiden sich sehr stark zwischen den einzelnen orthodoxen Kirchen.

Im Unterschied zur römisch-katholischen Kirche ist das Sakrament der Firmung nicht dem Bischof vorbehalten, sondern wird direkt nach der Taufe durch den Priester gespendet. Das Wesen der altkirchlichen Tradition der Herabbittung des Heiligen Geistes auf den Getauften durch den Bischof wird dennoch beibehalten, indem das zur Firmung verwendete Öl nur von bestimmten Bischöfen geweiht werden darf (meist durch den Vorsteher der jeweiligen autokephalen Kirche oder sogar nur durch den Ökumenischen Patriarchen).

Beim Bischofsamt handelt sich um die höchste Stufe des Weihesakramentes. Ein römisch-katholischer Bischof ist immer männlich und muss zuerst zum Diakon und dann zum Priester geweiht worden sein. Die Weihe zum Bischof erfolgt durch einen anderen Bischof, meist mit zwei assistierenden weiteren Bischöfen. Eine Weihe ist nur erlaubt, wenn sie der Papst zuvor bestimmt hat.

Ortsbischöfe (Diözesanbischöfe) werden je nach Bistum direkt vom Papst ernannt oder von verschiedenen Wahlgremien (in der Regel Domkapitel) in verschiedenen ortstypischen Verfahren gewählt. Die Wahl gilt als rechtmäßig, wenn sie vom Papst bestätigt wird. Voraussetzung für den Amtsantritt ist die Bischofsweihe, die dem Ernannten – so er noch nicht Bischof ist – in der Regel in einem Pontifikalamt gespendet wird.

Die Bischöfe sind dem Jurisdiktionsprimat des Papstes unterworfen. Dazu gehören:

Nach katholischer Lehre setzt sich in den Bischöfen die Lehr- und Leitungsvollmacht fort, die Jesus den zwölf Aposteln übertrug. In einer ununterbrochenen „Reihe der Handauflegungen“ (apostolische Sukzession) seien alle heutigen Bischöfe mit den Aposteln verbunden. Somit gehört das Bischofsamt zum sogenannten göttlichen Recht. Den obersten Dienst der Einheit hat der Bischof von Rom.

Nach Überlieferung und Tradition der katholischen Kirche war der Apostel Petrus der erste Bischof der Stadt Rom; hierauf beruht der Primat seines Nachfolgers auf dem Stuhl Petri. Dem Papst stehen in seinen Aufgaben die Römische Kurie und die Rota Romana als geistlicher Gerichtsstand der Bischöfe unterstützend zur Verfügung. Für den Papst gilt: Zwar kann jeder männliche Katholik, der zur Amtsübernahme fähig und willens ist, zum Bischof von Rom gewählt werden; ist der Gewählte aber kein Bischof, werden ihm noch im Konklave die nötigen Weihen gespendet. Praktisch hat das keine Bedeutung, da seit der Wahl Urbans VI. 1378 alle Päpste dem Kardinalskollegium entstammten. Als letzter Papst, der bei seiner Wahl zwar Kardinal, aber nicht Bischof war, wurde Gregor XVI. 1831 in dieses Amt gewählt.

Ein Bischof ist entweder Diözesanbischof (auch residierender Bischof oder Ortsbischof genannt) oder Titularbischof. Weihbischöfe sind stets Titularbischöfe und einem Diözesanbischof als Helfer bei den bischöflichen Funktionen zugeordnet. Der Diözesanbischof ist Vorsteher seiner Diözese (Bistum) und hat über sie die volle Leitungsgewalt (oberste Lehr- und Rechtsvollmacht) inne. Er ist allein dem Papst verantwortlich. Zur Verwaltung der Diözese stehen dem Bischof mehrere Kleriker zur Seite, die mit ihm die bischöfliche Kurie bilden; unter anderen der Generalvikar (der allgemeine und ständige Vertreter des Bischofs), der Offizial (Inhaber der ordentlichen Gerichtsgewalt) und der Kanzler (Vorsteher der bischöflichen Registratur). Priester- und Laiengremien haben beratende Funktionen. Bischöfe beraten sich über Bistumsgrenzen hinaus in der meist nationalen Bischofskonferenz. Bei Bedarf kann ein Bischof für seine Diözese auch eine Diözesansynode einberufen.

Der Diözesanbischof kann durch Weihbischöfe unterstützt werden, die meist jeweils einen Teil des Bistums im Auftrag des Diözesanbischofs betreuen. Andere Weihbischöfe haben besondere seelsorgerische Aufgaben oder sind Teil der bischöflichen Kurie. Im deutschen Sprachraum haben, auch aus geschichtlichen Gründen, fast alle Diözesen meist mehrere Weihbischöfe, was andernorts nicht so ist.

Ein Metropolit bzw. Erzbischof ist der Vorsteher einer Kirchenprovinz, die aus mehreren Bistümern, den Suffraganbistümern, besteht. Der Metropolit ist Diözesanbischof innerhalb der Kirchenprovinz. Eine Leitungsgewalt in den Suffraganbistümern hat er aber nicht.

Der Begriff Erzbischof war ursprünglich mit dem des Metropoliten gleichbedeutend. Als Erzbischof werden allerdings auch die Titularbischöfe ehemaliger Erzbistümer bezeichnet, die keinerlei Jurisdiktion besitzen. Von da ausgehend hat sich die Bezeichnung Erzbischof heute auch als eine Art Rang etabliert; sämtliche kirchenrechtlichen Funktionen des Metropoliten werden nur noch unter dem letzteren Titel im Kirchenrecht aufgeführt. Ranghohe Kurienbischöfe und alle Nuntii werden zu Titularerzbischöfen ernannt. Einzelne exemte und Suffragandiözesen haben den Ehrenrang "Erzdiözese" erhalten (z. B. Straßburg), und auch einzelne Bischöfe anderer Diözesen erhalten den Ehrentitel Erzbischof (z. B. Josef Stimpfle). Dabei sind als gängige Praxis vor allem zwei Dinge zu beobachten: Kurienerzbischöfe, die auf einen einfachen Bischofssitz versetzt werden, behalten stets ihren Titel (z. B. Johannes Dyba). Und bei Zirkumskriptionsveränderungen werden zwar Metropolitansitze aufgehoben oder verschoben, die so degradierten Bistümer aber ausnahmslos dadurch entschädigt, dass sie weiterhin im Range einer Erzdiözese verbleiben (z. B. Aix et Arles). Dennoch werden die Begriffe Erzbischof und Metropolit, wenigstens in Deutschland, weiterhin landläufig als Synonyme verwendet. Vor diesem Hintergrund ist es als Kuriosum zu werten, dass die Erzbischöfe von Udine und Izmir als „Metropoliten ohne Suffragane“ aufgeführt werden. Dennoch sind Erzbischöfe ohne Metropolitansitz weiterhin sowohl grundsätzlich wie zahlenmäßig die Ausnahme.

Manche römisch-katholische Bischöfe tragen den Ehrentitel eines Patriarchen (Venedig, Lissabon, Ostindien), andere sind Patriarchen im Sinne einer eigenen Jurisdiktion über ihr Patriarchat (Unierte Ostkirchen und Jerusalem), verbunden mit besonderen Vorrechten. Bis 2005 gehörte der Titel „Patriarch des Abendlandes“ (auch „Patriarch des Westens“) zu den Titeln des Papstes und kennzeichnete den Papst als Patriarchen mit der Jurisdiktion über die Westkirche.

Die Kardinäle werden vom Papst ernannt, und wählen nach dem Ende eines Pontifikates den Nachfolger. In der Regel ist ein Kardinal vor seiner Ernennung bereits zum Bischof geweiht, ansonsten hat dies gemäß Kirchenrecht nach der Ernennung zu geschehen. Im Einzelfall (z. B. bei hohem Alter), kann der Papst davon dispensieren (so geschehen bei Leo Kardinal Scheffczyk und Karl Josef Kardinal Becker SJ). Davon abgesehen, hat die Kardinalswürde nichts mit dem Amt des Bischofs zu tun. Lediglich die Kardinalbischöfe sind historisch aus bischöflichen Ämtern hervorgegangen, nämlich aus den Suffraganen des Papstes. Die Kardinalpriester und -diakone führen sich dagegen nicht auf Bischofsämter, sondern auf die der römischen Stadtpfarrer und -diakone zurück; bei der ursprünglich den Kardinalbischöfen zustehenden Papstwahl hatten diese Klassen zunächst ein Beratungsrecht und erhielten 1059 dann ein Stimmrecht.

Der Papst als Bischof von Rom leitet die Weltkirche und hat die oberste Jurisdiktion über alle Bischöfe inne (Jurisdiktionsprimat). Zur Verwaltung der Weltkirche steht dem Papst die römische Kurie zur Seite. Seine wichtigsten Mitarbeiter stehen im Rang eines Kardinals (Kurienkardinal) oder Titularbischofs (Erzbischof oder Bischof).

Der Bischöfliche Stuhl repräsentiert das Amt eines Bischofs und ist sowohl eigenständiges Rechtssubjekt als auch Vermögensträger, in Deutschland meist als Körperschaft des öffentlichen Rechts. Neben dem Bischof als Repräsentant gehören zum bischöflichen Stuhl auch die Verwaltungseinrichtungen der Diözesankurie. Wenn ein Bischof stirbt oder aus anderen Gründen sein Amt verlässt, ist der bischöfliche Stuhl vakant (Sedisvakanz). Die Bezeichnung „Stuhl“ leitet sich von der Funktion der Kathedra ab, ein seit der Antike überliefertes Symbol der Vollmacht eines öffentlichen Amtsträgers.

In der Alten Kirche wurde synonym die Bezeichnung „heiliger Stuhl“ für jeden Bischofssitz verwendet. Erst später hat sie sich auf den besonders bedeutsamen bischöflichen Stuhl des Bistums Rom fokussiert und wird seit dem 19. Jahrhundert nahezu ausschließlich auf diesen bezogen. Der Heilige Stuhl bildet als „nichtstaatliche souveräne Macht“ ein eigenes Völkerrechtssubjekt und vertritt in internationalen Beziehungen den Staat Vatikanstadt und die ganze römisch-katholische Kirche.

Der Bischof hat für seine Diözese, unbeschadet der Pflichten gegen den Papst, die Fülle der Leitungs-, Lehr- und Heiligungsgewalt inne („als Lehrer in der Unterweisung, als Priester im heiligen Kult, als Diener in der Leitung“) und ist damit auch der erste Spender der Sakramente. Vorbehalten sind ihm die Spendung des Weihesakramentes (Bischofsweihe, Priesterweihe und Diakonenweihe) und die Firmung (diese ist im Ausnahmefall an Priester delegierbar). Auch die Spendung bestimmter Sakramentalien – wie etwa die Jungfrauenweihe, die Weihe der Heiligen Öle und die Kirch- und Altarweihe – bleiben dem Ortsbischof vorbehalten.

Das Bischofsamt besteht auf Lebenszeit. Mit Vollendung des 75. Lebensjahres sind jedoch alle Diözesanbischöfe gemäß Kirchenrecht und dem Apostolischen Schreiben "Imparare a congedarsi" angehalten, dem Papst den Amtsverzicht anzubieten (siehe Altdiözesanbischof). Ein solcher Amtsverzicht wird allerdings nicht immer angenommen.

Die sogenannten Pontifikalien eines Bischofs sind Mitra, Stab (Verdeutlichung der Hirtenaufgabe), Bischofsring (bzw. Fischerring des Bischofs von Rom) und Brustkreuz. Des Weiteren kommen hierzu die nur noch selten verwendeten Pontifikalschuhe und Pontifikalhandschuhe sowie die unter dem Messgewand getragene Dalmatik (beim Bischof spricht man von Pontifikaldalmatik), die eigentliche Kleidung des Diakons, welche die sakramentale Vollmacht des Bischofs symbolisieren soll. Ein Diözesanbischof ist berechtigt, in allen Kirchen seines Bistums mit der Cappa magna einzuziehen. Einige dieser Insignien finden sich auch bei nichtbischöflichen Amtsträgern mit besonderer Jurisdiktion, wie zum Beispiel Äbten. Diese sind jedoch nicht berechtigt, Pontifikalschuhe, -handschuhe, oder -dalmatiken zu verwenden. Metropoliten tragen zusätzlich zu den beschriebenen Insignien das Pallium, das ihnen vom Papst verliehen wird. Außerdem sind die Erzbischöfe von Paderborn und Krakau sowie die Bischöfe von Eichstätt und Toul-Nancy berechtigt, zusätzlich das Rationale zu tragen.

Die standesgemäße Anrede eines Bischofs ist „Exzellenz“, „Hochwürdigster Herr“ oder „Herr Bischof“, für einen Erzbischof entsprechend „Herr Erzbischof“. Bis in die erste Hälfte des 20. Jahrhunderts war darüber hinaus die Anrede „Euer Bischöfliche Gnaden“ verbreitet, die im Schriftverkehr mit „Ew. Bischöfliche Gnaden“ abgekürzt werden konnte. Die protokollarische Anrede eines Kardinals lautet „Euer Eminenz“ oder „Herr Kardinal“.

Die Höhe der Bischofsbesoldung orientiert sich an der Beamtenbesoldung für leitende Positionen des höheren Verwaltungsdienstes, der Besoldungsordnung B. Es gibt hierbei Unterschiede zwischen den Diözesen. Erzbischöfe werden maximal nach Besoldungsgruppe B 11 bezahlt, dies entspricht einem Brutto-Monatseinkommen von etwa 12.000 Euro. Die Diözesanbischöfe von Freiburg (Erzbischof) und von Rottenburg-Stuttgart werden nach B 8 besoldet, die Weihbischöfe des Erzbistums Freiburg werden nach B 4 bzw. B 6 besoldet, die Weihbischöfe des Bistums Rottenburg-Stuttgart nur nach B 2/B 3.
Der Diözesanbischof von Speyer ist in B 7 eingewiesen, sein Weihbischof in B 4. Der Erzbischof von München-Freising wird etwa nach B 10 bezahlt, der Erzbischof von Bamberg nach B9 und die übrigen fünf bayerischen Diözesanbischöfe nach B 6. In kleineren Diözesen richtet sich die Bezahlung des Bischofs nach B 2 bis B 6 (insbesondere in den neuen Bundesländern).

Die Bezahlung der römisch-katholischen und evangelisch landeskirchlichen Bischöfe erfolgt jedoch nicht aus Kirchensteuermitteln, sondern durch das jeweilige Bundesland – mit Ausnahme Hamburgs und Bremens. Allerdings wird den leitenden Geistlichen in der Regel kein unmittelbares Gehalt ausgezahlt, sondern Grundlage dieser Zahlungen sind Verträge aus dem 19. Jahrhundert, als im Zuge der Säkularisation Kirchengüter enteignet wurden und zum Ausgleich in den Staatskirchenverträgen Gesamtbeträge für die jährlichen Zahlungen vereinbart wurden, so genannte Dotationen, die der Kirche zur freien Verfügung stehen. Für die Dotationen an die Kirchen wurden im Jahre 2010 insgesamt 459 Millionen in den Haushaltsplänen der Länder veranschlagt.

Nach Artikel 10 § 1a des Bayerischen Konkordats aus dem Jahre 1924 sollen diese Zahlungen ersetzt werden:

Mithin zahlt der bayerische Staat weiterhin die Reineinkünfte unmittelbar an die Bistümer. Die Zahlungen sind Teil der sog. Staatsleistungen an die Religionsgemeinschaften.

Nach altkatholischem Verständnis ist das Bischofsamt das höchste Amt der Kirche und an eine tatsächlich existierende Diözese gebunden. Hierin kommt der altkirchliche Grundsatz zum Ausdruck, der von Urs Küry um den zweiten Halbsatz erweitert wurde: "nulla ecclesia sine episcopo, nullus episcopus sine ecclesia" (keine Kirche ohne Bischof, kein Bischof ohne Kirche). Daher gibt es in den altkatholischen Kirchen Weihbischöfe nur noch in seltenen Fällen (z. B. schwere Krankheit oder hohes Alter des amtierenden Bischofs).

Voraussetzung für die Bischofsweihe ist, dass der Kandidat für das Bischofsamt vor der Bischofsweihe bereits zum Diakon und zum Priester geweiht wurde (Weihen, die in anderen katholischen Kirchen erfolgten, werden als gültig anerkannt und daher nicht wiederholt).
Folgende Schritte sind einzuhalten:
Merkmal eines altkatholischen Bischofs ist folglich, dass dieser sowohl gewählt als auch geweiht wurde. Fehlt der erste Schritt (wie dies bei Vagantenbischöfen der Fall ist), stellt das die Gültigkeit der Weihe in Frage. Ist dagegen der Konsekrand gültig gewählt, die Weihe jedoch noch nicht vollzogen, kann dieser als „Bischof electus“ – wenn die Ordnung seiner Ortskirche dies zulässt – bereits bischöfliche Funktionen ausüben, die nicht die Bischofsweihe voraussetzen.

Die altkatholischen Kirchen sind autonome Ortskirchen. So kommt dem Erzbischof von Utrecht, der zugleich Präsident der Internationalen Bischofskonferenz der Utrechter Union ist, als Inhaber des ältesten Bischofssitzes der Ehrenvorrang zu, er hat aber keine jurisdiktionellen Befugnisse, die über seinen Sprengel hinausgehen.

Der Eintritt in den Ruhestand und das Höchstalter des Bischofs sind auf ortskirchlicher, das heißt nationaler Ebene geregelt. In Deutschland etwa gilt, dass der Bischof oder die Bischöfin mit dem Erreichen des gesetzlichen Rentenalters in den Ruhestand tritt. In der Schweiz gilt 70 als Altersgrenze, nach der ein Bischof in den Ruhestand treten muss. Auch danach kann er oder sie in der Liturgie noch bischöfliche Funktionen ausüben, während die Leitung des Bistums allein dem Nachfolger oder der Nachfolgerin zukommt.

Die Insignien eines altkatholischen Bischofs entsprechen denen eines römisch-katholischen Bischofs: Mitra, Stab, Ring und Brustkreuz. Aufgrund der Trennung von Rom tragen die Erzbischöfe von Utrecht seit 1723 kein Pallium. Sie nehmen jedoch für sich beim feierlichen Einzug das Privileg eines Vortragekreuzes in Anspruch, welches mit dem Korpus zu ihnen gewendet ist. Dieses Privileg war ursprünglich mit der Verleihung des Palliums verbunden.

Entsprechend der altkirchlichen Überlieferung bleiben die Weihen der heiligen Öle, die Kirch- und Altarweihe sowie die Sakramente der Firmung und der Weihe dem geweihten Bischof vorbehalten. Ist er in einem Gottesdienst anwesend, so kommt ihm in der Regel die Leitung der Heiligen Messe und die etwaige Spendung anderer Sakramente zu, auch wenn sie ihm nicht ausdrücklich vorbehalten sind. Ein altkatholischer Bischof kann in allen Gemeinden seines Bistums aus seelsorgerischen Gründen gottesdienstliche Handlungen vornehmen (z. B. Taufen, Trauungen, Krankensalbungen, Requiem).

In einigen altkatholischen Kirchen kann, seit dort auch Frauen durch Synodenbeschlüsse zur Ordination zugelassen wurden, Frauen die Bischofsweihe gespendet werden. Altkatholische Bischöfe sind nicht zum Zölibat verpflichtet.

In den lutherischen Territorialkirchen im Heiligen Römischen Reich deutscher Nation übernahmen die jeweiligen Landesherren faktisch die Leitung der Kirchen („Landesherrliches Kirchenregiment“). Als „Ersatzbischöfe“ übten sie ihre Kompetenzen aber nicht direkt, sondern durch Konsistorien aus. Versuche, das Bischofsamt auf evangelischer Grundlage zu reformieren, waren erfolglos. Im weiteren Verlauf des 16. Jahrhunderts wurden in fast allen Territorien Generalsuperintendenten zur Ausübung der geistlichen Aufsicht eingesetzt. Das änderte sich in den deutschen Monarchien erst durch deren Abschaffung durch die Revolutionen 1918/1919. Im Ergebnis der Debatten in den 1920er Jahren entstand im Deutschen Reich ein „synodal-episkopales Mischsystem“. In Dänemark wurden im Zuge der Reformation 1537 die Bischöfe durch Superintendenten ersetzt, deren Kompetenzen den deutschen Generalsuperintendenten entsprachen. Nur in Schweden blieb das historische Bischofsamt weitgehend erhalten.

Heute gibt es in den lutherischen Kirchen sowohl in Deutschland (VELKD) als auch in Nordeuropa in der Regel das Amt des Bischofs, der für eine Region oder eine Landeskirche zuständig ist und gegenüber den Pfarrern der Ortsgemeinden eine Leitungsfunktion hat. Dieses Amt wird meist als Bischof bezeichnet, daneben ist die Bezeichnung Landesbischof verbreitet. Die Selbständige Evangelisch-Lutherische Kirche (SELK), eine altkonfessionelle lutherische Kirche in Deutschland, wird von einem Bischof geleitet. Er ist Bischof seiner Kirche für die gesamte Bundesrepublik Deutschland.

Unter den lutherischen Kirchen in den Vereinigten Staaten werden manche von einem Bischof geleitet (z. B. ELCA), bei anderen (z. B. Evangelisch-Lutherische Missouri-Synode) wird der leitende Geistliche als Präses bezeichnet.

In Deutschland gibt es, im Unterschied etwa zu den meisten lutherischen Kirchen in Skandinavien und Übersee, keinen eigenen Ritus der Ordination für Bischöfe, diese werden nur in ihr Amt eingeführt. Die Funktion wird nicht als höherer geistlicher Rang, sondern als eine Art Pfarrer im kirchenleitenden Dienst gesehen. Es gibt keine Sakramente, deren Spendung dem Bischof vorbehalten wäre. Im deutschen Sprachraum (anders als z. B. in den skandinavischen Ländern und im Baltikum) spielt die apostolische Sukzession im Bischofsamt für die lutherischen Kirchen keine Rolle. Evangelisch-lutherische Amtsinhaber werden in der Regel von der Synode (Kirchenparlament) für eine bestimmte Zeit oder auf Lebenszeit (meist bis zum 65. oder 68. Lebensjahr) gewählt.

In den meisten evangelischen Kirchen kann das Amt sowohl von Männern als auch von Frauen ausgeübt werden. In der SELK ist es, wie auch die Ordination zum Pfarrer, Männern vorbehalten.

Die meisten reformierten Kirchen haben eine presbyterianische Struktur, in der die Leitung der Kirche nicht bei einem Bischof, sondern bei einem Gremium von Ältesten liegt, das als Presbyterium, Synode oder Generalversammlung bezeichnet werden kann. Diese Ältesten sind in der Regel nicht ordiniert; ihr Amt wird jedoch als geistliches Amt gesehen, und in manchen Kirchen gibt es eine spezielle Ordination für Älteste.

Die Ältesten beschränken sich jedoch in der Regel im Gegensatz zu Bischöfen auf leitende Funktionen, die Sakramente werden von ordinierten Pfarrern verwaltet – bei den Ältesten liegt jedoch die Verantwortung, die Kirche gemäß der Tradition zu führen, die in episkopalen Konfessionen beim Bischof liegt.

Ausnahmen von dieser Regel finden sich heute in der Evangelisch-reformierten Kirche in Polen, der Reformierten Kirche in Ungarn und den weiteren ungarischen reformierten Kirchen Osteuropas (Rumänien, Serbien, Slowakei, Ukraine), bei denen, ähnlich wie in den meisten lutherischen Kirchen, Bischöfe den Dienst der personalen Aufsicht wahrnehmen und mit den Synoden gemeinsam die Kirche leiten. Auch die im 16. Jahrhundert aus der Reformierten Kirche in Ungarn heraus entstandene Unitarische Kirche (existiert heute in Ungarn und Siebenbürgen) kennt das Bischofsamt, da sie die Kirchenorganisation der Reformierten Kirche übernommen hat.

In den reformierten Landeskirchen in Deutschland heißt die oberste kirchenleitende Person General- oder Landessuperintendent (Lippische Landeskirche), Kirchenpräsident (Evangelisch-reformierte Kirche in Bayern und Nordwestdeutschland) oder Präsident beziehungsweise Schriftführer (Bremische Evangelische Kirche), in unierten Landeskirchen Präses (Rheinland, Westfalen) oder Kirchenpräsident (Evangelische Kirche der Pfalz, Evangelische Kirche in Hessen und Nassau). Die reformierten Kirchen der Schweiz sind presbyterianisch organisiert und kennen keine Bischöfe.

Die anglikanische Kirche kennt ebenfalls die sakramentale Bischofsweihe und eine bischöfliche Hierarchie mit Primas, Erzbischof, Bischof, und Assistenzbischof. Der Erzbischof von Canterbury, der gleichzeitig Oberhirte der Kirche von England ist, wird auch als primus inter pares der Weltkirche angesehen. Allerdings ist der Erzbischof von Canterbury gegenüber anderen Nationalkirchen nicht weisungsberechtigt. Anglikanische Bischöfe stehen nach vorherrschender Meinung ebenfalls in der apostolischen Sukzession (wobei dies jedoch von der römisch-katholischen Kirche bestritten wird).

Für Anglikaner ist die Diözese die wesentliche Einheit der Kirche. Diözesen sind zu Provinzkirchen zusammengeschlossen, die entweder mit dem Territorium eines Teils eines Nationalstaates, dem Territorium eines einzigen Nationalstaats oder mit den Territorien mehrerer Nationalstaaten übereinstimmen. Die Bischöfe einer Provinzkirche sind zu einer Bischofssynode zusammengeschlossen, die je nach Provinzkirche unterschiedliche Befugnisse und Aufgaben hat. Ein anglikanischer Bischof darf in einer anderen Diözese nur mit Zustimmung des Ortsbischofs tätig werden.

Anglikanische Bischöfe sind oft verheiratet, in vielen anglikanischen Kirchen (darunter seit 2014 in der Church of England) kann auch eine Frau zum Bischof geweiht werden (vgl. Anglikanische Gemeinschaft#Frauenordination). Die Bischofswahl erfolgt nach den Statuten der betreffenden Kirche, gewöhnlich durch ein Gremium von Priestern und Laien.

Der Beginn der methodistischen Bewegung liegt innerhalb der anglikanischen Kirche, deren Bischöfe in der apostolischen Sukzession stehen. Die ersten Methodisten nahmen daher die Sakramente der Anglikanischen Kirche in Anspruch.

Mit der Unabhängigkeitserklärung der Vereinigten Staaten kam für die Methodisten in den Staaten eine Zeit, in der es keine anglikanischen Bischöfe in erreichbarer Nähe gab. Zurückgehend auf die orthodoxe Tradition, beispielsweise im Patriarchat von Alexandria im dritten Jahrhundert, als die Presbyter einen der ihren zum Bischof wählten, definierte John Wesley das methodistische Verständnis vom Bischofsamt: zwischen einem Bischof und einem Ältesten (Presbyter, Pfarrer) gibt es keinen Unterschied im Weihegrad, sondern nur einen Unterschied in der Funktion: ein Bischof ist ein Presbyter, der eine leitende Funktion gegenüber den Presbytern seiner Region hat. Daher kann das Bischofsamt in einer methodistischen Kirche zeitlich begrenzt sein, und der Bischof ist nach Ablauf seiner Amtszeit wieder ein Presbyter wie jeder andere, leitet beispielsweise eine Gemeinde. Es gibt allerdings auch lokale Kirchenordnungen, in denen die Wahl eines Bischofs auf Lebenszeit möglich ist. Die ersten Bischöfe der methodistischen Kirche wurden von John Wesley und einigen anderen ordinierten Geistlichen der anglikanischen Kirche gewählt. In der methodistischen Tradition gibt es also keine apostolische Sukzession des Bischofsamts.

Das Bischofsamt in der evangelisch-methodistischen Kirche in Europa ist in vielen Fällen länderübergreifend: der nordeuropäische Sprengel umfasst beispielsweise die skandinavischen und baltischen Länder, der südosteuropäische Frankreich, Mitteleuropa ohne Deutschland, den Balkan und Nordafrika. Deutschland musste aus politischen Gründen in den Dreißigerjahren des letzten Jahrhunderts ein separater Sprengel werden und ist bis heute ein eigener Sprengel geblieben.

Kongregationalistisch strukturierte Konfessionen, beispielsweise die Baptisten und viele Pfingstgemeinden, verfügen nur in seltenen Fällen über ein übergemeindliches Bischofsamt. Ausnahmen sind zum Beispiel die Baptisten in Lettland, Georgien und in der Demokratischen Republik Kongo. Kongregationalistische Kirchengemeinschaften betonen die Autonomie der Ortsgemeinden und halten die Begriffe Bischof und Ältester für synonym. Die meisten dieser Gemeinden kennen jedoch unter verschiedenen Bezeichnungen die Funktionen des dreifachen Amtes auf Gemeindeebene: es gibt einen Gemeindeleiter (episkopos), ein Gremium von Ältesten (presbyteroi) und diakonische Funktionen. Sie begründen das unter anderem mit Hinweis auf Apostelgeschichte (Abschiedsrede des Paulus vor den Ältesten der Gemeinde Ephesus; siehe besonders die Verse 17 und 28). Dass das Bischofsamt ursprünglich eine Funktion der Ortsgemeinde war, wird ihres Erachtens auch an der alten katholischen Praxis deutlich, den Bischofstitel mit einem Ortsnamen zu verbinden.

Die Neuapostolische Kirche (NAK) kennt drei Amtsklassen: Diakone, Priester und Apostel. Die Apostel, im Apostolat zusammengefasst mit dem Stammapostel als Haupt, bilden die höchste Ämterhierarchie.

Unter den priesterlichen Ämtern ist die Amtsstufe des Bischofs die höchste. Bischöfe werden in der Regel, wie auch die Apostel, direkt durch den Stammapostel ordiniert. Sie unterstützen ihren Apostel teils in ehrenamtlicher Tätigkeit, teils auch im festen Dienst der Kirche. Die priesterlichen Ämter in der NAK führen Gottesdienste durch, spenden das Sakrament der Heiligen Wassertaufe und das Sakrament des Heiligen Abendmahls, nehmen neue Mitglieder in die Kirche auf, segnen die Kirchenmitglieder zu Konfirmationen, Verlobungen, Trauungen, Hochzeitsjubiläen und führen Trauerfeiern durch. Das Bischofsamt wird in dem 2012 erschienenen Katechismus der Neuapostolischen Kirche auch so beschrieben:

Aufgrund der Geschichte der Vereinigung Apostolischer Gemeinden und ihrer Ursprünge in katholisch-apostolischer und neuapostolischer Tradition kennen auch diese Gemeinschaften eine Dreiteilung des ordinierten Dienstes in: Apostel, priesterliche Ämter (Priester, Hirte, Evangelist, Ältester und Bischof) und Diakonat. Unter den priesterlichen Ämtern gibt es jedoch keine Rangordnung. Es gibt die drei charismatischen Ämter Priester, Hirte und Evangelist und die Leitungsämter Ältester und Bischof. Die Bischöfe sind die nächsten Mitarbeiter der Apostel. Sie stehen den Aposteln bei der geistlichen und organisatorischen Leitung zur Seite, was sich auch darin ausdrückt, dass sie seit einigen Jahren an den Apostelkonferenzen teilnehmen. Die Bischöfe tragen in der Regel die Verantwortung für mehrere Ältestenbezirke, die wiederum aus einzelnen Gemeinden bestehen. Das Ordinationsrecht hatten in der VAG bis vor einigen Jahren nicht die Bischöfe, sondern die Apostel. Dies wurde inzwischen geändert und auch Bischöfe haben das Ordinationsrecht.

In der deutschen Apostolischen Gemeinschaft bilden die Apostel, Bischöfe und Ältesten den satzungsgemäßen Vorstand, der sich gegenüber der Delegiertenversammlung, die von den Mitgliedern gewählt wird, verantworten muss. Ähnliches gilt für die Vereinigung Apostolischer Christen in der Schweiz. In Frankreich gibt es, aufgrund der sehr geringen Größe der Gemeinschaft, keinen Bischof, die niederländischen Gemeinden werden seit dem Ruhestand des Apostels Den Haan am 18. März 2012 von dem neu ordinierten Bischof Bert Wolthuis geleitet, der die Gemeinschaft auch in der Apostel- und Bischofsversammlung der europäischen Gemeinschaften vertritt.

Seit 2003 ist in der europäischen VAG die Frauenordination für alle Ämter möglich und z. T. umgesetzt, d. h. dass auch das Bischofsamt von beiden Geschlechtern ausgeübt werden kann.

Von den Bischöfen – : 

Von den Ältesten und Bischöfen – :

Das Bischofsamt in der altchristlichen Kirchenordnung (2. Jh.) – Didache 15,1-2:





</doc>
<doc id="12116" url="https://de.wikipedia.org/wiki?curid=12116" title="John Wyclif">
John Wyclif

John Wyclif [], auch "Wicklyf, Wicliffe, Wiclef, Wycliff, Wycliffe," genannt "Doctor evangelicus" (* spätestens 1330 in Hipswell, Yorkshire; † 31. Dezember 1384 in Lutterworth, Leicestershire), war ein englischer Philosoph, Theologe und Kirchenreformer.

Im Mai 1361 wurde Wyclif Rektor der Pfründe Fillingham (Lincolnshire), die zu Balliol gehörte. Diese und andere Pfründen ermöglichten Wyclif die Finanzierung seiner Studien; 1363 wurde er zum Studium der Theologie zugelassen. Er wirkte als Vorstand des Balliol College in Oxford und war 1365–1367 Vorsteher des neuen College Canterbury-Hall. Nach seiner Absetzung kam es zum inneren Bruch mit der Kirche, und Wyclif wandte sich der Politik in London zu. Während er als Doktor der Theologie das Recht hatte, theologische Vorlesungen zu halten, war er zugleich Pfarrer in Stellen, die von weltlichen Fürsten vergeben wurden, ein Amt, das er bis zu seinem Tod versah. Auf die Pfarrei Fillingham folgten 1368–1374 Ludgershall (Buckinghamshire) und schließlich 1374–1384 die reiche Gemeinde in Lutterworth (Leicestershire), die er als Dank für seine Dienste für die Krone vom späteren englischen Regenten Johann von Gent erhielt.
Wyclif proklamierte die Lehre von der „Macht allein durch Gnade“, der zufolge Gott selbst jede Autorität direkt verleiht, bestritt den politischen Machtanspruch des Papstes und propagierte ein frühes „König-Gottes-Gnadentum“. In seinen Werken von 1372 bis 1380 ("Von der Kirche", "Von der bürgerlichen Herrschaft" und "Vom Amt des Königs") vertrat er die völlige Unterordnung der Kirche unter den Staat. Er unterstützte den Machtwillen der weltlichen Herrscher (Investiturstreit) in mehreren Prozessen gegen den Papst und forderte für Kirchenmitarbeiter ein Leben in urchristlicher Bescheidenheit, obwohl er selbst bis zu seinem Tod von seiner reichen Pfründe gut lebte.

Im Jahr 1373 sandte ihn König Eduard III. mit anderen Geistlichen nach Brügge, um dem päpstlichen Nuntius Beschwerden gegen den päpstlichen Stuhl vorzutragen, insbesondere wurde der Kurie der Verkauf von Kirchenämtern vorgeworfen. Die „Beschwerden“ dienten dazu, die seit 33 Jahren ausstehenden vertraglich vereinbarten jährlichen Zahlungen nach Rom weiter aussetzen zu können. Wyclifs Anliegen drang 1375 durch. Als offizieller Ankläger im Namen des Königs gab sich Wyclif selbst nun den Titel „Pecularius regis clericus“ (Königlicher Kaplan).

Sein juristisch-theologischer und politischer Einfluss auf die Zusammenstellung königlicher Beschwerden gegen den Papst, die 1376 das Gute Parlament vortrug, war groß. Ein Prozess gegen den Papst, den Wycliff 1370 allein noch verloren hatte, wurde 1373–1375 von jenem über die ausstehenden Zahlungen gekrönt, in dem er sich durchsetzte, und mündete 1377 in einen Prozess, den der Papst gegen Sentenzen aus Wyclifs Werken führte, der dank des großen Ansehens, das Wyclif an der Universität und im Volk genoss, 1378 im Sande verlief. Dadurch ermutigt, wandte sich Wyclif nun offen gegen den politischen Einfluss des Klerus überhaupt und bekämpfte das päpstliche „Antichristentum“.

In seinem Hauptwerk, dem "Trialogus," lehrte Wyclif pantheistischen Realismus, Determinismus und die doppelte Prädestination ("determinatio gemina"). Er lehrte: „Alles ist Gott; jedes Wesen ist überall, da jedes Wesen Gott ist.“ und „Alles, was geschieht, geschieht mit absoluter Notwendigkeit, auch das Böse geschieht mit Notwendigkeit, und Gottes Freiheit besteht darin, daß er das Notwendige will.“ Er missbilligte folglich Bilder-, Heiligen-, Reliquienverehrung und den Priesterzölibat, verwarf aufgrund seines Realismus die Transsubstantiationslehre und die Ohrenbeichte. Von ihm ausgebildete rötlich gekleidete Reiseprediger („arme Priester“ genannt) verbreiteten Grundsätze im Volk, die an protestantische Lehren 150 Jahre später erinnern. Seine Lehren fanden in großen Teilen der Bevölkerung Zustimmung und beeinflussten maßgeblich den Aufstand der englischen Bauern von 1381.

Bettelmönche im Verein mit der Hierarchie setzten 1381 unterdessen die Verwerfung seiner Lehre durch die Universität und die 1382 in London tagende Synode durch. Seine Schriften wurden von der Synode in Oxford als ketzerisch verurteilt, er verlor seine Ämter am Hof in Bezug auf die Kirchenangelegenheiten. Aus Furcht vor einem Volksaufstand wurde Wyclif aber nicht offiziell angeklagt. Er führte in aller Ruhe sein Pfarramt fort und vollendete 1383 eine früher begonnene Sammlung früher englischer Bibelübersetzungen aus der Vulgata in die Landessprache. Diese Bibelübersetzung ist nicht die erste Übersetzung ins Englische, sondern stellt eine Zusammenstellung und Überarbeitung früherer Übersetzungen dar, wie schon Thomas Morus 1530 feststellte und Francis Aidan Gasquet OSB 1897 nachwies.

Wyclif starb 1384 an den Folgen eines Schlaganfalls während der Messe in Lutterworth (heute Harborough district, Leicestershire).

Die späteren Anhänger Wyclif’schen Gedankengutes, die Lollarden, wurden erst nach einer missglückten Revolte ab 1400 vom englischen Staat scharf verfolgt. 1401 wurde William Sawtrey ihr erster Märtyrer. Jedoch kann man die teilweise brutale Inquisition europäischer Ketzer, wie zum Beispiel bei den Katharern oder Waldensern, nicht mit der englischen Verfolgung vergleichen. Diese war durch ihre relative Milde und Rücksicht auf die im Untergrund weiter lebenden Lollarden geprägt, so dass sich in vielen Familien die Wyclif’schen Ansichten bis zur Reformation erhielten.

Im Jahr 1412, am Ende der Verfolgung durch den englischen König, wurden 267 Sentenzen von Wyclif in London als häretisch verurteilt. Drei Jahre später bestimmte das Konzil von Konstanz, alle Schriften Wyclifs zu verbrennen, und erklärte ihn 30 Jahre nach seinem Tod am 4. Mai 1415 zum Ketzer, verdammte weitere 45 Sentenzen von ihm und befahl, seine Gebeine auszugraben und zu verbrennen, was dreizehn Jahre später, 1428, durch Bischof Richard Fleming von Lincoln tatsächlich geschah.

Zu Ehren John Wyclifs hat sich eine nichtkommerzielle evangelikale Organisation, die sich für die weltweite Verbreitung der Bibel durch Erarbeitung von Bibelübersetzungen vor allem für Sprachgruppen einsetzt, die bisher noch nicht schriftlich fixiert sind, 1942 den Namen Wycliff gegeben.

Wyclifs Leben wurde 1984 von Tony Tew unter dem Titel "John Wycliffe" verfilmt.

In Berlin-Moabit ist die Wiclefstraße nach ihm benannt.



Am 31. Dezember erinnern die Evangelische Kirche in Deutschland und die Anglikanische Gemeinschaft an John Wyclif.





</doc>
<doc id="12117" url="https://de.wikipedia.org/wiki?curid=12117" title="Diözese">
Diözese

Eine Diözese, auch Bistum genannt, ist ein territorial abgegrenzter kirchlicher Verwaltungsbezirk. Die Bezeichnung Diözese leitet sich von der Untergliederung des spätantiken Römischen Reiches in Diözesen her. Der Begriff Bistum (von Bischoftum) hingegen bezieht sich auf das Jurisdiktionsgebiet eines Bischofs. Alte Bezeichnungen dafür sind Sprengel oder Kirch(en)sprengel.

Der Begriff "Diözese" ( "dioikesis" ‚Verwaltung‘) bezeichnete ursprünglich die staatliche Finanzverwaltung im alten Rom und wurde von Kaiser Diokletian (284–305) aufgegriffen, als er das Reich neu untergliederte. Diese Untergliederung wurde auch in der folgenden Spätantike beibehalten.

Die Regionaleinteilung Diokletians wurde im 4. Jahrhundert von der alten Kirche für die geographische Strukturierung ihrer Einflusssphäre übernommen. Während die orthodoxen Kirchen für die Bezeichnung übergeordneter Struktureinheiten dabei bis heute den Begriff Eparchie verwenden, kam im katholischen Westen ab dem 13. Jahrhundert der Begriff "Diözese" allgemein in Gebrauch, im deutschsprachigen Raum auch der Begriff "Bistum". Diese Form des Kirchenaufbaus wird heute außer in der katholischen Kirche noch in verschiedenen anderen Kirchen verwendet, z. B. in der orthodoxen Kirche, der anglikanischen Kirche, der methodistischen Kirche, altkatholischen Kirche und Teilen des Luthertums.

In der römisch-katholischen Kirche ist eine Diözese eine in der Regel territorial abgegrenzte Körperschaft. Konstitutiv für eine Diözese sind nach dem Dekret Christus Dominus des Zweiten Vatikanischen Konzils der Bischof, Presbyterium und Gottesvolk. Zudem ist eine Diözese in der Regel an einen Ritus gebunden.

Neben territorialen kann es auch personal umschriebene Diözesen geben. Dazu gehören die Teilkirchen für die Gläubigen eines anderen Ritus auf dem Gebiet einer oder mehrerer lateinischer Teilkirchen oder die Militärordinariate.

Derzeit gibt es in der römisch-katholischen Kirche 2.945 Diözesen. Jede Diözese gilt zugleich als Partikularkirche der römisch-katholischen Kirche. Für die Errichtung neuer Diözesen ist der Papst zuständig. Eine Diözese muss in Pfarreien untergliedert sein, die zu Dekanaten zusammengefasst werden können.

Die Errichtung, Umschreibung und Aufhebung von Diözesen ist in der Regel dem Apostolischen Stuhl vorbehalten (). Eine Ausnahme hiervon macht lediglich das Kanonische Recht der Orientalischen Kirchen (CCEO), das dem Patriarchen und der jeweiligen Synode gewisse Rechte bei der Errichtung, Neuumschreibung und Aufhebung von Diözesen zuweist. In diesem Fall ist der Apostolische Stuhl jedoch zumindest zu konsultieren.

Bei der Errichtung, Umschreibung oder Aufhebung von Diözesen ist die betreffende Bischofskonferenz zu hören (). Zudem kann es, wie etwa in Deutschland, aufgrund von Verträgen notwendig sein, für die Errichtung oder Umschreibung von Diözesen Vereinbarungen mit den betreffenden Staaten zu treffen.

Jede Diözese ist in Pfarreien zu untergliedern (). Dies gilt auch für die Personaldiözesen. In Militärordinariaten findet eine entsprechende Gliederung statt, die von Fall zu Fall verschieden sein kann. Angehörige des Militärordinariates sind rechtlich nicht vollständig aus ihrer Diözese ausgegliedert, wie es auch die Angehörigen der gegenwärtig bestehenden Personalprälatur, des Opus Dei, nicht sind.

Vorsteher einer Diözese ist der Bischof. Dieser besitzt volle Jurisdiktion und wird daher auch, zur Unterscheidung vom Titularbischof (Weihbischof), Diözesanbischof genannt. Er ist zur Residenz in seiner Diözese verpflichtet ().

Vertreter des Bischofs ist der Generalvikar, dem die ausführende Gewalt zukommt, die auch der Bischof innehat. Keinen Anteil dagegen hat er an der gesetzgeberischen Gewalt des Diözesanbischofs. Zudem soll er nicht an der richterlichen Gewalt teilhaben. Die Ernennung eines Generalvikars durch den Diözesanbischof ist verpflichtend ().

Neben dem Generalvikar können durch den Diözesanbischof Bischofsvikare bestellt werden, denen für einen bestimmten Aufgabenbereich die Kompetenzen des Generalvikars zukommen.

Der Vertreter des Bischofs in der kirchlichen Gerichtsbarkeit ist der Offizial und in der Priesterausbildung der Regens. Einigen Bischöfen ist zur Unterstützung in der Weihegewalt ein Weihbischof beigegeben, der jedoch, obwohl in der Weihe voll und ganz Bischof, in allem vom Diözesanbischof abhängig ist. In Ausnahmefällen kann vom Apostolischen Stuhl eine Visitation durch einen Apostolischen Visitator angeordnet werden.

Eine Diözese ist gewöhnlich mit anderen Diözesen zu einer Kirchenprovinz zusammengeschlossen. Der Vorsteher einer Kirchenprovinz trägt den Titel "Metropolit". Dieser ist selber Diözesanbischof einer Diözese der Kirchenprovinz, die als "Erzdiözese" oder "Erzbistum" bezeichnet wird. Es gibt jedoch auch Diözesen, die keiner Kirchenprovinz angehören und direkt dem Apostolischen Stuhl (Papst) unterstehen, so zum Beispiel die Diözesen in der Schweiz, das Erzbistum Vaduz und das Erzbistum Straßburg. Sie werden "exemte" oder "immediate" Diözesen genannt.

Eine Erzdiözese ist rechtlich von der Diözese nicht verschieden. Der Name zeigt eine historische Bedeutung oder den Sitz eines Metropoliten an. Die Erzdiözese bildet in letzterem Fall zusammen mit weiteren Diözesen, den Suffragandiözesen, die Kirchenprovinz. In seltenen Fällen kann eine Erzdiözese auch Suffragan einer weiteren Erzdiözese sein. So untersteht z. B. die Erzdiözese Aix dem Metropoliten in Marseille.

Üblicherweise halten sich die Diözesangrenzen der römisch-katholischen Kirche an politische Grenzen. Entsprechend bilden die Bischöfe eines Landes eine Bischofskonferenz. Nur in wenigen Fällen wie in der Karibik oder im Nahen Osten erstreckt sich eine Diözese über mehrere Länder. In Deutschland gab es zur Zeit der deutschen Teilung einige Bistümer, die west- wie ostdeutsche Gebiete umfassten.

Die Größe der Diözesen ist von Land zu Land verschieden, im Allgemeinen sind die Bistümer in den altchristlichen Gebieten der Mittelmeerländer sowohl von der Fläche als auch von der Bevölkerungszahl her wesentlich kleiner als in später christianisierten Gebieten wie Deutschland.

Faktisch im Range einer Diözese stehen auch die Jurisdiktionsbezirke in Missionsgebieten, wie Mission sui juris, Apostolische Präfektur, Apostolisches Vikariat und Apostolische Administratur. Gleiches gilt für die Territorialabteien und Territorialprälaturen. Die Vorsteher dieser Teilkirchen sind den Diözesanbischöfen rechtlich gleichgestellt. Siehe auch: Partikularkirche.

In der römisch-katholischen Kirche in Deutschland gibt es derzeit 27 Bistümer (7 Erzbistümer und 20 Bistümer).

In Österreich gibt es zwei Erzdiözesen, die die Kirchenprovinzen darstellen, und sieben territoriale Diözesen (Suffragandiözesen), weiters eine Militärdiözese und eine Territorialabtei (Immediat, direkt dem Heiligen Stuhl unterstellt). Die Grenzen der Diözesen entsprechen dabei weitgehend denen der österreichischen Bundesländer, die Erzdiözese Wien umfasst aber neben der Stadt selbst auch einen Teil Niederösterreichs, und der Osten Nordtirols gehört zur Erzdiözese Salzburg.


"Siehe auch:" Österreichische Bischofskonferenz

In der Schweiz gibt es keine Erzdiözesen mit einer Kirchenprovinz. Die Aufgaben einer Erzdiözese übernimmt direkt die römische Kurie. Dies ist in der römisch-katholischen Kirche eine Besonderheit, welche schon verschiedentlich in der Diskussion stand.

Es bestehen sechs Diözesen (üblicherweise als Bistümer bezeichnet):

Weiter gibt es die zwei Territorialabteien Abtei Saint-Maurice und Abtei Maria Einsiedeln.

Das Fürstentum Liechtenstein bildet das Erzbistum Vaduz, das direkt der römischen Kurie unterstellt ist.

Das Großherzogtum Luxemburg bildet das Erzbistum Luxemburg.

Die katholische Kirche von Belgien besteht aus einer Kirchenprovinz mit acht Bistümern. Der Erzbischof ist zugleich Primas.

Übersicht über alle katholischen Diözesen weltweit siehe: Liste der römisch-katholischen Diözesen.

Die Orthodoxe Kirche kennt unterschiedliche Verwaltungssysteme, die im Großen und Ganzen dem der Römisch-katholischen Kirche ähneln und in den einzelnen orthodoxen Landeskirchen unterschiedlich ausgestaltet sind. In der Regel findet das griechische Wort Eparchie oder Metropolie Anwendung, wobei darunter nicht die Römisch-katholische Kirchenprovinz zu verstehen ist. Das in der alten Kirche gängige Metropolitansystem (die Untergliederung eines Verbandes von Diözesen bzw. Eparchien unter einen Metropoliten), ähnlich der Kirchenprovinz, kennt heute nur noch die rumänisch-orthodoxe Kirche. In den griechisch-orthodoxen Kirchen trägt nahezu jeder Diözesanbischof den Titel eines Metropoliten und das Oberhaupt einer Lokalkirche, sofern es kein Patriarch ist, den eines Erzbischofes, in einzelnen Fällen aber auch den eines Metropoliten.

In der evangelischen Kirche wurde bis in das 19. Jahrhundert eine Verwaltungseinheit mehrerer Einzelgemeinden innerhalb einer Landeskirche gelegentlich als "Diözese" bezeichnet (z. B. Diözesen der Kirchenprovinz Ostpreußen oder Generaldiözesen der hannoverschen Landeskirche). Der Begriff ist in der evangelischen Kirche in Deutschland nicht mehr gebräuchlich.

Die evangelische Kirche A.B. in Österreich ist in sieben Diözesen gegliedert, wobei "Diözese" – auch in der Kirchenverfassung – als Alternativbezeichnung zu "Superintendenz" verwendet wird. Auch die Evangelisch-Augsburgische Kirche in Polen ist in Diözesen untergliedert. Bei den lutherischen Landeskirchen Skandinaviens heißt das Sprengel eines Bischofs "Stift", im Dänischen, Norwegischen und Schwedischen die Vokabel für Bistum.

Die Gliedkirchen der Anglikanischen Gemeinschaft sind ihrerseits ebenfalls in Bistümern organisiert.

Siehe z. B. Erzbischof von Canterbury, Liste der Diözesen der Episkopalkirche der Vereinigten Staaten von Amerika, sowie die einzelnen Artikel zu den Gliedkirchen (Querverweis in der Navigationsleiste zum Artikel über die Anglikanische Gemeinschaft).





</doc>
<doc id="12120" url="https://de.wikipedia.org/wiki?curid=12120" title="Vesper">
Vesper

Vesper (von althochdeutsch "vespera", mittelhochdeutsch "vesper", aus lateinisch "vespera", „Abend(zeit)“) bezeichnet:

Orte in den Vereinigten Staaten:

Personen:
Siehe auch:


</doc>
<doc id="12122" url="https://de.wikipedia.org/wiki?curid=12122" title="Franziskanische Orden">
Franziskanische Orden

Franziskanische Orden sind verschiedene vornehmlich römisch-katholische Ordensgemeinschaften, die sich an der von Franziskus von Assisi (1181/1182 bis 1226) für den von ihm gegründeten Bettelorden verfassten Ordensregel orientieren.

Zu den bedeutendsten franziskanischen Theologen und Philosophen des 13. und 14. Jahrhunderts gehörten Antonius von Padua, Alexander von Hales, Bonaventura von Bagnoregio, Roger Bacon, Johannes Duns Scotus und Wilhelm von Ockham. Der Franziskaner-Publizist Thomas Murner war Martin Luthers wortgewandtester Gegner in der Reformationszeit.

Die Gemeinschaften, die sich auf den heiligen Franziskus berufen, teilen sich in drei Gruppen.

Auch in anderen Konfessionen gibt es franziskanische Gemeinschaften, so anglikanische Franziskaner und evangelische Terziaren.

Heute betrachten sich alle genannten Gemeinschaften als Äste der franziskanischen Familie; im deutschsprachigen Raum haben sie sich in der INFAG ("Interfranziskanische Arbeitsgemeinschaft") organisiert und treten seit 2005 unter dem logo "clara.francesco" ("Ökumenisch-geschwisterliche Netzwerkinitiative franziskanischer Orden") etwa bei Kirchentagen, beim Weltjugendtag oder bei der Europäischen Ökumenischen Versammlung auf.

Franziskus von Assisi hatte zunächst nicht den Plan, einen Orden zu gründen; er wollte lediglich in vollständiger Nachfolge Christi leben gemäß dem Gebot des Evangeliums:

Als charismatische Persönlichkeit fand er jedoch begeisterte Gefährten, vor allem unter seinen zahlreichen Freunden. Der schließlich von Franziskus gegründete "Orden der Minderen Brüder" wurde um 1210 von Papst Innozenz III. bestätigt. Die Gemeinschaft lebte auf dem Gelände der kleinen Kirche Portiunkula unterhalb von Assisi. 1212 gelobte hier Klara von Assisi, die 1216/1217 das Regelwerk des 2. Ordens schrieb, ein Leben nach den Evangelischen Räten.
Der Orden der Minderen Brüder entwickelte sich zu einem der vier großen Bettelorden des Mittelalters. Mit der Zeit stellte sich allerdings heraus, dass zu große Freiheit zu Verweltlichung und Auflösung führte. Deshalb wurden die Brüder nach und nach immer fester organisiert und die Ordensregeln stärker an den „klassischen“ benediktinischen Regeln ausgerichtet. Schon zu Lebzeiten von Franziskus von Assisi machte der Orden den Schritt von einer Wanderbewegung zur Sesshaftigkeit; Franziskus selbst geht in seinem "Testament" am Ende seines Lebens davon aus, dass die Brüder „Kirchen und ärmliche Wohnungen“ haben. 1212 wurde das erste Franziskanerkloster in der Toskana gegründet: der "Convento di San Francesco" bei Cetona, derzeit als "Frateria di Padre Eligio" in Verwendung. Schon zwischen 1215 und 1217 fasste der Orden auch außerhalb Italiens Fuß; frühe Chroniken berichten über die – zunächst erfolglose – Ausbreitung nach Deutschland und England. Ab 1217 teilte man den Orden in Provinzen und Kustodien ein. Obere werden als "ministri" (von lat. "minister" „Diener“) und "custodes" (lat. "custos" „Hüter“) bezeichnet, da Franziskus die in anderen Orden übliche Bezeichnung Prior (von lat. "prior" „vorderer, bevorzugter“) ausdrücklich abgelehnt hatte zugunsten des Dienstcharakters des Oberenamtes.

Alle Brüder trafen sich jährlich zu sogenannten Ordenskapiteln, um miteinander zu diskutieren und Beschlüsse zu fassen. Mit dem schnellen Anwachsen der Zahl der Brüder und der räumlichen Ausdehnung wurde jedoch schon bald die Beschränkung der Kapitelsteilnehmer auf „Kapitulare“ üblich, und eine gewisse Hierarchisierung setzte ein. Für größere Reisen musste der Obere eine förmliche Erlaubnis erteilen, ab etwa 1225 sind Versetzungsschreiben von einer Kustodie in eine andere sowie Ernennungsschreiben für Oberenämter bekannt. Von England her setzte sich ab etwa 1230 die Praxis durch, Oberenämter mit Priestern zu besetzen.

Das Generalkapitel 1219 beschloss, auch heidnische Länder zu besuchen und die Heiden zu missionieren. Franziskus selbst zog in die "Kustodie des Heiligen Landes", die damals den gesamten südöstlichen Mittelmeerraum umspannte. Er predigte und versuchte unter anderem, den Sultan zu bekehren. Diese Missionsversuche in Damiette und später in Jerusalem blieben aber ohne Erfolg. Nach seiner Rückkehr 1220 aufgrund von Nachrichten über Streitigkeiten bestimmte er Pietro Catanii zum Leiter des Ordens. 1221 starb Bruder Pietro, und Elias von Cortona übernahm die Ordensleitung.

Auf Anweisung der römischen Kurie verfasste Franziskus 1223 in der Einsiedelei Fonte Colombo eine dritte Fassung der Ordensregel. Sie wurde im Juni auf dem Generalkapitel behandelt und am 29. November von Papst Honorius III. mit der Bulle "Solet annuere" bestätigt (darum „Bullierte Regel“). 1226 starb der Ordensgründer.

Franziskus hatte die Bestellung von Brüdern für höhere kirchliche Ämter abgelehnt; kein Bruder dürfe, so die nicht bestätigte Regel 1221, ein leitendes Amt in dem Haus versehen, in dem er diene. Die Aufforderung von Kardinal Hugolin von Ostia, Bischöfe aus dem Kreis seiner Brüder vorzuschlagen, wies er zurück mit der Begründung: „Mindere sind meine Brüder deswegen genannt, damit sie sich nicht herausnehmen, Höhere zu werden. Lasset sie daher unter keinen Umständen zu kirchlichen Ämtern emporsteigen, damit sie nicht umso stolzer werden, je ärmer sie sind, und gegen die übrigen sich überheblich zeigen.“ Im Zuge der Klerikalisierung des Ordens wurden jedoch bereits ab der Mitte des 13. Jahrhunderts Ernennungen von Minderbrüdern zu Bischöfen üblich, einige wurden zu Kardinälen erhoben wie etwa der Ordensgeneral Bonaventura 1273. Der erste Papst aus dem Franziskanerorden war Girolamo Masci d’Ascoli als Nikolaus IV. (Papst) (1288–1292).

Die erste Niederlassung in Deutschland gründeten die Franziskaner 1221 in Augsburg, nachdem eine Reise 1217 erfolglos geblieben war, weil man die Brüder für Ketzer (Katharer) gehalten und vertrieben hatte. Die Brüder wurden auf dem großen Mattenkapitel an Pfingsten 1221 an der Portiuncula-Kapelle in Assisi ausgesandt, und am 16. Oktober 1221 fand in Augsburg ein erstes Kapitel der nach Deutschland gekommenen Brüder statt. Im selben Jahr wurden von dort aus Niederlassungen in Würzburg und Regensburg gegründet, am 30. November brachen die Franziskaner nach Köln auf, wo sie sich 1222 niederließen. 1225 wurden Bremen und Lübeck erreicht, 1230 Riga. Binnen weniger Jahre breitete sich der Orden im Reich bis zur Ostsee aus. Bei ihrer Ankunft in Deutschland war bereits die Lebensweise in eigenen Häusern üblich, die jedoch im Eigentum der bisherigen Besitzer blieben. Mancherorts kamen die Brüder in Spitälern oder in verlassenen Klöstern anderer Orden unter. Häufig wurde ihnen eine Kirche überlassen, mitunter bevor sie ein Wohnhaus an dem Ort hatten.

Die Franziskaner bevorzugten Bischofsstädte und ordneten sich dem dortigen Klerus wie auch den weltlichen Autoritäten unter, bei Wahrung ihrer Eigenständigkeit. Die zu der Zeit expandierenden Städte waren offen für die Zuwanderung armer, aber arbeitsfähiger Menschen; Geld- und Marktwirtschaft sowie bürgerliche Autonomiebestrebungen führten zu sozialen Spannungen. In dieser Situation bot die Lebensweise der neuen, päpstlich anerkannten Wanderprediger ohne „Klaustrum“, also ohne fest umgrenzten Klosterbezirk offenbar überzeugende soziale und religiöse Lösungen. Die Weigerung der Franziskaner, Besitz, Macht über andere und sozialen Aufstieg anzustreben, sind Ursachen für ihre große Verbreitung und Popularität, genauso wie ihre Zuwendung zu den Armen und Ausgegrenzten; in Speyer wohnten sie nach Angabe des Chronisten Jordan von Giano „außerhalb der Mauern bei den Aussätzigen“. Die Minderbrüder stellten eine „vom Evangelium Jesu Christi her gelebte Alternative zur herrschenden Wirtschaft und Gesellschaft, ja zur damals herrschenden Mentalität, Kultur und Religiösität“ dar und waren deshalb erfolgreich. Von Vorteil für ihre Expansion bis zur ersten Hälfte des 14. Jahrhunderts war, dass die Franziskaner an vielen Orten in Mitteleuropa von den Fürsten und Stadtoberen gefördert und zur Klostergründung ermuntert wurden. 

Die Kleidung der Franziskaner „sollte billig und schlicht sein. Sie bestand aus einem locker fallenden, groben braunen Wollgewand mit Kapuze, einem zweiten, kapuzenlosen Gewand sowie Hosen und Gürtelstrick“. Sie erwarben ihren Lebensunterhalt durch Ausübung eines gelernten Handwerks gegen Unterkunft und Nahrungsmittel; falls das nicht ausreichte, sollten die Brüder betteln, was damals nicht kriminalisiert war, sondern eine durchaus gängige Art des Broterwerbs darstellte. Der Zusammenhalt der Ordensmitglieder wurde durch die Gehorssamsstruktur gegenüber den Ordensoberen sowie durch regelmäßige Zusammenkünfte aller auf den Kapiteln gewährleistet.

Bereits frühzeitig begann zur fundierten Aus- und Weiterbildung für die Klerikerbrüder der Aufbau eines eigenen Studiensystems. 1228 wurde in Magdeburg ein Studium begründet, das zum Zentrum eines hierarchisch geordneten ordenseigenen Bildungswesens in Sachsen wurde und für den Orden wie für Wissen und Bildung des Spätmittelalters bedeutsam war. Ab 1395 war das Ordensstudium in Erfurt, das in dem Jahr in die Universität Erfurt inkorporiert wurde, das "„studium generale“" der Sächsischen Provinz. Im 15. Jahrhundert wurden die leitenden Positionen in Provinz und Konvent von akademisch gebildeten Franziskanern besetzt, die sowohl als Dozenten als auch als Ordensobere Erfahrungen besaßen.
Die franziskanische Frömmigkeit ist bestimmt von den Merkmalen, die bereits für die Ordensgründer Franziskus und Klara prägend waren und sich in den Kennzeichen Krippe, Kreuz und Eucharistie zusammenfassen lassen. Die Leben-Jesu-Frömmigkeit des Franziskus betrachtete das arme irdische Leben Jesu von seinem Anfang in der Krippe bis zum Ende am Kreuz und in seiner Bedeutsamkeit für die Erlösung der Menschen, ihre Elemente waren "imitatio" (Nachahmung) und "compassio" (Mitleiden), was seinen mystischen Ausdruck in der Stigmatisation fand, die Franziskus erfuhr. Auch das kontemplative Leben der Klara war geprägt von Passionsmystik.

Für Franziskus und Klara setzte sich das Erlösungswerk Jesu Christi in der Feier der heiligen Messe und der Verehrung der Eucharistie fort, die in ihrer Frömmigkeit einen hohen Stellenwert einnahm. Franziskus selber war nicht Priester, und im Franziskanerorden hat der Aspekt der brüderlichen Gemeinschaft heute ein größeres Gewicht als die priesterliche Berufung der einzelnen Brüder. Krippenfrömmigkeit und die Kreuzverehrung, etwa beim Kreuzweg, wurden durch Franziskaner und Klarissen weltweit gefördert und verbreitet.

Von großer Bedeutung ist eine Haltung des Friedens. Franziskus beruft sich auf göttliche Weisung: „Der Herr hat mir geoffenbart, dass wir als Gruß sagen sollen: 'Der Herr gebe dir den Frieden!'“ (Testament, 23). Der „wahre Friede“ ist der Frieden, den Gott gibt, aber er ist vom Frieden mit den Menschen nicht zu trennen, und er entspringt wesentlich aus der Begegnung mit den Armen; Franziskus selber hatte am Beginn seiner Berufung Aussätzige gepflegt. In Verbindung mit dem Armutsideal bedeutet das franziskanische Friedensverständnis den Verzicht auf Waffen und Gewalt sowie eine Haltung von Demut und Geduld gegenüber allen Menschen.

Auch der Selbstanspruch, ein bewusstes Leben mit der Schöpfung zu führen, hängt eng mit der Abkehr von irdischem Reichtum zusammen. Durch die Betonung dieses Aspektes erlangen die franziskanischen Orden seit Beginn der ökologischen Bewegung in den 1980er-Jahren ein verstärktes Ansehen. Papst Franziskus wählte 2015 für seine Enzyklika "Laudato si’. Über die Sorge für das gemeinsame Haus" zum Thema Umwelt- und Klimaschutz als Titel und Incipit den Anfang des Sonnengesangs des heiligen Franziskus.
Grundlegend für die franziskanische Spiritualität ist ein brüderliches Leben in einer evangeliumsgemäßen Lebensweise mit apostolischem, den Armen zugewandten Akzent. Die Aufgaben, die die Brüder übernehmen, erwachsen aus dieser Lebensweise und müssen mit ihr vereinbar sein. Daher sollen nach dem Willen des Franziskus Machtpositionen ausgeschlossen bleiben. In der von Papst Honorius III. am 29. November 1223 approbierten Ordensregel, der "Regula bullata", werden "ministri" (von lat. "minister" „Diener“) und "custodes" (lat. "custos" „Hüter“) als Obere genannt (Kap. 4 und 8). Franziskus selber lehnte die in anderen Orden übliche Bezeichnung Prior (von lat. "prior" „vorderer, bevorzugter“) ausdrücklich ab zugunsten des Dienstcharakters des Oberenamtes.

Von Anfang an gingen die Franziskaner hinaus, um das Christentum in nichtchristliche Länder zu bringen. Franziskus selber reiste als Missionar nach Palästina und predigte in Ägypten vor dem Sultan Al-Kamil. Franziskus verstand seinen Auftrag als Friedensdienst und nicht in erster Linie als „Heidenbekehrung“. Bis heute sind Franziskaner in Nordafrika und in Palästina präsent. Dem friedlichen Missionsverständnis entspricht es, dass sie ein vorbildliches christliches Leben unter Andersgläubigen führen. Im 13. und 14. Jahrhundert waren Franziskaner in der Mongolei tätig, so Johannes de Plano Carpini im Auftrag von Papst Innozenz IV., und sie gelangten auch nach China – als erster Johannes von Montecorvino. Mit Christoph Kolumbus kamen auch Franziskaner nach Amerika. Die Namen von Städten wie San Francisco und Los Angeles, entstanden im 18. Jahrhundert, zeigen den Einfluss der Franziskaner. Das System der "Conquista" stellten sie nicht grundlegend in Frage, und es fanden sich Franziskaner unter den Förderern wie unter den Gegnern der Kolonisation.

Seit dem 19. Jahrhundert sind deutsche Franziskaner in Brasilien tätig. Auch China war bis zum Zweiter Weltkrieg ein Schwerpunkt für das Engagement mehrerer deutscher Ordensprovinzen. Ab den 1980er-Jahren wurden sie auch in Afrika tätig. Inzwischen sind überall einheimische, unabhängige Franziskanerprovinzen entstanden, in denen einheimische und europäische Brüder in „brüderlicher Assistenz“ zusammenarbeiten.
Neben den Dominikanern beteiligten sich ab der ersten Hälfte des 13. Jahrhunderts unter anderem auch die Franziskaner an der Untersuchung und Verurteilung von Häretikern im Rahmen der Inquisition, vor allem in Italien, Frankreich und dem Heiligen Römischen Reich. Da Dominikaner und Franziskaner aus christlichen Laienbewegungen hervorgegangen waren und sich ebenso wie die zeitgenössischen häretischen Bewegungen an persönlicher Armut orientierten, eigneten sie sich nach damaliger Meinung besonders gut, um inhaltliche Unterschiede zwischen rechtgläubigen und häretischen Standpunkten aufzuzeigen.

Aussagen über die Juden oder Begegnungen mit Juden sind von Franziskus nicht bekannt. Wie die Dominikaner, so führten auch die Franziskaner Disputationen mit jüdischen Gelehrten und waren enttäuscht, wenn diese das Christentum weiterhin ablehnten. Judenfeindliche Predigten von Franziskanern wurden im 14. und 15. Jahrhundert mehrfach von Päpsten kritisiert. Namentlich Johannes Capistranus trat als fanatischer Promotor der Judenverfolgung auf. Insgesamt ist die Beziehung zwischen den Franziskanern und den Juden aber noch nicht wissenschaftlich aufgearbeitet. 

Es ist nicht zu übersehen, dass sich die „kritische Alternative“ des Anfangs der Minderbrüder in Deutschland durch Verbürgerlichung und Klerikalisierung veränderte. „Aus zeitweiligen Ansiedlungen [wurden] nun auf innerstädtische Seelsorge und Gottesdienstangebote ausgelegte und dauerhaft etablierte Franziskanerklöster, orientiert an monastischen Abteien und Kanonikerstiften.“ Für die erfolgreiche Predigt- und Seelsorgetätigkeit brauchte man große Kirchen, die auch vielerorts gebaut wurden, unterstützt von den Gönnern des Ordens. Es entwickelte sich der Typus der Bettelordenskirche, die nach den Ordensstatuten baulich zwar wenig aufwändig zu sein hatten - mit einem kleinen Dachreiter statt eines turmbewehrten Westwerks, mit einer offenen Halle anstelle hierarchisch gegliederter Kirchenschiffe -, aber dennoch mancherorts eine kostspielige Ausstattung erhielten. Bereits 1231 kritisierte dies in Eisenach die den Franziskanern sehr nahe stehende Landgräfin Elisabeth von Thüringen.

Die Zählung der franziskanischen Generalminister begann erst mit Franziskus’ Tod. Der dem Armutsideal anhängende Johannes Parenti wurde zum ersten Generalminister gewählt und hatte das Amt 1227–1232 inne. Sein Gegenspieler war Elias von Cortona, der den Orden bereits in den Jahren vor Franziskus’ Tod geleitet hatte. Er und Papst Gregor IX. trieben die Idee voran, über dem Grab des Ordensgründers in dessen Geburtsstadt Assisi eine Basilika mit Klosteranlage zu bauen. Am 16. Juli 1228, nur zwei Jahre nach seinem Tod, wurde Franziskus heiliggesprochen, und am 17. Juli legte der Papst den Grundstein zur Basilika San Francesco. Zahlreiche Brüder störten sich an der Pracht des Kirchenbaus und an der Größe des Sacro Convento, die im Widerspruch zu den Armutsidealen des Ordens standen. Dennoch löste Elias von Cortona 1232 Johannes Parenti als Generalminister ab und hielt sich bis 1239 in dieser Position. Um den heftig entbrannten sogenannten Armutsstreit einzudämmen, verfügte Gregor IX., dass die gewählte Ordensverwaltung Gelder besitzen dürfe und der Bau von Klöstern den Absichten des Ordensgründers nicht widerspreche. Aber erst Bonaventura von Bagnoregio, Generalminister 1257–1274, gelang es, im Armutsstreit wirkungsvoll zu vermitteln und den Orden auf einen gemäßigten Kurs zu bringen.

Dennoch schwelte der Streit weiter und führte zu Konflikten mit den Päpsten, insbesondere unter Michael von Cesena, Generalminister 1316–1328. Die Minderen Brüder gaben ihre strenge Armut und ihre seelsorgerische Ausrichtung mehr und mehr auf zugunsten von Besitz und Gelehrsamkeit, zugunsten des Lebens in großen städtischen Konventen und des Wirkens an bedeutenden Universitäten. Eine Rolle spielte dabei auch die zunehmende Stiftung von Jahrtagen durch wohlhabende Bürger, wo als Gegenleistung für die Überschreibung von Geld oder Grundstücken von den Stiftungsempfängern Gebete verrichtet und Messen zelebriert werden mussten. Es hatte sich mit Billigung der Kirchenleitung die Praxis entwickelt, dass Franziskanerkonvente und sogar einzelne Brüder Eigentum aus solchen Stiftungen besitzen durften und dazu vom Gelübde der Armut dispensiert wurden.

Gegen diese Tendenzen einer Verwässerung des Armutsideals wandte sich eine Bewegung innerhalb des Ordens, die für eine Rückkehr zu einer strengeren Beachtung der ursprünglichen Ordensregel ("regularis observantia", „Observanz“) eintraten. Ähnliche Bestrebungen gab es zu der Zeit auch in anderen Orden. Für die Franziskaner war die radikale Befolgung des Ideals von Bedeutung, die sich durch ein ungesichertes Leben, eine Abwendung von den Städten und die Niederlassung in Einsiedeleien am leichtesten verwirklichen ließ. Erste Gruppen entstanden etwa um die Mitte des 14. Jahrhunderts in Italien, wo Paul von Trinci aus Foligno seine Gefährten 1368 „Brüder von der Familie der Observanz“ nannte, bald aber auch in Spanien und Frankreich. Diese Gruppen, denen im 15. Jahrhundert beispielsweise Bernhardin von Siena, Johannes von Capestrano, Albert von Sarteano (1385–1450) und Jakobus von der Mark (1394–1476) angehörten, erfreuten sich regen Zulaufs und bildeten schon bald die Mehrheit im Minderbrüderorden. Das Konzil von Konstanz erlaubte 1414 in seiner Konstitution "Supplicationibus" den Brüdern der strengen Observanz ("stricta observantia regularis"), sich in allen Provinzen des Ordens niederzulassen und reformwillige Ordensmänner aufzunehmen. Zur Schwächung des ursprünglichen Stammordens, der so genannten Konventualen, trugen auch äußere Einflüsse bei, etwa der Hundertjährige Krieg, die in den Städten wütende Pest und das Abendländische Schisma.

Die observanten Strömungen waren in verschiedenen Regionen uneinheitlich ausgeprägt. Als gemeinsame Kennzeichnung kann gelten:

Im Jahr 1517 reagierte Papst Leo X., indem er die Teilung des Ordens in Konventualen und Observanten anerkannte. Ein Generalkapitel des Ordens zu Pfingsten 1517, das auf Anordnung des Papstes die Separationsbewegungen im Orden beenden sollte, blieb erfolglos. Papst Leo X. verfügte in seiner Bulle "Ite et vos" am 29. Mai 1517 die Trennung des Ordens in zwei eigenständige Ordenszweige: die Konventualen mit 20.000 bis 25.000 Brüdern einerseits und die Observanten mit 30.000 bis 32.000 Brüdern andererseits. Die Observanten, die eine Vereinigung mehrerer unterschiedlicher Reformgruppen wie die Klarener, Amadener und Martinianer darstellen, werden in der Bulle als rechtmäßige Erben des ursprünglichen Ordens betrachtet, sie erhielten das Ordenssiegel und das Recht, den Namen „Minderbrüder“ zu führen. Wenig später spalteten sich auch die Observanten, sodass sich der franziskanische Männerorden (Erster Orden) heute in drei Untergruppen gliedert:


Vom 12. bis 14. Juni 2017 fand aus Anlass des 500. Jahrestages der Ordensteilung und des Reformationsgedenkens in Hofheim am Taunus ein gemeinsames Mattenkapitel mit 70 Brüdern der drei Männerorden in Deutschland statt, an dem auch die Provinziale der Deutschen Franziskanerprovinz, der Deutschen Kapuzinerprovinz und der Deutschen Minoritenprovinz teilnahmen. Man hielt eine künftige Wiedervereinigung der Minderbrüder für möglich und vereinbarte konkrete Schritte einer weiteren Annäherung. Die drei deutschen Provinzen haben zusammen 552 Mitglieder in 79 Niederlassungen (Franziskaner 321, Minoriten 112 und Kapuziner 117).








</doc>
<doc id="12124" url="https://de.wikipedia.org/wiki?curid=12124" title="Jewgeni Anatoljewitsch Popow">
Jewgeni Anatoljewitsch Popow

Jewgeni Anatoljewitsch Popow (, wiss. Transliteration ""; * 5. Januar 1946 in Krasnojarsk, Sibirien) ist ein russischer Prosa-Schriftsteller.

Popow betätigte sich nach seiner Mitwirkung am Literaturalmanach Metropol seit 1979 in der Sowjetunion im so genannten "Underground". Nach ersten literarischen Erfolgen, zu denen ihm u. a. Wassili Schukschin verholfen hatte, wurde er nach siebenmonatiger Mitgliedschaft aus dem Schriftstellerverband ausgeschlossen. Er arbeitete in seinem erlernten Beruf als Geologe, wodurch er als Schriftsteller unabhängig blieb.

1980 kam ein Sammelband mit Erzählungen von Popow in einem amerikanischen Exilverlag heraus. Erst nach der Perestroika erschienen seine Werke wieder in Russland.

Bis heute verfasste Popow über 200 Erzählungen und vier Romane.




</doc>
<doc id="12126" url="https://de.wikipedia.org/wiki?curid=12126" title="Leberzirrhose">
Leberzirrhose

Die Leberzirrhose (, von "kirros" ‚gelb-orange‘, ‚zitronengelb‘, von René Laënnec geprägter Begriff) ist das Endstadium chronischer Leberkrankheiten. Dieses Stadium gilt als irreversibler Prozess, auch wenn einzelne Berichte über Heilungen existieren. Typischerweise entwickelt sich eine Zirrhose über Jahre bis Jahrzehnte, seltener sind schnellere Verläufe von unter einem Jahr. Fast alle chronischen Leberkrankheiten führen im Endstadium zu einer Leberzirrhose. In Europa sind Alkoholmissbrauch und chronische Virushepatitis die häufigsten Ursachen.

Durch den chronischen Ablauf von Untergang und Regeneration des Lebergewebes entsteht eine gestörte Gewebearchitektur mit knotigen Veränderungen. Zusätzlich bildet sich übermäßig Bindegewebe (Fibrosierung). Diese narbigen Areale können mehr als 50 % des gesamten Gewebes einer zirrhotischen Leber einnehmen. Dadurch ist die Durchblutung der Leber gestört, im Bereich der Pfortader staut sich das Blut vor der Leber (portale Hypertension).

Die Inzidenz, d. h. die Anzahl der Neuerkrankungen, beträgt in den Industrieländern 250 pro 100.000 Einwohnern im Jahr. Das Verhältnis erkrankter Männer zu Frauen liegt bei 2:1.

Die Leberzirrhose ist nicht zu verwechseln mit der sehr viel häufigeren Fettleber.

Die alkoholische Leberzirrhose ist in Industrieländern mit 50 % die häufigste Ursache. Der massive Alkoholkonsum und die dadurch entstehende hohe Metabolisierungsrate von Ethanol zu Ethanal führen zu einem starken Anstieg des NADH/NAD-Quotienten im Körper. Die Erhöhung des Redoxpotentials der Pyridinnukleotide hemmt den Tricarbonsäurezyklus. Das
Acetyl-CoA dient in dieser Situation vor allem in der Leber in erhöhtem Umfang zur Fettsäuresynthese und schließlich zu Anlagerung von Triglyceriden (Fett). Diese Leberverfettung ist anfangs voll reversibel, führt bei fortgesetztem Alkoholkonsum allerdings zu einer Fettleber und damit letztendlich zur Ausbildung einer Leberzirrhose.

Die Ursache der Zirrhose ist die Nekrose (Absterben) von Leberzellen, verursacht z. B. durch Viren oder Gifte. Die Zellen setzen dabei Zytokine frei, die einerseits Leber-Makrophagen (Kupffer-Zellen) und Fettspeicherzellen der Leber (Ito-Zellen) und andererseits Monozyten und Granulozyten aus dem Blut aktivieren.
Durch diese Zellen wird die Organstruktur mit Parenchym-Nekrosen, Herausbildung von Regeneratknoten (Pseudolobuli) und Bindegewebssepten destruktiv umgebaut.
Durch diese Bindegewebsknoten werden die Kanäle der Leber unterbrochen, welche die Galle über die Gallenkanälchen (Canaliculi und Ductus) zur Gallenblase bringen, Nährstoffe aus dem portalen Blut in den Körper führen, den Hepatozyten Schadstoffe zum Entgiften anschwemmen und die Leber mit sauerstoffreichem Blut versorgen. Gallenkanäle bilden sich zwar neu aus, enden aber blind.
Als Folge entsteht ein Blutstau zwischen Leber und Verdauungstrakt (Portale Hypertension), durch welchen sich Aszites bildet und die Milz sich vergrößert. Im schlimmsten Fall kommt es zu Ösophagusvarizenblutungen. Der Ausfall der Hepatozyten führt zu weiteren Sekundärerkrankungen wie hepatische Enzephalopathie und Leberkoma.

Der Pathologe unterscheidet nach dem äußeren Erscheinungsbild des Organs die mikronoduläre, die makronoduläre und die gemischtknotige Zirrhose. Die Leber schrumpft, ihre Oberfläche wird runzlig und knotig. Mikroskopisch lassen sich aktive oder "floride" (d. h. voranschreitende) und inaktive Zirrhosen unterscheiden. Die Vorstufe der Leberzirrhose ist die Fettleber.

Eine Leberzirrhose beeinträchtigt das subjektive Empfinden des betroffenen Patienten häufig erst in einem recht späten Stadium. Die Leberfunktion kann hinsichtlich der Synthesefunktion (Gerinnungsfaktoren, Albumin) und der Entgiftungsfunktion (Leberkoma) sehr unterschiedlich beeinträchtigt sein. Zusätzlich spielen die Sekundärerkrankungen wie Umgehungskreisläufe, portale Hypertension (daraus folgende Ösophagusvarizen = Krampfadern in der Speiseröhre), Aszites oder Milzvergrößerung (Mangel an Thrombozyten) und hepatische Enzephalopathie oft eine große Rolle. Andere typische Symptome sind Rötungen der Handinnenflächen (Palmarerythem), Caput medusae, Spider-Naevi ("Naevus araneus"), Rhagaden, Lackzunge und Ödeme. In der sogenannten Child-Pugh-Score-Klassifikation werden mehrere dieser Faktoren einbezogen (Bilirubin, Quick-Wert, Albumin, Enzephalopathie und Aszites) und daraus ein Score errechnet; die daraus resultierende Einteilung in die Stadien A bis C erlaubt eine Aussage über die Prognose der Erkrankung, Patienten im Stadium C nach Child-Pugh haben eine sehr schlechte Prognose hinsichtlich der Überlebenszeit. Die Enzephalopathie und der Aszites werden nur in drei Schweregrade eingeschätzt, was zu mehr oder weniger subjektiven, ungenauen Werten führt. Darum nutzt man seit 2002 auch den MELD-Score, der nach einer bestimmten Formel aus Laborparametern (Kreatinin, Bilirubin und INR) berechnet wird.

Leberzirrhosen gelten als fakultative Präkanzerose, d. h., dass sich auf dem Boden einer Leberzirrhose ein bösartiger Tumor entwickeln kann, das hepatozelluläre Karzinom (HCC).

Als zentrale Symptome der auch als Schrumpfleber bezeichneten Leberzirrhose gelten Leistungsminderung, Konzentrationsschwäche und Müdigkeit. Hinzu treten die so genannten Leberhautzeichen, welche sich unter anderem durch rot gefärbte Kleinfingerballen, durch eine gelbliche Haut (Ikterus) sowie "Naevus araneus" bemerkbar machen.

Bei der Untersuchung fallen oft ein Ikterus, ein gesteigerter Bauchumfang (Aszites-bedingt), Ödeme, eine Gynäkomastie, Hautblutungen sowie bei einer hepatischen Enzephalopathie ein "flapping tremor" und Bewusstseinsstörungen auf.

Typisch, allerdings erst spät auftretend, sind die sogenannten "Leberhautzeichen": Spider Naevi (Gefäßspinne), „Milchglasnägel“ (Opake Weißverfärbung der Fingernägel mit distaler longitudinaler Rot-braun-Färbung, auch „Terry-Nägel“ genannt), Prurigo (Juckflechte), Lackzunge, Hautatrophie („Geldscheinhaut“) sowie Palmar- bzw. Plantarerytheme (Rötung der Handflächen bzw. der Fußsohle). Diese Zeichen sind allerdings nicht nur für die Leberzirrhose spezifisch, sondern können auch in diskreter Form bei anderen Leberstörungen, wie z. B. der Fettleber, auftreten. Dies kann z. B. auch temporär während einer Schwangerschaft der Fall sein. Weitere "Leberzeichen" sind Caput medusae, Dupuytren-Kontrakturen sowie eine fehlende Bauchbehaarung beim Mann (Abdominalglatze, „Bauchglatze“).

Im Labor fallen durch die verminderte Syntheseleistung der Leber verminderte Werte für die Cholinesterase, Albumin und einige Gerinnungsfaktoren (erniedrigter Quick-Wert) auf. Die Leberenzyme GOT (AST), GPT (ALT) sowie γ-GT, Bilirubin und Ammoniak können erhöht sein.

Im Ultraschall stellt sich die Leber inhomogen dar. Der Leberrand ist wellig, die Binnengefäße sind rarefiziert. Der Lobus caudatus kann vergrößert sein. Sehr gut können mit dem Ultraschall ein Aszites und eine Milzvergrößerung (Splenomegalie) erkannt werden. Eine verbesserte Form der Sonographie stellt der sogenannte Fibroscan, auch bekannt als transiente Elastografie, dar. Hiermit kann die Fibrosierung, also der Bindegewebsumbau, der Leber ermittelt werden, was ein sehr zuverlässiges Ergebnis zur Diagnosestellung liefert und zukünftig die Leberbiopsie (s. u.) ersetzen könnte.
Mit der Farbduplexsonographie lässt sich in den Lebervenen eine verminderte Elastizität der Leber, in der Pfortader ein verminderter Fluss sowie in der Leberarterie ein erhöhter peripherer Widerstand messen.

Die definitive Diagnose wird durch eine Leberbiopsie gestellt.

Aus verschiedenen Untersuchungsbefunden wird der Child-Pugh Score erstellt, der sowohl zur Stadieneinteilung (Child A–C) als auch zur Prognoseabschätzung dient.

Die Basis der Therapie bilden ernährungstherapeutische "Allgemeinmaßnahmen" wie das Weglassen aller potenziell lebertoxischen Substanzen (Alkohol, Medikamente), Ausgleich eines Vitaminmangels (z. B. Vitamin B1 bei Alkoholismus) und eine ausreichende Energiezufuhr, bei hepatischer Enzephalopathie Beschränkung der Proteinaufnahme. Mangelernährte Patienten haben sowohl eine erhöhte Mortalität im spontanen Krankheitsverlauf als auch eine erhöhte Komplikationsrate. Die Nahrungszufuhr sollte bevorzugt als orale Trinknahrung erfolgen, insbesondere Patienten mit fortgeschrittener Leberzirrhose profitieren auch von parenteraler Ernährung. Im frühen Stadium ist dies noch nicht angezeigt. Die Energiezufuhr sollte etwa 145-167 kJ (35–40 kcal) pro Kilogramm Körpergewicht betragen.

Eine unzureichende Kohlenhydratzufuhr kann eine bestehende katabole Stoffwechselsituation verschlechtern. Dies ist dadurch zu erklären, dass in dieser Situation neben Fetten auch Proteine zur Energiegewinnung verstoffwechselt werden. Die DDG rät Diabetikern mit Leberzirrhose grundsätzlich von kohlenhydratreduzierten Diäten ab.

Empfohlen wird eine tägliche Eiweißmenge von 1,2–1,5 g Protein pro kg Körpergewicht. Proteinrestriktion darf ausschließlich bei Patienten mit therapierefraktärer chronischer Enzephalopathie erfolgen. Gegebenenfalls sollte bei diesen Patienten Leucin, Isoleucin und Valin substituiert werden (verzweigtkettige Aminosäuren).

Bei allen Patienten sollte frühzeitig eine Osteoporoseprophylaxe eingeleitet werden. Dies geschieht durch Calcium-Substitution (1200–1500 mg/d). Bei Patienten mit cholestatischen Lebererkrankungen wird zusätzlich Vitamin D3 substituiert (400–800 IE/d). Bei Patienten fortgeschrittenen Alters (> 65 Jahre), untergewichtigen Patienten und Rauchern sollte eine Basisdiagnostik früh erfolgen.

Eine Vitamin-K-Substitution ist angezeigt bei erhöhtem Blutungsrisiko und niedrigen Quick-Werten. Da die enterale Resorption dieses lipophilen Vitamins bei Cholestase vermindert ist, sollte die Substitution hier in gesteigerter Dosis (10 mg alle 10 Wochen) oral oder parenteral erfolgen. Bei alkoholkranken Patienten liegt zu 50 % ein Vitamin-B1-Mangel vor. Dieses ist – besonders bei fortgesetztem Alkoholkonsum zur Prophylaxe einer Wernicke-Enzephalopathie – ebenfalls zu substituieren.

Die "Grunderkrankung" muss behandelt werden, bei Alkoholismus wird eine Entzugsbehandlung versucht, Patienten mit Autoimmunhepatitis werden mittels Immunsuppression behandelt, bei einer chronischen Hepatitis B kann eine Viruselimination mit Interferonen versucht werden. Bei Hepatitis C führt eine antivirale Therapie bei über 90 % der Betroffenen zur Viruselimination (keine Viren mehr im Blut nachweisbar).

Auf "Komplikationen" wird mit spezifischen Maßnahmen reagiert: Blutstillung bei Ösophagusvarizenblutung, Shunt-OP bei hepatischer Enzephalopathie, Punktion bei hartnäckigem Aszites.

Wichtig sind "regelmäßige Untersuchungen" zur Früherkennung eines Leberkarzinoms. Eine Ultima Ratio ist in vielen Fällen die Lebertransplantation.

Wahrscheinlich schützt Koffein die Leber vor der Bildung einer Zirrhose oder verzögert deren Entwicklung.

Die Prognose ist abhängig von der Ursache, der erfolgreichen ursächlichen Behandlung, den Komplikationen und dem Stadium. So sind die Ein-Jahres-Überlebensraten für Patienten im Stadium Child A fast 100 %, bei Child B etwa 85 % und bei Child C 35 %.
Mit dem MELD-Score lassen sich Aussagen für das Überleben in den nächsten drei Monaten treffen. So hat ein Patient im Krankenhaus mit einem Score von 20–30 ein Risiko von 25 %, in den nächsten drei Monaten zu sterben. Ein Zirrhotiker mit einem MELD von 40 ist in drei Monaten höchstwahrscheinlich verstorben.

Die erste makroskopische Beschreibung einer Leberzirrhose in der Medizingeschichte findet sich in den Anmerkungen zur Zeichnung "del vecchio" von Leonardo da Vinci (1452–1519). Die Zeichnungen zur Gefäßanatomie der Leber basieren auf einer Autopsie, die Leonardo da Vinci 1508 in Florenz an einem über 100-Jährigen vornahm.




</doc>
<doc id="12127" url="https://de.wikipedia.org/wiki?curid=12127" title="Faulgas">
Faulgas

Faulgas, auch je nach Vorkommen Sumpfgas, Kanalgas oder Klärgas genannt, ist ein Gemisch von zumeist brennbaren Gasen, das bei der anaeroben Gärung entsteht. Dabei werden biotische Stoffe unter Abwesenheit von Sauerstoff von Bakterien und Archaeen zersetzt, was typisch für Fäulnisprozesse ist (daher der Name Faulgas). Den größten Anteil an den brennbaren Bestandteilen des Gasgemischs hat Methan mit der Summenformel CH.

Für den unangenehmen Geruch des Faulgases sorgt neben anderen Gasen hauptsächlich Schwefelwasserstoff (Dihydrogensulfid, Summenformel: HS), der nach faulen Eiern riecht und bei der Gärung im ppm-Bereich frei wird. Ausschlaggebend dafür ist die Schwefelkonzentration des Ausgangsmaterials, die sich aus dem Anteil an vorhandenen Eiweißen ergibt. 

Das in Abwasserreinigungsanlagen (Kläranlagen) in Faultürmen entstehende Faulgas weist oft höhere Anteile Schwefelwasserstoff auf. Werden dort keine Eisen(II)-Ionen zur Bindung benutzt, kann die HS-Konzentration auf über 2500 ppm steigen. Wird das Faulgas wie das Biogas für die Stromerzeugung mit Gasmotoren genutzt, sind Werte über 200 ppm nachteilig. Eine Reinigung des Faulgases von Schwefelwasserstoff ist möglich.

Neben dem oben bereits erwähnten Methan enthält Faulgas noch Sauerstoff (O), Kohlenstoffdioxid (CO), Kohlenstoffmonoxid (CO) und Ammoniak (NH)

In der Natur entsteht Faulgas etwa in Sümpfen und anderen stehenden Gewässern unter Sauerstoffabschluss, da für die anaeroben Methanbildner (Archaeen) Sauerstoff ein tödliches Gift ist. Aufgrund dieser Entstehung wird es auch Sumpfgas genannt. Faulgase sind möglicherweise eine der Ursachen von Irrlichtern.

Faulgas wird jedoch nicht nur in Gewässern, sondern auch im Darm von Menschen (Flatulenzen) und Tieren sowie im Pansen von Wiederkäuern (prominentes Beispiel sind Rinder) von Archaeen erzeugt, da diese oft bei der Verdauung eine wichtige Rolle spielen. 

Da der Hauptbestandteil von Faulgas das für den Treibhauseffekt mitverantwortliche Gas Methan ist, wurden umfangreiche Untersuchungen zum Beitrag der Landwirtschaft gemacht. Das Ergebnis war, dass Reisfelder, Rinder und Gülle die Hauptquellen für anthropogenes Methan sind.

In Kanalisationen entstehendes Faulgas wird auch Kanalgas genannt. Es kann eine Gefahr für Kanalarbeiten darstellen.

Zur Verhinderung von Unfällen können beispielsweise Gaswarngeräte eingesetzt werden, die den Träger bei Erreichen bestimmter Konzentrationen potentiell giftiger oder erstickender Gasen warnen. Ideal ist hier der Einsatz von Mehrgas-Messgeräten.

In technischen Einrichtungen entsteht Faulgas beim Ausfaulen der Klärschlämme, die bei der Abwasserreinigung in Kläranlagen anfallen (Klärgas). Klärgas wird zum Teil schon seit den 1950er Jahren in Motoren zur Stromerzeugung genutzt. In Mülldeponien und in Biogasanlagen (daher die Bezeichnungen Deponiegas beziehungsweise Biogas) entsteht analog ebenfalls Faulgas. 

Das Vergären biotischer Stoffe in Kläranlagen, Deponien und Biogasanlagen und die anschließende Nutzung des Gases ist ökonomisch und ökologisch interessant: 



</doc>
<doc id="12129" url="https://de.wikipedia.org/wiki?curid=12129" title="Adolphe Sax">
Adolphe Sax

Adolphe Sax, eigentlich Antoine Joseph Sax, (* 6. November 1814 in Dinant; † 7. Februar 1894 in Paris) war ein belgischer Erfinder, Instrumentenbauer und Musiker; er war der Entwickler der Saxhörner und des Saxophons.

Adolphe Sax war eines von elf Kindern. Familie Sax zog 1835 nach Brüssel, wo der Vater Charles Joseph Sax, ein Kunsttischler, eine Instrumentenbauwerkstatt eröffnete. Adolphe besuchte das Brüsseler Konservatorium und studierte dort Flöte, Klarinette, Gesang und Harmonie. Seine erste selbstständige Arbeit als Instrumentenbauer in der Werkstatt seines Vaters war die Vervollkommnung von Klarinette und Bassklarinette (Patent 1838).

Ohne Mittel (sein Vater verbrauchte viel Geld durch seine Experimente und wurde mehrmals von der Regierung unterstützt) begab er sich 1842 nach Paris; als einzige Empfehlung nahm er ein Exemplar eines von ihm entwickelten völlig neuen Instruments, des (Sopran)-Saxophons, mit und erregte bald die Aufmerksamkeit verschiedener Persönlichkeiten des "Pariser Musiklebens" (Jacques Fromental Halévy, Daniel-François-Esprit Auber etc.). Namentlich fand er in Hector Berlioz einen tatkräftigen Helfer, dem sich auch bald Sponsoren anschlossen.

Sax baute nun das "Saxophon in acht verschiedenen Größen" (Sopranino, Sopran, Alt, Tenor, Bariton, Bass, Kontrabass, Subkontrabass). Seine Erfahrungen, besonders die seines Vaters bezüglich der besten Resonanz der Röhren, übertrug er sodann auf die Konstruktion der Trompeten, Hörner, Tuben etc. und gab diesen in ihrer neuen Gestalt die Namen Saxtromba, "Saxhorn", "Saxtuba" etc., die als "„Familie der Saxhörner“" bekannt wurden. Auch baute er im Auftrage Giuseppe Verdis mehrere Aida-Trompeten.

Am 21. März 1846 erhielt Sax in Frankreich ein Patent und gelangte so schnell zu großer Berühmtheit; seine Instrumente wurden besonders in der französischen Militärmusik eingeführt. Die Originalität seiner Verbesserungen wurden von neidischen Konkurrenten, denen er den Rang ablief, vielfach angefochten; doch fielen die gerichtlichen Entscheidungen immer zugunsten von Sax aus. Seine Werkstatt mit zeitweilig 100 Mitarbeitern soll über 20.000 Instrumente gebaut haben. Ein dauerhafter wirtschaftlicher Erfolg war Adolphe Sax hingegen nicht vergönnt: Er musste mehrfach Konkurs anmelden und starb verarmt.

Sax wurde 1857 Saxophon-Lehrer am Pariser Konservatorium (diese Stelle wurde 1871 aus Geldmangel eingestellt und erst 1942 wieder von Marcel Mule neu besetzt). Er gab außerdem eine Schule für das Saxophon-Spiel und die von ihm erbauten Instrumente heraus. Ab 1858 war Sax Direktor des Bühnenorchesters der Pariser Oper.

Er war Ritter im Orden der Eichenkrone.

Sax wurde 1842 in die Freimaurer-Loge "Les Vrais Amis de l'Union" aufgenommen. In Anerkennung seiner Errungenschaften wurde nach ihm der Asteroid (3534) Sax benannt.

2005 entstand die Kinderkonzertrevue „Die Abenteuer des Monsieur Sax“, gemeinsam entwickelt vom Kölner Schauspieler Martin Heim und dem Pindakaas Saxophon Quartett, die Regie führte Thomas Philipzen. Das Stück erzählt kindgerecht die Lebensgeschichte von Adolphe Sax sowie die Erfindung der Saxophonfamilie. 2007 sendete der Westdeutsche Rundfunk Köln eine Hörspielfassung des Stückes live, 2008 wurde die Kinderhörspiel-CD „Die Abenteuer des Monsieur Sax“ veröffentlicht. Diese erhielt eine lobende Erwähnung der Jury beim Deutschen Kinderhörspielpreis im Rahmen der ARD Hörspieltage 2008.

Der Belgier Karel Goetghebeur kaufte die Rechte an dem Namen „Adolphe Sax & Cie“ und versucht seit 2012, die untergegangene Marke wieder zu beleben.






</doc>
<doc id="12131" url="https://de.wikipedia.org/wiki?curid=12131" title="Paul Feyerabend">
Paul Feyerabend

Paul Karl Feyerabend (* 13. Januar 1924 in Wien; † 11. Februar 1994 in Genolier im schweizerischen Waadtland) war ein österreichischer Philosoph und Wissenschaftstheoretiker. Er war von 1958 bis 1989 Philosophieprofessor an der Universität von Kalifornien in Berkeley und lebte zeitweilig in England, Deutschland, Neuseeland, Italien, zuletzt in der Schweiz, wo er als Hochschullehrer an der ETH Zürich tätig war.

Bekannt wurde Feyerabend durch seinen wissenschaftstheoretischen Anarchismus. Nach Feyerabend lassen sich keine universellen und ahistorischen wissenschaftlichen Methoden formulieren, produktive Wissenschaft müsse vielmehr Methoden nach Belieben verändern, einführen und aufgeben dürfen. Zudem gebe es keine allgemeinen Maßstäbe, mit denen man verschiedene wissenschaftliche Methoden oder Traditionen bewerten könne. Das Fehlen allgemeiner Bewertungsmaßstäbe führt Feyerabend zu einem philosophischen Relativismus, nach dem keine Theorie allgemein wahr oder falsch ist.

Paul Feyerabend wurde 1924 in Wien geboren. Der Sohn einer Mittelstandsfamilie besuchte ein Realgymnasium und war ein Schüler mit überdurchschnittlichen Leistungen. Die Eltern hatten infolge des Ersten Weltkrieges sowie der Inflation lange gewartet, bevor sie ihr einziges Kind bekamen: Paul Feyerabends Mutter war bei seiner Geburt bereits vierzig Jahre alt.

In Kontakt mit der Philosophie kam Feyerabend nach eigenen Angaben durch einen Zufall: ""

Im März 1938 wurde Österreich Teil des deutschen Reiches, am 1. September 1939 begann der Zweite Weltkrieg und veränderte auch das Leben des 15-Jährigen. Feyerabends Eltern begrüßten den Anschluss Österreichs, Feyerabend beschreibt sein Verhältnis zu den Nazis als naiv und relativ emotionslos. Er wurde nicht zu einem glühenden Anhänger, reagierte jedoch auch auf die im Krieg erlebten Grausamkeiten nicht mit Empörung. 1940 begann Feyerabend mit dem Reichsarbeitsdienst, 1942 wurde er Teil eines Pionierkorps, 1943 besuchte er eine Offiziersschule. Er wurde für die Ausbildung nach Jugoslawien geschickt; nach Feyerabend war die Offiziersschule insbesondere ein Weg, den Kriegseinsatz zu umgehen. In Jugoslawien erfuhr er von der Selbsttötung seiner Mutter, ein Ereignis, das ihn damals nicht sehr bewegte. Feyerabend wurde noch im September 1943 nach Russland geschickt, wo er sich nach eigenen Angaben leichtsinnig und theatralisch verhielt und dafür bis zum Leutnant befördert wurde.

Im letzten Kriegsjahr wurde Feyerabend auf dem Rückzug von mehreren Kugeln in den Magen und die Hand getroffen. "" Feyerabends schwere Verletzungen bewirkten, dass er sein Leben lang starke Schmerzen hatte, an einem Stock gehen musste und impotent geworden war. Er wurde in eine Klinik in Apolda gebracht; nach Kriegsende studierte er für ein Jahr Gesang im nahen Weimar.

1947 kehrte Feyerabend aus Weimar nach Wien zurück. Seine frühere Leidenschaft – die Physik – schien ihm nach Kriegsende lebensfremd, und so begann er mit dem Studium der Geschichte und Soziologie. Bald langweilten ihn jedoch seine Vorlesungen, er wechselte noch im gleichen Jahr zur Physik. Unter den Physikern an der Universität Wien machte insbesondere Felix Ehrenhaft Eindruck auf Feyerabend. Bald kam er durch Victor Kraft zudem in Kontakt mit der akademischen Philosophie. Kraft war im Gegensatz zu den anderen bekannten Mitgliedern des Wiener Kreises in Österreich geblieben und hatte um sich eine Gruppe von Philosophen und Studenten versammelt – den so genannten „Kraft-Kreis“. Unter ihnen war auch Feyerabend, der im Kraft-Kreis die Gelegenheit bekam, mit Philosophen wie Walter Hollitscher, G.E.M. Anscombe oder Ludwig Wittgenstein zu diskutieren. In dieser Zeit übernahm Feyerabend zentrale Überzeugungen des logischen Empirismus: 

Entscheidend für Feyerabends weitere Entwicklung wurde das Forum Alpbach, an dem er 1948 erstmals teilnahm. In Alpbach lernte Feyerabend Hanns Eisler, Bertolt Brecht und nicht zuletzt Karl Popper kennen. Das Angebot, bei Brecht als Assistent zu arbeiten, schlug Feyerabend aus. Stattdessen wollte er nach seiner Promotion 1951 mit einem Stipendium des British Council bei Wittgenstein in Cambridge studieren. Da Wittgenstein jedoch 1951 verstarb, ging Feyerabend zu Popper an die London School of Economics and Political Science. Der Einfluss Poppers wurde in mehrfacher Hinsicht bestimmend für Feyerabends philosophische Entwicklung. Zunächst übernahm er den Falsifikationismus und wurde tief von Poppers Denken geprägt. Später wandte er sich jedoch von Poppers kritischem Rationalismus ab und machte ihn zum Hauptgegner des eigenen wissenschaftstheoretischen Anarchismus.

1955 bekam Feyerabend seine erste akademische Stelle an der University of Bristol, wo er eine Vorlesung über Wissenschaftstheorie zu halten hatte. Die Stelle war wohl nicht zuletzt dem Einfluss Poppers zu verdanken, allerdings zeigten sich nach Feyerabend erste Brüche: John Watkins ' Feyerabends Schriften der 1950er und frühen 1960er Jahre sind dennoch stark durch Poppers Falsifikationismus geprägt. Während seiner Zeit in Bristol heiratete Feyerabend zum zweiten Mal, die Ehe wurde jedoch, wie auch schon die erste, schnell geschieden. In dieser Situation war Feyerabend glücklich, dass ihm 1958 das Angebot gemacht wurde, ein Jahr an der University of California, Berkeley, zu verbringen.

Berkeley wurde für über 30 Jahre zum Hauptwohnsitz von Feyerabend. Der Wechsel von Europa in die USA war auf verschiedene Weisen prägend: Zunächst kam Feyerabend insbesondere durch seine Besuche am Minnesota Center for the Philosophy of Science schnell in engen Kontakt mit der amerikanischen Philosophieszene. Unter den Bekanntschaften waren zum einen viele alte Vertreter des Wiener Kreises wie Herbert Feigl, Rudolf Carnap und Carl Gustav Hempel, zum anderen jüngere Vertreter der amerikanischen analytischen Philosophie wie John Searle und Hilary Putnam. 1965 veröffentlichte Feyerabend seine erste ausführliche, wissenschaftstheoretische Schrift, "Problems of Empiricism". Dieser lange Essay enthält bereits viele radikale Überlegungen, basiert jedoch auf einem philosophischen Realismus und führte Feyerabend noch nicht zu einer unbedingten Konfrontation mit der zeitgenössischen Wissenschaftsphilosophie.

Des Weiteren war das politische Klima Berkeleys und der San Francisco Bay Area prägend: 1964 machte die Free Speech Movement Berkeley zum linksrevolutionären Zentrum der USA, drei Jahre später war die Hippiebewegung im benachbarten San Francisco mit dem Summer of Love auf ihrem Höhepunkt angelangt. Feyerabend hat in seinen Schriften immer wieder betont, dass die Erfahrungen mit den politischen Bewegungen und der Multikulturalität der Bay Area seine philosophischen Gedanken stark geprägt haben. So erklärt er etwa in Bezug auf die multikulturelle Studentenschaft: ""

Feyerabends lange Zeit in Berkeley änderte jedoch nichts an seiner Rastlosigkeit und der Unzufriedenheit mit seiner neuen Heimat. Über die Jahre nahm er viele (Gast-)Professuren an, ohne jedoch an einem Ort vollständig zufrieden zu sein. Längere Zeit verbrachte er in London und Berlin, wo er ebenfalls mit den Studentenbewegungen in Kontakt kam. Weitere Stationen waren Auckland, Kassel, Sussex und Yale.

In den 1960er Jahren hatte Feyerabend einige unkonventionelle Ideen publiziert, sich langsam vom kritischen Rationalismus gelöst und sich in Berkeley mit seinem unsteten Lehrstil einige Feinde gemacht. Insgesamt hatte er sich jedoch eine Reputation als ernstzunehmender und geachteter Wissenschaftstheoretiker erarbeitet. Die folgenden Jahre sollten diese Situation verändern. 1970 veröffentlichte Feyerabend einen Aufsatz mit dem Titel "Against Method", in dem er die bekannten wissenschaftstheoretischen Methodologien angriff. Seine Position entwickelte sich von einem liberalen und realistischen Methodenpluralismus zu einem relativistischen Angriff auf die Methodologie im Allgemeinen.

Mit seinem Freund Imre Lakatos plante Feyerabend eine gemeinsame Publikation zur Methodendebatte in der Wissenschaftstheorie. Lakatos sollte die Methode der Falsifikation gegen Feyerabends wütende Attacken auf jede Form von methodologischen Regeln verteidigen. Lakatos verstarb allerdings 1974 und Feyerabend veröffentlichte seine Kritik unter dem Titel "Against Method. Outline of an anarchistic Theory of Knowledge" als Monographie. Das Buch machte Feyerabend mit dem Slogan „anything goes“ über die Grenzen der Wissenschaftstheorie bekannt. In einer der positiveren Rezensionen des Buches finden sich häufig angeführte Bedenken: '

Plötzlich fand sich Feyerabend in der Rolle des Hauptgegners der etablierten wissenschaftsphilosophischen Ansätze wieder. Er hatte offenbar nicht mit einer so breiten und heftigen Reaktion gerechnet und empfand die oft scharfe Ablehnung seines Werkes als verletzend: "" Als Reaktion auf die Kritik entstand "Erkenntnis für freie Menschen", ein Buch, das selbst wiederum scharfe Angriffe und ein leidenschaftliches Bekenntnis zum Relativismus enthielt. Zudem vertiefte Feyerabend seine politischen Überlegungen, die gegen die Macht moderner Technik und Wissenschaft gerichtet waren.

Feyerabends späte Jahre werden von ihm selbst als seine glücklichsten beschrieben. Über die 1980er Jahre lehrte Feyerabend abwechselnd in Berkeley und an der ETH Zürich, eine Situation, die er sehr genoss. Zudem lernte er 1983 Grazia Borrini bei einer Vorlesung kennen. Sie heirateten sechs Jahre später und blieben bis zu Feyerabends Tod zusammen. Es war Feyerabends vierte Ehe.

Nach dem Erdbeben von San Francisco 1989 zog sich Feyerabend endgültig aus Kalifornien zurück, ein Jahr später wurde er auch an der ETH Zürich emeritiert. "" In den 1980er und 1990er Jahren hat Feyerabend eine große Zahl an Aufsätzen publiziert, seine letzte große Arbeit sollte die Autobiographie "Zeitverschwendung" (Originaltitel: "Killing Time") werden, an der er bis kurz vor seinem Tode schrieb. 1993 wurde bei Feyerabend ein Hirntumor diagnostiziert; am 11. Februar 1994 starb er in einer Klinik am Genfersee. Er erhielt ein ehrenhalber gewidmetes Grab auf dem Südwestfriedhof (Gruppe 10A, Reihe 3, Nummer 17) in Wien. Im Jahr 2016 wurde der Asteroid (22356) Feyerabend nach ihm benannt.

Zu Beginn seiner wissenschaftstheoretischen Laufbahn vertrat Feyerabend die Ansichten Karl Poppers bzw. des kritischen Rationalismus. Seine Beiträge kritisierten den von positivistischer Seite behaupteten Dualismus von Theorie- und Beobachtungssprache und die Annahme, es gebe atheoretische, d. h. nicht theoriegetränkte Beobachtungsbegriffe. Aus dem Erfordernis kontra-induktiver und kontra-intuitiver Widerlegungsversuche leitete er ab, dass die Prüfung durch alternative Theorien einen Theorienpluralismus benötige.

Um 1968 radikalisierte sich Feyerabends Wissenschaftsauffassung; fortan verstand er bestimmte Vernunftskriterien nur noch als eine mögliche Alternative unter vielen („anything goes“). Nach dieser wissenschaftstheoretischen Katharsis trat Feyerabend als Kritiker des Rationalismus auf, insbesondere der vorherrschenden Wissenschaftstheorie und Methodologie. So bezeichnete er etwa den kritischen Rationalismus zuweilen als „Law-and-Order-Rationalismus“. Feyerabend rebellierte gegen einen von ihm wahrgenommenen orthodoxen Dogmatismus der Wissenschaft, wobei er bewusst provokativ äußerte, Regentänze seien genauso gut wie Wettervorhersagen, Wahlprognosen nicht besser als Astrologie. Feyerabend sah Wissenschaft, neben beispielsweise Religion oder Kunst, nur als eine von vielen Möglichkeiten, Erkenntnis zu gewinnen. Den verschiedenen Zugängen zur Wahrheit eine feste Wertigkeit zuzuordnen, ist nach Feyerabend nicht möglich, teilweise auch deswegen, weil diese Wahrheitszugänge untereinander inkommensurabel seien.

Nach Feyerabend lässt sich aus der Wissenschaftsgeschichte der Schluss ziehen, dass die Praxis des Erkenntnisgewinns und der Erkenntnisveränderung in oftmals irrationaler und anarchischer Weise bestehende wissenschaftstheoretische Grundsätze verletzt hat und eben darum erfolgreich war. Feyerabend betont die Bedeutung von Intuition und Kreativität als Voraussetzung des Erkenntnisgewinns und Erkenntnisfortschritts, beide dürften nicht durch eine bestimmte dogmatische Rationalität und wissenschaftstheoretisch-methodologische Regeln und Zwänge, die ihrerseits nicht sakrosankt seien, sondern vielmehr im Erkenntnisprozess einem Wandel unterlägen, nutzlos und in irreführender Weise eingeschränkt werden. So prägte er den Begriff der Anti-Regel, die eine Regel bezeichnen soll, die der Induktion widerspricht. Der Wissenschaftler soll sich nicht scheuen, methodische Regeln aufzustellen, die zu Hypothesen führen, die anerkannten Theorien und beobachtbaren Tatsachen widersprechen. Für diese radikale Linie Feyerabends gab es in der Wissenschaftsgeschichte bereits Anknüpfungspunkte, etwa David Brewster, als er sich 1831 kritisch mit der Methodologie von Francis Bacon auseinandersetzte:

Feyerabend forderte eine scharfe Trennung von Staat und Wissenschaft, darüber hinaus wandte er sich gegen jeden Überlegenheitsanspruch von Wissenschaftlern gegenüber „Normalbürgern“. Sein Ziel war eine freie Gesellschaft, in der Bürger und Politiker direkt, ohne weitere administrative Umwege über abstrakte Theorien, am Erkenntnisprozess teilhaben. Eine objektive, von Lebens- und Erfahrungspraxis in einer freien Gesellschaft abgetrennte (und damit die bislang herrschende) Rationalität – in Form der Logik, Wissenschaftstheorie und bestimmter Sozialtheorien – sollte durch eine Beteiligung der Bürger ersetzt werden.

Feyerabend vertritt eine andere Auffassung des Begriffs „rational“ als Popper. Nach Feyerabend funktioniert auch die Wissenschaft anders, als Poppers methodologische Untersuchungen dies nahelegten: Wissenschaftler stellen selbst fest, nach welchen Maßstäben eine bestimmte Wissenschaft abzulaufen hat, und wann es erforderlich ist, nicht nur Theorien, sondern auch methodologische Grundsätze und Regeln abzuändern oder auszuwechseln. Feyerabend liest die Wissenschaftsgeschichte gegen Poppers „Strich“; er belegt an vielen Beispielen, dass sich Wissenschaftler in Wirklichkeit häufig nicht an feste Regeln halten und dennoch oder gerade deswegen zum Erfolg gelangen. Besser, als sich auf die Schaffung einer bestmöglichen Methodologie zu konzentrieren, sei es demnach, sich grundsätzlich opportunistisch zu verhalten, überspitzt formuliert bedeutet das: Alles geht! Feyerabends Anarchismus verkündet nicht die Regellosigkeit oder das Chaos als Zielsetzung, sondern fordert neben einem Theorienpluralismus genauso einen Pluralismus der Methoden unter der Flagge eines Methodenanarchismus.

Feyerabend lehnt Poppers Präokkupation mit dem Abgrenzungsproblem ab als direkten Weg in den Dogmatismus:

"„Kein Rationalist, kein kritischer Rationalist besitzt eine Einsicht in die Grenzen der Wissenschaften – dazu müsste er ja wissen, was außerhalb der Wissenschaften vorgeht, er müsste Mythen kennen, müsste ihre Funktion verstehen […] Man zeige einem kritischen Rationalisten einen Gegenstand, der außerhalb seiner Erfahrung liegt – damit kann er gar nichts anfangen, er benimmt sich wie ein Hund, der seinen Herrn in ungewöhnlichen Kleidern sieht; er weiß nicht, soll er ihn beißen, soll er davon laufen, oder soll er ihm das Gesicht lecken. Das ist auch der Grund, warum kritische Rationalisten an den Grenzen der Wissenschaft zu schimpfen beginnen – für sie ist das Ende ihres Glaubens erreicht und das einzige, was sie sagen können, ist: ‚irrationaler Unsinn‘ oder ‚ad hoc‘ oder ‚unfalsifizierbar‘ oder ‚degenerierend‘ – Bezeichnungen, die genau denselben Zweck haben wie die früheren Bezeichnungen ‚häretisch‘ etc. etc.“"

Nach David Miller merkt Feyerabend nicht, wie sehr seine Kritik in Wirklichkeit mit dem Kritischen Rationalismus konform geht, und ihm gar nicht widerspricht. Feyerabend übersieht demnach, dass das Ziel von Methoden im kritischen Rationalismus überhaupt nicht die Begründung einer Wahl von Theorien oder Methoden ist, also keine Theorien oder Methoden durch Grenzziehungen von der Erörterung ausgeschlossen werden sollen. Er liegt also zwar insofern richtig, als die Wahl einer Methode nicht begründet werden kann, er liegt aber falsch in der Annahme, dass sie daher alle gleichrangig sein müssen. Denn die Wahl einer Methode hat objektive Konsequenzen, weil die Methode Probleme, die sie lösen soll, gemäß ihren eigenen Maßstäben besser oder schlechter löst. Die Methode von Versuch und Irrtum, die nichts zu begründen versucht, funktioniert daher ebenso bei der Methodenauswahl und ist dabei auch auf sich selbst anwendbar. Performative Widersprüche treten nicht auf, weil Ziel nicht Selbstbegründung ist, sondern Selbstkritik.

Tatsächlich vertritt Feyerabend gemäß Miller selbst eine ähnliche Position, geht aber so weit, auch Methoden zulassen zu wollen, die sich gegen die Logik stellen und somit nur schwer zu kritisieren und auszusortieren sind, wenn sie fehlschlagen. In diesem Punkt unterscheidet sich Feyerabends Methodenanarchismus vom kritischen Methodenpluralismus des kritischen Rationalismus. Miller ist der Ansicht, dass Feyerabend kein wirkliches Argument gegen die Logik hat und – frei nach seinen eigenen Worten – ein Dieb ist, der seinem Diskussionsgegner erst die Logik stiehlt, um den Bestohlenen dann dafür zu kritisieren, dass er sie nicht mehr besitzt.





Paul Feyerabends Autobiographie "Zeitverschwendung" wird mit "Zeit" abgekürzt.


</doc>
<doc id="12133" url="https://de.wikipedia.org/wiki?curid=12133" title="Selene">
Selene

Selene ( „Mond“) ist eine Mondgöttin der griechischen Mythologie.

Selene (als Personifikation des Monats und poetisch manchmal auch "Mene"; bei den Römern Luna), die Göttin des Mondes, nach Hesiod Tochter des Hyperion und der Theia, Schwester des Helios und der Eos, auch Phoibe genannt, wird später mit der Mondgöttin Artemis (bei den Römern Diana) oder auch mit Persephone identifiziert. 

Als ihre Eltern werden auch Helios oder Passas und die Euryphaessa, "die weithin Leuchtende", ein anderer Name für Theia, genannt.

Selene gebar mit Zeus die Pandia und Ersa ("Tau"); mit Endymion, dem König von Elis, dem sie ewigen Schlaf schenkte, hat sie 50 Töchter. Eine Erzählung berichtet, dass dieser immer noch schläft, weil Selene zarte Küsse mehr geschätzt haben soll als eine fruchtbare Leidenschaft. Eine andere Erzählung berichtet, dass der im allgemeinen liebestolle Pan sie, in ein schönes weißes Vlies gehüllt, im Wald verführte.

Die Zahl 50 wird mit den 50 Monaten zwischen zwei Olympischen Spielen in Zusammenhang gesehen.

Auf Wunsch der Hera soll sie den nemeischen Löwen geschaffen haben, dem Herakles in seiner ersten Arbeit das Fell abziehen sollte.

Dargestellt wird Selene mit verschleiertem Hinterhaupt, den Halbmond über der Stirn und eine Fackel in der Hand, auf Rossen oder Kühen reitend, auch vom Zweigespann gefahren, in Endymionreliefs zu ihrem Liebling herabschwebend, so auch in statuarischen Einzelwerken (Vatikan). 

Umgeben von anderen Gottheiten, sieht man sie auf einem Altar des Louvre, wo sie vor sich den untergehenden Hesperos (Abendstern), hinter sich den Phosphoros (Morgenstern), unter sich die Maske des Okeanos hat, des Weltenstroms, aus dem sie hervortaucht. 

Nach der Titanin Selene ist das chemische Element Selen benannt.




</doc>
<doc id="12134" url="https://de.wikipedia.org/wiki?curid=12134" title="Semantisches Netz">
Semantisches Netz

Ein semantisches Netz ist ein formales Modell von Begriffen und ihren Beziehungen (Relationen). Es wird in der Informatik im Bereich der künstlichen Intelligenz zur Wissensrepräsentation genutzt. Gelegentlich spricht man auch von einem "Wissensnetz". Meist wird ein semantisches Netz durch einen verallgemeinerten Graphen repräsentiert. Die Knoten des Graphen stellen dabei die Begriffe dar. Beziehungen zwischen den Begriffen werden durch die Kanten des Graphen realisiert. Welche Beziehungen erlaubt sind, wird in unterschiedlichen Modellen sehr unterschiedlich festgelegt, den meisten Beziehungstypen wohnt jedoch ein kognitiver Aspekt inne.

Semantische Netze wurden in den frühen 1960ern von dem Sprachwissenschaftler Ross Quillian (* 1931)
als Repräsentationsform semantischen Wissens vorgeschlagen. Thesauri, Taxonomien und Wortnetze sind Formen semantischer Netze mit eingeschränkter Menge von Relationen.

Eine (meist binäre) Relation zwischen zwei Graphenknoten kann unter anderem sein:

Die psychologische Realität solcher semantischen Netze kann man z. B. mit Hilfe von Assoziationstechniken und Satzverifikationsaufgaben untersuchen.

Polysemie und Homonymie spielen bei der Modellierung von semantischen Netzen eine untergeordnete Rolle, da es um Beziehungen zwischen Begriffen geht. Ein polysemes (oder homonymes) Lexem wird zwei oder mehreren Begriffen zugeordnet bzw. liegt im lexikalischen Wertebereich zweier oder mehrerer Begriffe. Es ist allerdings eine in der Praxis oft schwierige Frage, wie vielen und welchen Begriffen ein Lexem zuzuordnen ist.

Ein weitaus größeres Problem für die Modellierung semantischer Netze stellen lexikalische Lücken dar. Dies sind Begriffe, denen in einer natürlichen Sprache kein einfaches lexikalisches Zeichen als Wert zugeordnet werden kann. Ein bekanntes Beispiel ist der Begriff 'nicht mehr durstig'.

Aktuelle Wissensrepräsentationsmethoden, die auf semantischen Netzen basieren, sind das von Stuart C. Shapiro entwickelte "Semantic Network Processing System" (SNePS) und das MultiNet-Paradigma der mehrschichtigen erweiterten semantischen Netze von Hermann Helbig. Für beide Ansätze gibt es auch Werkzeuge zur Unterstützung von Wissensakquisition und -verarbeitung. MultiNet ist besonders auf die semantische Repräsentation natürlichsprachlichen Wissens ausgerichtet und wird in verschiedenen Anwendungen des Natural Language Processing eingesetzt.

Schon im Wintersemester 1789/90 beschrieb Johann Friedrich Flatt in seiner Tübinger Vorlesung zur empirischen Psychologie – der auch Georg Wilhelm Friedrich Hegel, Friedrich Wilhelm Joseph Schelling und Friedrich Hölderlin beiwohnten – ein Netzwerkmodell des Gedächtnisses, um assoziative Aktivierungen beim Abruf von Gedächtnisinhalten zu erklären.

Bereits um 1900 wurden von Psychologen, z. B. Gustav Aschaffenburg, Untersuchungen
durchgeführt, wie Begriffe in unserem Gehirn miteinander verknüpft sind. Dabei wurde
herausgefunden, dass bestimmte Wörter bei den meisten Menschen die gleichen Assoziationen
hervorrufen, beispielsweise weiß–schwarz oder Mutter–Vater. Durch Assoziationen
ist es möglich, Begriffe auf der semantischen Ebene miteinander zu verbinden,
d. h., dass Wörter aus einem bestimmten Grund eine Beziehung zueinander haben. Wörter
und Bedeutungen sind im mentalen Lexikon weder alphabetisch noch völlig unorganisiert,
sondern netzartig gespeichert. Die Bedeutung eines Wortes wird in einem solchen
Netz durch Knoten repräsentiert sowie durch Nachbarschaftsbeziehungen zu anderen
Inhalten bzw. Begriffen. Solche Netze lassen sich im Ansatz aus den gerade
erwähnten Assoziationen gewinnen.

Dieses Konzept der Assoziationen und Nachbarschaftsbeziehungen versucht das Semantische
Netz aufzugreifen. Dadurch kann es möglich werden, dass semantisch verwandte
aber syntaktisch vollständig verschiedene Begriffe im Semantischen Netz gefunden werden.
Die nebenstehende Abbildung zeigt ein derartiges mentales Assoziationsgebilde, welches auch die grundlegende Struktur für ein Semantisches Netz bildet.

Nach Tim Berners-Lee soll über den als Hypertext organisierten Teil des Internets (WWW) ein semantisches Netz (→ semantic Web) gespannt werden. Die Inhalte der Ressourcen, die diesen Hypertext bilden, sollen mit Metadaten beschrieben werden. Diese Ressourcenbeschreibung soll mit Hilfe eines Resource Description Framework (RDF) erfolgen. Als Modellierungssprache soll die Web Ontology Language (OWL) verwendet werden. Das Ziel ist, dass die Ausdrücke, die in den Metadaten verwendet werden, mit wohldefinierten und damit auch maschinell interpretierbaren Bedeutungen versehen werden. Dies würde z. B. die inhaltsbezogene Informationssuche ermöglichen. Dabei sollen nicht alle Begriffe global in einer komplexen Ontologie erfasst werden, sondern es soll ein eher loses Netz aus dezentralen spezialisierten Ontologien entstehen.

Mögliche Formate zur Repräsentation semantischer Netze sind RDF, RDF-Schema, OWL und XML Topic Map.





</doc>
<doc id="12135" url="https://de.wikipedia.org/wiki?curid=12135" title="Tabu">
Tabu

Ein Tabu beruht auf einem stillschweigend praktizierten gesellschaftlichen Regelwerk bzw. einer kulturell überformten Übereinkunft, die bestimmte Verhaltensweisen auf elementare Weise gebietet oder verbietet. Tabus sind unhinterfragt, strikt, bedingungslos, sie sind universell und ubiquitär, sie sind mithin Bestandteil einer funktionierenden menschlichen Gesellschaft. Dabei bleiben Tabus als soziale Normen unausgesprochen oder werden allenfalls durch indirekte Thematisierung (z. B. Ironie) oder beredtes Schweigen angedeutet: Insofern ist das mit Tabu Belegte jeglicher rationalen Begründung und Kritik entzogen. Gerade auf Grund ihres stillschweigenden, impliziten Charakters unterscheiden sich Tabus von den ausdrücklichen Verboten mit formalen Strafen aus dem Bereich kodifizierter Gesetze. Nahezu alle Lebewesen, Gegenstände oder Situationen, die ins menschliche Blickfeld rücken, können tabuisiert werden. Tabus können sich beziehen auf Wörter, Dinge (z. B. Nahrungstabu), Handlungen (z. B. Inzesttabu), Konfliktthemen, auf Pflanzen und Tiere, auf die Nutzung von Ressourcen (siehe Tapu), auf einzelne Menschen oder soziale Gruppen.

Die Begriffe 'Tabu' und Political Correctness haben eine Schnittmenge; sie sind nicht leicht voneinander abzugrenzen.

Der Begriff „Tabu“ stammt aus dem Sprachraum Polynesiens und ist aus dem Wort „tapu“ abgeleitet. "Tabu" als Begriff fand Anfang des 20. Jahrhunderts weitgehend Eingang in die deutsche Sprache – und zwar sowohl als Adjektiv („etwas ist tabu“) als auch als Substantiv („etwas ist ein Tabu“). Als Eigenschaftswort bezeichnet tabu einen Zustand, der mit „unverletzlich“, „heilig“, „unberührbar“ beschrieben werden kann: Tabuisierte Dinge – so die religiöse Vorstellung der Polynesier – müssten streng gemieden werden, da sie gefährliche Kräfte besäßen. Auf den Tonga-Inseln bedeutet tabu oder tapu ursprünglich „unter Verbot stehend“, „nicht erlaubt“. In seinem heutigen Gebrauch heißt das Wort auf Tonga „heilig“, „geheiligt“, aber durchaus auch in dem Sinn von „eingeschränkt“ oder „durch Sitte und Gesetz geschützt“, Beispielsweise wird die Hauptinsel des Königreiches Tonga Tongatapu genannt, was hier eher „heiliger Süden“ bedeutet als „verbotener Süden“. Auf Timor gibt es mit „Lulik“ ein vergleichbares Konzept.

Bereits Ende des 18. Jahrhunderts brachte der mit James Cook umherreisende Georg Forster das Wort "tapu" = "Gebot zu meiden" nach Europa. Als der englische Forscher und Seefahrer Cook 1777 Tonga entdeckte, nannte er den Archipel „die freundlichen Inseln“. Über die Bewohner Tongas schrieb er:

Andere Quellen führen den Ausdruck auf das auch in der Fidschi-Sprache existierende Wort tabu zurück. Laut Auskunft einiger Einwohner der Salomon-Inseln existiere auch in ihrer Sprache das Wort tabu (ausgesprochen „tam-boo“), was „heilig“ bedeutet. Das Wort bezieht sich auf Orte im Busch, wo heilige Geister wohnen. Gewöhnlich ist eine solche Stelle markiert mit einem Gegenstand, einer großen Muschel oder einem geschnitzten Stein. Solche Gegenden dürfen so lange nicht gestört oder betreten werden, bis eine Erlösungs-Zeremonie stattfindet. Bei den Mono Alu (Shortland, Salomon-Inseln) bezieht sich tabu auf diejenigen Totemtiere, die von den Clanmitgliedern nicht verspeist werden dürfen.

Eine Definition des Tabu-Begriffs gab Sigmund Freud in seinem grundlegenden Werk Totem und Tabu:

Entsprechend ist ein Tabu etwas zutiefst Verbotenes, steht aber auch für etwas Unausgesprochenes, ja Unaussprechliches, das weit über eine Einschränkung durch vernünftige Verhaltensformen (Sitte oder Gesetz) hinausgehen kann. Wir haben es vielmehr zu tun mit Scheubarkeitsschranken, die aus vorrationalen, instinktiven und/oder religiösen Haltungen des Abscheus oder auch der Ehrfurcht herrühren: Ursächlich aus einer Haltung heraus, die in einem einzigen, mit Tabu belegten Objekt zwei scheinbar gegensätzliche Aspekte, das extrem Reine und Heilige ebenso wie das Unreine bzw. das zu Meidende, gleichermaßen wahrnimmt. Jedenfalls zeigt sich eine fundamental empfundene Distanznahme, die – aus oft konkret erlebten kollektiven Erfahrungen der Gefährdung heraus – kulturell vermittelt wird (vgl. Essbares – nicht Essbares, Rituale um Geburt, Hochzeit, Krankheit, Sterben und Tod). Tabuistischem Verhalten am nächsten kommt vielleicht der ins Lateinische übersetzte Ausspruch Jesu Noli me tangere nach der Auferstehung an Maria Magdalena (Evangelium nach Johannes 20,17), er heißt übersetzt „Rühre mich nicht an“ oder „Berühre mich nicht“: Hier geht es um die Reinheit des Jenseitigen oder auch des mit dem Sterben Kontaminierten, aber zugleich auch des Geheiligten einerseits und um den Schutz der Diesseitigen vor dem Numinosum andererseits. Tabu ist immer in der Wechselseitigkeit ambivalenter Bezüge zu sehen. Tabuistisches Verhalten entspricht den kulturell erworbenen Techniken der Überbrückung einer gesamtgesellschaftlich wahrgenommenen Spannung zwischen Begrenzung und Grenzüberschreitung.

Wer in einer Welt von Verboten lebt, deren Sinn rational nachvollziehbar ist (z. B. Verbot der Euthanasie oder der Wahrung der Totenruhe), dem mag es schwer fallen, Normen als sinnvoll hinzunehmen, die einem unausgesprochenen, vorrationalen Bereich entstammen, also Tabus sind. Faktisch ist das mit „mana“ versehene Objekt oder Lebewesen unbegreiflich und unangreifbar. Vermutlich ist es ganz zentral der mit dem Stigma „Sterben“ und „Tod“ versehene Mensch, der ursächlich allgemeines Tabu-Empfinden auslöst – eine Haltung, die sich sekundär auf andere, als befremdlich erscheinende Lebewesen oder Dinge erstreckt. Aufgrund seiner Ähnlichkeit mit den noch Lebenden wird der plötzlich unwiderruflich andersartig gewordene Leichnam in allen Kulturen als das tabu-trächtigste Phänomen überhaupt erlebt, zeigt er doch mit seiner Körperlichkeit Reste seiner anthropomorphen Existenz, aber zugleich bereits Gefährdung durch Zerfall und Vergänglichkeit auf. Der verstorbene Mensch ist tabu schlechthin. Vermutlich entspricht tabuistisches Empfinden – ebenso wie auch das Bedürfnis nach Religiosität und Gesetzmäßigkeit – einem Bollwerk gegen ein existenziell wahrgenommenes Chaos: Tabus sollen magischen Schutz bieten gegen jeglichen Einbruch in bestehende und lebenserhaltende Ordnung. Sterben und die Sterblichkeit zeigen existenzielle Gefährdung auf, Totsein ist das Tabu par excellence. Der Tote selbst ist, da er sich nicht vernehmen lässt, auch nicht ansprechbar, hörbar, sichtbar, berührbar. Nach vorrationaler Logik ist es darum auch „unmöglich“, den Tod anzusprechen, zu sehen, zu hören, greifen, oder in anderer Weise haptisch die „andere Welt“ zu erkennen. Unerlaubte Kontaktaufnahme mit dem Verstorbenen käme einem Übergriff in die Welt des „Ganz Anderen“ und „Numinosen“ (Rudolf Otto), des Heiligen, des Jenseitigen usw. gleich, in eine Anderswelt also, welche zwar die Welt der Lebenden übermächtig beeinflusst, welche aber die Menschen normalerweise nicht tangieren dürfen: Möglicherweise unkontrollierbare magisch wirkende Mächte müssen gebannt bleiben. Analog zu einem streng ritualisierten Verhalten gegenüber Verstorbenen sind es darum die Sprach-, Sicht-, Handlungs- und Berührungstabus, die den unterschiedlichsten Lebensthemen anhaften, wenn sie nur den Geruch einer Störung des kulturspezifischen Lebensrhythmus vermitteln.

Kennzeichnend für Tabu-Verhalten sind folgende Merkmale:
„Tabu“ und „Mana“, beides Bezeichnungen aus dem polynesischen Sprachraum, sind zwei Bezugsgrößen, die, ähnlich wie Aktion und Re-Aktion, nicht voneinander zu trennen sind: „Mana“ ist das sich verselbständigende, Distanz-Verhalten auslösende Stigma (eines Lebewesens, eines Objekts, eines Zustandes), „Tabu“ ist die entsprechende Distanznahme.

Gesamtgesellschaftliches Tabu-Verhalten manifestiert sich in Korrelation zu tiefer liegenden Glaubensschichten, die sich aus eher angstbesetzten, magischen, animistischen oder dämonischen Vorstellungen heraus ergeben: Mit Tabu belegte Handlungen unterliegen stillschweigenden Übereinkünften, die tiefer in das allgemeine Verhalten eingreifen als sprachlich sanktionierte Verhaltenscodices von Gesetz und offiziell vermittelter Religion. Jenseits des kodifizierten Rechts gewährleisten Tabus eine nahezu maximale Übereinstimmung auf einer bestimmten Ebene des Verhaltens und Handelns einer sozialen Gruppe; dennoch ermöglichen sie einigen Wenigen, die sozusagen über die Einhaltung von Tabus „wachen“, enormen Einfluss: Das sind in der Regel mit besonderer charismatischer Kompetenz versehene, oft eigens ernannte oder geweihte, sakrosankte und jedenfalls mit der Wirkung von Ritualen vertraute Personen (Zauberer, Medizinmänner, Priesterinnen, Belegärzte, Chefärzte, Kaiserinnen, Staatsoberhäupter u.a.m.). Tabus stabilisieren das Machtgefüge einer Gesellschaft, indem sie mit der existenziell empfundenen Strafangst ihre Angehörigen manipuliert. Tabus sind ubiquitär: Tatsächlich existiert keine Gesellschaft ohne Tabus.

In der Vorstellungswelt etlicher indigener Volksgruppen (beispielsweise in Polynesien) gelten die Person des Stammesführers, aber auch Begräbnisplätze, Kultstätten, manche Naturschauplätze etc. als „tabu“. In Ozeanien ist der tabuisierte Bereich mit Mana, also mit „magischer Energie“ behaftet. „Tabus“ und „Mana“ sind Qualitäten, die nicht nur unsichtbar existent sind, sondern auch wirken, ja sogar extrem wirksam sein können.

Tabu können grundsätzlich Personen, Lebewesen, Dinge und auch jede beliebige – mit entsprechenden Glaubensvorstellungen behaftete – Örtlichkeit sein: ein Baum, eine verlassene Wohnung, ja ein einzelnes Besitzstück: Diese Bereiche sind vor Annäherung, Berührung oder Wegnahme zu "schützen". Aus kollektiv wahrgenommenem, tabuistischem Empfinden heraus entstehen magische Schutzrituale: Solchermaßen stigmatisierte (mit „mana“ behaftete) Bereiche werden in Polynesien zum Beispiel mit einem einfachen Faden umgrenzt oder eingebunden. Die Angehörigen der gleichen Ethnie sind überzeugt, dass bei Verletzung dieses Fadens alle Übel, die der Knotenschürzer hineingeknüpft hatte, über solchen „Schadenzauber“ unfehlbar auf sie fallen würden.

Tabuvorstellungen erzeugen solche und auch eine Vielzahl anderer Sicherungsmaßnahmen bei den verschiedensten Völkern überall auf der Welt: Man denke nur an die verschiedensten Praktiken aus „weißer“ oder „schwarzer“ Magie, in Anlehnung daran an den in Westafrika und den Nachfahren der Sklaven in der Karibik sehr lebendigen „Voodoo-Zauber“.

Weitere Beispiele: Tabus können sich auch auf bestimmte Nahrungsmittel beziehen, dergestalt, dass es zum Beispiel einer Sippe nicht erlaubt ist, ihr Totemtier zu jagen oder zu essen. Marginalisierte Gruppen kannten darüber hinaus häufig etliche ethnoreligiös begründete Jagdtabus, die dafür sorgen sollten, die Wildbestände zu schonen. Sehr weitreichend galt das zum Beispiel für die Aché Paraguays. In Afrika existieren spezielle, teils sehr weitreichende Nahrungstabus für schwangere Frauen, in abgeschwächter Form findet es sich in allen Gesellschaften. Diese können zum Beispiel wie die jüdischen Speisegesetze oder das Fasten (Ramadan, Passionszeit, Verbot von Schweinefleisch in Judentum und Islam) religiös motiviert sein, sich aber auch an beliebigen Traditionen und ethischen wie moralischen Einflussfaktoren orientieren. In Deutschland ist das Verspeisen von Hunde- und Katzenfleisch weitgehend tabu, in der Volksrepublik China dagegen der Verzehr von Kaninchen und in Nordamerika wiederum der Genuss von Pferdefleisch. Unter ein in nahezu allen Gesellschaften umfassendes Tabu fällt auch der Kannibalismus oder auch das Inzesttabu.

In polynesischen, aber auch anderen Gesellschaften kann es ein Tabu sein, den Namen der angeheirateten Verwandten oder kürzlich Verstorbener auszusprechen. Für die volkskundliche / ethnologische Forschung handelt es sich dann um ein Sprechverbot (wie es rund um den Erdball häufig auch in Volkserzählungen – Sagen und Märchen – erscheint), daneben gibt es auch Sichttabus, Berührungstabus und Handlungstabus.
Festzustellen ist der auffallend körperliche Bezug von Tabus, speziell auch die Sinnesorgane betreffend. Tatsächlich sind Tabus sinnlich-konkret repräsentiert, sie äußern sich über alle Wahrnehmungskanäle (sehen, hören, fühlen, schmecken und riechen).

Gesellschaftliche Veränderungen werden häufig durch Tabubruch erzeugt: Jede Revolution hat ihre Tabus brechenden Protagonisten und Märtyrer. Die verbale Missachtung von Tabus dient in vielen Kulturen als Anknüpfungspunkt für Witze und Schimpfwörter: Die unübliche, direkte Erwähnung eines Tabus erzeugt erzählerisch wirksame Spannung in der Zuhörerschaft. Tabubrüche können Ängste durchbrechen und den Tabugegenstand entmystifizieren. Manche Märchen enthalten die Botschaft, dass Heldinnen und Helden durch das Übertreten von Verboten zwar in Gefahr geraten, diese aber meist erfolgreich überstehen.

Der Begriff „Tabu“ ist aus soziologischer und sozialpsychologischer Sicht von besonderer Bedeutung. Tabus "schützen" ein Thema vor dem Diskurs in einer Gruppe, Gemeinschaft oder Gesellschaft: „Darüber spricht man nicht!“. Dem Thema wird kein Platz, kein „Ort“ im öffentlichen „Raum“ des Bezugssystems gewährt, es kommt in der öffentlichen Meinung nicht vor.

Je mehr Mitglieder des Bezugssystems sich an dieser Form der Ausgrenzung eines Themas beteiligen, desto mehr „Macht“ hat das Tabu über den Einzelnen. Kollektive Verdrängungsmechanismen werden wirksam („Das darfst du noch nicht einmal denken!“). Diese starke emotionale Aufladung ist der Grund dafür, dass „die direkte Erwähnung eines Tabus eine Spannung im Zuhörer erzeugt“ (siehe oben).

Gemeinsame Tabus stabilisieren die Bezugssysteme von Menschen, insbesondere aufgrund ihrer gemeinschaftlich erfahrenen emotionalen Aufladung. Mitglieder, die einen "Tabubruch" wagen, sind daher in der Regel schweren Sanktionen bis hin zum Ausschluss aus der Gemeinschaft ausgesetzt. Andererseits entlasten diese gesellschaftlich ausgegrenzten Personen sozusagen „pars pro toto“ die Gesamtgesellschaft, sie machen ihre „dunkle“ Seite deutlich und stellen sich als ein von der Bezugsgruppe immer wieder gewolltes, ja notwendiges „Opfer“ zur Verfügung: In ihren zugewiesenen Rollen als Märtyrer, als Außenseiter usw., der nun seinerseits tabuisiert wird: Das Opferlamm, das die Schuld der Menschheit trägt – ein nicht nur biblisches Menschheitsthema. Wenn allerdings der Zusammenhalt des Bezugssystems aus anderen Gründen gefährdet ist, werden wiederholte Tabubrüche die Gruppe nicht mehr stabilisieren, sondern die Übertretungen machen das Bezugssystem unglaubwürdig und beschleunigen seinen Niedergang.

Tabubruch gilt in der Regel als verabscheuungswürdig. Doch sind Tabus und gesetzliche Vorschriften nicht immer identisch. Einige Tabu-Handlungen oder tabuistische Gebräuche sind gesetzlich verboten und Übertretungen führen dann zu schweren Strafen.

Als „Tabuthema“ wird ein Thema bezeichnet, das nicht oder nur eingeschränkt öffentlich thematisiert wird. Oft handelt es sich dabei um Gebiete, die wunde Punkte einer Gesellschaft berühren.
Auch wenn heute in westlichen Ländern vielfach von einer „Gesellschaft ohne Tabus“ gesprochen wird, gibt es auch hier, wie in jeder Gesellschaft Tabuthemen, die insbesondere bestimmte Zustände der Körperlichkeit ansprechen, z. B. Sexualität, die Pubertät, Krankheit, Stuhlgang und Intimreinigung, Alter und Tod. Ein bedeutender Tabu-Bereich in manchen westlichen Gesellschaften sind die eigenen, persönlichen oder finanziellen Verhältnisse. In manchen Ländern, in denen die Heirat zwischen Cousinen und Cousins verbreitet ist, ist es tabu, über Folgen für Kinder aus solchen Verbindungen (Erbkrankheiten) zu reden.

Tabus gibt es in den verschiedensten Bereichen wie beispielsweise der Nahrung (Nahrungstabu) oder der Sprache (Sprachtabu oder Euphemismus). Im Bereich der Politik gibt es Standpunkte, die tabu sind wie beispielsweise die Diskriminierung von Minderheiten.
Politische Korrektheit nennt man einen Sprachgebrauch (z. B. Wortwahl), der durch eine besondere Sensibilisierung gegenüber Minderheiten gekennzeichnet ist und sich der Anti-Diskriminierung verpflichtet fühlt.

Ein Fauxpas (französisch für „Fehltritt“) ähnelt einem Tabubruch.

Ein Tabu ist etwas anderes als ein No-Go oder ein ausdrückliches Verbot.

Eine Übersicht von modernen Tabuthemen stellte die Wochenzeitung Die Zeit im April 2010 in einem ausführlichen Artikel zusammen.

In der DDR waren Forschungen zur Flucht und Vertreibung Deutscher aus Mittel- und Osteuropa 1945 bis 1950 mit Rücksicht auf die Sowjetunion und die sozialistischen Bruderstaaten vollkommen tabu. Eine bahnbrechende Verletzung dieses Tabus war der Roman „Kindheitsmuster“ von Christa Wolf, der 1976 erschien.

Die Darstellung von Homosexualität im deutschen Fernsehen war lange Zeit ein Tabuthema.

2011 schrieb Hans-Werner Frohn rückblickend zum Begriff Planung:
Die Diskussion um die Legitimität der Kirchensteuer in der säkularen deutschen Republik galt bis in die 1990er-Jahre hinein als ein politisches Tabu. Spätestens seit 1969 begann dieses Tabu zu wanken.

In Frankreich, speziell im französischen Staatsapparat, galt es lange als tabu, von „Algerienkrieg“ (Guerre d’Algérie) zu sprechen. Dieser fand von 1954 bis 1962 statt und endete mit einem Sieg der Algerier gegen Frankreich, das Algerien lange Zeit als Kolonialmacht besetzt hatte. Vielmehr sprach man euphemistisch von „événements d’Algérie“ (etwa: Ereignisse in Algerien). Am 18. Oktober 1999 wurde ein Gesetz verabschiedet, das den Terminus ‚Guerre d’Algérie‘ offiziell erlaubt. Eine nennenswerte gesellschaftliche Debatte über den massiven systematischen Einsatz von Folter, illegalen Hinrichtungen und weiteren französischen Kriegsverbrechen an Algeriern während dieses Krieges (siehe dazu "Französische Doktrin") fand erst in den Jahren 2000 bis 2002 statt.

In Norwegen war es von 1945 bis etwa 1980 tabu, darüber zu sprechen, wie Norweger nach dem Zweiten Weltkrieg mit über 10.000 Tyskerbarn (Kindern zwischen deutschstämmigen Besatzungssoldaten und Norwegerinnen) und ihren Müttern seit 1945 umgegangen waren. Etwa 8.000 dieser Besatzungskinder wurden im Rahmen des Lebensborn-Programmes gezeugt und geboren.

Die meisten Österreicher sahen bis nach dem Fall der Mauer (1989) Österreich als ein Opfer des Nationalsozialismus, weil das NS-Regime Österreich im März 1938 annektiert hatte („Anschluss Österreichs“). Dass es zuvor einen Austrofaschismus gab, ignorierten viele.
Kurt Waldheim, österreichischer Bundespräsident von 1986 bis 1992, war Gegenstand der Waldheim-Affäre; diese trug dazu bei, das Tabu zu brechen, Österreich sei nur Opfer und Österreicher seien keine NS-Täter gewesen.

Auch in österreichischen Institutionen bestand dieses Tabu. Zum Beispiel machte die ÖBB (Österreichische Bundesbahnen) erst im Juni 2012 die erste Ausstellung zu diesem Thema (Titel: "Verdrängte Jahre - Eisenbahn und Nationalsozialismus in Österreich 1938–1945").

Bundeskanzler Franz Vranitzky trug maßgeblich zur Auflösung dieses Tabus bei:
Am 8. Juli 1991 nahm er im Nationalrat die positive Einschätzung der „ordentlichen Beschäftigungspolitik“ des Nationalsozialismus durch den Kärntner Landeshauptmann und FPÖ-Vorsitzenden Jörg Haider zum Anlass für eine ausführliche Reflexion über die Rolle Österreichs im veränderten Europa vor dem Hintergrund der Geschichte.

Am 9. Juni 1993 hielt Vranitzky anlässlich seiner Israel-Reise eine Rede an der Hebräischen Universität Jerusalem und bat die Opfer der österreichischen Täter im Namen der Republik um Verzeihung.

Am 15. November 1994 hielt Bundespräsident Thomas Klestil als erster Präsident der Republik Österreich eine Rede vor der Knesset, in der er von einem „schweren Erbe der Geschichte, zu dem auch wir Österreicher uns bekennen müssen“ spricht.
1998 setzte die Republik Österreich eine Historikerkommission ein. Sie legte am 24. Februar 2003 ihren Abschlussbericht vor.
Am 26. Oktober 2000 wurde das von Simon Wiesenthal initiierte Holocaust-Denkmal (Wien) am Judenplatz eröffnet.

Im Ostblock war es bis zu Glasnost, Perestroika und zum Fall des Kommunismus tabu, über Verbrechen der Roten Armee im Zweiten Weltkrieg und danach zu sprechen oder zu schreiben.

Zu Beginn des Prager Frühlings (1967/8) ging der Journalist und Romanautor Ludvík Vaculík hart mit dem Herrschaftssystem in der Tschechoslowakei ins Gericht und brach dabei zahlreiche Tabus.

Es gab Zensurbehörden (Glawlit, Militärzensur) (→Zensur in der Sowjetunion); sie übten Überwachungsdruck aus. Viele Autoren praktizierten Selbstzensur (vorauseilender Gehorsam). Bis etwa 1987 gab es Samisdat- und Tamisdat-Literatur; dort brachen Autoren Tabus bzw. schrieben Dinge, die sie sonst nicht schreiben durften.

Zivilcourage brachte den Mutigen oft lange Haftstrafen ein (siehe zum Beispiel Charta 77).

In Syrien war es nicht ausdrücklich verboten, vom Massaker von Hama zu sprechen; es galt gleichwohl in der syrischen Bevölkerung als Tabu, bis Demonstranten während des Arabischen Frühlings 2012 das Thema bei ihren Protestmärschen aufgriffen.

In der Türkei ist es immer noch tabu und war es lange Zeit strafbar, über den Völkermord an den Armeniern (Anfang des 20. Jahrhunderts) zu sprechen. Zwar werden die entsprechenden Ereignisse selbst im Wesentlichen nicht abgestritten, wohl aber vehement, dass sie in der Gesamtheit "Völkermord" waren.

In den USA taucht in keinem Lebenslauf eines Staatsbediensteten, in keinem Bewerbungsverfahren die Information auf, welche Konfession ein Bewerber hat. Es ist für den Arbeitgeber tabu, danach zu fragen, und für den Bewerber, diese Information zu geben, denn Religion ist Privatsache (Näheres hier). „… das Thematisierungsverbot ist Amerikas Rezept, um den Anschein zu wahren, dass wirklich keine Rolle spielt, was gemäß dem Anspruch des säkularen Staats keine Rolle spielen darf.“

In der US-Army galt bis Dezember 2010 der Grundsatz Don’t ask, don’t tell. Seine Praktizierung machte es für homosexuelle Mitglieder der US-Army tabu, sich zu outen.

Stephan J. Kramer (Generalsekretär des Zentralrates der Juden in Deutschland und Leiter des Berliner Büros des European Jewish Congress) sagte im Oktober 2012, die Beschneidung von männlichen Neugeborenen am achten Tag nach ihrer Geburt dürfe kein Tabu im Judentum sein.

Nach dem Zweiten Weltkrieg wurden Wirtschaftskartelle verboten und kriminalisiert. Vorher waren diese unternehmerischen Zusammenschlüsse legal und öffentlich erkennbar gewesen. Besonders in den 1930er und frühen 1940er Jahren waren sie sogar Instrumente staatlicher Wirtschaftspolitik gewesen. Die (tatsächlich schädlichen) heutigen Frühstückskartelle, die verschwiegen arbeiten (müssen) und vor allem die Preise hochtreiben, wurden bedenkenlos mit ihren nützlicheren Vorgängern, die Absatz und Produktion vielfältig rationalisiert hatten, in einen Topf geworfen. Über Kartelle allgemein soll nicht positiv gesprochen werden; der herrschende Umgang mit Kartellen gilt kritischen Historikern als „Ungereimtheit“ bis hin zu „Rufmord“. Auffällig ist, dass die zum Teil stattlichen Gebäude von früheren Groß-Kartellen, meist Syndikaten mit gemeinsamer Absatzverwaltung, in keinem Fall wegen ihrer besonderen Herkunft unter Denkmalschutz gestellt wurden. Das Essener Ruhrkohlehaus wurde stillschweigend abgerissen; das Walzstahlhaus und der Stahlhof in Düsseldorf stehen zwar unter Denkmalschutz, aber ohne eine erläuternde Ausschilderung als ehemalige Kartellgebäude.

Für viele Prostituierte ist es ein Tabu, ihrem privaten Umfeld zu sagen, dass sie als Prostituierte arbeiten. Für Eltern von Prostituierten kann es ein Tabu sein, anderen zu sagen, womit ihre Tochter oder ihr Sohn den Lebensunterhalt verdient.

Viele Theaterskandale gelten als solche, weil sie an gesellschaftliche, moralische, religiöse oder künstlerische Tabus rühren. Es kommt dabei zu Missfallenskundgebungen, Protesten oder sogar Tätlichkeiten im Zuschauerraum, gelegentlich auch zu Zeitungskampagnen oder politischen Konsequenzen bis hin zu Zensur oder Verbot.




</doc>
<doc id="12136" url="https://de.wikipedia.org/wiki?curid=12136" title="Skythen">
Skythen

Als Skythen werden einige der Reiternomadenvölker bezeichnet, die ab etwa dem 8./7. Jahrhundert v. Chr. die eurasischen Steppen nördlich des Schwarzen Meeres im heutigen Südrussland und der Ukraine von der unteren Wolga und dem Kuban bis zum Dnister besiedelten. Sie wurden im 4./3. Jahrhundert v. Chr. von den kulturell nahestehenden Sarmaten, die sich als Stammesverband zuvor zwischen der unteren Wolga und der Südspitze des Ural gebildet hatten, unterworfen und assimiliert, ein Teil flüchtete auf die Krim, wo noch bis ins 3. Jahrhundert n. Chr. skythische Stammesverbände lebten.

Sie hinterließen keine bekannten schriftlichen Aufzeichnungen, und alles, was man über sie weiß, beruht auf Bodenfunden und antiken Quellen anderer Kulturen. Nach dem antiken griechischen Geschichtsschreiber Herodot nannte sich der herrschende Klan "Skoloten"; die Bezeichnung "Skythen" stammt aus griechischen Quellen, ist jedoch nicht griechisch. Ihre Sprache wird dem (alt-)nordost-iranischen Zweig der indogermanischen Sprachen zugerechnet.

Griechische und römische Quellen bezeichnen manchmal pauschal das gesamte Gebiet der kulturell und wohl auch sprachlich nahe verwandten Reiternomaden Osteuropas und Mittelasiens im 1. Jahrtausend v. Chr. als "Skythien". Dort lebten u. a. auch die Stammesverbände der Saken (vgl. auch die griechische Bezeichnung der nach Indien ausgewanderten Saken als „Indo-Skythen“), Sarmaten und Massageten. In der Archäologie wird dieser Kulturraum Skythiens im weiteren Sinne als „skythisch-sakischer Kulturraum“ oder „Skythisch-sakischer Horizont“ bezeichnet. Zu ihm zählen als älteste Kulturen (seit dem 9. Jahrhundert v. Chr.) auch einige archäologische Kulturen Südsibiriens wie die Tagar-Kultur (Minussinsker Becken), Pasyryk-Kultur (Altai), Aldy-Bel-Kultur (Tuwa) und die Tes-Stufe (Tuwa). Diese sind nicht aus Schriftquellen bekannt, die sprachliche und ethnische Zugehörigkeit ihrer Träger ist unbekannt, aber ihre materielle Kultur ähnelt derjenigen der Skythen am Schwarzen Meer. Aufgrund des Alters dieser südsibirischen Kulturen, der archäologisch erforschten Ausbreitung dieser Kultur vom Osten in den Westen und Südwesten und Herodots Angaben, dass die Skythen aus dem Osten kamen, gehen Archäologen von einer Herkunft der Skythen, Saken u. a. aus dieser Region aus. Eine nach Osten abgewanderte Splittergruppe bildete die Ordos-Kultur.

Nach bisherigen archäologischen Erkenntnissen waren die Stammesverbände des skythisch-sakischen Kulturraums die ersten in der Geschichte der Steppen Asiens und Europas, die (bis auf wenige Ausnahmen) auf jahreszeitlich genutzte feste Ansiedlungen mit bescheidenem Ackerbau verzichteten und zum ganzjährig nomadisierenden Leben als Reitervolk übergingen.

Ab dem 3. Jahrhundert v. Chr. teilten die Griechen die Völker im Norden in zwei Gruppen ein: "Kelten" westlich des Rheins und "Skythen" östlich des Rheins, insbesondere nördlich des Schwarzen Meeres. Der Begriff "Skythen" diente später also meist nur als grober Oberbegriff für eine große Anzahl verschiedener barbarischer Völker.

Die Verwendung des Begriffs "Germanen" für die östlich des Rheins siedelnden Stämme ist erstmals vom griechischen Geschichtsschreiber Poseidonios um das Jahr 80 v. Chr. überliefert. Als "Kelten" wurden pauschal die westlich des Rheins lebenden Stämme bezeichnet. Endgültig eingeführt wurde dieses Schema von Gaius Iulius Caesar. Als Tacitus seine Germania schrieb, war dies eine als neu bekannte, aber bereits übliche Bezeichnung. Damit war nun eine Dreiteilung der Völker des Nordens und Ostens in Kelten, Germanen und Skythen üblich. Die obigen Einteilungen sind nach heutigem Kenntnisstand und modernen Anforderungen „falsch“ oder zumindest ungenau.

Im 3. Jahrhundert n. Chr. (etwa von Dexippos) sowie zur Zeit der Völkerwanderung (spätes 4. bis spätes 6. Jahrhundert) wurden alle Völker am Nordrand des Schwarzen Meeres von den klassizistisch orientierten Geschichtsschreibern als "Skythen" bezeichnet, etwa die Goten und später die Hunnen. Beispiele sind unter anderem Ammianus Marcellinus (20,8,1) oder die Berichte des Geschichtsschreibers Jordanes. Wie später "Hunnen" war das Wort zu einer allgemeinen Bezeichnung steppennomadischer Völker geworden. Für Jordanes grenzt Skythien an Germanien, es erstreckt sich vom Ister (der unteren Donau) bis an den Tyras (Dnister), Danaster (Donez) und Vagosola und bis zum Kaukasus und zum Araxes, einem Nebenfluss der Kura in der südlichen Kaukasusregion. Im Osten grenzte es an das Land der Seren (Kaspisches Meer), im Norden an der Weichsel an jenes der Germanen. Im Skythenland lägen die Riphäischen Berge (Ural), die Asien und Europa trennen, und die Städte Borysthenes, Olbia, Kallipodia, Chesona, Theodosia, Kareon, Myrmikon und Trapezunt, "welche die wilden Skythenvölker von den Griechen gründen ließen, damit sie Handel mit ihnen treiben konnten" (Gotengeschichte, 5). Auch in vielen byzantinischen Geschichtswerken, die in der klassizistischen Tradition standen, wurden fremde Völker an der Donau als Skythen bezeichnet.

Herodot berichtet, dass die Skythen von den Persern "Saken" genannt wurden. Wie im spätantiken und im mittelalterlichen Europa war bei den Persern "Skythe"/"Sake" oft einfach eine allgemeine Bezeichnung für jeden "barbarischen" Steppenbewohner (siehe dazu Ethnogenese, Reitervölker). Altpersische Inschriften aus dem 6. Jh. nennen drei Gruppen der Saka: "Paradraya", "Tigraxauda" und "Haumawarga". Zumindest die "Haumawarga" sind als Hauma bzw. soma-trinkende Indoarier auch in Indien bekannt, sodass hier wohl nur von den östlich des Tigris lebenden Skythen die Rede ist, die in dieser Zeit stark östlich des Kaspischen Meeres und in Nordindien präsent waren, was durch Tausende von Kurgane dieser Epoche auch sehr gut belegt ist. Im engeren Sinn bezeichnet dieser Name Stämme der Saken, deren Siedlungsgebiete hauptsächlich in der Kasachensteppe lagen.

Die Skythen tauchen in den assyrischen Quellen erstmals unter Sargon II. auf. Zur Zeit Assurhaddons (680–669 v. Chr.) verbündeten sie sich unter Išpakai mit dem Mannäer-Reich am Urmia-See und griffen die Assyrer an. Unter einem gewissen "Bartatua"/"Partatua" treten die Skythen als Verbündete der Assyrer auf, vielleicht wegen einer Heirat mit einer Tochter Assurhaddons.

Kimmerer und Skythen werden in den assyrischen Quellen oft als "umnan-manda" zusammengefasst, was jedoch ebenfalls eine recht ungenaue Bezeichnung darstellt, die sich generell auf Bergvölker bezieht. Ähnliche Bezeichnungen sind bereits von Akkadern in Zusammenhang mit älteren erwähnten Bergvölkern unbekannter Herkunft genannt worden.

Das Königreich Aschkenas, das von Jeremia (51, 27) zusammen mit Ararat (Urartu), Minni (Mannäer) zu einem Angriff auf Babylon aufgefordert wird, wird meist als skythisch identifiziert. Der entsprechende Text dürfte nach 594 formuliert worden sein. Die Form "Aschkenas" beruht auf einer Verwechslung, die auf die Ähnlichkeit der hebräischen Zeichen Waw (für „u“) und Nun zurückgeht. Die ursprünglich assyrische Form war "(A)sch-ku-zaa" oder "(I)sch-ku-zaa", soll (aufgrund von skythischen Gräbern) dem griechischen "Skythai" entsprechen.

In der Völkertafel der Genesis (Gen 10,3) taucht Aschkenas als Kind Gomers, des Sohn Japhets auf. Gomer wird mit den Kimmerern gleichgesetzt, wobei sich die Völkertafel weitestgehend auf das 1. bis 3. Jhd. v. Chr. bezieht. Ältere Vorstellungen entstammen wohl aus Babylonisch-assyrischen Bibliotheken während des Babylonischen Exils. Paulus erwähnt die Skythen (Σκύθης) um das Jahr 60 n. Chr. in seinem Brief an die Kolosser (3,11) und unterscheidet sie von anderen nichtgriechischen Völkern (βάρβαροι).

Die Skythen wurden von dem griechischen Historiker Herodot detailliert beschrieben.

Danach gab es vier Hauptabteilungen der Skythen: die Aucheten, Nachkommen von Leipoxais, dem ältesten Sohn des Gründerheros Targitaos; die Katairen und Traspier, Nachkommen des mittleren Sohnes Arpoxais; und die Paralaten oder königlichen Skythen, Nachkommen des jüngsten Sohnes Kolaxais (Herodot 4,6). Dieser Name taucht auch bei Alkman von Lesbos und Valerius Balba (70–96 v. Chr.) auf. Alle diese Abkömmlinge zusammen würden sich Skoloten nennen, die Griechen nannten sie Skythen. 

Wenige Seiten weiter beschreibt Herodot eine weitere Aufteilung der Skythen nach der Wirtschaftsweise. Ackerbau treibende Skythen wohnten danach im Lande Hyläa (von griech. ὕλη, "hýlē", ‚Wald‘, vermutlich ‚Berghochwald‘) zwischen Borysthenes (Dnepr) und Hypanis (Südlicher Bug), bis zum Fluss Pantikapes und elf Tagesreisen nach Norden. Sie nannten sich selbst Olbiopoliten (Olbia Polis). Östlich der Olbiopoliten beginnt die Steppe, hier lebten nomadische Skythen am Gerrhus (das Flüsschen Molotschna sowie dem größeren Tokmak, die heutige Bezeichnung des Oberlaufes der Molotschna). Wiederum östlich davon (gemeint ist östlich des Asowschen Meeres) lebten die königlichen Skythen, „die alle anderen Skythen für ihre Sklaven halten“ und am zahlreichsten waren. Ihr Siedlungsgebiet reichte bis an die Krim und den Tanais (Don). Östlich von ihnen siedeln die Sauromaten, nördlich davon die Melanchlänen, so benannt nach ihren schwarzen Mänteln, beides nach Herodot keine skythischen Stämme, obwohl die Melanchlänen skythische Sitten angenommen hatten (4,107).

Herodot gibt zahlreiche Berichte über die Entstehung der Skythen wieder. In einem davon (4,11), der vermutlich auf Hekataios von Milet und Aristeas von Prokonnesos zurückgeht, heißt es, die Skythen seien von den Massageten bedrängt worden und daraufhin über den Araxes (Aras) in das Land der Kimmerer eingefallen, die vor ihnen nach Asien flohen. Als Beleg führt Herodot zahlreiche Ortsnamen im Skythenland an, die auf die Kimmerer hinweisen.

Ob die „trefflichen Hippemolgen, dürftig, von Milch genährt“ (Ilias, 13. Gesang, 5–6) Kimmerer, Skythen oder einen anderen Stamm der nördlichen Schwarzmeerküste bezeichnen sollen, ist umstritten. Diese Stelle gilt manchen Forschern als die erste schriftliche Erwähnung der Skythen.
Vermutlich sind damit pauschal geschickte Reitervölker gemeint.

Nach Diodor wurde Skythes, der eponyme Heros der Skythen und König von Hylaia (am Borysthenes), ein Sohn des Zeus und einer schlangenfüßigen Göttin namens Echidna, am Tanais geboren. Seine Brüder sind Agathyrsos (vermutlich der sarmatische Stamm der Agathyrsen) und Gelonos (eventuell die Geten).

Das Werk des Hellanikos von Lesbos über die Skythen ist nur in wenigen Fragmenten überliefert. Auch Hippokrates von Kos, Aischylos (gefesselter Prometheus), Sophokles, Euripides (Iphigenie bei den Taurern, Rhesos), Pindar, Thukydides, Theopompos und Aristophanes überliefern einige Details über die Lebensweise und die Wohnsitze der Skythen und Sauromaten.

In den griechischen Quellen der klassischen Zeit werden die Skythen als typische Barbaren beschrieben, die gebrochenes Attisch sprachen und seltsame Beinkleider (Hosen) trugen. Wein unverdünnt zu trinken wurde geradezu als Trinken auf "skythische Art" bezeichnet und auch den Germanen nachgesagt. Der Spartanerkönig Kleomenes übernahm diese Unsitte von den Skythen und starb daraufhin im Delirium. Daraufhin soll die Wortschöpfung "ἐπισκυθίζειν" (= un- oder wenig verdünnten Wein trinken) entstanden sein. Nicht nur skythische Männer, sondern auch die Frauen sollen unverdünnten Wein getrunken und das Vergießen von Wein auf ihre Kleidung für einen vortrefflichen Brauch gehalten haben.

Arrian unterschied asiatische (Abier) und europäische Skythen, letztere nannte er das zahlreichste aller europäischen Völker. Die Abier bzw. "Abioi" kommen bereits in der Ilias vor (13,6), wo sie als gerechteste aller Erdenbewohner gerühmt wurden. Fraglich ist jedoch, ob Homer damit auch wirklich Skythen gemeint hat.

Quintus Curtius Rufus (7,7,1) nannte den Tanais als Grenzfluss zwischen den europäischen Skythen und Baktrien wie auch zwischen Europa und Asien. Dies erklärt sich daraus, dass einige antike Geographen den Amudarja für den Oberlauf des Tanais (Don) hielten und Asien in ihrer Vorstellung relativ kurz war. Von der wahren Ausdehnung Asiens hatten sie keinerlei Vorstellung. Diese Vorstellung der Welt hielt sich bis ins späte Mittelalter. Rufus sah daher die Skythen als Teil der Sarmaten an. Ihre Siedlungsgebiete lägen „unweit von Thrakien“, von der Waldgegend jenseits der Ister (Donau) bis nach Baktrien. Die damalige Vorstellungswelt kannte schlichtweg keine weiteren Völker Asiens. Rufus lobte die Skythen als nicht so roh und ungebildet wie die übrigen Barbaren, einige von ihnen seien „sogar für die Lehren der Weisheit empfänglich, soweit diese für ein immer unter den Waffen befindliches Volk fassbar sind“. (7,8,10).

Strabo unterschied Skythen und Sauromaten nicht, gilt aber ansonsten als wichtige Quelle. Unter den griechischen und römischen Autoren finden sich auch bei Plinius d. Ä., Orosius, Lukian, Horaz und Chrysostomos Angaben über die Skythen.

In Athen dienten skythische Sklaven zwischen der Mitte des 5. Jh. und dem 4. Jh. als bewaffnete Schutztruppe (Toxotai/Speusinoi), wie aus einer Rede von Andokides "Über den Frieden mit den Lakedaimonern" (391 v. Chr.) bekannt ist. Das polizeiähnliche Korps bestand aus 300, später 1000 Bogenschützen und war erst auf der Agora, später auf dem Areopag stationiert. Sie unterstanden dem Rat der 500 und sorgten primär für deren Sicherheit sowie für die Ordnung der Volksversammlungen und Gerichtsverhandlungen. Vermutlich wurden sie auch zur Unterstützung des Beamtenkollegiums der Elf eingesetzt. Zu deren Zuständigkeitsbereich gehörte die Aufsicht über das Staatsgefängnis und die Hinrichtung von geständigen oder auf frischer Tat betroffenen Verbrechern ("kakourgoi"). Die so verwendeten Skythen tauchen auch in den Komödien des Aristophanes auf (Acharner) (425), Die Ritter (424), Thesmophoriazusen (411) und schließlich Lysistrata (411) aus dem gleichen Jahr. Wahrscheinlich wurde die Einheit um 390 v. Chr. aus Kostengründen aufgelöst.

Wie Frolov (2000) ausführt, gab es in Athen neben den Staatssklaven der Schutzeinheit auch skythische Sklaven in Privatbesitz.

In den mittelalterlichen "mappae mundi" (Weltkarten) des 10. bis 13. Jhs. (beispielsweise Hereford-Karte, Ebstorfer Weltkarte) wurden die Skythen auf dem Gebiet der Kiewer Rus, westlich des Tanais (Don) eingezeichnet, wobei die Sarmaten zwischen Germanien und Skythien liegen. Dieses Skythien liegt nördlich des Schwarzen Meeres zwischen der unteren Donau und reicht bis zum Don. Eingezeichnet wurden dabei auch drei Bezeichnungen; Scitotauri (Königsskythen) in der Region Kiev, Scirhans (Skiren) südwestlich davon sowie in Chesona. Östlich des Don zeichnete man gewöhnlich Gog und Magog in ihrem Gefängnis, der Alexanderburg, ein. Dahinter liegt das Land der Greife, das gemäß dieser Vorstellung nicht größer als Thrakien war. Über die Völker jenseits des Tanais (Don) hatten die Kartenzeichner keine genaue Vorstellung; auch ist zu beachten, dass die Darstellung dieser Karten mehr von der Theologie als von geographischen Erkenntnissen geprägt war. Unmittelbar danach schließen Baktrien, China und Indien an. Mit Asien wird der Orient von Anatolien bis Indien bezeichnet. 
Die in Asien tatsächlich im Mittelalter ansässigen Völker (Chasaren, Petschenegen, Kumanen und Wolgabulgaren) waren zumindest im östlichen Europa bereits wohlbekannt. 
Die mittelalterlichen Karten orientieren sich in ihrem Aufbau an Karten oder Beschreibungen des antiken Geographen, Mathematikers und Philosophen Ptolemaios im 2. Jh., wobei Jerusalem nun in das Zentrum der Welt rückte. Weiter ergänzt wurden die Karten durch die mittelalterlichen Alexanderromane. Dies widerspricht der heute gängigen Sicht auf die Skythen, entspricht jedoch der Wortwahl des Mittelalters, in der auch Wikinger, Germanen, Slawen und Sarmaten als Skythen definiert wurden. In den ersten Mappa Mundi wurde die Welt des Ptolemaios einfach mit dem Wissen des Mittelalters erweitert.

Funde von der ersten Hälfte des 7. Jahrhunderts und dem späten 4. Jahrhundert v. Chr. aus dem nördlichen Schwarzmeergebiet werden wegen der Angaben Herodots in der Archäologie als "skythisch" bezeichnet. Diese spezielle materielle Kultur mit Verzierungen im skythischen Tierstil, eisernen Kurzschwertern, Lamellenpanzern, Bronzekesseln mit hohem Standfuß, speziellen Formen der Trensenknebel, Katakombengräbern unter Grabhügeln und anthropomorphen Großplastiken ist jedoch über ein wesentlich weiteres Gebiet verbreitet.

Während die meisten russischen und ukrainischen Archäologen den Begriff "Skythen" auf Funde zwischen dem Bug und dem Kuban und an der Küste des Asowschen Meeres beschränken, also dem Gebiet, in dem nach Herodot Stämme lebten, die sich selbst als Skythen bezeichneten, wird der Begriff im Westen meist auf die gesamte nordpontische (nördlich des Schwarzen Meeres) und westsibirische reiternomadische Kultur der frühen Eisenzeit übertragen und umfasst damit mit Sicherheit auch Stämme, die sich selbst "nicht" als Skythen bezeichneten.

Die materielle Kultur, die traditionell den Kimmerern zugeschrieben wird (Funde bei Tschernogorowka – heute Siwersk bei Bachmut – und Nowotscherkassk), endet im 7. Jh. abrupt und wird durch skythische Funde abgelöst. Dies stützt die Angaben Herodots über den Einfall der Skythen, die nach Meinung einiger Forscher aus dem Altai-Gebiet gekommen sein sollen. Seit dem 7. Jh. finden sich auch in der Koban-Kultur des nördlichen Kaukasus deutliche skythische Einflüsse.

Die archäologischen Funde stammen vor allem aus Ausgrabungen von Grabhügeln (Kurgane), die unter anderem Gold, Seide, Waffen, Pferde und Bestattungen enthielten. Ein unversehrter Kurgan wurde im Juli 2001 im Tal der Zaren bei Aržan in der südsibirischen Republik Tuwa entdeckt. Der Sensationsfund mit Tausenden von Goldobjekten gelang dem deutschen Archäologen Hermann Parzinger aufgrund von Reiseberichten über Kurgane von Reisenden des 18. Jahrhunderts. Der teilweise sehr gute Erhaltungszustand der Überreste, wie in den Kurganen von Pazyryk, ist Mumifizierungstechniken und dem sibirischen Permafrost zu verdanken.

Im Sommer 2006 wurde im Permafrostboden des Altaigebirges in Tuwa von Hermann Parzinger und Mitarbeitern des Deutschen Archäologischen Instituts in Kooperation mit russischen Archäologen aus einer Grabkammer die Eismumie eines skythischen Reiterkriegers geborgen. Ihr Alter wurde auf 2500 Jahre geschätzt. Außerdem liegen Dendro-Daten der Kammer vor. Die Mumie trug einen prächtigen Pelzmantel und einen kunstvoll verzierten und vergoldeten Kopfschmuck. Auch ein Kompositbogen ist erhalten.

Archäologische Belege für eine skythische Präsenz in Anatolien, von der sowohl griechische als auch assyrische Quellen berichten, sind, abgesehen von dreiflügligen Pfeilspitzen (siehe unten), spärlich.

Ein Grab aus İrminler, Provinz Amasya am Südrand des Pontus enthielt neben 21 zweiflügligen Bronzepfeilspitzen ein eisernes Langschwert mit herzförmigem Heft, einen Streitpickel, wie er für das Altai-Gebiet typisch ist, einen goldenen Armreif und eine Trensenstange. Die Grabkammer war mit einer Trockenmauer eingefasst und 2,8 m lang. Die Bestattung war modern gestört, enthielt aber Knochen von Menschen und Pferden.

Ein weiterer Fund aus dem Schwarzmeergebiet (Provinz Amasya) geht auf Raubgrabungen zurück und ist ohne genauen Fundort. Hier lagen 250 zweiflüglige Pfeilspitzen in einem Grab. Die Gräber werden in das 7. und frühe 6. Jahrhundert datiert. Auch hier ist aber nicht sicher zu sagen, ob es sich um kimmerische, skythische oder sarmatische Krieger handelt; das Langschwert spricht vielleicht eher für letztere.
Der Goldschatz von Ziwiye (Iran) aus einem Grab aus der zweiten Hälfte des 7. Jahrhunderts enthält sowohl skythische als auch rein vorderasiatische Gegenstände, die vermutlich Kriegsbeute darstellen. Auch die Nekropole von Sé Girdan im Uschnu-Tal scheint skythische Elemente zu enthalten.

Manche Archäologen wie Hans Albert Potratz nehmen einen skythischen Einfluss auf die assyrische Bewaffnung an, so im Falle der mondsichelförmigen Trensenknebel und der Bogenfutterale.

Ab dem 6. Jahrhundert finden sich griechische Importe im Gebiet der Skythen, besonders rhodische Weinkrüge (Oinochen).

Schwarz- und besonders rotfigurige Vasen aus Athen zeigen skythische Bogenschützen, die an ihrer enganliegenden Kleidung mit Hosen und den spitzen skythischen Mützen zu erkennen sind. Oft benutzten sie einen Reflexbogen, der jedoch auch zur Bewaffnung der Griechen gehörte (zum Beispiel Äginetenfries). Diese Darstellungen wurden als Beleg dafür gesehen, dass die Skythen athenischen Vasenmalern aus eigener Anschauung vertraut waren. Man nahm an, dass diese als Leibwache des Tyrannen Peisistratos und seiner Söhne in Athen weilten. Die Schriftquellen kennen jedoch nur thrakische Söldner und sogenannte „wolfsbeinige“ Sklaven.

Inschriften aus Olbia und dem Bosporanischen Reich überliefern Details zu Feldzügen gegen die Skythen.

König "Kanita" (3. Jahrhundert) prägte in Istros, "Skiluros" (2. Jahrhundert) in Olbia Münzen.

Seit dem 2. Jahrhundert wird es immer schwieriger, die skythische und sarmatische materielle Kultur zu trennen. Vermutlich kam es zu einer allmählichen Assimilation. Eine genaue archäologische Abgrenzung zu den für Mittelasien überlieferten, aber aufgrund erhaltener Inschriften iranischsprachigen Saken ist ab dieser Zeit ebenfalls schwierig.

Der Name der iranisch-afghanischen Region Sistan leitet sich von "Sakistan" ab, nach den Saken, die sich dort vor 120 v. Chr. ansiedelten.

Eine sakische Stammesföderation wanderte im 1. Jahrhundert v. Chr. aus dem östlichen Mittelasien nach Indien ein und begründete dort die kurzlebige Indo-Skythische Dynastie. Eine Anmerkung auf einer alten Stele der Edikte des Ashoka anlässlich einer Staudammreparatur im Jahr 150 v. Chr. schrieb "Rudradamana", ein Reichsleiter der Saken. Sie gilt als erstes schriftliches Zeugnis des Sanskrit.

Die Indo-Saken, von griechischen Geografen ihrer Zeit „Indo-Skythen“ genannt, wurden in Nordwestindien in die Adels- und Krieger-Kaste der Kshatriya integriert und allmählich sprachlich assimiliert, behielten aber länger eigene Bräuche und religiöse Kulte. Nach Zerstörung des indo-skythischen Reiches durch die indoarischen Kuschana gründeten Indo-Saken in Westindien das unabhängige Reich der Westlichen Satrapen (ca. 35–405 n. Chr.; ihre aus dem Persischen stammende Bezeichnung "Kshatrapa"/Satrap bezeichnete ursprünglich einen Provinzgouverneur, entwickelte sich hier aber zu einem Herrschertitel.)

Im 8. Jahrhundert v. Chr. fielen die Skythen in die Gebiete nördlich und östlich des Schwarzen Meeres ein und verdrängten die Kimmerer. Zwischen 630 und 625 v. Chr. unternahmen die Skythen einen Vorstoß nach Vorderasien, und Raubzüge bis nach Palästina. Herodot berichtet, wie sie durch Psammetich I. (670–626) gegen Lösegeld zum Abzug bewogen wurden. Auf dem Rückweg sollen sie Askalon geplündert und zerstört haben. 609 berichten babylonische Quellen, dass die Skythen in das Gebiet von Urartu eingedrungen seien, 608 wird von skythischen Ansiedlungen am Oberlauf des Tigris berichtet. Der Fall von Urartu im letzten Drittel des 7. Jahrhunderts v. Chr. wird daher auch auf Skythen zurückgeführt, wahrscheinlich aber als Verbündete des Medischen Reiches. Angaben Herodots über die Zerstörung Urartus durch Meder und Überlegungen zur Chronologie des Mächteverhältnisses in der Region lässt Forscher die Zerstörung Urartus aber hauptsächlich dem Medischen Reich zuschreiben. In den Brandschichten von Bastam, das allerdings schon Mitte des 7. Jh. zerstört wurde, und von Tušpa (Van), Toprakkale, Teischebani (Kamir Blur) bei Jerewan und Argištiḫinili fanden sich dreiflügelige Bronzepfeilspitzen und „skythisches“ Pferdegeschirr. Manche Forscher nehmen allerdings an, dass die Pfeilspitzen in Teischebani, die nicht in den Mauern, sondern in Vorratsräumen gefunden wurden, auf die Anwesenheit skythischer Söldner hinweisen. Vermutlich waren an der Eroberung von Urartu also auch Meder und transkaukasische Stämme beteiligt. Diese Feldzüge wurden vermutlich aus dem Kuban-Gebiet und dem nördlichen Kaukasus unternommen. Im Gebiet um Krasnodar und Stawropol wurden zahlreiche reich ausgestattete skythische Kurgane gefunden (zum Beispiel Ul'skij Aul mit über 400 Pferdebestattungen). Hier lokalisieren manche russische Forscher, wie zum Beispiel V. Murzin, das aus assyrischen Quellen belegte Reich Iškuza.

612 v. Chr. eroberten die Meder zusammen mit den Babyloniern und den Skythen Niniveh. Nach der Babylonischen Chronik eroberten die Skythen 609 Ägypten. Mit dem Beginn der Mederherrschaft (612 und 605 v. Chr.) ging der skythische Einfluss im vorderen Orient zurück. Herodot berichtet, die Skythen hätten 28 Jahre lang ganz Asien regiert, von dem Sieg des Madyes über den Medier Phraortes bis zur Niederlage gegen die Medier unter Kyaxares II. (624–585) im Jahr 594 v. Chr., der bei einem Gastmahl ihre Abgesandten umbringen konnte. Grakow erwägt allerdings, diesen Vorfall in die Regierungszeit von Astyages zu verlegen. Zu dieser Zeit war Madyes, Sohn des Protothyes Führer der Skythen. Danach zogen sich die Skythen nach Norden zurück. Manche Forscher setzen die verstärkte Besiedlung des nördlichen Schwarzmeerraumes erst in diese Zeit. 515/514 v. Chr. unternahm der Perserkönig Darius I. der Große mit einer mehrere hunderttausend Mann starken Armee einen erfolglosen Feldzug gegen die Skythen, deren Ostgrenze zu dieser Zeit am Don lag. Im ausgehenden 6. und 5. Jahrhundert steigt die Zahl der reichen Bestattungen im Dneprgebiet stark an.

Einer der bekanntesten Könige der Skythen war Atheas, der im Westen bis an die Donau vordrang und 339 v. Chr. hochbetagt gegen Philipp II. von Makedonien zu Felde zog und fiel.

331 führten die Makedonen unter Zopyrion einen weiteren Krieg gegen die Skythen. Sie stießen bis Olbia vor, konnten die Stadt aber nicht einnehmen und wurden auf dem Rückzug vernichtend geschlagen. In der Folge siedelten sich die Skythen in der Dobrudscha an. Alexander begann 330 Freundschaftsverhandlungen mit den Skythen, plante aber Arrian (Anabasis, 4,1) zufolge einen Feldzug zur Eroberung des nördlichen Schwarzmeergebietes und die Gründung einer Stadt am Tanais. Die Skythen boten ihm eine Heirat mit einer skythischen Prinzessin an, die er jedoch ablehnte. Im Jahre 329 kam es zu einem Zusammenstoß mit den Massageten in Baktrien, bei dem die makedonischen Truppen unter Krateros siegreich blieben. 323 wurde eine skythische Delegation in Babylon erwähnt.
Ab dem 4. Jahrhundert v. Chr. wurden die Skythen zunehmend von den Sarmaten verdrängt. Auch Klimaveränderungen werden jedoch für den Niedergang der Skythen verantwortlich gemacht. Auf der Krim, um die von König Skiluros gegründete neue Hauptstadt Neapolis bei Simferopol konnten sie sich noch bis ins 3. Jahrhundert n. Chr. halten. Skiluros und sein Sohn Palakos konnten ihrem Reich Teile des chersonesischen Reiches angliedern. In dem daraus entstehenden Konflikt mit Mithridates VI. (122–63 v. Chr.) verbündeten sich die Skythen mit dem roxolanischen König Tasius. Diophantes unterwarf die Krim jedoch zwischen 110 und 107 dem Pontischen Reich. Es kam zu einem Aufstand unter Saumakos, den Diophantes jedoch niederwerfen konnte. Ein erneuter Aufstand zwischen 89 und 84 war zunächst erfolgreich. 80 schlug Neoptolemos jedoch die skythische Flotte und besetzte Olbia und Tyras. Augustus erwähnt in seiner Autobiographie eine Gesandtschaft der Skythen. Sie kämpften zu dieser Zeit gegen Chersones und das Bosporanische Reich.

Die letzten, stark sarmatisierten Skythen wurden schließlich von den Goten in der zweiten Hälfte des 3. Jahrhunderts nach Christus vernichtet.

Nach Herodot waren die Skythen von Königen beherrscht und hielten Sklaven, die sie blendeten und zur Milchverarbeitung einsetzten. Die Diener der Könige stammten aus den weniger angesehenen Stämmen und wurden mit ihnen bestattet.

Nach Lukian wurde die soziale Stellung durch den Viehbestand bestimmt. Sogenannte „Achtfüßige“ – das sind Leute, die nur zwei Ochsen besaßen – standen an unterster Stelle. Pindar erwähnt sogar Skythen, die weder Vieh noch Wagen besaßen und denen deshalb die Bürgerrechte fehlten. Er kennt auch eine Aristokratie, die pilophorioi, also die Träger von Filzmützen.

Laut Herodot kannten die Skythen eine Form des Schwitzrituals, ähnlich dem der nordamerikanischen Lakota-Indianer. Dabei wurden ganze Hanfpflanzen auf den Steinen verräuchert. 

Im Weiteren berichtet Herodot über den Brauch der Skythen, sich bei Trauerfeierlichkeiten das Gesicht zu zerschneiden. Dieser Brauch ist auch später bei den Mongolen und Türken feststellbar.

Die Sprache der Skythen wird gemeinhin zur alt-nordost-iranischen Gruppe des Indogermanischen gerechnet. Die Sprache ist aber nur sehr bruchstückhaft überliefert.

Herodot überliefert einige Wörter der skythischen Sprache in seinen Etymologien der Völkernamen "Arimaspoi" 'Einäugige' (4.27) und "Oiorpata" 'Männertöterinnen' (4.110). Die Bestandteile dieser Namen lassen sich jedoch nur schwer identifizieren. Die meisten Forscher deuten ΟΙΟΡ (Oior) als iranisch "vīra-" 'Mann, Held', während ΠΑΤΑ (Pata) vielleicht eine Verschreibung für ΜΑΤΑ darstellt, d. h. iranisch "mar", 'töten'.

Herodot führt zusätzlich eine Reihe von Personen-, Götter- und Völkernamen an: beispielsweise die mythischen Vorfahren "Lipoxais", "Arpoxais" und "Kolaxais", deren Namen wahrscheinlich das iranische Wort "xšāy-" 'herrschen' enthalten; die Vorderglieder sind dagegen dunkler. Askold Ivančik vermutet *"ripa-" '(mythischer) Berg', "āfra-" (Nordostiran. *"ārfa-") 'Wasser' und "xvarya-" (Nordostiran. *"xola-") 'Sonne'. Laut Herodot sind diese drei Männer die Vorfahren von vier skythischen Stämmen: "Auchatai", "Katiaroi" + "Traspies" und "Paralatai", deren Namen Ivančik von "wahu-" 'gut, heilig', "hu-čahr-ya-" 'mit guten Weiden', "drv-asp-" 'mit festen Pferden' und "para-dāta-" 'vorgesetzt’ herleitet und im Rahmen des Dumézil'schen Systems der drei Funktionen erklärt.

Dass die Skythen tatsächlich eine Sprache des nordöstlichen Zweiges der iranischen Sprachgruppe hatten, wird auch dadurch indiziert, dass die Sauromaten laut Herodot eine korrupte Form (d. h. einen Dialekt) der skythischen Sprache verwendeten. Die Sauromaten wiederum werden mit den später auftauchenden Sarmaten gleichgesetzt, die als Sprecher einer iranischen Sprache gelten. In den späten griechischen Inschriften der Kolonien der nördlichen Schwarzmeerküste sind rund 300 iranische Namen überliefert, die sich nur durch sarmatischen Einfluss erklären lassen. Diese Namen zeigen gewisse geografische Unterschiede in der Lautentwicklung, was mutmaßlich auf die Existenz eines östlichen (= skythischen?) und eines westlichen (= sarmatischen?) Dialekts deutet.

Mit anderen Worten bildeten das Skythische, das Sarmatische und das Sakische im Altertum ein sprachliches Kontinuum, aus dem später auch das Sogdische†, das Alanische† und das Ossetische erwuchsen.

Ob bzw. inwieweit die Skythen nach Mitteleuropa vordrangen, ist äußerst umstritten. Archäologisch lassen sich diese Einfälle nicht sicher belegen. In den hallstattzeitlichen Siedlungen von Smolenice-Molpír (Slowakei), in Ungarn sowie im Gebiet der Billendorfer Kultur im heutigen Polen (Wiscina (Witzen) und Kamieniec) wurden Brandhorizonte nachgewiesen, die dreiflügelige Pfeilspitzen enthielten. Diese dreiflügeligen Pfeilspitzen werden gerne als Beleg für die Anwesenheit der Skythen herangezogen. Solche Pfeilspitzen wurden jedoch auch von anderen Reiternomaden verwendet, auch solchen, die in römischen Diensten standen. Der Goldschatz von Vettersfelde mit Artefakten im skythischen Stil könnte von der Anwesenheit eines skythischen Fürsten zeugen, aber auch Beutegut darstellen.











</doc>
<doc id="12137" url="https://de.wikipedia.org/wiki?curid=12137" title="Kamikaze (Begriffsklärung)">
Kamikaze (Begriffsklärung)

Kamikaze (jap. , dt. "göttlicher Wind") bezeichnet:

Schiffe und Schiffsklassen:

Film und Fernsehen:

Weitere Bedeutungen:


</doc>
<doc id="12140" url="https://de.wikipedia.org/wiki?curid=12140" title="Open Source">
Open Source

Als Open Source (aus englisch "", wörtlich "offene Quelle") wird Software bezeichnet, deren Quelltext öffentlich und von Dritten eingesehen, geändert und genutzt werden kann. Open-Source-Software kann meistens kostenlos genutzt werden. 

Software kann sowohl von Einzelpersonen aus altruistischen Motiven zu Open-Source-Software gemacht werden, wie auch von Organisationen oder Unternehmen, um Entwicklungskosten zu teilen oder Marktanteile zu gewinnen.

Open Source hat viele Ursprünge und Vorläufer, beispielsweise die Do-it-yourself-Bewegung, die Hacker-Bewegung der 1960/70er und die Freie-Software-Bewegung der 1980er Jahre, die der unmittelbare Vorläufer wurde.

Beeinflusst durch den 1997 publizierten Essay "Die Kathedrale und der Basar" von Eric S. Raymond, entschied Netscape im Jahr 1998 angesichts der wachsenden Dominanz von Microsoft am Browser-Markt, den Quelltext des wirtschaftlich nicht mehr verwertbaren Netscape Navigators freizugeben (aus dieser Freigabe entstand später das Mozilla-Projekt).

Kurz darauf befanden Raymond, der Informatiker Bruce Perens und Tim O’Reilly, Gründer und Vorstand des O’Reilly Verlags, dass die Freie-Software-Gemeinschaft ein besseres Marketing benötige. Um diese freie Software als frei von ethischen Werten und geschäftsfreundlich darstellen zu können, wurde dabei beschlossen, einen neuen Marketing-Begriff für "Freie Software" einzuführen – der Begriff "Open Source" wurde von da an flächendeckend im Marketing genutzt und war auch der Namensgeber für die von Raymond, Perens und O’Reilly gegründete "Open Source Initiative" (OSI).
Es wurden angepasste Open-Source-Lizenzen geschaffen, die den Bedürfnissen des Open-Source-Umfelds genügen und auch für Wirtschaftsunternehmen attraktiv sein sollten (Permissive licenses). Eine der bekanntesten Lizenzen, die aus diesen Bestrebungen hervorging, ist die "Mozilla Public License".

Die Open Source Initiative wendet den Begriff "Open Source" auf all die Software an, deren Lizenzverträge den folgenden drei charakteristischen Merkmalen entsprechen und die zehn Punkte der "Open Source Definition" erfüllen:


Open-Source bedeutet jedoch nicht, wie häufig angenommen, "alles-ist-erlaubt"; es sind Bedingungen an die Nutzung geknüpft. Völlig bedingungslose Nutzung existiert typischerweise nur bei gemeinfreier Software. Deswegen gibt ein BITKOM-Bericht zum Thema Open Source Software folgende korrekte Beschreibung: "„Die Verwertung, Vervielfältigung und Bearbeitung ist nicht vorbehaltlos gestattet, denn bei der Open Source Software wird vielfach die Einräumung von Nutzungsrechten von bestimmten Voraussetzungen abhängig gemacht. In dieser Weise kann Open Source Software abgegrenzt werden von Public Domain Software […]. Bei der Public Domain Software ist dem Nutzer die Vervielfältigung, Verbreitung und Veränderung uneingeschränkt und vorbehaltlos erlaubt.“"

Open-Source-Software (OSS) hat in der Praxis große Überschneidungen mit Freier Software wie sie die FSF definiert. Beide Konzepte haben gemeinsam, dass der Quellcode von Software für Anwender verfügbar sein sollte. Es werden auch die gleichen Copyleft- und Freizügige-Softwarelizenzen, bis auf seltene Ausnahmen, von beiden Seiten als „Frei“ bzw. „Offen“ eingeordnet. Der primäre Unterschied liegt in der Terminologie und Sichtweise: Freie Software fokussiert auf den Aspekt der Nutzerkontrolle über Software und sieht Freie Software als wichtiges soziales, politisches und ethisches Anliegen. Die OSI vertritt die Sichtweise, dass der praktische Nutzen für die Allgemeinheit (Nutzer, Gesellschaft, Firmen etc.) einer frei verfügbaren Softwareinfrastruktur, eines freien Softwaremarktes und einer kollaborativen Entwicklungsmethode der entscheidende Aspekt sind.

Der Begriff Open Source beschränkt sich nicht ausschließlich auf Software, sondern wird auch auf Wissen und Information allgemein ausgedehnt. Darüber hat die Freie-Software-Bewegung in anderen Bereichen die Begründung vieler neuer „Open“-Bewegungen inspiriert, beispielsweise Open Content, Open-Source-Hardware und Open Access.

Ein Beispiel dafür ist Wikipedia und die Wikimedia Commons, in deren Zusammenhang von freien Inhalten ("Open Content") gesprochen wird. Weitere bedeutende Beispiele für Open Content sind OpenStreetMap und Open Educational Resources. Eine Lizenzfamilie, die für solche freien Inhalte („free cultural works“) geschaffen wurde, sind die Creative Commons Lizenzen.

Übertragen wurde die Idee des öffentlichen und freien Zugangs zu Information auch auf Entwicklungsprojekte. In diesem Zusammenhang wird oft von freier Hardware ("Open Hardware") gesprochen, also freiem Zugang zu allen Informationen (Offener Standard, Offenes Format), um eine entsprechende Hardware herzustellen. Produktebeispiel auf Basis von offenen Standards und Rezepturen sind Vores Øl/Free Beer oder OpenCola.

Auch als „Offen“ und der "Open Source"- und "Open Hardware"-Bewegung nahestehend verstehen sich die FabLabs, die freien Zugang zur (Produktions-)Hardware vermitteln wollen.

Ein weiteres Beispiel ist die Open-Source Saatgut Lizenz, die das Prinzip auf die Pflanzenzüchtung zu übertragen versucht. Gemeinnützige Züchter können neue Sorten mit der viralen Lizenz (copyleft) ausstatten, damit sich das genetische Material nicht mehr in ein Privatgut überführen lässt.

Open Access (englisch für offener Zugang) versucht, den freien Zugang zu wissenschaftlicher Literatur und anderen Materialien im Internet zu ermöglichen und Paywalls abzubauen. Ähnlich versucht Open Government, den Zugang zu Regierungsressourcen für Bürger zu erleichtern.

Die Begriffe "Open-Source-Software" und "Freie Software" werden zwar häufig synonym verwendet, allerdings besteht die Möglichkeit einer unterschiedlich pointierten Interpretation. Obwohl sich in der eigentlichen Bedeutung die Open-Source-Definition kaum von freier Software unterscheidet, können beide Begrifflichkeiten bewusst verwendet werden, um unterschiedliche Assoziationen auszulösen. Der Begriff "open source" wurde eingeführt, nachdem die ursprüngliche Bezeichnung "free software" (freie Software) zu Irritationen führte, da sie oft als grundsätzlich kostenlose Software missverstanden wurde.

Der ältere Begriff "Freie Software" wird bereits seit den 1980ern von der Free Software Foundation (FSF) verwendet. Eine Fehlassoziation von "Freier Software" mit Freeware war häufig, da im Englischen "frei" für "kostenlos" wie auch "Freiheit" stehen kann, und außerdem freie Software in den meisten Fällen wirklich auch kostenlos erhältlich ist. Da mit "Frei" aber wirklich nur "Freiheit" von der FSF gemeint war, prägte diese den Slogan, "„“" – "„freie Meinungsäußerung, nicht Freibier“", um einer Assoziation von "Freier Software" mit "kostenloser Software" entgegenzuwirken.

Die mögliche Fehlinterpretation des zweideutigen Wortes "frei" war Teil der Motivation für den Terminus "Open Source", der ab Ende der 1990er Jahre mit Linux populär wurde. Der Vorschlag kam 1998 von Christine Peterson vom Foresight Institute bei der Gründung der OSI und Open-Source-Bewegung. Die frischgegründete Open-Source-Bewegung entschied sich, "Open Source" anstelle des bereits existierenden FSF Terminus "Freie Software" zu etablieren, da man hoffte, dass die Verwendung der Bezeichnung "Open Source" die "Frei"-Mehrdeutigkeit beseitigt und damit eine bessere Akzeptanz des "Open Source"-Konzepts auch bei der Wirtschaft ermöglicht. Außerdem vermied der Begriff "Open Source" eine Assoziation mit der Free Software Foundation und der GNU General Public License ("GPL"), die aus wirtschaftlicher Sicht problematisch sein kann. Der Begriff "Open-Source-Software" sollte auch eine Überlegenheit des kollaborativen, "offenen" Entwicklungsprozesses (siehe "The Cathedral and the Bazaar" von Eric Steven Raymond) hervorheben.

Seit der Einführung der konkurrierenden Bezeichnung "Open Source" kritisiert die FSF, dass auch diese Bezeichnung Verwirrung stiften kann. Der Begriff "Open Source" assoziiert die Verfügbarkeit des Quelltextes, sagt aber nichts über die gewährten Verwendungsrechte und Nutzungsfreiheiten aus. Ein Beispiel für eine solche Begriffsverwirrung ist "PGP Corporations" aktuelle Version ihres Kryptographie­programms PGP: diese wird als "Open Source" angepriesen, da der Quellcode betrachtet werden kann, jedoch unterliegt dieser keiner Open-Source-Lizenz. Weitergabe und Veränderung dieses Quellcodes sind verboten, so dass das Programm nicht unter die Open-Source-Definition fällt. GNU Privacy Guard ist als Reaktion darauf entstanden, die den Open-Source-Anforderungen durch ihre GPL-Lizenzierung gerecht wird.

Umgekehrt wird die von der FSF als "„frei“" angesehene GNU FDL als nicht „Open“ und „Frei“ kritisiert. Bei der GNU FDL ist eine problematische Besonderheit, dass sie die Möglichkeit bietet, die Modifikation bestimmter Abschnitte zu verbieten, also das Recht auf freie Weiterverwendung beschränkt. Die GNU FDL erfüllt somit eine grundlegende Anforderung der Open-Source-Definition, der Freie-Software-Definition sowie der Debian Free Software Guidelines für Software nicht.

Um den Konflikt zwischen "Freie Software" und "Open-Source-Software" zu umgehen und die Gemeinsamkeiten der Open-Source- und Freie-Software-Bewegung zu betonen, wurden die Begriffe FOSS und FLOSS (Free/Libre and Open Source Software) vorgeschlagen, die auch eine relevante Verbreitung erlangten.

Nachdem sich Open-Source-Software zunächst in den Bereichen Serverbetriebssysteme, Web- und Mailserver, Datenbanken und Middleware etabliert hatte, konnte Open-Source-Software allgemein in kommerzieller Software eine wichtige Rolle erringen. Eine Studie im Auftrag der Europäischen Kommission hat im Jahr 2006 die wirtschaftliche Bedeutung von Open Source für Europa untersucht. Demnach ist der Marktanteil in den vergangenen Jahren stetig gestiegen. Den Gesamtwert beziffert die Untersuchung auf rund zwölf Milliarden Euro. In den vier Haupt-Einsatzgebiete (Web-Server, Programmiersprachen, Datenbanken und Server-Betriebssysteme) setzen 2015 70 % der Schweizer Nutzer Open-Source-Software ein, das ist ein Wachstum von 20 % gegenüber 2012. Für das Jahr 2010 prognostizierte die Studie bei den IT-Dienstleistungen einen Open-Source-Anteil von 32 Prozent und befürwortet eine stärkere Förderung von freier Software, damit Europa das wirtschaftliche Potenzial von Open Source besser nutzen kann. So findet das Thema zunehmend in der Wirtschaftsförderung Beachtung. Ein Beispiel ist die Wirtschaftsförderung Region Stuttgart, die eine Initiative für einen Open-Source-Cluster gestartet hat.

Open-Source-Software wurde dabei in Unternehmen deutlich häufiger eingesetzt als in Behörden.

Viele Open-Source-Projekte besitzen einen hohen wirtschaftlichen Wert. Gemäß dem Battery Open-Source Software Index (BOSS) sind die wirtschaftlich zehn bedeutendsten Open-Source-Projekte:
Der angegebene Rang basiert dabei auf der Aktivität bezüglich der Projekte in Online-Diskussionen, auf GitHub, bezüglich der Suchaktivität in Suchmaschinen und dem Einfluss auf den Arbeitsmarkt.

Open-Source-Software wird sowohl von Unternehmen wie auch von Einzelpersonen genutzt. Es bietet sich eine Reihe von Vorteilen:





Bei der Frage, was die wichtigen Gründe für den Einsatz von Open Source sind, wird klar, dass den Anwendern maximale Flexibilität bei der Gestaltung ihrer IT-Landschaft wichtig ist: Für die große Mehrheit der Antwortenden sind die Einhaltung offener Standards (86 %), Wissensaustausch mit der Community (82 %), Kosteneinsparungen (77 %) und die Verringerung von Lieferantenabhängigkeiten (76 %) die Hauptmotive für den Einsatz von Open Source.

Open Source dient häufig als Basis für kommerzielle Software. So wird auf vielen Embedded-Systemen, Heim-Routern, Set-Top-Boxen und Mobiltelefonen das Open-Source-Betriebssystems Linux als Plattform verwendet. Auch das kommerzielle Betriebssystem webOS von HP Palm profitiert von Linux. Da diese Unternehmen von Linux abhängig sind, sind sie auch motiviert, zu seiner Weiterentwicklung beizutragen.

Eine andere Variante, wie Softwareentwicklungsunternehmen mit Open-Source-Produkten wirtschaftlich handeln können, ist die Bereitstellung von Support-Dienstleistungen für diese. Dies ist beispielsweise bei den Betriebssystemen Ubuntu, Red Hat/Fedora und SUSE Linux der Fall. Der Quellcode wird jedoch kostenlos weitergegeben.

Open-Source-Software kann auch durch Freiwillige aus altruistischen (selbstlosen) Motiven entwickelt werden. Größere Projekte schließen sich meist zu einer Stiftung zusammen, die dann auf Spendengeldern basiert.

Damit die fortwährende Unterstützung von Open-Source-Projekten gewährleistet ist (die Software nachhaltig weiterentwickelt wird), sind einige Bedingungen notwendig:

Zunächst sollte es eine aktive Community geben, damit das Wissen auf zahlreiche Personen verteilt ist. Ein Beispiel hierfür ist der Linux-Kernel. Gemäß der diesjährigen (2015) Kernel-Development-Studie der Linux Foundation beträgt der Anteil der Intel-Entwickler nur 10,5 %. Selbst wenn sich Intel also aus der Linux-Entwicklung zurückziehen oder ausfallen sollte, wäre die Weiterentwicklung dennoch sichergestellt. Anders wäre es hingegen, wenn die Entwicklung maßgeblich von einer Einzelperson oder Firma vorangetrieben wird. In diesem Fall wäre zwar der Quellcode der Allgemeinheit zugänglich, die Weiterentwicklung des Projekts könnte aber dennoch eingestellt werden (wodurch gleichzeitig wichtiges Wissen verloren gehen würde).

Zudem ist ein Ökosystem aus kommerziellen Anbietern wichtig für die Nachhaltigkeit von Open-Source-Projekten. Entwickler, die nicht unentgeltlich an dem Projekt tätig sind, sondern für die Arbeit bezahlt werden, können dem Projekt langfristig Zeit widmen. Beispielsweise sind an der Entwicklung von LibreOffice zahlreiche kleinere Firmen engagiert (etwa Collabora), die zertifizierte Versionen des Programms verkaufen. Gleichzeitig sind die Entwickler bei den Firmen angestellt und entwickeln das Programm hauptberuflich weiter. 

Schließlich ist auch noch die Existenz einer Non-Profit-Organisationen wichtig, die die Entwicklung koordiniert und Marketing durchführt. Koordination ist wichtig bei der Entwicklung einer Software. Bei proprietärer Software wird diese Aufgabe von dem Eigentümer der Software übernommen. In größeren Open-Source-Projekten (etwa dem Linux-Kernel, LibreOffice oder dem Content-Management-System TYPO3) verbindet eine Non-Profit-Organisationen die Entwickler-Firmen und die Software-Nutzer untereinander. Juristisch gesehen handelt es sich dabei um eine Stiftung. Besonders dem Marketing kommt eine wichtige Funktion zu, da in der Software-Industrie (etwa bei Adobe, Oracle oder Microsoft) doppelt so viel Geld für Verkauf und Werbung ausgegeben wird wie für die eigentliche Software-Entwicklung.

Eric S. Raymond beschreibt in seinem Buch "Die Kathedrale und der Basar" eine Entwicklungsmethode (den Basar), durch die Open-Source-Projekte selbstständig ohne zentrale Entität von der Gemeinschaft verwaltet werden können. Ob diese Entwicklungsmethode tatsächlich so Anwendung findet oder überhaupt praktisch umgesetzt werden kann, ist aber umstritten. Beispielsweise folgt der Linux-Kernel, eines der großen OSS-Erfolgsprojekte, mit Linus Torvalds an der Spitze eher einem zentralisierten Benevolent Dictator for Life ("Wohlwollender Diktator auf Lebenszeit") Entwicklungsmodell ("Kathedrale" in Raymond'scher Terminologie). Ähnliche Führungsstrukturen sind mit Richard Stallman an der Spitze der FSF/GNU-Projekt und auch bei der Mozilla Foundation zu finden. Als Gegenbeispiele werden die Community getriebenen Projekte Apache Hadoop und OpenStack angeführt.

Der Informatiker Niklaus Wirth äußert sich kritisch zur technischen Qualität komplexer Open-Source-Projekte: Die Open-Source-Bewegung ignoriere und behindere die Vorstellung, komplexe Softwaresysteme basierend auf streng hierarchischen Modulen aufzubauen. Entwickler sollten den Quelltext der von ihnen verwendeten Module nicht kennen. Sie sollten rein auf die Spezifikationen der Schnittstellen der Module vertrauen. Wenn, wie bei Open-Source, der Quelltext der Module vorhanden ist, führe das automatisch zu einer schlechteren Spezifikation der Schnittstellen, da ja das Verhalten der Module im Quelltext nachlesbar ist.

Die FSF, und insbesondere deren Gründer Richard Stallman, kritisiert prinzipiell an der Open-Source-Bewegung, dass sie sozialethische Aspekte außen vor lässt und sich lediglich auf technische und wirtschaftliche Fragestellungen konzentriert. So werde die Grundidee von "freier Software" nach Stallmans Meinung vernachlässigt. Die FSF kritisiert auch die von Teilen der Open-Source-Bewegung tolerierte Firmenpraxis, die Weiterentwicklungen von bestehender Open-Source-Software so weit an eigene (Hardware-)Systeme anzupassen, dass sie praktisch nicht mehr anders verwendet werden können, z. B. Tivoisierung oder unlesbar gemachter Quelltext. Die Weiterentwicklung steht dann zwar immer noch unter einer Open-Source-Lizenz, kann von der Gemeinschaft aber nicht mehr genutzt werden; eine Situation, welche die FSF über die GPLv3 zu verhindern versucht.

Das Ideal des Open Source, der freie Austausch und die beliebige Weiterverwendung von Quelltext, ist in der Realität durch u. a. Lizenzprobleme eingeschränkt. Besonders problematisch ist dies durch die inzwischen auf eine kaum überschaubare Anzahl angewachsene (und weiter wachsende) Menge an Softwarelizenzen und Versionsvarianten, ein als "License proliferation" bekanntes Problem. Auch anerkannte Open-Source-Lizenzen sind häufig nicht miteinander kompatibel und was damit eine Software-Weiterverwendung in manchen Kontexten verhindert. Deswegen wird dazu geraten keine eigenerstellten oder exotische Open-Source-Lizenz zu verwenden, deren rechtliche und praktische Probleme man unter Umständen nicht überschaut, sondern auf eine erprobte, anerkannte und weitverbreitete freie Lizenz (und Lizenzkombinationen) wie die GPL, die LGPL oder die BSD-Lizenz zurückzugreifen. Besonders die Freizügigen Lizenzen zeichnen sich durch eine sehr gute Lizenzkompatibilität aus.




</doc>
<doc id="12143" url="https://de.wikipedia.org/wiki?curid=12143" title="Leiter">
Leiter

Leiter bezeichnet:

Leiter heißen folgende geographischen Objekte:
Leiter ist der Familienname folgender Personen:


außerdem von:
Siehe auch:



</doc>
<doc id="12144" url="https://de.wikipedia.org/wiki?curid=12144" title="Reinhard Heydrich">
Reinhard Heydrich

Reinhard Tristan Eugen Heydrich (* 7. März 1904 in Halle an der Saale; † 4. Juni 1942 in Prag) war ein deutscher SS-Obergruppenführer und General der Polizei, der während der Diktatur des Nationalsozialismus als Leiter des Reichssicherheitshauptamts (RSHA) und Stellvertretender Reichsprotektor in Böhmen und Mähren für zahlreiche Kriegsverbrechen und Verbrechen gegen die Menschlichkeit verantwortlich war. 1941 wurde er von Hermann Göring mit der „Endlösung der Judenfrage“ beauftragt und war seit dieser Zeit einer der Hauptorganisatoren des Holocausts. In dieser Funktion leitete er am 20. Januar 1942 in Berlin die Wannseekonferenz, auf der die Vernichtung der im deutschen Machtbereich lebenden Juden abgesprochen wurde. 

Heydrich wurde am 27. Mai 1942 bei einem Attentat in Prag schwer verletzt und starb acht Tage später. Daraufhin ließ das NS-Regime Racheakte wie die Zerstörung von Lidice und Ležáky verüben.

Heydrichs Mutter Elisabeth Krantz (1871–1946) stammte aus einer wohlhabenden Familie und war die Tochter des Leiters des Königlichen Konservatoriums in Dresden, Eugen Krantz. Sein Vater Bruno Heydrich (1865–1938) kam aus ärmlichen Verhältnissen, brachte es aber nach einer durch ein Stipendium finanzierten Ausbildung in Komposition und Gesang am Königlichen Konservatorium in Dresden zu einem anerkannten Komponisten und Opernsänger. Die Ehe der Eltern war interkonfessionell, die Mutter römisch-katholisch und der Vater protestantisch. Die drei Kinder Reinhard, Heinz und Maria wurden katholisch erzogen.

1899 gründete Bruno Heydrich in Halle an der Saale eine Musikschule für Kinder der Mittelklasse, die bereits 1901 zu einem Konservatorium ausgebaut wurde, das 1904 elf Lehrer, vier Hilfskräfte und eine Sekretärin fest angestellt hatte, so dass sich die Familie Heydrich zwei Dienstmädchen und einen Butler leisten konnte, schnell Zugang zu den gehobenen Kreisen der Stadt fand und unter anderem zum Bürgermeister und Herausgeber der Lokalzeitung freundschaftliche Kontakte pflegte. Gerüchten, er sei jüdischer Herkunft, trat Bruno Heydrich 1916 erfolgreich mit einer Verleumdungsklage entgegen, da er fürchtete, sie könnten im politischen Klima der von Antisemitismus geprägten wilhelminischen Ära „geschäftsschädigend“ wirken.

Reinhard Heydrich wurde streng erzogen und besuchte auf Wunsch des Vaters ein nicht-konfessionelles „Reformgymnasium“, an dem ein Schwerpunkt auf das Erlernen moderner Fremdsprachen und Technik gelegt wurde. Besonders im letzteren Bereich (speziell im Fach Chemie) zeigte Heydrich überdurchschnittliche Leistungen. Abseits der Schule erlernte Heydrich als Kind zweier Musiker mehrere Instrumente. Vor allem beim Violinspiel zeigte er einiges Talent und beherrschte es bald auf einem nennenswerten Niveau. Seine Leidenschaft für dieses Instrument blieb auch im Erwachsenenalter ungebrochen. Die Ambition des Vaters, seinen Sohn zu einem professionellen Sänger auszubilden, wurde durch den dünnen, gebrechlichen Charakter von Reinhards Fistelstimme zunichtegemacht, was sich auch während der Jugendjahre nicht auswuchs und zu Hänseleien der Mitschüler führte.

Politisch und weltanschaulich wurde er früh durch einen extremen Nationalismus geprägt, der in der Familie vorherrschte. Die Niederlage des Deutschen Reiches im Ersten Weltkrieg und die Abdankung von Kaiser Wilhelm II. wurden von seiner Familie als Katastrophe empfunden. Wie auch viele weitere Schüler seines Realgymnasiums schloss Reinhard Heydrich sich 1919, nachdem er Zeuge von Kämpfen nahe seinem Elternhaus in seiner Heimatstadt geworden war, einer „freiwilligen Einwohnerwehr“ des Freikorps von Georg Maercker an, in der er als Melder Dienst tat, ohne selbst an Kampfhandlungen teilzunehmen. 1920 wurde er Mitglied der Jugendgruppe der halleschen Ortsgruppe des Deutschvölkischen Schutz- und Trutzbundes, der nach der Ermordung von Außenminister Rathenau 1922 verboten wurde.

Am 30. März 1922 trat Heydrich als Seekadett in die Reichsmarine ein; 1926 erhielt er sein Offizierspatent als Leutnant zur See und wurde zum Nachrichtendienst der Marine versetzt, 1928 wurde er zum Oberleutnant zur See befördert.

Zu Beginn seiner Ausbildung in der Marine galt Heydrich als Sonderling. In dieser Zeit machte er „einen seltsam unpolitischen Eindruck“ und galt – im außerordentlich rechten Offiziersmilieu der Marine negativ angesehen – als „Freisinniger“. Wilhelm Canaris, den späteren Chef der deutschen Abwehr, lernte er 1923 während seiner Dienstzeit auf dem Kreuzer "Berlin" kennen und befreundete sich mit ihm. Während seiner Marinezeit betrieb der ehrgeizige Heydrich intensiv Sport: Segeln, Schwimmen, Fechten; viel Zeit wandte er fürs Musizieren auf.

Im Dezember 1930 lernte Heydrich seine spätere Ehefrau, die 19-jährige Lina Mathilde von Osten (1911–1985), kennen. Zwei Wochen später verlobten sich die beiden, nachdem Heydrich bei ihrem Vater um sie angehalten hatte. Die Verlobte kam aus einer politisch rechtsextrem geprägten Familie. Ihr Bruder Hans von Osten gehörte seit 1928 der SA an, Lina von Osten selbst war, schon als sie Reinhard Heydrich kennenlernte, „überzeugte Nationalsozialistin und glühende Antisemitin“.

Heydrich hatte jedoch zur Zeit der Verlobung mit Lina von Osten eine Beziehung zu einer anderen Frau, deren Identität bis heute nicht geklärt ist. Diese Beziehung beendete er durch Zusendung der Anzeige seiner Verlobung. Der Vater der betroffenen Frau reichte beim Chef der Marineleitung, Admiral Erich Raeder, Beschwerde gegen Heydrich ein. Ein gebrochenes Heiratsversprechen galt als ehrenrührig, war aber kein schweres Vergehen und hätte ohne Strafe durch den Ehrenrat der Marine enden können. Die Angehörigen des Ehrenrats – Admiral Gustav Hansen, Heydrichs Ausbilder Gustav Kleikamp und Hubertus von Wangenheim – wurden jedoch durch Heydrichs arrogantes Auftreten, der schlecht über die Frau sprach, sie belastete und bestritt, ihr die Ehe versprochen zu haben, dazu gebracht, kein Urteil zu fällen und das Verfahren in die alleinige Entscheidung Raeders zu legen. Raeder entschied, ebenfalls aufgrund Heydrichs offensichtlicher Unaufrichtigkeit im Verfahren und seiner Versuche, sich durch Belastung der Frau reinzuwaschen, dass Heydrich als Offizier „unwürdig“ und seine Entlassung zu verfügen sei, die am 30. April 1931 wirksam wurde.

Für Heydrich war die unerwartete Entlassung eine vollständige Katastrophe, die ihn bis ins Mark erschütterte. Seine Lebensplanung war Makulatur. Einer beim Reichspräsidenten eingereichten Bitte um Aufhebung der Entlassung gnadenhalber wurde nicht entsprochen, Heydrich „schloss sich in seinem Zimmer ein und weinte tagelang vor Wut und Selbstmitleid“.

Inmitten der Weltwirtschaftskrise war Heydrich nun – abgefedert allein durch ein Übergangsgeld von 200 Reichsmark monatlich – weitgehend auf sich gestellt, die wirtschaftliche Situation seiner einstmals wohlhabenden Eltern war gleichfalls sehr schwierig.

Am 1. Juni 1931 trat Heydrich – nachdem er lange gegenüber der NSDAP indifferent geblieben war – unter dem Einfluss Lina von Ostens und ihrer Familie in die NSDAP (Mitgliedsnr. 544.916) und einen Monat später, am 14. Juli, als SS-Untersturmführer in die SS (SS-Nr. 10.120) ein. Sein früher Eintritt trug dazu bei, dass er später das Goldene Parteiabzeichen erhielt. Sein Eintritt in Partei und SS war wohl weniger ideologisch motiviert als durch den Wunsch, „zu einem strukturierten Leben in Uniform zurückzufinden“.

In den frühen 1930er Jahren baute Heinrich Himmler die „Schutzstaffel des Führers“, die SS, systematisch auf. Hauptsächlich zur Überwachung und Ausschaltung politischer Gegner benötigte die wachsende SS einen effizienten Nachrichtendienst. Über einen verwandten Jugendfreund, den Münchner SA-Führer und SA-Brigadeführer „Oberbayern“ Karl von Eberstein (dessen Mutter war Heydrichs Patentante), wurde Heydrich im August 1931 Himmler vorgestellt. Dieses Zusammentreffen wurde der Beginn eines engen Arbeitsverhältnisses. Heydrich skizzierte ihm kurz seine Vorstellungen vom Aufbau eines Nachrichtendienstes. Himmler war beeindruckt und beauftragte ihn mit dem Aufbau der Organisation, die den Namen „Sicherheitsdienst“ (SD) erhielt. Allerdings räumte Himmler später intern ein, dass die Heranziehung Heydrichs ursprünglich auf einem „Irrtum“ basierte: Das, was heute als Fernmeldetruppe bezeichnet wird, wurde seit 1917 als Nachrichtentruppe bezeichnet, und Heydrich war als „technischer Nachrichtenoffizier“ tatsächlich zum Funkoffizier ausgebildet worden. Mit nachrichtendienstlicher Tätigkeit im Sinne von geheimdienstlicher Aktivität hatte er jedoch nichts zu tun gehabt. Gleichwohl wusste er offenbar Himmler – der von Geheimdienstarbeit auch nichts verstand – zu überzeugen.

Am 26. Dezember 1931 fand in Großenbrode die Hochzeit mit Lina von Osten nach evangelischem Ritus und unter Absingen des "Horst-Wessel-Liedes" statt.

Durch seinen effizienten Arbeitsstil wurde Heydrich seinem Förderer Heinrich Himmler und dessen Ehrgeiz bald unentbehrlich, und er stieg rasch in der Hierarchie der SS auf. Am 1. Dezember 1931 wurde er zum Hauptsturmführer der SS, im Juli 1932 zum SS-Standartenführer und „Chef des Sicherheitsdienstes beim Reichsführer SS“ ernannt. Bis zu Heydrichs Tod verband beide eine enge Arbeitsbeziehung und wohl auch Freundschaft.

Für Heydrichs ideologische Verfestigung im Sinne des Nationalsozialismus wurde erst seine Tätigkeit in den Reihen der SS entscheidend, er sollte ihre Strukturen so prägen, wie diese sein Denken und seine Anschauungen formten.

Die Machtergreifung der NSDAP bedeutete für die SA und SS einen legalen Zugang zur Macht. Heydrich wurde noch im selben Jahr stellvertretender Chef der bayerischen Polizei. Das Ermächtigungsgesetz ermöglichte die Zerschlagung der Opposition. Das Tableau der „Reichsfeinde“ Heydrichs reichte von Juden, christlichen Kirchen, Freimaurern und Zigeunern bis zu „Asozialen“.

Ein frühes Ziel der Verfolgungsaktionen 1933 in Bayern war der Literaturnobelpreisträger Thomas Mann. Als Mann nach Hitlers Ernennung zum Reichskanzler beschloss, seinen Auslandsaufenthalt bis zu einer Klärung der Lage in Deutschland zu verlängern, durchsuchte die Bayerische Politische Polizei Manns Haus in München und beschlagnahmte das Haus nebst Inventar sowie das Bankkonto. Am 12. April 1933 forderte Heydrich, Mann sofort nach dessen Rückkehr in „Schutzhaft“ zu nehmen und schrieb an Reichstatthalter von Epp:

Die Konzeption der politischen Polizei in Bayern hatte für die spätere Entwicklung der Sicherheits- und Unterdrückungsstrukturen des Dritten Reiches Modellcharakter. Himmler und Heydrich gelang es, die Polizei aus den üblichen Verwaltungsstrukturen herauszulösen und mit der SS und ihrem Nachrichtendienst SD eng zu verzahnen. Damit bekam sie eine prominente Rolle bei der weltanschaulichen Formung der Gesellschaft im Sinne des Nationalsozialismus und wurde ein Mittel zum weiteren Bedeutungszuwachs der SS.

Heinrich Himmler wurde am 20. April 1934 zum Inspekteur der Preußischen Geheimen Staatspolizei ernannt, die zuvor unter Kontrolle Hermann Görings gestanden hatte, und ernannte seinen engen Gefolgsmann Heydrich zum Chef des Geheimen Staatspolizeiamtes (Gestapa). Heydrich, seit dem 5. April 1934 auch preußischer Staatsrat, verlegte den Sitz des SD an seinen neuen Wirkungsort in Berlin und begann damit, die Parteiformationen SS und SD wie zuvor in Bayern mit der Polizei zu verzahnen. Dieser Ausbau der Machtstellung Himmlers und Heydrichs stand im engen Zusammenhang mit der Furcht des Kontrollverlusts der NS-Führung um Hitler über die SA. Denn die Sturmabteilung unter Ernst Röhm war nach der Machtübernahme zunehmend unzufriedener geworden. Sie hatte Hitler ihrer Auffassung nach an die Macht gebracht, spielte jetzt jedoch nur noch eine untergeordnete Rolle. Ein Teil der SA forderte nach der ersten "nationalen" Revolution nun eine zweite, "sozialistische" Revolution, die Hitlers Bündnis mit den konservativen Eliten und der Reichswehr gefährdete. Hitler, dem die SA unbequem wurde, suchte nach Möglichkeiten, diese auszuschalten. Heydrichs SD operierte darum mit fingierten Beweisen für einen angeblich unmittelbar bevorstehenden Putsch. Bei der Niederschlagung dieses sogenannten Röhm-Putsches Ende Juni 1934 wurde die SA-Führungsriege durch Heydrich unterstehende Kommandos der SS und des SD exekutiert. Rückwirkend zu genau diesem 30. Juni 1934 wurde Heydrich für seine „Leistungen“ zum SS-Gruppenführer ernannt.

1936 wurde Himmler Chef der deutschen Polizei, Heydrich Chef der Sicherheitspolizei (Sipo). Letztere, die sich aus der politischen Polizei und der Kriminalpolizei zusammensetzte, wurde straff durchorganisiert, mit zuverlässigen und jungen Nationalsozialisten akademischer Prägung durchsetzt und zentral geführt. In ihr hatte Heydrich ein effizientes und ihm weltanschaulich eng verbundenes Instrument, um vermeintliche Staatsfeinde, gegebenenfalls aber auch persönliche Widersacher und Rivalen gnadenlos zu verfolgen. Er schuf ein Netz einer engen polizeilichen Überwachung, legte umfangreiche Akten an und beauftragte innerhalb des SD Wissenschaftler mit Analysen der Aktivitäten möglicher Staatsfeinde wie Juden, Kommunisten, Liberaler und religiöser Gruppen. Am 28. Mai 1936 forderte Heydrich in einem geheimen Befehl an die Staatspolizeidienststellen, dass „die Anwendung verschärfter Vernehmungsmethoden auf keinen Fall aktenkundig gemacht werden“ dürfe. Die Vernehmungsakten gefolterter Beschuldigter seien vom Leiter der jeweiligen Staatspolizeistelle persönlich unter Verschluss aufzubewahren.

Der Wehrmacht war die SS als zweite bewaffnete Organisation im Reich zunehmend ein Dorn im Auge. Die SS wiederum stärkte ihre Position gegenüber der Wehrmacht, indem sie den damaligen Oberbefehlshaber des Heeres, Generaloberst Werner von Fritsch, und den Reichskriegsminister Werner von Blomberg durch gezielte Intrigen aus dem Weg räumte. Damit festigte sie die Kontrolle des Nationalsozialismus über die Wehrmacht.

Rivalität herrschte auch zwischen Heydrichs SD und dem Geheimdienst des Heeres, der Abwehr, unter seinem ehemaligen Gönner Admiral Wilhelm Canaris. Die beiden Chefs unterhielten anfangs nach außen hin ein freundschaftliches Verhältnis und trafen sich jeden Morgen zum gemeinsamen Ausritt. Hinter den Kulissen versuchte jedoch jeder, den anderen auszuschalten – Heydrich ließ Canaris’ Diensträume verwanzen, Canaris ließ nach Belegen für Heydrichs angebliche jüdische Abstammung suchen.

In der Reichspogromnacht, die die SS unter Himmler und Heydrich insofern überraschte, als sie von der Partei und Joseph Goebbels ausging, sandte er am 10. November 1938 ein dringendes Fernschreiben an die StaPo mit verschiedenen Anweisungen. Beispielsweise erwähnt sei die Anordnung,

1939 wurden SD und Sicherheitspolizei (Kriminalpolizei und Geheime Staatspolizei) dem neu geschaffenen Reichssicherheitshauptamt (RSHA) unterstellt, mit Heydrich an der Spitze, der sich personalpolitisch in einer bitteren Kontroverse gegen SS-Juristen um Werner Best durchsetzen konnte. Heydrich wählte ganz bewusst eine Führungsschicht für das RSHA aus, die akademische Intelligenz mit kaltem völkischen Fanatismus zu verbinden wusste. Die Konzeption und die Verwirklichung des RSHA beruhte stark auf den Vorstellungen Heydrichs, der selbst das „politische Konzept einer Verschmelzung von SS und Polizei“ geradezu verkörperte. Mittlerweile war ein riesiger Polizeiapparat entstanden, der überall Informationen sammeln und liefern konnte – ein Instrument zur Ausübung absoluter Herrschaft. Heydrich arbeitete weiter an der Vervollkommnung dieses Apparates, der seine ganze Macht und ideologische Orientierung in der nationalsozialistischen Herrschaft über Osteuropa und der Planung und Durchführung des Holocaust zeigen sollte. Das von ihm nach seinen Vorstellungen geschaffene RSHA wurde „ein entscheidendes radikalisierendes Element der NS-Politik“.

Im August 1940 übernahm Heydrich auch die Präsidentschaft der Internationalen Kriminalpolizeilichen Kommission (IKPK). Bereits seit längerem hatten die Nationalsozialisten versucht, die IKPK unter ihre Kontrolle zu bringen. Nachdem die Präsidentschaft der IKPK 1937 für fünf Jahre an den Wiener Polizeipräsidenten Michael Skubl vergeben worden war, übernahm nach dem „Anschluss“ Österreichs 1938 mit Otto Steinhäusl ein Nationalsozialist die Präsidentschaft. Nach dessen Tod 1940 wurde dann der Sitz der IKPK nach Berlin verlegt, wo sie de facto in das Reichskriminalpolizeiamt, das Amt V des RSHA, integriert war. An ihrem neuen Dienstsitz, "Am kleinen Wannsee 16", sollte eigentlich am 9. Dezember 1941 die Wannseekonferenz stattfinden, die kurzfristig verschoben und verlegt wurde.

Anfang August 1939 leitete Heydrich die Vorbereitungen für den Überfall auf den Sender Gleiwitz und zu weiteren fingierten Zwischenfällen an der deutsch-polnischen Grenze, um polnische Übergriffe als Vorwand für den verbrecherischen Angriffskrieg gegen Polen vorzutäuschen. Ab dem 22. August 1939 lösten als polnische Freischärler verkleidete SD- und SS-Angehörige sowie dazu genötigte Gefangene des KZ Sachsenhausen (die ermordet und als Beweis für Kampfhandlungen liegen gelassen wurden) mehrere „Grenzzwischenfälle“ aus. Am 31. August 1939 überfiel eine Gruppe von SS-Männern unter Führung von Sturmbannführer Alfred Naujocks den Sender Gleiwitz. Am 1. September 1939 begann der Überfall auf Polen, der mit diesen angeblichen polnischen Übergriffen gerechtfertigt wurde.

Den rasch vorrückenden Truppen der Wehrmacht folgten sogenannte SS-Einsatzgruppen, die rücksichtslos gegen die Zivilbevölkerung vorgingen, insbesondere gegen die „Intelligenz“ und „Juden“. Mit ihrem Wirken begann der Vernichtungskrieg gegen die Zivilbevölkerung unterworfener Länder Osteuropas. „Die Einsatzgruppen, die in Polen im Herbst 1939 den polizeilichen "Sicherheitsauftrag" der Besatzungsmacht auf die "Völkische Flurbereinigung", auf Deportation und Massenerschießungen ausdehnten, stellten als mobile Einheiten des RSHA ebenjene "kämpfende Verwaltung" dar, die Heydrich gefordert hatte.“ Noch während des Polenfeldzuges gelang es Himmler und Heydrich, die Kompetenzen von SS und Polizeikräften gegenüber der Wehrmacht weiter auszubauen und durch eigenständige Polizei-Standgerichte neben der Wehrmachtjustiz über Erschießungen zu befinden. Waren zu Beginn des Feldzuges die Einsatzgruppen von SS, SD und Polizei zumindest nominell noch der Wehrmacht nachgeordnet, etablierte die SS sich als selbstständig handelnde Kraft neben der Wehrmacht.

Diese Unabhängigkeit, die eine Kooperation zwischen SS-Einsatzgruppen und Wehrmacht jedoch keineswegs ausschloss, wurde im späteren Verlauf des Krieges im Osten beibehalten. Als Unternehmen Barbarossa begann am 22. Juni 1941 der Krieg gegen die Sowjetunion. Auch hier verübten die Einsatzgruppen der SS Massaker.

Im Zweiten Weltkrieg nahm Heydrich als Reserveoffizier der Luftwaffe zunächst als Bordschütze im Kampfgeschwader 55 am Polenfeldzug, später als Jagdflieger im Jagdgeschwader 77 über Norwegen, Norddeutschland und Holland teil. Er flog eine Messerschmitt Bf 109E-7.

Im Sommer 1941 missachtete Heydrich ein ausdrückliches Verbot des Reichsführers SS Himmler von Kampfeinsätzen, meldete sich am Flugplatz Bălți im Südabschnitt der Ostfront in der Uniform eines Luftwaffenmajors und wurde der II. Gruppe des Jagdgeschwaders 77 zugeteilt, in der er schon früher geflogen war. Sein Flugzeug wurde am Nachmittag des 22. Juli über Jampol von einem sowjetischen Flakgeschoss getroffen, und der Motor fiel aus. Heydrich war gezwungen, zwischen den Frontlinien notzulanden. Im Luftwaffenstützpunkt befürchtete man, „Heydrich sei entweder tot oder – schlimmer noch – dem russischen NKWD in die Hände gefallen“, doch schon nach wenigen Stunden kam die Meldung, eine vorgeschobene Patrouille habe ihn gerettet.
Dass der Fliegereinsatz Heydrichs von Himmler nicht genehmigt war, thematisierte dieser noch in seiner Gedenkrede zu Heydrichs Tod 1942. Darin erwähnte Himmler „mit stolzer Freude“, dies sei „die einzige Heimlichkeit in den elf Jahren unseres gemeinsamen Weges [gewesen], die er vor mir hatte“.

Sein Biograph Robert Gerwarth urteilt, dass Heydrich mit diesen fliegerischen Einsätzen ein „‚heroisches‘ Fronterlebnis“ gesucht habe.

In der Ideologie der Nationalsozialisten galten Juden als Feind schlechthin. Sie wurden als „Untermenschen“ dargestellt und in der NS-Propaganda mitunter mit Ratten (so im Film "Der ewige Jude") und anderem Ungeziefer verglichen.

Schon vor dem Krieg sammelte Heydrich alle Informationen über jüdische Einrichtungen und ließ sie überwachen. Zunächst sollten die Juden durch ein System von Enteignung und Deportation aus dem Reich gedrängt werden. 1938 sandte Heydrich Adolf Eichmann nach Wien, um dort die Zentralstelle für jüdische Auswanderung einzurichten; sie wurde zum Vorbild für die im Januar 1939 eingerichtete Reichszentrale für jüdische Auswanderung in Berlin. Damit erhielt Heydrichs SD eine Schlüsselrolle bei der Judenverfolgung.

Nach der Eroberung Polens gab Heydrich den Befehl, Ghettos für die Juden einzurichten und dort so genannte Judenräte zu bilden. So wurden die jüdischen Gemeinden gezwungen, mit den Nationalsozialisten zusammenzuarbeiten und an ihrem eigenen Untergang mitzuwirken. Mit Eichmanns Hilfe organisierte Heydrich Deportationen von Juden aus dem ganzen Reichsgebiet sowie aus Österreich und Teilen Polens in diese neu errichteten Ghettos. In einer Anweisung vom 22. September 1939 unterschied Heydrich zwischen einem „geheimen Endziel“, dessen Verfolgung langfristig erfolgen müsse, und den Mitteln und Wegen dorthin. Ghettos waren für ihn nur Zwischenstationen. Als Endziel war zu diesem Zeitpunkt eine Deportation aller Juden aus den eingegliederten Gebieten in ein Territorium an der östlichen Grenze Polens angedacht.

Durch die Eroberung Osteuropas fielen Millionen von Juden und anderen Menschen, die als „Angehörige minderwertiger Rassen“ herabgesetzt wurden, in deutsche Hand. Wann der Entschluss zur Ermordung aller Juden gefasst wurde, ist strittig; die meisten Historiker datieren ihn zwischen September und Dezember 1941. Die systematische Ermordung der Juden begann in stufenweise radikalisierten Schritten durch die Einsatzgruppen. Schon acht Tage nach Beginn des Unternehmens Barbarossa unternahm Heydrich am 30. Juni 1941 seine erste Inspektionsreise und forderte umgehend in seinem Einsatzbefehl, der Einsatzgruppe B sollte es doch „bei geschicktem Vorgehen nicht schwer fallen, mit der militärischen Entwicklung Schritt zu halten“. Prompt meldete wenige Tage später Einsatzgruppenchef Arthur Nebe, in den ersten Tagen seien zwar in Grodno und Lidna „nur 96 Juden exekutiert worden“, er habe aber „Befehl gegeben, dass hier erheblich zu intensivieren sei“. Heydrichs Inspektionsreisen trugen zu einem massiven Anstieg der Massenmorde an jüdischen Zivilisten in den besetzten sowjetischen Gebieten bei, so dass schon wenige Wochen nach Kriegsbeginn dazu übergegangen wurde, Frauen und Kinder bei Massenerschießungen umzubringen, wobei das Einsatzkommando 9 unter der Leitung Alfred Filberts das erste war, „das von Ende Juli an systematisch jüdische Frauen und Kinder in Weißrussland umbrachte, offenbar auf ausdrücklichen Befehl Heydrichs“.

Am 31. Juli 1941 wurde Heydrich von Hermann Göring beauftragt, alle erforderlichen Vorbereitungen für eine „Gesamtlösung der Judenfrage“ zu treffen, seien sie finanzieller, organisatorischer oder verwaltungstechnischer Natur. Heydrich erkannte schnell, dass zu diesem Zweck eine zentrale Koordinierung aller beteiligten Stellen erforderlich war. So berief er zum 20. Januar 1942 die Wannsee-Konferenz ein, um Mittel und Wege zur „Endlösung der europäischen Judenfrage“ zu erörtern. Heydrich konkretisierte, was mit den deportierten Juden geschehen sollte:

Zwar sprach er nicht explizit von den nicht „arbeitsfähigen“ Frauen und Kindern, doch ist klar, dass er sie zu den „Keimzelle[n] eines neuen jüdischen Aufbaus“ rechnete, die ebenfalls einer „Sonderbehandlung“ zugeführt werden sollten – in der Sprache der Täter war dies die Tarnbezeichnung für Tötung. Dabei gingen die Massenmorde von Heydrichs SS-Einsatzgruppen weiter. Bis zum Jahresende 1941 töteten sie in den besetzten sowjetischen Gebieten bereits mehr als 500.000 Frauen, Kinder und Männer, meist durch Erschießen aus nächster Nähe. Während des Frühjahrs und Sommers 1942 erschossen sie in der Ukraine und in Weißrussland mindestens 360.000 Juden.

Nach der im Münchner Abkommen von 1938 erzwungenen Abtretung des Sudetenlandes wurde am 15. und 16. März 1939 auch die sogenannte Rest-Tschechei von deutschen Truppen besetzt. Für das Protektorat Böhmen und Mähren wurde ein „Reichsprotektor“ eingesetzt, der in Prag residierte. Mit diesem Amt wurde Konstantin von Neurath betraut, der am 5. Februar 1938 abgesetzte Außenminister des Deutschen Reichs. Neurath nahm seine Aufgabe nach Auffassung von Heydrich und dem SS-Funktionär Karl Hermann Frank nicht mit der nötigen Härte wahr – beide hatten Ambitionen auf Neuraths Posten. Heydrich sammelte Belege über Neuraths angebliche Unzuverlässigkeit, was dazu führte, dass dieser auf unbestimmte Zeit „beurlaubt“ wurde – offiziell wurde mitgeteilt, er habe den „Führer“ aus gesundheitlichen Gründen um seine Abdankung gebeten.

Heydrich wurde zum stellvertretenden Reichsprotektor ernannt, blieb aber gleichzeitig Chef des RSHA. Am 27. September 1941 traf er in Prag ein. In seiner Antrittsrede am 2. Oktober 1941 vor Mitarbeitern der deutschen Protektoratsverwaltung auf der Prager Burg äußerte er sich in drastischer Weise über die Behandlung der tschechischen Bevölkerung, solange man diese wegen ihrer Wirtschaftsleistung für die deutsche Kriegswirtschaft benötige:
Diese wirtschaftspolitische Ausbeutung verlief in Abstimmung mit Ernährungsstaatssekretär Herbert Backe, „einer der wenigen engen persönlichen Freunde Heydrichs“. Da insbesondere die Aufrechterhaltung der tschechischen Rüstungsindustrie für die Kriegführung des Deutschen Reiches von großer Bedeutung war, sollten die tschechischen Arbeiter im Unterschied zu den „minderwertigen Rassen Europas“, die keine kriegswichtige Arbeit für Deutschland leisteten, ausreichend ernährt werden. Nach dem Krieg könne man dann mit den Tschechen abrechnen. Heydrich führte unverzüglich drakonische Maßnahmen gegen die Bevölkerung ein. Bis Ende November 1941 wurden 6000 Menschen verhaftet und offiziell 404 Todesurteile vollstreckt. 1299 dieser alleine in diesen ersten zwei Monaten der Amtszeit Heydrichs Inhaftierten wurden im Winter in das Konzentrationslager Mauthausen deportiert; von ihnen überlebten nur 52 den Krieg. Dies brachte ihm bei der Prager Bevölkerung den Spitznamen „Der Henker von Prag“ ein. Er entschied, dass in "Theresienstadt" ein Konzentrationslager für die jüdische Bevölkerung Böhmens und Mährens errichtet wurde.

Das "Landgut Jungfern Breschan" bei Prag, das zuvor dem jüdischen Zuckerfabrikanten Ferdinand Bloch-Bauer abgenommen worden war, diente der Familie Heydrich als Herrschaftssitz. Es umfasste zwei Schlösser, eine Fläche von 125 Hektar Wald und eine ausgedehnte Gärtnerei. Lina Heydrich ließ fortan Häftlinge aus dem KZ Theresienstadt rekrutieren, um sie als Arbeiter auf dem Landsitz einzusetzen, auf dem zu diesem Zweck ein Außenlager errichtet wurde.

Nach dem Einmarsch deutscher Wehrmachttruppen war ein Teil der tschechischen Regierung nach Großbritannien geflohen. In London etablierte der ehemalige Präsident Edvard Beneš eine Exilregierung, die, um ihr Ansehen zu festigen, Sabotageakte in der besetzten Heimat durchführen ließ. Hierzu wurden von den Briten tschechoslowakische Soldaten ausgebildet, die nachts mit Fallschirmen über dem besetzten Gebiet absprangen. Die Agenten sollten zum tschechischen Untergrund Kontakt aufnehmen und Aktionen wie Sprengungen von Fabrikanlagen und Aufstellung von Funkpeilanlagen zur Orientierung für alliierte Bomber durchführen. Da aber das Überwachungssystem und der Druck der Deutschen auf die tschechische Bevölkerung unterschätzt wurden, blieben die Aktionen meist erfolglos.

Ende 1941 reifte der Plan, eine aufsehenerregende Aktion durchzuführen – ein Attentat auf den verhassten Reichsprotektor, der auch als Chef des Reichssicherheitshauptamtes im Visier des britischen Geheimdienstes stand. Mit harter Unterdrückung war es ihm zunächst gelungen, den tschechischen Widerstand erheblich zu schwächen. Die Aktion, die vom tschechoslowakischen Nachrichtendienst unter der Leitung von František Moravec vorbereitet wurde, erhielt den Decknamen Operation Anthropoid. Unter strengster Geheimhaltung wurde ein enger Kreis von Soldaten hierfür ausgebildet. Am frühen Morgen des 29. Dezember 1941 wurden Jozef Gabčík und Jan Kubiš von einem britischen Halifax-Bomber östlich von Pilsen mit Fallschirmen abgesetzt. Den beiden gelang es, sich nach Prag durchzuschlagen, zum dortigen Untergrund Kontakt aufzunehmen und für die nächsten Monate unterzutauchen. Hier erfuhren sie Einzelheiten über Heydrichs Gewohnheiten und seinen Tagesablauf. So ließ er sich jeden Tag ohne Begleitschutz, meist in offenem Wagen, stets die gleiche Strecke von seinem Landgut zum Prager Hradschin fahren.

In den Wochen vor dem Anschlag war der tschechische Widerstand erstarkt. Heydrich, der seit September 1941 beschönigende Berichte an Martin Bormann geschickt hatte, um „seine ‚Leistungen‘ im Protektorat ins rechte Licht zu rücken“, räumte in einem Schreiben an Bormann am 19. Mai 1942 erstmals ein, dass sich die Lage im Protektorat verschlechtert habe, und sagte auf einer Pressekonferenz in Prag am 26. Mai 1942, einen Tag vor dem Überfall:
Für den Anschlag wählten die Attentäter eine enge, abschüssige Haarnadelkurve in der Prager Vorstadt Libeň aus. In der Nähe gab es keine Polizeistation. Die Kurve konnte nur mit niedriger Geschwindigkeit durchfahren werden. Am Morgen des 27. Mai 1942 postierten sich Gabčík und Kubiš in der Nähe der Kurve. In Aktentaschen hatten sie eine zerlegbare Sten-Gun-Maschinenpistole sowie eine aus speziellem Sprengstoff gefertigte Handgranate mit hoher Explosivkraft. Ein weiterer Agent, Josef Valčík, nahm eine Position oberhalb ein, um Heydrichs Annähern mit einem Taschenspiegel zu signalisieren.
Heydrich verspätete sich an diesem Morgen. Als sein Wagen schließlich eintraf, musste sein Fahrer, SS-Oberscharführer Klein, vor der Kurve den Mercedes stark abbremsen. Gabčík hob seine Maschinenpistole und drückte aus kürzester Entfernung ab. Die Waffe hatte jedoch Ladehemmung, so dass sich kein Schuss löste. Heydrich, im Glauben, es nur mit einem Einzeltäter zu tun zu haben, traf eine für ihn persönlich verhängnisvolle Fehlentscheidung: Er befahl dem Fahrer anzuhalten, und zog gegen Gabčík seine Dienstpistole. Kubiš trat nun aus der Deckung und warf seine Handgranate. Diese prallte am rechten Hinterrad ab und explodierte neben dem Fahrzeug. Heydrich sprang aus dem Wagen und versuchte, auf die Attentäter zu schießen. Sein Fahrer Klein, „durch die Explosion desorientiert, torkelte auf Kubiš zu“, und „Heydrich brach plötzlich mit schmerzverzerrtem Gesicht zusammen, so dass auch Gabčík aus seinem Schussfeld entkommen konnte“. Erst nach einiger Zeit wurde er von tschechischen Polizisten gefunden und in einem Lastwagen ins nahe Krankenhaus "Na Bulovce" (an der Bulovka) gefahren.

Tschechische Ärzte untersuchten Heydrich. Eine Röntgenaufnahme zeigte eine zertrümmerte Rippe, einen Zwerchfellriss und Splitter in der Milz, während seine Nieren unverletzt geblieben waren.

Himmler sandte seinen Leibarzt Karl Gebhardt für die Operation nach Prag. Gebhardts Flugzeug landete mit Verspätung. Inzwischen hatten die deutschen, in Prag lebenden Ärzte Josef Hohlbaum und Walter Dick die Operation vorgenommen. Zunächst schien sich Heydrichs Zustand zu verbessern, doch am 3. Juni trat eine plötzliche Verschlechterung mit hohem Fieber und Sepsis aufgrund einer Bauchfellentzündung ein, die wahrscheinlich durch Partikel der Polsterung des Wagens verursacht wurde, die nicht erkannt in die Bauchhöhle gelangt waren. Wäre Penicillin eingesetzt worden, das nicht zur Verfügung stand, „hätte Heydrich wohl überlebt“. Er fiel ins Koma und starb am 4. Juni 1942 um 4:30 Uhr. Eine Studie im Jahre 2012 kam zu dem Schluss, dass die genaue Todesursache bis heute nicht abschließend geklärt sei; danach ist die bislang häufig vertretene These, er sei an Gasbrand gestorben, nicht haltbar.

Himmler übernahm zunächst selbst die Führung des Reichssicherheitshauptamtes, bis er Ernst Kaltenbrunner am 30. Januar 1943 als neuen Chef des RSHA in sein Amt einführte. Zum Nachfolger Heydrichs als "Stellvertretender Reichsprotektor in Böhmen und Mähren" bestimmte er den "Chef der Ordnungspolizei" Kurt Daluege.

Unmittelbar nach dem Attentat wurde der Gestapo-Beamte und Referatsleiter (Referat II g – "Attentate, illegaler Waffenbesitz und Sabotage") bei der Staatspolizeileitstelle Prag, Heinz Pannwitz, mit der Leitung einer Sonderkommission zur Aufklärung des Heydrich-Attentats betraut. Pannwitz war Autor des amtlichen Abschlussberichtes zum Heydrich-Attentat und verfasste in der zweiten Hälfte der 1950er Jahre zwei Niederschriften zum Attentat.
Das Attentat auf Heydrich traf die NS-Führung anscheinend bis ins Mark. Die Suche nach den Angreifern verlief zunächst hektisch und schlecht organisiert. Mit Hilfe des später hierfür heiliggesprochenen Bischofs Gorazd (mit bürgerlichem Namen Matěj Pavlík) versteckten sich die Attentäter in der Krypta der Karl-Borromäus-Kirche (seit 1935 Kirche St. Cyrill und Method) in Prag. In der Folgezeit übten die deutschen Besatzer vor allem durch Geiselnahme erheblichen Druck auf die tschechische Bevölkerung aus.

Nach dem Attentat auf Heydrich wurde noch am 27. Mai 1942 über das gesamte Protektoratsgebiet umgehend das Standrecht erklärt, das erst am 3. Juli 1942 aufgehoben wurde. Während dieses Standrechts, später in der Tschechoslowakei als "heydrichiáda" (Heydrichiade) bezeichnet, wie auch danach kam es zu umfangreichen Verhaftungswellen (über 3000 Menschen) mit Hinrichtungen von über 1300 Menschen. In der Folge wurde zuerst das Dorf Lidice und wenige Tage später auch Ležáky dem Erdboden gleichgemacht. Alle 184 männlichen Bewohner Lidices über 16 Jahre wurden erschossen (9./10. Juni 1942), die Frauen in Konzentrationslager deportiert, während sich die Kinder einer „rassischen Musterung“ zu unterziehen hatten. Neun der Kinder wurden als „germanisierbar“ eingestuft und zu deutschen Pflegeeltern geschafft, „die übrigen brachte man um“.

Als Rechtfertigung für die Ermordung der Menschen nannte man wider besseres Wissen angebliche Beweise für einen Zusammenhang zwischen Lidice und den Attentätern; denn eine solche Vermutung hatte sich schon vor der Vernichtung Lidices „als falsch erwiesen“. Zu den Opfern von Lidice und Ležáky kamen 3.188 im Sommer 1942 zum Tode verurteilte Tschechen, „davon 477 aus dem einzigen Grund, dass sie das Attentat auf Heydrich ‚gutgeheißen‘ hatten“.

Das Versteck in der Prager Kirche wurde schließlich auf indirektem Wege durch den Hinweis des Fallschirmagenten Karel Čurda gefunden, der am 16. Juni 1942, „um sein Leben zu retten und seine Familie zu schützen“, der Gestapo den Namen der Familie Moravec in Prag nannte, wo die beiden Attentäter zeitweilig untergekommen waren. Der noch minderjährige Sohn der Familie, Vlastimil, brach nach einem brutalen Verhör, „als ihm die Ermittler den abgeschnittenen Kopf seiner Mutter in einem mit Flüssigkeit gefüllten Glasbehälter zeigten und drohten, den Kopf des Vaters dazuzulegen“, zusammen und teilte seinen Peinigern das Versteck in der Karl-Borromäus-Kirche mit.

Nach mehrstündigem Kampf mit SS-Einheiten unter der Leitung von Karl von Fischer-Treuenfeld erschossen sich die Attentäter am 18. Juni 1942 in aussichtsloser Lage. Bischof Gorazd, der die Verantwortung für die Ereignisse in der Kirche auf sich genommen hatte, Pater Petrek, der in der Kirche angetroffen worden war, und zwei weitere orthodoxe Priester, die den Attentätern Zuflucht gewährt hatten, wurden von den Besatzern hingerichtet.

Heydrichs Leiche wurde nach seinem Tod zwei Tage im Hradschin aufgebahrt und anschließend nach Berlin überführt. Am 9. Juni fand in der Neuen Reichskanzlei die seit dem Staatsbegräbnis von Reichspräsident Paul von Hindenburg größte Totenfeier des Dritten Reiches statt, an der alle NS-Größen teilnahmen. Heydrichs Leichnam wurde auf dem Berliner "Invalidenfriedhof" beigesetzt. Die Grabrede hielt Himmler, der betonte, dass „alle Maßnahmen und Handlungen, die er traf, er als Nationalsozialist und SS-Mann anpackte. Aus den tiefen Gründen seines Herzens und Blutes heraus habe er die Weltanschauung Adolf Hitlers erfühlt, verstanden und verwirklicht.“ Hitler pries ihn als „Blutzeuge[n], gefallen für die Erhaltung und Sicherung des Reiches“, und zeichnete ihn postum, als zweiten Deutschen nach Fritz Todt, mit der Obersten Stufe des Deutschen Ordens aus, der höchsten Auszeichnung der NSDAP.

Im Monat nach Heydrichs Tod wurde die Reinhard-Heydrich-Stiftung in Prag gegründet. Formal als "Reichsstiftung für wissenschaftliche Forschung" eingerichtet, diente sie tatsächlich dazu, die deutsche Besetzung Böhmens und Mährens zu rechtfertigen und Arbeiten für die Germanisierungspläne zu leisten.

Zum Jahrestag gab es 1943 erneut eine Gedenkfeier, und am Ort des Attentats wurde eine Büste nach Heydrichs Totenmaske aufgestellt, vor der sich vorübergehende Passanten verbeugen mussten. Zum Todestag wurde eine Gedenkbriefmarke ausgegeben, Heydrich „zum mythisch verklärten ‚Märtyrer‘ im nationalsozialistischen Pantheon der gefallenen Helden“ erhoben und ein „neue[r] Höhepunkt des nationalsozialistischen Totenkults“ inszeniert. Hitler ließ ihn in eine Ehrenliste gefallener „Kämpfer der NSDAP“ aufnehmen und das SS-Gebirgs-Jäger-Regiment 11 der 6. SS-Gebirgs-Division „Nord“ nach ihm benennen, gleichfalls wurden Straßen und Plätze im Reichsprotektorat nach Heydrich benannt.

Thomas Mann urteilte ungefähr zeitgleich in einer Radioansprache in der BBC. Heydrichs Tod nannte er den „natürlichsten Tod“, den ein „Bluthund wie er“ habe sterben können. Denn „wohin dieser Mordknecht kam, floß das Blut in Strömen. Überall, auch in Deutschland, hieß er schlecht und recht: der Henker … Nun, er ist ermordet worden. Und wie nehmen die Nazis das auf? Sie fallen in Krämpfe. Sie stellen sich buchstäblich an, als sei die unfaßlichste Missetat geschehen, als sei der Menschheit Höchstes angetastet […] und ein anderer Metzgermeister [Himmler] sagt ihm am Grabe nach, er sei eine reine Seele und ein Mensch von hohem Humanitätsgefühl gewesen. Das alles ist verrückt ...“

Für viele seiner Zeitgenossen verkörperte Heydrich den Inbegriff des „Ariers“ – blond, schlank und großgewachsen. Dagegen gibt es von seiner auffallend hohen Stimme, die ihm den Spottnamen „Ziege“ einbrachte, trotz der hohen Positionen, die er einnahm, nur wenige Tonbandaufzeichnungen. Dazu war er ein sportlicher Mann und ein fähiger Sportfechter, der an nationalen und internationalen Turnieren teilnahm. Wenn es nach seinem Vater gegangen wäre, hätte er Musiker werden sollen. Heydrich lernte schon früh Klavier und Violine, die er virtuos beherrschte. In der Öffentlichkeit präsentierte er sich, besonders in seiner Prager Zeit, als fürsorglicher Familienvater.

Heydrich galt als Machtmensch und leistete als rechte Hand Himmlers wichtige Arbeiten bei der Integration der Politischen Polizei in den Apparat der NSDAP. Einige Historiker vertreten die These, der im Grunde kleinbürgerliche Himmler mit seinem Hang zur Esoterik hätte ohne den scharfsinnig planenden und entschlossen handelnden Heydrich in dem von Intrigen bestimmten internen Machtkampf der verschiedenen Gruppen in der NSDAP nicht bestehen können. „HHhH – Himmlers Hirn heißt Heydrich“ soll der ehemalige preußische Innenminister und spätere Reichsmarschall Hermann Göring über seine Konkurrenten gewitzelt haben, die ihm Stück für Stück die Hoheit über Polizei und Sicherheitsdienste streitig machten. Diese mehrfach kolportierte Sottise, die der französische Schriftsteller Laurent Binet 2010 zum Titel seines Romans "HHhH" über das Attentat auf Heydrich machte, wurde zuerst von dem Rechtsanwalt und Schriftsteller Carl Haensel verbreitet. Dieser behauptete 1950 in seinen Memoiren, Göring habe 1946 in der Nürnberger Untersuchungshaft zu ihm gesagt: „Sie kennen Himmler nicht. Natürlich nicht. Wissen Sie, er war abgründig dumm. Sein Gehirn hieß Heydrich.“ Die Biographen von Himmler und Heydrich, die Historiker Peter Longerich und Robert Gerwarth, erwähnten das Göring-Zitat allerdings nicht. Longerich betonte:

Heydrich, der den politischen Katholizismus neben den Juden für den Hauptfeind des Nationalsozialismus hielt, spielte sogar mit dem Gedanken, die katholische Kirche durch Einschleusung junger Nationalsozialisten in die Priesterseminare zu unterwandern. Daneben galten ihm auch die Freimaurer als sehr gefährliche Gegner, die, falls sie im Ringen mit dem Nationalsozialismus die Oberhand gewännen, „Orgien der Grausamkeit“ feiern würden, mit denen verglichen „die Strenge Adolf Hitlers sehr maßvoll erscheinen“ werde. In der Berliner Prinz-Albrecht-Straße hatte er in einem fensterlosen, schwarz ausgekleideten Saal ein „Museum der Freimaurer“ eingerichtet, in dem allerlei Kultgegenstände der Freimaurer von einem violetten Licht beleuchtet wurden.

Wenn er sich ablenken wollte, verabredete Heydrich sich angeblich mit engsten Mitarbeitern wie dem jungen SD-Auslandschef Walter Schellenberg zu nächtlichen Streifzügen durch Berliner Bars und Bordelle. In einer Kneipe lachten ihn einmal Gäste, die ihn nicht kannten, sogar aus, als er schrie: „Ich bin der Chef der Gestapo! Ich bin Heydrich! Ich kann euch alle ins Konzentrationslager schicken!“ Derartige Darstellungen basieren aber, so der Historiker Robert Gerwarth, ausschließlich auf Behauptungen Schellenbergs nach dem Krieg und auf Gerüchten.
Gerwarths Biographie von 2011 zufolge war Heydrich als junger Offizier ein noch eher wenig politischer Einzelgänger, und als er sich nach seiner Entlassung aus der Offizierslaufbahn 1931 „auf Druck seiner Verlobten“ um eine „zweite Karriere in Uniform“ bei der „damals noch winzigen SS“ in München bemühte, noch kein ideologisch gefestigter Nationalsozialist. Doch unter dem Einfluss seiner Ehefrau Lina, schon mit 19 Jahren eine überzeugte Nationalsozialistin, übernahm er sehr schnell die ideologischen Prämissen seines politischen Mentors Himmler, entwickelte sich zu „eine[m] der radikalsten Verfechter der nationalsozialistischen Weltanschauung und ihrer Verwirklichung durch rigide und immer ausgedehntere Verfolgungsmaßnahmen“, zum „selbstbewusst auftretenden und ideologisch gefestigten Leiter des RSHA und zum Organisator des Holocaust“. Dabei sah der außergewöhnlich „‚begabte‘ Organisator des Terrors“, Heydrich, sich selbst in erster Linie als „Tatmensch“, weniger als „Visionär“ wie Hitler und Himmler.

Heydrich war seit Dezember 1931 mit der glühenden Nationalsozialistin Lina Mathilde von Osten verheiratet. Aus der Ehe gingen vier Kinder hervor: Klaus (* 1933, der 1943 bei einem Verkehrsunfall starb), Heider (* 1934), Silke (* 1939) und Marte (* 1942).

Heydrich wurde von seiner Jugend an bis ins Erwachsenenalter mit Gerüchten über seine jüdische Abstammung väterlicherseits (angeblicher jüdischer Großvater) konfrontiert. Dies gipfelte in einer Untersuchung, 1932 angeordnet von Gregor Strasser und angestiftet von Rudolf Jordan, dem Gauleiter von Halle-Merseburg. Der Verdacht war in erster Linie auf dem Umstand begründet, dass der Vater, Bruno Heydrich, in „Riemanns Musikenzyklopädie von 1916“ als „Heydrich, Bruno, wirklicher Name Süß“ beschrieben wurde – Süß war ein gängiger jüdischer Name. Diese Formulierung stammte von Bruno Heydrichs ehemaligem Schüler Martin Frey, der mit dem Herausgeber der Enzyklopädie verwandt war und „sich auf diese Weise für seinen Ausschluss aus dem Konservatorium rächen wollte“. Heydrich strengte einen Prozess wegen „Verleumdung“ gegen die Herausgeber an, den er gewann.

Die Untersuchung erbrachte allerdings, dass Heydrichs Großvater früh verstorben war und die Großmutter in zweiter Ehe einen Mann namens Süß geheiratet hatte, somit Heydrich keinesfalls „jüdisches Blut“ in sich habe. Heydrichs Personalakte (einschließlich der Ahnentafel) wurde von Martin Bormann geführt und ist erhalten geblieben. Die Ahnentafel verzeichnet nur eine Generation der mütterlichen Linie; Name, Herkunft und Geburtsort der Großmutter fehlen, während dies eine Anforderung an die Ahnentafel selbst jedes einfachen SS-Mannes war.

Robert Kempner war bis in die 1950er Jahre davon überzeugt, der Leiter der Dienststelle des Sachverständigen für Rasseforschung beim Reichsinnenministerium, Achim Gercke, habe 1932 ein Gefälligkeitsgutachten geliefert. 1966 verfolgte Shlomo Aronson die Ahnentafel Heydrichs mütterlicherseits bis 1688, väterlicherseits bis 1738 zurück und lieferte damit den Beweis, dass alle Gerüchte um die jüdische Abstammung falsch sind. Zum gleichen Ergebnis kam Robert Gerwarth 2011, der zudem betonte, dass Aronsons Dissertation „das Verdienst zu[kommt], einen langlebigen Mythos widerlegt zu haben […], der immer wieder von ehemaligen SS-Kollegen und frühen Biographen neu belebt wurde: den Mythos von Heydrichs jüdischer Abstammung“.

Nachdem die Bundesrepublik Deutschland der Witwe Lina Heydrich (1911–1985) zunächst wegen der Verbrechen ihres Mannes das Anrecht auf eine Witwenrente abgesprochen hatte, prozessierte diese 1956 bis 1959 erfolgreich. Trotz „der führende[n] Rolle ihres verstorbenen Mannes bei der Judenvernichtung […] erging ein Gerichtsbeschluss, der ihr die Rente einer Generalswitwe zubilligte, deren Mann im Kampf gefallen war“, die sie bis zu ihrem Tod 1985 erhielt.

Die gerichtlichen Auseinandersetzungen um die Rente führten 1958 unter der Kanzlerschaft Konrad Adenauers zu einer Kabinettserörterung und einer Großen Anfrage der SPD im Bundestag.

„Als wollte sie den Staatsanwalt und die deutschen Medien verhöhnen, die das Urteil des Gerichts empört kritisiert hatten“, so der Historiker Robert Gerwarth, „wählte Lina Heydrich als Titel für ihre 1976 publizierten Memoiren ‚Leben mit einem Kriegsverbrecher‘“. Die Witwe – die in ihrer Zeit in Prag jüdische Zwangsarbeiter, welche in ihrem Garten zu arbeiten hatten, selber beleidigt und schikaniert hatte und vom Aufseher auch hatte peitschen lassen (bevor sie gegen Nichtjuden ausgetauscht und in die Vernichtungslager deportiert wurden) – betrieb nun auf der Ostseeinsel Fehmarn die Pension „Imbria Parva“, die häufig „ehemalige SS-Kameraden ihres Mannes zu Wiedersehensfeiern“ beherbergte, die dort „Erinnerungen an ‚bessere Zeiten‘ austauschten“.



Der aus Prag stammende und in den USA lebende expressionistische Künstler Stefan Krikl befasste sich im Rahmen seines fotografischen Langzeitprojektes "Postcards from the Front" bzw. "Achtung, Kamera, Aktion!", bei dem er reinszenierte „Schnappschussfotos“ von Miniaturszenarien vom Zweiten Weltkrieg und vom Holocaust erstellt, auch mit dem Attentat auf Heydrich und den Racheakten der Nationalsozialisten. Er schuf unter anderem von 2000 bis 2005 die Serie "Assassination of Reinhard Heydrich" (deutsch "Ermordung von Reinhard Heydrich"). Hierbei „verwendete“ Krikl für Heydrichs Person eine Charakterstudie eines „SS-Führer-Typus“; eine von ihm gestaltete Miniaturfigur, die er in verschiedenen Szenarien unterschiedlich einsetzte.

Biographien über Reinhard Heydrich:

Biographische Skizzen über Heydrich:

Publikationen speziell zum Attentat auf Heydrich:

Publikationen zu SS, SD und Gestapo:

Publikationen zu Spezialaspekten von Heydrichs Biographie und Wirksamkeit:



</doc>
<doc id="12146" url="https://de.wikipedia.org/wiki?curid=12146" title="Literaturalmanach Metropol">
Literaturalmanach Metropol

Im Jahr 1979 stellten die Schriftsteller Wassili Aksjonow, Andrei Bitow, Fasil Iskander, Wiktor Jerofejew und Jewgeni Popow einen Literaturalmanach zusammen, in dem sie Texte von anerkannten Schriftstellern mit solchen von noch relativ unbekannten Autoren aus dem "Untergrund" kombinieren wollten. Der Almanach sollte über die Entwicklungen in der Literatur der Metropole Moskau informieren - daher der Name.

Gemeinsam war den mehr als zwanzig Autoren neben ihrer literarischen Begabung, dass niemand von ihnen mit der sowjetischen Macht sympathisierte. Von Genre und Stil her waren die Texte bunt gemischt. Unter anderem sollte hier zum ersten Mal eine größere Auswahl von Liedtexten des berühmten Sängers Wladimir Semjonowitsch Wyssozki publiziert werden.

Was als literarisches Unternehmen geplant war, wurde zu einem der großen Skandale in der russischen Literaturgeschichte. Der Almanach sollte zuerst in zwölf von den Autoren selbst zusammengestellten und -geklebten Exemplaren im Eigenverlag (Samisdat) herausgegeben und dann an die Agentur für Autorenrechte (WAAP) und den Staatsverlag "Goskomisdat "weitergegeben werden. Außerdem wollten sie ihn am 21. Januar 1979 im Rahmen einer Vernissage in einem Café präsentieren.
An diesem Tag wurde der Häuserblock um das Café abgeriegelt, das Lokal selbst - angeblich wegen dort entdeckter Schaben - geschlossen. 

Doch bereits am Vortag der Vernissage begann mit einer Vorstandsversammlung des Schriftstellerverbandes eine monatelange Kampagne, in der die Herausgeber systematisch von ihren systemtreuen Schriftstellerkollegen denunziert und beschimpft wurden. Ihre Literatur wurde als "Pornographie des Geistes", der Almanach als politische Provokation bezeichnet. Man sah in Aksjonow, der von allen damals literarisch bereits am stärksten etabliert war, den von der CIA gelenkten Drahtzieher der Aktion.

Da sie sich trotz wiederholter Aufforderungen und Vorladungen nicht vom Almanach lossagen und öffentlich bereuen wollten, wurden Jerofejew und Popow schließlich aus dem Schriftstellerverband ausgeschlossen, was damals so viel wie den "literarischen Tod" bedeutete. Jerofejews Vater verlor seine Stelle als hoher Diplomat in Wien. Nach Ansicht Jerofejews hätte die Sache für sie wesentlich schlimmer - nämlich im Straflager - enden können, hätten sie nicht die öffentliche Unterstützung sowjetischer und amerikanischer Schriftstellerkollegen gehabt: die Amerikaner Kurt Vonnegut, William Styron, John Updike, Arthur Miller und Edward Albee setzten sich in einem am 12. August 1979 in der New York Times veröffentlichten Telegramm für die Metropol-Herausgeber ein, in der Sowjetunion schrieben Wassili Aksjonow, Andrei Bitow, Fasil Iskander, Inna Lisnjanskaja, Semjon Lipkin und Bella Achmadulina, die alle auch am Almanach mitgewirkt hatten, Protestbriefe. Aksjonow, Lisnjanskaja und Lipkin traten aus Solidarität aus dem Schriftstellerverband aus, Aksjonow ging auf Einladung einer dortigen Universität in die USA, und ihm wurde die sowjetische Staatsbürgerschaft aberkannt. Die anderen folgten der Bitte Popows und Jerofejews nicht auszutreten. 

Der Almanach wurde bald darauf im amerikanischen Verlag "Ardis "auf russisch herausgegeben und erschien dann übersetzt in den USA und in Frankreich.

Heute ist der Almanach Metropol nicht nur wegen seiner Geschichte, sondern auch wegen der Texte ein Denkmal der russischen Literaturgeschichte. Vor einigen Jahren gab ihn Viktor Jerofejew in einer Reihe von Anthologien zur russischen Literatur neu heraus.


</doc>
<doc id="12147" url="https://de.wikipedia.org/wiki?curid=12147" title="Typhon (Mythologie)">
Typhon (Mythologie)

Typhon ( "Typhón", auch "Typhoeus", "Typhaon") ist als Sohn der Gaia und des Tartaros eine Gestalt der griechischen Mythologie.

Gaia vereinte sich mit dem Tartaros, um sich für die Niederlage ihrer Kinder, der Titanen und Giganten, an Zeus zu rächen. Sie gebar den Typhon in den Korykischen Grotten bei Korykos in Kilikien im südöstlichen Kleinasien, so dass er ebenso wie Zeus in seiner eigenen Jugend ungestört heranwachsen konnte. Er wurde als unbeschreiblich grässliches Ungeheuer, als Riese mit hundert Drachen- oder Schlangenköpfen dargestellt, wobei diese in der Sprache der Götter und vieler Tiere sprechen konnten.

Die späthellenistischen Griechen setzten Typhon mit dem ägyptischen Gott Seth gleich.

Typhon kommt in der griechischen Mythologie die Rolle des Vaters der warmen und gefährlichen Winde zu. Dies steht in Verbindung zum griechischen "typhein" („rauchen“), aus dem sich das Wort typhon wahrscheinlich auch ableitet. Das persische طوفان "Tufân" und auch die Araber erweiterten diesen Begriff um die tropischen Wirbelstürme im indischen Ozean, woraus sich wahrscheinlich der heutige Begriff Taifun ableitet. Sehr ähnlich in Aussprache und Bedeutung ist das chinesische 颱風 "tái fēng" „Taifun“, wobei dieses Wort wahrscheinlich auf den Min-Ausdruck 風篩 "fēng shāi" „siebartiger Wind“ zurückgeht. Ob es eine linguistische Verbindung zwischen beiden Begriffen gibt, ist jedoch derzeit noch unbekannt.

In einer Variante des Mythos ist nicht Gaia, sondern Hera die Mutter des Typhon. Hera ist über Zeus erzürnt, der ohne ihre Beteiligung und Mutterschaft die Göttin Athene aus seinem Kopf geboren hat. Nun will Hera es ihm gleichtun und ohne Zeus’ Beteiligung ein Schrecken erregendes und dadurch auch im Olymp Respekt erzwingendes Geschöpf aus sich gebären.

Sie bittet Gaia um diese Gunst, Gaia gewährt sie, und nach Ablauf der Zeit gebiert Hera den Typhon, den sie in Pflege und Wacht eines bei Delphi hausenden weiblichen Drachen gibt, der Python, die später von Apollon erschossen wird.

Nachdem er groß geworden war, stieg er zum Olymp empor, wo er die Götter mit seinem Gebrüll aus hundert Kehlen so verängstigte, dass sie nach Ägypten flohen, wo sie sich als Tiere versteckten. So verwandelte sich Zeus in einen Widder – während Athene als einzige standgehalten hatte und Zeus und die anderen verspottete. Zeus nahm endlich wieder seine ursprüngliche Gestalt an und stellte sich dem Typhon zum Kampf. Von einem Blitz des Zeus getroffen, floh Typhon zum Berg Kasion, wo es erneut zum Kampf kam. Im entstehenden Kampfgemenge konnte Typhon den Zeus mit seinen zahlreichen Armen so umschlingen, dass er ihm schließlich die Sichel, die schon den Uranos entmannt hatte, entwenden konnte. Er trennte die Sehnen des Zeus aus dessen Leib und gab diese der Delphyne, einem Ungeheuer, das in der korykischen Höhle wohnte, in der er geboren worden war. Und dort wurde Zeus nun auch versteckt. Hermes fand den hilflosen Zeus endlich, lenkte Delphyne ab, stahl ihr die Sehnen und gab sie Zeus zurück.

Zeus holte sich vom Olymp einen neuen Vorrat an Donnerkeilen, da ihm auch diese von Typhon entwendet worden waren, und stellte sich Typhon erneut zum Kampf. Dieser musste sich auf den Berg Nysa zurückziehen, wo er dem Rat der Schicksalsgöttinnen folgend Nahrung zu sich nahm, wie sie auch die Menschen essen – Eintagsfrüchte, die ihn nur vorübergehend stärken sollten. Im Vertrauen auf seine nur scheinbar wiedergewonnene Kraft trat er Zeus auf dem thrakischen Berg Haimos entgegen und warf riesige Steine auf diesen, der mit Blitz und Donner antwortete und Typhon übel zurichtete. Das hier vergossene Blut des Typhon soll dem Berg den Namen gegeben haben, denn "haima" ist die griechische Bezeichnung für „Blut“.

Typhon floh nach Sizilien und Zeus warf den Ätna auf Typhon. Seitdem ist Typhon unter dem Ätna gefangen. In seiner Wut ließ er den Ätna immer wieder erbeben sowie Feuer und Gestein spucken.

Bevor Typhon besiegt wurde, zeugte er mit seiner Gemahlin Echidna mehrere Ungeheuer: den dreiköpfigen Kerberos, der als Höllenhund den Eingang zum Hades bewacht, den zweiköpfigen Orthos, die Sphinx, die Chimära, die Hydra und den Adler Aithon.




</doc>
<doc id="12149" url="https://de.wikipedia.org/wiki?curid=12149" title="Blasinstrument">
Blasinstrument

Blasinstrumente sind Musikinstrumente, bei denen ein Musiker üblicherweise durch seine Atemluft meistens mit einem Instrumentenmundstück die Luftsäule innerhalb eines Hohlkörpers – meistens einer Röhre – zum Schwingen bringt. Blasinstrumente zählen gemäß der Hornbostel-Sachs-Systematik zu den Aerophonen. Der Luftstrom kann auch mit einem Luftsack oder Blasebalg direkt oder mittels einer Spielmechanik wie bei der Orgel erzeugt werden. Es gibt verschiedene Möglichkeiten, die Vielfalt der vorhandenen Blasinstrumente in Gruppen einzuordnen. 
Die Einteilung der Blasinstrumente orientiert sich an der Hornbostel-Sachs-Systematik und erfolgt in Holz-, Blechblasinstrumente und Durchschlagzungeninstrumente nach der Art der Tonerzeugung:

Die Holzblasinstrumente lassen sich unterteilen in die Flöteninstrumente (mit Anblaskante) und die Rohrblattinstrumente mit einem oder zwei schwingenden Rohrblättern.

Die ältesten Instrumente mit Anblaskante, wahrscheinlich die ältesten Blasinstrumente überhaupt, sind jene in der Art der Blockflöte. Das kleinste und lauteste Instrument dieser Gattung ist die Trillerpfeife. Auch die Panflöte lässt sich weit in die Vorzeit zurückverfolgen. Die Flöte des Alten Testaments heißt Chalil. Aus der Schwegelpfeife des Mittelalters entstand die Traversflöte, die sich zur Querflöte und ihren Unterarten (Piccoloflöte, Altflöte) weiterentwickelte. Nur noch selten in Verwendung ist das mit der Blockflöte eng verwandte Flageolett.

"Vergleiche den Hauptartikel" Einfachrohrblattinstrument

Eine elastische Zunge schlägt gegen den Luftstrom auf eine Kehle und erzeugt somit einen Ton. Verwendet wird das Prinzip mit Metallzunge bei der Martinstrompete oder mit Holzzunge beim Bordun der Sackpfeife. Aus dem Chalumeau mit hölzernem Rohrblatt entwickelte sich die Klarinette (und ihre Unterarten Bassklarinette und Bassetthorn). Auch das Saxophon gehört auf Grund dieser Tonerzeugung zu den „Holzbläsern“. 

"Vergleiche den Hauptartikel" Doppelrohrblattinstrument

Das Doppelrohrblatt besteht aus zwei aufeinander liegenden symmetrischen Rohrblättern, die gegen den Luftstrom schwingen. Die ältesten Vertreter dieser Art sind die primitiven Doppelrohrblattinstrumente im antiken Ägypten und Griechenland (Aulos), spätere Vertreter sind Schalmei, Piffero, Rankett und Pommer. Im modernen Orchester findet man Oboen (Oboe d’amore, Englischhorn, Heckelphon) und Fagotte (Kontrafagott). Aber auch im Dudelsack und in anderen Instrumenten mit Windkapsel stecken Doppelrohrblätter.

Diese Instrumente werden mit den menschlichen Lippen des Mundes angeblasen. Die Tonerzeugung lässt sich mit Funktionsweise der Polsterpfeife erklären. Die Vorgänger der heutigen Blechblasinstrumente waren aus Hornsubstanz (Kuh-, Stier- oder Widderhorn). Blech als heutige Materialverwendung spielt für die Einteilung keine Rolle. Musiziert wird mit der Naturtonreihe, Längenveränderungen sind mit Ventilen oder Teleskoprohren möglich. Auch die Blechblasinstrumente können in zwei Familien gegliedert werden, die Unterscheidung erfolgt nach der verwendeten Mensur: Ist diese zum größeren Teil konisch, spricht man von Instrumenten der "Hornfamilie", wenn die zylindrische Bauweise überwiegt, von "Trompeteninstrumenten".

Die ältesten Naturhörner waren Tierhörner, der Schofar aus Widderhorn, der Olifant aus Elfenbein, das Alphorn aus Holz und die gegossenen Bronze-Luren. Aus Holz wurden der Zink und das Serpent hergestellt, mit großer Fertigkeit aus gewalztem Blech (Messing) gebaut wurden: Parforcehorn, später Jagdhorn und Ophikleide. Das Kornett wurde noch vor dem modernen Horn mit Ventilen ausgestattet. Eine vollständige Unterfamilie der Horninstrumente mit Vertretern in allen Lagen sind die Bügelhörner, zu denen auch Flügelhorn, Tenorhorn, Bariton, Helikon, Sousaphon und Tuba zählen.

Aus den Naturtrompeten, die für Fanfaren und militärische Signale benutzt wurden, entwickelte sich die verfeinerte, aber immer noch ventillose Barocktrompete und daraus später die moderne Trompete. Die tiefen Vertreter dieser Familie sind die Posaunen (Ventilposaunen oder Zugposaune).

Ein Didgeridoo hat ein zylindrisches oder leicht konisches Lumen mit etwa gerader Achse.

Hierzu gehören nur jene Instrumente, deren Schwingungen durch einen Luftstrom angeregt werden und bei denen eine Luftsäule zur Resonanz gebracht wird. Dazu zählen insbesondere die Zungenpfeifen der Orgel oder die Mundorgel Sheng, eines der wichtigsten Instrumente der klassischen chinesischen Musik. Ein ähnliches Instrument wird in Japan Shō und in Laos Khaen genannt.
Durchschlagzungeninstrumente ohne Resonator wie die Mundharmonika, trotzdem sie augenscheinlich angeblasen wird, sowie das Akkordeon oder das Harmonium zählen zu den freien Aerophonen.

Membranopipes wurden 2011 als weitere Gruppe von Blasinstrumenten in die Hornbostel-Sachs-Systematik eingefügt, weil sie keiner vorhandenen Kategorie zugeordnet werden konnten. Bei den Membranopipes ist der Tonerreger eine gespannte Membran, die durch einen Luftstrom angeregt wird so periodisch einen Luftdurchlass öffnet und schließt, wodurch die Luft in einer Röhre in Schwingung versetzt wird. Im Ruhezustand verschließt bei Membranopipes die Membran den Luftdurchlass, während bei den ansonsten nach Art der Tonerzeugung ähnlichen Rohrblattinstrumenten im Ruhezustand der Luftdurchlass geöffnet ist und sich erst durch die Blasluft periodisch schließt. 

Bei Blasinstrumenten wird eine Luftsäule im Resonanzkorpus durch Anblasen zu Schwingungen angeregt. Die Anblastechnik und die Form des Instrumentenmundstücks beeinflussen wesentlich den Toncharakter. Bei vielen Varianten des Instruments lässt sich die Grundschwingung auch überblasen: durch stärkeres Anblasen wird statt des Grundtones ein höherer Naturton angeregt, bei Blechblasinstrumenten wird dies über eine höhere Luftgeschwindigkeit und erhöhte Lippenspannung erreicht. Mittels Zirkularatmung kann ein Ton ununterbrochen erzeugt werden.


Der Versuch, den Klang von Blasinstrumenten elektronisch zu erzeugen, wird durch die komplexe individuelle Art der Anblasgeräusche und der Transienten im ausgehaltenen Ton erschwert.



</doc>
<doc id="12150" url="https://de.wikipedia.org/wiki?curid=12150" title="Aerophon">
Aerophon

Aerophone (von gr. ἀήρ „Luft“ und φωνή „Klang“; auch "Luftklinger") werden in der Hornbostel-Sachs-Systematik von 1914 alle Musikinstrumente genannt, bei denen der Klang durch direkte Schwingungsanregung der Luft erzeugt wird. Sie können eingeteilt werden in


Für den praktischen Gebrauch ist die Unterscheidung nach der Tonerzeugung, die im Schema von Hornbostel und Sachs erst auf der nächsten Ebene folgt, meist sinnvoller.

Dabei wird die vorbeiströmende Luft geteilt und so modifiziert, dass es zur Schallerzeugung kommt.


Bei Unterbrechungs-Aerophonen wird die vorbeiströmende Luft regelmäßig unterbrochen, wodurch es zur Schallerzeugung kommt.

Die meisten Unterbrechungs-Aerophone sind "selbstklingend", das heißt, der Luftstrom bewegt selbst das Element, das den Luftstrom regelmäßig unterbricht. Man unterscheidet:

Bei "nichtselbstklingenden" Unterbrechungs-Aerophonen wird das unterbrechende Element nicht vom Luftstrom selbst bewegt. Hierzu gehören das Schwirrholz, der Schnurrer und die Sirene.

Die Luft wird dabei einmalig verdichtet. Ein Beispiel ist das Kinderspielzeug Knallbüchse. Auch bei einer Peitsche entsteht der Knall durch Verdichtung der Luft am Riemenende.



</doc>
<doc id="12158" url="https://de.wikipedia.org/wiki?curid=12158" title="Stabreim">
Stabreim

Stabreim ist der deutsche Begriff für die Alliteration in germanischen Versmaßen. Die am stärksten betonten Wörter eines Verses werden durch gleiche Anfangslaute (Anlaute) hervorgehoben.

Die Bezeichnung Stabreim geht zurück auf Snorri Sturluson (1178–1241), den Verfasser der Snorra-Edda (Prosa-Edda oder auch Jüngere Edda); dort tritt altnord. "stafr" (Stab, Pfeiler, Buchstabe, Laut) in der Bedeutung „Reimstab“ auf. Der deutsche Ausdruck Stabreim ist eine Lehnübersetzung aus dem dänischen "stavrim".

Die gesamte altgermanische Versdichtung verwendete den Stabreim, bis er durch den Endreim abgelöst wurde. Der Stabreim bildete die metrische Grundlage für die Versmaße "Fornyrðislag" und "Dróttkvætt" sowie deren Urform, die germanische Langzeile. Bedeutende Werke in altenglischer ("Beowulf"), altsächsischer ("Heliand"), althochdeutscher ("Hildebrandslied") und altnordischer Sprache ("Lieder-Edda") sind in stabreimenden Langzeilen verfasst.

Auch in der modernen Alltagsrhetorik kommen stabreimartige Alliterationen häufig bei der Bildung von phraseologischen Zwillingsformeln vor (z. B. "frank und frei, klipp und klar, Leib und Leben").

Die Stabreimdichtung hat ihren Ursprung in mündlicher Rede. Der Übergang zwischen Prosa und Vers ist für sie deshalb, im Gegensatz zur heutigen deutlichen Trennung von Gedicht und normaler Rede, sehr einfach zu bewältigen. Der Stabreim setzt an den betonten Silben eines Satzes an und lässt sie alliterieren bzw. „staben“. Zeile 3 des "Hildebrandsliedes" soll dies verdeutlichen:

In diesem Satz gibt es vier Wörter, deren Anfang ein zeitgenössischer Redner besonders betont hätte (markiert durch " und '). Drei der vier betonten Silben, auch Hebungen genannt, staben (markiert durch "). Der Konsonant "h" trägt den Stab. Der Redner verteilt die Stäbe nach festen Regeln auf den Anfang und das Ende einer Zeile, die sich aus Anvers, Zäsur und Abvers zusammensetzt. Es ergibt sich folgende Struktur:

Während im Anvers ein bis zwei Stäbe vorkommen können, darf der Abvers nur einen Stab haben, der immer auf das erste der beiden betonten Wörter dieses Teilverses fallen muss. Das zweite betonte Wort bleibt immer stabfrei (im obigen Beispiel „tuem“). Da die Position des Stabes im Abvers immer gleich ist, nannte Snorri Sturluson ihn in seiner Snorra-Edda Hauptstab (hǫfuðstafr). Die Stäbe im Anvers nannte er Stützen (stuðlar), da es drei verschiedene Möglichkeiten gibt, sie zu stellen.

Das Stilmittel der Alliteration kommt u. a. auch in der keltischen und (seltener) in der lateinischen Sprache vor, weswegen der Ursprung nicht ausschließlich im Altgermanischen zu suchen ist. Eine Erklärung für die Ausbreitung des Stabreims in voneinander weitgehend unabhängigen Sprachgebieten könnte in der jeweils typischen sprachlichen Akzentuierung liegen. Einer Sprache, die durch einen dynamischen Akzent oder Stammsilbenakzent gekennzeichnet ist, fällt der Anlautreim ganz natürlich zu. So entstehen auch heute in der Werbesprache noch hin und wieder Stabreime (z. B. „Geiz ist geil“), deren Ursprünge ebenfalls nicht in der altgermanischen Versbautradition liegen.

Bei den Germanen muss der Stabreim bereits vor 2000 Jahren tief verwurzelt gewesen sein. Aus dieser Zeit stammen jedenfalls die ersten antiken Quellen, die die germanische Sitte bezeugen, Verwandtennamen miteinander staben zu lassen. Beispiele dafür sind die drei Cherusker Segestes, Segimundus und Segimerus, von denen u. a. Tacitus berichtet. Aus dem "Hildebrandslied" sind Heribrand, Hildebrand und Hadubrand bekannt und aus dem "Nibelungenlied" die Brüder Gunther, Gernot und Giselher.

Runeninschriften mit Stabreimen ("Runendichtungen") treten zahlenmäßig weit hinter die schriftlichen Quellen in lateinischer Schrift zurück. Es sind kaum mehr als 500 Zeilen überliefert. Sie sind für die Forschung von besonderem Wert, da man nur durch sie etwas über die frühe Stabreimdichtung erfahren kann. Allgemein gilt die Runeninschrift auf dem Goldhorn von Gallehus (Dänemark um 400 n. Chr.) als ältester Beleg eines germanischen Stabreims. Die Inschrift gibt eine Langzeile mit vier Hebungen und drei Stäben wieder.

<poem style="margin-left: 3em">
ek HléwagastiR HóltijaR : hórna táwido. (Ich HlewagastiR, Holts Sohn, fertigte das Horn.)
</poem>

Der früheste (und einzige) runische Beleg für einen Stabreim im südgermanischen Raum findet sich auf der Gürtelschnalle von Pforzen (6. Jh.). Allerdings muss man im Abvers die Runen „l“ und „t“ im vierten Wort als Binderune „el“ lesen, um eine vollständige Langzeile mit drei Stäben zu erhalten:

<poem style="margin-left: 3em">
Áigil andi Áïlr"ûn" : élah"u" gasók"un"
</poem>

Die Bedeutung der Inschrift ist in der Forschung umstritten. Einige Runologen sehen in den Namen das mythische Liebespaar Egil und Ölrún, von denen man im Wielandlied der "Lieder-Edda" und in der "Thidrekssaga" liest. Solche frühen Zeugnisse der Runendichtung sind jedoch selten. Zur Blüte gelangte sie erst zwischen dem 9. und 11. Jh. in Form der Nachrufgedichte auf Runensteinen. Oft werden in diesen die Regeln zum Versbau nicht so genau genommen, was aber als Anzeichen dafür gesehen wird, wie leicht der Stabreim aus der natürlichen Rede hervorgeht. In einigen der Inschriften kann man schon mehr oder weniger korrekt ausgeführte Versmaße erkennen: Stein von Rök ("Fornyrðislag"), Tunestein ("Ljóðaháttr"), Stein von Karlevi ("Dróttkvætt").

Da es im 8. Jh. n. Chr. vor allem Geistliche sind, die die Zeit und Befähigung haben in lateinischer Schrift zu schreiben, ist ein großer Teil der ersten überlieferten Stabreimverse christlich orientiert. Man verwendete den Stabreim teilweise, um den Heiden das Christentum nahezubringen. So ist zum Beispiel der altsächsische "Heliand" eine als Heldenlied gestaltete Erzählung von Jesus Christus. Heidnischen Werken wurde wenig Priorität zugemessen, oft ist ihre Überlieferung nur glücklichen Umständen zu verdanken. Das für die althochdeutsche Literatur bedeutende "Hildebrandslied" wurde beispielsweise auf die erste und letzte Seite eines geistlichen Codex geschrieben. Da der Platz nicht ausreichte, blieb das Lied unvollständig. Die ca. 63.000 Zeilen umfassende stabreimende Dichtung verteilt sich deshalb sehr unterschiedlich auf die germanischen Sprachen. Aus England und Skandinavien, wo sich die Geistlichkeit mehr als in Deutschland auf den Stabreim einließ, ist auch mehr überliefert.

Der Stabreim wurde für viele unterschiedliche Textarten verwendet. Es finden sich religiöse Texte heidnischen Glaubens (Götterlieder, Zaubersprüche) neben denen des christlichen (Gebete, Übertragungen der Genesis oder der Bergpredigt, Buchepik) und auch den weltlichen Bereich deckte man breitflächig ab (Heldenlieder und Epen, Gedichte, Grabinschriften). Der Stabreimvers lässt sich daher nicht auf einen speziellen Anwendungsbereich einschränken. Er ist stilisierte, nachdrücklich gesteigerte Prosarede, die man verwendete, wo man seinen Worten besonderes Gewicht verleihen wollte.

Die altdeutsche Stabreimdichtung löste sich im Laufe des 9. Jh. als erstes auf. Von den vier althochdeutschen und zwei altsächsischen Werken, die überhaupt im Stabreim überliefert wurden, stützen sich nur zwei (das "Hildebrandslied" und die "Merseburger Zaubersprüche") auf eine mündliche Tradition. Die restlichen sind neu und deshalb anfällig für neue Einflüsse. Deshalb mag es nicht verwundern, wenn sich der Endreim mit dem "Evangelienbuch" des Otfrid von Weißenburg in Deutschland durchsetzte und bis heute blieb. 

Es war aber nicht nur ein Umschwung von heidnischer zu christlicher Tradition, die den Stabreim gefährdete. Auch sprachliche Gründe haben eine Rolle gespielt. So behielt das Althochdeutsche viele kurze betonte Silben, die in anderen germanischen Dialekten zu unbetonten Silben geschwächt wurden (vgl. altnord. "haukr" und ahdt. "habuh"). Das Althochdeutsche bewahrte die Länge, wo andere Dialekte kürzten und geriet damit in Konflikt mit den metrischen Erfordernissen des Stabreims.

In Skandinavien und England hielt sich die Stabreimdichtung bedeutend länger. In England wurde er von der Geistlichkeit bis ins 11. Jh. verwendet, um biblische Geschichten nachzuerzählen (altenglische Buchepik). Diese Tradition brach schließlich ziemlich genau mit dem Ende der skandinavischen Herrschaft in England (1066, Schlacht bei Hastings) ab. Die letzten regeltreuen Verse stammen aus einer Chronik des Jahres 1065. Es gab jedoch noch im 14. Jh. Werke wie "Piers Plowman" die stabreimend waren, wenn auch nicht mehr regeltreu.

Im 13. Jh. öffnete sich auch Skandinavien endgültig dem Endreim. Es dauerte nicht lange, bis der Stabreim nur noch in festen Formeln oder in bewusst altertümelnder Absicht verwendet wurde.

<poem style="margin-left: 3em">
Og vil du ikke danse hos mig,
Sót og Sýgdom skal følge dig!
</poem>

Nur in Island gelang dem Stabreim der Sprung in die Neuzeit. Man verband ihn, zusammen mit anderen skaldischen Elementen wie den "Kenningen" oder der Silbenzählung, mit dem Endreim und dem alternierenden Rhythmus. Das Produkt waren die Rímur (Reime), welche in der volkstümlichen Dichtung bis ins 20 Jh. lebten und heute am Verschwinden begriffen sind.

<poem style="margin-left: 3em">
Vorið eg að vini kýs, 
verður nótt að degi, 
þegar glóærð geisladís 
gengur norðurvegi.
Frühling ich zum Freunde wähl,
es wird Nacht zum Tage,
wenn die gluthaarige Sonnengöttin
geht Nordwege. ("Gemeint: Die Sonne")

Frühling wähl zum Freund ich nur,
frisch wird Nacht zum Tage,
wenn der Göttin Glutfrisur
geht nach Nord in Lage.
</poem>

Reste des Stabreims überlebten besonders dort, wo sich die Sprache nicht oft änderte – also in Sprichwörtern, Formeln, Hausinschriften oder der Sprache im Rechtsgebrauch. Allerdings war es vielmehr als der Stabreim selbst der Hang zur altertümelnden Alliteration, der überlebte, weil man ohne die Einbindung in einen Vers nicht von einem Stabreim sprechen kann. Die Alliteration jedoch, die einen Stabreimvers bestimmt, lässt sich noch in vielen Zwillingsformeln nachvollziehen. Sie lassen sich in jeder germanischen Sprache finden:

Im 19. Jahrhundert entdeckten Dichter und Gelehrte den Stabreim wieder. Der Komponist Richard Wagner verwendet ihn in seinen Werken, doch aus Unwissenheit oder künstlerischer Freiheit lässt er der Alliteration so freien Lauf, dass er doppelte und sogar dreifache Stäbe nicht nur im Anvers, sondern auch im Abvers zulässt, was dem ursprünglichen Versbau stark widerspricht.

<poem style="margin-left: 3em">
Wer so die Wehrlose weckt, dem ward, erwacht, sie zum Weib!
</poem>
Auch J. R. R. Tolkien belebte in seinen Werken den Stabreim wieder. In dem Roman "Der Herr der Ringe" ist es das Volk der Rohirrim, dem er stabreimende Verse in den Mund legt.

<poem style="margin-left: 3em">
Arise now, arise, Riders of Théoden!
Dire deeds awake, dark is it eastward.
Let horse be bridled, horn be sounded!
Forth Eorlingas!
</poem>

Der Stabreim erfasst die am stärksten betonten Wörter eines Satzes und lässt den ersten Laut ihrer Wurzelsilben miteinander staben. Es trifft in der Regel Konsonanten ("konsonantischer Stabreim"), wobei die Konsonantenpaare sc/sk, sp und st jeweils als eine Einheit betrachtet werden. Sie staben also nur mit sich selbst und nicht mit einem einzelnen „s“ oder anderen Zusammensetzungen. Eine weitere Besonderheit ist, dass alle Vokale untereinander staben ("vokalischer Stabreim"), wie Zeile 33 aus dem "Beowulf" zeigt:

<poem style="margin-left: 3em">
isig ond utfus, æþelinges fær (eisig und auslaufbereit, das Gefährt des Edlen)
</poem>

Der vokalische Stabreim, der mit normaler Alliteration nicht mehr viel zu tun hat, wird oft mit einem Knacklaut ("Glottisschlag") erklärt, der dem gesprochenen Vokal vorangeht. Demnach wäre auch der vokalische Stabreim ein konsonantischer Stabreim, bei dem der Knacklaut stabt. Den Knacklaut gibt es heute noch im Deutschen und Dänischen. Seine frühere Existenz im Germanischen ist zweifelhaft. Der vokalische Stabreim wurde in der Dichtung oft verwendet. Man bevorzugte sogar die Kombination ungleicher Vokale gegenüber gleichen Vokalen. Für den konsonantischen Stabreim lässt sich dieselbe Vorliebe zur Variation nachweisen. Man bevorzugte hinter dem stabenden Konsonant ungleiche gegenüber gleichen Vokalen.

Die Langzeile ist der ursprünglichste der germanischen Stabreimverse und Vorlage für alle späteren eddischen und skaldischen Versmaße. Ob sie selbst eine Vorlage gehabt hat, ist unbekannt – aufgrund ihrer Nähe zur Prosarede bedarf es einer solchen jedoch nicht unbedingt. Die Langzeile zeichnet sich durch folgende, im Grundaufbau bereits beschriebene, Regeln aus:


Hinzu kommt eine unterschiedliche Gewichtung der Wortklassen bei der Verteilung der Stäbe. Da das Germanische eine ausgeprägte Nominalsprache ist, werden Nomina (Substantive, Adjektive, etc.) auch öfter betont und gegenüber den Verben bevorzugt mit Stäben versehen. Die meist unbetonten Formwörter (Pronomen, Hilfsverben, Konjunktionen, etc.) tragen nur in seltenen Ausnahmefällen den Stab. Die Reihenfolge Nomina→Verba→Formwörter ergibt sich also aus den natürlichen Tonverhältnissen der germanischen Sprachen. Die Langzeile passte sich immer den gerade gültigen Sprachverhältnissen an.

Das "Fornyrðislag" steht von allen nordischen Versmaßen der Langzeile am nächsten. Der Name selbst, am ehesten übersetzt als „Altredeton“, weist schon auf ein hohes Alter dieses Vermaßes hin. Es kommt fast nur in den Helden- und Götterliedern der "Edda" vor und unterscheidet sich von der Langzeile vor allem durch seine strophische Form.

<poem style="margin-left: 3em">
Ár var alda, þar er Ýmir bygði, 
vara sandr né sær, né svalar unnir, 
jörð fannsk æva, né upphiminn, 
gap var ginnunga, en gras hvergi.

Früh war’s der Zeiten, da Ymir lebte,
war nicht Sand noch See, noch kühle Wogen,
Erde gab es nicht, noch Obenhimmel,
der Schlund des Weltraums war, und Gras nirgends.

</poem>

Das Beispiel zeigt den Unterschied zwischen Satzgliederung und Langzeilengliederung im "Fornyrðislag". In den frühsten germanischen Langzeilen war eine Zeile meist auch ein vollständiger Satz (vgl. "Zweiter Merseburger Zauberspruch", Gallehus-Inschrift). In der epischen Langzeilendichtung (z. B. "Beowulf") geht der Satz meist über zwei Zeilen. Im "Fornyrðislag" sind Sätze über vier Zeilen keine Seltenheit. Oft geht man sogar noch darüber hinaus.

Überall wo in der "Edda" Spruch- und Merkdichtung vorkommt, z. B. im "Hávamál", finden wir das "Ljóðaháttr"-Versmaß. Übersetzt bedeutet "Ljóðaháttr" in etwa „Strophenvers“. Der wesentliche Unterschied zur Langzeile besteht in der strophischen Form, die jeweils eine Langzeile und eine Vollzeile, d. h. eine zäsurlose Zeile, die in sich stabt, kombiniert. Zwei oder mehr dieser Paare (Langzeile+Vollzeile) ergeben eine Strophe.

<poem style="margin-left: 3em">
Hjarðir þat vitu, nær þær heim skulu
ok ganga þá af grasi;
en ósviðr maðr, kann ævagi
síns of mál maga.

Herden wissen’s, wann sie heim müssen,
und gehen dann vom Gras;
aber der unkluge Mann, kennt niemals
seines Magens Maß.
</poem>

Wie auch das "Fornyrðislag" zeigt der "Ljóðaháttr" die typisch nordische Reduzierung der Gesamtsilbenanzahl, die die An- und Abverse teilweise bis zur Zweisilbigkeit zusammenschrumpfen lässt.

<poem style="margin-left: 3em">
Deyr fé, deyja frændr,
deyr sjalfr it sama,
en orðstírr, deyr aldregi
hveim er sér góðan getr.

Vieh stirbt, Verwandte sterben,
man selbst stirbt ebenso;
aber der Ruf stirbt niemals dem,
der sich guten erwirbt.
</poem>

Das Hauptversmaß der skaldischen Dichtung (mit einem Anteil von über 80 % an allen 20.000 Zeilen) ist das "Dróttkvætt" (der „Hofton“). Der Aufbau dieses Versmaßes ist verhältnismäßig kompliziert. Im Grunde besteht es aus zwei stabreimenden Langzeilen, die zusammen eine Strophe bilden. Das "Dróttkvætt" fügt jedoch einige strenge Regeln hinzu oder verschärft die schon bestehenden.

Im folgenden Beispiel aus der zweiten Strophe der "Lausavísur" des Skalden Sigvatr Þórðarson sind die Stabreime fett und die Binnenreime rot markiert.

<poem style="margin-left: 3em">
Hlýð mínum brag, / meiðir 
myrkblás, / þvít kank yrkja, 
alltíginn / – mátt eiga 
eitt skald – / drasils tjalda.

</poem>

Der Schrägstrich „/“ innerhalb der Halbverse markiert eine kleine Pause (nicht zu verwechseln mit der "Zäsur" die An- und Abverse trennt), die die Skalden einfügen, damit der Hörer die teilweise ineinander verschlungenen Inhalte heraushören kann. Wörtlich übersetzt klänge die Strophe nämlich so:

<poem style="margin-left: 3em">
Lausche meinem Gedicht, / vornehmer
des dunkelschwarzen, / denn ich kann dichten,
Vernichter / – (du) musst besitzen
einen Skalden – / Zeltpferdes
</poem>

Es gibt eine ganze Reihe weiterer Versmaße im skaldischen Gebrauch. Snorri zählt im "Háttatal" seiner "Prosa-Edda" verschiedene Typen auf und nennt Beispielstrophen. Erwähnenswert sind hier vor allem die Versmaße: "Kviðuháttr", "Tøglag", "Haðarlag", "Runhent", "Hrynhent" (alle skaldisch) sowie zwei weitere eddische Versmaße – "Málaháttr" und "Galdralag".
Einige dieser Versmaße erfüllen einen bestimmten Zweck. So ist das "Kviðuháttr" wohl für die genealogische Merkdichtung entwickelt worden (z. B. für die Auflistung von Königen eines bestimmten Geschlechts), während man das "Galdralag", mit seinen Wiederholungen, für Zaubersprüche verwendete (vgl. "Háttatal" 101 u. "Zweiter Merseburger Zauberspruch"). Die anderen Versmaße sind entweder komplizierte Varianten von "Dróttkvætt" ("Tøglag", "Haðarlag") oder "Fornyrðislag" ("Málaháttr") oder nähern sich dem christlichen Gebrauch an, durch Einbindung des Endreims ("Runhent") oder speziellen Rhythmus ("Hrynhent").




</doc>
<doc id="12160" url="https://de.wikipedia.org/wiki?curid=12160" title="Ungarische Sprache">
Ungarische Sprache

Die ungarische Sprache (Ungarisch, "") gehört zum ugrischen Zweig der finno-ugrischen Sprachen innerhalb der uralischen Sprachfamilie.

Ungarisch, das somit anders als die meisten europäischen Sprachen nicht zur indogermanischen Sprachfamilie gehört, ist im südmitteleuropäischen Raum verbreitet und wird von über 13,5 Millionen Menschen gesprochen. Andere Schätzungen gehen von bis zu 15 Millionen Menschen aus. Ungarisch ist Amtssprache in Ungarn und seit dem 1. Mai 2004 auch eine der Amtssprachen in der EU. Der Sprachcode "(Language Code)" ist codice_1 bzw. codice_2 (nach
ISO 639).

Die vergleichende Sprachwissenschaft ordnet das Ungarische zusammen mit dem Chantischen und dem Mansischen, den Sprachen zweier indigener Völker Westsibiriens mit jeweils wenigen tausend Sprechern, der ugrischen Untergruppe der finno-ugrischen Sprachen zu. Die finno-ugrischen Sprachen wiederum bilden zusammen mit der kleinen Gruppe der samojedischen Sprachen die uralische Sprachfamilie.

Die Verwandtschaft zwischen den verschiedenen dieser Familie angehörigen Sprachen lässt sich vielfach vor allem über die Sprachstruktur nachweisen, während der Wortschatz zuweilen nur noch wenige Ähnlichkeiten aufweist. So sind die Urformen des Finnischen und Ungarischen schon seit vielen Jahrtausenden getrennt, und die Verwandtschaft ist nicht näher als die Beziehung verschiedener indogermanischer Sprachen wie etwa Deutsch und Persisch.

Bis zu ihrer Landnahme an der Donau im 9. Jahrhundert lebten die Magyaren mehrere Jahrhunderte in intensivem Kulturkontakt mit den benachbarten turksprachigen Ethnien (Chasaren, Wolgabulgaren). Ein Einfluss auf die Sprachentwicklung erscheint daher möglich. Die Fremdbezeichnung „Ungar“ wird gelegentlich mit dem Namen einer hunno-bulgarischen Stammesföderation „Onogur“ mit der Bedeutung „zehn Pfeile“ in Verbindung gebracht. Während des Aufenthalts in der „Zwischenheimat“ in den Steppengebieten nördlich des Schwarzen Meeres " im 9. Jahrhundert können zudem Kultur- und Sprachkontakte mit den Krimgoten angenommen werden.

Erste Inschriften des Ungarischen sollen aus dem 9. Jahrhundert stammen, als sich die Magyaren noch der ungarischen Runenschrift bedienten. Die Datierung und Relevanz der ungarischen Runen ist allerdings umstritten. Mit der Christianisierung unter König Stephan I. kam das Lateinische als Quelle für zahlreiche Entlehnungen hinzu.

Als erstes Schriftdenkmal des Ungarischen gilt die Stiftungsurkunde der Benediktinerabtei von Tihany aus dem Jahre 1055. Das Schriftstück enthält in einem überwiegend lateinischen Text mehrere ungarische Wortverbindungen. Der früheste erhaltene Text in ungarischer Sprache ist die „Leichenrede“ " vom Ende des 12. Jahrhunderts.

Aus der Zeit der Herrschaft der Habsburger (1699–1867/1918) in Ungarn stammt der Einfluss der deutschen Sprache. Nach dem österreichisch-ungarischen Ausgleich von 1867 wurde in den Randgebieten (Slowakei, Kroatien, Siebenbürgen) eine Politik der intensiven Magyarisierung verfolgt, also der erzwungenen Durchsetzung des Ungarischen gegenüber den Regionalsprachen. Die Magyarisierung äußerte sich zahlenmäßig darin, dass der Anteil der magyarischen Bevölkerung im Königreich Ungarn nach offizieller Darstellung von etwa 29 % im Jahre 1780 auf 54 % im Jahre 1910 anstieg. Die aus der Magyarisierung resultierende Unzufriedenheit der nichtmagyarischen Bevölkerung des Königreichs Ungarn war 1918 eine der Hauptursachen für den Zerfall des Königreichs Ungarn.

Durch den Ersten Weltkrieg und die darauf folgenden Friedensverträge (Vertrag von Trianon) wurden etwa 3,2 Millionen Ungarn vom Mutterland getrennt; die Hälfte davon lebte in Grenzgebieten (vor allem in der Südslowakei), die andere Hälfte im Innern der Nachbarstaaten, besonders in Nordsiebenbürgen (Rumänien) und in der Vojvodina (Nordserbien). Dadurch gibt es heute noch viele (nur) Ungarischsprechende in den genannten Ländern.

Nach dem Ungarischen Volksaufstand 1956 wanderten viele Ungarn aus. Ihre Ziele waren vor allem Nord- und Südamerika, Australien, Österreich und die Schweiz.

Der große sprachliche Abstand zu den Idiomen der Nachbarvölker (Deutsch, Rumänisch, Slowakisch, Kroatisch, Serbisch, Ukrainisch) gehört zu den prägenden Momenten der ungarischen nationalen Identität. Ähnlich wie die Protobulgaren sind die Magyaren Nachfahren eurasischer Steppennomaden, die relativ spät nach Mitteleuropa eingewandert sind. Anders als die erstgenannten Völker haben sie jedoch ihre Sprache dauerhaft bewahrt.

Zusätzlich gibt es noch etwa eine Million weitere Sprecher auf allen Erdteilen verteilt, in Argentinien, Australien, Brasilien, Deutschland, Finnland, den Niederlanden, Italien, der Schweiz, Schweden, Tschechien und den USA existieren kleinere ungarischsprachige Gemeinden.

Ungarisch ist in der serbischen Region Vojvodina und den drei Gemeinden Hodoš, Dobrovnik und Lendava in der slowenischen Region Prekmurje neben den jeweiligen Staatssprachen Amtssprache. Außerdem ist die ungarische Sprache anerkannte Minderheitensprache in Österreich, Kroatien, Rumänien und der Slowakei.

Die ungarischen Dialekte zeigen im Allgemeinen weniger starke Abweichungen voneinander als beispielsweise die deutschen Dialekte. Die dialektalen Unterschiede liegen dabei hauptsächlich auf phonetischer Ebene. Die ungarischen -Dialekte, die vor allem noch im rumänischen Kreis Bacău verbreitet sind, bilden dabei eine Ausnahme. Durch die Isolation vom ungarischen Mutterland behielten die "Tschangos" ihren eigenständigen Dialekt bei, der sich durch rumänischen Einfluss stark veränderte. Der Dialekt der Szekler diesseits und der Tschango-Dialekt jenseits der rumänischen Karpaten werden gelegentlich auch zu den "Ostdialekten" zusammengefasst.

Folglich werden neun Dialektgruppen unterschieden:

"(in eckigen Klammern jeweils die Aussprache nach dem Internationalen Phonetischen Alphabet)"

Die Phonologie der ungarischen Sprache wird mit lateinischen Buchstaben umgesetzt. Dabei entsprechen alle Buchstaben jeweils genau einem Laut (im Gegensatz zum Deutschen, in dem es etwa für „e“ verschiedene Aussprachemöglichkeiten gibt).

Im Ungarischen gelten auch Digraphen sowie der Trigraph "dzs" als eigene Buchstaben, die mit mehreren Zeichen geschrieben werden. Somit ist die ungarische Rechtschreibung weitgehend regelmäßig. Die einzige Ausnahme bildet der j-Laut, der sowohl als „j“ als auch als „ly“ geschrieben wird. Historisch gesehen bezeichnete „ly“ den Laut , der inzwischen mit „j“ zu zusammengefallen ist. Von der vom Deutschen her gewohnten Aussprache weichen mehrere Buchstaben ab.

Alle Wörter werden stets auf der ersten Silbe betont, so lang sie auch sein mögen, vgl. ' [] „den Allerunbestechlichsten“ (13 Silben, die Form wird im Deutschen mit dem Dativ Plural wiedergegeben). Diese Regel gilt auch für Lehnwörter, vgl. ' = „Jackett“.

Zwischen kurzen und langen Vokalen wird genau unterschieden. Lange Vokale werden konsequent durch den Akut gekennzeichnet und nicht in der Schreibung verdoppelt. Die kurzen Vokale i, o, ö, u, ü werden stets geschlossen [] ausgesprochen. Phonologisch distinktiv ist also nur die Vokallänge. Sie dient dazu, Wörter verschiedener Bedeutung zu unterscheiden, vgl.:


Abweichend vom Deutschen werden die kurzen Vokale a und e ausgesprochen:

Die langen Vokale á und é unterscheiden sich somit deutlich von a und e. Dabei ist á stets [] ("nicht" []) und é stets [] ("nicht" []).

Lange Vokale können in allen Wortsilben vorkommen, vgl. "" „über seine Exzellenz“.


Die Buchstaben w und x werden nur in Namen oder Wörtern ausländischer Herkunft benutzt. Das y findet – abgesehen von den erwähnten Digraphen "gy", "ly", "ny" und "ty" – nur am Ende von Familiennamen Verwendung und wird als ausgesprochen. Ursprünglich handelt es sich um ein Adelszeichen, das mit dem deutschen „von“ vergleichbar ist, z. B. im Familiennamen "" (statt "Szalai").

In Namen deutschen oder slawischen Ursprungs wird das ch wie das ungarische h – ggf. wie ein deutsches ch ( bzw. ) – ausgesprochen (Lechner, Münnich). Im Wort "" ist es ein Ich-Laut.

Verdoppelte Konsonanten werden entsprechend länger ausgesprochen, vorangehende Vokale werden niemals verkürzt. Auch Digraphen können lang ausgesprochen werden, hier wird jedoch in der Schreibweise nur der erste Buchstabe verdoppelt: "ssz" = Doppel-sz, "lly" = Doppel-ly usw.

Im Ungarischen zählen – im Gegensatz zum Deutschen – auch die Buchstaben Ö, Ő, Ü und Ű sowie die Digraphen (cs, dz, gy, ly, ny, sz, ty, zs) und der Trigraph (dzs) als eigener Buchstabe. Man spricht bisweilen vom "großen" und "kleinen" ungarischen Alphabet, je nachdem, ob die nur in Fremdwörtern und historischen Schreibweisen (von z. B. Familiennamen) vorkommenden vier Buchstaben Q, W, X, Y hinzugezählt werden oder nicht. Im ersteren Fall hat das ungarische Alphabet somit 44, im zweiteren 40 Buchstaben.

Dies erscheint gegenüber den 26 Buchstaben des Deutschen viel, der Unterschied fällt jedoch weniger gravierend aus, wenn beim Vergleich berücksichtigt wird, dass im Deutschen Ä, Ö, Ü, ß sowie die Kombinationen ch, sch, tsch auch die Anzahl erhöhen würden, diese jedoch traditionell nicht als eigenständige Buchstaben gezählt werden.

In manchen ungarischen Namen hat sich eine alte Orthographie erhalten, bei der u. a. folgende Regeln gelten:

Ein Extrembeispiel ist der Name "Dessewffy", der wie "Dezsőfi" ausgesprochen wird.

Anders als in den flektierenden Sprachen erfolgt im Ungarischen die Bildung von Wortformen durch Agglutination. Darüber hinaus werden Verhältnisse des Besitzes, der Richtung, der Zeitlichkeit usw., die im Deutschen durch Possessivpronomina, Präpositionen oder Präpositionalphrasen gebildet werden, im Ungarischen ebenfalls durch Agglutination gebildet. Die Suffixe werden dabei in genau festgelegter Reihenfolge an die Wortstämme angehängt. Das Substantiv kann mit vielen Suffixen unterschiedlicher Funktion versehen werden.

Das Ungarische kennt 18 Kasus: Nominativ, Dativ, Akkusativ, Superessiv, Delativ, Sublativ, Inessiv, Elativ, Illativ, Adessiv, Ablativ, Allativ, Terminativ, Komitativ-Instrumental, Kausal-Final, Faktiv-Translativ, Essiv-Modal, Formal (so nach Béla Szent-Iványi: „Der ungarische Sprachbau“. Leipzig 1964, Hamburg 1995). Insgesamt gibt es im Ungarischen 27 Kasussuffixe, von denen 18 ohne Einschränkungen verwendet werden können. Sieht man von den Restriktionen im Gebrauch der übrigen Kasussuffixe ab, besitzt das Ungarische 27 Fälle. Wegen der Besonderheit der Wortbildung ist man sich aber unter den Sprachwissenschaftlern nicht einig, wie viele Fälle es insgesamt in der ungarischen Sprache gibt. Manche Sprachwissenschaftler gehen von nur fünf Fällen aus, andere zählen bis zu 40.

Von den Fällen haben lediglich drei – Nominativ, Dativ und Akkusativ – Entsprechungen im Deutschen. Unabhängig davon, ob die restlichen Konstrukte als „echte“ Kasus angesehen werden, lassen sie sich nur durch Präpositionalphrasen ins Deutsche übersetzen.

Allgemeine Erklärung der Menschenrechte, Artikel 1:

Deutsch: Alle Menschen sind frei und gleich an Würde und Rechten geboren. Sie sind mit Vernunft und Gewissen begabt und sollen einander im Geist der Brüderlichkeit begegnen.

Der Grundwortschatz hat einige hundert Wortwurzeln mit anderen uralischen Sprachen gemeinsam.

Als Beispiele seien die Zahlen von zwei bis vier genannt: ' (Finnisch ', Estnisch "kaks, kolm, neli", Mansi: китыг, хурум, нила / "kityg, churum, nila")

Außerdem die Wörter ' – Wasser, ' – Hand, ' – Blut, ' – Kopf (Finnisch und Estnisch "vesi, käsi, veri", Finnisch "pää", Estnisch "pea").

Darüber hinaus finden sich im ungarischen Wortschatz Entlehnungen aus mehreren Sprachen und Sprachgruppen:

Im Ungarischen herrscht die Tendenz vor, dass für Internationalismen eher ungarische Entsprechungen gebildet werden, meistens sind es Lehnübersetzungen, z. B. ' statt ' (international). Oft besteht neben den Internationalismen auch eine ungarische Entsprechung (' („Rechenmaschine“) und ', ' und ', ' und '). Es werden jedoch selbstverständlich auch Wörter aus anderen Sprachen direkt übernommen, die Rechtschreibung und die Aussprache passen sich aber dem Ungarischen an: ' (Byte), ' (Design), ' ("File", Datei) ' (Manager), ' (Schrapnell) ' (Sex), ' (Single), ' (trendig, modisch). Auch die Schreibweise der Vornamen fremder Herkunft wird „magyarisiert“: "Jennifer" und "Jessica" werden z. B. im Ungarischen ' und ' geschrieben.

Ungarische Bezeichnungen oder Ausdrücke fanden auch Einzug in viele andere europäische Sprachen. Die deutsche Sprache etwa übernahm Bezeichnungen wie "Dolmetscher", "Gulasch", "Kutsche" oder "Palatschinken" (vgl. Lehnwort). Zudem stammt das deutsche "Tollpatsch" vom ungarischen "talpas" (breitfüßig) ab.

Im Kroatischen wurden folgende Bezeichnungen entlehnt bzw. rück-entlehnt:

Eine auch heute, insbesondere in Kunstkreisen, sehr geläufige Entlehnung aus dem Ungarischen, die etwas an das Kroatische angepasst wurde, stellt die Bezeichnung "remek-djelo" dar, im Sinne von "Meisterwerk, Meisterstück" (von ung. "„remek“", "herrlich, prächtig" und kroat. "„djelo“", "Werk").

Im folgenden Beispielsatz befinden sich beispielsweise 13 Hungarismen. Kundige Ungarischsprechende könnten zumindest erraten, wovon die Rede ist:

Viele Bezeichnungen in diesen Sätzen könnten auch durch kroatische Bezeichnungen ersetzt werden. Insbesondere in Slawonien ist diese Sprache jedoch typisch. Viele Bezeichnungen, die für die heutige kroatische Sprache typisch sind, sind eigentlich Lehnübersetzungen ungarischer Bezeichnungen. Beispiele hierfür wären etwa: "povjerenstvo" ("bizottság", dt. Ausschuss, Kommission), "brzojav" ("sürgöny", dt. Telegramm), "prethodnica" ("elővéd", dt. Vorhut), "kolodvor" ("pályaudvar", dt. Bahnhof), "časnik" (von "tiszt" – dt. rein, dt. Offizier). Die Bezeichnung für „Eisenbahn“ wurde der ungarischen und gleichsam der deutschen Sprache entnommen: "željeznica (vasút)". Viele kroatische Ortschaften enthalten in ihrer Bezeichnung die ungarische Bezeichnung "vár" (dt. Burg), darunter Vukovar, Varaždin oder etwa Bjelovar.

Im Ungarischen wird zwischen der älteren und jüngeren Schwester "()" bzw. zwischen dem älteren und jüngeren Bruder "()" unterschieden.
Die (leiblichen) Eltern ( = Mutter, Vater) werden im Ungarischen unter Hinzufügung der Vorsilbe ' (wörtlich: „süß“) bezeichnet: ' = meine Mutter/mein Vater.

Auch für Vorfahren weiter zurückliegender Generationen gibt es eigene Vorsilben: ' = Groß~, ' = Urgroß~, ' = Ururgroß~, ' = Urururgroß~.

Die Verwandtschaftsbezeichnungen ' (Tante) und ' (Onkel) werden den Namen nachgestellt: '. Nicht nur Verwandte, auch Bekannte werden von Kindern so angesprochen. Im Kindergarten und in der Grundschule ist es ebenfalls üblich, dass die Kinder die Pädagogen mit ' oder ' ansprechen: ', '. Diese Form der Anrede hat sich während der Zeit der Habsburgermonarchie auch bis in die östlichen Dialekte Österreichs ausgebreitet: Die übliche Anrede für eine Tante Anna wäre im Wienerischen „Anna-Tant“. Eine Kurzform von ' ist ' in der Kombination mit dem Taufnamen, diese Form verwenden fast ausschließlich heranwachsende Jungen, wenn sie eine ihnen nahestehende männliche Bezugsperson – z. B. einen Fußballtrainer – anreden: '. Diese Anredeform wird zwar mit der Sie-Form verwendet, drückt aber ein familiäreres Verhältnis aus. In den höheren Schulen lautet die Anredeform „Familienname + '“: ' oder einfach "".

Da die Familiennamen meist aus Adjektiven entstanden sind, wird bei der Namensgebung zuerst der Familienname und erst dann der Vorname (' oder ') genannt (z. B. Bátori Gábor, zu Deutsch Gabriel von Bator oder der Bator’sche Gabriel). Diese Praxis wird jedoch nur auf ungarische Namen angewandt, ausländische Namen werden meist in der im Herkunftsland üblichen Reihenfolge genannt.

Dass eine Frau verheiratet ist, wird oft durch Anfügen der Endung ' an den Namen des Ehemannes angedeutet: ' (die Frau des József Kovács). Die Kurzform (eine offiziell klingende Anredeform) lautet ' (Frau Kovács). Während bis zu den 1990er Jahren diese Namensgebung sehr verbreitet war – es war lange Zeit die einzige Möglichkeit –, ist eine Tendenz zu beobachten, dass Frauen nach der Eheschließung entweder den Geburtsnamen behalten oder andere Formen bevorzugen (wenn z. B. Anna Kiss József Kovács heiratet, sind folgende Formen möglich: ').
Männer redet man mit ' an: '. In den Jahren des Kommunismus war noch die Anredeform ' und ' (Genosse und Genossin) üblich: "".

Die Begrüßungen und Anredeformen weisen in der ungarischen Sprache eine besondere Vielfalt auf. Einige Begrüßungsformen sind noch Relikte aus der Zeit der österreichisch-ungarischen Monarchie, z. B. die Form ' oder ' (Küss [die Hand]). Während diese Begrüßung ursprünglich Respekt gegenüber (oft älteren) Damen ausdrückte, ist die Form "csókolom" vor allem bei Kindern verbreitet, die ihre erwachsenen Bekannten so begrüßen. " ist wiederum die übliche Anrede von Männern gegenüber fremden Frauen, während erwachsene Frauen diesen Ausdruck praktisch nie verwenden.
Die offizielle Begrüßung " (Guten Morgen/Tag/Abend [wünsche ich]) klingt oft distanziert, vor allem in der Vollform.
Jugendliche und Freunde begrüßen einander mit ', ' oder ' (kann sowohl beim Treffen als auch beim Abschied gesagt werden). Die offizielle Formel für den Abschied ist ' (Auf Wiedersehen) oder am Telefon/im Rundfunk ' (Auf Wiederhören). Sie werden auch oft in den Kurzformen ', oder " verwendet.

Im Ungarischen gibt es mehrere Höflichkeitsformen. Für das deutsche „Sie“ gibt es zwei nicht ganz gleichwertige Entsprechungen: ' (Pl. ') und ' (Pl. '), wobei das ' besonders im offiziellen Leben (in den Medien, in der Politik, in den Geschäften – besonders in den größeren Städten) verwendet wird, das ' ist umgangssprachlicher und familiärer. Nach ' und ' wird die 3. Person Singular verwendet, nach ' die 3. Person Plural. Das Personalpronomen wird aber oft weggelassen, man kann einen auch nur mit der 3. Person (Singular oder Plural, je nachdem, wie viele Personen angesprochen werden) ansprechen. ' klingt offizieller, " vertraulicher.

Kinder verwenden älteren Personen gegenüber die umständliche '-Form mit Infinitiv, das sind ungefähr die Personen, die sie mit ' begrüßen: "?" (Möchten Sie sich setzen?). "" (wörtlich „gefallen“) kann man auch in der Mehrzahl sowie in allen Tempora und Modi verwenden: "?" (Wollten Sie sich setzen? bzw. Hätten Sie sich setzen wollen?).

In Ungarn gibt es die Tendenz, dass das Duzen immer mehr in den Vordergrund rückt, oft wird man in den Geschäften geduzt, besonders junge Leute von gleichaltrigem Personal. Mittlerweile duzen immer mehr Kinder ihre Erzieher und Lehrer. Bis zu den 1960er Jahren war es vor allem auf dem Lande noch üblich, dass man die Eltern und Großeltern siezte. Um die Jahrhundertwende (19./20. Jahrhundert) siezten sich sogar Eheleute. Auch heute hört man noch oft, dass die Schwiegereltern gesiezt werden. Oft werden (ältere) Eltern oder Schwiegereltern zusätzlich zur Sie-Form mit ' und ' (Mütterchen und Väterchen) angesprochen.

Das Ungarische ist reich an Schimpfwörtern, die teilweise sehr derb sind und mittlerweile unabhängig von Alter und Geschlecht verwendet werden.

Einige Schimpfwörter (Verben in Imperativform, die z. B. den Geschlechtsverkehr beschreiben: „baszd meg“ wie im Englischen entsprechend „fuck“ gebraucht wird) werden als Füllwörter oder zum nachdrücklichen Unterstreichen der Aussage verwendet. Es gibt auch derbe Substantive (z. B. ein Ausdruck für Prostituierte "()", der ein slawisches Lehnwort ist), die zur Steigerung von Adjektiven verwendet werden. Oft ist die Mutter des Beschimpften Bestandteil des Ausdrucks (' – „deine Mutter“; das Verb, das solches Schimpfen bezeichnet, ist ') – ähnliche Phrasen findet man auch in den slawischen Sprachen. Als besonders beleidigende Beschimpfung gilt es, wenn man den anderen – wortwörtlich übersetzt – zurück in seine Mutter schicken will (""). Auch diverse Synonyme für Homosexuelle oder Bezeichnungen eines Geschlechtsteils werden einfach als Schimpfwörter verwendet.

Es gibt aber Euphemismen, die statt Schimpfwörtern verwendet werden können und nicht derb sind, aber ähnlich wie die Schimpfwörter klingen: z. B. ' und ' (wörtlich „Bassschlüssel“) für die Ausdrücke mit dem verbreitetsten Verb für Geschlechtsverkehr, das mit einer ähnlichen Silbe anfängt.

Generell gilt, dass Beschimpfungen nicht wörtlich verstanden oder übersetzt werden dürfen. Einige höchst derb klingende Beleidigungen entsprächen bei sinngemäßer Übersetzung dem deutschen „Du spinnst“. Zudem fließen oftmals Beschimpfungen in Gespräche insbesondere zwischen befreundeten Männern ein, ohne dass sie als Beleidigung empfunden werden.

Der Sprachwissenschaftler Jacob Grimm hat das Studium des Ungarischen allen empfohlen, die neue einfach zu erlernende Plansprachen schaffen wollen.

George Bernard Shaw sagte bei einem Interview mit dem US-Sender CBS: 

Der Wiener Sprachforscher N. Ebersberg sagte über die ungarische Sprache im 19. Jh. 

Nach Ove Berglund, schwedischer Arzt und Übersetzer: 






</doc>
<doc id="12161" url="https://de.wikipedia.org/wiki?curid=12161" title="Liste der Friedensnobelpreisträger">
Liste der Friedensnobelpreisträger

Der Friedensnobelpreis ist eine Auszeichnung für besondere Verdienste in der Friedensarbeit. Er wurde vom schwedischen Erfinder und Industriellen Alfred Nobel gestiftet. Diese Auszeichnung wird seit 1901 jedes Jahr am Todestag Alfred Nobels, dem 10. Dezember, in Oslo verliehen.

Der Friedensnobelpreis wurde bis 2016 insgesamt 130-mal verliehen, davon gingen 88 Auszeichnungen (67,7 Prozent) an Männer, 16 (12,3 Prozent) an Frauen und 26 (20 Prozent) an Organisationen. Dabei war das Internationale Komitee vom Roten Kreuz (IKRK) bereits dreimal (1917, 1944 und 1963) und das Büro des Hohen UN-Kommissars für Flüchtlinge zweimal (1954 und 1981) Preisträger. Damit haben 23 Organisationen mindestens einen Friedensnobelpreis erhalten. Bisher wurde noch kein Mensch wiederholt ausgezeichnet.

Von den Friedensnobelpreisen wurden insgesamt 67 ungeteilt vergeben, davon 51-mal an eine Person und 15-mal an eine Organisation. Insgesamt 29-mal wurde der Friedensnobelpreis auf mehrere Preisträger aufgeteilt, davon 21-mal auf zwei Personen, zweimal auf zwei Organisationen und sechsmal auf eine Person und eine Organisation. Erst zweimal (1994 und 2011) wurde der Preis auf drei Personen aufgeteilt.

Diese Liste bietet eine chronologische Übersicht der Träger des Friedensnobelpreises. Eine alphabetische Übersicht bietet die .

Bis 1990 wurde beim Friedensnobelpreis im Gegensatz zu den anderen Nobelpreisen keine offizielle Preisbegründung genannt. Daher ist bei diesen Jahren bei der Begründung die Tätigkeit vermerkt, die anzunehmenderweise ursächlich für die Vergabe ist.



Die Daten zu den Preisträgern entsprechen den Angaben auf den offiziellen Internetpräsenzen der Nobelpreisstiftung.


</doc>

<doc id="11020" url="https://de.wikipedia.org/wiki?curid=11020" title="Bürgermeister">
Bürgermeister

Ein Bürgermeister – in der Schweiz meist Stadt- oder Gemeindepräsident – leitet die Verwaltung einer Gemeinde oder Stadt. Er wird je nach Staat direkt von den Bürgern oder indirekt vom Stadtrat oder vom Gemeinderat (Deutschland) gewählt. 

In größeren Städten Deutschlands gibt es "mehrere" Bürgermeister (z. B. Baubürgermeister, Sozialbürgermeister), die einem Oberbürgermeister beigeordnet und meist für spezielle Aufgabengebiete verantwortlich sind.

In den meisten größeren, vor allem in kreisfreien Städten gibt es in Deutschland einen Oberbürgermeister sowie einen oder mehrere Beigeordnete, die dann gelegentlich die Amtsbezeichnung "Bürgermeister" haben.

Man unterscheidet üblicherweise bei dem Begriff "Bürgermeister" bzw. "Oberbürgermeister" zwischen dem Amtsinhaber als Person (dem sog. Organwalter) und dem "Bürgermeister" als Organ im Sinne einer rechtlich geschaffenen Einrichtung eines Verwaltungsträgers. Als Organ ist der "Bürgermeister" bzw. "Oberbürgermeister" institutionell Behörde der Gemeinde. In seiner Funktion als Behörde nimmt der Bürgermeister Aufgaben der öffentlichen Verwaltung wahr und ist insoweit Teil der Exekutive (vgl. u. a. Abs. 4 VwVfG).

Der Begriff Erster Bürgermeister wird in Hamburg für den Regierungschef benutzt und in großen Kreisstädten und kreisfreien Städten in Baden-Württemberg für den Stellvertreter des Oberbürgermeisters.

Besonderheiten, wie in den Stadtstaaten und Hansestädten, siehe jeweilige Länder weiter unten.

Seit dem 13. Jahrhundert standen Bürgermeister an der Spitze des Stadtrats, dem Organ der Bürgerschaft zur Selbstverwaltung. Im Mittelalter war neben der mhd. Amtsbezeichnung "burge(r)meister" das noch ältere lat. "magister civium" in allgemeinem Gebrauch. Meist waren zwei Bürgermeister vorhanden, oft aber auch mehrere. Einer hatte den Vorsitz im Stadtrat, und alle vollzogen ursprünglich nur dessen Beschlüsse. Allmählich wuchs ihnen die Aufgabe der gesamten Selbstverwaltung zu. Sie erhielten die Polizeigewalt und oft auch die Gerichtsbarkeit in Bagatellsachen (vgl. Bezeichnung „Marktrichter“ in der ungarischen Verwaltung in der K&K Monarchie). Die ursprüngliche Unterordnung unter einen herrschaftlichen Vogt oder Schultheiß wich in der Regel bald einem Nebeneinander. Die Bürgermeister wurden vom Stadtherrn ernannt oder vom Stadtrat gewählt, aus dem Kreis der Patrizier oder aus den Zünften. Im 17. und 18. Jahrhundert wurde die Wahl nach und nach zur Formsache, die Bürgermeister waren nunmehr vom Stadtherrn ernannte Beamte (die Reichsstädte bildeten hier jedoch eine Ausnahme). Im Laufe des 19. Jahrhunderts wurden die Bürgermeister als Gemeindevorsteher wieder gewählt.

Auch die Dorfgemeinde hatte Verwaltungsfunktionen und übte die niedere Gerichtsbarkeit aus. Die Bürgermeister (Dorfmeister, Bauermeister) waren in den meisten Fällen zunächst Gemeindeschreiber und -rechner und dem Schultheiß oder dem Heimberger untergeordnet. Im Verlauf der frühen Neuzeit setzte sich der Bürgermeister in vielen Gemeinden als der wichtigste Amtsträger durch. Im Zug dieser Entwicklung erlosch das Schultheiß- oder Heimbergeramt meist vollkommen. – Siehe auch: Bürgermeister (historisch).

Der (hauptamtliche) Bürgermeister hat entsprechend der jeweiligen Gemeindeordnung unterschiedliche Aufgaben:


Die Bürgermeister sind je nach Bundesland, nach Größe der Gebietskörperschaft und ggf. nach Funktion (z. B. 2. Bürgermeister/ehrenamtlicher Bürgermeister) in Deutschland in unterschiedlichen Besoldungsgruppen, die Rechtsgrundlagen sind verschieden benannt (in NRW: Eingruppierungsverordnung). Der Oberbürgermeister von München ist in Besoldungsgruppe B 11 eingruppiert.

Der Bürgermeister in Baden-Württemberg ist Vorsitzender des Gemeinderates und Leiter der Gemeindeverwaltung. Er vertritt die Gemeinde nach außen. Die Amtszeit des Bürgermeisters beträgt acht Jahre. Das baden-württembergische Kommunalrecht folgt dem so genannten Modell der Süddeutschen Ratsverfassung.

In Bayern wird der "Erste Bürgermeister" (seit 1908 auch "Oberbürgermeister") von den Bürgern einer Gemeinde direkt gewählt. Die Amtszeit beträgt sechs Jahre. Zur Wahl ist eine absolute Mehrheit der gültigen Stimmen notwendig. Erzielt keiner der Bürgermeisterkandidaten diese im ersten Wahlgang, kommt es zu einer Stichwahl der beiden Bewerber mit den meisten Stimmen. Die Rechtsgrundlage hierfür findet sich im Bayerischen Gemeinde- und Landkreiswahlgesetz (GLKrWG).

Der Erste Bürgermeister vertritt die Gemeinde nach außen, führt den Vorsitz im Gemeinde-, Marktgemeinde- bzw. Stadtrat und vollzieht seine Beschlüsse. Er hat im Gemeinde-/Stadtrat volles Stimmrecht. In kreisfreien Gemeinden und in Großen Kreisstädten führt er die Bezeichnung Oberbürgermeister. In diesen Gemeinden und in kreisangehörigen Gemeinden mit mehr als 5000 Einwohnern ist er in der Regel Beamter auf Zeit.

In kreisangehörigen Gemeinden, die mehr als 5000, höchstens aber 10000 Einwohner haben, ist der erste Bürgermeister Ehrenbeamter (ehrenamtlicher Bürgermeister), wenn das der Gemeinderat spätestens am 90. Tag vor einer Bürgermeisterwahl durch Satzung bestimmt. In Gemeinden bis zu 5000 Einwohnern ist der erste Bürgermeister Ehrenbeamter, wenn nicht der Gemeinderat spätestens am 90. Tag vor einer Bürgermeisterwahl durch Satzung bestimmt, dass der erste Bürgermeister Beamter auf Zeit sein soll. (Art. 34 BayGO).

Der "Zweite Bürgermeister" (und eventuell auch ein "Dritter Bürgermeister", ehrenamtlich oder berufsmäßig) wird vom Stadtrat oder dem Gemeinderat aus seinen Mitgliedern gewählt. Die Bezeichnung in der Bayerischen Gemeindeordnung ist „ein oder zwei weitere Bürgermeister“ (Artikel 35).

Die gesetzlichen Grundlagen für die Rechtsstellung der Bürgermeister finden sich in der Bayerischen Gemeindeordnung – BayGO – und in dem bayerischen Gesetz über kommunale Wahlbeamte – KWBG.

In Brandenburg wird der Bürgermeister seit 1993 direkt gewählt. In den kreisfreien Städten (Brandenburg an der Havel, Cottbus, Frankfurt (Oder) und Potsdam) trägt er die Bezeichnung Oberbürgermeister. In amtsangehörigen Gemeinden ist der Bürgermeister ehrenamtlich tätig, in amtsfreien Gemeinden ist er hauptamtlicher Beamter auf Zeit. Der ehrenamtliche Bürgermeister wird zugleich mit der Gemeindevertretung auf fünf Jahre gewählt. Der hauptamtliche Bürgermeister oder Oberbürgermeister wird als hauptamtlicher Beamter auf Zeit auf die Dauer von acht Jahren gewählt. Zur Wahl ist eine absolute Mehrheit der gültigen Stimmen notwendig, erzielt keiner der Kandidaten diese im ersten Wahlgang, kommt es zu einer Stichwahl der beiden Bewerber mit den meisten Stimmen. Diese Mehrheit muss jeweils mindestens 15 vom Hundert der wahlberechtigten Personen umfassen. Erhält kein Bewerber diese Mehrheit, so wählt in diesem Fall die Vertretung den Bürgermeister oder Oberbürgermeister.

In den Ortsteilen von Städten und Gemeinden wird der Ortsvorsteher (vormals Ortsbürgermeister) durch den Ortsbeirat, aus dessen Mitte heraus, gewählt.

In Hessen sind die Kommunen nach der sogenannten Magistratsverfassung organisiert, die dem Bürgermeister auch nach der Einführung der Direktwahl im Jahr 1992 eine relativ schwache Stellung gegenüber der Gemeindevertretung gibt. Rechtsgrundlage ist die Hessische Gemeindeordnung (HGO).

In Mecklenburg-Vorpommern wird der Bürgermeister seit 1999 direkt gewählt. In größeren Städten gibt es einen Oberbürgermeister. Die Amtszeit beträgt nach dem Kommunalwahlrecht in hauptamtlich verwalteten Gemeinden mindestens sieben und höchstens neun Jahre, Näheres regelt die Hauptsatzung. Die Wahl findet unabhängig von der Wahl des Gemeinderats statt. In ehrenamtlich verwalteten Gemeinden ist die Amtszeit des Bürgermeisters an die Wahlperiode der Gemeindevertretung gebunden, sie dauert also fünf Jahre. Der direkt gewählte Bürgermeister kann nur durch Bürgerentscheid abberufen werden.

In Niedersachsen ist der Bürgermeister bzw. Samtgemeindebürgermeister stets hauptamtlich tätig. In kreisfreien Städten, großen selbständigen Städten und den Städten Hannover und Göttingen trägt er die Bezeichnung Oberbürgermeister. Der Bürgermeister wird in repräsentativen Angelegenheiten durch sogenannte "ehrenamtliche Bürgermeister" unterstützt. Die Bezeichnung Beigeordneter ist für sie nicht gebräuchlich. Als Beigeordnete bezeichnet man die Mitglieder des Verwaltungsausschusses, wobei die ehrenamtlichen Bürgermeister aus deren Reihen durch den Rat gewählt werden. Der (hauptamtliche) Bürgermeister wird in Niedersachsen unmittelbar durch die Einwohner der Gemeinde/Stadt gewählt. Er ist nicht kraft seines Amtes Vorsitzender des Rates und kann ab Inkrafttreten des Niedersächsischen Kommunalverfassungsgesetzes am 1. November 2011 nach § 61 NKomVG auch nicht mehr dazu gewählt werden. Seine Amtszeit beträgt 8 Jahre, sie ist damit 3 Jahre länger als die der Mitglieder des Rates. Entsprechend müssen Wahlen zum Rat und für das Amt des Bürgermeisters nicht gleichzeitig erfolgen. Ab 2016 wird die Amtszeit der Bürgermeister schrittweise auf 5 Jahre reduziert und die Wahltermine synchronisiert, so dass die Bürgermeisterwahlen spätestens ab 2021 zusammen mit den Kommunalwahlen stattfinden. In Niedersachsen war auch der bis dato dienstälteste Bürgermeister Deutschlands, Heinrich Meyer-Hüdig, von 1946 bis 2001 in der Ortschaft Ehestorf, Gemeinde Rosengarten, tätig.

In Nordrhein-Westfalen leitet der Bürgermeister die Verwaltung und ist kommunaler Wahlbeamter auf Zeit.

Bis 1994 bestand eine Aufteilung in den Chef der Verwaltung und Vertreter der Kommune in allen Rechts- und Verwaltungsangelegenheiten (Oberstadt-, Stadt- bzw. Gemeindedirektor) und den ehrenamtlich tätigen Bürgermeister als Vorsitzenden des Rates. Dieses System war nach dem Zweiten Weltkrieg 1945 von der britischen Besatzungsmacht eingeführt worden. Es wurde auch als "kommunale Doppelspitze" bezeichnet. Nach Abschaffung der Doppelspitze wurden die Bürgermeister zunächst vom Rat gewählt.

Seit der Kommunalwahl 1999 erfolgte die Direktwahl der hauptamtlichen Bürgermeister in Städten und Gemeinden durch die Bürger für eine Amtszeit von sechs Jahren. Durch das "Gesetz zur Stärkung der kommunalen Demokratie" vom 9. April 2013 wurde die Amtszeit von sechs auf fünf Jahre verkürzt. Ab 2020 sind die Bürgermeisterwahlen mit den Wahlen der Stadt- und Gemeinderäte verbunden.

Die Gemeinden in Rheinland-Pfalz werden, soweit ihnen nicht die Stadtrechte verliehen sind und sie einer Verbandsgemeinde angehören, Ortsgemeinde genannt. Das Oberhaupt einer Ortsgemeinde wird als Ortsbürgermeister bezeichnet, welcher ehrenamtlich tätig ist. Ist die Gemeinde eine Stadt, so führt das Stadtoberhaupt die Amtsbezeichnung Stadtbürgermeister. Einer Verbandsgemeinde gehören mehrere Ortsgemeinden oder Städte an, der Leiter der Verbandsgemeindeverwaltung führt die Amtsbezeichnung Bürgermeister und ist hauptamtlich tätig. Verbandsfreie, aber kreisangehörige Gemeinden sind in der Regel Städte. Das Oberhaupt einer solchen Gemeinde oder Stadt heißt unabhängig vom Typus der Gemeinde Bürgermeister (nicht etwa Stadtbürgermeister) und ist ebenfalls hauptamtlich tätig. Der Bürgermeister einer kreisfreien Stadt oder einer großen kreisangehörigen Stadt wird als Oberbürgermeister tituliert, die ihm zugeordneten Beigeordneten tragen die Amtsbezeichnung "Bürgermeister". Die Amtszeit der hauptamtlichen Bürgermeister dauert acht Jahre, die der ehrenamtlichen Bürgermeister entspricht der Wahlzeit des Gemeinderats (zurzeit fünf Jahre).

Im Saarland ist die Wahl des Bürgermeisters im Kommunalselbstverwaltungsgesetz (§§ 54 ff. KSVG) geregelt. Er wird direkt von den Wahlberechtigten für die Dauer von zehn Jahren (§ 31 Abs. 2 KSVG) gewählt. Wer von den Bewerbern für das Bürgermeisteramt im ersten Wahlgang mehr als die Hälfte der abgegebenen und gültigen Stimmen erhält, ist Bürgermeister der Gemeinde. Sofern keiner der Kandidaten die erforderliche Mehrheit auf sich vereinigen kann, treten die beiden Kandidaten, die im ersten Wahlgang die meisten Stimmen auf sich vereinigen konnten, im entscheidenden zweiten Wahlgang gegeneinander an (§ 56 i.V.m. § 46 KSVG).

Die Abwahl des Bürgermeisters muss vom Gemeinderat eingeleitet werden. Hierzu ist eine erste Abstimmung notwendig, in der ein entsprechender Antrag von mindestens der Hälfte der Mitglieder unterstützt wird (§ 58 KSVG). Sofern mehrheitlich für den Antrag votiert wurde, kann frühestens nach zwei Wochen ein Beschluss über den Antrag gefasst werden. Hier ist eine Zweidrittelmehrheit bei namentlicher Abstimmung notwendig. Nach erfolgreicher Beschlussfassung des Gemeinderats bedarf es einer Abwahl durch die Wahlberechtigten. Um den Bürgermeister aus dem Amt zu entfernen bedarf es einer einfachen Mehrheit am Wahltag, wobei mindestens 30 % der Wahlberechtigten für eine Abwahl votieren müssen. Die Abwahl eines Bürgermeisters vor Ende der Amtszeit wurde im Saarland erst einmal erfolgreich durchgeführt. Wolfgang Stengel, Bürgermeister der Gemeinde Schiffweiler, wurde am 29. März 2010 durch die Bekanntgabe des Wahlergebnisses offiziell als Bürgermeister abgewählt.

In Sachsen wird seit 1994 der Bürgermeister alle sieben Jahre direkt gewählt. In Kreisfreien Städten und Großen Kreisstädten führt der Bürgermeister die Bezeichnung Oberbürgermeister. In Gemeinden mit mehr als 5000 Einwohnern ist der Bürgermeister hauptamtlicher Beamter auf Zeit. In kleineren Gemeinden ist der Bürgermeister ehrenamtlich tätig, in Städten und Gemeinden mit mehr als 2000 Einwohnern (sofern diese weder einem Verwaltungsverband oder einer Verwaltungsgemeinschaft angehören) kann aber in der Hauptsatzung festgelegt werden, dass der Bürgermeister hauptamtlich tätig ist.
Unabhängig von der Größe der Gemeinde ist der Bürgermeister hauptamtlicher Beamter auf Zeit, wenn die Gemeinde erfüllende Gemeinde einer Verwaltungsgemeinschaft ist.
Größere Kommunen können, Kreisfreie Städte müssen einen oder mehrere Beigeordnete als Stellvertreter des Bürgermeisters haben. Die Beigeordneten werden vom Gemeinderat für eine Dauer von sieben Jahren gewählt, sie sind Wahlbeamte auf Zeit. In kleineren Kommunen werden meist zwei stellvertretende Bürgermeister vom Gemeinderat gewählt, die ehrenamtlich arbeiten.

In Sachsen-Anhalt wird der Bürgermeister seit 1994 direkt gewählt.
In Schleswig-Holstein werden die hauptamtlichen Bürgermeister direkt vom Volk für eine Amtszeit von sechs bis acht Jahren gewählt; die genaue Amtszeit legt die Hauptordnung der Gemeinde fest. Ehrenamtliche Bürgermeister, die es in der Regel in amtsangehörigen Gemeinden gibt, werden durch die Gemeindevertretung gewählt.

In Lübeck wird der „Bürgermeister“ aus historischen Gründen so bezeichnet und ist von seinem Rang einem Oberbürgermeister vergleichbar, den es in anderen schleswig-holsteinischen Städten vergleichbarer und kleinerer Größe gibt.

In Thüringen wird der Bürgermeister seit 1994 direkt für eine regelmäßige Amtszeit von sechs Jahren gewählt. In kreisfreien und Großen kreisangehörigen Städten führt er die Amtsbezeichnung „Oberbürgermeister“.

In den Stadtstaaten haben die Bürgermeister die Funktion, die einem Ministerpräsidenten in den anderen Ländern vergleichbar ist. Sie sind Landes- und Stadtoberhaupt zugleich. Auch ihre Stellvertreter tragen den Titel Bürgermeister. Während in den Freien Hansestädten Bremen und Hamburg traditionell der Titel Bürgermeister statt Oberbürgermeister für das Staatsoberhaupt verwendet wird, entstand der Begriff Regierender Bürgermeister zunächst für das Landesoberhaupt von West-Berlin, nachdem 1948 ein Oberbürgermeister für Ost-Berlin eingesetzt worden war. 1991 erfolgte die Wahl eines Regierenden Bürgermeisters dann für Gesamt-Berlin.

In Berlin ist der "Regierende Bürgermeister" Landes- und Stadtoberhaupt zugleich. Er wird vom Abgeordnetenhaus von Berlin gewählt. Er bildet mit den von ihm ernannten Senatoren (Ministern), denen je eine Senatsverwaltung (Ministerium) untersteht, den Senat von Berlin.

Unter dem Titel "Bürgermeister" ernennt er zwei Senatoren zu seinen Stellvertretern.

Die Verwaltungsvorsteher eines Bezirksamtes in einem der zwölf Bezirke Berlins trägt die Bezeichnung Bezirksbürgermeister.

In der Freien Hansestadt Bremen ist der Bürgermeister und Präsident des Senats Landesoberhaupt und zugleich Oberhaupt der Stadt Bremen (nicht jedoch Bremerhavens). Der Präsident des Senats wird vom Landesparlament, der Bremischen Bürgerschaft gewählt, die anschließend den weiteren Senat der Freien Hansestadt Bremen als Landesregierung wählt. Der Präsident des Senats und ein weiterer vom Senat aus den eigenen Reihen zu wählender Senator, als sein Stellvertreter, sind Bürgermeister (beide werden aus Tradition so offiziell bezeichnet).
Beide Bürgermeister sind zugleich auch Senatoren (Minister). Ihnen unterstehen wie den übrigen Senatoren verschiedene senatorische Behörden (Ministerium).

In der Seestadt Bremerhaven wird der Magistrat (Stadtrat), bestehend aus dem Oberbürgermeister, dem Bürgermeister (Stellvertreter) und den Magistratsmitgliedern, auf der Grundlage einer kommunalen Verfassung von der Stadtverordnetenversammlung gewählt.

In der Freien und Hansestadt Hamburg ist der "Erste Bürgermeister" Landes- und Stadtoberhaupt zugleich. Er wird vom Landesparlament, der Hamburgischen Bürgerschaft gewählt. Er ernennt seinen Stellvertreter den "Zweiten Bürgermeister" und die übrigen Senatoren (Minister), die von der Bürgerschaft zu bestätigen sind. Diese bilden die Landesregierung, den Senat der Freien und Hansestadt Hamburg. Der Erste Bürgermeister ist "Präsident des Senats". Der Zweite Bürgermeister ist zugleich Senator. Wie den übrigen Senatoren untersteht ihm eine Senatsbehörde (Ministerium).

Die Bezirksamtsleiter als Verwaltungsleiter eines der sieben Bezirksämter der Bezirke in Hamburg werden umgangssprachlich gelegentlich als Bezirksbürgermeister tituliert. Insbesondere in den ehemaligen selbständigen Städten, die nach Eingemeindung zu Hamburg 1938 ihre eigenen Bürgermeister verloren, ist dies manchmal der Fall.

Hauptartikel|Kommunalwahlrecht (Österreich)
Der Bürgermeister wird in den meisten Bundesländern direkt (vom Volk) gewählt, in Niederösterreich, der Steiermark und Wien jedoch von den Mitgliedern des Gemeinderates. In Wien ist der Bürgermeister auch Landeshauptmann, die Mitglieder des Gemeinderats sind zugleich Abgeordnete des Landtags. Wenn der Bürgermeister nicht direkt gewählt wird, stellt meistens die Mehrheitspartei den Bürgermeister. Dies ist aber von den jeweiligen Mehrheitsverhältnissen im Gemeinderat abhängig.

Der Bürgermeister ist das geschäftsführende Organ der Gemeinde und sorgt insbesondere für die Ausführung der Beschlüsse des Gemeinderates. Er besorgt die Angelegenheiten des übertragenen Wirkungsbereichs der Gemeinde im Rahmen der Weisungen von Bund und Ländern. Die Gemeindebediensteten sind ihm unterstellt. Er vertritt eine Gemeinde auch nach außen. In Krems und Waidhofen an der Ybbs, dies sind jene Statutarstädte, in denen die Landespolizeidirektion nicht Sicherheitsbehörde I. Instanz ist, ist der Bürgermeister als Bezirksverwaltungsbehörde Sicherheitsbehörde I. Instanz. Der Bürgermeister ist in allen Gemeinden Fundbehörde sowie Meldebehörde. In Gemeinden, die zum Wirkungsbereich einer Landespolizeidirektion gehören, ist der Bürgermeister auch Passbehörde.

In jeder Gemeinde gibt es als Vertretung einen, zwei oder drei Vizebürgermeister, je nach Wahlergebnis und Gemeindegröße. In manchen Bundesländern ist vorgesehen, dass für einzelne Ortsteile größerer Gemeinden „Ortsvorsteher“ als Vertreter des Bürgermeisters bestellt werden können. In Wien werden in den 23 Wiener Gemeindebezirken als Bezirksvertretung bezeichnete Bezirksparlamente gewählt, die wiederum jeweils einen Bezirksvorsteher wählen. In Graz werden in den 17 Stadtbezirken als Bezirksräte bezeichnete Bezirksparlamente gewählt, die wiederum jeweils einen Bezirksvorsteher und dessen Stellvertreter wählen.

Nachdem im August 2016 Christian Jachs, der amtierende Bürgermeister von Freistadt verstarb, musste eine Direktwahl (durch das Volk) – konkret am 4. Dezember 2016 – angesetzt werden, da seit der Wahl, den Gemeinde- und Bürgermeisterwahlen in Oberösterreich im Herbst 2015 noch keine 3/4 der Legislaturperiode, also 4 von 6 Jahren, vergangen waren. Neben der Bundespräsidentenwahl darf eigentlich keine weitere Wahl am selben Tag stattfinden. Für diesen Fall wurde vom Nationalrat eine Ausnahme beschlossen, um den 4. Wahlgang für den BP zugleich mit der Bürgermeisterwahl in Freistadt am 4. Dezember 2016 durchführen zu können.

In der Schweiz gibt es die Bezeichnung "Bürgermeister" als Vorsteher einer politischen Gemeinde seit Mitte des 19. Jahrhunderts nicht mehr. Die analoge Bezeichnung ist von der Funktion her (als gewähltes Gemeindeoberhaupt) meist "Gemeindepräsident," je nach Ort oder Kanton aber auch "Stadtpräsident, Gemeindeammann, Stadtammann, Talammann, Bezirkshauptmann" etc., in der Welschschweiz "Syndic" (VD, FR, VS), "Maire" (GE, BE, JU) oder "Président(e)" (NE), in der italienischsprachigen Schweiz (TI) "Sindaco" oder (GR) "Podestà".

Der Begriff kommt allerdings da und dort auf der Ebene der Bürgergemeinden vor. So lautet etwa die Amtsbezeichnung des Präsidenten der Bündner Bürgergemeinde Arosa "Bürgermeister." Dieser bildet zusammen mit den "Bürgerräten" die Exekutive der Bürgergemeinde.

In Liechtenstein erfolgt die freie Wahl der Ortsvorsteher und der übrigen Gemeindeorgane durch die Gemeindeversammlung. Nur der Ortsvorsteher im Hauptort Vaduz darf gemäß einem fürstlichen Erlass aus dem 19. Jahrhundert die Bezeichnung "Bürgermeister" tragen.

Der Bürgermeister wird in Südtirol direkt gewählt. Das Wahlgesetz unterscheidet zwischen Gemeinden mit mehr und weniger als 15.000 Einwohnern: In Ortschaften mit weniger als 15.000 Einwohnern sind prinzipiell alle Gemeinderatskandidaten auch Bürgermeisterkandidaten, sofern sie nicht ausdrücklich darauf verzichten. Gewählt ist der Kandidat mit den meisten Stimmen, eine Stichwahl ist nicht vorgesehen. In den größeren Städten wird ein dafür bestimmter Bürgermeisterkandidat einer Liste oder Koalition gewählt. Erreicht keiner der Kandidaten die absolute Mehrheit, kommt es zu einer Stichwahl.

In Südtirol wird der vorgesehene Stellvertreter des Bürgermeisters "Vizebürgermeister" genannt und vom Bürgermeister bestellt. In Orten mit mehr als 13.000 Einwohnern und in Orten, in denen dies von der Gemeindesatzung vorgesehen ist, darf der Vizebürgermeister nicht derselben Sprachgruppe wie der Bürgermeister angehören.

Der Bürgermeister ist der Chef der Gemeinderegierung, die je nach Gemeindestatus als "Stadtrat" oder als "Gemeindeausschuss" bezeichnet wird. Mitglieder dieses Gremiums sind die "Referenten", die in der Vergangenheit italianisierend den Titel "Assessoren" trugen.

Die Amtsentschädigung wird vom Regionalrat bestimmt und nach mehreren Parametern gewichtet (Einwohnerzahl, Zahl der Fraktionen, …). Die Bezüge des Vizebürgermeisters und der Referenten sind schließlich nach einem Prozent-Schlüssel an jene des Bürgermeisters gebunden.

In der Provinzhauptstadt Bozen ist er in derselben Zeit auch Bezirksvorsitzender, was einem österreichischen Bezirkshauptmann entspricht.

In den Niederlanden wird der Bürgermeister nicht gewählt, sondern in den großen Städten nach teilweise parteipolitischem Proporz bestimmt, in der Regel geht es jedoch vor allem um die politische Kräfteverteilung vor Ort. Der Bürgermeister bildet zusammen mit den "wethouders" (wörtlich: ‚Gesetzhalter‘, im Deutschen etwa Schöffen oder Beigeordnete), die vom Gemeinderat gewählt werden, die Regierung der Gemeinde. Man spricht vom "college van burgemeester en wethouders", abgekürzt "b & w". Außerdem ist der Bürgermeister Vorsitzender des Gemeinderats. Einer der "wethouders" wird als Stellvertreter des Bürgermeisters zum "loco-burgemeester" gewählt. Ebenso wie der Bürgermeister dürfen die "wethouders" nicht dem Gemeinderat angehören, obwohl letztere oft aus dessen Mitte kommen.

Es ist eine Diskussion darüber entstanden, ob der Bürgermeister in Zukunft gewählt werden soll, doch bislang kam es nur zu wenigen Referenden vor Ort (zuletzt 2008). Die Regierung schlägt dabei zwei Kandidaten vor und die Einwohner des Ortes entscheiden per Volksabstimmung. Allerdings gehören die beiden Kandidaten normalerweise derselben Partei an. Vor allem die sozialliberale Partei Democraten 66 setzt sich für die Direktwahl ein. D66-Minister Thom de Graaf war 2005 mit seinem Gesetzentwurf zur Direktwahl sehr weit gekommen, bis die Sozialdemokraten in der Ersten Kammer es doch noch zu Fall brachten. De Graaf trat zurück und wurde übrigens 2007 zum Bürgermeister von Nijmegen ernannt.

Einer Umfrage von 2004 zufolge wünschen sich zwei Drittel der Niederländer die Direktwahl, ein Drittel ist dagegen. Dafür ist die Mehrheit der Anhänger jeder einzelnen Partei. Der damalige Gesetzentwurf von De Graaf wurde allerdings von nur 53 % der Befragten begrüßt. Niederländische Gemeinderatsmitglieder, hat eine Umfrage 2010 ergeben, möchten vor allem die heutige Situation behalten: 46 % meinen, dass der Bürgermeister weiterhin von der Krone eingesetzt werden soll, nach Übereinkunft mit dem Gemeinderat. 32 % favorisieren eine formelle Wahl durch den Rat. 5 % wünschen, dass die Mitglieder der Gemeinderegierung den Bürgermeister unter sich wählen, so wie in der nationalen Regierung der Ministerpräsident gewählt wird. Für eine Wahl durch die Bürger sind lediglich 16 % der Gemeinderatsmitglieder.

Typischerweise ist ein niederländischer Bürgermeister ein Jurist oder Verwaltungsexperte, der für sechs Jahre eine Gemeinde leitet und danach eine andere, je nachdem, welche ihm angeboten wird. Der 1944 in Rotterdam geborene Rechtsliberale Ivo Opstelten zum Beispiel begann seine Karriere in der Gemeindeverwaltung von Vlaardingen (1970 bis 1972). Von 1972 bis 1977 war er Bürgermeister von Dalen, dann bis 1980 von Doorn und bis 1987 von Delfzijl. Nach einer Position im Innenministerium wurde er 1992 Bürgermeister von Utrecht und krönte seine Bürgermeisterkarriere mit der zweitgrößten Stadt des Landes, Rotterdam (1999 bis 2008).

Im karibischen Teil der Niederlande heißt die dem Bürgermeister entsprechende Funktion "gezaghebber", die dem "college van burgemeester en wethouders" entsprechende Institution heißt "bestuurscollege".

In Rumänien wird der Bürgermeister für vier Jahre gewählt und kann beliebig oft für dieses Amt erneut kandidieren.

In San Marino wird der Bürgermeister für fünf Jahre gewählt und kann beliebig oft für dieses Amt erneut kandidieren.

Der Bürgermeister (ungarisch: "polgármester") ist der Vorsitzende der örtlichen Vertretungskörperschaft. Er ist verantwortlich für die erfolgreiche und gesetzmäßige Tätigkeit der Selbstverwaltung. Er ernennt die Mitarbeiter des Amtes, leitet die Arbeit des Amtes und die Sitzungen der Gemeindevertretung, sowie vertritt die Selbstverwaltung. Der Bürgermeister wird durch direkte Wahlen für vier Jahre gewählt.

In der englischsprachigen Welt gibt es eine direkte Übersetzung von Bürgermeister als „Burgomaster“, jedoch wird das heutige Amt in der Regel als „Mayor“ ins Englische übersetzt, da es den heute üblichen Hauptvertreter der Bürgerschaft dort bezeichnet.

Mit derselben Wurzel wie Major war der englische Mayor ursprünglich der feudale Bezirksverwalter, die Bürgerschaft von London errang jedoch irgendwann das Recht, den Mayor selbst zu wählen. Dieses Recht verbreitete sich, und im heutigen Sprachgebrauch meint Mayor jeweils den Vorsitzenden des Gemeinderats (chief magistrate). In der Regel wird dieser indirekt vom Gemeinderat gewählt. Der Titel Mayor wird auch in Wales und Nordirland gebraucht, aber nicht in Schottland, wo der Titel „Provost“ lautet.

Der Titel „Mayor“ ist auch die übliche Bezeichnung in den USA, dort wird jedoch häufig die Stadtvertretung geteilt: bei der zweigleisigen Stadtvorsteherregierung (council-manager government) gibt es die Ämter des Ratsvorsitzenden und des Verwaltungsdirektors, wobei die Stadtvertretung hauptsächlich in den Händen des letzteren Stadtvorstehers liegt, der nicht dem Stadtrat angehört, aber von diesem ernannt wird. Die Bezeichnung Mayor bzw. Bürgermeister gilt hier dem Ratsvorsitzenden.




</doc>
<doc id="11022" url="https://de.wikipedia.org/wiki?curid=11022" title="Inn">
Inn

Der Inn (, lat. "Aenus", auch "Oenus", ) ist ein 517 km langer, durch die Schweiz, Österreich und Deutschland verlaufender rechter Nebenfluss der Donau. An der Mündung in Passau fließen im Mittel 738 m³/s Wasser in die nur 690 m³/s heranführende Donau. Der größere Mittelwert des Inn beruht auf den Hochwässern des Gebirgsflusses. Während sieben Monaten führt der Inn am Zusammenfluss in Passau weniger Wasser als die Donau.

Der Name "Inn" leitet sich von den keltischen Wörtern "en" sowie "enios" ab, die frei übersetzt Wasser bedeuten. In einer Urkunde des Jahres 1338 ist der Fluss mit dem Namen "Wasser" eingetragen. Die erste schriftliche Erwähnung stammt aus den Jahren 105 bis 109 von Tacitus (Publii Corneli Taciti historiarium liber tertius). Sie lautet: "„...Sextilius Felix ... ad occupandam ripam Aeni fluminis, quod Raetos Noricosque interfluit, missus.“" bzw.: „...wurde Sextilius Felix ... zum Einnehmen des Ufers des Flusses Inn, der zwischen Rätern und Norikern fließt, geschickt.“
Auch von weiteren Autoren der römischen Kaiserzeit wird der Fluss als "Ainos" (griechisch) oder "Aenus" (Latein) erwähnt. Im mittelalterlichen Latein wird er zumeist "Enus" geschrieben, von den Humanisten "Oenus". Durch den Lautwandel im Altbairischen von e zu i wird aus "Enus" "In". Bis ins 17. Jahrhundert wird es so oder "Yn" geschrieben, aber auch "Ihn" oder "Yhn". Das Doppel-n taucht erst im 16. Jahrhundert auf, etwa im Tiroler Landreim von 1557. Seit dem 18. Jahrhundert ist diese Schreibweise und die Aussprache mit kurzem Vokal üblich. Früher wurde die Bezeichnung meist als Neutrum betrachtet ("daz In" heißt es beispielsweise im Nibelungenlied), seit dem 16. Jahrhundert ausschließlich als Maskulinum.

Die Erwähnungen in der Römerzeit beziehen sich auf den Unterlauf, der Tiroler Abschnitt wird erstmals bei Venantius Fortunatus im 6. Jahrhundert als "Aenus" bezeichnet. Der Name "Engadin" und die rätoromanische Bezeichnung "En" deuten darauf hin, dass auch der Oberlauf seit jeher so bezeichnet wurde. Auch wenn vereinzelt die Auffassung vertreten wurde, dass der Inn in der Nähe der Etsch am Reschen entspringt, wird spätestens seit dem 16. Jahrhundert der Ursprung einheitlich im Bereich der Seen am Malojapass gesehen.

Möglicherweise besteht ein Zusammenhang zwischen dem Namen Inn und dem des französischen Flusses Ain.

Mit einer Gesamtlänge von 517 Kilometern (mit Aua da Fedoz 520 km) ist der Inn einer der längsten und mächtigsten Alpenflüsse. Nahezu zwei Drittel seines Flusslaufes liegen im Gebiet der Alpen. 193 km fließt der Inn durch Österreich.

Das Einzugsgebiet des Inns beträgt 26.130 km² (nach anderen Angaben 26.053 km²). Davon liegen 1689 km² im Kanton Graubünden, 254 km² (am Oberlauf des Spöl und des Stillebachs) in Italien, 7880 km² in Tirol,
8061 km² in Bayern und rund 8250 km² in Salzburg und Oberösterreich.

Im Einzugsgebiet des Inn befinden sich 823 Gletscher, die zusammen 395 km² oder 1,5 % der Fläche einnehmen.
Der höchste Punkt im Einzugsgebiet ist der Piz Bernina mit 

Mit einer mittleren Wassermenge von 738 Kubikmetern pro Sekunde ist der Inn, nach dem Rhein, der Donau und der Elbe (wenn die Nebenflüsse des Ästuars dazugerechnet werden), der viertwasserreichste Fluss Deutschlands sowie der zweitwasserreichste Österreichs. Er führt der Donau mehr Wasser zu als Lech, Isar, Enns und Traun zusammen. Obwohl die Elbe fünfmal so viel Stromgebiet entwässert, ist sie nur unwesentlich wasserreicher, denn in den Alpen sind die Niederschlagsmenge und die Abflussrate höher.

Das Abflussregime des Inns ist aufgrund der alpinen Schneeschmelze und der größeren mittleren Hangneigung in seinem Einzugsgebiet unausgeglichener als das der Donau. Insbesondere im Oberlauf ist das Abflussregime stark durch die Vergletscherung am Alpenhauptkamm (Zentralbereiche der Ötztaler, Stubaier, Zillertaler Alpen und Hohen Tauern) beeinflusst. Am Pegel Innsbruck weist der Inn ein nivo-glaziales Abflussregime mit einem Anteil von 10 % Gletscherwasser auf, das nur im Zeitraum von Mai bis Oktober anfällt und im Juli und August mit 25 % den höchsten Anteil am Abfluss erreicht.
Mittlere monatliche Abflüsse des Inns (in m³/s) am Pegel Passau-Ingling
Reihe 1920/2005

Der mittlere Abfluss des Inns in Passau ist zwar rund 7 % größer als der der Donau, der Inn führt aber die meiste Zeit des Jahres (vom Frühherbst bis zum Frühling) weniger Wasser. Auch wenn visueller Eindruck und Gesamtwasserführung nahelegen, von der Mündung der Donau in den Inn zu sprechen, ist der Name "Donau" für den vereinigten Strom zu rechtfertigen; denn die Donau ist hier mit 547 km länger als der Inn mit 517 km und die Donau behält, anders als der Inn, ihre Fließrichtung unverändert bei.

Der Fluss entspringt beim Malojapass im Schweizer Engadin in 2484 m Höhe nahe dem Lunghinsee. In der Nähe des Ursprungs liegt ein europäischer Hauptwasserscheidepunkt (Nordsee, Schwarzes Meer, Adria).

Im "Oberengadin" wird der Inn bis zum Zusammenfluss mit dem größeren Flaz auch "Sela" genannt und durchfließt zunächst den Silser-, den Silvaplaner-, den Champfèrer- und den St. Moritzersee. Der kleine Lej da Gravatscha nahe der Mündung des Flaz ist ein wichtiges Brutgebiet für Vögel. Im "Unterengadin" durchfließt der Inn mit deutlich stärkerem Gefälle mehrere Schluchten.

Unterhalb der schweizerisch-österreichischen Grenze am Engpass von Finstermünz wird sein Tal im Bundesland Tirol "Oberinntal" genannt und unterhalb der Einmündung der Melach bei Zirl "Unterinntal". Zwischen Kufstein und Erl verläuft die österreichisch-deutsche Staatsgrenze in Flussmitte. Danach durchquert der Inn die südöstliche Ecke Bayerns; ab der Mündung der Salzach bis zur Stadtgrenze von Passau markiert er wieder die deutsch-österreichische Grenze. Am unteren Inn stehen mehrere große Stauwerke. Hier erstreckt sich auch über eine Länge von 55 Kilometern das "Europareservat Unterer Inn". Der Inn zwischen Braunau und Schärding ist Namensgeber für das angrenzende oberösterreichische Innviertel (politische Bezirke Braunau, Schärding, Ried im Innkreis).
Der Inn mündet in der „Dreiflüssestadt“ Passau in die Donau. Noch ein längeres Stück nach dem Zusammenfluss bleiben das grüne Gletscherschmelzwasser des Inns, das blaue Donauwasser und das dunkle Moorwasser der von Norden mündenden Ilz in der Donau unvermischt unterscheidbar. Die 2,34 km unterhalb der Innmündung nahe dem rechten Ufer liegende Felseninsel Kräutelstein ist noch vom unvermischten Innwasser umspült. Auffallend ist, wie stark das grüne Wasser des Inns das Wasser der Donau beiseite drängt. Dies hängt mit der zeitweise sehr großen Wassermenge des Inns und den unterschiedlichen Tiefen der beiden Gewässer zusammen (Inn: 1,90 Meter, Donau: 6,80 Meter) – „der Inn überströmt die Donau“.


Bis Landeck verläuft der Inn in den Zentralalpen, wobei er hauptsächlich Kristallingebiete berührt und bei Ardez in das Engadiner Fenster mit seinen Bündnerschiefern eintritt. Von Fließ bis Landeck durchbricht er den Landecker Quarzphyllit. Ab Landeck bildet das Inntal als großes Alpenlängstal die Grenze zwischen den Nördlichen Kalkalpen und den Zentralalpen. Zwischen Schwaz und Brixlegg durchfließt der Inn die Grauwackenzone und anschließend die Nördlichen Kalkalpen. Bei Erl erreicht er das Alpenvorland und durchquert in Bayern die eiszeitlich überformte Flysch- und Molassezone, die von Moränenresten, diluvialen Schotterkörpern und Terrassen geprägt ist. Bei Schärding tritt er in die Böhmische Masse ein.

Die folgende Tabelle enthält alle Zuflüsse mit einem Einzugsgebiet von mehr als 500 km² oder einem mittleren Abfluss (MQ) von mehr als 10 m³/s. Eine umfassende Auflistung findet sich unter Liste von Zuflüssen des Inns.

Der Inn ist heute über weite Strecken begradigt und verbaut, die Fließstrecke ist durch zahlreiche Kraftwerke beeinträchtigt, eine längere freie Fließstrecke von 150 km besteht noch zwischen Fließ und Kirchbichl. Vereinzelt finden sich noch naturnähere Abschnitte und Reste der ursprünglichen Auwälder, die oft als Natur- oder Landschaftsschutzgebiet ausgewiesen sind. Am Unteren Inn im bayrisch-oberösterreichischen Grenzgebiet hat sich der Charakter des Inns in Folge der Kraftwerksbauten grundlegend vom alpinen zum Tieflandfluss mit großen, offenen Wasserflächen gewandelt. Neben diesen Wasserflächen entstanden Anlandungen und weitläufige Aubereiche, die ein international bedeutendes Brut-, Rast- und Überwinterungsgebiet für rund 300 Vogelarten darstellen.

Die Au- und Wasserflächen am unteren Inn sind unter anderem als Europaschutzgebiet ausgewiesen.

Innerhalb der Hochwasserschutzdämme am unteren Inn finden sich Silberweidenauen, die Auwälder außerhalb bestehen hauptsächlich aus Eschen und Grauerlen, in trockeneren Bereichen auch aus Bergahornen.

Die früher weit verbreitete Deutsche Tamariske ist durch Verbauungen fast ausgerottet worden, einzelne Bestände finden sich in den Mieminger und Rietzer Innauen, im Oberen Gericht sowie im Engadin.

An den Stauseen am unteren Inn finden sich als gefährdete Pflanzenarten Tannenwedel und Schwanenblume.

Vom Ursprung bis Landeck zählt der Inn zur Forellenregion, unterhalb zur Äschenregion, im Unterlauf besteht ein Übergang von der Barben- zur Brachsenregion. Im Tiroler Inn konnten von ursprünglich 31 Fischarten nur mehr 17 nachgewiesen werden. Zu den Arten, die im gesamten Tiroler Verlauf vorkommen, gehören Bachforelle, Regenbogenforelle und Äsche, als gefährdet gelten Huchen, Strömer und Aalrutte.

Im unteren Inn hat sich durch den Kraftwerksbau die Fischfauna verändert. Neben den früher typischen Arten Barbe, Nase und Huchen haben sich Fischarten, die ruhigere Flussabschnitte oder stehende Gewässer bevorzugen, angesiedelt, darunter Brachse, Karpfen, Hecht, Rotfeder und Rotauge.

Der Europäische Biber wurde in den 1970er Jahren auf der bayerischen Seite der Innstauseen wieder angesiedelt und hat sich seither ausgebreitet. Zwischen der Salzachmündung und der Antiesenmündung finden sich rund 15 Reviere, der Biber ist aber auch flussaufwärts bis ins Tiroler Oberinntal gewandert. Allmählich siedelt sich am unteren Inn auch der Fischotter wieder an.

Die Auen dienen zahlreichen, darunter etlichen gefährdeten, Vogelarten als Lebens- und Brutraum. Bedeutsam sind u. a. Brutvorkommen von Flußuferläufer, Nachtigall und Gartenbaumläufer in der Silzer Innau, vom Flussregenpfeifer in der Innschleife bei Kirchbichl, oder von Zwergdommel, Nachtreiher, Seidenreiher, Rohrweihe, Schwarzmilan, Schwarzkopfmöwe, Flussseeschwalbe, Eisvogel, Blaukehlchen, Brandgans, Weißkopfmöwe und Lachmöwe am Unteren Inn.

Der Inn weist in Tirol, Bayern und Oberösterreich im gesamten Verlauf Gewässergüteklasse II (mäßig verunreinigt) auf, mit Ausnahme des Tiroler Abschnittes von der Einmündung der Sanna bis zur Einmündung der Pitze, wo er Klasse I–II erreicht.

Schifffahrt auf dem Inn gab es schon zur Zeit der Römer. Im Jahr 1190 gewährte Kaiser Heinrich IV. die Einrichtung einer Salzstapelniederlassung in Mühldorf am Inn. Es folgten weitere Innstädte mit verschiedenen Rechten zum Handel auf dem Inn. Oberer Endpunkt der Innschifffahrt war Hall, das dadurch der wichtigste Warenumschlagplatz Nordtirols war und u. a. das Stapelrecht für Getreide besaß. Weiter hinauf bis Mötz konnte der Inn noch mit Flößen (überwiegend flussabwärts) befahren werden. Neben dem Salz aus Tirol wurden besonders Eisenerz, Silber, Kupfer, Kalk, Holz, Tuche und Tiroler Wein in Schiffszügen flussabwärts bis Wien geschifft. Aus dem Engadin wurde Holz nach Innsbruck, zur Salzpfanne nach Hall und zum Teil bis nach Rosenheim geflößt. Wasserburg am Inn war die bedeutendste Stadt der Innschifferei. Dort und in den anderen Städten brachten es die Schiffsmeisterfamilien zu erheblichem Wohlstand. 

Für die Fahrt flussabwärts wurden meist einfache, flache Plätten gezimmert, die am Zielort als Bau- oder Nutzholz verkauft werden konnten. Bei der Rückfahrt transportierte man besonders Weizen, Fleisch, Fett und österreichischen Wein. Dabei zog unter Führung des Stangenreiters auf dem Treidelweg ein Pferdevorspann die Schiffe. Sechs bis zwanzig hintereinander gespannte Pferde zogen die Schiffe, so konnten auf einem Leitschiff mit zwei bis drei Lastschiffen bis zu 100 Tonnen Getreide flussaufwärts transportiert werden. Die Fahrt von Hall nach Kufstein dauerte rund fünf Stunden, nach Wien knapp eine Woche. Flussaufwärts brauchte das Ziehen eines Lastzugs von Kufstein nach Hall vier bis fünf Tage.

Neben Gütern wurden auf dem Inn auch Personen befördert. Insbesondere für das Militär war der Fluss ein bedeutender und sicherer Nachschubweg. So wurden 1532 in Hall 20.000 Italiener und Spanier auf 45 Schiffen nach Wien verschifft, wo sie das Heer Kaiser Karls V. gegen die Türken verstärken sollten. 1765 wurde der Leichnam des in Innsbruck verstorbenen Kaisers Franz I. Stephan auf einem Schiff von Hall nach Wien transportiert, gefolgt von 19 Schiffen mit seiner Gattin Maria Theresia und ihrem Hofstaat.

Mit der Eröffnung der Unterinntalbahn von Kufstein nach Innsbruck im Jahre 1858 kam das Ende für die Innschifffahrt in Tirol. Mit dem Bau von Staustufen mit Wasserkraftwerken, die nicht über Schleusen verfügten, wurde eine durchgehende Schifffahrt unmöglich. Nur örtlich, beispielsweise zwischen Neuhaus am Inn/Schärding und Passau sowie in Wasserburg am Inn findet auf dem Inn Fahrgastschifffahrt statt.

Von Kufstein bis Niederndorf gab es ab 1998, anfangs mit dem kleineren Motorschiff Tirol, ab April 2000 mit dem zweischraubigen 85-t-116-Personen-Schiff St. Nikolaus bis 2011 eine touristisch orientierte Innschifffahrt. Eingestellt wurde sie, auch weil Hochwässer mitunter den Betrieb verhinderten, mangels ausreichender Fahrgastzahlen, das Schiff "St. Nikolaus" wurde im April 2013 nach Hamburg verkauft.

Der Inn ist zwar nicht in ganzer Länge seines Alpentals eine günstige natürliche Leitlinie für den Verkehr, zum einen wegen mehrerer Engpässe, zum anderen wegen der für den Alpenquerverkehr ungünstigen Längstalrichtung; seine Breite und relative Klimagunst machen das Inntal trotzdem zu einem früh besiedelten eigenen Wirtschaftsraum. Da es am breiten und stark strömenden Inn früher nur wenige Brücken gab, verlangte man seit dem Mittelalter für Bau und Erhalt Brückenzoll, meist von Fuhrwerken, etwa in Zams.

Heute verlaufen die Bundesautobahn 93 und die Inntalautobahn A 12 sowie die Unterinntalbahn und die Arlbergbahn im Inntal. Die Verteilerfunktion zu niedrigen Alpenpässen wie Reschenpass und Brennerpass ist einerseits der Wirtschaft förderlich, belastet das Tal aber zunehmend mit Umweltfolgen des Individualverkehrs. So werden die EU-Grenzwerte für die Luftreinhaltung im Unterinntal oft erheblich überschritten.
Derzeit ist die Neue Unterinntalbahn als Zulaufstrecke des Brennerbasistunnels in Bau.

In der Vergangenheit spielte die Fischerei am Inn eine große wirtschaftliche Rolle. So wurden Fische aus dem Inn und den Oberengadiner Seen bis ins 19. Jahrhundert nach Italien verkauft.
Die Fischerei nahm bisweilen überhand, so dass bereits um 1553 eine Fischordnung für das Herzogtum Bayern erlassen wurde, da der Fischbestand fast „verödigt“ war. Dabei wurden erstmals Fangbeschränkungen und Mindestmaße („Prüttelmaße“) eingeführt. Außerdem wurde der Betrieb von „Archen“, reusenartigen Einbauten im Fluss, untersagt, die nicht nur die Fischbestände dezimierten, sondern auch Hindernisse für die Schifffahrt darstellten. Heute hat die Fischerei keine kommerzielle Bedeutung mehr.

Am Oberlauf des Inn vom Schweizer Gebiet bis ins österreichische Landeck in Tirol befinden sich mehrere Wasserkraftwerke. Staustufen im Unterlauf ab Kufstein dienen sowohl der Energiegewinnung als auch dem Hochwasserschutz. Da diese Kraftwerke nicht über Schleusen verfügen, wird die Schiffbarkeit des Inn durch diese Kraftwerke stark eingeschränkt.

Das älteste Tiroler Kraftwerk ist in Kirchbichl, nach über 70 Jahren wird die bestehende Wehranlage für Extremereignisse erweitert und zur bestehenden Wehranlage eine zusätzliche Abflussmöglichkeit geschaffen. Dafür ist der Ausbau des rund einen Kilometer langen Triebwasserweges geplant. Dadurch und durch den Bau einer Hochwasserentlastung kann das Krafthaus um eine Turbine erweitert werden, die Regeljahreserzeugung wird von derzeit 131 GWh um etwa 45 GWh steigen.

Das Kraftwerk Imst nutzt eine für ein Laufkraftwerk ungewöhnlich große Fallhöhe von 143,5 m, indem der Inn in der Runserau bei Fließ aufgestaut und das Wasser durch einen 12,3 km langen Druckstollen quer durch das Venetmassiv in die Imsterau geleitet wird, wodurch es das Innknie bei Landeck abschneidet.

Der Inn bietet im Oberlauf vielfältige Möglichkeiten für den Wassersport, vor allem für Wildwasserpaddeln und Rafting, auf den Oberengadiner Seen (Silsersee, Silvaplanersee und St. Moritzersee) u. a. für Wind- und Kitesurfen. Ein beliebter Abschnitt bei Wildwassersportlern ist die 14 km lange Imster Schlucht, deren Schwierigkeitsgrad abhängig vom Wasserstand zwischen WW II-III und III-IV liegt.

Entlang der Hochwasserdämme führen auf weiten, zusammenhängenden Strecken Radwege. Der Inn-Radweg folgt dem Flusslauf von Maloja bis zur Mündung. Entlang des Inn liegen viele Baggerseen, die durch Kiesgewinnung entstanden sind. Örtlich verkehren linienmäßig Personenschiffe. Das Inn-Museum in Rosenheim dokumentiert die Geschichte des Inn und der Innschifffahrt.




</doc>
<doc id="11023" url="https://de.wikipedia.org/wiki?curid=11023" title="Landkreis Emsland">
Landkreis Emsland

Der Landkreis Emsland ist ein niedersächsischer Landkreis im Nordwesten Deutschlands. Er ist nach dem ihn durchquerenden Fluss Ems und dem Emsland als kulturell-historische Region und Flusslandschaft benannt.

Der Landkreis Emsland liegt im Westen Niedersachsens. Er umfasst große Teile der kulturellen und historischen Region Emsland. Prägend für das Kreisgebiet sind die Flüsse Ems und Hase mit ihren Zuflüssen, die ausgedehnten Moorflächen des Bourtanger Moores und des Küstenkanalmoores, sowie die Geestgebiete des Hümmlings und der Lingener Höhe.

Der Kreis hat eine maximale Nord-Süd-Ausdehnung von 93 km und eine Ost-West-Ausdehnung von 56 km. Überwiegend ist er ein Flachland zwischen 20 und 35 m Seehöhe. Größere Erhebungen gibt es nur im Nordosten und Südosten: Der Nattenberg mit bei Emsbüren, der Windmühlenberg auf der Grenze von Langen und Thuine mit und der Windberg mit bei Werpeloh im Hümmling.

Städte sind Haren (Ems), Haselünne, Freren, Lingen (Ems), Meppen (Kreisstadt), Papenburg und Werlte (seit dem 22. März 2017).

Der Landkreis war von seiner Gründung bis zum 5. Dezember 1993 der flächenmäßig größte Landkreis Deutschlands. Abgelöst wurde er vom damals neu geschaffenen brandenburgischen Landkreis Uckermark, welcher am 4. September 2011 seinerseits vom Landkreis Mecklenburgische Seenplatte abgelöst wurde.

Der Landkreis grenzt im Uhrzeigersinn im Norden beginnend an die Landkreise Leer, Cloppenburg und Osnabrück (alle in Niedersachsen), an den Kreis Steinfurt (in Nordrhein-Westfalen) sowie an den Landkreis Grafschaft Bentheim (wiederum in Niedersachsen). Im Westen grenzt er an die niederländischen Provinzen Drente und Groningen.

Im Landkreis Emsland gibt es 76 Naturschutzgebiete. Das größte (Tinner Dose-Sprakeler Heide) hat eine Fläche von rund 3.500 ha, das kleinste (Zitterteiche) eine Fläche von 2,9 ha. Das ehemals kleinste Naturschutzgebiet des Landkreises, das 2,6 ha große Naturschutzgebiet „Hasealtarm bei Wester“, ging 2017 im Naturschutzgebiet „Natura-2000-Naturschutzgebiet in der unteren Haseniederung“ auf.

Siehe auch:

Im Landkreis Emsland wird von der einheimischen Bevölkerung noch vielerorts – insbesondere im ländlichen Raum – die niedersächsische Sprache (umgangssprachlich plattdüütsch oder auch platt/plattdeutsch) gesprochen. Mit einer Bevölkerungsdichte von 150 Einwohnern pro Quadratkilometer (2017) gehört der Landkreis zu den dünnstbesiedelten Regionen in Deutschland. Der Landkreis ist katholisch geprägt. Der Anteil der Katholiken an der Gesamtbevölkerung wurde im Zensus 2011 auf 68,4 % geschätzt.
Der Landkreis Emsland weist eine hohe Geburtenrate auf.

Die Region wurde nach der letzten Eiszeit, in der hier zunächst Jäger und Sammler lebten, etwa um 4000 v. Chr. von der Trichterbecherkultur in den Ackerbausektor Nord-Mitteleuropas einbezogen. Ab 3500 v. Chr. entstanden im Emsland über 60 Megalithanlagen und Hünengräber primär zwischen Meppen und Lastrup sowie zwischen Lingen und Freren. Weitere Fundstücke aus der Jungsteinzeit stammen aus Gebieten entlang der Ems, der Hase sowie aus dem Dreieck Freren, Lengerich und Fürstenau.

Im Emsland, zwischen den Flüssen Ems und Hunte, lag in der jüngeren Bronzezeit (ca. 1100 bis 800 v. Chr.) das Verbreitungsgebiet der sogenannten Ems-Hunte-Gruppe. Fundstücke vom Nattenberg bei Emsbüren zeigen mythologische Schiffsdarstellungen dieser Kultur. Ein weiteres prächtiges Fundstück stammt vom Heiligenberg bei Gleesen in der Nähe von Bramsche. Als Kultanlage gilt die auf dem Höhenzug Hörtel bei Leschede aus sechs Pfostendoppelreihen bestehende, 16 Meter lange und 216 Pfosten umfassende Avenue dieser Kultur. Diese Avenue kam auf einem Gräberfeld aus der Übergangszeit von der jüngeren Bronzezeit zur frühen Eisenzeit zwischen zwei Grabhügeln zum Vorschein. Aus der Eisenzeit sind ebenfalls zahlreiche Relikte überkommen.

Eine germanische Besiedlung des Emslandes ist vor der Zeitenwende anzunehmen, wobei die Germanen in verhältnismäßig kleinen Siedlungen wohnten. Das Suffix „-ing“ ist ein Hinweis auf die Besitznahme von Gebieten durch die Germanen. Dieses gilt besonders für Orte südlich von Lingen. Orte mit dem Suffix „-ing“ sind:
Lingen, Messingen, Völleringhook, Sommeringen, Estringen, Münnigbüren, Mehringen, Imming, Schardingen, Rupingberg sowie die Orte Raming, Teglingen und Wippingen, nördlich von Lingen.

Die Topografie zeigt aber auch, dass diese Orte bereits vor der germanischen Inbesitznahme besiedelt waren: Zum Beispiel befinden sich in Mehringen und in Teglingen Grabanlagen aus dem Neolithikum. Das Gebiet des ehemaligen Landkreises Hümmling zeigt ebenfalls eine durchgehende Besiedlung seit dieser Zeit, aufgrund von mehr oder weniger gut erhaltenen Hünengräbern. Dieses Gebiet wird auch Sögeler Geest genannt. Des Weiteren belegen auch zahlreiche Funde, dass der Ort Estringen bei Bramsche schon in der Steinzeit große Bedeutung gehabt haben muss. Die Herkunft dieses Ortsnamens leitet sich von den Begriffen Esch und Ting ab. Tingplätze waren bei den Germanen geheiligte religiöse Plätze, zusätzlich dienten diese Plätze als Gerichts- und Versammlungsplatz. Meppen war wahrscheinlich schon in dieser Zeit ein wichtiger germanischer Handelsort. Vom Osnabrücker Land, einer Region mit bedeutenden Eisenerzvorkommen, konnte Eisen über die Hase und die Ems nach Ostfriesland und Groningen, im Tausch mit Friesensalz, verschifft werden.

Die Germanen kannten keinen Verwaltungsstaatswesen im heutigen Sinne. Die germanischen Stämme waren ähnlich wie Personenverbandsstaaten organisiert. Die Angehörigen eines Stammes schworen ihrem Herrscher die Treue. Der „Staat“ wurde nicht über eine räumliche Ausdehnung definiert, sondern über seine Menschen und deren Stellung zum Herrscher. Deshalb waren die Reiche stark mit dem jeweiligen Herrscher verbunden, und der Tod eines Herrschers bedeutete oft auch den Untergang des Reiches. Laut dem römischen Historiker Tacitus siedelte der germanische Stamm der Ampsivarier (germanisch „Ems-Männer“) in nördlichen Emsland, der germanische Stamm der Chasuarier an der Hase und der Volksstamm der Brukterer im südlichen Emsland. Zudem ist bekannt, dass die kaiserliche römische Armee mehrfach die Ems als Transportweg genutzt und an ihren Ufern auch Lager, wie das Lager Bentumersiel errichtet hat. Römische Funde, wie zum Beispiel in Lengerich, in Gersten, in Geeste, nördlich von Meppen, sowie verschiedenen Fundstätten im Bereich des Zusammenfluss der Großen Aa mit der Ems in Gleesen bei Bramsche zeigen zudem den Zusammenhang mit den römischen Aktionen in spätaugusteisch-frühtiberischer Zeit. In Hesselte wurde 2007 eine germanische Siedlung aus der römischen Kaiserzeit entdeckt, die vermutlich am Marschweg der römischen Truppen lag. Römische Funde an der Ems sind dabei besonders interessant, da sie Belege für die Aktionen des Germanicus 15–16 n. Chr. sein können.

Ab dem 5. Jahrhundert unterstand das Emsland dem Einfluss des fränkischen Reiches und dem Stammesherzogtum Sachsen. Territorial gehörte Lingen zum Venkigau und Meppen zum Hasegau. Die Gaugrafen des Hasegaues standen mit dem Hause Widukinds in verwandtschaftlicher Beziehung. Als erster Gaugraf des Hasegaus wird Mitte des 10. Jahrhunderts Heinrich genannt, der seinen Wohnsitz auf der festen Burg Arkenau (Arkenowa) in Brokstreek bei Essen (Oldenburg) hatte, sowie sein Sohn Gottschalk. Ab dem 9. Jahrhundert erfolgen durch die Christianisierung des Emslandes die ersten urkundlichen Erwähnungen von Ortschaften im Emsland. Im Jahre 834 erfolgt die urkundliche Erwähnung von Meppen in einer Schenkungsurkunde Vita secunda S. Liudgeri von Kaiser Ludwig dem Frommen, in der die Missionszelle Meppen der Benediktinerabtei Kloster Corvey in Höxter übertragen wurde. Im Heberegister der Benediktinerabtei vom Kloster Werden an der Ruhr werden 836 Messingen und Thuine, 891 Freren und 975 Lingen als zum Venkigau gehöriger Ort erwähnt.
Im Januar 1180 entzog Friedrich Barbarossa dem sächsischen Herzog Heinrich der Löwe die Reichslehen, so dass sich auf dem Territorium des heutigen Emslandes das Oberstift Münster und die Grafschaft Tecklenburg herausbildeten. Zum Beispiel büßte Hümmlingen 1394 seine territoriale Unabhängigkeit durch die Eroberung Cloppenburgs durch den Bischof Otto IV. von Hoya von Münster ein. In der Urkunde Freibauernurkunde wird die Abtrennung von Lahn bei Sögel vom Herrschaftsgebiet der Grafen von Tecklenburg an das Oberstift Münster bekundet. Nach der Einteilung des Heiligen Römischen Reiches in zehn Reichskreise gehörte ab dem 16. Jahrhundert das Emsland zum Niederrheinisch-Westfälischer Reichskreis. Durch den großen Anteil an Mooren und wenig ertragreichen Geestböden besaß diese Region nur sehr wenig Kulturland, das ackerbaulich bewirtschaftet werden konnte. Deshalb herrschte relativ viel Armut im Emsland.

Der Norden (der frühere Landkreis Aschendorf-Hümmling) und die Mitte (der frühere Landkreis Meppen) des heutigen Landkreises sowie ein schmaler Korridor zwischen den Grafschaften Bentheim und Lingen mit den Orten Lohne, Emsbüren und Salzbergen gehörten zum Hochstift Münster. Die Grafschaft Bentheim und die Vereinigten Niederlande grenzten an das Hochstift Münster westlich der Ems. Nördlich grenzte das Hochstift an die Grafschaften Ostfriesland und Oldenburg. Die Grafschaft Lingen war von den Hochstiften Osnabrück und Münster komplett umringt.
Von Jütland kommend durchquerte die Flämische Straße in Ost-West-Richtung und in Nord-Süd-Richtung die Friesische Straße, die der Ems folgend Westfalen mit der Nordseeküste verband, das Emsland. Entlang der Ems erstreckte sich im westlichen Emsland das Bourtanger Moor.


Im südlichen Emsland entstand 1493 durch die Teilung der Grafschaft Tecklenburg die Grafschaft Lingen. Graf Nikolaus IV. residierte bis 1541 auf der Burg Lingen. 1541 erbte Konrad von Tecklenburg-Schwerin die Grafschaft. Er führte 1543 die Reformation ein. Nach der Niederlage des Schmalkaldischen Bundes musste er im Jahr 1548 Lingen an Kaiser Karl V. abtreten. Durch den Verlust der Regentschaft Konrads wurde Lingen rekatholisiert. Das Lehen wurde seiner Schwester Maria von Ungarn, der Statthalterin der Niederlande, 1550 übertragen. Hiernach trat der Sohn von Karl V., Philipp II., König von Spanien und den Niederlanden, seine Lehnsschaft in Lingen an. Lingen, das sich in spanischer Hand befand, wurde während des Achtzigjährigen Krieges von Prinz Moritz von Oranien in seinem Feldzug von 1597 erobert. Am 19. August 1597/12. November 1597, nach mehrtägiger Belagerung, fiel die Festung. Lingen hatte aufgrund seiner Lage an der Flämischen Straße von Zwolle nach Hamburg sowie der Friesischen Straße von Münster nach Emden hohe militärische Bedeutung in diesem Krieg. Die Grafschaft Lingen wurde calvinistisch. 1605 wurde Lingen vom spanischen Feldherrn Ambrosio Spinola zurückerobert und rekatholisiert. In dieser Zeit bestand nur die Möglichkeit, in die Provinz Groningen über die Dieler Schanze vorzurücken, die aber sehr stark befestigt war. Während des dreißigjährigen Krieges rückte die Garnison aus Lingen am 11. Februar 1624 über die gefroren Moorflächen in Hesepe und Haren unter dem Kommando Lucas Kairos gegen Ter Apel in der Provinz Groningen vor. Nach heftigen Kämpfen wurden die Spanier bis nach Groß Fullen bei Meppen zurückgedrängt. Durch den Verlust der Festungen Oldenzaal und Wesel zogen sich 1630 die Spanier aus Lingen sowie aus allen deutschen Stellungen nördlich des Rheins zurück. Nach der Übergabe der Festung Lingen an die Kaiserlichen wurde sie 1632 geschleift. Lingen ging erneut Anfang 1633 in den Besitz der Oranier über und wurde wieder calvinistisch. In der angrenzenden Grafschaft Bentheim tobte während des dreißigjährigen Krieges auch von 1618 bis 1648 der Krieg. Die Grafschaft Bentheim war zwar neutrales Land, die Bevölkerung hatte jedoch trotzdem stark zu leiden. Münsterische, lüneburgerische, hessische und schwedische Truppen bezogen hier Quartier und erpressten Verpflegung und die letzte Habe.


Im nördlichen Emsland, während der Regentschaft des Fürstbischofs Franz von Waldeck, erhielt ab 1543 die Reformation Aufwind. Als Fürstbischof unterstützte Franz von Waldeck die Reformation in seinem Bistum. Er beschloss zum Beispiel 1543 die Osnabrücker Kirchenordnung, eine evangelische Kirchenordnung für die Landkirchen des Hochstifts Osnabrück. Auch militärisch unterstützte Franz von Waldeck die Protestanten gegen den noch katholischen Herzog Heinrich von Braunschweig-Wolfenbüttel. Waldeck suchte sogar um die Aufnahme in den Schmalkaldischen Bund nach. Dies hatte jedoch keinen Erfolg, da der Bund die endgültige Durchsetzung der Reformation zur Bedingung machte. Nach der Schlacht bei Mühlberg und der Niederlage des Schmalkaldischen Bundes sah sich Waldeck gezwungen, seine Reformationspläne aufzugeben. Im Hochstift Münster und Osnabrück war die Reformation indes so weit fortgeschritten, dass Rekatholisierungsansätze zum Scheitern verurteilt waren. Im September 1587, während des Achtzigjährigen Krieges zwischen der Utrechter Union und der spanischen Linie der Habsburger, drangen die Niederländer von Zwolle kommend über die Ems, um das Niederstift Münster und die Festung Meppen zu okkupieren. Meppen fiel am 11. September 1587 in die Hände der Vereinigten Niederlande. Im Oktober des gleichen Jahres konnte Meppen von spanischen Truppen für das Hochstift Münster zurückerobert werden, ebenso wie Lengerich im südlichen Emsland. Aufgrund der Tatsache, dass beide Kriegsparteien in den ausgebluteten Niederlanden kaum noch Nahrung fanden, weitete sich der Krieg auf das angrenzende Emsland endgültig aus. Zu besonders starken Plünderungen des Emslandes kam es bei Gefechten 1591 und 1592. Im Jahre 1605 versuchten die Oranier zudem durch die Sperrung der Ems oberhalb von Meppen das Emsland unter ihre Kontrolle zu bringen, was aber misslang.

Als Ferdinand von Bayern 1612 das Amt des Bischofs von Münsters antrat, leitete er im Hochstift Münster die Gegenreform ein. Im Jahr 1614 erließ er eine Religionsordnung, die das Bürgerrecht und öffentliche Ämter nur für Katholiken vorsah. Er förderte die Jesuiten, Kapuziner und andere neue Orden. Während des dreißigjährigen Krieges von 1618 bis 1648 wurde durch die protestantischen Mansfelder Truppen die Festung Meppen am 8. November 1622 vom Grafen zu Guthrum ohne Schwertstreich eingenommen. Die Festung Meppen war durch die natürliche Lage gut geschützt und hatte eine hohe strategische Bedeutung für die kriegsführenden Parteien. Nach der Niederlage Christians in der Schlacht bei Stadtlohn am 6. August 1623 zogen die kaiserlichen unter Graf Tilly in Begleitung des Grafen von Anholt in Richtung Meppen, um die Mansfelder von der Ems und aus den Grenzen Ostfrieslands zu verdrängen. Die Mansfelder räumten daraufhin Meppen am 12. August 1624 und begaben sich nach Greetsiel. Nach der Einnahme wurde in Meppen am 28. September 1624 der Reformer Melchior Balthasar hingerichtet.

Nach dem Abzug der tillischen Truppen aus dem nördlichen Emsland übernahmen die bischöfliche Freischar die Festung Meppen.
1626 wurde der Hümmling vom protestantischen Heerführer Bernhard von Sachsen-Weimar, der im dänischen Dienst stand, gebrandschatzt, bis dieser durch den Grafen von Anholt zurückgewiesen werden konnte. Unter dem Kommando von Knaphausen konnten das nördliche Emsland und die Festung Meppen durch schwedische Tuppen, nach der Schlacht von Lützen am 6. November 1632, eingenommen werden. Das nördliche Emsland wurde wieder protestantisch. Im Oktober 1635 drangen hierauf kaiserliche Truppen in das Emsland ein und eroberten am 10. Oktober 1635 die Stadt Haselünne. Achtzehn Tage später fiel Cloppenburg in die Hände der kaiserlichen, sowie Fürstenau, Quakenbrück, Vechta, Wildeshausen und Friesoythe. Hiernach zogen die Kaiserlichen in Richtung der Ems. Sie versuchten bei Aschendorf die Stadt Nienhus einzunehmen, um die Kontrolle über die Ems zu erlangen, was misslang. Im Januar 1636 konnte Haselünne wieder zurückerobert werden. Am 12. August 1637 überschritten französische und hessische Truppen die Ems bei Rhede und Aschendorf und nahmen die von Kaiserlichen gehaltene Dieler Schanze ein. Zu dieser Zeit war das die einzige Möglichkeit, um vom Emsland nach Ostfriesland zu gelangen. Gemäß den Bestimmungen des Westfälischen Friedensvertrags und der Beschlüsse auf dem Reichstag zu Nürnberg von 1650 wurde den Menschen im Hochstift Osnabrück mit der Immerwährenden Kapitulation "(Capitulatio perpetua osnabrugensis)" die freie Religionsausübung zugesichert. Aus der kaiserlichen Armee des Dreißigjährigen Krieges in Westfalen hatte Fürstbischof Christoph Bernhard von Galen 1651 Soldaten für sein Gebiet rekrutiert, um sich gegen die protestantischen Nachbarn zu behaupten. Vor allem die expandierenden Niederländer waren Galen ein Dorn im Auge. Während dieser Zeit wurde auch die Befestigung der Stadt Meppen weiter verstärkt.

Die Grafschaft Lingen blieb bis zum Tod von Wilhelm III. Teil der Niederlande. Am 25. März 1702 übernahm Preußen durch einen unblutigen Handstreich die Regentschaft über Lingen. Der Geheimrat Thomas Ernst Danckelman ließ in Absprache mit König Friedrich I. königlich-preußische Embleme anbringen und die Beamten auf das neue Herrscherhaus vereidigen.

Zudem erfolgte durch den Frieden von Lunéville die territoriale Neuordnung des Hochstifts Münster. Nach dem Reichsdeputationshauptschluss endete die weltliche Herrschaft des Bischofs von Münster über weite Teile des Emslandes endgültig.
Durch die Besetzung des Hochstifts Münster von preußische Truppen wurde das Emsland 1802 säkularisiert. Das südliche Emsland gehörte nach der Säkularisation zum Erbfürstentum Münster und das nördliche Emsland zum Herzogtum Arenberg-Meppen.

Nach dem Frieden von Tilsit 1807 wurde das Erbfürstentum Münster von den Franzosen besetzt und 1808 wurde das Emsland Teil des Großherzogtums Berg. Durch die Annexion nordwestdeutscher Gebiete am 1. Januar 1811 durch das napoleonischen Kaiserreich wurde das Emsland Teil des französischen Kaiserreichs. Es gehörte zum Département de l’Ems-Supérieur (Departement Ober-Ems) und Département de la Lippe (Departement Lippe). Das napoleonische Kaiserreich brachte die Errungenschaften der Französischen Revolution auch in das Emsland, den Liberalismus: Rechtsgleichheit, die Reisefreiheit, die Gewerbefreiheit, die Trennung von Kirche und Staat, den Schutz des Privateigentums und den Code civil („Code Napoléon“).

Im November 1813 endete die französische Herrschaft über das Emsland. 1814 fiel die Grafschaft Lingen wieder an Preußen, aber durch den Verzicht Preußens auf die Niedere Grafschaft Lingen wurde Lingen Teil des auf dem Wiener Kongress gegründeten Königreichs Hannover.

 Seit 1814 gehörte das Emsland zum Königreich Hannover. Mit diesem fiel es 1866 an Preußen.

Im vornehmlich katholisch geprägten Emsland war die NSDAP bis zur „Machtergreifung“ 1933 eine Außenseiterpartei. Sie mussten mit wenigen Stützpunkten auskommen, und wenn es Veranstaltungen der NSDAP gab, stammten die Redner und Vorsitzenden meistens nicht aus der Region. Die ersten NSDAP-Stützpunkte wurden aus der Grafschaft Bentheim, Ostfriesland und dem Osnabrücker Land gegründet oder mitbetreut. Durch den Kampf der Zentrumspresse, der katholischen Kirche und übrigen Parteien gegen sie vermochte die NSDAP vor 1933 lediglich in einigen wenigen Ortschaften, in denen ausgeprägte lokale Konflikte bestanden, so in Aschendorf und Haselünne, sowie in den Städten Fuß zu fassen, ohne aber an ihren reichsweiten Resultaten anknüpfen zu können.

Vor dem Zweiten Weltkrieg wurde der Reichsarbeitsdienst im Emsland für verschiedene Tätigkeiten eingesetzt. Ein bedeutender Schwerpunkt war der – allerdings wenig effektive – Einsatz zur Urbarmachung der riesigen Moor- und Heidefläche (Emslandkultivierung), auf der im Rahmen der Autarkiepolitik neue Höfe entstehen sollten. Im Rahmen der Emslandkultivierung erfolgte auch die Errichtung mehrerer Lager im Emsland. Die drei ersten Lager waren das KZ Neusustrum, das KZ Börgermoor und das KZ Esterwegen. Sie wurden 1933 für politische „Schutzhäftlinge“ errichtet. Einer der bekanntesten Schutzhäftlinge im KZ Esterwegen war der Friedensnobelpreisträger Carl von Ossietzky. Ein weiterer Schwerpunkt war der Bau des Seitenkanals Gleesen-Papenburg.

Ab 1945 planten die Niederlande, große Gebietsteile entlang der deutsch-niederländischen Grenze zu annektieren. Dies wurde als eine Möglichkeit der Kriegsreparation neben Geldzahlungen und dem Überlassen von Arbeitskräften in Betracht gezogen. Die Gebiete, die nach dem Bakker-Schuts-Plan hätten annektiert werden sollen, hatten den gesamten Landkreise Emsland umfasst. Das Emsland war nach dem Zweiten Weltkrieg Teil der britischen Besatzungszone. Die Ausnahme bildete Haren im Zentrum des Emslandes zwischen Meppen und Papenburg, das von 1945 bis 1948 unter polnischer Besatzung stand. Am 10. September 1948 verließen die letzten Polen Haren.

Seit 1946 gehört der jetzige Landkreis zum Bundesland Niedersachsen.

Im Emsland wurde durch die Inbetriebnahme des Kernkraftwerkes Lingen das Atomzeitalter eingeleitet. Das Kernkraftwerk war eines der ersten kommerziellen Kernkraftwerke in Deutschland. Bereits im Jahr 1977 wurde der nukleare Teil des Kraftwerks nach einem Schaden im Dampfumformersystem stillgelegt. Gegen die Errichtung eines Endlagers und einer Wiederaufbereitungsanlage (WAA) für radioaktiven Abfall fand am 29. November 1976 in Wippingen bei Dörpen eine große Anti-Atom-Demonstration statt. Dies war der Höhepunkt des Widerstandes der Region gegen die Pläne des niedersächsischen Ministerpräsidenten Ernst Albrecht. 1988 wurde als Nachfolger das Kernkraftwerk Emsland (KKE) in Betrieb genommen, welches das zweitjüngste deutsche Atomkraftwerk ist.

Der heutige Landkreis wurde durch die Kreisreform am 1. August 1977 aus den Landkreisen Aschendorf-Hümmling, Meppen und Teilen des Landkreises Lingen gebildet. In vielen Statistiken des Landkreises spiegelt sich diese Aufteilung in Emsland-Nord, Emsland-Mitte und Emsland-Süd wider.

Die Einwohnerzahlen vor 1977 beziehen sich auf das Gebiet der früheren Landkreise Aschendorf-Hümmling, Meppen und Lingen.

Sozialversicherungspflichtige Beschäftigte:

Arbeitslose (November 2017):

Der Landkreis Emsland gehört wie der Landkreis Vechta, der Landkreis Cloppenburg, die ehemaligen Stifte Osnabrück und Hildesheim und das Untereichsfeld zu den Regionen in Niedersachsen, die traditionell von der katholischen Kirche geprägt sind. Der Anteil der Katholiken beträgt nach wie vor etwa 80 %.

Zum Nachfolger des ersten hauptamtlichen Landrates Hermann Bröring (CDU) wurde der bisherige Erste Kreisrat Reinhard Winter (CDU) gewählt. Er erhielt bei der Kommunalwahl am 11. September 2011 68,02 % der Stimmen. Der Landrat gehört automatisch auch dem Kreistag an.

Landräte

ehrenamtliche Landräte

hauptamtliche Landräte

Oberkreisdirektoren


Der Kreistag hat 66 gewählte Mitglieder. Hinzu kommt der direkt gewählte hauptamtliche Landrat. Ihm gehören seit der Kommunalwahl am 11. September 2011 sechs Parteien bzw. Wählergemeinschaften an:

Das Wappen des Landkreises ist dreigeteilt. Im oberen Drittel ist ein graues Hünengrab auf rotem Untergrund dargestellt, es steht für den Nordteil des Landkreises und bezieht sich auf das Wappen des ehemaligen Landkreises Aschendorf-Hümmling. In der Mitte sind drei rote Mispelblüten auf goldenem Untergrund, sie stehen für den Zusammenschluss der drei ehemaligen Kreise und gehen auf das Wappen des ehemaligen Kreises Meppen zurück. Ein Wellenschnitt zwischen dem mittleren und dem unteren Bereich des Wappens steht für die Ems. Die Mispelblüten gehen auf das Wappen der Herzöge von Arenberg zurück, die 1803 Landesherren des vorher zum Fürstbistum Münster gehörigen Amtes Meppen wurden. Der goldene Anker auf blauem Untergrund im unteren Drittel des emsländischen Wappens geht auf das Wappen des ehemaligen Kreises Lingen zurück.

Seit dem Jahr 2004 verbindet eine Partnerschaft den Landkreis Emsland mit dem polnischen Landkreis (Powiat) Lidzbarski (dt. "Heilsberg in Ostpreußen") in der Woiwodschaft Ermland-Masuren. Diese Partnerschaft knüpft an die Patenschaft an, die der Landkreis Aschendorf-Hümmling 1954 für den Kreis Heilsberg übernommen hatte. In Werlte wurde die Heimatstube der Kreisgemeinschaft Heilsberg eingerichtet.

Zur Pflege kultureller Einrichtungen wurde die Emsländische Landschaft als eingetragener Verein gegründet.




Im Landkreis Emsland gibt es neun jüdische Friedhöfe: in Freren, Haren, Haselünne, Herzlake, Lathen, Lingen, Meppen, Papenburg und in Sögel. Es sind geschützte Kulturdenkmäler – steinerne Zeugen für ehemals existierende jüdische Gemeinden und eines regen jüdischen Gemeindelebens bis in die 1930er Jahre. Die Friedhöfe sind oft schlecht aufzufinden, zumal sie sich häufig am Rande der Gemeinden befinden.

Seit 1987 verleiht der Landkreis Emsland die "Emsland-Medaille" an Personen, die sich in außergewöhnlichem Maße um den Landkreis Emsland und seine Bewohner verdient gemacht haben. Die Zahl der lebenden Medaillenträger wurde auf 20 begrenzt. Neue Medaillen können nur vergeben werden, wenn ein vorheriger Preisträger verstorben ist.

Im Landkreis gibt es insgesamt 12 Gymnasien, diese sind in Handrup, Haren, Haselünne, Lingen, Meppen, Dörpen, Papenburg, Sögel und Werlte.

Ab 1988 existierte in Lingen die Berufsakademie Emsland. Sie war die älteste Berufsakademie Niedersachsens und bot drei Studiengänge (Betriebswirtschaft, Wirtschaftsinformatik und Wirtschaftsingenieurwesen) an. Die Berufsakademie Emsland wurde 2011 in die Fachhochschule Osnabrück integriert.

Im Jahr 2000 wurde in Lingen eine Außenstelle der Fachhochschule Osnabrück gegründet, die am Campus Lingen die Fakultät Management, Kultur und Technik mit den Instituten Management und Technik, Duale Studiengänge, Kommunikationsmanagement sowie Theaterpädagogik umfasst.

Im Zukunftsatlas 2016 belegte der Landkreis Emsland Platz 142 von 402 Landkreisen, Kommunalverbänden und kreisfreien Städten in Deutschland und zählt damit zu den Regionen mit „Zukunftschancen“.

Ursprünglich ist der Landkreis durch die Landwirtschaft und durch den Torfabbau geprägt. Heute sind im Maschinen-, Motoren- und Fahrzeugbau, der Papiererzeugung (UPM Nordland Papier in Dörpen), der Holzwerkstoffindustrie (Glunz AG in Meppen), dem Schiffbau (Meyer-Werft in Papenburg), der Erdöl- und Erdgasindustrie (GDF Suez in Lingen), der Ernährungswirtschaft und im Baugewerbe viele Arbeitskräfte beschäftigt. Die größte Privatforstverwaltung Niedersachsens (Arenberg-Meppen GmbH in Meppen) bewirtschaftet hier ihre Wälder. Neben diesen Branchen nimmt der Tourismus (beispielsweise sei hier das Ferienzentrum Schloss Dankern in Haren genannt) einen immer größeren Stellenwert ein.

Hauptverkehrsweg ist seit alters her die so genannte Emsachse, die den Landkreis von Nord nach Süd durchquert.

Parallel westlich zu dieser Achse verläuft die 2004 fertiggestellte Bundesautobahn 31 und östlich der Ems die Bundesstraße 70.

Nach Westen zu den Niederlanden hat es lange keine günstige Verkehrsverbindung gegeben, da das Bourtanger Moor eine kaum zu überwindende natürliche Grenze darstellte. Heute gibt es eine West-Ost-Verbindung von der deutsch-niederländischen Grenze bei Twist-Hebelermeer zur Bundesautobahn 1 bei Cloppenburg durch die Bundesstraßen 402, 213 und 72, die als E 233 heute die kürzeste Verbindung zwischen dem Raum Amsterdam und dem Raum Hamburg für den Autoverkehr darstellt. 2005 wurde der vierspurige Ausbau beschlossen und zwischen der A 31 sowie der niederländischen Grenze sind die Bauarbeiten abgeschlossen (Stand: 2008). Der Süden des Landkreises ist an die Bundesautobahn 30 (durchgängig befahrbar seit 1991) angebunden. Die B 401 läuft parallel zum Küstenkanal von der A 31 bei Heede/Dörpen nach Oldenburg, die B 408 von der B 70 an Haren vorbei bis zur niederländischen Grenze. In Salzbergen, an der Südspitze des Landkreises, verlief die B 65, die durch die Fertigstellung der A 30 zur Landesstraße herabgestuft wurde. Ein weiterer Knotenpunkt befindet sich in Lingen; hier treffen die Bundesstraßen B 70, B 213 und B 214 aufeinander. Die B 214 verbindet Lingen und das südliche Emsland mit der Region Hannover/Braunschweig.

Die Hauptachse des Schienennetzes ist die Emslandstrecke Leer (Ostfriesland)–Papenburg–Meppen–Lingen–Rheine, die 1856 durch die Königlich Hannöverschen Staatseisenbahnen eröffnet wurde. In Salzbergen mündet seit 1865 die Bahnstrecke Almelo–Salzbergen ein. Die Rheinische Eisenbahn-Gesellschaft führte 1879 ihre Strecke Rheine–Freren–Quakenbrück durch den Südostzipfel des Landkreises.

Der Landkreis betreibt in eigener Regie die Emsländische Eisenbahn GmbH, die aus den Kreisbahnen der Kreise Hümmling und Meppen entstanden ist: Die 1894 eröffnete Meppen-Haselünner Eisenbahn wurde 1902 bis Herzlake und 1907 zur oldenburgischen Staatsbahn in Lewinghausen verlängert. Im Jahr 1979 wurde auch die vormalige Bundesbahnstrecke bis nach Essen (Oldenburg) übernommen. Die Hümmlinger Kreisbahn von Lathen nach Werlte entstand 1898 als Schmalspurbahn und berührte auch die damalige Kreisstadt Sögel. Sie wurde 1957 auf Normalspur umgebaut.

Die Kleinbahn Lingen–Berge–Quakenbrück stellte ab 1904 eine weitere (schmalspurige) Querverbindung von der Ems in Richtung Oldenburgisches Münsterland her.

Angesichts der großen Fläche des Landkreises war die Bahndichte mit 209 km ziemlich gering. Trotzdem wurde fast die Hälfte des Netzes (etwa 100 km) zumindest im Personenverkehr stillgelegt:

Planmäßiger Personenverkehr findet nur noch auf der Emslandstrecke (Kursbuchstrecke 395 der Deutschen Bahn) sowie der Verbindung von Salzbergen nach Bad Bentheim (Teil der Kursbuchstrecke 375) statt. Alle zwei Stunden verkehren Intercity-Züge auf den Verbindungen (Norddeich Mole–)Papenburg–Meppen–Lingen(–Koblenz) und Amsterdam–Berlin, letztere jedoch ohne Halt im Kreisgebiet. Im Regionalverkehr gibt es stündliche Verbindungen Salzbergen–Papenburg (RE 15 „Emsland-Express“) und Salzbergen (RB 61 „Wiehengebirgs-Bahn“), jeweils mit Halt an allen dazwischenliegenden Stationen.

Die Strecken von Meppen nach Essen (Oldenburg) und von Lathen nach Werlte werden nach der Stilllegung des planmäßigen Personenverkehrs regelmäßig mit Museumszügen der Vereine "Eisenbahnfreunde Hasetal" bzw. "Museumseisenbahn Hümmlinger Kreisbahn" befahren, außerdem findet dort noch Güterverkehr der Emsländischen Eisenbahn statt.

Die Strecke von Rheine über Spelle und Freren nach Quakenbrück wurde nördlich von Spelle im Jahr 1996 auch im Güterverkehr stillgelegt, bis Spelle verkehren noch Güterzüge der Regionalverkehr Münsterland.

Außerdem befindet sich in Lathen die einzige Transrapid-Versuchsstrecke in Europa. Hier wurden im Testbetrieb Geschwindigkeiten von über 500 km/h erreicht. 2006 ereignete sich dort der bislang erste Unfall mit einem Transrapid. Der vollbesetzte Wagen fuhr mit ca. 150 km/h auf einen Werkstattwagen auf. 23 Menschen starben, nur wenige der Passagiere überlebten.

Der einzige Seehafen des Landkreises liegt in der Stadt Papenburg, die Ems ist von Papenburg bis zur Mündung als Seewasserstraße ausgewiesen.
Die kanalisierte Ems und ab Meppen der Dortmund-Ems-Kanal verbinden das Ruhrgebiet mit der Nordsee. Flussaufwärts wird die Ems ab Meppen nur noch im Freizeitverkehr befahren. Bereits seit 1829 umfährt der Güterverkehr die zahlreichen Mäander der Ems zwischen Lingen und Meppen auf dem ehemaligen Ems-Hase-Kanal, der seit 1899 überwiegend in den Dortmund-Ems-Kanal einbezogen ist. Bei Dörpen zweigt der Küstenkanal von der Ems ab und verbindet sie mit der Weser. Der Ems-Vechte-Kanal verbindet die Ems mit der Vechte, dieser ist aber nur für Sportboote bis zu einer Länge von 12 Metern freigegeben. Ferner gibt es den Haren-Rütenbrock-Kanal.

Zwischen Haren und Meppen wurde der Eurohafen Emsland als Stichhafen gebaut. Die ersten Unternehmen haben sich dort bereits angesiedelt.

Für die Erschließung des Bourtanger Moores hatte das zwischen 1870 und 1904 erbaute linksemsische Kanalnetz eine zentrale Bedeutung. Neben dem Gütertransport diente es vor allem der Entwässerung.

Der Landkreis Emsland ist Mitgesellschafter der Regionalflughäfen Klausheide (Grafschaft Bentheim) und Leer (Ostfriesland), sowie des Flughafens Münster/Osnabrück.

Die nächsten internationalen Flughäfen sind der Flughafen Münster/Osnabrück in Greven, der Flughafen Bremen und in den Niederlanden der Flughafen Eelde in Groningen.

In Klammern die Einwohnerzahl am .

Einheitsgemeinden


Mitgliedsgemeinden der Samtgemeinden

<nowiki>*</nowiki> Sitz der Samtgemeindeverwaltung
Nach der Bildung des Landkreises Emsland am 1. August 1977 wurden zunächst weiterhin die Kfz-Kennzeichen der Altkreise ausgegeben: "ASD" (für Aschendorf-Hümmling), "LIN" (für Lingen) und "MEP" (für Meppen). Am 5. April 1978 wurde dem Landkreis dann das Kfz-Unterscheidungszeichen "EL" zugewiesen; es wird durchgängig bis heute ausgegeben. Bis in die 1990er Jahre wurden die Kennzeichen in den einzelnen Zulassungsstellen nach folgendem Schema zugeteilt:





</doc>
<doc id="11026" url="https://de.wikipedia.org/wiki?curid=11026" title="Horst Antes">
Horst Antes

Horst Antes (* 28. Oktober 1936 in Heppenheim) ist ein deutscher Maler, Grafiker und Bildhauer.

Horst Antes studierte von 1957 bis 1959 bei HAP Grieshaber an der Akademie der Bildenden Künste in Karlsruhe Malerei. Durch Grieshaber gehörte er zu einer Gruppe von Malern mit eigenständigen Profilen, wie Hans Baschang, Walter Stöhrer und Heinz Schanz. An der Karlsruher Akademie ist er seit 1967 Professor für Malerei und leitet eine Malklasse. 1962 hielt er sich in Florenz (Villa Romana), 1963 in Rom (Villa Massimo) auf, wo der "Kopffüßler" erstmals dreidimensional ausgestellt wurde. 1966 entwarf er Metallplastiken für einen "Garten der sieben Denkmäler der Lüste", die Antes 1967 anlässlich der Bundesgartenschau für den Botanischen Garten in Karlsruhe ausstellte. 1968 erhielt er eine Gastprofessur an der Hochschule für Bildende Künste (heute Universität der Künste) in Berlin. Seit 1983 ist er Mitglied der Freien Akademie der Künste Hamburg.

Ausgehend von der informellen Nachkriegsmalerei suchte Horst Antes als einer der ersten Pioniere der gegenständlichen Malerei nach neuen Möglichkeiten der figurativen Malerei, wobei er in Willem de Kooning, der informelle und figurative Elemente verknüpfte, ein Leitbild fand. Er ist Mitbegründer der neuen figurativen Malerei in Deutschland. Kennzeichnend sind seine ab 1962 entstandenen "Kopffüßler", bei denen Anregungen durch die Kachina-Puppen der Puebloindianer eine Rolle spielten, die lange Zeit sein einziges Motiv waren. Ein "Kopffüßler", eine sogenannte „Kunstfigur“, besitzt keinen Hals, wenig Brust und Bauch und Kopf und Füße scheinen ineinander überzugehen.

Als Mitglied des Deutschen Künstlerbundes nahm Horst Antes zwischen 1961 und 1992 an fünfzehn großen Jahresausstellungen teil. Er war Teilnehmer der documenta III (1964), der 4. documenta (1968), und auch der documenta VI im Jahr 1977 in Kassel. Von 1977 bis 1979 war Antes Mitglied des DKB-Vorstands. Er lebt und arbeitet in Karlsruhe-Wolfartsweier, wo er ein 1970 von dem Architekten Heinz Mohl aus einem ehemaligen Lagerschuppen umgebautes Atelierhaus bewohnt. Zeitweilig ist er auch in Berlin und in Castellina in Chianti in Italien tätig.







</doc>
<doc id="11027" url="https://de.wikipedia.org/wiki?curid=11027" title="Karel Appel">
Karel Appel

Karel Appel (eigentlich "Christiaan Karel Appel"; * 25. April 1921 in Amsterdam; † 3. Mai 2006 in Zürich) war ein niederländischer Maler, Grafiker und Bildhauer. Er war Mitbegründer der Künstlergruppe CoBrA.

Appel verbrachte seine Jugend in einem Amsterdamer Arbeiterviertel, wo sein Vater ein Friseurgeschäft betrieb. Auf Wunsch des Vaters erhielt er eine Friseurausbildung. Nach Streitigkeiten um seinen Berufswunsch als Maler verließ er das Elternhaus und studierte 1940 bis 1944 an der Reichsakademie der bildenden Künste in Amsterdam. 1948 gründete er mit Corneille und Constant, die er während des Studiums kennengelernt hatte, die "Nederlands Experimentele Groep". Im gleichen Jahr gründete er mit weiteren Künstlern die Künstlergruppe CoBrA. 

1946 hatte Appel seine erste Einzelausstellung in "Het Beerenhui"s in Groningen und nahm kurz darauf an der Ausstellung "Jonge Schilders" (Junge Maler) im Stedelijk Museum in Amsterdam teil. In dieser Periode war er stark durch die Kunst von Picasso, Matisse und Dubuffet beeinflusst.
Er schuf das Wandgemälde "Fragende Kinder" in der Kantine des Stadthauses in Amsterdam. Die Empörung der Mitarbeiter und der Bevölkerung über das an Kinderzeichnungen erinnernde Bild führten dazu, dass das Gemälde zehn Jahre lang verhüllt wurde. 
1950 zog Appel nach Paris. Der internationale Durchbruch folgte 1953, als seine Werke auf der Biennale von São Paulo gezeigt wurden. Er wurde dort mit dem Großen Preis der Malerei ausgezeichnet. 1951 entstand ein Fresko für das Stedelijk Museum in Amsterdam, 1959 schuf er ein Wandbild für das UNESCO-Gebäude in Paris. Appel war Teilnehmer der documenta II (1959) und auch der documenta III (1964) in Kassel.

Er versuchte immer wieder, seine Malerei mit anderen Künsten zu verbinden. 1962 arbeitete Appel mit Bert Schiebeek an der Theaterproduktion "Een groot dier" („Ein großes Tier“). So nahm er 1970 mit Merrill Sanders und Chet Baker einige Musikstücke auf. 1987 gestaltete er gemeinsam mit dem Tänzer und Choreografen Min Tanaka unter dem Titel "Peut-on danser le paysage?" ein Tanzprojekt für die Pariser Oper, das seitdem auch an der Brooklyn Academy of Music in New York und an der Nederlandse Opera in Amsterdam gastierte. 1991 arbeitete er gemeinsam mit Allen Ginsberg und Gregory Corso an den „Poetry-Painting Series“, eine Kombination zwischen Poesie und Malerei. Ab 1994 entwickelte er gemeinsam mit Tanaka mehrere Bühnenausstattungen für niederländische Opernhäuser. An der Nederlandse Opera schuf er außerdem 1994 die Ausstattung für die Uraufführung der Oper "Noach" von Guus Janssen und Friso Haverkamp in einer Inszenierung von Pierre Audi. Für die Salzburger Festspiele 2006 entwarf Appel das Bühnenbild für Mozarts "Die Zauberflöte" (Regie Pierre Audi, Dirigent Riccardo Muti).

Um 1990 besaß der Künstler Ateliers in New York, in Connecticut, in Monaco und in der Toskana. Er hielt sich oft in New York auf, aber hatte auch einen Wohnsitz in Florenz und pendelte zwischen den USA und Europa. 

Im Jahre 1999 errichtete Appel eine "Karel-Appel Stiftung" (Karel Appel Foundation), der er sämtlich Rechte aus bis dahin entstandenen oder noch entstehenden Werken übertrug. Diese Verfügung wurde 2005 nochmals in einem in Zürich erstellten Testament bestätigt. In seinen letzten Lebensjahren hielt er sich oft in Zürich auf. Dort verstarb er am 3. Mai 2006. Appel wurde auf dem Friedhof Père Lachaise in Paris beerdigt.

Appel hat ein umfangreiches Werk aus etwa 10.000 Skulpturen, Plastiken, Zeichnungen und Gemälden hinterlassen. Der Künstler ist vor allem mit seinen Gemälden bekannt geworden, erst kürzlich findet auch sein gesamtes Werk Beachtung. Neben der CoBrA war Appel auch deutlich von Picasso, dem Surrealismus und der Art Brut beeinflusst. 

Die Zeit der "Cobra", der avantgardistischen Künstlergruppe aus den 1950er Jahren, hat Appels Stil deutlich geprägt. Der Einfluss findet sich in seinen Gemälden in einer primitiven, groben Darstellungsweise, in einer grellen Farbigkeit und einer kindlich-naiven Malweise wieder. Seine Motive waren stark vereinfachte, anfangs auch grafisch-stilisierte Landschafts-, Tier- und Menschenbildnisse. Nach der "Cobra"-Phase verlieren die Bilder ihre Figuration und der Charakter des Aggressiven und Brutalen verstärkt sich. Er nähert sich dem amerikanischen abstrakten Expressionismus an, versucht sich aber mit einem Stilwechsel davon abzugrenzen. „Abstrakte Malerei ist eine Mode, ein Stilprodukt. Ich verwende immer Formen, die auf sprechenden Gegenständen basieren. Man soll eine Sache als Ganzes betrachten und sich nicht in Details verlieren.“, sagte Appel zu dieser Zeit in einem Interview. Ab den 1970er Jahren wurden seine Arbeiten energischer und roher, bis er in den 1980er Jahren wieder zu einer figurativen Darstellungsweise zurückkehrt. Seine Bilder werden düster und dunkel, die grelle Farbigkeit verschwindet fast völlig, um in seiner Spätphase, Ende der 1980er Jahre, wiederzukehren. 

Für seine Skulpturen benutzte er die Assemblage-Technik. Er collagierte verschiedene Materialien, Gegenstände und Muster, die er in der Spätphase auch mit Acrylfarben bearbeitete.

Werke von Karel Appel sind u. a. im Cobra Museum zu besichtigen.








</doc>
<doc id="11028" url="https://de.wikipedia.org/wiki?curid=11028" title="Hans Arp">
Hans Arp

Hans oder Jean Arp (* 16. September 1886 in Straßburg als "Hans Peter Wilhelm Arp"; † 7. Juni 1966 in Basel) war ein deutsch-französischer Maler, Graphiker, Bildhauer und Lyriker. 

Er bewegte sich in den künstlerischen Kreisen der Konstruktivisten und den Pariser Surrealisten, wobei er 1916 den Dadaismus als eine literarische und künstlerische Bewegung als Antwort auf den Ersten Weltkrieg und gegen dessen soziale Konventionen in Zürich mitbegründete. Besonders eng arbeitete Arp mit seiner Frau Sophie Taeuber-Arp und zeitweise mit weiteren Künstlern, wie dem Konstruktivisten El Lissitzky, Max Ernst oder Kurt Schwitters. 1930 wurde er Mitglied der Gruppe Cercle et Carré und ein Jahr Mitbegründer der neuen abstrakten Pariser Künstlergruppierung Abstraction-Création. 

Arps Œuvre ist vom dadaistischen Prinzip des Zufalls und ab den 1920er-Jahren von einer "Objektsprache" des Alltäglichen geprägt. Besonders charakteristisch ist seine Auseinandersetzung mit "biomorphen", naturnahen, gerundeten Formen, die sein Werk bis heute unverkennbar machen.

Hans Arp entstammt väterlicherseits einer Hugenottenfamilie aus der holsteinischen Probstei und mütterlicherseits einer elsässisch-französischen Familie. Sein Vater Jürgen Peter Wilhelm Arp, 1853 in Kiel geboren, zog 1877 ins damals zum Deutschen Reich gehörende Straßburg, wo er 1880 Marie Joséphine Koeberlé heiratete. „Joe“, Tochter eines Tapeziermeisters aus Oberschäffolsheim, wurde 1857 in Straßburg geboren, ihre Familie mütterlicherseits stammt aus dem burgundischen Tournus. Der Vater von Hans besaß eine prosperierende Zigarrenfabrik – schon früh lernte Hans die für seine Kunst höchst charakteristischen runden Formen in Gestalt von Rauchschwaden kennen. Seine Mutter war eine talentierte Pianistin und Sängerin. Hans und sein 1891 geborener Bruder Wilhelm Franz Philipp, Willie genannt, wuchsen dreisprachig auf; mit ihrer Mutter sprachen sie Französisch, mit ihrem Vater und in der Schule Deutsch, im außerhäuslichen und außerschulischen Alltag Elsässisch, wobei der Elsässer Akzent auch auf die beiden anderen Sprachen abfärbte. In seiner Jugend interessierte sich Hans vor allem für die Dichter der deutschen Romantik wie Novalis, Clemens Brentano und Ludwig Tieck sowie für französische Dichter wie Arthur Rimbaud und Comte de Lautréamont.

Von 1904 bis 1908 studierte Arp Bildende Kunst an der Kunstschule Weimar und an der Académie Julian in Paris, die er wegen ihrer konventionellen Lehrmethoden enttäuscht verließ. Ab 1909 lebte er im schweizerischen Kanton Luzern, da sein Vater 1907 seine Fabrik in das nahegelegene Weggis verlegt hatte. 1911 wurde er Mitbegründer der Künstlervereinigung "Moderner Bund". Er lernte Wassily Kandinsky kennen und knüpfte über ihn Kontakte zur Gruppe "Der Blaue Reiter".

1915 wurden Arps abstrakte Werke erstmals in Zürich ausgestellt. 1916 illustrierte er Tristan Tzaras Lyrikband "25 Gedichte". Über Tzara lernte er Hugo Ball, Emmy Hennings, Marcel Janco und Richard Huelsenbeck kennen, mit denen er 1916 den Dadaismus in Zürich begründete. Ab 1916 war er mit der Künstlerin und Textilgestalterin Sophie Taeuber befreundet. Sie begannen, sich bezüglich der Erneuerung der Kunst auszutauschen und zusammenzuarbeiten. Arp führte Taeuber in den Kreis der Dadaisten ein, an deren Veranstaltungen sie sich aktiv beteiligte. 1919 zog Hans Arp nach Köln und schloss Freundschaft mit Max Ernst und Johannes Theodor Baargeld. Mit diesen begründete er den Kölner Dadaismus; sie gaben gemeinsam die marxistisch orientierte Zeitschrift "Der Ventilator" heraus. 1920 nahm Arp an der Ersten Internationalen Dada-Messe in der Berliner Galerie "Otto Burchard" teil und veröffentlichte auf Vermittlung von Kurt Schwitters den Gedichtband "Die Wolkenpumpe", dessen Gedichte Arp als "Textcollagen" bezeichnete. In ihnen war der Zufall ein wesentliches Gestaltungsprinzip.

1922 heiratete Arp Sophie Taeuber. Einzeln und zusammen schufen sie viele Werke. 1923 begann Arp eine engere Zusammenarbeit mit Schwitters. 1923 nahm Hans Arp in Paris an einer Gruppenausstellung der Surrealisten teil. 1925 mietete er ein Atelier in Paris, das auch Sophie Taeuber-Arp manchmal nutzte. Die Arps wurden Mitglieder der Künstlerbewegung "Cercle et Carré", später der Nachfolge-Organisation "Abstraction-Création". Arp hatte engen Kontakt mit internationalen Avantgardisten wie Kasimir Malewitsch und El Lissitzky. Zusammen mit Lissitzky veröffentlichte er 1925 das Buch "Die Kunstismen".

1926 zogen die Arps nach Straßburg. Sie luden den holländischen Künstler und Architekten Theo van Doesburg ein, am "Aubette-Projekt" in Straßburg mitzuarbeiten – es ging um die Gestaltung der Innendekoration eines großen Lokals mit Bar, Café, Salon usw. Auch übte das Ehepaar Arp einen wichtigen Einfluss auf die Stilrichtung des Hard Edge aus. Der abstrakte amerikanische Künstler Ellsworth Kelly hatte sie sehr oft in Paris besucht und die beiden prägten wesentlich seine frühe Entwicklung, eine unpersönliche, nicht-individuelle Kunst machen zu wollen. Im selben Jahr zogen die Arps nach Meudon bei Paris und nahmen dort am 20. Juli 1926 die französische Staatsangehörigkeit an. Ursprünglich Maler und Graphiker, trat Arp seit 1930 vermehrt als Plastiker hervor.

1940 wurden Arps Werke von den Nationalsozialisten als "entartete Kunst" eingestuft. Das Ehepaar Arp zog in den unbesetzten Teil Frankreichs, nach Grasse. Gedichte schrieb er nun vor allem auf Französisch. Er hatte kein Atelier und musste als Maler und Bildhauer notgedrungen mit leichten, transportablen und billigen Materialien arbeiten. So entstanden die "dessins aux doigts (Fingerzeichnungen)" und die "Papiers froissés (Zerknitterten Papiere)". Mit Zuwendungen von Maja Sacher, Erika Schlegel und anderen Gönnern wurden Arps über Wasser gehalten. Ende 1942 flohen sie vor der anrückenden deutschen Wehrmacht in die Schweiz.

Sophie Taeuber-Arp starb in der Nacht zum 13. Januar 1943 im Haus von Max Bill in Zürich an einer Kohlenmonoxidvergiftung. Arp brauchte Jahre, um sich von diesem Verlust zu erholen, und widmete Sophie viele seiner Werke. Zusammen mit Georg Schmidt arbeitete er an einer Monografie über ihr Werk. 1949 reiste Arp in die USA, wo seine Kunst dank der Hilfe des Galeristen Curt Valentin zunehmend Erfolg hatte. Da die Mehrzahl seiner Käufer nun dort lebte, überlegte Arp, ob er emigrieren sollte; er entschied sich aber am Ende dagegen.

Ab 1950 entwarf Arp mehrere Großplastiken für die Universitäten von Harvard und Caracas und das UNESCO-Gebäude in Paris. 1952 reiste Arp nach Rom und Griechenland und bekam dort neue Anregungen für plastische Arbeiten (beispielsweise "Kobra-Kentaur)," für die er auf der Biennale von Venedig 1954 den "Internationalen Preis für Skulptur" erhielt.

Dem nunmehr international erfolgreichen Künstler Arp wurde 1957 die erste umfassende Monografie gewidmet. Im selben Jahr wurde er Mitglied der Deutschen Akademie für Sprache und Dichtung. 1958 veranstaltete das Museum of Modern Art in New York eine umfassende Retrospektive. Arp war Teilnehmer der documenta 1 1955, der documenta II 1959 und der documenta III 1964. Seine Kunst war nun so gefragt, dass er Mitarbeiter beschäftigen konnte.

1959 heiratete Hans Arp seine langjährige Freundin Marguerite Hagenbach. Er starb 1966 im Alter von 79 Jahren in Basel. Sein Grab befindet sich auf dem "Cimitero di Santa Maria in Selva" in Locarno, Kanton Tessin. Im "Museo comunale Casa Rusca" von Locarno findet sich der von Arps zweiter Frau gestiftete Nachlass. Neben Werken des Künstlers selbst umfasst der Nachlass auch Arps private Kunstsammlung.

Wie der deutsche Kunsthistoriker Johannes Jahn schreibt, bewegen sich Arps Werke "in einer eigenartigen Welt zwischen Dadaismus, Surrealismus und Abstraktion. In seinen plastischen Gebilden bemüht es sich, das von innen heraus urhaft Keimende organischer Formen darzustellen." Die Kunsthistorikerin Carola Giedion-Welcker betont wiederum die Relevanz jener Naturvision in den 1930er-Jahren und machte diese bei Arp fest als "Sichtbarmachung eines Unsichtbaren, die Suche nach einer optischen Sprache, welche die geistigen Sphären, jenseits der Welt der Erscheinungen zu erfassen vermag." Arps "Biomorphismus" fand somit in vegetativen Formen den emblematischen Kodex für das Geistige in der Kunst. Juri Steiner erläutert weiter: "Arps plastische Konkretionen in weissem Marmor, Holz, Gips und Bronze beziehen sich auf das Festwerden der Masse im Stein, in der Pflanze, im Tier, im Menschen. Gerinnung, Verhärtung, Verdickung, Zusammenwachsen sind Sinnbilder der ewigen Verwandlung in der Natur. Die Kräfte dieser Prozesse nannte Arp 'tension de sol' oder 'Bodenspannung', in Anlehnung an die unaufhörlichen Naturzyklen. So produzierte auch Arp immer neue Konstellationen, wobei er die Erkenntnis für seine 'bewegten Ovale' nicht nur aus der Naturbeobachtung, sondern auch aus philosophischen Texten von Lao Tse oder Jakob Boehme bezog. Aus der Bipolarisierung von Mann und Frau – Adam und Eva – entwuchs bei Arp wie bei Constantin Brancusi die Auseinandersetzung mit dem Ei als dem Zeugungssymbol schlechthin. Intention war es, den Menschen wieder an seinen angestammten Platz innerhalb der Schöpfung zu stellen. Dabei brachte Arp die dominanten Kunstrichtungen der Zwischenkriegszeit, Surrealismus und Konstruktivismus/Neo-Plastizismus spielerisch unter einen Hut."

Das "Arp Museum Bahnhof Rolandseck" wird betrieben von der "Landes-Stiftung Arp Museum Bahnhof Rolandseck" mit Sitz in Remagen-Rolandseck. Eröffnet wurde es am 29. September 2007. Es präsentiert im Gebäude des Bahnhofs Rolandseck und in einem Neubau von Richard Meier Werke von Hans Arp und Sophie Taeuber-Arp aus dem Besitz des Landes Rheinland-Pfalz und Sonderausstellungen anderer Künstler.

Vor der Eröffnung des Museums kam es zu heftigen Diskussionen, da bei einigen der im Museum gezeigten Objekte, die aus Beständen des Vereins "Stiftung Hans Arp und Sophie Taeuber-Arp e. V." strittig war, ob es sich um von Arp selbst autorisierte Werke oder spätere Nachgüsse und Repliken handelt.

Das Land Rheinland-Pfalz warf dem Verein außerdem Vertragsbruch vor, weil er einige Werke, die für den Ausstellungsbetrieb gedacht waren, verkauft habe. Im Sommer 2008 kündigte das Land die Zusammenarbeit.
Die Fondation Arp befindet sich in dem von Sophie Taeuber 1929 entworfenen ehemaligen Atelierhaus der Arps in Clamart. Sophie Taeuber lebte in diesem Atelierhaus bis zu ihrem Lebensende.

Das Haus mit einer reichen Sammlung von Werken von Hans Arp und Sophie Taueber ist eine Stiftung von Marguerite Hagenbach aus dem Jahr 1976. Im Laufe der Jahre wurde die Sammlung durch weitere Stiftungen erweitert. Bemerkenswert ist Arps Atelier, in dem er die Gipse für die anzufertigenden Güsse herstellte. Ausgestellt sind dort 114 Plastiken und 32 Reliefs, die 1996 vom französischen Zoll beschlagnahmt worden sind.
Nach einer ersten Ausstellung im Centre Pompidou wird diese Sammlung seit Dezember 2006 in Clamart aufbewahrt. Zur Fondation Arp gehört eine Bibliothek.

Die Stiftung mit Sitz im ehemaligen Wohn- und Atelierhaus "Ronco dei Fiori" Hans Arps in Locarno-Solduno wurde 1988 von Marguerite Arp-Hagenbach gegründet. Seit 2000 kooperiert die Fondazione mit der Stiftung Liner in Appenzell. Ziel der Zusammenarbeit ist die Bewahrung der Fondazione Arp in ihrer heutigen Form, regelmäßige Ausstellungen der Werke von Arp und Sophie Taeuber in Appenzell sowie die Förderung ihrer Werke weltweit. Zu dem Haus gehört ein Skulpturengarten.

Die Stiftung Hans Arp und Sophie Taeuber-Arp ist vorläufig noch als Verein organisiert. Sie betreut Teile des Nachlasses von Hans Arp, insbesondere die Rechte an Bronze-Plastiken. Sie war ursprünglich in Remagen-Rolandseck ansässig, zog aber 2013 nach Berlin. Sie publizierte 2012 eine Bestandsaufnahme aller Skulpturen. Diese Liste beantwortet auch die Frage nach der Berechtigung von posthumen Nachgüssen mit einer Gussrechteliste aus dem Jahr 1977, die von Arps zweiter Frau Marguerite Arp-Hagenbach unterzeichnet wurde.

Die Konflikte um die Nachgussrechte hatten zwischen 2008 und 2015 die internationale Ausstellungstätigkeit mit Arps Werken eingestellt, da die Rechte an den Skulpturen als nicht gesichert galten.




Werkausgabe der Gedichte





</doc>
<doc id="11029" url="https://de.wikipedia.org/wiki?curid=11029" title="Gioachino Rossini">
Gioachino Rossini

Gioachino Antonio Rossini (auch Gioacchino) [] (* 29. Februar 1792 in Pesaro, Kirchenstaat, heute Marken; † 13. November 1868 in Passy, Paris) war ein italienischer Komponist. Er gilt als einer der bedeutendsten Opernkomponisten des Belcanto; seine Opern "Il barbiere di Siviglia" („Der Barbier von Sevilla“), "L'italiana in Algeri" („Die Italienerin in Algier“) und "La Cenerentola" („Aschenputtel“) gehören weltweit zum Standardrepertoire der Opernhäuser.

Gioachino Rossini war der einzige Sohn aus der Ehe des Hornisten Giuseppe Rossini (1758–1839) mit der Sängerin Anna Rossini geb. Guidarini (1771–1827). Eigentlich wurde der Sohn noch am Tag seiner Geburt in Pesaro auf den Namen Giovacchino getauft, doch bekannt wurde sein Name ohne „v“, und Rossini selbst schrieb ihn fast durchwegs als Gioachino, weshalb auch diese ungewöhnliche Namensform heute allgemein von der Musikwissenschaft verwendet wird.

Als Kind lernte Rossini Violine und Cembalo zu spielen; außerdem hatte er eine gute Gesangsstimme. Seine Mutter lehnte jedoch energisch den Vorschlag ihres Bruders ab, die Sopranstimme ihres Kindes als Sängerkastrat zu bewahren, wofür ihr Rossini später dankbar war. Als die Familie 1802 nach Lugo zog, machte Gioachino Rossini die Bekanntschaft mit dem wohlhabenden Giuseppe Malerbi, von dem ein nachhaltiger Einfluss ausging. In Malerbis Bibliothek lernte Rossini die Werke von Haydn und Mozart kennen. Am 22. April 1804 hatte der zwölfjährige Rossini gemeinsam mit seiner Mutter einen ersten öffentlichen Auftritt im kommunalen Theater von Imola. Im selben Jahr schrieb Rossini seine erste Komposition für zwei Violinen, Violoncello und Kontrabass, die "Sei sonate a quattro", deren komplette Urfassung erst 1954 im Druck erschien.

1805 zog die Familie nach Bologna, wo Gioachino Rossini als Sänger auftrat. Seit April 1806 besuchte er das "Liceo Musicale". Seine Mitschüler waren Francesco Morlacchi und Gaetano Donizetti. Hier erhielt er Unterricht in Komposition sowie Violoncello, Horn, Klavier und Gesang. 1810 verließ Rossini das Liceo ohne Abschluss und ging nach Venedig. Zu diesem Zeitpunkt hatte er mit "Demetrio e Polibio" bereits seine erste Oper sowie einige weitere Stücke komponiert. Für seine Leistungen als Sänger wurde er durch die Aufnahme in die Accademia Filarmonica di Bologna geehrt. In Venedig trat der zwanzigjährige Rossini mit der Uraufführung der Oper "La cambiale di matrimonio" am 3. November 1812 erstmals als Komponist an die Öffentlichkeit.

In den folgenden Jahren schrieb Rossini mehrere Opern, die jedoch noch nicht sonderlich bekannt wurden. Erst mit der Opera seria "Tancredi" hatte er im Februar 1813 seinen ersten durchschlagenden Erfolg. Die Uraufführung erfolgte in Venedig am Teatro La Fenice, mit der Altistin Adelaide Malanotte in der Titelrolle; ihre Auftrittsarie "Di tanti palpiti" wurde so berühmt, dass sie sogar von den Gondolieri gesungen wurde, und Niccolò Paganini schrieb darüber Variationen (Op. 13) für Violine und Orchester. Nur wenige Monate später, am 22. Mai 1813, erlebte auch Rossinis Opera buffa "L’italiana in Algeri" einen rauschenden Erfolg, ebenfalls in Venedig, aber am Teatro San Benedetto.

Nach einigen weiteren Opernkompositionen für verschiedene Opernhäuser in Italien wurde Rossini 1815 Leiter der beiden Opernhäuser in Neapel, dem Teatro San Carlo und dem Teatro del Fondo. Das Teatro San Carlo gehörte neben der Mailänder Scala zu den beiden führenden Opernhäusern Italiens, und so boten sich ihm einmalige Möglichkeiten: Es verfügte über ein ungewöhnlich gutes Orchester, und das Sängerensemble bestand aus lauter Virtuosen, wie der Primadonna Isabella Colbran, außerdem die Tenöre Andrea Nozzari, Manuel Garcia, Giovanni David, und der Bass Michele Benedetti. Für dieses außergewöhnliche Ensemble komponierte Rossini eine Reihe von Opere serie, die zu seinen am besten ausgearbeiteten und einfallsreichsten Partituren gehören: "Elisabetta regina d’Inghilterra" (1815), "Otello" (1816), "Armida" (1817), "Mosè in Egitto" (1818), "Ricciardo e Zoraide" (1818), "Ermione" (1819), "La donna del lago" (1819) und "Maometto II" (1820).

Obwohl er vertraglich verpflichtet war, für jedes der beiden neapolitanischen Häuser eine Oper pro Jahr zu schreiben, konnte Rossini daneben auch für andere Städte tätig sein. So komponierte er für die Karnevalssaison 1816 im Teatro Argentina in Rom seinen "Barbiere di Siviglia". Die Uraufführung war ein komplettes Fiasko, aber schon die zweite Aufführung erhielt großen Beifall, und in der gleichen Nacht machte das Publikum einen Fackelzug zu Rossinis Ehren zu seiner Herberge und weckte ihn aus dem Schlaf. Der "Barbier" wurde später und bis heute zu seiner beliebtesten Oper. Auch die Uraufführung der "Cenerentola" im Karneval 1817 im römischen Teatro Valle war zunächst kein Erfolg, erst durch spätere Aufführungen wurde das Werk beliebt. Einige Monate später war Rossini an der Mailänder Scala, wo die Premiere von "La gazza ladra" am 31. Mai 1817 bejubelt wurde.

In Neapel begann Rossini eine Liebschaft mit Isabella Colbran, der Primadonna seiner neapolitanischen Opern, die er schließlich am 16. März 1822 in Castenaso bei Bologna heiratete, wo die Colbran eine Villa besaß. Die Hochzeit fand im kleinsten Kreise in der kleinen Kirche "Vergine del Pilar" statt. Kurz darauf reiste das Ehepaar Rossini und die beiden Tenöre Giovanni David und Andrea Nozzari nach Wien, wo Barbaja eine Rossini-Saison am Kärntnertortheater organisiert hatte. Für diese Tournee hatte Rossini die Opera seria "Zelmira" komponiert, und es wurden außerdem unter anderem seine "Elisabetta, regina d'Inghilterra" und "Ricciardo e Zoraide" gegeben. Es war ein triumphaler Erfolg und ganz Wien lag im berühmten „Rossini-Taumel“. Bei dieser Gelegenheit besuchte Rossini den tauben Beethoven, der sich die Partitur des "Barbier von Sevilla" angesehen, und diejenigen der Seria-Opern "Tancredi", "Otello" und "Mosè" "durchgeblättert" hatte; er riet Rossini, sich ausschließlich auf komische Opern zu beschränken, und fügte hinzu: "...sehen Sie, die ernste Oper liegt nun einmal den Italienern nicht. Um das wahre Drama zu behandeln, haben sie zu geringe musikalische Kenntnisse...".

Rossini und seine Frau kehrten im Spätsommer desselben Jahres zurück nach Italien, wo seine letzte für die Colbran und Italien geschriebene Oper "Semiramide" am 3. Februar 1823 ihre Uraufführung am La Fenice in Venedig erlebte. Die Oper wurde allein bis zum 17. März, dem Tag ihrer Abreise, 28mal wiederholt, und wurde eine seiner beliebtesten Opern (siehe unten).

Ende 1823 ging das Ehepaar Rossini zunächst nach Paris, und einen Monat später für fünf Monate nach London, wo sich die feine Gesellschaft um sie riss. Im Januar 1824 wurde am King’s Theatre seine "Zelmira" aufgeführt. Es war kein Erfolg, aber Rossini wurde mit 7000 Pfund großzügig entlohnt. Ab August des Jahres 1824 waren sie in Paris, wo Rossini den Posten des Leiters der italienischen Oper annahm. Zwei Jahre später wurde er königlicher Hofkomponist und Generalinspekteur des Gesangs in Frankreich. Schon 1825 hatte Rossini zu den Krönungsfeierlichkeiten Karls X. die Oper "Il viaggio a Reims" (Die Reise nach Reims) komponiert, mit einem ungeheuren Staraufgebot an Sängern, zu denen unter anderem Giuditta Pasta, Laure Cinti-Damoreau, Ester Mombelli, Domenico Donzelli und Nicholas-Prosper Levasseur gehörten. Große Teile dieses unwiederholbaren Werkes verwendete er einige Jahre später für seine einzige komische Oper in französischer Sprache "Le comte Ory" (1828). Zuvor hatte er für die Pariser Oper zwei seiner neapolitanischen Seria-Opern zu französischen Grand Opéras umgearbeitet: So wurde aus "Maometto II" (von 1820) "Le siège de Corinthe" (1826), und aus "Mosé in Egitto" (von 1818) "Moïse et Pharaon" (1827). 1829 wurde Rossinis "Guillaume Tell" aufgeführt. Auch diese gehörte zum Genre der Grand Opéra. Es sollte die letzte Oper seines Lebens sein.

Das Jahr 1830 brachte für Rossini den Verlust seiner Ämter, da der französische König im Verlauf der Julirevolution abdanken musste. Es gelang Rossini jedoch, gerichtlich eine lebenslange Rente durchzusetzen.

Von seiner Frau Isabella Colbran lebte Rossini de facto seit 1830 getrennt; sie lebte zusammen mit seinem Vater in Castenaso und Bologna; dieser beschwerte sich häufig über sie in zahlreichen Briefen an Gioachino. Eine offizielle Trennung von Isabella erfolgte 1837. Zu dieser Zeit war er bereits mit seiner neuen Lebensgefährtin, der Französin Olympe Pélissier, zusammen, die er 1832 kennengelernt hatte. Nach dem Tode Isabellas 1845 heiratete er Olympe am 16. August; diese Ehe hielt bis zu seinem Tode. 1839 war auch Rossinis Vater gestorben.

Von 1836 bis 1848 wirkte Rossini in Bologna als Direktor des Musiklyzeums. Er war auch weiterhin zumindest sporadisch als Komponist tätig, widmete sich aber mehr der geistlichen und der Kammermusik. In dieser Zeit entstand sein berühmtes "Stabat Mater", das seine Uraufführung 1842 erlebte, am 7. Januar in Paris, in der Salle Ventadour des Théâtre-Italien, und am 13. März in Bologna unter Leitung von Gaetano Donizetti.

Wegen politischer Unruhen in Bologna floh Rossini 1848 nach Florenz.

Nach seinem Rückzug von der Bühne litt Rossini häufig an Depressionen; er litt außerdem an den Folgen einer Gonorrhoe, die er sich schon in jungen Jahren zugezogen hatte. Eine allgemeine Besserung zumindest seines seelischen Zustandes trat ab 1855 ein, nach seiner Rückkehr nach Paris (Passy), die von seiner Frau Olympe initiiert worden war. Ab 1858 gaben sie sogar jeden Samstag abend Soireen, bei denen auch musiziert wurde, und zu denen die Einladungen heißbegehrt waren.

Ab 1858 entstanden auch zahlreiche, heute meist unbekannte Werke, die sogenannten "Péchés de vieillesse", die „Alterssünden“, die Rossini in 13 Bänden und zwei Supplements sammelte. Darunter sind allein über 100 Klavierstücke, die für ihren Witz bekannt sind. Unter anderem heißen die Stücke "Gefolterter Walzer", "asthmatische Etüde", "chromatischer Drehteller" oder "Fehlgeburt einer Polka-Mazurka". Zu den bekannten und großen Werken nach seiner Zeit als Opernkomponist zählt die "Petite Messe solennelle", die trotz ihres Namens („kleine Messe“) ein neunzigminütiges Werk ist.

Rossini war bekannt für seinen humorvollen, liebenswerten Charakter, selbst im Gespräch mit einem musikalischen Gegner wie Richard Wagner (1860) hatte er noch lauter witzige Bonmots auf der Zunge, und besaß auch Selbstironie. Dies war allerdings teilweise eine Reaktion einerseits auf seine enorme Berühmtheit, und andererseits auf für ihn sicher schmerzliche und einengende Einstufungen als angeblich ausschließlicher Meister der Opera buffa, die er schon von dem tauben (!) Beethoven, aber vor allem in seiner zweiten Lebenshälfte auch sonst manchmal hinnehmen musste. Er war außerdem ein sehr hilfsbereiter Mensch, der jüngere Kollegen (und Konkurrenten) selbstlos unterstützte, so gut er konnte. Das gilt z. B. für Vincenzo Bellini, dem er 1834 einen Auftrag an der Pariser Opéra vermittelte, und danach bei der Arbeit an der Oper "I puritani" mit vielen guten Ratschlägen beistand. Ähnliches gilt auch für Gaetano Donizetti und selbst für Carl Maria von Weber, der sich zwar öffentlich nicht besonders wohlwollend über Rossinis Musik geäußert hatte, dem er aber 1826 aus Hochachtung vor seinem Genie und aus Mitleid wegen seiner tödlichen Schwindsucht, Empfehlungsschreiben zu einflussreichen Bekannten nach London mitgab.

Rossini starb am 13. November 1868 an den Folgen einer Darmoperation. Er wurde zunächst auf dem Pariser Friedhof Père-Lachaise beigesetzt, bevor man seine Gebeine 1887 in die Kirche Santa Croce in Florenz überführte.

Unter dem Eindruck von Rossinis Tod lud Giuseppe Verdi die zwölf bedeutendsten Komponisten Italiens seiner Zeit ein, sich an der Gemeinschaftskomposition einer Totenmesse für Rossini zu beteiligen, die am ersten Todestag aufgeführt werden sollte. Die "Messa per Rossini" wurde 1869 fertiggestellt, eine Aufführung kam jedoch wegen widriger Umstände nicht zustande. Die Gemeinschaftskomposition wurde erstmals 1988 postum aufgeführt. Verdi übernahm seinen eigenen Beitrag, das abschließende "Libera me", als Keimzelle für die Komposition seines eigenen "Requiems". Auch der Florentiner Musiker Guido Tacchinardi komponierte zur gleichen Zeit sein "Requiem a Rossini", das erst 2014 veröffentlicht wurde.

Der italienische Staat hat Rossinis Geburtshaus in Pesaro, das heute ein Museum beherbergt, zugleich mit den Geburtshäusern von Giacomo Puccini und Giuseppe Verdi, mit dem Europäischen Kulturerbe-Siegel ausgezeichnet. Rossini wurde vielfach geehrt, darunter durch seine Aufnahme als auswärtiges Mitglied in die Académie des Beaux-Arts (1823) und als ausländisches Mitglied in den preußischen Orden Pour le Mérite für Wissenschaft und Künste am 31. Mai 1842. Im Opernhaus seiner Geburtsstadt Pesaro und in der Scala von Mailand standen bereits um 1840 Büsten von Rossini, und in Bologna wurde sein Namenstag zum offiziellen Festtag erklärt.

Insgesamt hat Rossini bis 1829 in knapp zwei Jahrzehnten 39 Opern verfasst, die halb Europa in einen wahren Rossini-Rausch versetzten. Die meisten entstanden, wie damals üblich, unter enormem Zeitdruck und in unglaublich kurzer Zeit. Rossini selber erzählte Hiller (1854), dass "Semiramide" (1823) "...die einzige meiner italienischen Opern" war, "...die ich in Ruhe schreiben konnte; mein Vertrag erlaubte mir vierzig Tage", aber er lieferte die Partitur schon nach 33 Tagen ab. Für die Niederschrift des "Barbiere di Siviglia" brauchte er nur 13 Tage, wie er selbst 1860 Wagner berichtete.

Der einundzwanzigjährige Rossini stieg ab 1813 mit "Tancredi" und "L'Italiana in Algeri" innerhalb kurzer Zeit zum führenden und allgemein anerkannten Opernkomponisten Italiens auf, als Meister in allen Sparten der italienischen Oper (Opera seria, Opera buffa und Opera semiseria) und in der französischen Grand Opéra. Für seine Leistungen auf diesem Gebiet wurde er bereits zu Lebzeiten wie nur ganz wenige andere Komponisten als Genie verehrt und gefeiert.

Stilistisch war Rossini ein Komponist des Übergangs vom spätklassischen oder -klassizistischen Stil zur romantischen italienischen Oper, und er war während seiner Schaffenszeit die treibende Kraft bei diesem Stilwandel. Typisch für Rossini ist ein reich verzierter Gesang, eine schmelzende, bereits zur Romantik tendierende Melodik, eine farbige und für ihre Zeit manchmal ungewöhnliche Harmonik, ein fantasievoll-brillanter Orchestersatz mit oft virtuos eingesetzten Solo-Bläsern, effektvoller und oft rauschhafter Einsatz eines Orchester-( oder Tutti-)Crescendos. Alle italienischen Opernkomponisten seiner und der etwas jüngeren Generation folgten seinem Beispiel und komponierten bis etwa 1830 (und teilweise darüber hinaus) in einem "Rossini-Stil", das gilt besonders für Giovanni Pacini, Saverio Mercadante, und auch Gaetano Donizetti in seiner frühen Phase; selbst der deutsche Giacomo Meyerbeer erlebte in seinen sechs italienischen Opern eine Phase des "Rossinismus".

In seinen für Neapel komponierten Werken - beginnend mit "Elisabetta, regina d'Inghilterra" (1815) - verzichtete Rossini auf die traditionellen Secco-Rezitative, das heißt der Orchestersatz dieser Werke ist durchkomponiert. Dies war in der italienischen Oper der Zeit noch keineswegs üblich und erst recht nicht selbstverständlich, sondern wurde unter dem Einfluss der französischen Oper nach Gluck ("Orfeo ed Euridice" u. a.) und Spontini ("La vestale") speziell im französisch beherrschten Neapel so durchgesetzt; ein weiteres Vorbild für Rossini war in dieser Hinsicht Giovanni Simone Mayrs Oper "Medea in Corinto", die 1813 ebenfalls für das Teatro San Carlo entstanden war. In der Orchestrierung wurde Rossini wegen seiner reichen Verwendung von Holz- und Blechbläsern oft ein deutscher Einfluss vorgeworfen, und in der Tat erzählte der Komponist selber 1860 in seinem Gespräch mit Wagner, dass er als Jugendlicher Partituren von Haydns "Schöpfung" und Mozarts "Le nozze di Figaro", sowie der "Zauberflöte", eifrig studiert und kopiert habe; er habe dabei "...mehr gelernt als in allen Unterrichtsstunden des Bologneser Konservatoriums".

Bei Rossini kommen dazu noch weitere romantische Elemente - Melodik, Harmonik, und auch Dramatik - in Opern wie "Otello", "La donna del lago", "Mosè in Egitto", "Ricciardo e Zoraide", "Zelmira" und "Semiramide", die einen stark zukunftsweisenden Charakter und Einfluss hatten. Hier nimmt er oft stilistische Entwicklungen von Bellini, Donizetti und selbst Verdi vorweg.

Rossini hatte auch einen starken Einfluss auf die Entwicklung der französischen Oper, insbesondere auf Komponisten wie Daniel Auber, Meyerbeer, Jacques Halévy, Ferdinand Hérold und Adolphe Adam.

Manche Opern Rossinis waren ein triumphaler Erfolg, aber natürlich gilt das nicht für alle Opern Rossinis. Bei manchen stellte sich ein Erfolg erst nach einer manchmal misslungenen Premiere ein (wie beim "Barbier," siehe oben), und einige andere Opern waren zwar zunächst sehr beliebt, kamen aber schon nach 1830 oder 1840 aus der Mode.

Zu seinen größten und dauerhaftesten Erfolgen gehörten die drei Seria-Opern "Mosè in Egitto" (bzw. "Moïse et Pharaon"), "Otello" und "Semiramide". Für die beiden ersteren lassen sich über 130 Produktionen im 19. Jahrhundert nachweisen und für "Semiramide" mehr als 120.

"Otello" war besonders bei romantischen Heroinen wie Giuditta Pasta und Maria Malibran beliebt, vor allem wegen der Scena, Romanze und Preghiera der Desdemona "Assisa a' piè d'un salice". Diese wie überhaupt der gesamte dritte Akt mit dem tragischen Tod der Desdemona entsprach bereits vollkommen dem romantischen Zeitgeschmack, so dass Mazzini von "...einem göttlichen Werk..." sprach, "...das wegen seiner starken Dramatik, seiner fatalistischen Aura und der wunderbaren Einheit der Inspiration schon ganz der neuen Epoche zugehört". Die Oper wurde nach ihrer Premiere am 4. Dezember 1816 in Neapel neben zahlreichen Aufführungen in Italien schon 1818 in München gespielt, es folgten Dresden (1820), Barcelona (1821), Paris (1826; mit Giuditta Pasta und Rubini), Warschau und Palma de Mallorca (1828), Corfù (1830), Lissabon (1836), London (1839–1840), Valencia (1840); letzte Aufführungen gab es noch 1868–1869 am Teatro La Fenice in Venedig und 1870 an der Mailänder Scala und am Teatro Goldoni in Modena. Die Oper geriet später fast völlig in Vergessenheit durch Verdis "Otello" (UA 1887).

Ähnlich erfolgreich war auch Rossinis Meisterwerk "Mosè in Egitto", deren dritten Akt er nach der Uraufführung vom 5. März 1818 im nächsten Jahr bearbeitete und ergänzte, und die er als französische Grand Opéra für Paris 1827 in einer neuen Fassung als "Moïse et Pharaon" herausbrachte. Die Preghiera "Dal tuo stellato soglio" im dritten Akt war besonders berühmt, Paganini schrieb darüber eine Variationenreihe. Die Oper wurde in der einen oder anderen Form bis 1862 in ganz Italien regelmäßig und oft gespielt. Sie erlebte außerdem Produktionen in München und Dresden (1822), Wien (1825; mit Giuseppina Fodor, Giovanni David, Lablache und Carolina Ungher), Barcelona (1825), Lissabon, Oporto und Cádiz (1826), Madrid (1829), Nizza (1838), Saragossa (1843), und selbst in New York, Havanna und Antwerpen (1847). Letzte Aufführungen im 19. Jahrhundert fanden statt: 1869 an der Mailänder Scala, 1878–1879 in Turin, 1891 in Florenz (Chiesa di S. Giovannino) und 1899 in Lodi (Collegio S. Francesco); dabei fällt auf, dass sie manchmal wegen des biblischen Themas auch wie ein Oratorium in Kirchen aufgeführt wurde.

Ähnlich sieht das Bild für "Semiramide" aus, die nach ihrer Uraufführung in Venedig am 3. Februar 1823 in Italien regelmäßig und durchgehend bis 1861 gespielt wurde, und selbst Aufführungen in New York (1845) und in Konstantinopel (1852–1853) erlebte. Letzte Aufführungen gab es 1867 in Barcelona, 1876 in Turin und 1881 an der Mailänder Scala.

Es folgen in der Beliebtheitsskala vier Opern, die zum Genre der "buffa" und der "semiseria" gehören: "La Cenerentola", mit fast 100, "L' italiana in Algeri" mit über 90, "Il Barbiere di Siviglia" und "La gazza ladra" (Die diebische Elster), beide mit ungefähr 90 nachweisbaren Produktionen im 19. Jahrhundert.

Es mag etwas überraschen, dass - abgesehen von der fast vergessenen "La gazza ladra -" die anderen drei Opern heutzutage eindeutig die bekanntesten Werke Rossinis sind, aber im 19. Jahrhundert zwar sehr beliebt waren, aber nicht so oft aufgeführt wurden wie die zuvor genannten drei Seria-Opern. Es muss jedoch darauf hingewiesen werden, dass gerade in diesem komischen Genre (auch bei den anderen Opern) zu den genannten Zahlen noch einige Produktionen in Übersetzungen kommen, z. B. in Deutsch. Allein zum "Barbiere" sei gesagt, dass er nach der Premiere im römischen Teatro Argentina am 20. Februar 1816 (unter dem Namen "Almaviva, o sia L'inutile precauzione" (!)), in Italien regelmäßig und oft bis in die 1840er Jahre, und nach einem kleinen Tief ab ca. 1845 in den 1850er Jahren wieder regelmäßiger bis 1863 gespielt wurde. Außerhalb Italiens gab es Produktionen in Barcelona (1818, 1824, 1854), Paris (1819), Valencia (1825), Wien (1823), London (1831), Corfù (1847 und 1855), Lissabon (1853). Besonders erwähnenswert ist eine Familienproduktion in New York 1825 mit dem "Ur-Almaviva" Manuel Garcia, seinen Kindern Maria Garcia (der späteren Malibran) als Rosina und Manuel Garcia d. J. als Figaro, sowie mit der Mutter Joaquina Garcia als Berta. Letzte nachweisbare Produktionen waren 1863 in Barcelona, 1865 in Mailand, 1867 in Genua und 1894 in Lodi. Ganz ähnlich sieht das Bild für die drei anderen genannten Buffa- und Semiseria-Opern aus.

"Matilde di Shabran" (auch: "Bellezza e cuor di ferro", oder "Corradino"): über 75 Produktionen, "Tancredi": 66 Produktionen, "L'assedio di corinto" (bzw. "Maometto II" bzw. "Le siège de Corinthe"): 66 Produktionen, "La donna del lago": 64 Produktionen.

Außer der komischen Oper "Matilde di Shabran" sind alle anderen Opern dieser Kategorie vom Seria-Typ.

Zum besseren Verständnis sei hier die Oper "L'assedio di Corinto" herausgegriffen: Sie erlebte ihre Premiere als "Maometto II" (= "Maometto secondo") am 3. Dezember 1820 im Teatro San Carlo in Neapel, und lief unter diesem Namen in insgesamt 10 Produktionen von 1820 bis 1828. Nachdem Rossini in Paris eine überarbeitete französische Version als "Le siège de Corinthe" herausgebracht hatte (9. Oktober 1826), lief die Oper in Italien und anderswo (vermutlich in dieser zweiten Fassung) unter dem Namen "L'assedio di corinto" in 56 Produktionen regelmäßig zwischen 1827 und 1847, mit mehreren Produktionen im Jahr; danach 1849 bis 1860 etwas seltener. Letzte Aufführungen waren 1860 in Triest und 1867 in Venedig.

"Ricciardo e Zoraide" (bzw. "Zoraide"): 61 Produktionen, "Aureliano in Palmira": 51 Produktionen, "Eduardo e Cristina": 43 Produktionen, "Zelmira": 40 Produktionen, "Elisabetta, regina d'Inghilterra": 38 Produktionen, "L'inganno felice": 37 Produktionen, "Guillaume Tell" (italienisch: "Guglielmo Tell"): 35 Produktionen, "Torvaldo e Dorliska": 33 Produktionen.

Besondere Erwähnung verdient hier Rossinis letzte Oper "Guillaume Tell" (italienisch: "Guglielmo Tell"), die am 3. August 1829 in Paris uraufgeführt wurde, und als Grand Opéra und aufgrund stilistisch außerordentlich progressiver Merkmale aus seinem Gesamtschaffen herausfällt. Dies wurde zwar als genial gewürdigt (z. B. von Donizetti, der den zweiten Akt als "von Gott geschrieben" bezeichnete), doch kam die sehr lange Oper beim Publikum insgesamt nicht besonders gut an. Andererseits ist der überaus moderne Charakter der Musik und die weniger virtuose, romantischere Behandlung der Stimmen, die ursprünglich auf die Gesangstradition der französischen Oper zurückzuführen ist, vermutlich dafür verantwortlich, dass von der Gesamtzahl von 35 Inszenierungen allein 17, also fast die Hälfte, erst nach 1850 stattfanden, angefangen mit London 1851 bis hin zu Mailand 1881–1882. Es muss außerdem betont werden, dass die hier genannten Zahlen sich fast ausschließlich auf Italien bzw. auf italienische Produktionen beziehen. An der Pariser Oper lief der "Tell" immerhin so oft, dass man im Jahr von Rossinis Tod 1868 die 500ste Aufführung erreichte.

Als ein wesentlich typischeres Beispiel dieser Kategorie sei außerdem "Ricciardo e Zoraide" herausgegriffen. Sie erlebte ihre Uraufführung am 3. Dezember 1818 am San Carlo in Neapel, und lief regelmäßig bis 1835, mit zwei letzten Aufführungsreihen in Malaga 1841 und in Mailand 1846. Mindestens zehn Mal lief sie unter dem Namen "Zoraide" oder "La Zoraide," zwischen 1822 und 1832.

Ähnliche Aufführungsergebnisse hatten auch die anderen Opern dieser Stufe, abgesehen von dem späten "Guillaume Tell". Man kann insgesamt sagen, dass die meisten Opern dieser Stufe im Grunde ein ziemlich großer Erfolg waren, aber meistens nach 1830 oder 1840 aus der Mode kamen. Dies könnte meistens auf die stilistische Entwicklung hin zur Romantik zurückzuführen sein, denn abgesehen von den neapolitanischen Opern "Zelmira" und "Elisabetta" und dem späten "Guillaume Tell", waren alle anderen Opern ursprünglich für andere Theater und deshalb weniger sorgfältig komponiert, und enthielten z. T. sogar noch Secco-Rezitative (z. B. "Aureliano in Palmira"). Dabei fällt auf, dass die meisten dieser Opern eine weibliche Heldin im Titel führen, entweder allein ("Zelmira" und "Elisabetta"), oder gepaart mit einem männlichen Helden ("Eduardo e Cristina", "Ricciardo e Zoraide", "Torvaldo e Dorliska"). Ähnliche Werke wurden in dieser Epoche auch von anderen Komponisten und relativ häufig komponiert.
Rossinis "Elisabetta" war eine echte und typische Primadonnenoper, die nur von wenigen Sängerinnen interpretiert werden konnte, außer von Isabella Colbran (7mal), u. a. von Adelaide Tosi (6mal), Joséphine Fodor-Mainville (5mal), Adelaide Comelli-Rubini (5mal), Henriette Méric-Lalande (4mal), Giuseppina Ronzi de Begnis (3mal), und Giuditta Pasta (einmal). Ab ca. 1830 dürfte die große Konkurrenz von mehreren teilweise extrem erfolgreichen und natürlich stilistisch moderneren Opern von Donizetti dem Werk den Todesstoß gegeben haben. Die Rede ist hier von: "Elisabetta al castello di Kenilworth" (1829; mit der gleichen Königin Elisabeth als Hauptfigur), "Anna Bolena" (1830), Maria Stuarda (1834; mit Königin Elisabeth als zweiter Hauptfigur) und "Roberto Devereux" (1837; mit der gleichen Königin Elisabeth als Primadonna).

"Bianca e Falliero": ca. 30 Produktionen, "La pietra del paragone": 29 Produktionen, "Le Comte Ory": 22 Produktionen, "Ciro in Babilonia": 20 Produktionen.

Diese Opern sind im Vergleich zu Rossinis anderen Opern als relativ erfolglos einzustufen, andere Komponistenkollegen wären allerdings vielleicht schon glücklich gewesen über ähnliche Ergebnisse...

Rossinis größte und echte Misserfolge waren "Ermione", die über die erste Produktion in Neapel (UA: 27. März 1819) nicht hinauskam, und "Armida" (11. November 1817). Für die letztere sind nur fünf oder sechs italienische Produktionen nachweisbar - abgesehen von der Uraufführung mit einer ersten Spielzeit, lief sie nur noch 1818, 1819 und 1823 am selben Theater in Neapel, und später in Mailand 1836; laut Herbert Weinstock gab es auch eine deutsche Fassung, die 1821 in Wien, 1827 und 1836 in Hamburg, sowie 1832 in Berlin gespielt wurden. Die Gründe für diese Misserfolge sind nicht leicht zu finden, "Ermione" gilt heute unter Fachleuten als Meisterwerk; bei "Armida" spielte wahrscheinlich das Libretto eine Rolle, da Zauberopern nach Torquato Tassos Epos "Gerusalemme liberata" von 1574 zwar dem Geschmack des 18. Jahrhunderts, aber nicht mehr dem beginnenden 19. Jahrhundert entsprachen.

Zu den "Verlierern" (mit weniger als 10 nachweisbaren Produktionen) gehören außerdem: "La gazzetta" (26. September 1816, Neapel), "Adelaide di Borgogna" (27. Dezember 1817, Rom), "Adina" (22. Juni 1826, Lissabon), und die frühen Opern "La cambiale di matrimonio" (3. November 1810, Venedig), "L'equivoco stravagante" (26. Oktober 1811, Bologna), "Demetrio e Polibio" (18. Mai 1812, Rom), "L'occasione fa il ladro" (24. November 1812 - Venedig), und "Il signor Bruschino" (27. Januar 1813, Venedig).

Entscheidend zum Verständnis dieses Komponisten ist seine absolute Verwurzelung in der Tradition des italienischen Belcanto, das heißt einer Gesangstradition, die im Barock begründet wurde, und die ursprünglich stark durch den Gesang der Kastraten geprägt wurde. Dazu gehörte neben technisch perfekt ausgebildeten Stimmen vor allem ein verzierter Gesang mit vielen Koloraturen, der "canto fiorito".

Dieser war zu seiner Zeit in Italien völlig selbstverständlich, und die meisten und vor allem die wirklich guten Sänger, wie Angelica Catalani, Isabella Colbran, Manuel Garcia, Giovanni David, verzierten selber ihre Gesangslinien, was auch von den Komponisten mitberücksichtigt wurde (wie im Barock). Laut Stendhal soll der letzte große Kastrate Giovanni Battista Velluti in Rossinis Oper "Aureliano in Palmira" (1813–1814) in der Partie des Arsace seine eigene Verzierungskunst so sehr übertrieben haben, dass der Komponist sich in der Folge entschieden haben soll, alle Ornamente und Koloraturen seiner Musik selber zu notieren, angeblich weil er sie nicht mehr der Willkür der Sänger überlassen wollte. Obwohl Stendhal in seiner Biographie allgemein sehr zum Fabulieren neigte und diese Geschichte etwas zweifelhaft ist, da Rossini schon zuvor seine Partien stark verziert niedergeschrieben hatte, enthält sie jedoch den wahren Kern, dass er ungewöhnlich genaue Vorgaben machte. Dies führte zu einem stark bis sehr stark ornamentierten Gesang von einem oft rauschhaften Effekt, der an die Sänger der Hauptrollen die allerhöchsten Anforderungen stellt, und zwar nicht nur in den hohen (weiblichen) Sopranen, sondern in allen Stimmlagen, auch Alt, Tenor und Bass, und nicht nur in Soloarien, sondern auch in Duetten und anderen Ensembles (siehe die Notenbeispiele des Terzetts aus "Otello", für Sopran (Desdemona) und zwei Koloratur-Tenöre (Otello und Iago)).

Koloraturen spielten traditionell eine besondere Rolle vor allem in der Opera seria, sie gehörten zur Charakterisierung von hochstehenden, aristokratischen und königlichen Figuren und von mythologischen Helden und Göttern. So charakterisiert Rossini - wie vor und neben ihm schon andere Komponisten (wie Nicolini, Mayr, Pucitta usw.) - die in der klassizistischen Oper beliebten Ritter und Helden oft schon bei ihrem ersten Auftritt mit prunkenden Koloraturen. In der Praxis handelt es sich dabei entweder um Tenorpartien, wie z. B. Rinaldo in "Armida" oder die Titelfigur in "Otello", oder auch für "contralto musico", d. h. für weibliche Alte und Mezzosoprane, die als Ersatz für die seltenen Kastraten in männlichen Heldenrollen auftraten: z. B. die Titelpartie des "Tancredi", Arsace in "Aureliano in Palmira", oder Arsace in "Semiramide". Traditionell wurden sehr virtuose Koloraturen außerdem vom 17. bis zum frühen 19. Jahrhundert besonders in Momenten von Erregung oder Wut eingesetzt, berühmtestes Beispiel ist die zweite Arie der Königin der Nacht in Mozarts "Zauberflöte". Bei Rossini spielen solche Gefühle z. B. eine große Rolle in der Oper "Otello", sowohl in Arien des Protagonisten als auch in Ensembles (siehe Notenbeispiele). Ein typischer Einsatz für virtuose Koloraturen sind auch Momente der Freude, bei Rossini z. B. im Rondò finale der Elena in "La donna del lago"; weichere, anmutig-liebliche Ornamente auch in Liebesarien und -duetten (z. B. in "Armida"). Rossini setzte die Koloratur also in seinen Seria-Opern ganz ähnlich ein, wie vor und neben ihm viele andere Komponisten, allerdings hatte er eine Vorliebe für einen besonders virtuosen Gesang, wie in "Bianca e Falliero" oder "Semiramide".

Er ließ allerdings auch Figuren in Buffa- und Semiseria-Opern Koloraturen singen, wie z. B. Rosina und den Grafen d'Almaviva im "Barbier von Sevilla", in diesem Fall ist dies einerseits ein Zeichen für die edle Abkunft (Graf) bzw. den edlen Charakter, aber nicht selten auch der Freude, wie im Falle der berühmten Aria Finale der "Cenerentola". Aber gleichzeitig - und dies war etwas Neues - benutzte Rossini die Koloratur in der komischen Oper auch in einer ironischen Art und Weise (z. B. Rosinas berühmte Cavatina "Una voce posso fa" im "Barbiere"), genau wie ein extrem virtuoses, plapperndes und zungenbrecherisches Parlando und "Sillabato", die schon vor Rossini typisch für die Opera buffa waren, aber Rossini setzte es besonders gekonnt, effektvoll, und witzig ein, z. B. in Figaros berühmter Auftrittsarie "Largo al factotum" ("Barbiere di Siviglia", Akt I), und sehr oft in Ensembles und turbulenten Final-Szenen ("Barbiere di Siviglia", "Cenerentola" etc.). Der ironische Einsatz des Koloraturgesangs in der Buffa-Oper und zeitgleich die traditionelle edle, und meistens noch virtuosere Ornamentik von Rossinis aristokratischen Figuren in der Opera seria führte jedoch auf Dauer und besonders in der Nachwelt auch zu Missverständnissen. In gewisser Weise hatte er Alles "durcheinander" gebracht und auf den Kopf gestellt.

Rossini komponierte natürlich auch Beispiele für einen schlichteren und ergreifenden romantischen Gesang, vor allem in den Partien der Desdemona in "Otello", der Elena in "La donna del Lago", der "Zelmira", in "Ricciardo e Zoraide", oder des Titelhelden in "Mosè in Egitto", und erst recht für seinen "Guillaume Tell", der allerdings in der französischen Tradition steht, und stilistisch aus Rossinis Werk etwas herausfällt. Erwähnenswert sind in dem Kontext auch beinahe verdi-hafte Chor- und Ensemblepassagen wie in "Mosè".

Nach dem extremen Feuerwerk an Virtuosität der Rossini-Ära war es geradezu vorprogrammiert, dass jüngere italienische Komponisten wie vor allem Vincenzo Bellini und Donizetti begannen, den Gesang ab Ende der 1820er Jahre endgültig zu entschlacken und schlichter zu gestalten; dies gilt für alle Stimmlagen, aber besonders in den Männerstimmen, die nach 1830 kaum noch Ornamente zu singen hatten. Frauenrollen waren oft noch bis zum frühen Verdi bis etwa 1855 verziert, aber nicht (oder selten) so stark wie bei Rossini.

Dies Alles führte letztlich zu einem Verfall der Gesangskunst bereits ab 1830–1840, der auf einem anderen Verständnis des romantischen Operndramas basiert, im Sinne eines größeren Realismus, und heftiger Gefühle, die direkt, im Extremfall auch beinahe geschrien, geäußert werden. Eine Entwicklung, die sich im Verismo noch extremer fortsetzte.

In Frankreich war die Situation zumindest in den Frauenstimmen durch Rossinis Einfluss beinahe umgekehrt: Seit seinen für Paris geschriebenen Opern wurden auch von Komponisten wie Auber, Meyerbeer, Halévy, Adam, bis zu Gounod, Delibes und Offenbach Koloratursoprane und sogar -Mezzosoprane eingesetzt, obwohl es in der traditionellen französischen Oper von Lully bis Anfang des 19. Jahrhunderts gar keinen Koloraturgesang gegeben hatte. Es zeichnete sich außerdem eine Tendenz ab, Koloratursoprane in der "Opéra-comique" (Auber) oder in einem fröhlichen oder witzigen Kontext zu verwenden (Meyerbeer: der Page Urbain in "Les Huguenots", Offenbach: die Puppe Olympia in "Les contes d'Hoffmann"). Dies wurde auch in der deutschen (bzw. Wiener) Operette übernommen (z. B. Johann Strauss (Sohn): Adele in "Die Fledermaus", oder der "Frühlingsstimmenwalzer").

Die Folge all dieser Entwicklungen war, dass man Rossinis Opern, vor allem seine Seria-Opern mit ihren oft extremen Koloraturen, immer weniger verstand (teilweise bis heute). Es gab außerdem immer weniger Sänger, vor allem in den männlichen Tenor und Bass-Rollen, die den enormen Anforderungen an Agilität und Geläufigkeit, aber auch an Nuancen und Weichheit des Gesangs, gewachsen waren. Abgesehen von der Begeisterung für Verdi ab mindestens 1850, gab es ab spätestens 1880 oder 1890 so gut wie keine Tenöre und Bässe mehr, die Rossinis Seria-Rollen angemessen hätten singen können. Dies ist besonders ein Problem in den für Neapel geschriebenen Seria-Opern, für die Rossini ein Ensemble aus absoluten Spitzensängern zur Verfügung stand, mit mindestens zwei oder drei Koloratur-Tenören (urspr. Giovanni David, Andrea Nozzari, Manuel Garcia, Giovanni Battista Rubini u. a.) und mindestens einem Koloraturbassisten (ursprg. Michele Benedetti u. a.). Selbst die Besetzung der Frauenrollen wurde zu einem Problem, da es schon gegen Ende des 19. Jahrhunderts kaum noch (oder gar keine) Koloratur-Mezzosoprane und -Altstimmen gab, weshalb man z. B. die Partie der Rosina im "Barbier von Sevilla" - zu dieser Zeit eigentlich die einzige Oper, die überhaupt noch im Repertoire war - dann ausschließlich mit hohen Koloratursopranen besetzte, die die Partie natürlich nach oben legen mussten (dies galt noch bis in die 1980er Jahre). Vor allem die Partien für Isabella Colbran wären nicht mehr zu besetzen gewesen, weil sie einen ausdrucksvollen und stellenweise sogar dramatischen Koloraturgesang erfordern, wie es ihn um und nach 1900 nicht mehr gab. Die hohen Koloratursoprane des frühen zwanzigsten Jahrhunderts waren zwar oft hochvirtuos, aber sie beschränkten sich normalerweise auf eine Art kokettes "Gezwitscher", und sangen völlig ohne Ausdruck. Dies Alles führte dazu, dass man Koloraturgesang des Belcanto grundsätzlich für oberflächlich oder "komisch" hielt und verachtete.

Ein Meilenstein war daher eine Aufführung von Rossinis "Armida" beim Maggio Musicale 1952 in Florenz mit Maria Callas, die eigentlich nur kurzfristig einsprang, aber diese Rolle mit Ausdruck, und mit romantischen, sowie dramatischen Akzenten interpretierte; gleichzeitig war die Ausführung der Männerrollen allerdings ein großes Problem (wenn nicht ein Fiasko), besonders der sieben (!) Partien für Koloraturtenöre (die allerdings z.T. von der gleichen Person gesungen werden können). Eine eigentliche Rossini-Renaissance begann jedoch erst um 1960, sowohl durch die "Wiederentdeckung" des Koloratur-Mezzosoprans und -Alts, durch Sängerinnen wie Teresa Berganza und Giulietta Simionato, die Partien wie die Rosina im "Barbiere" oder in "L'italiana in Algeri" wieder in der Originallage sangen (dies hatte in den 1920ern allerdings mit etwas spezieller Technik auch die Spanierin Conchita Supervia gemacht). Ein besonderer Glanzpunkt in der Wiederbelebung der Rossini-Oper waren die Aufführung und Einspielung von Rossinis "Semiramide" mit Joan Sutherland und Marilyn Horne Mitte der 1960er Jahre.

Eine wirklich befriedigende Situation ergab sich allerdings erst ab ca. 1980, als auch Tenöre auftauchten, die den akrobatischen Schwierigkeiten der Rossini-Partien wieder besser gewachsen waren, genannt werden müssen hier: Ernesto Palacio, Raúl Giménez, Rockwell Blake, José Carreras, Chris Merritt, Bruce Ford, William Matteuzzi, John Aler, Jeffrey Kunde und Juan Diego Flórez; außerdem Baritone und Bässe wie Samuel Ramey, Ferruccio Furlanetto, Alastair Miles, Ildebrando d'Arcangelo. Um den Rossini-Gesang verdient gemacht haben sich außerdem virtuose Sängerinnen wie: Montserrat Caballé, Cecilia Gasdia, June Anderson, Lella Cuberli, Katia Ricciarelli, Edita Gruberová, Sumi Jo, Renée Fleming im Sopranfach; die Mezzosoprane Agnes Baltsa, Huguette Tourangeau, Frederica von Stade, Cecilia Bartoli, Vesselina Kasarova, Jennifer Larmore, Joyce DiDonato und Vivica Genaux; sowie die Altistinnen Lucia Valentini-Terrani, Bernadette Manca di Nisso, Ewa Podles, Daniela Barcellona.

Trotz der beschriebenen Rossini-Renaissance, stehen leider nach wie vor nur wenige Opern Rossinis wegen aufführungstechnischer Probleme, und manchmal auch wegen gewisser Vorbehalte von Rossini als einem ausschließlichen Meister der komischen Oper, auf dem Spielplan vieler Opernhäuser.

Selbst wenn viele Opern Rossinis vergessen waren oder noch sind, haben die Ouvertüren einiger Opern als beliebte Konzertstücke überlebt. Diese Praxis findet sich bereits bei Johann Strauss (Vater), der schon um 1830 in seinen Konzertprogrammen seine eigenen Walzer und Galoppe mit Ouverturen zu damals beliebten oder neuen Opern mischte, nicht nur von Rossini, sondern auch von Bellini, Auber und anderen Komponisten.

Viele frühe Ouvertüren Gioachino Rossinis sind vor allem als festliche Einleitung gedacht, und stehen deshalb inhaltlich in keiner direkten Verbindung zur folgenden Oper. Ein Extremfall einer beliebig austauschbaren Ouvertüre ist diejenige zu "Il barbiere di Siviglia" (1816), die er ursprünglich für "Aureliano in Palmira" (1813) komponiert hatte, und auch für "Elisabetta, regina d'Inghilterra" (1815) verwendete.

Eine solche Beliebigkeit überwandt er jedoch in der Folge. So steht beispielsweise die Ouvertüre zur Oper "La gazza ladra" (1817) in engster Verbindung zum Operninhalt: Bereits der überraschend militärische Beginn mit mehreren Trommelwirbeln spielt auf den Inhalt der Oper an - und zwar auf die drohende Exekution der Hauptfigur Ninetta und ihres Vaters, des flüchtigen Soldaten. Nach einem ersten Abschnitt in einem festlich punktierten, aber sehr eleganten Marschrhythmus, mit tragischen harmonischen und melodischen Inflexionen, gibt es mehrere Abschnitte, die später in der Gefängnisszene im zweiten Akt wiederkehren: Die von den Streichern gespielte, triolisch abfallende Melodie in Moll, die direkt nach der martialischen Einleitung folgt, kehrt im Duett von Ninetta und Pippo „E ben, per mia memoria“ wieder. Und das scheinbar witzig-kokette und „schmissige“ Thema im Dreiertakt gegen Ende der Ouverture verwendete Rossini als Brückenmotiv in der Cabaletta des zurückgewiesenen und wütenden Podestà, wo es allerdings keineswegs komisch wirkt. Auch die Ouverture zu "Semiramide" (1823) enthält musikalisches Material aus der Oper: So entspricht bereits das einleitende, romantisch mit Hörnern instrumentierte Largo dem Quartett „Giuro ai numi“ („Ich schwöre bei den Göttern“) im ersten Finale.

Die meisten Opernouvertüren von Rossini sind nach einem ähnlichen formalen Schema komponiert. Am Beginn steht in der Regel eine langsame und oft spannungsvolle Einleitung. In "L’italiana in Algeri" (1813) steht dagegen ein schneller Teil mit zwei kontrastierenden Themen, die in Dynamik und Tempo gesteigert werden, am Beginn. Eine Durchführung im klassischen Sinne fehlt, weshalb nur von einer verkürzten Sonatenhauptsatzform gesprochen werden kann, da auch der harmonische Ablauf meist zwischen Tonika und Dominante pendelt. Die effektvollen Finalwirkungen am Ende vieler Ouvertüren, die als schmissiger Vorhangöffner dienen sollen, verdanken sich Rossinis Handhabung des Crescendo im Orchester, wobei kurze, häufig zwei- oder viertaktige Motive stetig wiederholt werden, dabei aber in Dynamik und Instrumentation gesteigert werden. Beispielsweise geschieht dies in den Ouvertüren zu "Semiramide" und "Otello". Trotz dieser häufigen formalen Einheitlichkeit zeigen alle Ouvertüren Rossinis eigene melodische (und formale) Erfindungen, wie beispielsweise das überraschend intime Rezitativ am Beginn der Ouvertüre zu "Guillaume Tell", die elegischen Oboensoli in "Otello" und "L’italiana in Algeri", das anarchische Schlagen der Geigenbogen an die Notenpulte bei "Il signor Bruschino" oder der quasi kanonische Beginn bei "La scala di seta".

Es sei auch darauf hingewiesen, dass Rossini in mehreren seiner Opern keine eigenständigen Ouverturen verwendete, sondern Einleitungen (= "Introduzione") schrieb, die direkt in die Bühnenhandlung und in Gesangsnummern wie Chöre und Ensembles überleiten. Das betrifft beispielsweise die neapolitanischen Opern "Mosè in Egitto" (Fassung 1818/1819) und "Ricciardo e Zoraide" (1818). Bei der letzteren hat die Einleitung zwar die Länge einer Ouverture, erklingt aber bei geöffnetem Vorhang, und hat viele Einwürfe einer Banda aus dem Hintergrund der Bühne; sie geht dann direkt und untrennbar in den Einleitungschor über.

Rossinis Werk steht im Mittelpunkt mehrerer jährlich stattfindender Festspiele. Das "Rossini Opera Festival" (seit 1980) in seiner Geburtsstadt Pesaro und das Festival "Rossini in Wildbad" (seit 1989) in Bad Wildbad in Baden-Württemberg führen jedes Jahr mehrere selten aufgeführte Opern Rossinis und seiner Zeitgenossen auf. Auch die Knoxville Opera in Knoxville (Tennessee) veranstaltet seit 2001 ein jährliches "Rossini Festival", das von einem italienischen Straßenfest begleitet ist.
„Tournedos alla Rossini“ oder Tournedos Rossini, eine Zubereitungsart von Rinderfiletsteaks mit einer Scheibe Gänseleber, sind nach Gioachino Rossini benannt.

Gleiches gilt für den Rossini Point, eine Landspitze im Süden der antarktischen Alexander-I.-Insel.












</doc>
<doc id="11033" url="https://de.wikipedia.org/wiki?curid=11033" title="Abstrakter Expressionismus">
Abstrakter Expressionismus

Der abstrakte Expressionismus ist eine nordamerikanische Kunstrichtung der modernen Malerei, die vornehmlich durch die New York School in den späten 1940er bis frühen 1960er Jahren bekannt wurde. Ihre Hauptströmungen manifestierten sich im Action Painting und der Farbfeldmalerei. 

Allen Ausprägungen des abstrakten Expressionismus war gemeinsam, dass das Gefühl, die Emotion und die Spontanität wichtiger waren als Perfektion, Vernunft und Reglementierung. Die Darstellungsweise war abstrakt, teilweise auch abstrakt-figurativ. Er übernahm die surrealistische Technik des Automatismus und die kubistische Idee der flächigen Räumlichkeit. Die Maltechniken wurden variiert und der Farbauftrag auf den Malgrund wurde mit Pinseln, Spachteln, mit der Handfläche, mit Hilfe von durchlöcherten Behältern („dripping“) oder Eimern vollzogen. Der Gründungsdirektor des Museum of Modern Art in New York, Alfred Barr, charakterisierte die – nach Fauvismus und Kandinsky – zweite Strömung abstrakter Malerei „eher intuitiv und emotional als intellektuell, ihre Formen sind eher organisch und biomorph als geometrisch, eher kurvig als rechteckig, eher dekorativ als strukturell, und in ihrer Begeisterung für das Mystische, Spontane und Irrationale ist sie eher romantisch als klassisch“.

Die Bezeichnung "abstract expressionism" für diese Kunstrichtung geht auf den langjährigen Kunstkritiker des "New Yorker", Robert Coates, zurück. Er verwendete ihn anlässlich der Besprechung der ersten umfassenden Ausstellung von Hans Hofmann 1946 in der Mortimer Brandt Gallery.
In den Vereinigten Staaten entwickelte sich, unabhängig von der europäischen Entwicklung, das "Action Painting", mit Jackson Pollock als seinem Hauptvertreter, der Farbe auf die am Boden ausgebreitete Leinwand tropfte, rinnen ließ oder schleuderte (eine Technik, die auch schon Max Ernst verwandte). Auch Sam Francis, Helen Frankenthaler und der frühe Robert Rauschenberg praktizierten eine schnelle spontane Malerei. Hauptvertreter der meditativen "Farbfeldmalerei" ("Colorfield Painting") sind Barnett Newman und Mark Rothko. Rothko malte große, oft monochrom modulierte Farbflächen mit meditativem Charakter, die mit dem Begriff "expressionistisch" nicht zu fassen sind, und der immer abgestritten hat, seine Bilder seien "abstrakt".

Weitere wichtige Künstler des abstrakten Expressionismus waren Mark Tobey, Adolph Gottlieb, Arshile Gorky, Clyfford Still, Willem de Kooning, Franz Kline und Robert Motherwell. Ad Reinhardt wird dieser Richtung ebenfalls zugerechnet, obwohl er sich davon distanzierte. Neben der Ostküsten-Variante der "New York School" entstanden zwei pazifische Varianten, die "California School" mit Richard Diebenkorn und die "Northwest School" des abstrakten Expressionismus mit Mark Tobey und Morris Graves als bedeutendsten Vertretern.

Verwandt ist dem amerikanischen abstrakten Expressionismus die europäische abstrakte Kunst der Nachkriegszeit, die als "Informelle Kunst" oder "Tachismus" bekannt wurde, wobei "la tache = der Fleck" als Ausgangspunkt für den Malprozeß diente. Sie stammte aus Frankreich und fand in Deutschland (vornehmlich Düsseldorf) große Resonanz. Wichtige Künstler sind Wols, Jean Fautrier, Hans Hartung, Georges Mathieu aus Frankreich und Peter Brüning, Karl Otto Götz, Emil Schumacher aus Deutschland. In Österreich manifestiert sich die Strömung des abstrakten Expressionismus bis zum 21. Jahrhundert im Werk von Hermann Nitsch und Josef Trattner.

Zweiter Weltkrieg, Judenverfolgung und die Verdammung der modernen Kunst durch die Nationalsozialisten als Entartete Kunst führten zu einer Immigrationswelle europäischer Künstler in die USA, vor allem nach New York. Hans Hofmann eröffnete 1933 in New York die "Hofmann School of Fine Arts", Josef Albers lehrte ab 1933 am "Black Mountain College". Sie übten dadurch starken Einfluss auf zeitgenössische amerikanische Künstler aus.

Dabei handelt es sich weniger um eine Stilrichtung als ein Konzept, Kunst in spontaner Weise und ohne die Beschränkung durch herkömmliche Formen auszuführen. Zu den führenden Kräften der Bewegung zählten Jackson Pollock, Willem de Kooning und Mark Rothko. Die surrealistische Haltung zur freien Schaffung hatte einen bedeutenden Einfluss auf die Anfänge des abstrakten Expressionismus, vor allem durch den abtrünnigen Surrealisten Wolfgang Paalen, der in seiner Zeitschrift DYN aus Mexiko einen von der Quantenphysik, dem Totemismus und Kubismus neu bestimmten Raumbegriff propagierte. Peggy Guggenheims Museum und Galerie Art of This Century in New York, die von 1942 bis 1947 moderne Kunst ausstellte, war ein Treffpunkt europäischer surrealistischer und junger amerikanischer Künstler und bot den wichtigsten Ausstellungsraum in der Entwicklungszeit des abstrakten Expressionismus. Zu dieser Zeit stellte Barnett Newman eine Liste der gewünschten Vertreter für das zu gründende "New Art Movement" auf. Er nannte neben Gottlieb, Rothko, Pollock, Hofmann, Baziotes und Gorky auch Wolfgang Paalen. Motherwell dagegen versah er darin noch mit einem Fragezeichen.

Als abstrakt expressionistisch wurden in den USA zuerst Werke des russischen Malers Wassily Kandinsky bezeichnet und zwar von Alfred H. Barr, dem ersten Direktor des New Yorker Museum of Modern Art. Maler der europäischen Avantgarde, die während des Zweiten Weltkriegs nach New York emigriert waren, darunter Max Ernst, Marcel Duchamp, Marc Chagall, Yves Tanguy, Piet Mondrian sowie 1947 in einem mehrmonatigen Aufenthalt Joan Miró, belebten bei amerikanischen Künstlern das Interesse für abstrakte Malerei neu und bereiteten den Boden für den Triumph der abstrakten Malerei in den 1940er und 1950er Jahren vor.

Diese Generation der Künstler ist von einer tiefen Fortschrittskritik geprägt, vor allem durch die Erfahrungen des Zweiten Weltkriegs und dem Abwurf der Atombombe auf Hiroshima. Wolfgang Paalens Theaterstück "The Beam of the Balance", eine Tragik-Komödie, ist eine Reflexion auf die ungebrochene Macht des Stalinschen Staatsterrorismus, die US-amerikanischen Atombombenabwürfe auf Hiroshima und Nagasaki im Sommer 1945 und die Gefahr einer aus dem Gleichgewicht geratenen Wissenschaft im Allgemeinen. Es wurde erstmals bekannt durch eine halböffentliche Lesung im Hause Robert Motherwells in East Hampton im Sommer 1946. Bei Vertretern des abstrakten Expressionismus wie Barnett Newman und Mark Rothko wird eine ähnliche Haltung deutlich.

Der jüdisch-deutsche Emigrant Hans Sahl war nach eigenen Angaben dabei, als der Begriff „Abstrakter Expressionismus“ erfunden wurde: „Ich hatte ihre Anfänge in der Cedar Bar und im White Horse Inn kennengelernt, als mein Freund, der Bildhauer Peter Grippe, mich mit einigen jungen Leuten bekannt machte, die behaupteten, man müsse etwas Neues erfinden, etwas, das weder abstrakt noch expressionistisch wäre und doch beides zugleich. ‚Warum nicht abstrakter Expressionismus?‘ sagte ein stämmiger, etwas bedrohlich aussehender Mann, der Jackson Pollock hieß. Man prostete einander mit Bierflaschen zu. Das war kurz nach dem Zweiten Weltkrieg. Dann griffen die Kunsthändler die Idee auf, und der abstrakte Expressionismus eroberte die Welt“.

Der abstrakte Expressionismus wurde im Kalten Krieg als „Aushängeschild“ für den „freien Westen“ funktionalisiert. Obwohl er noch im eigenen Land erbitterte Gegner im konservativen Lager hatte, die abstrakte Kunst als unamerikanisch diffamierten, sollte er im internationalen Ausstellungsbetrieb für ein „modernes, liberales Amerika“ werben.

Anlässlich des Pariser „Kongresses für kulturelle Freiheit“ 1952 zeigte das Museum of Modern Art eine Ausstellung mit Meisterwerken des abstrakten Expressionismus. Der Kurator der Ausstellung verwies darauf, dass hier Werke gezeigt würden, „die in totalitären Systemen wie dem Deutschland der Nazi-Zeit oder dem heutigen Sowjet-Rußland und seinen Satelliten nicht hätten entstehen geschweige denn ausgestellt werden können.“ Diese Feststellung stellt sich als nicht ganz richtig heraus, wie es weiter unten gezeigt werden kann.

1953 wurden zwölf zeitgenössische US-amerikanische Maler und Bildhauer in Europa vorgestellt, darunter die Altmeister John Marin, Stuart Davis, Edward Hopper und der Sozialist Ben Shahn. Abstrakt-expressionistische Werke machten sogar nur ein Viertel der Ausstellung "Modern Art in the United States" aus der Sammlung des New Yorker Museum of Modern Art aus, die 1956 in Europa zu sehen war. Erst 1958/59er triumphierte die neueste Malerei, "The New American Painting" zeigte einundachtzig Bilder von siebzehn abstrakt-expressionistischen Künstlern in acht westeuropäischen Metropolen und anschließend im Museum of Modern Art in New York. Die von Dorothy Canning Miller, der einflussreichen Kuratorin des "MoMA", zusammengestellte Show veränderte das Bild Europas von der Kunst der USA. Ermöglicht worden war sie durch die Unterstützung der Rockefeller Foundation und das Engagement von Blanchette Ferry Rockefeller. In Rom, Basel, Amsterdam, Brüssel, Paris, Berlin und London und auf der documenta II in Kassel (1959) war die Jackson Pollock-Retrospektive zu sehen, die Frank O’Hara für die 4. Biennale von São Paulo (1957) zusammengestellt hatte. In Kassel wurden außerdem die Arbeiten aller Künstler der "New American Painting"-Show und weiterer Amerikaner ausgestellt, insgesamt 144 Arbeiten von 44 Künstlern.

Nach Karl Eimermacher berichtet ein amerikanischer Journalist von den Weltjugendfestspielen in Moskau 1957 „Unsere (amerikanischen Künstler, K.E.) meinten die Russen mit einer Welle von "aggressiven" Abstraktionen zu verblüffen. Man ging vom letzten Schrei avantgardistischer Richtungen aus und hoffte, mit all diesem Eklektizismus den sozialistischen Realismus k.o. zu schlagen. Ununterbrochen produzierte man Bilder, wie am Fließband. War eine Leinwand "fertig", griff man schon zur nächsten. Die Russen waren wie erschlagen. Ein solches Tempo hatten sie nicht erwartet. Den Zöglingen der Akademie blieb nichts anderes übrig als ihre Position mit Worten zu verteidigen. Man stritt heftig. Wir wurden wegen der Vernachlässigung sozialer Probleme angegriffen, wohingegen wir einwendeten: Zuerst müsse man lernen, mit dem Material umzugehen! So ging es, bis ein merkwürdig aussehender Bursche mit zwei Eimern Farbe auftauchte, die er sich bei den gelangweilt zusehenden Anstreichern zusammen mit einem an einem Stock hängenden Scheuerlappen geliehen hatte. Als er seine Leinwand ausgebreitet hatte, kippte er – soweit dies die Räumlichkeiten zuließen – beide Eimer über sie aus, sprang mitten in die blau-grüne Pfütze und begann verzweifelt, mit dem Schrubber zu arbeiten. Alles dauerte nicht länger als zehn Sekunden. Wir erstarrten vor Begeisterung. Zu unseren Füßen lichtete sich ein großes Frauenporträt, virtuos gestaltet, raffiniert und mit einem feinfühligen Verständnis. Der Bursche blinzelte einem der zu Stein gewordenen Amerikanern zu, klatschte ihm mit der völlig verschmierten Handfläche auf den Hintern und sagt: "Hört auf, euch mit Malerei zu beschäftigen, ich bringe euch erst mal Zeichnen bei.“

In ihrem Buch: "Who Paid the Piper. The CIA and the Cultural Cold War" vermerkte die britische Historikerin und Journalistin Frances Stonor Saunders (* 1966), dass die CIA Jackson Pollock und andere abstrakte Expressionisten subventionierte. Dies geschah im Wege des Congress for Cultural Freedom und in Übereinstimmung mit der Förderungspolitik der Rockefeller Foundation und der Ford Foundation. Während Stalin in seinem unmittelbaren Machtbereich den sozialistischen Realismus forcierte und in Paris linke Intellektuelle wie Jean-Paul Sartre und Pablo Picasso die Kulturszene dominierten, weiters der mexikanische Muralismus um Diego Rivera und David Alfaro Siqueiros, der in der Ära der Großen Depression, des Amerikanischen Regionalismus und der New Deal Wandmalerei auf die USA ausgestrahlt hatte, ebenfalls der KP-Seite zugeneigt war, bot sich nach dem Krieg der abstrakte Expressionismus auch im zerstörten Europa als Demonstration politischer und künstlerischer Freiheit (ohne sozialkritische Botschaft) an. Während die Kunstströmung als förderungswürdig im Sinne der Soft Power galt, waren die individuellen Künstler nicht unbedingt systemkonform.

Welche Erschütterung die New American Painting Show hervorrief, ist auch an dem Melbourner "Antipodean Manifesto" einer Gruppe figurativer Maler und des marxistischen Kunsthistorikers Bernard Smith gegen die amerikanisch dominierte Abstraktion abzulesen.

Unter den Galeristen der abstrakten Expressionisten sind neben Peggy Guggenheim die Künstlerin Betty Parsons die Kunsthändler Charles Egan, Samuel Kootz und Sidney Janis hervorzuheben. 

Bilder des Abstrakten Expressionismus erfreuen sich einer steigenden Nachfrage von privaten Kunstsammlern, während staatliche Museen entsprechende Ankäufe kaum noch finanzieren können. Seit der Jahrtausendwende erzielen Bilder von Willem de Kooning, Mark Rothko, Clyfford Still oder Barnett Newman auf Auktionen Spitzenpreise im zweistelligen Millionenbereich. Da diese Bilder ein enges, geschlossenes Marktsegment repräsentieren, die Anzahl der betreffenden Künstler und Objekte begrenzt bleibt, erfreuen sie sich von Seiten der Käufer eines Vertrauens in stetig wachsende Preise und sind daher beliebte Spekulationsobjekte, da man auf hohe Gewinne hoffen kann.





</doc>
<doc id="11034" url="https://de.wikipedia.org/wiki?curid=11034" title="Bamberg">
Bamberg

Bamberg (mittelalterlich: Babenberg, bambergisch: „Bambärch“) ist eine kreisfreie Stadt im bayerischen Regierungsbezirk Oberfranken und Standort des Landratsamtes Bamberg. Sie ist die größte Mittelstadt Bayerns. Sie ist Universitäts-, Schul- und Verwaltungsstadt, wichtiges Wirtschaftszentrum Oberfrankens sowie Sitz des gleichnamigen Erzbistums. Das bekannteste Bauwerk ist der viertürmige Bamberger Dom, einer der früheren Kaiserdome.

Die Stadt ist in der Landesplanung als Oberzentrum des westlichen Oberfrankens ausgewiesen und zählt zur Metropolregion Nürnberg. Die Stadt hat etwa 76.000 Einwohner und ist damit die größte Stadt Oberfrankens, die Agglomeration hat rund 112.000 Einwohner.

Die Altstadt ist der größte unversehrt erhaltene historische Stadtkern in Deutschland und seit 1993 als Weltkulturerbe in die Liste der UNESCO eingetragen. Darüber hinaus ist Bamberg überregional bekannt für seine vielfältige Biertradition.

Die alte fränkische Kaiser- und Bischofsstadt Bamberg erstreckt sich über die Talsenke der Regnitz. Zwischen ihrem rechten und linken Arm – der rechte wurde zum Main-Donau-Kanal ausgebaut – liegt die sogenannte "Inselstadt". Weitere zentrale Stadtteile sind der "Domberg" im Südwesten sowie die "Gärtnerstadt" im Nordosten und die "Wunderburg" im Südosten, wobei diese beiden in der genannten Senke liegen. Die Lage an den zwei Flussarmen prägt den Charakter der Altstadt.

Die Regnitz verlässt die Stadt in nordwestlicher Richtung und mündet an deren westlichster Grenze, fünf Kilometer vom Zentrum entfernt, bei Bischberg in den Main. Nach Süden erstreckt sich das Regnitztal bis nach Fürth, im Westen liegt der Steigerwald, im Nordwesten und Norden das Maintal und jenseits dessen der Naturpark Haßberge. Östlich von Bamberg befindet sich das Hügelland des Naturparks Fränkische Schweiz mit dem Geisberg () und Katzenberg (), bei gutem Wetter in Sichtweite.

Das Stadtgebiet grenzt an die umliegenden Gemeinden (von Norden beginnend im Uhrzeigersinn) Gundelsheim, Memmelsdorf, Litzendorf, Strullendorf, Pettstadt, Stegaurach, Bischberg, Oberhaid und Hallstadt.

Das Stadtgebiet hat eine Fläche von 54,58 Quadratkilometern. Die Nord-Süd-Ausdehnung beträgt 9,6 Kilometer, die Ost-West-Ausdehnung 9,7 Kilometer. Die Altstadt liegt auf der Insel zwischen den beiden Regnitzarmen sowie westlich des linken Regnitzarmes (Domberg). Später dehnte sich die Stadt vor allem nach Osten aus.

Bamberg gliedert sich in die Stadtteile: Berggebiet, Mitte (Inselstadt), Theuerstadt (Gärtnerstadt, St. Gangolf), Wunderburg, Gereuth, Nord (St. Otto), Ost (St. Heinrich), Gartenstadt (St. Kunigunda), Bruckertshof, Bug, Bughof, Gaustadt, Hirschknock, Kramersfeld und Wildensorg.

Zwischen 1750 und 1753 teilte die Verwaltung Bamberg in vier Viertel ein, die nach den Stadtheiligen benannt wurden: das Henriziviertel im Nordwesten, das Georgenviertel im Nordosten, das Kunigundenviertel im Südosten und das Ottonisviertel im Südwesten. Die Häuser wurden innerhalb jedes einzelnen Viertels durchnummeriert. Eine neue Einteilung in vier Distrikte, die den Pfarreien St. Martin, St. Gangolf, Unserer Lieben Frau und Dom entsprachen, erfolgte 1804. Zugleich wurden alle Häuser der Stadt fortlaufend durchnummeriert, beginnend mit dem Alten Rathaus. 1876 ging man zur straßenweisen Vergabe von Hausnummern über. Zu den bestehenden Distrikten kamen mit zunehmendem Stadtwachstum weitere, so 1909 der 5. Distrikt um die Pfarrei Maria Hilf in der Wunderburg und 1916 der 6. Distrikt um die Pfarrei St. Otto. Diese bis heute nicht offiziell abgeschaffte Einteilung dient nur noch als Grundlage für die Tätigkeit der 16 Bürgervereine, wobei es teilweise zwei Bürgervereine in einem Distrikt gibt. Die Bürgervereine Gaustadt und Kramersfeld-Bruckertshof-Hirschknock wurden gegründet, als Gaustadt noch selbständige Gemeinde war und Kramersfeld noch zu Hallstadt gehörte. Aktuell ist die Stadt in statistische Zählbezirke gegliedert, die sich an historischen und städtebaulichen Leitlinien orientieren.

Bamberg hat ca. 650 Straßen, Plätze und Gassen.

Die Temperaturmittel betragen im kältesten Monat Januar −1,1 °C, im Sommermonat Juli +17,8 °C. Der Jahrestemperaturdurchschnitt liegt bei +8,5 °C.

In Bamberg gibt es zwei Naturschutzgebiete, vier Landschaftsschutzgebiete, vier FFH-Gebiete und zwei Geotope (Stand August 2016).

Siehe auch:

Die ältesten Relikte der Bamberger Vorgeschichte sind vermutlich die im 19. Jahrhundert gefundenen Bamberger Götzen.

Im Jahre 902 wurde zum ersten Mal ein Castrum Babenberch auf dem heutigen Domberg genannt. Es gehörte dem ostfränkischen Geschlecht der älteren Babenberger, die das Lehen 903 in einer blutigen Fehde mit den rheinfränkischen Konradinern verloren. Bei der sogenannten Babenberger Fehde starben drei babenbergische Brüder. Die Besitzungen fielen an den König und blieben bis 973 Königsgut. Kaiser Otto II. schenkte das Castrum seinem Vetter, dem Herzog von Bayern, Heinrich dem Zänker.

1007 erfolgte die Gründung des Bistums durch Kaiser Heinrich II., den Sohn Heinrichs des Zänkers, und im gleichen Jahr ließ er den ersten Dom errichten, der aber zweimal abbrannte und durch den heutigen, aus dem 13. Jahrhundert stammenden Bau ersetzt wurde. 1208 wurde König Philipp von Schwaben in Bamberg durch Otto VIII. von Wittelsbach ermordet. 

Im Januar 1430 rückten die Hussiten auf Bamberg vor (siehe auch Hussitenkriege). Das Domkapitel floh mit dem Domschatz (heute im Diözesanmuseum Bamberg) auf die Giechburg, der Bischof selbst zog sich nach Kärnten zurück. Die wohlhabenden Bürger flüchteten nach Forchheim und Nürnberg. Die Hussiten nahmen Bamberg jedoch nicht ein. Als sie Scheßlitz erobert hatten, plünderten die in Bamberg verbliebenen Handwerker, Tagelöhner und Bauern erst die Weinkeller und dann die Bürgerhäuser und Klöster. Kurz darauf handelte Markgraf Friedrich von Brandenburg mit Andreas Prokop, Heerführer der Hussiten, auf Burg Zwernitz einen Waffenstillstand aus und Bamberg zahlte 12.000 Gulden Lösegeld, um der Brandschatzung zu entgehen.

Ein Aufstand der Bürger im 15. Jahrhundert gegen die fürstbischöfliche Macht, der sogenannte Immunitätenstreit, blieb erfolglos. Der Bauernkrieg 1524/1525 hinterließ in der Stadt seine Spuren.

Im Dreißigjährigen Krieg litt die Stadt sehr unter den schwedischen Truppen, im Siebenjährigen Krieg durch preußische und zu Zeiten Napoleons durch französische Truppen.

Unter den Fürstbischöfen Lothar Franz (1693–1729) und Friedrich Carl von Schönborn (1729–1746) erlebte die Stadt in der Barockzeit eine kulturelle Blüte.

Stadt und Stift wurden im Frieden von Lunéville dem Kurfürstentum Bayern als Kompensation für den Verlust der Pfalz an Frankreich in Aussicht gestellt. Noch vor der endgültigen Fixierung im Reichsdeputationshauptschluss begann Bayern am 2. September 1802 das Territorium des Hochstifts militärisch zu besetzen und erklärte das Gebiet am 29. November endgültig zu einer bayerischen Provinz. Fürstbischof Christoph Franz von Buseck trat zurück und besiegelte damit das Ende der Selbständigkeit Bambergs.

Während der Märzrevolution in den Jahren 1848/49 war Bamberg eine Hochburg der Demokraten, weshalb die Stadt bei der Regierung in München als besonders radikal galt. Bekannteste Persönlichkeiten waren die Anwälte Nikolaus Titus und Ignaz Prell, der Arzt Heinrich Heinkelmann und der Journalist Carl Heger. Dort wurden die sogenannten 14 Bamberger Artikel, ein Grundrechtekatalog, verlesen.

Am 25. und 26. Mai 1854 hielten acht deutsche Mittelstaaten (Bayern, Sachsen, Hannover, Württemberg, Baden, Kurhessen, Hessen-Darmstadt und Nassau) in Bamberg die Bamberger Konferenz ab, in der sie sich über ihre Stellung zu den beiden Großmächten Österreich und Preußen in der orientalischen Angelegenheit verständigten.

1909 wurde in Bamberg eine der ersten Pfadfindergruppen in Deutschland gegründet.

Nach dem Ersten Weltkrieg flüchtete am 7. April 1919 die kurz zuvor gewählte Bayerische Staatsregierung (Kabinett Hoffmann) in den Auseinandersetzungen um die Münchner Räterepublik nach Bamberg und forderte von dort aus militärische Unterstützung zur Niederschlagung der Räterepublik an. Nachdem die Räterepublik von Reichswehr und Freikorps gewaltsam beendet worden war, wurde am 14. August 1919 die Bamberger Verfassung als erste demokratische Verfassung für Bayern unterzeichnet.

Auch in Bamberg wurde 1933 die Macht an die Nationalsozialisten übergeben, und Bamberger Bürger beteiligten sich an der Verfolgung jüdischer Mitbürger. Willy Aron wurde am 10. März 1933 in „Schutzhaft“ genommen und dann in Dachau ermordet. Am 1. Juli 1933, sechs Wochen nach dem 10. Mai in Berlin, wurden auf der Hauptkampfbahn des Volksparks Bücher verbrannt. Der Unternehmer der Hofbräu Bamberg, Willy Lessing, wurde 1936 enteignet und bei den Novemberpogromen 1938 so schwer misshandelt, dass er kurze Zeit später starb. Durch zwei Luftangriffe, die insgesamt 310 Tote forderten und 708 Wohnungen vernichteten, wurde Bamberg zu 4,4 % zerstört.

Nach dem Ende des Zweiten Weltkriegs gehörte Bamberg zur Amerikanischen Besatzungszone. Von der Militärverwaltung wurde ein Lager für sogenannte Displaced Persons angelegt. In Bamberg befand sich seit dem Ende des Zweiten Weltkrieges bis September 2014 eine Garnison der US Army.

Die wechselnde Wasserführung der Regnitz stellt seit Jahrhunderten eine Bedrohung für die Stadt dar. Im Juli 1342 riss das Magdalenenhochwasser eine Brücke mit sich. Wohl das größte Hochwasser war am 27. Februar 1784, dem die Häuser am Ufer im Mühlenviertel zum Opfer fielen. Auch die Brücken wurden stark beschädigt. Insbesondere die erst 1756 fertiggestellte Seesbrücke, die heutige Kettenbrücke, mit ihrer barocken Ausstattung wurde durch Eisschollen und mitgerissene Baumstämme zerstört.

Im Stadtgebiet sind Hochwassermarken in der Langen Straße, am Hochzeitshaus, in der Fischerei und an der Walkmühle zu finden. Dort sind auch die Vergleichswerte des letzten großen Hochwassers von 2004 verzeichnet. Weitgehenden Hochwasserschutz bieten seit 1964 das Jahnwehr und das Hochwassersperrtor bei Bug.

Das ehemalige Hochstift Bamberg war gemeinsam mit den Hochstiften Würzburg und Eichstätt sowie in Kurmainz, dem benachbarten protestantischen Fürstentum Bayreuth, der kleinen schwäbischen Herrschaft Wiesensteig und Ellwangen eines der Hauptzentren der frühneuzeitlichen Hexen- und Zaubererverfolgung in Süddeutschland.

In Bamberg wurde 1507 die „Constitutio Criminalis Bambergensis“ in Kraft gesetzt, die unter anderem die Strafe für Hexerei auf Tod durch Verbrennen festlegte:

Infolge lang anhaltender, teils gewalttätiger Machtauseinandersetzungen zwischen Bürgern und dem jeweiligen regierenden Fürstbischof Bambergs, einer durch Missernten in der Kleinen Eiszeit und Kriegseinwirkungen ausgelösten Hungersnot und eines starken persönlichen Hexenglaubens des regierenden Bamberger Fürstbischofs Johann Georg II. Fuchs von Dornheim, genannt der "Hexenbrenner" (1623–1633), erreichten die Verfolgung und Hinrichtung von Personen und ganzer Familien unter dem Vorwurf der Hexerei in Bamberg in den 1620er und frühen 1630er Jahren ihren Höhepunkt. Der Weihbischof Friedrich Förner war der wichtigste Prediger und der eigentliche Scharfmacher der Hexenverfolgung. Johann Georg II. Fuchs von Dornheim errichtete speziell für die Inhaftierung von der Hexerei Beschuldigter im Jahr 1627 das einst im Bereich der heutigen Promenade gelegene sogenannte Drudenhaus, auch Malefizhaus genannt.

Neben zahlreichen anderen Bamberger Bürgern (beispielsweise Dorothea Flock und Christina Morhaubt, Georg Haan, Kanzler im Hochstift Bamberg) und Mitgliedern des Domkapitels wurde im August 1628 unter dem Vorwand der Hexerei auch der Bürgermeister der Stadt Bamberg Johannes Junius im Drudenhaus festgesetzt. Dieser schrieb dort vor seiner Hinrichtung in seinem Abschiedsbrief an seine Tochter:

Nach einer Liste mit den Namen der Opfer wurden bis 1632 weit über 300 Menschen in Bamberg als Hexen oder Hexer hingerichtet. Aus überlieferten Prozessakten geht hervor, dass von 1595 bis 1631 in drei Wellen über 880 Personen der Hexerei oder Zauberei angeklagt und hingerichtet wurden. Erst der Einmarsch schwedischer Truppen (1630–1635) im Februar 1632 setzte dem Treiben des Bischofs und seiner Häscher ein Ende. Fürstbischof Johann Georg II. Fuchs von Dornheim floh nach Oberösterreich und starb dort 1633.

Die Hochzeit der Bamberger Hexenverfolgung ist durch die in großem Umfang, wenn auch sicherlich lückenhaft erhaltenen Prozessakten gut dokumentiert. Der wichtigste und bei weitem größte Quellenbestand befindet sich in der Staatsbibliothek Bamberg. Kleinere Konvolute sind erhalten im Stadtarchiv Bamberg (als Depositum des Historischen Vereins Bamberg), im Staatsarchiv Bamberg und in der "Witchcraft Collection" der Cornell University Library in Ithaca, New York (USA). Aus dem der Hexerei beschuldigten Personenkreis und den Prozessumständen wird deutlich, dass es bei den Bamberger Hexenprozessen in erster Linie um machtpolitische Auseinandersetzungen ging. Fürstbischof Johann Georg II. Fuchs von Dornheim nutzte den Vorwand der Hexerei gezielt zur Ausschaltung machtpolitischer Gegner im Domkapitel sowie im städtischen Bürgertum Bambergs.

Im Oktober 2012 wurden in Bamberg Themenwochen zu den Hexenprozessen veranstaltet, um dieses Kapitel der Stadtgeschichte aufzuarbeiten. 2015 wurde auf einer Freifläche zwischen dem Ludwigskanal und dem Schloss Geyerswörth ein Mahnmal errichtet, geschaffen von den Essener Künstlern Miriam Giessler und Hubert Sandmann. Der Bürgerverein Bamberg-Mitte hatte das Projekt gemeinsam mit der Stadt Bamberg realisiert. Der Stadtrat hatte am 29. April 2015 einen Beschluss zu den Hexenprozessen im Hochstift Bamberg gefasst und einen Text für die Gedenktafel beschlossen: „Im Hochstift Bamberg wurden im 17. Jahrhundert etwa 1000 Frauen, Männer und Kinder unschuldig angeklagt, gefoltert und hingerichtet.“ Finanziert wurde das Mahnmal von der Stadt Bamberg, dem Erzbistum Bamberg, der Oberfrankenstiftung, dem Bürgerverein Bamberg-Mitte und vielen Einzelspendern.

Im ZDF-Spielfilm "Die Seelen im Feuer" nach dem gleichnamigen Roman von Sabine Weigand wird die Zeit der Hexenverfolgungen in Bamberg aufgegriffen.

Die 1647 gegründete Universität wurde im Zuge der Säkularisation des Hochstifts Bamberg durch die bayerischen Besatzer 1803 aufgehoben, bestand aber in reduzierter Form als Philosophisch-Theologische Hochschule fort. Erweitert mit allen universitären Fakultäten nahm sie im Wintersemester 1946/1947 in der Hoffnung, sich zur vierten bayerischen Landesuniversität entwickeln zu können, den Lehrbetrieb auf. Energischer Initiator war der damalige Rektor Benedikt Kraft. Er berief namhafte Professoren, die bisher in Königsberg oder Breslau gelehrt hatten, sowie Richter vom Reichsgericht in Leipzig. Viele aus dem Krieg zurückgekehrte ehemalige Soldaten begannen hier ihr Studium, das von Semester zu Semester erweitert wurde. Die Ausbaupläne zerschlugen sich dann, Regensburg wurde die vierte bayerische Landesuniversität. 1972 wurde die Bamberger Hochschule als Gesamthochschule wiedergegründet und 1979 zur Universität erhoben. Die nach ihren Gründern benannte Otto-Friedrich-Universität Bamberg beherbergt geistes- bzw. kultur-, human-, sozial- und informationswissenschaftliche Fakultäten.

Im Jahr 2009 wurde aufgrund von Sparmaßnahmen der Bayerischen Staatsregierung der Fachbereich Soziale Arbeit aufgelöst und in die Hochschule Coburg integriert. Die Gründungsfakultät Katholische Theologie wurde ebenfalls stillgelegt und zum Institut für Katholische Theologie innerhalb der Fakultät für Geistes- und Kulturwissenschaften umgewandelt.

Am 25. Mai 2009 erhielt die Stadt den von der Bundesregierung verliehenen Titel Ort der Vielfalt.

In der Zeit vor der Säkularisation und den Gebietsreformen war der Fränkische Reichskreis, dem das Bistum Bamberg Truppen stellte, für die Verteidigung Bambergs zuständig. Es waren Teile der Regimenter Hohenlohe und Ferntheil sowie Truppen der fränkischen Kreisartillerie. Bamberg war relativ schlecht befestigt und wurde im Siebenjährigen Krieg dreimal von preußischen Verbänden eingenommen, unter anderem, um den Bamberger Fürstbischof zur Neutralität zu zwingen, der am kaiserlichen Hof viel Einfluss hatte.

Bambergs „Hausregimenter“ waren das 5. Infanterie-Regiment (seit 1855) und das 1. Ulanen-Regiment mit dem Spitznamen Sekt-Ulanen (seit 1872) der Bayerischen Armee. Zu Beginn des Ersten Weltkriegs waren die Bamberger Reiter an dem Gefecht bei Lagarde beteiligt. Beide Regimenter standen bis 1918 in Bamberg. Nach 1919 war es Garnison für das Reiter-Regiment 17 der Reichswehr. Die Wiederaufrüstung unter dem NS-Regime brachte Kasernenneubauten und die Stationierung von Teilen der 4. Panzer-Division der Wehrmacht mit sich. Nach 1945 befand sich kein deutsches Militär mehr in Bamberg. Stattdessen übernahm die US Army die Kasernen bis zu ihrem Abzug im September 2014.

Planungen für die entstandene Konversionsfläche werden öffentlich diskutiert. Im Januar 2014 wurde zwischen der Stadt Bamberg und der Bundesanstalt für Immobilienaufgaben ein Gestattungsvertrag geschlossen, der es ermöglicht, fünf Teilflächen aus der Konversionsfläche schon im März 2015 an die Stadt zu übergeben.

Anfang Oktober 2014 wurde die „Nato-Siedlung“ des ehemaligen US-Armeegeländes mit 149 Wohneinheiten zur zivilen Nutzung übergeben. Eine Erschließung durch die Stadt findet bereits statt.

Im September 2015 wurde eine zweite Ankunfts- und Rückführungseinrichtung neben Manching in Bayern auf dem Gelände eröffnet, die zur Abschiebung von Balkanflüchtlingen mit geringer Bleibeperspektive dient.

Am 18. Juli 2016 wurde die Aufnahmeeinrichtung Oberfranken (AEO) auf klassische Asylbewerber ausgeweitet, in der Endausbaustufe wird sie auf 3400 Plätze ausgebaut. Derzeit sind mit Stand zum 1. Dezember 2016 1077 Plätze mit Flüchtlingen/Asylbewerbern belegt.

Am 1. September 2016 hat die Bundespolizei eine Schule auf dem Konversionsgelände errichtet, die große Teile in naher Zukunft in Anspruch nimmt. Der ursprünglich geplante Stadtteil mit 8000 Einwohnern durch die Stadt Bamberg wurde dadurch hinfällig.

Am 14. Februar 2017 erwarb die Stadt Bamberg die ehemalige US-Lagarde-Kaserne im Osten der Stadt, in der Nähe der vierspurigen Hauptverkehrsstraße Berliner Ring. Dort sollen Wohnungen, Arbeitsplätze und ein IT-Campus entstehen.

Die 1908 bis 1910 erbaute Bamberger Synagoge wurde während der Novemberpogrome 1938 zerstört und die Arisierung der jüdischen Wirtschaftsbetriebe abgeschlossen. Ab 1939 wurden die Bamberger Juden zur Zwangsarbeit meist in kommunalen Bereichen eingesetzt. Ab November 1941 begann man, die in Bamberg lebenden Juden zu deportieren. Der jüdische Friedhof wurde enteignet und das Taharahaus an die Firma Bosch vermietet, die es als Lagerhalle verwendete. Bis Mai 1945 blieben lediglich 15 Juden, die in sogenannten Mischehen lebten. Insgesamt fielen ca. 630 in Bamberg geborene oder längere Zeit dort wohnhafte Juden durch Deportation und Ermordung dem Holocaust zum Opfer. Am 14. April 1945 wurde Bamberg von Truppen der US Army eingenommen. Von deutscher Seite gab es nur geringen militärischen Widerstand, der aber amerikanischen Artilleriebeschuss zur Folge hatte. Insgesamt kamen dabei 23 deutsche Soldaten und vier Zivilisten ums Leben.

Am 1. Januar 1970 wurden die Ortsteile Kramersfeld und Bruckertshof der Gemeinde Hallstadt ins Stadtgebiet eingegliedert. Am 1. Juli 1972 folgten im Rahmen der Gebietsreform die Gemeinden Bug, Gaustadt, Wildensorg, der Ortsteil Bughof der Gemeinde Strullendorf und der Ortsteil Hirschknock der Gemeinde Gundelsheim.

Im Mittelalter und in der frühen Neuzeit wuchs die Einwohnerzahl von Bamberg nur langsam und ging durch die zahlreichen Kriege, Seuchen und Hungersnöte immer wieder zurück. Während des Dreißigjährigen Krieges sank sie auf 7.000 im Jahre 1648. Vor dem Krieg waren es noch 12.000. Mit dem Beginn der Industrialisierung im 19. Jahrhundert beschleunigte sich das Bevölkerungswachstum. 1811 lebten 17.000 Menschen in der Stadt, 1900 waren es bereits 42.000.

Bis 1939 stieg die Bevölkerungszahl auf 59.000. Kurz nach dem Zweiten Weltkrieg brachten die vielen Flüchtlinge und Vertriebenen aus den deutschen Ostgebieten der Stadt innerhalb weniger Monate einen Zuwachs um 16.000 auf 75.000 Einwohner im Dezember 1945. Im Jahre 1953 stieg die Einwohnerzahl auf den historischen Höchststand von 77.000. Bis Juni 1972 sank sie wieder auf 69.000. Eingemeindungen am 1. Juli 1972 brachten einen Zugewinn von 7.207 auf über 76.000 Einwohner. Am 30. Juni 2006 betrug die Amtliche Einwohnerzahl für Bamberg nach Fortschreibung des Bayerischen Landesamtes für Statistik und Datenverarbeitung 70.063 (nur Hauptwohnsitze und nach Abgleich mit den anderen Landesämtern). Seit 2009 wuchs die Einwohnerzahl jährlich und lag am 31. Dezember 2016 bei 75.743.

Die folgende Übersicht zeigt die Einwohnerzahlen nach dem jeweiligen Gebietsstand. Bis 1811 sind es meist Schätzungen, danach Volkszählungsergebnisse (¹) oder amtliche Fortschreibungen des Statistischen Landesamtes. Die Angaben beziehen sich ab 1871 auf die „ortsanwesende Bevölkerung“, ab 1925 auf die Wohnbevölkerung und seit 1987 auf die Bevölkerung am Ort der Hauptwohnung. Vor 1871 wurde die Einwohnerzahl nach uneinheitlichen Erhebungsverfahren ermittelt.

A¹ Volkszählungsergebnis
A² Bamberg schlägt Bayreuth
A³ Flüchtlinge und Uni: Bambergs Einwohnerzahl steigt

Die Kommunalwahl am 16. März 2014 führte in Bamberg zu folgendem Ergebnis:




Die Stadt Bamberg unterhält Partnerschaften mit folgenden Städten:
Auch Kirchdorf an der Krems in Österreich, Malborghetto, Montelabbate (beide in Italien), Nagaoka (Japan), Posen in Polen, Qufu in China, Tarvisio in Italien und Wolfsberg in Österreich sind „befreundete Städte“.

1958 übernahm die Stadt Bamberg für die aufgrund der Beneš-Dekrete aus ihrer Heimat vertriebenen Sudetendeutschen aus dem Gebiet um die tschechische Stadt Troppau die Patenschaft. Auch Baunach und Hallstadt in Bayern sind Patenstädte.






Bamberg wurde wie Rom auf sieben Hügeln (Stephansberg, Kaulberg, Domberg, Michaelsberg, Jakobsberg, Altenburg, Abtsberg) erbaut und deshalb auch manchmal als Fränkisches Rom bezeichnet.
Da Bamberg im Zweiten Weltkrieg weitgehend von Bombardierungen verschont blieb, bietet die Altstadt bis heute das nahezu unveränderte Bild der ursprünglichen Dreigliederung in geistliche Bergstadt (im Umfeld des Kaiserdoms), bürgerliche Inselstadt (zwischen den beiden Flussarmen der Regnitz) und der Gärtnerstadt. Überragt vom Dom stellt die Stadt ein denkmalgeschütztes Ensemble zwischen mittelalterlicher und barocker Baukunst dar.

1993 wurde die Altstadt in die Liste des Weltkultur- und Naturerbes der Menschheit der UNESCO aufgenommen. In der Begründung heißt es, Bamberg repräsentiere in einzigartiger Weise die auf einer Grundstruktur des Frühmittelalters entwickelte mitteleuropäische Stadt. Das in die UNESCO-Welterbeliste aufgenommene Areal umfasst die drei historischen Stadtzentren Berg-, Insel- und Gärtnerstadt mit einer Gesamtfläche von etwa 140 Hektar.

Die deutsche 100-Euro-Gedenkmünze aus Gold des Jahres 2004 aus der Reihe "UNESCO-Welterbe" zeigt eine historische Stadtansicht.

Umfangreiche Aktivitäten im Bereich der Denkmalpflege haben zu einem guten Restaurierungszustand geführt. Im Juli 2005 richtete die Stadt Bamberg ein eigenes "Dokumentationszentrum Welterbe" als Koordinierungs- und Anlaufstelle zu allen Belangen des UNESCO-Welterbes ein.

Die Lage einiger Bamberger Kirchen bildet ein imaginäres Kreuz. Der Längsbalken besteht aus den Kirchen St. Jakob, Dom, St. Martin und St. Gangolf, der Querbalken aus St. Stephan, Obere Pfarre, Dom und St. Michael. Letztere vier Kirchen sind auch als der sogenannte Vierkirchenblick bekannt. Der Mittelpunkt dieses Kreuzes ist eine 1777 eingelegte Säule, die den Namen "Tattermannsäule" trug. An ihre Stelle trat zum tausendjährigen Jubiläum des Bistums Bamberg im Jahr 2007 ein „unterirdisches“ Denkmal, das ein Künstler aus Israel – mit familiären Wurzeln in Bamberg – schuf.

Die etwas außerhalb des Querbalkens stehende Kirche St. Jakob wird als das gesenkte Haupt Christi gedeutet.

Ein weiteres imaginäres Kreuz bildet der Grundriss des Doms; das gesenkte Haupt Christi wird durch einen außerhalb der Linie angeordneten Scheitelstein im Westchor des Domes dargestellt.

Die im Jahr 1993 in die Liste des Weltkultur- und Naturerbes der Menschheit aufgenommene Stadt Bamberg besitzt in der Kernstadt über 1200 Baudenkmale.











Die Immunitäten waren geistliche Frei- oder Sonderbezirke der Stifte mit Ummauerung und eigener niederer Gerichtsbarkeit, in denen ab dem 12. Jahrhundert fast die Hälfte der Bamberger Bevölkerung lebte.









Im Bamberger Berggebiet gibt es ein recht ausgedehntes System von Katakomben und Felsenkellern, deren Ursprünge zum Teil bis in das Mittelalter zurückreichen. Während der Pest- und Cholera<nowiki>epidemien</nowiki> im 13. und 14. Jahrhundert wurden einige Stollen auch als Grabanlagen verwendet. Unter dem Lerchenbühl wurde um 1500 eine Felsenkapelle angelegt, das "Heilige Loch." In der Regel allerdings angelegt als kühle, aber frostgeschützte Lagerkeller für Lebensmittel und Getränke, vor allem Wein und Bier, unter dem Kaulberg als Sandschürfstollen zur Gewinnung von Scheuersand, dienten sie im Lauf der Zeit verschiedenen Zwecken (Zufluchträume bei kriegerischen Auseinandersetzungen, Versammlungsstätten, Gefängnis, Trinkwasserversorgung, Verlagerung industrieller Produktionsstätten während des Zweiten Weltkriegs). Heute dienen einige der Stollen als Lagerräume, Zivilschutzräume sowie als touristische Attraktion der Stadt. Befahrungen der Stollen unter dem Stephansberg sind daher möglich und werden von der Stadt Bamberg organisiert.


Der Basketballverein Brose Bamberg wurde 2005 (als GHP Bamberg), 2007, 2010, 2011, 2012, 2013, 2015, 2016 (als Brose Baskets) und 2017 Deutscher Basketball-Meister, in den Jahren 1993 (als TTL Bamberg), 2003 (als TSK uniVersa Bamberg) und 2004 (als GHP Bamberg) Vizemeister und 1992 (als TTL Bamberg) sowie 2010, 2011, 2012 (als Brose Baskets) und 2017 Deutscher Pokalsieger. Aufgrund der fanatischen Anhänger des Vereins und der breiten Begeisterung innerhalb der relativ kleinen Stadt ist Bamberg bei den deutschen Basketball-Fans unter der Bezeichnung „Freak-City“ bekannt.

Erfolgreichster Fußballverein ist (historisch gesehen) der 1. FC Eintracht Bamberg, welcher 2006 aus der Fusion des 1. FC 01 Bamberg und dem TSV Eintracht Bamberg entstand. Nach zwei Jahren in der Bayernliga stieg der Verein 2008 in die Regionalliga Süd auf, musste 2016 jedoch ebenfalls Insolvenz anmelden. 2017/18 tritt man in der Bezirksliga Oberfranken West an. Die erfolgreichste Zeit des Vorgängervereins 1. FC 01 Bamberg lag jedoch in den 1950er-Jahren. Zwischen 1990 und 1993 errang auch der zweitälteste Fußballverein SC 08 Bamberg einige bemerkenswerte Erfolge (Bayernliga und Achtelfinale im DFB-Pokal 1991/92). Aktuell (2017/18) vertritt die DJK Don Bosco Bamberg den Fußballsport Bambergs in der Bayernliga Nord.

Die 1. Männermannschaft der SKC 1947 Victoria Bamberg ist neunfacher deutscher Meister im Kegeln, sechsmal Pokalsieger des DKBC, dreimal Champions-League-Sieger, fünfmal Europapokalsieger und viermal Weltpokalsieger. Die 1. Frauenmannschaft der SKC Victoria ist neunmal deutscher Meister, fünfmal Pokalsieger des DKBC, einmal Europapokalsieger, fünfmal Champions-League-Sieger und viermal Weltpokalsieger.

Die Schachspieler vom SC 1868 Bamberg waren dreimal Deutscher Meister (1966, 1976, 1977) und einmal Deutscher Pokalsieger (1983/84).

Die UW-Rugby-Mannschaft des TC Bamberg (Tauchclub) war 2004 Deutscher Vize-Meister, wurde von 2007 bis 2016 zehnmal in Folge Deutscher Meister sowie 2008, 2011 und 2013 außerdem Vize-Champions-Cup-Sieger im Unterwasser-Rugby.

Der Bridgeclub "Bamberger Reiter" gewann 1994, 1998, 2003, 2006–2010 und 2013–2015 die Team Bundesliga. Weitere Erfolge: Deutscher Teammeister 2005, 2006 und 2008, Gewinner des Europäischen Championscup der Landesmeister 2006 (Sieg am 15. Oktober 2006 in Rom; 2007: 3. Platz; 2008: 4. Platz und 2009 2. Platz). 2008 stellte Bamberg mit Wilhelm Gromöller den Deutschen Meister der Senioren.
Durch den 3. Platz bei den Europameisterschaften 2008, bei denen Bamberg die deutsche Nationalmannschaft stellte, war die Mannschaft als erste deutsche Herrenmannschaft überhaupt für die Weltmeisterschaften 2009 qualifiziert und belegte den 3. Platz im Transnations Cup. Sabine Auken (geb. Zenkel) ist 3x Weltmeisterin, 2x Vizeweltmeisterin und bei den Damen seit vielen Jahren unter den besten Spielerinnen der Welt.
Auch 2017 spielen mit Sabine Auken und Michael Gromöller wieder zwei Bamberger mit um die Weltmeisterschaft. Der Bamberger Club ist damit der erfolgreichste deutsche Bridgeclub der letzten Jahre.

Seit der Saison 2009/2010 ist der VC Franken mit einer Herren-Volleyballmannschaft in Bamberg zuhause. Die Heimspiele des Vereins, der in der Deutschen Volleyball-Bundesliga der Herren spielt und auch am DVV-Pokal teilnimmt, finden in der Stechert-Arena statt.


Spezialitäten Bambergs sind die "Bamberger Hörnla", womit sowohl ein Croissant-artiges Gebäck als auch eine Kartoffelsorte bezeichnet werden. Eine weitere Spezialität ist der "Zwätschgabaamäs", (übersetzt etwa "Zwetschgenbäumernes") ein luftgetrockneter Rinderschinken, der seinen Namen der Räucherung mit Zwetschgenholz verdankt. Des Weiteren ist die Stadt Bamberg auch für das fränkische Schäuferla sowie für seine Biertradition und das Rauchbier bekannt.

In Bamberg wird seit dem frühen sechzehnten Jahrhundert Süßholz angebaut. Heutzutage ist Bamberg der einzige Ort im Norden Europas, an dem diese Pflanze noch kultiviert wird. Diese Tradition wird von der Bamberger Süßholzgesellschaft gepflegt, die die Bamberger Gärtner beim Anbau unterstützt.

Im Zukunftsatlas 2016 belegte die kreisfreie Stadt Bamberg Platz 32 von 402 Landkreisen, Kommunalverbänden und kreisfreien Städten in Deutschland und zählt damit zu den Orten mit „sehr hohen Zukunftschancen“.

Im Jahr 2014 gab es in Bamberg etwa 50.253 (Stichtag: 30. Juni 2014) sozialversicherungspflichtige Beschäftigte. Wichtigste Industriebranche ist die Kfz-Zulieferindustrie, gefolgt von der Elektrotechnik und dem Ernährungsgewerbe. Ende Mai 2014 hat der Automobilzulieferer Brose Fahrzeugteile mit dem Bau eines Bürogebäudes mit Sozialtrakt in Bamberg begonnen und will bis März 2016 insgesamt 600 Arbeitsplätze schaffen. Der traditionelle Wirtschaftszweig der Gemüsegärtner, der die Stadt seit ihren Anfängen über Jahrhunderte prägte, ist nach wie vor vorhanden. Daneben spielt der Tourismus für die Wirtschaft der Stadt eine wichtige Rolle. Darüber hinaus existieren in Bamberg zahlreiche kleinere und mittelständische Unternehmen anderer Branchen. Eine weitere Besonderheit ist die seit Jahrhunderten gepflegte Tradition des "Orgelbaus", die zurzeit von dem Meisterbetrieb Thomas Eichfelder fortgesetzt wird.

Die wichtigsten gewerblichen Arbeitgeber in der Stadt mit jeweils mehr als 400 Beschäftigten sind:


Die Wasserversorgung der Stadt war früher durch Brunnen gesichert.
Es handelte sich hierbei überwiegend um Brunnen auf Privatgrund, von denen es ca. 300 gab. Des Weiteren versorgten öffentliche Brunnen, gebaut und unterhalten durch die öffentliche Hand, die Stadt mit Wasser. Daneben gab es bereits eine Art Fernwasserversorgung mit Leitungen aus Brunnen in der Umgebung, die vor allem von kirchlichen und klösterlichen Einrichtungen genutzt wurde.

Der Ausbau eines städtischen Rohrnetzes begann im letzten Viertel des 19. Jahrhunderts. Seit den 1970er Jahren ist Bamberg – neben seinen eigenen Wasserbrunnen – an die Fernwasserversorgung Oberfranken (FWO) angeschlossen. Diese speist seit Frühjahr 1975 Wasser aus der Ködeltalsperre in das städtische Wasserversorgungsnetz ein.

Seit dem Jahr 2003 findet die Fernwasserversorgung ausschließlich über den Zweckverband Wasserversorgung Fränkischer Wirtschaftsraum (WFW) statt. Das Wasser wird ab dem Übergabe-Behälter in Hüttendorf (Stadt Erlangen) an das Stadtnetz der Stadt Bamberg geliefert.

Die Stadt ist Teil der Bierregion Franken an der Grenze zwischen Wein- und Bierfranken. Besondere Spezialität ist das Rauchbier. Von ehemals 68 historischen Braustätten gibt es noch acht Brauereien mit alter Tradition in Bamberg: Brauerei Mahr, Brauerei Fässla, Brauerei Schlenkerla, Brauerei Spezial, Klosterbräu Bamberg, Brauerei Greifenklau, Brauerei Keesmann und die Brauerei Kaiserdom im ehemals selbständigen Stadtteil Gaustadt. Eine Gasthaus-Brauerei, das Ambräusianum, die Röstmalzbierbrauerei und die Versuchsbrauerei der Mälzerei Weyermann eröffneten 2004. Seit 2016 besteht mit dem "Kronprinz" in Gaustadt eine weitere Gasthausbrauerei in Bamberg. Bis 2008 existierte noch die Maisel-Bräu. In der Stadt Bamberg bestehen daher derzeit elf Brauereien, die Bier verkaufen. Daneben gibt es noch die kleine zollrechtlich zugelassene Brauerei Robesbierre, die aber kein Bier verkauft.

Im Jahre 1907 gab es den so genannten Bamberger Bierkrieg, bei dem ein Boykott der Bevölkerung die Brauereien zwang, die Bierpreiserhöhung von zehn auf elf Pfennig wieder rückgängig zu machen.

Bamberg ist mit einem Binnenhafen am Main-Donau-Kanal (Regnitz), zwei Autobahnen (A 70/E48 und A 73) und einem Bahnhof ins Verkehrsnetz eingebunden. Außerdem verfügt die Stadt über einen Sonderlandeplatz für Flugzeuge (ICAO-Kennung:EDQA). Zum 1. Januar 2010 trat die Stadt dem Verkehrsverbund Großraum Nürnberg bei.

Der Bahnhof Bamberg ist nördlicher Endpunkt der S-Bahn-Linie S1 des Verkehrsverbunds Großraum Nürnberg (VGN). Damit ist Bamberg im Nahverkehr direkt mit Erlangen, Fürth und Nürnberg verbunden. Die Züge fahren im Stundentakt bis ins 100 km südöstlich gelegene Hartmannshof. Mit dem versetzt fahrenden Regionalexpress fahren tagsüber jeweils zwei, zu den Pendlerzeiten teilweise auch drei Züge pro Stunde, von Bamberg nach Nürnberg und zurück.

23 Buslinien und 4 Nachtbuslinien decken nahezu das ganze Stadtgebiet ab und bedienen den Nahverkehr in die angrenzenden Gemeinden Bischberg, Gundelsheim, Hallstadt, Memmelsdorf, Stegaurach und den Ort Schammelsdorf. Außerdem verkehren in Bamberg einige Regionalbuslinien des Omnibusverkehrs Franken GmbH und weiterer privater Omnibusunternehmen. Am Zentralen Omnibusbahnhof (ZOB) sind auch Haltestellen für Regionalbusse eingerichtet, um ein leichteres Umsteigen zu ermöglichen. Der ZOB ist der Mittelpunkt des Stadtnetzes.

Insgesamt befinden sich derzeit 60 Stadtbusse im Fuhrpark des Verkehrsbetriebes der Stadtwerke Bamberg, sie transportieren im Jahr durchschnittlich 10,2 Millionen Fahrgäste. Seit dem Wintersemester 2004/2005 erhalten alle Studenten der Universität Bamberg ein Semesterticket. Es gilt in allen Bussen und den Nahverkehrszügen der DB Regio und der Agilis in der Stadt und im Landkreis Bamberg.

Zwölf stark frequentierte Bushaltestellen wurden mit den sogenannten dynamischen Fahrgastinformationseinrichtungen versehen.
Zum Fahrplanwechsel 2015 zeigen sie auch die wirkliche Ankunft der Busse in Echtzeit an.

Seit dem 23. März 2011 werden die Haltestellen von Kinderstimmen angesagt.

Von 1897 bis 1922 führte die Elektrische Straßenbahn Bamberg AG den Stadtverkehr durch.

Im Fernverkehr liegt Bamberg an der Nord-Süd-Strecke Leipzig – Nürnberg und wird im Zwei-Stunden-Takt von ICE-Zügen angefahren. Bis zur Fertigstellung der Schnellfahrstrecke Nürnberg–Erfurt führte diese über Saalfeld und Jena, seit dem 10. Dezember 2017 wird die Verbindung über Erfurt geführt. Bamberg ist Endpunkt der Bahnstrecke Nürnberg–Bamberg, des Weiteren beginnen in Bamberg die Bahnstrecke Bamberg–Hof sowie die Bahnstrecke Bamberg–Rottendorf.

Der Bahnhof Bamberg ist der Ausgangspunkt von Regionalexpress- und Regionalbahnlinien nach

In der Nähe des Bahnhofs halten mehrere Fernbuslinien. Betreiber ist Flixbus.

Die vierspurige Hauptverkehrsstraße Berliner Ring (St2244), die im Osten durch die Stadt verläuft, ist die Verlängerung zur B22.

Bamberg ist an folgende Bundesstraßen angeschlossen:

Bamberg liegt an folgenden Bundesautobahnen:

In der Bamberger Innenstadt wird aufgrund der relativ flachen Topographie zwischen Main-Donau-Kanal und Regnitz traditionell viel Fahrrad gefahren. Die relativ kompakte Innenstadtfläche, kurze Distanzen zwischen den Universitätsstandorten, Einbahnstraßen, Parkplatzknappheit und die schnellere Erreichbarkeit des Naherholungsgebietes im Hain begünstigen die Nutzung des Fahrrades als alltägliches Transportmittel.

Durch die unmittelbare Lage am Main-Radweg und der sogenannte „RegnitzRadweg“ ist Bamberg auch Ziel vieler touristischer Radreiserouten.
Die Stadt hat sich zum Ziel gesetzt, eine Veränderung des Modal Splits zugunsten des Radverkehrs zu erreichen.

Das überörtlich ausgeschilderte Radwegenetz weist besonders im Altstadtbereich erhebliche Lücken auf. Der ADFC Bamberg sieht Nachbesserungsbedarf in der städtischen Verkehrspolitik.

Die Fahrradmitnahme in den DB-Regiozügen und den Bussen des VGN ist kostenpflichtig.

Auf dem Weg zu größerer urbaner Fahrradfreundlichkeit erhielt die Stadt im Jahr 2009 den Zuschlag beim Bundesmodellversuch Zero-Emission-Mobility des Bundesverkehrsministeriums. Im Sommer 2009 erfolgte dessen multimediale Umsetzung mit der Werbekampagne "Kopf an: Motor aus. Für Null CO auf Kurzstrecken."

In Bamberg befindet sich die Bundeswasserstraße Main-Donau-Kanal, auch RMD- oder Europakanal genannt, der historisch als Nachfolger des Ludwig-Donau-Main-Kanals zu betrachten ist.

Der Flugplatz Bamberg-Breitenau ist einer der ältesten noch in Betrieb befindlichen Landeplätze Deutschlands. Bereits 1912 landete dort das erste Motorflugzeug. Das Jahr 1909 wird als tatsächlicher Beginn der Luftfahrt in Bamberg angesehen. Zu diesem Zeitpunkt begann Willy Messerschmitt zusammen mit dem Stadtbaumeister Friedrich Harth mit der Entwicklung von Fluggeräten. Von 1945 bis 2012 diente der Flugplatz als amerikanischer Militärflugplatz mit ziviler Mitbenutzung. 2013 wurde er nach umfangreichen Umbaumaßnahmen als deutscher Sonderlandeplatz wiedereröffnet.

Insgesamt erscheinen in Bamberg bzw. haben in Bamberg das Verbreitungsgebiet 13 Zeitschriften und Zeitungen verschiedensten Formates und Gattungen: Wochenzeitungen, Tageszeitungen, Kirchenmagazine, Kulturzeitschriften und Studentenzeitungen, die entweder von privaten Verlagen oder von der Stadt und Landkreis Bamberg verlegt werden.

Der "Fränkische Tag" ist eine der größten Tageszeitungen Oberfrankens und hat seinen Sitz in Bamberg. Schwerpunkt liegt auf die lokale Berichterstattung inklusive eines regionalen und überregionalen Sportteils sowie eines Feuilletons. Nachrichten aus Deutschland, Europa und der Welt werden durch die Nachrichtenagentur dpa und durch Korrespondenten im In- und Ausland geliefert. Der Fränkische Tag geht auf die fürstbischöfliche Hofdruckerei des Kronacher Druckers Georg Andreas Gertner zurück.

Das Wochenblatt Bamberg ist eine lokale Wochenzeitung, die sich ausschließlich auf die Berichterstattung aus Bamberg und der Region konzentriert. Berichtet wird über Politik, Wirtschaft, Sport und Bamberger Ereignissen. Gegründet wurde die Zeitung 1981 in Bamberg als "Wochenblatt Bamberg" und erhielt im Volksmund den Namen "Wobla" genannt, was dazu führte, dass die Zeitung ihren heutigen Namen erhielt.

Das Sportecho ist eine, seit November 2014 zweimonatlich erscheinende kostenlose Sportzeitschrift. Das Verbreitungsgebiet ist Bamberg und Umkreis. Der thematische Schwerpunkt liegt auf der regionalen Sportwelt und Berichterstattung über die sportliche Jugend. Des Weiteren existiert eine Rubrik, die bedeutende Persönlichkeiten der Bamberger Sportszene näher beleuchtet.

2012 unter dem Namen "Art. 5|III" (Name mit Slogan: "Art. 5|III - der Lieferant für Kunst und Kultur"; auch geschrieben: "Art.5/III") in Bamberg gegründet, erscheint seitdem die Zeitschrift zweimonatlich - jährlich in sechs Ausgaben - im Rheinischen Format in einer Auflage von 20 000 Exemplaren. Das Verbreitungsgebiet ist die Stadt und der Landkreis Bamberg, die Metropolregion Nürnberg, ganz Unterfranken und Thüringen. Als einzige Bamberger Zeitung beleuchtet sie gezielt kulturelle Phänomene von Kunst bis Kulturpolitik. Der Schwerpunkt liegt sowohl auf dem lokalen nordbayerischen Raum als auch auf Berichten und Artikeln über das nationale Kunst- und Kulturgeschehen aus den Metropolen Berlin oder München. Neben der regelmäßigen Berichterstattung über die Bamberger Kurzfilmtage oder das Erlanger Poetenfest führte die Zeitschrift immer wieder Interviews mit prägenden Persönlichkeiten der internationalen Kulturszene: Zu nennen sind die Geigerin Anne-Sophie Mutter, der Chefdirigent der Prager Philharmoniker Jakub Hrůša, die Bratschistin und Trägerin des Frankfurter Musikpreises Tabea Zimmermann, die Schriftstellerin und P.E.N. Mitglied Tanja Kinkel sowie die zeitgenössischen Sänger Max Herre, Milow und Joris.

Die Vorläufer Radio Regnitzwelle und das Jugendradio Fun Boy Radio nahmen am 10. Oktober 1987 auf der Bamberger UKW-Frequenz 88,5 MHz ihren Sendebetrieb auf. Da die beiden Sender für sich allein nicht genug Zuhörer anzogen, wurde am 1. Juli die Fusion beschlossen und kurze Zeit später umgesetzt. Das Programm war anfangs auf Hits der 1980er Jahre konzentriert. Inzwischen wurde das Musikspektrum auf die 1970er Jahre bis heute erweitert. Des Weiteren umfasst das Programm Weltnachrichten zur vollen Stunde, Regionalnachrichten zur halben Stunde, Verkehrsmeldungen, Service und Comedy. Verantwortet wird der Sender von der Mediengruppe Oberfranken.

Der Sender ist in Bayern und Teilen Hessens empfangbar. Der Sitz liegt in Regensburg.
Das Programm richtet sich mit seinem „Young-CHR“-Format, d. h. Black Music, Hip-Hop, Dance Music, House Music und Popmusik vor allem an 14- bis 26-Jährige. Betrieben wird Radio Galaxy von der „Digitale Rundfunk Bayern GmbH & Co. KG“, programmbeauftragt ist die „Funkhaus Regensburg GmbH & Co. Studiobetriebs KG“.

TV Oberfranken ist ein bayerischer Regionalsender mit Sitz in Hof/Saale der "TV Oberfranken GmbH & Co. KG". Das Sendegebiet umfasst den gesamten Regierungsbezirk Oberfranken. Neben der Hauptsendeanstalt in Hof unterhält der Sender noch Regionalstudios in Bamberg, Bayreuth und Coburg.

Bamberg verfügt über einen Stadtfeuerwehrverband. Die Freiwillige Feuerwehr Bamberg besteht aus 11 Abteilungen. Diese sind eine Ständige Wache, eine Spezialeinheit des ehemaligen Katastrophenschutzes, der ABC-Zug (Löschgruppe 51) und neun weitere Löschgruppen. Eine weitere Sondereinheit des ehemaligen Katastrophenschutzes war der Technische Zug/Ölwehr (Löschgruppe 21), der allerdings zum 31. Dezember 2013 aufgelöst und in die Löschgruppe 3 eingegliedert wurde. Der Großteil der Fahrzeuge des ABC-Zuges und des ehemaligen Technischen Zuges tragen die seltene orangefarbene Katastrophenschutzlackierung. Darüber hinaus existiert eine Unterstützungsgruppe Örtliche Einsatzleitung (UG ÖEL) Bamberg-Stadt.

Darüber hinaus existiert in Bamberg noch ein Ortsverband des Technischen Hilfswerks (THW). Dieser Ortsverband besteht aus dem Stab, einem Technischen Zug mit einer Fachgruppe Räumen, einer Fachgruppe Wassergefahren und einer Fachgruppe Wasserschaden/Pumpen sowie weiteren intern beschafften Gerätschaften. Es existiert zudem eine Jugendgruppe. Zudem ist in Breitengüßbach bei Bamberg die Geschäftsstelle für den THW-Geschäftsführerbereich Bamberg ansässig.

Der Rettungsdienst in Bamberg wird durch zwei Rettungswachen (eine vom BRK und eine vom Malteser Hilfsdienst) sichergestellt. Des Weiteren gibt es mehrere Notarztstandorte.

Für den zivilen Katastrophenschutz sind mehrere Einheiten der Hilfsorganisationen BRK, Malteser und JUH aktiv. Der Kreisverband Bamberg des Bayerischen Roten Kreuzes stellt zwei SEG Behandlung, eine SEG Betreuung sowie Facheinheiten UG SanEL, Technik und Sicherheit. Weiterhin wird die Sanitätseinsatzleitung (SanEL) durch organisatorische Leiter und Leitende Notärzte gestellt.

Als Sitz eines Oberlandesgerichts, eines Landgerichts, eines Amtsgerichts, eines Arbeitsgerichts und einer Justizvollzugsanstalt ist Bamberg ein überregional bedeutsamer Gerichtsstandort.


In der Stadt gibt es elf Seniorenheime, die von verschiedenen Trägern betrieben werden.

1953 wurde das Stadionbad eröffnet, das im September 2001 renoviert wurde.

Das Hainbad bietet seit 1972 die Möglichkeit im linken Regnitzarm zu schwimmen, dazu gehört eine Holzliegefläche.

Durch die Eingemeindung der Gemeinde Gaustadt nach Bamberg im Jahre 1972 kam das Freibad Gaustadt (Einweihung: 1956) in der Badstraße 17 hinzu.

Ende 2011 wurde das „Bambados“ in Bamberg eröffnet, ein modernes Freizeit- und Sportbad mit Wellness- und Saunalandschaft.

Die Otto-Friedrich-Universität Bamberg gehört mit mehr als 13.000 Studenten zu den mittelgroßen Universitäten Bayerns. Die Gebäude der Universität sind über das gesamte Bamberger Stadtgebiet verteilt. Ein großer Teil liegt aber im Kern der Bamberger Altstadt. Sprach- und Literaturwissenschaften nehmen zum Teil Gebäude ein, die vorher zum Kaiser-Heinrich-Gymnasium gehörten. In den Altstadtstandorten befinden sich neben der Verwaltung die beiden Fakultäten der Geistes- und Kulturwissenschaften (GuK) und der Humanwissenschaften (Huwi). In der Feldkirchenstraße befinden sich das Rechenzentrum und die Sozial- und Wirtschaftswissenschaftliche Fakultät (SoWi). Dort befand sich auch die Fakultät für Wirtschaftsinformatik und Angewandte Informatik (WIAI), bis sie im Sommer 2012 in die neuen Gebäude auf der ERBA-Insel umzog. Die umfangreiche Universitätsbibliothek besitzt eine Zentralbibliothek, fünf Teilbibliotheken und die Zweigstelle ERBA-Bibliothek. Die Otto-Friedrich-Universität ist Mitglied des Netzwerkes Mittelgroßer Universitäten und wurde als Familiengerechte Hochschule und als Partnerhochschule des Spitzensportes ausgezeichnet. Sie gehört zu den führenden Universitäten für Sozial- und Wirtschaftswissenschaften sowie Psychologie in Bayern.

Die private Fachhochschule des Mittelstands unterhält seit der Übernahme des Lehrbetriebs zum 1. September 2013 von der Hochschule für angewandte Wissenschaften Bamberg einen Standort in Bamberg. Ihr Verwaltungssitz befindet sich in Bielefeld. Mit ihrem Angebot ist die Fachhochschule die einzige Hochschule in Bayern, die die beiden therapeutischen Berufe der Physiotherapie und Logopädie im Rahmen eines grundständigen Studiums anbietet.

Das Bamberger Institut für Erdmessung – ein Forschungsinstitut für Höhere Geodäsie auf dem Domberg – wurde 1945 von der US-Heeresvermessung gegründet und bestand bis Anfang der 1950er-Jahre, als es in das Frankfurter Institut für Angewandte Geodäsie eingegliedert wurde. Seine Hauptaufgabe war die Fertigstellung des im Dritten Reich begonnenen Zentraleuropäischen Dreiecksnetzes über Mitteleuropa, das 1949 fertiggestellt war. Weitere Großprojekte waren das ED50-Koordinatensystem und eine astro-geodätische Geoidbestimmung Mittel- und Westeuropas. Es gab auch Kooperationen mit der Bamberger Remeis-Sternwarte.

Erster Direktor war Prof. Erwin Gigas, unter dem auch die Schriftenreihe "Veröffentlichungen des Instituts für Erdmessung" gegründet wurde.


Zwei Realschulen (darunter eine reine Mädchenschule), eine Wirtschaftsschule, Volks- und Förderschulen, fünf berufliche Schulen, zehn Privatschulen, Volkshochschule Bamberg, Bildungszentrum der Industrie- und Handelskammer, Berufliche Fortbildungszentren der Bayerischen Wirtschaft (bfz), Erzbischöfliches Abendgymnasium für Berufstätige, Katholische Erwachsenenbildung in der Stadt Bamberg e. V. (KEB), neun berufliche Weiterbildungs- und Qualifizierungsstellen, die Städtische Musikschule, von-Lerchenfeld-Schule / Privates Förderzentrum, Förderschwerpunkt Hören (ehemalige Taubstummen-Anstalt) mit Internat.

Das Aufseesianum wurde 1738 von Domkapitular Jodocus Bernhard Freiherr von Aufseß gestiftet. Hier wurde 1973 der Roman "Das fliegende Klassenzimmer" von Erich Kästner verfilmt.

Das Maria-Ward-Internat in Trägerschaft der Englischen Fräulein (ab 1717) wurde 2011 geschlossen.

Zwei weitere kirchliche Internate wurden Ende des 20. Jahrhunderts geschlossen: das Ottonianum (1866–1999, in diözesaner Trägerschaft) und das Marianum (1918–1988, in Trägerschaft der Karmeliten).

Bamberg ist Drehort bekannter Kinofilm- und Fernsehproduktionen wie z. B.:

Der Film "Engelchen oder Die Jungfrau von Bamberg" thematisiert die in den späten 1960er Jahren empfundenen Gegensätze zwischen der fränkischen „Provinz“ und dem „freizügigen“ Schwabing.

Seit August 2008 gibt es ein eigenständiges Fernsehen für Bamberger Schulen.

Der hauptamtliche Bürgermeister wurde im Jahr 1818 erstmals von den Bürgern gewählt. Seit 1917 ist die Amtsbezeichnung Oberbürgermeister.

Die Stadt Bamberg verlieh bisher an 35 verdienstvolle Personen die Ehrenbürgerschaft.



Neben der Ehrenbürgerschaft vergibt die Stadt Bamberg noch weitere Ehrungen an verdiente Bürger, die „allgemeines Ansehen genießen“ und sich „besondere Verdienste erworben haben“:

Hier sind die Plaketten und Medaillen in Bezug auf Bamberger Persönlichkeiten und Ereignisse aufgeführt.

Im Jahr 2009 waren 40.891 Menschen in Bamberg römisch-katholisch, was einem Anteil von 58,5 Prozent entspricht. Der Anteil der evangelischen Bürger belief sich im selben Jahr auf 13.520 (19,4 Prozent). Unter Sonstige hat die Stadt 15.416 Personen gezählt (22,1 Prozent), dazu zählen nach Auskunft der Pressestelle der Stadt Bamberg „Juden, Moslems, Kirchenlose bzw. Angehörige anderer Religionen“. Die Gesamtzahl der Bamberger Bevölkerung betrug 2009: 69.827 Personen.

Insgesamt existieren in Bamberg folgende Religionsgemeinschaften:
Wegen seiner Lage auf sieben Hügeln wird Bamberg auch als Fränkisches Rom bezeichnet.

Klein Venedig nennt sich die ehemalige Schiffer- und Fischersiedlung an der Regnitz; siehe auch den Abschnitt Palais und profane Bauten dieses Artikels.

Die Einwohner Bambergs haben auch den Spitznamen "Zwiebeltreter". Er geht auf die Bamberger Gärtner zurück, bei denen das Zwiebeltreten eine wichtige Tätigkeit im Zwiebelanbau war. Sie mussten die Schalotten bei einer bestimmten Größe umtreten, damit das Kraut nicht zu sehr in die Höhe wuchs, sondern die Nährstoffe für das Wachstum der unterirdischen Zwiebel genutzt wurden. Die Bauern banden sich dazu kleine Brettchen an die Schuhe, um nicht versehentlich auf die Zwiebeln zu treten, und gingen damit auf die Felder. Ein ähnlicher Spitzname ist "Zwiebelfranken".

Mehrere kleine Auswanderergruppen aus Bamberg ließen sich in der ersten Hälfte des 18. Jahrhunderts in verschiedenen Orten in der Gegend von Posen nieder, wo sie als Bamber bezeichnet und bald auch geschätzt wurden.

Ein Marskrater mit einem Durchmesser von 58,3 km sowie der Asteroid (324) Bamberga, der 1892 von Johann Palisa an der Universitätssternwarte Wien entdeckt wurde, sind nach Bamberg benannt.

Die Bamberger Hörnchen, eine alte Kartoffelsorte aus Franken, ist nach Bamberg benannt.





</doc>
<doc id="11036" url="https://de.wikipedia.org/wiki?curid=11036" title="Wald">
Wald

Wald (Waldung) im alltagssprachlichen Sinn und im Sinn der meisten Fachsprachen ist ein Ausschnitt der Erdoberfläche, der mit Bäumen bedeckt ist und die eine gewisse, vom Deutungszusammenhang abhängige Mindestdeckung und Mindestgröße überschreitet. Die Definition von Wald ist notwendigerweise vage und hängt vom Bedeutungszusammenhang (alltagssprachlich, geographisch, biologisch, juristisch, ökonomisch, kulturell …) ab. Präzisere Definitionen decken jeweils nur einen Teil des Bedeutungszusammenhangs ab. Eine in der deutschen Forstwissenschaft verbreitete Definition definiert Wald als eine Pflanzenformation, die „im Wesentlichen aus Bäumen aufgebaut ist und eine so große Fläche bedeckt, dass sich darauf ein charakteristisches Waldklima entwickeln kann“. Nach § 2 des deutschen Bundeswaldgesetzes ist ein Wald „...jede mit Forstpflanzen bestockte Grundfläche. Als Wald gelten auch kahlgeschlagene oder verlichtete Grundflächen, Waldwege und Lichtungen“.

Das Wort "Wald" (althochdeutsch "walt") beruht auf einem rekonstruierten urgermanischen "*walþu" ‚Büschel‘, in diesem Fall ‚Laubwerk‘, ‚Zweige‘, das seinerseits aus indogermanisch "*wolɘt" ‚dichtbewachsen‘ hervorgegangen sein könnte. Auch eine Verwandtschaft zu lateinisch "vellere" ‚rupfen‘ (vgl. "Wolle") ist möglich.

Der umgangssprachliche Begriff Wald deckt sich in den typischen Fällen auch mit den fachlichen Definitionen. Zu den Bedeutungsrändern hin wird der Begriff unscharf und umfasst Flächen und Vegetationsformen, die je nach Auffassung und verwendeter Definition entweder als Wald gelten können oder nicht. Bei einem weltweiten Überblick wurden allein in juristischem Zusammenhang 63 voneinander verschiedene, nationale Definitionen von „Wald“ gezählt, für den für die Definition wesentlichen Begriff „Baum“ 149 Definitionen. Wichtig ist die Abgrenzung zum Beispiel gegenüber Plantagen aus Baumarten (zum Beispiel auch Energieholz-Plantagen, Ölpalmen-Plantagen,) baumbestandenen Parks und Grünanlagen, zumindest teilweise baumbestandenem Weideland (im englischen Sprachraum unter „rangeland“ gefasst, z. B. auch Almen) und offenen, zum Beispiel durch Beweidung oder Übernutzung degradierten, nur teilweise baumbestandenen Flächen, aber auch natürlicherweise teilweise offenen Baumsavannen.

Neben zahlreichen anderen, teilweise metaphorischen Verwendungen (wie zum Beispiel „Tangwald“) sind vier Bedeutungszusammenhänge wesentlich. Zu beachten ist, dass nach jeder dieser Definitionen Flächen als Wald definiert werden können, die nach den anderen nicht als solcher gelten würden:

International bedeutsame Walddefinitionen sind zum Beispiel:

Die Definition der FAO schließt jedoch Baum-Plantagen beispielsweise von Eukalypten nicht aus, die ökologisch weitgehend wertlos sind. Gegen diese Walddefinition regt sich daher unter Nichtregierungsorganisationen heftiger Widerstand. Die Organisationen Timberwatch, Rettet den Regenwald und andere haben daher während des World Forestry Congress 2015 in Durban eine Petition an die FAO übergeben, die Definition zu ändern.


Weltweit treten Wälder als Waldgesellschaften in Gebieten mit einer (je nach Temperatur) bestimmten minimalen Niederschlagsmenge auf. Fällt weniger Niederschlag, geht der Wald in eine Trocken-Savanne oder Steppe über. Für Hochlagen und kalte Klimate ist die Dauer der Vegetationsperiode für den Erfolg der Vegetation entscheidend. Ab einer bestimmten Höhe bzw. geografischen Breite gibt es eine Waldgrenze, jenseits derer kein Wald mehr wachsen kann und nur vereinzelt (verkrüppelte) Bäume vorkommen. Ihr folgt die Baumgrenze.

Wälder sind komplexe Ökosysteme. Mit optimaler Ressourcenausnutzung sind sie das produktivste Landökosystem. Nach den Ozeanen sind sie die wichtigste Einflussgröße des globalen Klimas. Sie stellen gegenüber anderen Nutzungsformen global die einzig wirksame Kohlendioxid­senke dar und sind die wichtigsten Sauerstoff­produzenten. Sie wirken ausgleichend auf den globalen Stoffhaushalt. Ihr Artenreichtum ist ein unschätzbarer Genpool, dessen Bedeutung zunehmend auch in der Industrie erkannt wird.

Innerhalb der Vegetationszonen der Erde bilden sich mit Überlagerung der Orobiome (siehe Höhenstufen) verschiedene Waldformen aus. In den Grenzbereichen des Lebens, bei starker Trockenheit oder Kälte, gehen die Wälder in Savannen, Tundren oder Wüsten über. Die ausgedehntesten Waldgebiete der Erde sind die tropischen Regenwälder um den Äquator und die borealen Wälder der kalten bis gemäßigten Gebiete der Nordhalbkugel (Finnland, Sibirien, Kanada).

Diese Ökosysteme sind naturbelassen weder ein zeitlich starres noch ein räumlich homogenes Gebilde. Entgegen der weitverbreiteten Meinung sind auch die zusammenhängenden rezenten „Urwälder“ (die Regenwälder, aber auch die heimischen Buchenwälder) ein Mosaik aus zonaler, azonaler und extrazonaler Vegetation, deren einzelne Flächen ("Patches") zudem auch einer zeitlichen Entwicklung unterworfen sind.

Die unterschiedlichen Einflüsse, zeitliche Faktoren als Grundlage der Waldentwicklung sowie die resultierende Schlusswaldgesellschaft werden im Mosaik-Zyklus-Konzept und der Megaherbivorentheorie diskutiert.

Ökologisch lässt sich eine Einteilung nach Sukzessionsstadien vornehmen: Das Mosaik-Zyklus-Konzept beschreibt die Formen der potenziell natürlichen Waldentwicklung. Zu einer vollständigen Artenausstattung (Flora und Fauna) von Klimaxwaldgesellschaften bedarf es Jahrhunderte ununterbrochener Bestockung. Auch die durch menschliche Nutzung eingestellten Bestandsformen lassen sich in natürlich vorkommende Sukzessionsstadien einordnen.

Die Megaherbivorentheorie misst den großen Pflanzenfressern eine größere Bedeutung in der Waldentwicklung zu. Wie groß ihr Einfluss auf die Vegetation wäre ohne Bejagung durch Menschen, aber mit Bejagung durch die in Mitteleuropa ausgestorbenen oder ausgerotteten Fleischfresser (Karnivoren), ist umstritten.

Zwischen den Wendekreisen der Sonne, in tropischen Klimaten, bildet sich bei entsprechender Feuchteversorgung durch Regen (1800–2000 mm) eine Vielfalt von verschiedenartigen Regen- und Nebelwäldern aus. Ein regionsweise hoher Anteil kann dabei im sogenannten „Kleinen Wasserkreislauf“ aus der Verdunstung des Waldes selbst entstammen, soweit diese Waldflächen eine gewisse Größe nicht unterschreiten.

Ein ganzjähriges Wachstum haben tropische Regenwälder, die die artenreichsten Landökosysteme der Erde sind. Schätzungsweise 70 % aller landgebundenen Arten dieser Erde leben in der tropischen Regenwaldzone. Für diese Produktivität spielt der Boden eine entscheidende Rolle. Die meisten tropischen Regenwälder stehen auf Laterit­boden. Dieser ist sehr unfruchtbar, weil er kaum Nährstoffe speichert. In Einflussbereichen des sauren und sauerstoffarmen Schwarzwassers (zum Beispiel am Rio Negro) gedeihen Schwarzwasserwälder. Es gibt Tiefland-Regenwälder und Regenwälder in mittleren Höhenlagen.

Mit zunehmender Höhe gehen in diesem Klima die Regenwälder in Nebel- oder Wolkenwälder über. In einem Wolkenwald wachsen zahlreiche Epiphyten. Dieser üppige Bewuchs wird nur noch von echten Bergnebelwäldern übertroffen, die in den feuchtheißen Tropen ab 2000 m über dem Meer anzutreffen sind. Hier findet man vor allem Hautfarne.

Oberhalb der echten Bergnebelwälder gehen tropische Wälder ab 3100 m Höhe (in Afrika am Kilimandscharo) oder ab 4000 m Höhe in den Anden in einen niederwaldartigen Bewuchs über. Mit zunehmender Höhe beginnt der hochandine Bereich über der Baumgrenze, der Paramo.

In der Gezeiten­zone tropischer Küsten wachsen Mangrovenwälder, die allerdings von einem starken Rückgang betroffen sind. Die Flora der Mangrovenwälder beschränkt sich auf eine verhältnismäßig kleine Anzahl von Mangrovenbaumarten mit speziellen Anpassungen an die schwierigen Lebensbedingungen dieses Lebensraums (z. B. Salinität, periodische Überflutung oder Brandung). Die höchste Diversität beobachtet man im indopazifischen Raum; Westafrika und Amerika beherbergen nur eine geringe Anzahl von Mangrovenbaumarten. In Richtung auf die nördlichen oder südlichen Verbreitungsgrenzen geht die Artenzahl weiter zurück, so kommt z. B. am Sinai (Ägypten) oder im nördlichen Neuseeland nur eine Art der Gattung "Avicennia" ("Avicennia marina") vor. Trotz der Artenarmut der Flora nutzt eine Vielzahl von Tieren die Mangrovenwälder.

Als Übergänge zu den Regenwäldern bilden sich die Saisonregenwälder, die in mehr oder weniger regelmäßigeren Abständen nicht durch Regen bewässert werden. Sie wachsen in Gebieten, die noch meistens niederschlagsreich sind, aber schon eine kürzere Trockenzeit aufweisen.

In den Subtropen bilden sich unter dem Einfluss von Jahreszeiten in der Nähe der Wendekreise die Monsunwälder und Passatwälder, die von den mit den namensgebenden Winden herangetragenen Regengüssen bewässert werden. Diese Regenzeitwälder haben keine typische Form, sind sehr variabel und prägen sich je nach Dauer der Trockenheit aus. Sie werfen unter normalen Umständen nicht durch Trockenheit deutlich Laub ab.

Trockenkahle Wälder gedeihen in Gebieten mit länger anhaltenden jährlichen Trockenzeiten und werfen in solchen vollständig ihr Laub ab. Sie grenzen an Passat- und Monsunwälder einerseits und an Dornwälder andererseits. Sie werden häufig bewirtschaftet und sind durch die Nachfrage an Teak und Mahagoni schon nicht mehr in ihrem natürlichen Zustand. Die Afrikanische Variante der trockenkahlen Wälder heißt Miombo.

Bei länger anhaltenden Trockenzeiten können in Venezuela, Brasilien, Indien und Nepal und Afrika nur noch Dornwälder gedeihen. Sie bestehen aus Schirmakazien, Mimosen- und Caesalpinaceen-Arten. Die trichterförmigen Kronen der Bäume stehen schütter und fangen den geringen Sommerregen auf. Einige Dornwälder sind auch durch die menschliche Nutzung aus trockenkahlen Wäldern entstanden.

Bei weiter abnehmenden Niederschlagsmengen entstehen Sukkulentenwälder und schließlich die Savanne. Neben der Beweidung, der Brandrodung und dem Holzfällen des Menschen üben Termiten einen Einfluss auf die Wälder der Subtropen aus.

In dieser Zone finden sich sowohl Hartlaubwälder als auch Laubwälder warm-feuchter Klimate. Erstere sind geprägt durch Hartlaubvegetation, also immergrüne Pflanzen mit Anpassungen an lange Phasen der Trockenheit im Sommer. Man findet diesen Bereich z. B. am Mittelmeer. Eine typische Baumart in solchen Wäldern ist die Steineiche.

Laubwälder warm-feuchter Klimate wachsen an der Ostseite der Kontinente mit kräftigen Monsunregen im Sommer und hohen Temperaturen; außerdem bei sommertrocken-winterfeuchtem Klima, wenn eine regelmäßige Wolkenbildung die Sommertrockenheit abschwächt.
Die boreale Nadelwaldzone umfasst einen Bereich von 1,4 Milliarden ha (14 Millionen km²) bzw. etwa ein Drittel der Gesamtwaldfläche der Erde. Etwa 150 Millionen ha davon sind jedoch, bedingt durch Sturm oder Feuer, vorübergehend nicht bestockt. Die boreale Klimazone schließt sich an den Süden der arktischen Tundra an und umfasst eine Nord-Süd-Ausdehnung von 700 km in Europa und Nordamerika sowie bis zu 2000 km in Sibirien. Die West-Ost-Ausdehnung umfasst das gesamte Eurasien von Norwegen bis Kamtschatka, dazu Kanada. Sie ist somit das ausgedehnteste geschlossene Waldgebiet der Erde. Boreale Wälder existieren nur auf der Nordhalbkugel. Die Vegetation wird in der Baumschicht von Koniferen dominiert, insbesondere Sibirische Lärche, Fichten, Zirbelkiefer und Gemeine Kiefer.

Wälder kommen ihrem natürlichen "(ahemeroben)" Zustand umso näher, je weniger ihre Baumartenzusammensetzung durch kulturelle menschliche Einfluss verändert ist und je weniger ihre Zusammensetzung und Organisationsweise von der zusätzlichen Zufuhr von Energie in die biologischen Produktionsprozesse über die einstrahlende Sonnenenergie hinaus abhängig ist.

"Urwälder" (auch: "Primärwälder") sind die natürlichsten Waldökosysteme. Sie sind nach Definition der FAO ("Food and Agriculture Organization of the United Nations") Waldgebiete, die eine natürliche Vegetation aufweisen, ohne sichtbaren menschlichen Einfluss sind und deren natürliche Dynamik ungestört verläuft. Weltweit entsprachen im Jahr 2005 etwa 36 % aller Wälder diesen Kriterien, wobei auch bereits wiederhergestellte Wälder dieser Definition der FAO genügen können. Die Fläche wird um jährlich 6 Millionen ha reduziert.

Umgangssprachlich versteht man unter Urwald häufig nur den tropischen Regenwald, der Begriff bezieht sich aber auch auf die Taiga der Nordhalbkugel, auf Bergwald, auf Busch weltweit und auf viele andere Waldformationen.

Nach der strengen IFL-Definition (siehe Kartenerläuterung) sind nur noch 21 % der Wälder der Erde unberührte Urwälder. Trotz weltweiter Anstrengungen zum Schutz der Urwälder ging ihre Fläche von 2000 bis 2014 um rund 8 % zurück. Davon entfallen fast 45 % auf die tropischen Regenwälder (vor allem im Amazonasbecken, zudem im Kongobecken und in Südostasien). 44 % liegen in der kaltgemäßigten Zone in den borealen Nadelwäldern. Die restlichen 11 % sind über die anderen Klimazonen verteilt, wobei die gemäßigten Laubwälder den geringsten Flächenanteil aufweisen. 64 % aller IFL liegen in den drei Ländern Kanada, Russland und Brasilien. Global gelten 19 % der IFL als geschützt, streng geschützt sind jedoch nur 10 %.

Nach den IFL-Kriterien gelten in Europa noch 6,4 % als intakte, natürliche Waldökosysteme. Weltweit stellen sie damit weniger als 3 % der Urwälder. Die weitaus größten Urwälder Europas (über 90 %) befinden sich in der Taiga Nordrusslands (westlich des Urals). Außerhalb Russlands existieren die größten Urwaldgebiete in Skandinavien, dort vor allem am Fuße der Skanden in Schweden (rund 1,4 Millionen ha nach IFL-Standard). Gegenüber den außereuropäischen Urwäldern sind jedoch auch diese Wälder vergleichsweise winzig.

Die vorgenannten Urwaldareale sind fast ausschließlich boreale Nadelwälder oder Gebirgswälder, lediglich 1 % der intakten Naturwälder liegen als „Urwaldrelikte“ in den Laub- und Mischwäldern der warmgemäßigten Klimazone. Da die Konzepte über die Wiederbewaldung und Theorien über die potentielle natürliche Vegetation nicht widerspruchsfrei sind, ist es darüber hinaus schwierig, einen Urwald in den seit Jahrhunderten dicht besiedelten und stark genutzten Regionen Europas zu definieren. Besser eignet sich dazu das Mosaik-Zyklus-Konzept, das heute häufiger herangezogen wird.


In Vorderasien befindet sich am östlichen Rande des Buchenareals zwischen Elburs-Gebirge und Kaspischem Meer der letzte großflächige, über Relikte hinausgehende Urwald, der teilweise die potentiell natürlichen Waldgesellschaften Mitteleuropas repräsentiert: 300.000 ha von 2.000.000 ha im Iran wurden noch 1998 als „unberührt“ bezeichnet (Kaspischer Hyrcania-Mischwald).

Ein Wald kann als verhältnismäßig naturnah gelten, wenn die Baumpopulation einheimisch und die Zusammensetzung gänzlich oder annähernd natürlich ist. Trotzdem sind solche Wirtschaftswälder ökonomischen Zielsetzungen unterworfen, die eine Festlegung des Erntealters lange vor Erreichen der natürlichen Altersgrenze herbeiführen. In Mitteleuropa sind solche Wälder den Standorten entsprechend oft durch Buchen geprägt, durch Bergmischwälder, Edellaubholz und Kiefern. Nicht autochthone Eichen-Wirtschaftswälder können noch als verhältnismäßig naturnah gelten.

Solche Wälder sind gekennzeichnet durch fremdländische Baumarten mit oder ohne künstlich herbeigeführte eingeschränkte genetische Vielfalt oder durch Baumarten, die an gegebene Standorte nicht angepasst sind. In vielen Gebieten sind dies Kiefern-Fichten-Mischwälder und Mischwälder unter der Beteiligung von Lärche. Noch naturferner sind ungemischte Fichtenkulturen und Lärchenbestände der planaren und kollinen Stufe der Mittelgebirge, in den Alpen sind reine Fichtenwälder über der Fichten-Tannen-Stufe, und darüber die Lärchenstufe heimisch.

Plantagenwälder stellen die naturfernsten Waldsysteme der Erde dar. Sie bestehen in der Regel aus nur einer einzigen schnellwüchsigen Baumart (oft Eukalypten und bestimmte Kiefern wie beispielsweise Monterey-Kiefer). Plantagen stellen eine Übergangsform zur Landwirtschaft dar und sind gekennzeichnet durch eine intensive Bodenbearbeitung, den regelmäßigen Einsatz von Düngemitteln und Pestiziden und sehr kurze Umtriebszeiten von oft weniger als 10 Jahren. Das Pflanzenmaterial wird durch Züchtungen konstant verbessert. Die Züchtungen werden in der Regel massenhaft geklont. Unter anderem kommen auch genetisch modifizierte Organismen zum Einsatz.

Plantagen zeichnen sich durch eine vergleichsweise sehr hohe Rentabilität (nicht selten im Bereich von 15–20 %) aus; in Mitteleuropa sind Plantagen selten vorzufinden. Pappelkulturen gewinnen jedoch als Energieträger an Bedeutung. Geregelte Forstwirtschaft findet besonders in Ländern der südlichen Hemisphäre beinahe ausschließlich in Form von Plantagen statt.

Umweltschützer warnen, dass Baum-Plantagen ökologisch weitgehend wertlos sind, das Grundwasser schädigen und zu sozialen Konflikten führen. Gegen diese „Fake Forests“ bzw. „planted forests“ regt sich daher unter Nichtregierungsorganisationen heftiger Widerstand. Die Organisationen Timberwatch, Rettet den Regenwald und andere haben daher während des World Forestry Congress 2015 in Durban eine Petition an die FAO übergeben, die Definition von Wald zu ändern.

Wälder erfüllen im Wesentlichen drei Gruppen von Kernfunktionen: die ökonomischen (wirtschaftlicher Nutzen), die ökologischen (Schutz des Lebensraums, der Lebensgrundlagen) und die sozialen Funktionen (Erholung/Freizeitraum). Manche dieser Funktionen werden durch den Wald ohne Zutun des Menschen erbracht (beispielsweise die Erzeugung von Sauerstoff), andere werden erst durch die Leistungen der Forstwirtschaft ermöglicht (z. B. Waldwege, die auch das Fahrradfahren ermöglichen). Hinzu kommen noch einige Sonderfunktionen. Die Realisierung der vielfältigen Funktionen obliegt dem Besitzer des Waldes. Werden alle Funktionen gleichzeitig, ausreichend und ohne Verlust ihrer Grundlage sowie Regenerationsfähigkeit erbracht, so spricht man von nachhaltiger Forstwirtschaft. Für das Jahr 1997 wurden die jährlich weltweit erbrachten Waldfunktionen auf einen Wert von 4,7 Billionen US-Dollar geschätzt. Das entsprach damals etwa einem Viertel des weltweiten Bruttosozialprodukts.

Durch diese Vielfalt der Anforderungen kommt es bei Bewirtschaftung und sonstigen Nutzungen zu Konflikten zwischen verschiedenen Interessengruppen (die Regelung dieser Konflikte ist die Aufgabe der Forstpolitik). Streitpunkte sind hierbei oft, inwieweit ein Waldbesitzer tatsächlich zur alleinigen Erbringung (oft unentgeltlicher) Leistungen durch sein Eigentum verpflichtet ist.

Welche Funktionen der Wald zu erfüllen hat, ist bereits ein erster Gegenstand von Diskussionen. Auf internationaler Ebene werden dazu Vereinbarungen zwischen Staaten unter der Beteiligung von Interessengruppen getroffen. Der Katalog der Waldfunktionen wird dabei kontinuierlich erweitert. Nach dem Schema der Ministerkonferenz zum Schutz der Wälder in Europa müssen Wälder derzeit (Februar 2008) 17 Aspekte bzw. Funktionen berücksichtigen.

Es wird in der wirtschaftlichen Nutzung des Waldes unterschieden:

Die phytogenen (pflanzlichen) Ressourcen – wie z. B. Holz – gehören zu den nachwachsenden Rohstoffen.

Seit der Urgeschichte des Menschen (Jäger und Sammler) werden Bestandteile des Ökosystems Wald als natürliche Ressource genutzt. Neben Tieren zählen dazu auch Wildpflanzen wie Beeren, Kräuter, Faserpflanzen sowie Pilze oder Sekrete wie Baumharz (Pech) und Ähnliches, Waldweide für Fütterungszwecke von Kulturtieren, Zeidlerei sowie Fallholz als Brennmaterial. Daneben entwickelte sich schon früh die Nutzung der lebenden Bäume als Brenn-, Werk- und Baustoff, aus der sich zu Beginn des 18. Jahrhunderts aufgrund einer absehbaren Holznot die Forstwirtschaft als Konzept zur nachhaltigen Nutzung entwickelte (bis zur Entdeckung fossiler Energieträger war das Holz aus dem Wald der wichtigste Energieträger). Damit wurde – vor dem Hintergrund einer ungeregelten, vernichtenden Übernutzung – der Wald ggf. zum Forst.

Historisch betrachtet haben die Wälder weltweit einen starken Wandel bezüglich ihrer Nutzung und Ausprägung erlebt. Je nach Nutzungsart und -intensität bilden sich innerhalb eines Waldsystems Ersatzgesellschaften aus, die in dichter besiedelten Regionen die Regel darstellen dürften.

Die Forstwirtschaft erbringt auch Dienstleistungen (das genannte Beispiel des Waldwegebaus, die Sicherung dieser Wege) und Güter, die jedoch von den Nutznießern normalerweise nicht bezahlt werden müssen, da eine gesetzliche Grundlage dafür fehlt, oder weil die Märkte nicht existieren. Es liegt somit Marktversagen vor. Dies betrifft insbesondere die CO-Speicherung und -sequestrierung, Tourismus und Naherholung sowie (besonders im Falle tropischer Regenwälder) genetisches Material. Auch die Erbringung von Boden-, Luft- und Wasserschutzfunktionen und der Erhalt von Biodiversität werden in der Regel nicht vergütet.

Zu ökologischen Problemen bei der zunehmenden Intensivierung der Waldbewirtschaftung können unter anderem eine übermäßige Abfuhr von Biomasse aus dem Wald sowie Bodenverdichtungen durch Forstmaschinen führen. So werden zunehmend Schwach- und Resthölzer zur Gewinnung von Hackschnitzeln genutzt und damit dem Ökosystem als Nährstoff- und Humuslieferant entzogen. Dies kann zu Nährstoffmangel beim Neuaufwuchs führen. Bei der Holzernte werden immer schwerere Maschinen verwendet. Die infolgedessen auftretenden Bodenverdichtungen können die Struktur des empfindlichen Waldbodens fast irreparabel schädigen.

Der Waldbestand gehört zu den wichtigen mikroklimatischen Faktoren.

Wald schützt den Boden, auf welchem er wächst, auf vielfältige Weise vor Bodenerosion. Das Kronendach vermindert die kinetische Energie von Regentropfen, ebenso der Stockwerkbau des Waldes. Die Durchwurzelung bewirkt eine Festigung des Bodens. Typische Beispiele sind der Mangrovenwald im Küstenschutz oder Wälder in Bereich von Wüstenbildung und Verkarstung.

Der Lawinen-, Steinschlag- und Murenschutz ist eine Waldfunktion, die nur im steileren Gelände relevant ist. Der Entstehung von Lawinen wird stark vorgebeugt, herabbrechende Lawinen werden durch Wald in ihrer Wucht gebremst und fangen einen großen Teil der Schneemasse ab. Die regulative Kraft des Waldes auf Gesteins- und Erdbewegungen beruht in einer Kombination von Durchwurzelung und dem Puffern der erosiven Kräfte von Wasser (Niederschlag, Versickerung, Wasserabfluss).

Neben dem Schutz vor der erosiven Kraft von Wasser haben die Wälder enorme Bedeutung für den Wasserkreislauf der Erde und die Verfügbarkeit von Trinkwasser und Wasser für die künstliche Bewässerung sowie Energiegewinnung durch Wasserkraft. Wälder können Wasser länger und in größerer Menge zur Verfügung stellen, als eine vergleichbare Freifläche. Oberflächenabfluss von Regenwasser wird minimiert, ähnlich wie ein Schwamm wird Wasser im Boden gespeichert. Die Evaporation sinkt aufgrund der Beschattung des Bodens durch die Vegetation (allerdings steigt die Transpiration).

Unter Immissionsschutzfunktionen versteht man die Filterung der Luft von Aerosolen aller Art sowie von Giftstoffen oder auch Radioaktivität. Einen wichtigen Beitrag zum Wasserschutz leisten Wälder, indem sie Wasser in gleicher Weise säubern.

Daneben verhindert ein Waldbestand auch weitgehend Erosion durch Wind (Bodenabtragung).

Zum Immissionsschutz zählen auch dämpfende Wirkungen in Bezug auf Licht und Schall. Für die Befindlichkeit des Menschen kann die Sichtschutzfunktion von Wäldern relevant sein. Ebenso können junge belaubte Wälder Lärm um etwa die Hälfte im Vergleich zu Freiflächen reduzieren. Andererseits können Altbestände die Lärmausbreitung erhöhen, da sich unter dem geschlossenen Kronendach der Schall wie in einer Halle ausbreitet.

Wälder sind vergleichsweise wenig intensiv genutzte Flächen. Der Eintrag von Düngemitteln und Pestiziden ist im Wald normalerweise geringer als in der Landwirtschaft. Auch ist der Stress durch Lärm und andere Reize vermindert. Deshalb stellen Wälder ein letztes Rückzugsgebiet für scheue Tiere dar. Ausgeprägte Waldtiere wie der Feuersalamander bezeichnet man als silvicol. Wie jedes andere Ökosystem gibt es aber auch im Wald Tier- und Pflanzenarten, die an das Leben dort speziell angepasst sind. Der Wald muss also Artenschutzfunktionen im Rahmen des Naturschutzes erfüllen. Bezüglich des Schutzes der Artenvielfalt stellt die natürliche Wiederbewaldung, wie auch beim Tourismus und beim Landschaftsschutz (siehe unten), manchmal jedoch auch ein Problem dar: Offene extensiv genutzte Flächen oder Brachland wird von Bäumen wiederbesiedelt. Ohne einen menschlichen Eingriff würden diese offenen Landschaften langfristig verschwinden. Dies bedeutet eine Habitatverarmung und einen Verlust an Biodiversität, da viele Pflanzen und Tiere nur auf Wiesen leben.

Der Wald bindet in seiner Biomasse Kohlenstoffdioxid (CO), das hauptverantwortlich für die derzeit beobachtete globale Erwärmung ist. Bei der Fotosynthese entziehen die grünen Pflanzen der Luft dieses Gas und setzen dafür Sauerstoff (O) frei. Insgesamt sind weltweit etwa 862 Mrd. Tonnen Kohlenstoff in Wäldern gebunden, der sich sowohl in der Vegetation selbst als auch in den Böden befindet. Etwa 471 Mrd. Tonnen Kohlenstoff sind in tropischen Wälder gespeichert, 272 Mrd. Tonnen in borealen Wäldern und 119 Mrd. Tonnen in Wäldern der gemäßigten Breiten, zu denen auch der Großteil der europäischen Wälder zählt. Im deutschen Wald sind 2,2 Mrd. Tonnen Kohlenstoff gebunden.

Die positive Kohlendioxidspeicherleistung durch das Baumwachstum kann nur im Verbund mit der sich zeitlich anschließenden Holznutzung umfassend betrachtet werden. Werden Wälder nachhaltig und naturnah bewirtschaftet – wie dies z. B. in den meisten Wäldern Europas geschieht und durch Zertifizierungssysteme gewährleistet wird – wird das vom Baum gebundene Kohlendioxid als Kohlenstoff im Holzkörper gespeichert. Durch die anschließende Holzverwendung wird dieses über einen langen Zeitraum gespeichert. Dies können z. B. bei einem Holzbauwerk 80 Jahre sein, in Einzelfällen auch deutlich darüber. Holzprodukte können sogar wiederverwendet werden, wodurch die im Wald erfolgte Kohlenstofffixierung zeitlich verlängert wird, bevor der gespeicherte Kohlenstoff, z. B. bei der energetischen Holznutzung, als Kohlendioxid wieder an die Atmosphäre abgegeben wird.

Im Rahmen der internationalen Klimaschutzabkommen wie z. B. dem Kyoto-Protokoll (KP) werden auch Wälder aufgrund ihrer Fähigkeit, Kohlendioxid zu binden und Sauerstoff zu produzieren als Klimafaktoren betrachtet. Grundsätzlich werden Wälder als Kohlenstoffsenken angesehen und können in die nationale CO-Bilanz Eingang finden. Dies ist jedoch nur bedingt richtig, weil Wälder vor allem im Wachstum eine reale Kohlenstoffsenke darstellen. Etablierte Wälder hingegen tragen zur Nettokohlendioxidfixierung in geringem Maße bei, ungestörte Urwälder ohne Nettozuwachs gar nichts. Sie stellen aber Speicher für Kohlenstoff dar, der bei ihrer Abholzung als Kohlendioxid freigesetzt wird.
Eine besondere Form von nationalen Minderungsmöglichkeiten, aber auch von JI- und CDM-Projekten (Joint Implementation und Clean Development Mechanism) stellen Senkenprojekte dar. Unter Senken wird prinzipiell die Kohlenstoffbindung und Speicherung in Vegetation und Böden verstanden. Unterschieden wird dabei zwischen Wäldern (Artikel 3.3 KP) und landwirtschaftlich genutzten Flächen (Artikel 3.4 KP). Mögliche Projekttypen sind Aufforstung und Wiederaufforstung, Bewirtschaftungsmaßnahmen auf bestehenden Forst-, Acker- und Grünlandflächen sowie Begrünung von Ödland. Die Freisetzung von Kohlenstoff durch Entwaldung muss allerdings ebenfalls eingerechnet werden. Um Risiken und Möglichkeiten der Senkenanrechnung zu untersuchen, wurde ein Bericht beim Intergovernmental Panel on Climate Change (IPCC) in Auftrag gegeben. Der im Jahr 2000 fertiggestellte Bericht Land use, Land-use change, and Forestry (LULUCF) konstatiert große Unsicherheiten in vielen Bereichen. So bestehen vor allem naturwissenschaftliche Unklarheiten bezüglich der gebundenen CO-Menge. Die Absorptionsraten während des Pflanzenwachstums sowie die Bindungszeiträume sind nur schwer zu bestimmen. Zusammen mit der Problematik der Bestimmung der Bewuchsdichte auf großen Flächen ergeben sich starke Unsicherheiten bei der Hochrechnung der Gesamtmenge. Bei der Speicherung in Böden sind diese Probleme noch gravierender, da die zugrunde liegenden biochemischen Prozesse komplizierter sind und zusätzlich mit stärkeren Freisetzungen von CO und Methan gerechnet werden muss. Über die naturwissenschaftlichen Unsicherheiten hinaus wird vor allem die Kontrolle der Vorschriften als problematisch angesehen. Genaue Regelungen bezüglich der Quantifizierung der Treibhausgasspeicherung und des Monitorings stehen noch nicht fest, sondern sollen vom Intergovernmental Panel of Climate Change entwickelt und vorgeschlagen werden. Trotz der hohen Unsicherheiten und des Widerstandes von einigen Vertragsstaaten wurde auf der Klimakonferenz in Bonn (COP 6b) beschlossen, Senkenprojekte bei der Erfüllung der Verpflichtungen einzubeziehen. Auf der nächsten Konferenz in Marrakesch (COP 7) wurden dann die ersten wichtigen Definitionen und Regelungen für die Anrechenbarkeit von Senken nach Artikel 3.3 und 3.4 vereinbart. Insbesondere die genaue Definition und Abgrenzung des Begriffes ‚Wald‘ wurde festgelegt. Hierbei wurden Bandbreiten für Mindestflächen (0,05–1 ha), die Mindestbewuchsdichte (10 bis 30 %) und die Mindesthöhe (2–5 m) des Pflanzenbewuchses festgelegt, aus denen die verpflichteten Parteien Rahmenwerte für eine nationale Definition des Begriffes ‚Wald‘ wählen müssen. Vor Beginn der ersten Verpflichtungsperiode (d. h. vor 2008) müssen die verpflichteten Staaten festlegen, welche der Bewirtschaftungsmaßnahmen, d. h. Forst-, Ackerland- und Grünlandbewirtschaftung sowie Begrünung von Ödland, für sie unter Artikel 3.4 KP anrechenbar sein sollen. Für Aufforstung und Wiederaufforstung ist keine Festlegung notwendig. Senkenprojekte im Inland generieren Emissionsreduktionsgutschriften, sogenannte Removal Units (RMU), die nicht in die nächste Verpflichtungsperiode übertragen werden können. Zudem unterliegen sie in der ersten Verpflichtungsperiode gewissen Einschränkungen bezüglich ihrer Anrechenbarkeit. So können Bewirtschaftungsmaßnahmen nur bis zu einer für jede Partei individuell festgelegten Obergrenze angerechnet werden. Für Deutschland beträgt diese Obergrenze 1,24 Millionen Tonnen Kohlenstoff pro Jahr. Auch für Senkenprojekte im Ausland existieren Restriktionen. Wichtig in diesem Zusammenhang ist, dass die Verhandlungen für die Post-2012-Periode beginnen. LULUCF ist in diesen Verhandlungen ein wichtiges Thema.

Der Wald war in verschiedenen Epochen der Kunstgeschichte und Literaturgeschichte ein beliebtes Motiv. So gehört der Wald zu den wichtigsten Schauplätzen von Mythen verschiedenster Kulturen sowie von Volkssagen und Volksmärchen. Besonders in der deutschen Romantik erfuhr der Wald als Sinnbild der malerischen Natur, aber auch der unergründlichen und gegensätzlichen Welt große Verehrung. In den Werken der Maler Caspar David Friedrich und Moritz von Schwind oder des Dichters Joseph von Eichendorff ist der Wald allgegenwärtig. Der Wald ist auch in der neueren Literatur und im Film immer wieder ein beliebter Schauplatz, wobei seine dramaturgische Funktion von der verklärten Idylle bis hin zur unheimlichen Horrorkulisse reicht.
Menschen halten sich gerne aus gesundheitlichen Gründen und zum Zweck der Naherholung in Wäldern auf. Unterschiedlichen Studien zufolge schätzen Besucher die saubere Luft in einem Wald (die Vegetation wirkt als Filter), Gerüche werden als angenehm empfunden, Stress verursachende Geräusche werden gedämpft, wodurch der Blutdruck gesenkt wird. Das ausgeglichene Waldinnenklima zeichnet sich durch eine höhere Luftfeuchtigkeit und angenehme Kühle im Sommer aus. Dem Wald wird außerdem eine positive Wirkung auf die psychische Verfassung (Ablenkung, Inspiration) und Möglichkeiten zur Pflege des Soziallebens bescheinigt (besonders bei Kindern und Sammlern).

Neben der Naherholung nutzen Menschen den durch ein Wegenetz erschlossenen Wald auch zu sportlicher Betätigung (Wandern, Nordic Walking, Jogging, Ski Nordisch, Mountainbiking usw.). Der Schwarzwald hat auf diesem Gebiet für seinen Waldtourismus weltweit Bekanntheit erlangt. Die Anfang des 20. Jahrhunderts angelegten Fernwanderwege (Westweg, Mittelweg, Ostweg) waren Vorbild für zahlreiche weitere Fernwanderwege. Eine speziell in Skandinavien populäre Natursportart im Wald ist der Orientierungslauf. Hier gilt es, sich mit Karte und Kompass zu orientieren und vorgegebene Punkte auf einer selbstgewählten Route anzulaufen, wenn nötig auch querfeldein.

Die Waldpädagogik versucht, die vielfältigen Bedeutungen des Waldes in der Öffentlichkeit bekannter zu machen und eine positive emotionale Beziehung zum Wald zu fördern. Neben Informationseinrichtungen wie den allein in Deutschland weit mehr als tausend Waldlehrpfaden wird permanent versucht, neue, stärker zielgruppenorientierte Methoden zu entwickeln.

Zunehmend beliebt sind in Deutschland Waldkindergärten, die das Spiel der Kinder in die freie Natur – häufig in den Wald – verlegen.

Wälder stellen einen Teil des kulturellen Erbes dar. Sie sind in ihrer heutigen Form ein Element unserer Landschaft, welche nach allgemeinem Dafürhalten und auch juristisch betrachtet ein schützenswertes Gut ist. Eine Umwandlung von Wald (also zu Bauland oder zur anderweiten Nutzung) ist aus diesem Grunde nur in Ausnahmefällen möglich. In der Regel müssen außerdem Konzessionsleistungen erbracht werden, die auch die Aufforstung von Land beinhalten können. Zu den Sonderfunktionen zählt auch der Beitrag zum Denkmalschutz (Naturdenkmäler sind sehr alte oder markante Bäume, Felsen, Wasserfälle, aber auch Hügelgräber und andere menschliche Spuren).

Wälder sind auch Objekt für Lehre und Forschung. Nicht nur die Grundlagenforschung hilft heute bei der Erforschung noch unbekannter Urwaldgebiete. Die Pharmaindustrie erzielt durch den Aufkauf von Urwaldflächen und die Entsendung von Biologen zur Erforschung des Areals bereits einige Erfolge bei der Auffindung neuer Wirkstoffe für Medikamente. Diese Form des „Sponsorings“ von Umweltschutz dient nicht ausschließlich zu propagandistischen Werbezwecken.

Weltweit werden Wälder als Lebensräume für Pflanzen- und Tierarten von Staaten unter Schutz gestellt. Verschiedenste Programme dienen dem Umweltschutz und werden zu diesem Zweck von den Industriestaaten auch finanziell gefördert. Damit ist nicht nur der Schutz von Urwäldern gemeint, sondern beispielsweise auch die Einrichtung von Bannwäldern in Europa. Diese Wälder dürfen sich, begleitet von der Forschung, wieder zu Urwäldern entwickeln.

Wälder dominieren unter natürlichen Umständen überall dort, wo sich Bäume gegenüber anderen Pflanzen wie Gräsern als konkurrenzstärker erweisen. Solche Bedingungen sind auf den Landflächen der Erde vielerorts großflächig vorzufinden. Störungen der Waldentwicklung waren seit jeher Katastrophenereignisse wie Waldbrände und Vulkanausbrüche, aber auch Klimaänderungen wie der Wechsel zwischen Wärme- und Kälteperioden im Quartär. Pollenanalysen zeigen die fortschreitende Wiederbesiedelung von ehemals vereisten Landflächen durch Bäume unterschiedlicher Arten aus ihren Refugien zum Ende der Eiszeiten. Wälder werden zudem durch Tiere gestört. So schaffen Elefanten durch ihre zuweilen zerstörerischen Aktivitäten an Bäumen das für Savannen charakteristische Erscheinungsbild einer Graslandschaft, die locker mit Gehölzen bestockt ist. In den monotonen borealen Wäldern Amerikas und Eurasiens kommt es immer wieder zu Störungen durch die Massenvermehrung von Insekten, die an Nadeln oder anderen Teilen des Pflanzenkörpers Schäden verursachen, die Bäume innerhalb kurzer Zeit großflächig sterben lassen.

Großen Einfluss auf die Waldentwicklung nimmt der Mensch seit den ersten Tagen der Zivilisation. Vor allem wurden Wälder gerodet, um Siedlungs- und Ackerfläche zu gewinnen. Später trat in den Ländern Europas die Nutzung des Holzes als Energieträger und als Rohstoff in den Vordergrund. Die ehemals bewaldeten Buschlandschaften des Mittelmeerraumes und das durch Entwaldung geprägte Erscheinungsbild der Länder ehemaliger Seefahrernationen zeugen von dieser Entwicklung. Insgesamt ist der für Europäer heute gewohnte Anblick der Landschaft mit ihrem Wechsel von Feldern, Grünland, Wald und Siedlungen in der Regel das nahezu alleinige Resultat menschlicher Tätigkeit. Der Anteil des Waldes an der Landnutzung ändert sich in wohlhabenden Ländern heute nur noch marginal.

Außerhalb Europas existieren heute noch große zusammenhängende Waldgebiete, deren Größe um etwa 13 Millionen ha netto jährlich reduziert wird. Die Schwerpunkte der Entwicklung sind Lateinamerika, das Kongobecken und Südostasien (Indonesien, Malaysia).

Mit 11.419.124 ha bedeckt Wald 32 % der deutschen Staatsfläche, wie die Dritte Bundeswaldinventur (2012) feststellte. Die deutsche Waldfläche hat zwischen 2002 und 2012 um rund 48.000 ha zugenommen. Der Holzvorrat stieg im selben Zeitraum um 227 Millionen Festmeter auf nun insgesamt 3,663 Milliarden Festmeter bzw. 336 m³/ha an, was einen historischen Rekordwert darstellt. Von der deutschen Waldfläche sind 48 % Privatwald, 32,5 % Staatswald (29 % Landeswald und 3,5 % Bundeswald) und 19,4 % Körperschaftswald. Der vergleichsweise hohe Waldanteil ist den Aufforstungsbemühungen hauptsächlich des 19. Jahrhunderts zu verdanken.

Die Waldfläche ist zwischen 1989 und 2003 um durchschnittlich 3500 ha pro Jahr gewachsen. Im Vergleich zur Waldfläche sind 25 % Deutschlands der Siedlungsfläche zuzurechnen, davon sind 50 % vollständig versiegelt (täglich um 129 ha oder 47.000 ha pro Jahr zunehmend). Dadurch werden jährlich rund 3500 ha Wald zerstört. Die Zunahme der Waldfläche ergibt sich durch Aufforstungen (hauptsächlich von landwirtschaftlichen Flächen) und die sukzessive Bewaldung degenerierter Moorstandorte. Deutschland ist damit dennoch wieder eines der waldreichsten Länder in der Europäischen Union.

Auch die Baumartenzusammensetzung nähert sich kontinuierlich der potentiell natürlichen Zusammensetzung. Von Natur aus wären 67 % der Landfläche Deutschlands von Buchenmischwäldern, 21 % von Eichenmischwäldern, 9 % von Auwäldern oder feuchten Niederungswäldern, 2 % von Bruchwäldern und 1 % von reinen Nadelwäldern bedeckt (Meister u. Offenberger, Zeit des Waldes, S. 36, s. u. Literatur). In der oberen Waldschicht, also den älteren Bäumen, liegt die Baumartenverteilung noch bei 14,8 % Buchen, 9,6 % Eichen, 15,7 % anderer Laubbäume, 28,2 % Fichten, 23,3 % Kiefern, 1,5 % Tannen und 4,5 % anderer Nadelbäume. Das macht ein Verhältnis von etwa 40 % Laubbäumen zu 60 % Nadelbäumen. In der sogenannten Unterschicht, also der jungen Waldgeneration, hat sich das Verhältnis aufgrund des seit vielen Jahrzehnten von den Waldbesitzern forcierten Waldumbaus ins Gegenteil verkehrt. In der jüngeren Waldgeneration stehen nur noch 30 % Nadelbäume und 70 % Laubbäume, überwiegend Buchen. Der große Anteil von Fichte und Kiefer in der Oberschicht liegt an dem hohen Holzbedarf zu Zeiten der Industrialisierung und zahlreiche Kriege der letzten 150 Jahre begründet: Diese Baumarten sind schnellwüchsig und anspruchslos und wurden daher zur Aufforstung von degenerierten Standorten wie Heiden, trockengelegten Mooren und übernutzten Niederwäldern insbesondere im 19. Jahrhundert verwendet. Andererseits leiden besonders Fichtenbestände unter Wind- und Schneewurf sowie Insektenschäden (z. B. durch Borkenkäfer) und führen zu einer Versauerung der Böden.

Fichten und Kiefern sind relativ unempfindlich gegen Wildverbiss (meist ist eine Umzäunung der Jungkulturen nicht nötig). Vielerorts behindern die relativ hohen Schalenwild­dichten das Aufkommen von stärker verbissgefährdeten Laubbäumen und Tannen. Darüber hinaus ist das Holz von Fichte und Kiefern vielseitiger einsetzbar als Laubholz. Etwa 80 % unserer Holzprodukte werden aus Nadelholz hergestellt. Aufgrund der holzartspezifischen Eigenschaften (Holzdichte, Festigkeit, Elastizität, Widerstandsfähigkeit gegen Pilze) von Fichte und Kiefer, sind die Holzarten kaum durch Laubholz zu ersetzen. Um die Nachfrage nach dem Rohstoff auch künftig befriedigen zu können, ist deshalb ein gewisser Anteil an Nadelholz in unseren gemischten Wäldern zu erhalten. Der Vorteil von Mischwäldern ist eine höhere Artenvielfalt, eine bessere Stabilität, geringere Anfälligkeit gegen extreme Insektenschäden und ein ausgeglichenes Portfolio an Holzarten, um die Nachfrage nach dem Ökorohstoff Holz decken zu können.

Das Bundeslandwirtschaftsministerium gibt jährlich einen Waldzustandsbericht über die Ergebnisse des forstlichen Umwelt-Monitorings der Waldbäume in Deutschland heraus.

In Österreich beträgt die Waldfläche etwa 4,0 Millionen ha, das sind 48 % des Staatsgebietes (8,4 Millionen ha). Nur 0,7 % der österreichischen Wälder sind noch in einem natürlichen Zustand oder streng geschützt. Aufgrund des gebirgigen Terrains beträgt der Anteil an Schutzwald etwa 20 % (755.000 ha). Mehr als 2/3 ist Nadelwald. Die häufigste Baumart ist Fichte mit über 50 % aller Bäume, es folgen Buche mit 10 %, weiters Kiefer 9 % und Lärche 6,8 %, alle weit seltenere Baumarten. Die größten Bewaldungsdichten liegen im Voralpengebiet von Salzburg bis Niederösterreich sowie am Alpenostrand, von Kor- und Saualpe über die Berge des Mur-Mürz-Gebiets bis zum Wechsel. Das Bundesland Steiermark besitzt die größte Waldfläche Österreichs, der waldreichste Bezirk in Österreich ist der Bezirk Lilienfeld in Niederösterreich, der an die 80 % Waldfläche aufweist.

Zwei Drittel der Wälder sind nach den letzten Waldinventuren intakt. Probleme bilden nur die Schutzwälder. Es wächst auch um 30 % mehr Holz nach als verbraucht wird oder durch Windbruch oder Wildverbiss geschädigt wird. Da teure Holzbringung im Wettbewerb zu billigeren Importen steht, wird oft das Holz im Wald nicht geschlagen. Nicht nur durch Aufforstungen, sondern auch durch Stilllegungen von landwirtschaftlichen Flächen erobert der Wald wieder Gebiete zurück.

Der Ertragswald umfasst 83 % der Waldfläche, hauptsächlich Hochwald (Verjüngung aus Samen, lange Umtriebszeit), Ausschlagwald liegt unter 3 %. Im Ertragswald hat die Fichte einen Anteil 61,4 %. Bezüglich des Holzvorrates je Fläche liegt Österreich mit 325,0 m³/ha im europäischen Vergleich an zweiter Stelle. Größter Waldeigentümer sind die österreichischen Bundesforste mit 523.000 ha, 1,73 Millionen ha sind bäuerlicher Wald, insgesamt gibt es 170.000 Waldeigentümer. Der Privatwald­anteil liegt  – weit über dem europäischen Durchschnitt – bei etwa 80 %, der Kleinwald­anteil (unter 200 ha Katasterfläche) wird zwischen 1,56 Millionen ha und 2,13 Millionen ha (40–50 % der gesamten Waldfläche) angegeben.

Große Waldbesitzer sind beispielsweise die Stiftung Fürst Liechtenstein, die Fürst Starhemberg'sche Familienstiftung (ab 1995, in Oberösterreich), vom Adel abstammende Familien wie die Esterházys, die Schwarzenbergs oder der Habsburg-Clan, weiters kirchliche Eigentümer wie Benediktinerorden in Admont oder das Chorherrenstift Klosterneuburg. Größter Waldbesitzer ist der Staat über die Bundesforste. Dazu kommen Wälder der Länder, Gemeinden, etwa der Stadt Wien insbesondere im Quellgebiet der Hochquellwasserleitungen oder auch der ÖBB.

Rund ein Drittel der Schweiz ist bewaldet. Das ist relativ viel, wenn man berücksichtigt, dass große Teile des Landes aufgrund der Topographie keine Bewaldung zulassen. Dazu beigetragen hat das Waldgesetz, das in verschiedenen Fassungen seit 1903 vorschreibt, dass die Waldfläche nicht vermindert werden soll und dass Rodungen grundsätzlich verboten sind (Ausnahmebewilligungen können erteilt werden).

Bezüglich des Holzvorrates je Fläche belegt die Schweiz mit 336,6 m/ha den europäischen Spitzenplatz. Obwohl Stürme wie Vivian oder Lothar große Schäden anrichteten, hat der Wald in den letzten zwanzig Jahren um 4 % zugenommen (Stand 2011); er dehnt sich allerdings primär im Gebirge und in anderen marginalen Lagen, auf sogenannten Grenzertragsböden, aus. Der Holzschlag könnte gemäß Fachkreisen aber trotzdem deutlich gesteigert werden. Aus wirtschaftlichen Gründen ist vielerorts der Holzschlag jedoch nicht lukrativ.

In den Alpen erfüllen die Wälder eine wichtige Schutzfunktion gegen Lawinen und Erosion. Diese Schutzwälder machen rund 10 % der Schweizer Waldfläche aus und stehen unter besonderem Schutz.

Die Eidgenössische Forschungsanstalt für Wald, Schnee und Landschaft beschäftigt sich mit der Nutzung und dem Schutz von Landschaften und Lebensräumen, mit Schwerpunkt auf Wäldern und Naturgefahren.





</doc>
<doc id="11037" url="https://de.wikipedia.org/wiki?curid=11037" title="Bäume">
Bäume

Bäume bezeichnet:

Siehe auch:



</doc>
<doc id="11038" url="https://de.wikipedia.org/wiki?curid=11038" title="Sprachphilosophie">
Sprachphilosophie

Die Sprachphilosophie ist die Disziplin der Philosophie, die sich mit Sprache und Bedeutung beschäftigt, vor allem mit dem Verhältnis von Sprache und Wirklichkeit und dem Verhältnis von Sprache und Bewusstsein (bzw. Denken). Sie ist auch eine Teildisziplin der allgemeinen Linguistik. Sie kann weiter auch als ein Teilbereich der Semiotik angesehen werden, d. h. der allgemeinen Zeichenlehre. Die Sprachphilosophie ist eng verwandt mit der Logik insofern, als zur Sprachphilosophie auch die Analyse der logischen Struktur von Sprache gehört. Zur Sprachphilosophie gezählt wird manchmal auch die sprachphilosophisch orientierte Philosophie, zu denen die anthropologischen Überlegungen zur Stellung des Menschen als sprachfähiges Wesen gehören. Zur Sprachphilosophie wird manchmal auch die Sprachkritik gezählt. Zu unterscheiden ist die Sprachanalyse als eine philosophische Methode von der Sprachphilosophie als Untersuchung des Gegenstands Sprache. Sprachphilosophische Untersuchungen gibt es seit der Antike, aber erst seit etwa Mitte des 19. Jahrhunderts werden sie als ‚Sprachphilosophie’ bezeichnet (wobei der Begriff schon vorher im Umlauf war, 1748 bei Maupertuis).

Die Sprachanalyse als philosophische Methode gibt es bereits seit der Antike. Ihr kommt jedoch eine zentrale Stellung in der Analytischen Philosophie des 20. Jahrhunderts zu, deren verschiedene Strömungen etwa in der Tradition des späten Wittgensteins oder Quines philosophische Probleme zum Beispiel in der Erkenntnistheorie oder der Philosophie des Geistes primär unter Bezug auf sprachphilosophische Methoden diskutierten. Die Sprachphilosophie wurde als Fundamentaldisziplin innerhalb der Philosophie angesehen. Peter Bieri bemerkt dazu kritisch:

Die Ansicht, dass die Sprachphilosophie Fundamentaldisziplin ist, bezeichnet man auch als Linguistic turn. Richard Rorty beschreibt es präziser als „die Ansicht, dass philosophische Probleme gelöst oder aufgelöst werden können, indem man entweder die Sprache reformiert oder besser die Sprache versteht, welche wir gegenwärtig verwenden.“ Damit benennt Rorty zwei verschiedene Zugänge, die so genannte Philosophie der idealen Sprache und die Philosophie der normalen Sprache.

Die Philosophie der idealen Sprache betrachtet die natürlichen Sprachen als defizitär, da diese aufgrund verschiedener Ungenauigkeiten nicht den strengen Ansprüchen der Logik genügten. Ziel dieses Zugangs ist die Revidierung oder gar Ersetzung der natürlichen Sprachen für Zwecke der Wissenschaften durch eine ideale, formale Sprache.

Das Projekt hat sich als schwierig in der Umsetzung erwiesen. Das grundsätzliche Problem ist, dass jede Sprache, auch eine formale Sprache, interpretiert werden muss, und die Sprache der Interpretation in der Regel unsere natürliche Sprache ist. Dennoch hat sich diese Zugangsweise als sehr fruchtbar erwiesen, denn dank der Erforschung von logischen und begrifflichen Zusammenhängen wurden wichtige Erkenntnisse über den Aufbau einer formalen Sprache gemacht.

Als Begründer der Philosophie der idealen Sprache gilt der Mathematiker, Logiker und Sprachphilosoph Gottlob Frege, der dieses Projekt in seiner Begriffsschrift verwirklichen wollte. Weitere wichtige Vertreter sind Bertrand Russell, der zusammen mit Alfred North Whitehead die Principia Mathematica verfasste, Ludwig Wittgenstein in seinen frühen Jahren, d. h als Verfasser des Tractatus Logico-Philosophicus, Rudolf Carnap und weitere Vertreter der frühen analytischen Philosophie, sowie Wilhelm Kamlah und Paul Lorenzen, die Begründer des Erlanger Konstruktivismus.

Die Philosophie der normalen Sprache betrachtet die natürlichen Sprachen nicht als defizitär, sondern als völlig brauchbar für den Zweck, für den sie eingesetzt werden, nämlich zur Verständigung im sozialen Umfeld. Die Aufgabe der Sprachphilosophie sei es nicht, die Sprache zu revidieren oder zu ersetzen, sondern beispielsweise durch das Ausweisen von begrifflichen oder regulativen Zusammenhängen zu beschreiben bzw. – wie einige Vertreter hinzusetzen würden – zu erklären.

Als Begründer der Philosophie der normalen Sprache gilt Ludwig Wittgenstein in seinen späten Jahren, d. h. als der Verfasser der Philosophischen Untersuchungen. Weitere wichtige Vertreter sind Gilbert Ryle, John Langshaw Austin und Peter Strawson.

Der Ansatz hat zur Entwicklung der Sprechakttheorie beigetragen, die zu einem wichtigen Bestandteil der linguistischen Pragmatik geworden ist. Die Fruchtbarkeit normalsprachlicher Methodik zeigt sich auch in zahlreichen philosophischen Debatten, darunter etwa in Debatten um die Beziehung von Geist und Materie (deren traditionelle Behandlung nach Ryle zu Scheinproblemen führe).

Einigen Kritikern scheint der damit einhergehende konservative Zug, also das Festhalten am bestehenden Sprachgebrauch, aus verschiedenen Motiven problematisch. Es wird moniert, im Rahmen normalsprachlicher Ansätze würden Erklärungen und Rechtfertigungen zirkulär oder hätten nur im Geltungsbereich bestimmter Sprachsysteme Gültigkeit. Hin und wieder wird behauptet, bei normativen Problemen führe die Philosophie der normalen Sprache zu naturalistischen Fehlschlüssen.

Man kann verschiedene Zugänge zur Sprache unterscheiden: die analytische Philosophie, die philosophische Anthropologie, die Sprachkritik und der Strukturalismus.

In der analytischen Philosophie wird der Gegenstand Sprache mithilfe sprachanalytischer Methoden untersucht. Als Gründerväter der analytischen Sprachphilosophie gelten unter anderen Gottlob Frege, Bertrand Russell und Ludwig Wittgenstein.

In der philosophischen Anthropologie wird das Wesen des Menschen untersucht. Die Sprachfähigkeit des Menschen bietet sich als wesentliches Unterscheidungsmerkmal zum Tier an. Dies ist Untersuchungsgegenstand unter anderen bei Johann Gottfried Herder und Wilhelm von Humboldt. Humboldt stellt die These auf, dass begriffsbildende Sprachunterschiede zwischen den Völkern nicht auf eine gemeinsame Vernunft zurückführbar sind, sondern stattdessen durch das Studium der Sprachen erklärbar seien. Weitergeführt wurden diese Überlegungen namentlich von Ernst Cassirer in seinem Werk "Versuch über den Menschen".

Sprache wird als ein "gesellschaftliches Mittel zur Machtausübung" untersucht und kritisiert. Gemäß der Diskurstheorie von Michel Foucault gibt es keinen Diskurs, der nicht von Machtbeziehungen bestimmt sei. Die Regeln des Diskurses definieren für einen bestimmten Zusammenhang, was gesagt werden soll und was nicht gesagt werden darf und welcher Sprecher was wann sagen darf.

Jürgen Habermas schlägt demgegenüber das Ideal eines machtfreien Diskurses vor. Er verbindet Kommunikation mit den normativen Grundlagen der Gesellschaft und liefert in seinem Hauptwerk Theorie des kommunikativen Handelns eine soziologisch fundierte Auseinandersetzung der Rolle der Kommunikation für das soziale Leben in demokratischen Gesellschaften.

Die sexistische Diskriminierung und Unterdrückung der Frauen durch Sprache – zum Beispiel durch Stereotypisierung, abfällige Bemerkungen – wird in der feministischen Linguistik untersucht. Die feministische Philosophie interessiert sich unter anderem für die Unterscheidung von Sex und Gender und die (auch sprachliche) Konstruktion des Geschlechts (Doing Gender).

Die Sprache wird im Strukturalismus als ein "System von Zeichen" untersucht. Als Begründer des Strukturalismus gilt Ferdinand de Saussure. Wichtige Beiträge lieferten Roman Jakobson und Claude Lévi-Strauss. In Auseinandersetzung mit dem Strukturalismus entwickelte sich der Poststrukturalismus. Wichtige Poststrukturalisten sind Michel Foucault, Jacques Derrida, Gilles Deleuze, Roland Barthes, Jacques Lacan und Judith Butler. Jacques Derrida entwickelte die Dekonstruktion. Inzwischen untersucht die Biosemiotik, ein Teilgebiet der Semiotik, die Verwendung von Zeichen in der nicht von Menschen belebten Natur.

Dass es referierende (d. h. Bezug nehmende) Ausdrücke gibt, scheint unbezweifelbar: Der Name „Sokrates“ bezeichnet den griechischen Philosophen. Wenn man nun eine referenzielle Bedeutungstheorie vertritt, d. h. wenn man behauptet, dass die Bedeutung eines Ausdrucks in seiner Referenz besteht, dann stellt sich folgendes Problem: Zwei Ausdrücke, welche dieselbe Referenz haben, d. h. die koextensional sind, haben nicht unbedingt denselben Erkenntniswert. Das berühmte Beispiel von Gottlob Frege ist:

Der Ausdruck „Abendstern“ und der Ausdruck „Morgenstern“ haben dieselbe Referenz, nämlich den Planeten Venus, aber der erste Ausdruck bezeichnet den hellsten Stern am Abend, der zweite den hellsten Stern am Morgen. Der Satz lässt sich also mit Hilfe von Kennzeichnungen, d. h. von Ausdrücken der Art „der/die/das A“ so formulieren:

Doch damit ist das Problem noch nicht gelöst, denn die erste Kennzeichnung hat dieselbe Referenz wie die zweite und müsste, wenn die referentielle Bedeutungstheorie wahr ist, dieselbe Bedeutung haben. Das ist jedoch nicht der Fall, denn jemand kann wissen, dass der hellste Stern am Abend die Venus ist, ohne zu wissen, dass der hellste Stern am Morgen auch die Venus ist. Wie ist das Problem zu lösen? Es bestehen grundsätzlich zwei Lösungsansätze, der Ansatz von Gottlob Frege und der Ansatz von Bertrand Russell.


Peter Strawson hat beide Ansätze kritisiert, ebenso Keith Donnellan, der das Problem durch eine Unterscheidung zwischen attributivem und referentiellem Gebrauch zu lösen versucht.

Ein weiteres Problem sind Eigennamen. Wie sind Eigennamen zu analysieren? Auch hierzu gibt es zwei Lösungsansätze, erstens den von Russell und Frege vertretenen Ansatz, zweitens den von Saul Kripke und Hilary Putnam vertretenen Ansatz.


Traditionelle Bedeutungstheorien gehen davon aus, dass mit der Bedeutung ein Gegenstand bezeichnet ist. Diese Theorien haben jedoch das Problem, dass Sätze, in denen Ausdrücke vorkommen, die auf nichts referieren – zum Beispiel: „Pegasus ist ein geflügeltes Pferd“ –, ihnen gemäß keine Bedeutung hätten. (Führt man zur Behebung dieses Problems fiktive Gegenstände ein, so ergeben sich andere Probleme.) Zudem gibt es viele Ausdrücke wie zum Beispiel Konjunktionen und Präpositionen, welche auf nichts zu referieren scheinen.

Moderne Bedeutungstheorien im Geist der Philosophie der normalen Sprache stellen die Frage, wie es überhaupt dazu kommt, dass ein Zeichen Bedeutung hat. Damit gelangen sie zur Ansicht, dass die Bedeutung eines Ausdrucks kein Gegenstand ist, sondern durch den Gebrauch des Zeichens gebildet ist. In der Folge haben sich verschiedene Bedeutungstheorien entwickelt.


Wer spricht, der stellt nicht nur etwas dar, der tut etwas. Diese Erkenntnis hat John Langshaw Austin in einer Vorlesungsreihe im Jahre 1955 formuliert (1962 als "How To Do Things With Words" publiziert). Austin unterscheidet in der Folge zwischen einem lokutionären, einem illokutionären und einem perlokutionären Akt, vereinfachend gesagt zwischen dem, was mit der Äußerung "gesagt" wird, was mit ihr "getan" wird und was mit ihr "bewirkt" wird. Wenn zum Beispiel jemand äußert „Schiess dieses Tier nieder!“, dann hat er damit gesagt, dass die angesprochene Person das Tier niederschießen soll (Lokution), er hat ihr geraten oder befohlen, das Tier niederzuschießen (Illokution) und er hat sie (unter Umständen) überzeugt, dass sie das Tier niederschießen soll (Perlokution).

Einige Äußerungen sind sogenannte explizit performative Äußerungen; der Sprecher gibt dabei die illokutionäre Rolle seiner Aussagen explizit an. Zum Beispiel: „Hiermit warne ich Dich!“. Eine performative Äußerung ist weder wahr noch falsch; sie kann gelingen oder nicht gelingen. Als Kriterium in der Analyse von Äußerungen gelten dabei die sogenannten Gelingensbedingungen von performativen Äußerungen.

John Searle versucht, Austins Ansätze zu einer Sprechakttheorie zu systematisieren. Er unternimmt unter anderem eine Klassifikation von Sprechakten. Er unterscheidet fünf Typen von Sprechakten: Repräsentivum/Assertivum (z. B. etwas behaupten), Direktivum (z. B. jemanden um etwas bitten), Kommissivum (z. B. jemandem etwas versprechen), Expressivum (z. B. jemandem danken) und Deklarativum (z. B. jemanden taufen). Es ist umstritten, wie hilfreich diese Einteilung ist.

Manchmal meinen wir das, was wir sagen; öfters meinen wir jedoch etwas anderes oder etwas mehr als das, was wir sagen; wir deuten es lediglich an. Zum Beispiel sagt jemand als Antwort auf die Frage, wo man Benzin tanken könne, dass es eine Tankstelle um die Ecke gebe. Damit hat die Person nicht gesagt, dass man dort Benzin tanken könne, sie hat es lediglich angedeutet.

Paul Grice hat versucht, diesen Aspekt der Bedeutung als Implikatur zu verstehen. Der Ausdruck „Implikatur“ ist ein Kunstwort, das nur innerhalb von Grice Theorie – und Weiterentwicklungen davon – eine klar umrissene Bedeutung hat. Die Grundidee von Grice ist, die sprachliche Verständigung als ein rationales Handeln anzusehen, das auf dem sogenannten Kooperationsprinzip beruht. Diesem Prinzip sind verschiedene Konversationsmaximen untergeordnet, beispielsweise dass ein Sprecher seinen Beitrag so informativ wie möglich gestalten soll. Wenn wir mehr oder etwas anderes sagen, als wir meinen, aber dennoch kooperativ sind, dann ist dies darauf zurückzuführen, dass eine dieser Maximen nicht eingehalten oder verletzt wird.

Wird ein Wort nicht in seiner wörtlichen, sondern in einer übertragenen Bedeutung gebraucht, so spricht man von einer Metapher (griechisch μεταφορά „Übertragung“, von metà phérein „anderswohin tragen“). Gemäß Aristoteles besteht zwischen der wörtlich bezeichneten Sache und der übertragen gemeinten eine Beziehung der Ähnlichkeit. Zum Beispiel ist mit der metaphorischen Redeweise „Du bist meine Sonne“ nicht gemeint, dass die angesprochene Person tatsächlich eine Sonne ist, sondern dass sie ihr in einer näher zu bestimmenden Hinsicht ähnlich ist. Inwiefern ist nun aber eine Person einer Sonne ähnlich? Man könnte sagen, dass eine Person wie eine Sonne „strahlt“ oder „glänzt“. Dann würde man aber wiederum eine Metapher brauchen. Versucht man diese Frage zu beantworten, scheint man immer wieder auf Metaphern zurückgreifen zu müssen.

Gemäß Donald Davidson ist es irreführend, von einer metaphorischen Bedeutung zu reden. Wörter haben wörtliche Bedeutung und können metaphorisch gebraucht werden. John Searle schlägt in Anlehnung an Paul Grice vor, diesen Gebrauch als Implikatur zu erklären: Sagt ein Sprecher „Du bist meine Sonne“, so impliziert er damit, dass die Person in einer noch näher zu bestimmenden Hinsicht wie eine Sonne ist. Doch damit ist immer noch nicht geklärt, wie das „wie“ zu verstehen ist.

Die Linguisten Edward Sapir und Benjamin Whorf vertreten wie vor ihnen Wilhelm von Humboldt die These der sprachlichen Relativität: Sie behaupten, dass die Gedanken insofern relativ zu einer Sprache sind, als sich gewisse Gedanken nur in bestimmten Sprachen formulieren und verstehen lassen. Sie glauben, dies unter anderem mit empirischen Studien der Sprache von Indianern und Eskimos belegen zu können. Donald Davidson vertritt dagegen die These, dass alle Menschen, insofern sie miteinander kommunizieren, über dasselbe Begriffsschema verfügen, weil ein grundsätzlich anderes Begriffsschema für uns gar nicht verständlich wäre.

Die Sprache ist auch Mittel des "Verstehens". Die Hermeneutik ist die Untersuchung des Verstehens und somit auch der Sprache als Mittel des Verstehens. Begründer der Hermeneutik ist Friedrich Schleiermacher. Wesentliche Impulse zu einer Erneuerung der Hermeneutik im zwanzigsten Jahrhundert lieferten Wilhelm Dilthey, Martin Heidegger und Hans-Georg Gadamer.

Die Sprache ist auch ein Mittel der Kommunikation. Ein besonders bekanntes Kommunikationsmodell ist das Organonmodell (1933) von Karl Bühler. Bühler unterscheidet zwischen einer Darstellungs-, Ausdrucks- und Appellfunktion des Zeichens. Roman Jakobson erweiterte 1960 das Modell auf sechs Funktionen.

Als Standardmodell der Nachrichtenübermittlung gilt das in der Informationstheorie von Claude Shannon und Warren Weaver entwickelte Sender-Empfänger-Modell (1949). Dan Sperber und Deirdre Wilson haben gezeigt, dass dieses Modell zur Erklärung der menschlichen Kommunikation zu kurz greift und durch ein inferentialistisches Modell erweitert werden muss.

Die von Sperber und Wilson im Buch "Relevance" (1986) entwickelte Relevanztheorie verbindet Fodors modulare Theorie des Geistes mit Gedanken von Grice. Die Theorie besteht grundsätzlich aus zwei Prinzipien der Relevanz. Das erste besagt, dass der menschliche Geist dazu tendiert, die Relevanz des Inputs zu maximieren. Die zweite besagt, dass jede kommunikative Äußerung eine Vermutung der optimalen Relevanz mit sich trägt. Damit lasse sich sprachliche Kommunikation erklären.

Wie können wir erklären, dass Menschen ihre Muttersprache so schnell erlernen können? In der Spracherwerbsforschung gibt es zwei klassische Ansichten, die von Noam Chomsky und von Jean Piaget erstmals formuliert wurden.


Die Anfänge der Sprachphilosophie gehen bis in die Antike zurück. Platons Ideenlehre führt zum Problem der Prädikation: Wie verhalten sich die Einzeldinge zu den Universalien? Aristoteles fährt mit den sprachphilosophischen Untersuchungen fort und entwickelt die Aussagenlogik. Im Mittelalter werden von Philosophen wie Abaelardus und Duns Scotus logische und sprachphilosophische Untersuchungen unternommen. William von Ockham entwickelt den Nominalismus (siehe Universalienstreit). Zur Abgrenzung gegenüber anderen Philosophien wurden verschieden Aspekte erwogen und verworfen, so etwa der methodologische Nominalismus und eine Gegnerschaft zum Psychologismus, aber kein Kriterium gilt hierbei als vollständig etabliert.

Die moderne Sprachphilosophie hat sich als eigenständige Disziplin mit der Entwicklung der modernen Logik durch Gottlob Frege in seinem epochalen Werk der Begriffsschrift etabliert; dieses Werk ist kennzeichnend für die Philosophie der idealen Sprache. Mit den "Philosophischen Untersuchungen" von Ludwig Wittgenstein beginnt die Philosophie der normalen Sprache. Beide Traditionen haben zur Entwicklung neuer Erkenntnisse und der Erforschung neuer Gebiete geführt.





Enzyklopädien
Ressourcen
Weitere Links


</doc>
<doc id="11040" url="https://de.wikipedia.org/wiki?curid=11040" title="Gift">
Gift

Als Gift (althochdeutsch "Gabe") oder auch Giftstoff bezeichnet man einen Stoff, der Lebewesen über ihre Stoffwechselvorgänge, durch Eindringen in den Organismus ab einer bestimmten, geringen Dosis einen Schaden zufügen kann. Mit der Zunahme der Expositionsmenge eines Wirkstoffes steigt die Wahrscheinlichkeit, dass Gesundheitsschädigungen durch eine Vergiftung auftreten. Ab einem bestimmten Dosisbereich ist somit nahezu jeder Stoff als giftig (toxisch) einzustufen.

Die wissenschaftliche Disziplin, die sich mit der Erforschung von giftigen Substanzen, ihrer Wirkung in verschiedenen Dosisbereichen sowie der Behandlung von Vergiftungen beschäftigt, ist die Toxikologie. Sie befasst sich mit Stoffen, Stoffgemischen, Tieren, Pflanzen und Mikroorganismen und mit den biochemischen Mechanismen der Giftwirkung in Bezug auf quantitative Aspekte.

Der durch ein Gift angerichtete Schaden kann in vorübergehender Beeinträchtigung, dauerhafter Schädigung oder Tod bestehen. Bei anhaltender schädigender Gifteinwirkung spricht man von "chronischer Vergiftung", bei einer Gifteinwirkung, die umgehend zu einer Schädigung führt, von einer "akuten Vergiftung".

Als Gefahrstoffe werden Gifte in Abhängigkeit von der Wirkmenge in "sehr giftig" und "giftig" sowie "gesundheitsschädlich" (früher "mindergiftig") eingeteilt.

Das Wort "Gift" ist eine germanische Abstraktbildung ("*gef-ti-") mit "t"-Suffix – und dadurch bedingtem Wandel von "b" zu "f" – der indoeuropäischen Wurzel des Wortes "geben". Die ursprüngliche Bedeutung „Gabe, Geschenk, Schenkung“, die "Gift" noch bei Goethe hatte, ist heute im Deutschen verschwunden (während sie im englischen „gift“ weiterlebt) und hat sich nur in der "Mitgift" („Heiratsgut der Braut, Aussteuer“) erhalten.

Der Bedeutungswandel von „Gabe“ zu „tödliche Gabe, Gift“, zuerst im Althochdeutschen bei Notker belegt, steht später unter dem Einfluss des griechisch-spätlateinischen Wortes "dosis", das Geschenk, „Gabe, bestimmte Menge Arznei“ bedeutet, aber auch als verhüllender (euphemistischer) Ausdruck für „Gift“ verwendet wird.

Aber auch schon der griechische Ausdruck "pharmacon" bei Homer stand sowohl für die Heilwirkung als auch die schädliche Wirkung eines Stoffes und auch bei Galenos gibt es Arzneimittel ("pharmaka") deren Wirkung als Gift von der Dosis abhängt.

Gift behält das ursprünglich feminine Genus in beiden Bedeutungen vorerst bei, wird dann als „schädlicher Stoff“ zuerst Maskulin (Anfang des 15. Jahrhunderts), später Neutrum (Mitte 16. Jahrhundert). Letzteres setzt sich im 18. Jahrhundert immer mehr durch, doch schreibt noch Schiller 1784 in "Kabale und Liebe" (5. Akt, 7. Szene): "Noch spür ich den Gift nicht".

Verwandte Verwendungen: Althochdeutsch (9. Jahrhundert), mittelhochdeutsch, mittelniederdeutsch "gift" (feminin) 'das Geben, Gabe, Geschenk, Gift', mittelniederländisch "ghifte", "ghichte", niederländisch "gift" (feminin) 'Gabe, Gift', altenglisch "gift", "gyft" (feminin, neutrum) 'Gabe, Belohnung, Brautpreis', Plural 'Hochzeit', altnordisch "gipt", "gift" (feminin) 'Gabe, Glück, Vermählung (der Frau)', gotisch "fragifts" (feminin) 'Verleihung', Plural 'Verlobung'.

Allgemein ist die nicht einfache Unterscheidung in "Schadstoff" und "Giftstoff" gegeben.

Von Lebewesen ausgeschiedene Giftstoffe oder Abfallprodukte werden in der Toxikologie als "Toxine" bezeichnet. Krankheitserregende Bakterien schädigen durch die Wirkung ihrer Gifte. Die charakteristischen Krankheitsbilder bei bakteriellen Infektionen werden durch die Wirkung der Bakterientoxine verursacht.

"Toxoide" sind entgiftete (inaktivierte) Toxine, die aber noch eine Immunantwort im geimpften Körper auslösen können. Toxoidimpfstoffe werden bei Impfungen gegen Diphtherie und Tetanus verwendet.

Viren sind Krankheitserreger, aber selbst nicht giftig. Substanzen oder Gegenstände, die ein Lebewesen ausschließlich mechanisch oder durch Strahlung schädigen, gelten ebenfalls nicht als Gift.

Die Verträglichkeit einer Substanz ist für viele Lebewesen oder Gruppen von Lebewesen unterschiedlich.
Grundsätzlich können alle dem Organismus zugeführten Stoffe oberhalb einer gewissen Dosis Schaden anrichten und sind somit ab dieser Wirkmenge als giftig anzusehen. Dies gilt sogar für unverzichtbare Substanzen wie Vitamine, Salze, Nährstoffe und Wasser. Schon 1538 hatte Paracelsus erkannt: „Alle Dinge sind Gift, und nichts ist ohne Gift; allein die dosis machts, daß ein Ding kein Gift sei.“

Die Toxizität, also das Ausmaß der Giftwirkung einer toxischen Substanz in Abhängigkeit von der Dosis, wird von vielen Faktoren bestimmt, die unter anderem bei der pharmazeutischen Technologie (Galenik, Herstellung von Arzneistoffen) und bei der Form der Verabreichung beachtet werden müssen.

Schnell toxisch wirken vor allem Substanzen mit guter Löslichkeit in Körperflüssigkeiten. Dies gilt insbesondere bei oraler Aufnahme durch die Einwirkung des Speichels. Da der Körper verschiedene Toxine abzubauen vermag, beeinflusst auch der zeitliche Verlauf der Aufnahme den Krankheitsverlauf (z. B. akut, subakut, chronisch).

Ebenso ist die körperliche Verfassung eines Lebewesens von großer Bedeutung. Bei Menschen und allgemein Säugetieren ist dabei vor allem der Gesundheitszustand, insbesondere der Zustand des Immunsystems, das Geschlecht, Alter, Körpergewicht und eine mögliche Toleranz durch frühere Gaben des Toxins von Bedeutung. Insekten wie beispielsweise Läuse zeigen dagegen oft eine höhere Immunität gegenüber Giftstoffen.

Bei der Nanotechnologie können Substanzen wegen ihrer (Nano-)Größe toxisch werden, weil ihnen diese erlaubt, Körperschranken (Haut, Lunge, Blutkreislauf, Gehirn usw.) zu durchbrechen.

Die Wirkungen toxischer Substanzen lassen sich teilweise durch natürliche oder künstlich hergestellte Gegengifte aufheben oder zumindest unter die tödliche Dosis abmildern.

Gifte greifen an unterschiedlichen Rezeptoren im Organismus an. Häufig betroffene Organe bei akuten Vergiftungen sind Leber (Hepatotoxine, zum Beispiel durch Paracetamol), Niere (Nephrotoxine) sowie Gehirn und Nerven (Nervengifte wie Botulinumtoxin und Kampfstoffe wie VX, Sarin oder Soman). Einige Gifte greifen in die innere Atmung ein, so zum Beispiel Nitrite und Kohlenstoffmonoxid, die das Hämoglobin blockieren, oder Kaliumcyanid (Cyankali), das die Atmungskette der Zellen blockiert.

Um die Giftigkeit (Toxizität) von Toxinen miteinander vergleichen zu können, werden Tierversuche unter standardisierten Bedingungen herangezogen. Die häufig angegebene LD zum Beispiel gibt an, welche Stoffmenge, bezogen auf das Körpergewicht, bei der Hälfte einer Versuchstierpopulation zum Tod führt. Dabei steht LD für "letale Dosis".

Einige der am stärksten bekannten Giftstoffe werden unter dem Sammelbegriff Botulinumtoxin zusammengefasst, diese können unter anderem in verdorbenen Fleisch- und Fischkonserven oder in Käse vorkommen.

Nach dem Verhalten des Giftstoffes an den Rezeptoren werden zwei Arten von Giften unterschieden:



Während allgemein giftige Schadstoffe als "umweltgefährlich" (N) eingestuft werden, werden Stoffe nach der Wirkung auf den Menschen als Gefahrstoff in "sehr giftig" (T+), "giftig" (T) sowie "gesundheitsschädlich" (Xn) (veraltet „mindergiftig“) eingestuft.

Nach der neueren Einstufung nach dem Global harmonisierten System zur Einstufung und Kennzeichnung von Chemikalien erfolgt die Einteilung in "Akut Toxisch" (Symbol 06), Gesundheitsgefahr (Symbol 08) und diversen anderen Gesundheitsgefahren (Symbol 07).

Die Regelungen sind EU-weit konform. Nach dem schweizerischen Giftgesetz erfolgte die Einteilung in Giftklassen, seit 2005 gelten aber auch die EU-Gefahrensymbole.

Als Gefahrgut im Transport, die auf der Straße durch das ADR geregelt wird, haben Giftstoffe die Gefahrgutklasse "6.1 – Giftige Stoffe" oder, im Fall von Gasen, "2" mit den Gefahrengraden T (giftig);
TF (giftig und entzündlich);
TC (giftig und ätzend);
TO (giftig und brandfördernd);
TFC (giftig, entzündlich und brandfördernd);
TOC (giftig, brandfördernd und ätzend) und eine Nummer zur Kennzeichnung der Gefahr (Kemler-Zahl) 6.

Als "Giftige Substanz" tragen Giftstoffe typischerweise die R-Sätze 20–28 ("Gesundheitsschädlich/Giftig/Sehr giftig beim Einatmen/bei Berührung mit der Haut/beim Verschlucken"), R29, 31, 32 ("Entwickelt giftige Gase" bei Berührung mit anderen Substanzen), sowie R50–59 (Umweltgifte). Aber auch etliche andere R-Sätze beschreiben Giftwirkungen im medizinischen oder rechtlichen Sinne (Reizwirkung, Krebsrisiko, Erbgutschädigend, …).

Eine Liste der in Wikipedia beschriebenen giftigen und sehr giftigen Stoffe befindet sich in der .

Nach herrschender Ansicht ist ein Gift jeder organische oder anorganische Stoff, der nach seiner Art, der beigebrachten Menge, der Form der Beibringung und der Körperbeschaffenheit des Opfers durch chemische oder chemisch-physikalische Wirkung die Gesundheit zu beschädigen geeignet ist.

Der Gesetzgeber bezieht sich dabei ausdrücklich auf die Klassifikation als Gefahrstoff (etwa § 3 Abs. 1 Z 6 und 7 ChemG 1996, Österreich), wobei insbesondere auch die als "gesundheitsschädlich" bezeichneten Stoffe miteinbezogen sind (etwa § 35 Z 1 ChemG 1996). Sowohl den Chemikaliengesetzen wie auch der Gefahrstoffverordnungen reicht schon ein hinreichend begründeter Verdacht auf Giftigkeit, einen Stoff als "Gift" einzustufen.

Das Beibringen von Gift wird (in Deutschland nach § 224 Abs. 1 Nr. 1 Alt. 1 StGB) als gefährliche Körperverletzung bestraft.

Tabelle der LD-Werte einiger Stoffe in verschiedenen Arten:
Toxine sind Gifte, die von Lebewesen synthetisiert werden.

Pflanzliche Gifte:

Von Mikroorganismen produzierte Gifte:

Pilzgifte (giftige Großpilze):

Tierische Gifte:

Anorganische Verbindungen:

Organische Verbindungen:





</doc>
<doc id="11041" url="https://de.wikipedia.org/wiki?curid=11041" title="Braunau">
Braunau

Braunau steht für:

Gemeinden:
Gemeindeteile:

Historisch:

Flüsse:


</doc>
<doc id="11042" url="https://de.wikipedia.org/wiki?curid=11042" title="Kolik">
Kolik

Als Kolik (von lat. "cōlicus" „den Grimmdarm betreffend“) werden stärkste, bewegungsunabhängige, meist wehenartige Schmerzen bezeichnet, die durch krampfhafte Kontraktionen der glatten Muskulatur eines Hohlorganes verursacht werden. Eine Kolik entsteht, wenn die Muskulatur einem großen Widerstand oder einer Blockade entgegenwirken muss. Das Wort "wehenartig" bedeutet hier, dass die Schmerzen wellenförmig kommen und sich mit Phasen relativer Beschwerdefreiheit abwechseln. Ursprünglich bezog sich der medizinische Fachbegriff auf Schmerzen im Dick- bzw. exakter Grimmdarm, dessen lateinische Bezeichnung "Colon" später als Adjektiv "colicus" substantiviert gebraucht wurde und das längere "dolor coli" „Schmerz im Grimmdarm“ ersetzte.

Koliken können sich organbezogen als Nierenkolik, Gallen- oder Darmkolik, bei Säuglingen als sogenannte Dreimonatskolik äußern. Seltener treten sie als Schmerzen im Bereich der Harnblase, des Magens, der Bauchspeicheldrüse, Gebärmutter, der Samenwege und der Speicheldrüsen auf. Damit verbunden sind Beschwerden wie Blutdruckanstieg, Tachykardie, Schweißausbrüche, Übelkeit und Erbrechen, evtl. auch Kreislaufkollaps. 

Koliken zählen zu den Symptomen der chronisch-entzündlichen Darmerkrankungen Colitis ulcerosa und Morbus Crohn.



</doc>
<doc id="11043" url="https://de.wikipedia.org/wiki?curid=11043" title="Oberösterreich">
Oberösterreich

Oberösterreich ist ein österreichisches Bundesland; Landeshauptstadt ist Linz. Oberösterreich ist mit 11.982 Quadratkilometern flächenmäßig das viertgrößte und mit 1,47 Millionen Einwohnern bevölkerungsmäßig das drittgrößte Bundesland Österreichs. Es grenzt an Bayern (Deutschland), Südböhmen (Tschechien) sowie innerösterreichisch an Niederösterreich, die Steiermark und das Land Salzburg. Der Name des Landes leitet sich ab vom Namen des Vorgängerterritoriums, des Erzherzogtums "Österreich ob der Enns", einem der habsburgischen Erblande.

Oberösterreich hat Anteil an drei großen Naturräumen. Von Norden nach Süden findet man in Oberösterreich eine geologisch-landschaftliche Dreiteilung, die sich westlich bis nach Bayern einerseits und östlich bis nach Niederösterreich andererseits fortsetzt.
Höchster Punkt des Landes ist der "Hohe Dachstein" (2995 Meter) an der Südspitze Oberösterreichs, mit dem einzigen Gletschergebiet des Landes. Da die Grenze zur Steiermark am Kalkalpen-Hauptkamm liegt, ist der höchste Berg, der sich vollständig auf oberösterreichischem Boden befindet, der Große Priel mit 2515 Metern. 
Der tiefste Punkt des Landes ist dort, wo die Donau endgültig nach Niederösterreich wechselt, östlich des Machlands am Eingang in den Nibelungengau.

Praktisch alle großen oberösterreichischen Seen liegen im Salzkammergut, so der Almsee, Attersee, die Gosauseen, Hallstätter See, Irrsee, Langbathseen, Mondsee, Offensee, Traunsee und der Wolfgangsee.

Ach, Ager, Aist, Alm, Antiesen, Aschach, Donau, Enns, Große Gusen, Kleine Gusen, Gusen, Inn, Krems, Mattig, Große Mühl, Kleine Mühl, Naarn, Rodl, Salzach, Steyr, Trattnach, Traun, Vöckla;
Nach der heutigen Verwaltungseinteilung gliedert sich das Bundesland in:

Oberösterreich wird traditionell in vier Teile eingeteilt, das "Hausruckviertel", das "Innviertel", das "Mühlviertel" und das "Traunviertel". Der Bereich zwischen den Städten Linz, Eferding, Wels, Steyr und Enns wird – als „fünftes Viertel“ – "Zentralraum" genannt.
Diese fünf Regionen bilden auch ungefähr die statistischen Oberösterreichs, wobei aber das Hausruckviertel aufgeteilt ist, um den Ansprüchen statistisch etwa gleichumfassender Areale nachzukommen:
Bevor 1779 das Innviertel ein Teil Oberösterreichs wurde, gab es folgende Vierteleinteilung: "Mühlviertel", "Schwar(t)zviertel" oder "Machlandviertel", "Haus(ruck)viertel", "Traunviertel". Mit der Eingliederung des Innviertels wurden Machland- und Mühlviertel unter letzterem Namen zusammengefasst, um weiterhin die Vierteilung aufrechtzuerhalten. Die heutige Abgrenzung zwischen Traun- und Hausruckviertel orientiert sich an den Bezirksgrenzen jüngeren Datums und entspricht somit nicht mehr der historischen Grenze, welche durch die Traun gebildet wurde. Die Viertel Oberösterreichs haben in ihren ursprünglichen Grenzen heute neben ihrer volkstümlich-identitätsstiftenden Bedeutung nur mehr den Zweck, Wahlkreise zu definieren.

Nördlich des Donautales befindet sich die Böhmische Masse (auch Böhmisches Massiv), die geologisch älteste Landschaft Österreichs. Sie ist ein altes Faltengebirge und besteht im westlichen Teil aus dem Moldanubikum, im östlichen Teil (außerhalb von Oberösterreich) aus dem Moravikum. Die Böhmische Masse stellt den Sockel eines abgetragenen, einstigen Hochgebirges (Grundgebirge genannt) dar, das im Zuge der Variszischen Orogenese (Gebirgsbildung) im Paläozoikum entstand. Weitere Reste dieser Gebirgsbildung in Mitteleuropa sind die deutschen Mittelgebirge. Es dominieren saure Plutonite wie Granite und Gneise. Das an sich zur Gänze abgetragene Gebirge wurde vermutlich im Zuge der alpidischen Gebirgsbildung in Schollen gebrochen und etwas gehoben, wodurch seine heutige Topographie eines Hügellandes resultiert "(Rumpflandschaft)".

Südlich der variszischen Gebirgskette erstreckte sich damals die Tethys, die beim Auseinanderdriften der Kontinentalplatten gegen Ende des Paläozoikums immer größer wurde. Unter tropischen bzw. subtropischen Bedingungen wurden hier während des Mesozoikums jene Sedimente abgelagert, die dann später bei der alpidischen Gebirgsbildung, die gegen Ende der Kreide einsetzte, überschoben und nach Norden transportiert wurden. So entstanden die Süd-Nord-Abfolge von Decken, die nördlichen Kalkalpen, die Flyschzone und die Subalpine Molasse, wobei auch noch Reste der Helvetischen Decke erhalten sind. Der in der Trias in der Tethys entstandene Kalk, ist reich an Fossilien, die man heute besonders im Dachsteingebirge und um Hallstatt findet. Besondere Fundorte für Ammoniten sind die Berge um Gosau, westlich des Dachsteins.

Während sich die Alpen zunächst als Inselkette aus der Tethys erhoben und immer weiter anwuchsen, setzte zur selben Zeit bereits der Abtragungsprozess des jungen Gebirges ein, der jedoch das Maß der Hebung nicht ausgleichen konnte.

Zwischen diesen beiden sehr unterschiedlichen Gebirgen befindet sich eine Sedimentationszone, die durch die Ablagerungen der Erosion in den Alpen entstanden ist, die sogenannte Titenzone.

Das nach Norden hin transportierte Material der Abtragung wurde zunächst in den flachen und immer schmaler werdenden Arm der Tethys zwischen den Alpen im Süden und dem Kontinent im Norden abgelagert (Molassebecken). So wurde bei gleichzeitig andauernder Hebung der Alpen und nordwärts gerichteter Bewegung der afrikanischen Platte der Meeresarm zugeschüttet (Süßwassermolasse) und es entstand das heutige Bild der geologischen Dreiteilung Oberösterreichs in die Böhmische Masse, das Tertiärhügelland als Ablagerungsgebiet für die klastischen Sedimente der alpinen Erosion in der Mitte und den Nördlichen Kalkalpen im Süden.

Der unserem heutigen Zeitalter, dem Holozän, vorangegangene Teil der Erdgeschichte, das Pleistozän oder Eiszeitalter, hat das heutige Landschaftsbild Oberösterreichs in den Alpen und im Alpenvorland am deutlichsten geprägt. Es war dies die Zeit der bis heute letzten großen Vereisungsphase in den Alpen, in welcher das Gebirge zu den größten Teilen von Eismassen bedeckt war, die mit gewaltigen Gletscherzungen weit ins Vorland hinaus vorstießen. Neben dem Dachsteingletscher stieß der Salzachgletscher ins heutige Oberösterreich vor und übertraf diesen an Größe beträchtlich. Gespeist von den Eismassen der Zentralalpen wälzte sich der Gletscher mindestens viermal (so die klassische Quartärstratigraphie) durch das Salzachtal und das Salzburger Becken nach Norden, um sich dann in ein verzweigtes, fächerförmiges System an Seitengletschern auszubreiten. Während Hausruck und Kobernaußerwald nördlich der Traun nicht erodiert wurden, sind die benachbarten Regionen im Salzburger und dem Südinnviertler Seengebiet vom Salzachgletscher überfahren worden.

Oberösterreich
befindet sich klimatisch in der Zone des mitteleuropäischen Übergangsklimas. Aufgrund der Lage am Nordrand der Alpen ist das Wetter deutlich atlantisch beeinflusst. Der Zentralraum zeigt warmgemäßig-vollfeuchten Typus (Buchenklima, codice_1 nach Köppen/Geiger) mit vorherrschendem Nordwestwindwetter. Der Süden liegt in der Zone eines ausgeprägten Nordstaus, und hat mithin die höchsten Niederschläge Österreichs aufzuweisen. Es kommt auch des Öfteren zu Föhn (Alpenklima, nördlicher Randalpentypus). Das Hochland des Nordens ist aber schon deutlicher subpolar geprägt, boreal-vollfeucht (codice_2 nach Köppen/Geiger) und gemäßigter als im Alpenraum, aber mit um ein Grad Celsius niedrigerem Jahresmittel.

Das wärmste Gebiet in Oberösterreich ist das Linzer Becken mit einem Jahresmittel von rund neun Grad Celsius. Mit Ausnahme der Gebirge liegen die Durchschnittstemperaturen der restlichen Landesteile wie Alpenvorland, Eferdinger Becken und Traun-Enns-Platte im Bereich von sechs bis acht Grad Celsius (Jahresmittel von 1961 bis 1990). In 2000 Metern Höhe beträgt die Jahresdurchschnittstemperatur etwa ein Grad.

Die niederschlagsärmsten Gebiete mit Jahresniederschlagsmengen zwischen 750 und 800 Millimetern liegen im östlichen Mühlviertel (Feldaistsenke) und im Eferdinger Becken. Die höheren Bergregionen des Mühlviertels und des Sauwaldes, sowie das Alpenvorland werden von der 1000 Millimeter Isohyete umschlossen. Im Gebirgsbereich sind die Niederschlagsmengen aufgrund der Stauwirkung der Wolken jedoch viel höher. In Höhen oberhalb von 1500 Metern Seehöhe werden Jahresniederschläge von 2000 Millimetern und mehr (im Dachsteingebirge 3000 Millimeter) erreicht.

Durch die Gliederung in Höhenstufen (von 239 bis 2995 Meter) sowie in die drei biogeographischen Regionen (Böhmische Masse, klimatisch begünstigter Zentralraum und Kalkalpen) beherbergt Oberösterreich eine für mitteleuropäische Verhältnisse artenreiche Flora von etwa 1800 Gefäßpflanzen. Wie überall in Mitteleuropa sind weite Teile der Landschaft stark durch den Menschen geprägt. Für den Erhalt von natürlichen und naturnahen Lebensräumen befinden sich in Oberösterreich 156 Schutzgebiete in Natur- und Landschaftsschutz, davon 24 Europaschutzgebiete (Natura-2000-Gebiete und andere, teils überlappend mit den 164 landesrechtlichen Gebieten), der Nationalpark Kalkalpen, 109 Naturschutzgebiete, 16 Landschaftsschutzgebiete (darunter 2 Naturparks), 7 geschützte Landschaftsteile, und 562 Naturdenkmale.

In Oberösterreich wurden in freier Natur bisher 75 Arten von Säugetieren festgestellt. Davon sind Manguste und Nutria Gefangenschaftsflüchtlinge; Mufflon und Alpenmurmeltier wurden ausgesetzt. In den letzten 100 Jahren wurden im Gebiet 361 Vogelarten nachgewiesen, viele jedoch nur ein einziges Mal. 163 Vogelarten brüten in Oberösterreich. Des Weiteren wird das Gebiet von sieben Arten von Schwanzlurchen, zwölf Froschlurcharten und elf unterschiedlichen Arten von Reptilien als Lebensraum bewohnt.

Im Mittelalter gehörte ein großer Teil Oberösterreichs lange Zeit zum Herzogtum Steiermark.
König Ottokar Přemysl von Böhmen trennte den zur Steiermark gehörenden Traungau 1254 im Frieden von Ofen und 1261 im Frieden von Wien von dieser ab und gestaltete das Land zum "Fürstenthum ob der Enns" aus. Julius Strnadt bezeichnet mit historischer Wahrscheinlichkeit das Jahr 1260 als Geburtsjahr des Landes ob der Enns. Im Jahre 1264 wurde der Name "supra anasum" (‚Ob(erhalb) der Enns‘) erstmals urkundlich erwähnt, und Konrad von Sumerau wird in einer Urkunde als "Landrichter der Provinz Oberösterreich" bezeichnet, obwohl die Bezeichnung "Austria superior" („Oberösterreich“) zu diesem Zeitpunkt noch für Tirol und Vorderösterreich galt.

Nach 1490 erlangte das Gebiet als Teilfürstentum "Österreich ob der Enns" eine gewisse Selbstständigkeit im Heiligen Römischen Reich und die Stände hielten eigene Landtage in Linz ab. Neben Herren, Rittern und Prälaten spielten dabei auch die landesfürstlichen Städte eine wichtige Rolle. Ab 1520 öffnete sich das Land der Reformation, 30 Jahre später waren die Oberösterreicher mehrheitlich evangelisch. Bei der Habsburgischen Länderteilung von 1564 fiel Oberösterreich zusammen mit Niederösterreich und den böhmischen Ländern an den römisch-deutschen Kaiser Maximilian II. Nach 1600 setzte unter Kaiser Rudolf II. und seinem Nachfolger Matthias die Gegenreformation ein. Deshalb gingen die Stände in Oberösterreich 1619 ein Bündnis mit den böhmischen Ländern ein. Im Auftrag Kaiser Ferdinands II. bekämpften und besiegten die Truppen des baierischen Kurfürsten Maximilian I. 1620 die Aufständischen. Für einige Jahre kam das Fürstentum unter die Herrschaft des Kurfürsten. Der evangelische Adel bekam die Wahl, zu konvertieren oder das Land zu verlassen.

1779 kam im Frieden von Teschen das vorher zum Herzogtum Baiern gehörende Innviertel zu Oberösterreich. Während der Napoléonischen Kriege wurde Oberösterreich mehrfach von französischen Truppen besetzt. Die Landeshauptstadt Linz verwaltete von 1814 bis 1854 auch das Land Salzburg. 1918, nach dem Ersten Weltkrieg und dem Untergang des Vielvölkerstaates Österreich-Ungarn, wurde von der neuen Republik Deutschösterreich „Oberösterreich“ als offizieller Name der Region festgelegt. Ein Jahr nach dem Anschluss Österreichs an das Deutsche Reich am 13. März 1938 wurde zum 1. Mai 1939 auf dem Gebiet Oberösterreichs der "Reichsgau Oberdonau" gebildet, der auch die deutsch besiedelten südböhmischen Gebiete gemäß dem Münchner Abkommen einschloss, sowie das von der Steiermark abgetrennte Ausseer Land. Nach dem Zweiten Weltkrieg wurden diese Gebiete 1945 wieder rückgegliedert, Oberösterreich südlich der Donau wurde inklusive des Ausseer Landes bis 1955 US-amerikanische Besatzungszone, nördlich der Donau war es bis 1955 sowjetisch besetzt.

Die Oberösterreichische Landesverfassung definiert Oberösterreich als ein selbständiges Bundesland der demokratischen Republik Österreich. Oberösterreich bekennt sich in seiner Verfassung zudem zu einem geeinten Europa, das demokratischen, rechtsstaatlichen, sozialen und bundesstaatlichen Grundsätzen sowie dem Grundsatz der Subsidiarität verpflichtet ist, die Eigenständigkeit der Regionen wahrt und deren Mitwirkung an europäischen Entscheidungen sichert. Oberösterreich definiert seine Stellung in Europa in der Landesverfassung als eigenständige, zukunftsorientierte und selbstbewusste Region, die an der Weiterentwicklung eines geeinten Europas selbst mitwirkt.

Die Legislative wird in Oberösterreich vom Oberösterreichischen Landtag ausgeübt. Die Hauptaufgabe des Landtags liegt in der Gesetzgebung für Oberösterreich. Des Weiteren wählt der Landtag die Landesregierung und kann diese mittels Misstrauensvotum entlassen. Zudem bewilligt der Landtag das Landesbudget und hat das Recht, schriftliche und mündliche Anfragen an die Landesregierung zu stellen. Derzeit bestehen in Oberösterreich rund 170 Landesgesetze. Diese können auf Vorschlag der Regierung, einem Landtagsausschuss, dreier Abgeordnete oder auf Initiative der Landesbevölkerung in den Landtag eingebracht werden. Der Landtag tritt mindestens einmal pro Monat zu einer öffentlichen Sitzung im Linzer Landhaus zusammen. Die Abgeordneten werden alle sechs Jahre durch Wahlen bestimmt. Die ÖVP erreichte bei den Landtagswahlen seit 1945 fast durchgehend die Mandatsmehrheit, mehrfach bestimmte sie den Landtag auch mit einer absoluten Mandatsmehrheit, zuletzt 1979 bis 1991. Lediglich 1967 konnte die SPÖ die ÖVP bei den Landtagswahlen stimmenmäßig überholen und an Mandaten mit der ÖVP gleichziehen. Seit 1967 verlor die SPÖ kontinuierlich an Stimmen, gewann jedoch bei der Landtagswahl 2003 massiv Stimmen von der FPÖ, die 1997 ihren Höchststand erreicht hatte, im Jahr 2003 wieder Wählerstimmen verlor und sogar von den Grünen überholt wurde. Die Landtagswahl 2009 brachte leichte Gewinne für die ÖVP und starke Verluste für die SPÖ. Die Grünen konnten mit minimalen Gewinnen ihr Landesrats-Mandat verteidigen, wurden aber aufgrund deren starker Zugewinne wieder durch die FPÖ überholt. Nach der Landtagswahl 2009 war die ÖVP mit 28, die SPÖ mit 14, die FPÖ mit 9 und die Grünen mit 5 Mandaten im Landtag vertreten. Nach der Landtagswahl 2015 ist die ÖVP mit 21, die FPÖ mit 18, die SPÖ mit 11 und die Grünen mit 6 Mandaten im Landtag vertreten.

Die Exekutive wird im Land von der Oberösterreichischen Landesregierung ausgeübt. Diese besteht aus dem Landeshauptmann, zwei Stellvertretern und sechs Landesräten. Die Zusammensetzung der Landesregierung erfolgt als „Konzentrationsregierung“ nach dem Proporzsystem, d. h. alle Parteien mit einer bestimmten Anzahl von Abgeordneten im Landtag sind durch mindestens einen Sitz in der Regierung vertreten. Die Wahl der Landesregierung erfolgt durch den Landtag. Die Landesregierung vollzieht als oberstes Vollzugsorgan der Landesverwaltung die Landesgesetze und verwaltet das Landesbudget. An der Spitze steht der Landeshauptmann, der die Regierung nach außen vertritt und den Vorsitz in den wöchentlichen, nichtöffentlichen Sitzungen im Landhaus führt. Nach der Geschäftsordnung der Oberösterreichischen Landesregierung behandelt die Landesregierung in ihren Sitzungen kollegial Regierungsvorlagen an den Landtag, Rechtsverordnungen und bestimmte Verwaltungsverordnungen, (verfassungs)gesetzlich an eine kollegiale Beschlussfassung gebundene Angelegenheiten und Entscheidungen über das Landesvermögen von besonderer Bedeutung wie Förderungen über 20.000 Euro. Sämtlich anderen Entscheidungen trifft das jeweilige Regierungsmitglied selbstständig, die Landesregierung kann jedoch monokratische Entscheidungen an sich ziehen und einer kollegialen Beschlussfindung zuführen.

Die ÖVP stellt seit 1945 durchgehend den Landeshauptmann. Ab dem 2. März 1995 hatte Josef Pühringer diese Funktion inne. Nach der Landtagswahl 2003 ging Pühringer eine Regierungsvereinbarung mit den Grünen ein und bildete in Österreich die erste Schwarz-Grüne Koalition auf Länderebene. Während die FPÖ ihre Sitze in der Landesregierung verlor, zogen die Grünen erstmals in die Landesregierung ein. Nach den Landtagswahlen 2015 wurde am 23. Oktober 2015 erstmals eine Landesregierung mit einem schwarz-blauen Arbeitsübereinkommen im Rahmen einer Proporzregierung gewählt und angelobt. In der Landesregierung Pühringer V waren vier ÖVP- sowie drei FPÖ-Regierungsmitglieder vertreten, die SPÖ und die Grünen stellen je einen Landesrat. Neben Landeshauptmann Pühringer wurde die ÖVP in der Regierung von Landeshauptmann-Stellvertreter Thomas Stelzer und den Landesräten Michael Strugl und Maximilian Hiegelsberger vertreten. Am 6. April 2017 wurde Thomas Stelzer Landeshauptmann, die Landesregierung Stelzer folgte der Landesregierung Pühringer V nach, neue Landesrätin wurde Christine Haberlander, Landeshauptmann-Stellvertreter Michael Strugl. Die FPÖ wird durch Landeshauptmann-Stellvertreter Manfred Haimbuchner und die Landesräte Elmar Podgorschek und Günther Steinkellner vertreten. Für die SPÖ sitzt Landesrätin Birgit Gerstorfer in der Regierung, für die Grünen Landesrat Rudi Anschober.

Oberösterreich ist der Sitz des Oberlandesgerichts (OLG) Linz, einem der vier Oberlandesgerichte in Österreich. Neben Oberösterreich betreut das OLG Linz auch das Nachbarbundesland Salzburg. An den Standorten Linz, Ried im Innkreis, Steyr und Wels verfügt Oberösterreich zudem über vier Landesgerichte. Die unterste Ebene des Gerichtswesens wird in Oberösterreich von den 28 Bezirksgerichten gebildet.
Das Land Oberösterreich bietet für die einfachere Bewältigung von Behördenwegen E-Government-Lösungen an. Mit diesen Online-Formularen können Bürger u. a. Anträge auf Beihilfen und Förderungen, die OÖ Familienkarte oder auch die Anerkennung einer Photovoltaikanlage als Ökostromanlage stellen. Dabei wird der Formularserver AFORMSOLUTION (AFS) des österreichischen IT-Dienstleisters aforms2web verwendet.

Landessymbole sind die Farben (Fahne und Flagge) des Landes Oberösterreich, das Landeswappen, das Landessiegel und die Landeshymne.

Das Oberösterreichische Wappen besteht aus einem, mit dem österreichischen Erzherzogshut gekrönten, gespaltenen Schild. Der Schild zeigt heraldisch rechts einen goldenen Adler mit roter Zunge und roten Krallen auf schwarzem Grund und ist heraldisch links dreimal von Silber und Rot gespalten. Das Landeswappen kann in Farbe oder Schwarz-Weiß dargestellt werden. Das Wappen wurde 1930 festgelegt und geht auf das Wappen der Herren von Machland zurück.

Die Farben des Landes Oberösterreich sind Weiß-Rot. Die Oberösterreichische Flagge besteht aus zwei gleich breiten waagrechten Streifen, wobei der obere Streifen in der Farbe Weiß und der untere Streifen in der Farbe Rot gehalten ist. Das Verhältnis der Höhe der Flagge zu ihrer Länge beträgt 2:3. Die Flagge wurde offiziell am 25. April 1949 eingeführt.

Das Lied "Hoamatgsang" wurde vom oberösterreichischen Landtag am 29. November 1952 zur oberösterreichischen Landeshymne erklärt. Der Text wurde 1841 von Franz Stelzhamer geschrieben, die Musik komponierte 1884 Hans Schnopfhagen. Von den ursprünglich acht Strophen sind die ersten zwei und die letzte Strophe Teil der Landeshymne.

Die Landespatrone von Oberösterreich sind gleichrangig der heilige Florian und der heilige Leopold.

Am wohnten im Land Menschen, davon 81.732 (5,58 %) EU/EWR/CH-Bürger und 90.940 (6,21 %) Drittstaatsangehörige. Das Mühlviertel wird schon seit dem Mittelalter von einigen Hundert Sinti bewohnt, die wenigsten davon bekennen sich bei Zählungen tatsächlich zu ihrer Volksgruppe.

Der Großteil der in den letzten Jahrzehnten zugewanderten Bevölkerung stammt aus Deutschland, Südosteuropa und Anatolien, wobei 1,56 % aus Deutschland, 1,41 % aus Bosnien und Herzegowina, 1,11 % aus Serbien, Montenegro und dem Kosovo und 1,02 % aus der Türkei stammen. Durch die große Flüchtlingswelle seit dem zweiten Halbjahr 2015 ist die Zahl der Menschen aus Afghanistan auf 7.196 (0,49 %) und aus Syrien auf 4.943 (0,34 %) angestiegen.

Die Oberösterreicher sind mehrheitlich christlich geprägt: 2001 gehörten noch 79,4 % der Menschen der römisch-katholischen Kirche an; etwa 61.000 Personen (4,4 %) waren Angehörige der evangelisch-lutherischen Kirche und 4,0 % bekannten sich zum Islam. 8,8 % waren ohne Bekenntnis.

Bis Ende 2016 ist der Anteil der Katholiken auf 65,9 % zurückgegangen, und damit erstmals unter zwei Drittel der oberösterreichischen Bevölkerung gefallen.

Die Entwicklung der Bevölkerungszahlen nach den Angaben des österreichischen Statistikamtes wurden auf den heutigen Gebietsstand Oberösterreichs umgerechnet. Die in der Tabelle angegebenen Zahlen bis 1700 wurden gerundet. Zwischen 1754 und 1857 zählte man nur die anwesende Zivilbevölkerung. Ab 1869 wurden Volkszählungen in zehnjährigen Abständen durchgeführt. Bis 1923 wurde weiterhin nur die anwesende Zivilbevölkerung gezählt und erst ab 1934 bis 1981 die Wohnbevölkerung. Die Zahlen von 1982 bis 2001 weisen die Jahresdurchschnittsbevölkerung aus; diese Zahlen wurden 2002 rückwirkend ermittelt. Seit 2002 werden die Hauptwohnsitze auf der Grundlage des Zentralen Melderegisters zur Bevölkerungsermittlung herangezogen.

In Oberösterreich wird hauptsächlich der mittelbairische Dialekt gesprochen.

Neben der Kulturförderung betreibt das Land Oberösterreich auch selbst einige Kultureinrichtungen wie etwa das Oberösterreichische Landesmuseum oder das Landestheater in der Landeshauptstadt Linz. Initiiert vom ORF-Oberösterreich und vom Linzer Brucknerhaus findet seit 1979 alljährlich in der Landeshauptstadt die Ars Electronica, das größte internationale Festival für digitale Kunst, statt.
In Oberösterreich bestehen zahlreiche Stifte und Klöster, die seit jeher Zentren der Kultur sind. Die bekanntesten sind Stift Sankt Florian, Stift Wilhering und Stift Kremsmünster, weiters die Stifte Schlägl, Schlierbach, Reichersberg, Engelszell und Lambach.

Die berühmtesten Kirchen sind der 1862 bis 1935 erbaute neugotische Linzer Mariä-Empfängnis-Dom, dies ist die größte Kirche Österreichs; die Linzer Martinskirche, eine der ältesten Kirchen Österreichs; die Wallfahrtskirchen Pöstlingbergkirche, Wallfahrtsbasilika Maria Puchheim und Wallfahrtskirche Stadl-Paura; die Stadtpfarrkirche Braunau mit dem dritthöchsten Kirchturm Österreichs; die Kirchen in St. Wolfgang und Kefermarkt mit ihren Flügelaltären. Die Pfarrkirchen von Niederkappel und Aigen stellen bedeutende Bauwerke des Historismus dar. Bemerkenswert ist auch die Pfarrkirche Ebelsberg mit der einzigen Jugendstileinrichtung in Oberösterreich. Bedeutende Kirchenbauten der Moderne sind die Linzer Friedenskirche und die Pfarrkirche Attnang.
Bekannte Burgen in Oberösterreich sind die Burg Clam, die Otto von Machland 1149 errichten ließ, weiters die Burg Altpernstein und Wildberg sowie die Ruinen Schaunburg, Scharnstein und Waxenberg.

Die bedeutendsten Schlösser sind das Linzer Schloss, Schloss Ort in Gmunden, Schloss Ennsegg in Enns, Schloss Lamberg in Steyr, Schloss Parz bei Grieskirchen, Schloss Starhemberg in Eferding, Schloss Greinburg, Schloss Weinberg und das Wasserschloss Aistersheim.
Das größte Theater in Oberösterreich ist das Landestheater Linz mit den Spielstätten Musiktheater und dem Schauspielhaus. Es bietet die Sparten Oper, Operette, Musical, Ballett und Schauspiel an. Eng mit dem Theater und dem Brucknerhaus verbunden ist auch das Bruckner Orchester Linz als größtes Symphonieorchester Oberösterreichs und eines der besten Orchester in Österreich.

Bekannt sind auch die Stadttheater in Wels, Grein und Bad Hall sowie das Lehár Festival Bad Ischl mit seinen berühmten Operettenaufführungen.

Bemerkenswert ist auch die Tradition der Blasmusik in Oberösterreich. Es existieren mehr Musikkapellen als das Land Gemeinden zählt. Diese sind vereinsmäßig organisiert und spielen großteils auf hohem musikalischen Niveau. Es gibt auch zahlreiche Musikgruppen, die die traditionelle Volksmusik pflegen.
Das größte Museum ist das Linzer Schlossmuseum der Oberösterreichischen Landesmuseen. Bekannt sind auch die Landesgalerie, das Lentos Kunstmuseum Linz, das Nordico-Stadtmuseum Linz, das Ars Electronica Center Linz, das Museum Arbeitswelt Steyr, das Mühlviertler Schlossmuseum Freistadt, das Museum Angerlehner in Thalheim bei Wels und das Kubin-Haus in Zwickledt. Des Weiteren existieren zahlreiche kleinere Museen in den Gemeinden.

In Oberösterreich bestehen derzeit vier historische Gartenanlagen, die seit dem 1. Jänner 2000 in die rechtliche Kompetenz des Bundes fallen und unter Denkmalschutz gestellt wurden. Zu den geschützten historischen Garten- und Parkanlagen gehören der Park der Kaiservilla in Bad Ischl, die Gartenanlage der Villa Toscana in Gmunden, der Jugendstilpark am Linzer Bauernberg und der Park von Schloss Neuwartenburg (Timelkam). Darüber hinaus bestehen in Oberösterreich rund 160 historische Gartenanlagen, insbesondere als Teil von Schlossanlagen. Eine besonders hohe Anzahl historischer Parks besteht in der Landeshauptstadt Linz sowie in den Zentren der Sommerfrische des 19. Jahrhunderts in Bad Ischl und Gmunden.

Seit dem Jahr 2005 werden in Oberösterreich alle zwei Jahre Landesgartenschauen veranstaltet. Zielsetzung der Landesgartenschauen sind die Schaffung von Lebensräumen und Grünzonen unter umweltpolitischen und ökologischen Gesichtspunkten in den oberösterreichischen Gemeinden. Die Landesgartenschauen sollen dabei Gestaltungsmöglichkeiten in der Grünraum- und Siedlungsgestaltung sowie in der Gartenkultur aufzeigen. Die Auswahl der veranstaltenden Gemeinde erfolgt durch einen Fachbeirat. Die letzten Landesgartenschauen wurden 2007 in Vöcklabruck, 2009 in Bad Schallerbach, 2011 in Ansfelden/Ritzlhof und 2015 in Bad Ischl ("Des Kaisers neue Gärten") durchgeführt.
Bedeutende industrielle Baudenkmäler sind das von Mauriz Balzarek entworfene Jugendstilkraftwerk Steyrdurchbruch, das an der Grenze zu Bayern errichtete Donaukraftwerk Jochenstein und die im Stil der Neuen Sachlichkeit erbaute Tabakfabrik Linz.

Erwähnenswert sind auch die in der NS-Zeit in Linz erbauten Brückenkopfgebäude mit der Nibelungenbrücke und die zahlreichen Wohnsiedlungen dieser Zeit, sogenannte Hitlerbauten, in den Linzer Stadtteilen Urfahr, Bindermichl und Spallerhof sowie in Steyr-Münichholz.

In Oberösterreich gab es im Jahr 2012 27 Kinos mit 87 Kinosälen und rund 2,7 Millionen Kinobesuchen. Nach einem langjährigen Rückgang dieser Zahlen mit einem Tiefpunkt im Jahr 1992 mit nur 1,2 Millionen Kinobesuchen bzw. 1994 mit nur 60 Kinosälen stiegen die Zahlen seither wieder an und halten nun einen Wert, der ungefähr jenem vom Anfang der 1970er Jahre entspricht. Die Struktur hat sich jedoch zugunsten von Megaplex-Kinos und auf Kosten von Kleinkinos mit ein und zwei Sälen verändert, von denen es 2012 nur noch 16 gab. Demgegenüber stehen sieben Kinos mit drei bis fünf Sälen und vier Kinos mit mehr als sechs Sälen. Digitalprojektion wurde 2012 in 19 Kinos eingeführt.

Im Vergleich mit dem Bruttoinlandsprodukt der Europäischen Union ausgedrückt in Kaufkraftstandards erreichte Oberösterreich einen Index von 132 (EU-28: 100 Österreich: 129).

Oberösterreich ist eines der Zentren der österreichischen Industrie. Mit Direktexporten im Wert von 18,9 Milliarden Euro (2006) erwirtschaftet das Bundesland 26,5 Prozent der österreichischen Exporte. Etwa 70 Prozent der Exporte gehen in den EURO-Raum. 68.626 Gewerbeunternehmen erwirtschaften mit 576.203 Beschäftigten ein Bruttoregionalprodukt von 32,6 Milliarden Euro. Die höchsten Beschäftigtenzahlen haben die Branchen (nach ÖNACE-Klassifizierung) Sachgütererzeugung mit 28 Prozent, Handel mit 18 % und unternehmensbezogene Dienstleistungen mit 11 %.

Wichtige Branchen sind:

Im Gebiet des Hausrucks wurde bis 1995 Braunkohle gefördert (etwa in Ampflwang, Thomasroith (Ottnang) und Wolfsegg).

Im Alpenvorland werden geringe Mengen Erdöl gefördert, (so in Lohnsburg am Kobernaußerwald, Voitsdorf und Sattledt). Erdgas wird z. B. bei Puchkirchen, Pfaffstätt, und Atzbach gefördert.

Historisch bedeutend ist das Salzbergwerk bei Hallstatt. Weiters wird auch in Bad Ischl das Steinsalz zur Sole gelöst, durch die Soleleitung in die Saline in Ebensee transportiert, um daraus Salz zugewinnen.

Bei Tragwein wird Kaolin im Tage- und Grubenbau gefördert. In St. Georgen an der Gusen wird Quarz abgebaut und veredelt. Gipsabbau wird bei Spital am Pyhrn betrieben.

Oberösterreich ist durch internationale Verkehrswege gut erschlossen. Wichtige Straßenverbindungen sind die West Autobahn A 1, Mühlkreis Autobahn A 7, Innkreis Autobahn A 8 und Pyhrn Autobahn A 9. Die Verlängerung der Mühlkreisautobahn als Mühlviertler Schnellstraße S 10 nach Tschechien und die Linzer Autobahn A 26 sind wichtige Straßenbauprojekte der Zukunft.

Mit der Westbahn führt eine der wichtigsten österreichische Eisenbahnstrecken durch Oberösterreich. Weitere wichtige Bahnstrecken sind die Summerauer Bahn und die Pyhrnbahn. Bedeutende Bahnhöfe befinden sich in Linz, Wels und Attnang-Puchheim.

Weitere Verkehrsknoten sind zwei große Donauhäfen in Linz und Enns sowie der Flughafen Linz.

Auflagenstärkste Tageszeitung Oberösterreichs sind die Oberösterreichischen Nachrichten. Die Oberösterreichische Rundschau publiziert wöchentlich drei Ausgaben (Regionalausgabe am Donnerstag, Sonntagsrundschau und Korrekt-Kleinanzeiger). Weiters erscheint wöchentlich der Wochenblick sowie die Tips.

Der Österreichische Rundfunk (ORF) ist mit einem Funkhaus in Linz vertreten. Größter privater Fernsehsender ist LT1. Er sendet zusammen mit HT1 europaweit unverschlüsselt über den Fernsehsatelliten Astra 1H. Seit Juni 2010 sendet der nichtkommerzielle Fernsehsender DORF TV in weiten Teilen des Bundeslandes.
Neben den staatlichen ORF-Radioprogrammen kämpfen verschiedene Privatsender um den Radiomarkt: Life Radio, Welle 1, Radio Arabella und KroneHit. Als nichtkommerzielle Privatradios konnten sich Radio FRO im Großraum Linz, Freies Radio Freistadt und Freies Radio Salzkammergut etablieren.

Die Energieversorgung Oberösterreichs wird hauptsächlich durch die landeseigene Energie AG gewährleistet. Diese betreibt 34 Wasser- und zwei thermische Kraftwerke in Riedersbach und Timelkam sowie Photovoltaikanlagen in Eberstalzell und am Loser. Sechs der Wasserkraftwerke befinden sich in Salzburg, ein Wasser- und das Solarkraftwerk in der Steiermark. Die Verbund Hydro Power AG betreibt die fünf Donaukraftwerke in Oberösterreich. Die Energie-AG-Tochter AVE betreibt zwei Müllverbrennungsanlagen (Wels und Lenzing). Das Linzer Kommunalunternehmen Linz AG besitzt drei Fernheizkraftwerke, die auch zur Stromerzeugung genutzt werden. Daneben betreibt die Linz AG auch vier Wasserkraftwerke.

Oberösterreich verfügt über vier Hochschulen, die sich alle in Linz befinden. Mit circa 19.300 Studierenden ist die staatliche Universität Linz "(Johannes Kepler Universität Linz)" die größte Bildungseinrichtung. Sie bietet akademische Ausbildung im Bereich Sozial- und Wirtschaftswissenschaften, Rechtswissenschaften sowie Technik und Naturwissenschaften an. Zweite staatliche Universität ist die Universität für künstlerische und industrielle Gestaltung Linz "(Kunstuniversität)". Neben diesen befinden sich noch die Katholisch-Theologische Privatuniversität Linz "(KTU)" und die Anton Bruckner Privatuniversität im Land.

Die FH Oberösterreich bietet an vier Standorten (Hagenberg, Linz, Steyr und Wels) Bildung in unterschiedlichen Bereichen (Technik, Wirtschaft und Soziales) an. Derzeit nehmen ungefähr 5900 Studenten an diesen Bildungseinrichtungen die angebotenen Ausbildungsmöglichkeiten wahr.

Seit 1. Oktober 2007 ebenfalls als Hochschule geführt werden die vormaligen Pädagogischen Akademien. Beide derartige Einrichtungen in Oberösterreich befinden sich in Linz: die staatliche Pädagogische Hochschule Oberösterreich und die katholische Pädagogische Hochschule der Diözese Linz.

Aus dem alten Namen Oberösterreichs – Land ob der Enns – hat sich im Ungarischen die Beginnphrase von Märchen entwickelt. So wie im Deutschen die meisten Märchen mit „Es war einmal …“ beginnen, steht am Anfang der ungarischen Märchen meist „Messzi, messzi földön, még az "operencián" is túl“ (in einem fernen, fernen Land, jenseits von "Ob der Enns"). Auch kommt in ungarischen Märchen immer wieder das „operenciai tenger“ (das "ob-der-Enns'ische Meer") vor, damit sind die Seen im Salzkammergut gemeint. 





</doc>
<doc id="11047" url="https://de.wikipedia.org/wiki?curid=11047" title="Ben Nicholson">
Ben Nicholson

Benjamin Lauder „Ben“ Nicholson OM (* 10. April 1894 in Denham in der Grafschaft Buckinghamshire; † 6. Februar 1982 in London) war ein britischer Maler und Objektkünstler. Nicholson war der Sohn des Malers William Nicholson und der Bruder der Malerin Nancy Nicholson. Von 1938 bis 1951 war er mit der Bildhauerin Barbara Hepworth verheiratet.

Nicholson besuchte die Gresham’s School in Norfolk und 1911 für ein Semester die Slade School of Art in London. Zwischen 1911 und 1918 hielt er sich in Tours, Mailand, auf Madeira, in London, Nord-Wales und in Pasadena auf.

1920 heiratete Nicholson die Malerin Winifred Roberts, mit der er drei Kinder hatte. 1925 schloss er sich der Gruppe „7 and 5“ in London an, die er jedoch 1936 wieder verließ. Mitglied der Gruppe „Unit One“ wurde er 1933. Im Jahr 1938 heiratete er in zweiter Ehe Barbara Hepworth, mit der er ebenfalls drei Kinder hatte, es waren Drillinge, geboren 1934. Von 1932 bis 1939 lebte er in London und hatte Kontakte mit Piet Mondrian und Naum Gabo. 1939 bezog das Paar seinen Wohnsitz in St Ives in Cornwall, wo er bis 1955 lebte und wo sich um ihn und Barbara Hepworth ein Zentrum abstrakter Kunst, die sogenannte „Penwith Society of Artists“, bildete. Die Scheidung von Barbara Hepworth erfolgte 1951.

Eine Retrospektive seiner Arbeiten wurde 1955 in der Tate Gallery in London ausgestellt. Im Jahr 1958 ließ er sich mit seiner dritten Frau, die er 1957 geheiratet hatte, der Schweizer Fotografin Felicitas Vogler in Castagnola am Lago Maggiore nieder. 1971 trennte er sich von ihr und zog allein nach Cambridge. Die Scheidung erfolgte im Jahr 1977. Sein letzter Wohnort ab 1972 war Hampstead in London, wo er bis zu seinem Tod im Atelier arbeitete.

In seinem Schaffen wurde er von Pablo Picasso und dem Niederländer Piet Mondrian beeinflusst. Von letzterem stammt der stilistische Begriff Neoplastizismus, der auch für Nicholsons Werke bestimmend wurde, nämlich die Betonung der Senkrechten und Waagrechten sowie von Gelb, Rot und Blau als Primärfarben und der farblosen Stufen Weiß, Grau und Schwarz. Bis 1937 war er Mitglied der Gruppe „Abstraction-Création“.

Einige seiner Werke sind in der Tate Gallery, St Ives und Kettle’s Yard Art Gallery in Cambridge ausgestellt.

Nicholson schuf Landschaften und Stillleben sowie Reliefs.





</doc>
<doc id="11050" url="https://de.wikipedia.org/wiki?curid=11050" title="Joseph McCarthy">
Joseph McCarthy

Joseph Raymond „Joe“ McCarthy (* 14. November 1908 in Grand Chute, Wisconsin; † 2. Mai 1957 in Bethesda, Maryland) war ein US-amerikanischer Politiker. Er gehörte der Republikanischen Partei an und wurde bekannt wegen seiner Kampagne gegen eine angebliche Unterwanderung des Regierungsapparates der Vereinigten Staaten durch Kommunisten. Nach ihm benannt ist die so genannte McCarthy-Ära der frühen 1950er Jahre, in der antikommunistische Verschwörungstheorien und Denunziationen das politische Klima in den USA bestimmten.

McCarthy wurde als fünftes von sieben Kindern streng katholischer Farmer geboren. Um zum Unterhalt der Familie beizutragen, brach er 1922 die Schule vorzeitig ab und betrieb unter anderem eine kleine Geflügelzucht und einen Lebensmittelladen. Er holte seinen High-School-Abschluss 1928 in nur einem Jahr nach und nahm ein Jurastudium an der Marquette-Universität auf. Ab 1935 war er als Rechtsanwalt tätig, 1939 wurde er zum Bezirksrichter von Wisconsin gewählt. 1942 meldete er sich zum Einsatz im Zweiten Weltkrieg und wurde als Nachrichtenoffizier der US-Luftstreitkräfte eingesetzt. In der Heimat stilisierte er sich durch geschickte Publicity und Manipulation zum kampferfahrenen Heckschützen „Tail-Gunner Joe“, inklusive angeblicher Verwundung im Gefecht mit (gefälschter) späterer Belobigung; tatsächlich hatte er bei der Truppe einen Partyunfall. An ernsthaften Kampfeinsätzen war er aufgrund seiner Dienststellung kaum mit persönlichem Risiko beteiligt; „Bordschütze“ beim Marine Corps auf Bougainville war er nur im bereits militärisch gesicherten Hinterland, wo er Zivilschäden anrichtete.

Nach seiner Rückkehr nutzte McCarthy sein fabriziertes Image als Kriegsheld bei den Vorwahlen der Republikaner von Wisconsin: „Wisconsin needs a tail-gunner“ war der Claim seiner Kampagne. Sein Gegenkandidat, Senator Robert M. La Follette junior, der altersbedingt nicht am Weltkrieg hatte teilnehmen können, konnte dem wenig entgegensetzen. Dass La Follette von 1934 bis 1944 die Wisconsin Progressive Party vertreten hatte und erst kurz zuvor zu den Republikanern zurückgekehrt war, kostete ihn weitere Sympathien beim Parteivolk. McCarthy, der zudem verbreitete, La Follette sei ein Kriegsgewinnler gewesen, gewann knapp die Vorwahlen. Im November 1946 setzte er sich dann mit 61,2 % der Stimmen gegen seinen demokratischen Konkurrenten Howard McMurray durch und zog in den Senat ein.

Den 80. Kongress der Vereinigten Staaten (1947–1948) dominierten die Republikaner, die erstmals seit 1933 die Mehrheit in beiden Kammern stellten. Als jüngster Senator verstand es McCarthy schnell, sowohl durch zahlreiche Gesetzesinitiativen als auch durch beste Kontakte zur Presse (darunter der Kolumnist Jack Anderson, später sein erbitterter Gegner) und zu den Spitzenleuten der Republikanischen Partei sein Image als Mann mit großer politischer Zukunft auszubauen. Die konservative "Saturday Evening Post" beschrieb ihn im August 1947 als „bemerkenswerten Neuling im Senat“.

Politisch harmonierte McCarthy in den meisten Fragen mit dem konservativen Flügel der Republikaner um deren Senatsführer Robert Taft. Die Themen, denen er sich besonders widmete, wurden von den Sorgen der unmittelbaren Nachkriegszeit bestimmt: Er setzte sich für ein schnelles Ende der Rationierung von Zucker ein und engagierte sich für das Überwinden der prekären Wohnungssituation in weiten Teilen der USA. Politische Gegner warfen ihm später vor, in beiden Feldern als Lobbyist agiert zu haben, in der Zuckerfrage für Pepsi-Cola und in der Wohnungsfrage für ein Bauunternehmen aus Ohio.

Frühzeitig trat McCarthy auch mit scharf antikommunistischen Äußerungen hervor. Er sprach sich schon 1947 für ein Verbot der Kommunistischen Partei der USA (KPUSA) aus und erklärte im Juli desselben Jahres: 

Mit seiner ruppigen und bedenkenlosen Art hatte sich McCarthy in Washington wenig Freunde gemacht. Als die Legislaturperiode sich 1950 ihrem Ende zuneigte, fehlte ihm noch ein Thema, das ihm Popularität und die Wiederwahl sichern konnte.

McCarthy begann seine Kampagne gegen die angebliche Unterwanderung des Regierungsapparats durch Kommunisten Anfang 1950. Vor dem "Republican Women’s Club" in Wheeling, West Virginia, erklärte er am 9. Februar 1950, er sei im Besitz einer Liste mit den Namen von 205 Personen, von denen der demokratische Außenminister Dean Acheson wisse, dass es sich bei ihnen um „Mitglieder der Kommunistischen Partei“ handle und die „dennoch weiterhin im Außenministerium arbeiten und dessen Politik mitbestimmen“ dürften.

McCarthy war nicht imstande, seine Behauptung zu untermauern und konkrete Namen zu nennen. In den folgenden Wochen variierte er seine Angaben zur Zahl der ihm angeblich namentlich bekannten Kommunisten im Staatsdienst erheblich. Tatsächlich existierte eine solche Namensliste überhaupt nicht. Dennoch stießen seine Äußerungen auf großes Echo in Medien und Gesellschaft. Im aufgeheizten Klima der Frühphase des Kalten Krieges wirkte die indirekte Beschuldigung, der amtierende amerikanische Außenminister decke eine kommunistische Unterwanderung seines Ministeriums, sensationell. Nicht wenige Menschen waren geneigt, McCarthy Glauben zu schenken, weil sie sich die jüngsten Erfolge des lange Zeit in wissenschaftlichen und militärischen Belangen als hoffnungslos rückständig angesehenen kommunistischen Lagers nur durch Verrat und verdeckte Kollaboration von Amerikanern erklären konnten.

Erst im September 1949 war es der Sowjetunion gelungen, erfolgreich eine eigene Atombombe zu testen; Schlagzeilen über das Geständnis des deutsch-britischen Atomspions Klaus Fuchs bestätigten Anfang Februar 1950 den in der amerikanischen Öffentlichkeit grassierenden Verdacht, diese Entwicklung sei durch Geheimnisverrat aus dem Westen ermöglicht worden. Außerdem hatte Mao Zedong am 1. Oktober 1949 die Volksrepublik China proklamiert und damit den für die meisten Amerikaner gleichsam überraschenden wie schockierenden Sieg der Kommunisten im chinesischen Bürgerkrieg markiert. In Ostasien standen zudem direkte militärische Auseinandersetzungen der beiden politischen Systeme in Form des Koreakrieges unmittelbar bevor.

Die "Wheeling-Rede" und einige öffentliche Auftritte, die ihr folgten, machten den zuvor weitgehend unbekannten Senator aus Wisconsin innerhalb weniger Tage zu einem begehrten Interview-Partner. Vielen Zeitungen waren McCarthys Anschuldigungen Schlagzeilen wert. Im Rahmen einer seiner regelmäßigen Pressekonferenzen sah sich Präsident Harry S. Truman bereits am 16. Februar dazu gedrängt, auf McCarthys mehrfache Stellungnahmen über Kommunisten im Außenministerium zu reagieren, indem er erklärte, diese enthielten „nicht ein wahres Wort“.

Von Journalisten dazu aufgefordert, seine Behauptungen über Kommunisten im Außenministerium zu belegen, erklärte McCarthy auf einer eigenen Pressekonferenz, er werde dem Senat auf Wunsch „detaillierte Informationen“ hierüber präsentieren. Noch im Februar 1950 bildete der Ausschuss für auswärtige Angelegenheiten des Senats daher einen betreffenden Unterausschuss, das "Subcommittee on the Investigation of Loyalty of State Department Employees" („Unterausschuss zur Überprüfung der Staatstreue von Angestellten des Außenministeriums“), besser bekannt unter der Bezeichnung "Tydings Committee" "(Tydings-Ausschuss)" – benannt nach seinem Vorsitzenden, dem demokratischen Senator Millard Tydings.

Dieser Unterausschuss verlangte von McCarthy, die Namen der angeblichen Kommunisten im State Department zu nennen, doch der Senator konnte nur wenige verdächtige Beamte namhaft machen. Nach fünf Monaten intensiver Untersuchungen kam das Tydings-Komitee in einem am 17. Juli 1950 veröffentlichten Bericht zu dem Ergebnis, dass die von McCarthy genannten Personen weder Kommunisten seien noch mit dem Kommunismus sympathisierten. McCarthys Beschuldigungen seien „Betrug und Schwindel, die am US-Senat und dem amerikanischen Volk verübt worden“ seien. Der Bericht war von den drei demokratischen Ausschussmitgliedern unterzeichnet worden, nicht jedoch von den zwei Republikanern im Komitee.

Das Tydings Committee hatte McCarthy die Publizität gegeben, die er sich gewünscht hatte. Im November 1950 wurde er mit 54,2 % der Stimmen wiedergewählt, sein demokratischer Gegenkandidat Thomas E. Fairchild erhielt 45,6 %. McCarthy setzte seine Kampagne mit unverminderter Heftigkeit fort. So beschuldigte er etwa den bekannten liberalen Kolumnisten Drew Pearson, ein „von Moskau gesteuerter Rufmörder“ zu sein, der den ehemaligen Verteidigungsminister James V. Forrestal „zu Tode gehetzt“ habe. Forrestal hatte sich, an einer schweren Depression erkrankt, im Mai 1949 aus dem sechzehnten Stock des Marinekrankenhauses in Bethesda gestürzt. General George C. Marshall, der demokratische Ex-Außenminister, wurde von McCarthy im Juni 1951 verdächtigt, mit den Kommunisten im Bunde zu stehen, denn 1947 hatte der spätere Friedensnobelpreisträger empfohlen, die Militärhilfe für die nationalchinesische Bewegung Chiang Kai-sheks einzustellen. Die seines Erachtens kritische Lage der Nation könne nur mit bösen Absichten erklärt werden, die in der Truman-Regierung insgeheim verfolgt würden:

Im Wahlkampf 1952 konzentrierte McCarthy seine Angriffe auf den demokratischen Präsidentschaftskandidaten Adlai Stevenson, aus dessen diversen Beziehungen zu ehemaligen Linken er eine „guilt by association“ konstruierte: Weil sich seine Mitarbeiter und Bekannten nicht eindeutig gegen den Kommunismus positioniert hätten, mache sich auch Stevenson einer Unterstützung des Kommunismus schuldig. Im Januar 1954 diffamierte McCarthy in einer Pressekonferenz schließlich die gesamte Regierungszeit der demokratischen Präsidenten Roosevelt und Truman seit 1933 als „zwanzig Jahre Hochverrat“. Vor Verleumdungsklagen war McCarthy bei all diesen ehrverletzenden Behauptungen stets geschützt, weil er als Senator politische Immunität genoss.

Im November 1952 gewann der Kandidat der Republikanischen Partei Dwight D. Eisenhower die Präsidentschaftswahlen, die Republikaner erreichten eine knappe Mehrheit in beiden Kammern des Kongresses. Umstritten ist, ob McCarthys Kampagne ihnen dabei geschadet oder genutzt hat. Kandidaten, die eng mit ihm verbunden waren, schnitten schlechter ab als solche, die größere Distanz gewahrt hatten. Auch die Wahlergebnisse aus Wisconsin rechtfertigen Zweifel an McCarthys damaliger Popularität: Im Heimatstaat des Senators wurde der moderate Eisenhower mit 61 % der Stimmen gewählt, während McCarthy selbst nur 54 % erzielte – sieben Prozentpunkte weniger als sechs Jahre zuvor.

Dennoch herrschte in der Republikanischen Partei die Überzeugung vor, der Wahlerfolg sei zumindest zum Teil dem Thema Antikommunismus im Allgemeinen und McCarthy im Besonderen geschuldet. Als Dankesgeste wurde McCarthy 1953 von der Parteiführung der begehrte Posten des Vorsitzenden eines Senatsausschusses offeriert – allerdings nur für das eher unbedeutende "Government Operations Committee" (GOC). Ein Ziel war dabei bereits, den als mögliche Kompromittierung empfundenen Parteifreund von weiteren politischen Eskapaden abzuhalten und in parlamentarischer Routinearbeit dort einzubinden, „wo er keinen Schaden anrichten kann“, wie es der republikanische Mehrheitsführer Taft ausdrückte. McCarthy trat öffentlichen Spekulationen, er werde sich in Zukunft weniger mit der kommunistischen Bedrohung der USA auseinandersetzen, sofort entschieden entgegen.

Aufgabe des Ausschusses war eine allgemeine Kontrolle staatlicher Behörden und Institutionen. McCarthys Interesse lag vor allem darin, dessen 1952 eingerichteten "Ständigen Unterausschuss für Untersuchungen" "(Permanent Subcommittee on Investigations)" zu einem Instrument zur Erforschung der von ihm behaupteten kommunistischen Unterwanderung der amerikanischen Gesellschaft zu machen und dabei öffentlichkeitswirksame Gesinnungsprüfungen innerhalb des Regierungsapparates durchzuführen. McCarthys Ausschuss rivalisierte dabei mit dem "Ausschuss für unamerikanische Umtriebe" des Repräsentantenhauses (HUAC) und dem Justizministerium darum, wer die sensationellsten Schlagzeilen bekam. Im HUAC hatte Eisenhowers Vizepräsident Richard Nixon ab 1948 eine Rolle gespielt, wie McCarthy sie jetzt für sich selbst im GOC ausmalte. In der Öffentlichkeit wurden beide Ausschüsse daher später oft verwechselt.

McCarthy nutzte die neuen Vorrechte eines Ausschussvorsitzenden dazu aus, den Kurs des Unterausschusses fast im Alleingang zu bestimmen. So überraschte er selbst dessen republikanische Mitglieder oft dadurch, dass er ihnen erst am Tage einer Anhörung den Gegenstand der Beratungen offenbarte. Mit einigem Geschick formte er außerdem in kürzester Zeit einen schlagkräftigen Stab, indem er junge, ambitionierte Juristen wie Roy Cohn und Robert F. Kennedy engagierte. Innerhalb weniger Wochen entwickelte McCarthy ein so breit gefächertes Interesse an der Kontrolle von Regierung und Behörden, dass die spektakulären und fast täglich abgehaltenen "Anhörungen" seines Unterausschusses im ersten Halbjahr 1953 sein Image bis heute entscheidend prägen. Cohn wurde sehr bald zu McCarthys wichtigstem Mitarbeiter, der großen Einfluss auf den Gang dieser Untersuchungen gewann.

Insgesamt wurden während McCarthys Amtszeit als Ausschussvorsitzender 653 Zeugen vorgeladen, deren Bürgerrechte systematisch missachtet wurden. Die Anhörungen nahmen den Charakter von Gerichtsverhandlungen an, mit dem Unterschied, dass an ihrem Ende nicht eine rechtskräftige Verurteilung oder ein Freispruch stand, sondern häufig der Ruin des Rufes und des öffentlichen Ansehens, wogegen den Betroffenen keine Rechtsmittel zur Verfügung standen. Beriefen sie sich auf ihr im 5. Verfassungszusatz verbrieftes Recht, die Aussage zu verweigern, gab McCarthy ihre Namen der Öffentlichkeit preis und beschimpfte die Personen als "„5. Zusatz-Kommunisten“". In seiner Rolle als Ankläger und Richter in einer Person stilisierte er sich selbst zum Bewahrer US-amerikanischer Werte, zum Beschützer vor der „Roten Gefahr“. Dabei nahm er auch keine Rücksicht auf die Tatsache, dass die Regierung nun von seiner eigenen Partei gestellt wurde. Die Protokolle der in nichtöffentlichen Sitzungen („Executive Sessions“) abgehaltenen Anhörungen wurden erst im Jahr 2003 für die Öffentlichkeit freigegeben.

In der ersten Jahreshälfte 1953 profilierte sich McCarthy vor allem mit Anhörungen über die "International Information Administration" (IIA). Die Behörde mit halbautonomem Status im State Department beschäftigte zu Beginn des Jahres 1953 weltweit rund 10.000 Mitarbeiter und verfügte über einen Etat von 100 Millionen Dollar. Neben dem Radiodienst der "Voice of America" (VoA), der Sendungen in 40 Sprachen für potentiell 300 Millionen Hörer produzierte, gehörte in ihren Verantwortungsbereich vor allem der "United States Information Service" (USIS, auch "Foreign Information Service", FIS genannt). In dessen annähernd 200 "Information Centers" im Ausland waren die unterschiedlichsten Kultur-, Informations- und Weiterbildungsprogramme lokalisiert. Stark frequentiert wurden dabei insbesondere die Bibliotheken dieser auch als "Amerika-Häuser" bezeichneten Einrichtungen. Allein 41 "Information Centers" waren mit Blick auf die Reeducation in der Bundesrepublik Deutschland und in West-Berlin eingerichtet worden. Seit 1945 war es fast schon Tradition unter Republikanern im Kongress geworden, das Personal von VoA und USIS als kommunistisch unterwandert und das ganze Programm als eine einzige gigantische Misswirtschaft zu brandmarken – ein Leitmotiv, das nun von McCarthy aufgegriffen wurde.

McCarthy erklärte am 12. Februar 1953, sein Komitee werde sich mit sofortiger Wirkung mit „Missmanagement, Subversion und Vetternwirtschaft“ innerhalb der VoA beschäftigen. Einer der Vorwürfe lautete, dass ihr Programm zu stark links orientiert sei. Belegt wurde dies mit einer IIA-Instruktion, wonach Werke des kommunistischen Schriftstellers Howard Fast "(Spartacus)" als Demonstrationsmaterial benutzt werden konnten. McCarthy verschwieg, dass die Erlaubnis, Werke von Fast zu verwenden, im Kontext einer tatsächlich "gegen" kommunistische Autoren und deren Werke zielenden IIA-Richtlinie lediglich als Ausnahmefall aufgeführt worden war.

In den sechs Wochen andauernden, zum Teil vom Fernsehen übertragenen Anhörungen dehnte er seine Untersuchung gleich auf das gesamte "Foreign Information Program" des USIS aus. Er schuf dabei mit seinem Komitee ein Forum, in dem selbst extreme und ehrverletzende Beschuldigungen aufmerksame Zuhörer fanden. Statt sich, wie von der Parteiführung erhofft und von McCarthy selbst zunächst zugesagt, auf die Untersuchung der Verschwendung öffentlicher Gelder zu konzentrieren, rückte McCarthy die angebliche kommunistische Unterwanderung der amerikanischen Gesellschaft ins Zentrum der Anhörungen. Handfeste Enthüllungen gab es dabei kaum: sofern die von Zeugen erhobenen Vorwürfe juristischen Belang besaßen, waren die eigentlich verantwortlichen Strafverfolgungsbehörden ihnen längst, teilweise schon vor Jahren, nachgegangen. Dennoch wurden eine ganze Reihe von VoA- und sonstigen IIA-Mitarbeitern infolge der Anhörungen versetzt oder gekündigt. Ein VoA-Angestellter beging nach seiner Zeugenaussage Selbstmord.

Da während der VoA-Untersuchung ein Zeuge ausgesagt hatte, dass die Werke von nicht weniger als 75 kommunistischen Autoren zum Bestand von Bibliotheken des USIS gehörten, geriet nun auch die Arbeit der "Information Centers" im Ausland in McCarthys Schussfeld. Daraufhin gab Außenminister John Foster Dulles am 17. März 1953 die Anweisung aus: "„Werke kommunistischer Autoren sind aus allen öffentlichen Bibliotheken und Information Centers des USIS zu entfernen“", wofür McCarthy ihn öffentlich lobte. Auch bei der Definition dessen, was ein "„Kommunist“" sei, richtete man sich im Außenministerium nach den Richtlinien des Senators: Als Kommunist, dessen Bücher dem Bann unterliegen sollten, galt unter anderem auch jeder, der mit Hinweis auf den Fünften Verfassungszusatz die Aussage vor einem Kongressausschuss verweigert hatte.

Nach weiteren Untersuchungen in der Sache schickte McCarthy seine Mitarbeiter Roy Cohn und G. David Schine auf eine Inspektionsreise nach Europa, wo sie sich davon überzeugen sollten, dass die neuen, strikten Richtlinien des State Department inzwischen umgesetzt worden waren. Weil einer der zwangsgeladenen Zeugen McCarthy bei einer Ausschusssitzung vorgehalten hatte, er plane eine „Bücherverbrennung“ im Stile der Nazis, wurde diese Reise in verschiedenen Medien bald auch als „Bücherverbrennungsmission“ („book-burning mission“) bezeichnet. Sie führte zwischen dem 4. und dem 18. April 1953 in zehn europäische Städte (Paris, Bonn, Berlin, Frankfurt/Main, München, Wien, Belgrad, Athen, Rom, London) und entwickelte sich sehr schnell zu einem umfassenden PR-Desaster, wobei McCarthys Mitarbeiter insbesondere in europäischen Zeitungen und Zeitschriften häufig als jugendliche Rowdys dargestellt wurden. Cohn schrieb später in seinen Memoiren:

Weil er in der Bibliothek des Frankfurter Amerika-Hauses auf die Kriminalromane "Der Malteser Falke" und "Der dünne Mann" von Dashiell Hammett (der sich zuvor vor McCarthys Unterausschuss auf den Fünften Verfassungszusatz berufen hatte) gestoßen war, konnte Cohn triumphierend erklären, die Dulles-Richtlinien seien offenbar noch nicht überall befolgt worden.

Wenig später, im Herbst 1953, begann McCarthys Ausschuss, nach Kommunisten in den Streitkräften zu suchen. Zum Konflikt kam es beim Fall eines New Yorker Zahnarztes, der zum Major befördert und ehrenhaft aus der Armee entlassen worden war, obwohl er sich geweigert hatte, Angaben zu einer etwaigen Mitgliedschaft in subversiven Organisationen zu machen. Als der zuständige Brigadegeneral vor dem Ausschuss ausweichend antwortete, schrie McCarthy ihn an, er habe den "„Verstand eines fünfjährigen Kindes“" und sei "„ungeeignet, die Uniform eines Generals zu tragen“". Diese Beschimpfungen führten dazu, dass der United States Secretary of the Army, Robert T. Stevens, seinen Offizieren verbot, vor McCarthys Ausschuss zu erscheinen. Allerdings konnte er diese Anweisung nicht aufrechterhalten.

Stattdessen begann die Armee nun, wie McCarthy selber glaubte, die Demütigung eines ihrer Mitarbeiter mit gleicher Münze heimzuzahlen. Anfang 1954 beschuldigte sie McCarthy und Cohn, unzulässigen Druck auszuüben, um die militärische Karriere ihres ehemaligen Mitarbeiters David Schine zu fördern. Im März 1954 erschien die Zeitschrift TIME mit Cohn und Schine auf der Titelseite und der höhnischen Unterzeile: "„The army got its orders“ – „Die Armee hat ihre Befehle“". McCarthy antwortete sofort mit einer Verschwörungstheorie: Er sei überzeugt, dass die Armee seinen ehemaligen Mitarbeiter als "„Geisel“" festhalte, um die Enttarnung weiterer Kommunisten in ihren Reihen durch sein Komitee zu verhindern.

Um die Sache aufzuklären, wurde ein Unterausschuss unter Vorsitz des republikanischen Senators Karl Mundt einberufen, der seine Arbeit am 22. März 1954 aufnahm. Nachdem 32 Zeugen gehört worden waren, darunter auch McCarthy und Cohn, kam der Ausschuss zu der Schlussfolgerung, dass zwar nicht der Senator, wohl aber sein engster Mitarbeiter Cohn "„unangebracht nachdrückliche oder aggressive Anstrengungen“" unternommen hätten, um Schines Karriere zu fördern.

Wichtiger noch als diese Teilniederlage war ein Wortwechsel am 9. Juni zwischen McCarthy und dem Anwalt Joseph Welch, der die Armee vertrat. Der Senator konterte Welchs Anwürfe mit dem Gegenvorwurf, in dessen Bostoner Kanzlei arbeite ein junger Mann, der Mitglied einer angeblich der KPUSA nahestehenden Juristenorganisation sei. Damit verstieß er gegen die Absprachen, die vor der Anhörung gemacht worden waren, weshalb ihm Welch, spürbar empört von der Beiläufigkeit, mit der der Senator die Karriere eines Unbeteiligten ruinierte, das Wort abschnitt:

„Wir wollen diesen Burschen nicht weiter ermorden. (…) Sie haben schon genug getan. Haben Sie denn überhaupt keinen Sinn für Anstand, Sir? Ist bei Ihnen gar kein Sinn für Anstand mehr übrig?“
Diese Kritik an seiner persönlichen Integrität, die landesweit live im Fernsehen übertragen wurde, brachte McCarthy erstmals eine schlechte Presse ein. Sein öffentliches Bild eines zwar ruppigen, aber redlichen Kämpfers gegen die Subversion hatte erste Risse bekommen – die öffentliche Meinung begann sich gegen ihn zu richten.

Der nächste Angriff erfolgte am 20. Oktober 1953, als das populäre Politmagazin "See It Now" des Fernsehjournalisten Edward R. Murrow über die Entlassung eines Leutnants der US Air Force berichtete, der beschuldigt wurde, Kommunist zu sein. Noch negativer war die Wirkung der Sendung von "See It Now" vom 9. März 1954, die fast ausschließlich aus Aufnahmen von McCarthy bestand, wie er seine üblichen Anschuldigungen verbreitete, demokratische Politiker des Hochverrats bezichtigte oder Zeugen in seinem Untersuchungsausschuss beschimpfte. McCarthy trat daraufhin selbst in der Sendung auf, doch seine bewährte Methode, Gegner durch Verdächtigungen einzuschüchtern, hatte entgegengesetzte Wirkung. (Die Geschichte dieser ersten Demontage eines Politikers mit den Mitteln des Fernsehjournalismus wird in George Clooneys Film aus dem Jahr 2005 "Good Night, and Good Luck" erzählt.)

1954 verlor McCarthy auch die Unterstützung des Präsidenten. Weil die konspirationistischen Töne des Senators von weiten Teilen der Bevölkerung zunächst positiv aufgenommen worden waren, hatte ihn Eisenhower lange gewähren lassen, obwohl der Präsident McCarthys Weltsicht durchaus nicht teilte. Noch im Wahlkampf hatte er etwa eine Verteidigung General Marshalls gegen McCarthys Verdächtigungen in eines seiner Redemanuskripte eingefügt, die Passage aber auf Bitten seiner Berater wieder gestrichen.

Im Amt rückte Eisenhower mehr und mehr von ihm ab, allerdings ohne ihn je öffentlich zu kritisieren. Ursache für diese wachsende Distanz war, dass McCarthy weiter die Regierung scharf angriff, so, als ob die Republikaner noch immer in der Opposition seien. In seinem Tagebuch monierte Präsident Eisenhower keine drei Wochen nach seinem Amtsantritt, manchen Republikanern sei offenbar nur sehr schwer zu vermitteln, „dass sie jetzt bei dem Team mitspielen, dem das Weiße Haus angehört“.

In der Öffentlichkeit mahnte Eisenhower McCarthy nur sehr vorsichtig zu Mäßigung. So erklärte er bei einer Pressekonferenz, der Kongress solle sein Recht, Untersuchungen von Subversion vorzunehmen, mit „Selbstbeschränkung“ nutzen und vor allem das fundamentale Prinzip der Unschuldsvermutung nicht beschädigen. Bedeutsam war jedoch seine Anordnung, McCarthys Ausschuss keinerlei Unterlagen von Exekutivorganen zur Verfügung zu stellen und solche auch nicht unter Eid aussagen zu lassen, da dabei Fragen der nationalen Sicherheit berührt werden könnten. Dadurch wurden die Untersuchungsmöglichkeiten stark eingeschränkt.

McCarthy dagegen setzte seinen Kurs gegen vermeintliche Kommunisten, ihre Unterstützer und Verharmloser fort. Aus Enttäuschung über die Handelspolitik gegenüber dem kommunistischen China variierte er 1953 seinen Wahlkampfslogan und sprach von „21 Jahren Hochverrat“, verdächtigte sogar Eisenhower selbst als "„verkappten Kommunisten“" und begann damit eine direkte Konfrontation mit der Eisenhower-Regierung.

Kritik an McCarthys Vorgehen hatte es auch in seiner eigenen Partei schon seit Längerem gegeben. Senator Ralph Flanders etwa wird mit den Worten zitiert, McCarthys Antikommunismus weise auffällige Parallelen mit dem von Hitler auf.

Ende Juli 1954 beantragte Flanders im Senat, McCarthy wegen unpassenden Verhaltens zu rügen. Ein Unterausschuss unter Vorsitz von Senator Arthur Vivian Watkins wurde eingerichtet, um die 46 Vorwürfe zu untersuchen, die gegen McCarthy erhoben wurden. Die meisten dieser Punkte erwiesen sich nicht als stichhaltig oder fanden keine Mehrheit unter den Ausschussmitgliedern. Übrig blieben zwei Punkte: McCarthy hatte sich 1952 gegenüber einem Unterausschuss des Senats unkooperativ gezeigt, und zweitens hatte er das "Watkins-Komitee" als „unwissentliche Magd“ der Kommunisten bezeichnet. Aufgrund der Ergebnisse dieser Überprüfung stimmte nach tagelanger Diskussion am 2. Dezember 1954 eine Mehrheit von 67 zu 22 für eine Verurteilung McCarthys. Er blieb zwar bis zu seinem Tod Senator von Wisconsin, doch seine Machtstellung im Senat war gebrochen: Er musste den Vorsitz in seinem Ausschuss an den Demokraten John L. McClellan abgeben, der das Government Operations Committee bis 1977 leiten sollte.

Am 28. April 1957 wurde McCarthy in das Naval Medical Center in Bethesda (Maryland) eingeliefert. Wie bereits bei anderen ähnlichen Gelegenheiten seit Sommer 1956, als der Senator sich in stationäre Behandlung hatte begeben müssen, erklärte seine Frau Jean auch diesmal gegenüber Reportern, Grund für den Krankenhausaufenthalt sei eine alte Knieverletzung. Joseph McCarthy starb am 2. Mai 1957 um 17:02 Uhr Ortszeit. Während im Totenschein als Todesursache „akute Hepatitis, Ursache unbekannt“ angegeben wurde, erklärten seine Ärzte (ohne weitere Details liefern zu wollen), McCarthy habe schon seit Wochen an einer „nichtinfektiösen“ Lebererkrankung gelitten. Medien wie das Nachrichtenmagazin "Time" meldeten daraufhin, der Senator sei an Leberzirrhose gestorben. Heute wird allgemein Alkoholismus als Ursache für McCarthys gesundheitliche Probleme und seinen Tod angenommen.

McCarthy galt als ein leidenschaftlicher Politiker von mitunter heftigem bis rücksichtslosem Wesen. Ihm wird nachgesagt, dass er gegenüber seinen politischen Gegnern ausgesprochen aggressiv werden konnte. Nach einer Voraufführung des Films "Good Night, and Good Luck" urteilte das Publikum einhellig, der Schauspieler, der den Senator spiele, habe stark übertrieben – obwohl der Film ausschließlich Originalaufnahmen von McCarthy zeigte. Mindestens einmal wurde McCarthy auch gewalttätig: in der Garderobe eines Washingtoner Clubs soll er einen Journalisten geohrfeigt haben, einer anderen Quelle zufolge hat er ihm in die Leisten getreten.

Seine Kritiker und auch einige Historiker führen diesen Mangel an Selbstbeherrschung auf schwere Alkoholprobleme zurück. Dagegen behaupteten Unterstützer von McCarthy, dass er zwar durchaus kein Blaukreuzler, aber auch kein Alkoholiker gewesen sei; die Hepatitis sei vielmehr durch eine Infektion hervorgerufen worden.

Noch zu seinen Lebzeiten streuten seine Gegner das Gerücht aus, McCarthy und sein enger Mitarbeiter Roy Cohn seien homosexuell. Als eine Zeitung aus Las Vegas dies am 25. Oktober 1952 kolportierte, verzichtete der Senator auf eine Verleumdungsklage und heiratete seine Sekretärin Jeannie Kerr. Später adoptierte das bis dahin kinderlose Paar einen Säugling aus einem New Yorker Waisenhaus. Auf Roy Cohn trafen die Unterstellungen zu: Er war ein „closet homosexual“, dessen sexuelle Orientierung in seinem Umkreis über Jahrzehnte hinweg ein offenes Geheimnis war; einer breiten Öffentlichkeit wurde dies erst bekannt, als Cohn 1986 an Aids starb.

Joseph McCarthy wurde zur Symbolfigur für das antikommunistisch aufgeheizte Klima des ersten Nachkriegsjahrzehnts. Für die so genannte „Second Red Scare“, die „zweite Rote Panik“ – nach der ersten in den Jahren nach der Oktoberrevolution – werden auch die Begriffe McCarthy-Ära oder "McCarthyismus" verwendet. Dieser tauchte erstmals am 29. März 1950 in einer Karikatur der "Washington Post" auf. Sie zeigt Robert A. Taft und andere führende republikanische Politiker, die einen Elefanten, das Symbol ihrer Partei, zu einem wackligen Turm aus Schmutzkübeln lotsen, deren oberster die Aufschrift „McCarthyism“ trägt. Die Überschrift gibt die ängstlichen Worte des Elefanten wieder: „Meint ihr wirklich, ich soll da oben drauf stehen?“ McCarthy übernahm diesen ursprünglich kritisch gemeinten Begriff und wendete ihn ins Positive: „McCarthyism is Americanism with its sleeves rolled“.

Die Politik des Senators war typisch für die Verfolgung angeblicher und tatsächlicher Kommunisten in den USA der frühen fünfziger Jahre. In dieser frühen Ära des Kalten Krieges dominierte eine verschwörungstheoretische, manichäische Sicht auf den Kommunismus, die im Ostblock das schlechthin Böse sah, das die Werte und den Lebensstil Amerikas zu vernichten trachtete. McCarthy kann insofern als gutes Beispiel dafür gelten, was Richard Hofstadter 1964 in einem viel zitierten Essay als „den paranoiden Stil in der amerikanischen Politik“ bezeichnete. Aus dieser Perspektive heraus schienen ihm rechtsstaatliche Prinzipien als verzichtbarer Luxus. 

Gleichwohl war er nicht der Urheber dieser Verfolgung. Öffentliche Äußerungen über eine angebliche Unterwanderung des Außenministeriums gab es bereits 1947. Auch setzten die konkreten Verfolgungen bereits Jahre vor McCarthys politischem Kreuzzug ein: Spätestens als im November 1947 zehn Filmschaffende zu Haftstrafen verurteilt wurden, weil sie vor dem Ausschuss für unamerikanische Umtriebe des Repräsentantenhauses auf ihren verfassungsgemäßen Rechten der Meinungsfreiheit und der Aussageverweigerung bestanden, war klar, dass der Wind sich gesellschaftspolitisch gedreht hatte: Galt in den langen Jahren der Großen Depression der Kapitalismus bei vielen Intellektuellen noch als offenkundig gescheitert und der Sozialismus als humanere Alternative, stand seit Beginn des Kalten Krieges grundsätzlich alles Linke bis in den linksliberalen Flügel der Demokraten unter dem Generalverdacht der Subversion und der Spionage. Die Verfolgungen, mit denen sich der amerikanische Staat dagegen zu wehren suchte, etwa die spektakulären Prozesse gegen Alger Hiss oder Ethel und Julius Rosenberg, waren ebenfalls nicht auf die Initiative McCarthys zurückzuführen. Trotzdem ist die antikommunistische Verfolgung, die vom GOC, vom HUAC und nicht zuletzt vom FBI betrieben wurde, untrennbar mit seinem Namen verbunden. Die Historikerin Ellen Schrecker meint daher:

McCarthy war zu seinen Lebzeiten Gegenstand erbitterter Kontroversen und ist es bis heute. 2003 veröffentlichte die konservative Publizistin Ann Coulter ihr Buch "Treason: Liberal Treachery from the Cold War to the War on Terrorism" (zu Deutsch etwa: "Hochverrat: Liberale Treulosigkeit vom Kalten Krieg bis zum Krieg gegen den Terror"). Darin verteidigt sie den von ihr bewunderten McCarthy, da der KGB, wie man spätestens seit der Entzifferung der Telegramme seiner Agenten in den USA durch das VENONA-Projekt wisse, tatsächlich über 350 Spione in den USA gehabt habe, die wie Harry Dexter White bis in höhere Ämter des Finanzministeriums aufgestiegen seien. Daher sei McCarthys Kampagne keine Hexenjagd, sondern berechtigt, notwendig und heilsam gewesen.

Dem wird entgegengehalten, dass Coulter gar kein Interesse an historischer Wahrheit habe; sie benutze die Geschichte nur dazu, die Neokonservativen und die erneute Einschränkung von Bürgerrechten, etwa durch den USA PATRIOT Act, zu rechtfertigen ("Liberaler" ist in der amerikanischen Öffentlichkeit ein seit der Nixonzeit auf wirksame Weise abwertend gebrauchter Begriff für die Vertreter jeder nichtrevolutionären politischen Position links der Mitte). Zwar habe es durchaus sowjetische Spione in den USA gegeben, doch McCarthys Hexenjagd habe sich eben nicht gegen KGB-Agenten, sondern gegen die weit harmlosere KPUSA und deren angebliche Sympathisanten gerichtet. So sei es McCarthy nicht gelungen, auch nur einen einzigen kommunistischen Spion innerhalb der USA zu identifizieren.

Der Politikwissenschaftler Harvey Klehr nimmt eine vermittelnde Position ein: Zwar betont auch er, dass zu Beginn der fünfziger Jahre die Bedrohung durch Kommunisten und Sowjetspione real und größer war, als allgemein angenommen. Gleichzeitig aber unterstreicht er, all dies biete keine Rechtfertigung für McCarthys aufgeregtes und rücksichtsloses Agieren gegenüber dieser Bedrohung:











</doc>
<doc id="11051" url="https://de.wikipedia.org/wiki?curid=11051" title="McCarthy">
McCarthy

McCarthy oder MacCarthy ist der Familienname folgender Personen:


















McCarthy steht außerdem für:
Siehe auch:


</doc>
<doc id="11054" url="https://de.wikipedia.org/wiki?curid=11054" title="Leonidas I.">
Leonidas I.

Leonidas I. (griechisch Λεωνίδας, "der Löwengleiche"; † August 480 v. Chr. bei den Thermopylen) war von 490 bis 480 v. Chr. König von Sparta. Er stammte aus dem Geschlecht der Agiaden.

Leonidas war ein Sohn Anaxandridas’ II. und dessen erster Frau. Sein jüngerer Bruder Kleombrotos wird von manchen Autoren auch als sein Zwillingsbruder bezeichnet. Da sein Halbbruder Kleomenes I. ohne männliche Nachkommen verstarb und Leonidas älterer Bruder Dorieus auch schon gestorben war, fiel die Herrschaft an ihn. Er heiratete Kleomenes’ Tochter Gorgo und hatte mit ihr einen Sohn, Pleistarchos, der sein Nachfolger wurde. Gleichzeitig mit ihm regierte der Eurypontide Leotychidas II. aus dem anderen spartanischen Königshaus.

Historische Bedeutung erlangte Leonidas hauptsächlich durch sein Verhalten als Feldherr des Hellenenbundes in der Schlacht bei den Thermopylen im Jahre 480 v. Chr. In dieser Schlacht blockierte eine griechische Streitmacht (etwa 5.200 Mann, darunter 300 Spartiaten) den Thermopylenpass, um das persische Reichsheer mit einer Gesamtstärke von 50.000 bis 100.000 Mann unter Xerxes I. aufzuhalten. Zunächst konnten sie den Persern, durch die enge Passage im Vorteil, widerstehen – während die Perser herbe Verluste hinnehmen mussten, gab es auf griechischer Seite kaum Ausfälle. Bevor sie von den Gegnern eingekesselt wurden, zog ein Großteil der griechischen Streitmacht ab, und Leonidas blieb, wohl um den Rückzug der restlichen Truppen zu decken, mit ca. 1000 Kämpfern zurück (300 Spartiaten und 700 Thespier) und fiel gemeinsam mit den gesamten spartanischen und thespischen Truppen.

Nach dem Ende der Perserkriege im Jahre 479 v. Chr. stellte die nordgriechische Amphiktyonie eine Tafel zur Erinnerung an den letzten Kampf des Leonidas und seiner 300 Spartiaten auf. 40 Jahre nach der Schlacht soll König Pausanias die Gebeine des Leonidas nach Sparta überführt und in einem Grabmal, dem Theater gegenüber, beigesetzt haben.








</doc>
<doc id="11056" url="https://de.wikipedia.org/wiki?curid=11056" title="König">
König

König (weibl. "Königin") ist die Amtsbezeichnung für den höchsten monarchischen Würdenträger in der Rangfolge eines souveränen Staates. Hierarchisch dem König übergeordnet ist nur der Kaiser wie im Falle der historischen Großreiche. Im Europa des späten Mittelalters und der frühen Neuzeit war der König in der Regel höchster Souverän seines Landes: Oberhaupt der Regierung, oberster Richter und Gesetzgeber in einer Person. Darüber hinaus nahm er in manchen Staaten – beispielsweise in England – die Funktion eines geistlichen Oberhaupts wahr. In modernen Monarchien ist der König meist Staatsoberhaupt mit ausschließlich repräsentativen und zeremoniellen Aufgaben. Die Anrede eines Königs ist „Majestät“.

Die Vorstufen des deutschen Wortes "König" und eng verwandte Wörter sind nicht nur in den älteren deutschen Sprachstufen (ahd. "kuning", mhd. "künic"), sondern auch in den meisten anderen altgermanischen Sprachen bezeugt (altengl. "cyning", altnord. "konungr") und aus einer germanischen Sprache des 2./3. Jahrhunderts ins Finnische entlehnt worden (, "kuningas"). Die zugrundeliegende Form des althochdeutschen "kuning", (protogermanisch) "*kuninga-z", enthält das Suffix "-ing/-ung", das Zugehörigkeit und Abstammung bezeichnet. "*kuninga-z" bedeutete also ursprünglich „der zum "kuni/kunja-" Gehörige“ oder „der von einem "kuni/kunja-" Abstammende“. Die genaue Interpretation dieser Wortableitung ist jedoch umstritten. Eine weithin akzeptierte Deutung sieht das altgermanische Wort "*kunja-" „Sippe, Geschlecht“ (got. "kuni", ahd. und altsächs. "kunni", mhd. "künne", engl. "kin") als Ausgangspunkt der Bildung. Der "*kuningaz" wäre dann „der einem (edlen) Geschlecht Entstammende“ (von vornehmer Herkunft) gewesen.

Das deutsche Wort "König" stammt aber nicht direkt vom protogermanischen "kuningaz" ab, sondern von dem in Form und Bedeutung eng verwandten protogermanischen "kuniz". Das deutsche Wort ist eng mit dem neuniederländischen "koning", dem neuenglischen "king", dem neuschwedischen "konung" und "kung" und dem neuisländischen "kon(un)gur" verwandt.

Die weibliche Form "Königin" kann nicht nur eine dem männlichen König entsprechende Würdenträgerin bezeichnen, sondern auch die Ehefrau eines Königs (siehe Titularkönigin). Der Ehemann einer regierenden Königin wird hingegen meist nicht als König (Titularkönig), sondern als Prinzgemahl bezeichnet. Das englische Wort für Königin, "queen", bedeutet eigentlich nur "Ehefrau", von altenglisch "cwēn", „Ehefrau; Königin“. Dieses gehört zu einem indogermanischen Wortstamm, der einfach „Frau“ bedeutet, wie norwegisch "kvinne", das Wort "žena" bzw. "жeнa" für „Frau“ in den slawischen Sprachen und das Griechische "γυνή" (gesprochen altgriechisch "gynḗ", neugriechisch "jini").

Zum lateinischen Königstitel "rēx" (Genitiv "rēgis") gehört der Begriff "regnum" ("Königreich") und das Verb "regere/regnare" ("herrschen"). Er ist etymologisch verwandt mit "rājā", dem indischen Wort für „König“ (gesprochen "raadschaa" auf Sanskrit und Hindi). Das deutsche Wort "Reich" gehört zur selben indogermanischen Wortfamilie und ist wohl ein altes keltisches Lehnwort: keltisch wahrscheinlich *"rīgjom" zu *"rīgs" = König (vgl. den Namen des gallischen Häuptlings Vercingetorix). Aus diesem *"rīgs" leiten sich das irische "rí" und das schottisch-gälische "righ" für „König“ und das walisische "rhi" für „Adliger“ ab. „König“ heißt auf walisisch "brenin".

In slawischen Sprachen war das ursprüngliche Wort für "König" der Knjaz, später wurde der slawische Königstitel vom Eigennamen "Karl" abgeleitet, nach Karl dem Großen (Analog der Ableitung der Begriffe "Kaiser" und "Zar" vom Namen Caesar): Sorbisch: "kral", tschechisch "král", polnisch "król" [krul], slowenisch, kroatisch, bosnisch und serbisch "kralj", russisch "кopoль" (korol').

In der ungarischen Sprache ist das Wort für "König" vermutlich slawischen Ursprungs: "király" (vgl. kroat. bosn."kralj").

Für den Begriff "König" in außereuropäischen Ländern wird die Bezeichnung bei der Übersetzung oft willkürlich gewählt, um die lebenslange Herrschaftsfunktion zum Ausdruck zu bringen. Bei kleinen Königtümern und Stammeskönigtümern ist der Übergang vom Häuptling zum König oft fließend, in der Landessprache nicht selten ein und derselbe Begriff.

Der chinesische Titel des Wang war in den frühen Dynastien (bis zur Einigung Chinas als Kaiserreich) die Bezeichnung des souveränen Herrschers, weshalb er in der westlichen Übersetzung mit dem König gleichgesetzt wird. Später wurde der Wang jedoch zum höchsten chinesischen Adelstitel im Kaiserreich, in westliche Sprachen üblicherweise übersetzt als Prinz.

Im Orient war das Königtum die am weitesten verbreitete Herrschaftsform. Bei den Persern, Hethitern und Sumerern gab es neben den Großkönigen auch untergeordnete Kleinkönige. Die sumerische Kultur ist ein spezieller Fall: Später wurde die Herrschaft von dem im Norden gelegenen Akkad übernommen. Auch in anderen Königtümern, zum Beispiel bei den Ägyptern kam es zu der Regierungsübernahme von vorher abhängigen Ethnien (den Nubiern). Doch bei den Sumerern herrschte ein häufiger Wandel, der schließlich in der Bildung von Kleinreichen, wie zum Beispiel Der und Susa sowie zwei größeren Reichen, dem assyrischen Reich und dem babylonischen Reich, endete. Welche Funktionen der König im Einzelnen hatte, ist nicht sicher. Auch ist nicht bekannt, wie der König zu seinem Amt kam. Ursprünglich muss er auch Priester gewesen sein. Auf jeden Fall hatte er anders als die frühen skandinavischen Könige eine rechtsprechende Funktion. Sein Selbstverständnis drückte er durch die Formel „Hirte der Völker“ aus, was zum ersten Mal für Lugalzagesi bezeugt ist.

Bei den Ägyptern handelte es sich um eine Art Gottkönigtum oder Gottkaisertum, bei dem der Pharao, ägyptisch "Per-aa" (großes Haus) ebenso geistliches Oberhaupt und göttliche Reinkarnation war.

Fast alle diese Königreiche wurden mit dem Eroberungsfeldzug von Alexander dem Großen zerschlagen. Ihnen folgten die Diadochenreiche, als Alexanders Generäle eigene Reiche gründeten (Hellenismus). Das Seleukidenreich und das Ptolemäerreich hielten sich am längsten (spätes 1. Jahrhundert v. Chr.). In der Tradition Alexanders beriefen auch sie sich auf ihre göttliche Abkunft, jedoch in erster Linie zur Legitimation; Pflichten als religiöses Oberhaupt gingen nicht primär damit einher. Schließlich übertrugen die Römer nach der Eroberung großer Teile des Orients die Vorstellung von der Göttlichkeit des Herrschers auf das Kaisertum, das seit der Spätantike christlich legitimiert war.

"Siehe auch:" Liste der Pharaonen, Liste der nubischen Könige, Liste der hethitischen Großkönige, Liste der babylonischen Könige, Liste der assyrischen Könige, Liste der Seleukidenherrscher, Liste der Ptolemäer unter dem Artikel Ptolemäer, Punkt 3.

Das antike Griechenland war eine sehr lose, oft in widerstreitende Allianzen gespaltene Staatengemeinschaft. In den griechischen Staaten existierten verschiedene, teils wechselnde Staatsformen; das Königtum war in archaischer und klassischer Zeit (ca. 800 bis 336 v. Chr.) eine seltene Ausnahme im griechischen Kernland. In Sparta existierte jedoch ein Doppelkönigtum. Sinn dahinter war eine gegenseitige Kontrolle, wobei die Königsherrschaft ohnehin eingeschränkt war. In der Zeit des Hellenismus war das Königtum hingegen die gängige Staatsform in den Nachfolgereichen des Alexanderreichs, wobei die Macht der hellenistischen Könige in ihrem jeweiligen Reich weitgehend unbeschränkt war.

Das eigentliche Wort für König, βασιλεύς [basileus], wurde später auch auf die römischen Imperatoren angewandt. Vom Königtum zu unterscheiden ist die Tyrannis.

"Siehe: Römische Königszeit" und "Rex (Titel)"

In seiner Anfangszeit seit der (angeblichen) Gründung durch Romulus und seinen Bruder Remus war der römische Staat ausschließlich durch Könige regiert worden, wenngleich vieles von legendären Erzählungen verschleiert ist. Nach dem Sturz des siebten und letzten Königs Lucius Tarquinius Superbus durch die Adligen der Stadt (angeblich 509 v. Chr.) war Rom bis ins späte 1. Jahrhundert v. Chr. eine Republik. Der Königstitel "(rex)" war so verpönt, dass bei der Umwandlung der Diktatur Caesars in eine erneute dauerhafte Monarchie durch Augustus die Amtsbezeichnung Imperator in ihrer Bedeutung verändert und mit Caesars Eigennamen (eigentlich seinem Cognomen) ausgeschmückt wurde. Als Ausnahme übertrug Konstantin der Große seinem Neffen Hannibalianus um 335/36 den Titel "rex", was sich jedoch auf separate römische Klientelherrschaften bezog.

Die Frage nach dem Königtum bei den Germanen wird in der neueren Forschung kontrovers diskutiert. In der älteren Forschung wurde aus den antiken Quellen auf ein germanisches Königtum geschlossen, das in unterschiedlichen Ausprägungen (siehe Sakralkönigtum und Heerkönigtum sowie Kleinkönig) bei diversen Stämmen existiert habe, wobei den antiken Autoren zufolge bei einigen Stämmen gar kein Königtum mehr existierte. In neuerer Zeit wird allerdings auf methodische Mängel älterer Arbeiten hingewiesen. Insofern agierten auf germanischer Seite zwar Anführer/Herrscher, bei der in lateinischen Texten auftauchende Bezeichnung "rex" (König) handelte es sich allerdings wohl eher um eine Hilfskonstruktion, um so von römischer Seite mit vertrauten Begriffen außenpolitisch agieren zu können. Ob die jeweiligen Anführer aber im eigentlichen Sinne als Könige (mit allen damit verbundenen Erwartungen) zu betrachten sind, wird in der neueren Forschung in Zweifel gezogen.

In den Quellen werden eine ganze Reihe verschiedener Arten von Königen aufgeführt: Könige, Kleinkönige, Heerkönige und Seekönige. Letztere besaßen kein Herrschaftsgebiet.

Die Quellen der frühen Zeit schweigen sich über Stellung und Funktion des Königs aus. Auch weiß man nicht, wie man ursprünglich König wurde. Allerdings spricht viel dafür, dass am Anfang ein Wahlkönigtum bestanden hat. Es ist anzunehmen, dass immer Personen aus den vornehmsten Familien und schließlich der Familie des Vorgängers zur Wahl standen, so dass sich allmählich ein Erbkönigtum entwickelte. Es deutet vieles darauf hin, dass zumindest in Schweden am Anfang ein Sakralkönigtum bestanden hat. In diesem Kontext hatte der König die Aufgabe, durch seine familiäre Beziehung zur göttlichen Sphäre (die Könige leiteten sich von Göttern als Stammeltern her) Wachstum und Gedeihen in ihrem Bereich zu garantieren.
Bei diesem Vorgang spielte neben der Schaffung eines Zentralkönigtums durch Harald Hårfagre die Kirche eine besondere Rolle, indem sie König Olav Haraldsson zum Heiligen erklärte, der sein göttlich legitimiertes Königsheil auf seine Nachkommen überträgt.

Harald Hårfagre stammte von einem Kleinkönig ab, konnte aber ein Oberkönig werden. Es ist unbekannt, ob diese Könige ihr Königtum auf den Familienstamm oder auf ihre militärische Stärke gründeten. Harald jedenfalls baute vor allem auf seine Militärmacht. Des Weiteren war diese aufwendig zu unterhalten, weshalb er in großem Umfang Bauern enteignete.

Torbjørn Hornklove dichtet über Harald:

Das ist die Beschreibung eines typischen Wikingerkönigs. Offenbar hatten fremde Vorbilder ihn dazu gebracht, dass er eine andere Art von König sein wollte. So könnte auch an ein Gerichtskönigtum gedacht gewesen sein. Torbjørn Hornklove bezeichnet in der "Glymdrápa" Haralds Gegner als "hlennar" = Diebe, was ein Hinweis auf den Versuch, Recht und Ordnung durchzusetzen, gedeutet werden könnte. Der Ausdruck wird aber eher nur eine Herabsetzung der Feinde bedeuten.

Der König hatte eine große Zahl an Schiffen und Mannschaften zu unterhalten. Dazu benötigte er verschiedene Arten von Einkünften. Eine davon waren die Königshöfe, die an der Küste aufgereiht waren und aus Enteignungen stammten. Diese Stellen zahlten ihre „Steuer“ dadurch, dass sie den König mit Mannschaft für eine gewisse Zeit mit Kost und Logis beherbergten. Es handelte sich also um ein Reisekönigtum. Das entspricht ganz der Art, wie die übrigen Wikingerkönige z. B. in Irland vorgingen. Der Vorteil für die Bauern war, dass der König andere Räuber fernhielt, so dass die Belastung auf viele Bauern verteilt überschaubar war.

Die Funktion des Königs beschränkte sich lange auf die Vertretung des Gesamtstaates nach außen (Der König musste entscheiden, ob man in den Krieg zog), auf das Heerwesen und die Verwaltung, soweit sie für die Gesamtheit erforderlich war. Eine andere Hauptfunktion war die Verteilung der Kriegsbeute.

Der Kleinkönig war ein Stammesführer, der nur über einen begrenzten Raum und nur über einen Teil eines größeren Stammesverbands herrschte.

Der Unterkönig, auch Skattkönig "(Steuerkönig)" genannt, war ein mediatisierter König, der zwar in seinem Machtbereich weitgehende Souveränität besaß, aber einen Oberkönig anerkennen musste, dem er abgabepflichtig war und der die Reichseinheit wahrte und für die Gesamtverteidigung zuständig war.

Der Heerkönig und der Seekönig waren eigentlich Feldherren in unserem Sinne. Sie sammelten Schiffe und Mannschaft um sich und zogen zu Plünderungszügen aus. Sie waren aber an bestimmte Regeln in ihrer Befehlsgewalt gebunden. Insbesondere gab es ungeschriebene Gesetze über die Verteilung der Beute, an die sie sich zu halten hatten. Das galt übrigens auch für die fränkischen Könige in der frühen Zeit. Snorri definiert in der Ynglingasaga den Seekönig so: „"Da gab es viele Seekönige, die über große Heere geboten, aber kein Land besaßen. Den allein erkannte man mit Fug als einen richtigen Seekönig an, der nie unter rußigem Hausdach schlief und nie im Herdwinkel beim Trunke saß"“. Sie sollen sogar auf den Schiffen überwintert haben. Denn in einer Beratung zwischen König Olav dem Heiligen und dem Schwedenkönig Önund sagt Olav: "„Wir haben doch ein sehr starkes Heer und gute Schiffe die Menge, und wir können sehr wohl den ganzen Winter hindurch an Bord unserer Schiffe bleiben nach der Art der alten Wikingerkönige.“"

Der Heerkönig auf dem Festland war während der Völkerwanderungszeit gleichzeitig Identifikationsfigur. Die germanischen "gentes" sind nach heutiger Ansicht durchaus multiethnisch gewesen. Sie erhielten ihre Identität durch die Zugehörigkeit zu einem bestimmten Heerkönig und dessen Familie, an deren Seite sie kämpften und deren Traditionen sie übernahmen. Die frühmittelalterliche ethnische Terminologie ist nicht kulturell, linguistisch oder geographisch, sondern militärisch und politisch. Die Ethnie war also nicht eine objektive Kategorie mit einer präzisen Definition, sondern ein subjektiver Prozess, durch den sich die Individuen selbst und auch die anderen definierten, und zwar in bestimmten Situationen, besonders im Zusammenhang mit Konflikt und Krieg. Die ethnischen Gruppen veränderten sich daher schnell und definierten sich auch um und zwar mit verblüffender Schnelligkeit.

Alle diese Königsbezeichnungen dürften sekundär und erst in der Wikingerzeit entstanden sein, also im 8. Jahrhundert Der Begriff „König“ für einen Herrscher in einem Gebiet ist aber offenbar älter. Wahrscheinlich haben Söhne von Königen, die zum Wikingern auszogen, den Königstitel für ihre Heerfahrt angenommen.

Sobald der Königstitel erblich geworden war, waren offenbar seine männlichen Nachkommen gleichberechtigt zur Nachfolge berufen, entweder, indem sie gemeinsam regierten oder das Reich teilten oder indem einer die Regierung allein übernahm, der andere mit Vermögen abgefunden wurde. Die Mündigkeit zur Herrschaft wird allgemein auf das 12. Lebensjahr angesetzt. Das Königtum war Eigentum und Erbgut des regierenden Hauses. Im Norwegen des christlichen Mittelalters war es das 15. Lebensjahr. Erik Magnusson stand 1280 mit 12 Jahren noch unter der Vormundschaft des Reichsrates.

Für Frauen gab es eine „latente“ Thronfolgeberechtigung. Sie konnten zwar selbst nicht Herrscherinnen werden, aber den ihnen an sich zukommenden Herrschaftsanspruch auf ihren Ehemann oder Sohn weitergeben. Die Heimskringla (keine Geschichtsschreibung, aber ein Spiegel der Kenntnisse der Verfasser über bestimmte Gesellschaftsstrukturen) berichtet, dass König "Eysteinn Halfdánarson" Vestfold geerbt habe, als sein Schwiegervater, König "Eiríkur Agnarsson" kinderlos gestorben war. König Halvdan Svarte, der Vater Harald Hårfagres soll erst einen Teil von Agdir von König "Haraldur granrauði", seinem Großvater mütterlicherseits und dann auch noch Sogn über seinen Sohn Harald von dessen mütterlichen Großvater "Harald gullskegg" geerbt haben. Das war auch mit dem normalen Erbrecht vereinbar. Danach konnten Frauen eine Grundherrschaft erben, allerdings die Herrschaft nicht persönlich ausüben.

Bei der Thronfolge wurde das normale Erbrecht nachgebildet. So schloss der nähere Verwandtschaftsgrad den ferneren vollständig aus. Dabei wurde allerdings nicht vom verstorbenen König aus gerechnet, sondern vom Stammvater, von dem das Königtum abgeleitet wurde. So schloss der Sohn zwar den Enkel aus. Aber wenn der verstorbene König einen Sohn und eine Tochter hatte, so waren die Söhne des Sohnes und ihre Söhne gleichberechtigt. Bei der Erbfolge in einen Gutshof galt: Die männlichen Nachkommen schlossen die weiblichen zwar aus, nahmen ihnen aber nicht das latente Nachfolgerecht. Bei zwei Schwestern verdrängte diejenige, die einen Sohn hatte, die Schwester, die nur eine Tochter hatte, vom Hof. Hatte in der nächsten Generation der Sohn nur eine Tochter und die Schwester-Tochter einen Sohn, so verdrängte dieser umgekehrt die Tochter. Dies ist alles so im Gulathingslov geregelt. Wie weit diese Regeln auch auf die Thronfolge angewendet wurden, lässt sich nicht feststellen. Jedenfalls gab es einen Unterschied: Während nach der zivilen Erbfolgeregelung uneheliche Söhne erst nach den Geschwisterkindern erben konnten, waren außereheliche Kinder ohne weiteres thronfolgeberechtigt. "Håkon der Gute" war unehelicher Sohn von "Harald Hårfagri", "Magnus der Gute" war unehelicher Sohn von "Olav dem Heiligen". Die meisten Könige damals waren unehelich.

Bei der gemeinsamen Regierung mehrerer Brüder folgte der Sohn eines versterbenden Königs seinem Vater nicht nach, sondern dessen Königsherrschaft wuchs den verbleibenden Königen zu.

Harald Hårfagre versuchte, durch Hausgesetz die Erbfolge erstmals abweichend zu regeln, indem er bestimmte, dass seine Söhne das Reich teilen sollten, aber einer das Oberkönigtum innehaben sollte. Jeder sollte sein Königtum im Mannesstamme vererben. Die Söhne von Töchtern sollten – ebenfalls erblich – die Jarlswürde erhalten, womit eine kleinere Herrschaft, dem König untergeordnet, bezeichnet war. Mit Hilfe des Oberkönigtums sollte trotz der Teilung der Herrschaft eine Einheit des Reiches nach außen gewahrt bleiben.

Die Funktion des Königtums änderte sich im christlichen Mittelalter, insbesondere um 1300, allmählich. Unter Erik II. und besonders unter seinem Nachfolger Håkon Magnusson bekam der König eine im frühen Skandinavien unbekannte Rolle als oberster Gesetzgeber und oberster Richter. Um diese Zeit wurde der Königsspiegel in altnorwegischer Sprache verfasst, der die Stellung des Königs ausschließlich biblisch begründet. Hier kommen die kontinentalen Strömungen der Rechts- und Staatswissenschaften zum Tragen.

Nachdem die ostfränkische Linie der Karolinger ausgestorben war, entstand ein Wahlkönigtum im "Ostfränkischen Reich", aus dem das Heilige Römische Reich hervorging. Der König wurde von einem bestimmten Kreis der Großen des Reichs gewählt (nicht alle Fürsten waren am Wahlakt beteiligt bzw. konnten das Recht beanspruchen), es existierte keine Erbmonarchie. Die Königsmacht war nie absolut, vielmehr waren die römisch-deutschen Könige auf die Kooperation der Großen angewiesen (Konsensuale Herrschaft). Die Könige konnten den Papst darum bitten, sie zum Kaiser zu krönen, wofür nun nur noch die römisch-deutschen Könige in Frage kamen. Ihr Kaisertum und ihr Königtum war (wie im Mittelalter allgemein üblich) mit dem Gottesgnadentum verbunden und stand nun auch in Verbindung mit der universalen Reichsidee. Römisch-deutsche Könige ohne Kaiserwürde trugen den Titel "Rex Francorum", ab dem 11. Jahrhundert "Rex Romanorum" (siehe Römisch-deutscher König). Der Kreis der Wahlberechtigten engte sich immer mehr ein, da unter den damaligen Bedingungen nur ein Bruchteil von ihnen praktisch an der Wahl beteiligt war. Seit dem staufisch-welfischen Thronstreit von 1198 war eine Königswahl nur gültig, wenn daran die Erzbischöfe von Mainz, Köln und Trier sowie der rheinische Pfalzgraf beteiligt waren. Aus dieser Gruppe gingen dann im Spätmittelalter die Kurfürsten (von "küren" = wählen) hervor, die spätestens seit 1273 die alleinigen Wähler waren, was 1356 in der Goldenen Bulle verbindlich festgeschrieben wurde.

Nach 1530 war der gewählte König automatisch Kaiser. Die Kaiserkrönung fand nun ohne Beteiligung des Papstes in Aachen statt. Gleichwohl war der Kaiser immer noch römisch-deutscher König. Neben der deutschen Königswürde gab es im Heiligen Römischen Reich nur die Königswürde von Burgund (zuletzt von Karl IV. wahrgenommen) und die von Böhmen.

Unter diesen Bedingungen wählten in der Zeit des Absolutismus nach Glanz strebende deutsche Territorialherrscher den Ausweg, außerhalb des Reiches König zu werden:
August der Starke, Kurfürst von Sachsen, ließ sich 1697 zum König von Polen wählen. Kurfürst Friedrich III. von Brandenburg war Souverän im außerhalb des Reichs gelegenen Herzogtum Preußen. Im Jahr 1701 erreichte er nach Verhandlungen mit Kaiser Leopold I. die Anerkennung seiner Selbstkrönung zum "König in Preußen". Die welfischen Kurfürsten von Hannover waren seit 1714 in Personalunion Könige von England.

Bayern, Württemberg und Sachsen wurden erst nach dem Ende des "Heiligen Römischen Reiches", Hannover nach dem Wiener Kongress Königreiche. Die Hannoverschen Welfen trugen danach bis zum Ende der Personalunion mit England 1837 die Kronen Englands und Hannovers.

Der Königstitel wird in den meisten Ländern Europas durch Erbgang nach dem Tod oder Rücktritt (Abdankung) des Vorgängers übertragen. In den Erbmonarchien galt früher fast immer das männliche Erstgeburtsrecht. Nachfolger wurde also stets der älteste männliche Erbe des verstorbenen Königs. Die meisten europäischen Monarchien haben in den letzten Jahren die Erbfolge zugunsten des ältesten leiblichen Erben – gleichgültig ob Mann oder Frau – geändert.

Einige Königreiche, wie etwa Polen und heute noch Malaysia und der Vatikanstaat (Papst), waren dagegen Wahlmonarchien. In ihnen bestimmte ein festgelegter Kreis von Wählern – in Deutschland waren dies die Kurfürsten – den Nachfolger eines verstorbenen oder abgesetzten Königs.

Der formelle Amtsantritt eines Königs erfolgt im Rahmen einer feierlichen Krönung, wie im Vereinigten Königreich oder in einer Huldigungszeremonie, wie in den Niederlanden.

"Siehe auch:" Monarchie

Die folgenden Links verweisen auf die Listen der Herrscher im jeweiligen Land.




</doc>
<doc id="11057" url="https://de.wikipedia.org/wiki?curid=11057" title="Klaus Kinski">
Klaus Kinski

Klaus Kinski (* 18. Oktober 1926 in Zoppot, Freie Stadt Danzig, als "Klaus Günter Karl Nakszynski"; † 23. November 1991 in Lagunitas, Kalifornien) war ein deutscher Schauspieler. Er ist der Vater von Pola, Nastassja und Nikolai Kinski.

Er war auf die Darstellung psychopathischer und getriebener Figuren spezialisiert und zählte in diesem Rollenfach auch international zu den gefragtesten Filmschauspielern. Als künstlerisch herausragend gilt seine jahrelange Zusammenarbeit mit dem deutschen Regisseur Werner Herzog, der ihn in Filmen wie "Nosferatu – Phantom der Nacht", "Aguirre, der Zorn Gottes" und "Fitzcarraldo" engagierte. International bekannt war Klaus Kinski zuvor durch Rollen in Edgar-Wallace-Filmen und Italowestern geworden.

Kinski galt als schwierige und zu extremen Gefühls- und Wutausbrüchen neigende Persönlichkeit.

Der Sohn des Apothekers Bruno Nakszynski und dessen Frau, der Krankenschwester Susanne Nakszynski, geb. Lutze, hatte drei ältere Geschwister: Inge, Arne und Hans-Joachim („Achim“). 1930 zog die Familie nach Berlin und bezog eine Wohnung in der Wartburgstraße 3 in Berlin-Schöneberg. Nach eigenen Aussagen musste sich Kinski während der Schulzeit Geld zum Unterhalt selbst verdienen. Dass er, wie er behauptete, Schuhputzer, Laufjunge und Leichenwäscher gewesen sei, ist nicht weiter belegt. Kinskis Behauptungen, die Familie sei arm gewesen, widersprechen seine älteren Brüder. Die Familie sei „gutbürgerlich“ gewesen und Klaus „besonders umhegt“.<ref name="spiegel45/75"></ref>

Im Zweiten Weltkrieg wurde er 1944 zu einer Fallschirmjägereinheit der Wehrmacht eingezogen und geriet an der Westfront in den Niederlanden in britische Kriegsgefangenschaft. Die genauen Umstände seiner Gefangennahme sind nicht endgültig geklärt, doch geschah dies wahrscheinlich am 14. November 1944 bei Helmond durch die 2. Britische Armee. Nach eigenen Angaben sei er Ende Oktober 1944 desertiert, jedoch gefasst und wegen Fahnenflucht zum Tode verurteilt worden. Nachdem er dem Soldaten, der zu seiner Bewachung abgestellt worden war und der sich als schwul herausstellte, Analverkehr angeboten habe, habe er ihn bewusstlos geschlagen und erneut fliehen können. Er habe sich unbewaffnet in einem Erdloch versteckt, wo er von britischen oder kanadischen Soldaten zunächst schwer verwundet und anschließend gefangengenommen worden sei. Mit zwei glatten Durchschüssen an Schulter und Arm habe er anschließend 14 Wochen in einem Lazarett verbracht.

Im Februar oder März 1945 wurde Kinski aus einem Lager in Deutschland in das Kriegsgefangenenlager „Camp 186“ in Berechurch Hall bei Colchester in Essex gebracht. Hier spielte er am 11. Oktober 1945 in der Groteske „Pech und Schwefel“ seine erste Theaterrolle auf der provisorischen Lagerbühne, die vom Schauspieler und Regisseur Hans Buehl geleitet wurde. In den folgenden Aufführungen spielte er regelmäßig Frauenrollen. Nachdem er im Anschluss an eine Theaterprobe einen anderen Schauspieler geschlagen hatte, musste er die Gruppe verlassen und wechselte zu einer Kabarett-Gruppe im Lager. Im Frühjahr 1946 gehörte er zu den letzten Gefangenen, die aus dem Lager zurück nach Deutschland geschickt wurden. Nach eigener Darstellung habe er zunächst mit einer sechzehnjährigen Hure, die er im Zug kennen gelernt hätte, sechs „wilde“ Wochen in Heidelberg verbracht, habe diese aber verlassen und danach an Theatern in Tübingen und Baden-Baden gearbeitet, wo er auch vom Tod seiner Mutter durch einen Luftangriff in Berlin erfahren habe. Im Herbst habe er sich illegal nach Berlin begeben.

Ab 1946 wirkte Kinski, obwohl er nicht klassisch ausgebildet war, als Schauspieler an prominenten Berliner Bühnen, zunächst an dem von Boleslaw Barlog geleiteten Schlosspark Theater. Als er einmal vor Wut die Scheiben des Theaters einschlug, wurde er von Barlog entlassen. Arbeitslos geworden, besuchte Kinski kurz die Schauspielschule von Marlise Ludwig, wo er unter anderem mit Harald Juhnke Szenen aus William Shakespeares "Romeo und Julia" einstudierte.

Privat unterhielt Kinski auch Beziehungen zu Berliner Halbweltkreisen. Vorübergehend wohnte er bei dem Regisseur, Maler und Bühnenbildner Eduard Matzig in der Berliner Künstlerkolonie am Laubenheimer Platz. Seine erste Filmrolle erhielt er in "Morituri", gedreht zwischen September 1947 und Januar 1948. Produzent war Artur Brauner, Regisseur Eugen York. "Morituri" erzählt von geflohenen KZ-Insassen, die sich vor den Deutschen verstecken. Der Film war umstritten; es gab Drohbriefe, und ein Hamburger Kino wurde zerstört.

Kinski befand sich im Jahr 1950 für drei Tage in psychiatrischer Behandlung in der Berliner Karl-Bonhoeffer-Nervenklinik, nachdem er eine ihm bekannte Ärztin belästigt und tätlich angegriffen hatte und einen Suizidversuch mit Medikamenten unternommen hatte. Dies wurde 2008 durch die datenschutzrechtlich umstrittene Veröffentlichung einer in diesem Jahr gefundenen Akte bekannt. Die Witwe von Klaus Kinski, Minhoï Loanic, erstattete nach der Veröffentlichung Strafanzeige gegen das Landesarchiv Berlin, den Gesundheitskonzern Vivantes und „alle weiteren in Betracht kommenden Personen“.

Ab 1952 wurde Kinski einem stetig wachsenden Publikum als „Ein-Mann-Wanderbühne“ in Berlin, München und Wien bekannt. Er rezitierte auf kleinen Bühnen und um 1960 im Berliner Sportpalast – einem großen Haus – Arthur Rimbaud, François Villon, Friedrich Nietzsche, Kurt Tucholsky und das Neue Testament. Darüber hinaus kam er mit Größen wie Bertolt Brecht und dem Theaterregisseur Fritz Kortner in Kontakt. 1955 verursachte Kinski einen Autounfall, zudem ereignete sich ein Bootsunfall auf dem Starnberger See. Gerichtsverfahren und Strafen schlossen sich an, die finanziellen Folgen belasteten den Schauspieler jahrelang.

Im Sommer 1955 drehte Kinski mit Kortner in Wien den Film "Um Thron und Liebe". Seine Partnerin war die österreichische Schauspielerin Erika Remberg. Sie verliebten sich während der Dreharbeiten und wurden vorübergehend ein Paar. Zu einem in der Presse vielbeachteten Skandal kam es, als Kinski und Remberg einander vor einem Münchner Freibad küssten. Seine Karriere erlitt in dieser Zeit einen Knick, und er unternahm zwei Selbstmordversuche.

Kinskis Rezitationen, beispielsweise aus Werken von Johann Wolfgang von Goethe, Friedrich Schiller und Brecht, wurden auf über 25 Sprechplatten eingespielt. Spätestens durch die deutschen Edgar-Wallace-Verfilmungen wurde Kinski dem Kino-Publikum und damit der breiten Öffentlichkeit bekannt. Die Aufmerksamkeit des internationalen Publikums erregte vor allem seine eindrucksvoll gespielte Nebenrolle in David Leans "Doktor Schiwago" (1965). Am 20. November 1971 versuchte sich Kinski als Jesus-Rezitator mit einem skandalträchtigen Auftritt in der Berliner Deutschlandhalle mit dem Titel "Jesus Christus Erlöser". Nach Zwischenrufen von Zuschauern und einem harten Wortgefecht kam es zu einem frühen Abbruch der Veranstaltung und der geplanten Tournee.

Kinski agierte in Filmen wie "Fitzcarraldo", "Für ein paar Dollar mehr", "Doktor Schiwago", "Nosferatu – Phantom der Nacht", "Leichen pflastern seinen Weg", in 16 Edgar-Wallace-Filmen und gemeinsam mit Romy Schneider in "Nachtblende". 1979 erhielt er das Filmband in Gold als bester deutscher Schauspieler, erschien jedoch nicht zur Preisverleihung. Der Film "Fitzcarraldo" wurde für den „Golden Globe“ nominiert. Kinski wirkte auch in mehreren Hollywood-Spielfilmen mit, unter anderem spielte er mit Jack Lemmon und Walter Matthau im letzten Billy-Wilder-Film "Buddy Buddy". In "Little Drummer Girl" ("Die Libelle") spielte er neben Diane Keaton die Hauptrolle. In "The Beauty and the Beast" war er Hauptfigur neben Susan Sarandon und Anjelica Huston.

Kinskis Schallplatten verkauften sich weltweit mehrere Millionen Mal.

1983 trat er unter anderem in der Talkshow von David Letterman auf und erschien 1985 im US-amerikanischen Playboy mit einer Titelgeschichte. Das Filmmagazin "American Film" titelte im Jahr 1982: "Ist Kinski der größte Schauspieler der Welt?" Sein Buch "Kinski Uncut" wurde in den USA ein Bestseller.

Mitte der 1980er Jahre drehte er die Action-Filme "" und "Kommando Leopard" mit Lewis Collins in der Hauptrolle. Die beiden Schauspieler kamen jedoch nicht miteinander aus, sodass im zweiten Film keine einzige Szene mit beiden zusammen gedreht wurde. 1989 stellte er mit "Kinski Paganini" sein letztes Filmwerk fertig. Nachdem er den Stoff über Jahre hinweg vergeblich Produzenten und Regisseuren angetragen hatte, übernahm er schließlich Regie, Drehbuch, Schnitt und Hauptrolle selbst. Werner Herzog hatte zuvor mit der Begründung abgelehnt, das Drehbuch sei „unverfilmbar“. Nach vereinzelten Aufführungen in Europa anlässlich des Todes Kinskis kam der Film in den späten 1990er Jahren doch noch in die Kinos.

Als Synchronsprecher lieh Kinski seine Stimme unter anderem Pawel Kadotschnikow in Sergei Eisensteins "Iwan der Schreckliche" und Sabu in "Die schwarze Narzisse".

Kinski war dreimal verheiratet. 1951 lernte er Gislinde Kühbeck auf dem Schwabinger Fasching in München kennen. Nach der Geburt der gemeinsamen Tochter Pola heirateten die beiden 1952. Die Ehe wurde 1955 geschieden. Von 1955 bis 1960 lebte er in Wien. Danach siedelte Kinski nach Berlin über und traf dort die 20-jährige Brigitte Ruth Tocki, die in dem Jazzlokal "Eierschale" auftrat. Sie heirateten 1960. Aus dieser Ehe, die 1969 geschieden wurde, ging die Tochter Nastassja Kinski hervor. Von 1964 bis 1975 lebte Kinski in Rom. Auf einer Party in seiner Villa an der Via Appia lernte er 1969 die 19-jährige vietnamesische Sprachstudentin Minhoï Geneviève Loanic kennen, die er 1971 heiratete. Von 1975 bis 1980 lebte er in Paris. Am 30. Juli 1976 kam der Sohn Nanhoï Nikolai zur Welt. Im Februar 1979 ließen sich Klaus und Minhoï Kinski scheiden. Ende 1980 zog er nach Los Angeles in den Stadtteil Bel Air. Im Frühjahr 1981 kaufte er ein Grundstück in Lagunitas-Forest Knolls, Marin County, und ließ dort ein Haus errichten, in das er einzog. Ab 1987 führte Kinski eine Beziehung mit der zu diesem Zeitpunkt 19-jährigen italienischen Schauspielerin Debora Caprioglio, die Verbindung hielt bis 1989.

Kinski hatte ein sehr wechselhaftes Temperament, das von liebenswürdiger Sanftheit bis zu fürchterlichen Zornesausbrüchen mit wüsten öffentlichen Beschimpfungen reichte. Für ihn wurde oft der Begriff Enfant terrible verwendet.
Seine Reifejahre waren von hypochondrischen Befürchtungen geprägt. Zugleich litt er tatsächlich an gesundheitlichen Problemen: Bei den Dreharbeiten zu "Cobra Verde" brach er einmal zusammen, später in Südamerika konnte er mehrere Tage lang nicht drehen.

In dem Dokumentarfilm "Mein liebster Feind" schildert der Regisseur Werner Herzog das Verhältnis zwischen sich und Kinski, mit dem er in seiner Jugend kurze Zeit in derselben Pension gelebt hatte. Herzog berichtet, dass er einerseits von Kinski verachtet und bei Dreharbeiten oft gedemütigt und wüst beschimpft wurde. Andererseits habe sich in ihrem Verhältnis eine kreative und künstlerische Kraft entwickelt, die sich auf ihre gemeinsamen Filme "Aguirre, der Zorn Gottes" (1972), "Nosferatu – Phantom der Nacht" (1978), "Woyzeck" (1978), "Fitzcarraldo" (1981) und "Cobra Verde" (1987) übertrug. Herzog beschreibt Kinski auch als außerordentlich fleißigen Schauspieler, der seine Rollen tagelang einstudierte, allerdings auch (oft grundlose) Wutanfälle entwickelte, insbesondere dann, wenn er den Eindruck hatte, nicht genügend Aufmerksamkeit zu bekommen. In ruhigen Momenten habe Kinski seine harschen Ausbrüche und auch die Skandale als Versuche ausgegeben, Aufmerksamkeit zu erregen.

Besonders heftig verliefen die Auseinandersetzungen zwischen Herzog und Kinski während der Dreharbeiten zu "Cobra Verde". Kinski verlangte die Absetzung des Kameramanns Thomas Mauch. Herzog gab nach und ließ einen tschechischen Kameramann nach Ghana einfliegen. Herzogs Regie wurde von Kinski heftig kritisiert. An manchen Tagen versuchte Kinski, selbst Regie zu führen. Ein Abbruch der Dreharbeiten konnte verhindert werden.

Oft verkörperte Kinski in Filmen Schurken und Psychopathen; diese Festlegung schien Rückschlüsse auf seinen Charakter zu gestatten. In der Tat trat er in der Öffentlichkeit sehr exzentrisch und oft aggressiv auf. Aufmerksamkeit erregte er etwa in der WDR-Talkshow "Je später der Abend" im Jahr 1977, in der er auf viele Fragen des Moderators Reinhard Münchenhagen nicht einging, ihn aber immer wieder mit „Herr Münchhausen“ anredete und sich mit einem Zuschauer anlegte. Ebenso legendär war die Berliner Vorstellung seiner polarisierenden „Jesus Christus Erlöser“-Bühneninszenierung, in der er Zwischenrufer aus dem Publikum wütend mit „Du dumme Sau“ und „Scheiß-Gesindel“ beschimpfte.

Kinski pflegte einen aufwendigen Lebensstil und übernahm nach eigener Aussage den größten Teil seiner Rollen aus Geldnot. So trat er in Produktionen des europäischen Horrorfilms und Sexfilms der 1970er und des internationalen B-Actionfilms der 1980er Jahre auf.

In einem Interview mit der Zeitschrift Stern erzählte Nikolai Kinski, er habe niemals erlebt, dass sein Vater privat aggressiv oder ausfallend geworden sei, und sagte über ihn: „Mein Vater war privat der sanfteste Mensch, den man sich vorstellen konnte“.

Kinskis Tochter Nastassja beschreibt ihren Vater hingegen als „Tyrannen“, vor dem sie stets fürchterliche Angst hatte und der die Familie terrorisierte. Er habe sie zwar nicht geschlagen, „aber niederträchtig beschimpft“. Sie habe ihn nicht als Vater gesehen: „Ich würde alles dafür tun, dass er auf Lebzeiten hinter Gitter kommt. Ich bin froh, dass er nicht mehr lebt.“

In ihrem Buch "Kindermund", das im Januar 2013 erschien, und Vorabberichten dazu erhob Pola Kinski gegen ihren 1991 verstorbenen Vater den Vorwurf, sie von ihrem 5. bis 19. Lebensjahr sexuell missbraucht zu haben.
Klaus Kinski selbst hatte in seiner 1975 erschienenen Autobiografie "Ich bin so wild nach deinem Erdbeermund" Inzesterlebnisse mit seiner Mutter, seiner Schwester und den Missbrauch seiner Tochter Nastassja geschildert.

Kinskis ältere Brüder bestritten damals, dass seine Ausführungen der Wahrheit entsprächen, und warfen ihm Falschdarstellungen in Bezug auf seine Kindheit und Jugend vor. Während seine zweite Tochter Nastassja Kinski die Inzest- und Missbrauchschilderungen ihres Vaters 1991 noch als „gemeine Lügen“ bezeichnete, gab sie als Reaktion auf die Missbrauchsschilderungen ihrer Schwester 2013 an, ihr Vater habe sie zwar nicht missbraucht, aber „viel zu sehr angefasst“, und sie habe gespürt, dass es sich dabei nicht um die liebevolle Umarmung eines Vaters handle.

Kinski starb am 23. November 1991 im Alter von 65 Jahren in seinem Anwesen in Lagunitas (Kalifornien) an einem Herzproblem. Die Obduktion ergab, dass das Herz vernarbt war, was wahrscheinlich eine Folge mehrerer unbehandelter Herzinfarkte war.

Seinem Wunsch gemäß wurde sein Leichnam verbrannt und seine Asche bei San Francisco in den Pazifik gestreut.

Kinski erhielt mit dem Filmband in Gold 1979 seinen einzigen deutschen Filmpreis für seine darstellerische Leistung in "Nosferatu: Phantom der Nacht". Für dieselbe Rolle erhielt er auch einen Darstellerpreis beim Filmfestival von Cartagena. Am 5. Februar 1986 gab der damalige französische Kulturminister Jack Lang die Ernennung Kinskis zum Commandeur de l’Ordre des Arts et des Lettres bekannt. Dieser Orden ist eine der höchsten Ehrungen Frankreichs für einen ausländischen Künstler. Am 12. April 2011 wurde Kinski mit einem Stern auf dem Boulevard der Stars in Berlin geehrt.
Kinski war Ehrenbürger der Stadt Danzig.

"Klaus is dead", das Debütalbum der deutschen Band Swoons, ist Kinski gewidmet.

Die amerikanische Band "Kinski" und die Duisburger Punkband "Die Kinskis" benannten sich nach dem deutschen Schauspieler, ebenso wie die Formation "Hello Kinski" aus den USA.

Die Ärzte verwendeten Zitate aus Kinskis „Villon, das bin ich“ als Einleitung für ihren Song „Lieber Tee“.

Die amerikanische Band "Elastic No-No" widmete im Jahr 2006 ihrem Idol den Song "I am Klaus Kinski".

Der Frankfurter DJ und Musikproduzent Oliver Lieb veröffentlichte 2002 die Single "Jesus ist da", die Samples aus Kinskis "Jesus-Christus-Erlöser" enthält.

In dem Song "Glaubenskrieg" der deutschen Band Feindflug werden Samples von "Jesus Christus Erlöser" verwendet: „Ich bin nicht der offizielle Kirchenjesus, […] Ich bin nicht euer Superstar“. Luke Haines verwertete 2009 das gleiche Zitat; ebenso das deutsche Rap-Duo Pimpulsiv in ihrem Song „Minimal Klaus“ aus dem Album Hepatitis P.

Rex Joswig verarbeitet die Rezitation in seinem Stück „Kinski in Dub“.

Der Rapper Kool Savas platzierte auf seinem 2010 erschienenen Album „John Bello Story 3“ den Song „Mach doch deinen Scheiss“, in dem viele Samples und Zitate von Kinski eingebaut sind.

Das „Jesus-Christus-Erlöser“-Sample wurde auch im Song „Mohn auf weißen Laken (WITS-Mix)“ der Band Samsas Traum verwendet.

Die dunkelromantische Musikgruppe Adversus lässt das Stück „Die letzte Glocke“ mit einem Kinski-Zitat beginnen.

In den Songs "Mann aus Stein" und "Bei den Sternen" der Band Eïs auf dem 2012 erschienenen Album "Wetterkreuz" werden Kinskizitate als Samples verwendet. Diese stammen aus seiner Lesung von "Der Steinmann" von August Strindberg.

Zudem wurde Klaus Kinski in Liedern von Lou Reed und Udo Lindenberg erwähnt.

Der Musiker Torch nutzte Samples aus Interviews und Filmen mit Klaus Kinski, z. B. in dem Lied "Kapitel 29".

Die Metalband Hideous Divinity benannte 2014 das Album „Cobra Verde“ nach dem Film mit Kinski und verwendete einige Zitate als Sprachsamples.

Lange Zeit waren, abgesehen von vereinzelten Zeitungsartikeln, Kinskis Autobiografien die einzigen Quellen zu seinem Leben. In den beiden Verkaufsschlagern "Ich bin so wild nach deinem Erdbeermund" (erschienen 1975) und "Ich brauche Liebe" (1991) stellte er sich mit einem Schuss dichterischer Freiheit vor allem als Libertin und Sexualprotz dar. 1992 erschien "Paganini". In den 1980er Jahren veröffentlichte Philippe Setbon ein Buch, das sich vor allem mit Kinskis Filmen beschäftigte und auch biografische Details lieferte. 1995 drehte Dagmar Cuntze für den SFB die Dokumentation "Ich bin so wild nach deinem Erdbeermund" und sprach dafür mit Kollegen Kinskis (z. B. Brigitte Grothum). 1998 erschien "Kinski, Werk der Leidenschaft" von Georg Wend, das sich vorrangig den Filmen widmete und auch neue Informationen zur Person bot. 1999 verarbeitete Werner Herzog erstmals seine Erfahrungen mit Kinski zum Dokumentarfilm "Mein liebster Feind", unterhielt sich u. a. mit ehemaligen Mitwirkenden in dessen Filmen und besuchte Schauplätze gemeinsamer Filme. 2001 wurden zum zehnten Todestags Kinskis zwei Ausstellungen organisiert, die mit Buchpublikationen verbunden waren. Für arte und den WDR entstand der Dokumentarfilm "Ich bin kein Schauspieler" von Christoph Rüter, der auch Kollegen Kinskis zu Wort kommen ließ (z. B. Mario Adorf).
2003 erschien im Brandstätter-Verlag der Bild- und Textband „Ich bin so wie ich bin“, herausgegeben von Peter Reichelt und Ina Brockmann, die auch eine an verschiedenen Orten gezeigte Kinski-Ausstellung organisierten.

Zum 80. Geburtstag Kinskis erschienen 2006 zwei Bücher über den Schauspieler, die neue Seiten ans Tageslicht brachten. Der Wiener Filmwissenschaftler und Kritiker Christian David stellte "Kinski. Die Biographie" vor, die erste große Biografie, die auf rund 450 Seiten detailliert, mit Interviews von Zeitzeugen, Kollegen und Freunden (darunter Bruno Ganz, Peter Berling, Judith Holzmeister, Peter Hajek u. a.) sowie unter Verwendung bisher unbekannter Dokumente und privater Briefe das Leben und Werk des Schauspielers darstellt. Kurz darauf veröffentlichte Kinskis Nachlassverwalter Peter Geyer das Taschenbuch "Klaus Kinski", das auf 160 Seiten Leben und Werk zusammenfasst und Aufsätze zum Schaffen des Künstlers sowie Interpretationen von Kinskis Filmen enthält.

Im Jahr 2011 produzierten DLF und hr gemeinsam unter Regie von Michael Farin das Hörspiel "Klaus Kinski: Um mich herum ist es dunkel – und in mir wächst das Licht". Der Autor Peter Geyer verwendete für den Text ausschließlich Zitate von Klaus Kinski; es sprechen Blixa Bargeld, Ulrich Matthes und Nadeshda Brennicke.








</doc>
<doc id="11058" url="https://de.wikipedia.org/wiki?curid=11058" title="Absolute Häufigkeit">
Absolute Häufigkeit

Der Begriff absolute Häufigkeit ist gleichbedeutend mit dem umgangssprachlichen Begriff Anzahl. Die absolute Häufigkeit ist ein Maß der deskriptiven Statistik und soll sich vom Begriff relative Häufigkeit abgrenzen. 

Die absolute Häufigkeit ist das Ergebnis einer einfachen Zählung von Objekten oder Ereignissen (besser Elementarereignissen). Sie gibt an, wie viele Elemente mit dem gleichen interessierenden Merkmal gezählt wurden.

Als Anzahl kann sie nur eine natürliche Zahl sein und auch nicht negativ werden. Wegen ihres festen Nullpunkts und der festen ganzzahligen Einheiten ist sie eine Absolutskala. Das heißt, ihr Nullpunkt und die Größe der Einheiten kann nicht sinnvoll verändert werden. Im Gegensatz zur relativen Häufigkeit sind die Werte der absoluten Häufigkeit also absolut, sprich unveränderlich. Ihr Wertebereich geht von 0 bis Unendlich.

Für den Vergleich von Teilmengen unterschiedlich großer Grundmengen eignet sich hingegen die absolute Häufigkeit nicht. Die Höhe der absoluten Häufigkeit hängt vom Umfang der betrachteten Grundmenge ab, was diesen Vergleich unsinnig macht. Für einen solchen Vergleich wird deshalb ein normiertes Maß, die relative Häufigkeit, verwendet.

Wenn bei formula_1 Beobachtungen eines Zufallversuchs bzw. bei der Überprüfung einer Stichprobe das Ereignis formula_2 insgesamt formula_3-mal auftritt, dann heißt diese Größe die absolute Häufigkeit des Ereignisses formula_2. Die Abkürzung der relativen Häufigkeit ist "h".

Bei der Betrachtung symmetrischer Daten bietet sich eine vorherige Klassierung an. Man bildet dann die absoluten Häufigkeiten der Klassen.
In einer Umfrage werden 453 Personen nach ihrem Alter befragt. Bei der Auszählung stellt man fest, dass 197 Personen in die Klasse "von 20 Jahre bis unter 30 Jahre" fallen. Damit ist die absolute Häufigkeit dieser Klasse 197.

Die absolute Häufigkeit kann anstelle der Wahrscheinlichkeit angegeben werden, um das Verständnis von Risiken und Testbefunden zu erleichtern und wird daher besonders in der Statistik und Wahrscheinlichkeitsrechnung verwandt. Die Angabe erfolgt in „X von Y“, also zum Beispiel „80 von 1000“. Diese Angabe ist eine Normierung der natürlichen Häufigkeit (zum Beispiel „1 von 125“).

Mittels der Darstellung in absoluten Häufigkeiten können medizinische Testergebnisse (HIV-Test, Mammogramm) einfacher interpretiert werden. Eine alternative Berechnung bietet der Satz von Bayes.

Ein Beispiel ("ohne" Angaben von Wahrscheinlichkeiten):


Ein Entscheidungsbaum ist hilfreich, um das Problem zu visualisieren! 

Eine Darstellung im Entscheidungsbaum:


"Ergebnis": Von den 107 (8 + 99) Personen mit positivem Testergebnis sind nur 8 Personen wirklich erkrankt, also weniger als jeder 10. der untersuchten Personen. Das alles ohne andere Untersuchungen.

Bemerkung: Falsch sind die Ergebnisse offensichtlich bei 101 Personen. 99 Personen sind gesund, werden aber im Testergebnis als krank betrachtet (falsch positiv) und 2 Personen sind krank, werden aber im Testergebnis als gesund betrachtet (falsch negativ). 

Diese Visualisierung der Häufigkeit mit einem Entscheidungsbaum hat folgende Vorteile für das Verstehen des Satzes von Bayes:



</doc>
<doc id="11062" url="https://de.wikipedia.org/wiki?curid=11062" title="Trigonometrie">
Trigonometrie

Die Trigonometrie ( ‚Dreieck‘ und ‚Maß‘) ist ein Teilgebiet der Geometrie und somit der Mathematik. Soweit Fragestellungen der ebenen Geometrie (Planimetrie) trigonometrisch behandelt werden, spricht man von "ebener Trigonometrie"; daneben gibt es die sphärische Trigonometrie, die sich mit Kugeldreiecken (sphärischen Dreiecken) befasst, und die hyperbolische Trigonometrie. Die folgenden Ausführungen beziehen sich im Wesentlichen auf das Gebiet der ebenen Trigonometrie.

Die Grundaufgabe der Trigonometrie besteht darin, aus drei Größen eines gegebenen Dreiecks (Seitenlängen, Winkelgrößen, Längen von Dreieckstransversalen usw.) andere Größen dieses Dreiecks zu berechnen. Als Hilfsmittel werden die trigonometrischen Funktionen (Winkelfunktionen, Kreisfunktionen, goniometrischen Funktionen) Sinus (sin), Kosinus (cos), Tangens (tan), Kotangens (cot), Sekans (sec) und Kosekans (csc) verwendet. Trigonometrische Berechnungen können sich aber auch auf kompliziertere geometrische Objekte beziehen, beispielsweise auf Polygone (Vielecke), auf Probleme der Stereometrie (Raumgeometrie) und auf Fragen vieler anderer Gebiete (siehe unten).

Besonders einfach ist die Trigonometrie des rechtwinkligen Dreiecks. Da die Winkelsumme eines Dreiecks 180° beträgt, ist der rechte Winkel eines solchen Dreiecks der größte Innenwinkel. Ihm liegt die längste Seite (als Hypotenuse bezeichnet) gegenüber. Die beiden kürzeren Seiten des Dreiecks nennt man Katheten. Wenn man sich auf einen der beiden kleineren Winkel bezieht, ist es sinnvoll, zwischen der Gegenkathete (dem gegebenen Winkel gegenüber) und der Ankathete (benachbart zum gegebenen Winkel) zu unterscheiden. Man definiert nun:

Diese Definitionen sind sinnvoll, da verschiedene rechtwinklige Dreiecke mit dem gegebenen Winkel untereinander ähnlich sind, sodass sie in ihren Seitenverhältnissen übereinstimmen. Beispielsweise könnte ein Dreieck doppelt so lange Seiten haben wie ein anderes. Die Brüche der genannten Definitionsgleichungen hätten in diesem Fall die gleichen Werte. Diese Werte hängen also nur vom gegebenen Winkel ab. Aus diesem Grund ist es sinnvoll, von Funktionen der Winkel zu sprechen.

Die folgenden Zahlenwerte sind abgerundet.
In einem Dreieck ABC sind folgende Größen gegeben:

Aus diesen Angaben soll die Seitenlänge c ermittelt werden. Da die Ankathete von formula_3 bekannt und die Hypotenuse gesucht ist, wird die Kosinus-Funktion verwendet.

Von einem Dreieck ABC ist bekannt:

Gesucht ist der Winkel formula_7. Die beiden gegebenen Seiten formula_8 und formula_9 sind die Ankathete und die Gegenkathete von formula_7. Daher ist es sinnvoll, die Tangens-Funktion einzusetzen.

Während im letzten Beispiel für einen bekannten Winkel der Kosinuswert zu berechnen war, ist hier die Situation umgekehrt. Aus einem bekannten Tangenswert soll der zugehörige Winkel bestimmt werden. Man benötigt hierfür die Umkehrfunktion der Tangens-Funktion, die so genannte Arcustangens-Funktion (arctan) oder ein Tabellenwerk, aus dem Winkel und zugehöriger Tangenswert abgelesen werden können. Damit erhält man:

Die bisher verwendeten Definitionen sind nur für Winkel unter 90° brauchbar. Für viele Zwecke ist man jedoch an trigonometrischen Werten größerer Winkel interessiert. Der Einheitskreis, das ist ein Kreis mit Radius 1, erlaubt eine solche Erweiterung der bisherigen Definition. Zum gegebenen Winkel wird der entsprechende Punkt auf dem Einheitskreis bestimmt. Die x-Koordinate dieses Punkts ist der Kosinuswert des gegebenen Winkels, die y-Koordinate der Sinuswert.

Die oben gegebene Definition von Sinus- und Kosinuswert durch x- und y-Koordinate lässt sich problemlos auf Winkel über 90° ausdehnen. Man erkennt dabei, dass für Winkel zwischen 90° und 270° die x-Koordinate und damit auch der Kosinus negativ ist, entsprechend für Winkel zwischen 180° und 360° die y-Koordinate und somit auch der Sinus. Auch auf Winkel, die größer als 360° sind, sowie auf negative Winkel lässt sich die Definition ohne Weiteres übertragen.

Man beachte, dass in der modernen Herangehensweise die Beziehung zwischen Winkel und Sinus bzw. Kosinus dazu benutzt wird, um den Winkel zu definieren. Die Sinus- und Kosinusfunktion selbst werden über ihre Reihendarstellung eingeführt.

Die weiteren vier trigonometrischen Funktionen sind definiert durch:

Auch für allgemeine Dreiecke wurden etliche Formeln entwickelt, die es gestatten, unbekannte Seitenlängen oder Winkelgrößen zu bestimmen. Zu nennen wären hier insbesondere der Sinussatz und der Kosinussatz. Die Verwendung des Sinussatzes

ist nützlich, wenn von einem Dreieck entweder zwei Seiten und einer der beiden gegenüber liegenden Winkel oder eine Seite und zwei Winkel bekannt sind. Der Kosinussatz

ermöglicht es, entweder aus drei gegebenen Seiten die Winkel auszurechnen oder aus zwei Seiten und ihrem Zwischenwinkel die gegenüber liegende Seite. Weitere Formeln, die für beliebige Dreiecke gelten, sind der Tangenssatz, der Halbwinkelsatz (Kotangenssatz) und die mollweideschen Formeln.

Die Artikel über die sechs trigonometrischen Funktionen (Sinus, Kosinus, Tangens, Kotangens, Secans, Kosecans) und die Formelsammlung Trigonometrie enthalten zahlreiche Eigenschaften dieser Funktionen und Formeln zum Rechnen mit diesen. Besonders häufig gebraucht werden die Komplementärformeln für Sinus und Kosinus

sowie der „trigonometrische Pythagoras“

Wichtig sind auch die Additionstheoreme der trigonometrischen Funktionen und die Folgerungen daraus. Es geht dabei um trigonometrische Werte von Summen oder Differenzen von Winkeln. So gilt beispielsweise für alle formula_3 und formula_7:

Weitere Identitäten finden sich in der Formelsammlung Trigonometrie.

Trigonometrie spielt in vielen Bereichen eine entscheidende Rolle:

In der Geodäsie (Vermessung) spricht man von Triangulation, wenn man von Punkten bekannter Position aus andere Punkte anpeilt (Winkelmessung) und daraus trigonometrisch die Positionen der neuen Punkte bestimmt. In der Astronomie lassen sich auf entsprechende Weise die Entfernungen von Planeten, Monden und nahe gelegenen Fixsternen ermitteln. Ähnlich groß ist die Bedeutung der Trigonometrie für die Navigation von Flugzeugen und Schiffen und für die sphärische Astronomie, insbesondere für die Berechnung von Stern- und Planetenpositionen.

In der Physik dienen Sinus- und Kosinus-Funktion dazu, Schwingungen und Wellen mathematisch zu beschreiben. Entsprechendes gilt für den zeitlichen Verlauf von elektrischer Spannung und elektrischer Stromstärke in der Wechselstromtechnik.

Vorläufer der Trigonometrie gab es bereits während der Antike in der griechischen Mathematik. Aristarchos von Samos nutzte die Eigenschaften rechtwinkliger Dreiecke zur Berechnung der Entfernungsverhältnisse zwischen Erde und Sonne bzw. Mond. Von den Astronomen Hipparch und Ptolemäus ist bekannt, dass sie mit Sehnentafeln arbeiteten, also mit Tabellen für die Umrechnung von Mittelpunktswinkeln (Zentriwinkeln) in Sehnenlängen und umgekehrt. Die Werte solcher Tabellen hängen unmittelbar mit der Sinus-Funktion zusammen: Die Länge einer Kreissehne ergibt sich aus dem Kreisradius formula_32 und dem Mittelpunktswinkel formula_3 gemäß

Ähnliche Tabellen wurden auch in der indischen Mathematik verwendet. Arabische Wissenschaftler übernahmen die Ergebnisse von Griechen und Indern und bauten die Trigonometrie, insbesondere die sphärische Trigonometrie weiter aus. Im mittelalterlichen Europa wurden die Erkenntnisse der arabischen Trigonometrie erst spät bekannt. Die erste systematische Darstellung des Gebiets erfolgte im 15. Jahrhundert. Im Zeitalter der Renaissance erforderten die zunehmenden Problemstellungen der Ballistik und der Hochseeschifffahrt eine Verbesserung der Trigonometrie und des trigonometrischen Tafelwerks. Der deutsche Astronom und Mathematiker Regiomontanus (Johann Müller) fasste Lehrsätze und Methoden der ebenen und sphärischen Trigonometrie in dem fünfbändigen Werk "De triangulis omnimodis" zusammen. Aufgrund dieser Anwendung waren außer Sinus und Kosinus auch andere Winkelfunktionen gebräuchlich, wie etwa der Sinus versus = 1 - cos.

Der Begriff Trigonometrie wurde durch Bartholomäus Pitiscus in seinem "Trigonometria: sive de solutione triangulorum tractatus brevis et perspicuus" von 1595 eingeführt.

Die heute verwendeten Schreibweisen und die analytische Darstellung der trigonometrischen Funktionen stammen zum größten Teil von Leonhard Euler.




</doc>
<doc id="11067" url="https://de.wikipedia.org/wiki?curid=11067" title="Ruhr (Begriffsklärung)">
Ruhr (Begriffsklärung)

Ruhr steht für:

Siehe auch:



</doc>
<doc id="11069" url="https://de.wikipedia.org/wiki?curid=11069" title="Kapitalismus">
Kapitalismus

Kapitalismus bezeichnet zum einen eine spezifische Wirtschafts- und Gesellschaftsordnung, zum anderen eine Epoche der Wirtschaftsgeschichte. Die zentralen Merkmale sind in Anbetracht des historischen Wandels und der zahlreichen Kapitalismusdefinitionen sowie ideologischer Unterschiede umstritten. Allgemein wird unter Kapitalismus eine Wirtschafts- und Gesellschaftsordnung verstanden, die auf Privateigentum an den Produktionsmitteln und einer Steuerung von Produktion und Konsum über den Markt beruht. Als weitere konstitutive Merkmale werden genannt: die Akkumulation, für manche das „Herzstück“, Hauptmerkmal und Leitprinzip des Kapitalismus, und das „Streben nach Gewinn im kontinuierlichen, rationalen kapitalistischen Betrieb“.

Als Epoche der Wirtschaftsgeschichte versteht man unter Kapitalismus eine wirtschaftsgeschichtliche Periode, die heute noch andauert. Sie folgte auf die Epochen des Feudalismus des europäischen Mittelalters bzw. des Merkantilismus zur Zeit des Absolutismus. In historischer Betrachtung wird dabei die Epoche des Kapitalismus in unterschiedliche Phasen oder Entwicklungsstufen eingeteilt.

Etymologisch leitet sich Kapitalismus ab von Kapital, welches sich selbst von lat. "„capitalis“" („den Kopf“ oder „das Leben betreffend“) ableitet, dieses selbst geht auf "„caput“" – „Kopf“ zurück. Ab dem 16. Jahrhundert findet sich das italienische Lehnwort "„capitale“" – „Vermögen“ im Sinne der "Kopf"zahl eines Viehbestandes, als Gegensatz zu den frisch geworfenen Tieren als „Zinsen“. Nach anderen Quellen machte schon im Lateinischen "„caput“" und "„capitalis“" einen Bedeutungswandel durch, der im deutschen durch „Haupt-“ nachvollzogen wird. "„Summa capitalis“" war die Hauptsumme in Wirtschaftsrechnungen, woraus „Kapital“ entstanden sei.

Ausgehend von diesem Wortstock werden Worte wie „Kapital“ und „kapitalistisch“ bereits im 18. und 19. Jahrhundert gebraucht, jedoch mit vagem und unspezifischem Sinn. Das Wort "„capitaliste“" ist erstmals 1753 in Frankreich belegt und meint hier "Person, die Güter besitzt". Julius von Soden verwendet in "National-Oekonomie (1805)" „kapitalistisch“, um einen „Überschuss an Genußstoff, ein[en] Vorrat“ zu bezeichnen. Theodor Mommsen verwendet „Kapital“ in seiner "Römischen Geschichte (1854–1856)".

In seinem heutigen Sinn wird es erstmals von Richard de Radonvilliers 1842 verwandt. Weitere Belege für sein Auftreten finden sich bei Pierre Leroux 1848 und im Englischen erstmals bei William Thackeray 1854. Im Englischen geht seine weitere Verwendung wesentlich von David Ricardo aus. Zur Beschreibung einer Klassengesellschaft wird er vor Marx bereits 1840 in Louis Blancs "Organisation du travail" gebraucht; bereits dort ist er negativ wertend. Karl Marx und Friedrich Engels sprechen zunächst von „kapitalistischer Produktionsweise“, später im ersten Bande von "Das Kapital (1867)" von „Kapitalist“. Das Wort „Kapitalismus“ wird dagegen nur einmal in dem 1885 von Friedrich Engels herausgegebenen zweiten Band von "Das Kapital" genannt. Häufiger findet sich das Wort "Kapitalismus" in seiner Korrespondenz und in den späteren Schriften von Friedrich Engels.

Zu Beginn des 20. Jahrhunderts häuft sich seine Verwendung und erlangt Bekanntheit insbesondere durch Werner Sombarts "Der moderne Kapitalismus (1902)" sowie durch Max Webers "Die protestantische Ethik und der Geist des Kapitalismus (1904)".

Kapitalismus ist ein Essentially Contested Concept und wird sehr unterschiedlich wahrgenommen. Bachinger/Matis unterscheiden drei verschiedene Wahrnehmungen.

In der "markteuphorischen Wahrnehmung" werden Kapitalismus und Marktwirtschaft de facto gleichgesetzt. Kapitalismus wird als entbehrlicher Begriff gesehen, der aus der "sozialistischen Mottenkiste" komme.

In der "marktkritischen Wahrnehmung" steht Kapitalismus für ein ausschließlich an einer kapitalistischen Rationalität orientiertes Denken, das auf Profit und die optimierte Verwertung der eingesetzten Produktionsmittel abzielt, ohne dabei Aspekte der Nachhaltigkeit, der Ethik und möglicher sozialer Verwerfungen zu berücksichtigen.

In der "sozialkritischen Wahrnehmung" kommt der Phase des Hochkapitalismus eine besondere Bedeutung zu. Die Durchsetzung des Kapitalismus bewirkte demnach weitreichende Verschiebungen der Wirtschafts- und Gesellschaftsstruktur und einschneidende Brüche in den Lebens- und Arbeitsbedingungen. Es sei der Antagonismus zwischen Kapitaleignern und Kapitallosen (Proletariern) entstanden. Das „industrielle Proletariat“ hätte den Unternehmen als reichliche Reservearmee zur Verfügung gestanden und sei dadurch gezwungen gewesen, niedrige Löhne und materielle Unsicherheit in Kauf zu nehmen. Bei den meisten Industriearbeitern lagen die Löhne damals knapp am Existenzminimum. Frauen und Kinder mussten daher ebenfalls arbeiten gehen, um den Lebensunterhalt der Familie zu sichern. Arbeitszeiten von 16 Stunden pro Tag waren keine Seltenheit. Das rasante Wachstum der städtischen Ballungsräume führte zu einer dramatisch niedrigen Lebenserwartung. Die Polarisierung der Gesellschaft durch die Entstehung der „sozialen Frage“ als Folge des ungehemmten „Manchesterkapitalismus“ ist der soziale und wirtschaftliche Hintergrund der wirkungsmächtigen Kapitalismusanalyse von Karl Marx.

Einige Autoren befürworten anstelle des in Deutschland als wertend verstandenen Begriffs Kapitalismus die neutralere Bezeichnung "Marktwirtschaft". Unter angelsächsischen Ökonomen ist der Gebrauch des Begriffs "capitalism" durchgängig üblich. Nach John Kenneth Galbraith wurde der Begriff "„market system“" in den USA nach dem Zweiten Weltkrieg gezielt eingeführt, da "„capitalism“" durch die Weltwirtschaftskrise in Misskredit geraten war. Einige Politiker, Journalisten und Wissenschaftler bevorzugten den Begriff "Marktwirtschaft", da die Frage des beherrschenden Einflusses von Unternehmen und allgemein die Frage wirtschaftlicher Macht so nicht thematisiert werden müsse. Lediglich als Bezeichnung für die moderne Finanzwelt habe sich der Begriff Kapitalismus halten können, da hier der Zusammenhang zwischen Vermögen und Macht besonders augenfällig sei.

Andere Autoren unterscheiden zwischen beiden Begriffen. Danach hänge das Vorliegen einer kapitalistischen Wirtschaftsordnung von den Eigentumsverhältnissen der Produktionsmittel ab, eine Marktwirtschaft zeichne sich durch die Koordination der Wirtschaftsprozesse über den Marktmechanismus aus. Beide Merkmale treten im Wirtschaftssystem der kapitalistischen Marktwirtschaft gemeinsam auf. Eine Marktwirtschaft könne theoretisch jedoch ebenso ohne Kapitalismus vorliegen (Beispiel: Sozialistische Marktwirtschaft in Jugoslawien) wie Kapitalismus ohne Marktwirtschaft (was auf die Wirtschaft im nationalsozialistischen Deutschland zutreffe). Gleichwohl treten die beiden letztgenannten Wirtschaftssysteme vergleichsweise selten auf. Mankiw und andere Autoren verstehen Kapitalismus als Marktwirtschaft mit Privateigentum an Produktionsmitteln, bezweifeln jedoch, dass Marktwirtschaft ohne Privateigentum funktionsfähig ist.

Erste für den Kapitalismus grundlegende Ideen finden sich in der spätscholastischen Schule von Salamanca und bei den Physiokraten.

Ein bedeutender Theoretiker des Kapitalismus ist der schottische Nationalökonom und Moralphilosoph Adam Smith mit seinem Hauptwerk "Der Wohlstand der Nationen (1776)". Er begründet den Eigennutz als einen wichtigen Motor für Wohlstand und gerechte Verteilung und meint in "Theorie der ethischen Gefühle (1759)", dass die Selbstregulation des Marktes durch Gleichgewichtspreise Vertrauen verdient (die „unsichtbare Hand“).

Smith beschreibt im zweiten Buch von "Der Wohlstand der Nationen", wie der Einsatz von Kapital zu einem „Ertrag oder Gewinn“ (engl. "„revenue or profit“") führen könne. Die Ansammlung von Kapital hält er für notwendig, um durch dessen Einsatz technische Neuerungen zu finanzieren.

Im vierten Buch wendet er sich gegen den vorherrschenden Merkantilismus, der Außenhandel als ein Nullsummenspiel betrachtete. Er entwickelt als Gegenmodell die Theorie vom absoluten Kostenvorteil, bei der durch Arbeitsteilung alle beteiligten Länder profitieren würden. David Ricardo führt Smiths Ideen in der Theorie vom komparativen Kostenvorteil fort.

Die wichtigsten Autoren der klassischen Nationalökonomie neben Smith sind David Ricardo, John Stuart Mill, Thomas Robert Malthus und Jean-Baptiste Say.

Kapitalismus bezeichnet in der marxistischen Tradition "„die auf Warenproduktion, Marktwirtschaft, Investition von Kapital, Lohnarbeit und Profit beruhende Produktionsweise“" als auch die "„von der Herrschaft des Kapitals bedingten sozialen, politischen, rechtlichen und kulturellen Verhältnisse als Gesellschaftsordnung“".

Marx selber hat den Begriff „Kapitalismus“ in seinen Werken selten benutzt, stattdessen spricht er von „kapitalistischer Produktionsweise“. Nach Marx wird im Kapitalismus die Produktionsweise durch das „Kapital“ bestimmt. Das Kapital kann viele Formen annehmen: Geld, Produktionsmittel, Land, Immobilien, Waren etc. Nach Marx durchläuft es typischerweise die Formen "Geld – Ware – (mehr) Geld" (als Formel: G-W-G', wobei G' einen größeren Wert darstellt als G). Kapital ist nach Marx ein „sich selbst verwertender (= sich vergrößernder) Wert“. Die Anwender des Kapitals (Kapitalisten oder Manager) produzieren Waren, die mehr wert sind als die zu ihrer Herstellung verausgabten Produktionsmittel (Arbeitskraft, Maschinen, Rohstoff). Der in der Produktionsphäre erzielte „Mehrwert“ muss sich in der „Zirkulationsphäre“ durch den Verkauf der produzierten Waren realisieren. Der Mehrwert werde ausschließlich von den Arbeitern geschaffen. Da sie seiner Meinung nach keine Gegenleistung erhalten, nennt Marx dies Ausbeutung, häufig verwendet er dafür auch den englischen Begriff „Exploitation“. Die Mehrwertproduktion setzt voraus, dass es Arbeiter gibt, die ohne Besitz von Produktionsmitteln oder alternativen Subsistenzmitteln (z. B. eigens erzeugten Lebensmittel) gezwungen sind, ihre Arbeitskraft an die kapitalistischen Produktionsmittelbesitzer zu verkaufen. Dadurch entsteht die spezifische Spaltung der Gesellschaft in „Kapitalisten“ und „Arbeiterklasse“. Allerdings ist der „Kapitalist“ nur eine Bezeichnung für eine Funktion im Produktionsprozess (Marx spricht von „Charaktermaske“). Wie er im Vorwort zum „Kapital“ ausführt, handeln die einzelnen Kapitalisten nur als „Personifikationen ökonomischer Kategorien“.

Der Kapitalismus ist nach Marx ferner charakterisiert durch die allgemeine Warenproduktion. Auch die Arbeitskraft wird als Ware gehandelt. Marx’ Hauptwerk „Das Kapital“ beginnt mit der Analyse der Ware und ihren Eigenschaften. Im „Wert“ einer Ware drückt sich die für dieses Produkt verausgabte, gesellschaftlich notwendige (Lohn-)Arbeitszeit aus. Im Wert erscheint ein gesellschaftliches Verhältnis als Eigenschaft der Ware. Das Kapitalverhältnis ist „ein durch Sachen vermitteltes gesellschaftliches Verhältnis zwischen Personen“, zwischen Kapitalisten auf der einen und Lohnarbeitern auf der anderen Seite, aber auch zwischen den Kapitalisten selbst, die ja auch untereinander Waren tauschen.

Marx und Engels beschreiben die kapitalistische Gesellschaft als eine Gesellschaft des revolutionären Umbruchs aller traditionellen Verhältnisse, in seinem Kern, dem Produktionsbereich, als eine Gesellschaft der „Ausbeutung“ und der „Entfremdung“.

In seinen Frühschriften, unter anderem in den "Ökonomisch-philosophischen Manuskripten (1844)", betont Marx den Aspekt der Entfremdung. Die Arbeiter würden dem Produkt ihrer Arbeit entfremdet, weil dieses, von den Kapitalisten angeeignet, die Form des Kapitals annehme, das die Arbeiter beherrsche. Wesentliche Potentiale und Entfaltungsmöglichkeiten des menschlichen „Gattungswesens“, das heißt der menschlichen Schaffensmöglichkeiten, würden so „pervertiert“ und durch eine subtile Form der Knechtschaft ersetzt, auch wenn diese auf einer scheinbaren, jedoch nur juristischen Freiheit beruhe. Arbeit sei im Kapitalismus nicht eine Möglichkeit der Selbstverwirklichung, sondern durch den Lohnarbeiterstatus erzwungene Arbeit.

Den ausbeuterischen Charakter der kapitalistischen Produktionsweise leitet Marx aus der Analyse der kapitalistischen „Warenform“ ab. Jede Ware habe einen Doppelcharakter und besitze sowohl Tauschwert als auch Gebrauchswert (siehe auch Warenfetischismus). Die Vermehrung des Kapitals erfolge über die Ausbeutung fremder Arbeitskraft als Lohnarbeit, wobei die Ausbeutung darin bestehe, dass der Kapitalist dem Arbeiter nicht den ganzen, vom Arbeiter geschaffenen Wert bezahle, sondern lediglich die gesellschaftlich durchschnittlichen Kosten, die der Arbeiter zur „Reproduktion seiner Arbeitskraft“ (sowie zur Aufzucht seiner Nachkommen) benötige. Den restlichen, vom Arbeiter geschaffenen „Neuwert“ streiche der Kapitalist als „Mehrwert“ ein, aus dem er seinen Profit schöpfe. Jedoch sinke die vom Kapitalisten erwirtschaftete Profitrate durch das "Gesetz des tendenziellen Falls der Profitrate" immer weiter, unter anderem aufgrund der Konkurrenz der Kapitalisten untereinander sowie durch den zunehmenden Ersatz menschlicher Arbeitskraft durch Maschinen, die nach Marx selbst keinen Mehrwert zu schöpfen imstande sind. Dieser Widerspruch zwischen sinkender Profitrate und Verwertungsbedürfnis bestimme den grundsätzlich krisenhaften Charakter der kapitalistischen Produktionsweise, die in regelmäßigen Krisen des Kapitalismus ihren Ausdruck fänden.

Laut Marx findet die Entwicklung zu marktbeherrschenden Oligopolen und Monopolen, die zu überhöhten Preisen bzw. einer Unterversorgung des Marktes führten, zwangsläufig statt. Er bezeichnet dies als die „Zentralisation“ des Kapitals.

Der grenzenlose Ausdehnungsdrang des Kapitals, der die Bourgeoisie „über die ganze Erdkugel jagt“, sei letztlich nichts als eine verzweifelte Flucht nach vorn, um den der kapitalistischen Gesellschaft systematisch inhärenten Widersprüchen durch Eroberung neuer Märkte zu entkommen. Mit dem letztlich unausweichlichen Unerträglichwerden dieser Widersprüche schlage schließlich die weltgeschichtliche Stunde der sozialistischen Revolution durch das Proletariat. Das Kapital, so Marx und Engels im "Manifest der Kommunistischen Partei (1848)", produziere seine eigenen „Totengräber“.

In marxistischer Tradition wird der Kapitalismus in die Phasen Früh- oder Übergangskapitalismus, Konkurrenzkapitalismus, Monopolkapitalismus, Imperialismus unterteilt. Nach dem Zweiten Weltkrieg spalteten sich die „Schulen“ in Staatsmonopolistischer Kapitalismus (orthodoxer Marxismus) und Spätkapitalismus (westlicher Marxismus).

Die Neoklassische Theorie hat wesentliche Grundlagen der modernen Wirtschaftswissenschaft entwickelt. Diese geht davon aus, dass die wirtschaftlichen Akteure sich rational verhalten (Modell des sog. "Homo oeconomicus") und versuchen, ihren eigenen Nutzen zu maximieren. Dieser Nutzen muss dabei nicht zwangsläufig monetärer Nutzen, also finanzieller Gewinn sein. Es kann sich ebenso gut um einen Zugewinn an emotionalem Nutzen (also Glück und Fröhlichkeit), Zugewinn an Rechten und Einfluss, an ideellem Nutzen oder Ähnliches sein. Durch diese Ausrichtung am ökonomischen Prinzip kann der Markt – unter sehr restriktiven und oft nicht realistisch vorhandenen Annahmen – für eine optimale Verteilung knapper Ressourcen sorgen. Voraussetzungen für einen Markt, der optimal funktioniert, sind beispielsweise vollständige Information, atomistische Akteure und Freiwilligkeit der Teilnahme am Markt. Sind diese Annahmen nicht erfüllt, so sagt die Neoklassische Theorie ein sogenanntes Marktversagen voraus.

Die ab 1850 in Deutschland aufkommende Historische Schule der Nationalökonomie lehnt die auf die Klassische Nationalökonomie und den Rationalismus zurückgehende Vorstellung von allgemein geltenden Wirtschaftsgesetzen ab, sondern sucht stattdessen ihre – oft auch soziologischen – Erkenntnisse durch die Herausarbeitung von historischen Entwicklungsgesetzen zu untermauern. Die allgemeinen Gesetze der Klassischen Nationalökonomie hätten nur Gültigkeit für das kapitalistische Wirtschaftssystem.

Ihre wichtigsten Vertreter sind Wilhelm Roscher, Bruno Hildebrand und Gustav von Schmoller.

Georg Friedrich Knapp unterscheidet den Kapitalismus durch das Aufkommen von Großbetrieben von früheren Wirtschaftsepochen.

Karl Bücher beschreibt in seiner klassisch gewordenen "Entstehung der Volkswirtschaft" (1917) Kapitalismus als die Wirtschaftsepoche, bei der alle ökonomischen Verhältnisse über ihre Beziehung zum Kapital definiert werden. Werner Sombart wandte sich in der zweiten Auflage von "Der moderne Kapitalismus" entschieden gegen diese Charakterisierung. Richard Passow wandte ein, dass dies dem üblichen wirtschaftswissenschaftlichen Gebrauch zuwiderlaufe.

Die sogenannte "Jüngste Historische Schule" charakterisiert den Kapitalismus über eine auftretende "kapitalistische" Gesinnung und begründete die soziologische Untersuchung des Kapitalismus.

Werner Sombart sah diese Gesinnung in Erwerbsprinzip, Rationalität und Individualismus manifestiert. Er entwarf in "Der moderne Kapitalismus (1902)" die verbreitete Einteilung des Kapitalismus in die Entwicklungsphasen Früh-, Hoch- und Spätkapitalismus. Im Spätkapitalismus sah er in den zunehmenden Staatseingriffen erste Anzeichen eines Entwicklungsgesetzes hin zur Vergesellschaftung der Produktionsmittel. Von ihm stammt der später von Joseph Schumpeter verbreitete Begriff der „schöpferischen Zerstörung“.

Max Weber versteht und erklärt den Kapitalismus als okzidentalen Rationalismus und stellt das in allen Gesellschaftsebenen umgreifende Rationalitätsstreben in den Mittelpunkt. Alle Entscheidungen im kapitalistischen System basieren auf Nutzen- bzw. Gewinnmaximierung. Dabei kann ein soziales Handeln unterstellt werden, das zweckrational orientiert ist. „Kapitalistische Wirtschaftsakte“ sind bestimmt durch „Erwartung von Gewinn durch Ausnützung von Tausch-Chancen“.

Der Staat, die Bürokratie und das Recht geben dem aufkommenden (Früh-)Kapitalismus für seine Entfaltung eine gefestigte gesellschaftliche Form. Religion in Gestalt von Kultur als "soziales Handeln" ist dabei die stärkste Macht hinsichtlich rational-methodischer Lebensführung.

Weber stellt in seinem Buch "Die protestantische Ethik und der Geist des Kapitalismus" die These auf, dass der Kapitalismus in Nordwesteuropa und den USA aus religiösen Gründen entstanden sei und eine – im geistigen Sinne – Weiterentwicklung der Reformationsbewegung darstelle (vgl. das protestantische Arbeitsethos und die protestantische Ethik allgemein). Da dies für Japan nicht haltbar war, untersuchte Weber die (funktional entsprechende) Rolle der Samurai.

Arthur Spiethoff bezog eine vermittelnde Position („anschauliche Theorie“) zwischen der historisierenden Charakterisierung des Kapitalismus in der Historischen Schule und der "reinen" Theorie der klassischen und neoklassischen Nationalökonomie.

Ende des 19. Jahrhunderts bildete sich in Wien um Carl Menger die Österreichische Schule. Diese lehnte geschichtsrelativistische und geschichtsdeterministische Kapitalismustheorien ab. Ökonomische Gesetze gelten für sie immer und überall und ergeben sich aus der Knappheit der Güter und der subjektiven Beziehung der Menschen zu jenen.

Die Österreichische Schule lehnt den "Homo oeconomicus" der Klassischen Nationalökonomie als unrealistisch ab und bezieht auch außerwirtschaftliche Ziele in ihre Theorie ein. Staatsinterventionismus in das Wirtschaftssystem wird generell abgelehnt (Ölflecktheorem).

Ludwig von Mises hielt den Kapitalismus für das einzig logisch mögliche Wirtschaftssystem. Der Sozialismus sei nicht funktionsfähig aufgrund der Unmöglichkeit der Wirtschaftsrechnung im Sozialismus. Mises schreibt: „Die Wirtschaftsforschung hat den Beweis erbracht, dass keine andere denkbare Wirtschaftsordnung den gleichen Grad von Prosperität erreichen könnte wie der Kapitalismus. Sie hat alle zugunsten von Sozialismus und Interventionismus vorgebrachten Beweisgründe völlig zu entkräften gewußt.“

Für Österreichische Ökonomen ist das Gewinnstreben der kapitalistischen Gesellschaft kein charakteristisches Merkmal, da für die Produktion zur Bedürfnisbefriedigung eine Wertsteigerung der entsprechenden Güter angestrebt werden muss, d. h. zwischen der „kapitalistischen“ Produktion für Profit und der „sozialistischen“ Produktion für Bedürfnisse gibt es keinen Unterschied. Der Unterschied bestehe nur darin, dass im Kapitalismus „Gewinn“ durch sinnvolle Kostenrechnung erst rational erzielbar wird.

Nach Mises ergibt sich der Gewinn des Unternehmers daraus, dass er die zukünftigen Bedürfnisse der Verbraucher besser vorhersieht als seine Konkurrenten und sein Kapital dementsprechend einsetzt. Zur Monopolbildung vertrat Mises, dass Monopole in einer freien Marktwirtschaft nicht entstehen können bzw. nicht von Dauer seien. Monopole entstünden immer nur durch staatliche Intervention.

Die bedeutendsten Vertreter der Österreichischen Schule sind Ludwig von Mises ("Human Action (1949)") und der Nobelpreisträger Friedrich von Hayek. Der Thatcherismus beruht in Teilen auf Hayeks Analyse ("The Road to Serfdom (1944)").

Joseph Schumpeter definierte einen funktionierenden Kapitalismus als das „liberale Modell einer interventionsfreien Wirtschaft, in der nur die Gesetze des freien Marktes gelten und in der keine monopolistischen Strukturen bestehen, denen es möglich ist, mithilfe der Staatsmacht partielle Interessen auf Kosten der Allgemeinheit durchzusetzen.“

Schumpeter urteilte, die „Maschine Kapitalismus“ funktioniere nicht schlecht. Ihr Antrieb sei das "freie Unternehmertum"; gerade der Erfolg, der sich auch in Monopolen zeige, bringe es jedoch mit sich, dass der Kapitalismus seine eigene soziale Struktur, die ihn schützt und stützt, immer wieder zerstört. Schumpeter sah zwar die Möglichkeit zur ständigen Erneuerung, ging aber in "Kapitalismus, Sozialismus und Demokratie (1942)" davon aus, dass der Kapitalismus letztendlich an seinen Erfolgen zugrunde ginge.

Er sah ihn zunächst als Motor der gesellschaftlichen Entwicklung. Jedoch produziere er zunehmend einen Wasserkopf bürokratischer Strukturen und eine „Krise des Steuerstaats“ (indem er den Staat zu schwächen unternehme). Die Automatisierung des technischen Fortschritts führe zu immer größerer Kapitalkonzentration und diese schließlich zur Aushöhlung der Vertragsfreiheit durch kollektive Absprachen.

Der Ordoliberalismus fordert eine Wirtschaftsordnung, in der ein durch den Staat geschaffener Ordnungsrahmen den ökonomischen Wettbewerb und die Freiheit der Bürger auf dem Markt gewährleisten soll. Durch die Einbettung der historisierenden Betrachtungsweise in eine allgemein geltende Ordnungstheorie erscheint für Walter Eucken der analytische Nutzen des Begriffs „Kapitalismus“ für die Wirtschaftswissenschaften zweifelhaft. Er nennt die marxistische Verwendung des Begriffs „Hypostase“ und „säkularisierte Gnosis“. Wirtschaftsordnungen bestehen vielmehr zeitlos nebeneinander zur Lösung von Knappheitsproblemen und sozialen Interessenskonflikten. "Kapitalismus" und "Sozialismus" sind demnach mit ihren historischen und wertenden Konnotationen überflüssig. Auf diese Ordnungstheorie geht die heutige Verwendung von "Marktwirtschaft" und "Zentralverwaltungswirtschaft" zurück.

Der Keynesianismus geht auf das 1937 erschienene Werk "Allgemeine Theorie der Beschäftigung, des Zinses und des Geldes" von John Maynard Keynes zurück, das den Grundstein für einen Paradigmen-Wechsel in der Nationalökonomie legte. Das Werk ist in erster Linie eine Kritik der Theorie des allgemeinen Gleichgewichts der Neoklassik und der von ihr geforderten minimalistischen Rolle des Staates im Wirtschaftsprozess. Es wird bis heute kontrovers diskutiert, unbestritten ist aber, dass Keynes sich darum verdient gemacht hat, das Denken in gesamtwirtschaftlichen Größen wie Konsum, Sparen, Investition und Einkommen auf eine neue Grundlage zu stellen.

Keynesianer halten den Kapitalismus für inhärent instabil. Schwankungen der gesamtwirtschaftlichen Endnachfrage (u. a. der Investitionen) bergen demnach die Gefahr eines anhaltenden gesamtwirtschaftlichen Ungleichgewichts mit hoher Arbeitslosigkeit, aus dem sich die Wirtschaft allein aufgrund der von den (Neo-)Klassikern betonten „Selbstheilungskräfte“ des Marktes nicht befreien könne. Keynesianer fordern daher durch konjunkturpolitische Maßnahmen das reale Wirtschaftswachstum anzukurbeln.

Im Gegensatz zu Keynesianern, die den gesamten Instrumentenkasten staatlicher Wirtschaftspolitik fordern (insbesondere eine antizyklische Finanzpolitik), setzt der Monetarismus den Schwerpunkt auf die stabilisierende Wirkungen einer mittelfristig orientierten Geldpolitik. Nach monetaristischer Analyse tendiert der Kapitalismus bei flexiblen Preisen zu einem stabilen Gleichgewicht. Es wird daher lediglich empfohlen, eine kontinuierliche trendorientierte Geldmengenpolitik zu betreiben, die für die monetäre Alimentierung des realen Wachstums sorgt. Das Ziel der Preisniveaustabilität genießt Vorrang, weil diese als Voraussetzung für das Funktionieren des marktwirtschaftlichen Anpassungsprozesses angesehen wird. Dagegen werde das Beschäftigungsziel von selbst erreicht, wenn dem freien Spiel des Marktes Raum geschaffen wird. Der Staat solle sich im Wesentlichen auf ordnungs- und wettbewerbspolitische Aufgaben beschränken. Interventionen können z. B. beim Vorliegen von externen Effekten angezeigt sein, müssen aber in jedem Einzelfall begründet werden. Milton Friedman vertritt unter Verweis auf die demokratischen Entwicklungen in Europa, Amerika und Teilen von Asien, dass kapitalistische Gesellschaften langfristig zu Rechtsstaat und Demokratie tendieren.

Libertäre und anarchokapitalistische Theoretiker verweigern den europäischen Staaten und den USA seit Mitte des 20. Jahrhunderts überhaupt die Bezeichnung „kapitalistisch“ und sehen fortschreitende sozialistische Tendenzen: Der Kapitalismus sei zugunsten eines Mischsystems aufgegeben worden; alle von Kapitalismuskritikern gerügten ökologischen und sozialen Mängel seien in Wahrheit durch staatliche Intervention entstanden und nicht das Ergebnis des freien Marktes. So auch David D. Friedman, der mit "The Machinery of Freedom (1971)" eine anarchokapitalistische Theorie entwarf.

Auf Basis neoklassischer Theorie argumentierend führt Thomas Piketty in seinem 2013 erschienenen Werk "Le capital au XXIe siecle" (deutsch "Das Kapital im 21. Jahrhundert") aus, dass Ungleichheit kein zufälliges, sondern ein notwendiges Merkmal des Kapitalismus sei. Piketty plädiert dafür, die Ungleichheit bspw. durch eine progressive Einkommensbesteuerung bzw. Vermögenssteuern zu begrenzen.

Einschlägige Lexika der Soziologie definieren den Kapitalismus als Wirtschafts- und Gesellschaftsordnung mit den Merkmalen: Güterproduktion unter Bedingungen des Privateigentums an den Produktionsmitteln, über das eine Minderheit verfügt, während die Mehrheit ein Lohnarbeitsverhältnis eingehen muss. Triebkraft der wirtschaftlichen Prozesse ist das Interesse der Produktionsmittelbesitzer an der Vermehrung des eingesetzten Kapitals, d. h. an Profitmaximierung und Akkumulation.

Max Weber betrachtete den Kapitalismus als „die schicksalsvollste Macht unseres modernen Lebens“. Keineswegs sei er gleichzusetzen mit dem „Streben nach maximalem Gewinn“, vielmehr trage er zur Bändigung des irrationalen Triebes schrankenloser Erwerbsgier bei. Ihm zufolge beruhe ein kapitalistischer Wirtschaftsakt auf der Erwartung von Gewinn durch die Wahrnehmung von friedlichen Erwerbschancen. Allerdings sei Kapitalismus identisch „mit dem Streben nach Gewinn, im kontinuierlichen, rationalen kapitalistischen Betrieb: nach immer erneutem Gewinn: nach Rentabilität.“ Weber unterscheidet zwischen "rationalem Kapitalismus", "Politischem Kapitalismus" und "traditionellem Handelskapitalismus". Der rationale Kapitalismus (= „bürgerlicher Betriebskapitalismus“), der um den modernen Typ des Marktes konzentriert ist, habe sich nur im Westen entwickelt.

Die Kritische Theorie sah in der Ideologie des gerechten Tausches die zentrale Rechtfertigungsstrategie des kapitalistischen Ausbeutungssystems. Walter Benjamin charakterisiert den Kapitalismus als eine „essentiell religiöse Erscheinung“ zur „Befriedigung derselben Sorgen, Qualen, Unruhen, auf die ehemals die sogenannten Religionen Antwort gaben“. Die Soziologen Dirk Baecker und Christoph Deutschmann haben den Benjaminschen Gedanken aufgegriffen und ihn in den Kontext der sozioökonomischen Situation der Jahrtausendwende unter den Bedingungen des globalisierten Kapitalismus ohne gesellschaftspolitische Alternative gestellt. Laut Dirk Baecker „glaubt diese Gesellschaft an den Kapitalismus“, seit „die soziologische Alternative nicht mehr verfügbar ist und damit die Form der Gesellschaft nicht mehr Gegenstand einer ideologisch begründeten politischen Entscheidung ist“. Der Sozialhistoriker Jürgen Kocka teilt die Ansicht, dass gegenwärtig „überlegene Alternativen "zum" Kapitalismus nicht erkennbar“ sind.

Laut "Duden Wirtschaft" ist „Kapitalismus“ ein unter den Produktions- und Arbeitsbedingungen des ausgehenden 18. Jahrhunderts und des beginnenden 19. Jahrhunderts geprägter Begriff. Er beschreibe eine Wirtschafts- und Gesellschaftsordnung, in der Privateigentum an Produktionsmitteln, das Prinzip der Gewinnmaximierung und Marktwirtschaft typisch sind, wobei Kapitalbesitz die Voraussetzung für die Verfügungsgewalt über die Produktionsmittel und das Weisungsrecht über die Arbeitskräfte ist. Die Arbeiter waren typischerweise besitzlos und von den wenigen Kapitalbesitzern wirtschaftlich abhängig. Die Gesellschafts- und Wirtschaftsverhältnisse der damaligen Zeit seien mit den gegenwärtigen Produktionsbedingungen nicht zu vergleichen. Seit Ende des 19. Jahrhunderts wurden die Wirtschaftsordnungen der westlichen Industrieländer durch eine große Anzahl von Sozial- und Wirtschaftsgesetzen reformiert und starke Gewerkschaften sorgten für einen Kräfteausgleich zwischen Arbeitgebern und Arbeitnehmern. Auch habe der wirtschaftlich-technische Fortschritt gerade in marktwirtschaftlichen Wirtschaftsordnungen zu erheblichen sozialen Fortschritten geführt und für große Teile der Bevölkerung seien solide Wohlstandsverhältnisse entstanden. Der Begriff Kapitalismus beschreibe deshalb die heute existierende marktwirtschaftliche Wirtschaftsordnung der westlichen Industrieländer nicht richtig.

Nach Ansicht des "Gabler Wirtschaftslexikons" ist „Kapitalismus“ eine „historisierende und, v. a. durch die Vertreter des Marxismus, wertende Bezeichnung für die neuzeitlichen kapitalistischen Marktwirtschaften mit dominierendem Privateigentum an den Produktionsmitteln und dezentraler Planung des Wirtschaftsprozesses.“ Versuche zur Periodisierung der Wirtschaftsgeschichte beruhten auf individuellen Wertungen und es werde nicht beachtet, dass es Grundprobleme des Wirtschaftens gebe, die in jeder Wirtschaftsordnung gelöst werden müssten. Der Kapitalismus werde unterschiedlich charakterisiert und die Unterteilung in unterschiedliche Phasen sei nicht einheitlich. Die prinzipiell übereinstimmende Auffassung in den einzelnen Theorien sei, dass der Kapitalismus eine Übergangserscheinung sei und sich mit systemimmanenter Zwangsläufigkeit selbst zerstöre: Die marxistische Theorie leite aus dem historischen Materialismus den Übergangscharakter des Kapitalismus ab. Demgegenüber sahen Ökonomen wie Werner Sombart und Joseph Schumpeter zunehmende Machtkonzentration, immer größer werdende Unternehmen und zunehmende Zurückdrängung der Vertragsfreiheit durch kollektive Absprachen und zunehmende Bürokratisierung und Staatseingriffe als Indizien für die zukünftige zwangsläufige Vorherrschaft des Sozialismus. Da laut Gabler jedoch wissenschaftslogisch keine zwingenden Aussagen über die zukünftige geschichtliche Entwicklung abgeleitet werden könnten, sei die im Kapitalismus-Begriff implizierte Annahme des Übergangscharakters nicht zu beweisen.

Die Vielfalt der Versuche, den Begriff „Kapitalismus“ zu definieren, bestätige nach Meinung des Handwörterbuchs der Wirtschaftswissenschaft die Meinung derer, die ihn für den wissenschaftlichen Gebrauch untauglich halten. Auch Komposita wie Früh-, Hoch-, Spät-, Staats-, Finanzkapitalismus oder viele mehr legten den Verdacht nahe, dass diese Komposita mittels völlig unterschiedlicher Merkmale definiert wurden. Anstelle der Nutzung des Begriffs Kapitalismus wird eine Schärfung und Erweiterung der Ordnungstheorie vorgeschlagen.

Für Franz Oppenheimer und seinen Schüler Ludwig Erhard war Kapitalismus ein Wirtschaftssystem, das Ungleichheit geradezu statuiere:
Für Ludwig Erhard und Alfred Müller-Armack ging es dabei um die Frage der Sozialen Gerechtigkeit, darüber hinaus sahen sie Reallohnsteigerungen in Höhe des Produktivitätsvortschritts aber auch als notwendig an, damit Angebot und Nachfrage zum Ausgleich kommen.

Thomas Piketty analysierte in seinem vieldiskutierten Buch "Le capital au XXIe siecle" (deutsch "Das Kapital im 21. Jahrhundert") 2013, dass Ungleichheit ein notwendiges Merkmal des Kapitalismus sei, wenn der Staat nicht korrigierend eingreife.

Unter angelsächsischen Ökonomen wie z. B. die Wirtschaftsnobelpreisträger Paul Krugman, Joseph E. Stiglitz etc. ist der Gebrauch des Begriffs "capitalism" durchgängig üblich als Bezeichnung der westlichen Wirtschaftssysteme.

Privateigentum und verschiedene andere Merkmale des Kapitalismus finden sich in unterschiedlich starker Ausprägung bereits ab der neolithischen Revolution. Der Autor Peter Temin vertritt die Meinung, dass bereits im Römischen Reich eine Marktwirtschaft existierte. Andere sehen im Kalifat vom 9. bis zum 12. Jahrhundert bereits wesentliche Merkmale des Kapitalismus: Geldwirtschaft, Marktwirtschaft, Frühformen der Gesellschaft ("„mufawada“" und "„mudaraba“") und Kapital "(„al-mal“)". Jürgen Kocka geht davon aus, dass mit der Existenz von individuellen Eigentumsrechten, Märkten und Kapital Frühformen des Kapitalismus im mittelalterlichen China, der arabischen Welt und im Europa der Renaissance gegeben waren.

Demgegenüber vertreten marxistische Historiker die Auffassung, dass von Kapitalismus erst mit der "verallgemeinerten" Produktion für den Markt, die sich zum ersten Mal in England ausgebreitet hat, zu sprechen ist.

In Europa entstanden der Fernhandel und mit ihm erste Institutionen, die wesentliche Merkmale des Kapitalismus trugen, wesentlich später als der florierende Fernhandel der islamischen Großkaufleute, nämlich ab dem 13. Jahrhundert. Diese Verspätung war maßgeblich bedingt durch die territoriale Zersplitterung und die kleinräumlichen Gerichtsbarkeiten. Das Aufkommen des Fernhandels in Oberitalien (Venedig, Pisa, Genua, Florenz) und in Portugal, sodann ausgeprägt im 15. Jahrhundert im Gebiet des heutigen Belgien und der Niederlande mit den Zentren Brügge und Antwerpen und noch später in den deutschen Hansestädten, war erst möglich, nachdem die Kaufleute selbst das Stadtregiment übernahmen und eine eigene Rechtsordnung mit eigenen Streitkräften aufbauten.
Der Industriekapitalismus nahm seinen Ausgang in dem am Ende des 18. Jahrhunderts entstehenden Fabriksystem, und zwar in den Baumwollspinnereien Englands. Die gleichzeitig mit der Industriellen Revolution entstandenen Fabriken konnten dank der mechanischen Spinnmaschine (Spinning Jenny) den Engpass in der Nachfrage nach zu Garn gesponnener Baumwolle beseitigen und vollends mit der weiteren Erfindung des mechanischen Webstuhls eine gewaltige Produktionssteigerung bei der Erzeugung von gewebten Textilien herbeiführen, für die es auf den Binnen- und Außenmärkten eine große Nachfrage gab. Auch in Branchen der Metallerzeugung und anderen Gewerben wurden Fabriken für eine neue Klasse „industrieller Kapitalisten“ zu Profit generierenden Anlageobjekten. In ihnen erstellten Lohnarbeiter in einer neuen, maschinenvermittelten Arbeitsteilung ein „gesellschaftliches“ Produkt.

Während Marx (wie die Klassiker) noch davon ausging, dass unter dem Regime der kapitalistischen Produktionsweise den Lohnarbeitern nur ein Lohn gezahlt würde, der zur Reproduktion ihrer Arbeitskraft erforderlich sei, zwangen die Arbeiter durch ihre kollektive Organisierung in Gewerkschaften und Arbeiterparteien die ökonomischen Eliten, sie an den erzielten Produktionsfortschritten und Wohlstandsgewinnen des sich entfaltenden Industriekapitalismus zu beteiligen. Deshalb traten prognostizierte Verelendungstendenzen der Arbeiterklasse – ebenso wie die sich zuspitzenden Klassenkämpfe mit einem proletarischen Umsturz der kapitalistischen Produktionsweise "(Weltrevolution)" – bisher nicht in dem von Marxisten erwarteten Ausmaß ein.
Stattdessen wurde der Industriekapitalismus im 19. und 20. Jahrhundert zur weltweit dominierenden Gesellschaftsformation, obwohl zeitweilig auch nichtkapitalistische Industriegesellschaften im sowjetischen Herrschaftsbereich und nach nationalen Revolutionen in ehemaligen Kolonialgebieten entstanden.

Die Ungleichheit des Reichtums und der Entwicklungschancen war das große Thema der klassischen Ökonomen Thomas Malthus, David Ricardo sowie Karl Marx, von denen jeder argumentiert hat, dass die wirtschaftliche Entwicklung letztlich die gesellschaftlichen Gegensätze verschärfen müsste. Doch haben sie den technologischen Wandel unterschätzt, der letztlich allen Schichten eine deutliche Wohlstandszunahme beschert hat.

In den vergangenen 300 Jahren ist die Weltwirtschaft inflationsbereinigt im Schnitt um 1,6 Prozent jährlich gewachsen. Die Vermögen wuchsen schneller. Historisch gesehen liegt deren Wachstumsrate eher bei vier Prozent, wenn man die Erträge vor Abzug der Steuern betrachtet. Laut Thomas Piketty war die Vermögenswachstumsrate bis zum 19. Jahrhundert in der Geschichte tatsächlich meist größer als die der Wirtschaft und damit des Gesamteinkommens und wird es seiner Ansicht nach auch im 21. Jahrhundert bleiben. Die größere Gleichheit in dieser Beziehung im 20. Jahrhundert bis nach dessen Mitte erklärt sich Piketty mit den großen politischen Umwälzungen, den Weltkriegen und den schweren Wirtschaftskrisen dieser Zeit, die den hergebrachten Vermögen deutlich zugesetzt haben.

Gegen Ende des 19. Jahrhunderts wird die Rolle von Bankiers und Financiers zunehmend bedeutender. Monopole und Kartelle häufen sich; die Unternehmenseigentümer delegieren den Produktionsprozess an Manager. Das Bankensystem, die Unternehmensverflechtungen und der Aktienmarkt werden zunehmend komplexer. In marxistischer Diktion wird diese Phase auch als Zeit des „Finanzkapitalismus“, „Monopolkapitalismus“ oder „Staatsmonopolkapitalismus“ bezeichnet. Ende des 19. und zu Beginn des 20. Jahrhunderts werden Boom und Depressionen (1857/58, 1873) zum sich häufenden Problem. Auch außerhalb der marxistischen Geschichtsdeutung wird auf die enorme Zahl von Monopolen und Trusts hingewiesen.

Schon vor dem Ersten Weltkrieg hatte sich ein Finanzkapitalismus entwickelt, der keiner Kontrolle unterlag. Dieser brach in der Weltwirtschaftskrise beginnend ab 1929 zusammen. Es entwickelte sich eine schwere weltweite Rezession zu einer Phase der Depression. Der Staat musste eingreifen und schuf Institutionen der Stabilisierung. In den USA erfolgten im Rahmen des New Deal unter Präsident Franklin D. Roosevelt bedeutende Wirtschafts- und Sozialreformen.

Auch die Soziale Marktwirtschaft war das Resultat eines gesellschaftlichen Lernprozesses, der durch die Weltwirtschaftskrise angestoßen worden war. Die ordnungspolitische Alternative hatte sich in Deutschland bereits Mitte der 1930er Jahre auf die Alternativen zwischen „gelenkter Marktwirtschaft“ ordoliberalen Typs und der „marktwirtschaftlichen Lenkungswirtschaft“ keynesianischen Typs verengt. Nach dem Zweiten Weltkrieg begründeten vor allem Ludwig Erhard und Alfred Müller-Armack in Regierungsverantwortung die Soziale Marktwirtschaft. Anstelle eines reinen bzw. ungezügelten Kapitalismus sollte staatliche Rahmensetzung das Funktionieren der Marktwirtschaft absichern. Diese Idee beruhte auf ordoliberalen Theorien. Die Soziale Marktwirtschaft sollte der Verwirklichung von sozialer Sicherheit und sozialer Gerechtigkeit dienen. Nach Erhards Vorstellung sollte eine gut funktionierende, weil gelenkte Marktwirtschaft Wohlstand für alle bringen. Eine breite Vermögensbildung aller Gesellschaftsschichten sollte als "Volkskapitalismus" gefördert werden. Seine Zielvorstellung war die Utopie einer entproletarisierten Gesellschaft von Eigentumsbürgern die staatlicher Sozialpolitik nicht mehr bedürften. In der Praxis kam der Volkskapitalismus jedoch nicht voran, es wuchs vielmehr die Einsicht in die Unzulänglichkeit der sich aus dem Marktmechanismus ergebenden Verteilung von Einkommen und Vermögen. Bereits in den 1950er Jahren war der Trend zur Ungleichverteilung von Einkommen und Vermögen mit Händen greifbar. Trotz relativ niedriger Beiträge waren die Ansprüche aus der gesetzlichen Rentenversicherung für die Altersvorsorge der Arbeitnehmer wichtiger als jede andere Einkunftsquelle und das Volumen der gesetzlichen Rentenversicherung übertraf bei weitem das Volumen der Vermögensbildung der privaten Haushalte. Die bismarcksche Sozialstaatlichkeit wurde deshalb nicht nur beibehalten, sondern ausgebaut. Die Formel Soziale Marktwirtschaft wurde seit 1957 von der Erhardschen Auslegung als Volkskapitalismus zu einer Marktwirtschaft mit eigenständiger Sozialstaatlichkeit umgedeutet. Erst dadurch wurde der Begriff Soziale Marktwirtschaft zur zentralen Konsens- und Friedensformel des mittleren Weges.

Das so entstandene deutsche Kapitalismusmodell wird auch als Rheinischer Kapitalismus bezeichnet.

Die Geschichte des Kapitalismus war stets eng mit der Internationalisierung des Handels verknüpft. Der Prozess des Abbaus von Handelsschranken (GATT 1948) und die daraus folgende internationale Verflechtung des Handels und Kapitalverkehrs, insbesondere seit Abschaffung des Bretton-Woods-Systems, werden als "Globalisierung" bezeichnet. Einige Autoren bestreiten jedoch, dass die Globalisierung im 20. Jahrhundert stärker als in früheren Epochen ist.

Die Folgen dieser Entwicklung sind umstritten: Globalisierungskritiker machen den Kapitalismus für die Fortdauer oder Verschärfung der weltweiten Kluft zwischen Arm und Reich verantwortlich. Globalisierungsbefürworter machen dagegen geltend, dass die Übernahme des westlichen Wirtschaftssystems und der Abbau von Handelsschranken die einzige Möglichkeit sei, Armut einzudämmen, und sprechen angesichts des globalen Bevölkerungswachstums von der „Unvermeidlichkeit des Kapitalismus“.

Nach dem Untergang der Sowjetunion und des Realsozialismus sprachen einige Beobachter vom "Ende der Geschichte", bei dem Kapitalismus und Demokratie als einzige Regierungs- und Wirtschaftssysteme überlebt hätten.

Nach Ende des Globalisierungsoptimismus nimmt seit einigen Jahren die Diskussion darüber an Intensität zu, ob Kapitalismus und Demokratie langfristig vereinbar sind.

Karl Marx verwendet selbst kaum den Begriff "Kapitalismus". In der marxistischen Tradition fand er indessen nicht nur eine breite Rezeption, sondern erfuhr auch eine Auffächerung in Varianten wie Organisierter Kapitalismus, Neo- und Spätkapitalismus, Finanz- und Konkurrenzkapitalismus oder auch Monopol- und Staatsmonopolistischer Kapitalismus. In den jüngeren Diskussionen der Wirtschaftswissenschaft und der Soziologie wurden weitere neue Komposita geprägt, die zum Teil große Resonanz in der Öffentlichkeit erfahren haben, wie etwa Rheinischer Kapitalismus, Kasino-Kapitalismus, Finanzmarkt-Kapitalismus und Turbokapitalismus sowie die politischen Schlagwörter wie Killerkapitalismus, Raubtierkapitalismus oder Heuschreckenkapitalismus.

Zudem wurden realsozialistische Wirtschaftssysteme innerhalb der Linken kritisch auch als Staatskapitalismus beschrieben.


Klassische Nationalökonomie

Marxismus

Historische Schule

Joseph Schumpeter

Österreichische Schule

Soziologie

Keynesianismus

Ordoliberalismus, Neoliberalismus

Einführungen
Geschichte

Politikwissenschaft

Soziologie



</doc>
<doc id="11071" url="https://de.wikipedia.org/wiki?curid=11071" title="Ruhr">
Ruhr

Die Ruhr ist ein 219,3 km langer, rechter und östlicher Nebenfluss des Rheins in Nordrhein-Westfalen (Deutschland) mit einem Einzugsgebiet von 4485 km².

Der Fluss verläuft über rund 124 km auf dem Gebiet des Regionalverbands Ruhr und ist für die größte Agglomeration Deutschlands namensgebend. Dessen Name erklärt sich daraus, dass die Industrialisierung im ausgehenden 18. Jahrhundert im Bereich des Flusses, wo die Kohle dicht unter der Erdoberfläche gelagert ist, begann.

Eine wichtige wirtschaftliche Bedeutung des Flusses liegt heute in der Trink- und Brauchwasserversorgung des Ruhrgebiets, für die der Ruhrverband zuständig ist, sowie in der Energiegewinnung. Im 19. Jahrhundert war die Ruhr zeitweilig die meistbefahrene Wasserstraße Deutschlands. Gegenwärtig findet Güterverkehr nur noch auf den letzten zwölf Flusskilometern zwischen dem Mülheimer Rhein-Ruhr-Hafen und dem Rhein statt. Das Tal der Ruhr ist ein Naherholungsgebiet für die Metropolregion Rhein-Ruhr.

Im Unterlauf beträgt die mittlere Abflussmenge der Ruhr am Pegel Mülheim 76 m³/s. Der von wasserwirtschaftlichen Maßnahmen unbeeinflusste, natürliche Abfluss beträgt an der Mündung im Mittel 81,6 m³/s; nach diesen Werten ist die Ruhr nach ihrer Wasserführung der sechstgrößte Nebenfluss des Rheins.

Die Ruhr entspringt im hochsauerländischen Teil des Rothaargebirges auf dem Nordosthang des im Naturpark Sauerland-Rothaargebirge liegenden Ruhrkopfes (). Die dortige Ruhrquelle befindet sich etwa 3 km nordöstlich von Winterberg und 1,5 km nordwestlich von dessen Stadtteil Elkeringhausen in einem morastigen Bereich auf etwa Höhe. Etwas unterhalb davon durchfließt das Quellwasser ein kleines Steinrondell ().

Oberhalb der Quelle verläuft über den Ruhrkopf die Rhein-Weser-Wasserscheide: Während die Ruhr in den westlich gelegenen Rhein mündet, fließt das Wasser der kurzen Bäche, die auf dem Südosthang des Ruhrkopfes entspringen, durch die etwas weiter östlich am Reetsberg quellende und überwiegend ostwärts verlaufende Orke und dann durch die Eder und Fulda letztlich nordostwärts in die Weser.

Die Kilometrierung der Ruhr bezieht sich weder auf die eigentliche Ruhrquelle noch auf das vorgenannte Steinrondell, sondern auf die Quelle () eines etwa 200 m langen Ruhr-Zuflusses. Sie beginnt rund 180 m ostsüdöstlich des Rondells auf Höhe, wobei eine Flusslänge von 219,3 km angegeben ist. Der Ruhrquelle entspringt allerdings nicht der längste Quellast: Sowohl der bei Ruhrkilometer 6,7 von rechts einmündende Hillebach, der 7,7 km lang ist, als auch die nach 16,8 km des Ruhrlaufes von links einmündende Neger, die 17,7 km lang ist, sind wasserreicher und länger.

Die Ruhr fließt, mit Ausnahme der ersten zwanzig Kilometer, im Wesentlichen in Ost-West-Richtung; dabei wird sie beidseitig von Gebirgszügen des Rheinischen Schiefergebirges begrenzt.

Aus dem im Naturpark Rothaargebirge liegenden Quellbereich fließt die Ruhr zunächst nach Norden entlang der Bundesstraße 480 auf den Naturpark Arnsberger Wald zu, vor dem sie bei Olsberg in Richtung Westen abbiegt. Nach Olsberg schließen sich im Norden die recht hoch aufragenden südlichen Berge des zuletzt genannten Naturparks an, der bis in das Tal des Flusses reicht. Innerhalb der Gemeindegebiete von Bestwig und Meschede fließt das Ruhrwasser weiter in Richtung Westen nach Arnsberg, dem Sitz der Bezirksregierung.

Etwas weiter nordwestlich, zwischen Ense und Wickede (Ruhr), erreicht der Fluss die Südseite des Haarstrangs, knickt bei Wickede nach Westen ab und fließt von nun an zwischen dem Haarstrang und den Höhen des Sauerlandes nach Westen, vorbei an Fröndenberg/Ruhr, Menden (Sauerland), Holzwickede, Iserlohn und Schwerte. Auf der Strecke vor dem Haarstrang hat die Ruhr den größten Teil des von ihr zu überwindenden Gesamthöhenunterschiedes bereits hinter sich. Bei Fröndenberg hat der Fluss noch eine Höhe von .
Das Gebiet Dortmunds reicht im Süden am steilen Hang des Ardey bis an die Ruhr. Die südlichen Stadtteile haben sich vielfach zu bevorzugten Wohnlagen entwickelt, so auch Syburg. Dort liegt hoch über der Ruhr jenes Bauwerk, mit dem Dortmund erste geschichtliche Erwähnung fand: die Hohensyburg. Am gegenüberliegenden Ufer, auf dem Stadtgebiet Hagens münden Lenne und Volme in die Ruhr. Mit der aus Süden kommenden Lenne nimmt die Ruhr hier ihren größten Nebenfluss auf, durch den sich ihr mittleres Abflussvolumen von 30 auf 55 Kubikmeter pro Sekunde fast verdoppelt. Kurz darauf passiert die Ruhr mit dem Hengsteysee den ersten großen Ruhrstausee, der vom Ruhrverband gebaut wurde und betrieben wird.

Herdeckes Zentrum befindet sich über einer Flussschleife, genau zwischen dem Hengstey- und dem Harkortsee, an dessen rechtem Ufer Wetter liegt. Durch ihre direkte Lage am Fluss waren Wetter und das etwas ruhrabwärts gelegene Witten wichtige Zentren der frühen Industrialisierung des Ruhrgebietes.

Am Harkortsee schlägt die Ruhr einen markanten Bogen um einen Ausläufer des Ardeygebirges, um schließlich weiter durch das südliche Ruhrgebiet zu fließen. In Bochum ragen heute weithin sichtbar die Gebäude der Ruhr-Universität auf den Höhen über dem Kemnader See empor. Nahe bei Hattingen führt seit Jahrhunderten eine Brücke über den Fluss, die lange vom Hilinciweg genutzt wurde. Erst 2002 wurde fast an derselben Stelle eine neue Brücke in Richtung Bochum errichtet.

Von Dahlhausen nach Essen-Steele wendet sich die Ruhr eine Weile nach Westen, strömt dann wieder südwestwärts um Essen-Überruhr herum durch den Baldeneysee, tangiert Essen-Werden, das mit seinem Kloster bis 1803 eine selbständige Reichsabtei war und seit dem 1. August 1929 zur Stadt Essen gehört, und wendet sich hinter dem Kettwiger See wieder nach Nordwesten. Kettwig (seit 1975 Stadtteil von Essen) hat einen historischen Fachwerkkern. Kettwigs Ruhrbrücke wurde 1282 erstmals urkundlich erwähnt. Die südlichen Stadtteile Essens liegen auf etwa 30 Kilometer Länge an der Ruhr.
Bei der historischen Furt des Hellwegs in Mülheim an der Ruhr, nahe Schloss Broich, erreicht der Fluss das Niederrheinische Tiefland und ändert seinen Lauf ein letztes Mal in der Hauptrichtung nach Westen. Mülheim ist die einzige Stadt im Ruhrgebiet, deren Innenstadt vom Fluss durchquert wird. Der Rumbach mündet als letzter nennenswerter Zufluss in unmittelbarer Nähe der Mülheimer Innenstadt von rechts. Oberhausen wird lediglich an der Südgrenze des Stadtteils Alstaden vom Fluss berührt, bevor die Ruhr Duisburg erreicht, wo sie nördlich des Stadtzentrums in die Duisburg-Ruhrorter Häfen übergeht.

Nachdem das Ruhrwasser etwa 160 Brücken unterquert hat, mündet der Fluss auf bei Duisburg-Ruhrort in den Rhein. Die Ruhrmündung liegt bei Rheinkilometer 780 und ist durch die Landmarke Rheinorange gekennzeichnet. Diese 25 m hohe Stahlbramme stammt von dem Kölner Bildhauer Lutz Fritsch und wurde 1992 in Duisburg-Neuenkamp errichtet.

Der Ruhrlauf wird (u. a.) in Obere Ruhr, Mittlere Ruhr und Untere Ruhr unterteilt, wobei es indes, anders als beim Rhein, divergente Untergliederungen gibt.

Johann Georg Kohl schrieb 1851: 

Ähnlich beschrieb es auch Hermann Adalbert Daniel 1867: .

Die Mündung der Möhne, mit der der Abfluss sich um etwa 44 % vergrößert (s. u.), leitet Kohl zufolge die mittlere Ruhr ein, die Mündung der Volme als des letzten großen Zuflusses die untere Ruhr. Die obere Ruhr wäre danach 81,9 km, die mittlere 49,8 km und die untere Ruhr 87,6 km lang.

Zu beachten ist dabei, dass die Stauseen im südlichen Ruhrgebiet im 19. Jahrhundert noch nicht existierten. Diese beginnen mit dem Hengsteysee unmittelbar unterhalb der Mündung der Lenne, welche den Abfluss der Ruhr verdoppelt, und oberhalb der Volme-Mündung.

Die naturräumliche Feingliederung des ehemaligen Instituts für Landeskunde teilte die Täler der Ruhr wie folgt auf:

Implizit definiert diese Gliederung einen Beginn der Mittelruhr mit dem Mittelruhrdurchbruch und ein Ende derselben mit der Mittelruhrsenke. Damit ergibt sich Folgendes:

Die Bezirksregierung Arnsberg, Geschäftsstelle Ruhr, teilt das Einzugsgebiet der Ruhr in Bearbeitungsgebiete auf:

Zu beachten ist indes, dass es sich in erster Linie um Namen für zu bearbeitende Teileinzugsgebiete, nicht für Flussabschnitte handelt. Die drei obigen Bearbeitungsgebiete enthalten je die Ruhrabschnitte nebst den Einzugsgebieten der kleineren und mittleren Zuflüsse, während die Einzugsgebiete der größeren Nebenflüsse Möhne und Volme eigene Bearbeitungsgebiete darstellen, das der Lenne sogar deren drei (Obere Lenne, Untere Lenne und Bigge).

Die Begriffe Ober-, Mittel- und Unterruhr werden noch in weiteren, z. T. deutlich abweichenden Zuordnungen gebraucht:

Letztlich gibt es für die Ruhr, anders als für ihren Vorfluter Rhein, keine alleinige, verbindlich anerkannte Definitionen von Ober-, Mittel- und Unterlauf.

Die folgenden Grafiken sind je in orografischer Reihenfolge von unten (Quelle) nach oben (Mündung) geordnet.

Die 13 längsten Nebenflüsse der Ruhr (Länge über 15 km) sind:

Die 12 Nebenflüsse der Ruhr mit dem größten Einzugsgebiet (mindestens 50 km²) sind:

Nachfolgend die MQ-Bilanz (mittlerer Abfluss) der 7 wasserreichsten, allein knapp 80 % (60,5 der am Pegel Mülheim rund 77 m³/s) beisteuernden Ruhr-Zuflüsse (Datengrundlage siehe hier):

Erkennbar ist insbesondere, dass die Lenne, trotz eines nur etwa zwei Drittel so großen Einzugsgebietes (1352,2 km² vs. 2088,9 km²), in etwa die gleiche Wassermenge zur Vereinigung mit der Ruhr mitbringt wie diese selber. Innerhalb von nur 5 km Fließstrecke fließt der Ruhr in Lenne und Volme mehr als die Hälfte ihres Gesamtwassers zu.

Im Oberlauf bringen auch die Neger vergleichbar viel und der Hillebach mehr Wasser mit als die Ruhr unterhalb. Für beide existieren indes keine Pegelwerte.

Nachfolgend sind alle Nebenflüsse der Ruhr mit mindestens 20 km² Einzugsgebiet, Zuflüsse aus dem Rothaargebirge ab 15 km² aufgelistet:

→ "zur kompletten Tabelle"

An der Wasserscheide des Rothaargebirges, wo die Ruhr ihren Ursprung hat, entspringen etliche Nebengewässer der oberen und mittleren Ruhr. Die Quelle der Namenlose beispielsweise liegt in nur rund 1,5 km Entfernung von der Ruhrquelle entfernt, um sich dann über die Neger unterhalb von Siedlinghausen am Rande des Rothaargebirges mit der Ruhr zu vereinen. Von der Ruhrquelle bis zum Ursprung ihres größten Nebenflusses, der Lenne, beträgt die Distanz nur 8 km. Der hohe Kamm des Rothaargebirges ist in der Erdgeschichte weiter aufgefaltet worden und bis heute erosionsbeständiger als die übrigen Bereiche des umgebenden Mittelgebirges, durch das die Ruhr und ihre Nebenflüsse das Wasser nach Süden ableiten.

Bereits nach Verlassen des Rothaargebirge bei Olsberg hat die Ruhr etwa die Hälfte des bis zum Rhein zu überwindenden Höhenunterschiedes überwunden. Dabei wird das Ruhrtal von bis zu 400 m höheren Kuppen des Mittelgebirges überragt. Die runden Wölbungen der Berge des Sauerlandes sind die nach langer Erosion entstandenen Überbleibsel eines Gebirges, das im Paläozoikum und Mesozoikum entstanden ist. Die Täler wurden hier seit dem Tertiär und Quartär ausgewaschen.

In ihrem Mittellauf im Niedersauerland durchfließt die Ruhr ein 100 bis 200 m in das Bergland eingeschnittenes Sohltal. Das mittlere Ruhrtal zeigt dabei eine mehrstufige Terrassenlandschaft, die im Verlauf der wechselnden Vereisungen während des Pleistozän ausgebildet worden ist. Während der Drenthestadien der Saaleeiszeit reichte die Vergletscherung Norddeutschlands bis an die Ruhr vor dem Nordrand des Mittelgebirges heran. Die Oberflächengestalt des mittleren und unteren Ruhrtals wurde damals vom abfließenden Schmelzwasser und von der schiebenden Kraft des Eises geformt. Die Schmelzwasser des Gletschers strömten durch das Ruhrtal nach Westen. Zeitweilig war der Abfluss durch eine Barriere aus Eismassen und Geröll beim heutigen Essen behindert, so dass ein gewaltiger eiszeitlicher See aufgestaut wurde, der das Tal noch beim heutigen Schwerte füllte. Der Ost-West-Verlauf des Ruhrtals sammelt in der Gegenwart überwiegend das vom südlich gelegenen Mittelgebirge herangeführte Wasser.

Die letzten Kilometer der Ruhr und ihre Mündung liegen im Niederrheinischen Tiefland. In der Ebene hat sich der Lauf des Flusses, wie auch der des Rheins, mit der Zeit immer wieder verlagert. Zuletzt jedoch hat der Mensch die Mündung verändert und ihre vorläufig endgültige Position bestimmt.

Von der Ruhr bekam das Ruhrgebiet seinen Namen. Die Industrialisierung der Region nahm hier ihren Anfang, da die Ruhr zu einem schiffbaren Verkehrsweg für die Ruhrschifffahrt im 18. Jahrhundert ausgebaut wurde. Die Kohleflöze traten überall in der Nähe des Flusses an die Oberfläche und fallen nach Norden hin immer tiefer ab. So wurden die ersten Zechen in der Nähe der Ruhr angelegt, und wanderten später weiter nach Norden, um in größeren Tiefen die Kohle abzubauen. So entwickelte sich das Ruhrgebiet immer weiter nördlich der Ruhr in Richtung Emscher.

Der Lauf der Ruhr und ihr Tal mit teils steilen Hängen stellen ein natürliches Hindernis dar. Aufgrund dieser landschaftlich günstigen Situation, und um die Flussübergänge der Handelsstraßen zwischen dem Bergischen Land im Süden und der Hellwegzone im Norden zu sichern, entstanden an den Hängen des Ruhrtals zahlreiche Burgen und Adelssitze. Die früheste, noch erhaltene Befestigungsanlage ist Schloss Broich, das Ende des 9. Jahrhunderts als Sperrfort gegen die Wikinger erbaut wurde. Broich gehört zu den bedeutendsten mittelalterlichen Wehrbauten der späten Karolingerzeit im Ruhrgebiet. Andere Burgen gehörten im 13. Jahrhundert zum Erzbistum Köln, zur Grafschaft Mark, zum Herzogtum Berg oder zur Grafschaft Limburg.

Heute noch genutzt oder als Ruinen erhalten sind zum Beispiel das Schloss Arnsberg, in Dortmund die Hohensyburg, in Hagen das Wasserschloss Werdringen, in Wetter die Burg Wetter und die Burg Volmarstein, in Witten das Schloss Steinhausen, die Burg Hardenstein, das Haus Witten und das Haus Herbede, in Hattingen das Haus Kemnade, die Burg Blankenstein und die Isenburg, in Essen die Burg Altendorf und die Neue Isenburg sowie in Mülheim das Schloss Styrum.

Bereits vor der industriellen Revolution wurden im Ruhrtal vor allem Mühlen, aber auch die verschiedensten Handwerke und Fertigungen unterhalten, zu deren Betrieb Wasser oder Wasserkraft notwendig waren. In Witten wurden die "Ruhrmühlen" bereits 1321 erwähnt. In Mülheim entwickelte sich um das Jahr 1650 die Lederherstellung, die auf Ihrem Höhepunkt im Jahre 1920 über 50 Betriebe umfasste. Auch in den ehemals selbständigen Ortschaften Kettwig und Werden lebte man von jeher an und mit dem Fluss. Der Deilbachhammer in Kupferdreh ist ein erhaltenes Zeugnis vorindustrieller Eisenverarbeitung an einem Nebengewässer der Ruhr. Jahrzehntelang hat die Henrichshütte im Ruhrtal Hattingens Wirtschaft dominiert. Jetzt sind die alten Hochöfen Teil des Westfälischen Industriemuseums. Die Route der Industriekultur widmet sich dem Thema der Industriegeschichte der Ruhr.

Die Übertragung des Flussnamens der Ruhr auf die Bezeichnung der Städtelandschaft des "Ruhrgebiets", die heute überwiegend nördlich des Flusses liegt, war an die historische Entwicklung der Industrialisierungsphase gekoppelt, insbesondere an die Nordwanderung des Bergbaus. Kohleabbau war zunächst vor allem im Ruhrtal leicht möglich, denn hier treten die Kohleflöze zu Tage. Auch konnte im Stollenbergbau ohne aufwändige Steilschächte der Abbau untertage vorangetrieben werden. Mülheim, Witten oder Wetter an der Ruhr waren Zentren dieser frühen Industrialisierung an der Ruhr. Entsprechend verstand man unter dem Ruhrgebiet im 19. Jahrhundert zunächst das industrialisierte Gebiet an der mittleren und unteren Ruhr.

In der zweiten Hälfte des 19. Jahrhunderts erreichte der Bergbau die Emscherregion. Die Technik war so weit fortgeschritten, dass die dort tiefer liegenden kohleführenden Schichten unter mächtigen Deckgebirgen erreicht werden konnten. Bereits im frühen 20. Jahrhundert wurde der Begriff Ruhrgebiet gelegentlich für die gesamte Industrieregion nördlich der Ruhr benutzt, die sich dabei noch in einer Phase schnellen Wachstums befand. Seit etwa 1930 wird das Ruhrgebiet als der durch Montanindustrien entstandene und geprägte Ballungsraum wahrgenommen, wie wir ihn heute kennen.

Gegenwärtig wird damit, wie selbstverständlich, die gesamte Region des vom Bergbau geprägten Gebietes zwischen Ruhr und Lippe als Ruhrgebiet definiert. Der Name ist dabei eng mit den Grenzen der "Regionalverbandes Ruhr" verknüpft, dessen Vorläuferorganisationen "Siedlungsverband Ruhrkohlenbezirk" seit 1920 und ab 1979 "Kommunalverband Ruhrgebiet" jeder auf seine Art den Namen des Flusses in der Bezeichnung führten, zunächst als Gebiet des Kohlebergbaus an der Ruhr, dann als städtisch geprägtes Gebiet an der Ruhr. Heute tritt der Flussname als eigenständiges Synonym für den Verbund der Städte und Kreise der Region hervor.

Im Zweiten Weltkrieg griffen während der Operation Chastise im Mai 1943 fünf englische Militärflugzeuge den Möhnesee an mit dem Ziel, dessen Staumauer zu zerstören und eine Flutwelle zu erzeugen. Dies gelang mit Hilfe spezieller Rollbomben; die Flutwelle richtete auf ihrem Weg durch das Möhne- und Ruhrtal bis in das rund 100 km entfernte Essen verheerende Schäden an und tötete über tausend Menschen (je nach Quelle 1300 bis 2400).

Bereits um 1033 verlieh König Konrad II. der Benediktinerabtei Werden das Regal der Schifffahrt auf der Ruhr von der Mündung bis Werden. Auch eine alte Ruhrmündung, die nahe der Duisburger Altstadt, vor dem Stapeltor, auf einen Altrheinarm traf, wurde im Mittelalter wahrscheinlich als Hafen genutzt. Bedeutung für den Transport erlangte die Ruhr im 18. Jahrhundert für die Kohle. 1770 veranlasste die preußische Regierung unter König Friedrich II. daher eine Studie über einen möglichen Ruhrausbau, der zwischen 1774 und 1780 umgesetzt wurde. 16 Schleusen, Buhnen und der heute vielfach als Rad- und Fußweg genutzte Leinpfad stammen aus dieser Zeit.

Schiffbar war die Ruhr zuvor natürlicherweise nur zwischen der Ruhrmündung bei Ruhrort und Mülheim an der Ruhr. Seit dem 14. Jahrhundert befuhren Duisburger, Ruhrorter und Mülheimer Schiffe den Unterlauf des Flusses. Nur die Zechen im Mülheimer Raum konnten von diesem preiswerten Transportweg profitieren. In Mülheim entstand die erste Kohlenniederlage des Ruhrgebiets, verbunden durch Schiebewege zu den Kleinzechen. Ab etwa 1750 wurde die Ruhr hier gezielt als Transportweg für den Absatz der Kohle ins übrige Rheinland genutzt. Die Werdener Zechen mussten ihre Kohle über Kohlfurth nach Solingen und Cronenberg schaffen.

Nach dem Ruhrausbau, also in den Jahren zwischen 1780 und 1801, fuhren Schiffe die Ruhr hinauf bis Fröndenberg-Langschede. In dem kleinen Langscheder Hafen wurden Getreide aus dem Umland und Salz aus der Saline Königsborn verladen. Der nun deutlich vereinfachte Kohlentransport ins Rheinland führte wiederum zu einer höheren Nachfrage. Die für die Ruhrschifffahrt eingesetzten Aaken zeichneten sich durch ihren geringen Tiefgang aus, bei einer Nutzlast von bis zu 175 Tonnen. Ab Mitte des 19. Jahrhunderts verlagerte sich jedoch der Transport auf die Eisenbahn, so dass die Ruhr für den Güterverkehr wirtschaftlich uninteressant und oberhalb von Mülheim eingestellt wurde.

An der Mündung in Duisburg-Ruhrort entstand mit den Duisburg-Ruhrorter Häfen der größte Binnenhafen Europas.

Von der Mündung ist der Fluss heute 12,21 km hinauf nach Mülheim an der Ruhr zur Binnenwasserstraße des Bundes (Wasserstraßenklasse Vb und Va) ausgebaut. Somit ist die Ruhr bis oberhalb der Nordbrücke Mülheim für das „Große Rheinschiff“ befahrbar. Im Mülheimer Stadtteil Speldorf liegt der 1927 eröffnete Rhein-Ruhr-Hafen. In Duisburg ist die Ruhr oberhalb der Staustufe mit dem Rhein-Herne-Kanal verbunden.

Von Mülheim weiter flussaufwärts bis Essen-Rellinghausen (km 41,40) ist die Ruhr als Landeswasserstraße für Fahrzeuge mit einem maximalen Tiefgang von 1,7 m, einer maximalen Länge von 38 m und einer maximalen Breite von 5,2 m befahrbar. Weiter flussaufwärts verkehren auf dem Kemnader See, auf der Ruhr selbst zwischen dem Kemnader See und Witten-Bommern, auf dem Harkortsee und der Ruhr in Herdecke sowie auf dem Hengsteysee Fahrgastschiffe zur Naherholung.

Das geringe Gefälle im mittleren und unteren Ruhrtal begünstigte den Eisenbahnbau. Der erste Gleisabschnitt entlang des Flusses wurde bereits 1847 mit der Prinz-Wilhelm-Eisenbahn zwischen Überruhr, Kupferdreh und Steele in Betrieb genommen. Im Laufe der folgenden Jahrzehnte wurde die Bahnstrecke zur Ruhrtalbahn komplettiert. Die Strecke hatte eine wichtige Rolle beim Kohletransport in der Industrialisierungsphase des Ruhrgebiets.

Zu Beginn führte die Untere Ruhrtalbahn flussaufwärts bis 1978 von Mülheim-Styrum nach Kettwig im Essener Südwesten.
Die Trasse wird heute zwischen dem Haltepunkt Kettwig-Stausee und Essen-Werden im Nahverkehr des Verkehrsverbundes Rhein-Ruhr von der S-Bahnlinie S6 befahren. Auch der älteste Abschnitt der Ruhrtalbahn ist nach wie vor in Betrieb, jetzt mit der S9 auf ihrem Weg zwischen Haltern und Wuppertal. Von Steele bis Hattingen verkehrt heute die S-Bahnlinie S3. Auf dem mittleren Abschnitt der Strecke gibt es heute einen mit Dampflokomotiven betriebenen Museumszugverkehr des Eisenbahnmuseums von Dahlhausen über Hattingen nach Hagen. Die Züge des Ruhr-Sieg-Express und der Ruhr-Lenne-Bahn benutzen von Wetter bis Hagen-Vorhalle ein Stück der Ruhrtalbahn auf ihrem Weg zur Lenne.
Auf der oberen Ruhrtalbahn, die von Hagen bis Olsberg im Tal der Ruhr verläuft, fahren der Sauerland-Express von Hagen in Richtung Warburg und der Dortmund-Sauerland-Express nach Winterberg.

In Herdecke überspannt ein 313 m langer, 1879 erbauter Viadukt das Ruhrtal. Auf der Strecke verkehrte zwischen Dortmund und Düsseldorf die ehemalige Rheinische Eisenbahn. Noch heute wird dieses Brückenbauwerk von der Volmetalbahn auf der Strecke zwischen Hagen und Herdecke genutzt.

Eine weitere imposante Eisenbahnbrücke überspannt in Witten das Ruhrtal. Der 1916 für die Bahnstrecke Witten–Schwelm erbaute Viadukt wird heute noch von Güterzügen zwischen Witten und Hagen befahren.

Während die Ruhrtalbahn zumeist am linken Ufer der Ruhr ihre Trasse besitzt, wird die gegenüberliegende Seite häufig für wichtige Verkehrsverbindungen genutzt. Nach der Quelle fließt das Wasser der Ruhr zunächst entlang der B 480 nach Norden, später führen zwischen Bestwig und Wickede die A 46 und A 445 zu wesentlichen Teilen am Fluss entlang. Zwischen Wetter und Witten ist es die B 226. Am Kemnader See, zwischen Bochum und Witten, verkehrt die A 43 direkt am Rande des Sees, nahe Essen-Heisingen ist es die B 227. Über die Ruhr führen über 100 Brücken. In früheren Zeiten nutzte man auch Furten, etwa die Kölner Furt bei Hattingen. Die A 52 wird über die größte Ruhrtalbrücke bei Mintard über das Ruhrtal geführt. Die B 51 ist die moderne Variante der historischen Verbindung über die Ruhrbrücke zwischen Hattingen und Bochum.

Siehe: Liste der Ruhrbrücken

Die Ruhr wird seit Beginn der Elektrifizierung als Energielieferant genutzt. So gibt es entlang des Flusses zahlreiche private und auch kommunale Wasserkraftwerke, zumeist Laufwasserkraftwerke wie in Wiemeringhausen, Olsberg, Nuttlar, Alfert, Velmede, Eversberg, Heinrichsthal, Stockhausen, Freienohl, Wildshausen, Arnsberg, Fröndenberg, Schwerte-Westhofen, Wetter und Witten. In Herdecke steht das große Pumpspeicherkraftwerk Koepchenwerk in relativer Nachbarschaft zum Laufwasserkraftwerk Hengstey. Die gesamte Ausbauleistung der Wasserkraftwerke an der Ruhr beträgt rund 85.000 kW. Des Weiteren wird das Ruhrwasser auch als Kühlwasser für mit Kohle befeuerte Dampfkraftwerke genutzt.

Infolge der Industrialisierung im 19. Jahrhundert wurde die Ruhr durch die Einleitung von Abwässern aus Industrie, Bergbau, Landwirtschaft und Haushalten stark belastet. Kurz vor dem Ersten Weltkrieg charakterisierte der Münsteraner Zoologe August Thienemann den Fluss bei extremem Niedrigwasser des Sommers 1911 so:

Bereits im Vorjahr erstellte Karl Imhoff bei der Emschergenossenschaft ein Gutachten mit dem Titel "Die Reinhaltung der Ruhr". Diese Arbeit des Pioniers der Abwasserreinigung war die Grundlage für das 1913 erlassene Ruhrreinhaltungsgesetz. Eine weitere Folge war der Aufbau des Ruhrverbandes, dessen Geschäfte Imhoff von 1922 bis zu seiner Absetzung durch das NS-Regime 1934 führte. Aufgabe des Ruhrverbandes ist bis heute die Gewässergütewirtschaft, unter anderem mit dem Ziel, die Trinkwassergewinnung im Bereich der Ruhr sicherzustellen. Durch diese Maßnahmen wurde die Ruhr, lange vor anderen Flüssen in Ballungsräumen, in einen verhältnismäßig guten biologischen Zustand versetzt.

Durch Bevölkerungsanstieg und wachsende Industrieproduktion nahm die Verschmutzung der Ruhr im Verlauf des Wirtschaftswunders der 1950er Jahre wieder zu.

Seit 2003 werden im Stadtgebiet von Arnsberg zahlreiche Maßnahmen zur Renaturierung des Flusslaufs umgesetzt. Von 2018 bis 2020 soll die Ruhr auf etwa 5 km Fließstrecke von Möhnemündung bis Haus Füchten renaturiert werden.

Die Ruhr ist ein windungsreicher Fluss, der in seinem Oberlauf im gebirgigen Hochsauerland zunächst eher als Bach erkennbar ist, durch die Zuflüsse seiner Nebengewässer schließlich zum fünftgrößten Nebenfluss des Rheins wird. Das Verhältnis von Hoch- zu Niedrigwasser läge unreguliert bei 1:500, was sich aus der fast ausschließlichen Lage des Einzugsgebiets der Ruhr im Rheinischen Schiefergebirge erklärt. Dessen Böden können nur sehr wenig Wasser aufnehmen. Für die Wasserwirtschaft an der Ruhr ist der Ruhrverband zuständig. Seine Talsperren an den Nebenflüssen der Ruhr im Sauerland dienen der Abflussregulierung sowie in geringem Maße der Trinkwassergewinnung. Die durch den Ruhrverband errichteten fünf Ruhrstauseen zwischen Dortmund und Essen sind vorrangig Teil des umfangreichen Systems zur Qualitätsverbesserung des Ruhrwassers.

Der Pegel der Ruhr wird an 23 Messpunkten durch den Ruhrverband und das Landesamt für Natur, Umwelt und Verbraucherschutz NRW ermittelt.

Aufgrund des zuvor erwähnten geomorphologischen Landschaftsbildes liegt das Einzugsgebiet der Ruhr überwiegend südlich von ihr, und die Wasserscheide zum Emscher-Flusssystem verläuft in geringer Entfernung nördlich vom Ruhrtal. Einschließlich der Nebenflüsse liegt das Einzugsgebiet der Ruhr fast vollständig im Bundesland Nordrhein-Westfalen (4.481 von 4.485 km²) und zu einem kleinen Teil in Rheinland-Pfalz. Als Teileinzugsgebiet gehört das gesamte Flusssystem der Ruhr zum Einzugsgebiet des Rheins.

Im direkten Einzugsgebiet der Ruhr leben ungefähr 2,2 Millionen Menschen, die ihr Trink- und Brauchwasser aus ihr beziehen. Das Wasser wird im Kies und Sand des Flussbetts filtriert und zur Grundwasseranreicherung eingesetzt. Neben diesem Verfahren existieren auch Brunnen zur Gewinnung von Uferfiltrat und Grundwasser ohne Anreicherung. Insgesamt wird in den Wassergewinnungsanlagen von 20 Wasserwerken gefördert. Die Gewinnung, Förderung, Aufbereitung und Bereitstellung von Trinkwasser an der Ruhr wird unter anderen durch die Unternehmen Rheinisch-Westfälische Wasserwerksgesellschaft, Gelsenwasser AG, Wasserwerke Westfalen GmbH, Wassergewinnung Essen GmbH und Wasserbeschaffung Mittlere Ruhr GmbH realisiert. 15 Unternehmen der öffentlichen Wasserversorgung an der Ruhr sind in der Arbeitsgemeinschaft der Wasserwerke an der Ruhr organisiert.

Insgesamt werden jährlich etwa 510 Millionen Kubikmeter Trinkwasser gefördert. Aus dem Flussgebiet der Ruhr wird über Rohrleitungen auch Wasser in die benachbarten Fluss-Systeme von Emscher und Lippe übergeleitet, so dass die Ruhr insgesamt rund 5,2 Millionen Menschen mit Wasser versorgt. Federführend ist der Ruhrverband.

Bereits im Gebiet um Fröndenberg wird die Ruhr stark zur Wasserversorgung genutzt. Von dort aus wird Wasser entnommen, um größere Teile des östlichen Ruhrgebiets mit Wasser zu versorgen.

Die Wasserqualität des Flusses liegt dank eines dichten Netzes von Kläranlagen, die durch den Ruhrverband gebaut und betrieben werden, heute wieder überwiegend in der Gewässergüteklasse II. Im Bereich der Quelle wird sogar die höchste Güteklasse I erreicht. Nur in wenigen Flussabschnitten wird die Güteklasse II–III bestimmt. Im Sommer 2006 wurden im Wasser der Ruhr erhöhte Konzentrationen perfluorierter Tenside (PFT) nachgewiesen. Sie stammten aus kontaminierten Düngemitteln, die auf Äckern im Einzugsgebiet des Flusses ausgebracht wurden. Die Hauptbelastung wurde über die Möhne eingetragen. Als Folge musste das Wasser für die Trinkwassergewinnung zusätzlich mit Aktivkohle gefiltert werden. Im Jahr 2010 betrug die PFT-Belastung nur noch ein Drittel der Werte von 2006. Alle gemessenen Konzentrationen liegen unterhalb jeglicher gesundheitlich bedenklicher Konzentrationen.

Im April 2008 wurde erneut die Entdeckung einer schädlichen Chemikalie im Ruhrwasser bekannt. Die Chemikalie 2,4,8,10-Tetraoxaspiro[5.5]undecan (TOSU) wurde schon seit 1996 in der Ruhr nachgewiesen. Der Emittent ist ein Arnsberger Betrieb, der vom Hochsauerlandkreis die Genehmigung zur Einleitung in die Kanalisation erhalten hatte. Die Bezirksregierung Arnsberg hat eine Verringerung der Einleitung bis zur Einhaltung des Grenzwertes angeordnet.

Das Ruhrtal hat insbesondere für das Ruhrgebiet eine wichtige Funktion als Erholungsraum. Die Ruhrufer sind weitgehend von Industrie und Bebauung verschont und von Wiesen geprägt. Entlang der Ufer verlaufen auf den alten Leinpfaden an vielen Stellen Rad- und Fußwege. Auf der Ruhr verkehren abschnittweise Ausflugsschiffe. Zu erwähnen sind in diesem Zusammenhang die Ruhrstauseen Hengsteysee, Harkortsee, Kemnader See, Baldeneysee und Kettwiger See, sowie die Stadt Mülheim an der Ruhr, deren Wasserbahnhof Ausgangspunkt der Weißen Flotte, einer Reihe von Ausflugsschiffen, ist. Auf dem Kemnader See verkehren Fahrgastschiffe zwischen dem Kemnader Wehr und der Lakebrücke an der Ruhr oberhalb des Sees in Witten-Herbede.

Auf dem Fluss und den Stauseen wird vielfältig Wasser- und anderer Sport betrieben. Zahlreiche Kanu- und Rudervereine sind an den Ufern beheimatet. Auf den Seen wird gesegelt und es können Ruder-, Paddel- oder Tretboote gemietet werden. Hinzu kommen Plätze am Ufer für Beachvolleyball oder als Ausgangspunkt für Windsurfing. Die ebenen Wege rund um die Stauseen sind insbesondere im Sommer ein beliebtes Ziel für die Inline-Skater des Ruhrgebietes. Die Ruhr ist allerdings kein Badegewässer, da weiterhin gereinigte Abwässer über die Zuflüsse und einige Kläranlagen eingeleitet werden. Dennoch baden viele Menschen in der Ruhr.

Entlang der Ruhr führt der gut 240 Kilometer lange Ruhrhöhenweg des Sauerländischen Gebirgsvereins Wanderer von der Quelle bis zur Mündung.

Die Ruhr war schon früh Naherholungsgebiet und die Höhen über dem Fluss Wohngebiet der wohlhabenden Bevölkerungsschicht. So ließ Alfred Krupp sich zum Beispiel seine "Villa Hügel" 1873 auf den Anhöhen über der Ruhr im Essener Süden errichten.

Seit April 2006 führt der gutbeschilderte Ruhrtalradweg über 220 Kilometer von der Quelle bis zur Mündung. Der ausgeschilderte Weg führt überwiegend in Flussnähe auf der Talsohle entlang. Vom Frühling bis zum Herbst setzt in Witten bei der Burgruine Hardenstein die Fähre "Hardenstein" die Radfahrer auf Zuruf über den Fluss.

In der unteren und mittleren Ruhr leben derzeit 28 verschiedene Arten von Fischen und Rundmäulern sowie eine höhere Krebsart, der Amerikanische Flusskrebs ("Orconectes limosus"). Im Oberlauf der Ruhr sind beispielsweise Bachforelle und Koppe zu finden, dann, mit dem Flusslauf, Äsche und Gründling, in der Barbenregion Barbe und Rotfeder und Hecht, im Unterlauf schließlich als Beispiele Brasse, Döbel, Schleie und Karpfen. Im Januar 2000 wurden im Fischaufstieg Mülheim-Kahlenberg Flussneunaugen ("Lampetra fluviatilis") entdeckt, eine Art, die bis dahin in der Ruhr als ausgestorben galt. Bei Wetter wurde vor einigen Jahren die Quappe ("Lota lota") wieder eingebürgert. Die im Fluss natürlicherweise zu erwartenden Langstreckenwanderer wie Lachs, Maifisch, Stör und Meerneunauge sind in der Ruhr ausgestorben, was auf die Unterbrechung der Wanderungswege durch Wehre zu erklären ist. Dies soll im Rahmen des Ruhrauenkonzeptes in den nächsten Jahren geändert werden. Durch den Bau von Fischtreppen an den anthropogenen Barrieren will man die ökologische Durchgängigkeit erreichen. Eine natürliche Reproduktion des Lachses konnte im Jahr 2008 erstmals wieder am Wehr Raffelberg bei Mülheim nachgewiesen werden. Dass sich die Lachse seitdem dort fortpflanzen, ist insbesondere der verbesserten Wasserqualität geschuldet.

Zahlreiche Naturschutz- und FFH-Gebiete im Ruhrtal bieten Tieren und Pflanzen eine gute Lebensgrundlage. An der Ruhr und ihren Stauseen brüten unter anderem Gebirgsstelze, Eisvogel, Graureiher, Wasseramsel, Stockente, Graugans, Nilgans, Kanadagans, Höckerschwan, Uferschwalbe, Kormoran, Haubentaucher, Zwergtaucher, Teichralle und Blässhuhn. Der Schwarzstorch findet sich als Nahrungsgast am Oberlauf der Ruhr ein. Verschiedene Entenarten, Lachmöwen, Gänsesäger und weitere Vogelarten sind vor allem im Winter als Rastvögel anzutreffen.





</doc>
<doc id="11072" url="https://de.wikipedia.org/wiki?curid=11072" title="Schönebeck">
Schönebeck

Schönebeck oder Schoenebeck steht für:



Orte:

Gewässer:
Schönebeck ist der Familienname folgender Personen:


von Schoenebeck ist der Familienname von:
Siehe auch:


</doc>
<doc id="11073" url="https://de.wikipedia.org/wiki?curid=11073" title="Emscher">
Emscher

Die Emscher (plattdeutsch "Iämscher") ist ein 83,1 Kilometer langer, rechter Nebenfluss des Rheins im Ruhrgebiet. Sie war mit ihrem Einzugsgebiet "Flusslandschaft des Jahres" in den Jahren 2010 und 2011.

Die Emscher entspringt südöstlich von Dortmund bei Holzwickede (Kreis Unna) am Haarstrang auf etwa in einem Quellteich. Genau genommen existieren mehrere kleinere Rinnsale, die in besagten Teich münden und hier den Ursprung bilden. Das Einzugsgebiet des Flusses beträgt mit einem System von verzweigten Nebenläufen 775,466 km².

In ihrem Oberlauf durchfließt die Emscher – nur durch den Höhenzug Haarstrang beziehungsweise das Ardeygebirge vom Ruhrtal getrennt – den Südosten von Dortmund und wendet sich dann nach Nordwesten. Im nördlichen Castrop-Rauxel unterquert sie den Rhein-Herne-Kanal in einem Durchlassbauwerk mit drei Betonröhren. Danach fließt sie bis Oberhausen fast durchgehend parallel zu diesem Kanal in westliche Richtung. Beim Bau des Kanals hat man die Geografie des Emschertals genutzt.

In Oberhausen knickt der Fluss nach Nordwesten ab und fließt dann bis zu seiner heutigen Mündung in den Rhein bei Dinslaken-Eppinghoven. Dort ist ihre Abflussmenge auf durchschnittlich 16 m³/s angewachsen.

Die Emscher fließt durch das Stadtgebiet von Dortmund, Castrop-Rauxel, Recklinghausen, Herten, Herne, Gelsenkirchen, Essen, Bottrop, Oberhausen, Duisburg und Dinslaken.

Das Tal der Emscher zwischen der Dortmunder Innenstadt und der Trennung vom bis dahin parallelen Rhein-Herne-Kanal in Oberhausen stellt die naturräumliche Untereinheit Emschertal der Haupteinheit Emscherland dar. Letztere stimmt weitgehend mit dem Einzugsgebiet der Emscher ohne Quell- und Mündungslauf überein, spart jedoch nach Süden die Quellgebiete einiger Nebenbäche auf den Castroper Platten aus, welche zum Westenhellweg gehören. Im Norden enthält das Emscherland auch zur Lippe abdachende Gebiete.

Das Emschertal gliedert sich wie folgt:

Schon vor der Saale-Vereisung hat das Emschertal existiert. Noch zu Ende des 19. Jahrhunderts mäandrierte die Emscher durch ihr zwischen Herten und Wanne-Eickel über 5 km breites Tal. Überflutungen größerer Teilgebiete waren nicht selten. In den Auen herrschten Eichen-Hainbuchen-Wälder und in sumpfigeren Gebieten Bruchwälder.

Seit Gründung der Emschergenossenschaft im Jahre 1899 sind die Emscher und ihre Nebenbäche kanalisiert und begradigt worden, nachdem es in Bergsenkungsgebieten zu Versumpfungen gekommen war. Dabei wurde auch das Flussbett mehrfach tiefer gelegt. Ferner wurde die Mündung in den Rhein zweimal nach Norden verlagert. In Teilgebieten kam es dadurch zu einer Absenkung des Grundwasserspiegels.

Da es indes bis heute immer wieder zu Absenkungen infolge des Steinkohleabbaus gekommen ist, ist die natürliche Vorflut auch weiterhin gestört, sodass größere Teilgebiete regelmäßig über Pumpstationen trocken gehalten werden müssen.

Auf dem Gebiet des eigentlichen Emschertals sinkt die Höhenlage des namensgebenden Flusses heute von 70 m in Dortmund bis auf etwa 35 m südlich Bottrops.

Nachdem die Emscher in der Witten-Hörder Mulde den Dortmunder Rücken zunächst südlich und dann westlich umflossen hat, tritt sie unmittelbar westlich der Dortmunder Innenstadt ins eigentliche Emschertal ein. Auch der Dortmunder Hafen am Dortmund-Ems-Kanal liegt, östlich der Emscher, in diesem Naturraum, den der Kanal indes bald verlässt.

Eine deutliche Talverbreiterung stellt sich in Castrop-Rauxel-Henrichenburg ein, wo die Emscher auf etwa 60 m über NN den Rhein-Herne-Kanal unterquert und ihren Verlauf von Nordwest in Südwest ändert.

Die darüber hinaus erste auffällige Talweitung findet sich unmittelbar südlich Recklinghausens, wo sich das Nebental des Hellbach bis unmittelbar vor die Innenstadt zieht. Ähnlich lang, wenngleich weniger breit, schneidet sich etwas weiter westlich und ebenfalls rechts der Emscher das Tal des Holzbaches bis Herten-Westerholt. Parallel zu beiden letztgenannten Bächen und genau zwischen ihnen verlief das Tal des heute Resser Bach genannten Bachlaufes an der Stadtgrenze zwischen Recklinghausen und Herten. Allerdings ist der Resser Bach heute auf den Holzbach umgeleitet, den er östlich von Gelsenkirchen-Resse trifft.

Westlich von Schloss Grimberg teilte sich die Emscher bis in das 19. Jahrhundert in den Hauptfluss, der nördlich um Schloss Horst floss und einen südlichen Nebenarm, Kleine Emscher genannt. Die begradigte Emscher folgt bei Gelsenkirchen-Horst diesem Nebenarm, während der frühere Hauptfluss in seinem unteren Teil als Alte Emscher bei Horst erhalten ist.

Weiter emscherabwärts findet sich noch, ebenfalls auf der rechten, nördlichen Seite, der – etwas weniger auffällige – Taleinschnitt des Lanferbaches und, ihm südlich gegenüber gelegen, der Einschnitt des Schwarzbaches – beide in Gelsenkirchen mündend.

Die nördlichen und südlichen Randplatten der Emscherniederung liegen je nur 5 m – 20 m höher als die Emscherniederung selber.

Während die südlichen Randplatten nicht durch Nebentäler unterbrochen werden, auch nicht durch das des Schwarzbaches, werden die nördlichen durch Hellbach, Resser Bach, Holzbach und Lanferbach in Einzelpatten segmentiert. Die westlichste jener fünf Platten bei Gelsenkirchen-Beckhausen geht nach Westen fließend in die "Boyeplatten" der Boye, des zweitgrößten Emscher-Nebenflusses, über.

Die Boyeplatten, auf denen ein großer Teil Bottrops liegt, nehmen mehr Fläche ein als alle anderen Randplatten zusammen. Überdies stellen sie bei Kirchhellen im Norden über eine nur 51 m hohe Niederwasserscheide einen Korridor zur Dorstener Talweitung der Lippe bei Dorsten her.

Folgende Naturräume grenzen flussabwärts an das Emschertal, das die Witten-Hörder Mulde nach Norden fortsetzt (nach Gedankenstrich je die zugehörige Haupteinheit):

Im südlichen Westen tritt die Emscher (beidseitig) in die "Ruhr-Emscher-Platte", Rechtsrheinische Niederterrassenebene, Haupteinheit Mittlere Niederrheinebene, ein, und verlässt die Westfälische Bucht.

Im Mittelalter war der Fluss im Ober- und Mittellauf in weiten Teilen natürliche Grenze von Territorien. Nördlich des Flusses lag in weiten Bereichen das Gebiet des Vest Recklinghausen, südlich das der Grafschaft Mark und des Stift Essen. Die Emscher bildete auch die Süd- und Westgrenze der Grafschaft Dortmund. Entlang der Emscher waren daher zahlreiche Wasserburgen angelegt, an den Grenzen der Grafschaft Dortmund Warten. In einer Urkunde des Erzbischof Pilgrim von Köln von 1027 für das Stift Essen wird die Emscher als "Embiscara" genannt, bis zu der, nach Angabe der Äbtissin Sophia von Essen, der Tochter Kaiser Ottos II., dem Stift der Zehnte zustünde. Die Emscher als Grenze hatte Bestand, doch wandelte sich ihr Name bis zum Spätmittelalter zu "Emschar".

Ursprünglich handelte es sich bei der Emscher um einen im Mittellauf stark mäandrierenden Fluss, die Gesamtlänge betrug 109 Kilometer. Im Mittelalter wurde an den Hängen des oberen Emschertals bei Hörde Weinbau betrieben. Heute erinnern in Hörde die Straßennamen Winzerweg und Weingartenstraße an diese Nutzung. Im weiten Tal an der mittleren Emscher und an ihrem Unterlauf in der Rheinebene erstreckten sich weite Bruchwälder. Das Bruch wurde extensiv landwirtschaftlich als Wildbahn zur Pferdezucht der Emscherbrücher genutzt. Im Tal existierten, regionaltypisch, dörfliche Streusiedlungen und Einzelhöfe, zumeist in geografisch vor Hochwasser sicher gewählter Lage. In einigem Abstand zu einander fanden sich Kirchdörfer, zum Beispiel Herne, Buer, Gelsenkirchen, Katernberg und Borbeck, entstanden auf den hochwasserfreien Terrassenkanten.

Zahlreiche Urkunden belegen, dass die Emscher wichtiger Fischgrund war: 1484 teilen die Brüder Walrave, Johan und Hinrick van Ekell die Fischerei in der "Emesscher" bei Crange, 1563 prozessiert der Adelige Goddert von Strünkede zu Strünkede gegen Philipp und Arnd von Viermund zu Bladenhorst um die Fischereigerechtsame auf der Emscher vor dem Richter in Castrop, der Prozess wird bis vor das Reichskammergericht geführt, und im April 1599 verpfänden Bürgermeister und Rat der Stadt Recklinghausen drei Adeligen Fischereirechte auf der Emscher für ein Darlehen.

Den Gewohnheiten der Zeit entsprechend wurde der Fluss in der Frühen Neuzeit in Hexenprozessen eingesetzt. Von 1589 existiert ein Protokoll des Stifts Essen über die auf eigenen Antrag an 18 der Zauberei Verdächtigten an der Emscher vorgenommene Wasserprobe, bevor man zur peinlichen Befragung überging.

Ein erstes Projekt des 18. Jahrhunderts zur Schiffbarmachung des Flusses wurde nach mehrjährigen Verhandlungen vom preußischen König Friedrich II. am 23. August 1774 abgelehnt. Auch eine Initiative unter der Führung von William Thomas Mulvany ab 1873 zum Ausbau des Flusses als Schifffahrtsweg hatte keinen direkten Erfolg. Die Idee eines Seitenkanals für die Schifffahrt wurde schließlich realisiert: Anfang des 20. Jahrhunderts wurde der Rhein-Herne-Kanal entlang der Emscher gebaut.

Mit Einsetzen der Frühindustrialisierung wurde Brauchwasser aus dem Flusssystem für Prozesse entnommen. 1793 beklagte das Kloster Sterkrade, ihm sei das Wasser verdorben, da an der St.-Antony-Hütte der Raseneisenstein gewaschen wurde. Ab der Mitte des 19. Jahrhunderts begann durch die voranschreitende Industrialisierung im Ruhrgebiet auch ein starkes Bevölkerungswachstum. Der erhöhte Trinkwasserbedarf wurde durch das Ruhr- und Lippegebiet gedeckt. Das Abwasser und das Grubenwasser der Bergwerke wurde vor allem ab der 2. Hälfte des 19. Jahrhunderts in die Emscher entlassen.

Die Emscher war durch die Einleitungen stark belastet. Das geringe Gefälle, der ausgeprägt mäandernde Flusslauf und vom Bergbau hervorgerufene Absenkungen des Bodens verursachten Ende des 19. Jahrhunderts zahlreiche Überschwemmungen, was aufgrund der mitgeführten Fäkalien zu steigender Seuchengefahr führte. Da die beteiligten Kommunen und Großbetriebe aus eigenem Antrieb nicht in der Lage waren, das Problem zu lösen, wurde schließlich 1899 die Emschergenossenschaft als Zwangsvereinigung der betroffenen Kommunen und einleitenden Großbetriebe gegründet. Ihre Aufgaben liegen in der Abwasserreinigung, der Sicherung des Abflusses, im Hochwasserschutz und in der Gewässerunterhaltung.

Unter der Ägide der Emschergenossenschaft wurde die Emscher um ca. drei Meter tiefer gelegt, größtenteils befestigt und begradigt. Mehrfach wurde der Flusslauf reguliert. Die Mündung wurde im 20. Jahrhundert zweimal verlegt: 1910 von Duisburg-Alsum nach Duisburg-Walsum und 1949 nach Dinslaken. Dementsprechend teilt sich die Emscher an ihrem Unterlauf in Alte Emscher, Kleine Emscher und Neue Emscher. Die Alte Emscher fließt von Oberhausen durch Duisburg-Hamborn, Duisburg-Beeck und Alsum, die Kleine Emscher von Oberhausen durch Hamborn und Walsum und die Neue Emscher von Oberhausen durch Dinslaken in den Rhein.

Die durch den Bergbau hervorgerufenen Bergsenkungen wurden durch immer höhere Deiche ausgeglichen, so dass die Emscher heute an einigen Stellen sogar einige Meter über dem Niveau der Umgebung liegt. Dies bedeutet jedoch auch, dass Zuflüsse zur Emscher, die das umliegende Land entwässern, nach oben in die Emscher gepumpt werden müssen. Ohne die Eindeichung und das Abpumpen des Wassers stünden große Teile der Emscherregion als Polder unter Wasser.

Bis vor kurzem gab es jedoch keine Alternativen zur offenen Abwasserentsorgung, da unterirdische Kanäle bedingt durch Bergschäden regelmäßig abgesunken wären.

Bis Ende der 1990er wurden vier zentrale Klärwerke errichtet:
Der Verlauf der Emscher dient als Trasse für verschiedene Energieleitungen. Im Bild sieht man Hochspannungsleitungen, die Steinkohlenkraftwerke (im Hintergrund das Steag-Kraftwerk in Herne) und Verbraucher verbinden. Auf dem linken Ufer verlaufen die olivgrünen Rohre der Fernwärmeschiene Ruhr, in die Kraftwerke Wasser von 110 bis 180 °C einspeisen und damit die Übergabepunkte zu den städtischen Fernwärmeversorgern im Ruhrgebiet beliefern.

Bis 1939 verkehrte eine Personenfähre in Duisburg. 1960 wurde an gleicher Stelle eine Brücke errichtet.

Nach heftigen Niederschlägen von lokal bis zu 200 l/m² kam es am 26. Juli 2008 im Stadtgebiet von Dortmund zu Überschwemmungen der Emscher und des Roßbaches, die besonders die Ortsteile Dorstfeld und Marten trafen. Die Emscher erreichte an vielen Messpunkten neue Hochwasserhöchststände. So wurden am Pegel in Mengede ein Wasserstand von über 520 Zentimetern, bei einem üblichen Pegel von etwa 100 Zentimetern, gemessen.

Um die Mitte des zwanzigsten Jahrhunderts galt die Emscher als der schmutzigste Fluss Deutschlands und die „Kloake des Ruhrgebietes“. Mit dem überwiegenden Ende des Bergbaus im Ruhrgebiet bzw. seiner Nordwanderung stellen Bergsenkungen in der Emscher-Region nun kein Hindernis mehr dar, so dass mit dem Bau von unterirdischen Kanälen und der Renaturierung der Emscher begonnen wurde.

Erste Schritte in Richtung ökologischem Umbau des Emschersystems wurde mit der IBA Emscherpark unter anderem mit den Radwegen Emscher-Weg und Emscher Park Radweg und dem Emscher Landschaftspark gelegt. In den 90er Jahren wurde bereits ein kurzer Abschnitt der Emscher im Rahmen der Bundesgartenschau in Dortmund renaturiert.

Das zentrale Bauwerk im Rahmen der Renaturierung der Emscher bildet der Emscherkanal. Am 13. August 2008 wurde der Planfeststellungsbeschluss der Bezirksregierung Münster für den Abwasserkanal entlang der Emscher an den Vorstandsvorsitzenden der Emschergenossenschaft Dr. Jochen Stemplewski überreicht. Der von Dortmund nach Dinslaken verlaufende Kanal wird 51 km lang und hat einen maximalen Durchmesser von 2,80 m. Er wird das Abwasser zu den bestehenden Kläranlagen Bottrop und Emschermündung ableiten, die bisherige offene Abwasserableitung ersetzen und bis 2017 fertiggestellt werden. Daran anschließend kann die Emscher in weiteren Bereichen naturnah umgestaltet werden.

Der an der Quelle der Emscher liegende Emscherquellhof wurde 2005 von der Emschergenossenschaft grundsaniert und wird für Ausstellungen und als Tagungs- und Bildungszentrum genutzt.

Am 18. Dezember 2009 wurde in Hörde das renaturierte, oberirdische Bett der Emscher geflutet. Nachdem die Emscher an dieser Stelle über 100 Jahre verrohrt unter der Hermannshütte geflossen ist, strömt nun sauberes Wasser durch ein naturnahes Flussbett parallel zum Phoenixsee.

Im Rahmen des Kulturhauptstadtjahres Ruhr.2010 wurde mit der Emscherkunst.2010 auf die bespielte Emscherinsel und den aktuellen Umbau des Emschersystems aufmerksam gemacht.

Die Umgestaltung des Emschertals wurde 2014 von der UNO als „Beispiel für ein partizipatives Öko-Großprojekt“ gewürdigt. Die Emschergenossenschaft lässt sich die Renaturierung des Flusses insgesamt 4,5 Mrd. Euro kosten.

Nachfolgend sind alle Nebenflüsse der Emscher ab 8 km² Einzugsgebiet aufgelistet:

Die Altarme Kleine Emscher (277134; 10,3 km; 35,0 km²) und Alte Emscher (277132, 7,8 km, 29,2 km²), die durch die Verlegung der Emschermündung nach Norden entstanden sind, fließen direkt in den Rhein und sind damit nicht mehr Teile des Flusssystems der Emscher.

Das kanalisierte Flussbett der Emscher birgt durch die abgeschrägten, sehr glatten und glitschigen Betonwände ein hohes Gefahrenpotential für Menschen, die ins Flussbett gelangen. Trotz Einzäunung und Ausschilderung hat die kanalisierte Emscher daher schon viele Menschen das Leben gekostet. So ertrank 1983 der Schriftsteller Michael Holzach in der Emscher bei dem Versuch, seinen Hund zu retten.

Folgende Institutionen, Organisationen und Einrichtungen tragen den Namen "Emscher" in ihrem Namen:




</doc>
<doc id="11075" url="https://de.wikipedia.org/wiki?curid=11075" title="Neutral">
Neutral

Neutralität oder neutral (von lateinisch "ne-utrum" „keines von beiden“) bedeutet unparteiisch, geschlechtslos, ungeladen, ausgewogen und steht für:

Neutrale im Speziellen:
Siehe auch:


</doc>
<doc id="11081" url="https://de.wikipedia.org/wiki?curid=11081" title="Wissenschaftsgeschichte">
Wissenschaftsgeschichte

Die Wissenschaftsgeschichte ist eine akademische Disziplin, die sich mit der Entstehung und Entwicklung der Wissenschaften beschäftigt. Ihr Ziel ist es, die Geschichte einzelner Fachrichtungen und der wissenschaftlichen Praktiken und Vorstellungen nachzuzeichnen. Die Erforschung und Lehre von Ideen bzw. Denkansätzen wird als Ideengeschichte bezeichnet und stellt im wissenschaftlichen Kontext einen Teilbereich der Wissenschaftsgeschichte dar. Diejenigen Wissenschaftler, die sich dieser Disziplin widmen, werden als Wissenschaftshistoriker bezeichnet und stammen oft selbst aus jener Einzelwissenschaft, die sie historisch bearbeiten. Wissenschaftsgeschichte kann sich dann als historische Reflexion der jeweiligen Fachdisziplin darstellen. Hierzu bedienen sich Wissenschaftshistoriker unter anderem der Methoden der Geschichtsforschung.

In den Gegenstandsbereich der Wissenschaftsgeschichte fallen sämtliche Teildisziplinen der Human-, Sozial-, Geistes-, Formal-, Gesellschafts-, Kultur-, Naturwissenschaften, der Technik und anderer Wissenschaften, einschließlich deren Anwendungen und Entwicklungen sowie teilweise auch Disziplinen, die nach heutigem Verständnis anders klassifiziert würden, wie beispielsweise der Künste. Häufig wird für "Wissenschaftsgeschichte" im Englischen der Terminus "history of science" gebraucht, was aber missverständlich im Sinne von Geschichte der Naturwissenschaften sein kann, da unter „science“ oftmals spezifisch die Naturwissenschaften verstanden werden. Gelegentlich werden zudem die umfassenderen Begriffe „Wissensgeschichte“ oder „history of knowledge“ verwendet.

Außer der eher „internen“ Geschichte wissenschaftlicher Praxis, Theorien und Erkenntnisse können auch umliegende Themengebiete Inhalt der Wissenschaftsgeschichte sein: z. B. Biographien ausgewählter Forscher, wissenschaftlich bedeutsame Expeditionen oder die Entwicklung wissenschaftlicher Zeitschriften, Verlage, Sammlungen oder Organisationen; eine solche eher „externe“ Wissenschaftsgeschichte befasst sich mit den Wechselwirkungen der Forschungstätigkeit mit der gesellschaftlichen Umwelt. Dazu gehört auch die Geschichte wissenschaftlicher Ausbildungsordnungen und Abschlüsse.

Eine erste Stufe der Wissenschaftsgeschichtsschreibung knüpft am Wirken einzelner Gelehrter an, im Sinne einer Darstellung berühmter Männer. Ein frühes Beispiel dafür ist die biographisch angeordnete Darstellung der Geschichte der Astronomie in Wien durch Georg Tannstetter ("Viri Mathematici", 1514). Wenn es zur Geschichtsschreibung aus einem Erfolgsbewusstsein heraus kommt, dann steht das Erreichen des gegenwärtigen Standes im Vordergrund; es geht dann darum, wer „der erste“ war, der zu einer noch heute anerkannten Sichtweise vorgestoßen war. Die Leistungen der früheren Forscher werden dann zweigeteilt, indem wiederholt gefragt wird, was sie bereits erkannt hatten und was noch nicht.

Diese einfache Betrachtungsweise dominierte bis nach 1900. Im 20. Jahrhundert kam es zu neuen Ansätzen. In den USA wurde Material aus der Geschichte der Naturwissenschaften für Fragestellungen anderer Fachrichtungen zugrundegelegt: Robert K. Merton entwickelte seit ungefähr 1940 die (externe) Wissenschaftssoziologie. Thomas S. Kuhn stützte sich vor allem auf Astronomie- und Physikgeschichte bei seinem Konzept der Struktur wissenschaftlicher Revolutionen (englisch 1962), wodurch er eine neue Sichtweise der Wissenschaftstheorie verbreitete. Die Verbindung dieser drei Disziplinen – Wissenschaftsgeschichte, -soziologie und -theorie – war dann das Anliegen des Universitätsschwerpunktes Wissenschaftsforschung in Bielefeld. Die aus dem angloamerikanischen Bereich übernommene Bezeichnung STS ("science, technology & society") soll die Breite des zu untersuchenden Gegenstandsbereichs betonen, stärker als das durch eine Bezeichnung wie "Wissenschaftsgeschichte" möglich wäre. Außerdem gewann die von Derek de Solla Price praktizierte quantitative Betrachtung der Naturwissenschaftsgeschichte (bekannt wurde sein Buch „Little Science, Big Science“, 1963) an Einfluss; es wurde die Szientometrie entwickelt.

Seit den späten 1970ern werden die praktischen Dimensionen der Wissenschaften ("practical turn") stärker beachtet, mit ihren Objekten, Repräsentationen und Instrumenten sowie mit den Interaktionen und Aushandlungsprozessen der wissenschaftlichen Institutionen und der Forschungspraxis, so z. B. durch Morris Berman.

Wissenschaftsgeschichte ist eine noch relativ junge wissenschaftliche Disziplin. Die älteste wissenschaftshistorische Fachgesellschaft der Welt ist die 1901 gegründete „Deutsche Gesellschaft für Geschichte der Medizin und der Naturwissenschaften“. Pioniere der Wissenschaftsgeschichte waren der Cambridger Mineraloge und Philosoph William Whewell (1794–1866), der französische theoretische Physiker Pierre Duhem (1861–1916) sowie der österreichische Experimentalphysiker, Sinnesphysiologe und Philosoph Ernst Mach (1838–1916), der 1895 auf einen Lehrstuhl für "Philosophie, insb. Geschichte und Theorie der induktiven Wissenschaften" an der Wiener Universität berufen wurde - einem der ersten Lehrstühle für Wissenschaftsgeschichte und -theorie weltweit. Ein weiterer Pionier war Karl Sudhoff, der das 1906 gegründete "Institut für Geschichte der Medizin und der Naturwissenschaften" aufbaute – es war das weltweit erste medizinhistorische Institut. 1907 begann er die Zeitschrift "Archiv für Geschichte der Medizin", die später nach ihm Sudhoffs Archiv genannt und auf die Naturwissenschaftsgeschichte erweitert wurde (und schließlich auf die Wissenschaftsgeschichte).

Das erste Institut für die "Geschichte der Naturwissenschaften" wurde 1943 in Frankfurt/Main eingerichtet (durch Willy Hartner); es folgten Institute in Hamburg (1960, initiiert von Bernhard Sticker und Hans Schimank) sowie München (1963, initiiert von Kurt Vogel). Weitere Institute bzw. Lehrstühle wurden in Tübingen, Stuttgart, Mainz und Berlin eingerichtet. Die geisteswissenschaftliche Methoden verwendende Erforschung der Geschichte unterscheidet sich deutlich von naturwissenschaftlicher Forschung, weshalb für die Naturwissenschaftsgeschichte (und ähnlich für Mathematik-, Medizin- und Technikgeschichte) solche Initiativen nötig waren – diese Disziplingeschichten waren in besonderem Maße von ihrer Institutionalisierung abhängig. Den Geistes- und Sozialwissenschaften liegt eine Reflexion über die Geschichte des jeweils eigenen Faches näher. 

Die "Gesellschaft für Wissenschaftsgeschichte" gibt seit 1978 die "Berichte zur Wissenschaftsgeschichte" heraus. Hier wurde gegenüber der früheren Einschränkung auf Naturwissenschaftsgeschichte ein umfassenderes Konzept von Wissenschaftsgeschichte vertreten. Diese Tendenz wurde etwa seit 1990 verstärkt sichtbar. Die Verbreiterung der Naturwissenschaftsgeschichte führt aber nicht von selbst zu vertieften Einsichten. Das Werden der Gesamtheit der Wissenschaften lässt sich von einem einzelnen Historiker kaum erfassen. Zum Erkennen von Querverbindungen und Parallelentwicklungen verschiedener Disziplinen ist die Zusammenarbeit von Disziplinhistorikern erforderlich. Wenn ein Einzelner eine Gesamtschau versucht, gerät eine solche Wissenschaftsgeschichte in die Nähe der Philosophiegeschichte. 

Mit der Gründung des Max-Planck-Instituts für Wissenschaftsgeschichte im Jahr 1994 setzte die Max-Planck-Gesellschaft einen nachhaltigen Impuls für die Forschung auf diesem Gebiet. An bundesdeutschen Hochschulen ist das Fach meist in den Bereichen Philosophie, Geschichte (etwa verbunden mit der Universitätsgeschichte) oder innerhalb der jeweiligen Disziplin (z. B. Medizingeschichte) angesiedelt. An der Universität Hamburg, der Universität Regensburg, der TU Berlin, der Friedrich-Schiller-Universität Jena und der Universität Stuttgart werden eigene Hauptfachstudiengänge angeboten. Als historisches Fach besteht hinsichtlich der Methodik ein enger Bezug zu den Geschichtswissenschaften. Gleichzeitig ist die Verankerung in der jeweiligen Fachdisziplin unabdingbar. Mit der Reform der Studiengänge im Zuge des Bologna-Prozesses schränkte sich das Angebot an rein wissenschaftshistorischen Studiengängen weiter ein. So wird lediglich an der Universität Stuttgart ein Hauptfach-Bachelor angeboten, Masterstudiengänge im Fach Wissenschaftsgeschichte gibt es nur mehr an der Friedrich-Schiller-Universität Jena und der Universität Regensburg, die aus Kapazitätsgründen aber lediglich ein Bachelor-Ergänzungsfach anbieten können. Ferner gibt es integrative Masterstudiengänge zu Wissenskulturen (so etwa in Frankfurt und Stuttgart) oder zu Digital Humanities, in denen Wissenschaftsgeschichte eine wichtige Rolle spielt. 2011 wurde nach dreijähriger Vakanz der Lehrstuhl Wissenschaftsgeschichte an der Ludwig-Maximilians-Universität München neu besetzt und ist mit einer eigenen Abteilung in die Geschichtswissenschaften integriert.

An bundesdeutschen Hochschulen gibt es inzwischen eine Reihe von Professuren mit unterschiedlichen Ausrichtungen und Schwerpunkten sowie verschiedene Graduiertenkollegs. Fachübergreifende Forschungsaktivitäten (Transdisziplinarität) werden für die Ausdifferenzierung des Faches künftig eine stärkere Bedeutung erlangen. In der deutschen Wissenschaftspolitik ist die Wissenschaftsgeschichte als Kleines Fach eingestuft.

Im englischen Sprachraum war George Sarton ein wichtiger Pionier. 1912 gründete er die Zeitschrift Isis. Seit 1955 vergibt die von Sarton und Lawrence Joseph Henderson gegründete History of Science Society (HSS) die George-Sarton-Medaille für besondere Leistungen auf dem Gebiet der Wissenschaftsgeschichte. Anlässlich des 6. Geschichtswissenschaftlichen Kongresses 1928 entstand außerdem die Académie internationale d’histoire des sciences, die die Zeitschrift "Archives internationales d'histoire des sciences" herausgibt. 1947 wurde die „International Union of the History of Science“ (IUHS) gegründet, die sich 1956 mit der „International Union for the Philosophy of Science“ (IUPS) zur „International Union of the History and Philosophy of Science“ (IUHPS) zusammenschloss. Innerhalb dieser wiederum wird die Wissenschaftsgeschichte durch die „Division of History of Science and Technology“ (DHST) repräsentiert.


Einführung in die Wissenschaftsgeschichte

Geschichte der Wissenschaftsgeschichte

Aktuelle Tendenzen der Wissenschaftsgeschichte

Practical Turn der Wissenschaftsgeschichte

Zeitschriften für das Gebiet der gesamten Wissenschaftsgeschichte



</doc>
<doc id="11083" url="https://de.wikipedia.org/wiki?curid=11083" title="Objektivität">
Objektivität

Objektivität (von lateinisch "obiectum", dem Partizip Perfekt Passiv von "obicere": das Entgegengeworfene, der Vorwurf oder der Gegenwurf) bezeichnet die Unabhängigkeit der Beurteilung oder Beschreibung einer Sache, eines Ereignisses oder eines Sachverhalts vom Beobachter beziehungsweise vom Subjekt. Die Möglichkeit eines neutralen Standpunktes, der absolute Objektivität ermöglicht, wird verneint. Objektivität ist ein Ideal der Philosophie und der Wissenschaften. Da man davon ausgeht, dass jede Sichtweise subjektiv ist, werden wissenschaftlich verwertbare Ergebnisse an bestimmten, anerkannten Methoden und Standards des Forschens gemessen.

Der Begriff der Objektivität unterliegt wie alle philosophischen Begriffe einem historisch schwankenden Sprachgebrauch. Der Sprachgebrauch „objektiv“ hat sich sogar von einer bestimmten Bedeutung in deren Gegenteil verwandelt. Als im 14. Jahrhundert bei Philosophen wie Duns Scotus und Wilhelm von Ockham die Eigenschaft „objektiv“ auftauchte, stand das »esse objektive« – im Unterschied zur modernen Auffassung – für die Beurteilung eines Gegenstandes oder Sachverhaltes, die sich aus den praktischen und kulturell erworbenen Kenntnissen eines Menschen ergab. In diesem Sinne galt »esse objektive« als gesicherte Aussage über Fakten. Dieses Erkennen wurde als „intuitive Erkenntnis“ bezeichnet. Die „intuitive Erkenntnis“ tauchte in etwas gewandelter Weise bei Edmund Husserl in seiner Idee der „Wesensschau“ wieder auf.

Der Beginn des modernen Sprachgebrauchs von „Objektivität“ wird der Zeit der Aufklärungsphilosophie zugerechnet. Mit Kants Transzendentalphilosophie wurde Objektivität zum ersten Leitprinzip der Philosophie und der Wissenschaften überhaupt. „Objektiv“ war für Kant das, was der Verstand mit Hilfe der ihm ohne jede Erfahrung, d. h. apriorisch vorhanden, bereits innewohnenden Kategorien und Begriffe nach bestimmten Methoden erkannte. So werde absolut verlässliches Wissen erworben. Mit Kants Analyse („Kritik“) der „reinen Vernunft“ wurden Philosophen und Wissenschaftlern in ihrem Wunsch bestärkt, dass objektive Erkenntnisse erreichbar seien. Bereits Platon hatte unter Voraussetzung von unveränderlichen Ideen, die jedem zugänglich seien, behauptet, dass Menschen sichere Kenntnisse über die ständig sich verändernden sinnlichen Gegenstände möglich sind. Weitestgehend unbeachtet blieben Zeitgenossen Kants wie Johann August Heinrich Ulrich und Johann Christian Lossius – die von sensualistischen Sichten ausgingen und Kants Begründung der Objektivität durch apriorische Kategorien und Begriffe nicht teilten.

Im Laufe des 19. Jahrhunderts zeigte sich, dass die Probleme mit der transzendentalphilosophischen Objektivität mit der Verbreitung materialistischer und marxistischer Philosophien, zusammen mit den Forschungsergebnissen in Physik, Physiologie und Psychologie massiv zunahmen. Die Philosophen Karl August Traugott Vogt, Jakob Moleschott, Ludwig Büchner und Heinrich Czolbe veröffentlichten ihre materialistischen Auffassungen, die objektive Erkenntnisse in Frage stellten. Aber auch Naturwissenschaftler wie Richard Avenarius mit seiner dem Kantischen Ansatz widersprechenden "Kritik der reinen Erfahrung" und Ernst Mach mit "Die Analyse der Empfindungen und das Verhältnis des Physischen zum Psychischen" verwarfen Objektivität, weil sie sich nicht aus dem sinnlich Erfahrbaren (Empirie) ableiten lasse. Husserls und Roman Ingardens Ideen für eine „starke“ Objektivität im Sinne Kants gegen diese materialistischen Ansätze erwiesen sich nicht als Ausweg.

Mit der Jahrhundertwende meldeten sich Philosophen und Wissenschaftler u. a. Friedrich Nietzsche, Fritz Mauthner, Hartwig Kuhlenbeck (1897–1984), Richard Wahle (1857–1935), Werner Heisenberg und der frühe Michel Foucault zu Wort: Sie hielten rationalistische Objektivität und objektives Wissen für unmöglich und rieten davon ab, entsprechende Ideen weiter zu verfolgen.

Sprachwissenschaftler stellten für die Gegenwart fest, dass Philosophie- und Wissenschaftshistoriker ungeachtet des Sprachgebrauchs den Begriff „Objektivität“ häufig so verwenden, als ob er keinen historischen Veränderungen unterliege. „Die begriffsgeschichtliche Wörterbucharbeit der letzten Jahrzehnte hat … das traditionelle Selbstbild des philosophischen Denkens in einem Maße erschüttert, das bis heute noch kaum wahrgenommen worden ist.“ Heute werden Begriffe nicht mehr an „reinen Ideen“ gemessen, sondern an ihrem Sprachgebrauch.

In pragmatischen, psychologischen und naturwissenschaftlichen Bereichen schließt man Objektivität inzwischen aus, ohne darauf zu verzichten, den Begriff weiterhin zu verwenden. Seine Bedeutung wird einem anderen Verständnis angepasst. Objektivität soll durch Beachtung gesellschaftlicher Übereinkünfte und Normen begründet werden. Man bezeichnet dies auch als intersubjektive Objektivität. Davon gehen auch die gegenwärtigen Diskurstheorien aus. Es wird daher gefordert, wissenschaftliche Ergebnisse und Erfahrungen von Fachleuten immer wieder zu hinterfragen. Auch neurobiologische Forschungsergebnisse lassen den Schluss zu, dass Objektivität Illusion ist. Im Gehirn des Menschen entsteht kein Abbild der Welt, wie Philosophen seit Jahrhunderten in ihren Erkenntnistheorien voraussetzten. Laien und Wissenschaftler konstruieren – durch neuronale Prozesse und Aktivitäten im Gehirn veranlasst - ihr Weltbild. Wissenschaftler haben – wie Laien auch – individuelle Sichten, die ihre Arbeitsweise und ihre Forschungsergebnisse beeinflussen. Die Fragen zu klären, wie dies geschieht und welche Folgen dies für die Wissenschaften haben kann, haben Philosophen wie z. B. George Berkeley und David Hume schon vor 300 Jahren als ihre Aufgabe angesehen und ihre Beiträge dazu veröffentlicht.

Die Brisanz des Begriffes nahm mit den Erkenntnistheorien der Aufklärungszeit zu. Einige Philosophen gingen davon aus, dass sie Fundamente für Objektivität legen können, indem sie „mentale Prozesse oder die Aktivität des Vorstellens oder Darstellens“ bei Menschen untersuchten. Andere beschränkten sich darauf festzustellen, dass Menschen aus sinnlichen Reizerlebnissen das konstituieren, was sie glauben zu wissen. Wie dies geschähe, könne im Einzelnen nicht beobachtet werden.

Spätestens das Ereignis der Reformation (1517) hatte für viele erlebbar eine bis dahin dominante Sicherheit und Einheit von Glauben und Wissen in Frage gestellt. Diese schon im 13. Jahrhundert durch Philosophen und Theologen des Klerus, sowie durch Juristen und Mediziner eingeleitete Ernüchterung hatte weitreichende Folgen. Philosophen – damals noch ein Sammelbegriff für Geistes- und Naturwissenschaftler – konnten sich für die Zuverlässigkeit ihrer Forschungsergebnisse, nicht mehr unbestritten auf die scholastische Metaphysik, die überlieferte Autorität renommierter Gelehrter bzw. auf Gott als Garanten berufen.

Francis Bacon hatte 1620 mit seinem "„Novum Organum scientiarum“", („Neues Werkzeug der Kenntnisse“), für die Wissenschaften gefordert, dass sie im Unterschied zur herkömmlichen Praxis frei von scholastisch-dogmatischen Prinzipien des Denkens bzw. der Vernunft vorangehen und dass ihre Forschungsergebnisse experimentell nachprüfbar sein sollten.

René Descartes folgte mit Ideen für wissenschaftliche Methoden ("Discours de la méthode", 1637) und deren Begründung durch seine Erkenntnistheorie ("Meditationes de prima philosophia",1641). Wenn Gelehrte sich an seinen Methoden und seiner Erkenntnistheorie orientierten, sollte Objektivität i. S. von 'so ist die Welt beschaffen' möglich sein. Descartes behauptete dazu eine grundlegende Zweiteilung der Welt in 'Etwas, das ausgedehnt ist' ("res extensa") und in 'Etwas, das denkt' ("res cogitans"). Das Denken, genauer der Verstand verarbeite Repräsentationen des 'Ausgedehnten', die ihm über die Sinne "direkt" zugänglich seien, mit Hilfe 'apriorischer Ideen' ('ideae innatae'). Diese 'apriorischen Ideen' erkenne der Mensch 'klar und deutlich'. Objektivität ergab sich so für ihn aus dem selbstgewissen Denken, bzw. aus dem Vermögen, diese 'ideae innatae' auf das Ausgedehnte anzuwenden. Philosophen entwickelten in den folgenden Jahrhunderten unter den Bezeichnungen 'Erkenntnistheorie' bzw. 'Epistemologie' variierende Antworten auf die Probleme, die der Cartesianische Vorschlag aufwarf

John Locke widerlegte in seinem Hauptwerk "Ein Versuch über den menschlichen Verstand", 1690 in London erschienen, Descartes Behauptung, dass wissenschaftliche Objektivität sich durch Denken bzw. Vernunft allein begründen ließe. Apriorische Ideen seien sowohl unerkennbar als auch für den Erwerb von Wissen unnötig. Das menschliche Bewusstsein sei bei der Geburt wie ein weißes Blatt Papier (Tabula rasa), auf das die Erfahrung erst schreibe. Ausgangspunkt jeder Erkenntnis sei die sinnliche Wahrnehmung, bzw. die Erfahrung, die auch für einfache Ideen sorge, die anlässlich sinnlicher Ereignisse abstrahiert werden. Dieses Verfahren wird auch Induktion genannt. Die Erkenntnis entstehe daher aus der Erfahrung, der Abstraktion einfacher Ideen und dem Vermögen der Vernunft, Wahrnehmungen zu Abbildern, komplexen Ideen und Begriffen zu verarbeiten. Objektivität ließ sich so nicht begründen. Wissenschaftler, so Locke, sollten stattdessen Hypothesen als Leitgedanken ihrer Forschung bilden und benutzen. Objektivität gäbe es nur in den abstrakten Wissenschaften, wie der Mathematik, wo sinnliche Phänomene keine Rolle spielten.

George Berkeley und David Hume hielten Objektivität für unerreichbar. Das was Menschen körperlich wahrnehmen ('perzipieren') und ausschließlich Gegenstand des Denkens ist, ließe sich mit der 'ausgedehnten Welt' nicht abgleichen. Von diesen beiden Aufklärern wurde kein eigentlicher, erkenntnistheoretischer Beitrag geleistet. Sie beschäftigten sich mit epistemologischen Themen. Beide verwarfen die Behauptung Lockes, dass 'einfache Vorstellungen' abstrahiert werden können, als reine Spekulation. Beide gingen davon aus, dass Menschen nur Vorstellungen ('perceptions') haben, die durch Sinnesreize und Veränderungen der Organlagen('sensations') hervorgerufen werden. Diese Vorstellungen werden nach einfachen Prinzipien der menschlichen Natur zu komplexen Vorstellungen verbunden und daraus Schlussfolgerungen (Wissen) gezogen. Diese Art 'Wissen' betrachteten sie stets als vorläufig und irrtumsträchtig. Hume empfahl daher den Gelehrten seine moderate skeptische Methode: „Ich beginne mit klaren und sich aus der Sache ergebenden Grundannahmen, gehe behutsam und jeden Schritt sichernd weiter, überdenke immer wieder meine Schlussfolgerungen und prüfe die sich daraus ergebenden Schlussfolgerungen sehr genau. … ich halte dies für die einzige Methode, durch die ich hoffen kann, Zutreffendes herauszufinden und einigermaßen dauerhafte und begründete Aussagen machen zu können.“

Die Mehrheit der deutschen Aufklärungsphilosophen behauptete, dass Objektivität durch exaktes Definieren von Begriffen aus apriorischen Ideen möglich sei. Alexander Gottlieb Baumgarten, ein Schüler Christian Wolffs schlug vor, den Begriff 'Objektivität' nicht mehr – wie Descartes - als mentale Eigenschaft des Erkennenden zu verwenden. Der Begriff 'Objektivität' sollte stattdessen als eine vom Erkennenden unabhängige Eigenschaft von Ereignissen, Aussagen oder Einstellungen sein, der mit 'Wahrheit' bedeutungsgleich wurde. Unter 'Begriffen' verstand Baumgarten Sachen, die körperlich nicht wahrnehmbar sind. Er definierte jeden Begriff und setzte definierte Begriffe in Beziehung zueinander. Auf diese Weise entstand – wie in der Mathematik – ein geschlossenes System, das widerspruchsfrei und in diesem Sinne objektiv war. Es sollte widerspruchsfreie, d. h. objektive Aussagen über Ereignisse und sinnliche Gegenstände ermöglichen. Baumgartens 'Metaphysik' wurde im 18. Jahrhundert als die am weitesten verbreitete Textgrundlage philosophischer Vorlesungen an deutschen Universitäten von Philosophieprofessoren benutzt. Kant verwendete sie fast vierzig Jahre lang als Grundlage für seine Vorlesungen über Metaphysik, Anthropologie und Religion.

Die erste deutsche Übersetzung von Humes "Enquiry of Human Understanding" erschien 1755, von Johann Georg Sulzer unter dem Titel "Philosophische Versuche über die menschliche Erkenntnis" verfasst. Immanuel Kant fühlte sich durch Hume aus seinem „dogmatischen Schlummer geweckt“ und schrieb seine "Kritik der reinen Vernunft", mit der er Grundlagen für ein objektives, wissenschaftliches Forschen aufzeigen wollte. Er begegnete damit, dem - wie er urteilte - Humeschen Skeptizismus, um diesen für alle Zeiten aus der Philosophie auszuschließen. Kant akzeptierte die sinnlichen Wahrnehmungen als Beginn allen Erkennens. Er begründete die Objektivität der Erkenntnis durch die Behauptung, dass jedem Erkennenden formale mentale Eigenschaften zur Verfügung stehen, wie die Anschauungsformen von Raum und Zeit, den Kategorien und Begriffen des Verstandes, die vor jeder Erfahrung gegeben seien und die er deshalb als 'apriorisch' charakterisierte. Da man bei diesen mentalen Eigenschaften den Eindruck habe, dass die allen Menschen zu Verfügung stehen, nannte er sie 'transzendental', d. h. "scheinbar" außerhalb des Subjektiven gültig und daher auch "scheinbar" objektiv. Er fügte eine "transzendentale Methodenlehre" hinzu, die den richtigen Gebrauch dieser Eigenschaften verbürgen sollte. Diese Methodenlehre erst begründete die allgemeine Gültigkeit der 'transzendentalphilosophischen Erkenntnisse'. Allgemeingültigkeit war schließlich - neben den "spontanen apriorischen Hervorbringungen des Verstandes" - bei Kant das ausschlaggebende Kennzeichen der objektiven Gültigkeit von Aussagen und Begriffen. Interpretatoren gingen davon aus, dass er damit 'intersubjektive Objektivität' gemeint habe.

Nach dem semiotischen Modell von Charles Sanders Peirce ist Objektivität die Zielvorstellung einer 'wahren Gesamttheorie der Realität', die nie fassbar ist, weil Menschen es immer mit 'Zeichen' zu tun haben und nicht mit der Realität. Ein Zeichen ist etwas, das für etwas anderes steht und für jemanden eine Bedeutung hat. Zeichen, bzw. Interpretationen können Menschen nicht aufheben. Sie werden spontan vom Verstand hervorgebracht, sie werden kommuniziert und bei Bedarf weiter verändert. Dies wiederholt sich endlos. Menschen brechen den prinzipiell unendlichen Interpretationsprozess ab, wenn sie handeln. Eine Gesamttheorie, bzw. Objektivität sei höchstens als gemeinsame, intersubjektive Leistung denkbar.

Für den Soziologen Max Weber, der in seinem berühmten Aufsatz von 1904 seinem eigenen Selbstverständnis nach auf Marx und Nietzsche antwortet, gibt es „keine schlechthin ‚objektive‘ wissenschaftliche Analyse des Kulturlebens oder ... der ‚sozialen Erscheinungen‘“. Erkenntnis von Kulturvorgängen geschehe in der „individuell geartete[n] Wirklichkeit des Lebens“ in Abhängigkeit von „Wertideen“ und sei „stets eine Erkenntnis unter spezifisch besonderten Gesichtspunkten“.

Popper der Begründer des Kritischen Rationalismus, verteidigte den Begriff der Objektivität. Er kritisierte zwar die klassische Sichtweise zum Begriff der Objektivität, nach der Wissen und Erkenntnis durch Begründungsmethoden seine Objektivität erhalte und die Objektivität für die Richtigkeit und Zuverlässigkeit des Wissens garantieren könne. Aber er wies darauf hin, dass Objektivität zumindest im Sinne von intersubjektiver Überprüfbarkeit möglich sei. Später erweiterte er seine Sicht und sprach sich für Objektivität im Sinn von 'so ist die Welt' aus, denn auch wenn eine Annahme nicht begründet werden könne, könne sie dennoch wahr sein und mit der Wirklichkeit übereinstimmen. Wenn sie tatsächlich wahr sei, dann könne sie nicht nur intersubjektiv überprüft werden, sondern auch ihre Konsequenzen wären objektiv zutreffend. Er übernahm Churchills Beispiel der Sonne: Man könne die zutreffende Annahme, dass sie extrem heiß und daher für Lebewesen tödlich sei, nicht nur überprüfen, sondern wer in die Sonne fliege, der erleide auch "objektiv" den Tod. Popper blieb damit im unhintergehbaren Zirkel des kulturell erworbenen Wissens und verwendete für dessen Einordnung und dem Umgang damit den Glauben an die Evolution und die Objektivität.

Habermas hält Objektivität für unmöglich. Sie sei auch nicht wünschenswert, da die Wissenschaften durch erreichte Objektivität eine „‚spezifische Lebensbedeutsamkeit‘ einbüßten“. Er setzt die Offenlegung „erkenntnisleitender Interessen“[17] an die Stelle der Objektivität. Beispielhaft vorgeführt wird das von Hans-Ulrich Wehler in der Einleitung seiner „Deutsche[n] Gesellschaftsgeschichte“.

Für Niklas Luhmann sind Objektivität und Subjektivität keine Gegensätze, sondern ähnliche Begriffe in verschiedenartigen Systemen. Objektiv ist, was sich im Kommunikationssystem (= Gesellschaft) bewährt, subjektiv ist, was sich im einzelnen Bewusstseinssystem (grob gesprochen: im Kopf eines Menschen) bewährt. Bewusstseinssysteme können dann „subjektiv das für objektiv halten, was sich in der Kommunikation bewährt, während die Kommunikation ihrerseits Nicht-Zustimmungsfähiges als subjektiv marginalisiert“.
Nach Ernst von Glasersfeld, einem Vertreter des Radikalen Konstruktivismus, ist alle Wahrnehmung und jede Erkenntnis subjektiv. Intersubjektiv wird eine Erkenntnis dann, wenn auch andere Menschen diese Erkenntnis erfolgreich anwenden. Da auch deren Erkenntnis aber subjektiv ist, wird damit keine Objektivität gewonnen, sondern eben nur Intersubjektivität. Damit ist aber auch keine Erkenntnis der Realität, 'so wie sie ist', möglich. Von Glasersfeld beansprucht daher, die in erkenntnistheoretischen Konzepten für Objektivität vorausgesetzte Trennung von Objekt und Subjekt – wie bei Descartes – überwunden zu haben.
Die Feministinnen des Poststrukturalismus Sandra Harding und Donna Haraway unterscheiden zwei Arten von Objektivität: die „schwache“ und die „strenge“. Die 'schwache Objektivität' ist die traditionelle, männlich dominierte Objektivität der Wissenschaften. Um eine 'strenge Objektivität' zu erreichen, müssten Forscher den Standpunkt ihrer eigenen sozialen Gruppenzugehörigkeit in die wissenschaftliche Arbeit bewusst mit einbeziehen. Es sei davon auszugehen, dass Gruppen, die beherrscht werden, zu besseren Objektivierungen kommen.

Allgemeine Aussagen über den gegenwärtigen Stand, bzw. Standard von Objektivität sind angesichts der Fülle von Interpretationen nur unter Vorbehalt möglich. Darauf verweisen neue Bezeichnungen, wie z. B. Objektivierung, Objektivation und deren Pluralbildungen. Es ist von 'Objektivitäten' die Rede und außerdem hat jede Wissenschaft ihre spezifischen Vorstellungen von und Umgangsweisen mit Objektivität, die ständigen Veränderungen unterworfen sind und individuell benutzt werden. Objektivität wird daneben als Eigenschaft von Einstellung bzw. Verhalten verstanden: 'objektiv' hat hier dann die Mitbedeutung von 'neutral' oder 'sachlich'. 
Objektivität ist in den vorwiegend empirisch orientierten Wissenschaften, die es auch in traditionellen Geisteswissenschaften gibt, inhaltlich und zeitlich begrenzt. Jede einzelne Wissenschaft fasst ihre Objektivität, indem sie Kriterien bestimmt, die in dieser gemeinsam akzeptiert sind. Sie sind einerseits allgemeiner Art und werden andererseits für konkrete Forschungsprojekte detailliert bestimmt. Dies trifft z. B. für Testtheorien und andere Verfahren der Datenerhebung bzw. experimentelle Vorgehensweisen in den Naturwissenschaften und Kulturwissenschaften zu. In den jeweiligen Geistes- bzw. Kulturwissenschaften werden gemeinsam akzeptierte theoretische Rahmen gesetzt, innerhalb deren längerfristig wissenschaftsspezifische 'Objektivitäten' entwickelt werden. Dies gilt z. B. für den Rahmen der Hermeneutik. Es ist außerdem feststellbar, dass in den Geistes- bzw. Kulturwissenschaften über mögliche Objektivierungen verhandelt wird. D. h. es wird daran gearbeitet subjektive Erlebnisse und Zustände zum Gegenstand objektiver Untersuchungen zu machen und so zu objektivieren.

Hans Georg Gadamer veröffentlichte im letzten Jahrhundert „Wahrheit und Methode“, einen philosophischen Beitrag, in dem er den Begriff 'Verstehen' als Grundvoraussetzung allgemein geteilter Objektivität in den Mittelpunkt der Betrachtung stellte. Dieser Ansatz fand Eingang in die Theorien der geistes- und kulturwissenschaftlichen Forschungen.

Auch Otto Friedrich Bollnow ein Zeitgenosse Gadamers hielt Hermeneutik für den Ansatz, mit dem die Geisteswissenschaften ein objektives Profil entwickeln konnten, das auch die ausgeprägte Beziehung dieser Wissenschaften zum menschlichen Leben mit einschloss. Bollnow verband dabei Objektivität mit Wahrheit und ging davon aus, dass sich in den Geisteswissenschaften Allgemeingültigkeit nicht mit gleicher Strenge wie in den Naturwissenschaften erreichen ließe.

Erreichbar aber sei

Für die Germanistik, Literaturwissenschaft und Komparatistik ergab sich daraus, das objektiv i. S. von allgemeingültig „sich auf Bedeutungen und Werte erstreckt, so … dass diese in einer gegebenen Gemeinschaft verstanden, diskutiert, angenommen oder verworfen werden können.“ Subjektivität schließt Objektivität mit ein, solange sie sich an der Sache orientiert.

Einen inhaltlich vergleichbaren und umfassenden Rahmen machte Erich Weniger für die geisteswissenschaftliche Pädagogik geltend, als er feststellte, dass Objektivität hier immer die Befangenheit bzw. den Standpunkt des Forschers deutlich macht. Erst diese Befangenheit ermöglicht wahre Objektivität.

Der Historiker Leopold von Ranke wollte „die Dinge reden lassen und sie so zeigen, wie sie waren“. Schon Jakob Burckhardt hielt die Objektivität der Geschichtswissenschaft für fragwürdig. Historiker sind sich heute darin einig, dass sie Vergangenes, nicht objektiv rekonstruieren können. Es gibt keine vereinzelten beobachtbaren Tatsachen in der Geschichtsschreibung, mit denen experimentiert werden könne. Eine empirische Geschichtswissenschaft bleibt daher eine Illusion. Dagegen setzt man hier im Rahmen der Hermeneutik auf die "Objektivität des Geschichtsforschers", die immer auch dessen jeweilige Interpretation mit einschließt. Betont wird, dass erst Quelle, Vorwissen, Interpretation zusammen ein objektives Bild ergeben.

Die relative Objektivität des hermeneutischen Rahmens 'Verstehen' bzw. 'Verständnis' rief Kritik hervor. Die Gefahr sei groß, dass die Wissenschaften sich zu „Instrumentierungen“ der Herrschaftsausübung von ausgebildeten Weltanschauungen verändern.

Der Sozialwissenschaftler Max Weber wendete sich gegen die Vermischung von Objektivität und Parteilichkeit und betonte die Pflicht zur Deutlichkeit. Von Sozialwissenschaftlern wird erwartet, dass sie nach wissenschaftlicher Integrität und Objektivität streben und sich den bestmöglichen Standards in Forschung, Lehre und sonstiger beruflicher Praxis verpflichten. Im Dienst der Objektivität sozialwissenschaftlicher Forschungen werden so u. a. Arbeitsprogramme entwickelt, die vielfältige Formen sozialen Handelns unter den Bedingungen von (aktuellen) Modernisierungsprozessen untersuchen und versuchen den (je typischen) Sinn dieser Handlungsformen zu verstehen. Dabei werden folgende Gütekriterien angewendet: "Durchführungsobjektivität", "Auswertungsobjektivität" und "Interpretationsobjektivität", die jeweils durch den Grad der Übereinstimmung von Messergebnissen und Interpretationen abgeglichen werden. Nach ähnlichen Kriterien wird forschend in der empirischen Psychologie und empirischen Pädagogik verfahren.

In der Psychologie wird die wechselhafte Natur der psychischen Phänomene unter strikten Kriterien experimenteller Situationen beobachtet, um so Objektivität im Sinne von Allgemeingültigem zu erhalten. Auf diese Weise werde die Sicht auf komplexe Zusammenhänge verstellt und das Allgemeine nur sehr eingeschränkt gültig. In der Psychiatrie gäbe es nur eine scheinbare Objektivität der "angewandten Behandlungsmethoden". Therapie sei stets ein Konstrukt für einen ganz bestimmten Patienten und trotz allen Wissens, bzw. objektiver Kriterien sei nicht zu klären, warum Patienten gesund werden.

Zurzeit ist keine Veröffentlichung bekannt, die aus naturwissenschaftlicher Perspektive die von Philosophen behauptete Bedeutung von 'Verstehen' im Zusammenhang mit naturwissenschaftlichen Diskursen thematisierte. Wissenschaftstheoretische Hinweise auf 'tief sitzende Denkstile eines bestimmten Denkkollektivs' – wie z. B. Ludwik Fleck sie lieferte – scheinen bisher überwiegend verhallt. Wissenschaftliche Erkenntnisse der Physik und Chemie beziehen sich stets auf Experimente. Hier ein Beispiel aus der Chemie: „Der Wissenschaftler macht seine Experimente … Er tritt unvoreingenommen an die Natur heran (blank page) und fasst die Ergebnisse der Experimente zu Gesetzen zusammen. Ein Beispiel wäre die chemische Synthese. Es werden mehrere chemische Verbindungen Syntheseschritten unterworfen, und das Ergebnis der Synthese wird mit einem analytischen Gerät (z. B. Kernresonanzspektrometer, NMR) untersucht. Man erhält ein bestimmtes Signal. Die Synthese wird mehrfach wiederholt, und man erhält jedes Mal das gleiche Spektrum. Eine derartige wiederholte Wahrnehmung wird Beobachtung genannt. Die Beobachtung wird dann als allgemeingültiger Satz formuliert: ‚Wenn Substanz A und Substanz B unter den Bedingungen XY zusammengegeben werden, entsteht C‘. Dieser Satz gilt … für alle späteren möglichen Experimente unter entsprechenden Bedingungen.“

Die so gewonnenen empirischen Daten werden ausgewertet und auf allgemein beschreibbare Vorgänge untersucht. Die quantitativen Messergebnisse werden nach mathematischen Zusammenhängen der gemessenen Größen bewertet. Die Mathematik gilt als das wichtigste Instrument zur Beschreibung der Natur und ist Bestandteil der meisten Theorien. Das Quantitative wird von Naturwissenschaftlern als Begriffsform verwendet; sie ist eine Methode, die auf Messung und Formalisierung des Beobachteten beruht. Naturwissenschaftler gehen mehrheitlich davon aus, dass die von ihnen verwendeten 'Begriffe und Gesetze' der Interpretation ihrer Arbeitsergebnisse „naturgegebenen Bestandteilen unserer Welt“, entsprechen.

Wenn experimentelle Ergebnisse – unabhängig überprüft – sich bestätigen, so ist ihre Objektivität bewiesen. Darüber hinaus werden philosophische Konzepte empfohlen, die Forscher als Rahmen für ihre naturwissenschaftlichen Forschungen nützen können, wenn sie ihre Aussagen bewerten, einordnen bzw. zu Theorien ausarbeiten möchten. Popper, Kuhn, Feyerabend und Lakatos werden als mögliche Ideengeber genannt.

Der Biologe Jakob Johann von Uexküll schloss aus seinen Forschungen über die ausschließlich subjektiven Umwelten von Tieren und Menschen, dass Objektivität nichts als eine "Denkbequemlichkeit" sei. Die "objektiven Naturgesetze" charakterisierte er als "konventionelle Objektivität", als Vereinbarungen von Wissenschaftlern. Die allen Menschen gemeinsame biologische Ausstattung, ähnliche Empfindungen und die Gewohnheit, es gäbe "Objektivität", veranlassten Wissenschaftler Objektives zu behaupten. „Der Versuch, eine ... absolute objektive Welt in der Vorstellung zu erbauen, hat sich totgelaufen.“
Die Aussage: „Alle wissenschaftlichen Erkenntnisse beruhen nur auf Glauben.“ erregte vor einigen Jahren Aufsehen in der Presse, als zwei physikalisch qualifizierte Fachjournalisten das Buch "Was zu bezweifeln war" veröffentlichten. Philipp Frank – ein österreichischer Philosoph, Mathematiker und Physiker – benannte ähnliches: Das, was Physiker experimentell wahrnehmen, entspricht, so Frank, keiner außerhalb ihrer Wahrnehmungen existierenden Wirklichkeit. Die ältere Behauptung Werner Heisenbergs von 1930, dass der Beobachter eines Experimentes immer nur sich selber im Kontext des Experimentes beobachten könne und so Objektivität fragwürdig werde, wird damit wieder aufgegriffen. Wenn dies zutrifft, dann fehlt auch für die Naturwissenschaften – wie für die Geistes- u. Kulturwissenschaften beschrieben – eine wichtige Bedingung für Objektivität: nämlich die Unabhängigkeit vom Subjekt.

Medienforscher sind sich inzwischen einig, dass es in der Berichterstattung immer zu einer Verzerrung der Realität kommt. Es wäre ein Irrtum davon auszugehen, dass es sich bei Dokumentationen um „abbildliche Reproduktionen von hochgradiger Objektivität“ handelte. Bildungsmedien – wie die vom FWU (Institut für Film und Bild in Wissenschaft und Unterricht) produzierten – werden seit den 1960/70er Jahren kritisch reflektiert und mit entsprechendem Begleitmaterial zur Verfügung gestellt. Seit der Verbreitung digitaler Medien setzt man auf die Pluralisierung und Demokratisierung der Wissenschaftsentwicklung, um mit qualitativen Kriterien der Kulturwissenschaften, die Aspekte der Wissensgewinnung (Epistemologie) mit einbeziehen, die Objektivität von Medien angemessen zu sichern.

Repräsentationen von Wirklichkeit erwecken im Medium 'Fernsehen' beim Zuschauer den Eindruck von Objektivität. Dies mache es nötig, Forschungsprojekte zu initiieren und zu unterstützen, die den Einfluss der Fernsehsendungen auf die Gesellschaft untersuchen. Ohne derartige Bemühungen verliere der multimediale Journalismus zunehmend an Sinn, Originalität und Objektivität.

Objektivität ist auch ein Ideal von Internet-Dokumentationen. Objektivität einer Internet-Enzyklopädie z. B. bedeute, „dem Benutzer des Lexikons durch das Angebot von Fakten die Bildung eines eigenen Urteils zu ermöglichen…“





</doc>
<doc id="11086" url="https://de.wikipedia.org/wiki?curid=11086" title="U-Bahn">
U-Bahn

U-Bahn oder Metro (Kurzform für Untergrundbahn bzw. Metropolitan) bezeichnet ebenso ein sich vorwiegend unterirdisch bewegendes, anfänglich dampf- und später elektrisch betriebenes Schienenfahrzeug wie auch ein Verkehrssystem (→ Verkehrsmittel) des öffentlichen Personennahverkehrs (ÖPNV, Stadtverkehr) ähnlich den ebenfalls, aber nur teilweise unterirdisch bzw. in Tunneln fahrenden Verkehrsmitteln wie der S-, Straßen- oder Stadtbahn, Seil- bzw. Bergbahnen sowie den Eisenbahnen.

Der Begriff wird für das Gesamtsystem, eine U-Bahn-Strecke und -Linie und umgangssprachlich auch für das einzelne Fahrzeug (U-Bahn-Triebwagen, U-Bahn-Zug) verwendet. Während das "»U«" eigentlich eine Abkürzung für "Untergrund" ist, haben viele U-Bahnen auch Streckenabschnitte an der Oberfläche, im Einschnitt, auf einem Bahndamm oder aufgeständert als Hochbahn. Aus diesem Grund wird das "U" im deutschen Sprachraum mitunter als „unabhängig“ interpretiert – handelt es sich doch um Schienenverkehrssysteme, die eigenständig, kreuzungsfrei und unabhängig von anderen städtischen Verkehrssystemen konzipiert sind.

Der international am meisten verwendete Begriff außerhalb des deutschen Sprachgebrauchs ist "Metro". Dieser dürfte auf die Begriffe "Metropolitan Railway" in London (heute "Metropolitan Line") sowie "Chemin de fer métropolitain", kurz "Métro", in Paris zurückgehen. Dieser Name hat sich auch in Spanien und Italien eingebürgert. Auch das Russische und das Polnische verwenden die Kurzform dieses Ausdrucks. Weiter sind auch "Underground" beziehungsweise "Tube" (London) gebräuchlich, im skandinavischen Raum auch "T-Bana" ("Tunnelbana") oder "T-bane" ("Oslo T-bane"). Die U-Bahnen in Manila, Singapur und Taipeh tragen die Bezeichnung "MRT" für englisch "Mass Rapid Transit", während in Hongkong die Abkürzung "MTR" für Mass Transit Railway verwendet wird. In Nordamerika ist der Begriff "subway" gebräuchlich; bei einigen weitgehend oberirdischen Systemen auch "rapid transit". Einzelne wenige Netze in Nordamerika heißen auch "Metro" oder "Metrorail", wie beispielsweise in Washington, D.C., Los Angeles und Miami oder im französischsprachigen Montreal. Im Sprachgebrauch sind auch Abkürzungen wie "BART" (San Francisco und Umland) oder "MARTA" (Atlanta) gebräuchlich. In Buenos Aires schließlich heißen die U-Bahnen "Subte" (von "Subterráneo"). Allerdings ist „Metro“ nicht immer ein generischer Begriff, sondern insbesondere in Spanien und Frankreich als Marke für die U-Bahn-Betreiber geschützt.

Der Internationale Verband für öffentliches Verkehrswesen (UITP) definiert eine Metro so: „Metro-Eisenbahnen sind ein urbanes, sich in ein jeweiliges Nahverkehrsnetzwerk flexibel einfügendes, elektrisch betriebenes Personentransportsystem, das seinen Dienst in hohem Takt und hoher Kapazität anbietet und sich unabhängig von jeglichem anderen Verkehr und Verkehrsteilnehmern auf eigenen Tunnel-, ebenerdigen oder Brückentrassen fortbewegt“. Im Gegensatz dazu stehen Straßenbahnen, deren Hauptmerkmale gemäß der Veröffentlichungen des UITP das Geführtsein (zumeist durch Schienenstränge) und eine sehr hohe Anpassbarkeit sind.

Nach deutschem ( Abs. 2 "Personenbeförderungsgesetz") und österreichischem ("Straßenbahnverordnung" – StrabVO) Recht sind U-Bahnen jedoch "Straßenbahnen", wenn sie überwiegend zum Personentransport im Nahverkehr eingesetzt und keine Berg- oder Seilbahnen sind. Der Überbegriff Straßenbahn wird definiert als Bahnen besonderer Bauart, die sich der Eigenart des Straßenverkehrs anpassen und einen besonderen Bahnkörper haben. Eine U-Bahn gilt somit im juristischen Sinn als eine Straßenbahn, obwohl sie im Unterschied zu dieser nicht nur über einen besonderen, sondern sogar faktisch über einen vom Straßenverkehr völlig unabhängigen Bahnkörper verfügt. Sie wird nach der "Verordnung über den Bau und Betrieb der Straßenbahnen" (BOStrab) betrieben.

Der Verband Deutscher Verkehrsunternehmen (VDV) definiert eine U-Bahn als schienengebundenes und, angelehnt an den UITP, als vom Individualverkehr völlig getrennt geführtes Massenverkehrsmittel, das ein "geschlossenes System" bildet. Ihre Strecken können sowohl im Tunnel als auch auf Dämmen und Hochstrecken oder im freien Gelände geführt sein. Unabhängig bedeutet in diesem Fall auch, dass eine U-Bahn in der Regel keine niveaugleichen Kreuzungen mit anderen Schienenverkehrsmitteln und keine Bahnübergänge besitzt. Die Fahrstromzuführung bei U-Bahnen erfolgt über eine seitlich am Gleis angeordnete Stromschiene oder eine Oberleitung.

U-Bahn-Netze nach der VDV-Definition gibt es in Deutschland in den Städten Berlin, Hamburg, München und Nürnberg, in Österreich in Wien und in der Schweiz in Lausanne. Keines dieser U-Bahn-Netze verläuft ausschließlich unterirdisch.

In Frankfurt am Main gibt es ein Stadtbahnsystem, das offiziell als U-Bahn bezeichnet wird. Da die Unabhängigkeit des Streckennetzes aber nur dadurch gegeben ist, dass sämtliche Straßenkreuzungen mit Andreaskreuzen beschildert sind und formell Bahnübergänge darstellen, wobei es nur an wenigen Stellen beschrankte Bahnübergänge gibt, ist die Bezeichnung als U-Bahn in diesem Falle inkorrekt. Ähnliches gilt für die Stadt-/U-Bahnen von Hannover, Köln, Bonn, Stuttgart und im Ruhrgebiet. Die Ruhrgebietsstädte Bochum und Herne besitzen mit der Stadtbahnlinie U35 fast eine richtige U-Bahn-Linie, da die U35 komplett linienrein und mit Ausnahme des oberirdischen Abschnitts entlang der Universitätsstraße durch Querenburg durchgehend in einem vollständig kreuzungsfreien Tunnel durch Bochum und Herne verläuft. Der oberirdische Abschnitt in Querenburg verläuft komplett auf eigenem Bahnkörper und weist lediglich drei plangleiche Kreuzungen mit dem Straßenverkehr auf, an denen die U35 aber dank Vorrangschaltung stets freie Fahrt besitzt.

Zugleich grenzt der VDV die U-Bahn begrifflich von der "Straßenbahn" und der "Stadtbahn" ab, die zumindest in Teilen eine Streckenführung auf öffentlichen Straßen haben können, in deren Bereich die Straßenverkehrs-Ordnung zu beachten ist. Die Abgrenzung zur S-Bahn ergibt sich in Deutschland vor allem aus deren rechtlicher Stellung als Vollbahn beziehungsweise Eisenbahn, die zum Beispiel auch niveaugleiche Kreuzungen mit anderen Verkehrsmitteln haben kann. In anderen Ländern sind die Grenzen zwischen U-Bahnen und Eisenbahnen oft fließend und werden auch nicht rechtlich differenziert. Die Abgrenzung orientiert sich hier eher an der betrieblichen Geschlossenheit oder der Eigentümerfunktion, da U-Bahnen – anders als Eisenbahnen – meist in kommunalem Besitz sind.

Eine U-Bahn hat demnach zusammengefasst mindestens folgende Eigenschaften: Sie wird ohne niveaugleiche Kreuzung mit anderen Verkehrsmitteln betrieben, fährt in dichter Fahrplan-Taktfolge im städtischen Bereich und wird elektrisch angetrieben und gesteuert.

Es gibt auch U-Bahn-ähnliche Systeme nur für Gütertransporte, solche waren die U-Bahnen zur Postbeförderung in einigen Städten. Grubenbahnen und Kasemattenbahnen haben einige Gemeinsamkeiten mit U-Bahnen, dienen im Gegensatz zu diesen jedoch nicht primär der Personenbeförderung.

Das Verkehrsmittel Untergrundbahn, wie es heute in zahlreichen Städten eingesetzt wird, ist das Ergebnis einer längeren Entwicklung, die sich durch die ganze zweite Hälfte des 19. Jahrhunderts zog. Am Anfang standen Pläne zur innerstädtischen, unterirdischen Verbindung zwischen Fernbahn-Endbahnhöfen oder anderen Verkehrsknotenpunkten – eine Aufgabe, die heute eher einer S-Bahn zukäme. Solche Pläne gab es etwa bereits 1844 in Wien. Die ersten Realisierungen fanden ab 1863 mit der "Metropolitan Railway" in London und 1869 in Athen mit der "Athens & Piraeus Railway Company" statt. Für den Bau von Tunneln für U-Bahnen unter Hochbauten, also in Städten, war die Entwicklung einer Technologie erforderlich, bei der das Tunnelgewölbe während des Baus nicht nachgibt. Das konnten die bis dahin zur Stollenabstützung verwendeten reinen Holzkonstruktionen nicht mehr gewährleisten. Eisen als Stützmaterial wurde in einem Pionierprojekt erstmals beim Bau des Naenser Tunnels eingesetzt.

Die Londoner Tunnelstrecke wurde mit Dampfzügen betrieben, was keine akzeptable Lösung darstellte und deshalb auch keine Nachahmungen in anderen Städten fand. Ein wichtiger Durchbruch zur Entwicklung des unterirdischen Stadtverkehrs war deshalb der Einsatz von Elektromotoren in Schienenverkehrsmitteln. In Deutschland leistete auf diesem Gebiet der Berliner Unternehmer Werner Siemens wichtige Pionierarbeit. Auf der Berliner Gewerbeausstellung 1879 stellte Siemens eine elektrische Lokomotive vor; 1881 eröffnete er zwar in Berlin-Lichterfelde die erste elektrische Straßenbahn der Welt. Bedenkenträger und Bürokratie hinderten Siemens jedoch über Jahrzehnte hinweg am Bau eines elektrischen Schnellbahnnetzes in Berlin, während bereits ab 1890 in London der elektrische Betrieb der Tube mit E-Loks der Manchester Firma Mather & Platt begann, womit die wichtigste Voraussetzung zum Siegeszug des neuen Verkehrsmittels geschaffen war. 

Die zweite zunächst unbeantwortete Grundsatzentscheidung der frühen Jahre war die Frage der Trassierung der Schnellbahnstrecken: Die Londoner U-Bahn verkehrte überwiegend im Tunnel. Projekte in anderen europäischen Städten und in Nordamerika bevorzugten die Streckenführung auf eisernen Viadukten als Hochbahn. Die Mehrzahl der vor dem Ersten Weltkrieg gebauten Metrostrecken entstand letztlich als Hochbahn – die Baukosten lagen deutlich unter denen einer Tunnelstrecke, und insbesondere in den Stadtteilen der Unterschicht sah man es nicht als erforderlich an, Rücksicht auf die städtebaulichen Folgen nehmen zu müssen. Nach dem Weltkrieg wendete sich das Blatt jedoch; neue Strecken wurden nun fast ausschließlich im Tunnel errichtet. In Nordamerika, besonders in New York, wurden sogar ganze Hochbahnstrecken abgebaut und durch Tunnel ersetzt. Anstatt der "Hochbahn" entwickelte sich die "Untergrundbahn" zur Standardlösung.

Als erste U-Bahn der Welt gilt allgemein die am 10. Januar 1863 in London eröffnete "Metropolitan Railway". Es handelte sich dabei jedoch zunächst noch um eine mit Dampflokomotiven betriebene Eisenbahn. Sie war als Verbindungslinie zwischen den Fernbahnhöfen "Paddington", "King’s Cross", "St Pancras" und "Euston", die alle relativ weit außerhalb der Innenstadt lagen, und der City of London gedacht.

Die erste elektrische U-Bahn, die somit den heutigen Vorstellungen entspricht, war die "City and South London Railway" (heute "Northern Line"), eröffnet am 4. November 1890 in London. Sie führte von "Stockwell" zur "King William Street". Somit löste London einen U-Bahn-Boom aus, da zur gleichen Zeit auch viele andere europäische Metropolen nach Möglichkeiten suchten, ihre innerstädtischen Verkehrsprobleme zu lösen. Man glaubte, mit dem Konzept der Untergrundbahn alle diese Probleme lösen zu können.

Die erste elektrische Metropolitan Großbritanniens außerhalb Londons war die vollständig überirdisch als Hochbahn verkehrende "Liverpool Overhead", die am 4. Februar 1893 wie auch die Athener Metro als eine Verbindung zwischen Stadtzentrum und dem Hochseehafen eröffnet wurde. Die Hochbahn in Liverpool erhielt bei einer Erweiterung im Dezember 1896 einmalig einen Tunnelbahnhof. Die gesamte Strecke wurde jedoch am 30. Dezember 1956 stillgelegt.
Neben der von der Athens & Piraeus Railway Company ersten auf dem Festland Europas betriebenen unterirdischen Bahn zur Personenbeförderung befand sich in Istanbul eine weitere. 1875 wurde die Tünel-Standseilbahn im europäischen Teil der Stadt eröffnet.

Die erste elektrische unterirdische und regulär verkehrende Metro auf dem europäischen Festland wurde am 2. Mai 1896 in Pest, dem östlichen Stadtteil Budapests, eröffnet. Diese Linie, heute als "Millenniums-U-Bahn" (ungarisch "föld" „Erde, Boden“, "alatt" „unter“: „die Unterirdische“) bezeichnet, war auf Initiative des Erfinders Werner von Siemens entstanden und ursprünglich für Berlin geplant. Da sich dort die lokalen Behörden jedoch nicht einigen konnten, ließ Siemens die Budapester U-Bahn quasi als Demonstrationsobjekt für weitere europäische U-Bahn-Strecken bauen. Zuvor hatte allerdings Siemens-Konkurrent AEG auf seinem Berliner Werksgelände 1895 eine U-Bahn-Versuchsstrecke mit einem 295 m langen und 3,15 m hohen Tunnel errichtet. Einen zweiten 454 m langen U-Bahn-Tunnel zur Probe und Repräsentation, den Spreetunnel, initiierte ebenfalls die AEG. Er wurde durch ein Firmenkonsortium ab 1895 mittels Schildvortrieb (»"Tunnelling shield"«), einem Verfahren, das bereits 1870 beim Bau der Londoner Tower-Metropolitan der ersten tief-nivellierten Metro der Welt zur Unterquerung der Themse Anwendung fand, unter der Spree zwischen den damaligen Berliner Vororten Stralau und Treptow gebaut und 1899 fertiggestellt. Da die AEG für Berlin keine Genehmigung zum Metrobau erhielt, fuhr nicht wie geplant eine U-Bahn durch diese eingleisige Röhre, sondern ab Dezember 1899 regulär eine Straßenbahn, die von der Berliner Ostbahnen GmbH betrieben wurde.

Im gleichen Jahr wie die ungarische Földalatti ging die Glasgow Subway als vierte Schnellbahn in Betrieb. Die Strecke wurde ab dem 14. Dezember 1896 zuerst als Kabelbahn befahren und erst 1935 elektrifiziert. Ähnliches geschah mit der 1898 eröffneten Wiener Dampfstadtbahn, aus der 1925 die Wiener Elektrische Stadtbahn hervorging.

Im Jahr 1900 folgte Paris mit der (von Beginn an elektrisch betriebenen) "Métropolitain". In nur wenigen Jahren wurde hier ein Netz aus zahlreichen Linien erbaut. Auch heute trägt das Pariser Métro-System einen Großteil der Verkehrsströme der französischen Hauptstadt.

Die 1901 in den westdeutschen Nachbarstädten Barmen und Elberfeld eröffnete "einschienige Hängebahn System Eugen Langen", heute besser bekannt als Wuppertaler Schwebebahn, stellte eine Sonderform einer Hochbahn dar: die Züge fahren nicht "auf" Schienen, sondern hängen "unter" ihnen. Die Viaduktkonstruktion fiel dadurch aufwändiger aus als bei einer konventionellen Hochbahn, weil die Trägerrahmen über die Züge hinwegreichen müssen. Die Schwebebahn ist damit die erste Stadtbahn im heutigen deutschen Bundesgebiet.

Nach langen Diskussionen wurde schließlich am 15. Februar 1902 auch in Berlin eine reguläre Metrostrecke zwischen "Stralauer Thor" – "Potsdamer Platz" (alter Bahnhof) und "Zoologischer Garten" eröffnet. Werner von Siemens erlebte seinen späten Sieg über die Berliner Baubürokratie jedoch nicht mehr, er war bereits 1892 verstorben. Die "Elektrische Hoch- und Untergrundbahn" des Betreibers Siemens & Halske verlief größtenteils auf einem Viadukt. Spätere Erweiterungen durch die Stadtmitte und durch wohlhabende Wohnviertel wurden jedoch unterirdisch errichtet. Aus Berlin stammt auch der Begriff "U-Bahn", er wurde 1929 eingeführt, nachdem die Deutsche Reichsbahn (1920–1945) für ihre "Stadt-, Ring- und Vorortbahnen" das griffige Kürzel "S-Bahn" eingeführt hatte.

1904 wurde auch die Strecke der 1869 gebauten bisher dampfbetriebenen Athener U-Bahn "»Attiko Metro«" elektrifiziert und nun ausschließlich zum Nahverkehr genutzt.

Doch nicht nur in Berlin und Budapest hatte Siemens seine Idee einer elektrischen Schnellbahn vorgetragen, auch für die Hansestadt Hamburg hatte er ein Netz geplant. Deren erste Strecke wurde genau zehn Jahre nach Berlin, am 15. Februar 1912, der Öffentlichkeit übergeben. Der Betrieb trug den Namen Hamburger Hochbahn AG, da der größte Teil der Strecken auf Viadukten und Dämmen angelegt wurde.

Nach ebensolchen Schwierigkeiten mit den spanischen Behörden wie in anderen europäischen Städten konnten in Madrid (1919) und Barcelona (1924) die ersten Metrolinien eröffnet werden.

Doch nicht nur im europäischen Raum gediehen die ersten U-Bahn-Netze, auch in Nordamerika setzte sich nach und nach die Idee der unabhängigen Schnellbahn durch. Das erste System, eine reine Hochbahn, wurde in Chicago 1892 als "Chicago & South Side Rapid Transit" eröffnet, die Elektrifizierung folgte drei Jahre später. Das zweite System ging 1897 in Boston in Betrieb. Die erste Tunnelstrecke diente dem Straßenbahnverkehr, wie man es 70 Jahre später (als „Stadtbahn“) in zahlreichen Städten wiederentdeckte. Auf dieses relativ frühe Eröffnungsdatum sind die Einwohner von Boston bis heute sehr stolz, da erst 1904 die wohl bekannteste amerikanische Großstadt, New York, eine U-Bahn unter dem Namen „Subway“ eröffnete. Die letzte der alten Ostküstenmetropolen, Philadelphia, folgte 1907.

Im Jahr 1913 ging die U-Bahn-Linie im argentinischen Buenos Aires in Betrieb, die heute den Buchstaben 'A' trägt. Diese erste südamerikanische U-Bahn wird heute noch mit Zügen aus der Anfangszeit betrieben. Buenos Aires verfügt heute über sechs Linien in Betrieb mit insgesamt mehr als 40 Kilometer Länge.

Der Kreativität der Konstrukteure waren keine Grenzen gesetzt. Nachdem der elektrische Betrieb und die wachsende Erfahrung beim Tunnelbau viele Städte zur Planung von U-Bahn-Netzen animierten, lag es nahe, die neue Technik auch für den innerstädtischen Güterverkehr einzusetzen. Die zwei realisierten Anlagen waren allerdings weniger Weiterentwicklungen des konventionellen Güterverkehrs auf der Eisenbahn als vielmehr solche der Rohrposttechnik.

Die erste Rohrpostanlage der Welt, die "Pneumatic Despatch Railway", wurde 1859 in London in Betrieb genommen. In der Folge entstanden solche unterirdischen Netze in einigen Dutzend europäischen sowie einigen außereuropäischen Städten. Das Berliner Netz war 1940 rund 400 Kilometer lang und bediente 79 Post- und Telegrafenämter. Die Postsendungen wurden in verschlossenen Kapseln befördert, zum Antrieb diente Druckluft. Die Kapazität dieser Anlagen war gering. In Berlin durften Briefe maximal 20 Gramm, in München 100 Gramm wiegen, das Netz in New York konnte immerhin Päckchen befördern. Zur innerstädtischen Verteilung schwererer Güter unter der Erde musste also wieder auf Rad-Schiene-Technik zurückgegriffen werden, und die Untergrundbahnen des Personenverkehrs hatten die Grundlagen dafür geschaffen.

In Chicago begann ab 1899 der Bau eines solchen Untergrundnetzes, der "Chicago Tunnel Company Railroad", das 1906 fertiggestellt war und Tunnelstrecken unter nahezu jeder Straße der Innenstadt besaß. Das Netz erreichte eine maximale Länge von 97 Kilometer, mit 149 Lokomotiven und 3000 Güterwagen wurden Fracht und Kohle von Güterbahnhöfen der Eisenbahn zu Warenhäusern, Büros und Lagern in der Innenstadt und Asche von dort wegbefördert. Der aufkommende Lastwagenverkehr und die Umstellung von Kohlen- auf Gasheizung ließ die Umsätze in den 1940er Jahren einbrechen, die Betreiberin musste 1956 Konkurs anmelden. Das Netz wurde 1959 stillgelegt. Die Tunnel werden jedoch heute noch zur Verlegung von Strom- und Telefonleitungen verwendet.

Nach Vorbild des Systems in Chicago entstand 1927 die "London Post Office Railway" (auch "Mail Rail") in London. Dieses kleine U-Bahn-Netz versorgte acht Postämter. Die Tunnel liegen bis zu 21 Meter unter dem Straßenniveau. Die 10,5 Kilometer lange Strecke verlief vom Postsortieramt am Bahnhof Paddington in west-östlicher Richtung zum Bezirkspostamt im östlichen Stadtteil Whitechapel. Da fünf der angeschlossenen Postämter im Laufe der Zeit geschlossen wurden, wurde die Anlage 2003 stillgelegt. Weitere Beispiele sind die Post-U-Bahn München von 1910 bis 1988 (450 Meter Strecke) und die in Zürich von 1938 bis 1981.

Auch Kasemattenbahnen und Grubenbahnen können als Güter-U-Bahnen bezeichnet werden, wobei diese auch zum Personentransport dienen können.

Mit dem Beginn des Ersten Weltkriegs endete die erste Phase des U-Bahn-Baus in den Metropolen der westlichen Welt. Die wesentlichen Systemfragen waren beantwortet. Die Untergrundbahn hatte sich gegen die Hochbahn durchgesetzt. Die Fahrzeuge der ältesten Systeme hatten sich als zu klein erwiesen, man ging zu größeren Tunnelprofilen und Zügen mit größerem Fassungsvermögen über. Die Fahrtreppe war praxisreif entwickelt und ermöglichte den Transport großer Fahrgastzahlen zwischen tiefliegenden U-Bahnhöfen und der Erdoberfläche. Das System U-Bahn war in allen wesentlichen Bereichen ausgereift und wird bis heute weitgehend unverändert angewandt.

Zwischen den Weltkriegen gingen im europäischen Raum nur drei Netze in Betrieb: eines 1919 in der spanischen Hauptstadt Madrid und wenig später, 1924, eines in Barcelona. Das dritte Netz entstand in Moskau. Dort wurde die erste unterirdische Schnellbahn im Jahr 1935 in Betrieb genommen. Zuvor war eine Expertenkommission nach Berlin geschickt worden, um das System zu begutachten und Erfahrungen zu sammeln. Bekannt ist Moskau vor allem durch seine sehr tief liegenden und prunkvoll ausgeschmückten Bahnhöfe. Der damalige sowjetische Führer Stalin wollte die U-Bahnhöfe als „Paläste der Arbeiterklasse“ betrachtet sehen.

Die bereits vor dem Weltkrieg bestehenden Netze wurden weiter ausgebaut, teilweise als völlig neue, mit den älteren Linien inkompatible Systeme, um den Einsatz größerer Fahrzeuge zu ermöglichen.

Die erste asiatische U-Bahn ging in der japanischen Hauptstadt Tokio 1927 in Betrieb. Die Ginza-Linie zwischen "Asakusa" und "Ueno" war der erste Abschnitt der dortigen U-Bahn. 1933 folgte die japanische Metropole Osaka mit der Midosuji-Linie.

Während und nach dem Zweiten Weltkrieg stagnierte der U-Bahn-Bau nahezu überall. Nach 1945 nahm in den Städten der westlichen Welt die Anzahl der Kraftfahrzeuge rapide zu, der wachsende Wohlstand sorgte für die "Massenmotorisierung".

Um dem dadurch stark zunehmenden Straßenverkehr gerecht zu werden, galt es in vielen Städten als modern, die dortigen Straßenbahnnetze stillzulegen, da man die Straßenbahnen als erhebliche Behinderung des motorisierten Individualverkehrs betrachtete. Somit verschwanden unter anderem in London, Paris, West-Berlin und Hamburg alle Straßenbahnen aus dem Stadtbild.

Auch das Baugeschehen im Bereich der U-Bahn-Netze kam weitgehend zum Erliegen. In vielen Metropolen, etwa in Paris, wurde jahrzehntelang keine einzige neue Strecke eröffnet. Ausnahmen waren teilweise politisch motiviert wie in West-Berlin, das von der Deutschen Reichsbahn und deren S-Bahn-Netz unabhängig werden wollte, oder in Moskau, das zur repräsentativen Hauptstadt der zur Weltmacht aufgestiegenen Sowjetunion ausgebaut wurde.

Dort, wo es noch keine U-Bahn gab, kam häufig die Idee auf, Straßenbahnstrecken abschnittsweise in den Untergrund zu verlegen, um dem Straßenverkehr an der Oberfläche mehr Platz zu verschaffen und das verbleibende Straßenbahnnetz mittelfristig eventuell aufgeben zu können. Diese Lösung wurde vor allem im deutschsprachigen Raum (Köln, Stuttgart, Frankfurt am Main) und in Belgien angewandt.

Dabei sind zwei unterschiedliche Grundkonzepte zu unterscheiden. Bei der einfacheren Variante wird ein Stück Straßenbahnstrecke unter die Erde verlegt, aber ansonsten wie zuvor betrieben, wie bereits 1897 in Boston. Bei diesen als U-Straßenbahn bezeichneten Anlagen gibt es beispielsweise unterirdische Gleisdreiecke und recht enge Kurvenradien.

Das aufwändigere Stadtbahnkonzept sieht in den inneren Stadtbereichen dagegen Tunnelstrecken vor, die hinsichtlich Kurvenradien, Kreuzungsfreiheit und Zugsicherung teilweise mit den Anlagen klassischer U-Bahnen identisch sind, die jedoch in den Außenbezirken vorhandene Straßenbahnstrecken nutzen. Der Grundgedanke dabei war, dass ein fertiggestellter Tunnelabschnitt sofort in das bestehende Straßenbahnnetz eingebunden werden kann, anstatt wie etwa eine kurze „klassische“ U-Bahn-Strecke über viele Jahre einen Fremdkörper im Verkehrsnetz darzustellen.

Dieser kurzfristige Vorteil wurde jedoch durch gewisse Nachteile erkauft, dazu gehören die Kreuzungen (in manchen Fällen sogar gemeinsame Fahrbahnbenutzung) mit dem Straßenverkehr und die damit verbundene Störungsanfälligkeit sowie zahlreiche schwere Verkehrsunfälle.

Die ersten Straßenbahntunnel wurden 1966 in Wien und Stuttgart eröffnet, in rascher Folge kamen weitere Städte hinzu (Essen 1967, Frankfurt und Köln 1968, Brüssel 1969, Bielefeld 1971, Antwerpen, Hannover und Bonn 1975, Bochum 1979, Düsseldorf 1981, Charleroi und Dortmund 1983, Zürich 1986, Duisburg, 1992).

Auch mittelgroße westdeutsche Großstädte wie Kassel oder Ludwigshafen errichteten unterirdische Straßenbahnstationen. Ob der finanzielle Aufwand dabei jedoch im Verhältnis zum erzielten verkehrlichen Nutzen steht, ist fragwürdig.

Einige Städte, etwa Köln oder Stuttgart, die zunächst auf die preiswertere Lösung U-Straßenbahn setzten, änderten in der Folge ihre Planungen und entwickelten sie zu einem (leistungsfähigeren) Stadtbahnbetrieb weiter.

Einige Stadtbahnnetze entwickeln sich mit zunehmendem Ausbau tendenziell in Richtung Metro-Standards. So gibt es in Frankfurt am Main bereits seit 1980 eine „echte“ U-Bahn-Linie. In Essen, Bochum und Dortmund verkehren Stadtbahnlinien, die nahezu keine Querungen mit dem Straßenverkehr mehr aufweisen. In Brüssel wurden zwei Stadtbahntunnel nach Erreichen einer verkehrlich sinnvollen Länge auf Stromschienenbetrieb umgerüstet und werden seitdem mit Metrofahrzeugen betrieben.

Das Stadtbahnkonzept konnte sich in Nordamerika erst in den 1980er Jahren durchsetzen ("Light rail", auch Metrorail), wobei in den meisten Fällen auf Tunnelstrecken verzichtet wurde. Die meisten neuen Schnellbahnnetze außerhalb Europas wurden deshalb als klassisches Metrosystem gebaut, so etwa in Cleveland, Montreal, Toronto und Nagoya.

Später wurde die sogenannte Métro sur pneumatiques (Métro auf Gummireifen) eingeführt. Erstmals wurde diese ab 1954 auf einer Versuchsstrecke der Pariser Métro getestet, wo 1959 auch die erste Linie damit ausgestattet wurde. Dieses System, das weiterhin die Rad-Schiene-Technik beibehält, zeichnet sich besonders durch gute Brems- und Anfahrwerte aus. Unter anderem verwenden heute ungefähr die Hälfte der Métrolinien in Paris, die Netze in Marseille, Lyon, Lille, Montréal, Mexiko-Stadt, Santiago de Chile und Sapporo (U-Bahn Sapporo) gummibereifte Züge. Bei der U-Bahn Lausanne wurde gerade wegen der starken Steigungen der Strecke die Luftbereifung mit ihren höheren Haftreibungswerten verwendet.

Auch in Deutschland wurden noch zwei neue U-Bahn-Netze gebaut, das erste in München. Ursprünglich war auch in der bayerischen Hauptstadt ein unterirdisches Straßenbahnnetz geplant. Doch später wurde das Konzept überarbeitet und zu einer Voll-U-Bahn umgeplant. Die anfangs für 1974 vorgesehene Eröffnung wurde aufgrund der Olympischen Spiele 1972 auf 1971 vorgezogen.

Das vierte und jüngste deutsche U-Bahn-Netz ging 1972 in Nürnberg in Betrieb. Ursprünglich war auch dort ein Stadtbahnnetz geplant. (Eine Besonderheit war, dass die U-Bahn-Fahrzeuge von München und Nürnberg ursprünglich baugleich und somit grundsätzlich austauschbar waren. So konnten sich die beiden Städte bei Engpässen aushelfen. Mit der Beschaffung neuer Fahrzeuggenerationen, der Modernisierung der Zugsicherungstechnik und des teilweise automatischen Betriebs besteht die Kompatibilität nicht mehr.) Seit dem 15. Juni 2008 verkehrt auf der U 3 in Nürnberg die erste planmäßige vollautomatische U-Bahn Deutschlands – auf dem gemeinsamen Abschnitt mit der U 2 bis Ende 2009 mit deren noch konventionell gesteuerten Zügen im Mischbetrieb, seitdem verkehrt auch die U2 vollautomatisch.

Seit den 1960er Jahren wurden in der Sowjetunion und anderen osteuropäischen Staaten zahlreiche neue U-Bahn-Betriebe gegründet. Neue U-Bahn-Städte waren zum Beispiel Leningrad (1955), Kiew (1960), Tbilissi (1966), Baku (1967), Prag (1974), Charkiw (1975), Taschkent (1977), Bukarest (1979),Jerewan (1981), Minsk (1984), Nischni Nowgorod (1985), Samara (1987), Dnipropetrowsk (1995), Warschau (1995), Kasan (2005) und Almaty (2011). In Budapest wurden zusätzlich zur 1896 eröffneten Linie zwei moderne Linien gebaut, der erste Abschnitt eröffnete 1970. Hinzu kommen Stadtbahnnetze, beispielsweise die Metro (auch MetroTram) Wolgograd.

Die technischen Grundlagen, die Fahrzeuge und sogar die Netzkonzeption waren relativ einheitlich. In den meisten Städten wurde ein "Sekantennetz" mit drei Linien konzipiert. Streckentunnel und Bahnhöfe liegen teilweise sehr tief unter Straßenniveau, lange Rolltreppen verbinden Straße und Bahnsteig. Der Abstand zwischen einzelnen Stationen ist größer als in den westeuropäischen Netzen der gleichen Periode, was die Durchschnittsgeschwindigkeit steigert, aber weiterhin Straßenbahn- oder Buslinien zur Feinerschließung der Quartiere erfordert. Anders als in westlichen Städten gab es im sozialistischen Europa allerdings auch keine Großstädte, die auf ihre Straßenbahn verzichteten.

Neue Metronetze entstanden und sind geplant in den Industriestaaten Ostasiens, ferner auch in Megastädten der sogenannten Schwellenländer wie mit der U-Bahn Mexiko-Stadt, Metrô São Paulo, Metrô Rio de Janeiro, Metro Kairo, U-Bahn Teheran, Metro Delhi, Metro Caracas und Bangkok Metro.

Seit Ende der 1980er Jahre verringerte sich die Zahl der Neueröffnungen besonders aufgrund der hohen Baukosten für Tunnelstrecken. Auf anderen Kontinenten werden bestehende Netze erweitert, neue aber kaum noch gebaut. Eine Ausnahme sind dabei die spanische Hauptstadt Madrid sowie die GUS-Staaten, wobei in letzteren aufgrund der Finanzknappheit seit den 1980er Jahren immer noch an noch nicht fertiggestellten Netzen gearbeitet wird, wie bei der Metro Tscheljabinsk oder der Metro Donezk. Deshalb hat sich die Eisenbahnindustrie auf die Errichtung von kostengünstigeren „Light Metros“ ausgerichtet, während konventionell konzipierte U-Bahnen („Heavy Metros“) derzeit wenig gefragt sind.

Seit 2013 ist auch eine U-Bahn in Australien in Bau, die Metro Sydney, die 2019 eröffnet werden soll. Sie soll einige Vorortbahnstrecken übernehmen, in der Innenstadt jedoch durch eine neue Tunnelstrecke verkehren und so die bisherigen Innenstadtstrecken entlasten.

Nachdem Frankreich schon in den 1950er Jahren mit dem Gummiradantrieb als Innovationsstandort für U-Bahnen galt, wurde in den 1980er Jahren mit dem VAL-System (ausgeschrieben "Véhicule automatique léger") ein hochgradig automatisiertes Bahn-System erprobt und erfolgreich eingesetzt. Mit zahlreichen standardisierten Komponenten ist es kostengünstiger zu erstellen als U-Bahnen mit herkömmlichem Konzept. Als erste wurde die neue Métro Lille mit diesem System gebaut, in einem Ballungsraum mit nur ungefähr einer Million Einwohnern. Damit wurde gezeigt, dass auch mittelgroße Städte ein rentables und effizientes U-Bahn-Netz betreiben können. Weitere Orte folgten diesem Konzept, so mit der Métro Toulouse ab 1993, der MRT (Taipei) seit 1996, der Métro Rennes seit 2002 und der Metropolitana di Torino seit 2006.

VAL-Metros dienen auch dem internen Personentransport mehrerer Großflughäfen, etwa in Atlanta, Paris-Charles-de-Gaulle ("CDGVAL"), Paris-Orly ("Orlyval") und Chicago.

U-Bahnen und Stadtbahnen waren traditionell als hochflurige Verkehrssysteme konzipiert, bei denen Hochbahnsteige einen barrierefreien Einstieg in ein ebenfalls hochfluriges Fahrzeug ermöglichen. Dies war in der Vergangenheit erforderlich – und deshalb in allen vier deutschen U-Bahn-Netzen, Berlin, Hamburg, München und Nürnberg so angelegt –, da es technisch nicht möglich war, Motoren und elektrische Anlagen in einem Niederflurfahrzeug unterzubringen. Während Straßenbahnen im Regelfall nicht auf eine Barrierefreiheit ausgelegt waren und an den Haltestellen keine aufwändigen Bauwerke erforderten, zeigte sich schnell, dass der Preis für einen barrierefreien Betrieb sehr hoch ist, da hier auch der Bau von oberirdischen Stationen einen großen Kostenfaktor darstellt. Weil Hochbahnsteige wegen städtebaulicher Gegebenheiten vielerorts nicht realisierbar sind, musste man vielfach auch solche Streckenabschnitte in Tunnel verlegen, wo dies vom Verkehrsaufkommen her sonst nicht erforderlich wäre.

In den 1990er Jahren kamen die ersten Niederflurfahrzeuge auf den Markt, die auch im Straßenbahnbetrieb einen barrierefreien Einstieg ermöglichen. Seitdem haben sich auch Städte für einen Niederflurbetrieb entschlossen, die ein neues Metro-Netz aufbauen, darunter die spanischen Städte Sevilla, Málaga und Granada. Zu diesem Zwecke werden auch Niederflurfahrzeuge mit einer Gesamtlänge über 80 m gebaut, die im Mischbetrieb mit dem Straßenverkehr nicht eingesetzt werden dürfen.

Während in den Geburtsstädten der U-Bahn, London und Paris, die ersten Strecken von Beginn an unterirdisch gebaut wurden, legte man diese in anderen Städten oft als "Hochbahn" auf Viadukten an, bevor zunehmend Tunnelstrecken in den Verlauf eingefügt wurden, wie etwa in Liverpool, Chicago, Berlin, Hamburg, Wien und New York. Dies lag vor allem daran, dass die Konstrukteure noch keine Erfahrungen mit dem Tunnelbau unter schwierigen Bedingungen hatten. Aber auch heute gibt es Städte, die wegen des schlammigen Untergrundes nur wenige, dafür aber in der Realisierung sehr teure U-Bahnen haben. Das Problem ist der statische Auftrieb der hohlen, luftgefüllten Tunnel. Außerdem besteht die Gefahr des Absinkens von Straßen und Gebäuden. Deshalb gibt es in Städten mit hoch stehendem Grundwasser wie Glasgow, Amsterdam und Sankt Petersburg nur wenige U-Bahn-Linien. Die unterirdische Bauweise ist in felsigem Untergrund, wie z. B. in Stockholm, dagegen verhältnismäßig einfach zu realisieren.

Der Hauptvorteil der U-Bahn liegt in der Unabhängigkeit der Strecke durch Verlegung in eine andere Ebene. Durch Vermeidung sowohl von Kreuzungen mit Straßen als auch mit auf Straßen verlaufenden Schienen können Störungen des Betriebs auf ein Minimum reduziert werden. So wird dann auch, wie inzwischen öfters praktiziert, ein vollautomatischer Betrieb möglich. Vollautomatische U-Bahn-Strecken gibt es mittlerweile in Paris, Lyon, Rennes, Lausanne und Kopenhagen. Erste Versuche mit vollautomatischem Betrieb in Deutschland erfolgten in Berlin, Hamburg und Frankfurt am Main. Die erste vollautomatische U-Bahn in Deutschland im Normalbetrieb war die U3 in Nürnberg, die am 14. Juni 2008 eröffnet wurde. Sie fuhr – weltweit einmalig – bis 2009 auf einer Teilstrecke im Mischbetrieb mit konventionellen Zügen. Seit Anfang 2010 ist auch die Linie U2 auf vollautomatischen Betrieb umgestellt, es findet daher derzeit kein Mischbetrieb statt.

U-Bahnen zeichnen sich in der Regel durch eine dichte Taktfolge aus. Allerdings geht der Zeitvorteil auf kurzen Strecken bei tiefliegenden Stationen durch den Weg zum unterirdischen Bahnhof verloren. Ebenso ist Menschen mit Behinderungen, besonders bei älteren Systemen, die Benutzung oft nur erschwert oder gar nicht möglich. Nur nach und nach werden die U-Bahnhöfe behindertengerecht mit Aufzügen ausgestattet. So haben die Berliner Verkehrsbetriebe eine Rechnung aufgestellt, dass der Einbau eines Aufzugs ungefähr genauso viel kostet wie die Sanierung eines kompletten Bahnhofs. Die Nürnberger U-Bahn ist eines der wenigen Systeme, das inzwischen komplett barrierefrei erreicht werden kann. Generell wird bei Neubauten seit Ende der 1980er Jahre immer ein Aufzug mit eingeplant.

Die U-Bahnen sind nach der S-Bahn der leistungsfähigste Verkehrsträger im städtischen Verkehrsnetz. Pro Stunde können je Richtung 35.000 bis 40.000 Fahrgäste befördert werden (S-Bahn: 40.000 bis 50.000; zum Vergleich Pkw bei einem angenommenen Besetzungsgrad von 1,3: 2500 Personen pro Stunde und Fahrstreifen).

Die Energieversorgung erfolgt zwecks Verringerung des Tunnelquerschnitts häufig durch eine zwischen oder neben den Schienen liegende Stromschiene. Bei einigen U-Bahn-Systemen vor allem in Südeuropa besitzen die Züge dagegen Stromabnehmer auf dem Dach. Außerdem gibt es zahlreiche Sonderformen, allerlei Stromsysteme und -versorgungsmöglichkeiten. So wird in manchen Städten eine zweite Stromschiene zur Vermeidung von Streustromkorrosion eingesetzt. Auch bei den Stromschienen selbst existieren verschiedene Varianten. Die meistverbreitete Art ist die Bestreichung durch den Stromabnehmer von unten. Beim Berliner Kleinprofil und in London geschieht dies von oben, was jedoch ein größeres Sicherheitsrisiko darstellt. Die nach dem Zweiten Weltkrieg in Budapest gebauten Strecken sind ein Beispiel für von oben bestrichene Stromschienen, die trotzdem weitgehend abgedeckt sind.

Bei der Betriebsspannung hat sich inzwischen ein Bereich von 600 bis 900 Volt Gleichspannung etabliert, unabhängig davon, ob die Stromzufuhr über Stromschienen oder Oberleitungen erfolgt. So wird in Berlin beispielsweise bei 750 Volt gefahren, in allen Städten der früheren Sowjetunion bei 825 Volt. Dass die elektrische Spannung von U-Bahn-Systemen generell etwas höher als bei Straßenbahnen ist, könnte historisch bedingt sein, da die später als die Straßenbahnen eingeführten U-Bahn-Systeme jeweils auf einen höher entwickelten Standard der Stromversorgung und Elektromotorentechnik zugreifen konnten.

Es gibt bei den zahlreichen auf der Welt vorhandenen U-Bahn-Systemen verschiedene Netzformen. Die ersten U-Bahn-Netze bestanden aus Halbmesserlinien, die ihr Streckenende in der Innenstadt fanden, oder aber auch aus Durchmesserlinien, die diese querten. Dagegen entstanden beispielsweise die "Ringnetze" meistens auf gleich verlaufenden Ringstraßen. Eine Weiterentwicklung ist dabei das Ring-Radialen-Netz. Die "Sekantennetze" sind sehr typisch für U-Bahn-Systeme in Städten in ehemals realsozialistischen Ländern wie zum Beispiel in Minsk, Charkiw oder Prag. Diese Netztypen werden trotz des Zusammenbruchs der Sowjetunion noch immer weitergeplant und -gebaut. Vermaschte Netze entstehen meistens unter einem bereits vorhandenen Straßennetz wie zum Beispiel in New York oder Paris. Verständlicherweise weichen einige Streckennetze von diesen im Folgenden dargestellten Idealtypen ab oder stellen Mischtypen dar.

Charakteristikum zahlreicher U-Bahn-Netze ist ein reiner Linienbetrieb, was bedeutet, dass eine U-Bahn-Strecke ausschließlich durch eine Linie bedient wird. Hauptgrund dafür ist, dass der reine Linienbetrieb weniger Aufwand bei der technischen Sicherung verursacht als eine Strecke mit zwei Zweigstrecken. U-Bahn-Netze sind deshalb im Gegensatz zur S-Bahn oder auch Straßenbahn üblicherweise so ausgebildet, dass der Übergang eines Fahrzeuges zwischen Strecken zweier verschiedener Linien im normalen Fahrgastbetrieb in der Regel nicht beziehungsweise nur unter Nutzung nicht für den Fahrgastbetrieb bestimmter Gleisverbindungen (z. B. Kehrgleisen) möglich ist. Die Linienreinheit führt dazu, dass die Streckenführung einer U-Bahn-Linie auf langfristige Zeit bestehen bleibt und sich lediglich durch spätere Streckenverlängerungen verändert. Eine Neuverknüpfung von Streckenästen wie bei S-Bahnen oder Straßenbahnen ist bei U-Bahn-Netzen gewöhnlich mit umfangreichen baulichen Veränderungen verbunden. In diesem Punkt lassen S-Bahnen und Straßenbahnen mehr Flexibilität zu.

Vor allem kleinere U-Bahn-Systeme sind jedoch nicht konsequent linienrein. Hierbei handelt es sich zumeist um Strecken, die sich stadtauswärts in zwei Zweigstrecken aufspalten wie bspw. in Stockholm, Kopenhagen, Brüssel, München oder Bilbao. In der Regel überlagern sich dabei die Takte der Zweigstrecken durch Fahrplanabstimmungen und erlauben so eine dichtere Zugfolge auf der gemeinsam bedienten Teilstrecke. Dahinter steht zumeist die Erkenntnis, dass die in der Innenstadt benötigte dichte Taktfolge für die außerhalb der Innenstadt zu bedienenden Strecken unangemessen hoch ist. Hier wiegen die Vorteile eines verzweigten Netzes und entsprechend höherer Fahrgastzahlen die Nachteile der erhöhten technischen Sicherung auf. Jedoch werden auch in New York und in London sehr große Netze nicht linienrein betrieben.

Für U-Bahnen sind prinzipiell die gleichen Sicherheitseinrichtungen notwendig wie für alle Schienenfahrzeuge mit Personenbeförderung. Gegenüber der Eisenbahn sind jedoch die Risikofelder verschoben. Bei der Eisenbahn liegen die Risiken vorrangig bei der Streckenfahrt mit hohen Geschwindigkeiten auf einem offenen Gleiskörper. Demgegenüber sind die Gefahrenmomente bei der U-Bahn weit mehr in der „Bahnsteigsituation“ gegeben, wobei vor allem der Massenandrang in Stoßzeiten mit seinem intensiven Fahrgastwechsel eine Rolle spielt. Die überwiegende Ausführung in Tunnelstrecken führt dazu, dass auch das Stehenbleiben auf „freier“ Strecke wegen der engen Umschließung durch den Tunnel zu einem besonderen Risikofeld wird, vor allem wenn vom Zug zusätzlich Gefahrenmomente, etwa durch den Brand von Betriebseinrichtungen, ausgehen. Besonders heikel sind die engen Röhrentunnel in London, in denen ein Verlassen eines Zuges an den Seiten nicht möglich ist. Daneben bieten U-Bahnen und ihre Bahnhöfe mit ihrer leichten Zugänglichkeit und den zeitlich und räumlich kurzen Halteabständen in Ballungsräumen weit mehr als die Eisenbahn ein Feld für kriminelle Vorgänge, insbesondere in den Zeiten mit geringer Fahrgastfrequentierung, sowohl auf den Bahnsteigen als auch in den Fahrzeugen.

Seit der Eröffnung der London Underground als erste unterirdische Schnellbahn wurden viele Maßnahmen eingeführt, um eine höchstmögliche Sicherheit für die Fahrgäste zu gewährleisten. Zum Standard eines heutigen U-Bahn-Systems gehört neben der Abfahrtsansage auch ein Abfahrtssignal, das auch für ausländische Fahrgäste verständlich ist. Visuelle Abfahrtssignale, die insbesondere für Gehörlose gedacht sind, werden erst seit einigen Jahren nachgerüstet, während sie an anderen Orten bereits von Anfang an vorhanden waren.

Ein tödlicher Unfall in München und ein sehr ähnlicher Zwischenfall mit einer Verletzten in Nürnberg haben zu einer stärkeren Beachtung der Empfindlichkeit des Türschließmechanismus geführt. Die Fahrer hatten in den Türen eingeklemmte Personen nicht bemerkt. Auch die technischen Einrichtungen, die ein Abfahren in solchen Situationen verhindern sollten, haben nicht angesprochen. So wird bei der Münchener U-Bahn jetzt bei Neubauzügen die Türkante mit einem druckempfindlichen Sensor versehen, alte Züge wurden nachgerüstet. Ebenso werden die neu ausgelieferten DT3-Züge in Nürnberg mit entsprechend empfindlichen Türgummis ausgeliefert und die bereits im Betriebsdienst befindlichen Züge der Bauarten DT2 und DT1 (mit Ausnahme der DT1, die in Kürze ausgemustert werden sollen) werden mit den gleichen Türgummis und mit visuellen Türschließwarnungen nachgerüstet.

Ein weiteres Sicherheitsrisiko ist die Lücke zwischen Zug und Bahnsteig, die in einigen Fällen bis zu fünfzig Zentimeter breit ist; dies ist vor allem bei Bahnsteigen der Fall, die in engem Bogen liegen. An sich nicht unüberwindbar, ergibt sich das Risiko durch den Zeitdruck beim Ein- und Aussteigen sowie der Unübersichtlichkeit bei starkem Fahrgastandrang. Lösungen dafür sind beispielsweise das berühmte „Mind the Gap“ in London, das durch Ansagen und Schriftzügen auf dem Bahnsteigboden bis hin zu Plakatwänden mit der Aufschrift „The Gap kills!“ verdeutlicht wird. Eine Alternative ist die Anbringung von zusätzlichen Schiebetritten, wie sie bei den DT3-Zügen in Nürnberg vorhanden sind.

Gefahrensituationen können sich auch durch die Anwesenheit von einer Person auf den Gleisen ergeben. Sie nötigen den Fahrzeugführer zu Schnellbremsungen, die unter Umständen ein Unglück auch nicht mehr verhindern können. Um dagegen vorzugehen, wurden vor allem in asiatischen Städten und seit 1999 auch auf Neubaustrecken der London Underground Bahnsteigtüren installiert. Diese öffnen sich synchron mit den Zugtüren und können so ein unbeabsichtigtes „Auf-die-Gleise-Fallen“ vor dem Zug verhindern. Alternativ wurden in Kopenhagen auf den Hochbahnhöfen elektronische Sensorsysteme an den fahrerlosen Zügen angebracht, die Gefahrensituationen automatisch erkennen sollen und gegebenenfalls eine Schnellbremsung auslösen. In Nürnberg wurden auf den für führerlosen Betrieb vorgesehenen Strecken (U2 und U3) auf den Bahnhöfen Mikrowellenschranken montiert. Diese befinden sich unter der Bahnsteigkante und an der gegenüberliegenden Wand und sollen in den Gleisbereich fallende Gegenstände und Menschen ab 20 bis 30 Zentimeter Größe erkennen und Zwangsbremsungen von herannahenden führerlosen Zügen veranlassen. Bei der Realisierung des Systems gab es erhebliche zeitliche Verzögerungen.
Am 14. Juni 2008 nahm die U 3 den offiziellen Fahrgastbetrieb auf.

Die in Deutschland für U-Bahnen geltende Straßenbahn-Bau- und Betriebsordnung besagt in § 31:

Aufgrund von Sparzwängen und Rationalisierungsmaßnahmen vieler Verkehrsbetriebe gibt es inzwischen größtenteils weder die anfänglich noch üblichen Zugbegleiter noch die Bahnhofswärter, die auf den Stationen die Züge abfertigten und eine allgemeine Aufsicht führten. Ihre Funktion wurde größtenteils durch eine Überwachung mit Kameras übernommen. Für die allgemeine Sicherheit der Fahrgäste wurden beispielsweise in Berlin und Hamburg Sicherheitsrufsäulen errichtet, die eine direkte Sprechverbindung zur Leit- und Informationsstelle ermöglichen.
In London ist noch jede Haltestelle durchgehend mit mehreren Mitarbeitern besetzt, die den Betrieb koordinieren, Ansagen machen und die Züge abfertigen.

Durch das in den letzten Jahrzehnten in Mode gekommene Scratching und Graffiti sind die Fahrzeuge teilweise kaum noch wiederzuerkennen (siehe auch: Bahnfrevel). Auch für das allgemeine Sicherheitsgefühl der Fahrgäste werden in vielen Metrostädten ständige Kameraüberwachungen auch in den Fahrzeugen installiert. Gegen die Graffiti und das Scratching werden zudem auch Spezialfolien an den Fenstern verwendet sowie auf den Sitzpolsterungen ein „Würmchenmuster“, auf dem Graffiti nur schwer auffallen und deren Anbringung daher unattraktiv machen. Zusätzlich gibt es für den Fahrgast die von der Eisenbahn übernommenen Notbremsen, an denen meistens noch ein Notruf, das heißt eine direkte Sprechverbindung zum Fahrer, gekoppelt ist. Die Notbremse in modernen U-Bahn-Zügen sind in der Regel nur die ersten zehn Sekunden nach Anfahrt aktiv, danach bewirkt ein Auslösen der Notbremse nur noch eine Sprechverbindung zum Fahrzeugführer.

Um Gefahrensituationen im Tunnel zu begegnen, sind bei etlichen U-Bahnen zwischen den Stationen zusätzliche Notausgänge eingebaut sowie auch an den Tunnelwänden grafische Hinweise auf die günstigste „Fluchtrichtung“ angebracht. Die für U-Bahnen in Deutschland geltende Straßenbahn-Bau- und Betriebsordnung (BOStrab) fordert in § 30: „"Im Tunnel müssen ins Freie führende Notausstiege vorhanden und so angelegt sein, dass der Rettungsweg bis zum nächsten Bahnsteig, Notausstieg oder bis zur Tunnelmündung jeweils nicht mehr als 300 m lang ist. Notausstiege müssen auch an Tunnelenden vorhanden sein, wenn der nächste Notausstieg oder der nächste Bahnsteig mehr als 100 m entfernt ist."“

Nach einem Brand in der Berliner U-Bahn-Station "Deutsche Oper" wurden in Deutschland einige Tunnelstationen, die bislang nur auf einer Seite einen Ausgang hatten, auf der entgegengesetzten Seite mit einem weiteren Ausgang oder einem Notausgang versehen.

Ein großer Teil der Unfälle bei U-Bahnen geschieht absichtlich. Die leichte Zugänglichkeit von U-Bahnhöfen und Streckentunneln und die relativ hohe Geschwindigkeit eines in den Bahnhof einfahrenden Zuges werden immer wieder für Selbsttötungsversuche ausgenutzt. Entgegen der weitverbreiteten Annahme, ein Schienensuizid sei eine „sichere“ Suizidmethode, enden mehr als die Hälfte aller U-Bahn-Suizidversuche nicht tödlich. Wohl aber hat ein Suizidversuch auf der Schiene in aller Regel schwerste und bleibende Verletzungen zur Folge, in den meisten Fällen Invalidität durch abgetrennte Gliedmaßen wie Arme oder Beine.

Der Suizid auf der Schiene gehört zu den Suizidmethoden, die (über die eigenen Angehörigen hinaus) ganz erhebliche Folgen für Unbeteiligte haben. Ein Schienensuizid auf einer Eisenbahnstrecke außerhalb von Ortschaften zieht für den Lokführer meist eine schwere Traumatisierung nach sich. Auch für die Rettungskräfte geht das „Einsammeln“ weit verstreuter Leichenteile weit über das ihnen üblicherweise Zugemutete hinaus. Bei einer Selbsttötung in einer innerstädtischen U-Bahn-Station trifft das Beschriebene sogar auf einen noch größeren Personenkreis zu, weil auch wartende Fahrgäste zu unmittelbaren Augenzeugen des Suizids werden.

Häufig führt die Berichterstattung in Medien über Suizidversuche bei der U-Bahn zu Nachahmungstaten ("Werther-Effekt"). In Wien wurde daher bereits in den 1980er Jahren auf Grund einer freiwilligen Vereinbarung zwischen den Verkehrsbetrieben und den Medien darauf verzichtet, über Suizidversuche zu berichten, die Zahl der versuchten Suizide in der U-Bahn nahm daraufhin um 50 Prozent ab. Inzwischen wird dieses Modell auch in mehreren deutschen Städten (München, Hamburg) erfolgreich praktiziert.

Auf der ganzen Welt gibt es gut 140 U-Bahn-Systeme. Alle haben sich unterschiedlich entwickelt, auch wenn durchaus regional einige Parallelen zu erkennen sind. Manche stagnieren auf ihrem Eröffnungszustand, andere entwickeln sich rasant weiter.

Das längste U-Bahn-Netz war seit Ende Dezember 2017 das erst 1995 eröffnete Metronetz von Shanghai mit 637 Kilometern Länge und 16 (Ende 2009: 11) Linien. Am 29. April wurde für die am 1. Mai beginnende Expo 2010 die Linie 13 eröffnet. Die U-Bahn Peking ist mit 552 Kilometern (Stand Dezember 2015) zweitlängstes Netz. Das früher längste Metronetz der London Underground liegt mit 408 Kilometern und zwölf teilweise mehrfach verzweigten Linien im Jahr 2018 auf Platz drei. Darauf folgt die New York City Subway mit 398 Kilometern Länge. Dort gibt es 26 Linien, wobei auch Expresslinien mitgezählt sind. Weitere große Netze befinden sich in Moskau (317,5 km), Tokio (316 km), Seoul (286 km), Madrid (226 km) und Paris (220 km). Das größte deutsche Netz, das der Berliner U-Bahn (144 km), ist auf der Weltrangliste auf Platz 12 zu finden. (Dabei wird allerdings das 332 km lange S-Bahn-Netz nicht mitgezählt, während bei Tokio alle Systeme zählen.)

Ab dem 26. Dezember 2015 war das U-Bahn-Netz von Peking das längste der Welt. Zwei Jahre später wurde es im Dezember 2017 wieder längenmäßig von Shanghai übertroffen. Sowohl Shanghai wie auch Peking bauen ihre U-Bahn-Netze weiter aus.

Besonders tiefgelegene U-Bahnhöfe entstanden in den ehemals sozialistischen Staaten zur Zeit des Kalten Krieges, auch um als Schutzbunker bei einem möglichen Atomkrieg zu dienen.
So liegt die Budapester Metrolinie M 2 bis zu sechzig Meter unter der Oberfläche. Noch tiefer liegen Teile der U-Bahn-Netze von Moskau und Sankt Petersburg. Aktueller Tiefen-Rekordhalter ist derzeit der im Jahr 1960 eröffnete U-Bahnhof Arsenalna der Metro Kiew mit 105,5 m. Direkt danach folgt die im Jahr 2011 eröffnete St. Petersburger Station "Admiralteiskaja" der Linie 5 mit 102 m. Zuvor war die 2005 eröffnete Station "Komendantskij Prospekt" auf derselben Linie in 75 m Tiefe die zweittiefste U-Bahn-Station. In westlichen Staaten wurden U-Bahn-Stationen meist aus archäologischen (Athen, Rom) oder geologischen (Oslo, Washington) Gründen überdurchschnittlich tief verlegt. In Frankfurt am Main liegt die U-Bahn-Station Dom/Römer wegen einer darüberliegenden Tiefgarage auf 22 m Tiefe. In Rom wurden wegen der römischen Fundschicht (10 bis 20 m Dicke) mehrere Stationen im Zentrum auf 30 m Tiefe angelegt. Die Washingtoner Station Forest Glen liegt auf 60 m Tiefe und hat keine Rolltreppen, die bis zur Oberfläche führen, sondern ausschließlich Aufzüge. Die neue Linie 9 der Metro Barcelona, die seit 2014 betriebsbereit war, erhält mehrere Stationen auf bis zu 90 m Tiefe und ist damit die am tiefsten verlegte U-Bahn der Welt.

In der Moskauer Metro-Station Park Pobedy (Siegespark) befinden sich auch die vier längsten "ununterbrochenen" Rolltreppen der Welt mit jeweils 126 Metern Länge (für 63 m Höhenunterschied). Die weltweit längsten "mehrteiligen" Rolltreppen befinden sich in der U-Bahn-Station "Wheaton" von Washington D.C. Sie sind 155 m lang.

Hinsichtlich der Fahrgastzahlen gilt die Tokioter U-Bahn mit 3,17 Milliarden jährlich transportierten Fahrgästen (2008) als meistfrequentierte Metro der Welt. Ungefähr 2,39 Milliarden Mal werden die dunkelblauen Züge der russischen Hauptstadt Moskau jährlich benutzt (2009). Mit 2,05 Milliarden im Jahr 2009 liegt auf dem dritten Platz die Metro Seoul. Es folgen die New York City Subway mit 1,58 Milliarden (2009), die Metro Paris mit 1,53 Milliarden (2013), Peking mit 1,46 Milliarden (2009), Mexiko Stadt mit 1,41 Milliarden (2009), die Hong Kong MTR mit 1,32  Milliarden (2009), die Shanghaier Metro mit 1,3 Milliarden (2009) und als letzte Metro mit mehr als einer Milliarde beförderten Passagieren die London Underground mit 1,09 Milliarden beförderten Personen im Jahr 2008. Die Wiener U-Bahn beförderte 2009 etwa 510 Millionen Fahrgäste und lag damit knapp vor der Berliner U-Bahn, die 2009 etwa 509 Millionen Fahrgäste beförderte (bei Wien wie auch Berlin ohne Berücksichtigung der S-Bahn). Damit lagen die beiden Städte auf Rang 22 und 23. Die Berliner S-Bahn brachte es 2008 auf 387 Millionen Fahrgäste. U- und S-Bahn zusammengerechnet brächten Berlin mit 896 Millionen nach Sao Paulo und vor Osaka und St. Petersburg auf den 12. Rang. Anzumerken ist allerdings, dass die Zählweise weltweit nicht einheitlich ist. In Deutschland werden U- und S-Bahn getrennt ausgewiesen, in Paris addiert und in Tokio sogar beim Umsteigen zwischen unterschiedlichen Systemen doppelt gezählt.

Als schnellste U-Bahn-Linie der Welt gilt die gelbe Linie der Chicago ′L′. Sie schafft die 8,1 Kilometer Wegstrecke von "Dempster" nach "Howard" in sechseinhalb Minuten. Der Rekord ist allerdings gefährdet, sollten entlang der Strecke Zwischenhalte eingefügt werden.

Mit 141,2 km/h stellte ein New Yorker U-Bahn-Wagen vom Typ "R44" im Jahre 1972 einen Geschwindigkeitsrekord auf. Dabei wurde festgestellt, dass der Wagen gegen Ende der Teststrecke immer noch beschleunigte, sodass der Versuch vorzeitig abgebrochen werden musste, ohne die eigentlich mögliche Höchstgeschwindigkeit erreicht zu haben.

Als Stadt mit den meisten U-Bahn-Planungen der Welt gilt Wien. Es wird sogar von Plänen aus dem Jahr 1844 berichtet. Hingegen wurde das erste Teilstück der Warschauer U-Bahn erst 1995 eröffnet, obwohl die ersten Planungen bereits aus dem Jahre 1925 datieren. Eine weitere rekordverdächtige Zeitspanne zwischen Planung und Eröffnung zeichnet sich in New York ab: Die U-Bahn unter der 2. Avenue Manhattans ist seit 1929 fest geplant und dürfte nach derzeitigem Stand zwischen 2015 und 2020 fertiggestellt werden.

Die U-Bahn in Glasgow existiert bereits seit 1896. Sie ist aber seitdem nie erweitert worden und hält damit den Rekord des am längsten unveränderten U-Bahn-Netzes.

Den Rekord für die kürzeste Bauzeit einer längeren U-Bahn dürfte sich die Metro Dubai holen: die 52,1 Kilometer lange fahrerlose rote Linie (davon aber nur 4,7 Kilometer im Tunnel) wurde im September 2009 nach einer Bauzeit von nur 42 Monaten eröffnet. Außerdem ist die Strecke damit die zweitlängste einzelne, allerdings überwiegend oberirdisch verlaufende Metrostrecke der Welt.

Die längste rein unterirdisch verlaufende U-Bahn-Strecke Deutschlands ist mit 31,8 Kilometern Länge und 40 Stationen die der Linie U7 der Berliner U-Bahn, die längste Strecke insgesamt in Deutschland ist mit 55,8 Kilometern die Linie U1 der U-Bahn Hamburg. Die derzeit in Bau befindliche neue Strecke der Linie 9 der Metro Barcelona ist seit 2014, dem Jahr ihrer Eröffnung, mit 47,2 Kilometern und 50 Stationen die längste rein unterirdische Strecke der Welt.

In Deutschland bewältigt die Linie U3 zwischen Frankfurt am Main-Südbahnhof und Oberursel-Hohemark im Taunus 204 Meter Höhenunterschied und hält damit den Rekord in Deutschland. Den internationalen Rekord hält aber die Linie 1 der Teheraner U-Bahn mit mehr als 480 Metern, diese zur Gänze unterirdisch.

Wie schon unter "Rekorde" in Bezug auf die Moskauer Metro erwähnt, gibt es U-Bahnen, die aus kulturellen Gründen speziell gestaltet sind.

So zum Beispiel in München vor allem die Haltestellen Königsplatz (Kunst) und Tierpark (Tierwelt). In der Haltestelle Königsplatz (Kunst-Zentrum) gibt es nicht nur Anklänge an Gemälde, sondern auch an Skulpturen, die in Vitrinen auf dem Bahnsteig in Form von Kopien zu bewundern sind.

In Wien wiederum wurden während des Baues entdeckte Relikte in die Gestaltung der U-Bahn-Stationen mit einbezogen. So findet sich die vermutlich aus dem 13. Jahrhundert stammende Virgilkapelle in der U1-Station Stephansplatz direkt im Herzen der Stadt. Ebenso lässt sich in der U3-Station "Stubentor" ein Rest der ursprünglichen Wiener Stadtmauer entdecken. Weiters sind noch eine Menge weiterer Stationen, vor allem auf der Linie U3, künstlerisch ausgestaltet.

Bemerkenswert ist, dass zwar die Ausgestaltung der Moskauer Metro weltbekannt ist, andere Städte in der ehemaligen Sowjetunion in dieser Hinsicht jedoch absolut unbekannt sind. Zu der Unkenntnis hat wohl beigetragen, dass es vor der Wende verboten war, in diesen Metros Fotos zu machen.

So sind damals von der Sowjetunion in vielen Städten, zum Beispiel im damaligen Leningrad und in Minsk, die zentralen Stationen ebenso wie in Moskau als künstlerische Paläste gestaltet worden.

In Taschkent (Usbekistan) findet man eine Haltestelle, deren Decke sich auf Säulen stützt, die den Holzsäulen der usbekischen älteren islamischen Tempel nachempfunden sind. Nebenbei gibt es eine Haltestelle, die der Raumfahrt gewidmet ist, in tiefem Nachtblau als Grundton.

Der Künstler Martin Kippenberger errichtete ab 1993 das fiktive weltumspannende Ubahn-Netz Metro-Net.

"Siehe Liste der Städte mit U-Bahn oder Metros"

In Deutschland gibt es vier vollständig kreuzungsfreie U-Bahn-Netze. Als erste nahm die Berliner U-Bahn am 18. Februar 1902 ihren Betrieb auf. Nach dem Kleinprofil-Netz mit 2,30 Meter Wagenbreite wurde ab 1923 das Großprofil-Netz mit 2,65 Meter Wagenbreite in Betrieb genommen. Das Gesamtnetz besteht heute aus zehn Linien mit einer Gesamtlänge von 145 Kilometern. Die damals selbstständige Stadt Schöneberg eröffnete 1910 eine eigene U-Bahn-Strecke mit Umsteigemöglichkeit zum Berliner U-Bahn-Netz. Schienenmäßig wurde die heutige Linie U4 erst 1926 verbunden.

Im Februar 1912 folgte die Hamburger Hochbahn, deren heute insgesamt 104 Kilometer langes Streckennetz auf vier Linien mit einem Abzweig aufgeteilt ist.

1971 wurde das U-Bahn-Netz in München als drittes deutsches straßenkreuzungsfreies U-Bahn-System in Betrieb genommen. Die damals bevorstehenden Olympischen Spiele beschleunigten den Bau. Heute ist das Netz rund 103 Kilometer lang und besteht aus sechs Linien. Im Zentrum werden die Strecken von jeweils zwei Linien befahren.

Nur ein Jahr danach ging mit der U-Bahn Nürnberg das vierte und bisher jüngste und kleinste Netz mit 37,1 Kilometern Länge in Betrieb. 2008 wurde die dritte Linie mit einem fahrerlosen Betrieb eröffnet, die im Zentrum mit der U 2 zusammenläuft. Nürnberg ist weltweit die bisher einzige Stadt, in der ein Mischbetrieb mit fahrerlosen und fahrergeführten U-Bahn-Zügen existierte.

Die U-Bahn-Netze in Hamburg, München und Nürnberg reichen in benachbarte Städte hinein, die in Nürnberg verbindet sogar zwei Großstädte (Nürnberg und Fürth, bei einer Streckenführung, die in weiten Teilen derjenigen der ersten deutschen Eisenbahn entspricht). So besitzt etwa Garching bei München eigene U-Bahn-Strecken, deren Betrieb mit Hilfe komplizierter Vertrags- und Finanzierungsregelungen an das U-Bahn-Netz des Oberzentrums angeschlossen wird.

Viele andere deutsche Städte und Ballungsräume haben U-Bahn-ähnliche Systeme, deren Strecken außerhalb der Tunnel überwiegend nicht auf unabhängigen Bahnkörpern verlaufen und als Stadtbahn bezeichnet werden. Ihre besonderen Bahnkörper haben höhengleiche Straßenkreuzungen. Teilweise sind sogar noch straßenbündige Bahnkörper vorhanden, bei denen sich die Bahnen die Verkehrsfläche mit dem Individualverkehr teilen.

Zu diesen Systemen gehören etwa das Stadtbahnnetz Rhein-Ruhr, die Bahnen der Stadtbahn Köln, die Stadtbahn Hannover, die Stadtbahn Bonn, die Stadtbahn Stuttgart und die U-Bahn Frankfurt, deren erste Strecke am 4. Oktober 1968 eröffnet wurde. In Frankfurt war die U-Bahn-Linie U 4 bis zu ihrer Verlängerung erst nach Schäfflestraße und anschließend nach Enkheim im Juni 2008 eine vollständig straßenkreuzungsfreie U-Bahn.

Zu den Metros zählen auch die in vielen deutschen Ballungsräumen neben U-Bahnen für den Stadtverkehr betriebenen S-Bahnen, die etwa in Berlin, München, Frankfurt am Main, Leipzig, Stuttgart und Hamburg in der Innenstadt hohe Haltestellendichten und sogar längere Tunnelstrecken aufweisen.

Die einzige klassische U-Bahn Österreichs befindet sich in Wien, die Wiener U-Bahn. In Serfaus, Tirol, gibt es die Dorfbahn Serfaus, eine 1280 Meter lange unterirdische Luftkissenschwebebahn mit Seilantrieb auf über 1100 Meter Seehöhe. In Tirol verkehrt noch eine weitere Nahverkehrsbahn völlig unabhängig vom Individualverkehr: Die neue Hungerburgbahn in Innsbruck wird teilweise unterirdisch geführt. In Linz verkehrt seit 2004 die Straßenbahn auf einem 1,9 Kilometer langen unterirdischen Abschnitt mit drei unterirdischen Stationen, die zwar als "Mini-U-Bahn" bezeichnet wird, aber eher einer Stadtbahn entspricht. Sie trifft nun unterquerend den Hauptbahnhof der Westbahn. 2011 wurde eine weitere auf 1,3 Kilometern unterirdisch verlaufende Trasse vom Bahnhof nach Westen eröffnet.

In Graz existieren zwei unterirdische Straßenbahn-Haltestellen ("Brauhaus Puntigam" und "Hauptbahnhof"). Beide Haltestelle sind allerdings nach oben offen, um an teuren Brandschutzeinrichtungen zu sparen. Bereits in den 1990er-Jahren war der Bau einer U-Bahn mit drei Linien in der steierischen Landeshauptstadt geplant. Diese Pläne wurden allerdings verworfen, nachdem eine Machbarkeitsstudie zeigte, dass ein Ausbau des Straßenbahnnetzes sinnvoller wäre. Im April 2018 wurden die U-Bahn-Pläne in reduzierter Form erneut aufgegriffen. Dabei soll eine Ost-West-Verbindung von Eggenberg nach Sankt Leonhard errichtet werden, auf der vollautomatische Züge rollen sollen.

In Salzburg verläuft die Lokalbahn auf einer Länge von etwa 300 Metern im Bereich des Hauptbahnhofs unterirdisch und endet in der Tunnel-Station "Hauptbahnhof", wobei über eine Erweiterung durch den Stadtkern und dann weiter oberirdisch auf der nach Süden verlaufenden Alpenstraße nach Anif gestritten wird. Im April 2018 wurde die Verlängerung bis zum Mirabellplatz, die 2024 fertig gestellt sein soll, beschlossen.

In Lausanne ist die Zahnradbahn Lausanne-Ouchy in eine vollautomatische U-Bahn umgebaut und bis nach Epalinges verlängert worden. Die Eröffnung der "m2" (Linie 2 der Métro Lausanne) war im August 2008. Aufgrund der Steigungen sind die Züge nach Pariser Vorbild mit Luftreifen ausgestattet. Die Linie "m1" verkehrt nicht kreuzungsfrei; sie stellt damit keine U-Bahn dar, sondern eine Stadtbahn.

Darüber hinaus existierte bis in die 1970er Jahre die Planung eines U-Bahn-Netzes in Zürich, der größten Stadt der Eidgenossenschaft. Dabei kam es zwar zur teilweisen Verlegung einer Straßenbahntrasse in den Untergrund, doch lehnte 1973 das Zürcher Stimmvolk die Vorlage über eine U-Bahn mehrheitlich ab. Lediglich der S-Bahn-Tunnel, der auch zur Abstimmung stand, wurde später realisiert. Wenn auch keine U-Bahn von der Definition her, sind die zwei Tramlinien 7 und 9 der Verkehrsbetriebe Zürich (VBZ) auf rund 2,5 Kilometern Länge unterirdisch geführt (Tramtunnel Milchbuck–Schwamendingen).



Allgemein

Speziell:


</doc>
<doc id="11087" url="https://de.wikipedia.org/wiki?curid=11087" title="Edvard Beneš">
Edvard Beneš

Edvard Beneš [] (* 28. Mai 1884 in Kožlany, damals Kronland Böhmen; † 3. September 1948 in Sezimovo Ústí) war ein tschechoslowakischer Politiker (ČSNS), einer der Mitbegründer der Tschechoslowakei sowie tschechoslowakischer Außenminister (1918–1935), Ministerpräsident (1921–1922) und Staatspräsident (1935–1938 und 1945–1948 sowie 1940–1945 Präsident im Exil).

Edvard Beneš war das zehnte Kind eines Kleinbauern und wurde auf den Namen Eduard getauft (als Kind „Edek“ genannt), später änderte er seinen Namen auf Edvard. Nach seinem Studium in Prag und in Frankreich (Paris und Dijon) arbeitete Beneš zunächst als Hochschullehrer für Soziologie an der Karls-Universität Prag.

Während des Ersten Weltkriegs gründete Beneš zusammen mit anderen die tschechische anti-österreichische Widerstandsorganisation Maffie. Ab 1915 setzte er sich (zusammen mit vor allem Tomáš Garrigue Masaryk und dem Slowaken Milan Rastislav Štefánik) von Paris aus für tschechische und slowakische nationale Bestrebungen ein: Er hielt Vorlesungen zum Slawentum an der Sorbonne und war Mitbegründer und Generalsekretär des 1916 gegründeten Tschechoslowakischen Nationalausschusses (anfangs kurz „Tschechischer Nationalrat“ genannt).

Mit seinem Engagement für die tschechische Sache erwirkten er und andere im Frühjahr 1917 die Aufstellung der Tschechoslowakischen Legion. Sie erreichten, dass der Tschechoslowakische Nationalrat 1918 von Frankreich als alleiniger Vertreter des geplanten tschechoslowakischen Staates anerkannt wurde und ein Recht auf Mitsprache bei den Verhandlungen zum Vertrag von Versailles bekam.

Von 1918 bis 1935 war Beneš ununterbrochen Außenminister der ČSR unter Staatspräsident Tomáš Garrigue Masaryk, 1935 wurde er dessen Nachfolger. 1921–1922 war er auch Regierungschef.

Politisch beheimatet war er in der Tschechischen Volkssozialistischen Partei ("Československá Strana Národně Socialistická", kurz "ČSNS"), deren stellvertretender Vorsitzender er bis 1935 auch war. Zu dieser Partei stieß er mehr oder weniger unfreiwillig, da er vom Kabinett von 1923 als Außenminister nur geduldet wurde, wenn er seine politische Unabhängigkeit formal aufgab und einer der Parlamentsparteien beitrat. Sie besteht bis heute und hatte zu keiner Zeit Verbindungen zum deutschen Nationalsozialismus.

Hinsichtlich der tschechisch-slowakischen Beziehungen zählte er zu den führenden Verfechtern des Tschechoslowakismus. So erklärte er 1943 im Londoner Exil:

Beneš war gegen die kommunistische Oktoberrevolution in Russland 1917 und orientierte die tschechoslowakische Politik als Außenminister eher anti-sowjetisch und neoslawistisch. Er war sich allerdings der Notwendigkeit der Zusammenarbeit mit der Sowjetunion bewusst. Nachdem 1933 Adolf Hitler an die Macht gekommen war, erkannte die Tschechoslowakei am 9. Juni 1934 unter der Führung von Beneš (und mit der Zustimmung Frankreichs) die UdSSR de jure als Staat an und schloss 1935 einen Freundschaftsvertrag mit ihr. Seinen westlichen Verbündeten, insbesondere Frankreich, war dieser Vertragsschluss jedoch suspekt.

Als Hitler nach dem „Anschluss Österreichs“ im März 1938 auch die Eingliederung der Sudetengebiete forderte, ließ Beneš die tschechoslowakische Armee mobilisieren und hoffte auf Unterstützung Frankreichs, mit dem seit Januar 1924 ein Bündnis bestand, und der Bündnispartner aus der Kleinen Entente im Falle eines deutschen Angriffs. Im September 1938 schlug Beneš in einem internen Schreiben an seinen Gesundheitsminister Nečas in Paris vor, Deutschland einen Teil des Sudetenlandes abzutreten (rund 5.000 von 28.000 Quadratkilometern, also ca. 18 Prozent) und gleichzeitig einen großen Teil der in der Tschechoslowakei verbleibenden deutschsprachigen Bevölkerung (nach Beneš’ überschlägigen Berechnungen etwa 2,2 Mio. Personen) zwangsauszusiedeln. Großbritannien und Frankreich verweigerten nach anfänglichen Zusagen ihre Zustimmung zu diesem Plan. Stattdessen wurde Hitler im Münchner Abkommen (September 1938) das Sudetenland zugestanden, in der Hoffnung, einen Krieg zu vermeiden.

Nach dem Münchner Abkommen lehnte Beneš das Angebot militärischer Hilfe seitens der Sowjetunion als unrealistisch ab.

Beneš verschaffte, vor allem während seiner Zeit als Staatspräsident, vielen von den Nationalsozialisten verfolgten Deutschen und Österreichern Pässe, mit deren Hilfe sie nach Übersee emigrieren konnten.

Am 5. Oktober 1938 trat Beneš zurück und flog einige Tage später nach London. Seinem Nachfolger Emil Hácha gratulierte er kurz darauf zu dessen Wahl.

Nach einiger Zeit als Privatperson im Exil gründete Edvard Beneš dort im Jahre 1940 die Tschechoslowakische Exilregierung und beanspruchte das Präsidentenamt wieder für sich. Im Laufe des Zweiten Weltkrieges wurde Beneš von den Alliierten schließlich als tschechoslowakischer Präsident anerkannt. Intensiv arbeitete er nun auf die Wiederherstellung der Tschechoslowakei in den Grenzen vor dem Münchner Abkommen und der möglichst vollständigen Vertreibung der insgesamt 3,4 Millionen Deutschen hin. Bei einem Gespräch mit US-Präsident Franklin D. Roosevelt am 12. März 1943 ermächtigte Roosevelt seinen Gast – so jedenfalls die Darstellung von Beneš – zur Ausweisung der Deutschen aus der Tschechoslowakei nach Ende des Krieges. In einer von Großbritannien aus gesendeten Rundfunkansprache erklärte Beneš am 27. Oktober 1943:

Infolge seiner Enttäuschung über die 1938 ausgebliebene Unterstützung durch die Westmächte näherte sich Beneš ab 1943 zunehmend an die Sowjetunion als dem wichtigsten Garanten für eine Wiedererrichtung des tschechoslowakischen Staates an. Nachdem die Niederlage Deutschlands im Osten absehbar wurde, unterzeichnete er am 12. Dezember 1943 in Moskau mit Stalin einen tschechoslowakisch-sowjetischen Beistandsvertrag, der auch eine enge Zusammenarbeit in der Nachkriegszeit festlegte. Bei diesem Treffen stimmte Stalin den Plänen Beneš’ zur Vertreibung der Sudeten- und Karpatendeutschen sowie einer teilweisen Vertreibung und Enteignung der 720.000 Ungarn in der Südslowakei zu. Nachfolgend wurde Beneš zu einem der deutlichsten Befürworter von Stalins Absichten einer Expansion der Sowjetunion nach Westen. Er begrüßte die polnische Westverschiebung, da dadurch Deutschland verkleinert wurde, und sicherte Stalin die Karpatenukraine zu. In Moskau vereinbarte Beneš mit den Kommunisten und Linkssozialisten unter Klement Gottwald die Errichtung einer Nationalen Front, bei der die anderen Parteien der Ersten Republik ausgeschlossen blieben. Im März 1945 reiste er erneut nach Moskau und führte Verhandlungen mit Gottwald über eine Beteiligung der Moskauer Gruppe an seiner Regierung, bei denen er umfangreiche Zugeständnisse machte.

Das im Ergebnis seiner Moskauer Verhandlungen erarbeitete "Kaschauer Programm" wurde am 5. April 1945 in Košice, dem provisorischen Sitz der Regierung der Nationalen Front, durch den Ministerpräsidenten Zdeněk Fierlinger verkündet. Darin wurden unter anderem das Verbot der konservativen Parteien der Ersten Republik, eine Wiederangliederung der Slowakei unter Wahrung von Autonomie, die Aussiedlung von Bürgern deutscher und ungarischer Nationalität, die die Nationalsozialisten unterstützt hatten, die Verstaatlichung des Großgrundbesitzes und von Industrieunternehmen und Banken, die Bestrafung von Kollaborateuren und eine Zusammenarbeit mit der Sowjetunion erklärt.

Im Mai 1945 kehrte Beneš aus der Sowjetunion in seine Heimat zurück und übernahm wieder das Amt des Staatspräsidenten. Die Schaffung eines einheitlichen tschechoslowakischen Nationalstaates blieb der Dreh- und Angelpunkt seines politischen Programms. Am 29. Juni 1945 unterzeichnete Edvard Beneš die Abtretung der Karpatenukraine an die Sowjetunion.

Unmittelbar nach seiner Rückkehr nach Prag am 16. Mai 1945 verkündete er einer begeisterten Menge auf dem Altstädter Ring:

In seiner Rede in Mělník am 14. Oktober 1945 hat Beneš jedoch andererseits dann erklärt:

Auf der Potsdamer Konferenz (Abschluss am 2. August 1945) stimmten die drei alliierten Siegermächte USA, Großbritannien und UdSSR der Überführung („Transfer“) der Ost- und Sudetendeutschen in „ordnungsgemäßer und humaner Weise“ zu. Auch sollten die Sudetendeutschen wie auch die Ungarn als „Kollaboranten und Verräter“ und unerwünschte Ethnien entschädigungslos enteignet werden. Deutsche, die während der deutschen Besatzung als Bürger loyal zur Tschechoslowakei gestanden hatten, sollten nach dem "Kaschauer Programm" von der Enteignung und anderen Repressionsmaßnahmen unbehelligt bleiben. Ein Teil der im Oktober 1945 erlassenen Beneš-Dekrete bestimmte nicht nur die teilweise Verstaatlichung der tschechoslowakischen Wirtschaft, sondern auch eine über das Kaschauer Programm hinausgehende generelle Enteignung und Vertreibung der Deutschen bis auf wenige Ausnahmen.

Am 25. Februar 1948, bereits schwer krank, nahm Beneš unter Druck das Rücktrittsangebot der nichtkommunistischen Minister an und ermöglichte damit die Machtergreifung durch die Kommunisten. Im Mai 1948 verweigerte er noch die Unterschrift unter die neue kommunistische Verfassung, am 7. Juni 1948 trat er zurück.

Sein Nachfolger wurde Klement Gottwald.

Beneš war verheiratet mit Hana Benešová (geborene Anna Vlčková), mit der er seit der Jugendzeit in Paris bekannt war.

Die American Philosophical Society zeichnete ihn 1939 für seine "Penrose Memorial Lecture" mit dem Titel „Politics as Art and Science“ mit ihrer Benjamin Franklin Medal aus.

Während für Edvard Beneš bis 1989 kein Denkmal errichtet wurde, setzte seine öffentliche Verehrung verstärkt nach der Samtenen Revolution ein. So steht seit Mai 2005 ein überlebensgroßes Standbild von Karel Dvořák (1893–1950) für Edvard Beneš auf dem "Loretánské náměstí" (Loretoplatz), auf dem Prager Hradschin unmittelbar gegenüber dem Außenministerium im Stadtzentrum von Prag ("Praha 1"). In seinem Sommersitz und Sterbeort Sezimovo Ústí wurde im Oktober 2001 eine Gedenkstätte für ihn eröffnet. Ferner wurden zahlreiche Straßen, Brücken und Plätze nach ihm benannt.

Von Jiří Gruša wurde 2011 die "zweifache Kapitulation" Beneš’ vor Hitler und vor Stalin thematisiert. Er bezeichnete Beneš als „das tschechische Enigma“ (Rätsel).




</doc>
<doc id="11089" url="https://de.wikipedia.org/wiki?curid=11089" title="Metro">
Metro

Metro (aus französisch ', Kurzform von ' = ‚Stadtbahn‘) steht für:

Metro ist der Eigenname von:

EDV:

Börsennotierter deutscher Handelskonzern:
Weitere Unternehmen:
Medien:

Kultur:

Wirtschaft:

Siehe auch:


</doc>
<doc id="11096" url="https://de.wikipedia.org/wiki?curid=11096" title="Sofja Wassiljewna Kowalewskaja">
Sofja Wassiljewna Kowalewskaja

Sofja Wassiljewna Kowalewskaja (, wiss. Transliteration ; *  in Moskau; † in Stockholm) war eine russische Mathematikerin, die 1884 an der Universität Stockholm die weltweit erste Professorin für Mathematik wurde, die selbst Vorlesungen hielt.

Zu "Sofja Kowalewskaja" gibt es viele verschiedene Namensversionen: In englischen Arbeiten heißt sie meistens "Sofia Kovalevskaia" oder "Kovalevskaya". Weil in den westeuropäischen Ländern unbekannt war, dass es in den slawischen Ländern auch eine weibliche Form des Nachnamens gibt, wird sie in Westeuropa bis heute häufig unter dem Namen ihres Mannes "Kowalewski" (auch "Kowalewsky" oder "Kovalewsky") geführt; ihr Vorname wurde in Deutschland zumeist zu "Sonja", in Frankreich zu "Sophie". Ihre in deutscher Sprache verfasste Dissertation veröffentlichte sie unter dem Namen "Sophie von Kowalevsky geb. von Corvin-Krukovskoy".

Kowalewskaja leistete nicht nur in der Mathematik Bedeutendes, sondern hatte auch mit ihren 1889 erstmals erschienenen "Kindheitserinnerungen" großen Erfolg. Politisch war sie ebenfalls aktiv und setzte sich für das Recht aller Frauen auf Ausbildung ein.

Geboren wurde Kowalewskaja als zweite Tochter von Elisabeth Fjodorowna Schubert (1820–1879) und General Wassili Wassiljewitsch Krukowski (1800–1874, auch Corwin-Krukowski). Ihre Mutter war eine gebildete Frau, die den zwanzig Jahre älteren Artillerie-Offizier der Kaiserlich Russischen Armee und Gutsbesitzer geheiratet hatte, um ihrem Elternhaus zu entfliehen. Sie war die Tochter des deutschstämmigen Offiziers in russischen Diensten, Militärkartografen und Geodäten Friedrich Theodor Schubert (* 1789 in Sankt Petersburg, † 1865), und dieser wiederum der Sohn von Friedrich Theodor von Schubert.

Wie im damaligen Russland in ihrer Schicht üblich, wurde Sofja direkt nach der Geburt in die Obhut einer Kinderfrau gegeben, die sich um ihre Erziehung kümmerte. Ihre Eltern sah sie nur zu den Mahlzeiten, und auch mit ihren Geschwistern hatte sie aufgrund des Altersunterschiedes (ihre Schwester Anna Corwin-Krukowski (1844–1887) war sechs Jahre älter, ihr Bruder Fjodor fünf Jahre jünger) in der Kindheit nicht viel Kontakt. Anna wurde aber später ihre engste Vertraute – ihr verdankt sie den Kontakt mit einer intellektuellen Jugendbewegung in Russland, den sog. Nihilisten, die auch für die Befreiung der Frau kämpften und Sofja schließlich ihren Traum vom Studium im Ausland erfüllen halfen.

Als Sofja etwa acht Jahre alt war, nahm ihr Vater seinen Abschied von der Armee und zog mit der Familie auf das Landgut Palibino (heute in der russischen Oblast Pskow). Hier bekam sie auch eine neue Gouvernante: Miss Smith aus England, eine resolute Frau, die von nun an für Sofjas Erziehung und Ausbildung verantwortlich war.

Sofjas Interesse für Mathematik entstand unter anderem durch mathematische Dokumente in ihrer häuslichen Umgebung. Als das Gut Palibino renoviert wurde, reichte die Tapete für das Kinderzimmer nicht mehr aus. Daher wurden die Wände dieses Zimmers mit Papier beklebt, das man auf dem Dachboden des Hauses gefunden hatte. So wurden die Wände von Sofjas Zimmer mit dem Skript einer Vorlesung von Michail Ostrogradski über Differential- und Integralrechnung, die ihr Vater in seiner Jugend gehört hatte, tapeziert. Mit diesen Skripten beschäftigte sie sich intensiv.

Gefördert wurde ihr Interesse an der Mathematik besonders durch einen ihrer Onkel väterlicherseits, der gerne las und darüber sprach. Er hatte sich als Nichtmathematiker autodidaktisch Fachwissen angeeignet. Sofja hörte ihm bei seinen mathematischen Ausführungen zu und entwickelte Interesse dafür. So hörte sie zum ersten Mal von der „Quadratur des Kreises“ und von Asymptoten, „auf die eine Kurve beständig zuläuft, um sie doch erst im Unendlichen zu berühren“.

Der elementare mathematische Unterricht, den sie bei ihrem polnischen Hauslehrer erhielt, erschien ihr daher zunächst langweilig. Als ihr Interesse an Algebra und Geometrie schließlich zunahm, verbot ihr ihr Vater den Mathematikunterricht. Sie ging weiterhin heimlich ihrem Interesse nach.

Mit fünfzehn Jahren las sie in einem Physikbuch, das ein Nachbar, Professor Tyrtow, geschrieben hatte. Die trigonometrischen Formeln im Kapitel Optik interpretierte und erfasste sie selbstständig. Nach ihrer Erläuterung der Interpretation zum Sinus dem Verfasser gegenüber setzte sich dieser dafür ein, dass Sofja Unterricht in höherer Mathematik bekommen sollte.

So konnte sich Sofja schließlich gegen ihren Vater durchsetzen und erhielt Unterricht bei Professor Strannolubski in Petersburg. In Sankt Petersburg traf sie auch Dostojewski, für den sie eine schwärmerische Neigung empfand, wie sie in ihren Memoiren schrieb, der sich aber selbst zu ihrer Schwester Anna hingezogen fühlte. Anna hatte in der Zeitschrift von Dostojewski ihre erste Erzählung veröffentlicht und besuchte diesen in Sankt Petersburg.

Zu dieser Zeit durften Frauen in Russland weder studieren noch als Gasthörerinnen an Vorlesungen teilnehmen und planten deshalb oft ein Studium im als fortschrittlich geltenden Westen. In Russland, das damals gesellschaftlich und politisch im europäischen Vergleich relativ rückständig war, herrschten bei vielen jungen Frauen überzogene Vorstellungen von der Gleichberechtigung der Frauen im Westen. Viele Russinnen, die deshalb ihr Heimatland verließen, mussten daher im Westen Vorreiterrollen übernehmen.

Eine Reise nach Westeuropa war nicht einfach, denn russische Frauen besaßen zu dieser Zeit keinen eigenen Reisepass. Eine Auslandsreise war ihnen nur in Begleitung des Vaters oder eines Ehemanns möglich, in dessen Pass sie eingetragen wurden. Da Sofja Kowalewskaja unbedingt Mathematik und Naturwissenschaften studieren wollte, setzte sie sich gegen den Willen ihres Vaters durch und ging im September 1868 mit dem Studenten Wladimir Onufrijewitsch Kowalewski (1842–1883), einem Anhänger der Nihilisten, eine Scheinehe ein. Im April 1869 reisten beide nach Wien, wo Kowalewski Geologie studieren wollte. Er wurde später ein bekannter Paläontologe.

Die Ehe war als reine Zweckehe gedacht, doch im Laufe ihres Lebens gab es immer wieder Zeiten, in denen Kowalewskaja mit ihrem Mann zusammen wohnte und lebte; diese wechselten mit Zeiten, in denen die Eheleute getrennt lebten oder sogar an Scheidung dachten.

Kowalewskaja erhielt zwar in Wien von einem Physikprofessor die Genehmigung, an seinen Vorlesungen teilzunehmen. Allerdings war ihr Wien zu teuer, und weil es ihr dort auch sonst nicht so gut gefiel, beschloss sie, nach Heidelberg zu gehen. Hier musste sie feststellen, dass Frauen eine Immatrikulation nicht gestattet war. Erst nach persönlichen Gesprächen mit einzelnen Professoren der Mathematik und Physik konnte sie schließlich ihr Studium an der Ruprecht-Karls-Universität Heidelberg zum Sommersemester 1869 aufnehmen – wenn auch nur als Gasthörerin. Sie hörte Mathematik bei Paul du Bois-Reymond und Leo Koenigsberger, Physik bei Hermann von Helmholtz und Gustav Kirchhoff und Chemie bei Robert Wilhelm Bunsen. Zusammen mit ihrer Schwester Anja und ihrem Mann wohnte Kowalewskaja während der Heidelberger Studienjahre in der Unteren Straße der Heidelberger Altstadt.

Zum Wintersemester 1870 wechselte Sofja Kowalewskaja auf Anraten von Professor Koenigsberger nach Berlin zu Karl Weierstraß, einem der bedeutendsten Mathematiker der damaligen Zeit.

Trotz guter Empfehlungsschreiben ihrer Heidelberger Professoren prüfte Weierstraß sie zunächst, indem er ihr eine schwere Aufgabe stellte. Eine Woche später zeigte sie ihm ihre Lösung, von der er so beeindruckt war, dass er sich von nun an für Kowalewskaja einsetzte. Doch gegen die konservative Verwaltung konnte auch er nichts ausrichten. So bot er ihr schließlich Privatstunden an.

Vier Jahre lang studierte sie in Berlin. Einmal in der Woche besuchte ihr Lehrer sie in ihrer kleinen Wohnung, am Sonntag besuchte sie ihn. So entstand ein enges Verhältnis zwischen den beiden, das wohl weit über eine normale Lehrer-Schüler-Beziehung hinausging.

Dazwischen war Sofja Kowalewskaja April bis Mai 1871 mit ihrem Mann in Paris, da sie sich um ihre Schwester Anna sorgte, die mit ihrem Mann Victor Jaclard, einem Offizier der Nationalgarde, aktiv auf Seiten der Aufständischen in der Pariser Kommune war. Nach der Niederschlagung der Kommune eilten sie wieder nach Paris, wo ihre Schwester zwar aus Paris fliehen konnte, ihr Mann aber inhaftiert war. Dieser konnte schließlich auch entkommen oder wurde (nach anderen Berichten) auf Intervention ihres Vaters General Korwin-Krukowski, den die Schwestern dafür einspannten, bei Adolphe Thiers befreit. In der Zeit in Paris versorgte Kowalewskaja auch Verwundete im Hospital, nahm aber nicht aktiv am Aufstand teil.

Nachdem Kowalewskaja ihren Lehrer Weierstraß vom unkonventionellen Charakter ihrer Ehe unterrichtet hatte, unterstützte er sie bei ihrer Dissertation, an der sie ab November 1872 arbeitete  – überwiegend in ihrer kleinen Wohnung, manchmal bis zu sechzehn Stunden am Tag. Sie verließ das Haus nur selten und schien kein Interesse mehr für irgendetwas anderes als die Mathematik zu haben. So fertigte sie bis zum Sommer 1874 drei Arbeiten an, die sie als Doktorarbeit einreichen konnte.

Komplizierter als die Anfertigung der Arbeiten selbst gestaltete sich die Suche nach einer Universität, an der Kowalewskaja promoviert werden konnte. Schließlich entschied sich Weierstraß für die Universität Göttingen. Obwohl er selbst das Frauenstudium nicht unterstützte, setzte er sich für Kowalewskaja ein und erreichte letztlich, dass sie "in absentia" (ohne mündliche Prüfungen) promovieren konnte.

Ernst Schering, der ihre Arbeiten ("Theorie der partiellen Differentialgleichungen", "Gestalt der Saturnringe" und "Klassen abelscher Integrale") begutachtete, stellte fest, dass alle drei mit viel Sachkenntnis und Fleiß erstellt wurden und schon eine von ihnen für die Doktorwürde ausreichen würde. Im August 1874 erhielt sie ihren Titel "summa cum laude".

Nach ihrer Promotion reiste Kowalewskaja nach Hause. Sie wollte in Russland unterrichten, hätte aber dafür ein russisches Magisterexamen machen müssen. Da sie als Frau nicht zur Universität zugelassen wurde, konnte sie auch keine Prüfung ablegen. Die einzige Möglichkeit zu unterrichten wäre in den unteren Klassen von Mädchenschulen gewesen.

Nicht nur aus diesem Grund wendete sie sich von der Mathematik ab. Sie versuchte nun ein normales Leben zu führen, wohnte wieder mit ihrem Ehemann zusammen und versuchte sogar, eine konventionelle Ehefrau zu werden. Um finanziell unabhängig zu werden, verstrickte sie sich mit ihrem Mann in riskante Grundstücksspekulationen, welche die Familie an den Rand des Ruins brachten. Am 17. Oktober 1878 brachte sie ihre Tochter zur Welt, die auch auf den Namen Sofja getauft, aber allgemein Fufa gerufen wurde.

1880 beschloss Kowalewskaja, sich wieder der Mathematik zuzuwenden. Da sie in Russland immer noch keine Stelle finden konnte, kehrte sie zur Forschung zurück. Sie übersetzte ihre dritte Dissertation, die sie noch nicht veröffentlicht hatte, ins Russische und trug sie Anfang 1880 auf dem 6. Kongress der Naturforscher und Ärzte vor.

Obwohl die Ergebnisse schon sechs Jahre alt waren, waren sie noch nicht überholt.

Um ihren Gläubigern zu entkommen, zog sie im selben Jahr mit ihrem Mann und ihrer Tochter nach Moskau und besuchte dort regelmäßig die Veranstaltungen der Moskauer Mathematischen Gesellschaft. Sie wurde wieder so von der Mathematik in Bann gezogen, dass sie beschloss, für zwei Monate nach Berlin zu reisen, um Anschluss an die aktuelle Forschung zu finden. Weil sie ihm nicht mehr helfen konnte, verließ sie im März 1881 ihren Mann, der inzwischen ins Ölgeschäft eingestiegen war und sich finanziell völlig ruiniert hatte.

Mit ihrer kleinen Tochter machte sie sich nun auf den Weg nach Berlin, wo sie sich gleich wieder in die Arbeit stürzte. Ende des Jahres zog sie nach Paris. Ihre Tochter wurde zusammen mit ihrer Kinderfrau zurück nach Russland gebracht und wuchs dann bei Julija Lermontowa auf, einer guten Freundin von Kowalewskaja, die sie auch schon während ihres Studiums in Heidelberg und Berlin begleitet hatte.

Im Mai 1882 besuchte der schwedische Mathematiker Gösta Mittag-Leffler, ein Schüler von Weierstraß, Kowalewskaja in Paris und stellte sie den wichtigsten französischen Mathematikern vor. Bereits im Juli desselben Jahres wurde sie von ihnen in die Pariser Mathematische Gesellschaft gewählt. Ein Jahr später trug sie erneut eine Arbeit auf dem 7. Kongress der Naturforscher und Ärzte vor. Nachdem ihr Mann im April 1883 Selbstmord begangen hatte – was Kowalewskaja sehr getroffen hatte –, besaß sie nun den respektablen Status einer Witwe. Gösta Mittag-Leffler, der sich schon monatelang erfolglos um eine Stelle für sie bemüht hatte (für eine getrennt von ihrem Mann lebende Frau war das zu dieser Zeit ganz unmöglich), konnte ihr nun eine Stelle als Privatdozentin an der Universität Stockholm anbieten.

Ihre Ankunft in Stockholm Ende 1883 wurde in allen Zeitungen Schwedens erwähnt. So ungewöhnlich war es, dass eine Frau eine Dozentur erhielt und dafür auch noch in ein ihr völlig fremdes Land ging.

In einem 1884 erschienenen Artikel von August Strindberg hieß es, dass „eine Frau als Mathematikprofessor eine schädliche und unangenehme Erscheinung sei, ja, daß man sie sogar ein Scheusal nennen könnte. Die Einladung dieser Frau nach Schweden, das an und für sich männliche Professoren genug habe, die sie an Kenntnissen bei weitem überträfen, sei nur durch die Höflichkeit der Schweden dem weiblichen Geschlecht gegenüber zu erklären.“

Kowalewskaja aber ließ sich von solchen Angriffen nicht entmutigen. Im ersten Semester hielt sie ihre Vorträge noch auf Deutsch, im nächsten Semester bereits auf Schwedisch.

Mittag-Leffler, Herausgeber der einzigen mathematischen Zeitschrift für Skandinavien, beauftragte sie mit der Beschaffung mathematischer Artikel von russischen, aber auch deutschen und französischen Mathematikern. 1884 wurde sie Mitherausgeberin und damit die erste Frau, die zum Herausgeberstab einer wissenschaftlichen Zeitung gehörte. Im Sommer desselben Jahres erhielt sie durch den Einsatz von Mittag-Leffler – gegen den Widerstand vieler Professoren der nicht-naturwissenschaftlichen Fächer – eine ordentliche Professur in Stockholm, zunächst allerdings auf fünf Jahre befristet. Sie erhielt zwar kein großes Gehalt, war aber die erste Professorin in Europa seit Laura Bassi (1711–1778) und Maria Gaetana Agnesi (1718–1799).

Ende 1887 lernte Kowalewskaja Alfred Nobel kennen. Dieser machte ihr zwar den Hof, allerdings kam es nicht zu einer Affäre. Bis heute hält sich hartnäckig das Gerücht, es gebe keinen Nobelpreis für Mathematik, weil Sofja Kowalewskaja eine Liaison mit Nobel gehabt und ihn wegen Gösta Mittag-Leffler verlassen habe. Für dieses Gerücht gibt es keine reale Grundlage, denn auch mit Mittag-Leffler hatte Sofja Kowalewskaja keine Beziehung. Es steht eher zu vermuten, dass für Nobel – der Arbeiten auszeichnen wollte, die einen „Nutzen für die Menschheit“ haben – dieser Nutzen in der Mathematik nicht unmittelbar erkennbar war.

1886 gelang Kowalewskaja die Lösung eines Spezialfalles des Problems der Rotation fester Körper um einen Fixpunkt. So wurde der nächste Bordin-Preis der Académie des sciences (für das Jahr 1888) – einer ihrer renommiertesten Preise – für einen Beitrag "zur Theorie der Bewegung eines starren Körpers um einen festen Punkt" ausgeschrieben, siehe Kowalewskaja-Kreisel. Das bedeutete für Kowalewskaja die Möglichkeit, diesen mit 3000 Franc dotierten Preis zu gewinnen. Die Tatsache, dass die Preisausschreibung speziell auf Kowalewskajas Arbeitsthema zugeschnitten wurde, zeigt, wie sehr sie von ihren Mathematikerkollegen in der ganzen Welt unterstützt wurde. Die Menschen, die ihr in ihrem Leben Steine in den Weg legten und an ihren Fähigkeiten zweifelten, waren in der Regel fachfremde Professoren oder ganz Außenstehende.

Im Mai 1887 starb Anna, Kowalewskajas Schwester, nach langer Krankheit. Kowalewskaja war in dieser Zeit so viel wie möglich bei ihr. Nach ihrem Tod schrieb Kowalewskaja:

Diesem Ausspruch folgend vertiefte sie sich ganz in die letzte Ausarbeitung ihrer Arbeit für den Bordin-Preis.

Die Arbeiten für diesen Preis mussten anonym eingereicht werden; die Namen der Einsender wurden erst nach der Entscheidung über die Preisvergabe bekannt gegeben. Kowalewskajas Arbeit wurde ausgewählt und für so gut erachtet, dass das Preisgeld auf 5000 Franc erhöht wurde.

Nach der Verleihung des Bordin-Preises begann Kowalewskaja mit der Niederschrift ihrer Kindheitserinnerungen. Das Buch erschien Weihnachten 1889 in Schweden und war sofort ein großer Erfolg.

Als 1889 ihre Professur auslief, bemühte sie sich in Frankreich sowie in Russland um eine Stelle. In Stockholm setzte sich Mittag-Leffler erneut für sie ein und erreichte, dass ihr im Juni 1889 eine Professur auf Lebenszeit übertragen wurde. In Frankreich wurde sie zum "Officier de l'Instruction publique" ernannt, was für sie jedoch außer einer beeindruckenden Urkunde keine Vorteile brachte. Auch in Russland wurde ihr keine Stelle angeboten, stattdessen wurde sie dort zum „korrespondierenden Mitglied der Russischen Akademie der Wissenschaften“ gewählt.

Von ihrer Stellung auf Lebenszeit hatte Kowalewskaja nicht mehr viel, da sie am 10. Februar 1891 an einer Lungenentzündung starb, die sie sich in Cannes zugezogen hatte, und die sich auf der Rückreise über Paris und Berlin, wo sie mit bekannten Mathematikern zusammentraf, verschlimmerte. Sie wurde nur 41 Jahre alt. Die Nachricht ihres frühen Todes erschütterte ihre Mathematikerkollegen in ganz Europa.

Der Mathematiker Leo Koenigsberger schreibt in "Mein Leben", S. 116–117:
Leopold Kronecker widmete ihr folgenden Nachruf:
Auch außerhalb der Fachwissenschaften haben Leistungen und Lebenslauf Sofja Kowalewskajas Eindrücke hinterlassen beziehungsweise beeindrucken noch heute. So weckte deren „Doppelbegabung als Schriftstellerin und Mathematikerin“ beispielsweise das Interesse der Schriftstellerin Alice Munro (Literaturnobelpreis 2013) und inspirierte sie zur (etwas längeren) Kurzgeschichte "Zu viel Glück" im 2009 erschienenen Band "Too much Happiness".



Das Deutsche Theater Göttingen bringt das Leben der Sofja Kowalewskaja am 22. Dezember 2016 unter der Regie von Antje Thoms erstmals auf die Bühne. Die Autorin Anne Jelena Schulte hat die Widersprüche, Sehnsüchte und Kämpfe der Mathematikerin für die Bühne bearbeitet und mit Gegenwartstexten zum Alltag von Frauen in den naturwissenschaftlichen Fakultäten gespiegelt. Diese recherchierte sie an der Universität Göttingen, an der Sofja Kowalewskaja "in absentia" promovierte.






</doc>
<doc id="11097" url="https://de.wikipedia.org/wiki?curid=11097" title="Slowakische Sprache">
Slowakische Sprache

Die slowakische Sprache (slowakisch "slovenský jazyk") gehört gemeinsam mit Tschechisch, Polnisch, Kaschubisch und Sorbisch zu den westslawischen Sprachen und damit zur indogermanischen Sprachfamilie.
Slowakisch wird von rund fünf Millionen Slowaken in der Slowakei und etwa zwei Millionen Auswanderern, davon ungefähr eine Million in Nordamerika (USA, Kanada) gesprochen. Seit dem 1. Mai 2004, dem Beitritt der Slowakei zur Europäischen Union ist Slowakisch eine der EU-Amtssprachen.

Slowaken und Tschechen verstehen einander recht problemlos. Gefördert wurde dies durch die gemeinsame Geschichte in der Tschechoslowakei von 1918 bis 1992. Allerdings tut sich die jüngere tschechische Generation, die nach der Trennung der Slowakei und Tschechiens aufgewachsen ist, schon deutlich schwerer. Offizielle Dokumente in der jeweils anderen Landessprache werden in Tschechien und der Slowakei automatisch anerkannt. Das Recht, im Amtsverkehr die andere Sprache zu verwenden, ist im slowakischen Minderheitensprachengesetz und im tschechischen Verwaltungordnungsgesetz verankert. Fernsehsendungen in der jeweils anderen Sprache werden in der Slowakei fast immer und in Tschechien oft unübersetzt ausgestrahlt.

Das Slowakische entstand im 10. Jahrhundert nach dem Untergang des Großmährischen Reiches aus der Sprache der "slověne" (ausgesprochen etwa [slowäne] (offenes e) oder [slowene] (mittleres e)), das heißt der Bevölkerung dieses Reichs (siehe unter Slawen), in Form mehrerer Dialekte. Vom 10. bis zum 19. Jahrhundert wurde im Königreich Ungarn (dessen Bestandteil die Slowakei im 11. Jahrhundert geworden war) als Amts- und Literatursprache vorwiegend Latein verwendet. Außerdem wurde zum Teil Deutsch und Ungarisch verwendet. Daneben begann vor allem das Bürgertum der Slowakei im 13. und 14. Jahrhundert, die eigene Sprache als (parallele) Amtssprache zu verwenden, wobei man allerdings schon bald (am Ende des 14. Jahrhunderts) dazu überging, in dieser Funktion und später auch als Literatursprache das Tschechische einzusetzen. Die Gründe hierfür waren anfangs vor allem, dass es sich um eine bereits „fertige“ Schriftsprache eines verwandten Landes mit einer berühmten Universität in Prag handelte, im 15. Jahrhundert auch der Einfluss tschechischer Hussiten in der Slowakei und später auch der Einfluss tschechischer protestantischer Emigranten in der Slowakei. Umgekehrt waren unter den vier wichtigsten Erneuerern der tschechischen Sprache im 18. und 19. Jahrhundert zwei Slowaken, Jan Kollár und Pavel Jozef Šafárik. Die tschechischen Texte wurden aber sehr oft (bewusst oder unbewusst) mit slowakischen Elementen versehen (so genanntes slowakisiertes Tschechisch, siehe auch Žilina). Im mündlichen Gebrauch wurden natürlich weiterhin die jeweiligen slowakischen Dialekte verwendet.

Obwohl es schon im 16. Jahrhundert Versuche gab, eine gemeinsame slowakische Schriftsprache zu etablieren, wurde die erste richtige einheitliche Schriftsprache erst 1787 von Anton Bernolák auf der Grundlage des westslowakischen Dialektes aus der Umgebung Trnavas festgelegt. Die heutige slowakische Schriftsprache wurde in den 1840er Jahren von Ľudovít Štúr auf der Basis eines mittelslowakischen Dialektes festgelegt (siehe Nationale Wiedergeburt der Slowaken). Die späte Kodifizierung der Sprache ermöglichte, wesentliche Bereiche des slowakischen Formensystems einfacher zu halten als das tschechische.

Slowakisch ist eine stark flektierende Sprache mit sechs grammatikalischen Fällen.

Grundregeln:

Die meisten Buchstaben werden ausgesprochen wie im Deutschen. Anders gesprochen werden (zum Teil aufgrund der vorstehend genannten Grundregeln):

Noch zu beachten:

Die als „rhythmische Kürzung“ bekannte Regel ist für das Slowakische charakteristisch. Gemäß dieser Regel dürfen im Slowakischen zwei lange Silben nicht aufeinander folgen. Wenn zwei lange Silben aufeinander folgen sollten, wird meistens die zweite gekürzt, zum Beispiel:

Es gibt allerdings auch eine Reihe von Ausnahmen, so etwa die Possessivadjektive auf "-í" (z. B. "vtáčí / vtačí" „zum Vogel gehörig“), Substantive mit den Suffixen "-ie" (z. B. "prútie" „Reisig“) u. a. m., außerdem wird die Endung der 3. Person Plural auf "-ia" nie gekürzt. Die Menge der Ausnahmen nimmt jedoch im Alltagsgebrauch und entsprechend bei jeder Neukodifizierung der Sprache ständig ab. So hieß es z. B. bis in die 1990er Jahre "píšúci" (schreibend), "mliekáreň" (Molkerei), "kamzíčí" (zur Gämse gehörend), seit etwa 1997 jedoch bereits "píšuci, mliekareň, kamzičí" usw.

Das gesprochene Slowakisch zerfällt in zahlreiche Dialekte. Diese lassen sich jedoch in drei Hauptgruppen unterteilen:

Manchmal wird noch eine vierte Gruppe angegeben, die sogenannten Unterland-Dialekte (slowakisch "dolnozemské nárečia"), die im heutigen Südostungarn (rund um Békéscsaba), Vojvodina (Serbien), westlichen Rumänien und kroatischen Syrmien gesprochen werden, also außerhalb der Grenzen der heutigen Slowakei. Neben örtlichen Einflüssen weist der Dialekt Ähnlichkeiten mit dem Novohrad-Dialekt auf.

Für eine genauere Beschreibung der Regionen siehe Liste traditioneller Regionen der Slowakei.

Folgendermaßen schreibt man sie im Web:

Allgemeine Erklärung der Menschenrechte, Artikel 1: 

Die slowakische Sprache wird vom Jazykovedný ústav Ľudovíta Štúra SAV (Sprachwissenschaftliches Ľudovít-Štúr-Institut der Slowakischen Akademie der Wissenschaften) reguliert, normiert und kodifiziert. Die jeweils aktuellen Auflagen folgender 4 Werke sind per Gesetz für jeglichen offiziellen Gebrauch der Sprache absolut verpflichtend:

Ján Doruľa u. a. (spätere Auflagen von J. Kačala J., M. Pisarčíková u. a. redigiert): "Krátky slovník slovenského jazyka." [Kurzwörterbuch der slowakischen Sprache] (Abk. KSSJ; dies ist ein Äquivalent des deutschen Duden - Deutschen Universalwörterbuchs):

Diverse Autoren: "Pravidlá slovenského pravopisu." [Regeln der slowakischen Rechtschreibung] (Abk. PSP; dies ist ein Äquivalent des deutschen Duden Rechtschreibwörterbuchs):

Ábel Kráľ: "Pravidlá slovenskej výslovnosti." (Regeln der slowakischen Aussprache) (Abk. PSV; dies ist ein Äquivalent des deutschen Duden Aussprachewörterbuchs):

J. Ružička u. a.: "Morfológia slovenského jazyka." (Morphologie der slowakischen Sprache) (Abk. MSJ), Vydavateľstvo SAV, Bratislava, 1966, Online-Version juls.savba.sk




Mit Unterstützung der EU entstand das E-Learning-Projekt Slovake.eu, das ein kostenloses Lernen der Sprache über Internet ermöglicht.





</doc>
<doc id="11099" url="https://de.wikipedia.org/wiki?curid=11099" title="Amateurfunkdienst">
Amateurfunkdienst

Der Amateurfunkdienst (kurz: "Amateurfunk"; englisch "amateur radio service" oder umgangssprachlich "ham radio") ist im Sinne der Internationalen Fernmeldeunion ein von Amateuren ausgeübter globaler nichtkommerzieller 2-Wege-Funkdienst () mit den Selbstzwecken:
Daneben gibt es für denselben Personenkreis einen zweiten Funkdienst, den Amateurfunkdienst über Satelliten, der Stationen an Bord von Amateurfunksatelliten oder Raumstationen nutzt.

Funkamateure werden nach einer Prüfung zum Amateurfunkdienst zugelassen und betreiben die Funktechnik als Hobby und nicht aus finanziellem Interesse.

Nur in Notfällen und bei Katastrophen dürfen Nachrichten auch von und an Dritte übermittelt werden. Die Regelungen der ITU berühren ausdrücklich keine politischen Aspekte wie die Meinungsfreiheit oder die Souveränität der Mitgliedsstaaten. Amateurfunk ist unmittelbare Völkerverständigung über Grenzen hinweg und dient nichtkommerziellen persönlichen Interessen wie Freundschaftspflege, Weiterbildung, Forschung, Wissenschaft und Technik aber auch der Befriedigung der Sammelleidenschaft.

Das völkerrechtliche Vertragswerk der Internationalen Fernmeldeunion (ITU) ist die Vollzugsordnung für den Funkdienst (in Österreich und Deutschland: "VO Funk", in der Schweiz: "Radioreglement", aktuell in der Fassung von 2016). Dieser internationale Vertrag reglementiert den Sendeempfangsbetrieb aller Funkdienste, um gegenseitige Störungen an den Landesgrenzen und über diese hinweg auszuschließen. Das internationale Recht wird über Amateurfunkgesetze, Verordnungen und nationale Frequenzpläne in Landesrecht umgesetzt. Die jeweilige Landesverwaltung erlässt auch Vorschriften über die Ausbildung, Prüfung, Zulassung und Erteilung von Rufzeichen. Die Regeln unterliegen Änderungen, z. B. ist die Morse-Prüfung in Deutschland seit 2003 auf Kurzwelle auch im Amateurfunk nicht mehr zwingend erforderlich.

In der VO Funk wird der Amateurfunkdienst wie folgt definiert:
Die Amateurfunklizenzen (in Deutschland Zulassung zur Teilnahme am Amateurfunkdienst) sind entsprechend dem Schwierigkeitsgrad der abgelegten Prüfung in mehrere Klassen abgestuft mit Einschränkungen bei der Frequenznutzung, Sendeleistung, Modulation oder bei der Betriebsart. Zunächst wird eine Prüfung bei der zuständigen Landesstelle abgelegt. In Deutschland ist es die Bundesnetzagentur BNetzA, in Österreich das Bundesministerium für Verkehr, Information und Technologie, in der Schweiz das Bundesamt für Kommunikation BAKOM. In den USA prüft ausnahmsweise der Amateurfunkverband ARRL selbst. Nach der Prüfung kann der Inhaber des Prüfzeugnisses seine Lizenz beantragen, sofern er das Amateurfunkrufzeichen nicht schon nach bestandener Prüfung erhielt.

Das personalisierte Rufzeichen dient der eindeutigen Identifikation einer Sendefunkstelle und besteht aus einer alphanumerischen Kombination. Die ITU-Präfixe der Rufzeichen sind einzelnen Staaten, davon abhängigen oder unabhängigen Gebieten und internationalen Organisationen zugeordnet. Einige Länder haben mehrere Präfixe.

Auch dem Präfix folgende Zeichen können eine regionale Bedeutung haben, in Österreich entspricht die dem Landespräfix OE folgende Zahl 1-9 einem Bundesland.

Der reine Empfang des Amateurfunks ist wie beim Rundfunk allgemein erlaubt.

Der Sendeempfangsbetrieb mit Funkgeräten ist von den Landesbehörden mehr oder weniger streng reglementiert, so gibt es lizenzfreie Allgemeinzuteilungen beim Jedermannfunk, daneben gibt es weitere lizenzfreie Funkanwendungen kleiner Reichweite mit Allgemeinzuteilungen für Sprache im Kfz oder in der Wohnung, Funkkopfhörer, Babyphon, Funkmikrofone oder zur Datenübertragung (Bluetooth, Funkmaus, Wetterstation, Funkfernsteuerungen) und Funkanwendungen für industrielle, wissenschaftliche und medizinische Zwecke. Die gehandelten Geräte bedürfen einer Konformitätserklärung für die der verantwortliche Inverkehrbringer wie Hersteller, Importeur oder Händler haftet und sie dürfen für den Sendebetrieb nicht geändert werden.

Die Regulierungen bezüglich Betriebsmodus, Leistung und Frequenzzuweisung sind streng geregelt und ändern sich bei Bedarf.

Für lizenzierte Funkanwendungen in Sicherheitsbereichen wie Betriebsfunk, Behördenfunk und Flugfunk gelten von Land zu Land unterschiedliche Abhörverbote.

Amateurfunk ist ein sehr vielfältiges Hobby:
Über spezialisierte Händler ist eine Vielzahl an Amateurfunkgeräten verfügbar. Die dort verwendete Technik ist häufig sehr kompliziert; selbst das Modifizieren dieser Geräte stößt schnell an Grenzen.

Damit sich Funkamateure leichter mit der einschlägigen Technik auseinandersetzen können, bieten verschiedene Firmen und Funkamateure Bausätze an. Dieser Weg erspart die teilweise schwierige Bauteilbeschaffung und erleichtert mit den zugehörigen Unterlagen Aufbau, Erweiterung und Modifikation. Selbstbaugeräte besitzen häufig nur eine geringe Sendeleistung.

Das Funken mit geringer Leistung (bis 5 Watt Senderausgangsleistung) nennt man QRP-Betrieb. („QRP“ ist ein Betriebszeichen aus der Telegrafie und bedeutet im eigentlichen Sinne: „Reduzieren Sie Ihre Sendeleistung.“)

Die funktionstüchtige Zusammenstellung von Funkgerät, Antenne und messtechnischem Zubehör nennt man Amateurfunkstelle oder in der Amateurfunkwelt auch "Rig" (engl. Anlage). Die Räumlichkeit, in der diese Geräte aufgestellt bzw. betrieben werden, wird als "Shack" bezeichnet (engl. Bude, Hütte).

Den Funkamateuren stehen verschiedene Frequenzbereiche, die sogenannten Amateurfunkbänder, zwischen 135 kHz und 250 GHz im Langwellen-, Mittelwellen-, Kurz- und Ultrakurzwellen- bis in den Gigahertz-Bereich zur Verfügung. Auch im optischen Bereich und im Bereich der Terahertzstrahlung sind Funkamateure aktiv und insbesondere in diesem Bereich auch aktiv an der Forschung beteiligt.

Alle Funkamateure haben einen gemeinsamen Verhaltenskodex, den so genannten "Ham Spirit"; exemplarisch ist der vom US-amerikanischen Verband ARRL zu Beginn des 20. Jahrhunderts publizierte Text.

Wegen der besonders zu Morse-Zeiten eher langsamen Übertragung hat sich eine ausgeprägte Kultur der Abkürzungen entwickelt. Die Abkürzungen stammen durchweg aus dem englischen Sprachraum und sind weltweit gültig. Beispielsweise steht „OM“ (von "old man") für einen männlichen Funkamateur sowie „YL“ (von "young lady") für eine Funkamateurin.

Die Funkverbindungen werden mit QSL-Karten bestätigt. Besonders begehrt sind QSL-Karten aus Amateurfunk-Ländern, in denen es sehr wenige oder keine Funkamateure gibt, aber auch von selten arbeitenden oder schwer zu erreichenden Amateurfunk-Stationen wie der Internationalen Raumstation ISS oder von prominenten Funkamateuren wie Juan Carlos von Spanien. Die Jagd nach weit entfernten Amateurfunk-Stationen wird DXen genannt. Die QSL-Karten werden entweder über den eigenen Amateurfunk-Verband an die Amateurfunk-Verbände im jeweiligen Land geschickt – oder direkt an die Adresse geschickt, die man aus dem Callbook erhält.

Mittlerweile gibt es auch Websites wie EQSL.CC für diesen Zweck: Wenn beide Kommunikationspartner zueinander passende Verbindungsdaten eingeben, gilt die Verbindung als bestätigt.

Für bestimmte Leistungen, beispielsweise für Funkkontakte in eine bestimmte Anzahl Gebiete, werden Amateurfunkdiplome ausgestellt. Dafür ist meist vorher das Sammeln von QSL-Karten für die Beantragung erforderlich.

Es kommen traditionelle Modulationsarten und Betriebsarten wie Telegrafie und Telefonie genauso zum Einsatz, wie Funkfernschreiben und moderne digitale Übertragungsverfahren wie Packet Radio, Pactor, APRS oder PSK31, welche hauptsächlich für die Textübertragung Verwendung finden. Auch Bild- und Videoübertragungen sind mit Betriebsarten wie FAX, SSTV (Slow Scan Television) und ATV (Amateurfunk-Fernsehen) möglich. Auch eine Amateurfunk-Version des neuen digitalen Kurzwellenrundfunks Digital Radio Mondiale (DRM) wurde entwickelt. Seit kurzem gibt es auch digitalen Sprechfunk, wie der in Japan entwickelte digitale Übertragungsstandard D-STAR.

Viele der modernen Betriebsarten lassen sich mit Hilfe von zum Teil kostenloser, von Funkamateuren entwickelter Software betreiben. Dazu verbindet man lediglich das Funkgerät mit der Soundkarte eines handelsüblichen PC.

Neben direkten Verbindungen sind auch Kontakte via Relaisstationen, Echolink, Amateurfunksatelliten (z. B. OSCAR), Erde-Mond-Erde oder auch Meteorscatter möglich. Damit kann man auch auf den UKW-Bändern, mit denen man terrestrisch nur Entfernungen bis 300 km zurücklegen kann, mit fast der ganzen Welt sprechen. Funkamateure haben eigene Satelliten gebaut, die man als Relaisstation nutzen kann. Aber auch nur kurzzeitig vorhandene natürliche Erscheinungen, wie beispielsweise Aurora (Reflexion der Funkwellen an Polarlichtern) oder die Reflexion von Funkwellen an Flugzeugen, werden zur Überwindung größerer Entfernungen auf UKW genutzt.

Eine Funkverbindung kann mit einer der oben erwähnten Betriebsarten aufgebaut werden:

Unmittelbar neben den in der WLAN-Technik genutzten ISM-Bändern bei 2,4 und 5,8 GHz gibt es Amateurfunk-Zuweisungen. Das macht es möglich, mit sehr preiswerter, nur geringfügig modifizierter WLAN-Ausrüstung breitbandige Richtfunkstrecken zu betreiben. Häufig werden dabei neben handelsüblichen WLAN-Komponenten lediglich Richtantennen mit hohem Gewinn benutzt. Unter der Bezeichnung HAMNET entsteht seit einiger Zeit eine breitbandige Richtfunk-Infrastruktur, die vor allem in Österreich schon recht weit ausgebaut ist.
Die einzelnen Frequenzbereiche des elektromagnetischen Spektrums, die der Amateurfunkdienst nutzen darf, nennt man auch Amateurfunkbänder.

Die einzelnen Bänder werden bestimmten Funkdiensten auf "primärer" oder "sekundärer" Basis zugewiesen. So ist der Frequenzbereich 144-146 MHz im 2-Meter-Band in Europa der Nutzung für Amateurfunkdienste vorbehalten. Dort dürfen also nur Funkamateure senden. Das 23-cm-Band (1240–1300 MHz) ist dem Amateurfunk auf sekundärer Basis zugewiesen. Funkamateure haben ihren Sendebetrieb dort so einzurichten, dass die primären Funkdienste nicht gestört werden, und müssen ihrerseits Störungen hinnehmen.

Grundsätzlich werden Frequenzbereiche durch die Internationale Fernmeldeunion (ITU) in der VO Funk international zugewiesen. Das Entscheidungsgremium dafür ist die Weltfunkkonferenz. Die für das Hoheitsgebiet der Bundesrepublik Deutschland verbindlichen Regelungen und Festlegungen enthält die vom 27. August 2013 (BGBl. I S. 3326).

Meist werden Frequenzbereiche für die ITU-Region 1 (Europa, Afrika, Russland), Region 2 (Amerika) und/oder 3 (restliches Asien und Ozeanien) zugewiesen unter der Überlegung, dass sich Funkwellen nicht von politischen Grenzen aufhalten lassen. Insbesondere in höheren Frequenzbereichen, bedingt durch Verringerung der Ausbreitung EM-Wellen, sind nationale abweichende Frequenzbereichzuweisungen für den Amateurfunkdienst zulässig, wenn dies durch andere Funkdienste hinnehmbar ist. So ist in Skandinavien das 70-cm-Amateurfunkband nur 6 MHz breit (432–438 MHz), während es im restlichen Europa 10 MHz breit ist (430–440 MHz).

Für Deutschland legt die Bundesregierung die Frequenzbereichszuweisung in der Frequenzverordnung fest ( Abs. 1 TKG).

Innerhalb der einzelnen Amateurfunkbänder stellen die Amateurfunkverbände Bandpläne auf. Auf Kurzwelle wird traditionell der unterste Bandabschnitt exklusiv dem Morsen zugeteilt, am oberen Ende wird auch Sprechfunk betrieben. In den letzten Jahren geht man langsam von einer Sortierung nach einzelnen Betriebsarten über zu einer Sortierung nach benutzten Bandbreiten. Die Überlegung dahinter ist, dass Modulationsarten mit geringen Bandbreiten auch geringere Sendeleistungen erfordern und sich schwächere Sender gegenseitig weniger stören, als das sehr starke Sender gegenüber schwachen Sendern tun. Zudem gibt es durch die Digitalisierung eine Vielzahl von Betriebsarten, für die keine exklusiven Bandabschnitte mehr zur Verfügung gestellt werden können.

Innerhalb des Amateurfunks sind diverse Projekte für junge Funkamateure entstanden. Nachfolgend sind einige internationale Veranstaltungen aufgeführt:

Dazu kommen noch viele weitere regionale und lokale Veranstaltungen, wie etwa Jugendfielddays, Ferienspaßaktionen, Bastelaktionen und Jugendgruppen. An Schulen und Hochschulen gibt es oftmals Klubstationen (Schulstationen) sowie Projekte für Funkkontakte mit der Internationalen Raumstation ISS (Amateur Radio on the International Space Station – ARISS).

Die Interessen von jugendlichen Funkamateuren sieht ein Magazin (Stand November 2006) so: „Eine niederländische Befragung unter Jugendlichen darüber, was ihnen denn am Amateurfunk besonders läge, brachte als Ergebnis folgende Reihung der Interessen: Conteste, Diplome, QRP "(!)", Funkgerät und PC, Amateurfunk in Gruppen, Notfunk, Naturerscheinungen, Funk und Astronomie. Keine Technik …“ QRP bezieht sich dabei allerdings auf den Selbstbau von einfachen Funkgeräten kleiner Leistung und repräsentiert den Großteil des heutigen Selbstbaus.

Der Empfang von Aussendungen des Amateurfunkdienstes ist in Deutschland jedermann gestattet. Die Frequenzbänder im Kurzwellenbereich können in einfacher Weise mit einem Taschen-Weltempfänger empfangen werden. Die aktive Teilnahme am Amateurfunkdienst, d. h. der Betrieb eines Senders, ist an ein qualifizierendes Zeugnis und eine Zulassung zur Teilnahme am Amateurfunkdienst mit gleichzeitiger Rufzeichenzuteilung gebunden (Ausnahme: Betrieb unter Aufsicht mit Ausbildungsrufzeichen). Das Amateurfunkzeugnis erwirbt man durch eine Prüfung bei der nationalen Fernmeldeverwaltung, in Deutschland der Bundesnetzagentur für Elektrizität, Gas, Telekommunikation, Post und Eisenbahnen.

Damit "unterscheidet" sich der Amateurfunkdienst von diversen Funkanwendungen für Jedermann, die ohne Prüfung genutzt werden dürfen (CB-Funk, PMR-Funk, SRD-Funk).

Kurse zur Vorbereitung auf die Amateurfunkprüfung bieten verschiedene Vereine und Organisationen an. Die meisten Kurse werden von den Amateurfunk-Verbänden angeboten, organisiert oder gefördert:

Häufig finden die Kurse an Schulen, Volkshochschulen oder an Universitäten statt. Besonders hervorzuheben ist die Bücherreihe von E. Moltrecht, die eine Vorbereitung auf die Prüfung auch mit wenig Vorwissen ermöglicht. Die Nutzung eines Ausbildungsrufzeichens bietet dabei die Möglichkeit, schon vor der Amateurfunkprüfung unter Aufsicht eines Funkamateurs Funkbetrieb zu beobachten und so das erworbene Wissen auszuprobieren und zu festigen.

Die Pioniere der Funktechnik wie Heinrich Hertz oder Guglielmo Marconi schufen in den beiden letzten Jahrzehnten des 19. Jahrhunderts die Grundlagen der heutigen Funktechnik. In der Pionierzeit gab es nur wenige Regulierungen. Das führte in vielen Ländern zu einem Chaos auf den Frequenzen. 1906 wurde in Berlin die "Convention Radiotélégraphique Internationale" beschlossen, die beispielsweise größere Schiffe zum Betrieb einer Funkstation verpflichtete. Diese Konvention ratifizierten die USA erst wenige Monate vor der Titanic-Katastrophe.

Als die RMS Titanic 1912 sank, hätte eine bessere Kommunikation die Zahl der Opfer deutlich senken können. Das führte in den USA zum Radio Act of 1912, der u. a. „private Funkstationen“ auf Wellenlängen unterhalb von 200 m (über 1,5 MHz) verwies, ihre Sendeleistung auf 1 kW Input begrenzte und offizielle Rufzeichen einführte. Diese "Kurzwellen"-Frequenzen hielt man damals für wertlos, da man irrtümlich nur eine geringe Reichweite vermutete. Der 2. Weltfunkvertrag von 1912 spricht erstmals von „privaten Funkstationen“, ohne den Begriff näher zu definieren. Offiziell taucht der Begriff „Funkamateur“ bei der Washingtoner Welt-Wellenkonferenz 1927 auf.

In den USA gab es bis 1939 "Experimentalstationen", deren Rufzeichenschema dem bis heute bei den US-Funkamateuren üblichen entspricht (1–2 Buchstaben aus dem ITU-Rufzeichenblock der USA, 1 Zahl, 1-3 Buchstaben). 1939 durften solche Experimentalstationen ins kommerzielle Lager wechseln , dann natürlich mit den Rufzeichen aus 3–4 Buchstaben, die auch bis heute üblich sind.

Die Geschichte des Amateurfunkdienstes verlief in der Anfangszeit in den einzelnen Staaten sehr unterschiedlich. Viele Länder, wie die USA, Großbritannien und Frankreich standen dem Thema sehr liberal gegenüber und förderten die Entwicklung. So gab der britische Generalpostmeister 1905 die ersten gedruckten Experimentierlizenzen an Amateure aus.

Andere Länder, wie beispielsweise Deutschland, sahen den Amateurfunk misstrauisch und waren eher bestrebt, die staatliche Fernmeldehoheit und das Postmonopol zu schützen.

In den USA gab es ab 1905 für $8,50 den „Telimco-Telegraphen“ frei zu kaufen, mit dem man etwa eine Meile überbrücken konnte. Bedingt durch den Ersten Weltkrieg war der private Funkbetrieb auch in den USA von 1914 bis 1919 verboten.

Bis 1924 galt allein das „Gesetz über das Telegrafenwesen des Deutschen Reiches“ vom 6. April 1892, das dem Staat das absolute Fernmeldemonopol sicherte. Am 24. Mai 1924 veröffentlichte das Reichspostministerium eine Verfügung, die das Rundfunkwesen neu regelte. Ab da konnten Privatpersonen die „Audionversuchserlaubnis“ erwerben, die den Besitz und den Betrieb eines einfachen Empfängers erlaubte. Das war eine reine Empfangserlaubnis. Bis dahin war selbst der Besitz eines Empfängers verboten. In Deutschland wurden einige wenige Clubstationen lizenziert, während es in Großbritannien zur gleichen Zeit schon 1200 offiziell lizenzierte Funkamateure gab. Ende Mai 1933 wurden 180 alte „Schwarzfunker“ offiziell lizenziert – wohl aus Propagandagründen. Mit Kriegsbeginn am 1. September 1939 wurden alle 529 erteilten Lizenzen eingezogen.

Während des Zweiten Weltkriegs wurde eine niedrige dreistellige Zahl von Kriegsfunkgenehmigungen (KFSG) ausgegeben. Während des Krieges erkannte man auch den Wert der Kenntnisse, die sich Funkamateure erworben hatten, und versuchte, sie in der Industrie oder Funkdienststellen nutzbar zu machen.

Nach dem Zusammenbruch des Deutschen Reiches galt zunächst, für jede der vier Zonen getrennt, alliiertes Militärrecht. Unkontrollierte Kommunikation ist in solchen Fällen immer suspekt. Zonen-übergreifende Organisationen waren nicht möglich, Kommunikation und Reisen nur schwer möglich. Die französische Verwaltung war bedeutend restriktiver als die britische und vor allem die amerikanische. Die sowjetische Zone war fast völlig isoliert. Die erste Kurzwellentagung nach dem Krieg fand am 7. und 8. Juni 1947 in Stuttgart statt und hatte rund 500 Teilnehmer. In der amerikanischen und britischen Zone war manches Gentlemen’s Agreement möglich. So konnte schon 1947 die QSL-Karten-Vermittlung „Box 585, Stuttgart“ eröffnet werden.

Ihre Bewährungsprobe mussten Organisation und Disziplin der deutschen Funkamateure in der Zeit von 23. bis 30. April 1948 bestehen: Die deutschen Funkamateure verpflichteten sich gegenüber der Militärregierung zu absoluter Funkstille, die auch fast vollständig eingehalten wurde. Anschließend überschlugen sich die Ereignisse: Vom 8. bis 9. Mai 1948 fand in Bad Lauterberg eine Kurzwellentagung statt, bei der sich die Amateurfunkverbände der Westzonen vereinigten. Kurz darauf kündigte die Deutsche Post an, dass ab Mai 1948 Amateurfunk-Lizenzprüfungen stattfinden sollten. Das Amateurfunkgesetz ließ dann aber doch noch bis zum 19. Januar 1949 auf sich warten. Damit konnten im Vereinigten Wirtschaftsgebiet offiziell Amateurfunklizenzen ausgegeben werden. Das erste Amateurfunkgesetz ist also älter als das Grundgesetz.

Das Saarland war nach dem Krieg von Frankreich annektiert worden, galt also nicht mehr als Teil Deutschlands. Hier trat das erste Amateurfunkgesetz erst am 4. April 1951 in Kraft. Am 1. Januar 1954 waren in der Bundesrepublik Deutschland 3389 Funkamateure lizenziert. Am 31. Dezember 2017 waren bei der Bundesnetzagentur 64.548 Funkamateure der Klasse A und E registriert. Höhepunkt der Anzahl war der Stichtag (31. Dezember) des Jahres 2002 mit 80.874 Amateurfunkzulassungen. Seitdem geht diese Anzahl stetig zurück.

Die erste offizielle Erwähnung des Amateurfunks auf dem Gebiet der DDR gab es 1950 im Rahmen der Freien Deutschen Jugend (FDJ). Dort gab es „Interessengemeinschaften für Sondersportarten“, aus denen die Gesellschaft für Sport und Technik (GST) hervorging. Ein Schreiben des Initiativkomitees zur Gründung der GST erwähnt die Forderung Jugendlicher nach Ausübung des Amateurfunks. Die GST gab dann die Zeitschrift "Sport und Technik" heraus, die regelmäßig nachrichtentechnische Beiträge enthielt. Daraus entstand die Zeitschrift "Funkamateur", die nach der Wende privatisiert wurde und bis heute existiert.

Am 6. Februar 1953 wurde die „Verordnung über den Amateurfunk“ verkündet. Die ersten Lizenzen wurden am 14. Juli 1953 ausgegeben. Eine Amateurfunklizenz war in der DDR immer an die Mitgliedschaft in der Gesellschaft für Sport und Technik (GST) gebunden.

Die GST förderte durch materielle Zuwendungen die Errichtung sogenannter Klubstationen, an denen mehrere Funkamateure die meist selbst gebaute Technik gemeinsam nutzen konnten. Mitunter wurden den Klubstationen neue kommerzielle Geräte – Beispiele sind der KW-Empfänger "Dabendorf" sowie der Transceiver "Teltow 215B" – sowie auch ausgesonderte Geräte der bewaffneten Organe der DDR zur Verfügung gestellt. Neben dem "Klubstationsleiter" (Chefoperator) gab es lizenzierte sogenannte "Mitbenutzer" der Amateurfunkstelle, deren Rufzeichen aus dem Stationsrufzeichen abgeleitet wurde. Die Klubstationen haben sich bei der Ausbildung am Amateurfunk Interessierter sehr verdient gemacht. Unter besonderen Bedingungen wurden "Privatlizenzen" an Einzelpersonen erteilt.

Am 23. April 1954 wurde die erste Lizenzurkunde ausgegeben an den Präsidenten des OeVSV, Erwin Heitler, OE1ER.

Die Amateurfunk-Aktivitäten in Österreich sind aber bedeutend älter: Der Österreichische Versuchssenderverband (OeVSV) wurde 1926 (nach anderer Quelle: Oktober 1925) gegründet. Die „OEM“, das Mitteilungsblatt des OeVSV erschien 1933 bis 1938 (also wohl bis zum Anschluss Österreichs an das Deutsche Reich) und dann wieder ab 1945.

Wie damals üblich begann der Amateurfunk auch in der Schweiz um den Ersten Weltkrieg als „Schwarzfunk“. Juristisch konnte man ab dem 1. Juli 1925 an der Obertelegraphen-Direktion die Prüfung für eine Sendekonzession ablegen. Die erste Lizenz wurde im April 1926 ausgegeben. Die ersten offiziellen Rufzeichen hatten den Präfix H9, der noch vor 1930 durch HB9 ersetzt wurde.

Der Amateurfunk hat vielen technisch interessierten Menschen den Zugang zu Elektronik und Nachrichtentechnik geebnet. Damit leistete der Amateurfunk einen erheblichen Beitrag zur Förderung des technisch-wissenschaftlichen Nachwuchses. Entsprechend förderten Institutionen wie die Deutsche Bundespost, Deutsche Telekom, das Technische Hilfswerk oder die Bundeswehr den Amateurfunk. In der Deutschen Demokratischen Republik gehörte der Amateurfunk zur paramilitärischen Ausbildung; der Zugang zum Amateurfunk war nur über die Gesellschaft für Sport und Technik möglich.

Eine wichtige Aufgabe des Amateurfunks ist die Völkerverständigung. Verbindungen zwischen Funkamateuren aus West und Ost waren auch zu Zeiten des Kalten Krieges möglich, wobei die Nachrichteninhalte system- und vorschriftsbedingt stark eingeschränkt waren. Heute bieten Internet und niedrige Telefon- oder Flugkosten hierzu Alternativen, nicht jedoch in Schwellenländern mit niedriger Internetabdeckung. Der Reiz des Amateurfunks liegt ebenfalls darin, den Standort der Gegenstelle zu kennen und dadurch Rückschlüsse auf die Verbindung zu ziehen.

Der Amateurfunk hat sich große Verdienste bei der Katastrophenhilfe erworben. Besonders in Ländern mit großen Entfernungen und teilweise recht fragiler Infrastruktur, wie beispielsweise den USA oder in den Alpen, führen Naturkatastrophen und Großschadensereignisse immer wieder zum vollständigen Ausfall der normalen Kommunikations-Infrastruktur. Beispiele in Mitteleuropa sind Einsätze wie anlässlich der Hamburger Sturmflut oder der Lawinenkatastrophe von Galtür; Amateurfunk bietet häufig eine schnelle Möglichkeit, einen Notruf abzusetzen. Bei der nach der Flutkatastrophe von 1953 modellierten multinationalen Übung FloodEx waren 2009 Notfunker vor allem aus den Niederlanden und Großbritannien fest eingebunden, weil die Lage den weitgehenden Ausfall des zellularen TETRA vorsah; das Technische Hilfswerk hatte allerdings angemeldeten deutschen Notfunkern abgesagt. In Frankreich wird das Abhören von Notruffrequenzen und die Unterstützung bei der Suche nach abgestürzten Flugzeugen mit Peilgeräten vom Amt für Zivilschutz besonders gefördert. Amateurfunk ist ebenfalls ein wichtiges Standbein der Kommunikation von im Ausland eingesetzten Helfern mit dem Heimatland. Satellitentelefone haben sich durch die begrenzten Bandbreiten, die vor allem von der Presse und privaten Firmen mit Priorität angekauft werden, als nur bedingt tauglich erwiesen. Weiteres hierzu unter Notfunk.

In dünn besiedelten Regionen der Erde mit mangelhafter Telekommunikations-Infrastruktur kann der Amateurfunk in Not- oder Katastrophenfällen ein erstes Mittel zur Nachrichtenübermittlung darstellen. Die Freiräume des Amateurfunkdienstes ermöglichen auch unkonventionelle Lösungen wie ein 2-m- bzw. 80-m-Relais in Namibia: Rund um Windhoek kann man es wie ein ganz normales UKW-Relais nutzen, während Funkamateure im restlichen Land den 80-m-Zugang nutzen können. Über den Echolink-Anschluss ist der Rest der Welt problemlos zu erreichen.

Manch ein Leben ist durch die Übermittlung eines Notrufes durch Funkamateure gerettet worden, und so mancher Angehörige eines Katastrophenopfers konnte auf diesem Wege etwas über den Verbleib eines Verwandten erfahren (welfare traffic).

In den dicht besiedelten Regionen der Erde, also etwa den Industrieländern der nördlichen Halbkugel, existiert heute eine Vielzahl öffentlicher und behördlicher Kommunikationsmittel. Katastrophen von der Hamburger Sturmflut 1962 bis zu den Erdbeben- und Tsunami-Katastrophen im Indischen Ozean Dezember 2004 und in Japan im Jahr 2011 haben gezeigt, dass diese hochtechnologischen öffentlichen Kommunikationsnetze anfällig gegenüber Störungen sind.

Selbst wenn die Hilfsdienste mit ihren eigenen Funksystemen vor Ort sind, kann der Amateurfunk eine wichtige Rolle übernehmen: Viele der benutzten Funksysteme sind nicht interoperabel, der Hilfsdienst A kann keinen Funkkontakt mit Hilfsdienst B aufnehmen. Funkamateure können diese Grenze häufig entweder mit ihrer eigenen Technik oder mit den beim Hobby erworbenen Kenntnissen überbrücken.

Seit etwa 1990 wird der Amateurfunk in der Gesellschaft weniger deutlich wahrgenommen, was sich deutlich am geringen Nachwuchs bemerkbar macht. Die Gründe hierfür sind vielfältig:

Auch heute sind aus dem Bereich des Amateurfunks Veröffentlichungen in wissenschaftlicher Qualität zu beobachten. In Amateurfunksatelliten werden innovative Techniken erforscht. An vielen Universitäten gibt es Vereinigungen von Funkamateuren, deren Mitglieder, meist Studenten und Mitarbeiter technischer Fachrichtungen, in selbstorganisierter Teamarbeit teils sehr anspruchsvolle und aufwändige Projekte realisieren. Beispiele:

Der Amateurfunk bietet auch künftig die Möglichkeit, die Grundlagen der Elektronik und der Funktechnik näher kennenzulernen. Gerade in der Hochfrequenztechnik kann dadurch eine für die praktische Arbeit notwendige Intuition erworben werden, die in den hoch verdichteten Studiengängen an den Universitäten und Fachhochschulen nicht vermittelt wird.

Durch die allgemeine Verfügbarkeit von Computern und Elektronik sowie des Internets und der drahtlosen Vernetzung hat die Amateurfunktechnik einen Teil ihres besonderen Reizes verloren. Die allgemeine Verfügbarkeit von Kommunikationstechnik und von Mobilfunktechnik bedient einen großen Teil der Bedürfnisse technisch Interessierter ohne weitere Erlaubnisse.

Ein gegenläufiger Trend zeigt das steigende Interesse an QRP, dem Senden mit sehr kleiner Leistung. Seit etwa 2003 steht zunehmend das Thema Software Defined Radio im Fokus der Funkamateure, was mit einer deutlichen Wiederbelebung des Selbstbaus von Funkgeräten und deren Eigen- bzw. Weiterentwicklung einhergeht. Mit den im Internet zugänglichen WebSDR können auch Amateurfunkbänder im Internet ohne physisches Empfangsgerät am Computer empfangen werden.

Noch nicht so recht abzuschätzen ist der Einfluss der Digitalisierung auf den Amateurfunk. Einerseits gibt es immer mehr digitale Übertragungsverfahren von PSK31 bis HAMNET. Andererseits wird die Digitaltechnik immer billiger und leistungsfähiger, so dass die benutzten Geräte immer weniger Funktionen in herkömmlicher Analogtechnik verwirklichen. Dies zeigt sich ganz deutlich beim Software Defined Radio. Die Eigenbauaktivitäten von Funkamateuren werden also deutlich weggehen vom Lötkolben und hin zum Einsatz von Computern.






</doc>
<doc id="11100" url="https://de.wikipedia.org/wiki?curid=11100" title="Lithiumhydrid">
Lithiumhydrid

Lithiumhydrid LiH ist eine salzartige chemische Verbindung von Lithium und Wasserstoff. Da Lithiumhydrid sehr stabil ist, stellt es in Verbindung mit der niedrigen molaren Masse des Lithiums einen hervorragenden Wasserstoffspeicher mit einer Kapazität von 2,8 m³ Wasserstoff pro Kilogramm dar. Der Wasserstoff kann durch Reaktion mit Wasser freigesetzt werden.

Lithiumhydrid wird durch die Umsetzung von flüssigem metallischem Lithium mit molekularem Wasserstoff bei 600 °C hergestellt.

Lithiumhydrid ist ein weißes bis graues, brennbares Pulver, das mit einer Dichte von 0,76 g/cm³ einer der leichtesten nicht porösen Feststoffe ist. Es schmilzt bei 688 °C. Die Bildungsenthalpie beträgt −90,43 kJ/mol.

Lithiumhydrid ist brennbar, reagiert also mit elementarem Sauerstoff. Dabei entsteht Lithiumhydroxid:

Es reagiert mit Wasser, Säuren und Basen unter Freisetzung von Wasserstoff:

Es reduziert beziehungsweise hydriert organische Verbindungen, zum Beispiel Formaldehyd zu Methanol:

Lithiumhydrid beginnt bei 900–1000 °C, sich in elementares Lithium und Wasserstoff zu zersetzen und ist damit das thermisch stabilste Alkalimetallhydrid.

Beim Erhitzen im Stickstoffstrom bildet sich Lithiumnitrid. Als Zwischenstufen entstehen Lithiumamid (LiNH) und Lithiumimid (LiNH).

Lithiumhydrid dient als Reduktionsmittel zur Herstellung von Hydriden und Doppelhydriden. Des Weiteren wird es zur Deprotonierung CH-acider Verbindungen benutzt.
Ein weiteres Einsatzgebiet ist mit der Herstellung der Hydriermittel Lithiumboranat und Lithiumalanat gegeben.

Aufgrund seines hohen Dipolmoments ist Lithiumhydrid im Zusammenhang mit der Bose-Einstein-Kondensation ultrakalter Atome interessant.

Bei Lithumdeuterid (LiD) handelt es sich um deuteriertes Lithiumhydrid, d. h., es wurde das Wasserstoff-Isotop Deuterium anstelle von normalem Wasserstoff verwendet. Lithiumdeuterid ist einer der Kernbestandteile der festen Wasserstoffbombe, durch den die Aufbewahrung und Handhabung des ansonsten gasförmigen Deuteriums und die Erzeugung des zur Fusion nötigen Tritiums immens vereinfacht wurde.<ref name="DOI10.1002/ciuz.19850190505">Richard Bauer: "Lithium - wie es nicht im Lehrbuch steht." In: "Chemie in unserer Zeit." 19, 1985, S. 167–173. .</ref>

Da Lithiumhydrid mit gängigen Feuerlöschmitteln wie Wasser, Kohlendioxid, Stickstoff oder Tetrachlorkohlenstoff stark exotherm reagiert, müssen Brände mit inerten Gasen wie z. B. Argon gelöscht werden.


</doc>
<doc id="11104" url="https://de.wikipedia.org/wiki?curid=11104" title="Hexenring">
Hexenring

Als Hexenringe oder Feenringe (engl. Fairy rings) werden halbrunde oder runde Wuchsbilder von Pilz-Fruchtkörpern bezeichnet, die dadurch entstehen, dass das Myzel eines Pilzes in alle Richtungen gleich schnell wächst. Die Durchmesser dieser Gebilde können altersabhängig sehr groß werden. Hexenringe werden von verschiedenen Pilz-Arten an ganz unterschiedlichen Standorten gebildet.

Am Ende der Myzelfäden bildet sich das, was der Volksmund als „Pilz“ bezeichnet, der sichtbare Fruchtkörper. Da mit der Zeit die Nährstoffe im Boden im inneren Bereich der „kreisförmigen Pilzansammlung“ zur Neige gehen, stirbt das Myzel dort ab und übrig bleibt eine ringförmige Struktur, der so genannte Hexenring. In der Regel ist ein Hexenring daher "ein einziger" Organismus. Bei günstigen Bedingungen kann das Myzel sehr schnell Fruchtkörper bilden, somit können solche Ringe buchstäblich über Nacht entstehen. Im Allgemeinen haben Hexenringe Durchmesser von 20 cm bis zu einigen Metern. Der größte bisher entdeckte Ring maß etwa 600 Meter im Durchmesser. In Europa können bei mehr als 60 der bekannten Pilzarten solche Ringstrukturen vorkommen.

Auf Rasen verursachen die Pilze häufig ganzjährig sichtbare Verfärbungen, weshalb Hexenringe auch zu den Rasenkrankheiten gezählt werden.

Der Name Hexenring oder Feenring geht auf den Volksglauben zurück, da man in diesen runden Formen Versammlungsorte der Hexen oder Feen sah, deren Betreten magisch oder verboten war. Wachstumsorte besonderer Pflanzen, zum Beispiel solcher mit speziellen Heileigenschaften, werden in den verschiedenen Kulturen mit Geistwesen verbunden.

Vor allem in Namibia sind auch sogenannte Feenkreise beobachtet worden. Dabei handelt es sich nicht um Kreise sichtbarer Pilzfruchtkörper, sondern um Ringstrukturen höherer Pflanzen, zumeist von Gräsern. Sehr wahrscheinlich geht das kahle Innere der Kreise auf die Tätigkeit der 'Sand-Termiten’ "Psammotermes allocerus" zurück, die im Umfeld ihres Nests das Gras abfressen.

Des Weiteren werden manchmal auch kreisförmig ausgetretene Grasnarben als Hexenringe bezeichnet, die in der Brunftzeit der Rehe entstehen können, wenn der Rehbock die Ricke treibt. Allerdings sind diese nur selten genau kreisförmig.

Es sind etwa 60 Pilzsorten bekannt, die zu Hexenringen wachsen.

Einer der größten jemals entdeckten Hexenringe wurde nahe Belfort in Frankreich gefunden. Dieser von Mönchsköpfen gebildete Ring hat einen Durchmesser von ungefähr 600 Meter und das Alter wird auf über 700 Jahre geschätzt. In den South Downs im Süden von England gibt es große Hexenringe aus Maipilzen, deren Alter auf mehrere hundert Jahre geschätzt wird.



</doc>
<doc id="11105" url="https://de.wikipedia.org/wiki?curid=11105" title="Pilze">
Pilze

Die Pilze (Fungi) sind in einer heute noch gebräuchlichen, aber veralteten Klassifikation das dritte große Reich eukaryotischer Lebewesen neben den Tieren (Animalia) und den Pflanzen (Plantae). Zu ihnen gehören vor allem Vielzeller wie die Ständerpilze, aber auch Einzeller wie die Backhefe sowie coenocytische Formen mit vielen Zellkernen, aber ohne zellige Untergliederung. Nachdem sie bis in das späte 20. Jahrhundert zu den Pflanzen gerechnet wurden, sind sie nach heutiger Kenntnis näher mit den Tieren als mit den Pflanzen verwandt.

Die Wissenschaft von den Pilzen ist die Mykologie.

Das Wort "Pilz" () ist aus entlehnt; die weitere Herkunft ist unklar. Wahrscheinlich hängt das Wort mit "bolites" Champignon zusammen; "vo̱líti̱s" bezeichnet Dickröhrlinge ("Boletus"), insbesondere den Steinpilz (), aber auch den Satans-Röhrling ().

Die botanische Bezeichnung "Fungi" ( Pilz) lässt sich auf "sphóngos" zurückführen; dies bezeichnete ursprünglich Schwämme. Da sich Pilze ebenso mit Wasser vollsaugen wie Schwämme, wurde der Begriff im Laufe seiner Geschichte auf Pilze übertragen.

Im deutschen Sprachraum existierten die Begriffe "Pilz" und "Schwamm" oder "Schwammerl" einige Zeit parallel. Dabei wurden die Arten mit fleischiger Konsistenz als Pilze und solche, die ein festeres holz-, leder- oder korkartiges Gewebe haben, als Schwämme aufgefasst. Gleichzeitig wurde aber erkannt, dass diese Einteilung aus wissenschaftlicher Sicht nicht sinnvoll ist: Einige ansonsten sehr ähnliche Arten, die derselben Gruppe angehören, wären in die beiden Kategorien aufzuteilen gewesen; außerdem hätte ein und dieselbe Art in der Jugend zu den Pilzen und im Alter zu den Schwämmen oder umgekehrt gehört. Manchmal wurden unter "Schwämme" auch die essbaren und unter "Pilze" die ungenießbaren Arten aufgefasst. Aber auch diese Einteilung ist unhaltbar. Noch heute herrscht im Südosten des deutschen Sprachraums die Bezeichnung "Schwammerl" vor, während sie im Hochdeutschen durch "Pilz" verdrängt wurde.

"Mykologie", der Wortbestandteil "mycetes" und ähnliche Begriffe sind abgeleitet von "mýkēs" Pilz, Mehrzahl .

Nachdem die Pilze wegen ihrer sesshaften Lebensweise lange dem Reich der Pflanzen zugeordnet wurden, gelten sie heute aufgrund phylogenetischer, biochemischer und anatomischer Befunde als eigenes Reich und als enger mit Tieren als mit Pflanzen verwandt. Wie die Tiere gehören sie der Verwandtschaftsgruppe (Taxon) Opisthokonta an.

Pilze sind wie Tiere (zu denen in der Biologie auch die Menschen gezählt werden) heterotroph (speziell chemoorganotroph) und ernähren sich von organischen Nährstoffen ihrer Umgebung, die sie meist durch Abgabe von Enzymen aufschließen und dadurch löslich und für sie verfügbar machen. Eine weitere Gemeinsamkeit von Pilzen und Tieren ist, dass beide das Polysaccharid Glykogen als Speichersubstanz bilden, während Pflanzen Stärke bilden. Die Abgrenzung vom Reich der Tiere erfolgt nicht aufgrund der Unbeweglichkeit der Pilze, da auch manche Tiere, wie Schwämme oder Steinkorallen, den größten Teil ihres Lebens ortsfest verbringen. Wesentliche Unterschiede zu den Tieren bestehen in der Ultrastruktur, so im Vorhandensein von Zellwänden und Vakuolen (wie bei Pflanzen).

Von den Pflanzen unterscheiden sich die Pilze vor allem durch das Fehlen von Plastiden und damit der auf Chlorophyll basierenden Photosynthese. Außerdem enthält die Zellwand der meisten Pilze neben anderen Polysacchariden auch Chitin, das im Pflanzenreich nicht vorkommt, aber der Hauptbestandteil des Exoskeletts der Gliederfüßer ist. Dagegen fehlt den Pilzen das für Pflanzen charakteristische Polysaccharid Zellulose.

Ein grundlegender Unterschied zu den anderen höher organisierten Lebewesen ist, dass – abgesehen von der Ausbildung komplexer Strukturen wie den Fruchtkörpern – jeder Teil des Organismus autark ist und keine Kommunikation zwischen den Teilen stattfindet. Auch ein eigener Tagesrhythmus, wie ihn Tiere und Pflanzen haben, scheint bei Pilzen allenfalls als Ausnahme vorzukommen.

Der Vegetationskörper der meisten Pilze ist ein wenig differenzierter Thallus, der aus mikroskopisch feinen (2–10 µm), fädigen Hyphen besteht. Diese bilden ein weit verzweigtes Myzel, welches sich in oder auf einem festen Substrat, beispielsweise Erdboden, Holz oder anderem lebendem oder abgestorbenem organischem Gewebe, ausbreitet. Viele Pilze bilden außerdem Fruchtkörper, die sich vom Substrat abheben (z. B. die Hüte der Ständerpilze) und ebenfalls aus Hyphen bestehen (siehe unten). Daneben gibt es auch einzellige Pilze wie die Hefen.

Hyphen können durch quer liegende Septen in Zellen untergliedert sein, wobei jede Zelle einen oder mehrere Kerne enthält, oder unseptiert (coenocytisch) sein und viele Kerne enthalten, die sich mit der (auch für Pflanzen charakteristischen, aber bei Metazoa nicht vorkommenden) Plasmaströmung frei bewegen können. Auch die Septen sind aber keine vollständigen Grenzen wie die Zellwände der Pflanzen, sondern haben jeweils in ihrer Mitte eine Pore (z. B. den Doliporus bei den Ständerpilzen), der einen Übergang von Cytoplasma und Organellen, darunter teils auch Kernen, ermöglicht. Daher schreiben David H. Jennings und Gernot Lysek in ihrem Buch "Fungal Biology", dass die Septen keine Querwände sind und die Hyphen der Pilze grundsätzlich nicht zellig gegliedert sind, sondern ein cytoplasmatisches Kontinuum bilden. Die Septen erhöhen die Festigkeit der Hyphen; in unseptierten Hyphen finden sich stattdessen balkenartige Strukturen.

Die Formen der Hyphen können sich artspezifisch stark unterscheiden und spezialisiert sein; so bilden pflanzenparasitische Pilze oft Haustorien (Saugorgane) aus. Diese stülpen sich in pflanzliche Zellen, um dort Nährstoffe aufzunehmen. Einige bodenbewohnende, carnivore (fleischfressende) Pilze sind in der Lage, mit ihren Hyphen "Schlingfallen" für kleine Fadenwürmer (Nematoden) auszubilden (siehe auch nematophage Pilze). Beim Durchkriechen werden die Nematoden dadurch festgehalten, dass sich der Hyphendurchmesser der Schlingenhyphe schnell vergrößert und sich somit die Schlingenöffnung schnell verkleinert. Eine andere Abwandlung vegetativer Hyphen sind die Substrat- oder Lufthyphen: Mehrere Bündel von Hyphen legen sich parallel aneinander und bilden makroskopisch sichtbare Hyphenstränge (Synnemata), aus denen je nach Milieu- oder Umweltänderung entweder Überdauerungsorgane (Sklerotien, Chlamydosporen) oder ungeschlechtlich erzeugte Sporen entstehen können (Konidiosporen).
Die Fruchtkörper der Großpilze, die hut-, keulen-, knollen- oder krustenförmig sein können, bestehen aus verflochtenen Hyphen, welche ein Scheingewebe (Plektenchym) bilden. Dabei sind die Fruchtkörper nur ein kleiner Teil des gesamten Organismus und dienen der Vermehrung, Überdauerung und Ausbreitung durch Bildung von Sporen, die aus einer Meiose hervorgehen. Die Sporen werden bei vielen Pilzen in besonderen Fruchtschichten der Fruchtkörper gebildet, den Hymenien. Bei Hutpilzen befindet sich die Fruchtschicht unter dem Hut und bedeckt dort die Oberflächen der Leisten, Lamellen oder Röhren. Bei vielen Schlauchpilzen befindet sich das Hymenium knapp unter der Oberfläche des Fruchtkörpers in kleinen Kammern, den Perithezien.

Pilze ernähren sich saprotroph, indem sie gelöste Nährstoffe durch die Oberfläche ihrer Hyphen aufnehmen. Um makromolekulare, nicht lösliche Nahrungsquellen aufzuschließen, scheiden sie Enzyme aus, die den Verdauungsenzymen der Tiere (und Menschen) entsprechen.

Die Hyphen wachsen apikal (an der Spitze), ebenso wie die Wurzelhaare und Pollenschläuche der Pflanzen, aber im Unterschied zu fadenförmigen Grünalgen. Sie verzweigen sich durch seitlich aussprossende neue Spitzen, können aber auch an den Spitzen miteinander fusionieren (Anastomose) und so Netzwerke bilden.

Bei einem sich ausbreitenden Myzel können vier Zonen unterschieden werden:

In der Wachstumszone wandern membranumschlossene Bläschen, die als Vesikel bezeichnet werden, zur Hyphenspitze und versammeln sich dort zu dem auch lichtmikroskopisch sichtbaren "Spitzenkörper". Schließlich verbinden sie sich mit der Membran an der Spitze und entlassen dabei ihren Inhalt in die sich ausdehnende Wand jenseits der Membran. Angetrieben wird das Wachstum durch die Aufnahme von Elektrolyten, insbesondere von Kalium-Ionen, und Wasser in der Absorptionszone. Dadurch wird – wie auch bei Pflanzen und Algen – der Turgor, der Druck auf die umgebende Wand, erhöht, und diese dehnt sich an der Stelle (der Hyphenspitze), wo sie dehnbar ist. Der Transport der Vesikel erfolgt entlang von Aktin-Mikrofilamenten, doch scheinen auch Mikrotubuli für die Ausrichtung der Bewegung von Bedeutung zu sein.

Neben Kalium und anderen anorganischen Elektrolyten nimmt die Hyphe im Absorptionsbereich auch lösliche Kohlenhydrate (Zucker) und Aminosäuren, die löslichen Monomere der Proteine, als Nährstoffe auf. Dies bewirkt sie durch einen Export von Protonen (H) durch die Membran mittels eines als Protonenpumpe bezeichneten Enzyms. Dadurch wird das umgebende Medium stark angesäuert, und es resultiert ein elektrochemischer Gradient. Die Enzyme für die externe „Verdauung“ (Hydrolyse) makromolekularer Nahrungsquellen werden an der Hyphenspitze ausgeschieden.

Hyphen können prinzipiell unbegrenzt weiterwachsen, so lange günstige Bedingungen vorliegen und insbesondere Nährstoffe zur Verfügung stehen. Dabei ist das Wachstum nicht "chemotrop" gerichtet, d. h. die Hyphen wachsen nicht in Richtung organischer Nahrungsquellen; vielmehr breitet sich das Myzel, wenn möglich, gleichmäßig in alle Richtungen aus.

Dieses "trophische" Wachstum endet, wenn keine Nährstoffe oder kein Sauerstoff mehr zur Verfügung steht oder wenn durch andere externe Faktoren die Fortpflanzung angeregt wird. Dies wird als Übergang von der "Trophophase" in die "Idiophase" bezeichnet. In der Idiophase werden, im Unterschied zur Trophophase, "Sekundärstoffe" gebildet (vgl. Sekundäre Pflanzenstoffe), die für das bloße Wachstum nicht erforderlich sind, und/oder es werden spezielle Strukturen für die Fortpflanzung ausgebildet. Das Ende der Trophophase ist für die jeweilige Hyphenspitze irreversibel.

Die meisten Pilze vermehren sich überwiegend oder ausschließlich asexuell (ungeschlechtlich). So sind bei vielen Schimmelpilzen und generell bei den Arbuskulären Mykorrhizapilzen keine sexuellen Vorgänge bekannt. Die sexuelle und überwiegend auch die asexuelle Fortpflanzung erfolgt über die Bildung und Verbreitung von Sporen.

Asexuell werden die einzelligen Sporen entweder an den Enden der Hyphen abgeschnürt (Konidien), oder es werden Sporangien gebildet, in deren Innerem auf unterschiedliche Weise Sporen entstehen. Die Sporen werden dann freigesetzt, verbreiten sich und keimen schließlich zu neuen Myzelien aus. Die einzelligen Hefen vermehren sich (mit Ausnahme der Spalthefen) durch Sprossung: Nach einer Kernteilung bildet sich ein Auswuchs, in den einer der Tochterkerne einwandert und der dann abgeschnürt wird. Außerdem können die meisten Pilze sich auch durch Fragmentierung ihrer sich ausbreitenden Myzelien vermehren, weil jeder Teil des Myzels in der Lage ist, sich als eigenständiger Organismus weiterzuentwickeln.

Pilze sind normalerweise haploid, haben also in ihren Zellkernen nur je einen einfachen Chromosomensatz, und durchlaufen nur bei der sexuellen Fortpflanzung eine kurze diploide Phase mit zwei Chromosomensätzen. Dazwischen liegt bei den Ständerpilzen und bei den meisten Schlauchpilzen noch eine zweikernige oder dikaryotische Phase, die bei anderen Lebewesen nicht bekannt ist. In dieser Phase enthält jede Zelle zwei haploide Kerne unterschiedlicher „elterlicher“ Herkunft. Der Ablauf der sexuellen Vorgänge unterscheidet sich bei den verschiedenen systematischen Abteilungen der Pilze sehr.
Bei den Ständerpilzen wird der Übergang von der haploiden zur diploiden Phase dadurch eingeleitet, dass zwei haploide Myzelien sich zu einem Netzwerk verbinden, indem ihre Hyphen paarweise apikal fusionieren (Anastomose). Daraus geht zunächst ein dikaryotisches Myzel hervor, in dem vor jeder Zellteilung beide Kerne sich synchron teilen und jede Tochterzelle dann zwei Kerne unterschiedlicher Herkunft erhält. Die korrekte Zuteilung der Kerne wird durch die seitliche Ausbildung einer Schnalle gewährleistet, durch welche einer der vier Kerne in eine der Tochterzellen gelangt. Das dikaryotische Myzel kann sich lange Zeit rein vegetativ ausbreiten. Die diploide Phase beginnt erst dann, wenn die beiden Kerne eines Dikaryons (einer zweikernigen Zelle) verschmelzen (Karyogamie). Das geschieht in dem als Hut aus dem Substrat herauswachsenden Fruchtkörper, wo die Enden dikaryotischer Hyphen anschwellen und sich zu den charakteristischen Basidien entwickeln, nach denen die Ständerpilze auch als "Basidiomycota" bezeichnet werden. In jeder Basidie entstehen durch Verschmelzung der beiden Kerne und anschließende Meiose vier haploide Zellkerne. Zugleich bildet die Basidie vier Fortsätze, in welche dann je ein Kern einwandert. Die Fortsätze werden abgeschnürt und entwickeln sich zu je einer Basidiospore, die schließlich aktiv abgeschleudert wird.

Bei den Schlauchpilzen bilden benachbarte Hyphen haploider Myzelien vielkernige sogenannte Gametangien aus, die als Ascogon und Antheridium bezeichnet werden. Das Ascogon trägt gewöhnlich eine dünne Hyphe, die Trichogyne, durch welche der Inhalt des Antheridiums in das Ascogon gelangt (Plasmogamie). Die Kerne unterschiedlicher Herkunft lagern sich eng aneinander (Kernpaarung), verschmelzen aber noch nicht miteinander. Nun wachsen aus dem Ascogon dikaryotische (oder paarkernige) Hyphen heraus. Schließlich erfolgt in der apikalen Zelle eine spezielle Zellteilung, die Hakenbildung, die der Schnallenbildung bei den Ständerpilzen ähnelt: Die Hyphenspitze krümmt sich hakenförmig zurück, die beiden Kerne teilen sich synchron, und durch Ausbildung zweier Septen resultiert eine zweikernige Tochterzelle, die jetzt an der Spitze liegt, sowie eine einkernige Stielzelle und der ebenfalls einkernige Haken. Letztere vereinigen sich dann unter Auflösung der zwischen ihnen liegenden Hyphenwände. In der jetzt apikal liegenden Zelle erfolgen nun die Karyogamie und anschließend drei Kernteilungen: eine gewöhnliche Mitose und die beiden meiotischen Teilungen (Meiose I und II). So entsteht der namengebende Schlauch oder Ascus, in dem 8 haploide Kerne in einer Reihe liegen. Anschließend werden im Ascus 8 (oder – nach weiteren Teilungen – ein Vielfaches) dickwandige Ascosporen ausgebildet und freigesetzt.

Die Jochpilze bilden keine Fruchtkörper aus, sondern existieren nur als vielkernige Myzelien. Bei ihnen senden benachbarte Hyphen als Gametangien bezeichnete Fortsätze aus, die sich zu dem namengebenden „Joch“ verbinden. Die Berührungsstelle schwillt dann an, die trennenden Zellwände lösen sich auf, und das vielkernige Verschmelzungsprodukt kapselt sich durch Trennwände von den beiden Gametangien ab. Durch paarweise Verschmelzung der Zellkerne wird die diploide Phase erreicht, und die resultierende Coenozygote (vielkernige Zygote) wird durch Ausbildung einer dicken Wand zur sogenannten Zygospore, die unter widrigen Umständen längere Zeit überdauern kann. Wenn die Zygospore unter günstigen Bedingungen auskeimt, durchlaufen die Kerne die Meiose, und es entwickelt sich wieder ein haploides vielkerniges Myzel.

Die Ausbildung von Fruchtkörpern ist mit einer erheblichen Steigerung der Stoffwechsel-Aktivität verbunden, weil in den Fruchtkörpern wesentlich mehr Proteine und Nukleinsäuren gebildet werden als im Myzel und dies einen erhöhten Energieaufwand erfordert, der sich auch in einem entsprechenden Anstieg des Sauerstoff-Verbrauchs zeigt. Deshalb können Fruchtkörper nur bei einer guten Versorgung mit Sauerstoff gebildet werden, während Myzelien rein vegetativ auch in recht Sauerstoff-armen Umgebungen wie etwa in vermoderndem Holz wachsen können.

Die Chemie der Pilzfarbstoffe ist wegen der Vielzahl der Verbindungen sehr komplex. Einige Farbstoffe liegen in reduzierter Form als Leukoverbindung vor. So wird der Farbstoff Atromentin, ein Terphenylchinon, des Samtfußkremplings bei Verletzung des Pilzes mit Luftsauerstoff und im Pilz vorhandener Oxidasen zu einer blauen Form oxidiert. Ähnliches kann man beim Anschneiden vieler Pilze beobachten. Farbstoffe vom Pulvinsäuretyp kommen bei Dickröhrlingsverwandten, insbesondere der Gattungen Boletus und Xerocomus, vor. Der rote Farbstoff von Hexen-Röhrlingen ist Variegatorubin, der gelbe Farbstoff des Gold-Röhrlings ein Gemisch von Grevillin B und C. Grevilline sind als Farbstoffe bei den Schmierlingen von Bedeutung. Die Huthaut des Fliegenpilzes enthält zahlreiche gelbe, orange und rote Komponenten, die zur Gruppe der Betalaine gehören, sowie Muscaflavin, das auch für die orangen bzw. roten Farben von Saftlingen verantwortlich ist. Im Strubbelkopfröhrling konnte man L-Dopa nachweisen, das bei Verletzung des Fruchtkörpers unter Melaninbildung zu einer Schwarzfärbung führt. Bei bestimmten Pilzen gilt die Biosynthese von Betalaminsäure, die mit Aminosäuren Betalaine bildet, aus L-Dopa als gesichert. Darüber hinaus kommen in Pilzen häufig als Chromophore Carotinoide, Azulenderivate, Anthrachinone, Phenoxazine und Riboflavin vor.

Pilze wirken als Zersetzer toten organischen Materials (Destruenten), ernähren sich als Parasiten von anderen Lebewesen (von Amöben bis zum Menschen), oder sie leben in einer wechselseitigen (mutualistischen) Symbiose mit Pflanzen (Mykorrhiza). Aufgrund der sehr effektiven Verbreitung ihrer Sporen sind sie praktisch überall vorhanden, wo ein geeignetes Substrat verfügbar wird, und insgesamt können sie eine sehr große Bandbreite an Nahrungsquellen nutzen.

Die Pilze bilden die wichtigste Gruppe der am Abbau organischer Materie (tote Lebewesen, Exkremente, Detritus) beteiligten Lebewesen (Destruenten). So sind es fast ausschließlich Pilze, die Lignin (komplexe Verbindungen in verholzten Zellwänden von Pflanzen) aufspalten und verwerten können. Auch beim Abbau von Zellulose, Hemizellulose und Keratin sind sie die wichtigsten Verwerter. Zusammen mit Bakterien und tierischen Kleinstlebewesen bilden sie aus organischem Abfall den Humus.

Die Bedeutung der Pilze beim Abbau des Lignins und namentlich der sehr Lignin-reichen Stämme abgestorbener Bäume ragt in mehrfacher Hinsicht heraus. Nur Pilze, und zwar speziell gewisse Ständerpilze, die als Weißfäulepilze zusammengefasst werden, sind in der Lage, größere Holzstücke effektiv zu zersetzen. Im Unterschied zu Bakterien, von denen manche in begrenztem Maß leicht verfügbare späte Produkte des Ligninabbaus verwerten können, dringen Pilze mit ihren Hyphen aktiv in das Holz ein. Und nur darauf spezialisierte Ständerpilze verfügen über die notwendigen Enzyme für den komplizierten und energieaufwendigen Abbau des Lignins. Dieser ist unter anderem deshalb besonders schwierig, weil Lignin sehr hydrophob (wasserabstoßend) und dadurch für die gewöhnlichen hydrolytischen Abbauprozesse nicht zugänglich ist, und weil er grundsätzlich nur aerob möglich ist, also eine gute Versorgung mit Sauerstoff erfordert. Wo diese nicht gegeben ist, bleibt das Holz lange Zeit erhalten (etwa in Mooren) und wird schließlich über geologisch sehr lange Zeiträume in Kohle umgewandelt (Inkohlung). Von Weißfäule spricht man bei der Zersetzung von Holz durch Pilze, wenn diese vorwiegend das braune Lignin zersetzen und die farblose Zellulose übrig bleibt, während Braunfäulepilze das Lignin nur insoweit abbauen, als es für den Zugang zur Zellulose und den Hemizellulosen nötig ist.

Parasitisch lebende Pilze sind zumeist auf bestimmte Wirtsorganismen spezialisiert. Um geeignete Wirte zu finden, haben sie unterschiedliche Methoden entwickelt. So produzieren Rostpilze große Mengen an Sporen und erhöhen dadurch die Chance, dass einige von ihnen auf kompatible Wirtspflanzen gelangen. Effektiver ist dagegen die Verbreitung durch Insekten, welche die Wirtspflanzen besuchen. Auf diese Weise werden etwa Hefen, die im Nektar leben, von Blüte zu Blüte transportiert. Die Sporen von "Monilinia fructigena", dem Erreger der Fruchtfäule bei Obstbäumen, werden durch Wespen verbreitet, die zugleich durch Anfressen der Früchte den Zugang für den Pilz schaffen.

Brandpilze können jahrelang ohne Wirtspflanzen saprophytisch im Erdreich leben. So sind in einem von "Ustilago maydis", dem Maisbeulenbrand, befallenen Acker noch bis zu 12 Jahre danach infektiöse Myzelien vorhanden, die erneut ausgesäte Maispflanzen sofort parasitieren. Auch Tiere und Menschen ziehen sich Pilzinfektionen zumeist dadurch zu, dass sie mit Sporen (Beispiel Fußpilz) oder mit anderen Wirten in Kontakt kommen.

Wenn ein Pilz eine Pflanze parasitiert, dringt er mit spezialisierten Hyphen, den Haustorien, in dessen Zellen ein. Dabei durchdringen die Haustorien die Zellwand, lassen aber die Zellmembran intakt (denn andernfalls würde das Zellplasma austreten und die Wirtszelle absterben) und stülpen sie nur ein, sodass sie nun von einer Doppelmembran umgeben sind. Durch diese kann das Haustorium dann Nährstoffe aus dem Plasma der Wirtszelle entnehmen, ohne wie in toten Substraten Enzyme ausscheiden und durch fortwährendes Wachstum immer neue Nahrungsquellen erschließen zu müssen, denn die Wirtspflanze liefert die benötigten Substanzen nach, so lange die befallene Zelle am Leben bleibt.
Andererseits können Pilze die Physiologie ihrer Wirtspflanzen erheblich beeinflussen. So zeigt die Zypressen-Wolfsmilch einen stark abweichenden Habitus, wenn sie von dem Rostpilz "Uromyces fabae" befallen ist. Und viele Gräser sind für Weidetiere giftig, wenn sie bestimmte Pilze beherbergen. Da sie selbst dabei keine Anzeichen einer Schädigung aufweisen, kann man hier von einer mutualistischen Symbiose sprechen.

Viele Pilzarten sind als Parasiten an wirtschaftlich wichtigen Nutzpflanzen bedeutende Pflanzenschädlinge. Als solche können sie schwere Pflanzenkrankheiten hervorrufen. Wichtige Beispiele sind die weit verbreiteten Pilzerkrankungen der Kastanien oder der Ulmen. Pilzliche Erkrankungen der Pflanzen können ohne Vorbeugung oder Gegenmaßnahmen zu Totalausfällen und Missernten führen. Zu den Pflanzenschädlingen gehören auch viele Arten der Baumpilze.

Wirtschaftlich bedeutsame Pilzkrankheiten sind Maisbeulenbrand, Weizensteinbrand, Mutterkorn bei Roggen, Verticillium-Welke bei vielen Kulturpflanzen, Apfelschorf ("Venturia"), Birnengitterrost ("Gymnosporangium sabinae"), Obstbaumkrebs ("Nectria galligena") und Echter Mehltau (Erisyphaceae). Daneben existieren noch circa 10.000 weitere pilzliche Pflanzenkrankheiten.

Etwa 90 Prozent aller Landpflanzen können mit bestimmten Pilzen eine Mykorrhiza bilden. Die beteiligten Pilze gehören ganz überwiegend der Klasse der Arbuskulären Mykorrhizapilze an, die mit ihren Hyphen in die Wurzelzellen eindringen ("Endomykorrhiza", von gr. "endo" = innen) und dort durch reiche Verzweigung die namengebenden "Arbuskeln" (von lat. "arbusculum" = Bäumchen) bilden. Seltener, aber für mitteleuropäische Wälder typisch, ist die "Ektomykorrhiza" (von gr. "ekto" = außen), bei der das Pilzmyzel die Wurzeln der Bäume in Form eines Myzelmantels umschlingt und in die Rinde, nicht aber in die Zellen eindringt. Hier sind die beteiligten Pilze zumeist Ständerpilze. Wie bei jeder Symbiose profitieren beide Partner: Die Pflanze erhält über den Pilz mehr mineralische Nährstoffe, da sein feines Myzel den Boden enger durchwirkt, als ihre eigenen Saugwurzeln das könnten. Diese bessere Versorgung macht sich insbesondere in sehr nährstoffarmen Böden bemerkbar. Umgekehrt erhält der Pilz Zucker, den die Pflanze durch Photosynthese erzeugt, als Energiequelle und für die Bildung anderer organischer Substanzen. Daneben sind Mykorrhizapilze allerdings vielfach auch in der Lage, saprophytisch organische Nährstoffe aus dem Erdreich zu gewinnen.
Einen Extremfall stellen die Orchideen dar, von denen viele schon bei der Keimung ihrer Samen unter natürlichen Bedingungen obligat auf ihre pilzlichen Symbiosepartner angewiesen sind. Manche Orchideen, z. B. die Vogel-Nestwurz, enthalten kein Chlorophyll und können daher keine Photosynthese treiben, sondern beziehen alle Nährstoffe von dem Pilz, auf dem sie somit parasitieren. Die gleichen Verhältnisse finden sich auch bei manchen Heidekrautgewächsen wie dem Fichtenspargel. In beiden Fällen bilden die beteiligten Pilze zugleich eine Mykorrhiza mit Bäumen und beziehen von diesen Zucker, wovon sie einen Teil an die Nestwurz bzw. den Fichtenspargel weitergeben (Epiparasitismus). Auf diese Weise können diese Pflanzen in Form von blassen Blütenständen auch an schattigen Stellen im Wald gedeihen.

Flechten sind Pilze, die einzellige Grünalgen oder Cyanobakterien als Symbionten beherbergen und dadurch photoautotroph, d. h. dank der Photosynthese ihrer Symbionten nicht auf externe Nahrungsquellen angewiesen sind. Sie können, ganz anders als jeder der Partner allein, extreme Lebensräume besiedeln. Dabei sind die betreffenden Pilze ohne ihre jeweiligen Symbionten kaum lebensfähig, während letztere auch isoliert gedeihen. Für sie liegt der Vorteil der Symbiose darin, dass sie ihnen ein viel breiteres Spektrum an Lebensräumen eröffnet.

Auch in marinen Lebensräumen, also in stark salzhaltigem Milieu, sind Pilze, insbesondere Schlauchpilze, verbreitet. Dem hohen osmotischen Druck begegnen sie durch eine entsprechende Anreicherung von Polyolen (höherwertigen Alkoholen), hauptsächlich Glyzerin, aber auch Mannit und Arabit, in den Hyphen. Ähnlich verhält es sich bei xerophilen Schimmelpilzen und Hefen, die etwa auf Salzheringen oder auf Marmelade wachsen können.

Die allermeisten Pilze benötigen Sauerstoff; sie sind obligat aerob. Manche können jedoch zeitweilig ohne Sauerstoff auskommen (fakultative Anaerobie) oder haben sogar die Fähigkeit verloren, ihn überhaupt zu nutzen (obligate Anaerobie). Letzteres trifft auf die Neocallimastigaceae zu, die im Pansen von Wiederkäuern leben und auf die Verwertung von Zellulose spezialisiert sind. Fakultative Anaerobier sind dagegen die Hefen, die unter anaeroben Bedingungen zur Gärung übergehen, mit der sie – wesentlich weniger effektiv als mit der aeroben Atmung – z. B. von Zucker leben können. Auch manche Schimmelpilze sind dazu in der Lage, und z. T. gehen sie dann auch morphologisch in ein Hefe-artiges Stadium über.

Myzelien wachsen zumeist im Dunkeln. Wenn Hyphenspitzen die dem Licht ausgesetzte Oberfläche des Substrats erreichen, regt das Licht (genauer: dessen blaue Anteile) die Bildung von Sporen an, und auch die Entwicklung der Fruchtkörper kann lichtabhängig sein. Dieser Effekt ist jedoch lokal begrenzt und wirkt sich nicht auf das übrige Myzel aus. Bei vielen Pilzen wachsen die sporenbildenden Hyphen (Konidiophoren oder Sporangiophoren) in die Richtung des einfallenden Lichtes. Bei der Gattung "Pilobolus" (Mucorales) wird schließlich das gesamte Sporangium, das die reifen Sporen enthält, exakt in Richtung der Lichtquelle abgeschleudert.

Etwa 180 Pilzarten können beim Menschen verschiedene Pilzkrankheiten hervorrufen. Weit größer ist aber der Nutzen vieler Pilze für den Menschen, etwa als Speisepilze oder bei der Herstellung von Hefeteig und alkoholischen Getränken.

Viele Pilzarten sind bekannte und beliebte Nahrungsmittel. Dazu gehören nicht kultivierbare Arten wie Steinpilz und Pfifferling, aber auch Kulturarten und -sorten von Champignon, Shiitake und Austernpilz. Beim Sammeln von Wildpilzen ist größte Sorgfalt geboten, um nicht durch versehentlich geerntete Giftpilze eine Pilzvergiftung zu riskieren. Zudem ist zu beachten, dass Pilze Schwermetalle und Radionuklide aufnehmen und anreichern. Dies kann zu gesundheitsgefährdenden Konzentrationen von Schwermetallen beziehungsweise Radionukliden im Fruchtkörper von Wildpilzen führen. Wer Pilze für den Verzehr sammelt, muss unbedingt die Speise- und Giftpilze gründlich kennen und darf nur zweifelsfrei erkannte Speisepilze nehmen. Viele Pilzarten enthalten Hämolysine oder andere hitzelabile Gifte, die erst durch Erhitzen zerstört werden. Die meisten Speisepilze erfordern daher Erhitzen durch Kochen oder Braten vor dem Verzehr, um Verdauungsbeschwerden oder Vergiftungen zu vermeiden.

Die meisten Speisepilze gehören zu den Ständerpilzen. Relativ wenige Speisepilzarten, darunter die Morcheln und die Trüffeln, stammen aus der Abteilung der Schlauchpilze.

Bei manchen Pilzen unterscheidet sich der Speisewert in verschiedenen Regionen. Einige Arten wie beispielsweise der Wollige Milchling, die gemeinhin als ungenießbar gelten, werden in Osteuropa für Speisezwecke verwendet. Selbst giftige Arten wie die Frühjahrslorchel werden in Skandinavien verzehrt. Auch in derselben Region kann sich die Einstufung der Genießbarkeit innerhalb mehrerer Jahrzehnte ändern. Beispielsweise galt der heute als giftig angesehene Kahle Krempling früher als essbar.

"Siehe auch: Liste der Giftpilze, Pilzberatungsstelle"
"Siehe auch die Kategorien und "

Von den einzelligen Pilzen sind die Zuckerhefen der Gattung "Saccharomyces", insbesondere die Backhefe ("S. cerevisiae"), die bekanntesten Nutzpilze. Sie erzeugen durch alkoholische Gärung aus Zucker Alkohol und Kohlendioxid und werden in der Bierbrauerei, bei der Herstellung von Wein, sonstiger alkoholischer Getränke und bestimmter (auch alkoholischer) Sauermilchprodukte sowie zum Backen verwendet. In der Regel verwendet man heute Reinzuchthefen, doch insbesondere bei der Weinherstellung werden vielfach weiterhin die natürlicherweise auf der Oberfläche der Weinbeeren lebenden Hefen verwendet. Der beim Brotbacken verwendete Sauerteig enthält neben Milchsäurebakterien auch Hefe.

Bei der Weinherstellung spielt außerdem der Myzelpilz "Botrytis cinerea" eine Rolle. Er erzeugt bei herbstlich kühlfeuchtem Wetter bei den Beeren eine Edelfäule, die bewirkt, dass die Beerenhaut perforiert wird. Der dadurch bedingte Wasserverlust erhöht die Zuckerkonzentration.

Viele Arten spielen auch beim Reifeprozess von Milchprodukten, insbesondere von Sauermilchprodukten und Käse, eine bedeutende Rolle.

Seit Beginn des 20. Jahrhunderts nutzt man Pilze auch für medizinische Zwecke. Medikamente wie das Antibiotikum Penicillin werden aus Pilzen gewonnen. Weitere Stoffwechselprodukte von Pilzen wirken cholesterinsenkend oder helfen gegen Malaria.

Andererseits verursachen Pilze bei Menschen Erkrankungen. Die am häufigsten betroffenen Körperstellen sind die Haut (insbesondere an Kopf, Füßen und Händen), Haare, Nägel und Schleimhäute. Die wohl bekanntesten Pilzkrankheiten des Menschen sind Haut- und Nagelpilzerkrankungen.

Auf der Haut des Menschen lebt eine Vielzahl von Bakterien und Pilzen, die ihm aber normalerweise nicht schaden. Sie siedeln in den oberen Hautschichten und ernähren sich von abgestorbenen Hautzellen und Schweiß. Faktoren wie Stress, ein geschwächtes Immunsystem, hormonale Umstellungen o. Ä. können dazu führen, dass ansonsten harmlose Pilze Krankheiten auslösen, die die Kopfhaut, die Scheide (bei einer beginnenden Schwangerschaft) oder andere innere Organe befallen.

Fußpilze sind weit verbreitet, da sie sehr leicht übertragen werden. Einige ihrer Sporen überleben jahrelang und sind gegen normale Hygienemaßnahmen unempfindlich. Weiterhin werden sie sehr leicht von den Füßen auf andere Körperstellen wie Geschlechtsorgane, Mund und Schleimhäute übertragen. Schwimmbäder gehören zu den Hauptquellen von Fußpilzen.

Weitere Beispiele sind:

Medikamente zur Behandlung von Pilzkrankheiten werden Antimykotika genannt. Sie werden bei lokalem Pilzbefall von Haut oder Schleimhäuten und auch bei systemischen Pilzinfektionen angewendet.

Bestimmte Pilze werden auch als Heilpilze verwendet. In China sind zahlreiche Großpilze seit Jahrhunderten Bestandteil der traditionellen chinesischen Medizin. Der Shiitake ("Lentinula edodes") galt schon in der Ming-Dynastie (1368–1644) als Lebenselixier, das Erkältungen heilen, die Durchblutung anregen und die Ausdauer fördern sollte. Der Glänzende Lackporling ("Ganoderma lucidum"), bekannt als Ling-Zhi oder Reishi, soll ein besonders wirksames Tonikum sein. Der Pom-Pom-Pilz oder Igel-Stachelbart/Affenkopfpilz ("Hericium erinaceus") wird bei Erkrankungen des Magens empfohlen. Der europäische Apothekerschwamm oder Lärchenbaumschwamm ("Laricifomes officinalis") ist als Heilmittel hoch geschätzt. Sein wirksamer Bestandteil ist Agaricinsäure, die stark abführend wirkt und für den außerordentlich bitteren Geschmack verantwortlich ist.

Als "psychoaktive Pilze" oder "Rauschpilze" werden Pilze bezeichnet, die psychotrope Stoffe wie Psilocybin, Psilocin, Baeocystin, Muscimol oder Ergin enthalten. Am bekanntesten sind psilocybinhaltige Pilze, die oftmals als "Magic Mushrooms" bezeichnet werden. Ihre Wirkung wird oft als ähnlich dem LSD beschrieben. Zu ihnen gehören exotische Arten wie der Kubanische ("Psilocybe cubensis") oder der Mexikanische Kahlkopf ("Psilocybe mexicana"), aber auch einheimische Arten, wie der Spitzkegelige Kahlkopf ("Psilocybe semilanceata"). Unerfahrene Pilzsucher riskieren mit dem Sammeln psilocybinhaltger Pilze ihre Gesundheit wegen der Verwechslungsgefahr mit anderen, giftigen Pilzarten. Der Fliegenpilz enthält die giftige und selbst schon psychotrope Ibotensäure, die beim Trocknen in das wesentlich wirksamere Alkaloid Muscimol umgewandelt wird; beide Substanzen werden den Delirantia zugerechnet. Das Mutterkorn beinhaltet neben anderen (giftigen) Stoffen auch das psychoaktive Ergin. Psychoaktive Pilze hatten und haben noch heute bei verschiedenen Völkern eine spirituelle Bedeutung als entheogene Stoffe.

Der als Baumschädling vor allem in Buchen und Birken wachsende Zunderschwamm, "Fomes fomentarius", ein Weißfäulepilz, wurde früher zum Feuermachen verwendet: Das Innere der aus den Baumstämmen konsolartig herauswachsenden Fruchtkörper wurde gekocht, getrocknet, weichgeklopft, mit Salpeterlösung getränkt und erneut getrocknet. Der so erhaltene Zunder kann durch Funken entzündet werden.

Durch bloßes Kochen, Trocknen und Weichklopfen kann aus dem Fruchtkörper-Inneren auch ein dem Filz ähnliches Material gewonnen werden, das zur Herstellung verschiedener Gebrauchsgegenstände (Mützen, Taschen und dergleichen, siehe Bild) verwendet werden kann.

Die nächsten Verwandten der Pilze sind neben den Holozoa (zu denen auch die vielzelligen Tiere (Metazoa) und deren Schwestergruppe, die Kragengeißeltierchen (Choanomonada) gehören) vor allem die parasitisch lebenden Rozella. Ob auch die einzelligen Mikrosporidien (Microsporidia, auch Microspora genannt) zu den Pilzen zu zählen sind, ist derzeit noch unklar. Die kleinste gemeinsame Klade von Pilzen und Tieren wird als Opisthokonta bezeichnet und nach Adl et al. 2012 folgendermaßen aufgestellt:


Als gemeinsamer Vorfahr von Tieren und Pilzen kann ein geißeltragender Einzeller (Flagellat) angenommen werden, der biologisch demnach sowohl den heutigen Töpfchenpilzen als auch den Kragengeißeltierchen (Choanoflagellata) ähnelte.

Aufgrund der im Laufe der Zeit erweiterten Erkenntnisse zur Systematik, werden einige Taxa, die früher als dem Reich der Pilze zugehörig angesehen wurden, inzwischen nicht mehr zu den Pilzen gezählt. Dies trifft auf die früher als "Niedere Pilze" bezeichneten Schleimpilze und andere pilzähnliche Protisten wie die Eipilze (Oomycota), die Netzschleimpilze (Labyrinthulomycetes), "Hyphochytriales" (einzige Ordnung der "Hyphochytriomycota"), die früher "Plasmodiophoromycetes" genannten "Phytomyxea" und die früher den "Phycomycetes" (hingegen heute den "Ichthyosporea") zugerechneten Taxa "Ichthyophonae" (auch "Amoebidiidae" genannt) und "Eccrinales" zu.

Man kennt heute etwa 100.000 Pilzarten. Fachleute schätzen aber weit über 1.000.000 Arten. 2017 wurde die Gesamtartenzahl aller Pilze weltweit zwischen 2,2 und 3,8 Millionen Arten berechnet. Viele Pilzarten haben die Fähigkeit zur geschlechtlichen Vermehrung verloren. Die früher auch Echte Pilze oder "Höhere Pilze" (Eumycota) genannten Lebensformen werden in die folgenden fünf Abteilungen unterteilt:

Technische Fortschritte in der molekularen Genetik und die Anwendung von computerunterstützten Analysemethoden haben detaillierte und sichere Aussagen über die systematischen Beziehungen der oben aufgeführten Pilztaxa zueinander ermöglicht. Beispielsweise wurden manche Verwandtschaften bestätigt, die vorher aufgrund morphologischer, anatomischer und physiologischer Unterschiede oder Gemeinsamkeiten nur vermutet werden konnten.

Die Töpfchenpilze haben sich demnach sehr früh von den anderen Pilzen abgespalten und viele ursprüngliche Merkmale, wie begeißelte Sporen, bewahrt. Die Jochpilze dagegen stellen sehr wahrscheinlich keine einheitliche (monophyletische) Verwandtschaftsgruppe, sondern eine polyphyletische Gruppe verschiedener Abstammungslinien dar. Die Gattung "Amoebidium", die bisher zu den Jochpilzen gezählt wurde, gehört demnach nicht einmal zum Pilzreich. Die arbuskulären Mykorrhizapilze, die ursprünglich ebenfalls zu den Jochpilzen gestellt wurden, werden heute als eigenständige Verwandtschaftsgruppe angesehen, die meist in den Rang einer eigenen Abteilung erhoben wird. Sie wird dann als evolutionäre Schwestergruppe eines Taxons aus Schlauch- und Ständerpilzen angesehen, das man als Dikaryomycota bezeichnet.

Diejenigen Arten, die vorläufig nicht eindeutig einer der oben genannten Gruppen zugeordnet werden können, wurden provisorisch zu den Fungi imperfecti (Deuteromycota) gestellt; dies stellte jedoch nur ein provisorisches und künstliches Formtaxon dar.
Im Frühjahr 2007 veröffentlichten 67 Wissenschaftler aus 13 Ländern als Teil des Projekts "Assembling the Fungal Tree of Life" das abschließende Resultat einer konzertierten und umfassenden Forschungsanstrengung mit dem Ziel, die bislang inkonsistente und unklare Taxonomie der Pilze zu bereinigen. Dabei berücksichtigten sie nicht nur neueste molekulare und genetische Daten verschiedener Pilzspezies, sondern auch die Entwicklungsgeschichte der jeweiligen Nomenklatur.
Als Ergebnis schlagen die Forscher eine neue Klassifizierung vor, in der das Reich der Pilze in 195 Taxa untergliedert wird. Auf diese Weise hoffen die Taxonomen, den bestehenden Bezeichnungswirrwarr in der wissenschaftlichen Literatur zu beenden und eine durchgehende Konsistenz der verschiedenen (Online-)Datenbanken zu erreichen.

Ein Beispiel der Veränderungen in der derzeitigen Systematik der Pilze, die sich daraus ergeben, ist die Auflösung des Phylums der Jochpilze ("Zygomycota"), zu denen auch bestimmte auf Früchten lebende Schimmelpilze gehören. Die betroffenen Taxa würden auf andere Gruppen aufgeteilt werden.

Vermutlich existieren Pilze schon seit 900 bis 1200 Millionen Jahren. Ein Fund aus 850 Millionen Jahre altem Schiefergestein in Kanada wird manchmal als Pilzfossil gedeutet. Angebliche, ältere Funde aus China und Australien mit einem Alter von 1,5 Milliarden Jahren müssen jedoch erst noch als Pilze bestätigt werden.

Die ersten weitgehend unumstrittenen Pilzfunde stammen aus der erdgeschichtlichen Epoche des Ordoviziums und können vielleicht den Arbuskulären Mykorrhizapilzen zugeordnet werden. Der erfolgreiche Landgang der Pflanzen wäre ohne Pilzsymbiosen vermutlich nicht möglich gewesen.

Fossile Pilze sind ferner aus Bernsteinfunden u. a. auf karbonischer Lagerstätte in Schottland und England (sogenannter "Middletonit"), aus dem Karnium (Obertrias) in Deutschland und in bemerkenswerter Artenvielfalt aus kreidezeitlichem kanadischen Bernstein sowie dem mexikanischen, dominikanischen und baltischen Bernstein (alle Tertiär) bekannt. Bei einigen dieser Funde handelt es sich um Pilze, die Termiten und Nematoden befallen hatten und zusammen mit ihren Wirten vom Harz eingeschlossen wurden.

Die ältesten bekannten Fossilien fleischfressender Pilze sind etwa 100 Millionen Jahre alt (Grenze zwischen Ober- und Unterkreide). Sie wurden von Forschern der Humboldt-Universität zu Berlin um Alexander Schmidt in Bernstein aus dem Südwesten Frankreichs gefunden. Die Art lebte im küstennahen Wald und bildete wohl eine Übergangsform zwischen hefeähnlichen aquatischen Pilzen und modernen fleischfressenden Pilzen.

Der griechische Arzt Pedanios Dioscurides schrieb schon im ersten Jahrhundert nach Christus in seinem Lehrbuch davon, dass es zwei Arten von Schwämmen gebe: Die einen sind zum Essen bequem, die anderen aber ein tödlich Gift.
Dioscurides vermutete (fälschlich), dass die Giftigkeit eines Pilzes von seinem Standort abhänge: Pilze, die neben verrostetem Eisen, faulendem Tuch, Schlangenhöhlen oder Bäumen mit giftigen Früchten wachsen, seien alle miteinander giftig. Er erkannte aber schon damals die schwere Verdaulichkeit von übermäßiger Speisepilz-Kost, die den Menschen würgen und ersticken ließen. Auch Adamus Lonicerus schrieb im 16. Jahrhundert in seinem Kräuterbuch über die Pilze, dass es die Natur aller Schwämme sei, zu bedrängen; sie seien kalter, phlegmatischer, feuchter und roher Natur.

Auch später und teilweise bis heute haben sich einige Vermutungen über Anhaltspunkte gehalten, die zur Unterscheidung essbarer und giftiger Pilze dienen sollen. Eine der bekanntesten ist der Trugschluss, dass Fruchtkörper, die von Tieren angefressen wurden, nicht giftig seien. Diese Annahme entspricht der Vorstellung, dass Pilze, die für Tiere unschädlich sind, auch für Menschen ungiftig seien. Weitere vermeintliche Indikatoren sind, dass Pilze, die bei Schlangennestern, Schimmelstellen oder giftigen Bäumen wachsen, giftig seien oder dass bei Berührung mit Gift Löffel aus Zinn oder Silber braun anlaufen, Zwiebeln sich schwarz färben, Eiweiß bleigrau oder Salz gelb. Spätestens seit Mitte des 19. Jahrhunderts ist jedoch bekannt, dass all diese Erscheinungen keine Anhaltspunkte für die Unterscheidung essbarer und giftiger Pilze bieten.

Bis in die Neuzeit hinein wurde das Erscheinen von Pilzen mit Miasmen erklärt: Pilze entstünden durch schlechte Ausdünstungen der Erde oder durch faulenden Untergrund.
Auch der Glaube an die Urzeugung ("generatio spontanea") wurde durch Pilze genährt, weil man ihre Sporen vor der Erfindung des Mikroskops nicht sehen konnte. Adamus Lonicerus schrieb, dass bestimmte Pilze "Schwämme der Götterkinder" seien, weil sie ohne einen Samen wüchsen, daher würden sie auch von den Poeten "Gygenais", "terra nati" (Kinder der Erde), genannt.

Zum lange Zeit eher sinistren Bild der Pilze in der Öffentlichkeit haben früher unerklärliche Phänomene wie der Hexenring und das nächtliche grüne Leuchten des Hallimasch-Myzels beigetragen.

Der größte bekannte Pilz der Welt ist ein Hallimasch. Er befindet sich in Oregon und wird mit einer Ausdehnung des Myzels über 880 Hektar als das größte bekannte Lebewesen betrachtet. Sein Gewicht wird von Fachleuten auf 600 Tonnen geschätzt.

Der Pilz mit dem größten bekannten "Fruchtkörper" ist ein Exemplar der Art "Phellinus ellipsoideus" (Borstenscheiblingsartige), das im Jahr 2010 in der chinesischen Provinz Hainan gefunden wurde. Der Fruchtkörper war 10,85 Meter lang, 82 bis 88 Zentimeter breit und 4,6 bis 5,5 Zentimeter dick. Untersuchungen der Dichte des Pilzes ergaben, dass der gesamte Fruchtkörper 400 bis 500 Kilogramm wog. Sein Alter wurde auf etwa 20 Jahre geschätzt.

Der vorherige Rekordhalter war laut Guinness-Buch der Rekorde ein Porenpilz der Art "Rigidioporus ulmarius", der mehrjährige Hüte ausbildet. Er befindet sich in den Royal Botanic Gardens in Kew in einer schattigen Ecke. Der Fruchtkörper wird jedes Jahr im Rahmen eines Rituals gemessen. Im Jahr 1996 hatte er eine Länge von 170 Zentimetern und eine Breite von 146 Zentimetern. Sein Gewicht wird auf 284 Kilogramm geschätzt. Zuvor hielt ein Exemplar der Art "Bridgeoporus nobilissimus" mit 160 Kilogramm den Rekord; die Spezies erreicht Hutdurchmesser von bis zu zwei Metern.

Der Blätterpilz mit den größten Fruchtkörpern ist "Termitomyces titanicus" mit einem Hutdurchmesser von bis zu 100 Zentimetern; sein Stiel wird bis zu 50 Zentimeter lang. Der Pilz ist in der afrikanischen Savanne anzutreffen und lebt in Symbiose mit bestimmten Termitenarten; er gilt als guter Speisepilz. Ähnliche Ausmaße erreicht "Macrocybe titans".

Unter den Dickröhrlingsartigen zählt "Phlebopus marginatus" mit bis zu 100 Zentimeter im Hutdurchmesser zu den größten Arten. Solche Einzelexemplare können bis zu 29 Kilogramm wiegen. Der Pilz ist in Indonesien, Malaysia und Sri Lanka sowie in Australien und Neuseeland verbreitet. Ein weiterer Vertreter der Gattung ist "Phlebopus colossus", der einen Hutdurchmesser von bis zu 60 Zentimetern erreicht.

Zu den größten Fruchtkörpern eines Leistenpilzes gehört ein Exemplar, das Anfang August 1711 von einem Gerichtsältesten und Förster in Türchau (heute Turoszów) gefunden wurde. Der betreffende Pilz gehörte zur Gruppe der Ziegenbärte (Korallen); der Fruchtkörper besaß ein Gewicht von 42 Pfund (ca. 19,1 kg), ein weiterer 15 Pfund (ca. 6,8 kg). Ersterer hatte einen Umfang von 4,5 Ellen (2,55 m, nach "Dresdner Elle" = 56,638 cm) sowie einen Durchmesser von 1 Elle 1,5 Viertel (78 cm). Beide wurden in einer Schubkarre ins Dorf gefahren und unter den Nachbarn verteilt.

Der Pilz mit dem stärksten Gift ist laut Guinness-Buch der Rekorde "Galerina sulciceps", der bei 72 Prozent der Menschen, die diesen Pilz verzehrt haben, zum Tod führt. Aber der Grüne Knollenblätterpilz ("Amanita phalloides") ist mit einem Anteil von 90 Prozent an tödlichen Vergiftungen der gefährlichere Giftpilz, obwohl ersterer Studien zufolge das stärkere Gift enthält.

Allgemeines

Gesundheit

Historisches

Deutschsprachig

Englischsprachig


</doc>
<doc id="11108" url="https://de.wikipedia.org/wiki?curid=11108" title="Unterschrift">
Unterschrift

Unterschrift (auch Signatur, von lateinisch "signatum" „das Gezeichnete“ zu "signum" „Zeichen“) ist die handschriftliche, eigenhändige Namenszeichnung auf Schriftstücken durch eine natürliche Person mit mindestens dem Familiennamen. Die Unterschriftsleistung ist zur Gültigkeit von Rechtsgeschäften, die mindestens der Schriftform bedürfen, erforderlich. „Unter-schrift“ ist eine Lehnübersetzung zum lateinischen "sub-scriptio" zu "sub" „unter“ und "scrībere" „schreiben“.

Fehlt auf Schriftstücken die erforderliche Unterschrift oder ist sie aus bestimmten Gründen ungültig, so entfalten diese Schriftstücke keinerlei Rechtswirkungen, Verträge sind entsprechend nichtig. Auch ein guter Glaube an die Echtheit von Unterschriften genießt keinen Rechtsschutz, sodass ungültige oder gefälschte Unterschriften nicht zu rechtswirksamen Verträgen führen. 

Historisch geht die Verwendung der Unterschrift in Rechtsakten wahrscheinlich auf das "Siegel" zurück.

Schon im Frühmittelalter finden sich "Signaturen" unter Dokumenten, etwa der Ostarrîchi-Urkunde Kaiser Ottos III. von 996. Hierbei schreibt der Schreiber das "Monogramm" unter den Text, der Herrscher signiert mit einem Punkt von eigener Hand ("Autograph"). Signaturen finden sich, über ein reines Symbol wie etwa die Steinmetzzeichen hinausgehend ab der Renaissance. In der Malerei etwa als „ops fec“ (lat. "opus fecit" „das Werk hat gemacht“) mit Namensnennung als Urheberangabe eines Künstlers auf seinem Werk, oder als "Hausmarke". Diese "Signierung" wird im Barock zu einem Identitätsnachweis, aber auch einem Identifikationszeichen im Sinne eines personalisierten Markenzeichens, das Eindeutigkeit als "Namenszeichen" über Lesbarkeit des Namens stellt ("Autogramm"). Auch heute gilt geschäftlich ein Handzeichen anstelle einer vollständigen Unterschrift, sofern es notariell beurkundet ist.

Während in Europa seit der beginnenden Neuzeit die handschriftliche Unterzeichnung vor Zeugen als rechtsverbindlich gilt, ist etwa im ostasiatischen Kulturkreis noch immer das "gestempelte Siegel" (Chinesisches Siegel , japanisches Hanko ) die verbindliche rechtsgültige Unterschrift. Signaturstempel sind auch in anderen Ländern oder Institutionen gebräuchlich.

Die moderne Datenverarbeitung erfordert neue rechtsverbindliche Formen einer Unterschrift im Sinne einer persönlichen Willensäußerung, die "elektronische Signatur". Der Versuch, in elektronischen Kommunikationsmedien die Unterschrift wieder zu einem persönlichen Merkmal zu machen, hat die "Signatur" hervorgebracht, einen kurzen Textabschnitt unter E-Mails und Usenet-Beiträgen. Die Unterschrift dagegen auf einem Schreibtablet ohne elektronische Signatur genügt nach einem Urteil des Oberlandesgerichts München nicht einer gesetzlich erforderlichen Schriftform.

Die eigenhändige Unterschrift unter einem Text wahrt nach deutschem Zivilrecht sowohl die in Abs. 1 BGB gesetzlich vorgeschriebene Schriftform als auch die freiwillige – also ohne gesetzliche Notwendigkeit verwendete – Schriftform sowie den Urkundencharakter von privaten Urkunden gemäß ZPO. Sinn der Unterschrift ist, den Aussteller der Urkunde erkennbar zu machen und ihre Echtheit zu garantieren (siehe auch Unterschriftenfälschung und Überweisungsbetrug). Ein solcher Namenszug gilt der Rechtsprechung zufolge als "einmalig" und Bekundung des Willens, in der Rechtspraxis vor allem bei Willenserklärungen, Beglaubigungen sowie als Identitätsnachweis. Weiteres wesentliches Merkmal einer Unterschrift ist, dass sie von Dritten nicht ohne weiteres nachgeahmt werden kann.

Rechtlich unterscheidet man zwei Arten von Unterschriften:



Der Personenname muss als Name erkennbar sein, mindestens müssen Andeutungen von Buchstaben zu erkennen sein, sonst fehlt es am Merkmal einer Schrift. Schrift sind alle Zeichen, die dazu bestimmt sind, einen beliebigen Gedankeninhalt für andere lesbar zu machen. Dabei ist die vollständige Lesbarkeit einer Unterschrift jedoch nicht erforderlich. Die Unterschrift muss bei Unleserlichkeit wenigstens einen individuellen Charakter aufweisen. Das Schriftzeichen muss einzelne individuelle Merkmale enthalten. Nicht rechtswirksam sind senkrechte oder schräg nach oben oder unten gezogene Striche, Wellenlinien oder gekrümmte Linien. Erforderlich, aber auch ausreichend ist ein die Identität des Unterschreibenden hinreichend kennzeichnender individueller Schriftzug, der einmalig ist, entsprechend charakteristische Merkmale aufweist und sich als Wiedergabe eines Namens darstellt. Die Lesbarkeit des Vornamens allein genügt nicht, wenn der Familienname in der Unterschrift völlig fehlt.

Der BGH hat die Bedingungen, die an eine Unterschrift zu stellen sind, wie folgt zusammengefasst: Unterschiedlich beurteilt wird die Frage, ob und inwieweit einzelne Buchstaben – wenn auch nur andeutungsweise – erkennbar sein müssen, weil es sonst am Merkmal einer Schrift fehlt. Wenn lediglich ein Buchstabe erkennbar ist und darüber hinaus keine ausreichenden individuellen Merkmale hervortreten, erfüllt das nicht die Voraussetzungen einer Unterschrift. Wird eine Erklärung mit einem Handzeichen unterschrieben, das nur einen Buchstaben verdeutlicht, oder mit einer Buchstabenfolge, die erkennbar als bewusste und gewollte Namensabkürzung erscheint, liegt keine Namensunterschrift im Rechtssinne vor. Ob ein Schriftzeichen eine Unterschrift oder lediglich eine Abkürzung (Handzeichen, Paraphe) darstellt, beurteilt sich nach dem äußeren Erscheinungsbild; dabei ist ein großzügiger Maßstab anzulegen, sofern die Autorenschaft gesichert ist. Steht nach ZPO die Echtheit der Namensunterschrift fest, so hat die über der Unterschrift stehende Schrift die Vermutung der Echtheit für sich.

Die schriftlich abgefasste Urkunde ist vom Aussteller eigenhändig durch Namensunterschrift oder durch notariell beglaubigtes Handzeichen zu unterzeichnen ( Abs. 1 BGB). Bei einem Vertrag müssen die Beteiligten auf derselben Urkunde unterzeichnen ( Abs. 2 BGB). Die Urkunden haben das gesamte Rechtsgeschäft zu enthalten, Unterschriften müssen den Urkundentext räumlich abschließen – sie stehen immer unter dem Text. Schreibt das Gesetz für eine Erklärung die Schriftform vor, verlangt Satz 1 BGB lediglich, dass die Urkunde von dem Aussteller durch Namensunterschrift eigenhändig unterzeichnet ist. Danach braucht der Text nicht fertig gestellt zu sein, wenn die Unterschrift geleistet wird. Der Erklärende kann das Papier auch blanko unterzeichnen, die Schriftform ist in diesem Falle mit Vervollständigung der Urkunde gewahrt.

Die Unterschrift muss den Urkundentext räumlich abschließen und darf deshalb nicht „Überschrift“ sein. Damit bezweckt das Gesetz, dass der Unterschriftsleistende den vorangehenden Text auch gelesen hat und aus diesem Grunde mit seiner Unterschrift den Inhalt der Urkunde für Beteiligte als verbindlich anerkennt. Mit der Unterschrift bringt der Unterzeichner den unbedingten Willen zum Ausdruck, die volle Verantwortung für den Inhalt des Schriftsatzes zu übernehmen. Eine „Oberschrift“ am oberen Rand wie bei den zeitweilig von Kreditinstituten eingesetzten Überweisungsformularen genügt ebenso nicht wie „Nebenschriften“, denn beide erfüllen jedenfalls nicht die einer Unterschrift zukommende Funktion, den Urkundentext räumlich und zeitlich abzuschließen, weil sie nicht einmal vom äußeren Erscheinungsbild her geeignet sind, die Übernahme der Verantwortung für den auf dem Schriftstück befindlichen Text auszudrücken. 

Rechtsverbindlich und zulässig ist die Unterschrift mit einem Pseudonym, sofern die als Aussteller in Betracht kommende Person ohne Zweifel feststeht oder mit einem Teil eines Doppelnamens. Wird mit dem Künstlernamen unterschrieben, so ist damit der gesetzlichen Schriftform genügt und die Eigenhändigkeit gewahrt. Die Unterzeichnung mit einer Verwandtschaftsbezeichnung, einem Titel, einer Rechtsstellung oder den Anfangsbuchstaben, den Initialen (so genannte Paraphe) sind keine Unterschrift. Eine Schreibhilfe durch Führen der Hand des Schreibenden macht die so zustande gekommene Unterschrift noch nicht ungültig, selbst wenn die Unterschrift anschließend mehr der Schrift des Schreibhelfers ähnelt, solange gewährleistet ist, dass der „Unterschreibende“ die Unterschrift tatsächlich leisten will.

Das Kürzel „"gez."“ ("gezeichnet") wird in der Regel dann verwendet, wenn auf eine handschriftliche Unterschrift verzichtet wird und lediglich der (gedruckte) Name des Unterzeichnenden folgt. Das Kürzel „gez.“ bedeutet: . Entsprechende Briefe enthalten oft zusätzliche Hinweise wie "„Dieses Schreiben wurde maschinell erstellt und ist auch ohne Unterschrift gültig“". Beide Varianten sind meist nur im Massenbetrieb großer Firmen oder Behörden üblich. Dieser Satz genügt nicht der Schriftform, es sei denn, das Gesetz lässt im Massenverkehr Ausnahmen zu ( Abs. 2 Satz 2 BGB, Satz 1 AktG, Abs. 1 Satz 2, Abs. 1 Satz 1, Abs. 4 Versicherungsvertragsgesetz). Dahingegen genügt ein solcher Satz jedoch der Textform nach BGB, wonach die Informationsfunktion einer schriftlichen Erklärung in den Vordergrund tritt und auf eine eigenhändige Unterschrift verzichtet wird.

Sogar eine gefälschte Unterschrift ist formgültig, sie bindet jedoch nicht den Namensträger, sondern analog BGB den Fälscher.

Häufig steht vor Unterschriften unter Briefen das Kürzel "i. A." ("im Auftrag"). Es soll verdeutlichen, dass nicht der Verantwortliche selbst unterschrieben hat, sondern ein von ihm per Vollmacht Beauftragter (in Vollmacht: bei Behörden für „in Vertretung“).

Damit Dritte einen rechtsgeschäftlichen Vertreter als Handlungsbevollmächtigten einer Firma erkennen können, unterzeichnet er mit dem Zusatz „in Vollmacht“, „im Auftrag“ oder „i. V.“/„i. A.“ ( HGB). Nach HGB hat auch der Prokurist seinem Namen einen die Prokura andeutenden Zusatz beizufügen. Diese Zusätze bewirken jedoch als reine Ordnungsvorschrift nicht die Unwirksamkeit derjenigen Rechtsgeschäfte, die ohne diesen Zusatz eingegangen wurden. 

Unterschriftsproben oder -verzeichnisse mit Unterschriftsmustern von Unterzeichnungsberechtigten werden zwischen Unternehmen ausgetauscht, wenn eine dauerhafte Geschäftsverbindung besteht, bei der die Vertragspartner von häufig wechselnden Unterzeichnern ausgehen müssen (Bankvollmachten im Bankwesen).

Legitimationsprüfungen sollen den Vergleich einer geleisteten Unterschrift mit der Unterschrift auf einem amtlichen Legitimationspapier (Personalausweis oder Reisepass) ermöglichen und zielen darauf ab, die Übereinstimmung der Unterschriften nachzuweisen, um die Rechtsverbindlichkeit eines Schriftstücks festzustellen. Insbesondere im Bankwesen sind derartige Legitimationsprüfungen gesetzlich vorgeschrieben, und zwar aus steuerlicher ( AO) und aus Geldwäschesicht ( Abs. 3 i. V. m. GwG). Das Geldwäschegesetz versteht unter „Identifizieren“ das Feststellen des Namens auf Grund eines Personalausweises oder Reisepasses sowie des Geburtsdatums, der Anschrift und das Feststellen von Art, Nummer und ausstellender Behörde des amtlichen Ausweises.
Legitimationsprüfungen sollen die Echtheit der Unterschriften durch optischen Vergleich sicherstellen und den Nachweis für etwaige Vertretungsberechtigungen bei Firmen führen. Auch Notare müssen bei Unterschriftsbeglaubigungen prüfen, ob die im Beglaubigungsvermerk namentlich aufgeführte Person und der Erklärende identisch sind. Die Beglaubigung bezieht sich nur auf die Echtheit der Unterschrift und die Prüfung einer etwaigen Vertretungsberechtigung. Diese wird gemäß BNotO nach notarieller Einsichtnahme in ein Register in Form einer Bescheinigung über eine Vertretungsberechtigung erteilt. Nach Abs. 1 Satz 1 GBO kann bei im Handels-, Genossenschafts-, Partnerschafts- oder Vereinsregister eingetragenen Vertretungsberechtigungen sowie das Bestehen juristischer Personen und Gesellschaften durch eine Bescheinigung nach Abs. 1 BNotO bestätigt werden. Nach Satz 3 dieser Vorschrift kann der Nachweis auch durch einen amtlichen Registerausdruck oder eine beglaubigte Registerabschrift erbracht werden. 

Ein schwieriges Thema sind die von Unterschriftsproben abweichenden Unterschriften. Es ist anerkannt, dass es im Zeitablauf zu Veränderungen des Unterschriftenbildes kommen kann. Bei nur geringfügigen Abweichungen ist dies eher unproblematisch; bei größeren Abweichungen von der vorhandenen Unterschriftsprobe kann jedoch die Gefahr der Unterschriftsfälschung bestehen, die durch eine Legitimationsprüfung gerade entdeckt werden soll. Kreditkartenunternehmen regeln hierzu in ihren AGB, dass die Unterschrift bei Kartennutzung der Unterschrift auf der Karte zu entsprechen hat und eine abweichende Unterschrift nicht die Haftung des Karteninhabers für die Erfüllung seiner mit der Karte eingegangen Verpflichtungen ändert. Es liegt im Ermessen des die Unterschrift Prüfenden, ob er eine Unterschrift als mit der Unterschrift auf dem Ausweisdokument übereinstimmend anerkennt oder nicht. Um nicht in Schwierigkeiten zu geraten, muss sich jeder im eigenen Interesse über die gesamte Laufzeit des Ausweisdokuments an die einmal geleistete Musterunterschrift halten. Unterschriften auf Ausweisdokumenten wie Personalausweis oder Reisepass gelten jedoch nicht als Original-Unterschrift.

Im außergerichtlichen Bereich genügt ein Fax nicht dem Schriftformerfordernis des BGB. Das Schriftformerfordernis bei Übermittlung wird bei einer Willenserklärung durch Telefax nicht gewahrt, weil es – obwohl ein Original existiert – am formgerechten Zugang der Willenserklärung fehlt. Bei durch Telex oder Fax übermittelten Bürgschaften ist die Schriftform deshalb nicht gewahrt. Auch die durch das Signaturgesetz eingeführte „digitale Signatur“ erfüllt nicht das Erfordernis der „eigenhändigen Unterschrift“. 

Allerdings genügt ein Fax den Anforderungen des Gerichts an einen bestimmten Schriftsatz ( Nr. 6 ZPO). Das gilt aber nur dann, wenn ein unterschriebenes Original gefaxt wurde. Ein vom Prozessbevollmächtigten eigenhändig unterschriebener Berufungsschriftsatz ist auch dann formwirksam, wenn er nicht auf „normalem“ Weg gefaxt, sondern direkt als Computerfax mit eingescannter Unterschrift elektronisch an das Berufungsgericht übermittelt wird. Dies stellt eine lediglich äußerliche (technische, nicht aber inhaltliche) Veränderung des von dem Prozessbevollmächtigten durch seine eigenhändige Unterschrift autorisierten bestimmenden Schriftsatzes dar. Der Zweck der Schriftform, die Rechtssicherheit und insbesondere die Verlässlichkeit der Eingabe zu gewährleisten, könne auch im Falle einer derartigen elektronischen Übermittlung gewahrt werden. Maßgeblich für die Beurteilung der Wirksamkeit des elektronisch übermittelten Schriftsatzes sei allein die auf Veranlassung des Prozessbevollmächtigten am Empfangsort (Gericht) erstellte körperliche Urkunde.

Bei Behördenschreiben mit Regelungscharakter (Verwaltungsakten) ist eine Unterschrift entbehrlich (vgl. z. B. Abs. 5 Verwaltungsverfahrensgesetz (VwVfG) des Bundes, entsprechende Regelungen enthalten auch die Verwaltungsverfahrensgesetze der Länder); selbst dann, wenn für Behördenhandeln ausnahmsweise Schriftform vorgeschrieben ist, bedarf es keiner Unterschrift, sondern lediglich der Angabe des Namens des Behördenleiters oder eines seiner Mitarbeiter. Wenn die dem Betroffenen zugestellte Ausfertigung den Namen des Unterzeichners enthält, liegt damit ein ordnungsgemäßer schriftlicher Verwaltungsakt vor. Verwaltungsakte können schriftlich, elektronisch, mündlich oder in anderer Weise erlassen werden (§ 37 Abs. 2 S. 1 Verwaltungsverfahrensgesetz). 

Unterschriften sind nicht urheberrechtlich geschützt (siehe Rechtsschutz von Schriftzeichen).

In Österreich wurden die Anforderungen an eine Unterschrift (zumindest im Behördenverkehr) 1979 vom Verwaltungsgerichtshof festgelegt: 

Auszug aus dem Obligationenrecht, OR:

In einem Urteil des Bundesgerichtes vom August 2015 stellte das Bundesgericht klar, dass ein eingescanntes Dokument mit Unterschrift nicht als rechtsgültiger Nachweis dient, sondern im Original vorliegen muss. 
Das Bundesgericht hält fest:

Somit bleibt für Dokumente mit potenziell hohem Streitwert bei der heutigen Rechtslage trotz Trend zum papierlosen Büro nur die Möglichkeit die Originaldokumente aufzubewahren.

Im Bereich der Politik werden Unterschriften bei Unterschriftenaktionen im Sinne einer Meinungsäußerung gesammelt, um einer politischen Forderung Nachdruck zu geben. Die "Unterschriftenlisten", welche die Namen, Anschriften und Unterschriften möglichst vieler Bürger und Bürgerinnen beinhalten, werden dann öffentlichkeitswirksam politischen Entscheidungsträgern übergeben. Während solche Unterschriftenlisten in Deutschland rechtlich unverbindlich sind, wird bei dem im österreichischen Staatsrecht vorgesehenen Volksbegehren eine Unterschrift geleistet.

Neben der archivarischen Auseinandersetzung mit "Autographen" ist das Sammeln von "Autogrammen" mehr oder minder berühmter Persönlichkeiten ein weitverbreitetes Hobby.

In Verträgen, Formularen oder Briefen ist teilweise die vorangestellte Ortsangabe üblich, sie ist rechtlich jedoch nur noch für Wechsel und Schecks notwendig.




</doc>
<doc id="11111" url="https://de.wikipedia.org/wiki?curid=11111" title="Elektronische Signatur">
Elektronische Signatur

Unter einer elektronischen Signatur versteht man mit elektronischen Informationen verknüpfte Daten, mit denen man den Unterzeichner bzw. Signaturersteller identifizieren und die Integrität der signierten elektronischen Informationen prüfen kann. In der Regel handelt es sich bei den elektronischen Informationen um elektronische Dokumente. Die elektronische Signatur erfüllt somit technisch gesehen den gleichen Zweck wie eine eigenhändige Unterschrift auf Papierdokumenten. Sie ist eine Umsetzung des elektronischen Identitätsnachweises (eID).

Für bestimmte Bereiche stellen die nationalen Gesetzgeber zusätzliche Anforderungen an elektronische Signaturen. In manchen Ländern, wie Finnland, Estland oder Österreich ist die elektronische Signatur dann der handschriftlichen Unterschrift rechtlich völlig gleichgestellt, auch die mobile Signatur (Mobile-ID) mit dem Mobiltelefon.

Genutzt wird sie vorerst primär im E-Government (öffentliche Verwaltung), E-Justice (Justiz), zunehmend aber auch E-Commerce (Onlinehandel) und ähnlichen Vertragsunterzeichnungen in der Privatwirtschaft.

Im Allgemeinen werden die Begriffe „digitale Signatur“ und „elektronische Signatur“ synonym verwendet. Im Speziellen ist aber die "digitale Signatur" eine Klasse von kryptografischen (d. h. mathematischen) Verfahren, während "elektronische Signatur" ein primär rechtlicher Begriff ist. Der Terminus „elektronische Signatur“ wurde zuerst von der Europäischen Kommission in einem überarbeiteten Entwurf der EU-Richtlinie 1999/93/EG verwendet, um die rechtlichen Regelungen nicht an eine bestimmte Technologie zu koppeln; in einem früheren Entwurf war noch der Begriff „digitale Signatur“ verwendet worden. Die Richtlinie und die darauf basierenden nationalen Signaturgesetze der Mitgliedstaaten fassten den Begriff bewusst sehr weit: Diese Definition umfasst neben digitalen Signaturen auch andere, nicht auf kryptographischen Methoden, insbesondere nicht auf digitalen Zertifikaten basierende Verfahren. Außerdem bezieht sich der Begriff der digitalen Signatur in der Softwaretechnik auf Identifikationen aller Art, etwa für einzelne Dokumente, während der rechtliche Begriff speziell auf „Signatur“ im Sinne einer persönlichen Unterschrift eingeschränkt ist.

Am 28. August 2014 hat die Europäische Kommission im Amtsblatt der EU die EU-Verordnung 910/2014 über elektronische Identifizierung und Vertrauensdienste für elektronische Transaktionen im Binnenmarkt und zur Aufhebung der Richtlinie 1999/93/EG (EIDAS bzw. IVT) veröffentlicht. Die Verordnung ersetzt die Signaturrichtlinie, stärkt und erweitert aber gleichzeitig die bestehenden Rechtsvorschriften, die mit der Signaturrichtlinie bereits eingeführt wurden. Als Übergangsfrist wird die Verordnung erst ab dem 1. Juli 2016 angewendet, am selben Tag wird auch die Signaturrichtlinie 1999/93/EG aufgehoben.

Ausgangspunkt für die aktuelle Signaturgesetzgebung in der Europäischen Union ist die EU-Richtlinie 1999/93/EG "(Signaturrichtlinie)". Diese definiert die Vorgaben für die Regelungen elektronischer Signaturen, die durch die Mitgliedstaaten und die anderen Staaten des Europäischen Wirtschaftsraumes in nationalen Gesetzen umgesetzt wurden.

Die Signaturrichtline definiert die elektronische Signatur technologieneutral als Daten, die anderen Daten „beigefügt oder logisch mit ihnen verknüpft sind und die zur Authentifizierung dienen“. Jeder einem elektronischen Dokument oder einer Nachricht angehängte Name des Urhebers bzw. Absenders erfüllt diese Definition. Einen höheren Beweiswert besitzen dagegen "fortgeschrittene elektronische Signaturen", die es ermöglichen, die Authentizität und Unverfälschtheit der durch sie signierten Daten zu prüfen. Derzeit erfüllen nur auf digitalen Signaturen basierende elektronische Signaturen diese Anforderungen. Schließlich behandelt die Richtlinie fortgeschrittene elektronische Signaturen, die auf einem qualifizierten Zertifikat beruhen und mit einer sicheren Signaturerstellungseinheit (SSEE) erstellt wurden. Die Richtlinie definiert für diese Art von Signaturen zwar keinen Begriff, geht aber in wesentlichen Punkten speziell auf sie ein; inzwischen hat sich europaweit fast überall der Begriff "qualifizierte elektronische Signatur" durchgesetzt.

Die Richtlinie legt Anforderungen an die Ausstellung von "Zertifikaten" und an andere "Zertifizierungsdienste" fest. Nach Artikel 2 Nr. 9 ist ein Zertifikat „eine elektronische Bescheinigung, mit der Signaturprüfdaten einer Person zugeordnet werden und die Identität dieser Person bestätigt wird“. Zertifizierungsdienste umfassen nach Artikel 2 Nr. 11 auch „anderweitige Dienste im Zusammenhang mit elektronischen Signaturen“, also z. B. Auskunftsdienste für Zertifikate, Identifizierungs- und Registrierungsdienste für die Ausstellung von Zertifikaten oder Zeitstempeldienste. Besondere Anforderungen stellt die Signaturrichtlinie an ein qualifiziertes Zertifikat. Zum einen muss es den Aussteller, den Schlüsselinhaber und den Geltungsbereich des Zertifikates spezifizieren und die fortgeschrittene elektronische Signatur des Ausstellers tragen; zum anderen muss der Aussteller umfangreiche und weitgehende Anforderungen bezüglich der Sicherheit und Nachvollziehbarkeit der Ausgabe der Zertifikate erfüllen.

Die wichtigsten Regelungen der Signaturrichtlinie waren, dass
Als Pendant zur qualifizierten elektronischen Signatur (QES) werden mit der neuen EU-Verordnung qualifizierte elektronische Siegel eingeführt, um juristischen Personen die Möglichkeit zu geben, den Ursprung und die Unversehrtheit von elektronischen Dokumenten rechtsverbindlich zu garantieren. Sie können auch verwendet werden, um digitale Besitzgegenstände einer juristischen Person wie beispielsweise Softwarecode zu kennzeichnen. Mit Inkrafttreten der Verordnung werden ab dem 1. Juli 2016 qualifizierte elektronische Signaturen und Siegel grenzüberschreitend in Europa anerkannt.

In Deutschland erfüllen nur qualifizierte elektronische Signaturen gemäß Nr. 3 Signaturgesetz (SigG) die Anforderungen an die elektronische Form gemäß BGB, die die gesetzlich vorgeschriebene Schriftform ersetzen kann. Auch erhalten nur mit einer qualifizierten elektronischen Signatur versehene elektronische Dokumente den gleichen Beweiswert wie (Papier-)Urkunden im Sinne der Zivilprozessordnung (§ 371a Abs. 1 "ZPO").

In Fällen, in denen eine qualifizierte elektronische Signatur nicht gesetzlich vorgeschrieben ist, können Dokumente, die „nur“ mit einer fortgeschrittenen elektronischen Signatur gemäß Nr. 2 SigG versehen wurden, jedoch per Augenscheinsbeweis ebenfalls als Beweismittel vor Gericht verwendet werden.

Die elektronische Signatur ist durch mehrere Rechtsvorschriften geregelt:

Die EIDAS Verordnung definiert in Art. 3 Nr. 10–12 folgende Formen von elektronischen Signaturen:


Die verschiedenen Formen der elektronischen Signaturen stehen für unterschiedliche Anforderungen an die Signaturen. An qualifizierte Signaturen werden die höchsten Anforderungen hinsichtlich Erstellung von Signaturschlüsseln zur Signaturerstellung und Signaturprüfschlüsseln sowie Zertifikaten gestellt. Außerdem müssen die bei der Signaturerstellung eingesetzten Anwendungskomponenten ebenfalls bestimmten Anforderungen entsprechen.

An eine einfache (d. h. nicht fortgeschrittene) elektronische Signatur werden keine besonderen Anforderungen gestellt. So gilt z. B. auch die Angabe des Urhebers oder Absenders ohne digitale Signatur als „einfache“ Signatur. In einem Zivilprozess unterliegen Dokumente bzw. Dateien mit einfachen elektronischen Signaturen der Beweiswürdigung durch das Gericht, das in seiner Bewertung frei ist. Im Rechtsstreit kommt es also darauf an, ob ein Signaturverfahren eingesetzt wurde, das vom Gericht als beweiswürdig eingestuft wird, was gegebenenfalls durch Gutachter festgestellt wird. Einfache elektronische Signaturen können gemäß BGB für formfreie Vereinbarungen eingesetzt werden.

Für eine "fortgeschrittene elektronischen Signatur" übernimmt Nr. 2 SigG im Wesentlichen die Definition der Richtlinie: Eine fortgeschrittene Signatur muss mit einem einmaligen – praktisch also geheimen – Signaturschlüssel, der dem Signaturersteller während der Signaturerstellung zur Verfügung stehen muss, und mit Mitteln, die unter seiner alleinigen Kontrolle stehen, erstellt worden sein. Zusätzlich muss der Signaturersteller bei Bedarf identifizierbar sein. Dies erfolgt entweder über den dem Signaturersteller zugewiesenen Prüfschlüssel oder gegebenenfalls mittels während der Signaturerstellung erfasster biometrischer Unterschriften.

Der Begriff „Signaturschlüssel“ bezieht sich außerdem nicht notwendigerweise nur auf kryptographische Schlüssel, und für die Identifizierbarkeit des Signaturerstellers ist nicht zwingend ein Zertifikat erforderlich, so dass z. B. auch mit PGP und einem auf der Festplatte gespeicherten Signaturschlüssel (Soft-PSE) fortgeschrittene elektronische Signaturen erstellt werden können.

Im Rechtsstreit werden fortgeschrittene elektronische Signaturen genauso wie „einfache“ elektronische Signaturen als Objekte des Augenscheins behandelt, d. h., die sich auf die Signatur beziehende Partei muss beweisen, dass digitale Signatur und Identifizierungsmerkmal echt sind. Fortgeschrittene elektronische Signaturen können gemäß § 127 BGB für formfreie Vereinbarungen eingesetzt werden.

Nur Dokumente mit einer "qualifizierten elektronischen Signatur" gemäß Nr. 3 SigG können als elektronische Form eine per Gesetz geforderte Schriftform auf Papier ersetzen, vgl. BGB. In Übereinstimmung mit der europäischen Richtlinie ist eine qualifizierte elektronische Signatur eine fortgeschrittene elektronische Signatur, die auf einem zum Zeitpunkt ihrer Erzeugung gültigen qualifizierten Zertifikat beruht und mit einer sicheren Signaturerstellungseinheit (SSEE) erstellt wurde. Der Signaturschlüssel darf dabei ausschließlich in der SSEE gespeichert und angewendet werden, und die Übereinstimmung der SSEE mit den Vorgaben des Signaturgesetzes muss durch eine anerkannte Stelle geprüft und bestätigt werden. Dagegen ist auch für qualifizierte elektronische Signaturen eine Prüfung und Bestätigung der Signaturanwendungskomponente, welche Signatursoftware, Treiber und Chipkartenleser umfasst, nicht zwingend vorgeschrieben, jedoch ist mindestens eine Herstellererklärung nötig, in der der jeweilige Hersteller die Konformität der Komponente zum SigG und zur SigV gemäß SigG bestätigt. Eine solche Herstellererklärung wird später von der Bundesnetzagentur im Bundesanzeiger veröffentlicht, ist aber bereits mit der Einreichung bei der Bundesnetzagentur genügend.

Das bürgerliche Gesetzbuch erlaubt den Ersatz der per Gesetz vorgeschriebenen – also nicht freiwilligen – Schriftform (max. 5 % aller unterzeichneten Vereinbarungen bzw. Erklärungen) durch die elektronische Form, soweit durch Gesetz nichts anderes bestimmt ist ( BGB). Die elektronische Form ist gewahrt, wenn dem elektronischen Dokument der Name des Unterzeichners/Signierenden hinzugefügt und mit einer qualifizierten elektronischen Signatur versehen wird ( BGB).

Für formfreie Vereinbarungen, die nicht per Gesetz der Schriftform benötigen, jedoch aus Beweisgründen freiwillig schriftlich verfasst und unterzeichnet bzw. signiert werden, können die Vertragspartner für elektronische Dokumente eine andere Signaturform vereinbaren, also entweder eine „einfache“ oder eine fortgeschrittene elektronische Signatur wählen ( BGB).

Die für qualifizierte elektronische Signaturen zugelassenen Kryptoalgorithmen werden von der Bundesnetzagentur genehmigt und veröffentlicht. Dort sind auch die für eine qualifizierte elektronische Signatur zugelassenen Produkte aufgelistet.

Zertifizierungsdienste sind genehmigungsfrei, aber anzeigepflichtig. Bei der Anzeige ist darzulegen, dass und wie die gesetzlichen Anforderungen (finanzielle Deckungsvorsorge, Zuverlässigkeit, Fachkunde) erfüllt sind.

Österreich war das erste Land, das die Richtlinie 1999/93/EG des Europäischen Parlaments und des Rates über gemeinschaftliche Rahmenbedingungen für elektronische Signaturen umsetzte.

Die Grundlage für die Anerkennung elektronischer Signaturen im österreichischen Recht bildet das Signaturgesetz. Dieses unterschied bis 2008 zwischen der (einfachen) elektronischen Signatur und der "sicheren elektronischen Signatur", welche im Wesentlichen der qualifizierten elektronischen Signatur in Deutschland entsprach. Zum 1. Januar 2008 trat eine Novelle des Signaturgesetzes in Kraft. Nunmehr existieren auch in Österreich neben der einfachen eine fortgeschrittene und eine qualifizierte elektronische Signatur. Die einfache elektronische Signatur ist in § 2 Nr. 1 ÖSiG definiert und unterscheidet sich nicht von der deutschen Regelung. Die fortgeschrittene elektronische Signatur findet sich in § 2 Nr. 3 ÖSiG und muss über die Anforderungen der einfachen elektronischen Signatur hinaus zusätzliche Voraussetzungen erfüllen. Sie muss ausschließlich dem Signator (Unterzeichner) zugeordnet werden können, die Identifikation des Signators ermöglichen, mit Mitteln erstellt werden, die unter der alleinigen Kontrolle des Signators stehen und mit den Daten, auf die sie sich bezieht, so verknüpft sein, dass jede nachträgliche Änderung der Daten festgestellt werden kann. Der neu hinzugefügte § 2 Nr. 3a ÖSiG nennt nun die qualifizierte elektronische Signatur und passt den zuvor verwendeten Begriff der sicheren elektronischen Signatur dem in der Signaturrichtlinie verwendeten Terminus technicus an. Inhaltlich entspricht sie aber noch der sicheren Signatur.

Das "Bundesgesetz über Regelungen zur Erleichterung des elektronischen Verkehrs mit öffentlichen Stellen" (E-Government-Gesetz) ermöglicht die Nutzung einer "Bürgerkarte" mit sicherer elektronischer Signatur für die Teilnahme an elektronischen Verwaltungsverfahren. Als Übergangslösung konnte gemäß § 25 bis zum 31 Dezember 2007 alternativ eine "Verwaltungssignatur" verwendet werden, deren spezifische Anforderungen in der Verwaltungssignaturverordnung geregelt sind. Diese Übergangslösung wird nicht verlängert, so dass seit 1. Januar 2008 zwingend eine sichere bzw. qualifizierte elektronische Signatur im E-Government vorgeschrieben ist.

Die "Bürgerkarte" ist ein allgemeines technologisches System, im Speziellen die Freischaltung von SmartCards (wie die Sozialversicherungskarte e-Card oder Bankomatkarten) zur qualifizierten elektronischen Signierung. Diese Funktion nutzen per 2014 etwa 150.000 Österreicher. Mit der Variante der "Handy-Signatur" gibt es seit 2007 auch eine staatlich kontrollierte mobile Signatur (Mobile-ID). Sie wird 2014 schon von etwa 300.000 Bürgern verwendet. Bei der Handy-Signatur gibt es große Bedenken wegen der Sicherheit.

Die elektronische Signatur ist durch das Bundesgesetz über Zertifizierungsdienste im Bereich der elektronischen Signatur (ZertES) sowie durch die Verordnung über Zertifizierungsdienste im Bereich
der elektronischen Signatur (VZertES) geregelt. Das Obligationenrecht (OR) sieht in Art. 14 Abs. 2 bis bzw. Art. 59a eine Gleichstellung von ZertES-konformer elektronischer Signatur und Handunterschrift im Bereich gesetzlicher Formvorschriften sowie eine Haftung des Inhabers des Signierschlüssels für den sorgfältigen Umgang mit dem Schlüssel vor. ZertES, VZertES und die entsprechende OR-Novelle sind am 1. Januar 2005 in Kraft getreten.

Ein wesentlicher Unterschied zur Regelung in der EU-Signaturrichtlinie liegt darin, dass für eine Rechtswirkung der erwähnten obligationenrechtlichen Normen jeweils die "Anerkennung" des jeweiligen Zertifizierungsdienstes durch eine "Anerkennungsstelle" vorausgesetzt wird. Diese Anerkennungsstelle ist durch die Schweizerische Akkreditierungsstelle akkreditiert. Es braucht also in der Schweiz die gesetzeskonforme elektronische Signatur eines "anerkannten" Zertifizierungsdienstes, während in der EU nur eine gesetzeskonforme Signatur vorausgesetzt wird und die Akkreditierung damit freiwillig bleibt. Die Anerkennung ist eine Bestätigung dafür, dass der Zertifizierungsdienst die Anforderungen des Gesetzes erfüllt.

Die Schweizerische Akkreditierungsstelle (SAS) publiziert eine "Liste der anerkannten Zertifizierungsdienste". Derzeit sind Swisscom (Schweiz), QuoVadis Trustlink Schweiz, die SwissSign AG der Schweizerischen Post und das Bundesamt für Informatik und Telekommunikation (BIT) anerkannte Anbieter von Zertifizierungsdiensten (Stand: 2016).

Aufgrund der weiten und technologie-neutralen Definition lassen sich elektronische Signaturen durch völlig verschiedene technische Verfahren umsetzen. So stellt auch die Angabe des Absenders in einer E-Mail bereits eine elektronische Signatur dar. Auch ein über das Internet geschlossener Vertrag enthält eine elektronische Signatur, sofern geeignete Verfahren, etwa eine Passwortabfrage, den Vertragsabschluss durch eine bestimmte Person hinreichend belegen.

Fortgeschrittene oder gar qualifizierte elektronische Signaturen, die eine zuverlässige Identifizierung des Unterzeichners ermöglichen und eine nachträgliche Veränderung der Daten erkennen lassen müssen, können technisch mit digitalen Signaturen in Verbindung mit digitalen Zertifikaten von einer Public-Key-Infrastruktur (PKI) realisiert werden. Bei diesen Verfahren wird ein Schlüsselpaar verwendet. Ein Schlüssel wird für die Erzeugung der Signatur verwendet (Signaturschlüssel) und ein Schlüssel für die Prüfung (Signaturprüfschlüssel). Bei qualifizierten Signaturen ist die Zuordnung der asymmetrischen Schlüsselpaare gemäß deutschem Signaturgesetz zwingend erforderlich.

Bei fortgeschrittenen Signaturen ist die Identifizierung des Unterzeichners nicht an ein Zertifikat gebunden. So können neben Zertifikaten auch andere Identifizierungsmerkmale, z. B. während des Signaturerstellungsprozesses erfasste eigenhändige Unterschriften, eingesetzt werden.

Ablauf einer elektronischen Signierung mit einer digitalen Signatur:

Aufgrund neuer oder verbesserter Methoden der Kryptoanalyse und immer leistungsfähigerer Rechner nimmt die Effizienz von Angriffen auf digitale Signaturverfahren wie z. B. RSA im Laufe der Zeit zu. Daher ist die Sicherheit – und damit die Aussagekraft – einer digitalen Signatur zeitlich begrenzt.

Aus diesem Grund sind die heute ausgestellten Zertifikate in der Regel nicht länger als drei Jahre gültig, was bedeutet, dass der zugewiesene Signaturschlüssel nach Ablauf des Zertifikats nicht mehr benutzt werden darf (manche Signiersoftware verweigert das Setzen einer Signatur mit einem ungültigen Zertifikat). Das Alter elektronischer Daten ist jedoch praktisch nicht bestimmbar. Dokumente könnten folglich ohne weiteres um Jahre oder gar Jahrzehnte rückdatiert werden, ohne dass dies nachweisbar wäre. Eine Rückdatierung kann etwa durch Verstellen der Systemzeit des verwendeten Rechners erfolgen. Gelingt es einem Fälscher nach Jahren, den Signaturschlüssel aus dem öffentlichen Zertifikat zu berechnen, kann er damit ein rückdatiertes Dokument mit einer gefälschten qualifizierten elektronischen Signatur versehen.

In Deutschland müssen die Anbieter die Nachprüfbarkeit der Zertifikate für fünf Jahre – akkreditierte Anbieter für 30 Jahre – nach Ende des Gültigkeitszeitraums ermöglichen, indem sie ein öffentliches Zertifikatsverzeichnis bereitstellen ( SigV). Danach kann die Nachprüfung eines Zertifikats unmöglich werden.

Auch wenn ein Zertifikat bereits lange ungültig ist bzw. der damit verknüpfte Signaturschlüssel nicht mehr verwendet werden darf, sind Dokumente, die innerhalb des Gültigkeitszeitraums signiert wurden, nach wie vor rechtsgültig.

Die Problematik besteht in der Beweiseignung elektronischer Signaturen nach dem Ablauf des Zertifikats. In der Literatur wird die Meinung vertreten, dass der Anscheinsbeweis (eine Beweislastumkehr) für die Echtheit einer elektronischen Signatur mit Anbieterakkreditierung nicht die Tatsache betreffen kann, dass die Signatur vor dem Ablauf des Zertifikats erstellt wurde, weil der Nachweis des Signierzeitpunktes für denjenigen, der sich auf die Signatur stützt, leicht möglich ist und daher keiner Beweiserleichterung bedarf. Mit dem Ablauf des Zertifikats muss daher derjenige, der sich auf eine Signatur stützt, "voll beweisen", dass die Signatur vor diesem Zeitpunkt gesetzt wurde. Dies kann durch eine Nachsignierung oder durch einen Zeitstempel geschehen.

Im Fall archivierter, signierter Dokumente kann eine Signierung des Archivs selbst oder von Teilen davon die darin enthaltenen Dokumente absichern.

Für den Fall elektronischer Rechnungen und anderer Unternehmensdokumente gilt gemäß den Grundsätzen ordnungsgemäßer Buchführung die Verpflichtung, Rechnungen für 10 Jahre revisionssicher zu archivieren. Wenn diese Bedingung durch ein entsprechendes elektronisches Archiv sichergestellt ist, ist eine erneute Signierung der einzelnen Dokumente nicht notwendig, da das revisionssichere Archiv die Unveränderbarkeit der im Archiv gehaltenen Dokumente garantiert.

Eine Fälschung der Signatur kann nur zuverlässig ausgeschlossen werden, wenn geeignete Software zur Erstellung und zur Prüfung der Signatur verwendet wird. Die Schwierigkeit dabei ist, dass kaum feststellbar ist, ob diese Voraussetzung tatsächlich erfüllt ist. Allein der Signatur kann nicht angesehen werden, ob sie tatsächlich mit sicheren technischen Komponenten erstellt wurde. Das deutsche Signaturgesetz definiert daher in auch noch Anforderungen an Produkte für qualifizierte elektronische Signaturen.

Generell ist zur Prüfung der Signatur eine Software erforderlich. Die Software auf einem PC kann praktisch immer auch so genannte Malware enthalten. Eine tatsächlich zuverlässige Prüfung, ob die Software tatsächlich den Spezifikationen entspricht und nicht manipuliert wurde, ist sehr aufwändig. Hier werden normalerweise Sicherheitsmechanismen des Betriebssystems und/oder Signaturen an der Software verwendet.

Häufig werden die Aspekte der Betrachtung der Sicherheit auf rein mathematisch-technische Aspekte reduziert. Fast alle Pilotprojekte zeigen, dass der Faktor Mensch zu gering gewichtet wird. Noch nicht wirklich absehbar scheint eine bezahlbare und pragmatische Handhabung von verlorenen Signaturkarten oder vergessenen Geheimzahlen. In der Testregion Flensburg wurde der 10.000er-Feldversuch mit der elektronischen Gesundheitskarte (eGK) im März 2008 gestoppt: „Von 25 Ärzten in 17 Praxen, die freiwillig die Testphase bestritten, sperrten 30 Prozent ihren Heilberufsausweis, weil sie sich partout nicht mehr an die 6-stellige Signatur-PIN erinnern konnten. 10 Prozent davon sperrten ihren neuen Arztausweis irreversibel.“

Die Hoffnung von Anbietern von Signaturkarten ruht bereits seit 2002 auf dem ELENA-Verfahren (früher JobCard). Es soll nach den Vorstellungen der Bundesregierung die Nutzung digitaler Signaturen fördern und käme dem Wunsch der Anbieter von Signaturkarten nach, der Staat möge endlich obligatorische Anwendungsfälle schaffen. In diesem Kontext beschäftigen sich die Medien wieder vermehrt mit der digitalen Signatur und den Herausforderungen bei einer Einführung. Mögliche Alternativen bei vergessenen Geheimzahlen oder verlorenen Signaturkarten zeigte eine Reportage des Deutschlandfunks am 28. Juni 2008 auf. Die derzeit beabsichtigte Vorgehensweise hätte entweder eine Aufweichung der Sicherheit und des Datenschutzes zur Folge oder würde ein hochkomplexes und kaum bezahlbares Verfahren erfordern. Erwogen werden entweder Generalschlüssel, mit dem Mitarbeiter der zentralen Speicherstelle auf alle Verdienstbescheinigungen zugreifen könnten, oder ein mehrstufiges Umschlüsselungsverfahren.

In den letzten Jahren bekommen die Signaturkarten und mit ihnen die digitalen Signaturen auf dem Feld elektronischer Signaturen Konkurrenz. Zunehmend häufiger und ausgefeilter werden die Angebote für eine vertrauenswürdige Digitalisierung der eigenhändigen Unterschrift. Das elektronische Unterschreiben am Computer ist nicht mehr allein mit Chipkarte und Geheimzahl zu realisieren. Es hat dort überall seine Einsatzfelder, wo heute die sogenannte „gewillkürte Schriftform“ verwendet wird. Darunter verstehen Juristen die gegenseitige Festlegung auf ein Papierdokument mit eigenhändiger Unterschrift als Beweismittel. Mittlerweile sind selbst Kreditinstitute dazu übergegangen, bei Prozessen wie der Kontoeröffnung während des Signaturprozesses eigenhändige Unterschriften über ein Unterschriftentablett digital zu erfassen und diese biometrischen Daten als Identifikationsmerkmale – und damit als Zertifikatsersatz – in die elektronischen Anträge (z. B. PDF-Formulare) mit der digitalen Signatur verknüpft einzubetten. In Österreich haben Wirtschaftsunternehmen die Möglichkeit, für sichere Online-Verfahren eine Reihe von Open-Source-Modulen der Plattform Digitales:Österreich wie z. B. für den Einsatz der elektronischen Signatur bei vorsteuerabzugsfähigen E-Rechnungen zu nutzen. Weiters bietet das österreichische Bundeskanzleramt einen Prüfservice zur Prüfung elektronisch signierter Dokumente. Bei der österreichischen Handy-Signatur wurden grundsätzlich erhebliche Zweifel an der Sicherheit laut. Insbesondere soll diese anfällig für Phishing-Angriffe sein, weil für Login und Signatur die gleichen Mechanismen genutzt werden.

Trotz der diesbezüglichen Vorgaben der Signaturrichtlinie hat eine elektronische Signatur nicht in allen Ländern die gleiche rechtliche Relevanz. Zwar ist eine qualifizierte Signatur in allen Ländern als rechtlich äquivalent zu einer handgeschriebenen Unterschrift definiert, doch variiert die rechtliche Relevanz einer handgeschriebenen Unterschrift unter den Staaten erheblich. Daher wird ein Benutzer die rechtliche Relevanz einer qualifizierten elektronischen Signatur aus einem anderen Mitgliedstaat nicht einschätzen können, solange er nicht die dortigen Regelungen zur handgeschriebenen Unterschrift kennt.

Ein extremes Beispiel ist Großbritannien, in dem eine handgeschriebene Unterschrift keinen über ein Indiz hinausgehenden Status besitzt; sie stellt lediglich ein Beweismittel dar, dessen Beweiswert von Fall zu Fall zu entscheiden ist. Aus diesem Grund sah die britische Regierung auch keine Notwendigkeit, die Regelung zur Gleichstellung qualifizierter elektronischer Signaturen zu handgeschriebenen Unterschriften in die nationalen Gesetze aufzunehmen. Nicht einmal die Konzepte der sicheren Signaturerstellungseinheit und der qualifizierten elektronischen Signatur wurden in die britische Gesetzgebung aufgenommen.

Unterschiede zeigen sich auch in der Frage, ob qualifizierte Zertifikate und fortgeschrittene elektronische Signaturen nur natürlichen Personen oder auch Organisationen zugeordnet sein können. Da die EG-Richtlinie in diesem Punkt nicht eindeutig ist, wird diese Frage in den einzelnen Mitgliedstaaten unterschiedlich geregelt. Es stellt sich somit die Frage, inwieweit z. B. ein in Belgien für ein Unternehmen ausgestelltes qualifiziertes Zertifikat und die darauf basierenden Signaturen in Deutschland anerkannt werden.

Ein weiteres Problem ist, dass die einzelnen Länder in vielen Bereichen (in Deutschland z. B. in der Sozialgesetzgebung) nur qualifizierte Signaturen zulassen, deren Zertifikate von einem akkreditieren Zertifizierungsdiensteanbieter ausgestellt wurden. Da die Anforderungen und Verfahren für eine Akkreditierung auf nationaler Ebene sehr unterschiedlich geregelt sind, erschwert diese Anforderung nach einer Anbieter-Akkreditierung den Marktzugang für ausländische Zertifizierungsdiensteanbieter.

Der neue deutsche Personalausweis wird seit dem 1. November 2010 im Scheckkartenformat mit Chipkarte ausgestellt und beinhaltet die gegen eine Gebühr aktivierbare Möglichkeit zur Nutzung als Signatur-Erstellungseinheit für qualifizierte elektronische Signaturen. Die Bundesregierung erhofft sich damit eine Verbreitung der elektronischen Signatur.

Im Rahmen der deutschen Nachweisverordnung ist seit dem 1. April 2010 zwingend, dass die Abfallentsorger jeden Transport gefährlicher Abfälle elektronisch qualifiziert signieren (Elektronisches Abfallnachweisverfahren, eANV). Spätestens ab dem 1. Februar 2011 trifft diese Regelung auch für Abfallerzeuger und Abfallbeförderer zu.




</doc>
<doc id="11112" url="https://de.wikipedia.org/wiki?curid=11112" title="Wald (Begriffsklärung)">
Wald (Begriffsklärung)

Wald steht für:

Wald ist der Name folgender Orte: 

Gemeinden:

Ortsteile in Deutschland:



Gemeindeteile in Österreich

Gemeindeteile in der Schweiz

sowie:
Wald ist der Familienname folgender Personen:


Siehe auch:


</doc>
<doc id="11115" url="https://de.wikipedia.org/wiki?curid=11115" title="Irrtumswahrscheinlichkeit">
Irrtumswahrscheinlichkeit

Die Irrtumswahrscheinlichkeit kann sein:
Die Irrtumswahrscheinlichkeit entspricht "nicht" dem berechneten p-Wert bei der Durchführung eines Tests.


</doc>
<doc id="11117" url="https://de.wikipedia.org/wiki?curid=11117" title="Konzern">
Konzern

Als Konzern (von lat. "concernere" „(ver-)mischen“) bezeichnet man den Zusammenschluss eines herrschenden und eines oder mehrerer abhängiger Unternehmen zu einer wirtschaftlichen Einheit unter der Leitung des herrschenden Unternehmens, wobei jedes Unternehmen einen eigenen Jahresabschluss erstellt. Dafür geben die einzelnen Unternehmen ihre wirtschaftliche und finanzielle Unabhängigkeit auf, rechtlich bleiben die Unternehmen selbständig. Die dabei verbundenen Unternehmen nennt man "Konzernunternehmen". Der Konzern wird von der Kooperation abgegrenzt, der es regelmäßig an einer einheitlichen Leitung fehlt.

In der Betriebswirtschaftslehre und dem Handelsrecht versteht man unter Konzern eine unter der einheitlichen Leitung eines herrschenden Unternehmens zusammengefasste Unternehmensgruppe. Unter einheitlicher Leitung im Sinne des Konzernrechts stehen Unternehmen, zwischen denen ein Beherrschungsvertrag besteht ( AktG) oder von denen das eine in das andere eingegliedert ist ( AktG). Ein Konzern besteht aus einem Mutterunternehmen und einem oder mehreren Tochterunternehmen. Die Tochterunternehmen sind wirtschaftlich und finanziell gegenüber dem Mutterunternehmen unselbständig, rechtlich aber selbständig und erstellen eigene Bilanzen und Gewinn- und Verlustrechnungen, die dann in der Konzernbilanz und Konzernerfolgsrechnung zusammengeführt (konsolidiert) werden. Je mehrgliedriger ein Konzern ist, umso häufiger werden Konzernebenen installiert, die jeweils einen Teilkonzern bilden. Dieser wird oft von einer Tochtergesellschaft der zweiten Ebene angeführt, deren Aufgabe in der Führung mehrerer Tochterunternehmen der dritten Ebene besteht.

Der Begriff "Konzern" ist deutschen Ursprungs und auch in anderen deutschsprachigen Ländern gebräuchlich. Grundsätzlich aber ist er nicht immer begriffsidentisch übersetzbar. In der englischsprachigen Welt ist "concern" = „Firma, Unternehmen“ zwar geläufig, gilt jedoch als deutscher Import und wird nur selektiv verwendet. Gebräuchlich ist dort eher die „corporate group“ oder einfach „group“.

Umgangssprachlich wird "Konzern" – bisweilen mit negativer Konnotation – als Synonym für ein „mächtiges Großunternehmen“ verwendet, selbst wenn dieses nicht rechtlich als Konzern organisiert ist.

Anfang des 19. Jahrhunderts wurden Unternehmensverbindungen in Deutschland erstmals als „Conzern“ bezeichnet. Vorher dominierten in der Wirtschaft Einzelunternehmen. Die vereinzelten frühen ‚Konzerne‘ bezeichnete man in der industrialisierten Welt mit allen möglichen Begriffen wie "trust", combination, combine, syndicat, gemischtes Werk, Fusion. In anderen Sprachgebieten setzte sich der deutsche Neologismus „Konzern“ nicht durch, sodass konkurrierende Formulierungen dominant blieben. In den USA gibt es keinen einheitlichen Konzernbegriff wie in Deutschland, man spricht von „affiliated corporations“, „affiliated groups“, „groups of corporations“, „integrated companies“ oder „corporate groups“. Meist wird der Begriff „corporation“ als wichtigstem Bestandteil für Konzerngebilde auch dann verwendet, wenn es sich um einen Konzern handelt.

Als weltweit erster Konzern gilt die vom italienischen Bankier Cosimo de’ Medici gehaltene Unternehmensgruppe. De’ Medici hielt im Jahre 1458 Mehrheitsbeteiligungen an 13 Unternehmen, die alle seinen und den Namen eines Partners trugen und rechtlich selbständig blieben.
In der Gründerzeit bildeten sich weltweit die ersten Konzerne wegen des enormen Kapitalbedarfs der Wirtschaft. Größere Unternehmen kauften kleinere auf und übten über sie Kontrolle aus – die Grundlage auch für die heutige Konzernbildung. Ab 1870 entstanden in den USA die Trusts, deren Komitee von Treuhändern (der „Board of Trustees“) sich lediglich mit der Verwaltung der Anteile der von ihm beherrschten Unternehmen befasste. Nachdem im Juli 1890 der Sherman Antitrust Act in Kraft trat und alle die Handelsfreiheit beeinträchtigenden Trusts verbot, umging man dieses Gesetz durch die Gründung von "Holding Companies". Berühmteste Gründung war im Oktober 1889 die „Standard Oil Company of New Jersey Holding“, der Rechtsnachfolgerin des im Januar 1882 entstandenen „Standard Oil Trust“ – dem ersten bekannten Trust der amerikanischen Wirtschaftsgeschichte. Die „Securities Holding Company“ erwarb gerade so viele Aktien, um die ausschlaggebende Stimmenzahl zur Einflussnahme zu besitzen. Der US-Bundesstaat New Jersey ließ 1888 erstmals eine Holding company zu und ebnete damit den Weg für Konzerne.

Das deutsche Konzernrecht begann mit der Unternehmenskonzentration, die im Jahre 1864 einsetzte. Als sich 1864 das Kapital der Essener Alfred-Krupp-Gussstahlfabrik auf andere Unternehmen ausdehnte, kam es zur ersten deutschen Konzernbildung, und Krupp avancierte zum „Kanonenkönig“. Weitere Konzerne in Deutschland entstanden als „Syndicatsgesellschaften“ in der Montan- und Kaliindustrie. Hier blieben die Unternehmen wirtschaftlich selbständig, nur in Erzeugung und Absatz unterwarfen sie sich Beschränkungen. Weitere Konzerne schufen Hugo Stinnes, der unter der 1892 gegründeten Hugo Stinnes GmbH einen unüberschaubaren Mischkonzern zusammenkaufte, oder die AEG, die 1884 mit der "Städtischen Elektrizitätswerke Berlin AG" ihre Konzerntätigkeit aufnahm, und Siemens & Halske. Friedrich Flick schlug als Direktor der „Actien-Gesellschaft Charlottenhütte“ im September 1915 den Zusammenschluss seiner Firma mit den „Cöln-Müsener Bergwerks-Aktienverein“ vor.

Einen wesentlichen Impuls zur Konzernbildung brachte die im neuen KStG vom März 1920 verschärfte Besteuerung der Fusion, das Schachtelprivileg sowie die im Umsatzsteuerrecht entwickelte Organtheorie. Es kam zu Konzernbildungen insbesondere im Bereich der Metallurgie, Schwerindustrie und chemischen Industrie.

Konzern und Konzernunternehmen sind in Aktiengesetz legaldefiniert, wonach ein herrschendes und ein oder mehrere abhängige Unternehmen unter der einheitlichen Leitung des herrschenden Unternehmens zusammengefasst als Konzern anzusehen sind. Selbst wenn keine Abhängigkeit vorhanden ist, bilden Unternehmen nach § 18 Abs. 2 AktG unwiderlegbar einen Konzern, wenn sie einheitlich geleitet werden. Der Begriff der „einheitlichen Leitung“ wird zwar im Aktienrecht häufig verwandt, aber nicht definiert. Für die Zusammenfassung mehrerer rechtlich selbständiger Unternehmen unter „einheitlicher Leitung“ werden in Abs. 1 AktG zwei Vermutungsregelungen aufgestellt:

Das für den Konzern bestimmende Begriffsmerkmal der einheitlichen Leitung wurde aus einer Definition des Reichsfinanzhofs (RFH) vom Januar 1930 in das AktG vom Januar 1937 (§ 15 AktG a.F.) übernommen. Seither ist von einheitlicher Leitung auszugehen, wenn die Leitungstätigkeit in mindestens einem wesentlichen unternehmerischen Entscheidungsbereich (Beschaffung, Produktion, Absatz, Finanzwesen, Personalpolitik) ausgeübt wird. Die Leitung muss weder ausdauernd noch umfassend sein; sie kann sich vielmehr auch in einzelnen Leitungsmaßnahmen erschöpfen. Leitung bedeutet, dass die eigenverantwortliche Tätigkeit des Vorstands der beherrschten Gesellschaft ( AktG) durch eine fremdbestimmte und weisungsgebundene Tätigkeit ersetzt wird ( Abs. 1 AktG). Einheitliche Leitung liegt auch vor, wenn Vorstand oder Geschäftsführung mehrerer Unternehmen in Personalunion besetzt sind.

Der Konzern ist im Schweizer Aktienrecht nur fragmentarisch im Artikel 963 OR (vormals Artikel 663e OR) als Zusammenfassung mehrerer Gesellschaften unter einheitlicher Leitung definiert. Ein umfassendes Konzernrecht wie in Deutschland gibt es jedoch nicht. In der Schweiz gibt es einige Konzerne von Weltrang wie etwa Nestlé oder die Pharma-Multis Novartis und Hoffmann-La Roche.

In Österreich besteht kein gesetzlich kodifiziertes Konzernrecht. Vielmehr beruht es auf der österreichischen Rechtsliteratur und Rechtsprechung. In der Rechtsliteratur wird das österreichische Konzernrecht durch Peter Doralt geprägt, der zu Konzernfragen rechtsdogmatisch und auch rechtspolitisch in zahlreichen Veröffentlichungen Stellung genommen hat.

Eine grundlegende Definition gibt beispielsweise aber GmbH-Gesetz: 

Insbesondere gelten schon zwei Unternehmen, . Weitere zentrale Regelungen finden sich im dritten Abschnitt des Unternehmensgesetzbuches () zum Konzernabschluss. Das Körperschaftsteuergesetz 1988 spricht von ().

Als einen vertikalen Konzern bezeichnet man Konzerne, die die vor- und nachgelagerten Stufen der eigentlichen Wertschöpfung bzw. Leistungserstellung umfassen, d. h. ein breites Spektrum der Leistungserstellung selbst abdecken und nicht von externen Unternehmen beziehen. Beispiel wäre dafür eine Unternehmensgruppe der Montanindustrie, die sowohl Kohle und Eisenerz abbaut als auch Stahl produziert und evtl. auch vermarktet.

Allerdings ist diese Konzernform heutzutage zum Teil überholt – sie erweist sich in der modernen Wirtschaft oft als unökonomisch. Rohstoffe für die Weiterverarbeitung, Zulieferungsteile usw. produziert man heute nicht mehr selbst, sondern kauft sie bei wechselnden Lieferanten zu den jeweils günstigsten Preisen. Beispiel für diese Form der Produktion, auch "Lean Production" genannt, wären Unternehmen der Automobilindustrie wie beispielsweise Daimler oder BMW.

Diese Form ist eine alte, aber immer noch aktuelle gesellschaftsrechtliche Konstruktion. Man versteht darunter Unternehmen, die auf der gleichen Produktions- oder Handelsstufe arbeiten. So produziert der Volkswagen-Konzern vom Kleinwagen bis zur Luxuslimousine alles. Dabei wird versucht, im Allgemeinen eine Monopolstellung in ihrem Markt durch Eingliederung oder Verdrängung von Konkurrenten zu erreichen.

Aldi Nord und Aldi Süd bilden einen horizontalen Konzern.

Der "laterale Konzern", auch Mischkonzern, Konglomerat oder anorganischer Konzern genannt, besteht aus Unternehmen, die in unterschiedlichen Bereichen tätig sind. Die einzelnen Unternehmen haben nur geringe geschäftliche Beziehungen untereinander.

Als multinationale Konzerne werden des Weiteren jene Konzerne bezeichnet, die Standorte in mehreren Staaten besitzen.

Häufig ergeben sich Konzerne als Folge organischen Wachstums, das die Gründung von Tochterunternehmen als zweckmäßig erscheinen lässt. Daneben führen auch gezielte Expansions- und Diversifikationsstrategien zur Bildung von Konzernen bzw. deren Erweiterung (vertikale und/oder horizontale Expansion sowie laterale Expansion oder Diversifikation, Outsourcing). Zudem kann auch das Motiv, Transparenz zu verhindern, zur Verschachtelung von Konzernunternehmen führen.

Das Konzerngebilde wirft zum einen Fragen auf nach der rechtlichen Zuständigkeit und Verantwortung für Finanzrisiken im Konzern, der Thematik von verdeckten Sacheinlagen und Nachgründungen im Konzern oder der Haftung aufgrund von Konzernbesicherungsverhältnissen (siehe Konzernhaftung). Auch das Steuerrecht entwickelt im Konzerngebilde eine eigenständige, über die einzelne Konzerngesellschaft als Steuersubjekt hinausgehende Bedeutung (Konzern-Steuerpolitik, Organschaft). Schließlich gewinnt im Konzern auch die Bilanzierungspolitik als Mittel der Ergebnissteuerung (zum Beispiel phasengleiche Gewinnvereinnahmung, Verrechnungspreisstrategien: Fremdvergleichsgrundsatz „arm’s length“, bilanzielle Behandlung derivativer Firmenwerte) ein besonderes Gewicht. Die Globalisierung der Wirtschaft führt zu neuen Fragestellungen des internationalen Konzernrechts.

Eine Muttergesellschaft beteiligt sich an anderen Unternehmungen und übernimmt die Verwaltung und Führung für die dann so genannten Tochtergesellschaften. Sie ist jedoch noch im angestammten Gebiet der Produktion und des Verkaufs tätig. Man spricht somit von einer "Mischholding", die eine Doppelfunktion hat.

Die Holding als eine spezielle Form der Muttergesellschaft ist eine Finanz- und Dachgesellschaft, die aber keine Betriebstätigkeit ausübt – außer den Kapitalbeteiligungen an ihren Tochtergesellschaften – und keinen Kundenkontakt führt. Die Vorteile der Holding sind die einfachere Führung des Konzerns und das steuerrechtliche Holdingprivileg, durch das sie keine Gewinnsteuern zahlen muss.




</doc>
<doc id="11119" url="https://de.wikipedia.org/wiki?curid=11119" title="Gerhard Seyfried">
Gerhard Seyfried

Gerhard Seyfried (* 15. März 1948 in München) ist ein deutscher Comiczeichner, Karikaturist und Schriftsteller. Seyfried machte sich international einen Namen als grafischer Chronist der links-alternativen Szene, die er auf humorvolle, liebenswürdige Weise und mit viel Wortwitz karikierte. Besonders beliebt wurden seine großformatigen Wimmelbilder als Plakate. Seit den 2000er-Jahren veröffentlicht er akribisch recherchierte historische Romane, die meist in der Zeit vor dem Ersten Weltkrieg spielen.

Sein Vater Fritz Seyfried arbeitete als Einkaufsleiter und seine Mutter war bei der Bundesbahn beschäftigt. Gerhard Seyfried wuchs mit seiner drei Jahre jüngeren Schwester Sylvia in einfachen, aber behüteten Verhältnissen in München-Pasing auf. Von 1963 bis 1967 machte er eine Lehre zum Industriekaufmann, die er jedoch kurz vor Prüfung abbrach. Nach der Lehrzeit durfte er noch ein Jahr lang ein Praktikum bei dem Grafiker der Werbe-Abteilung machen und konnte so noch „sehr, sehr viel“ lernen. Eine weitere Ausbildung zum Gebrauchsgrafiker in München schloss sich an. Als er den Einberufungsbescheid zur Bundeswehr bekam, fragte er Kriegsdienstgegner um Rat. Diese vermittelten ihn zu einem Nervenarzt, der ihn wegen eines früheren Oberkieferrisses wehruntauglich schrieb, da er damit keinen Stahlhelm tragen durfte. Sein Vater unterstützte ihn in dem Wunsch, den Wehrdienst zu verweigern. 1967 durfte er wegen seiner besonderen zeichnerischen Begabung und trotz fehlendem Abitur Malerei und Grafik an der Münchner Akademie für das Graphische Gewerbe studieren. Ende 1969 wurde er aus der Akademie entlassen wegen der Rädelsführer­schaft von Streiks gegen die Notstandsgesetze – offiziell nun wegen mangelnder Begabung. 

Ab 1970 arbeitete er als selbstständiger Grafiker und Karikaturist für Werbeagenturen, lokale Firmen und das Münchner Stadtmagazin "Blatt". Von 1971 an bezeichnete sich Seyfried als freischaffender Karikaturist. Seine Freundschaft mit Fritz Teufel und anderen linksorientierten Aktivisten hatte zur Folge, dass seine zwölfköpfige Wohngemeinschaft häufig von der Polizei durchsucht worden ist. Seyfried schätzt eine Summe von insgesamt 20 Hausdurchsuchungen und „ein paar Dutzend“ Festnahmen, obwohl er „nichts weiter als eine Randfigur“ war. Diese „endlosen Polizeischikanen“ waren für ihn ein gewichtiges Motiv, 1976 von München nach Berlin zu ziehen. Doch „in Berlin ging das dann weiter. Erst Ende der 70er haben sie mich in Ruhe gelassen.“

Seit 1976 bildet West-Berlin den Hintergrund seiner Comics und Cartoons. Die Geschichten sind in der links-alternativen Hausbesetzerszene und Ökologiebewegung angesiedelt, die Seyfried satirisch und liebenswürdig zugleich aufs Korn nimmt. Seyfrieds Zeichnungen und Plakate gehörten in den Wohngemeinschaften der Bundesrepublik Deutschland zum festen Inventar. „In den späten 1970er und bis Mitte der 1980er Jahre war Seyfried denn auch der meist geklaute Zeichner der Republik – die Zahl der Raubdrucke in Schülerzeitungen und Szeneblättern war Legion“. Zu einem Markenzeichen wurde seine Comic-Figur "Zwille", ein schwarzes anarchistisches Männchen mit kugelförmig abstehenden Kopf- und Barthaaren, das breit grinsend oft eine an der Zündschnur glimmende, kugelförmige Bombe in der Hand hält. Darüber hinaus erhält er bis heute von Polizisten aus aller Welt Anfragen nach Plakaten, auf denen Polizisten mit den Knollennasen abgebildet sind.

Nach dem Verkaufserfolg seiner Karikaturensammlung aus dem Münchner "Blatt" „Wo soll das alles enden“ verbrachte er von 1978 an mehrere Studienaufenthalte in den Vereinigten Staaten. In San Francisco traf er seine Vorbilder Gilbert Shelton und , die zu seinen Freunden wurden. Diese Begegnungen lösten bei ihm einen Wandel im Selbstverständnis aus: „Von da an begriff er sich nicht mehr „nur“ als Polit-Cartoonist der Szene, sondern als Künstler, der auch einmal längere Geschichten erzählt.“ Seyfried übersetzte später gemeinsam mit Harry Rowohlt die Geschichten der Underground-Comicserie "The Fabulous Furry Freak Brothers" von Shelton und Mavrides.

1984 zog er von seiner instandbesetzten Wohnung in Kreuzberg in das bürgerliche Charlottenburg um, da er dort die notwendige Ruhe zum Arbeiten fand.

1990 lernte er die ebenso aus München kommende Wahl-Berlinerin, Autorin und Zeichnerin Ziska kennen. Mit dieser Beziehung wechselte seine Themenwahl zum dystopischen Science Fiction. „Für meine Fans war das ein riesiger Schock. Comics die nicht lustig sind. Düstere Science Fiction.“ Seyfried veröffentlichte mit seiner „Freundin und Kollegin Ziska“ vier Comic-Alben. Das Künstlerduo nannte sich die „Harmonian Anarchists“ und hatte 2008 noch Material für weitere „drei Comicbände, die wir liebend gerne machen würden.“ Wegen des geringen Verdienstes bei Comicalben blieb es beim Projekt: „In Deutschland kann man davon nicht leben. Für ,Starship Eden', das letzte gemeinsame Buch mit meiner Kollegin Ziska Riemann haben wir hinterher einen Stundenlohn von fünf Mark ausgerechnet.“

1996 schrieb er mit Mathias Bröckers "Hanf im Glück". Seyfried zeichnete u.a. ein satirisches „Conspiracy Diagramm“ für das 9/11-Buch von Mathias Bröckers, das 2002 zweimal wegen Verwendung von Kennzeichen verfassungswidriger Organisationen beschlagnahmt wurde. Nach einem Arbeitsaufenthalt im Jahr 2003 zusammen mit Bröckers in Solothurn (Schweiz) kehrte er 2004 nach Berlin zurück.

Gerhard Seyfried arbeitete gelegentlich auch als Übersetzer, Modellbauer, Fotograf und Journalist. Er besitzt eine Sammlung internationaler Polizeisterne.

Die Bibliothek seines Vaters, die zu einem Teil aus einer Kolonialbibliothek mit Erinnerungen und Abenteuerromanen bestand, weckte ihn ihm später den Wunsch zur Erforschung der deutschen Geschichte vor dem Ersten Weltkrieg. Nach dem Herero-Aufstand (2003) und der Münchner Stadt- und Spaßguerilla (2004) schildert Seyfrieds dritter historischer Roman "Gelber Wind" (2008) abermals einen Aufstand von Unterdrückten, diesmal den sogenannten Boxeraufstand 1900 in Peking. 

Der Grünen-Politiker Hans-Christian Ströbele in Friedrichshain-Kreuzberg war der einzige Politiker, für den der Anarchist Seyfried politische Werbung machte, da Ströbele im Gegensatz zu Bündnis 90/Die Grünen Kriegseinsätze der Bundeswehr ablehnt. Seine Plakate mit den beliebten Wimmelbildern trugen 2002, 2005 und 2009 zum Erfolg für das erste Direktmandat eines grünen Bundestagsabgeordneten bei. Seit 2013 unterstützt Seyfried Die Linke mit Wahlkampf-Plakaten.

Von Juni 2006 bis Oktober 2010 veröffentlichte Seyfried einen Blog in der Internet-Ausgabe der Berliner Tageszeitung "taz", in dem er seinen Wort- und Bilderwitz vorstellen konnte. Bis heute durfte er jedoch nicht in einer Tageszeitung eine Zeichenkolumne publizieren, was sein Freund Arnulf Rating empört zum Ausdruck brachte anlässlich der Eröffnung einer Seyfried-Retrospektive in der Frankfurter "caricatura".

Weil Seyfried nicht allein von seinen politischen Comics und Plakaten leben kann, musste er in der Regel kommerzielle grafische Auftragsarbeiten übernehmen. Das Schreiben historischer Romane ab 2003 besserte seine Einnahmen. Die wenig ertragreiche Liebe zu seinen Bildergeschichten habe mit dazu beigetragen, keine Familie gründen zu können. Insgesamt veröffentlichte er bis 2018 fünfzehn Comic-Alben.

Mit "Flucht aus Berlin" (1989/90) änderte er seinen Zeichenstil und wechselte er von der „wuselige[n] Linie, die sich um die winzigsten Kleinigkeiten kringelt“ (F.W. Bernstein) zur ligne claire. Zur Kolorierung seiner Figuren benutzt er mittlerweile den Computer: „Ich zeichne mit Bleistift, pause es dann mit Tusche durch, koloriere aber nicht mehr mit der Hand. Das ist zu teuer und zu giftig.“ Seine Kollegin Ziska urteilte: „Er ist sehr präzise und ein unheimlich guter Techniker.“ Wenn er allein an einem neuen Comicband arbeitet, verzichtet er auf ein „storyboard“, einem visualisierten Szenenbuch, und verlässt sich dabei ganz auf seine spontane Intuition. Lediglich bei seinen Comic-Alben mit Ziska entwickelten sie gemeinsam ein "storyboard".

Beim Verfassen seiner historischen Romane dagegen rekonstruiert er zuerst das „Gerüst der historischen Ereignisse“. Dazu beschränkt er sich jedoch nicht auf die historisch-wissenschaftliche Sekundärliteratur, sondern recherchiert in Archiven nach Originaldokumenten und Primärquellen. Hilfreich sind für ihn auch alte Fotografien, da er daraus „eine Unmenge an Sachen herauslesen“ kann. Erst am Ende der Recherchen verbindet er den Zusammenhang der Ereignisse mit fiktiven Figuren, die vor allem als Beobachter agieren.









Interviews


</doc>
<doc id="11122" url="https://de.wikipedia.org/wiki?curid=11122" title="Weibliches Geschlecht">
Weibliches Geschlecht

Das weibliche Geschlecht ist bei der zweigeschlechtlichen Fortpflanzung dasjenige Geschlecht, das die weiblichen Keimzellen (Eizellen) bereitstellt, die von der größeren Menge der männlichen Keimzellen (Spermien) befruchtet werden und einen oder mehrere Nachkommen (Mehrlinge) entstehen lassen. Es wird mit dem Venussymbol ♀ gekennzeichnet.

Höhere Tiere und Pflanzen vermehren sich in der Regel geschlechtlich (sexuell). Es kommen zwar viele Arten (oder höhere systematische Einheiten) mit ungeschlechtlicher Fortpflanzung vor, diese ist aber dann evolutiv abgeleitet, das heißt die Vorfahren dieser Gruppen besaßen geschlechtliche Fortpflanzung. Meist sind dabei weibliches (feminines) "und" das männliches (maskulines) Geschlecht auf verschiedene Individuen, bei Tieren Weibchen und Männchen genannt, verteilt. Auch Arten, bei denen echte Zwitter vorkommen, solche mit Selbstbefruchtung und einhäusige Pflanzen vermehren sich geschlechtlich, hier sind die Individuen, entweder gleichzeitig (simultan) oder nacheinander (sukzessiv), sowohl weiblich wie auch männlich. Die Zweigeschlechtlichkeit hat sich im Laufe der Evolution mehrmals unabhängig voneinander entwickelt. Unterscheiden sich beide Keimzellen nicht nach Größe oder Form, liegt eine Isogamie vor.

Beim Menschen, ebenso wie bei einigen anderen Arten, kommen gelegentlich Individuen vor, die sich nicht eindeutig einem der beiden Geschlechter zuordnen lassen, früher oft auch Hermaphroditen genannt. Dies wird in der Biologie allgemein als Gynandromorphismus, beim Menschen eher als Intersexualität, gesellschaftlich als Drittes Geschlecht bezeichnet. Im Gegensatz zum echten Zwittertum oder Hermaphroditismus sind solche Individuen aber nur eingeschränkt oder gar nicht fortpflanzungsfähig. Eine "Verweiblichung" liegt vor, wenn bei einem Individuum im Laufe seiner Entwicklung Eigenschaften deutlicher werden, die den weiblichen Phänotypen seiner Spezies entsprechen.

Beim Menschen wird das weibliche Geschlecht durch drei Kennzeichen (Geschlechtsmerkmale) bestimmt:

Bei menschlicher Intersexualität sind die primären Geschlechtsmerkmale weniger stark ausgeprägt und es sind teilweise daneben auch männliche Geschlechtsmerkmale vorhanden. Im Laufe des Lebens wird Weiblichkeit vor allem durch soziale Rollenverständnisse geprägt (siehe auch Frau).

Bei Tieren bestimmen unterschiedliche körperliche und genetische Mechanismen das Geschlecht eines Individuums:

Bei Samenpflanzen wird danach unterschieden, ob ein Pflanzenindividuum


</doc>
<doc id="11124" url="https://de.wikipedia.org/wiki?curid=11124" title="Steinlaus">
Steinlaus

Die Steinlaus ("Petrophaga lorioti") ist ein von Loriot gezeichnetes, fiktives Nagetier, das dieser 1976 in seinem Sketch "Die Steinlaus (Prof. Grzimek)" im Rahmen der zweiten Folge der Fernsehsendung "Loriot" präsentierte. Loriot selbst tritt darin in einer Imitation des Zoologen und Fernsehmoderators Bernhard Grzimek auf.

1983 nahm das medizinische Wörterbuch Pschyrembel die Steinlaus als fingierten Lexikonartikel (Nihilartikel) ins Nachschlagewerk auf, welcher bei verschiedenen Neuauflagen mehrfach erweitert und ergänzt wurde. Dies wiederum führte zu weiteren Artikeln und Ausführungen in diversen wissenschaftlichen und populärwissenschaftlichen Publikationen und Einlassungen. Seitdem ist die Steinlaus ein bekanntes Beispiel des wissenschaftlichen Witzes.

In einer 1976 in der ARD ausgestrahlten Parodie auf die Sendereihe "Ein Platz für Tiere" beschreibt Loriot – in der Rolle des Bernhard Grzimek – die Steinlaus als scheuen Nager, der sich von Silicaten, also von Steinen, ernähre. Gelegentlich würde auch ein Eisenträger nicht verschmäht. Das geschlechtsreife Männchen habe einen Tagesbedarf von etwa 28 Kilogramm Beton und Ziegelsteinen, das Weibchen verzehre in der Schwangerschaft beinahe die doppelte Menge. Am Anfang des Sketches informiert Loriot die Zuschauer, er habe eine Steinlaus mitgebracht, was auf Grzimeks Fernsehsendung rekurriert, der in aller Regel ein mitgebrachtes Tier präsentierte, wenn er seine Moderation begann. Loriot hatte einige Steinbrocken vor sich auf dem Tisch liegen, von denen nach einem Einspielfilm – in dem einstürzende Gebäude, darunter Hochhäuser und sogar eine Kirche, vermeintlich durch Steinlaus-Fraß, gezeigt werden – nach wenigen Minuten nur noch einige Bröckchen übrig sind, da die mitgebrachte Steinlaus inzwischen „ihren gröbsten Hunger gestillt“ habe. 

Der sei vom Aussterben bedroht, bei wissenschaftlichen Grabungen im Erdreich seien jedoch in mehr als 20 Metern Tiefe noch einzelne Tiere gefunden und in zoologische Gärten verbracht worden.

Die Steinlaus wird außer im originalen Fernsehsketch auch in gedruckten Publikationen Loriots erwähnt.

1983 verzeichnete das renommierte medizinische Wörterbuch Pschyrembel aus dem Berliner Wissenschaftsverlag Walter de Gruyter, ein Standard-Nachschlagewerk in seinem Fachgebiet, in der 255. Auflage erstmals die Steinlaus. Der Nihilartikel scheint Loriots „Erkenntnisse“ zu belegen. Darüber hinaus informiert das Lexikon über fingierte Forschungsarbeiten, die den Wert der Steinlaus bei der Therapie von Gallen-, Blasen- und Nierensteinen erkannt hätten und die Unterarten "Gallensteinlaus" und "Nierensteinlaus" werden erwähnt. In der 257. Auflage des Pschyrembel wurde der Eintrag über die Steinlaus wieder getilgt. Wegen unerwartet heftiger Leserproteste wurde die Steinlaus in der folgenden Ausgabe von 1997 in erweiterter Form wieder aufgenommen. Der skandalöse Vorgang wird unter anderem in Torsten Roelckes "Fachsprachen" detailliert beschrieben. 

In der revidierten Fassung des Pschyrembel fanden „neueste Erkenntnisse“ Eingang, die das zeitweilige Verschwinden der Steinlaus mit dem Fall der Berliner Mauer als Nahrungsgrundlage in Verbindung bringen.

In der 260. Auflage des Pschyrembel wurden weitere „neuere Forschungsergebnisse“ zur Steinlaus verzeichnet, beispielsweise deren Anwendung in der Homöopathie. In der am 24. September 2007 erschienenen 261. Auflage wurde der Artikel zur Steinlaus wiederum erweitert. So wird beispielsweise unter „weitere Anw.“ erklärt, dass die Bedingungen für eine Feinstaubplakette durch den Einsatz von spezialisierten Steinläusen in Kombination mit Filtern erfüllt werden könnten.

In der 1. Auflage des „Pschyrembel Psychiatrie, Klinische Psychologie, Psychotherapie“ von 2009 wird eine wissenschaftliche Einordnung und Neubewertung der "Steinlausphobie" vorgenommen. Diese phobische Störung äußere sich in einer unbegründeten und anhaltenden Angst vor Steinläusen, Steinlaus-Bildern und entsprechenden Texten. In der Regel sei die Steinlausphobie gekoppelt mit einem übermäßigen Wunsch und Drang, den Anlass der Angst zu vermeiden.

("Eine Einführung in die Wissenschaft von den mineralischen Bodenschätzen", 4. Auflage von 1992, ISBN 3 510 65150 2) In Kapitel II: "Lagerstättenbildung durch Verwitterung" wird auf Seite 60 ebenfalls die "Gemeine Steinlaus" kurz erwähnt, wobei auf den "Pschyrembel" 1986; Abb. 34 verwiesen wird. Eine kleine Zeichnung der Steinlaus illustriert das fiktive Tier zusätzlich.
Ein Eintrag der mittlerweile aufgrund eines Skandals von der Universität Zürich freigestellten Schweizer Medizinhistorikerin Iris Ritzmann in der Enzyklopädie Medizingeschichte behauptet eine Vorgängerart, den Steinfresser oder Lithophagus. Dieser sei bereits im Lemma Vielfraß bei Zedlers Universallexikon erwähnt und spielte eine Rolle bei mittelalterlichen Trepanationen. Ritzmann behauptet eine Ausrottung der Steinfresser, auf die die heutigen Steinläuse mutativ zurückgingen, durch homöopathische Anwendungen im 19. Jahrhundert, namentlich mit Hilfe von Lapis infernalis C 30.

Auf die Steinlaus wird immer wieder in Nachrichtenmedien Bezug genommen. Natürlicherweise wird auch in humoristischen und satirischen Veröffentlichungen auf die Steinlaus verwiesen.



Zum Schutz gegen Betrug und Fälschung in der Wissenschaft und auch in anderen Bereichen werden Steinläuse sowie geeignete Synonyme gelegentlich in Plagiatsfallen freigesetzt.



</doc>
<doc id="11128" url="https://de.wikipedia.org/wiki?curid=11128" title="Baum (Begriffsklärung)">
Baum (Begriffsklärung)

Baum (westgermanisch "boum", ‚Baum‘, ‚Baumstamm‘) bezeichnet allgemein:

Baum ist der Name folgender Orte:

in den Vereinigten Staaten:
B.A.U.M. steht als Abkürzung für:

Siehe auch:


</doc>
<doc id="11130" url="https://de.wikipedia.org/wiki?curid=11130" title="Statistische Mechanik">
Statistische Mechanik

Die statistische Mechanik war ursprünglich ein Anwendungsgebiet der Mechanik. Heutzutage wird der Begriff oft synonym zur statistischen Physik und zur statistischen Thermodynamik gebraucht und steht somit für die (theoretische und experimentelle) Analyse zahlreicher, fundamentaler Eigenschaften von Systemen vieler Teilchen (Atome, Moleküle usw.). U.a. liefert die statistische Mechanik eine mikroskopische Fundierung der Thermodynamik. Sie ist daher von großer Bedeutung für die Chemie, insbesondere für die physikalische Chemie, in der man auch von statistischer Thermodynamik spricht.

Darüber hinaus beschreibt sie eine Vielzahl weiterer thermischer Gleichgewichts- und Nichtgleichgewichtseigenschaften, die mit Hilfe moderner Messmethoden (z. B. Streuexperimente) untersucht werden.

In der (ursprünglichen) statistischen Mechanik wird der Zustand eines physikalischen Systems nicht durch die Trajektorien, d. h. durch den zeitlichen Verlauf von Orten und Impulsen der einzelnen Teilchen bzw. deren quantenmechanischen Zuständen, charakterisiert, sondern durch die Wahrscheinlichkeit, derartige mikroskopische Zustände vorzufinden.

Die statistische Mechanik ist vor allem durch Arbeiten von James Clerk Maxwell, Ludwig Boltzmann und Josiah Willard Gibbs entstanden, wobei letzterer den Begriff prägte. Im Folgenden sollen einige Begriffe aus der statistischen Physik erläutert werden, die insbesondere bei der Analyse von Eigenschaften des thermischen Gleichgewichts eine wichtige Rolle spielen.

Historisch von zentraler Bedeutung ist die Boltzmann’sche Entropieformel (die auch auf dem Grabstein von Ludwig Boltzmann eingraviert ist):

Hier bezeichnet "S" die (statistische) Entropie eines abgeschlossenen Systems, d. h. eines "mikrokanonischen Ensembles". Die Größe formula_2 gibt die Zahl der Mikrozustände an (z. B. Orte und Impulse aller Teilchen in einem Gas), die mit den thermodynamischen Zustandsgrößen Energie, Volumen und Teilchenzahl verträglich sind (Boltzmann bezeichnete diese Größe als „Komplexionzahl“ gleich dem statistischen Gewicht, manchmal auch als W angegeben, des makroskopischen Zustands). Die Konstante formula_3 wird als Boltzmannkonstante bezeichnet und hat wie die Entropie die Einheit Joule pro Kelvin.

Es wird also berücksichtigt, dass nicht ein einzelner mikroskopischer Zustand, sondern vielmehr "alle möglichen Zustände" das makroskopische Verhalten eines physikalischen Systems bestimmen. "Statistische Ensembles" spielen in der statistischen Physik eine entscheidende Rolle; man unterscheidet zwischen dem mikrokanonischen, dem kanonischen und dem großkanonischen Ensemble.

Ein klassisches und einfaches Beispiel für die Anwendung der statistischen Mechanik ist die Herleitung der Zustandsgleichung des idealen Gases und auch des Van-der-Waals-Gases.

Sind Quanteneigenschaften (Ununterscheidbarkeit der Teilchen) wesentlich, z. B. bei tiefen Temperaturen, können besondere Phänomene auftreten und von der statistischen Physik vorhergesagt werden. Für Systeme mit ganzzahligem Spin (Bosonen) gilt die Bose-Einstein-Statistik. Unterhalb einer kritischen Temperatur und bei hinreichend schwachen Wechselwirkungen zwischen den Teilchen tritt ein besonderer Effekt auf, bei dem eine Vielzahl von Teilchen den Zustand niedrigster Energie einnehmen: Es gibt eine Bosekondensation.

Systeme mit halbzahligem Spin (Fermionen) gehorchen der Fermi-Dirac-Statistik. Wegen des Pauli-Prinzips werden auch Zustände höherer Energie angenommen. Es gibt eine charakteristische obere „Energiekante“, die Fermienergie. Sie bestimmt u. a. zahlreiche thermische Eigenschaften von Metallen und Halbleitern.

Die Konzepte der statistischen Mechanik lassen sich nicht nur auf Ort und Impuls der Teilchen, sondern auch auf andere, z.  B. magnetische Eigenschaften anwenden. Hierbei ist die Modellbildung von großer Bedeutung; z. B. sei auf das ausführlich untersuchte Ising-Modell hingewiesen.


Grundlagen

Lehrbücher

Populärwissenschaftliche Literatur

Einführungen in philosophische Themenfelder


</doc>
<doc id="11132" url="https://de.wikipedia.org/wiki?curid=11132" title="Höhere Säugetiere">
Höhere Säugetiere

Die Höheren Säugetiere oder Plazentatiere (Eutheria; Placentalia meist nur wenn die Kronengruppe gemeint ist) bilden wie die eierlegenden Kloakentiere (Protheria) und die Beuteltiere (Metatheria) eine Unterklasse der Säugetiere (Mammalia). Die Höheren Säuger sind die artenreichste – zu diesem Taxon zählen rund 94 Prozent der rezenten Spezies – und auch, hinsichtlich des Körperbaues und der Lebensräume, vielfältigste Gruppe der Säugetiere (1135 von 1229 Gattungen).

Beide deutschen Namen für dieses Taxon können missverständlich sein: das Wort „höher“ darf nicht im Sinne eines Werturteils verstanden werden, das einen „Fortschritt“ widerspiegelt, sondern deutet eher die in einer phylogenetischen Darstellung „höher“ gelegene Entwicklungslinie an. Ebenso ist „Plazentatiere“ nicht völlig zutreffend, da auch manche Beuteltiere (wie die Nasenbeutler und der Koala) eine einfache Plazenta besitzen.

Die Höheren Säugetiere haben die für Säugetiere typischen Merkmale wie ein Fell­kleid aus Haaren, die drei Gehörknöchelchen, Milchdrüsen, das Zwerchfell und andere, die unter "Körperbau der Säugetiere" beschrieben sind. Die auffälligsten Unterschiede sind im Bau des Geschlechtsapparates und in der Fortpflanzungsweise zu finden. Infolge der zahlreichen Anpassungen an die unterschiedlichsten Lebensweisen und Habitate gibt es daneben nur wenige Exklusivmerkmale, die diese Gruppe von den übrigen Säugern unterscheiden.

Wie alle Säugetiere sind die Höheren Säugetiere in der Regel durch ein heterodontes Gebiss mit vier verschiedenen Zahntypen charakterisiert, die Schneidezähne ("Incisivi"), Eckzähne ("Canini") und zwei Arten von Backenzähnen (Prämolaren und Molaren). Im Vergleich zu den Beuteltieren, die oft zwischen 40 und 50 Zähne besitzen, haben die Höheren Säuger meist weniger Zähne. Die ursprüngliche Zahnformel lautete 3/3-1/1-4/4-3/3, das heißt pro Kieferhälfte drei Schneidezähne, ein Eckzahn, vier Prämolaren und drei Molaren, insgesamt also 44 Zähne. Diese ursprüngliche Zahnformel findet sich noch bei manchen Arten, zum Beispiel beim Wildschwein, in den meisten Fällen hat sich die Zahl der Zähne infolge spezialisierter Ernährung vermindert. Einige wenige Taxa, zum Beispiel die Ameisenbären oder die Schuppentiere, sind gänzlich zahnlos geworden. Der umgekehrte Fall, eine evolutionsbedingte Erhöhung der Anzahl der Zähne, ist nur in wenigen Fällen eingetreten: das Riesengürteltier ("Priodontes maximus") hat bis zu 100 stiftartige Zähne in der röhrenförmigen Schnauze, die höchste Zahl aller Landsäugetiere. Einen Sonderfall stellen die Zahnwale dar, deren Zähne wieder gleichförmig ("homodont") geworden sind; manche Delfinarten haben bis zu 260 Zähne. Im Gegensatz zu den Beuteltieren, die die meisten Zähne nur einmal anlegen, werden in der Regel die Zähne mit Ausnahme der Molaren zunächst als Milchgebiss angelegt und dann durch das bleibende Gebiss ersetzt.

Auch in der Anordnung der Schädelknochen gibt es einige Unterschiede zwischen den Höheren und den übrigen Säugern, unter anderem im Bau des Keil- und des Felsenbeins. Im Bau des Gehirns zeichnen sich Plazentatiere durch das Vorhandensein des Corpus callosum oder Balken, eine große, quer verlaufende Verbindung zwischen den beiden Hirnhemisphären des Großhirns, aus.

Die rezenten Höheren Säugetiere unterscheiden sich von den Kloaken- und Beuteltieren durch das Fehlen der Beutelknochen ("Ossa epubica"), zwei vom Schambein des Beckens nach vorne ragende Knochen. Allerdings sind diese bei den urtümlichen Vertretern dieser Gruppe noch vorhanden und dürften ein ursprüngliches Säugetiermerkmal darstellen. Da diese Knochen bei den übrigen Säugern bei beiden Geschlechtern vorhanden sind, dürften sie ursprünglich nichts mit der Fortpflanzung zu tun gehabt haben, sondern eher dem Muskelansatz für die Bewegung der hinteren Gliedmaßen gedient haben.

Im Bau des Harn- und Geschlechtsapparates sind die weiblichen Höheren Säugetiere durch eine einfache Vagina ("Monodelphie") gekennzeichnet – Beuteltiere haben deren zwei. Der Bau der Gebärmutter ("Uterus") ist unterschiedlich, generell herrscht jedoch eine Tendenz zur Verschmelzung der beiden Müller-Gänge. Einige Gruppen haben einen paarigen Uterus ("Uterus duplex"), etwa Hasenartige und viele Nagetiere, bei anderen erfolgt die Verschmelzung nur zum Teil, es entsteht eine „zweihörnige Gebärmutter“ ("Uterus bicornis"). Am weitesten erfolgt die Verschmelzung bei den meisten Primaten und manchen Fleder- und Nebengelenktieren, es entsteht eine einfache Gebärmutter ("Uterus simplex"). Bei den männlichen Plazentatieren liegt das Skrotum – soweit vorhanden – hinter dem Penis (bei den Beuteltieren befindet es sich davor). Allerdings gibt es auch Gruppen ohne einen Hodenabstieg; bei ihnen bleiben die Hoden in der Bauchhöhle (etwa beim Elefanten oder Rüsselspringer) oder der Hodenabstieg ist nur unvollständig vollzogen (wie bei den Walen).

Generell gibt es kaum Spezialisierungen im äußeren Körperbau, die den Kloaken- oder Beuteltieren vorbehalten blieben und die sich nicht in konvergenter Form bei den Höheren Säugern finden. Im Gegenzug finden sich viele Formen innerhalb dieser Gruppe, die kein Pendant bei den beiden übrigen Säugetiertaxa haben. Beispielsweise ist es nur innerhalb der Plazentatiere zur Entstehung von meeresbewohnenden Säugetieren (Wale, Seekühe und Robben) gekommen. Auch die einzigen zum aktiven Flug fähigen Säuger, die Fledertiere, zählen zu dieser Gruppe – die passive Gleitfähigkeit hat sich neben einigen Plazentatieren (beispielsweise Riesengleiter und Gleithörnchen) allerdings auch bei manchen Beuteltieren (wie den Gleitbeutlern) entwickelt.

Höhere Säugetiere sind weltweit verbreitet. Sie finden sich auf allen Kontinenten, in allen Ozeanen sowie auf den meisten Inseln. Lediglich in Australien waren sie nicht die dominante Säugergruppe. Es gab, bevor der Mensch dort landete und zahlreiche Neozoen einführte, nur relativ wenige Arten, namentlich Fledertiere und Altweltmäuse. Auf abgelegenen Inseln gab es bis zur Ankunft des Menschen nur eine eingeschränkte Säugetierfauna. So waren auf vielen Inseln, darunter Neuseeland, Fledertiere die einzigen Säuger.

Höhere Säugetiere haben nahezu alle Regionen der Erde besiedelt und kommen in den meisten Lebensräumen vor. Man findet sie sowohl in Wüsten und Regenwäldern als auch im Hochgebirge und in den Polarregionen. Zu den wenigen Regionen, in denen sich (zumindest bis auf zeitweilige Aufenthalte des Menschen) keine Höheren Säuger finden, zählen die Tiefsee und das Innere des antarktischen Kontinents.

Hinsichtlich der Lebensweise lassen sich kaum verallgemeinernde Aussagen treffen. Einzig beim Sozialverhalten lässt sich beobachten, dass fast alle Säugetierarten, die komplexe Gruppenstrukturen und eine hierarchische Rangordnung pflegen, in dieser Gruppe zu finden sind. Dies ist jedoch nicht die bevorzugte Lebensweise dieser Tiere, es gibt genauso einzelgängerische, in Paaren oder anderen Sozialformen lebende Arten. Die Ernährung variiert stark, hieraus lassen sich keine Besonderheiten ableiten.

Ein Exklusivmerkmal der Höheren Säugetiere ist der Trophoblast, die äußere Zellschicht einer befruchteten Eizelle. Dieser bewirkt eine Immunbarriere und verhindert, dass das Immunsystem der Mutter auf den Embryo anspricht. Damit verbunden ist eine leistungsfähige Plazenta, eine Chorioallantoisplazenta, die aus Embryonalhüllen (Chorion und Allantois) und der Uterusschleimhaut besteht und den Embryo mit Nährstoffen versorgt. Dadurch wird eine im Vergleich zu den Beuteltieren längere Tragzeit und ein fortgeschrittenerer Entwicklungsgrad bei der Geburt gewährleistet. Die Trächtigkeitsdauer variiert stark, bei manchen Hamsterarten liegt sie bei nur 16 Tagen, beim Afrikanischen Elefanten kann sie bis zu 25 Monate dauern. Der Entwicklungszustand der Neugeborenen ist ebenfalls unterschiedlich und auch von der Lebensweise abhängig, es finden sich Nesthocker (beispielsweise Raubtiere und Nagetiere) ebenso wie Nestflüchter (wie etwa Paarhufer und Wale).

Die frühesten bekannten Vertreter der Höheren Säugetiere lebten im Mesozoikum; als ältester bekannter Vertreter gilt "Juramaia sinensis" aus der Liaoning-Provinz im nordöstlichen China, das 160 Millionen Jahre alt ist (Oberjura), (siehe auch Evolution der Säugetiere).

Im Vergleich zu anderen Säugetiertaxa wie den Beuteltieren oder den Multituberculata ist der fossile Befund aus der Kreidezeit jedoch relativ dürftig, bislang sind nur in Ostasien und Nordamerika Vertreter dieser Tiere gefunden worden. Zu den bekanntesten Gattungen dieser Epoche zählen "Asioryctes", die Leptictida, die möglicherweise Vorfahren der Insektenfresser sind, die Zalambdalestidae (mögliche Vorfahren der Nagetiere), die Zhelestidae (mögliche Vorfahren der „Huftiere“) und "Cimolestes" (eventuell ein Urahn der Raubtiere). Generell ist aber die Zuordnung zu heutigen Taxa umstritten, zweifelsfrei mit heutigen Arten verwandte Säugetiere traten erst im Paläozän auf.

Mit dem Aussterben der Dinosaurier wurden viele ökologische Nischen frei, die von einer Vielzahl neu entstehender Säugetiergruppen besetzt wurden. Im Verlauf des Känozoikums entwickelten sich die Säugetiere zu der dominanten Wirbeltiergruppe auf den meisten Kontinenten, lediglich in Südamerika, der Antarktis (bis zu ihrer Vereisung) und in Australien konnten sich nennenswerte Beuteltierfaunen entwickeln. Die Entwicklungsgeschichte der Plazentatiere verlief jedoch keineswegs geradlinig, sondern war durch evolutionäre Sackgassen, Verdrängungsprozesse und wieder gänzlich ausgestorbene Säugetiergruppen geprägt. Insbesondere im Paläozän und Eozän gab es eine Reihe von Ordnungen, die mit den heutigen Gruppen nicht verwandt waren. Die meisten heutigen Säugetierordnungen sind seit dem Eozän belegt, darunter auch die Vorfahren der wohl spezialisiertesten Gruppen, der Fledertiere und Wale. Ihre größte Artenvielfalt erreichten die Säuger im Miozän; seither verschlechterten sich die Klimabedingungen kontinuierlich, bis hin zu den Eiszeiten des Pleistozäns. Die klimatischen Verschiebungen, verbunden mit den Einflüssen des Menschen, sorgen seither für einen Rückgang der Artenvielfalt.

Für dieses Taxon sind zwei wissenschaftliche Namen gebräuchlich, Eutheria und Placentalia. In manchen Systematiken werden diese Bezeichnungen synonym verwendet, in anderen jedoch unterschieden: So werden die Placentalia als die heutigen Vertreter sowie alle deren fossile Verwandte definiert (Kronengruppe), während Eutheria weiter gefasst ist und auch die basalen Vertreter wie "Eomaia" miteinschließt. Da jedoch viele urtümliche Vertreter nur durch spärliche Fossilien­funde bekannt sind und die Placentalia in diesem Sinn paraphyletisch sein könnten, ist diese Trennung umstritten.

Die Höheren Säugetiere und die Beuteltiere werden meist zur Gruppe der Theria zusammengefasst. Diese Gruppe wird durch eine Reihe von gemeinsamen abgeleiteten Merkmalen (Autapomorphien) definiert, wozu unter anderem die Viviparie (das Gebären lebenden Nachwuchses) und das Vorhandensein von Zitzen zählen. Einige Forscher hingegen lehnen die Theria ab, sie halten die Kloaken- und Beuteltiere für enger miteinander verwandt und stellen sie in ein Taxon Marsupionta. Für dieses Taxon werden gewisse genetische Daten angeführt, die überwältigenden morphologischen und genetischen Befunde sprechen jedoch gegen diese These.

Die rezenten Höheren Säuger lassen sich verhältnismäßig eindeutig in rund 20 Ordnungen einteilen, die meist morphologisch und auch molekulargenetisch weitgehend abgesichert sind:

Die Verwandtschaftsverhältnisse dieser Ordnungen waren lange Zeit umstritten. Einige Überordnungen wurden hauptsächlich mittels morphologischer Gemeinsamkeiten aufgestellt, die heute als veraltet oder zumindest stark bezweifelt gelten. Dazu zählen etwa die „Huftiere“ (Ungulata), die Paarhufer, Unpaarhufer, Schliefer und andere zusammenfassen. Auch die Insektenfresser schlossen früher mehr Gruppen als heute ein, so wurden auch die Tenrekartigen und manchmal die Rüsselspringer, Spitzhörnchen und Riesengleiter dazu gerechnet. Die beiden letzten fanden sich aber manchmal auch in den Archonta, zu denen außerdem Primaten und Fledertiere gezählt wurden. Nagetiere und Hasen fasste man als Glires zusammen; Gürteltiere und Zahnarme als Nebengelenktiere (Xenarthra), auch die Schuppentiere wurden manchmal zu dieser Gruppe gerechnet. Besonders umstritten war die systematische Zugehörigkeit einiger schwierig einzuordnender Ordnungen wie Erdferkel und Riesengleiter.

Seit dem Einzug der Molekularbiologie in die systematische Forschung hat sich das Bild der Verwandtschaftsverhältnisse innerhalb der Höheren Säugetiere gewandelt. Einige der oben genannten Überordnungen stellten sich als polyphyletisch heraus, das heißt, sie fassten nicht näher verwandte, nur konvergent entwickelte Formen zusammen. Es herrscht heute ein langsam steigender Konsens über vier Überordnungen innerhalb der Plazentatiere. Die vermutete Aufteilung in diese Überordnungen in der Kreidezeit stimmt zeitlich mit dem plattentektonischen Aufbrechen der Kontinente Laurasia und Gondwana überein.

Die Beziehungen der vier Überordnungen untereinander sind zum Teil immer noch umstritten. Die meisten Forscher gehen aber von einem Schwesterverhältnis von Laurasiatheria und Euarchontoglires aus, das gemeinsame Taxon wird Boreoeutheria genannt und fasst die in den nördlichen Kontinenten entstandenen Taxa zusammen.

Klassischerweise gelten die Nebengelenktiere als Schwestergruppe der übrigen Höheren Säugetiere, die als Epitheria zusammengefasst werden. Diese Theorie ist jedoch umstritten, aufgrund einiger molekularer Übereinstimmungen wird manchmal vermutet, dass die Afrotheria sich schon vor den Nebengelenktieren von den Höheren Säugern abgespalten hätten (Boreoeutheria und Nebengelenktieren gründen Taxon Exafroplacentalia). Eine dritte Theorie schließlich sieht in Afrotheria und Nebengelenktieren ein gemeinsames Taxon Atlantogenata, das dann das Schwestertaxon der Boreoeutheria wäre. Eine Kladogramm aus einer aktuellen Veröffentlichung (Februar 2013) in der US-amerikanischen Fachzeitschrift Science wird rechts gezeigt.

Da die innere Systematik der Höheren Säuger durch die Molekularbiologie quasi neu definiert wurde, ist die Zuordnung ausgestorbener Ordnungen schwieriger geworden. Da von Fossilien kein genetisches Material zum Vergleich entnommen werden kann, ist man auf morphologische Vergleiche angewiesen; diese haben sich wie oben erwähnt oft als trügerisch erwiesen. Darum können an dieser Stelle nur die wichtigsten ausgestorbenen Gruppen aufgelistet werden, eine systematische Zuordnung ist oft schwierig.



</doc>
<doc id="11137" url="https://de.wikipedia.org/wiki?curid=11137" title="Fortpflanzung">
Fortpflanzung

Fortpflanzung ist die Erzeugung neuer, eigenständiger Nachkommen eines Lebewesens. In der Regel (außer bei manchen Einzellern) ist sie mit einer Vermehrung der Anzahl der Exemplare verbunden. Man unterscheidet die geschlechtliche Fortpflanzung, bei der gewöhnlich zwei Geschlechter sich paaren, und die ungeschlechtliche Fortpflanzung, bei der keine Paarung erfolgt.

Die heute selbstverständliche Vorstellung, dass Lebewesen sich fortpflanzen, tauchte erst gegen Ende des 18. Jahrhunderts auf: Bis dahin betrachtete man die Entstehung, die „Zeugung“ eines Lebewesens als einen Schöpfungsakt. Dabei unterschied man die „Samenzeugung“, wie sie beim Menschen und bei höheren Tieren vorliegt, von der Spontanzeugung, durch die niedere Tiere wie etwa Schlangen oder Fliegen aus fauliger und schlammiger Materie hervorzugehen schienen. In jedem Fall betrachtete man das Eingreifen des Schöpfers als notwendig.

Im 17. Jahrhundert kam die Vorstellung auf, dass Menschen und höhere Tiere nicht jeweils neu gezeugt würden, sondern bereits vorgeformt (präformiert) seien und sich nur noch „auswickeln“ müssten (Präformationslehre). Sie gewann große Überzeugungskraft durch die mikroskopischen Untersuchungen von Antoni van Leeuwenhoek und Anderen, die im Sperma von Menschen und Tieren „Samentierchen“ (Spermien) fanden und in diesen winzige „Menschlein“ (homunculi) zu sehen meinten. Parallel dazu wurde vor allem durch die Experimente Francesco Redis zum Hervorgehen von Fliegen aus faulendem Fleisch deutlich, dass auch solche niederen Tiere nicht spontan entstehen, sondern aus winzigen Eiern. Nun stellte man sich vor, dass alle Lebewesen bereits ineinander geschachtelt vorhanden und bei der Schöpfung zugleich erschaffen worden seien. Die Entdeckung Charles Bonnets im Jahre 1740, dass weibliche Blattläuse sich auch ohne Männchen fortpflanzen können (Parthenogenese), galt als ein glänzender Beweis, wenngleich sie im Widerspruch zu der Annahme stand, dass die künftigen Generationen in den „Samentierchen“ eingeschachtelt seien.

Dass Lebewesen Eigenschaften "beider" Eltern in sich vereinen können, war im Falle des Maultiers schon seit der Antike bekannt, und Joseph Gottlieb Kölreuter beschrieb 1761, dass dies auch bei Kreuzungen verschiedener Tabak-Arten auftritt. Bekannt war darüber hinaus, dass Missbildungen an Nachkommen sowohl vom Vater als auch von der Mutter weitergegeben werden können. Dies waren jedoch wenig beachtete Phänomene, die man im Rahmen der herrschenden Vorstellungen nicht erklären konnte. Ebenso konnten die detaillierten Untersuchungen Caspar Friedrich Wolffs (1759) über die allmähliche Herausbildung von Küken im Ei aus der anfangs ganz formlosen Dottermasse (Epigenese) die Zeitgenossen nicht überzeugen.

Eine neue Denkrichtung regte Johann Friedrich Blumenbach an, indem er 1781 einen Bildungstrieb als „eine der ersten Ursachen von Generation, Nutrition und Reproduction“ postulierte. Dieses vitalistische Konzept bot eine Alternative zu den präformistischen Vorstellungen und umfasste auch den Begriff der Fortpflanzung (Reproduktion). Offen blieb zunächst noch die Frage, ob sich auch Menschen und Säugetiere wie andere Tiere aus Eiern oder, wie es etwa Albrecht von Haller postuliert hatte, aus gerinnender Menstruationsflüssigkeit entwickeln, bis Karl Ernst von Baer 1827 das menschliche Ei im Ovarialfollikel entdeckte.
Den Vorgang der Befruchtung, also der Vereinigung von Eizelle und Spermium, beschrieb Oscar Hertwig 1876–1878 bei einem Seeigel: Er verwendete Osmiumtetroxid zur Fixierung der mikroskopischen Präparate und Borax-Karmin zur spezifischen Anfärbung der Zellkerne. So entdeckte er beim Vergleich aufeinanderfolgender Stadien, dass das Spermium mit dem Kopf in die Eizelle eintritt und seinen Kern freisetzt, woraufhin sich beide Kerne aufeinander zu bewegen und vereinigen. Die Kernteilung (Mitose), bei der die ebenfalls mit Karmin anfärbbaren Chromosomen erscheinen und gleichmäßig auf die beiden Tochterzellen verteilt werden, hatte bereits 1873 Friedrich Anton Schneider beschrieben.

Eine Bedeutung oder Funktion konnte man diesen Vorgängen jedoch nicht zuschreiben. Zwar hatte Gregor Mendel die Ergebnisse seiner Kreuzungsversuche, bei denen er die später nach ihm benannten Vererbungsregeln herausgearbeitet hatte, schon 1866 veröffentlicht, aber er fand dafür zu seinen Lebzeiten kein Verständnis. Es dauerte bis ins Jahr 1900, dass Hugo de Vries, Carl Correns und Erich Tschermak, nachdem sie selbst – zeitgleich, aber unabhängig voneinander – entsprechende Ergebnisse erhalten hatten, auf diesen Vorarbeiter aufmerksam wurden. Inzwischen war auch das Verhalten der Chromosomen bei der Meiose besser bekannt, und 1902 wies Walter Sutton darauf hin, dass das paarweise Auftreten gleichgestalteter Chromosomen etwas mit den ebenfalls paarweise vorhandenen Merkmalen in den Arbeiten Mendels und seiner Wiederentdecker zu tun haben könnte, was schließlich Theodor Boveri 1904 explizit als Chromosomentheorie der Vererbung formulierte.

Boveri, Correns und Andere nahmen jedoch an, dass der Zellkern bzw. die Chromosomen nur eine eher untergeordnete Funktion bei der Vererbung hätten und das Zytoplasma die Hauptrolle spiele. Dagegen erarbeiten Thomas Hunt Morgan und Hermann Joseph Muller aufgrund ihrer Untersuchungen über gemeinsam vererbte Merkmale (Genkopplung) Genkarten, auf denen eine bestimmte Anordnung von Genen auf einem Chromosom verzeichnet war, und formulierten in den 1920er Jahren die Theorie, dass die Gene grundsätzlich auf den Chromosomen lokalisiert seien und das Zytoplasma nur eine sekundäre Rolle spiele. Die Genkarten basierten darauf, dass Koppelungsgruppen getrennt werden können, und man nahm an, dass dies umso häufiger geschieht, je weiter die betreffenden Gene auf dem Chromosom voneinander entfernt sind. Der dem Koppelungsbruch zugrunde liegende Vorgang des Crossing-over wurde 1930/31 durch Barbara McClintock und Harriet B. Creighton aufgeklärt.

Noch war allerdings keineswegs klar, welche Bestandteile der Chromosomen die materiellen Träger der Erbinformation sind. Man wusste (Richard Altmann 1889), dass die Chromosomen basische Proteine und „Nucleinsäure“ enthalten. Letztere schien als Erbmaterial kaum in Frage zu kommen, weil sie nur aus Zucker, Phosphat und fünf verschiedenen Nukleinbasen besteht, während bei den Proteinen eine immer größere Anzahl von Komponenten (Aminosäuren) entdeckt wurde. Erst 1953 wurde durch das Doppelhelix-Strukturmodell von James Watson und Francis Crick klar, dass die Desoxyribonukleinsäure (DNA) tatsächlich eine sehr komplexe Struktur hat.

Die geschlechtliche oder sexuelle Fortpflanzung ist dadurch gekennzeichnet, dass im Wechsel Zellkerne miteinander verschmelzen (Karyogamie), wobei die Zahl der Chromosomen sich verdoppelt, und bei einer besonderen Form der Kernteilung, der Meiose, die Chromosomenzahl wieder halbiert wird. Dieser Kernphasenwechsel führt dazu, dass die Chromosomen und damit die auf ihnen befindlichen Gene von Generation zu Generation neu kombiniert werden (Rekombination).

Die meisten Eukaryoten (Lebewesen mit Zellkernen) pflanzen sich zumindest gelegentlich auf sexuelle Weise fort. Beim Menschen und bei höher organisierten Tieren ist sie die einzige Form der Fortpflanzung, während bei anderen Eukaryoten auch eine asexuelle Fortpflanzung auftritt. Im häufigsten Fall sind (wie beim Menschen) zwei Geschlechter vorhanden, die unterschiedliche Geschlechtszellen (Gameten) bilden, welche sich bei der Befruchtung vereinigen. Vielfach besitzen Lebewesen Geschlechtsorgane beiderlei Geschlechts, was als Hermaphroditismus bezeichnet wird. Davon zu unterscheiden ist die Intersexualität, die Ausprägung von Merkmalen beider Geschlechter, die zumeist mit Unfruchtbarkeit verbunden ist.

Wo keine Geschlechtsunterschiede bestehen, wie bei Pilzen und bei vielen Algen, spricht man von Paarungstypen. Von diesen können auch mehr als zwei vorhanden sein. Eine abgeleitete Sonderform, bei der aus unbefruchteten Eizellen Nachkommen hervorgehen, ist die eingeschlechtliche oder unisexuelle Fortpflanzung. Sie wird bei Tieren als Parthenogenese, bei Pflanzen als Apomixis bezeichnet und kann im Wechsel mit der zweigeschlechtlichen Fortpflanzung stattfinden, wie etwa bei Blattläusen, oder die einzige Form der Fortpflanzung sein, wie bei den meisten Löwenzahn-Arten.

Bei der ungeschlechtlichen oder asexuellen Fortpflanzung findet keine Befruchtung und keine Meiose statt; der Ploidiegrad bleibt unverändert. Sie tritt bei Pflanzen, Algen, Pilzen und Einzellern sehr häufig auf und ist bei diesen Organismen oft die primäre Form der Vermehrung, etwa durch Sporen. Im Tierreich kommt sie nur bei relativ einfach organisierten Vertretern vor, so bei Würmern, Polypen und Manteltieren.



</doc>
<doc id="11139" url="https://de.wikipedia.org/wiki?curid=11139" title="John von Neumann">
John von Neumann

John von Neumann (* 28. Dezember 1903 in Budapest, Österreich-Ungarn als "János Lajos Neumann von Margitta"; † 8. Februar 1957 in Washington, D.C.) war ein ungarisch-US-amerikanischer Mathematiker. Er leistete bedeutende Beiträge zur mathematischen Logik, Funktionalanalysis, Quantenmechanik und Spieltheorie und gilt als einer der Väter der Informatik. Später veröffentlichte er als Johann von Neumann; heutzutage ist er vor allem unter seinem in den USA gewählten Namen John von Neumann bekannt.

János Neumann entstammte einer jüdischen Bankiersfamilie. Sein Vater, der königlich ungarische Regierungsrat Max Neumann, wurde am 1. Juli 1913 in den ungarischen Adelsstand erhoben. Schon als Kind zeigte John Neumann jene überdurchschnittliche Intelligenz, die später selbst Nobelpreisträger – zum Beispiel Eugene Paul Wigner – zum Staunen brachte. Als Sechsjähriger konnte er mit hoher Geschwindigkeit achtstellige Zahlen im Kopf dividieren. Er besaß ein außergewöhnliches Gedächtnis, das ihm beispielsweise erlaubte, den Inhalt einer Buchseite nach einem kurzen Blick darauf präzise wiederzugeben. Später konnte er ganze Bücher wie Goethes Faust auswendig und so zum Beispiel auch durch detailliertes historisches Wissen glänzen. Er besuchte in Budapest das humanistische deutschsprachige Lutheraner-Gymnasium, wie auch gleichzeitig Eugene Paul Wigner. Schon als Gymnasiast glänzte er durch mathematische Leistungen und veröffentlichte mit 17 Jahren seinen ersten mathematischen Artikel. Dem Wunsch seiner Eltern folgend, studierte er jedoch zunächst von 1921 bis 1923 Chemieingenieurwesen in Berlin und dann bis zu seinem Diplom an der ETH Zürich. Sein eigentliches Interesse galt allerdings immer der Mathematik, der er sich gewissermaßen als „Hobby“ widmete. Er besuchte, da er in Budapest zunächst nicht zugelassen wurde (Kontingentierung für Juden), Mathematikkurse von Hermann Weyl und George Pólya an der ETH und machte schon bald auf sich aufmerksam. Von Neumann war von 1928 bis 1933 (jüngster) Privatdozent der Berliner Universität und im Sommersemester 1929 an der Universität Hamburg. Davor arbeitete er 1926/1927 in Göttingen mit David Hilbert zusammen.
Am Anfang seiner Karriere als Mathematiker beschäftigte sich von Neumann unter anderem mit der Entwicklung der axiomatischen Mengenlehre, für die er noch als Student einen neuen Ansatz fand (Dissertation in Budapest 1926 bei Leopold Fejér), und mit der Hilbertschen Beweistheorie. Diese Themen waren damals das aktuelle Forschungsgebiet der Gruppe um Hilbert in Göttingen, damals eines der Weltzentren der Mathematik. Seine Definition der Ordinalzahlen ist heute ein Standard: Eine neue Ordinalzahl wird durch die Menge der bereits eingeführten definiert. Die Phase seiner Beschäftigung mit mathematischer Logik endete mit dem Bekanntwerden von Gödels Unvollständigkeitssatz, der Hilberts Programm einen schweren Schlag versetzte. Gödel war später ein enger Freund und Kollege von Neumann und Albert Einstein in Princeton.

Von Neumann war ebenfalls Verfasser des ersten mathematisch durchdachten Buches zur Quantenmechanik, in dem er den Messprozess und die Thermodynamik der Quantenmechanik behandelte (siehe dazu Dichtematrix, von ihm 1927 eingeführt, von-Neumann-Entropie). Das damals „heiße“ Thema der sich stürmisch entwickelnden Quantenmechanik war auch der Hauptgrund, warum er sich der Funktionalanalysis zuwandte und die Theorie linearer Operatoren in Hilberträumen entwickelte, genauer die der "unbeschränkten selbstadjungierten" Operatoren. Die Mathematiker in Göttingen wandten gegen die neue Quantenmechanik ein, dass mit den bis dahin untersuchten linearen beschränkten Operatoren die kanonischen Vertauschungsrelationen nicht zu erfüllen waren. Von Neumann klärte das und lieferte gleichzeitig zahlreiche weitere Beiträge zu diesem Gebiet. Als man allerdings später Werner Heisenberg fragte, ob er von Neumann deswegen nicht dankbar sei, stellte er nur die Gegenfrage, wo denn der Unterschied zwischen beschränkt und unbeschränkt liege. Von Neumanns Buch über Quantenmechanik genoss einen derartigen Ruf, dass selbst sein „Beweis“ der Unmöglichkeit von Hidden-Variable-Theorien, der zwar korrekt war, aber von falschen Voraussetzungen ausging, lange nicht hinterfragt wurde. Die Physiker bevorzugten jedoch zu von Neumanns Leidwesen die fast gleichzeitig veröffentlichten "Principles of Quantum mechanics" von Paul Dirac, in der das angesprochene mathematische Problem durch Einführung von Distributionen umgangen wurde, die bei den Mathematikern zunächst verpönt waren, ehe sie auch dort Ende der 1940er Jahre ihren Siegeszug antraten (Laurent Schwartz).

Mit Eugene Wigner veröffentlichte von Neumann 1928/29 eine Reihe von Arbeiten über die Anwendung der Gruppentheorie in den Atomspektren. Auch hier war die Begeisterung der Physiker gedämpft, es wurde sogar von „Gruppenpest“ gesprochen, die sich von Seiten der Mathematiker in der Quantenmechanik breitzumachen versuchte.

Das Stone-von Neumann-Theorem drückt die Eindeutigkeit der "kanonischen Kommutatoren" von zum Beispiel Orts- und Impulsoperatoren in der Quantenmechanik aus und zeigt die Äquivalenz von deren beiden grundlegenden Formulierungen von Schrödinger (Wellenfunktion) und Heisenberg (Matrizen).

Seine Arbeiten über Quantenmechanik begründeten seinen Ruf in Amerika – und nicht zuletzt im Hinblick auf einen Wechsel auf besser bezahlte Positionen in den USA hat er sich so intensiv mit ihr beschäftigt. Im Herbst 1929 wurde er von Oswald Veblen eingeladen, an die Princeton University in New Jersey zu kommen und Vorträge darüber zu halten, und er wechselte auch in den folgenden Jahren zwischen Princeton und Deutschland. Ab 1933 wirkte er am neu gegründeten, anspruchsvollen Institute for Advanced Study in Princeton als Professor für Mathematik. Einige seiner Kollegen dort waren Albert Einstein und Hermann Weyl. Wie diese emigrierte auch von Neumann nach der Machtergreifung Hitlers dauerhaft in die USA.

John von Neumann erbrachte auf vielen Gebieten der Mathematik herausragende Beiträge. Schon 1928 hatte ihn ein Aufsatz des Mathematikers Émile Borel über Minimax-Eigenschaften zu Ideen geführt, die später auf einen seiner originellsten Entwürfe hinausliefen, die Spieltheorie. Von Neumann bewies 1928 das Min-Max-Theorem für die Existenz einer optimalen Strategie in „Nullsummenspielen“. Mit dem Wirtschaftswissenschaftler Oskar Morgenstern schrieb er 1944 das zum Klassiker gewordene Buch "The Theory of Games and Economic Behavior" (3. Auflage 1953), wo auch die für die Ökonomie wichtige Verallgemeinerung auf n-Personen Spiele behandelt wird. Er wurde damit zum Begründer der Spieltheorie, die er allerdings weniger auf klassische Spiele anwendet, als auf alltägliche Konflikt- und Entscheidungssituationen bei unvollkommener Kenntnis der Absichten des Gegenspielers (wie beim Pokern). In den Wirtschaftswissenschaften wird auch ein Seminarvortrag von 1936 zur mathematischen Modellierung expandierender Wirtschaften häufig zitiert. In der zweiten Auflage von "The Theory of Games and Economic Behavior" (1947) präsentierten Morgenstern und von Neumann den Von-Neumann-Morgenstern-Erwartungsnutzen und leisteten damit bedeutende Beiträge zur Nutzentheorie.

In den 1930er Jahren entwickelte von Neumann in einer Serie von Arbeiten mit Francis Murray eine Theorie von Algebren beschränkter Operatoren in Hilberträumen, die Jacques Dixmier später von-Neumann-Algebren nannte. Diese sind heute ein aktuelles Forschungsgebiet (zum Beispiel Alain Connes, Vaughan F. R. Jones), das auch – wie von Neumann vorhersah – Anwendungen in der Physik hat, allerdings weniger in der Quantenmechanik als in der Quantenfeldtheorie und Quantenstatistik. Von Neumann und Murray bewiesen ein Klassifikationstheorem für Operatoralgebren als direkte Summe von „Faktoren“ (mit trivialem Zentrum) vom Typ I, II, III, jeweils mit Unterteilungen.

Operatoralgebren waren Teil seiner Suche nach einer Verallgemeinerung des quantenmechanischen Formalismus, denn er sagte in einem Brief an Birkhoff 1935, er würde nicht mehr an Hilberträume glauben. Weitere Versuche in dieser Richtung waren die Untersuchung der „lattice theory“ (Theorie der Verbände), zunächst als Algebra von Projektionsoperatoren im Hilbertraum (an der auch Birkhoff beteiligt war), später als Erweiterung der Logik zur „Quantenlogik“ interpretiert, und kontinuierliche Geometrien, die sich aber am Ende als kein Fortschritt gegenüber Operatoralgebren erwiesen.

Ein weiteres Arbeitsfeld der 1930er Jahre in Princeton war das berühmte Ergodenproblem, bei dem es um die mathematische Grundlegung der statistischen Mechanik in klassischen Systemen geht (Gleichverteilung der Bahnen im Phasenraum). Von Neumann hatte in Deutschland diese Fragen schon von quantenmechanischer Seite behandelt. Nachdem Bernard Koopman das Problem in Operator-Form gebracht hatte, griff von Neumann es auf und lieferte sich unfreiwillig ein „Duell“ mit dem bekannten amerikanischen Mathematiker George David Birkhoff. Wie er später sagte, hätte er eine Zusammenarbeit vorgezogen.

Von Neumann arbeitete ab 1943 am Manhattan-Projekt in Los Alamos. Er war schon in den Jahren zuvor bei der Army und Navy ein gefragter Berater, etwa für Ballistikfragen, Hohlladungen, operations research, Bekämpfung deutscher Magnetminen oder Optimierung der Wirkung von Bomben mit „schrägen Stoßwellen“. Eines seiner Hauptarbeitsgebiete war die Theorie der Stoßwellen, die in den 50er Jahren für den Überschallflug aktuell wurde und die er unter anderem für die Entwicklung von Sprengstofflinsen für den Implosionsmechanismus der Plutoniumbombe nutzte. In diesen Zusammenhang gehört auch seine Entwicklung des ersten numerischen Verfahrens zur Lösung von hyperbolischen partiellen Differentialgleichungen, des Monte-Carlo-Verfahrens mit Stanislaw Ulam, die von-Neumann-Stabilitätsanalyse sowie seine Pionierleistungen in der Rechnerarchitektur. Übrigens optimierte er mit seiner Expertise in der Theorie der Stoßwellen während des Zweiten Weltkriegs auch britische Luftminen über Deutschland. Auch an der Weiterentwicklung des amerikanischen Nuklearbomben-Programms bis hin zur Wasserstoffbombe war von Neumann beteiligt.

Von Neumann war einerseits geschätzt, weil er seine Ideen freigiebig weitergab und Kollegen weiterhalf (bei Besuchen in Los Alamos war er oft von einer Traube von Wissenschaftlern umgeben, die schnellen Rat wollten), andererseits gefürchtet, da er Ideen schnell aufgriff und mit atemberaubender Geschwindigkeit eigene Theorien daraus entwickelte.

Neben seinen mathematischen Leistungen war von Neumann als Regierungsberater auch politisch einflussreich. Vor dem Abwurf der Atombomben auf Japan war er ein Mitglied des Target Committee, das die genauen Ziele der Bomben mitbestimmte. Er berechnete dabei auch die optimale Detonationshöhe der Atombomben, um einen möglichst großen Schaden durch die Explosion am Boden zu erzielen. Mit dem Namen John von Neumann ist angeblich auch die Idee verbunden, die Ost-West-Konfrontation durch die Explosion einer Wasserstoffbombe über unbewohntem sowjetischem Gebiet zu beenden, die Sowjetunion von der Entwicklung einer eigenen Bombe abzuhalten und dauerhaft einzuschüchtern. Ob US-Präsident Eisenhower allerdings tatsächlich durch von Neumann zu einem solchen Schritt gedrängt wurde, ist umstritten. Er war aber wesentlich daran beteiligt, das militärische Raketenprogramm der USA auf den Weg zu bringen.

Von Neumann gilt als einer der Väter der Informatik. Nach ihm wurde die Von-Neumann-Architektur (auch "Von-Neumann-Rechner") benannt, ein Computer, in dem Daten und Programm binär codiert im selben Speicher liegen. Das Programm selbst kann somit im laufenden Rechenvorgang verändert werden und durch bedingte Sprungbefehle von der festgelegten Reihenfolge der gespeicherten Anweisungen abgewichen werden. Es definiert in loser Analogie zum menschlichen Hirn (wie er im Report schreibt) eine Rechnerarchitektur aus Steuereinheit und arithmetischer Einheit sowie eine Speichereinheit. Die Befehle werden seriell abgearbeitet. Er beschrieb dieses Prinzip 1945 im "First Draft of a Report on the EDVAC". Der Bericht war als Diskussionsbericht mit der ENIAC-Gruppe gedacht und blieb zunächst unveröffentlicht, kursierte jedoch schnell in wissenschaftlichen Kreisen. So gut wie alle modernen Rechner beruhen auf von Neumanns Idee.

Von Neumanns Rolle als alleiniger Erfinder der nach ihm benannten modernen Rechnerarchitektur ist bestritten worden und seit längerem Gegenstand von Auseinandersetzungen. Heutzutage wird deshalb vorzugsweise statt „Von-Neumann-Rechner“ die Bezeichnung „speicherprogrammierter Rechner“ (stored program computer) verwendet. Insbesondere betrifft das die Ansprüche der eigentlichen Erbauer des ersten Röhrencomputers ENIAC und dessen Nachfolgemodells EDVAC, John Presper Eckert und John William Mauchly von der Moore School der University of Pennsylvania in Philadelphia, mit denen von Neumann und Herman Goldstine anfangs eng zusammenarbeiteten. Von Neumann stieß durch eine zufällige Begegnung auf einem Bahnsteig mit dem ihm zuvor nicht bekannten Mathematiker Goldstine im August 1944 zu den Computerentwicklern der Moore School, wo Goldstine Verbindungsoffizier der US-Army war. Wie Goldstine berichtete, beendete die von ihm selbst betriebene freizügige Verbreitung des Edvac-Reports die enge Beziehung von ihm und von Neumann zu Eckert und Mauchly, die ihren Beitrag in dem (eigentlich nicht für die Öffentlichkeit bestimmten) Edvac-Report nicht gewürdigt sahen und für wesentliche Teile des "Von-Neumann-Rechners" Prioritätsansprüche geltend machten. Bei Eckert und Mauchly standen Patentüberlegungen im Vordergrund, die dazu führten, dass sie schon 1946 die Moore School verließen, um eine eigene Firma zu gründen, und die später zu einem jahrzehntelangen Streit vor Gericht führten (sie schalteten schon 1945 Patentanwälte ein). Von Neumann sah dagegen zunächst Bedarf für weitere Forschung und Entwicklung und trat für eine offene Diskussion und weite Verbreitung der Ergebnisse ein. Teile des Konzepts wurden unabhängig auch von anderen Computerpionieren – darunter Konrad Zuse in Deutschland – entwickelt, u. a. die Idee der Trennung von Speicher und Prozessor, die schon in Zuses noch rein mechanischen Z1 im Jahr 1938 erfolgte. Zuses frühen Rechnern, die für Spezialaufgaben ausgelegt waren, fehlte jedoch das wesentliche Konzept der bedingten Verzweigung, obwohl es ihm bekannt war und er es in seinem Plankalkül verwendete. Von Neumann setzte sich seinerzeit vehement für die weitere Entwicklung der Rechenmaschinen ein. Die Verdienste von Neumanns beruhen insbesondere auf der Mathematisierung und Verwissenschaftlichung der Rechenmaschinen.
Von Neumann leitete ab 1949 am Institute for Advanced Study schließlich ein eigenes Computerprojekt, den IAS-Computer, in dem er seine Ideen verwirklichen konnte, darunter auch viele Programmierkonzepte. Auf ihn gehen Unterprogramme mit Parameterübergabe über einen Verweis auf eine Speicherstelle, verschiedene Verfahren zur Erzeugung von Zufallszahlen (unter anderem die Mittquadratmethode und die Verwerfungsmethode) und der Mergesort zurück. Er trug maßgeblich zur Verwendung von Binärcodes in den Rechnersystemen bei und propagierte die Verwendung von Flussdiagrammen, in denen er auch eine Art von Assertions vorsah, die als Vorläufer für Schleifeninvarianten im Hoare-Kalkül angesehen werden können. Ein enger Mitarbeiter wurde Goldstine, den er aus der ENIAC-Gruppe übernahm. Auch die Reports aus Princeton ab 1949 ließ er frei zirkulieren, und schon bald entstanden überall in den USA und England Rechner nach diesen Vorbildern. Genutzt wurde der IAS-Rechner und der nach von Neumanns Ideen umgebaute ENIAC vor allem für militärische Berechnungen (Ballistik). Von Neumann nutzte den Princeton-Rechner allerdings auch für Pionierarbeiten in der numerischen Wettervorhersage, wie die erste rechnergestützte 24-Stunden-Wetterprognose.

1953 entwickelte er auch die Theorie selbstreproduzierender Automaten (von Burks 1966 als "Theory of self reproducing automata" herausgegeben), für die er ein kompliziertes Beispiel angab (heute ergeben sich viel einfachere aus der Theorie der zellulären Automaten, zum Beispiel John Horton Conways Spiel des Lebens). Ideen dafür soll er auch beim Spielen mit einem Bauklötzchen-Spiel (Tinkertoy) ausprobiert haben. Science-Fiction-Autoren stellten sich die Besiedlung unserer Galaxie mit solchen Automaten vor und prägten dafür den Namen Von-Neumann-Sonden. Von Neumanns zellulare Automaten bilden eine wichtige Grundlage für die Forschungsdisziplin "Artificial life" und ermöglicht die Simulation biologischer Organisation, Selbstreproduktion und Evolution von Komplexität.

Über von Neumann kursierten zahlreiche Anekdoten (einige hat Halmos in dem in der Literatur zitierten Artikel gesammelt). Beispielsweise versuchte jemand ihn durch folgendes Rätsel zu testen: „Die Endpunkte einer Strecke s bewegen sich mit der Geschwindigkeit v aufeinander zu, ein Läufer flitzt zwischen den beiden Endpunkten mit einer Geschwindigkeit w > v hin und her. Welche Strecke legt er zurück?“ Es gibt eine einfache und eine etwas kompliziertere Lösungsmethode (Summation der Teilstrecken). Von Neumann gab die Antwort blitzschnell und erklärte auf Nachfrage, die Reihe summiert zu haben – er hatte also den komplizierten Weg gewählt, was für ihn jedoch keinen höheren Zeitaufwand bedeutete.

Wegen seiner Fähigkeit, komplexe Sachverhalte schnell in einfache Fragestellungen zu zergliedern und oft aus dem Stand einer Lösung zuzuführen, sowie seiner streng sachbezogenen, jeden unnötigen Streit vermeidenden Haltung wurde von Neumann gerne als technischer Berater engagiert; so von IBM, Standard Oil oder der RAND Corporation. Er veröffentlichte 1952 das Von-Neumann-Gesetz, das die zeitliche Änderung der Größe von Zellen zweidimensionalen Schaumes beschreibt. Für Standard Oil half er Methoden zu entwickeln, Öl-Lagerstätten besser auszunutzen. Sein Tod verhinderte eine geplante größere Zusammenarbeit mit IBM. Für die RAND Corporation wandte er die Spieltheorie auf strategische Denkspiele an, wie auch gleichzeitig andere Mathematiker wie John Nash und John Milnor. In einer unveröffentlichten Arbeit 1953 legte er auch die Prinzipien des Halbleiterlasers dar.

John von Neumann war ein lebenslustiger und geselliger Mensch (Spitzname „Good Time Johnny“); er war zweimal verheiratet - mit Marietta Kövesi und Klára Dán - und hatte eine Tochter (Marina). Sein Haus in Princeton war Mittelpunkt der akademischen Kreise auf den legendären Princeton-Partys. Von Neumann liebte auch schnelle Wagen wie Cadillac oder Studebaker, sein Fahrstil war aber gefürchtet, da er sich bei ruhigem Verkehr schnell langweilte und dann in Geistesabwesenheit verfiel. Auch mitten aus einer Party konnte er sich plötzlich verabschieden, um ein mathematisches Problem zu durchdenken. Sein Alkoholkonsum war teilweise nur vorgetäuscht, wie das Kind eines Gastes einmal überrascht feststellte. Ein weiterer Aspekt des „Unterhaltungskünstlers“ von Neumann war sein unerschöpfliches Reservoir oft schlüpfriger Witze und seine Vorliebe für Limericks.

Von Neumann starb nach einem qualvollen Krebsleiden, das möglicherweise durch seine Teilnahme an Nukleartests verursacht worden war, im Washingtoner Walter-Reed-Militärkrankenhaus. Ein Soldat hielt vor dem Zimmer Wache, damit er im Delirium - der Krebs griff am Ende auch sein Gehirn an - keine Staatsgeheimnisse preisgab. Noch auf dem Totenbett schrieb er an seinem Buch „Die Rechenmaschine und das Gehirn“, in dem er den Besonderheiten des „Computers“ im menschlichen Kopf nachging. 


Nach Neumann ist die John-von-Neumann-Medaille der IEEE, der John-von-Neumann-Theorie-Preis in Operations Research, die John von Neumann Lecture der SIAM sowie der Von-Neumann-Mondkrater benannt.
Die Institute für Informatik und Mathematik der Humboldt-Universität zu Berlin sitzen im Johann von Neumann-Haus.

Von Neumann in einer Diskussion mit Jacob Bronowski 1943 beim Studium von Bombenkratern auf Luftbildern:

Bronowski berichtet, dass er auf diesen Rat hin das besprochene Problem neu durchdachte und spät in der Nacht von Neumanns Sicht bestätigt fand – als er ihm dies am nächsten Morgen mitteilte, bat ihn von Neumann nur, ihn doch bitte das nächste Mal zu so einer für von Neumann frühen Stunde nur zu stören, falls er falsch läge, und nicht falls er recht habe.


Einige Aufsätze und Bücher online:

Einige in Los Alamos entstandene Arbeiten von Neumanns (zum Beispiel über Schockwellen, Detonationswellen) sind bei der Federation of American Scientists online verfügbar.

Einige weitere Arbeiten zum Beispiel zu kontinuierlichen Geometrien, Operatorenringen oder zur Ergodentheorie sind bei der National Academy of Sciences online verfügbar.







</doc>
<doc id="11141" url="https://de.wikipedia.org/wiki?curid=11141" title="Erwin Schrödinger">
Erwin Schrödinger

Erwin Rudolf Josef Alexander Schrödinger (* 12. August 1887 in Wien-Erdberg; † 4. Jänner 1961 in Wien-Alsergrund) war ein österreichischer Physiker und Wissenschaftstheoretiker.

Schrödinger gilt als einer der Begründer der Quantenmechanik und erhielt für die "Entdeckung neuer produktiver Formen der Atomtheorie" gemeinsam mit Paul Dirac 1933 den Nobelpreis für Physik.

Erwin Schrödingers Vater Rudolf Schrödinger (1857–1919) war Wachstuchfabrikant und Botaniker. Seine Mutter Georgine Emilia Brenda (1867–1921) war die Tochter von Alexander Bauer, dem Professor für Allgemeine Chemie an der k. k. Technischen Hochschule in Wien. Sein Vater war katholisch, seine Mutter evangelisch-lutherisch. Die Kinder wurden in der evangelischen Konfession erzogen. Schrödinger ging 1898 auf das Akademische Gymnasium. Danach studierte er von 1906 bis 1910 in Wien Mathematik und Physik und habilitierte sich am Wiener Physikalischen Institut. Dort arbeitete er unter anderem mit Franz-Serafin Exner, Friedrich Hasenöhrl und K. W. F. Kohlrausch zusammen. Er war während seines Studiums eng befreundet mit dem Botaniker Franz Frimmel.

Nach seiner Kriegsteilnahme am Ersten Weltkrieg folgte er Berufungen nach Jena (1920), Stuttgart (1920), Breslau (1921) und Zürich (1922). In Zürich vertrat er den Lehrstuhl für Theoretische Physik, den vor ihm bereits Albert Einstein und Max von Laue innehatten. Hier formulierte er auch die nach ihm benannte Schrödingergleichung, die er Ende 1925 während eines Ferienaufenthalts in Arosa entdeckt hatte. Damit begründete er die Wellenmechanik als Beschreibung der Quantenmechanik.

Am 6. April 1920 heiratete er Annemarie Bertel, genannt Annie. Die Ehe blieb kinderlos. Schrödinger und seine Frau Annie lebten in offener Beziehung – Schrödinger hatte offen außereheliche Beziehungen, zum Beispiel zur Frau seines Kollegen und Freundes Arthur March, und Annie hatte eine langjährige Beziehung zu Hermann Weyl, was die Freundschaft von Weyl und Schrödinger nicht störte. Mit Hildegunde March hatte er eine Tochter (Ruth Braunizer, * 1934), die beide von 1939 bis 1945 bei Schrödinger in Dublin lebten.

1927 ging Schrödinger nach Berlin, wo er die Nachfolge von Max Planck an der Friedrich-Wilhelms-Universität antrat. Zahlreiche Physiker von Weltrang versammelten sich in jenen Jahren in Berlin. Dort arbeitete er u. a. mit Victor Weisskopf zusammen. Nach der Machtergreifung der Nationalsozialisten 1933 entschloss sich Schrödinger, der schon zuvor in bemerkenswerter Deutlichkeit seine Ablehnung des Nationalsozialismus zum Ausdruck gebracht hatte, Deutschland zu verlassen und eine Stelle am Magdalen College in Oxford anzunehmen. Im selben Jahr wurde ihm der Nobelpreis für Physik verliehen.

1936 kehrte er nach Österreich zurück, um in Graz an der Karl-Franzens-Universität eine Berufung anzunehmen. Sein Verhalten während des Anschlusses 1938 ist widersprüchlich: Obwohl er bereits in Berlin als NS-Gegner hervorgetreten war, ging er zunächst davon aus, seine Grazer Professur behalten zu können und veröffentlichte am 31. März 1938 in der „Grazer Tagespost“ einen Aufsatz mit dem Titel "Die Hand jedem Willigen. Bekenntnis zum Führer – Ein hervorragender Wissenschaftler meldet sich zum Dienst für Volk und Heimat". Die Sommerferien 1938 verbrachte Schrödinger, der sich offenbar sicher fühlte, in den Dolomiten, wo er unter anderem mit Max Planck zusammentraf. In einer Notiz der neuen nationalsozialistischen Universitätsführung wurde Schrödinger als „fachlich hervorragend“, „im persönlichen Verhalten widersprüchlich“ und politisch „semitophil“ bezeichnet; seine Professur wurde während der Ferien 1938 ohne Wissen des zuständigen Dekans Karl Polheim vom Ministerium neu ausgeschrieben. Am 26. August wurde er schließlich wegen „politischer Unzuverlässigkeit“ entlassen und reiste am 14. September 1938 per Bahn nach Rom aus.

Schrödinger ging nach Dublin, wo er ab 1940 wirkte und Direktor der Schule für Theoretische Physik des Dublin Institute for Advanced Studies war. 1943 gab er am dortigen Trinity College seine berühmten „Schrödinger lectures“. 1949 wurde er korrespondierendes Mitglied der Bayerischen Akademie der Wissenschaften und auswärtiges Mitglied der Royal Society.

1956 kehrte er nach Wien zurück. Hier lehrte er bis zu seinem Tod am Institut für Theoretische Physik der Universität Wien. Schrödinger nahm auch an den Hochschultagen in Alpbach teil. Da es ihm im Ort gefiel, verbrachte er hier seine letzten Jahre. Seine Tochter Ruth Braunizer lebt heute noch in dem Tiroler Dorf. Erwin Schrödinger starb am 4. Jänner 1961 in Wien an Tuberkulose. Er wurde seinem Wunsch entsprechend in Alpbach in Tirol beerdigt. Als Inschrift trägt das Grabkreuz die Gleichung, die seinen Namen trägt.

1920 wurde Erwin Schrödinger mit dem Haitinger-Preis der Akademie der Wissenschaften in Wien ausgezeichnet.

1926 formulierte Schrödinger die nach ihm benannte Schrödingergleichung. Der Zugang zur Quantenmechanik, den Schrödinger mit Hilfe dieser partiellen Differentialgleichung fand, kam etwas später als Heisenbergs Matrizenmechanik, hat aber den Vorteil, dass er die aus der klassischen Mechanik bekannte Mathematik benutzt. Diese Arbeiten brachten ihm Weltruhm und schließlich auch den Nobelpreis für Physik im Jahr 1933 ein. In dieser berühmten Artikelserie (Annalen der Physik Bd. 79, S. 361, 489, 734, und Bd. 81, S. 109, 1926) bewies er auch gleich die Äquivalenz seiner Formulierung mit der Matrizenmechanik von Heisenberg und Born.

Die Auseinandersetzung mit den Arbeiten von Ernst Mach führten ihn zur Beschäftigung mit der Theorie der Farbwahrnehmung. Auf diesem Gebiet wurde er bald zum anerkannten Experten. Er untersuchte auch Farben-Räume mit speziellen Metriken und gab so wichtige theoretische Anregungen beispielsweise bei der Erarbeitung des späteren XYZ-Farbraumes der CIE. Die additive Farbmischung folgt den Regeln der Vektoraddition, deshalb führte Schrödinger die vektorielle Darstellung in die Farbmessung ein.

1937 wurde ihm die Max-Planck-Medaille verliehen.

Schrödinger nahm auch zu philosophischen Aspekten der Quantenmechanik Stellung. In seinem 1944 erschienenen Werk "Was ist Leben?" (im Original "What is Life?") führt er den Begriff der Negentropie ein. Sie hatte damals großen Einfluss auf Wissenschaftler wie Maurice Wilkins, Francis Crick und James D. Watson in der sich entwickelnden Molekularbiologie, indem sie versucht, biologische Themen physikalisch zu erklären, und das Interesse auf den damals unbekannten Mechanismus der Vererbung lenkte, für den er den Begriff des „aperiodischen Kristalls“ prägte, den er sich zum Zeitpunkt der Veröffentlichung noch als Protein vorstellte. Er war damals in Dublin relativ isoliert und kannte die frühe Forschung zum Beispiel von Oswald Avery zur Rolle der DNA und Max Delbrück zu Bakteriophagen in den USA nicht, sein auch stilistisch herausragendes Buch stellte aber in der Rückschau von Freeman Dyson zur richtigen Zeit die richtigen Fragen.

Sein wohl bekanntestes Gedankenexperiment ist Schrödingers Katze, womit er die kontraintuitiven Auswirkungen der Quantenmechanik auf Gegenstände des täglichen Lebens übertrug und so seine Ablehnung der üblichen statistischen Interpretation der Quantenmechanik zum Ausdruck bringen wollte.

Außerdem veröffentlichte er 50 weitere Publikationen zu verschiedenen Themen. In den letzten Lebensjahren beschäftigte er sich intensiv mit Verallgemeinerungen der Allgemeinen Relativitätstheorie („einheitliche Feldtheorien“), worüber er auch mit Albert Einstein korrespondierte – das Verhältnis kühlte aber ab, als Schrödinger seinen Enthusiasmus für seine Theorie auch in überzogenen Pressemitteilungen verlauten ließ.

1950 wurde Schrödinger in die American Academy of Arts and Sciences gewählt. 1956 wurde er in der Orden Pour le Mérite aufgenommen und wurde erster Preisträger des nach ihm benannten Erwin Schrödinger-Preises der Österreichischen Akademie der Wissenschaften.

Von der Republik Österreich erhielt er 1957 das Österreichische Ehrenzeichen für Wissenschaft und Kunst.







</doc>
<doc id="11143" url="https://de.wikipedia.org/wiki?curid=11143" title="Thomas Newcomen">
Thomas Newcomen

Thomas Newcomen (* 26. Februar 1663 in Dartmouth; † 5. August 1729 in London) war ein englischer Erfinder.

Newcomen war Schmied und Eisenwarenhändler und hatte einige große Bergwerksgesellschaften als Kunden. Durch das Vorstoßen dieser Bergwerke in immer größere Tiefen ergab sich die Notwendigkeit, effiziente Maschinen zum Abpumpen des eindringenden Grundwassers zu konstruieren.

In zehnjähriger Arbeit erfand Newcomen die atmosphärische Dampfmaschine zur Wasserhaltung in Bergwerken, die der Dampfmaschine von Thomas Savery deutlich überlegen war. Da aber Savery ein weitreichendes Patent auf seine Erfindung hatte, konnte Newcomen seine Maschine nicht patentieren lassen. Deshalb schloss er sich mit Savery zu einer Partnerschaft zusammen.

Seine Maschine nutzte eine Wassereinspritzung, um den Wasserdampf im Zylinder zu kühlen und damit kondensieren zu lassen. Dadurch entstand im Zylinderraum ein Unterdruck, so dass der von außen auf den Kolben wirkende Luftdruck bzw. der Normaldruck der Außenluft diesen wieder in den Zylinder hinein schob. Die bis dahin üblichen Maschinen warteten für die Kondensation einfach, bis der Volumeninhalt im Zylinderraum über das Material des Kolbens und den Zylinder als Wärmeleiter – durch die kältere Außenluft bedingt – von selbst wieder abkühlte – Newcomens Erfindung ermöglichte also deutlich höhere Kolbentakte.

Die erste Newcomen-Maschine wurde 1712 in einem Kohlebergwerk in Staffordshire installiert. Sie wirkte ohne Kurbelwelle und Schwungrad über einen Balancier auf die anzutreibenden Pumpen. Die Verbindung zwischen dem Kolben und dem Balancier wurde über eine Kette realisiert. Der Wirkungsgrad der Maschine lag bei nur 0,5 Prozent. Trotzdem wurden die Dampfmaschinen von Newcomen erst gegen Ende des 18. Jahrhunderts durch die Dampfmaschinen von James Watt verdrängt.

Zu Beginn wurden die Ventile zum Einlassen des Dampfes in den Zylinder und zum Einspritzen des Kühlwassers mit Hand von einem Knaben bedient. Einer dieser Knaben, Humphrey Potter, der auch an einer solchen Maschine zum Öffnen und Schließen der Hähne angestellt war, kam (1712 oder 1713) – wohl aus Bequemlichkeit – um sich die Arbeit zu erleichtern (und weil er mit den anderen Jungen spielen wollte), auf die Idee, die Betätigung der Ventile durch den Gang der Maschine selbst besorgen zu lassen. Er verband in geeigneter Weise den auf- und nieder gehenden Balancier(balken) durch Schnüre mit den Hähnen und hatte damit, ohne sich dessen selbst bewusst zu sein, die Erfindung der selbsttätigen Steuerung gemacht.




</doc>
<doc id="11146" url="https://de.wikipedia.org/wiki?curid=11146" title="Lehnwort">
Lehnwort

Ein Lehnwort ist ein Wort, das aus der Sprache (der "Geber-" oder "Quellsprache)" in die "Nehmersprache (Zielsprache)" übernommen "(entlehnt)" wurde. Die Gebersprache muss dabei nicht unbedingt auch die Ursprungssprache sein, sondern kann auch eine vermittelnde Sprache "(Vermittlersprache)" sein.
Ein Wort kann mehrmals, zu verschiedenen Zeiten und auch aus verschiedenen vermittelnden Gebersprachen in die Nehmersprache übernommen werden sowie in dieser dann auch in verschiedenen Bedeutungen, Lautungen oder Schreibungen auftreten. Der übergeordnete Vorgang, der zur Bildung von Lehnwörtern führt, wird Entlehnung genannt. Die Entlehnung stellt einen wichtigen Faktor im Sprachwandel dar und ist Gegenstand der Bezeichnungslehre (Onomasiologie).

Die Bestimmung der Herkunft von Wörtern ist Sache der Etymologie; mit den Motiven, Gründen und Auslösern von Entlehnungen – sowie ganz allgemein mit Bezeichnungswandel – beschäftigen sich die Onomasiologie und die Sprachwandelforschung.

Der Gegenbegriff zu Lehnwort ist Erbwort. Von einem Erbwort spricht man dann, wenn das Wort aus einer älteren oder der ältesten rekonstruierbaren Entwicklungsstufe der untersuchten Sprache stammt. Die Anwendung des Begriffs hängt allerdings vom Untersuchungszeitraum ab und setzt eine zureichende Kenntnis der Wortgeschichte voraus. So kann z. B. ein Wort wie "Pfalz" (Wohngebäude eines mittelalterlichen Fürsten), das sich aus dem Neuhochdeutschen über das Mittelhochdeutsche bis ins Althochdeutsche "(phalanza, phalinza)" zurückverfolgen lässt, gegenüber mittelhochdeutschen und neuhochdeutschen Entlehnungen aus anderen Sprachen als Erbwort erscheinen, obwohl es in voralthochdeutscher Zeit aus mittellateinisch "palantia" (wohl 7. Jh.; aus vulgärlat. "palātia," dem als Sing. aufgefassten Plur. von "palātium)" übernommen wurde und insofern im Deutschen nicht weniger ein Lehnwort ist als die vom gleichen lateinischen Wortstamm abstammenden, jüngeren Gallizismen "Palast" (12. Jh.; aus mhd. "pallas," entlehnt aus altfranzösisch "paleis)" oder "Palais" (17. Jh.; entlehnt aus neufranz.).

Von einem Lehnwort im engeren Sinn spricht man dann, wenn das übernommene Wort in seiner Flexion, Lautung und Schreibung an den Sprachgebrauch der Nehmersprache angepasst ist. Zu den Lehnwörtern im weiteren Sinn zählen auch die Fremdwörter, bei denen eine solche Anpassung nicht oder in geringerem Maße erfolgt und die fremde Herkunft des Wortes vergleichsweise deutlicher kenntlich bleibt. Der Übergang zwischen Lehnwörtern im engeren Sinn und Fremdwörtern ist fließend, eine eindeutige Abgrenzung oft nicht möglich. Ein klares Beispiel, das Dublettenpaar "Moneten" und "Münze," wäre jeweils als Fremdwort bzw. Lehnwort im engeren Sinn zu charakterisieren, da sie beide auf das gleiche lateinische Wort (Lexem), genauer gesagt den Plural "monētae" und Singular "monēta," zurückgehen.

Bei einem Lehnwort im engeren Sinn und einem Fremdwort wird der fremde Wortkörper mit seiner Bedeutung oder einem Teil dieser Bedeutung übernommen. Man spricht hierbei von lexikalischer Entlehnung. Hiervon abzugrenzen, wenn auch den Lehnwörtern in einem weiteren Sinn oft zugerechnet, ist die nur semantische Entlehnung oder Lehnprägung (französisch und englisch "calque)," bei der mit den sprachlichen Mitteln der Nehmersprache, aber ohne Übernahme des Lautkörpers, eine Bedeutung aus der Gebersprache übernommen wird, und zwar in Form einer Lehnbedeutung oder Lehnbildung.

Bei einer Lehnbedeutung wird die Bedeutung eines fremden Wortes übernommen und auf ein einheimisches Wort übertragen. Das gotische "daupjan" mit der Grundbedeutung ‚ein-, untertauchen‘ bekam unter dem Einfluss des kirchensprachlich griechischen "baptízein" die Bedeutung ‚jemanden durch Untertauchen zum Christen machen‘ (d. h. ‚taufen‘), und das deutsche Wort „schneiden“ erhielt von der englischen Redewendung "cut a person" die Zusatzbedeutung ‚jemanden absichtlich nicht kennen‘; siehe Begriffsübernahme.

Als Lehnbildung bezeichnet man die Bildung eines neuen Wortes im Rückgriff auf vorhandene Wörter oder Wortstämme der Nehmersprache. Der Unterschied zur Lehnbedeutung besteht darin, dass bei der Lehnbildung ein neues Wort oder eine neue Wortzusammensetzung entsteht. Man unterscheidet folgende Arten der Lehnbildung:

Einen Sonderfall bildet die Scheinentlehnung, bei der ein Wort oder Fremdwort aus Bestandteilen der Gebersprache neu gebildet wird, das in dieser Gebersprache selbst so nicht existiert oder eine andere Bedeutung hat, z. B. „Friseur“ (französisch "coiffeur)," „Handy“ (britisch "mobile phone," amerikanisch "cell phone)" und „Smoking“ (britisch "dinner jacket," amerikanisch "tuxedo)." Sofern dabei auf in der Nehmersprache bereits vorhandene Fremdwörter zurückgegriffen wird, kann man Scheinentlehnungen auch als Lehnprägungen (Lehnschöpfungen) einstufen.

Deutsche Wörter, die in einer anderen Sprache als Lehnwort oder Fremdwort integriert wurden, nennt man Germanismen. 
Eine umfangreiche Liste von Germanismen findet sich in der Liste deutscher Wörter in anderen Sprachen. Viele Germanismen sind dargestellt in


Der Deutsche Sprachrat sammelte 2006 in Zusammenarbeit mit der Gesellschaft für deutsche Sprache und dem Goethe-Institut in einer internationalen Ausschreibung „Ausgewanderte Wörter“ die interessantesten Beiträge weltweit. Eine Auswahl ist veröffentlicht in:

2007 / 2008 sammelte der Sprachrat in Zusammenarbeit mit dem Goethe-Institut vier Monate lang „Wörter mit Migrationshintergrund“, um das schönste „eingewanderte Wort“ im Deutschen zu finden. Eine Auswahl der Einsendungen ist veröffentlicht worden in:

Viele Wörter sind über den Umweg anderer Sprachen in die deutsche Sprache gelangt. Ein Beispiel ist die „Pistazie“, ursprünglich aus dem Mittelpersischen (vgl. mpers. "pstk," ausgesprochen als "pistag)," die durch Vermittlung des Griechischen, des Lateinischen und schließlich des Italienischen ins Deutsche gelangt ist.




































In vielen Untersuchungen werden sowohl Lehn- als auch Fremdwörter behandelt. Siehe daher Literatur unter Fremdwort und unter den oben angeführten Stichwörtern Gallizismus, Latinismus etc.




</doc>
<doc id="11148" url="https://de.wikipedia.org/wiki?curid=11148" title="Fritz Cockerell">
Fritz Cockerell

Fritz Cockerell (* 25. November 1889 in München; † 16. April 1965 ebenda) war ein deutscher Pionier des Motorrad-, Automobil-, und Motorenbaus.
Sein eigentlicher Name war Friedrich Gockerell, es taucht aber zumeist die Schreibweise mit C auf, auch in seinen Patent- und Offenlegungsschriften ist er als Fritz Cockerell bezeichnet.

Fritz Cockerell arbeitete zunächst als Maschinist in einem Luftschiff, später im Dampfturbinenbau bei Maffei. Danach wechselte er zu den Rapp-Motorenwerken, die später mit der Gustav Otto Maschinenfabrik als Bayerische Flugzeugwerke A.G. fusionierte und sich in Bayerische Motorenwerke A.G (BMW) umbenannte. Dort arbeitete Cockerell als Versuchsingenieur und war anschließend Werkstattleiter der mit Meixner und Landgraf gegründeten Deutschen-Megola-Werke G.m.b.H. (Hans Meixner, Friedrich Gockerell, Otto Landgraf) in München zur Herstellung des Megola Motorrades. Dieses zeichnete sich durch einen umlaufenden Fünfzylinder-Sternmotor im Vorderrad aus und wurde in einer kleinen Serie produziert (ungesicherten Angaben zufolge wurden 2000 Stück hergestellt). Die Megola wurde vom Solomon R. Guggenheim Museum 1998 in die temporäre Ausstellung "the art of the motorcycle" aufgenommen.

Cockerell betrieb die Cockerell Fahrzeugwerke und entwickelte in Eigenregie einen Achtzylinder-Zweitakt-Motor für einen dem "deutschen Volk bestimmten Sportwagen" und einen Vierzylinder-Zweitaktmotor, der dem Einbau in wenige Prototypen eines Wagens und Zweirades diente. Mehr Erfolg erzielte Cockerell mit Fahrradhilfsmotoren und Leichtkrafträdern, die als sehr zuverlässig galten und unter seinem eigenen Namen vertrieben wurden. Er widmete sich später Forschungsarbeiten zu Dieselmotoren für Flugzeuge, Turbinenmotoren und dem Wankelmotor. 

Nachlassverwaltung Cockerells: Deutsches Museum in München.





</doc>
<doc id="11150" url="https://de.wikipedia.org/wiki?curid=11150" title="Bildhauerei">
Bildhauerei

Der Begriff Bildhauerei umfasst das ganze Feld der Herstellung von Skulpturen und Plastiken in Kunst und Kunsthandwerk. Das Wort steht allgemein für die Tätigkeit. In der Umgangssprache kann es auch das fertige Kunstwerk bezeichnen („eine gelungene Bildhauerei“) und die Bildhauerkunst insgesamt. Seltener wird eine Werkstatt oder das Unternehmen eines Bildhauers als "Bildhauerei" bezeichnet.

Ursprünglich war ein Bildhauer ein Handwerker, der das Bild aus dem Stein oder Holz „haute“, also aus dem Material herausschlug. Schon in der Enzyklopädie von Krünitz (18. Jahrhundert) ist aber nachzulesen, dass es nicht nur um eine hauende Tätigkeit geht; er definierte den Bildhauer als Künstler, „der […] Bilder schnitzet, hauet, gräbt und schneidet“. Inzwischen hat sich die Bedeutung erweitert und umfasst meist auch den Bereich modellierend-künstlerischer Arbeit. Beim bildhauerisch-plastischen Arbeiten können heute ganz verschiedene Materialien kreativ bearbeitet und zusammengefügt werden. 
Viel augenfälliger als in der Malerei wird in den plastischen Künsten das Material mit seinem jeweils eigenen Charakter zur Geltung gebracht. Es ist Träger von Bedeutungsinhalten wie Dauerhaftigkeit oder Kostbarkeit, es kann Spannung, Härte, Weichheit oder Schärfe ausdrücken, es verhält sich ganz unterschiedlich zu Licht und Raum, auch zeigt es vor allem fast immer überaus deutlich die Spuren der Bearbeitung durch den ausführenden Künstler.

Der Entstehungsprozess einer Skulptur kann Ausführungen in verschiedenen Materialien durchlaufen. Griechische Bronzen wurden von den Römern in Marmor kopiert. Der mittelalterliche Bronzegießer benutzte ein Wachsmodell. Seit der Renaissance diente ein Bozzetto aus Ton, Wachs, Stuck oder Weichholz den Bildhauern als Entwurf. Die Gipsform mit ihrer unbeschränkten Möglichkeit des An- und Abtragens ist eine Vorstufe vieler neuzeitlicher Bronzegüsse.

Als Alabaster werden einige chemisch unterschiedlich zusammengesetzte Gesteine bezeichnet, die ähnliche Eigenschaften haben. Die weiße Farbe verleiht ihnen Ähnlichkeit mit Marmor, Alabaster ist aber weicher, leichter polierbar und noch durchscheinender als dieser. Im 14. bis 16. Jahrhundert sind in England gefertigte Alabasterreliefs in viele europäische Länder exportiert worden.

Weißer Marmor selbst scheint erstmals auf den Kykladen um 3000 vor Christus bildhauerisch Verwendung gefunden zu haben und bestimmte seitdem in seinen verschiedenen Varietäten die Bildhauerkunst der Antike. Sorten: Marmor aus Naxos ist nicht durchscheinend und hat eine grobkristalline Struktur. Auf Paros wird eine leicht grau schimmernde Sorte, aber auch ein körniger, ganz weißer Marmor gebrochen. Athen bezog den Stein oft vom nahegelegenen Pentelikon, seine Farbe tendiert zu einer leicht grauen oder gelblichen („goldenen“) Färbung. Die Römer gewannen weißen Marmor in den Apuanischen Alpen. Dort liegt auch Carrara, dessen blendend weiße Sorte seit dem 13. Jahrhundert wieder von den Bildhauern geschätzt wurde.

In Frankreich war Kalkstein, in Deutschland Sandstein das häufigste Material für die gotische Bauplastik. 
Die Neuzeit kennt kaum noch Einschränkungen für die Materialwahl der Bildhauer.

Jade ist ein hartes, in allen Schattierungen von Grün erscheinendes Mineral. Es kommt selten in Stücken vor, die größer als etwa 30 Zentimeter sind. Am wichtigsten war Jade in der chinesischen Kultur.

Andere Hartgesteine aus der Familie der Quarze: Karneol, Chalzedon, Hämatit, Achat und andere wurden für die kleinen Kunstwerke verwendet, die Gemmenschneider zu Schmuckstücken und Siegelsteinen verarbeiteten und im kleinen Format oft erstaunliche Beispiele der Reliefkunst schufen. Bei Kameen wurden die unterschiedlichen Schichten der Steine gern für eine farblich unterschiedliche Heraushebung bestimmter Bildelemente benutzt.

Leichter als Hartgestein lassen sich bestimmte Muschelschalen zu Kameen verarbeiteten, so verwendeten italienische Handwerker im 19. Jahrhundert bestimmte Schneckenmuscheln für solche Reliefs.

Im großen Format wurden harte Eruptivgesteine wie Granit, Porphyr oder Diorit für Herrscherdarstellungen in der ägyptischen Kunst verwendet. Ein anderes historisches Zentrum war Südindien.

Porphyr wurde in der römischen Kaiserzeit nur für dekorative und architektonische Elemente benutzt. Die berühmten Tetrarchen vom Markusdom in Venedig aus dem vierten Jahrhundert sind als Skulpturen eine Ausnahme, zumal die Gewinnung im fünften Jahrhundert eingestellt worden war und erst in der italienischen Renaissance des 16. Jahrhunderts vereinzelt wiederentdeckt wurde. Porphyr lässt sich nur schleifen, aber kaum mit dem Meissel bearbeiten.
In der Moderne, in der technische Hilfsmittel zur leichteren Bearbeitung zur Verfügung stehen, werden Granit und ähnlich widerstandsfähige Steine gern als dauerhaftes Material für Skulpturen im Freien benutzt.

Die Geschichte der Wachsplastik (Ceroplastik) hat Vorläufer in der Herstellung von Bronzegüssen im Wachsausschmelzverfahren, bei dem eine feucht aufgetragene hitzebeständige Masse (z. B. Ton) eine Wachsplastik dicht umhüllt, welche beim Einguss von flüssigem Metall wegschmilzt und formgetreu vom erkaltenden Erz ersetzt wird. Diese Technik erlaubt feinste Detaillierung und jede Form von Durchbrüchen und Hinterschneidungen.

Seit der Renaissance wurde Wachs gerne als Material für Bozzetti, kleine plastische Entwurfsskizzen gewählt. Da Wachs besonders geeignet ist, die Hautoberfläche täuschend ähnlich wiederzugeben, lag eine Verwendung für Bildnisbüsten nahe (z. B. Wachsbüste der Flora), wie sie auch von bedeutenden Künstlern nachweisbar sind. Aus gleichem Grund und seit der gleichen Zeit wurden auch anatomische Präparate aus Wachs gefertigt. Im 18. Jahrhundert widmete sich ein eigener Berufsstand, der "Wachsbossierer" dieser Kunst. Es entstanden die ersten Wachsfigurenkabinette. Zur Volkskunst gehören plastische Votivgaben aus Wachs.

Während aus den frühen Hochkulturen von der Schnitzkunst in Holz und Bein nur wenige zufällige Werke erhalten blieben, ist die Situation bei den Plastiken aus keramischem Material, das im Brennofen seine Dauerhaftigkeit bekam, deutlich besser. Herstellungstechnisch können diese Massen sowohl in Modeln (Hohlformen), als auch durch freies Modellieren geformt werden. Berühmt sind die mit Modeln geformten und mit Zinnglasur versehenen mesopotamischen Reliefs des 6. Jahrhunderts vor Chr. (z. B. das Ischtar-Tor). Ein kleineres Format haben die unglasierten Tonwaren Nordindiens aus dem 2. Jahrhundert vor Christus.

Besondere Berühmtheit hat die Kunst der Chinesen, in Ton zu arbeiten, vor einigen Jahrzehnten durch die Entdeckung der Terrakotta-Armee des Mausoleums Qin Shihuangdis (210 v. Chr.) erlangt. Neben dieser monumentalen Anlage aus tausenden lebensgroßen, frei aus Ton modellierten Statuen gab es auch kleinere Figuren, die ebenfalls als Grabbeigaben dienten. Viele davon sind hohle, in zwei Formenhälften gepresste und glasierte Serienprodukte.

Eine ganz ähnliche Herstellungsweise und Funktion wird mit den im Griechenland des 4. und 3. Jahrhunderts beliebten Tanagra-Figuren verbunden. Nicht alle hatten die hohe Qualität der detailreichen Aphrodite Heyl, einer Terrakottafigur des 2. Jahrhunderts v. Chr.

Aus dem Mittelalter gibt es nur vergleichsweise wenige Beispiele von Tonplastik. Bedeutende, aber vereinzelte Altarfiguren entstanden in Mitteleuropa um 1400, eine Massenware dagegen waren kleinformatige Andachtsfigürchen aus weißem Pfeifenton, eine rheinische Spezialität des späten Mittelalters.

Gegenüber diesen unglasierten Beispielen ist die florentinische Tonplastik durch eine weiße, sparsam kolorierte Zinnglasur gekennzeichnet. Luca della Robbia und seine Werkstatt stellen die Hauptvertreter dieses Stils.
Neben diesen „marktgängigen“ Produkten war Ton seit der Renaissance ein beliebter Rohstoff für die Anfertigung von Entwurfsmodellen. Diese Bozzetti mit ihrem unmittelbar die Hand des Künstlers erfahrbar machenden Duktus sind seltene und künstlerisch wertvolle Einzelstücke.

Eine ganz eigene Tradition ist in der Porzellanplastik zu verfolgen. Voraufgegangen war auch hier wieder China, hier waren seit dem 13. Jahrhundert in den Manufakturen auch figürliche Arbeiten entstanden. In Meißen, wo seit 1708 das Porzellan neu erfunden worden war, brachte man seit 1713 auch figürliche Ware auf den Markt. Johann Joachim Kändler in Meißen und Franz Anton Bustelli sind die bedeutendsten deutschen Bildhauer des 18. Jahrhunderts in dieser Technik. Während die Figuren Kändlers und seiner Zeitgenossen meist bemalt (staffiert) wurden, waren Bustellis Werke, wie auch die der klassizistischen Modelleure oft weiß gelassen und sollten so an Elfenbein und Marmor erinnern, ein Effekt, der im Biskuitporzellan durch unglasiert bleibende Oberflächen noch gesteigert wurde. Die frühe Porzellanplastik entstand durch Formpressen einzelner Teile, die vor dem Brand zusammengefügt und aneinandermodelliert wurden. Dann, seit der zweiten Hälfte des 18. Jahrhunderts nutzt man eine Gießtechnik: in eine Gipsform wird eine dünnflüssige Porzellanaufschwemmung gegossen, deren feste Bestandteile sich an der Form als dicke Schicht anlegen und als hohle Gebilde zum Brennen entnommen werden können.

Das plastische Arbeiten in Metall benutzt sehr unterschiedliche Techniken. So kann eine Form aus geschlagenen („getriebenen“) Blechen heraus modelliert werden. Kleine, schmuckhafte Reliefs sind so entstanden wie auch monumentale Denkmäler, z. B. die Freiheitsstatue, die Quadriga auf dem Brandenburger Tor oder das Hermannsdenkmal.

Ganz anders entsteht ein Metallguss. Das Gießen ist prinzipiell mit allen Metallen möglich.

In der Bildgießerei wird ein Bildhauermodell benötigt, das aus Ton, Wachs, Gips oder anderen Stoffen bestehen kann. Beim Guss wird in den Kunstgießereien zunächst eine Gussform auf der Basis des vom Bildhauer entworfenen Modells angefertigt, die mit dem flüssigen Metall so ausgefüllt wird, dass eine Hohlform entsteht. Mögliche Verfahren für den nächsten Schritt sind das Wachsausschmelzverfahren und die sogenannte verlorene Form (siehe auch Sandformverfahren).

Galvanische Verfahren zur Herstellung von Plastiken haben mit der Gusstechnik die Notwendigkeit eines zugrundeliegenden Modells gemeinsam, das weitere Verfahren ist jedoch gänzlich anders (siehe Galvanoplastik).

Ein technisch uneingeschränkter Umgang in der Verwendung fester Metalle ist seit Jahrzehnten in der Metallbildhauerei zu beobachten.






</doc>
<doc id="11152" url="https://de.wikipedia.org/wiki?curid=11152" title="Victor Vasarely">
Victor Vasarely

Victor Vasarely (ungarisch: "Vásárhelyi Győző"; * 9. April 1906 in Pécs; † 15. März 1997 in Paris) war ein französischer Maler und Grafiker ungarischer Abstammung. Er zählt zu den Mitbegründern der künstlerischen Richtung Op-Art.

Victor Vasarely studierte in Budapest an der Podolini-Volkmann Akademie. Später besuchte er die von Sándor Bortnyik in der Tradition des Bauhauses geführte Mühely Schule für Grafik.

1930 zog er nach Paris, wo er zwischen 1930 und 1940 als Werbegrafiker arbeitete und hauptsächlich Poster entwarf. Er entwickelte dabei Interesse an augentäuschenden Trompe-l’œil, grafischen Mustern und Illusionen des Raumes.

Ab 1944 widmete er sich ausschließlich dem Malen. In diesem Jahr stellte er das erste Mal in der Galerie Denise René in Paris aus. Hier zeigte er neben Schachbrettmustern und sich widerstrebenden Mustern auch figürliche Motive. Ab 1947 konzentrierte sich Vasarely auf konstruktiv geometrische, abstrakte Motive.

In den 1950er Jahren entwickelte er sein Programm einer kinetischen Kunst. In seinem "Gelben Manifest" ("Manifest Jaune") zur Gruppenausstellung "Le Mouvement" bei Denise René (1955) forderte er das Kunstwerk als Prototyp – mit den Eigenschaften "Wiederholbarkeit" als "serielle Vervielfältigbarkeit" und eine über die Kunst hinausreichende Anwendbarkeit seiner Formen. Er erfüllte diese Vorgaben: Seine eigenen Bilder und Skulpturen sind jetzt gekennzeichnet durch das aggressive Zusammenspiel von standardisierten Grundformen und Farben, die auf verschiedene Arten zu Mustern zusammengesetzt werden. Seit 1961 lebte er in Annet-sur-Marne.

Victor Vasarely gewann in den Jahren 1965 und 1967 zahlreiche internationale Kunstpreise. Er war Teilnehmer der documenta 1 (1955), der documenta II (1959), der documenta III (1964) und auch der 4. documenta im Jahr 1972 in Kassel. 1972 entwickelte er ein neues Rauten-Logo im Stil des Op-Art für die Automobilfirma Renault.

Vasarely erhielt 1964 den Guggenheim-Preis in New York. Er wurde 1965 in Paris zum Ritter des Ordens für Kunst und Literatur ernannt. Weitere Preise waren: Großer Preis der VIII. Kunstbiennale von Sao Paulo, 1970 Ernennung zum Ritter des Ordens der Legion d’Honneur.

Sein Sohn Jean Pierre (1934–2002) wurde unter dem Namen Yvaral als Künstler bekannt.

Sein erstes großes Werk "Zebra" gilt heute als das erste Werk der Op Art und Vasarely als ein Mitbegründer dieser Richtung. Das Formenvokabular seines künstlerischen Schaffens umfasst Quadrat, Raute, Dreieck, Kreis und Stabform. Dabei nutzte er konsequent kinetische Effekte und optische Phänomene.

Sein Werk ist von verschiedenen Perioden geprägt, die mitunter parallel verliefen oder sich überschnitten. Bekannte Werke stammen vor allem aus folgenden Perioden:

Bedeutende Einzelwerke:

1970 gründete er ein Museum mit seinen eigenen Arbeiten auf dem Château de Gordes (1996 geschlossen) und 1976 die Vasarely Foundation in Aix-en-Provence. Dort sind 46 Monumentalwerke sowie Werkstudien zu ihrer Entstehung ausgestellt. Ebenfalls 1976 eröffnete das Vasarely-Museums in Pécs, Ungarn, im Geburtshaus des Künstlers. 1987 eröffnete das Vasarely-Museum im Schloss Zichy in Budapest. 1978 wurde das Vasarely-Center in New York eröffnet.


In späteren Lebensjahren kritisierte er die Entwicklungen innerhalb der abstrakten Kunst mit den bekannten Worten:



</doc>
<doc id="11153" url="https://de.wikipedia.org/wiki?curid=11153" title="Carl Andre">
Carl Andre

Carl Andre (* 16. September 1935, in Quincy, Massachusetts) ist ein US-amerikanischer Bildhauer des Minimalismus.

Carl Andre besuchte von 1951 bis 1953 das angesehene Internat Phillips Academy in Andover (Massachusetts), wo er bei Patrick und Maud Morgan den einzigen formellen Kunstunterricht seines Lebens erhielt. Andre freundete sich mit Hollis Frampton an. Ein Studium am Kenyon College, Ohio wurde frühzeitig abgebrochen. 1954 besuchte Andre England und Frankreich, 1955 bis 1956 leistete er seinen Militärdienst in Fort Bragg, North Carolina ab.

Er zog 1957 nach New York, wo ihn Hollis Frampton mit Frank Stella bekannt machte, der ebenfalls die Phillips Academy besucht hatte. Andre arbeitete in den Jahren 1958–60 in Stellas Atelier. Frampton machte ihn auch auf Ezra Pound aufmerksam, in dessen Essays er auf Constantin Brancusi stieß. Erste Holzskulpturen entstanden, die Arbeiten wurden aber anscheinend von einem Nachmieter verheizt. Es handelte sich um neun Pyramiden aus vorgeschnittenen Holzscheiten, die zu einer festen Konstruktion zusammengestapelt waren. 1970 baute Andre eine dieser Pyramiden nach. Sie befindet sich heute im Dallas Art Museum.

Von 1960 bis 1964 arbeitete er bei der "Pennsylvania Railroad" in New Jersey als Bremser auf Güterzügen und Schaffner, eine Periode, in der er kein Geld, Platz oder Zeit für bildhauerische Tätigkeit hatte, die für die Entwicklung seiner künstlerischen Intentionen von ihm selbst aber als außerordentlich wichtig eingeschätzt worden ist. Aus dieser Zeit stammen nach verschiedenen Zufallsprinzipien aus vorgefundenen Texten konstruierte Schreibmaschinengedichte. In New Jersey lernte er Robert Smithson kennen, mit dem er Wanderungen in die Umgebung unternahm. Andres erste Einzelausstellung fand 1965 in der "Tibor de Nagy"-Gallery in New York statt. In der Folge wurde sein Werk in namhaften Museen und Galerien gezeigt. Im Oktober 1966 nahm er mit Ad Reinhardt, Robert Smithson, Robert Morris, Agnes Martin, Jo Baer, Dan Flavin, Michael Steiner, Sol Le Witt und Donald Judd an der Ausstellung "10" in der (Virginia) "Dwan Gallery" teil. Das Städtische Museum in Mönchengladbach zeigte ihn Ende 1968. Im März 1969 nahm Andre an Harald Szeemanns als 'legendär' bezeichneten Ausstellung "Live in your head: When Attitudes become Form (Wenn Attitüden Form werden)" in der Kunsthalle Bern teil. Die Ausstellung reiste anschließend von der Kunsthalle Bern zum Museum Haus Lange in Krefeld und zum Institute of Contemporary Arts in London. 1968 waren seine Arbeiten auf der 4. documenta in Kassel zu sehen, 1970 im Guggenheim-Museum in New York, 1977 auf der documenta 6 in Kassel, 1981 auf der Westkunst in Köln, 1982 auf der documenta 7 in Kassel, 1987 im Van Abbemuseum in Eindhoven, 1994 in Brüssel im Musée des Beaux-Arts, 1996 im Museum of Modern Art in Oxford, England, 1997 im Musée Cantini in Marseille und im selben Jahr in Deutschland in der Synagoge in Stommeln. 2011 wurde Andre mit dem hochdotierten Roswitha Haftmann-Preis ausgezeichnet. 2015 wurde er in die American Academy of Arts and Sciences gewählt.

1985 hatte Andre die kubanisch-amerikanische Künstlerin Ana Mendieta geheiratet. Mendieta stürzte am 8. September 1985 aus dem Fenster ihrer gemeinsamen New Yorker Wohnung im 34. Stock. Carl Andre wurde 1988 vom Mordvorwurf freigesprochen. Der Tod von Mendieta ist bis heute ungeklärt.

Andre ist einer der bedeutenden Vertreter des Minimalismus. Von den Künstlern seiner Zeit schätzte er besonders Frank Stella, mit dem er 1958/59 zusammenarbeitete, und vor allem Constantin Brâncuși, mit dessen Skulptur "Endlose Säule" er sich lange und intensiv auseinandergesetzt hat. Brâncuși wird von manchen Kunsthistorikern als Vorläufer der Minimal-Art angesehen, während Andres erste Skulptur-Arbeit, die neun geschichteten Pyramiden, als erste minimalistische Arbeit überhaupt eingeordnet wird.
Andres frühe Arbeiten kreisen um das Problem eines "Schnitt in den Raum", und in Brâncușis Arbeit sieht er verwandte Tendenzen. In den folgenden Arbeiten konzentrierte er sich auf raumgreifende Anordnungen von Linien, Reliefs oder Flächen aus verschiedenen Materialien, wie unbearbeitetem Holz, Holzbohlen, Ziegelsteinen, Granitblöcken, Kreidestücken oder flachen Stahl- und Kupferplatten, die gelegentlich in angrenzende Räume übergreifen.

Seit Mitte der 1960er Jahre konzentrierte sich Andre auf extrem flache Skulpturen: Felder, die aus quadratischen oder rechteckigen Platten aus verschiedenen Materialien, wie Kupfer, Blei, Aluminium, Stahl usw. bestehen, die Kante an Kante gelegt sind, die daher keine Schatten werfen und die vom Betrachter betreten werden können. Die sinnliche Wahrnehmung des Betrachters, der das Feld betritt, die unterschiedlichen Materialien unter seinen Füßen spürt, die unterschiedlichen Klänge und Töne beim Betreten der verschiedenen Platten hört und der die Veränderungen des Lichts auf den Materialien durch seinen eigenen Schatten sehen kann, wird in Andres Skulpturen gefordert. Andre nennt seine Skulpturen "roads", Straßen, und "zones", Zonen oder Gebiete. Das heißt, dass sowohl der Raum um das Kunstwerk und über dem Kunstwerk Teil desselben ist, dem der Betrachter nicht in der Distanz, frontal, gegenübersteht, sondern dass er sich im Raum des Kunstwerks selbst befindet, welches er von innen mit allen Sinnen wahrnimmt.

Konstante in den Arbeiten Andres ist neben der Auseinandersetzung mit dem Phänomen des Raumes seine Aufmerksamkeit und Sensibilität für die verschiedenen Materialien, zum Beispiel für rohes, wenig bearbeitetes Holz. Eine weitere Konstante in seiner künstlerischen Arbeit ist das Spiel mit der Sprache und den Wörtern.








</doc>
<doc id="11156" url="https://de.wikipedia.org/wiki?curid=11156" title="Talmud">
Talmud

Der Talmud (, , "Studium") ist eines der bedeutendsten Schriftwerke des Judentums. Er besteht aus zwei Teilen, der älteren Mischna und der jüngeren Gemara, und liegt in zwei Ausgaben vor: Babylonischer Talmud (hebräisch "Talmud Bavli") und Jerusalemer Talmud (hebräisch "Talmud Jeruschalmi"), der auch Palästinischer Talmud genannt wird. Der Talmud enthält selbst keine biblischen Gesetzestexte (Tora), sondern zeigt auf, wie diese Regeln in der Praxis und im Alltag von den Rabbinern verstanden und ausgelegt wurden.

Der Talmud liegt in zwei großen Ausgaben vor: "Babylonischer Talmud" (abgekürzt: bT) und "Jerusalemer Talmud", der auch "Palästinischer Talmud" (pT) genannt wird. Wenn einfach vom "Talmud" gesprochen wird, ist in der Regel der Babylonische Talmud gemeint.

Nach Umfang und inhaltlichem Gewicht ist der Babylonische Talmud (hebräisch תַּלְמוּד בַּבְלִי "Talmud Bavli", aramäisch תַּלְמוּדָא דְבָבֶל "Talmuda deVavel") das bedeutendere Werk. Er entstand in den relativ großen, geschlossenen jüdischen Siedlungsgebieten, die nach der Zerstörung Jerusalems durch die Römer im judenfreundlicheren Perserreich existierten, genauer gesagt in Sura und Pumbedita. Dieses Gebiet wurde im Judentum traditionell als „Babylon“ bezeichnet, obwohl eine Stadt oder ein Staat solchen Namens seit dem Untergang des neubabylonischen Reiches im 5. Jahrhundert v. Chr. nicht mehr existierte. Als maßgebliche Autoren gelten die Rabbiner Abba Arikha (genannt Raw), Samuel Jarchinai (Mar) sowie Rav Aschi.

Der erheblich kürzere "Talmud Jeruschalmi" entstand in Palästina. Er ist weniger wichtig als der Babylonische Talmud und in seinen Bestimmungen oft weniger streng. Hier gilt nach jüdischer Tradition, die auf Maimonides zurückgeht, als wichtigster Autor Rabbi Jochanan. Für den Jerusalemer Talmud gibt es verschiedene Bezeichnungen:

Der erste Druck des Talmud aus dem Jahr 1523, editiert von Jacob Ben Chajim, stammt aus der Druckerei von Daniel Bomberg, einem aus Antwerpen stammenden Christen, der zwischen 1516 und 1539 in Venedig tätig war. Die von Bomberg eingeführte Folio-Zählung wird heute noch benutzt.

Es gibt verschiedene Methoden der Stoffgliederung im Talmud:

Kernstück des Talmud ist die Mischna ( ‚(Lehre durch) Wiederholung‘). Es handelt sich hierbei um jenen Teil der Tora (), den Gott nach jüdischer Tradition Moses am Berg Sinai "mündlich" offenbart haben soll und der in der Folgezeit über Jahrhunderte auch zunächst nur mündlich weitergegeben, im 1. oder 2. Jahrhundert schließlich aber doch kodifiziert wurde. Ihre endgültige Form gefunden hat die in Hebräisch abgefasste Mischna im 2. Jahrhundert unter redaktioneller Federführung von Jehuda ha-Nasi. Sie ist im Babylonischen und im Jerusalemer Talmud im Wesentlichen identisch.

Die zweite Schicht des Talmud ist die Gemara (aramäisch: גמרא ‚Lehre‘, ‚Wissenschaft‘), die aus Kommentaren und Analysen zur Mischna in aramäischer Sprache besteht. Sie sind die Frucht umfangreicher und tief philosophischer Diskussionen unter jüdischen Gelehrten insbesondere in den Akademien von Sura und Pumbedita. Ausgehend von den meist rein juristischen Fragestellungen wurden Verbindungen zu anderen Gebieten wie Medizin, Naturwissenschaft, Geschichte oder Pädagogik hergestellt. Auch wurde der eher sachliche Stil der Mischna mit diversen Fabeln, Sagen, Gleichnissen, Rätseln etc. erweitert. Die Gemara war zwischen dem 5. und 8. Jahrhundert abgeschlossen. Anders als die einheitliche Mischna weichen die Fassungen der Gemara in der babylonischen und der palästinischen Talmudausgabe voneinander ab.

Beim Babylonischen Talmud kommen schließlich als dritte Schicht die Kommentare aus späterer Zeit hinzu. Hervorzuheben sind insofern insbesondere jene von Raschi (Rabbi Schlomo ben Jizchak), einem im 11. Jahrhundert in Frankreich und Deutschland wirkenden Talmud-Gelehrten.

Die ständige Fortentwicklung der Tradition durch Diskussionen, Kommentare und Analysen prägt den durchgängig dialektischen Stil des Talmud. Das bevorzugte Mittel der Darstellung ist der Dialog zwischen verschiedenen rabbinischen Lehrmeinungen, der am Ende zu einer Entscheidung führt und den maßgeblichen Stand der Tradition wiedergibt.

Üblicherweise sind die einzelnen Textteile so angeordnet, dass sich die Mischna in der Mitte jeder Seite befindet. Links und unten wird sie L-förmig von der Gemara umrahmt. Der Textstreifen am oberen Innenrand einer Seite enthält die Kommentare Raschis, der am Außenrand und ggf. am unteren Rand schließlich etwaige weitere Kommentare.

Quer zur bereits genannten Einteilung des Talmud in die drei Überlieferungsschichten steht die Einteilung in die praxisnahe Auslegung der gesetzlichen Vorschriften (Halacha, הלכה) und die erzählerischen und erbaulichen (homiletischen) Betrachtungen (Aggada, אגדה). Sie findet sich nur in den beiden Kommentarschichten, jedoch kaum in der nahezu ausschließlich aus Halacha bestehenden Mischna.

In seinem Gedicht "Jehuda Ben Halevy" vergleicht Heinrich Heine die Halacha mit einer „Fechterschule, wo die besten dialektischen Athleten […] ihre Kämpferspiele trieben“. Die Aggada, die er fälschlich „Hagada“ nennt, sei indes „ein Garten, hochphantastisch“, in dem es „schöne alte Sagen, Engelmärchen und Legenden“ gebe, „stille Märtyrerhistorien, Festgesänge, Weisheitssprüche (…)“.

Eine dritte Gliederungssystematik schließlich fußt auf sachlichen Prinzipien. Beide Talmude sind, wie die ihnen zugrundeliegende Mischna, in 6 „Ordnungen“ (Seder, סדר) eingeteilt, diese wiederum in 7 bis 12 Traktate (masechet, מסכת). Die Traktate wiederum bestehen aus Abschnitten und letztlich aus einzelnen Mischnajot.

Die Titel der Ordnungen lauten:

Neben dem Hebräischen ist vor allem Aramäisch Sprache des Talmuds. Der Talmud wird gewöhnlich in den Originalsprachen studiert.

Im Jüdischen Verlag erschien 1929 bis 1936 die erste und bisher einzige vollständige und unzensierte deutsche Übersetzung des Babylonischen Talmud. Die Übersetzung stammt von Lazarus Goldschmidt. Diese Ausgabe umfasst 12 Bände. Im Seitenaufbau weicht sie von den gängigen Ausgaben ab. Die Mischna ist in Kapitälchen gesetzt. Darunter folgt die Gemara im normalen Satz. Sie wird jeweils mit dem in Großbuchstaben gesetzten Wort „Gemara“ eingeleitet. Zusätzliche Anmerkungen zur Mischna oder Gemara sind als Fußnoten gesetzt. In der Originalausgabe und in den Nachdrucken gibt es nur ein Inhaltsverzeichnis pro Band, kein Gesamtverzeichnis für alle Bände. Auch die Einteilung in Sektionen geben diese Verzeichnisse nicht wieder.

Da der Talmud in der Wahrnehmung sehr mit dem Wesen des Judentums selbst identifiziert wurde, richteten sich Angriffe gegen das Judentum meist auch gegen diesen. 

Bereits frühzeitig wurde Juden die Beschäftigung mit dem Religionsgesetz mehrfach untersagt. Solch ein Verbot wird von der rabbinischen Geschichtsschreibung als einer der Gründe des Bar-Kochba-Aufstands angegeben. Im Jahr 553 erließ Kaiser Justinian I. ein Gesetz, das Juden das Studium der "deuterosis" verbot, womit die Mischna oder Beschäftigung mit der Halacha allgemein gemeint war. Papst Leo VI. erneuerte später dieses Verbot.

Im Mittelalter kam es zu stärkeren Anfeindungen gegenüber dem Talmud. Manche dieser Angriffe stammten von zum Christentum konvertierten Juden. So ging die Talmuddisputation von Paris 1240 von dem Konvertiten Nikolaus Donin aus, der 1224 von den Rabbinern in den Bann getan worden war und 1236 zum Christentum konvertiert war. 1238 forderte er in einer Schrift mit 35 Punkten gegen den Talmud dessen Verbot von Papst Gregor IX. Als Folge der Disputation zwischen Donin und Rabbi Jechiel ben Josef kam es 1242 zur ersten großen Talmudverbrennung.

1244 verfügte Papst Innozenz IV. zunächst die Vernichtung aller Ausgaben des Talmud. Er revidierte dieses Urteil 1247 auf jüdische Bitte hin, veranlasste aber die Zensur des Talmud und beauftragte gleichzeitig eine Untersuchungskommission der Universität von Paris, der 40 Sachverständige angehörten, darunter Albertus Magnus. Die Kommission kam zu einer erneuten Verurteilung, die 1248 verkündet wurde.

In einer weiteren Disputation über den Talmud zwischen dem vom Judentum abgefallenen und konvertierten Pablo Christiani und dem jüdischen Gelehrten Nachmanides 1263 in Barcelona erklärte der spanische König dagegen Nachmanides zum Sieger. Bis zum Ende des 16. Jahrhunderts gingen dann Disputationen, Konzile und Kirchenversammlungen mit Verboten, Beschlagnahmungen und Verbrennungen des Talmud einher. Papst Julius III. ließ im Jahr 1553 in Rom das Werk beschlagnahmen und die eingesammelten Exemplare am 9. September, dem jüdischen Neujahrstag, öffentlich verbrennen. Danach trat die Inquisition auf den Plan, die in einem Dekret Talmudverbrennungen den Herrschern in allen christlichen Ländern empfahl. Unter Androhen ihres Vermögensverlustes sollten Juden zur Ablieferung der Talmudexemplare binnen dreier Tage gezwungen werden. Christen sollten mit der Exkommunikation belangt werden, falls sie es wagen sollten, den Talmud zu lesen, aufzubewahren oder Juden in dieser Sache behilflich zu sein. 

In judenfeindlichen Publikationen wurden Stellen aus dem Talmud zitiert, um die jüdische Religion und Tradition in Misskredit zu bringen. Teilweise handelt es sich bei den „Zitaten“ um Fälschungen. Aber auch die echten Zitate sind in der Regel aus dem Zusammenhang gerissen und tragen der im Talmud vorherrschenden Form der dialogischen, oft kontroversen Annäherung an ein Thema nicht Rechnung. Im talmudischen Diskurs werden oft auch bewusst unhaltbare Thesen (etwa: „Nichtjuden sind keine Menschen“) in die Diskussion geworfen, um sie daraufhin im Dialog zu widerlegen. Antijudaisten verwenden bis in die Gegenwart bevorzugt solche „Thesen“, verschweigen jedoch die folgenden Antithesen, so dass ein verfälschter Gesamteindruck der religiösen Leitlinien des Talmuds und der jüdischen Religion insgesamt entsteht. 

Eine seltene Ausnahme war der Humanist Johannes Reuchlin, der als erster deutscher und nichtjüdischer Hebraist gilt, welcher zum besseren Verständnis die hebräische Sprache und Schrift erlernte. Er veröffentlichte eine hebräische Grammatik, schrieb über die Kabbala und verteidigte den Talmud und die jüdischen Schriften im Streit mit Johannes Pfefferkorn.

Der Reformator Martin Luther forderte 1543 in seiner Schrift "Von den Jüden vnd jren Lügen" neben dem Verbrennen von Synagogen und jüdischen Häusern auch die Konfiszierung aller jüdischen Bücher einschließlich des Talmuds. Aber auch die katholische Kirche setzte in der Gegenreformation den Talmud 1559 auf den ersten Index verbotener Bücher.

Im 17. Jahrhundert gab es einige Humanisten und christliche Hebraisten, welche den Talmud gegen den damaligen Antijudaismus in Schutz nahmen und versuchten, mit Hilfe des Talmuds und der rabbinischen Literatur das Neue Testament und das Christentum besser zu verstehen. Der Basler Theologe Johann Buxtorf der Jüngere übersetzte 1629 das religionsphilosophische Werk "Führer der Unschlüssigen" des mittelalterlichen jüdischen Gelehrten Maimonides und vollendete 1639 das von seinem Vater Johann Buxtorf dem Älteren begonnene "Lexicon chaldaicum, talmudicum et rabbinicum". Der anglikanische Theologe John Lightfoot stellte in "Horae Hebraicae Talmudicae" von 1685 erstmals die talmudischen Parallelen zum Neuen Testament zusammen.

Der antijüdische Autor Johann Andreas Eisenmenger sammelte die Textstellen aus der ihm bekannten rabbinischen Literatur, besonders des Talmuds, die geeignet waren, das Judentum zu diskreditieren und antijüdische Vorurteile zu bestärken, und veröffentlichte sie 1700 unter dem Titel "Entdecktes Judenthum". Das Werk gilt als das populärste der zahlreichen von christlichen Autoren gegen die rabbinische Literatur verfassten Polemiken und diente beispielsweise auch für August Rohlings Hetzschrift "Der Talmudjude" und für viele Antisemiten des 19. und 20. Jahrhunderts als Quelle für ihre Diffamierungen.

Die Praxis, den Talmud zur Verunglimpfung des Judentums und der Juden zu missbrauchen, ist auch heute verbreitet in christlichem, muslimischem oder säkularem Antijudaismus/Antisemitismus.







</doc>
<doc id="11162" url="https://de.wikipedia.org/wiki?curid=11162" title="Tetrahydrocannabinol">
Tetrahydrocannabinol

Tetrahydrocannabinol [THC, genauer (–)-Δ-"trans"-Tetrahydrocannabinol] ist eine psychoaktive Substanz, die zu den Cannabinoiden zählt.

Das Molekül stellt den hauptsächlich rauschbewirkenden Bestandteil der Hanfpflanze ("Cannabis") dar. In der Pflanze liegt die Substanz jedoch weitgehend in natürlicher Form zweier THC-Säuren vor. Diese werden erst durch Decarboxylierung wirksam, was durch Trocknung des Pflanzenmaterials erreicht wird.

Die bekannteste natürliche Quelle für Cannabinoide ist mit bis zu 80 % das Harz der Cannabispflanze, wobei der Gehalt an Δ-9-THC bis 22 % betragen kann. In Europa von der Polizei beschlagnahmtes Cannabiskraut enthielt im Jahr 2015 zwischen 3 und 22 % THC, durchschnittlich etwa 10 %.

Der amerikanische Chemiker Roger Adams isolierte und identifizierte Cannabidiol aus Pflanzenmaterial und zeigte damit den Zusammenhang zu Cannabidiol und Tetrahydrocannabinol. In reiner Form wurde THC erstmals 1964 von Yehiel Gaoni und Raphael Mechoulam am Weizmann-Institut für Wissenschaften in Israel isoliert. Tetrahydrocannabinol unterliegt in Deutschland den Bestimmungen des Betäubungsmittelgesetzes.

Gewonnen wird THC hauptsächlich aus der Hanfpflanze (Cannabis). Besonders reich an THC sind hierbei die unbefruchteten weiblichen Blütenstände (etwa 6 bis 20 %), der THC-Gehalt der übrigen Pflanzenteile ist weit geringer (knapp 1 %). In den Samen der Pflanze ist gar kein THC enthalten. Die Blätter nahe der Blüte enthalten etwa 5 bis 6 % THC. Männliche Pflanzen haben im Unterschied zu weiblichen einen sehr geringen THC-Gehalt.

Δ-Tetrahydrocannabinol weist vier Stereoisomere auf:

(–)-Δ-"trans"-Tetrahydrocannabinol und (+)-Δ-"trans"-Tetrahydrocannabinol sowie
(–)-Δ-"cis"-Tetrahydrocannabinol und (+)-Δ-"cis"-Tetrahydrocannabinol

Das wesentliche psychoaktiv wirksame Isomer ist das (–)-Δ-"trans"-THC (Dronabinol), das 6 bis 100 mal stärker wirksam ist als das (+)-Δ-"trans"-THC. Die "cis"-Formen besitzen keine psychoaktive Wirksamkeit, über ihr natürliches Vorkommen gibt es unterschiedliche Angaben.

Tetrahydrocannabinol liegt in der Cannabispflanze überwiegend als THC-Säure vor: Durch enzymatische Kondensation aus den beiden Präkursoren Geranylpyrophosphat und Olivetolsäure wird Cannabigerol­säure gebildet, die anschließend enzymatisch in Tetrahydrocannabinolsäure umgelagert wird. Durch Wärme und UV-Strahlung decarboxyliert die Säure teilweise zum THC. Eine Umwandlung oral aufgenommener THC-Carbonsäure in THC ließ sich in Fütterungsexperimenten mit Ratten nicht nachweisen.

THC ist sehr lipophil. Es kann per Extraktion aus THC-haltigem Pflanzenmaterial isoliert werden, wozu unpolare und schwach polare Lösungsmittel wie "n"-Alkane, Aceton, Isopropylalkohol oder Ethanol geeignet sind. Nach dem Abdampfen des Lösungsmittels bleibt ein harziger, ölartiger Extrakt zurück. Die Zusammensetzung des Extrakts ist abhängig von der Wahl des Lösungsmittels. Bei geeigneten Bedingungen können sehr hohe THC-Konzentrationen erreicht werden. Dieser Extrakt wird auch als Haschischöl bezeichnet.

Mit "n"-Butan lassen sich lipophile Inhaltsstoffe bei sehr tiefen Temperaturen aus dem Pflanzenmaterial extrahieren; diese Methode bringt allerdings hohe Brand- und Explosionsgefahr mit sich. Butan verdampft bereits bei Zimmertemperatur. Der so erhaltene Extrakt hat ein Aussehen ähnlich wie Bernstein, bei Zimmertemperatur ist er dickflüssig und zieht Fäden wie Kunstharz. Wenn man ihn abkühlt, erstarrt er relativ schnell.

Neben THC enthält der Extrakt weitere Cannabinoide; bei Verwendung stärker polarer Extraktionsmittel wie Ethanol können entsprechend polare Stoffe enthalten sein, wie Chlorophyll, Alkaloide (Trigonellin, Hordenin), Aminosäuren, Aminozucker, eventuell auch ungelöste feine Teile des Ausgangsmaterials. Durch geeignete Verfahren kann der Extrakt noch weiter gereinigt werden.

Dronabinol kann aus dem in Zitrusfrüchten vorkommenden Limonen synthetisiert oder mit aufwendigen Verfahren aus THC-armem Nutzhanf teilsynthetisch hergestellt werden (Extraktion von Cannabidiol und Umwandlung in THC). Es ist dann sehr viel teurer, als wenn man es aus potentem Medizinalhanf extrahieren würde. Die direkte Extraktion von Dronabinol aus THC-reichen Sorten (welche eine Biosyntheseleistung von 18 bis 22 %, bezogen auf die Trockenmasse der Pflanzenteile, aufweisen können) ist in vielen Ländern aus rechtlichen Gründen nicht möglich.

Sofern THC durch Cannabis-Konsum aufgenommen wird, ist die häufigste Konsumform das Rauchen von Haschisch oder Marihuana pur oder gemischt mit Tabak als Joint. Häufig wird THC-haltiges Material auch mit Hilfe speziellen Rauchzubehörs wie Bongs und Pfeifen geraucht oder mit dem Vaporizer verdampft und inhaliert.

Daneben wird THC auch in Speisen und Getränken verarbeitet. Da THC lipophil ist, kann es in fettreichen Nahrungsmitteln wie Milch, Kuchen, Muffins verarbeitet werden. Dass Konsumenten Cannabis/THC spritzen würden, ist nicht bekannt. THC ist auf Grund seiner Lipophilie ohne Emulgator nicht intravenös applizierbar. Aufgrund seiner schlechten Wasserlöslichkeit kann es in Form von Lösungen oder Emulsionen mit Ethanol, Dimethylsulfoxid, Polysorbat 80, Cremophor EL oder Polyvinylpyrrolidon verabreicht werden.

Der Wirkmechanismus von THC ist noch nicht vollständig geklärt.

THC wirkt auf mindestens zwei Arten von Rezeptoren, die bei Säugetieren vorkommen, CB und CB. CB-Rezeptoren befinden sich vorwiegend in zentralen und peripheren Nervenzellen, wo sie die Ausschüttung von Neurotransmittern modulieren. Sie kommen aber auch in anderen Zellen vor, zum Beispiel in der Hypophyse, Immunzellen, gastrointestinalem Gewebe, sympathetischen Ganglien, Herz, Lunge, Harnblase und Nebennieren. CB-Rezeptoren kommen hauptsächlich in Immunzellen vor und sind an der Zytokinausschüttung beteiligt.

Endocannabinoide sind körpereigene Substanzen, die auf die CB- und CB-Rezeptoren wirken. Sie sind Eikosanoide und werden vom Organismus bei Bedarf erzeugt. Die bekanntesten sind Arachidonylethanolamid (Anandamid) und 2-Arachidonylglycerol (2-AG). Die Endocannabinoide und die Cannabinoid-Rezeptoren bilden das sogenannte Endocannabinoid-System.

THC bindet an die CB-Rezeptoren und beeinflusst die Signalübertragung an diesen Synapsen, mit Auswirkungen auf das zentrale und periphere Nervensystem, wie Glücksgefühl, Entspannung und Analgesie (Schmerzlinderung). Die Aktivierung hemmt über G-Proteine die Adenylylcyclase, blockiert Ca-Kanäle und aktiviert K-Kanäle. Die Transduktionsmechanismen ähneln hierbei den Opioidrezeptor-Subtypen "μ", "δ" und "κ".

Über die Rolle der CB-Rezeptoren ist weniger bekannt, man nimmt jedoch an, dass sie an der Immunmodulation beteiligt sind, weil sie vorwiegend in B-Zellen und in natürlichen Killerzellen vorkommen.

Im Tiermodell wirkt THC antagonistisch auf 5-HT-Rezeptoren, welche am Brechreiz beteiligt sind. THC wirkt auch auf andere pharmakologische Ziele, wie auf Capsaicin empfindliche perivaskuläre sensorische Nerven.

Das Verteilungsmuster der CB-Rezeptoren im Gehirn bedingt viele der pharmakologischen Eigenschaften von THC. Im Stammhirn, wo lebenswichtige Funktionen wie Atmung koordiniert werden, sind nur sehr wenige bis gar keine dieser Rezeptoren vorhanden. Im Hippocampus, wo das Kurzzeitgedächtnis angesiedelt ist, finden sich hingegen viele dieser Rezeptoren. CB-Rezeptoren in den Basalganglien bieten eine Erklärung für den Einfluss von THC auf die Motorik.

Das schwach psychoaktive Cannabidiol (CBD) hat neben eigenen therapeutischen Wirkungen einen modulierenden Einfluss auf THC. Sowohl THC als auch CBD wirken antioxidativ und entfalten so eine neuroprotektive Wirkung, zum Beispiel bei Glutamat-induzierter Excitotoxizität. THC hemmt die Glutamat-Ausschüttung, möglicherweise auch den Eintritt von Calcium über die Ionenkanäle, und könnte deshalb eine neuroprotektive Wirkung entfalten.

Das in Cannabis in geringer Menge enthaltene Δ-Tetrahydrocannabinol (Δ-THC) ist psychoaktiv, aber etwas weniger potent als Δ-THC.

THC und CBD können Zeichen des apoptotischen und nekrotischen Zelltods bei Tumorzellen induzieren.

Δ-THC wird im Menschen überwiegend zu 11-Hydroxy-Δ-THC (11-OH-Δ-THC) oxidiert. Dieses Stoffwechselprodukt ist ebenfalls psychoaktiv und wird weiter zu 11-Nor-9-carboxy-Δ-THC (11-COOH-THC, THC-COOH, THC-Carbonsäure, nicht psychoaktiv) verstoffwechselt. In Menschen und Tieren wurden über 100 verschiedene Δ-THC-Metabolite identifiziert, nahezu alle sind nicht psychoaktiv. Die Metabolisierung findet im Wesentlichen in der Leber und durch die Cytochrom-P450-Enzyme 2C9, 2C19 und 3A4 statt. Die Metaboliten werden dann aufgrund ihrer lipophilen Eigenschaften im Fettgewebe eingelagert, woraus sie anschließend nur sehr langsam wieder entfernt werden. Mehr als 65 % des ursprünglich vorhandenen THCs werden so in Form von Metaboliten im Stuhl ausgeschieden und rund 25 % im Urin, ein geringer Teil wird im Körper selbst abgebaut. Die Hauptmetaboliten im Urin sind mit Glucuronsäure verestertes THC-COOH und freies THC-COOH, während im Stuhl 11-OH-THC dominiert.

Die LD bei der Maus beträgt 42 mg/kg Körpergewicht intravenös und 482 mg/kg bei oraler Verabreichung, beim Rhesusaffen tritt nach intravenöser Gabe von 128 mg/kg Körpergewicht der Tod durch Atemstillstand und Herzversagen ein.

Der LD-Wert wird am Menschen nicht ermittelt und lässt sich nicht verlässlich hochrechnen. Nimmt man in einer groben (und niedrig angesetzten) Schätzung, den potentiellen peroralen LD-Wert für Menschen mit 150 mg/kg Körpergewicht an, dann würde eine 70 kg schwere Person nach oralem Akut-Konsum von 10,5 g THC mit einer Wahrscheinlichkeit von 50 % sterben. Diese Menge ist enthalten in rund 70 bis 130 g eines Cannabisprodukts mit 8 bis 15 % THC-Gehalt. Andere Autoren geben niedrigere letale Dosen von etwas über 4 Gramm an. Da THC über den Darm nur zu etwa 6 % und über die Lunge zu rund 20 % resorbiert wird, ist es praktisch unmöglich, letale Mengen THC durch den Konsum natürlicher Cannabisprodukte zuzuführen, zumal die erforderliche Menge um etwa den Faktor 1000 über der üblichen Konsummenge liegt. Noch nie ist beim Menschen ein Fall einer Überdosis mit Todesfolge durch Aufnahme natürlicher Cannabisprodukte bekannt geworden. Nach Angaben der FDA wurden in den USA im Zeitraum zwischen dem 1. Januar 1997 und dem 30. Juni 2005 nach Anwendung des THC-haltigen Fertigarzneimittels „Marinol“ fünf Todesfälle gemeldet.

Psychische Effekte treten bei folgenden Dosierungen auf: 30 bis 50 μg/kg intravenös, 50 μg/kg bei Rauchinhalation, 120 μg/kg oral. Bei Rauchinhalation geringerer Mengen THC (5 bis 7 mg) überwiegt die sedative Komponente, bei Mengen von 15 mg oder darüber überwiegt Vigilanz, die sich bis zu psychotischen Zuständen steigern kann.

Bei Rauchinhalation gehen ungefähr 20 % des im Rauch vorhandenen Δ-THC in das Blut über, oral nur etwa 6 %. THC geht vom Rauch sehr schnell ins Blut über, hierbei ist die Entwicklung der Plasmakonzentration mit intravenöser Einnahme vergleichbar. Bei oraler Einnahme in Form von Sesamölkapseln ist die Wirkung wegen des First-Pass-Effekts vermindert, die Bioverfügbarkeit beträgt nur etwa 10 bis 20 %, die höchste THC-Konzentration wird nach etwa zwei Stunden erreicht.

THC ist im Blutplasma überwiegend an Proteine gebunden; maximal 10 % kommen in den roten Blutkörperchen vor. Die Plasmahalbwertszeit nach intravenöser Gabe entwickelt sich in vier Phasen, was nahelegt, dass es mindestens vier Gewebearten gibt, in die THC einsickert, mit jeweils unterschiedlicher Durchlässigkeit und Bindungskapazität. Nach starker Verringerung in den ersten Minuten sinkt die THC-Konzentration nur noch langsam. Die Halbwertszeiten der ersten drei Phasen betragen jeweils 1 Minute, 4 Minuten und 1 Stunde. Die anfänglich kurze Halbwertszeit ist auf den schnellen Übergang von THC in bestimmte Gewebearten sowie auf die schnelle Verstoffwechslung der Substanz zurückzuführen. Nach ungefähr 6 Stunden besteht ein Pseudogleichgewicht zwischen dem THC-Gehalt im Blutplasma und in den Geweben. Die Halbwertszeit der vierten Phase (terminale Halbwertszeit nach Erreichen des Pseudogleichgewichts) wird unterschiedlich mit 19–36 Stunden angegeben. Nach 5 Tagen ist etwa 80 bis 90 % des THC in Form von Metaboliten ausgeschieden, etwa zu zwei Dritteln im Stuhl und zu einem Fünftel im Harn.

Die THC-Konzentration im Gehirn erreicht nach rund 30 Minuten ihr Maximum; die Konzentration ist drei- bis sechsmal höher als im Plasma. Die THC-Konzentrationskurven im Gehirn und im Plasma verlaufen parallel, was für ein uneingeschränktes Passieren der Blut-Hirn-Schranke spricht. Tierversuche haben gezeigt, dass sich THC als lipophile Substanz in bestimmten Gewebearten stark anreichert, zum Beispiel in Körperfett, Herz, Leber und Lunge. Ebenso wurde im Tierversuch nachgewiesen, dass THC durch die Plazenta auf Föten übergeht. Welche Auswirkungen dies hat, ist weitgehend unbekannt.

Bekannte Wirkungen von Δ-THC auf den Menschen beziehungsweise Wirkungen von Cannabis, die auf Δ-THC zurückgeführt werden:

Bei regelmäßigem, intensivem Konsum kann sich ein Toleranzeffekt (erforderliche Dosissteigerung, um die gewohnte Wirkung zu erzielen) entwickeln. Entzugssymptome und eine damit einhergehende Entwicklung von Abhängigkeit sind bedingt durch eine Unterfunktion des mesolimbischen Systems (subkortikale Belohnungssysteme), die nach Absetzen des Konsums wirksam wird und andauert bis sich in diesen Arealen ein neuronales Gleichgewicht (Entwöhnung) wieder hergestellt hat.

Eine Vielzahl von Studien hat zu der heute unstrittigen Erkenntnis geführt, dass Cannabiskonsum mit einem erhöhten Risiko für die Auslösung psychotischer Erkrankungen verbunden ist. Ferner steigt das Risiko mit der Menge des Konsums. Ein ursächlicher Zusammenhang ist bislang jedoch noch nicht gefunden worden. Deshalb bleibt unklar, ob Cannabis hier als alleiniger Faktor oder nur in Kombination mit anderen als Auslöser auftritt. Als möglicher neurobiologischer Mechanismus wurde eine durch Cannabinoide verursachte Störung dopaminerger Systeme diskutiert.

Wird Cannabis geraucht, entstehen bei seiner Verbrennung ähnlich wie beim Tabak krebserregende Produkte. Wird es als Joint, also als Mischung mit Tabak geraucht, kommen die Risiken des Nikotinkonsums, wie z. B. das Risiko einer Arteriosklerose, hinzu.

Es bestehen keine Hinweise, dass THC selbst mutagen, karzinogen oder teratogen (fruchtschädigend) ist. Schwangere und Stillende sowie Heranwachsende sollten auf den Konsum von THC verzichten, weil Schäden am ungeborenen oder gestillten Kind nicht ausgeschlossen werden können und es Hinweise darauf gibt, dass THC die Entwicklung des nicht ausgereiften Gehirns nachhaltig beeinflussen könnte. Dazu liegen auch zahlreiche Erkenntnisse aus tierexperimentellen Studien vor.

Metaanalysen von 2013 und 2014, die die bis dahin vorliegenden Gehirnstudien durch bildgebende Verfahren auswerteten, gelangten zu dem Ergebnis, dass Cannabiskonsum im präfrontalen Cortex (Stirnseite des Frontallappens der Großhirnrinde) zu einem verminderten Gehirnvolumen und zu einer Beeinträchtigung der weißen Substanz (Nervenverbindungen) führt, sowie zu einem beidseitigen verminderten Volumen des Hippocampus. Bei letzterer Gehirnregion, die eine Schlüsselrolle bei allen Gedächtnisfunktionen hat, bestand zusätzlich eine Korrelation (Entsprechung) zwischen Volumenabnahme und Menge des bisherigen Cannabiskonsums.

Die Bundesregierung Deutschland beschloss am 4. Mai 2016 einen Gesetzesentwurf, der die Versorgung der Patienten mit natürlichem Cannabis und die Erstattungsfähigkeit durch die Krankenkassen ermöglichen soll und der am 19. Januar 2017 vom Bundestag einstimmig verabschiedet wurde. Nach der am 9. März 2017 veröffentlichten Verkündung können bedürftige, chronisch Schwerkranke Cannabis auf Rezept bekommen, wobei die Kosten teilweise von den Krankenkassen übernommen werden. Ärzte sollen eigenverantwortlich entscheiden, ob eine Cannabis-Therapie sinnvoll ist, auch wenn im Einzelfall noch andere Behandlungsoptionen bestehen. "Die Patienten müssen also nicht "austherapiert" sein, wie es anfangs hieß, bevor sie einen Anspruch auf ein Cannabis-Rezept haben."

Dronabinol ist in Deutschland und anderen Staaten als verschreibungspflichtiges Betäubungsmittel für die Herstellung von Rezepturarzneimitteln erhältlich. Unter dem Handelsnamen "Marinol" ist es in den Vereinigten Staaten zur Behandlung von Anorexie und Kachexie bei AIDS und als Antiemetikum im Rahmen einer Krebstherapie zugelassen, kein zugelassenes Anwendungsgebiet hingegen ist die Therapie eines zu hohen Augeninnendruckes (Glaukom).

Das vollsynthetische THC-Analogon Nabilon hat ähnliche Indikationen wie Dronabinol. Das THC-Analogon "Levonantradol" wird in Deutschland nur für Forschungszwecke genutzt. Außerdem befindet sich THC in der klinischen Erprobungsphase für die Behandlung von Glaukomen und Autoimmunerkrankungen, wie Multipler Sklerose, Morbus Crohn oder Colitis ulcerosa. Dass THC Tics bei Betroffenen des Tourette-Syndroms wirksam reduziert, bestätigten die Ergebnisse einer sechswöchigen Studie an der Medizinischen Hochschule Hannover.

Cannabisblüten (lat. "„Cannabis flos“ ") sind in den Niederlanden in vier Varietäten mit verschiedenen THC-Nenngehalten verschreibungspflichtig für die Human- und Tiermedizin erhältlich: "Bedrocan" (THC ca. 22 %; CBD <1 %), "Bedrobinol" (THC ca. 13,5 %; CBD <1 %), "Bediol" (THC ca. 6,3 %; CBD ca. 6 %) und "Bedica" (THC ca. 14 %; CBD <1 %; gemahlene Blüten). Der Verkaufspreis wird mit 34,50 € exkl. Mwst. für 5 g Blüten angegeben (Stand Juli 2017).<ref name="DOI10.1016/j.ejca.2007.09.010">Frederike K. Engels, Floris A. de Jong u. a.: "Medicinal cannabis in oncology." In: "European Journal of Cancer." 43, 2007, S. 2638–2644, .</ref> Der Hanf wird in den Niederlanden unter staatlicher Aufsicht angebaut, der Handel untersteht dem "Bureau voor Medicinale Cannabis" (BMC).

In Österreich, Kanada und Großbritannien ist ein Mundspray mit den Handelsnamen "Sativex" (Wirkstoff: Nabiximols, bestehend aus pflanzlichem THC und Cannabidiol) für die Behandlung neuropathischer Schmerzen und Spasmen bei multipler Sklerose sowie zur Behandlung von Schmerzen, Übelkeit und Erbrechen in Zusammenhang mit Krebs- und AIDS-Erkrankungen zugelassen. Weitere Anwendungsgebiete befinden sich in der klinischen Prüfung. In Deutschland ist das Mundspray nach einer Änderung des Betäubungsmittelgesetzes im Mai 2011 seit dem 1. Juli 2011 als verschreibungspflichtiges BTM für die Behandlung von Spastik bei MS zugelassen.

Die Inhalation von THC habe laut einer kleinen, plazebo-kontrollierten Studie aus dem Jahr 2007 einen geringfügig positiven Effekt auf neuropathischen Schmerz im Rahmen einer Polyneuropathie bei AIDS.

Die Nachweisdauer von THC beträgt in Abhängigkeit vom Konsum zwei bis 35 Tage im Urin beziehungsweise etwa 12 Stunden im Blut. Der Nachweis im Urin erfolgt meist über die THC-Metabolite THC-Carbonsäure und 11-Hydroxy-THC. Neben der vergleichsweise aufwendigen LC/MS-Methode existieren für den Nachweis von THC-Metaboliten im Harn eine Reihe von Immunassay-Tests wie etwa Radioimmunassay (RIA), Enzyme-multiplied Immunoassay Technique (EMIT), CEDIA ("cloned enzyme donor immunoassay") und FPIA ("fluorescence polarization immunoassay"). Um die Zahl falsch-positiver Ergebnisse mit diesen Tests zu reduzieren, empfiehlt die US-amerikanische Substance Abuse and Mental Health Services Administration (SAMHSA) einen Cutoff-Wert von 50 ng/mL. Zur hochspezifischen und hochsensitiven Quantifizierung der THC-Carbonsäure im fg-Bereich kann die GC/MS-Methode eingesetzt werden. Dabei werden hochfluorierte Derivate, wie etwa das THC-COOH-HFBA-PFPOH-Derivat unter Verwendung des deuterierten Derivats als internem Standard nach dem Prinzip der Isotopenverdünnungsanalyse mit der NCI-Technik (Negative Chemische Ionisation) vermessen. Diese Methodik vermeidet die oben geschilderten Probleme der falsch-positiven oder falsch-negativen analytischen Ergebnisse, die bei Enzymimmunassays (ELISA) immer wieder beobachtet werden, und findet daher auch in der forensischen Analytik bei Schiedsanalysen Verwendung.

Falsch-negative Ergebnisse können etwa durch verdünnte Harnproben verursacht werden, zum Beispiel bei einer Verdünnung "in vivo" durch vermehrte Flüssigkeitszufuhr. Über die Verdünnung des Harns können der Kreatinin-Gehalt und die Osmolalität Anhaltspunkte bieten, jedoch herrscht Uneinigkeit darüber, ab welchem Kreatinin-Wert eine Harnprobe als „unverdünnt“ gilt.

Falsch-positive Ergebnisse wurden bei einigen intensivmedizinisch behandelten Patienten berichtet, außerdem bei Personen, welche den Cannabiskonsum zwar aufgegeben haben, jedoch mehr Sport betreiben: Da THC im Fettgewebe gespeichert wird, können beim Abbau von Fettreserven THC-Metabolite freigesetzt werden.

Besondere Vorsicht ist bei der Interpretation von Haaranalysen geboten. Wie neuere Untersuchungen zeigen, können positive Messwerte nicht zwangsläufig mit einem aktiven Cannabiskonsum in Verbindung gebracht werden.

Δ9-THC ist in Deutschland in die des Betäubungsmittelgesetzes (BtMG) als ein "verkehrsfähiges, aber nicht verschreibungsfähiges Betäubungsmittel" eingestuft. Das zu medizinischen Zwecken verwendete (–)-Δ9-"trans"-THC (Dronabinol), im Handel als Rezeptursubstanz oder als Fertigarzneimittel "Marinol" (Einzelimport aus USA oder Kanada möglich gemäß § 73 Abs. 3 AMG), hingegen ist "verkehrs- und verschreibungsfähig" nach . Isomere des Δ9-THC wie etwa Δ6a-, Δ6a(10a)-, Δ7-, Δ8-, Δ9(11)- und Δ10-THC sind nicht verkehrsfähig (). In Deutschland wird Dronabinol von "Bionorica Ethics" und "THC Pharm", zwei Tochterunternehmen der Bionorica SE, produziert. Dronabinol-haltige Fertigarzneimittel sind bisher in Deutschland nicht zugelassen.

Die gesetzlichen Krankenkassen (z. B. AOK) übernehmen nicht regelhaft die Kosten der Medikation, die im Einzelfall Kosten bis hin zu 800 Euro pro Monat verursachen kann, auch wenn diese Form einer Therapie oft der letzte Ausweg für diverse Krankheitsbilder ist und sein könnte.

THC wirkt auf das Zentralnervensystem, deshalb sollte nach dem Konsum auf das Benutzen von Maschinen und das Führen von Fahrzeugen verzichtet werden. Die Polizei kann bei Fahrerkontrollen mit einem Schweiß-, Speichel-, Haar- oder Urintest oder durch Untersuchung des Blutes auch längere Zeit nach dem Konsum Spuren von THC nachweisen. Die Nachweisdauer hängt vor allem vom jeweiligen Konsummuster (Dauer, Art der Einnahme, Frequenz, Dosis) ab und kann im Urin zwischen einer Woche und zwei Monaten betragen. Zurzeit ist die gesetzliche Situation allerdings noch nicht eindeutig beschlossen, es drohen aber Geldbußen von mindestens 500 Euro, Fahrverbote bis zu drei Monaten und vier Punkte in Flensburg. Die Polizisten vor Ort können nur orientierende Vortests durchführen, die Blutprobe wird später in einem Labor untersucht und die Menge an THC und seiner Abbauprodukte bestimmt. Aus rechtlicher Sicht handelt es sich um eine Ordnungswidrigkeit, sobald THC im Blut nachweisbar ist.

Im Beschluss des Bayerischen VGH vom 25. Januar 2006, Az. 11 CS 05.1711, steht: „Der derzeitige medizinisch-naturwissenschaftliche Erkenntnisstand rechtfertigt es nicht, bereits ab einer THC-Konzentration von 1,0 ng/ml im Blut eines Kraftfahrzeugführers eine Erhöhung des Risikos für die Verkehrssicherheit als derart gesichert im Sinne des § 11 Abs. 7 FeV (Fahrerlaubnis-Verordnung) anzusehen, dass dem Betroffenen ohne weitere Sachverhaltsaufklärung die Fahrerlaubnis zwingend zu entziehen ist. Bei gelegentlichem Konsum von Cannabis und Fahren mit einer THC-Konzentration zwischen 1,0 und 2,0 ng/ml ist vor einer etwaigen Entziehung der Fahrerlaubnis gemäß § 14 Abs. 1, S. 4 FeV ein medizinisch-psychologisches Gutachten einzuholen.“ (FeV § 11 Abs. 7, FeV § 14 Abs. 1, S. 4, StVG § 3 Abs. 1) Dies gilt aber nur, wenn keine Fahrfehler gemacht wurden. In vielen Fällen ordnet die Verwaltungsbehörde (Fahrerlaubnis) eine Überprüfung der Kraftfahreignung (MPU) zum Nachweis der Kraftfahrtauglichkeit an.

In der Schweiz muss für eine Therapie mit Dronabinol vom Arzt eine patientenspezifische Ausnahmebewilligung beim Bundesamt für Gesundheit (BAG) beantragt werden. Da Dronabinol keine Pflichtleistung der Krankenkassen ist, muss eine Kostenübernahme im Vorfeld und im Einzelfall abgeklärt werden; bei manchen Kassen braucht es dafür eine Zusatzversicherung. Etwa 500 Patienten mit Angststörungen, Epilepsie oder Morbus Crohn profitierten von der ärztlichen Verschreibung von Cannabidiol; Multiple Sklerose-Betroffene benutzen das rezeptpflichtige Medikament Sativex, das CBD und THC enthält, gegen Verkrampfungen.

In der Schweiz ist seit Anfang 2005 mit einem Drogenschnelltest (engl. „Drug Wipe“) in Verkehrskontrollen zu rechnen.

Seit 2011 ist in der Schweiz Cannabisanbau mit einem THC-Gehalt bis zu 1 % zulässig, dies vor allem wegen der natürlichen Schwankungen in den Hanfpflanzen; zuvor lag der Grenzwert bei 0.3 %, der aber nicht regelmässig eingehalten werden konnte. Seither nimmt der industrielle Hanfanbau in der Schweiz zu.

Die Methode des THC-Nachweises im Straßenverkehr ist umstritten, da der Konsument nicht unter direktem Einfluss der Droge stehen muss, sondern es für einen positiven Test ausreicht, Tage und Wochen zuvor THC konsumiert zu haben. Dies gilt für alle Urintests, da diese nicht direkt THC nachweisen, sondern ein Abbauprodukt des THC, die Tetrahydrocannabinolsäure (THC-COOH, auch THC-Carbonsäure genannt). Die Cannabinolsäure hat keine berauschende Wirkung mehr. Sie wird allerdings relativ langsam und je nach Konstitution verschieden schnell aus dem Körper ausgeschieden und ist somit längere Zeit, manchmal sogar über Wochen im Urin nachweisbar. Die derzeit zuverlässigste Nachweismethode ist die Gaschromatographie mit Massenspektrometrie-Kopplung (GC/MS) von Derivaten (häufig als Trimethylsilyl-Derivate) der THC-Carbonsäure. Im Gegensatz dazu weisen Speichel- und Schweißtests wie die oben zitierten Drogentests THC mit ausreichender Empfindlichkeit direkt nach.

Konventionelles, in Mitteleuropa gewachsenes Freiland-Marihuana enthält im Schnitt rund 6 % THC, während unter Kunstlicht gewachsenes, speziell auf hohen THC-Gehalt gezüchtetes Marihuana (fälschlich auch als „Genhanf“ bekannt) einen Wirkstoffgehalt von etwa 20 % vorweisen kann. In den USA begann man in den 1970er-Jahren mit solchen Züchtungen; insbesondere in den Niederlanden setzte man diese seit den 1980er-Jahren fort, so dass der durchschnittliche THC-Gehalt des sogenannten „Nederwiet“ zuletzt (2004) bei etwa 20 % lag. Dennoch sind Meldungen über angeblich bis zu 50-fach erhöhte THC-Gehalte als maßlos übertrieben zu betrachten. US-amerikanische Forscher wiesen darauf hin, dass die als zum Vergleich angegebenen äußerst niedrigen Werte für in den 60er/70er-Jahren beschlagnahmtes Cannabis (z. T. unter 1 %) darauf zurückzuführen sein dürften, dass seinerzeit die ganzen Pflanzen inklusive Stängel und Blätter analysiert wurden, während heute nur die tatsächlich konsumierten Blütenstände untersucht werden.

Haschisch enthält im Schnitt zwischen 5 und 10 % THC, wobei – wie auch bei Marihuana – die Spanne sehr groß sein kann: Hochwertiges Haschisch kann ebenfalls über 20 % THC enthalten. Das in den Niederlanden aus hochwertigem Marihuana hergestellte Haschisch (welches jedoch nur einen sehr kleinen Marktanteil hat) enthält mitunter bis zu 40 % THC. Eine Studie der Universität Leiden aus dem Jahre 2006 untersuchte elf Cannabiskraut-Proben aus niederländischen Coffee-Shops, der THC-Gehalt lag zwischen 11,7 und 19,1 %. Zwei Vergleichsproben von Cannabiskraut aus niederländischen Apotheken enthielten 12,2 beziehungsweise 16,5 % THC. Das auf dem Schwarzmarkt eher selten erhältliche Haschischöl kann je nach Produktionsweise bis zu 90 % THC enthalten.




</doc>
<doc id="11163" url="https://de.wikipedia.org/wiki?curid=11163" title="Geodreieck">
Geodreieck

Das Geodreieck (eigentlich Geometrie-Dreieck) ist eine Kombination aus Lineal und Winkelmesser in Form eines rechtwinkligen, gleichschenkligen Dreiecks. Heute ist es vor allem ein Hilfsmittel für den Zeichen- und Mathematikunterricht, das dort speziell im Teilbereich Geometrie zum Messen und Zeichnen von Winkeln genutzt wird und das Zeichnen paralleler Geraden erleichtert. Für technische Anwendungen werden besonders hochwertige Geodreiecke gefertigt, die unter dem Namen TZ-Dreieck ("TZ" für Technisches Zeichnen) im Handel sind.

Geodreiecke gibt es mit und ohne Griff auf der Oberseite, mit und ohne erhabene Punkte an der Unterseite (sog. Tuschenoppen) sowie in unterschiedlichen Größen, die nach der Länge der Hypotenuse unterschieden werden. Diese längste Seite des Geodreiecks wird auch "Linealkante" genannt; sie trägt eine Zentimetereinteilung mit dem Nullpunkt in der Mitte. Dort beginnt die senkrecht zur Linealkante eingezeichnete "Mittellinie", die das Zeichengerät in zwei Hälften teilt und die Höhe darstellt, mit deren Hilfe sich rechte Winkel genau zeichnen lassen. In das Dreieck sind zur Linealkante parallele Linien eingearbeitet. Entlang der Schenkel des Dreiecks sind Markierungen im Abstand eines Winkelgrades angebracht, die Gradeinteilung läuft von 0° bis 180° bzw. 180° bis 0°. Die Winkelskala erlaubt die Konstruktion eines Winkels mit der Genauigkeit etwa eines halben Grades.

Gezeichnet wird entlang der Linealkante. Insbesondere ist der rechte Winkel am Scheitelpunkt der Katheten des Geodreiecks zum Zeichnen ungeeignet; damit kann allenfalls die Rechtwinkeligkeit schnell überprüft werden. Zum Zeichnen von Orthogonalen und Loten dagegen legt man die Mittellinie auf die gegebene Gerade und zeichnet entlang der Linealkante. Beim Zeichnen von anderen Winkeln verfährt man entsprechend: Nullpunkt und Winkelmarkierung liegen auf dem gegebenen Schenkel, der freie Schenkel wird an der Linealkante gezeichnet.

Auf einigen Geodreiecken sind neben dem rechten und den halbrechten Winkeln die Winkel 7° und 42° (bzw. 138° und 173°) besonders markiert. Dies erleichtert eine axonometrische Darstellung nach ISO 5456-3 (Dimetrie).

Ein dem Geodreieck ähnliches Instrument wurde von den ägyptischen Geometern zu Beginn des dritten Jahrtausends v. Chr. als sog. Konstruktions-Remen verwendet für die nach den jährlichen Nilschwemmen notwendigen Feldvermessungen.

Das heute übliche Geodreieck aus durchsichtigem Kunststoff (PMMA oder PVC) wurde im Jahr 1964 von der Firma "Aristo" entwickelt.

Ein Geodreieck mit Griff und anderer Winkeleinteilung wird im Kartenbesteck für Aufgaben der Navigation benutzt und dort Kursdreieck genannt.

Geodreiecke werden auch zusammen mit einem Abschiebedreieck oder an einer Reißschiene eingesetzt.


</doc>
<doc id="11164" url="https://de.wikipedia.org/wiki?curid=11164" title="Arthur Holmes">
Arthur Holmes

Arthur Holmes (* 14. Januar 1890 in Gateshead; † 20. September 1965 in London) war ein britischer Geologe.

Als einer der ersten Geowissenschaftler schlug Holmes vor, die Anfang des 20. Jahrhunderts entdeckte Radioaktivität als Hilfsmittel zur Geochronologie – der zeitlichen Einordnung geologischer Epochen – anzuwenden. Seine 1911 mit einfachsten Mitteln durchgeführten Messungen (etwa 600 Millionen Jahre für den Beginn des Kambrium) liegen relativ nahe am heute gültigen Wert von 541 Millionen Jahren. <br>

Aufsehen erregte auch sein Buch "The Age of the Earth" (1913), in dem er archäischen Gneisen das damals noch unvorstellbare Alter von 1,5 Mrd. Jahren zuschrieb. 

Etwa 1930 schlug er einen Mechanismus zur Erklärung der von Alfred Wegener entwickelten Theorie der Kontinentaldrift vor: Konvektive Wärmeströme im Erdinneren erzeugen genügend Kraft, um die Erdplatten zu bewegen. Da Holmes keine experimentellen Methoden zur Überprüfung dieser Hypothese zur Verfügung standen, nannte er seinen Vorschlag „Spekulation“; er kann zu den Pulsationshypothesen der Geophysik gerechnet werden. Heute gilt seine Annahme als sehr ähnlich mit jenen Prozessen, mit denen die moderne Theorie der Plattentektonik arbeitet.

Der Lehrbuchautor László Egyed ordnet Holmes’ Konvektionsmodell hingegen eher den Expansionstheorien der Erde zu: Das unter dem Urkontinent Pangäa aufströmende Magma habe diesen zerrissen und beim Abströmen die Gebirgswurzeln ausgestaltet; nach Aufhören der Strömungen hätten sich die alpidischen Gebirge beiderseits des Pazifik gehoben.

Holmes war Professor für Geologie an der Universität Durham und in der Nachfolge von Thomas John Jehu, 1943, bis zu seinem Tod im Jahr 1965 Regius Professor of Geology an der Universität Edinburgh. Sein Lehrbuch "Principles of Physical Geology" gilt auch heute noch als Standardwerk. 1934 wurde er in die American Academy of Arts and Sciences gewählt. 1956 erhielt er die Penrose-Medaille der Geological Society of America und die Wollaston-Medaille der Geological Society of London.

Die Arthur Holmes Medal der European Geosciences Union und das "Arthur Holmes Isotope Geology Laboratory" am "Department of Earth Sciences" der University of Durham sind nach ihm benannt. Darüber hinaus tragen der Holmes Summit, ein Berg im ostantarktischen Coatsland, und die Holmes Hills, eine Gruppe von Bergrücken und Nunatakker im westantarktischen Palmerland, seinen Namen.




</doc>
<doc id="11166" url="https://de.wikipedia.org/wiki?curid=11166" title="Abendmahl">
Abendmahl

Abendmahl steht für:
Abendmahl, Letztes Abendmahl oder Das letzte Abendmahl steht für folgende Kunstwerke:
Siehe auch:


</doc>
<doc id="11168" url="https://de.wikipedia.org/wiki?curid=11168" title="Bulgarische Sprache">
Bulgarische Sprache

Die bulgarische Sprache (bulgarisch български език "bǎlgarski ezik" ) gehört zur südslawischen Gruppe des slawischen Zweiges der indogermanischen Sprachen. Gemeinsam mit der mazedonischen Sprache bildet sie innerhalb der südslawischen Gruppe die Untergruppe der "ostsüdslawischen Sprachen".

Die bulgarische Sprache wird von rund 8 Millionen Menschen gesprochen; vor allem in Bulgarien (ca. 7,72 Millionen), aber auch in anderen Staaten Südost- und Osteuropas, in Griechenland (1970: 20.000), Rumänien (1970: 13.000), Mazedonien, Moldau (2005: 40.000), Ukraine (2001: 205.000), Serbien (1991: 25.200), Weißrussland, der Slowakei (2001: 1.176) und der Türkei (2001: 30.000 sog. Pomaken).

Die bulgarische Sprache ist eine der ältesten dokumentierten slawischen Sprachen. Ihre historische Entwicklung kann man in drei Perioden festhalten:

Die Periode der "altbulgarischen Sprache" umfasst die Zeit zwischen der Übernahme der slawischen Sprache als offizielle Sprache im Ersten Bulgarischen Reich und dessen Fall 1018 unter byzantinische Herrschaft. Einige Linguisten sehen jedoch den Anfang der Periode mit der Erschaffung des ersten slawischen Alphabets, der Glagoliza im Jahre 862 durch Kyrill Philosoph. In diese Periode des "Goldenen Zeitalters der bulgarischen Kultur" fällt auch die Entstehung des kyrillischen Alphabets am Hofe der bulgarischen Zaren in Preslaw. Ein weiteres Zentrum bildete Ohrid, das sich zu jener Zeit im westlichen Teil des bulgarischen Reiches befand und einen Großteil der altbulgarischen Literatur hervorbrachte. Wegen der Verbreitung der altbulgarischen Sprache und Kultur auf die anderen slawischen Völker spricht man von dem „Ersten Südslawischen Einfluss“ und von der „altkirchenslawischen“ Sprache.

Die Periode der "Mittelbulgarischen Sprache" umfasst die Zeit zwischen der Restaurierung des bulgarischen Reiches bis zu dessen Unterwerfung durch die osmanischen Türken. Die Sprache mit den durch die Tarnower Schule in der "Orthographie von Tarnowo" festgelegten grammatischen Regeln wurde zur Grundlage der weiteren sprachlichen Entwicklung in den Gebieten der heutigen Staaten Rumänien, Moldau und Serbien, Ukraine und Russland, so dass man von einem „Zweiten Südslawischen Einfluss“ auf diese Länder spricht. Die Sprache des Zweiten Bulgarischen Reichs wird auch heute immer noch in den slawischen orthodoxen Kirchen als Liturgiesprache genutzt, weswegen sie auch Kirchenslawisch genannt wird.

Vom Ende des 14. bis ins 16. Jahrhundert wurde Bulgarisch von den walachischen Fürsten als Kanzleisprache verwendet.

Die Neubulgarische Epoche wird zunächst durch die sogenannten Damaskini des 17. und 18. Jahrhunderts belegt. Dabei handelte es sich primär um die übersetzten griechischen Predigten des Damaskenos Studites, die mehrfach in das Bulgarische übertragen wurden und in denen sich auch die wichtigsten Merkmale fast aller neubulgarischen Mundarten wiederfinden.

Während der Bulgarischen Nationalen Wiedergeburt eingetretene Veränderungen in der Entwicklung der bulgarischen Kultur und des Bildungswesens begründeten die Notwendigkeit einer weltlichen Bildung sowie einer in der Volkssprache geschriebenen Literatur. Obwohl die Anfänge der bulgarischen Wiedergeburt in Makedonien liegen, spielten die Ostbulgaren bei der Herausbildung der Standardsprache eine führende Rolle.

Durch eine reichhaltige Publikationstätigkeit in den ostbulgarischen Dialekten kam es nach anfänglichem Überwiegen westbulgarischer Mundarten zur allmählichen Dominanz der Ersteren, die durch die Herkunft der Publizisten bedingt war. So betrachtete Wasil Aprilow, einer der größten Förderer des Schul- und Kirchenwesens, wie Petar Beron und Najden Gerow bei der Herausbildung der "neubulgarischen Sprache" die ostbulgarischen Dialekte als Grundlage der Bildung einer einheitlichen Schriftsprache. Die Nähe dieser Dialekte zum Russischen ebnete den kulturellen Einfluss Russlands auf die Bulgaren. Einen weiteren Schritt in diese Richtung stellt das erste bulgarische Schulbuch dar: Die "Fibel mit unterschiedlichen Belehrungen", die in den für die Heimatstadt des Autors typischen ostbulgarischen Dialekten geschrieben wurde, wurde von Petar Beron 1824 in Kronstadt publiziert.

Ihnen gegenüber standen andere Förderer, wie die Brüder Miladinowi aus Struga. Ihr Werk "Bulgarische Volkslieder" erschien 1861 in Zagreb und basierte auf den westbulgarischen Dialekten. Auch der Gelehrte Neofit Rilski verwendete zunächst die westbulgarischen Dialekte, versuchte jedoch in seiner Grammatik (1835), die ost- und westbulgarischen Dialekte zu vereinen. Josif Kowatschew setzte sich für den zentralbulgarischen Dialekt ein, der als Bindeglied fungieren sollte. Im Laufe des 20. Jahrhunderts gewann dennoch das Westbulgarische einen stärkeren Einfluss auf die Sprache.

Das Bulgarische darf nicht mit dem "Protobulgarischen" verwechselt werden, das eine Turksprache (nach anderen Theorien eine nordostiranische Sprache) war. Heute existieren jedoch immer noch einige Wörter in der neubulgarischen Sprache, die der protobulgarischen entstammen, wie z. B. "Тояга/Tojaga" (Stock) oder "Баща/Baschta" (Vater). Außerdem gibt es einige wenige Wörter, die dem thrakischen Substrat entstammen wie katerja se (klettern) von thrakisch katerdass und kacna, kacvam (sich niederlassen).

Die bulgarischen Dialekte sind in den letzten hundert Jahren umfassend erforscht und dokumentiert worden. Traditionell werden sie entlang der Aussprache des altbulgarischen ›jat‹ (auch jat-Grenze genannt) in zwei Gruppen unterteilt: "Ostbulgarisch" (Aussprache des ›jat‹ "*ě" als und "e": "bjal" – "beli") und "Westbulgarisch" (Aussprache des ›jat‹ als : "bel" – "beli"). Davon abgesehen definieren einige Linguisten die "rupzische" Mundart als eine dritte Dialektgruppe, die eigene Parallelen zum Altbulgarischen, sowie zu benachbarten türkischen und griechischen Dialekten als Merkmale aufweist. Die Dialektgruppen gliedern sich in folgenden Mundarten:




Verschiedene phonetische, akzentologische, morphologische und lexikalische Isoglossen verbinden die westbulgarischen mit den östlich der Jat-Grenze gesprochenen Dialekten des Rhodopen und Strandscha-Gebirges bis hin zum Schwarzen Meer. Diese Dialekte haben mehrere gemeinsame Charakteristika, weswegen sie von einigen Forschern als eine dritte Dialektgruppe, das Rupzische, definiert werden. Zu ihren Eigenschaften gehört der Reflex des urslawischen "*ě" als offenes e, des urslawischen д und ъ als offenes "*ô" und der sogenannte dreifache Artikel. Die Formen für Singular und Plural werden in diesen Mundarten zum Teil aus Kasusformen für den Dativ abgeleitet. Ein weiteres Merkmal ist das Aufbewahren zahlreicher lexikalischer Archaismen, die oft Parallelen zum Altbulgarischen aufweisen, zu denen sich jedoch kein Äquivalent in den übrigen bulgarischen Mundarten findet. Sprecher der Rhodopenmundarten sind einerseits christliche, andererseits muslimische Bulgaren (Pomaken). Der auf die Mitte des 17. Jahrhunderts zurückgehende Glaubensunterschied hat sich kaum auf die Mundarten ausgewirkt, betrifft jedoch insbesondere den Wortschatz im religiösen Bereich und die arabisch-türkischen Vornamen der Muslime.

Die Rhodopen- und rupzischen Mundarten reichten vor dem Ersten Weltkrieg über die heutige bulgarische Staatsgrenze hinaus. So waren die thrakischen Dialekte in Verbindung vor allem mit türkischen und zum Teil auch mit griechischen Dialekten bis zur Küste der Ägäis verbreitet.

Die Übergangsdialekte weisen Merkmale jeweils zweier Sprachen auf (Serbisch und Bulgarisch bzw. Mazedonisch und Bulgarisch) und werden über das Bulgarische Dialektkontinuum determiniert. Die Zugehörigkeit dieser Mundarten, die sich auf der anderen Seite der Grenze in Serbien und Mazedonien fortsetzen, war in der Vergangenheit unter serbischen bzw. ist heutzutage unter mazedonischen und bulgarischen Linguisten umstritten. Während die einen die bulgarische Sprachgrenze weit nach Westen bis nach Niš, Prizren und Ohrid zogen, ziehen die anderen die Sprachgrenze im Osten bis nach Sofia und das gesamte Pirin-Gebirge hinaus (Mazedonismus). In Bulgarien ordnet man aus diesem Grund das Mazedonische bisweilen als Dialekt dem Bulgarischen zu. Da es kein hinreichendes linguistisches Abstandskriterium für diese Mundarten zu den jeweiligen Sprachen gibt, kann nur das Kriterium der nationalen Selbstidentifikation der Sprecher und der von ihnen anerkannten Standardsprache herangezogen werden. Danach wären die Mundarten westlich der heutigen bulgarischen Staatsgrenze als Serbisch bzw. Mazedonisch und jene östlich der Landesgrenze als Bulgarisch zu bezeichnen bzw. als Dialekte dem Bulgarischen zuzuordnen.

Die dem Bulgarischen nächstverwandte Sprache ist das Mazedonische.

Der Wortschatz besteht überwiegend aus slawischen Erbwörtern; Lehnwörter entstammen vor allem dem Griechischen und dem Türkischen. Seit dem 19. Jahrhundert gab es immer wieder Bestrebungen, türkische Wörter durch Slawismen, die vorwiegend aus dem Russischen stammen, zu ersetzen. Auswirkungen hatten diese Bemühungen vor allem auf die Schriftsprache; die Umgangssprache ist nach wie vor reich an türkischen Elementen, wobei der Großteil davon (z. B. Диван/Diwan für Sofa, Тефтер/Tefter für Notizbuch; Пехливан/Pehlivan für Ringer) arabischen und persischen Ursprungs sind. Im technischen Bereich sind viele französische und deutsche Wörter übernommen worden (siehe unten), sowie in letzter Zeit Anglizismen.

Das Bulgarische wird in der bulgarischen Variante der kyrillischen Schrift geschrieben.
Das bulgarische Alphabet ("Азбука"/"Asbuka") umfasst 30 Buchstaben in folgender Reihenfolge:

In alten Texten können darüber hinaus die Buchstaben Ѣ/ѣ ("Jat"; Aussprache in der Regel je nach Kontext wie "е" oder "я"; z. B. "голѣм" ‚goljam‘ ↔ "голѣми" ‚golemi‘; ursprüngliche Aussprache ) sowie Ѫ/ѫ ("Großes Jus" ‚голям юс‘; Aussprache in der Regel ; ursprüngliche Aussprache ; der Buchstabe sollte nicht mit dem kyrillischen Buchstaben "Kleines Jus" Ѧ/ѧ verwechselt werden) auftauchen. Im heutigen Bulgarisch werden diese alten Zeichen jedoch nicht mehr verwendet; sie wurden im Zuge einer Rechtschreibreform 1945 abgeschafft.

Für die Kleinbuchstaben werden häufig die so genannten "kursiven" Formen auch in der aufrechten Schrift verwendet. Da sich diese von den (russischen) Standardformen teilweise stark unterscheiden, die auch in den meisten Lexika erscheinen, entstehen für Personen ohne Kenntnisse slawischer Sprachen (Touristen etc.) oft Probleme beim Entziffern etwa von Straßenschildern.

Beim Buchstabieren wird der Lautwert der Vokale beibehalten. Den Konsonanten wird stets der Laut ъ (Aussprache: ) nachgestellt; Ausnahmen sind das й, welches als "i kratko" (и кратко, ʿkurzes iʾ) sowie das ь, welches als "er malăk" (ер малък, ʿkleines Jerʾ) buchstabiert wird. Somit ergibt sich im Bulgarischen folgendes Buchstabieralphabet:

Dies stellt eine deutliche Abweichung vom Deutschen, aber auch vom (ebenfalls in kyrillischer Schrift geschriebenen) Russischen dar. Beispiele:


Die meisten Buchstaben werden im Großen und Ganzen wie im Deutschen bzw. wie ihre Entsprechungen im Deutschen ausgesprochen. Die Hauptunterschiede zur standarddeutschen Aussprache liegen


Palatalisierungen treten nicht so häufig auf wie beispielsweise im Russischen. Starke Unterschiede zwischen palatalisierter und nicht palatalisierter Aussprache sind nur bei wenigen Buchstaben deutlich hörbar, z. B. bei "n" und "l":


Wie im Deutschen und in anderen slawischen Sprachen gibt es eine Auslautverhärtung.

Die bulgarische Grammatik unterscheidet sich in vielen Punkten von anderen slawischen Sprachen. Auch benachbarte Sprachen, wie z. B. Albanisch oder Rumänisch, welche selbst keine slawischen Sprachen sind, weisen teilweise die gleichen Eigenheiten auf. Deshalb werden diese Sprachen auch unter dem Begriff Balkansprachen zusammengefasst, obwohl sie nicht nahe miteinander verwandt sind. Man spricht in diesem Zusammenhang von einem Sprachbund.

Unter den slawischen Sprachen gibt es Artikel nur im Bulgarischen und im nahe verwandten Mazedonischen. Die bestimmten Artikel werden im Unterschied zu vielen anderen Sprachen an das Nomen (bzw. das erste Wort seiner Nominalgruppe) angehängt (postponierte Artikel). Im Bulgarischen gibt es ferner nur sehr schwach ausgeprägte Kasūs, außer bei Pronomina sowie bei den Artikelformen der Maskulina treten sie nicht in Erscheinung. In den wenigen Fällen, wo sie sichtbar werden, unterscheidet man Nominativ, Dativ und Akkusativ; der Genitiv wird durch Präposition "на"+Dativ ersetzt (vergleichbar zum im Deutschen nur umgangssprachlichen Ersatz des Genitivs durch "von"+Dativ).

Bei der Kommunikation im Freundes- und Familienkreis findet der Vokativ Verwendung.

Das Bulgarische verfügt über eine sehr ausgeprägte Formenvielfalt bei den Verben. Man unterscheidet neun verschiedene Zeitformen: Präsens, zwei Futurformen ("Futurum" und "Futurum exactum"), vier Vergangenheitsformen (Imperfekt, Aorist, Perfekt, Plusquamperfekt) sowie zwei Mischformen aus Vergangenheit und Futur (s. u.), wobei Aorist und Imperfekt als sog. "synthetische" Formen, Perfekt und Plusquamperfekt als "periphrastische" Formen der Vergangenheitstempora bezeichnet werden. Die synthetischen Formen sind nicht zusammengesetzt, wohingegen die periphrastischen Formen meist zusammengesetzt gebildet werden. Beispiel: "аз четох" [Aorist] „ich habe (einmal) gelesen“ und "аз четях" [Imperfekt] „ich las“ sind nichtzusammengesetzte Vergangenheitsformen; hingegen sind "аз съм чел" [Perfekt] „ich habe gelesen“ und "аз бях чел" [Plusquamperfekt] „ich hatte gelesen“ genauso wie im Deutschen zusammengesetzte Vergangenheitsformen, die stets unter Verwendung des Hilfszeitworts съм ‚sein‘ gebildet werden. Wie deutlich wird, ist die Wiedergabe des (im Deutschen nicht existenten) Aorists überaus schwierig, da die Einmaligkeit der Handlung im deutschen Sprachgebrauch nur umschrieben werden kann und keine eigene grammatikalische Kategorie darstellt. Meist wird der Aorist im Deutschen einfach mit dem Perfekt wiedergegeben.

Darüber hinaus gibt es zwei „Mischformen“ aus Zukunft und Vergangenheit, nämlich das "Futurum praeteriti" sowie das recht ungebräuchliche "Futurum exactum praeteriti". Mit den letzteren beiden Formen lässt sich ausdrücken, dass man in der Vergangenheit davon ausgegangen ist, dass etwas geschehen würde; eine Entsprechung im Deutschen wäre ungefähr eine Konstruktion wie "„Ich dachte, dass er es erledigen würde“" oder "„Er wollte es erledigen“" (Futurum praeteriti) bzw. "„Ich dachte, er würde es mittlerweile erledigt haben“" oder "„Er wollte es schon bis gestern erledigt haben“" (Futurum exactum praeteriti). Aufgrund der Tatsache, dass die beschriebene Handlung möglicherweise doch nicht ausgeführt wurde, nehmen diese eigentlich indikativischen Formen oft auch die Funktion des Konjunktivs ein.

Wie andere slawische Sprachen macht auch das Bulgarische in (fast) allen Zeitformen von der grammatikalischen Kategorie des Verbalaspektes Gebrauch. Somit existieren rein rechnerisch 9·2 = 18 verschiedene Kombinationen aus Aspekt und Tempus. Allerdings kommen einige Aspekt-Tempus-Paare nur sehr selten vor (z. B. Imperfekt perfektiver Verben).

Das sogenannte „Aspektparadigma“ im Bulgarischen beruht auf der Tatsache, dass man eine Handlung auf Seiten des Sprechers auf zwei verschiedene Arten betrachten kann (das Wort "Aspekt" leitet sich vom lateinischen "aspicere" ‚erblicken, anschauen, betrachten‘ ab):


Beispiele für Aspektpaare:

Die Formenbildung der Aspektpaare ist im Bulgarischen sehr divers und komplex (im Gegensatz zum Russischen). Um aus imperfektiven Verben perfektive Formen zu generieren, lassen sich circa 18 mögliche Präfixe und Suffixe identifizieren.

Die Zweiteilung der Verben in perfektiv und imperfektiv setzt sich auch in den Tempora fort und muss dort der entsprechenden Bildungsweise der einzelnen Zeitformen angepasst werden, was zu einer fast unüberschaubaren Fülle unterschiedlicher Bildungsweisen von Konjugationsklassen und Konjugationsunterklassen führt. Hinzu kommt, dass bei manchen Verben nur eine der beiden Dublettformen existiert (man nennt diese Formen dann Imperfektiva tantum oder Perfektiva tantum). Weiterhin können oftmals perfektive Verben "sekundär" imperfektiviert werden, was zu Formen-Tripletts führen kann, z. B. пиша (imperfektiv) → напиша (perfektiv) → написвам (sekundär imperfektiv).
Im Deutschen können die meisten perfektiven Zeitformen (der Begriff hat hier nichts mit dem Tempus „Perfekt“ zu tun!) – wie der Aorist – aufgrund des fehlenden Aspektparadigmas in den germanischen Sprachgruppen nur meist bedeutungsneutral wiedergegeben werden (sofern die Übersetzbarkeit mit Wortzusätzen wie "einmal" oder "öfters" nicht funktioniert).

Die Verbalaspekte erweisen sich für den Nicht-Muttersprachler beim Erlernen einer slawischen Sprache im Allgemeinen als äußerst schwierig und führen unter anderem dazu, dass slawische Sprachen allgemein als relativ schwierig zu erlernen gelten.

Ebenfalls typisch für slawische Sprachen ist die Vielfalt an Partizipien: Partizip Präsens Aktiv, Aktivpartizip des Imperfekts, Aktivpartizip des Aorists, Passivpartizip Präsens, Passivpartizip des Aorist, Passivpartizip praeteriti, Adverbialpartizip, sowie der nur selten anzutreffende sogenannte „Restinfinitiv“.

Interessanterweise existiert, wie beim Neugriechischen, im Bulgarischen – ebenfalls im Gegensatz zu anderen slawischen und auch den meisten anderen indogermanischen Sprachen – kein Infinitiv. In Wortlisten wie beispielsweise Wörterbüchern wird an seiner Stelle normalerweise die 1. Person Singular Präsens Indikativ Aktiv verwendet (welche man dann als „Nennform“ des Verbes bezeichnet). Bei Satzkonstruktionen wie beispielsweise „Möchtest du "essen"?“ („essen“ im Deutschen im Infinitiv) wird stattdessen mit dem Wort "да" (da) das zweite Verb in konjugierter Form angeschlossen: „Искаш ли "да ядеш"?“ („Iskasch li da jadesch?“; wörtlich übersetzt ungefähr: „Möchtest du, "dass du isst"?“).

Als Verbmodi existieren neben Indikativ, Imperativ und Konditional (welcher ungefähr die Funktion des Konjunktivs im Deutschen übernimmt) auch der "Konklusiv" (zeigt an, dass man einen Sachverhalt aus einem anderen logisch erschließt), der Renarrativ (zeigt an, dass der Sprecher einen Sachverhalt nicht selbst erlebt hat, sondern dass er die Schilderung eines Dritten weitergibt, vergleichbar der indirekten Rede im Deutschen) sowie der "dubitative Renarrativ" (wie Renarrativ; allerdings zweifelt der Sprecher den Wahrheitsgehalt an).

Bei Entscheidungsfragen (d. h. Sätze, auf die eine ja/nein-Antwort erwartet wird) findet fast immer die Partikel "ли" (li) Verwendung. Sie tritt nur bei Entscheidungsfragen, jedoch nicht bei anderen Fragen auf, und wird typischerweise hinter das Verb oder aber einen dadurch besonders betonten Teil der Frage gesetzt. Beispiele:


Zunächst einige kurze Aussprachehinweise zur folgenden Tabelle:


Allgemeine Erklärung der Menschenrechte, Artikel 1:

Die Konsonanten werden härter ausgesprochen, lange Vokale kurz (siehe Bohrmaschine) und die einzelnen Silben werden oft anders betont.


Das Bulgarische zeigt zum Russischen und zu den meisten anderen slawischen Sprachen zahlreiche sprachliche Unterschiede, die aus dessen Zugehörigkeit zum Balkansprachbund resultieren, wie beispielsweise den beinahe vollständigen Verlust der Kasūs (Kasussynkretismus) oder die Existenz nachgestellter (postponierter) Artikel. Weiterhin gibt es im Bulgarischen sehr viel mehr Zeitformen als im Russischen.

Beim Alphabet ergeben sich einige kleine Unterschiede zum Russischen.


Die Rechtschreibung ist wesentlich einfacher:






</doc>
<doc id="11169" url="https://de.wikipedia.org/wiki?curid=11169" title="Monokel">
Monokel

Das Monokel, auch Einglas genannt, ist eine Sehhilfe, die im Gegensatz zur heute verwendeten Brille („Binokel“, Lorgnette) aus nur einem Glas besteht und am Auge eingeklemmt wird (im Gegensatz zum Einglas mit Stiel).

Das Monokel entwickelte sich aus dem Lesestein, einer geschliffenen Linse aus Quarz, besonders Bergkristall, oder Beryll, daher das deutsche Wort "Brille". Dieser wurde zum Vergrößern direkt auf das Schriftstück gelegt. Ab dem 14. Jahrhundert wurde die Linse vors Auge gehalten. Im 16. Jahrhundert entstand die Idee, die Linse durch den Augenlidmuskel direkt vor dem Auge festzuklemmen, um beide Hände frei zu haben.

Das im Deutschen verwendete Wort Monokel wurde im 19. Jahrhundert aus dem gleichbedeutenden französischen Wort "monocle" entlehnt, entstand aber ursprünglich als zweisprachiges Kunstwort aus "monos" für „allein, einzig“ und für „Auge“.

Es werden Monokel mit und ohne Galerie unterschieden. Als Galerie wird beim Monokel ein Klemmträgerrand bezeichnet, der aus zwei Stegen an der Ober- und Unterseite des Monokels besteht. Die Galerie ist jeweils bis zu fünf Millimeter vom Glas entfernt und soll neben der besseren Griffigkeit auch dazu dienen, das Glas auf Abstand von den Wimpern zu halten und vor Verschmutzung durch diese zu schützen. Monokel mit Galerie haben immer eine Fassung. Monokel ohne Galerie können mit und ohne Fassung geliefert werden. Bei Monokeln ohne Galerie und ohne Fassung ist das Glas rundum gerändelt, um leichter gehalten werden zu können. Um Monokel vor Beschädigung beim Herausfallen zu schützen, hängen sie gewöhnlich an einem dünnen Kettchen oder einem Band. Zur Uniform wurden Kette oder Band als Schlaufe um den Hals des Trägers gelegt und mit einem Schieber vor dem Kragenknopf zusammengezogen. Zum Zivilanzug war am Ende der Kette oder des Bandes ein Knopf angebracht, der in das Knopfloch am Revers der Anzugjacke eingeknöpft wurde.

Das Monokel war gegen Ende des 19. Jahrhunderts besonders in Deutschland und Großbritannien populär und galt als ein Statussymbol der höheren Gesellschaftsschichten. Besonders verbreitet war es in den Offizierkorps der Armeen dieser beiden Länder – und führte da zu Streitigkeiten. So verbot der spätere britische Kriegsminister Earl Kitchener Anfang des 20. Jahrhunderts den Soldaten des Heeres die Benutzung von Monokeln, weil er in ihnen einen „Auswuchs alberner Eitelkeit“ sah, die „eines Offiziers unwürdig ist.“

Einige Mediziner waren damals der Meinung, das Verzerren des Gesichts zum Festhalten des Monokels sei gesundheitsschädlich. Dagegen wurde gewöhnlich argumentiert, dass nur Personen Monokel tragen sollten, die es ohne solche Gesichtsverzerrungen tragen könnten. Dem wurde von Seiten der Optiker auch durch unterschiedliche Größen entsprochen.

Noch in den 1920er Jahren war in Großbritannien strittig, ob das Monokel zu den optischen Instrumenten zählte oder, zusammen mit der Brille, eine eigene Kategorie bildete. Es ging dabei um die im "Finance Act" von 1926 festgelegten Importzölle für optische Instrumente. Am 6. Oktober 1927 entschied ein Handelsausschuss, dass „Sehhilfen in der Umgangssprache wie auch in der normalen Diktion des Handels keine optischen Instrumente im gleichen Sinne [sind] und deshalb auch nicht in die gleiche Klasse wie optische Instrument fallen.“ Sie unterlagen damit weiterhin nicht den Zollbestimmungen für optische Geräte.

In Karikaturen seit dem ausgehenden 19. Jahrhundert ist das Monokel das stereotype Attribut des, meist adligen, preußischen (Reserve-)Offiziers.

In der Fernsehserie Ein Käfig voller Helden ist ein Monokel das Markenzeichen von Oberst Wilhelm Klink.

Im Film Eins, zwei, drei besiegeln OttoLudwig Pfiff (Horst Buchholz) und der Graf Waldemar von und zu Droste-Schattenburg (Hubert von Meyerinck) die Adoption durch den Monokeltausch.

Im Film Zeugin der Anklage führt Charles Laughton bei Tyrone Power den "Monokeltest" durch, indem er Power mittels reflektierten Lichts blendet und nervös machen will.

Das Markenzeichen von Roberto Rastapopoulos (einer Figur der "Tim-und-Struppi"-Comics) ist das Monokel, welches ihm bei Wutanfällen regelmäßig aus dem Gesicht fällt. "Kapitän Haddock" dagegen trägt es lediglich in "Die sieben Kristallkugeln" und verliert es auf jeder Seite durchschnittlich einmal.

Berühmte Monokel-Träger: Fritz Lang, Roda Roda, Erich von Stroheim, Richard Tauber, António de Spínola, Hans von Seeckt, Walter Model, Wilhelm Keitel

Kameraobjektive, die lediglich aus einer einzelnen Linse (üblicherweise eine Sammellinse) bestehen, werden ebenfalls Monokel genannt.




</doc>
<doc id="11171" url="https://de.wikipedia.org/wiki?curid=11171" title="Jüdischer Kalender">
Jüdischer Kalender

Der jüdische Kalender ("hebr." הלוח העברי ha-lu'ach ha-iwri) ist ein Lunisolarkalender, der im Jahr 3761 v. Chr. mit der Zählung beginnt. Die Monate sind wie bei einfachen Mondkalendern an den Mondphasen ausgerichtet. Neben einem Normaljahr mit 12 Mondmonaten (ordentlich 354 Tage lang) gibt es Schaltjahre mit 13 Mondmonaten (ordentlich 384 Tage lang) zur Angleichung an das Sonnenjahr. Die kalendarischen Ausnahmeregeln können zu einer Verlängerung oder Verkürzung der ordentlichen Jahreslängen um jeweils einen Tag führen.

Das jüdische Jahr beginnt heute im Herbst mit dem Tischri, der nach jüdischer Auffassung der Monat ist, in dem die Menschheit erschaffen wurde. In biblischer Zeit begann das Jahr mit dem Nisan im Frühjahr. Der Nisan ist der Monat der Erlösung, in dem die jüdischen Vorfahren aus Ägypten auszogen. Die Tatsache der Erlösung wird auch heute noch höher bewertet als die der Schöpfung, indem beim religiösen Gebrauch des jüdischen Kalenders der Nisan weiter als erster, der Tischri aber erst als siebter Monat des Jahres betrachtet wird.

Die Monatsnamen sind chaldäisch und stammen aus dem babylonischen Exil.

Der jüdische Kalender wurde in seiner Entwicklung sowohl von eigenen, israelitischen Traditionen geprägt als auch von Elementen fremder Kulturen, besonders während des babylonischen Exils. Die Systematik des heutigen jüdischen Kalenders beruht im Wesentlichen auf Festlegungen des Patriarchen Hillel II. aus dem Jahr 359 n. Chr., hat sich aber – insbesondere für die Zählung der Jahre – erst ab dem 11. Jahrhundert durchgesetzt. Aktuell ist er im Kreis angeordnet. Er beginnt immer im September oder Oktober.

Jeder hebräische Monat beginnt heute ungefähr bei Neumond. Im Altertum wurde der Monatsbeginn nach dem sichtbaren Beweis der „Geburt des Mondes“ durch Zeugen bestimmt. Der Patriarch Hillel II. legte den bis heute gültigen Kalender fest, damit nicht die Römer durch Verfolgung und Behinderung der Gerichte das jüdische Leben gefährdeten. Anhand dieses Kalenders weiß jeder im Voraus, welcher Monat 29 und welcher 30 Tage hat.

Der jüdische Kalender rechnet den Tag vom Abend zum Abend (, Gen. 1,5). Der Tag endete im früheren jüdischen Kalender, wenn mindestens drei „mittlere“ Sterne sichtbar wurden, die zu einem Himmelsdreieck verbunden werden konnten, wobei sich die Bezeichnung „mittlere“ auf Sterne erster und zweiter Größe bezieht.

Trafen die Boten des Gerichts in Jerusalem nicht rechtzeitig bei den in der Diaspora lebenden Juden ein, um ihnen das genaue Datum des Monatsbeginns mitzuteilen, feierten diese zur Sicherheit jedes in der Tora erwähnte Fest zwei Tage lang anstelle von nur einem, denn sie wussten ja nicht, ob der vorherige Monat 29 oder 30 Tage gehabt hatte. Um einem Irrtum vorzubeugen und um das Fest nicht zu entweihen, führten sie diesen Feiertag ein. Dieses ist der „zweite Feiertag in der Diaspora“ geworden, ein Brauch, der sich bis heute erhalten hat.

Nur der Versöhnungstag wird an einem einzigen Tag begangen, da man berücksichtigte, dass es dem Menschen schwerfällt, hintereinander 48 Stunden zu fasten. Das Neujahrsfest dauert auch im Land Israel zwei Tage, weil man selbst dort nicht immer wusste, ob der Monat Tischri nach 29 oder 30 Tagen beginnen würde. Zur Sicherheit hörte das Volk am Ende des 29. Elul auf zu arbeiten und „tat heilig“ wie an einem gewöhnlichen Feiertag. Um irgendwelche Irrtümer von vornherein auszuschließen, ordneten die jüdischen Gelehrten schließlich die zweitägige Dauer des Festes an. Trotz des später exakt bestimmten Kalenders feiert man in der Diaspora immer noch den „zweiten Feiertag“, mit der Begründung, es sei „Brauch der Vorfahren“.

Der jüdische Kalender gliedert sich in Jahre, Monate und Tage. Da dieser Kalender ein Lunisolarkalender (Mond-Sonnen-Kalender) ist, er sich also sowohl am Mondjahr wie auch am Sonnenjahr orientiert, verschieben sich die Monate im Vergleich zu reinen Sonnenkalendern, da zwölf Mondmonate nur 354 und nicht 365 Tage ergeben. Damit die Monate nicht wie bei reinen Mondkalendern durch das Sonnenjahr wandern, benötigt der jüdische Kalender eine Schaltregulierung zum Ausgleich. Um diesen Ausgleich zu dem um 11 Tage längeren Sonnenjahr zu schaffen, wird in einem Zyklus von 19 Jahren siebenmal ein dreißigtägiger Schaltmonat vor dem eigentlichen Adar hinzugefügt.

Während der Schaltmonat in jenem Jahr den Namen "Adar" übernimmt, erhält der 29 Tage umfassende eigentliche "Adar" die Bezeichnung "We-Adar" („noch ein Adar“). Als Schaltjahre sind das 3., 6., 8., 11., 14., 17. und 19. Jahr bestimmt.

Die jüdischen Feste sind eng mit der Jahreszeit verbunden, sie hängen mit ihren Symbolen zusammen und haben Naturereignisse oder landwirtschaftliche Bräuche zum Inhalt, die jedem Fest seinen besonderen Charakter verleihen.

Der jüdische Kalender zählt die Jahre ab dem Zeitpunkt der biblischen Schöpfung der Welt, die Hillel II. nach den biblischen Chroniken auf das Jahr 3761 v. Chr. berechnete. Dadurch befindet sich der jüdische Kalender bereits im sechsten Jahrtausend. Gelegentlich werden jüdische Jahreszahlen aber nur dreistellig angegeben, hier wird das jüdische Jahrtausend als bekannt vorausgesetzt. In der Umschrift wird diese Schreibweise gelegentlich mit dem Zusatz „nach der kleinen Zählung“ (n. d. k. Z. - לפ"ק) kenntlich gemacht.

Das jüdische neue Jahr (mit der jeweils nächsthöheren Jahreszahl) beginnt im Herbst mit dem ersten Tag des siebten Monats Tischri, der Rosch ha-Schana („Haupt des Jahres“) genannt wird. Dieser Tag kann nur auf einen Montag, Dienstag, Donnerstag oder Sonnabend fallen. Als erster "Monat" des Jahres wird hingegen der Frühlingsmonat Nisan nach biblischer Tradition mit dem Auszug der Israeliten aus Ägypten nummeriert (Ex 12,2). Nach Meinung von Historikern geht diese ungewöhnliche Anordnung auf die Übernahme der babylonischen Monatsnamen durch die Israeliten zurück. Im babylonischen Kalender war der Nisannu der erste Monat. Die Diskrepanz zwischen der Monatszählung und dem Jahresbeginn folgt aus der Verbindung zwischen diesen fremden und den eigenen Traditionen Israels. Dies würde auch erklären, warum die Samaritaner bis auf den heutigen Tag Neujahr mit Beginn des ersten Monats feiern.

Die Länge eines Mondzyklus – die durchschnittliche Zeit von einem Neumond zum nächsten – dauert genau 29 Tage, 12 Stunden, 44 Minuten und 3 1/3 Sekunden (nach der jüdischen Zeitzählung 29 Tage, 12 Scha'a und 793 Halakim – siehe Abschnitt "Der Tag"). Mit dieser Monatsdauer zwischen 29 und 30 Tagen hat ein Mondjahr 354 Tage, wenn die Monate abwechselnd 30 und 29 Tage dauern. Ein solches Jahr wird „reguläres Jahr“ genannt. Es ist jedoch gegenüber dem Lunarjahr um acht Stunden, 48 Minuten und 40 Sekunden (acht Sha'a 870 chalakim) zu kurz. Daher wird ungefähr alle drei Jahre dem Monat Cheschwan ein Tag hinzugefügt, das betreffende Jahr hat dann 355 Tage und wird „übermäßig“ genannt. Ferner wird in anderen Jahren dem Monat Kislew ein Tag abgezogen, so dass er nur 29 Tage zählt. Ein solches Jahr wird „vermindert“ genannt. Durch diese Maßnahmen, die durch Listen von Sonnen- und Mondfinsternissen ermittelt wurden, ergeben sich im Durchschnitt lediglich Abweichungen von 0,42 Sekunden gegenüber der tatsächlichen Monatsdauer.

Da das Sonnenjahr mit einer Dauer von zurzeit durchschnittlich 365,2422 Tagen nicht mit dem Mondjahr übereinstimmt, das durchschnittlich 354,3671 Tage dauert, muss der Ausgleich durch eine Schaltregelung geschaffen werden. 19 Sonnenjahre als sogenannte Meton-Periode sind fast genau 235 Mondmonate. Daher werden im jüdischen Kalender innerhalb von 19 Jahren die Jahre 3, 6, 8, 11, 14, 17 und 19 zu Schaltjahren mit jeweils 1 zusätzlichen Monat von 30 Tagen. Dieser Schalt-Monat wird vor dem Monat "Adar" eingefügt. Der eigentliche Adar wird dann „We-Adar“ („Und-Adar“), „Adar-scheni“ („zweiter Adar“) oder einfach „Adar II“ genannt. So entstehen zwölf Gemeinjahre mit je zwölf Monaten (144 Monate) und sieben Schaltjahre mit je 13 Monaten (91 Monate), die alle nach der Schaltregel des Mondjahres wiederum „regulär“, „übermäßig“ oder „vermindert“ sein können. Dadurch wird der Kalender so angepasst, dass er sich zum Lauf der Sonne und den Jahreszeiten nur geringfügig ändert. Das Kalenderjahr hat eine durchschnittliche Länge von 365,2468 Tagen (365 Tage, 5 Stunden, 55 Minuten und 25 Sekunden). Eine eintägige Abweichung gegenüber dem Sonnenjahr mit 365,2422 Tagen tritt nach 219 Jahren ein.

Die Karaiten lehnen die regelbasierte Einfügung des Schaltmonats ab und entscheiden nach der Reife der Gerste in Israel, in wörtlicher Auslegung allein der schriftlichen Tora und unter Ablehnung der mündlichen Tora.

Der Beginn eines jeden Monats wird, wie bei den meisten mondorientierten Kalendern, durch das erste Sichtbarwerden der Mondsichel nach Neumond bestimmt, das sogenannte Neulicht ("Moled"). Allerdings orientieren sich die Monate nicht immer ganz exakt an den Mondphasen: Wenn sich durch den so ermittelten Monatsanfang für Rosch Ha-Schanah eine Aneinanderreihung von mehreren Tagen mit Arbeitsverbot (siehe Sabbat) ergeben würde, wird der Jahresbeginn um einen oder zwei Tage hinausgeschoben, um diese Härte zu vermeiden (denn Gott will nach jüdischem Verständnis das Leben der Menschen durch seine Gebote nicht schlechter, sondern besser machen). Es gibt insgesamt fünf Regeln für die Verschiebung des Jahresbeginns, die sich sowohl an den kalendarischen Erfordernissen aus Sonnen- und Mondjahr orientieren als auch die kultischen Bedürfnisse aus der Feiertags- und Sabbatordnung berücksichtigen.

Tritt das Neulicht des Tischri erst nach 18:00 Uhr jüdischer Zeit ein, so ist Rosch ha-Schana auf den folgenden Tag zu verschieben. Dies ist in etwa zwei von fünf Jahren der Fall.
Fällt das Neulicht des Tischri auf einen Mittwoch, Freitag oder Sonntag, so ist Rosch ha-Schana ebenfalls auf den folgenden Tag zu verlegen. Diese Regel findet in etwa drei von sieben Jahren Anwendung.
Würde nach Anwendung der Jach-Regel Rosch ha-Schana auf einen Mittwoch, Freitag oder Sonntag fallen, so muss zusätzlich die Regel Adu zur Anwendung kommen, das Neujahr also um einen weiteren Tag verschoben werden.
Tritt das Neulicht des Tischri in einem Gemeinjahr an einem Dienstag nicht vor 9 Uhr und 204 Halakim jüdischer Zeit (siehe Stundeneinteilung) ein, so muss Rosch ha-Schana um zwei Tage verschoben werden. Diese Regel findet in etwa drei von 100 Jahren Anwendung.
Fällt das Neulicht des Tischri in einem Jahr, das auf ein Schaltjahr folgt, auf einen Montag nicht vor 15 Uhr und 589 Halakim jüdischer Zeit, so wird Rosch ha-Schana auf den folgenden Tag verschoben. Dieser sehr seltene Fall tritt nur in etwa einem von 200 Jahren ein. Zuletzt wurde die Regel im Jahr 5766 (gregorianisch: 2005/2006) angewandt, das nächste Mal erst wieder im Jahr 6013 (gregorianisch: 2252/2253).
Die Regeln "Jach" und "Adu" haben religiöse Gründe, die drei anderen sind zur Aufrechterhaltung der Regeln erforderlich. Nur etwa 39 % aller Jahre beginnen tatsächlich am Tage des Neulichtes, somit sind Ausnahmen beim Jahresbeginn häufiger als die Regel.

Praktisch wird die Verschiebung von Rosch Ha-Schanah erreicht durch Hinzufügen eines Tages zum Monat Cheschwan oder Abziehen eines Tages vom Monat Kislew im jeweils vorangehenden Jahr. Dadurch erhält man für Cheschwan und Kislew eine Gesamtdauer von 58, 59 oder 60 Tagen.

Unter Berücksichtigung der fünf Ausnahmeregelungen ergeben sich also sechs verschiedene Jahreslängen: Ein Gemeinjahr kann 353, 354 oder 355 Tage haben, ein Schaltjahr 383, 384 oder 385 Tage. Man unterscheidet deshalb nicht nur Gemein- und Schaltjahre, sondern auch verminderte (353/383 Tage), reguläre (354/384 Tage) und übermäßige (355/385 Tage) Jahre.

Die folgende Übersicht bietet die Monatsnamen mit ihrer ungefähren Position im gregorianischen Kalender. Die Zuordnung zu den Tierkreiszeichen beruht auf jüdischer Tradition und ist idealtypisch. Sie berücksichtigt nicht die Ausnahmeregeln und stimmt daher nicht mit astronomisch exakten Berechnungen überein.

Ein Tag des jüdischen Kalenders beginnt mit dem vorausgehenden Abend. Begründet wird dies mit Aussagen aus dem Schöpfungsbericht in der Thora: (1. Buch Mose). Aus der Schöpfungsgeschichte der jüdischen Bibel ergebe sich somit, dass jeder Tag auch einen „Vor-Abend“ ( "Erev") hat.

Der jüdische Tag hat keine feste Länge. Er läuft vom Beginn des Abends bis zum nächsten Abend. An Orten höherer Breitengrade im Sommer, z. B. wo die Sonne nicht unter den Horizont sinkt, wird ein Tag vom Tageshöchststand der Sonne, örtlicher Mittag, zum nächsten Mittag gezählt. An Orten höherer Breitengrade im Winter, z. B. wo die Sonne nicht über den Horizont steigt, wird ein Tag vom Zeitpunkt des tiefsten Stands der Sonne auf ihrer Himmelsbahn, örtliche Mitternacht, zur nächsten Mitternacht gezählt. Diese lokalen und im Jahreslauf variierenden Tageslängen müssen auch z. B. für die Berechnungen der jüdischen Gebetszeiten berücksichtigt werden.

Anstelle des Gebrauchs der internationalen Datumsgrenze gibt es verschiedene jüdische Standpunkte dazu, wo der Tag wechselt. Ein Standpunkt vertritt den Gebrauch des 180° Anti-Meridians von Jerusalem. Jerusalem liegt auf dem 35. östlichen Meridian (geographische Lage: 35° 13’ östliche Länge) also ist der Anti-Meridian der 144. westliche Meridian (144° 47' westliche Länge, geht durch Alaska).

Im vereinfachten Zeitmaß beginnt der jüdische Tag unabhängig vom Sonnenuntergang um 18:00 Uhr am Abend. 6 Uhr jüdischer Zeit entspricht daher 24 oder 0 Uhr bürgerlicher Zeit. Unterteilt wird der Tag in 24 Stunden ("Sha'a, pl. Sha'ot"). Die Unterteilung der Stunde erfolgt in 1080 Teile ("Chalakim"). Ein Teil ("Chelek", mit der Einheit 1 P) dauert somit 3 1/3 Sekunden und entspricht der kürzesten babylonischen Zeiteinheit Digiti. Die internationale Zeitangabe 10:30 Uhr (vormittags) entspricht 16 H 540 P nach dem jüdischen Kalender. Diese Zeitangaben finden sich auch im öffentlichen Leben Israels.

Die Namen der jüdischen Wochentage sind:


In Bezug auf den ersten Tag der erschaffenen Welt setzten jüdische Forscher den 6. Oktober 3761 v. d. Z. fest. Rückgerechnet, nach der biblischen Überlieferung, muss unmittelbar mit Jom Rischon, dem 6. Oktober (6. September) 3761 v. d. Z., 23 Uhr 11 Minuten 20 Sekunden, das biblische Gotteswort zur Schöpfung () geworden sein: „Es werde Licht, und es wurde Licht.“

Der Jüdische Kalender findet in Israel ganz normale Anwendung. Nicht nur die religiösen jüdischen Festtage, sondern auch die säkularen orientieren sich am jüdischen und nicht am gregorianischen Kalender. Da auf internationaler Ebene sowie im Tourismus der gregorianische Kalender bestimmend ist, nutzen die Israelis beide Kalender parallel im Alltag.

Die Fest-, Feier- und Gedenktage mit festem Termin im jüdischen Kalender sind:

Am 15. Nisan wird das Pessachfest gefeiert, spätestens seit dem 10. Jahrhundert unabhängig vom tatsächlichen Vollmond. Dieser Tag stellt einen der wichtigsten jüdischen Feiertage dar. Auch der Todestag Jesu von Nazareth steht in unmittelbarem Zusammenhang mit dem Pessachfest, wobei unklar ist, ob es sich dabei um den 14. Nisan (den sogenannten Rüsttag) oder den 15. Nisan (das Pessachfest selbst) handelte. Das christliche Osterfest findet daher in der Regel ebenfalls in diesem Monat statt, wenn es nicht wegen des jüdischen Schaltjahres in einen Adar II fällt.

Am 10. Tischri ist Jom Kippur, einer der höchsten jüdischen Feiertage.

In Schaltjahren findet das Purimfest im Adar II statt.

Die hier angegebenen Tagesdaten aus dem Bürgerlichen Kalender bezeichnen stets den Tag, der um Mitternacht (0 Uhr) des entsprechenden jüdischen Tages beginnt. Nach dem jüdischen Kalender beginnt das Fest tatsächlich jedoch schon bei Sonnenuntergang des Vortages; der Abend dieses Vortages ist der „Vorabend“ des Festes.

Wichtige Eckdaten des jüdischen Kalenders lassen sich durch Hilfsformeln ausrechnen. Das Datum des Pessach-Festes (15. Nisan) lässt sich für beliebige Jahre nach der Gaußschen Pessach-Formel berechnen. Weiterhin folgt aus dieser Formel auch das Datum des folgenden jüdischen Neujahrsfests (1. Tischri). Der Charakter eines beliebigen jüdischen Jahres (vermindertes, reguläres oder übermäßiges Gemein- oder Schaltjahr) und die Wochentage des Neujahrs- und Pessach-Festes können durch die Slonimski-Formel bestimmt werden. Somit ist durch diese beide Formeln der gesamte jüdische Kalender eines Jahres festgelegt und leicht bestimmbar.





</doc>
<doc id="11172" url="https://de.wikipedia.org/wiki?curid=11172" title="Same (Pflanze)">
Same (Pflanze)

Ein Same oder Samen (wie lat. "semen": ‚Samen‘, ‚Setzling‘, „das Fallengelassene“, aus indogermanischer Wurzel "sēi", ‚entsenden‘, ‚werfen‘, und verwandt mit "säen ") ist eine der Ausbreitung dienende Gewebestruktur der Samenpflanzen (Spermatophyta), das aus einer Samenschale (Testa), dem Embryo und häufig noch einem Nährgewebe (Endosperm oder Perisperm) besteht. Der Same enthält damit alle Anlagen, um unter günstigen Keimungs­bedingungen zu einer neuen Pflanze heranzuwachsen.

Samen entstehen, nachdem eine in den Samenanlagen sitzende Eizelle durch eine generative Zelle eines Pollen­korns befruchtet wurde. Aus der befruchteten Eizelle (Zygote) entsteht durch mitotische Zellteilungen ein pflanzlicher Embryo, der aber nicht weiterwächst, sondern durch teilweise Austrocknung in einer Art vorläufigen Wartestellung gehalten wird.

Dabei gibt es bedeutende Detailunterschiede zwischen

Der Gärtner oder Landwirt spricht nicht von Samen, sondern von Saat oder Saatgut.



</doc>
<doc id="11173" url="https://de.wikipedia.org/wiki?curid=11173" title="Same">
Same

Same steht für:
Samen sind:
SAME steht für:
SAMe steht für:

Siehe auch:



</doc>
<doc id="11174" url="https://de.wikipedia.org/wiki?curid=11174" title="Sterilität">
Sterilität

Sterilität, adj. steril, bedeutet:
Siehe auch:


</doc>
<doc id="11175" url="https://de.wikipedia.org/wiki?curid=11175" title="Vakuumfluktuation">
Vakuumfluktuation

Vakuumfluktuationen (auch Quanten- oder Nullpunktsfluktuation) sind Begriffe, die in Zusammenhang mit der Quantenfeldtheorie verwendet werden. In populärwissenschaftlichen Artikeln wird der Begriff häufig auf die quantenmechanische Energie-Zeit-Unschärferelation oder auf virtuelle Teilchen reduziert. 

In der Physik versteht man unter Fluktuation die zufällige Änderung einer ansonsten bekannten konstanten oder schwingenden Systemgröße, wie zum Beispiel Fluktuation im Gravitationsfeld der Erde. In diesem Sinne ist jedoch die Vakuumfluktuation "nicht" zu verstehen. Das Vakuum ist in Raum und Zeit gleichmäßig und ändert sich überhaupt nicht.

In den Formeln der Quantenfeldtheorie von Werner Heisenberg und Wolfgang Pauli treten Unendlichkeiten auf, die von Richard Feynman und Julian Seymour Schwinger 1948 und etwas früher während des Krieges von Shin’ichirō Tomonaga durch Renormierung aufgelöst wurden. Im Zusammenhang mit den dabei entstehenden Termen entwickelten die Physiker die Vorstellung von Wolken aus virtuellen Teilchen, welche die nicht störungsbehafteten Teilchen der klassischen Theorie umgeben. In der Vorstellung können virtuelle Teilchen in einem sehr kurzen Zeitraum real und sofort wieder absorbiert werden. Durch die entstehende Fluktuation der Energie verändert sich die messbare Masse und Ladung der Teilchen. Somit ist diese Fluktuation in den beobachtbaren Teilchen wie Elektronen oder Photonen bereits enthalten und kann niemals isoliert betrachtet werden. Die virtuellen Teilchen haben daher "keine" physikalische Bedeutung.

In der Vergangenheit wurde insbesondere der Casimir-Effekt (Anziehungskräfte zwischen parallelen Metallplatten), als Beweis dafür angesehen, dass Vakuumfluktuationen bzw. virtuelle Teilchen eine eigenständige physikalische Bedeutung haben könnten. Robert L. Jaffe zeigte 2005, dass diese Effekte durch quantentheoretische Störungsrechnung auch ohne Vakuumfluktuationen hergeleitet werden können. (Der Casimir-Effekt ergibt sich dabei bereits aus der Van-der-Waals-Wechselwirkung für Platten unendlicher Ausdehnung und Leitfähigkeit.) Auch Joseph Cugnon hat bestätigt, dass die Ursache des Casimir-Effekts eher mit der Van-der-Waals-Wechselwirkung zu erklären ist. 

In populärwissenschaftlichen Artikeln wird Vakuumfluktuation unter Annahme von Nullpunktsenergie, die auch Vakuumenergie genannt wird, gelegentlich hergeleitet aus der Unschärferelation zwischen Zeit und Energie. Dabei wird manchmal der Eindruck vermittelt, dass diese Fluktuationen physikalische Effekte auslösen könnten. Vakuumfluktuationen werden als Beleg dafür angeführt, dass das quantenmechanische Vakuum nicht im klassischen Sinne „leer“ ist. Vakuumfluktuationen werden gelegentlich als mögliche Erklärung für die Dunkle Energie angesehen, jedoch unterscheiden sich die errechneten Werte um einen Betrag von 10.

Aus der Quantenfeldtheorie hat der Physiker Gerald T. Moore schon 1970 hergeleitet, dass virtuelle Teilchen, die sich in einem Vakuum befinden, real werden können, wenn sie von einem Spiegel reflektiert werden, der sich fast mit Lichtgeschwindigkeit bewegt. Dieser Effekt ist jedoch eher auf thermische Emission zurückzuführen. Er wurde später auch dynamischer Casimir-Effekt genannt. Der Experimentalphysiker Per Delsing und Kollegen von der Universität Göteborg glauben diesen Effekt 2011 nachgewiesen zu haben.

Physiker an der Universität Konstanz haben nach eigener Aussage Vakuumfluktuationen direkt nachgewiesen. Mit einem sehr kurzen Laserpuls im Bereich einer Femtosekunde wurden Effekte gemessen, die sich angeblich nur mithilfe von Vakuumfluktuationen erklären lassen. Leitenstorfer und Kollegen kommen zu dem Schluss, dass die beobachteten Effekte von virtuellen Photonen ausgelöst wurden. Mit diesem Artikel setzt sich der Mathematiker Arnold Neumaier in seinem populärwissenschaftlichen Forumsbeitrag kritisch auseinander. Er betont, dass die Verwendung von Vakuumerwartungswerten in der verwendeten quantenmechanischen Formel kein Anhaltspunkt für Vakuumfluktuationen ist, da diese Erwartungswerte in allen Berechnungen auftreten werden, solange sie in einer störungstheoretischen Einstellung durchgeführt werden. In nicht störenden Gitterfeldtheoretischen Studien hat niemand die geringste Spur von Vakuumfluktuationen gesehen.



</doc>
<doc id="11176" url="https://de.wikipedia.org/wiki?curid=11176" title="Gefangenendilemma">
Gefangenendilemma

Das Gefangenendilemma ist ein mathematisches Spiel aus der Spieltheorie. Es modelliert die Situation zweier Gefangener, die beschuldigt werden, gemeinsam ein Verbrechen begangen zu haben. Die beiden Gefangenen werden einzeln verhört und können nicht miteinander kommunizieren. Leugnen beide das Verbrechen, erhalten beide eine niedrige Strafe, da ihnen nur eine weniger streng bestrafte Tat nachgewiesen werden kann. Gestehen beide, erhalten beide dafür eine hohe Strafe, wegen ihres Geständnisses aber nicht die Höchststrafe. Gesteht jedoch nur einer der beiden Gefangenen, geht dieser als Kronzeuge straffrei aus, während der andere als überführter, aber nicht geständiger Täter die Höchststrafe bekommt.

Das Dilemma besteht nun darin, dass sich jeder Gefangene entscheiden muss, entweder zu leugnen (also mit dem anderen Gefangenen zu kooperieren) oder zu gestehen (also den anderen zu verraten), ohne die Entscheidung des anderen Gefangenen zu kennen. Das letztlich verhängte Strafmaß richtet sich allerdings danach, wie die beiden Gefangenen zusammengenommen ausgesagt haben und hängt damit nicht nur von der eigenen Entscheidung, sondern auch von der Entscheidung des anderen Gefangenen ab.

Beim Gefangenendilemma handelt es sich um ein symmetrisches Spiel mit vollständiger Information, das sich entsprechend in Normalform darstellen lässt. Die dominante Strategie beider Gefangenen ist, zu gestehen. Diese Kombination stellt auch das einzige Nash-Gleichgewicht dar. Hingegen würde eine Kooperation der Gefangenen für beide zu einer niedrigeren Strafe und damit auch zu einer niedrigeren Gesamtstrafe führen.

Das Gefangenendilemma taucht bei einer Vielzahl soziologischer und ökonomischer Fragestellungen auf. In den Wirtschaftswissenschaften wird das Gefangenendilemma als Teil der Spieltheorie auch den entscheidungsorientierten Organisationstheorien zugeordnet. Es ist nicht zu verwechseln mit dem Gefangenenparadoxon über bedingte Wahrscheinlichkeiten und dem Problem der 100 Gefangenen der Kombinatorik.

Mit dem Thema beschäftigte sich schon Thomas Hobbes (1588–1679). Hobbes war ein englischer Mathematiker, Staatstheoretiker und Philosoph der Neuzeit; in seinem Hauptwerk "Leviathan" entwickelte er eine Theorie des Absolutismus. Hobbes war neben John Locke und Jean-Jacques Rousseau einer der bedeutendsten Vertragstheoretiker. (Siehe auch: Gefangenendilemma und Wirtschaftsethik im Leviathan.)

Die Grundkonzeption des Gefangenendilemmas wurde in den 1950er Jahren von zwei Mitarbeitern der Rand Corporation formuliert. Um ihre abstrakten theoretischen Resultate zu veranschaulichen, beschrieben Merrill M. Flood und Melvin Dresher ein Zweipersonenspiel, das zeigt, wie individuell rationale Entscheidungen zu kollektiv schlechteren Ergebnissen führen können.

Die Bezeichnung „Gefangenendilemma“ geht auf Albert William Tucker von der Universität Princeton zurück. Dieser hatte die Auszahlungsmatrix 1950 bei Melvin Dresher gesehen und übernahm sie wegen ihrer Anschaulichkeit. Als er vor Psychologen einen Vortrag über die Spieltheorie halten sollte, entschloss er sich, die abstrakte Auszahlungsmatrix mit dem Szenario eines sozialen Dilemmas zu veranschaulichen. Dabei stehen zwei (schuldige) räumlich getrennte Untersuchungshäftlinge vor der Wahl zu leugnen oder zu gestehen. Für den Einzelnen ist es am sichersten, zu gestehen, beidseitiges Leugnen aber verspricht das beste Gesamtergebnis.

Seitdem hat sich die Bezeichnung Gefangenendilemma für sämtliche Interaktionsbeziehungen mit denselben Rahmenbedingungen (zwei Akteure, je zwei Handlungsalternativen, symmetrische Auszahlungsmöglichkeiten, keine Möglichkeit der Absprache, wechselseitige Interdependenzen) etabliert.

Zur Veranschaulichung formulierte Tucker die spieltheoretische Fragestellung als soziales Dilemma:

Zwei Gefangene werden verdächtigt, gemeinsam eine Straftat begangen zu haben. Beide Gefangene werden in getrennten Räumen verhört und haben keine Möglichkeit, sich zu beraten und ihr Verhalten abzustimmen. Die Höchststrafe für das Verbrechen beträgt sechs Jahre. Wenn die Gefangenen sich entscheiden zu schweigen (Kooperation), werden beide wegen kleinerer Delikte zu je zwei Jahren Haft verurteilt. Gestehen jedoch beide die Tat (Defektion), erwartet beide eine Gefängnisstrafe, wegen der Zusammenarbeit mit den Ermittlungsbehörden jedoch nicht die Höchststrafe, sondern lediglich vier Jahre Haft. Gesteht nur einer (Defektion) und der andere schweigt (Kooperation), bekommt der Geständige als Kronzeuge eine symbolische einjährige Bewährungsstrafe, der andere bekommt die Höchststrafe von sechs Jahren Haft.

In einer Auszahlungsmatrix (Bimatrix) eingetragen ergibt sich inklusive des Gesamtergebnisses folgendes Bild:

In allgemeiner Form lässt sich das Gefangenendilemma für zwei Spieler A und B mit folgender Auszahlungsmatrix darstellen:

mit formula_1 und formula_2<ref name="Axelrod2005/9">Axelrod, R. (2005), Die Evolution der Kooperation, 6. Auflage, München 2005, S. 9.</ref>

Die Auszahlung eines Spielers hängt somit nicht nur von der eigenen, sondern auch von der Entscheidung des Komplizen ab (Interdependenz des Verhaltens).

"Kollektiv" ist es objektiv für beide vorteilhafter zu schweigen. Würden beide Gefangenen kooperieren, dann müsste jeder nur zwei Jahre ins Gefängnis. Der Verlust für beide zusammen beträgt so vier Jahre, und jede andere Kombination aus Gestehen und Schweigen führt zu einem höheren Verlust.

"Individuell" scheint es für beide vorteilhafter zu sein auszusagen. Für den einzelnen Gefangenen stellt sich die Situation individuell so dar:


Individuell gesehen ist als Strategie also auf jeden Fall „gestehen“ zu empfehlen. Diese Aussage hängt nicht vom Verhalten des anderen ab, und es ist anscheinend immer vorteilhafter zu gestehen. Eine solche Strategie, die ungeachtet der gegnerischen gewählt wird, wird in der Spieltheorie als "dominante Strategie" bezeichnet.

Das Dilemma beruht darauf, dass kollektive und individuelle Analyse zu unterschiedlichen Handlungsempfehlungen führen.

Die Spielanlage verhindert die Verständigung und provoziert einen einseitigen Verrat, durch den der Verräter das für ihn individuell bessere Resultat „ein Jahr“ (falls der Mitgefangene schweigt) oder vier statt sechs Jahre (falls der Mitgefangene gesteht) zu erreichen hofft. Verfolgen aber beide Gefangenen diese Strategie, so verschlimmern sie – auch individuell – ihre Lage, da sie nun je vier Jahre statt der zwei Jahre Gefängnis erhalten.

In diesem Auseinanderfallen der möglichen Strategien besteht das Dilemma der Gefangenen. Die vermeintlich rationale, schrittweise Analyse der Situation verleitet beide Gefangene, zu gestehen, was zu einem schlechten Resultat führt (suboptimale Allokation). Das bessere Resultat wäre durch Kooperation erreichbar, die aber anfällig für einen Vertrauensbruch ist. Die rationalen Spieler treffen sich an dem Punkt, an dem sich die jeweils dominanten Strategien treffen. Dieser Punkt wird als Nash-Gleichgewicht bezeichnet. Das Paradoxe ist, dass beide Spieler keinen Grund haben, vom Nash-Gleichgewicht abzuweichen, obwohl das Nash-Gleichgewicht hier kein pareto-optimaler Zustand ist.

Das Dilemma der Spieler beruht auf der Unkenntnis des Verhaltens des jeweils anderen Spielers. Die Spieltheorie befasst sich mit optimalen Strategien beim Gefangenendilemma. Die optimale Strategie für beide wäre, einander zu vertrauen und zu kooperieren. Das Vertrauen kann auf zwei Arten hergestellt werden: zum einen durch – nach den Spielregeln nicht erlaubte – Kommunikation und entsprechende Vertrauensbeweise, zum anderen durch Bestrafung des Mitspielers im Falle eines Vertrauensbruchs.

Der Ökonom und Spieltheoretiker Thomas Schelling geht in seinem Werk "The Strategy of Conflict" (Die Strategie des Konflikts) auf solche Probleme unter den Bedingungen des Kalten Kriegs ein („Gleichgewicht des Schreckens“). Die Bestrafung für Vertrauensbruch wäre so drastisch gewesen, dass er sich nicht lohnte. Beim wiederholten Spiel des Gefangenendilemmas beruhen die meisten Strategien darauf, dass man Informationen aus vorhergehenden Runden verwendet. Wenn der andere in einer Runde kooperiert, vertraut die erfolgreiche Strategie "Tit for Tat" („Wie du mir, so ich dir“) darauf, dass er es weiterhin tut, und gibt ihrerseits einen Vertrauensbeweis. Im entgegengesetzten Fall bestraft sie, um zu verhindern, dass sie ausgenutzt wird.

William Poundstone weist darauf hin, dass es sich nicht um ein Dilemma handele, wenn man aufgrund des Vertrauens sofort und immer Kooperation wählt.

Beim Gefangenendilemma wird die Frage einer tatsächlich bestehenden Schuld oder Unschuld ausgeklammert. So profitiert ein Gefangener immer von einem Geständnis, auch wenn er gesteht, obwohl er tatsächlich unschuldig ist. Hingegen erreicht er ein schlechteres Ergebnis, wenn ihn moralische Bedenken und die Hoffnung auf den Erweis seiner Unschuld von einem Geständnis abhalten.

Ist die Strafe für ein Nichtgestehen sehr hoch, tendieren auch Unschuldige zu einem Geständnis; dieser Effekt kommt insbesondere bei Schauprozessen zum Tragen.

Gemäß der klassischen Analyse des Spiels ist im nur einmal gespielten Gefangenendilemma (engl.: One Shot) die einzig rationale Strategie für einen am eigenen Wohl interessierten Spieler, zu gestehen und den Mitgefangenen damit zu verraten. Denn durch seine Entscheidung kann er das Verhalten des Mitspielers nicht beeinflussen, und unabhängig von der Entscheidung des Mitspielers stellt er sich immer besser, wenn er selbst nicht mit dem Mitgefangenen kooperiert. Diese Analyse setzt voraus, dass die Spieler nur einmal aufeinander treffen und ihre Entscheidungen keinen Einfluss auf spätere Interaktionen haben können. Da es sich um ein echtes Dilemma handelt, folgt aus dieser Analyse aber keine eindeutige Handlungsanweisung (präskriptive Aussage) für reale Interaktionen, die einem Gefangenendilemma entsprechen.

Im einmaligen, alles entscheidenden Spiel muss jedoch darauf hingewiesen werden, dass es egal ist, ob sich beide Parteien zuvor absprechen. Die Situation nach einem evtl. geführten Gespräch bleibt gleich.

Empirie

In Experimenten wurde nachgewiesen, dass sehr viele Mitspieler auch bei einmaligem Spiel kooperieren. Es wird angenommen, dass es verschiedene Spielertypen gibt. Die tatsächliche Verteilung der in den Experimenten beobachteten Kooperation kann durch die Standardtheorie der „rationalen Strategie“ nicht erklärt werden. In einem Experiment mit 40 Mitspielern, die jeweils 20 Spiele paarweise absolvierten, betrug die Kooperationsrate im Durchschnitt 22 %.

Nach einem von Frank, Gilovich und Regan 1993 veröffentlichten Experiment wurde das Verhalten von Ökonomiestudenten im ersten Studienjahr mit Studenten im Jahr vor dem Examen sowie mit dem Verhalten von Studenten anderer Fachrichtungen unter den Bedingungen eines Gefangenendilemmas verglichen. Dabei erhielten die Studenten, wenn sie beide kooperierten, je zwei Dollar, und wenn sie beide nicht kooperierten, je einen Dollar; bei einseitiger Kooperation bekam der kooperierende Student nichts, der nicht kooperierende Student dagegen drei Dollar. Es zeigte sich, dass sowohl Erstsemester als auch Studenten anderer Fachrichtungen sich mit großer Mehrheit für Kooperationsstrategien entschieden. Studenten im vierten Jahr ihres Ökonomiestudiums tendierten dagegen zu unkooperativem Verhalten. Frank u. a. schlossen daraus, dass Ökonomen in ihrer Lehre mit Rücksicht auf das Allgemeinwohl als auch auf das Wohlergehen ihrer Studenten eine weniger verengte Perspektive hinsichtlich menschlicher Motivation einräumen sollten, als dies bisher der Fall war.

Die Situation kann sich ändern, wenn das Spiel über mehrere Runden gespielt wird (iteriertes oder wiederholtes Gefangenendilemma). Ob sich die Situation dann ändert, hängt davon ab, ob den Spielern die Anzahl der Runden bekannt ist oder nicht. Ist den Spielern das Spielende bekannt, lohnt es sich für eigentlich kooperierende Spieler, in der letzten Runde zu verraten, weil dafür eine Vergeltung nicht mehr möglich ist. Somit wird aber die vorletzte Runde zur letzten, in der effektiv eine Entscheidung zu fällen ist, worauf sich wieder dieselbe Situation ergibt. Durch Induktion folgt, dass das Nash-Gleichgewicht in diesem Fall der ständige Verrat ist. Das heißt, wenn beide Seiten sich permanent verraten, ist dies die einzige Strategie, bei der durch einen Strategiewechsel kein besseres Ergebnis erzielt werden kann. Deshalb ist ein Spiel, bei dem beiden Spielern die Anzahl der Runden bekannt ist, genau wie ein Einmalspiel (One Shot) zu behandeln. In der Praxis wird dieses theoretisch rationale (auch Backward Induction genannte) Verhalten jedoch nicht immer beobachtet. Dies liegt daran, dass ein rationaler Spieler nicht wissen kann, ob der andere Spieler auch rational agiert. Wenn die Möglichkeit besteht, dass der Mitspieler irrational agieren könnte, ist es auch für den rationalen Spieler von Vorteil, vom ständigen Verrat abzuweichen und stattdessen Tit-for-Tat zu spielen.

Grundsätzlich anders verhält es sich erst, wenn den Spielern die Anzahl der Runden nicht bekannt ist. Da die Spieler nicht wissen, welche Runde die letzte sein wird, kommt es nicht zur Backward Induction. Das unbekannt oft wiederholte Spiel ist damit einem unendlich oft wiederholten Spiel (Single Shot) gleichzusetzen.

Bei unendlich wiederholten Spielen (Single Shot) kommt es wie bei unbekannt oft wiederholten Spielen nicht zur Backward Induction. Die wiederholte Interaktion ermöglicht es, Kooperation in folgenden Runden zu belohnen, was zu höheren Gesamtauszahlungen führt, oder Defektion zu vergelten, was zu geringeren Auszahlungen führt. "Tit for Tat" („wie du mir, so ich dir“) bedeutet in der nächsten Periode Bestrafung für den Verrat. Man spricht in dem Fall von kalkulativem Vertrauen.

Zur Interpretation der Ergebnisse eines Spiels werden bei endlichen Spielen die Auszahlungen der einzelnen Runden zu einer Gesamtauszahlung zusammengefasst, welche dann den Erfolg eines Spielers in einem Spiel wiedergibt. Hierfür werden die Auszahlungen der einzelnen Runden üblicherweise ungewichtet addiert, können aber auch in Form eines Diskontfaktors abgezinst werden.
Beim mehrmaligen Spiel wird die Auszahlungsmatrix in der Regel so gestaltet, dass zusätzlich zur allgemein gültigen Ungleichung formula_3 außerdem formula_4 gilt, was in der Beispiel-Auszahlungsmatrix aus der Einleitung erfüllt ist: formula_5. Im entgegengesetzten Fall könnten sich zwei Spieler sonst durch abwechselndes Ausbeuten und Ausgebeutet-Werden einen Vorteil gegenüber kooperierenden Spielern verschaffen, oder sie könnten sich schlicht die Summe der Einzelergebnisse für einseitige Kooperation und einseitige Defektion teilen.

Es ist ein Unterschied, ob man siegen oder gewinnen will. Wenn man den Sieg erringen will, handelt es sich eigentlich um ein anderes Spiel. Das Spiel wird zu einem Nullsummenspiel, wenn am Ende nur der Sieg gezählt wird. Wenn man gewinnen will (einen Gewinn erzielen will), lohnt es sich, dem anderen Mitspieler auch Kooperation anzubieten, indem man kooperiert. Wenn der andere darauf eingeht, erzielt man am Ende einen höheren Gewinn, als wenn man ausschließlich Verrat übt. Auch wenn man selbst auf die Kooperation des anderen eingeht durch eigene Kooperation, steigert man seinen Gewinn.

Der amerikanische Politologe Robert Axelrod veranstaltete zum mehrmaligen Gefangenendilemma zu Beginn der 1980er Jahre ein Computerturnier, in dem er Computerprogramme mit verschiedenen Strategien gegeneinander antreten ließ. Die insgesamt erfolgreichste Strategie, und gleichzeitig eine der einfachsten, war besagte "Tit-for-Tat"-Strategie, entwickelt von Anatol Rapoport.<ref name="Axelrod1980/7">Axelrod, R. (1980), Effective Choice in the Prisoner's Dilemma, in: Journal of Conflict Resolution, 24. Jg., Nr. 1, 1980, S. 3–25; hier: S. 7.</ref> Sie kooperiert im ersten Schritt (freundliche Strategie) und den folgenden und „verzichtet auf den Verrat“, solange der andere ebenfalls kooperiert. Versucht der andere, sich einen Vorteil zu verschaffen („Verrat“), tut sie dies beim nächsten Mal ebenfalls (sie lässt sich nicht ausbeuten), kooperiert aber sofort wieder, wenn der andere kooperiert (sie ist nicht nachtragend).<ref name="Axelrod1980/4ff">Axelrod, R. (1980), Effective Choice in the Prisoner's Dilemma, in: Journal of Conflict Resolution, 24. Jg., Nr. 1, 1980, S. 3–25; hier: S. 4 ff.</ref>

Eine Weiterentwicklung des Spiels über mehrere Runden ist das Spielen über mehrere Generationen. Sind alle Strategien in mehreren Runden gegeneinander und gegen sich selbst angetreten, werden die erzielten Resultate für jede Strategie zusammengezählt. Für einen nächsten Durchgang ersetzen die erfolgreichen Strategien die weniger erfolgreichen. Die erfolgreichste Strategie ist in der nächsten Generation am häufigsten vertreten. Auch diese Turnier-Variante wurde von Axelrod durchgeführt.

Strategien, die zum Verraten tendierten, erzielten hier zu Beginn relativ gute Resultate – solange sie auf andere Strategien stießen, die tendenziell eher kooperierten, also sich ausnutzen ließen. Sind verräterische Strategien aber erfolgreich, so werden kooperative von Generation zu Generation seltener – die verräterischen Strategien entziehen sich in ihrem Gelingen selbst die Erfolgsgrundlage. Treffen aber zwei Verräter-Strategien zusammen, so erzielen sie schlechtere Resultate als zwei kooperierende Strategien. Verräter-Strategien können nur durch Ausbeutung von Mitspielern wachsen. Kooperierende Strategien wachsen dagegen am besten, wenn sie aufeinandertreffen. Eine Minderheit von miteinander kooperierenden Strategien wie z. B. "Tit for Tat" kann sich so sogar in einer Mehrheit von verräterischen Strategien behaupten und zur Mehrheit anwachsen. Solche Strategien, die sich über Generationen hin etablieren können und auch gegen Invasionen durch andere Strategien resistent sind, nennt man evolutionär stabile Strategien.

"Tit for Tat" konnte erst 2004 von einer neuartigen Strategie „Master and Servant“ (Ausbeuter und Opfer) der Universität Southampton geschlagen werden, wobei dazugehörige Teilnehmer sich bei gegenseitigem Aufeinandertreffen nach einem Initial-Austausch in eine Ausbeuter- bzw. eine Opferrolle begeben, um dem Ausbeuter (individuell) so eine Spitzenposition zu ermöglichen. Betrachtet man das Ergebnis des Ausbeuters und des Opfers zusammen (kollektiv), so sind sie bei den o. g. Auszahlungswerten schlechter als "Tit for Tat". Nötig für die individuell guten Ergebnisse ist aber eine gewisse kritische Mindestgröße, d. h., "Master and Servant" kann sich nicht aus einer kleinen Anfangspopulation etablieren. Da die Spielpartner über ihr anfängliches Spielverhalten codiert kommunizieren, besteht der Einwand, dass die "Master-and-Servant"-Strategie die Spielregeln verletzt, wonach die Spielpartner isoliert voneinander befragt werden. Die Strategie erinnert an Insektenvölker, in denen Arbeiterinnen auf Fortpflanzung gänzlich verzichten und ihre Arbeitskraft für das Wohlergehen der fruchtbaren Königin aufwenden.

Notwendige Bedingungen für das Ausbreiten von kooperativen Strategien sind: a) dass mehrere Runden gespielt werden, b) sich die Spieler von Runde zu Runde gegenseitig wiedererkennen können, um nötigenfalls Vergeltung zu üben, und c) dass nicht bekannt ist, wann sich die Spieler zum letzten Mal begegnen.

Die Variante des Gefangenendilemma, bei der die Spieler nacheinander entscheiden, stellt die Spieler in eine asymmetrische Position. Eine solche Situation ergibt sich beispielsweise bei der Ausführung von bei eBay zustande gekommenen Geschäften. Zunächst muss der Käufer entscheiden, ob er kooperieren, d. h. den Kaufbetrag an den Verkäufer überweisen möchte. Anschließend entscheidet der Verkäufer, ob er die Ware versendet. Trivialerweise wird der Verkäufer in keinem Fall die Ware versenden, wenn der Käufer den Kaufbetrag nicht überweist.

Der Käufer befindet sich also in einer Situation der „Angst“, dass der Verkäufer die Ware nicht versenden könnte, auch wenn er – der Käufer – den Kaufpreis überweist. Ist das Geld beim Verkäufer eingegangen, gibt es für diesen die Versuchung („Gier“), die Ware dennoch nicht zu versenden. Angst und Gier können als Emotionen in diesem Fall den beiden Spielern also getrennt zugeordnet werden, während bei der üblichen, zeitgleichen Entscheidungsfindung beide Spieler gleichermaßen beide Emotionen empfinden bzw. erfahren können.

Dieser Unterschied macht die Analyse des Einflusses der Sozialen Identität (vereinfacht: „Wir-Gefühl“) möglich. Die traditionelle Hypothese ist, dass ein vorhandenes Wir-Gefühl die Tendenz zur Kooperation generell verstärkt. Yamagishi und Kiyonari stellten jedoch die These auf, dass ein Einfluss eines Wir-Gefühls zwar existiert, im Falle des sequentiellen Gefangenendilemmas jedoch ein viel stärkerer Effekt der reziproken Kooperation das Vorhandensein oder Nicht-Vorhandensein eines Wir-Gefühls unerheblich macht: Der Käufer motiviert den Verkäufer durch seine eigene Kooperation ebenfalls zur Kooperation. Simpson konnte jedoch zeigen, dass die Belege, die Yamagishi und Kiyonari für ihre These anführen, ebenfalls mit der Annahme verträglich sind, dass ein vorhandenes Wir-Gefühl die Spieler zwar dazu bringt, der Gier nicht nachzugeben, die Angst, der andere könne nicht kooperieren, jedoch weiterhin ein entscheidender Einfluss bleibt.

Ein solcher Sachverhalt wäre insbesondere dazu geeignet, zu erklären, dass bei den Minimal-group-Experimenten von Tajfel nicht beobachtet wurde, dass die Spieler den Gewinn ihrer eigenen Gruppe zu maximieren trachteten, sondern den Gewinnunterschied zur anderen Gruppe zu maximieren und den Unterschied innerhalb der eigenen Gruppe zu minimieren trachteten: Geht man einmal davon aus, dass zwei Spieler eines Gefangenendilemmas sich in irgendeiner Weise beide als Teil einer Gruppe fühlen und die Gruppenzugehörigkeit im Moment des Spiels salient ist, muss man annehmen, dass die beiden Spieler zum einen eine möglichst gleiche Verteilung, zum anderen eine möglichst geringe Summe an Strafen (bzw. möglichst hohe Summe an Belohnung) anstreben. Nimmt ein Spieler an, der andere kooperiere (er könne also durch Gier von der Kooperation abgehalten werden), so können beide Ziele durch Kooperation (Differenz: formula_6; Summe: formula_7) erreicht werden; nimmt der Spieler jedoch an, der andere kooperiere nicht (Angst vor Ausnutzung), so werden beide Ziele mit unterschiedlichen Strategien erreicht (Differenz schlägt Nicht-Kooperation vor: formula_8; aber Summe schlägt Kooperation vor: formula_9).

Für das über mehrere Runden gespielte Gefangenendilemma gibt es viele verschiedene Strategien. Für einige Strategien haben sich Namen eingebürgert (Übersetzung in Klammern). Dahinter steht, wie hoch der durchschnittliche Gewinn ist. (Unter der Voraussetzung, dass die Anzahl der Runden unbekannt ist und es nach jedem Zug mit einer Wahrscheinlichkeit von formula_10 einen weiteren Zug gibt. - Die Wahrscheinlichkeit, dass das Spiel mindestens i Züge dauert, ist also formula_11.)


Die Strategie "Tit for Tat" ist – wenn sie strikt gespielt wird – eine einfache, aber sehr wirkungsvolle und langfristig erfolgreiche Strategie. Sind aber im Spiel auch Fehlkommunikation und Missverständnisse möglich (z. B. ein "Kooperieren" wird als "Verraten" missverstanden), weist striktes "Tit for Tat" einen Schönheitsfehler auf: Ein durch ein Missverständnis aufgetauchter Verrat wird dann durch eine Abfolge wechselseitiger Vergeltungen perpetuiert und nicht verziehen. Beide Spieler können sich so in einem andauernden Konflikt aus Vergeltungsreaktionen blockieren und ihr Spielergebnis wesentlich schmälern. Dieser Umstand wird Vendetta (ital. "Blutrache") oder auch Echo­effekt (das eigene Handeln hallt eine Runde zeitversetzt wider) genannt. Vendetta kann unter "Tit-for-Tat"-Spielenden nur durch Fehlkommunikation entstehen, da die "Tit-for-Tat"-Strategie nie unprovoziert von sich aus "verraten" spielt. Die Vendetta kann auch nur wieder durch eine weitere Fehlkommunikation unterbrochen werden (wenn ein "Verraten" als "Kooperieren" missverstanden wird), da die "Tit-for-Tat"-Strategie von sich aus nie eine Vergeltung unterlässt.

Eine mögliche Adaption der "Tit-for-Tat"-Strategie, um das Risiko einer ausgedehnten Vendetta zu verkleinern, ist deshalb, die Strategie etwas weniger unerbittlich bei der Vergeltung zu machen, also der Strategie einen "Verzeih-Mechanismus" einzubauen. Dieser bewirkt, dass nicht "jeder" Verrat vergolten wird, sondern mit einer gewissen Wahrscheinlichkeit ein Verrat auch ohne Vergeltung toleriert wird. Ein solches „gutmütiges "Tit for Tat"“ ist das oben erwähnte "Tit for Two Tat". Solange die Häufigkeit der Fehlkommunikation zwischen den Spielern nicht so hoch ist, dass sie die Erkennbarkeit der gespielten "Tit-for-Tat"-Strategie verhindert, ist es noch möglich, optimale Ergebnisse zu erzielen. Dazu muss die Häufigkeit des Verzeihens proportional zur Häufigkeit der Kommunikations-Fehler gewählt werden.

Das Gefangenendilemma lässt sich auf viele Sachverhalte in der Praxis übertragen. Vereinbaren beispielsweise zwei Länder eine Rüstungskontrolle, so wird es immer individuell besser sein, heimlich doch aufzurüsten. Keines der Länder hält sich an sein Versprechen und beide sind durch die Aufrüstung schlechter gestellt (höheres Gefahrenpotential, höhere ökonomische Kosten), allerdings besser, als wenn nur der jeweils andere aufrüstete (Gefahr einer Aggression durch den anderen).

Die sogenannte Politikverflechtungsfalle basiert nicht unwesentlich auf den Mechanismen des Gefangendilemmas, was Entscheidungsblockaden und -einschränkungen sowohl bei Sachentscheidungen wie auch in Bezug auf institutionelle Fragen zur Folge hat.

Auch in der Wirtschaft finden sich Beispiele für das Gefangenendilemma, etwa bei Absprachen in Kartellen oder Oligopolen: Zwei Unternehmen vereinbaren eine Outputquote (zum Beispiel bei der Ölförderung), aber individuell lohnt es sich, die eigene Quote gegenüber der vereinbarten zu erhöhen. Beide Unternehmen werden mehr produzieren. Das Kartell platzt. Die Unternehmen im Oligopol sind aufgrund der erhöhten Produktion gezwungen, die Preise zu senken, wodurch sich ihr Monopol­gewinn schmälert.

Konkurrieren mehrere Firmen auf einem Markt, erhöhen sich die Werbeausgaben immer weiter, da jeder die anderen ein wenig übertreffen möchte. Diese Theorie konnte 1971 in den USA bestätigt werden, als ein Gesetz zum Werbeverbot für Zigaretten im Fernsehen verabschiedet wurde. Es gab kaum Proteste aus den Reihen der Zigarettenhersteller. Das Gefangenendilemma, in das die Zigarettenindustrie geraten war, wurde durch dieses Gesetz gelöst.

Ein weiteres Beispiel ist ein Handelsreisender, der seine Kunden bei Vorkasse (gegebenenfalls ungedeckte Schecks) mit guter Ware (kleinerer Profit, aber langfristig sicher) oder gar keiner Ware (hoher kurzzeitiger Profit) beliefern kann. Händler mit schlechtem Ruf verschwinden in solchen Szenarien vom Markt, da keiner mit ihnen Geschäfte macht und sie ihre Fixkosten nicht decken können. Hier führt "Tit for Tat" zu einem Markt mit wenig „Betrug“. Ein bekanntes Beispiel nach diesem Muster ist die Funktionsweise des eBay-Bewertungsschemas: Händler, die trotz erhaltener Bezahlung die vereinbarte Ware nicht liefern, erhalten schlechte Bewertungen und verschwinden so vom Markt.

Beachtenswert ist das Anbieterdilemma, das zu einer Beeinflussung der Preise für angebotene Güter führt. Zwar profitieren Anbieter bei Vorliegen des Dilemmas nicht, jedoch kann sich die Wohlfahrt einer Volkswirtschaft insgesamt erhöhen, da der Nachfrager durch niedrige Preise profitiert. Durch staatlichen Eingriff in Form von Wettbewerbspolitik wird ein Anbieterdilemma häufig künstlich generiert, indem beispielsweise Absprachen zwischen Anbietern untersagt werden. Somit sorgen Institutionen für mehr Wettbewerb, um den Verbraucher zu schützen.

Auch die Versteigerung der UMTS-Lizenzen in Deutschland kann als Beispiel dienen. Es wurden zwölf Frequenzblöcke für UMTS versteigert, die entweder als Zweier- oder Dreier-Paket erworben werden konnten. Sieben Bieter (E-Plus/Hutchison, Mannesmann, T-Mobile, Group 3G/Quam, debitel, mobilcom und Viag Interkom) nahmen an der Versteigerung im August 2000 teil. Wie im theoretischen Original waren Absprachen unter den Spielern, also den Mobilfunkanbietern, unterbunden worden. Nach dem Ausscheiden von debitel nach der 126. Runde am 11. August 2000 waren zwölf Lizenzen für sechs Mobilfunkanbieter vorhanden, also zwei für jeden; die Summe aller Lizenzen betrug zu diesem Zeitpunkt 57,6 Mrd. DM. Da die Mobilfunkanbieter jedoch auf das Ausscheiden eines weiteren Anbieters und die Möglichkeit, drei Lizenzen zu erwerben, spekulierten, reichten sie weiter Gebote ein. In der 173. Runde am 17. August 2000 gingen je zwei Lizenzen an die sechs verbliebenen Mobilfunkanbieter – ein Ergebnis also, das auch schon in der 127. Runde hätte erreicht werden können. Die Summe, die die Mobilfunkanbieter für alle Lizenzen zahlten, lag nun aber bei 98,8 Mrd. DM.
Die sogenannte „Omertà“ (Schweig oder stirb!) der Mafia versucht das Schweigen (Kooperieren) dadurch sicherzustellen, dass ein Verstoß mit besonders drastischen Sanktionen bedroht wird. Damit wird die Kooperation gefestigt, während zugleich ein einseitiges Geständnis durch extremen Verlust demotiviert wird. Dies wäre eine Internalisierung eines negativen externen Effektes („negativ“ in rein spieltheoretischem Sinn).

Omertà versucht die Spieler zu gegenseitigem Vertrauen anzuhalten, kann aber das grundsätzliche Dilemma nicht auflösen.
Als Gegenmittel kann die Justiz z. B. Verrätern Straffreiheit und/oder eine neue Identität anbieten, um das Vertrauen der Komplizen zu untergraben (Kronzeugen­regelung). Eine einfache (wenngleich in Deutschland nach StPO unzulässige) Verhörstrategie der Polizei kann darin bestehen, den Verdächtigten zu verunsichern, indem fälschlich behauptet wird, der Komplize hätte bereits gestanden.

Rilling hat in einer Studie an psychisch gestörten Probanden herausgefunden, dass ein Defizit an Kooperation mit Defiziten im emotionalen und behavioralen Bereich einhergeht. Psychopathie wird als Störung vor allem der Affekte für soziale Interaktion angesehen. Sie wird definiert als sozial beeinträchtigende Persönlichkeitsstörung mit affektiven, sozialen und Verhaltensproblemen. Psychopathen verspüren in Übereinstimmung mit den Annahmen Axelrods (1987) viel weniger den Wunsch, stabile Beziehungen einzugehen und zu unterhalten. Dass bei einer klinischen Population, welche überzufällig beim iterierten Gefangenendilemma defektiert, gleichzeitig die genannten Defizite auftreten, deutet auf die nahe Verwandtschaft der Fähigkeit zu kooperieren mit Empathie und emotionalem Affekt hin.

Inwiefern das Gefangenendilemma die soziale Wohlfahrt verbessert oder verschlechtert, hängt vom betrachteten Sachverhalt ab. Im Fall eines Kartells oder Oligopols führt das Gefangenendilemma zu einer Verbesserung der Situation. Das „Marktversagen“ durch ein verringertes Angebot kann behoben werden. Betrachtet man allerdings die Waffenaufrüstung von Staaten oder die Werbeausgaben von Firmen, dann führt das Gefangenendilemma zu einer schlechteren Wohlfahrt, da lediglich Kosten geschaffen werden, die zu keinem neuen Nutzen führen.

Karl Homann geht in seiner Konzeption einer Wirtschaftsethik davon aus, dass es Aufgabe der Staaten bzw. des Gesetzgebers sei, in der Gestaltung der Rahmenordnung darauf hinzuwirken, dass erwünschte Dilemmasituationen aufrechterhalten werden und dass unerwünschte Dilemmasituationen durch die Schaffung bzw. Veränderung von Institutionen überwunden werden. So können beispielsweise gesetzliche Mindeststandards bei der Sicherung von Konsumentenrechten (z. B. AGB-Gesetz) ein Misstrauen dem Verkäufer gegenüber (unerwünschte Dilemmasituation) ausräumen und so zu mehr Handel führen; gleichzeitig ist die Konkurrenz zwischen den jeweiligen Verkäufern und den jeweiligen Käufern als erwünschte Dilemmasituation aufrechtzuerhalten.

Ob die beiden Möglichkeiten, sich zu verhalten, sinnvollerweise als Vertrauen/Verrat, Kooperation/Verweigerung oder Altruismus/Egoismus beschrieben werden, hängt unter anderem von der genauen Form der Auszahlungsmatrix ab. Ersetzt man im Vergleich zu obiger Matrix −2 durch 2, 0 durch 3, −5 durch 0 und −4 durch 1, liegt beispielsweise Altruismus/Egoismus als Interpretation näher: Beide Spieler beginnen mit einem Gut. Ein Spieler kann auf sein Gut verzichten (Altruismus). Der Mitspieler erhielte dafür zwei (!) Güter. Behält er sein Gut (Egoismus), erfolgt keine Bestrafung oder Ähnliches. Er kann das Spiel bei einem altruistischen Mitspieler mit drei Gütern abschließen, ansonsten behält er sein eigenes Gut.

Zu den symmetrischen Zweipersonen-Nichtnullsummenspielen gehören auch das Spiel mit dem Untergang ("Feiglingsspiel", "chicken game"), die Hirschjagd, das Urlauberdilemma und das Spiel Kampf der Geschlechter.

Weitere Beispiele dafür, dass individuelle und kollektive Rationalität zu unterschiedlichen Ergebnissen führt, sind das Braess-Paradoxon und die Rationalitätenfalle.




</doc>
<doc id="11178" url="https://de.wikipedia.org/wiki?curid=11178" title="Fermi-Dirac-Statistik">
Fermi-Dirac-Statistik

Die Fermi-Dirac-Statistik (nach dem italienischen Physiker Enrico Fermi und dem britischen Physiker Paul Dirac) ist ein Begriff der physikalischen Quantenstatistik. Sie beschreibt das makroskopische Verhalten eines Systems, das aus vielen gleichen Teilchen vom Typ Fermion besteht, und gilt z. B. für die Elektronen, die in Metallen und Halbleitern für die elektrische Leitfähigkeit sorgen.

Die Ausgangspunkte der Fermi-Dirac-Statistik sind:

Die Fermi-Verteilung gibt an, mit welcher Wahrscheinlichkeit formula_1 in einem idealen Fermi-Gas bei gegebener absoluter Temperatur formula_2 ein Zustand der Energie formula_3 von einem der Teilchen besetzt ist. In der statistischen Physik wird die Fermi-Verteilung aus der Fermi-Dirac-Statistik für gleichartige Fermionen für den wichtigen Spezialfall der "Wechselwirkungsfreiheit" hergeleitet.

Zur vollständigen Beschreibung der Fermi-Dirac-Statistik siehe Quantenstatistik. Für eine vereinfachte Herleitung siehe Ideales Fermigas.

In einem System der Temperatur formula_4 lautet die Fermi-Verteilung formula_5, die die Besetzungswahrscheinlichkeit misst:

mit

Wird die Energie formula_14 vom tiefstmöglichen Einteilchenzustand aus gerechnet, heißt formula_15 auch Fermi-Energie. Die Besetzungswahrscheinlichkeit formula_1 für einen Zustand mit der Energie des Fermi-Niveaus formula_17 ist bei allen Temperaturen:

Um die bei der Energie formula_19 herrschende Teilchendichte formula_20 zu berechnen, z. B. für Elektronen in einem Metall, muss die Fermi-Verteilung noch mit der Zustandsdichte formula_21 multipliziert werden:

Am absoluten Temperaturnullpunkt formula_23 befindet sich das Fermi-Gas als ganzes in seinem energetisch tiefst möglichen Zustand, also im Grundzustand des Vielteilchensystems. Da (bei genügend großer Teilchenzahl) nach dem Pauli-Prinzip nicht alle Teilchen den Einteilchengrundzustand besetzen können, müssen sich auch am absoluten Temperaturnullpunkt formula_23 Teilchen in angeregten Einteilchenzuständen befinden. Anschaulich lässt sich das mit der Vorstellung eines "Fermi-Sees" beschreiben: jedes hinzugefügte Fermion besetzt den tiefstmöglichen Energiezustand, welcher noch nicht von einem anderen Fermion besetzt ist. Die „Füllhöhe“ bestimmt sich aus der Dichte der besetzbaren Zustände und der Anzahl der unterzubringenden Teilchen.

Entsprechend hat die Fermi-Verteilung für die Temperatur formula_9 einen scharfen Sprung bei der Fermi-Energie formula_26, die daher auch Fermi-Kante oder Fermi-Grenze genannt wird (siehe Abbildung).

Das Fermi-Niveau bei formula_23 ist daher durch die Anzahl und energetische Verteilung der Zustände und die Anzahl der Fermionen, die in diesen Zuständen unterzubringen sind, festgelegt. In der Formel erscheint nur eine Energiedifferenz. Gibt man die Größe der Fermi-Energie allein an, ist es die Energiedifferenz des höchsten besetzten zum tiefstmöglichen Einteilchenzustand. Zur Veranschaulichung oder zur schnellen Abschätzung von temperaturabhängigen Effekten wird diese Größe oft als Temperaturwert – die Fermi-Temperatur – ausgedrückt:

Bei der Fermi-Temperatur wäre die thermische Energie formula_33 gleich der Fermi-Energie. Dieser Begriff hat nichts mit der realen Temperatur der Fermionen zu tun, er dient nur der Charakterisierung von Energieverhältnissen.

Die Fermi-Verteilung gibt die Besetzungswahrscheinlichkeit im Gleichgewichtszustand zur Temperatur formula_34 an. Ausgehend von formula_35 werden bei Erwärmung Zustände oberhalb der Fermi-Energie formula_36 mit Fermionen besetzt. Dafür bleiben gleich viele Zustände unterhalb der Fermi-Energie leer und werden als "Löcher" bezeichnet.

Die scharfe Fermi-Kante ist in einem symmetrisch um formula_37 gelegenen Intervall der Gesamtbreite formula_38 abgerundet („aufgeweicht“, s. Abb.). Zustände mit kleineren Energien sind nach wie vor nahezu voll besetzt (formula_39), die Zustände bei höheren Energien nur sehr schwach (formula_40).

Da nach wie vor die gleiche Teilchenzahl auf die möglichen Zustände mit der Zustandsdichte formula_21 zu verteilen ist, kann sich die Fermi-Energie mit der Temperatur verschieben: Ist die Zustandsdichte im Bereich der angeregten Teilchen kleiner als bei den Löchern, steigt die Fermi-Energie, im entgegengesetzten Fall sinkt sie.

Im Temperaturbereich formula_42 bezeichnet man das System als entartetes Fermi-Gas, denn die Besetzung der Zustände wird maßgeblich durch das Pauli-Prinzip (Ausschließungsprinzip) bestimmt. Dies führt dazu, dass alle Zustände mit formula_43 die gleiche Wahrscheinlichkeit (von nahezu eins) haben, besetzt zu sein; dies betrifft einen im Vergleich zum Aufweichungsintervall großen Energiebereich.

Bei Energien formula_3 von mindestens einigen formula_12 oberhalb von formula_37, d. h. für formula_47, lässt sich die Fermi-Verteilung durch die klassische Boltzmann-Verteilung nähern:

„Sehr hohe Temperaturen“ sind solche weit oberhalb der Fermi-Temperatur, d. h. formula_49. Weil damit das Aufweichungsintervall sehr groß wird, so dass auch für Energien weit oberhalb der Fermi-Energie die Besetzungswahrscheinlichkeit merklich von null verschieden ist, führt die Teilchenzahlerhaltung dazu, dass die Fermi-Energie unter dem niedrigsten besetzbaren Niveau liegt. Das Fermi-Gas verhält sich dann wie ein klassisches Gas, es ist nicht entartet.

Für die Leitungselektronen in einem Metall liegt die Fermi-Energie formula_50 bei einigen Elektronenvolt, entsprechend einer Fermi-Temperatur formula_51 von einigen 10.000 K. Dies hat zur Folge, dass die thermische Energie formula_12 viel kleiner ist als die typische Breite des Leitungsbands. Es handelt sich um ein "entartetes Elektronengas". Der Beitrag der Elektronen zur Wärmekapazität ist daher schon bei Raumtemperatur vernachlässigbar und kann störungstheoretisch berücksichtigt werden. Die Temperaturabhängigkeit der Fermi-Energie ist sehr gering (meV-Bereich) und wird oft vernachlässigt.

Für Halbleiter und Isolatoren liegt das Fermi-Niveau in der verbotenen Zone. Im Bereich der Fermi-Kante existieren daher "keine" Zustände, deren Besetzung deutlich von der Temperatur abhängen kann. Dies führt dazu, dass bei einer Temperatur formula_9 das Valenzband vollständig mit Elektronen besetzt und das Leitungsband unbesetzt ist, und dass es bei formula_54 nur sehr wenige Löcher bzw. angeregte Elektronen gibt. Durch Einbringen von Fremdatomen mit zusätzlichen Ladungsträgern (Donator- oder Akzeptordotierung) kann das Fermi-Niveau nach unten bzw. nach oben verschoben werden, was die Leitfähigkeit stark erhöht. In diesem Fall verschiebt sich auch mit der Temperatur das Fermi-Niveau deutlich. Daher arbeiten z. B. elektronische Schaltungen auf Basis von Halbleitern (wie im Computer) nur in einem engen Temperaturbereich richtig.

Aus der Bedingung, dass im thermischen Gleichgewicht (bei festem formula_55 und Volumen formula_56) die freie Energie formula_57 ein Minimum annimmt, kann die Fermi-Dirac-Statistik auf schöne Art hergeleitet werden. Dazu betrachten wir formula_58 Fermionen – beispielsweise Elektronen –, die über Niveaus formula_59 verteilt sind. Die Niveaus haben Energien formula_60 und sind jeweils formula_61 - fach entartet (s. Abb.), können demnach maximal formula_62 Elektronen aufnehmen (Pauli-Prinzip). Die Anzahl Elektronen im formula_63-ten Niveau wird mit formula_64 bezeichnet. Für den Makrozustand des Systems ist unerheblich, welche der formula_58 Elektronen im formula_63-ten Niveau sind und welche der formula_62 Zustände darin sie besetzen. Der Makrozustand wird daher vollständig durch die Folge der Zahlen formula_68 bestimmt.

Für eine beliebige Verteilung der Elektronen auf die Niveaus gilt:

Gleichung (1) gibt die Gesamtzahl der Teilchen wieder, die konstant gehalten werden soll, während die einzelnen formula_64 variiert werden, um das Minimum von formula_71 zu finden. Gleichung (2) gibt die zur vorliegenden Verteilung gehörende Energie formula_3 des Systems an, wie sie in die Formel für formula_71 einzusetzen ist. Gleichung (3) ist (nach Ludwig Boltzmann) die Entropie des Zustands des Systems (Makrozustand), wobei formula_74 die thermodynamische Wahrscheinlichkeit für die betreffende Folge der Besetzungszahlen formula_68, angibt, also die Anzahl der möglichen Verteilungen (Mikrozustände) von jeweils formula_64 Elektronen auf formula_77 Plätze, für alle Niveaus formula_59 zusammen.

Um die Verteilung zu finden, bei der durch Variation der formula_64 unter der Nebenbedingung formula_80 die freie Energie formula_71 minimal wird, benutzen wir die Methode der Lagrange-Multiplikatoren. Es ergibt sich

Darin ist formula_84 der (von formula_83 unabhängige) Lagrange-Multiplikator. Für die Berechnung der Ableitung formula_86 wird die explizite Formel für formula_87 benötigt:

Dabei ist
der Binomialkoeffizient, d. h. die Anzahl der Möglichkeiten, unter formula_77 Objekten formula_64 verschiedene auszuwählen.

Mit Hilfe der vereinfachten Stirlingformel
formula_92
ergibt sich weiter

und damit

Insgesamt wird Gleichung (2) zu

Einsetzen der durch formula_96 gegebenen Besetzungswahrscheinlichkeit formula_97 und Umstellung ergibt:

Dies ist die Fermi-Dirac-Statistik. Der Lagrangemultiplikator erweist sich als ihr chemisches Potential formula_99.

In Festkörpern kann die Fermi-Verteilung sehr gut beobachtet werden, wenn die elektronische Besetzungsdichte des Leitungsbandes in Abhängigkeit von der Energie gemessen wird. Ein besonders gutes Beispiel für das ideale Fermigas liegt bei Aluminium vor. Mit solchen Studien lässt sich auch das Auflösungsvermögen einer Messapparatur bestimmen, indem man den Verlauf der Verteilung bei einer bestimmten Temperatur misst und mit der Formel für die Fermi-Verteilung vergleicht.

Weitere Beispiele zur Bedeutung siehe unter Fermi-Energie.




</doc>
<doc id="11179" url="https://de.wikipedia.org/wiki?curid=11179" title="Spin-Statistik-Theorem">
Spin-Statistik-Theorem

Unter dem Spin-Statistik-Theorem der Quantenphysik versteht man die theoretische Begründung für den empirischen Befund, dass alle Elementarteilchen mit halbzahligem Spin der Fermi-Dirac-Statistik folgen, d. h. sog. Fermionen sind, hingegen alle Teilchen mit ganzzahligem Spin der Bose-Einstein-Statistik folgen, d. h. sog. Bosonen sind.

Spin ist der Eigendrehimpuls der Teilchen. Alle derzeit nachgewiesenen Teilchen haben entweder ganzzahligen (0, 1, 2, ...) oder halbzahligen (1/2, 3/2, 5/2, ...) Spin, jeweils in Einheiten der reduzierten Planck-Konstanten formula_1.

Andererseits folgen alle Teilchen entweder der Fermi-Dirac- oder der Bose-Einstein-Statistik. Diese Statistiken beschreiben das kollektive Verhalten ununterscheidbarer Teilchen (der gleichen Sorte): jeweils nur ein einziges Fermion (Pauli-Prinzip), aber beliebig viele Bosonen können sich in einem bestimmten Quantenzustand befinden. Im Formalismus der Quantenmechanik wird das dadurch ausgedrückt, dass die Wellenfunktion einer Gruppe ununterscheidbarer Fermionen antisymmetrisch ist, d. h. bei Vertauschung der Parameter zweier Fermionen ihr Vorzeichen wechselt, während die Wellenfunktion einer Gruppe ununterscheidbarer Bosonen symmetrisch ist, d. h. bei Vertauschung der Parameter zweier Bosonen ihr Vorzeichen "nicht" ändert.

Beispiele für Fermionen sind Elektronen, Protonen und Neutronen, Beispiele für Bosonen sind Photonen, He-Atome und deren Kerne, die Alphateilchen.

Die Fermi-Dirac-Statistik liefert u. a. die Grundlage für die Erklärung des Periodensystems der Elemente, die Bose-Einstein-Statistik u. a. die Erklärung für die Suprafluidität des He bei niedrigen Temperaturen.

Obwohl der Spin und die beiden Statistiken schon 1926 bekannt waren, fanden erst Markus Fierz 1939 und Wolfgang Pauli 1940 theoretische Begründungen für den Zusammenhang von Spin und Statistik. In beiden Begründungen und den zahlreichen späteren Verallgemeinerungen und Verfeinerungen spielt die relativistische Quantenfeldtheorie eine entscheidende Rolle. Feynman kritisierte diese Begründungen wegen ihrer Kompliziertheit und schloss, dass das grundlegende Prinzip nicht vollständig verstanden sei. Als unzureichend werden die vereinzelten Versuche kritisiert, das Theorem auch im einfacheren Rahmen der nichtrelativistischen Quantenmechanik zu beweisen, die einem anderen Vorschlag Feynmans folgten.



</doc>
